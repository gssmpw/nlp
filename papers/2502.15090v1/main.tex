\pdfoutput=1

\documentclass[11pt]{article}

\usepackage[preprint]{acl}

\usepackage{times}
\usepackage{latexsym}
\usepackage{wrapfig}
\usepackage{dblfloatfix}
\usepackage{amsmath}

\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}

\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{subcaption} 

\usepackage{csquotes}


\title{Analyze the Neurons, not the Embeddings: Understanding When and Where LLM Representations Align with Humans}


\newcommand{\eg}[1]{\textcolor{blue}{{\bf EG}:  \textit{#1}}}
\newcommand{\todo}[1]{\textcolor{red}{#1}}



\author{Masha Fedzechkina \and Eleonora Gualdoni \and Sinead Williamson \and Katherine Metcalf \and \\ \bf{Skyler Seto} \and Barry-John Theobald \\
         Apple \\
\texttt{\{mfedzechkina, e\_gualdoni,  sa\_williamson, 
         kmetcalf, bjtheobald,
         sseto\}@apple.com} \\}
         
        
         

\begin{document}
\maketitle
\begin{abstract}
Modern large language models (LLMs) achieve impressive performance on some tasks, while exhibiting distinctly non-human-like behaviors on others.  This raises the question of how well the LLM's learned representations align with human representations. In this work, we introduce a novel approach to the study of representation alignment: we adopt a method from research on activation steering to identify neurons responsible for specific concepts (e.g., ``cat'') and then analyze the corresponding activation patterns. Our findings reveal that LLM representations closely align with human representations inferred from behavioral data. Notably, this alignment surpasses that of word embeddings, which have been center stage in prior work on human and model alignment. Additionally, our approach enables a more granular view of how LLMs represent concepts. Specifically, we  show that LLMs organize concepts in a way that reflects hierarchical relationships interpretable to humans (e.g., ``animal''-``dog''). 
\end{abstract}

\section{Introduction}

Large language models (LLMs) exhibit impressive performance on a variety of tasks from text summarization \citep{basyal2023text, jin2024survey} to zero-shot common-sense reasoning \citep{park2024zeroshot, shwartz-etal-2020-unsupervised}, and are increasingly deployed as a human proxy \cite{just2024data, klissarov2023motif, cui2024ultrafeedback, peng2024_bstractions}. At the same time, there is a growing body of evidence suggesting that LLMs exhibit patterns of behavior distinctly different from humans --- for instance, hallucinating information \citep{bubeck2023sparks, lin2022truthful} or memorizing complex patterns to solve reasoning tasks \cite{ullman2023_tom}. Such behaviors raise the question of how closely the conceptual representations learned by these models align with the conceptual representations in humans as safe and trustworthy deployment of LLMs requires such alignment. Overall, unveiling aspects of representation alignment and understanding how to foster it can help us %
identify and mitigate misaligned LLM behaviors, thus increasing trust in and safety of models \citep{gpt4, shen2024valuecompassframeworkfundamentalvalues}. 

Prior work has examined the relationship between human-perceived similarity among concepts (i.e., word/image meaning) and various LLM-derived measures of similarity, such as confidence \citep{shaki2023cognitive} or the embedding distance \citep{bruni-etal-2012-distributional,digutsch2023overlap, muttenthaler2023human}. While these approaches have significantly advanced our understanding of how conceptual representations align between humans and models, they suffer from a major limitation: they do not reveal where in the model the concepts are stored and make it difficult to draw conclusions beyond coarse alignment.
For example, the cosine distance between embeddings might indicate that ``animal'' and ``dog'' are more similar than ``animal'' and ``daffodil'', but it can not tell us if ``dog'' and ``animal'' are processed with similar neural pathways or architectural components, %
limiting our ability to understand the existence of structures such as hierarchical relationships in the model.


Here, we propose a novel way to study human - LLM alignment in concept representation. We borrow a method from activation steering \cite{suau2023self, suau2024whispering, rodriguez2024controlling}, to identify which neurons are most responsible for processing and understanding of a particular concept, so-called \textit{expert neurons}. 
 This approach enables us not only to measure alignment between human and model representations, but also to explore
additional questions, such as whether LLMs organize concepts in a hierarchy interpretable to humans (e.g., ``dog'', ``cat'', and ``cheetah'' being categorized as ``animal''). We also track how alignment evolves during training for different model sizes, shedding light on the impact of model capacity on the development of aligned representations --- an aspect largely overlooked in previous work on text-based models \citep{shen2024valuecompassframeworkfundamentalvalues, wei2022emergentabilitieslargelanguage}. Ultimately, understanding these internal structures and factors that lead to mis-alignment can provide valuable insight for designing interventions targeted at guiding model behaviors towards human-like solutions and enhancing their transparency \citep{Fel2022, peterson2018, Toneva2022}.




In our experiments, we focus on causal LLMs using the Pythia models ($70$m, $1$b and $12$b) for which multiple training checkpoints are publicly available  \cite{pythia}. Given a diverse set of concepts across multiple domains (see Sec.~\ref{sec:data}), we identify each LLM's corresponding expert neurons. We measure their similarity at the LLM level as the amount of overlap between the expert neurons. We then evaluate the alignment between human and LLM representations by testing whether the similarity between neural activations correlates with human-perceived concept similarity, and whether the LLMs learn hierarchical structures similar to those observed in human category systems \citep{rosch1978principles}.
Finally, we identify the location of the model's concept representations and the point they form  during training.

Our results show that LLM representations are generally aligned with humans. Crucially,
expert neurons capture human alignment significantly better than the single word embeddings used in prior work. Moreover, such alignment emerges early in training, with model size playing only a small role: a $70$m LLM is less aligned than a $1$b or $12$b LLM trained on the same data, but there is no difference between the larger models.  Finally, patterns in expert neurons reveal that the LLMs show a human-like hierarchical organization of concepts.









\section{Related work}

\paragraph{Representation alignment}

Studies on the kinds of representations used by humans and machines have been of interest to many fields \citep[e.g., cognitive science, neuroscience, and machine learning;][]{hebart2020revealing, Khosla2022, muttenthaler2023human, tian2022contrastive}. Studies on \textit{representation alignment} \cite{sucholutsky2024gettin} look specifically at the extent to which the internal representations of humans and neural networks converge on a similar structure. 
Across vision and text domains, models show notable alignment with human similarity judgments ---typically used as a window into human representational structures. \citet{peterson2018} report significant alignment between human similarity judgments and representations of object classification networks, while \citet{digutsch2023overlap} report similar alignment with GPT-3’s \citep{gpt3} embeddings. However, \citet{shaki2023cognitive} finds that GPT-3's concept alignment is highly sensitive to prompt phrasing and
\citet{misra-etal-2020-exploring} show that alignment in BERT \citep{bert} is very context-dependent.  Investigating general factors that can cause mis-alignment, %
\citet{muttenthaler2023human} conclude that the training dataset and objective function impact alignment, but model scale and architecture have no significant effect. Of note, alignment and performance are not inherently tied: mis-aligned models can exhibit significant capabilities \cite{sucholutsky2023alignment, dessì2023communication}. %


 



\paragraph{Activation steering} refers to a class of methods that intervene on a generative model's activations to perform targeted updates for controllable generation \cite{rodriguez2024controlling, iti, rimsky_steering}. \citet{suau2023self} propose a method to identify sets of neurons in pre-trained transformer models that are responsible for detecting inputs in a specific style \citep[e.g., toxic language]{suau2024whispering} or about a specific concept \citep[e.g., ``dog'']{suau2023self}. Intervening on the expert neuron activations, %
successfully guides text generation into the desired direction. In a similar spirit, \citet{turner2024_steering} use a contrastive prompt (one positive and one negative) to induce sentiment shift and detoxification, while
\citet{kojima2024multilingual} steer multilingual models to produce more target language tokens in open-ended generation. Finally, \citet{rodriguez2024controlling} introduce a unified approach to steer activations in LLMs and diffusion models based on optimal transport theory.
 




\section{Methods}
\subsection{Finding expert neurons}
\label{sec:expert_extraction}
We adopt the \textit{finding experts} approach introduced by \citet{suau2023self} for activation steering, to study representational alignment. The motivation is two-fold: a) this approach has been successfully applied to detect neurons responsible for everyday concepts like ``dog'', which is the focus of this work; b) it is able to distinguish the different senses of a homophone (e.g., ``apple'' as a fruit or company), suggesting that this method is able to pick up fine-grained semantic distinctions. 


To identify experts neurons for a given concept, each neuron is evaluated in isolation as a binary classifier: a neuron is considered an expert if its activations effectively distinguish between input data where the concept is present (henceforth \emph{positive set}) and input data where the concept is absent (henceforth \emph{negative set}). The performance of each neuron as a classifier for the concept (i.e., its expertise) is measured as the area under the precision-recall curve (AP).
We consider neurons with an AP score above a given threshold, $\tau$, for a concept to be expert neurons for that concept.  $\tau$ can be thought of as quality of an expert neuron -- the larger the $\tau$ values the greater a neuron's expertise for a given concept.  %
In our experiments, we consider a range of values for $\tau\in[0.5, 0.9]$ ranging from a low (classification accuracy above chance) to a high level of expertise.



\subsection{Data}
\label{sec:data}
To understand the alignment between human and model representations, we examine how patterns in expert neurons relate to perceived concept similarity in humans.  We obtain human similarity judgments from the MEN dataset \cite{MEN_dataset}, which contains $3,000$ word pairs annotated with human-assigned similarity judgments crowd-sourced from Amazon Mechanical Turk. 

For each concept under consideration, we generate a set of sentences containing that concept. To ensure dataset diversity, half of each positive dataset is generated with a prompt eliciting story descriptions and half of the dataset is generated 
with a prompt eliciting factual descriptions of the target concept (the prompts, along with sample generations, are provided in App.~\ref{app:appendix_prompts}). The negative sets are sampled from the datasets for 
the remaining non-target concepts (e.g., if we are considering $1000$ concepts, one of which is ``cat'', the negative set is sampled from $999$ concepts excluding ``cat''). 

To study whether the LLMs represent concepts hierarchically (Sec.~\ref{sec:main_experiments}), we manually generate lists of ten domains, organized in human-interpretable hierarchies with four concepts per domain (e.g.,  the domain ``animal'' containing concepts ``cat'', ``dog'', ``cheetah''', and ``horse''; the full set of domains and concepts is provided in App.~\ref{app:domains_list}). We choose not to use WordNet \citep{wordnet} --- a lexical database of English annotated with a hierarchical structure --- because of drawbacks identified in its hierarchical structure, which often make the hierarchical relationships it presents unintuitive \citep[for a discussion, see][]{gangemi2001conceptualanalysislexicaltaxonomies}.



For dataset generation, we experiment with three models of different performance levels: GPT-4 \cite{gpt4}, Mistral-7b-Instruct-v0.2 \cite{mistral}, and an internal 80b-chat model. 





\subsection{Models}
We use GPT-2 \cite{gpt2} to select hyper-parameters (e.g., the size of a positive and negative datasets) and validate that our data identifies a stable set of experts (see Sec.~\ref{sec:pilot} for details).
For all other experiments, we use models from the Pythia family \cite{pythia}, specifically focusing on model sizes $70$m (smallest), $1$b, and $12$b (largest), to understand the impact of model size on representational alignment. The size of each  model is connected to its performance. The mean accuracy and standard error across eight benchmarks (Table \ref{tab:benchmarks})
is $0.27$ ($0.01$) for the $70$m, $0.28$ ($0.01$) for the $1$b model, and $0.32$ ($0.02$) for the $12$b model at the end of training.


\begin{table}[h!]
\centering

\setlength{\tabcolsep}{3pt}
\begin{tabular}{p{4cm} l}
\toprule
\multicolumn{2}{c}{Benchmarks} \\
\midrule
\small LAMBADA – OpenAI & \small \citet{lambada} \\
\small PIQA & \small \citet{piqua} \\
\small SciQ & \small \citet{SciQ}\\
\small  ARC (easy and hard) & \small  \citet{arc}\\
\small  WinoGrande & \small \citet{winogrande} \\
\small  MMLU & \small \citet{mmlu}\\
\small  LogiQA & \small \citet{logiqa}\\
\small Winograd Schema Challenge & \small \citet{winograd}\\
\bottomrule
\end{tabular}
\caption{Pythia evaluation benchmarks.} 
\label{tab:benchmarks}
\end{table}


For each model, we work with  checkpoints $1$, $512$, $1$k, $4$k, $36$k, $72$k, and $143$k, to track how representational alignment develops throughout training. All Pythia models were trained on the same data presented in the same order and thus allow us to evaluate the impact of model size and number of training steps on representational alignment while controlling for the data.

\section{Can we reliably identify experts?}
\label{sec:pilot}
While the success of expert-based methods at steering model activations is well-documented \cite{suau2023self, suau2024whispering}, our interest is in studying model representations through the patterns in experts. Given the novel application of the method, we conduct a pilot study to explore the impact of dataset size, the model used to generate the dataset, and the exact sentences used to represent a concept on the stability of the discovered expert sets. 

For the pilot study, we sample $50$ word pairs from the training split of the MEN dataset. For each concept in the word pair, we generate a positive set containing $7000$ sentences from three models: GPT-4, Mistral-7b-Instruct-v0.2, and an internal 80b-chat model. We sweep over positive set sizes of $100$, $200$, $300$, $400$, and $500$ sentences, and negative set sizes of $1000$ and $2000$ sentences. For each positive and negative set combination, we repeat expert extraction eight times (folds) with the sets randomly sampled from the full pool of sentences. 

We examine how sensitive the discovered experts are to the specific slice of the positive and negative sets (the 8 folds). We measure sensitivity in terms of the stability in experts across the folds, where high stability occurs when there is large overlap in the experts across folds. To assess overlap, we look at Jaccard similarity between expert sets across folds, using a range of thresholds $\tau$. 



\begin{figure*}[h]
     \centering
         \includegraphics[width=\textwidth]{pilot_sweep_no_cos.pdf}
         \caption{Expert discovery is relatively stable across various dataset characteristics. Points represent condition means; error bars represent bootstrapped $95\%$ confidence intervals. Columns represent the size of the positive set (number of unique sentences); rows represent the size of the negative set (number of unique sentences).}
         \label{fig:pilot_size_sweep}
     \hfill
\end{figure*}


The findings are shown in Fig. \ref{fig:pilot_size_sweep} for each dataset configuration (subplot) and value of $\tau$ (x-axis). The expert neurons discovered across different data configurations and folds (indicated by the error bars) are stable as indicated by a high ($\sim 0.8$) overlap proportion and show little sensitivity to our manipulations. Interestingly, the LLM (line color) used to generate the probing dataset matters little --- while stronger models generate more diverse datasets (mean type/token ratio of $0.34$, $0.21$ and $0.18$ for GPT-4,  internal 80b-chat, and Mistral-7b-Instruct-v0.2 respectively), resulting in a somewhat higher expert overlap, the gain is too small to warrant their increased cost. Expert overlap increases with every increase in the size of the positive set but the increases are small beyond $300$ sentences, and performance for $400$ sentences is virtually indistinguishable from $500$ sentences. 
Interestingly, a larger negative set results in lower expert overlap at higher $\tau$ values and an increased variability across folds. One reason could be that as the size of the negative set increases so does the probability of the negative set containing sentences related to the target concept. For example, a sentence about ``cats'' may also talk about ``dogs''.  A second explanation could be that the larger negative set activates more polysemous neurons.
Based on these findings, we conduct all subsequent analyses with a positive set of $400$ sentences and a negative set of $1000$ sentences, all generated with Mistral-7b-Instruct-v0.2.

\section{Are model and human representations aligned?}
\label{sec:main_experiments}
Having determined the appropriate hyper-parameters to capture a stable set of experts, we turn to the first main question of our study --- whether expert neurons capture semantic information meaningful to humans. We measure the alignment between LLM and human representations as the correlation between the human versus the 
LLM's similarity score for a each pair of concepts in the test split of the MEN data ($1000$ pairs). The LLM's similarity score is the Jaccard similarity between expert sets for $\tau\in \{0.5, 0.6, 0.7, 0.8, 0.9\}$. In App.~\ref{app:cosine}, we consider cosine similarity between the raw AP values as an LLM similarity score, finding very similar correlations to those obtained with Jaccard similarity ($\tau=0.5$), suggesting that what matters most for alignment is not the magnitude of the AP value, but rather whether it is above or below 0.5 (i.e., whether the neuron is positively or negatively associated with the concept).


\begin{figure}[h]
     \centering
         \includegraphics[width=\linewidth]{men_correlations_main_exp.pdf}
         \caption{Model representations of similarity are closely aligned with human ones. Points represent Spearman correlations between the expert neuron overlap and perceived human similarity in the MEN dataset; error bars represent bootstrapped $95$\% confidence intervals. The subplots are $\tau$. The correlations are statistically (p<0.05) at all checkpoints except for the first one.}
         \label{fig:correlations}
     \hfill
\end{figure}




\paragraph{Expert neuron overlap is highly aligned with human similarity judgments} 


We find that model representations are closely aligned with humans, with the highest alignment occurring at $\tau=0.5$. At the final checkpoint, the Spearman correlations between expert overlap ($\tau=0.5$) and MEN similarity are: $0.70$, $0.77$, $0.79$ for $70$m, $1$b, and $12$b respectively. For reference, agreement between humans has a Spearman correlation of $0.84$.
Interestingly, model size has a small impact on this alignment \citep[in line with findings from][]{muttenthaler2023human}: the $1$b and $12$b models are virtually indistinguishable, with the $70$m model slightly less aligned. The models start diverging in how well aligned they are with humans as $\tau$ increases, with larger models being more aligned. The reason for this is that smaller models have fewer experts compared to larger models (see Fig. \ref{fig:expert_set_size}) resulting in a lot of empty expert set intersections for higher levels of $\tau$.

\paragraph{Word embeddings are less aligned than expert sets} Prior work has focused on the analysis of embeddings when considering alignment in LLM and human representations \cite{digutsch2023overlap}. We hypothesize that expert sets are more correlated with human representations than word embeddings as they disambiguate different word senses \cite{suau2023self}. To test this, we extract the embeddings for each word in the MEN test split from the final hidden layer of the three Pythia models at each checkpoint and compute cosine similarity between the embeddings for each word pair in the MEN test split. We then correlate the cosine similarity with the corresponding human similarity judgement \cite{digutsch2023overlap}. We find statistically significant correlations (p<0.05) between the cosine similarity of the embeddings for a given concept pair and their similarity in the MEN dataset, at all checkpoints except for the first (see Fig. \ref{fig:embed_correlations}), consistent with prior work \cite{digutsch2023overlap}. However, as expected under our hypothesis, the correlations with human similarity are significantly lower for single word embeddings compared to the experts neurons (highest correlations are~$0.25$ vs.~$0.79$ for the embeddings vs.\ experts). Single word embeddings exhibit  more variability in alignment, as indicated by larger confidence intervals within each checkpoint, and their pattern of alignment is less stable across checkpoints compared to that of the experts.







\begin{figure}[h]

\includegraphics[width=\linewidth]{emb_correlations.pdf}
 \caption{Spearman correlations between the cosine similarity in the embeddings and perceived human similarity in the MEN test split. Error bars represent bootstrapped $95$\% confidence intervals. The correlations are statistically (p<0.05) at all checkpoints except for the first one.}
         \label{fig:embed_correlations}
\end{figure}


\section{Do models organize concepts in hierarchies?}
\label{sec:hierarchies}

Some domains within the human conceptual system are organized in hierarchies, where broader categories include more specific categories. For example, the concept ``dog'' falls under ``animal'', meaning that all dogs are animals \citep{graf2016animal, murphy2004big, rosch1978principles}. This raises the question of whether models organize concepts in a human-like hierarchy. We propose that, if the model organizes domains in a hierarchical fashion interpretable to humans, concepts from related sub-categories should share a set of experts (e.g., ``dog'', ``cat'',  ``horse'', and ``cheetah'' under the concept ``animal''). Additionally, some of these shared experts should also be associated with the broader concept (``animal'' in our example), suggesting that the model recognizes it as an overarching concept that includes its sub-categories.


To assess this, we consider the list of hierarchically organized domains we generated (see Sec.~\ref{sec:data} and App.~\ref{app:domains_list}), the experts associated with each concept in the list ($\tau$=$0.5$), and their reciprocal overlap. We discuss the final training checkpoint of Pythia $12$b in the main text and present other model sizes and checkpoints in App.~\ref{app:hierarchies}.

Let $super$ be a super-ordinate concept (e.g.,``animal'') and let $sub_1, sub_2, ... sub_i$ be sub-ordinate concepts falling under it (e.g., ``dog'', ``cat'', ``horse'', ``cheetah''). We call \( E(c) \) the set of experts specialized for a concept \( c \). For each domain, Table \ref{tab:hierarchies_12b_143000} reports
the percentage of experts shared among the sub-ordinate concepts that are also shared with the super-ordinate concept: \[
    \textstyle \frac{|E(super) \cap \bigcap_{i=1}^{4} E(sub_i)|}{\left| \bigcap_{i=1}^{4} E(sub_i) \right|} \times 100.
    \]
For example, Table~\ref{tab:hierarchies_12b_143000} indicates that 42.46\% of the experts shared by ``dog'', ``cat'', ``horse'', and ``cheetah''  are also shared with ``animal''. %
 


\begin{table}[h!]
\centering

\setlength{\tabcolsep}{3pt}
\begin{tabular}{l|c|c|c}
\toprule
Domain & 
\% overlap &
baseline 1 &
baseline 2 \\
\midrule
animal   & 42.46   &  \textcolor{gray}{0.58}   &  \textcolor{gray}{ 8.31}\\
clothes  & 43.07 &  \textcolor{gray}{6.67} & \textcolor{gray}{7.84} \\
colour   & 54.35  & \textcolor{gray}{4.83}  & \textcolor{gray}{2.66} \\
furniture & 69.98    & \textcolor{gray}{9.20}  &  \textcolor{gray}{ 5.48}\\
occupation & 32.31   &  \textcolor{gray}{2.56}  & \textcolor{gray}{5.68}\\
organ   & 57.95  &  \textcolor{gray}{7.40}  &  \textcolor{gray}{8.21}\\
sport   & 75.76   & \textcolor{gray}{9.24}   & \textcolor{gray}{12.67} \\
subject & 65.77   &  \textcolor{gray}{5.44}   &  \textcolor{gray}{6.27} \\
vegetable & 69.25   &  \textcolor{gray}{6.09}  & \textcolor{gray}{ 10.92} \\
vehicle   & 73.61    & \textcolor{gray}{6.12}   &  \textcolor{gray}{14.61}\\
\bottomrule
\end{tabular}
\caption{Pythia $12$b. Results of our exploration of hierarchically-organized domains at step $143$k. For each domain, we report %
the percentage of experts shared among the sub-ordinate concepts that is also shared with the super-ordinate one. Numbers in gray correspond to our baselines. The percentage of experts shared in real domains is significantly higher than in the baselines as assessed via a two-sample permutation test \citep[][p-values $< 0.001$]{scipy}.} %
\label{tab:hierarchies_12b_143000}
\end{table}







Our results confirm that the model captures hierarchical domain structures that characterize human conceptual systems. Within each domain, a large portion of experts for sub-ordinate concepts is shared with the super-ordinate concept. This suggests that the model recognizes the sub-ordinate concepts as part of the super-ordinate concept (e.g., that dogs are animals).

To rule out the possibility that expert sets overlap by chance, we compare the values we have obtained against two baselines (see Table \ref{tab:hierarchies_12b_143000}, numbers in gray). In the first baseline, we randomize the super-ordinate concepts in our dataset by assigning each of them to \textit{N} randomly selected sub-ordinates (e.g., associating ``animal'' with a random list of concepts like ``jacket'', ``liver'', ``doctor'', and ``red''). In the second baseline, we shuffle the associations across concept categories, assigning each super-ordinate concept to a random set of internally related sub-ordinates (e.g., associating ``animal'' with ``sock'', ``shirt'', ``jeans'', and ``jacket'').\footnote{Of note, the randomization procedure does not prevent us from sampling, in some of the iterations, correct super-ordinate - sub-ordinate concept pairs (baseline 1; e.g., ``animal'' and ``dog'') or correct domain associations (baseline 2; e.g., ``animal'' with its sub-ordinates). Baseline values likely would be even lower if we enforced incorrect associations only.} The results show that, when the domain structure is randomized, no hierarchical pattern emerges, reinforcing the robustness of our findings.

Interestingly, the patterns observed for the largest model do not largely differ from those found for smaller models. Additionally, the model seems to converge on a stable hierarchical representation around checkpoint $4$k (App.~\ref{app:hierarchies}). This finding aligns with our later analyses, highlighting this checkpoint as a crucial transition point during training.




\begin{figure}[ht]
    \centering
    \includegraphics[width=\columnwidth]{HEATMAP_experts_step143000_12b_t0.5.pdf}
    \caption{Proportion of expert overlap between word pairs belonging to hierarchically organized domains. Domains can be identified based on the stronger associations among their concepts.}
    \label{fig:hierarchies}
\end{figure}



Having examined the overall structure of hierarchically organized domains by analyzing the relationship between groups of sub-ordinate concepts and their super-ordinate counterpart, we now shift our focus to concept pairs. We explore how super-ordinate concepts relate to individual sub-ordinate concepts, as well as the relationship between sub-ordinate concept pairs. We compute the overlap between the expert sets of each pair of concepts in our domains. A subset of the results is shown in Fig.\ \ref{fig:hierarchies} (see App.~\ref{app:hierarchies2} for full results).


Overall, even if domain-specific differences are visible, concepts within the same domain, thus semantically related, tend to have a higher overlap in their expert sets. This is in line with the findings on representation alignment discussed in Sec.~\ref{sec:main_experiments}, showing that concepts perceived as similar by humans show high expert overlap and are also perceived as more similar by the LLMs.
When exploring the internal organization of the domains, we notice that super-ordinate concepts have a weaker association with their sub-ordinate concepts compared to the association between pairs of sub-ordinate concepts. For instance, the super-ordinate concept ``vehicle'' is associated with all of its sub-ordinates, but these associations are weaker than those between ``motorcycle'' and ``bicycle''. The same happens in the ``animal'' domain for ``dog'' and ``cat''.
This may suggest that individual sub-ordinate concept pairs may be distributionally more similar to each other than sub-ordinate - super-ordinate pairs. 

Taking stock, our results show that the model captures human-interpretable hierarchical structures in concept representations, assigning some experts to be shared between sub-ordinate and corresponding super-ordinate concepts. 

\section{Characterizing model knowledge}
We conclude by characterizing the differences in experts as a function of model size and stage of training by reanalyzing the data from Sec.~\ref{sec:main_experiments}.


\paragraph{Larger models have more experts} Larger models allocate more experts to a given concept (see Fig. \ref{fig:expert_set_size}; the pattern does not change after scaling the raw number of experts by the number of neurons in the model). As $\tau$ increases, fewer experts are identified and the drop is more pronounced for smaller models. 
Overall, larger models have a greater capacity to learn a higher number of experts and a higher number of \textit{more specialized} experts. This increased specialization may contribute to finer-grained concept representations and ultimately better performance on downstream tasks.


\begin{figure}[h]

\includegraphics[width=\linewidth]{expert_size.pdf}
 \caption{Expert set size (log) by model size and checkpoint. The points represent averages over all concepts. The error bars are bootstrapped $95$\% confidence intervals. Subplots correspond to different values of $\tau$.}
         \label{fig:expert_set_size}
\end{figure}

\paragraph{More specialized experts take longer to learn} 
We next look at the dynamics of learning experts across checkpoints. We calculate expert overlap (Jaccard similarity) for each concept across subsequent checkpoints in our data. As shown in Fig. \ref{fig:expert_overlap_cps}, the stability of the discovered expert set grows as training progresses. 
Early in training (prior to step $36$k), the expert overlap between subsequent checkpoints is low across model sizes, suggesting that semantic knowledge has not been acquired yet. The more $\tau$ increases (corresponding to higher expert specialization), the more checkpoints it takes for the expert set to stabilize, suggesting that higher-quality experts take longer to learn.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{expert_overlap_across_cps.pdf}
    \caption{Proportion of expert overlap across subsequent checkpoints. Points represent across concept averages; error bars represent bootstrapped $95$\% confidence intervals. Subplots correspond to different values of $\tau$.}
    \label{fig:expert_overlap_cps}
\end{figure}


\paragraph{More experts are found in MLPs and deeper layers} Pythia models consist of intertwined self-attention and MLP layers \citep{pythia, vaswani2023attentionneed}, each serving different functions \citep{geva2021transformer, jawahar-etal-2019-bert, liu-etal-2019-linguistic}. We analyze the distribution of experts within these layers. %
Fig.~\ref{fig:layers_12b_totals} shows the patterns for Pythia $12$b ($\tau$=0.5). Larger numbers of experts are located in the MLP layers compared to attention layers with the allocation of experts to different layer types stabilizing at checkpoint $4$k. 
We see the same trend in smaller models (App.~\ref{app:add_plots_totals_layers}) after controlling for the number of neurons in the respective layers.
Moreover, the mean number of experts generally increases with layer depth in MLPs, with checkpoint $4$k again displaying the first recognizable structure (see Fig. \ref{fig:layers_MLP_12b} and App.~\ref{app:add_layers_plots_MLPs}). 
For attention layers, high numbers of experts are located in deep layers and, interestingly, in the first layer (see App.~\ref{app:add_layer_plots_attention}). Of note, if we focus our analysis on highly specialized experts only ($\tau$=0.9), we find higher numbers of experts in earlier layers (see App.~\ref{app:add_layer_plots_MLP_Suau} and \ref{app:add_layer_plots_atts_Suau}), recovering the same patterns as identified in \citet{suau2020findingexpertstransformermodels}.
Our findings align with prior research on the role of layers at different depths, identifying %
deeper layers as responsible for processing higher-level semantic knowledge captured by expert neurons \citep{geva2021transformer, jawahar-etal-2019-bert}. 



\begin{figure}[htb]
\centering
\begin{subfigure}{\linewidth}
    \includegraphics[width=\linewidth]{total_experts_layers_12b.png}
    \caption{}
    \label{fig:layers_12b_totals}
\end{subfigure}
\hfill
\begin{subfigure}{\linewidth}
    \includegraphics[width=\linewidth]{layers_stats_12b_united_MLPs.png}
    \caption{}
    \label{fig:layers_MLP_12b}
\end{subfigure}

\caption{Pythia $12$b. (a) Total number of experts in MLP and attention layers across checkpoints; more experts are located in MLPs; {(b)} Average number of experts identified in MLP layers at different depths, for different checkpoints.}
\label{fig:layers_12b}
\end{figure}









\section{Conclusion}

We present a novel approach to study alignment between human and model representations based on the patterns in expert neurons. Representations captured by these neurons align with human representations significantly more than word embeddings, and approach the levels of alignment between humans. Consistent with prior work \cite{muttenthaler2023human}, we find that model size has little influence on alignment. Moreover, our approach reveals that models generally organize concepts into human-interpretable hierarchies.
However, some domains are more structured than others, and this pattern remains consistent across model sizes. We leave it to future work to investigate the factors that could give rise to this pattern, such as the frequency of each domain in the training data. 





\section{Limitations}

\paragraph{We consider only a simple case of similarity} Consistent with prior work \cite{digutsch2023overlap, shaki2023cognitive, misra-etal-2020-exploring}, we study alignment between human and model representations, which we operationalize as the similarity between two concepts. We find that model size does not play a large role in alignment: even models as small as $70$m excel in this alignment test. While this finding is consistent with previous literature \cite{muttenthaler2023human}, it is also possible that our task is too simple to distinguish between the models. This is supported by the observations that semantic relationships studied here start emerging early in training (around checkpoint $4$k out of $143$k). Future work will consider more complex cases of alignment, such as value alignment.

\paragraph{We do not study patterns in expert neurons through activating these neurons} Since the approach we are using was designed for activation steering \cite{suau2023self}, one obvious application is to examine the intersections between the expert sets for two concepts through the lens of controllable generation. For instance, we could have activated the shared experts between ``animal'' and ``dog'' and examined model generations after the activation. We chose not to do this for the following reason: the approach we are using requires choosing the number of experts and the original work \cite{suau2023self} has shown that this choice impacts the quality of generations and the degree to which a concept is expressed --- an effect that we also observed in our preliminary investigations. We leave such hyper-parameter search to future work: a priori, we do not have a clear hypothesis about whether activating more specialized experts \textit{vs} less specialized ones within the intersection would lead to distinct generation patterns; or if any discernible pattern in those generations should be expected at all. 
Given these uncertainties,
we did not feel confident that this analysis would yield reliable results. Other approaches do not require choosing the number of experts \cite{rodriguez2024controlling}, but these approaches are designed to change the activations of all neurons in the network and are thus not applicable for our use case. 


\paragraph{We do not have access to training data} To fully understand how knowledge develops in LLMs, we need to know what the model has seen at different points in training. Unfortunately, the Pile \citep{gao2020pile} that Pythia models were trained on is no longer available. %

\paragraph{Model choice} Given the nature of our research question, it is crucial to be able to analyze multiple checkpoints from models of varying sizes, prioritizing interpretability over direct evaluations of model performance. For this reason, we rely on the Pythia family of models, publicly released in the interest of fostering interpretability research. We leave to future work the exploration of alignment and its emergence in alternative model families \citep[e.g., the recent OLMo 2 family;][]{olmo20252olmo2furious}.


    






\bibliography{main}

\newpage
\appendix
\section{Prompts used for probing dataset generation and sample generations}
\label{app:appendix_prompts}
\paragraph{Fact prompt:} ``Generate a set of $10$ sentences, including as many facts as possible, about the concept [concept name] as [a/an] [adjective/noun/verb] and defined as [WordNet definition]. Refer to the concept only as [concept name] without including specific classes, types, or names of [concept name]. Make sure the sentences are diverse and do not repeat.''

\paragraph{Sample fact sentences} for concept \textbf{poppy} defined as 'annual or biennial or perennial herbs having showy flowers':\\
\textbf{GPT-4}: Gardeners often classify poppies as easy to care for due to their hardy nature.\\
\textbf{Mistral-7b-Instruct-v0.2}: As the farmer tended to his fields, he couldn't help but admire the poppies that grew among his crops, their beauty a welcome distraction.\\
\textbf{Internal 80b-chat model}: Poppies have been used in traditional medicine for centuries, with various parts of the plant being employed to treat ailments like pain, insomnia, and digestive problems.


            
\paragraph{Story prompt:} ``Generate a set of $10$ sentences, where each sentence is a short story about the concept [concept name] as [a/an] [adjective/noun/verb] and defined as [WordNet definition]. Refer to the concept only as [concept name] without including specific classes, types, or names of [concept name]. Make sure the sentences are diverse and do not repeat.''


\paragraph{Sample story sentences} for concept \textbf{poppy} defined as 'annual or biennial or perennial herbs having showy flowers':\\
\textbf{GPT-4}: As the wedding gift from her grandmother, a dried poppy was framed and hung on her wall. \\
\textbf{Mistral-7b-Instruct-v0.2}: Poppies are herbaceous plants that can grow annually, biennially, or perennially, depending on the specific species.\\
\textbf{Internal 80b-chat model}: The poppy, a harbinger of spring, adorned the hillsides with a colorful tapestry, signaling the end of winter's slumber.


\clearpage


\section{Analyses of correlations between human similarity judgments and cosine similarity for the full network}
\label{app:cosine}


\begin{minipage}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{men_cors_both_cosine.pdf}
    \captionof{figure}{Spearman correlations between human similarity judgments, cosine similarity over raw AP values,  negative-adjusted cosine similarity [abs(AP)-0.5], and the best-performing $\tau$ of Jaccard similarity (0.5). Points represent Spearman correlations between LLM's similarity and perceived human similarity in the MEN dataset; error bars represent bootstrapped $95$\% confidence intervals. }
    \label{fig:full_cosine}
\end{minipage}


\clearpage

\section{List of words in hierarchically-organized domains}
\label{app:domains_list}

\begin{table}[h]
    \centering
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{p{2.5cm} p{3.5cm}}
        \textbf{Super-ordinate} & \textbf{Sub-ordinates} \\
        \hline
        animal & cat, dog, cheetah, horse \\
        clothes & jacket, jeans, shirt, sock \\
        colour & red, blue, green, black\\
        furniture & chair, bookshelf, table, couch \\
        occupation & doctor, teacher, driver, musician\\
        organ & heart, kidney, lung, brain\\
        sport & golf, racing, gymnastics, swimming \\
        subject & mathematics, geography, biology, chemistry \\
        vegetable & carrot, potato, pumpkin, corn \\
        vehicle & bus, tank, motorcycle, bicycle \\
    \end{tabular}
    \caption{List of words in our dataset of hierarchically-organized domains.}
\end{table}


\section{Hierarchical structures: results for additional models and checkpoints}
\label{app:hierarchies}



\begin{table}[h!]
\centering

\setlength{\tabcolsep}{3pt}
\begin{tabular}{l|c|c|c}
\toprule
Domain & 
\% overlap &
baseline 1 &
baseline 2 \\
\midrule
animal     & 0.00  &  \textcolor{gray}{ 0.00} &  \textcolor{gray}{ 0.00} \\
clothes    & 0.00  &   \textcolor{gray}{ 0.00} &  \textcolor{gray}{ 0.00}\\
colour     & 0.00  &   \textcolor{gray}{ 0.00} &  \textcolor{gray}{ 0.00}\\
furniture  &  0.00 &    \textcolor{gray}{  0.00}&  \textcolor{gray}{ 0.00}\\
occupation &  0.00 &    \textcolor{gray}{  0.00} &  \textcolor{gray}{ 0.00}\\
organ      &  0.00 &    \textcolor{gray}{ 0.00} &  \textcolor{gray}{ 0.00} \\
sport      &  0.00 &     \textcolor{gray}{  0.00} &  \textcolor{gray}{ 0.00} \\
subject    &  0.00 &    \textcolor{gray}{0.00} &  \textcolor{gray}{ 0.00}\\
vegetable  &  0.00 &    \textcolor{gray}{0.00} &  \textcolor{gray}{ 0.00} \\
vehicle    &  0.00 &  \textcolor{gray}{0.00} &  \textcolor{gray}{ 0.00}\\
\bottomrule
\end{tabular}
\caption{\textbf{Pythia 70m.} Results of our exploration of hierarchically-organized domains, at step $1$.}
\label{tab:hierarchies_70m_1}
\end{table}



\begin{table}[h!]
\centering

\setlength{\tabcolsep}{3pt}
\begin{tabular}{l|c|c|c}
\toprule
Domain & 
\% overlap &
baseline 1 &
baseline 2 \\
\midrule
clothes    &  0.00 & \textcolor{gray}{ 0.00} & \textcolor{gray}{0.00} \\
colour     &  0.00 & \textcolor{gray}{0.00} & \textcolor{gray}{0.00} \\
furniture  &  0.00 & \textcolor{gray}{  0.00} & \textcolor{gray}{0.00}\\
occupation & 0.00  & \textcolor{gray}{  0.00} & \textcolor{gray}{0.00 }\\
organ      & 66.67 & \textcolor{gray}{ 0.00} & \textcolor{gray}{3.81} \\
sport      &  0.00 & \textcolor{gray}{  0.00} & \textcolor{gray}{0.00} \\
subject    &  57.14 & \textcolor{gray}{ 0.00} & \textcolor{gray}{5.56} \\
vegetable  &  0.00 & \textcolor{gray}{  0.00} & \textcolor{gray}{0.00} \\
vehicle    &  0.00 & \textcolor{gray}{ 0.00} & \textcolor{gray}{0.00}\\
\bottomrule
\end{tabular}
\caption{\textbf{Pythia 70m.} Results of our exploration of hierarchically-organized domains, at step $512$.}
\label{tab:hierarchies_70m_512}
\end{table}



\begin{table}[h!]
\centering

\setlength{\tabcolsep}{3pt}
\begin{tabular}{l|c|c|c}
\toprule
Domain & 
\% overlap &
baseline 1 &
baseline 2 \\
\midrule
animal     &  0.00 & \textcolor{gray}{ 0.00} & \textcolor{gray}{1.00}\\
clothes    & 20.00 &\textcolor{gray}{0.00} & \textcolor{gray}{2.38} \\
colour     &  42.86 & \textcolor{gray}{0.00} & \textcolor{gray}{2.12} \\
furniture  &   100.00 & \textcolor{gray}{ 0.00} & \textcolor{gray}{0.22}\\
occupation &  0.00 & \textcolor{gray}{ 0.00} & \textcolor{gray}{1.11 }\\
organ      &   16.67 & \textcolor{gray}{1.11} & \textcolor{gray}{1.67} \\
sport      &  100.00& \textcolor{gray}{ 0.00}& \textcolor{gray}{ 3.22} \\
subject    &  81.25 & \textcolor{gray}{0.00} & \textcolor{gray}{0.00} \\
vegetable  & 66.67  &\textcolor{gray}{ 0.00} & \textcolor{gray}{0.00} \\
vehicle     & 75.00 & \textcolor{gray}{0.00} & \textcolor{gray}{0.83 }\\
\bottomrule
\end{tabular}
\caption{\textbf{Pythia 70m.} Results of our exploration of hierarchically-organized domains, at step $4000$.}
\label{tab:hierarchies_70m_4000}
\end{table}

\begin{table}[h!]
\centering

\setlength{\tabcolsep}{3pt}
\begin{tabular}{l|c|c|c}
\toprule
Domain & 
\% overlap &
baseline 1 &
baseline 2 \\
\midrule
animal     &  100.00 & \textcolor{gray}{ 0.00}  & \textcolor{gray}{0.73}\\
clothes    &  56.00 & \textcolor{gray}{0.00}  & \textcolor{gray}{0.00} \\
colour     &   52.00& \textcolor{gray}{0.00} & \textcolor{gray}{2.30} \\
furniture  &  75.00 & \textcolor{gray}{ 0.00}  & \textcolor{gray}{2.80}\\
occupation &  14.29 & \textcolor{gray}{ 0.00}  & \textcolor{gray}{5.34 }\\
organ      & 45.00  &\textcolor{gray}{0.00}  & \textcolor{gray}{0.92} \\
sport      &  85.71 &\textcolor{gray}{ 0.00}  & \textcolor{gray}{1.03} \\
subject    &  75.86 &\textcolor{gray}{0.00}  & \textcolor{gray}{4.93} \\
vegetable  & 68.00  & \textcolor{gray}{ 0.00}  & \textcolor{gray}{0.00} \\
vehicle    & 100.00 & \textcolor{gray}{0.00}  & \textcolor{gray}{3.57 }\\
\bottomrule
\end{tabular}
\caption{\textbf{Pythia 70m.} Results of our exploration of hierarchically-organized domains, at step $143$k.}
\label{tab:hierarchies_70m_143000}
\end{table}





\begin{table}[h!]
\centering

\setlength{\tabcolsep}{3pt}
\begin{tabular}{l|c|c|c}
\toprule
Domain & 
\% overlap &
baseline 1 &
baseline 2 \\
\midrule
animal     &  0.00 & \textcolor{gray}{ 0.00} & \textcolor{gray}{0.00}\\
clothes    &  0.00 & \textcolor{gray}{0.00} & \textcolor{gray}{0.00} \\
colour     &  2.38 &  \textcolor{gray}{3.33} & \textcolor{gray}{0.00} \\
furniture  &  0.00 & \textcolor{gray}{ 0.00} & \textcolor{gray}{0.00}\\
occupation &  0.00 &  \textcolor{gray}{ 0.00} & \textcolor{gray}{0.00 }\\
organ      &  0.00 &  \textcolor{gray}{0.00}& \textcolor{gray}{ 0.00} \\
sport      &  0.00 &  \textcolor{gray}{ 0.00}& \textcolor{gray}{0.00 } \\
subject    & 0.00  & \textcolor{gray}{0.00} & \textcolor{gray}{0.32} \\
vegetable  &  0.00 &  \textcolor{gray}{ 0.00} & \textcolor{gray}{0.00} \\
vehicle    &   0.00 & \textcolor{gray}{0.00} & \textcolor{gray}{0.24}\\
\bottomrule
\end{tabular}
\caption{\textbf{Pythia 1b.} Results of our exploration of hierarchically-organized domains, at step $1$.}
\label{tab:hierarchies_1b_1}
\end{table}



\begin{table}[h!]
\centering

\setlength{\tabcolsep}{3pt}
\begin{tabular}{l|c|c|c}
\toprule
Domain & 
\% overlap &
baseline 1 &
baseline 2 \\
\midrule
animal     &  0.00 & \textcolor{gray}{0.00} & \textcolor{gray}{0.26}\\
clothes    &   0.00& \textcolor{gray}{3.33} & \textcolor{gray}{0.00} \\
colour     & 2.56 & \textcolor{gray}{0.00} & \textcolor{gray}{1.07} \\
furniture  &  0.00 &\textcolor{gray}{ 1.67} & \textcolor{gray}{0.00}\\
occupation &  23.08 & \textcolor{gray}{ 0.00} & \textcolor{gray}{0.15 }\\
organ      &  32.94 & \textcolor{gray}{0.37} & \textcolor{gray}{3.65} \\
sport      &   0.00 &\textcolor{gray}{ 0.00} & \textcolor{gray}{0.20} \\
subject    &  37.40 & \textcolor{gray}{0.00}& \textcolor{gray}{ 1.98} \\
vegetable  &    0.00& \textcolor{gray}{ 0.00}& \textcolor{gray}{ 0.33} \\
vehicle    &  0.00 & \textcolor{gray}{0.00} & \textcolor{gray}{0.29 }\\
\bottomrule
\end{tabular}
\caption{\textbf{Pythia 1b.} Results of our exploration of hierarchically-organized domains, at step $512$.}
\label{tab:hierarchies_1b_512}
\end{table}



\begin{table}[h!]
\centering

\setlength{\tabcolsep}{3pt}
\begin{tabular}{l|c|c|c}
\toprule
Domain & 
\% overlap  &
baseline 1 &
baseline 2 \\
\midrule
animal     &  84.06 & \textcolor{gray}{ 0.00} & \textcolor{gray}{0.98}\\
clothes    &  45.78 & \textcolor{gray}{3.33} & \textcolor{gray}{1.08} \\
colour     & 48.48 & \textcolor{gray}{1.11} & \textcolor{gray}{1.26} \\
furniture  & 61.73 & \textcolor{gray}{ 0.00} & \textcolor{gray}{3.28}\\
occupation &  16.67 & \textcolor{gray}{ 0.67} & \textcolor{gray}{0.98 }\\
organ      & 45.15 & \textcolor{gray}{4.24} & \textcolor{gray}{2.12} \\
sport      &  81.40 & \textcolor{gray}{ 1.33} & \textcolor{gray}{2.02} \\
subject    &  58.26 & \textcolor{gray}{1.67} & \textcolor{gray}{1.68} \\
vegetable  &   76.71& \textcolor{gray}{ 0.00} & \textcolor{gray}{1.13} \\
vehicle    &82.80  & \textcolor{gray}{4.17} & \textcolor{gray}{1.87 }\\
\bottomrule
\end{tabular}
\caption{\textbf{Pythia 1b.} Results of our exploration of hierarchically-organized domains, at step $4000$.}
\label{tab:hierarchies_1b_4000}
\end{table}




\begin{table}[h!]
\centering

\setlength{\tabcolsep}{3pt}
\begin{tabular}{l|c|c|c}
\toprule
Domain & 
\% overlap &
baseline 1 &
baseline 2 \\
\midrule
animal     &  61.09 & \textcolor{gray}{0.00} & \textcolor{gray}{1.47}\\
clothes    &  56.67 & \textcolor{gray}{10.37} & \textcolor{gray}{1.04} \\
colour     & 64.71  &\textcolor{gray}{5.00} & \textcolor{gray}{1.66} \\
furniture  &   71.96 &\textcolor{gray}{ 0.00}& \textcolor{gray}{ 2.40}\\
occupation & 15.38 & \textcolor{gray}{ 1.41}& \textcolor{gray}{ 1.53}\\
organ      & 55.38 & \textcolor{gray}{0.61} & \textcolor{gray}{2.76} \\
sport      &  74.27& \textcolor{gray}{ 3.89} & \textcolor{gray}{2.41} \\
subject    &77.54  & \textcolor{gray}{4.44} & \textcolor{gray}{2.04} \\
vegetable  & 77.89 &\textcolor{gray}{ 0.56} & \textcolor{gray}{1.07} \\
vehicle    & 83.54 & \textcolor{gray}{0.48} & \textcolor{gray}{2.19 }\\
\bottomrule
\end{tabular}
\caption{\textbf{Pythia 1b.} Results of our exploration of hierarchically-organized domains, at step $143$k.}
\label{tab:hierarchies_1b_143000}
\end{table}




\begin{table}[h!]
\centering

\setlength{\tabcolsep}{3pt}
\begin{tabular}{l|c|c|c}
\toprule
Domain & 
\% overlap &
baseline 1 &
baseline 2 \\
\midrule
animal     & 0.00 & \textcolor{gray}{0.00} & \textcolor{gray}{0.00}\\
clothes    &   0.00& \textcolor{gray}{0.18} & \textcolor{gray}{0.20} \\
colour     &   1.20& \textcolor{gray}{0.00}  & \textcolor{gray}{0.00} \\
furniture  & 0.00 & \textcolor{gray}{ 0.00}  & \textcolor{gray}{0.00}\\
occupation & 0.00 & \textcolor{gray}{ 0.00}  & \textcolor{gray}{0.00 }\\
organ      &  0.00 &\textcolor{gray}{0.00}  & \textcolor{gray}{0.00} \\
sport      &  0.00 & \textcolor{gray}{ 3.33} & \textcolor{gray}{0.00} \\
subject    &  0.00 & \textcolor{gray}{0.00} & \textcolor{gray}{0.48} \\
vegetable  & 0.00 & \textcolor{gray}{1.67} & \textcolor{gray}{0.12} \\
vehicle    &  0.00 & \textcolor{gray}{0.00} & \textcolor{gray}{8.45 }\\
\bottomrule
\end{tabular}
\caption{\textbf{Pythia 12b.} Results of our exploration of hierarchically-organized domains, at step $1$.}
\label{tab:hierarchies_12b_1}
\end{table}





\begin{table}[h!]
\centering

\setlength{\tabcolsep}{3pt}
\begin{tabular}{l|c|c|c}
\toprule
Domain & 
\% overlap &
baseline 1&
baseline 2 \\
\midrule
animal     &  0.00 &\textcolor{gray}{0.00} &\textcolor{gray}{0.08}\\
clothes    &    0.00&\textcolor{gray}{0.00} &\textcolor{gray}{0.00} \\
colour     &  2.38 & \textcolor{gray}{6.67} &\textcolor{gray}{7.78} \\
furniture  &  0.00 & \textcolor{gray}{ 0.00} &\textcolor{gray}{1.46}\\
occupation &  0.00 & \textcolor{gray}{ 0.00}&\textcolor{gray}{ 3.70 }\\
organ      & 14.28 &\textcolor{gray}{0.00} &\textcolor{gray}{0.00} \\
sport      &  0.00 &\textcolor{gray}{ 0.00} &\textcolor{gray}{0.00} \\
subject    &   33.33  &\textcolor{gray}{0.00} &\textcolor{gray}{0.63} \\
vegetable  & 0.00 & \textcolor{gray}{0.30} &\textcolor{gray}{0.08} \\
vehicle    & 0  0.00 & \textcolor{gray}{0.00} &\textcolor{gray}{0.00}\\
\bottomrule
\end{tabular}
\caption{\textbf{Pythia 12b.} Results of our exploration of hierarchically-organized domains, at step $512$.}
\label{tab:hierarchies_12b_512}
\end{table}



\begin{table}[h!]
\centering

\setlength{\tabcolsep}{3pt}
\begin{tabular}{l|c|c|c}
\toprule
Domain & 
\% overlap &
baseline 1 &
baseline 2 \\
\midrule
animal     &  62.81 & \textcolor{gray}{0.30} & \textcolor{gray}{1.05}\\
clothes    &   46.76 & \textcolor{gray}{13.94} & \textcolor{gray}{1.42} \\
colour     &  56.74 &\textcolor{gray}{2.22} & \textcolor{gray}{1.04} \\
furniture  &   69.07 &\textcolor{gray}{ 3.33} & \textcolor{gray}{3.7}\\
occupation &  16.30& \textcolor{gray}{1.50} & \textcolor{gray}{2.16 }\\
organ      & 44.88 & \textcolor{gray}{4.24}& \textcolor{gray}{ 2.75} \\
sport      &   75.42  &\textcolor{gray}{ 1.45} & \textcolor{gray}{2.19} \\
subject    & 68.39 & \textcolor{gray}{9.44} & \textcolor{gray}{1.84} \\
vegetable  &  80.40 & \textcolor{gray}{ 1.00} & \textcolor{gray}{1.18} \\
vehicle    & 79.76 & \textcolor{gray}{2.16} & \textcolor{gray}{2.10}\\
\bottomrule
\end{tabular}
\caption{\textbf{Pythia 12b.} Results of our exploration of hierarchically-organized domains, at step $4000$.}
\label{tab:hierarchies_12b_4000}
\end{table}






\clearpage
\section{Hierarchically-organized domains: additional plots}
\label{app:hierarchies2}


\begin{minipage}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{HEATMAP_ALL_experts_step1_70m_t0.5.pdf}
    \captionof{figure}{\textbf{Pythia 70m.} Proportions of expert overlap between word pairs belonging to hierarchically organized domains, at checkpoint $1$. Domains can be identified based on the stronger associations among their words compared to unrelated terms.}
    \label{fig:hierarchies_all_words_70m_1}
\end{minipage}

\clearpage
\begin{minipage}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{HEATMAP_ALL_experts_step512_70m_t0.5.pdf}
    \captionof{figure}{\textbf{Pythia 70m.} Proportions of expert overlap between word pairs belonging to hierarchically organized domains, at checkpoint $512$. Domains can be identified based on the stronger associations among their words compared to unrelated terms.}
    \label{fig:hierarchies_all_words_70m_512}
\end{minipage}

\clearpage
\begin{minipage}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{HEATMAP_ALL_experts_step4000_70m_t0.5.pdf}
    \captionof{figure}{\textbf{Pythia 70m.} Proportions of expert overlap between word pairs belonging to hierarchically organized domains, at checkpoint $4000$. Domains can be identified based on the stronger associations among their words compared to unrelated terms.}
    \label{fig:hierarchies_all_words_70m_4000}
\end{minipage}

\clearpage
\begin{minipage}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{HEATMAP_ALL_experts_step143000_70m_t0.5.pdf}
    \captionof{figure}{\textbf{Pythia 70m.} Proportions of expert overlap between word pairs belonging to hierarchically organized domains, at checkpoint $143000$. Domains can be identified based on the stronger associations among their words compared to unrelated terms.}
    \label{fig:hierarchies_all_words_70m_143000}
\end{minipage}

\clearpage




\begin{minipage}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{HEATMAP_ALL_experts_step1_1b_t0.5.pdf}
    \captionof{figure}{\textbf{Pythia 1b.} Proportions of expert overlap between word pairs belonging to hierarchically organized domains, at checkpoint $1$. Domains can be identified based on the stronger associations among their words compared to unrelated terms.}
    \label{fig:hierarchies_all_words_1b_1}
\end{minipage}

\clearpage
\begin{minipage}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{HEATMAP_ALL_experts_step512_1b_t0.5.pdf}
    \captionof{figure}{\textbf{Pythia 1b.} Proportions of expert overlap between word pairs belonging to hierarchically organized domains, at checkpoint $512$. Domains can be identified based on the stronger associations among their words compared to unrelated terms.}
    \label{fig:hierarchies_all_words_1b_512}
\end{minipage}

\clearpage
\begin{minipage}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{HEATMAP_ALL_experts_step4000_1b_t0.5.pdf}
    \captionof{figure}{\textbf{Pythia 1b.} Proportions of expert overlap between word pairs belonging to hierarchically organized domains, at checkpoint $4000$. Domains can be identified based on the stronger associations among their words compared to unrelated terms.}
    \label{fig:hierarchies_all_words_1b_4000}
\end{minipage}

\clearpage
\begin{minipage}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{HEATMAP_ALL_experts_step143000_1b_t0.5.pdf}
    \captionof{figure}{\textbf{Pythia 1b.} Proportions of expert overlap between word pairs belonging to hierarchically organized domains, at checkpoint $143000$. Domains can be identified based on the stronger associations among their words compared to unrelated terms.}
    \label{fig:hierarchies_all_words_1b_143000}
\end{minipage}

\clearpage



\begin{minipage}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{HEATMAP_ALL_experts_step1_12b_t0.5.pdf}
    \captionof{figure}{\textbf{Pythia 12b.} Proportions of expert overlap between word pairs belonging to hierarchically organized domains, at checkpoint $1$. Domains can be identified based on the stronger associations among their words compared to unrelated terms.}
    \label{fig:hierarchies_all_words_12b_1}
\end{minipage}

\clearpage
\begin{minipage}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{HEATMAP_ALL_experts_step512_12b_t0.5.pdf}
    \captionof{figure}{\textbf{Pythia 12b.} Proportions of expert overlap between word pairs belonging to hierarchically organized domains, at checkpoint $512$. Domains can be identified based on the stronger associations among their words compared to unrelated terms.}
    \label{fig:hierarchies_all_words_12b_512}
\end{minipage}

\clearpage
\begin{minipage}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{HEATMAP_ALL_experts_step4000_12b_t0.5.pdf}
    \captionof{figure}{\textbf{Pythia 12b.} Proportions of expert overlap between word pairs belonging to hierarchically organized domains, at checkpoint $4000$. Domains can be identified based on the stronger associations among their words compared to unrelated terms.}
    \label{fig:hierarchies_all_words_12b_4000}
\end{minipage}

\clearpage
\begin{minipage}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{HEATMAP_ALL_experts_step143000_12b_t0.5.pdf}
    \captionof{figure}{\textbf{Pythia 12b.} Proportions of expert overlap between word pairs belonging to hierarchically organized domains, at checkpoint $143000$. Domains can be identified based on the stronger associations among their words compared to unrelated terms.}
    \label{fig:hierarchies_all_words_12b_143000}
\end{minipage}




\clearpage
\section{Additional materials for layer analyses}
\label{app:layers}


\subsection{Total number of experts in MLP and attention layers}
\label{app:add_plots_totals_layers}
\vspace{1cm}

\begin{minipage}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{total_experts_layers_70m.png}
    \captionof{figure}{\textbf{Pythia 70m.} Total number of experts in MLP and attention layers across checkpoints}
    \label{fig:layers_MLP_70m_total}

     \vspace{1cm}
     
    \centering
    \includegraphics[width=\textwidth]{total_experts_layers_1b.png}
    \captionof{figure}{\textbf{Pythia 1b.} Total number of experts in MLP and attention layers across checkpoints}
    \label{fig:layers_MLP_1b_total}
\end{minipage}







\clearpage
\subsection{Distribution of experts across MLP layers}
\label{app:add_layers_plots_MLPs}
\vspace{1cm}

\begin{minipage}{\textwidth} 
    \centering
    \includegraphics[width=\textwidth]{layers_stats_70m_united_MLPs.png}
    \captionof{figure}{\textbf{Pythia 70m.} Average number of experts identified in MLP layers at different depths, for different checkpoints. %
    }
    \label{fig:layers_MLP_70m}

     \vspace{1cm}

    \centering
    \includegraphics[width=\textwidth]{layers_stats_1b_united_MLPs.png}
    \captionof{figure}{\textbf{Pythia 1b.} Average number of experts identified in MLP layers at different depths, for different checkpoints. %
    }
    \label{fig:layers_MLP_1b}
\end{minipage}





\clearpage
\subsection{Distribution of experts across attention layers}
\label{app:add_layer_plots_attention}
\vspace{1cm}

\begin{minipage}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{layers_stats_70m_united_atts.png}
    \captionof{figure}{\textbf{Pythia 70m.} Average number of experts identified in attention layers at different depths, for different checkpoints. %
    }
    \label{fig:layers_att_70m}

    \vspace{1cm}


    \centering
    \includegraphics[width=\textwidth]{layers_stats_1b_united_atts.png}
    \captionof{figure}{\textbf{Pythia 1b.} Average number of experts identified in attention layers at different depths, for different checkpoints. %
    }
    \label{fig:layers_att_1b}

     \vspace{1cm}

    \centering
    \includegraphics[width=\textwidth]{layers_stats_12b_united_atts.png}
    \captionof{figure}{\textbf{Pythia 12b.} Average number of experts identified in attention layers at different depths, for different checkpoints. %
    }
    \label{fig:layers_att_12b}
\end{minipage}




\clearpage
\subsection{Distribution of experts across \texttt{MLP.dense.h\_to\_4h} layers}
\label{app:add_layer_plots_h_to_4h}
\vspace{1cm}

\begin{minipage}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{layers_stats_70m_h_to_4h.png}
    \captionof{figure}{\textbf{Pythia 70m.} Average number of experts identified in the MLP {\texttt{h\_to\_4h}} (part of the MLP layers) at different depths, for different checkpoints. } 
    \label{fig:layers_h_to_4h_70m}

    \vspace{1cm}


    \centering
    \includegraphics[width=\textwidth]{layers_stats_1b_h_to_4h.png}
    \captionof{figure}{\textbf{Pythia 1b.} Average number of experts identified in the MLP {\texttt{h\_to\_4h}} (part of the MLP layers) at different depths, for different checkpoints.}
    \label{fig:layers_h_to_4h_1b}

     \vspace{1cm}
     

    \centering
    \includegraphics[width=\textwidth]{layers_stats_12b_h_to_4h.png}
    \captionof{figure}{\textbf{Pythia 12b.} Average number of experts identified in the MLP {\texttt{h\_to\_4h}} (part of the MLP layers) at different depths, for different checkpoints.}
    \label{fig:layers_h_to_4h_12b}
\end{minipage}


\clearpage
\subsection{Distribution of experts across \texttt{MLP.dense.4h\_to\_h} layers}
\label{app:add_layer_plots_4h_to_h}
\vspace{1cm}

\begin{minipage}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{layers_stats_70m_4h_to_h.png}
    \captionof{figure}{\textbf{Pythia 70m.} Average number of experts identified in the MLP {\texttt{4h\_to\_h}} (part of the MLP layers) at different depths, for different checkpoints.}
    \label{fig:layers_4h_to_h_70m}

    \vspace{1cm}


    \centering
    \includegraphics[width=\textwidth]{layers_stats_1b_4h_to_h.png}
    \captionof{figure}{\textbf{Pythia 1b.} Average number of experts identified in thr MLP {\texttt{4h\_to\_h}} (part of the MLP layers) at different depths, for different checkpoints.}
    \label{fig:layers_4h_to_h_1b}

     \vspace{1cm}
     

    \centering
    \includegraphics[width=\textwidth]{layers_stats_12b_4h_to_h.png}
    \captionof{figure}{\textbf{Pythia 12b.} Average number of experts identified in the MLP {\texttt{4h\_to\_h}} (part of the MLP layers) at different depths, for different checkpoints.}
    \label{fig:layers_4h_to_h_12b}
\end{minipage}


\clearpage
\subsection{Distribution of experts across \texttt{attention.query\_key\_value} layers}
\label{app:add_layer_plots_attention_query}
\vspace{1cm}

\begin{minipage}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{layers_stats_70m_attention.query.png}
    \captionof{figure}{\textbf{Pythia 70m.} Average number of experts identified in the {\texttt{attention.query\_key\_value}} (part of the attention layers) at different depths, for different checkpoints.}
    \label{fig:layers_att_query_70m}

    \vspace{1cm}


    \centering
    \includegraphics[width=\textwidth]{layers_stats_1b_attention.query.png}
    \captionof{figure}{\textbf{Pythia 1b.} Average number of experts identified in the {\texttt{attention.query\_key\_value}} (part of the attention layers) at different depths, for different checkpoints.}
    \label{fig:layers_att_query_1b}

     \vspace{1cm}
     

    \centering
    \includegraphics[width=\textwidth]{layers_stats_12b_attention.query.png}
    \captionof{figure}{\textbf{Pythia 12b.} Average number of experts identified in the {\texttt{attention.query\_key\_value}} (part of the attention layers) at different depths, for different checkpoints.}
    \label{fig:layers_att_query_12b}
\end{minipage}



\clearpage
\subsection{Distribution of experts across \texttt{attention.dense} layers}
\label{app:add_layer_plots_attention_dense}
\vspace{1cm}

\begin{minipage}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{layers_stats_70m_attention.dense.png}
    \captionof{figure}{\textbf{Pythia 70m.} Average number of experts identified in the {\texttt{attention.dense}} (part of the attention layers) at different depths, for different checkpoints.}
    \label{fig:layers_att_dense_70m}

    \vspace{1cm}


    \centering
    \includegraphics[width=\textwidth]{layers_stats_1b_attention.dense.png}
    \captionof{figure}{\textbf{Pythia 1b.} Average number of experts identified in the {\texttt{attention.dense}} (part of the attention layers) at different depths, for different checkpoints.}
    \label{fig:layers_att_dense_1b}

     \vspace{1cm}
     

    \centering
    \includegraphics[width=\textwidth]{layers_stats_12b_attention.dense.png}
    \captionof{figure}{\textbf{Pythia 12b.} Average number of experts identified in the {\texttt{attention.dense}} (part of the attention layers) at different depths, for different checkpoints.}
    \label{fig:layers_att_dense_12b}
\end{minipage}



\clearpage
\subsection{Distribution of highly specialized experts across MLP layers}
\label{app:add_layer_plots_MLP_Suau}
\vspace{1cm}

\begin{minipage}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{layers_stats_70m_united_MLPs_suau.png}
    \captionof{figure}{\textbf{Pythia 70m.} Average number of highly specialized experts ($\tau = 0.9$) identified in MLP layers at different depths, for different checkpoints. 
    }

    \vspace{1cm}


    \centering
    \includegraphics[width=\textwidth]{layers_stats_1b_united_MLPs_suau.png}
    \captionof{figure}{\textbf{Pythia 1b.} Average number of highly specialized experts ($\tau = 0.9$) identified in MLP layers at different depths, for different checkpoints.
    }

     \vspace{1cm}

    \centering
    \includegraphics[width=\textwidth]{layers_stats_12b_united_MLPs_suau.png}
    \captionof{figure}{\textbf{Pythia 12b.} Average number of highly specialized experts ($\tau = 0.9$) identified in MLP layers at different depths, for different checkpoints.
    }
   
\end{minipage}




\clearpage
\subsection{Distribution of highly specialized experts across attention layers}
\label{app:add_layer_plots_atts_Suau}
\vspace{1cm}

\begin{minipage}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{layers_stats_70m_united_atts_suau.png}
    \captionof{figure}{\textbf{Pythia 70m.} Average number of highly specialized experts ($\tau = 0.9$) identified in attention layers at different depths, for different checkpoints. 
    }

    \vspace{1cm}

    \centering
    \includegraphics[width=\textwidth]{layers_stats_1b_united_atts_suau.png}
    \captionof{figure}{\textbf{Pythia 1b.} Average number of highly specialized experts ($\tau = 0.9$) identified in attention layers at different depths, for different checkpoints.
    }

     \vspace{1cm}

    \centering
    \includegraphics[width=\textwidth]{layers_stats_12b_united_atts_suau.png}
    \captionof{figure}{\textbf{Pythia 12b.} Average number of highly specialized experts ($\tau = 0.9$) identified in attention layers at different depths, for different checkpoints.
    }
   
\end{minipage}





\clearpage
\section{Computational budget}
The concept dataset was parallelized over 8 A100 GPUs (80GB).
Expert extraction took about $136$ seconds per concept for the $12$b Pythia model; about $27$ seconds per concept for the $1$b Pythia model; about $8$ seconds per concept for the $70$m Pythia model; and about $25$ seconds per concept for GPT-2.

\section{License and Attribution}
The MEN dataset used in this work is released under Creative Commons Attribute license. The pre-trained models are supported by public licenses the Pythia Scaling Suite (Apache), Mistral (Apache), and GPT-2 (MIT). GPT-4 is supported a proprietary license. We use an internal 80b-chat model and are unable to provide license information on it at this time.




\end{document}

\typeout{get arXiv to do 4 passes: Label(s) may have changed. Rerun}
