\section{Related work}
\paragraph{Representation alignment}

Studies on the kinds of representations used by humans and machines have been of interest to many fields \citep[e.g., cognitive science, neuroscience, and machine learning;][]{hebart2020revealing, Khosla2022, muttenthaler2023human, tian2022contrastive}. Studies on \textit{representation alignment} \cite{sucholutsky2024gettin} look specifically at the extent to which the internal representations of humans and neural networks converge on a similar structure. 
Across vision and text domains, models show notable alignment with human similarity judgments ---typically used as a window into human representational structures. \citet{peterson2018} report significant alignment between human similarity judgments and representations of object classification networks, while \citet{digutsch2023overlap} report similar alignment with GPT-3’s \citep{gpt3} embeddings. However, \citet{shaki2023cognitive} finds that GPT-3's concept alignment is highly sensitive to prompt phrasing and
\citet{misra-etal-2020-exploring} show that alignment in BERT \citep{bert} is very context-dependent.  Investigating general factors that can cause mis-alignment, %
\citet{muttenthaler2023human} conclude that the training dataset and objective function impact alignment, but model scale and architecture have no significant effect. Of note, alignment and performance are not inherently tied: mis-aligned models can exhibit significant capabilities \cite{sucholutsky2023alignment, dessì2023communication}. %


 



\paragraph{Activation steering} refers to a class of methods that intervene on a generative model's activations to perform targeted updates for controllable generation \cite{rodriguez2024controlling, iti, rimsky_steering}. \citet{suau2023self} propose a method to identify sets of neurons in pre-trained transformer models that are responsible for detecting inputs in a specific style \citep[e.g., toxic language]{suau2024whispering} or about a specific concept \citep[e.g., ``dog'']{suau2023self}. Intervening on the expert neuron activations, %
successfully guides text generation into the desired direction. In a similar spirit, \citet{turner2024_steering} use a contrastive prompt (one positive and one negative) to induce sentiment shift and detoxification, while
\citet{kojima2024multilingual} steer multilingual models to produce more target language tokens in open-ended generation. Finally, \citet{rodriguez2024controlling} introduce a unified approach to steer activations in LLMs and diffusion models based on optimal transport theory.