\section{Related Work}
\para{RAG background} 
The concept of retrieval-augmented generation (RAG) was first introduced in \cite{ragInKnowledgeIntensiveNLP}. Subsequent studies have adapted it for a number of different applications, e.g., for improving the performance of knowledge-intensive language tasks (KILT) \cite{petroni-etal-2021-kilt} which include question answering \cite{kwiatkowski-etal-2019-natural,talmor-etal-2019-commonsenseqa}, hallucination and factual incorrectness mitigation \cite{FLARE}, summarisation \cite{edge2024localglobalgraphrag}, software code generation \cite{li2023acecoderutilizingexistingcode}. For a more comprehensive review on RAG literature, see \cite{ragsurvey,ragReview}.

Optimal context selection plays an important role in determining RAG's performance. While the most common approach to select this context is based on lexical or semantic similarities with the input text, e.g., question, instruction, query etc. \cite{liu-etal-2022-makes,demoRetrieveInICL}, other selection methodologies with different objectives have also been explored, e.g., \cm{for example selecting diverse documents as retrieved context}~\cite{levy-etal-2023-diverse},
or learning a task-specific ranking model to maximise the likelihood of the \fzA{retrieved} 
context improving the downstream performance \cite{rubin-etal-2022-learning}.
It has also been empirically verified that RAG performance depends not only on the set of documents selected as the context, but also on the order in which these are appended to the input text of an LLM \cite{DBLP:conf/acl/KumarT21,lu-etal-2022-fantastically}.

Existing research, however, has not explicitly investigated the \textit{effect of topical relevance of the RAG context} on the \textit{quality of the generated output} (in our case, an answer), which is the core objective of this paper.

\para{Analysing the influence of documents on downstream RAG performance}

Sauchuk et al. \cite{roleOfRelevanceInNLP} \cm{reported} that topically non-relevant documents lead to substantial degradation of downstream performance of cascaded IR-NLP systems (precursor to an end-to-end RAG system but characteristically similar to it). {More recently,} \cite{powerOfNoise,lostInTheMiddle} also showed that not only does the presence of relevant documents lead to improved performance in the downstream tasks compared with 0-shot generation, but the order of those relevant documents also affects the performance.
In particular, the authors of \cite{lostInTheMiddle} reported that
``\textit{model performance is highest when relevant information occurs at the beginning or end of its input context}'' and coined the term "lost in the middle". Their observation was further corroborated by the work of \cite{powerOfNoise}, with some further analysis on the effects of non-relevant vs.\ randomly sampled off-topic documents.

\looseness -1 In our work, we explore if the observations reported in \cite{powerOfNoise,lostInTheMiddle} for a notion of relevance restricted to \textit{answer containment} are consistent across topical relevance in the form of manually assessed ground truths. Similar to \cite{zamani-rag}, we also report the correlation between retrieval effectiveness and downstream RAG performance.

\para{Evaluation Methods of RAG Performance}

A RAG model's performance measure depends on the specific downstream tasks \cite{surveyLLMEvaluation}, e.g., in QA or summarization tasks, the common evaluation metrics measure the overlap between the ground-truth and a generated output either at a lexical level or at a semantic level. Lexical measures include
METEOR~\cite{metero}, ROUGE~\cite{rouge}, BLEU~\cite{bleuScore}, whereas semantic ones include BERTScore~\cite{finegrainedAnalysisOfBERTScore,bertScore}. Other overlap measures have also been proposed, e.g., FActScore \cite{factScore_EMNLP23} employs aspect-based matching to measure the factual correctness of generated answers, whereas the studies reported in \cite{huang2024empiricalstudyllmasajudgellm,zhuo2024icescoreinstructinglargelanguage} used LLMs to evaluate the quality of generated output.

In contrast to evaluating factual correctness, the authors of \cite{benchmarkOfAnswerEvaluationFromRetriever} proposed a \textit{relevance-based} evaluation strategy for measuring the quality of generated answers by conducting their experiments on an IR dataset where relevance ground-truths are available. Different from evaluation methodologies that compute the overlap of a generated output with a ground-truth one, the authors of \cite{benchmarkOfAnswerEvaluationFromRetriever} used the centroid of the relevant documents for a query to construct a pseudo-ground truth output - an approach that we also adapt in our work.

\looseness -1 \para{Research Gap} Overall, the closest prior work concerned with the relation between RAG context and RAG performance are \cite{powerOfNoise,lostInTheMiddle}, but neither directly investigated the problem with a conventional IR definition of topical relevance, as is addressed in our work. Moreover, those studies focused on overall RAG performance; in contrast, our work distinguishes the LLM's inherent performance in the downstream RAG task and the relative change brought by the retrieval context, which we define as utility. To investigate this problem systematically, we extended the framework that leverages IR benchmarks to evaluate RAG~\cite{benchmarkOfAnswerEvaluationFromRetriever}. We introduce our RAG Utility evaluation framework next.