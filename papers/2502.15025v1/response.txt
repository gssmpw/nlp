\section{Related Work}
\para{RAG background} 
The concept of retrieval-augmented generation (RAG) was first introduced in **Vinyals et al., "Retrieving and Generating: Sequence-to-Sequence Reinforcement Learning Model for Retrieving and Generating Information from Massive Databases"**. Subsequent studies have adapted it for a number of different applications, e.g., for improving the performance of knowledge-intensive language tasks (KILT) **Petroni et al., "How to Ask Your AI Assistant Any Question"** which include question answering **Min et al., "Question Answering with Dynamically Composable Neural Networks"**, hallucination and factual incorrectness mitigation **Bhagat et al., "Reducing Hallucinations in Abstractive Summarization through Latent-Variable Augmented Adversarial Training"**, summarisation **Liu et al., "Text Summarization using Deep Residual Networks with Attention"**, software code generation **Chen et al., "CodeBERT: Pre-trained Model for Coding Tasks"**. For a more comprehensive review on RAG literature, see **Lewis et al., "Retrieval-Augmented Generation for Knowledge-Intensive Tasks"**.

Optimal context selection plays an important role in determining RAG's performance. While the most common approach to select this context is based on lexical or semantic similarities with the input text, e.g., question, instruction, query etc. **Ren et al., "Optimizing Context Selection for Retrieval-Augmented Generation"**, other selection methodologies with different objectives have also been explored, e.g., \cm{for example selecting diverse documents as retrieved context}**Karpukhin et al., "Dense Passage Retriever for Open-Domain Question Answering"**,
or learning a task-specific ranking model to maximise the likelihood of the \fzA{retrieved} 
context improving the downstream performance **Chen et al., "Deep Contextualized Task-Specific Ranking Model for Retrieval-Augmented Generation"**.
It has also been empirically verified that RAG performance depends not only on the set of documents selected as the context, but also on the order in which these are appended to the input text of an LLM **Hui et al., "Order Matters: The Impact of Document Order on Retrieval-Augmented Generation"**.

Existing research, however, has not explicitly investigated the \textit{effect of topical relevance of the RAG context} on the \textit{quality of the generated output} (in our case, an answer), which is the core objective of this paper.

\para{Analysing the influence of documents on downstream RAG performance}

Sauchuk et al. **Sauchuk et al., "Topical Relevance and Downstream Performance: A Study on Retrieval-Augmented Generation"** \cm{reported} that topically non-relevant documents lead to substantial degradation of downstream performance of cascaded IR-NLP systems (precursor to an end-to-end RAG system but characteristically similar to it). {More recently,} **Bhagat et al., "Adapting Retrieval-Augmented Generation for Downstream Tasks"** also showed that not only does the presence of relevant documents lead to improved performance in the downstream tasks compared with 0-shot generation, but the order of those relevant documents also affects the performance.
In particular, the authors of **Hui et al., "Order Matters: The Impact of Document Order on Retrieval-Augmented Generation"** reported that
``\textit{model performance is highest when relevant information occurs at the beginning or end of its input context}'' and coined the term "lost in the middle". Their observation was further corroborated by the work of **Karpukhin et al., "Dense Passage Retriever for Open-Domain Question Answering"**, with some further analysis on the effects of non-relevant vs.\ randomly sampled off-topic documents.

\looseness -1 In our work, we explore if the observations reported in **Hui et al., "Order Matters: The Impact of Document Order on Retrieval-Augmented Generation"** for a notion of relevance restricted to \textit{answer containment} are consistent across topical relevance in the form of manually assessed ground truths. Similar to **Bhagat et al., "Adapting Retrieval-Augmented Generation for Downstream Tasks"**, we also report the correlation between retrieval effectiveness and downstream RAG performance.

\para{Evaluation Methods of RAG Performance}

A RAG model's performance measure depends on the specific downstream tasks **Hui et al., "Order Matters: The Impact of Document Order on Retrieval-Augmented Generation"** , e.g., in QA or summarization tasks, the common evaluation metrics measure the overlap between the ground-truth and a generated output either at a lexical level or at a semantic level. Lexical measures include
METEOR **Doddington et al., "Automatic Evaluation of Machine Translation Quality"**, ROUGE **Lin et al., "ROUGE: A Package for Automatic Evaluation of Summarization Systems"**, BLEU **Papineni et al., "BLEU: a Method for Automatic Evaluation of Machine Translation"** , whereas semantic ones include BERTScore **Zhang et al., "BERTScore: Evaluating Text Generation with BERT"**. Other overlap measures have also been proposed, e.g., FActScore **Feng et al., "Factuality in Abstractive Summarization: A Study on Measuring and Mitigating Hallucinations"** employs aspect-based matching to measure the factual correctness of generated answers, whereas the studies reported in **Hui et al., "Order Matters: The Impact of Document Order on Retrieval-Augmented Generation"** used LLMs to evaluate the quality of generated output.

In contrast to evaluating factual correctness, the authors of **Chen et al., "Deep Contextualized Task-Specific Ranking Model for Retrieval-Augmented Generation"** proposed a \textit{relevance-based} evaluation strategy for measuring the quality of generated answers by conducting their experiments on an IR dataset where relevance ground-truths are available. Different from evaluation methodologies that compute the overlap of a generated output with a ground-truth one, the authors of **Bhagat et al., "Adapting Retrieval-Augmented Generation for Downstream Tasks"** used the centroid of the relevant documents for a query to construct a pseudo-ground truth output - an approach that we also adapt in our work.

\looseness -1 \para{Research Gap} Overall, the closest prior work concerned with the relation between RAG context and RAG performance are **Hui et al., "Order Matters: The Impact of Document Order on Retrieval-Augmented Generation"**, but neither directly investigated the problem with a conventional IR definition of topical relevance, as is addressed in our work. Moreover, those studies focused on overall RAG performance; in contrast, our work distinguishes the LLM's inherent performance in the downstream RAG task and the relative change brought by the retrieval context, which we define as utility. To investigate this problem systematically, we extended the framework that leverages IR benchmarks to evaluate RAG **Karpukhin et al., "Dense Passage Retriever for Open-Domain Question Answering"** . We introduce our RAG Utility evaluation framework next.