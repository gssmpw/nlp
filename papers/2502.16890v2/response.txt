\section{Related work}
\textbf{Advancement in Recent Deep Learning-based Time Series Forecasting \quad} Recent advancements in deep learning-based time series forecasting can be broadly categorized into three key areas: (1) the application of sequential models to time series data, (2) the tokenization of time series, and (3) the exploration of intrinsic patterns within time series. 
%
Efforts in the first area have focused on deploying various architectures for time series forecasting, including Transformer **Vaswani et al., "Attention Is All You Need"**, Mamba **Zhang et al., "Mamba: A Deep Learning Framework for Time Series Forecasting"**, MLPs **Ji et al., "MLPs for Time Series Forecasting"**, RNNs **Graves et al., "Supervised Sequence Labelling with Recurrent Neural Networks"**, Graph Neural Networks **Kipf and Welling, "Semi-Supervised Classification with Graph Convolutional Networks"**, TCNs **Bai et al., "Empirical Evaluation of Generic Conv1d on Time Series Forecasting"**, and even Large Language Models (LLMs) **Brown et al., "Language Models are Few-Shot Learners"**.
%
The second direction has witnessed groundbreaking developments, particularly in Patch Embedding **Dosovitskiy et al., "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"** and Variate Embedding **Huang et al., "Temporal Graph Attention Network"**.
%
The final area explores modeling complex relationships, including the inter-series dependencies **Wang et al., "Graph Convolutional Networks for Time Series Forecasting"**, the dynamic evolution within a sequence **Kim et al., "Dynamic Evolutionary Process (DEP) Model"**, or both **Zhang et al., "Joint Modeling of Temporal and Spatial Dependencies in Time Series Data"**. 

\textbf{Time Series Modeling with Frequency \quad} Frequency as a key feature of time series data, has inspired numerous works **Bengio et al., "Convolutional Sequence to Sequence Learning"**.
%
FITS **Wang et al., "Frequency-Informed Temporal Shift Networks"** employs a simple frequency-domain linear, getting results comparable to SOTA models with 10K parameters.
%
Autoformer **Guo et al., "Autoformer: A Novel Attention-Based Method for Time Series Forecasting"** introduces the auto-correlation mechanism, leveraging FFT to improve self-attention. FEDformer **Wang et al., "FEDformer: Frequency Enhanced Transformer for Time Series Forecasting"** further calculates attention weights from the spectrum of queries and keys. FiLM **Dwivedi et al., "FiLM: Few-Shot Image Recognition by Progressive Fusion"** applies Fourier analysis to preserve historical information while filtering out noise. FreTS **Bai et al., "Frequency-ResNet for Time Series Forecasting"** incorporates frequency-domain MLP to model both channel and temporal dependencies. TimesNet **Kim et al., "TimesNet: A Novel Network Architecture for Time Series Forecasting"** utilizes FFT to extract periodic patterns. FilterNet **Dong et al., "FilterNet: A Deep Learning Model for Time Series Filtering"** proposes a filter-based method from the perspective of signal processing. 

However, they do not address the Mid-Frequency Spectrum Gap and shared Key-Frequency modeling. In contrast, our method employs `Adaptive Mid-Frequency Energy Optimizer' to improve mid-frequency feature extraction and introduces `Energy-based Key-Frequency Picking Block' with `Key-Frequency Enhanced Training' strategy to capture shared Key-Frequency across channels.