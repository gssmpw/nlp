\section{Methodology}

\subsection{Problem Definition}

Given a multivariate time series input $X \in \mathbb{R}^{C  \times T}$, multivariate time series forecasting tasks are designed to predict its future $F$ time steps $\hat{Y}\in \mathbb{R}^{C \times F}$ using past $T$ steps. $C $ is the number of variates or channels.

\subsection{Preliminary Analysis}

This section presents why RevIN~\citep{Kim_revin,liu2022non}, High-pass, and Low-pass filters fail to address the Mid-Frequency Spectrum Gap. Let the input univariate time series be $ x(t) $ with length $ T $ and target $ y(t) $ with length $ F $. 

\begin{definition}[Frequency Spectral Energy]\label{def:energy}
The Fourier transform of $x(t)$, $X(f)$, and its spectral energy $E_X(f)$ is given by:
\vspace{-0.2cm}
\begin{align}
X(f) = \sum_{t=0}^{T-1} x(t) e^{-i 2 \pi f t / {T-1}}, \quad &f = 0, 1, \dots, T-1\notag\\
E_X(f) = |X(f)|^2.
\end{align}
\vspace{-0.2cm}
\end{definition}

\textbf{Impact of RevIN on Frequency Spectrum \quad}
\begin{definition}[Reversible Instance Normalization]\label{def:RevIN}
Given a \textbf{forecast model} $ f: \mathbb{R}^T \rightarrow \mathbb{R}^F $ that generates a forecast $ \hat{y}(t) $ from a given input $x(t)$, RevIN is defined as:
\vspace{-0.2cm}
\begin{align}
&\hat{x}(t) = \frac{x(t) - \mu}{\sigma},\quad t = 0, 1, \dots, T-1\notag\\
&\hat{y}(t) = f(\hat{x}(t)), \quad \hat{y}(t)_{rev}= \hat{y}(t) \cdot \sigma + \mu,\notag\\
&\mu = \frac{1}{T} \sum_{t=0}^{T-1} x(t), \quad \sigma = \sqrt{\frac{1}{T} \sum_{t=0}^{T-1} (x(t) - \mu)^2}.
\end{align}
\vspace{-0.2cm}
\end{definition}

\begin{theorem} [Frequency Spectrum after RevIN] \label{theorem:RevIN}
\vspace{-0.2cm}
The spectral energy of $\hat{x}(t)$ (transformed using RevIN):
\begin{align}
E_{\hat{X}}(0)=0,& \quad f=0, \notag\\
E_{\hat{X}}(f) = \left( \frac{1}{\sigma} \right)^2 |X(f)|^2,&\quad f = 1,2,\dots, T-1 . 
\end{align}
\vspace{-0.2cm}
\end{theorem}
The proof is in Appendix~\ref{app:RevIN}. Theorem~\ref{theorem:RevIN} suggests that RevIN scales the absolute spectral energy by $ \sigma^2 $ but does not affect its relative distribution except $E_{\hat{X}}(0)=0$. Thus, RevIN preserves the relative spectral energy distribution and leaves the Mid-Frequency Spectrum Gap unresolved. \textit{However, our experiments still employ RevIN to ensure a fair comparison with other baselines.}
\begin{figure*}[h]
  \centering
  \includegraphics[width=1.\linewidth]{Faker/source/assets/jpg/ReFocus.jpg}
  \caption{General structure of \textbf{ReFocus}. `Adaptive Mid-Frequency Energy Optimizer (AMEO)' enhances mid-frequency components modeling, and `Energy-based Key-Frequency Picking Block' (EKPB) effectively captures shared Key-Frequency across channels}
  \label{fig:refocus}
\end{figure*}

\begin{figure*}[h]
  \centering
  \includegraphics[width=0.7\linewidth]{Faker/source/assets/jpg/ket.jpg}
  \caption{General process of the \textbf{Key-Frequency Enhanced Training strategy (KET)}, where spectral information from other channels is randomly introduced into each channel, to enhance the extraction of the shared Key-Frequency.}
  \label{fig:reshuffle}
\end{figure*}
\textbf{Impact of High- and Low-pass filter \quad}
We still define $\hat{x}(t)$ to be the filtered (processed) signal, obtained by applying a filter $H(f)$ (High/Low-pass filter). The filter $ H(f) $ is 1 in the passband (High/Low frequency) and 0 in the stopband (Middle frequency). So $E_{\hat{X}}(f)=0,\quad E_{\hat{X}}\leq E_X(f)$ for middle frequencies, which creates even larger gap.

\subsection{Overall Structure of The Proposed ReFocus}

In this section, we elucidate the overall architecture of \textbf{ReFocus}, depicted in Figure \ref{fig:refocus}. We define frequency domain projection as $D1\rightarrow D2$ representing a projection from dimension $D1$ to $D2$ in the frequency domain~\citep{xu2024fits}. Initially, we apply \textbf{AMEO} to the input $X \in \mathbb{R}^{C \times T}$, yielding the processed spectrum $ X_{am} \in \mathbb{R}^{C  \times T} $. Next, we use a projection $T\rightarrow D$ to transform $ X_{am}$ into the Variate Embedding $ X_{em} \in \mathbb{R}^{C  \times D}$~\citep{LiuiTransformer}. Then, $X_{em}$ go through $N$ \textbf{EKPB} to generate representation $H_{N+1}$, which is projected to obtain final prediction $\hat{Y}$. 

\textbf{Adaptive Mid-Frequency Energy Optimizer \quad}
Building upon the \textbf{Preliminary Analysis}, we propose a convolution- and residual learning-based solution to address the Mid-Frequency Spectrum Gap, which we denoted as AMEO. 
\begin{definition}[Adaptive Mid-Frequency Energy Optimizer]\label{def:AMEO}
AMEO is defined as:
\begin{align}
&\hat{x}(t) = x(t)-\frac{\beta}{K}\sum_{k=0}^{K-1} \tilde{x}(t+K-1-k),\notag\\
&\tilde{x}(t) =\notag\\
&\begin{cases}
x(t-(\frac{K}{2}+1)), \quad \text{if } \frac{K}{2}+1 \leq t < T+\frac{K}{2}+1, \\
0,  \quad\text{if } 0 \leq t < \frac{K}{2}+1 \text{ or } T+\frac{K}{2}+1 \leq t < T+K.
\end{cases}
\end{align}
\vspace{-0.2cm}
\end{definition}

It is equivalent to $x=x-\beta \cdot Conv(x)$. $Conv$ is a 1D convolution (Zero-padding at both ends, stride $s=1$, kernel size $K$, with values initialized as $ \frac{1}{K} $). $\beta \in \mathbb{R}^{1}$ is a hyperparameter.

\begin{theorem} [Frequency Spectrum after AMEO] \label{theorem:AMEO}
The spectral energy of $\hat{x}(t)$ obtained using AMEO:
\begin{align}
E_{\hat{X}}(f) =|X(f)|^2 \left\{1 - \beta \cdot \underbrace{\frac{1}{K} \sum_{k=0}^{K-1} e^{i 2 \pi f (\frac{3K}{2}-k -2) / {T-1}}}_{G(f)}\right\}^2
\end{align}
\vspace{-0.2cm}
\end{theorem}

The proof is in Appendix~\ref{app:AMEO}. We have $E_{\hat{X}}(f) =|X(f)|^2(1-\beta  \cdot G(f))^2$. Generally, $ G(f) $ behaves as a decay function, gradually reducing its value from \textbf{One} to \textbf{Zero}. Such \textbf{decay behavior} makes AMEO relatively enhances mid-frequency components, thus addressing the Mid-Frequency Spectrum Gap.

\textbf{Energy-based Key-Frequency Picking Block \quad} In each \textbf{EKPB}, the input $ H_i \in \mathbb{R}^{C  \times D} (H_1=X_{em}) $ is first processed through an MLP to generate $ H_i^k \in \mathbb{R}^{C  \times Q}$. Then, FFT is applied to get $ H_i^f \in \mathbb{R}^{C  \times (Q/2+1)}$. For $ H_i^f$, we calculate its energy, denoted as $ H_i^e \in \mathbb{R}^{C  \times (Q/2+1)}$. A cross-channel softmax is then applied to $ H_i^e$ per frequency to obtain a probability distribution $ H_i^{soft} \in \mathbb{R}^{C  \times (Q/2+1)}$. Using $H_i^{soft}$, we select values from $ H_i^f$ across channels for each frequency, resulting in $K^f_i \in \mathbb{R}^{1  \times (Q/2+1)}$, which represents the Shared Key-Frequency across all channels. Then iFFT is performed on $K^f_i$ to get $K_i\in \mathbb{R}^{1  \times Q}$, followed by projection $Q\rightarrow D$ and repeating (C times) to get $\hat{K}_i \in \mathbb{R}^{C  \times D}$. This $\hat{K}_i$ is point-wisely added to $\hat{H_i}\in \mathbb{R}^{C  \times D}$ , which is the projection of $ H_i$ using projection $D\rightarrow D$. Then, an MLP and $Add\&Norm$ is applied to the result $HK\in \mathbb{R}^{C  \times D}$ to fuse inter-series dependencies information, and another MLP and $Add\&Norm$ is used to capture intra-series variations~\citep{LiuiTransformer}. The output of each \textbf{EKPB} is $\hat{O_i} \in \mathbb{R}^{C  \times D}$, where $H_{i+1}=\hat{O_i}$.

\subsection{Key-Frequency Enhanced Training strategy}

In real-world time series, certain channels often exhibit spectral dependencies, which may not be fully captured in the training set, and the specific channels with such dependencies are also unknown~\citep{geweke1984freqchannel,Zhao2024freqchannel}. So this work borrows insight from recent advancement of mix-up in time series~\citep{zhou2023mixup,ansari2024mixup}, randomly introducing spectral information from other channels into each channel, to enhance the extraction of the shared Key-Frequency, as in Figure~\ref{fig:reshuffle}. Given a multivariate time series input $X \in \mathbb{R}^{C \times T}$ and its ground-truth $Y \in \mathbb{R}^{C \times F}$, we generate a pseudo sample pair: 

\begin{align}
X' = iFFT(FFT(X) +\alpha \cdot FFT(X[\text{perm},:]))&,  \notag\\ 
Y' = iFFT(FFT(Y) +\alpha \cdot FFT(Y[\text{perm},:]))&.
\end{align}

$\alpha \in \mathbb{R}^{C \times 1}$ is a weight vector sampled from a normal distribution, $\text{perm}$ is a reshuffled channel index. Since $FFT$ and $iFFT$ are linear operations, this mix-up process can be equivalently simplified in the \textbf{Time Domain}:
\begin{align}
X' = X +\alpha \cdot X[\text{perm},:]&,  \notag\\
Y' = Y +\alpha \cdot Y[\text{perm},:]&
 \end{align}
We alternate training between real and synthetic data to preserve the spectral dependencies in real samples. This combines the advantages of data augmentation, such as improved generalization, while mitigating potential drawbacks like over-smoothing and training instability~\citep{ryu2024tf,alkhalifah2022tf}.











