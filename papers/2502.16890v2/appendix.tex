\appendix
\section{Proof}
\label{app:Proof}
This section is dedicated to proving Theorem~\ref{theorem:RevIN} and Theorem~\ref{theorem:AMEO}.

\subsection{Impact of RevIN on Frequency Spectrum}
\label{app:RevIN}

RevIN~\citep{Kim_revin,liu2022non} normalizes inputs using sample-wise mean and variance, then reverts scaling post-prediction to ensure consistent distributions, mitigating non-stationary effects in time series. 

Let the original time series be $ x(t) $ with length $ T $. The series $\hat{x}(t)$ that processed by RevIN is given by:
\vspace{-0.2cm}
\begin{align}
&\hat{x}(t) = \frac{x(t) - \mu}{\sigma},t = 0, 1, \dots, T-1,\notag\\
&\mu = \frac{1}{T} \sum_{t=0}^{T-1} x(t), \quad \sigma = \sqrt{\frac{1}{T} \sum_{t=0}^{T-1} (x(t) - \mu)^2}.
\end{align}
\vspace{-0.2cm}

The Fourier transform of $x(t)$ and $\hat{x}(t)$ are:
\vspace{-0.2cm}
\begin{align}
X(f) &= \sum_{t=0}^{T-1} x(t) e^{-i 2 \pi f t / {T-1}}, \quad f = 0, 1, \dots, T-1,\notag\\
\hat{X}(f)&=\sum_{t=0}^{T-1} \left(\frac{x(t) - \mu}{\sigma}\right) e^{-i 2 \pi f t / {T-1}} \notag\\
&=\frac{1}{\sigma} \sum_{t=0}^{T-1} x(t) e^{-i 2 \pi f t / {T-1}} - \frac{\mu}{\sigma} \sum_{t=0}^{T-1} e^{-i 2 \pi f t / {T-1}}.
\end{align}
\vspace{-0.2cm}

The spectral energy is computed as the squared magnitude of the Fourier transform. For $ x(t) $ and $ \hat{x}(t) $, we have: 
\begin{align}
E_X(f) = |X(f)|^2, \quad E_{\hat{X}}(f) = |\hat{X}(f)|^2.
\end{align}

When $ f = 0 $, the exponential term $ e^{-i 2 \pi f t / {T-1}} = 1 $, so: 
\begin{align}
\hat{X}(0)&=\frac{1}{\sigma} \sum_{t=0}^{T-1} x(t)-\frac{\mu T}{\sigma}\notag\\
&=\frac{\mu T}{\sigma}-\frac{\mu T}{\sigma}\notag\\
&=0
\end{align}

Since $\frac{\mu}{\sigma}$ is a constant, we have:
\begin{align}
\frac{\mu}{\sigma} \cdot \sum_{t=0}^{T-1}& e^{-i 2 \pi f t / {T-1}} = 0,\quad  f = 1,2\dots,T-1 ,\notag\\
\hat{X}(f)=&\frac{1}{\sigma} \sum_{t=0}^{T-1} x(t) e^{-i 2 \pi f t / {T-1}} - \frac{\mu}{\sigma} \sum_{t=0}^{T-1} e^{-i 2 \pi f t / {T-1}}\notag\\
=&\frac{1}{\sigma}X(f),\notag\\
E_{\hat{X}}(f)& = \left( \frac{1}{\sigma} \right)^2 |X(f)|^2. 
\end{align}
\textbf{This suggests that RevIN scales the spectral energy by $ \sigma^2 $ but does not affect its relative distribution except $\hat{X}(0)=0$.} Thus, RevIN preserves the relative spectral energy distribution and leaves the Mid-Frequency Spectrum Gap unresolved.


\subsection{Impact of AMEO on Frequency Spectrum}
\label{app:AMEO}

Referring back to Definition~\ref{def:AMEO}, AMEO is defined as: 
\begin{align}
&\hat{x}(t) = x(t)-\frac{\beta}{K}\sum_{k=0}^{K-1} \tilde{x}(t+K-1-k),\notag\\
&\tilde{x}(t) =
\begin{cases}
x(t-(\frac{K}{2}+1)), \quad \text{if } \frac{K}{2}+1 \leq t < T+\frac{K}{2}+1 \\
0,  \quad\text{if } 0 \leq t < \frac{K}{2}+1 \text{ or } T+\frac{K}{2}+1 \leq t < T+K
\end{cases}
\end{align}

The Fourier transform of $\hat{x}(t)$ is:
\begin{align}
&\hat{X}(f) = \sum_{t=0}^{T-1} \left[ x(t) - \frac{\beta}{K} \sum_{k=0}^{K-1} \tilde{x}(t + K - 1 - k) \right] e^{-i 2 \pi f t / {T-1}} \notag\\
&= \underbrace{\sum_{t=0}^{T-1} x(t) e^{-i 2 \pi f t / {T-1}}}_{X(f)} - \frac{\beta}{K} \sum_{k=0}^{K-1} \underbrace{\sum_{t=0}^{T-1} \tilde{x}(t + K - 1 - k) e^{-i 2 \pi f t / {T-1}}}_{T_k(f)}.
\end{align}

For $T_k(f)$, given $FFT\{x(t-a)\}=X(f)e^{-i 2 \pi f a / {T-1}}$, we have:
\begin{align}
T_k(f)&=\sum_{t=0}^{T-1} \tilde{x}(t + K - 1 - k) e^{-i 2 \pi f t / {T-1}}\notag\\
&=\sum_{t=0}^{T-1} x(t + \frac{3K}{2} - k-2) e^{-i 2 \pi f t / {T-1}}\notag\\
&=FFT\{x(t + \frac{3K}{2} - k-2)\}\notag\\
&=X(f)e^{i 2 \pi f (\frac{3K}{2} - k-2) / {T-1}}
\end{align}

So, we have the Fourier transform of $\hat{x}(t)$ and its spectral energy:
\begin{align}
\hat{X}(f) &=X(f)-\frac{\beta}{K} \sum_{k=0}^{K-1}X(f)e^{i 2 \pi f (\frac{3K}{2} - k-2) / {T-1}}\notag\\
&=X(f) \left[ 1 - \beta \cdot \underbrace{\frac{1}{K} \sum_{k=0}^{K-1} e^{i 2 \pi f (\frac{3K}{2} -k-2) / {T-1}}}_{G(f)} \right],\notag\\
E_{\hat{X}}(f)& =|X(f)|^2 \left\{1 - \beta \cdot \underbrace{\frac{1}{K} \sum_{k=0}^{K-1} e^{i 2 \pi f (\frac{3K}{2}-k -2) / {T-1}}}_{G(f)}\right\}^2\notag\\
&=|X(f)|^2 (1 - \beta \cdot G(f))^2.
\end{align}

In this paper, we set $ K = 25 $ (i.e.,$ T/4 + 1 $, $T=96$), and the function graph of $G(f)$ is shown in Figure~\ref{fig:gfunction}.
\begin{figure}[h]
  \centering
  \includegraphics[width=1\linewidth]{Faker/source/assets/jpg/gfunction.png}
  \caption{\small{The function $ G(f)$ is plotted for $ T = 96 $ and $ K = 25 $. Due to the symmetry of the $FFT$, we only need to plot the values for $ f = 0, 1, \dots, 48 $.}}
  \label{fig:gfunction}
\end{figure}

It is evident that $ G(f) $ is a gradually decay function, with its values decreasing \textbf{from 1 to 0}. This ensures that $ E_{\hat{X}}(f) = |X(f)|^2 (1 - \beta \cdot G(f))^2 $, where, relative to $ E_X $, the low-frequency components are attenuated, and the mid-frequency components are enhanced.


\section{EXPERIMENTAL DETAILS}
\begin{figure*}[ht]
  \centering
  \includegraphics[width=1\linewidth]{Faker/source/assets/jpg/loss.jpg}
  \caption{\small{We select the $input-96-forecast-96$ task on Traffic and visualize the validation loss and weight of our \textbf{ReFocus} model. \textbf{LEFT}: Visualization of the \textbf{Validation Loss} during $100$ training epochs with (\textbf{KET}) and without KET (\textbf{Raw}). \textbf{RIGHT}: Visualization about the Weight (Obtained using the approach outlined in \textbf{Analysis of linear model}~\citep{toner2024alinear}) of the trained model. Two significant metrics for assessing the information richness of the weight matrix-the information \textbf{Entropy} and the \textbf{Sum of Eigenvalues}-are calculated. Both indicate higher quality with greater values.}}
  \label{fig:loss_wei}
\end{figure*}
\label{app:exp}
\subsection{Dataset Statistics}
\label{app:datasets details}
We elaborate on the datasets employed in this study with the following details.

\begin{itemize}
    \item \textbf{ETT Dataset}~\citep{Zhou2020informer} comprises two sub-datasets: \textbf{ETTh} and \textbf{ETTm}, which were collected from electricity transformers. Data were recorded at 15-minute and 1-hour intervals for ETTm and ETTh, respectively, spanning from July 2016 to July 2018.

    \item \textbf{Solar\_Energy}~\citep{Lai2018lstnet} records the solar power production of 137 PV plants in 2006, which are sampled every 10 minutes.

    \item \textbf{Electricity Dataset}\footnote{\url{https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014}} encompasses the electricity consumption data of 321 customers, recorded on an hourly basis, covering the period from 2012 to 2014. 

    \item \textbf{Traffic Dataset}\footnote{\url{https://pems.dot.ca.gov/}} consists of hourly data from the California Department of Transportation. It describes road occupancy rates measured by various sensors on San Francisco Bay area freeways. 

    \item \textbf{Weather Dataset}\footnote{\url{https://www.bgc-jena.mpg.de/wetter/}} contains records of $21$ meteorological indicators, updated every $10$ minutes throughout the entire year of 2020. 

\end{itemize}
We follow the same data processing and train-validation-test set split protocol used in iTransformer~\citep{LiuiTransformer}, where the train, validation, and test datasets are strictly divided according to chronological order to make sure there are no data leakage issues. We fix the input length as $T = 96$ for all datasets, and the forecasting length $F \in \{96, 192, 336, 720\}$.

\subsection{Implementation Details and Model Parameters}
\label{app:implementation}
We trained our ReFocus model using the MSE loss function and employed the ADAM optimizer. For evaluation purposes, we used two key performance metrics: the mean square error (MSE) and the mean absolute error (MAE). We initialized the random seed as $rs = 2024$ and set the hyperparameter $K = 25$-kernel size of the convolution kernel in AMEO. The dimension of the Layer is set to $D =512$ and $Q=128$. The batch size $bs=32$ for the Traffic dataset due to its large channel will cause \textbf{out of memory} when employed with large batch size, and $bs=128$ for others. The learning rate is searched from $lr \in \{1e-5,1e-4\}$ except for the Traffic dataset ($lr=5e-4$). The number of EKPB is searched from $N \in \{1,2,3,4\}$, and hyperparameter $\beta$. which controls the scale magnitude, from $\beta \in \{ 0.01,0.1,0.5,1.0\}$. Our implementation was carried out in PyTorch and executed on a single NVIDIA GeForce RTX 4090 with 24G VRAM. To foster reproducibility, we make our code, and training scripts available in this \textbf{GitHub Repository}\footnote{\url{https://github.com/Levi-Ackman/ReFocus}}. 

All the compared multivariate forecasting baseline models that we reproduced are implemented based on the benchmark of \textbf{Time series Lab}~\citep{Wangtslb2024} Repository~\footnote{\url{https://github.com/thuml/Time-Series-Library}}, which is fairly built on the configurations provided by each model's original paper or official code. Those that have not yet been included in \textbf{Time series Lab} are directly reproduced from their official code repositories. It is worth noting that both the baselines used in this paper and our \textbf{ReFocus} have fixed a long-standing bug. This bug was originally identified in Informer~\citep{Zhou2020informer} \textbf{(AAAI 2021 Best Paper)} and subsequently addressed by FITS~\citep{xu2024fits}. For specific details about the bug and its resolution, please refer to \textbf{GitHub Repository}\footnote{\url{https://github.com/VEWOXIC/FITS}}.

\section{Further Analysis of the proposed Key-Frequency Enhanced Training strategy}
\label{app:further ket}
To further investigate the impact of the proposed \textbf{`Key-Frequency Enhanced Training (KET) strategy'} on model training and forecasting ability, we visualize its training process regarding Validation Loss and the model weights obtained after training in Figure~\ref{fig:loss_wei}. We also compute the \textbf{Entropy} and the \textbf{Sum of Eigenvalues} of the weight matrix. 

The results show that, in the absence of KET, the model quickly overfits around the \textbf{24}th epoch, exhibiting poor generalization. In contrast, with the aid of KET, the model consistently performs better on the validation set, converging smoothly without overfitting, and the training process becomes more stable. Additionally, weight visualization results indicate that the model trained with KET has higher information \textbf{Entropy} and a greater \textbf{Sum of Eigenvalues}, suggesting that the trained model possesses a stronger capacity for feature representation extraction. The predictive results further validate this, as our KET improves the MSE from 0.414 to 0.380, achieving an \textbf{8.2\%} reduction.

\section{Full Results}
\label{app:full results}

The full experiment results are provided in the following section due to the space limitation of the main text. 

\paragraph{Full multivariate forecasting results }
\label{app:full bench}
Table~\ref{tab:full_bench} contains the detailed results of Ten baselines and our ReFocus on eight well-acknowledged forecasting benchmarks. ReFocus consistently achieves the best overall performance across all datasets, especially in tasks with a large number of channels, such as the Solar\_Energy dataset (\textbf{137} channels), ECL dataset (\textbf{321} channels), and Traffic dataset (\textbf{862} channels). It obtains the best performance in terms of MSE: \textbf{34 out of 40} tasks, and MAE: \textbf{36 out of 40} tasks. These results demonstrate the outstanding performance of ReFocus in multivariate time series forecasting tasks.
\input{Faker/source/appendix/full_bench}

\paragraph{Full results of ablation on AMEO and KET }  
\label{app:full ablation}
Table~\ref{app:full ablation} presents the full results of the ablation study on `Adaptive Mid-Frequency Energy Optimizer (AMEO)' and `Key-Frequency Enhanced Training (KET)'. KET and AMEO contribute significantly to the model's performance, each providing substantial improvements. Moreover, their combination further enhances the model, achieving peak performance. These results provide strong evidence of the effectiveness of both AMEO and KET.
\input{Faker/source/appendix/full_ablation}

\paragraph{Full results of further ablation study on KET }
\label{app:full ket}
Table~\ref{app:full ket} exhibits the full results of a further ablation study on the `Key-Frequency Enhanced Training (KET)' strategy. Introducing Pseudo samples—obtained by randomly incorporating spectral information from other channels into the current channel—generally leads to performance improvement. However, on more complex datasets, it results in performance degradation. In contrast, alternating training between Real and Pseudo samples (\textbf{Our KET}) overcomes this issue, yielding a further and consistent enhancement in performance.
\input{Faker/source/appendix/full_ket}

\paragraph{Full results of ablation study of different Key-Frequency Picking strategies }
\label{app:full pick}
Table~\ref{app:full pick} illustrates the complete results of the ablation study on various Key-Frequency Picking strategies. Notably, our \textbf{Softmax-based random sampling} strategy consistently achieves the best overall performance, particularly on more complex datasets.

\input{Faker/source/appendix/full_pick}

\paragraph{Full results of EKPB and other inter-series dependencies modeling backbone }
\label{app:full ekpb}
Table~\ref{app:full ekpb} presents the full results of `Energy-based Key-Frequency Picking Block (EKPB)' and other inter-series dependency modeling backbones on multivariate time series forecasting tasks. The proposed EKPB achieves overall optimal performance, demonstrating exceptional capability in modeling inter-series dependencies.

\input{Faker/source/appendix/full_ekpb}

