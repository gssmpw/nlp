\section{Experiments}
\subsection{Experimental Settings}
This section first introduces the whole experiment settings under a fair comparison.
Secondly, we illustrate the experiment results by comparing \textbf{ReFocus} with the \textbf{TEN} well-acknowledged baselines.
Further, we conducted an ablation study to comprehensively investigate the effectiveness of the `Adaptive Mid-Frequency Energy Optimizer' (\textbf{AMEO}), `Energy-based Key-Frequency Picking Block' (\textbf{EKPB}), and `Key-Frequency Enhanced Training strategy' (\textbf{KET}).

\input{Faker/source/table/dataset}
\textbf{Datasets \quad} We conduct extensive experiments on selected \textbf{Eight} widely-used real-world multivariate time series forecasting datasets, including Electricity Transformer Temperature (ETTh1, ETTh2, ETTm1, and ETTm2), Electricity, Traffic, Weather used by Autoformer~\citep{Wu2021autoformer}, and Solar\_Energy datasets proposed in LSTNet~\citep{Lai2018lstnet}.
For a fair comparison, we follow the same standard protocol~\citep{LiuiTransformer} and split all forecasting datasets into training, validation, and test sets by the ratio of 6:2:2 for the ETT dataset and 7:1:2 for the other datasets. The characteristics of these datasets are shown in Table \ref{tab:datasets} (More can be found in the Appendix). 

\textbf{Evaluation protocol \quad} Following TimesNet~\citep{wu2022timesnet}, we use Mean Squared Error (MSE) and Mean Absolute Error (MAE) for the evaluation. We follow the same evaluation protocol, where the input length is set as $T=96$ and the forecasting lengths $F \in \{96, 192, 336, 720\}$. All the experiments are conducted on a single NVIDIA GeForce RTX 4090
with 24G VRAM. The MSE loss function is utilized for model optimization. To foster reproducibility, we make our code, and training scripts available in this \textbf{GitHub Repository}\footnote{\url{https://github.com/Levi-Ackman/ReFocus}}. Full implementation details and other information are in Appendix~\ref{app:exp}.

\textbf{Baseline setting \quad} We carefully choose \textbf{TEN} well-acknowledged forecasting models as our baselines, including 1) Transformer-based methods: iTransformer~\citep{LiuiTransformer}, Crossformer~\citep{zhang2023crossformer}, PatchTST~\citep{Nie2022patchtst}; 2) Linear-based methods: TSMixer~\citep{chen2023tsmixer}, DLinear~\citep{zeng2023dlinear}; 3) TCN-based methods: TimesNet~\citep{wu2022timesnet}, ModernTCN~\citep{donghao2024moderntcn}; 4)Recent cutting-edge frequency-based methods that discussed earlier: FilterNet~\citep{yi2024filternet}, FITS~\citep{xu2024fits}, FreTS~\citep{yi2023fremlp}. These models represent the latest advancements in multivariate time series forecasting and encompass all mainstream prediction model types. The results of ModernTCN, FilterNet, FITS, and FreTS are taken from FilterNet~\citep{yi2024filternet} and other results are taken from iTransformer~\citep{LiuiTransformer}.


\subsection{Experiment Results}
\input{Faker/source/table/bench_avg.tex}
\textbf{Quantitative comparison \quad} Comprehensive forecasting results are listed in Table~\ref{tab:bench_avg}. We leave full forecasting results in APPENDIX to save place. It is quite evident that \textbf{ReFocus} has demonstrated superior predictive performance across all datasets, significantly outperforming the second-best method.  Especially, Compared to the previous SOTA \textbf{iTransformer}, we have reduced the MSE by \textbf{4\%}, \textbf{6\%}, and \textbf{5\%} on the three most challenging benchmarks: Traffic, ECL, and Solar, respectively, indicating a significant breakthrough. These significant improvements indicate that the \textbf{ReFocus} model possesses robust performance and broad applicability in multivariate time series forecasting tasks, especially in tasks with a large number of channels, such as the Solar\_Energy dataset (\textbf{137} channels), ECL dataset (\textbf{321} channels), and Traffic dataset (\textbf{862} channels).

\subsection{Model Analysis}

\input{Faker/source/table/ablation_avg.tex}
\input{Faker/source/table/KET_avg.tex}
\input{Faker/source/table/pick_avg.tex}
\input{Faker/source/table/ekpb_avg}
\input{Faker/source/table/refocus_eff.tex}
\input{Faker/source/table/ameo_avg.tex}

\begin{figure*}[!h]
  \centering
  \includegraphics[width=0.95\linewidth]{Faker/source/assets/jpg/ameo.jpg}
  \caption{\small{The time-frequency domain visualization of the original sequence (ETTm1, \textbf{\textit{the last variate}}), the sequence processed by high-pass and low-pass filters, by RevIN, and by AMEO. We selected the $input-96-forecast-96$ task.}}
  \label{fig:ameo}
\end{figure*}
\begin{figure}[!h]
  \centering
  \includegraphics[width=1\linewidth]{Faker/source/assets/jpg/embed.jpg}
  \caption{\small{T-SNE visualization of the series embeddings with and without `Energy-based Key-Frequency Picking Block' (EKPB) on ECL dataset. We choose the $input-96-forecast-96$ task. Three example variates are highlighted: variates 2\&3 shared a common Key-Frequency, while variate 1 does not.}}
  \label{fig:embed}
  \vspace{-5mm} 
\end{figure}

\textbf{Ablation study of AMEO and KET \quad} To evaluate the contributions of each module in ReFocus, we performed ablation studies on the `Adaptive Mid-Frequency Energy Optimizer (AMEO)' and the `Key-Frequency Enhanced Training (KET)' strategy. The results are summarized in Table~\ref{tab:ablation_avg}. Notably, integrating both modules achieves the best performance, highlighting the effectiveness of their synergy. Additionally, each module delivers substantial improvements over baseline models in most cases.

\textbf{Further study of KET \quad}  We conducted further ablation studies on the KET to demonstrate the importance of alternate training between real and synthetic data. The experimental results in Table~\ref{tab:ket_avg} reveal that while training on pseudo samples can partially enhance the model's generalization performance on the test set, it also tends to cause over-smoothing and training instability on more complex datasets, such as Solar\_Energy. In contrast, training on real and synthetic data alternatively (KET) improves generalization and mitigates over-smoothing and training instability by preserving the spectral dependencies of real samples. More Analyses are in Appendix~\ref{app:further ket}. 

\textbf{Ablation study of different Key-Frequency Picking strategy \quad} We conducted an ablation study on various key-frequency selection strategies. The evaluated methods include Maximum-based, Minimum-based, and \textbf{Softmax-based} random sampling strategies. Our experimental results in Table~\ref{tab:pick_avg} reveal that purely relying on Maximum or Minimum-based strategies may overlook certain critical Key-Frequency. In contrast, the random sampling strategy based on a Softmax probabilistic distribution consistently achieved the best overall performance, particularly on datasets with a larger number of channels and higher complexityâ€”key challenges in multivariate time series forecasting. 

\textbf{Outstanding inter-series modeling ability of the EKPB \quad} We compared `Energy-based Key-Frequency Picking Block' (EKPB) with several well-established backbones, including iTransformer~\citep{LiuiTransformer}, TSMixer~\citep{chen2023tsmixer}, and Crossformer~\citep{zhang2023crossformer}, which have demonstrated exceptional performance in modeling inter-series dependencies. Additionally, we included FECAM
~\citep{jiang2023fecam}, a method also designed for modeling cross-channel frequency-domain dependencies. The results presented in Table~\ref{tab:ekpb_avg} demonstrate that our EKPB outperforms in modeling inter-series dependencies across multiple datasets. Additionally, when comparing the number of parameters and inference time during prediction under identical configurations on the ECL dataset, our EKPB method still outperforms other baselines by a significant margin, as in Table~\ref{tab:efficiency}. To illustrate EKPB's functionality, we visualize the series embeddings with and without its adjustment in Figure~\ref{fig:embed}. The T-SNE visualization of the series embeddings shows that without EKPB, using only the channel-independent strategy~\citep{Nie2022patchtst}, the MSE is 0.171. After applying EKPB, channels sharing Key-Frequency (variates 2\&3) are clustered, while others (variates 1\&3) are separated. This adjustment improves the MSE from 0.171 to 0.145, a \textbf{15\%} reduction. These indicate that EKPB not only achieves better predictive performance but also offers a more resource-efficient solution than other baselines.

\textbf{Superiority of AMEO over RevIN and Filters \quad} We investigated the roles of AMEO, RevIN, and Filters in addressing the Mid-Frequency Spectrum Gap through time-frequency domain visualization analysis. The results presented in Figure~\ref{fig:ameo} align perfectly with our theoretical analysis before. High-pass and low-pass filters fail to address the Mid-Frequency Spectrum Gap and exacerbate this issue. RevIN, on the other hand, merely eliminates the energy of the zero-frequency component while scaling other components using the variance \( \sigma^2 \), which also does not effectively resolve the problem. In contrast, our AMEO successfully amplifies the mid-frequency energy. Furthermore, compared to the original sequence and the sequence processed by RevIN, we observe that the sequence processed by AMEO exhibits significantly higher stationarity with much more stable means and variance.

In Table~\ref{tab:ameo_avg}, the performance of AMEO on two prediction tasks across two datasets consistently surpasses the results achieved by methods based on RevIN and Filters. Furthermore, while Filters and RevIN occasionally lead to degraded performance on certain datasets, AMEO consistently delivers results that outperform the original methods. These findings further highlight the superiority of AMEO over alternative approaches.