@misc{alabi2022adapting,
      title={Adapting Pre-trained Language Models to African Languages via Multilingual Adaptive Fine-Tuning}, 
      author={Jesujoba O. Alabi and David Ifeoluwa Adelani and Marius Mosbach and Dietrich Klakow},
      year={2022},
      eprint={2204.06487},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2204.06487}, 
}

@misc{thangaraj2024crosslingual,
      title={Cross-lingual transfer of multilingual models on low resource African Languages}, 
      author={Harish Thangaraj and Ananya Chenat and Jaskaran Singh Walia and Vukosi Marivate},
      year={2024},
      eprint={2409.10965},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.10965}, 
}

@inproceedings{wang2020minilm,
 author = {Wang, Wenhui and Wei, Furu and Dong, Li and Bao, Hangbo and Yang, Nan and Zhou, Ming},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {5776--5788},
 publisher = {Curran Associates, Inc.},
 title = {MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{wang2020minilmv2,
      title={MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers}, 
      author={Wenhui Wang and Hangbo Bao and Shaohan Huang and Li Dong and Furu Wei},
      year={2021},
      eprint={2012.15828},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2012.15828}, 
}

