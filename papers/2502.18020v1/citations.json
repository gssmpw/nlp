[
  {
    "index": 0,
    "papers": [
      {
        "key": "alabi2022adapting",
        "author": "Jesujoba O. Alabi and David Ifeoluwa Adelani and Marius Mosbach and Dietrich Klakow",
        "title": "Adapting Pre-trained Language Models to African Languages via Multilingual Adaptive Fine-Tuning"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "thangaraj2024crosslingual",
        "author": "Harish Thangaraj and Ananya Chenat and Jaskaran Singh Walia and Vukosi Marivate",
        "title": "Cross-lingual transfer of multilingual models on low resource African Languages"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "wang2020minilm",
        "author": "Wang, Wenhui and Wei, Furu and Dong, Li and Bao, Hangbo and Yang, Nan and Zhou, Ming",
        "title": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "wang2020minilmv2",
        "author": "Wenhui Wang and Hangbo Bao and Shaohan Huang and Li Dong and Furu Wei",
        "title": "MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers"
      }
    ]
  }
]