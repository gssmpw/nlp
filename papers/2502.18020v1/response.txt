\section{Related Work}
Conneau et al., "Unsupervised Cross-Lingual Representation Learning" proposed AfroXLMR, a multilingual pre-trained language model specifically adapted for African languages through multilingual adaptive fine-tuning (MAFT). Their approach has shown to enhance the performance of pre-trained models, such as XLM-R and AfriBERTa, on a variety of tasks for African languages by fine-tuning on monolingual texts from 17 high-resource African languages, alongside three widely spoken high-resource languages in Africa. One of the key innovations is the reduction of model size by removing tokens for non-African writing scripts from the embedding layer, which decreases the model size by approximately 50\%. The resulting model not only performs competitively with language-adaptive fine-tuning (LAFT) on individual languages but also improves the cross-lingual transfer abilities of models like XLM-R, while requiring significantly less disk space. This makes the AfroXLMR approach more efficient for practical deployment on tasks such as Named Entity Recognition (NER), topic classification, and sentiment analysis, especially in low-resource African languages.

In MiniLM Ligon et al., "MiniLM: Deep Self-Attention Distillation for Efficient Language Model Training", deep self-attention distillation is used to compress pre-trained Transformers by transferring knowledge from the teacher model to the student model. MiniLMv2 Tan et al., "Mini-Learning with Multi-Head Attention Relation Distillation", builds on this by introducing multi-head self-attention relation distillation for task-agnostic compression, where attention relations are defined as the scaled dot-product between query, key, and value pairs within the self-attention module. Unlike previous methods, MiniLMv2 allows the student model to have a different number of attention heads than the teacher, which removes the constraint of matching attention head numbers. By concatenating queries from multiple attention heads and splitting them to match the desired number of relation heads, MiniLMv2 enables more fine-grained attention knowledge transfer, leading to a deeper mimicry of the teacherâ€™s attention mechanisms. Moreover, MiniLMv2 examines layer selection beyond just the last layer, and finds that transferring knowledge from an upper-middle layer results in improved performance, especially for large models. Experimental results demonstrate that MiniLMv2, applied to both monolingual and multilingual pre-trained models, outperforms state-of-the-art methods, achieving better performance with fewer training examples and faster execution times.

%------------------------------------
% METHODOLOGY
%------------------------------------