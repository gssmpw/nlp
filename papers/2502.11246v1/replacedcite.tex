\section{Related work}
\noindent\textbf{Visual in-context learning}: In-context learning (ICL) has transformed LLM adaptation, enabling task generalization with few-shot demonstrations, and recent advancements have extended it to multimodal models for vision-language tasks like VQA____. However, ICL suffers from computational inefficiency due to long input sequences and sensitivity to demonstration selection____. To mitigate this, in-context vectors (ICVs) distill task-relevant information into compact representations, reducing the need for multiple examples____. While early non-learnable ICVs improved NLP efficiency, they struggled with multimodal tasks like VQA due to diverse vision-language inputs____. Recently, learnable ICVs dynamically capture essential task information, significantly enhancing VQA performance while lowering computational costs____. These advancements underscore the importance of optimizing vector-based representations and refining ICL strategies to improve multimodal reasoning____.\\
% In-context learning (ICL) has revolutionized adaptation in LLMs by enabling task generalization through a few example demonstrations, and recent advancements have extended this capability to large multimodal models for vision-language tasks like visual question answering (VQA)____. However, ICL in LMMs faces challenges such as computational inefficiency due to long input sequences and sensitivity to demonstration selection____. To address these, researchers have introduced in-context vectors (ICVs), which distill task-relevant information into a compact vector, reducing the need for multiple examples at inference time____. While early non-learnable ICVs improved efficiency in NLP tasks, they struggled with complex multimodal tasks like VQA due to the diverse nature of vision-language inputs____. Recently, Learnable In-Context Vectors have been proposed to dynamically capture essential task information, significantly enhancing VQA performance while reducing computational cost____. These advances indicate that optimizing vector-based representations and refining ICL strategies in LMMs remain crucial for improving multimodal reasoning____.
\textbf{Intervention generation}: Intervention strategies for online toxicity have largely focused on text-based issues like hate speech____, misinformation____ and harm____, with limited exploration of multimodal challenges such as memes. While counterspeech interventions reshape discourse____, their reliance on manual curation____ or supervised datasets limits scalability. Advances in LLMs and VLMs____ have improved intervention capabilities but often lack contextual grounding, requiring knowledge-driven approaches____. To address this, MemeGuard enhances meme interpretation using VLMs and knowledge ranking, enabling more precise and contextually relevant interventions____.