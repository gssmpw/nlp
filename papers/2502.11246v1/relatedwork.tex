\section{Related work}
\noindent\textbf{Visual in-context learning}: In-context learning (ICL) has transformed LLM adaptation, enabling task generalization with few-shot demonstrations, and recent advancements have extended it to multimodal models for vision-language tasks like VQA~\cite{brown2020languagemodelsfewshotlearners, alayrac2022flamingovisuallanguagemodel}. However, ICL suffers from computational inefficiency due to long input sequences and sensitivity to demonstration selection~\cite{peng2024livelearnableincontextvector}. To mitigate this, in-context vectors (ICVs) distill task-relevant information into compact representations, reducing the need for multiple examples~\cite{hendel2023incontextlearningcreatestask, todd2024functionvectorslargelanguage}. While early non-learnable ICVs improved NLP efficiency, they struggled with multimodal tasks like VQA due to diverse vision-language inputs~\cite{li2023configuregoodincontextsequence, yang2024exploringdiverseincontextconfigurations}. Recently, learnable ICVs dynamically capture essential task information, significantly enhancing VQA performance while lowering computational costs~\cite{peng2024livelearnableincontextvector}. These advancements underscore the importance of optimizing vector-based representations and refining ICL strategies to improve multimodal reasoning~\cite{Yin_2024}.\\
% In-context learning (ICL) has revolutionized adaptation in LLMs by enabling task generalization through a few example demonstrations, and recent advancements have extended this capability to large multimodal models for vision-language tasks like visual question answering (VQA)~\cite{brown2020languagemodelsfewshotlearners, alayrac2022flamingovisuallanguagemodel}. However, ICL in LMMs faces challenges such as computational inefficiency due to long input sequences and sensitivity to demonstration selection~\cite{peng2024livelearnableincontextvector}. To address these, researchers have introduced in-context vectors (ICVs), which distill task-relevant information into a compact vector, reducing the need for multiple examples at inference time~\cite{hendel2023incontextlearningcreatestask, todd2024functionvectorslargelanguage}. While early non-learnable ICVs improved efficiency in NLP tasks, they struggled with complex multimodal tasks like VQA due to the diverse nature of vision-language inputs~\cite{li2023configuregoodincontextsequence, yang2024exploringdiverseincontextconfigurations}. Recently, Learnable In-Context Vectors have been proposed to dynamically capture essential task information, significantly enhancing VQA performance while reducing computational cost~\cite{peng2024livelearnableincontextvector}. These advances indicate that optimizing vector-based representations and refining ICL strategies in LMMs remain crucial for improving multimodal reasoning~\cite{Yin_2024}.
\textbf{Intervention generation}: Intervention strategies for online toxicity have largely focused on text-based issues like hate speech~\cite{qian-etal-2019-benchmark, jha2024memeguardllmvlmbasedframework}, misinformation~\cite{10.1145/3543507.3583388} and harm~\cite{banerjee2024safeinfercontextadaptivedecoding, hazra2024safetyarithmeticframeworktesttime, banerjee2025navigatingculturalkaleidoscopehitchhikers}, with limited exploration of multimodal challenges such as memes. While counterspeech interventions reshape discourse~\cite{SchiebGoverningHS}, their reliance on manual curation~\cite{mathew2018analyzinghatecounterspeech} or supervised datasets limits scalability. Advances in LLMs and VLMs~\cite{ghosh2024exploringfrontiervisionlanguagemodels} have improved intervention capabilities but often lack contextual grounding, requiring knowledge-driven approaches~\cite{dong2024surveyincontextlearning}. To address this, MemeGuard enhances meme interpretation using VLMs and knowledge ranking, enabling more precise and contextually relevant interventions~\cite{jha2024memeguardllmvlmbasedframework}.