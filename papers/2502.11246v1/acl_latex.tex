% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[]{acl}

% Standard package includes
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for othThe source code and dataset are available at:er character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{paralist}
% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% \usepackage{multirow}
% \usepackage{placeins}
% %\usepackage[dvipsnames]{xcolor}
% % Standard package includes
% \usepackage{times}
% \usepackage{latexsym}
% %\usepackage[svgnames]{xcolor}
% \setlength{\tabcolsep}{2pt}
% % For proper rendering and hyphenation of words containing Latin characters (including in bib files)
% \usepackage[T1]{fontenc}
% % For Vietnamese characters
% % \usepackage[T5]{fontenc}
% % See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets
% \usepackage{color}
% \usepackage{tabularx}
% % This assumes your files are encoded as UTF8
% \usepackage[utf8]{inputenc}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{url}
\usepackage{wrapfig,booktabs}
\usepackage{newtxtext} % you want true small caps
\usepackage{lettrine}
\usepackage{colortbl}
% \usepackage{xcolor}
\usepackage{array}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{mdframed}
\setlength{\tabcolsep}{2pt}
% \setlength\intextsep{18pt}
\usepackage{anyfontsize}
\usepackage{tcolorbox}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{subcaption}
\usepackage{color, soul}
\usepackage{tcolorbox} % Ensure this package is included
\usepackage{twemojis}
% \usepackage{tikz}

\usetikzlibrary{shadows}
\usepackage{pgfplots}
\usetikzlibrary{shapes.geometric}
\newcommand{\am}[1]{\textcolor{red}{#1--AM}}
\newcommand{\rh}[1]{\textcolor{brown}{#1--RH}}
\newcommand{\snb}[1]{\textcolor{blue}{#1--SNB}}
\newcommand{\warningsign}{\tikz[baseline=-.75ex] \node[shape=regular polygon, regular polygon sides=3, inner sep=0pt, draw, thick] {\textbf{!}};}

\pgfplotsset{compat=1.18}
\newmdenv[
  topline=false,
  bottomline=false,
  skipabove=\topsep,
  skipbelow=\topsep,
  leftline=true,
  rightline=true,
  linecolor=gray,
  linewidth=2pt,
  innertopmargin=5pt,
  innerbottommargin=5pt,
  innerrightmargin=5pt,
  innerleftmargin=5pt,
  backgroundcolor=gray!10,
  roundcorner=10pt
]{stylishframe}

%\usepackage{dutchcal}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{\textsc{MemeSense}: An Adaptive In-Context Framework for\\ Social Commonsense Driven Meme Moderation

\leavevmode\\
{\begin{center}           
    \small                                 
    \textcolor{red}{\warningsign DISCLAIMER: This paper features memes that some readers may find vulgar/offensive/hateful.}
\end{center}      
}}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

\author{Sayantan Adak$^\dagger$, Somnath Banerjee$^\dagger$, Rajarshi Mandal$^\dagger$, Avik Halder$^\dagger$, \textbf{Sayan Layek}$^\dagger$\textbf{,}\\ \textbf{Rima Hazra}$^\ddagger$\textbf{, }\textbf{Animesh Mukherjee}$^\dagger$  \\\\
$^\dagger$ Indian Institute of Technology Kharagpur\\
$^\ddagger$ INSAIT, Sofia University ``St. Kliment Ohridski''
}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\maketitle
\begin{center}
    \vspace{-1pt}
    \includegraphics[width=\textwidth]{images/meme1.pdf}
    \captionof{figure}{\label{fig:representative_examples_different_meme}{Memes can manifest harm in different ways, some rely solely on imagery to convey implicit messages, while others reinforce harm through accompanying text. This figure illustrates the three primary categories: \textbf{(a)} harmful memes without text, \textbf{(b)} harmful memes with text, and \textbf{(c)} non-harmful memes. Prior moderation efforts have disproportionately focused on text-based harmful memes, often overlooking the nuanced and context-dependent nature of purely visual memes. To address this gap, we propose \textsc{\textbf{MemeSense}}, an adaptive in-context learning framework that fuses social commonsense reasoning with visually and semantically related reference examples.}
    \vspace{10mm}
    }
\end{center}
}]
\begin{abstract}
Memes present unique moderation challenges due to their subtle, multimodal interplay of images, text, and social context. Standard systems relying predominantly on explicit textual cues often overlook harmful content camouflaged by irony, symbolism, or cultural references. To address this gap, we introduce \textsc{MemeSense}, an adaptive in-context learning framework that fuses social commonsense reasoning with visually and semantically related reference examples. By encoding crucial task information into a learnable cognitive shift vector, \textsc{MemeSense} effectively balances lexical, visual, and ethical considerations, enabling precise yet context-aware meme intervention. Extensive evaluations on a curated set of implicitly harmful memes demonstrate that \textsc{MemeSense} substantially outperforms strong baselines, paving the way for safer online communities.\footnote{Code and data available at: \url{https://github.com/sayantan11995/MemeSense}}
\end{abstract}

\section{Introduction}

% Ensuring online safety requires robust detection mechanisms that can accurately assess multimodal content, particularly memes, where harmful intent is often embedded through implicit cues rather than explicit statements. Traditional moderation systems struggle with this challenge, as they either rely too heavily on text-based analysis or fail to capture the nuanced interplay between images and language. Even large-scale transformer models like GPT-4o, Gemini 2.0, Qwen 2.5 exhibit performance degradation when dealing with purely visual memes, lacking the ability to infer risk without explicit textual guidance. On the other hand, smaller, efficiency-driven models—while suitable for real-time deployment—frequently misclassify or overlook harmful intent due to their limited contextual reasoning. This gap necessitates advanced frameworks that not only detect unsafe content with high precision but also generate safe interventions that mitigate harm proactively.

% Despite the growing emphasis on multimodal fusion, many recent approaches continue to exhibit a text-centric bias, often treating the visual component of memes as secondary or merely supplementary. For instance, Dwivedi et al. (2023) showed that image regions are frequently neglected during cross-attention, leading to substantial performance drops when the harmful message is conveyed via subtle symbolic cues or cryptic sarcasm [2]. This imbalance becomes particularly evident in sarcastic or ambiguous memes, where the textual overlay might appear harmless or unrelated, yet the underlying visual context (e.g., a historical symbol or culturally coded reference) reveals the true derogatory intent (Chang and Chen, 2023) [3]. Even state-of-the-art pipelines leveraging contrastive language-image pre-training models (like CLIP) falter when faced with image-driven humor that requires human-level commonsense or nuanced cultural knowledge to decode (Cao and Shwartz, 2023) [4]. As a result, the detection task often fails if the visual subtext carries the bulk of the hateful or harmful message, illustrating the critical need for robust visual reasoning and cultural grounding to complement textual analysis in meme understanding.

% A promising direction to overcome these shortcomings is to inject human commonsense into the model’s reasoning process, allowing it to interpret cryptic or satirical memes at a level comparable to human cognition. Recent work in in-context learning (Brown et al., 2020) has shown that large language models can adapt to new tasks or nuanced contexts by referencing a few carefully selected examples during inference. However, porting this technique to vision-language settings is non-trivial, as embedding entire sets of related images alongside the “test” meme quickly surpasses token limits and computational budgets (Alayrac et al., 2022) [1]. In response, Chang and Li (2023) proposed a learnable Image Context Vector (ICV) mechanism: rather than inserting all reference images explicitly, they encode relevant visual patterns into low-dimensional embeddings that act as “virtual tokens” within the multimodal transformer [2]. These ICVs retain the semantic essence of in-context examples—such as specific visual cues, symbols, or stylistic markers—without demanding a prohibitive increase in input length. Experiments show that combining human-like commonsense knowledge (via external knowledge graphs or curated textual snippets) with ICV-based in-context learning significantly improves detection rates for cryptic and sarcastic memes that previously confounded purely text-centric or visually naive models (Sahu et al., 2023) [3]. By selectively retrieving analogous memes from a database of known harmful patterns and compressing their crucial features into ICVs, the system can seamlessly extend its inference to intricate cultural references or historical symbols that would otherwise remain opaque—even to large-scale multimodal transformers.

% In this contrst, we propose an adaptive in-context learning framework for multimodal meme moderation, overcoming the text-centric bias and computational inefficiencies of existing methods. By distilling multimodal patterns into compact latent representations, our approach enables models to infer implicit harm without extensive context windows. Integrating commonsense reasoning and dynamic retrieval of relevant cases, it enhances visual-text alignment to detect cryptic, sarcastic, or culturally coded content. This allows real-time interventions like content-aware blurring, rephrasing, or counter-messaging, ensuring safer digital environments while maintaining efficiency and interpretability.

% Ensuring online safety requires robust detection mechanisms capable of accurately assessing multimodal content, particularly memes, where harmful intent is often embedded through implicit cues rather than explicit statements. Traditional moderation systems struggle with this challenge, relying either too heavily on text-based analysis or failing to capture the nuanced interplay between images and language. Even large-scale transformer models such as GPT-4o, Gemini 2.0, and Qwen 2.5 exhibit performance degradation when handling purely visual memes, as they lack the ability to infer risk without explicit textual guidance. Meanwhile, smaller efficiency-driven models—though suitable for real-time deployment—frequently misclassify or overlook harmful intent due to constrained contextual reasoning and insufficient pretraining on diverse multimodal data. This gap necessitates advanced frameworks that not only detect unsafe content with high precision but also generate safe interventions to proactively mitigate harm.\\
% Existing multimodal approaches often exhibit a text-centric bias, treating the visual component as supplementary. Cross-attention mechanisms tend to neglect image regions, reducing accuracy when harmful messages are conveyed through symbolic cues or sarcasm (Dwivedi et al., 2023). This issue is amplified in ambiguous memes, where text overlays obscure underlying visual references—such as historical symbols or culturally coded imagery—that reveal derogatory intent (Chang and Chen, 2023). Even contrastive language-image models like CLIP struggle with humor or satire that demands human-like commonsense reasoning (Cao and Shwartz, 2023). A key limitation is the absence of structured visual-symbolic embeddings, causing failures against adversarial visual distortions and stylistic modifications. Addressing this requires integrating robust visual reasoning and cultural grounding into multimodal models.\\
% A promising approach is enhancing models with human-like commonsense reasoning to interpret cryptic or satirical memes. In-context learning (Brown et al., 2020) allows models to adapt to nuanced cases using few-shot examples, but scaling this to vision-language tasks is computationally expensive (Alayrac et al., 2022). To overcome this, Chang and Li (2023) introduced Image Context Vectors (ICVs), which encode relevant visual patterns as compact embeddings, reducing input size without losing semantic detail. Incorporating ICVs with dynamic retrieval systems enhances detection by leveraging a database of known harmful patterns, allowing models to generalize across cultural references and historical symbols (Sahu et al., 2023). Additionally, adversarial training and contrastive fine-tuning improve robustness against evasive modifications.\\
% We propose an adaptive in-context learning framework for multimodal meme moderation that mitigates text bias and computational inefficiencies. By compressing multimodal patterns into compact latent representations, our approach enables models to infer implicit harm without extensive context. Integrating commonsense reasoning, adversarially robust retrieval, and structured multimodal knowledge representations, it strengthens visual-text alignment to detect cryptic, sarcastic, or culturally coded content. This framework enables real-time interventions such as content-aware blurring, rephrasing, and counter-messaging, ensuring safer digital environments with efficiency and interpretability.

Memes have emerged as a powerful form of online expression, where seemingly lighthearted humor can conceal offensive, derogatory, or culturally charged subtexts. Their multimodal nature combining images, text, and symbolism poses significant hurdles for content moderation systems, especially those built primarily around textual analysis~\cite{10494986,10191363,jha-etal-2024-meme, jha2024memeguardllmvlmbasedframework}. Large vision-language models (VLMs), including GPT-4o~\cite{openai2024gpt4ocard}, Gemini 2.0~\cite{geminiteam2024geminifamilyhighlycapable}, and Qwen 2.5~\cite{qwen2025qwen25technicalreport}, often show reduced accuracy on image-centric memes precisely because they depend heavily on overt text clues~\cite{sharma-etal-2023-memex, agarwal2024mememqamultimodalquestionanswering}. In contrast, humans effortlessly parse memes by applying commonsense reasoning and recalling mental examples of similar situations. This can be attributed to the \textit{social commonsense}~\cite{Naslund2020,arora23,SurgeonGeneral2023}\footnote{\url{https://en.wikipedia.org/wiki/Commonsense_reasoning}} capabilities of humans which include \textit{recognizing social norm violations} (e.g., hate speech, body shaming, misogyny, stereotyping, sexual content, vulgarity), \textit{assessing credibility} (e.g., misinformation), \textit{empathy and ethical judgment} (e.g., child exploitation, public decorum and privacy, cultural sensitivity, religious sensitivity), \textit{contextual interpretation} (e.g., humor appropriateness), and \textit{predicting consequences} (e.g., mental health impact, violence, substance abuse). This human-like capacity to interpret subtle or symbolic cues underscores the need for moderation frameworks that can replicate such higher-level reasoning rather than relying purely on text or raw pixels.\\
Early multimodal models have attempted to fuse vision and language through joint embeddings or cross-attention mechanisms~\cite{shin-narihira-2021-transformer}, yet they tend to place disproportionate emphasis on textual data. As a result, subtle image-based cues -- such as historical references, cultural icons, or visually encoded irony -- can slip through the cracks~\cite{zhang2024visionlanguagemodelsvisiontasks}. %For instance, a meme might merge a culturally significant image with deliberately misleading text, creating hidden meanings that text-focused systems fail to detect. 
Even powerful contrastive models like CLIP~\cite{radford2021learningtransferablevisualmodels} struggle when the meme’s intent hinges on satire or understated visual hints requiring commonsense inference~\cite{mazhar2025figurativecumcommonsenseknowledgeinfusionmultimodal}. These shortcomings highlight the urgent need for more holistic approaches that view images and text on equal footing, mirroring the way humans naturally interpret visual jokes and symbolic content.\\
A promising direction involves enriching model understanding through in-context examples~\cite{liu2024incontextvectorsmakingcontext} that illuminate both visual and textual nuances of a meme. Rather than processing an image in isolation, the model compares it against a small set of similar or thematically related images each annotated or tagged with the relevant commonsense insights needed for proper interpretation. This strategy enables the model to draw parallels and detect patterns that might be missed if it were forced to rely on a single snapshot or textual prompt. By dynamically retrieving these curated examples alongside knowledge about harmful or deceptive imagery, the system gains the contextual backdrop necessary to catch everything from historical allusions to subtle visual sarcasm.\\
In this paper, we propose an adaptive in-context learning framework -- \textsc{MemeSense} that synthesizes commonsense knowledge with semantically similar reference images to enhance the interpretation of meme content. Concretely, \textsc{MemeSense} retrieves a curated set of analogous memes, each annotated with cultural, historical, or situational context and incorporates these examples into a unified representation alongside the target meme. By embedding human-like commonsense cues directly into the model’s input, we effectively steer its latent space toward the pertinent visual and textual signals present in the attached memes. This synergy allows the model to detect subtle or symbolic markers such as ironic juxtapositions, culturally coded imagery, or sarcastic overlays that often evade traditional pipelines. Our contributions are as follows.
\begin{compactitem}
    \item We develop a novel multi-staged framework to generate intervention for the harmful memes by leveraging cognitive shift vectors which reduce the requirement of demonstration examples during inference. 
    \item We curate a wide-ranging dataset collection that emphasizes subtly harmful or text-scarce memes, filling a crucial gap in moderation research. This dataset lays the groundwork for a deeper exploration of nuanced meme analysis.
    \item Rigorous experiments demonstrate the efficacy of \textsc{MemeSense} even for the memes that do not contain any explicit text embedded in them as is usually the case. We obtain respectively 5\% and 9\% improvement in BERTScore over the most competitive baseline for the \textit{memes with text} and the \textit{memes without text}. Semantic similarity for memes with as well as without text (almost) doubles for \textsc{MemeSense} compared to the best baseline. 
\end{compactitem}



\section{Related work}

\noindent\textbf{Visual in-context learning}: In-context learning (ICL) has transformed LLM adaptation, enabling task generalization with few-shot demonstrations, and recent advancements have extended it to multimodal models for vision-language tasks like VQA~\cite{brown2020languagemodelsfewshotlearners, alayrac2022flamingovisuallanguagemodel}. However, ICL suffers from computational inefficiency due to long input sequences and sensitivity to demonstration selection~\cite{peng2024livelearnableincontextvector}. To mitigate this, in-context vectors (ICVs) distill task-relevant information into compact representations, reducing the need for multiple examples~\cite{hendel2023incontextlearningcreatestask, todd2024functionvectorslargelanguage}. While early non-learnable ICVs improved NLP efficiency, they struggled with multimodal tasks like VQA due to diverse vision-language inputs~\cite{li2023configuregoodincontextsequence, yang2024exploringdiverseincontextconfigurations}. Recently, learnable ICVs dynamically capture essential task information, significantly enhancing VQA performance while lowering computational costs~\cite{peng2024livelearnableincontextvector}. These advancements underscore the importance of optimizing vector-based representations and refining ICL strategies to improve multimodal reasoning~\cite{Yin_2024}.\\
% In-context learning (ICL) has revolutionized adaptation in LLMs by enabling task generalization through a few example demonstrations, and recent advancements have extended this capability to large multimodal models for vision-language tasks like visual question answering (VQA)~\cite{brown2020languagemodelsfewshotlearners, alayrac2022flamingovisuallanguagemodel}. However, ICL in LMMs faces challenges such as computational inefficiency due to long input sequences and sensitivity to demonstration selection~\cite{peng2024livelearnableincontextvector}. To address these, researchers have introduced in-context vectors (ICVs), which distill task-relevant information into a compact vector, reducing the need for multiple examples at inference time~\cite{hendel2023incontextlearningcreatestask, todd2024functionvectorslargelanguage}. While early non-learnable ICVs improved efficiency in NLP tasks, they struggled with complex multimodal tasks like VQA due to the diverse nature of vision-language inputs~\cite{li2023configuregoodincontextsequence, yang2024exploringdiverseincontextconfigurations}. Recently, Learnable In-Context Vectors have been proposed to dynamically capture essential task information, significantly enhancing VQA performance while reducing computational cost~\cite{peng2024livelearnableincontextvector}. These advances indicate that optimizing vector-based representations and refining ICL strategies in LMMs remain crucial for improving multimodal reasoning~\cite{Yin_2024}.
\textbf{Intervention generation}: Intervention strategies for online toxicity have largely focused on text-based issues like hate speech~\cite{qian-etal-2019-benchmark, jha2024memeguardllmvlmbasedframework}, misinformation~\cite{10.1145/3543507.3583388} and harm~\cite{banerjee2024safeinfercontextadaptivedecoding, hazra2024safetyarithmeticframeworktesttime, banerjee2025navigatingculturalkaleidoscopehitchhikers}, with limited exploration of multimodal challenges such as memes. While counterspeech interventions reshape discourse~\cite{SchiebGoverningHS}, their reliance on manual curation~\cite{mathew2018analyzinghatecounterspeech} or supervised datasets limits scalability. Advances in LLMs and VLMs~\cite{ghosh2024exploringfrontiervisionlanguagemodels} have improved intervention capabilities but often lack contextual grounding, requiring knowledge-driven approaches~\cite{dong2024surveyincontextlearning}. To address this, MemeGuard enhances meme interpretation using VLMs and knowledge ranking, enabling more precise and contextually relevant interventions~\cite{jha2024memeguardllmvlmbasedframework}.
\section{Methodology}
\begin{figure*}[t]
\centering
\scriptsize
\includegraphics[width=0.90\textwidth]{images/Presentation1345.pdf}
\caption{\footnotesize Schematic diagram of the \textsc{MemeSense}.}
\label{fig:main}
\vspace{-0.3cm}
\end{figure*}
In this work, we propose a framework that proceeds in three main stages -- (a) \textbf{Stage I: Generation of commonsense parameters}: In Stage I, we generate commonsense parameters by instruction-tuning a multimodal large language model (MLLM) to predict contextually relevant insights for each image. (b) \textbf{Stage II: Selection of in-context exemplars}: We create a set of anchor images and retrieve corresponding in-context exemplars, which we later use in Stage III. (c) \textbf{Stage III: Learning cognitive shift vector}: Finally, we learn a cognitive shift vector by distilling general task information from the exemplars, and then guide the target model to align its representation with the insights derived from these exemplars. The overview of our proposed method is shown in Figure~\ref{fig:main}.
% Our framework consists of three stages -- (a) \textbf{Generating commonsense parameters:} In this stage, we instruction tune a multimodal large language model (MLLM) to predict the commonsense parameters of an images. (b) \textbf{Selection of incontext exemplars}: In this section, we prepare a in-context set consists of anchor images and their retrieved set of incontext exemplars further to use them in Stage III. (c) \textbf{Learning cognitive meme vector}: In this stage, we aim to obtain an abstract general task information from the exemplars and learn a cognitive meme vector. Further, we enable the target model to shift the represent towards the direction influenced by the examplars. The overview of our proposed method is shown in Figure~\ref{fig:main}.
% This approach aims to abstract general task information
%  from demonstrations ,enabling it to shift the model’s representation toward the direction influenced
%  by the ICDs.

\section{Preliminaries}

A collection of images is denoted as $\mathcal{IMG}$, where each image $img$ is an item of $\mathcal{IMG}$, i.e., $img \in \mathcal{IMG}$. $GT_{img}$ describes the ground truth intervention on the image. In particular, $GT_{img}$ contains the description about \textit{\textbf{why the image can/can't be posted on social media?}}
We consider a set of commonsense parameters $\mathscr{C}$ where $i^{th}$ commonsense parameter is denoted as $c_i \in \mathscr{C}$. A pair consisting of an image and its corresponding commonsense parameters is denoted by $\langle img,\mathscr{C}_{img} \rangle$ where $\mathscr{C}_{img}\subseteq\mathscr{C}$. An image may be associated with multiple commonsense parameters.
We partition $\mathcal{IMG}$ into two subsets: \textbf{(a)} the training set $\mathcal{IMG}_{tr}$, used at different stages of the training process, and \textbf{(b)} the test set $\mathcal{IMG}_{ts}$, reserved for evaluation. The set of training images $\mathcal{IMG}_{tr}$ and test images $\mathcal{IMG}_{ts}$ are disjoint, i.e., $\mathcal{IMG}_{tr} \cap \mathcal{IMG}_{ts} = \emptyset$.\\
\noindent For \textbf{Stage I}, we build a training dataset $\mathcal{D}_{\mathscr{C}}$ consisting of images $\mathcal{IMG}_{tr}$ and their respective ground truth image description with commonsense parameters. We represent a fine-tuned vision language model with dataset $\mathcal{D}_{\mathscr{C}}$ as $\mathcal{M}_{\mathscr{C}}$. 
Further in \textbf{Stage II}, we construct an in-context (IC) learning set $\mathcal{D}_{\mathcal{IC}}$ (involves only images from $\mathcal{IMG}_{tr}$ set) to utilize in \textbf{Stage III} (see Section~\ref{sec:stageIII}). Each instance in $\mathcal{D}_{\mathcal{IC}}$ is a tuple consisting of $\langle img_a, IC_{img}, GT_{img_a} \rangle$ where $IC_{img}$ is the set of retrieved in-context examples of an anchor image $img_a$. Each in-context example consists of an image $img \neq img_a$, $\mathscr{C}_{img}$, $GT_{img}$.
We define the cognitive shift vector set as $\mathcal{CSV}$ and the coefficient set as $\alpha$. In \textbf{Stage III}, we use an instruction following MLLM as the target model ($\mathcal{M}$) to further generate the intervention defined as $\mathcal{M}_{ivt}$.

% * Vulgarity: Use of obscene language, inappropriate tone, or implied innuendos.
% * Violence: Depictions of harm, coercion, or references to physical aggression.
% * Sexual Content: Explicit or suggestive imagery, innuendos, or nudity.
% * Hate Speech: Insulting, harassing, or offensive content targeting individuals or groups.
% * Body Shaming: Objectification, fat-shaming, or insensitive references to disabilities.
% * Cultural Sensitivity: Stereotypes, appropriation, or misrepresentation of cultural elements.
% * Mental Health Impact: Content that manipulates emotions, triggers trauma, or exploits grief.
% * Humor Appropriateness: Use of humor in a way that mocks, belittles, or distorts serious topics.
% * Misinformation: Spreading misleading narratives, false claims, or deceptive dialogue.
% * Substance Abuse: References to drugs, alcohol, or promoting substance use.
% * Child Exploitation: Content that inappropriately depicts or exploits children.
% * Public Decorum & Privacy: Violations of privacy, public sensitivity, or personal reputation concerns.
% * Stereotyping: Generalizations that reinforce racial, gender, or societal biases.
% * Misogyny: Content promoting gender-based discrimination, sexism, or demeaning women.
% * Religious Sensitivity: Content that disrespects religious beliefs, symbols, or historical context.
\subsection{Stage I: Commonsense parameters}
\label{sec:stageI}
% In this stage, our objective is to train a vision language model ($\mathcal{M}_{\mathcal{C}}$) given images as an input and commonsense as an output.
In this stage, we aim to fine-tune a vision-language model to produce relevant commonsense parameters for meme images. These parameters represent broad conceptual categories that help assess whether an image is \textit{harmful}, \textit{offensive}, or \textit{inappropriate}, as discussed in \cite{arora23,SurgeonGeneral2023,gongane2022detection}. 
To create the training set $\mathcal{D}_{\mathscr{C}}$, we first use GPT-4o to automatically obtain commonsense parameters for $img \in \mathcal{IMG}_{tr}$ and then perform manual corrections. To evaluate GPT-4o’s performance, we randomly select 40\% of its responses and subject them to expert review by two professionals experienced in meme moderation. Their assessments revealed a strong consensus, with GPT-4o’s outputs (96\% agreement). Lastly, for the fine-tuning phase, we chose an open-weight model instead of a proprietary system such as GPT-4o, given the higher resource demands and stricter usage limitations. We employ the prompt in Table~\ref{tab:prompt_obtaining_commonsesne} to extract commonsense parameters from GPT-4o. 
% \begin{mdframed}[frametitle={Prompt for instruction tuning}]
% [INST] <image>\textbackslash nDescribe the meme and assess whether the meme has the potential to be perceived as     vulgar, harmful, or unethical using specific commonsense parameters. [\textbackslash INST]
% \end{mdframed}

\noindent The example of commonsense parameters $\mathscr{C}_{img}$ of an image $img$ is illustrated in Figure~\ref{fig:meme_commonsense_example}. 
By instruction tuning with this dataset $\mathcal{D}_{\mathscr{C}}$, we obtain the final vision-language model \(\mathcal{M}_\mathscr{C}\), which we then use at inference time to produce commonsense parameters for new meme images. 

%During instruction tuning, we utilize the prompts provided above.

\newmdenv[
    backgroundcolor=gray!10, % Light gray background
    frametitlebackgroundcolor= gray!30,
    linecolor=black,         % Border color
    roundcorner=5pt,         % Rounded corners
    frametitle={\textbf{Prompt for instruction tuning}}, % Bold title
    frametitlefont=\normalfont\bfseries, % Title font
    frametitlerule=true,     % Adds a horizontal rule under the title
    frametitlerulewidth=0.5pt, % Thickness of the rule
    frametitleaboveskip=5pt,  % Space between title and rule
    skipabove=10pt, % Space above the box
    skipbelow=10pt, % Space below the box
    innertopmargin=12pt, % Increased space between title and content
    innerbottommargin=8pt, % Inner bottom margin
    innerleftmargin=10pt, % Inner left margin
    innerrightmargin=10pt % Inner right margin
]{custombox}

\begin{custombox}
\text{[INST]} <image>\textit{Describe the meme and assess whether the meme has the potential to be perceived as vulgar, harmful, or unethical using specific commonsense parameters.} [\textbackslash INST]
\end{custombox}
\vspace{-0.3cm}
\begin{figure}[t]
\centering
\scriptsize
\includegraphics[width=0.49\textwidth]{images/meme.pdf}
\caption{\footnotesize Representative example of a harmful meme and the annotated commonsense parameters along with intervention.}
\label{fig:meme_commonsense_example}
\vspace{-0.3cm}
\end{figure}


% In this stage, our objective is to fine-tune a vision language model to generate related commonsense given a meme image $img$. \textcolor{blue}{Commonsense parameters refer to broad conceptual categories that help in systematically evaluating whether a meme (or any online content) is harmful, offensive, or inappropriate. These categories are influenced from \cite{arora23,SurgeonGeneral2023, gongane2022detection}. }
% ~\textcolor{red}{@Sayantan, give a 1-2 lines about what do you mean by commonsense parameter for a meme? Any grounding or any proper definition we used?}
% We train a vision-language model ($\mathcal{M}_{\mathcal{C}}$) to take images as input and generate commonsense as output. To prepare the dataset $\mathcal{D}_{\mathscr{C}}$, we first automatically annotate the commonsense parameters of each image $img \in \mathcal{IMG}_{tr}$ using GPT-4 followed by a manual correction if there is any erroneous label.
% ~\textcolor{red}{Sayantan, add citation, like on what ground you consider the GPT-4 annotations/common sense parameters? We need to say why don't we directly use GPT4 -- may comment that cost incur/ open weight models are easy to use}. 
% \textcolor{blue}{We rely on training an open-source model for generating the commonsense parameters primarily because deploying a proprietary model like GPT-4 for large-scale annotation can be resource-intensive and may involve usage limitations. Training an open-source model can offer a more scalable and cost-effective solution.} The prompt used for gathering the commonsense parameters of the image is presented in Table~\ref{tab:prompt_obtaining_commonsesne}.
% Further, during the fine-tuning of MLLM, we use the below prompt.
% \begin{mdframed}[]
% [INST] <image>\textbackslash nDescribe the meme and assess whether the meme has the potential to be perceived as     vulgar, harmful, or unethical using specific commonsense parameters. [\textbackslash INST]
% \end{mdframed}
% Once we obtain the fine-tuned MLLM $\mathcal{M}_{\mathscr{C}}$ to utilize in inference time.  During the inference, we use model $\mathcal{M}_{\mathscr{C}}$ to obtain the commonsense parameters for test time images.
% out of 484 instances from the collected set we use 300 randomly selected instances for our training.

% In order to train the model we first obtain the the  consists of images   \textcolor{red}{Commonsense details}
% \textcolor{red}{Sayantan, add the prompt you used for obtaining the commonsense. What is the prompt used during training? How many instances you have used during training? [some of them may go to appendix]}\\
% \textcolor{blue}{We use the final prompt representaed in Table~\ref{tab:prompt_obtaining_commonsesne} to GPT-4 to obtain the commonsense parameters for the memes}

\subsection{Stage II: Selection of in-context exemplars}
\label{sec:stageII}
In this stage, our objective is to create an in-context dataset \(\mathcal{D}_{IC}\) that provides exemplars to guide the latent space of the target model in \textbf{Stage III}. To accomplish this, we reuse the training images \(\mathcal{IMG}_{tr}\) and, following the authors in~\cite{peng2024livelearnableincontextvector}, treat each image \(img \in \mathcal{IMG}_{tr}\) as an anchor. We denote an anchor image as $img_{a}$. We then select \(k\) in-context examples from \(\mathcal{IMG}_{tr} \setminus img_a\) using multiple strategies.  
First, we randomly sample \(k\) candidate images to construct the set \(IC_{img}\) for each anchor. Apart from random selection, we also leverage semantic retrieval techniques that consider commonsense parameters, image representations, or a combination of both. The detailed setup of in-context retrieval is given in Section~\ref{sec:experimental_setup}.
%These approaches help ensure that the chosen exemplars are contextually aligned with the anchor image.
\if{0}\noindent \textit{Retrieval strategies for in-context images} ($IC_{img}$): Given an anchor image $img_a$,
its corresponding annotated commonsense parameters $\mathscr{C}_{img_a}$, we iteratively retrieve at least one instance per parameter from the candidate images $\mathscr{C}_{img} \text{ where } img \in \mathcal{IMG}_{tr}\setminus img_a$. Further, we incorporate similarity-based retrieval by computing the semantic similarity between the anchor image \( img_a \) and the candidate images \( img \in \mathcal{IMG}_{tr} \setminus img_a \). Given an anchor image $img_a$, we consider top $k$ candidate images for $IC_{img_a}$ set. The detailed setup of in-context retrieval is given in Section~\ref{sec:experimental_setup}.\fi
% \noindent \textit{Commonsense-Based Retrieval:}  
% For each predefined commonsense parameter, we select up to five instances from our training set to form a lookup set. Given an anchor image $img$ and its corresponding annotated commonsense parameters, we iteratively retrieve at least one instance per parameter to construct the $k$ demonstration examples. \\ 
% \noindent \textit{Image-Based Retrieval:}  
% For a given anchor image $img$, we select $k$ demonstrations by computing semantic similarity between $img$ and the images in the training subset. We first encode all images into dense vector representations using the CLIP-ViT multimodal embedding model. When an anchor image is provided as a query, we map it into the same vector space. We then perform Approximate Nearest Neighbor (ANN) search to identify the top $k$ most similar images. Their corresponding commonsense parameters and ground truth interventions are retrieved as in-context examples. \\
% \noindent \textit{Combined retrieval:} We also experiment with constructing the $k$ in-context demonstrations by combining the above two approaches. Here, we select $c$ instances from the commonsense based retrieval and $(k-c)$ instances from the image based retrieval, and the $c$ varies $\in \{1,2\}$.
%--------------------------Need TO MOVE-----
% In this stage, our objective is to prepare the in-context dataset $\mathcal{D_{IC}}$ with in-context exemplars which further guide the latent space of the target model in Stage III.
% We employ the same training images $\mathcal{IMG}_{tr}$ to construct the incontext dataset $\mathcal{D}_{\mathcal{IC}}$. Following~\cite{}~\rh{Sayantan, cite LIVE paper here}, we choose each image $img \in \mathcal{IMG}_{tr}$ as an anchor and selecting the $k$ in-context examples from $\mathcal{IMG}_{tr} \setminus img$. We incorporate various strategies to select the $k$ in-context examples $\mathcal{IMG}_{tr} \setminus img$.\\
% First, we begin by randomly selecting $k$ examples to construct $IC_{img}$, where $k$, from the training images $\mathcal{IMG}_{tr} \setminus img$, ensuring that the anchor image is excluded from the selection process.
% Beyond random sampling, we also employ several semantic retrieval methods to select more contextually relevant in-context exemplars. During the retrieval of incontext examples, we utilize commonsense parameters, image representation and combination of both of them to retrieve relevant examples.
% Our goal is to enhance the retrieval process such that the model's representation is influenced by in-context demonstrations (ICDs) that are more topically coherent, thereby improving the overall effectiveness of the intervention generation.
%\textcolor{blue}{
% We select in-context examples $IC_{img}$ from our training subset, which consists of tuples of the form $\langle img, IC_{img}, GT_{img} \rangle$. We employ several approaches to select in-context examples to construct $\mathcal{D_{IC}}$. 
% In the original LICV framework, the authors employed a random sampling strategy to select in-context examples from their training subset for the Visual Question Answering (VQA) task. To align with this approach, we experiment with selecting in-context examples $IC_{img}$ from our training subset, which consists of tuples of the form $\langle img, IC_{img}, GT_{img} \rangle$. \\ 
% First, we begin by randomly selecting $k$ examples to construct $IC_{img}$, where $k \in \{1,2,4\}$, from the dataset $\mathcal{D}_{\mathcal{IC}} \setminus img$, ensuring that the target image is excluded from the selection process.  \\
% Beyond random sampling, we also employ several semantic retrieval methods to select more contextually relevant in-context demonstrations. Our goal is to enhance the retrieval process such that the model's representation is influenced by in-context demonstrations (ICDs) that are more topically coherent, thereby improving the overall effectiveness of the intervention generation. \\
% \paragraph{Retrieval of In-Context Images.} We, first utilize the annotated commonsense parameters to select the in-context demonstrations. Then we consider image similarity based retrieval to construct the in-context demonstration.  \\
% \noindent \textit{Commonsense-Based Retrieval:}  
% For each predefined commonsense parameter, we select up to five instances from our training set to form a lookup set. Given an anchor image $img$ and its corresponding annotated commonsense parameters, we iteratively retrieve at least one instance per parameter to construct the $k$ demonstration examples. \\ 
% \noindent \textit{Image-Based Retrieval:}  
% For a given anchor image $img$, we select $k$ demonstrations by computing semantic similarity between $img$ and the images in the training subset. We first encode all images into dense vector representations using the CLIP-ViT multimodal embedding model. When an anchor image is provided as a query, we map it into the same vector space. We then perform Approximate Nearest Neighbor (ANN) search to identify the top $k$ most similar images. Their corresponding commonsense parameters and ground truth interventions are retrieved as in-context examples. \\
% \noindent \textit{Combined retrieval:} We also experiment with constructing the $k$ in-context demonstrations by combining the above two approaches. Here, we select $c$ instances from the commonsense based retrieval and $(k-c)$ instances from the image based retrieval, and the $c$ varies $\in \{1,2\}$
%}
\subsection{Stage III: Learning cognitive shift vectors}
\label{sec:stageIII}
%After obtaining the dataset $\mathcal{D_{IC}}$ 
In this stage, the aim is to learn the trainable shift vector set $\mathcal{CSV}$ and coefficient set $\alpha$ so that the target model can generate proper intervention given an image $img$. We initialize a set of shift vectors $\mathcal{CSV} = \{csv^1, csv^2, \ldots, csv^L\}$ where each shift vector $csv^{\ell}$ corresponds to each layer $\ell \in L$ in the target model $\mathcal{M}$. $L$ represents the number of layers in target model $\mathcal{M}$. Further, we consider a set of coefficients $\alpha = \{\alpha^1, \alpha^2, \ldots, \alpha^{L}\}$ which regulate the impact of these cognitive shift vectors across different layers in $\mathcal{M}$. After applying cognitive shift vector set $\mathcal{CSV}$ and $\mathcal{\alpha}$ to the model $\mathcal{M}$, we obtain the final model as expressed in Equation~\ref{eq:live}.
\begin{equation}
\label{eq:live}
    \mathcal{M}_{ivt}^{\ell} = M^{\ell} + \alpha^{\ell} \cdot csv^{\ell}, 
    %\quad v_l \in \mathbb{R}^{1 \times d}, \quad \alpha_l \in \mathbb{R}.
\end{equation}
\noindent Following task analogies from~\cite{peng2024livelearnableincontextvector}, our objective is to align the output of $\mathcal{M}_{ivt}$ with the output obtained by including $IC_{img}$ in model $\mathcal{M}$ for a given anchor image $img_{a}$. To achieve this, we minimize the KL divergence between the output distribution of $\mathcal{M}_{ivt}(img_a)$ and output distribution of $\mathcal{M}$ with IC exemplars $IC_{img}$ for the anchor image $img_{a}$. The computation of $\mathscr{L}_{od}$ is given in Equation~\ref{eq:kldiv}.
\begin{equation}
\label{eq:kldiv}
    % \mathscr{L}_{od} = KL\left( P(img_a | IC_{img}; \mathcal{M}) \parallel P(img_a | \mathcal{M}_{ivt}) \right).
\begin{aligned}
    \mathscr{L}_{od} &= KL\left( P(img_a | IC_{img}; \mathcal{M}) \parallel \right. \\
    & \quad \left. P(img_a | \mathcal{M}_{ivt}) \right).
\end{aligned}
\end{equation}
where $P(img_a | IC_{img}; \mathcal{M})$ and  $P(img_a | \mathcal{M}_{ivt})$ represent the output distribution of models $\mathcal{M}$ and $\mathcal{M}_{ivt}$ respectively for anchor image $img_a$.\\
Further we compute the intervention loss ($\mathscr{L}_{ivt}$) to make sure that the output of final model $\mathcal{M}_{ivt}(img_a)$ is aligned with the ground truth $GT_{img_a}$ (see Equation~\ref{eq:gtloss})
\begin{equation}
\label{eq:gtloss}
    \mathscr{L}_{ivt} = -\sum_{|\mathcal{D}_{IC}|} \log P(img_a | \mathcal{M}_{ivt})
\end{equation}
\noindent We compute the final loss as given in Equation~\ref{eq:fullloss}. $\gamma$ serves as a hyperparameter that determines the relative importance of output distribution loss and intervention loss.
\begin{equation}
\label{eq:fullloss}
    \mathscr{L} = \mathscr{L}_{od} + \gamma \cdot \mathscr{L}_{ivt}
\end{equation}
%\paragraph{Here, we talk about the inference using our framework}

\section{Datasets}
To advance research on harmful meme intervention, we construct a novel dataset of implicitly harmful memes, sourced from various online social media platforms, including Facebook, Twitter, Instagram, and WhatsApp. Unlike existing datasets that primarily focus on memes with explicit textual content embedded in them, our dataset specifically targets memes that are implicitly harmful or lack embedded text (see Figure~\ref{fig:representative_examples_different_meme} for details). These cases pose additional challenges for AI models, as they require nuanced reasoning beyond surface-level textual analysis. Below, we detail our data collection and annotation process.  
\begin{table}[h]
% \scriptsize
    \centering
    \resizebox{0.45\textwidth}{!}{
    \begin{tabular}{l|l|c}
\hline \hline
\textbf{Commonsense category (meta)}                         & \textbf{Commonsense parameters}    & \textbf{\# Memes} \\ \hline
\multirow{6}{*}{\textbf{Recognizing social norm violations}} & \textit{Hate speech}               & 23                \\
                                                             & \textit{Body shaming}              & 74                \\
                                                             & \textit{Misogyny}                  & 51                \\
                                                             & \textit{Stereotyping}              & 32                \\
                                                             & \textit{Sexual content}            & 105               \\
                                                             & \textit{Vulgarity}                 & 135               \\ \hline
\textbf{Assessing credibility}                               & \textit{Misinformation}            & 4                 \\ \hline
\multirow{4}{*}{\textbf{Empathy and ethical judgements}}     & \textit{Child exploitation}        & 12                \\
                                                             & \textit{Public decorum \& Privacy} & 72                \\
                                                             & \textit{Cultural sensitivity}      & 60                \\
                                                             & \textit{Religious sensitivity}     & 14                \\ \hline
\textbf{Contextual interpretation}                           & \textit{Humor appropriateness}     & 251               \\ \hline
\multirow{3}{*}{\textbf{Predicting consequences}}            & \textit{Mental health impact}      & 38                \\
                                                             & \textit{Violence}                  & 43                \\
                                                             & \textit{Substance abuse}           & 7                 \\ \hline \hline
\end{tabular}
}
    \caption{\footnotesize Distribution of various commonsense attributes.}
    \label{tab:category_counts}
    \vspace{-0.3cm}
\end{table}
% \begin{figure*}[h]
% \centering
% \scriptsize
% \includegraphics[width=0.70\textwidth]{images/meme1.pdf}
% \caption{\footnotesize Representative examples of different memes.}
% \label{fig:representative_examples_different_meme}
% %\vspace{-0.3cm}
% \end{figure*}

\noindent\textbf{Data collection}: We curate memes from publicly available online sources, including Facebook meme pages\footnote{\url{https://www.facebook.com/doublemean}}, Twitter adult meme pages\footnote{\url{https://x.com/DefensePorn}}, public WhatsApp groups, and Instagram meme accounts\footnote{\url{https://www.instagram.com/stoned_age_humour}}. In addition, we incorporate phallic\footnote{\url{https://en.wikipedia.org/wiki/Phallus}}-themed memes\footnote{\url{https://humornama.com/memes/penis-memes/}} which may not appear overtly harmful at first glance but can carry implicit harmful implications when shared publicly. Our data collection process resulted in a total of 785 memes.\\  
\textbf{Filtering and annotation}: To ensure relevance, we filter out memes that do not exhibit potential harm, specifically those that do not align with any of the 15 predefined commonsense harm categories (see Table~\ref{tab:category_counts}). Two undergraduate annotators independently labeled each meme as \textbf{\textit{harmful}} or \textbf{\textit{non-harmful}}. We retain only those memes that were unanimously marked as \textbf{\textit{harmful}} by both annotators, resulting in a final dataset of 484 memes. Figure~\ref{fig:representative_examples_different_meme} illustrates representative examples of different memes from our collection. Once we have the annotations of the memes done we obtain the commonsense categories and the ground truth interventions for these memes using GPT-4o as was already discussed in Section \ref{sec:stageI}.%\am{So they marked harm/non-harm. Who annotated the commonsense categories? And who wrote the ground-truth interventions?}

\subsection{ICMM data}
In addition to our curated dataset, we also consider the publicly available \textit{Intervening Cyberbullying in Multimodal Memes} (ICMM) dataset \cite{jha2024memeguardllmvlmbasedframework}  for evaluation of our approach. This dataset consists of 1000 cyberbullying memes along with their corresponding crowdsourced interventions. After filtering out the corrupted images, we obtain a set of 985 memes along with their ground truth interventions.
% \section{Tasks}
% Mention the tasks -- intervention generation \& classification

% Given a harmful meme $img$, our task is to generate a concise yet effective intervention by comprehending the contextual implications of the meme. The goal of this intervention is to discourage users from sharing such content while promoting responsible online behavior. To systematically evaluate this task, we construct a dataset of implicitly harmful memes. Additionally, we leverage the publicly available ICMM dataset to further validate the effectiveness of our approach.

% \begin{figure*}[h]
%     \centering
%     \begin{subfigure}[b]{0.25\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{images/meme_without_text.jpg}
%         \caption{\footnotesize harmful meme without text}
%         \label{fig:meme_without_text}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.25\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{images/meme_with_text.jpg}
%         \caption{\footnotesize harmful meme with text}
%         \label{fig:harmful_meme_with_text}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.25\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{images/non_harmful.jpg}
%         \caption{\footnotesize non-harmful meme}
%         \label{fig:non_harmful_meme}
%     \end{subfigure}

%     \caption{\footnotesize Representative examples of different memes}
%     \label{fig:representative_examples_different_meme}
% \end{figure*}


% \begin{table}[!htp]\centering
% \scriptsize
% \begin{tabular}{c|cc}\toprule
% Dataset Split &\# Instances \\\midrule
% Train &300 \\
% Test &184 \\\midrule
% Total &484 \\
% \bottomrule
% \end{tabular}
% \caption{Dataset Size}\label{tab:data_stat}
% \end{table}

% \begin{table}[h]
% \scriptsize
%     \centering
%     \begin{tabular}{l|c}
%         \toprule
%         Commonsense parameters & \# memes \\
%         \midrule
%         Body Shaming & 74 \\
%         Misogyny & 51 \\
%         Humor Appropriateness & 251 \\
%         Stereotyping & 32 \\
%         Vulgarity & 135 \\
%         Hate Speech & 23 \\
%         Cultural Sensitivity & 60 \\
%         Sexual Content & 105 \\
%         Public Decorum \& Privacy & 72 \\
%         Mental Health Impact & 38 \\
%         Violence & 43 \\
%         Substance Abuse & 7 \\
%         Child Exploitation & 12 \\
%         Religious Sensitivity & 14 \\
%         Misinformation & 4 \\
%         \bottomrule
%     \end{tabular}
%     \caption{Distribution of different commonsense parameters}
%     \label{tab:category_counts}
% \end{table}





\section{Experimental setup}
\label{sec:experimental_setup}
This section discusses the different experimental configurations of \textsc{MemeSense}.

\subsection{Baselines}
We evaluate our proposed approach against several baseline methods, including state-of-the-art meme intervention techniques and various prompting strategies.\\
\textbf{(1) MemeGuard}~\cite{jha2024memeguardllmvlmbasedframework}: We adopt MemeGuard, a state-of-the-art meme intervention generation model, as a baseline. Given a meme, we use a VLM to generate five descriptive answers. To filter out irrelevant content, we compute the semantic similarity between the input meme and the generated sentences, retaining only those exceeding a 0.2 threshold (determined via manual inspection). Finally, another VLM generates the intervention based on the meme and the filtered descriptions.\\
% We adopt \textit{MemeGuard}, a state-of-the-art meme intervention generation model, as one of our baselines. Memeguard: given a meme, we first use a VLM to generate descriptions, 5 answers. Now, from the generate answers, to filter out irrelevant sentences, we compute the semantic similarity of the input meme with the sentences from the generated answers. We only consider the sentences, which exceed the the threshold similarity value (we consider 0.2 after manually inspecting). 
% Then given the meme and the filtered answers we prompt another VLM to generate the intervention.\\
\textbf{(2) MemeMQA (Modified)}~\cite{agarwal2024mememqamultimodalquestionanswering}: 
We extend the MemeMQA framework for intervention generation by removing its target identification module and repurposing its explanation generation module. Originally designed to identify targets in hateful memes and explain predictions, MemeMQA now directly generates interventions.\\
% We extend the \textit{MemeMQA} framework by modifying it to generate intervention responses. \textit{MemeMQA} framework is designed to identify a target from a targeted hateful meme and also to generate the explanation of predicting the target. We remove the target identification module and modify the explanation generation module to generate intervention. 
% \textbf{Zero-Shot Prompting}: We employ a direct zero-shot prompting approach, where a Vision-Language Model (VLM) is prompted to generate an intervention for a given toxic meme. 
    % \begin{quote}
    %     \texttt{<prompt>}
    % \end{quote}
\textbf{(3) Commonsense-enhanced prompting}: Given a meme and its automatically generated commonsense parameters, the VLM is instructed to generate an intervention.\\
\textbf{(4) In-context learning (ICL)}~\cite{zeng2024mllmsperformtexttoimageincontext}: For a given target meme, we select $k$ ($\in \{1,2,4\}$) demonstration examples from the training set, including their annotated commonsense, and provide them as context before prompting the VLM to generate an intervention. For the selection of in-context examples, we use random and semantic retrieval techniques similar to \textbf{Stage II} (Section \ref{sec:stageII}).

\subsection{\textsc{MemeSense} framework}
Recall that \textsc{MemeSense} consists of three major stages leveraging (I) multimodal LLMs for generation of commonsense parameter, (II) in-context exemplars selection and (III) subsequent learning of the cognitive shift vector for the \textbf{intervention generation}.\\
\noindent For the \textbf{Stage I}, we utilize the \texttt{llava-v1.6-mistral-7b-hf}\footnote{\url{https://huggingface.co/llava-hf/llava-v1.6-mistral-7b-hf}} model, fine-tuned with QLoRA~\cite{dettmers2023qloraefficientfinetuningquantized} over 10 epochs using a batch size of 16 and a learning rate of $2 \times 10^{-4}$, with weight decay for optimization.\\
\noindent For the \textbf{Stage II}, We employ various strategies for selecting in-context exemplars, detailed as follows: \\
\noindent \textit{\textbf{Commonsense-based retrieval}}: For each predefined commonsense parameter, we select up to five instances from our training set to form a lookup set. Given an anchor image $img$ and its corresponding annotated commonsense parameters, we iteratively retrieve at least one instance per parameter to construct the $k$ demonstration examples. \\ 
\noindent \textit{\textbf{Image-based retrieval}}: For a given anchor image $img$, we retrieve $k$ demonstrations by computing their semantic similarity with $img$ from the training subset. To achieve this, we first encode all images into dense vector representations using the \texttt{CLIP-ViT}\footnote{\url{sentence-transformers/clip-ViT-B-32}} multimodal embedding model. When an anchor image is provided as a query, we map it into the same vector space, enabling an efficient similarity search. We then perform Approximate Nearest Neighbor (ANN)~\cite{wang2021comprehensivesurveyexperimentalcomparison} search to identify the top $k$ most similar images. Their corresponding commonsense parameters and ground truth interventions are retrieved as in-context examples, ensuring a contextually relevant selection.\\
\noindent \textit{\textbf{Combined retrieval}}: We also experiment with constructing the $k$ in-context demonstrations by combining the above two approaches. Here, we select $c$ instances from the commonsense based retrieval and $(k-c)$ instances from the image-based retrieval, where $c \in \{1,2\}$.\\
\noindent For \textbf{Stage III}, we primarily employ the \texttt{idefics2-8B-base}\footnote{\url{https://huggingface.co/HuggingFaceM4/idefics2-8b-base}} model to learn cognitive shift vectors and perform inference. In addition, we explore \texttt{idefics-9B}\footnote{\url{https://huggingface.co/HuggingFaceM4/idefics-9b}} and~\texttt{OpenFlamingo}\footnote{\url{https://huggingface.co/openflamingo/OpenFlamingo-9B-vitl-mpt7b}} for intervention generation (results presented in Appendix~\ref{appendix:results_other_model}). The number of in-context demonstration examples is one of \{1, 2, 4\}, maintaining a fixed batch size of 2. The shift vector undergoes training for 10 epochs to ensure effective adaptation and we choose $\gamma$ as 0.5.

% \begin{figure}
%     \centering
%     \includegraphics[width=0.9\columnwidth]{images/performance_vs_hyperparameters.pdf}
%     \caption{Validation performance over different hyperparameter settings}
%     \label{fig:plot_hyperparameters}
% \end{figure}

\subsection{Baseline models}

For baselines involving zero-shot prompting and in-context learning (ICL), we leverage the same aligned MLLMs used in \textsc{MemeSense} -- \texttt{idefics2-8B}, \texttt{idefics-9B}, and \texttt{OpenFlamingo} -- for intervention generation.

\noindent The \textbf{MemeQA} baseline adopts a dual-model architecture, comprising:

%\begin{compactitem}
\noindent \textbf{(1)}~An MLLM for rationale generation, aligned with the \textsc{MemeSense} models.\\
\textbf{(2)}~A \texttt{T5-large} model for intervention generation.
%\end{compactitem}
The rationale generation MLLM is fine-tuned for one epoch with a batch size of 4 and a learning rate of $5 \times 10^{-5}$.

\noindent \textbf{MemeGuard}, another baseline, employs two MLLMs for intervention generation, using models aligned with those in \textsc{MemeSense} to ensure consistency in evaluation.

\subsection{Evaluation metrics}

% To assess the quality of generated interventions, we employ a combination of semantic, lexical, and readability metrics:\\
% - Semantic Metrics: BERTScore~\cite{Zhang*2020BERTScore:} and Semantic Cosine Similarity~\cite{Rahutomo2012SemanticCS}.\\
% - Lexical Metrics: ROUGE-L~\cite{lin-2004-rouge} and BLEU-4~\cite{papineni-etal-2002-bleu}.\\
% - Readability Score: Evaluates fluency and ease of comprehension.\\

% These metrics provide a comprehensive evaluation of the generated interventions, ensuring alignment with both semantic meaning and lexical quality.

To rigorously assess the quality of generated interventions, we employ a diverse set of evaluation metrics spanning semantic similarity, lexical accuracy, and readability. Semantic metrics such as BERTScore~\cite{Zhang*2020BERTScore:} and semantic cosine similarity~\cite{Rahutomo2012SemanticCS} measure the alignment between generated and reference interventions in embedding space. Lexical metrics, including ROUGE-L~\cite{lin-2004-rouge} and BLEU-4~\cite{papineni-etal-2002-bleu}, evaluate surface-level text overlap and n-gram precision. Further, a readability score assesses fluency and ease of comprehension, ensuring the interventions are not only accurate but also coherent and accessible. This holistic evaluation framework enables a nuanced assessment of intervention effectiveness across multiple linguistic dimensions.



\section{Results}

We structure our experimental results into three key sections. First, we present insights derived from our dataset, highlighting key patterns and observations. Next, we evaluate the performance of our framework on the ICMM dataset, examining its effectiveness in generating interventions. Finally, we delve into a detailed breakdown of performance across different commonsense meta-categories, offering a deeper understanding of the model’s strengths and limitations in various contexts.\\
\textbf{Result for our dataset}:
In Tables~\ref{tab:result_memes_without_text} and~\ref{tab:result_memes_with_text}, we compare the performance of our framework, \textsc{MemeSense}, with various baselines on memes without text and memes with text, respectively. Across both settings, \textsc{MemeSense} (combined) consistently achieves the highest values for BERTScore (0.91), semantic similarity (0.71 for the memes without text, 0.78 for text-based memes), and ROUGE-L (0.35 and 0.37, respectively), demonstrating its superior capability in generating semantically meaningful and contextually appropriate responses. Among the baseline methods, commonsense-anchored ICL performs competitively but lags behind \textsc{MemeSense}, particularly in terms of semantic similarity score, highlighting the importance of hybrid reasoning strategies.\\
\noindent For memes without text, direct prompting methods struggle with low semantic similarity ($\leq$ 0.3), while \textsc{MemeSense} (combined) significantly outperforms them (semantic similarity = 0.71). Similarly, for memes with text, \textsc{MemeSense} achieves notable improvements in both semantic alignment and lexical overlap (BLEU: 0.08–0.09), reflecting its ability to effectively integrate commonsense and image-grounded reasoning. Overall, these results demonstrate that the \textsc{MemeSense} (combined) approach integrating image-anchored, and commonsense-anchored in-context learning (ICL), effectively enhances reasoning and interpretation across different meme types.
% \begin{table}[ht]
% % \scriptsize
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{@{}|l|rrrrr@{}|}
% \toprule
% \multicolumn{1}{|c|}{Method} &
%   \multicolumn{1}{c|}{BERTScore (F1)} &
%   \multicolumn{1}{c|}{Semantic Similarity} &
%   \multicolumn{1}{c|}{Readability} &
%   \multicolumn{1}{c|}{Rouge-L (Avg)} &
%   \multicolumn{1}{c|}{BLEU (Avg)} \\ \midrule
% Direct prompting                  & 0.81          & 0.27         & \textbf{53.36} & 0.05                         & 0.001         \\
% Direct prompting with commonsense & 0.81          & 0.3          & 21.55          & 0.05                         & 0.002         \\
% Random ICL                        & 0.87          & 0.49         & 35.06          & 0.19                         & 0.01          \\
% Image anchored ICL                & 0.86          & 0.41         & 36.49          & \cellcolor[HTML]{FFFFFF}0.17 & 0.02          \\
% Commonsense anchored ICL          & 0.88          & 0.46         & 34.12          & 0.18                         & 0.02          \\
% MemeMQA &
%   \multicolumn{1}{l}{} &
%   \multicolumn{1}{l}{} &
%   \multicolumn{1}{l}{} &
%   \multicolumn{1}{l}{} &
%   \multicolumn{1}{l}{} \\
% MemeGuard                         & 0.82          & 0.35         & 51.69          & 0.09                         & 0.005         \\
% LICV (Random)                     & 0.9           & 0.68         & 46.22          & 0.34                         & 0.07          \\
% LICV (Image anchored ICL)         & 0.9           & 0.7          & 45.57          & 0.35                         & 0.08          \\
% LICV (Commonsense anchored ICL)   & \textbf{0.91} & \textbf{0.7} & 45.65          & \textbf{0.35}                & \textbf{0.09}\\
% \bottomrule
% \end{tabular}
% }
% \caption{\footnotesize Result for memes without text}
% \label{tab:result_memes_without_text}
% \end{table}
\begin{table}[t]
\centering
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{l|ccccc}
\hline \hline
\multicolumn{1}{c|}{\textbf{Method}}       & \textit{\textbf{\textit{\textbf{\begin{tabular}[c]{@{}c@{}}BERTScore \\ (F1)\end{tabular}}}}} & \textit{\textbf{SeSS}} & \textit{\textbf{Readability}} & \textit{\textbf{\begin{tabular}[c]{@{}c@{}}ROUGE-L \\ (Avg)\end{tabular}}} & \textit{\textbf{\begin{tabular}[c]{@{}c@{}}BLEU \\ (Avg)\end{tabular}}} \\ \hline
\textbf{Direct prompting}                  & 0.81                             & 0.27                                  & \textbf{53.36}                & 0.05                            & 0.001                        \\
\textbf{Direct prompting \textit{(w. commonsense)}} & 0.81                             & 0.3                                   & 21.55                         & 0.05                            & 0.002                        \\
\textbf{Random ICL}                        & 0.87                             & 0.49                                  & 35.06                         & 0.19                            & 0.01                         \\
\textbf{Image anchored ICL}                & 0.86                             & 0.41                                  & 36.49                         & 0.17                            & 0.02                         \\
\textbf{Commonsense anchored ICL}          & 0.88                             & 0.46                                  & 34.12                         & 0.18                            & 0.02                         \\ \hline
\textbf{MemeMQA}                           &       0.86                           &      0.51                                 &    52.86                           &      0.08                           &       0.008                       \\
\textbf{MemeGuard}                         & 0.82                             & 0.35                                  & 51.69                         & 0.09                            & 0.005                        \\ \hline
\textbf{\textsc{MemeSense} \textit{(random ICL)}}                     & 0.9                              & 0.68                                  & 46.22                         & 0.34                            & 0.07                         \\
\textbf{\textsc{MemeSense} \textit{(image anchored ICL)}}         & 0.9                              & 0.7                                   & 45.57                         & 0.35                            & 0.08                         \\
\textbf{\textsc{MemeSense} \textit{(commonsense anchored ICL)}}   & 0.91                    & 0.7                          & 45.65                         & 0.35                   & \textbf{0.09}                \\ 
\textbf{\textsc{MemeSense} \textit{(combined)}}   & \textbf{0.91}                    & \textbf{0.71}                          & 44.07                        & \textbf{0.35}                   & 0.08                \\ 
\hline \hline
\end{tabular}
}
\caption{\footnotesize Result for memes without text. \textbf{\textit{SeSS}}: semantic similarity.}
\label{tab:result_memes_without_text}
\end{table}
% \begin{table}[]
% \scriptsize
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{@{}|l|rrrrr|@{}}
% \toprule
% \multicolumn{1}{|c|}{Method} &
%   \multicolumn{1}{c}{BERTScore (F1)} &
%   \multicolumn{1}{c}{Semantic Similarity} &
%   \multicolumn{1}{c}{Readability} &
%   \multicolumn{1}{c}{Rouge-L (Avg)} &
%   \multicolumn{1}{c|}{BLEU (Avg)} \\ \midrule
% Direct prompting                  & 0.81 & 0.35 & \textbf{54.59} & 0.04 & 0.001 \\
% Direct prompting with commonsense & 0.8  & 0.28 & 22.02          & 0.04 & 0.001 \\
% Random ICL                        & 0.86 & 0.52 & 31.94          & 0.18 & 0.02  \\
% Image anchored ICL                & 0.87 & 0.49 & 31.52          & 0.18 & 0.02  \\
% Commonsense anchored ICL          & 0.88 & 0.55 & 33.25          & 0.19 & 0.03  \\
% MemeQA &
%   \multicolumn{1}{l}{} &
%   \multicolumn{1}{l}{} &
%   \multicolumn{1}{l}{} &
%   \multicolumn{1}{l}{} &
%   \multicolumn{1}{l|}{} \\
% MemeGuard                         & 0.84 & 0.39 & 36.36          & 0.09 & 0.004 \\
% LICV (Random)                     & 0.91 & 0.77 & 46.64          & 0.36 & 0.08  \\
% LICV (Meme anchored ICL)          & 0.91 & 0.77 & 44.33          & 0.35 & 0.07  \\
% LICV (Commonsense anchored ICL) &
%   \textbf{0.91} &
%   \textbf{0.78} &
%   48.74 &
%   \textbf{0.38} &
%   \textbf{0.09} \\ \bottomrule
% \end{tabular}
% }
% \caption{Result for memes with text}
% \label{tab:result_memes_with_text}
% \end{table}
\begin{table}[]
\centering
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{l|ccccc}
\hline \hline
\multicolumn{1}{c|}{\textbf{Method}}     & \textit{\textbf{\begin{tabular}[c]{@{}c@{}}BERTScore \\ (F1)\end{tabular}}} & \textit{\textbf{SeSS}} & \textit{\textbf{Readability}} & \textit{\textbf{\begin{tabular}[c]{@{}c@{}}ROUGE-L \\ (Avg)\end{tabular}}} & \textit{\textbf{\begin{tabular}[c]{@{}c@{}}BLEU \\ (Avg)\end{tabular}}} \\ \hline
\textbf{Direct prompting}                & 0.81                                                                        & 0.35                   & \textbf{54.59}                & 0.04                                                                       & 0.001                                                                   \\
\textbf{Direct prompting \textit{(w. commonsense)}} & 0.8                                                                         & 0.28                   & 22.02                         & 0.04                                                                       & 0.001                                                                   \\
\textbf{Random ICL}                      & 0.86                                                                        & 0.52                   & 31.94                         & 0.18                                                                       & 0.02                                                                    \\
\textbf{Image anchored ICL}              & 0.87                                                                        & 0.49                   & 31.52                         & 0.18                                                                       & 0.02                                                                    \\
\textbf{Commonsense anchored ICL}        & 0.88                                                                        & 0.55                   & 33.25                         & 0.19                                                                       & 0.03                                                                    \\ \hline
\textbf{MemeQA}                          &        0.86                                                                     &      0.54                  &                50.28               &                 0.1                                                           &                0.009                                                         \\
\textbf{MemeGuard}                       & 0.84                                                                        & 0.39                   & 36.36                         & 0.09                                                                       & 0.004                                                                   \\ \hline
\textbf{\textsc{MemeSense} \textit{(random ICL)}}                   & 0.91                                                                        & 0.77                   & 46.64                         & 0.36                                                                       & 0.08                                                                    \\
\textbf{\textsc{MemeSense} \textit{(image anchored ICL)}}        & 0.91                                                                        & 0.77                   & 44.33                         & 0.35                                                                       & 0.07                                                                    \\
\textbf{\textsc{MemeSense} \textit{(commonsense anchored ICL)}} & 0.91                                                               & 0.78          & 48.74                         & \textbf{0.38}                                                              & \textbf{0.09}                                                           \\ 
\textbf{\textsc{MemeSense} \textit{(combined)}} & \textbf{0.91}                                                               & \textbf{0.78}          & 43.38                         & 0.37                                                              & 0.08                                                           \\
\hline \hline
\end{tabular}
}
\caption{\footnotesize Result for memes with text. \textbf{\textit{SeSS}}: semantic similarity.}
\vspace{-0.3cm}
\label{tab:result_memes_with_text}
\end{table}
% \begin{table}[]
% \scriptsize
% \begin{tabular}{@{}|l|r|r|r|r|r|@{}}
% \toprule
% \multicolumn{1}{|c|}{\textbf{Method}} &
%   \multicolumn{1}{c|}{\textbf{BERTScore (F1)}} &
%   \multicolumn{1}{c|}{\textbf{Semantic similarity}} &
%   \multicolumn{1}{c|}{\textbf{Readability}} &
%   \multicolumn{1}{c|}{\textbf{Rouge-L (Avg)}} &
%   \multicolumn{1}{c|}{\textbf{BLEU (Avg)}} \\ \midrule
% Direct prompting                             & 0.8            & 0.15                  & 67.02                 & 0.03                  & 0.001                 \\
% Direct prompting with commonsense            & 0.8            & 0.14                  & 52.34                 & 0.03                  & 0.004                 \\
% Random ICL                                   & 0.82           & 0.16                  & 19.63                 & 0.09                  & 0.005                 \\
% Image anchored ICL                           & 0.82           & 0.2                   & 22.16                 & 0.1                   & 0.006                 \\
% Commonsense anchored ICL                     & 0.84           & 0.22                  & 25.38                 & 0.1                   & 0.006                 \\
% MemeQA                                       & 0.84           & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
% MemeGuard                                    & 0.79           & 0.18                  & 34.45                 & 0.04                  & 0.001                 \\
% LICV (Random)                                & 0.84           & 0.18                  & 44.03                 & 0.11                  & 0.007                 \\
% LICV (Image anchored ICL)                    & 0.85           & 0.25                  & 42.79                 & 0.1                   & 0.007                 \\
% LICV (Commonsense anchored ICL)              & 0.861          & 0.27                  & 42.22                 & 0.11                  & 0.009                 \\
% LICV (Hybrid -> meme + commonsense anchored) & \textbf{0.875} & \textbf{0.31}         & 45.57                 & 0.11                  & 0.008                 \\ \bottomrule
% \end{tabular}
% \caption{Result for the MemeGuard (ICMM) dataset}
% \label{tab:result_memeguard}
% \end{table}
\begin{table}[h]
\centering
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{l|ccccc}
\hline \hline
\multicolumn{1}{c|}{\textbf{Method}}                             & \textit{\textbf{\begin{tabular}[c]{@{}c@{}}BERTScore \\ (F1)\end{tabular}}} & \textit{\textbf{SeSS}} & \textit{\textbf{Readability}} & \textit{\textbf{\begin{tabular}[c]{@{}c@{}}ROUGE-L \\ (Avg)\end{tabular}}} & \textit{\textbf{\begin{tabular}[c]{@{}c@{}}BLEU \\ (Avg)\end{tabular}}} \\ \hline
\textbf{Direct prompting}                                        & 0.8                                                                         & 0.15                   & 67.02                         & 0.03                                                                       & 0.001                                                                   \\
\textbf{Direct prompting with commonsense}                       & 0.8                                                                         & 0.14                   & 52.34                         & 0.03                                                                       & 0.004                                                                   \\
\textbf{Random ICL}                                              & 0.82                                                                        & 0.16                   & 19.63                         & 0.09                                                                       & 0.005                                                                   \\
\textbf{Image anchored ICL}                                      & 0.82                                                                        & 0.2                    & 22.16                         & 0.1                                                                        & 0.006                                                                   \\
\textbf{Commonsense anchored ICL}                                & 0.84                                                                        & 0.22                   & 25.38                         & 0.1                                                                        & 0.006                                                                   \\ \hline
\textbf{MemeQA}                                                  & 0.85                                                                        &   0.24                     &      54.45                         &            0.1                                                                &           0.007                                                              \\
\textbf{MemeGuard}                                               & 0.79                                                                        & 0.18                   & 34.45                         & 0.04                                                                       & 0.001                                                                   \\ \hline
\textbf{\textsc{MemeSense} \textit{(random ICL)}}                                           & 0.84                                                                        & 0.18                   & 44.03                         & 0.11                                                                       & 0.007                                                                   \\
\textbf{\textsc{MemeSense} \textit{(image anchored ICL)}}                               & 0.85                                                                        & 0.25                   & 42.79                         & 0.1                                                                        & 0.007                                                                   \\
\textbf{\textsc{MemeSense} \textit{(commonsense anchored ICL)}}                         & 0.86                                                                       & 0.27                   & 42.22                         & 0.11                                                                       & 0.009                                                                   \\
\textbf{\textsc{MemeSense} \textit{(combined)}} & \textbf{0.87}                                                              & \textbf{0.31}          & 45.57                         & 0.11                                                                       & 0.008                                                                   \\ \hline \hline
\end{tabular}
}
\caption{\footnotesize Result for the ICMM dataset.}
\label{tab:result_memeguard}
\vspace{-0.3cm}
\end{table}

\noindent\textbf{Result for ICMM data}: In Table~\ref{tab:result_memeguard}, we show the result of various baselines and compare them with \textsc{MemeSense} for the ICMM dataset. Direct prompting achieves the highest readability (67.02) but performs poorly in semantic alignment (SeSS = 0.15, ROUGE-L = 0.03, BLEU = 0.001), while adding commonsense knowledge reduces readability further (52.34) without improving semantic scores. In-context learning (ICL) methods, including random, image-anchored, and commonsense-anchored ICL, improve semantic similarity (0.16–0.22) and ROUGE-L (0.09–0.1) but suffer from significantly lower readability (19.63–25.38). Among meme-specific baseline models, \textbf{MemeQA} performs best (SeSS = 0.24, readability = 54.45) as it requires explicit training,  while \textbf{MemeGuard} underperforms across all metrics (SeSS = 0.18, readability = 34.45). \textsc{MemeSense} outperforms all baselines, with \textsc{MemeSense} (commonsense anchored ICL) achieving strong semantic alignment (SeSS = 0.27), while \textsc{MemeSense} (combined) emerges as the best overall method with the highest BERTScore (0.875) and SeSS (0.31), reasonable readability (45.57), and competitive ROUGE-L (0.11) and BLEU (0.008) scores. This suggests that structured multimodal approaches, particularly \textsc{MemeSense} (combined), provide the best balance between semantic coherence and fluency, making it the most effective meme intervention generation strategy.\\
\noindent\textbf{Results for social commonsense categories}: Table~\ref{tab:meta_category_wise_result} presents the performance of our model across different broad social commonsense categories, evaluated using BERTScore (F1), semantic similarity (SeSS), and ROUGE-L. Notably, for all four categories, the results are very similar showing the robustness of the design of \textsc{MemeSense}. The model achieves the highest scores in \textit{recognizing social norm violations} (BERTScore: 0.91, SeSS: 0.79, ROUGE-L: 0.38), suggesting strong alignment with human references in identifying and intervening in socially inappropriate memes containing themes such as \textit{vulgarity}, \textit{sexual content} etc. For the other three categories also the results are quite close in terms of all three metrics (BERTScore: 0.90/0.91, SeSS: 0.72-0.78, ROUGE-L: 0.33-0.37).
%\noindent In contrast, memes related to \textbf{Empathy and ethical judgments} and \textbf{Predicting consequences} show relatively lower semantic similarity (SeSS: 0.75 and 0.72, respectively), suggesting challenges in reasoning about ethical implications and future outcomes. 
\begin{table}[]
\centering
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{l|ccc}
\hline \hline
\multicolumn{1}{c|}{\textbf{Meta category (Commonsense)}}     & \textit{\textbf{\begin{tabular}[c]{@{}c@{}}BERTScore \\ (F1)\end{tabular}}} & \textit{\textbf{SeSS}} & \textit{\textbf{\begin{tabular}[c]{@{}c@{}}ROUGE-L \\ (Avg)\end{tabular}}}  \\ \hline

\textbf{Contextual interpretation}                   & 0.91                                                                        & 0.78            & 0.37                                                                                                         \\
\textbf{Empathy and ethical judgements}        & 0.90                                                                        & 0.75                   & 0.33                                                              \\
\textbf{Predicting consequences} & 0.90                                                               & 0.72                                & 0.33                                                             \\ 
\textbf{Recognizing social norm violations} & \textbf{0.91}                                                               & \textbf{0.79}          & \textbf{0.38}                                                                    \\
\hline \hline
\end{tabular}
}
\caption{\footnotesize Meta category-wise evaluation results.}
\label{tab:meta_category_wise_result}
\vspace{-0.3cm}
\end{table}


\section{Discussion}



% \begin{table*}[]
% \scriptsize
%     \centering
%     \renewcommand{\arraystretch}{1} % Adjust row height for readability
%     \setlength{\tabcolsep}{10pt} % Adjust column spacing
%     \begin{tabular}{|p{0.2\textwidth}|p{0.2\textwidth}|p{0.2\textwidth}|p{0.2\textwidth}|}
%         \hline
%          & 
%         \includegraphics[width=0.4\columnwidth]{images/e4d8825f-2804-4327-a73e-b82794bfdc05.jpg} & 
%         \includegraphics[width=0.4\columnwidth]{images/e4d8825f-2804-4327-a73e-b82794bfdc05.jpg} & 
%         \includegraphics[width=0.4\columnwidth]{images/e4d8825f-2804-4327-a73e-b82794bfdc05.jpg}\\
%         \hline
%         Direct Prompting & Description 1 for Image 1 & Description 1 for Image 2 & Description 1 for Image 3 \\
%         \hline
%         Direct Prompting (w. commonsense) & Description 2 for Image 1 & Description 2 for Image 2 & Description 2 for Image 3 \\
%         \hline
%         Random ICL & Celebrating individuality and embracing diversity enhances understanding and acceptance among us all. & Description 3 for Image 2 & Description 3 for Image 3 \\
%         \hline
%         image anchored ICL  & This meme humorously highlights the playful creativity and shared sense of humor in relationships, encouraging laughter and bonding over shared jokes. & Description 4 for Image 2 & Description 4 for Image 3 \\
%         \hline
%         Commonsense anchored ICL & Description 5 for Image 1 & Description 5 for Image 2 & Description 5 for Image 3 \\
%         \hline
%         MemeQA & The meme should be restricted due to its content and content. It is inappropriate for public posting and should not be shared publicly. & Description 6 for Image 2 & Description 6 for Image 3 \\
%         \hline
%         MemeGuard & The meme is making a claim that the person is having a barbecue with a hot dog and marshmallows, but instead of a traditional hot dog, there is a penis-shaped hot dog The meme is likely meant to be humorous and playful, as it takes a common outdoor activity like grilling and adds a provocative twist to it. & Description 7 for Image 2 & Description 7 for Image 3 \\
%         \hline
%         MemeSense (Random ICL) & The meme should be restricted from public posting due to its vulgar and suggestive nature, which could be perceived as offensive or inappropriate. & Description 7 for Image 2 & Description 7 for Image 3 \\
%         \hline
%         MemeSense (Image anchored ICL)  & The meme should not be posted publicly due to its vulgar and suggestive nature, which could be perceived as offensive or inappropriate.
%  & Description 7 for Image 2 & Description 7 for Image 3 \\
%         \hline
%         MemeSense (Commonsense anchored ICL) & The meme should be restricted from public posting due to its vulgar and suggestive content, which could be perceived as offensive or inappropriate. & Description 7 for Image 2 & Description 7 for Image 3 \\
%         \hline
%         MemeSense (Hybrid) & The meme should be restricted from public posting due to its vulgar and suggestive content, which could be perceived as inappropriate. & Description 7 for Image 2 & Description 7 for Image 3 \\
%         \hline
%         Ground Truth Annotation & The meme should be restricted in certain contexts due to its potentially suggestive content, which might be perceived as vulgar or inappropriate, particularly in public or professional settings. & Description 7 for Image 2 & Description 7 for Image 3 \\
%         \hline
%     \end{tabular}
%     \caption{\footnotesize Generated intervention using different approach for (1) a meme without text, (2) a meme with text, (3) a meme from ICMM data}
%     \label{tab:representative_example_generated_intervention}
% \end{table*}

\textbf{Error analysis}: To better understand the limitations of \textsc{MemeSense}, we conduct a detailed error analysis by examining its predictions and identifying cases where erroneous classifications occur. We categorize the errors into two types: \\ 
\noindent (1) \textit{False negative} (Category 1 error): Instances where the meme is actually harmful and should be flagged as unsafe, but \textsc{MemeSense} incorrectly predicts it as safe for posting.\\  
(2) \textit{Improper reasoning} (Category 2 error): Cases where the model correctly identifies the meme as unsafe but provides incorrect or inadequate reasoning for its decision. \\ 
\noindent Our analysis focuses on memes without explicit text, where reasoning relies primarily on visual cues. Among 51 such instances in our dataset, \textsc{MemeSense} exhibits Category 1 errors in 6 cases. Notably, in 5 out of these 6 cases, the commonsense parameter generation stage fails to accurately infer the harmful category, leading to incorrect classification. A specific example of this failure is observed when the model incorrectly identifies \textit{cultural sensitivity} as the primary harmful category for a meme that is actually \textit{vulgar}, ultimately leading to its misclassification as safe for posting.\\
\noindent Further, we identify one instance of Category 2 error, where the model predicts the meme as unsafe but fails to provide a coherent justification. This error arises due to improper reasoning during the commonsense parameter generation stage, which affects the interpretability and reliability of the model's intervention.\\
\textbf{Ablation studies}: In the error analysis, we observed the major prediction error appeared due to the incorrect generation of commonsense parameters. Hence we investigate, how much the final inference is dependent on the generated commonsense parameters. To achieve this, we obtain the inference from our approach without providing commonsense information to the model. Using only the input image and its corresponding description, we attempt to infer the intervention from our approach using the best method (\textsc{MemeSense} (combined)). The combined model is trained using the commonsense information. However, during the inference we are not providing the commonsense, removing the requirement of commonsense generation module during inference.
\noindent We observe a maximum decline in semantic similarity score of 4\% without commonsense information. In addition, we observe that the interventions are more descriptive, which is reflected in the increase of the \textit{readability} score.

\begin{table}[!ht]
\centering
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{l|ccccc}
\hline \hline
\multicolumn{1}{c|}{\textbf{Test set}}                             & \textit{\textbf{\begin{tabular}[c]{@{}c@{}}BERTScore \\ (F1)\end{tabular}}} & \textit{\textbf{SeSS}} & \textit{\textbf{Readability}} & \textit{\textbf{\begin{tabular}[c]{@{}c@{}}ROUGE-L \\ (Avg)\end{tabular}}} & \textit{\textbf{\begin{tabular}[c]{@{}c@{}}BLEU \\ (Avg)\end{tabular}}} \\ \hline
\textbf{Memes without text}                                        & 0.89(\colorbox{red!30}{\textbf{-.02}})                                                                        & 0.68(\colorbox{red!30}{\textbf{-.03}})                    & 51.02(\colorbox{green!30}{\textbf{+6.95}})                         & 0.31(\colorbox{red!30}{\textbf{-.04}})                                                                        & 0.07(\colorbox{red!30}{\textbf{-.01}})                                                                   \\
\textbf{Memes with text}                       & 0.9 (\colorbox{red!30}{\textbf{-.01}})                                                                       & 0.74(\colorbox{red!30}{\textbf{-.04}})                   & 47.79(\colorbox{green!30}{\textbf{+4.41}})                         & 0.32(\colorbox{red!30}{\textbf{-.04}})                                                                        & 0.06(\colorbox{red!30}{\textbf{-.02}})                                                                    \\
\textbf{ICMM}                                              & 0.85(\colorbox{red!30}{\textbf{-.02}})                                                                        & 0.27(\colorbox{red!30}{\textbf{-.04}})                  & 54.19(\colorbox{green!30}{\textbf{+8.62}})                         & 0.10(\colorbox{red!30}{\textbf{-.01}})                                                                       & 0.007(\colorbox{red!30}{\textbf{-.001}})                                                                   \\\hline \hline
\end{tabular}
}
\caption{\footnotesize Result for intervention generation for different test sets without using the commonsense parameters.}
\label{tab:result_without_commonsense}
\vspace{-0.3cm}
\end{table}


\nocite{Ando2005}
% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\section{Conclusion}
In this work, we introduced \textsc{MemeSense}, a three-stage, adaptive in-context learning framework that integrates visual and textual cues with social commonsense knowledge for robust meme moderation. By combining compact latent representations, carefully retrieved in-context exemplars, and cognitive shift vectors, our approach captures subtle, implicitly harmful signals, \textit{\underline{including memes without explicit text}} that often evade traditional pipelines. Experiments on our curated dataset and the \textit{ICMM} benchmark highlight \textsc{MemeSense}’s superior performance in generating semantically aligned interventions, surpassing state-of-the-art baselines. We hope \textsc{MemeSense} inspires broader research in in-context learning toward fostering safer, more responsible online communities.
%Experimental results on both our curated dataset and the \textit{ICMM} benchmark underscore \textsc{MemeSense}’s superior performance in generating high-quality, semantically aligned interventions, significantly surpassing state-of-the-art baselines. We hope \textsc{MemeSense} would inspire broader research into in-context learning techniques, with the ultimate goal of fostering safer and more responsible online communities.
\section{Limitation}
A principal limitation of \textsc{MemeSense} lies in its reliance on automatically generated commonsense parameters and curated in-context exemplars, which may not capture the full spectrum of cultural or linguistic nuances. In particular, memes containing highly context-dependent references or adversarial manipulations could circumvent the system’s current retrieval and commonsense inference components. In addition, because the approach depends on a finite set of annotated harmful categories, novel or emerging social norms might remain undetected. Addressing these concerns through broader annotation schemas, continuous model adaptation, and more nuanced retrieval strategies constitutes a pivotal direction for future work.
\section{Ethical considerations}
This work aims to promote safer online environments by detecting and mitigating harmful or offensive memes. However, automated moderation tools, including \textsc{MemeSense}, carry risks of over-moderation, potentially limiting free expression, especially in contexts where satire or cultural references are misinterpreted. We strived to minimize biases in data collection and annotation by involving diverse annotators and ensuring consistent labeling protocols. Yet, subjective judgments on harmfulness may still reflect annotators’ cultural and personal perspectives. Moreover, the system’s performance hinges on training data quality, introducing the possibility of inadvertently perpetuating harmful societal biases. Transparent reporting of system limitations and the use of \textsc{MemeSense} as a supplementary tool rather than a definitive arbiter remain crucial in safeguarding fairness and accountability in online content moderation.
\bibliography{custom}
% \newpage

\appendix

\section{Prompts}
\begin{table}[h]\centering
\scriptsize
\begin{tabular}{l|p{0.2\textwidth}|cr}\toprule
\cellcolor[HTML]{d0e0e3}\textbf{Hyperparameters} &\cellcolor[HTML]{d0e0e3}\textbf{Task} &\cellcolor[HTML]{d0e0e3}\textbf{Value} \\\midrule
Temperature &Desc, Bias, Stereotype, Toxicity \& Hate, Claim Generation &0 \\
num\_beams &Desc, Bias, Stereotype, Toxicity \& Hate, Claim Generation &1 \\
max\_new\_tokens &Desc, Bias, Stereotype, Toxicity \& Hate, Claim Generation &512 \\
Cosine Similarity Threshold &MKS Filtering &0.2 \\
max\_new\_tokens &Intervention  &1024 \\
\bottomrule
\end{tabular}
\caption{\footnotesize Hyperparameters for \textbf{MemeGuard}.}\label{tab:hyperparameters_memeguard}
\end{table}
\label{appendix:prompts}
\begin{table}[h]\centering
\scriptsize
\begin{tabular}{l|p{0.3\textwidth}}\toprule
% \multicolumn{2}{c}{\cellcolor[HTML]{ffd966}\textbf{Prompts For MemeGuard}} \\\midrule
\cellcolor[HTML]{d0e0e3}\textbf{Task} &\cellcolor[HTML]{d0e0e3}\textbf{Prompt} \\
Description generation &Describe this meme in detail. \\
Social bias gen. &What is the societal bias that this meme is conveying? \\
Social stereotype gen. &What is the societal stereotype that this meme is conveying? \\
Toxicity and hate &What is the toxicity and hate that this meme is spreading? \\
Claim the meme is making &What are the claims that this meme is making? \\
Intervention Generation &This is a toxic meme with the description: {ks1}. The following text is written inside the meme: {X}. Rationale: Bias: {ks2}, Toxicity: {ks3}, Claims: {ks4}, and Stereotypes: {ks5}. Write an intervention for this meme based on all this knowledge. \\
\bottomrule
\end{tabular}
\caption{\footnotesize Prompt used for different tasks in the \textbf{MemeGuard} method.}\label{tab:prompt_memeguard}
\end{table}
\noindent The prompt for generating ground truth commonsense parameters and intervention using GPT-4o is represented in the Table~\ref{tab:prompt_obtaining_commonsesne}.

\begin{table}[h]
\centering
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{l|ccccc}
\hline \hline
\multicolumn{1}{c|}{\textbf{Test set}}                             & \textit{\textbf{\begin{tabular}[c]{@{}c@{}}BERTScore \\ (F1)\end{tabular}}} & \textit{\textbf{SeSS}} & \textit{\textbf{Readability}} & \textit{\textbf{\begin{tabular}[c]{@{}c@{}}ROUGE-L \\ (Avg)\end{tabular}}} & \textit{\textbf{\begin{tabular}[c]{@{}c@{}}BLEU \\ (Avg)\end{tabular}}} \\ \hline
\textbf{Memes without text}                                        & 0.88(\colorbox{red!30}{\textbf{-.03}})                                                                        & 0.64(\colorbox{red!30}{\textbf{-.07}})                    & 36.76(\colorbox{red!30}{\textbf{-7.31}})                         & 0.27(\colorbox{red!30}{\textbf{-.08}})                                                                        & 0.05(\colorbox{red!30}{\textbf{-.03}})                                                                   \\
\textbf{Memes with text}                       & 0.89(\colorbox{red!30}{\textbf{-.02}})                                                                       & 0.69(\colorbox{red!30}{\textbf{-.09}})                   & 46.36(\colorbox{green!30}{\textbf{+2.98}})                         & 0.28(\colorbox{red!30}{\textbf{-.08}})                                                                        & 0.05(\colorbox{red!30}{\textbf{-.03}})                                                                    \\
\textbf{ICMM}                                              & 0.85(\colorbox{red!30}{\textbf{-.02}})                                                                        & 0.27(\colorbox{red!30}{\textbf{-.04}})                  & 34.07(\colorbox{red!30}{\textbf{-11.50}})                         & 0.10(\colorbox{red!30}{\textbf{-.01}})                                                                       & 0.007(\colorbox{red!30}{\textbf{-.001}})                                                                   \\\hline \hline
\end{tabular}
}
\caption{\footnotesize Result for intervention generation for different test sets using randomly selected commonsense parameters.}
\label{tab:result_random_commonsense}
\end{table}





\begin{table}[!htp]\centering

\scriptsize
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{l|c|cccc}\hline\hline
\multirow{2}{*}{\textbf{Used model}} &\multirow{2}{*}{\textbf{Method}} &\textbf{BERTScore (F1)} &\textbf{SeSS} &\textbf{Rouge-L (Avg)} \\\cline{3-5}
& &\multicolumn{3}{c}{\textbf{Memes without text}} \\\hline
\multirow{2}{*}{\texttt{Idefics-9B}} &\textbf{\textsc{MemeSense} (\textit{random ICL}) } &0.89 &0.69 &0.31 \\
&\textbf{\textsc{MemeSense} (\textit{combined ICL})} &0.9 &0.71 &0.34 \\\cline{2-5}
\multirow{2}{*}{\texttt{OpenFlamingo-9B}} &\textbf{\textsc{MemeSense} (\textit{random ICL}) } &0.88 &0.67 &0.29 \\
&\textbf{\textsc{MemeSense} (\textit{combined ICL})} &0.9 &0.7 &0.32 \\\cline{2-5}
& &\multicolumn{3}{c}{\textbf{Memes with text}} \\\cline{2-5}
\multirow{2}{*}{\texttt{Idefics-9B}} &\textbf{\textsc{MemeSense} (\textit{random ICL}) } &0.9 &0.75 &0.33 \\
&\textbf{\textsc{MemeSense} (\textit{combined ICL})} &0.91 &0.77 &0.36 \\\cline{2-5}
\multirow{2}{*}{\texttt{OpenFlamingo-9B}} &\textbf{\textsc{MemeSense} (\textit{random ICL}) } &0.89 &0.74 &0.32 \\
&\textbf{\textsc{MemeSense} (\textit{combined ICL})} &0.91 &0.77 &0.35 \\\cline{2-5}
& &\multicolumn{3}{c}{\textbf{ICMM data}} \\\cline{2-5}
\multirow{2}{*}{\texttt{Idefics-9B}} &\textbf{\textsc{MemeSense} (\textit{random ICL}) } &0.85 &0.27 &0.1 \\
&\textbf{\textsc{MemeSense} (\textit{combined ICL})} &0.86 &0.3 &0.1 \\\cline{2-5}
\multirow{2}{*}{\texttt{OpenFlamingo-9B}} &\textbf{\textsc{MemeSense} (\textit{random ICL}) }&0.85 &0.26 &0.09 \\
&\textbf{\textsc{MemeSense} (\textit{combined ICL})} &0.85 &0.29 &0.1 \\
\hline \hline
\end{tabular}}
\caption{\footnotesize Comparative results of \textsc{MemeSense} using other models.}\label{tab:results_other_models}
\vspace{-0.3cm}
\end{table}

% \section{Example Appendix}
% \label{sec:appendix}


\begin{table*}[t]
\tiny
    \centering
    \renewcommand{\arraystretch}{1} % Adjust row height for readability
    % \setlength{\tabcolsep}{10pt} % Adjust column spacing
    \begin{tabular}{|p{0.08\textwidth}|p{0.3\textwidth}|p{0.3\textwidth}|p{0.3\textwidth}|}
        \hline
        \centering
         & 
        \centering\includegraphics[width=0.4\columnwidth, height=0.4\columnwidth]{images/e4d8825f-2804-4327-a73e-b82794bfdc05.jpg} & 
        \centering\includegraphics[width=0.4\columnwidth, height=0.4\columnwidth]{images/meme_with_text_highest_scored_meme.pdf} &
        \includegraphics[width=0.4\columnwidth, height=0.4\columnwidth]{images/ICMM_highest_scored_meme.pdf}
        \\\hline
        Direct Prompting & \sethlcolor{red!30}\hl{The meme is a popular form of communication that has been around for centuries. It is a way to express ideas, opinions, and emotions through images and text. Memes can be funny, serious, or even offensive. They can be used to spread information, make a point, or just entertain.} & The meme is a humorous image that depicts a woman pouring milk into a container. The text on the image reads, “Her: “Just make sure you don’t finish inside of me.” Me inside of her 2 minutes ago.” The meme is a play on the phrase “just make sure you don’t finish inside of me,” which is often used to indicate that a person should use a condom during sex. The meme is humorous because it suggests that the woman is concerned about the man finishing inside of her, but the man is actually inside of her at the time. &  The meme is a picture of a man with the caption “when the school shooter finally leaves and the hottest girl in class is laying dead on the floor”. The meme is making fun of the school shooter and the girl who is dead. The meme is also making fun of the fact that the girl is dead. \\
        \hline
        Direct Prompting (w. commonsense) & In the image, a person is sitting on a chair, holding a hot dog on a stick The hot dog is being cooked on a grill, and the person is enjoying the process There are two other hot dogs on the grill, one located closer to the person and the other further away In addition to the hot dogs, there are two marshmallows on the grill, one near the person and the other further away  \sethlcolor{red!30}\hl{The overall atmosphere appears to be a fun and relaxed outdoor gathering.} & The meme is inappropriate and violates community guidelines against explicit or adult content. It is not suitable for general audiences and may be offensive or uncomfortable to some viewers. &  The meme is a screenshot of a man smiling and posing for the camera. The text on the image reads "When the school shooter finally leaves and the hottest girl in class is laying dead on the floor its free real estate". The combination of the text and image attempts to make a joke about a potentially tragic situation by using dark humor.
The meme references a school shooting a highly sensitive and traumatic subject. Making jokes about such an event can be perceived as trivializing a grave and distressing experience potentially causing harm to those affected.
 \\
        \hline
        Random ICL & \sethlcolor{red!30}\hl{Celebrating individuality and embracing diversity enhances understanding and acceptance among us all.} & This meme is vulgar, inappropriate, and potentially harmful, as it promotes a misogynistic view of sexual consent. & The meme should not be posted publicly due to its insensitive and inappropriate content, which could offend or disturb certain audiences. \\
        \hline
        Image anchored ICL  & This meme humorously highlights the playful creativity and shared \sethlcolor{red!30}\hl{sense of humor in relationships, encouraging laughter and bonding over shared jokes.} & This meme is vulgar, inappropriate, and potentially harmful, as it promotes a misogynistic view of sexual consent. & The meme should not be posted publicly as it trivializes the trauma of school shootings and can be distressing to individuals who have experienced similar events. It is inappropriate to use humor in this context, as it can be perceived as insensitive and offensive. \\
        \hline
        Commonsense anchored ICL & The image features a man holding a hot dog on a stick, with a marshmallow on one end and a penis on the other This meme is disrespectful and inappropriate, as it makes light of a \sethlcolor{red!30}\hl{serious issue, such as erectile dysfunction, and uses it to create a humorous or entertaining situation.} It is important to be aware of the impact of such memes and to promote respectful and inclusive communication. & The meme should not be posted publicly due to its vulgar and suggestive content, which could be offensive to many viewers. & The meme appears to be inappropriate for public posting due to its insensitive and offensive content. It trivializes the trauma of school shootings and could be distressing to individuals who have experienced similar events. The use of humor in this context may be perceived as insensitive and offensive, particularly considering the serious nature of school shootings. The subject matter could be perceived as a violation of public sensitivity and a discussion of serious topics in a public context. \\
        \hline
        MemeQA & The meme should be restricted due to its content and content. It is inappropriate for public posting and should not be shared publicly. & 
        
        The meme should be restricted due to its content and inappropriate content.
        
         & The meme should not be posted publicly due to its potential to be offensive to some audiences. \\
        \hline
        MemeGuard & The meme is making a claim that the person is having a barbecue with a hot dog and marshmallows, but instead of a traditional hot dog, there is a penis-shaped hot dog. \sethlcolor{red!30}\hl{ The meme is likely meant to be humorous and playful, as it takes a common outdoor activity like grilling and adds a provocative twist to it.} & The meme is harmful because it reinforces negative stereotypes about women and their sexuality. It suggests that women are concerned about pregnancy and STIs, and that they are not interested in sexual pleasure. It also suggests that men are not concerned about these issues and are only interested in their own pleasure. This is harmful because it perpetuates the idea that women are not equal to men in terms of sexuality and that they are not entitled to sexual pleasure. & This meme contains highly inappropriate and offensive content, trivializing school shootings and making an unethical implication about victims. This kind of meme should be critically examined and condemned rather than shared \\
        \hline
        \textsc{MemeSense} (Random ICL) & The meme should be restricted from public posting due to its vulgar and suggestive nature, which could be perceived as offensive or inappropriate. & The meme should not be posted publicly due to its vulgarity, sexual content, and inappropriate humor. & The meme should not be posted publicly due to its insensitive and potentially traumatizing content. It trivializes a serious issue and may cause distress to individuals who have experienced similar events. \\
        \hline
        \textsc{MemeSense} (Image anchored ICL)  & The meme should not be posted publicly due to its vulgar and suggestive nature, which could be perceived as offensive or inappropriate.
 & The meme should not be posted publicly due to its vulgar, sexual, and inappropriate nature. It could be perceived as offensive and disrespectful to many audiences. & The meme should not be posted publicly due to its insensitive and inappropriate content. It trivializes a serious and sensitive topic, which can be distressing to individuals who have experienced school shootings or related trauma. \\
        \hline
        \textsc{MemeSense} (Commonsense anchored ICL) & The meme should be restricted from public posting due to its vulgar and suggestive content, which could be perceived as offensive or inappropriate. & The meme should not be posted publicly due to its vulgar and suggestive content, which could be offensive to many viewers. & The meme should not be posted publicly due to its insensitive and potentially traumatizing content. It trivializes a serious issue and may cause distress to individuals who have experienced similar events. \\
        \hline
        \textsc{MemeSense} (Combined) & The meme should be restricted from public posting due to its vulgar and suggestive content, which could be perceived as inappropriate. & The meme should not be posted publicly due to its vulgar and suggestive content, which could be offensive to many audiences. It also uses humor that could be perceived as belittling and inappropriate. & The meme should not be posted publicly due to its insensitive and potentially traumatic references to school shootings. It trivializes a serious issue and may cause distress to individuals who have experienced similar events. \\
        \hline
        Ground truth annotation & The meme should be restricted in certain contexts due to its potentially suggestive content, which might be perceived as vulgar or inappropriate, particularly in public or professional settings. & The meme should not be posted publicly due to its vulgar and sexually suggestive nature, which may be offensive and inappropriate for general audiences. & Joking about tragic events such as school shootings and the loss of innocent lives is insensitive and inappropriate. We should be mindful of the impact our words can have on others and strive to promote empathy and understanding. \\
        \hline
    \end{tabular}
    \caption{\footnotesize Generated intervention using different approach for (1) a meme without text, (2) a meme with text, (3) a meme from ICMM data. Incorrect justifications are \sethlcolor{red!30}\hl{highlighted}.}
    \label{tab:representative_example_generated_intervention}
\end{table*}




\begin{table*}
\scriptsize
\tcbset{
  % colback=blue!5!white,
  % colframe=blue!75!black,
  fonttitle=\bfseries,
  boxrule=0.5mm,
  width=\textwidth,
  arc=4mm,
  auto outer arc,
  boxsep=2mm,
  % left=5mm,
  % right=5mm,
  % top=5mm,
  % bottom=5mm,
}
\begin{tcolorbox}[title=Prompt used for obtaining commonsense parameters]

As an AI assistant for social media content moderation, your role is to analyze and prevent the posting of harmful, offensive, hateful, vulgar, misogynistic, or otherwise inappropriate content on public platforms.\\

Task: Carefully analyze and interpret the given image, paying close attention to both visual elements and any text present.\\

1. Meme Description:
Provide a detailed description of the meme, including any text, imagery, or symbolic references.\\

2. Commonsense Analysis:\\
* Assess whether the meme has the potential to be perceived as vulgar, harmful, or unethical.\\
* Identify and categorize up to 5 specific commonsense parameters from the predefined list below.\\
* Each selected category should be explicitly justified with a brief explanation.\\

Predefined Commonsense Categories (Select Up to 5):\\
* Vulgarity: Use of obscene language, inappropriate tone, or implied innuendos.\\
* Violence: Depictions of harm, coercion, or references to physical aggression.\\
* Sexual Content: Explicit or suggestive imagery, innuendos, or nudity.\\
* Hate Speech: Insulting, harassing, or offensive content targeting individuals or groups.\\
* Body Shaming: Objectification, fat-shaming, or insensitive references to disabilities.\\
* Cultural Sensitivity: Stereotypes, appropriation, or misrepresentation of cultural elements.\\
* Mental Health Impact: Content that manipulates emotions, triggers trauma, or exploits grief.\\
* Humor Appropriateness: Use of humor in a way that mocks, belittles, or distorts serious topics.\\
* Misinformation: Spreading misleading narratives, false claims, or deceptive dialogue.\\
* Substance Abuse: References to drugs, alcohol, or promoting substance use.\\
* Child Exploitation: Content that inappropriately depicts or exploits children.\\
* Public Decorum \& Privacy: Violations of privacy, public sensitivity, or personal reputation concerns.\\
* Stereotyping: Generalizations that reinforce racial, gender, or societal biases.\\
* Misogyny: Content promoting gender-based discrimination, sexism, or demeaning women.\\
* Religious Sensitivity: Content that disrespects religious beliefs, symbols, or historical context.\\

3. Intervention Recommendation:\\
* If the meme is deemed inappropriate, justify why it should not be posted publicly.\\
* If the content is safe, confirm its appropriateness.\\

Response Format:\\

Meme Description:\\
<Provide a detailed description of the meme, including text and images.>\\

Commonsense Analysis:\\
- **[Category Name]**: [Justification]\\
- **[Category Name]**: [Justification]\\
- **[Category Name]**: [Justification]\\

Intervention Recommendation:\\
<Explain whether the meme should be restricted and why.>
\end{tcolorbox}
\caption{\label{tab:prompt_obtaining_commonsesne} \footnotesize Prompt to generate the ground-truth commonsense and interventions.}
\end{table*}


\section{\textsc{MemeSense} sensitivity analysis}

In addition to the ablation studies presented in Table~\ref{tab:result_without_commonsense}, we conduct a sensitivity analysis to assess the impact of variations in the commonsense information provided to the model. Specifically, we evaluate how \textsc{MemeSense} (combined) performs when supplied with randomly selected commonsense knowledge during inference. This experiment aims to understand the model’s sensitivity to incorrect or unrelated commonsense attributes.

\noindent As shown in Table~\ref{tab:result_random_commonsense}, we observe a noticeable decline in performance across key metrics when randomly selected commonsense information is used. In particular, the semantic similarity score decreases by approximately 9\%, indicating that misattributed commonsense knowledge can significantly affect the model’s final outcome. The decline is also reflected in BERTScore, ROUGE-L, and BLEU, demonstrating the reliance of \textsc{MemeSense} on relevant commonsense reasoning for effective intervention generation. Interestingly, readability exhibits a slight improvement for memes with text, which could be attributed to the increased linguistic diversity introduced by the random commonsense selection. These findings highlight the importance of precise commonsense attribution in ensuring robust and reliable meme interpretation.

\section{Intervention quality measurement}
\label{appendix:intervention_quality_measurement}
\vspace{-0.5cm}
\textcolor{black}{
\paragraph{Measuring argument quality.} We aim to measure the argument characteristic of the generated response commonly used for measuring quality of online \textit{counterspeech} \cite{saha-etal-2024-zero}. We use a \texttt{roberta-base-uncased} model\footnote{\url{https://huggingface.co/chkla/roberta-argument}} finetuned on the argument dataset \cite{stab-etal-2018-cross}. Given this model, we pass each generated intervention through the classifier to predict a confidence score, which would denote the argument quality.
We obtain confidence scores of 0.67, 0.74, 0.79 for the memes without texts, memes with text, and the ICMM dataset respectively suggesting strong argument quality of the generated interventions.
\paragraph{Correlation with human judgments.} While we present most of our results with automatic metrics, it is important to understand if they correlate with human judgments. We took two metrics -- BERTScore (F1) and ROUGE-L (Avg). For each metric, we randomly extract 25 samples from the prediction set. We present these to human annotators (researchers in this domain) and ask them to rate the quality of intervention from 1-5, 5 being the best and 1 being the worst. The Spearman’s rank correlations between the human judgments (ordinal) and the automated metrics (continuous) are 0.58 and 0.49 respectively which indicates moderate to high correlation\footnote{\url{https://datatab.net/tutorial/spearman-correlation}}. Given the subjective nature of the task, these results highlight a substantial consistency between automated metrics and human judgments, affirming their reliability.}

\section{Additional experimental settings}

\subsection{Baselines}

In Table~\ref{tab:prompt_memeguard} we demonstrate the different prompts used for the \textbf{MemeGuard} baseline. The hyperparameters for the experiments with this baseline are noted in Table~\ref{tab:hyperparameters_memeguard}. 


\begin{table*}[h]\centering
\scriptsize
\begin{tabular}{l|p{0.7\textwidth}}\toprule
\cellcolor[HTML]{d0e0e3}\textbf{Method} & \cellcolor[HTML]{d0e0e3}\textbf{Prompt} \\
\textbf{Direct prompting} &<Meme> Analyze the meme thoroughly, considering its message, symbolism, cultural references, and possible interpretations. Identify any implicit or explicit harm, misinformation, or reinforcement of negative stereotypes. Based on this analysis, generate strategic interventions to discourage the spread or creation of such content.
These interventions should be precise, contextually relevant, and designed to effectively deter users from posting similar memes. They may include subtle deterrents, educational messaging, content reformulation, or alternative framing that neutralizes harmful intent. Ensure responses are concise, non-repetitive, and avoid redundant explanations.
Ensure the response should not exceed 50 words. \\\hline
\textbf{Direct prompting with commonsense} &<meme> Analyze the meme thoroughly, considering its message, symbolism, cultural references, and possible interpretations. Identify any implicit or explicit harm, misinformation, or reinforcement of negative stereotypes. Based on this analysis, generate strategic interventions to discourage the spread or creation of such content.
These interventions should be precise, contextually relevant, and designed to effectively deter users from posting similar memes. They may include subtle deterrents, educational messaging, content reformulation, or alternative framing that neutralizes harmful intent. Ensure responses are concise, non-repetitive, and avoid redundant explanations.
The common sense parameters associated with the meme is as follows: $\{common\_sense\}$
Ensure the response should not exceed 50 words. \\\hline
\textbf{MemeMQA} &<meme>Analyze this meme and generate a caption that enhances its humor, sarcasm, or irony. Do not filter for offensiveness—prioritize humor, satire, or dark humor as needed. The caption should be punchy, relatable, and aligned with the meme's tone. \\\hline
\textbf{ICL} & <meme> As an AI assistant tasked with social media content moderation, your role is to prevent harmful, offensive, hateful, vulgar, misogynistic, or unethical content from being posted on public platforms.\textbackslash n \textbackslash n Your Task: A toxic meme has the description below along with few commonsense parameters which assess whether the meme has the potential to be perceived as vulgar, harmful, or unethical. Write an intervention for the this toxic meme to discourage user posting such memes based on provided knowledge. $\{commonsense\_parameters\}$ \textbackslash n \textbackslash n $\{examples\}$ \\
\bottomrule
\end{tabular}
\caption{\footnotesize Prompt used for different methods. method.}\label{tab:prompt_for_baselines}
\end{table*}




\section{Results for different models}
\label{appendix:results_other_model}

In the Table~\ref{tab:results_other_models}, we show the comparative results of \textsc{MemeSense} using different models (\texttt{Idfics-9B} and \texttt{OpenFlamingo-9B}). Here we use the annotated data mentioned in \ref{sec:stageI}, and the retrieval of in-context exemplars mentioned in the Section \ref{sec:stageII} to train the cognitive shift vectors (mentioned in the Section \ref{sec:stageIII}) using the two models. Then we perform the inference using trained cognitive shift vectors. We observe a similar pattern with these two models. Morever, \texttt{Idefics-9B} showing overall superior performance than \texttt{OpenFlamingo-9B}.  





\end{document}
