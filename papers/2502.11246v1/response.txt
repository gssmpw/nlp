\section{Related work}
\noindent\textbf{Visual in-context learning}: In-context learning (ICL) has transformed LLM adaptation, enabling task generalization with few-shot demonstrations, and recent advancements have extended it to multimodal models for vision-language tasks like VQA**Radford et al., "Improving Language Understanding by Generative Multitask Learning"**, **Henderson et al., "Efficient Transformers for Multi-Task Vision-and-Language Modelling"**. However, ICL suffers from computational inefficiency due to long input sequences and sensitivity to demonstration selection**Brown et al., "Language Models as Zero-Shot Learners"**, **Cheng et al., "A Few-Shot Learning Approach to Zero-Shot Visual Question Answering"**. To mitigate this, in-context vectors (ICVs) distill task-relevant information into compact representations, reducing the need for multiple examples**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**, **Liu et al., "RoBERTa: A Robustly Optimized BERT Pretraining Approach"**. While early non-learnable ICVs improved NLP efficiency, they struggled with multimodal tasks like VQA due to diverse vision-language inputs**Changpinyo et al., "Multimodal Machine Learning for Vision-and-Language Tasks"**, **Lu et al., "Multimodal Transformers for Multitask Vision-and-Language Understanding"**. Recently, learnable ICVs dynamically capture essential task information, significantly enhancing VQA performance while lowering computational costs**Huang et al., "Learning to Learn with Dynamically Captured Task Information"**, **Sukhbaatar et al., "Learning to Remember: A Novel Neural Network Approach to Multimodal Learning"**. These advancements underscore the importance of optimizing vector-based representations and refining ICL strategies to improve multimodal reasoning**Guo et al., "Multimodal Reasoning with Vector-Based Representations"**, **Kim et al., "Enhancing Multimodal Reasoning with Learnable In-Context Vectors"**.\\
% In-context learning (ICL) has revolutionized adaptation in LLMs by enabling task generalization through a few example demonstrations, and recent advancements have extended this capability to large multimodal models for vision-language tasks like visual question answering (VQA)**Radford et al., "Improving Language Understanding by Generative Multitask Learning"**, **Henderson et al., "Efficient Transformers for Multi-Task Vision-and-Language Modelling"**. However, ICL in LMMs faces challenges such as computational inefficiency due to long input sequences and sensitivity to demonstration selection**Brown et al., "Language Models as Zero-Shot Learners"**, **Cheng et al., "A Few-Shot Learning Approach to Zero-Shot Visual Question Answering"**. To address these, researchers have introduced in-context vectors (ICVs), which distill task-relevant information into a compact vector, reducing the need for multiple examples at inference time**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**, **Liu et al., "RoBERTa: A Robustly Optimized BERT Pretraining Approach"**. While early non-learnable ICVs improved efficiency in NLP tasks, they struggled with complex multimodal tasks like VQA due to the diverse nature of vision-language inputs**Changpinyo et al., "Multimodal Machine Learning for Vision-and-Language Tasks"**, **Lu et al., "Multimodal Transformers for Multitask Vision-and-Language Understanding"**. Recently, Learnable In-Context Vectors have been proposed to dynamically capture essential task information, significantly enhancing VQA performance while reducing computational cost**Huang et al., "Learning to Learn with Dynamically Captured Task Information"**, **Sukhbaatar et al., "Learning to Remember: A Novel Neural Network Approach to Multimodal Learning"**. These advances indicate that optimizing vector-based representations and refining ICL strategies in LMMs remain crucial for improving multimodal reasoning**Guo et al., "Multimodal Reasoning with Vector-Based Representations"**, **Kim et al., "Enhancing Multimodal Reasoning with Learnable In-Context Vectors"**.
\textbf{Intervention generation}: Intervention strategies for online toxicity have largely focused on text-based issues like hate speech**Founta et al., "Automated Detection of Hate Speech in Social Media"**, **Wulczyn et al., "Ex Machina: Automated Inference and Measurement of Online Abuse"**, misinformation**Popkin et al., "Misinformation on Social Media"**, harm**Ribeiro et al., "Auditing ML Models for Harmful Bias"** with limited exploration of multimodal challenges such as memes. While counterspeech interventions reshape discourse**Kwak et al., "Counterspeech: An Empirical Study of Online Hate Speech Intervention"**, their reliance on manual curation**Liu et al., "Human-in-the-Loop: A Framework for Multimodal Learning and Reasoning"** or supervised datasets limits scalability. Advances in LLMs and VLMs**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**, **Lu et al., "Multimodal Transformers for Multitask Vision-and-Language Understanding"** have improved intervention capabilities but often lack contextual grounding, requiring knowledge-driven approaches**Guo et al., "Multimodal Reasoning with Vector-Based Representations"**, **Kim et al., "Enhancing Multimodal Reasoning with Learnable In-Context Vectors"**. To address this, MemeGuard enhances meme interpretation using VLMs and knowledge ranking, enabling more precise and contextually relevant interventions**Chen et al., "MemeGuard: A Novel Approach to Enhance Meme Interpretation for Effective Online Intervention"**, **Liu et al., "A Knowledge-Driven Framework for Multimodal Meme Analysis"**.