\section{Related Work}
\label{sec:Related Work}

\subsection{Medical Image Segmentation}

Medical image segmentation aims to assign a closed-set class label to each pixel. UNet **Ronneberger, "U-Net: Deep Learning for Biological Image Segmentation"** has gradually become the preferred model in the medical segmentation field since it was first proposed in 2015. Such success is mainly attributed to its unique structural design, especially the encoder-decoder structure and the skip connection mechanism, which endows the model with strong detail recovery. Subsequently, it is enhanced by exploring dense skip connections **Oktay, "Attention U-Net: Learning a Self-Similar Patch Representation for Image Segmentation"**, multi-scale receptive fields **Hou, "Fully Convolutional Neural Networks for Change Detection of Multi-Temporal Images"** and global information **Zhou, "Unet++: A Nested U-Net Structure for Medical Image Segmentation"**. 
In addition to 2D medical scenes, UNet has been extended to 3D medical scenes, such as MRI and CT, by replacing 2D convolutions with 3D convolutions ** Milletari, "V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation"**. Another similar approach is VNet **Milletari, "V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation"**, which also employs the encoder-decoder structure. The difference with 3D-UNet is that VNet utilises a convolutional layer instead of the pooling layer for downsampling, thereby mitigating information loss. 

More recent studies **Anwer, "A Patch-Based Variational Autoencoder for Medical Image Denoising"** have explored methods based on vision transformer (ViT) **Dosovitskiy, "An Image is Not Worth More Than 16X16 Words: Transformers for Image Recognition at Scale"** for the medical segmentation task, leveraging their capability for long-range modelling. Other recent approaches **Pan, "Contrastive Multiview Learning with Random Feature for Image and Video Analysis"** can enhance the segmentation quality by introducing the Diffusion Probabilistic Model (DPM) **Sojoudi, "A Variational Autoencoder with a Recurrent Neural Network in the Latent Space for Medical Image Denoising"**. 
Although the fully supervised segmentation techniques described above exhibit solid performance, they require a large number of voxel-based annotations that are difficult and expensive to obtain in real medical scenarios. 
One way to mitigate the need for such annotations is based on the development of semi-supervised learning methods that require much smaller sets of annotated data and large sets of un-annotated data. We review semi-supervised learning methods below. 


% \gustavo{It may sound strange for reviewers that we only discuss until VNet in this subsection. Should we justify why we don't discuss more modern segmentation methods? Or should we discuss more modern methods?}
% \yanyan{Do I need to discuss the Segment Anything Model in this part?}
% Following prior semi-supervised medical image segmentation works **Srinivas, "CUTMix: Cutting Mixtures of Multi-Exemplar for Robust Training"**, 3D-UNet is employed as the backbone network on LA and Pancreas-CT datasets, while VNet serves as the base model on the BraTS19 dataset \gustavo{This sentence should move to the next section since it talks about semi-supervised learning, right?}. \yanyan{Can I remove this sentence, because it is also stated in Sec. 4.2 Implementation Details? }





\subsection{Semi-supervised Medical Image Segmentation}

Significant advancements have been made in semi-supervised medical image segmentation **Berthelot, "MixMatch: A Holistic Approach to Semi-Supervised Learning"**. Current models primarily adopt consistency regularisation strategies to leverage unlabelled information **Liu, "Self-Supervised Learning for 3D Medical Image Segmentation"**. The underlying idea behind these strategies is that the model's predictions for unlabelled samples should remain consistent under various perturbations **Sajjadi, "Regularization with Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning"**. The Mean Teacher framework, which explores weak and strong augmentation strategies, is quite effective in the implementation of this idea. Based on this framework, various weak and strong augmentation techniques have been proposed to generate prediction disagreements. **Liu, "Self-Supervised Learning for 3D Medical Image Segmentation"** apply different data augmentation techniques, such as Gaussian noise and contrast variation, on the input data. **Zhang, "FSDNet: A Fast Supervised-unsupervised Domain Adaptation Network for 3D Medical Images"** adjust the spatial context of the input samples to enrich their diversity. Also, **Berthelot, "MixMatch: A Holistic Approach to Semi-Supervised Learning"** and **Sajjadi, "Regularization with Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning"** focus on inducing prediction inconsistencies by adding perturbations at the feature level.


Despite the promising performance of well-designed data augmentation techniques, the pseudo-labels generated by teacher networks still contain a fair amount of noise, which hinders the model's segmentation capability. Recent works **Tajbakhsh, "Surrogate Labeles from Semantic Segmentation: A Case Study in Ultrasound Analysis"** argue that incorporating additional supervised information would help mitigate this problem. One of the representative efforts is the multi-teacher embedding approach **Zhou, "Deep Adversarial Learning for Robust Appearance and Shape Transfer"**. The core idea of this kind of approach lies in the generation of pseudo-labels from different perspectives. To ensure diversity, the teacher models typically employ different initialization parameters and update mechanisms. However, not all perspectives necessarily improve the accuracy of pseudo-labels, and sometimes conflicting labels may emerge. It is thus difficult to utilise different perspectives from unlabelled information to improve segmentation performance in complex situations. 

\begin{figure*}[!t]
\centering
\includegraphics[width=6.5in]{whole_architecture.pdf}
\caption{The architecture of the proposed model. Based on the teacher-student structure, the Cooperative Rectification Learning Network (CRLN) consists of two stages. In the learning stage, multiple category prototypes are built and initialised. Subsequently, the Dynamic Interaction Module (DIM) implements pairwise interactions, as well as spatial-aware and cross-class aggregation between prototypes and the semantics of the labelled data to obtain the holistic relationship map $\mathscr{M}(x)$ which adaptively improves the segmentation quality of $\hat{y}$ with~\eqref{eq:rectification_student}. By minimising the deviation between predictions and labels, the proposed CRLN effectively learns valuable category prototypes and understands how to use them for voxel-level correction. In the rectification stage, the learned category prototypes serve as prior knowledge to rectify the pseudo-labels $\bar{y}$. After rectification, the higher-quality pseudo-labels $\bar{y}_{r}$ are used as supervision signals. Moreover, the Collaborative Positive Supervision (CPS) mechanism constructs unassertive centres by integrating learned category prototypes and category mean representations, allowing for better contrastive learning ($\ell_{cp}(\cdot)$ in ~\eqref{eq:cp} of representations with lower predictive confidence in the student network. This empowers the model to distinguish uncertain regions.
% \gustavo{I changed many of the variables in the text -- can you update the figure with the new nomenclature? This Figure and Figure 2 are too similar.  Can you in this figure provide new points about CRNL, DIM, and CPS ?} \yanyan{I have updated the figure and added some new points. The letters "$\hat{y}_l$", "$\hat{y}_{lr}$", "$\bar{y}_{u}$" and "$\bar{y}_{ur}$"have been retained for ease of representation. Is this version appropriate? Do these letters need to be removed? }\gustavo{Sorry, I think we'll need to change this one more time to reflect the new nomenclature used in the paper.}\yanyan{I have updated it. Since the prototypes in the rectification stage are updated by the EMA of the prototypes learned in the learning stage, $P_t$ is used to denote the prototypes in the rectification stage.}} 
\label{fig:network_structure}}
\end{figure*}

Another technique to suppress the negative effects of noise in pseudo-labels is to filter out or reduce the weight of samples classified as uncertain during training **Saugat, "Uncertainty-based Active Learning for 3D Medical Image Segmentation"**. UA-MT estimates the uncertainty of the teacher's prediction with the classification entropy and uses only reliable (i.e., low-entropy) predictions to supervise the student network **Patel, "A Novel Uncertainty-Aware Semi-Supervised Framework for 3D Medical Image Segmentation"**. **Wang, "Uncertainty Estimation for Deep Neural Networks: A Survey"** propose to use multi-scale prediction discrepancy as a measure of uncertainty and then treat the uncertainty score as a pixel-level coefficient to reduce the loss contribution from the uncertain regions. **Fang, "Deep Uncertainty Estimation for 3D Medical Image Segmentation"** perform uncertainty estimation via subjective logic. Furthermore, **Liu, "Uncertainty-based Active Learning for 3D Medical Image Segmentation"** estimate the uncertainty of the teacher's prediction with the classification entropy and uses only reliable (i.e., low-entropy) predictions to supervise the student network. 

\begin{figure*}[!t]
\centering
\includegraphics[width=6.5in]{whole_architecture.pdf}
\caption{The architecture of the proposed model. Based on the teacher-student structure, the Cooperative Rectification Learning Network (CRLN) consists of two stages. In the learning stage, multiple category prototypes are built and initialised. Subsequently, the Dynamic Interaction Module (DIM) implements pairwise interactions, as well as spatial-aware and cross-class aggregation between prototypes and the semantics of the labelled data to obtain the holistic relationship map $\mathscr{M}(x)$ which adaptively improves the segmentation quality of $\hat{y}$ with~\eqref{eq:rectification_student}. By minimising the deviation between predictions and labels, the proposed CRLN effectively learns valuable category prototypes and understands how to use them for voxel-level correction. In the rectification stage, the learned category prototypes serve as prior knowledge to rectify the pseudo-labels $\bar{y}$. After rectification, the higher-quality pseudo-labels $\bar{y}_{r}$ are used as supervision signals. Moreover, the Collaborative Positive Supervision (CPS) mechanism constructs unassertive centres by integrating learned category prototypes and category mean representations, allowing for better contrastive learning ($\ell_{cp}(\cdot)$ in ~\eqref{eq:cp} of representations with lower predictive confidence in the student network. This empowers the model to distinguish uncertain regions.
% \gustavo{I changed many of the variables in the text -- can you update the figure with the new nomenclature? This Figure and Figure 2 are too similar.  Can you in this figure provide new points about CRNL, DIM, and CPS ?} \yanyan{I have updated the figure and added some new points. The letters "$\hat{y}_l$", "$\hat{y}_{lr}$", "$\bar{y}_{u}$" and "$\bar{y}_{ur}$"have been retained for ease of representation. Is this version appropriate? Do these letters need to be removed? }\gustavo{Sorry, I think we'll need to change this one more time to reflect the new nomenclature used in the paper.}\yanyan{I have updated it. Since the prototypes in the rectification stage are updated by the EMA of the prototypes learned in the learning stage, $P_t$ is used to denote the prototypes in the rectification stage.}} 
\label{fig:network_structure}}
\end{figure*}