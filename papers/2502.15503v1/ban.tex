%!TEX program = pdflatex 
% !BIB program = bibtex
%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[journal]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}
\usepackage{graphicx}
\usepackage[table,xcdraw]{xcolor}
\usepackage[TABBOTCAP]{subfigure}
\usepackage{bm}
\usepackage{upgreek} 
\usepackage{amsmath}
\usepackage{breqn}
\usepackage{color}
\usepackage{cite}
\usepackage[none]{hyphenat}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amssymb}

\usepackage{multirow}


% Neurocomputing
\usepackage{float}
\usepackage{caption}
\usepackage{array}
\usepackage{booktabs}
% 设置超链接跳转
\usepackage[colorlinks,
linkcolor=blue,
anchorcolor=blue,
citecolor=blue]{hyperref} 
\usepackage{amsmath}

% 改变字体颜色
\usepackage{color,xcolor}
\usepackage{makecell}

% 智能引用
%\usepackage{hyperref}
%\usepackage{cleveref}
%
%% 跨文件引用
\usepackage{xr}
\externaldocument{figs/prediction_accuracies}

% include文件不分页
%\usepackage{newclude}

% 给表格添加注脚
\usepackage{threeparttable}
\usepackage{footnote}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{BAN: Neuroanatomical Aligning in Auditory Recognition between Artificial Neural Network and Human Cortex}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%


\author{Haidong~Wang$^*$,~Pengfei~Xiao,Ao~Liu, Jianhua~Zhang,~and Qia~Shan% <-this % stops a space
	\thanks{H. Wang, P.Xiao, A.Liu, J. Zhang and Q. Shan are from Hunan University Of Technology and Business, Changsha 410082, China (e-mail: whd@hutb.edu.cn; 2893666867@qq.com; 2496556459@qq.com; zhangjianhua6682@126.com; s540534349@163.com).}
% % <-this % stops a space
}


% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
Drawing inspiration from neurosciences, artificial neural networks (ANNs) have evolved from shallow architectures to highly complex, deep structures, yielding exceptional performance in auditory recognition tasks.
However, traditional ANNs often struggle to align with brain regions due to their excessive depth and lack of biologically realistic features, like recurrent connection.
To address this, a brain-like auditory network (BAN) is introduced, which incorporates four neuroanatomically mapped areas and recurrent connection, guided by a novel metric called the brain-like auditory score (BAS).
BAS serves as a benchmark for evaluating the similarity between BAN and human auditory recognition pathway.
We further propose that specific areas in the cerebral cortex, mainly the middle and medial superior temporal (T2/T3) areas, correspond to the designed network structure, drawing parallels with the brain's auditory perception pathway.
Our findings suggest that the neuroanatomical similarity in the cortex and auditory classification abilities of the ANN are well-aligned. 
In addition to delivering excellent performance on a music genre classification task, the BAN demonstrates a high BAS score.
In conclusion, this study presents BAN as a recurrent, brain-inspired ANN, representing the first model that mirrors the cortical pathway of auditory recognition.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords} 
Auditory Recognition, Artificial Neural Network, Brain-like Model, Neuroanatomical Similarity. 
\end{IEEEkeywords}


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\begin{figure*}
	\centering
	\includegraphics[width=6.7in]{figs/introduction.pdf}
	\caption{
		\textbf{
		Collaborative model between ANNs and neuroanatomy with brain-like auditory score (BAS).}
		Using the quantitative BAS, we draw inspiration from brain and apply this idea to inform the model of BAN. 
		BAN consists of four regions mapped to the primary auditory cortex (A1), the belt or peripheral area (Belt), the parabelt cortex (PB), and the middle and superior temporal cortex areas (T2/T3). 
		The CONV\textsubscript{V1} layer is a traditional convolutional layer responsible for preprocessing and data size reduction. 
		The RNN\textsubscript{Belt} and RNN\textsubscript{PB} and RNN\textsubscript{T2/T3} reference recurrent neural networks for modeling\cite{park2020circuit}, as detailed in Sec.~\ref{sec:cornet_s_def}. 
		The upper-right image illustrates the auditory recognition activations and predicted genre labels in the brain-aligned neural network, while the lower-right figure shows human activations and choices for the same stimulus on the far left. 
		This comparison highlights the relationship between BAN's prediction performance and brain auditory recognition responses, as explained in Sec.\ref{sec:experiments}.
	}
	\label{fig:introduction}
\end{figure*}


\section{Introduction}
% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps.
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.+

% problem: time, optimization joint
% the development of object detection
% joint detection and embeding, -> problem
% joint detection and association
% contribution: end-to-end, data preparation, training

% /data3/dong/data/brain/auditory/Music_Genre_Classification_Pytorch-master/report.pdf, 1. Music
% 一定要在路径中添加latex/bin/win32/bibtex.exe，否则参考文献为问号，且不会报错
\IEEEPARstart{H}{earing} plays a vital role in human sound recognition and is especially important for the comprehension and creation of music.
One key task in this domain is genre classification, which involves predicting the genre of a piece of music based on its audio signal.
Music genres, such as jazz, rock, and classical, serve as descriptive labels that provide high-level information about a musical piece.
As noted by previous work~\cite{tzanetakis2002musical}, genres are classes introduced by humans to categorize musical works. 
These genres are defined by shared characteristics among the music that belongs to them.
These characteristics are generally linked to specific musical instruments, rhythmic patterns, and harmonic structures.
Automating music genre classification can assist or even replace human effort in this process, making it a valuable tool in music information retrieval systems.
As Nam~\cite{nam2018deep} highlights, music genre recognition plays a important role in auditory recognition and recommendation algorithms for music streaming services.
In solving the tasks of music classification and tagging, it is essential for the computer to recognize and identify musical patterns.
The traditional framework's features are key to designing the pipeline, involving both feature engineering and classifier development.


% 之前已有的方法
When humans distinguish music, they rely on features such as the sound of instruments or rhythm. 
Teaching machines to recognize these features has traditionally involved hand-engineering using domain-specific knowledge\cite{nakai2018encoding}.
However, with the rise of deep learning, new approaches and improved performance have emerged. 
Deep learning methodologies reduce the need for extensive domain knowledge by automatically extracting features. 
Typically, deep learning models build pipelines through linear transformations, nonlinear activation functions, and optional pooling operations\cite{thompson2021training}. 
As data passes through these layers, the model learns to extract features, enabling end-to-end learning.


%
Deep learning has also made significant strides in modeling neural mechanisms within the neuroscience community~\cite{kubilius2019brain-like}. 
Remarkably, in artificial neural networks (ANNs) trained for image classification ~\cite{Deng2009ImageNet}, the middle layers can partially explain why neurons in the visual cortex's middle layer respond to specific image features~\cite{yamins2014performance,suk2012novel,gucclu2015deep,murugesan2017brain,hu2021neuroscience,wang2021memristive}. 
In addition, these networks can predict primate image classification behavior and cortical activations to some extent~\cite{rajalingham2018large,kubilius2016deep}. 
Such brain-inspired models open up exciting possibilities to brain-computer interfaces, where these modles can infer expected responses along human cortical pathway~\cite{bashivan2019neural}.


% 存在的问题
To more accurately model brain processing, relying solely on traditional visual datasets for architectural improvements is becoming increasingly impractical. 
While deeper ANNs have significantly improved performance~\cite{ILSVRC15,luo2021trajectory}, raising brain-like scores remains a challenge~\cite{rajalingham2018large,su2020incremental}. 
Additionally, in earlier stages, only certain modules could be clearly mapped to specific areas of the visual pathway, and these connections between a few visual areas and numerous complex modules in architectures like GoogleNet~\cite{szegedy2015going} or other deep network are not well-defined. 
Furthermore, as networks for audio recognition tasks achieve higher accuracy, they tend to become progressively deeper, with only a few exceptions where brain-inspired architectures are used for audio recognition~\cite{kim2024spontaneous}. 


% 我们提出的方法
To deal with the interpretability challenges of auditory recognition models, we propose that aligning artificial neural networks (ANNs) with neuroanatomy can produce more interpretable, shallower, and brain-like neural networks, which we term the brain-like auditory recognition network (BAN). 
BAN is a recurrent and shallow architecture that mimics the auditory cortical pathway, giving it a structure more closely aligned with neuroanatomy. 
In Sec.\ref{sec:data_analysis}, we introduce a method to analyze cortical data and human behavior during auditory recognition. 
In Sec.\ref{sec:bas}, we develop an innovative measurement approach for predicting neural activations and human auditory recognition choice, referred to as the brain-like auditory score (BAS).


Our researches involve a novel benchmark consisting of cortical and behavioral recordings, where BAN demonstrates excellent performance in evaluating the similarity between the model and the auditory recognition pathway, while also achieving strong results on a music genre classification task~\cite{nakai2022music}. 
These outcomes are largely attributed to BAN's brain-like structure, consistent with existing knowledge of the human cortical pathway's response to audio~\cite{TangSchrimpfLotter2018Recurrent, yin2020deep, kar2019evidence}. 
Lastly, in comparing the activations in BAN's RNN$_{\text{T2/T3}}$ layers with responses in human middle and superior temporal regions (T2/T3), we find BAN accurately infers cortical responses. 
This is also the first auditory recognition network to achieve this based on neural activations.


% You must have at least 2 lines in the paragraph with the drop letter
% (should never be an issue)

\section{Related Works} 

% 类脑视觉模型
\subsection{Brain-like Neural Network}\par

As research in computer vision and human vision advances, there has been significant growth in brain-like vision studies. 
Compared to models of the brain’s ventral stream, which have seen extensive development, image-computable models of the dorsal stream have been relatively scarce. 
Previous efforts to model the dorsal stream have involved training deep neural networks to detect motion \cite{rideaux2020but} or classify actions \cite{gucclu2017increasingly} using video inputs. 
However, these models fail to fully align with the neuroscientific understanding that the dorsal stream is responsible for object localization and action guidance, often referred to as the ``where" or ``how" visual pathway. 
More recent work focused on training a dorsal-stream model to mimic human head movements during visual exploration\cite{mineault2021your}. 
Additionally, the predictive learning is employed to train parallel pathways, leading to the emergent development of ventral-like and dorsal-like representations as a result of structural segregation\cite{bakhtiari2021functional}.


% 除了视觉的其他类脑功能
In addition to brain-like vision, numerous studies have explored brain-like approaches in other domains. 
These methods have successfully linked the structure of neural activity to computational functions in areas such as audition\cite{kell2018task}, olfaction \cite{wang2021evolving,singh2023emergent}, thermosensation\cite{haesemeyer2019convergent}, perceptual discrimination\cite{mante2013context}, facial recognition\cite{higgins2021unsupervised}, and navigation\cite{banino2018vector,cueva2018emergence}. 
Additionally, pioneering efforts have demonstrated the ability of simple brain-inspired controllers to replicate animal locomotion\cite{ijspeert2007swimming,grillner2007modeling,J2020Reproducing}, illustrated how biomechanics can shape neural representations of movement\cite{2013Preference}, and uncovered similarities between the representation of movement in artificial and biological neural networks\cite{2000Direct,2020A,2015A}. 
Our study leverages brain-like mechanisms, implemented through a recurrent network, to effectively extract audio features from multiple music fragments. 
Furthermore, we aim to design a brain-like network that not only achieves a high brain-like auditory score (BAS) but also surpasses existing recognition models on the Music Genres dataset~\cite{nakai2022music}.

\par


% 人类的音频识别 
\subsection{Auditory Recognition}

Recent researches utilizing artificial neural networks (ANNs) have shed light on the principles based on the development of sensory functions in cortex\cite{richards2019deep,hassabis2017neuroscience,kell2019deep,saxe2021if}. 
It has been suggested that brain-like sensory encoding can emerge as a by-product of optimizing ANNs to process natural stimuli.
For instance, ANN models trained to classify natural images have been shown to predicte visual cortical activations and even influence real neuron responses beyond their natural limits\cite{cadieu2014deep,yamins2014performance,bashivan2018neural}. 
Similarly, a trained ANN to classify music was able to mimic human auditory cortical activations\cite{kell2018task}, suggesting that task-specific learning could serve as a efficient method for modeling auditory cortex functions. 
Additionally, research has explored how music-selectivity in neural circuits might develop as a by-product of adapting to natural audio processing\cite{hauser2003evolution,trainor2015origins,honing2015without,mlynarski2019ecological}, with the statistical patterns of natural sounds potentially shaping the brain’s innate musical foundation. 
To further align brain-like models with human intelligence and neurosciences, the brain-like score has been introduced as an integrated benchmark for evaluating such models~\cite{schrimpf2020integrative}.


% 我们的工作音频识别工作
In this work, we fully leverage the latest advancements in audio perception and music classification techniques. 
By applying these approches,  functional magnetic resonance imaging (fMRI) is processed from the Music Genres dataset ~\cite{nakai2022music} to identify cortical regions involved in music genre perception. 
Furthermore, we developed the BAS to assess the similarity between BAN activations and brain activations, providing a benchmark for evaluating their alignment.



% 网络的设计
\section{BAN: Brain-like Auditory Network}

In this part, we present the proposed BAN with three steps. 
Firstly, the design principles and aim behind our approach is outlined. 
Secondly, a detailed explanation of each component within the BAN pipeline is provided. 
Finally, the loss function used to train the BAN is introduced.


\subsection{Design Criteria}

% 2个设计准则
We designed the BAN based on the following two criteria~\cite{kubilius2018predict}:

(1) \textbf{Architecture}: Among neural networks with same recognition performance, we prioritize brain-like networks due to their interpretability and their ability to align with anatomical constraints. 
We utilize ANNs as their neurons serve as fundamental units of information processing, and all neural responses in ANNs can be directly mapped to cortical activations ~\cite{yamins2016using}. 
Additionally, recurrent connections are naturally incorporated for auditory recognition because of the temporal nature of audio sequences. 
Since responses in auditory ventral pathway also exhibit temporal characteristics, the BAN is designed to generate activations over time.


(2) \textbf{Predictivity}: 
% 预测激活
The intermediate layers and final outputs of the model that aligns with neuroanatomical constraints (neural responses) exhibit more accurate behavior. 
It enables the proposed neural network to effectively infer both brain activations and category choice.


% 预测输出
Our goal is to achieve a high level of auditory recognition similarity between ANN and human brain. 
Moreover, auditory recognition in the human brain is reflected through human decision-making. 
In the auditory recognition pathway, the primary auditory cortex (A1) serves as the initial stage for preprocessing input signals, while the Belt and PB regions integrate sound signals across spatial dimensions, and the T2/T3 regions generate predictive auditory labels~\cite{b11,b13,b14}.
We now introduce the pipeline for BAN. 


\begin{figure*}
	\centering
	\includegraphics[width=5.7in]{figs/pipeline.pdf}
	\caption{
		\textbf{
			The BAN circuitry is designed based on the human auditory cortex pathway.}
		Key cortical areas involved in auditory recognition are highlighted with orange modules. 
		Auditory neural signals are generated from the input music $F_t$ via the cochlear hair cells. 
		In the ventral pathway, A1 processes auditory representations $v_t$, and neurons in the ventral ``what" pathway (including A1, Belt, PB, and T2/T3) extract auditory features from the preprocessed neural signal $m_t$. 
		The Belt and PB are particularly important for refining features based on the temporal and rate codes $v_t$, and the output $p_t$ from PB is passed to the next module. 
		The Belt, PB, and T2/T3 regions are recurrent. 
		Finally, fully connected layers serve as the coder to generate auditory labels. 
		Solid arrows show feature flow within a single time step, while dashed black arrows indicate temporal connections.
	}
	\label{fig:pipeline}
\end{figure*}

\subsection{BAN Pipeline} \label{sec:cornet_s_def}

% 通路对齐设计的思路
Inspired by the brain’s auditory recognition pathway, we establish a neuroanatomical mapping between cortical areas and ANN layers, as illustrated in Fig.~\ref{fig:introduction}. 
To facilitate neural network comparison, this mapping by identifying the ANN layer that best corresponds to activations in a specific cortical region is created. 
Ideally, these responses are inferred by the neural network without requiring unnecessary parameters. 
BAN consists of four parts: a convolutional layer and three recurrent layers, which correspond to the auditory recognition pathway regions A1, Belt, PB, and T2/T3. 
Additionally, the recognition predictor converts the output of T2/T3 into action selection activations, as shown in Fig.~\ref{fig:pipeline}. 
This straightforward approach of explicitly segmenting brain regions is a key step in designing a brain-like auditory recognition model. 
We are also focused on discovering more generalized structures, such as a unified neural network model without distinct cortical regions or varied connections that could enhance BAS in future research.


As shown in Fig.~\ref{fig:pipeline}, we compute a decoding $c_t$ from the cochlea field based on the current input $F_t$, representing the ventral pathway responsible for extracting audio features. 
The auditory pathway selects a view $m_t$ for the cochlea response $c_t$, and this output is then transmitted to the ventral ``what" cortical pathways.



%Given the input audio sequence $A$ parameterized with appearance $\alpha _t$ including A1 and this ventral pathway, to obtain the listened audio representations $a_t$ 
%and update hidden state $h_t$ in recurrent network.
%Then, we decode the output of T2/T3 and use it to infer the audio classification $\Delta c_t$, spatial attention $a_{t+1}$ and the temporal attention $\alpha _{t+1}$ in the next moment.
%Spatial attention is driven by top-down information $\alpha _t$ and bottom-up information. 
%However, spatial attention only relies on top-down information $a_t$.
%In this case, bottom-up information only has localized impacts and relies on salience input at a position, 
%but top-down information combines global features into localized analysis. 
%The attention mechanism, which intensified with recurrence, imitates the auditory cortex~\cite{attention}.
%Then, we detail each component of the brain-like auditory recognition architecture.


The ventral pathway is responsible for recognizing sounds within the auditory field. 
The BAN (Fig.~\ref{fig:introduction}) seeks to compete with state of art methods on the BAS by converting very deep feedforward structures into a shallow and recurrent network. 
Besides, BAN is inspired by ResNets, which are among the best models on behavioral benchmarks\cite{nakai2022music} and can be viewed as unrolled recurrent structure. 
Recent research has also shown that sharing weight in recurrent neural networks is feasible without significantly compromising performance\cite{liao2016bridging}.


% 大脑映射
Moreover, BAN is explicitly designed with an anatomical mapping to cortical regions. 
While model comparison establish this mapping by identifying the layer that best corresponds to neural activations in a specific cortical region, BAN aims to provide this mapping inherently, without the need for additional parameters. 
BAN features four computational areas, modeled after the auditory regions A1, Belt, PB, and T2/T3, along with a linear class decoder that translates neuron activity in the final auditory area into behavioral decisions. 
This initial assumption of distinct areas with recurrent circuitry was our first step toward creating a shallow model. 
%In the future, we are eager to explore less rigid mappings (such as treating all components uniformly as neurons without regional distinctions) and more complex circuitry, which may further enhance model performance.


Next, we provide a detailed explanation of each component within the brain-like auditory recognition architecture.




% 初级听觉皮层
\subsubsection{Primary Auditory Modules}
Typically, processed auditory signals reach A1 in the cortical temporal lobe, which is essential for processing fundamental sound features like pitch and volume. 
The input is a data matrix represented by a time-frequency representation $ m_t $. 
In our work, this input is a Mel-spectrogram.


%The ventral pathway converts the cochlea field $f_t$ into a fixed dimension vector $m_t$ including temporal features of the listened music. 
%The network structure architecture relies on the specific experimental analysis. 
%In the architecture of ventral pathway, we implement A1 using a CNN, which is a feature extraction module in the ventral pathway.
%In addition, we implement A1 using several convolutional layers and max pooling layers. 
%These layers, which imitate A1 in humans~\cite{theoretical_neuroscience}, are shared with the dorsal pathway.
%Finally, we implement the ventral pathway by CNN that extracts music representations $c_t$.


% 每一块的细节
Each auditory area is designed with specific neural circuitry where neurons perform fundamental computations, such as convolution, addition, normalization, nonlinearity, or pooling. 
The circuitry remains consistent across all auditory areas (except for A1$_{\text{COR}}$), though the total number of neurons varies in each region. 
To manage the high computational costs, the first region, A1$_{\text{COR}}$, applies a 7 $\times$ 7 convolution with a stride of 2, followed by 3 $\times$ 3 max pooling, and then a 3 $\times$ 3 convolution.


%The Belt and PB in dorsal pathway is implemented as a recurrent neural network~\cite{brabandere2016dynamic} 
%and is used to address temporal relations between previous and next moments. 
%Let FC$(\cdot)$ indicate a fully connected layer.
%The Belt and PB module utilize temporal representation $\alpha_t$ to dynamically predict the convolutional filter $\phi_t$
%\begin{equation}
%	\left\{ \phi _t ^i \right\}_{i=1}^N = \text{FC}(\alpha_t).
%\end{equation}
%The filters with corresponding nonlinearities in $N$ convolutional layers are applied to the feature in A1. 
%Then, we apply a $1 \times 1$ convolution and a sigmoid function to convert the representation into a two-dimensional mask $d_t$.
%
%The position map of the ventral pathway is integrated over the music representations learned by the ventral pathway, which simulate the noise restraining mechanism in the audial system. 
%This can handle noise and disturbance in a real scene, because the music representation is not overwritten in LSTM when the current frame was disturbed by others. 
%Therefore, the final output of ventral pathways are depicted as
%\begin{equation}
%	m_t = \text{FC}(conc(v_t \odot d_t)),
%\end{equation}
%where $\odot$ indicates the Hadamard product that performs music feature extraction through the representation mask, 
%and $conc$ means a concatenation operator that concatenates all rows of a matrix into a vector.

% 带状区和伞状区
\subsubsection{Belt and PB Modules}

% 带状+伞状区
The Belt and Parabelt (PB) areas are key regions within the auditory cortex, primarily responsible for processing complex auditory information. 
The RNN$_{\text{Belt}}$, surrounding Conv$_{\text{A1}}$, represents the first level of auditory cortex beyond Conv$_{\text{A1}}$ and receives direct inputs $ v_t $ from the primary auditory cortex. 
As a secondary auditory processing region, RNNBelt is essential for analyzing more complex sound features than Conv$_{\text{A1}}$, integrating information from Conv$_{\text{A1}}$ to provide a more refined understanding of sounds, such as recognizing intricate patterns and spatial localization of sound sources.

RNN$_{\text{PB}}$, located adjacent to RNN$_{\text{Belt}}$, represents an even higher level of auditory processing. 
It receives inputs $ b_t $ from RNN$_{\text{Belt}}$ and connects with various brain regions involved in memory, emotion, and multisensory integration. 
RNN$_{\text{PB}}$ plays a crucial role in advanced auditory scene analysis, such as distinguishing multiple voices in a noisy environment or interpreting modulated sounds like music and speech. 
It also integrates auditory information with other sensory inputs, helping create a unified perception of the surrounding environment.


%The proposed method depends on the ability to estimate the response in next frame; 
%and therefore, it heavily relies on state prediction. 
%The recurrent module can utilize spatial-temporal features, 
%and enables the proposed model to handle temporal prediction.

%The recurrent module, presented in Fig~\ref{fig:pipeline} and named the Belt and PB. 
%Thus, recurrent module requires inferring music temporal change without music input.
%The refined representations $b_t$ and $ p_t $ are used to update the hidden state in the T2/T3. 
%\begin{equation} \label{LSTM}
%	h_t, o_t = \text{LSTM}(h_{t-1}, m_t).
%\end{equation}


% 中颞皮层和上颞皮层
\subsubsection{Middle and Superior Temporal Modules (T2/T3)}

% 总
The RNN$_{\text{T2/T3}}$ cortices are crucial brain regions involved in multiple aspects of sensory processing, particularly auditory recognition. 
Located in the temporal lobe, these areas are essential for interpreting and comprehending complex auditory signals, such as music.


% 中颞
T2 is primarily known for its role in auditory processing, particularly in perceiving motion and integrating audio-visual information. 
In auditory recognition, the T2 cortex becomes active when visual cues need to be integrated with auditory signals. 
As shown in TABLE~\ref{tab:prediction_accuracies}, T3, particularly the superior temporal gyrus (STG), plays a direct role in auditory processing and is essential for recognizing complex sounds, such as music. 
Within the STG, Heschl's gyrus (HG) is the first brain area to receive input audio, while surrounding regions, including the planum temporale (PT), are involved in processing higher-order sound features like language comprehension. 
The posterior part of the STG and the nearby superior temporal sulcus contribute to analyzing more complex sound attributes, such as prosody and the emotional content of speech. 
T3 also has extensive connections with other brain regions, supporting its role in integrating auditory information with other sensory modalities and cognitive functions.


In auditory recognition, these areas collaborate to decode and interpret sounds, enabling individuals to effectively recognize and respond to various auditory stimuli, such as distinguishing speech sounds, understanding spoken language, and appreciating music. 
The Belt$_{\text{COR}}$, PB$_{\text{COR}}$ and T2/T3$_{\text{COR}}$ areas each perform two 1 $\times$ 1 convolutions, followed by a bottleneck-style 3 $\times$ 3 convolution, which expands the feature set fourfold, and concludes with another 1 $\times$ 1 convolution. 
Recurrence is implemented by passing the output of an region through that same region multiple times. 
For example, as shown by the ``gate" in Fig.~\ref{fig:introduction}, after Belt$_{\text{COR}}$ processes the input feature once, the result is reprocessed by Belt$_{\text{COR}}$ as new input. 
As depicted in Fig.~\ref{fig:structure_analysis}, Belt$_{\text{COR}}$ and PB$_{\text{COR}}$ are repeated twice respectively, while T2/T3$_{\text{COR}} $ is repeated four times, as this configuration was found to produce the best model performance with the fewest layers, according to our scores. 
Similar to ResNet, a convolution module is followed by batch normalization\cite{ioffe2015batch} and ReLU. 
%However, batch normalization is not shared over time\cite{jastrzkebski2017residual}. 
In the current definition of BAN, there are no bypass or feedback connections across areas, and cochlea and auditory pathway processing are not explicitly modeled.

\subsubsection{Classification Decoder}

The decoder in BAN uses a straightforward linear classifier, which consists of weighted sums, with one sum for every object label. 
To minimize the number of neural activations feeding into the classifier, the responses for each feature map is averaged.


\subsection{Training Loss} \label{sec:loss}

The proposed BAN is trained by optimizing a combination of losses: a recognition loss and an auxiliary loss. 
The total BAN loss $L_{b}$ is defined as follows:
\begin{equation}
	L_{b} = L_r + L_u,
\end{equation}
where $ L_r $ is recognition loss depicted in Equ.~\ref{equ:recognition_loss}, $ L_u $ is auxiliary loss described in Equ.~\ref{equ:auxiliary_loss}.


\subsubsection{Recognition Loss}

To facilitate music recognition, we design the recognition loss to measure the difference between the output of BAN and the true label. 
This recognition loss $ L_r $ is defined as:
\begin{equation} \label{equ:recognition_loss}
	L_r = \frac{1}{N} 
		\sum_{i=1}^{N}(
			-(d_i * log(p_i) + 
			(1-d_i) * log(1-p_i))
		),
\end{equation}
where $ L_r $ the cross-entropy loss between the ground truth $d_i$ and the predicted label $p_i$, $ N $ is the number of samples.


\subsubsection{Auxiliary Loss}

L2 regularization is applied to both the dynamic parameter $\phi_t(s_t)$ and the model parameter $\theta$ to ensure proper regularization.
\begin{equation} \label{equ:auxiliary_loss}
	L_u = \frac{1}{2} \left\vert \left\vert \phi_t \right\vert \right\vert _2 ^2 
	+ \frac{1}{2} \left\vert \left\vert \theta \right\vert \right\vert _2 ^2.
	%- \sum_{i} \log (\lambda_i^{-1})
\end{equation}


% 类脑分数
\section{BAS: Brain-like Auditory Score} \label{sec:bas}
In this section, the BAS metrics is introduced, which assess the similarity between the ANN and brain. 
BAS is a measurement evaluated on given data, incorporating both cortical and behavioral metrics.


To get quantitative measurement for cortical similarity, we reference the open-source platform Brain-Score~\cite{SchrimpfKubilius2018BrainScore} and introduce the BAS.
It is a measurement that measure the performance to predict 
(a) the average behavioral choices when listening to target music clips from the Music Genre dataset~\cite{nakai2022music}, and 
(b) the average cortical activation at each brain region in response to the same music clips in the human auditory areas from the Music Genre neuroimaging dataset~\cite{nakai2022music}. 
To provide a unified evaluation of BAN, we calculate the average of both behavioral and cortical measurement.


\subsection{Cortical Metrics}
\label{sec:neural-pred}

Cortical metrics is utilized, which capture responses in regions of brain, such as activations in T2/T3, to assess how accurately BAN predicts the audio in ANN~\cite{yamins2014performance}. 
This measurement requires two sets of audio in the format of $\text{audio} \times \text{neuroids}$, where neuroids represent either model interlayer responses.

A total of 540 audio from 10 different music were randomly shown to 5 subjects, and neural activations were recorded in the T2/T3 regions. 
Additionally, we identify and present the most predictive layers or areas, RNN$_{\text{T2/T3}} $, within BAN model.


In our sutdy, relationships are established to map the ANN to cortical regions, and these relationships are used to infer neural activations to the given music clips. 
To speed up it, we compress the dimensionality of the activations to specific components using principal component analysis~\cite{2002Principal}. 
The responses in T2/T3 are used to learn these mappings. 
The final neural similarity score for the auditory recognition cortex is represented by the Pearson correlation coefficient $s_r$, calculated as follows:
\begin{equation} \label{equ:cortical_metrics}
	s_r= \frac{1}{N} \sum_{j=1}^{N}
	\frac{\sum_{i=1}^{n} (y_i-\bar{y}) (y_i^\prime - \bar{y}^\prime) }{\sqrt{\sum_{i=1}^{n} (y_i - \bar{y})^2 (y_i^\prime - \bar{y}^\prime)^2 }},
\end{equation}
where $y$ represents the actual neural activation, and $y^\prime$ is the model's predicted activation, $n$ is the dimension of corresponding layer in BAN, 
$ N $ is the number of music sample used in our experiment. 
$\bar{y}$ and $\bar{y}^\prime$ are the median values of the actual and predicted neural responses, respectively, across all data points.


\subsection{Behavioral metrics}

The goal of behavioral metrics is to assess the similarity between BAN's outputs and human behavior in music recognition tasks~\cite{nakai2022music}. 
In human listening experiments, participants provide a music genre label, so the behavioral model is represented as a categorical label in auditory recognition. 
The primary focus is to achieve human-like intelligence, not just auditory classification accuracy~\cite{schrimpf2020integrative}. 
BAN excels in behavioral similarity, accurately predicting both labels and neural activations. 
In contrast, while traditional ANNs may achieve excellent classification performance, they often fall short in delivering strong brain-like prediction performance.


As outlined in Sec.\ref{sec:behavioral_data_collection}, to compare brain activation patterns with behavioral performance, we gathered music genre choices from subjects through additional behavioral experiments. 
The corresponding output of the BAN is the predicted genre label. 
Thus, the music class predictivity, or behavioral score, is modeled as the similarity between the actual music genre choices made by the subjects and the predictions made by BAN. 
We calculate the overall behavioral metric $s_b$ across all audio sequences to determine the predictivity score,
\begin{equation} \label{equ:behavioral_metrics}
	s_b = 
	\frac{\sum_{i=1}^{M} TP_i}
	{\sum_{i=1}^{M} (TP_i + FP_i + FN_i)},
\end{equation}
where $ TP_i $ represents the number of true positives for music genres $ i $ (correct predictions), 
$ FP_i $ represents the number of false positives for music genres $ i $ (incorrectly predicted as genres $ i $), 
$ FN_i $ represents the number of false negatives for class $ i $ (instances of class $ i $ incorrectly predicted as another music genres),
$ M $ is the total number of genres.


\subsection{Overall score}

To assess the overall performance of the BAN, we use BAS, which combines both the behavioral metric and the T2/T3 cortical metrics. 
The BAS, denoted as $s_{a}$, is calculated as the average of these two scores.
\begin{equation} \label{equ:score_btn}
	s_{a} = \frac{1}{3} [\frac{min(m,n)}{max(m,n)} + s_r + s_b].
\end{equation}
where $ m $, $ n $ is the number of modules in cortical autitory recognition pathway and BAN, respectively.
The first item as a whole measures the similarity between cortical and BAN structures.
$ s_r $ is the similarity of cortical metrics depicted in Equ.~\ref{equ:cortical_metrics}, $ s_b $ is the similarity of behavioral metrics described in Equ.~\ref{equ:behavioral_metrics}.
The BAS is designed without normalization across different score magnitudes, as this could unfairly punishment with small variances. 
Instead, every score is treated equally to ensure fair significance in the overall BAS calculation.


\section{Experiments} \label{sec:experiments}

We validate the effectiveness of our designed BAN through three steps. 
Firstly, the datasets and provide details on the implementation of our model is introduced. 
Secondly, that BAN functions as a valid, brain-like auditory recognition model through circuitry analysis is shown. 
Thirdly, the model's classification accuracy and its representation of music genres in both BAN and the cortex is discussed.


\subsection{Datasets} \label{sec:datasets}

% 刺激
We used the GTZAN dataset, which is one of the most widely utilized in music genre recognition tasks\cite{nakai2022music}. 
The dataset consists of 30-second audio files spanning 10 different genres: blues, reggae, classical, rock, country, disco, jazz, pop, metal, and hip-hop. 
From the original collection, we randomly selected 54 music pieces from each genre, resulting in a total of 540 music pieces for the study. 
%A 15-second clip was randomly extracted from each piece, with 2-second fade-in and fade-out effects applied. 
All clips were normalized based on their root mean square values.


% fMRI
The fMRI experiment included 12 training runs and 6 test runs, for a total of 18 runs. 
Each run lasted 10 minutes and consisted of 40 music clips. 
In the training phase, 480 music clips were used, while the remaining 60 clips were reserved for the test runs. 
During each test run, a set of 10 music clips was shown four times in the same order. 
There is no repetition of clips in the training runs.


% 行为
%To compare brain activation patterns and behavioral performance, we utilized the subjects to participate in additional behavioral experiments. 
%These music clips were selected randomly from the 460 music pieces which were not used in the MRI experiment. 
%The subjects selected the music genre of the target music clip by filling in 1 of 10 cells on the answer sheet displayed on a laptop computer screen. 
%The music clips were presented through closed headphones. 
%Music clips were presented in the same order as in the MRI experiments. 
%Subjects were allowed to listen to each music clip only once.


% \makecell [c]
%\begin{table}[h]
%	\centering
%	\footnotesize
%	\caption{The segmentation and mergence of raw film.
%		Raw segments of Forrest Gump that constitute the true input. 
%		The 7 segments are merged~\cite{2014A}, and then, they are segmented into 8 segments, each corresponding to an fMRI recording. 
%		TRs indicates the number of repetition times.
%		The unit of duration is seconds.
%		In these experiments, we utilize the 2002 movie release.}
%	
%	\label{tab:movie_seg}
%	
%	\begin{tabular}{p{0.9cm}<{\centering}p{1.2cm}<{\centering}p{1.45cm}<{\centering}p{1.15cm}<{\centering}p{0.95cm}<{\centering}}
%		\toprule[1.5pt]
%		{Video ID} & {Begin Frame} & {Frame Number} & {Duration}  & {TRs}   \\ \midrule[1pt]
%		1       & 0           & 22,550      & 902.00 & 451.00 \\ 
%		2       & 22,150      & 22,050      & 882.00 & 441.00 \\ 
%		3       & 43,802      & 21,900      & 876.00 & 438.00 \\ 
%		4       & 65,304      & 24,400     & 976.00 & 488.00 \\ 
%		5       & 89,305     & 23,100     & 924.00 & 462.00 \\ 
%		6       & 112,007     & 21,950     & 878.00 & 439.00 \\ 
%		7       & 133,559     & 27,100     & 1,084.00 & 542.00 \\
%		8       & 160,261     & 16,876     & 575.04 & 337.52 \\ 
%		\bottomrule[1.5pt]
%	\end{tabular}
%	
%\end{table}


% 2<-->4
\begin{table}[h]
	\centering
	\footnotesize
	\caption{General Information of fMRI for participants.}
	
	\label{tab:fMRI_runs}
	
	\begin{tabular}{p{2.9cm}<{\centering}p{0.6cm}<{\centering}p{0.6cm}<{\centering}p{0.7cm}<{\centering}p{0.7cm}<{\centering}p{0.7cm}<{\centering}}
		\toprule[1.0pt]
		 & sub-01  & sub-02    & sub-03  & sub-04 & sub-05  \\ \midrule
		Sex     		  & M  &  M	   & M  & F  & F		  \\
		Age     		  & 30 			   & 30 	   & 23  & 33  & 25		  \\
		Music experience (years)     		  & 12  &  12	   & 4  & 15  & 10		  \\
		test samples (volumes) & 1350  & 1350 	   & 1350  & 1350  & 1350		  \\
		train samples (volumes) & 4050  & 4050 	   & 4050  & 4050  & 4050		  \\
		Total fMRI runs     		  & 18  & 18 	   & 18  & 18  & 18		  \\
		\bottomrule[1.0pt]
	\end{tabular}
	
\end{table}



% 数据分析
\subsection{Human Data Analysis} \label{sec:data_analysis}

To assess the similarity between the ANN and human auditory recognition, we extract task-related activations and then identify the corresponding brain regions for comparison.


%\subsubsection{fMRI analysis}

\subsubsection{fMRI Data Processing}
% Correspondence, 2.5.1
% 头动矫正
Motion correction is applied to each run using the Statistical Parameter Mapping toolbox (SPM 12), with all volumes aligned to the first image for every participant. 
% EPI: epipolar plane image 核极线平面
% 两个平面的参数化，分别为相机平面和图像平面
We remove Low-frequency drift with a filter using a 240-second window. 
To improve method accuracy, the activation for each voxel is normalized by subtracting the mean and variance. 
Cortical surfaces are identified using FreeSurfer~\cite{dale1999cortical,fischl1999cortical}, which registers the anatomical data with functional voxels. 
For analysis, only cortical voxels are used as targets, and for a participant, we focus on the voxels identified within the cortex.


% 获得感兴趣区域
\subsubsection{Genre Representation Region}

To obtain reliable estimates of human areas associated with music genres, we use the these step: 
Firstly, all activation recordings is randomly split into training sets (75\%) and test sets (25\%). 
Utilizing the optimal weights from the genre-label model\cite{nakai2021correspondence}, we fit an encoding model with genre-label features using the training data and assess model accuracy with the test sets. 
Parameter fitting is done with general linear model. 
The random resampling is executed 100 times, and voxels that show prediction accuracy in more than 75\% of repetitions are picked out for region of interest. 
As a result, we induce 473 voxels in the region of interest for participant, 468 for sub-01, 581 for sub-02, 1,593 for sub-03, and 529 for sub-05. 
And all subsequent analyses use these extracted region of interest.


\subsubsection{Behavioral Data Collection} \label{sec:behavioral_data_collection}

To verify that cortical activation in response to music was linked to behavioral accuracy in musci recognition, we utilized additional behavioral data\cite{nakai2022music}. 
As shown in TABLE~\ref{tab:fMRI_runs}, these experiments took place in a soundproof place with the same participants from fMRI study. 
Participants first listened to 3 original 30-second music for each genre, selected from the 460 clips randomly not utilized in fMRI test, as a reference. 
During this training session, participants were informed of the correct genres. 
They listened to the 60 audio utilized in fMRI experiment and classified each clip's genre by selecting one of 10 options on an answer sheet. 
Each audio was played once, in the same sequence as in the fMRI test.




\subsection{Implementation Details}
% 数据增强
However, we determined that the available data was insufficient to effectively train the parameters of the CNN. 
As a result, we applied data augmentation techniques to enhance the dataset.
%
Data augmentation is the process of generating new synthetic training samples by applying small modifications to the original dataset. 
The goal is to make the model robust to these variations and improve its generalization ability. 
For this approach to be effective, the added perturbations must preserve the original label of the training sample. 
Common techniques include adding noise, shifting the audio start point, altering speed, and changing pitch.

%\begin{itemize}
%	\item Add noise. 
%	Add the data sampled from the Gaussian distribution at the same position by 0.005 times as much as the length of the data, scale normalize to a smaller number and then apply element wise add to the data.
%	
%	\item Shift. We slightly shift the starting point of the audio, then pad it to original length.
%	Roll array elements along a given axis.
%	Elements that roll beyond the last position are re-introduced at the first.
%	
%	\item Speed change. 
%	Time stretching is the process of changing the speed or duration of an audio signal without affecting it pitch.
%	Time stretch an audio series by a fixed rate.
%	Slightly change the speed of the audio, then pad or slice it.
%	We conducted two augmentations by dividing the standard of the speed change by one second and a standard of variation of 0.9-1.1.
%	
%	\item  Pitch shift.
%	Pitch shift is the adjustment of the frequency part while preserving the structural characteristics of the music.
%	Bins per octave is held at 12 and pitch change is 2.
%	This was reflected in the characteristics of Western music structure.
%	
%	\item Pitch and speed.
%	% 同时产生音高和速度的变化
%	Simultaneously engenders pitch and speed variants.
%	
%	\item Multiply value.
%	Value change is similar to add noise.
%	Bug multiply the data sampled from the uniform distribution at the same position.
%	This uniform distribution value is contain low 1.5, high 3.
%	
%	\item Percussive.
%	Using harmonic-percussive sound separation (HPSS), separation factor parameter into the decomposition process that allows for tightening separation results.
%	The percussive part is expected to reflect the musical characteristics of particular genre.
%	
%	\item After feature engineering, divide data.
%	In 3D CNN, we have feature length of 1024 in the 128 mel frequency region.
%	30 seconds of data will be compressed in 1024 area.
%	However, learning 30 seconds of data at once is not very effective.
%	It is hypothesized that human beings are able to grasp only about 3 to 4 seconds of information in order to reflect their musical characteristics.
%	For sample CNN, this is done by dividing the waveform.
%\end{itemize}


% 梅尔倒谱系数 特征
% \subsubsection{Cochlear and MFCC features}

The Mel-frequency cepstral coefficient (MFCC) model is utilized to extract various genres-related information from the dataset \cite{rabiner2010theory}. 
For timbral and loudness information, the frame duration is set to 25 ms, with a 50\% overlap between adjacency. 
For  tonal and rhythm information, the frame duration is 3 seconds, with a 33\% overlap, and each feature was averaged over 1.5 seconds. 
Additionally, the MIR toolbox was utilized to extract MFCC features \cite{lartillot2008matlab}. 
%Feature extraction for MFCC models is restricted to the 100–8,000 Hz frequency range, with the MIRT model using 24 features and the MFCC model using 12 features.


Each auditory cortical region is represented by a specific ANN that performs classical operations such as convolution. 
These modules correspond to audio cortical areas, and we adjust the number of neurons in each auditory brain area. 
Given the computational demands, we implemented the model using Python and the PyTorch framework~\cite{paszke2019pytorch}.


%and model the A1 with the headmost 3 convolutional module from the revised AlexNet~\cite{imagenet}. 
%The input size of raw AlexNet is $227 \times 227$ and is downsized to $14 \times 14$ after 3 convolutional modules. 
%Because low-resolution features will lead to poor tracking results, 
%we convert the preliminary step size of 4 to 1 
%and remove a pooling module to retain spatial features, as depicted in Fig~\ref{fig:pipeline}. 
%In this way, the size of the obtained feature is $14 \times 14 \times 384$, and the size of the input foveal view is $56 \times 56$.
%In addition, at the tail of V1, there is a 20$\%$ probability of randomly omitting the feature. 
%The ventral pathway includes a convolutional module in which the kernel size is $1 \times 1$. 
%The filters of the DFN are $3 \times 3$ and $1 \times 1$.
%We used 100 LSTM units with the zoneout operation~\cite{zoneout} that have a probability of 0.05.
%We train BAN~\cite{curriculum}, and utilize a similar learning configuration except for the learning rate, which is $2 \times 10 ^{-6}$.
%The starting sequence length is five and increases every twelve iterations. 

% BAN结构 回路分析图
\subsection{Circuitry and Ablation Analysis}
While we believe that BAN offers a closer approximation to the anatomy of the auditory pathway compared to current ANNs—primarily due to its limited number of regions and inclusion of recurrence—it remains incomplete in several aspects. 
From a neuroscientific perspective, in addition to lacking biologically plausible learning methods, an ideal method of the auditory stream would incorporate more anatomical and circuitry-level details, such as the cochlea or the medial geniculate nucleus. 
Similarly, the addition of skip connections was not based on brain circuitry properties but rather adopted from complexity gradient\cite{he2016deep}  as a solution to the degradation problem in deep model. 
It's important to note that not all  structural choices are effective. 
We tested thousands of structures before identifying the BAN circuitry, as shown in Fig.~\ref{fig:structure_analysis}.


\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figs/structure_analysis}
	\caption{
		\textbf{
			BAN circuitry analysis.  
		}
		Each row shows how the highest accuracy on the GTZAN dataset and the BAS score change relative to the baseline model when a specific hyperparameter is modified, including cortical score $ s_r $ and behavioral score $ s_b $.
	}
	\label{fig:structure_analysis}
\end{figure}


% 流派分类
% 分类混淆矩阵图
\subsection{Classification Accuracy based on BAN}
% 模型输出分类
To verify that the brain activity of participants and BAN captured enough information to differentiate between music label in the experimental launch, we executed music recognition using a decoding model based on brain and BAN activity. 
As shown in Fig.~\ref{fig:confusion_matrix}, we evaluated the confusion matrix and recognition accuracy (the diagonal elements of the confusion matrix) by analyzing brain response within genre representation region of interest. 
The recognition results varied across music label, with classical music consistently classified accurately (average classification accuracy of 100\%), while rock music showed poor classification performance across participants (40\%). 
Additionally, participants often misclassified reggae as rock (confusion rate of 33.3\%) and rock as country (confusion rate of 28.6\%). 
The confusion matrices derived from activity were highly consistent across participants (Spearman’s correlation coefficient, $ \rho = 0.562 \pm 0.087$; $ p < 0.001 $ for all participant combinations).


% 行为测试
%To investigate how brain activation associated with music genre is related to behavioral performance during genre classification, we utilized an additional behavioral and BAN output test for each participant (MRI participants) after the MRI scanning.
%The confusion matrix revealed that participants' genre classification performance varied for each genre, in that classical music was always accurately recognized (average classification accuracy, 100\%), while rock music was less accurately recognized across all participants (36.7\% $\pm$ 13.9 \%).
%Behavioral confusion matrices identified brain activity-like error tendencies, such that rock music tended to be classified as country music (confusion from rock to country, 20.0\% $\pm$ 13.9\%).
%The behavior-based confusion matrices were highly consistent across all participants (Spearman's correlation coefficient, $ \rho = 0.669 \pm 0.038 $; $ p < 0.001 $ for all combinations of participants).
%The confusion matrices of behavior and brain activity were significantly correlated for all participants (Spearman's correlation coefficient, $ \rho = 0.438 \pm 0.081 $, $ p < 0.001 $), suggesting that the genre representational specificity of human behavior mimicked of brain activity.


% 重复听
%Further, because the fMRI participants listened to the music twice (once in the MRI scanner and again in the behavioral test), there may have been a learning effect.
%Moreover, it is necessary to test whether the five fMRI participants have similar perceptual properties for music genres as those of the general population.
%To confirm the generalizability of the behavioral results of these participants, we recruited an additional 21 participants for the behavioral tests only (non-MRI participants).
%The non-MRI participants exhibited variable genre classification accuracy (mean $\pm SD $, 56.3\% $ \pm 6.1\% $; max, 68.3\%; min, 43.3\% ), with performance similar to those of the MRI participants, in that they always recognized classical music accurately (100\%) whereas they did not always do so with rock music (17.5\%). 
%Accordingly, the average behavioral confusion matrices of the MRI participants and the non-MRI participants were significantly correlated ($ \rho = 0.826, p < 0.001 $).


\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figs/confusion_matrix}
	\caption{
		\textbf{
			Classification precision of the proposed BAN on Music Genres dataset. 
		}
		For various audio clips, the inferred classification outputs are determined with BAN.
	}
	\label{fig:confusion_matrix}
\end{figure}

% Visualize Predictions
\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figs/predicted_results}
	\caption{
		\textbf{
			Visualize predictions. 
		}
		View a sample of the input data along with the true and predicted class labels. 
		The $ x $-axis represents time, the $y$-axis represents frequency, and the colormap indicates decibel levels. 
		For several classes, distinct features are clearly visible. 
		For instance, the spectrogram for the country music class displays simple melodies and steady rhythms over time, characteristic of country music. 
		Additionally, it highlights the low-frequency sounds produced by musical instruments typical of this genre.
	}
	\label{fig:predicted_results}
\end{figure}


\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figs/specific_class}
	\caption{
		\textbf{
			Investigate predictions. 
		}
		Analyze the predictions of the validation Mel spectrograms. For each input, generate Grad-CAM, LIME, and occlusion sensitivity maps for the predicted classes. 
		These techniques take an input image and a class label, producing a map that highlights the regions of the image most important to the score for the specified class. 
		Each visualization method employs a distinct approach that influences the output it generates.
	}
	\label{fig:specific_class}
\end{figure}

As shown in Fig.~\ref{fig:specific_class}, Grad-CAM uses the gradient of the classification score with respect to the BAN features to identify which parts of the input are most critical for classification. 
Regions with high gradients indicate areas where the final score is most influenced by the data. 
% LIME
LIME approximates BAN's classification behavior using a simpler, more interpretable model, such as a linear model or a regression tree, to assess the importance of input features as a proxy for their relevance to BAN. 
% 遮挡敏感度
Occlusion sensitivity works by perturbing small parts of the input with an occluding mask, and as the mask moves across the input, the method measures changes in the probability score for a given class.


\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figs/investigate_predictions}
	\caption{
		\textbf{
			Investigate predictions for specific class. 
		}
		Explore the interpretability maps for spectrograms associated with a specific class.
	}
	\label{fig:investigate_predictions}
\end{figure}


As shown in Fig.~\ref{fig:investigate_predictions}, spectrograms corresponding to the blues class were identified. 
Interpretability maps were generated and plotted using the input spectrograms and predicted class labels. 
The maps reveal that the network focuses on areas of high intensity and low frequency. 
This result is surprising, as one might expect the network to also consider the high-frequency noise that repeats over time. 
Identifying such patterns is crucial for understanding the features the network relies on to make predictions. 
Similarly, as depicted in Fig.~\ref{fig:investigate_misclassifications}, maps were generated and plotted for both the true class (blues) and the predicted class (jazz).


\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figs/investigate_misclassifications}
	\caption{
		\textbf{
			Use the interpretability maps to examine misclassifications. 
		}
		Analyze a spectrogram where the true class is blues, but the predicted class is jazz.
	}
	\label{fig:investigate_misclassifications}
\end{figure}




% 皮层组织
% 在每个解剖区域的预测精度
% Genre-representing cortical organization
\subsection{Genre Representation in BAN and Cortex} \label{sec:cortical_organization}

The cortical areas involved in genre representation were evaluated using BAN. 
Significant prediction accuracy was observed in the bilateral STG across all participants ($ p < 0.05 $, FDR corrected). 
% 使用重采样 对每一个受试者都确定一个ROI
To identify brain region that consistently shown music labels, regardless of sample selection, we determined the genre-representing functional region of interest for each participant utilizing a resampling procedure. 
This analysis confirmed significant prediction accuracy in the bilateral STG, and the functional region of interest was utilized as an mask in subsequent analyses.

% 用经过压缩维度的 流派标签权重 来映射到 皮层表面
To evaluate the contribution of each brain voxel to 10 music label, we mapped genre representations on the cortical surface using principal component analysis with BAN weights. 
For each voxel within the region of interest, we extracted the learned BAN weights and applied principal component analysis for dimensionality reduction on the aggregated BAN features. 
The score indicated how the 10 principal components were shown in each brain voxel, while the loading matrix showed the contribution of each principal component to the representation of 10 music genres. 
To visualize the cortical organization of music genres for each participant, we extracted and normalized the principal component analysis scores from their respective voxels. 
This analysis revealed various genre-specific representations within the bilateral STG. 
As shown in TABLE~\ref{tab:prediction_accuracies}, music genres were more distinctly represented in Heschl's sulcus and the lateral STG compared to Heschl's gyrus, the planum temporale, or the lateral sulcus (with the exception of sub-03, who exhibited genre-specific activations in large brain regions, including planum temporale). 
Despite individual variability, a consistent pattern emerged: pop, disco, country, and hip-hop had stronger representations in the LSTG, while blues music showed larger contributions around the HS.

%To illustrate the relative relationships between the cortical activation patterns for different music genres, we visualized the weight values of 10 music genres in each voxel using 2-D coordinates derived from the top two principal components.
%Using PCA, we embedded genre-specific representation based on multiple cortical voxels (i.e.,high-dimensional data) into the 2-D space, maintaining their representational similarity.
%These findings indicated that the activation patterns induced by classical and jazz music were relatively similar, as were those induced by rock and mental music, whereas blues and hip-hop music seemed to have distinct activation patterns.
%Using the weight values of the genre-label model, we further visualized how the 10 music genres were represented differently in each cortical voxel based on the same 2-D coordinates.

\begin{table}
	%\begin{threeparttable}
	
	\centering  % 使表格整体居中
	\caption{The comparison between classical model (Music Class, MFCC) and BAN prediction performance in each anatomical area. 
		LSTG, lateral superior temporal gyrus; HG, Heschl's gyrus; HS, Heschl's sulcus; PT, planum temporale; MFCC, mel-frequency cepstral coefficient}
	\label{tab:prediction_accuracies} % label需要在caption后面，否则引用时是问号
	
	% \linewidth: 这指得是目前环境的宽度，是依赖于上下文的一个宽度值
	\resizebox{\linewidth}{!}{ %此处！表示根据根据宽高比进行自适应缩放
		\begin{tabular}{llll} 
			\toprule[1.0pt]
			&    	    Music Class &		  MFCC &    				BAN 			    \\
			\midrule
			L.HG &  	0.148 $\pm$ 0.023 &    0.091 $ \pm $ 0.031 &	 0.102 $ \pm $ 0.029 \\
			L.HS &  	0.219 $\pm$ 0.079 &    0.102 $ \pm $ 0.039 &	 0.171 $ \pm $ 0.063 \\
			L.PT &  	0.127 $\pm$ 0.029 &    0.057 $ \pm $ 0.018 &	 0.117 $ \pm $ 0.039 \\
			L.LSTG &  	0.081 $\pm$ 0.011 &   0.032 $ \pm $ 0.008 &	 0.082 $ \pm $ 0.013 \\
			R.HG &  	0.154 $\pm$ 0.021 &    0.103 $ \pm $ 0.018 &	 0.124 $ \pm $ 0.032 \\
			R.HS &  	0.199 $\pm$ 0.036 &    0.108 $ \pm $ 0.047 &	 0.146 $ \pm $ 0.032 \\
			R.PT &  	0.117 $\pm$ 0.041 &   0.061 $ \pm $ 0.022 &	 0.091 $ \pm $ 0.045 \\
			R.LSTG &  	0.103 $\pm$ 0.018 &    0.048 $ \pm $ 0.011 &	 0.102 $ \pm $ 0.021 \\
			\bottomrule[1.0pt]
		\end{tabular}
		%		\input{figs/prediction_accuracies}				
	}
	
	%\end{threeparttable}
\end{table}


% 特定音乐风格表征独立于声音刺激
% 嗓音刺激
% Genre-specific representation independent from voice stimuli
%\subsection{Genre representation independence}
%
%Most of the hip-hop and pop music clips contained voice stimuli,  whereas the classical and jazz music clips did not.
%To test whether the genre-specific cortical representation was not explained by the inclusion of voice stimuli, we performed additional encoding model analysis.
%We concatenated the voice features with the original genre-label features in the encoding model fitting.
%Model testing excluded the voice features from the concatenated feature matrix and the corresponding weight matrix.
%In this process, the voice features indicate whether certain music clip contains voice stimuli.
%As this regressor was excluded in model testing, we regressed out the effect of voice stimuli.
%This model predicted activations in most of the bilateral STG ROI that were used in the original genre-label model (77.8\% $ \pm $ 19.2\% of voxels were significant across all participants; prediction accuracy, $ r=0.246 \pm 0.027 $; original genre-label model, $ r=0.284 \pm 0.025 $).
%% 回归去除语音刺激后
%To examine how the cortical representation of 10 music genres is affected by regressing out the voice effect, we mapped the weight values of 10 music genres in each voxel using PCA.
%We also visualize the relative relationships of 10 music genres derived from the top two PCs and their representation in each cortical voxel.
%Similar to the results in Sec.~\ref{sec:cortical_organization}, the activation patterns induced by classical and jazz music were relatively similar, as were those induced by rock and metal music, whereas blues and hip-hop music had distinct activation patterns.
%These findings suggest that the genre-specific activation patterns were not explained merely by the inclusion of the voice stimuli.


% 可解释
% Genre-specific brain activity was explained by the spectro-temporal modulation of music genres
%\subsection{Genre representation in brain was explained by the spectro-temporal modulation of BAN}
%
%Next, we tested how well acoustic feature-based genre specificity corresponds to brain-based feature specificity.
%To achieve this, we extracted the acoustic features of each music stimulus using BAN.
%%In this study, we only show the MTF model results for the purpose of visualization;
%%however, the following analyses were performed similarly for all acoustic models.
%Spectro-temporal modulation of each music genre was evaluated according to the feature used for encoding model fitting.
%Interpretable spectro-temporal information was obtained for each MTF feature by restoring the MTF feature matrix to the original size through multiplying it by the transposed PCA coefficient matrix.
%Moreover, we transformed the MTF weight matrix by multiplying it by the PCA loading matrix.
%In order to visualize the MTF model, we further averaged the feature values obtained at the 20 central frequencies for each of the 10 $ \times $ 10 combinations of spectral modulation $ \Omega $ (cyc/oct) and temporal modulation $ \omega $ (Hz).
%Genre-specific feature vectors were calculated for the other models using the same procedure.
%By averaging the MTF features for the 48 clips of the same music genre in the training dataset, we obtained reference MTF features for the 10 music genres.
%When then obtained the MTF features of each cortical voxel using the weight matrix of the MTF model.
%The response of each cortical voxel to spectro-temporal modulation was assessed using the transformed weight matrix.
%Voxel-specific weight vectors were calculated by averaging the MTF weight values of the 48 training clips for each genre.
%Voxel-specific weight vectors were calculated for the other models using the same procedure.
%In order to visualize the MTF model, we further averaged the weight values obtained at the 20 central frequencies for each of the 10 $\times$ 10 combinations of spectral modulation $\Omega$ (cyc/oct) and temporal modulation $\omega$ (Hz).
%
%
%To examine whether genre-representing activation patterns were explained by the extracted features, we calculated the Person's correlation coefficients between the reference genre-specific feature vector and the voxel-specific weight vector in each cortical voxel.
%Through this analysis, we obtained an FBS cortical map of each music genre based on its MTF features.
%The FBS map based on the MTF model was very similar to the cortical map obtained for genre-label weight, 
%and they were significantly correlated for all music genres (Pearson's correlation coefficient, $ p < 0.001 $ with Bonferroni correction).
%A significant correlation was observed consistently across all participants ($ r = 0.729 \pm 0.111 $), indicating that the different spectro-temporal modulations of the 10 music genres explained genre-specific activity patterns.
%The FBS map also obtained for the other acoustic models, and we found that MTF model outperformed the other models (cochlear, MIRT, MFCC, and voice models) in terms of the mean correlation between the FBS maps and the genre-weight maps (Wilcoxon signed-rank test, $ p < 0.020 $ for participants except for ID02).
%Although we used earphones that attenuate scanning noise, the remaining noise could have degraded the stimulus quality and modulated activity patterns in the auditory cortex.
%To evaluate the effects of scanning noise on our result, we therefore performed additional analyses.
%Specifically, we recorded the MRI noise inside the scanner using an MRI-compatible microphone and added this noise to the original auditory stimuli.
%Since the relative intensity of noise depends on the depth of insertion, we added the noise with three different relative intensities (0.2, 0.5, and 1.0 to the mean RMS of the original stimuli).
%% 当数据符合正态分布、方差齐性时，常用参数检验，如u检验、t检验等，但当数据不符合正态分布或方差不齐时，则需要利用非参数检验。
%% https://www.cnblogs.com/djx571/p/10216940.html
%For participants ID01, ID04, and ID05, we found that the MTF model exhibited the largest correlation coefficients between the FBS maps and genre-weight maps independent of relative noise intensities (Wilcoxon signed-rank test, $ p < 0.050 $; except for the comparison between the MTF and cochlear models for participant ID01 with noise intensities of 1.0 and 0.5, for which $ p = 0.065 $, and between the MTF and MIRT models for participant ID04 with a noise intensity of 0.5, for which $ p=0.084 $).
%For participants ID02 and ID02, a relative noise intensity of 0.2 more clearly demonstrated the advantage of the MTF model (Wilcoxon signed-rank test, $ p < 0.010 $).
%These results suggest that the MTF model accurately captured genre-specific cortical organization in the bilateral STG. 





%\begin{figure}
%	\centering
%	\includegraphics[width=.9\linewidth]{figs/structure_analysis.pdf}
%	\caption{\textbf{
		%		BTN architecture analysis.} 
	%		\textcolor{red}{
		%		We analyse four main factors including the initial step size in $\rm{CONV_{V1}}$ and the number of dynamic filter layers in $\rm{DFN_{MT/MST}}$, the number of hidden units in $\rm{LSTM_{FEF}}$ and the number of hidden units in $\rm{FC}$.
		%		Each row indicates how cortical and behavioral score change based on the trained BTN when a certain hyperparameter changes. 
		%		}
	%	}
%	\label{fig:structure_analysis}
%\end{figure}





%
%\section{Discussion}
%\label{sec:discussion}

%Using fMRI, the current study revealed the cortical organization underlying different music genres.
%As the genre-label model did not assume any acoustic properties, we used genre-weight maps to reflect music genre information in general.
%% FBS (Feature-Brain Similarity)
%%Thus, it was important to obtain similar weight patterns between the genre-label model and the FBS map based on the MTF model.
%%The FBS map shows how the spectro-temporal modulation of each cortical voxel corresponds to the reference spectro-temporal modulation profile for each music genre.
%%Thus, it is likely that the weight values in the bilateral STG for the genre-label model were determined by the degree to which each STG voxel's spectro-temporal modulation property resembles that of the music stimuli.
%
%
%Among the multiple subregions in STG, music genres were represented more clearly in both HS and LSTG than in the other subregions.
%% 音区定位
%Previous studies on frequency-selective (i.e., tonotopic) maps of the human STG have indicated that the primary auditory cortex (A1) is located around the posterior part of HG and HS accompanied by a gradient of low- to high-frequency selectivity from the anterior to posterior direction~\cite{ahveninen2016intracortical,leaver2016functional}.
%While cochlear features correspond to positions in the frequency axis and may therefore reflect tonotopic properties, the MTF model further captures the modulation property around each position on the frequency axis.
%% MTF 优于 耳蜗模型
%Santoro et al.~\cite{santoro2014encoding} showed that the MTF model outperformed the cochlear model in terms of predicting STG activation in response to natural sound stimuli~\cite{santoro2014encoding}, which is consistent with the current results.
%% norman2015distinct 音乐和说话的不同皮层通路
%LSTG has been reported to represent different sound categories such as the sound of guitar versus. voice of a cat~\cite{staeren2009sound}, and it exhibits human speech-selective activation~\cite{norman2015distinct}.
%The MTF model captures detailed spectro-temporal modulation properties both in human speech~\cite{elliott2009the} and in musical instruments~\cite{patil2012music}, which may explain the more general acoustic features that can encompass the feature space of simple categorical models such as genre-label or voice models.
%To summarize, the spectro-temporal modulations obtained in our study seem to reflect the general processing of auditory stimuli in the bilateral STG.
%
%Several studies have reported that perceived music genres can be decoded from brain activity.
%Ghaemmaghami and Sebe~\cite{ghaemmaghami2016brain} used magnetoencephalogram and electroencephalogram datasets to classify musical stimuli as either pop or rock using SVM.
%Further, Casey~\cite{casey2017music} and Sengupta et al.~\cite{sengupta2018spatial} used fMRI data with five distinct music genres, followed by activity-based multi-class classification using SVM.
%However, these studies did not provide answers to how cortical representations of music genres contribute to genre classification.
%Collectively, the present findings demonstrate the underlying mechanisms of such activity-based genre classification.
%
%We investigated classification accuracy using five models (cochlear, MTF, MIRT, MFCC, and voice).
%% Mel-Frequency Cepstral Coefficient / MIRtoolbox
%Both the MFCC and the MIRT models were developed in the field of computational science and have been employed previously in studies of music-induced brain activity~\cite{alluri2012large,gucclu2016brains,toiviainen2014capturing}.
%The cochlear model has been employed to test cortical activation in the spectral domain~\cite{de2017hierarchical};
%however, it cannot capture the dynamic temporal modulation of spectra.
%The MTF model was constructed based on the physiological properties of neurons in the auditory cortex~\cite{chi2005multiresolution} and is used widely in neuroscience research into auditory perception~\cite{norman2015distinct,patil2012music,santoro2014encoding,santoro2017reconstructing}.
%Therefore, it is likely that the MTF model is more biologically plausible for addressing the auditory processing of music genres.
%Our current findings are consistent with this view, because, of all the models, the MTF model showed the highest correlation coefficients between the FBS maps and genre-weight maps.
%
%% 能捕获快节奏
%One might argue that the fMRI signal change is too slow to capture the rapid acoustic features of music stimuli and that this could affect the model performance with up-tempo (e.g.,metal) and slow-tempo (e.g., classical) music genres.
%However, the MTF model includes temporal modulations of frequency (from 2.8 to 64.0 Hz) and the estimated model weights show signals in high temporal modulation rates suggesting that this model can capture the fine-scale musical information necessary to distinguish relatively up-tempo music genres (e.g., metal and hip-hop).
%Indeed, the difference in decoding accuracy in not explained by the difference in tempo, given that both classical and hip-hop music showed higher decoding accuracies.
%
%% 使用行为实验补充 没有主动听的问题
%In the MRI experiments in this study, participants listened passively to music stimuli and did not carry out any genre classification tasks during scanning.
%It could be argued that we did not confirm that the participants listened attentively to the stimuli and that we overlooked the brain regions activated for top-down decision-making on music genre classification.
%To address this, we conducted behavioral experiments of genre classification for MRI participants and confirmed that there were significant correlations between the confusion matrices based on brain activation and behavior.
%These findings suggested that passive listening to music stimuli captured enough brain information for use in behavioral music genre classification.
%
%In this study, we adopted a small-N design (five participants).
%The small number of subjects is compensated for by the large number of samples for each participant (i.e., three hours).
%The small-N design has attracted substantial attention in recent studies combining fMRI data and machine learning~\cite{smith2018small}.
%Instead, of group-level statistical analyses, as are often used in conventional neuroimaging, we performed subject-wise analyses.
%The correspondence of genre representation among participants was confirmed using Person's correlation of confusion matrices for both activity-based decoding and behavior-based analyses.
%In contrast, cortical organization differed across participants.
%% SD 标准差
%In contrast, cortical organization much larger $ SD $ of prediction accuracy than the other anatomical ROI (genre-label model: left HS, $ SD = 0.085 $; mean $ SD $ across other ROIs = 0.026), indicating that the left HS is the most sensitive region to the individual variability of music genre representation.
%
%% 语言的影响
%It is worth considering whether linguistic factors could explain genre-specific organization because most classical and jazz pieces employed in the current study were instrumental (i.e., without human voice), whereas other genres included the human voice.
%Previous studies have reported voice-selective and non-voice-selective cortical areas around STG~\cite{kell2018task,leaver2010cortical,norman2015distinct}.
%However, our study demonstrated that such linguistic factors do not explain genre-specific patterns.
%The genre-label model in our study predicted brain activity even after regressing out voice-related features.
%In addition, we also showed that classical and jazz music were not confused with each other and that some music genres containing voice stimuli were not confused with each other (e.g., hip-hop and country music).
%Thus, it is likely that genre-specific activation patterns in the bilateral STG reflected detailed spectro-temporal modulation even within nonvoice music pieces.
%
%
%% 结果不完美的原因
%Although we have shown that FBS maps and genre-weight maps largely corresponded, the correspondence is not perfect.
%There are several possible reasons for such imperfection.
%% 1. 容易受噪声影响
%First, the FBS maps and genre-weight maps were susceptible to the noise of brain activity.
%The upper limit of prediction accuracy also affects the accuracy of both FBS maps and genre-weight maps.
%% 2. MTF模型本身不是最好->深度网络
%Second, the MTF might not be the best model.
%Indeed, an acoustic model might exist that captures more detailed characteristics of music genres.
%% 3. 其他非声音特征
%Third, some nonacoustic features (such as the participants' preference, knowledge, and experience related to music) may play important roles in producing the genre-specific cortical organization.
%% 音乐类别的神经基础 
%Further research is therefore required to clarify the detailed neural basis of music categorization.


%\subsection{Humans have specialized cortical neurons that encode pitch and harmonics}
%
%Pitch perception is crucial for perceiving speech and music and for recognizing auditory objects in a complex acoustic environment.
%Pitch is the percept that allows harmonically structured periodic sounds to be perceived and ordered on a musical scale.
%Pitch carries crucial linguistic information in tonal languages such as Chinese and prosodic information in European languages.
%We use pitch to identify a particular voice from a noisy background in a cocktail party.
%When listening to an orchestra, we hear the melody of the soloist over the background of accompanying instruments.
%
%
%An important phenomenon for understanding pitch is the perception of “missing fundamental,” also referred to as the residue pitch.
%When the harmonics of a fundamental frequency are played together, the pitch is perceived as the fundamental frequency even if the fundamental frequency is missing.
%For example, the harmonics of the fundamental frequency of 200 Hz are at 400, 600, 800 Hz, and so on.
%Playing the frequencies 400, 600, and 800 Hz together will generate a pitch perception of 200 Hz, even though a distinct frequency component of 200 Hz is not physically present in the sound.
%We encounter this phenomenon routinely when we listen to music over speakers that are too small to generate sounds at low frequencies.
%
%
%Many combinations of frequencies can give rise to a common fundamental frequency or pitch, making it a particularly valuable auditory cue.
%This is especially useful when pitch conveys behaviorally important information, as in the case of human speech or animal vocalizations.
%Sounds propagated through the environment can become spectrally degraded, losing high or low frequencies.
%While such spectral filtering distorts spectral information, the perception of the missing fundamental is robust despite the loss of some harmonic components.
%
%
%The ability to perceive pitch is not unique to humans; 
%birds, cats, and monkeys can also pick out pitch.
%Monkeys are capable of spectral pitch discrimination, melody recognition, and octave generalization, each of which requires the perception of pitch.
%Marmoset monkeys (Callithrix jacchus), a highly vocal New World primate species whose hearing range is similar to that of humans, exhibit human-like pitch perception.
%Marmosets are able to discriminate a missing fundamental in harmonic sounds with a precision as small as one semitone for the periodicity above 440 Hz.
%
%
%Given that both humans and some animals experience a pitch that generalizes across a variety of sounds with the same periodicity (including harmonic sounds with a missing fundamental), it is reasonable to expect that some neurons extract pitch from complex sounds. 
%Xiaoqin Wang and his colleagues discovered a decade ago that a small region in the auditory cortex of marmoset monkeys contains “pitch-selective neurons.”
%These neurons are tuned to pure tones with a best frequency and respond to harmonic complexes with a fundamental frequency near its best frequency even when the harmonics lay outside the neuron’s excitatory-frequency response area (Fig 28–13A).
%
%
%A pitch-selective neuron responds to pitch-evoking sounds (eg, harmonic sounds, click trains) when the pitch is near the neuron’s preferred best frequency.
%Pitch-selective neurons increase their firing rates as the behavioral salience of pitch increases and prefer sounds with periodicity over aperiodic sounds.
%It is important to note that the pitch-selective neurons in marmoset monkeys, which extract and code for pitch embedded in harmonic sounds (a highly nonlinear computation), are distinctly different from neurons in subcortical areas or A1 that merely “reflect” information on pitch in their firing patterns.
%
%
%The region containing the pitch-selective neurons in marmoset monkeys is confined to the low-frequency border of A1, the rostral auditory cortex (area R), and lateral belt areas (Fig 28–13B).
%Human imaging studies have identified a restricted region at the lateral end of Heschl’s gyrus anterolateral to A1 that extracts pitch of harmonic complex sounds and is sensitive to changes in pitch salience.
%The location of this region mirrors the location of the pitch center in marmoset monkeys (Fig 28–13B).
%
%
%The core regions of auditory cortex in marmosets also contain a class of harmonic template neurons that respond weakly or not at all to pure tones or twotone combinations but respond strongly to particular combinations of multiple harmonics.
%The harmonic template neurons show stronger responses to harmonic sounds than inharmonic sounds and selectivity for particular harmonic structures.
%In contrast to the pitch-selective neurons that are localized within a small cortical region lateral to the low-frequency border between A1 and R and have best frequencies less than 1,000 Hz, the harmonic template neurons are distributed across A1 and R and have best frequencies ranging from approximately 1 kHz to approximately 32 kHz, a range that covers the entire hearing range of marmosets.
%
%
%Whereas in the periphery single auditory nerve fibers encode individual components of harmonic sounds, the properties of the harmonic template neurons reveal harmonically structured receptive fields for extracting harmonic patterns.
%The change in neural representation of harmonic sounds from auditory nerve fibers to the auditory cortex reflects a principle of neural coding in sensory systems.
%Neurons in sensory pathways transform the representation of physical features, such as the frequency of sounds in hearing or luminance of images in vision, into a representation of perceptual features, such as pitch in hearing or curvature in vision.
%Such features lead to the formation of auditory or visual percepts.
%The harmonic template neurons in the auditory cortex are key to processing sounds with harmonic structures such as animal vocalizations, human speech, and music.


\section{Conclusion}

Inspired by the auditory processing mechanisms of the human brain, our study introduces a brain-like model tailored for auditory recognition tasks. 
By incorporating neuroanatomical constraints, the model achieves strong recognition performance with enhanced interpretability. 
Additionally, we demonstrate how the cortical model aligns with human auditory recognition of real music and propose a new method for calculating the similarity between model activations and brain responses. 
In addition, the neuroanatomically aligned model offers improved predictions of cortical responses. 
We believe that the proposed BAN will inspire new approaches in ANN interpretability and potentially drive advancements in brain-computer interfaces.

% artificial motion consciousness
% artificial motion intelligence

% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


%\section*{Acknowledgment}
%This work was partially supported by the financial support provided by the Natural Science Foundation of Hunan Province (No.2024JJ6190), the Open Project of Xiangjiang Laboratory (No.23XJ03009), Xiangjiang Laboratory key project subproject (No.22XJ01001-2,24XJ01003), the ``Digital Intelligence +" interdisciplinary research project of Hunan University Of Technology and Business (No.2023SZJ19), Scientific research project of Hunan Provincial Department of Education (No.22B0646), research on a new generation of brain-like intelligent computing framework based on ultra-large-scale real neural systems (No.24XJJCYJ01001).




\bibliographystyle{IEEEtran}
\bibliography{reference}

%\appendices
%\section{Proof of the First Zonklar Equation}
%Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank

%\section{}
%Appendix two text goes here.


% use section* for acknowledgment



% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

%\begin{IEEEbiography}{Michael Shell}
%Biography text here.
%\end{IEEEbiography}

% if you will not have a photo at all:
%\begin{IEEEbiographynophoto}{John Doe}
%Biography text here.
%\end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage
%
%\begin{IEEEbiographynophoto}{Jane Doe}
%Biography text here.
%\end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}

% 作者信息
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{./authors/HaidongWang}}]{Haidong Wang}
%	 received the B.E. degree in Computer Science and Technology from Hunan University Of Technology and Business, Changsha, China, in 2014, and the M.Sc. degree in Computer Technology from the University of Chinese Academy of Sciences (UCAS), Beijing, 	China, in 2017. At present, he is currently pursuing the Ph.D. degree in Hunan University, Changsha, China. His current research interests include machine learning,  multi-object tracking and reinforcement learning.
%\end{IEEEbiography}


%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{./Authors/ZhiyongLi.eps}}]{Zhiyong Li}
%	received the MSc degree in System Engineering from National University of Defense Technology, Changsha, China, in 1996 and PhD degree in Control Theory and Control Engineering from Hunan University, Changsha, China, in 2004.
%	Since 2004, he joined the College of Computer Science and Electronic Engineering of Hunan University. 
%	Now, he is a Full Professor with Hunan University, member of IEEE, China Computer Federation (CCF) and Chinese Association for Artificial Intelligence (CAAI). 
%	His research interests include Intelligent Perception and Autonomous Moving Body, Machine Learning and Industrial Big Data, Intelligent Optimization Algorithms with Applications. He has published more than 100 papers in international journals and conferences.
%\end{IEEEbiography}
%
%
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{./Authors/XuanHe.eps}}]{Xuan He}
%	received her B.S. and M.S. degree from Hunan Normal University (HNU), China, in 2016 and 2020. She is currently pursuing the Ph.D. degree in Computer science and technology with Hunan  University. Her research interests include object detection and recognition, machine learning.
%\end{IEEEbiography}
%
%
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{./Authors/KeNai.eps}}]{Ke Nai}
%	received the Ph.D. degree in Computer Science and Technology from Hunan University, Changsha, China, in 2019. Currently, he is working as a Postdoctoral Researcher at Hunan University. His current research interests include visual tracking, face recognition, computer vision, pattern recognition and machine learning. He has published several papers in IEEE-TIP, Information Sciences, Knowledge Based Systems, Neural Computing and Applications, ICIP2019 and so on.
%\end{IEEEbiography}
%
%
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{./Authors/MingWen.eps}}]{Ming Wen}
%	 received his M.S. degree and Ph.D. degree from Hunan University in 2006 and 2010, respectively. He has been a senior engineer at State Grid Hunan Electric Power Company Limited Economical \& Technical Research Institute. His research interests include energy internet demand forecasting, electricity price, smart grid, renewable energy accommodation and power system planning.
%\end{IEEEbiography}


% that's all folks
\end{document}


% C#:  https://www.monodevelop.com/download/#fndtn-download-lin-ubuntu
