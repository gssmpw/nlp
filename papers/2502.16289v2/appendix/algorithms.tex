\section{Algorithms} \label{sec:apx_algorithms}

\begin{algorithm}[ht]
\caption{Multiscale Object-based Graph Neural Network (MOB-GCN)}
\begin{algorithmic}[1]
\Require Input features $X$, adjacency matrix $A$, edge weights $W$ (optional)
\Ensure Output features

\State Initialize bottom encoder GCN
\State Initialize middle encoders and pools for each resolution level
\State Initialize final fully connected layer

\Function{Forward}{$X$, $A$, $W$}
    \State $L \gets$ BottomEncoder($X$, $A$, $W$)
    \State $L \gets \text{ReLU}(L)$
    \State allLatents $\gets [L]$
    \State $A \gets \text{ToDenseAdj}(A, W)$
    
    \State product $\gets$ None
    \For{level $= 1$ to numLevels}
        \State assign $\gets \text{GumbelSoftmax}(\text{MiddlePool}_\text{level}(L))$
        \If{level $= 1$}
            \State product $\gets$ assign
        \Else
            \State product $\gets$ product $\cdot$ assign
        \EndIf
        
        \State $X \gets \text{Normalize}(\text{assign}^T \cdot L)$
        \State $A \gets \text{Normalize}(\text{assign}^T \cdot A \cdot \text{assign})$
        
        \State $L \gets \text{ReLU}(\text{MiddleEncoder}_\text{level}(A \cdot X))$
        \State extendedLatent $\gets$ product $\cdot L$
        \State allLatents.append(extendedLatent)
    \EndFor
    
    \If{useNorm}
        \State allLatents $\gets$ [Normalize(latent) for latent in allLatents]
    \EndIf
    
    \State representation $\gets$ Concatenate(allLatents)
    \State output $\gets$ FinalFC(representation)
    
    \Return output
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[ht]
\caption{Training MGN with LGC Loss}
\begin{algorithmic}[1]
\Require Training dataset $\mathcal{D} = \{(X_i, A_i, W_i, y_i)\}_{i=1}^N$, number of epochs $E$, learning rate $\eta$, regularization parameter $\mu$
\Ensure Trained MGN model $\mathcal{M}$

\State Initialize MGN model $\mathcal{M}$ with parameters $\theta$
\State Initialize optimizer $\mathcal{O}$

\For{$e = 1$ to $E$}
    \For{$(X, A, W, y) \in \mathcal{D}$}
        \State $\hat{y} \gets \mathcal{M}(X, A, W; \theta)$
        
        \State // Compute supervised loss
        \If{$\mathcal{M}_{\text{train}}$ exists}
            \State $L_{\text{sup}} \gets -\sum_{i \in \mathcal{M}_{\text{train}}} y_i \log(\hat{y}_i)$
        \Else
            \State $L_{\text{sup}} \gets -\sum_{i=1}^{|y|} y_i \log(\hat{y}_i)$
        \EndIf
        
        \State // Compute smoothness regularization
        \State $d \gets \text{diag}(A\mathbf{1})$ \Comment{Node degrees}
        \State $\hat{y}_{\text{norm}} \gets D^{-1/2}\hat{y}$ \Comment{$D$ is diagonal matrix of $d$}
        \State $L_{\text{smooth}} \gets \frac{1}{|E|} \sum_{(i,j) \in E} \|\hat{y}_{\text{norm},i} - \hat{y}_{\text{norm},j}\|_2^2$
        
        \State // Compute total loss
        \State $L_{\text{total}} \gets L_{\text{sup}} + \mu L_{\text{smooth}}$
        
        \State // Update model parameters
        \State $\theta \gets \theta - \eta \nabla_\theta L_{\text{total}}$
    \EndFor
\EndFor

\Return Trained MGN model $\mathcal{M}$
\end{algorithmic}
\end{algorithm}