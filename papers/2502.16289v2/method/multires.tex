\subsection{Multiresolution Graph Learning}

\subsubsection{General Construction}

Multiresolution Graph Networks (MGN), introduced by \citet{hy2019covariant}, offer a framework for analyzing graphs across multiple resolutions. Given an undirected, weighted graph \( \mathcal{G} = (V, E, \mathbf{A}, \mathbf{F}_v) \), where \( V \) and \( E \) denote the sets of nodes and edges, respectively, the graph structure is defined by the adjacency matrix \( \mathbf{A} \in \mathbb{R}^{|V| \times |V|} \). Each node is associated with a feature vector, represented as \( \mathbf{F}_v \in \mathbb{R}^{|V| \times d_v} \), capturing relevant attributes for downstream processing.

\paragraph{Graph Coarsening}
A \emph{K-cluster partition} of a graph divides its nodes into \( K \) mutually exclusive clusters, \( V_1, \ldots, V_K \), where each cluster forms a subgraph. The process of \emph{coarsening} involves constructing a reduced graph \( \widetilde{\mathcal{G}} \), in which each node represents an entire cluster, and edges between these nodes are weighted based on inter-cluster connections in the original graph. This procedure is applied iteratively across multiple levels, resulting in an \emph{L-level coarsening}, where the topmost level condenses the graph into a single node.

\paragraph{Multiresolution Graph Network (MGN)}
MGN operates by iteratively transforming a graph into coarser representations through three key components:
\begin{enumerate}
    \item \textbf{Clustering}: This step partitions the graph into clusters.
    \item \textbf{Encoder}: A graph neural network encodes each cluster into latent node features.
    \item \textbf{Pooling}: Latent features from each cluster are combined into a single vector, which is used to represent the coarser graph at the next level.
\end{enumerate}
These steps are repeated across all levels of resolution, with learnable parameters governing each component. The goal is to predict properties of the original graph by leveraging hierarchical structures.

\subsubsection{Learning to Cluster}

The clustering operation in MGN is differentiable and uses a soft assignment of nodes to clusters, optimized during training via a Gumbel-softmax approximation. This ensures that the clustering procedure can be incorporated into backpropagation for efficient learning.

In summary, MGN provides a scalable way to learn hierarchical representations of graphs by iteratively coarsening them while preserving node and edge information through learnable neural network layers.
