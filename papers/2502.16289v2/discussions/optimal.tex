\subsection{Classification Results of MOB-GCN} 

The MOB-GCN model, especially in its optimized form (MOB-GCN (Optimal)), consistently achieved the highest OA compared to single-scale GCNs and the non-optimized MOB-GCN. This indicates that incorporating multiscale information and selecting optimal scales significantly enhances classification performance.

See table \ref{table:5_percent_results}, with only 5\% of the data used for training , the MOB-GCN (Optimal) achieved an OA of 94.28\% on the Indian Pines dataset, while the single-scale GCN only achieved 92.85\%. For the Salinas dataset with 5\% training data, the MOB-GCN (Optimal) showed a substantial improvement with an OA of 98.85\%, compared to the GCN's 89.35\%. Similar trends were observed across other datasets like Pavia, Kennedy Space Center, Botswana and University of Toronto, with the optimized MOB-GCN consistently outperforming the other models.

The superior performance of the MOB-GCN (Optimal) is not limited to small training datasets, with the optimized model performing the best across all sample sizes (5\%, 10\%, and 20\%). This shows the model's robustness and adaptability to varying amounts of training data. The MOB-GCN models show marked improvement over the GCN, especially with smaller sample sizes.

The key to the MOB-GCN's performance is its ability to integrate features extracted from multiple segmentation scales, enabling it to capture both fine-grained details and broader contextual information. The multiscale approach allows for a more comprehensive understanding of complex structures within hyperspectral images, which leads to more accurate classification outcomes. By using superpixels as nodes in the graph, the MOB-GCN reduces the computational overhead associated with processing large hyperspectral images\cite{sellars2020}. 

\subsection{Comparing MOB-GCN with Single-Scale Methods}

The MOB-GCN consistently surpassed the single-scale GCN in performance across all datasets. The single-scale GCN often exhibited lower classification accuracy and a higher prevalence of speckle noise in the output maps, highlighting its limitations in capturing the hierarchical
spatial-spectral relationships within HSI data. In contrast, the MOB-GCN, which integrates information from multiple scales, generated smoother and more accurate classification maps.

\subsection{Impact of scale on MOB-GCN Performance}

The automatic optimal scale selection method, which identifies the most informative segmentation scales, is crucial to the success of the MOB-GCN. This method determines optimal scales based on the relative changes in the coefficient of variation (CV) across different segmentation scales. It selects the "peaks" of these changes, which indicate significant variations in image object heterogeneity. Our experiments demonstrated that combining features from 4â€“6 optimal segmentation scales was generally sufficient to achieve the desired classification performance across most datasets. The specific optimal scales for each dataset are provided in Table VI. For instance, the optimal scales for the Indian Pines dataset were 42, 24, 17, 8, and 4, while for the Salinas dataset, they were 55, 31, 23, 14, 10, and 4.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{discussions/images/merged_inertia_std.pdf}
    \caption{Inertia and Superpixel standard deviation for K-means clustering assessment on the SALINAS dataset.}
    \label{fig:inertia_standard_deviation}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{discussions/images/NN-nROC.pdf}
    \caption{NN-nRoC on every number of clusters on the SALINAS Dataset.
}
    \label{fig:nn_nRoC}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{discussions/images/candidate_clusters.pdf}
    \caption{MGN Performance on training with 10 sizes of candidate clusters, each are chosen by descending NN-nROC value. Each test is repeated 5 times, and the last 3 are omitted due to poor performance. The test was performed on the SALINAS dataset.}
    \label{fig:candidate_size}
\end{figure}

