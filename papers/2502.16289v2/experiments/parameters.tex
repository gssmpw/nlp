\subsection{Parameters} \label{sec:parameters}

In our proposed framework, there are seven hyperparameters that come from the four tasks of our framework.
\begin{itemize}
    \item Superpixel construction: $s$.
    \item Feature Extraction: $k$.
    \item Graph construction: $\sigma$, $K$ and $\beta$.
    \item LGC classification: $\mu$, $R$.
\end{itemize}
The fixed parameters are reused from Sellarsâ€™ \citep{sellars2020} original implementation.
\begin{table}[h]
\caption{Parameters Description.}
\label{tab:fixed_parameters}
\center
\begin{tabular}{|l|l|r|}
\hline
\textbf{Parameter} & \textbf{Description} & \textbf{Value} \\
\hline
$k$ & Weighted filtering kernel & 15.0 \\
\hline
$\sigma$ & Kernel parameter for constructing $s_{ij}$ & 0.20 \\
\hline
$K$ & k-NN Construction & 8 \\
\hline
$\mu$ & Weighting in the LGC classifier & 0.01 \\
\hline
$\beta$ & Weighting for construction $s_{ij}$ & 0.9 \\
\hline
$s$ & Minimum segmentation siize & See Table \ref{tab:hyperspectral_params} \\
\hline
$R$ & List of resolutions & See Table \ref{tab:hyperspectral_params} \\
\hline
\end{tabular}
\end{table}

For the superpixels construction step, we set the number of superpixels N to be at most 1000 and with a segmentation quality of $\geq 99\%$. Rule of thumb to find a good segmentation size is $width \times height / \textit{number of nodes}$. 


\begin{table}[h]
\centering
\caption{Hyperspectral Image Parameters.}
\label{tab:hyperspectral_params}
\begin{tabular}{|c|c|c|c|c|}
\hline
 & \textbf{Shape (h, w)} & \textbf{$s$} & \textbf{\# nodes} & \textbf{$R$} \\
\hline
\textbf{INDIAN} & (145, 145) & 10 & 668 & [16] \\
\hline
\textbf{SALINAS} & (512, 217) & 100 & 239 & [16] \\
\hline
\textbf{PAVIAN} & (1096, 715) & 200 & 921 & [9] \\
\hline
\textbf{BOTSWANA} & (1476, 256) & 200 & 431 & [14] \\
\hline
\textbf{KENNEDY} & (512, 614) & 100 & 522 & [13] \\
\hline
\textbf{TORONTO} & (724, 632) & 200 & 403 & [4] \\
\hline
\end{tabular}
\end{table}

\begin{table}[h!]
\centering
\caption{Number of Parameters across Datasets.}
\label{tab:number_of_params}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Dataset} & \textbf{GCN} & \textbf{MGN} & \textbf{MGN (Optimal)} \\ \hline
Indian   & 11,280   & 48,672   & 133,103    \\ \hline
Salinas  & 3,216    & 40,608   & 125,942    \\ \hline
Pavia    & 3,337    & 38,930   & 123,260    \\ \hline
Toronto  & 11,396   & 45,704   & 131,215    \\ \hline
Kennedy  & 13,709   & 50,330   & 140,707    \\ \hline
Botswana & 7,566    & 44,444   & 124,497    \\ \hline
\end{tabular}
\end{table}

\begin{table}[h!]
\centering
\caption{Memory Size across Datasets.}
\label{tab:memory_size}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Dataset} & \textbf{GCN (MB)} & \textbf{MGN (MB)} & \textbf{MGN (Optimal, MB)} \\ \hline
Indian   & 0.043  & 0.186  & 0.508  \\ \hline
Salinas  & 0.012  & 0.115  & 0.480  \\ \hline
Pavia    & 0.013  & 0.149  & 0.470  \\ \hline
Toronto  & 0.043  & 0.174  & 0.501  \\ \hline
Kennedy  & 0.052  & 0.192  & 0.537  \\ \hline
Botswana & 0.029  & 0.170  & 0.475  \\ \hline
\end{tabular}
\end{table}

We conducted experiments using two methods for selecting learned resolutions: one based on the number of classes in the dataset and the other on identifying the optimal resolutions. A more detailed discussion on determining these optimal resolutions will be provided in the Discussion section.