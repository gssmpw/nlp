[
  {
    "index": 0,
    "papers": [
      {
        "key": "ho2020denoising",
        "author": "Ho, Jonathan and Jain, Ajay and Abbeel, Pieter",
        "title": "Denoising diffusion probabilistic models"
      },
      {
        "key": "rombach2022high",
        "author": "Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\\\"o}rn",
        "title": "High-resolution image synthesis with latent diffusion models"
      },
      {
        "key": "flux_github",
        "author": "Black Forest",
        "title": "Flux"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "rombach2022high",
        "author": "Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\\\"o}rn",
        "title": "High-resolution image synthesis with latent diffusion models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "radford2021learning",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh and others",
        "title": "Learning transferable visual models from natural language supervision"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "saharia2022photorealistic",
        "author": "Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily L and Ghasemipour, Kamyar and Gontijo Lopes, Raphael and Karagol Ayan, Burcu and Salimans, Tim and others",
        "title": "Photorealistic text-to-image diffusion models with deep language understanding"
      },
      {
        "key": "chenpixart",
        "author": "Chen, Junsong and Jincheng, YU and Chongjian, GE and Yao, Lewei and Xie, Enze and Wang, Zhongdao and Kwok, James and Luo, Ping and Lu, Huchuan and Li, Zhenguo",
        "title": "PixArt-$\\alpha$: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis"
      },
      {
        "key": "sd3_5_github",
        "author": "Stability AI",
        "title": "Stable Diffusion 3.5"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "zhang2023adding",
        "author": "Zhang, Lvmin and Rao, Anyi and Agrawala, Maneesh",
        "title": "Adding conditional control to text-to-image diffusion models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "mou2024t2i",
        "author": "Mou, Chong and Wang, Xintao and Xie, Liangbin and Wu, Yanze and Zhang, Jian and Qi, Zhongang and Shan, Ying",
        "title": "T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "ye2023ip",
        "author": "Ye, Hu and Zhang, Jun and Liu, Sibo and Han, Xiao and Yang, Wei",
        "title": "Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "ruiz2023dreambooth",
        "author": "Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir",
        "title": "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "galimage",
        "author": "Gal, Rinon and Alaluf, Yuval and Atzmon, Yuval and Patashnik, Or and Bermano, Amit Haim and Chechik, Gal and Cohen-or, Daniel",
        "title": "An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion"
      },
      {
        "key": "wang2024moa",
        "author": "Wang, Kuan-Chieh and Ostashev, Daniil and Fang, Yuwei and Tulyakov, Sergey and Aberman, Kfir",
        "title": "Moa: Mixture-of-attention for subject-context disentanglement in personalized image generation"
      },
      {
        "key": "li2024photomaker",
        "author": "Li, Zhen and Cao, Mingdeng and Wang, Xintao and Qi, Zhongang and Cheng, Ming-Ming and Shan, Ying",
        "title": "Photomaker: Customizing realistic human photos via stacked id embedding"
      },
      {
        "key": "wang2024instantid",
        "author": "Wang, Qixun and Bai, Xu and Wang, Haofan and Qin, Zekui and Chen, Anthony and Li, Huaxia and Tang, Xu and Hu, Yao",
        "title": "Instantid: Zero-shot identity-preserving generation in seconds"
      },
      {
        "key": "qian2024omniid",
        "author": "Guocheng Qian and Kuan-Chieh Wang and Or Patashnik and Negin Heravi and Daniil Ostashev and Sergey Tulyakov and Daniel Cohen-Or and Kfir Aberman",
        "title": "Omni-ID: Holistic Identity Representation Designed for Generative Tasks"
      },
      {
        "key": "wang2024visual",
        "author": "Wang, XuDong and Zhou, Xingyi and Fathi, Alireza and Darrell, Trevor and Schmid, Cordelia",
        "title": "Visual Lexicon: Rich Image Features in Language Space"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "pan2023kosmos",
        "author": "Pan, Xichen and Dong, Li and Huang, Shaohan and Peng, Zhiliang and Chen, Wenhu and Wei, Furu",
        "title": "Kosmos-g: Generating images in context with multimodal large language models"
      },
      {
        "key": "berman2024mumu",
        "author": "Berman, William and Peysakhovich, Alexander",
        "title": "Mumu: Bootstrapping multimodal image generation from text-to-image data"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "peebles2023scalable",
        "author": "Peebles, William and Xie, Saining",
        "title": "Scalable diffusion models with transformers"
      },
      {
        "key": "flux_github",
        "author": "Black Forest",
        "title": "Flux"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "gemaking",
        "author": "Ge, Yuying and Zhao, Sijie and Zeng, Ziyun and Ge, Yixiao and Li, Chen and Wang, Xintao and Shan, Ying",
        "title": "Making LLaMA SEE and Draw with SEED Tokenizer"
      },
      {
        "key": "pan2023kosmos",
        "author": "Pan, Xichen and Dong, Li and Huang, Shaohan and Peng, Zhiliang and Chen, Wenhu and Wei, Furu",
        "title": "Kosmos-g: Generating images in context with multimodal large language models"
      },
      {
        "key": "sun2023generative",
        "author": "Sun, Quan and Yu, Qiying and Cui, Yufeng and Zhang, Fan and Zhang, Xiaosong and Wang, Yueze and Gao, Hongcheng and Liu, Jingjing and Huang, Tiejun and Wang, Xinlong",
        "title": "Generative pretraining in multimodality"
      },
      {
        "key": "koh2024generating",
        "author": "Koh, Jing Yu and Fried, Daniel and Salakhutdinov, Russ R",
        "title": "Generating images with multimodal language models"
      },
      {
        "key": "wu2023next",
        "author": "Wu, Shengqiong and Fei, Hao and Qu, Leigang and Ji, Wei and Chua, Tat-Seng",
        "title": "Next-gpt: Any-to-any multimodal llm"
      },
      {
        "key": "ye2024x",
        "author": "Ye, Hanrong and Huang, De-An and Lu, Yao and Yu, Zhiding and Ping, Wei and Tao, Andrew and Kautz, Jan and Han, Song and Xu, Dan and Molchanov, Pavlo and others",
        "title": "X-VILA: Cross-Modality Alignment for Large Language Model"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "xiao2024omnigen",
        "author": "Xiao, Shitao and Wang, Yueze and Zhou, Junjie and Yuan, Huaying and Xing, Xingrun and Yan, Ruiran and Wang, Shuting and Huang, Tiejun and Liu, Zheng",
        "title": "Omnigen: Unified image generation"
      },
      {
        "key": "shi2024llamafusion",
        "author": "Shi, Weijia and Han, Xiaochuang and Zhou, Chunting and Liang, Weixin and Lin, Xi Victoria and Zettlemoyer, Luke and Yu, Lili",
        "title": "LlamaFusion: Adapting Pretrained Language Models for Multimodal Generation"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "wu2023next",
        "author": "Wu, Shengqiong and Fei, Hao and Qu, Leigang and Ji, Wei and Chua, Tat-Seng",
        "title": "Next-gpt: Any-to-any multimodal llm"
      },
      {
        "key": "ye2024x",
        "author": "Ye, Hanrong and Huang, De-An and Lu, Yao and Yu, Zhiding and Ping, Wei and Tao, Andrew and Kautz, Jan and Han, Song and Xu, Dan and Molchanov, Pavlo and others",
        "title": "X-VILA: Cross-Modality Alignment for Large Language Model"
      },
      {
        "key": "tong2024metamorph",
        "author": "Tong, Shengbang and Fan, David and Zhu, Jiachen and Xiong, Yunyang and Chen, Xinlei and Sinha, Koustuv and Rabbat, Michael and LeCun, Yann and Xie, Saining and Liu, Zhuang",
        "title": "MetaMorph: Multimodal Understanding and Generation via Instruction Tuning"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "radford2021learning",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh and others",
        "title": "Learning transferable visual models from natural language supervision"
      },
      {
        "key": "fang2023eva",
        "author": "Fang, Yuxin and Wang, Wen and Xie, Binhui and Sun, Quan and Wu, Ledell and Wang, Xinggang and Huang, Tiejun and Wang, Xinlong and Cao, Yue",
        "title": "Eva: Exploring the limits of masked visual representation learning at scale"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "li2023blip",
        "author": "Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven",
        "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models"
      },
      {
        "key": "liu2023llava",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual Instruction Tuning"
      },
      {
        "key": "zhu2023minigpt",
        "author": "Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed",
        "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"
      },
      {
        "key": "llama3_meta",
        "author": "Meta AI",
        "title": "LLaMA 3: Vision and Edge AI for Mobile Devices"
      },
      {
        "key": "wang2024qwen2",
        "author": "Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others",
        "title": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "brown2020language",
        "author": "Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "key": "openai2024gpt4technicalreport",
        "author": "Josh Achiam and Steven Adler and Sandhini Agarwal et. al.",
        "title": "GPT-4 Technical Report"
      },
      {
        "key": "llama3_meta",
        "author": "Meta AI",
        "title": "LLaMA 3: Vision and Edge AI for Mobile Devices"
      },
      {
        "key": "yang2024qwen2",
        "author": "Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng and others",
        "title": "Qwen2.5 Technical Report"
      }
    ]
  }
]