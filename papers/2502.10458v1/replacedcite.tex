\section{Related Work}
\subsection{Diffusion models}

Diffusion models have become powerful tools for text-to-image generation____. Early models, e.g. Stable Diffusion____, use CLIP____ for prompt embedding, while recent works integrate large language models (LLMs)____ for complex prompts. Methods such as ControlNet____, T2I-Adapter____, and IP-Adapter____ introduce structural and image-level controls by reconstruction-based fine-tuning. Personalized generation has been enhanced by methods like DreamBooth____, and other methods____, some of which use interleaved image-text inputs____. However, these methods focus on reconstruction fidelity rather than in-context reasoning. In contrast, our method equips diffusion models with the multimodal in-context reasoning capabilities of VLMs.


\subsection{Unified understanding and generation}

Recent work on large language models (LLMs) and diffusion transformers____ has inspired unified models for multimodal understanding and generation. These models either finetune LLMs to generate image tokens, which are then decoded into images via diffusion decoders____, or integrate text, image, and noise tokens within a transformer architecture____. They are typically trained end-to-end with diffusion losses or align output image tokens with CLIP text features using cosine similarity losses____. While some methods exhibit preliminary reasoning capabilities, these capabilities remain constrained by the limits of diffusion training paradigms, the availability of reasoning datasets, and the representational limits of CLIP embeddings. In contrast, our method leverages vision-language training to transfer advanced multimodal reasoning capabilities in VLMs to diffusion models. 



\subsection{Vision-language training}
Vision-language training has proven effective in developing powerful multimodal models. CLIP-like models____ use contrastive learning to align image and text embeddings. Recent large vision-language models (LVLMs)____ align CLIP visual features with advanced large language models (LLMs)____ by fine-grained text prediction. This vision-language training enables robust multimodal feature alignment, developing multimodal understanding and reasoning by leveraging powerful LLMs.  Inspired by these advancements, our method employs vision-language training as a proxy task to bridge VLMs with diffusion models, inheriting their advanced multimodal reasoning capabilities.

\begin{figure*}[t]
  \centering
   \includegraphics[width=\linewidth]{imgs/framework.pdf}
   \vspace{-20pt}
   \caption{(a) In \textbf{ThinkDiff-LVLM} training, the LVLM processes an image and a text to generate text tokens and token features, with some token features randomly masked. Unmasked token features are passed to a trainable aligner network and an LLM decoder, predicting masked text tokens supervised by cross-entropy loss. In inference, the LLM decoder is replaced by a diffusion decoder, enabling in-context reasoning image generation from interleaved images and texts. (b) In \textbf{ThinkDiff-CLIP} training, a CLIP vision model extracts image token features which are then mapped by a trainable aligner network. A part of the image caption is encoded by the LLM encoder and concatenated with image tokens. These combined tokens are passed to the LLM decoder to predict the next part of the caption supervised by cross-entropy loss. In inference, the LLM decoder is replaced by a diffusion encoder, allowing coherent image generation based on multimodal context.
   }
   \label{fig:framework}
\vspace{-12pt}
\end{figure*}