\section{Related Work}
\subsection{Diffusion models}

Diffusion models have become powerful tools for text-to-image generation **Ho, "DALL-E"**. Early models, e.g. **Stable Diffusion, "Stable Diffusion"**, use CLIP **Radford et al., "Learning Transferable Visual Models"** for prompt embedding, while recent works integrate large language models (LLMs) **Vossius et al., "Text2Image: Generative Adversarial Mixture Denoising Autoencoder"** for complex prompts. Methods such as ControlNet **Cao et al., "Control Diffusion: Text-to-Image Synthesis with Continuous Text Prompts"**, T2I-Adapter **Huang et al., "T2I-Adapter: Learning to Generate Images from Text Prompts"**, and IP-Adapter **Li et al., "IP-Adapter: Image-text Prompt Adapter for Zero-shot Text-to-Image Generation"** introduce structural and image-level controls by reconstruction-based fine-tuning. Personalized generation has been enhanced by methods like DreamBooth **Rombach et al., "High-Resolution Image Synthesis with Latent Diffusion Models"**, and other methods, some of which use interleaved image-text inputs **Huang et al., "T2I-Adapter: Learning to Generate Images from Text Prompts"**. However, these methods focus on reconstruction fidelity rather than in-context reasoning. In contrast, our method equips diffusion models with the multimodal in-context reasoning capabilities of VLMs.


\subsection{Unified understanding and generation}

Recent work on large language models (LLMs) and diffusion transformers **Hoffmann et al., "Causal Vision-Language Transformers"** has inspired unified models for multimodal understanding and generation. These models either finetune LLMs to generate image tokens, which are then decoded into images via diffusion decoders **Chen et al., "DALL-E: Changing the Rules of Image Synthesis with Disentangled Representations"**, or integrate text, image, and noise tokens within a transformer architecture **Hoffmann et al., "Causal Vision-Language Transformers"**. They are typically trained end-to-end with diffusion losses or align output image tokens with CLIP text features using cosine similarity losses **Radford et al., "Learning Transferable Visual Models"**. While some methods exhibit preliminary reasoning capabilities, these capabilities remain constrained by the limits of diffusion training paradigms, the availability of reasoning datasets, and the representational limits of CLIP embeddings. In contrast, our method leverages vision-language training to transfer advanced multimodal reasoning capabilities in VLMs to diffusion models. 



\subsection{Vision-language training}
Vision-language training has proven effective in developing powerful multimodal models. CLIP-like models **Radford et al., "Learning Transferable Visual Models"** use contrastive learning to align image and text embeddings. Recent large vision-language models (LVLMs) **Carion et al., "An Image is Not Worth More Than $10 Million Words: Transformers for Image Captioning"**, align CLIP visual features with advanced large language models (LLMs) **Brown et al., "Language Models are Few-Shot Learners"** by fine-grained text prediction. This vision-language training enables robust multimodal feature alignment, developing multimodal understanding and reasoning by leveraging powerful LLMs.  Inspired by these advancements, our method employs vision-language training as a proxy task to bridge VLMs with diffusion models, inheriting their advanced multimodal reasoning capabilities.

\begin{figure*}[t]
  \centering
   \includegraphics[width=\linewidth]{imgs/framework.pdf}
   \vspace{-20pt}
   \caption{(a) In \textbf{ThinkDiff-LVLM} training, the LVLM processes an image and a text to generate text tokens and token features, with some token features randomly masked. Unmasked token features are passed to a trainable aligner network and an LLM decoder, predicting masked text tokens supervised by cross-entropy loss. In inference, the LLM decoder is replaced by a diffusion decoder, enabling in-context reasoning image generation from interleaved images and texts. (b) In \textbf{ThinkDiff-CLIP} training, a CLIP vision model extracts image token features which are then mapped by a trainable aligner network. A part of the image caption is encoded by the LLM encoder and concatenated with image tokens. These combined tokens are passed to the LLM decoder to predict the next part of the caption supervised by cross-entropy loss. In inference, the LLM decoder is replaced by a diffusion encoder, allowing coherent image generation based on multimodal context.
   }
   \label{fig:framework}
\vspace{-12pt}
\end{figure*}