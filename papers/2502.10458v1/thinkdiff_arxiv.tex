%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025_1}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\usepackage{caption}
\captionsetup[figure]{labelfont={it}, labelsep=period}
\captionsetup[table]{labelfont={it}, labelsep=period}
% \usepackage{caption}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{I Think, Therefore I Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models}

\begin{document}

\twocolumn[
\icmltitle{{I Think, Therefore I Diffuse}: \\Enabling Multimodal In-Context  Reasoning in Diffusion Models}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Zhenxing Mi}{hkust}
\icmlauthor{Kuan-Chieh Wang}{snap}
\icmlauthor{Guocheng Qian}{snap}
\icmlauthor{Hanrong Ye}{hkust}
\icmlauthor{Runtao Liu}{hkust} \\
\icmlauthor{Sergey Tulyakov}{snap}
\icmlauthor{Kfir Aberman}{snap}
\icmlauthor{Dan Xu}{hkust}
\end{icmlauthorlist}

\icmlaffiliation{hkust}{Department of Computer Science and Engineering (CSE), The Hong Kong University of Science and Technology (HKUST).}
\icmlaffiliation{snap}{Snap Inc}
\icmlcorrespondingauthor{Dan Xu}{danxu@cse.ust.hk}

% \icmlkeywords{Machine Learning, ICML}



{
\vspace{6pt}
\includegraphics[width=0.99\textwidth]{imgs/teaser_arxiv.pdf}
\vspace{-5pt}
\captionof{figure}{
(a) Our ThinkDiff reasons over interleaved images (\textit{a flying monkey} and \textit{a flying cat}) and text prompts (\textit{monkey}, \textit{cat}, and \textit{zebra}) to generate a logically correct and high-quality image (\textit{a flying zebra}). The ground truth reasoning answer is provided as a reference for readers. (b) ThinkDiff composes images and texts into a coherent and reasonable image.
}
\vspace{-12pt}
\label{fig:teaser}%
}
\vskip 0.3in
]
% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution



\begin{abstract}

This paper presents ThinkDiff, a novel alignment paradigm that empowers text-to-image diffusion models with multimodal in-context understanding and reasoning capabilities by integrating the strengths of vision-language models (VLMs). Existing multimodal diffusion finetuning methods largely focus on pixel-level reconstruction rather than in-context reasoning, and are constrained by the complexity and limited availability of reasoning-based datasets. ThinkDiff addresses these challenges by leveraging vision-language training as a proxy task, aligning VLMs with the decoder of an encoder-decoder large language model (LLM) instead of a diffusion decoder. This proxy task builds on the observation that the \textbf{LLM decoder} shares the same input feature space with \textbf{diffusion decoders} that use the corresponding \textbf{LLM encoder} for prompt embedding. As a result, aligning VLMs with diffusion decoders can be simplified through alignment with the LLM decoder. Without complex training and datasets, ThinkDiff effectively unleashes understanding, reasoning, and composing capabilities in diffusion models. Experiments demonstrate that ThinkDiff significantly improves accuracy from 19.2\% to 46.3\% on the challenging CoBSAT benchmark for multimodal in-context reasoning generation, with only 5 hours of training on 4 A100 GPUs. Additionally, ThinkDiff demonstrates exceptional performance in composing multiple images and texts into logically coherent images. Project page: \href{https://mizhenxing.github.io/ThinkDiff}{\textit{https://mizhenxing.github.io/ThinkDiff}}.
\end{abstract}


\section{Introduction}\label{sec:Introduction}

\textit{Can diffusion models take ``IQ tests"?}
Figure~\ref{fig:teaser}a presents an example of a visual analogy IQ test. The model is provided with images of \textit{a flying monkey} and \textit{a flying cat}, along with text prompts of \textit{monkey}, \textit{cat}, and \textit{zebra}, and asked to generate the next image. A reasonable output image should be an image of \textit{a flying zebra}, requiring the model's ability to reason and recognize implicit patterns in context, such as the shared attribute of the \textit{flying} action in this example.

The concept of enabling diffusion models to think and then generate is compelling yet underexplored. Current text-to-image diffusion models~\cite{sd3_5_github,flux_github} excel at generating high-quality images by strictly following explicit prompts, while typically lacking multimodal in-context reasoning. Unlocking reasoning capabilities in them can enable them to handle more sophisticated tasks, such as interpreting complex instructions, solving visual analogy problems that require inferring implicit logic relationships, and composing multiple images and text in a logically consistent manner.


With rapid advancements in vision-language models (VLMs) such as CLIP~\cite{radford2021learning} and GPT-like models~\cite{radford2018improving}, we now have powerful tools for advanced multimodal understanding and reasoning. This leads us to a question: \textit{can we equip diffusion models with the reasoning capabilities of VLMs?}


\begin{figure}[t]
  \centering
   \includegraphics[width=\linewidth]{imgs/arch_compare.pdf}
   \vspace{-20pt}
   \caption{
   \textbf{(a)} Reconstruction-based diffusion finetuning integrates image features using a diffusion loss, focusing on pixel-level image reconstruction without reasoning. 
   \textbf{(b)} ThinkDiff aligns a VLM to an LLM decoder by vision-language training on image-caption datasets. In inference (dotted lines), it transfers multimodal in-context reasoning capabilities from the VLM to a diffusion decoder.
   }
   \label{fig:arch_compare}
\vspace{-17pt}
\end{figure}


Existing multimodal diffusion adapters~\cite{zhang2023adding, ye2023ip, mou2024t2i} primarily rely on reconstruction-based diffusion finetuning to incorporate visual conditions into text-to-image diffusion models. Figure~\ref{fig:arch_compare}a illustrates the typical training pipeline of IP-Adapter~\cite{ye2023ip}, where the model is finetuned to replicate input images at the pixel level. While effective for pixel-level control and high-fidelity image generation, adapting this finetuning paradigm to support in-context reasoning introduces several challenges.
\textbf{First}, this multimodal finetuning primarily focuses on pixel-level reconstruction of explicit image inputs rather than performing multimodal reasoning based on input context.
\textbf{Second}, the pixel-level reconstruction training does not focus on aligning vision representations with the textual feature space, limiting the model's ability to reason effectively across modalities. \textbf{Third}, instead of readily available image-caption pairs, it requires multimodal reasoning datasets that pair multimodal inputs with logically consistent output images and cover different reasoning tasks. Collecting such datasets is significantly more complex than captioning images. Existing instruction-guided datasets such as the synthetic InstructPix2Pix~\cite{brooks2023instructpix2pix} dataset primarily focus on image editing tasks, lacking the diversity needed for reasoning-based generation tasks. \textbf{Finally}, finetuning diffusion models for reasoning from scratch using limited datasets constrains their performance across a broad range of reasoning tasks.




\begin{figure}[t]
  \centering
   \includegraphics[width=\linewidth]{imgs/encoder_decoder_llm.pdf}
   \vspace{-20pt}
   \caption{Several diffusion models share a language encoder with encoder-decoder LLMs, allowing aligning with diffusion decoders through aligning with LLM decoders.
   }
   \label{fig:encoder_decoder_llm}
   \vspace{-22pt}
\end{figure}


To tackle these challenges, we propose \textbf{ThinkDiff}, a novel alignment paradigm to transfer multimodal in-context reasoning capabilities from VLMs to diffusion models. Instead of directly aligning VLMs with a diffusion decoder, we design a proxy task to align VLMs with a large language model (LLM) decoder by vision-language training. The foundation of this proxy task is depicted in Figure~\ref{fig:encoder_decoder_llm}. Recent diffusion models~\cite{deepfloyd_if, chenpixart, flux_github, sd3_5_github} have adopted the \textbf{encoder} of an encoder-decoder LLM~\cite{raffel2020exploring} as diffusion models' prompt encoder. This shared text encoder establishes a shared input feature space for both the diffusion \textbf{decoder} and LLM \textbf{decoder}. Therefore, aligning a VLM with a diffusion decoder can be achieved by the proxy task of aligning a VLM with the LLM decoder by vision-language training. 



Figure~\ref{fig:arch_compare}b depicts the vision-language training in ThinkDiff. The input images and text prompts are processed by a VLM and an aligner network, after which they are fed into an LLM decoder. The LLM decoder generates text autoregressively, supervised by a cross-entropy loss against ground truth texts. After training, the VLM is aligned to the LLM decoder, and inherently to the diffusion decoder.


Our method offers several advantages. \textbf{First}, it fully leverages the multimodal in-context understanding and reasoning capabilities of VLMs without requiring expensive training from scratch. \textbf{Second}, by aligning multimodal features to the input space of the LLM decoder through fine-grained text supervision, the model effectively captures rich semantic details from multimodal inputs, enabling seamless collaboration between vision and text modalities. \textbf{Finally}, ThinkDiff is lightweight, efficient and highly versatile. The vision-language training in it only requires readily available image-caption pairs, eliminating the need for complex reasoning-based datasets while achieving remarkable in-context reasoning capabilities. 


This paper introduces two variants of ThinkDiff, each using a different VLM. ThinkDiff-LVLM aligns generated tokens of a large vision-language model (LVLM) to diffusion models. ThinkDiff-CLIP aligns image tokens from a CLIP vision encoder~\cite{radford2021learning} to diffusion models. Our contributions are summarized as follows:


\begin{itemize}
\vspace{-10pt}
    \item We propose ThinkDiff, a novel alignment paradigm that equips diffusion models with multimodal in-context reasoning capabilities from VLMs.
\vspace{-5pt}
    \item ThinkDiff designs a proxy task to align VLMs into a shared feature space of both an LLM decoder and a diffusion decoder by vision-language training, fully transferring VLM's reasoning capabilities to diffusion models with efficient training and simple datasets.
\vspace{-5pt}
    \item We address the poor convergence problem in ThinkDiff for robust feature alignment. After training for only 5 hours on 4 A100 GPUs, ThinkDiff improves state-of-the-art accuracy on the major visual in-context learning benchmark~\cite{zeng2024can} from 19.2\% to 46.3\%. It also demonstrates powerful abilities to compose multiple images and texts into logically coherent images.
\end{itemize}


\section{Related Work}

\subsection{Diffusion models}

Diffusion models have become powerful tools for text-to-image generation~\cite{ho2020denoising, rombach2022high, flux_github}. Early models, e.g. Stable Diffusion~\cite{rombach2022high}, use CLIP~\cite{radford2021learning} for prompt embedding, while recent works integrate large language models (LLMs)~\cite{saharia2022photorealistic, chenpixart, sd3_5_github} for complex prompts. Methods such as ControlNet~\cite{zhang2023adding}, T2I-Adapter~\cite{mou2024t2i}, and IP-Adapter~\cite{ye2023ip} introduce structural and image-level controls by reconstruction-based fine-tuning. Personalized generation has been enhanced by methods like DreamBooth~\cite{ruiz2023dreambooth}, and other methods~\cite{galimage, wang2024moa, li2024photomaker, wang2024instantid, qian2024omniid, wang2024visual}, some of which use interleaved image-text inputs~\cite{pan2023kosmos, berman2024mumu}. However, these methods focus on reconstruction fidelity rather than in-context reasoning. In contrast, our method equips diffusion models with the multimodal in-context reasoning capabilities of VLMs.


\subsection{Unified understanding and generation}

Recent work on large language models (LLMs) and diffusion transformers~\cite{peebles2023scalable, flux_github} has inspired unified models for multimodal understanding and generation. These models either finetune LLMs to generate image tokens, which are then decoded into images via diffusion decoders~\cite{gemaking, pan2023kosmos, sun2023generative, koh2024generating, wu2023next, ye2024x}, or integrate text, image, and noise tokens within a transformer architecture~\cite{xiao2024omnigen, shi2024llamafusion}. They are typically trained end-to-end with diffusion losses or align output image tokens with CLIP text features using cosine similarity losses~\cite{wu2023next, ye2024x, tong2024metamorph}. While some methods exhibit preliminary reasoning capabilities, these capabilities remain constrained by the limits of diffusion training paradigms, the availability of reasoning datasets, and the representational limits of CLIP embeddings. In contrast, our method leverages vision-language training to transfer advanced multimodal reasoning capabilities in VLMs to diffusion models. 



\subsection{Vision-language training}
Vision-language training has proven effective in developing powerful multimodal models. CLIP-like models~\cite{radford2021learning, fang2023eva} use contrastive learning to align image and text embeddings. Recent large vision-language models (LVLMs)\cite{li2023blip, liu2023llava, zhu2023minigpt, llama3_meta, wang2024qwen2} align CLIP visual features with advanced large language models (LLMs)\cite{brown2020language, openai2024gpt4technicalreport, llama3_meta, yang2024qwen2} by fine-grained text prediction. This vision-language training enables robust multimodal feature alignment, developing multimodal understanding and reasoning by leveraging powerful LLMs.  Inspired by these advancements, our method employs vision-language training as a proxy task to bridge VLMs with diffusion models, inheriting their advanced multimodal reasoning capabilities.

\begin{figure*}[t]
  \centering
   \includegraphics[width=\linewidth]{imgs/framework.pdf}
   \vspace{-20pt}
   \caption{(a) In \textbf{ThinkDiff-LVLM} training, the LVLM processes an image and a text to generate text tokens and token features, with some token features randomly masked. Unmasked token features are passed to a trainable aligner network and an LLM decoder, predicting masked text tokens supervised by cross-entropy loss. In inference, the LLM decoder is replaced by a diffusion decoder, enabling in-context reasoning image generation from interleaved images and texts. (b) In \textbf{ThinkDiff-CLIP} training, a CLIP vision model extracts image token features which are then mapped by a trainable aligner network. A part of the image caption is encoded by the LLM encoder and concatenated with image tokens. These combined tokens are passed to the LLM decoder to predict the next part of the caption supervised by cross-entropy loss. In inference, the LLM decoder is replaced by a diffusion encoder, allowing coherent image generation based on multimodal context.
   }
   \label{fig:framework}
\vspace{-12pt}
\end{figure*}



\section{Method}

\subsection{Overview}

ThinkDiff employs VLMs to enable diffusion decoders to perform multimodal in-context reasoning. This is achieved by an aligner network that bridges a VLM and a diffusion decoder. As described in Section~\ref{sec:Introduction}, ThinkDiff simplifies the alignment process by introducing a proxy task that aligns the VLM with an LLM decoder using text supervision. This task is based on the shared input feature space between the LLM decoder and diffusion decoder. Figure~\ref{fig:arch_compare}b and Figure~\ref{fig:framework} illustrate the overall network structure and two model variants, respectively. The multimodal input comprises a set of images $\{I_i\}$ and text tokens $\{T_i\}$. The aligner network processes its input token features $\{x_i\}$ into its output token features $\{x_i'\}$. In training, ThinkDiff generates text tokens $\{y_i'\}$, supervised by ground truth text tokens $\{y_i\}$. In inference, it generates an image $I'$.

\textbf{Module Overview.} ThinkDiff comprises three submodules: a source VLM ($\mathcal{M}_\text{VLM}$), an aligner network ($\mathcal{M}_\text{AN}$), and a decoder. The decoder is a LLM decoder ($\mathcal{M}_\text{LLMD}$) in training and a diffusion decoder ($\mathcal{M}_\text{DiffD}$) in inference. 

\textbf{Source VLM.} The source VLM generates multimodal token features $\{x_i\}$, capturing the reasoning and understanding derived from multimodal inputs and transferring this information to diffusion decoders. The generation is expressed as: $\{x_i\} = \mathcal{M}_\text{VLM}(\{I_i\}, \{T_i\})$. This paper introduces two variants of ThinkDiff, each utilizing a different VLM. ThinkDiff-LVLM uses a large vision-language model (LVLM) to deliver advanced multimodal reasoning capabilities while ThinkDiff-CLIP leverages the semantically rich image embeddings provided by a CLIP vision encoder for image understanding.  Detailed descriptions of these variants can be found in Sections~\ref{sec:ThinkDiffLVLM} and~\ref{sec:ThinkCLIP}.

\textbf{Aligner network.} The aligner network bridges the source VLM with the LLM and diffusion decoder. It transforms token features $\{x_i\}$, which encapsulate rich reasoning information, into $\{x_i'\}$, making them interpretable by the LLM and diffusion decoder. This transformation is represented as: $\{x_i'\} = \mathcal{M}_\text{AN}(\{x_i\})$.
 
\textbf{Decoder.} The decoder operates differently during training and inference. The LLM decoder ($\mathcal{M}_\text{LLMD}$) is central to ThinkDiff's vision-language training. It is derived from an encoder-decoder LLM. In this LLM, the LLM encoder encodes token features and the LLM decoder generates text autoregressively from these token features. In ThinkDiff training, the VLM token features $\{x_i\}$ are mapped to $\{x_i'\}$ by the aligner network. The LLM decoder then treats $\{x_i'\}$ as if they were outputs from the LLM encoder and autoregressively decodes them into text $\{y_i'\}$. This process is expressed as: $\{y_i'\} = \mathcal{M}_\text{LLMD}(\{x_i'\})$. By this training, VLM token features are aligned with the decoder's input space, transferring reasoning capabilities from the VLM to $\mathcal{M}_\text{LLMD}$ in training and to $\mathcal{M}_\text{DiffD}$ in inference.

In inference, the LLM decoder is replaced by a diffusion decoder ($\mathcal{M}_\text{DiffD}$), which can interpret VLM's outputs and leverage the VLM's multimodal reasoning abilities for image generation. ThinkDiff can handle multiple images, texts, or interleaved sequences of images and texts during inference, thanks to their shared feature space. The generated image $I'$ is given by $I' = \mathcal{M}_\text{DiffD}(\{x_i'\})$.


\textbf{Loss.} We employ a cross-entropy loss between the LLM decoder's generated tokens $\{y_i'\}$ and the ground truth text tokens $\{y_i\}$ in training. Let $N$ be the length of $\{y_i'\}$, the loss is defined as: $L_\text{text} =  - \frac{1}{N}\sum_{i=1}^{N} \log{p(y_i' = y_i)}$.

In the following sections, we detail the design of the aligner network and two variants of ThinkDiff.

\subsection{Aligner network}\label{sec:Alignernetwork}

The aligner network $\mathcal{M}_\text{AN}$  is a lightweight module comprising two linear layers ($\mathcal{L}_\text{Linear}$), a GELU activation ($\mathcal{L}_\text{GELU}$) and an RMSNorm layer~\cite{zhang2019root} ($\mathcal{L}_\text{Norm}$). Given the VLM's output $\{x_i\}$, the output $\{x_i'\}$ of $\mathcal{M}_\text{AN}$ is:
\begin{equation}
    \{x_i'\} = \mathcal{L}_\text{Norm}(\mathcal{L}_\text{Linear}(\mathcal{L}_\text{GELU}(\mathcal{L}_\text{Linear}(\{x_i\}))))
\end{equation}
In training, only $\mathcal{M}_\text{AN}$ is updated. Despite its simplicity, $\mathcal{M}_\text{AN}$ can effectively aligns feature spaces of the powerful VLM and the LLM decoder in the training.

\textbf{Stable training.} Our experiments revealed that without a carefully initialized RMSNorm layer, ThinkDiff encounters convergence issues due to a scale mismatch between the VLM output space and the LLM decoder input space. To address this, we incorporate an RMSNorm~\cite{zhang2019root} layer into $\mathcal{M}_\text{AN}$, initialized with parameters from the LLM \textbf{encoder's} final RMSNorm layer. Since the LLM encoder output space aligns naturally with the LLM decoder input space, this initialization ensures consistent scale alignment at the start of training, significantly improving training stability and convergence.


\subsection{ThinkDiff-LVLM}\label{sec:ThinkDiffLVLM}

ThinkDiff-LVLM incorporates a decoder-only large vision-language model (LVLM) that excels at advanced in-context reasoning tasks, as its VLM. It aligns the deep features of the LVLM's \textbf{generated} tokens to both $\mathcal{M}_\text{LLMD}$ and $\mathcal{M}_\text{DiffD}$.

\noindent \textbf{Training.} The training framework is illustrated in Figure~\ref{fig:framework}a. The LVLM autoregressively generates text tokens $\{y_i\}$ from an input image $I$ and text prompt $T$. The corresponding token features $\{x_i\}$ are extracted from the LVLM's final RMSNorm layer. These features $\{x_i\}$ are then passed to $\mathcal{M}_\text{AN}$ and $\mathcal{M}_\text{LLMD}$, where they are decoded into text tokens $\{y_i'\}$, supervised by LVLM's generated tokens $\{y_i\}$. This setup is self-supervised, as both the token features $\{x_i\}$ and the supervision $\{y_i\}$ are all generated by the LVLM itself. This enables the aligner network to accurately transfer information from the LVLM to $\mathcal{M}_\text{LLMD}$ and $\mathcal{M}_\text{DiffD}$.

However, in this setup, token features $\{x_i\}$ have a one-to-one correspondence with the supervision text tokens $\{y_i\}$. This may cause the aligner to learn a trivial mapping between $\{x_i\}$ and $\{y_i\}$ without truly aligning features. We refer to this issue as ``shortcut mapping".

\noindent \textbf{Random masked training.} To address the ``shortcut mapping" issue, we introduce a random masked training strategy. In this strategy, text tokens $\{y_i\}$ and features $\{x_i\}$ are randomly split into two parts: $\{y_i^1\}$, $\{y_i^2\}$ and $\{x_i^1\}$, $\{x_i^2\}$, where $\{y_i^1\}$ correspond to $\{x_i^1\}$ and $\{y_i^2\}$ correspond to $\{x_i^2\}$. Only the first part $\{x_i^1\}$ is passed to the aligner and LLM decoder, generating text tokens $\{y_i'\}$ supervised by the second part of tokens $\{y_i^2\}$. This breaks the one-to-one correspondence, encouraging a more robust feature alignment. The generated tokens $\{y_i'\}$ are computed as:
\begin{equation}
    \{y_i'\} = \mathcal{M}_\text{LLMD}(\mathcal{M}_\text{AN}(f_\text{mask}(\mathcal{M}_\text{LVLMG}(I, T)))),
\end{equation}
where $f_{mask}$ is the random masking and $\mathcal{M}_\text{LVLMG}$ is the LVLM's generation process. The cross-entropy loss is: $L_\text{LVLM} =  - \frac{1}{N}\sum_{i=1}^{N} \log{p(y_i' = y_i^2)}$.


\textbf{Why use LVLM's generated tokens.} Some diffusion models~\cite{liu2024playground, xie2024sana} incorporate decoder-only LLMs for prompt encoding but actually treat them as \textbf{encoders} by using the deep features of \textbf{input} tokens. In contrast, ThinkDiff-LVLM uses the deep features of the \textbf{generated} tokens from the LVLM decoder as input to the aligner. This design is motivated by the insight that, in autoregressive models, reasoning is embedded in the generation process. Tokens are generated sequentially, conditioned on both the input context and the prior generated tokens. As a result, the full sequence of generated tokens captures the model's logical reasoning about the input context. By aligning these generated token features with diffusion models, ThinkDiff-LVLM ensures that the diffusion models inherit the LVLM's advanced multimodal reasoning capabilities.

\noindent \textbf{Inference for in-context reasoning.} In inference, as shown in Figure~\ref{fig:framework}a, the LLM decoder is replaced by a diffusion decoder for image generation. 
As shown in Figure~\ref{fig:teaser}a and~\ref{fig:reasoning_shot2}, ThinkDiff-LVLM effectively leverages the LVLM's multimodal in-context reasoning capability, using the context of interleaved images $\{I_i\}$ and texts $\{T_i\}$ to generate high-quality, logically coherent images that go beyond simply reconstructing the input content. The generated image $I'$ is:
\vspace{-5pt}
\begin{equation}
I' = \mathcal{M}_\text{DiffD}(\mathcal{M}_\text{AN}(\mathcal{M}_\text{LVLMG}(\{I_i\}, \{T_i\})))
\end{equation}




\begin{figure*}[t]
  \centering
   \includegraphics[width=\linewidth]{imgs/reasoning_shot2.pdf}
\vspace{-20pt}
   \caption{2-shot evaluation results on CoBSAT. The input structure is similar to Figure~\ref{fig:teaser}a. Given multimodal inputs, ThinkDiff-LVLM accurately captures both implicit attributes (e.g., wicker material) and explicit attributes (e.g. car), and generates a logically correct image (wicker car). In contrast, methods such as SEED-LLaMA~\cite{gemaking}, Emu~\cite{sun2023generative} and GILL~\cite{koh2024generating} produce inaccurate and lower-quality images. The ground truth implicit attribute is highlighted in red for readers' reference. See more results in Appendix Figure~\ref{fig:appendix_reasoning_shot2_compare} and \ref{fig:appendix_reasoning_shot2}.
   }
   \label{fig:reasoning_shot2}
   % \vspace{-15pt}
\end{figure*}




\begin{table*}[t]
  \centering
  \vspace{-7pt}
  \caption{2-shot CoBSAT accuracy of ThinkDiff-LVLM. It achieves SoTA accuracy on 9 of 10 tasks by large margins, increasing accuracy by more than 20\% on Action-I, Color-II, Action-II tasks which are particularly hard for other methods.
  }
  \vspace{-10pt}
  \resizebox{\linewidth}{!}{
    \begin{tabular}{l|ccccc|ccccc}
    \toprule
          & Color-I & Background-I & Style-I & Action-I & Texture-I & Color-II & Background-II & Style-II & Action-II & Texture-II \\
    \midrule
    SEED-LLaMA & \textbf{0.680}  & 0.348 & 0.203 & 0.182 & 0.196 & 0.287 & 0.467 & 0.297 & 0.261 & 0.163 \\
    Emu   & 0.065 & 0.051 & 0.057 & 0.052 & 0.078 & 0.062 & 0.109 & 0.081 & 0.092 & 0.074 \\
    GILL  & 0.171 & 0.054 & 0.069 & 0.063 & 0.074 & 0.010  & 0.043 & 0.024 & 0.022 & 0.040 \\
    ThinkDiff-LVLM  & \underline{0.622} & \textbf{0.349} & \textbf{0.237} & \textbf{0.459} & \textbf{0.290} & \textbf{0.511} & \textbf{0.534} & \textbf{0.340} & \textbf{0.534} & \textbf{0.292} \\
    \bottomrule
    \end{tabular}%
    }
  \label{tab:shot2cobsat}%
  \vspace{-11pt}
\end{table*}%




\subsection{ThinkDiff-CLIP}\label{sec:ThinkCLIP}


ThinkDiff-CLIP employs the vision encoder of a CLIP vision-language model~\cite{radford2021learning} pretrained on contrastive vision-language tasks, as its VLM. This encoder produces semantically rich image features, enabling aligned diffusion decoders to generate images based on the semantic understanding of input images.

\noindent \textbf{Training.}
Figure~\ref{fig:framework}b illustrates the training framework. The model is trained to predict partial captions for an input image. The CLIP vision encoder encodes the input image $I$ into image tokens $\{x_i\}$, which are downsampled via 2D pooling to reduce token count. The aligner network then maps $\{x_i\}$ to $\{x_i'\}$. Meanwhile, the image caption $T$ is randomly split into two parts: $T_1$ and $T_2$. The first part, $T_1$, is encoded into text token features $\{t_i\}$ by the LLM encoder. The aligned image tokens $\{x_i'\}$ are concatenated with $\{t_i\}$, and fed to the LLM decoder, which autoregressively predicts text $\{y_i'\}$ supervised by the second caption part $T_2$ (tokens $\{y_i^2\}$). 
The text generation process is formulated as:
\begin{equation}
    \{y_i'\} = \mathcal{M}_\text{LLMD}(f_\text{cat}(\mathcal{M}_\text{AN}(\mathcal{M}_\text{CLIP}(I)), \mathcal{M}_\text{LLME}(T_1))),
\end{equation}
where $f_\text{cat}$ denotes concatenation, and $\mathcal{M}_\text{LLME}$ is the LLM encoder. The cross-entropy loss is: $L_\text{CLIP} =  - \frac{1}{N}\sum_{i=1}^{N} \log{p(y_i' = y_i^2)}$. After training, the aligned image tokens $\{x_i'\}$ capture semantic details of the input image and can be interpreted by both $\mathcal{M}_\text{LLMD}$ and $\mathcal{M}_\text{DiffD}$.

\noindent \textbf{Inference.} In inference, as shown in Figure~\ref{fig:framework}b, the LLM decoder is replaced by a diffusion decoder for image generation. As shown in Figure~\ref{fig:teaser}b,~\ref{fig:multimodal_vision},~\ref{fig:multimodal_vision_only_2I}, and~\ref{fig:multimodal_vision_2I}, with an image as input, ThinkDiff-CLIP preserves semantic details of this image in the generated image. With multiple input images and text prompts, it seamlessly combines them into a semantically coherent image, as both image and text features are well-aligned within a shared feature space. These results highlight ThinkDiff-CLIP's ability to understand and compose multimodal context. In contrast, reconstruction-based diffusion finetuning methods like FLUX Ultra~\cite{flux_github}, often struggle to simultaneously adhere to image and text prompts. The generation of ThinkDiff-CLIP is:
\vspace{-5pt}
\begin{equation}
    I' = \mathcal{M}_\text{DiffD}(f_\text{cat}(\mathcal{M}_\text{AN}(\mathcal{M}_\text{CLIP}(\{I_i\})), \mathcal{M}_\text{LLME}(\{T_i\})))
\end{equation}



\begin{figure*}[htbp]
  \centering
   \includegraphics[width=\linewidth]{imgs/multimodal_vision.pdf}
  \vspace{-20pt}
    \caption{Generation results for single image (I) and single image with text prompt (I + T) inputs. Our method effectively integrates semantic details of both image and text modalities to produce coherent images. FLUX excels at replicating the input image but struggles to maintain consistency with additional text prompts. See more results in Figure~\ref{fig:appendix_multimodal_vision}.}
   \label{fig:multimodal_vision}
  \vspace{-5pt}
\end{figure*}




\section{Experiments}

\subsection{Implement details}\label{sec:Training_details}

\noindent \textbf{Base models.} We use publicly available FLUX.1-dev~\cite{flux_github} as the diffusion decoder as it employs T5~\cite{raffel2020exploring}, an LLM, as its prompt encoder. We use the corresponding T5 decoder as $\mathcal{M}_\text{LLMD}$. ThinkDiff-LVLM uses Qwen2-VL~\cite{wang2024qwen2} as the VLM, which excels at vision-language reasoning on interleaved images and texts. ThinkDiff-CLIP employs the vision encoder from the ViT-G/14 model of EVA-CLIP~\cite{fang2023eva}.

\textbf{Training and evaluation resources.} We use public image-caption datasets for training. ThinkDiff-LVLM is trained for 25,000 steps on 4 A100 GPUs for 5 hours, with a total batch size of 96. ThinkDiff-CLIP is trained for 100,000 steps on 4 A100 GPUs by one day, with a total batch size of 168. See Appendix~\ref{datasetdetails} for detailed dataset settings. The multimodal in-context reasoning capabilities of ThinkDiff-LVLM are evaluated on the challenging CoBSAT benchmark~\cite{zeng2024can} and measured by prediction accuracy. More details are in its paper.
We assess ThinkDiff-CLIP's reasoning and composition abilities on various prompts and images from ~\cite{ruiz2023dreambooth, peng2024dreambench, ye2023ip}.



% Table generated by Excel2LaTeX from sheet 'cobsat'
\begin{table*}[t]
  \centering
  \caption{4-shot CoBSAT accuracy of ThinkDiff-LVLM shows a 27\% average improvement over other methods and a 4.7\% increase over its 2-shot results, highlighting its ability to handle complex in-context reasoning. In contrast, SEED-LLaMA~\cite{gemaking}, Emu~\cite{sun2023generative}, and GILL~\cite{koh2024generating} exhibit reduced performance in 4-shot evaluations, indicating their struggle with increased input complexity. Improvement ratios over SoTA are also provided.
  }
  \vspace{-7pt}
  \resizebox{\linewidth}{!}{
    \begin{tabular}{cccccc|ccccc}
    \toprule
          & Color-I & Background-I & Style-I & Action-I & Texture-I & Color-II & Background-II & Style-II & Action-II & Texture-II \\
    \midrule
          SEED-LLaMA & 0.482 & 0.211 & 0.141 & 0.053 & 0.122 & 0.252 & 0.076 & 0.268 & 0.207 & 0.105\\
    Emu   & 0.063 & 0.018 & 0.045 & 0.048 & 0.097 & 0.037 & 0.122 & 0.109 & 0.077 & 0.088 \\
    GILL  & 0.106 & 0.044 & 0.041 & 0.073 & 0.087 & 0.022 & 0.059 & 0.044 & 0.032 & 0.067 \\
    % Table generated by Excel2LaTeX from sheet 'cobsat'
    Ours & \textbf{0.638} & \textbf{0.362} & \textbf{0.254} & \textbf{0.434} & \textbf{0.317} & \textbf{0.610} & \textbf{0.590} & \textbf{0.432} & \textbf{0.664} & \textbf{0.332} \\

    \midrule
    \textbf{Improvement ($\Delta \%$)} & \textbf{32.4\%}  & \textbf{71.6\%}  & \textbf{80.1\%}  & \textbf{718.9\%} & \textbf{159.8\%} & \textbf{142.1\%} & \textbf{676.3\%} & \textbf{61.2\%}  & \textbf{220.8\%} & \textbf{216.2\%} \\
    \bottomrule
    \end{tabular}%
    }
  \label{tab:shot4cobsat}%
  \vspace{-11pt}
\end{table*}%





\begin{figure}[t]
  \centering
   \includegraphics[width=0.8\linewidth]{imgs/loss_plot.pdf}
   \vspace{-10pt}
   \caption{Training losses (log scale) of ThinkDiff-LVLM comparing different RMSNorm designs. Disabling RMSNorm (w/o RMSNorm) or using the default RMSNorm initialization (RMSNorm w/ Default init.) results in significantly unstable training.
   }
   \vspace{-15pt}
   \label{fig:rmsnorm_loss_curve}
\end{figure}





% Table generated by Excel2LaTeX from sheet 'cobsat'
\begin{table*}[htbp]
  \centering
  \caption{2-shot results on CoBSAT ablating models with and without masking, and using deep features of input tokens.
  }
  \vspace{-7pt}
  \resizebox{\linewidth}{!}{
    \begin{tabular}{cccccc|ccccc}
    \toprule
          & Color-I & Background-I & Style-I & Action-I & Texture-I & Color-II & Background-II & Style-II & Action-II & Texture-II \\
    \midrule

    Ours using input tokens & 0.024 & 0.004 & 0.03  & 0.011 & 0.032 & 0.007 & 0.008 & 0.012 & 0.019 & 0.011 \\
    Ours w/o masked training & 0.548 & 0.215 & 0.105 & 0.256 & 0.187 & 0.510  & 0.338 & 0.156 & 0.325 & 0.228 \\
    \midrule
    Ours & \textbf{0.622} & \textbf{0.349} & \textbf{0.237} & \textbf{0.459} & \textbf{0.290} & \textbf{0.511} & \textbf{0.534} & \textbf{0.340} & \textbf{0.534} & \textbf{0.292} \\
    \bottomrule
    \end{tabular}%
    }
  \label{tab:ablatemaskedtraininginputtoken}%
  \vspace{-10pt}
\end{table*}%






% Table generated by Excel2LaTeX from sheet 'cobsat'
\begin{table}[htbp]
  \centering
  \caption{Training resources and 4-shot accuracy. ThinkDiff-LVLM drastically reduces GPU usage and training time and improves accuracy from 0.192, 0.07, and 0.058 to 0.463.}
  \vspace{-10pt}
    \resizebox{0.8\linewidth}{!}{
    \begin{tabular}{c|ccc}
    \toprule
          & GPU No. & Time / h & Average Acc. \\
    \midrule
    SEED-LLaMA & 64 A100 & 216   & 0.192 \\
    Emu   & 128 A100 & 48    & 0.070 \\
    GILL  & 2 A6000 & 48    & 0.058 \\
    ThinkDiff-LVLM  & 4 A100 & 5     & \textbf{0.463} \\
    \bottomrule
    \end{tabular}%
    } 
    \vspace{-5pt}
  \label{tab:trainingresources}%
\end{table}%



\textbf{Baselines.} We compare ThinkDiff-LVLM with SEED-LLaMA~\cite{gemaking}, Emu~\cite{sun2023generative} and GILL~\cite{koh2024generating} that can generate images based on image and text inputs. SEED-LLaMA is the previous state-of-the-art (SoTA) model on the CoBSAT benchmark. We compare ThinkDiff-CLIP with FLUX1.1-pro-Ultra API~\cite{flux_ultra}, which supports image generation from image and text inputs. FLUX1.1-pro-Ultra is possibly finetuned by diffusion training and image reconstruction supervision, which differs fundamentally from our method.




\subsection{Evaluation results of ThinkDiff-LVLM}\label{Evaluation_results_of_ThinkDiff-LVLM}

We evaluate ThinkDiff-LVLM on the 10 multimodal in-context reasoning generation tasks in the CoBSAT, in both 2-shot and 4-shot settings. In each setting, 2 or 4 input images and corresponding texts are provided as input, with an additional instruction prompt to make the model generate the next image that contains the correct object and attribute, based on in-context reasoning, (see Appendix Section~\ref{datasetdetails}). Tables~\ref{tab:shot2cobsat} and~\ref{tab:shot4cobsat} report the accuracy for 2-shot and 4-shot evaluations, respectively. Results of SEED-LLaMA~\cite{gemaking}, Emu~\cite{sun2023generative} and GILL~\cite{koh2024generating} are token from the CoBSAT~\cite{zeng2024can} paper.

As shown in Table~\ref{tab:shot2cobsat} for 2-shot evaluation, ThinkDiff-LVLM achieves SoTA performance on 9 out of 10 tasks, outperforming other methods by a large margin. Baselines like Emu and GILL perform poorly on most tasks with accuracy below 10\%, reflecting the difficulty of these tasks. While SEED-LLaMA performs well on task Color-I, it underperforms ThinkDiff-LVLM on other tasks. Notably, ThinkDiff-LVLM exceeds the previous SoTA by over 20\% in accuracy on Action-I, Color-II, and Action-II tasks, showcasing its superior in-context reasoning generation capabilities.







More importantly, in the more complex 4-shot evaluation (Table~\ref{tab:shot4cobsat}), ThinkDiff-LVLM further demonstrates its superior performance, outperforming all methods across every task, with an average accuracy improvement of 27\%. Notably, it also shows a consistent 4.7\% accuracy increase over its 2-shot performance, highlighting its ability to effectively leverage additional complex information. In contrast, the accuracy of baselines drops significantly with 4-shot inputs, indicating their difficulties with the increased complexity of multimodal inputs. This underscores that ThinkDiff-LVLM not only excels in advanced in-context reasoning but also adapts more effectively to complex multimodal inputs. Figures~\ref{fig:reasoning_shot2}, \ref{fig:appendix_reasoning_shot2_compare}, and \ref{fig:appendix_reasoning_shot2} present the qualitative comparison, where ThinkDiff-LVLM generates both correct and significantly higher-quality images compared to other methods.



\subsection{Evaluation results of ThinkDiff-CLIP}\label{sec:EvaluationresultsofThinkDiffCLIP}

We evaluate ThinkDiff-CLIP on various test cases to demonstrate its ability to semantically understand images and enable coherent composing of image and text modalities.

\noindent \textbf{Single image + text prompt.}
Figure~\ref{fig:multimodal_vision} and Appendix Figure~\ref{fig:appendix_multimodal_vision} show results with a single image as input. FLUX Ultra~\cite{flux_ultra}, possibly finetuned by reconstruction-based diffusion training, performs well in ``copy-pasting" the input image (FLUX Ultra + I), but struggles to maintain coherence when an additional \textbf{text} prompt is included (FLUX Ultra + I + T). In contrast, ThinkDiff-CLIP excels at understanding the semantic details of the input image and effectively integrates both image and text to generate logically coherent outputs (Ours + I and Ours + I + T). 

\noindent \textbf{Multiple images + text prompt.} ThinkDiff-CLIP is flexible and can handle multiple images and text prompts. As shown in Figure~\ref{fig:multimodal_vision_only_2I} and Appendix Figure~\ref{fig:multimodal_vision_2I}, it can combine semantic details from two images in a reasonable and coherent manner. Figure~\ref{fig:multimodal_vision_2I} further demonstrates that with an additional text prompt (Ours + 2I + T), ThinkDiff-CLIP effectively incorporates the prompt into the generation. 

These multimodal generation results highlight the advantage of our vision-language training, which aligns multimodal features into a shared space, enabling flexible handling of complex multimodal understanding and composing tasks.




\noindent \textbf{Video generation.} ThinkDiff-CLIP is agnostic to diffusion decoders, and is versatile for integrating models like CogVideoX-5B~\cite{yang2024cogvideox}, a text-to-video diffusion model. As shown in Appendix Figure~\ref{fig:image_text_video}, a background image is fed to the vision encoder and aligner network, along with a text prompt, and then to CogVideoX decoder. The model generates a coherent video by seamlessly integrating images and text. This shows ThinkDiff-CLIP's flexibility and broad applicability for multimodal generation tasks.





\begin{figure}[t]
  \centering
   \includegraphics[width=\linewidth]{imgs/multimodal_vision_only_2I_arxiv.pdf}
  \vspace{-20pt}
   \caption{Results of \textbf{ThinkDiff-CLIP} composing two images. It creatively merge semantic details of both images. See more results in Appendix Figure~\ref{fig:appendix_multimodal_vision_only_2I}.}
   \label{fig:multimodal_vision_only_2I}
   \vspace{-15pt}
\end{figure}



% \vspace{-10pt}
\subsection{Ablation study}


\noindent \textbf{RMSNorm in the aligner network.}
As discussed in Section~\ref{sec:Alignernetwork}, the RMSNorm layer and its initialization are critical for training convergence. Figure~\ref{fig:rmsnorm_loss_curve} compares training losses of three setups: without a RMSNorm layer, with default initialization, and with our final design. Without a RMSNorm layer or using default initialization, the training loss fails to converge while with our design, the loss converges to a reasonable value, leading to strong evaluation performance. This comparison validates the effectiveness of our design.



\noindent \textbf{Random masked training strategy.} As discussed in Section~\ref{sec:ThinkDiffLVLM}, we introduce a masked training strategy to address the ``shortcut mapping" problem in ThinkDiff-LVLM training. In Table~\ref{tab:ablatemaskedtraininginputtoken}, we compare the 2-shot accuracy on CoBSAT benchmark for models trained with and without this strategy. Without the random masked training, ThinkDiff-LVLM converges quickly but achieves inferior evaluation accuracy, indicating incomplete feature space alignment. In contrast, with the random masked training, the model achieves SoTA accuracy on the evaluation tasks. This validates the critical role of the random masked training for proper feature alignment in ThinkDiff-LVLM.



\noindent \textbf{Using generated tokens of LVLM.} As discussed in Section~\ref{sec:ThinkDiffLVLM}, ThinkDiff-LVLM uses deep features of generated tokens from the LVLM to effectively transfer reasoning information to diffusion decoders. In this study, we train a model using the deep features of input tokens of LVLM for alignment, with these features extracted from the final normalization layer of the LVLM. As shown in Table~\ref{tab:ablatemaskedtraininginputtoken}, using input token features for alignment leads to a significant performance drop, underscoring the critical role of generated tokens in successfully transferring reasoning capabilities.


\noindent \textbf{Training time and GPU usage.} Table~\ref{tab:trainingresources} summarizes the training time, GPU requirements, and 4-shot average accuracy on CoBSAT for different methods. Our method drastically reduces GPU usage from 128 A100 GPUs to just 4 and cuts training time from 216 hours to only 5 hours. Meanwhile, it achieves a significant improvement in average accuracy, increasing from 0.192, 0.070, and 0.058 to an impressive 0.463. These results highlight the efficiency and effectiveness of our novel alignment paradigm.


\section{Conclusion}
\vspace{5pt}
We introduced ThinkDiff, a novel alignment paradigm equipping diffusion models with multimodal in-context reasoning of VLMs by vision-language training. ThinkDiff sets a new SoTA on the CoBSAT benchmark and excels in various reasoning tasks. Future work will address its limitations (Appendix~\ref{sec:Limitation}), and extend its capabilities to modalities like audio and video to develop any-to-any foundation models.


\section{Impact Statements}

This paper proposed ThinkDiff, a novel alignment method that enhances text-to-image diffusion models by integrating multimodal in-context reasoning capabilities from vision-language models. By simplifying the alignment process between the VLM and diffusion decoder, ThinkDiff democratizes complex multimodal reasoning generation tasks and make them more accessible and efficient to train. ThinkDiff has potential applications across different fields, such as education, design, and creative industries. However, similar to other text-to-image diffusion models and large vision-language models, ThinkDiff could be potentially misused for generating misleading and harmful content. To mitigate these problems, it is essential to deploy the model responsibly and implement robust safeguards to prevent misuse.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section*{APPENDIX}


\section{Limitation}\label{sec:Limitation}


Despite ThinkDiff's strong performance in reasoning generation tasks, several limitations remain for future work. First, while it substantially outperforms existing methods,  ThinkDiff still encounters difficulties with certain complex cases. Enhancing reasoning accuracy may require stronger VLMs, better data quality, advanced diffusion models, and improved training strategies. Second, although this work primarily focuses on logical reasoning rather than preserving image fidelity, improving fidelity could expand its applications in tasks like image editing. Finally, more diverse evaluation tasks are needed to better assess reasoning performance and advance research in this area.



\section{Dataset details}\label{datasetdetails}
For ThinkDiff-LVLM, the training process requires images and their corresponding VLM-generated tokens. We randomly sample 1.7 million images from the CC3M~\cite{sharma2018conceptual}, CC12M~\cite{changpinyo2021conceptual}, and SBU~\cite{ordonez2011im2text} datasets. These images are preprocessed using Qwen2-VL, which generates detailed descriptions based on randomly selected text prompts from a predefined set. The generated text tokens and token features are stored for training the alignment. We generate 64 tokens for each data sample. Data processing is accelerated using the vLLM framework~\cite{kwon2023efficient}. 

For ThinkDiff-CLIP, the training utilizes images and their corresponding captions, sampled from a combination of CC3M~\cite{sharma2018conceptual}, CC12M~\cite{changpinyo2021conceptual}, SBU~\cite{ordonez2011im2text}. 


The predefined prompts for ThinkDiff-LVLM are designed to encourage the VLM to generate detailed descriptions of the image. Below is a list of the prompts we use, some of which are adapted from LLaVA~\cite{liu2023llava}.

\begin{itemize}
\item Describe the image concisely.
\item Provide a brief description of the given image.
\item Offer a succinct explanation of the picture presented.
\item Summarize the visual content of the image.
\item Give a short and clear explanation of the subsequent image.
\item Share a concise interpretation of the image provided.
\item Present a compact description of the photo's key features.
\item Relay a brief, clear account of the picture shown.
\item Render a clear and concise summary of the photo.
\item Write a terse but informative summary of the picture.
\item Create a compact narrative representing the image presented.
\item Generate a prompt that can recreate the image in a 2D diffusion model.
\item Provide a descriptive prompt to reproduce the given image using a diffusion model.
\item Create a prompt suitable for a 2D diffusion model to generate the same image.
\item Summarize the visual details as a prompt for a 2D diffusion model.
\item Write a clear prompt to guide a 2D diffusion model in recreating the image.
\end{itemize}

\textbf{Evaluation on CoBSAT.} As described in Section~\ref{Evaluation_results_of_ThinkDiff-LVLM}, when evaluating ThinkDiff-LVLM on the CoBSAT dataset, we use an instruction prompt to guide Qwen2-VL to generate the next image based on multimodal inputs. Qwen2-VL is a vision-language model primarily designed to answer questions by text. It does not automatically know that we want it to generate the next image and we also do not finetune it for this specific task. Therefore, the instruction prompt is necessary. The instruction prompt used in our evaluation is:

\begin{itemize}
    \item I give you several words and pictures. First, please analyse what the next picture is. Then give me a detailed diffusion prompt to describe the next picture. Please only provide me the detailed prompt and start the answer with `Create an image'.
\end{itemize}




\section{More high-quality results}\label{Morehighqualityresults}

\subsection{ThinkDiff-LVLM}


Figure~\ref{fig:appendix_reasoning_shot2_compare} and~\ref{fig:appendix_reasoning_shot2} demonstrate more high-quality results of ThinkDiff-LVLM on 2-shot evaluation in CoBSAT benchmark. ThinkDiff-LVLM can not only generate images with logically correct objects and attributes based on advanced reasoning, but also generate much higher-quality images than SEED-LLaMA~\cite{gemaking}, Emu~\cite{sun2023generative}, and GILL~\cite{koh2024generating}. These compared methods typically generate wrong images of lower quality.


\begin{figure*}[htbp]
  \centering
   \includegraphics[width=0.82\linewidth]{imgs/appendix_reasoning_shot2_compare.pdf}
   \caption{More 2-shot reasoning results of ThinkDiff-LVLM on CoBSAT benchmark.}
   \label{fig:appendix_reasoning_shot2_compare}
\end{figure*}


\begin{figure*}[htbp]
  \centering
   \includegraphics[width=\linewidth]{imgs/appendix_reasoning_shot2.pdf}
   \caption{More 2-shot reasoning results of ThinkDiff-LVLM on CoBSAT benchmark.}
   \label{fig:appendix_reasoning_shot2}
\end{figure*}



\subsection{ThinkDiff-CLIP}

Figure~\ref{fig:appendix_multimodal_vision} shows more results with a single image (I) or a single image with a text prompt (I + T) as input. FLUX Ultra~\cite{flux_ultra} struggles to maintain coherence when an additional text prompt is included (FLUX Ultra + I + T) while ThinkDiff-CLIP excels at integrating both image and text to generate logically coherent images (Ours + I and Ours + I + T). 

Figure~\ref{fig:appendix_multimodal_vision_only_2I} and~\ref{fig:multimodal_vision_2I} shows more results of our ThinkDiff-CLIP handling multiple images and text prompts. ThinkDiff-CLIP effectively combines semantic details from two input images in a coherent manner and seamlessly integrates text prompts to guide the generation, showcasing its flexibility and capability for complex multimodal tasks.


\begin{figure*}[htbp]
  \centering
   \includegraphics[width=\linewidth]{imgs/appendix_multimodal_vision.pdf}
   \caption{Generation results of a single image and a text prompt of ThinkDiff-CLIP.}
   \label{fig:appendix_multimodal_vision}
\end{figure*}



\begin{figure*}[htbp]
  \centering
   \includegraphics[width=0.8\linewidth]{imgs/appendix_multimodal_vision_only_2I_arxiv.pdf}
   \caption{Multiple input image generation results of ThinkDiff-CLIP.}
   \label{fig:appendix_multimodal_vision_only_2I}
\end{figure*}

\begin{figure*}[htbp]
  \centering
   \includegraphics[width=0.92\linewidth]{imgs/multimodal_vision_2I_arxiv.pdf}
   \caption{Generation results for multiple images (2I) and multiple images with a text prompt (2I + T) of ThinkDiff-CLIP.}
   \label{fig:multimodal_vision_2I}
\end{figure*}




\section{Video results of ThinkDiff-CLIP}\label{sec:Video_results_of_ThinkDiff-CLIP}

As discussed in Section~\ref{sec:EvaluationresultsofThinkDiffCLIP}, ThinkDiff-CLIP can integrate CogVideoX~\cite{yang2024cogvideox} model for text-to-video generation. Figure~\ref{fig:image_text_video} demonstrates frames of video generation results, validating ThinkDiff-CLIP's flexibility and broad applicability for multimodal generation tasks.





\begin{figure*}[htbp]
  \centering
   \includegraphics[width=\linewidth]{imgs/image_text_video.pdf}
   \caption{Image + text to video generation results of ThinkDiff-CLIP.}
   \label{fig:image_text_video}
\end{figure*}





% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
