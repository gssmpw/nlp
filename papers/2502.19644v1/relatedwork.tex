\section{Related Work}
This section first reviews VR video quality assessment. Then, we review continual learning techniques and their applications.

% \subsection{Video Quality Assessment}

\subsection{VR Video Quality Assessment}
In this study, VR video refers to 360-degree videos in VR for the sake of simplicity.
360-degree videos \cite{pirker2021potential}, also known as panoramic videos, or spherical videos, are recordings that simultaneously capture the entire surrounding environment using an omnidirectional camera or a network of cameras. Existing VR video quality assessment (VQA) methods derive a large part from image and video quality assessment techniques but require adaptation due to the different data formats of VR content.

These methods can be categorized into projected 2D plane, spherical surface, and projected rectilinear viewports.
Planar methods \cite{dataset2018bridge,kim2019deep,sun2017weighted,zakharchenko2016quality} address sphere-to-plane projection issues with techniques such as latitude-dependent weighting \cite{sun2017weighted} and Craster parabolic projection \cite{zakharchenko2016quality}. Spherical methods, such as S-PSNR \cite{yu2015framework} and S-SSIM \cite{chen2018spherical}, aggregate local quality estimates over the sphere. Viewport-based methods, such as those of Xu et al. \cite{xu2020blind} and Li et al. \cite{li2019viewport}, focus on evaluating key viewports, while recent approaches convert panoramic images to planar formats for use with established planar I/VQA methods \cite{sui2021perceptual}. 

Firstly, we observed that some methods in VQA and VR-VQA, such as \cite{wen2024perceptual,li2022blindly}, primarily focus on optimizing the correlation objective, often neglecting precision. 
For instance, Wen et al. \cite{wen2024perceptual} rely solely on a hybrid correlation loss for VR-VQA, achieving high correlation scores but exhibiting significant absolute errors (see \cref{tab:joint_cmp}). 
Such shortcomings highlight the practical limitations of these methods in user-centric applications, where precise quality evaluation is crucial, emphasizing the need for a correlation-precision trade-off.
In contrast, our work differs from related approaches in action quality assessment \cite{zhou2023hierarchical,zhou2024cofinal,zhou2024comprehensivesurveyactionquality}, which predominantly focus on optimizing the precision objective, often overlooking the importance of correlation.

Secondly, these methods often rely on static datasets \cite{wen2024perceptual,meng2021viewport,lopes2018subjective,dataset2018bridge}, which limits their adaptability to dynamic and evolving VR video content. For example, the VOD-VQA dataset \cite{meng2021viewport}, a large panoramic video database, includes only 18 reference videos and 774 distorted videos with limited diversity. This constrained scope hampers the model's ability to generalize across distortions in real-world VR applications. 
Such weak generalization exacerbates the challenge of balancing precision and correlation, highlighting the need for methods that adapt to evolving changes for accurate quality evaluations.

\subsection{Continual Learning}
Continual Learning (CL), also known as lifelong learning, is a field of machine learning focused on enabling models to learn from new data continuously while retaining previously acquired knowledge \cite{wang2021memory,wang2024comprehensive,wang2023incorporating}. This is particularly relevant in scenarios where data evolves, as seen in various applications \cite{graffieti2022continual} including action quality assessment \cite{zhou2024magr,li2024continual,dadashzadeh2024pecop}, semantic segmentation \cite{zhu2023continual,toldo2024learning}, and object detection \cite{liu2023continual,wu2023label}. 

The key challenge of CL is to mitigate catastrophic forgetting \cite{wang2021ordisco,wang2022coscl,wang2023hierarchical}, where a model trained on new tasks rapidly loses its performance on previously learned tasks. This problem arises because traditional machine learning methods typically update the entire modelâ€™s parameters based on the most recent data, potentially overwriting information crucial for earlier tasks \cite{van2024continual}.
Representative strategies for mitigating catastrophic forgetting include weight regularization \cite{tang2021gradient}, memory replay \cite{buzzega2020dark,rolnick2019experience,gao2023ddgr}, parameter isolation \cite{james2017ewc}, etc.
% \ly{Representative strategies for mitigating catastrophic forgetting include weight regularization \cite{tang2021gradient}, memory replay \cite{buzzega2020dark,rolnick2019experience,gao2023ddgr}, parameter isolation \cite{james2017ewc}, etc.}
% Addressing catastrophic forgetting involves various strategies such as regularization techniques \cite{tang2021gradient}, replay mechanisms \cite{buzzega2020dark,rolnick2019experience,gao2023ddgr}, memory augmentation, and meta-learning. 
These methods aim to balance acquiring new information with retaining existing knowledge, thereby enhancing the model's capacity to learn continuously without degrading its performance on earlier tasks.

Although traditional replay-based methods \cite{buzzega2020dark,rolnick2019experience,zhou2024magr} are effective in mitigating catastrophic forgetting, they present challenges for applications with limited storage capacity, particularly for VR devices. To this end, Wang et al. \cite{wang2021memory} first introduce the data compression concept to reduce the storage cost of representative samples, thereby allowing more representative samples to be stored in the memory buffer. Yang et al. \cite{yang2024probing} propose an effective compression rate selection strategy to balance the quality and quantity of stored samples, mitigating the domain shift issue \cite{janeiro2023visual} that arises when compressed samples degrade classification accuracy.
However, these approaches \cite{wang2021memory,yang2024probing} applied to image data neglect the additional complexity of video data, particularly in VR, where maintaining temporal coherence is essential.
To address these issues, we propose a tailored feature adapter for VR-VQA, designed to reconstruct temporal coherence directly from the latent space.
% Thus, it is necessary to develop new solutions tailored for VR-VQA to address these challenges effectively.