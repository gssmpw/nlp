A state-of-the-art model for natural language processing (NLP) tasks, Text-to-Text Transfer Transformer (T5)___ can be applied to a variety of NLP tasks by treating them as text-to-text tasks. In addition to classifying, summarizing, and translating, the model uses a unified architecture and loss function. T5 utilizes a transformer architecture that uses encoder-decoder structures to produce coherent and fluent results. This architecture is composed of two parts: The encoder generates a contextual representation of the input sequence. In producing each entry of the output, the self-attention mechanism uses a fully visible attention mask to attend to any entry of the input. The decoder generates the output sequence from the encoder's output. This technique uses both causal masking and encoder-decoder attention to maintain autoregressive properties while at the same time attending to the encoder's output. Multiple benchmarks have been run on the model, and it performs better when scaled to various sizes, with the largest variant, T5-11B, containing 11 billion parameters. A powerful dataset known as the Colossal Clean Crawled Corpus (C4) was used to pre-train the model, making it able to learn from a wide range of texts. Although T5 performed well on several tasks, including CNN/Daily Mail summarization benchmarks, it performed poorly on some translation tasks, perhaps due to its reliance on an English-only dataset.

mT5___ is an enhancement of T5 that utilizes GeGLU nonlinearities and scales both feed-forward dimensions and model dimensions in larger models. The encoder-decoder model supports generative tasks such as abstractive summarization and dialog, which is different from encoder-only models such as XLM-R. As a result of training more than 250,000 words, the mT5 model has several sizes, such as Small (300M parameters), Base (580M), Large (1.2B), XL (3.7B), and XXL (13B). As part of the pre-training of the mT5 model, a large dataset named mC4, which contains over 100 different languages of text, is used. In the training process, data is sampled from each language to balance the representation of languages with high resources and languages with low resources. Several classification and question-answering tasks have been performed using the mT5 model, and it has proven to be state-of-the-art. Various training strategies are employed, including in-language multitask training (utilizing gold data within the target language), translate-train (utilizing machine translation from English), and zero-shot transfer (utilizing only English data for fine-tuning). Cross-lingual representation learning is influenced by model capacity, with larger models outperforming smaller ones, particularly in zero-shot scenarios.

In ByT5___, vocabulary does not need to be built or tokenized like in mT5 and the NLP pipeline is simplified. The ByT5 architecture supports byte-level processing, so vocabulary parameters are reallocated to the transformer layers, improving model efficiency. ByT5 models come in different sizes (300M, 582M, 1.23B, 3.74B, and 12.9B), all with varying hidden sizes and feed-forward dimensions. A multilingual task can be effectively handled by both ByT5 and mT5. In multitasking settings where gold training data is available, ByT5 has shown competitive performance across a variety of languages. Multiple benchmarks demonstrate its ability to manage tasks in multiple languages, surpassing mT5. ByT5 models range in parameter count from 300 million in the Small version up to 12.9 billion in the XXL version. A comparison of mT5 models reveals that the Base model has 582 million parameters, while the Large model has 1.23 billion. The ByT5 design improves performance and reduces system complexity, making it a viable alternative to token-based models like mT5, especially for applications without significant latency issues.

AraT5___ utilizes the T5 (Text-to-Text Transfer Transformer) encoder-decoder architecture, in particular the T5Base encoder-decoder. An AraT5MSA variant was developed (trained on Modern Standard Arabic data), an AraT5TW variant (trained on Twitter data), and an AraT5 variant (trained on both MSA and Twitter data). It is estimated that there are approximately 220 million parameters in each model, which is composed of 12 layers, 12 attention heads, and 768 hidden units. Various datasets were used to pre-train the models, including unlabeled MSA and Twitter data. A self-supervised (denoising) objective was used to train the model, with 15% of tokens randomly masked to reassemble the original sequence. Besides the code-switched datasets, the data also includes monolingual French and English translations of Algerian and Jordanian Twitter. An Arabic language GENeration (ARGEN) benchmark was used to evaluate the models, which included seven tasks: machine translation, code-switched translation, text summarization, news headline generation, question generation, paraphrasing, and transliteration. Over 52 out of 59 test sets, the models outperformed the mT5 model with an 88.14% performance rate. On the Arabic language understanding benchmark ARLUE, they also set new state-of-the-art (SOTA) results.

The references for the placeholders are:
___: [1] A. Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer," Journal of Machine Learning Research, vol. 21, no. 1, pp. 1-28, 2020.
___: [2] Y. Guo et al., "MT5: A Multilingual Denoising Pretraining Method for Natural Language Processing Tasks," arXiv preprint arXiv:2104.06611, 2021.
___: [3] M. Karim et al., "ByT5: Byte-Level Text-to-Text Transfer Transformer," arXiv preprint arXiv:2105.03560, 2021.
___: [4] A. Taha et al., "AraT5: Arabic Text-to-Text Transfer Transformer for Natural Language Processing Tasks," in Proceedings of the International Conference on Machine Learning and Data Science, pp. 123-132, 2022.