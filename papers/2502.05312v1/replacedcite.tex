\section{Related Work}
In this section, we provide an overview of Arabic synthetic data techniques, grammatical error correction multi-label classification, pre-trained BERT models, and large language Text-to-Text Transformers used in literature.

\subsection{Arabic Synthetic Data Techniques}
This section discusses the synthetic data technique used in prior ArabGEC research. Solyman et al.____ generated only spelling errors using the confusion function. The corrupted sentences were constructed by selecting random words in sentences, then deleting them, duplicating them, substituting letters within them, or inserting new letters. Every sentence had a probability distribution of 10\% for duplicating and deleting words and 40\% for changing letters or deleting characters. An Al-Watan 20021 \footnote{\url{https://sourceforge.net/projects/arabiccorpus/files/latest/download}}corpus was used. Originally, the corpus contained ten million words written by professional journalists in Modern Standard Arabic (MSA). Al-Watan newspaper provided the data, which is divided into six categories. In the first step, the data was compiled and reorganized into a single file, followed by the normalization of sentences containing emoticons, non-alphanumeric characters, and exclamation marks. After splitting the sentences into maximum lengths of 50 words, they dropped sentences that were less than 5 words in length. In training and development corpora, 18,061,610 million words were generated. Synthetic examples generated are of low quality and contain only spelling errors, making them inefficient.

Solyman et al.____  developed a technique called the noising method for generating synthetic training data for ArabGEC models. It consists of a combination of back-translation and direct error injection. Despite using a neural noise model based on multi-head attention, back translation cannot predict sentences correctly because it is based on a limited dataset. A sequence of four words is randomly selected and fed into the back translation model for each original sentence of more than ten words. This technique generates spelling, syntax, and grammar errors without predicting their types and numbers. A direct error injection inserts spelling errors directly into words or normalizes Arabic characters that have a similar shape. They used the Open Source International Arabic News (OSIAN) corpus\footnote{\url{https://wortschatz.uni-leipzig.de/en/download/Arabic}} containing 15,001,707 sentences and 367,572,569 words. The data was preprocessed and cleaned of mentions, non-UTF8 encoding, diacritics, hashtags, over-space, and links. Additionally, they divided the text into sentences of at least five words each. After data cleaning, there were 13,333,929 sentences.

Solyman et al.____ proposed data augmentation approaches for ArabGEC. They describe seven specific approaches to data augmentation: Misspelling, Swapping, Tokens, Sources, Reverses, Monos, and Replaces.  Misspelling involves inserting or deleting spelling errors within a target sentence. In Swap, a random swap is performed to create a new word order in the target sentence. Token involves substituting specific words for an alternative token in a target sentence, such as <UNK>. A source sentence is copied to a target sentence to improve accuracy. In reverse, the target sentence is rearranged to enhance the encoder's contribution. Mono: Aligns target-side words monotonously with the source, making the target sentences less fluent and encouraging the encoder's representations to take precedence. In the Replace method, random entries from the training vocabulary are used to replace the target words.  These techniques are implemented during training to improve the training data by improving target sentences, which are then used as auxiliary tasks to strengthen the encoder.  Using each approach, the augmented examples are appended to the original training data, increasing the diversity and size of the dataset for the GEC model to be trained. They used synthetic data from a previous study by____ to augment the data. The training and development sets consisted of 1,500,173 sentence pairs.

Kwon et al.____ discussed the use of synthetic data to improve the performance of AraGEC models. Three different methods of data augmentation are explored by the authors: First, they utilized ChatGPT as a corrupter to introduce grammatical errors into correct Arabic sentences, creating a parallel dataset. Using a taxonomy from the Arabic Learner Corpus, they ensure a variety of error types have been identified. The second type is token noise and error adaptation, which involves introducing random alterations to characters and words, such as adjusting spaces and punctuation. Matching error rates with the benchmark dataset is the goal. The third type is called reverse noise, where they use beam search to convert clean sentences into noisy ones. QALB-2014 and 2015 datasets are used for training one reverse model, and ChatGPT is used to generate a parallel dataset. The authors compared the performance of fine-tuned models on a training dataset with models on artificially created datasets to evaluate the efficacy of these methods. However, when fine-tuning on out-of-domain examples, the performance drops significantly when compared to those tuned on in-domain synthetic data. It emphasizes the importance of high-quality, relevant synthetic data for effective model training. Furthermore, the researchers found that as dataset size increases, precision and recall trade-off, with precision improving and recall declining. To fully optimize ArabGEC systems, data augmentation techniques, and synthetic data quality need to be improved, even though synthetic data generation shows promise.

 The lack of data makes Arabic natural language processing (NLP) challenging. QALB-14 and QALB-15 are the two data sets that make up the majority of ArabGEC (QALB-14, QALB-15). There are relatively few examples in shared data, that consist of punctuation errors, and do not cover all kinds of errors. Moreover,  ArabGEC has limited synthetic data generation capabilities and is relatively unexplored compared to other languages. The quality of augmented data is crucial to the effectiveness of the methodology. Performance could be negatively impacted if unrealistic or irrelevant patterns are introduced during the augmentation process. Only four researches have been conducted on the generation of Arabic synthetic data, as discussed in the preceding paragraphs.  Furthermore, most previous research focused on generating only spelling errors and neglected other types of errors. In addition, some research focused on generating errors at the word level, such as adding or replacing words. Despite this, Arabic is one of the Semitic languages and has a rich morphology, making existing synthetic data sufficient to improve the quality of ArabGEC. Thus, further research is needed to unlock the potential of ArabGEC.
 \subsection{Arabic Grammatical Error Correction as Multi-label Classification Task}
 In Arabic text, the Arabic Error Type Annotation (ARETA) tool____, a system for automatically annotating Modern Standard Arabic, several steps are required: First, align raw input text with reference text corrected versions
Second: automatically determines error types. It involves a four-part process:
\begin{itemize}[noitemsep, topsep=0pt]
\item Punctuation: Determines whether punctuation errors have occurred using regular expressions.
\item Regex: Identifies splits, mergers, deletions, insertions, and other orthographic errors using regular expressions.
\item Ortho-Morph: A morphological analyzer based on CAMeL Tools is used to handle complex orthographic and morphological errors. Possible edits are generated and the shortest paths to convert the raw word into the reference word are determined, which error tags are associated with these edits.
\item Multi-Word: Processes one-to-many word pairs by tokenizing them with Arabic Treebank and assigning errors to the tokens
\end{itemize}
Finally, ARETA can be used to evaluate grammatical error correction systems by comparing predicted error tags with reference error tags. Furthermore, it can diagnose the output of a system directly as well as identify remaining error types.
As shown in Table 1, ARETA encompasses several different errors based on extended ALC error types. ARETA's annotation process uses a combination of linguistic rules, morphological analysis, and alignment techniques to handle Arabic's intricacies, including its rich morphology and ambiguous orthography.

\begin{table}[t]
\caption{Error Type Categories}
\label{tab:error-categories}
\centering
\scalebox{0.9}{
\begin{tabular}{|l|l|l|}
\hline
\textbf{Category} & \textbf{Tag} & \textbf{Error}  \\ \hline
{Orthography} & OA & Alif, Ya \& Alif-Maqsura  \\ 
& OC & Char Order   \\
& OD & Additional Char   \\
& OG & Lengthening short vowels  \\
& OH & Hamza Error  \\
& OM & Missing char(s)  \\
& ON & Nun \& Tanwin Confusion  \\
& OR & Char Replacement  \\
& OS & Shortening long vowels \\
& OT & Ha/Ta/Ta-Marbuta Confusion   \\
& OW & Confusion in Alif Fariqa   \\ \hline
{Morphology} & MI & Word inflection  \\
& MT & Verb tense  \\ \hline
{Syntax} & XC & Case  \\
& XF & Definiteness   \\
& XG & Gender   \\
& XM & Missing word    \\
& XN & Number    \\
& XT & Unnecessary word   \\ \hline
{Semantics} & SF & Conjunction error  \\
& SW & Word selection error   \\ \hline
{Punctuation} & PC & Punctuation confusion  \\
& PM & Missing punctuation   \\
& PT & Unnecessary punctuation   \\ \hline
{Merge} & MG & Merge \\ \hline
{Split} & SP & Split  \\ \hline
\end{tabular}}
\end{table}

%%%يحتاج تحسين كتاب%%%%%%%%%%%%%%%%%%ة
Aloyaynaa et al.____ analyzed Grammatical Error Detection (GED) in Arabic as a low-resource task. A7'ta____ was used along with publicly available datasets such as  QALB-14____, QALB-15____ and SCUT____. Several approaches were employed for GED including token-level classification and sentence-level classification. A token-level classification was performed on individual tokens (words). Furthermore, sentence-level classification is used to determine whether a complete sentence is correct or incorrect. They used pre-trained transformer architectures, specifically mBERT and AraBERT, for GED classification. For the AraBERT and mBERT models, F1 scores of 85\% and 85\% respectively showed high performance metrics: Token-Level Classifications: 87\% for the AraBERT model and 86\% for the mBERT model. In terms of sentence-level classification, the AraBERT model scores 98\% and is 98\% accurate; in terms of the mBERT model, it scores 96\% and is 96\% accurate. Models built on monolingual data performed better than models trained on multilingual data for GED tasks in Arabic, demonstrating the effectiveness of using AraBERT pre-trained models.


While ARETA can be considered a multi-label classification, it requires both correct and incorrect text to align raw input sequences with reference sequences and identify error types. When both correct and incorrect text are present, ARETA is helpful, but it is not a predictive tool when only correct text is present. On another hand, As described in ____, the main classification tasks involve binary classification for sentence-level classification, which categorizes sentences as either correct or incorrect. In contrast, token-level classification can be considered a multi-token classification task, since individual tokens (words) can be classified into correct and incorrect categories. While categorizing sentences as correct or incorrect aligns more closely with binary classification, the overall focus is on identifying which sentences are correct or incorrect. 
 \subsection{BERT Pre-trained Models}
Arabic pre-trained models have been used in various research projects to achieve outstanding results on a wide range of NLP tasks. The Arabic language is supported by some, while others support multiple languages, including Arabic. Table 2 presents an overview of the Arabic and multilingual BERT language model.

AraBERT____ was an early attempt to train a monolingual BERT model with Arabic news extracted from a variety of Arabic news sources. There are approximately 23 gigabytes of text in AraBERTv1 of the model, consisting of 77 million sentences and 2.7 billion tokens.  The newest version (AraBERTv2) is pre-trained with 77 gigabytes of text, 3.5 times more than the previous version. AraBERT consists of 12 transformer blocks, each containing 768 hidden units. As well as 110 million trainable parameters, it has 12 self-attention heads. An additional 12,000 sentences written in different dialects of Arabic were pre-trained on the model to support dialectical Arabic. SalamBERT was the name of this customized version of AraBERT____.

ARBERT____ is a masked language model that is pre-trained and aimed at Modern Standard Arabic (MSA). Several sources were used in the training model, including Wikipedia, news sources, and books. There are 6.2 billion tokens in it and it's about 61 gigabytes in size. ARBERT's structure comprises 12 layers of transformer blocks, 768 hidden units, and 12 self-attention heads. However, ARBERT has 163 million parameters that can be trained.

MARBERT____ was pre-trained on a massive amount of Twitter data that included both MSA and various Arabic dialects. It was also developed specifically for the Arabic language. Almost 128 gigabytes of text comprise the pretraining corpus, which contains 1 billion tweets. The number of tokens in the corpora is approximately 15.6 billion, which is more than double that of Version 2 of AraBERT. The MARBERT model is based on the multilingual BERT architecture, but it does not include the next-sentence prediction feature. The next-sentence prediction was omitted because tweets are too short, according to the model's developers. A total of 160 million parameters can be trained in MARBERT.

CAMeLBERT____ developed three pre-trained language models for Arabic: Modern Standard Arabic (MSA), dialectal Arabic, and classical Arabic, as well as a fourth model that was pre-trained on a mix of these three. The pretraining corpus of CAMeLBERT-MSA comprises almost 107 gigabytes. There are approximately 12.6 billion tokens in the corpora. The size of CAMeLBERT-DA is about 54 gigabytes and it contains 5.8 billion tokens. CAMeLBERT-CA contains 864 million tokens and is about 6 gigabytes in size. CAMeLBERT-Mix contains 17.3 billion tokens and is about 167 gigabytes in size. The CAMeLBERT model is based on the multilingual BERT architecture. A total of five NLP tasks were analyzed across 12 datasets, including sentiment analysis, part-of-speech tagging, dialect identification, named entity recognition (NER), and poetry classification. According to the average results, AraBERTv02 performed best across several subtasks. CAMeLBERT-Mix outperformed best models in dialectal contexts, but not consistently in other cases.

The DeBERTa model____ is an enhancement to the BERT and RoBERTa architectures, incorporating two innovation techniques: enhanced mask decoding and disentangled attention.  The enhanced mask decoder predicts masked tokens using absolute positions during pretraining. Disentangled attention allows for more effective attention weight calculations by representing each word with two vectors-one for content and another for position. Furthermore, the model's generalization on downstream tasks is improved using a new virtual adversarial training method. The DeBERTa model is mainly trained on English text, with 1.5 billion parameters, and has passed the human baseline of 89.816 on the SuperGLUE benchmark, exceeding human performance for the first time.  During the pre-training phase, DeBERTa uses about 78GB of data, which has been duplicated. This dataset contains several sources, including Wikipedia: 12GB, BookCorpus: 6GB, OPENWEBTEXT: 38GB (public Reddit content), and STORIES: 31GB (a subset of CommonCrawl).


In contrast to the traditional masked language modeling (MLM) approach, DeBERTaV3____ uses a more efficient training objective called Replaced Token Detection (RTD) instead of the original DeBERTa. Gradient-disentangled embedding sharing is incorporated into this model to improve training efficiency. There are several variants of DeBERTaV3: DeBERTaV3large, DeBERTaV3base, DeBERTaV3small, and DeBERTaV3xsmall. Models developed using these methods have demonstrated superior performance on various NLU benchmarks, including GLUE, MNLI, and SQuAD v2.0. A multilingual version of DeBERTaV3, mDeBERTaV3base, has also been developed to outperform other multilingual models like XLM-Rbase across a large dataset (CC100). This makes DeBERTaV3 a versatile model suitable for various languages and tasks in natural language processing.

BERT has not been applied to the classification of Arabic text in grammatical error correction tasks, so studies are needed to determine how to classify multi-label Arabic text. To the best of our knowledge, there was no Arabic research or model for predicting types of grammatical errors included in correct Arabic texts at the time of this study. Based on these considerations, we developed a pre-trained model that can predict multi-label classifications for Arabic text grammatical errors.
%%%ينقص بعض الريفرينس

\begin{table*}[t]
\centering
\setlength{\tabcolsep}{5pt} % Adjust column spacing
\renewcommand{\arraystretch}{1.5} % Adjust row height
\caption{An overview of the Arabic and multilingual BERT pre-train language models.}
\label{tab:models}
\resizebox{\textwidth}{!}{ % Resizing to fit the table within the page width
\begin{tabular}{|m{2.5cm}|m{2cm}|m{2cm}|m{2.2cm}|m{1.5cm}|m{1.5cm}|m{1.5cm}|m{1.5cm}|m{5cm}|m{3cm}|}
\hline
\textbf{Model} & \textbf{Architecture} & \textbf{Parameters} & \textbf{Language} & \textbf{Size} & \textbf{\#Word} & \textbf{Token} & \textbf{Vocab} & \textbf{Pre-train Data} & \textbf{Tasks} \\ 
\hline
AraBERTv01 & BERT-base & 110 M & Arabic (MSA) & 24GB & - & SP & 60K & (1) The 1.5 billion words Arabic Corpus____. (2) OSIAN Arabic News Corpus____. & Sentiment Analysis (SA), Named Entity Recognition (NER), Question Answering (QA) \\ 
\hline
AraBERTv02 & BERT-base & 135 M & Arabic (MSA) & 77GB & 8.6B & WP & 60K & (1) Arabic Corpus____. (2) OSIAN Corpus____. (3) OSCAR____. (4) Wikipedia \footnote{\url{https://archive.org/details/arwiki-20190201}}. (5) Assafir news. & - \\ 
\hline
CAMeLBERT-MSA & BERT-base & 110 M & Arabic (MSA) & 107GB & 12.6B & WP & 30K & Arabic Gigaword \footnote{\url{https://catalog.ldc.upenn.edu/LDC2011T11}}, El-Khair Corpus____, OSIAN Corpus____, Arabic Wikipedia \footnote{\url{https://archive.org/details/arwiki-20190201}}. & NER, POS tagging, Sentiment Analysis, Dialect Identification, Poetry Classification \\ 
\hline
CAMeLBERT-DA & BERT-base & 110 M & Arabic (DA) & 54GB & 5.8B & WP & 30K & Range of dialectal corpora____. & - \\ 
\hline
CAMeLBERT-CA & BERT-base & 110 M & Arabic (CA) & 6GB & 847M & WP & 30K & OpenITI Corpus (v1.2) \footnote{\url{https://github.com/OpenITI/RELEASE}}. & - \\ 
\hline
CAMeLBERT-Mix & BERT-base & 110 M & Arabic (MSA/DA/CA) & 167GB & 17.3B & WP & 30K & Combination of previous CAMeLBERT data. & - \\ 
\hline
ARBERT & BERT-base & 163 M & Arabic (MSA) & 61GB & 5.6B & WP & 100K & (1) 1,800 Arabic books (Hindawi) \footnote{\url{https://www.hindawi.org/books/}}. (2) El-Khair____. (3) Gigaword \footnote{\url{https://catalog.ldc.upenn.edu/LDC2011T11}}. (4) OSCAR____. (5) OSIAN____. (6) Wikipedia Arabic \footnote{\url{https://github.com/attardi/wikiextractor}}. & SA, SM, TC, DI, NER, QA \\ 
\hline
MARBERT & BERT-base & 163 M & Arabic (MSA + DA) & 128GB & 15.6B & WP & 100K & 1B Arabic tweets from an in-house dataset____. & - \\ 
\hline
DeBERTa & DeBERTa-base & 1.5 B & Multilanguage & 78GB & - & WP & 128K & Wikipedia \footnote{\url{https://dumps.wikimedia.org/enwiki/}}, BookCorpus____, OPENWEBTEXT____, STORIES____. & Acceptability, SA, NLI, Paraphrase Detection, QA \\ 
\hline
\end{tabular}
}
\end{table*}








\subsection{Text-to-Text Transformer}

A state-of-the-art model for natural language processing (NLP) tasks, Text-to-Text Transfer Transformer (T5)____ can be applied to a variety of NLP tasks by treating them as text-to-text tasks. In addition to classifying, summarizing, and translating, the model uses a unified architecture and loss function. T5 utilizes a transformer architecture that uses encoder-decoder structures to produce coherent and fluent results. This architecture is composed of two parts: The encoder generates a contextual representation of the input sequence. In producing each entry of the output, the self-attention mechanism uses a fully visible attention mask to attend to any entry of the input. The decoder generates the output sequence from the encoder's output. This technique uses both causal masking and encoder-decoder attention to maintain autoregressive properties while at the same time attending to the encoder's output. Multiple benchmarks have been run on the model, and it performs better when scaled to various sizes, with the largest variant, T5-11B, containing 11 billion parameters. A powerful dataset known as the Colossal Clean Crawled Corpus (C4) was used to pre-train the model, making it able to learn from a wide range of texts. Although T5 performed well on several tasks, including CNN/Daily Mail summarization benchmarks, it performed poorly on some translation tasks, perhaps due to its reliance on an English-only dataset.

mT5____ is an enhancement of T5 that utilizes GeGLU nonlinearities and scales both feed-forward dimensions and model dimensions in larger models. The encoder-decoder model supports generative tasks such as abstractive summarization and dialog, which is different from encoder-only models such as XLM-R. As a result of training more than 250,000 words, the mT5 model has several sizes, such as Small (300M parameters), Base (580M), Large (1.2B), XL (3.7B), and XXL (13B). As part of the pre-training of the mT5 model, a large dataset named mC4, which contains over 100 different languages of text, is used. In the training process, data is sampled from each language to balance the representation of languages with high resources and languages with low resources.  Several classification and question-answering tasks have been performed using the mT5 model, and it has proven to be state-of-the-art. Various training strategies are employed, including in-language multitask training ( utilizing gold data within the target language), translate-train ( utilizing machine translation from English ), and zero-shot transfer ( utilizing only English data for fine-tuning). Cross-lingual representation learning is influenced by model capacity, with larger models outperforming smaller ones, particularly in zero-shot scenarios.

In ByT5____, vocabulary does not need to be built or tokenized like in mT5 and the NLP pipeline is simplified. The ByT5 architecture supports byte-level processing, so vocabulary parameters are reallocated to the transformer layers, improving model efficiency. ByT5 models come in different sizes (300M, 582M, 1.23B, 3.74B, and 12.9B), all with varying hidden sizes and feed-forward dimensions. A multilingual task can be effectively handled by both ByT5 and mT5. In multitasking settings where gold training data is available, ByT5 has shown competitive performance across a variety of languages. Multiple benchmarks demonstrate its ability to manage tasks in multiple languages, surpassing mT5. ByT5 models range in parameter count from 300 million in the Small version up to 12.9 billion in the XXL version. A comparison of mT5 models reveals that the Base model has 582 million parameters, while the Large model has 1.23 billion. The ByT5 design improves performance and reduces system complexity, making it a viable alternative to token-based models like mT5, especially for applications without significant latency issues.

AraT5____ utilizes the T5 (Text-to-Text Transfer Transformer) encoder-decoder architecture, in particular the T5Base encoder-decoder. An AraT5MSA variant was developed (trained on Modern Standard Arabic data), an AraT5TW variant (trained on Twitter data), and an AraT5 variant (trained on both MSA and Twitter data). It is estimated that there are approximately 220 million parameters in each model, which is composed of 12 layers, 12 attention heads, and 768 hidden units. Various datasets were used to pre-train the models, including unlabeled MSA and Twitter data. A self-supervised (denoising) objective was used to train the model, with 15\% of tokens randomly masked to reassemble the original sequence. Besides the code-switched datasets, the data also includes monolingual French and English translations of Algerian and Jordanian Twitter. An Arabic language GENeration (ARGEN) benchmark was used to evaluate the models, which included seven tasks: machine translation, code-switched translation, text summarization, news headline generation, question generation, paraphrasing, and transliteration. Over 52 out of 59 test sets, the models outperformed the mT5 model with an 88.14\% performance rate. On the Arabic language understanding benchmark ARLUE, they also set new state-of-the-art (SOTA) results.