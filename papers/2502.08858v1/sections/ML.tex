\section{Machine Learning Prediction}\label{sec:ml}
To evaluate the feasibility of machine learning in predicting the bounds of the PNS, we employed five distinct machine learning models to assess their effectiveness in this task: Support Vector Machine (SVM) \citep{svm}, Random Forest (RF) \citep{rf}, Gradient Boosting Decision Trees (GBDT) \citep{gbdt}, Transformer \citep{vaswani2017attention}, and Multilayer Perceptron (MLP) \citep{mlp}. These models were chosen to represent a diverse range of machine learning paradigms, including kernel-based methods (SVM), ensemble learning techniques (RF and GBDT), and deep learning approaches (MLP and Transformer). This selection ensures a comprehensive evaluation of their ability to approximate causal quantities across different settings. A detailed pipeline is illustrated in Figure \ref{fig:flow chart}.

\begin{figure*}[!htbp] 
    \centering
    \includegraphics[width=\textwidth]{imgs/flow_chart.pdf}
    \caption{Framework for Causal Data Generation and Machine Learning Prediction.}
    \label{fig:flow chart}
\end{figure*}

\begin{figure*}[!htb]
    \centering
    % 第一行：SVM, RF, GBDT, Transformer Lower Bound
    \begin{subfigure}[b]{0.24\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/svm_true_vs_pred_lb.pdf}
        \caption{SVM (Lower bound)}
        \label{fig:svm2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/rf_true_vs_pred_lb.pdf}
        \caption{RF (Lower bound)}
        \label{fig:rf2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/gbdt_true_vs_pred_lb.pdf}
        \caption{GBDT (Lower bound)}
        \label{fig:gbdt2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/transformer_true_vs_pred_lb.pdf}
        \caption{Transformer (Lower bound)}
        \label{fig:transformer2}
    \end{subfigure}

    \vspace{0.3cm} % 调整上下间距

    % 第二行：SVM, RF, GBDT, Transformer Upper Bound
    \begin{subfigure}[b]{0.24\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/svm_true_vs_pred_ub.pdf}
        \caption{SVM (Upper bound)}
        \label{fig:svm3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/rf_true_vs_pred_ub.pdf}
        \caption{RF (Upper bound)}
        \label{fig:rf3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/gbdt_true_vs_pred_ub.pdf}
        \caption{GBDT (Upper bound)}
        \label{fig:gbdt3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/transformer_true_vs_pred_ub.pdf}
        \caption{Transformer (Upper bound)}
        \label{fig:transformer3}
    \end{subfigure}

    \caption{Comparison of true and predicted values across different models for both lower and upper bounds.}
    \label{fig:combined_model_comparison}
\end{figure*}
\subsection{Support Vector Machine}
Support Vector Machines (SVM) \citep{svm} are widely used and well-established supervised learning models. Given their strengths, we selected Support Vector Regression (SVR), a variant of SVM, as the first model for our experiments. To effectively capture complex patterns, we employed the Radial Basis Function (RBF) kernel to map the data into a high-dimensional feature space.

Key hyperparameters include the penalty parameter (\( C \)), the insensitive loss threshold (\( \epsilon \)), and the kernel coefficient (\( \gamma \)). The parameter \( C \) controls the trade-off between model complexity and error tolerance, where larger values may lead to overfitting. The threshold \( \epsilon \) defines the margin of tolerance for errors, while \( \gamma \) determines the influence range of individual data points.

A two-stage hyperparameter tuning strategy was adopted. First, Randomized Search \citep{randomsearch} was employed to efficiently explore the parameter space and identify promising ranges. Then, Grid Search \citep{randomsearch} was used to fine-tune parameters within these ranges. Cross-validation ensured robust generalization throughout the tuning process.

\begin{figure*}[!htb]
    \centering
    % 第一行：SVM, RF, GBDT Lower Bounds
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/confusion_matrix_svm_lb.pdf}
        \caption{SVM Lower bound.}
        \label{fig:svm_lb} % 放在 caption 后
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/confusion_matrix_rf_lb.pdf}
        \caption{RF Lower bound.}
        \label{fig:rf_lb}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/confusion_matrix_gbdt_lb.pdf}
        \caption{GBDT Lower bound.}
        \label{fig:gbdt_lb}
    \end{subfigure}

    % 第二行：SVM, RF, GBDT Upper Bounds
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/confusion_matrix_svm_ub.pdf}
        \caption{SVM Upper bound.}
        \label{fig:svm_ub}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/confusion_matrix_rf_ub.pdf}
        \caption{RF Upper bound.}
        \label{fig:rf_ub}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/confusion_matrix_gbdt_ub.pdf}
        \caption{GBDT Upper bound.}
        \label{fig:gbdt_ub}
    \end{subfigure}

    % 第三行：Transformer
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/confusion_matrix_transformer_lb.pdf}
        \caption{Transformer Lower bound.}
        \label{fig:transformer_lb}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/confusion_matrix_transformer_ub.pdf}
        \caption{Transformer Upper bound.}
        \label{fig:transformer_ub}
    \end{subfigure}

    \caption{Confusion matrices of SVM, RF, GBDT, and Transformer models.}
    \label{fig:confusion_matrices_combined}
\end{figure*}

% \begin{table}[!h]
%     \centering
%     \caption{SVM Result} \label{tab:svm}
%     \begin{tabular}{rll}
%         \toprule % from booktabs package
%         \bfseries Dataset & \bfseries  MSE & \bfseries MAE\\
%         \midrule % from booktabs package
%         Lower bound & 0.0112 & 0.0868\\
%         Upper bound & 0.0304 & 0.1527\\
%         \bottomrule % from booktabs package
%     \end{tabular}
% \end{table}
Finally, the mean squared error (MSE) and mean absolute error (MAE) values of the SVR model can be found in Table \ref{tab:comparison}. Confusion matrices are presented in Figures \ref{fig:svm_lb} and \ref{fig:svm_ub}, while Figures \ref{fig:svm2} and \ref{fig:svm3} provide a clearer comparison with the true PNS bounds. For the prediction of the lower bound, SVR demonstrates reasonable effectiveness; however, for the more complex upper bound, it exhibits a significant decline in accuracy.

\subsection{Random Forest}
Random Forests (RF) \citep{rf} are a widely used ensemble learning method for classification, regression, and other predictive tasks. The core idea behind RF is to construct multiple decision trees during training and aggregate their outputs to enhance overall performance. As an ensemble model, RF exhibits strong robustness, motivating us to assess its effectiveness in predicting PNS bounds.

Key hyperparameters of RF include the number of trees (\( n_{\text{estimators}} \)), maximum tree depth (\( \text{max\_depth} \)), minimum samples required to split a node (\( \text{min\_samples\_split} \)), and the number of features considered for splitting (\( \text{max\_features} \)). Increasing \( n_{\text{estimators}} \) generally improves performance but at the expense of higher computational costs. The parameters \( \text{max\_depth} \), \( \text{min\_samples\_split} \), and \( \text{max\_features} \) regulate tree complexity, balancing bias-variance trade-offs.

For hyperparameter optimization, we employed a two-stage tuning strategy similar to that used for SVM. Table \ref{tab:comparison} also presents RF's MAE and MSE results, while Figures \ref{fig:rf_lb} and \ref{fig:rf_ub} show its confusion matrices. A more direct comparison with true PNS bounds is provided in Figures \ref{fig:rf2} and \ref{fig:rf3}. RF performs comparably to SVM on the lower bound but exhibits significantly higher accuracy on the upper bound.
% \begin{table}[!h]
%     \centering
%     \caption{RF Result} \label{tab:rf}
%     \begin{tabular}{rll}
%         \toprule % from booktabs package
%         \bfseries Dataset & \bfseries  MSE & \bfseries MAE\\
%         \midrule % from booktabs package
%         Lower bound & 0.0116 & 0.0919\\
%         Upper bound & 0.0205 & 0.1242\\
%         \bottomrule % from booktabs package
%     \end{tabular}
% \end{table}
\subsection{Gradient Boosting Decision Trees}
Gradient Boosting Decision Trees (GBDT) \citep{gbdt} is an ensemble learning method that builds models sequentially, with each new tree correcting the errors of its predecessors. Unlike traditional boosting, GBDT optimizes pseudo-residuals, enabling flexible loss function optimization. Simple decision trees serve as weak learners, allowing GBDT to effectively capture complex data patterns.

Key hyperparameters include the number of trees (\( n_{\text{estimators}} \)), learning rate (\( \text{learning\_rate} \)), maximum tree depth (\( \text{max\_depth} \)), and subsample ratio (\( \text{subsample} \)). The learning rate determines each tree’s contribution, while \( n_{\text{estimators}} \) and \( \text{max\_depth} \) regulate model complexity and performance.

Following the approach used for SVM and RF, we applied a two-stage tuning strategy. Again, table \ref{tab:comparison} presents the MSE and MAE results, while Figures \ref{fig:gbdt_lb} and \ref{fig:gbdt_ub} show the confusion matrices. A more direct comparison with true PNS bounds is provided in Figures \ref{fig:gbdt2} and \ref{fig:gbdt3}. GBDT demonstrates moderate performance on both the lower and upper bounds.
% \begin{table}[!h]
%     \centering
%     \caption{GBDT Result} \label{tab:gbdt}
%     \begin{tabular}{rll}
%         \toprule
%         \bfseries Dataset & \bfseries MSE & \bfseries MAE \\
%         \midrule
%         Lower bound & 0.0159 & 0.1049 \\
%         Upper bound & 0.0261 & 0.1399 \\
%         \bottomrule
%     \end{tabular}
% \end{table}
\subsection{Transformer}
The Transformer \citep{vaswani2017attention}, originally developed for Natural Language Processing, has expanded into Computer Vision and become a cornerstone of deep learning, particularly with the rise of large language models. Given its significant impact, this study also evaluates the Transformer for testing.

The model architecture begins with an input layer processing 15-dimensional feature vectors, followed by a linear embedding layer that projects inputs into a 64-dimensional space. Positional encoding is applied to retain feature order information, and two Transformer encoder layers with four attention heads each capture complex feature interactions. The final output is generated through a fully connected layer with a Sigmoid activation function, ensuring predictions remain within the range \([0, 1]\). Key hyperparameters include an embedding dimension of 64, four attention heads, two encoder layers, and a dropout rate of 0.1.

Similarly, table \ref{tab:comparison} presents the MSE and MAE results, while Figures \ref{fig:transformer_lb} and \ref{fig:transformer_ub} show the confusion matrices. A direct comparison with true PNS bounds is provided in Figures \ref{fig:transformer2} and \ref{fig:transformer3}. The Transformer demonstrates strong performance on the lower bound and moderate performance on the upper bound.
% \begin{table}[!h]
%     \centering
%     \caption{Transformer Result} \label{tab:transformer}
%     \begin{tabular}{rll}
%         \toprule
%         \bfseries Dataset & \bfseries MSE & \bfseries MAE \\
%         \midrule
%         Lower bound & 0.0030 & 0.0348 \\
%         Upper bound & 0.0156 & 0.1060 \\
%         \bottomrule
%     \end{tabular}
% \end{table}
\subsection{Multilayer Perceptron}
MLP \citep{mlp} consists of an input layer, one or more hidden layers, and an output layer. With appropriate activation functions, it can effectively model both linear and nonlinear relationships. As a fundamental structure in deep learning, MLP holds significant representativeness, motivating its inclusion in our experiments.

A key consideration for MLP is the choice of activation function, particularly for predicting the lower bound. Since the lower bound of PNS cannot be negative, we initially selected the ReLU \citep{relu} activation function (\ref{equ:relu}). However, ReLU can lead to the loss of negatively correlated features, prompting us to adopt LeakyReLU \citep{lkrelu} (\ref{equ:lkrelu}) as a complementary solution. Furthermore, given the considerable number of zero values in the data, the non-differentiability of ReLU and LeakyReLU at \(s = 0\) imposes limitations on backpropagation. To address this, we proposed using Mish \citep{mish} (\ref{equ:mish}) as an alternative activation function. The corresponding equations are:

\begin{equation}\label{equ:relu}
    \text{ReLU}(s) = \max(0, s)
\end{equation}

\begin{equation}\label{equ:lkrelu}
    \text{LeakyReLU}(s) = 
    \begin{cases}
        s, & \text{if } s \ge 0 \\
        \alpha s, & \text{if } s < 0
    \end{cases}
\end{equation}

\begin{equation}\label{equ:mish}
    \text{Mish}(s) = s \cdot \tanh(\ln(1 + e^s))
\end{equation}

Additionally, we implemented an MLP with the architecture \( 15 \rightarrow 64 \rightarrow 32 \rightarrow 16 \rightarrow 1 \), utilizing ReLU-like functions and Sigmoid as activation functions. The model was optimized using the Adam optimizer with a learning rate of $0.01$ and trained for $1000$ epochs. 

Again, the final results are presented in Table \ref{tab:comparison}. With the Mish activation function, the MLP achieved an MSE of \textbf{0.0011} on the lower bound and \textbf{0.0010} on the upper bound. For MAE, it attained \textbf{0.0225} on the lower bound and \textbf{0.0247} on the upper bound. The confusion matrix is shown in Figure \ref{fig:mlp_comparison}, and a clearer comparison with the true PNS bounds is provided in Figure \ref{fig:mlp2} (Only the best performance comparisons with Mish are shown). 

Overall, MLP significantly outperformed other machine learning models, with Mish yielding the best results among the activation functions. The comparison with the true PNS bounds further confirms that MLP (Mish) provides an accurate and practical model for predicting PNS.
% \begin{table}[!h]
%     \centering
%     \caption{MLP Result} \label{tab:mlp}
%     \begin{tabular}{rlll}
%         \toprule
%         \bfseries Dataset & \bfseries Activation Function & \bfseries MSE & \bfseries MAE \\
%         \midrule
%         Lower bound &ReLU & 0.0045 & 0.0434 \\
%         Upper bound &ReLU & 0.0023 & 0.0357 \\
%         Lower bound &LeakyReLU & 0.0038 & 0.0379 \\
%         Upper bound &LeakyReLU & 0.0024 & 0.0380 \\
%         Lower bound &Mish & \textbf{0.0011} & \textbf{0.0225} \\
%         Upper bound &Mish & \textbf{0.0010} & \textbf{0.0247} \\
%         \bottomrule
%     \end{tabular}
% \end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!htb]  % 使用 figure* 跨越两列
    \centering
    % 第一排：ReLU, Leaky ReLU, Mish 的 Lower Bound
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/confusion_matrix_nn_relu_lb.pdf}
        \caption{ReLU (Lower bound)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/confusion_matrix_nn_lkrelu_lb.pdf}
        \caption{Leaky ReLU (Lower bound)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/confusion_matrix_nn_mish_lb.pdf}
        \caption{Mish (Lower bound)}
    \end{subfigure}

    \vspace{0.3cm} % 调整上下间距

    % 第二排：ReLU, Leaky ReLU, Mish 的 Upper Bound
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/confusion_matrix_nn_relu_ub.pdf}
        \caption{ReLU (Upper bound)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/confusion_matrix_nn_lkrelu_ub.pdf}
        \caption{Leaky ReLU (Upper bound)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/confusion_matrix_nn_mish_ub.pdf}
        \caption{Mish (Upper bound)}
    \end{subfigure}

    \caption{Confusion matrices of MLP with different activation functions: ReLU, Leaky ReLU, and Mish for both lower and upper bounds.}
    \label{fig:mlp_comparison}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{figure}[!htb]
    \centering
    % 左侧：Lower bound
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/mlp_true_vs_pred_lb.pdf}
        \caption{Lower bound.}
        \label{fig:mlp_lb}
    \end{subfigure}
    \hfill
    % 右侧：Upper bound
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/mlp_true_vs_pred_ub.pdf}
        \caption{Upper bound.}
        \label{fig:mlp_ub}
    \end{subfigure}
    
    \caption{Comparison of MLP (Mish) for lower and upper bounds.}
    \label{fig:mlp2}
\end{figure}





\subsection{Experimental Comparison}
% As shown in Table \ref{tab:comparison}, the overall performance of MLP is undoubtedly the best. Among the other four machine learning models, SVM achieves good results on the lower bound but performs the worst on the upper bound, with performance close to complete failure. RF performs significantly better, achieving acceptable results on both the lower and upper bounds. In contrast, GBDT performs worse than RF, despite also being a tree-based model, showing inferior results across the board with only a slight improvement over SVM on the upper bound. The Transformer, as a model based on MLP, performs better than the other machine learning models but still falls short of MLP's performance.

% For MLP models, due to the special characteristics of the dataset around zero, different activation functions exhibit significant performance differences. The basic ReLU exhibits suboptimal performance on the lower bound. Considering negative values, LeakyReLU shows slightly better performance than ReLU on the lower bound. Mish, which not only accounts for negative values but also ensures differentiability around zero, achieves the best results.
As shown in Table \ref{tab:comparison}, MLP delivers the best overall performance. Among the other four machine learning models, SVM performs well on the lower bound but fails almost entirely on the upper bound. RF shows significantly better results, achieving acceptable performance on both bounds. Despite also being a tree-based model, GBDT underperforms compared to RF, with only a slight improvement over SVM on the upper bound. The Transformer, as an MLP-based model, outperforms the other machine learning models but still falls short of MLP’s performance.  

For MLP models, the dataset’s characteristics around zero (we will discuss these characteristics in the discussion section) lead to notable differences in activation function performance. Basic ReLU shows suboptimal performance on the lower bound, while LeakyReLU, which accounts for negative values, performs slightly better. Mish, which not only handles negative values but also ensures differentiability around zero, achieves the best results.

\begin{table}[!ht]
    \centering
-    \caption{Comparison of Model Performance} 
    \label{tab:comparison}
    \begin{tabular}{rlll}
        \toprule
        \bfseries Model & \bfseries Dataset & \bfseries MSE & \bfseries MAE \\
        \midrule
        SVM  & Lower bound & 0.0112 & 0.0868 \\
             & Upper bound & 0.0304 & 0.1527 \\
        RF   & Lower bound & 0.0116 & 0.0919 \\
             & Upper bound & 0.0205 & 0.1242 \\
        GBDT & Lower bound & 0.0159 & 0.1049 \\
             & Upper bound & 0.0261 & 0.1399 \\
        Transformer & Lower bound & 0.0030 & 0.0348 \\
             & Upper bound & 0.0156 & 0.1060 \\
        MLP(ReLU)  & Lower bound & 0.0045 & 0.0434 \\
             & Upper bound & 0.0023 & 0.0357 \\
        MLP(LeakyReLU)  & Lower bound & 0.0038 & 0.0379 \\
             & Upper bound & 0.0024 & 0.0380 \\
        MLP(Mish)  & Lower bound & \textbf{0.0011} & \textbf{0.0225} \\
             & Upper bound & \textbf{0.0010} & \textbf{0.0247} \\
             
        \bottomrule
    \end{tabular}
\end{table}