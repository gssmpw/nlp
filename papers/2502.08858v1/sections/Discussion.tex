\section{Discussion}
Our study demonstrates that among common machine learning methods, the MLP (Mish) is the most effective and accurate in estimating the bounds of PNS. Below, we discuss key considerations and future directions.  

First, in complex settings—especially those involving non-binary probabilities of causation, as indicated in \citep{li2024probabilities,zhang2024causal}—the lower bounds of PNS often approach zero, further emphasizing the potential of MLP (Mish) in such scenarios.  

Second, while our model successfully addresses the challenge of predicting from approximately 2,000 subpopulations to \(2^{15}\) subpopulations, a significant issue remains: the 2,000 reliable training samples may require up to 50 million data points at the population level. This is due to data sparsity and the minimum data requirements needed to estimate PNS bounds for specific subpopulations (this paper uses 1,300 observational and experimental data points). However, collecting such a large volume of data is often impractical. Fortunately, we propose three practical approaches to data collection: instead of gathering population-level data, we can directly collect data for the 2,000 predefined subpopulations; the required 1,300 data points can be reduced to approximately 400, as suggested by \cite{li2022probabilities}, while maintaining basic accuracy; and we can rely solely on experimental data, as proposed by \cite{li2023probabilities}, or use only observational data and estimate experimental data through adjustment formulas \citep{pearl1993aspects, pearl1995causal}.  

A fundamental goal in causal inference is identifying subpopulations exhibiting desirable counterfactual behavior patterns \citep{li2019unit} to effectively guide policy-making and decision implementation. Specifically, this involves finding subpopulations with sufficiently small PNS upper bounds or sufficiently large PNS lower bounds. Therefore, accurately predicting all subpopulations is unnecessary for this task. A promising research direction is determining a minimal training set that can reliably predict these key subpopulations.  

Next, the current data-generating process consists of a relatively simple causal structure with only 20 confounders, making MLP the optimal choice. However, if the underlying causal structure were more complex or included additional confounding variables, would more sophisticated models be necessary? Since this paper focuses solely on illustrating the feasibility and potential of machine learning models for predicting causal quantities, a deeper exploration of the relationship between model complexity and causal structure remains an important direction for future research.

Finally, traditional methods for estimating probabilities of causation, such as PNS, PS, and PN, rely on direct computation using observational and experimental data within SCMs. However, they assume sufficient data for each subgroup, which is often unrealistic. Our work addresses this challenge by developing a machine learning-based framework to predict PNS for subpopulations with insufficient data, a gap not covered by traditional causal inference techniques.  

Although there are no classical methods explicitly designed for this task, we establish a robust benchmark for evaluation by leveraging the known data-generating process in our synthetic experiments. For subgroups with insufficient data, we compare our model predictions to the true PNS bounds derived from SCM equations, ensuring that our results are grounded in a well-defined standard. This explicit benchmarking demonstrates the validity of our approach and allows us to assess model accuracy relative to the traditional PNS estimation under ideal conditions.  
% Our results suggests that machine learning can serve as a powerful tool for causal inference in real-world scenarios where direct estimation using SCM formulas is not feasible due to data limitations.

% Finally, the traditional approach to estimating probabilities of causation, including PNS, PS, and PN, relies on directly computing these values using observational and experimental data within Structural Causal Models (SCMs). However, this approach assumes the availability of sufficient data for each subgroup, which is often unrealistic in practice. Our work specifically addresses this challenge by developing a machine learning-based framework to predict PNS for subpopulations with insufficient data, a problem not handled by traditional causal inference techniques.

% Although there are no classical methods explicitly designed for this task, we establish a robust benchmark for evaluation by leveraging the known data-generating process in our synthetic experiments. For subgroups with insufficient data, we compare our model predictions to the true PNS bounds derived from SCM equations, ensuring that our results are grounded in a well-defined standard. This explicit benchmarking demonstrates the validity of our approach and allows us to assess model accuracy relative to the traditional PNS estimation under ideal conditions.

% Our results show that machine learning models, particularly MLP with the Mish activation function, can effectively predict these causal bounds, even when data is scarce. This suggests that machine learning can serve as a powerful tool for causal inference in real-world scenarios where direct estimation using SCM formulas is not feasible due to data limitations.


% We anticipate that the release of our dataset will facilitate the development of Structural Causal Models (SCMs) for causality prediction using machine learning. The current dataset is based on Bernoulli distributions, and future work should consider expanding the dataset to include a wider variety of probability distributions, enabling more comprehensive testing scenarios. However, it is important to note, as highlighted by the performance of Mish near zero, that users must carefully handle values close to zero, as higher-dimensional data tends to gravitate towards boundary regions.
% graph methods
%MLP效果很好，已经达到了可以实际应用的程度了。
%然而我们2000预测32768其实是要5000w数据（1300作门槛）
%如何用更少的data sample去estimate distribution （也就是用远小于5000w来得到2000；甚至不需要2000，也就是说如果不需要预测32768，能否能知道需要哪些feature来达到不错的置信度）
%终极目的之一是找到上界min，下界max的个体，这样可以断言他们。对于这个task，我们可能不需要这么多的数据。
%我们的模型相对简单，节点只有xyz，只有20个confounder，所以用的模型相对简单。如果图模型更复杂，或者有更多的confounder，是否可能需要更复杂的ml模型 in general。
%%%%%%%%特殊的图结构，会不会有更好的效果

%如果能做到200门槛，会降低准确率，但是数据的收集会好很多
%复读一遍为什么MLP mish最好，以及关注0附近的问题（尤其是当x和y趋于高维，PNS更容易是0）

%dataset会publish，会产生更多的scm model去test这些ml task，希望我们的paper是ml预测Causation的一个start。 
%我们的数据都是伯努利分布，可能需要更多的分布形式来扩展test dataset


%contribution 
%提出了一整个从模拟数据生成到ML预测PNS的piepline
%提出了一个行之有效的模型可以很好的预测PNS bound，第一个验证了可行性
%提出了一个模拟实验数据集