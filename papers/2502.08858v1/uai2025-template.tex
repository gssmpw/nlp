% \documentclass{uai2025} % for initial submission
\documentclass[accepted]{uai2025} % after acceptance, for a revised version; 
% also before submission to see how the non-anonymous paper would look like 
                        
%% There is a class option to choose the math font
% \documentclass[mathfont=ptmx]{uai2025} % ptmx math instead of Computer
                                         % Modern (has noticeable issues)
% \documentclass[mathfont=newtx]{uai2025} % newtx fonts (improves upon
                                          % ptmx; less tested, no support)
% NOTE: Only keep *one* line above as appropriate, as it will be replaced
%       automatically for papers to be published. Do not make any other
%       change above this note for an accepted version.

%% Choose your variant of English; be consistent
\usepackage[american]{babel}
% \usepackage[british]{babel}

%% Some suggested packages, as needed:
\usepackage{natbib} % has a nice set of citation styles and commands
    \bibliographystyle{plainnat}
    \renewcommand{\bibsection}{\subsubsection*{References}}
\usepackage{mathtools} % amsmath with fixes and additions
% \usepackage{siunitx} % for proper typesetting of numbers and units
\usepackage{booktabs} % commands to create good-looking tables
\usepackage{tikz} % nice language for creating drawings and diagrams



\usepackage{tikz-cd}
\usetikzlibrary{arrows}
\usepackage{comment}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{tikz-qtree,tikz-qtree-compat}
\usetikzlibrary{positioning}
\usepackage{float}


\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{tikz}
\usepackage{tikz-qtree,tikz-qtree-compat}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand{\notindep}{\not\!\independent}

\usepackage{stackengine}
\def\delequal{\mathrel{\ensurestackMath{\stackon[1pt]{=}{\scriptstyle\Delta}}}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newtheorem{theorem}{Theorem}
% \newtheorem{definition}[theorem]{Definition}
% \newtheorem{lemma}[theorem]{Lemma}
% \newtheorem{conjecture}[theorem]{Conjecture}
% \newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}



\usepackage{subcaption}

%% Provided macros
% \smaller: Because the class footnote size is essentially LaTeX's \small,
%           redefining \footnotesize, we provide the original \footnotesize
%           using this macro.
%           (Use only sparingly, e.g., in drawings, as it is quite small.)

%% Self-defined macros
\newcommand{\swap}[3][-]{#3#1#2} % just an example

\title{Estimating Probabilities of Causation with Machine Learning Models}

% The standard author block has changed for UAI 2025 to provide
% more space for long author lists and allow for complex affiliations
%
% All author information is authomatically removed by the class for the
% anonymous submission version of your paper, so you can already add your
% information below.
%
% Add authors
\author[1]{\href{mailto:<sw23v@fsu.edu>?Subject=Your UAI 2025 paper}{Shuai Wang}}
\author[1]{\href{mailto:<angli@cs.fsu.edu>?Subject=Your UAI 2025 paper}Ang Li}
% Add affiliations after the authors
\affil[1]{%
    Dept. of Computer Science\par
    Florida State University\par
    Tallahassee, FL, USA
}

  
  \begin{document}
\maketitle
\thispagestyle{empty} % 避免过多的空白
\begin{abstract}
% \sloppy
Probabilities of causation play a crucial role in modern decision-making. This paper addresses the challenge of predicting probabilities of causation for subpopulations with insufficient data using machine learning models. Tian and Pearl first defined and derived tight bounds for three fundamental probabilities of causation: the probability of necessity and sufficiency (PNS), the probability of sufficiency (PS), and the probability of necessity (PN). However, estimating these probabilities requires both experimental and observational distributions specific to each subpopulation, which are often unavailable or impractical to obtain with limited population-level data. We assume that the probabilities of causation for each subpopulation are determined by its characteristics. To estimate these probabilities for subpopulations with insufficient data, we propose using machine learning models that draw insights from subpopulations with sufficient data. Our evaluation of multiple machine learning models indicates that, given sufficient population-level data and an appropriate choice of machine learning model and activation function, PNS can be effectively predicted. Through simulation studies, we show that our multilayer perceptron (MLP) model with the Mish activation function achieves a mean absolute error (MAE) of approximately 0.02 in predicting PNS for 32,768 subpopulations using data from around 2,000 subpopulations.
\end{abstract}


\section{Introduction}\label{sec:intro}
Understanding causal relationships and estimating probabilities of causation are crucial in fields such as healthcare, policy evaluation, and economics \citep{pearl2009causality, imbens2015causal, heckman2015causal}. Unlike correlation-based methods, causal inference enables decision-makers to determine whether an action or intervention directly leads to a desired outcome. This is particularly essential in personalized medicine, where accurately assessing treatment effects ensures both efficacy and safety \citep{mueller:pea23-r530}. Moreover, causal reasoning enhances machine learning applications by improving accuracy \citep{li2020training}, interpretability, and fairness \citep{plecko2022causal} in automated decision-making. Despite its broad significance, estimating probabilities of causation remains challenging due to data limitations. In this paper, we address this challenge by leveraging machine learning techniques to predict probabilities of causation for subpopulations with insufficient data.

The study of probabilities of causation began around 2000 when \cite{pearl1999probabilities} first defined three fundamental probabilities—PNS, PS, and PN—within Structural Causal Models \citep{galles1998axiomatic,halpern2000axiomatizing,pearl2009causality}. Subsequently, \cite{tian2000probabilities} derived tight bounds for these probabilities using Balke's linear programming \citep{balke1995probabilistic}, incorporating both observational and experimental data. Nearly two decades later, \cite{li2019unit} formally proved these bounds and introduced the unit selection model, a decision-making framework based on their linear combination. More recently, \cite{li2024unit} extended the definitions and bounds to a more general form. Additionally, \cite{pearl:etal21-r505}, as well as \cite{dawid2017}, demonstrated that these bounds could be further refined given specific causal structures.

However, any above estimation of the probabilities of causation requires both observational and experimental data. Additionally, estimating (sub)populations, based on \cite{li2022probabilities}'s suggestions, requires approximately $1,300$ entries of both data types for each (sub)population, making the process impractical. \cite{li2022learning,li2022unitlearning} demonstrated the potential of machine learning models to achieve accurate estimations for (sub)populations. In this research, we select five diverse machine learning models based on the characteristics of the probabilities of causation. We then evaluate their performance in accomplishing the task.

\subsection{Contributions}
Despite the extensive theoretical research on probabilities of causation, practical estimation methods have remained unexplored. Our work provides the first systematic approach to predicting probabilities of causation using machine learning. Specifically, we make the following contributions:
\begin{itemize}[nosep]
    \item \textbf{First Machine Learning Pipeline for Predicting Probabilities of Causation:} We propose a novel machine learning framework to estimate the bounds of PNS, PS, and PN, filling a critical gap between theoretical causal inference and practical applications.
    \item \textbf{First Accurate Machine Learning Model for PNS Prediction:} We demonstrate that a MLP can accurately predict PNS, proving that machine learning is a feasible and effective tool for estimating probabilities of causation.
    \item \textbf{First Dataset for PNS Bound Prediction:} We construct and release the first synthetic dataset specifically designed to evaluate machine learning models for estimating PNS, providing a foundation for future research in this area.
\end{itemize}
To the best of our knowledge, no prior work has applied machine learning to the problem of predicting probabilities of causation. Our study establishes a new research direction by bridging causal inference and machine learning for practical estimation tasks.

The remainder of the paper is structured as follows: first, we review key causal inference concepts to provide necessary context. Next, we introduce the model and dataset used in our study. Finally, we present our five machine learning models developed for the task. All code for data generation and machine learning models is included in the appendix.

\section{Preliminaries}
\label{related work}
In this section, we review the fundamental concepts of causal inference necessary for understanding the rest of the paper. We begin by discussing the definitions of PNS, PS, and PN as introduced by \cite{pearl1999probabilities}, followed by the definitions of identifiability and the conditions required to identify PNS, PS, and PN \citep{tian2000probabilities}. Additionally, we examine the tight bounds of PNS, PS, and PN in cases where they are unidentifiable \citep{tian2000probabilities}. Readers already familiar with these concepts may skip this section.

Similar to the works mentioned above, we adopt the causal language of Structural Causal Models (SCMs) \citep{galles1998axiomatic,halpern2000axiomatizing}. In this framework, the counterfactual statement ``Variable \( Y \) would have the value \( y \) had \( X \) been \( x \)'' is denoted as \( Y_x = y \), abbreviated as \( y_x \). We consider two types of data: experimental data, expressed as causal effects \( P(y_x) \), and observational data, represented by the joint probability function \( P(x, y) \). Unless otherwise specified, we assume \( X \) and \( Y \) are binary variables in a causal model \( M \), with \( x \) and \( y \) denoting the propositions \( X = \text{true} \) and \( Y = \text{true} \), respectively, and \( x' \) and \( y' \) representing their complements. For simplicity, we focus on binary variables; extensions to multi-valued cases are discussed by \cite{pearl2009causality} (p. 286, footnote 5) and \cite{li2024probabilities}.

First, the definitions of three basic probabilities of causation defined using SCM are as follow \citep{pearl1999probabilities}:

\begin{definition}[Probability of necessity (PN)]
Let $X$ and $Y$ be two binary variables in a causal model $M$, let $x$ and $y$ stand for the propositions $X=true$ and $Y=true$, respectively, and $x'$ and $y'$ for their complements. The probability of necessity is defined as the expression 
\begin{eqnarray}
\text{PN} &\delequal& P(Y_{X=false}=false|X=true,Y=true)\nonumber\\
 &\delequal&  P(y'_{x'}|x,y) \nonumber
\end{eqnarray}
\end{definition}

\begin{definition}[Probability of sufficiency (PS)]
Let $X$ and $Y$ be two binary variables in a causal model $M$, let $x$ and $y$ stand for the propositions $X=true$ and $Y=true$, respectively, and $x'$ and $y'$ for their complements. The probability of sufficiency is defined as the expression
\begin{eqnarray}
\text{PS} &\delequal& P(Y_{X=true}=true|X=false,Y=false)\nonumber\\
&\delequal& P(y_x|x',y') \nonumber
\end{eqnarray}
\end{definition}

\begin{definition}[Probability of necessity and sufficiency (PNS)] Let $X$ and $Y$ be two binary variables in a causal model $M$, let $x$ and $y$ stand for the propositions $X=true$ and $Y=true$, respectively, and $x'$ and $y'$ for their complements. The probability of necessity and sufficiency is defined as the expression
\begin{eqnarray}
\text{PNS} &\delequal& P(Y_{X=true}=true,Y_{X=false}=false)\nonumber\\
&\delequal& P(y_x,y'_{x'}) \nonumber
\end{eqnarray}
\end{definition}
Then, we review the identification conditions for PNS, PS, and PN \citep{tian2000probabilities}.

\begin{definition} (Monotonicity)
A Variable $Y$ is said to be monotonic relative to variable $X$ in a causal model $M$ iff
\begin{eqnarray*}
y'_x\land y_{x'}=\text{false}.
\end{eqnarray*}
\end{definition}

\begin{theorem}
If $Y$ is monotonic relative to $X$, then PNS, PN, and PS are all identifiable, and
\begin{eqnarray*}
PNS = P(y_x) - P(y_{x'}),\\
PN = \frac{P(y) - P(y_{x'})}{P(x,y)},\\
PS = \frac{P(y_x) - P(y)}{P(x', y')}.
\end{eqnarray*}
\end{theorem}

If PNS, PN, and PS are not identifiable, informative bounds are given by \cite{tian2000probabilities}.

% \begin{eqnarray}
% \max \left \{
% \begin{array}{cc}
% 0, \\
% P(y_x) - P(y_{x'}), \\
% P(y) - P(y_{x'}), \\
% P(y_x) - P(y)
% \end{array}
% \right \} \leqslant
% \text{PNS}\leqslant
% \min \left \{
% \begin{array}{cc}
%  P(y_x), \\
%  P(y'_{x'}), \\
% P(x,y) + P(x',y'), \\
% P(y_x) - P(y_{x'}) +\\
% P(x, y') + P(x', y)
% \end{array} 
% \right \}
% \label{pns}
% \end{eqnarray}

% \begin{eqnarray}
% \max \left \{
% \begin{array}{cc}
% 0, \\
% \frac{P(y)-P(y_{x'})}{P(x,y)}
% \end{array} 
% \right \} \leqslant
% \text{PN} \leqslant
% \min \left \{
% \begin{array}{cc}
% 1, \\
% \frac{P(y'_{x'})-P(x',y')}{P(x,y)}
% \end{array}
% \right \}
% \label{pn}
% \end{eqnarray}

% \begin{eqnarray}
% \max \left \{
% \begin{array}{cc}
% 0, \\
% \frac{P(y')-P(y'_{x})}{P(x',y')}
% \end{array} 
% \right \} \leqslant
% \text{PS}\leqslant
% \min \left \{
% \begin{array}{cc}
% 1, \\
% \frac{P(y_{x})-P(x,y)}{P(x',y')}
% \end{array}
% \right \}
% \label{ps}
% \end{eqnarray}

\begin{eqnarray}
\max \left \{
\begin{array}{cc}
0, \\
P(y_x) - P(y_{x'}), \\
P(y) - P(y_{x'}), \\
P(y_x) - P(y)
\end{array}
\right \} \le
\text{PNS}\label{pnslb}\\
\min \left \{
\begin{array}{cc}
 P(y_x), \\
 P(y'_{x'}), \\
P(x,y) + P(x',y'), \\
P(y_x) - P(y_{x'}) +\\
P(x, y') + P(x', y)
\end{array} 
\right \}\ge
\text{PNS}
\label{pnsub}\\
\max \left \{
\begin{array}{cc}
0, \\
\frac{P(y)-P(y_{x'})}{P(x,y)}
\end{array} 
\right \} \le
\text{PN} \label{pnlb}\\
\min \left \{
\begin{array}{cc}
1, \\
\frac{P(y'_{x'})-P(x',y')}{P(x,y)}
\end{array}
\right \}\ge \text{PN}
\label{pnub}\\
\max \left \{
\begin{array}{cc}
0, \\
\frac{P(y')-P(y'_{x})}{P(x',y')}
\end{array} 
\right \} \le
\text{PS} \label{pslb}\\
\min \left \{
\begin{array}{cc}
1, \\
\frac{P(y_{x})-P(x,y)}{P(x',y')}
\end{array}
\right \} \ge \text{PS}
\label{psub}
\end{eqnarray}

Therefore, the primary objective of this paper is then to predict Equations \ref{pnslb} to \ref{psub} (i.e., the lower and upper bounds of the PNS, PS, and PN) for any (sub)populations using those with sufficient data (i.e., sufficient data to estimate the distributions $P(X,Y)$ and $P(Y_X)$.) Due to space constraints, the focus will be on the bounds of PNS (i.e., Equations \ref{pnslb} and \ref{pnsub}). Unless otherwise specified, the discussion will be limited to binary treatment and effect, meaning both $X$ and $Y$ are binary.

\section{Structural Causal Model}
\label{scm}
In general, the equations in SCMs are in implicitly form (e.g., $Z=f_Z(X,Y,U_Z)$). However, in order to verify the accuracy of the learned bounds of PNS, we need to explicitly define the SCM and the data-generating process to determine the true PNS value and its bounds. Followed the setup in \cite{li2022learning}, we will use the following SCM. 
\begin{eqnarray*}
    \begin{cases}
        Z_i &= U_{Z_i} \text{ for } i \in \{1,...,20\},\\
        X&=f_X(M_X,U_X)\\
        &=\begin{cases}
            1& \text{ if } M_X+U_X > 0.5\\
            0& \text{ otherwise, }\\
        \end{cases}\\
        Y&=f_Y(X,M_Y,U_Y)\\
        &=\begin{cases}
            1& \text{ if } 0<C_Y \cdot X+M_Y+U_Y <1 \\
            1& \text{ if } 1<C_Y \cdot X+M_Y+U_Y <2 \\
            0& \text{ otherwise. }\\
        \end{cases}
    \end{cases}
\end{eqnarray*}
where $X,Y,Z_i$ are all binary, $U_{Z_i}, U_X, U_Y$ are binary exogenous variables with Bernoulli distributions, $C_Y$ is a constant, and $M_X, M_Y$ are linear combinations of $Z_i$. The randomly generated value of $C_Y,M_X,M_Y$ and the distributions of $U_X,U_Y,U_{Z_i}$ for the model are provided in the appendix.

\section{Data Generating Process}
Based on the defined model, $20$ binary features are considered (i.e., $Z_1,...,Z_{20}$). We made $15$ observable ($Z_1,...,Z_{15}$) and $5$ unobservable, and the exogenous variables are also unobservable, leading to $2^{15}$ observed subpopulations (i.e., the combination of $Z_1,...,Z_{15}$ defined a subpopulation).

\subsection{Informer Data}  
To evaluate the learned bounds, the informer data must have access to the actual PNS bounds for each subpopulation. Given the explicit form of the SCM and the distributions of all exogenous variables, the PNS bounds, as well as the experimental and observational distributions, can be computed for each combination of the features \( Z_1, \dots, Z_{15} \) (i.e., a subpopulation) using the SCM. For detailed mathematical formulations, refer to the appendix.

\subsection{Sample Collection}
A total of $50,000,000$ experimental and $50,000,000$ observational samples were generated as follows for each sample: In both settings, the exogenous variables \( U_X \), \( U_Y \), and \( U_{Z_i} \) were randomly generated according to their distributions specified in Section \ref{scm}. In the experimental setting, \( X \) was then assigned according to a \( \text{Bernoulli}(0.5) \) distribution, while \( Y \) and \( Z_i \) were computed using the structural functions described in Section \ref{scm}. In the observational setting, \( X \), \( Y \), and \( Z_i \) were all determined by the structural functions. The final datasets include only the observable features \( Z_1, \dots, Z_{15} \), along with \( X \) and \( Y \), while \( Z_{16}, \dots, Z_{20} \) were masked.

\subsection{Data for Machine Learning Models}  
We selected subpopulations from the $2^{15}$ possible groups that contained at least $1,300$ experimental and observational samples ($1,300$ based on \cite{li2022probabilities}'s suggestions). For these selected subpopulations, we computed the experimental and observational distributions and determined the bounds of PNS using Equations \ref{pnslb} and \ref{pnsub}. These results served as the data for our machine learning models (i.e., each data entry consists of 15 features and the PNS bounds as the label.) The obtained data includes $2,054$ entries for the lower bound (LB) and $2,065$ entries for the upper bound (UB) of the PNS.
\input{sections/ML}
\input{sections/Discussion}
% \include{sections/ML}
% \include{sections/Discussion}

\section{Conclusion}
In this paper, we demonstrated that the bounds of probabilities of causation can be effectively learned and predicted using machine learning models. Specifically, we proposed five different models to predict the bounds of PNS. Experiments showed that an MLP with the Mish activation function achieved a mean absolute error of approximately 0.02 for an SCM with 15 observed and 5 unobserved confounders. Our results suggest that machine learning is a powerful tool for causal inference, particularly in real-world scenarios where direct estimation using SCM formulas is infeasible due to data limitations. Future research will explore larger datasets with more complex SCMs.

Although our study demonstrates the feasibility of machine learning for estimating probabilities of causation, we acknowledge that our experiments are based on synthetic data generated from a structured SCM. Most existing research on probabilities of causation remains theoretical, often without practical validation, despite claims of real-world applicability. Due to page limitations, we could not extend our study to real-world applications, but this remains a critical direction for future research. We believe that bridging this gap will require developing datasets from real-world causal systems where experimental and observational data can be systematically collected. Our work serves as a first step in this direction, providing a foundation for future studies to explore the practical deployment of machine learning models for causality estimation.


% \begin{acknowledgements} % will be removed in pdf for initial submission,
% 						 % (without ‘accepted’ option in \documentclass)
%                          % so you can already fill it to test with the
%                          % ‘accepted’ class option
%     Briefly acknowledge people and organizations here.

%     \emph{All} acknowledgements go in this section.
% \end{acknowledgements}


% \newpage
\clearpage
\input{sections/ref}
% References
% \bibliography{uai2025-template,ang}
\clearpage
% \newpage

\onecolumn

\title{Supplementary Material}
\maketitle

\appendix
\section{The Causal Model}
The coefficients for \( M_X, M_Y \), and \( C_Y \) were uniformly generated from the range \([-1,1]\), while the parameters of the Bernoulli distribution were uniformly generated from \([0,1]\). The detailed model is as follows:
\begin{eqnarray*}
    &&\begin{cases}
        Z_i &= U_{Z_i} \text{ for } i \in \{1,...,20\},\\
        X&=f_X(M_X,U_X)\\
        &=\begin{cases}
            1& \text{ if } M_X+U_X > 0.5\\
            0& \text{ otherwise, }\\
        \end{cases}\\
        Y&=f_Y(X,M_Y,U_Y)\\
        &=\begin{cases}
            1& \text{ if } 0<C_Y \cdot X+M_Y+U_Y <1 \\
            1& \text{ if } 1<C_Y \cdot X+M_Y+U_Y <2 \\
            0& \text{ otherwise. }\\
        \end{cases}
    \end{cases}\\
&&\text{where, } U_{Z_i}, U_X, U_Y \text{ are binary exogenous variables with Bernoulli distributions.}\\
&&s.t., \\
&M_X& =
\begin{bmatrix}
Z_1~Z_2~...~Z_{20}
\end{bmatrix}\times
\begin{bmatrix}
0.259223510143\\ -0.658140989167\\ -0.75025831768\\ 0.162906462426\\ 0.652023463285\\ -0.0892939586541\\ 0.421469107769\\ -0.443129684766\\ 0.802624388789\\ -0.225740978499\\ 0.716621631717\\ 0.0650682260309\\ -0.220690334026\\ 0.156355773665\\ -0.50693672491\\ -0.707060278115\\ 0.418812816935\\ -0.0822118703986\\ 0.769299853833\\ -0.511585391002
\end{bmatrix},
M_Y =
\begin{bmatrix}
Z_1~Z_2~...~Z_{20}
\end{bmatrix}\times
\begin{bmatrix}
-0.792867111918\\ 0.759967136147\\ 0.55437722369\\ 0.503970540409\\ -0.527187144651\\ 0.378619988091\\ 0.269255196301\\ 0.671597043594\\ 0.396010142274\\ 0.325228576643\\ 0.657808327574\\ 0.801655023993\\ 0.0907679484097\\ -0.0713852594543\\ -0.0691046005285\\ -0.222582013343\\ -0.848408031595\\ -0.584285069026\\ -0.324874831799\\ 0.625621583197
\end{bmatrix}
\end{eqnarray*}
\begin{eqnarray*}
&&U_{Z_1} \sim \text{Bernoulli}(0.352913861526), U_{Z_2} \sim \text{Bernoulli}(0.460995855543),\\
&&U_{Z_3} \sim \text{Bernoulli}(0.331702473392), U_{Z_4} \sim \text{Bernoulli}(0.885505026779),\\
&&U_{Z_5} \sim \text{Bernoulli}(0.017026872706), U_{Z_6} \sim \text{Bernoulli}(0.380772701708),\\
&&U_{Z_7} \sim \text{Bernoulli}(0.028092602705), U_{Z_8} \sim \text{Bernoulli}(0.220819399962),\\
&&U_{Z_9} \sim \text{Bernoulli}(0.617742227477), U_{Z_{10}} \sim \text{Bernoulli}(0.981975046713),\\
&&U_{Z_{11}} \sim \text{Bernoulli}(0.142042291381), U_{Z_{12}} \sim \text{Bernoulli}(0.833602592350),\\
&&U_{Z_{13}} \sim \text{Bernoulli}(0.882938907115), U_{Z_{14}} \sim \text{Bernoulli}(0.542143191999),\\
&&U_{Z_{15}} \sim \text{Bernoulli}(0.085023436884), U_{Z_{16}} \sim \text{Bernoulli}(0.645357252864),\\
&&U_{Z_{17}} \sim \text{Bernoulli}(0.863787135134), U_{Z_{18}} \sim \text{Bernoulli}(0.460539711624),\\
&&U_{Z_{19}} \sim \text{Bernoulli}(0.314014079207), U_{Z_{20}} \sim \text{Bernoulli}(0.685879388218),\\
&&U_{X} \sim \text{Bernoulli}(0.601680857267), U_{Y} \sim \text{Bernoulli}(0.497668975278),\\
&&C_Y=-0.77953605542.
\end{eqnarray*}

\section{Detailed Data Generating Process}
If all 20 binary features are observable, then for a given feature set \( z = (z_1, \dots, z_{20}) \), the values of \( M_X \) and \( M_Y \) are fixed (denoted as \( M_X(z) \) and \( M_Y(z) \)). Under these conditions, the PNS, experimental distribution, and observational distribution corresponding to this set of features are:
\begin{eqnarray*}
PNS(z) &=& P(Y=0_{X=0}, Y=1_{X=1}|z)\\
&=& P(U_Y=0)\cdot T_0 + P(U_Y=1)\cdot T_1, \\
\text{where}, &T_0& =
\left \{
\begin{array}{cc}
1& \text{ if } f_Y(0,M_Y(z),0)=0 \text{ and } f_Y(1,M_Y(z),0)=1, \\
0& \text{otherwize},
\end{array}
\right.\\
&T_1& =
\left \{
\begin{array}{cc}
1& \text{ if } f_Y(0,M_Y(z),1)=0 \text{ and } f_Y(1,M_Y(z),1)=1, \\
0& \text{otherwize.}
\end{array}
\right.
\end{eqnarray*}
\begin{eqnarray*}
&&P(Y=1|do(X),z)\\
&=& P(U_Y=0)\cdot f_Y(X,M_Y(z),0) + P(U_Y=1)\cdot f_Y(X,M_Y(z),1).
\end{eqnarray*}
\begin{eqnarray*}
&&P(Y=1|X,z)\\
&=& P(U_X=0)\cdot P(U_Y=0)\cdot f_Y(f_X(M_X(z),0),M_Y(z),0)+\\
&&P(U_X=0)\cdot P(U_Y=1)\cdot f_Y(f_X(M_X(z),0),M_Y(z),1)) +\\ &&P(U_X=1)\cdot P(U_Y=0)\cdot f_Y(f_X(M_X(z),1),M_Y(z),0)) +\\ &&P(U_X=1)\cdot P(U_Y=1)\cdot f_Y(f_X(M_X(z),1),M_Y(z),1)).
\end{eqnarray*}

% We assumed $15$ of the features are observable (i.e., $Z_1,...,Z_{15}$), which means each subpopulation $c=(z_1,...,z_{15})$ consists $32$ sets of $20$ binary features (i.e., $s_{0}=(z_1,...,z_{15},0,0,0,0,0), s_{1}=(z_1,...,z_{15},0,0,0,0,1), s_{2}=(z_1,...,z_{15},0,0,0,1,0), ...,s_{31}=(z_1,...,z_{15},1,1,1,1,1)$), then we have the PNS, experimental distribution, and observational distribution of all observed subpopulations as follow:
We assume that $15$ of the features are observable (i.e., \( Z_1, \dots, Z_{15} \)). This implies that each subpopulation, denoted as \( c = (z_1, \dots, z_{15}) \), consists of 32 possible sets of $20$ binary features. Specifically, these sets are: $s_{0}=(z_1,...,z_{15},0,0,0,0,0), s_{1}=(z_1,...,z_{15},0,0,0,0,1), s_{2}=(z_1,...,z_{15},0,0,0,1,0), ...,s_{31}=(z_1,...,z_{15},1,1,1,1,1)$.

Under this setup, we obtain the $PNS_{\text{subpopulation}}$, experimental distribution, and observational distribution for any observed subpopulation $c$ as follows:
\begin{eqnarray*}
PNS_{\text{subpopulation}}(c) &=& P(Y=0_{X=0}, Y=1_{X=1}|c)\\
&=& P(s_{0})/P(c)PNS(s_{0})+P(s_{1})/P(c)PNS(s_{1})+\\
&&P(s_{2})/P(c)PNS(s_{2})+...+P(s_{31})/P(c)PNS(s_{31})\\
&=& P(Z_{16}=0)P(Z_{17}=0)P(Z_{18}=0)P(Z_{19}=0)P(Z_{20}=0)PNS(s_{0})+\\
&&P(Z_{16}=0)P(Z_{17}=0)P(Z_{18}=0)P(Z_{19}=0)P(Z_{20}=1)PNS(s_{1})+...+\\
&&P(Z_{16}=1)P(Z_{17}=1)P(Z_{18}=1)P(Z_{19}=1)P(Z_{20}=1)PNS(s_{31}).
\end{eqnarray*}
\begin{eqnarray*}
&&P(Y=1|do(X),c)\\
&=& P(Z_{16}=0)P(Z_{17}=0)P(Z_{18}=0)P(Z_{19}=0)P(Z_{20}=0)P(Y=1|do(X),s_{0})+\\
&& P(Z_{16}=0)P(Z_{17}=0)P(Z_{18}=0)P(Z_{19}=0)P(Z_{20}=1)P(Y=1|do(X),s_{1})+\\
&& P(Z_{16}=0)P(Z_{17}=0)P(Z_{18}=0)P(Z_{19}=1)P(Z_{20}=0)P(Y=1|do(X),s_{2})+...+\\
&& P(Z_{16}=1)P(Z_{17}=1)P(Z_{18}=1)P(Z_{19}=1)P(Z_{20}=1)P(Y=1|do(X),s_{31}).
\end{eqnarray*}
\begin{eqnarray*}
&&P(Y=1|X,c)\\
&=& P(Z_{16}=0)P(Z_{17}=0)P(Z_{18}=0)P(Z_{19}=0)P(Z_{20}=0)P(Y=1|X,s_{0})+\\
&& P(Z_{16}=0)P(Z_{17}=0)P(Z_{18}=0)P(Z_{19}=0)P(Z_{20}=1)P(Y=1|X,s_{1})+\\
&& P(Z_{16}=0)P(Z_{17}=0)P(Z_{18}=0)P(Z_{19}=1)P(Z_{20}=0)P(Y=1|X,s_{2})+...+\\
&& P(Z_{16}=1)P(Z_{17}=1)P(Z_{18}=1)P(Z_{19}=1)P(Z_{20}=1)P(Y=1|X,s_{31}).
\end{eqnarray*}
The true bounds of the $PNS_{\text{subpopulation}}(c)$ can be obtained using Equations \ref{pnslb} and \ref{pnsub}, along with the above observational and experimental distributions.

\section{Code}
All code for data generation and machine learning models is available at the following anonymous link: \url{https://anonymous.4open.science/r/2025uai-ED50/}.
\end{document}
