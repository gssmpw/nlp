\section{Introduction}
\label{sec:introduction}

Computationally efficient robust covariance estimation is a well-known example of a problem where guarantees for the Gaussian distribution are significantly stronger than for other, even very well-behaved, distributions. In particular, the stability-based filtering algorithm for the Gaussian case from \cite{DiakonikolasKK016} relies not only on strong concentration bounds, but also on a specific algebraic relationship between the second and fourth moments of the Gaussian distribution. These assumptions are clearly quite restrictive and prevent the extension of this algorithm to any meaningful class of distributions beyond Gaussians.

Recently, significant progress has been made on \emph{non-optimal} computationally efficient robust covariance estimation for certain natural classes of distributions that generalize Gaussians. The result of \cite{sos-subgaussian} implies that dimension-independent error in relative spectral norm is achievable in polynomial time for all sub-Gaussian distributions. Similarly, \cite{sos-poincare} showed that dimension-independent error in relative Frobenius norm is achievable in polynomial time for all distributions satisfying the Poincar√© inequality (with a dimension-independent constant). Their results also imply nearly optimal estimation with \emph{quasi-polynomial} number of samples (and quasi-polynomial running time).

We show that the class of \emph{elliptical distributions} is, in some sense, the right generalization of Gaussian distributions for robust covariance estimation. We hope that our work will motivate further research on elliptical distributions within algorithmic high-dimensional robust statistics.

Elliptical distributions are well-studied in the statistical literature and form a rich class of distributions.
Notable examples include the Gaussian distribution, multivariate Student's $t$-distributions (including the multivariate Cauchy), symmetric multivariate stable distributions, multivariate logistic distributions, multivariate symmetric hyperbolic distributions, and the multivariate Laplace distribution.

Scatter\footnote{Since each elliptical distribution can be written as $Ax$, where $x$ has spherically symmetric distribution, $AA^\top$ is proportional to the covariance whenever it exists. $\Sigma = AA^\top$ is called the scatter matrix. Note that it is only defined up to a multiplicative factor.} matrix estimation is not an easy problem even in the absence of corruptions. In the setting $d=\Theta(1)$, consistent estimation of the scatter matrix $\Sigma$ of an elliptical distribution is challenging, and has been studied, particularly, in \cite{Tyler1987StatisticalAF} and \cite{MAGYAR-TYLER}. 
Consistent estimation of the eigenvectors of $\Sigma$ is significantly easier, and is called \emph{elliptical component analysis} (see, for example, \cite{ECA}). 
This is because the covariance matrix of the spatial sign (the projection onto the sphere) has the same eigenvectors as $\Sigma$, and can be easily estimated, since the spatial sign has sub-Gaussian distribution. In the high-dimensional setting, the eigenvalues of $\Sigma$ can also be accurately estimated (as we show, up to factor $\Paren{1 + O(1/d)}$).


%The eigenvalues of the spatial sign, however, are (slightly) different from the eigenvalues of the scatter matrix.  As a result, in some cases, the spatial sign covariance may not serve as a reasonable estimator of $\Sigma$ in the classical setting, where vanishing estimation error is usually required as the number of samples tends to infinity.

Robust scatter matrix estimation of elliptical distributions was studied\footnote{In Huber's contamination model.} in \cite{chen2018robust}. They provided a robust estimator with optimal error in spectral norm. Their estimator, however, requires exponential computation time. In this work we show that a comparable, nearly optimal error can be achieved in polynomial time.
