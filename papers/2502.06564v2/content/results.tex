\subsection{Problem Set-Up}\label{sec:set-up}
% Definitions

%% definition corruption model + (very) short discussion
%% definition spherical distributions and longer discussion - they cover Gaussians but can have arbitrarily heavy tails, product distributions, e.g., product Cauchy, and transelliptical with identity covariance matrix


We use the \emph{strong contamination model} for corruptions. 
\begin{definition}\label{def:corruption}
    Let $ x_1, \ldots,  x_n$ be \iid samples from a distribution $\cD$ and let $\e > 0$.
    We say that $z_1, \ldots, z_n$ are an {$\e$-corruption} of $x_1, \ldots,  x_n$, if they agree on at least $(1-\e)n$ points.
    The remaining $\e$-fraction of the points can be arbitrary and in particular, can be corrupted by a computationally unbounded adversary with full knowledge of the model, our algorithm, and all problem parameters.
\end{definition}
This model captures other models frequently studied in the literature, in particular Huber's contamination model, where
$z_1\ldots z_n \simiid \Paren{1-\e} \cD + \e \cQ$,
where $\cQ$ is an arbitrary (unkown) distribution in $\R^d$.

Below we give a formal definition of \emph{elliptical distributions}.
\begin{definition}
    \label{def:elliptical}
    A distribution $\cD$ over $\R^d$ is {elliptical}, if for $ x \sim \cD$,
    \[
         x = \mu + \xi A  U\,,
    \]
    where $U$ is a uniformly distributed over the ($(d-1)$-dimensional) unit sphere in $\R^d$, $\xi$ is a positive random variable independent of $U$, $\mu\in \R^d$ is a fixed vector,  and $A\in \R^{d\times d}$ is a fixed linear transformation.
\end{definition}
Vector $\mu$ is called the \emph{location} of $\cD$, and the matrix $\Sigma = A\transpose{A}$ is called the \emph{scatter} matrix of $\cD$. Note that the scatter matrix is only defined up to a positive constant factor. If $\cD$ has finite covariance matrix, then $\Sigma$ is proportional to it, and if $\cD$ has finite expected value, it is equal to $\mu$.


To formulate our main result, we need a notion of the \emph{effective rank}.
\begin{definition}\label{def:effective-rank}
    Let $\Sigma$ be a positive definite matrix. Its {effective rank} is 
    \[
    \effrank(\Sigma) := \frac{\Tr{\Sigma}}{\norm{\Sigma}}\,.
    \]
\end{definition}
Note that $\effrank(\Sigma)$ is scale-invariant, and hence is the same for all scatter matrices. For $\Sigma \in \R^{d\times d}$, $1 \le \effrank(\Sigma) \le d$. In addition, $\effrank(\Sigma) \ge d / \kappa(\Sigma)$, where $\kappa(\Sigma)$ is the condition number of $\Sigma$. Furthermore, the effective rank is almost not affected by small eigenvalues of $\Sigma$: In particular, if we take the smallest half of the eigenvalues of $\Sigma$ and replace them by arbitrarily small positive values, the effective rank of the resulting matrix is at least $\effrank(\Sigma)/2$.


\paragraph{Error Metrics}
We use \emph{relative spectral norm} and \emph{relative Frobenius norm}. Relative Frobenius norm is defined as follows
\[
 \normf{\Sigma^{-1/2}\, \hat{\Sigma}\, \Sigma^{-1/2} - \Id}\,,
\]
If  it is bounded by some $\Delta$, then 
\[
\normf{\hat{\Sigma} - \Sigma}
=\normf{\Sigma^{1/2}\Paren{\Sigma^{-1/2}\, \hat{\Sigma}\, \Sigma^{-1/2} - \Id}\Sigma^{1/2}}
\le\norm{\Sigma} \cdot \Delta\,,
\]
since for all $A,B\in \R^{d\times d}$, $\Normf{AB}\le \Norm{A}\cdot \Normf{B}$. In addition, if
\[
\normf{\Sigma^{-1/2}\, \hat{\Sigma}\, \Sigma^{-1/2} - \Id} \le \Delta \le 1\,,
\]
then $ \normf{\hat{\Sigma}^{-1/2}\Sigma^{-1/2}\hat{\Sigma}^{-1/2} - \Id} \le O\Paren{\Delta}$.

Relative spectral norm is
\[
 \norm{\Sigma^{-1/2}\, \hat{\Sigma}\, \Sigma^{-1/2} - \Id}\,,
\]
Similarly, if it is bounded by some $\Delta$, then 
\[
\norm{\hat{\Sigma} - \Sigma}
\le\norm{\Sigma} \cdot \Delta\,.
\]
If
\[
\norm{\Sigma^{-1/2}\, \hat{\Sigma}\, \Sigma^{-1/2} - \Id} \le \Delta \le 1\,,
\]
then $ \norm{\hat{\Sigma}^{-1/2}\Sigma^{-1/2}\hat{\Sigma}^{-1/2} - \Id} \le O\Paren{\Delta}$.

Relative Frobenius norm is always at least as large as relative spectral norm, but it can be larger up to $\sqrt{d}$-factor. 


\paragraph{Notation} We denote by $[m]$ the set $\Set{1,\ldots,m}$ and by $\log(\cdot)$ the logarithm base $e$. For matrix $A\in \R^{d\times d}$, we denote by $\Norm{A}$ its spectral norm, by $\normf{A}$ its Frobenius norm, and by $\text{vec}(A)$ the vectorization of $A$. The notations $\Omega(\cdot), O(\cdot), \gtrsim, \lesssim$ hide constant factors. $\tilde{O}(\cdot), \tilde{\Omega}(\cdot)$ hide polylogarithmic factors.


\subsection{Results}\label{sec:results}
Our main result is the following theorem.
\begin{theorem}
    \label{thm:main}
    Let $C > 0$ be a large enough absolute constant.
    Let $d,n\in \N, \e\in\R$ be such that $0 < C\log(d)/d \le \e \le 1/C$ and
    \[
    n \ge  C\cdot d^2\log^5(d)/\e^2\,.
    \]
    
    Let $\cD$ be an elliptical distribution in $\R^d$ (see \definitionref{def:elliptical}) whose scatter matrix is positive definite and satisfies 
    \[
    \effrank(\Sigma) := \frac{\Tr{\Sigma}}{\norm{\Sigma}} \ge  C\cdot \log(d)\,.
    \]
    Let $x_1,\ldots,x_n \simiid \cD$, and let $z_1,\ldots,z_n \in \R^d$ be an $\e$-corruption of $x_1,\ldots, x_n$ (see \definitionref{def:corruption}).
    
    There exists an algorithm that, given $\e$ and $z_1,\ldots,z_n$, runs in time $\poly(n)$, and  outputs  $\hat{\Sigma} \in \R^{d\times d}$ that with high probability satisfies
    \[
    \norm{\Sigma^{-1/2}\, \hat{\Sigma}\, \Sigma^{-1/2} - \Id} \le O\Paren{\e\log(1/\e)}\,,
    \]
    where $\Sigma$ is some scatter matrix of $\cD$. 

    Furthermore, if $\e \ge C\log(d)/\sqrt{d}$, then with high probability
    \[
    \normf{\Sigma^{-1/2}\, \hat{\Sigma}\, \Sigma^{-1/2} - \Id} \le O\Paren{\e\log(1/\e)}\,.
    \]
\end{theorem}

First let us discuss our assumptions. Our assumption on the effective rank $\effrank(\Sigma)$ is really mild: In particular, it is satisfied for matrices with condition $\kappa(\Sigma)\lesssim d/\log(d)$.
Our assumptions on $\e$ can be restrictive in certain settings, but they are typical in high-dimensional robust statistics. In particular, works on fast robust estimation (for example, \cite{fast, fast-covariance}) usually focus on the regime $\e = d^{-o(1)}$.

To the best of our knowledge, prior to this work, no scatter matrix estimator with dimension-independent error in relative Frobenius norm was known for general elliptical distributions. For (weaker) spectral norm error and under (weaker) Huber's contamination model, \cite{chen2018robust} showed that if $n \gtrsim d/\e$, there exists an estimator computable in \emph{exponential} time, such that  $\norm{\hat{\Sigma} - \Sigma} \le O(\norm{\Sigma}\cdot\e)$ with high probability. This error is information theoretically optimal, even for the Gaussian case. 

Due to the statistical query lower bound from \cite{SQ-bounds}, error $O\Paren{\e\log^{1-\Omega(1)}(1/\e)}$ likely cannot be achieved by polynomial time algorithms (even in spectral norm), so our error bound seems to be not improvable. While their lower bound is not for scatter estimation, but for (more complicated) covariance estimation, we show that it is possible to very accurately estimate the scaling factor (up to factor $(1 + \tilde{O}(\e)/\sqrt{d})$), and hence  scatter matrix estimation  up to error $O\Paren{\e\log^{1-\Omega(1)}(1/\e)}$ would imply the same error for robust covariance estimation in relative spectral norm, which means that our scatter matrix estimation error bound is likely not improvable for estimators computable in time $\poly(d)$.

 To the best of our knowledge, prior to this work, computationally efficient robust covariance estimation with dimension-independent error was only known for distributions with moment bounds certifiable in sum-of-squares. We show that dimension-independent errors are achievable for the \emph{scatter} matrix estimation of arbitrary elliptical distributions, without any assumptions on the moments (recall that even the first moment is not required to exist).


Further in this section we discuss some applications of \cref{thm:main}.

\subsubsection{Applications for Robust PCA}

The scatter matrix contains a lot of useful information about the distribution. This information can be used in applications, in particular, for \emph{principal component analysis}\footnote{The (non-robust) principal component analysis for elliptical distributions was first studied in \cite{ECA}.}. In particular, we can robustly learn the leading eigenvector of $\Sigma$ assuming some gap between the first and the second largest eigenvalues:

\begin{corollary}\label{cor:pca}
    Let $C > 0$ be a large enough absolute constant.
    Let $d,n\in \N, \e\in\R$ be such that $0 < C\log(d)/d \le \e \le 1/C$ and
    \[
    n \ge  C\cdot d^2\log^5(d)/\e^2\,.
    \]
    Let $\cD$ be an elliptical distribution in $\R^d$ (see \definitionref{def:elliptical}) whose scatter matrix is positive definite and satisfies 
    \[
    \effrank(\Sigma) := \frac{\Tr{\Sigma}}{\norm{\Sigma}} \ge  C\cdot \log(d)\,.
    \]
     Suppose that for some $\gamma > 0$, $\frac{\lambda_1 - \lambda_2}{\lambda_1} \ge \gamma$, where $\lambda_1$ and $\lambda_2$ are respectively the first and the second largest eigenvalues of $\Sigma$.
    
    Let $x_1,\ldots,x_n \simiid \cD$, and let $z_1,\ldots,z_n \in \R^d$ be an $\e$-corruption of $x_1,\ldots, x_n$ (see \definitionref{def:corruption}).
    
    There exists an algorithm that, given $\e$ and $z_1,\ldots,z_n$, runs in time $\poly(n)$, and  outputs  $\hat{v} \in \R^{d}$ that with high probability satisfies
    \[
    \normf{\hat{v}\hat{v}^\top - vv^\top}\le O\Paren{\frac{\e\log(1/\e)}{\gamma}}\,,
    \]
    where $v$ is the leading eigenvector of $\Sigma$. 
\end{corollary}

This result is an easy consequence of the spectral norm bound from \cref{thm:main}. 
The error bound matches the best known guarantees for the Gaussian distribution \cite{robust-pca-li, robust-pca-diak}. However, the sample complexity is worse. This is not surprising, since the algorithms for the Gaussian case were specifically designed for robust PCA. Improving the sample complexity of robust PCA for elliptical distributions is beyond the scope of this work, but we believe it to be an interesting direction for future research.






\subsubsection{Applications for Robust Covariance Estimation}
    \cref{thm:main} shows that it is possible to robustly estimate the scatter matrix. As was previously mentioned, this allows us to learn many useful things about the covariance (when it exists): the eigenvectors, the effective rank, the condition number, the ratio between the first and the second largest eigenvalues, etc. However, learning the scaling factor between the scatter matrix that we estimate and the covariance (and hence the covariance itself) might not always be possible even when there are no corruptions. For example, in the one-dimensional case, scatter matrices are just positive numbers, and the scale can be an arbitrary positive distribution, so without assumptions on higher moments, learning its covariance (when it exists) can require arbitrarily many samples. Clearly, the same issue appears in the high-dimensional case, where the scatter matrix is non-trivial and contains all scale-invariant information about the covariance. Hence we need some moment assumptions if we want to estimate the scaling factor.

    A reasonable strategy to estimate the scaling factor is to robustly estimate\footnote{Here we assume that the location of $\cD$ is zero.} $\Tr(\Sigma) = \E\norm{x}^2$ (in this subsection we denote the covariance of $x$ by $\Sigma$ and the scatter matrix that we accurately estimated by $\tilde{\Sigma}$). Since it is a one-dimensional problem, it is easy to do in polynomial time as long as $\norm{x}^2$ is well-concentrated. 
    A natural assumption for Gaussian-type concentration of $\norm{x}^2$ is the
    \emph{Hanson-Wright inequality}:
    \begin{definition}[Hanson-Wright property]\label{def:hanson-wright}
    Let $\cD$ be a distribution in $\R^d$ with mean $\mu\in \R^d$ and positive definite covariance $\Sigma\in\R^{d\times d}$, and let $C_{HW} > 0$. We say that $\cD$ satisfies \emph{$C_{HW}$-Hanson-Wright property}, if for all $A\in \R^{d\times d}$ and $t>0$, 
    \[
    \Pr_{x\sim\cD} \Paren{\Abs{z^\top Az - \Tr A}\ge t} \le 2\exp\Paren{-\frac{1}{C_{HW}} \min\Set{\frac{t^2}{\normf{A}^2}, \frac{t}{\norm{A}}}},
    \]
    where $z = \Sigma^{-1/2}(x-\mu)$.
    \end{definition} 
    In particular, isotropic distributions that satisfy this property with $C_{HW} = O(1)$ include the standard Gaussian distribution, the uniform distribution over the sphere, the uniform distribution over the Euclidean ball, and their $O(1)$-Lipschitz transformations\footnote{Of course our results can only be applied to Lipchitz transformations of these distributions that preserve the elliptical structure.}. More generally, all $O(1)$-log-Sobolev distributions also satisfy this property. Note that the Hanson-Wright property (in our formulation) is preserved under affine transformations.
    
    If $x$ satisfies the $O(1)$-Hanson-Wright property, we can estimate $\E\norm{x}^2$ up to factor $\Paren{1+\tilde{O}(\e)/\sqrt{d}}$, and it is enough for robust covariance estimation with error $\tilde{O}(\e)$ in relative Frobenius norm. 
    
    In order to estimate the covariance in relative \emph{spectral} norm, it is enough to assume that $x$ is \emph{sub-exponential}. 
    
    \begin{definition}[Sub-exponential distributions]\label{def:sub-gaussain}
 Let $\cD$ be a distribution in $\R^d$ with mean $\mu\in\R^d$, and let $C_{SE} > 0$. We say that $\cD$ is \emph{$C_{SE}$-sub-exponential}, if for all $u\in \R^{d}$ and even $p\in \N$, 
    \[
    \Paren{\E_{x\sim \cD}\iprod{u, x-\mu}^p}^{1/p}\le C_{SE}\cdot {p}\cdot \Paren{\E_{x\sim \cD}\iprod{u, x-\mu}^2}^{1/2}\,.
    \]
    \end{definition}
    $O(1)$-sub-exponential distributions satisfy the $O(1)$-Hanson-Wright property, but the converse is not true.
    For distributions that are {simultaneously} sub-exponential and elliptical (in particular, the multivariate Laplace distribution), we can achieve nearly optimal error in relative spectral norm in polynomial time.
    Specifically, we prove the following theorem.

    \begin{theorem}\label{thm:covariance-estimation}
    Let $C > 0$ be a large enough absolute constant.
    Let $d,n\in \N, \e\in\R$ be such that $0 < C\log(d)/d \le \e \le 1/C$ and
    \[
    n \ge  C\cdot d^2\log^5(d)/\e^2\,.
    \]
    
    Let $\cD$ be an elliptical distribution in $\R^d$ (see \definitionref{def:elliptical}) with mean $\mu$ and positive definite covariance $\Sigma$ that satisfies 
    \[
    \effrank(\Sigma) := \frac{\Tr{\Sigma}}{\norm{\Sigma}} \ge  C\cdot \log(d)\,.
    \]
    Suppose in addition that $\cD$ is also $O(1)$-sub-exponential (see \cref{def:sub-gaussain}).
    Let $x_1,\ldots,x_n \simiid \cD$, and let $z_1,\ldots,z_n \in \R^d$ be an $\e$-corruption of $x_1,\ldots, x_n$ (see \definitionref{def:corruption}).
    
    There exists an algorithm that, given $\e$ and $z_1,\ldots,z_n$, runs in time $\poly(n)$, and outputs $\hat{\Sigma} \in \R^{d\times d}$ that with high probability satisfies
    \[
    \norm{\Sigma^{-1/2}\, \hat{\Sigma}\, \Sigma^{-1/2} - \Id} \le O\Paren{\e\log(1/\e)} \,.
    \]
    
    Furthermore, if $\e \ge C\log(d)/\sqrt{d}$ and $\cD$ satisfies the $O(1)$-Hanson-Wright property, then with high probability
    \[
    \normf{\Sigma^{-1/2}\, \hat{\Sigma}\, \Sigma^{-1/2} - \Id} \le O\Paren{\e\log(1/\e)}\,.
    \]

    \end{theorem}
    While it might seem surprising that we have $O(\e\log(1/\e))$ error for sub-exponential distributions instead of $O(\e\log^2(1/\e))$ that we expect to have in the one-dimensional case for (possibly symmetric) sub-exponential distributions,
    this happens due to the high-dimensional nature of the problem. Recall that $x\sim \cD$ with location $0$ can be written as $ \xi A \sqrt{d} \cdot U$, where $U$ is uniformly distributed over the unit sphere, and assume that the singular values of $A$ are $\Theta(1)$. 
    In high-dimensions, $\E\iprod{\sqrt{d} U, v}^p$ is very close to $\E_{g\sim\cN(0,\Id_d)}\iprod{g, v}^m$ unless $p$ is very large (of order $d$). Therefore, the variable $A\sqrt{d} \cdot U$ has sub-Gaussian moments\footnote{We call a distribution $\cD$ $O(1)$-\emph{sub-Gaussian} if for all $u\in \R^d$ and all even $p\in\N$, $\Paren{\E_{x\in \cD}\iprod{u, x-\mu}^p}^{1/p}\le O(\sqrt{p})\Paren{\E_{x\in \cD}\iprod{u, x-\mu}^2}^{1/2}$. Such distributions are also sometimes called \emph{hypercontractive sub-Gaussian}.}, and hence the moments of $\xi$ have to be sub-Gaussian (at least for $p \le d$), since $x$ is sub-exponential. On the other hand, since $\norm{A \sqrt{d} U}$ is a bounded random variable, $\norm{x} = \xi  \norm{A \sqrt{d} U}$ has \emph{sub-Gaussian moments}, so it is possible to robustly learn $\E \norm{x}^2$ up to factor $\Paren{1 + O(\e\log(1/\e))}$. 
    
Let us compare our results with prior work. As was previously mentioned, \cite{DiakonikolasKK016} developed a technique that achieves error $O(\e\log(1/\e))$ in relative Frobenius norm in $\poly(n)$ time with $n=\poly(d)$ samples for Gaussians.
This result exploits exact algebraic relations between Gaussian second and fourth moments, and hence is only valid for the Gaussian distribution\footnote{It can also be extended for certain mixtures of Gaussians, see section 4.3.2 of \cite{DK_book}.}.


Recently \cite{sos-poincare} showed that moment bounds on quadratic forms of arbitrary Poincar\'e distributions can be certified by (low degree) sum-of-squares proofs. \cite{Bakshi} showed that it implies that the error $O\Paren{t^2\e^{1-1/t}}$ in relative Frobenius norm is achievable for robust covariance estimation of all Poincare distributions with $d^{\poly(t)}$ samples in time $d^{\poly(t)}$. In particular, there exists an estimator that achieves error $\tilde{O}(\e)$ for all Poincar\'e distributions with $d^{\polylog(1/\e)}$ time and sample complexity.
Another recent result \cite{sos-subgaussian} shows that moment bounds of arbitrary sub-Gaussian distributions are certifiable in sum-of-squares. \cite{KS17} showed that it implies the guarantees similar to those of \cite{Bakshi} for all sub-Gaussian distributions, but in (weaker) relative spectral norm. Note that both results require \emph{quasi-polynomial} number of samples (and running time) for error $\tilde{O}(\e)$. 

Elliptical distributions admit much better estimation guarantees than sub-Gaussian or PoincarÃ© distributions: Our estimator achieves nearly optimal error $\tilde{O}(\varepsilon)$, with a sample complexity that is nearly optimal, and runs in polynomial time.



    


