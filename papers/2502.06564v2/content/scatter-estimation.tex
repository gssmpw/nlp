\section{First Estimation}
First, if we have more samples than needed, we simply take only the necessary number of samples (choosing them at random) and do not use the other samples. We need this step to make sure that $n=\poly(d)$, and small probability events do not happen with any of the (uncorrupted) samples.
For simplicity, further we denote by $n$ the number of samples that the algorithm actually uses.

For $i \in [\lfloor n / 2 \rfloor]$, let $z'_i = \paren{z_i - z_{\lfloor n / 2 \rfloor + i}}/\sqrt{2}$. Then, by Theorem 4.1 from \cite{frahm2004generalized}, $z'$ is a $4\e$-corruption of the sample $x'_1,\ldots x'_{\lfloor n/2 \rfloor} \simiid \cD'$, where $\cD'$ is an elliptical distribution with location $0$ and scatter matrix $\Sigma$. To simplify the notation, we assume that this preprocessing step has been done, and the input $z_1,\ldots,z_n$ is an $\e$-corruption of an iid sample from an elliptical distribution with location $0$ and scatter matrix $\Sigma$.

Then we project $z_1,\ldots, z_n$ onto the sphere of radius $\sqrt{d}$. 
We can assume that the projected samples are $\spsign(y'_1),\ldots\spsign(y'_n)$, where $y'_1,\ldots y'_n$ is an $\e$-corruption of $y_1,\ldots, y_n\simiid \cN(0,\Sigma)$ (and $\spsign$ is the projection). 
Even though $\spsign(y)$ has nice properties, for technical reasons it is  not convenient to analyze it. Instead, we analyze the function $\cF:\R^d \to \R^d$ defined as
    \[
    \cF(x) = 
    \begin{cases}
        \spsign(x) = {\sqrt{d}} \cdot x/{\norm{x}} &\text{if } 0.9d \le \norm{x}^2 \le 1.1d\,,\\
        \sqrt{10/9}\cdot x &\text{if } \norm{x}^2 < 0.9d\,,\\
        \sqrt{10/11} \cdot x &\text{if } \norm{x}^2 > 1.1d\,.
    \end{cases}
    \]

By Hanson-Wright inequalities (\cref{prop:Hanson-Wright}), if $\effrank(\Sigma) \gtrsim \log(d)$, then with high probability for all $i\in[n]$, $\cF(y_i) = \spsign(y_i)$. 
Hence we can assume that we are given an $\e$-corruption of iid samples that are distributied as $\cF(y)$, where $y\sim \cN(0,\Sigma)$. We call these corrupted samples $\zeta_1,\ldots, \zeta_n$, and true samples $\cF(y_1),\ldots , \cF(y_n)$. 


The following lemma shows that the covariance of $\cF(y)$ is close to $\Sigma$.

\begin{lemma}\label{lem:closeness}
    Let $\Sigma \in \R^{d\times d}$ be a positive semidefinite matrix such that $\Tr(\Sigma) = d$ and $\effrank(\Sigma) \gtrsim \log(d)$.
    
    Then 
    \[
    \Norm{\Sigma^{-1/2}\,
    \Paren{\Cov_{y \sim \cN(0,\Sigma)}{\cF(y)} - \Sigma}
    \,\Sigma^{-1/2}}  \le O\Paren{\frac{1}{\effrank(\Sigma)}}\,.
    \]

\end{lemma}
\begin{proof}
    Observe that $\Sigma' := \Cov_{y \sim \cN(0,\Sigma)}\brac{\cF(y)}$ has the same eigenvectors as $\Sigma$. Indeed, in the basis where $\Sigma$ is diagonal, 
    \[
    \Sigma'_{ij}= \E\Brac{\frac{\sqrt{\lambda_i\lambda_j} \cdot g_ig_j}{\tfrac{1}{d}\sum_{i=1}^d \lambda_i g_i^2 \cdot \ind{\sum_{i=1}^d \lambda_i g_i^2\in[0.9d,1.1 d]} 
    + \tfrac{9}{10} \ind{\sum_{i=1}^d \lambda_i g_i^2 < 0.9d} 
    + \tfrac{11}{10} \ind{\sum_{i=1}^d \lambda_i g_i^2 > 1.1d}
    }} 
    \,,
    \]
    where $\lambda_1,\ldots, \lambda_d$ are the eigenvalues of $\Sigma$. Since the signs of $g_i$ and $g_j$ are independent and are independent of all $g_1^2,\ldots, g_d^2$, $\Sigma'_{ij} = 0$ if $i\neq j$.
    
    Hence if we take these eigenvectors as the basis, the matrices $\Sigma^{-1/2}$, $\Sigma'$ and $\Sigma$ become diagonal, and the value of the relative spectral norm does not change. In this basis, the eigenvalues of $\Sigma'$ are its diagonal entries.
    
    Note that
       \[
     \Norm{\Sigma^{-1/2}\,
    \Paren{\Sigma' - \Sigma}
    \,\Sigma^{-1/2}} = \max_{j\in [d]} \Abs{\frac{\lambda_j'}{\lambda_j} - 1} \,.
    \]

    Consider 
    \[
    q = 1 - \Paren{\tfrac{1}{d}\sum_{i=1}^d \lambda_i g_i^2 \cdot \ind{\sum_{i=1}^d \lambda_i g_i^2\in[0.9d,1.1 d]} 
    + \tfrac{9}{10} \ind{\sum_{i=1}^d \lambda_i g_i^2 < 0.9d} 
    + \tfrac{11}{10} \ind{\sum_{i=1}^d \lambda_i g_i^2 > 1.1d}}\,.
    \]
    Note that since $\abs{q} \le 1/10$, $\frac{1}{1 - q} = \sum_{k=0}^{\infty} q^k$.
    Therefore,
    \[
    \frac{\lambda'_i}{\lambda_i}
    =
    \sum_{k=0}^{\infty} \E{g_j^2 q^k} 
    = \E{g_j^2}+ \E\Brac{g_j^2 q} +  \E \Brac{g_j^2 q^2}  + \sum_{k=3}^{\infty} \E \Brac{g_j^2 q^k}\,.
    \]
    The first term is $1$. 
    Let us bound the second term. Let $N=d^{1000}$, and for all $i\in[d]$ consider $g_{1i},\ldots,g_{Ni}\simiid \cN(0,1)$. By Hanson-Wright inequalities (Proposition \ref{prop:Hanson-Wright}) with high probability, for all $m \in [N]$,  
    $\tfrac{1}{d}\sum_{i=1}^d \lambda_i (1-g_{mi}^2) \in [0.9,1.1]$. 
    Also, with high probability, the sample average of 
    $g_{mj}^2 \cdot \tfrac{1}{d}\sum_{i=1}^d \lambda_i (1-g_{mi}^2)$
    is $d^{-10}$-close to both $\E\Brac{g_j^2 q}$ and $\E g_j^2 \cdot\tfrac{1}{d}\sum_{i=1}^d \lambda_i (1-g_i^2) = \frac{2\lambda_j}{d}$ (since both random variables $g_j^2 q$ and $g_j^2 \cdot\tfrac{1}{d}\sum_{i=1}^d \lambda_i (1-g_i^2)$ have variance $O(1)$). Therefore, the second term is bounded by $\frac{2\lambda_j}{d} + 2d^{-10} \le \frac{3}{\effrank(\Sigma)}$ in absolute value.
    
    Since $\abs{q} \le \abs{1-\tfrac{1}{d}\sum_{i=1}^d \lambda_i g_i^2}$,
    \[
    \E \Brac{g_j^2 q^2} \le 
    \tfrac{1}{d^2}\sum_{i=1}^d \lambda_i^2 \E g_j^2 (1-g_i^2)^2
    \le \tfrac{20}{d^2} \sum_{i=1}^d \lambda_j^2
    \le \tfrac{20}{d}\Norm{\Sigma} 
    \le \tfrac{20}{\effrank(\Sigma)}\,.
    \]
    
    Now let us bound the fourth term. Since $\abs{q} \le 1/10$,
    \[
    \Abs{\sum_{k=3}^{\infty} \E \Brac{g_j^2 q^k}} \le \E \Brac{g_j^2 q^2} 
    \le \frac{20} {\effrank(\Sigma)}\,.
    \]

    Hence for all $i\in[d]$, $\frac{\lambda'_i}{\lambda_i} = 1 + O(\frac{1}{\effrank(\Sigma)})$.
\end{proof}

The following proposition shows that a good estimator of $\Sigma'$ is also a good estimator of $\Sigma$.

\begin{proposition}\label{prop:triangle-inequality}
Let $\Sigma, \Sigma', \hat{\Sigma}\in \R^{d\times d}$ 
be positive definite matrices such that for some $\Delta \le 1$,
\[
\normf{{\Sigma}^{-1/2}\, \Sigma'\, {\Sigma}^{-1/2} - \Id} \le \Delta
\]
and
\[
\normf{\Paren{\Sigma'}^{-1/2}\, \hat{\Sigma}\, \Paren{\Sigma'}^{-1/2} - \Id} \le \Delta\,.
\]
Then
\[
\normf{{\Sigma}^{-1/2}\, \hat{\Sigma}\, {\Sigma}^{-1/2} - \Id} \le 3\Delta\,.
\]
The same is true if Frobenius norm is replaced by spectral norm.
\end{proposition}
\begin{proof}
\begin{align*}
\normf{{\Sigma}^{-1/2}\, \hat{\Sigma}\, {\Sigma}^{-1/2} - \Id} 
&\le
\normf{{\Sigma}^{-1/2}\, \hat{\Sigma}\, {\Sigma}^{-1/2} -{\Sigma}^{-1/2}\, \Sigma'\, {\Sigma}^{-1/2}} +\normf{{\Sigma}^{-1/2}\, \Sigma'\, {\Sigma}^{-1/2} - \Id}
\\&\le \norm{\Sigma^{-1/2}\Paren{\Sigma'}^{1/2}}^2 \cdot \normf{\Paren{\Sigma'}^{-1/2}\, \hat{\Sigma}\, \Paren{\Sigma'}^{-1/2} - \Id} + \Delta
\\&\le \norm{\Sigma^{-1/2}\Paren{\Sigma'}^{1/2}}^2\cdot \Delta + \Delta\,.
\end{align*}
Denote $A = \Sigma^{-1/2}\Paren{\Sigma'}^{1/2}$. Since $\norm{AA^\top - \Id} \le \normf{AA^\top - \Id} \le \Delta$, 
\[
\norm{A}^2 = \norm{AA^\top} \le 1 + \norm{AA^\top - \Id} \le 1 + \Delta \le 2\,.
\]
Hence we get the desired bound. It is clear that this derivation is also correct if we replace Frobenius norm by spectral norm.
\end{proof}

Let us show how to estimate $\Sigma' = \Cov_{y \sim \cN(0,\Sigma)}{\cF(y)}$ up to error $O\Paren{\frac{1}{\log d} + \sqrt{\e}}$ in relative spectral norm. 
 
Let us split the samples into $2$ equal parts, and run 
the covariance estimation algorithm in relative spectral norm from Theorem 1.2 from \cite{KS17} on the projection onto the sphere of the first part. 
We can do that, since $\cF(y)$ is $O(1)$-sub-Gaussian. Indeed, for all $u\in\R^d$,
\[
\Paren{\iprod{u, \cF(y)}^p}^{1/p}\le 1.1\Paren{\iprod{u, y}^p}^{1/p} \cdot C_G\cdot \sqrt{p}\Paren{\E\iprod{u,y}^2}^{1/2} \le  \frac{1.1}{0.9} \cdot C_G\cdot \sqrt{p}\Paren{\E\iprod{u,\cF(y)}^2}^{1/2}\,,
\]
where $C_G \le O(1)$ is the sub-Gaussian parameter for the Gaussian distribution.

With high probability we get an estimator $\hat{\Sigma}_1$ such that 
    \[
    \norm{(\Sigma')^{-1/2}\, \hat{\Sigma}_1\, (\Sigma')^{-1/2} - \Id} \le O\Paren{\sqrt{{\e}}}\,.
    \]
\lemmaref{lem:closeness} and our assumption on the effective rank imply that 
    \[
    \Norm{\Sigma^{-1/2}\,
    \Paren{\Cov_{y \sim \cN(0,\Sigma)}{\cF(y)} - \Sigma}
    \,\Sigma^{-1/2}}  
    \le  O\Paren{\frac{1}{\log(d)}} \,.
    \]
Hence, by \cref{prop:triangle-inequality}, 
$0.99 \cdot \Id \preceq \hat{\Sigma}_1^{-1}\Sigma \preceq 1.01\cdot \Id$. 
If we multiply the second part of the samples by $\hat{\Sigma}_1^{-1}$, we get an elliptical distribution with new scatter matrix $\tilde{\Sigma} = \rho \hat{\Sigma}_1^{-1} \Sigma$, where $\rho = d/\Tr(\hat{\Sigma}_1^{-1} \Sigma)$. $\tilde{\Sigma}$ has trace $d$ and its eigenvalues are between $0.9$ and $1.1$, and, in particular, its effective rank is $\Omega(d)$.







\section{Fourth Moment of the Spatial Sign}
In this section and further in the paper we use a slightly modified definition of the function $\cF$:
\begin{definition}\label{def:function}
    \[
    \cF(x) = 
    \begin{cases}
        \spsign(x) = {\sqrt{d}} \cdot x/{\norm{x}} &\text{if } d-\Delta \le \norm{x}^2 \le d+\Delta\,,\\
        \sqrt{d/(d-\Delta)}\cdot x &\text{if } \norm{x}^2 < d-\Delta\,,\\
        \sqrt{d/(d+\Delta)}\cdot x &\text{if } \norm{x}^2 > d+\Delta\,.
    \end{cases}
    \]    
for $\Delta = C\cdot \sqrt{d\log d}$, where $C$ is a large enough absolute constant.
\end{definition} 

If $\effrank(\Sigma)\ge \Omega(d)$, then with high probability $\cF(y_i)$ coincides with $\spsign(y_i)$ for all $n=\poly(d)$ samples $y_1,\ldots y_n\simiid\cN(0,\Sigma)$. Note that as long as $\effrank(\Sigma) \ge \Omega(d)$, the proof of \cref{lem:closeness} remains the same, so $\Cov \cF(y)$ is $O(1/d)$-close to $\Sigma$ in relative spectral norm.

\begin{lemma}\label{lem:tensor}
    Let $\Sigma\in \R^{d\times d}$ be a positive definite matrix such that $0.9\Id \preceq \Sigma \preceq 1.1\Id$, $\Tr(\Sigma) = d$.
    Let $g\sim \cN(0,\Id)$, $\Sigma'= \Cov(\cF(\Sigma^{1/2}g))$ and $\phi(g)= \Paren{\Sigma'}^{-1/2} \cF(\Sigma^{1/2}g)$.
    
    Let $T = \E\Paren{\phi(g)\transpose{\phi(g)} - \Id}^{\otimes 2}$ and $S = \E_{g\sim N(0,\Id)}\Paren{gg^\top / \norm{g}^2 - \Id}^{\otimes 2}$. Then for all $i_1 i_2 i_3 i_4, i, j \in [d]$, the components of $T$ in the eigenbasis of $\Sigma$ are as follows:
    \begin{enumerate}
        \item  $T_{i_1 i_2 i_3 i_4} = S_{i_1 i_2 i_3 i_4} = 0$ if some of the $i_m$ is not equal to the others.
        \item $T_{iiii} = 2 + O(1/d) = S_{iiii} + O(1/d)$.
        \item For $i\neq j$, $T_{ijij} = 1 + O(1/d) = S_{ijij} + O(1/d)$.
        \item For $i\neq j$, $T_{iijj} = S_{iijj} + O\Paren{{\Norm{\Sigma-\Id}}/{d}} + O(\log^2(d)/d^2)$.
    \end{enumerate}
\end{lemma}
\begin{proof}
 Without loss of generality we assume $\Normf{M}=1$. 
    Let us bound the entries of the tensor 
    $T := \E\Paren{\phi(g)\transpose{\phi(g)} - \Id}^{\otimes 2}$ in the eigenbasis of $\Sigma$:
    \[
    T_{i_1 i_2 i_3 i_4} = 
    \E
    \Paren{\sqrt{\tfrac{\lambda_{i_1}}{\lambda'_{i_1}}\cdot\tfrac{\lambda_{i_2}}{\lambda'_{i_2}}}\cdot g_{i_1}g_{i_2}/R(g)^2 - \Id_{i_1i_2}} 
    \Paren{\sqrt{\tfrac{\lambda_{i_3}}{\lambda'_{i_3}}\cdot\tfrac{\lambda_{i_4}}{\lambda'_{i_4}}}\cdot g_{i_3}g_{i_4}/R(g)^2 - \Id_{i_3i_4} }
     \,,
    \]
    where $R(g)^2 = {\tfrac{1}{d}\sum_{i=1}^d \lambda_i g_i^2 \cdot \ind{\sum_{i=1}^d \lambda_i g_i^2\in[d - \Delta,d + \Delta]} 
    + \tfrac{d - \Delta}{d} \ind{\sum_{i=1}^d \lambda_i g_i^2 < d-\Delta} 
    + \tfrac{d+\Delta}{d} \ind{\sum_{i=1}^d \lambda_i g_i^2 > d+\Delta}
    }$, where $\Delta= O\Paren{\sqrt{d\log d}}$.
    
    First, note that in the case $\lambda_1 = \ldots = \lambda_d = 1$, the components $T_{i_1 i_2 i_3 i_4}$ and $S_{i_1 i_2 i_3 i_4}$ are expectations of random variables that coincide with probability at least $1/\poly(d)$ and have variance bounded by $O(1)$. Hence the difference between them is at most $O(d^{-10})$.
    
    Consider the entries $T_{i_1 i_2 i_3 i_4}$ such that some $i_m$ is not equal to any of the others. 
    In this case, since $\sign(g_{i_m})$ is independent of $\abs{g_{i_m}}$ and of all other $g_j$, $T_{i_1 i_2 i_3 i_4} = 0$.
    
    Consider the terms of the form $T_{iijj}$ (including the case $i=j$). 
    \[
    T_{iijj} = \frac{\lambda_i\lambda_j}{\lambda'_i\lambda'_j}\cdot \E\frac{g_i^2g_j^2}{R(g)^4} - 1 = \Paren{1 + O(1/d)}\cdot \E\frac{g_i^2g_j^2}{R(g)^4} - 1 \,.
    \]
    As in the proof of \lemmaref{lem:closeness}, let $q = 1-R(g)^2$. Since $\abs{2q - q^2} \le 1/2$,
    \[
    \E\frac{g_i^2g_j^2}{R(g)^4} = \E\frac{g_i^2g_j^2}{1-(2q - q^2)} 
    = \E g_i^2g_j^2  + \sum_{k=1}^\infty \E g_i^2g_j^2 (2q - q^2)^k\,.
    \]
    The first term is $1$ if $i\neq j$ and $3$ if $i=j$. Let us bound the other terms.
    By the same argument as in \lemmaref{lem:closeness}, 
    $\E g_i^2g_j^2 q = \frac{2\lambda_j + 2\lambda_i}{d} + O(d^{-10})$.
     If we write $\lambda_k = 1 + \delta_k$, this expression differs from the same expression for $\lambda_k = 1$ by $O(\delta_i / d + \delta_j/d)$.
    Similarly,
\[
    \E \Brac{g_i^2g_j^2 q^2} 
    = \E g_i^2g_j^2\Paren{\tfrac{1}{d}\sum_{m=1}^d\lambda_m (1-g_m^2)}^2+ O(d^{-10})\,.
    \]
    Since $g_i$ are independent and $\E(1-g_m^2)=1$,
    \[
    \E \E g_i^2g_j^2  \Paren{\tfrac{1}{d}\sum_{m=1}^d\lambda_m (1-g_m^2)}^2
    = \tfrac{1}{d^2}\sum_{m=1}^d \lambda_m^2 \E g_i^2g_j^2 (1-g_m^2)^2 + \tfrac{2}{d^2}\lambda_i\lambda_j \E g_i^2g_j^2 (1-g_i^2)(1-g_j^2) \,.
    \]

    If we write $\lambda_k = 1 + \delta_k$ for $\abs{\delta_k}\le 0.1$, this expression differs from the same expression for $\lambda_k = 1$ by at most $O(\max_{k\in[d]} \delta_k / d)$.
     
    Finally, consider
\[
    \E \Brac{g_i^2g_j^2 q^3} 
    = \E g_i^2g_j^2\Paren{\tfrac{1}{d}\sum_{m=1}^d\lambda_m (1-g_m^2)}^3 + O(d^{-10})\,.
    \]
    Note that,
    \begin{align*}
    \E g_i^2g_j^2\Paren{\tfrac{1}{d}\sum_{m=1}^d\lambda_m (1-g_m^2)}^3
    = &\tfrac{1}{d^3} \sum_{m=1}^d \lambda_m^3 \E g_i^2g_j^2 (1-g_m^2)^3 + \tfrac{1}{d^3} \sum_{m=1}^d \lambda_i\lambda_m^2\E g_i^2g_j^2(1-g_i^2)(1-g_m^2)^2 
    \\&+ \tfrac{1}{d^3} \sum_{m=1}^d \lambda_j\lambda_m^2\E g_i^2g_j^2(1-g_j^2)(1-g_m^2)^2
    + O(1/d^3) \,.
    \end{align*}
    Since each term in each sum is bounded by $O(1)$, $\abs{\E \brac{g_i^2g_j^2 q^3} }\le O(1/d^2)$.
    Since $q^4 \le O(\log^2(d)/d^2)$, the sum of the terms of the series $\sum_{k=1}^\infty \E g_i^2g_j^2 (2q - q^2)^k$ that are not linear in $q$, $q^2$, or $q^3$, is bounded by $O(\log^2(d)/d^2)$. Hence we get the desired expressions for $T_{iijj}$. Note that $T_{ijij}$ for $i\neq j$ is equal to $1+T_{iijj}$, so $T_{ijij} = 1 + O(1/d)$. 

    

    

    % Now we can finish the proof.
    % \begin{align*}
    % \E_{g\sim \cN(0,\Id)} \Iprod{M, \phi(g)\transpose{\phi(g)} - \Id}^2 
    % &= 
    % \sum_{1\le i_1i_2i_3i_4\le d} M_{i_1i_2}M_{i_3i_4}T_{i_1 i_2 i_3 i_4} 
    % \\&=
    % \sum_{i=1}^d M_{ii}^2 T_{iiii} + 2\sum_{1\le i,j\le d} M_{ij}^2T_{ijij} + 
    %  2\sum_{1\le i,j\le d} M_{ii}M_{jj}T_{iijj}
    %  \\&\le O(\normf{M}^2) + O(1/d) \Paren{\sum_{i=1}^d M_{ii}}^2
    %  \\&\le O(\normf{M}^2) + O(1/d) \Paren{\sum_{i=1}^d M_{ii}^2}\cdot d
    %   \\&\le O(\normf{M}^2)\,,
    % \end{align*}
    % where the second to last inequality follows from the  Cauchy--Schwarz inequality.
\end{proof}






% \begin{lemma}\label{lem:moments}
%     Let $\Sigma\in \R^{d\times d}$ be a positive definite matrix such that $\effrank(\Sigma) \ge \Omega(d)$.
%     Let $g\sim \cN(0,\Id)$, $\Sigma'= \Cov(\cF(\Sigma^{1/2}g))$, and $\phi(g)= \Paren{\Sigma'}^{-1/2} \cF(\Sigma^{1/2}g)$.
%     Then for all even numbers $p$ and all matrices $M\in \R^{d\times d}$,
%     \[
%     \Paren{\E_{g\sim \cN(0,\Id)} \Iprod{M, \phi(g)\transpose{\phi(g)} - \Id}^p}^{1/p}
%     \le  O(\Normf{M}\cdot p)\,. 
%     \]
% \end{lemma}

% \begin{proof}
%     Without loss of generality we assume $\Normf{M}=1$. 
%  By \Gnote{reference}, $\phi(g)$ satisfies $LS(3)$. Hence by \Gnote{reference},
%     \[
%      \E\Iprod{M, \phi(g)\transpose{\phi(g)} - \Id}^p \le \Paren{\E \Iprod{M, \phi(g)\transpose{\phi(g)} - \Id}^2}^{p/2} + 3\sqrt{p}\cdot \E\Norm{M \phi(g)}^p\,.
%     \]
%     By \lemmaref{lem:variance}, $\Iprod{M, \phi(g)\transpose{\phi(g)} - \Id}^2 \le O(1)$. 
%     Let us bound $\E\Norm{M \phi(g)}^p$. Consider $M' = M\Paren{\Sigma'}^{-1/2}\Sigma^{1/2}$. By \lemmaref{lem:closeness}, $M'_{ij} \le \Paren{1+O(1/d)} M_{ij}$, hence $\Normf{M'}\le 2$. Note that $\Sigma^{1/2}\Paren{\Sigma'}^{1/2}\phi(g) = g/ R(g)$, 
%     where $R(g) \in [\sqrt{1/2},\sqrt{3/2}]$. Hence
%     \[
%     \E\Norm{M \phi(g)}^p = \E\Iprod{\transpose{M'}M', g\transpose{g}\cdot \rho^2(g) }^{p/2} \le 
%     2 \E\Iprod{\transpose{M'}M', g\transpose{g}}^{p/2}\,.
%     \]
%     Note that $\Tr\Paren{\transpose{M'}M'} = \Normf{M}^2 \le 4$. Let $u_1,\ldots, u_d$ be unit eigenvectors of $\transpose{M'}M'$, and let $\alpha_1,\ldots,\alpha_d$ be its eigenvalues. It follows that
%     \begin{align*}
%     \E\Iprod{\transpose{M'}M', g\transpose{g}}^{p/2} 
%     &\le
%     4^{p/2}\E \Iprod{\sum_{j=1}^d \tfrac{\alpha}{4} u_j \transpose{u_j}, g}^{p/2} 
%     \\&\le
%     2^p \sum_{j=1}^d  \tfrac{\alpha}{4}  \E \Iprod{u_j \transpose{u_j}, g}^{p/2}
%     \\&\le 
%     2^p \max_{j\in[d]}\E \Iprod{u_j, g}^p 
%     \\&\le 
%     2^p \cdot p^{p/2} \,,
%     \end{align*}
%     where we used the convexity of $\Iprod{\cdot, g\transpose{g}}^{p/2}$.
% \end{proof}

