\section{Techniques}
\label{sec:techniques}

Let us recall the assumptions that are used for nearly optimal computationally efficient robust covariance estimation in the Gaussian case (see section 5.2 of \cite{DiakonikolasKK016} or chapter 4 of \cite{DK_book} for the description of the algorithm). First, we need very strong concentration assumptions. In particular, $O(1)$-sub-Gaussianity is not enough, while $O(1)$-Hanson-Wright property is enough for the analysis to work (with $\tilde{O}(d^2/\e^2)$ samples). 
Second, we need the fourth moment of $\Sigma^{-1/2}x$ to coincide with the fourth moment of the standard Gaussian distribution. 

For elliptical distributions we do not have any assumptions on the moments (covariance or even mean might not even exist), and we have to rely only the elliptical structure. Fortunately, this structure allows us to reduce the problem of scatter matrix estimation to a problem of robust covariance estimation of some specific $O(1)$-sub-Gaussian distribution. The properties of this distribution depend on the spectrum of $\Sigma$, and, as we show, the closer $\Sigma$ is to $\Id$, the better (for the robust covariance estimation) these properties are. This allows us to estimate the covariance in several steps that we describe below. 

\paragraph{Spatial Sign.} The distribution reverenced above is the \emph{spatial sign} of an elliptical distribution. It is a projection of an elliptical vector with location zero\footnote{We can always reduce the problem to this case by considering $\paren{x_{i}-x_{\lfloor n/2\rfloor+i}}/\sqrt{2}$. The resulting samples also come from (an $O(\e)$-corruption of) an elliptical distribution with the same scatter matrix.} onto some (arbitrary) sphere centered at $0$. In this paper we use the sphere of radius $\sqrt{d}$, and denote the projection of vector $x\in\R^d$ onto this sphere by $\spsign(x)$. 

If $x$ is an elliptical vector with location $0$ and scatter matrix $\Sigma$, $\spsign(x)$ depends \emph{only} on $\Sigma$, and is the same for \emph{all} elliptical distributions with the same scatter matrix. Indeed, since  $x = \xi A U$ (as in Definition \ref{def:elliptical}), the projection onto the sphere satisfies
\[
 \spsign(x) = \frac{x}{\tfrac{1}{\sqrt{d}}\norm{x}} = \frac{\xi A U}{\tfrac{1}{\sqrt{d}}\norm{\xi A U}} = \frac{A U}{\tfrac{1}{\sqrt{d}}\norm{A U}}\,.
\]
So when we study the properties of $\spsign(x)$, we can simply assume that $x$ comes from our favorite elliptical distribution: $\cN(0,\Sigma)$.

The spatial sign was extensively studied in prior works on elliptical distributions, because $\Sigma' := \Cov_{x\sim \cN(0,\Sigma)} \spsign(x)$ has the same eigenvectors as $\Sigma$, and hence $\Sigma'$ is very useful for the principal (elliptical) component analysis.
However, the eigenvalues of $\Sigma'$ differ from the eigenvalues of $\Sigma$, so even in the classical setting (without corruptions), if we use the empirical covariance of the spatial sign to estimate the eigenvalues of $\Sigma$ (or $\Sigma$ itself), the error of the estimator is not vanishing, even if we take infinitely many samples. 
Fortunately, in robust statistics we anyway have a term that does not depend on the number of samples (it should be at least $\Omega(\e)$ in the Gaussian case). So if we prove that $\Sigma'$ is $\tilde{O}(\e)$-close to $\Sigma$ (in relative Frobenius or at least relative spectral norm), then good robust estimators of $\Sigma'$ are also good robust estimators of $\Sigma$. Since $\Tr(\Sigma') = d$, we fix the scale of $\Sigma$ so that $\Tr(\Sigma) = d$.

Let us discuss how to bound $\norm{{\Sigma}^{-1/2}\, \Sigma'\, {\Sigma}^{-1/2} - \Id}$ (then we also obtain the bound on relative Frobenius norm by multiplying the spectral norm bound by $\sqrt{d}$). Since the eigenvectors of $\Sigma$ and $\Sigma'$ are the same, we can work in the basis where both matrices are diagonal. It follows that
\[
\norm{{\Sigma}^{-1/2}\, \Sigma'\, {\Sigma}^{-1/2} - \Id} = \max_{i\in[d]}\Abs{\frac{\lambda'_i}{\lambda_{i}} - 1} = \max_{i\in[d]}\Abs{\E_{x\sim \cN(0,\Sigma)}{\frac{x_{j}^2/\lambda_j}{\tfrac{1}{d}\norm{x}^2} - 1}}
\]
where $\lambda_i = \Sigma_{ii}$, and $\lambda'_i = \Sigma'_{ii}$.
The Hanson-Wright inequality implies that if $\effrank(\Sigma) \gtrsim \log(d)$, then with high probability for all of the $n = \poly(d)$ samples $x_1,\ldots, x_n\simiid \cN(0,\Sigma)$, 
\[
\norm{x_i}^2 = d\cdot \Paren{1 \pm O\Paren{\sqrt{\frac{\log d}{\effrank(\Sigma)}}}}\,.
\]

Assuming that it holds with probability $1$\footnote{In the formal argument, we analyze not exactly the spatial sign, but some function that with high probability coincides with it on all of the samples, and for this function this statement is true.}, we get a bound
$\norm{{\Sigma}^{-1/2}\, \Sigma'\, {\Sigma}^{-1/2} - \Id} \le O\Paren{\sqrt{\frac{\log d}{\effrank(\Sigma)}}}$. While this bound can be useful for (non-robust) eigenvector estimation\footnote{It was used, in particular, in \cite{ECA}, and it is enough for consistent estimation of the eigenvectors of $\Sigma$.}, for our purposes it is too bad: Even if the effective rank is as large as possible ($\effrank(\Sigma) \ge \Omega(d)$), we only get error $O(\log(d))$ in relative Frobenius norm.

To get a better bound, we get rid of the denominator by expanding it as a series: 
\[
\E\frac{x_{j}^2/\lambda_j}{\tfrac{1}{d}\norm{x}^2} = 
\sum_{k=0}^{\infty} \E{\frac{x_j^2}{\lambda_j} \Paren{1 - \tfrac{1}{d}\norm{x}^2}^k}
=\sum_{k=0}^{\infty} \E{g_j^2 \Paren{1 - \tfrac{1}{d}\sum_{i=1}^{d}\lambda_i g_i^2}^k}
\,,
\]
where $g = \Sigma^{-1/2}x \sim \cN(0,\Id)$. The term that corresponds to $k=0$ is $1$. The term that corresponds to $k=1$ is
\[
\E g_j^2\paren{1 - \tfrac{1}{d}\sum_{i=1}^{d}\lambda_i g_i^2} = 
1 - \tfrac{1}{d}\sum_{i\neq j} \lambda_i - \frac{3\lambda_j}{d} =
1 - \tfrac{1}{d}\sum_{i=1}^d \lambda_i - \frac{2\lambda_j}{d}
= \frac{2\lambda_j}{d} \le O\Paren{\frac{1}{\effrank(\Sigma)}}\,,
\]
where we used $\sum_{i=1}^d \lambda_i = \Tr(\Sigma) = d$. Similarly, the term that corresponds to $k = 2$ is also $O\Paren{\frac{1}{\effrank(\Sigma)}}$, and the other terms are much smaller. Hence we get a bound
\[
\norm{{\Sigma}^{-1/2}\, \Sigma'\, {\Sigma}^{-1/2} - \Id} \le O\Paren{\frac{1}{\effrank(\Sigma)}}\,.
\]
In the case $\effrank(\Sigma)\ge\Omega(d)$, we get $O(1/d)$ error in relative spectral norm, and $O(1/\sqrt{d})$ error in relative Frobenius norm. This is the reason why we require the lower bounds on $\e$ in \cref{thm:main}: We want to make these errors smaller than the robust estimation error $\tilde{O}(\e)$. However, if the effective rank is small, this error is still too large: For example, the error bound in relative Frobenius norm is $\Omega(1)$ if $\effrank(\Sigma) \le \sqrt{d}$. 

Furthermore, if $\effrank(\Sigma) \le o(d)$, the $\paren{d^2\times d^2}$-dimensional covariance of the $d^2$-dimensional random variable $\Paren{\Sigma'}^{-1/2}\spsign(x)\spsign(x)^\top\Paren{\Sigma'}^{-1/2}$ might not even be bounded by $O(1)$ (in spectral norm), which makes robust estimation of $\Sigma'$ with dimension-independent error in  relative Frobenius norm challenging, even if we did not aim to achieve nearly optimal error.

In order to fix these issues, first we estimate $\Sigma'$ up to some small constant error in relative spectral norm. For this, we split the sample into several (more precisely, 3) sub-samples, and for each new estimation we use fresh samples.

\paragraph{First Estimation.} It is not difficult to see that as long as $\effrank(\Sigma)\gtrsim \log(d)$, then $\spsign(x)$ is $O(1)$-sub-Gaussian. By recent result \cite{sos-subgaussian}, the bound on its fourth moment can be certified via a degree-$4$ sum-of-squares proof\footnote{This fact can be also easily verified directly without referring to \cite{sos-subgaussian}.}. Hence we can use the sum-of-squares algorithm from \cite{KS17} that estimates $\Sigma'$ in relative spectral norm up to error $O(\sqrt{\e})$ (this algorithm works if we use $n\gtrsim d^2\log^2(d)/\e^2$ samples). Since $\Sigma'$ is close to $\Sigma$, with high probability we get an estimator $\hat{\Sigma}_1$ such that 
\[
\norm{\hat{\Sigma}_1^{-1/2} \Sigma \hat{\Sigma}_1^{-1/2} - \Id}  \le O\paren{\sqrt{{\e}} + \frac{1}{\effrank(\Sigma)}}\,.
\]
In particular, $0.9 \cdot \Id \preceq \hat{\Sigma}_1^{-1}\Sigma \preceq 1.1\cdot \Id$. 
Hence if we multiply the next subsample by $\hat{\Sigma}_1^{-1}$, we get ($O(\e)$-corruption of) samples from an elliptical distribution with new scatter matrix $\tilde{\Sigma} = \rho \hat{\Sigma}_1^{-1} \Sigma$, where $\rho = d/\Tr(\hat{\Sigma}_1^{-1}\Sigma)$. 

So we can assume that we work with (corrupted) samples from an elliptical  distribution $\cD$ with scatter matrix $\Sigma$ such that $0.9 \cdot \Id \preceq \Sigma \preceq 1.1\cdot \Id$. If we fix the scale $\Tr(\Sigma) = d$, then
$\Sigma' = \Cov_{x\sim \cD}\spsign(x)$ is $O(1/d)$-close to $\Sigma$ in relative spectral norm (since $\effrank(\tilde{\Sigma}) \ge \Omega(d)$).

Now we can try to estimate $\Sigma'$ up to error $\tilde{O}(\e)$. As was previously mentioned, the algorithm for the Gaussian distribution from \cite{DiakonikolasKK016} requires strong assumptions: Hanson-Wright concentration, and the Gaussian fourth moment. 
Fortunately, as long as $\effrank(\Sigma)\ge \Omega(d)$, $\spsign(x)$ satisfies the $O(1)$-Hanson-Wright property. Indeed, it is not hard to see that with overwhelming probability $\Paren{\Sigma'}^{-1/2}\spsign(x) = \Paren{\Sigma'}^{-1/2}\spsign(\Sigma^{1/2} g)$ (where $g\sim \cN(0,\Id)$) coincides with some $O(1)$-Lipschitz function of $g$ (concretely, a composition of linear transformations with with bounded spectral norm, and a function that projects onto the sphere only the points that are close to it, and is linear otherwise). It is known that\footnote{See, for example, \cite{log-sobolev-are-hanson-wright}.}  any $O(1)$-Lipschitz function of a standard Gaussian vector satisfies the $O(1)$-Hanson-Wright property. Therefore, we do not have to worry about the concentration, and can focus on dealing with the fourth moment.

Recall that the covariance filtering algorithm with nearly optimal error uses the $(d^2\times d^2)$-dimensional covariance of the distribution in the isotropic position: $T=\E\Paren{\Cov(y)^{-1/2} yy^\top\Cov(y)^{-1/2} - \Id_d}^{\otimes 2}$. If $y\sim \cN(0,\Sigma)$, then $Q = 2\Id_{d^2}$. However, in our case $y = \spsign(x)$, and the situation is more complicated. This covariance is not only different from $2\Id_{d^2}$, it is also unknown to us, since it depends on $\Sigma$.
We study this dependence and show that the entries $T_{ijij}$ and $T_{iijj}$ are $O\Paren{\Norm{\Sigma - \Id_d}/d + \tilde{O}(1/d^2)}$-close to the entries of  $S := \E\Paren{gg^\top / \norm{g}^2 -\Id_d}^{\otimes 2}$ (and the other entries are zero for both of them). While at the first glance $T$ and $S$ seem to be very close, it is in fact not true: their values (as quadratic forms on unit vectors in $\R^{d^2}$) can differ by $O(\Norm{\Sigma - \Id_d})$. Even if we again use the algorithm from \cite{KS17} and guarantee that $\norm{\Sigma - \Id_d}\le O(\sqrt{\e})$, this bound is only $O(\sqrt{\e})$, while we need it to be $\tilde{O}(\e)$. 
Hence we have to somehow estimate $\Sigma'$ up to error $\tilde{O}(\e)$ in \emph{spectral} norm, before estimating it in  Frobenius norm. 

\paragraph{Second Estimation.}
Let us first describe why the difference between the values of $T$ and $S$ on unit $V\in\R^{d^2}$ can be $O(\Norm{\Sigma - \Id_d})$. The reason is the terms $V_{ii}V_{jj} \Paren{T_{iijj} - S_{iijj}}$ for $i\neq j$. Since $\sum_{i=1}^d V_{ii}$ can be as large as $\sqrt{d}$, the sum of $V_{ii}V_{jj} \Paren{T_{iijj} - S_{iijj}}$ can be as large as $d \cdot \normi{S-T} = O(\Norm{\Sigma - \Id_d})$. 

However, if we only consider unit $d^2$-dimensional vectors of the form $V = uu^\top$, this issue disappears. Indeed, $\sum_{i=1}^d V_{ii}$ is now bounded by $1$, and $T$ is $O(1/d)$-close to $S$ on such vectors. Furthermore, they are both $O(1/d)$-close (as quadratic forms on unit $d^2$-dimensional vectors of the form $uu^\top$) to $2\Id_{d^2}$.

In fact, if we use the covariance filtering algorithm only for such vectors, we get the desired spectral norm bound. However, the optimization problem involved is computationally hard, since we need to optimize over the set $\Set{u^{\otimes 4} \suchthat \norm{u} = 1}$.
Fortunately, we can work with the (canonical) \emph{sum-of-squares relaxation} of this set, that is, with the set of degree-$4$ pseudo-expectations of $v^{\otimes 4}$ that satisfy the constraint $\norm{u}^2 =  1$. 

In order to show that this approach works, we introduce a generalized notion of stability that we use not only for estimation in spectral norm, but also in Frobenius form at the final step. 

\begin{definition}[Generalized Stability]\label{def:stability}
    Let $m\in \N$, $\cV \subseteq \R^m$, $\cP \subseteq \R^{m\times m}$ such that $\cV \otimes \cV \subseteq \cP$, $\mu\in \R^m$ and $Q\in \R^{m\times m}$. Let $\e, \delta, r > 0$ such that $\delta \ge \e$. 
    
    A finite multiset $M$ of points from $\R^m$ is \emph{$\Paren{\e,\delta,r,\cV,\cP}$-stable with respect to $\mu$ and $Q$} if for every $v\in \cV$, every $P\in \cP$, and every $M'\subseteq M$ with $\Card{M'}\ge \paren{1-\e}\Card{M}$, the following three conditions hold:
    \begin{enumerate}
        \item $\Abs{\tfrac{1}{\card{M'}} \sum_{x\in M'} \iprod{v, x-\mu}}\le \delta$,
        \item $\Abs{\tfrac{1}{\card{M'}} \sum_{x\in M'} \iprod{P, \paren{x-\mu}\paren{x-\mu}^\top} - \iprod{P, Q}}\le \delta^2/\e$, and
        \item $\Abs{\iprod{P, Q}} \le r^2$.
    \end{enumerate}
\end{definition}

Note that we use it with $m = d^2$ and apply it to the set $S$ of samples $y_1y_1^\top,\ldots, y_ny_n^\top$, where $y = \spsign(x)$.
For estimation in spectral norm, $\cV$ is $\Set{uu^\top \suchthat \norm{v} = 1}$, $\cP$ is the set of $\pE u^{\otimes 4}$ described above, $Q = 2\Id_{d^2}$, and $r = \sqrt{2}$ (and, as in the standard filtering algorithm, $\delta = O(\e\log(1/\e)$). 

As previously mentioned, if an isotropic distribution $\cY$ satisfies the Hanson-Wright property, then it can be shown that the set of iid samples drawn from this distribution is $(\e,O(\e\log(1/\e)), O(1), \cB, \Conv(\cB\otimes \cB))$-stable with respect tp $\mu = \Id_d$ and $Q = \E_{y\sim \cY}\Paren{yy^\top - \Id_d}^{\otimes 2}$ with high probability, where $\cB = \{V\in \R^{d^2} : \normf{V} = 1\}$. Since our $\cV$ is a subset of $\cB$, and our $\cP$ is a subset of $\Conv(\cB\otimes \cB)$, we also get the stability for $\cV$ and $\cP$.

The filtering algorithm works in a similar way to the Gaussian case: It assigns weights to the samples, in the case of the covariance estimation transforms them\footnote{For the covariance filtering the samples are transformed via linear transformation $y_iy_i^\top \mapsto C^{-1/2}y_iy_i^\top C^{-1/2}$, where $C$ is the current candidate for the covariance estimator.}, and checks whether the value $\lambda := \max_{P\in\cP}\Abs{\iprod{P, Q-\hat{Q}^{(t)}}}$  is too large
(where $\hat{Q}^{(t)}$ is the weighted empirical $m\times m$-dimensional covariance of the (transformed) samples). 
In particular, it requires $Q$ (but, of course, not $\mu$) to be known.
If $\lambda$ is large, it reassigns the weights, so that the weights of the samples that made it large decrease. In the end $\lambda$ should be small, and if it is small, then the stability guarantees that the current weighted sample mean is close to $\mu$.
Note that since we can optimize linear functions over $\cP$ efficiently, the algorithm runs in polynomial time.

This notion of stability, however, is not enough for the filtering algorithm to work. Apart from the condition that  the set of samples is stable at each iteration\footnote{The transformed set needs to be stable for certain $\delta' > \delta$. This condition also appears in the Gaussian setting, and for our case can be shown in a similar way.} of the algorithm, we need some additional conditions that $\cP$ is in certain sense not much larger than $\cV\otimes \cV$. Concretely, for all $A,B\in \R^{d^2}$ and $P\in \cP$, we need to show that
\[
\Iprod{A\otimes B, P}^2 \le \Paren{\sup_{V\in \cV}\Iprod{A, V}^2}\cdot\Paren{\sup_{V\in \cV}\Iprod{B, V}^2}\,.
\]
and $\Iprod{A\otimes A, P} \ge 0$.
For $P = v^{\otimes 4}$, it is possible to show that these inequalities can be certified via degree-$4$ sum-of-squares proofs, so $\cP$ satisfies the desired properties. 

Thus, we show that the filtering algorithm for spectral covariance estimation finds $\hat{\Sigma}_2$ such that 
$\norm{\hat{\Sigma}_2^{-1/2} \Sigma \hat{\Sigma}_2^{-1/2} - \Id} \le O\Paren{\e\log(1/\e)}$. After we multiply fresh samples by $\hat{\Sigma}_2^{-1/2}$, we can assume that we work with a sample from an elliptical distribution with scatter matrix $\Sigma$ such that $\Norm{\Sigma - \Id} \le O(\e\log(1/\e))$.

\paragraph{Final Estimation.}
To estimate $\Sigma$ in Frobenius norm, we again use the stability-based filtering. We use it with $Q = S$ (recall that $S := \E\Paren{gg^\top / \norm{g}^2 -\Id_d}^{\otimes 2}$).
Since $\Sigma$ is very close to $\Id_d$, the situation is simpler than before: we show the stability of the \emph{non-transformed} samples (with the same $\delta = O(\e\log(1/\e)$), and then simply apply filtering without transformations (in other words, this filtering algorithm is oblivious to the matrix structure in $\R^{d^2}$).  This algorithm finds $\hat{\Sigma}_3$ such that $\normf{\hat{\Sigma}_3^{-1/2} \Sigma \hat{\Sigma}_3^{-1/2} - \Id} \le O\Paren{\e\log(1/\e)}$. 

Now let us use the notation $\Sigma$ for the original scatter matrix with $\Tr(\Sigma) = d$, 
$\Sigma_1 = \rho_1\hat{\Sigma}_1^{-1/2}\Sigma\hat{\Sigma}_1^{-1/2}$, 
$\Sigma_2 = \rho_2\hat{\Sigma}_2^{-1/2}\Sigma_1\hat{\Sigma}_2^{-1/2}$, where $\rho_1$ and $\rho_2$ are chosen so that $\Tr(\Sigma_1) = \Tr(\Sigma_2) = d$. Then $\hat{\Sigma} := \hat{\Sigma}_1 \hat{\Sigma}_2 \hat{\Sigma}_3$
satisfies
\[
\normf{\hat{\Sigma}^{-1/2} \Paren{\rho_1\rho_2\Sigma} \hat{\Sigma}^{-1/2}  - \Id}\le O(\e\log(1/\e))\,.
\]
So $\hat{\Sigma}$ is the desired estimator of the scatter matrix $\rho_1\rho_2\Sigma$.



%\subsection{Robust Covariance Estimation under Moment Assumptions}

