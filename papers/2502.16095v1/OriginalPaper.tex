%% 
%% Copyright 2007-2025 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.3 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.3 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%% SP 2008/03/01
%% $Id: elsarticle-template-num.tex 272 2025-01-09 17:36:26Z rishi $
%%
\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[colorlinks,urlcolor=blue,linkcolor=blue,citecolor=blue]{hyperref}
\usepackage{amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{color, array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{tabularx}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{cleveref}
\usepackage{geometry}
\usepackage{multirow}
\usepackage{array}
\usepackage{arydshln}
\graphicspath{{./}}
\renewcommand{\arraystretch}{1.25}
\definecolor{bluebox}{rgb}{0.000, 0.000, 1.000}   % Blue
\definecolor{greenbox}{rgb}{0.000, 1.000, 0.000}  % Green
\definecolor{redbox}{rgb}{1.000, 0.000, 0.000}    % Red

\journal{Pattern Recognition}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \affiliation for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%% \fntext[label3]{}

\title{Good Representation, Better Explanation: Role of Convolutional Neural Networks in Transformer-Based Remote Sensing Image Captioning}

\author[label1]{Swadhin Das} %% Author name
\author[label2]{Saarthak Gupta}
\author[label3]{Kamal Kumar}
\author[label1]{Raksha Sharma}
%% Author affiliation
\affiliation[label1]{organization={Department of Computer Science and Engineering;\\ Indian Institute of Technology, Roorkee},%Department and Organization
            addressline={Roorkee}, 
            city={Haridwar},
            postcode={247667}, 
            state={Uttarakhand},
            country={India}}
\affiliation[label2]{organization={Department of Mechanical Engineering;\\ Indian Institute of Technology,  BHU},%Department and Organization
            addressline={Varanasi}, 
            city={Varanasi},
            postcode={221005}, 
            state={Uttar Pradesh},
            country={India}}
\affiliation[label3]{organization={Department of Information Technology;\\ National Institute of Technology, Srinagar},%Department and Organization
    addressline={Srinagar}, 
    city={Srinagar},
    postcode={190006}, 
    state={Jammu and Kashmir},
    country={India}}

%% Abstract
\begin{abstract}
Remote Sensing Image Captioning (RSIC) is the process of generating meaningful descriptions from remote sensing images. Recently, it has gained significant attention, with encoder-decoder models serving as the backbone for generating meaningful captions. The encoder extracts essential visual features from the input image, transforming them into a compact representation, while the decoder utilizes this representation to generate coherent textual descriptions. Recently, transformer-based models have gained significant popularity due to their ability to capture long-range dependencies and contextual information. The decoder has been well explored for text generation, whereas the encoder remains relatively unexplored. However, optimizing the encoder is crucial as it directly influences the richness of extracted features, which in turn affects the quality of generated captions. To address this gap, we systematically evaluate twelve different convolutional neural network (CNN) architectures within a transformer-based encoder framework to assess their effectiveness in RSIC. The evaluation consists of two stages: first, a numerical analysis categorizes CNNs into different clusters, based on their performance. The best performing CNNs are then subjected to human evaluation from a human-centric perspective by a human annotator. Additionally, we analyze the impact of different search strategies, namely greedy search and beam search, to ensure the best caption. The results highlight the critical role of encoder selection in improving captioning performance, demonstrating that specific CNN architectures significantly enhance the quality of generated descriptions for remote sensing images. By providing a detailed comparison of multiple encoders, this study offers valuable insights to guide advances in transformer-based image captioning models.
\end{abstract}

%%Graphical abstract
% \begin{graphicalabstract}
% \includegraphics[width=\textwidth,height=450px]{graphical_abstract.png}
% \end{graphicalabstract}

%% Keywords
\begin{keyword}
Remote Sensing Image Captioning (RSIC) \sep Transformer-based Encoder \sep Transformer-based Decoder \sep Subjective Evaluation \sep K-Means Clustering

\end{keyword}

\end{frontmatter}

\section{Introduction}
With the advancement of remote sensing technologies and machine learning-based methods, the demand for Remote Sensing Image Captioning (RSIC)~\cite{lu2017exploring,das2024textgcn} is growing rapidly. It plays a crucial role in various fields, including environmental monitoring, urban planning, and disaster management, by providing automated textual descriptions of satellite images.  Unlike traditional machine learning tasks such as image classification, RSIC requires not only identifying objects in an image but also describing their relationships, spatial context, and scene composition in a meaningful way. While classification assigns predefined labels to images, RSIC generates natural language descriptions that capture fine-grained details, making it a more complex and context-dependent task. Since RSIC is highly domain specific, the quality of generated captions is heavily based on understanding the unique properties of remote sensing images. These images often contain diverse landscapes, multi-scale objects, and varying lighting conditions, requiring careful feature extraction and interpretation to produce accurate and informative captions.

The encoder-decoder-based model~\cite{qu2016deep} is one of the most widely used approaches in RSIC, where a suitable encoder (preferably a CNN) extracts visual features from the image and a decoder (such as RNN, LSTM, or GRU) generates captions based on the encoded representation. Although this approach produces reasonable results, it has several limitations. Traditional sequential models struggle with long-range dependencies, leading to issues such as loss of important contextual information, poor generalization of unseen images, and difficulty in capturing complex spatial relationships in remote sensing images. Furthermore, due to the high variability in satellite imagery, these models often do not generate semantically rich and contextually accurate captions. To address these challenges, attention-based models~\cite{zhang2023multi,li2021recurrent} were introduced, significantly improving RSIC performance. In this approach, the decoder selectively focuses on the relevant regions of the image while generating each word in the caption, ensuring better alignment between visual features and textual descriptions. This technique allows models to dynamically adjust their focus, improving the ability to describe objects, their relationships, and fine-grained scene details. As a result, many researchers adopted attention mechanisms in RSIC, achieving more accurate and contextually relevant captions. However, a major breakthrough came with the introduction of transformers, which use multi-head self-attention to process the entire input sequence simultaneously, capturing both local and global dependencies more effectively. With the success of transformers~\cite{vaswani2017attention} in natural language processing and vision tasks, RSIC research also started to shift toward transformer-based architectures, leveraging their ability to model complex relationships in remote sensing images. Despite these advancements, transformer-based RSIC~\cite{wu2024trtr} is still a relatively new concept and only a limited number of studies have explored its full potential. There remains significant scope for improving encoder design, attention mechanisms, and integration strategies to further enhance the accuracy and informativeness of generated captions.

The existing methods~\cite{hoxha2021novel,hoxha2020new,zhang2019multi} are well developed and produce strong results. However, most of the research has focused primarily on improving the decoder to generate high-quality captions, while the role of the encoder remains relatively underexplored. In RSIC, the encoder plays a crucial role, as the selection of an efficient encoder can significantly enhance the overall performance of the system. CNNs continue to be the preferred choice for image encoding, making it essential to thoroughly understand their effectiveness in this task. Given the diverse properties of different CNN architectures, a thorough evaluation is necessary to assess their effectiveness for RSIC. A systematic analysis of different CNNs as encoders will provide deeper insight into their impact on captioning performance, ultimately contributing to the generation of more accurate and informative descriptions.

To overcome the above challenges, we examined twelve different CNNs within a transformer-based image encoder framework for the RSIC. We analyze these widely used CNN architectures, evaluating their ability to extract meaningful visual features that enhance caption generation. To assess their behavior, we performed experiments from multiple perspectives. First, a numerical analysis was performed, aggregating CNNs into three groups:~\emph{Good},~\emph{Medium}, and~\emph{Bad}. The~\emph{Good} CNNs were further evaluated by a human annotator to assess the quality of their generated captions from a human perspective. The contributions of this paper are as follows.  
\begin{itemize}  
    \item We conducted a thorough numerical evaluation of CNNs and identified the top performing architectures.  
    \item We performed a subjective evaluation of the selected CNNs.    
    \item We strengthened the analysis by conducting a classification task using CNNs in our image dataset.    
    \item We demonstrated the impact of multi-head transformers in RSIC through ablation studies.  
\end{itemize} 
The rest of the paper is organized as follows:~\Cref{sec_relwork} reviews related work, discussing previous research on remote sensing image captioning and the role of different encoder-decoder architectures.~\Cref{sec_methodology} details the proposed methodology, including the integration of CNN-based feature extraction within the transformer-based encoder framework.~\Cref{sec_CNN} presents an overview of the CNN architectures evaluated in this study.~\Cref{sec_exp_setup} describes the experimental setup, including datasets and evaluation metrics.~\Cref{sec_experimental} provides an in-depth analysis of numerical results, subjective evaluations, and error patterns. Finally,~\Cref{sec_conclusion} concludes the study with key findings and future research directions.
\section{Related Work}
\label{sec_relwork}
Remote Sensing Image Captioning (RSIC) has advanced significantly with deep learning techniques. Traditional methods relied on handcrafted feature extraction and statistical models, but the introduction of encoder-decoder architectures revolutionized the field~\cite{qu2016deep,hoxha2021novel,li2019vision}. The encoder extracts meaningful features from satellite and aerial imagery, while the decoder generates descriptive captions. 

Early RSIC models used convolutional and recurrent neural networks. Qu et al.,~[2016]~\cite{qu2016deep} proposed a deep multimodal neural network, integrating image and text embeddings for caption generation. Lu et al.,~[2017]~\cite{lu2017exploring} contributed by introducing the RSICD dataset, addressing dataset limitations in SYDNEY and UCM. Hoxha et al.,~[2019]~\cite{hoxha2020new,hoxha2020toward} refined beam search strategies and image retrieval techniques using CNN-RNN models for better caption generation accuracy. Li et al.,~[2019]~\cite{li2019vision} introduced a two-level attention mechanism that combines text-guided and semantic-guided attention to improve spatial correlation. Zhang et al.,~[2019]~\cite{zhang2019multi} developed a multiscale cropping approach to improve fine-grained feature extraction.

To overcome the limitations of earlier methods, attention-based mechanisms were introduced to improve contextual modeling. Xu et al.,~[2015]~\cite{xu2015show} pioneered attention-based image captioning, influencing subsequent RSIC works. Sumbul et al.,~[2020]~\cite{sumbul2020sd} explored summarization-driven attention to improve caption coherence. Despite improvements, these models struggled with long-term dependencies and feature aggregation inefficiencies. Li et al.,~[2021]~\cite{li2021recurrent} proposed the RASG framework, integrating recurrent attention to refine context vectors. Wang et al.,~[2022]~\cite{wang2022glcm} introduced GLCM, using global-local feature extraction for better word discrimination.

Transformer-based architectures brought about a paradigm shift in RSIC by enhancing sequence modeling and linguistic consistency~\cite{vaswani2017attention}. Liu et al.,~[2022]~\cite{liu2022remote} introduced MLAT, a multilayer aggregated transformer that improves the utilization of multiscale features. Meng et al.,~[2023]~\cite{meng2023prior} developed the PKG transformer, incorporating graph neural networks to refine the relationship between objects. Zhang et al.,~[2023]~\cite{zhang2023multi} introduced a stair attention mechanism, integrating CIDEr-based reinforcement learning for improved coherence. Despite these advancements, efficient encoder selection remained an open challenge. Wu et al.,~\cite{wu2024trtr} proposed a dual transformer model integrating the Swin transformer for multiscale feature extraction.

A significant limitation is identified when evaluating these models. They predominantly emphasize the decoder instead of the encoder. However, selecting an effective encoder can substantially enhance performance in the RSIC. Taking into account this aspect, Das et al.,~[2024]~\cite{das2024unveiling} conducted an investigation involving eight CNN-based encoders for RSIC. Their study used an LSTM as the decoder. The encoders were grouped on the basis of performance to determine the highest performing CNNs. The top performing CNNs were subsequently subjected to a subjective evaluation to determine the optimal choice. Their findings recognized ResNet as the most effective encoder. However, transformer-based models have garnered considerable attention in recent years. Recognizing these advances, this work conducts a similar experiment using twelve distinct encoders. It incorporates a transformer-based encoder along with a GPT-2 transformer-based decoder.
\section{Methodology}
\label{sec_methodology}
\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.95\textwidth]{model.png}
    \caption{Architecture of the model}
    \label{architecture}
\end{figure*}
In this work, we utilize an encoder-decoder-based architecture to generate captions from remote sensing images. We incorporate a multi-head self-attention transformer layer after the CNN layer. First, an image is passed through a CNN layer, and the output is permuted into the shape (batch size, patch size, feature size). Then, we apply a convolution layer to standardize the~\emph{feature size}, as different CNN architectures produce outputs of varying dimensions. Following this, we add~\emph{patch embeddings} which are necessary because they allow the model to process images in smaller regions, thereby capturing local and global features more effectively while maintaining positional information. Unlike classification-based models, we do not use a~\emph{CLS token}, as it is unnecessary for our captioning task. Then, we pass the processed features through a self-attention-based transformer layer in a repetitive manner. Finally, the transformed features are passed to the transformer-based decoder layer.  

In our approach, we employ GPT-2 as the transformer-based decoder, enhanced with cross-attention mechanisms to generate captions from remote sensing images. GPT-2 is a substantial transformer-based language model trained on a diverse corpus of text data, excelling at producing coherent and contextually relevant text sequences, making it well-suited for captioning tasks. Within GPT-2, the self-attention mechanism enables the model to consider all positions in the input sequence simultaneously when making predictions, effectively capturing long-range dependencies and relationships between elements. To prevent the model from attending to future tokens during training or generation, masked self-attention is employed. This ensures that the model focuses only on the current and previous tokens, maintaining the autoregressive nature of text generation. In our work, we integrate a cross-attention layer that aligns the encoded image features with the text generation process. This mechanism allows the decoder to focus on relevant visual information while predicting each token, effectively bridging the gap between visual and textual modalities. The cross-attention mechanism operates by using the decoder's queries to attend to the encoder's key-value pairs, enabling the model to incorporate pertinent image features into the caption generation process. By leveraging GPT-2's pre-trained language understanding capabilities and incorporating both masked self-attention and cross-attention mechanisms, our model generates semantically meaningful captions that accurately describe remote sensing images.  

Each sentence is appended with a start token \(\langle s\rangle\) and an end token \(\langle/s\rangle\) to mark the beginning and end of a sentence. We use GPT2Tokenizer (fast version) to tokenize the sentences. Both the tokenizer and the decoder model are fine-tuned on the training caption dataset, as the captions are highly domain-specific. Fine-tuning ensures that the model learns the specialized vocabulary and structure required to accurately describe remote sensing images.  

Figure~\ref{architecture} illustrates the workflow of our model. The training phase differs significantly from the RNN- or LSTM-based decoders. Here, we pass the entire sequence of sentence tokens, including the start token but excluding the end token, and compare the output with the sequence excluding the start token. The main objective is to train the model to predict the next token for each input token.  

The sentence generation process follows a straightforward approach. It begins with the start token as the input sequence. The image feature extraction process remains the same. Using these two inputs, the model predicts the next word and appends it to the input sequence. This iterative process continues until the model generates the end token.
\section{Types of CNN}
\label{sec_CNN}
\begin{table*}[!ht]
\centering
\caption{Results of Various CNNs on the  SYDNEY Dataset with Greedy Search}
\label{SYDNEY_greedy}
\resizebox{0.98\textwidth}{!}{
\begin{tabular}{|ccccccccc|}
\hline
CNN & BLEU-1 & BLEU-2 & BLEU-3 & BLEU-4 & METEOR & ROUGE-L & CIDEr & Cluster\\
\hline
ResNet & 0.7480 & 0.6484 & 0.5719 & 0.5039 & 0.3630 & 0.6723 & 2.0973 & Good\\
Wide ResNet & 0.7131 & 0.6295 & 0.5621 & 0.5063 & 0.3813 & 0.6636 & 2.1904 & Medium\\
ResNext & 0.7609 & 0.6658 & 0.5815 & 0.5078 & 0.3760 & 0.6852 & 2.1877 & Good\\
RegNet & 0.7105 & 0.6087 & 0.5367 & 0.4778 & 0.3492 & 0.6395 & 1.8811 & Bad\\
VGGNet & 0.7118 & 0.6264 & 0.5592 & 0.4991 & 0.3744 & 0.6739 & 2.0582 & Medium\\
DenseNet & 0.7250 & 0.6331 & 0.5597 & 0.4977 & 0.3548 & 0.6527 & 1.8792 & Bad\\
AlexNet & 0.7239 & 0.6314 & 0.5517 & 0.4881 & 0.3569 & 0.6738 & 2.2170 & Medium\\
GoogleNet & 0.7391 & 0.6419 & 0.5634 & 0.4971 & 0.3552 & 0.6681 & 2.0574 & Medium\\
InceptionNet & 0.7567 & 0.6667 & 0.5926 & 0.5299 & 0.3760 & 0.6767 & 2.1791 & Good\\
MobileNetV2 & 0.6986 & 0.5919 & 0.5161 & 0.4528 & 0.3328 & 0.6227 & 1.7561 & Bad\\
MobileNetV3 & 0.7461 & 0.6528 & 0.5769 & 0.5122 & 0.3465 & 0.6623 & 2.0603 & Medium\\
ConvNext & 0.7997 & 0.6844 & 0.6325 & 0.5694 & 0.4073 & 0.7349 & 2.4945 & Good\\
\hline
\end{tabular}}
\end{table*}
\begin{table*}[!ht]
\centering
\caption{Results of Various CNNs on the  SYDNEY Dataset with Beam Search}
\label{SYDNEY_beam}
\resizebox{0.98\textwidth}{!}{
\begin{tabular}{|ccccccccc|}
\hline
CNN & BLEU-1 & BLEU-2 & BLEU-3 & BLEU-4 & METEOR & ROUGE-L & CIDEr & Cluster\\
\hline
ResNet & 0.7578 & 0.6663 & 0.5906 & 0.5228 & 0.3729 & 0.6883 & 2.1498 & Good\\
Wide ResNet & 0.7202 & 0.6385 & 0.5721 & 0.5139 & 0.3635 & 0.6380 & 2.1634 & Medium\\
ResNext & 0.7298 & 0.6386 & 0.5628 & 0.4964 & 0.3589 & 0.6559 & 2.1403 & Medium\\
RegNet & 0.7060 & 0.6125 & 0.5408 & 0.4781 & 0.3490 & 0.6422 & 1.9920 & Bad\\
VGGNet & 0.7267 & 0.6387 & 0.5745 & 0.5206 & 0.3796 & 0.6860 & 2.3559 & Good\\
DenseNet & 0.7359 & 0.6465 & 0.5730 & 0.5107 & 0.3557 & 0.6706 & 2.0070 & Medium\\
AlexNet & 0.6879 & 0.6027 & 0.5315 & 0.4786 & 0.3266 & 0.6174 & 2.1447 & Bad\\
GoogleNet & 0.7204 & 0.6293 & 0.5515 & 0.4855 & 0.3627 & 0.6570 & 2.0371 & Medium\\
InceptionNet & 0.7384 & 0.6538 & 0.5859 & 0.5281 & 0.3610 & 0.6654 & 2.2171 & Good\\
MobileNetV2 & 0.7044 & 0.6039 & 0.5284 & 0.4647 & 0.3419 & 0.6395 & 1.7789 & Bad\\
MobileNetV3 & 0.7055 & 0.6139 & 0.5417 & 0.4789 & 0.3490 & 0.6451 & 1.9865 & Bad\\
ConvNext & 0.7490 & 0.6582 & 0.5831 & 0.5187 & 0.3807 & 0.6770 & 2.2202 & Good\\
\hline
\end{tabular}}
\end{table*}
\begin{table*}[!ht]
\centering
\caption{Results of Various CNNs on the  UCM Dataset with Greedy Search}
\label{UCM_greedy}
\resizebox{0.98\textwidth}{!}{
\begin{tabular}{|ccccccccc|}
\hline
CNN & BLEU-1 & BLEU-2 & BLEU-3 & BLEU-4 & METEOR & ROUGE-L & CIDEr & Cluster\\
\hline
ResNet & 0.8350 & 0.7686 & 0.7143 & 0.6658 & 0.4425 & 0.7822 & 3.4078 & Good\\
Wide ResNet & 0.8511 & 0.7856 & 0.7290 & 0.6777 & 0.4501 & 0.8071 & 3.5221 & Good\\
ResNext & 0.8210 & 0.7491 & 0.6905 & 0.6418 & 0.4332 & 0.7704 & 3.3276 & Medium\\
RegNet & 0.8079 & 0.7347 & 0.6747 & 0.6199 & 0.4326 & 0.7777 & 3.2392 & Medium\\
VGGNet & 0.8034 & 0.7286 & 0.6748 & 0.6321 & 0.4266 & 0.7585 & 3.2563 & Medium\\
DenseNet & 0.8199 & 0.7559 & 0.7032 & 0.6555 & 0.4565 & 0.7905 & 3.4760 & Good\\
AlexNet & 0.7785 & 0.6951 & 0.6327 & 0.5820 & 0.4024 & 0.7314 & 3.0124 & Bad\\
GoogleNet & 0.7855 & 0.7095 & 0.6516 & 0.5995 & 0.4161 & 0.7468 & 3.0661 & Bad\\
InceptionNet & 0.8338 & 0.7552 & 0.6919 & 0.6365 & 0.4477 & 0.8003 & 3.3445 & Medium\\
MobileNetV2 & 0.8203 & 0.7460 & 0.6884 & 0.6364 & 0.4291 & 0.7642 & 3.3664 & Medium\\
MobileNetV3 & 0.8088 & 0.7371 & 0.6825 & 0.6327 & 0.4208 & 0.7658 & 3.2499 & Medium\\
ConvNext & 0.8369 & 0.7712 & 0.7143 & 0.6612 & 0.4566 & 0.8119 & 3.4582 & Good\\
\hline
\end{tabular}}
\end{table*}
\begin{table*}[!ht]
\centering
\caption{Results of Various CNNs on the  UCM Dataset with Beam Search}
\label{UCM_beam}
\resizebox{0.98\textwidth}{!}{
\begin{tabular}{|ccccccccc|}
\hline
CNN & BLEU-1 & BLEU-2 & BLEU-3 & BLEU-4 & METEOR & ROUGE-L & CIDEr & Cluster\\
\hline
ResNet & 0.8434 & 0.7795 & 0.7259 & 0.6770 & 0.4523 & 0.7930 & 3.4061 & Good\\
Wide ResNet & 0.8459 & 0.7874 & 0.7339 & 0.6850 & 0.4591 & 0.8060 & 3.4676 & Good\\
ResNext & 0.8026 & 0.7312 & 0.6729 & 0.6229 & 0.4213 & 0.7564 & 3.1233 & Bad\\
RegNet & 0.8339 & 0.7662 & 0.7111 & 0.6604 & 0.4408 & 0.7809 & 3.2729 & Medium\\
VGGNet & 0.8054 & 0.7343 & 0.6775 & 0.6297 & 0.4389 & 0.7711 & 3.2628 & Bad\\
DenseNet & 0.8330 & 0.7741 & 0.7238 & 0.6767 & 0.4623 & 0.7983 & 3.4211 & Good\\
AlexNet & 0.8003 & 0.7240 & 0.6653 & 0.6158 & 0.4174 & 0.7401 & 3.0186 & Bad\\
GoogleNet & 0.7973 & 0.7285 & 0.6735 & 0.6243 & 0.4276 & 0.7573 & 3.0818 & Bad\\
InceptionNet & 0.8420 & 0.7663 & 0.7063 & 0.6527 & 0.4566 & 0.7988 & 3.3176 & Medium\\
MobileNetV2 & 0.8252 & 0.7603 & 0.7064 & 0.6563 & 0.4346 & 0.7655 & 3.3252 & Medium\\
MobileNetV3 & 0.8281 & 0.7609 & 0.7077 & 0.6590 & 0.4466 & 0.7829 & 3.2713 & Medium\\
ConvNext & 0.8442 & 0.7802 & 0.7319 & 0.6806 & 0.4602 & 0.8126 & 3.4953 & Good\\
\hline
\end{tabular}}
\end{table*}
Our work incorporates twelve widely recognized CNN architectures within the transformer-based encoder framework. Although there are multiple variants for each CNN type, we selected the most recent versions to maintain a fair and consistent comparison.
\begin{enumerate}
    \item\textbf{ResNet~\cite{he2016deep}:} ResNet addresses the problem of vanishing gradients in deep neural networks by implementing residual connections. These connections allow models to learn identity mappings, facilitating stable gradient flow and efficient feature learning. Comprising multiple residual blocks, ResNet enables the training of very deep networks without performance degradation. We used ResNet with $152$ layers in our work.
    \item\textbf{Wide ResNet~\cite{zagoruyko2016wide}:} Wide ResNet (WRN) is a variant of ResNet developed through an experimental study of its architecture. It enhances training efficiency and performance by reducing the depth while increasing the width of residual networks. This modification enhances feature representation and mitigates the degradation problem in deep networks. 
    \item\textbf{ResNext~\cite{xie2017aggregated}:} ResNext is a deep learning architecture that expands on ResNet by introducing~\emph{cardinality}, the number of transformation sets in each block. It uses a simple, repeatable multibranch design with minimal hyperparameters, enhancing model flexibility and performance. This approach improves feature learning while maintaining efficiency.
    \item\textbf{RegNet~\cite{radosavovic2020designing}:} RegNet defines a design space of simple, regular networks with quantized linear parameterization of widths and depths, allowing for scalable and efficient models. This approach optimizes performance across different computational budgets, balancing accuracy and efficiency.
    \item\textbf{VGGNet~\cite{simonyan2014very}:} VGGNet is one of the earliest and most widely used CNNs for tasks like image captioning. It is known for its substantial number of parameters. We used VGGNet with $19$ layers in our work.
    \item\textbf{DenseNet~\cite{huang2017densely}:} DenseNet connects each layer to every other layer in a feed-forward fashion. This dense connectivity improves the flow of the gradient and facilitates better feature reuse. We used DenseNet with $201$ layers in our work.
    \item\textbf{AlexNet~\cite{krizhevsky2012imagenet}:} AlexNet was among the first deep CNNs and is capable of performing parallel processing using two GPUs.
    \item\textbf{GoogleNet~\cite{szegedy2015going}:} GoogleNet is the first version of InceptionNet. Using inception modules, it integrates multiple parallel convolutional and pooling operations with varying kernel sizes.
    \item\textbf{InceptionNet~\cite{szegedy2016rethinking}:} InceptionNet is an advanced version of GoogleNet. Improves the architecture. For our research, we used InceptionNet version $3$ in our work.
    \item\textbf{MobileNetV2~\cite{sandler2018MobileNetV2}:} MobileNetV2 introduces a mobile architecture that improves performance across various tasks and model sizes. It uses an inverted residual structure with lightweight depthwise convolutions and eliminates non-linearities in narrow layers to enhance representational power. 
    \item\textbf{MobileNetV3~\cite{howard2019searching}:} MobileNetV3 improves on previous MobileNet architectures by using hardware-aware network architecture search (NAS) and the NetAdapt algorithm, optimizing models for mobile devices. With two variants, MobileNetV3-Large and MobileNetV3-Small, the architecture achieves superior performance in classification, detection, and segmentation tasks, offering higher accuracy and reduced latency compared to MobileNetV2. We used MobileNetV3 of a large variant in our work.
    \item\textbf{ConvNext~\cite{liu2022convnet}:} ConvNext is an advanced convolutional neural network (CNN) that adopts key design principles of vision transformers. By integrating techniques such as depthwise convolutions and layer normalization, it enhances traditional CNN architectures, offering better performance on image classification tasks while maintaining computational efficiency. This approach positions ConvNext as a strong alternative to vision transformers.
\end{enumerate}
\section{Experimental Setup}
\label{sec_exp_setup}
\begin{table*}[!ht]
\centering
\caption{Results of Various CNNs on the  RSICD Dataset with Greedy Search}
\resizebox{0.98\textwidth}{!}{
\label{RSICD_greedy}
\begin{tabular}{|ccccccccc|}
\hline
CNN & BLEU-1 & BLEU-2 & BLEU-3 & BLEU-4 & METEOR & ROUGE-L & CIDEr & Cluster\\
\hline
ResNet & 0.6295 & 0.4607 & 0.3574 & 0.2878 & 0.2535 & 0.4671 & 0.8085 & Good\\
Wide ResNet & 0.6159 & 0.4451 & 0.3446 & 0.2788 & 0.2439 & 0.4551 & 0.7972 & Medium\\
ResNext & 0.6351 & 0.4579 & 0.3507 & 0.2811 & 0.2550 & 0.4703 & 0.8061 & Good\\
RegNet & 0.6231 & 0.4478 & 0.3439 & 0.2746 & 0.2462 & 0.4617 & 0.7971 & Good\\
VGGNet & 0.6111 & 0.4308 & 0.3252 & 0.2576 & 0.2390 & 0.4434 & 0.7321 & Bad\\
DenseNet & 0.6142 & 0.4376 & 0.3310 & 0.2603 & 0.2506 & 0.4552 & 0.7614 & Medium\\
AlexNet & 0.6046 & 0.4273 & 0.3227 & 0.2555 & 0.2345 & 0.4376 & 0.7092 & Bad\\
GoogleNet & 0.6213 & 0.4460 & 0.3417 & 0.2727 & 0.2445 & 0.4586 & 0.7787 & Medium\\
InceptionNet & 0.6146 & 0.4409 & 0.3370 & 0.2692 & 0.2443 & 0.4599 & 0.7839 & Medium\\
MobileNetV2 & 0.6174 & 0.4460 & 0.3416 & 0.2729 & 0.2471 & 0.4551 & 0.7618 & Medium\\
MobileNetV3 & 0.6209 & 0.4438 & 0.3386 & 0.2709 & 0.2454 & 0.4572 & 0.7706 & Medium\\
ConvNext & 0.6431 & 0.4665 & 0.3602 & 0.3013 & 0.2560 & 0.4945 & 0.8415 & Good\\
\hline
\end{tabular}}
\end{table*}
\begin{table*}[!ht]
\centering
\caption{Results of Various CNNs on the  RSICD Dataset with Beam Search}
\resizebox{0.98\textwidth}{!}{
\label{RSICD_beam}
\begin{tabular}{|ccccccccc|}
\hline
CNN & BLEU-1 & BLEU-2 & BLEU-3 & BLEU-4 & METEOR & ROUGE-L & CIDEr & Cluster\\
\hline
ResNet & 0.6314 & 0.4661 & 0.3642 & 0.2949 & 0.2548 & 0.4711 & 0.8243 & Good\\
Wide ResNet & 0.6162 & 0.4442 & 0.3428 & 0.2767 & 0.2436 & 0.4516 & 0.8070 & Bad\\
ResNext & 0.6310 & 0.4514 & 0.3431 & 0.2715 & 0.2531 & 0.4616 & 0.7971 & Medium\\
RegNet & 0.6182 & 0.4455 & 0.3429 & 0.2739 & 0.2462 & 0.4578 & 0.8112 & Bad\\
VGGNet & 0.6173 & 0.4449 & 0.3410 & 0.2731 & 0.2459 & 0.4519 & 0.7752 & Bad\\
DenseNet & 0.6264 & 0.4505 & 0.3450 & 0.2736 & 0.2588 & 0.4663 & 0.8020 & Medium\\
AlexNet & 0.6176 & 0.4396 & 0.3369 & 0.2697 & 0.2414 & 0.4472 & 0.7470 & Bad\\
GoogleNet & 0.6185 & 0.4452 & 0.3423 & 0.2735 & 0.2448 & 0.4579 & 0.7771 & Bad\\
InceptionNet & 0.6279 & 0.4530 & 0.3476 & 0.2780 & 0.2497 & 0.4659 & 0.8132 & Good\\
MobileNetV2 & 0.6273 & 0.4542 & 0.3491 & 0.2798 & 0.2539 & 0.4648 & 0.7991 & Good\\
MobileNetV3 & 0.6220 & 0.4435 & 0.3378 & 0.2691 & 0.2445 & 0.4546 & 0.7724 & Bad\\
ConvNext & 0.6380 & 0.4855 & 0.3782 & 0.3168 & 0.2869 & 0.4923 & 0.8506 & Good\\
\hline
\end{tabular}}
\end{table*}
In our work, we used an encoder-decoder-based transformer model, where the encoder transformer extracts features that undergo cross-attention with the decoder transformer to generate meaningful captions. On the encoder side, the patch size is set to $49$, and both the convolution layer output and the transformer-based encoder output are $768$. The multi-head encoder transformer consists of $8$ heads, and the transformer is repeated $12$ times. For the decoder, the number of hidden layers and attention heads is set to $12$. The vocabulary size and maximum sequence length are determined based on the training dataset captions. Training is carried out with a learning rate of $10^{-4}$ and a batch size of $64$. For prediction using beam search, the number of beams is set to $5$, the length penalty to $2.0$, and the repetition of the n-gram size to $3$.  

Our experiments were carried out on a Linux-based Ubuntu 22.04 system equipped with~\emph{NVIDIA RTX A6000 GPU} $(48GB)$ and $124GB$ RAM.
\subsection{Dataset Utilized}
To evaluate the performance of our model, we used three well-established RSIC datasets. The details of these datasets are provided below.
\begin{itemize}
    \item\textbf{SYDNEY:} The SYDNEY dataset~\cite{qu2016deep} comprises a total of $613$ images, with 497 designated for training, 58 for testing, and 58 for validation. It originates from the Sydney dataset~\cite{zhang2014saliency} and has been meticulously refined to include seven distinct categories:~\emph{airport, industrial, meadow, ocean, residential, river,} and~\emph{runway}, achieved through careful selection and cropping.
    \item\textbf{UCM:} The UCM dataset~\cite{qu2016deep} consists of 2,100 images, with 1,680 allocated for training, 210 for testing, and 210 for validation. It is an adapted version of the~\emph{UC Merced Land Use} dataset~\cite{yang2010bag}, which contains 21 land use categories, each comprising 100 images. These categories include~\emph{agriculture, airport, baseball diamond, beach, buildings, chaparral, denseresidential, forest, freeway, golfcourse, harbour, intersection, mediumresidential, mobilehomepark, overpass, parking, river, runway, sparseresidential, storagetanks,} and~\emph{tenniscourt}.
    \item\textbf{RSICD:} The RSICD dataset~\cite{lu2017exploring} contains an extensive collection of 10,921 images, with 8,034 reserved for training, 1,093 for testing, and 1,094 for validation. Powered by multiple platforms, including Google Earth~\cite{xia2017aid}, Baidu Map, MapABC, and Tianditu, this dataset covers 31 distinct categories, such as~\emph{airport, bareland, baseballfield, beach, bridge, center, church, commercial, denseresidential, desert, farmland, forest, industrial, meadow, mediumresidential, mountain, park, parking, playground, pond, port, railwaystation, resort, river, school, sparseresidential, square, stadium, storagetanks,} and~\emph{viaduct}.
\end{itemize}
For this study, we utilized the corrected versions of these datasets~\cite{das2024textgcn}, which rectify common issues, including spelling inconsistencies, grammatical inaccuracies, and variations in English dialects. The train-validation-test split was maintained as originally defined in these datasets.
\subsection{Performance Metrics}  
The evaluation metrics used in our model are detailed below. These metrics are commonly used in RSIC~\cite{qu2016deep, lu2017exploring, das2024textgcn, das2024unveiling}.  
\begin{itemize}  
    \item\textbf{BLEU:} The Bilingual Evaluation Understudy (BLEU)~\cite{papineni-etal-2002-bleu} is a precision-based metric that assesses the similarity between a generated caption and reference captions by analyzing n-gram overlap. Calculate the geometric mean of the precisions in n gram while applying a brevity penalty to discourage excessively short outputs. BLEU is widely adopted in tasks such as machine translation and image captioning. In this study, we employ BLEU-1 to BLEU-4. 
    \item\textbf{METEOR:} The Metric for the Evaluation of Explicit Ordering Translations (METEOR)~\cite{lavie-agarwal-2007-meteor} evaluates generated captions considering factors such as stemming, synonym matching, and word order. Unlike BLEU, which is precision focused, METEOR integrates both precision and recall, assigning a higher weight to recall for improved accuracy.  
    \item\textbf{ROUGE:} The Recall-Oriented Understudy for Gisting Evaluation (ROUGE)~\cite{lin-2004-ROUGE} is a recall-based metric that measures n-gram overlap between generated and reference captions. In this work, we utilize ROUGE-L, which leverages the longest common subsequence (LCS) to assess text similarity.  
    \item\textbf{CIDEr:} The Consensus-based Image Description Evaluation (CIDEr)~\cite{vedantam2015cider} quantifies the quality of generated image captions by comparing them to a collection of human-written references. CIDEr evaluates how closely a generated caption aligns with the consensus of human descriptions, aiming to capture the overall semantic agreement between the generated text and human perception.
\end{itemize} 
\section{Results}
\label{sec_experimental}
\subsection{Numerical Evaluation of different CNNs}
The performance of twelve encoder-decoder models across three datasets is presented in~\Cref{SYDNEY_greedy,SYDNEY_beam,UCM_greedy,UCM_beam,RSICD_greedy,RSICD_beam} using seven evaluation metrics. It is observed that ConvNext and ResNet consistently belong to the~\emph{Good} cluster. In contrast, AlexNet performs poorly, remaining in the~\emph{Bad} cluster for all datasets except for the SYDNEY dataset with greedy search, where it is placed in the~\emph{Medium} cluster. GoogleNet and MobileNetV3 never appear in the~\emph{Good} cluster. RegNet, VGGNet, and MobileNetV2 are selected for the~\emph{Good} cluster once, while Wide ResNet, ResNext, and DenseNet are selected twice. InceptionNet appears in the~\emph{Good} cluster three times. Additionally, it can be concluded that the impact of beam search and greedy search is not significantly different across all CNNs and datasets.

For each scenario, the CNNs in the cluster~\emph{Good} will be further analyzed by human evaluation, assessing the captions from a human perspective (\Cref{sec_subjective}). The combined numerical and subjective results will indicate which CNN performs best in all scenarios.
\subsection{Effectiveness of CNNs in Capturing the Core Meaning of Images}
\begin{table}[!ht]
\centering
\caption{Classification Performance Across Three Datasets (\%): \textcolor{redbox}{Red} denotes the first place,~\textcolor{bluebox}{Blue} denotes the second place, and~\textcolor{greenbox}{Green} denotes the third place}
\label{classification}
\resizebox{0.475\textwidth}{!}{
\begin{tabular}{|cccccc|}
\hline
CNN & SYDNEY & UCM & RSICD & MACRO & MICRO \\
\hline
ResNet &~\textcolor{green}{91.38} & 91.43 &~\textcolor{green}{93.69} & 92.17 &~\textcolor{blue}{93.24} \\
Wide ResNet &~\textcolor{blue}{94.83} &~\textcolor{blue}{92.86} & 92.96 &~\textcolor{green}{93.55} & 93.02 \\
ResNext & 89.66 & 90.00 & 93.60 & 91.09 & 92.87 \\
RegNet & 89.66 & 90.00 &~\textcolor{blue}{93.96} & 91.21 &~\textcolor{green}{93.17} \\
VGGNet &~\textcolor{red}{96.55} & 86.19 & 89.75 & 90.83 & 89.49 \\
DenseNet &~\textcolor{red}{96.55} &~\textcolor{green}{91.90} & 93.32 &~\textcolor{blue}{93.92} &~\textcolor{blue}{93.24} \\
AlexNet &~\textcolor{blue}{94.83} & 89.52 & 85.45 & 89.93 & 86.48 \\
GoogleNet &~\textcolor{red}{96.55} & 90.95 & 91.22 & 92.91 & 91.40 \\
InceptionNet &~\textcolor{red}{96.55} & 87.14 & 92.22 & 91.97 & 91.62 \\
MobileNetV2 & 89.66 & 89.05 & 91.31 & 90.01 & 90.89 \\
MobileNetV3 &~\textcolor{green}{91.38} & 90.00 & 92.59 & 91.32 & 92.14 \\
ConvNext &~\textcolor{blue}{94.83} &~\textcolor{red}{93.81} &~\textcolor{red}{94.88} &~\textcolor{red}{94.51} &~\textcolor{red}{94.71} \\
\hline
\end{tabular}
}
\end{table}
The encoder plays a crucial role in remote sensing image captioning (RSIC) tasks. Its output significantly assists the decoder in generating accurate captions. The efficiency of an encoder depends on how well it interprets an image. A deep understanding of the core meaning is essential for an encoder, as remote sensing images exhibit intricate multi-scale structures and complex backgrounds. Proper interpretation enables the extraction of informative multiscale features, which are vital to generate precise and meaningful descriptions.  

To evaluate encoder performance, we performed a classification test on all encoders. The classification labels correspond to the image categories in each dataset, where SYDNEY has 7 classes, UCM has 21, and RSICD has 31. The classification model is defined by first taking the encoder's output in a 3D format (batch size, patch size, feature size) and then applying a weighted average to each patch to obtain a 2D representation (batch size, feature size). A linear layer is then added to map the features to the respective number of classes.  

\Cref{classification} presents the classification results for all CNNs across the three datasets, considering both micro and macro averages. For macro averaging, ConvNext ranks first, followed by DenseNet in second and Wide ResNet in third. In micro averaging, ConvNext retains the top position, while DenseNet and ResNet share second place, with RegNet in third. However, rankings differ across datasets. In the SYDNEY dataset, VGGNet, DenseNet, GoogleNet, and InceptionNet secure the first position, while Wide ResNet, AlexNet, and ConvNext rank second, and ResNet together with MobileNetV3 share the third position. The similarity in results among multiple CNNs in the Sydney dataset can be attributed to its small size and class imbalance. With only 58 test samples available, the model has limited variation for learning, resulting in uniform predictions across many CNNs~\cite{lu2017exploring}. This minimizes the likelihood of significant performance differences between them. In the UCM dataset, ConvNext holds the first position, Wide ResNet comes second, and DenseNet secures third. For the RSICD dataset, ConvNext ranks highest, followed by ResNet in second place and RegNet in third. These results highlight the variation in CNN performance across datasets, with ConvNext consistently demonstrating superior effectiveness.    
\subsection{Effect of Multi-Head Transformer After the CNN}
\begin{table*}[!ht]
\centering
\caption{Performace of Ablation Studies on the SYDNEY Dataset}
\label{SYDNEY_abl}
\resizebox{0.98\textwidth}{!}{
\begin{tabular}{|cccccccc|}
\hline
Model & BLEU-1 & BLEU-2 & BLEU-3 & BLEU-4 & METEOR & ROUGE-L & CIDEr \\
\hline
R-WOT-G & 0.7195 & 0.6123 & 0.5286 & 0.4565 & 0.3566 & 0.6404 & 1.9592 \\
R-SHT-G & 0.7270 & 0.6331 & 0.5418 & 0.4864 & 0.3545 & 0.6671 & 2.0706 \\
R-MHT-G & 0.7480 & 0.6484 & 0.5719 & 0.5039 & 0.3630 & 0.6723 & 2.0973 \\
\hdashline
C-WOT-G & 0.7520 & 0.6648 & 0.5949 & 0.5364 & 0.3755 & 0.6945 & 2.3312 \\
C-SHT-G & 0.7455 & 0.6571 & 0.5818 & 0.5152 & 0.3856 & 0.7129 & 2.3365 \\
C-MHT-G &~\textbf{0.7997} &~\textbf{0.6844} & \textbf{0.6325} & \textbf{0.5694} & \textbf{0.4073} & \textbf{0.7349} & \textbf{2.4945} \\
\hdashline
R-WOT-B & 0.7031 & 0.6004 & 0.5238 & 0.4625 & 0.3476 & 0.6437 & 1.9377 \\
R-SHT-B & 0.7150 & 0.6154 & 0.5393 & 0.4735 & 0.3620 & 0.6573 & 2.0411 \\
R-MHT-B & 0.7578 & 0.6663 & 0.5906 & 0.5228 & 0.3729 & 0.6883 & 2.1498 \\
\hdashline
C-WOT-B & 0.7250 & 0.6322 & 0.5567 & 0.4901 & 0.3639 & 0.6651 & 2.1380 \\
C-SHT-B & 0.7348 & 0.6475 & 0.5702 & 0.4989 & 0.3684 & 0.6713 & 2.1677 \\
C-MHT-B & 0.7490 & 0.6582 & 0.5831 & 0.5187 & 0.3807 & 0.6770 & 2.2202 \\
\hline
\end{tabular}}
\end{table*}
\begin{table*}[!ht]
\centering
\caption{Performace of Ablation Studies on the UCM Dataset}
\label{UCM_abl}
\resizebox{0.98\textwidth}{!}{
\begin{tabular}{|cccccccc|}
\hline
Model & BLEU-1 & BLEU-2 & BLEU-3 & BLEU-4 & METEOR & ROUGE-L & CIDEr \\
\hline
R-WOT-G & 0.8105 & 0.7430 & 0.6897 & 0.6406 & 0.4373 & 0.7604 & 3.3090 \\
R-SHT-G & 0.8269 & 0.7518 & 0.6961 & 0.6471 & 0.4331 & 0.7674 & 3.3512 \\
R-MHT-G & 0.8350 & 0.7686 & 0.7143 & 0.6658 & 0.4425 & 0.7822 & 3.4078 \\
\hdashline
C-WOT-G & 0.7805 & 0.7014 & 0.6385 & 0.5829 & 0.4124 & 0.7354 & 3.0045 \\
C-SHT-G & 0.8197 & 0.7468 & 0.6873 & 0.6360 & 0.4448 & 0.7871 & 3.3232 \\
C-MHT-G & 0.8369 & 0.7712 & 0.7143 & 0.6612 & 0.4566 & 0.8119 & 3.4582 \\
\hdashline
R-WOT-B & 0.8283 & 0.7654 & 0.7124 & 0.6621 & 0.4363 & 0.7797 & 3.2854 \\
R-SHT-B & 0.8244 & 0.7559 & 0.7001 & 0.6502 & 0.4524 & 0.7867 & 3.3968 \\
R-MHT-B & 0.8394 & 0.7795 & 0.7259 & 0.6770 & 0.4523 & 0.7930 & 3.4061 \\
\hdashline
C-WOT-B & 0.8179 & 0.7569 & 0.6955 & 0.6178 & 0.4253 & 0.7545 & 3.0367 \\
C-SHT-B & 0.8339 & 0.7743 & 0.7065 & 0.6453 & 0.4554 & 0.8053 & 3.4370 \\
C-MHT-B & \textbf{0.8442} & \textbf{0.7802} & \textbf{0.7319} &~\textbf{0.6806} & \textbf{0.4602} & \textbf{0.8126} & \textbf{3.4953} \\
\hline
\end{tabular}}
\end{table*}
\begin{table*}[!ht]
\centering
\caption{Performace of Ablation Studies on the RSICD Dataset}
\label{RSICD_abl}
\resizebox{0.98\textwidth}{!}{
\begin{tabular}{|cccccccc|}
\hline
Model & BLEU-1 & BLEU-2 & BLEU-3 & BLEU-4 & METEOR & ROUGE-L & CIDEr \\
\hline
R-WOT-G & 0.6127 & 0.4368 & 0.3312 & 0.2627 & 0.2325 & 0.4261 & 0.7921 \\
R-SHT-G & 0.6199 & 0.4413 & 0.3420 & 0.2772 & 0.2311 & 0.4413 & 0.7815 \\
R-MHT-G & 0.6295 & 0.4607 & 0.3574 & 0.2878 & 0.2535 & 0.4671 & 0.8085 \\
\hdashline
C-WOT-G & 0.6176 & 0.4540 & 0.3477 & 0.2866 & 0.2503 & 0.4798 & 0.8197 \\
C-SHT-G & 0.6280 & 0.4607 & 0.3505 & 0.2892 & 0.2614 & 0.4862 & 0.8279 \\
C-MHT-G & \textbf{0.6431} & 0.4665 & 0.3602 & 0.3013 & 0.2560 & 0.4945 & 0.8415 \\
\hdashline
R-WOT-B & 0.6121 & 0.4409 & 0.3385 & 0.2708 & 0.2579 & 0.4609 & 0.7594 \\
R-SHT-B & 0.6171 & 0.4594 & 0.3318 & 0.2737 & 0.2422 & 0.4556 & 0.8054 \\
R-MHT-B & 0.6314 & 0.4661 & 0.3642 & 0.2949 & 0.2548 & 0.4711 & 0.8243 \\
\hdashline
C-WOT-B & 0.6294 & 0.4684 & 0.3612 & 0.2994 & 0.2668 & 0.4801 & 0.8336 \\
C-SHT-B & 0.6231 & 0.4648 & 0.3592 & 0.3084 & 0.2784 & 0.4859 & 0.8257 \\
C-MHT-B & 0.6380 & \textbf{0.4855} & \textbf{0.3782} & \textbf{0.3168} & \textbf{0.2869} & \textbf{0.4923} & \textbf{0.8506} \\
\hline
\end{tabular}}
\end{table*}
We conducted an ablation study to evaluate the impact of incorporating a multi-head transformer in our model. For this purpose, we performed experiments with three different encoder setups. In the first set-up, the encoder output was taken directly from the convolution layer. In the second setup, a single-head transformer was used in the encoder. Finally, in the third architecture, a multi-head transformer was employed.  

The numerical comparisons of these ablation studies are presented in~\Cref{SYDNEY_abl,UCM_abl,RSICD_abl}.The notation used in these tables follows the convention:~\emph{Encoder-Orientation-Search}.
\begin{itemize}
    \item\emph{Encoder}: Indicates the CNN used in the encoder. Here,~\emph{C} refers to ConvNext and~\emph{R} refers to ResNet. The reason behind selecting these two CNNs is that they are included in~\emph{Good} cluster for all the cases in the numerical evaluation.
    \item\emph{Orientation}: Indicates the orientation of the encoder used in the model. Here,~\emph{WOT} denotes that the output of the encoder is obtained from the convolution layer (\Cref{architecture}),~\emph{SHT} denotes that the encoder transformer has a single head, and~\emph{MHT} denotes that the encoder transformer is multi-headed.
    \item\emph{Search}: Indicates the search technique used to generate the caption. Here,~\emph{G} represents greedy search, and~\emph{B} represents beam search.
\end{itemize}
The results clearly demonstrate that integrating a transformer after the CNN enhances performance compared to using only a CNN. Specifically, the~\emph{Single-Head Transformer (SHT)} setup outperforms the~\emph{Without Transformer (WOT)} approach, highlighting the benefits of self-attention in capturing long-range dependencies that standard CNNs alone struggle with. While CNNs are effective at extracting local features, they lack the ability to model global context efficiently. By introducing a self-attention mechanism, SHT enables the model to focus on different regions of the image dynamically, leading to improved feature representation and better caption generation.  

Furthermore,~\emph{Multi-Head Transformer (MHT)} consistently surpasses both the WOT and SHT configurations. Unlike SHT, which uses a single attention head, MHT employs multiple attention heads that can attend to different parts of the image simultaneously. This allows the model to capture a richer set of dependencies, improving its ability to generate more contextually accurate and semantically meaningful captions. The parallel attention mechanism in MHT enhances the diversity of features, ensuring that local and global relationships within the image are effectively learned. These improvements demonstrate that while self-attention alone improves performance over a pure CNN encoder, the use of multiple attention heads further strengthens the model’s ability to process complex image structures, leading to better captioning results.
\subsection{Comparison of Different RSIC Methods}
\begin{table*}[!ht]
\centering
\caption{Performance of Different Methods on the SYDNEY Dataset}
\label{SYDNEY_results}
\resizebox{0.98\textwidth}{!}{
\begin{tabular}{|cccccccc|}
\hline
Model & BLEU-1 & BLEU-2 & BLEU-3 & BLEU-4 & METEOR & ROUGE-L & CIDEr \\
\hline
CSMLF~\cite{wang2019semantic} & 0.4441 & 0.3369 & 0.2815 & 0.2408 & 0.1576 & 0.4018 & 0.9378 \\
CSMLF-FT~\cite{wang2019semantic} & 0.5998 & 0.4583 & 0.3869 & 0.3433 & 0.2475 & 0.5018 & 0.7555 \\
SVM-DBOW~\cite{hoxha2021novel} & 0.7787 & 0.6835 & 0.6023 & 0.5305 & 0.3797 & 0.6992 & 2.2722 \\
SVM-DCONC~\cite{hoxha2021novel} & 0.7547 & 0.6711 & 0.5970 & 0.5308 & 0.3643 & 0.6746 & 2.2222 \\
TrTr-CMR~\cite{wu2024trtr} & \textbf{0.8270} & \textbf{0.6994} & 0.6002 & 0.5199 & 0.3803 & 0.7220 & 2.2728 \\
R-LSTM-G~\cite{das2024unveiling} & 0.7417 & 0.6592 & 0.5925 & 0.5343 & 0.3817 & 0.6903 & 2.2032 \\
R-LSTM-B~\cite{das2024unveiling} & 0.7472 & 0.6603 & 0.5870 & 0.5254 & 0.3908 & 0.7018 & 2.1980 \\
C-MHT-G & 0.7997 & 0.6844 & \textbf{0.6325} & \textbf{0.5694} & \textbf{0.4073} & \textbf{0.7349} & \textbf{2.4945} \\
C-MHT-B & 0.7490 & 0.6582 & 0.5831 & 0.5187 & 0.3807 & 0.6770 & 2.2202 \\
\hline
\end{tabular}}
\end{table*}
\begin{table*}[!ht]
\centering
\caption{Performance of Different Methods on the UCM Dataset}
\label{UCM_results}
\resizebox{0.98\textwidth}{!}{
\begin{tabular}{|cccccccc|}
\hline
Model & BLEU-1 & BLEU-2 & BLEU-3 & BLEU-4 & METEOR & ROUGE-L & CIDEr \\
\hline
CSMLF~\cite{wang2019semantic} & 0.3874 & 0.2145 & 0.1253 & 0.0915 & 0.0954 & 0.3599 & 0.3703 \\
CSMLF-FT~\cite{wang2019semantic} & 0.3671 & 0.1485 & 0.0763 & 0.0505 & 0.0944 & 0.2986 & 0.1351 \\
SVM-DBOW~\cite{hoxha2021novel} & 0.7635 & 0.6664 & 0.5869 & 0.5195 & 0.3654 & 0.6801 & 2.7142 \\
SVM-DCONC~\cite{hoxha2021novel} & 0.7653 & 0.6947 & 0.6417 & 0.5942 & 0.3702 & 0.6877 & 2.9228 \\
TrTr-CMR~\cite{wu2024trtr} & 0.8156 & 0.7091 & 0.6220 & 0.5469 & 0.3978 & 0.7442 & 2.4742 \\
R-LSTM-G~\cite{das2024unveiling} & 0.8001 & 0.7273 & 0.6675 & 0.6131 & 0.4084 & 0.7501 & 3.0616 \\
R-LSTM-B~\cite{das2024unveiling} & 0.8283 & 0.7654 & 0.7124 & 0.6621 & 0.4363 & 0.7797 & 3.2865 \\
C-MHT-G & 0.8369 & 0.7712 & 0.7143 & 0.6612 & 0.4566 & 0.8119 & 3.4582 \\
C-MHT-B & \textbf{0.8442} & \textbf{0.7802} & \textbf{0.7319} &~\textbf{0.6806} & \textbf{0.4602} & \textbf{0.8126} & \textbf{3.4953} \\
\hline
\end{tabular}}
\end{table*}
\begin{table*}[!ht]
\centering
\caption{Performance of Different Methods on the RSICD Dataset}
\label{RSICD_results}
\resizebox{0.98\textwidth}{!}{
\begin{tabular}{|cccccccc|}
\hline
Model & BLEU-1 & BLEU-2 & BLEU-3 & BLEU-4 & METEOR & ROUGE-L & CIDEr \\
\hline
CSMLF~\cite{wang2019semantic} & 0.5759 & 0.3859 & 0.2832 & 0.2217 & 0.2128 & 0.4455 & 0.5297 \\
CSMLF-FT~\cite{wang2019semantic} & 0.5106 & 0.2911 & 0.1903 & 0.1352 & 0.1693 & 0.3789 & 0.3388 \\
SVM-DBOW~\cite{hoxha2021novel} & 0.6112 & 0.4277 & 0.3153 & 0.2411 & 0.2303 & 0.4588 & 0.6825 \\
SVM-DCONC~\cite{hoxha2021novel} & 0.5999 & 0.4347 & 0.3355 & 0.2689 & 0.2299 & 0.4577 & 0.6854 \\
TrTr-CMR~\cite{wu2024trtr} & 0.6201 & 0.3937 & 0.2671 & 0.1932 & 0.2399 & 0.4895 & 0.7518 \\
R-LSTM-G~\cite{das2024unveiling} & 0.6407 & 0.4676 & 0.3608 & 0.2878 & 0.2574 & 0.4745 & 0.7990 \\
R-LSTM-B~\cite{das2024unveiling} & 0.6121 & 0.4409 & 0.3385 & 0.2708 & 0.2579 & 0.4609 & 0.7594 \\
C-MHT-G & \textbf{0.6431} & 0.4665 & 0.3602 & 0.3013 & 0.2560 & 0.4945 & 0.8415 \\
C-MHT-B & 0.6380 & \textbf{0.4855} & \textbf{0.3782} & \textbf{0.3168} & \textbf{0.2869} & \textbf{0.4923} & \textbf{0.8506} \\
\hline
\end{tabular}}
\end{table*}
We present a comparison of various RSIC models in~\Cref{SYDNEY_results,UCM_results,RSICD_results}. The models evaluated include the RSIC model based on the Collective Semantic Metric Learning Framework (CSMLF)~\cite{wang2019semantic} and its variant CSMLF-FT, where FT indicates fine-tuning. The next set of models utilizes support vector machines (SVM) as decoders~\cite{hoxha2021novel}, namely SVM-DBOW and SVM-DCONC, where BOW and CONC represent two distinct sentence representations. The TrTr-CMR model~\cite{wu2024trtr} is a dual transform RSIC model based on cross-mode reasoning. It employs a Swin transformer-based encoder with a shifted window partitioning scheme for multi-scale visual feature extraction. Additionally, a Transformer language model (TLM) is designed that incorporates self- and cross-attention mechanisms of the model decoder. Finally, we consider the results of an encoder-decoder model with ResNet as the encoder and LSTM as the decoder~\cite{das2024unveiling}, with greedy search (R-LSTM-G) and beam search (R-LSTM-B) used as search techniques for caption generation.
The numerical analysis (\Cref{SYDNEY_results,UCM_results,RSICD_results}) presents the numerical results of various RSIC methods along with the ablation studies of our work. It is evident that efficient selection of the CNN and decoder improves the quality of generated captions. For the SYDNEY dataset (\Cref{SYDNEY_results}), TrTr-CMR~\cite{wu2024trtr} outperforms other models for lower-order BLEU metrics (BLEU-1 and BLEU-2), while C-MHT-G achieves the best performance for other metrics. For the UCM dataset (\Cref{UCM_results}), C-MHT-B performs the best for all metrics. Similarly, for the RSICD dataset (\Cref{RSICD_results}), C-MHT-G achieves the highest score for BLEU-1, while C-MHT-B outperforms all other models for the rest. These results indicate that ConvNext consistently delivers superior performance across all datasets, with greedy search being optimal for the SYDNEY dataset and beam search for the other two.
\subsection{Subjective Evaluation of Different CNNs}
\label{sec_subjective}
\begin{table}[!ht]
\centering
\caption{Results of subjective evaluation on SYDNEY dataset (in \%)}
\label{subjective_SYDNEY}
\begin{tabular}{|c|cccc|}
\hline
Search & CNN & Rel & Part Rel & Unrel\\
\hline
\multirow{4}{*}{Greedy} 
 & ConvNext & 89.65 & 3.45 & 6.90 \\
 & InceptionNet & 77.59 & 8.62 & 13.79 \\
 & ResNet & 84.49 & 10.34 & 5.17 \\
 & ResNext & 86.21 & 5.17 & 8.62 \\
\hline
\multirow{4}{*}{Beam} 
 & ConvNext & 87.93 & 3.45 & 8.62 \\
 & InceptionNet & 79.31 & 5.17 & 15.52 \\
 & ResNet & 81.04 & 8.62 & 10.34 \\
 & VGGNet & 82.76 & 5.17 & 12.07 \\
\hline
\end{tabular}
\end{table}
\begin{table}[!ht]
\centering
\caption{Results of subjective evaluation on UCM dataset (in \%)}
\label{subjective_UCM}
\begin{tabular}{|c|cccc|}
\hline
Search & CNN & Rel & Part Rel & Unrel\\
\hline
\multirow{4}{*}{Greedy} 
 & ConvNext & 92.86 & 3.81 & 3.33 \\
 & DenseNet & 86.67 & 5.71 & 7.62 \\
 & ResNet & 88.09 & 2.86 & 9.05 \\
 & Wide ResNet & 87.14 & 4.29 & 8.57 \\
\hline
\multirow{4}{*}{Beam} 
 & ConvNext & 91.43 & 3.81 & 4.76 \\
 & DenseNet & 88.09 & 3.81 & 8.10 \\
 & ResNet & 88.10 & 5.71 & 6.19 \\
 & Wide ResNet & 89.52 & 3.81 & 6.67 \\
\hline
\end{tabular}
\end{table}
\begin{table}[!ht]
\centering
\caption{Results of subjective evaluation on RSICD dataset (in \%)}
\label{subjective_RSICD}
\begin{tabular}{|c|cccc|}
\hline
Search & CNN & Rel & Part Rel & Unrel\\
\hline
\multirow{4}{*}{Greedy} 
 & ConvNext & 83.63 & 6.95 & 9.42 \\
 & RegNet & 79.69 & 6.68 & 13.63 \\
 & ResNet & 80.61 & 8.23 & 11.16 \\
 & ResNext & 80.15 & 7.50 & 12.35 \\
\hline
\multirow{4}{*}{Beam} 
 & ConvNext & 83.71 & 6.13 & 10.16 \\
 & InceptionNet & 81.34 & 6.95 & 11.71 \\
 & MobileNetV2 & 77.95 & 8.60 & 13.45 \\
 & ResNet & 80.51 & 6.86 & 12.63 \\
\hline
\end{tabular}
\end{table}
The captioning task differs from other machine learning tasks, such as classification, where numerical analysis alone is sufficient. In tasks like classification, where the output is fixed or the number of outputs is limited, numerical analysis is often enough. However, captioning is not the same. A single image can be described by many different sentences, each of which can be equally correct. Therefore, a fixed number of test captions, with a maximum of five unique captions per image across all three datasets, is insufficient to fully evaluate the quality of the caption. In addition to numerical evaluation, subjective evaluation is also necessary~\cite{das2024unveiling,lu2017exploring}. The main purpose of subjective evaluation is to assess the generated caption from a human perspective. For this reason, we assigned the task to an annotator~\footnote{The annotator is a highly skilled professional with comprehensive training in working with RSIC models.}. Three labels were used for this evaluation, as described below:
\begin{itemize}
    \item\textbf{Related:} The generated caption accurately identifies the main object and conveys the core meaning of the image with minimal or no errors.
    \item\textbf{Partially Related:} The generated caption identifies the main object but does not fully capture the core meaning of the image or contains notable issues.
    \item\textbf{Unrelated:} The generated caption is completely unrelated to the image and misidentifies the main object.
\end{itemize}
\Cref{subjective_SYDNEY,subjective_UCM,subjective_RSICD} illustrate the subjective evaluation results for the three datasets and two search techniques. Analyzing these results, we can conclude that ConvNext performs consistently well across all scenarios. ResNet also maintains consistent performance, securing the second position in all greedy search scenario scenarios, outperforming other CNNs except ConvNext. However, the results differ for beam search. For the SYDNEY dataset, ResNet shows better performance in terms of the~\emph{Unrelated} label compared to InceptionNet and VGGNet. For the UCM dataset, Wide ResNet outperforms both ResNet and DenseNet. For the RSICD dataset, InceptionNet surpasses ResNet and MobileNetV2.
\subsection{Visual Analysis}
\begin{figure*}[!ht]
  \centering
  \subfloat[\label{example1}]{\includegraphics[width=0.25\textwidth,height=125px]{Example1.png}}
  \subfloat[\label{example2}]{\includegraphics[width=0.25\textwidth,height=125px]{Example2.png}}
  \subfloat[\label{example3}]{\includegraphics[width=0.25\textwidth,height=125px]{Example3.png}}

  \subfloat[\label{example4}]{\includegraphics[width=0.25\textwidth,height=125px]{Example4.png}}
  \subfloat[\label{example5}]{\includegraphics[width=0.25\textwidth,height=125px]{Example5.png}}
  \subfloat[\label{example6}]{\includegraphics[width=0.25\textwidth,height=125px]{Example6.png}}

  \subfloat[\label{example7}]{\includegraphics[width=0.25\textwidth,height=125px]{Example7.png}}
  \subfloat[\label{example8}]{\includegraphics[width=0.25\textwidth,height=125px]{Example8.png}}
  \subfloat[\label{example9}]{\includegraphics[width=0.25\textwidth,height=125px]{Example9.png}}

   \caption{Examples of RSIC on different CNNs}
   \label{examples}
\end{figure*}

We provide some visual examples for the CNNs in the~\emph{Good} cluster together with their captions from various datasets in~\Cref{examples}. In~\Cref{example1}, both ConvNext and ResNet effectively identify the main object~\emph{industrial area}, while InceptionNet incorrectly detects~\emph{runways} and ResNext incorrectly detects~\emph{ocean}. In~\Cref{example2}, ConvNext and ResNet correctly identify the main object~\emph{river}, while ResNet incorrectly detects~\emph{industrial area}, which is not present in the image, and both InceptionNet and VGGNet mistakenly classify it as~\emph{meadow}. In~\Cref{example3}, ConvNext and ResNet detect the primary object~\emph{airplane}, but ResNet also incorrectly identifies an additional~\emph{small airplane}, which is not present. Wide ResNet detects~\emph{runway} but does not detect~\emph{airplane}, while DenseNet completely misclassifies the image as~\emph{medium residential area}. In~\Cref{example4}, only ConvNext accurately identifies the main object~\emph{storage tank}, while DenseNet, ResNet, and Wide ResNet incorrectly classify it as~\emph{medium residential area}. In~\Cref{example5}, only ConvNext properly describes the image with the main object~\emph{villa} and the location~\emph{sparse residential area}, while DenseNet, ResNet and Wide ResNet not only fail to detect~\emph{villa} but also misclassify the location as~\emph{forest}. In~\Cref{example6}, ConvNext and ResNet successfully detect the location~\emph{park}, although ConvNext does not identify~\emph{pond}. RegNet and ResNext fail to detect~\emph{park}, but correctly identify objects such as~\emph{baseball field},\emph{green trees}, and\emph{buildings} present in the image. In~\Cref{example7}, ConvNext and ResNet successfully identify the major object~\emph{square}, though ConvNext fails to detect~\emph{parking lot}, while RegNet and ResNext miss the main object~\emph{square}. In~\Cref{example8}, both ConvNext and ResNet accurately detect the main object~\emph{port}, while InceptionNet misclassifies it as~\emph{resort} and MobileNetV2 misclassifies it as~\emph{commercial area}. Lastly, in~\Cref{example9}, ConvNext and ResNet detect the main object~\emph{school}, while InceptionNet incorrectly classifies it as~\emph{resort} and MobileNetV2 incorrectly classifies it as~\emph{park}.
\subsection{Error Analysis}  
Several common errors~\cite{das2024unveiling} are identified in the three datasets when analyzing the captions generated by different CNNs. The most frequent issue is~\emph{misclassification}, which occurs when objects in an image are incorrectly identified. Some common examples include~\emph{church} and~\emph{storage tanks} being misclassified as~\emph{building},~\emph{road} being misclassified as~\emph{river} and vice versa, and~\emph{river} with~\emph{green water} being misclassified as~\emph{meadow} (\Cref{example2}).  

The second major issue is~\emph{absence of a major object}, where the generated caption is not entirely incorrect but does not include the most significant object. For example, in an~\emph{square} image, side objects such as buildings and trees are detected, but the square itself is not (\Cref{example7}). Similarly, in an image of a villa in a forest, the model detects the forest but fails to identify the villa (\Cref{example5}).  

Another recurring issue is~\emph{correlation problem}, where two words frequently cooccur in the training captions. In such cases, even if only one object is present in an image, both may appear in the caption. A common example is the pair~\emph{building} and~\emph{green trees}, which are often mentioned together, even when one is absent.  

Furthermore,~\emph{counting errors} occur in images that contain multiple instances of the same object, such as~\emph{playgrounds},~\emph{storage tanks}, and~\emph{airplanes} (\Cref{example3}). The model often fails to provide an accurate count due to the lack of explicit numerical references in the training data, where descriptions usually use vague terms like~\emph{some},~\emph{several}, or~\emph{many}.  

Other errors include~\emph{incomplete captions},~\emph{failure to detect minor objects}, and~\emph{unnecessary repetition of words or fragments},~\emph{etc.}, in the generated descriptions. However, these errors occur in only a small number of captions.
\section{Conclusion}
\label{sec_conclusion}
Although significant progress has been made in RSIC in recent days, the optimality of the encoder remains an open discussion, as most research focuses on the decoder. In this work, we analyze the role of the encoder in a transformer-based RSIC model by evaluating twelve different CNN encoders from various points of view. Our findings indicate that ConvNext consistently outperforms other CNNs in all aspects. With depthwise convolutions, layer normalization, and hierarchical feature representation, it excels in feature extraction, stability, and computational efficiency. Its strong and consistent performance across multiple tasks, including remote sensing image captioning, establishes it as an optimal CNN architecture. In the future, our goal is to explore the use of multiple CNNs within a single transformer to enhance the use of encoder output.
\bibliographystyle{elsarticle-num-names} 
\bibliography{mybib}
\end{document}

\endinput
%%
%% End of file `elsarticle-template-num.tex'.
