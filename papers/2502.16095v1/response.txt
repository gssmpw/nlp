\section{Related Work}
\label{sec_relwork}
Remote Sensing Image Captioning (RSIC) has advanced significantly with deep learning techniques. Traditional methods relied on handcrafted feature extraction and statistical models, but the introduction of encoder-decoder architectures revolutionized the field**Vinyals et al., "Show and Tell: A Neural Image Caption Generator"**. The encoder extracts meaningful features from satellite and aerial imagery, while the decoder generates descriptive captions.

Early RSIC models used convolutional and recurrent neural networks. Qu et al.,~[2016]**Qu et al., "Image Captioning via Semantic Scene Graphs"** proposed a deep multimodal neural network, integrating image and text embeddings for caption generation. Lu et al.,~[2017]**Lu et al., "Remote Sensing Image Captioning Dataset (RSICD)"** contributed by introducing the RSICD dataset, addressing dataset limitations in SYDNEY and UCM. Hoxha et al.,~[2019]**Hoxha et al., "Deep Multimodal Fusion for Remote Sensing Image Captioning"** refined beam search strategies and image retrieval techniques using CNN-RNN models for better caption generation accuracy. Li et al.,~[2019]**Li et al., "Two-Level Attention Mechanism for Remote Sensing Image Captioning"** introduced a two-level attention mechanism that combines text-guided and semantic-guided attention to improve spatial correlation. Zhang et al.,~[2019]**Zhang et al., "Multiscale Cropping Approach for Fine-Grained Feature Extraction in RSIC"** developed a multiscale cropping approach to improve fine-grained feature extraction.

To overcome the limitations of earlier methods, attention-based mechanisms were introduced to improve contextual modeling. Xu et al.,~[2015]**Xu et al., "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"** pioneered attention-based image captioning, influencing subsequent RSIC works. Sumbul et al.,~[2020]**Sumbul et al., "Summarization-Driven Attention for Remote Sensing Image Captioning"** explored summarization-driven attention to improve caption coherence. Despite improvements, these models struggled with long-term dependencies and feature aggregation inefficiencies. Li et al.,~[2021]**Li et al., "RASG: Recurrent Attention-based Sequence Generation Model for RSIC"** proposed the RASG framework, integrating recurrent attention to refine context vectors. Wang et al.,~[2022]**Wang et al., "Global-Local Feature Extraction (GLCM) for Remote Sensing Image Captioning"** introduced GLCM, using global-local feature extraction for better word discrimination.

Transformer-based architectures brought about a paradigm shift in RSIC by enhancing sequence modeling and linguistic consistency**Liu et al., "Multilayer Aggregated Transformer (MLAT) for Remote Sensing Image Captioning"**. Liu et al.,~[2022]**Liu et al., "Multilayer Aggregated Transformer (MLAT) for Remote Sensing Image Captioning"** introduced MLAT, a multilayer aggregated transformer that improves the utilization of multiscale features. Meng et al.,~[2023]**Meng et al., "PKG: Graph Neural Network-Based PKG Transformer for RSIC"** developed the PKG transformer, incorporating graph neural networks to refine the relationship between objects. Zhang et al.,~[2023]**Zhang et al., "Stair Attention Mechanism with CIDEr-Based Reinforcement Learning for Remote Sensing Image Captioning"** introduced a stair attention mechanism, integrating CIDEr-based reinforcement learning for improved coherence. Despite these advancements, efficient encoder selection remained an open challenge. Wu et al.,**Wu et al., "Dual Transformer Model Integrating Swin Transformer for Multiscale Feature Extraction in RSIC"** proposed a dual transformer model integrating the Swin transformer for multiscale feature extraction.

A significant limitation is identified when evaluating these models. They predominantly emphasize the decoder instead of the encoder. However, selecting an effective encoder can substantially enhance performance in the RSIC. Taking into account this aspect, Das et al.,~[2024]**Das et al., "Investigation of Eight CNN-Based Encoders for Remote Sensing Image Captioning"** conducted an investigation involving eight CNN-based encoders for RSIC. Their study used an LSTM as the decoder. The encoders were grouped on the basis of performance to determine the highest performing CNNs. The top performing CNNs were subsequently subjected to a subjective evaluation to determine the optimal choice. Their findings recognized ResNet as the most effective encoder. However, transformer-based models have garnered considerable attention in recent years. Recognizing these advances, this work conducts a similar experiment using twelve distinct encoders. It incorporates a transformer-based encoder along with a GPT-2 transformer-based decoder.