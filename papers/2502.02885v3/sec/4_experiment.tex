\begin{table*}[th]
    \centering
    \caption{Comparison with SOTA methods on MSR-VTT dataset \textit{(underline marks the best ViT-based results)}}
      \vspace{-0.8em}
    % \small
  %  \begin{tabular}{lp{1.4em}<{\centering}p{2em}<{\centering}|p{2.2em}<{\centering}p{2.2em}<{\centering}cc|p{2.2em}<{\centering}p{2.2em}<{\centering}cc}
    \begin{tabular}{llc|cccc|cccc}
        \toprule
        \multirow{2}{*}{Method} & \multirow{2}{*}{Backbone} &  Extra &  \multicolumn{4}{c|}{text-to-video}   & \multicolumn{4}{c}{video-to-text}   \\
        \cline{4-11}
          & &  data  & R@1$\uparrow$ &  R@5$\uparrow$ &  R@10$\uparrow$ & MeanR$\downarrow$ & R@1$\uparrow$ &  R@5$\uparrow$ &  R@10$\uparrow$    & MeanR$\downarrow$   \\
        \midrule
       \multicolumn{1}{l}{ \textit{\textbf{Pre-trained foundation model}} } \\
         InternVideo \cite{wang2022internvideo}  & \textit{ViT-H/14} & \ding{51} & 55.2 &	79.6	& 87.5	&10.7 & 57.9	& 79.2	& 86.4	& 	7.5\\
         UMT-L  \cite{li2023unmasked}  & \textit{ViT-L/14} & \ding{51} &  58.8 &  81.0 & 87.1 & -- & 58.6 & 81.6 & 86.5 & --\\

         CLIP-VIP \cite{xue2022clip} & \textit{ViT-B/16} & \ding{51}  &  57.7	    & 80.5	   & 88.2	& --  & -- & --  & -- & --\\
         
          mPLUG-2 \cite{xu2023mplug} &  \textit{ViT-L/14} & \ding{51} & 53.1 & 77.6 & 84.7  & -- & --  & -- & --  & -- \\
          VAST \cite{chen2024vast} & \textit{ViT-G/14}  & \ding{51} & 63.9 & 84.3 & 89.6 & --  & -- & --  & -- & --\\
          
        \midrule
        \textit{\textbf{ViT-based}} \\
         CLIP2TV \cite{gao2021clip2tv} & \textit{ViT-B/16} & \ding{51}& 49.3& 74.7 &83.6 &13.5& 46.9 &75.0 &85.1 &10.0 \\
        DRL \cite{wang2022disentangled}  & \textit{ViT-B/16} & \ding{51}   & 49.4& 76.4 &84.2 & 13.2& 47.0 &77.1 &84.4 &9.2 \\
        TS2-Net \cite{liu2022ts2} & \textit{ViT-B/16} & \ding{55}  &47.8 &76.8 &85.2 & 13.7& 47.8 &76.0& 84.6 &8.5 \\
        Clip4Clip \cite{luo2022clip4clip}  & \textit{ViT-B/16} & \ding{51} & 46.4  & 72.1 &  82.0 &  14.7  & 45.4 &  73.4  & 82.4 & 10.7 \\
        X-CLIP \cite{ma2022x}  & \textit{ViT-B/16} &  \ding{55}  & 49.3&   75.8 &  84.8 &   12.2 &  48.9 &  76.8&   84.5 &   8.1\\
        DMAE \cite{jiang2023dual}  & \textit{ViT-B/16} &  \ding{55}   &  49.9 & 75.8 & 85.5 &  12.5  & 49.6 & 76.3& 85.0&  8.5  \\
        Cap4Video++ \cite{cap4video++}  & \textit{ViT-B/16}  &  \ding{55} & 52.3&  76.8&  85.8 & 11.5 & 50.0 & 75.9 & 86.0 & 7.8 \\
        TeachClip \cite{holistic}  & \textit{ViT-B/16} &  \ding{55} & 48.0 & 75.9 & 83.5 & --  & -- & --  & -- & --\\
    % \hline
    \cdashline{1-11}
    \multirow{4}{*}{ExCae \textit{(\textbf{ours})}} &  {\textit{ViT-B/16}} & \ding{55} & 55.0 & 84.6 & 91.3 & 6.0 & 56.1 & 84.2 & 90.9 & 7.4 \\
       & {\textit{ViT-L/14}} & \ding{55}   & 60.5	 & 87.3 & 	95.0	 & 3.8	 & 62.5	 & 89.0	 & 94.1 & 	4.1 \\
      & {\textit{ViT-H/14}}  & \ding{55}  & 62.2	 &88.2 &	93.7 &	3.9	& 65.1	&89.4&	94.0	&3.8  \\
       & {\textit{ViT-G/14}} & \ding{55}   &  \underline{\textbf{67.7}} & 	 \underline{\textbf{92.7}}	 &  \underline{\textbf{96.2}}	 &  \underline{\textbf{2.9}}	 &  \underline{\textbf{69.3}}	 &  \underline{\textbf{92.5}}	 &  \underline{\textbf{96.3}}	 &  \underline{\textbf{2.3}}   \\
         \bottomrule
    \end{tabular}
    \label{tab:msr_comparison}
\end{table*}

\section{Experiment}
\subsection{Datasets}
\noindent\textbf{\textit{MSR-VTT}} \cite{xu2016msr} is a large-scale dataset containing 10,000 video clips. Each video is described in 20 English sentences. This paper uses
9,000 clips for training and 1,000 clips for testing, as in the setup of previous work \cite{Liu2019UseWY}.
% , each of which is annotated with 20 English sentences. Around 29K unique words are contained in all sentences. Following previous work \cite{Liu2019UseWY}, we use 9,000 clips for training and 1,000 clips for testing.

\noindent\textbf{\textit{MSVD}} \cite{chen2011collecting} consists of 1,970 videos. Each video contains around 40 English captions. In total, around 80K captions are provided. The dataset is derived by 1,200/100/670 for training, validation and testing, respectively. 

% The samples are split by 1,200/100/670 for training, validation and testing, respectively. 


\noindent\textbf{\textit{DiDeMo}} \cite{anne2017localizing} contains over 10,000 videos downloaded from Flickr.
Each video has a caption recording detailed information on camera movement, temporal transition indicators, and activities. 
% collected from Flickr. 
% contains over 10,000 videos collected from Flickr. Each video is described by natural language, with detailed information on camera movement, temporal transition indicators, and activities. 
We split the dataset into training, validation and test sets, each containing 8,395, 1,065 and 1,004 videos, respectively.



\begin{table*}[th]
    \centering
      \caption{Comparison with SOTA methods on MSVD and DiDeMo datasets \textit{(underline marks the best ViT-based results)}}
     \vspace{-0.8em}
       %  \begin{tabular}{lp{1.4em}<{\centering}p{2em}<{\centering}|p{2.2em}<{\centering}p{2.2em}<{\centering}cc|p{2.2em}<{\centering}p{2.2em}<{\centering}cc}
    \begin{tabular}{llc|cccc|cccc}
        \toprule
        \multirow{2}{*}{Method} & \multirow{2}{*}{Backbone} &  Extra &  \multicolumn{4}{c|}{text-to-video}   & \multicolumn{4}{c}{video-to-text}   \\
        \cline{4-11}
        &  & data  & R@1$\uparrow$ &  R@5$\uparrow$ &  R@10$\uparrow$ & MeanR$\downarrow$ & R@1$\uparrow$ &  R@5$\uparrow$ &  R@10$\uparrow$    & MeanR$\downarrow$  \\
        \midrule
        \multicolumn{10}{c}{\textbf{MSVD dataset}} \\
        \midrule
       \multicolumn{1}{l}{ \textit{\textbf{Pre-trained foundation model}} } \\
         InternVideo \cite{wang2022internvideo}  &  \textit{ViT-H/14} & \ding{51}  & 58.4 &	\textbf{84.5} &	90.4	& 8.2 &  76.3	& \textbf{96.8} &	98.7	& 2.1\\
         UMT-L  \cite{li2023unmasked}  &  \textit{ViT-L/14} & \ding{51} &58.2 & 83.9 & 89.6 & -- &  --  & -- & --  & -- \\
     %   VAST \cite{chen2024vast}  & \ding{51} \\
         
        \midrule
        \textit{\textbf{ViT-based}} \\
         CLIP2TV \cite{gao2021clip2tv} & \textit{ViT-B/16}   & \ding{51} & 50.2 & 79.8 & 87.9 & 8.6 & --& --& --& --\\
         DRL \cite{wang2022disentangled}  & \textit{ViT-B/16} & \ding{51} & 50.0 & 81.5 &89.5 &--&68.7 &92.5 &95.6&--\\
        Clip4Clip \cite{luo2022clip4clip} & \textit{ViT-B/16} & \ding{51} &  47.2&  77.7  & --&9.1   & 63.2 & 87.2&-- & 4.2   \\
        X-CLIP \cite{ma2022x}  & \textit{ViT-B/16} &  \ding{55} & 50.4&  80.6 & -- &  8.4&  66.8 & 90.4 &  -- &  4.2 \\
        % Cap4Video \cite{wu2023cap4video} &  \textit{ViT-B/16} &  \ding{55}&  51.8 &80.8& 88.3&8.3 & 70.0&	93.2	&96.2	&	2.4\\
        Cap4Video++ \cite{cap4video++} &  \textit{ViT-B/16} &  \ding{55}&  51.8 &80.8& 88.3&8.3 & 70.0&	93.2	&96.2	&	2.4\\
     %\hline
       \cdashline{1-11}
     \multirow{4}{*}{ExCae \textit{(\textbf{ours})}} &  {\textit{ViT-B/16}}  & \ding{55}   & 51.3  & 78.9	& 86.5	 & 10.6 & 60.6	 & 83.9	 &	88.7	& 5.5 \\
       & {\textit{ViT-L/14}} & \ding{55}   & 53.6&	80.3&	86.9	&10.4&	67.6&	90.9&	96.5	&3.1 \\
      & {\textit{ViT-H/14}} & \ding{55}&  56.5 &	82.2	&88.4	&9.4	&74.1	&93.4&	97.2	&2.3  \\
      &  {\textit{ViT-G/14}} & \ding{55} & \underline{\textbf{59.2}}		& \underline{83.4}		& \underline{\textbf{90.7}}		& \underline{\textbf{5.1}}	&	\underline{\textbf{76.9}}	&	\underline{96.6}	&	\underline{\textbf{99.0}}		& \underline{\textbf{1.6}} \\
        \midrule
         \multicolumn{10}{c}{\textbf{DiDeMo dataset}} \\
        \midrule
        \multicolumn{1}{l}{ \textit{\textbf{Pre-trained foundation model}} }\\
         InternVideo \cite{wang2022internvideo}  &  \textit{ViT-H/14} & \ding{51}  & 57.9	& 82.4	& 88.9	&	9.2 & 59.1 & 	81.8 &	89.0&7.2\\
         UMT-L  \cite{li2023unmasked}  & \textit{ViT-L/14} & \ding{51} & 70.4 & 90.1 & \textbf{93.5} & -- & \textbf{65.7}  & \textbf{89.6} & \textbf{93.3} & --\\
         CLIP-VIP \cite{xue2022clip} & \textit{ViT-B/16} & \ding{51} &  55.3 & 82.0	  & 89.3	& --  & -- & --  & -- & --\\
         mPLUG-2 \cite{xu2023mplug} & \textit{ViT-L/14} & \ding{51} & 56.4 & 79.1 & 85.2  & -- & --  & -- & --  & -- \\
        VAST \cite{chen2024vast} &  \textit{ViT-G/14} & \ding{51} & \textbf{72.0} & \textbf{89.0} & 91.4 & -- & --  & -- & --  & --\\
             
        \midrule
        \textit{\textbf{ViT-based}} \\
       DRL \cite{wang2022disentangled}  & \textit{ViT-B/16} & \ding{51}  &  49.0 & 76.5 & 84.5 & 11.5& 49.9 & 75.4 & 83.3 & 7.9 \\
        Clip4Clip \cite{luo2022clip4clip} & \textit{ViT-B/16}  & \ding{51} & 44.8  &73.4 & -- & 13.5  &44.7 & 74.0  &-- &10.6\\
        X-CLIP \cite{ma2022x}  & \textit{ViT-B/16} &  \ding{55} & 47.8 &79.3& --& 12.6& 47.8& 76.8 & --&10.5\\
        Cap4Video++ \cite{cap4video++}& \textit{ViT-B/16} &  \ding{55}  & 52.5&  	80.0&  	87.0	&  	10.3	&  51.2	&78.5&  	87.4	&7.3	\\      
        TeachClip \cite{holistic} & \textit{ViT-B/16} &  \ding{55}  & 43.7 & 72.7 &  & -- & -- &-- & -- & -- \\
        \cdashline{1-11}
      \multirow{4}{*}{ExCae \textit{(\textbf{ours})}} &  {\textit{ViT-B/16}} & \ding{55} &  53.9  & 80.6 &	87.3  &	 	9.9 & 54.0 & 81.1 & 87.5 &  6.9  \\
       & {\textit{ViT-L/14}} & \ding{55} & 56.9 &	80.9 &	87.9 &	6.5	 & 58.8	 & 83.2	 & 90.0 & 4.8 \\
       &  {\textit{ViT-H/14}} & \ding{55}  &  60.1	 &83.3	&90.0	 &5.9 &	61.0	 &85.6	 &92.3	 &4.5 \\
        &  {\textit{ViT-G/14}} & \ding{55}  & \underline{62.0}	&\underline{85.5}	& \underline{92.3} & \underline{\textbf{4.9}}	& \underline{63.7} &	\underline{87.9}	& \underline{\textbf{93.3}} &	\underline{\textbf{4.1}} \\
      \bottomrule
    \end{tabular}
  
    \label{tab:msvd_comparison}
\end{table*}

\subsection{Implementation Details}
\label{sec:implement}

\paragraph{\textbf{Preprocessing}}
Following the standard text preprocessing \cite{radford2021learning}, we split the textual data, including original texts and video-derived captions, into word tokens with a max length of 70 using a CLIP tokenizer. For videos, we sample 8 frames from each item as visual data.

\paragraph{\textbf{Model Setup}}
This paper reports the results of our method on multiple backbones, \textit{i.e.}, ViT-B/16, ViT-L/14, ViT-H/14, ViT-G/14, compared to SOTA video-text retrieval methods, showing the effect of model scale on performance as well. Ten captions are derived for each video. 
Sixteen experts are learned in the ECS module, two of which are activated to select appropriate representations in both training and inference processes.  In the training phase, the CSI module is pre-learned to acquire the optimal prompt for caption generation, while the ECS module is co-trained alongside the backbone parameters.
In the evaluation phase, multiple captions are derived from each video using the optimal prompt, together with the video itself as video-side expressions.
% \textit{(See \underline{Supplemental Materials} for the learned best prompt and examples of video-derived captions.)}
The \textit{Captioner} and the \textit{Prompt engineer} are implemented by GPT-4o, a large cross-modal inference model recently released by OpenAI in 2024. The initial prompt asking the \textit{Captioner} is set as ``\textit{Generate 10 captions from different perspectives about this video.}" The semantic embedding extractor is Equation \eqref{eq:score} is  implemented by \textit{M3-embedding}~\cite{chen2024bge}.
In addition to the comparison experiment, we encapsulate ExCae as a plug-in unit and test its effect when introduced into existing video-text retrieval methods; specifically, we enhance video expression via CSI and professionalize the video-text matching process by ECS.
Ablation studies and analytical experiments are conducted on MSR-VTT using ViT-G/14 as the base model. 


\paragraph{\textbf{Environment and Evaluation Metrics}}
Our model is trained on 8 NVIDIA  A800 GPUs in a batch size of 16. The pretrained weights from CLIP \cite{radford2021learning} are used to initialize the text and video encoder in ExCae. The model is trained using the Adamax optimizer with a learning rate of 4e-6. 
Experimental results are evaluated by standard video-text retrieval metrics: Recall at Rank K (R@K) and Mean Rank (MeanR). R@K denotes the percentage of correct samples in the top K retrieved samples. In this paper, K is set to 1, 5 and 10, respectively, the same as in previous works~\cite{xue2022clip,jiang2023dual}.
MeanR records the average rank of correct items in the retrieved list. The higher value in R@K means better retrieval performance. In contrast, a lower MeanR is preferred in VTR.
% and the lower va

\subsection{Comparision to State-of-the-arts}
The results of our method are compared to those of the SOTAs, including pre-trained foundation models like InternVideo \cite{wang2022internvideo}, UMT-L \cite{li2023unmasked}, mPLUG-2 \cite{xu2023mplug}, VAST \cite{chen2024vast}, and ViT-based methods that adapt the image-text models to the video-text task. Among the ViT-based methods, CLIP2TV \cite{gao2021clip2tv}, DRL \cite{wang2022disentangled}, Clip4Clip \cite{luo2022clip4clip} and CLIP-VIP \cite{xue2022clip} require the acquisition of auxiliary data for post-pretraining and/or adaptation. In contrast, TS-Net \cite{liu2022ts2}, X-CLIP \cite{ma2022x}, DMAE \cite{jiang2023dual}, Cap4Video++ \cite{cap4video++} and TeachClip \cite{holistic} are learned without extra data. 


Results in Table \ref{tab:msr_comparison} show that, on the MSR-VTT dataset, our method is superior to all existing methods, including pre-training foundation models and the ViT-based methods. 
% relies on any s
% only to the ViT-based methods but also to the pre-training models relying on large-scale pre-training data. 
The results of existing methods show that the Top-1 recall accuracy could hardly transcend 50\% without assistance from extra data. Instead, great improvement \textit{(up to 10\%)} is obtained when additional data is involved, either they are used for pre-training or fine-tuning. 
% Our outstanding performance also does not request any extra data.
Despite not using any extra data, the Top-1 recall accuracy of our method (ExCae) boosts to \textbf{55.0}\% and \textbf{56.1}\% in text-to-video and video-to-text retrieval, respectively, when using ViT-B/16 as the base model.
We are also encouraged by the observation that ExCae's performance demonstrates an increase in correlation with the enlarged scale of the backbone model, resulting in an around \textbf{7\%} rise of R@1 from ViT-L/14 to Vit-G/14. Our results have comprehensively surpassed the current SOTA pre-training models, such as mPLUG-2 and VAST, when using the Vit-G/14 as the backbone. The rising trend of performance indicates the potential for further improvement of our method through scaling up the base model.
Similar results are obtained in experiments on MSVD and DiDeMo datasets, as recorded in Table~\ref{tab:msvd_comparison}. Our results are comparable to or even superior to those of pre-training models. 
Compared to existing ViT-based methods, ExCae shows a resounding victory, with Top-1 recall accuracy suppressing the best one by \textbf{7\%} on MSVD and \textbf{13\%} on DiDeMo. 
These results validate that ExCae is robust to VRT tasks for different data sources. 

Moreover, we demonstrate several VTR cases to show our results more intuitively.
Fig. \ref{fig.comp_t2v_example} presents the top three videos retrieved when given a specific caption. It is shown that 
our method successfully picks the ground-truth videos in the first place. We appreciate seeing that the second and third hit videos are also highly related to the sentences in terms of content.
In Fig. \ref{fig.comp_v2t_example}, we display the text located at the top of the retrieval list when providing a video. Our results are compared to the ``Base" obtained without any caption enhancement.
% Besides, Figure \ref{fig.comp_v2t_example} compares our video-to-text retrieval results to the ``Base" obtained without any caption enhancement.
The ground-truth texts normally take the first rank when using our method. Compared to the base results, our method preferentially selects samples with semantics and descriptive perspectives similar to the ground truths.
For example, our method successfully recognizes keywords of ``voices", ``critic", and ``guitar" that are missed by the Base model.
This is owing to the ECS module that adaptively learns the cross-modal matching for specific samples, benefiting the recognition of key semantics during video-text retrieval.


\begin{figure*}[t]
	\centering
	\includegraphics[width=0.95\textwidth]{sec/images/t2v_demo_new.png}
        \vspace{-0.5em}
	\caption{Examples of text-to-video retrieval results on MSR-VTT dataset.  \textit{(The ground truths are marked in a red box.)}}
	\label{fig.comp_t2v_example}
\end{figure*}



\begin{figure*}[t]
	\centering
	\includegraphics[width=0.95\textwidth]{sec/images/v2t_demo_new.png}
        \vspace{-0.5em}
	\caption{Examples of video-to-text retrieval results on MSR-VTT dataset.  \textit{(The ground truths are marked in a red box.)}}
	\label{fig.comp_v2t_example}
\end{figure*}




\subsection{Plug-in Experiment} 
ExCae is encapsulated and supplemented with existing video-text retrieval methods, whose results are compared to those of the original. As shown in Table \ref{tab:plug-in}, our method successfully increases retrieval results of different video-text retrieval methods. The Top-1 recall accuracy is boosted by \textbf{4.8\%}, \textbf{9.4\%} and \textbf{7.7\%} for DRL, TSR-Net and Clip4Clip, respectively. It is observed that the results of plug-in experiments are similar to those obtained by our method with the same backbone \textit{(refer to Table \ref{tab:msr_comparison})}. This indicates that ExCae has strong adaptability and expandability, which can flexibly combine with different models and constantly upgrade its performance as the technology evolves.
% \begin{table*}[th]
%     \centering
%     \small
%   %  \begin{tabular}{lp{1.4em}<{\centering}p{2em}<{\centering}|p{2.2em}<{\centering}p{2.2em}<{\centering}cc|p{2.2em}<{\centering}p{2.2em}<{\centering}cc}
%     \begin{tabular}{l|cccc|cccc}
%         \toprule
%         \multirow{2}{*}{Method}  &  \multicolumn{4}{c|}{text-to-video}   & \multicolumn{4}{c}{video-to-text}   \\
%         \cline{2-9}
%            & R@1$\uparrow$ &  R@5$\uparrow$ &  R@10$\uparrow$ & MeanR$\downarrow$ & R@1$\uparrow$ &  R@5$\uparrow$ &  R@10$\uparrow$    & MeanR$\downarrow$   \\
%         \midrule
%         CLIP2TV \cite{gao2021clip2tv} & 49.3& 74.7 &83.6 &13.5& 46.9 &75.0 &85.1 &10.0 \\
%         DRL \cite{wang2022disentangled}   & 49.4& 76.4 &84.2 & 13.2& 47.0 &77.1 &84.4 &9.2 \\
%         TS2-Net \cite{liu2022ts2}  &47.8 &76.8 &85.2 & 13.7& 47.8 &76.0& 84.6 &8.5 \\
%         Clip4Clip \cite{luo2022clip4clip} & 46.4  & 72.1 &  82.0 &  14.7  & 45.4 &  73.4  & 82.4 & 10.7 \\
%         X-CLIP \cite{ma2022x}  & 49.3&   75.8 &  84.8 &   12.2 &  48.9 &  76.8&   84.5 &   8.1\\
%         DMAE \cite{jiang2023dual}  &  49.9 & 75.8 & 85.5 &  12.5  & 49.6 & 76.3& 85.0&  8.5  \\
%         Cap4Video \cite{wu2023cap4video}  & 51.4&  75.7&  83.9 & 12.4 & 49.0 & 75.2 & 85.0 & 8.0 \\
%     % \hline ExCae
%     \cdashline{1-9}
%         & 60.5	 & 87.3 & 	95.0	 & 3.8	 & 62.5	 & 89.0	 & 94.1 & 	4.1 \\
%         & 62.2	 &88.2 &	93.7 &	3.9	& 65.1	&89.4&	94.0	&3.8  \\
%          &  \underline{\textbf{67.7}} & 	 \underline{\textbf{92.7}}	 &  \underline{\textbf{96.2}}	 &  \underline{\textbf{2.9}}	 &  \underline{\textbf{69.3}}	 &  \underline{\textbf{92.5}}	 &  \underline{\textbf{96.3}}	 &  \underline{\textbf{2.3}}   \\
%          \bottomrule
%     \end{tabular}
%      \caption{Comparison with SOTA methods on MSR-VTT dataset.}
%     \label{tab:plug-in}
% \end{table*}

