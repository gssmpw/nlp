\section{Introduction}
\label{sec:intro}
Video-text understanding has become a new mainstream research with the population of short videos on social networks. In virtue of the learning ability of large models, multi-modal pre-training models \cite{li2023unmasked,chen2024vast} are developed and achieve initial success in video-text retrieval (VTR) tasks. However, a stunning amount of data and computing resources are the essential basis to support the effectiveness of these pre-training models.
A lighter and more feasible approach to achieve VTR is adapting the image-text pretrained model to video-text scenarios \cite{luo2022clip4clip,xue2022clip}. In this approach, 
the visual embeddings are adapted to the video field by introducing temporal information \cite{sun2022long-form,Chen2022LiteVLEV} or refining the visual-text matching strategies \cite{gao2021clip2tv,xue2022clip}. 
% They adapt the visual embeddings to the video field by incorporating temporal information \cite{sun2022long-form,Chen2022LiteVLEV} or refining the visual-text matching strategies \cite{gao2021clip2tv,xue2022clip}. 
But unlike image-text pairs, the textual descriptions of videos tend to be overly simplistic and one-sided, describing partial \textit{(even a single)} frames of the videos. 
The improper alignment between one-sided descriptions and multifaceted video embeddings would confuse text understanding \textit{(Fig. \ref{fig:gap}~(a))}, resulting in biased cross-modal retrieval, and a bottleneck in performance improvement arises.

% The improper alignment between texts and 
% These one-sided descriptions would enlarge the modality gap, confusing cross-modal assignment

% mproperly matched to multifaceted video embeddings \textit{(Figure \ref{fig:gap} (a))}, resulting in biased cross-modal retrieval, and a bottleneck in performance improvement arises.

% As shown in Figure \ref{fig:gap}, the text describing a specific frame \textit{(red box)} is 

\begin{figure}[t]
\centering
\includegraphics[width=0.98\columnwidth]{sec/images/gap_new2.png} 
\caption{Comprehension gap between videos and texts}
\label{fig:gap}
\end{figure}

Frame sampling is one proper solution to handle the improper alignment issue. For instance, ClipBERT \cite{lei2021less} sparsely samples video frames to meet the text descriptions. X-CLIP~\cite{ma2022x} further cuts text descriptions into words, searching for video frames for fine-grained cross-modal matching. As video information is sparsely divided and broken, these methods are limited in comprehensively utilizing and exploring deep relations across videos and texts. 
Another branch to promote video-text understanding is data augmentation. Extra data, such as HD-VILA-100M \cite{xue2022advancing} and LF-VILA~\cite{sun2022long-form}, are collected to expand the receptive field for model adaptation, thereby enriching matching knowledge of the two modalities.
To address the substantial data collection burden, subsequent research has explored text rewriting strategies to enhance expressions on the text side. For example, Wu \textit{et al.} \cite{wu2023bidirectional} rewrite texts using lexical analysis, in which attributes are extracted to enhance expressive ability in the text modality. She \textit{et al.}~\cite{yu2025she} further ameliorate text presentation using syntax-tree, emphasizing key words to assist better VTR. 
% Large language models (LLMs), such as ChatGPT and LLaMA \cite{touvron2023llama}, are additionally introduced for the text rewriting tasks~\cite{liu2023visual,fan2024improving}. 
Besides generating textual descriptions, text enhancement is executed in the feature space~\cite{croitoru2021teachtext,wang2024text}, providing embedding variants for cross-modal alignment. 
Despite achieving a retrieval precision increase of up to 3\% with the aid of supplementary textual expressions, minimal additional semantics are introduced during the text-to-text rephrasing processes, which hinders further reduction of the modality gap. This restricts significant breakthroughs in cross-modal retrieval capability.

% Although up to a 3\% increase in retrieval precision is achieved with the assistance of supplementary texts, few additional semantics are introduced during these rephrasing processes, preventing further reduction of the modality gap; consequently, cross-modal retrieval capability has limited break through.

Instead, this paper suggests enhancing the representations on the video side, deriving captions from videos to meet the expression modes and semantic information of texts. The rich semantics of videos are preserved, and video-text matching would be simplified due to the reduced modality gap achieved by this directional data augmentation.
% This approach facilitates a more straightforward video-text matching process through directional data augmentation.
Although multimodal Large Language Models (mLLMs) have shown a decent ability in video understanding \cite{cap4video++}, generating appropriate video captions for VTR remains challenging, as it is revealed that the performance of mLLMs is query-sensitive. The writing quality of mLLMs relies heavily on lexicon engineering~\cite{parashar2024neglected} and hand-crafted prompts \cite{momeni2023verbs}. Recent studies~\cite{bansal2024videocon,wang2024havtr} have been mired in the meticulous work of designing better prompts for caption augmentation, limited in human empiricism. 



% Contrary to prevailing empiricism, this paper investigates reinforcement learning (RL) and proposes an Expertized Caption Auto-Enhancement (ExCae) method. ExCae adaptively generates and selectively personalizes captions for VTR, thereby alleviating the additional burden of data acquisition and eliminating the complexities associated with manual prompt design.
% Specifically, ExCae incorporates a two-stage RL framework: Caption Self-Improvement (CSI) and Expertized Caption Selection (ECS), which respectively search for the best generation and utilization of video captions according to the rewards from the video-text environment. In the CSI stage, a Captioner implemented by a multimodal LLM (mLLM) is asked to derive captions with rich content and diverse perspectives from videos. 
% An agent, \textit{Prompt engineer}, is introduced, specializing in refining the action to adjust and improve the query prompts for mLLM via self-evolution, as illustrated in Figure \ref{fig:overview} (b). 
% a Caption Self-improvement (CSI) module is designed to derive captions with rich content and diverse perspectives from videos. A \textit{Prompt engineer} is introduced in the CSI module, which gradually adjusts and improves the query prompts via self-learning. 
% With the enriched expressions on the video side \textit{(video+video-derived captions)}, cross-modal matching becomes a more challenging task. Thus, in the ECS stage, we further incorporate a \textit{Visual Selector} agent, which consists of multiple learnable experts to automatically decide and specify appropriate text expressions for cross-modal matching. 
% develop an Expertized Caption Selection (ECS) module consisting of multiple learnable experts to automatically specify appropriate expressions for cross-modal matching.
% Seeing Figure \ref{fig:gap} (b), video representation space dilates with caption augmentation, and the cross-modal information gap is reduced after expertized selection by the \textit{Visual Selector}. 
% This indicates that the proposed method improves the generalization ability through the RL-based caption auto-augmentation while enhancing the cross-modal retrieval ability with adaptive screening by the \textit{Visual Selector}. As a fully data-driven process, sampling bias and empiricism that may confuse the cross-modal matching are avoided. We further encapsulate ExCae as a plug-in unit, which successfully improves the retrieval performance of current methods, validating ExCae's flexibility and adaptability.

% \noindent The contributions of this work are summarized as follows:
% \begin{itemize}
% \item We propose eliminating the grunt workload of data acquisition and prompt design in caption augmentation through self-improvement and expertized selection, thereby mitigating empirical bias in supplemental texts while facilitating cross-modal matching without extra data.

% \item We design a two-stage RL framework for video-text retrieval, in which video expressions are expanded using a \textit{Prompt engineer} and are adaptively matched to texts under the guidance of the \textit{Visual Selector}. 
% The \textit{Prompt engineer} facilitates generating effective and multi-angled captions, while the \textit{Visual Selector} personalizes the appropriate expression angles for cross-modal matching. 

% \item Experimental results show that our method outperforms \textit{(or is comparable to)} current works even without using extra data, especially surpassing the best benchmark on the MSR-VTT by \textbf{3.8\%}. This paper also taps the application potentials of ExCae in video-text retrieval via extensive analytical and plug-in experiments.
% \end{itemize}


Inspired by automatic prompt optimization (APO) \cite{pryzant2023automatic,yang2023dynamic}, this paper proposes an Expertized Caption Auto-Enhancement (ExCae) method, as shown in Fig. \ref{fig:overview} (b).
% ExCae adaptively generates and selectively personalizes captions for VTR, thereby alleviating the additional burden of data acquisition and eliminating the complexities associated with manual prompt design.
ExCae incorporates two main components: Caption Self-Improvement (CSI) and Expertized Caption Selection (ECS) modules, which adaptively generates and selectively personalizes captions for VTR. The two modules provide ExCae with the advantage of alleviating the additional burden of data acquisition and eliminating the complexities associated with manual prompt design.
%respectively search for the best generation and utilization of video captions. 
Specifically, in the CSI module, a \textit{Captioner} implemented by a multimodal LLM (mLLM) is asked to derive captions from videos. We optimize the query prompt of the \textit{Captioner} to ensure the generated captions contain rich content and diverse perspectives. To overcome the interpretability loss of prompt in existing gradient-based APO, this paper introduces a \textit{Prompt Engineer}, which gradually improves the query prompts of the \textit{Captioner} via repetitively inquest and output adjustment.
% , which \textit{\textbf{adaptively generates}} and \textit{\textbf{personalized select}} captions for video-text retrieval, \underline{\textit{releasing extra work of data acquisition}} and \underline{\textit{avoiding the trouble of manual prompt design.}}
% Specifically, a Caption Self-improvement (CSI) module is designed to derive captions with rich content and diverse perspectives from videos. A \textit{Prompt engineer} is introduced in the CSI module, which gradually adjusts and improves the query prompts via self-learning. 
With the enriched expressions on the video side \textit{(video+video-derived captions)}, the ECS module consisting of multiple learnable experts is designed to automatically specify appropriate expressions for cross-modal matching. 
Seeing Fig. \ref{fig:gap} (b), video representation space dilates with caption augmentation, and the cross-modal information gap is reduced after expertized selection. This indicates that the proposed method improves the generalization ability through caption augmentation while enhancing the cross-modal retrieval ability with adaptive screening. As a fully data-driven process, sampling bias and empiricism that may confuse the cross-modal matching are avoided. We further encapsulate ExCae as a \underline{\textit{plug-in unit}}, which successfully improves the retrieval performance of current methods, validating ExCae's flexibility and adaptability.

\noindent The contributions of this work are summarized as follows:
\begin{itemize}
\item We propose reducing the modality comprehension gap for VTR by enhancing visual presentations via self-improved caption derivation from videos. Video captions are progressively refined, bridging video expression closer to text space while eliminating the grunt workload of data acquisition.
% by bringing video expression closer to textual representation via
% eliminating the grunt workload of data acquisition and prompt design in caption augmentation through self-improvement and expertized selection, thereby mitigating empirical bias in supplemental texts while facilitating cross-modal matching without extra data.

\item We design an automatic video-text learning framework for VTR, in which video expressions are expanded using a \textit{Prompt Engineer} and are matched with texts under the guidance of learnable experts. The \textit{Prompt Engineer} facilitates effective and multi-angle caption generation through a self-improvement strategy that mitigates empirical bias in prompt design, while the learnable experts personalize the appropriate expression angles for cross-modal matching. 

\item Experimental results show that our method outperforms \textit{(or is comparable to)} current works even without using extra data, especially surpassing the best benchmark on the MSR-VTT by \textbf{3.8\%}. This paper also taps the application potentials of ExCae in video-text retrieval via extensive analytical and plug-in experiments.
\end{itemize}

\begin{figure}[t]
\centering
\includegraphics[width=0.95\columnwidth]{sec/images/overview-ExCae-h.png} 
\vspace{-0.5em}
\caption{Comparison of our method to the traditional}
\label{fig:overview}
\end{figure}

% We propose eliminating the grunt workload of data acquisition and prompt design in caption augmentation through self-improvement and expertized selection, thereby mitigating empirical bias in supplemental texts while facilitating cross-modal matching without extra data.
