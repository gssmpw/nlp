\section{Related Work}
\label{sec:related_work}

Developing and pre-training video foundation models (VFMs) \cite{li2023unmasked,wang2022internvideo} enhances the comprehension of video data, demonstrating efficacy in downstream tasks such as video-text retrieval. However, the effectiveness of VFMs heavily relies on huge amounts of data and computing resources. 
% CLIP \cite{radford2021learning} is an exceptionally powerful cross-modal model that has achieved significant success in image-text retrieval. 
Much of the current work in video-text retrieval turns to a lighter modeling approach, adapting image-text retrieval models, such as CLIP \cite{radford2021learning}, to video-text tasks.
For example, CLIP4Clip \cite{luo2022clip4clip} and CLIP2TV \cite{gao2021clip2tv} 
transform CLIP's embeddings into effective video representations through various frame aggregation strategies. 
In addition, X-CLIP \cite{ma2022x} proposes using an independent encoder for each frame to derive video representations.
Video embeddings are further enhanced in TS2-Net \cite{liu2022ts2}, which dynamically selects video tokens with richer information. 
Transfer learning is another appropriate solution to adapt knowledge from image to video field, based on which Jin \textit{et al.} \cite{jin2024mv} introduce a transformation block to adaptively transfer knowledge in the pre-trained CLIP from image-text to video-text.
This adaptation is also achieved by a teacher-student framework in TeachClip \cite{holistic}.
Considering the feature redundancy issue, DRL \cite{wang2022disentangled} designs a loss in channel decorrelation regularization to improve the quality of video representation. 
However, these methods suffer from ill-suited cross-modal matching due to the information imbalance between videos and texts. 
% utilize the image knowledge encapsulated in CLIP, transforming it into effective video representations through different image aggregation networks.
% In addition, TS2-Net \cite{liu2022ts2} dynamically adjusts the video token sequence, selecting tokens with richer information to enhance video representation. 

% X-CLIP \cite{ma2022x} employs an independent encoder for each frame, using a network that aggregates frames multiple times to generate the final video representation. 

% DRL \cite{wang2022disentangled} introduces an additional loss to address the feature redundant issue. 


To improve cross-modal matching, existing works, such as BIKE \cite{wu2023bidirectional} and SHE-NET \cite{yu2025she}, enhance text expression with lexical and syntax analysis. Although Large language models (LLMs), such as ChatGPT and LLaMA \cite{touvron2023llama}, are additionally introduced in the text rewriting tasks~\cite{du2024reversed,liu2023visual,fan2024improving}, visual details remain deficient in the rewritten texts through these text-to-text literal restatements.
Generating captions from videos as auxiliaries is a brighter approach for text enhancement. These methods form new pairs using videos and derived captions to promote cross-modal representation learning \cite{cap4video++}. 
However, carefully designed prompts are necessary, and later works manually craft various query prompts, asking LLMs to describe empirically selective concepts, such as special objectives \cite{parashar2024neglected} or actions \cite{momeni2023verbs}. 
The hand-drafted lexicon and prompts severely limit the rewrite quality, motivating us to explore a self-learning caption enhancement method. 
To the best of our knowledge, this paper is a \textit{\textbf{pioneer}} in investigating \underline{\textit{\textbf{automatic caption enhancement}}} for videos to facilitate the comprehension of videos and texts.
% texts with empirically selective concepts, such as special objectives \cite{parashar2024neglected} or actions \cite{momeni2023verbs}. 


 % They carefully designed prompts, asking LLMs to rephrase the texts with empirically selective concepts, such as special objectives \cite{parashar2024neglected} or actions \cite{momeni2023verbs}. 
 %However, visual details remain deficient in the rewritten captions through these text-to-text literal restatements.

% Moreover, the hand-drafted lexicon and prompts severely limit the rewrite quality. 
% This motivates us to explore another data augmentation method from the video perspective.








% All text must be in a two-column format.
% The total allowable size of the text area is $6\frac78$ inches (17.46 cm) wide by $8\frac78$ inches (22.54 cm) high.
% Columns are to be $3\frac14$ inches (8.25 cm) wide, with a $\frac{5}{16}$ inch (0.8 cm) space between them.
% The main title (on the first page) should begin 1 inch (2.54 cm) from the top edge of the page.
% The second and following pages should begin 1 inch (2.54 cm) from the top edge.
% On all pages, the bottom margin should be $1\frac{1}{8}$ inches (2.86 cm) from the bottom edge of the page for $8.5 \times 11$-inch paper;
% for A4 paper, approximately $1\frac{5}{8}$ inches (4.13 cm) from the bottom edge of the
% page.

% %-------------------------------------------------------------------------
% \subsection{Margins and page numbering}

% All printed material, including text, illustrations, and charts, must be kept
% within a print area $6\frac{7}{8}$ inches (17.46 cm) wide by $8\frac{7}{8}$ inches (22.54 cm)
% high.
% %
% Page numbers should be in the footer, centered and $\frac{3}{4}$ inches from the bottom of the page.
% The review version should have page numbers, yet the final version submitted as camera ready should not show any page numbers.
% The \LaTeX\ template takes care of this when used properly.



% %-------------------------------------------------------------------------
% \subsection{Type style and fonts}

% Wherever Times is specified, Times Roman may also be used.
% If neither is available on your word processor, please use the font closest in
% appearance to Times to which you have access.

% MAIN TITLE.
% Center the title $1\frac{3}{8}$ inches (3.49 cm) from the top edge of the first page.
% The title should be in Times 14-point, boldface type.
% Capitalize the first letter of nouns, pronouns, verbs, adjectives, and adverbs;
% do not capitalize articles, coordinate conjunctions, or prepositions (unless the title begins with such a word).
% Leave two blank lines after the title.

% AUTHOR NAME(s) and AFFILIATION(s) are to be centered beneath the title
% and printed in Times 12-point, non-boldface type.
% This information is to be followed by two blank lines.

% The ABSTRACT and MAIN TEXT are to be in a two-column format.

% MAIN TEXT.
% Type main text in 10-point Times, single-spaced.
% Do NOT use double-spacing.
% All paragraphs should be indented 1 pica (approx.~$\frac{1}{6}$ inch or 0.422 cm).
% Make sure your text is fully justified---that is, flush left and flush right.
% Please do not place any additional blank lines between paragraphs.

% Figure and table captions should be 9-point Roman type as in \cref{fig:onecol,fig:short}.
% Short captions should be centred.

% \noindent Callouts should be 9-point Helvetica, non-boldface type.
% Initially capitalize only the first word of section titles and first-, second-, and third-order headings.

% FIRST-ORDER HEADINGS.
% (For example, {\large \bf 1. Introduction}) should be Times 12-point boldface, initially capitalized, flush left, with one blank line before, and one blank line after.

% SECOND-ORDER HEADINGS.
% (For example, { \bf 1.1. Database elements}) should be Times 11-point boldface, initially capitalized, flush left, with one blank line before, and one after.
% If you require a third-order heading (we discourage it), use 10-point Times, boldface, initially capitalized, flush left, preceded by one blank line, followed by a period and your text on the same line.

% %-------------------------------------------------------------------------
% \subsection{Footnotes}

% Please use footnotes\footnote{This is what a footnote looks like.
% It often distracts the reader from the main flow of the argument.} sparingly.
% Indeed, try to avoid footnotes altogether and include necessary peripheral observations in the text (within parentheses, if you prefer, as in this sentence).
% If you wish to use a footnote, place it at the bottom of the column on the page on which it is referenced.
% Use Times 8-point type, single-spaced.


% %-------------------------------------------------------------------------
% \subsection{Cross-references}

% For the benefit of author(s) and readers, please use the
% {\small\begin{verbatim}
%   \cref{...}
% \end{verbatim}}  command for cross-referencing to figures, tables, equations, or sections.
% This will automatically insert the appropriate label alongside the cross-reference as in this example:
% \begin{quotation}
%   To see how our method outperforms previous work, please see \cref{fig:onecol} and \cref{tab:example}.
%   It is also possible to refer to multiple targets as once, \eg~to \cref{fig:onecol,fig:short-a}.
%   You may also return to \cref{sec:formatting} or look at \cref{eq:also-important}.
% \end{quotation}
% If you do not wish to abbreviate the label, for example at the beginning of the sentence, you can use the
% {\small\begin{verbatim}
%   \Cref{...}
% \end{verbatim}}
% command. Here is an example:
% \begin{quotation}
%   \Cref{fig:onecol} is also quite important.
% \end{quotation}

% %-------------------------------------------------------------------------
% \subsection{References}

% List and number all bibliographical references in 9-point Times, single-spaced, at the end of your paper.
% When referenced in the text, enclose the citation number in square brackets, for
% example~\cite{Authors14}.
% Where appropriate, include page numbers and the name(s) of editors of referenced books.
% When you cite multiple papers at once, please make sure that you cite them in numerical order like this \cite{Alpher02,Alpher03,Alpher05,Authors14b,Authors14}.
% If you use the template as advised, this will be taken care of automatically.

% \begin{table}
%   \centering
%   \begin{tabular}{@{}lc@{}}
%     \toprule
%     Method & Frobnability \\
%     \midrule
%     Theirs & Frumpy \\
%     Yours & Frobbly \\
%     Ours & Makes one's heart Frob\\
%     \bottomrule
%   \end{tabular}
%   \caption{Results.   Ours is better.}
%   \label{tab:example}
% \end{table}

% %-------------------------------------------------------------------------
% \subsection{Illustrations, graphs, and photographs}

% All graphics should be centered.
% In \LaTeX, avoid using the \texttt{center} environment for this purpose, as this adds potentially unwanted whitespace.
% Instead use
% {\small\begin{verbatim}
%   \centering
% \end{verbatim}}
% at the beginning of your figure.
% Please ensure that any point you wish to make is resolvable in a printed copy of the paper.
% Resize fonts in figures to match the font in the body text, and choose line widths that render effectively in print.
% Readers (and reviewers), even of an electronic copy, may choose to print your paper in order to read it.
% You cannot insist that they do otherwise, and therefore must not assume that they can zoom in to see tiny details on a graphic.

% When placing figures in \LaTeX, it's almost always best to use \verb+\includegraphics+, and to specify the figure width as a multiple of the line width as in the example below
% {\small\begin{verbatim}
%    \usepackage{graphicx} ...
%    \includegraphics[width=0.8\linewidth]
%                    {myfile.pdf}
% \end{verbatim}
% }


% %-------------------------------------------------------------------------
% \subsection{Color}

% Please refer to the author guidelines on the \confName\ \confYear\ web page for a discussion of the use of color in your document.

% If you use color in your plots, please keep in mind that a significant subset of reviewers and readers may have a color vision deficiency; red-green blindness is the most frequent kind.
% Hence avoid relying only on color as the discriminative feature in plots (such as red \vs green lines), but add a second discriminative feature to ease disambiguation.