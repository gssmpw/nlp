\section{Discussion}
\subsection{Superiority}
ExCae attains remarkable video-text retrieval performance primarily due to its efficient narrowing of the video-text modality gap. As shown in Fig. \ref{fig:convergence} (a), the representations of video-text pairs are drawn closer to each other through self-learning diffusional data augmentation (CSI) and self-adaptive cross-modal matching  (ECS).
The modality gap $||\Delta||$ \cite{liang2022mind} is reduced from 0.82 to 0.75, from 0.79 to 0.67, and from 0.84 to 0.74 on MSR-VTT, MSVD and DiDeMo, respectively.  
Moreover, ExCae has advantages in objectivity, generalization and self-adaptation.
1) ExCae is a \underline{\textit{\textbf{data-driven method}}}, in which the query prompts and caption selection are learned according to the training data. No additional human work \textit{(despite initialization)}, such as prompt design and lexicon establishment, is involved in the whole learning process. Thus, ExCae effectively avoids the adverse effects of empiricism.
2) As the \textit{Captioner} is asked to generate multi-angle captions using a diversity constraint, data with more expression perspectives are provided for model learning. Therefore, ExCae inherits the strength of \underline{\textit{\textbf{generalization improvement}}} from data augmentation.
3) ExCae also has \underline{\textit{\textbf{personalized ability}}} owing to the parametric learning of the ECS module. The router in the ECS module would adaptively select the best expressions for cross-modal retrieval, thus maintaining the robustness of ExCae in different applications. 



\begin{figure}[t]
\centering
\includegraphics[width=0.98\columnwidth]{sec/images/gap_convergence2.png} 
\caption{Modality gap and Convergence on MSR-VTT}
\label{fig:convergence}
\end{figure}


\subsection{Convergence}
Since the generated captions are changeless once the optimal prompt is determined, the cross-modal post-training of our method owns the same convergence as existing ViT-based methods. 
Regarding learning the CSI module, the LLM would optimize query prompts in the given direction. With the increase of iteration, the semantic changes in query prompts become smaller, and thus the caption evaluation scores tend to be stable \textit{(See Fig. \ref{fig:convergence} (b))}.
According to expertise from experiments, the CSI module tends to converge in 400 iterations.


\subsection{Storage and Efficiency}
Although a prompt optimization process is additionally involved, we believe that the storage and computation cost of ExCae is appreciable, especially compared to the pre-training methods. 
With regard to storage, no extra data or related lexicon needs to be stored. We only need to occupy a very small amount of storage space to record several generated captions in each epoch, which greatly reduces the storage cost. For the computation cost, our method is more efficient than the multi-modal foundation models without a pre-training process. In comparison to the ViT-based methods, the only additional computation is from the training of the CSI module. However, prompt optimization is a one-off training process, and we have also found that the learned prompt has generalization ability across different data sets. Therefore, the efficiency of our approach is comparable to existing VTR methods. 

