

\subsection{Ablation Studies}
Our method expands the expression on the video side; thus, this section respectively evaluates the key components, CSI and ECS modules, under different input data cases. To test the effectiveness of the CSI module, we replace the optimal prompt with the initial one to simulate the ablation of the CSI module.  
The removal of the ECS module is realized by directly encoding all the video-side data as visual embeddings. 
As shown in Table \ref{tab:ablation}, the R@1 is around 50\% when only using the original video as video-side data without adding any components; this result is close to the existing ViT-based methods. The ECS module shows only slight positive effects on the original video.  
Similar results can be found if we merely change the video-side data from the original videos to video-derived captions. An obvious performance boost occurs when the original videos and video-derived captions are input in combination, validating the indispensability of these two data. 
In addition, we find that both CSI and ECS modules facilitate cross-modal retrieval. Despite in the absence of the other one, our method achieves over \textbf{6\%} gain in Top-1 recall accuracy. The effectiveness of the CSI and ECS modules proves the superiority of captions' auto-enhancement and multi-perspective cross-modal matching. The best results are obtained by our full method, where both the CSI and ECS modules play mutually reinforcing roles in promoting video-text understanding.

\subsection{Analytical Experiments}

\begin{table}[t]
    \centering
     \caption{Results of plug-in experiments on MSR-VTT dataset.}
     \vspace{-0.8em}
   \begin{tabular}{l|p{1.5em}<{\centering}p{1.5em}<{\centering}p{2.2em}<{\centering}|p{1.5em}<{\centering}p{1.5em}<{\centering}p{2.2em}<{\centering}}
    % \begin{tabular}{l|ccc|ccc}
        \toprule
        \multirow{2}{*}{Method}  &  \multicolumn{3}{c|}{text-to-video}   & \multicolumn{3}{c}{video-to-text}   \\
        \cline{2-7}
           & R@1 &  R@5&  R@10 & R@1 &  R@5 &  R@10      \\

        \midrule
        DRL \cite{wang2022disentangled}  & 49.4 & 76.4 & 84.2 & 47.0 & 77.1 & 84.4 \\
        ~~~~~~~~ ~ + Ours & 54.2 & 81.4 & 90.2 & 54.4 & 81.1 & 89.2 \\
         
         \midrule  
        TS2-Net \cite{liu2022ts2}  & 47.8 & 76.8 & 85.2 & 47.8 & 76.0 & 84.6 \\
        ~~~~~~~~ ~ + Ours & 57.2 & 86.2 & 92.6 & 56.4 & 85.0 & 91.3  \\
        
        \midrule  
        Clip4Clip \cite{luo2022clip4clip}  & 46.4  & 72.1 &  82.0 &  45.4 &  73.4  & 82.4 \\
        ~~~~~~~~ ~ + Ours & 54.1 & 82.1 & 89.5 & 51.2 & 80.6 & 88.8 \\

    % \hline ExCae
         \bottomrule
    \end{tabular}
    
    \label{tab:plug-in}
\end{table}


\paragraph{\textit{Effect of video-derived caption number ($N_c$)}}
Fig. \ref{fig:analytical_cn} plots the video-text retrieval results when raising $N_c$ from 1 to 10. The performance is really poor if only one caption is derived, with a Top-1 recall accuracy of lower than 30\% on text-to-video retrieval. This result is inferior to any existing methods, reflecting that insufficient caption enhancement would even damage cross-modal learning. The reason is that all generated captions are forcibly activated to match the text if too few caption samples are input to the ECS module. Consequently, the original text may align with a description from a completely different perspective.
Results are much better when increasing the number of generated captions to 3, achieving good performance close to the existing pre-training models. The performance further improves with the increase of the caption number, and the upward trend becomes stable when $N_c$ is greater than 7. This experiment suggests an approximate range of $[7,10]$ for the selection of video-derived caption numbers. 



% \paragraph{\textit{b) Effect of caption number}}
\paragraph{\textit{Effect of activated expert number ($N_e$)}}
Video-text retrieval results with different $N_e$ values are displayed in Fig. \ref{fig:analytical_en}. We also show the results obtained without the ECS module, labeled as 0 activated expert, as a reference. The retrieval results gradually improve as $N_e$ increases from 0 to 2. 
However, no further improvement in performance is achieved with the subsequent increase of activated experts. This may be because ECS would automatically favor partial experts and suppress the attention to less relevant expressions, although the majority of experts are activated.

\begin{table*}[t]
	\centering
     \caption{Ablation studies of critical components}
 \small
	\begin{tabular}{c|cc||c|c|c|c|c|c|c|c}
		\toprule
		 &\multicolumn{2}{c||}{ Component}  &  \multicolumn{4}{c|}{text-to-video}   & \multicolumn{4}{c}{video-to-text} \\
   \cline{2-11}
      Video-side Data   &CSI &ECS & R@1$\uparrow$ &  R@5$\uparrow$ &  R@10$\uparrow$ & MeanR$\downarrow$ & R@1$\uparrow$ &  R@5$\uparrow$ &  R@10$\uparrow$    & MeanR$\downarrow$  \\
		\midrule
	 \multirow{2}{*}{Video} & & & 49.9 &	78.1 &	\underline{88.1} &	\underline{7.9}	& 49.6 &\underline{78.8}&	\underline{87.3}	& \underline{7.3} \\
    && \ding{51}&   \underline{52.9}	&\underline{78.5}	& 86.7&	10.1	&\underline{51.3}&	78.5&	87.1	&10.3 \\
  \midrule
	&&& 51.9	& 79.7	&88.3	&9.1	&51.4&	79.8	&87.1&	10.9  \\
   \multirow{2}{*}{ Video-derived Caption}  &\ding{51} && 53.1 &	78.6	&86.7&	10.2	&51.0	&78.6	&87.4&	10.6   \\
	 	&&\ding{51} &   52.3 &	78.2	& 87.9 & 9.0	 & 51.1 &	80.6 &	87.3	& 9.0 \\
		&\ding{51}&\ding{51}&  \underline{54.3}	& \underline{80.3} &	\underline{90.0}	&\underline{6.7}	&\underline{51.8}&	\underline{80.9}	&\underline{88.5}&	\underline{6.8} \\
		\midrule
        & & &   56.2 &	83.7 &	90.9	 &7.2  &	56.9 &	84.8 &	91.4 &	6.6      \\
        Video+ &\ding{51} & &  63.2 &	90.1	&95.2 &	3.3 &	65.0 &	89.9 &	94.6 &	3.3 \\
		\multirow{2}{*}{ Video-derived Caption} 	&&\ding{51} &63.3 &	89.1 &	95.0	 &4.3 &	62.9 &	89.1	 &94.2 &	4.2\\
		
		&\ding{51}&\ding{51}& \underline{67.7} & 	\underline{92.7}	 & \underline{96.2}	 & \underline{2.9}	 & \underline{69.3}	 & \underline{92.5}	 & \underline{96.3}	 & \underline{2.3}\\
		\bottomrule
	\end{tabular}

	\label{tab:ablation}
\end{table*}

\begin{figure}[t]
\centering
\includegraphics[width=0.98\columnwidth]{sec/images/sensitivity_cn.png} 
\caption{Sensitivity to the number of video-derived captions}
\label{fig:analytical_cn}
\end{figure}


\begin{figure}[t]
\centering
\includegraphics[width=0.98\columnwidth]{sec/images/sensitivity_en.png} 
\caption{Sensitivity to the number of activated experts}
\label{fig:analytical_en}
\end{figure}

