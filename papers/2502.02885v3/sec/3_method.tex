\begin{figure}[t]
\centering
\includegraphics[width=0.98\columnwidth]{sec/images/CSI.png} 
 \vspace{-0.5em}
\caption{Illustration of the CSI module}
\label{fig:CSI}
\end{figure}

\section{Method}
Unlike traditional text augmentation that rewrites captions on the text side, our method (ExCae) automatically derives captions from videos by the CSI module and adaptively selects appropriate expressions (ECS module) to encode video for video-text matching \textit{(see Fig. \ref{fig:overview})}. Video and text embeddings would be aligned with standard cross-modal contrastive loss, as most existing SOTA methods \cite{liu2022ts2,luo2022clip4clip}. CSI and ECS are self-learning modules whose details are introduced as follows. 

\subsection{Caption Self-improvement (CSI)}
In the CSI module, a \textit{Captioner} is included, which is an mLLM asked to generate a set of captions based on a query prompt to enhance video understanding. As intermediaries for video-text matching, the captions derived from videos are expected to be informative and text-oriented. To achieve this goal, CSI performs an iterative prompt refinement (Fig.~\ref{fig:CSI}), gradually updating the query prompts of the \textit{Captioner} by repeatedly inquiring a \textit{Prompt Engineer}. 
% In each iteration, the current query prompt is evaluated from two aspects (caption scoring and semantic assessment), resulting in the update of the prompt candidates' pool.
In each iteration, the current query prompt is evaluated by caption scoring, an approximate assessment of VTR performance, resulting in the update of the prompt candidate.
The \textit{Prompt engineer} then rewrites the current best prompt, whose output is used as the latest query prompt of the \textit{Captioner}. This prompt update iteration would end when no new candidates are added. Pseudo code is provided in Algorithm \ref{alg:CSI}. 

% The main flow of CSI is illustrated in Figure~\ref{fig:CSI}.
% Initially, the 









% The main flow is illustrated in Figure~\ref{fig:CSI}, which is an iteration process gradually refining the query prompt. Specifically, a \textit{Captioner} is asked to generate multiple captions to describe the video from \textit{\textbf{different perspectives}}. We then evaluate the semantics of the generated captions, scoring them to update the prompt candidates' pool. A \textit{Prompt engineer} is introduced to rewrite the current best prompt, whose output is used as the latest query prompt of the \textit{Captioner}. 

% This prompt update iteration would end when no new candidates are added to the prompt candidates' pool. Pseudo code is provided in Algorithm \ref{alg:CSI}.

\paragraph{\textbf{\textit{Caption scoring}}} 
We hope that the adjustment of the query prompt will be positively correlated with the increase in VTR results throughout the iterative process. Directly evaluating VTR results as feedback for prompt refinement involves multimodal encoder optimization, where slight fluctuations in embeddings can significantly affect the direction of prompt optimization. 
Therefore, this paper designs a scoring mechanism that evaluates the quality of generated captions, implicitly reflecting the performance of VTR.
Denote the query prompt for the \textit{Captioner} at iteration $t$ as $p^t$. The generated caption list for video $v_i$, using $p^t$ is denoted as $\mathbf{c}_i^t = \{c_{i,1}^t, c_{i,2}^t, \ldots, c_{i,K}^t\}$, where $K$ is the number of captions per video.
To balance semantic fidelity and diversity, $\mathbf{c}_i^t$ is evaluated from two perspectives:    
\begin{enumerate}
    \item \textbf{Semantic Consistency}: Ensures $\mathbf{c}_i^t$ semantically align with the original text $t_i$, mitigating the presence of cluttered and vacuous descriptions.
    \item \textbf{Caption Diversity}: Encourages multi-angle descriptions in $\mathbf{c}_i^t$, avoiding redundancy to maintain the original intention of expression enhancement.
\end{enumerate}

\noindent The overall score for iteration $t$ is computed as:
\begin{equation}
Score^t = \mathbb{E}_{i,k}[sim(\psi(c^t_{i,k}),\psi(t_i))] + div(\psi(\mathbf{c}^t_i))
\label{eq:score}
\end{equation}
where $k \in [1,K]$ is the caption index, and $\psi(\cdot)$ represents a semantic extractor. $sim(\cdot)$ calculates the cosine similarity on semantics. $div(\cdot)$ quantifies diversity within $\mathbf{c}^t_i)$:
\begin{equation}
\hspace{-0.5em} div(\psi(\mathbf{c}^t_i)) = \frac{1}{K(K-1)} \sum_{p \neq q} \left( 1 - sim \left( \psi(c_{i,p}^t), \psi(c_{i,q}^t) \right) \right)
\label{eq:div}
\end{equation}
% are similarity and diversity calculations. In specific, $sim(\cdot)$ is computed by consine similarity, \textit{i.e.}, $sim(\psi(c^t_{i,k}),\psi(t_i))=\frac{\psi(c^t_{i,k}) \cdot \psi(t_i)}{\|\psi(c^t_{i,k})\| \|\psi(t_i)\|}$.


% , implemented by \textit{M3-embedding}~\cite{chen2024bge}.


% where $\psi(\cdot)$ is a semantic embedding extractor (e.g., \textit{M3-embedding}~\cite{chen2024bge} ), $\text{sim}(\cdot)$ measures cosine similarity, and $\text{div}(\cdot)$ quantifies diversity.


\noindent The higher value of $Score^t$ indicates better alignment with the original text and greater caption diversity. 
If $Score^t$ surpasses the historical records, the prompt candidate is updated by $p^t$, and the best prompt used for prompt refinement is updated accordingly.


% If $\text{Score}^t$ exceeds historical records, $p^t$ is added to the prompt candidate pool for refinement.


% Denote the query prompt of the \textit{Captioner} as $p^t$, where $t$ records the iteration index.
% To ensure the
% Write the generated caption list for a video $v_i$ based on $p^t$ is written as $c^t_i$. 
% As literal expressions, $c^t_i$ is evaluated from a semantic perspective. We advocate for the generation of captions that are semantically akin to the original texts, in order to mitigate the presence of cluttered and vacuous descriptions. However, merely preserving the semantic consistency may lead to generating a set of captions similar to the original text. This is opposite to the original intention of expression enhancement. To promote \textbf{\textit{multi-angle}} descriptions, a diversity constraint that encourages variety within the caption list is supplemented. Suppose $K$ captions are contained in $c^t_i$, the caption generation score in the $t$ iteration round can be expressed as
% \begin{equation}
% Score^t = \mathbb{E}_{i,k}[sim(\psi(c^t_{i,k}),\psi(t_i))] + div(\psi(c^t_i))
% \label{eq:score}
% \end{equation}
% where $t_i$ denotes the original text corresponding to $v_i$, $k\in[1,K]$ is the caption index in $c^t_i$. 
% Suppose \( K \) captions are contained in \( c^t_i \), the caption generation score in the \( t \) iteration round can be expressed as:
% \begin{equation}
% \text{Score}_t = \mathbb{E}_{i,k}[\text{sim}(\psi(c^t_{i,k}), \psi(t_i))] + \text{div}(S)
% \label{eq:score}
% \end{equation}
% where \( t_i \) denotes the original text corresponding to \( v_i \), \( k \in [1, K] \) is the caption index in \( c^t_i \), and \( S \) is the similarity matrix between the captions, defined as \( S[i,j] = \text{sim}(\psi(c^t_{i}), \psi(c^t_{j})) \) for \( i \neq j \). The term \(\text{div}(S)\) represents the diversity of the captions and can be defined as:
% \begin{equation}
% \text{div}(S) = \frac{1}{K(K-1)} \sum_{i \neq j} (1 - S[i,j])
% \end{equation}
% This definition ensures that higher diversity is achieved when the similarity between different captions is lower.
% $\psi(\cdot)$ represents a semantic extractor, implemented by \textit{M3-embedding}~\cite{chen2024bge}. $sim(\cdot)$ and $div(\cdot)$ are similarity and diversity calculations.

% In specific, $sim(\psi(c^t_{i,k}),\psi(t_i))=\frac{\psi(c^t_{i,k}) \cdot \psi(t_i)}{\|\psi(c^t_{i,k})\| \|\psi(t_i)\|}$.
% Moreover, the term \( \text{div}(\psi(c^t_i)) \) calculates the diversity of samples in the caption list, which can be defined as \( \text{div}(S_i) \), where \( S_i \) is the similarity matrix between captions, defined as \( S_i[p,q] = \text{sim}(\psi(c^t_{i,p}), \psi(c^t_{i,q})) \), \( p \neq q \). Then, $div(\psi(c_i^t))$ can be rewritren as:

% \begin{equation}
% div(\psi(c_i^t))= \frac{1}{K(K-1)} \sum_{p \neq q} (1 - S_i[p,q])
% \end{equation}
% where $K$ denotes the caption number in the caption list. 
% This definition ensures that higher diversity is achieved when the similarity between different captions is lower, thus effectively promoting multi-angle descriptions.


\paragraph{\textbf{\textit{Prompt refinement}}}
Current prompt engineering normally encodes the query prompts and adjusts the prompt tokens/embeddings through gradient descent \cite{pryzant2023automatic,yang2023dynamic}. Although this parametric fine-tuning strategy is convenient for optimization, the adjusted embeddings could hardly be precisely re-projected to the text space, losing linguistic meaning and interpretability. Consequently, their generalization in video-text-related applications is limited.
Inspired by language rewrites \cite{fan2024improving}, this paper introduces a \textit{Prompt engineer} to rewrite the query prompt. Take the current best prompt $p^t$ as input; the \textit{Prompt engineer} is asked to rewrite $p^t$ with reference to the corresponding video-text pairs $(x,t)$ involved in this iteration. The rewritten prompt then inquires the \textit{Captioner} in the next iteration, as shown in Fig. \ref{fig:CSI}.

\begin{algorithm}[t]
\caption{Caption Self-improvement (CSI) algorithm}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Video-text pairs \((x,t)\), Initial query prompt \(p_0\), Maximum iteration $T$
\STATE \textbf{Output:} \(Best\_prompt\) 

% \STATE Put $p_0$ in Prompt candidate' pool; 
\STATE \textbf{Initialization}:
\STATE ~~~~\(Best\_prompt \gets p_0;\) 
\STATE ~~~~\(Best\_score \gets -\infty;\) 
% \STATE Generate the caption list \(\mathbf{c}\) for videos using $p_0$;
% \STATE Calculate \(Score^0\) using Equation \eqref{eq:score}
% \WHILE{\(Score^t-Score^t\)}
\FOR{$t = 1, 2, \dots, T$}
    % \STATE \# Prompt refinement:
    \STATE $p_t \gets PE(Best\_prompt)$; ~~~~~// Refine prompt
    \IF{\(Best\_prompt == p_t\)} 
    \STATE break; ~~~~~~~~~~~~~~~~~~~~~~~~~~// No update
    \ENDIF
    % \STATE \# Generate caption lists for videos: 
    \STATE $\mathbf{c} \gets Captioner(x,p_t)$; ~~~~~~~~// Generate Captions
    \STATE Calculate \(Score^t\) using Equation \eqref{eq:score};
    \IF{\(Score^t > Best\_score\)}
    \STATE \(Best\_score \gets Score^t;\)
    \STATE \(Best\_prompt \gets p_t;\) ; 
    \ENDIF
    % \STATE Refining using \textit{Prompt Engineer}: \textit{Captioner($Best\_prompt$)}; 
    % \STATE \textbf{Step 1: Generate captions}
    % \STATE \(C_t \gets Captioner(p_t)\)
    
%     \STATE \textbf{Step 2: Evaluate captions and update prompt candidates}
%     \STATE \(caption\_scores, semantic\_assessment \gets f_1(C_i), f_2(C_i)\)
%     \STATE Update prompt candidates: \(P\_candidates \gets (caption\_scores, semantic\_assessment)\)
    % \STATE Prompt refinement: \textit{PromptEngineer($Best\_prompt$)};
%     \STATE \(best\_prompt \gets select\_best\_prompt(P\_candidates)\)
%     \STATE \(refined\_prompt \gets E.refine\_prompt(best\_prompt, V, C_i)\)
    
%     \STATE \textbf{Check if prompt has changed}
%     \IF{\(refined\_prompt = P_i\)}
%         \STATE break
%     \ELSE
%         \STATE \(P_i \gets refined\_prompt\)
%     \ENDIF
\ENDFOR
\end{algorithmic}
\label{alg:CSI}
\end{algorithm}


\subsection{Expertized Caption Selection (ECS)}
With the optimized CSI module, video-text pairs are updated as $((v,\tilde{c}),t)$, where $(v,\tilde{c})$ represents the video data $v$ and its generated captions $\tilde{c}$ using the optimal prompt, and $t$ denotes the original text corresponding to $v$.
As displayed in Fig. \ref{fig:overview} (b), $(v,\tilde{c})$ and $t$ are the video-side and text-side input data, respectively. 
A straightforward approach for cross-modal representation learning is directly matching the encoded representations of $(v,\tilde{c})$ and $t$.
However, the information dilation of video modality brings confounded knowledge while improving generalization, increasing the difficulty of cross-modal matching \textit{(see Fig. \ref{fig:gap})}.

Considering that the derived captions describe video content from multiple perspectives, we propose inserting an ECS module designed based on a mixture of experts to facilitate cross-modal matching. Write the outputs of visual encoders as $e_v=[e_{v1},e_{v2},...]$, which consists of $N+K$ elements, where $N$ is the number of sampled frames from videos, and $K$ is the number of derived captions. The ECS module is composed of multiple expert networks with learnable parameters ($[f_1, f_2, ...,f_M]$), each of which refers to a description angle. $M$ denotes the total number of experts. A router $r_m$ is allocated to each expert $f_m$ as a gate, which is automatically learned to explore the most matched visual expression fusion for each video-text pair. Through this individualized video expression screening and fusion, the expression that is overly different from the corresponding texts is filtered on the video side, thereby narrowing the video-text modality gap.
% In the ECS module, each expert represents one aspect of video expressions, and a router is automatically learned to explore the most matched expression fusion for each video-text pair. 
% The ECS module is composed of multiple expert networks with learnable parameters ($[f_1, f_2, ...,f_M]$), each of which refers to a description angle. $M$ denotes the total number of experts. A router $r_m$ is allocated to each expert $f_m$ as a gate to automatically select visual expressions meeting this expert. 
%Write the outputs of visual and textual encoders as $e_v$ and $e_t$, respectively. 
Mathematically, the visual codes are residually updated by a mixture of experts' outputs:
\begin{equation}
\tilde{e}_v = \frac{1}{M}\sum_m f_m ( r_m(e_v) \otimes e_v) + e_v
\end{equation}
where $r_m(e_v)$ outputs attention weights of $(N+K)$ video-side expressions for each sample. Specifically, a top-$R$ filter is applied on $r_m(e_v)$, which activates partial expressions to learn a specific expert. $\otimes$ denotes the dot-product operation.  
As visual and textual embeddings will be aligned after further encoding by a shared encoder, parameters of the ECS module ($f_m$ and $r_m$) are optimized along with other encoding networks under the supervision of textual information. 



% Unlike traditional text augmentation that rewrites captions on the text data side, ExCae is a two-stage RL approach (CSI and ECS), which automatically derives captions from videos and adaptively selects appropriate expressions to encode video for video-text matching \textit{(see Figure \ref{fig:overview})}.
% The agents (\textit{Prompt Engineer} and \textit{Visual Selector}) are successively evolved according to the feedback from the VTR environment. When one of the two agents learns, the other will freeze as part of the environment.
% % by the CSI module and adaptively selects appropriate expressions (ECS module) to encode video for video-text matching \textit{(see Figure \ref{fig:overview})}. 
% Video and text embeddings would be aligned with standard cross-modal contrastive loss, as most existing SOTA VTR methods \cite{liu2022ts2,luo2022clip4clip}.
% Both CSI and ECS are self-supervised RL processes, whose details are introduced as follows. 


% % Pseudo code of the CSI module is shown in Algorithm \ref{alg:CSI}. Take the initial prompt \textit{(init)} and video-text pairs (data) as inputs, CSI iteratively updates the best prompt by comparing the current score to the best one until there is no improvement. 

% \subsection{Caption Self-improvement (CSI)}
% In the VTR environment, it is assumed that there exists a Captioner, which is an mLLM designed to generate a set of captions based on a query prompt, enhancing the understanding of videos. The primary objective of CSI is to learn the optimal prompt for caption generation. To achieve this goal, we introduce a \textit{Prompt Engineer}, an agent updated via RL, which progressively refines the query prompt by taking the most effective actions. 
% As illustrated in Figure \ref{fig:CSI}, an engine is included in the \textit{Prompt Engineer}, unremittingly popping up two refined prompt candidates in each RL round.


% the action space is created in virtue of a pool of prompt candidates recording the 



% Suppose that the VTR environment includes a Captioner that is an mLLM to generate a set of captions using a query prompt. The objective of CSI is to learn the best prompt for caption generation.
% To achieve this goal, we introduce a \textit{Prompt Engineer}, which is an agent updated through RL, gradually taking the best action to refine the query prompt.

% with the state represented by the current query prompt. 

% % The main flow is illustrated in Figure \ref{fig:CSI}, which is a reinforcement learning process of the \textit{Prompt Engineer} to gradually refine the query prompt. 


% Specifically, a \textit{Captioner} is asked to generate multiple captions to describe the video from \textit{\textbf{different perspectives}}. We then evaluate the semantics of the generated captions, scoring them to update the prompt candidates' pool. A \textit{Prompt engineer} is introduced to rewrite the current best prompt, whose output is used as the latest query prompt of the \textit{Captioner}. 

% The RL process would end when the reward becomes stable.

% Pseudo code is provided in Algorithm \ref{alg:CSI}.


% \paragraph{\textbf{\textit{Caption scoring}}} 
% Denote the query prompt of the \textit{Captioner} as $p^t$, where $t$ records the iteration index. Correspondingly, the generated caption list for a video $v_i$ based on $p^t$ is written as $c^t_i$. As literal expressions, $c^t_i$ is evaluated from a semantic perspective. We advocate for the generation of captions that are semantically akin to the original texts to mitigate the presence of cluttered and vacuous descriptions. However, merely preserving the semantic consistency may lead to generating a set of captions similar to the original text. This is opposite to the original intention of expression enhancement. To promote \textbf{\textit{multi-angle}} descriptions, a diversity constraint that encourages variety within the caption list is supplemented. Suppose $K$ captions are contained in $c^t_i$, the caption generation score in the $t$ iteration round can be expressed as
% \begin{equation}
% Score^t = \mathbb{E}_{i,k}[sim(\psi(c^t_{i,k}),\psi(t_i))] + div(\psi(c^t_i))
% \label{eq:score}
% \end{equation}
% where $t_i$ denotes the original text corresponding to $v_i$, $k\in[1,K]$ is the caption index in $c^t_i$. 
% %Suppose \( K \) captions are contained in \( c^t_i \), the caption generation score in the \( t \) iteration round can be expressed as:
% %\begin{equation}
% %\text{Score}_t = \mathbb{E}_{i,k}[\text{sim}(\psi(c^t_{i,k}), \psi(t_i))] + \text{div}(S)
% %\label{eq:score}
% %\end{equation}
% %where \( t_i \) denotes the original text corresponding to \( v_i \), \( k \in [1, K] \) is the caption index in \( c^t_i \), and \( S \) is the similarity matrix between the captions, defined as \( S[i,j] = \text{sim}(\psi(c^t_{i}), \psi(c^t_{j})) \) for \( i \neq j \). The term \(\text{div}(S)\) represents the diversity of the captions and can be defined as:
% %\begin{equation}
% %\text{div}(S) = \frac{1}{K(K-1)} \sum_{i \neq j} (1 - S[i,j])
% %\end{equation}
% %This definition ensures that higher diversity is achieved when the similarity between different captions is lower.
% $\psi(\cdot)$ represents a semantic extractor, implemented by \textit{M3-embedding}~\cite{chen2024bge}. $sim(\cdot)$ and $div(\cdot)$ are similarity and diversity calculations \textit{(See \underline{Supplemental Materials} for details)}.

% In the original formula \textit{(Eq. (1) in the manuscript)} for caption scoring, the term $sim(\cdot)$ computes the similarity between generated captions and original text. In specific, $sim(\psi(c^t_{i,k}),\psi(t_i))=\frac{\psi(c^t_{i,k}) \cdot \psi(t_i)}{\|\psi(c^t_{i,k})\| \|\psi(t_i)\|}$.
% Moreover, the term \( \text{div}(\psi(c^t_i)) \) calculates the diversity of samples in the caption list, which can be defined as \( \text{div}(S_i) \), where \( S_i \) is the similarity matrix between captions, defined as \( S_i[p,q] = \text{sim}(\psi(c^t_{i,p}), \psi(c^t_{i,q})) \), \( p \neq q \). Then, $div(\psi(c_i^t))$ can be rewritren as:

% \begin{equation}
% div(\psi(c_i^t))= \frac{1}{K(K-1)} \sum_{p \neq q} (1 - S_i[p,q])
% \end{equation}where $K$ denotes the caption number in the caption list. This definition ensures that higher diversity is achieved when the similarity between different captions is lower, thus effectively promoting multi-angle descriptions.

% If $Score^t$ surpasses the historical records, $p^t$ would be added to the prompt candidates' pool, and the best prompt is updated accordingly.

% \paragraph{\textbf{\textit{Prompt refinement}}}
% The current prompt engineering normally encodes the query prompts and adjusts the prompt tokens/embeddings through gradient descent \cite{pryzant2023automatic,yang2023dynamic}. Although this parametric fine-tuning strategy is convenient for optimization, the adjusted embeddings could hardly be precisely re-projected to the text space, losing linguistic meaning and interpretability. Consequently, their generalization in video-text-related applications is limited.
% Inspired by language rewrites \cite{fan2024improving}, this paper introduces a \textit{Prompt engineer} to rewrite the query prompt. Take the current best prompt $p^t$ as input; the \textit{Prompt engineer} is asked to rewrite $p^t$ with reference to the corresponding video-text pairs $(x,t)$ involved in this iteration. The rewritten prompt is then used as the query prompt for the \textit{Captioner} in the next iteration, as shown in Figure \ref{fig:CSI}.




