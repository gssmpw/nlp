
\section{Conclusion}
% This paper proposes strengthening cross-modal understanding by enhancing video expressions in a self-learning manner, which effectively reduces the modality gap without additional workload on data acquisition.
% We innovatively develop a CSI module that gradually refines query prompts to generate high-quality video captions with rich and diverse expressions. The 

% facilitating the while eliminating human empiricism on prompt design.
% % avoids extra workload on data acquisition and prompt design. 
% % A CSI module that gradually refines query prompts is designed to generate high-quality captions with rich and diverse expressions, so as to expand the receptive field for cross-modal learning.
% In addition, multiple learnable experts are introduced as a filter to automatically select the appropriate perspectives of expressions for video-text matching. Our method is non-empirical and self-adaptive in comparison with current text rewriting methods. 
% Experimental results demonstrate that our method achieves SOTA performance on benchmark datasets such as MSR-VTT, MSVD, and DiDemo. Looking ahead, we aspire to explore the variants adaptive to various cross-modal scenarios, thereby aiding a multitude of video-text-related tasks.


This paper proposes strengthening cross-modal understanding by enhancing video expressions rather than text rewriting, which effectively reduces the modality gap without additional workload on data acquisition.
We innovatively developed a CSI module that automatically refines query prompts to generate high-quality video captions with rich and diverse expressions.   The visual domain is expanded to meet the expression modes and semantic information of texts, so as to facilitate cross-modal understanding.
In addition, multiple learnable experts are introduced as a filter to automatically select the appropriate perspectives of expressions for video-text matching.    Our method is non-empirical and self-adaptive in comparison with current text rewriting methods.
Experimental results demonstrate that our method achieves SOTA performance on benchmark datasets such as MSR-VTT, MSVD, and DiDemo.    Looking ahead, we aspire to explore the variants adaptive to various cross-modal scenarios, thereby aiding a multitude of video-text-related tasks.