% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% \usepackage[table,xcdraw]{xcolor}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{kotex}
\usepackage{amssymb}
\usepackage{amsmath}

\lstdefinelanguage{dict}{
    breaklines=true,
    breakatwhitespace=true,
    basicstyle=\ttfamily\small,
    upquote=true,
    breakindent=0pt,
}
\usepackage{arydshln}
\makeatletter
\def\adl@drawiv#1#2#3{%
        \hskip.5\tabcolsep
        \xleaders#3{#2.5\@tempdimb #1{1}#2.5\@tempdimb}%
                #2\z@ plus1fil minus1fil\relax
        \hskip.5\tabcolsep}
\newcommand{\cdashlinelr}[1]{%
  \noalign{\vskip 1pt
           \global\let\@dashdrawstore\adl@draw
           \global\let\adl@draw\adl@drawiv}
  \cdashline{#1}[.4pt/2pt]
  \noalign{\global\let\adl@draw\@dashdrawstore
           \vskip 1pt}}
\makeatother

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{CoME: An Unlearning-based Approach to Conflict-free Model Editing}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}

\author{Dahyun Jung \qquad Jaehyung Seo \qquad Jaewook Lee \\ \textbf{Chanjun Park}\thanks{Corresponding author.} \qquad \textbf{Heuiseok Lim}\footnotemark[1] \\
  Korea University\\
  \texttt{\{dhaabb55,seojae777,jaewook133,bcj1210,limhseok\}@korea.ac.kr}}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Large language models (LLMs) often retain outdated or incorrect information from pre-training, which undermines their reliability. While model editing methods have been developed to address such errors without full re-training, they frequently suffer from knowledge conflicts, where outdated information interferes with new knowledge. In this work, we propose Conflict-free Model Editing (CoME), a novel framework that enhances the accuracy of knowledge updates in LLMs by selectively removing outdated knowledge. CoME leverages unlearning to mitigate knowledge interference, allowing new information to be integrated without compromising relevant linguistic features. Through experiments on GPT-J and LLaMA-3 using Counterfact and ZsRE datasets, we demonstrate that CoME improves both editing accuracy and model reliability when applied to existing editing methods. Our results highlight that the targeted removal of outdated knowledge is crucial for enhancing model editing effectiveness and maintaining the model's generative performance. Our code is available at \url{https://github.com/ekgus9/COME}.
\end{abstract}

\section{Introduction}
Large language models (LLMs) encode vast amounts of knowledge during pre-training, enabling them to perform effectively across a wide range of natural language processing (NLP) tasks~\cite{hao2021selfattention,cao2021knowledgeableeducatedguessrevisiting,jiang2023mistral,hernandez2023inspecting,haviv2023understanding,openai2023gpt4}. However, LLMs often incorporate outdated, incorrect, or biased information learned from training data, which can directly affect the reliability of their outputs~\cite{hase2021language,pagnoni2021understanding,Ji_2023,mousavi2024llm}. Such issues may lead to unexpected results or undesirable biases in the generated responses.

There is a growing need for research aimed at correcting erroneous knowledge in LLMs or injecting new knowledge while preserving the general performance of the models. 
% The most reliable approach for correcting a model trained on flawed data is to retrain it from scratch with accurate data. However, pre-training a model from the beginning is highly inefficient in terms of both time and computational resources.
Recent studies explore model editing, which offers the potential to modify a model's knowledge without requiring full re-training~\cite{mitchell2022memorybased,wang2023knowledge,yao2023editinglargelanguagemodels,pinter2023emptyingoceanspoonedit,zhang2024comprehensive}. Model editing enables the integration of new information into a model through minimal parameter updates while preserving its existing knowledge. This is particularly useful for correcting errors introduced by flawed data or incorporating new knowledge while selectively updating only the necessary parts of the model.

Existing model editing methods primarily focus on identifying and modifying the parameters where knowledge is stored in order to update the model~\cite{dai2022knowledgeneuronspretrainedtransformers,meng2023massediting,hu2024wilkewiselayerknowledgeeditor,chen2024largelanguagemodelbias,sharma2024locatingeditingfactualassociations,wang2024detoxifyinglargelanguagemodels}. These approaches allow the model to retain learned information efficiently while updating specific knowledge. However, when generating responses based on newly integrated knowledge, the model may encounter conflicts between the new and outdated knowledge, leading to degraded performance~\cite{li2024unveilingpitfallsknowledgeediting}. \citet{ni2024forgettinglearningutilizingparametric} propose a full fine-tuning-based approach that first performs forgetting outdated knowledge before editing the model with new information. However, fine-tuning-based editing is susceptible to overfitting~\cite{decao2021editing}, and updating all layers incurs significant memory overhead. Additionally, the gap between the unlearning and editing stages may lead to unintended knowledge distortions.

\begin{figure*}[]
\centering
\includegraphics[width=0.91\linewidth]{figure/main.png}
\caption{The overall framework of CoME. (a) Existing model editing creates a situation where outdated and new knowledge coexist, and (b) we resolve this issue by unlearning the parameters representing the outdated knowledge.}
\label{fig:main}
\end{figure*}

% To address these issues, we propose a novel approach, \textbf{CoME (Conflict-free Model Editing)}, designed to enhance the accuracy of knowledge updates by focusing on the selective removal of outdated knowledge. This process mirrors the way the human brain refines its understanding—when we learn new information, the brain selectively weakens outdated or conflicting memories to avoid cognitive interference and confusion. In a similar manner, CoME mitigates knowledge conflicts in LLMs by identifying and unlearning parameters tied to obsolete knowledge, thereby creating space for new, relevant knowledge to be fully integrated. Unlike merely overwriting knowledge, this approach ensures that the model doesn’t retain conflicting information, which could degrade its performance.

% By doing so, CoME effectively improves the model’s ability to generate accurate outputs based on new knowledge, similar to how humans improve cognitive clarity by letting go of irrelevant or erroneous memories. Importantly, CoME achieves this without unnecessary loss of linguistic understanding, as we carefully preserve critical language-processing features shared between old and new knowledge. The result is a more refined and reliable model that adapts to new information while minimizing interference from outdated knowledge.

To address these issues, we propose \textbf{Conflict-free Model Editing (CoME)}, which selectively removes outdated knowledge while simultaneously updating the model with new knowledge. This process mirrors the way the human brain refines its understanding—when we learn new information, the brain selectively weakens outdated or conflicting memories to avoid cognitive interference and confusion~\cite{geiselman1983disrupted,bjork1996continuing,wixted2004psychology,alves2017retroactive,kliegl2021mechanisms}. In a similar manner, CoME identifies parameters associated with outdated knowledge and unlearns them during the integration of new knowledge, thereby reducing knowledge conflicts within the LLM. By performing both steps simultaneously, CoME minimizes unintended knowledge transformations. This process is analogous to how humans enhance cognitive clarity by discarding irrelevant or erroneous memories.
% In a similar manner, CoME mitigates knowledge conflicts in LLMs by identifying and unlearning parameters tied to obsolete knowledge, thereby creating space for new, relevant knowledge to be fully integrated. 
% By performing both steps simultaneously, CoME not only prevents the retention of conflicting information but also minimizes unintended knowledge transformations.
% CoME mitigates knowledge conflicts within the model by removing parameters associated with outdated knowledge, similar to how humans enhance cognitive clarity by discarding irrelevant or erroneous memories. 
Importantly, CoME achieves this without unnecessary loss of linguistic understanding, as we carefully preserve critical language-processing features shared between outdated and new knowledge. Furthermore, we limit the parameter space subject to modification during the unlearning process to minimize unnecessary parameter adjustments. 


We apply CoME to state-of-art model editing methods, including MEMIT~\cite{meng2023massediting} and PMET~\cite{li2024pmet}, which are designed to mitigate overfitting and memory overhead issues in knowledge editing. We conduct large-scale knowledge editing experiments on 10,000 samples from the Counterfact~\cite{meng2023locating} and ZsRE~\cite{levy2017zero} datasets, utilizing the GPT-J (6B)~\cite{gpt-j} and LLaMA-3 (8B)~\cite{dubey2024llama3herdmodels}. The results demonstrate that applying CoME significantly improves the accuracy of knowledge updates. In particular, we show that CoME suppresses interference from outdated knowledge during inference, resulting in consistent and accurate responses, while maintaining the LLM's pre-existing capabilities.

Our main contributions are as follows:
\begin{itemize}
    \item We propose a new framework to mitigate conflicts between outdated and new knowledge in LLMs' knowledge editing. 
    \item We introduce unlearning to remove outdated knowledge while integrating new information, and we design an algorithm that applies unlearning selectively to relevant parameters. Our method is designed to complement and enhance existing model editing methods.
    \item Our experiments demonstrate that CoME suppresses interference from outdated knowledge, yielding more reliable and consistent responses. This highlights the framework’s ability to enhance the robustness of LLMs when handling updated information.
\end{itemize}

\section{Related Work}
\subsection{Knowledge Editing}
% Knowledge editing involves correcting erroneous information embedded in a model at a lower cost than retraining the entire model~\cite{wang2023knowledge,zhang2024comprehensive}. 
Existing knowledge editing methods can generally be divided into two categories: preserve and modify parameters.
One involves editing a model's knowledge without directly modifying its parameters. SERAC~\cite{mitchell2022memorybased} stores corrections in external memory and adjusts the model’s responses as needed. 
% highlights the resource-intensive nature of existing methods for knowledge editing in LLMs and 
IKE~\cite{zheng2023edit} proposes a solution based on in-context learning, which enables knowledge modification without parameter updates. GRACE~\cite{hartvigsen2023aging} maps keys to the latent space of the model without changing the weights, constructing a local codebook for knowledge editing. While these methods are resilient to catastrophic forgetting due to the lack of parameter modifications, they require additional memory, which increases with the number of knowledge updates.

Early approaches that modify model parameters for knowledge editing often relied on fine-tuning techniques using multi-loss optimization, as proposed by \citet{sinitsin2020editable}. However, fine-tuning methods can lead to overfitting, prompting the development of hyperparameter-based optimization methods. Knowledge editor (KE)~\cite{decao2021editing} addresses this by utilizing a hypernetwork to edit specific knowledge without affecting unrelated knowledge. ROME~\cite{meng2023locating} identifies the multi-layer perceptrons (MLPs) where factual knowledge is stored and inserts new key-value pairs into those MLPs to modify the model’s knowledge. MEMIT~\cite{meng2023massediting} extends this approach, allowing the insertion of large volumes of knowledge simultaneously. PMET~\cite{li2024pmet} further optimizes the hidden states of both the multi-head self-attention (MHSA) and feed-forward network (FFN) layers to update the feed-forward weights efficiently.

\subsection{Unlearning}
The concept of machine unlearning, introduced by \citet{cao2015towards}, focuses on the removal of knowledge that has already been learned by a model. 
% Although unlearning has been extensively studied in traditional machine learning, its application in the context of LLMs remains underexplored~\cite{bourtoule2020machineunlearning}. 
\citet{jang2022knowledge} employ gradient ascent to perform unlearning with the goal of alleviating privacy concerns, while \citet{eldan2023s} demonstrate unlearning by erasing specific knowledge related to the Harry Potter books from a model. \citet{chen2023unlearn} proposes freezing the LLM and introducing an unlearning layer to construct a forgotten model. \citet{yao-etal-2024-machine} presents a comprehensive framework for performing unlearning in LLMs using gradient ascent and KL divergence. \citet{hu2024separate} utilize parameter-efficient modules to preserve general model capabilities while removing untruthful or toxic information from LLMs.

% Traditional knowledge editing approaches often retain erroneous knowledge in the model, which can lead to confusion between pre-existing and new information~\cite{li2024unveilingpitfallsknowledgeediting}. 
% \citet{ni2024forgettinglearningutilizingparametric} propose an approach that first forgets existing knowledge before performing knowledge editing, though this method is prone to overfitting due to its reliance on fine-tuning. In contrast, our approach involves searching for the parameters where knowledge is stored and optimizing them for model updates. We develop a framework that applies unlearning to effectively remove outdated knowledge before editing the model with new information, integrating this into existing state-of-the-art editing techniques.

\citet{ni2024forgettinglearningutilizingparametric} propose an approach that performs unlearning of existing knowledge before knowledge editing. However, this method is prone to overfitting due to its reliance on fine-tuning. In contrast, our approach is applied to state-of-the-art model editing methods that address such issues. By effectively removing outdated knowledge during the injection of new information, we mitigate conflicts between the two processes.

\section{Preliminaries}
\subsection{Model Editing}
The goal of model editing is to update the knowledge contained in LLM by replacing incorrect or outdated information with new knowledge. In this work, we focus on knowledge represented as triples consisting of a subject $s$, a relation $r$, and an object $o$. Our approach performs batch editing, where multiple pieces of knowledge are updated simultaneously. Specifically, given a model $f$ with parameters $\theta$, we update its parameters to $\theta^*$ by modifying $N$ pieces of knowledge in one step. The knowledge $G$ embedded in the model is represented as:
\begin{eqnarray}
G = \{(s_i, r_i, o_i), i \in [1,N]\}.
\end{eqnarray}

When editing knowledge, we replace the object in the outdated triple $(s, r, o)$ with a new object $o^*$, yielding updated knowledge $(s, r, o^*)$. The target knowledge $G^*$ that the updated model should encode is represented as:
\begin{eqnarray}
G^* = \{(s_i, r_i, o^*_i), i \in [1,N]\}.
\end{eqnarray}

For example, consider the case where $s_i = \texttt{``Motion,''}$ $r_i = \texttt{``manufactured by,''}$ and $o_i = \texttt{``Microsoft,''}$ which reflects an incorrect fact. The updated knowledge should modify the object to $o^*_i = \texttt{``Apple,''}$ while keeping the subject and relation intact. The prompt $x_i$ provided to the model might be ``\texttt{Motion, a product manufactured by},'' and the model's response should be updated to reflect the correct object $o^*_i$ rather than the incorrect $o_i$. Thus, the updated model must satisfy:
\begin{eqnarray}
f_{\theta^*}(x_i) = o^*_i, i \in [1,N].
\end{eqnarray}

If the model has been correctly edited, it satisfies \textbf{Efficacy}, a key attribute that should be prioritized in the editing process. Beyond efficacy, the following properties are essential for evaluating the quality of model editing:

\paragraph{Generality} ensures that the edited knowledge remains intact even when the prompt is paraphrased. This is measured by providing a paraphrased prompt $x^{gen}_i$ and checking whether the model still outputs the updated object $o^*_i$. For instance, if the paraphrased prompt is $x^{gen}_i = \texttt{``He}$ \texttt{was re-elected on the Hapoel HaMizrachi list in 1951. Motion, created by,''} the model should respond with $o^*_i$ to demonstrate that it retains the updated knowledge $(s_i, r_i, o^*_i)$ and applies it consistently across different prompts.

\paragraph{Locality} ensures that editing does not negatively impact unedited knowledge. The updated model must accurately modify only the target knowledge, leaving other unrelated information unchanged. For example, given a prompt that includes unchanged knowledge $x^{loc}_i =$ ``\texttt{Windows was developed by},'' the model’s response should remain consistent with the unedited knowledge $o_i$. This requirement can be formalized as:
\begin{eqnarray}
f_{\theta^*}(x^{loc}_i) = f_\theta(x^{loc}_i), i \in [1,N],
\end{eqnarray}
which ensures that the model’s responses to prompts involving unedited knowledge remain identical before and after the editing process.

\subsection{Locate-then-Edit}
Following the approach of \citet{meng2023massediting}, our goal is to efficiently update the weights of specific layers within the model in response to editing requests. Each edit request involves optimizing target vectors, which gradually adjust the weights of the layers.

We compute the update for one layer and then distribute it uniformly across the target layers. This allows us to update multiple layers efficiently with minimal computational overhead. Specifically, we focus on the final target layer $l$ among the set of target layers $T$. Given an input $x_i$, we calculate a replacement vector $z_i$ for the hidden state $h_i^l$ in layer $l$ as follows: $z_i = h_i^l + \delta_i$. The residual vector $\delta_i$, used to update $z_i$, is optimized via gradient descent:
\begin{equation} \label{eqn:opt_new}
\arg\min_{\delta_i} \frac{1}{P} \sum_{j=1}^{P} -\log \mathbb{P}_{f_\theta(h_i^l += \delta_i)}\left[ o_i^* \mid p_j + x_i \right],
\end{equation}
where $p_j$ represents the $P$ additional prompts introduced to enhance the diversity of inputs.

The computed update is then distributed across the target layers by modifying the MLP weights. Let $W$ represent the original weights, and $\hat{W}$ the updated weights. The incremental update $\Delta$ is added to the original weights, resulting in $\hat{W} = W + \Delta$. The incremental update $\Delta$ is calculated as follows:
\begin{eqnarray}
\Delta = R\hat{K}^T(C + \hat{K}\hat{K}^T)^{-1},
\end{eqnarray}
where $\hat{K}$ encodes the key associated with the target knowledge to be updated. The matrix $C \triangleq KK^T$ represents a set of previously memorized keys obtained through sampling. $R \triangleq \hat{V} - W\hat{K}$ represents the difference between the model's original knowledge representation $W\hat{K}$ and the target knowledge representation $\hat{V}$. This represents a set of values where the residual vector is distributed across the target layers using $\frac{\delta_i}{l - t + 1}, t \in T$.

\section{CoME: Conflict-free Model Editing}

As shown in Figure~\ref{fig:main}, we propose CoME that improves the accuracy of knowledge editing by utilizing parameter subtraction-based unlearning. Our method introduces three key steps to enhance existing locate-then-edit methods: (1) extracting parameters associated with outdated knowledge, (2) performing targeted unlearning during the integration of new knowledge, and (3) restricting the unlearning process to a specific parameter range to ensure that only essential portions are affected.

\subsection{Extraction of Outdated Knowledge Parameters}

To minimize conflicts between outdated knowledge and new knowledge, we remove the outdated information from the updated parameters $z_i$ before distributing the updates across the target layers. First, we obtain the parameters $\delta'_i$ that update the model with outdated knowledge in order to extract the parameters associated with this knowledge. This process closely mirrors the procedure for obtaining the parameters $\delta_i$ corresponding to the new knowledge. $\delta'_i$ is obtained by replacing the new knowledge $o^*$ with the outdated knowledge $o$ in Equation~\ref{eqn:opt_new} and learning accordingly. Therefore, it represents the parameters associated with outdated knowledge.
% The model’s log-likelihood for the outdated knowledge $(s_i, r_i, o_i)$ is maximized using the following equation:
% \begin{equation} \label{eqn:opt_old}
% \arg\min_{\delta'_i} \frac{1}{P} \sum_{j=1}^{P} -\log \mathbb{P}_{f_\theta(h_i^l += \delta'_i)}\left[ o_i \mid p_j + x_i \right],
% \end{equation}
% where $\delta'_i$ represents the vector optimized to reflect the outdated knowledge. 
Inspired by \citet{ilharco2023editingmodelstaskarithmetic, zhang2023composing}, we hypothesize that subtracting the parameters associated with outdated knowledge from the model can facilitate effective unlearning of that knowledge.
By performing $z_i - \delta'_i$, we aim to remove the portions of the parameters updated with new knowledge that still contain outdated information. 

% Both the old knowledge update vector ($\delta'_i$) and the new knowledge update vector ($\delta_i$) are trained on the same input prompts. Thus, $\delta'_i$ does not solely encode outdated knowledge, but also captures the model’s linguistic capacity for processing the input. We hypothesize that this shared capacity, crucial for producing accurate responses, should not be removed from the model. Therefore, we extract this common linguistic capacity and retain it, ensuring that only the outdated knowledge is removed. Inspired by \citet{hu2024separate}, we achieve this by fusing the vectors $\delta_i$ and $\delta'_i$. Since these two linearly independent vectors define distinct hyperplanes, we calculate their common component. We first normalize $\delta_i$ and $\delta'_i$, and then sum them to obtain the direction vector of their common component, $\vec{\delta}_i$:

Following the insights from \citet{hu2024separate}, we assume that $\delta'_i$ not only encapsulates outdated knowledge but also encompasses the model's linguistic abilities. As shown in Equation \ref{eqn:opt_new}, both the outdated knowledge update vector $\delta'_i$ and the new knowledge update vector $\delta_i$ are trained using the same input prompt $x_i$. Therefore, both vectors inherently include the linguistic capacity necessary for the model to generate correct responses based on this information. If this shared capability is removed, the model’s ability to provide accurate responses to inputs may be compromised, making it essential to preserve this feature. To achieve this, we extract the common linguistic features by fusing $\delta_i$ and $\delta'_i$. Since the two linearly independent vectors span distinct hyperplanes, we obtain the direction vector $\vec{\delta}_i$ representing the common component by adding their normalized values:
\begin{eqnarray}
\vec{\delta}_i = \frac{\delta_i}{|\delta_i|} + \frac{\delta'_i}{|\delta'_i|}.
\end{eqnarray}

The common part of the outdated and new knowledge vectors is then extracted using vector projection:
\begin{eqnarray}
\delta''_i = \delta'_i \cdot \frac{\vec{\delta}_i}{|\vec{\delta}_i|}.
\end{eqnarray}

\subsection{Unlearning During Knowledge Update}

After extracting the common component, we subtract it from the outdated knowledge update vector. The remaining component, which encodes only outdated knowledge, is subtracted from the updated parameters:
\begin{eqnarray}
z'_i = z_i - \alpha(\delta'_i-\delta''_i),
\end{eqnarray}
where $\alpha$ is a hyperparameter controlling the weight of the subtraction operation\footnote{The process of determining the optimal $\alpha$ is detailed in Section~\ref{sec:alpha}.}.

\subsection{Restricting Unlearning to Critical Parameters}

% To further minimize the negative effects of parameter changes, we limit the unlearning process to the most critical parameters. Specifically, we delete outdated knowledge only from the top-p\% of parameters based on the magnitude of the unlearning update, similar to the approach of \citet{gu2024modeleditingharmsgeneral}. Parameters with unlearning values in the top-p\% are considered essential for knowledge modification, while the remaining parameters are considered irrelevant. The final normalized update for the parameter $z'_i$ is as follows:
Through the experiment shown in Figure~\ref{fig:alpha}, we confirm that unlearning outdated knowledge negatively affects Locality. To address this, inspired by \citet{gu2024modeleditingharmsgeneral}, we limit the scope of unlearning to only the parameters most influenced by outdated knowledge, leaving other knowledge unaffected. Specifically, we restrict unlearning to the top-p\% of parameters based on the magnitude of the unlearning update\footnote{The hyperparameter p is empirically set to 20 in this work.}. Parameters in the top-p\% are considered essential for unlearning, while the remaining parameters are treated as irrelevant. The final update for parameter $z'_i$ is as follows:
\begin{eqnarray}
z'_i =
\begin{cases} 
      z'_i & \text{if } (|\delta'_i-\delta''_i|) \text{ in the top-p\%}, \\
      z_i & \text{otherwise.}
\end{cases}
\end{eqnarray}

\input{table/main_coun}

\input{table/main_zsre}

\section{Experiments}
\subsection{Setup}
\paragraph{Datasets}
We adopt two widely used evaluation datasets from existing model editing research: Counterfact~\cite{meng2023locating} and ZsRE~\cite{levy2017zero}. The Counterfact dataset contains counterfactual knowledge, statements that have a lower generation probability than factual knowledge, which are provided as new knowledge for editing. To assess large-scale knowledge editing capabilities, we conduct experiments on 10,000 samples. ZsRE is a context-free question-answering dataset designed for zero-shot relation extraction. We extract 10,000 samples from ZsRE to evaluate the models' ability to accurately edit knowledge. 


\paragraph{Metrics}
In the Counterfact dataset, we evaluate the models on Efficacy, Generality, and Locality, using success rates as metrics. Additionally, we assess the models' generative capabilities through Fluency and Consistency. Score is the harmonic mean of Efficacy, Generality, and Locality. Since ZsRE does not measure generative capabilities, we evaluate the models based only on accuracy in terms of Efficacy, Generality, and Locality. A detailed description of the evaluation metrics can be found in Appendix~\ref{sec:metric}.

\paragraph{Baselines}
To enable a direct comparison with existing model editing methods, we follow the baselines outlined in \citet{li2024pmet}. The first baseline is the unedited model. FT-W~\cite{zhu2020modifyingmemoriestransformermodels}, involves fine-tuning using weight decay for knowledge editing. FT fine-tunes all parameters of the base model. F-Learning~\cite{ni2024forgettinglearningutilizingparametric} is a fine-tuning-based approach that forgets existing knowledge and learns new knowledge. MEND~\cite{mitchell2022fastmodeleditingscale} leverages additional training data to fine-tune the model through a hypernetwork-based approach. ROME~\cite{meng2023locating} is an optimization-based method for single-editing tasks, while MEMIT~\cite{meng2023massediting} extends ROME to enable large-scale knowledge editing in a single pass. PMET~\cite{li2024pmet} optimizes both MHSA and FFN components simultaneously for knowledge editing.

\paragraph{Implementation Details}
We conduct our experiments using GPT-J (6B)~\cite{gpt-j} and LLaMA-3 (8B)~\cite{dubey2024llama3herdmodels}. The model checkpoints used are `EleutherAI/gpt-j-6B' and `meta-llama/LLaMA-3.1-8B', both of which are available on HuggingFace\footnote{\url{https://huggingface.co/}}. For GPT-J, following \citet{meng2023massediting}, we update layers \{3, 4, 5, 6, 7, 8\}. For LLaMA-3, following \citet{wang2023easyedit}, we update layers \{4, 5, 6, 7, 8\}. To estimate the covariance matrix $C$, we sample 10K times from WikiText in fp32 precision. For MEMIT, we set the covariance adjustment factor $\lambda = 15000$, and for PMET, we set $\lambda = 6000$. All experiments are performed using a single RTX A6000 GPU. GPT-J is run in fp32, while LLaMA-3 uses fp16 due to memory constraints. 
Unlike MEMIT, in the PMET setting, only the weights of the FFN are updated separately, so CoME is applied to the FFN residual vector $\delta^{FFN}_i$.
Further implementation details can be found in the official MEMIT\footnote{\url{https://github.com/kmeng01/memit}} and PMET\footnote{\url{https://github.com/xpq-tech/PMET}} repositories.

\begin{figure*}[hbt!]
\centering 
\includegraphics[width=0.97\linewidth]{figure/number.png}
\caption{Scaling curves that represent editing performance based on the size of edits. These experiments are conducted on the Counterfact dataset using GPT-J.}
\label{fig:number} 
\end{figure*}

\subsection{Main Results}

\paragraph{Editing Knowledge in Counterfact}
Table~\ref{tab:main_coun} presents the editing performance of CoME on 10,000 samples from the Counterfact dataset. Both $\text{CoME}_{\text{MEMIT}}$ and $\text{CoME}_{\text{PMET}}$ improve Score, which evaluates the overall performance of editing. On GPT-J, both methods achieve Score of 86.4, compared to 85.8 for MEMIT and 86.2 for PMET, demonstrating the efficacy of our approach. Similarly, on LLaMA-3, $\text{CoME}_{\text{PMET}}$ achieves 82.3, outperforming PMET of 81.1. These results show that by removing outdated knowledge, our method enhances the model’s ability to handle new knowledge. The most notable improvement arises in the accuracy of newly updated knowledge, particularly in terms of Efficacy and Generality. Not only does the accuracy of the edited knowledge increase, but interference from outdated knowledge is minimized, resulting in higher overall performance.

In contrast, Locality, which measures the preservation of unrelated knowledge, slightly decreases compared to MEMIT and PMET. This trade-off between editing accuracy and Locality is expected, as our primary objective is to inject new knowledge rather than minimize changes to the model. Furthermore, Fluency and Consistency of the model’s outputs are maintained at levels comparable to the original model, further supporting the robustness of our method. Appendix~\ref{sec:case} presents a case study demonstrating how CoME enhances the utilization of new knowledge by unlearning outdated knowledge.

\paragraph{Editing Knowledge in ZsRE}
Table~\ref{tab:main_zsre} shows the performance of our method on 10,000 ZsRE samples using GPT-J and LLaMA-3. Similar to the results on the Counterfact dataset, $\text{CoME}_{\text{MEMIT}}$ and $\text{CoME}_{\text{PMET}}$ demonstrate superior performance in Efficacy and Generality on both models. For GPT-J, $\text{CoME}_{\text{PMET}}$ achieves Efficacy of 89.4 and Generality of 83.1, both surpassing the results of baseline PMET. These outcomes suggest that our method effectively integrates new knowledge while minimizing the influence of outdated information. 

In terms of Locality, the results on ZsRE show significant improvements compared to the Counterfact dataset. $\text{CoME}_{\text{PMET}}$ achieves the highest Locality scores on both models, indicating that our approach reduces the negative impact on unrelated knowledge. Particularly on LLaMA-3, $\text{CoME}_{\text{PMET}}$ not only updates knowledge but also improves the model’s ability to generate factual responses compared to the original model.

\subsection{Analysis}

\input{table/ablation}

\paragraph{Number of Edits}

Figure~\ref{fig:number} illustrates the performance of the model as the number of simultaneous edits increases. The results show that $\text{CoME}_{\text{MEMIT}}$ and $\text{CoME}_{\text{PMET}}$ remain robust in terms of Efficacy and Generality, even as the number of edits increases. Our method ensures a high success rate for Generality, even when the number of edits is low, and maintains initial performance levels as the number of edits grows. However, as with other methods, Locality begins to decline sharply once the number of edits exceeds a certain threshold. In terms of Fluency and Consistency, our methods perform similarly to or exceed the original model’s performance, unlike ROME, which experiences significant drops in language generation quality as the number of edits increases.

\paragraph{Ablation Study}
The results of the ablation study, presented in Table~\ref{tab:ablation}, examine the effects of unlearning and the application of restricting unlearning parameters on $\text{CoME}_{\text{MEMIT}}$ and $\text{CoME}_{\text{PMET}}$. We analyze the impact of removing each component: $\delta'$, $\delta''$, and restricting unlearning parameters.

% 전반적으로 결과는 각 구성 요소가 CoME의 성능에 뚜렷하게 기여한다는 것을 보여줍니다. 모든 구성 요소를 각각 제외할 경우 Score가 떨어지는 것을 알 수 있다. 

Excluding $\delta'$, we observe a decline in performance across most metrics, particularly in Generality and Efficacy. Notably, in $\text{CoME}_{\text{MEMIT}}$, performance drops significantly from 91.1 to 88.6. This suggests that the removal of outdated knowledge plays a crucial role in improving the accuracy of knowledge editing. 


Excluding $\delta''$ primarily affects Locality, where we observe significant performance degradation. This suggests that $\delta''$ plays a vital role in preserving the model’s ability to handle unrelated information. On the other hand, Fluency shows an upward trend, likely due to the increased capacity to handle structured knowledge, which comes at the cost of penalties in generation fluency.

Excluding restricting the unlearning parameter method leads to the greatest drop in Locality, while Efficacy and Generality are only slightly affected. This shows that unlearning is effectively performed only on the top-p\% of parameters where outdated knowledge resides, preventing unnecessary parameter updates without sacrificing accuracy.

\paragraph{Unlearning Weight Variation} \label{sec:alpha}

\begin{figure}[]
\centering 
\includegraphics[width=0.99\columnwidth]{figure/alpha.png}
\caption{Effect of unlearning weight variation in $\text{CoME}_{\text{MEMIT}}$. The experiments are conducted using GPT-J and 10,000 Counterfact samples.}
\label{fig:alpha} 
\end{figure}

To control the degree of outdated knowledge removal, we introduce the hyperparameter $\alpha$. Figure~\ref{fig:alpha} shows the effect of varying $\alpha$ from 0 to 2 on performance metrics such as Score, Efficacy, Generality, and Locality. We observe that both Efficacy and Generality increase as $\alpha$ rises, indicating that more effective removal of outdated knowledge improves model performance. However, Locality decreases as $\alpha$ increases, suggesting that excessive knowledge removal may negatively impact unrelated information. Based on these findings, we use $\alpha = 0.1$ as the default setting, and restrict the unlearning scope to minimize the drop in Locality.

\section{Conclusion}

In this paper, we proposed CoME to address the conflict between outdated and new knowledge that can arise during the editing process in LLMs. CoME enhanced the accuracy of knowledge editing by simultaneously unlearning outdated knowledge and integrating new information. Experiments showed that our method improved the editing accuracy of existing model editing methods and successfully integrated new knowledge. This approach can be an effective solution for correcting inaccurate or biased information in large language models, and we expect it to make significant contributions to improving the reliability and consistency of LLMs.

\section*{Limitations}

While CoME successfully enhances the usability of new knowledge by removing outdated information, several limitations must be acknowledged:

\begin{itemize}
    \item The unlearning process requires additional computational resources. Since CoME introduces a separate stage to remove outdated knowledge, it incurs higher computational costs than traditional model editing techniques.
    \item CoME is designed to remove outdated or false knowledge, which may not always be desirable in cases of temporal knowledge. For example, older information that reflects past realities can still be useful in certain contexts. 
\end{itemize}

% Second, experimental comparison with the method proposed by \citet{ni2024forgettinglearningutilizingparametric} is not feasible. Their approach requires full fine-tuning of large LLMs, necessitating 4 × A100-80G GPUs. Due to out-of-memory issues, we were unable to conduct these experiments.

% Third, our evaluation primarily focused on GPT-J and LLaMA-3, which are representative but specific models within the broader LLM ecosystem. Although these models reflect modern architectures, further evaluation is necessary to ensure the generalizability of CoME across other architectures and larger-scale models.

\section*{Ethical Considerations}

Our research aims to enhance the reliability and safety of LLMs by addressing issues stemming from the retention of incorrect or biased information. By developing and improving model editing methods, we seek to contribute to the responsible use of LLMs, particularly in mitigating the spread of misinformation and harmful biases. However, it is essential to recognize that any modification to a model’s knowledge must be handled with caution, ensuring that only erroneous or biased information is removed while preserving the integrity of factual content. Ensuring that model editing is performed transparently and based on clearly defined ethical guidelines will be critical as this technology develops.

\section*{Acknowledgments}

This work was supported by Institute for Information \& communications Technology Promotion(IITP) grant funded by the Korea government(MSIT) (RS-2024-00398115, Research on the reliability and coherence of outcomes produced by Generative AI). This research was supported by Basic Science Research Program through the National Research Foundation of Korea(NRF) funded by the Ministry of Education(NRF-2021R1A6A1A03045425). This work was supported by ICT Creative Consilience Program through the Institute of Information \& Communications Technology Planning \& Evaluation(IITP) grant funded by the Korea government(MSIT)(IITP-2025-RS-2020-II201819).

% This document has been adapted
% by Steven Bethard, Ryan Cotterell and Rui Yan
% from the instructions for earlier ACL and NAACL proceedings, including those for
% ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
% NAACL 2019 by Stephanie Lukin and Alla Roskovskaya,
% ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu,
% NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
% Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
% ACL 2017 by Dan Gildea and Min-Yen Kan,
% NAACL 2017 by Margaret Mitchell,
% ACL 2012 by Maggie Li and Michael White,
% ACL 2010 by Jing-Shin Chang and Philipp Koehn,
% ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui,
% ACL 2005 by Hwee Tou Ng and Kemal Oflazer,
% ACL 2002 by Eugene Charniak and Dekang Lin,
% and earlier ACL and EACL formats written by several people, including
% John Chen, Henry S. Thompson and Donald Walker.
% Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{main}

\clearpage

\appendix


\section{Metric Details}
\label{sec:metric}

We follow the evaluation metrics setup of CounterFact and ZsRE as outlined in \citet{meng2023locating, meng2023massediting, li2024pmet}.

\subsection{Metrics for Counterfact}

An effective model editing method should satisfy three fundamental criteria: Efficacy, Generality, and Locality.

\paragraph{Efficacy} evaluates whether the targeted knowledge has been correctly edited. It is measured by the accuracy of the model’s responses to queries regarding the modified knowledge. 
Given a set of knowledge prompts $X=\{x_1, x_2, \ldots, x_i\}$, the modified model $f_{\theta^*}$ should assign a higher probability to the correct answer set $O^* = \{o^*_1, o^*_2, \ldots, o^*_i\}$ compared to the outdated answer set $O = \{o_1, o_2, \ldots, o_i\}$. Thus, the formula for calculating Efficacy is as follows:
\begin{equation}
    \frac{1}{|X|} \sum_{i=1}^{|X|} \mathbb{I}(\mathbb{P}_{f_{\theta^*}}[o^*_i|x_i] > \mathbb{P}_{f_{\theta^*}}[o_i|x_i]),
\end{equation}
where $\mathbb{I}(\cdot)$ is the indicator function that returns 1 if the condition is true and 0 otherwise.

\paragraph{Generality} measures the model's ability to answer paraphrased or generalized queries related to the edited knowledge, assessing the robustness and generalization of the modified knowledge. Given a set of paraphrased queries $X^{gen} = \{x^{gen}_1, x^{gen}_2, \ldots, x^{gen}_i\}$, Generality is calculated as follows:
\begin{equation}
    \frac{1}{|X^{gen}|} \sum_{i=1}^{|X^{gen}|} \mathbb{I}(\mathbb{P}_{f_{\theta^*}}[o^*_i|x_i^{gen}] > \mathbb{P}_{f_{\theta^*}}[o_i|x_i^{gen}]).
\end{equation}

\paragraph{Locality} evaluates whether the model editing method has affected knowledge that was not intended to be modified. Given a set of queries unrelated to the edited knowledge $X^{loc} = \{x^{loc}_1, x^{loc}_2, \ldots, x^{loc}_i\}$, Locality is defined as:
\begin{equation}
    \frac{1}{|X^{loc}|} \sum_{i=1}^{|X^{loc}|} \mathbb{I}(\mathbb{P}_{f_{\theta^*}}[o^*_i|x_i^{loc}] < \mathbb{P}_{f_{\theta^*}}[o_i|x_i^{loc}]).
\end{equation}

\paragraph{Score} is the harmonic mean of Efficacy, Generality, and Locality.

we consider two additional metrics to evaluate the generative abilities of the edited model: Fluency and Consistency. 

\paragraph{Fluency} measures the model's response by evaluating the n-gram distribution to detect excessive repetition.

\paragraph{Consistency} calculates the TF-IDF vector between the generated output and the reference Wikipedia text. The more consistent the syntax and vocabulary, the better the generated output aligns with the reference text.

\input{table/case}

\subsection{Metrics for ZsRE}

\paragraph{Efficacy} measures whether the answers generated by the modified model reflect the intended edits:
\begin{equation}
    \frac{1}{|X|} \sum_{i=1}^{|X|} \mathbb{I}(f_{\theta^*}(x_i) = o^*_i),
\end{equation}
where $f_{\theta^*}(x_i)$ represents the response of the modified model to query $x_i$.

\paragraph{Generality} measures whether the model's response is correctly updated for paraphrased sentences. The accuracy of Generality is expressed as follows:
\begin{equation}
    \frac{1}{|X^{gen}|} \sum_{i=1}^{|X^{gen}|} \mathbb{I}(f_{\theta^*}(x^{gen}_i) = o^*_i).
\end{equation}

\paragraph{Locality} measures how well the model provides correct answers to prompts that have not been edited. The accuracy of Locality is defined as follows: 
\begin{equation}
    \frac{1}{|X^{loc}|} \sum_{i=1}^{|X^{loc}|} \mathbb{I}(f_{\theta^*}(x^{loc}_i) = o).
\end{equation}


\section{Case Study}
\label{sec:case}

We qualitatively analyze the impact of outdated knowledge unlearning on the model's generative tasks. Table~\ref{tab:case} presents the generative results of models edited using MEMIT, PMET, and CoME on GPT-J and LLaMA-3. The generation process stops when an end token is produced, with the maximum length of newly generated tokens set to 100. Sentences truncated due to token length are excluded. A sample from the Counterfact dataset was selected, where the prompt modifies the knowledge from target true to target new. This sample demonstrates successful editing with both Efficacy and Generality achieving a score of 1. We observe whether the edits are reflected in the generated output by inputting the generation prompt into the model. In this sample, the subject, \texttt{``El Correo}," is one of the best-selling newspapers in Spain. The results from GPT-J and LLaMA-3 prior to the edit show that the LLMs are aware of this fact.

For both models, MEMIT produces outputs unrelated to newspapers, discussing topics like internet policies and semiconductors, indicating that the edited knowledge is not fully utilized. In PMET, as highlighted by the \underline{underlined} text, the outdated knowledge, \texttt{Spanish}, persists, demonstrating a conflict between the old and new knowledge. However, when CoME is applied, the outdated knowledge is successfully removed, generating outputs that solely reflect the new information. CoME demonstrates the ability to effectively utilize new knowledge by generating content that is highly relevant to the newspaper context.

\end{document}
