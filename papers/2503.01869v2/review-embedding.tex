\subsection{Text Terminology}
We regard text as unstructured data with layered information (e.g. semantic and syntactic). We will denote the set of 85 \textit{Federalist Papers} as $\mathcal{D}$, where each document $D_i=\{w_{ij}\}$ for $1\leq i\leq d$ consists of words $w_{ij}$ chosen from a vocabulary $\mathcal{W}$, where the index $j$ denotes the position of the word inside the text. The size of the vocabulary will be denoted by $|\mathcal{W}|=n$. 
We will use a simplified dictionary (as explained later in Section~\ref{sec:word-screening}) consisting of the vocabulary of size \textcolor{red}{$n \ll 5,834$}. 


A language model, denoted by $\mathcal{M}$, is defined as a probabilistic mechanism for modeling the joint distribution over sequences of words $\mathbf{w} = (w_1, w_2, \ldots, w_T)'$, where $w_t$ is the word at position $t$ in a sequence of length $T$. A sequence $\mathbf{w}$ can also be the sequence of tokens, depending on the model of interest. Token refers to a single unit of text, which could either be word, or a word stem. Many language models operate on a word count matrix instead of directly operating the sequence of words. We denote the term-document matrix as $X \in \mathbb{R}^{d \times n}$ where each row represents a document ($D_i$) and each column represents the occurrence of word $j$ in the document $i$. While simple and computationally efficient, Bag-of-Words (BoW) type models, operating on a word count matrix, lose information about the sequence, potentially missing context-dependent meanings. Formally, the BoW representation of the collection of documents can be denoted as:

$$ X = [\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_d]^T \in \mathbb{R}^{d \times |\mathcal{W}|} $$ where $ x_{ij}$ is the count of the $j$-th word in the document $i$. 

Our main interest is text embeddings, where embeddings denote $p$ dimensional vector representation of given data. Let $f \in \mathcal{M}$ be the embedding function we consider, then $f: \mathcal{W} \to \mathbb{R}^p$. In usual dimensionality reduction settings, the embedding dimension $p$ is much smaller than the vocabulary size $|\mathcal{W}|$. We denote the vector representation of text data (or text embeddings) as $\mathbf{z} \in \mathbb{R}^p$. Estimating $f$ can be done in various way depending on the model assumptions. Models like LDA operates on probabilistic assumption that $f$ can be estimated through Variational Inference (VI) \citep{Blei2017variational}. Word2Vec \citep{mikolov2013distributed, mikolov2013efficient} or GloVE \citep{pennington2014glove} leverage neural network to estimate $f$. 
%n-gram: The parameter `n" defines the range of dependency, and the popular choice is n = 2, 3, 4. 

%For statistical language models, such as n-gram models, this conditional probability $ P(x_t \mid x_1, x_2, \ldots, x_{t-1}) $ is typically approximated by $P(x_t \mid x_{t-n+1}, \ldots, x_{t-1}) $, assuming a fixed context window of size \( n-1 \). Parameters are estimated using maximum likelihood estimation (MLE) on large corpora, often incorporating smoothing techniques to handle the sparsity of observed n-grams.

%LLMs, such as Transformer-based models like GPT and BERT, leverage deep learning architectures to capture long-range dependencies. The conditional probability \( P(x_t \mid x_1, x_2, \ldots, x_{t-1}) \) is parameterized by a neural network with parameters $\mathbf{\theta}$, trained to minimize the model-specific training loss. For example, BERT\cite{devlin2019bert} introduces a masked Language Model(LM) and next sentence prediction task for its model-specific training tasks(and corresponding loss). These models utilize attention mechanisms to weigh the influence of all previous tokens in the sequence, allowing for a dynamic and context-aware estimation of probabilities.

%Let \( \mathbf{h}_t \) represent the hidden state at time step \( t \), capturing the contextual information up to \( x_{t-1} \). In an LLM, \( \mathbf{h}_t \) is computed as:
%$$\mathbf{h}_t = f_\theta(x_1, x_2, \ldots, x_{t-1}) $$ where \( f_\theta \) denotes the transformation applied by the neural network. The probability of the next token is then given by:
%$$ P(x_t \mid x_1, x_2, \ldots, x_{t-1}) = \text{softmax}(\mathbf{h}_t)$$ where the softmax function converts these $|\mathcal{W}|$-dimensional vectors to a probability distribution.

\subsection{Language Models}

The fundamental challenge in language modeling lies in estimating the joint probability distribution of text sequences. 
%Early statistical approaches relied on direct probability estimation through counting and Markov assumptions, while modern language models learn these distributions through increasingly sophisticated neural architectures.
Following the notation earlier, let's denote the sequence of texts of length $T$ as $\mathbf{w} = \{w_1,...,w_T\}$, then the joint probability can be written as:
$$\pi(\mathbf{w}) = P(w_1,...,w_T)$$ 
However, estimating this joint distribution directly is often intractable due to the high dimensionality and dependencies across the sequence. To address this, common strategies involve simplifying the generative process or approximating the joint probability through more manageable objectives, such as conditional densities.

Autoregressive models, for example, approximate the joint distribution by factorizing it using the chain rule of probability. This factorization expresses the joint probability as the product of conditional probabilities:
\begin{equation}\label{eq-ar}
P(\mathbf{w}) = P(w_1) P(w_2|w_1) P(w_3|w_1, w_2) \cdots P(w_T|w_1, w_2, ..., w_{T-1})
\end{equation}
This factorization reduces the complexity of estimating the full joint distribution by leveraging the assumption that each word is generated sequentially, depending only on the words that came before.

In contrast, encoder-based models like BERT employ a different strategy. These models do not assume a strict left-to-right (or right-to-left) dependence but instead use a bidirectional mechanism. In this approach, each word $w_i$ is conditioned on both past and future words, allowing the model to capture context from the entire sequence. Formally, this can be viewed as estimating the probability of $\mathbf{w}$ based on conditional probabilities that incorporate context from both directions:
\begin{equation}\label{eq-ae}
P(w_i | w_1, ..., w_{i-1}, w_{i+1}, ..., w_T)
\end{equation}
The encoder learns a latent representation for the entire sequence, which encodes dependencies across the entire text. This contrasts with autoregressive models, where the prediction of each word is based only on past words.

Thus, autoregressive and encoder-based models represent two different strategies for approximating the joint distribution $P(\mathbf{w})$. Autoregressive models build the distribution sequentially by considering forward dependencies, while encoder-based models leverage bidirectional context to estimate conditional probabilities for each word. Both methods, in essence, approximate the joint distribution through conditionals, though the manner of doing so differs.

The relationships between these different types of models are illustrated in Figure~\ref{fig:lm-cartoon}. 

\paragraph{Connection to Directed Acyclic Graphs (DAGs)} We could draw an analogy between the autoregressive formulation and a Directed Acyclic Graph (DAG) structure to represent dependencies in a sequence of words. In a DAG, nodes represent variables (in this case, words in a sequence), and directed edges represent dependencies between these variables, forming a non-cyclic structure. As shown in the chain rule factorization:
$$P(\mathbf{w}) = P(w_1) P(w_2|w_1) P(w_3|w_1, w_2) \cdots P(w_T|w_1, w_2, ..., w_{T-1})$$
Here, the sequence of words follows a strictly left-to-right dependency structure. This can be viewed as a linear chain in a DAG, where each node (word) depends only on its preceding nodes in the sequence. The directed edges reflect this forward progression, forming a path with no cycles. Since it is a directed acyclic graph, there is no feedback loop that would allow any word to depend on future words in this framework.


\paragraph{Connection to Gibbs Sampling} The connection to Gibbs sampling \citep{Alan1990Sampling,gelman1995bayesian} further illustrates the idea of approximating a joint distribution through conditionals. Gibbs sampling is a Markov Chain Monte Carlo (MCMC) method that approximates the joint distribution by iteratively sampling from the conditional distributions of each variable, given the current values of the others. For a sequence of words $ \mathbf{w}$, the Gibbs sampler would sample each word $w_i$ from its conditional distribution, $P(w_i | \mathbf{w}_{-i})$, where $\mathbf{w}_{-i}$ denotes the sequence excluding $w_i$:
$$w_i^{(t+1)} \sim P(w_i | w_1^{(t+1)}, ..., w_{i-1}^{(t+1)}, w_{i+1}^{(t)}, ..., w_T^{(t)})$$
This iterative process approximates the joint distribution by updating one word at a time, similar to how autoencoder models sequentially predict words based on previous context. Both methods reflect the principle of using conditional distributions to indirectly approximate the full joint distribution.




%\begin{definition}[Small Language Model]
%We will refer to the language model that is not a Large Language model defined in Def~\ref{def:llm} as a Small Language Model.
%\end{definition}





\subsection{Small Language Models}
We will focus on the selected models, which are crucial in understanding language models' advancement. There are varied usage of terms small language models, but in this paper, we will refer to statistical language model as well as neural language models not relying on attention mechanism as ``small language models". 

\subsubsection{Statistical Language Models}\label{sec:statistical-lm}

The characteristic that defines a statistical language model is its reliance on probabilistic assumptions regarding the language generation process. One prominent example of a statistical language model is the n-gram model, which is based on Markov’s assumption that the occurrence of a word depends only on a limited number of preceding words. Specifically, an n-gram model looks at the previous $n-1$ words to predict the next word. Under this assumption, the general formulation of the autoregressive language model, Equation~\ref{eq-ar}, can be further simplified to Equation~\ref{eq:n-gram} reflecting the constrained dependencies inherent in the n-gram approach.
\begin{equation}\label{eq:n-gram}
    P(w_i|w_{i-1}, w_{i-2},...,w_{1}) = P(w_i|w_{i-1},...,w_{i-n+1}).
\end{equation}

Beyond n-gram models, other statistical models such as \textbf{Latent Semantic Analysis (LSA)} and \textbf{Latent Dirichlet Allocation (LDA)} \citep{blei2003latent} introduce more advanced probabilistic structures for language modeling. LSA is grounded in the assumption that words that frequently occur in similar contexts are likely to share meaning. By applying singular value decomposition (SVD), LSA reduces the dimensionality of a term-document matrix to capture latent semantic relationships between words and documents. Given the term-document matrix $X$ where each of the elements $X_{ij}$ is the number of occurrences of word $j$ on document $i$ (i.e. $X \in \mathbb{R}^{|D| \times |\mathcal{W}|}$), $|D|$ is the number of documents and $|\mathcal{W}|$ is the size of the vocabulary. A reduced SVD form would give $p \ll |D|,|\mathcal{W}|$ dimensional representation either on term space or document space. % Although LSA is not explicitly probabilistic, it assumes that the latent structure in the co-occurrence patterns can provide meaningful insights into the relationships between terms and documents.

On the other hand, LDA represents a fully probabilistic model and takes a different approach by assuming that documents are mixtures of latent topics. In LDA, each topic is characterized by a probability distribution over words, and each document is generated through a probabilistic process where a distribution of topics is first chosen, followed by word selection from these topics. Formally, for each document $i$, a distribution over topics $\boldsymbol{\theta}_i$ is drawn from a Dirichlet distribution parameterized by $\alpha$ (i.e., $\theta_i \sim \text{Dirichlet}(\alpha))$. Similarly, each topic $k$ is represented by a word distribution $ \boldsymbol{\beta}_k $, which is also drawn from a Dirichlet distribution parameterized by $\eta$ (i.e., $\beta_k \sim \text{Dirichlet}(\eta) )$. Given these distributions, the generative process for each word $w_{ij}$ in document $i$ proceeds by first drawing a topic $z_{ij}$ from the document-specific topic distribution $\boldsymbol{\theta}_i$ (i.e., $ z_{ij} \sim \text{Multinomial}(\theta_i) )$, and then drawing the word $w_{ij} $ from the word distribution associated with the selected topic $\boldsymbol{\beta}_{z_{ij}}$ (i.e., $ w_{ij} \sim \text{Multinomial}(\beta_{z_{ij}}) $). 

The inference of the latent topic distributions in LDA is typically carried out using techniques such as variational inference or Gibbs sampling. %Variational inference aims to maximize the Evidence Lower Bound (ELBO), which provides a lower bound on the log-likelihood of the data. The ELBO is given by:
%$$ \log p(\mathbf{w}|\alpha, \eta) \geq \mathbb{E}_q[\log p(\boldsymbol{w}, \boldsymbol{z}, \boldsymbol{\theta}, \boldsymbol{\beta}|\alpha, \eta)] - \mathbb{E}_q[\log q(\boldsymbol{\theta}, \boldsymbol{z}, \boldsymbol{\beta})] $$
%To make this optimization tractable, the variational distribution \( q(\boldsymbol{\theta}, \boldsymbol{z}, \boldsymbol{\beta}) \) is factorized under the **mean-field assumption** as \( q(\boldsymbol{\theta}, \boldsymbol{z}, \boldsymbol{\beta}) = \prod_i q(\theta_i)q(z_i)\prod_k q(\beta_k) \). The variational distributions for the topic assignments \( z_{ij} \), document-topic distributions \( \theta_i \), and topic-word distributions \( \beta_k \) are then approximated by categorical or Dirichlet distributions, with parameters \( \phi_{i w_{ij} k} \), \( \gamma_i \), and \( \lambda_k \), respectively. By iteratively updating these parameters, LDA can uncover the underlying topic structure within a collection of documents, providing insights into the latent themes present in the corpus.
This Bayesian framework allows LDA to uncover hidden thematic structures in large text corpora by modeling the latent topics governing word choices within documents. Unlike n-gram models that focus on local word dependencies, LDA or LSA could capture more abstract relationships by grouping words into semantically coherent latent variables that span across documents.


%\subsubsection{Latent Semantic Analysis}
%Latent Semantic Analysis is simply an application of Singular Value Decomposition(SVD) on text analysis. It operates on the principle of identifying latent (hidden) semantic structure within the documents by creating a high-dimensional semantic space based on the co-occurrence patterns of terms across documents. Given the term-document matrix, which encodes the number of word occurrences within the document, SVD enables to extract low-dimensional representation of the term/document spaces. Given the term-document matrix X where each of the elements $X_{ij}$ is the number ofoccurrencese of word-j on document i($X \in \mathbb{R}^{|D| \times |\mathcal{W}|}$). A reduced SVD form would give $p<<|D|,|\mathcal{W}|$ dimensional representation either on term space or document space. 
  %  \item k-dimensional low-rank approximation of X: $X_k = U_k \Sigma_k V_k ^\top$, $k < |D|, |\mathcal{W}|$
  %  \item k-dimensional projection on term space: $\Sigma_k^{-1}V_k^\top X^\top$
  %  \item k-dimensional projection on document space: $\Sigma_k^{-1}U_k^\top X$


%\subsubsection{Latent Dirichlet Allocation}
%Latent Dirichlet Allocation\cite{blei2003latent} (LDA) is a probabilistic generative model commonly used in natural language processing and machine learning for topic modeling. The underlying idea is that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words. LDA assumes that documents exhibit multiple topics and that each word in a document is generated from one of these topics. Through inference techniques like Gibbs sampling or variational inference, LDA can uncover the latent topics in a collection of documents and assign topic distributions to each document and word distributions to each topic, providing insights into the underlying themes or subjects present in the corpus.

%\begin{figure}
%    \centering
%    \includegraphics[width = 0.5\textwidth]{figure/LDA_plate_boxonly.png}
%    \caption{The schematic view of the generative procedure. Figure from \cite{blei2003latent}}
%    \label{fig:enter-label}
%\end{figure}

%The generative modeling scheme follows

%\begin{itemize}
%    \item $\boldsymbol{w}_{ij}$ : j-th word in document i(i = 1,2,...,d)
%    \item $\boldsymbol{z}_{ij}$: associated topic for $\boldsymbol{w}_{ij}$
%    \item $\boldsymbol{\beta}_{z_{ij}}$: preference for specific words defining the topics parametreized by $\boldsymbol{\eta}$
%    \item $\boldsymbol{\theta}_i$: topic preference of document i parameterized by $\boldsymbol{\alpha}$
%\end{itemize}
%\begin{align}
%    \theta_i &\sim \text{Dirichlet}(\alpha)\\
%    \beta_k &\sim \text{Dirichlet}(\eta)\\
%    z_{ij} &\sim \text{multinomial}(\theta_i)\\
%    w_{ij} &\sim \text{multinomial}(\beta_{z_{ij}})
%\end{align}

%The updates can be done by variational inference:
%\begin{align}
%\text{log}p(\mathbf{w}|\alpha,\eta ) & \geq \underbrace{ \mathbb{E}_q[\text{log}p(\boldsymbol{w},\boldsymbol{z},\boldsymbol{\theta},\boldsymbol{\beta}|\alpha,\eta)] - \mathbb{E}_q[\text{log}q(\boldsymbol{\theta}, \boldsymbol{z},\boldsymbol{\beta})]
%}_{\text{Evidence Lower BOund(ELBO)}}
%\end{align}

%With a factorization assumption, or referred to as mean field:
%$$q(\boldsymbol{\theta}, \boldsymbol{z},\boldsymbol{\beta}) = \prod_i %q(\theta_i)q(z_i)\prod_k q(\beta_k) $$

%Factors of the posteriors are given as:
%\begin{align*}
%q(z_{ij}) &= \phi_{i w_{ij}k}\\
%q(\theta_i) &= \text{Dirichlet}(\theta_i; \gamma_i)\\
%q(\beta_k) &= \text{Dirichlet}(\beta_k; \lambda_k)
%\end{align*}

\subsubsection{Neural Language Model}
A statistical language model (Section~\ref{sec:statistical-lm}) is straightforward in postulating the text-generating procedure under certain generative assumptions. The n-gram model, while simple and interpretable, suffers from the trade-off between expressiveness and scalability. Specifically, n-grams struggle with the sparsity problem, where the model fails to capture sufficient context due to the limited number of preceding words it considers, which restricts its ability to generalize to unseen word combinations. This limitation creates the need for more flexible approaches, where the neural language model emerges. Similarly, models like Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA), which focus on capturing latent structures within text, also have limitations. LSA assumes a linear relationship between terms and documents and relies on matrix factorization, which often lacks interpretability at the word level. LDA, while probabilistic, assumes independence between words within a topic and struggles to capture deeper syntactic relationships and complex word dependencies in longer contexts. %Both models primarily rely on word counts, operating in discrete count spaces, which limits their ability to fully leverage the rich, unstructured information present in text, such as word order, context, and semantic nuance.


The neural language model generalizes data-generating functions using neural networks, allowing it to overcome the limitations of both n-grams and probabilistic models like LDA. \cite{bengio2003neural} introduced the concept of representing words as distributed, continuous vectors, enabling the model to capture semantic relationships between words in a way that traditional statistical models could not. %By learning these word representations, the model could predict word sequences based on both local and longer-range dependencies, bridging the gap between statistical language models and neural network-based approaches. 
In such models, neural networks learn dense, continuous vector representations of words, capturing semantic relationships based on the surrounding context. Unlike LDA or LSA, which rely on fixed latent topics or linear transformations, neural language models can handle large vocabularies, generalize more effectively, and process variable-length dependencies. Examples of early neural language models include \textbf{Word2Vec} \citep{mikolov2013distributed, mikolov2013efficient} and \textbf{GloVe} \citep{pennington2014glove}. %Unlike LDA or LSA, which rely on fixed latent topics or linear transformations, neural language models like Word2Vec and GloVe can handle large vocabularies, generalize more effectively, and process variable-length dependencies, allowing them to capture deeper semantic and syntactic relationships in text.


%\subsubsection{Word2Vec}
%Word2Vec\citep{mikolov2013distributed, mikolov2013efficient} is one of the key milestone neural network-based techniques that uses a shallow neural network to learn the vector representations of words. 
Word2Vec operates on the principle that words with similar meanings tend to occur in similar contexts. It learns dense, low-dimensional vector representations for each word in the vocabulary, wherein words with similar meanings are positioned closer together in the vector space in cosine similarity. Mathematically, Word2Vec leverages assumption that a word ($w_i$) only depends on the window of size $t$ around it, simplifying the Equation~\ref{eq-ae}. The algorithm typically involves training a shallow neural network model by maximizing the conditional probabilities, either using the Continuous Bag of Words (CBOW) 
\begin{equation*}\label{eq-cbow}
    P(w_i|w_{i+t-1}, w_{i+1},w_{i-1},...,w_{i-t+1})
\end{equation*} or Skip-gram architecture, 
\begin{equation*}\label{eq-skip-grapm}
    P(w_{i+t-1}, w_{i+1},w_{i-1},...,w_{i-t+1}|w_i)
\end{equation*} to predict the surrounding words given a target word or vice versa.
The learned word embeddings are then used in various natural language processing tasks such as text classification, sentiment analysis, and machine translation, where they often lead to improved performance compared to traditional sparse representations like one-hot encodings.

%The objective for the skip-gram model is:
%$$-\frac{1}{T}\sum_{t=1}^T \sum_{-c \leq j \leq c, c \neq 0} \log p(w_{t+j}|w_t)$$ 
%where the probability is defined by softmax function $p(w_O|w_I) = \frac{\text{ext}(v_{w_O}'^Tv_{w_I})}{\sum_{w=1}^W \text{exp}(v_w'v_{w_I})}$.



%\subsubsection{GloVE}

 
GloVe (Global Vectors for Word Representation) learns word embeddings by aggregating global word co-occurrence statistics, focusing on the overall context of a corpus to model word relationships. It is an unsupervised learning algorithm for generating word embeddings, which aims to capture both global co-occurrence statistics of words across the entire corpus and local context information within individual sentences. Unlike some other word embedding techniques like Word2Vec, GloVe directly constructs a global word-to-word co-occurrence matrix from the corpus statistics and then factorizes this matrix to obtain word embeddings. This approach allows GloVe to effectively capture the semantic relationships between words based on their co-occurrence frequencies while mitigating the issues of data sparsity and computational complexity. 

The loss function to be used for training is given as:
$$\mathcal{L}_{\text{GloVe}}(\mathbf{w}) = \sum_{i,j=1}^V f(X_{ij})(v_i^\top \tilde{v_j}+b_i+\tilde{b_j}-\log X_{ij})$$
where $v_i$ is an embedding of word $w_i$ learned by neural networks, $X_{ij}$ is the number of occurrences of word $j$ in the context of word $i$, 
%$P_{ij} = P(j|i) = \frac{X_{ij}}{\sum_k X_{ik}}$, 
and $f$ is some weight function chosen beforehand. ($f(x) = (x/x_\text{max})^{3/4}$ is suggested based on the empirical performance.) 

Word2Vec and GloVe both generate dense, continuous word embeddings, but one key difference between the two models lies in how they treat input data. Word2Vec operates on discrete one-hot encoded vectors, where each word is treated as a unique entity, but the output embeddings are continuous and represent the words in a dense vector space. In contrast, GloVe starts with discrete co-occurrence counts, using these to learn continuous word vectors through matrix factorization. Word2Vec's embeddings are shaped by the local context in which words appear, and the model is better suited for learning semantic relationships based on proximity within sentences. However, because it relies on local context windows, it may miss out on broader word relationships that occur across the corpus. In contrast, GloVe emphasizes global co-occurrence patterns, capturing both local and global relationships by analyzing how words co-occur throughout the corpus. Still, both models have not yet incorporated the sequential nature of the languages, but focused on getting quality word-level embeddings. 

\subsection{Large Language Model (LLMs)}

%The importance of this paper lies in its profound impact on the development of large language models (LLMs), such as the GPT and Llama series. By leveraging the transformer architecture, these models can process and generate text with remarkable coherence and contextual understanding, leading to breakthroughs in various NLP applications, including text generation, translation, and comprehension.

The development of large language models (LLMs) is closely tied to the introduction of the transformer architecture (Sec~\ref{review:transformer}). Large language models (LLMs) can be divided into two types: \textbf{autoregressive} (Decoder-Only) and \textbf{autoencoder} (Encoder-Only) models. Autoregressive models, such as GPT series \citep{brown2020gpt3} and LLaMA series \citep{touvron2023llama}, generate text by predicting the next word in a sequence based on previous words, making them suitable for tasks like text generation and completion. Autoencoder models, like BERT \citep{devlin2019bert, reimers2019sentencebert}, and RoBERTa \citep{liu2019robertarobustlyoptimizedbert}, focus on understanding the context by masking certain words and predicting the missing parts, making them effective for tasks like text classification and question answering. Models like BART \citep{lewis2019bart} combine both approaches, using an autoencoding process for input reconstruction while employing an autoregressive decoder for text generation, making it versatile for both generation and understanding tasks.

To highlight key differences among the models, while GPT and LLaMA excel at generative tasks due to their autoregressive nature, their embeddings may be less contextually rich compared to the bidirectional embeddings produced by models like BERT, RoBERTa, and BART. Since GPT and LLaMA only capture past context (i.e., words to the left of the current word) and do not incorporate future information, they may miss deeper bidirectional context required for certain tasks. In contrast, BERT generates bidirectional embeddings by processing the entire input sequence in both directions. RoBERTa, in particular, improves upon BERT with a more optimized pretraining strategy, resulting in more robust embeddings. These differences are reflected in our study on the Federalist Papers (see Table~\ref{tab:l2_loss_for_all}), where RoBERTa produced the best results, followed closely by BART, outperforming even the most advanced models like GPT-4 (released March 2023) and LLaMA 3 (released April 2024). See Table~\ref{tab:summary-lm} for the summary of each model. 




\subsubsection{The Transformer}\label{review:transformer}
%Based on the definition, we first need to review the transformer\cite{vaswani2017attention} architecture proposed by a group of researchers at Google in 2017. The transformer introduced a novel approach that relied entirely on self-attention mechanisms to process sequential data, effectively replacing the need for recurrent neural networks (RNNs) and convolutional neural networks (CNNs) in many natural language processing (NLP) tasks. 



The original transformer model, introduced by \cite{vaswani2017attention}, transformed natural language processing by leveraging attention mechanisms to enable parallelization and better handling of long-range dependencies. This architecture, which consists of an encoder and a decoder, significantly improves efficiency and scalability during training. % Both components are composed of multiple identical layers, each contributing to the model's ability to process sequences effectively.


\paragraph{Attention Mechanism}
The attention mechanism in Transformers can be understood as a process of computing a \textbf{weighted average} of word representations in a sequence, where the weights reflect the relevance of each word to the word being processed. For each word in the sequence, the model computes a query vector $Q$, a key vector $K$, and a value vector $V$, all of which are linear transformations of the word's embedding. The attention score between two words is given by the dot product of the query from one word and the key from the other. Specifically, the attention scores for word $w_i$ with respect to all other words $w_j$ in the sequence are computed as:
$$\text{score}(w_i,w_j) = \frac{Q_i K_j^\top}{\sqrt{d_k}}$$
where $d_k$ is the dimension of the key vectors, used to scale the scores and prevent the values from growing too large. This score reflects the similarity or relevance between the two words based on their query and key vectors.

The attention scores are then normalized using the Softmax function to produce a set of weights that sum to 1:

$$\alpha_{ij} = \frac{\exp\left(\text{score}(w_i,w_j)\right)}{\sum_{k=1}^{T} \exp\left(\text{score}(w_i,w_k)\right)}$$
Here, $\alpha_{ij}$ represents the weight or attention that word $w_i$ assigns to word $w_j$. These weights are then used to compute a weighted average of the value vectors $V$ from all the words in the sequence, producing the final output for word $w_i$:
$$\text{output}_i = \sum_{j=1}^{T} \alpha_{ij} V_j.$$

In this way, the attention mechanism outputs a weighted sum of the value vectors, where the weights $\alpha_{ij}$ determine how much each word contributes to the representation of the current word $w_i$. This allows the model to focus on the most relevant words in the sequence when computing a new representation for each word.


\paragraph{Encoder Architecture} In the \textbf{encoder}, input tokens are first converted into dense vectors using an embedding layer(\textit{input embeddings}). Since transformers do not inherently capture the order of tokens, \textit{positional encodings}
%\footnote{The positional encodings are defined as $$\text{PE}_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right), \text{PE}_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right),$$ where $pos$ represents the token position, $i$ is the dimension, and $d$ is the dimensionality of the embeddings.} 
are added to the embeddings to incorporate information about the token positions within the sequence. These encodings are based on sinusoidal functions, ensuring the model can differentiate between the positions of tokens. 

At the heart of the encoder is the \textit{self-attention mechanism}, which allows the model to focus on different parts of the input sequence simultaneously. The model looks at relationships within a single sequence, allowing each word to attend to other words in the same sequence. This multi-head self-attention allows the model to capture relationships between all tokens in the input sequence, improving its ability to understand context.

Each encoder layer also contains a \textit{feed-forward neural network (FFN)}, which is applied to each token's representation independently. This network consists of two linear transformations with a ReLU activation function in between, expressed as 
$$\text{FFN}(\mathbf{x}) = \text{ReLU}(\mathbf{x} \mathbf{W}_1 + \mathbf{b}_1) \mathbf{W}_2 + \mathbf{b}_2.$$ The feed-forward layers allow for non-linear transformation of the input, further refining the token representations.

To maintain stability and improve performance, the encoder employs \textit{residual connections} and \textit{layer normalization} after each sub-layer (self-attention and FFN). These operations ensure that the input to each sub-layer is preserved, allowing for more stable gradient flow during training. Formally, after self-attention, the output is combined with the input through a residual connection and then normalized, as in 
$$\mathbf{H}^{(l)} = \text{LayerNorm}(\mathbf{H}^{(l-1)} + \text{SelfAttention}(\mathbf{H}^{(l-1)})).$$ The same process applies to the feed-forward layer: 
$$\mathbf{H}^{(l)} = \text{LayerNorm}(\mathbf{H}^{(l)} + \text{FFN}(\mathbf{H}^{(l)})).$$

\paragraph{Decoder Architecture} The \textbf{decoder} in the transformer architecture follows a similar structure to the encoder but incorporates two additional mechanisms to facilitate text generation. First, the \textit{masked self-attention mechanism} prevents the model from attending to future tokens during training, ensuring that each prediction is made based solely on previously generated tokens and the input sequence. This is crucial for autoregressive tasks such as text generation, where the model must generate tokens sequentially. Second, the \textit{encoder-decoder attention mechanism} enables the decoder to focus on relevant parts of the input sequence. In this mechanism, the queries come from the decoder's previous layer, while the keys and values are derived from the encoder's output, allowing the decoder to use contextual information from the input sequence during generation. 

After processing through these layers, the decoder's output is passed through a linear layer followed by a softmax function to generate the probability distribution over the target vocabulary, enabling text prediction.


%The original transformer model\cite{vaswani2017attention} revolutionized natural language processing by allowing parallelization through attention mechanisms. This architecture allows for better handling of long-range dependencies and parallelization during training, which significantly improves efficiency and scalability. The Transformer architecture consists of two main components: the encoder and the decoder. Each component is composed of multiple identical layers. First, the main components of the encoder can be summarized into five:
%\begin{enumerate}
%    \item Input Embedding: The input tokens are first converted into dense vectors using an embedding layer.
%    \item Positional Encoding: Since the Transformer does not inherently capture token positions, positional encodings are added to the input embeddings to provide information about the token positions in the sequence. The positional encodings are defined as:
%    $$\text{PE}_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right), \quad \text{PE}_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)$$
%   where $pos $ is the position, $i$ is the dimension, and  $d$ is the dimensionality of the embeddings.
%   \item Self-Attention Mechanism: The core of the encoder layer is the multi-head self-attention mechanism, which allows the model to focus on different parts of the input sequence simultaneously. The self-attention mechanism computes a weighted sum of the input vectors, where the weights are determined by the similarity between the input vectors. Formally, given the input $\mathbf{H}$, the attention scores are computed as:
%   $$\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q} \mathbf{K}^T}{\sqrt{d_k}}\right) \mathbf{V}$$
%   where $\mathbf{Q} = \mathbf{H} \mathbf{W}_Q $, $\mathbf{K} = \mathbf{H} \mathbf{W}_K $, and $\mathbf{V} = \mathbf{H} \mathbf{W}_V $ are the query, key, and value matrices, respectively, and $d_k$ is the dimensionality of the keys.
%   \item Feed-Forward Neural Network (FFN): Each encoder layer contains a position-wise feed-forward neural network, applied independently to each position. It consists of two linear transformations with a ReLU activation in between:
%    $$\text{FFN}(\mathbf{x}) = \text{ReLU}(\mathbf{x} \mathbf{W}_1 + \mathbf{b}_1) \mathbf{W}_2 + \mathbf{b}_2$$

% \item Residual Connections and Layer Normalization: Each sub-layer (self-attention and FFN) is followed by a residual connection and layer normalization:
%    $$\mathbf{H}^{(l)} = \text{LayerNorm}(\mathbf{H}^{(l-1)} + \text{SelfAttention}(\mathbf{H}^{(l-1)}))$$
%   $$\mathbf{H}^{(l)} = \text{LayerNorm}(\mathbf{H}^{(l)} + \text{FFN}(\mathbf{H}^{(l)}))$$
%\end{enumerate}


%The decoder has mostly symmetric to the encoder except the \textit{masked} attention. We will elaborate the two main components that are distinguished from the encoder
%\begin{enumerate}
%    \item Masked Self-Attention Mechanism: Similar to the encoder's self-attention mechanism, but with a mask applied to prevent attending to future tokens. This ensures that the prediction for each token only depends on the known outputs and the tokens before it.
%    \item Encoder-Decoder Attention Mechanism: This layer allows the decoder to focus on relevant parts of the input sequence. The queries come from the previous decoder layer, while the keys and values come from the encoder's output.
%\end{enumerate}

%The final output of the Transformer is generated by passing the decoder's output through a linear layer followed by a softmax function to obtain the probability distribution over the target vocabulary.


\subsubsection{Encoder-Only Models}
Encoder-only models, such as BERT \citep{devlin2019bert}, RoBERTa \citep{liu2019robertarobustlyoptimizedbert}, are designed for understanding and processing input text. These models utilize a stack of transformer \textbf{encoder} layers to process the entire input sequence simultaneously, leveraging self-attention mechanisms to capture bidirectional context. 
Given an input sequence of tokens $\mathbf{w} = (w_1, w_2, \ldots, w_T)$, the encoder processes these tokens through multiple layers, each layer comprising self-attention and feed-forward neural networks. 

BERT (Bidirectional Encoder Representations from Transformers) \citep{devlin2019bert}, by leveraging the bidirectional encoder layer, allows to capture the context from both directions (left and right) of a sequence simultaneously. This bidirectional training distinguishes BERT from autoregressive models like GPT, which only use past context. BERT's core innovation is its masked language model(MLM) objective, where a certain percentage of input tokens are masked, and the model is trained to predict these masked tokens based on their context. Additionally, BERT incorporates a next sentence prediction (NSP) task, where the model predicts whether two sentences appear consecutively in the original text. RoBERTa (Robustly Optimized BERT Pretraining Approach) \citep{liu2019robertarobustlyoptimizedbert} improves upon BERT with dynamic masking and training on larger datasets. 

%Building on BERT's bidirectional architecture, RoBERTa (Robustly Optimized BERT Pretraining Approach) \citep{liu2019robertarobustlyoptimizedbert} improves upon BERT with several key modifications that enhance robustness in embeddings. RoBERTa is trained on significantly larger datasets and eliminates the next sentence prediction task, which was found to contribute little to BERT’s performance. Additionally, RoBERTa introduces dynamic masking, where the masked tokens in the input change across training epochs, enabling the model to learn more diverse contextual representations.

% RoBERTa 16GB text(1 GB = 250 mil words) ~ 40 bil words
% BERT 3.3 mil words



\subsubsection{Decoder-Only Models(Autoregressive Models)}

Decoder-only transformer models, such as GPT \citep{Radford2018ImprovingLU} and Llama \citep{touvron2023llama}, are designed primarily for tasks that involve generating text. They consist of a stack of transformer \textbf{decoder} layers that predict the next word in a sequence based on the previous context, making them autoregressive. %Each decoder layer includes masked self-attention mechanisms to ensure the model only attends to past and current tokens, preventing it from ``seeing" future tokens during training. 

The GPT \citep{Radford2018ImprovingLU, brown2020gpt3} and LLaMA \citep{touvron2023llama} series are both based on autoregressive architectures, but they take distinct approaches to scaling model size, efficiency, and training data. The GPT series, beginning with GPT \citep{Radford2018ImprovingLU} and evolving through GPT-2, GPT-3, and most recently GPT-4, has consistently scaled up in terms of parameters and training data. GPT-2 was trained on about 40 GB of text, or roughly 10 billion words, primarily from OpenWebText, which allowed it to generate fluent and coherent text. With GPT-3, the scale increased dramatically—trained on around 570 GB of text, or approximately 300 billion words. GPT-4, released in March 2023, further pushed these boundaries by using even larger datasets, although the exact size remains undisclosed. 

On the other hand, the LLaMA series, developed by Meta, prioritizes parameter efficiency while still leveraging massive training datasets. LLaMA 1 was trained on 1.4 trillion tokens (approximately 1.4 trillion words), far exceeding the dataset size of GPT-3. However, LLaMA models are designed to be more efficient, achieving strong performance with significantly fewer parameters than GPT models. This efficiency makes LLaMA more accessible for research and practical deployment, especially in resource-constrained environments. %LLaMA 2, released in 2023, maintained the focus on efficiency but improved model performance further with better optimization techniques and even larger training data. 
The trend continues with the release of LLaMA 3 in April 2024. LLaMA 3 maintains its emphasis on achieving state-of-the-art results with fewer parameters but scales up the training data even further, using advanced pretraining techniques to make the model more robust while keeping its computational footprint lower compared to models like GPT-4. While GPT-4 has scaled up in both parameter count and computational demands, making it one of the largest models to date, LLaMA 3 continues to prioritize accessibility, showing that performance gains can be achieved without requiring extreme resource consumption.

In essence, while GPT models, particularly GPT-3 and GPT-4, focus on scaling up parameters and training data to achieve unparalleled performance, especially in generative tasks, the LLaMA series seeks to balance efficiency and effectiveness. LLaMA 3 exemplifies this by maintaining fewer parameters (8B, 70B) while leveraging immense datasets (over 15 trillion tokens), enabling high performance with a smaller computational footprint. This makes the LLaMA models more accessible for a broader range of applications and research settings, while GPT-4 remains the go-to for the most resource-intensive, high-performance tasks.


\subsubsection{Discussion: What Defines LLMs}\label{sec:llm-def}

The definition of Large Language Models (LLMs) is indeed a topic of divergence in the literature, with different researchers focusing on various aspects of model architecture, parameter count, and the scale of data used for training. For instance, as mentioned by \cite{zhao2023survey}, LLMs are typically defined by three key components: (1) the use of Transformer architecture, (2) the presence of hundreds of billions (or more) parameters, and (3) training on massive datasets. This definition focuses on the scale and architecture of models like GPT-3, PaLM, and LlaMA, highlighting the power of attention mechanisms and massive parameterization as core distinguishing features. 

However, other works take a narrower or more focused view of what qualifies as an LLM. For example, \cite{lu2024smalllanguagemodelssurvey} argue that encoder-only transformer models, like BERT, are not classified as LLMs under their framework, even though these models can be highly parameterized. This exclusion is based on the idea that LLMs are primarily defined by their capacity for generating human-like text, which models like BERT, trained for tasks like classification and sentence encoding, do not directly perform. Thus, this definition limits LLMs to models designed primarily for generative tasks, rather than those for representation learning or other specialized applications.

Other perspectives emphasize the role of scalability and generalization rather than just size. For instance, \cite{bommasani2022opportunities} note that LLMs are defined not only by their massive scale in terms of data and parameters but also by their general-purpose capability across a wide range of tasks, often with minimal fine-tuning. In this view, models like GPT-3 are LLMs not just because they are large but because they exhibit emergent properties like few-shot learning and can generalize across domains, whereas smaller models or task-specific models, even if large, may not exhibit these behaviors.




\begin{table}[ht]
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}ccccc@{}}
\toprule
\textbf{Model} &
  \textbf{Developer} &
  \textbf{Release Date} &
  \textbf{Training Data} &
  \textbf{\# of Parameters} \\ \midrule
Word2vec &
  Google &
  2013-01 &
  Google News dataset (about 100 billion words) &
  100 million - 3 billion \\
BERT &
  Google &
  2018-10 &
  BooksCorpus (800M words), English Wikipedia (2,500M words) &
  110M \\
RoBERTa &
  Meta &
  2019-07 &
  160GB of text data, including Common Crawl, OpenWebText, Stories, and the BooksCorpus dataset &
  355M \\
BART &
  Meta &
  2019-10 &
  160GB of text data, including Common Crawl, OpenWebText, Stories, and the BooksCorpus dataset &
  140M \\
GPT-3.5 &
  OpenAI &
  2022-03 &
  Various large text datasets, including Common Crawl, WebText, Books, and Wikipedia &
  175B \\
GPT-4  & OpenAI & 2023-03 & Various large text datasets, including Common Crawl, WebText, Books, Wikipedia, and other licensed and created datasets & 1.76T (estimated) \\
Llama2 & Meta   & 2023-07 & Various large text datasets, including Common Crawl, C4, GitHub, Wikipedia, Books, and other diverse internet sources   & 7B, 13B, 30B, 70B \\
Llama3 &
  Meta &
  2024-07 &
  Expected to use similar diverse and large-scale text datasets &
  8B, 70B \\ \bottomrule
\end{tabular}%
}
\caption{Summary of Selected Language Models}
\label{tab:summary-lm}
\end{table}

\subsubsection{llm-old}
Modern language models extend the principle to learn joint distribution of text sequence through scale and architectural refinements (of the Transformer) although the basic probabilistic formulation remains the same. For example, the autoregressive model estimates 
\begin{equation*}
P(\mathbf{w}_t|w_{1:t-1}) = \text{softmax}\big ( \text{Transformer}_\theta(w_{1:t-1}) \big)
\end{equation*}
where $\text{Transformer}_\theta(w_{1:t-1})\in \mathbb{R}^{|\mathcal{W}|}$ denotes the final output embedding of the transformer decoder. 

\paragraph{Encoder-Only Models}
Encoder-only models, such as BERT \citep{devlin2019bert}, RoBERTa \citep{liu2019robertarobustlyoptimizedbert}, are designed for understanding and processing input text. These models utilize a stack of transformer \textbf{encoder} layers to process the entire input sequence simultaneously, leveraging self-attention mechanisms to capture bidirectional context. 
Given an input sequence of tokens $\mathbf{w} = (w_1, w_2, \ldots, w_T)$, the encoder processes these tokens through multiple layers, each layer comprising self-attention and feed-forward neural networks. 

BERT (Bidirectional Encoder Representations from Transformers) \citep{devlin2019bert}, allows to capture the context from both directions (left and right) of a sequence simultaneously. BERT's core innovation is its masked language model(MLM) objective, where a certain percentage of input tokens are masked, and the model is trained to predict these masked tokens based on their context. Additionally, BERT incorporates a next sentence prediction (NSP) task, where the model predicts whether two sentences appear consecutively in the original text. RoBERTa (Robustly Optimized BERT Pretraining Approach) \citep{liu2019robertarobustlyoptimizedbert} improves upon BERT with dynamic masking and training on larger datasets. 



\paragraph{Decoder-Only Models (Autoregressive Models)}

Decoder-only transformer models consist of a stack of transformer \textbf{decoder} layers that predict the next word in a sequence based on the previous context, making them autoregressive. The only difference is when applying the attention mechanism, the mask is added to remove the future information (\textit{Masked Attention}). That is,
$$\alpha_{ij}^M = \text{Softmax}(M + \text{score}(w_i,w_j))$$ where
\begin{equation*}
M =
    \begin{cases}
        -\infty \quad \text{if } i<j\\ 
        0 \quad \quad o.w.
    \end{cases}
\end{equation*}
These models are designed primarily for tasks that involve generating text. The GPT \citep{Radford2018ImprovingLU, brown2020gpt3} and LlaMA \citep{touvron2023llama} series are both based on autoregressive architectures, but they take distinct approaches: GPT models scale all dimensions simultaneously, while LLaMA optimizes architectural efficiency. Both seek to better approximate the true probability distribution of language sequence, but through different statistical trade-offs.

GPT-3's architecture directly implemented the scaling laws by \cite{kaplan2020scaling}, which showed that model performance follows a power-law relationship with compute and parameters.
%$L(N) \approx \left(\frac{N_c}{N}\right)^\alpha$, where $L$ is the loss, $N$ is the number of parameters, $N_c$ is a critical threshold, and $\alpha \approx 0.076$. 
GPT-3 scaled to 175B parameters with increased model depth (96 layers) and width (12,288 dimensional embeddings). This approach prioritized raw performance gains through large model size, validating the theoretical prediction that language model performance would improve smoothly with scale. 

In contrast, LlaMA prioritized computational efficiency over raw scale. Instead of simply following scaling laws, LlaMA introduced architectural optimizations: pre-normalization for training stability \citep{shoeybi2019megatron}, SwiGLU activations \citep{shazeer2020glu} for better compute utilization, and rotary embeddings for enhanced position modeling \citep{su2024roformer}.  
This efficiency makes LlaMA more accessible for research and practical deployment, especially in resource-constrained environments.



\subsection{Classification Methods}\label{sec-classifier}
In the previous section (Section~\ref{sec:proposed-approach}), we described the procedure to encode the text data into a numeric vector (``embedding"). In this section, we will elaborate on the model we use to determine the authorship based on these embeddings. We aim to focus on the effect of different types of embeddings in the authorship prediction task, so we limit our choice of classifiers to rather classical ones: the least absolute shrinkage and selection operator (LASSO) \citep{tibshirani1996lasso} and BART \citep{Chipman2010bart}. Note the BART \citep{Chipman2010bart} here refers to the Bayesian Additive Regression Tree, which has the same acronym as LLM BART (Bidirectional and Auto-Regressive Transformers) \citep{lewis2019bart}. Most of the context they will be distinguished without any confusion as LLM will be used to extract the embeddings while the classifier BART will be used for prediction.
%\subsection{Latent Semantic Analysis} Given the bag-of-words approach, we run the standard SVD to go through latent semantic analysis. With the left singular vectors, we can easily find the lower dimensional projection onto the document spaces. 

\paragraph{LASSO} The LASSO \citep{tibshirani1996lasso} is a regularized regression technique which imposes a constraint on the sum of the absolute values of the model parameters. 
The LASSO regression aims to minimize the residual sum of squares subject to the constraint that the sum of the absolute values of the coefficients is less than a fixed value. Mathematically, it is defined as:
$$\hat\beta = \underset{\beta}{\text{argmin}} \left\{\|\mathbf{y} - \mathbf{X}\mathbf{\beta}\|_2^2 + \lambda \|\mathbf{\beta}\|_1 \right\}$$ where $\mathbf{y}$ is the response variable, $\mathbf{X}$ represents the $n \times p$ predictor variables, $\beta$ are the coefficients, $\lambda$ is the tuning parameter that controls the shrinkage, and $n$ and $p$ denote the number of observations and predictors, respectively.
One of the key advantages of the LASSO model is its ability to perform variable selection and regularization simultaneously, leading to sparse models where some coefficients are exactly zero. This feature is particularly useful in high-dimensional settings where the number of predictors exceeds the number of observations. By shrinking less important coefficients to zero, Lasso effectively reduces the complexity of the model and enhances its generalization performance.

A Bayesian counterpart of LASSO introduces a prior distribution over the coefficients, typically a Laplace (double-exponential) prior \citep{park2008bayesian}, which mirrors the $\ell_1$ penalty imposed by the classical Lasso. The Laplace prior for $j$-th coefficient, $\beta_j$ is given by:
$$ p(\beta_j | \lambda) = \frac{\lambda}{2} \exp(-\lambda |\beta_j|), $$
where $\lambda$ controls the shrinkage effect, similar to the regularization parameter in the classical Lasso. This prior encourages sparsity in the model by shrinking small coefficients toward zero. The Bayesian LASSO estimates the posterior distribution of the coefficients by combining the likelihood of the data with this prior.

Another important Bayesian regularization technique is the spike-and-slab model, which provides an alternative approach to variable selection. The spike-and-slab prior assumes a mixture of two distributions for the coefficients: a ``spike" at zero to shrink irrelevant coefficients and a ``slab" that allows non-zero coefficients to remain large. This creates a more explicit framework for handling sparsity, as it separates variables that should be included from those that should be excluded. Building up on this, the Spike-and-Slab LASSO \citep{rovckova2018spike} integrates these ideas by using a hierarchical prior that combines the spike-and-slab structure with the $\ell_1$ regularization of Lasso. Mathematically, the spike-and-slab prior for $\beta_j$ can be written as:

$$ p(\beta_j | \gamma, \lambda_0, \lambda_1) = (1-\gamma_j) \cdot \text{Laplace}(\beta_j|\lambda_0) + \gamma_j \cdot \text{Laplace}(\beta_j | \lambda_1), $$
where Laplace distribution has a probability density function as $\psi(x) = (\lambda/2)e^{-\lambda|x|}$, Lapalce with large $\lambda_0$ (spike) representing the probability that the coefficient is exactly zero, and the Laplace distribution (slab) allows for non-zero values with some shrinkage. Here, $\gamma_j$ is the probability of a coefficient from the ``slab" distribution. Unlike regular lasso, SSLASSO shrinks the coefficients adaptively and it would also allow the model the dependency among covariates. 

In the authorship attribution context, without preselection of words, the LASSO model successfully identifies the set of words that were found to be significant in \cite{Mosteller1963Inference}. For example, when training the binary lasso model using the 65 papers with known authorship, `whilst' turns out to be the the word with the highest coefficient in absolute value (.57), among 10 words selected by the model. We used \verb|gamlr| function from \verb|gamlr| package for the lasso with binary responses, and \verb|bmlasso| function from \verb|BhGLM| package for the spike-and-slab lasso for binary responses. 

\paragraph{Bayesian Additive Regression Tree (BART)} 
BART is a non-parametric regression technique introduced by \cite{Chipman2010bart}. It is an ensemble method that combines Bayesian inference with decision trees, providing a tool for predictive modeling. Unlike traditional regression methods, BART does not assume a specific functional form for the relationship between the predictors and the response variable, making it adaptable to various data structures.

The BART model can be formally expressed as: $$ y_i = f(\mathbf{x}_i) + \epsilon_i$$ where $y_i$ is the response variable for the $i$-th observation, $\mathbf{x}_i \in \mathbb{R}^p$ is the $p$-dimensional vector of predictor variables, $f$ is an unknown function to be estimated, and $\epsilon_i$ represents the random error term, typically assumed to follow a normal distribution $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$. Furthermore, the unknown function $f$ is represented as a sum of $m$ regression trees: $$f(\mathbf{x}) = \sum_{j=1}^m g(\mathbf{x}, T_j, M_j)$$,% where $g_j(\cdot)$ denotes the $j$-th regression trees.
where $g(\mathbf{x}; T_j, M_j)$ denotes the $j$-th regression tree, $T_j$ represents the tree structure (i.e., how the predictor space is partitioned), and $ \mathcal{M}_j$ denotes the set of terminal node parameters (i.e., the predicted values in the leaves of the tree).

Given the priors and the likelihood, the posterior distribution of the parameters is obtained using Bayesian inference. The goal is to sample from the posterior distribution: $$p((T_1, M_1), \ldots, (T_m, M_m), \sigma \mid \mathbf{y}, \mathbf{X})$$ where $\mathbf{y}\in \mathbb{R}^n$ is the vector of response variables and $\mathbf{X}\in\mathbb{R}^{n \times p}$ is the matrix of predictor variables. This is typically achieved using Markov Chain Monte Carlo (MCMC) methods. %, specifically a combination of Gibbs sampling and Metropolis-Hastings algorithms.
Once samples from the posterior distribution are obtained, predictions can be made by averaging over the sampled trees. For a new observation $\mathbf{x}^* $, the predicted response is given by:
$$\hat{y}^* = \frac{1}{N} \sum_{s=1}^N f^{(s)}(\mathbf{x}^*)$$
where $f^{(s)}$ denotes the $s$-th posterior sample of the function $f$. The uncertainty in the predictions can be quantified by the posterior predictive distribution, providing credible intervals for the predicted values.

We used \verb|gbart| function from \verb|BART| package for the BART with binary responses.


\paragraph{Models with Binary Response} Generalized Linear Models (GLMs) are an extension of linear models, where the relationship between the predictors and the response variable is mediated through a link function \citep{agresti2015foundations}. For binary classification, the logistic regression model can be used, where the probability $p_i = P(y_i=1|\mathbf{x}_i, \beta)$ of the response variable $y_i \in \{0,1\}, i \in [n]$ is modeled using the \textit{logit} function:
$$\text{logit}(p_i) = \log\left(\frac{p_i}{1 - p_i}\right) = \mathbf{x}_i^\top\beta$$
Here, $\mathbf{x}_i \in \mathbb{R}^p$ is $i$-th predictor variable of dimension $p$, and $\beta$ represents the coefficients to be estimated. A logistic regression model can be regularized with an $\ell_1$-norm penalty to perform feature selection by forcing some $\beta$ coefficients to be zero as regular lasso model does. The objective function becomes:

$$\hat\beta = \underset{\beta}{\text{argmin}} \left( - \sum_{i=1}^n \Big( y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \Big) + \lambda \|\beta\|_1 \right)$$
where $\lambda$ controls the strength of the regularization.

Similarly, binary classification with BART \citep{Chipman2010bart}, the probability of the binary outcome $Y \in \{0, 1\}$ is modeled using a latent variable approach. The probability is linked to the predictor variables through the \textit{probit} function, which uses the cumulative distribution function (CDF) of the standard normal distribution, denoted as $ \Phi(\cdot)$. The model is expressed as:
$$P(y_i = 1 \mid \mathbf{x}) = \Phi\left( \sum_{j=1}^{m} g(\mathbf{x},T_j,M_j) \right)$$
where $g(\cdot, T_j, M_j)$ represents the additive trees, and $m$ is the number of trees in the ensemble. The use of the probit function $ \Phi \left( \cdot \right)$ implies a latent variable formulation where the binary outcome depends on an underlying continuous latent variable that is normally distributed.
