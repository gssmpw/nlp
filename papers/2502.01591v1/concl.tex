\section{Conclusion and future work}
In this paper, we present three
improvements to vision-based MBRL agents which use transformer
world models for background planning: Dyna with warmup,
patch nearest-neighbor tokenization and \btf. 
We also present improvements to the MFRL baseline, which may be of independent interest.
Collectively, these improvements result in a MBRL agent that achieves a significantly higher reward and score than previous SOTA agents on the challenging Craftax-classic benchmark. 
Notably, our MBRL agent surpasses expert human reward for the first time.
In the future, we plan to examine how well our techniques generalize
%to other environments.
beyond Craftax.
However, we believe our current results will already be of interest to the community.


We see several paths to build upon our method.
Prioritized experience replay is a promising approach to accelerate TWM training, and an off-policy RL algorithm could improve policy updates by mixing imagined and real data.
% Our MBRL agent could further benefit from a prioritized experience replay \citep{Kauvar2023} to learn the TWM faster by leveraging the most relevant past experiences. 
% We would also like to move away from training the policy on images, and instead directly accept the tokens generated by TWM.
In the longer term, we would like to generalize our tokenizer to extract
patches and tokens from large pre-trained models,
such as SAM \citep{ravi2024sam} and Dino-V2 \citep{Oquab2024}.
This inherits the stable codebook of our approach, but reduces sensitivity to patch size and ``superficial" appearance variations.
% (Note, however, that such a feature extractor
% is not easily inverted, so reconstructing
% the image from the latent codes may
% require learning an explicit decoder.)
To explore this direction, and other non-reconstructive world models which cannot generate future pixels,  we plan to modify the policy to directly accept latent tokens generated by the TWM.
% We leave the exploration of this idea to future work.




 \eat{
\textbf{Beyond BTF.}
One caveat of block teacher forcing is that the tokens get sampled independently in parallel. One token cannot influence another one, which may results in mutually exclusive results.
\KPM{Earlier you claimed that BTF helped accuracy. Which is it?}
To improve the accuracy of the WM, we could replace the independent predictions
with a conditional discrete diffusion model
\citep{Shi2024},
which avoids the need to choose an ordering
of the tokens within a time slice,
but can use multiple denoising steps
to capture inter-slice correlations.
We leave exploration of this idea to future work.
}




\eat{
Orthogonal ideas. Adding PER from "Curious Replay".
Adding  contrastive loss from Moon paper.

Symbolic input.

Off-policy PPO.

SAM/VLM.

Why RNN? Achievement history
Refer to appendix for RNN+CNN Xinghua changes

Dreamer + real data

\todo{Future work to understand why MBRL does not always strictly improve over MFRL.}
}
