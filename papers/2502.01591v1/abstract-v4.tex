\begin{abstract}
We present an approach to model-based RL that achieves a new state of the art performance on the challenging Craftax-classic benchmark, an open-world $2$D survival game that requires agents to exhibit a wide range of general abilities---such as strong generalization, deep exploration, and long-term reasoning. With a series of careful design choices aimed at improving sample efficiency, our MBRL algorithm achieves a reward of $67.42\%$ after only $1$M environment steps, significantly outperforming DreamerV3, which achieves $53.2\%$, and, for the first time, exceeds human performance of $65.0\%$. Our method starts by constructing a SOTA model-free baseline, using a novel policy architecture that combines CNNs and RNNs.
We then add three improvements to the standard MBRL setup: (a) ``Dyna with warmup'', which trains the policy on real and imaginary data, (b) ``nearest neighbor tokenizer'' on image patches, which improves the scheme to create the transformer world model (TWM) inputs, and (c) ``block teacher forcing'', which allows the TWM to reason jointly about the future tokens of the next timestep.
%It then adds a series of improvements to the standard MBRL setup, including: (a) training the policy on real and imaginary data (a method we call ``Dyna with warmup''), (b) improving the visual tokenization scheme used to create the input to the transformer world model (a method we call ``online patch tokenizer''), and (c) speeding up the world model by using block teacher forcing.
\end{abstract}