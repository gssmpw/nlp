\begin{table*}[btp]
\caption{Results on Craftax-classic after 1M environment interactions. 
* denotes results on Crafter,
which may not exactly match Craftax-classic.
%\href{https://github.com/danijar/crafter?tab=readme-ov-file}{Crafter leaderboard}.
%$\dagger$ denotes results from the relevant paper.
--- means unknown.
\textdagger denotes the reported timings on a single A100 GPU.
Our DreamerV3 results are based
on the code from the author,
but differ slightly from the reported
number, perhaps due to 
hyperparameter discrepancies.
% discrepancies between Crafter and Craftax.
IRIS and $\Delta$-IRIS do not report standard errors for the score.
}
\label{tab:best_scores}
\small
\vspace{-.75em}
\centering
    \begin{tabular}{ccccc}
    \toprule
    Method & Parameters & Reward (\%) & Score (\%) & Time (min) \\
    \midrule
    Human Expert & NA & $*65.0 \pm 10.5$ & $*50.5 \pm 6.8$  & NA \\
    \midrule
    %M0: Baseline & $56.4\text{M}$ & $29.45 \pm 1.47$ & $4.43 \pm 0.36$ & $833$ \\
    M1: Baseline
    & $60.0\text{M}$ & $31.93 \pm 2.22$ & $4.98 \pm 0.50$ & $560$ \\
    M2: M1 + Dyna
    & $60.0$M & $43.36 \pm 1.84$ & $8.85 \pm 0.63$ & $563$ \\
    M3: M2 + patches
    & $56.6$M & $58.92 \pm 1.03$ & $19.36 \pm 1.42$ & $746$ \\
    M4: M3 + NNT
    & $58.5$M & $64.96 \pm 1.13$ & $25.55 \pm 0.86$  & $1328$ \\
    M5: M4 + BTF. Our best MBRL
    & $58.5$M & $\mathbf{67.42} \pm 0.55$ & $\mathbf{27.91} \pm 0.63$ & $759$ \\
    \midrule
    Previous best MFRL \citep{moon2024discovering} 
    & $4.0\text{M}$
    & $*46.91 \pm 2.41$ 
    & $*15.60 \pm 1.66$
    & ---\\
    Previous best MFRL (our implementation) & $4.0\text{M}$ & $47.40 \pm 0.58$ & $10.71 \pm 0.29$ & $26$ \\
    Our best MFRL & $55.6$M & $55.49 \pm 1.33$  & $16.77 \pm 1.11$ & $15$\\
    \midrule
    %DreamerV3 & 201M & $10.92 \pm 0.53$ & 14.77 $\pm$ 1.42 \\
    DreamerV3 \citep{hafner2023mastering} & $201$M  & $*53.2 \pm 8.$ & $*14.5 \pm 1.6$ & --- \\
    %\\ DreamerV3 XL \citep{micheli2024efficient} &  & $9.2 \pm 0.3$
    Our DreamerV3 & $201$M & $47.18 \pm 3.88$& --- & $2100$ \\
    IRIS \citep{micheli2022transformers} 
    & $48$M & $*25.0 \pm 3.2$  & $*6.66$ & \textdagger$8330$  \\
    $\Delta$-IRIS \citep{micheli2024efficient} & 25M & $*35.0 \pm 3.2$  & $*9.30$  & \textdagger$833$  \\
    Curious Replay \citep{Kauvar2023} & --- & --- & $*19.4 \pm 1.6$ & ---- \\
    \bottomrule
    \end{tabular}
%\vspace{-.75em}
\end{table*}


\section{Results}
\label{sec:results}

In this section, we report our experimental
results on the Craftax-classic benchmark.  Each experiment is run on $8$ H100 GPUs.
All methods are compared after interacting with the environment for
$T_{\text{total}}=1$M steps.
All the methods collect trajectories of length $T_{\text{env}}=96$ in $N_{\text{env}}=48$ environment (in parallel).
For MBRL methods, the imaginary rollouts
are of length $T_{\text{WM}}=20$,
and we start generating these (for policy training) 
after $T_{\text{BP}} = 200\text{k}$ 
environment steps. We update the TWM $N^{\text{iters}}_{\text{WM}}=500$ times and the policy $N^{\text{iters}}_{\text{AC}}=150$ times.
For all metrics, we report the mean and standard error over $10$ seeds as $x(\pm y)$.


\subsection{Climbing up the MBRL ladder}\label{sec:ladder}

First, we report the normalized reward (the reward divided by the maximum reward of $22$) for a series of agents that progressively climb our ``MBRL ladder" of improvements in \cref{sec:methods}.
\cref{fig:main} show the reward vs. the number of environment steps for the following methods, which we detail in Appendix \ref{ap:mbrl_modules}:
%$\bullet$ \textbf{M0: Baseline}. Our baseline MBRL method, described in \cref{sec:MBRLbaseline}, reaches $29.45\%$ reward, and slightly improves over IRIS which gets $25.0\%$.
\smallskip
\newline
$\bullet$ \textbf{M1: Baseline}. Our baseline MBRL agent, described in \cref{sec:MBRLbaseline}, reaches a reward of $31.93\%$, and improves over IRIS, which gets $25.0\%$.
\smallskip
\newline
$\bullet$ \textbf{M2: M1 + Dyna}. Training the policy on both (real) environment and (imagined) TWM trajectories, as described in Section \ref{sec:dyna}, increases the reward to $43.36\%$.
\smallskip
\newline
$\bullet$ \textbf{M3: M2 + patches}. 
Factorizing the VQ-VAE over the $L=81$ observation patches, as presented in Section \ref{sec:nnt}, increases the reward to $58.92\%$.
\smallskip
\newline
$\bullet$ \textbf{M4: M3 + NNT}.
With patch factorization, replacing the VQ-VAE with
NNT, as presented in Section \ref{sec:nnt},
further boosts the reward to $64.96\%$.
\smallskip
\newline
$\bullet$ \textbf{M5: M4 + BTF. Our best MBRL}: Finally, incorporating BTF, as described in  \cref{sec:btf}, leads to our best agent. It achieves a reward of $67.42\% (\pm 0.55)$, while BTF reduces the training time by a factor of two.

As in IRIS \citep{micheli2022transformers}, methods M1-3 use a codebook size of $512$. 
For M4 and our best MBRL, which use NNT, we found it critical to use a larger codebook size of $K=4096$ and a threshold of $\tau=0.75$. 
Interestingly, when training in imagination begins (at step $T_{\text{BP}}=200\text{k}$), there is a temporary drop in performance as the TWM rollouts do not initially match the true environment dynamics, resulting in a distribution shift for the policy. 
%See \cref{sec:ablations} for an analysis of the effect of varying $T_{\text{BP}}$.
% We found that all other models with VQ-VAE did not benefit from larger codebook sizes.


% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=.95\linewidth]{figures/ladder.pdf}
%     \vspace{-1.25em}
%     \caption{
%     The ladder of improvements presented in Section \ref{sec:methods} progressively transforms our baseline MBRL agent into a state-of-the-art method on Craftax-classic, reaching a reward of $67.42 (\pm 0.55)$ (averaged over $10$ seeds) after $1\text{M}$ environment steps.
%     Training in imagination
%     starts at step 200k, indicated
%     by the dotted vertical line.
%     % [Right] Our best MBRL and MFRL agents outperform all the previously published MFRL and MBRL results.
%     }
%     \label{fig:main}
% \vspace{-1.75em}
% \end{figure}


\begin{figure}[h!]
    \begin{minipage}{0.48\textwidth} % Adjust width as needed
    \centering
    \includegraphics[width=.95\linewidth]{figures/ladder.pdf}
    \vspace{-0.5em}
    \caption{
    The ladder of improvements presented in Section \ref{sec:methods} progressively transforms our baseline MBRL agent into a state-of-the-art method on Craftax-classic. %reaching a reward of $67.42 (\pm 0.55)$ (averaged over $10$ seeds) after $1\text{M}$ environment steps.
    Training in imagination
    starts at step 200k, indicated
    by the dotted vertical line.
    }
    \label{fig:main}
    \end{minipage}
    \hspace{0.4em}
    \begin{minipage}{0.48\textwidth} 
    \caption{Ablations results on Craftax-classic after 1M environment interactions.}
    \label{tab:ablation}
    \centering
    \resizebox{0.9\textwidth}{!}{
    \begin{tabular}{ccc}
    \toprule
    Method & Reward $(\%)$ & Score $(\%)$ \\
    \midrule
    Best MBRL  & $\mathbf{67.42} \pm 0.55$ &
    $\mathbf{27.91} \pm 0.63$ \\
    \midrule
    $5\times5$ quantized  & $57.28 \pm 1.14$ & $18.26 \pm 1.18$ \\
    $9\times9$ quantized  & $45.55 \pm 0.88$ & $10.12 \pm 0.40$ \\
    $7\times7$ continuous & $21.20 \pm 0.55$ & $2.43 \pm 0.09$ \\
    \midrule
    Remove Dyna & $55.02 \pm 5.34$ & $18.79 \pm 2.14$ \\
    Remove NNT  & $60.66 \pm 1.38$ & $21.79 \pm 1.33$ \\
    Remove NNT \& patches & $45.86 \pm 1.42$ & $10.36 \pm 0.69$ \\
    Remove BTF  & $64.96 \pm 1.13$ & $25.55 \pm 0.86$ \\
    %Remove RNN  & $42.08 \pm 1.35$ & $8.59 \pm 0.57$ \\
    %Moon MFRL & $45.24 \pm 1.23$ & $10.06 \pm 0.61$ \\
    \midrule
    Use $T_{\text{BP}}=0$ & $33.54 \pm 10.09$ & $12.86 \pm 4.05$  \\
    \midrule
    \midrule
    Best MFRL  & $55.49 \pm 1.33$ & $16.77\pm 1.11$ \\
    Remove RNN & $41.82 \pm 0.97$ & $8.33 \pm 0.44$ \\
    Smaller model  & $51.35 \pm 0.80$ & $12.93 \pm 0.56$ \\
    \bottomrule
    \end{tabular}
    }
    \end{minipage}
% \vspace{-.25em}
\end{figure}




\subsection{Comparison to existing methods}
\label{sec:cmp}

Figure \ref{fig:teaser}[left]
compares the performance of our best MBRL and MFRL agents against various previous methods. 
See also \cref{fig:score} in Appendix \ref{sec:appendix_score}
for a plot of the score,
and  \cref{tab:best_scores}
for a detailed numerical comparison
of the final performance.
% We observe the following.
First, we observe that our best MFRL agent outperforms almost
all of the previously published MFRL and MBRL results,
reaching a reward of $55.49\%$
and a score of $16.77\%$\footnote{
%
The only exception is Curious Replay \citep{Kauvar2023}, which builds on DreamerV3 with prioritized experience replay (PER)
% ~\citep{Schaul2016} 
to train the WM.
% and improves the score of DreamerV3 from 14.5\% to $19.4\%$
% The vanilla DreamerV3 method
% uses  uniform random sampling
% from the replay buffer,
% and achieves a reward of 11.7
% and a score of 14.5\%.
However, 
% the detailed breakdown of the results in \citep{Kauvar2023} show that 
PER is only better
% than DreamerV3's uniform sampling
on a few achievements;
this improves the score but not the reward.
}.
Second, our best MBRL agent
achieves a new SOTA  reward of $67.42\%$
and a score of $27.91\%$.
This marks the first agent to surpass human-level reward, derived from 100 episodes played by 5 human expert players \citep{hafner2021benchmarking}.
Note that although we achieve superhuman reward, our score is significantly below that of a human expert.


\subsection{Ablation studies}
\label{sec:ablations}

We conduct ablation studies to assess the importance of several components of our proposed MBRL agent.
Results are presented in Figure \ref{fig:ablation} and Table \ref{tab:ablation}.

\paragraph{Impact of patch size.} We investigate the sensitivity of our approach to the patch size used by NNT. While our best results are achieved when the tokenizer uses the oracle-provided ground truth patch size of $7\times 7$, Figure \ref{fig:ablation}[left] shows that performance remains competitive when using smaller ($5\times 5$) or larger ($9\times 9$) patches.

\paragraph{The necessity of quantizing.}
Figure \ref{fig:ablation}[left] shows that, when the $7\times7$ patches are not quantized, but instead the TWM is trained to reconstruct the continuous $7\times7$ patches, MBRL performance collapses. This is consistent with findings in DreamerV2 \citep{hafner2021benchmarking}, which highlight that quantization is critical for learning an effective world model.


\paragraph{Each rung matters.} To isolate the impact of each individual improvement, we remove each individual ``rung'' of our ladder from our best MBRL agent. As shown in Figure \ref{fig:ablation}[middle], each removal leads to a performance drop. This underscores the importance of combining all our proposed enhancements to achieve SOTA performance. 


\paragraph{When to start training in imagination?} Training the policy on imaginary TWM rollouts requires a reasonably accurate world model. This is why background planning (Step 4 in Algorithm \ref{algo:MBRL})
only begins after $T_{\text{BP}}$ environment steps.
Figure \ref{fig:ablation}[right]
explores the effect of varying $T_{\text{BP}}$.
% between $0, 100\text{k}, 200\text{k}, 400\text{k}, 600\text{k}$. 
Initiating imagination training too early ($T_{\text{BP}}=0$) leads to performance collapse due to the inaccurate TWM dynamics.
% Note that, 
%Figure \ref{fig:wm_quality} suggests that TWM training stabilizes after around $100\text{k}$ steps, supporting the choice of delaying imagination training.

\paragraph{MFRL ablation.} The final 3 rows in Table \ref{tab:ablation} show that either removing the RNN or using a smaller model as in \citet{moon2024discovering} leads to a drop in performance.


% \begin{table}[h!]
% \caption{Ablations results.}
% \label{tab:ablation}
% \small
% \vspace{-1em}
% \centering
% \begin{tabular}{ccc}
% \toprule
% Method & Reward $(\%)$ & Score $(\%)$ \\
% \midrule
% Best MBRL  & $\mathbf{67.42} \pm 0.55$ &
% $\mathbf{27.91} \pm 0.63$ \\
% \midrule
% $5\times5$ quantized  & $57.28 \pm 1.14$ & $18.26 \pm 1.18$ \\
% $9\times9$ quantized  & $45.55 \pm 0.88$ & $10.12 \pm 0.40$ \\
% $7\times7$ continuous & $21.20 \pm 0.55$ & $2.43 \pm 0.09$ \\
% \midrule
% Remove Dyna & $55.02 \pm 5.34$ & $18.79 \pm 2.14$ \\
% Remove NNT  & $60.66 \pm 1.38$ & $21.79 \pm 1.33$ \\
% Remove NNT \& patches & $45.86 \pm 1.42$ & $10.36 \pm 0.69$ \\
% Remove BTF  & $64.96 \pm 1.13$ & $25.55 \pm 0.86$ \\
% %Remove RNN  & $42.08 \pm 1.35$ & $8.59 \pm 0.57$ \\
% %Moon MFRL & $45.24 \pm 1.23$ & $10.06 \pm 0.61$ \\
% \midrule
% Use $T_{\text{BP}}=0$ & $33.54 \pm 10.09$ & $12.86 \pm 4.05$  \\
% \midrule
% \midrule
% Best MFRL  & $55.49 \pm 1.33$ & $16.77\pm 1.11$ \\
% Remove RNN & $41.82 \pm 0.97$ & $8.33 \pm 0.44$ \\
% Smaller model  & $51.35 \pm 0.80$ & $12.93 \pm 0.56$ \\
% \bottomrule
% \end{tabular}
% \vspace{-1em}
% \end{table}

 
\begin{figure*}[t!]
    \centering
    \begin{tabular}{c}
        \includegraphics[width=0.31\textwidth]{figures/patch_size.pdf}
    \end{tabular}
    \hspace{-5mm}
    \begin{tabular}{c}
        \includegraphics[width=0.31\textwidth]{figures/remove_ladder_step.pdf}
    \end{tabular}
    \hspace{-5mm}
    \begin{tabular}{c}
        \includegraphics[width=0.31\textwidth]{figures/start_bp.pdf}
    \end{tabular}
    \vspace{-1em}
    \caption{[Left] MBRL performance decreases when NNT uses patches of smaller or larger size than the ground truth, but it remains competitive. However, performance collapses if the patches are not quantized. 
    [Middle] Removing any rung of the ladder of improvements leads to a drop in performance.
    [Right] Warming up the world model before using it to train the policy on imaginary rollouts is required for good performance. BP denotes background planning. For each method, training in imagination
    starts at the color-coded vertical line, and leads to an initial drop in performance.}
    \label{fig:ablation}
\vspace{-.5em}
\end{figure*}
 

\subsection{Comparing TWM rollouts}
\label{sec:stationary}

In this section, we compare the TWM rollouts learned by three world models in our ladder, namely M1, M3 and our best model M5. 
To do so, we first create an evaluation dataset of $N_{\text{eval}}=160$ trajectories, each of length $T_{\text{eval}}=T_{\text{WM}}=20$, collected during the training of our best MFRL agent:
$\mathcal{D}_{\text{eval}} = \left\{O^{1:N_{\text{eval}}}_{1:T_{\text{eval}}+1}, a^{1:N_{\text{eval}}}_{1:T_{\text{eval}}}, r^{1:N_{\text{eval}}}_{1:T_{\text{eval}}} \right\}$.
We evaluate the quality of imagined trajectories generated by each TWM. 
Given a TWM checkpoint at 1M steps and the $n$th trajectory in $\mathcal{D}_{\text{eval}}$, we execute the sequence of actions $a^n_{1:T_{\text{eval}}}$, starting from $O^n_{1}$, to obtain a rollout trajectory $\hat{O}^{\text{TWM},~n}_{1:T_{\text{eval}} + 1}$. 

% in $\mathcal{D}_{\text{eval}}$:
%However, while some errors may be due to TWM incorrectly predicting the game dynamics, other may be due to unpredictable environment changes (lighting, sprites moving...) To focus on the former, and control for the latter, we use a reference rollout (RR), which always predicts the initial observation and does not capture the game dynamics. RR has a reconstruction error of $\mathcal{E}^n_{t, ~\text{ref.}} = \| O^n_1 - O^n_t \|_2^2, ~\forall t$. The normalized observation reconstruction error at time $t$, averaged over the $N_{\text{eval}}$ trajectories is then:
% \vspace{-.5em}




\eat{
\textbf{Quantitative evaluations.}
To quantitatively evaluate the rollouts, in Figure \ref{fig:rollout_eval}[left] we plot the average L$2$ reconstruction error at each timestep $t$:
$
    \mathcal{E}_t = 
    \frac{1}{N_{\text{eval}}} \sum_{n=1}^{N_{\text{eval}}} \| \hat{O}^{\text{TWM},~n}_t - O^n_t \|_2^2,
    ~~~\forall t.
$
As expected, the error increases with $t$ as mistakes compound over the rollout.
Our best method, which uses NNT, achieves the lowest errors for all timesteps, highlighting that a stationary codebook makes learning simpler for the TWM, and leads to more accurate TWM rollouts.


We include two additional quantitative evaluations in Appendix \ref{ap:symbol_accuracy}. 
First, M5 achieves the lowest tokenizer reconstruction error in Figure \ref{fig:wm_quality_appendix}[left]. 
Second, we leverage the fact that each observation in Craftax-classic comes with a symbolic representation to compare symbol prediction accuracy from TWM rollouts in Figure \ref{fig:wm_quality_appendix}[right].
This metric further highlights that M5 best captures the game dynamics. 
}






\paragraph{Quantitative evaluations.}
For evaluation,
we leverage an appealing property of Craftax-classic: each observation $O_t$ comes with an array of ground truth symbols $S_t=(S_t^{1:R})$, with $R=145$. Given $100\text{k}$ pairs $(O_t, S_t)$, we train a CNN $f_{\mu}$, to predict the symbols from the observation; $f_{\mu}$ achieves a $99\%$ validation accuracy. Next, we use $f_{\mu}$ to predict the symbols from the generated rollouts. Figure \ref{fig:rollout_eval}[left] displays the average symbol accuracy at each timestep $t$:
\begin{equation*}
    \mathcal{A}_t = \frac{1}{N_{\text{eval}}R} \sum_{n=1}^{N_{\text{eval}}} \sum_{r=1}^{R}\mathbf{1}(f_{\mu}^r(\hat{O}^{\text{TWM},~n}_t), S^{r,n}_t), ~ \forall t,
\end{equation*}
where $\mathbf{1}(x,y) = 1 ~\text{iff.}~ x=y $ (and $0$ o.w.), $S^{r,n}_t$ denotes the ground truth $r$th symbol in the array $S^{n}_t$ associated with $O^n_t$, and $f_{\mu}^r(\hat{O}^{\text{TWM},~n}_t)$ its prediction for the rollout observation.
As expected, symbol accuracies decrease with $t$ as mistakes compound over the rollouts.
Our best method, which uses NNT, achieves the highest accuracies for all timesteps, as it best captures the game dynamics. This highlights that a stationary codebook makes TWM learning simpler.

We include two additional quantitative evaluations in Appendix \ref{ap:symbol_accuracy}, showing that M5 achieves the lowest tokenizer reconstruction errors and rollout reconstruction errors.




\paragraph{Qualitative evaluations.}
Due to environment stochasticity, TWM rollouts can differ from the environment rollout but still be useful for learning in imagination---as long as they respect the game dynamics.
Visual inspection of rollouts in Figure \ref{fig:rollout_eval}[right] reveals (a) map inconsistencies, (b) feasible hallucinations that respect the game dynamics and (c) infeasible hallucinations. 
% For learning in imagination, it is important for the model to generate realistic, diverse and eventful rollouts in which the
M1 can make simple mistakes in both the map and the game dynamics. 
M3 and M5 both generate feasible hallucinations of mobs, however M3 more often hallucinates infeasible rollouts.



\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/wm_rollout_norm_symbol_accuracy_by_step.pdf}
        \hrule
        \vspace{0.3em}
        \includegraphics[width=0.8\linewidth]{figures/rollouts_key.pdf}
    \end{subfigure}
    \hspace{0.5em}
    \begin{subfigure}[b]{0.65\textwidth}
        \includegraphics[width=\linewidth]{figures/rollouts.pdf}
    \end{subfigure}
    \vspace{-0.5em}
    \caption{
    \small
    Rollout comparison for world models M1, M3 and M5.
    [Left]
    % The observation reconstruction error increases with the TWM rollout step. 
    Symbol accuracies decrease with the TWM rollout step. 
    The stationary NNT codebook used by M5 makes it easier to learn a reliable TWM.
    [Right]
    Best viewed zoomed in.
    \textbf{Map.}
    All three models accurately capture the agent's motion.
    All models can struggle to use the history to generate a consistent map when revisiting locations, however only M1 makes simple map errors in successive timesteps.
    \textbf{Feasible hallucinations.}
    M3 and M5 generate realistic hallucinations that respect the game dynamics, such as spawning mobs and losing health.
    \textbf{Infeasible hallucinations.}
    M1 often does not respect game dynamics; M1 incorrectly adds wood inventory, and incorrectly places a plant at the wrong timestep without the required sapling inventory.
    M3 exhibits some infeasible hallucinations in which the monster suddenly disappears or the spawned cow has an incorrect appearance.
    M5 rarely exhibits infeasible hallucinations.
    Figure \ref{fig:more_rollouts} in Appendix \ref{ap:rollout_comparison} shows more rollouts with similar behavior.
    % However this property may not be crucial for learning in imagination, since the policy does not have an accurate memory and tends to explore to find new resources rather than navigating to a previously seen resource.
    }
    \label{fig:rollout_eval}
\vspace{-.25em}
\end{figure*}
 



\eat{
\begin{figure}[t!]
    \centering
    %\includegraphics[width=0.35\textwidth]{figures}
    \caption{MFRL ablations between ours and Moon.}
\end{figure}
}
\eat{
\begin{figure}[t!]
    \centering
    %\includegraphics[width=0.35\textwidth]{figures}
    \caption{Sweep over $T_{WM}$ from 5-30. TWM sequence length is always 20. Link to rollout accuracy plot.}
\end{figure}
}


\subsection{Craftax Full}
\label{sec:craftax_full}

Table \ref{tab:craftax_full} compares the performance of various agents on
the full version of Craftax \citep{matthews2024craftax}, a significantly harder extension of Craftax-classic,
with more levels and achievements. While the previous SOTA agent reached $2.3\%$ reward (on symbolic inputs), our MFRL agent reaches
$4.63\%$ reward and our MBRL agent reaches a new SOTA reward of $5.44\%$.
See Appendix \ref{sec:classic_vs_full} for implementation details.
These results show that our techniques can generalize to harder environments.
%although further evaluation on more diverse environments is left to future work.


\begin{table}[h!]
\caption{
Results on Craftax after 1M environment interactions.
The previous SOTA reward uses symbolic input (score is unknown),
whereas our results use image input.
}
\label{tab:craftax_full}
\small
\vspace{-.5em}
\centering
\begin{tabular}{ccc}
\toprule
Method & Reward $(\%)$ & Score $(\%)$ \\
\midrule
Prev. SOTA MFRL  & $2.3$ (symbolic) & --- \\
Our best MFRL &  $4.63 \pm 0.20$ & $1.22 \pm 0.07$ \\
Our best MBRL  & $5.44 \pm 0.25$ & $1.53 \pm 0.10$ \\
\bottomrule
\end{tabular}
\vspace{-.25em}
\end{table}

% (see \cref{tab:classic_vs_full}).




 