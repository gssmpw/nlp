\section{Methods}
\label{sec:methods}

% Here, we describe the components
% of our system, each of which improves
% performance, as we show in \cref{sec:results}.

\subsection{MFRL Baseline}
\label{sec:MFRL}

Our starting point is the previous SOTA
MFRL approach which was proposed as a baseline in
\citet{moon2024discovering}\footnote{%
The authors' main method uses external knowledge about the achievement hierarchy of Crafter, so cannot be compared with other general methods. We use their baseline instead.
}.
This method achieves a reward of $46.91\%$ and a score of $15.60\%$ after $1$M environment steps.
This approach trains a stateless CNN policy without frame stacking
using the PPO method \citep{schulman2017proximal}, and
adds an entropy penalty to ensure sufficient exploration.
The CNN used is a modification of the Impala
ResNet \citep{Espeholt2018}.


\subsection{MFRL Improvements}

We improve on this MFRL baseline
by both increasing the model size and adding a RNN (specifically a GRU) to give the policy memory.
Interestingly, we find that naively increasing the model size harms performance, while combining a larger model with a carefully designed RNN helps (see Section \ref{sec:ablations}). 
For the RNN,
we find it crucial to ensure
the hidden state is low-dimensional,
so that the memory is forced to focus on the relevant
bits of the past that cannot be extracted
from the current image.
We concatenate the GRU output to the image
embedding, and then pass this to the actor and critic networks,
rather than directly passing the GRU output. Algorithm \ref{algo:ac_network}, Appendix \ref{ap:mfrl_agent}, presents a pseudocode for our MFRL agent.


With these architectural changes,
we increase the reward to $55.49\%$ and the score to $16.77\%$.
This result is notable since our MFRL agent beats the considerably more complex (and much slower) DreamerV3 agent, which obtains a reward of $53.20\%$ and a score of $14.5$. 
It also beats other MBRL methods, such as IRIS \citep{micheli2022transformers} (reward of $25.0\%$) and $\Delta$-IRIS \citep{micheli2024efficient}
\footnote{
This is consistent with results on Atari-100k, which show that well-tuned model-free methods, such as BBF \citep{Schwarzer2023}, can beat more sophisticated model-based methods.} (reward of $35.0\%$). 
In addition, our MFRL agent only takes
$15$ minutes to train for $1$M environment
steps on one A100 GPU.
%whereas our MBRL takes about $12$ hours, and Dreamer V3 takes about $35$ hours. \todo{Making the model larger harms performance, but when combined with the RNN, this improves performance. Reference plots in results.}

% \begin{algorithm}[h]
% \caption{
% Rollout with actor-critic network.
% \AD{should we add reset if done to the algorithm?}
% \JO{Remove this psuedocode?}
% }
% \label{algo:mfrl_collect}
% \begin{algorithmic}
% \STATE {\bfseries Input:} obs. $O_1$, AC parameters $\Phi$, horizon $T$ \\
% An environment transition function $\Mtrue$
% \STATE {\bfseries Output:} An environment rollout $(O_1, a_1, r_1 \ldots O_t)$ \\
% as well as a sequence of values $(v_1, \ldots v_{T-1})$
% \STATE {\bfseries Initialize:} hidden state $h_0=0$
% \FOR{$t=1$ {\bfseries to} $T$}
% \STATE \textit{// Run the actor-critic network}
% \STATE $z_t = \text{ImpalaCNN}_{\Phi}(O_t)$
% \STATE $h_t, y_t = \text{RNN}_{\Phi}( [h_{t-1}, z_t])$
% \STATE $a_t \sim \pi_{\Phi}([y_t, z_t])$
% \STATE $V_t = V_{\Phi}([y_t, z_t])$
% \STATE \textit{// Collect reward and next observation}
% \STATE $r_t, O_{t+1} = \Mtrue(O_t, a_t)$
% \ENDFOR
% \end{algorithmic}
% \end{algorithm}


\subsection{MBRL baseline}
\label{sec:IRIS}
\label{sec:MBRLbaseline}

We now describe our MBRL baseline,
which combines our MFRL baseline above with a transformer world model (TWM)---as in
IRIS \citep{micheli2022transformers}. Following IRIS, our MBRL baseline uses a VQ-VAE, which quantizes the
$8 \times 8$ feature map $Z_t$ of a CNN
to create a set of latent codes,
$(q_t^1,\ldots,q_t^L) = \text{enc}(O_t)$,
where $L=64$, $q_t^i \in \{1,\ldots,K\}$
is a discrete code,
and $K=512$ is the size of the codebook.
These codes are then passed to a TWM, which is trained using
teacher forcing---see \cref{eqn:lossAR} below. 
Our MBRL baseline achieves a reward of $31.93\%$, and improves over the reported results of IRIS, which reaches $25.0\%$. 

Although these MBRL baselines leverage recent advances in generative world modeling, they are largely outperformed by our best MFRL agent. This motivates us to enhance our MBRL agent, which we explore in the following sections.

%Next, we improve this baseline by replacing the stateless CNN policy networks with our CNN + GRU described in \cref{sec:MFRL}. This increases the reward to . 


\subsection{MBRL using Dyna with warmup}
\label{sec:dyna}
\label{sec:warmup}

As discussed in \cref{sec:introduction}, we propose to train our MBRL agent on a mix of real trajectories (from the environment) and imaginary trajectories (from the TWM), similar to Dyna \citep{sutton1990integrated}.
\cref{algo:MBRL} presents the pseudocode for our MBRL approach.
Specifically, unlike many other recent MBRL methods \citep{Ha2018, micheli2022transformers, micheli2024efficient, hafner2020mastering, hafner2023mastering} which train their policies exclusively using world model rollouts (Step 4), we include Step 2 which updates the policy with real trajectories.
Note that, if we remove Steps 3 and 4 in Algorithm \ref{algo:MBRL}, the approach reduces to MFRL.
The function $\text{rollout}(O_1, \pi_{\Phi},  T, \model)$ returns a trajectory of length $T$ generated by rolling out the policy $\pi_{\Phi}$ from the initial state $O_1$ in either the true environment $\Mtrue$ or the world model $\mathcal{M}_{\Theta}$.
A trajectory contains collected observations, actions and rewards during the rollout $\tau = (O_{1:T+1}, a_{1:T}, r_{1:T})$.
Algorithm \ref{algo:rollout} in Appendix \ref{ap:mbrl_agent} details the rollout procedure.
We discuss other design choices below. 

\begin{algorithm}[!htbp]
\caption{MBRL agent. See Appendix \ref{ap:mbrl_agent} for details.}
\label{algo:MBRL}
\begin{algorithmic}
\STATE {\bfseries Input:} number of environments $N_{\text{env}}$,
\newline
environment dynamics $\Mtrue$,
\newline
rollout horizon for environment $T_{\text{env}}$ and for TWM $T_{\text{WM}}$,
\newline
background planning starting step $T_{\text{BP}}$,
\newline
total number of environment steps $T_{\text{total}}$,
\newline
number of TWM updates $N^{\text{iters}}_{\text{WM}}$ and policy updates $N^{\text{iters}}_{\text{AC}}$
\medskip
\STATE {\bfseries Initialize:} observations $O^n_1 \sim \Mtrue ~\text{for}~ \small{n=1:N}$,
\newline
data buffer $\mathcal{D}=\emptyset$, 
\newline
TWM model $\mathcal{M}$ and parameters $\Theta$, 
\newline
AC model $\pi$ and parameters $\Phi$,
\newline
number of environment steps $t=0$.
\smallskip
\REPEAT
\STATE \textit{// 1. Collect data from environment}
\STATE $\tau^n_{\text{env}} = \text{rollout}
(O_1^n, \pi_{\Phi}, T_{\text{env}}, \Mtrue),~~n=1:N_{\text{env}}$
\STATE $\mathcal{D} = \mathcal{D} \cup \tau^{1:N}_{\text{env}} ~;~ O^{1:N}_1 = \tau^{1:N}_{\text{env}}[-1] ~;~ t += N_{\text{env}}T$
\medskip
\STATE \textit{// 2. Update policy on environment data}
\STATE $\Phi=\text{PPO-update-policy}(\Phi,\tau_{\text{env}}^{1:N})$\\
\medskip
\STATE \textit{// 3. Update world model}
\FOR{$\text{it}=1$ {\bfseries to} $N^{\text{iters}}_{\text{WM}}$}
\STATE  $\tau_{\text{replay}} ^{n} =\text{sample-trajectory}(\mathcal{D}, T_{\text{WM}}), ~~n=1:N_{\text{env}}$ \\
\STATE  $\Theta = \text{update-world-model}(\Theta, \tau_{\text{replay}}^{1:N_{\text{env}}})$\\
\ENDFOR
\medskip
\STATE \textit{// 4. Update policy on imagined data}
\IF{ $t \ge T_{\text{BP}}$}
\FOR{$\text{it}=1$ {\bfseries to} $N^{\text{iters}}_{\text{AC}}$}
\STATE $O_1^n = \text{sample-obs}(\mathcal{D}), ~~n=1:N_{\text{env}}$ 
\STATE $\small{\tau_{\text{WM}}^{n}=\text{rollout}
(O_1^n,\pi_{\Phi}, T_{\text{WM}},\mathcal{M}_{\Theta}), ~n=1:N_{\text{env}}}$ 
\STATE $\Phi=\text{PPO-update-policy}(\Phi,\tau_{\text{WM}}^{1:N_{\text{env}}})$
\ENDFOR
\ENDIF
\UNTIL $t \ge T_{\text{total}}$
\end{algorithmic}
\end{algorithm}

\textbf{PPO.}
Since PPO \citep{schulman2017proximal} is an on-policy algorithm, trajectories should be used for policy updates immediately after they are collected or generated. 
For this reason, policy updates with real trajectories take place in Step 2 immediately after the data is collected. 
An alternative approach is to use an off-policy algorithm and mix real and imaginary data into the policy updates in Step 4, hence removing Step 2. We leave this direction as future work. 

\textbf{Rollout horizon.}
We set $T_{\text{WM}} \ll  T_{\text{env}}$,
to avoid the problem of compounding errors
due to model imperfections \citep{Lambert2022}.
However, we find it beneficial to use
$T_{\text{WM}} \gg 1$,
consistent with
\citet{Holland2018,van2019use},
who observed that the
Dyna approach with $T_{\text{WM}}=1$
is no better than
MFRL with experience replay.

\textbf{Multiple updates.} Following IRIS, we update TWM $N^{\text{iters}}_{\text{WM}}$ times and the policy on imagined trajectories $N^{\text{iters}}_{\text{AC}}$ times.

\textbf{Warmup.}
When mixing imaginary trajectories with real ones, we need to ensure the WM is sufficiently accurate so that it does not ``pollute" the replay buffer, thus harming policy learning. 
Consequently, we only begin training the policy
on imaginary trajectories after the agent has interacted with the environment for $T_{\text{BP}}$ steps, which ensures it has seen enough data to learn a reliable WM. We call this technique ``Dyna with warmup''. In \cref{sec:ablations}, we show that removing this warmup, and using $T_{\text{BP}}=0$, drops the reward dramatically, from $67.42\%$ to $33.54\%$. We additionally show that removing the Dyna method (and only training the policy in imagination) drops the reward to $55.02\%$. 



\eat{
In \cref{sec:results}, we show the advantage of using Dyna rather than only training the policy
in imagination,
and the advantage of waiting until the WM is warmed up
rather than using it immediately.
}

\subsection{Patch nearest-neighbor tokenizer}
\label{sec:nnt}
\label{sec:patches}

Many MBRL methods based on TWMs use a VQ-VAE to map between images and tokens.
In this section, we describe our alternative which leverages a property of Craftax-classic: each observation is composed of $9\times9$ patches of size $7 \times 7$ each (see \cref{fig:teaser}[middle]). Hence we propose to (a) factorize the tokenizer by patches and (b) use a simpler nearest-neighbor style approach to tokenize the patches.

\paragraph{Patch factorization.}
Unlike prior methods which process the full image $O$ into tokens $(q^1,\ldots,q^L) = \text{enc}(O)$, we first divide $O$ into $L$ non-overlapping patches
$(p^1, \ldots, p^L)$ which are independently encoded into $L$ tokens:
\begin{equation*}
    (q^i,\ldots, q^L) = (\text{enc}(p^1), \ldots, \text{enc}(p^L))
    ~.
\end{equation*}
To convert the discrete tokens back to pixel
space, we just decode each token independently into patches, and rearrange to form a full image:
\begin{equation*}
    (\hat{p}^1, \ldots, \hat{p}^L) = 
    (\text{dec}(q^1), \ldots, \text{dec}(q^L))
    ~.
\end{equation*}
Factorizing the VQ-VAE on the $L=81$ patches of each observation boosts performance from $43.36\%$ to $58.92\%$.

\paragraph{Nearest-neighbor tokenizer.}
On top of patch factorization, we propose a simpler nearest-neighbor tokenizer (NNT) to replace the VQ-VAE.
The encoding operation for each patch $p \in [0, 1]^{h\times w\times3}$
is similar to a nearest neighbor classifier w.r.t the codebook.
% as defined in Equation \ref{eqn:opt}.
The difference is that, if the nearest neighbor
is too far away, 
% (beyond a threshold distance of $\tau$)
we add a new code equal to $p$ to the codebook.
%similar to a greedy online version of VQ.
More precisely, let us denote  $\mathcal{C}_{\text{NN}}=\{e_1, \ldots,e_K\}$ the current codebook,
consisting of $K$ codes $e_i \in [0, 1]^{h\times w\times3}$, 
and $\tau$ a threshold on the Euclidean distance. 
The NNT encoder is defined as:
\begin{equation}
    \small
    q = \text{enc}(p) = 
    \begin{cases}
    \begin{alignedat}{2}
        &\argmin_{1\le i \le K} \| p -  e_i\|_2^2 &&\quad\text{if $\min_{1\le i \le K} \| p - e_{i} \|_2^2 \le \tau$} \\
        &K+1 &&\quad\text{otherwise.}
    \end{alignedat}
    \end{cases}
    \label{eqn:opt}
\end{equation}
The codebook can be thought of as a
greedy approximation to the coreset of 
the patches seen so far
\citep{Mirzasoleiman2020}. 
To decode patches, we simply return the code associated with the codebook index, i.e. $\text{dec}(q^i)=e_{q^i}$.

A key benefit of NNT is that once codebook entries are added, they are never updated.
A static yet growing codebook makes the target distribution for the TWM stationary, greatly simplifying online learning for the TWM.
In contrast, the VQ-VAE codebook is continually updated, meaning the TWM must learn from a non-stationary distribution, which results in a worse WM.
Indeed, we show in \cref{sec:ladder} that with patch factorization, and when $h=w=7$---meaning that the patches are aligned with the observation---replacing the VQ-VAE with NNT boosts the agent's reward from $58.92\%$ to $64.96\%$. 
Figure \ref{fig:teaser}[right] shows an example of the first 64 code patches extracted by our NNT.

The main disadvantages of our approach are that (a) patch tokenization can be sensitive to the patch size (see Figure \ref{fig:ablation}[left]),
and (b) NNT may create a large codebook if there is a lot
of appearance variation within patches.
% (depending on the threshold $\tau$)
In Craftax-classic, these problems are not very severe due to the grid structure of the game and limited sprite vocabulary (although continuous variations exist due to lighting and texture randomness).


%Furthermore each patch is derived from a fixed vocabulary of "sprites"---although there is continuous variation due to lighting changes (between day and night), and random variations in the texture of terrain regions such as grass and water).



\subsection{Block teacher forcing}
\label{sec:btf}


\begin{figure}[h!]
    \centering
    \begin{tabular}{c}
        \includegraphics[width=.45\linewidth]{figures/btf_part1.pdf} 
    \end{tabular}
    %\hspace{-1em}
    \begin{tabular}{c}
        \includegraphics[width=.47\linewidth]{figures/btf_part2.pdf}
    \end{tabular}
    \caption{
    Approaches for TWM training with $L=2$, $T=2$.
    $q_t^{\ell}$ denotes token $\ell$ of timestep $t$. Tokens in the same timestep have the same color. 
    We exclude action tokens for simplicity.
    [Left] Usual autoregressive model training with teacher forcing. 
    [Right] Block teacher forcing predicts token $q_{t+1}^{\ell}$ from input token $q_{t}^{\ell}$ with block causal attention.
    }
    % \vspace{-.75em}
    \label{fig:btf}
\end{figure}

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=.9\linewidth]{figures/btf.pdf}
%     \caption{
%     Approaches for TWM training with $L=2$, $T=2$.
%     $q_t^{\ell}$ denotes token $\ell$ of timestep $t$. Tokens in the same timestep have the same color. 
%     We exclude action tokens for simplicity.
%     [Top] Usual autoregressive model training with teacher forcing. 
%     [Bottom] Block teacher forcing predicts token $q_{t+1}^{\ell}$ from input token $q_{t}^{\ell}$ with block causal attention.
%     }
%     \vspace{-1em}
%     \label{fig:btf}
% \end{figure}


Transformer WMs are typically trained by teacher forcing which maximizes
the log likelihood of the token sequence generated autoregressively over time
and within a timeslice:
\vspace{-.75em}
\begin{equation}
\small
    \mathcal{L}_{\text{TF}} = 
    \log \prod_{t=1}^{T} \prod_{i=1}^L \mathcal{L}_t^i ~,~~~~
    \mathcal{L}_t^i
    =
    p(q_{t+1}^i | q_{1:t}^{1:L},  q_{t+1}^{1:i-1}, a_{1:t})
    \label{eqn:lossAR}
\end{equation}
We propose a more effective alternative, which we call
\btf~(\btfac).
BTF modifies both the supervision and the attention of the TWM. Given the tokens from the previous timesteps,
BTF independently predicts all the latent tokens at the next timestep, removing the conditioning on previously generated tokens from the current step:
\vspace{-.5em}
\begin{equation}
\small
    \mathcal{L}_{\text{BTF}} = 
    \log \prod_{t=1}^{T} \prod_{i=1}^L \mathcal{\tilde{L}}_t^i ~,~~~~
    \mathcal{\tilde{L}}_t^i
    =
    p(q_{t+1}^i | q_{1:t}^{1:L}, a_{1:t})
    \label{eqn:lossBTF}
\vspace{-.4em}
\end{equation}
Importantly BTF uses a block causal attention pattern (see Figure \ref{fig:btf}), in which tokens within the same timeslice are decoded in-parallel in a single forward pass.
This attention structure allows the model to reason jointly about the possible future states of all tokens within a timestep, before the tokens are ultimately sampled with independent readouts.
% \citep{bachmann24a}. 
%\MLG{Unclear. Learning happens to make it deterministic, or is it deterministic by design, so that it can't capture stochastic environments? What about unobserved patches that a appear from an edge, shouldn't it be a stochastic prediction?}
This property mitigates autoregressive drift. As a result, we find that BTF returns more accurate TWMs than fully AR approaches.
Overall, adding BTF increases the reward from $64.96\%$ to $67.42\%$, leading to our best MBRL agent.
% Even though this ignores correlation between 
% the symbols within a time slice (a "naive Bayes" assumption), it significantly accelerates world model training and rollouts, which are the main bottlenecks in MBRL.
In addition, we find that BTF is twice as fast,
even though in theory,
%In theory, 
when using key-value caching, BTF and AR both have complexity $\mathcal{O}(L^2 T)$ for generating all the $L$ tokens at one timestep, and $\mathcal{O}(L^2 T^2)$ for generating the entire rollout.
%However, in practice we find that BTF speeds up training by a factor of two for $L=81$ and $T=20$. 

\eat{
This also seems to slightly
improve the accuracy of the WM and hence the agent's performance: removing \btfac~ drops
the reward from 14.83 to 14.29,
as we show in \cref{sec:results}.
}