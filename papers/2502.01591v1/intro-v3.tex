\section{Introduction}
\label{sec:introduction}


Reinforcement learning (RL) \citep{sutton2018reinforcement} 
provides a framework for training agents to act in environments so as to maximize their rewards. Online RL algorithms interleave taking actions in the environment---collecting observations and rewards---and updating the policy using the collected experience. 
Online RL algorithms often employ a model-free approach (MFRL), where the agent learns a direct mapping from observations to actions,
but this can require a lot of data to be collected from the environment.
%However, MFRL methods do not leverage any prior over the model of the world to increase their sample efficiency, potentially requiring a large amount of interaction data to exhibit reasonable performance.
Model-based RL (MBRL) aims to reduce the amount of data needed to train the policy
 by also learning a world model (WM), and using this WM to plan ``in imagination".
%Model-based RL (MBRL), on the other hand, learns a world model (WM) alongside the policy, which allows to reduce the amount of real world interaction data collected. 



To evaluate sample-efficient RL algorithms, it is common to use the
Atari-$100$k benchmark \citep{Kaiser2019}. However, the near-deterministic nature of Atari games allows agents to memorize action sequences without demonstrating true generalization \citep{Machado2018}.
In addition, although the benchmark encompasses a variety of  skills (memory, planning, etc), each individual game typically only emphasizes one or two such skills.
To promote the development of agents with broader capabilities, we focus on the Crafter domain \citep{hafner2021benchmarking},
a 2D version of Minecraft that challenges a single agent to master a diverse skill set.
Specifically, we use the  Craftax-classic environment \citep{matthews2024craftax},  a fast, near-replica of Crafter, implemented in JAX \citep{jax2018github}.
Key features of Craftax-classic include: % some of the details below do not apply to the non-classic craftax
(a) procedurally generated stochastic environments (at each episode the agent encounters a new environment sampled from a common distribution); 
(b) partial observability, as the agent only sees a $63 \times 63$ pixel image representing a local view of the agent's environment, plus a visualization of its inventory (see \cref{fig:teaser}[middle]);
and (c) an achievement hierarchy that defines a sparse reward signal, requiring deep and broad exploration.


\begin{figure}[t!]
    \centering
    \begin{tabular}{c}
    \includegraphics[width=.5\linewidth]{figures/benchmark.pdf}
    \end{tabular}
    \hspace{-.5em}
    \begin{tabular}{c}
        \includegraphics[width=.2\linewidth]{figures/craftax.pdf} 
        %\includegraphics[width=.15\linewidth]{figures/tokens.pdf}
    \end{tabular}
    \hspace{-1em}
    \begin{tabular}{c}
        %\includegraphics[width=.15\linewidth]{figures/craftax.pdf} \\
        \includegraphics[width=.21\linewidth]{figures/tokens.pdf}
    \end{tabular}
    \vspace{-.75em}
    \caption{
    [Left]
    Reward on Craftax-classic.
    Our best MBRL and MFRL agents outperform all the previously published MFRL and MBRL results, and for the first time, surpass the reward achieved by a human expert.
    We display published methods which report the reward at 1M steps with horizontal line from 900k to 1M steps. 
    [Middle] The Craftax-classic observation is a $63 \times 63$ pixel image, composed of $9\times9$ patches of $7\times7$ pixels. 
    The observation shows the map around the agent and the agent's health and inventory. Here we have rendered the image at $144 \times 144$ pixels for visibility. 
    [Right] $64$ different patches.
    }
    \label{fig:teaser}
\vspace{-1.em}
\end{figure}



In this paper, we study improvements to MBRL methods,
based on transformer world models (TWM),
in the context of the Craftax-classic environment.
\eat{
In particular, we address three main questions:
(1) What is the form of the world model (WM)?;
(2) How is the WM trained?;
(3) How is the WM used?.
This paper makes a contribution to each one of these questions.
}
We make contributions across
the following three axes:
(a) how the TWM is used (Section \ref{sec:dyna});
(b) the tokenization scheme used to create TWM inputs
(Section \ref{sec:nnt});
(c) and how the TWM is trained (Section \ref{sec:btf}).
Collectively, our improvements result in an agent that,
with only $1$M environment steps,
achieves a Craftax-classic reward of $67.42\%$ and a
score of $27.91\%$,
significantly improving over the previous state of the art (SOTA) reward of $53.20\%$ \citep{hafner2023mastering} and the previous SOTA score of $19.4\%$ \citep{Kauvar2023}\footnote{The score $S$ is given by the geometric mean
of the success rate $s_i$ for each of the
$N=22$ achievements;
this
 puts more weight on occasionally
solving many achievements than on consistently solving
a subset.
More precisely, the score is given by
$S = \exp \left(\frac{1}{N} \sum_{i=1}^N
\ln (1+s_i) \right)-1$,
where $s_i \in [0,100]$ is the success percentage
for achievement $i$
(i.e., fraction of episodes in which
the achievement was obtained at least once).
By contrast, the rewards are just the expected sum of rewards, or in percentage, the arithmetic mean
$R=\frac{1}{N} \sum_{i=1}^N s_i$
(ignoring minor contributions to
the reward based on
 the health of the agent).
 The score and reward are correlated, but are not the same. Unlike some prior work, we report both metrics to make comparisons easier.
}.


\eat{
Our first contribution is to the form 
of the world model.
Following prior work, we 
 use transformers
\citep{vaswani2017attention}
to create a Transformer World Model (TWM).
However, 
since our goal is training agents with visual input,
we need to address the issue of
where the tokens come from.
}

Our first contribution relates to the way the world model is used:
in contrast to recent MBRL methods like IRIS \citep{micheli2022transformers} and DreamerV3 \citep{hafner2023mastering}, which train the policy solely on imagined trajectories (generated by the world model), we train our policy using both imagined rollouts from the world model and real experiences collected in the environment.
This is similar to the original Dyna method \citep{sutton1990integrated}, although this technique has been abandoned in recent work.
In this hybrid regime, we can view the WM as a form
of generative data augmentation
\citep{van2019use}.

Our second contribution addresses the tokenizer which converts between images and tokens that the TWM ingests and outputs.
Most prior work uses a vector quantized variational autoencoder (VQ-VAE, \citealt{van2017neural}), e.g.
IRIS \citep{micheli2022transformers},
DART \citep{agarwal2024learning}.
\eat{
These methods train a CNN to map images $O_t$ to a 
feature map $Z_t$, whose elements $Z_{t}^i$ are then quantized into a set of discrete tokens
$Q_t=\{q_{t}^i\}_{i=1}^L$, where $q_{t}^{i} \in \{1,\ldots,K\}$
is the index of the codebook vector representing
$Z_{t}^i$,
and $i \in \{1,\ldots,L\}$ is the token index.
The tokens $Q_t$ are concatenated into a sequence $Q_{1:T}$ to represent the observations
which, along with the sequence of actions, is used to train the WM.
}
These methods train a CNN to process images into a 
feature map, whose elements are then quantized into discrete tokens, using a codebook. 
The sequence of observation tokens across timesteps is used, along with the actions and rewards, to train the WM.
We propose two improvements to the tokenizer.
First, instead of jointly quantizing the image, we split the image into patches and independently tokenize each patch.
% $p_t^i$, using a simple online greedy VQ method, described in \cref{sec:nnt}.
Second, we replace the VQ-VAE with a simpler nearest-neighbor tokenizer (NNT) for patches. 
% VQ-VAE suffers from a key drawback: the ``meaning" of the tokens changes over time as the CNN and codebook evolve during training.
% However, VQ-VAE generates a set of tokens whose ``meaning" changes over time (since the CNN features change, and the mapping from features to codebook entries change).
Unlike VQ-VAE, NNT ensures that the ``meaning" of each code in the codebook is constant through training, which simplifies the task of learning a reliable WM.

Our third contribution addresses the way the world model is trained.
TWMs are trained by maximizing the log likelihood of the sequence of tokens,
which is typically generated autoregressively both over time and within a timeslice.
We propose an alternative, 
which we call \btf ~(BTF),
that allows TWM to reason jointly about the possible future states of all tokens within a timestep, before sampling them in parallel and independently given the history.
% tokens from the previous timesteps.
With BTF, imagined rollouts for training the policy are both faster to sample and more accurate.
% mitigates autoregressive drift.


Our final contributions are some minor architectural changes to the MFRL baseline upon which our MBRL
approach is based. These changes are still significant, resulting in a simple MFRL method that is much faster than Dreamer V3 and yet obtains a much better average reward and score.

Our improvements are complementary to each other, and  can be combined into a  ``ladder of improvements"---similar to the ``Rainbow" paper's
\citep{Hessel2018} series of improvements on top of model-free DQN agents. 

%In particular, we start with the previous SOTA MFRL approach \citep{moon2024discovering}, which achieves a score of $15.60$ and a reward of $46.91\%$, and make some minor architectural modifications, described in \cref{sec:MFRL}, increasing the score to $16.77$ and the reward to $55.50\%$. This result is notable since it beats the considerably more complex (and much slower) Dreamer V3 method, which obtains a score of $14.5$ and a reward of $53.20\%$. It also significantly beats other model-based methods, such as IRIS \citep{micheli2022transformers} (reward of $25.0\%$) and $\Delta$-IRIS \citep{micheli2024efficient}(reward of $35.0\%$).\footnote{ This is consistent with some results on Atari-100k benchmark, which shows that well-tuned model-free methods, such as the  value-based  ``Bigger, Better, Faster" method \citep{Schwarzer2023}, can beat more sophisticated model-based methods.}

\eat{
It is also consistent
with the results in \citep{Jesson2024},
which  shows that minor architectural
changes to the policy,
trained with PPO without a WM,
can lead to big gains on the ProcGen benchmark.
} %



