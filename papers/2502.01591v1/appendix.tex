%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

%\section{Architecture and hyperparameters}
\section{Algorithmic details}
\label{sec:hyperparameters}

\subsection{Our Model-free RL agent}
\label{ap:mfrl_agent}

We first detail our new state-of-the-art MFRL agent. As mentioned in the main text, it relies on an actor-critic policy network trained with PPO.


\subsubsection{MFRL architecture}

We summarize our MFRL agent in Algorithm \ref{algo:ac_network} and further detail it below.
\begin{algorithm}[h!]
\caption{MFRL agent}
\label{algo:ac_network}
\begin{algorithmic}
\STATE {\bfseries Input:} Image $O_t$, last hidden state $h_{t-1}$, parameters $\Phi$. 
\STATE {\bfseries Output:} action $a_t$, value $v_t$, new hidden state $h_t$.
\STATE $z_t = \text{ImpalaCNN}_{\Phi}(O_t)$
\STATE $h_t, y_t = \text{RNN}_{\Phi}( [h_{t-1}, z_t])$
\STATE $a_t \sim \pi_{\Phi}([y_t, z_t])$
\STATE $v_t = V_{\Phi}([y_t, z_t])$
\end{algorithmic}
\end{algorithm}

\textbf{Imapala CNN architecture: }
Each Craftax-classic image $O_t$ of size $63\times63\times3$ goes through an Impala CNN \citep{espeholt2018impala}.
The CNN consists of three stacks with channel sizes of ($64, 64, 128$).  Each stack is composed of (a) an instance normalization \citep{ulyanov2016instance}, (b) a convolutional layer with kernel size $3\times3$ and stride of $1$, (c) a max pooling layer with kernel size $3\times3$ and stride of $2$, and (d) two ResNet blocks \citep{he2016deep}. Each ResNet block is composed of (a) a ReLU activation followed by an instance normalization, (b) a convolutional layer with kernel size $3\times3$ and stride of $1$. The CNN last layer output, of size $8 \times 8 \times 128$ passes through a ReLU activation, then gets flattened into an embedding vector of size $8192$, which we call $z_t$.

\paragraph{RNN architecture:} The CNN output $z_t$ (a) goes through a layer norm operator, (b) then gets linearly mapped to a $256$-dimensional vector, (c) then passes through a ReLU activation, resulting in the new input for the RNN. The RNN then updates its hidden state, and outputs a $256$-dimensional vector $y_t$, which goes through another ReLU activation.


\paragraph{Actor and critic architecture:} Finally, the CNN output $z_t$ and the RNN output $y_t$ are concatenated, resulting in the $8448$-dimensional embedding input shared by the actor and the critic networks. For the actor network, this shared input goes through (a) a layer normalization \citep{lei2016layer}, (b) a fully-connected network whose $2048$-dimensional output goes through a ReLU, (c) two dense residual blocks whose $2048$-dimensional output goes through a ReLU, (d) a last layer normalization and (e) a final fully-connected network which predicts the action logits.

Similarly, for the critic network, the shared input goes through (a) a layer normalization, (b) a fully-connected network whose $2048$-dimensional output goes through a ReLU, (c) two dense residual blocks whose $2048$-dimensional output goes through a ReLU, (d) a last layer normalization and (e) a final layer which predicts the value (which is a float).






\subsubsection{PPO training}
We train our MFRL agent with the PPO algorithm \citep{schulman2017proximal}. PPO is a policy gradient algorithm, which we briefly summarize below.

\paragraph{Training objective:}
We assume we are given a trajectory, $\tau=(O_{1:T+1}, a_{1:T}, r_{1:T}, \text{done}_{1:T}, h_{0:T})$ collected in the environment, where $\text{done}_t$ is a binary variable indicating whether the current state is a terminal state, and $h_t$ is the RNN hidden state collected while executing the policy. Algorithm \ref{algo:rollout} discusses how we collect such a trajectory.

Given the fixed current actor-critic parameters $\Phi_{\text{old}}$,
PPO first runs the actor-critic network on $\tau$, starting from the hidden state $h_0$ and returns two sequences of values $v_{1:T+1}=V_{\Phi_{\text{old}}}(O_{1:T+1})$ and probabilities $\pi_{\Phi_{\text{old}}}(a_t | O_t)$\footnote{We drop the ImpalaCNN and the RNN for simplicity.}. It then defines the generalized advantage estimation (GAE) as in \citet{schulman2015high}:
%\begin{equation*}
\begin{equation*}
A_t = \delta_t + (1 - \text{done}_t)  \gamma \lambda A_{t+1} = \delta_t + (1 - \text{done}_t)  \left(\gamma \lambda \delta_{t+1} + \ldots + (\gamma \lambda)^{T-t} \delta_{T}\right). ~~~~ \forall t \le T \\
\end{equation*}
where
$$
\delta_t = r_t + (1 - \text{done}_t)  \gamma v_{t+1} - v_t.
$$
PPO also defines the TD targets $q_t = A_t + v_t$. 


PPO optimizes the parameters $\Phi$, to minimize the objective value:
\begin{equation}\label{ppo}
\mathcal{L}_{\text{PPO}}(\Phi)
=
\frac{1}{T} \sum_{t=1}^T \left\{ -\min \left(r_t(\Phi) A_t, \text{clip}(r_t(\Phi)) A_t) \right)
+ \lambda_{\text{TD}} (V_{\Phi}(O_t) - q_t)^2
- \lambda_{\text{ent}} \mathcal{H}(\pi_{\Phi}(. | O_t)) \right\},
\end{equation}
where 
$\text{clip}(u)$ ensures that $u$ lies in the interval $[1-\epsilon, 1+\epsilon]$,
$r_t(\Phi)$ is the probability ratio $r_t(\Phi) = \frac{\pi_{\Phi}(a_t | O_t)}{ \pi_{\Phi{\text{old}}}(a_t | O_t) }$ and $\mathcal{H}$ is the entropy operator. 

%The first term in Equation \eqref{ppo} is the actor loss, the second term is the TD-loss, and the last one is the entropy loss.


\paragraph{Algorithm:}
Algorithm \ref{algo:ppo_update} details the PPO-update-policy, which is called in Steps 1 and 4 in our main Algorithm \ref{algo:MBRL} to update the PPO parameters on a batch of trajectories. PPO allows multiple epochs of minibatch updates on the same batch and introduces two hyperparameters: a number of minibatches $N^{\text{mb}}$ (which divides the number of environments $N_{\text{env}}$), and a number of epochs $N^{\text{epoch}}$.

\medskip
\begin{algorithm}[h!]
\small
\caption{PPO-update-policy}
\label{algo:ppo_update}
\begin{algorithmic}
\STATE {\bfseries Input:} Actor-critic model $(\pi, V)$ and parameters $\Phi$
\newline
Trajectories $\tau^{1:N_{\text{env}}}=(O^{1:N_{\text{env}}}_{1:T+1}, a^{1:N_{\text{env}}}_{1:T}, r^{1:N_{\text{env}}}_{1:T}, \text{done}^{1:N_{\text{env}}}_{1:T}, h^{1:N_{\text{env}}}_{0:T})$
\newline
Number of epochs $N^{\text{epoch}}$ and of minibatches $N^{\text{mb}}$
\newline
PPO objective value parameters $\gamma, \lambda, \epsilon$
\newline
Learning rate lr and max-gradient-norm
\newline
Moving average mean $\mu_{\text{target}}$, standard deviation $\sigma_{\text{target}}$ and discount factor $\alpha$
\medskip
\STATE {\bfseries Output:} Updated actor-critic parameters $\Phi$
\medskip
\STATE {\bfseries Initialize:} Define $\Phi_{\text{old}} = \Phi$ 
\STATE Compute the values $v^{1:N_{\text{env}}}_{1:T+1} = V_{\Phi_{\text{old}}}(O^{1:N_{\text{env}}}_{1:T+1})$
\STATE Compute PPO GAEs and targets $A^{1:N_{\text{env}}}_{1:T}, q^{1:N_{\text{env}}}_{1:T} = \text{GAE}(r^{1:N_{\text{env}}}_{1:T}, v^{1:N_{\text{env}}}_{1:T+1}, \gamma, \lambda)$
\STATE Standardize PPO GAEs $A^{1:N_{\text{env}}}_{1:T} = \frac{A^{1:N_{\text{env}}}_{1:T} - \text{mean}(A^{1:N_{\text{env}}}_{1:T})}{\text{std}(A^{1:N_{\text{env}}}_{1:T})}$
\medskip
\FOR{$\text{ep}=1$ {\bfseries to} $N^{\text{epoch}}$}
\FOR{$k=1$ {\bfseries to} $N^{\text{mb}}$}
\STATE $N^{\text{start}} = (k-1) N^{\text{mb}}+1, ~~ N^{\text{end}} = k N^{\text{mb}}+1$
\STATE \textit{// Standardize PPO target}
\STATE Update $\mu_{\text{target}} = \alpha \mu_{\text{target}} + (1-\alpha) \text{mean}(q^{N^{\text{start}}:N^{\text{end}}}_{1:T})$
\STATE Update $\sigma_{\text{target}} = \alpha \sigma_{\text{target}} + (1-\alpha) \text{std}(q^{N^{\text{start}}:N^{\text{end}}}_{1:T})$
\STATE Standardize $q^{N^{\text{start}}:N^{\text{end}}}_{1:T} = (q^{N^{\text{start}}:N^{\text{end}}}_{1:T} - \mu_{\text{target}}) / \sigma_{\text{target}}$

\medskip
\STATE \textit{// Run the actor-critic network}
\STATE Define $\tilde{h}^{N^{\text{start}}:N^{\text{end}}}_0= h^{N^{\text{start}}:N^{\text{end}}}_0$
\FOR{$t=1$ {\bfseries to} $T+1$}
\STATE $z^n_t = \text{ImpalaCNN}_{\Phi}(O^n_{t}) ~~~; ~~~\tilde{h}^n_t = \text{RNN}_{\Phi}( [\tilde{h}^n_{t-1}, z^n_t])$ 
~~~~~~~~~~~~~~~~~~~~~ for $n=N^{\text{start}}:N^{\text{end}}$
\STATE Compute $V^n_{\Phi}([y^n_t, z^n_t])$ and $\pi^n_{\Phi}([y^n_t, z^n_t])$ 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ for $n=N^{\text{start}}:N^{\text{end}}$
\ENDFOR

\medskip
\STATE \textit{// Take a gradient step}
\STATE Compute $\mathcal{L}^n_{\text{PPO}}(\Phi)$ using Equation \eqref{ppo}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ for $n=N^{\text{start}}:N^{\text{end}}$
\STATE Define the minibatch loss $\mathcal{L}_{\text{PPO}}(\Phi) = \frac{1}{N^{\text{mb}}} \sum_{n=N^{\text{start}}}^{N^{\text{end}}} \mathcal{L}^n_{\text{PPO}}(\Phi) $
\STATE Update $\Phi = \text{Adam}
\left(\Phi,
~~ \text{clip-gradient}(\nabla_{\Phi} \mathcal{L}_{\text{PPO}}(\Phi), \text{max-norm)}, ~~\text{lr} \right)$
\ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}

We make a few comments below:

$\bullet$ We use gradient clipping on each minibatch to control the maximum gradient norm, and update the actor-critic parameters using Adam \citep{kingma2014adam} with learning rate of $0.00045$. 

$\bullet$ During each epoch and minibatch update, we initialize the hidden state $\tilde{h}_0$ from its value $h_0$ stored while collecting the trajectory $\tau$.

$\bullet$ In Algorithm \ref{algo:ppo_update}, we introduce two changes to the standard PPO objective, described in Equation \eqref{ppo}.
First, we standardize the GAEs
(ensure they are zero mean and unit variance)
across the batches. Second, similar to \citet{moon2024discovering}, we maintain a moving average with discount factor $\alpha$ for the mean
and standard deviation of the target $q_t$ and we update
the value network to predict the standardized targets.


\paragraph{Implementation: }
Note that for implementing PPO, we start from the code available in the \texttt{purejaxrl} library \citep{lu2022discovered} at \url{https://github.com/luchris429/purejaxrl/blob/main/purejaxrl/ppo.py}.

\subsubsection{Hyperparameters}
Table \ref{table:mfrl_hyperparameters} displays the PPO hyperparameters used for training our SOTA MFRL agent.

\begin{table*}[h!]
\small
\centering
\caption{MFRL hyperpameters.}
\label{table:mfrl_hyperparameters}
\begin{tabular}{p{0.2\textwidth} p{0.35\textwidth}p{0.3\textwidth} }
    \toprule
    Module & Hyperparameter & Value\\
    \toprule
    \toprule
    Environment & Number of environments $N_{\text{env}}$ & $48$ \\
    & Rollout horizon in environment $T_{\text{env}}$ & $96$ \\
    \midrule
    Sizes & Image size & $63\times63\times3$ \\
    %& CNN activation & Instance normalization \\
    & CNN output size & $8\times8\times128$ \\
    & RNN hidden layer size & $256$ \\
    & AC input size & $8448$ \\
    & AC layer size & $2048$ \\
    %& AC normalization & Layer normalization
    \midrule
    PPO & $\gamma$ & $0.925$ \\
    & $\lambda$ & $0.625$ \\
    & $\epsilon$ clipping & $0.2$ \\
    & TD-loss coefficient $\lambda_{\text{TD}}$ & $2.0$ \\
    & Entropy loss coefficient $\lambda_{\text{ent}}$ & $0.01$ \\
    & PPO target discount factor $\alpha$ & $0.95$ \\
    \midrule
    Learning 
    & Optimizer & Adam \citep{kingma2014adam}\\
    & Learning rate & $0.00045$ \\
    & Max. gradient norm & $0.5$ \\
    & Learning rate annealing (MFRL) & True (linear schedule) \\
    & Number of minibatches (MFRL) & $8$ \\
    & Number of epochs (MFRL) & $4$ \\
\bottomrule
\end{tabular}
\end{table*}

\textbf{MBRL experiments.}
We make two additional changes to PPO in the MBRL setting, and keep all the other hyperparameters fixed.
First,
we do not use learning rate annealing for MBRL,
 while MFRL uses learning rate annealing (with a linear schedule).
 Second, as we discuss in Section \ref{sec:minibatch_epoch}, the differences between the PPO updates on real and imaginary trajectories lead to varying the number of minibatches and epochs.


\paragraph{Craftax experiments.} We also keep all but two of our PPO hyperparameters fixed for Craftax (full), which we discuss in Appendix \ref{sec:classic_vs_full}.


\newpage
\subsection{Model-based modules}
\label{ap:mbrl_modules}

In this section, we detail the two key modules for model-based RL: the tokenizer and the transformer world model. 


\subsubsection{Tokenizer}\label{sec:appendix_ae}

\paragraph{Training objective: }
Given a Craftax-classic image $O_t$ and a codebook $\mathcal{C} = \{e_1, \ldots, e_K\}$, an encoder $\mathcal{E}$ returns a feature map $Z_t = (Z_t^1, \ldots, Z^t_L)$. Each feature $Z_t^{\ell}$ gets quantized, resulting into $L$ tokens $Q_t = (q_t^1,\ldots, q_t^L)$---which serves as input to the TWM---then projected back to $\hat{Z}_t = (e_{q_t^1}, \ldots e_{q_t^L})$. Finally, a decoder $\mathcal{D}$ decodes $\hat{Z}_t$ back to the image space: $\hat{O}_t = \mathcal{D}(\hat{Z}_t)$. Following \citet{van2017neural,micheli2022transformers}, we define the VQ-VAE loss as:
% a weighted sum of L1 and L2 reconstruction losses and a commitment loss:
\begin{equation}\label{vqvae}
\mathcal{L}_{\text{VQ-VAE}}(\mathcal{E}, \mathcal{D}, \mathcal{C}) =
\lambda_1 \|O_t - \hat{O}_t \|_1 
+ \lambda_2 \|O_t - \hat{O}_t \|_2^2 
+ \lambda_3 \| \text{sg}(Z_t) - \hat{Z}_t \|_2^2
+ \lambda_4 \| Z_t - \text{sg}(\hat{Z}_t) \|_2^2,
~.
\end{equation}
where $\text{sg}$ is the stop-gradient operator. 
The first two terms are the reconstruction loss, the third term is the codebook loss and the last term is a commitment loss.
% The codebook is updated using the exponential moving average, as proposed in \citet{van2017neural,razavi2019generating}.


\medskip
We now discuss the different VQ and VQ-VAE architectures used by the models M1-5 in the ladder described in Section \ref{sec:ladder}. 


\paragraph{Default VQ-VAE:} Our baseline model M1, and our next model M2 build on IRIS VQ-VAE \citep{micheli2022transformers} and follow the authors' code: \url{https://github.com/eloialonso/iris/blob/main/src/models/tokenizer/nets.py}. The encoder uses a convolutional layer (with kernel size $3\times3$ and stride $1$), then five residual blocks with two convolutional layers each (with kernel size $3\times3$, stride $1$ and ReLU activation). The channel sizes of the residual blocks are $(64, 64, 128, 128, 256)$. A downsampling is applied on the first, third and fourth blocks. Finally, a last convolutional layer with $128$ channels returns an output of size $8 \times 8 \times 128$.
The decoder follows the reverse architecture. Each of the $L=64$ latent embeddings gets quantized individually, using a codebook of size $K=512$, to minimize Equation \eqref{vqvae}. 
We use codebook normalization, meaning that each code in the codebook $\mathcal{C}$ has unit L$2$ norm. Similarly, each latent embedding $Z_t^{\ell}l$ gets normalized before being quantized. As in IRIS, we use $\lambda_1=1, \lambda_2=0, \lambda_3=1, \lambda_4=0.25$. 
We train with Adam and a learning rate of $0.001$.

\paragraph{VQ-VAE(patches):} For the next model M3, the encoder is a two-layers MLP that maps each flattened $7\times 7 \times 3$ patch to a $128$-dimensional vector, using a ReLU activation. Similarly, the decoder learns a linear mapping from the embedding vector back to the flattened patches. Each embedding gets quantized individually, using a codebook of size $K=512$, and codebook normalization, to minimize Equation \eqref{vqvae}. Following \citet{micheli2024efficient}, we use $\lambda_1=0.1, \lambda_2=1, \lambda_3=1, \lambda_4=0.02$.


\paragraph{Nearest neighbor tokenizer:} NNT does not use Equation \eqref{vqvae} and directly adds image patches to a codebook of size $K=4096$, using a Euclidean threshold $\tau=0.75$.



\subsubsection{Transformer world model}\label{sec:appendix_twm}

\paragraph{Training objective:}
We train the TWM on real trajectories (from the environment) of $T_{\text{WM}}=20$ timesteps sampled from the replay buffer (see Algorithm \ref{algo:MBRL}). 
We set $T_{\text{WM}}=20$ as it is the largest value that will fit into memory on 8 H100 GPUs.


Given a trajectory $\tau=(O_{1:T+1}, a_{1:T}, r_{1:T}, \text{done}_{1:T})$, the input to the transformer is the sequence of tokens:
$$
(q_1^1,\ldots,q_1^L,a_1, \ldots q_T^T,\ldots,q_T^L, a_T),
$$
where $\text{enc}(O_t)=(q_t^1,\ldots,q_t^L)$ and $q_t^i \in \{1,\ldots,K\}$ where $K$ is the codebook size. 
These tokens are then embedded using an observation embedding table and an action embedding table. 
After several self-attention layers (using the standard causal mask or the block causal mask introduced in Section \ref{sec:btf}), the TWM returns a sequence of output embeddings:
$$
(E(q_1^1),\ldots,E(q_1^L),E(a_1), \ldots E(q_1^T),\ldots,E(q_1^T)).
$$
The TWM then output embeddings are then used to decode the following predictions:

\smallskip
$\textbf{(1)}$ Following \citep{micheli2022transformers}, $E(a_t)$ passes through a reward head and predicts the logits of the reward $r_t$. 


\smallskip
$\textbf{(2)}$ $E(a_t)$ also passes through a termination head and predicts the logits of the termination state $\text{done}_t$. 

\smallskip
$\textbf{(3)}$ Without block teacher forcing, $E(q_t^i)$ passes through an observation head and predicts the logits of the next codebook index at the same timestep $E(q_t^{i+1})$, when $t \le L-1$. Similarly $E(a_t)$ passes through an observation head and predicts the logits of the first codebook index at the next timestep $E(q_{t+1}^{1})$.

\smallskip
$\textbf{(3')}$ With block teacher forcing, $E(q_t^i)$ passes through an observation head and predicts the logits of the same codebook index at the next timestep $E(q_{t+1}^i)$.


\medskip

TWM is then trained with three losses:

\smallskip
$\textbf{(1)}$
The first loss is the cross-entropy for the reward prediction. Note that Craftax-classic provides a (sparse) reward of 1 for the first time each achievement is``unlocked'' in each episode.
In addition, it gives a smaller (in magnitude) but denser reward, penalizing the agent by $0.1$ for every
point of damage taken, and rewarding it by $0.1$ for every point recovered. 
However, we found that
we got better results by ignoring the points damaged and recovered,
and using a binary reward target.
This is similar to the recommendations in 
\citet{Farebrother2024}, where the authors show that value-based RL methods work better when replacing MSE loss for value functions with cross-entropy on a quantized version of the return.




\smallskip
$\textbf{(2)}$
The second loss is the cross-entropy for the termination predictions. 

\smallskip
$\textbf{(3}$ 
The third loss is the cross-entropy for the codebook predictions, where the predicted codes vary between $1$ and the codebook size $K$.


\bigskip

\paragraph{Architecture:} We use the standard GPT2 architecture \citep{radford2019language}. We use a sequence length $T_{\text{WM}}=20$ due to memory constraints.
We implement key-value caching to generate rollouts fast. Table \ref{table:twm_hyperparameters} details the different hyperparameters.
\begin{table*}[h!]
\small
\centering
\caption{Hyperparameters for the transformer world model.}
\label{table:twm_hyperparameters}
\begin{tabular}{p{0.2\textwidth}p{0.35\textwidth}p{0.2\textwidth} }
\toprule
Module & Hyperparameter & Value\\
\toprule
\toprule
Environment & Sequence length $T_{\text{WM}}$ & $20$ \\
\midrule
Architecture  & Embedding dimension & $128$ \\
& Number of layers & $8$ \\
& Number of heads & $3$ \\
& Mask & Causal or Block causal\\
& Inference with key-value caching & True \\
& Positional embedding & RoPE \citep{su2024roformer} \\
\midrule
Learning & Embedding dropout & $0.1$ \\
& Attention dropout & $0.1$ \\
&  Residual dropout & $0.1$ \\
& Optimizer & Adam \citep{kingma2014adam} \\
& Learning rate & $0.001$ \\
& Max. gradient norm & $0.5$ \\
\bottomrule
\end{tabular}
\end{table*}

% Note that train our transformer on sequence of length $T_{\text{WM}}$, and use a context of length $T_{\text{WM}}$ in the rollouts. In theory, our transformer can generate longer rollouts by shifting the KV-cache once we hit the context length. To do so, a requirement is to use 



\newpage
\subsection{Our Model-based RL agent}
\label{ap:mbrl_agent}



In this section, we detail how we combine the different modules above to build our SOTA MBRL agent, which is described in Algorithm \ref{algo:MBRL} in the main text.


\subsubsection{Collecting environment rollout or TWM rollout}
Algorithm \ref{algo:rollout} presents the rollout method, which we call in Steps 1 and 4 of Algorithm \ref{algo:MBRL}. It requires a transition function which can either be the environment or the TWM.

\begin{algorithm}[h!]
\small
\caption{~Environment rollout or TWM rollout}
\label{algo:rollout}
\begin{algorithmic}
\STATE {\bfseries Input:} Initial observation $O_1$,
\newline
Previous $M$ observations $O_{\text{past}} = (O_{-M+1}, \ldots, O_0)$ if available else $O_{\text{past}} = \emptyset$,
\newline
AC model $\pi$ and parameters $\Phi$,
\newline
Rollout horizon $T$,
\newline
An environment transition $\Mtrue$ or a TWM $\mathcal{M}$ with parameters $\Theta$.
\medskip
\STATE {\bfseries Output:} A trajectory $\tau=(O_{1:T+1}, a_{1:T}, r_{1:T}, \text{done}_{1:T}, h_{0:T})$

\medskip
\STATE {\bfseries Initialize:} hidden state $h_0 =0$ if $O_{\text{past}} = \emptyset$ else set $h_{-M} =0$
\IF{$O_{\text{past}} \ne \emptyset$}
\STATE \textit{// Burn-in the hidden state}
\FOR{$m=1$ {\bfseries to} $M$}
\STATE $z_{-M + m} = \text{ImpalaCNN}_{\Phi}(O_{-M + m})$
\STATE $h_{-M + m} = \text{RNN}_{\Phi}( [h_{-M -1 + m}, z_{-M + m}])$
\ENDFOR
\ENDIF

\medskip
\STATE {\bfseries Initialize:} $\tau = (h_0)$

\medskip
\FOR{$t=1$ {\bfseries to} $T$}
\STATE \textit{// Run the actor network}
\STATE $z_t = \text{ImpalaCNN}_{\Phi}(O_t)$
\STATE $h_t = \text{RNN}_{\Phi}( [h_{t-1}, z_t])$
\STATE $a_t \sim \pi_{\Phi}([h_t, z_t])$
\medskip
\STATE \textit{// Collect reward and next observation}
\IF{~environment rollout~}
\STATE $O_{t+1}, r_t, \text{done}_t \sim \Mtrue(O_t, a_t)$
\ELSIF{~TWM rollout~}
\STATE $Q_t = (q_t^1, \ldots, q_t^L) = \text{enc}(O_t)$
\STATE $Q_{t+1} \sim p_{\Theta}(Q_{t+1} | Q_{1:t}, a_{1:t})$
\STATE $O_{t+1} = \text{dec}(Q_{t+1})$
\STATE $r_t \sim p_{\Theta}(r_t | Q_{1:t}, a_{1:t})$
\STATE $\text{done}_t \sim p_{\Theta}(\text{done}_t | Q_{1:t}, a_{1:t})$
\ENDIF
\STATE $\tau += (O_t, a_t, r_t, \text{done}_t, h_t)$
\ENDFOR
\STATE $\tau += (O_{T+1})$
\end{algorithmic}
\end{algorithm}

Below we discuss various components of Algorithm \ref{algo:rollout}.

\paragraph{Parallelism.}
Note that in Algorithm \ref{algo:MBRL}, we call Algorithm \ref{algo:rollout} in parallel starting from $N_{\text{env}}$ observations $O^{1:N_{\text{env}}}_1$.


\paragraph{Burn-in.} The first time we collect data in the environment, we initialize the hidden state to zeros. 
The next time, we use burn-in to refresh the hidden state before rolling out the policy \citep{kapturowski2018recurrent}. 
We do so by passing the $M$ observations prior to $O_1$ to the policy, which updates the hidden state of the policy using the latest parameters.
(To use burn-in TWM rollout, we sample a trajectory of length $M+1$ in Step 4 of Algorithm \ref{algo:MBRL}.)
To enable burn-in, when collecting data, in Step 1 of Algorithm \ref{algo:MBRL}, we must also store the last $M$ environment observations $(O_{-M+1}, \ldots, O_0)$ prior to $O_1$.


\paragraph{TWM sampling.} As explained in the main text, sampling from the distribution $Q_{t+1} \sim p_{\Theta}(Q_{t+1} | Q_{1:t}, a_{1:t})$ is different when using (or not) block teacher forcing. For the former, the tokens of the next timestep $(q_{t+1}^1, \ldots, q_{t+1}^L)$ are sampled in parallel, while for the latter, they are sampled autoregressively.


\paragraph{Maximum buffer size.} To avoid running out of memory, we use a maximum buffer size and restrict the data buffer $\mathcal{D}$ in Algorithm \ref{algo:MBRL} to contain at most the last $128\text{k}$ observations. When the buffer is at capacity, we remove the oldest observations before adding the new ones. We use flashbax \citep{flashbax} to implement our replay buffer in JAX.




\subsubsection{World model update}

In practice, we decompose the world model updates into two steps. First, we update the tokenizer $N^{\text{iters}}_{\text{tok}}$ times. Second, we update the TWM $N^{\text{iters}}_{\text{TWM}}$ times. For both updates, we use $N_{\text{WM}}^{\text{mb training}}=3$ minibatches.
That is, Step 3 of Algorithm \ref{algo:MBRL} is implemented as in \cref{algo:step3}.


\begin{algorithm}[h!]
\small
\caption{Step 3 of Algorithm \ref{algo:MBRL}}
\begin{algorithmic}
\FOR{$\text{it}=1$ {\bfseries to} $N^{\text{iters}}_{\text{tok}}$}
\FOR{$k=1$ {\bfseries to} $N_{\text{WM}}^{\text{mb training}}$}
\STATE $N^{\text{start}} = (k-1) N_{\text{WM}}^{\text{mb training}}+1, ~~ N^{\text{end}} = k N_{\text{WM}}^{\text{mb training}}+1$
\STATE  $\tau_{\text{replay}} ^{n} =\text{sample-trajectory}(\mathcal{D}, T_{\text{WM}}), ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ n=1:N_{\text{env}}$ \\
\STATE  $\Theta = \text{update-tokenizer}(\Theta, \tau_{\text{replay}} ^{N^{\text{start}}:N^{\text{end}}})$ with Equation \eqref{vqvae}\\
\ENDFOR
\ENDFOR
\FOR{$\text{it}=1$ {\bfseries to} $N^{\text{iters}}_{\text{TWM}}$}
\FOR{$k=1$ {\bfseries to} $N_{\text{WM}}^{\text{mb training}}$}
\STATE $N^{\text{start}} = (k-1) N_{\text{WM}}^{\text{mb training}}+1, ~~ N^{\text{end}} = k N_{\text{WM}}^{\text{mb training}}+1$
\STATE  $\tau_{\text{replay}} ^{n} =\text{sample-trajectory}(\mathcal{D}, T_{\text{WM}}), ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ n=1:N_{\text{env}}$ \\
\STATE  $\Theta = \text{update-TWM}(\Theta, \tau_{\text{replay}} ^{N^{\text{start}}:N^{\text{end}}} )$ following Appendix \ref{sec:appendix_twm}\\
\ENDFOR
\ENDFOR
\end{algorithmic}
\label{algo:step3}
\end{algorithm}

We always set $N^{\text{iters}}_{\text{TWM}}=500$ to perform a large number of gradient updates. For M1-3, we set $N^{\text{iters}}_{\text{tok}}=500$ as well, but for M5 we reduce it to $N^{\text{iters}}_{\text{tok}}=25$ for the sake of speed---since NNT only adds new patches to the codebook.




\subsubsection{PPO policy update}
\label{sec:minibatch_epoch}

Finally, the PPO-policy-update procedure called in Steps 1 and 4 of Algorithm \ref{algo:MBRL} follows Algorithm \ref{algo:ppo_update}. 

When using PPO for MBRL, we found it critical to use different numbers of minibatches and different numbers of epochs on the trajectories collected on the environment and with TWM. 

In particular, as the trajectories collected in imagination are longer, we  reduce the number of parallel environments,
and use
$N_{\text{env}}^{\text{mb}}=8$ and $N_{\text{WM}}^{\text{mb}}=1$.
This guarantees that the PPO updates are on batches of comparable sizes---$6 \times 96$ for real trajectories, and $48 \times 20$ for imaginary trajectories.

In addition, while the environment trajectories are limited, we can simply rollout our TWM to collect more imaginary trajectories. Consequently, we set $N_{\text{env}}^{\text{epoch}}=4$, and $N_{\text{WM}}^{\text{epoch}}=1$. 

Finally, we do not use learning rate annealing for MBRL training.


% \newpage
\subsubsection{Hyperparameters}
Table \ref{table:mbfrl_high_level} summarizes the main parameters used in our MBRL training pipeline.
\begin{table*}[h!]
\small
\centering
\caption{MBRL main parameters.}
\label{table:mbfrl_high_level}
\begin{tabular}{p{0.45\textwidth}p{0.1\textwidth} }
\toprule
Hyperparameter & Value\\
\toprule
\toprule
Number of environments $N_{\text{env}}$ & $48$ \\
Rollout horizon in environment $T_{\text{env}}$ & $96$ \\
Rollout horizon for TWM $T_{\text{WM}}$ & $20$ \\
Burn-in horizon $M$ & $5$ \\
Buffer size & $128,000$ \\
Number of tokenizer updates $N^{\text{iters}}_{\text{tok}}$ (with VQ-VAE)  & $500$ \\
Number of tokenizer updates $N^{\text{iters}}_{\text{tok}}$ (with NNT) & $25$ \\
Number of TWM updates $N^{\text{iters}}_{\text{TWM}}$ & $500$ \\
Number of minibatches for TWM training $N_{\text{WM}}^{\text{mb training}}$ & $3$ \\
Background planning starting step $T_{\text{BP}}$ & $200\text{k}$ \\
Number of policy updates $N^{\text{iters}}_{\text{AC}}$ & $150$ \\
Number of PPO minibatches in environment $N_{\text{env}}^{\text{mb}}$ & $8$ \\
Number of PPO minibatches in imagination $N_{\text{WM}}^{\text{mb}}$ & $1$ \\
Number of epochs in environment $N_{\text{env}}^{\text{epoch}}$ & $4$ \\
Number of epochs in imagination $N_{\text{WM}}^{\text{epoch}}$ & $1$ \\
Learning rate annealing  & False \\
\bottomrule
\end{tabular}
\end{table*}





\newpage
\section{Comparing scores}

\label{sec:appendix_score}


Figure \ref{fig:score} completes the two main Figures \ref{fig:teaser}[left] and \ref{fig:main} by reporting the scores the different agents. Specifically, Figure \ref{fig:score}[left] compares our best MBRL and MFRL agents to the best previously published MBRL and MFRL agents. Figure \ref{fig:score}[right] displays the scores for the different agents on our ladder of improvements.

\bigskip

\begin{figure*}[h!]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/benchmark_score.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/ladder_score.pdf}
    \end{subfigure}
    \caption{[Left] In addition to reaching higher rewards, our best MBRL and MFRL agents also achieve higher scores compared to the best previously published MBRL and MFRL results. [Right] MBRL agents' scores increase as they climb up the ladder of improvements.
    }
    \label{fig:score}
\vspace{-.75em}
\end{figure*}




\newpage
\section{Additional world model comparisons}
\label{ap:symbol_accuracy}

This section complements Section \ref{sec:stationary} and presents two additional results to compare the different world models.

\begin{figure*}[h!]
    \centering
    \begin{tabular}{c}
        \includegraphics[width=0.4\textwidth]{figures/wm_rollout_tok_rec.pdf}
    \end{tabular}
    \begin{tabular}{c}
        \includegraphics[width=0.4\textwidth]{figures/wm_rollout_norm_obs_rec_by_step.pdf}
    \end{tabular}
    \vspace{-.75em}
    \caption{TWM performance.[Left] Tokenizer L$2$ reconstruction error, averaged over rollouts. Lower is better. By construction, our best MBRL agent, which uses NNT, constantly reaches the lowest error, as NNT directly adds observation patches to its codebook. 
    [Right] TWM rollouts L$2$ observation reconstruction error, averaged over rollouts. Lower is better. M3 and M5, which both use patch factorization, achieve the lowest errors.
    }
    %\MLG{For M3[left]: why does reconstruction error increase with more steps?} 
    \label{fig:wm_quality_appendix}
\vspace{-.75em}
\end{figure*}

\subsection{Tokenizer reconstruction error}
We first use the evaluation dataset $\mathcal{D}_{\text{eval}}$ (introduced in Section \ref{sec:stationary}) to compare the tokenizer reconstruction error of our world models M1, M3, and M5---using the checkpoints at $1$M steps.
To do so, we independently encode and decode each observation $O^n_t \in \mathcal{D}_{\text{eval}}$, to obtain a tokenizer reconstruction $\hat{O}_t^{\text{tok},~n}$. Figure \ref{fig:wm_quality_appendix}[left] compares the average L$2$ reconstruction errors over the evaluation dataset:
$$
\frac{1}{(T + 1) N_{\text{eval}}} \sum_{n=1}^{N_{\text{eval}}} \sum_{t=1}^{T_{\text{eval}} +1} \| \hat{O}^{\text{tok},~n}_t - O^n_t \|_2^2,
$$
showing that all three models achieve low $L$2 reconstruction error. 
However our best model M5, which uses NNT, reaches a very low reconstruction error from the first iterations, since it directly adds image patches to its codebook rather than learning the codes online.


\subsection{Rollout reconstruction error}
Second, given a sequence of observations in a TWM rollout
$\hat{O}^{\text{TWM},~n}_{1:T_{\text{eval}} + 1}$, and the corresponding sequence of observations in the environment $O^{n}_{1:T_{\text{eval}} + 1}$ (which both have executed the same sequence of actions),
Figure \ref{fig:wm_quality_appendix}[right] compares the observation L$2$ reconstruction errors at each timestep $t$ (averaged over the evaluation dataset):
$$
\mathcal{E}_t = 
\frac{1}{N_{\text{eval}}} \sum_{n=1}^{N_{\text{eval}}} \| \hat{O}^{\text{TWM},~n}_t - O^n_t \|_2^2,
~~~\forall t.
$$
As expected, the errors increase with $t$ as mistakes compound over the rollout.
Our best method and M3, which both uses patch factorization, achieve the lowest reconstruction errors.

\eat{
\subsection{Symbol accuracy}
In this section, we leverage an appealing property of Craftax-classic: each observation $O_t$ comes with an array of ground truth symbols $S_t=(S_t^{1:R})$, with $R=145$. Given $100\text{k}$ pairs $(O_t, S_t)$, we learn a CNN $f_{\mu}$, to predict the symbols from the observation.
Next, we use $f_{\mu}$ to predict the symbols from the predicted rollout observation $\hat{O}^n_{1:T_{\text{eval}}+1}$. As above, we define the symbol accuracy at timestep $t$:
\begin{equation*}
    \mathcal{A}_t = \frac{1}{R . N_{\text{eval}}} \sum_{n=1}^{N_{\text{eval}}} \sum_{r=1}^{R}\mathbf{1}(f_{\mu}^r(\hat{O}^n_t), S^{r,n}_t), ~ \forall t
\end{equation*}
where $\mathbf{1}(x,y) = 1 ~\text{iff.}~ x=y $ (and $0$ o.w.), $S^{r,n}_t$ denotes the ground truth $r$th symbol from the array $S^{n}_t$ associated with $O^n_t$, and $f_{\mu}^r(\hat{O}^n_t)$ its prediction for the rollout $\hat{O}^n_t$.
Figure \ref{fig:wm_quality_appendix}[right] shows that, as expected, symbol accuracies decrease as the rollout horizon increases. It further highlights that our best model M5 is better able to capture the game dynamics.
}


\subsection{Symbol extractor architecture}
Herein, we discuss the symbol extractor architecture introduced in Section \ref{sec:stationary}. $f_{\mu}$ consists of (a) a first convolution layer with kernel size $7 \times 7$, stride of $7$, and channel size $128$, which extracts a feature for each patch, (b) a ReLU activation, (c) a second convolution layer with kernel size $1 \times 1$, a stride of $1$, and a channel size $128$, (d) a second ReLU activation, (e) a final linear layer which transforms the $3$D convolutional output into a $2$D array of logits of size $145 * 17=1345$---where $R=145$ is the number of ground truth symbols associated with each image of Craftax-classic and each symbol $S_t^r\in \{1, \ldots, 17\}$. The symbol extractor is trained with a cross-entropy loss between the predicted symbol logits and their ground truth values $S_t$, and achieves a $99.0\%$ validation accuracy.

\medskip



% \newpage
\subsection{Rollout comparison}
\label{ap:rollout_comparison}

In Figure \ref{fig:more_rollouts}, we show an additional rollout that exhibits similar properties to those in Figure \ref{fig:rollout_eval}[right].
M1 and M3 make more simple mistakes in the map layout. 
All models generate predictions that can be inconsistent with the game dynamics. 
However the errors by M1 and M3 are more severe, as M5's mistake relates to the preconditions of the make sword action.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{figures/more_rollouts.pdf}
    \caption{Additional rollout comparison for world models M1, M3 and M5. Best viewed zoomed in.
    \textbf{Map.}
    All models exhibit some map inconsistencies. M1 can make simple mistakes after the agent moves. Both M3 and M5 have map inconsistencies after the sleep actions, however the mistakes for M3 are far more severe.
    \textbf{Feasible hallucinations.}
    All models make feasible hallucinations when the agent exposes a new map region. 
    The sleep action is stochastic, and only sometimes results in the agent sleeping after taking the action. As a result, M3 and M5 make reasonable generations in predicting that the agent does not sleep in the final frame.
    \textbf{Infeasible hallucinations.}
    M1 generates cells that do not respect the game dynamics, such as spawning a plant without taking the place plant action, and creating a block type that cannot exist in that location.
    M3 turns the agent to face downwards without the down action. 
    M5 makes the wood sword despite the precondition of having wood inventory not being satisfied. 
    }
    \label{fig:more_rollouts}
\end{figure}

%\todo{Accuracy of the reward prediction.}




\newpage
\section{Comparing Craftax-classic and Craftax (full)}
\label{sec:classic_vs_full}
This section complements Section \ref{sec:craftax_full} and discusses the main differences between Craftax-classic and Craftax.
The first and second block Table \ref{tab:classic_vs_full} compares both environments. Note that we only use the first five parameters in our experiments in Section \ref{sec:craftax_full}.
The third and fourth blocks report the parameters used by our best MFRL and MBRL agents. In Craftax (full), for MFRL, we use $N_{\text{env}}=64$ environments and a rollout length $T_{\text{env}}=64$. 
Our SOTA MBRL agent uses $T_{\text{env}}=96$ 
and $T_{\text{WM}}=20$ as in Craftax-classic, but reduces the number of environments to $N_{\text{env}}=16$ to fit in GPU.
All the others PPO parameters are the same as in Table \ref{table:mfrl_hyperparameters}.

\medskip

\begin{table*}[h!]
\small
    \centering
    \caption{Environment Craftax-classic vs Craftax (full).}
    \label{tab:classic_vs_full}
    \vspace{-.5em}
    \begin{tabular}{p{0.2\textwidth} p{0.3\textwidth}p{0.15\textwidth}p{0.15\textwidth}}
    \toprule
    Module & & Classic & Full \\ 
    \toprule
    \toprule
    Environment (used)  & Image size & $63 \times 63$ & $130 \times 110$ \\
    & Patch size & $7 \times 7$ & $10 \times 10$ \\
    & Grid size & $9 \times 9$ & $13 \times 13$ \\
    & Action space size & $17$ & $43$ \\
    & Max reward (\# achievements) & $22$ & $226$ \\
    \midrule
    Environment (not used) & Symbolic (one-hot) input size & $1345$ & $8268$ \\
    & Max cardinality of each symbol & $17$ & $40$ \\
    & Number of levels & $1$ & $10$ \\
    \midrule
    \midrule
    MFRL parameters & Number of environments $N_{\text{env}}$ & $48$ & $64$ \\
    & Rollout horizon in environment $T_{\text{env}}$ & $96$ & $64$ \\
    \midrule
    MBRL parameters & Number of environments $N_{\text{env}}$ & $48$ & $16$ \\
    & Rollout horizon in environment $T_{\text{env}}$ & $96$ & $96$ \\
    & Rollout horizon for TWM $T_{\text{WM}}$ & $20$ & $20$ \\
    \bottomrule
    \end{tabular}
\end{table*}