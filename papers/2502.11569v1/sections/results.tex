\begin{table*}
\centering
\scriptsize
\begin{adjustbox}{width=\textwidth}
\begin{tabulary}{1.2\textwidth}{LCCCCCCCCS[table-format=2.2]} % Adjusted width and column spec, S column for numbers
\toprule
    \multicolumn{3}{c}{\textbf{Models}} & \multicolumn{2}{c}{\textbf{Computational Requirements (GB)}} & \textbf{GSM8K} & \multicolumn{2}{c}{\textbf{ARC}} & \textbf{CommonsenseQA} & \textbf{Sorting\_Avg} \\
\cmidrule(lr){1-3} \cmidrule(lr){4-5} \cmidrule(lr){7-8}
\textbf{Model} & \textbf{Param.} & \textbf{Optimization} &\textbf{ GPU Memory} & \textbf{Disk Space} &  & \textbf{Easy} & \textbf{Challenge} &  &  \\
\midrule
\textbf{Qwen2.5} & \textbf{0.5B} & None & 2.02 & 0.95 & 46.80$_{\pm0.77}$ & 62.50$_{\pm0.21}$ & 44.28$_{\pm1.03}$ & 46.90$_{\pm1.49}$ & 2.61 \\
\textbf{Llama-3.2} & \textbf{1B} & None & 4.73 & 2.4 & 36.39$_{\pm0.47}$ & 67.23$_{\pm1.40}$ & 47.50$_{\pm0.22}$ & 48.38$_{\pm0.45}$ & 7.83 \\
\textbf{Qwen2.5} & \textbf{1.5B} & None & 6.68 & 2.9 & 70.00$_{\pm0.53}$ & 87.58$_{\pm0.21}$ & 73.81$_{\pm0.30}$ & 71.85$_{\pm0.48}$ & 29.11 \\
\textbf{SmolLM2} & \textbf{1.7B} & None & 6.55 & 3.2 & 46.17$_{\pm0.41}$ & 75.04$_{\pm0.18}$ & 54.21$_{\pm1.07}$ & 53.18$_{\pm1.27}$ & 16.83 \\
\textbf{Qwen2.5} & \textbf{3B} & None & 12.42 & 5.8 & 84.74$_{\pm0.28}$ & 93.49$_{\pm0.07}$ & 83.73$_{\pm0.38}$ & 76.25$_{\pm0.29}$ & 39.89 \\
\textbf{Phi-3.5 }& \textbf{3.8B} & None & 14.6 & 7.2 & 85.47$_{\pm0.47}$ & 95.09$_{\pm0.14}$ & 86.89$_{\pm0.16}$ & 76.11$_{\pm0.04}$ & 47.33 \\
\textbf{Qwen2.5 }& \textbf{7B} & None & 30.05 & 15 & 91.76$_{\pm0.20}$ & 96.03$_{\pm0.17}$ & 90.53$_{\pm0.12}$ & 82.66$_{\pm0.14}$ & 57.67 \\


\cmidrule{3-10}

\multirow{2}{*}{\textbf{Mistral$_{\text{v0.3}}$}} & \multirow{2}{*}{\textbf{7B}} & None & 27.67 & 14 & 54.84$_{\pm0.56}$ & 88.99$_{\pm0.34}$ & 76.82$_{\pm0.29}$ & 69.83$_{\pm0.10}$ & 23.11 \\
 &  & pruned2.4 & 27.6 & 14 & 30.30$_{\pm0.94}$ & -- & -- & -- & 9.94 \\


\cmidrule{3-10}

\multirow{2}{*}{\textbf{Llama$_{\text{3.1}}$}} & \multirow{2}{*}{\textbf{8B}} & None & 30.65 & 15 & 83.45$_{\pm0.41}$ & 92.07$_{\pm0.28}$ & 79.58$_{\pm0.26}$ & 74.28$_{\pm0.52}$ & 60.11 \\
 &  & pruned (2of4) & 30.65 & 15 & 51.86$_{\pm0.34}$ & -- & -- & -- & 15.94 \\
 
 \cmidrule{3-10}


 \multirow{1}{*}{\textbf{Mistral$_{\text{Nemo}}$}} & \multirow{1}{*}{\textbf{12B}} & None & 57.89 & 23 & 86.76$_{\pm0.57}$ & 92.79$_{\pm0.07}$ & 83.70$_{\pm0.32}$ & 72.78$_{\pm0.78}$ & 60.89 \\

\cmidrule{3-10}

\multirow{3}{*}{\textbf{Llama$_{\text{3.1}}$}} & \multirow{3}{*}{\textbf{70B}} & None & 57.04 & 28 & 94.29$_{\pm0.40}$ & 97.87$_{\pm0.05}$ & 93.37$_{\pm0.22}$ & 84.08$_{\pm0.54}$ & 74.61 \\
&  & GPTQ 8-bit & 17.24 & 16 & 94.49$_{\pm0.25}$ & 97.90$_{\pm0.12}$ & 93.71$_{\pm0.24}$ & 84.22$_{\pm0.14}$ & 74.78 \\
&  & GPTQ 4-bit & 10.65 & 9.4 & 94.74$_{\pm0.32}$ & 97.57$_{\pm0.10}$ & 93.17$_{\pm0.21}$ & 83.10$_{\pm0.19}$ & 69.56 \\

\cmidrule{3-10}

\multirow{3}{*}{\textbf{Qwen$_{\text{2.5}}$}} & \multirow{3}{*}{\textbf{32B}} & None & 125 & 62 & 95.40$_{\pm0.29}$ & 98.26$_{\pm0.10}$ & 95.25$_{\pm0.18}$ & 87.11$_{\pm0.37}$ & 87.17 \\
&  & GPTQ 8-bit & 33.81 & 33 & 95.73$_{\pm0.19}$ & 98.34$_{\pm0.02}$ & 95.16$_{\pm0.40}$ & 86.62$_{\pm0.10}$ & 87.39 \\
&  & GPTQ 4-bit & 52.42 & 19 & 95.73$_{\pm0.09}$ & 98.09$_{\pm0.05}$ & 95.19$_{\pm0.11}$ & 87.06$_{\pm0.58}$ & 87.28 \\

% \cmidrule{3-10}
\cmidrule{3-10}

\multirow{3}{*}{\textbf{Llama$_{\text{3.1}}$}} & \multirow{3}{*}{\textbf{70B}} & None & 269.17 & 132 & 95.10$_{\pm0.28}$ & 98.34$_{\pm0.05}$ & 94.43$_{\pm0.28}$ & 83.73$_{\pm0.58}$ & 97.33 \\
&  & W8A8 & 69.34 & 68 & 94.72$_{\pm0.34}$ & 98.43$_{\pm0.22}$ & 94.62$_{\pm0.14}$ & 83.92$_{\pm0.28}$ & 96.89 \\
&  & W4A16 & 107.34 & 38 & 95.15$_{\pm0.33}$ & 98.26$_{\pm0.08}$ & 94.51$_{\pm0.26}$ & 82.77$_{\pm0.19}$ & 95.28 \\

\bottomrule
\end{tabulary}
\end{adjustbox}
\caption{Performance and computational requirements of models on GSM8K, ARC, CommonsenseQA, and Sorting Tasks. The table reports the model size (in billions of parameters), optimization type (if any), GPU memory usage and disk space (in GB), and accuracy scores for each benchmark. The Sorting\_Avg column represents the average accuracy across 6 different sorting tasks (detailed in Section \ref{main: Sorting Task Results}).}
\label{main:overall_performance}
\end{table*}

\section{Results and Insights}
\label{section:4}
We evaluated \textbf{72} SLMs across \textbf{six} families: (1) SLMs trained from scratch, (2) Llama-3.2, (3) Llama-3.1, (4) Mistral and Mistral-Nemo, (5) Qwen2, and (6) Qwen2.5. Additionally, we reported computational requirements (GPU and Disk Space) to provide a holistic comparison. Complete results are detailed in Appendix \ref{app: Detailed Results}. In addition to the GPT-based judge for evaluation, we also evaluated all the model performance with the widely used framework, \textbf{lm-eval-harness}, on eight benchmarks. Complete Results are detailed in Appendix \ref{app: Results with lm-eval-harness}.




\subsection{Overall Performance}
Our analysis show that emergent properties, i.e., performance improvements not observed in smaller models are highly model family-dependent (Table \ref{main:overall_performance}) rather than a universal trend across families. For example, Qwen2.5 (7B) outperforms Mistral (7B) by nearly 35 points on GSM8K despite having nearly the exact parameter count. This is primarily due to Qwen2.5's extensive pre-training data (18T tokens) \cite{qwen2025qwen25technicalreport} and a post-training recipe using supervised fine-tuning and multi-stage reinforcement learning to align output better with humans. \emph{This suggests that training data and recipes are more critical than parameter size alone.}




On ARC-E, larger models such as Llama-3.1 (70B) consistently achieve near-perfect scores, where the more straightforward reasoning tasks align well with their knowledge recall capabilities. In contrast, smaller models like Qwen2.5 (0.5B) and SmolLM2 (1.7B) exhibit performance drops, especially on ARC-C, which demands more nuanced reasoning. The performance gap between ARC-E and ARC-C suggests that factual recall (ARC-E) strongly depends on model size. In contrast, reasoning under ambiguity (ARC-C) benefits more from diverse training data and dataset quality than just scale.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{figures/final-prompt-effect.pdf}
        \caption{Effect of Prompts on SLM Performance on the GSM8K. The x-axis represents different model (with the model size in billions of parameters), and the y-axis represents \textbf{mean} accuracy, and the bar represents \textbf{variance} (3-folds). Each line corresponds to different prompting strategies (Direct I/O, Chain-of-Thought (CoT), 5-shot, 5-shot CoT, and 8-shot).}
    \label{fig:prompt-effect}
\end{figure}

\subsection{Effect of Prompting}
We find that prompt complexity had a minimal impact on performance across recent models. Recent models exhibit strong internal reasoning capabilities when either direct I/O or COT multi-shot prompts are used. \emph{This suggests that recent training methodologies have already embedded reasoning capabilities,} diminishing the effect of \textbf{explicit} reasoning prompts. This calls for more advanced prompting techniques to enhance reasoning further. More analysis on why explicit COT does not work is detailed in Appendix \ref{app: Why explicit chain-of-thought does not Elicit Reasoning?}.

On GSM8K, Direct I/O prompts consistently outperform or match closely with complex prompts such as CoT and multi-shot settings. \emph{This suggests that excessive instructions can confuse SLMs rather than improve their reasoning.} Figure \ref{fig:prompt-effect} shows that providing too many instructions or few-shot examples does not improve performance. SLMs perform better with straightforward queries rather than complex prompts. This suggest the need for task-specific prompt engineering rather than relying on general-purpose strategies like CoT. 

\subsection{Sorting Task Results}
\label{main: Sorting Task Results}
For the sorting tasks, Figure \ref{fig:sorting} demonstrates that SLMs perform well on shorter lists but struggle as the list length increases. This highlights limitations in their ability to handle long-context numerical reasoning. Larger models like Llama-70B manage long sequences better, but even they show declining performance as task complexity increases (introducing -ve numbers).

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{figures/final-sorting.pdf}
        \caption{Performance of SLMs on Sorting Tasks. The x-axis represents different models (with parameters in billions), the y-axis represents the \textbf{mean} accuracy, and the bar represents \textbf{variance} (3-folds). Each line corresponds to different sorting tasks (8 +ve, 8 mixed, 16 +ve, 16 mixed, 32 +ve, and 32 mixed numbers).}
    \label{fig:sorting}
\end{figure}


Furthermore, models struggle more when sorting mixed numbers (both +ve and -ve), with accuracy dropping compared to positive-only sorting. For example, Llama-3.1 (70B) achieves near-perfect accuracy on positive-only datasets, but performance drops on mixed numbers. \textit{This suggests that handling negative numbers introduces an additional layer of complexity that current architectures do not adequately address.} Also, as the number length increases, performance drops significantly. \textit{This indicates a scalability bottleneck in current architectures for algorithmic reasoning.}


We also observe failures where models introduce numbers not present in the input or simply repeat the entire input as the output. These errors become more frequent as list length increases, particularly for 32-number lists. More details are provided in Appendix \ref{app: Sorting incorrect examples}.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1\linewidth]{figures/quantization_plot.pdf}
            \caption{Impact of Quantization on Model Performance across Different Benchmarks. The figure shows the performance of different models on GSM8K (Direct I/O), Average of ARC-E, and CommonsenseQA, and Average of all sorting tasks with varying quantization levels. All results are from \textbf{Qwen2.5 Family}. The x-axis represents the parameters size (in billions), and the y-axis represents the \textbf{mean} accuracy and bar represents \textbf{variance} (3-folds).}
    \label{fig:quant}
\end{figure*}

\subsection{Quantization Effects}
We find that quantization has minimal impact on the reasoning performance of larger models, whereas smaller models suffer more from compression. However, quantizing smaller models is often unnecessary since they are already compact and computationally efficient. On the other hand, compressing larger models allows them to match the efficiency of smaller models while retaining superior reasoning capabilities.

Figure \ref{fig:quant} shows that even with aggressive quantization (4-bit), larger models retain their reasoning ability with near-identical performance as their uncompressed counterparts. For example, Llama-3.1 (70B) with W8-A8 quantization retains $\sim$100\% of its original accuracy while reducing computational cost (GPU) by over 75\% (Table \ref{main:overall_performance}). Notably, a GPTQ 4-bit quantized Qwen2.5 (14B) outperforms Qwen2.5 (7B) while requiring $\sim$1/3rd computational resources (GPU), which demonstrate that quantized versions of larger models are often more effective than standalone small models.

\textit{These findings show that quantization can make models significantly more efficient without a substantial drop in accuracy.} This could be a viable strategy for deploying models in resource-constrained environments while preserving strong reasoning capabilities.





\subsection{Pruning and Distillation Effects}
Compared to quantization, pruning and distillation have a more detrimental impact on reasoning performance. After pruning, knowledge distillation is typically used with a recovery dataset to mitigate performance loss. In most cases, GSM8K was used as the recovery dataset. We observed that pruned models performed reasonably well on GSM8K (Table \ref{main:overall_performance}) when evaluated with direct prompting. However, on datasets such as ARC-E, ARC-C, and CommonsenseQA, pruned models frequently produce nonsensical outputs unrelated to the input query. Example cases are reported in Appendix \ref{app: Poor Performance of Pruned Models}. Similarly, for sorting tasks, pruned models perform worse than their quantized counterparts, with frequent errors such as misplaced numbers, duplicated inputs, or missing elements in the final output. Some models, like Llama-3.1 and Mistral-7B, retain partial reasoning ability but remain far behind quantized models in overall performance. \emph{This indicates that pruning disrupts their ability to generalize reasoning across tasks.}


\begin{table*}
\centering
\scriptsize
\begin{adjustbox}{width=\textwidth}
\begin{tabulary}{\textwidth}{LCC CCCCC CCCCCCCC} % Adjusted column count
\toprule
\multicolumn{3}{c}{\multirow{3}{*}{\textbf{Models}}} & \multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}GSM-Plus\end{tabular}}} & \multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Original\\ (GSM8K)\end{tabular}}} & \multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}c@{}} $\Delta (\downarrow)$ \\ (\% drop)\end{tabular}}} & \multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}c}MR-GSM8K\\ (MR Score)\end{tabular}}} & \multicolumn{8}{c}{\textbf{Mr-Ben (MR-Score)}} \\
\cmidrule(lr){8-15}
 & & & & & &  & \textbf{Bio.} & \textbf{Math} & \textbf{Phy.} & \textbf{Medicine} & \textbf{Code.} & \textbf{Chem.} & \textbf{Logic} & \textbf{Avg} \\
\cmidrule(lr){1-3}
\textbf{Model} & \textbf{Param.} & \textbf{Optimization} &  &  &  &  &  &  & & & & & & \\
\midrule
\multirow{2}{*}{\textbf{Qwen2.5}} & \multirow{2}{*}{\textbf{3B}} & None (B) & 60.44 & 77.91 & 17.47 & 8.1 & 7.8 & 10.3 & 9 & 6.1 & 0.2 & 8.5 & 6.8 & 6.9 \\
 &  & None (Ins) & 68.33 & 84.74 & 16.41 & 11 & 10.1 & 11.2 & 10.4 & 7.8 & 3.5 & 10.6 & 8.4 & 8.8 \\
  \cmidrule{2-15}
\textbf{Mistral} & \textbf{7B} & pruned2.4 & 25.44 & 30.30 & 4.89 & 4 & 0 & 1.3 & 2.3 & 0 & 0 & 1.8 & 0.3 & 0.8 \\
\cmidrule{2-15}
\multirow{3}{*}{\textbf{Llama-3.1}} & \multirow{3}{*}{\textbf{8B}} & None & 67.10 & 83.45 & 16.35 & 24.2 & 12.9 & 10.8 & 10.9 & 12.7 & 6 & 13.5 & 10 & 11 \\
& & w8a16 & 66.78 & 83.95 & 17.17 & 23.3 & 12.7 & 11.9 & 11.2 & 13 & 6.6 & 12.8 & 9.3 & 11.1 \\
  & & pruned2of4 & 35.17 & 51.86 & 16.69 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
\cmidrule{2-15}

\multirow{2}{*}{\textbf{Qwen2.5}} & \multirow{2}{*}{\textbf{32B}} & None & 82.71 & 95.40 & 12.69 & 55.6 & 23.4 & 24.7 & 24.3 & 19.9 & 14.3 & 24.7 & 18.4 & 21.4 \\
 &  & GPTQ-8 & 82.78 & 95.73 & 12.95 & 55.6 & 23.5 & 24.7 & 23.9 & 19.9 & 14.3 & 24.3 & 18.4 & 21.3 \\

 \cmidrule{2-15}
 \multirow{2}{*}{\textbf{Llama-3.1}} & \multirow{2}{*}{\textbf{70B}} & None &	83.65 &	95.10 &	11.45 &	40.6 &	22 &	19.8 &	19.3 &	19.9 &	13.3 &	25 &	17.8 & 19.6 \\
 & & w8a16 & 80.03 &	92.92 & 12.89 & 44.2	& 20.1 &	17.4 &	14.9 &	15.1 &	11.6 &	21.2 & 14.6 & 16.4 \\


\bottomrule
\end{tabulary}
\end{adjustbox}
\caption{Performance of various SLMs on reasoning robustness, including adversarial robustness (GSM-Plus), intermediate reasoning (MR-GSM8K), and identifying errors in reasoning (MR-Ben). The metrics reported include accuracy scores, percentage drop in accuracy ($\Delta$), and MR-Scores, covering various models with different parameter sizes and optimizations. Detailed individual task results for MR-GSM8K is reported in Appendix \ref{app: [Task 5.2] MR-GSM8K: Intermediate reasoning test}.}
\label{tab:robustness}
\end{table*}




\subsection{Robustness}
\label{main: Robustness}

\paragraph{Adversarial Robustness (GSM-Plus)} From Table \ref{tab:robustness}, we observe that models generally experience a drop in accuracy on adversarial data but not drastically. Results reveal a clear performance hierarchy: Larger models (Qwen2.5-32B, Llama-3.1-70B) demonstrate higher resilience to adversarial perturbations, with relatively small accuracy drops (12.69 and 11.45 points, respectively). Smaller models (Qwen2.5-3B, Llama-3.1-8B) experience larger drops (16.41 and 16.35 points, respectively), highlighting their fragility in adversarial settings. \emph{This shows that larger models are inherently more robust in adversarial settings.} 

We observe that quantization does not significantly impact adversarial robustness. This suggests that \emph{quantized models retain their original reasoning resilience,} further reinforcing their viability. Conversely, pruned models struggle significantly with adversarial robustness. Sparse-Llama-8B-2of4 and OpenHermes-7B, for example, experience drastic accuracy declines. Their inability to generalize under adversarial conditions likely stems from reduced model capacity and insufficient training diversity, making them vulnerable to edge-case scenarios. 


\paragraph{Intermediate Reasoning (MR-GSM8K)} Table \ref{tab:robustness} also reports MR-GSM8K scores, which measure models' ability to generate and refine intermediate reasoning steps. Quantization has minimal impact on intermediate reasoning for larger models. For example, Qwen2.5-32B and its GPTQ-INT8 variant achieve identical MR-Scores, indicating that precision reduction does not degrade logical consistency. In contrast, pruned models fail to maintain coherent reasoning, with MR-Scores dropping to zero. This suggests that pruned architectures are more vulnerable to reasoning degradation when architectural integrity is compromised. Interestingly, the open-source Qwen2.5-32B (55.6) even surpasses closed propriety models like GPT-4-Turbo (53.0) and Claude3-Sonnet (20.8) in intermediate reasoning, based on reported results \cite{zeng2024mrgsm8kmetareasoningbenchmarklarge}. 


\paragraph{Identifying Errors in Reasoning (MR-Ben)} Table \ref{tab:robustness} also reports MR-Ben scores. The trends observed are consistent: 1) Qwen-32B and Llama-70B outperform other models, 2) Quantization has minimal overall impact, and 3) Pruned models perform worse. Models perform best in biology and math, while coding and logic remain challenging. 

Findings from these 3 datasets further support the argument that SLMs are not just retrieving answers from pre-training but engaging in structured reasoning. These findings also contribute to the debate on whether neural networks genuinely reason. The original CoT paper \cite{wei2022chain} raised concerns that \emph{\enquote{CoT elicits reasoning but does not confirm whether the model is actually reasoning.}} These three benchmarks also help address this question, as they were released after many of the SLMs we tested, ensuring no contamination.


