\section{Conclusions and Future Directions}
\label{section:5}
In this work, we systematically evaluated the reasoning ability of 72 SLMs, including their compressed variant, across 14 benchmarks. We also examine their robustness under adversarial conditions and intermediate reasoning. Overall, we observed: \textbf{\textit{1)}} LLMs tend to outperform SLMs in reasoning, but certain SLMs, such as the Qwen2.5 family, perform on par with LLMs. This is primarily attributed to their extensive pre-training (18T tokens, more than double that of Qwen2’s 7T) and a robust post-training recipe using supervised fine-tuning and multi-stage reinforcement learning. \textbf{\textit{2)}} Among compression techniques, quantization proves to be a safer approach, preserving reasoning capabilities with minimal trade-offs. However, pruning drastically degrades performance, often leading to nonsensical or incomplete outputs. This suggests that compressing pre-trained LLMs is more effective than training SLMs from scratch. \textbf{\textit{3)}} LLMs exhibit stronger robustness in adversarial settings and intermediate reasoning tasks. However, quantization does not significantly impact a model’s resilience in these scenarios, reinforcing its practicality as a compression method. \textbf{\textit{4)}} SLMs lag behind LLMs in instruction following, which may limit their applicability in tasks requiring precise adherence to input constraints. We hope these insights provide practical guidance for researchers in selecting an SLM. Future research should focus on improving the instruction-following capabilities of SLMs and exploring compression strategies that maintain reasoning performance while enhancing efficiency and robustness.

\newpage

