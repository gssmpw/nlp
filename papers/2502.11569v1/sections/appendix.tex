\onecolumn
\newpage
\addtocontents{toc}{\protect\setcounter{tocdepth}{3}}

\renewcommand{\contentsname}{Contents of the Appendix}  
\newpage

\tableofcontents

\twocolumn

\section{Detailed Results}
\label{app: Detailed Results}

\subsection{[Task 1] GSM8K}
In Table \ref{app: gsm8k}, we detail the results of 70 different SLMs with their compressed versions on GSM8K dataset. We report results with 5 different prompts and we also report their computational requirements: GPU Memory Usage and Disk Space requirements. \medskip

\textbf{Further Insights:} Model size influences performance, with larger models like Llama-3.1 (8B, 70B) outperforming smaller ones such as SmolLM2 (1.7B), Minitron (4B), and Hymba (1.5B). However, the performance gap between Llama-3.2 and Qwen2.5 (3B) suggests that increasing parameters alone does not guarantee proportional improvements—architectural design and training data are also crucial factors.\medskip

Quantization, even with aggressive techniques like W4-A16, has minimal impact on mathematical reasoning. This suggests that compact models can be effectively deployed in resource-constrained environments without significant performance degradation. Notably, Llama-3.1-8B retains strong accuracy even with INT4/INT16 quantization. While quantization strategies such as FP8 and dynamic FP8 provide substantial memory savings, they maintain competitive performance. Figure \ref{fig:app1}, \ref{fig:app2}, \ref{fig:app3} shows an analysis of model performance across various tasks and the impact of quantization.

%##################################################################################################

\subsection{[Task 2\&3] ARC-E, ARC-C, CommonsenseQA}
In Table \ref{app: remaining}, we detail the results of 71 different SLMs with their compressed versions on ARC-Easy, ARC-Challenge, and CommonsenseQA datasets. We report results with direct prompting since COT or multi-shots does not help much here. \medskip

\textbf{Further Insights:} Findings from ARC-E, ARC-C, and CommonsenseQA align with GSM8K results. Larger models, especially from the Qwen and LLama family, demonstrate superior reasoning abilities in both scientific and commonsense tasks. On ARC-Easy and ARC-Challenge, performance scales predictably with model size, with Llama-3.1-70B achieving near-optimal scores. FP8 quantization proves highly effective, maintaining performance parity with full-precision models up to 8B. 

%##################################################################################################

\subsection{[Task 4] Sorting Numbers}
In Table \ref{app: remaining}, we detail the results of 71 different SLMs along with their compressed versions on 6 different sorting datasets. We report results with direct prompting also here. Sorting tasks (positive-only, mixed numbers, varying lengths of 8, 16, 32) serve as a strong benchmark for evaluating algorithmic reasoning, as they are unlikely to have been seen in pre-training. Unlike GSM8K and ARC, which may contain learned patterns, sorting purely tests a model’s ability to reason and execute structured tasks.\medskip


\begin{table*}[t]
    \centering
    \begin{align*}
    \text{MCC} &= \frac{TP \times TN - FP \times FN}{\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}} \\
    ACC_{step} &= \frac{N_{correct\_first\_error\_step}}{N_{incorrect\_sols}} \\
    ACC_{reason} &= \frac{N_{correct\_error\_reason}}{N_{incorrect\_sols}} \\
    MR-Score &= w_1 \times \max(0, MCC) + w_2 \times ACC_{step} + w_3 \times ACC_{reason}
    \end{align*}
\end{table*}

\textbf{Further Insights:} Larger models like Llama-3.1-70B achieve near-perfect accuracy on positive-only sorting and maintain strong performance on mixed datasets. In contrast, smaller models such as SmolLM2 (1.7B) struggle, especially as input length increases, with accuracy dropping to near zero on mixed datasets.\medskip

Quantization effects vary: Llama-3.2-8B with FP8 quantization closely matches its full-precision counterpart in positive-only sorting but sees performance degradation in mixed datasets. More aggressive compression (W4-A16) slightly weakens performance. This highlights that fine-grained reasoning tasks are more sensitive to precision reduction. Smaller models like Qwen2.5-7B and Mistral-7B suffer significant accuracy drops under 4-bit quantization.\medskip

Also, as task complexity increases, even large models struggle with longer sequences in mixed datasets. This reveals a fundamental bottleneck in current architectures for algorithmic reasoning. This suggests the need for specialized training techniques or architectural modifications to improve structured problem-solving in SLMs.

%##################################################################################################

\subsection{[Task 5.1] GSM-PLUS: Perturbation Analysis}


In section \ref{main: Robustness}, we saw the performance of models across adversarial samples. In Table \ref{app: perturbation}, we detail the analysis of SLMs across various perturbations. Mostly, we see that models struggle most with critical thinking variation. \medskip

Larger models, such as \texttt{Qwen2.5-32B} and \texttt{Llama-3.1-70B}, exhibit strong resilience, achieving over 85\% accuracy across most variations, particularly in numerical substitution and digit expansion. However, they show a notable drop in performance for critical thinking, suggesting that sheer scale does not entirely mitigate reasoning challenges introduced by complex perturbations.\medskip

Smaller models, including \texttt{Mistral-7B (pruned2.4)} and \texttt{Llama-3.1-8B (2of4)}, struggle significantly, especially in fraction conversions and arithmetic operations, with performance dropping below 40\%. Quantized variants (\texttt{GPTQ-8} and \texttt{w8a16}) show marginal degradation compared to full-precision instruct models, suggesting quantization has a limited impact on robustness for well-trained models. 

%##################################################################################################

\subsection{[Task 5.2] MR-GSM8K: Intermediate reasoning test}
\label{app: [Task 5.2] MR-GSM8K: Intermediate reasoning test}
The MR-GSM8K benchmark evaluates models' ability to perform intermediate reasoning, focusing on logical consistency across multi-step problems. In Table \ref{app: mrgsm8k}, we detail the results of 10 selected SLMs with their compressed versions on the MR-GSM8K dataset.\medskip

\textbf{About Metric:} The MR-Score is a composite metric used to evaluate the meta-reasoning abilities of language models in the MR-GSM8K benchmark. It combines the performance of LLMs across three tasks: determining solution correctness (measured by MCC), identifying the first error step (ACC\_{\small step}), and explaining the error reason (ACC\_{\small reason}). The final MR-Score is a weighted combination of these three metrics, with weights chosen empirically to balance the contribution of each task. This comprehensive evaluation provides a holistic assessment of LLMs' meta-reasoning capabilities, going beyond simply solving math problems to assess their ability to reason about the reasoning process itself.\medskip

Below are the different task descriptions, which are shown in Table \ref{app: mrgsm8k}:

\begin{enumerate}

\item \textbf{Task 1 TPR (k=0)}: This is the true positive rate for Task 1 in a zero-shot setting (k=0). Task 1 determines the correctness of a given solution to a math problem. The true positive rate measures the model's ability to identify incorrect solutions correctly.

\item \textbf{Task 1 TNR (k=0)}: This is the true negative rate for Task 1 in a zero-shot setting. It measures the model's ability to identify correct solutions correctly.

\item \textbf{Task 1 MCC Score (k=0)}: This is the Matthews Correlation Coefficient (MCC) score for Task 1 in a zero-shot setting. MCC assesses the overall performance of a binary classification model, in this case, classifying solutions as correct or incorrect.

\item \textbf{Task 2 Accuracy (k=0)}: This represents the model's accuracy in Task 2 under a zero-shot setting. Task 2 identifies the first error step in an incorrect solution.

\item \textbf{Task 3 Accuracy (k=0)}: This is the model's accuracy in Task 3 under a zero-shot setting. Task 3 requires the model to provide a reason for the error identified in Task 2.

\item \textbf{MR-Score (k=0)}: This is a composite score that combines the model's performance across all three tasks in a zero-shot setting. It provides a holistic evaluation of the model's meta-reasoning abilities.
\end{enumerate}

\textbf{Further Insights:} Larger models, such as Qwen2.5-32B and Llama-3.1-70B, consistently outperform smaller counterparts. For example, Qwen2.5-32B achieves an MR-Score of 55.6, significantly higher than smaller models like Mistral-7B (4.0), which struggle to perform intermediate reasoning steps. Based on reported results, Qwen2.5-32B outperforms GPT-4-turbo (53.0) on intermediate reasoning. Quantization has minimal impact on intermediate reasoning for larger models. For instance, Qwen2.5-32B and its GPTQ-INT8 variant achieve identical MR-Scores, retaining performance despite the precision reduction. \medskip

Interestingly, intermediate reasoning performance is not purely scale-dependent but also highly architecture-specific. For example, Llama-3.1-70B slightly underperforms Qwen2.5-32B despite having more parameters. These findings highlight the critical importance of model design choices in achieving superior intermediate reasoning performance.

%##################################################################################################

\subsection{Pruned/Distilled Models}
In Table \ref{app: prun1} and \ref{app: prun2}, we detail the results of pruned and distilled variants of SLMs on the GSM8K dataset and sorting tasks. We skipped the evaluation of ARC-E, ARC-C, and CommonsenseQA since the outputs were nonsensical and accuracy was nearly 0. CoT prompting significantly improved performance in these models, boosting scores by 10–20 points. However, multi-shot prompts were less effective, with some models failing to generate any output.\medskip

These results highlight that pruning disproportionately affects reasoning-intensive tasks, reducing both logical consistency and robustness across diverse datasets. This emphasize the need for improved recovery strategies or alternative compression techniques when applying pruning to models used for reasoning tasks.

%##################################################################################################

\input{sections/appendix/Tables/table-gsm8k}
\input{sections/appendix/Tables/table-remaining}



\begin{figure*}
    \centering
    \begin{subfigure}[b]{1\linewidth}
        \centering
        \includegraphics[width=\linewidth]{sections/appendix/Figures/app_gsm8k.pdf}
        \caption{GSM8K (Direct I/O)}
        % \label{fig:app1}
    \end{subfigure}
    
    \vspace{0.5cm} % Adjust vertical space as needed
    
    \begin{subfigure}[b]{1\linewidth}
        \centering
        \includegraphics[width=\linewidth]{sections/appendix/Figures/app_arc.pdf}
        \caption{ARC and CommonsenseQA}
    \end{subfigure}
    
    \vspace{0.5cm} % Adjust vertical space as needed
    
    \begin{subfigure}[b]{1\linewidth}
        \centering
        \includegraphics[width=\linewidth]{sections/appendix/Figures/app_sorting.pdf}
        \caption{Sorting Tasks}
    \end{subfigure}
    
    \caption{Performance of different models on GSM8K (Direct I/O), ARC, CommonsenseQA, and sorting tasks. The x-axis represents the parameters size (in billions), and the y-axis represents the \textbf{mean} accuracy, with error bars indicating the \textbf{variance} (3-folds).}
    \label{fig:app1}
\end{figure*}



\begin{figure*}
    \centering
    \begin{subfigure}[b]{1\linewidth}
        \includegraphics[width=1\linewidth]{sections/appendix/Figures/quantization_plot_2x2.pdf}
        \caption{Qwen2.5 Family with GPTQ Quantization.}
    \end{subfigure}
    
    \vspace{1cm} % Add some vertical space between the subfigures

    \begin{subfigure}[b]{1\linewidth}
        \includegraphics[width=1\linewidth]{sections/appendix/Figures/quantization_plot_2x2-1.pdf}
        % \caption{Impact of Quantization on Model Performance (Part 2). The figure shows the performance of different models on Average of all sorting tasks with varying quantization levels. All results are from \textbf{Qwen2.5 Family}. The x-axis represents the parameters size (in billions), and the y-axis represents the \textbf{mean} accuracy and bar represents \textbf{variance} (3-folds).}
        % \label{fig:app6}
    \end{subfigure}
    
    \caption{Impact of Quantization on Model Performance across Different Benchmarks.}
    \label{fig:app2}
\end{figure*}



\begin{figure*}
    \centering
    \begin{subfigure}[b]{1\linewidth}
        \includegraphics[width=1\linewidth]{sections/appendix/Figures/quantization_plot_2x3.pdf}
        \caption{Qwen2.5 Family with GPTQ Quantization.}
        % \label{fig:app5}
    \end{subfigure}
    
    \vspace{0.5cm} % Adjust spacing as needed

    \begin{subfigure}[b]{1\linewidth}
        \includegraphics[width=1\linewidth]{sections/appendix/Figures/quantization_plot_2x3-1.pdf}
        % \caption{Impact of Quantization on Model Performance (Part 2): Sorting Tasks.}
        % \label{fig:app7}
    \end{subfigure}
    
    \caption{Impact of Quantization on Model Performance across Sorting Tasks.}
    \label{fig:app3}
\end{figure*}



% \newpage
% ~ \newpage
% ~ \newpage
% ~ \newpage

\input{sections/appendix/Tables/Perturbation}
\input{sections/appendix/Tables/MR-GSM8K}
\input{sections/appendix/Tables/Pruned}


\newpage
% ~ \newpage
% ~ \newpage
% ~ \newpage
% ~ \newpage
% ~ \newpage
% ~ \newpage

% ~\newpage

% ~\newpage
%##################################################################################################


\section{Complete Results with lm-eval-harness}
\label{app: Results with lm-eval-harness}

Here, we present the evaluations of all models using the open-sourced framework \textbf{-- lm-eval-harness}. Table \ref{app: lm-1} reports the results for GSM8K (5-shot), ARC Easy, and ARC Challenge. Table \ref{app: lm-2} contains the results for CommonsenseQA, HellaSwag, MathQA, and OpenBookQA. Similarly, the results for the pruned models across these seven datasets are provided in Table \ref{app: lm-3} and Table \ref{app: lm-4}. Additionally, we observed that results for small models are generally not reported on hard MATH datasets. So, we also evaluated some SLMs on the MATH dataset (Table \ref{tab:MATH}). 

\medskip

Lm-eval-harness is a standardized tool to benchmark language models across diverse tasks, including reasoning, common sense, and question-answering. All other tasks were assessed in a zero-shot configuration except for GSM8K, which was evaluated using a 5-shot prompting strategy (the framework’s default setting).\medskip

A notable discrepancy was observed between the results from lm-eval-harness and our evaluations using GPT-4. While larger models performed well in both, smaller models fared worse under lm-eval-harness. This variation could be from differences in evaluation metrics, task design, or prompt structures between the two frameworks. 

\input{sections/appendix/Tables/lm-eval}

\newpage
~\newpage
~\newpage
% ~\newpage
% ~\newpage
% ~\newpage

%##################################################################################################


\section{Prompts and Scripts}
\label{app: Prompts and Scripts}

In this section we provide all the prompts template we used in this work. 

\subsection{Model Prompts (1-9)}
Here, we provide the complete prompts to elicit model responses from Prompt 1 to Prompt 9. We used several prompts, including direct I/O, COT, and few-shot, to encourage reasoning responses from the models. These nine carefully designed prompts aimed to elicit diverse reasoning behaviors, ranging from simple questions to multi-step reasoning tasks. This ensures a thorough evaluation of the models' reasoning capabilities across different datasets.

\input{sections/appendix/Tables/prompts}


\subsection{GPT-4 Evaluation Prompts}
\label{app: GPT-4 Evaluation Prompts}
In this section, we provide the prompt used for our judge (GPT). We used GPT-4 as the LLM-as-a-judge, using carefully designed prompts to guide its evaluation of responses generated by SLMs. These prompts were chosen to ensure consistency and minimize potential biases in the evaluation process.

\input{sections/appendix/Tables/gpt-prompts}

\subsection{Sorting Parsing Script: 13 Variations}
\label{app: Sorting Parsing Script: 13 Variations}

Here's a list of scenarios that Parsing Script is designed to handle, categorized by the parsing strategy employed:

\vspace{0.5em}

\noindent \textbf{Bracketed Lists (High Confidence)}
\vspace{0.2em}

\noindent 1. \texttt{[1, 2, 3, 4]}: Standard, comma-separated list within square brackets.

\noindent 2. \texttt{[ 1,  2, 3,4]}: List with extra spaces between elements and brackets.

\noindent 3. \texttt{[-1, 0, 2, 5]}: List containing negative numbers.

\noindent 4. \texttt{[1,2,3,4]}: List with no spaces between elements.

\noindent 5. \texttt{[ 1 , 2 , 3 , 4 ]}: List with spaces before and after the brackets.

\vspace{0.5em}

\noindent \textbf{Textual Lists (Medium Confidence)}
\vspace{0.2em}

\noindent 6. \texttt{"The sorted list is: 3, 5, 9, 12"}: Standard textual list with commas and spaces.

\noindent 7. \texttt{"The sorted list is: 3,5,9,12"}: Textual list with commas but no spaces.

\noindent 8. \texttt{"The Sorted List Is: -1, 0, 4, 7"}: Case-insensitive matching of the key phrase.

\noindent 9. \texttt{"The sorted list is:3, 5, 9"}: Handles missing space immediately after the colon.

\vspace{0.5em}

\noindent \textbf{Numbered Lists (Medium Confidence)}
\vspace{0.2em}

\noindent 10. \texttt{1. 9\\2. 13\\3. 29}: Standard numbered list format.  (Note: Represented with line breaks using \texttt{\textbackslash\textbackslash})

\noindent 11. \texttt{1.  9\\2.  13\\3. 29}: Numbered list with extra spaces after the numbering. (Note: Represented with line breaks).

\noindent 12. \texttt{1.9\\2.13\\3.29}: Numbered List with no space. (Less robust, covered by Fallback) (Note: Represented with line breaks).
\vspace{0.5em}

\noindent \textbf{Fallback (Lowest Confidence)}
\vspace{0.2em}

\noindent 13. If none of the above formats are found, the script extracts all numbers present in the response after removing potential numbered list prefixes. Example: If the response is \texttt{"The initial list was 5, 1, 4. The sorted version, however, is 1, 4, 5."}, this fallback would extract \texttt{[5, 1, 4, 1, 4, 5]}.


\input{sections/appendix/Tables/sorting_regex}


\newpage
% ~\newpage


\section{Further Analysis/Insights}
\subsection{Why explicit chain-of-thought does not Elicit Reasoning?}
\label{app: Why explicit chain-of-thought does not Elicit Reasoning?}
We observed that COT prompting does not significantly improve the reasoning performance of SLMs. Recent models often generate intermediate reasoning steps even when presented with direct questions. For instance, models frequently preface their responses with phrases like, \textit{"Let’s break this down into steps"} or \textit{"Here’s how we solve this step-by-step."} This behavior implies that CoT-like reasoning processes are already internalized during pretraining, reducing the explicit need for CoT prompting.\medskip

While the original CoT paper demonstrated clear benefits, the marginal gains observed here likely reflect advancements in training corpora and model architectures, which incorporate reasoning capabilities intrinsically. These findings suggest that for modern SLMs, CoT prompting may offer diminishing returns, as they already employ such techniques implicitly during reasoning tasks. These response demonstrates how CoT-style reasoning emerges organically, even without explicit prompting.

\input{sections/appendix/Figures/D1-COT}

\newpage
~\newpage
~\newpage
~\newpage

\subsection{Poor Performance of Pruned Models}
\label{app: Poor Performance of Pruned Models}
Pruned models exhibit significant performance degradation on reasoning-intensive datasets like ARC-E, ARC-C, and CommonsenseQA. Their responses often lack coherence and logic, with outputs like "Let me know in the comments" or "Sure, I can solve this problem." This was mostly evident in multi-shots prompting in Pruned Model. We hypothesize that pruning disrupts the internal representations responsible for reasoning, particularly in tasks requiring complex thought processes.\medskip

Additionally, many pruned models are fine-tuned on datasets like GSM8K to recover lost accuracy, which may limit their generalization to other domains. This recovery strategy prioritizes mathematical reasoning tasks but does not address the broader reasoning challenges posed by datasets like ARC-C and CommonsenseQA. These results underscore the importance of using diverse recovery datasets during fine-tuning to preserve performance across varied tasks.


\input{sections/appendix/Figures/D2}
\newpage
~\newpage



\subsection{Pruned Models: No response or Nonsensical response}
A recurring issue with pruned models is their tendency to produce nonsensical or empty responses. Even for simple questions like "What is 2+2?" pruned models may respond with irrelevant statements or fail to provide an answer altogether. This indicates that pruning can disrupt key reasoning pathways, rendering models unreliable in generating meaningful outputs.\medskip

The instability of pruned models emphasizes the need for post-pruning fine-tuning strategies that prioritize reasoning tasks. By using more diverse and challenging datasets, it may be possible to mitigate these issues and improve the robustness of pruned models.


\input{sections/appendix/Figures/D3}
\newpage
~\newpage


\subsection{Instructions following capability of SLMs}
SLMs exhibit variability in their ability to follow instructions, particularly when the instructions are complex or lengthy. When instructions are presented before the question, models often fail to comply, whereas placing the instructions after the question improves adherence in most cases. However, this improvement is inconsistent, especially in smaller models.\medskip

Interestingly, when models are given an excessive number of instructions, they occasionally solve previously unsolved problems, possibly due to triggering alternative reasoning pathways. Conversely, this can also lead to more mistakes. These observations highlight the importance of designing clear and concise instruction templates that align with the model’s processing capabilities to improve performance and reliability.

We did a small experiment to see how good an SLM is in following instructions. As shown in Table \ref{tab:instruction_following}, we can see that although large models are good at following instructions, sometimes they provide unnecessary extra information than asked for.

\begin{table}
\centering
\scriptsize
\begin{tabular}{lcc}
\toprule
 & \textbf{Llama-3.1 (8B)} & \textbf{SmolLM2 (1.7B)} \\
\midrule
Total Responses          & 100          & 100          \\
Correct Answers          & 18           & 5            \\
Incorrect Answers        & 82           & 95           \\
Instruction Not Followed & 54           & 23           \\
\begin{tabular}[c]{@{}l@{}}Unable to Follow Instruction\\ but Correct\end{tabular} & 17 & 3 \\
\begin{tabular}[c]{@{}l@{}}Unable to Follow Instruction\\ and Incorrect\end{tabular} & 37 & 20 \\
Accuracy                 & 18.00\%      & 5.00\%        \\
\bottomrule
\end{tabular}
\caption{Instruction Following Capability of SLMs}
\label{tab:instruction_following}
\end{table}

\subsection{Sorting Incorrect Examples}
\label{app: Sorting incorrect examples}
In this section, we present cases where the models occasionally include extra numbers, exclude existing numbers, output the exact same list, or produce the correct sorting but with different numbers.


\input{sections/appendix/Figures/D5}
\newpage
~\newpage

%##################################################################################################

\subsection{LLM-as-a-judge: TPR and TNR}
\label{app: LLM-as-a-judge: TPR and TNR}
In Table \ref{app: tpr-tnr}, we present the True Positive Rate (TPR) and True Negative Rate (TNR) for our LLM-as-a-judge evaluation. TPR measures the proportion of cases where both the GPT model and human judge agree on a correct decision. TNR reflects the cases where both agree on a correct rejection of an incorrect decision. These rates help better evaluate the model's accuracy in aligning with human judgment, considering both when the LLM correctly matches human decisions and when it diverges.
\input{sections/appendix/Tables/gpt-tpr-tnr}
\newpage
~\newpage
%##################################################################################################

\subsection{Sorting Incorrect Evaluations}
\label{app: Sorting Incorrect Evaluations}
Here, we provide instances where we were unable to accurately parse the sorted lists produced by the models, resulting in incorrect evaluations.

\input{sections/appendix/Figures/D6}

\newpage
~\newpage

%##################################################################################################

\subsection{GPT-4 Incorrect Evaluations}
\label{app: GPT-4 Incorrect Evaluations}
In this section, we present instances where GPT-4's evaluations were incorrect. For example, in one case, the ground truth was "stand in line," but the model chose option B, "get in line," and GPT-4 incorrectly labeled the evaluation as correct instead of incorrect.\medskip

Additionally, most incorrect evaluations by GPT-4 were observed when model responses were nonsensical or excessively long. In these cases, GPT-4 sometimes classified incorrect answers as correct, likely due to the presence of partially correct reasoning in the responses.\medskip

To mitigate this, we parsed the model's responses before sending them to GPT-4 for evaluation, especially when the responses were lengthy.

\input{sections/appendix/Figures/D7}
\newpage
~\newpage

%##################################################################################################

% \subsection{Problem with open-sourced Pruned Models}
% Open-sourced pruned models, such as Sheared Llama, exhibit poor performance even on basic tasks like answering "What is 2+2?". This suggests that aggressive pruning techniques can significantly impair a model's reasoning and language processing capabilities.\medskip

% The failure of these models emphasizes the importance of balancing model compression with the preservation of architectural integrity. Pruning should be paired with strategies like fine-tuning on diverse, reasoning-intensive datasets to maintain essential capabilities. Without such measures, pruned models may struggle with even simple tasks, let alone more complex reasoning challenges.

% \newpage
% ~\newpage

%##################################################################################################

\section{Model Compression Techniques Explained}

Deep learning models, particularly large-scale language models, require significant computational resources. To make these models more efficient, various model compression techniques are used, including \textit{quantization}, \textit{pruning}, and \textit{distillation}. These methods reduce model size and computational requirements while attempting to preserve accuracy. This section provides an in-depth explanation of these techniques.

\subsection{Quantization}

Quantization reduces the precision of model parameters (weights and activations) to lower-bit representations, thereby reducing memory footprint and accelerating inference. Instead of using full-precision floating-point numbers (e.g., FP32), quantization represents values using lower-bit formats such as INT8, INT4, or FP8.\medskip

Mathematically, given a full-precision weight matrix $\mathbf{W} \in \mathbb{R}^{m \times n}$, quantization maps each weight $w \in \mathbf{W}$ to a lower-precision representation $\hat{w}$:
\begin{equation}
    \hat{w} = S \cdot \text{round} \left(\frac{w}{S}\right),
\end{equation}
where $S$ is a scaling factor that determines how real-valued weights are mapped to discrete levels.

There are various quantization schemes used in our experiments:

\begin{table*}[ht]
    \centering

    \begin{tabular}{ccc}
\toprule
        \textbf{Scheme} & \textbf{Weight Precision} & \textbf{Activation Precision} \\
\midrule
        W8A8  & 8-bit weights & 8-bit activations \\
        W8A16 & 8-bit weights & 16-bit activations \\
        W4A16 & 4-bit weights & 16-bit activations \\
        FP8 & 8-bit floating point weights & 8-bit floating point activations \\
        FP8-Dynamic & 8-bit floating point (dynamic scaling) & 8-bit floating point activations \\
        GPTQ 8-bit & 8-bit quantization using GPTQ (post-training) & - \\
        GPTQ 4-bit & 4-bit quantization using GPTQ (post-training) & - \\
\bottomrule
    \end{tabular}
        \caption{Quantization Schemes referred in this paper.}
    \label{tab:quantization}
\end{table*}

Here, FP8-Dynamic refers to an adaptive floating-point scheme where scaling factors change dynamically based on tensor statistics. GPTQ (Generalized Post-Training Quantization) applies quantization-aware optimization post-training to minimize performance loss.

\subsection{Pruning}

Pruning eliminates less significant parameters from the model to reduce its size while preserving essential computations. Formally, given a weight matrix $\mathbf{W}$, pruning removes elements below a threshold $\tau$, setting them to zero:

\begin{equation}
    \hat{\mathbf{W}}_{i,j} =
    \begin{cases}
        \mathbf{W}_{i,j}, & \text{if } |\mathbf{W}_{i,j}| \geq \tau \\
        0, & \text{otherwise}
    \end{cases}
\end{equation}

Different types of pruning exist:

\begin{itemize}
    \item \textbf{Unstructured Pruning:} Individual weights below $\tau$ are removed.
    \item \textbf{Structured Pruning:} Entire rows, columns, or channels are removed, leading to more hardware-efficient reductions.
    \item \textbf{Magnitude Pruning:} Weights with the smallest magnitudes are pruned first.
    \item \textbf{Gradient-Based Pruning:} Pruning is based on the impact of removing a weight on the loss function.
\end{itemize}

Pruning helps in reducing memory usage and improving inference speed, especially for deployment on edge devices. However, aggressive pruning can degrade model accuracy, requiring fine-tuning to recover performance.

\subsection{Distillation}

Distillation compresses a large, pre-trained model (teacher) into a smaller model (student) by transferring knowledge. The student model learns not only from ground-truth labels but also from the teacher’s softened output probabilities.\medskip

Given a teacher model output $\mathbf{z}^T$ and a student model output $\mathbf{z}^S$, distillation minimizes the loss:

\begin{equation}
    \mathcal{L} = \alpha \mathcal{L}_{\text{CE}}(\mathbf{z}^S, y) + (1 - \alpha) \mathcal{L}_{\text{KD}}(\mathbf{z}^S, \mathbf{z}^T),
\end{equation}

where $\mathcal{L}_{\text{CE}}$ is the standard cross-entropy loss, $\mathcal{L}_{\text{KD}}$ is the knowledge distillation loss:

\begin{equation}
    \mathcal{L}_{\text{KD}} = \sum_i p^T_i \log p^S_i,
\end{equation}

and $p^T_i$, $p^S_i$ are the softened class probabilities from the teacher and student, respectively. The temperature parameter $T$ controls how much the logits are softened before computing probabilities:

\begin{equation}
    p_i = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)}.
\end{equation}

Distillation enables smaller models to achieve near-state-of-the-art performance with significantly fewer parameters, making them ideal for deployment in resource-constrained environments.


\section{Implementation Details}
We conducted all model inferences using NVIDIA H100-80GB, A100-80GB, L40-48GB, and A40-48GB GPUs. For efficient inference, we used the vLLM library \cite{kwon2023efficientmemorymanagementlarge} \footnote{\url{https://docs.vllm.ai/en/latest/}}, dynamically allocating the required number of GPUs to load each model. Multi-GPU utilization was enabled using Hugging Face Accelerate \footnote{\url{https://github.com/huggingface/accelerate}} for model sharding and speed optimization. We use the default hyperparameters settings mentioned in the huggingface repo for a fair comparison. We only adjusted the max input tokens (4096 for multi-shot prompts). All the computational requirements (GPU Memory Usage) reported are on NVIDIA-A100-80GB. All Quantized and Pruned models used are from Neural Magic \footnote{\url{https://huggingface.co/collections/neuralmagic/}} and Qwen \footnote{\url{https://huggingface.co/collections/Qwen/}} Hugging Face repository. All models are open-sourced and hosted on Hugging Face \footnote{\url{https://huggingface.co/models}}. In addition to GPT-4, we also use open-sourced framework lm-eval-harness \footnote{\url{https://github.com/EleutherAI/lm-evaluation-harness}} for the evaluation of SLMs. \medskip

% \section{Responsible NLP Checklist}

\section{Human Evaluation Details}

All human evaluations in this study were conducted by a computer science graduate student who has worked in the field of NLP and LLMs for more than three years. The evaluations were performed independently three times to ensure consistency and correctness. The authors then verified the results to minimize errors. No external annotators or crowdsourcing platforms were involved, and no compensation was provided. Due to the double-blind review process, further identifying details cannot be disclosed.

% \subsection{}



\section{Datasets Statistics}
\label{sec:datasets}

We evaluate our approach on seven benchmarks of different reasoning. In the following we briefly describe each dataset along with their splits. In this paper, we use test split to evaluate all SLMs. Table \ref{tab:dataset_splits} summarizes the splits for each datasets.


\begin{table}[ht]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Dataset}   & \textbf{Train} & \textbf{Validation} & \textbf{Test} \\
\midrule
GSM8K              & 7,473        & --                  & 1,319       \\
ARC-Easy           & 2,251        & 570                 & 2,376       \\
ARC-Challenge      & 1,119        & 299                 & 1,172       \\
CommonsenseQA      & 9,741        & 1,221               & 1,140       \\
HellaSwag          & 39,905       & 10,042              & 10,003      \\
MathQA             & 29,837       & 4,475               & 2,985       \\
OpenBookQA         & 4,957        & 500                 & 500         \\
MATH               & 10,000       & --                  & 5,000       \\
\bottomrule
\end{tabular}
\caption{Dataset splits and example counts. Note that GSM8K and MATH are provided with only training and test splits.}
\label{tab:dataset_splits}
\end{table}



\paragraph{GSM8K} \cite{cobbe2021trainingverifierssolvemath} is a collection of high-quality grade school math word problems that require multi-step reasoning. In the \texttt{main} configuration, the dataset contains a total of 8,790 examples, with 7,473 examples in the training split and 1,319 examples in the test split. 

\paragraph{ARC} \cite{clark2018thinksolvedquestionanswering} comprises two subsets of multiple-choice science questions:
\begin{itemize}
    \item \textbf{ARC-Easy}: Contains 2251 train, 570 validation, and 2376 test splits. 
    \item \textbf{ARC-Challenge}: Contains 1119 train, 299 validation, and 1172 test splits. 
\end{itemize}

\paragraph{CommonsenseQA} \cite{talmor-etal-2019-commonsenseqa} requires using commonsense reasoning to answer multiple-choice questions. It has 9,741 training examples, 1,221 validation examples, and 1,140 test examples.

\paragraph{HellaSwag} \cite{zellers2019hellaswag} is designed to evaluate commonsense inference by selecting the most plausible continuation of a given context. The default split comprises 39,905 for training, 10,042 for validation, and 10,003 for testing.

\paragraph{MathQA} \cite{amini2019mathqa} focuses on interpretable math word problem solving using operation-based formalism. The default split comprises 29,837 for training, 4475 for validation, and 2985 for testing.

\paragraph{OpenBookQA} \cite{mihaylov2018suitarmorconductelectricity} tests the ability to combine a small “open book” of core science facts with additional commonsense reasoning. The default split comprises 4957 for training, 500 for validation, and 500 for testing.

\paragraph{MATH} \cite{hendrycksmath2021} is a challenging dataset of competition-level math problems designed to measure advanced mathematical problem solving. It contains a total of 12,500 examples, split into 10,000 training, 5,000 test problems.




