\section*{Limitations}
\label{section:6}
In this work, we tried our best to ensure a rigorous and fair evaluation, but we acknowledge that some limitations should be considered when interpreting the results. First, our reliance on GPT-4 as an evaluator introduces potential biases and errors. While GPT-4 is a strong baseline for evaluation, it is not 100\% accurate and may misclassify responses, especially in edge cases (shown in \ref{app: GPT-4 Incorrect Evaluations}). We observed instances where models producing many nonsensical responses were sometimes marked as correct by GPT-4, leading to potential overestimations of performance. Although we tried to mitigate this issue by limiting token generation and applying pre-evaluation parsing, it was not feasible to manually supervise the entire evaluation process. 

Second, our sorting task evaluations relied on regex-based parsing to assess correctness. There could be cases where a modelâ€™s response was correct but misclassified due to parsing errors. Although we tried to account for most of the variations in model outputs, ensuring 100\% accuracy in automatic parsing remains a challenge. Additionally, our study focuses primarily on widely used benchmarks. However, reasoning abilities could be further assessed on more diverse datasets, including real-world problem-solving tasks and domain-specific reasoning benchmarks. Exploring these additional settings could provide a more comprehensive understanding of how different compression techniques impact model performance.
