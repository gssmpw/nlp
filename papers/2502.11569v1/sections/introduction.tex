\section{Introduction}

For a long time, reasoning in language models was considered an emergent property of large language models (LLMs), appearing at or above a certain scale ($\sim$100B parameters). Early studies \cite{wei2022emergentabilitieslargelanguage, chowdhery2022palmscalinglanguagemodeling, NEURIPS2020_1457c0d6} suggested that multi-step reasoning only emerges in models exceeding 100B parameters, as shown by models like GPT-4 \cite{achiam2023gpt} and Gemini \cite{geminiteam2024geminifamilyhighlycapable}. However, recent findings challenge this assumption. Phi-3.5-mini \cite{abdin2024phi}, with just 3.8B parameters, performs comparably to GPT-3.5, which suggests that reasoning ability can be achieved in small language models (SLMs) as well. 

A more recent breakthrough, DeepSeek-R1 \cite{deepseekai2025deepseekr1incentivizingreasoningcapability}, has shown impressive reasoning ability. While DeepSeek-R1 is a large model (671B), its reasoning abilities were distilled into smaller models (1.5Bâ€“70B parameters, Qwen Family \cite{qwen2025qwen25technicalreport}). This further challenges the assumption that reasoning ability only comes from scaling and raises an important question: \textit{Can SLMs also develop strong reasoning capabilities?} Before answering this, we need first to define what qualifies as an SLM. The definition of SLM varies widely, depending on model size, efficiency, and deployment constraints.


\begin{testexample}[Small Language Model]
\textit{In this work, we define SLMs as models significantly smaller than state-of-the-art LLMs, typically ranging from a few hundred million to tens of billions of parameters, or models that achieve similar computational efficiency through compression (e.g., quantization, pruning).}
\end{testexample}

There has been growing interest in SLMs due to their lower inference costs, reduced latency, and local deployment feasibility. Unlike LLMs that rely on cloud APIs for deployment, SLMs can be deployed locally \cite{wang2024comprehensive}, mitigating data exposure risks. However, their reasoning capabilities remain underexplored, particularly in compressed \cite{zhu2024survey} variants. For example, can a quantized LLaMA-70B outperform an 8B variant? This raises another question: \textit{Can SLMs retain reasoning ability after undergoing compression (e.g., Quantization)? And to what extent?} Prior research has lacked a detailed benchmarking effort that \textbf{quantifies} how different SLM strategies impact reasoning. In this work, we aim to fill this gap by systematically benchmarking SLMs' reasoning ability and providing clear guidance for researchers developing or deploying SLMs. 

\textbf{First}, we establish a \textbf{reliable evaluation metric} for assessing reasoning performance. Since reasoning is a generative task, defining an objective evaluation metric is non-trivial. Different methods often produce conflicting results compared to human evaluation, which makes it difficult to assess the model's actual reasoning ability. Manual evaluation is impractical, whereas rule-based evaluation expects the model to follow specific instructions \cite{huang2024largelanguagemodelsselfcorrect}. Sometimes, it can be unfair since we are testing the model's "reasoning," not "instructions following" ability. Studies \cite{wei2022finetunedlanguagemodelszeroshot} further show that this instruction following ability appears when scaled to $\sim$100B parameters. To determine the best evaluation framework, we systematically compare different parsing-based methods, LLM-as-a-Judge, and widely used benchmarks like lm-eval-harness to our human evaluation. Our results show that GPT-4-Turbo and GPT-4o align most closely with human judgment (98\% agreement), which we use as the main evaluation metric to benchmark SLM reasoning. 

\textbf{Second,} we conduct a \textbf{comprehensive evaluation} of \textbf{72} SLMs of \textbf{six} different families (such as Llama and Qwen), including their quantized, pruned, and distilled variants. We evaluate across \textbf{eight} widely used reasoning benchmarks: GSM8K, MATH, MathQA ARC-C, ARC-E, CommonsenseQA, OpenBookQA, Hellaswag and \textbf{six} sorting tasks: 8, 16, 32 numbers with only positive and mixed randomly generated numbers to ensure that performance reflects the model's actual reasoning ability rather than memorization. We observed that all models do not respond similarly to different prompting strategies. Recent findings \cite{plaat2024reasoning, qwen2025qwen25technicalreport, yang2024qwen2technicalreport} suggest that some language models internally generate step-by-step reasoning \cite{wei2022chain}, even when prompted directly. So, on GSM8K, we tested SLMs prompt sensitivity using \textbf{5} different prompting strategies: Direct I/O, COT, 5-Shot, COT 5-Shot, and 8-Shot. We conduct all experiments \textbf{three} times and report the standard deviation to ensure a robust evaluation of the model's performance. 

\textbf{Finally,} we test the \textbf{robustness of SLM reasoning} on \textbf{three} specialized benchmarks: \textbf{MR-Ben,} which evaluates the ability to locate and analyze potential errors in reasoning steps \cite{zeng2024mrbenmetareasoningbenchmarkevaluating}; \textbf{MR-GSM8K,} which evaluates intermediate reasoning ability \cite{zeng2024mrgsm8kmetareasoningbenchmarklarge}; and \textbf{GSM-Plus,} which measures resilience to adversarial perturbations \cite{li2024gsmpluscomprehensivebenchmarkevaluating}. To evaluate actual reasoning and not memorization, we select these datasets, released after the models' knowledge cut-off time, to ensure no prior exposure. Our results indicate that certain open-sourced SLMs like Qwen2.5-32B rival proprietary LLMs like GPT-4-Turbo in intermediate reasoning. This suggests that reasoning is not solely a function of scale but also structured training and optimization.


The remaining sections of this paper are structured as follows: Section \ref{section:2} reviews the recent work on SLMs' reasoning and evaluation methodologies. Section \ref{section:3} discusses our benchmarking setup, evaluation process, and reasoning tasks. Section \ref{section:4} presents experimental results and insights, analyzing reasoning performance and its robustness. Finally, Section \ref{section:5} concludes with key takeaways and directions for future research.
