
\section{Benchmarking Experiment Setup}
\label{section:3}
Unless stated otherwise, each experiment was repeated \textbf{three times}, and we reported the mean and standard deviation of model performance across all datasets to ensure the reliability of the results. Appendix \ref{app: Prompts and Scripts} details all parsing scripts and prompt templates, including those used for different prompting strategies and GPT-based evaluations.


\begin{table*}
% \xw{All table captions go to the top. All figure captions go to the bottom.}

\centering
\scriptsize
\begin{adjustbox}{width=\textwidth}
\begin{tabulary}{1.2\textwidth}{LCCCC|CCCC} % Adjusted width and column spec
\toprule
\multirow{2}{*}{\textbf{Metric}} & \multicolumn{4}{c}{\textbf{SmolLM2-1.7B-Instruct}} & \multicolumn{4}{c}{\textbf{Llama-3.1-8B-Instruct}} \\
\cmidrule(lr){2-5} \cmidrule(lr){6-9}
 & \textbf{(GSM8K)} & \textbf{(ARC-E)} & \textbf{(ARC-C)} & \textbf{(CommonsenseQA)} & \textbf{(GSM8K)} & \textbf{(ARC-E)} & \textbf{(ARC-C)} & \textbf{(CommonsenseQA)} \\
\midrule
Human Evaluation & 43 & 75 & 56 & 62 & 81 & 93 & 82 & 69 \\
lm-eval-harness & 18 & 70 & 37 & 50 & 22 & 82 & 51 & 76 \\
Parsing & 37 & 8 & 16 & 9 & 84 & 3 & 6 & 7 \\
Direct Answer & 5 & 58 & 49 & 42 & 18 & 93 & 82 & 77 \\
% \midrule
% \multicolumn{9}{l}{\textbf{LLM-as-a-judge (Human Agreement)}} \\
\midrule

% \multicolumn{9}{|>{\columncolor[gray]{.8}}l|}{\textbf{LLM-as-a-judge $\vert$ Correctness (Human Agreement)}} \\ 

\multicolumn{9}{l}{\textbf{LLM-as-a-judge}} \\ 


\midrule
gpt-3.5-turbo & 49 (94) & 75 (100) & 55 (99) & 62 (100) & 83 (98) & 91 (98) & 81 (99) & 66 (97) \\
gpt-4-turbo & 42 (99) & 75 (100) & 56 (100) & 61 (99) & 81 (100) & 93 (100) & 82 (100) & 69 (100) \\
gpt-4o & 41 (98) & 75 (100) & 56 (100) & 63 (97) & 81 (100) & 93 (100) & 82 (100) & 70 (99) \\
gpt-4o-mini & 41 (98) & 75 (100) & 55 (99) & 61 (99) & 80 (99) & 93 (100) & 76 (94) & 69 (100) \\

\bottomrule
\end{tabulary}
\end{adjustbox}
\caption{Comparison of Human Evaluation with different evaluation metrics and LLM-as-a-judge on \textbf{100} randomly sampled data points across four datasets with two models. Also, includes a comparison of four different GPTs as judges. \textbf{Scores are reported as [<Accuracy Score> (Human Agreement \%)].} Closer to Human Evaluation is better.}
\label{tab:evaluator}
\end{table*}


\subsection{Evaluation Process}
Our first step was to select a reliable assessment method. Instead of using standard parsing techniques to compare model responses with ground truth, we opted for LLM-as-a-Judge, using GPT-4 as the primary evaluator for most tasks.

\paragraph{Parsing Issues} Standard parsing techniques rely on fixed patterns, which can be challenging for generative models to follow consistently. We observed that smaller models, in particular, struggle to follow strict output formats. This leads to cases where a model provides a correct answer but is penalized for deviating from the expected structure. Prior work \cite{wei2022chain} also shows that instruction-following capabilities improve with model scale ($\sim$100B), making parsing an unfair metric for smaller models. 


To establish a more reliable evaluation metric, we conducted three rounds of human evaluation on 100 randomly sampled data points from the GSM8K, ARC-E, ARC-C, and CommonsenseQA datasets. Table \ref{tab:evaluator} compares evaluation methods, including standard parsing, the widely used lm-evaluation-harness framework, and GPT-based evaluation (LLM-as-a-judge).

\paragraph{Choosing the Best Judge} To select the most reliable judge, we evaluated GPT models based on two factors: \textbf{1) Reliability (Correctness):} How closely does the judge’s evaluation align with human assessments? \textbf{2) Human Agreement:} How often does the judge agree with human evaluators?



Table \ref{tab:evaluator} shows that GPT-4-Turbo provides the closest match to human evaluation, with GPT-4o performing nearly as well (only one point lower). Given its comparable accuracy and 50\% lower cost, we selected GPT-4o as our primary evaluator for ARC-Easy, ARC-Challenge, and CommonsenseQA. For GSM8K, we opted for GPT-4-Turbo due to its slightly higher reliability in mathematical reasoning tasks.

\paragraph{Task-Specific Evaluation Methods} For sorting tasks, standard LLM-based evaluation was unsuitable due to the need for precise numerical ordering. Instead, we used a robust regex-based parsing approach, identifying 13 common response patterns (more details in Appendix \ref{app: Sorting Parsing Script: 13 Variations}) to extract and validate the sorted lists against the ground truth. Unlike prior work \cite{Besta_2024}, we did not apply for partial credit. Our evaluation was strictly based on whether the model returned the correct final list.

For MR-Ben (identifying errors in reasoning) and MR-GSM8K (intermediate reasoning evaluation), we used the provided evaluation script with GPT-4o as the judge. Appendix \ref{app: LLM-as-a-judge: TPR and TNR} includes more details on \text{Judge}(s) reliability (TPR and TNR).


\subsection{Reasoning Tasks}
\label{main: Reasoning Tasks}

\paragraph{Task 1 - Math Reasoning} We evaluated mathematical reasoning using GSM8K \cite{cobbe2021trainingverifierssolvemath}, an arithmetic and word problems benchmark. We also evaluate on MathQA \cite{amini2019mathqa} and MATH \cite{hendrycksmath2021} dataset using lm-eval-harness (Results in Appendix \ref{app: Results with lm-eval-harness}).

Models were tested under five prompting strategies: Direct I/O, Chain-of-Thought (CoT), 5-shot, 5-shot CoT, and 8-shot. 

\paragraph{Task 2 - Science Reasoning} We used ARC-Easy and ARC-Challenge \cite{clark2018thinksolvedquestionanswering} for science reasoning, which includes multiple-choice questions requiring logical deduction. Unlike GSM8K, where CoT and multi-shot prompting are effective, these tasks rely more on factual knowledge retrieval. Therefore, we used direct I/O prompting for consistency in science reasoning.

\paragraph{Task 3 - Commonsense Reasoning} We assessed commonsense reasoning using CommonsenseQA \cite{talmor-etal-2019-commonsenseqa}, which tests everyday knowledge and inference. Similar to science reasoning, we use direct I/O prompting for consistency. We also evaluate on OpenBookQA \cite{OpenBookQA2018} and Hellaswag \cite{zellers2019hellaswag} dataset using lm-eval-harness (Results in Appendix \ref{app: Results with lm-eval-harness})

\paragraph{Task 4 - Sorting Numbers} We designed a custom dataset (randomly generated) to evaluate logical reasoning in structured numerical tasks. The task was divided into two categories: sorting positive integers and sorting mixed integers (positive and negative). We use positive numbers in the range [1, 100] and mixed numbers in the range [-100, 100], testing lists of length 8, 16, and 32. Ground truth labels were generated using merge sort algorithm. 
This task measures the models’ logical reasoning abilities and capability to handle sequential numerical data. Unlike datasets such as GSM8K, ARC-E, and ARC-C, which may have been seen during pre-training, the sorting task consists of randomly generated numbers. This ensures that performance reflects a model’s reasoning ability rather than memorization. Direct I/O prompts were used, with responses evaluated using our regex-based parsing.

\paragraph{Task 5 - Robustness} To test the SLMs' reasoning robustness, we used three benchmarks (as below) that were published after June 2024, ensuring that models trained earlier had no exposure to them. \textbf{1) MR-Ben} evaluates the model's ability to locate and analyze potential errors in reasoning steps. \textbf{2) MR-GSM8K} assesses step-by-step intermediate reasoning. \textbf{3) GSM-Plus} introduces adversarially perturbed inputs to test resilience.
