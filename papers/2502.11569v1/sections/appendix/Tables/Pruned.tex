\begin{table*}

\centering
\scriptsize
\begin{adjustbox}{width=\textwidth}
\begin{tabulary}{\textwidth}{LCC CC CC CC C}
\toprule
\multicolumn{3}{c}{\multirow{3}{*}{\textbf{Models}}} & \multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}\textbf{GPU}\\ \textbf{(GB)}\end{tabular}} & \multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}\textbf{Disk}\\ \textbf{(GB)}\end{tabular}} &  \multirow{3}{*}{\textbf{Direct I/O}} & \multirow{3}{*}{\textbf{COT}} & \multirow{3}{*}{\textbf{5-shot}}  & \multirow{3}{*}{\textbf{5-shot COT}}  & \multirow{3}{*}{\textbf{8-shot}}  \\
\\
\\
\cmidrule(lr){1-3}
\textbf{Model} & \textbf{Param.} & \textbf{Pruning type, Method} &  &  & & & & &  \\
\cmidrule{1-10}
\multicolumn{10}{|>{\columncolor[gray]{.8}}c|}{\textbf{No Knowledge Distillation}} \\
\cmidrule{1-10}

\textbf{phi-2} & - & SparseGPT & -- & -- &    36.52\%$_{\pm0.73\%}$ &    40.38\%$_{\pm0.59\%}$ &    30.22\%$_{\pm0.59\%}$ &    32.47\%$_{\pm0.99\%}$ &    22.21\%$_{\pm0.77\%}$ \\
\textbf{TinyLlama} & \textbf{1.1B} & SparseGPT & 4.2 & -- & 0.48\%$_{\pm0.19\%}$ & 0.51\%$_{\pm0.07\%}$ & 1.01\%$_{\pm0.29\%}$ & 1.09\%$_{\pm0.22\%}$ & 1.09\%$_{\pm0.38\%}$ \\

\cmidrule{1-10}
\multicolumn{10}{|>{\columncolor[gray]{.8}}c|}{\textbf{Retrained by Cerebras with 50B tokens from SlimPajama}} \\
\cmidrule{1-10}

\multirow{1}{*}{\textbf{Llama-2}} & \multirow{1}{*}{\textbf{7B}} & SparseGPT (70\%) & -- & -- & 2.38\%$_{\pm0.25\%}$ & 4.60\%$_{\pm0.28\%}$ & 4.14\%$_{\pm0.25\%}$ & 3.18\%$_{\pm0.12\%}$ & 4.37\%$_{\pm0.35\%}$ \\

\cmidrule{1-10}
\multicolumn{10}{|>{\columncolor[gray]{.8}}c|}{\textbf{Knowledge Distillation for 13B tokens using SquareHead Approach}} \\
\cmidrule{1-10}

\multirow{1}{*}{\textbf{Llama-3.1}} & \multirow{1}{*}{\textbf{8B}} & 2of4 Sparsity, SparseGPT & 30.65 & 15 & 51.86\%$_{\pm0.34\%}$ & 60.27\%$_{\pm0.66\%}$ & 9.68\%$_{\pm0.31\%}$ & 1.95\%$_{\pm0.36\%}$ & 8.77\%$_{\pm0.50\%}$ \\

\cmidrule{1-10}
\multicolumn{10}{|>{\columncolor[gray]{.8}}c|}{\textbf{Fine-tuned on GSM8K}} \\
\cmidrule{1-10}

\multirow{3}{*}{\textbf{Llama-2}} & \multirow{3}{*}{\textbf{7B}} & No Pruning & -- & 26 & 37.78\%$_{\pm0.93\%}$ & 34.34\%$_{\pm1.21\%}$ & 10.31\%$_{\pm0.89\%}$ & 10.84\%$_{\pm0.65\%}$ & 9.17\%$_{\pm0.45\%}$ \\
&  & SparseGPT (50\%) & -- & 13 & 39.85\%$_{\pm0.07\%}$ & 36.42\%$_{\pm0.50\%}$ & 24.01\%$_{\pm0.36\%}$ & 29.34\%$_{\pm0.90\%}$ & 24.51\%$_{\pm1.09\%}$ \\
& & SparseGPT (70\%) & -- & 13 &    38.41\%$_{\pm0.70\%}$ &    36.34\%$_{\pm0.29\%}$ & 34.27\%$_{\pm0.65\%}$ &    33.43\%$_{\pm0.92\%}$ &    35.03\%$_{\pm0.60\%}$ \\

\cmidrule{2-10}

\multirow{3}{*}{\textbf{Sparse-Llama-3.1}} & \multirow{3}{*}{\textbf{8B}} & pruned-2of4 & -- & 15 & 37.25\%$_{\pm0.39\%}$ & 61.97\%$_{\pm0.28\%}$ & 0.00\%$_{\pm0.00\%}$ & 0.00\%$_{\pm0.00\%}$ & 0.00\%$_{\pm0.00\%}$ \\
&  & pruned-2of4, INT4 Quant. & -- & 4.5 & 39.93\%$_{\pm0.36\%}$ & 60.58\%$_{\pm0.49\%}$ & 8.06\%$_{\pm0.46\%}$ & 8.09\%$_{\pm0.74\%}$ & 5.64\%$_{\pm0.60\%}$ \\
&  & pruned-2of4, FP8 Quant. & -- & 8.5 & 37.45\%$_{\pm0.65\%}$ & 61.36\%$_{\pm1.21\%}$ & 0.00\%$_{\pm0.00\%}$ & 0.00\%$_{\pm0.00\%}$ & 0.00\%$_{\pm0.00\%}$ \\


\cmidrule{1-10}
\multicolumn{10}{|>{\columncolor[gray]{.8}}c|}{\textbf{Mistral Fine-tuned}} \\
\cmidrule{1-10}

\textbf{OpenHermes-2.5} & \textbf{7B} & SparseGPT & 27.65 & 14 & 30.30\%$_{\pm0.94\%}$ & 40.79\%$_{\pm0.81\%}$ & 35.63\%$_{\pm0.94\%}$ & 36.04\%$_{\pm0.62\%}$ & 35.71\%$_{\pm1.07\%}$ \\


\bottomrule
\end{tabulary}
\end{adjustbox}
\caption{Performance and Resource Usage of Various Pruned and Distilled Models on GSM8K. The table reports the model size (in billions of parameters), compression type, GPU memory and disk space usage (in GB), and accuracy scores for each prompt type.}
\label{app: prun1}
\end{table*}
























\begin{table*}

\centering
\scriptsize
\begin{adjustbox}{width=\textwidth}
\begin{tabulary}{\textwidth}{LCC CCCCCC} % Adjusted for 6 data columns
\toprule
\multicolumn{3}{c}{\multirow{3}{*}{\textbf{Models}}} & \multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Sorting-8\\ (+ve)\end{tabular}}} & \multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Sorting-8\\ (mixed)\end{tabular}}} & \multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Sorting-16\\ (+ve)\end{tabular}}} & \multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Sorting-16\\ (mixed)\end{tabular}}} & \multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Sorting-32\\ (+ve)\end{tabular}}} & \multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Sorting-32\\ (mixed)\end{tabular}}} \\
\\
\\
\cmidrule(lr){1-3}

\textbf{Model} & \textbf{Param.} & \textbf{Optimization} &  &  &  &  &  &  \\
\cmidrule{1-9}
\multicolumn{9}{|>{\columncolor[gray]{.8}}c|}{\textbf{No Knowledge Distillation}} \\
\cmidrule{1-9}

\textbf{phi-2} & - & SparseGPT &	$19.67\%_{\pm4.51\%}$ &	$6.67\%_{\pm2.83\%}$ &	$0.67\%_{\pm0.92\%}$ &	$0.00\%_{\pm0.00\%}$ &	$0.00\%_{\pm0.00\%}$ &	$0.00\%_{\pm0.00\%}$ \\
\textbf{TinyLlama} & \textbf{1.1B} & SparseGPT & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ \\

\cmidrule{1-9}
\multicolumn{9}{|>{\columncolor[gray]{.8}}c|}{\textbf{Retrained by Cerebras with 50B tokens from SlimPajama}} \\
\cmidrule{1-9}

\multirow{1}{*}{\textbf{Llama-2}} & \multirow{1}{*}{\textbf{7B}} & SparseGPT (70\%) & $7.67\%_{\pm3.02\%}$ & $0.33\%_{\pm0.65\%}$ & $0.33\%_{\pm0.65\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ \\


\cmidrule{1-9}
\multicolumn{9}{|>{\columncolor[gray]{.8}}c|}{\textbf{Knowledge Distillation for 13B tokens using SquareHead Approach}} \\
\cmidrule{1-9}

\multirow{1}{*}{\textbf{Llama-3.1}} & \multirow{1}{*}{\textbf{8B}} & 2of4 Sparsity, SparseGPT & $47.33\%_{\pm5.66\%}$ & $15.33\%_{\pm4.08\%}$ & $20.00\%_{\pm4.53\%}$ & $9.33\%_{\pm3.30\%}$ & $1.67\%_{\pm1.45\%}$ & $0.00\%_{\pm0.00\%}$ \\

\cmidrule{1-9}
\multicolumn{9}{|>{\columncolor[gray]{.8}}c|}{\textbf{Fine-tuned on GSM8K}} \\
\cmidrule{1-9}

\multirow{3}{*}{\textbf{Llama-2}} & \multirow{3}{*}{\textbf{7B}} & No Pruning & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ \\
&  & SparseGPT (50\%) & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ \\
& & SparseGPT (70\%) &	$0.00\%_{\pm0.00\%}$ &	$0.00\%_{\pm0.00\%}$ &	$0.00\%_{\pm0.00\%}$ &	$0.00\%_{\pm0.00\%}$ &	$0.00\%_{\pm0.00\%}$ &	$0.00\%_{\pm0.00\%}$ \\

\cmidrule{2-9}

\multirow{3}{*}{\textbf{Sparse-Llama-3.1}} & \multirow{3}{*}{\textbf{8B}} & pruned-2of4 & $28.33\%_{\pm5.11\%}$ & $17.67\%_{\pm4.32\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ \\
&  & pruned-2of4, INT4 Quant. & $50.67\%_{\pm5.67\%}$ & $23.33\%_{\pm4.79\%}$ & $0.00\%_{\pm0.00\%}$ & $1.67\%_{\pm1.45\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ \\
&  & pruned-2of4, FP8 Quant. & $32.67\%_{\pm5.32\%}$ & $16.33\%_{\pm4.19\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ \\


\cmidrule{1-9}
\multicolumn{9}{|>{\columncolor[gray]{.8}}c|}{\textbf{Mistral Fine-tuned}} \\
\cmidrule{1-9}

\textbf{OpenHermes-2.5} & \textbf{7B} & SparseGPT & $37.00\%_{\pm5.47\%}$ & $22.00\%_{\pm4.70\%}$ & $0.67\%_{\pm0.92\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ \\

\bottomrule
\end{tabulary}
\end{adjustbox}
\caption{Performance of Various Pruned and Distilled Models on ARC-E, ARC-C, CommonsenseQA, and Sorting Tasks. The table reports the model size (in billions of parameters), optimization type (if any), and accuracy scores for each benchmark.}
\label{app: prun2}
\end{table*}





















% \begin{table*}
% \centering
% \scriptsize
% \begin{adjustbox}{width=\textwidth}
% \begin{tabulary}{\textwidth}{LCC CC CC CC C}
% \toprule
% \multicolumn{3}{c}{\multirow{3}{*}{\textbf{Models}}} & \multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}\textbf{GPU}\\ \textbf{(GB)}\end{tabular}} & \multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}\textbf{Disk}\\ \textbf{(GB)}\end{tabular}} &  \multirow{3}{*}{\textbf{Direct I/O}} & \multirow{3}{*}{\textbf{COT}} & \multirow{3}{*}{\textbf{5-shot}}  & \multirow{3}{*}{\textbf{5-shot COT}}  & \multirow{3}{*}{\textbf{8-shot}}  \\
% \\
% \\
% \cmidrule(lr){1-3}
% \textbf{Model} & \textbf{Param.} & \textbf{Optimization} &  &  & & & & &  \\
% \midrule
% \multirow{4}{*}{Sparse-Llama-3.1} & \multirow{4}{*}{8B} & 2of4 & -- & -- & 51.86\%$_{\pm0.34\%}$ & 60.27\%$_{\pm0.66\%}$ & 9.68\%$_{\pm0.31\%}$ & 1.95\%$_{\pm0.36\%}$ & 8.77\%$_{\pm0.50\%}$ \\
%  &  & gsm8k-2of4 & -- & -- & 37.25\%$_{\pm0.39\%}$ & 61.97\%$_{\pm0.28\%}$ & 0.00\%$_{\pm0.00\%}$ & 0.00\%$_{\pm0.00\%}$ & 0.00\%$_{\pm0.00\%}$ \\
%  &  & gsm8k-2of4-quantized.w4a16 & -- & -- & 39.93\%$_{\pm0.36\%}$ & 60.58\%$_{\pm0.49\%}$ & 8.06\%$_{\pm0.46\%}$ & 8.09\%$_{\pm0.74\%}$ & 5.64\%$_{\pm0.60\%}$ \\
%  &  & gsm8k-2of4-FP8-dynamic & -- & -- & 37.45\%$_{\pm0.65\%}$ & 61.36\%$_{\pm1.21\%}$ & 0.00\%$_{\pm0.00\%}$ & 0.00\%$_{\pm0.00\%}$ & 0.00\%$_{\pm0.00\%}$ \\
% \cmidrule{2-10}
% \multirow{3}{*}{Llama-2} & \multirow{3}{*}{7b} & pruned70-retrained & -- & -- & 2.38\%$_{\pm0.25\%}$ & 4.60\%$_{\pm0.28\%}$ & 4.14\%$_{\pm0.25\%}$ & 3.18\%$_{\pm0.12\%}$ & 4.37\%$_{\pm0.35\%}$ \\
%  &  & gsm8k & -- & -- & 37.78\%$_{\pm0.93\%}$ & 34.34\%$_{\pm1.21\%}$ & 10.31\%$_{\pm0.89\%}$ & 10.84\%$_{\pm0.65\%}$ & 9.17\%$_{\pm0.45\%}$ \\
%  &  & gsm8k-pruned\_50 & -- & -- & 39.85\%$_{\pm0.07\%}$ & 36.42\%$_{\pm0.50\%}$ & 24.01\%$_{\pm0.36\%}$ & 29.34\%$_{\pm0.90\%}$ & 24.51\%$_{\pm1.09\%}$ \\
%   & & gsm8k-pruned\_70 & -- & -- &	38.41\%$_{\pm0.70\%}$ &	36.34\%$_{\pm0.29\%}$ & 34.27\%$_{\pm0.65\%}$ &	33.43\%$_{\pm0.92\%}$ &	35.03\%$_{\pm0.60\%}$ \\
% \cmidrule{2-10}
%  OpenHermes-2.5-Mistral & 7B & pruned2.4 & -- & -- & 30.30\%$_{\pm0.94\%}$ & 40.79\%$_{\pm0.81\%}$ & 35.63\%$_{\pm0.94\%}$ & 36.04\%$_{\pm0.62\%}$ & 35.71\%$_{\pm1.07\%}$ \\
%  \cmidrule{2-10}
% TinyLlama-1.1B-Chat-v1.0 & - & pruned2.4 & -- & -- & 0.48\%$_{\pm0.19\%}$ & 0.51\%$_{\pm0.07\%}$ & 1.01\%$_{\pm0.29\%}$ & 1.09\%$_{\pm0.22\%}$ & 1.09\%$_{\pm0.38\%}$ \\
%  \cmidrule{1-10}
% phi-2 & - & pruned50 & -- & -- &	36.52\%$_{\pm0.73\%}$ &	40.38\%$_{\pm0.59\%}$ &	30.22\%$_{\pm0.59\%}$ &	32.47\%$_{\pm0.99\%}$ &	22.21\%$_{\pm0.77\%}$ \\
% \bottomrule
% \end{tabulary}
% \end{adjustbox}
% \caption{Performance and Resource Usage of Various Pruned Models on GSM8K. The table reports the model size (in billions of parameters), compression type, GPU memory and disk space usage (in GB), and accuracy scores for each prompt type.}
% \label{app: prun1}
% \end{table*}

% Performance and Resource Usage of Various Pruned Models on GSM8K. The table reports the model size (in billions of parameters), compression type, GPU memory and disk space usage (in GB), and accuracy scores for each prompt type.







% \begin{table*}
% \centering
% \scriptsize
% \begin{adjustbox}{width=\textwidth}
% \begin{tabulary}{\textwidth}{LCC CCCCCC} % Adjusted for 6 data columns
% \toprule
% \multicolumn{3}{c}{\multirow{3}{*}{\textbf{Models}}} & \multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Sorting-8\\ (+ve)\end{tabular}}} & \multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Sorting-8\\ (mixed)\end{tabular}}} & \multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Sorting-16\\ (+ve)\end{tabular}}} & \multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Sorting-16\\ (mixed)\end{tabular}}} & \multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Sorting-32\\ (+ve)\end{tabular}}} & \multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Sorting-32\\ (mixed)\end{tabular}}} \\
% \\
% \\
% \cmidrule(lr){1-3}

% \textbf{Model} & \textbf{Param.} & \textbf{Optimization} &  &  &  &  &  &  \\
% \midrule
% \multirow{4}{*}{Sparse-Llama-3.1} & \multirow{4}{*}{8B} & 2of4 & $47.33\%_{\pm5.66\%}$ & $15.33\%_{\pm4.08\%}$ & $20.00\%_{\pm4.53\%}$ & $9.33\%_{\pm3.30\%}$ & $1.67\%_{\pm1.45\%}$ & $0.00\%_{\pm0.00\%}$ \\
%  &  & gsm8k-2of4 & $28.33\%_{\pm5.11\%}$ & $17.67\%_{\pm4.32\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ \\
%  &  & gsm8k-2of4-quantized.w4a16 & $50.67\%_{\pm5.67\%}$ & $23.33\%_{\pm4.79\%}$ & $0.00\%_{\pm0.00\%}$ & $1.67\%_{\pm1.45\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ \\
%  &  & gsm8k-2of4-FP8-dynamic & $32.67\%_{\pm5.32\%}$ & $16.33\%_{\pm4.19\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ \\
% \cmidrule{2-9}
% \multirow{3}{*}{Llama-2} & \multirow{3}{*}{7b} & pruned70-retrained & $7.67\%_{\pm3.02\%}$ & $0.33\%_{\pm0.65\%}$ & $0.33\%_{\pm0.65\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ \\
%  &  & gsm8k & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ \\
%  &  & gsm8k-pruned\_50 & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ \\
%   & & gsm8k-pruned\_70 &	$0.00\%_{\pm0.00\%}$ &	$0.00\%_{\pm0.00\%}$ &	$0.00\%_{\pm0.00\%}$ &	$0.00\%_{\pm0.00\%}$ &	$0.00\%_{\pm0.00\%}$ &	$0.00\%_{\pm0.00\%}$ \\
% \cmidrule{2-9}
%  OpenHermes-2.5-Mistral & 7B & pruned2.4 & $37.00\%_{\pm5.47\%}$ & $22.00\%_{\pm4.70\%}$ & $0.67\%_{\pm0.92\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ \\
% \cmidrule{2-9}
% TinyLlama-1.1B-Chat-v1.0 & - & pruned2.4 & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ & $0.00\%_{\pm0.00\%}$ \\
% \cmidrule{1-9}
% phi-2 & - & pruned50 &	$19.67\%_{\pm4.51\%}$ &	$6.67\%_{\pm2.83\%}$ &	$0.67\%_{\pm0.92\%}$ &	$0.00\%_{\pm0.00\%}$ &	$0.00\%_{\pm0.00\%}$ &	$0.00\%_{\pm0.00\%}$ \\

% \bottomrule
% \end{tabulary}
% \end{adjustbox}
% \caption{Performance of Various Pruned Models on ARC-E, ARC-C, CommonsenseQA, and Sorting Tasks. The table reports the model size (in billions of parameters), optimization type (if any), and accuracy scores for each benchmark.}
% \label{app: prun2}
% \end{table*}