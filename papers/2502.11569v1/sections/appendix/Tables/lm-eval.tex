\begin{table*}

\centering
\tiny
\begin{adjustbox}{width=\textwidth}
\begin{tabulary}{1.2\textwidth}{L CC CC CC CC CC} % Adjusted width and added columns.
\toprule
\multicolumn{3}{c}{\multirow{3}{*}{\textbf{Models}}} & \multicolumn{2}{c}{\textbf{gsm8k (exact\_match)}} & \multicolumn{2}{c}{\textbf{arc\_easy}} & \multicolumn{2}{c}{\textbf{arc\_challenge}} \\
\cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
 &  &  & \textbf{(strict-match)} & \textbf{(flexible-extract)} & \textbf{(acc)} & \textbf{(acc\_norm)} & \textbf{(acc)} & \textbf{(acc\_norm)} \\
 % &  & & \textbf{} &  &  &  &  &  \\
 \cmidrule(lr){1-3}
\textbf{Model} & \textbf{Param.} & \textbf{Quantization} &  &  &  &  &  &  \\


% \midrule
% \multicolumn{9}{|>{\columncolor[gray]{.8}}c|}{\textbf{Llama-3.2 Family (evaluator=lm-eval-harness)}} \\
% \midrule

\midrule
 \multirow{2}{*}{\textbf{HF/SmolLM2}} & \multirow{2}{*}{\textbf{1.7B}} & None (Base) & 29.87$_{\pm1.26}$ & 30.02$_{\pm1.26}$ & 77.78$_{\pm0.85}$ & 73.36$_{\pm0.91}$ & 44.11$_{\pm1.45}$ & 47.27$_{\pm1.46}$ \\
 &  & None (Instruct) & 0.30$_{\pm0.15}$ & 0.30$_{\pm0.15}$ & 68.98$_{\pm0.95}$ & 62.92$_{\pm0.99}$ & 38.31$_{\pm1.42}$ & 43.86$_{\pm1.45}$ \\
 \cline{3-9}
\multirow{2}{*}{\textbf{nvidia/Hymba}} & \multirow{2}{*}{\textbf{1.5B}} & None (Base) & 17.13$_{\pm1.04}$ & 17.59$_{\pm1.05}$ & 76.60$_{\pm0.87}$ & 77.15$_{\pm0.86}$ & 45.39$_{\pm1.45}$ & 49.91$_{\pm1.46}$ \\
 &  & None (Instruct) & 44.28$_{\pm1.37}$ & 47.31$_{\pm1.38}$ & 76.52$_{\pm0.87}$ & 76.01$_{\pm0.88}$ & 44.62$_{\pm1.45}$ & 49.06$_{\pm1.46}$ \\
 \cline{3-9}
\textbf{nvidia/Minitron} & \textbf{4B} & None (Base) & 24.11$_{\pm1.18}$ & 23.58$_{\pm1.17}$ & 75.93$_{\pm0.88}$ & 75.97$_{\pm0.88}$ & 39.76$_{\pm1.43}$ & 44.88$_{\pm1.45}$ \\
 \cline{3-9}
\multirow{3}{*}{\textbf{Qwen2.5 (Base)}} & \textbf{0.5B} & None & 34.72$_{\pm1.31}$ & 35.33$_{\pm1.32}$ & 64.65$_{\pm0.98}$ & 58.21$_{\pm1.01}$ & 29.27$_{\pm1.33}$ & 32.34$_{\pm1.37}$ \\
 & \textbf{1.5B} & None & 62.32$_{\pm1.33}$ & 62.62$_{\pm1.33}$ & 75.38$_{\pm0.88}$ & 71.63$_{\pm0.92}$ & 41.30$_{\pm1.44}$ & 44.97$_{\pm1.45}$ \\
 & \textbf{3B} & None & 70.74$_{\pm1.25}$ & 76.19$_{\pm1.17}$ & 77.36$_{\pm0.86}$ & 73.15$_{\pm0.91}$ & 44.54$_{\pm1.45}$ & 47.01$_{\pm1.46}$ \\
\cline{3-9}
\textbf{Llama-3.2 (Base)} & \textbf{1B} & None & 6.37$_{\pm0.67}$ & 6.60$_{\pm0.68}$ & 65.32$_{\pm0.98}$ & 60.61$_{\pm1.00}$ & 31.23$_{\pm1.35}$ & 36.01$_{\pm1.40}$ \\



\midrule
\multirow{8}{*}{\textbf{Llama-3.2-Instruct}} & \multirow{4}{*}{\textbf{1B}} & None & 33.36$_{\pm1.30}$ & 33.36$_{\pm1.30}$ & 68.48$_{\pm0.95}$ & 63.34$_{\pm0.99}$ & 35.67$_{\pm1.40}$ & 38.05$_{\pm1.42}$ \\
 &  & w8a8 & 33.51$_{\pm1.30}$ & 33.59$_{\pm1.30}$ & 68.73$_{\pm0.95}$ & 63.30$_{\pm0.99}$ & 35.49$_{\pm1.40}$ & 37.80$_{\pm1.42}$ \\
&  & FP8 & 31.54$_{\pm1.28}$ & 30.86$_{\pm1.27}$ & 67.80$_{\pm0.96}$ & 62.50$_{\pm0.99}$ & 35.07$_{\pm1.39}$ & 37.88$_{\pm1.42}$ \\
 & & FP8-dynamic & 33.36$_{\pm1.30}$ & 33.21$_{\pm1.30}$ & 67.63$_{\pm0.96}$ & 62.96$_{\pm0.99}$ & 35.49$_{\pm1.40}$ & 38.23$_{\pm1.42}$ \\
 
 \cline{3-9}
 % \cmidrule{3-9}
 
 & \multirow{4}{*}{\textbf{3B}} & None & 64.97$_{\pm1.31}$ & 65.73$_{\pm1.31}$ & 73.91$_{\pm0.90}$ & 67.97$_{\pm0.96}$ & 43.77$_{\pm1.45}$ & 45.90$_{\pm1.46}$ \\
 &  & w8a8 & 64.06$_{\pm1.32}$ & 64.82$_{\pm1.32}$ & 73.86$_{\pm0.90}$ & 67.80$_{\pm0.96}$ & 43.34$_{\pm1.45}$ & 46.25$_{\pm1.46}$ \\
 &  & FP8 & 57.62$_{\pm1.36}$ & 57.54$_{\pm1.36}$ & 68.90$_{\pm0.95}$ & 63.01$_{\pm0.99}$ & 40.96$_{\pm1.44}$ & 43.34$_{\pm1.45}$ \\
 &  & FP8-dynamic & 62.85$_{\pm1.33}$ & 63.46$_{\pm1.33}$ & 73.48$_{\pm0.91}$ & 67.09$_{\pm0.96}$ & 42.49$_{\pm1.44}$ & 45.05$_{\pm1.45}$ \\
\midrule
% \midrule
% \multicolumn{9}{|>{\columncolor[gray]{.8}}c|}{\textbf{Llama-3.1 (Instruct) Family (evaluator=lm-eval-harness)}} \\
% \midrule
\multirow{12}{*}{\textbf{Llama-3.1}} & \multirow{6}{*}{\textbf{8B}} & None & 74.75$_{\pm1.20}$ & 77.26$_{\pm1.15}$ & 81.78$_{\pm0.79}$ & 79.50$_{\pm0.83}$ & 51.54$_{\pm1.46}$ & 55.46$_{\pm1.45}$ \\
 &  & w8a8 & 75.51$_{\pm1.18}$ & 77.94$_{\pm1.14}$ & 81.65$_{\pm0.79}$ & 79.63$_{\pm0.83}$ & 51.96$_{\pm1.46}$ & 55.12$_{\pm1.45}$ \\
 &  & w8a16 & 75.51$_{\pm1.18}$ & 77.79$_{\pm1.14}$ & 82.03$_{\pm0.79}$ & 79.88$_{\pm0.82}$ & 51.79$_{\pm1.46}$ & 55.20$_{\pm1.45}$ \\
 &  & w4a16 & 70.58$_{\pm1.26}$ & 70.58$_{\pm1.26}$ & 78.70$_{\pm0.84}$ & 75.88$_{\pm0.88}$ & 47.27$_{\pm1.46}$ & 51.71$_{\pm1.46}$ \\
 &  & FP8 & 71.57$_{\pm1.24}$ & 73.16$_{\pm1.22}$ & 80.72$_{\pm0.81}$ & 78.49$_{\pm0.84}$ & 51.37$_{\pm1.46}$ & 53.67$_{\pm1.46}$ \\
 &  & FP8-dynamic & 75.21$_{\pm1.19}$ & 77.56$_{\pm1.15}$ & 81.10$_{\pm0.80}$ & 80.13$_{\pm0.82}$ & 52.13$_{\pm1.46}$ & 54.44$_{\pm1.46}$ \\
\cline{3-9}
 & \multirow{6}{*}{\textbf{70B}} & Instruct & 88.32$_{\pm0.88}$ & 92.19$_{\pm0.74}$ & 86.78$_{\pm0.69}$ & 83.63$_{\pm0.76}$ & 62.46$_{\pm1.42}$ & 63.57$_{\pm1.41}$ \\
 &  & w8a8 & 88.32$_{\pm0.88}$ & 92.34$_{\pm0.73}$ & 86.57$_{\pm0.70}$ & 83.59$_{\pm0.76}$ & 62.37$_{\pm1.42}$ & 63.05$_{\pm1.41}$ \\
 &  & w8a16 & 87.49$_{\pm0.91}$ & 88.02$_{\pm0.89}$ & 80.81$_{\pm0.81}$ & 79.92$_{\pm0.82}$ & 52.65$_{\pm1.46}$ & 56.23$_{\pm1.45}$ \\
 &  & w4a16 & 89.23$_{\pm0.85}$ & 91.81$_{\pm0.76}$ & 86.49$_{\pm0.70}$ & 83.71$_{\pm0.76}$ & 61.60$_{\pm1.42}$ & 63.82$_{\pm1.40}$ \\
 &  & FP8 & 89.31$_{\pm0.85}$ & 90.60$_{\pm0.80}$ & 85.02$_{\pm0.73}$ & 83.21$_{\pm0.77}$ & 59.30$_{\pm1.44}$ & 61.86$_{\pm1.42}$ \\
 &  & FP8-dynamic & 88.17$_{\pm0.89}$ & 92.12$_{\pm0.74}$ & 86.41$_{\pm0.70}$ & 83.50$_{\pm0.76}$ & 62.20$_{\pm1.42}$ & 62.71$_{\pm1.41}$ \\
\midrule
%  \midrule
% \multicolumn{9}{|>{\columncolor[gray]{.8}}c|}{\textbf{Mistral-v0.3 and Nemo-2407 (Instruct) Family (evaluator=lm-eval-harness)}} \\
% \midrule
\multirow{4}{*}{\textbf{Mistral$_{\text{v0.3}}$}} & \multirow{4}{*}{\textbf{7B}} & None & 49.36$_{\pm1.38}$ & 49.66$_{\pm1.38}$ & 84.22$_{\pm0.75}$ & 82.66$_{\pm0.78}$ & 57.17$_{\pm1.45}$ & 58.36$_{\pm1.44}$ \\
 &  & w8a8 & 49.20$_{\pm1.38}$ & 49.43$_{\pm1.38}$ & 84.18$_{\pm0.75}$ & 82.83$_{\pm0.77}$ & 57.00$_{\pm1.45}$ & 58.36$_{\pm1.44}$ \\
 &  & w8a16 & 50.42$_{\pm1.38}$ & 50.49$_{\pm1.38}$ & 84.30$_{\pm0.75}$ & 82.70$_{\pm0.78}$ & 57.68$_{\pm1.44}$ & 59.04$_{\pm1.44}$ \\
 &  & w4a16 & 44.05$_{\pm1.37}$ & 44.12$_{\pm1.37}$ & 82.53$_{\pm0.78}$ & 81.44$_{\pm0.80}$ & 53.92$_{\pm1.46}$ & 56.91$_{\pm1.45}$ \\
\cline{3-9}
\multirow{2}{*}{\textbf{Mistral$_{\text{Nemo}}$}} & \multirow{2}{*}{\textbf{12B}} & None & 74.07$_{\pm1.21}$ & 75.82$_{\pm1.18}$ & 82.58$_{\pm0.78}$ & 80.01$_{\pm0.82}$ & 56.23$_{\pm1.45}$ & 58.87$_{\pm1.44}$ \\
 &  & w4a16 & 70.81$_{\pm1.25}$ & 72.63$_{\pm1.23}$ & 81.78$_{\pm0.79}$ & 79.00$_{\pm0.84}$ & 54.52$_{\pm1.46}$ & 58.11$_{\pm1.44}$ \\
% \midrule
% \multicolumn{9}{|>{\columncolor[gray]{.8}}c|}{\textbf{Qwen2 (Instruct) Family (evaluator=lm-eval-harness)}} \\
% \midrule
\midrule
\multirow{18}{*}{\textbf{Qwen$_{\text{2}}$}} & \multirow{6}{*}{\textbf{0.5B}} & None & 33.13$_{\pm1.30}$ & 33.74$_{\pm1.30}$ & 58.63$_{\pm1.01}$ & 54.88$_{\pm1.02}$ & 26.62$_{\pm1.29}$ & 29.86$_{\pm1.34}$ \\
 &  & GPTQ-Int8 & 32.68$_{\pm1.29}$ & 33.36$_{\pm1.30}$ & 58.25$_{\pm1.01}$ & 54.88$_{\pm1.02}$ & 26.19$_{\pm1.28}$ & 30.12$_{\pm1.34}$ \\
 &  & GPTQ-Int4 & 18.12$_{\pm1.06}$ & 21.46$_{\pm1.13}$ & 57.41$_{\pm1.01}$ & 55.18$_{\pm1.02}$ & 26.71$_{\pm1.29}$ & 29.44$_{\pm1.33}$ \\
 &  & w8a16 & 37.98$_{\pm1.34}$ & 38.21$_{\pm1.34}$ & 58.46$_{\pm1.01}$ & 54.92$_{\pm1.02}$ & 26.28$_{\pm1.29}$ & 30.29$_{\pm1.34}$ \\
 &  & w8a8 & 32.45$_{\pm1.29}$ & 33.28$_{\pm1.30}$ & 58.71$_{\pm1.01}$ & 54.84$_{\pm1.02}$ & 27.30$_{\pm1.30}$ & 30.55$_{\pm1.35}$ \\
 &  & w4a16 & 27.75$_{\pm1.23}$ & 28.81$_{\pm1.25}$ & 52.86$_{\pm1.02}$ & 48.86$_{\pm1.03}$ & 26.71$_{\pm1.29}$ & 27.82$_{\pm1.31}$ \\
\cline{3-9}
 & \multirow{6}{*}{\textbf{1.5B}} & None & 54.21$_{\pm1.37}$ & 55.19$_{\pm1.37}$ & 69.91$_{\pm0.94}$ & 66.96$_{\pm0.97}$ & 37.20$_{\pm1.41}$ & 40.10$_{\pm1.43}$ \\
 &  & GPTQ-Int8 & 54.89$_{\pm1.37}$ & 55.95$_{\pm1.37}$ & 69.74$_{\pm0.94}$ & 66.75$_{\pm0.97}$ & 37.12$_{\pm1.41}$ & 40.02$_{\pm1.43}$ \\
 &  & GPTQ-Int4 & 49.73$_{\pm1.38}$ & 50.80$_{\pm1.38}$ & 68.60$_{\pm0.95}$ & 65.11$_{\pm0.98}$ & 35.84$_{\pm1.40}$ & 38.99$_{\pm1.43}$ \\
 &  & w8a16 & 57.85$_{\pm1.36}$ & 58.45$_{\pm1.36}$ & 69.65$_{\pm0.94}$ & 66.58$_{\pm0.97}$ & 37.12$_{\pm1.41}$ & 39.85$_{\pm1.43}$ \\
 &  & w8a8 & 54.28$_{\pm1.37}$ & 55.04$_{\pm1.37}$ & 69.40$_{\pm0.95}$ & 66.41$_{\pm0.97}$ & 37.20$_{\pm1.41}$ & 39.85$_{\pm1.43}$ \\
 &  & w4a16 & 54.66$_{\pm1.37}$ & 54.89$_{\pm1.37}$ & 68.27$_{\pm0.96}$ & 64.98$_{\pm0.98}$ & 34.56$_{\pm1.39}$ & 38.05$_{\pm1.42}$ \\
\cline{3-9}
 & \multirow{6}{*}{\textbf{7B}} & None & 63.53$_{\pm1.33}$ & 72.33$_{\pm1.23}$ & 80.22$_{\pm0.82}$ & 76.47$_{\pm0.87}$ & 50.94$_{\pm1.46}$ & 54.01$_{\pm1.46}$ \\
 &  & GPTQ-Int8 & 63.46$_{\pm1.33}$ & 73.46$_{\pm1.22}$ & 80.35$_{\pm0.82}$ & 76.35$_{\pm0.87}$ & 51.11$_{\pm1.46}$ & 54.35$_{\pm1.46}$ \\
 &  & GPTQ-Int4 & 57.85$_{\pm1.36}$ & 70.05$_{\pm1.26}$ & 80.68$_{\pm0.81}$ & 77.19$_{\pm0.86}$ & 51.62$_{\pm1.46}$ & 54.69$_{\pm1.45}$ \\
 &  & w8a16 & 68.39$_{\pm1.28}$ & 75.74$_{\pm1.18}$ & 80.43$_{\pm0.81}$ & 76.18$_{\pm0.87}$ & 51.02$_{\pm1.46}$ & 54.01$_{\pm1.46}$ \\
 &  & w8a8 & 64.29$_{\pm1.32}$ & 73.62$_{\pm1.21}$ & 80.18$_{\pm0.82}$ & 76.52$_{\pm0.87}$ & 50.60$_{\pm1.46}$ & 54.27$_{\pm1.46}$ \\
 &  & w4a16 & 66.19$_{\pm1.30}$ & 74.75$_{\pm1.20}$ & 79.76$_{\pm0.82}$ & 75.88$_{\pm0.88}$ & 52.05$_{\pm1.46}$ & 54.35$_{\pm1.46}$ \\

% \multicolumn{9}{|>{\columncolor[gray]{.8}}c|}{\textbf{Qwen2.5 (Instruct) Family (evaluator=lm-eval-harness)}} \\

\midrule
\multirow{18}{*}{\textbf{Qwen2.5}} & \multirow{3}{*}{\textbf{0.5B}} & None & 21.68$_{\pm1.14}$ & 32.75$_{\pm1.29}$ & 65.53$_{\pm0.98}$ & 58.84$_{\pm1.01}$ & 30.29$_{\pm1.34}$ & 33.28$_{\pm1.38}$ \\
 &  & GPTQ-Int8 & 19.03$_{\pm1.08}$ & 31.84$_{\pm1.28}$ & 65.91$_{\pm0.97}$ & 59.72$_{\pm1.01}$ & 30.72$_{\pm1.35}$ & 34.22$_{\pm1.39}$ \\
 &  & GPTQ-Int4 & 2.27$_{\pm0.41}$ & 17.59$_{\pm1.05}$ & 61.49$_{\pm1.00}$ & 61.70$_{\pm1.00}$ & 28.33$_{\pm1.32}$ & 30.97$_{\pm1.35}$ \\
\cline{3-9}
 & \multirow{3}{*}{\textbf{1.5B}} & None & 30.48$_{\pm1.27}$ & 50.87$_{\pm1.38}$ & 76.60$_{\pm0.87}$ & 76.01$_{\pm0.88}$ & 43.86$_{\pm1.45}$ & 46.84$_{\pm1.46}$ \\
 &  & GPTQ-Int8 & 31.16$_{\pm1.28}$ & 51.93$_{\pm1.38}$ & 76.81$_{\pm0.87}$ & 75.97$_{\pm0.88}$ & 43.34$_{\pm1.45}$ & 46.16$_{\pm1.46}$ \\
 &  & GPTQ-Int4 & 32.75$_{\pm1.29}$ & 49.05$_{\pm1.38}$ & 75.93$_{\pm0.88}$ & 75.67$_{\pm0.88}$ & 42.58$_{\pm1.44}$ & 45.39$_{\pm1.45}$ \\
 \cline{3-9}
& \multirow{3}{*}{\textbf{3B}} & None & 10.99$_{\pm0.86}$ & 63.68$_{\pm1.32}$ & 77.06$_{\pm0.86}$ & 72.94$_{\pm0.91}$ & 45.73$_{\pm1.46}$ & 48.04$_{\pm1.46}$ \\
 &  & GPTQ-Int8 & 10.08$_{\pm0.83}$ & 64.44$_{\pm1.32}$ & 77.10$_{\pm0.86}$ & 73.15$_{\pm0.91}$ & 46.16$_{\pm1.46}$ & 48.12$_{\pm1.46}$ \\
 &  & GPTQ-Int4 & 10.24$_{\pm0.83}$ & 59.44$_{\pm1.35}$ & 78.07$_{\pm0.85}$ & 74.16$_{\pm0.90}$ & 46.25$_{\pm1.46}$ & 49.49$_{\pm1.46}$ \\
 \cline{3-9}
& \multirow{3}{*}{\textbf{7B}} & None & 76.04$_{\pm1.18}$ & 81.80$_{\pm1.06}$ & 81.52$_{\pm0.80}$ & 81.40$_{\pm0.80}$ & 52.90$_{\pm1.46}$ & 55.20$_{\pm1.45}$ \\
 &  & GPTQ-Int8 & 76.12$_{\pm1.17}$ & 82.71$_{\pm1.04}$ & 81.57$_{\pm0.80}$ & 81.06$_{\pm0.80}$ & 52.30$_{\pm1.46}$ & 54.69$_{\pm1.45}$ \\
 &  & GPTQ-Int4 & 71.49$_{\pm1.24}$ & 79.30$_{\pm1.12}$ & 81.44$_{\pm0.80}$ & 80.05$_{\pm0.82}$ & 51.62$_{\pm1.46}$ & 54.35$_{\pm1.46}$ \\
 \cline{3-9}
& \multirow{3}{*}{\textbf{14B}} & None & 80.06$_{\pm1.10}$ & 45.56$_{\pm1.37}$ & 85.73$_{\pm0.72}$ & 81.61$_{\pm0.79}$ & 60.41$_{\pm1.43}$ & 62.29$_{\pm1.42}$ \\
 &  & GPTQ-Int8 & 79.61$_{\pm1.11}$ & 46.47$_{\pm1.37}$ & 86.15$_{\pm0.71}$ & 81.86$_{\pm0.79}$ & 60.92$_{\pm1.43}$ & 62.37$_{\pm1.42}$ \\
 &  & GPTQ-Int4 & 0.61$_{\pm0.21}$ & 1.06$_{\pm0.28}$ & 39.77$_{\pm1.00}$ & 37.29$_{\pm0.99}$ & 21.59$_{\pm1.20}$ & 23.38$_{\pm1.24}$ \\
 \cline{3-9}
& \multirow{3}{*}{\textbf{32B}} & None & 75.13$_{\pm1.19}$ & 69.60$_{\pm1.27}$ & 82.24$_{\pm0.78}$ & 77.31$_{\pm0.86}$ & 57.76$_{\pm1.44}$ & 58.79$_{\pm1.44}$ \\
 &  & GPTQ-Int8 & 75.21$_{\pm1.19}$ & 69.75$_{\pm1.27}$ & 82.15$_{\pm0.79}$ & 77.19$_{\pm0.86}$ & 58.11$_{\pm1.44}$ & 58.79$_{\pm1.44}$ \\
 &  & GPTQ-Int4 & 74.91$_{\pm1.19}$ & 81.27$_{\pm1.07}$ & 81.52$_{\pm0.80}$ & 76.14$_{\pm0.87}$ & 56.74$_{\pm1.45}$ & 57.68$_{\pm1.44}$ \\


% \midrule
% \multicolumn{9}{|>{\columncolor[gray]{.8}}c|}{\textbf{Others (evaluator=lm-eval-harness)}} \\
% \midrule
 % \cmidrule{2-9}
 
\bottomrule
\end{tabulary}
\end{adjustbox}
\caption{Performance Comparison on GSM8K, ARC-Easy, and ARC-Challenge with lm-eval-harness framework. GSM8K is evaluated using exact match and flexible-extract string matching. ARC-Easy and ARC-Challenge are evaluated using accuracy (acc) and normalized accuracy (acc\_norm).}
\label{app: lm-1}
\end{table*}






% ##################################################################################################


\begin{table*}

\centering
\scriptsize
\begin{adjustbox}{width=\textwidth}
\begin{tabulary}{1.4\textwidth}{L CC C CC CC CC} % Adjusted width and columns
\toprule
\multicolumn{3}{c}{\multirow{3}{*}{\textbf{Models}}} & \multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}commonsense\_qa\\ (acc)\end{tabular}}} & \multicolumn{2}{c}{\textbf{hellaswag}} & \multicolumn{2}{c}{\textbf{mathqa}} & \multicolumn{2}{c}{\textbf{openbookqa}} \\
\cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10}
 &  &  &  & \textbf{(acc)} & \textbf{(acc\_norm)} & \textbf{(acc)} & \textbf{(acc\_norm)} & \textbf{(acc)} & \textbf{(acc\_norm)} \\
\cmidrule(lr){1-3}
\textbf{Model} & \textbf{Param.} & \textbf{Quantization} &  &  &  &  &  &  &  \\
\midrule

\multirow{2}{*}{\textbf{HF/SmolLM2}} & \multirow{2}{*}{\textbf{1.7B}} & None (Base) & 41.85$_{\pm1.41}$ & 53.36$_{\pm0.50}$ & 71.43$_{\pm0.45}$ & 34.47$_{\pm0.87}$ & 34.07$_{\pm0.87}$ & 32.20$_{\pm2.09}$ & 43.80$_{\pm2.22}$ \\
 &  & None (Instruct) & 50.53$_{\pm1.43}$ & 53.48$_{\pm0.50}$ & 71.80$_{\pm0.45}$ & 31.12$_{\pm0.85}$ & 31.96$_{\pm0.85}$ & 33.80$_{\pm2.12}$ & 45.80$_{\pm2.23}$ \\
\cmidrule{3-10}
\multirow{2}{*}{\textbf{nvidia/Hymba}} & \multirow{2}{*}{\textbf{1.5B}} & None (Base) & 63.31$_{\pm1.38}$ & 53.49$_{\pm0.50}$ & 71.49$_{\pm0.45}$ & 28.94$_{\pm0.83}$ & 27.97$_{\pm0.82}$ & 32.40$_{\pm2.10}$ & 41.60$_{\pm2.21}$ \\
 &  & None (Instruct) & 63.55$_{\pm1.38}$ & 53.57$_{\pm0.50}$ & 71.06$_{\pm0.45}$ & 29.18$_{\pm0.83}$ & 29.25$_{\pm0.83}$ & 31.00$_{\pm2.07}$ & 42.00$_{\pm2.21}$ \\
 \cmidrule{3-10}
\textbf{nvidia/Minitron} & \textbf{4B} & None (Base) & 71.01$_{\pm1.30}$ & 53.89$_{\pm0.50}$ & 72.29$_{\pm0.45}$ & 31.49$_{\pm0.85}$ & 31.36$_{\pm0.85}$ & 30.80$_{\pm2.07}$ & 42.60$_{\pm2.21}$ \\
 \cmidrule{3-10}
\multirow{3}{*}{\textbf{Qwen2.5 (Base)}} & \textbf{0.5B} & None & 54.79$_{\pm1.42}$ & 40.53$_{\pm0.49}$ & 52.20$_{\pm0.50}$ & 28.88$_{\pm0.83}$ & 29.45$_{\pm0.83}$ & 24.80$_{\pm1.93}$ & 35.40$_{\pm2.14}$ \\
 & \textbf{1.5B} & None & 74.61$_{\pm1.25}$ & 50.20$_{\pm0.50}$ & 67.86$_{\pm0.47}$ & 34.57$_{\pm0.87}$ & 35.34$_{\pm0.88}$ & 31.80$_{\pm2.08}$ & 40.40$_{\pm2.20}$ \\
 & \textbf{3B} & None & 77.07$_{\pm1.20}$ & 55.02$_{\pm0.50}$ & 73.68$_{\pm0.44}$ & 37.22$_{\pm0.88}$ & 37.42$_{\pm0.89}$ & 29.40$_{\pm2.04}$ & 42.80$_{\pm2.21}$ \\
  \cmidrule{3-10}
\multirow{1}{*}{\textbf{Llama-3.2 (Base)}} & \multirow{1}{*}{\textbf{1B}} & None & 46.93$_{\pm1.43}$ & 47.69$_{\pm0.50}$ & 63.78$_{\pm0.48}$ & 29.11$_{\pm0.83}$ & 29.21$_{\pm0.83}$ & 26.00$_{\pm1.96}$ & 36.80$_{\pm2.16}$ \\

\midrule

\multirow{8}{*}{\textbf{Llama-3.2 (Instruct)}} & \multirow{4}{*}{\textbf{1B}} & None & 55.45$_{\pm1.42}$ & 45.15$_{\pm0.50}$ & 60.76$_{\pm0.49}$ & 33.30$_{\pm0.86}$ & 32.86$_{\pm0.86}$ & 24.20$_{\pm1.92}$ & 34.80$_{\pm2.13}$ \\
 &  & w8a8 & 55.36$_{\pm1.42}$ & 45.15$_{\pm0.50}$ & 60.61$_{\pm0.49}$ & 33.13$_{\pm0.86}$ & 32.56$_{\pm0.86}$ & 24.20$_{\pm1.92}$ & 34.80$_{\pm2.13}$ \\
 &  & FP8 & 54.63$_{\pm1.43}$ & 44.88$_{\pm0.50}$ & 60.07$_{\pm0.49}$ & 32.66$_{\pm0.86}$ & 32.46$_{\pm0.86}$ & 23.60$_{\pm1.90}$ & 33.80$_{\pm2.12}$ \\
 &  & FP8-dynamic & 55.45$_{\pm1.42}$ & 45.00$_{\pm0.50}$ & 60.68$_{\pm0.49}$ & 32.93$_{\pm0.86}$ & 32.63$_{\pm0.86}$ & 24.80$_{\pm1.93}$ & 35.00$_{\pm2.14}$ \\
\cmidrule{3-10}
 & \multirow{4}{*}{\textbf{3B}} & None & 67.73$_{\pm1.34}$ & 52.29$_{\pm0.50}$ & 70.55$_{\pm0.45}$ & 34.71$_{\pm0.87}$ & 34.44$_{\pm0.87}$ & 27.60$_{\pm2.00}$ & 36.20$_{\pm2.15}$ \\
 &  & w8a8 & 67.49$_{\pm1.34}$ & 52.25$_{\pm0.50}$ & 70.53$_{\pm0.45}$ & 34.81$_{\pm0.87}$ & 34.71$_{\pm0.87}$ & 28.20$_{\pm2.01}$ & 36.40$_{\pm2.15}$ \\
 &  & FP8 & 62.16$_{\pm1.39}$ & 50.40$_{\pm0.50}$ & 68.87$_{\pm0.46}$ & 35.28$_{\pm0.87}$ & 34.71$_{\pm0.87}$ & 27.40$_{\pm2.00}$ & 37.20$_{\pm2.16}$ \\
 &  & FP8-dynamic & 67.73$_{\pm1.34}$ & 52.16$_{\pm0.50}$ & 70.25$_{\pm0.46}$ & 34.84$_{\pm0.87}$ & 34.77$_{\pm0.87}$ & 28.60$_{\pm2.02}$ & 37.60$_{\pm2.17}$ \\
 \midrule
\multirow{12}{*}{\textbf{Llama-3.1 (Instruct)}} & \multirow{6}{*}{\textbf{8B}} & None & 77.40$_{\pm1.20}$ & 59.13$_{\pm0.49}$ & 79.17$_{\pm0.41}$ & 39.43$_{\pm0.89}$ & 39.53$_{\pm0.90}$ & 33.20$_{\pm2.11}$ & 43.20$_{\pm2.22}$ \\
 &  & w8a8 & 76.99$_{\pm1.21}$ & 58.98$_{\pm0.49}$ & 79.28$_{\pm0.40}$ & 39.97$_{\pm0.90}$ & 39.97$_{\pm0.90}$ & 33.00$_{\pm2.10}$ & 42.80$_{\pm2.21}$ \\
 &  & w8a16 & 76.99$_{\pm1.21}$ & 59.14$_{\pm0.49}$ & 79.21$_{\pm0.40}$ & 39.77$_{\pm0.90}$ & 39.77$_{\pm0.90}$ & 33.20$_{\pm2.11}$ & 43.20$_{\pm2.22}$ \\
 &  & w4a16 & 68.39$_{\pm1.33}$ & 58.03$_{\pm0.49}$ & 77.79$_{\pm0.41}$ & 38.02$_{\pm0.89}$ & 37.96$_{\pm0.89}$ & 31.80$_{\pm2.08}$ & 42.40$_{\pm2.21}$ \\
 &  & FP8 & 76.58$_{\pm1.21}$ & 58.91$_{\pm0.49}$ & 78.12$_{\pm0.41}$ & 38.86$_{\pm0.89}$ & 39.16$_{\pm0.89}$ & 32.80$_{\pm2.10}$ & 44.40$_{\pm2.22}$ \\
 &  & FP8-dynamic & 77.15$_{\pm1.20}$ & 59.07$_{\pm0.49}$ & 79.10$_{\pm0.41}$ & 39.87$_{\pm0.90}$ & 39.77$_{\pm0.90}$ & 34.40$_{\pm2.13}$ & 43.60$_{\pm2.22}$ \\
\cmidrule{3-10}
 & \multirow{6}{*}{\textbf{70B}} & None & 80.92$_{\pm1.13}$ & 65.22$_{\pm0.48}$ & 84.66$_{\pm0.36}$ & 56.05$_{\pm0.91}$ & 54.67$_{\pm0.91}$ & 37.20$_{\pm2.16}$ & 47.40$_{\pm2.24}$ \\
 &  & w8a8 & 80.59$_{\pm1.13}$ & 65.10$_{\pm0.48}$ & 84.59$_{\pm0.36}$ & 56.01$_{\pm0.91}$ & 54.67$_{\pm0.91}$ & 36.60$_{\pm2.16}$ & 46.40$_{\pm2.23}$ \\
 &  & w8a16 & 69.04$_{\pm1.32}$ & 64.56$_{\pm0.48}$ & 84.06$_{\pm0.37}$ & 44.92$_{\pm0.91}$ & 44.96$_{\pm0.91}$ & 32.20$_{\pm2.09}$ & 44.00$_{\pm2.22}$ \\
 &  & w4a16 & 79.93$_{\pm1.15}$ & 64.55$_{\pm0.48}$ & 84.04$_{\pm0.37}$ & 54.97$_{\pm0.91}$ & 54.24$_{\pm0.91}$ & 35.40$_{\pm2.14}$ & 45.40$_{\pm2.23}$ \\
 &  & FP8 & 78.79$_{\pm1.17}$ & 63.03$_{\pm0.48}$ & 83.16$_{\pm0.37}$ & 51.89$_{\pm0.91}$ & 50.99$_{\pm0.92}$ & 37.00$_{\pm2.16}$ & 47.40$_{\pm2.24}$ \\
 &  & FP8-dynamic & 80.51$_{\pm1.13}$ & 65.11$_{\pm0.48}$ & 84.54$_{\pm0.36}$ & 55.78$_{\pm0.91}$ & 54.54$_{\pm0.91}$ & 36.00$_{\pm2.15}$ & 47.20$_{\pm2.23}$ \\
\midrule
\multirow{4}{*}{\textbf{Mistral$_{\text{v0.3}}$ (Instruct)}} & \multirow{4}{*}{\textbf{7B}} &  & 69.29$_{\pm1.32}$ & 64.91$_{\pm0.48}$ & 82.86$_{\pm0.38}$ & 37.39$_{\pm0.89}$ & 38.56$_{\pm0.89}$ & 36.00$_{\pm2.15}$ & 47.20$_{\pm2.23}$ \\
 &  & w8a8 & 69.62$_{\pm1.32}$ & 64.70$_{\pm0.48}$ & 82.88$_{\pm0.38}$ & 37.62$_{\pm0.89}$ & 38.32$_{\pm0.89}$ & 35.40$_{\pm2.14}$ & 47.00$_{\pm2.23}$ \\
 &  & w8a16 & 69.94$_{\pm1.31}$ & 64.78$_{\pm0.48}$ & 83.01$_{\pm0.37}$ & 38.16$_{\pm0.89}$ & 39.26$_{\pm0.89}$ & 35.80$_{\pm2.15}$ & 46.60$_{\pm2.23}$ \\
 &  & w4a16 & 62.49$_{\pm1.39}$ & 62.69$_{\pm0.48}$ & 81.46$_{\pm0.39}$ & 37.25$_{\pm0.89}$ & 37.59$_{\pm0.89}$ & 33.60$_{\pm2.11}$ & 43.00$_{\pm2.22}$ \\
\cmidrule{3-10}
\multirow{2}{*}{\textbf{Mistral$_{\text{Nemo}}$ (Instruct)}} & \multirow{2}{*}{\textbf{12B}} & None & 70.52$_{\pm1.31}$ & 63.27$_{\pm0.48}$ & 82.35$_{\pm0.38}$ & 39.36$_{\pm0.89}$ & 39.90$_{\pm0.90}$ & 37.40$_{\pm2.17}$ & 46.40$_{\pm2.23}$ \\
 &  & w4a16 & 69.12$_{\pm1.32}$ & 62.00$_{\pm0.48}$ & 81.37$_{\pm0.39}$ & 38.36$_{\pm0.89}$ & 38.76$_{\pm0.89}$ & 36.80$_{\pm2.16}$ & 46.20$_{\pm2.23}$ \\
% \midrule
\midrule
\multirow{18}{*}{\textbf{Qwen$_{\text{2}}$ (Instruct)}} & \multirow{6}{*}{\textbf{0.5B}} & None & 52.74$_{\pm1.43}$ & 39.01$_{\pm0.49}$ & 49.79$_{\pm0.50}$ & 25.93$_{\pm0.80}$ & 26.83$_{\pm0.81}$ & 24.00$_{\pm1.91}$ & 33.40$_{\pm2.11}$ \\
 &  & GPTQ-Int8 & 52.99$_{\pm1.43}$ & 39.07$_{\pm0.49}$ & 49.92$_{\pm0.50}$ & 25.63$_{\pm0.80}$ & 26.43$_{\pm0.81}$ & 24.00$_{\pm1.91}$ & 33.80$_{\pm2.12}$ \\
 &  & GPTQ-Int4 & 47.75$_{\pm1.43}$ & 38.33$_{\pm0.49}$ & 48.05$_{\pm0.50}$ & 25.76$_{\pm0.80}$ & 26.13$_{\pm0.80}$ & 20.60$_{\pm1.81}$ & 33.20$_{\pm2.11}$ \\
 &  & w8a16 & 53.24$_{\pm1.43}$ & 39.08$_{\pm0.49}$ & 49.77$_{\pm0.50}$ & 25.70$_{\pm0.80}$ & 26.50$_{\pm0.81}$ & 24.40$_{\pm1.92}$ & 33.60$_{\pm2.11}$ \\
 &  & w8a8 & 53.15$_{\pm1.43}$ & 39.19$_{\pm0.49}$ & 49.95$_{\pm0.50}$ & 25.76$_{\pm0.80}$ & 26.67$_{\pm0.81}$ & 24.00$_{\pm1.91}$ & 32.60$_{\pm2.10}$ \\
 &  & w4a16 & 46.44$_{\pm1.43}$ & 37.86$_{\pm0.48}$ & 48.14$_{\pm0.50}$ & 25.26$_{\pm0.80}$ & 25.53$_{\pm0.80}$ & 21.00$_{\pm1.82}$ & 32.80$_{\pm2.10}$ \\
\cmidrule{3-10}
 & \multirow{6}{*}{\textbf{1.5B}} & None & 70.19$_{\pm1.31}$ & 49.28$_{\pm0.50}$ & 66.03$_{\pm0.47}$ & 32.90$_{\pm0.86}$ & 32.80$_{\pm0.86}$ & 27.80$_{\pm2.01}$ & 37.20$_{\pm2.16}$ \\
 &  & GPTQ-Int8 & 70.19$_{\pm1.31}$ & 49.28$_{\pm0.50}$ & 65.89$_{\pm0.47}$ & 33.10$_{\pm0.86}$ & 32.96$_{\pm0.86}$ & 28.00$_{\pm2.01}$ & 37.00$_{\pm2.16}$ \\
 &  & GPTQ-Int4 & 69.62$_{\pm1.32}$ & 48.15$_{\pm0.50}$ & 64.83$_{\pm0.48}$ & 31.26$_{\pm0.85}$ & 32.19$_{\pm0.86}$ & 26.80$_{\pm1.98}$ & 36.60$_{\pm2.16}$ \\
 &  & w8a16 & 69.78$_{\pm1.31}$ & 49.29$_{\pm0.50}$ & 66.02$_{\pm0.47}$ & 33.00$_{\pm0.86}$ & 33.00$_{\pm0.86}$ & 28.00$_{\pm2.01}$ & 37.00$_{\pm2.16}$ \\
 &  & w8a8 & 70.11$_{\pm1.31}$ & 49.39$_{\pm0.50}$ & 66.01$_{\pm0.47}$ & 33.03$_{\pm0.86}$ & 32.83$_{\pm0.86}$ & 27.80$_{\pm2.01}$ & 36.40$_{\pm2.15}$ \\
 &  & w4a16 & 68.39$_{\pm1.33}$ & 48.15$_{\pm0.50}$ & 64.46$_{\pm0.48}$ & 32.63$_{\pm0.86}$ & 33.10$_{\pm0.86}$ & 26.40$_{\pm1.97}$ & 38.00$_{\pm2.17}$ \\
\cmidrule{3-10}
 & \multirow{6}{*}{\textbf{7B}} & None & 80.75$_{\pm1.13}$ & 61.05$_{\pm0.49}$ & 80.67$_{\pm0.39}$ & 44.15$_{\pm0.91}$ & 41.98$_{\pm0.90}$ & 35.40$_{\pm2.14}$ & 46.20$_{\pm2.23}$ \\
 &  & GPTQ-Int8 & 81.00$_{\pm1.12}$ & 61.04$_{\pm0.49}$ & 80.75$_{\pm0.39}$ & 44.29$_{\pm0.91}$ & 42.21$_{\pm0.90}$ & 35.20$_{\pm2.14}$ & 46.60$_{\pm2.23}$ \\
 &  & GPTQ-Int4 & 79.03$_{\pm1.17}$ & 60.21$_{\pm0.49}$ & 79.34$_{\pm0.40}$ & 43.48$_{\pm0.91}$ & 42.48$_{\pm0.90}$ & 34.80$_{\pm2.13}$ & 43.60$_{\pm2.22}$ \\
 &  & w8a16 & 80.84$_{\pm1.13}$ & 61.04$_{\pm0.49}$ & 80.53$_{\pm0.40}$ & 44.15$_{\pm0.91}$ & 42.28$_{\pm0.90}$ & 34.40$_{\pm2.13}$ & 46.40$_{\pm2.23}$ \\
 &  & w8a8 & 80.67$_{\pm1.13}$ & 61.08$_{\pm0.49}$ & 80.55$_{\pm0.39}$ & 43.58$_{\pm0.91}$ & 41.51$_{\pm0.90}$ & 34.80$_{\pm2.13}$ & 46.20$_{\pm2.23}$ \\
 &  & w4a16 & 79.44$_{\pm1.16}$ & 59.93$_{\pm0.49}$ & 79.79$_{\pm0.40}$ & 43.32$_{\pm0.91}$ & 42.38$_{\pm0.90}$ & 33.40$_{\pm2.11}$ & 46.40$_{\pm2.23}$ \\

\midrule
\multirow{18}{*}{\textbf{Qwen$_{\text{2.5}}$ (Instruct)}} & \multirow{3}{*}{\textbf{0.5B}} & None & 56.92$_{\pm1.42}$ & 40.53$_{\pm0.49}$ & 52.53$_{\pm0.50}$ & 28.98$_{\pm0.83}$ & 29.65$_{\pm0.84}$ & 23.80$_{\pm1.91}$ & 34.40$_{\pm2.13}$ \\
 &  & GPTQ-Int8 & 57.08$_{\pm1.42}$ & 40.53$_{\pm0.49}$ & 52.39$_{\pm0.50}$ & 28.78$_{\pm0.83}$ & 29.85$_{\pm0.84}$ & 24.00$_{\pm1.91}$ & 34.60$_{\pm2.13}$ \\
 &  & GPTQ-Int4 & 47.26$_{\pm1.43}$ & 38.91$_{\pm0.49}$ & 49.33$_{\pm0.50}$ & 27.91$_{\pm0.82}$ & 28.27$_{\pm0.82}$ & 23.80$_{\pm1.91}$ & 33.20$_{\pm2.11}$ \\
\cmidrule{3-10}
 & \multirow{3}{*}{\textbf{1.5B}} & None & 74.53$_{\pm1.25}$ & 50.89$_{\pm0.50}$ & 68.28$_{\pm0.46}$ & 33.63$_{\pm0.86}$ & 34.07$_{\pm0.87}$ & 31.60$_{\pm2.08}$ & 41.00$_{\pm2.20}$ \\
 &  & GPTQ-Int8 & 75.51$_{\pm1.23}$ & 50.86$_{\pm0.50}$ & 68.37$_{\pm0.46}$ & 33.23$_{\pm0.86}$ & 33.84$_{\pm0.87}$ & 31.80$_{\pm2.08}$ & 40.20$_{\pm2.19}$ \\
 &  & GPTQ-Int4 & 71.42$_{\pm1.29}$ & 49.38$_{\pm0.50}$ & 65.93$_{\pm0.47}$ & 31.26$_{\pm0.85}$ & 32.53$_{\pm0.86}$ & 31.20$_{\pm2.07}$ & 40.60$_{\pm2.20}$ \\
 \cmidrule{3-10}
& \multirow{3}{*}{\textbf{3B}} & None & 78.79$_{\pm1.17}$ & 56.35$_{\pm0.49}$ & 74.94$_{\pm0.43}$ & 35.18$_{\pm0.87}$ & 35.28$_{\pm0.87}$ & 32.60$_{\pm2.10}$ & 42.00$_{\pm2.21}$ \\
 &  & GPTQ-Int8 & 78.79$_{\pm1.17}$ & 56.34$_{\pm0.49}$ & 75.02$_{\pm0.43}$ & 35.58$_{\pm0.88}$ & 35.78$_{\pm0.88}$ & 32.80$_{\pm2.10}$ & 41.80$_{\pm2.21}$ \\
 &  & GPTQ-Int4 & 76.74$_{\pm1.21}$ & 55.24$_{\pm0.50}$ & 73.27$_{\pm0.44}$ & 36.78$_{\pm0.88}$ & 37.39$_{\pm0.89}$ & 29.40$_{\pm2.04}$ & 42.00$_{\pm2.21}$ \\
\cmidrule{3-10}
& \multirow{3}{*}{\textbf{7B}} & None & 82.64$_{\pm1.08}$ & 62.04$_{\pm0.48}$ & 80.50$_{\pm0.40}$ & 40.57$_{\pm0.90}$ & 40.13$_{\pm0.90}$ & 34.60$_{\pm2.13}$ & 48.80$_{\pm2.24}$ \\
 &  & GPTQ-Int8 & 82.64$_{\pm1.08}$ & 62.00$_{\pm0.48}$ & 80.52$_{\pm0.40}$ & 40.57$_{\pm0.90}$ & 40.17$_{\pm0.90}$ & 34.60$_{\pm2.13}$ & 48.40$_{\pm2.24}$ \\
 &  & GPTQ-Int4 & 81.82$_{\pm1.10}$ & 60.98$_{\pm0.49}$ & 79.73$_{\pm0.40}$ & 40.30$_{\pm0.90}$ & 39.80$_{\pm0.90}$ & 35.40$_{\pm2.14}$ & 47.20$_{\pm2.23}$ \\
 \cmidrule{3-10}
& \multirow{3}{*}{\textbf{14B}} & None & 84.03$_{\pm1.05}$ & 65.56$_{\pm0.47}$ & 84.38$_{\pm0.36}$ & 49.78$_{\pm0.92}$ & 48.41$_{\pm0.91}$ & 36.20$_{\pm2.15}$ & 47.80$_{\pm2.24}$ \\
 &  & GPTQ-Int8 & 83.87$_{\pm1.05}$ & 65.67$_{\pm0.47}$ & 84.50$_{\pm0.36}$ & 48.94$_{\pm0.92}$ & 47.67$_{\pm0.91}$ & 37.20$_{\pm2.16}$ & 47.20$_{\pm2.23}$ \\
 &  & GPTQ-Int4 & 19.41$_{\pm1.13}$ & 31.46$_{\pm0.46}$ & 36.94$_{\pm0.48}$ & 21.98$_{\pm0.76}$ & 22.48$_{\pm0.76}$ & 14.60$_{\pm1.58}$ & 27.60$_{\pm2.00}$ \\
 \cmidrule{3-10}
& \multirow{3}{*}{\textbf{32B}} & None & 86.73$_{\pm0.97}$ & 66.88$_{\pm0.47}$ & 85.22$_{\pm0.35}$ & 57.02$_{\pm0.91}$ & 54.67$_{\pm0.91}$ & 35.60$_{\pm2.14}$ & 45.60$_{\pm2.23}$ \\
 &  & GPTQ-Int8 & 87.06$_{\pm0.96}$ & 66.77$_{\pm0.47}$ & 85.24$_{\pm0.35}$ & 57.15$_{\pm0.91}$ & 54.91$_{\pm0.91}$ & 35.40$_{\pm2.14}$ & 45.40$_{\pm2.23}$ \\
 &  & GPTQ-Int4 & 86.49$_{\pm0.98}$ & 66.40$_{\pm0.47}$ & 84.74$_{\pm0.36}$ & 54.27$_{\pm0.91}$ & 52.76$_{\pm0.91}$ & 36.00$_{\pm2.15}$ & 46.00$_{\pm2.23}$ \\

\bottomrule
\end{tabulary}
\end{adjustbox}
\caption{Performance Comparison on CommonsenseQA, HellaSwag, MATHQA, and OpenBookQA with lm-eval-harness framework. CommonsenseQA is evaluated using accuracy (acc). HellaSwag, OpenBookQA, and MATHQA are evaluated using both accuracy (acc) and normalized accuracy (acc\_norm).}
\label{app: lm-2}
\end{table*}

% ##################################################################################################


\begin{table*}

\centering
\scriptsize
\begin{adjustbox}{width=\textwidth}
\begin{tabulary}{1.2\textwidth}{LCC CC CC CC}
\toprule
\multicolumn{3}{c}{\multirow{3}{*}{\textbf{Models}}} & \multicolumn{2}{c}{\textbf{gsm8k (exact\_match)}} & \multicolumn{2}{c}{\textbf{arc\_easy}} & \multicolumn{2}{c}{\textbf{arc\_challenge}} \\
\cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
 &  &  & \textbf{(strict-match)} & \textbf{(flexible-extract)} & \textbf{(acc)} & \textbf{(acc\_norm)} & \textbf{(acc)} & \textbf{(acc\_norm)} \\
\cmidrule(lr){1-3}
\textbf{Model} & \textbf{Param.} & \textbf{Pruning type, Method} &  &  &  &  & &  \\
\cmidrule{1-9}
\multicolumn{9}{|>{\columncolor[gray]{.8}}c|}{\textbf{No Knowledge Distillation}} \\
\cmidrule{1-9}

\textbf{phi-2} & - & SparseGPT &	43.97$_{\pm1.37}$ &	44.05$_{\pm1.37}$ &	77.31$_{\pm0.86}$ &	75.08$_{\pm0.89}$ &	47.10$_{\pm1.46}$ &	48.63$_{\pm1.46}$\\
\textbf{TinyLlama} & \textbf{1.1B} & SparseGPT & 0.76$_{\pm0.24}$ & 1.97$_{\pm0.38}$ & 45.66$_{\pm1.02}$ & 42.00$_{\pm1.01}$ & 22.10$_{\pm1.21}$ & 25.60$_{\pm1.28}$ \\

\cmidrule{1-9}
\multicolumn{9}{|>{\columncolor[gray]{.8}}c|}{\textbf{Retrained by Cerebras with 50B tokens from SlimPajama}} \\
\cmidrule{1-9}

\multirow{1}{*}{\textbf{Llama-2}} & \multirow{1}{*}{\textbf{7B}} & SparseGPT (70\%)& 7.88$_{\pm0.74}$ & 8.19$_{\pm0.76}$ & 72.52$_{\pm0.92}$ & 69.74$_{\pm0.94}$ & 38.57$_{\pm1.42}$ & 41.21$_{\pm1.44}$ \\

\cmidrule{1-9}
\multicolumn{9}{|>{\columncolor[gray]{.8}}c|}{\textbf{Knowledge Distillation for 13B tokens using SquareHead Approach}} \\
\cmidrule{1-9}


\multirow{1}{*}{\textbf{Llama-3.1}} & \multirow{1}{*}{\textbf{8B}} & 2of4 Sparsity, SparseGPT & 61.49$_{\pm1.34}$ & 61.56$_{\pm1.34}$ & 77.23$_{\pm0.86}$ & 66.88$_{\pm0.97}$ & 45.56$_{\pm1.46}$ & 47.95$_{\pm1.46}$ \\


\cmidrule{1-9}
\multicolumn{9}{|>{\columncolor[gray]{.8}}c|}{\textbf{Fine-tuned on GSM8K}} \\
\cmidrule{1-9}

\multirow{3}{*}{\textbf{Llama-2}} & \multirow{3}{*}{\textbf{7B}} & No Pruning & 37.30$_{\pm1.33}$ & 37.53$_{\pm1.33}$ & 74.58$_{\pm0.89}$ & 70.62$_{\pm0.93}$ & 41.47$_{\pm1.44}$ & 43.77$_{\pm1.45}$ \\
&  & SparseGPT (50\%) & 36.54$_{\pm1.33}$ & 36.54$_{\pm1.33}$ & 73.06$_{\pm0.91}$ & 69.15$_{\pm0.95}$ & 36.69$_{\pm1.41}$ & 40.27$_{\pm1.43}$ \\
& & SparseGPT (70\%) &	34.19$_{\pm1.31}$ & 34.19$_{\pm1.31}$ & 70.71$_{\pm0.93}$ & 66.12$_{\pm0.97}$ & 35.58$_{\pm1.40}$ & 37.71$_{\pm1.42}$ \\

\cmidrule{3-9}

\multirow{2}{*}{\textbf{Sparse-Llama-3.1}} & \multirow{2}{*}{\textbf{8B}} & pruned-2of4 & 57.24$_{\pm1.36}$ & 57.24$_{\pm1.36}$ & 81.06$_{\pm0.80}$ & 78.58$_{\pm0.84}$ & 51.19$_{\pm1.46}$ & 53.07$_{\pm1.46}$ \\
% &  & pruned-2of4, INT4 Quant. & -------	& -------	& -------	& -------	& -------	& ------- \\
&  & pruned-2of4, FP8 Quant.	& 62.55$_{\pm1.33}$ & 62.70$_{\pm1.33}$ & 77.44$_{\pm0.86}$ & 67.09$_{\pm0.96}$ & 45.22$_{\pm1.45}$ & 47.53$_{\pm1.46}$ \\

\cmidrule{1-9}
\multicolumn{9}{|>{\columncolor[gray]{.8}}c|}{\textbf{Mistral Fine-tuned}} \\
\cmidrule{1-9}


\textbf{OpenHermes-2.5} & \textbf{7B} & SparseGPT	& 19.79$_{\pm1.10}$ & 34.19$_{\pm1.31}$ & 73.86$_{\pm0.90}$ & 73.02$_{\pm0.91}$ & 41.81$_{\pm1.44}$ & 44.28$_{\pm1.45}$ \\


\bottomrule
\end{tabulary}
\end{adjustbox}
\caption{Performance Comparison of Pruned and Distilled models on GSM8K, ARC-Easy, and ARC-Challenge with lm-eval-harness framework.}
\label{app: lm-3}
\end{table*}

% ##################################################################################################

\begin{table*}
\centering
\scriptsize
\begin{adjustbox}{width=\textwidth}
\begin{tabulary}{1.4\textwidth}{LCC C CC CC CC}
\toprule
\multicolumn{3}{c}{\multirow{3}{*}{\textbf{Models}}} & \multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}commonsense\_qa\\ (acc)\end{tabular}}} & \multicolumn{2}{c}{\textbf{hellaswag}} & \multicolumn{2}{c}{\textbf{mathqa}} & \multicolumn{2}{c}{\textbf{openbookqa}} \\
\cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10}
 &  &  &  & \textbf{(acc)} & \textbf{(acc\_norm)} & \textbf{(acc)} & \textbf{(acc\_norm)} & \textbf{(acc)} & \textbf{(acc\_norm)} \\
\cmidrule(lr){1-3}
\textbf{Model} & \textbf{Param.} & \textbf{Pruning type, Method} &  &  &  &  &  &  &  \\
\cmidrule{1-10}
\multicolumn{10}{|>{\columncolor[gray]{.8}}c|}{\textbf{No Knowledge Distillation}} \\
\cmidrule{1-10}


\textbf{phi-2} & - & SparseGPT	& $59.21_{\pm1.41}$ & $48.75_{\pm0.50}$ & $65.71_{\pm0.47}$ & $30.79_{\pm0.85}$ & $30.15_{\pm0.84}$ & $36.60_{\pm2.16}$ & $46.20_{\pm2.23}$ \\
\textbf{TinyLlama} & \textbf{1.1B} & SparseGPT & $18.92_{\pm1.12}$ & $33.00_{\pm0.47}$ & $40.43_{\pm0.49}$ & $23.82_{\pm0.78}$ & $23.28_{\pm0.77}$ & $16.60_{\pm1.67}$ & $29.80_{\pm2.05}$ \\

\cmidrule{1-10}
\multicolumn{10}{|>{\columncolor[gray]{.8}}c|}{\textbf{Retrained by Cerebras with 50B tokens from SlimPajama}} \\
\cmidrule{1-10}


\multirow{1}{*}{\textbf{Llama-2}} & \multirow{1}{*}{\textbf{7B}} & SparseGPT (70\%) & $24.24_{\pm1.23}$ & $53.69_{\pm0.50}$ & $72.25_{\pm0.45}$ & $27.74_{\pm0.82}$ & $27.27_{\pm0.82}$ & $31.60_{\pm2.08}$ & $40.20_{\pm2.19}$ \\

\cmidrule{1-10}
\multicolumn{10}{|>{\columncolor[gray]{.8}}c|}{\textbf{Knowledge Distillation for 13B tokens using SquareHead Approach}} \\
\cmidrule{1-10}

\multirow{1}{*}{\textbf{Llama-3.1}} & \multirow{1}{*}{\textbf{8B}} & 2of4 Sparsity, SparseGPT & $74.45_{\pm1.25}$ & $58.84_{\pm0.49}$ & $77.60_{\pm0.42}$ & $38.09_{\pm0.89}$ & $37.82_{\pm0.89}$ & $34.80_{\pm2.13}$ & $45.40_{\pm2.23}$ \\

\cmidrule{1-10}
\multicolumn{10}{|>{\columncolor[gray]{.8}}c|}{\textbf{Fine-tuned on GSM8K}} \\
\cmidrule{1-10}


\multirow{3}{*}{\textbf{Llama-2}} & \multirow{3}{*}{\textbf{7B}} & No Pruning & $41.44_{\pm1.41}$ & $57.56_{\pm0.49}$ & $75.31_{\pm0.43}$ & $26.06_{\pm0.80}$ & $25.86_{\pm0.80}$ & $31.20_{\pm2.07}$ & $43.60_{\pm2.22}$ \\
&  & SparseGPT (50\%) & $32.19_{\pm1.34}$ & $56.07_{\pm0.50}$ & $71.02_{\pm0.45}$ & $26.33_{\pm0.81}$ & $26.03_{\pm0.80}$ & $29.20_{\pm2.04}$ & $39.40_{\pm2.19}$ \\
& & SparseGPT (70\%) &	$30.38_{\pm1.32}$ &	$54.40_{\pm0.50}$ &	$68.47_{\pm0.46}$ &	$24.99_{\pm0.79}$ &	$24.99_{\pm0.79}$ &	$29.00_{\pm2.03}$ &	$40.80_{\pm2.20}$ \\

\cmidrule{2-10}

\multirow{3}{*}{\textbf{Sparse-Llama-3.1}} & \multirow{3}{*}{\textbf{8B}} & pruned-2of4 & $39.72_{\pm1.40}$ & $60.24_{\pm0.49}$ & $78.84_{\pm0.41}$ & $26.73_{\pm0.81}$ & $27.87_{\pm0.82}$ & $36.00_{\pm2.15}$ & $46.00_{\pm2.23}$ \\
% &  & pruned-2of4, INT4 Quant. & ------- & ------- & ------- & ------- & ------- & ------- & ------- \\
&  & pruned-2of4, FP8 Quant. & $40.70_{\pm1.41}$ & $60.04_{\pm0.49}$ & $78.63_{\pm0.41}$ & $26.57_{\pm0.81}$ & $27.60_{\pm0.82}$ & $36.20_{\pm2.15}$ & $45.80_{\pm2.23}$ \\

 \cmidrule{1-10}
\multicolumn{10}{|>{\columncolor[gray]{.8}}c|}{\textbf{Mistral Fine-tuned}} \\
\cmidrule{1-10}

\textbf{OpenHermes-2.5} & \textbf{7B} & SparseGPT & $64.54_{\pm1.37}$ & $46.78_{\pm0.50}$ & $62.36_{\pm0.48}$ & $32.13_{\pm0.85}$ & $32.73_{\pm0.86}$ & $23.40_{\pm1.90}$ & $36.00_{\pm2.15}$ \\
 \cmidrule{3-10}


\bottomrule
\end{tabulary}
\end{adjustbox}
\caption{Performance Comparison of Pruned and Distilled models on CommonsenseQA, HellaSwag, MATHQA, and OpenBookQA with lm-eval-harness framework. }
\label{app: lm-4}
\end{table*}




% ##################################################################################################


\begin{table*}
\centering
\scriptsize
\begin{adjustbox}{width=\textwidth}
\begin{tabulary}{\textwidth}{L CCCCCCCCCC}
\toprule
\multicolumn{3}{c}{\multirow{3}{*}{\textbf{Models}}} & \multicolumn{8}{c}{\textbf{MATH Dataset}} \\
\cmidrule(lr){4-11}
 & & & {\begin{tabular}[c]{@{}c@{}}\textbf{Pre-}\\\textbf{algebra}\end{tabular}}
 & {\begin{tabular}[c]{@{}c@{}}\textbf{Algebra}\end{tabular}}
 & {\begin{tabular}[c]{@{}c@{}}\textbf{Number}\\\textbf{Theory}\end{tabular}}
 & {\begin{tabular}[c]{@{}c@{}}\textbf{Counting}\\\&\\\textbf{Probability}\end{tabular}}
 & {\begin{tabular}[c]{@{}c@{}}\textbf{Geometry}\end{tabular}}
 & {\begin{tabular}[c]{@{}c@{}}\textbf{Intermediate}\\\textbf{Algebra}\end{tabular}}
 & {\begin{tabular}[c]{@{}c@{}}\textbf{Pre-}\\\textbf{calculus}\end{tabular}}
 & {\begin{tabular}[c]{@{}c@{}}\textbf{Average}\end{tabular}} \\
\cmidrule(lr){1-3}
\textbf{Model} & \textbf{Param.} & \textbf{Optimization} &  &  &  &  &  & & & \\
\midrule
\multirow{2}{*}{\textbf{SmolLM2}} & \multirow{2}{*}{\textbf{1.7B}} & None (Base)  & 8.27$_{\pm0.93}$ & 5.31$_{\pm0.65}$ & 4.81$_{\pm0.92}$ & 6.12$_{\pm1.10}$ & 3.13$_{\pm0.80}$ & 4.76$_{\pm0.71}$ & 2.75$_{\pm0.70}$ & 5.26$_{\pm0.32}$ \\
&  & None (Instruct) & 1.72$_{\pm0.44}$ & 1.10$_{\pm0.30}$ & 0.74$_{\pm0.37}$ & 0.84$_{\pm0.42}$ & 0.00$_{\pm0.00}$ & 1.77$_{\pm0.44}$ & 0.73$_{\pm0.37}$ & 1.12$_{\pm0.15}$ \\
\cmidrule{1-11}

\textbf{Minitron (Base)} & \textbf{4B} & None & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ \\

\cmidrule{1-11}

\multirow{2}{*}{\textbf{Qwen2.5 (Base)}} & \textbf{0.5B} & None & 1.49$_{\pm0.41}$ & 0.84$_{\pm0.27}$ & 0.19$_{\pm0.19}$ & 0.42$_{\pm0.30}$ & 1.04$_{\pm0.46}$ & 0.66$_{\pm0.27}$ & 1.47$_{\pm0.51}$ & 0.90$_{\pm0.13}$ \\
& \textbf{3B} & None & 1.95$_{\pm0.47}$ & 1.85$_{\pm0.39}$ & 0.93$_{\pm0.41}$ & 0.84$_{\pm0.42}$ & 1.25$_{\pm0.51}$ & 1.22$_{\pm0.37}$ & 1.28$_{\pm0.48}$ & 1.44$_{\pm0.17}$ \\

\cmidrule{3-11}

\multirow{6}{*}{\textbf{Llama-3.2}} & \multirow{3}{*}{\textbf{1B}} & None & 0.23$_{\pm0.16}$ & 0.17$_{\pm0.12}$ & 0.19$_{\pm0.19}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.10$_{\pm0.04}$ \\
& & w8a8 & 0.11$_{\pm0.11}$ & 0.00$_{\pm0.00}$ & 0.19$_{\pm0.19}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.04$_{\pm0.03}$ \\
& & FP8 & 0.46$_{\pm0.23}$ & 0.08$_{\pm0.08}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.10$_{\pm0.04}$ \\

\cmidrule{3-11}
&\multirow{3}{*}{\textbf{3B}} & None & 0.23$_{\pm0.16}$ & 0.34$_{\pm0.17}$ & 0.00$_{\pm0.00}$ & 0.21$_{\pm0.21}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.14$_{\pm0.05}$ \\
& & w8a8  & 0.23$_{\pm0.16}$ & 0.25$_{\pm0.15}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.10$_{\pm0.04}$ \\
& & FP8 & 0.23$_{\pm0.16}$ & 0.17$_{\pm0.12}$ & 0.19$_{\pm0.19}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.10$_{\pm0.04}$ \\

\cmidrule{1-11}

\multirow{2}{*}{\textbf{Llama-3.1}} & \multirow{2}{*}{\textbf{8B}} & None & 0.69$_{\pm0.28}$ & 0.08$_{\pm0.08}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.11$_{\pm0.11}$ & 0.18$_{\pm0.18}$ & 0.18$_{\pm0.06}$ \\
 &  & FP8 & 1.15$_{\pm0.36}$ & 0.17$_{\pm0.12}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.37$_{\pm0.26}$ & 0.28$_{\pm0.07}$ \\

\cmidrule{1-11}
 \multirow{3}{*}{\textbf{Mistral-v0.3}} & \multirow{3}{*}{\textbf{7B}} & None & 3.44$_{\pm0.62}$ & 0.93$_{\pm0.28}$ & 1.11$_{\pm0.45}$ & 0.63$_{\pm0.36}$ & 1.46$_{\pm0.55}$ & 0.55$_{\pm0.25}$ & 0.18$_{\pm0.18}$ & 1.26$_{\pm0.16}$ \\
& & w8a8 & 3.67$_{\pm0.64}$ & 0.93$_{\pm0.28}$ & 1.11$_{\pm0.45}$ & 0.63$_{\pm0.36}$ & 1.25$_{\pm0.51}$ & 0.44$_{\pm0.22}$ & 0.00$_{\pm0.00}$ & 1.24$_{\pm0.16}$ \\
& & w8a16 & 3.33$_{\pm0.61}$ & 0.76$_{\pm0.25}$ & 1.30$_{\pm0.49}$ & 0.63$_{\pm0.36}$ & 1.46$_{\pm0.55}$ & 0.55$_{\pm0.25}$ & 0.00$_{\pm0.00}$ & 1.20$_{\pm0.15}$ \\

% \multirow{2}{*}{\textbf{Llama-3.1}} & \multirow{2}{*}{\textbf{8B}} & None & 0.69$_{\pm0.28}$ & 0.08$_{\pm0.08}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.11$_{\pm0.11}$ & 0.18$_{\pm0.18}$ & 0.18$_{\pm0.06}$ \\
%  &  & FP8 & 1.15$_{\pm0.36}$ & 0.17$_{\pm0.12}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.37$_{\pm0.26}$ & 0.28$_{\pm0.07}$ \\

% \cmidrule{3-11}
%  \multirow{3}{*}{\textbf{Mistral}} & \multirow{3}{*}{\textbf{7B}} & Instruct-v0.3 & 3.44$_{\pm0.62}$ & 0.93$_{\pm0.28}$ & 1.11$_{\pm0.45}$ & 0.63$_{\pm0.36}$ & 1.46$_{\pm0.55}$ & 0.55$_{\pm0.25}$ & 0.18$_{\pm0.18}$ & 1.26$_{\pm0.16}$ \\
% & & w8a8  & 3.67$_{\pm0.64}$ & 0.93$_{\pm0.28}$ & 1.11$_{\pm0.45}$ & 0.63$_{\pm0.36}$ & 1.25$_{\pm0.51}$ & 0.44$_{\pm0.22}$ & 0.00$_{\pm0.00}$ & 1.24$_{\pm0.16}$ \\
% & & w8a16  & 3.33$_{\pm0.61}$ & 0.76$_{\pm0.25}$ & 1.30$_{\pm0.49}$ & 0.63$_{\pm0.36}$ & 1.46$_{\pm0.55}$ & 0.55$_{\pm0.25}$ & 0.00$_{\pm0.00}$ & 1.20$_{\pm0.15}$ \\



\cmidrule{1-11}

\multirow{13}{*}{\textbf{Qwen2}} & \multirow{6}{*}{\textbf{0.5B}} & None  & 0.69$_{\pm0.28}$ & 0.67$_{\pm0.24}$ & 0.19$_{\pm0.19}$ & 0.21$_{\pm0.21}$ & 0.21$_{\pm0.21}$ & 0.22$_{\pm0.16}$ & 0.18$_{\pm0.18}$ & 0.40$_{\pm0.09}$ \\
& & GPTQ-8  & 1.03$_{\pm0.34}$ & 0.76$_{\pm0.25}$ & 0.37$_{\pm0.26}$ & 0.00$_{\pm0.00}$ & 0.63$_{\pm0.36}$ & 0.11$_{\pm0.11}$ & 0.00$_{\pm0.00}$ & 0.48$_{\pm0.10}$ \\
& & GPTQ-4 & 0.34$_{\pm0.20}$ & 0.42$_{\pm0.19}$ & 0.00$_{\pm0.00}$ & 0.21$_{\pm0.21}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.18$_{\pm0.18}$ & 0.20$_{\pm0.06}$ \\
& & w8a16  & 1.03$_{\pm0.34}$ & 0.84$_{\pm0.27}$ & 0.00$_{\pm0.00}$ & 1.48$_{\pm0.55}$ & 0.21$_{\pm0.21}$ & 0.22$_{\pm0.16}$ & 0.00$_{\pm0.00}$ & 0.58$_{\pm0.11}$ \\
& & w8a8  & 0.69$_{\pm0.28}$ & 0.84$_{\pm0.27}$ & 0.37$_{\pm0.26}$ & 0.84$_{\pm0.42}$ & 0.63$_{\pm0.36}$ & 0.22$_{\pm0.16}$ & 0.18$_{\pm0.18}$ & 0.56$_{\pm0.11}$ \\
& & w4a16  & 0.80$_{\pm0.30}$ & 0.17$_{\pm0.12}$ & 0.19$_{\pm0.19}$ & 0.42$_{\pm0.30}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.24$_{\pm0.07}$ \\
\cmidrule{3-11}
 &\multirow{6}{*}{\textbf{1.5B}} & None  & 0.92$_{\pm0.32}$ & 0.67$_{\pm0.24}$ & 0.19$_{\pm0.19}$ & 0.21$_{\pm0.21}$ & 0.42$_{\pm0.29}$ & 0.00$_{\pm0.00}$ & 0.18$_{\pm0.18}$ & 0.42$_{\pm0.09}$ \\
& & GPTQ-8  & 1.03$_{\pm0.34}$ & 0.59$_{\pm0.22}$ & 0.19$_{\pm0.19}$ & 0.21$_{\pm0.21}$ & 0.21$_{\pm0.21}$ & 0.11$_{\pm0.11}$ & 0.18$_{\pm0.18}$ & 0.42$_{\pm0.09}$ \\
& & GPTQ-4  & 1.03$_{\pm0.34}$ & 0.51$_{\pm0.21}$ & 0.56$_{\pm0.32}$ & 0.21$_{\pm0.21}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.18$_{\pm0.18}$ & 0.40$_{\pm0.09}$ \\
& & w8a16 & 0.69$_{\pm0.28}$ & 0.08$_{\pm0.08}$ & 0.56$_{\pm0.32}$ & 0.21$_{\pm0.21}$ & 0.00$_{\pm0.00}$ & 0.11$_{\pm0.11}$ & 0.18$_{\pm0.18}$ & 0.26$_{\pm0.07}$ \\
& & w8a8  & 1.15$_{\pm0.36}$ & 0.59$_{\pm0.22}$ & 0.37$_{\pm0.26}$ & 0.00$_{\pm0.00}$ & 0.42$_{\pm0.29}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.42$_{\pm0.09}$ \\
& & w4a16 & 0.57$_{\pm0.26}$ & 0.25$_{\pm0.15}$ & 0.37$_{\pm0.26}$ & 0.42$_{\pm0.30}$ & 0.00$_{\pm0.00}$ & 0.11$_{\pm0.11}$ & 0.55$_{\pm0.32}$ & 0.32$_{\pm0.08}$ \\
\cmidrule{3-11}

 & \textbf{7B} & None  & 1.95$_{\pm0.47}$ & 1.52$_{\pm0.35}$ & 0.56$_{\pm0.32}$ & 1.69$_{\pm0.59}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.37$_{\pm0.26}$ & 0.96$_{\pm0.14}$ \\


\midrule
\multirow{10}{*}{\textbf{Qwen2.5}} & \multirow{3}{*}{\textbf{0.5B}} & None  & 0.80$_{\pm0.30}$ & 0.67$_{\pm0.24}$ & 0.00$_{\pm0.00}$ & 0.84$_{\pm0.42}$ & 0.21$_{\pm0.21}$ & 0.22$_{\pm0.16}$ & 1.65$_{\pm0.55}$ & 0.62$_{\pm0.11}$ \\
& & GPTQ-8 & 0.69$_{\pm0.28}$ & 0.59$_{\pm0.22}$ & 0.00$_{\pm0.00}$ & 0.63$_{\pm0.36}$ & 0.21$_{\pm0.21}$ & 0.22$_{\pm0.16}$ & 1.10$_{\pm0.45}$ & 0.50$_{\pm0.10}$ \\
& & GPTQ-4  & 0.57$_{\pm0.26}$ & 0.34$_{\pm0.17}$ & 0.37$_{\pm0.26}$ & 0.00$_{\pm0.00}$ & 0.00$_{\pm0.00}$ & 0.22$_{\pm0.16}$ & 0.37$_{\pm0.26}$ & 0.30$_{\pm0.08}$ \\

 \cmidrule{3-11}

& \multirow{3}{*}{\textbf{1.5B}} & None & 1.03$_{\pm0.34}$ & 0.59$_{\pm0.22}$ & 0.56$_{\pm0.32}$ & 0.42$_{\pm0.30}$ & 3.13$_{\pm0.80}$ & 1.44$_{\pm0.40}$ & 2.38$_{\pm0.65}$ & 1.24$_{\pm0.16}$ \\
& & GPTQ-8  & 0.92$_{\pm0.32}$ & 0.59$_{\pm0.22}$ & 0.37$_{\pm0.26}$ & 0.63$_{\pm0.36}$ & 2.92$_{\pm0.77}$ & 1.44$_{\pm0.40}$ & 1.65$_{\pm0.55}$ & 1.12$_{\pm0.15}$ \\
& & GPTQ-4 & 0.46$_{\pm0.23}$ & 0.42$_{\pm0.19}$ & 0.19$_{\pm0.19}$ & 1.48$_{\pm0.55}$ & 0.42$_{\pm0.29}$ & 0.55$_{\pm0.25}$ & 1.65$_{\pm0.55}$ & 0.66$_{\pm0.11}$ \\
 \cmidrule{3-11}

& \textbf{3B} & None  & 1.95$_{\pm0.47}$ & 1.43$_{\pm0.35}$ & 0.19$_{\pm0.19}$ & 1.27$_{\pm0.51}$ & 1.46$_{\pm0.55}$ & 0.33$_{\pm0.19}$ & 0.55$_{\pm0.32}$ & 1.08$_{\pm0.15}$ \\

& \textbf{7B} & None  & 0.23$_{\pm0.16}$ & 0.25$_{\pm0.15}$ & 0.00$_{\pm0.00}$ & 0.63$_{\pm0.36}$ & 0.42$_{\pm0.29}$ & 0.33$_{\pm0.19}$ & 0.37$_{\pm0.26}$ & 0.30$_{\pm0.08}$ \\

& \textbf{14B} & None & 2.41$_{\pm0.52}$ & 2.27$_{\pm0.43}$ & 2.59$_{\pm0.68}$ & 3.16$_{\pm0.80}$ & 6.26$_{\pm1.11}$ & 3.88$_{\pm0.64}$ & 7.69$_{\pm1.14}$ & 3.68$_{\pm0.27}$ \\

& \textbf{32B} & None  & 1.95$_{\pm0.47}$ & 2.61$_{\pm0.46}$ & 2.22$_{\pm0.63}$ & 3.80$_{\pm0.88}$ & 4.38$_{\pm0.94}$ & 2.55$_{\pm0.52}$ & 2.75$_{\pm0.70}$ & 2.74$_{\pm0.23}$ \\





\bottomrule
\end{tabulary}
\end{adjustbox}
\caption{Performance Comparison on MATH dataset with lm-eval-harness framework. MATH is evaluated using exact match.}
\label{tab:MATH}
\end{table*}