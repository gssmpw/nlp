
\section{Related Work}
\label{section:2}
\paragraph{Recent Surveys on SLMs}
Recent surveys provide insights into SLM advancements. Some focus on reasoning and task-specific improvements \cite{subramanian2025small, wang2024comprehensive}, while others survey SLM performance across various applications \cite{lu2024small, van2024survey}. These efforts highlight the increasing viability of SLMs as efficient alternatives to LLMs, particularly in resource-constrained settings. However, existing surveys lack a systematic benchmarking of diverse SLMs to quantify their performance across multiple reasoning benchmarks.

\paragraph{SLM Reasoning}
Recent studies have explored the reasoning abilities of SLMs, such as Hymba-1.5B \cite{dong2024hymba} and Llama-3-1B \cite{fedorov2024llama}, particularly for mathematical and logical tasks. Some approaches train SLMs directly on reasoning tasks, such as rStar-Math \cite{guan2025rstar}, which uses Monte Carlo Tree Search (MCTS) and a process preference model. Specialization through fine-tuning on specific datasets also enhances reasoning \cite{fu2023specializing} but may reduce generalization.

Another line of research uses knowledge distillation \cite{gou2021knowledge, phuong2019towards} to transfer reasoning capabilities from LLMs to SLMs \cite{zhu2024distilling}. Similarly, distillation strategies, like feedback-driven \cite{zhu2024improving} and counterfactual distillation \cite{feng2024teaching}, refine reasoning abilities and improve generalization to out-of-distribution tasks. Instruction-tuning-CoT \cite{ranaldi2024aligning} and fine-tuning on CoT-generated outputs \cite{magister2022teaching} have also shown improvements in multi-step reasoning.

Furthermore, structural modifications, such as equation-only formats \cite{kim2024small} and synthetic data training (e.g., Orca-Math \cite{mitra2024orca}), have also improved performance. Efficient architectures like Phi-3-mini \cite{abdin2024phi} match the performance of larger models while being deployable on edge devices. Self-correction mechanisms like SCORE \cite{zhang2024small} enhance reasoning reliability, while models like Orca 2 \cite{mitra2023orca} and OpenELM \cite{mehta2024openelm} optimize efficiency through improved training strategies. In this paper, we evaluate a broad spectrum of SLMs, including trained-from-scratch, via different methods and their quantized, pruned, and distilled variants.


\paragraph{Reasoning Evaluation} 
Assessing reasoning in language models is challenging due to the complexity of evaluating open-ended, multi-step responses. Various evaluation methods have been explored, including rule-based parsing, human evaluation, and LLM-as-a-Judge frameworks. Parsing-based methods provide precise accuracy but struggle with models that generate responses in unpredictable formats, often penalizing correct answers due to formatting inconsistencies. Human evaluation remains the gold standard but is expensive, time-consuming, and prone to subjectivity.

Recently, LLM-as-a-Judge has gained popularity as an alternative, with studies showing that models like GPT-4 Turbo and Llama-3.1 70B align closely with human judgments, validating their effectiveness in evaluation tasks \cite{thakur2024judging}. LLM-based assessments have been particularly effective for structured tasks, where models like InstructGPT and ChatGPT produce results comparable to expert human raters \cite{chiang2023can}. Similar trends are observed in summarization and grammatical error correction, where GPT-4 demonstrates strong agreement with human rankings \cite{sottana2023evaluation}. Beyond accuracy, studies on NLG evaluation \cite{wang2023chatgpt} highlight ChatGPTâ€™s strong correlation with human assessments in creative text generation. Recent surveys \cite{gu2024survey, chang2024survey} further validate LLM-as-a-Judge as a reliable benchmarking tool.

However, these evaluation methods for assessing reasoning in SLMs have not been systematically compared. It remains unclear which approach best reflects true reasoning capabilities across different models. Our work addresses this gap by assessing multiple evaluation methods, comparing LLM-as-a-Judge against human evaluation, rule-based parsing, and widely used open-source frameworks like lm-eval-harness. Then, we systematically benchmark SLMs across multiple reasoning tasks using a metric that closely aligns with human evaluation. 

