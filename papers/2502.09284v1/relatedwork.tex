\section{Related Works}
The availability  of instruction-tuned LLMs~\cite{llama3modelcard, touvron2023llama, jiang2023mistral} open a new research direction for speech processing by connecting speech directly to these multi-task models. \citet{chen2023salm} proposed multitask speech-language modeling with unified LLM framework that shows in-context learning ability. \citet{yu2023connecting} utilized three approaches for adapting speech to text modality: Fully Connected Linear Layers following ~\cite{houlsby2019parameter} adapter method, multi-head cross attention mechanism described in \cite{vaswani2017attention}, and query transformer~\cite{li2023blip}. For processing speech input, they utilized two models: Whisper Large-v2~\cite{radford2023robust} and BEATS~\cite{pmlr-v202-chen23ag}. The SpeechVerse~\cite{das2024speechverse} framework used WavLM-based speech encoder interfaced with a Flan-T5-XL~\cite{JMLR:v25:23-0870} language model.
In a another study, \citet{ma2024embarrassingly} demonstrated the sufficiency of a single linear layer for speech-LLM integration in ASR, albeit with limited exploration beyond this task.
Speech as language modeling was also studied in SpeechGPT \citep{zhang2023speechgpt} which integrates both speech and text modalities. The model incorporates a speech tokenizer that converts raw audio waveforms into discrete speech tokens, enabling efficient processing within the transformer architecture. Through multi-task fine-tuning on downstream tasks such as ASR, translation, and generation, the model demonstrates remarkable versatility. Qwen2-Audio~\cite{Qwen2-Audio}, designed as a general-purpose audio understanding model, exhibits broad applicability across various audio-related tasks. The model employs self-supervised learning techniques, such as masked audio modeling and contrastive learning, to capture rich audio representations.

Table \ref{tab:related_works_comparison} summarizes the features of most relevant related works. Note that our proposed model, SparQLe, is the only one that relies exclusively on SSL features (i.e. HuBERT) as input, and a simple adapter between the frozen speech encoder and LLM; previous approaches relied on complex encoder that have already been aligned with text through supervised training, such as Whisper, in addition to complex adaptation mechanism.