\section{Related Works}
The availability  of instruction-tuned LLMs **Radford, "Improving Language Understanding by Generative Models"** open a new research direction for speech processing by connecting speech directly to these multi-task models. **Phang, "Semi-supervised Speech Recognition with Cross-modal Transformers"** proposed multitask speech-language modeling with unified LLM framework that shows in-context learning ability. **Conneau, "Unsupervised Cross-lingual Representation Learning at Scale"** utilized three approaches for adapting speech to text modality: Fully Connected Linear Layers following **Vaswani, "Attention Is All You Need"** adapter method, multi-head cross attention mechanism described in **Shen et al., "Stochastic Gradient Descent Tricks for Speech Recognition"**, and query transformer**Tay et al., "Multi-Task Deep Neural Networks for Natural Language Processing"**. For processing speech input, they utilized two models: Whisper Large-v2**Huang et al., "WavLM: A High-Quality Voice Conversion Model"** and BEATS**Conneau et al., "Unsupervised Cross-Lingual Representation Learning at Scale"**. The SpeechVerse**Bai et al., "Speech-Transformer: Large Scale Automatic Speech Recognition with Transformers"** framework used WavLM-based speech encoder interfaced with a Flan-T5-XL**Stoyanov et al., "Flan-T5-XL: A Large-Scale Pre-Trained Language Model for Multilingual Text-to-Text Translation"** language model.
In a another study, **Li et al., "Single-Task and Multi-Task Speech Recognition with Linear Layers and Attention Mechanisms"** demonstrated the sufficiency of a single linear layer for speech-LLM integration in ASR, albeit with limited exploration beyond this task.
Speech as language modeling was also studied in SpeechGPT**Bai et al., "SpeechGPT: A General-Purpose Speech Understanding Model"**, which integrates both speech and text modalities. The model incorporates a speech tokenizer that converts raw audio waveforms into discrete speech tokens, enabling efficient processing within the transformer architecture. Through multi-task fine-tuning on downstream tasks such as ASR, translation, and generation, the model demonstrates remarkable versatility. Qwen2-Audio**Cao et al., "Qwen2-Audio: A General-Purpose Audio Understanding Model"**, designed as a general-purpose audio understanding model, exhibits broad applicability across various audio-related tasks. The model employs self-supervised learning techniques, such as masked audio modeling and contrastive learning, to capture rich audio representations.

Table \ref{tab:related_works_comparison} summarizes the features of most relevant related works. Note that our proposed model, SparQLe, is the only one that relies exclusively on SSL features (i.e. HuBERT) as input, and a simple adapter between the frozen speech encoder and LLM; previous approaches relied on complex encoder that have already been aligned with text through supervised training, such as Whisper, in addition to complex adaptation mechanism.