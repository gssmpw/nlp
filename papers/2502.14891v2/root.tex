%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{amsmath, amssymb, mathtools}
\usepackage{subfigure}
\usepackage{amsfonts,amssymb}
\usepackage[table,xcdraw]{xcolor}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{svg}
%% HYPERLINK
\makeatletter
\let\NAT@parse\undefined
\makeatother
\usepackage[colorlinks=true, citecolor=green, linkcolor=red]{hyperref}  %hyperref still needs to be put at the end!
\usepackage{cite}
\usepackage{algorithm}
\usepackage{algorithmic}

\title{\LARGE \bf
CoDiff: Conditional Diffusion Model for  Collaborative 3D Object Detection
}


\author{Zhe Huang$^{1\dagger}$, Shuo Wang$^{1\dagger}$, Yongcai Wang$^{1*}$, Deying Li$^{1}$, Lei Wang$^{2}$
\thanks{*Corresponding author, $\dagger$ Equal contribution.}% <-this % stops a space
\thanks{$^{1}$Renmin University of China, China.
        {\tt\small \{huangzhe21,shuowang18,ycw,deyingli\}@ruc.edu.cn}}%   
\thanks{$^{2}$University of Wollongong, Australia.
        {\tt\small lei\_wang@uow.edu.au}}%   
}

% 

\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

Collaborative 3D object detection holds significant importance in the field of autonomous driving, as it greatly enhances the perception capabilities of each individual agent by facilitating information exchange among multiple agents. However, in practice, due to pose estimation errors and time delays, the fusion of information across agents often results in feature representations with spatial and temporal noise, leading to detection errors. Diffusion models naturally
have the ability to denoise noisy samples to the ideal data,
which motivates us to  explore the use of diffusion models to address the noise problem between multi-agent systems. 
In this work, we propose CoDiff, a novel robust collaborative perception framework that leverages the potential of diffusion models to generate more comprehensive and clearer feature representations. To the best of our knowledge, this is the first work to apply diffusion models to multi-agent collaborative perception. Specifically, we project high-dimensional feature map into the latent space of a powerful pre-trained autoencoder. Within this space, individual agent information serves as a condition to guide the diffusion model's 
sampling. This process denoises coarse feature maps and progressively refines the fused features.
Experimental study on both simulated and real-world datasets demonstrates that the proposed framework CoDiff consistently outperforms existing relevant methods in terms of the collaborative object detection performance, and exhibits 
highly desired robustness when the pose and delay information of agents is with high-level noise. 
The code is released at \url{https://github.com/HuangZhe885/CoDiff}.


\end{abstract}


\section{INTRODUCTION}
3D object detection \cite{pointpillars, 3d, pointnet++,votenet,votenet++,huang} is a fundamental task in autonomous driving, primarily focusing on the localization and identification of specific vehicles in real-world scenarios \cite{aerial}. It is also attracting considerable interest in domains such as as drones, robots, and the metaverse \cite{I1,I2,D1,D2,D3}. However, 3D object detection with a single agent has inherent limitations, such as occlusion and distant objects 
\cite{qi2017pointnet,huang,huang2}. Recently, researchers have addressed these limitations by enabling multiple agents to share complementary perception information through communication, leading to more comprehensive and accurate detection.
Cooperative 3D object detection \cite{HEAL, where2comm, roco,SycNet} is an important field that has made significant progress  in terms of high-quality datasets and methods. However, it also faces many challenges, including time delays \cite{CoBEVFlow}, pose misalignment \cite{roco}, communication bandwidth limitations \cite{where2comm}, and adversarial attacks \cite{adversarial}. 
% This work focuses on mitigating the negative effects of inaccuracies in information sharing between multiple agents.

To effectively share information, multiple agents need to transmit real-time, precisely aligned features to synchronize their data within a consistent timestamp and spatial coordinate system, which is the foundation for maintaining effective collaboration. However, in real-world scenarios, this information may suffer from time delays and spatial misalignment due to the following reasons: 1) Unstable communication between agents, such as congestion and interruptions, leading to time asynchrony issues; 2) The 6-degree-of-freedom (DoF) poses estimated by each agent’s localization module are imperfect, resulting in unavoidable relative pose errors. 
These factors severely affect the reliability and quality of information exchange between agents, leading to inaccurate features during fusion.
Some previous studies have considered various methods to address these issues. CoAlign \cite{CoAlign} introduces a novel agent-object pose graph modeling approach to enhance pose consistency between collaborative agents. RoCo \cite{roco} designs an object matching and optimization strategy to correct pose errors. V2XViT \cite{v2x-vit} incorporates latency as an input for feature compensation, while CoBEVFlow \cite{CoBEVFlow} introduces the concept of feature flow to handle irregular time delays. 
However, in practice, time delays and pose errors may usually coexist and entangle with each other, and existing methods are unable to address both issues simultaneously. As a result, they still produce feature representations with harmful noise, hindering current collaborative perception systems from reaching their full potential.

\begin{figure}[t]
\centering
\includegraphics[width=1\columnwidth]{img/diffusion.png} 
\caption{Illustration of the proposed robust collaborative perception system with the conditional diffusion model, where  $q$ is the diffusion process and $p_{\theta}$ is the reverse process.}
\label{Fig1}
\end{figure}

To address these limitations, we propose a novel hybrid collaborative framework called CoDiff, which leverages Conditional Diffusion Probabilistic Models (CDPM) to address spatial and temporal noise issues in information sharing, thereby improving the quality of multi-agent fused features.
In CoDiff, we treat the time delays and pose errors between multiple agents as a unified noise to be learned and handled. We employed diffusion models for feature fusion, replacing the existing feature fusion methods based on regression models and attention mechanisms.
CoDiff is primarily consists of  two components: perception compression and conditional generation. The perception compression module compresses features into a latent space which makes the computation more efficient while well retaining the perception information. High-resolution feature generation is then performed within this space, well reducing the complexity of both training and sampling processes . 
The conditional generation module takes the features from different collaborating  agents  as input and incrementally refine the noisy  features, as illustrated in Fig. \ref{Fig1}.
CoDiff has three main advantages: 
i) It can adaptively establish relationships between single-agent features and the final fused features, contributing a more compact perception representation; 
ii) It is capable of learning the distribution resulted from the mixture of different  types of noise, mitigating the impact of time delays and pose errors on collaborative information; 
iii) Unlike existing methods that design specialized fusion modules, our generative approach naturally prevents the introduction of additional noise into the features.

We conduct extensive experiments on both simulation and real-world datasets, including V2XSet\cite{v2x-vit}, OPV2V \cite{opv2v} and DAIR-V2X\cite{dair}. 
Results show that CoDiff consistently achieves the best performance in the task of collaborative 3D object detection with the presence of pose errors and time delays.  In summary, the main contributions of this work are:

\begin{itemize}
\item We propose CoDiff, a novel LiDAR-based robust multi-agent collaborative 3D object detection framework that addresses the problem of noisy feature map fusion between multiple agents. To the best of our knowledge, CoDiff is the first work to use a diffusion model to solve time delays and pose inaccuracies in collaborative perception.
\item The proposed CoDiff conducts perception compression to obtain a low-dimensional and computationally  more efficient latent space. Within this space, CoDiff generates and refines multi-agent features, ultimately producing a globally fused feature map.
\item Extensive experiments have shown that CoDiff achieves more accurate and robust 3D object detection performance even in scenarios with significant pose noise and large time delays.
\end{itemize}




\section{Related Work}


\textbf{Diffusion Models for Perception Tasks.}
Diffusion models \cite{D1,D2,D3,D4}, known for their powerful denoising capabilities, have attracted considerable interest in domains such as natural language processing, text transformation, and multimodal data production. Recently, an increasing number of researchers have started investigating the use of diffusion models in perception tasks. DDPM-Segmentation \cite{DDPMSegmentation} is the initial study that uses diffusion models for semantic segmentation. On the other hand, DiffusionDet \cite{DiffusionDet} considers object detection as a noise-to-box task, aiming to provide accurate object bounding boxes by progressively denoising randomly generated proposals. Diffusion-SS3D \cite{Diffusion-SS3D} utilises the powerful properties of diffusion models in a semi-supervised 3D object detection framework, with the goal of generating more dependable pseudo-labels. Expanding upon this, Diff3DETR \cite{diff3detr} extends the approach of Diffusion-SS3D by proposing the first diffusion-based DETR framework.
In addition, DifFUSER \cite{diffbev} utilises the noise reduction capabilities of diffusion models to address noise for multimodal fusion (such as image, text, video).
In this work, we are motivated to further explore the potential of employing
the diffusion model to generate a high-quality representation, proposing the first diffusion-based multi-agent cooperative perception 3D detection method.

\textbf{Collaborative 3D Object Detection.}
Collaborative 3D object detection \cite{syncnet,CoAlign,cobevt,v2x-vit,opv2v} is a particular application in multi-agent systems that allows multiple agents to share information to overcome the inherent limitations of single-agent perception.
Many researchers have designed various methods to enhance the perception performance and robustness of collaborative perception systems. 
In terms of perception performance, the study in the literature  \cite{cobevt,v2x-vit,opv2v,where2comm} implemented a transformer architecture to aggregate information from different agents.
In terms of robustness, CoBEVFlow\cite{CoBEVFlow} creates a synchrony-robust collaborative system that aligns asynchronous collaboration messages sent by various agents using motion compensation. HEAL\cite{HEAL} smoothly integrates  emerging heterogeneous agent types into collaborative perception tasks. CoAlign \cite{CoAlign} uses an agent-object pose graph to address pose inaccuracies, and RoCo \cite{roco} tackles  pose errors with object matching and graph optimization techniques.
However, we observe that these methods tend to address only a single aspect of the problem and fail to ensure both perceptual performance and robustness of the model under multiple noise conditions. In contrast, our CoDiff can simultaneously address the effects of various types of noise, such as time delays and pose errors, thereby further improving the accuracy of collaborative 3D object detection.



\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{img/CoDiffusion.png} 
\caption{ 
Overview of the proposed  CoDiff framework. Single-agent features are transmitted to the ego vehicle. These features are concatenated and sent into the compression model $\mathcal E$, before entering the latent diffusion model. 
The diffusion model generates denoised features in latent space, which are then decompressed to obtain the denoised multi-agent fused features.
}
\label{fig:all}
\end{figure*}

\section{Problem Definition and Preliminaries}
\subsection{Collaborative 3D object detection.}
Assuming there are $N$ agents in the scene, given the point clouds of these agents as input, the goal of collaborative 3D object detection is to classify and locate the 3D bounding boxes of objects within the scene through the cooperation of the agents. This process can be formally expressed as:
\begin{subequations}
\begin{align}
	F_{i}&=\Phi_{Enc} \left( \mathcal{X}_{i}\right),\quad i=1,\cdots,N,  \\
 F^{\prime }_{i}&=\Phi_{Agg} \left( F_{i},\{ \mathcal{M}_{j\rightarrow i}\}_{j=1,2,...,N; j\neq{i}} \right), \\
 \mathcal{O}_i&=\Phi_{Dec} \left( F^{\prime }_{i}\right).  
\end{align}
\end{subequations}
Within the collaborative perception framework, each agent 
$i$ can extract features $F_{i}$ from the raw point cloud  observations $\mathcal{X}_{i}$ using an encoding network as in Step (1a).
$\mathcal{M}_{j\rightarrow i}$ in Step (1b) denotes the collaboration message sent from agent $j$ to agent $i$.
After receiving all the $(N-1)$ messages, agent $i$ will aggregate  its feature $F_{i}$ with these messages via a fusion network, producing a fused feature $F'_{i}$ as in Step (1b). Finally, Step (1c)  uses a decoding  network  to convert $F'_{i}$ to the final perception output $\mathcal{O}_{i}$. 
As seen above, provided  that all networks are well trained, the fused features $ F^{\prime }_{i}$ in Step (1b) will heavily rely on the quality of the information $\mathcal{M}_{j\rightarrow i}$ shared between the agents. However, in the real world, time asynchrony and pose estimation errors between agents are inevitable, leading to issues such as time delays and inaccurate pose estimates. These problems make it difficult to achieve accurate feature fusion. In this paper, we propose a novel approach that utilizes a generative model $\Phi_{diff}$ to replace step 1(b) by generating more fine-grained and detailed features $ F_{i}^{\prime}=\Phi_{diff} \left( F_{i},\left\{ M_{j\rightarrow i} \right\}_{j=1,2,...,N; j\neq{i}} \right)$ , thereby achieving more precise perception results.



\subsection{Diffusion Model.} 
Diffusion models are generative models inspired by nonequilibrium thermodynamics \cite{non,markov}, designed to learn a data distribution $P\left( x \right)$ by gradually denoising a normally distributed variable. 
The diffusion forward process transforms an initial data sample
$x_0$ into a noisy sample $x_t$ at time $t$ by introducing noise that is determined by the noise variance schedule $\beta_{t}$. For convenience, we denote a series of constant: $\alpha_{t} :=1-\beta_{t} ,\bar{\alpha}_{t} :=\prod_{s=1}^{t} a_{s}$. The forward process to generate a noisy
sample $x_t$ from $x_0$ can be defined by
\begin{eqnarray}
q\left( x_{t}|x_{0} \right) =\mathcal{N}\left( x_{t};\sqrt{\bar{\alpha}} x_{0},\left( 1-\bar{\alpha}_{t} \right) \mathbf{I} \right), 
\end{eqnarray}
where $\mathcal{N}$ denotes a Gaussian distribution and  $\mathbf{I}$ is an identity matrix. A neural network is then trained to
reverse this diffusion process. This network can be interpreted as an equally weighted
sequence of denoising autoencoders $\epsilon_{\theta} \left( x_{t},t \right) ;t=1, ..., T$, which are trained to predict a denoised variant of their input $x_t$, where $x_t$ is a noisy version of the input $x$. The corresponding
objective can be simplified to
\begin{eqnarray}
L_{DM}=\mathbb{E}_{x,\epsilon \sim \mathcal{N}\left( 0,1 \right) ,t}\left[ \| \epsilon - \epsilon_{\theta} \left( x_{t},t \right) \|_{2}^{2} \right]
\label{eqn:dm}
\end{eqnarray}
Diffusion  models  can learn diverse data distribution in multiple domains. We aim to leverage this advantage and design more suitable generative conditions to address the issues of time delays and pose inaccuracies between agents in collaborative perception.

\section{Our Proposed Method}
CoDiff is designed as a generative model specifically multi-agent 3D object detection which is capable of simultaneously addressing pose errors and time delays. Figure \ref{fig:all} shows the overall architecture of CoDiff.
It consists of two key ideas. Firstly, we propose a perceptual compression model to compress high-dimensional feature maps into a low-dimensional, latent space. This compression focuses on the essential semantic information of the data. Secondly, we design a conditional diffusion model that treats feature maps from different agents as conditions, combining the model's generative capabilities with these conditions to produce more accurate and fine-grained multi-agent fused features. These features are ultimately fed into the detection head to achieve precise 3D object detection results.





\subsection{Perceptual Compression Model}
Most feature fusion methods employ autoregressive, attention-based transformer models, which are trained in high-dimensional feature spaces \cite{v2x-vit, where2comm}. However, we observe that diffusion models incur costly function evaluations during high-resolution feature sampling, presenting significant challenges in terms of computation time and energy resources. Therefore, we aim to perform sampling  in a lower-dimensional space, where diffusion models can learn within this  perceptually equivalent, lower-dimensional space, thereby reducing computational complexity. 
This approach has demonstrated its feasibility in the domain of high-resolution image synthesis \cite{latent_diffusion}.
Based on the above analysis, we propose a perceptual compression model that abstracts high-dimensional feature details into a lower-dimensional latent space. This space is more suitable for likelihood-based generative models because it makes computations more efficient and enabling a focus on the important semantic aspects of the data.

Our perceptual compression model is based on a neural network autoencoder architecture. This autoencoder comprises an encoder and a decoder, focusing on compressing and reconstructing input tensors along the channel dimension. The encoder employs a series of convolutional layers, batch normalization, and ReLU activation functions to compress high-dimensional input tensors into a lower-dimensional latent space. The decoder, using a similar structure, aims to reconstruct a low-dimensional latent representations back to the original dimensionality. This autoencoder effectively reduces computational overhead while preserving key features of the data.




Precisely, given a feature map $F_i \in \mathbb{R}^{H\times W\times C}$, where $H,W,C$ represents height, width and channels, respectively. The encoder ${\mathcal E}$ encodes $F_i$ into a latent representation $z={\mathcal E} \left( F_{i} \right)$ using a compression rate $\tau$, and the decoder $\mathcal D$ reconstructs the feature from the latent space using the same compression rate.
Therefor, the process of compressing features $F_i$ into the latent space is:
\begin{eqnarray}
\tilde{F}_{i} =\mathcal D\left( z \right) =\mathcal D\left( {\mathcal E}\left( F_{i} \right) \right)
\end{eqnarray}
The perceptual compression loss $\mathcal{L}_{cmp}$ defined as follows
\begin{eqnarray}
\mathcal{L}_{cmp}=D_{KL}\left( F_{i}||\tilde{F}_{i} \right)
\end{eqnarray}
where $D_{KL}(p||q)$ denotes the Kullback-Leibler (KL) \cite{KL} divergence of distribution $p$ from distribution $q$.

\subsection{Latent Diffusion  Model}
With the perceptual compression model trained in the previous step, the diffusion model can now be trained in this lower-dimensional, computationally more efficient latent space. 
Our goal is to generate noise-free features $ F^{\prime }_{i}$ (Step (1b)) through this latent space and the decoder $\mathcal D$, which should represent the aggregation of features from multiple agents. 
We can decode the latent representation of features  $ F^{\prime }_{i}$ from the latent space back to the original feature space with a single pass through $\mathcal D$ during training. 


Inspired by literature \cite{latent_diffusion}, diffusion models can leverage conditional mechanisms $p\left( x|y \right)$ to control the target generation process, thereby improving the model's accuracy. 
We find that in multi-agent collaborative perception, the information from individual agents can be  naturally used as conditional inputs to generate the final fused features. Thus, the generation of multi-agent features can be controlled by manipulating the input conditions $y$.

\textbf{The Design of Condition. }
In practice, we select the feature information from $N-1$ individual agents as the condition $y_{cond}$. These informations are obtained by generating features $F_{j}$ through point cloud encoders, which are then transmitted to the Ego agent $i$, denoted as $\mathcal{M}_{j\rightarrow i}$. 
We strictly follow the standard DPM model to add noise, while the difference
is that we employ condition-modulated denoising, which is shown in Figure \ref{fig:all}. By progressively denoising the samples, we hope that the conditional diffusion model can act as a more flexible condition generator, aiding in learning fine-grained object information, such as precise boundaries and highly detailed shapes, especially under noisy conditions. 

Given noisy feature $z_t$ and condition $y_{cond}=\{ \mathcal{M}_{j\rightarrow i}\}$ at time step $t$, $z_t$ is further encoded and interacts with $y_{cond}$ through concatenation. This form $p\left( z_t |y_{cond} \right)$ can
be implemented with a conditional denoising autoencoder $\epsilon_{\theta} \left( z_{t},t,y_{cond} \right)$. 
A Unet-style structure \cite{unet}, whose components include an encoder and a decoder, severs as the denoising network.
Based on feature-conditioning pairs, we then learn the conditional LDM via
\begin{eqnarray}
L_{LDM}:=\mathbb{E}_{\mathcal E\left( x \right),y,\epsilon \sim \mathcal{N}\left( 0,1 \right) ,t}\left[ \parallel \epsilon -\epsilon_{\theta} \left( z_{t},t,y_{cond} \right) \parallel_{2}^{2} \right]
\label{eqn:diffusion_loss}
\end{eqnarray}
The definition of a symbols can be refer to the equation (\ref{eqn:dm}).

\subsection{Detection Head}
After generating the  feature map $ F^{\prime }_{i}$,
we decode them into the detection layer $\Phi_{Dec}\left( \cdot \right)  $ to obtain final object detections $\mathcal{O}_{i}$. 
The classification output is the confidence score of being an vehicle or background for each anchor box, the regression output is $\left(x,y,z,w,l,h,\theta \right)  $, representing the centroid coordinates, size and yaw of the anchor boxes, respectively.



\subsection{Training details and loss function}
Following common practice \cite{latent_diffusion, condition1,condition2}, we separate training into two distinct stages. First,  we train a perception compression model to provide a low-dimensional representation space. Then, we train the latent diffusion model in this space.
In the detection task, we use  the cross entropy and weighted smooth L1 loss  for the classification $\mathcal{L}_{cls}$ and regression $\mathcal{L}_{reg}$. 
%  \begin{eqnarray}
% \mathcal{L}_{detection}=\lambda_{cls} \mathcal{L}_{cls}+\lambda_{reg} \mathcal{L}_{reg}
% \label{eqn:detection_loss}
% \end{eqnarray}
The total loss is the weighted sum of the diffusion loss in Equation (\ref{eqn:diffusion_loss})  and the detection loss.
 \begin{eqnarray}
\mathcal{L}_{total}=\mathcal{L}_{LDM}+\lambda_{cls} \mathcal{L}_{cls}+\lambda_{reg} \mathcal{L}_{reg}
\end{eqnarray}




\begin{table*}[]
 \caption{3D object detection performance on  DAIR-V2X\cite{dair}, V2XSet\cite{v2x-vit} and OPV2V \cite{opv2v} datasets. 
Experiments show that CoDiff achieves the overall best performance under various noise levels. The symbol '-' means the results are unavailable. }
 \setlength{\tabcolsep}{2.5pt}
\begin{tabular}{llllllllllllllll}
\hline
\multicolumn{1}{l|}{Dataset}       & \multicolumn{5}{c}{DAIR-V2X}                                                                                                                                                                                          & \multicolumn{5}{c}{V2XSet}                                                                                                                                                                                            & \multicolumn{5}{c}{OPV2V}                                                                                                                                                                        \\ \hline
\multicolumn{1}{l|}{Method/Metric} & \multicolumn{15}{c}{AP@0.5 ↑}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \\ \hline
\multicolumn{1}{l|}{Noise Level ($\sigma_{t} /\sigma_{r} \left( m/^{\circ }\right) $)}   & 0.0/0.0                              & 0.1/0.1                              & 0.2/0.2                              & 0.3/0.3                              & \multicolumn{1}{l|}{0.4/0.4}                              & 0.0/0.0                              & 0.1/0.1                              & 0.2/0.2                              & 0.3/0.3                              & \multicolumn{1}{l|}{0.4/0.4}                              & 0.0/0.0                              & 0.1/0.1                              & 0.2/0.2                              & 0.3/0.3                              & 0.4/0.4                              \\ \hline
\multicolumn{1}{l|}{F-Cooper \cite{f-cooper}}      & 73.4                                 & 70.2                                 & 72.3                                 & 69.5                                 & \multicolumn{1}{l|}{70.5}                                 & 78.3                                 & 77.7                                 & 76.3                                 & 73.5                                 & \multicolumn{1}{l|}{71.2}                                 & 83.4                                 & 82.6                                 & 78.8                                 & 72.3                                 & 68.1                                 \\
\multicolumn{1}{l|}{V2VNet \cite{v2vnet}}        & 66.0                                 & 65.7                                 & 65.5                                 & 64.9                                 & \multicolumn{1}{l|}{64.6}                                 & 87.1                                 & 86.6                                 & 86.0                                 & 84.3                                 & \multicolumn{1}{l|}{83.2}                                 & 94.2                                 & 93.9                                 & 93.8                                 & 93.0                                 & 92.9                                 \\
\multicolumn{1}{l|}{Self-ATT \cite{opv2v}}      & 70.5                                 & 70.4                                 & 70.3                                 & 69.8                                 & \multicolumn{1}{l|}{69.5}                                 & 87.6                                 & 87.3                                 & 86.8                                 & 85.9                                 & \multicolumn{1}{l|}{85.4}                                 & 94.3                                 & 93.9                                 & 93.3                                 & 92.1                                 & 91.5                                 \\
\multicolumn{1}{l|}{V2X-ViT \cite{v2x-vit}}       & 70.4                                 & 70.3                                 & 70.0                                 & 68.5                                 & \multicolumn{1}{l|}{68.9}                                 & 91.0                                 & 90.7                                 & 90.1                                 & 88.7                                 & \multicolumn{1}{l|}{86.9}                                 & 94.6                                 & 93.9                                 & 94.2                                 & 93.3                                 & 93.1                                 \\
\multicolumn{1}{l|}{CoBEVFlow \cite{CoBEVFlow}}     & 73.8                                 & -                                    & 73.2                                 & -                                    & \multicolumn{1}{l|}{70.3}                                 & -                                    & -                                    & -                                    & -                                    & \multicolumn{1}{l|}{-}                                    & -                                    & -                                    & -                                    & -                                    & -                                    \\
\multicolumn{1}{l|}{CoAlign \cite{CoAlign}}       & 74.6                                 & 74.5                                 & 73.8                                 & 72.4                                 & \multicolumn{1}{l|}{72.0}                                 & 91.9                                 & 91.6                                 & 90.9                                 & 89.7                                 & \multicolumn{1}{l|}{88.1}                                 & 96.6                                 & 96.5                                 & 96.2                                 & 95.9                                 & 95.8                                 \\
\multicolumn{1}{l|}{RoCo \cite{roco}}          & 76.3                                 & 74.9                                 & 74.8                                 & 73.1                                 & \multicolumn{1}{l|}{73.3}                                 & 91.9                                 & 91.7                                 & 91.0                                 & 89.6                                 & \multicolumn{1}{l|}{90.0}                                 & 96.6                                 & 96.6                                 & 96.6                                 & 95.8                                 & 95.7                                 \\ \hline
\multicolumn{1}{l|}{Ours (CoDiff)} & {\color[HTML]{FE0000} \textbf{77.4}} & {\color[HTML]{FE0000} \textbf{77.0}} & {\color[HTML]{FE0000} \textbf{75.4}} & {\color[HTML]{FE0000} \textbf{73.6}} & \multicolumn{1}{l|}{{\color[HTML]{FE0000} \textbf{73.4}}} & {\color[HTML]{FE0000} \textbf{92.1}} & {\color[HTML]{FE0000} \textbf{92.0}} & {\color[HTML]{FE0000} \textbf{91.0}} & {\color[HTML]{FE0000} \textbf{90.0}} & \multicolumn{1}{l|}{{\color[HTML]{FE0000} \textbf{90.0}}} & {\color[HTML]{FE0000} \textbf{97.0}} & {\color[HTML]{FE0000} \textbf{97.0}} & {\color[HTML]{FE0000} \textbf{96.7}} & {\color[HTML]{FE0000} \textbf{96.5}} & {\color[HTML]{FE0000} \textbf{96.0}} \\ \hline
\multicolumn{11}{l}{}                                                                                                                                                                                                                                                                                                                                                                                                                                                              & \multicolumn{5}{l}{}                                                                                                                                                                             \\ \hline
\multicolumn{1}{l|}{Method/Metric} & \multicolumn{15}{c}{AP@0.7 ↑}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \\ \hline
\multicolumn{1}{l|}{Noise Level ($\sigma_{t} /\sigma_{r} \left( m/^{\circ }\right) $)}   & 0.0/0.0                              & 0.1/0.1                              & 0.2/0.2                              & 0.3/0.3                              & \multicolumn{1}{l|}{0.4/0.4}                              & 0.0/0.0                              & 0.1/0.1                              & 0.2/0.2                              & 0.3/0.3                              & \multicolumn{1}{l|}{0.4/0.4}                              & 0.0/0.0                              & 0.1/0.1                              & 0.2/0.2                              & 0.3/0.3                              & 0.4/0.4                              \\ \hline
\multicolumn{1}{l|}{F-Cooper \cite{f-cooper}}      & 55.9                                 & 53.4                                 & 55.2                                 & 53.7                                 & \multicolumn{1}{l|}{54.2}                                 & 48.6                                 & 47.3                                 & 46.0                                 & 44.1                                 & \multicolumn{1}{l|}{43.4}                                 & 60.2                                 & 55.3                                 & 50.4                                 & 46.8                                 & 41.2                                 \\
\multicolumn{1}{l|}{V2VNet \cite{v2vnet}}        & 48.6                                 & 48.4                                 & 48.3                                 & 48.0                                 & \multicolumn{1}{l|}{47.8}                                 & 64.6                                 & 63.5                                 & 62.0                                 & 57.2                                 & \multicolumn{1}{l|}{56.2}                                 & 85.4                                 & 85.1                                 & 84.8                                 & 84.0                                 & 83.7                                 \\
\multicolumn{1}{l|}{Self-ATT \cite{opv2v}}      & 52.2                                 & 52.1                                 & 52.0                                 & 51.7                                 & \multicolumn{1}{l|}{51.7}                                 & 67.6                                 & 67.2                                 & 66.2                                 & 65.4                                 & \multicolumn{1}{l|}{65.1}                                 & 82.7                                 & 81.6                                 & 80.4                                 & 79.5                                 & 78.0                                 \\
\multicolumn{1}{l|}{V2X-ViT \cite{v2x-vit}}       & 53.1                                 & 53.2                                 & 52.9                                 & 52.9                                 & \multicolumn{1}{l|}{52.5}                                 & 80.3                                 & 79.1                                 & 76.8                                 & 74.2                                 & \multicolumn{1}{l|}{71.8}                                 & 85.6                                 & 85.0                                 & 85.1                                 & 84.3                                 & 84.1                                 \\
\multicolumn{1}{l|}{CoBEVFlow \cite{CoBEVFlow}}     & 59.9                                 & -                                    & 57.9                                 & -                                    & \multicolumn{1}{l|}{56.0}                                 & -                                    & -                                    & -                                    & -                                    & \multicolumn{1}{l|}{-}                                    & -                                    & -                                    & -                                    & -                                    & -                                    \\
\multicolumn{1}{l|}{CoAlign \cite{CoAlign}}       & 60.4                                 & 60.0                                 & 58.8                                 & 58.3                                 & \multicolumn{1}{l|}{57.9}                                 & 80.5                                 & 79.4                                 & 77.3                                 & 75.1                                 & \multicolumn{1}{l|}{73.0}                                 & 91.2                                 & 90.8                                 & 90.0                                 & 89.4                                 & 88.9                                 \\
\multicolumn{1}{l|}{RoCo \cite{roco}}          & 62.0                                 & 59.8                                 & 59.4                                 & 58.4                                 & \multicolumn{1}{l|}{{\color[HTML]{FE0000} \textbf{58.4}}} & 80.5                                 & 79.4                                 & 77.4                                 & 76.1                                 & \multicolumn{1}{l|}{{\color[HTML]{FE0000} \textbf{77.3}}} & 91.3                                 & 90.9                                 & 90.1                                 & 89.4                                 & 89.1                                 \\ \hline
\multicolumn{1}{l|}{Ours (CoDiff)} & {\color[HTML]{FE0000} \textbf{62.7}} & {\color[HTML]{FE0000} \textbf{61.6}} & {\color[HTML]{FE0000} \textbf{59.6}} & {\color[HTML]{FE0000} \textbf{58.4}} & \multicolumn{1}{l|}{{\color[HTML]{000000} 58.3}}          & {\color[HTML]{FE0000} \textbf{82.3}} & {\color[HTML]{FE0000} \textbf{80.7}} & {\color[HTML]{FE0000} \textbf{77.5}} & {\color[HTML]{FE0000} \textbf{77.2}} & \multicolumn{1}{l|}{{\color[HTML]{000000} 76.3}}          & {\color[HTML]{FE0000} \textbf{91.7}} & {\color[HTML]{FE0000} \textbf{91.3}} & {\color[HTML]{FE0000} \textbf{91.0}} & {\color[HTML]{FE0000} \textbf{90.0}} & {\color[HTML]{FE0000} \textbf{89.3}} \\ \hline
\end{tabular}
\label{tab-all}
\end{table*}


\section{Experiment Result}
We validate our CoDiff  on both simulated and real-world scenarios.
The task of the experiments on the three datasets is point-cloud-based 3D cooperative object detection. Following the literature, the detection performance are evaluated by using Average Precision (AP) at Intersection-over-Union (IoU) thresholds of 0.50 and 0.70.

\subsection{Datasets}

\textbf{DAIR-V2X \cite{dair}.} DAIR-V2X is a large-scale vehicle-infrastructure cooperative perception dataset containing over 100 scenes and 18,000 data pairs, featuring two agents: vehicle and road-side unit (RSU), capturing simultaneous data from infrastructure and vehicle sensors at an equipped intersection as an autonomous vehicle passes through. 

\textbf{V2XSet \cite{v2x-vit}.}  V2XSet is built with the co-simulation of OpenCDA \cite{opencda} and CARLA \cite{carla}.
It is a large-scale simulated dataset designed for Vehicle-to-Infrastructure (V2X) communication. The dataset consists of a total of 11,447 frames, and the train/validation/test splits are 6,694/1,920/2,833, respectively.  

\textbf{OPV2V \cite{opv2v}.} OPV2V is a large-scale dataset specifically designed for vehicle-to-vehicle (V2V) communication. It is jointly developed using the CARLA and OpenCDA simulation tools\cite{carla}. It includes 12K frames of 3D point clouds and RGB images with 230K annotated 3D boxes. 




\subsection{Implementation Details}

We keep PointPillar \cite{pointpillars} as LiDAR encoder backbone. It converts  the
LiDAR point cloud into voxels with the  resolution to 0.4m for both height and width. 
We train an autoencoder to compress the features to an 8-dimensional space with a 32x compression rate. The diffusion model is trained using the Adam optimizer with a learning rate of 0.0002 and weight decay of 0.001. The training is conducted over 20 epochs with a total of 500 steps, adding diffusion conditions at each step. Finally, the object detection model is fine-tuned for 10 epochs.
To simulate noisy error and time delay, we follow the noisy settings  in RoCo \cite{roco} and the delay setting in CoBEVFlow \cite{v2x-vit} during the training process.  We add Gaussian noise $N\left( 0,\sigma^2_{t} \right)   $ on $x,y$ and $N\left( 0,\sigma^2_{r} \right)  $ on $\theta $, where $x,y,\theta $ are the coordinates of the 2D centers of a vechicle and the yaw angle of accurate global poses. The time delay is set to 100 ms. In the training process, the conditions are sequentially added to the Unet according to the steps.
All models are trained on six NVIDIA RTX 2080Ti GPUs. 

\subsection{Quantitative evaluation}
To validate the overall performance of CoDiff in 3D object detection, 
we compare it with seven state-of-the-art methods on the three datasets. 
For a fair comparison, all models take the same 3D point clouds as input data. All methods use the same feature encoder based on PointPillars \cite{pointpillars}. 
Table \ref{tab-all} shows  the AP at IoU threshold of 0.5 and 0.7 in DAIR-V2X, V2XSet and OPV2V dataset. We see that CoDiff significantly outperforms the previous methods at various noise levels across the three datasets and the leading gap is larger when the noise level is higher.
% In the real-world dataset DAIR-V2X, in the case of noise levels of $0.0m/0.0^{\circ }$, our approach achieves 1.1\% (77.4\% $vs.$ 76.3\%) and 0.7\% (62.7\% $vs.$ 62.0\%)  improvement over RoCo for AP@0.5/0.7. As the noise level increases, CoDiff still maintains strong detection performance. We conduct experiments on a simulated dataset V2XSet and OPV2V which involves more agents. CoDiff still performs well across all noise settings. Our method maintains a higher level of accuracy even under high-level of noise. When the noise level reaches $0.4m/0.4^{\circ }$, our approach still achieves 1.7\% (96.0\% $vs.$ 95.7\%) and 0.2\% (89.3\% $vs.$ 89.1\%) improvement over RoCo  for AP@0.5/0.7 in OPV2V.  We further investigate the impact of time delay with range [0, 400] ms.
Fig. \ref{fig-time} shows the detection performances  of the proposed CoDiff and the baseline methods under varying levels of time delay on  DAIR-V2X.
We see that  the proposed CoDiff achieves the best performance at all time delay settings, as shown by the red line in the graph. 
% In the case of time delay of 200ms, our approach achieves 2.6\% (82.0\% $vs.$ 79.4\%) and 0.5\% (59.5\% $vs.$ 59.0\%) improvement over RoCo  for AP@0.5/0.7.

\begin{figure}[htbp]
\centering
\includegraphics[width=1\columnwidth]{img/DAIR.png} 
\caption{Comparison of the performance of CoDiff and other baseline methods on DAIR-V2X dataset under the time delay from 0 to 400ms.}
\label{fig-time}
\end{figure}

\begin{figure}[htbp]
\centering
\subfigure[V2X-VIT]{\label{fig:subfig:a}
\includegraphics[width=0.485\linewidth]{img/detect1.png}}
\subfigure[CoAlign]{\label{fig:subfig:b}
\includegraphics[width=0.46\linewidth]{img/detect2.png}}
\vfill
\subfigure[RoCo]{\label{fig:subfig:a}
\includegraphics[width=0.485\linewidth]{img/detect3.png}}
\subfigure[Ours]{\label{fig:subfig:b}
\includegraphics[width=0.46\linewidth]{img/detect4.png}}
\caption{Visualization of detection results for V2X-ViT, CoAlign, RoCo and  our CoDiff with the noisy level $\sigma^2_{t} /\sigma^2_{r} \left( m/^{\circ }\right) $ of 0.4/0.4 and 100ms time delay  on V2XSet dateset.  CoDiff achieves much more precise detection.
}
\label{fig-view}
\end{figure}








\begin{figure}[htbp]
\centering
\includegraphics[width=1\columnwidth]{img/att_map.png} 
\caption{Visualization of attention maps with diffusion model (b) and without the  diffusion model (c). (a) is the ground truth of the different road types.}
\label{fig-att}
\end{figure}

\begin{table}[b]
\centering
\caption{Selection of the compression rate $\tau$ in CoDiff.}
\begin{tabular}{c|c|l}
\hline
Compression rate $\tau$ & Channel $c$  & mAP                                  \\ \hline
8x                 & 32 & 82.5                                 \\
16x               & 16 & 83.5                                 \\
32x                & 8  & {\color[HTML]{FE0000} \textbf{84.6}} \\
64x                & 4  & 82.5                                 \\ \hline
\end{tabular}
 \label{fig-compression}
\end{table}

\begin{table}[htbp]
\caption{Ablation study on V2XSet val set.}
 \setlength{\tabcolsep}{2pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{ccc|ccc|lll}
\hline
\multicolumn{3}{c|}{Modules}                                       & \multicolumn{3}{c|}{AP@0.5}                                                                                        & \multicolumn{3}{c}{AP@0.7}                                                                                         \\ \hline
\multicolumn{1}{l|}{Condition} & \multicolumn{1}{l|}{Add.} & Concat. & 0.0/0.0                              & 0.2/0.2                              & 0.4/0.4                              & 0.0/0.0                              & 0.2/0.2                              & 0.4/0.4                              \\ \hline
\multicolumn{1}{l|}{\textbf{×}}         & \multicolumn{1}{l|}{\textbf{×}}   & \textbf{×}      & 94.3                                 & 92.1                                 & 89.5                                 & 86.5                                 & 83.0                                 & 78.6                                 \\
\multicolumn{1}{l|}{\checkmark}         & \multicolumn{1}{l|}{\checkmark}   & \textbf{×}      & 95.2                                 & 92.6                                 & 90.8                                 & 87.4                                 & 84.1                                 & 79.5                                 \\
\multicolumn{1}{l|}{\checkmark}         & \multicolumn{1}{l|}{\textbf{×}}   & \checkmark      & {\color[HTML]{FE0000} \textbf{96.2}} & {\color[HTML]{FE0000} \textbf{95.4}} & {\color[HTML]{FE0000} \textbf{91.7}} & {\color[HTML]{FE0000} \textbf{89.7}} & {\color[HTML]{FE0000} \textbf{86.0}} & {\color[HTML]{FE0000} \textbf{80.4}} \\ \hline
\end{tabular}
}
 \label{fig-condition}
\end{table}


\begin{table}[htbp]
\centering
\caption{Performance comparison of different
sampling methods at varying steps in the proposed CoDiff.}
 \setlength{\tabcolsep}{3.5pt}
\begin{tabular}{l|ccccc}
\hline
Method/Metric & \multicolumn{5}{c}{AP@0.7}       \\ \hline
Sampling Step & 1    & 2    & 4    & 8    & 10   \\ \hline
DDPM  \cite{DDPM}        & 84.7 & 85.3 & 85.7 & {\color[HTML]{000000} \textbf{86.0}} & 85.4 \\
DDIM  \cite{DDIM}        &  85.8    &   85.4   &  85.6    &  85.8    &   85.3   \\ \hline
\end{tabular}
 \label{tab-sample}
\end{table}

\begin{table}[htbp]
\centering
\caption{Inference time and Parameters.}
\begin{tabular}{l|ll}
\hline
Method  & Interfence\_time & Parameters \\ \hline
% V2X-ViT \cite{v2x-vit} & 130  ms          & 1180.54 MB \\
% CoAlign \cite{CoAlign} & 69.5 ms          & 780.89  MB \\
% RoCo \cite{roco}   & 69.9 ms          & 781.39  MB \\
CoDiff (DDPM)   & 127.7 ms          & 769.17  MB \\
CoDiff (DDIM) & 125.8  ms          & 776.91  MB \\ \hline
\end{tabular}
 \label{tab-time}
\end{table}



\subsection{Qualitative evaluation}
\textbf{Detection visualization.}
We  show the 3D detection results in the Bird's-eye-view (BEV) format on the V2XSet. Red and green boxes denote the detection results and the ground-truth, respectively.
The degree of overlapping of these boxes reflects the performance of a method.  
Figure \ref{fig-view}  depicts the detection results of V2X-ViT, CoAlign, RoCo and the proposed CoDiff at an intersection to validate the effectiveness of our method. 
We set the noise level and time delay of $0.4m/0.4^{\circ }$ and 100ms to produce high noises to make the perception task challenging.  From the figure, it can be observed that V2X-ViT has many missed detections, while CoAlign \cite{CoAlign} and RoCo \cite{roco} generate many predictions with relatively large offsets. In contrast, our CoDiff demonstrates strong performance in the presence of  pose errors and time delays.


\textbf{Attention map visualization.}
To better understand the advantages of the diffusion model, we visualize the multi-agent fused feature maps learned by different methods in Fig. \ref{fig-att}. In these visualizations, brighter areas indicate higher feature intensity, which is more beneficial for subsequent detection. Fig. \ref{fig-att}(b) shows the feature map generated by the conditional DPM, while Fig. \ref{fig-att}(c) visualizes the feature map learned by the other multi-scale feature fusion method \cite{v2x-vit,where2comm}. It is evident from the figures that the feature intensity is higher with the conditional DPM, and the features at greater distances are also clearer (highlighted in the yellow boxes).


\subsection{Ablation Studies}

To determine the optimal compression rate $\tau$ during the perceptual compression, we conduct ablation experiments on V2XSet dataset, 
as shown in Table \ref{fig-compression}.  
We found that setting $\tau$ to 32 and compressing the feature dimensions  to 8 achieved the optimal mAP.
% To validate the efficacy of Conditional diffusion module, Table \ref{fig-condition} shows the AP at IoU 0.5/0.7 at various noise levels. Please note: (i) The diffusion conditions enhance the stability of the model; (ii) Using Concat for condition fusion is superior to directly applying element-wise addition.
To validate the effectiveness of the conditional diffusion module, Table \ref{fig-condition} shows the results of using or not using conditions, as well as how the conditions were applied in the model. Please note: (i) The diffusion conditions enhance the stability of the model; (ii) Using Concat for condition fusion is superior to directly applying element-wise addition.
In testing, we used two samplers: DDPM \cite{DDPM} and DDIM \cite{DDIM}. To achieve optimal performance, we tested the effect of different samplers and sampling steps on the results. The experimental results are shown in Table \ref{tab-sample}. We selected DDPM with 8 sampling steps as the default solver in the experiments due to its simplicity and efficiency.
We  also measure the inference time and the parameters of our proposed CoDiff on a single
NVIDIA 2080Ti GPU in Table \ref{tab-time}. At 1-step DDPM sampling, CoDiff can reach the inference time  of 127.7 (ms) with 769.17 (MB) parameters. 
% At 1-step DDIM sampling, CoDiff can reach the inference time  of 125.8 (ms) with 776.91 (MB) parameters.






\section{CONCLUSION}
This paper proposes a novel robust collaborative perception framework \textbf{CoDiff} for 3D object detection.  This framework leverages diffusion models to address information inaccuracies caused by pose errors and time delays between multiple agents, enhancing the precision and reliability of information exchange. 
% The core idea of \textbf{CoDiff} is to use the different features learned by  each agent as conditions for the diffusion model, progressively refining noisy samples to generate highly detailed multi-agent fusion information. Additionally, the use of simple yet efficient perception compression module enhances the training and sampling efficiency of the denoising diffusion model. 
Comprehensive experiments demonstrate that \textbf{CoDiff} achieves outstanding performance across all settings and exhibits exceptional robustness under extreme noise conditions. 
In future work, we aim to further explore the potential of CoDiff and expand its application to a broader range of perception tasks.



\section*{ACKNOWLEDGMENT}
This work was supported by the  National Natural Science Foundation of China Grant No. 12071478, No. 61972404; Public Computing Cloud and the Blockchain Lab, School of Information, Renmin University of China. 




%\begin{thebibliography}{99}   
\bibliographystyle{IEEEtran}
\bibliography{root}
%\end{thebibliography}




\end{document}
