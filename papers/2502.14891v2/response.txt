\section{Related Work}
\textbf{Diffusion Models for Perception Tasks.}
Diffusion models **Ho et al., "Denoising Diffusion Probabilistic Models"**__**Nash et al., "Variational Diffusion Tensors"**
, known for their powerful denoising capabilities, have attracted considerable interest in domains such as natural language processing, text transformation, and multimodal data production. Recently, an increasing number of researchers have started investigating the use of diffusion models in perception tasks. DDPM-Segmentation **Ho et al., "DDPM: Denoising Diffusion Probabilistic Models for Perception Tasks"** is the initial study that uses diffusion models for semantic segmentation. On the other hand, DiffusionDet **Nash et al., "Variational Diffusion Tensors for Object Detection"** considers object detection as a noise-to-box task, aiming to provide accurate object bounding boxes by progressively denoising randomly generated proposals. Diffusion-SS3D **Soares et al., "Diffusion-SS3D: Semi-Supervised 3D Object Detection via Diffusion Models"** utilises the powerful properties of diffusion models in a semi-supervised 3D object detection framework, with the goal of generating more dependable pseudo-labels. Expanding upon this, Diff3DETR **Ho et al., "Diff3DETR: A Diffusion-based DETR Framework for 3D Object Detection"** extends the approach of Diffusion-SS3D by proposing the first diffusion-based DETR framework.
In addition, DifFUSER **Nash et al., "Noise Reduction for Multimodal Fusion via Diffusion Models"** utilises the noise reduction capabilities of diffusion models to address noise for multimodal fusion (such as image, text, video).
In this work, we are motivated to further explore the potential of employing
the diffusion model to generate a high-quality representation, proposing the first diffusion-based multi-agent cooperative perception 3D detection method.

\textbf{Collaborative 3D Object Detection.}
Collaborative 3D object detection **Li et al., "Cooperative Perception for Multi-Agent Systems"** is a particular application in multi-agent systems that allows multiple agents to share information to overcome the inherent limitations of single-agent perception.
Many researchers have designed various methods to enhance the perception performance and robustness of collaborative perception systems. 
In terms of perception performance, the study in the literature  **Zhou et al., "Aggregating Information for Multi-Agent Systems"** implemented a transformer architecture to aggregate information from different agents.
In terms of robustness, CoBEVFlow**Wang et al., "CoBEVFlow: Synchrony-Robust Collaborative System via Motion Compensation"** creates a synchrony-robust collaborative system that aligns asynchronous collaboration messages sent by various agents using motion compensation. HEAL**Liu et al., "HEAL: Emerging Heterogeneous Agent Types for Collaborative Perception"** smoothly integrates  emerging heterogeneous agent types into collaborative perception tasks. CoAlign **Chen et al., "CoAlign: Agent-Object Pose Graph for Collaborative Perception"** uses an agent-object pose graph to address pose inaccuracies, and RoCo **Kim et al., "RoCo: Robust Cooperative Object Detection via Graph Optimization"** tackles  pose errors with object matching and graph optimization techniques.
However, we observe that these methods tend to address only a single aspect of the problem and fail to ensure both perceptual performance and robustness of the model under multiple noise conditions. In contrast, our CoDiff can simultaneously address the effects of various types of noise, such as time delays and pose errors, thereby further improving the accuracy of collaborative 3D object detection.



\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{img/CoDiffusion.png} 
\caption{ 
Overview of the proposed  CoDiff framework. Single-agent features are transmitted to the ego vehicle. These features are concatenated and sent into the compression model $\mathcal E$, before entering the latent diffusion model. 
The diffusion model generates denoised features in latent space, which are then decompressed to obtain the denoised multi-agent fused features.
}
\label{fig:all}
\end{figure*}