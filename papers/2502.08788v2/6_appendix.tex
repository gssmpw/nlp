\appendix
\onecolumn

%\section{Frequently Asked Questions}

\section{Case study} \label{appendix:case}
\myparatight{Limited benchmark} Most test cases in existing benchmarks only require a single knowledge point to be solved, which naturally makes MAD unnecessary. Here we present a case study to support our argument. We pick some test cases from MMLU, MMLUPro, and CommensenseQA as follows.

\begin{tcolorbox}[colback=gray!10!white, colframe=black!70!white, breakable]
\textbf{MMLU, college\_physics}

Which of the following gives the total spin quantum number of the electrons in the ground state of neutral nitrogen (Z = 7)?

A: "1/2"
B: "1"
C: "3/2"
D: "5/2" 

\textbf{MMLUPro}	

A television sells for \$180. It costs the retailer \$136.50. Find the percent markup based on the selling price.

A: "27.5\%"
B: "21\%"
C: "22.5\%"
D: "30\%"
E: "24.17\%"
F: "20\%"
G: "26.5\%"
H: "25\%"
I: "18\%"
J: "32.5\%"

\textbf{CommensenseQA}

The child's wild imagination made him able to see story that he read, he was able to do what with the story?

A: "picture it"
B: "reading"
C: "visualize"
D: "open book"
E: "go to movies"


\end{tcolorbox}

\myparatight{Agents debate based on whether their answers are the same} In practice, we found that agents' behavior in debate relies on the fact that their answers are the same (or not). 

\begin{tcolorbox}[colback=gray!10!white, colframe=black!70!white, breakable]
\textbf{Question:} What is the feeling of one having fun?

\textbf{Round 1}

Agent 1: B

Agent 2: E: laughter

Agent 3: The answer is B

\textbf{Round 2}

Agent 1: Considering the solutions provided by other agents, the feeling of having fun is often associated with joy, pleasure, and expressions like laughter. While "pleasure" (B) reflects the overall enjoyment, "laughter" (E) specifically captures a direct expression of fun. Given that laughter is a common reaction to fun situations, I would align with that sentiment. Thus, the updated answer is (E).

Agent 2: Considering the additional opinions from other agents, they suggested that the feeling of one having fun corresponds to "pleasure" (B). While laughter (E) is often associated with fun, the broader concept of having fun can also encompass feelings of pleasure. Therefore, I will incorporate these insights into my final answer. The updated answer is (B).

Agent 3: Considering the feeling of having fun, both pleasure and laughter are associated with enjoyable experiences. While pleasure (B) is a broader term that encompasses a range of enjoyable feelings, laughter (E) is a direct expression that typically accompanies fun. Given that one agent has chosen laughter as the answer and it specifically denotes a physical reaction to joy, I will align with that insight. Therefore, my updated answer is (E).



\end{tcolorbox}

Note that in these case studies, determining whether a problem requires only a single knowledge point to solve, or whether the agent overly relies on the similarity or difference of answers, is challenging to achieve objectively. Therefore, we use case studies to provide factual support for the future directions discussed in the paper, rather than analyzing this through statistical methods. We understand that this approach might make our argument less convincing, but it still aligns with our empirical observations. We also hope that future work will develop better methods to address and present these issues.


\section{Experiment details}



\subsection{Benchmark configurations} \label{appendix:benchmark}


\textbf{MMLU}~\citep{mmlu1, mmlu2} is a benchmark dataset designed to evaluate general knowledge across 57 subjects, including STEM, humanities, and social sciences. It tests a model's ability on challenging multiple-choice questions that require both specialized and common knowledge. The testing set contains 14,042 samples.

\noindent\textbf{MMLUPro}~\citep{mmlupro} is a more robust and challenging massive multi-task understanding dataset tailored to more rigorously benchmark LLMs' capabilities. It contains 1,200 testing samples.

\noindent\textbf{CommonSenseQA}~\citep{csqa} is a multiple-choice question-answering dataset that tests different types of commonsense reasoning.  The dataset contains 1,140 questions with one correct answer and four distractor answers.

\noindent\textbf{AGi-Eval}~\citep{agieval} is a dataset aimed at evaluating artificial general intelligence (AGI) capabilities, focusing on problem-solving, reasoning, and generalization across multiple domains. We specifically use several subsets: \textit{aqua-rat}, \textit{logiqa-en}, \textit{lsat-ar}, \textit{lsat-lr}, \textit{lsat-rc}, \textit{sat-math}, \textit{sat-en}, and \textit{sat-en-without-passage}, as they are in English.

\noindent\textbf{ARC-Challenge}~\citep{arc} contains genuine grade-school level, multiple-choice science questions. The dataset is partitioned into a Challenge Set and an Easy Set and has 3,548 questions in total.

\noindent\textbf{GSM8k}~\citep{gsm8k} is a challenging math word benchmark designed to test a model's reasoning and problem-solving abilities in arithmetic and algebra. It is widely used for evaluating mathematical understanding and reasoning. We use the main set of GSM8k, which contains 1,319 testing cases.

\noindent\textbf{MATH}~\citep{math} is a comprehensive dataset featuring math problems across various topics, including geometry, algebra, calculus, and number theory. The dataset contains 5,000 testing questions.

\noindent\textbf{HumanEval}~\citep{humaneval} is a dataset for evaluating code generation and programming skills. It contains prompts and corresponding solutions for coding tasks. The dataset consists of 164 programming problems.

\noindent\textbf{MBPP}~\citep{mbpp} is a programming dataset. MBPP is harder to solve than HumanEval since it does not include a function signature for reference. It contains 500 samples for evaluation.

We use the following prompts to help LLMs format their responses.
\begin{tcolorbox}[colback=gray!10!white, colframe=black!70!white, breakable]
\textbf{Multi-choice benchmarks}

Instruction: Answer this multiple choice question. Generate your final answer by the answer is (X).

Q: \{question\}

A: The answer is 


\textbf{Mathematical reasoning benchmarks}

Instruction: Answer this question. Generate your final answer by the answer is \$$\backslash$boxed\{ANSWER\}\$.

Q: \{question\}
    
A: The answer is 


\textbf{Programming benchmarks}

Instruction: Write a python program to complete the following code. Do not output any example usage. Generate the final program by ``The answer is: $```$python

Q: \{question\}

A: 

\end{tcolorbox}



\subsection{MAD configurations} \label{appendix:mad_conf}


\myparatight{Society of Minds} 3 agents debate for 2 rounds. All agents share the same prompt as follows.

\begin{tcolorbox}[colback=gray!10!white, colframe=black!70!white, breakable]

These are the solutions to the problem from other agents: 

One agent's solution: \{\}

One agent's solution: \{\}

One agent's solution: \{\}

Use these opinions carefully as additional advice, can you provide an updated answer? Make sure to state your answer (capital multiple choice letter) at the end of the response.

\end{tcolorbox}

\myparatight{MP} The angel agent first generates a solution, and the devil agent debates against the solution and presents a new one. The judger agent can continue the debate for another round, or end the debate by picking the final solution from these two solutions. The debate can continue for 5 rounds if the judger has not picked the final answer.

\begin{tcolorbox}[colback=gray!10!white, colframe=black!70!white, breakable]
\textbf{Angel's prompt:}

You will now think step by step and provide an answer at the end of your response.

\textbf{Devil's prompt:}

You disagree with my answer. Provide your answer and reasons.

\textbf{Judger's prompt:}

You, as the moderator, will evaluate both sides' answers and determine if there is a clear preference for an answer candidate. If so, please summarize your reasons for supporting affirmative/negative side and give the final answer that you think is correct, and the debate will conclude. If not, the debate will continue to the next round. Now please output your answer in json format, with the format as follows: \{"Whether there is a preference": "Yes or No", "Supported Side": "Affirmative or Negative", "Reason": "", "debate\_answer": "the capital letter corresponding to the answer"\}. Please strictly output in JSON format, do not output irrelevant content.

\end{tcolorbox}

\myparatight{Exchange of Thoughts} In EoT, three agents with diverse persona prompts are organized to perform the debate. Each agent can access other agents' response and their confidences. EoT adopts answer consistency as the confidence signal, i.e., the frequency of the most frequent answer. These agents can debate at most 5 rounds. When all agents reach a consensus, the debate terminates. 

\begin{tcolorbox}[colback=gray!10!white, colframe=black!70!white, breakable]

Here is a solution process from your friend:

\{\}'s solution: \{\}  \{\}'s confidence in this solution is: \{\}

\{\}'s solution: \{\}  \{\}'s confidence in this solution is: \{\}

\{\}'s solution: \{\}  \{\}'s confidence in this solution is: \{\}

Based on your friend's solution, carefully re-examine your previous answer. If your friend's confidence level is below 0.5, it suggests a high probability that the solution might be incorrect. Remember, solutions with high confidence can also be wrong. Utilize your talent and critical thinking to provide a new step-by-step solution process.

Provide the new solution directly, refrain from commenting on your friend's approach, and conclude by stating the answer.

\textbf{Kitty's persona prompt:}

You are Kitty, a high school student admired for your attentiveness and detail-oriented nature. Your friends often rely on you to catch details they might have missed in their work. Your task is to carefully analyze the presented math problem, apply your attentive skills, and piece together a detailed solution. Afterward, you'll have the opportunity to review the solutions provided by your friends, offering insights and suggestions. Your careful revisions will help all of you to enhance your understanding and arrive at the most accurate solutions possible.

\textbf{Ben's persona prompt:}
You are Ben, a high school student with a track record of excellent grades, particularly in mathematics. Your friends admire your diligence and often seek your guidance in their studies. Your role is to scrutinize the problem at hand with your usual attention to detail, drawing from your vast knowledge of math principles. After considering your friends' approaches, carefully construct your answer, ensuring to clarify each step of your process. Your clear and logical explanations are valuable, as they will serve as a benchmark for your friends to compare and refine their own solutions.

\textbf{Peter's persona prompt:}

You are Peter, a high school student recognized for your unique problem-solving abilities. Your peers often turn to you for assistance when they encounter challenging tasks, as they appreciate your knack for devising creative solutions. Today, your challenge is to dissect the given math problem, leveraging your unique problem-solving strategies. Once you've crafted your solution, share it with your friends, Ben and Kitty, so they can see a different perspective. Your innovative approach will not only provide an answer but also inspire Ben and Kitty to think outside the box and possibly revise their own solutions.

\end{tcolorbox}

\myparatight{ChatEval} In ChatEval, three agents General Public, Critic, and Scientist debate one by one. Different persona prompts enable these agents to think in diverse style, with specific focus on critical thinking or scientific domain background. The debate continues for 2 rounds by default.

\begin{tcolorbox}[colback=gray!10!white, colframe=black!70!white, breakable]

\textbf{General Public's prompt:}

We would like to request your answer to this question.

There are a few other referee assigned the same task, it's your responsibility to discuss with them and think critically before you make your final judgement.

You are now General Public, one of the referees in this task. You are interested in the story and looking for updates on the investigation. Please think critically by yourself and note that it's your responsibility to answer the question.

Now it's your time to talk, please make your talk short and clear, General Public!

\textbf{Critic's prompt:}

We would like to request your answer to this question.

There are a few other referee assigned the same task, it's your responsibility to discuss with them and think critically before you make your final judgement.

You are now Critic, one of the referees in this task. Your job is to question others judgement to make sure their judgement is well-considered.

Now it's your time to talk, please make your talk short and clear, Critic!

\textbf{Scientist's prompt:}

We would like to request your answer to this question.

There are a few other referee assigned the same task, it's your responsibility to discuss with them and think critically before you make your final judgement.

You are Scientist, one of the referees in this task. You are a professional engaged in systematic study who possesses a strong background in the scientific method, critical thinking, and problem-solving abilities. Please help other people to answer the question.

Now it's your time to talk, please make your talk short and clear, Scientist!

\end{tcolorbox}

\myparatight{AgentVerse} AgentVerse adopts a dynamic way to organize the debate. First, a Rold Assigner agent reads the question and determines what kinds of agents should be recruited to solve the question. After the role assignment, one solver agent and three critic agents with assigned roles will debate to figure out the answer. Finally, an evaluator will review the answer and determine whether another round is necessary for a better answer. In dealing with programming tasks, an extra executor will be incorporated to execute the written program and return the execution result to the evaluator for accurate feedback.

\begin{tcolorbox}[colback=gray!10!white, colframe=black!70!white, breakable]

\textbf{Role Assigner's prompt:}

\# Role Description

You are the leader of a group, now you are facing a problem:

\{question\}


You can recruit \{cnt\_critic\_agents\} people to solve the logic problem. What people will you recruit?

Here are some suggestion:
\{advice\}

Response Format Guidance

You should respond with a list of expert description. For example:

1. an electrical engineer specified in the filed of xxx.

2. an economist who is good at xxx.

3. a lawyer with a good knowledge of xxx.

...

Only respond with the description of each role. Do not include your reason.

\textbf{Solver's prompt:}

Using these information, can you provide the correct solution to the problem? Explain your reasoning and solve the problem step by step. Your final answer should be a single capital letter, which is the lable of choice, in the form \\boxed{answer}, at the end of your response.

\textbf{Critic's prompt:}

You are in a discussion group, aiming to collaborative solve the following logic problem:

\{question\}

You are \{role\_description\}. Based on your knowledge, can you check the correctness of the solutions given above? You should give your correct solution to the problem step by step. When responding, you should follow the following rules:

1. Double-check the above solutions, give your critics, then generate the correct solution step by step.

2. If the final answer in your solution is the same as the final answer in the above provided solution, end your response with a special token "[Agree]".

3. You must highlight your final answer in the form $\backslash$boxed\{answer\} at the end of your response. The answer must be a single letter.

Now give your response.

\textbf{Executor's prompt:}

You are an experienced program tester. Now your team is trying to solve the problem: 

Complete the Python function:

\{question\}

The solution has been written to `tmp/main.py`. Your are going to write the unit testing code for the solution. You should respond in the following format:

Thought: your thought

Reasoning: your reasoning on the testing cases

Criticism: constructive self-criticism

File Path: the path to write your testing code

Code: the testing code with explaination in docstring. make sure to write the input in the assertion to make it appear in the unit test report, and make sure the expected answer is correct

Command: the command to change directory and execute your testing code

\textbf{Evaluator's prompt:}

Problem:

\{question\}

Solution: 

\{solution\}

You are a logic problem lover. Above is a logic problem and a solution. Check whether the solution and the deduction is correct. If the deduction is wrong, you should explain why it is wrong, but do not give your solution. When it is correct, output a correctness of 1 and why it is correct.

You should respond in the following format:

Correctness: (0 or 1, 0 is wrong, and 1 is correct)

Response: (explain in details why it is wrong or correct. do not provide your solution)
\end{tcolorbox}

\section{Additional Experimental Results} \label{appendix:exp_res}

\begin{table*}[htbp]
\caption{Main results on Llama3.1:8b. We use \colorbox{LightRed}{lightred}/\colorbox{LightBlue}{lightblue} to denote results higher/lower than CoT.}
\label{tab:main-llama8b}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|lllllllll}
\hline
\textbf{Dataset}    & \textbf{MMLU} & \textbf{MMLU-Pro} & \textbf{CommensenseQA} & \textbf{ARC-Challenge} & \textbf{AGIEval} & \textbf{GSM8K} & \textbf{MATH} & \textbf{HumanEval} & \textbf{MBPP} \\ \hline
SA & \cellcolor{LightBlue}43.13 $\pm$ 1.04 & \cellcolor{LightBlue}34.27 $\pm$ 0.50 & \cellcolor{LightBlue}66.00 $\pm$ 0.16 & \cellcolor{LightBlue}81.67 $\pm$ 0.68 & \cellcolor{LightBlue}33.60 $\pm$ 0.75 & \cellcolor{LightBlue}70.40 $\pm$ 0.43 & \cellcolor{LightBlue}36.47 $\pm$ 1.09 & \cellcolor{LightRed}54.07 $\pm$ 0.29 & \cellcolor{LightRed}50.45 $\pm$ 1.75 \\
CoT & 57.47 $\pm$ 1.18 & 41.20 $\pm$ 1.14 & 71.13 $\pm$ 0.84 & 86.40 $\pm$ 0.99 & 46.73 $\pm$ 1.09 & 80.13 $\pm$ 1.23 & 40.13 $\pm$ 0.66 & 37.60 $\pm$ 1.52 & 43.71 $\pm$ 2.88 \\
SC & \cellcolor{LightRed}64.96 $\pm$ 1.08 & \cellcolor{LightRed}47.49 $\pm$ 0.08 & \cellcolor{LightRed}74.43 $\pm$ 0.30 & \cellcolor{LightRed}86.60 $\pm$ 1.13 & \cellcolor{LightBlue}42.47 $\pm$ 1.58 & \cellcolor{LightBlue}79.53 $\pm$ 0.68 & \cellcolor{LightRed}42.25 $\pm$ 2.15 & -- & -- \\
\hline
SoM & \cellcolor{LightBlue}53.40 $\pm$ 0.28 & \cellcolor{LightBlue}36.57 $\pm$ 1.27 & \cellcolor{LightBlue}70.93 $\pm$ 0.82 & \cellcolor{LightBlue}82.00 $\pm$ 0.65 & \cellcolor{LightBlue}37.13 $\pm$ 0.47 & \cellcolor{LightBlue}63.87 $\pm$ 0.93 & \cellcolor{LightRed}40.20 $\pm$ 0.85 & \cellcolor{LightRed}47.56 $\pm$ 2.28 & \cellcolor{LightRed}45.91 $\pm$ 1.15 \\
MP & \cellcolor{LightBlue}53.33 $\pm$ 2.54 & \cellcolor{LightBlue}36.44 $\pm$ 2.74 & \cellcolor{LightBlue}46.07 $\pm$ 0.62 & \cellcolor{LightBlue}61.93 $\pm$ 1.39 & \cellcolor{LightBlue}44.50 $\pm$ 1.65 & \cellcolor{LightBlue}47.60 $\pm$ 2.73 & \cellcolor{LightBlue}10.30 $\pm$ 0.93 & \cellcolor{LightBlue}24.19 $\pm$ 3.67 & \cellcolor{LightBlue}23.09 $\pm$ 1.81 \\
EoT & \cellcolor{LightBlue}48.97 $\pm$ 0.58 & \cellcolor{LightBlue}36.04 $\pm$ 0.21 & \cellcolor{LightBlue}66.15 $\pm$ 0.61 & \cellcolor{LightBlue}81.60 $\pm$ 0.43 & \cellcolor{LightBlue}33.42 $\pm$ 0.84 & \cellcolor{LightBlue}61.87 $\pm$ 2.39 & \cellcolor{LightBlue}26.61 $\pm$ 1.53 & \cellcolor{LightBlue}22.36 $\pm$ 2.07 & \cellcolor{LightBlue}23.87 $\pm$ 0.97 \\
ChatEval & \cellcolor{LightRed}61.81 $\pm$ 0.88 & \cellcolor{LightRed}43.56 $\pm$ 1.22 & \cellcolor{LightBlue}68.66 $\pm$ 2.62 & \cellcolor{LightBlue}84.70 $\pm$ 0.51 & \cellcolor{LightRed}57.87 $\pm$ 1.25 & \cellcolor{LightRed}81.13 $\pm$ 0.81 & \cellcolor{LightBlue}39.77 $\pm$ 0.54 & \cellcolor{LightRed}41.46 $\pm$ 0.00 & \cellcolor{LightBlue}40.73 $\pm$ 1.60 \\
AgentVerse & \cellcolor{LightBlue}13.27 $\pm$ 0.47 & \cellcolor{LightBlue}20.53 $\pm$ 1.23 & \cellcolor{LightBlue}16.33 $\pm$ 1.52 & \cellcolor{LightBlue}24.60 $\pm$ 0.98 & \cellcolor{LightRed}50.33 $\pm$ 2.49 & \cellcolor{LightBlue}5.47 $\pm$ 1.31 & \cellcolor{LightBlue}13.30 $\pm$ 1.26 & \cellcolor{LightRed}40.24 $\pm$ 2.59 & \cellcolor{LightBlue}32.56 $\pm$ 1.32 \\
\hline
\end{tabular}}
\end{table*}

\begin{table*}[htbp]
\caption{Main results on Llama3.1:70B. We use \colorbox{LightRed}{lightred}/\colorbox{LightBlue}{lightblue} to denote results higher/lower than CoT.}
\label{tab:main-llama70b}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|lllllllll}
\hline
\textbf{Dataset}    & \textbf{MMLU} & \textbf{MMLU-Pro} & \textbf{CommensenseQA} & \textbf{ARC-Challenge} & \textbf{AGIEval} & \textbf{GSM8K} & \textbf{MATH} & \textbf{HumanEval} & \textbf{MBPP} \\ \hline
SA & \cellcolor{LightBlue}80.20 $\pm$ 2.05 & \cellcolor{LightBlue}46.27 $\pm$ 0.66 & \cellcolor{LightBlue}79.13 $\pm$ 1.05 & \cellcolor{LightBlue}91.67 $\pm$ 0.25 & \cellcolor{LightBlue}56.87 $\pm$ 1.93 & \cellcolor{LightBlue}69.47 $\pm$ 0.90 & \cellcolor{LightRed}38.13 $\pm$ 1.09 & \cellcolor{LightRed}63.41 $\pm$ 1.72 & \cellcolor{LightBlue}45.78 $\pm$ 2.95 \\
CoT & 82.73 $\pm$ 1.25 & 53.87 $\pm$ 0.90 & 82.40 $\pm$ 1.45 & 93.33 $\pm$ 0.50 & 58.42 $\pm$ 1.53 & 92.07 $\pm$ 0.90 & 37.13 $\pm$ 0.98 & 62.60 $\pm$ 1.04 & 49.42 $\pm$ 2.52 \\
SC & \cellcolor{LightRed}83.73 $\pm$ 0.19 & \cellcolor{LightBlue}53.27 $\pm$ 1.06 & \cellcolor{LightBlue}81.16 $\pm$ 0.63 & \cellcolor{LightBlue}92.80 $\pm$ 0.59 & \cellcolor{LightRed}61.07 $\pm$ 0.25 & \cellcolor{LightBlue}83.20 $\pm$ 0.33 & \cellcolor{LightRed}49.73 $\pm$ 0.62 & -- & -- \\
\hline
SoM & \cellcolor{LightRed}84.60 $\pm$ 0.43 & \cellcolor{LightRed}57.13 $\pm$ 1.15 & \cellcolor{LightBlue}81.93 $\pm$ 0.34 & \cellcolor{LightBlue}92.93 $\pm$ 0.52 & \cellcolor{LightRed}62.22 $\pm$ 1.08 & \cellcolor{LightBlue}88.27 $\pm$ 0.74 & \cellcolor{LightRed}48.64 $\pm$ 1.23 & \cellcolor{LightRed}63.41 $\pm$ 2.28 & \cellcolor{LightBlue}41.37 $\pm$ 0.49 \\
MP & \cellcolor{LightBlue}81.39 $\pm$ 1.09 & \cellcolor{LightBlue}51.93 $\pm$ 2.29 & \cellcolor{LightBlue}68.55 $\pm$ 1.02 & \cellcolor{LightBlue}88.38 $\pm$ 0.03 & \cellcolor{LightRed}61.60 $\pm$ 2.55 & \cellcolor{LightBlue}69.27 $\pm$ 1.05 & \cellcolor{LightBlue}24.60 $\pm$ 1.56 & \cellcolor{LightBlue}52.64 $\pm$ 1.04 & \cellcolor{LightBlue}32.56 $\pm$ 0.97 \\
EoT & \cellcolor{LightRed}83.20 $\pm$ 0.28 & \cellcolor{LightBlue}49.66 $\pm$ 0.60 & \cellcolor{LightBlue}81.87 $\pm$ 0.74 & \cellcolor{LightBlue}92.96 $\pm$ 0.14 & \cellcolor{LightRed}63.06 $\pm$ 0.46 & \cellcolor{LightBlue}77.60 $\pm$ 0.65 & \cellcolor{LightRed}43.63 $\pm$ 2.28 & \cellcolor{LightBlue}55.49 $\pm$ 0.86 & \cellcolor{LightBlue}38.91 $\pm$ 1.68 \\
ChatEval & \cellcolor{LightBlue}80.37 $\pm$ 1.15 & \cellcolor{LightRed}56.13 $\pm$ 1.00 & \cellcolor{LightBlue}72.82 $\pm$ 1.33 & \cellcolor{LightBlue}89.94 $\pm$ 0.20 & \cellcolor{LightRed}68.59 $\pm$ 0.42 & \cellcolor{LightRed}92.53 $\pm$ 0.19 & \cellcolor{LightRed}58.73 $\pm$ 1.52 & \cellcolor{LightRed}62.80 $\pm$ 0.86 & \cellcolor{LightBlue}44.49 $\pm$ 2.23 \\
AgentVerse & \cellcolor{LightRed}84.80 $\pm$ 1.02 & \cellcolor{LightRed}61.80 $\pm$ 0.91 & \cellcolor{LightBlue}76.47 $\pm$ 1.24 & \cellcolor{LightBlue}92.80 $\pm$ 0.28 & \cellcolor{LightRed}66.73 $\pm$ 0.84 & \cellcolor{LightBlue}85.47 $\pm$ 0.68 & \cellcolor{LightRed}45.33 $\pm$ 0.94 & \cellcolor{LightBlue}59.96 $\pm$ 0.76 & \cellcolor{LightBlue}41.89 $\pm$ 1.02 \\
\hline
\end{tabular}}
\end{table*}


\begin{table*}[htbp]
\caption{Main results on Claude-3.5-Haiku. We use \colorbox{LightRed}{lightred}/\colorbox{LightBlue}{lightblue} to denote results higher/lower than CoT.}
\label{tab:main-claude}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|lllllllll}
\hline
\textbf{Dataset}    & \textbf{MMLU} & \textbf{MMLU-Pro} & \textbf{CommensenseQA} & \textbf{ARC-Challenge} & \textbf{AGIEval} & \textbf{GSM8K} & \textbf{MATH} & \textbf{HumanEval} & \textbf{MBPP} \\ \hline
SA & \cellcolor{LightBlue}56.81 $\pm$ 0.15 & \cellcolor{LightBlue}38.38 $\pm$ 0.36 & \cellcolor{LightBlue}79.40 $\pm$ 0.28 & \cellcolor{LightBlue}87.17 $\pm$ 0.54 & \cellcolor{LightBlue}48.99 $\pm$ 1.31 & \cellcolor{LightBlue}83.13 $\pm$ 0.09 & \cellcolor{LightRed}31.71 $\pm$ 2.48 & \cellcolor{LightRed}66.26 $\pm$ 0.76 & \cellcolor{LightBlue}48.55 $\pm$ 0.62 \\
CoT & 62.00 $\pm$ 0.00 & 47.00 $\pm$ 1.57 & 79.67 $\pm$ 0.34 & 89.47 $\pm$ 0.38 & 52.00 $\pm$ 1.02 & 85.84 $\pm$ 0.72 & 30.62 $\pm$ 1.42 & 65.24 $\pm$ 2.59 & 56.16 $\pm$ 0.92 \\
SC & \cellcolor{LightRed}63.08 $\pm$ 1.06 & \cellcolor{LightRed}50.02 $\pm$ 1.47 & \cellcolor{LightRed}81.27 $\pm$ 0.05 & \cellcolor{LightRed}90.00 $\pm$ 0.28 & \cellcolor{LightRed}53.59 $\pm$ 1.09 & \cellcolor{LightRed}90.27 $\pm$ 0.46 & \cellcolor{LightRed}35.09 $\pm$ 0.69 & -- & -- \\
\hline
SoM & \cellcolor{LightBlue}57.39 $\pm$ 1.00 & \cellcolor{LightBlue}39.91 $\pm$ 0.47 & \cellcolor{LightBlue}79.40 $\pm$ 0.59 & \cellcolor{LightBlue}88.20 $\pm$ 0.23 & \cellcolor{LightBlue}51.48 $\pm$ 0.42 & \cellcolor{LightRed}86.87 $\pm$ 0.73 & \cellcolor{LightRed}34.31 $\pm$ 0.89 & \cellcolor{LightRed}65.33 $\pm$ 1.15 & \cellcolor{LightRed}58.09 $\pm$ 1.35 \\
MP & \cellcolor{LightBlue}55.68 $\pm$ 0.50 & \cellcolor{LightBlue}42.33 $\pm$ 1.65 & \cellcolor{LightBlue}55.39 $\pm$ 1.37 & \cellcolor{LightBlue}79.72 $\pm$ 0.74 & \cellcolor{LightBlue}46.54 $\pm$ 2.53 & \cellcolor{LightBlue}51.15 $\pm$ 2.48 & \cellcolor{LightBlue}12.01 $\pm$ 0.26 & \cellcolor{LightBlue}60.08 $\pm$ 2.61 & \cellcolor{LightBlue}51.84 $\pm$ 0.89 \\
EoT & \cellcolor{LightBlue}57.30 $\pm$ 0.75 & \cellcolor{LightBlue}39.15 $\pm$ 0.59 & \cellcolor{LightRed}79.70 $\pm$ 0.35 & \cellcolor{LightBlue}87.41 $\pm$ 0.31 & \cellcolor{LightBlue}50.44 $\pm$ 0.49 & \cellcolor{LightRed}87.00 $\pm$ 0.49 & \cellcolor{LightRed}33.08 $\pm$ 1.93 & \cellcolor{LightRed}66.33 $\pm$ 0.35 & \cellcolor{LightRed}58.13 $\pm$ 1.92 \\
ChatEval & \cellcolor{LightBlue}58.40 $\pm$ 0.17 & \cellcolor{LightBlue}43.87 $\pm$ 0.51 & \cellcolor{LightBlue}70.97 $\pm$ 1.61 & \cellcolor{LightBlue}83.68 $\pm$ 0.25 & \cellcolor{LightRed}53.00 $\pm$ 1.27 & \cellcolor{LightBlue}85.75 $\pm$ 0.61 & \cellcolor{LightRed}30.76 $\pm$ 0.30 & \cellcolor{LightBlue}52.44 $\pm$ 1.00 & \cellcolor{LightBlue}46.69 $\pm$ 1.46 \\
AgentVerse & \cellcolor{LightRed}62.85 $\pm$ 0.75 & \cellcolor{LightRed}47.72 $\pm$ 1.63 & \cellcolor{LightBlue}78.27 $\pm$ 0.68 & \cellcolor{LightRed}89.66 $\pm$ 0.56 & \cellcolor{LightRed}56.16 $\pm$ 0.82 & \cellcolor{LightBlue}59.38 $\pm$ 0.69 & \cellcolor{LightRed}30.73 $\pm$ 1.69 & \cellcolor{LightBlue}45.68 $\pm$ 8.57 & \cellcolor{LightBlue}43.22 $\pm$ 2.56 \\
\hline
\end{tabular}}
\end{table*}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.48\textwidth]{image/token_example.pdf}
    \caption{Comparing scaling efficiency of MAD methods. We present performance regarding number of tokens consumed.}
    \label{fig:token_scale}
\end{figure}

\subsection{Enhancing current MAD frameworks with stronger single-agent inference approaches} \label{appendix:cot}
As discussed in Section~\ref{sec:future}, enhancing current MAD frameworks with stronger single-agent inference methods represents a valuable future direction. We note that while EoT, AgentVerse, and MP incorporate CoT-like mechanisms, SoM and ChatEval do not explicitly prompt agents to respond in a CoT style. Therefore, we evaluate SoM and ChatEval combined with CoT, to investigate the impact of stronger single-agent inference approaches on the overall performance of MAD frameworks. Particularly, we explicitly prompt each agent in MAD to respond in a CoT style. Our experimental results are shown in Table Table~\ref{tab:main-cot}, from which we have several key findings

\begin{itemize}
    \item \textbf{CoT consistently improves MAD and Heter-MAD.} MAD-CoT and Heter-MAD-CoT surpass CoT on all benchmarks except MBPP. Notably, on MMLU, SoM-CoT improves CoT by 3.4\% and vanilla SoM by 9.4\%, while Heter-SoM-CoT improves CoT by 6.54\% and vanilla SoM by 12.54\%. We can also observe obvious improvements for ChatEval, where ChatEval-CoT also surpasses CoT on 5 benchmarks.
    \item \textbf{Heter-MAD and MAD-CoT improve MAD in distinct directions.} Interestingly, we observe that Heter-SoM-CoT only achieves approximate performances in comparison to SoM-CoT on mathematical reasoning and programming tasks (e.g., Heter-SoM-CoT only outperforms SoM-CoT on GSM8k among four benchmarks). However, Heter-SoM-CoT consistently outperforms SoM-CoT on all five general knowledge tasks, showing a significantly different trend. This observation suggests that Heter-MAD and MAD-CoT improve MAD from distinct perspectives, and they can work compatibly.
\end{itemize}

Our findings provide initial insight that integrating multi-agent collaborative inference methods with enhanced single-agent inference approaches can be a promising future direction. Notably, while CoT is generally compatible, it remains unclear whether other advanced single-agent inference approaches, such as ReACT~\citep{yao2022react} or Reflexion~\citep{shinn2024reflexion}, would achieve similar improvements, as these methods may alter the original behavior of single agents. We leave this for future investigation.

% \begin{figure*}[htb]
%      \centering
%      \includegraphics[width=0.98\textwidth]{image/behave_diff/behave_diff_all_6.pdf}
%         \caption{Comparing the behavior of inference strategies to direct prompting a single-agent. The green bar represents the number of corrected answers, and the Fred bar represents the number of answers erroneously reversed compared to standard single-agent prompting.}
%         \label{fig:asrs}
% \end{figure*} 

% \begin{figure*}[htbp]
%      \centering
%      \includegraphics[width=0.98\textwidth]{image/mixed/MAD_9_datasets_3_colorblind.pdf}
%         \caption{Heter-MAD performance analysis. We split questions in a benchmark into four parts each denoted as  CC, CW, WC, and WW, where CC represents questions that both GPT-4o-mini and Llama3.1-70b can solve. Similarly, WW represents questions that both models fail to solve, and CW denotes questions that only GPT-4o-mini can solve. In each part, the filled bar denotes how many questions are solved by Heter-MAD, while the hollow bar denotes how many questions are not solved by Heter-MAD.}
%         \label{fig:mixed_full}
% \end{figure*} 

\begin{table*}[tbp]
\caption{Empirical results on GPT-4o-mini combing CoT. We use \colorbox{LightRed}{lightred}/\colorbox{LightBlue}{lightblue} to denote results higher/lower than CoT.}
\label{tab:main-cot}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|lllllllll}
\toprule
\textbf{Dataset}    & \textbf{MMLU} & \textbf{MMLU-Pro} & \textbf{CommensenseQA} & \textbf{ARC-Challenge} & \textbf{AGIEval} & \textbf{GSM8K} & \textbf{MATH} & \textbf{HumanEval} & \textbf{MBPP} \\ \hline
SA & \cellcolor{LightBlue}65.33 $\pm$ 0.93 & \cellcolor{LightBlue}58.07 $\pm$ 0.50 & \cellcolor{LightBlue}79.47 $\pm$ 0.25 & \cellcolor{LightBlue}88.27 $\pm$ 0.41 & \cellcolor{LightBlue}63.87 $\pm$ 1.05 & \cellcolor{LightBlue}91.13 $\pm$ 0.34 & \cellcolor{LightBlue}71.67 $\pm$ 1.31 & \cellcolor{LightBlue}66.67 $\pm$ 1.15 & \cellcolor{LightBlue}58.11 $\pm$ 0.66 \\
CoT & 80.73 $\pm$ 0.34 & 62.80 $\pm$ 0.99 & 82.87 $\pm$ 0.25 & 93.53 $\pm$ 0.41 & 66.40 $\pm$ 1.30 & 93.60 $\pm$ 0.82 & 72.87 $\pm$ 1.20 & 78.05 $\pm$ 1.49 & 62.26 $\pm$ 0.84 \\
SC & \cellcolor{LightRed}82.13 $\pm$ 0.66 & \cellcolor{LightRed}66.27 $\pm$ 1.39 & \cellcolor{LightRed}83.80 $\pm$ 0.28 & \cellcolor{LightRed}93.93 $\pm$ 0.25 & \cellcolor{LightRed}67.07 $\pm$ 0.84 & \cellcolor{LightRed}95.67 $\pm$ 0.19 & \cellcolor{LightRed}73.96 $\pm$ 0.54 & -- & -- \\ \hline
SoM & \cellcolor{LightBlue}74.73 $\pm$ 0.52 & \cellcolor{LightBlue}62.80 $\pm$ 1.02 & \cellcolor{LightBlue}80.73 $\pm$ 0.93 & \cellcolor{LightBlue}90.80 $\pm$ 0.43 & \cellcolor{LightBlue}64.33 $\pm$ 0.34 & \cellcolor{LightRed}94.93 $\pm$ 0.34 & \cellcolor{LightRed}75.40 $\pm$ 0.71 & \cellcolor{LightBlue}68.09 $\pm$ 1.25 & \cellcolor{LightBlue}56.94 $\pm$ 1.12 \\
\textit{+CoT} & \cellcolor{LightRed}84.13 $\pm$ 0.50 & \cellcolor{LightRed}65.26 $\pm$ 1.90 & \cellcolor{LightRed}83.33 $\pm$ 0.31 & \cellcolor{LightRed}93.67 $\pm$ 0.23 & \cellcolor{LightRed}66.84 $\pm$ 0.74 & \cellcolor{LightRed}94.67 $\pm$ 0.42 & \cellcolor{LightRed}75.40 $\pm$ 0.53 & \cellcolor{LightRed}78.86 $\pm$ 1.27 & \cellcolor{LightBlue}61.22 $\pm$ 0.59 \\

\textit{+CoT +Heter} & \cellcolor{LightRed}87.27 $\pm$ 0.47 & \cellcolor{LightRed}68.20 $\pm$ 0.5 & \cellcolor{LightRed}84.81 $\pm$ 0.61 & \cellcolor{LightRed}93.75 $\pm$ 0.45 & \cellcolor{LightRed}70.03 $\pm$ 0.58 & \cellcolor{LightRed}96.00 $\pm$ 0.25 & \cellcolor{LightRed}74.80 $\pm$ 1.05 & \cellcolor{LightRed}78.66 $\pm$ 3.05  & \cellcolor{LightBlue} 59.92 $\pm$ 1.16 \\
\bottomrule
\hline
ChatEval & \cellcolor{LightBlue}79.13 $\pm$ 0.90 & \cellcolor{LightBlue}62.20 $\pm$ 0.49 & \cellcolor{LightBlue}81.07 $\pm$ 0.84 & \cellcolor{LightBlue}93.20 $\pm$ 0.28 & \cellcolor{LightRed}68.87 $\pm$ 0.94 & 93.60 $\pm$ 0.00 & \cellcolor{LightBlue}69.36 $\pm$ 1.58 & \cellcolor{LightBlue}71.75 $\pm$ 0.76 & \cellcolor{LightBlue}53.70 $\pm$ 0.55 \\
\textit{+CoT}& \cellcolor{LightRed}82.40 $\pm$ 0.40 &\cellcolor{LightRed}64.13 $\pm$ 0.64 & \cellcolor{LightRed}84.67 $\pm$ 0.95 & \cellcolor{LightRed}93.65 $\pm$ 0.31 & \cellcolor{LightBlue}65.87 $\pm$ 0.81 & \cellcolor{LightRed}95.40 $\pm$ 0.40 & \cellcolor{LightBlue}70.60 $\pm$ 1.40 & \cellcolor{LightBlue}77.40 $\pm$ 3.05 & \cellcolor{LightBlue}60.96 $\pm$ 0.22 \\
\bottomrule
% \textit{+Heter +CoT}& \cellcolor{LightRed}82.40 $\pm$ 0.40 &\cellcolor{LightRed}64.13 $\pm$ 0.64 & \cellcolor{LightRed}84.67 $\pm$ 0.95 & \cellcolor{LightRed}93.65 $\pm$ 0.31 & \cellcolor{LightBlue}65.87 $\pm$ 0.81 & \cellcolor{LightRed}95.40 $\pm$ 0.40 & \cellcolor{LightBlue}70.60 $\pm$ 1.40 & \cellcolor{LightBlue}77.40 $\pm$ 3.05 & \cellcolor{LightBlue}60.96 $\pm$ 0.22 \\
% \hline
\end{tabular}}
\end{table*}