

% \begin{table*}[htbp]
% \caption{Main results on Llama3.1:70B. We use \colorbox{LightRed}{lightred}/\colorbox{LightBlue}{lightblue} to denote results higher/lower than CoT.}
% \label{tab:main-llama70b}
% \resizebox{\textwidth}{!}{
% \begin{tabular}{l|lllllllll}
% \hline
% Dataset    & MMLU & MMLU-Pro & CommensenseQA & ARC-Challenge & AGIEval & GSM8K & MATH & HumanEval & MBPP \\ \hline
% SA & \cellcolor{LightBlue}80.20 $\pm$ 2.05 & \cellcolor{LightBlue}46.27 $\pm$ 0.66 & \cellcolor{LightBlue}79.13 $\pm$ 1.05 & \cellcolor{LightBlue}91.67 $\pm$ 0.25 & \cellcolor{LightBlue}56.87 $\pm$ 1.93 & \cellcolor{LightBlue}69.47 $\pm$ 0.90 & \cellcolor{LightRed}38.13 $\pm$ 1.09 & \cellcolor{LightRed}63.41 $\pm$ 1.72 & \cellcolor{LightBlue}45.78 $\pm$ 2.95 \\
% CoT & 82.73 $\pm$ 1.25 & 53.87 $\pm$ 0.90 & 82.40 $\pm$ 1.45 & 93.33 $\pm$ 0.50 & 58.42 $\pm$ 1.53 & 92.07 $\pm$ 0.90 & 37.13 $\pm$ 0.98 & 62.60 $\pm$ 1.04 & 49.42 $\pm$ 2.52 \\
% SC & \cellcolor{LightRed}83.73 $\pm$ 0.19 & \cellcolor{LightBlue}53.27 $\pm$ 1.06 & \cellcolor{LightBlue}81.16 $\pm$ 0.63 & \cellcolor{LightBlue}92.80 $\pm$ 0.59 & \cellcolor{LightRed}61.07 $\pm$ 0.25 & \cellcolor{LightBlue}83.20 $\pm$ 0.33 & \cellcolor{LightRed}49.73 $\pm$ 0.62 & -- & -- \\
% \hline
% SoM & \cellcolor{LightRed}84.60 $\pm$ 0.43 & \cellcolor{LightRed}57.13 $\pm$ 1.15 & \cellcolor{LightBlue}81.93 $\pm$ 0.34 & \cellcolor{LightBlue}92.93 $\pm$ 0.52 & \cellcolor{LightRed}62.22 $\pm$ 1.08 & \cellcolor{LightBlue}88.27 $\pm$ 0.74 & \cellcolor{LightRed}48.64 $\pm$ 1.23 & \cellcolor{LightRed}63.41 $\pm$ 2.28 & \cellcolor{LightBlue}41.37 $\pm$ 0.49 \\
% MP & \cellcolor{LightBlue}81.39 $\pm$ 1.09 & \cellcolor{LightBlue}51.93 $\pm$ 2.29 & \cellcolor{LightBlue}68.55 $\pm$ 1.02 & \cellcolor{LightBlue}88.38 $\pm$ 0.03 & \cellcolor{LightRed}61.60 $\pm$ 2.55 & \cellcolor{LightBlue}69.27 $\pm$ 1.05 & \cellcolor{LightBlue}24.60 $\pm$ 1.56 & \cellcolor{LightBlue}52.64 $\pm$ 1.04 & \cellcolor{LightBlue}32.56 $\pm$ 0.97 \\
% EoT & \cellcolor{LightRed}83.20 $\pm$ 0.28 & \cellcolor{LightBlue}49.66 $\pm$ 0.60 & \cellcolor{LightBlue}81.87 $\pm$ 0.74 & \cellcolor{LightBlue}92.96 $\pm$ 0.14 & \cellcolor{LightRed}63.06 $\pm$ 0.46 & \cellcolor{LightBlue}77.60 $\pm$ 0.65 & \cellcolor{LightRed}43.63 $\pm$ 2.28 & \cellcolor{LightBlue}55.49 $\pm$ 0.86 & \cellcolor{LightBlue}38.91 $\pm$ 1.68 \\
% ChatEval & \cellcolor{LightBlue}80.37 $\pm$ 1.15 & \cellcolor{LightRed}56.13 $\pm$ 1.00 & \cellcolor{LightBlue}72.82 $\pm$ 1.33 & \cellcolor{LightBlue}89.94 $\pm$ 0.20 & \cellcolor{LightRed}68.59 $\pm$ 0.42 & \cellcolor{LightRed}92.53 $\pm$ 0.19 & \cellcolor{LightRed}58.73 $\pm$ 1.52 & \cellcolor{LightRed}62.80 $\pm$ 0.86 & \cellcolor{LightBlue}44.49 $\pm$ 2.23 \\
% AgentVerse & \cellcolor{LightRed}84.80 $\pm$ 1.02 & \cellcolor{LightRed}61.80 $\pm$ 0.91 & \cellcolor{LightBlue}76.47 $\pm$ 1.24 & \cellcolor{LightBlue}92.80 $\pm$ 0.28 & \cellcolor{LightRed}66.73 $\pm$ 0.84 & \cellcolor{LightBlue}85.47 $\pm$ 0.68 & \cellcolor{LightRed}45.33 $\pm$ 0.94 & \cellcolor{LightBlue}59.96 $\pm$ 0.76 & \cellcolor{LightBlue}41.89 $\pm$ 1.02 \\
% \hline
% \end{tabular}}
% \end{table*}


% \begin{table*}[htbp]
% \caption{Main results on Claude-3-Haiku. We use \colorbox{LightRed}{lightred}/\colorbox{LightBlue}{lightblue} to denote results higher/lower than CoT.}
% \label{tab:main-claude}
% \resizebox{\textwidth}{!}{
% \begin{tabular}{l|lllllllll}
% \hline
% Dataset    & MMLU & MMLU-Pro & CommensenseQA & ARC-Challenge & AGIEval & GSM8K & MATH & HumanEval & MBPP \\ \hline
% SA & \cellcolor{LightBlue}56.81 $\pm$ 0.15 & \cellcolor{LightBlue}38.38 $\pm$ 0.36 & \cellcolor{LightBlue}79.40 $\pm$ 0.28 & \cellcolor{LightBlue}87.17 $\pm$ 0.54 & \cellcolor{LightBlue}48.99 $\pm$ 1.31 & \cellcolor{LightBlue}83.13 $\pm$ 0.09 & \cellcolor{LightRed}31.71 $\pm$ 2.48 & \cellcolor{LightRed}66.26 $\pm$ 0.76 & \cellcolor{LightBlue}48.55 $\pm$ 0.62 \\
% CoT & 62.00 $\pm$ 0.00 & 47.00 $\pm$ 1.57 & 79.67 $\pm$ 0.34 & 89.47 $\pm$ 0.38 & 52.00 $\pm$ 1.02 & 85.84 $\pm$ 0.72 & 30.62 $\pm$ 1.42 & 65.24 $\pm$ 2.59 & 56.16 $\pm$ 0.92 \\
% SC & \cellcolor{LightRed}63.08 $\pm$ 1.06 & \cellcolor{LightRed}50.02 $\pm$ 1.47 & \cellcolor{LightRed}81.27 $\pm$ 0.05 & \cellcolor{LightRed}90.00 $\pm$ 0.28 & \cellcolor{LightRed}53.59 $\pm$ 1.09 & \cellcolor{LightRed}90.27 $\pm$ 0.46 & \cellcolor{LightRed}35.09 $\pm$ 0.69 & -- & -- \\
% \hline
% SoM & \cellcolor{LightBlue}57.39 $\pm$ 1.00 & \cellcolor{LightBlue}39.91 $\pm$ 0.47 & \cellcolor{LightBlue}79.40 $\pm$ 0.59 & \cellcolor{LightBlue}88.20 $\pm$ 0.23 & \cellcolor{LightBlue}51.48 $\pm$ 0.42 & \cellcolor{LightRed}86.87 $\pm$ 0.73 & \cellcolor{LightRed}34.31 $\pm$ 0.89 & \cellcolor{LightRed}65.33 $\pm$ 1.15 & \cellcolor{LightRed}58.09 $\pm$ 1.35 \\
% MP & \cellcolor{LightBlue}55.68 $\pm$ 0.50 & \cellcolor{LightBlue}42.33 $\pm$ 1.65 & \cellcolor{LightBlue}55.39 $\pm$ 1.37 & \cellcolor{LightBlue}79.72 $\pm$ 0.74 & \cellcolor{LightBlue}46.54 $\pm$ 2.53 & \cellcolor{LightBlue}51.15 $\pm$ 2.48 & \cellcolor{LightBlue}12.01 $\pm$ 0.26 & \cellcolor{LightBlue}60.08 $\pm$ 2.61 & \cellcolor{LightBlue}51.84 $\pm$ 0.89 \\
% EoT & \cellcolor{LightBlue}57.30 $\pm$ 0.75 & \cellcolor{LightBlue}39.15 $\pm$ 0.59 & \cellcolor{LightRed}79.70 $\pm$ 0.35 & \cellcolor{LightBlue}87.41 $\pm$ 0.31 & \cellcolor{LightBlue}50.44 $\pm$ 0.49 & \cellcolor{LightRed}87.00 $\pm$ 0.49 & \cellcolor{LightRed}33.08 $\pm$ 1.93 & \cellcolor{LightRed}66.33 $\pm$ 0.35 & \cellcolor{LightRed}58.13 $\pm$ 1.92 \\
% ChatEval & \cellcolor{LightBlue}58.40 $\pm$ 0.17 & \cellcolor{LightBlue}43.87 $\pm$ 0.51 & \cellcolor{LightBlue}70.97 $\pm$ 1.61 & \cellcolor{LightBlue}83.68 $\pm$ 0.25 & \cellcolor{LightRed}53.00 $\pm$ 1.27 & \cellcolor{LightBlue}85.75 $\pm$ 0.61 & \cellcolor{LightRed}30.76 $\pm$ 0.30 & \cellcolor{LightBlue}52.44 $\pm$ 1.00 & \cellcolor{LightBlue}46.69 $\pm$ 1.46 \\
% AgentVerse & \cellcolor{LightRed}62.85 $\pm$ 0.75 & \cellcolor{LightRed}47.72 $\pm$ 1.63 & \cellcolor{LightBlue}78.27 $\pm$ 0.68 & \cellcolor{LightRed}89.66 $\pm$ 0.56 & \cellcolor{LightRed}56.16 $\pm$ 0.82 & \cellcolor{LightBlue}59.38 $\pm$ 0.69 & \cellcolor{LightRed}30.73 $\pm$ 1.69 & \cellcolor{LightBlue}45.68 $\pm$ 8.57 & \cellcolor{LightBlue}43.22 $\pm$ 2.56 \\
% \hline
% \end{tabular}}
% \end{table*}

\section{Revisiting the Status Quo: A Comprehensive Evaluation} \label{sec:exp}

Existing MAD research claims to improve LLM performance by leveraging computational resources---through more LLM calls or more token consumption---during inference time to generate better responses. 
%Naturally, we expect a comprehensive empirical study and analysis of the overall behavior of MAD methods in comparison to single-agent ones, which may have not been convincingly delivered in previous research. Therefore, 
In this section, we systematically evaluate MAD frameworks to assess their performance and efficiency, aiming to provide insights into current MAD approaches. 
%For a comprehensive assessment, our evaluation encompasses 9 benchmarks, convering diverse domains such as mathematical reasoning, general knowledge, and programming  of MAD methods. By conducting this comprehensive analysis, we aim to provide deeper insights into the strengths and limitations of MAD approaches.


\subsection{Experimental Setup}
% remove "three types of datasets...."
\myparatight{Datasets} We conduct our evaluation on 9 widely adopted standard benchmarks that cover 3 top-level capabilities of LLMs: general knowledge, mathematical reasoning, and programming. The benchmarks are:  MMLU~\citep{mmlu1}, MMLU-Pro~\citep{mmlupro}, AGIEval~\citep{agieval}, CommensenseQA~\citep{csqa}, ARC-Challenge~\citep{arc}, GSM8K~\citep{gsm8k}, MATH~\citep{math}, HumanEval~\citep{humaneval}, and MBPP~\citep{mbpp}.  
%To eliminate the impact of randomness in LLM-generated text on the evaluation results, we repeated the experiment three times and reported the average and standard deviation of the results to present the performance differences at a statistical level.
We briefly summarize their basic information in Table \ref{tab:dataset_desc}. More details are included in Appendix \ref{appendix:benchmark}.

\begin{table}[t]
\caption{Benchmark configurations}
\label{tab:dataset_desc}
\centering
\begin{tabular}{l|ll}
\hline
Benchmark     & Category                   & Metric\\ \hline
MMLU          & General Knowledge           & accuracy, 0-shot\\
MMLUPro       & General Knowledge          & accuracy, 0-shot\\
CommensenseQA & General Knowledge           & accuracy, 0-shot\\
ARC-Challenge & General Knowledge          & accuracy, 0-shot\\
AGIEval       & General Knowledge           & accuracy, 0-shot\\
GSM8k         & Mathematical Reasoning & accuracy, 0-shot\\
MATH          & Mathematical Reasoning & accuracy, 0-shot\\
HumanEval     & Programming            & Pass@1, 0-shot\\
MBPP          & Programming            & Pass@1, 0-shot\\ \hline
\end{tabular}
\end{table}
%As shown in Table \ref{tab:dataset_desc}, these benchmarks use clear evaluation criteria, such as accuracy, which ensure that the evaluation results are not influenced by the preferences of specific metrics, allowing for definitive evaluation outcomes.
%To save evaluation expenses, we consider 500 test samples for each benchmark.
%to 500 if there are more than 500 samples in the testing set.

% \begin{table}[htbp]
% \begin{tabular}{l|lll}
% \hline
% Benchmark     & \#questions & Type                   & Description                                                             \\ \hline
% MMLU          & 14042       & Knowledge QA           & general knowledge across 57 subjects                                    \\
% MMLU-Pro       & 1200        & Knowledge QA           & challenging massive multi-task understanding dataset                    \\
% CommensenseQA & 1140        & Knowledge QA           & commonsense reasoning                                                   \\
% ARC-Challenge &             & Knowledge QA           & problem-solving, reasoning, and generalization across multiple domains. \\
% AGIEval       & 3548        & Knowledge QA           & genuine grade-school level, multiple-choice science questions.          \\
% GSM8K         & 1319        & Mathematical Reasoning & problem-solving abilities in arithmetic and algebra.                    \\
% MATH          & 5000        & Mathematical Reasoning & math problems across geometry, algebra, calculus, and number theory     \\
% HumanEval     & 164         & Programming            & code generation and programming skills                                  \\
% MBPP          & 500         & Programming            & code generation and programming skills                                  \\ \hline
% \end{tabular}
% \end{table}

%Note that we intentionally avoid including ``vague'' benchmarks, e.g., machine translation or context extraction, which often rely on unstable metrics (BLEU~\citep{bleu}, ROUGE~\citep{rouge}) to evaluate the tested LLM. 

\myparatight{Foundation models} We consider both proprietary LLMs and open-sourced LLMs. They are \textit{gpt-4o-mini-2024-07-18}, \textit{claude-3-5-haiku-2024-1022}, \textit{Llama3.1:8b-instruct}, and \textit{Llama3.1:70b-instruct}.
%\footnote{gpt-4o-mini-2024-07-18}~\citep{gpt4o-mini}, Claude-3.5-haiku\footnote{claude-3-5-haiku-2024-1022}~\citep{claude3-haiku},  Llama3.1-8b\footnote{https://ollama.com/library/llama3.1:8b-instruct-q4\_K\_M} and Llam3.1-70b\footnote{https://ollama.com/library/llama3.1:70b-instruct-q4\_K\_M}~\citep{llama3} served with ollama. 
Unless stated otherwise, we maintain consistent inference configurations throughout our evaluation, setting the temperature $T=1$ and $top\text{-}p=1$, to balance generation quality and diversity.

\begin{table*}[tbp]
\caption{High-level comparison of MAD frameworks.}
\label{tab:mad_features}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|lllllll}
\toprule
           & Role-Play & Answer Aggregation         & \#Agents   & \#Rounds       & Post-processing & Role Diversity \\ \midrule
SoM              & N/A       & Majority Voting & Adjustable & Fixed          & N/A             & No            \\
MP                 & Fixed     & Judger          & Fixed      & Early-stopping & N/A             & Yes           \\
EoT              & Fixed     & Majority Voting & Adjustable & Early-stopping & Confidence      & No            \\
ChatEval           & Fixed     & Majority Voting                & Adjustable & Early-stopping & N/A             & Yes           \\
AgentVerse        & Dynamic   & Judger          & Adjustable & Early-stopping & N/A             & Yes           \\ 
\bottomrule
\end{tabular}
}
\end{table*}

\myparatight{MAD methods and baselines} We consider five representative %state-of-the-art 
 MAD frameworks and three single-agent baselines: 
 single-agent (SA), 
Chain-of-Thought (CoT)~\citep{cot}, Self-Consistency (SC)~\citep{wang2022self}, Society-of-Minds (SoM)~\citep{duimproving}, Multi-Persona (MP)~\citep{liang2023encouraging}, Exchange-of-Thoughts (EoT)~\citep{yin2023exchange}, AgentVerse~\citep{chen2023agentverse}, and ChatEval~\citep{chanchateval}.  
SA simply prompts the agent with only the necessary problem description to generate the response.
CoT prompts the agent with ``Let's think step by step'' to elicit step-by-step reasoning.
SC repetitively samples from a CoT agent and utilizes majority voting to determine the final answer.
SoM is the first MAD method proposed, serving as the foundation of a number of recent attempts~\citep{liang2023encouraging, chen2024IoA, xiong-FORD, qian2024scaling}. 
MP, EoT, AgentVerse, and ChatEval, are representative MAD frameworks that differ in their approaches to role-play, communication, answer aggregation, as summarized in Table \ref{tab:mad_features}.
Despite these differences, they have all attracted significant interest.  

For all MAD methods considered,%~\citep{duimproving, yin2023exchange, liang2023encouraging, chen2023agentverse,chanchateval}, 
we follow the authors' open-source implementations. 
%For ChatEval, which does not have an official open-source implementation available as of 2024/10, we carefully reproduce it based on the implementation details provided in the paper~\citep{chanchateval}.
For fair comparison across different MAD methods, we slightly adjust the number of debate rounds of these methods to ensure that they all align to a similar amount of inference budget measured by the number of LLM calls.
%---a single invocation of an agent to generate a response.
%We intentionally adopt this  metric, rather than the amount of token consumed, as the latter generally does not allow for precise control. 
Unless otherwise mentioned, we consider the number of LLM calls to be 6, following the convention \citep{duimproving, chanchateval}.
%provided in the raw papers~\citep{duimproving, yin2023exchange, liang2023encouraging, chen2023agentverse,chanchateval}, 
%so that in SoM, 3 agents debate for 2 rounds, which in MP, two solver agents and a judger agent debate until early-stopping, etc.
%In MP, the angel and devil agent debate for at most 5 rounds, and a judge agent can terminate the debate in advance. Similarly, EoT incorporates 3 agents with early stopping when all agents reach a consensus. We consider the one-by-one setting for ChatEval, in which three agents give feedback iteratively for 2 rounds. Finally, AgentVerse includes 6 agents with diverse roles like recruiter, solver, or evaluator. \footnote{For all MAD methods except ChatEval, we followed the authors' open-source implementations. As of 2024/10, ChatEval does not have an official open-source implementation available, so we reproduced it based on the implementation details provided in its paper.} 
We present more implementation details, including agents' prompts, communication strategies, and agent roles in Appendix \ref{appendix:mad_conf}.
Our code is publicly available at \url{https://anonymous.4open.science/r/MAD-eval-E4C4/} for reproducibility.



\subsection{Experimental results}

\myparatight{Does MAD outperform simple single-agent baselines?} We first compare MAD frameworks to single-agent baselines to assess their relative performance. For robustness, we repeated the experiments three times, reporting the standard deviations. Table \ref{tab:main-gpt} and Tables \ref{tab:main-llama8b}, \ref{tab:main-llama70b}, \ref{tab:main-claude} in Appendix \ref{appendix:exp_res} present empirical results on GPT-4o-mini, Llama3.1-8b, Llama3.1-70b, Claude-3.5-haiku, respectively.
In these tables, all methods are compared against CoT, with results higher or lower than CoT denoted in \colorbox{LightRed}{lightred} and \colorbox{LightBlue}{lightblue}, respectively. Our results indicate that MAD methods fail to consistently outperform CoT across different models and benchmarks. Specifically, in Table \ref{tab:main-gpt}, SoM underperforms CoT on all nine datasets, when utilizing the GPT-4o-mini model. Similarly, more advanced frameworks such as ChatEval and AgentVerse merely outperform CoT on one out of nine datasets. Furthermore, analyses across other models reveal that while MAD frameworks occasionally achieve better performance, they generally underperform CoT. For instance, AgentVerse is the only MAD framework that outperforms CoT on MMLU using Claude-3.5-haiku, achieving a +0.85\% performance gain. However, all other MAD frameworks underperform CoT by at least -3.60\%. 
When being compared to SC, the underperformance of MAD approaches is more noticeable. %We have a similar observation when comparing MAD frameworks with SC. 
%Observe that SC surpasses MAD frameworks by 1.73\% to 14.26\% on MMLU with GPT-4o-mini. 
In most cases when SC can be applied\footnote{We follow~\citep{wang2022self} which assumes the need for a single correct answer to be determined by majority voting. As such, for SC, we exclude programming tasks that allow multiple valid programs.}, SC achieves the highest performance, defeating CoT, not to mention MAD methods.

\begin{table*}[tbp]
\caption{Performance results on GPT-4o-mini. We use \colorbox{LightRed}{lightred}/\colorbox{LightBlue}{lightblue} to denote results higher/lower than CoT.}
\label{tab:main-gpt}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|lllllllll}
\hline
\textbf{Dataset}    & \textbf{MMLU} & \textbf{MMLU-Pro} & \textbf{CommensenseQA} & \textbf{ARC-Challenge} & \textbf{AGIEval} & \textbf{GSM8K} & \textbf{MATH} & \textbf{HumanEval} & \textbf{MBPP} \\ \hline
SA & \cellcolor{LightBlue}65.33 $\pm$ 0.93 & \cellcolor{LightBlue}58.07 $\pm$ 0.50 & \cellcolor{LightBlue}79.47 $\pm$ 0.25 & \cellcolor{LightBlue}88.27 $\pm$ 0.41 & \cellcolor{LightBlue}63.87 $\pm$ 1.05 & \cellcolor{LightBlue}91.13 $\pm$ 0.34 & \cellcolor{LightBlue}71.67 $\pm$ 1.31 & \cellcolor{LightBlue}66.67 $\pm$ 1.15 & \cellcolor{LightBlue}58.11 $\pm$ 0.66 \\
CoT & 80.73 $\pm$ 0.34 & 62.80 $\pm$ 0.99 & 82.87 $\pm$ 0.25 & 93.53 $\pm$ 0.41 & 66.40 $\pm$ 1.30 & 93.60 $\pm$ 0.82 & 72.87 $\pm$ 1.20 & 78.05 $\pm$ 1.49 & 62.26 $\pm$ 0.84 \\
SC & \cellcolor{LightRed}82.13 $\pm$ 0.66 & \cellcolor{LightRed}66.27 $\pm$ 1.39 & \cellcolor{LightRed}83.80 $\pm$ 0.28 & \cellcolor{LightRed}93.93 $\pm$ 0.25 & \cellcolor{LightRed}67.07 $\pm$ 0.84 & \cellcolor{LightRed}95.67 $\pm$ 0.19 & \cellcolor{LightRed}73.96 $\pm$ 0.54 & -- & -- \\ \hline
SoM & \cellcolor{LightBlue}74.73 $\pm$ 0.52 & \cellcolor{LightBlue}62.80 $\pm$ 1.02 & \cellcolor{LightBlue}80.73 $\pm$ 0.93 & \cellcolor{LightBlue}90.80 $\pm$ 0.43 & \cellcolor{LightBlue}64.33 $\pm$ 0.34 & \cellcolor{LightRed}94.93 $\pm$ 0.34 & \cellcolor{LightRed}75.40 $\pm$ 0.71 & \cellcolor{LightBlue}68.09 $\pm$ 1.25 & \cellcolor{LightBlue}56.94 $\pm$ 1.12 \\
MP & \cellcolor{LightBlue}75.47 $\pm$ 0.84 & \cellcolor{LightBlue}60.53 $\pm$ 1.27 & \cellcolor{LightBlue}68.07 $\pm$ 1.57 & \cellcolor{LightBlue}90.27 $\pm$ 0.25 & \cellcolor{LightBlue}61.67 $\pm$ 1.43 & \cellcolor{LightBlue}90.87 $\pm$ 0.19 & \cellcolor{LightBlue}51.87 $\pm$ 0.66 & \cellcolor{LightBlue}63.01 $\pm$ 2.30 & \cellcolor{LightBlue}45.78 $\pm$ 0.80 \\
EoT & \cellcolor{LightBlue}67.87 $\pm$ 0.41 & \cellcolor{LightBlue}61.20 $\pm$ 0.65 & \cellcolor{LightBlue}80.07 $\pm$ 0.52 & \cellcolor{LightBlue}86.40 $\pm$ 0.28 & \cellcolor{LightBlue}65.07 $\pm$ 0.66 & \cellcolor{LightBlue}91.40 $\pm$ 0.57 & \cellcolor{LightRed}75.93 $\pm$ 1.23 & \cellcolor{LightBlue}73.78 $\pm$ 2.17 & \cellcolor{LightBlue}56.16 $\pm$ 0.49 \\
ChatEval & \cellcolor{LightBlue}79.13 $\pm$ 0.90 & \cellcolor{LightBlue}62.20 $\pm$ 0.49 & \cellcolor{LightBlue}81.07 $\pm$ 0.84 & \cellcolor{LightBlue}93.20 $\pm$ 0.28 & \cellcolor{LightRed}68.87 $\pm$ 0.94 & 93.60 $\pm$ 0.00 & \cellcolor{LightBlue}69.36 $\pm$ 1.58 & \cellcolor{LightBlue}71.75 $\pm$ 0.76 & \cellcolor{LightBlue}53.70 $\pm$ 0.55 \\
AgentVerse & \cellcolor{LightBlue}80.40 $\pm$ 0.00 & \cellcolor{LightBlue}62.07 $\pm$ 0.52 & \cellcolor{LightBlue}80.73 $\pm$ 0.41 & \cellcolor{LightBlue}92.47 $\pm$ 0.09 & \cellcolor{LightBlue}63.87 $\pm$ 1.23 & \cellcolor{LightBlue}92.73 $\pm$ 0.50 & \cellcolor{LightBlue}64.49 $\pm$ 1.38 & \cellcolor{LightRed}85.57 $\pm$ 1.25 & \cellcolor{LightBlue}58.88 $\pm$ 0.18 \\
\hline
\end{tabular}}
\end{table*}

To gain a more rigorous and holistic view of MAD's performance relative to CoT, we aggregated results from 36 experimental configurations (four models, nine datasets). For each configuration, we conducted an ANOVA test with a significance level of $0.05$ to assess whether MAD frameworks statistically outperformed, tied, or underperformed compared to CoT. Based on the results, each comparison was categorized as a Win, Tie, or Lose. As shown in Figure \ref{fig:main_pie}, SoM, EoT, ChatEval, and AgentVerse only outperformed CoT in approximately 15\% cases, while MP did not demonstrate significant improvement over CoT. Although ChatEval achieved the lowest loss rate, its win rate is still not greater than 15\%. 
When examining performance by task type, MAD frameworks performed worse on programming tasks (only SoM/AgentVerse had positive win rates) but better on mathematical reasoning, surpassing their overall performance levels.

% \begin{figure}[htbp]
%      \centering
%      \begin{subfigure}[b]{0.235\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{image/rounds_example1.pdf} 
%          \caption{Debate Rounds}
%         \label{fig:rounds}
%      \end{subfigure}
%      \hfill
%      \begin{subfigure}[b]{0.235\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{image/agent_count_example1.pdf}
%          \caption{Number of Agents}
%         \label{fig:numOfAgents}
%      \end{subfigure}
%      \caption{We explore the impact of hyperparameters on the performance of MAD frameworks by increasing the debate rounds or number of agents, and compare the results to CoT.}
% \end{figure}

\begin{figure}[tbp]
     \centering
     \begin{subfigure}[b]{0.98\textwidth}
         \centering
         \includegraphics[width=\textwidth]{image/rounds_example.pdf} 
         \caption{Debate Rounds}
         \label{fig:rounds}
     \end{subfigure}
     \vfill
     \begin{subfigure}[b]{0.98\textwidth}
         \centering
         \includegraphics[width=\textwidth]{image/agent_count_example.pdf}
         \caption{Number of Agents}
         \label{fig:numOfAgents}
     \end{subfigure}
     \caption{We explore the impact of hyperparameters on the performance of MAD frameworks by increasing the debate rounds or number of agents, and compare the results to CoT.}
\end{figure}


\myparatight{Do we replicate previous results?} The empirical results presented above demonstrate that the considered  MAD frameworks typically underperform the much simpler single-agent baseline CoT, a somewhat surprising finding that has not been reported before.  
However, we do observe that, in general, these MAD frameworks are able to outperform single-agent (SA) that are instructed to directly generate their answers. 
Specifically, we observe that MAD frameworks outperform SA in most conditions (34 out of 45 conditions utilizing GPT-4o-mini), which perfectly aligns with previous findings in the field. 
%a finding that may initially seem unexpected. However, these observations are consistent with prior research in the MAD domain. Specifically, our evaluation successfully replicates earlier findings, as illustrated in Tables \ref{tab:main-gpt}, \ref{tab:main-llama8b}, \ref{tab:main-llama70b}, and \ref{tab:main-claude}. These tables reveal that most MAD frameworks outperform standard single-agent baselines, aligning with previous studies in the field.

We acknowledge that ChatEval and SoM were not compared to CoT though, EoT, AgentVerse, and MP were compared to CoT in their papers.
However, MP was evaluated against CoT solely on the CIAR dataset~\citep{liang2023encouraging}, which was initially proposed
%has rarely been adopted 
and has not been fully disclosed. 
EoT was shown to outperform CoT~\citep{yin2023exchange}. Our evaluation replicates that EoT can surpass CoT on the GSM8K and CommensenseQA benchmarks, but this advantage was observed only when using Claude-3.5-haiku among the four models.
Furthermore, as shown in Table \ref{tab:main-gpt}, we also replicate that AgentVerse can achieve a superior performance on the HumanEval benchmark, significantly outperforming other methods including CoT. 
However, note that when handling programming tasks, AgentVerse incorporates an additional execution-evaluation stage so that agents can utilize the execution results from the generated programs, which is usually absent in other MAD frameworks and arguably beyond the scope of MAD designs.
%albeit by utilizing a code execution tool that exceeds the constraints of our experimental setup.
%our evaluation showed that EoT was indeed able to outperforms CoT, but only with the use of one particular foundation model out of the four we consdiered. on only 1 out of 4 models (as shown in Table~\ref{tab:main-claude}).
%This discrepancy may be attributed to differences in the models used between our study and the original paper. Furthermore, Consequently, our empirical findings remain in alignment with previous observations in MAD research.

\myparatight{How do hyperparameters influence MAD performance?} As mentioned earlier, we by default followed the conventional choice of hyperparameters (the number of agents and the number of debate rounds). That said, one might be interested in how varying the choices influences MAD performance as well as our key findings. 
%Our prior analysis revealed two key findings: (i) MAD generally fail to outperform single-agent baselines, and (ii) these empirical results, though initially surprising, are consistent with previous studies in the field. This naturally raises the concern of whether the observed underperformance is caused by suboptimal hyperparameter configurations, as existing research indicates that specific hyperparameters—such as the number of agents and the number of debate rounds—can significantly influence the performance of MAD frameworks~\citep{qian2024scaling, duimproving}. 
Thus, we conducted a systematic ablation study, %to investigate how hyperparameters impact MAD performance. Specifically, we varied the number of agents and the number of debating rounds. The study 
utilizing GSM8K, MMLU, and HumanEval as representative benchmarks for each top-level LLM capability. MAD frameworks with fixed numbers of agents, such as ChatEval and MP, were excluded from experiments varying the number of agents. Similarly, EoT, MP, and AgentVerse were excluded from experiments involving debate rounds due to their early stopping mechanism, which does not allow precisely adjusting the number of debate rounds.

Our empirical results, summarized in Figures~\ref{fig:rounds} and \ref{fig:numOfAgents}, indicate that 
%the impact of hyperparameter variations differs across different datasets, models, and MAD methods. 
in most scenarios, increasing the number of agents or debate rounds does not significantly change the outcomes. For instance, the SoM framework continues to underperform CoT on the MMLU and HumanEval benchmarks, even as debate rounds increase from 2 to 4 or the number of agents increases from 3 to 9 on HumanEval. Conversely, SoM always surpasses CoT on GSM8K when varying debate rounds. The sole notable exception is observed with EoT on the GSM8K benchmark, where increasing the number of agents from 3 to 9 leads to a continuous improvement in performance, ultimately surpassing CoT. Nonetheless, aside from this exception, increasing the number of agents or debate rounds often results in either stagnation or even a decline in performance. These results show that superior performance over CoT across various benchmarks cannot be realized by merely adjusting hyperparameters. Consequently, our study rules out suboptimal hyperparameter configurations as a universal explanation for the inferior performance of MAD when compared to CoT.



\myparatight{Can MAD efficiently utilize more inference budget?} Beyond performance, we also assess the efficiency of MAD frameworks in utilizing inference-time computational resources. We measured the number of tokens consumed by MAD methods in our experiments. Unlike the number of LLM calls, which can be pre-defined, most LLMs do not allow precise control over token consumption. Moreover, for proprietary LLMs, the number of tokens consumed is typically positively correlated with cost. 
We present how the performance of MAD scales with the increase of token consumption in Figure \ref{fig:token_scale} in Appendix \ref{appendix:exp_res}. 
Note that we exclude MP in the figure as it must involve 2 agents and is not allowed to pre-define the number of debating rounds in advance due to its early-stopping mechanism. 
Moreover, for comparison, we additionally include the results of SC by increasing the number of samples it draws.

%Similar to previous experiments, we increased the number of agents from 3 to 9 and the number of debate rounds from 2 to 4, thereby granting MAD frameworks access to more inference budget. Correspondingly, we increased the repetition of SC to a maximum of 18, which is equivalent to 9 agents debating for 2 rounds in MAD frameworks. As discussed above, MP was not included in this experiment as it is not trivial to adjust its number of agents or debate rounds. We extracted the number of tokens consumed by MAD and SC, along with their corresponding performance metrics, from our experimental records to analyze the trend of each evaluated method in utilizing additional inference budget.
We observe that 
SC effectively utilizes the increased inference budget. However, MAD frameworks either: (i) show no positive trend in achieving stable performance improvements with more inference budget, e.g., SoM does not stably achieve better performance on MMLU as consuming more tokens, or (ii) continue to underperform SC while consuming a comparable number of tokens although positively scaling up, e.g., EoT performs better as more tokens consumed on MMLU and GSM8K, but still obviously underperforms SC or even other MAD frameworks. These observations indicate that, in comparison to SC, MAD is generally a less efficient method for leveraging token consumption.

\begin{figure*}[tb!]
     \centering
     \includegraphics[width=0.98\textwidth]{image/behave_diff/behave_diff_all_6.pdf}
        \caption{Comparing the behavior of inference strategies to direct prompting a single-agent. The green bar represents the number of corrected answers, and the Fred bar represents the number of answers erroneously reversed compared to standard single-agent prompting.}
        \label{fig:asrs}
\end{figure*} 

\myparatight{Why do MAD methods underperform single-agent baselines?} We analyze the performance of MAD on individual questions to gain deeper insights. Specifically, we compared each evaluated MAD method against SA by examining two key metrics: the number of incorrect answers corrected by the method and the number of correct answers erroneously altered by the method. 
Ideally, MAD should be able to correct a substantial number of errors while introducing minimal new errors.
These results are visualized in Figure \ref{fig:asrs}.
%, where green bars represent the number of corrected answers, and red bars represent the number of errors introduced. 


%As illustrated in Figure \ref{fig:asrs}, 
We find that
%SC demonstrates strong performance by effectively minimizing errors while correcting a significant number of wrong answers. In contrast, methods like 
MP, ChatEval, and AgentVerse, while capable of correcting many wrong answers, also frequently introduce a high number of misstatements by mistakenly altering the initially correct answers. This overly aggressive behavior prevents these methods from delivering stable and consistent improvements. On the other hand, methods such as SoM and EoT exhibit a more conservative approach, effectively limiting the frequency of errors while also reducing their ability to correct mistakes.

These observations not only explain why the MAD methods typically fail to outperform CoT and SC, but also highlight a key trade-off in the MAD design: overly aggressive methods may introduce instability, while conservative methods may struggle to capitalize on opportunities for correction. 
%This trade-off helps explain the observed behavior of MAD frameworks and sheds light on why they fail to consistently outperform single-agent methods.


% \subsection{Discussion} 

% Our experimental findings reveal that, despite the intuitive appeal of MAD frameworks, they are less effective than currently believed---they usually fail to  significantly outperform simple single-agent baselines such as CoT and SC. Specifically, the interaction of multiple agents through natural language exchanges does not inherently lead to more complex or in-depth reasoning that improves the overall performance of LLMs. The key observations from our study are as follows:

% \begin{itemize}
%     \item \textbf{Limited improvement over single-agent baselines:} MAD frameworks generally fail to achieve significant performance gains over single-agent baselines such as CoT and SC. This suggests that the collaborative debate process does not universally enhance reasoning capabilities beyond what a single agent can accomplish.
%     \item \textbf{Hyperparameter configuration is not the primary issue:} Our ablation studies indicate that the negative outcomes are not primarily due to suboptimal hyperparameter settings. Even when MAD frameworks were configured with increased debate rounds or a greater number of agents, consistent improvements over single-agent baselines were not realized. This underscores that factors beyond hyperparameter tuning may be limiting the efficacy of MAD approaches.
%     \item \textbf{Efficiency constraints:} MAD frameworks do not demonstrate significant efficiency in our experiments. As the inference budget increases —- measured by the number of tokens consumed --- MAD methods do not consistently achieve better results. In contrast, traditional methods like SC improve or at least maintain performance with an increased inference budget, highlighting an efficiency gap in MAD frameworks.
%     \item \textbf{Behavioral trade-offs in MAD frameworks:} The performance of MAD frameworks typically manifests in one of two ways: \textit{(i) overly aggressive correction:} MAD methods may correct a substantial number of errors made by single-agent baselines but simultaneously introduce new mistakes on problems that the single agent had previously solved correctly. This aggressive approach undermines the overall reliability of the framework; \textit{(ii) conservative error control:} Alternatively, MAD frameworks might effectively limit the introduction of new errors but fail to sufficiently address the mistakes made by single-agent baselines. This conservative stance results in minimal improvements and does not leverage the potential of multi-agent interactions to enhance factual accuracy.
% \end{itemize}




% \subsection{Limitation}
% % 1. limitation
% %     a. hyperparameter
% %     b. prompt design

% In this part, we discuss the limitations of our evaluation design to mitigate concern about whether our evaluation can fairly compare MAD frameworks against other baselines. First, our evaluation of MAD configuration sensitivity is limited to the number of agents, the number of debating rounds, and the different foundation models incorporated. These configurations are paid extra attention during evaluation due to most MAD frameworks provide flexibility in adjusting them. Therefore these configurations are the prior candidates to be changed when the MAD framework is deployed. Due to the heavy expense involved in inferring with LLMs, we limit the scale of our ablation studies to control the total expense. However, in our evaluation, we didn't observe obvious trends in performance concerning more agents or more debating rounds.

% Another limitation in our evaluation is the prompt used to invoke LLM agents. We strictly followed the original prompt configuration of considered MAD frameworks with necessary modifications to make the MAD run in an expected way. While we didn't extensively search for an optimal combination of MAD prompts, we made sure that all key ideas and components proposed in existing MAD frameworks were carefully implemented and observed in the debating procedure, e.g., the confidence level of agent in EoT and the expert-hiring stage in AgentVerse. Therefore, our implementation can be considered a qualitative reproduction of existing MAD frameworks.