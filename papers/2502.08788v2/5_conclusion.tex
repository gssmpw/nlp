%\section{Discussion}

% split discussion into section 3 and 4, add discussion for why MAD-CoT not working, and is CoT test-time computation

% \subsection{We obtain consistent observation with existing research}
% While the empirical results shown above are surprising that most MAD frameworks cannot even perform better than CoT, we want to clarify that our observation is not contradictory with previous outcomes in MAD research.  %We observe the same result in our evaluation. Furthermore, we observed that AgentVerse achieved the highest performance on HumanEval by introducing an extra execution-evaluation stage. This stage enables the solver agent in AgentVerse to view the execution results of the generated program, which slightly violates our experimental setup. Therefore, our empirical results are consistent with former observations. 


% \subsection{Gap between concurrent works on advanced inference methods}
% There is also a range of concurrent works discussing advanced inference techniques. A part of them dive into the hidden logic of how widely-used inference techniques take effect. In the paper \textbf{To CoT or not to CoT}, the authors investigate the impact of CoT-style output on the final accuracy of LLM agents. They showed that CoT-style answers achieve significantly higher accuracy on mathematical reasoning tasks like GSM8K or MATH. Contrastively, CoT-style answers fail to improve the performance on comment sense reasoning, and context-aware QA. However, ``Direct Answering'' (``not to CoT'' baselines in this paper) explicitly prompts LLM to avoid outputting a chain of thought in the response, which differs from the baseline ``standard prompting'' in our evaluation. Another batch of works tries to guide LLM inference in a more fine-grained way, e.g., the Monte-Carlo Tree Search (MCTS) with Process Reward Model (PRM). In the paper \textbf{Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters}, the authors proposed to use advanced searching algorithms like Beam Search or Look-ahead Search together with an external PRM during the inference stage to better utilize the inference computation. Searching algorithms split the complete response into several reasoning steps and search for the optimal next step iteratively under the guidance of PRMs. 


%\section{Potential future directions}
\section{Key Questions for Future MAD Research} \label{sec:future}

Heter-MAD is far from the final answer, even though we have shown that Heter-MAD can consistently improve the performance of existing MAD methods.
MAD research remains in its early stages, with many fundamental questions about the mechanisms driving effective multi-agent collaboration still unexplored:
%In the following, we highlight key questions---rooted in our experimental observations---that, if addressed, could propel the field forward.
%Although the previous part of this paper mostly reflected a negative outcome of current MAD methods, we want to emphasize that this is by no means indicating that MAD is not worth further exploration. Instead, we would argue that the development of MAD methods is still in its early stages, with limited exploration of the deep-level mechanisms behind MAD frameworks. We list several possible future directions in the development of MAD frameworks and provide evidence from our observation.

\myparatight{How to fully leverage model heterogeneity in MAD?} Our empirical evaluation of Heter-MAD demonstrates that incorporating model heterogeneity within MAD frameworks is feasible and promising. By querying different foundation models, Heter-MAD has achieved notable improvements across most benchmarks. Additionally, we observed that SoM achieves the best performance when accessing a more cost-effective model, Llama3.1-70b, indicating that model heterogeneity can enhance performance while reducing computational costs. However, current MAD designs are not optimized for aggregating heterogeneous models effectively, as evidenced by the variable performance gains across different frameworks, as well as that SoM was the most foundational MAD framework without complex mechanisms. This highlights the need for developing more suitable MAD methods that can seamlessly integrate diverse models, thereby maximizing the benefits of model heterogeneity.


\myparatight{How to enhance MAD frameworks with single-agent inference approaches?} While MAD primarily focuses on aggregating multiple agents for collaborative inference, empowering individual agents remains crucial. Powerful single agents can generate more insightful and in-depth reasoning paths, thereby enhancing the collective performance. 
%Notably, current foundational models are trained to inherently reason in CoT-style~\citep{llama3, liu2024deepseek_v3,guo2025deepseek_r1}, which can be leveraged to improve MAD frameworks. 
%While some existing MAD frameworks (e.g., EoT, Multi-Persona, and AgentVerse) have incorporated CoT-style answers in their designs, some MAD frameworks (e.g., SoM and ChatEval) do not explicitly prompt agents to do so. Consequently, we evaluate SoM and ChatEval in combination with CoT in Appendix~\ref{appendix:cot}. 
Correspondingly, we evaluate SoM and ChatEval, which do not explicitly incorporate CoT-style responses\footnote{EoT, MP, and AgentVerse have incorporated CoT-style answers in their designs.}, in combination with CoT in Appendix~\ref{appendix:cot}. We have two key findings: (i) CoT consistently improves MAD and Heter-MAD and (ii) Heter-MAD and MAD-CoT improve MAD in distinct directions. These results suggest that it is valuable to explore integrating more powerful single-agent inference approaches~\citep{yao2022react, shinn2024reflexion}, or advanced models with inherently strong reasoning capabilities~\citep{guo2025deepseek_r1, liu2024deepseek_v3, o1}, to further optimize collaborative inference outcomes.
%Our findings indicate consistent improvements when combining MAD and Heter-MAD with CoT. 


\myparatight{How to implement fine-grained interaction mechanisms?} Current MAD methods lack fine-grained interaction capabilities, as agents engage in debates based solely on their complete responses to a given query. This approach leads agents to emphasize the final answer, neglecting underlying reasoning steps. When responses diverge, rebuttals focus on outcome differences rather than analyzing the logic behind discrepancies. Future frameworks should prioritize agents that scrutinize reasoning processes, enabling debates targeting logical gaps to improve overall reasoning quality. A case study illustrating how agents debate when their answers differ is provided in Appendix~\ref{appendix:case}.

% \myparatight{Optimizing model selection and training for MAD}

% Directly deploying pre-trained LLMs within MAD frameworks may be sub-optimal, as these models inherently possess biases towards certain types of answers. Such biases can impede debaters from accurately selecting the most correct answers based solely on their correctness. Therefore, it may be beneficial to employ in-context learning or supervised fine-tuning to train models specifically for effective debating within MAD frameworks. Recent research has proposed various methods to adjust agents for improved collaboration, suggesting that tailored training approaches can enhance the performance and reliability of MAD systems~\citep{subramaniam2025multiagent}.


\myparatight{What kind of application scenarios better reflect the utility of MAD?} Most existing benchmarks predominantly include test cases that require only a single knowledge point for resolution, suggesting that more advanced single-agent methods could suffice and that MAD frameworks may be unnecessary in these contexts. %Under such conditions, MAD degenerates into a computationally expensive resampling technique that aggregates responses through redundant natural language debates. 
To illustrate this limitation, we provide a case study in Appendix \ref{appendix:case}, demonstrating how simplistic benchmarks do not reflect the true potential of MAD in facilitating intricate reasoning processes. Consequently, it is essential to find scenarios that can better reflect the utility of MAD, e.g., scenarios naturally requiring diverse knowledge or capability from multiple agents.

\section{Conclusions}
This work critically examines the current landscape of MAD research, challenging widely held assumptions about its efficacy. Through systematic evaluation, we demonstrate that existing MAD frameworks fail to reliably outperform simple single-agent baselines like CoT and SC, despite consuming additional computational resources. This finding underscores the urgent need to reevaluate common practices in MAD research, particularly the reliance on narrow benchmarks and inconsistent baselines, which risk overestimating the benefits of collaborative inference.

Moreover, we put forward multiple call-to-actions and outline multiple potential directions to improve MAD, ranging from more robust evaluations to promoting knowledge and reasoning diversity through model heterogeneity. We intentionally refrain from delving into technical solutions as we firmly believe that each direction represents a fruitful avenue for future research, and our goal is to spark a broad conversation and inspire the community to collaboratively explore these avenues further. After all, if the age-old saying, `two heads are better than one,' holds true, then collaboration---whether among human researchers or LLM agents---has the potential to make transformative advancements. We invite the community to embrace this wisdom and continue exploring how it can shape the future of LLMs.

% \section*{Impact Statement}
% This paper presents a critical evaluation of MAD frameworks. Through a systematic evaluation across multiple benchmarks and models, our findings challenge the widely held assumption that MAD methods consistently outperform simpler single-agent approaches. We highlight concerns regarding the reproducibility, efficiency, and generalizability of MAD research, emphasizing the need for more rigorous evaluation.

% The broader impact of this work is twofold. First, by identifying key shortcomings in current MAD research practices, we contribute to the ongoing discourse on improving empirical evaluation in MAD studies. Second, our proposal of incorporating model heterogeneity into MAD presents a promising avenue for future advancements in MAD. While our study does not introduce immediate ethical risks, it underscores the importance of responsible benchmarking. We hope that our work will guide researchers in refining MAD methodologies and inspire further investigation into optimizing LLM collaborations.


%As a position paper, this work takes a critical view of the status quo of MAD research and contends that current evaluation practices are problematic. By demonstrating representative MAD methods generally fail to outperform the simple single-agent baselines CoT and SC, we emphasize the urgent need for a more comprehensive evaluation. At a minimum, newly proposed methods should be compared against single-agent baselines across multiple widely adopted benchmarks that encompass diverse LLM capabilities. Moreover, we put forward multiple call-to-actions and outline multiple potential directions to improve MAD, ranging from more robust evaluations to promoting knowledge and reasoning diversity through model heterogeneity. We intentionally refrain from delving into technical solutions as we firmly believe that each direction represents a fruitful avenue for future research, and our goal is to spark a broad conversation and inspire the community to collaboratively explore these avenues further.  After all, if the age-old saying, `two heads are better than one,' holds true, then collaboration---whether among human researchers or LLM agents---has the potential to make transformative advancements. We invite the community to embrace this wisdom and continue exploring how it can shape the future of LLMs.




% \section{Conclusion}
% In this paper, we review representative MAD frameworks, with a specific focus on the comparison between MAD frameworks and single-agent baselines. With comprehensive evaluation, we conclude that current MAD frameworks do not consistently outperform baseline methods including Chain-of-Thought and Self-Consistency. Our ablation study further substantiates that this finding holds across varied experimental configurations. Nonetheless, we believe that MAD frameworks still hold significant potential, particularly as combining heterogeneous foundation models has demonstrated promising performance improvements. Based on our evaluations and observations, we provide insights and evidence for several potential future directions in enhancing current MAD designs.


% To summarize, as a position paper, this work takes a critical view of the status quo of MAD research and contends that current evaluation practices are problematic. By demonstrating representative MAD methods generally fail to outperform the simple single-agent baselines COT and SC, we emphasize the urgent need for more comprehensive evaluation. At a minimum, newly proposed methods should be compared against single-agent baselines across multiple widely adopted benchmarks that encompass diverse LLM capabilities.
% Moreover, we put forward multiple call-to-actions and outline multiple potential directions to improve MAD, ranging from more robust evaluations to adjusting LLMs to make them suited for MAD, and promoting knowledge and reasoning diversity through model heterogeneity. We intentionally refrain from delving into technical solutions. 
% This is because we firmly believe that each direction represents a fruitful avenue for future research, and our goal is to spark a broad conversation and inspire the community to collaboratively explore these avenues further. 
% After all, if the age-old saying, `two heads are better than one,' holds true, then collaboration---whether among human researchers or LLM agents---has the potential to make transformative advancements. We invite the community to embrace this wisdom and continue exploring how it can shape the future of LLMs.


% 1. limitation
%     a. hyperparameter
%     b. prompt design
% 2. why current MAD fails
% 3. no obviously contradictory results with previous works
% 4. comparison with concurrent works (to cot or not to cot)
% 5. when MAD works
% 6. potential future directions

