
% \begin{table*}[htbp]
% \caption{Comparing Heter-MAD against baselines. Heter-MAD queries GPT-4o-mini or Llama3.1-70b with the same ratio, while Heter-MAD-Oracle represents the optimal performance achieved by querying these two models with a specific ratio. CoT-Oracle represents the best performance achieved by these two models with CoT reasoning. We use  \colorbox{green!25}{light green}
% to denote the highest performance achieved by a single MAD framework, and \colorbox{green!75}{green} the highest one overall. We record positive performance gain in \textcolor{red}{red} by comparing Heter-MAD and Heter-MAD-Oracle to the average performance of MAD using two models, as well as CoT-Oracle.}
% \label{tab:mixed}
% \resizebox{\textwidth}{!}{
% \begin{tabular}{l|lllllllll|l}
% \toprule
% MAD                     & MMLU & MMLU-Pro & CommensenseQA & ARC-Challenge & AGIEval   & GSM8K & MATH & HumanEval & MBPP  & Average Performance Gain \\ \midrule
% CoT-Oracle              & 83.2 $\pm$ 0.9 & 62.8 $\pm$ 1.0 & 82.9 $\pm$ 0.3 & \cellcolor{green!75}93.5 $\pm$ 0.4 & 66.4 $\pm$ 1.3 & 93.6 $\pm$ 0.8 & 72.9 $\pm$ 1.2 & 78.1 $\pm$ 1.5 & \cellcolor{green!75}62.3 $\pm$ 0.8  & 77.3 $\pm$ 0.3    \\ \hline
% SoM-GPT                 &  74.7 $\pm$ 0.5 &  62.8 $\pm$ 1.0 &  80.7 $\pm$ 0.9 &  90.8 $\pm$ 0.4 &  64.3 $\pm$ 0.3 &  94.9 $\pm$ 0.3 & \cellcolor{green!25}75.4 $\pm$ 0.7 &  68.1 $\pm$ 1.3 &  \cellcolor{green!25}56.9 $\pm$ 1.1&  74.3 $\pm$ 0.3  \\
% SoM-Llama               &  84.6 $\pm$ 0.4 & 57.1 $\pm$ 1.2 &  81.9 $\pm$ 0.3 &  \cellcolor{green!25}92.9 $\pm$ 0.5 & 62.2 $\pm$ 1.1 &  88.3 $\pm$ 0.7 & 57.3 $\pm$ 0.3 &  63.4 $\pm$ 2.3 &  41.4 $\pm$ 0.5 & 69.9 $\pm$ 0.3 \\
% SoM-Heter               & 83.5 $\pm$ 0.1 & \cellcolor{green!25}65.0 $\pm$ 0.6    & 83.3 $\pm$ 0.1        & 92.1 $\pm$ 0.5          & \cellcolor{green!25}70.1 $\pm$ 0.4     & 94.6 $\pm$ 0.2  & 71.1 $\pm$ 0.9 & 75.8 $\pm$ 3.3      & 54.7  $\pm$ 1.9  & 76.7 $\pm$ 0.4  \\
% \textit{- vs SoM-Average} &\textcolor{red}{4.8\%} & \textcolor{red}{8.4\%} & \textcolor{red}{2.5\%} & \textcolor{red}{0.3\%} & \textcolor{red}{10.8\%} & \textcolor{red}{3.3\%} & \textcolor{red}{7.2\%} & \textcolor{red}{15.3\%} & \textcolor{red}{11.3\%} & \textcolor{red}{6.4\%} \\
% \textit{- vs CoT-Oracle} &\textcolor{red}{0.4\%} & \textcolor{red}{3.5\%} & \textcolor{red}{0.5\%} & -1.5\% & \textcolor{red}{5.6\%} & \textcolor{red}{1.1\%} & -2.5\% & -2.9\% & -12.2\% & -0.8\%\\
% SoM-Heter-Oracle        & \cellcolor{green!25}85.1 $\pm$ 0.7 & \cellcolor{green!25}65.0 $\pm$ 0.6    & \cellcolor{green!25}83.6 $\pm$ 1.0          & \cellcolor{green!25}92.9 $\pm$ 0.2         & \cellcolor{green!25}70.1 $\pm$ 0.4      & \cellcolor{green!75}95.7 $\pm$ 0.2  & 72.9 $\pm$ 0.7 & \cellcolor{green!25}77.0 $\pm$ 1.7      & 56.4 $\pm$ 1.4  & \cellcolor{green!25}77.6 $\pm$ 0.3   \\
%  \textit{- vs SoM-Average}&\textcolor{red}{6.8\%} & \textcolor{red}{8.4\%} & \textcolor{red}{2.8\%} & \textcolor{red}{1.1\%} & \textcolor{red}{10.8\%} & \textcolor{red}{4.5\%} & \textcolor{red}{9.9\%} & \textcolor{red}{17.1\%} & \textcolor{red}{14.8\%} & \textcolor{red}{7.6\%} \\
% \textit{- vs CoT-Oracle} &\textcolor{red}{2.3\%} & \textcolor{red}{3.5\%} & \textcolor{red}{0.8\%} & -0.6\% & \textcolor{red}{5.6\%} & \textcolor{red}{2.2\%} & \textcolor{red}{0.0\%} & -1.4\% & -9.5\% & \textcolor{red}{0.4\%} \\
% \hline
% EoT-GPT                 & 67.9 $\pm$ 0.4 &  61.2 $\pm$ 0.6 &  80.1 $\pm$ 0.5 &  86.4 $\pm$ 0.3 &  65.1 $\pm$ 0.7 &  \cellcolor{green!25}94.4 $\pm$ 0.6 &  \cellcolor{green!75}75.9 $\pm$ 1.2 &  73.8 $\pm$ 2.2 &  56.2 $\pm$ 0.5 &     73.4 $\pm$ 0.3 \\
% EoT-Llama               & 83.2 $\pm$ 0.3 &  49.7 $\pm$ 0.6 &  81.9 $\pm$ 0.7 &  93.0 $\pm$ 0.1 &  63.1 $\pm$ 0.5 &  77.6 $\pm$ 0.7 &  55.3 $\pm$ 0.3 &  55.5 $\pm$ 0.9 &  38.9 $\pm$ 1.7    & 66.5 $\pm$ 0.3        \\
% EoT-Heter              &  79.7  $\pm$ 4.4   &    63.6 $\pm$ 3.1     &    83.9 $\pm$ 0.5           &  92.7 $\pm$ 0.5             &     \cellcolor{green!25}69.8 $\pm$ 0.5      & 93.2 $\pm$ 0.3      &   73.1 $\pm$ 0.6   &    70.9 $\pm$ 1.1      &    54.3 $\pm$ 0.4   &       75.7 $\pm$ 0.6 \\
% \textit{- vs EoT-Average} &\textcolor{red}{5.5\%} & \textcolor{red}{14.7\%} & \textcolor{red}{3.6\%} & \textcolor{red}{3.3\%} & \textcolor{red}{8.9\%} & \textcolor{red}{8.4\%} & \textcolor{red}{11.4\%} & \textcolor{red}{9.7\%} & \textcolor{red}{14.2\%} & \textcolor{red}{8.2\%} \\
% \textit{- vs CoT-Oracle} &-4.2\% & \textcolor{red}{1.3\%} & \textcolor{red}{1.2\%} & -0.9\% & \textcolor{red}{5.1\%} & -0.4\% & \textcolor{red}{0.3\%} & -9.2\% & -12.8\% & -2.1\% \\
% EoT-Heter-Oracle        &    \cellcolor{green!25}85.1 $\pm$ 0.3  & \cellcolor{green!75}66.6 $\pm$ 0.4        &      \cellcolor{green!75}84.2 $\pm$ 0.2         &  \cellcolor{green!25}93.1 $\pm$ 0.5             & \cellcolor{green!25}69.8 $\pm$ 0.5          &   94.3 $\pm$ 0.4    &  73.1 $\pm$ 0.6    &   \cellcolor{green!25}77.2 $\pm$ 1.1        &   \cellcolor{green!25}61.1 $\pm$ 0.7    &     \cellcolor{green!75}78.3 $\pm$ 0.2  \\ 
% \textit{- vs EoT-Average} &\textcolor{red}{12.6\%} & \textcolor{red}{20.1\%} & \textcolor{red}{4.0\%} & \textcolor{red}{3.8\%} & \textcolor{red}{8.9\%} & \textcolor{red}{9.7\%} & \textcolor{red}{11.4\%} & \textcolor{red}{19.4\%} & \textcolor{red}{28.5\%} & \textcolor{red}{11.9\%} \\
% \textit{- vs CoT-Oracle} &\textcolor{red}{2.3\%} & \textcolor{red}{6.1\%} & \textcolor{red}{1.6\%} & -0.4\% & \textcolor{red}{5.1\%} & \textcolor{red}{0.7\%} & \textcolor{red}{0.3\%} & -1.2\% & -1.9\% & \textcolor{red}{1.3\%} \\ \hline
% ChatEval-GPT            &  79.1 $\pm$ 0.9 &  62.2 $\pm$ 0.5 &  81.1 $\pm$ 0.8 &  \cellcolor{green!25}93.2 $\pm$ 0.3 &  68.9 $\pm$ 0.9 & 93.6 $\pm$ 0.0 &  69.4 $\pm$ 1.6 &  \cellcolor{green!25}71.8 $\pm$ 0.8 &  \cellcolor{green!25}53.7 $\pm$ 0.6 & 74.8 $\pm$ 0.3 \\
% ChatEval-Llama          &  80.4 $\pm$ 1.2 &  56.1 $\pm$ 1.0 &  72.8 $\pm$ 1.3 &  89.9 $\pm$ 0.2 &  68.6 $\pm$ 0.4 &  92.5 $\pm$ 0.2 &  58.7 $\pm$ 1.5 & 62.8 $\pm$ 0.9 &  44.5 $\pm$ 2.2  & 69.6 $\pm$ 0.4 \\
% ChatEval-Heter          & 82.6 $\pm$ 0.5 & 64.9 $\pm$ 0.4    & 78.8 $\pm$ 1.4          & 92.3 $\pm$ 0.5          & 70.5 $\pm$ 0.7      & 94.6 $\pm$ 0.2  & 71.4 $\pm$ 0.7 & 70.9 $\pm$ 0.8      & 49.8 $\pm$ 2.2  & 75.1 $\pm$ 0.3 \\
% \textit{- vs CE-Oracle} &\textcolor{red}{3.6\%} & \textcolor{red}{9.7\%} & \textcolor{red}{2.4\%} & \textcolor{red}{0.8\%} & \textcolor{red}{2.5\%} & \textcolor{red}{1.7\%} & \textcolor{red}{11.5\%} & \textcolor{red}{5.3\%} & \textcolor{red}{1.4\%} & \textcolor{red}{4.0\%} \\
% \textit{- vs CoT-Oracle} &-0.7\% & \textcolor{red}{3.3\%} & -4.9\% & -1.3\% & \textcolor{red}{6.2\%} & \textcolor{red}{1.1\%} & -2.1\% & -9.2\% & -20.1\% & -2.8\% \\
% ChatEval-Heter-Oracle   & \cellcolor{green!25}83.5 $\pm$ 0.6 & \cellcolor{green!25}66.2 $\pm$ 0.7    & \cellcolor{green!25}82.4 $\pm$ 0.5          & 92.5 $\pm$ 0.2          & \cellcolor{green!75}71.3 $\pm$ 0.2      & \cellcolor{green!25}94.7 $\pm$ 0.1  & \cellcolor{green!25}72.5 $\pm$ 0.4 &71.7 $\pm$ 0.8      & 51.8 $\pm$ 0.8  & \cellcolor{green!25}76.3 $\pm$ 0.2   \\ 
% \textit{- vs CE-Oracle} &\textcolor{red}{4.7\%} & \textcolor{red}{11.9\%} & \textcolor{red}{7.1\%} & \textcolor{red}{1.0\%} & \textcolor{red}{3.7\%} & \textcolor{red}{1.8\%} & \textcolor{red}{13.2\%} & \textcolor{red}{6.5\%} & \textcolor{red}{5.5\%} & \textcolor{red}{5.7\%} \\
% \textit{- vs CoT-Oracle} &\textcolor{red}{0.4\%} & \textcolor{red}{5.4\%} & -0.6\% & -1.1\% & \textcolor{red}{7.4\%} & \textcolor{red}{1.2\%} & -0.5\% & -8.2\% & -16.9\% & -1.3\% \\
% \hline
% AgentVerse-GPT          &  80.4 $\pm$ 0.0 &  62.1 $\pm$ 0.5 &  \cellcolor{green!25}80.7 $\pm$ 0.4 &  92.5 $\pm$ 0.1 &  63.9 $\pm$ 1.2 &  \cellcolor{green!25}92.7 $\pm$ 0.5 &  \cellcolor{green!25}64.5 $\pm$ 1.4 &  \cellcolor{green!75}85.4 $\pm$ 0.0 &  \cellcolor{green!25}58.9 $\pm$ 0.2 & \cellcolor{green!25}75.7 $\pm$ 0.2 \\
% AgentVerse-Llama        &84.8 $\pm$ 1.0 &  61.8 $\pm$ 0.9 &  76.5 $\pm$ 1.2 &  92.8 $\pm$ 0.3 &  66.7 $\pm$ 0.8 &  85.5 $\pm$ 0.7 &  45.3 $\pm$ 0.9 &  60.0 $\pm$ 0.8 &  41.9 $\pm$ 1.0& 68.4 $\pm$ 0.3   \\
% AgentVerse-Heter        & 84.3 $\pm$ 1.0 & 63.0 $\pm$ 0.4    & 79.3 $\pm$ 0.8          & 92.6 $\pm$ 0.6          & 66.7 $\pm$ 1.0      & 90.7 $\pm$ 0.8  & 58.1 $\pm$ 0.2 & 78.5 $\pm$ 0.2      & 53.0 $\pm$ 0.2 & 74.0 $\pm$ 0.2  \\
% \textit{- vs AGV-Oracle} &\textcolor{red}{2.1\%} & \textcolor{red}{1.7\%} & \textcolor{red}{0.9\%} & -0.1\% & \textcolor{red}{2.1\%} & \textcolor{red}{1.8\%} & \textcolor{red}{5.8\%} & \textcolor{red}{8.0\%} & \textcolor{red}{5.2\%} & \textcolor{red}{2.7\%}\\
% \textit{- vs CoT-Oracle} &\textcolor{red}{1.3\%} & \textcolor{red}{0.3\%} & -4.3\% & -1.0\% & \textcolor{red}{0.5\%} & -3.1\% & -20.3\% & \textcolor{red}{0.5\%} & -14.9\% & -4.3\% \\
% AgentVerse-Heter-Oracle & \cellcolor{green!75}85.3 $\pm$ 0.2 & \cellcolor{green!25}65.3 $\pm$ 1.1    & 80.2 $\pm$ 0.6          & \cellcolor{green!25}93.1 $\pm$ 0.6          & \cellcolor{green!25}68.2 $\pm$ 0.5      & 92.3 $\pm$ 0.3  & 59.8 $\pm$ 1.3 & 80.1 $\pm$ 0.8      & 54.2 $\pm$ 1.1  & 75.4 $\pm$ 0.3  \\ 
% \textit{- vs AGV-Oracle} &\textcolor{red}{3.3\%} & \textcolor{red}{5.4\%} & \textcolor{red}{2.0\%} & \textcolor{red}{0.5\%} & \textcolor{red}{4.4\%} & \textcolor{red}{3.6\%} & \textcolor{red}{8.9\%} & \textcolor{red}{10.2\%} & \textcolor{red}{7.5\%} & \textcolor{red}{4.6\%} \\
% \textit{- vs CoT-Oracle} &\textcolor{red}{2.5\%} & \textcolor{red}{4.0\%} & -3.3\% & -0.4\% & \textcolor{red}{2.7\%} & -1.4\% & -18.0\% & \textcolor{red}{2.6\%} & -13.0\% & -2.5\% \\
% \bottomrule


% \begin{table*}[htbp]
% \caption{Comparing Heter-MAD against baselines. Heter-MAD queries GPT-4o-mini or Llama3.1-70b with the same ratio, while Heter-MAD-Oracle represents the optimal performance achieved by querying these two models with a specific ratio. CoT-Oracle represents the best performance achieved by these two models with CoT reasoning. We use  \colorbox{green!25}{light green}
% to denote the highest performance achieved by a single MAD framework, and \colorbox{green!75}{green} the highest one overall.}
% \label{tab:mixed}
% \resizebox{\textwidth}{!}{
% \begin{tabular}{l|lllllllll|l}
% \hline
% MAD                     & MMLU & MMLU-Pro & CommensenseQA & ARC-Challenge & AGIEval   & GSM8K & MATH & HumanEval & MBPP  & Average \\ \hline
% CoT-Oracle              & 82.7 & 62.8    & 82.9          & 93.5          & 66.4      & 93.6  & 72.9 & 78.1      & \cellcolor{green!75}62.3  & 77.2    \\ \hline
% SoM-GPT                 & 75.2 & 64.2    & 80.4          & 91.0          & 64.2      & 94.8  & \cellcolor{green!75}75.8 & 68.3      & 56.8  & 74.5    \\
% SoM-Llama               & 84.2 & 55.8    & 81.8          & 93.2          & 62.0      & 84.0  & 47.2 & 69.5      & 41.2  & 68.8    \\
% SoM-Heter               & 83.5 $\pm$ 0.1 & \cellcolor{green!25}65.0 $\pm$ 0.6    & \cellcolor{green!75}83.3 $\pm$ 0.1        & 92.8          & \cellcolor{green!25}69.8      & 94.4  & 69.8 & \cellcolor{green!25}79.3      & 57.2  & 77.4    \\
% SoM-Heter-Oracle        & \cellcolor{green!25}86.1 & \cellcolor{green!25}65.7    & \cellcolor{green!75}84.4          & \cellcolor{green!25}93.4          & \cellcolor{green!25}69.8      & \cellcolor{green!75}95.8  & 73.0 & \cellcolor{green!25}79.3      & \cellcolor{green!25}57.6  & \cellcolor{green!75}78.2    \\ \hline
% EoT-GPT                 & 68.4 &  62.0       &   81.0            &   89.6            &   67.2        &  91.8     &  \cellcolor{green!25}74.4    &   75.0        &  60.3    &     74.4   \\
% EoT-Llama               & 83.0 &   50.5      &    83.6          &    \cellcolor{green!25}93.0           &   \cellcolor{green!25}69.1        & 76.8      &  43.8    &    54.3       &   49.0    &66.5         \\
% EoT-Heter              &  74.0    &    59.4     &    \cellcolor{green!25}84.0           &  92.0             &     \cellcolor{green!25}69.1      & 93.0      &   72.4   &    72.6      &    54.9   &       74.6  \\
% EoT-Heter-Oracle        &    \cellcolor{green!25}85.6  & \cellcolor{green!75}67.0        &      \cellcolor{green!25}84.0         &  92.8             & \cellcolor{green!25}69.1          &   \cellcolor{green!25}94.6    &  72.4    &   \cellcolor{green!25}75.6        &   \cellcolor{green!25}61.1    &     \cellcolor{green!25}77.7    \\ \hline
% ChatEval-GPT            & 80.2 & 61.6    & 80.8          & 93.0          & 68.2      & 93.6  & 71.6 & 70.7      & \cellcolor{green!25}53.3  & 74.8    \\
% ChatEval-Llama          & 79.5 & 56.4    & 72.5          & 89.8          & 68.6      & 84.0  & 56.6 & 67.7      & 44.7  & 68.9    \\
% ChatEval-Heter          & 82.7 & 65.4    & 80.7          & 92.7          & 70.0      & \cellcolor{green!25}94.8  & 72.0 & 70.1      & 52.5  & 75.7    \\
% ChatEval-Heter-Oracle   & \cellcolor{green!25}84.3 & \cellcolor{green!25}66.0    & \cellcolor{green!25}82.9          & \cellcolor{green!75}93.6          & \cellcolor{green!75}72.4      & \cellcolor{green!25}94.8  & \cellcolor{green!25}73.6 & \cellcolor{green!25}75.6      & 52.5  & \cellcolor{green!25}77.2    \\ \hline
% AgentVerse-GPT          & 80.4 & 61.8    & \cellcolor{green!25}80.8          & 92.4          & 63.0 & \cellcolor{green!25}93.4  & \cellcolor{green!25}66.0 & \cellcolor{green!75}85.4      & \cellcolor{green!25}58.8  & 75.8    \\
% AgentVerse-Llama        & 84.4 & 62.0    & 75.4          & 93.0          & 67.0      & 85.2  & 46.0 & 59.1      & 42.4  & 68.3    \\
% AgentVerse-Heter        & 85.6 & 62.2    & 78.4          & 92.0          & 65.4      & 91.0  & 58.0 & 81.1      & 53.3 & 74.2    \\
% AgentVerse-Heter-Oracle & \cellcolor{green!75}87.0 & \cellcolor{green!25}66.6    & 80.0          & \cellcolor{green!25}93.4          & \cellcolor{green!25}69.0      & 92.4  & 61.2 & 81.1      & 55.6  &     \\ \hline

% % \hline
% % Performance gain - GPT & -2.5 & 1.4    & -1.9         & -0.5          & 0.8      & 1.2  & \cellcolor{green!75}2.9 & \cellcolor{green!75}7.3     & -2  & 0.74    \\ 
% % Performance gain - Llama & 1.7 & -0.8    & 0.7          & -0.5          & 2.7      & -8.4  & -19.2 & -8.6      & -13.3  & -5.1   \\ 
% % Performance gain - Mixed & \cellcolor{green!75}4.3 & \cellcolor{green!75}4.2   & \cellcolor{green!75}1.5        & \cellcolor{green!75}0.1       & \cellcolor{green!75}6.0   & \cellcolor{green!75}2.2  & 0.7 & 3.0      & \cellcolor{green!75}-1.2 & \cellcolor{green!75}2.3  \\ 

% \hline
% \end{tabular}
% }
% \end{table*}

\section{Improving the Status Quo: Model Heterogeneity as an Antidote} %Heter-MAD: from homogeneity to heterogeneity} \label{sec:exp_mixed}
In this section, we investigate the potential of model heterogeneity in MAD, an aspect that has been largely underexplored in the literature. 
Note that we do not aim to propose a MAD design optimized for leveraging model heterogeneity, as this is beyond the scope of this paper. We leave this to future work---an avenue we believe holds significant promise and warrants further exploration.
\subsection{Heterogeneous MAD}
Intuitively, models trained on different data and paradigms may exhibit distinct strengths and weaknesses. Building on this idea, we posit that MAD designs leveraging model diversity can effectively compensate for individual model limitations while amplifying their strengths, ultimately leading to overall performance improvements.

To validate this hypothesis, we introduce \textbf{Heter-MAD}, a simple and general method that can be integrated into \emph{any} existing MAD framework. 
Heter-MAD differs from existing MAD methods with only one key difference---every time that an agent generates an output, the agent queries a foundation model $i$ (where $i\in \{1,..., n\}$) with probability $p_i$ (such that $\sum_{i=1}^n p_i =1$) from a pool of candidate models. 
%Here, for simplicity, we consider two candidate models where $n=2$.
%Formally, for a candidate model pool including two foundation models, the probability that Mixex-MAD picks a certain model with index $I \sim Bern(p)$, with a default configuration $p=0.5$.
Therefore, Heter-MAD effectively reuses the prompts and architecture of any MAD method without requiring deliberate adjustments to incorporate different foundation models. 
This makes it well-suited for evaluating whether model heterogeneity can enhance MAD.
%Through communication between agents, diverse knowledge is integrated during the debating process, potentially leading to more robust reasoning outcomes. Therefore, Heter-MAD effectively introduces model heterogeneity into MAD frameworks through natural language communication without significantly accommodating existing designs.

%In the following part, we evaluate Heter-MAD against existing baselines to explore the significance of model heterogeneity. This evaluation aims to determine whether incorporating diverse foundation models can address the limitations observed in homogeneous MAD frameworks and lead to consistent performance improvements across various benchmarks.

\subsection{Experimental Results}
\begin{table*}[tb!]
\caption{Performance results of Heter-MAD. CoT-Average represents the average performance achieved by these two models with CoT reasoning. We use  \colorbox{green!25}{light green}
to denote the highest performance achieved by a single MAD framework, and \colorbox{green!75}{green} the highest one overall. We record positive performance gain in \textcolor{red}{red} by comparing Heter-MAD to the average performance of MAD using two models, as well as CoT-Average.}
\label{tab:mixed}

\resizebox{\textwidth}{!}{
\begin{tabular}{l|lllllllll|l}
\toprule
\textbf{Dataset}                    & \textbf{MMLU} & \textbf{MMLU-Pro} & \textbf{CommensenseQA} & \textbf{ARC-Challenge} & \textbf{AGIEval}   & \textbf{GSM8K} & \textbf{MATH} & \textbf{HumanEval} & \textbf{MBPP}  & \textbf{Average} \\ \midrule
CoT-Average              & 81.7$\pm$1.3&58.3$\pm$1.3&82.6$\pm$1.5&\cellcolor{green!75}93.4$\pm$0.6&62.4$\pm$2.0&92.8$\pm$1.2&55.0$\pm$1.5&70.3$\pm$1.8&55.8$\pm$2.7&72.5$\pm$4.9   \\ \hline
SoM-GPT                 &  74.7 $\pm$ 0.5 &  62.8 $\pm$ 1.0 &  80.7 $\pm$ 0.9 &  90.8 $\pm$ 0.4 &  64.3 $\pm$ 0.3 &  \cellcolor{green!75}94.9 $\pm$ 0.3 & \cellcolor{green!25}75.4 $\pm$ 0.7 &  68.1 $\pm$ 1.3 &  \cellcolor{green!25}56.9 $\pm$ 1.1&  74.3 $\pm$ 0.3  \\
SoM-Llama               &  \cellcolor{green!25}84.6 $\pm$ 0.4 & 57.1 $\pm$ 1.2 &  81.9 $\pm$ 0.3 &  \cellcolor{green!25}92.9 $\pm$ 0.5 & 62.2 $\pm$ 1.1 &  88.3 $\pm$ 0.7 & 57.3 $\pm$ 0.3 &  63.4 $\pm$ 2.3 &  41.4 $\pm$ 0.5 & 69.9 $\pm$ 0.3 \\
SoM-Heter               & 83.5 $\pm$ 0.1 & \cellcolor{green!75}65.0 $\pm$ 0.6    & \cellcolor{green!25}83.3 $\pm$ 0.1        & 92.1 $\pm$ 0.5          & \cellcolor{green!25}70.1 $\pm$ 0.4     & 94.6 $\pm$ 0.2  & 71.1 $\pm$ 0.9 & \cellcolor{green!25}75.8 $\pm$ 3.3      & 54.7  $\pm$ 1.9  & \cellcolor{green!75}76.7 $\pm$ 0.4  \\
\textit{- vs SoM-Average} &\textcolor{red}{4.8\%} & \textcolor{red}{8.4\%} & \textcolor{red}{2.5\%} & \textcolor{red}{0.3\%} & \textcolor{red}{10.8\%} & \textcolor{red}{3.3\%} & \textcolor{red}{7.2\%} & \textcolor{red}{15.3\%} & \textcolor{red}{11.3\%} & \textcolor{red}{+6.4\%} \\
\textit{- vs CoT-Average} &\textcolor{red}{2.2\%}&\textcolor{red}{11.4\%}&\textcolor{red}{0.8\%}&-1.4\%&\textcolor{red}{12.3\%}&\textcolor{red}{1.9\%}&\textcolor{red}{29.3\%}&\textcolor{red}{7.8\%}&-2.0\%&\textcolor{red}{+5.8\%}\\
\hline
EoT-GPT                 & 67.9 $\pm$ 0.4 &  61.2 $\pm$ 0.6 &  80.1 $\pm$ 0.5 &  86.4 $\pm$ 0.3 &  65.1 $\pm$ 0.7 &  \cellcolor{green!25}94.4 $\pm$ 0.6 &  \cellcolor{green!75}75.9 $\pm$ 1.2 &  \cellcolor{green!25}73.8 $\pm$ 2.2 &  \cellcolor{green!25}56.2 $\pm$ 0.5 &     73.4 $\pm$ 0.3 \\
EoT-Llama               & \cellcolor{green!25}83.2 $\pm$ 0.3 &  49.7 $\pm$ 0.6 &  81.9 $\pm$ 0.7 &  \cellcolor{green!25}93.0 $\pm$ 0.1 &  63.1 $\pm$ 0.5 &  77.6 $\pm$ 0.7 &  55.3 $\pm$ 0.3 &  55.5 $\pm$ 0.9 &  38.9 $\pm$ 1.7    & 66.5 $\pm$ 0.3        \\
EoT-Heter              &  79.7  $\pm$ 4.4   &    \cellcolor{green!25}63.6 $\pm$ 3.1     &    \cellcolor{green!75}83.9 $\pm$ 0.5           &  92.7 $\pm$ 0.5             &     \cellcolor{green!25}69.8 $\pm$ 0.5      & 93.2 $\pm$ 0.3      &   73.1 $\pm$ 0.6   &    70.9 $\pm$ 1.1      &    54.3 $\pm$ 0.4   &       \cellcolor{green!25}75.7 $\pm$ 0.6 \\
\textit{- vs EoT-Average} &\textcolor{red}{5.5\%} & \textcolor{red}{14.7\%} & \textcolor{red}{3.6\%} & \textcolor{red}{3.3\%} & \textcolor{red}{8.9\%} & \textcolor{red}{8.4\%} & \textcolor{red}{11.4\%} & \textcolor{red}{9.7\%} & \textcolor{red}{14.2\%} & \textcolor{red}{+8.2\%} \\
\textit{- vs CoT-Average} &-2.5\%&\textcolor{red}{9.0\%}&\textcolor{red}{1.5\%}&-0.8\%&\textcolor{red}{11.8\%}&\textcolor{red}{0.4\%}&\textcolor{red}{32.9\%}&\textcolor{red}{0.8\%}&-2.8\%&\textcolor{red}{+4.4\%}\\
\hline
ChatEval-GPT            &  79.1 $\pm$ 0.9 &  62.2 $\pm$ 0.5 &  \cellcolor{green!25}81.1 $\pm$ 0.8 &  \cellcolor{green!25}93.2 $\pm$ 0.3 &  68.9 $\pm$ 0.9 & 93.6 $\pm$ 0.0 &  69.4 $\pm$ 1.6 &  \cellcolor{green!25}71.8 $\pm$ 0.8 &  \cellcolor{green!25}53.7 $\pm$ 0.6 & 74.8 $\pm$ 0.3 \\
ChatEval-Llama          &  80.4 $\pm$ 1.2 &  56.1 $\pm$ 1.0 &  72.8 $\pm$ 1.3 &  89.9 $\pm$ 0.2 &  68.6 $\pm$ 0.4 &  92.5 $\pm$ 0.2 &  58.7 $\pm$ 1.5 & 62.8 $\pm$ 0.9 &  44.5 $\pm$ 2.2  & 69.6 $\pm$ 0.4 \\
ChatEval-Heter          & \cellcolor{green!25}82.6 $\pm$ 0.5 & \cellcolor{green!25}64.9 $\pm$ 0.4    & 78.8 $\pm$ 1.4          & 92.3 $\pm$ 0.5          & \cellcolor{green!75}70.5 $\pm$ 0.7      & \cellcolor{green!25}94.6 $\pm$ 0.2  & \cellcolor{green!25}71.4 $\pm$ 0.7 & 70.9 $\pm$ 0.8      & 49.8 $\pm$ 2.2  & \cellcolor{green!25}75.1 $\pm$ 0.3 \\
\textit{- vs CE-Average} &\textcolor{red}{3.6\%} & \textcolor{red}{9.7\%} & \textcolor{red}{2.4\%} & \textcolor{red}{0.8\%} & \textcolor{red}{2.5\%} & \textcolor{red}{1.7\%} & \textcolor{red}{11.5\%} & \textcolor{red}{5.3\%} & \textcolor{red}{1.4\%} & \textcolor{red}{+4.0\%} \\
\textit{- vs CoT-Average} &\textcolor{red}{1.1\%}&\textcolor{red}{11.3\%}&-4.6\%&-1.2\%&\textcolor{red}{13.0\%}&\textcolor{red}{1.9\%}&\textcolor{red}{29.8\%}&\textcolor{red}{0.8\%}&-10.8\%&\textcolor{red}{+3.6\%} \\

\hline
AgentVerse-GPT          &  80.4 $\pm$ 0.0 &  62.1 $\pm$ 0.5 &  \cellcolor{green!25}80.7 $\pm$ 0.4 &  92.5 $\pm$ 0.1 &  63.9 $\pm$ 1.2 &  \cellcolor{green!25}92.7 $\pm$ 0.5 &  \cellcolor{green!25}64.5 $\pm$ 1.4 &  \cellcolor{green!75}85.4 $\pm$ 0.0 &  \cellcolor{green!75}58.9 $\pm$ 0.2 & \cellcolor{green!25}75.7 $\pm$ 0.2 \\
AgentVerse-Llama        &\cellcolor{green!75}84.8 $\pm$ 1.0 &  61.8 $\pm$ 0.9 &  76.5 $\pm$ 1.2 &  \cellcolor{green!25}92.8 $\pm$ 0.3 &  \cellcolor{green!25}66.7 $\pm$ 0.8 &  85.5 $\pm$ 0.7 &  45.3 $\pm$ 0.9 &  60.0 $\pm$ 0.8 &  41.9 $\pm$ 1.0& 68.4 $\pm$ 0.3   \\
AgentVerse-Heter        & 84.3 $\pm$ 1.0 & \cellcolor{green!25}63.0 $\pm$ 0.4    & 79.3 $\pm$ 0.8          & 92.6 $\pm$ 0.6          & \cellcolor{green!25}66.7 $\pm$ 1.0      & 90.7 $\pm$ 0.8  & 58.1 $\pm$ 0.2 & 78.5 $\pm$ 0.2      & 53.0 $\pm$ 0.2 & 74.0 $\pm$ 0.2  \\
\textit{- vs AGV-Average} &\textcolor{red}{2.1\%} & \textcolor{red}{1.7\%} & \textcolor{red}{0.9\%} & -0.1\% & \textcolor{red}{2.1\%} & \textcolor{red}{1.8\%} & \textcolor{red}{5.8\%} & \textcolor{red}{8.0\%} & \textcolor{red}{5.2\%} & \textcolor{red}{+2.7\%}\\
\textit{- vs CoT-Average} &\textcolor{red}{3.1\%}&\textcolor{red}{8.0\%}&-4.0\%&-0.9\%&\textcolor{red}{6.9\%}&-2.3\%&\textcolor{red}{5.6\%}&\textcolor{red}{11.6\%}&-5.1\%&\textcolor{red}{+2.1\%}\\
\bottomrule
\hline
\end{tabular}}

\end{table*}

We validate the effectiveness of Heter-MAD by considering GPT-4o-mini~\citep{gpt4o-mini} and Llama3.1-70b~\citep{llama3} as candidate foundation models, with the probability of selecting each model simply setting to $0.5$. We instantiate Heter-MAD with SoM, EoT, ChatEval, and AgentVerse, while we exclude MP due to two reasons: (i) previous studies indicate that the agent roles in MP are unbalanced, and (ii) its performance is generally weak, achieving $0\%$ win rate compared to CoT.      %compare Heter-MAD to CoT-Average which denotes the average performance achieved by CoT using GPT-4o-mini and Llama3.1-70b.

%Note that we exclude MP since previous experimental results indicate that the agent roles in MP are unbalanced and the overall framework performance is subpar, with Figure~\ref{fig:main_pie} showing that MP is the only MAD framework with a 0\% win rate.
%SoM-GPT and SoM-Llama denote SoM with GPT-4o-mini and Llama3.1-70b as the foundation model, respectively. SoM-Heter-Oracle denotes the highest accuracy achieved by SoM-Heter with varied probability to query a specific model from the candidate models. 

\begin{figure}[tb!]
     \centering
     \includegraphics[width=0.95\textwidth]
     {image/mixed/MAD_9_datasets_3_colorblind.pdf}
     \vspace{-10pt}
        \caption{Heter-MAD performance analysis. We split questions in a benchmark into four parts each denoted as  CC, CW, WC, and WW, where CC represents questions that both GPT-4o-mini and Llama3.1-70b can solve. Similarly, WW represents questions that both models fail to solve, and CW denotes questions that only GPT-4o-mini can solve. In each part, the filled bar denotes how many questions are solved by Heter-MAD, while the hollow bar denotes how many questions are not solved by Heter-MAD.
        %A full version of our experiment containing all benchmarks can be found in Figure \ref{fig:mixed_full} in Appendix \ref{appendix:exp_res}.
        }
        \label{fig:mixed}
\end{figure} 

\myparatight{Heter-MAD consistently improves MAD} We present the performance results in Table \ref{tab:mixed}. Notably, we find that Heter-MAD consistently improves the performance of all considered MAD frameworks.
Specifically, by incorporating model heterogeneity, Heter-SoM improves SoM-average (the average performance achieved by SoM when utilizing the two candidate models separately) by 6.4\%, and 
Heter-EoT improves EoT-average by 8.2\%. 
Moreover, Heter-SoM, with SoM being the most simple and foundational MAD, achieves the highest performance, surpassing all the other, more recently developed MAD methods considered.
Last but not least, by incorporating model heterogeneity, all the considered MAD methods outperform CoT-Average (the average performance achieved by CoT when utilizing the two candidate models separately) by up to 5.8\%.

 

On the other hand, we observe that the performance gains brought by incorporating model heterogeneity on ChatEval and AgentVerse are also less significant compared to SoM or EoT, despite that ChatEval and AgentVerse represent more recent advancements. We hypothesize that the significant performance gain of Heter-SoM and Heter-EoT stems from the rather simple design of SoM and EoT, and thus they are more compatible with heterogeneous models. Conversely, more complex frameworks, like ChatEval and AgentVerse, struggle with compatibility when incorporating model heterogeneity and lack the ability to effectively aggregate the strengths of diverse models.
 This phenomenon highlights substantial opportunities for optimizing the compatibility of MAD frameworks with heterogeneous model ensembles. By enhancing the ability of MAD systems to integrate and leverage the strengths of varied models, future research can achieve more consistent and robust performance improvements, fully harnessing the potential of model heterogeneity in multi-agent collaborations.


%Based on our empirical observations, we identify three key findings:
%\begin{itemize}
%    \item \textbf{Weaker models can still help stronger models:} When integrating GPT-4o-mini with a less powerful and more cost-effective model, Llama3.1-70b, we still observe considerable performance improvements. This outcome highlights that Heter-MAD can attain higher performance levels while incurring lower computational costs compared to current MAD baselines. The ability to leverage model heterogeneity not only enhances reasoning capabilities but also optimizes resource utilization, making Heter-MAD a more efficient and effective approach.

 %   \item \textbf{Simple MAD yields strong Heter-MAD performance:} 
%Our experiments reveal significant variations in performance gains across different Heter-MAD frameworks. Specifically, SoM, which is the most simple and foundational MAD framework, achieves the highest performance when combining two models, surpassing CoT-Average by 5.8\%. We hypothesize that Heter-SoM's superior performance in a heterogeneous setting stems from its simplicity and compatibility with heterogeneous models. 

%    \item \textbf{Complex MAD frameworks struggle with model heterogeneity:} 
%Furthermore, we observe that more complex MAD frameworks, such as ChatEval and AgentVerse, perform worse than simpler ones like SoM and EoT within the Heter-MAD configuration. Specifically, the improvement brought by Heter-MAD on ChatEval and AgentVerse is less significant compared to SoM or EoT. This suggests that existing, deliberately designed complex MAD frameworks may lack the mechanisms to effectively aggregate insights from diverse models. This phenomenon highlights substantial opportunities for optimizing the compatibility of MAD frameworks with heterogeneous model ensembles. By enhancing the ability of MAD systems to integrate and leverage the strengths of varied models, future research can achieve more consistent and robust performance improvements, fully harnessing the potential of model heterogeneity in multi-agent collaborations
%\end{itemize}





%We note that AgentVerse is designed to be more complicated in coding tasks, in which a verification process is introduced. This makes AgentVerse-Llama behave obviously worse than AgentVerse-GPT, given that Llama3.1-70b is not an approximately strong model compared to GPT-4o-mini. 
\myparatight{How Heter-MAD improves MAD?} To elucidate how model heterogeneity contributes to performance improvements, we conducted a detailed analysis of Heter-MAD’s outcomes. As depicted in Figure \ref{fig:mixed}, we categorized questions with their solvability by SoM-GPT and SoM-Llama into four groups: \textbf{CC} (both models correctly solve), \textbf{WW} (both models incorrectly solve), \textbf{CW} (SoM-GPT is correct while SoM-Llama is wrong), and \textbf{WC} (SoM-GPT is wrong while SoM-Llama is correct). Our observations reveal that the \textbf{CC} category constitutes the largest proportion of questions, and SoM-Heter consistently maintains high accuracy.

The primary booster of Heter-MAD's performance gains is its ability to effectively handle \textbf{CW} and \textbf{WC} questions, which together account for a significant portion of the dataset. In these categories, Heter-MAD successfully leverages the strengths of each model, correcting errors that a single-agent baseline might miss. Conversely, for \textbf{WW} questions---where neither model can provide correct answers---Heter-MAD naturally exhibits low accuracy. However, the substantial improvements in the \textbf{CC}, \textbf{CW}, and \textbf{WC} categories sufficiently elevate the overall performance of Heter-MAD. This confirms that incorporating model heterogeneity enables MAD to leverage the diverse strengths of different models. By allowing agents to generate outputs using various models, this simple adjustment proves highly effective, significantly enhancing overall performance and paving the way for future research.

% \myparatight{Summary}

% Our empirical evaluation of Heter-MAD demonstrates that integrating diverse models within a single MAD process is both feasible and promising. Notably, EoT, which achieved the worst performance with homogeneous models, achieves superior performance with heterogeneous while weaker models, indicating that incorporating diverse models can enhance overall outcomes. These counter-intuitive observations highlight the critical need for developing advanced MAD frameworks that effectively leverage model heterogeneity. Consequently, further exploration of combining model diversity with the MAD paradigm represents a valuable direction for future research.

