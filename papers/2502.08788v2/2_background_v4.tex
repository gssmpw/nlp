\section{Background}

MAD methods have garnered significant attention in recent years due to their potential to enhance the reasoning and decision-making capabilities of LLMs. 
%These methods build on the idea of leveraging multiple interacting agents to collaboratively solve problems. In this section, we briefly review the background of MAD methods, with a special focus on their unique mechanisms. We also present a systematic comparison between these MAD methods from several commonly shared dimensions. 
At their core, MAD methods share several common principles. In the initial design proposed by \textbf{SoM}~\citep{duimproving}, MAD typically involves multiple agents following three steps to generate the final response: (1) Response Generation, where each agent produces an initial solution based on its unique perspective; (2) Debate, where agents debate to identify logical inconsistencies or knowledge gaps; and (3) Consensus Building, where the consensus is determined by majority voting or a judge agent.
%generating and evaluating responses, engaging in structured communication to refine their reasoning, and reaching a consensus through collaboration. 
%This collaborative process utilizes more test-time inference budget (e.g., more LLM calls or more tokens generated) than single-agent reasoning to improve the reasoning capability.

A series of following works explored enhancing the reasoning capabilities of MAD by assigning different roles to agents, enabling agents to debate from various perspectives.
%, thereby improving the overall performance of the MAD framework. 
\cite{zhang2023exploring} explores the behavioral logic of MAD from the perspective of social psychology. The authors found that certain combinations of individual traits can enhance the overall performance of the MAD system. \textbf{Multi-Persona (MP)} \citep{liang2023encouraging} incorporates an affirmative agent (angel) and a negative agent (devil) presenting their answer to a judge agent, which ultimately determines the final solution. \textbf{Exchange-of-Thoughts (EoT)} \citep{yin2023exchange} assigns three diverse roles to agents: detail-oriented nature, diligence, and problem-solving abilities. Additionally, it implements a confidence evaluation mechanism designed to reduce the adverse effects of erroneous reasoning processes. \textbf{COMM}~\citep{chen2024comm} encourage diverse thinking in the debate by assigning different reasoning paths to agents with different roles.

A part of the research focused on improving communication topology. 
\textbf{IoA}~\citep{chen2024IoA} organizes agents in a network structure, splitting agents into blocks for better collaboration. In \citep{li2024Sparse}, agents communicate through a sparse topological structure. In \textbf{AgentVerse} \citep{chen2023agentverse}, the verifier can dynamically determine the subsequent execution of MAD processing, allowing dynamic adjustment of communication topology. \cite{qian2024scaling} investigated the scaling effects of MAD systems with more agents using varied communication structures, and found that MAD systems can achieve consistent performance improvements with more agents involved.

Another line of work enhanced the way to exchange and integrate information between agents. \textbf{FORD}~\citep{xiong-FORD} mitigates the inconsistency as the debate processes, and introduces a judge agent to summarize the debate results. \textbf{ReConcile}~\citep{chen2024reconcile} adopts confidence-weighted voting to help consensus seeking. \cite{pham2023let} introduced a novel approach where agents interact using token embeddings instead of natural language. 
%Compared to natural language, token embeddings can convey additional information, such as an agentâ€™s confidence in specific key information. 
\textbf{ChatEval} \citep{chanchateval} explores communication strategies among agents through three frameworks, focusing on the impact of asynchronous responses and round-by-round summarization on agent performance.

With the emergence of an increasing number of MAD frameworks, some recent studies have reviewed various MAD methods from different aspects. \cite{smit2023we} found that MAD methods do not reliably outperform other ensembling reasoning strategies. However, they specifically focus on medical prompting methods and medical benchmarks, which limits the generalizability. 
\cite{khan2024debating} analyzed the performance of MAD systems from the perspective of persuasiveness and found that more persuasive models could enhance the overall MAD performance. \cite{wang2024rethinking} compared single-agent methods with MAD methods and found that providing sufficiently detailed problem descriptions can enhance single-agent inference to a level comparable to MAD methods. However, the single-agent inference approach used in the comparison was specifically calibrated for these detailed descriptions, rather than being a widely adopted single-agent method. Additionally, the evaluation was limited to only three datasets.
% \citept{zhang2023exploring} explores the behavioral logic of MAD from the perspective of social psychology. By combining agents with different individual traits (e.g., stubbornness or conformity ), the authors found that certain combinations of personality can enhance the overall performance of the MAD system.

In summary, while there are positive results celebrating MAD, there are also recent efforts questioning whether MAD is a reliable general approach for enhancing LLM performance.
However, limitations in their evaluation leave the answer unresolved and inconclusive.
This underscores the urgent need for a more thorough and comprehensive evaluation, and necessitates rethinking common evaluation practices in MAD research, particularly the reliance on narrow benchmarks and inconsistent baselines.  
%In summary, although some studies have analyzed the performance of MAD frameworks, the limitations of their evaluation leave uncertainty about whether MAD can serve as a general approach to improving the factual accuracy and reasoning capabilities of LLMs, necessitating a more comprehensive evaluation of MAD frameworks.