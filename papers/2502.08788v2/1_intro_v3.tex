% The remaining part of this paper is organized as follows. In the second section, we will review previous MAD methods and propose a general framework integrating these MAD methods. Following the background we will introduce how we design our experiments and show our empirical results. Finally, we analyze the pattern found from the outcome and discuss several future directions based on our observation.

% \begin{figure*}[htbp]
%     \centering
%     \includegraphics[width=\textwidth]{image/main_pie.pdf}
%     \caption{Performance comparison of MAD across 4 models and 9 benchmarks. Each pie chart represents the distribution of conditions where MAD outperforms or does not outperform CoT.}
%     \label{fig:main_pie}
% \end{figure*}

% \begin{figure}[htb]
%      \centering
%      \includegraphics[width=0.48\textwidth]{image/fig1_2.pdf}
%         \caption{Our evaluation is structured around three pivotal dimensions: performance, efficiency, and robustness.}
%         \label{fig:1}
% \end{figure} 
\section{Introduction}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{image/fig1_4.pdf}
    %\caption{Our evaluation is structured around three pivotal dimensions: performance, efficiency, and robustness.}
    \caption{The promotion and recognition of MAD research require a systematic and comprehensive evaluation, structured around three pivotal dimensions: performance, efficiency, and robustness. }
    \label{fig:1}
\end{figure}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\textwidth]{image/win_rate.pdf}
    \caption{Performance comparison of MAD across 4 LLMs and 9 benchmarks, covering 3 top-level categories: general knowledge, mathematical reasoning, and programming. Each bar represents the distribution of conditions where MAD is better/comparable/worse than CoT. We employ the ANOVA test to evaluate differences among group means, considering a $p$-value greater than 0.05 as an indicator that no significant differences---a tie---exist. 
    %For SoM, MP, EoT, and AGV, we follow the authors' open-source implementations. For CE, which is in lack of an official open-source implementation as of 2024/10, we carefully reproduce it based on implementation details provided in the paper~\citep{chanchateval}. 
    }
    \label{fig:main_pie}
\end{figure*}



%\textcolor{purple}{Everybody's business is nobody's business. vs. two heads are better than one}

The age-old saying, \emph{two heads are better than one}, encapsulates the enduring lesson that collaboration often triumphs over solitary effort.
Can this human wisdom be applied to enhance the capabilities of large language models (LLMs)?
An emerging line of research---commonly known as multi-agent debate or discussion (MAD)---suggests it can. 
Research has shown that after multiple LLM agents independently produce initial answers to a question, by having them engage in several rounds of discussing and reviewing answers from each other, they can improve the factual accuracy and reasoning quality of their final aggregated response~\citep{duimproving}. As such, LLM performance is enhanced at inference time, without the need for additional training, suggesting MAD as an inference-time solution to boosting LLM capabilities.
Thus, unsurprisingly, this line of research has garnered significant attention, with prestigious venues witnessing a surge in the number of publications~\citep{ yin2023exchange, liang2023encouraging, chen2023agentverse, chanchateval, wang2024rethinking, smit2023we}.



Although we, like the community, believe in the potential of MAD, we position that \emph{this field has thus far suffered from a glaring lack of sufficiently systematic and comprehensive evaluations.}
%~\citep{duimproving, liang2023encouraging, yin2023exchange, chen2023agentverse, chanchateval}.
The datasets used for evaluation are limited and have minimal overlap between different MAD methods. 
While some focus on mathematical reasoning~\citep{duimproving, yin2023exchange}, others may target machine translation~\citep{liang2023encouraging}, 
%open-ended dialogue generation~\citep{chanchateval}, 
or programming tasks~\citep{chen2023agentverse}.
In some cases, a newly proposed MAD method is evaluated on another new dataset introduced in the same paper, which has not been fully disclosed~\citep{chanchateval}.
Moreover, newly proposed methods are often compared solely against the very basic approach of directly prompting LLM to generate answers. This overlooks simple single-agent techniques, such as Chain-of-Thought~\citep{cot}, as well as more recent advancements in MAD.
In addition, while evaluations are frequently performed using only proprietary LLMs, the efficiency of MAD that trade-off token consumption and performance gains have rarely been considered, with only few exceptions~\citep{smit2023we}.
%with a single set of parameter configurations. 

% \begin{wrapfigure}{r}{0.5\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{image/fig1.png}
%     \caption{Our evaluation is structured around three pivotal dimensions: performance, efficiency, and robustness.}
%     \label{fig:1}
% \end{wrapfigure}


These current common practices of MAD evaluation can easily give rise to significant doubts about the reproducibility and generalizability of MAD methods, while also creating confusion about the status quo of MAD research. Specifically, do MAD methods genuinely outperform existing methods? To what extent do they outperform simpler single-agent baselines? Among them, what is the state of the art? Do these methods demonstrate robust performance across diverse conditions, or is their success a mere result of cherry-picking---critically dependent on specific dataset choices and parameter settings? Do they excel in both performance and efficiency?
%To put it simply, one might suspect ``\emph{results demonstrating the superiority of a newly proposed method are obtained by an experimental design favorable to that method} \citep{herrmann2024position}''---a concern that has plagued many areas of empirical ML.
Undoubtedly, as long as these questions remain unresolved, they will continue to hinder the recognition and promotion of MAD research, even eroding the trust in MAD research, just as they have plagued many other areas of empirical ML~\citep{herrmann2024position}.


To this end, in this paper, we critically evaluate 5 representative MAD methods across 9 widely adopted benchmarks, spanning various key high-level LLM capabilities: general knowledge, mathematical reasoning, and programming, with 4 different LLMs and various parameter configurations. As illustrated in Figure \ref{fig:1}, our evaluation is structured around three pivotal dimensions: performance, efficiency, and robustness.
Our first set of experiments, comparing these MAD methods to single-agent approaches, including Chain-of-Thought (CoT) and Self-consistency (SC)~\citep{wang2022self}, reveal an astonishing negative result---\emph{these MAD methods generally fail to outperform CoT}, despite CoT being much simpler.
As summarized in Figure 1, none of these MAD methods achieve a win rate higher than 20\% when compared to CoT across 36 scenarios (4 models $\times$ 9 benchmarks).
Further experiments varying the number of agents and debate rounds show that simply adjusting these hyperparameters can hardly reverse this negative outcome.
Moreover, the underperformance becomes even more pronounced when compared to SC, especially when using a comparable number of LLM calls or tokens.
All these results point to a crucial reality---\emph{existing MAD approaches are less effective than currently believed, even underperforming simple single-agent methods like CoT and SC}---an insight that would have remained elusive without a systematic and comprehensive evaluation.

%This suggests that even consuming more tokens, these methods cannot effectively leverage the extra inference budget, reinforcing their non-superiority compared to single-agent approaches.

While the above results seem discouraging, we emphasize that they do \emph{not} imply MAD research is a frustrating dead end. After all, countless examples have demonstrated the power of human collaboration. 
Then, we must ask ourselves: \emph{do current MAD methods truly emulate how people engage in productive discussion?}
A key factor enabling meaningful discussion is the diversity of knowledge and experience among individuals. While different LLMs trained on different data and paradigms may exhibit distinct strengths likewise, this crucial aspect remains largely unexplored in current MAD research---where, when multiple LLM agents are employed, they are typically instantiated from the same model.
We hypothesize that \emph{incorporating model heterogeneity is an effective solution to improving MAD}.
To allow for a broad validation on this hypothesis, we introduce a simple twist that can be incorporated into \emph{any} existing MAD methods: instead of relying on a single model, agents randomly select an LLM from a diverse set of candidate models at inference time to generate responses. 
Despite its simplicity, this twist proves to be broadly effective, leading to performance gains for \emph{all} MAD methods considered.
Beyond model heterogeneity, based on our analysis, we also propose multiple key research questions for future work: (i) \emph{How can MAD be calibrated to fully leverage model heterogeneity?} (ii) \emph{What kind of application scenarios better reflect the utility of MAD?} and (iii) \emph{How to enhance MAD frameworks with single-agent inference approaches?}
Note that we intentionally refrain from delving into well-calibrated technical solutions, such as designing a MAD framework explicitly optimized for leveraging model heterogeneity, as we believe that the development of solutions along the aforementioned directions holds significant promise and merits further exploration within the community.


