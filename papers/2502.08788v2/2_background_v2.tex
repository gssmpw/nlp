\section{Background}

MAD methods have garnered significant attention in recent years due to their potential to enhance the reasoning and decision-making capabilities of large language models (LLMs). These methods build on the idea of leveraging multiple interacting agents to collaboratively solve problems. In this section, we briefly review the background of MAD methods, with a special focus on their unique mechanisms. We also present a systematic comparison between these MAD methods from several commonly shared dimensions. 

At their core, MAD methods share several common principles. They typically involve multiple agents generating and evaluating responses, engaging in structured communication to refine their reasoning, and reaching a consensus through collaboration. This collaborative process utilizes more test-time inference budget than single-agent reasoning to improve the reasoning capability.

Despite these shared principles, different MAD methods exhibit varied considerations. 

\begin{itemize}
    \item Dynamic Adjustments: AgentVerse\cite{chen2023agentverse} dynamically assigns roles to agents to handle evolving problem contexts effectively. Such dynamic adjustment grants MAD more flexible capability to handle varied tasks.

    \item Consensus Mechanisms: Another key consideration is how consensus is derived. Beyond simple majority voting\cite{duimproving}, some MAD frameworks\cite{liang2023encouraging} adopted a judger agent to formulate the final answer.

    \item Agent Heterogeneity: Agents equivalently solve the task in basic MAD designs\cite{yin2023exchange, duimproving}. However, by introducing diverse roles such as judger, reviewer, or verifier, ChatEval\cite{chanchateval} and AgentVerse\cite{chen2023agentverse} make it possible for agents to view the problem through diverse aspects. 
\end{itemize}

In summary, MAD research explores a range of strategies for fostering collaboration among agents while addressing unique challenges such as communication efficiency and adaptability. These innovations collectively highlight the growing importance of MAD frameworks in advancing LLM capabilities. 

However, we note that current MAD research has certain shortcomings in evaluation. Specifically, SoM, AgentVerse, and ChatEval were not compared against CoT on these widely used benchmarks. MP was evaluated against CoT only on a handcrafted dataset named Counter-Intuitive Arithmetic Reasoning (CIAR). EoT was compared to CoT on GSM8k only, in which EoT was shown to be slightly better than CoT. Recently, \cite{smit2023we} analyzed the MAD method and found that the frequency of agents agreeing with each other in MAD has a direct impact on overall performance. However, \cite{smit2023we} did not consider MAD as a test-time computation method and did not reach a consistent conclusion regarding the performance comparison between MAD and single-agent inference methods. In contrast, this paper systematically and comprehensively evaluates the performance of representative MAD frameworks across different scenarios and compares them with single-agent methods. Specifically, the testing and analysis in this paper focus on the perspective of test-time computation, thereby verifying whether MAD frameworks are suitable as test-time computation methods.