% \section{Introduction}
% With the widespread application of Large Language Models (LLMs), extensive attention has been paid to developing more advanced LLMs. As a simple taxonomy of techniques contributing to stronger LLMs, training-time computing and test-time computing improve LLMs in two stages during their lifetime. We have recently observed an increasing interest in test-time computing, which did not receive equivalent attention compared to training-time methods. 

% In this paper, we revisit one of the test-time computing methods, named \textbf{multi-agent debate (MAD)}. Multi-agent LLM debate primarily focuses on combining multiple LLM agents to collaboratively receive, analyze, exchange, integrate, and output information. In a typical workflow of MAD, agents first receive the query and give a solution independently. Then agents debate with each other to determine the final solution. This approach leverages the reasoning capabilities of multiple LLMs without additional training. Society of Minds (SoM) \hf{footnote for clarification}is the first MAD framework incorporating a group of LLM agents simultaneously solving the question and discussing a final answer. Some following works such as Multi-Persona (MP), and Exchange of Thoughts (EoT) continued to improve the MAD framework by introducing new mechanisms, e.g., agent confidence levels, to help agents debate more efficiently.

% However, these previous explorations into MAD have certain limitations. For instance, the evaluation in these works is limited to specific tasks without a comprehensive evaluation of the generalization capability of the proposed MAD framework. Additionally, we observe a lack of holistic assessment of how different decision designs might impact performance, e.g., the number of incorporated agents or the depth of the debating process. Furthermore, previous work also lacked sufficient comparisons between MAD and other advanced inference-stage baselines.

% Considering these limitations, in this paper we try to take a further step toward the design decision of existing MAD methods. Specifically, we try to answer three research questions: \textbf{RQ1)} Can current MAD methods perform consistently better than single-agent inference methods? \textbf{RQ2)} How do different dimensions of design decisions impact the performances of MAD? \textbf{RQ3)} What could be the future directions of MAD? We hope that, by answering these research questions, we can shed light on the future development of MAD methods. 

% The remaining part of this paper is organized as follows. In the second section, we will review previous MAD methods and propose a general framework integrating these MAD methods. Following the background we will introduce how we design our experiments and show our empirical results. Finally, we analyze the pattern found from the outcome and discuss several future directions based on our observation.

% \begin{comment}
% \section{Introduction v2}
% An age-old saying, \emph{two heads are better than one}, encapsulates the enduring lesson that collaboration often triumphs over solitary effort. While traditionally rooted in human interactions, this principle has recently found applications in the field of large language models (LLMs).
% One emerging line of research, commonly referred to as multi-agent debate or discussion (MAD), explores the potential of engaging multiple LLM agents in structured communications. 
% By enabling these agents to iteratively debate or discuss, researchers have shown that, without any additional training, the resulting final outputs can surpass those of a single model. Thus, MAD, attracting growing interest, is recognized as a potential strategy for test-time (or inference-time) computation. 


% A typical MAD works as follows: given a question, multiple LLM agents independently generate initial answers in parallel; over several rounds, agents review other agents' answers and incorporate collective feedback to refine their answers; finally, the refined answers are aggregated to output the final answer. 
% To stimulate critical thinking and divergent feedback, agents are often prompted to role-play certain personas, such as [.....]. 
% Moreover, [...show in what ways (e.g., summarizing the answers?) these approaches vary...] 
% Since MAD somewhat mirrors how humans engage in group discussions, it is perhaps unsurprising that MAD can improve the performance of LLMs at test-time.


% However, critical questions remain--- 
% %However, despite its appeal---aligning naturally with the intuition that deliberative collective thinking can improve the outputs---questions remain regarding its performance and efficiency, particularly when compared to single-agent test-time computation strategies.
    
% \end{comment}

\section{Introduction}
An age-old saying, \emph{two heads are better than one}, encapsulates the enduring lesson that collaboration often triumphs over solitary effort. While traditionally rooted in human interactions, this principle has recently found applications in the field of large language models (LLMs).
%~\citep{chatgpt, instructgpt, vicuna, guanaco, chatglm}.
An emerging line of research, commonly referred to as multi-agent debate or discussion (MAD)~\cite{duimproving, yin2023exchange, liang2023encouraging, chen2023agentverse, chanchateval, wang2024rethinking, smit2023we}, explores the potential of engaging multiple LLM agents in structured communications. 
By enabling these agents to iteratively debate or discuss, researchers have shown that, without any additional training, the resulting final outputs can surpass those of a single model. Thus, MAD, attracting growing interest, is recognized as a potential strategy for test-time (or inference-time) computation. 

\begin{table*}[htb]
\caption{High-level comparison of MAD frameworks.}
\label{tab:mad_features}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|lllllll}
\hline
           & Role-Play & Answer Generation         & \#Agents   & \#Rounds       & Post-procesisng & Heterogeneous \\ \hline
SoM              & N/A       & Majority Voting & Adjustable & Fixed          & N/A             & No            \\
Multi-Persona                 & Fixed     & Judger          & Fixed      & Early-stopping & N/A             & Yes           \\
EoT              & Fixed     & Majority Voting & Adjustable & Early-stopping & Confidence      & No            \\
ChatEval           & Fixed     & Majority Voting                & Adjustable & Early-stopping & N/A             & Yes           \\
AgentVerse        & Dynamic   & Judger          & Adjustable & Early-stopping & N/A             & Yes           \\ \hline
\end{tabular}}
\end{table*}

A typical MAD works as follows: given a question, multiple LLM agents independently generate initial answers in parallel; over several rounds, agents review other agents' answers and incorporate collective feedback to refine their answers; finally, the refined answers are aggregated to form the final answer. 
Besides, agents are often prompted to role-play certain personas, as shown by existing research, to be effective in stimulating critical thinking and divergent feedback.
Moreover, some advanced MAD frameworks incorporate a summarizer agent to dynamically construct the final answer when simple majority voting fails, e.g., on coding tasks~\cite{chen2023agentverse}. 
Since MAD somewhat mirrors how humans engage in group discussions, it is perhaps unsurprising that MAD can improve the performance of LLMs at test time by utilizing more inference budgets and aggregating diverse information extracted from multiple agents.

Some recent works \cite{smit2023we, wang2024rethinking} have paid attention to the working mechanism of MAD frameworks, investigating how different agents can collaborate to improve model performance. However, despite the appealing intuition that deliberative collective thinking can integrate larger inference-time computation as well as diverse knowledge to improve the outputs, questions remain regarding its performance w.r.t. efficiency, particularly when compared to single-agent test-time computation strategies. Can MAD obtain a corresponding level of performance improvement with a reasonable extra test-time computational cost? Can MAD gain consistent performance improvements as computational costs increase (in other words, can a stable scaling law be observed)? Can MAD meet our expectations by effectively aggregating the knowledge of different agents to produce more refined answers?

In this paper, we take a systematic review of existing MAD frameworks, with a special focus on their performance and efficiency in comparison to widely used single-agent inference techniques, such as Chain-of-Thought (CoT) and Self-consistency (SC). Our evaluation of 5 typical MAD frameworks on 9 benchmarks led us to a surprising conclusion---\emph{current MAD frameworks fail to consistently outperform simple single-agent test-time computation strategies, even with more test-time computation}. We further explored the impact of various configurations on MAD performance, including the number of agents, the rounds of debate, and the foundation models used by the agents, in an effort to mitigate the negative effects of suboptimal hyperparameter settings on MAD performance. We did not observe MAD effectively leveraging extra test-time computation to improve reasoning outcomes. After these extensive evaluations, we concluded that current MAD frameworks may not be a good choice for enhancing agent performance at inference time. 

This counter-intuitive observation naturally challenges our previous belief that more test-time computation through multi-agent debate can improve overall performance. We believe this outcome is due to the current MAD framework not effectively leveraging the differences between models for more efficient reasoning. This hinders MAD from integrating the strengths of different agents, leading it to degrade into repetitive single-agent reasoning. We hypothesize that introducing model heterogeneity into the MAD framework can help MAD integrate the knowledge of different models, enabling more effective collaborative reasoning. To validate this hypothesis, we propose a simple and general strategy called \textbf{Mixed-MAD}. Our empirical results show that Mixed-MAD, by mixing the outputs of multiple different LLMs, can achieve further performance gain, even surpassing the best model among all incorporated ones. Analysis indicates that Mixed-MAD can effectively integrate the useful information generated by different base models, thereby enhancing overall reasoning ability. Mixed-MAD demonstrates that model heterogeneity will be an important future direction for the further development of MAD, considering we have shown that MAD is not an efficient test-time computing method. Future work may focus more on flexibly merging the knowledge of different models through debate, integrating their strengths, and complementing each other's knowledge gaps. In summary, our contributions are as follows:

\begin{itemize}
    \item We take a systematic review of 5 MAD frameworks, with comprehensive evaluations on 9 benchmarks. Our empirical results showed that MAD cannot consistently outperform baselines even with a larger inference budget.

    \item We conduct an in-depth analysis of the scaling efficiency of MAD, and from the analysis, we find that MAD does not effectively utilize extra inference budget, indicating that MAD is not an optimal test-time computing method.

    \item From the perspective of model heterogeneity, we proposed Mixed-MAD. Mixed-MAD can consistently improve MAD's performance across different benchmarks. This phenomenon suggests that incorporating different foundation models into MAD is a promising future direction. Based on our findings, we also discuss feasible future development directions for MAD.
\end{itemize}


%However, despite its appeal---aligning naturally with the intuition that deliberative collective thinking can improve the outputs---questions remain regarding its performance and efficiency, particularly when compared to single-agent test-time computation strategies.



