% Recently, impressive attention has been paid to exploring Multi-Agent Debate (MAD) leveraging collaboration among multiple large language model (LLM) agents to improve test-time performance. In this paper, we take a systematic review of existing MAD frameworks, with a special focus on their performance and efficiency in comparison to widely used single-agent inference techniques, such as Chain-of-Thought (CoT). However, in this paper, we empirically show that MAD fails to consistently outperform simpler single-agent strategies, even with increased computational resources. Our analysis of factors such as agent configurations and debate rounds suggests that existing MAD designs fall short in fully utilizing additional inference-time computation. From our analysis, we found that one previously overlooked factor, model heterogeneity, can significantly improve MAD frameworks. We propose Heter-MAD, which enables a single LLM agent to access the output from heterogeneous foundation models. Though being extremely simple and naive, Heter-MAD surprisingly boosts the performance of current MAD frameworks. The success of Heter-MAD demonstrates that model heterogeneity may be a promising direction for future MAD development. 
\begin{abstract}
Multi-agent debate (MAD) has emerged as a promising approach to enhance the factual accuracy and reasoning quality of large language models (LLMs) by engaging multiple agents in iterative discussions during inference. Despite its potential, we argue that current MAD research suffers from critical shortcomings in evaluation practices, including limited dataset overlap and inconsistent baselines, raising significant concerns about generalizability. Correspondingly, this paper presents a systematic evaluation of five representative MAD methods across nine benchmarks using four foundational models. Surprisingly, our findings reveal that MAD methods fail to reliably outperform simple single-agent baselines such as Chain-of-Thought and Self-Consistency, even when consuming additional inference-time computation. From our analysis, we found that model heterogeneity can significantly improve MAD frameworks. We propose Heter-MAD enabling a single LLM agent to access the output from heterogeneous foundation models, which boosts the performance of current MAD frameworks. Finally, we outline potential directions for advancing MAD, aiming to spark a broader conversation and inspire future work in this area.     
\end{abstract}
