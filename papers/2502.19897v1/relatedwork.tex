\section{Related Work}
Our research is built upon graph-based and fuzzy-based methods. Therefore, we briefly review the development of both fuzzy and graph clustering approaches. We use $\mathcal{X}=\{\boldsymbol{x}_i \}_{i=1}^n$ to denote an $n$-point set, where $\boldsymbol{x}_{i}\in\mathbb{R}^{d}$ is the i-th $d$-dimensional sample. $\boldsymbol{W}$ is the $n \times n$ similarity graph based on $\mathcal{X}$. Our work aims to study the problem that the number of classes is known to be $c \in \mathbb{N}$, therefore the clustering goal aims to divide $\mathcal{X}$ into $c$ mutually disjoint clusters. In our previous work, Probability Aggregation Clustering (PAC) \cite{yan2024pac}, we introduced a preliminary centerless fuzzy clustering model based on the probability aggregation operation. Building on this approach, the current paper presents an effective improvement leveraging graph knowledge and provides a thorough theoretical analysis.

\subsection{Fuzzy Clustering}
In hard (or crisp) clustering, each instance is assigned to a single specific cluster. However, such approaches struggle to capture data where instances are intertwined or exhibit overlapping characteristics. To address this limitation, fuzzy clustering methods have been proposed, drawing on fuzzy mathematics to provide a more flexible representation of the data. 

Set $\boldsymbol{P}=[p_{i,l}]_{n\times c}$ denote the fuzzy membership matrix, where $p{i,l}$ represents the membership degree of instance $i$ to cluster $l$, which can also be interpreted as the assignment probability.
$\boldsymbol{P}$ satisfies $\boldsymbol{P}\in \mathcal{P}=\{[\phi_{i,l}]_{n\times c}|\phi_{i,l} \in [0,1]; \sum_{l=1}^{c}\phi_{i,l}=1; 0<\sum_{i=1}^{n}\phi_{i,l}<n,i=1,\cdots,n,l=1,\cdots,c\}$.  
Fuzzy C-Means (FCM) \cite{bezdek1984fcm} formulates the clustering problem based on Euclidean distances between instances and its corresponding cluster centers. The formulation of FCM is:
\begin{equation}\label{eqn-0-1} 
\min\limits_{\boldsymbol{P} \in \mathcal{P}} \sum_{i=1}^n \sum_{l=1}^c p_{i,l}^m \Vert \boldsymbol{x}_i- \boldsymbol{v}_l\Vert_2^2,
\end{equation}  
where $m>1$ is the fuzzy weighting exponent, and $\boldsymbol{v}_l$ represents the cluster center which is defined as ($l=1,\cdots,c$):
\begin{equation}\label{eqn-0-2} 
\boldsymbol{v}_l = \frac{\sum_{i=1}^n p_{i,l}^m\boldsymbol{x}_i}{\sum_{i=1}^n p_{i,l}^m}.
\end{equation}  
The optimal solution of FCM is ($i=1,\cdots,n; l=1,\cdots,c$):
\begin{equation}\label{eqn-3} 
    p_{i,l}= \frac{\Vert \boldsymbol{x}_i- \boldsymbol{v}_l\Vert_2^{2/(m-1)}}{\sum_{r=1}^c\Vert \boldsymbol{x}_i- \boldsymbol{v}_r\Vert_2^{2/(m-1)}}.
\end{equation} 
In FCM, the final cluster prediction of $\boldsymbol{x}_{i}$ is the highest probability, i.e., $\displaystyle \hat{p_{i}}=\arg\max\limits_{l}(\boldsymbol{p}_i)_l$.

\subsection{Graph-based Clustering}
In graph-based clustering, data points are represented as nodes in a graph. Among various techniques, the Spectral Clustering (SC) family is one of the most widely adopted graph-based methods. Self-Constrained Spectral Clustering (Self-CSC) \cite{bai2022self} enhances the objective function of SC by incorporating pairwise and label self-constrained terms, allowing high-quality clustering without relying on prior information. However, the time complexity of Self-CSC is cubic, limiting its scalability. Beyond modifications to the basic SC algorithm, researchers have also combined the SC framework with other clustering approaches to leverage the advantages of multiple techniques. For example, Centerless Clustering (K-sums) \cite{pei2022centerless} integrates SC with K-means to achieve efficient clustering. The objective function of K-sums minimizes the sum of distances between points within the same cluster, but it lacks the modeling of local detail structure. Despite these efforts, graph-based approaches often struggle to find a good balance between complexity and performance.