\section*{Limitations}
This paper discusses approaches to endowing models with safety-aware reasoning capabilities. Limited by the size and the inherent capabilities of the foundation models, we focus primarily on reasoning distillation from the reasoning model $\mathcal{M_R}$ to improve safety, rather than relying on methods such as reinforcement learning and test-time scaling, which encourage the model to reason and self-explore. Future work could focus on how to integrate safety-aware model reasoning into ReFT~\cite{trung-etal-2024-reft}-like approaches. Additionally, the safety of multi-modal reasoning models still remains to be explored, which can expand the application boundaries of safety-aware reasoning in enhancing the safety of LLMs.


\newpage

\section*{Ethics Statement}
This paper is aimed at exploring a defense technique against different jailbreak attacks. In order to better demonstrate the effects of jailbreaks and defenses, it is inevitable that we include some potentially controversial LLM-generated content in our paper. During our investigations, we may also fool some of LLMs to follow harmful instructions with existing jailbreak attack approaches. However, it is exactly what we are eager to do to prevent LLMs from causing potentially harmful behaviors in real-world use and to improve the LLMs' robustness against adversarial jailbreaks. It is useful for the overall safety of LLM usage.

\section*{Acknowledgement}

This work was supported by the National Science Fund for Excellent Young Scholars (Overseas) under grant No.\ KZ37117501, National Natural Science Foundation of China (No.\ 62306024).