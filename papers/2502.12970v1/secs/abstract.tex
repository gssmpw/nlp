\begin{abstract}
The reasoning abilities of Large Language Models~(LLMs) have demonstrated remarkable advancement and exceptional performance across diverse domains. However, leveraging these reasoning capabilities to enhance LLM safety against adversarial attacks and jailbreak queries remains largely unexplored. To bridge this gap, we propose \textbf{\textit{Reasoning-to-Defend}}~(\texttt{R2D}), a novel training paradigm that integrates safety reflections of queries and responses into LLMs' generation process, unlocking a safety-aware reasoning mechanism. This approach enables self-evaluation at each reasoning step to create safety pivot tokens as indicators of the response's safety status. Furthermore, in order to improve the learning efficiency of pivot token prediction, we propose Contrastive Pivot Optimization~(CPO), which enhances the model's ability to perceive the safety status of dialogues. Through this mechanism, LLMs dynamically adjust their response strategies during reasoning,  significantly enhancing their defense capabilities against jailbreak attacks. Extensive experimental results demonstrate that \texttt{R2D} effectively mitigates various attacks and improves overall safety, highlighting the substantial potential of safety-aware reasoning in strengthening LLMs' robustness against jailbreaks.\footnote{Code is available at \url{https://github.com/chuhac/Reasoning-to-Defend}}
\end{abstract}