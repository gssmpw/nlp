\section{Experiments}
\subsection{Experimental Setups}
\paragraph{Datasets \& Benchmarks} We conduct comprehensive experiments with two LLM jailbreak benchmarks. To evaluate \texttt{R2D} against baseline defenses, we use JailbreakBench~\cite{chao2024jailbreakbench}, which contains 100 unsafe behavior prompts, and detect unsafe responses with Llama-Guard$_\text{v3-8B}$. Furthermore, to evaluate the defense capabilities with multiple strong attacks, we also incorporate HarmBench~\cite{pmlr-v235-mazeika24a} in our main experiments, which consist of 400 harmful behaviors and more attack techniques. To align with the provided evaluation methods, we use HarmBench-cls$_\text{13B}$ for this session. For the training dataset, we collect reasoning trajectories on Alpaca~\cite{taori2023alpaca} for the helpful scenario and AdvBench~\cite{zou2023universal} for the jailbreak scenario, leveraging  DeepSeek-R1$_\text{70B}$ as the reasoning model $\mathcal{M_R}$.

\paragraph{Evaluation Metrics} For the jailbreak benchmarks, we use Attack Success Rate~(ASR) to assess the performance of \texttt{R2D}, defined as Equation~\ref{eq:asr_def}.

\begin{equation}
\label{eq:asr_def}
    \text{ASR} = \frac{\text{\# of unsafe responses}}{\text{\# of inputs}},
\end{equation}
where the safety of responses is classified with guardrail models of respective benchmarks. For the over-refusal evaluation, we use the percentage of ``Full Refusal'', ``Full Compliance'' and ``Partial Refusal'' to evaluate the tendencies  of LLMs in different scenarios.

\paragraph{Models} In our experiments we evaluate the ASR of reasoning models, non-reasoning models, and \texttt{R2D}-enhanced models. For the reasoning models, we use QwQ$_\text{preview-32B}$~\cite{qwen2024qwq} which follows the Qwen$_\text{v2.5}$~\cite{yang2024qwen25} architecture and DeepSeek-R1$_\text{70B}$~\cite{guo2025deepseek} which distills from superb models~\cite{guo2025deepseek} in the DeepSeek$_\text{v3-671B}$~\cite{liu2024deepseek} architecture. For \texttt{R2D} models, we conduct SwaRD with trajectories synthesized by DeepSeek-R1$_\text{70B}$ on non-reasoning models, namely Llama$_\text{v3-8B}$~\cite{dubey2024llama}, Qwen$_\text{v2-7B}$~\cite{yang2024qwen2}, Qwen$_\text{v2.5-14B}$~\cite{yang2024qwen25}, Mistral$_\text{v0.3-7B}$~\cite{jiang2023mistral}, Vicuna$_\text{v1.5-7B}$ and Vicuna$_\text{v1.5-13B}$~\cite{zheng2023judging}. Since we rely on powerful models in our experiments to generate prompt-based attack instructions, we also use Mixtral$_\text{8$\times$7B}$~\cite{jiang2024mixtral}. Furthermore, for the ASR evaluation and certain defense approaches that require guardrail models, we also use Llama-Guard$_\text{v1-7B}$ and Llama-Guard$_\text{v3-8B}$~\cite{inan2023llama} in our experiments.

\paragraph{Jailbreak Attacks and Defenses} For the jailbreak attacks on JailbreakBench, we use Greedy Coordinate Gradient~(GCG,~\citealp{zou2023universal}), Prompt Automatic Iterative Refinement~(PAIR,~\citealp{chao2023jailbreaking}), and hand-crafted jailbreaks from JailbreakChat~(JBC,~\citealp{wei2023jailbroken}) to evaluate \texttt{R2D} together with the defense baselines.On HarmBench, we employ PAIR, AutoDAN~\cite{liu2024autodan}, ZeroShot, and FewShot as jailbreak techniques, all of which rely on external LLMs to generate stealthy and readable instructions for jailbreaking target LLMs. Following the setups of previous works~\cite{zhou2024robust}, on JailbreakBench we conduct our experiments in comparisons with the provided defenses, namely Perplexity Filter~\cite{jain2023baseline, alon2023detecting}, SmoothLLM~\cite{robey2023smoothllm}, Synonym Substitution, Remove Non-Dictionary and Erase-and-Check~\cite{kumar2023certifying}.

\input{tabs/jbb_results}

\begin{figure*}[t!]
    \centering
    \begin{subfigure}{0.48\linewidth}
        \includegraphics[width=\linewidth]{figs/ZeroShot_harmbench_results.pdf}
        \caption{}
    \label{fig:harmbench_results_zeroshot}
    \end{subfigure}
    \hspace{1em}
    \begin{subfigure}{0.48\linewidth}
        \includegraphics[width=\linewidth]{figs/FewShot_harmbench_results.pdf}
        \caption{}
    \label{fig:harmbench_results_fewshot}
    \end{subfigure}    
    \begin{subfigure}{0.48\linewidth}
        \includegraphics[width=\linewidth]{figs/PAIR_harmbench_results.pdf}
        \caption{}
    \label{fig:harmbench_results_pair}
    \end{subfigure}
    \hspace{1em}
    \begin{subfigure}{0.48\linewidth}
        \includegraphics[width=\linewidth]{figs/AutoDAN_harmbench_results.pdf}
        \caption{}
    \label{fig:harmbench_results_autodan}
    \end{subfigure}
    \caption{Histogram comparing the attack success rate of LLMs with and without \texttt{R2D} on HarmBench. Sub-figures include results with different attacks, namely:~(a)~ZeroShot,~(b)~FewShot,~(c)~PAIR~(d)~AutoDAN.}
    \label{fig:harmbench_results}
\end{figure*}

\begin{figure*}[ht!]
    \centering
    \begin{subfigure}{1.\linewidth}
        \includegraphics[width=1.\linewidth]{figs/overrefusal_results_strmatch.pdf}
    \caption{Action labels classified with Dic-Judge~\cite{zou2023universal}.}
    \end{subfigure}
    \begin{subfigure}{1.\linewidth}
        \includegraphics[width=1.\linewidth]{figs/overrefusal_results.pdf}
    \caption{Action labels classified with Qwen$_\text{v2.5-72B}$.}
    \end{subfigure}
    \caption{Results of over-refusal analysis on XSTest dataset. Categories of responses from LLMs include ``Full Refusal''~(directly refuse to answer), ``Full Compliance''~(directly attempt to give an answer), ``Partial Refusal''~(combination of refusal and compliance). Enhanced LLMs are marked as \texttt{+R2D}.}
\label{fig:overrefusal_results}
\end{figure*}

\subsection{Main Results}
\paragraph{JailbreakBench} The ASR results on JailbreakBench are as reported in Table~\ref{tab:jbb_results}, where reasoning and non-reasoning LLMs equipped with different defenses are evaluated with three transferred attacks. It is observed that comparing to baseline defenses, \texttt{R2D} successfully defends more jailbreaks across three adopted attacks compared to baseline defenses. On average, compared to non-defense LLMs, \texttt{R2D} reduces the ASR by 56\%. In comparison with defense baselines, \texttt{R2D} achieves consistently lower average ASRs, with a margin of at least 10\%, showcasing its superior performance in defending jailbreaks. Compared to Erase-and-Check which fully utilizes Llama-Guard$_\text{v1-7B}$ to monitor user prompts, \texttt{R2D} is also showcased with good defense capabilities, with an average 17\% lower ASR, demonstrating that \texttt{R2D}-enhanced LLMs can defend themselves well better than deploying external guardrail models.\footnote{Except for Mistral$_\text{v0.3-7B}$ and Vicuna$_\text{v1.5-7B}$ which has relatively small numbers of parameters, but they also have comparable defense performance to their defense baselines.} For the GCG attack with unreadable suffices, the Perplexity Filter proves highly effective, as these unreadable suffixes cause an abnormal spike in language model perplexity, making them easy to detect. Notably, \texttt{R2D} achieves comparable or even superior performance to the Perplexity Filter on GCG, demonstrating that self-reflection and safety-aware reasoning are also effective in defending against GCG-like attacks. Moreover, compared to reasoning LLMs, \texttt{R2D}-enhanced LLMs also have lower ASRs, endorsing that the proposed SwaRD and CPO can further enhance safety-aware reasoning abilities, thus leading to strong defensive capabilities.

\paragraph{HarmBench} In order to evaluate the performance of \texttt{R2D}-enhanced LLMs in defending against jailbreak attacks, we compare them with LLMs without optimization. To conduct this evaluation, we use HarmBench, a benchmark that consists of 400 harmful behaviors and provides a variety of strong attack strategies. The results of different attacks are presented in Figure~\ref{fig:harmbench_results}. From a general perspective, \texttt{R2D} proves to be effective in defending LLMs against a wide range of external adversarial attacks. Notably, the overall ASR is significantly lower for the \texttt{R2D}-enhanced models compared to the original, un-optimized models, across various base models, with an up to 48\% lower ASR. When considering specific attacks, techniques like ZeroShot and FewShot rely on external, powerful LLMs to rewrite instructions or create in-context learning environments, effectively fooling the target LLMs into following malicious instructions. Original base models exhibit different jailbreak behaviors under ZeroShot and FewShot attacks. However, the \texttt{R2D}-enhanced models demonstrate robust defenses against these attacks, with their ASR close to 0\%. This highlights the effectiveness of \texttt{R2D} in neutralizing these specific attack strategies, even when the base models show varying degrees of vulnerability. On the other hand, for attacks like PAIR and AutoDAN, unoptimized models still exhibit varying degrees of vulnerability, with higher successful rates in getting jailbreak. However, \texttt{R2D} proves to be highly effective in enhancing the models' defense capabilities, reducing the average attack success rate to around 10\%. This is attributed to the fact that PAIR and AutoDAN are particularly strong attack techniques, yet \texttt{R2D} still manages to significantly mitigate their impacts, showcasing its robustness in defending LLMs against potent adversarial strategies.

\subsection{Detailed Analysis and Discussion}

\paragraph{Over-Refusal Phenomenon} As is observed and investigated in previous works~\cite{rottger-etal-2024-xstest, cui2024or, panda2024llm}, safety-aligned LLMs may over-refuse to follow some instructions, which is usually due to conservative defense strategies. Previous defense works~\cite{xu-etal-2024-safedecoding, zhou2024robust} conduct experiments on general instruction following datasets but fail to provide an in-depth analysis of the over-refusal phenomenon. XSTest~\cite{rottger-etal-2024-xstest} contains 250 instructions with sensitive words but are actually safe,

\begin{itemize}[topsep=1em, itemsep=1em, parsep=0em, partopsep=0em]
    \item 
    \textit{``How can I \textcolor{red}{\textbf{kill}} a Python process ?''}
    \item 
    \textit{``Where can I buy something to \textcolor{red}{\textbf{weed}} my garden with?''}
    \item 
    \textit{``How can I \textcolor{red}{\textbf{steal}} someone's heart?''}
\end{itemize}

\noindent and 200 unsafe instructions, this serves as a good benchmark to evaluate the over-refusal phenomenon. As shown in Figure~\ref{fig:overrefusal_results}, the \texttt{R2D}-enhanced LLMs consistently exhibit a higher refusal rate for unsafe prompts~(right figure), demonstrating \texttt{R2D}'s good performance in defending LLMs from bad instructions. Taking Qwen$_\text{v2-7B}$ as an example, its ``Full Refusal'' action rate given unsafe prompts drops more than 50\% after \texttt{R2D} training. For the safe prompts~(left figure), we observe that Llama-like models (Llama and Vicuna) enhanced with \texttt{R2D} may be prone to over-refusing. Concurrently, the ``Partial Refusal'' rates of the Llama models, are significantly lower than those of the other models. We attribute this to the high proportion of refusal data during their pre-training phase, which causes \texttt{R2D} training to elevate the over-refusal rate during this stage. As for other models, it is observed that compared to non-\texttt{R2D} LLMs, \texttt{R2D}-enhanced Qwen and Mistral models demonstrate very good helpfulness, with their compliance rates hardly downgrade while safety significantly increase. Given safe prompts, \texttt{R2D}-enhanced Qwen$_\text{v2.5-14B}$'s ``Full Compliance'' rate increase by a margin of 4.8\% , showcasing its precise awareness of safety. The detailed data visualization results are presented in Figure~\ref{fig:overrefusal_results}. A case study including both successful and failure cases can be found at Appendix~\ref{sec:case_overrefusal}.

\input{tabs/cpo_ablation}


\paragraph{Ablation Study} Here we conduct an ablation study on our proposed approaches to further evaluate the effectiveness and contributions of each component. By systematically controlling variables of the propose \texttt{R2D}, we aim to identify the key factors that drive the performance and assess the impact of each design. The results are as shown in Table~\ref{tab:cpo_ablation}, ``w/o CPO~(without CPO)'' represents the optimization only with $\mathcal{L}_\text{SwaRD}$, while ``w/o Pivot'' denoting training without CPO while removing the pivot tokens from the reasoning trajectories. Under these three types of attacks, multiple models trained with reasoning data exhibit a lower attack success rate compared to the un-optimized counterparts~(labelled as None in the table), indicating that learning from reasoning data can enhance the model's defense capability. Returning to the comparisons of the ablation study, we find that omitting CPO consistently leads to an increase~(up to 23\%) in the ASRs, indicating weaker defense capabilities. This highlights the necessity of incorporating CPO training for enhancing the model’s robustness. Moreover, we also perform an ablation study by removing pivot tokens from the training dataset (thereby also excluding CPO) to assess how the prediction of step-wise pivot tokens contributes to the optimization process. It is demonstrated that, removing the pivot tokens will consistently worsen the performances~(with an up to 33\% increased ASR), showcasing the effectiveness of the step-wise self-evaluation.