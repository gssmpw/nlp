\section{\textit{Reasoning-to-Defend}}
In this section, we provide a detailed introduction to \texttt{R2D} starting from an overview of its framework. The safety-aware reasoning capabilities are enhanced through reasoning distillation. Moreover, we introduce contrastive pivot optimization to further improve LLMs' awareness of safety at each step.

\subsection{Overview of \texttt{R2D} Framework}
The overview of \texttt{R2D} is as depicted in Figure~\ref{fig:overview_r2d}.
Note that conventional safe LLMs encountering jailbreak prompts will hard refuse the requests without giving any reasons, which is proved hard to generalize~\cite{andriushchenko2025does}. 
While the unsafe LLMs usually fail to be safe because they try to be more ``helpful'' and end up giving dangerous pieces of advice.

Unlike the aforementioned conditions, \texttt{R2D} unlocks a safety-aware reasoning paradigm for LLMs through reasoning trajectory distillation. Specifically, during \texttt{R2D}'s inference, it generates an inner reasoning process with step-wise self-evaluation, forming safety-aware \textsc{pivot tokens} of each step of responses, and indicating whether this step is safe~(marked as \texttt{[SAFE]}), unsafe~(marked as \texttt{[UNSAFE]}) or requires further refinement~(marked as \texttt{[RETHINK]}). The \textsc{pivot tokens} also serve as indicators, reminding LLMs of the safety situations during generation. After the reasoning and generation are finished, the \texttt{[UNSAFE]} reasoning process and answers are kept invisible from users, maintaining the safety of the conversations. Thereby, this post-processing strategy won't sacrifice the helpfulness given a safe instruction.

\subsection{Safety-Aware Reasoning Distillation}
In order to achieve safety-aware reasoning, first and foremost we concentrate on trajectory distillation, which transfers the decision-making and reasoning process from strong reasoning LLMs. Previous work~\cite{shridhar-etal-2023-distilling, li2024turning} has explored the feasibility of distilling the CoT process from larger models to gain better performances on math problems~\cite{cobbe2021training, hendrycks2021measuring}. Different from math domains, capabilities and defense need a trade-off in the context of safety, which places different requirements for the distillation recipes. 

\paragraph{Reasoning Trajectory Synthesis} To this end, \texttt{R2D} begins by synthesizing long reasoning trajectory in both normal-use and jailbreaking scenarios, which reflect a wide range of potential situations, hereby improving the reasoning capabilities of LLMs while enhancing their safety. In the normal-use scenario, LLM learns how the reasoning LLM solves complex problems, ensuring optimal performance. In contrast, in the jailbreaking scenario, LLMs learn to keep aware of the safety of the responses, thus identifying and defending potential malicious instructions. In practice, the safety-aware reasoning skills are distilled from a strong reasoning LLM~\cite{guo2025deepseek} to non-reasoning LLMs. The original reasoning trajectories are collected with safety-aware contexts, which is formalized as Equation~\ref{eq:dr_compositions}.
\begin{equation}
\label{eq:dr_compositions}
    \mathcal{D_R} = \bigcup_{\substack{\mathcal{I} \in \{\mathcal{I_S}, \mathcal{I_J}\}, \\ \mathcal{Y} \in \{\mathcal{Y_A}, \mathcal{Y_R}\}}} \mathcal{M_R}(\mathcal{Y} \mid \mathcal{I, C_S}),
\end{equation}
where $\mathcal{M_R}$ denotes the reasoning model, $\mathcal{C_S}$ denotes the safety-aware context that guides the model to maintain a sense of safety during reasoning. The dataset $\mathcal{D_R}$ consists of the responses given~(\romannumeral 1)~Instructions $\mathcal{I}$: safe instructions $\mathcal{I_S}$ and jailbreaking instructions $\mathcal{I_J}$;~(\romannumeral 2)~Responses $\mathcal{Y}$: $\mathcal{Y_R}$ is the reasoning trajectory of $\mathcal{M_R}$, $\mathcal{Y_A}$ represents the final answer after reasoning. 

\paragraph{Distillation Objective} The reasoning trajectories are utilized in the Safety-aware Reasoning Distillation~(SwaRD) process, where a non-reasoning LLM acquires reasoning skills from a safety perspective. Likewise supervised fine-tuning, non-reasoning LLMs are optimized with $\mathcal{D_R}$ as depicted in Equation~\ref{eq:distill_loss}:
\begin{equation}
\label{eq:distill_loss}
    \mathcal{L}_{\text{SwaRD}} = - \mathbb{E}_{\mathcal{X}, \mathcal{Y} \sim D_R} \Big[ \log P_{\mathcal{M}}(\mathcal{Y} \mid \mathcal{X}) \Big],
\end{equation}
where $P_{\mathcal{M}}(\cdot\mid \mathcal{X})$ represents the probability distribution modeled by the optimized LLM $\mathcal{M}$ given the instruction $\mathcal{X}$. Minimizing $\mathcal{L}_{\text{SwaRD}}$ increases the likelihood that LLMs engage in reasoning before generating, effectively mimicking the reasoning model $M_R$ and thereby achieving the goal of distillation. According to the properties of conditional probability, when expanded into a token-by-token form—making it more compatible with next-token prediction—the language model probability can be expressed as shown in Equation~\ref{eq:p_conditional}.
\begin{equation}
\label{eq:p_conditional}
\begin{aligned}
&P_{\mathcal{M}}(\mathcal{Y} \mid \mathcal{X}) = P_{\mathcal{M}}(\mathcal{Y_R \oplus Y_A} \mid \mathcal{X}) \\
&= \prod_{t=1}^{\mathcal{T_R}} P_{\mathcal{M}}(\mathcal{Y_R}_{;t} \mid \mathcal{Y_R}_{;<t}; \mathcal{X})^{\frac{1}{\mathcal{T_R}}} \\
&\quad \cdot \prod_{t=1}^{\mathcal{T_A}} P_{\mathcal{M}}(\mathcal{Y_A}_{;t} \mid \mathcal{Y_A}_{;<t}; \mathcal{Y_R}, \mathcal{X})^{\frac{1}{\mathcal{T_A}}},
\end{aligned}
\end{equation}
where $\oplus$ is the concatenation of reasoning and final answer, $t$ represents a single token in each response, $\mathcal{T}_{(\cdot)}$ denotes the length of response.

\subsection{Contrastive Pivot Optimization}
To further strengthen LLMs' abilities to self-defend during reasoning, \texttt{R2D} incorporates a mechanism in which LLMs are trained to predict a pivot token at the end of each reasoning step. The pivot token serves as a critical checkpoint, guiding the model to assess the safety of its current reasoning trajectory or responses and enabling it to modify or discard unsafe paths. To encourage more effective learning of this process, thereby improving the safety of responses, we propose Contrastive Pivot Optimization~(CPO), whose training objective is as formalized in Equation~\ref{eq:cpo_loss}. 

\begin{equation}
\label{eq:cpo_loss}
\begin{aligned}
\mathcal{L}_{\text{CPO}} = -\mathbb{E}_{\mathcal{X, Y\sim D_R}}&\Big\{\log \sigma\big[\log P_\mathcal{M}(\mathbf{t_{p}^+} \mid \mathcal{Y,X}) \\
- &\log P_\mathcal{M}(\mathbf{t_{p}^-} \mid \mathcal{Y,X})\big]\Big\},
\end{aligned}
\end{equation}
where $\sigma(\cdot)$ denotes the sigmoid function. $\mathbf{t_{p}^+}$ denotes the ground truth pivot token at each reasoning step, while $\mathbf{t_{p}^-}$ represents the opposite token of $\mathbf{t_{p}^+}$, thus achieving contrastive learning. In practice, $\mathcal{L}_\text{CPO}$ is added to the final loss together with $\mathcal{L}_\text{SwaRD}$. During data synthesis, the \textsc{pivot tokens} are initially generated through the reasoning LLM's self-evaluation, primarily yielding the pivot token \texttt{[RETHINK]}. Subsequently, a guardrail model~\cite{inan2023llama} is employed to perform safety-aware tagging, ensuring that each reasoning step is accompanied by more precise and contextually appropriate \textsc{pivot tokens}. This process helps align the predicted \textsc{pivot tokens} with safety protocols by evaluating the reasoning trajectory for potential risks at each step. The tagged \textsc{pivot tokens}, along with their corresponding reasoning trajectories, are then aggregated to construct the safety-aware reasoning dataset, denoted as $\mathcal{D_R}$. This dataset serves as the foundation for \texttt{R2D} training, effectively balancing capability and safety, thereby enabling more robust decision-making in real-world scenarios.

