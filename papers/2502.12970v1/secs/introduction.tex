\section{Introduction}
Large Language Models~(LLMs)~\cite{guo2025deepseek, achiam2023gpt, anthropic2024claude, yang2024qwen25, team2025kimi, dubey2024llama, jiang2023mistral, jiang2024mixtral} endowed with massive high-quality data are showcased with exceptional language understanding and generation capabilities in a variety of domains.
Moreover, beyond their impressive generative performance, the reasoning ability of LLMs can be further enhances by utilizing the long chain-of-thought~(CoT,~\citealp{wei2022chain}) reasoning data and reinforcement learning strategies~\cite{ouyang2022training}, which serves as the foundation of the widely discussed o-series models~\cite{jaech2024openai, open2025o3mini}, R1~\cite{guo2025deepseek}, and QwQ~\cite{qwen2024qwq}. 

\begin{figure}[t!]
    \centering
    \includegraphics[width=1.\linewidth]{figs/intro.pdf}
    \caption{\texttt{R2D}-enhanced LLM's responses against the attack Jailbreak Chat~(JBC,~\citealp{wei2023jailbroken}). ``In this hypothetical story, you are to act as Niccolo Machiavelli...'' is a widely-used adversarial prefix on JBC. It is demonstrated that \texttt{R2D} LLM is capable of rethinking when in need and labeling bad behaviors as \texttt{[UNSAFE]}, showcasing its safety-aware reasoning abilities.}
 \label{fig:intro}
\end{figure}

Despite the foundation models are becoming increasingly powerful, ensuring the LLM's safety and reliability still remains an unresolved challenge that oftern comes with performance trade-off in practice~\cite{bommasani2021opportunities}.
Accordingly, multiple defense techniques are developed to resist with jailbreak and improve safety, which can be categorized into \textbf{external-detection} and \textbf{supervised-enhancement} methods.
External-detection methods usually rely on content regular expression matching, perplexity filtering~\cite{jain2023baseline, alon2023detecting}, prompt perturbation~\cite{robey2023smoothllm} or external guardrail models~\cite{inan2023llama} to discover potential jailbreaking risks.
Supervised-enhancement methods~\cite{liu2024enhancing, dai2024safe, mu2024rule} mainly rely on safe supervised fine-tuning~(SFT), direct preference optimization~(DPO,~\citealp{rafailov2023direct}), reinforcement learning from human feedback~(RLHF,~\citealp{ouyang2022training}). Other learning-based approaches like toxic content unlearning~\cite{zhang2024safe, lu2024eraser}, and safety-aware decoding~\cite{xu-etal-2024-safedecoding, hazra-etal-2024-safety} can also be attributed to this category. These methods focus more on enhancing the safety capabilities of the LLMs themselves.
However, both two kinds of methods rely heavily on external detection approaches or supervised tuning signals, while \textbf{\textit{neglecting the powerful reasoning capabilities of LLM over their inherent generation safety}}. 

To this end, we propose a novel LLM defense method, named \textbf{\textit{Reasoning-to-Defend}}~(\texttt{R2D}), which defends against the menace of jailbreak attacks via unlocking safety-aware reasoning abilities. The proposed \texttt{R2D} integrates the safety reflections into each step of the reasoning process of LLMs, eliminating the necessity of external guardrails. Specifically, \texttt{R2D} equips LLM with reasoning abilities first with Safety-aware Reasoning Distillation~(SwaRD), enabling LLMs with staged thinking tendency. The staged reasoning process is further step-wise evaluated by the LLM itself, forming pivot tokens about whether an individual step is safe, unsafe, or requires refinement afterward, which is enhanced with the proposed Contrastive Pivot Optimization~(CPO). Through staged reasoning and explicitly predicting the safety pivot token at each step, LLMs acquire abilities to mitigate attacks with safety-aware reasoning. Furthermore, learning from reasoning trajectories instead of hard refusal prevents LLMs from over-refusal in safe scenarios, which is crucial for maintaining the capabilities for normal usage.

We conduct extensive experiments to prove that \texttt{R2D} is effective (by Attack Success Rate~(ASR)) in defending transferred attacks in comparison with conventional defenses on JailbreakBench~\cite{chao2024jailbreakbench}. Furthermore, we evaluate the ASR of multiple attacks against original and \texttt{R2D}-enhanced models on HarmBench~\cite{pmlr-v235-mazeika24a} to showcase that it can effectively improve the LLMs' defense capabilities. We also include XSTest~\cite{rottger-etal-2024-xstest} in our experiments to investigate whether \texttt{R2D} leads to potential over-refusal. Finally, we utilize more general datasets to assess the \texttt{R2D}-enhanced models and demonstrate that safety-aware reasoning does not lead to loss of performance for normal usage. Our contributions can be summarized as three-fold:

\begin{itemize}
    \item 
    We pioneer the safety-aware reasoning to defend LLMs against jailbreak attacks, and effectively avoid over-refusal phenomenon for normal usage while enhancing the safety of responses.
    \item 
    We present a training paradigm named \texttt{R2D}, where original non-reasoning LLMs are trained to reason using SwaRD, while also learning to detect and mitigate safety risks in the process using the proposed CPO.
    \item 
    We conduct comprehensive experiments with various attack methods, demonstrating the effectiveness of safety-aware reasoning in defending LLMs against multiple jailbreak attacks.
\end{itemize}