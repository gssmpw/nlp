\section{Related Works}

\subsection{Safety-Aware Training}
Various training-based methods have explored multiple tuning approaches, to empower LLMs or external guardrail models~\cite{inan2023llama}, to recognize unsafe inputs and responses. Constitutional AI~\cite{bai2022constitutional} adopts SFT and Reinforcement Learning from AI Feedback~(RLAIF,~\citealp{pmlr-v235-lee24t}) to enhance the safety of LLMs. Safety-tuned Llamas~\cite{bianchi2024safetytuned} explores the mixture recipes of Alpaca~\cite{taori2023alpaca} and safe-sensitive dataset to trade-off between capabilities and safety. Llama-Guard~\cite{inan2023llama} trains foundation models to follow safety principles and conduct binary discrimination of whether given messages are safe or unsafe, which serve as external guardrails in practice. RPO~\cite{zhou2024robust} regards the jailbreaks and defense on LLMs as adversarial training, training a bodyguard model to add defensive suffices to protect LLMs.

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=1.\linewidth]{figs/overview_r2d.pdf}
    \caption{Overview of the \texttt{R2D} framework. Compared to hard refusal responses, \texttt{R2D} LLMs refuse to answer with after concrete reasoning. The safety-aware reasoning process also improves the defensive performances based on the inner reasoning steps, thus reducing the possibility of generating unsafe responses.}
\label{fig:overview_r2d}
\end{figure*}

\subsection{Reasoning and LLM Safety}
Reasoning abilities benefited from CoT~\cite{wei2022chain} or process supervision training~\cite{lightman2024lets} unlocks long reasoning contexts for LLMs to think more before coming to the answers. Likewise reasoning, the Self-Refine paradigm~\cite{madaan2023selfrefine} also provides LLMs with the possibility to reflect and correct errors. In the field of safety, some works also focus on reasoning-based self-reflection, which is proved to be valid as discussed in Self-Reminder~\cite{xie2023defending} and \textit{backtracking}~\cite{zhang2025backtracking}, where LLMs critique themselves given current prompts and responses. \textbf{$\mathbb{IA}$}~\cite{zhang-etal-2025-intention} explicitly requires LLMs to conduct intention analysis on the prompts, but fails to endow them with reasoning capabilities. The reasoning is also adopted in guardrail models, such as \texttt{$\text{R}^2$-Guard}~\cite{kang2025rguard} which enhances the decision-making process of safety using probabilistic graphical models~\cite{richardson2006markov, kisa2014probabilistic}. GuardReasoner~\cite{liu2025guardreasoner} enhances the guardrails with long-trace reasoning and alleviates misjudgment.

Different from the works above, \texttt{R2D} accomplishes the decision-making process of context safety through long contextual reasoning. We focus on enhancing the LLMs' own safety through learning from the reasoning process, which also enhances the capabilities and helpfulness of LLMs.
