\section{Related Work}
\subsection{Argumentation-based Dialogues}

According to the influential work by \citet{walton1995commitment}, dialogues can be categorized based on the knowledge of the participants, the objectives they wish to achieve through the dialogue, and the rules that are intended to govern the dialogue. Contextual to each type, each dialogue revolves around a topic, typically a proposition, that is the subject matter of discussion. Related dialogue types include:~Persuasion \citep{gordon1994,prakken2006formal}, where an agent attempts to convince another agent to accept a proposition they initially do not hold; information-seeking \citep{parsons2003properties,fan2012agent}, where an agent seeks to obtain information from another agent believed to possess it; and inquiry \citep{hitchcock2017some,black2009inquiry}, where two agents collaborate to find a joint proof for a query that neither could prove individually. The advent of argumentative dialogue-based systems \cite{black2021argumentation} illustrates the great potential of argumentation for collaborative decision-making and consensus-building in human-AI interaction settings. However, these approaches often neglect the dynamic nature of belief updating during dialogues.
 
On a similar thread, our work fits well within the literature on argumentation-based explainable AI \cite{fan2015computing,shams2016normative,fan2018on,collins2019towards,budan2020proximity,dennis2022explaining,Rago_23,vasileiou_DR}. While these approaches provide a solid foundation for argumentation-based explanations, they do not explicitly focus on approximating the human users model, which is central to this paper. 

\subsection{Human Model Approximation}

Accurate human models are crucial for effective human-AI interactions. In argumentation, several approaches have emerged. \citet{rienstra2013opponent} proposed a probabilistic opponent model for move selection based on perceived awareness. \citet{hadjinikolis2013opponent} explored dialogue history analysis to predict opponent arguments. \citet{hadoux2015optimization} introduced probabilistic finite state machines and partially observable Markov decision processes for modeling dialogue progression under uncertainty.

In other domains, various approaches to human model approximation exist. Deep learning has been used to simulate and predict human behavior from large datasets \citep{HAMRICK20198,lake2017building}. Game-theoretic models reveal how agents' mental states affect choices and strategies in competitive scenarios \citep{yoshida2008game,camerer2011behavioral}. Planning formalisms have been utilized to learn human models in human-AI interaction settings \cite{sreedharan2018handling,ijcai2018-671,black2014automated}. The problem of learning human preferences has also been extensively studied, particularly in recommendation systems \cite{furnkranz2010preference}. Preferences are often elicited via ranking or comparisons \cite{ailon2012active,wirth2017survey}, or reinforcement learning paradigms \cite{wilson2012bayesian,biyik2022aprel}.

% {\color{blue}Our approach builds on prior state-of-the-art methods by \citet{hunter2015modelling,hunter2016persuasion}, which model human beliefs as probability distributions and update them dynamically in persuasion dialogues.}
 
% {\color{blue} Most relevant to our work are the works by \citet{hunter2013probabilistic, hunter2015modelling, hunter2016persuasion}, which represent a foundational effort in modeling human beliefs as probability distributions and updating them dynamically during persuasion dialogues. These methods have significantly advanced the understanding of belief modeling in argumentation, offering a robust probabilistic framework for dialogue-based persuasion. }

Most relevant to our work are those by \citet{hunter2013probabilistic, hunter2015modelling, hunter2016persuasion}, which present methods for representing and updating human beliefs through probability distributions during persuasion dialogues. While they provided essential theoretical groundwork, our approach extend them by incorporating insights from prospect theory and introducing personalized modeling capabilities that account for individual differences in probability assessment.
 
 % Most relevant to our work are the works by \citet{hunter2013probabilistic,hunter2015modelling,hunter2016persuasion}, which present methods for modeling human beliefs as probability distributions and updating them dynamically throughout persuasion dialogues. We consider these methods state of the art and compare against them in our empirical evaluations.

As our approach is specifically designed for approximating human models in argumentation-based dialogues, we compare it to the most relevant work in this space, namely the work by \citet{hunter2015modelling,hunter2016persuasion}. We do not compare it against non-argumentation approaches, as they lack the specific structures and mechanisms necessary for handling structured arguments and belief updates in dialogue settings. Our focus on argumentative reasoning and uncertainty in dialogues requires specialized techniques that these general approaches do not provide.