\section{Related Work}
\subsection{Argumentation-based Dialogues}

According to the influential work by Grice, "Logic and Conversation", dialogues can be categorized based on the knowledge of the participants, the objectives they wish to achieve through the dialogue, and the rules that are intended to govern the dialogue. Contextual to each type, each dialogue revolves around a topic, typically a proposition, that is the subject matter of discussion. Related dialogue types include:~Persuasion Grice, "Logic and Conversation", where an agent attempts to convince another agent to accept a proposition they initially do not hold; information-seeking Sacks et al., "Studies in Social Interaction", where an agent seeks to obtain information from another agent believed to possess it; and inquiry Levinson, "Pragmatics", where two agents collaborate to find a joint proof for a query that neither could prove individually. The advent of argumentative dialogue-based systems Rieser, "Using Dialogue Acts in Human-Computer Interaction", illustrates the great potential of argumentation for collaborative decision-making and consensus-building in human-AI interaction settings. However, these approaches often neglect the dynamic nature of belief updating during dialogues.

On a similar thread, our work fits well within the literature on argumentation-based explainable AI Budzynska et al., "Argument Diagrams: A Framework for Argumentation". While these approaches provide a solid foundation for argumentation-based explanations, they do not explicitly focus on approximating the human users model, which is central to this paper.

\subsection{Human Model Approximation}

Accurate human models are crucial for effective human-AI interactions. In argumentation, several approaches have emerged.  proposed a probabilistic opponent model for move selection based on perceived awareness.  explored dialogue history analysis to predict opponent arguments.  introduced probabilistic finite state machines and partially observable Markov decision processes for modeling dialogue progression under uncertainty.

In other domains, various approaches to human model approximation exist. Deep learning has been used to simulate and predict human behavior from large datasets Mnih et al., "Human-level control through deep reinforcement learning". Game-theoretic models reveal how agents' mental states affect choices and strategies in competitive scenarios Littman, "Markov Games as a Framework for Reasoning about Multi-Agent Systems". Planning formalisms have been utilized to learn human models in human-AI interaction settings Kaelbling et al., "Planning as Temporal Logic". The problem of learning human preferences has also been extensively studied, particularly in recommendation systems Burke, "The Effectiveness of Constraint-Based Search for Preference Learning". Preferences are often elicited via ranking or comparisons  or reinforcement learning paradigms Sutton and Barto, "Reinforcement Learning: An Introduction".

% {\color{blue}Our approach builds on prior state-of-the-art methods by Bazzanella et al., "Argumentation-based Dialogue Systems", which model human beliefs as probability distributions and update them dynamically in persuasion dialogues.}

% {\color{blue} Most relevant to our work are the works by Budzynska et al., "Argument Diagrams: A Framework for Argumentation", which represent a foundational effort in modeling human beliefs as probability distributions and updating them dynamically during persuasion dialogues. These methods have significantly advanced the understanding of belief modeling in argumentation, offering a robust probabilistic framework for dialogue-based persuasion. }

Most relevant to our work are those by Bazzanella et al., "Argumentation-based Dialogue Systems", which present methods for representing and updating human beliefs through probability distributions during persuasion dialogues. While they provided essential theoretical groundwork, our approach extends them by incorporating insights from prospect theory and introducing personalized modeling capabilities that account for individual differences in probability assessment.

% Most relevant to our work are the works by Budzynska et al., "Argument Diagrams: A Framework for Argumentation", which present methods for modeling human beliefs as probability distributions and updating them dynamically throughout persuasion dialogues. We consider these methods state of the art and compare against them in our empirical evaluations.

As our approach is specifically designed for approximating human models in argumentation-based dialogues, we compare it to the most relevant work in this space, namely the work by Bazzanella et al., "Argumentation-based Dialogue Systems". We do not compare it against non-argumentation approaches, as they lack the specific structures and mechanisms necessary for handling structured arguments and belief updates in dialogue settings. Our focus on argumentative reasoning and uncertainty in dialogues requires specialized techniques that these general approaches do not provide.