\section{Related Work and Background}
\label{app:sec:related-work}
\paragraph{Image-Text Models}
Large-scale pretraining has revolutionized the field of image-text models, enabling significant advances. Models such as CoCa~\citep{yu2022coca} and SimVLM~\cite{wang2022simvlm}, which are trained from scratch on billions of image-text pairs, have set new benchmarks in generative tasks such as open-ended visual question answering (VQA) and visual captioning. BLIP-2 addresses the computational demands of pretraining from scratch by reusing existing pre-trained parameters from Vision Transformer (ViT) and LLMs and integrating them with a frozen pre-trained state. A key innovation in BLIP-2 is the introduction of the Q-former connector, carefully designed to enhance the interaction between visual and language modalities \citep{li2023blip}. This methodology has inspired subsequent innovations in visual-lingual tuning, with newer models often incorporating the pre-trained Q-former alongside the \texttt{eva-vit-g} model from BLIP-2, demonstrating the lasting impact of this methodology \citep{instructblip, zhu2023minigpt, yang2024cliperase, 2023videochat}.

\paragraph{Video-Text Models}
Video-text models typically extend the capabilities of image-text models by integrating temporal feature aggregation to capture dynamic content, as exemplified by VideoCoCa \citep{yan2022videotext}. In addition, specialized models such as Video-LLaMA enhance the processing of temporal dynamics by embedding multiple temporal Q-former layers, facilitating nuanced interactions across modalities. Such advances refine the synergy between video Q-formers and LLMs within the model architecture, building on the foundation of BLIP-2 \citep{damonlpsg2023videollama}. Building on these developments, recent studies, including VideoChat, PandaGPT, Valley, and Video-ChatGPT, investigate the embedding of frozen LLMs into video LMs, pushing the boundaries of the field \citep{2023videochat, su2023pandagpt, luo2023valley, Maaz2023VideoChatGPT}. In our study, we use BLIP-2 as a basic model for captioning, first pre-trained on images and then adapted to video by incorporating a video frame merging mechanism that effectively captures temporal nuances. This simplicity allows us to focus on evaluating the effects of model size, data volume, and training strategies on video captioning performance as we scale. 

\paragraph{Difference between Image and Video Captioning}
The fundamental difference between image and video annotation stems from their source inputs: image annotation processes a single static image, while video annotation requires an understanding of the temporal dynamics over a sequence of frames. When adapted to video, pre-trained image models such as GIT~\citep{wang2022git}, VideoCoCa~\citep{yan2022videotext}, and IcoCap~\citep{liang2023icocap} show remarkable adaptability to video with only moderate modifications, demonstrating their transferability. Conversely, video-specific models, including Video-LLaMA~\citep{damonlpsg2023videollama} and VideoChat \citep{2023videochat}, use different sampling techniques to effectively capture temporal dynamics. Furthermore, models such as ALPRO \citep{li2021alignprompt} and VIOLET \citep{fu2023empiricalmvm} utilize extensive web-crawled datasets to achieve end-to-end training, enriching their learning process. In our study, instead of emulating the complex adaptations typical of specialized video models, we adopt a streamlined approach that uses averaging or concatenation to merge temporal information from sampled video frames. This method allows us to focus on evaluating the effects of model size, data volume, and training strategies on video captioning performance as we scale.