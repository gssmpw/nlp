The rapid advancement of Large Vision-Language Models (VLMs) has spurred significant debate regarding their capacity to achieve human-level cognition. These models are increasingly deployed as general reasoning systems capable of addressing complex problems across diverse domains, with applications extending into dynamic, real-world scenarios such as physical agent-based tasks and planning \cite{DBLP:journals/corr/abs-2406-14852, xi2023rise, zeng2023large}. However, critical gaps persist in their spatial reasoning and visual alignment capabilities, areas essential for understanding, interpreting, and manipulating objects and their spatial relationships \cite{DBLP:conf/emnlp/KamathHC23a, DBLP:journals/corr/abs-2405-17247, DBLP:journals/corr/abs-2411-00238}.

Spatial reasoning, a foundational aspect of problem-solving, navigation, and interaction with the physical world, requires models to bridge vision and cognition by interpreting visual information to understand spatial arrangements. Tasks such as mentally rotating shapes, predicting object movement, and recognizing patterns exemplify the importance of visual-spatial reasoning. Despite these critical requirements, progress in VLMs has been hampered by evaluation benchmarks that fail to capture the dynamic and multi-step complexity of real-world spatial reasoning. Existing benchmarks predominantly rely on static, text- or image-based setups that often oversimplify spatial contexts, focusing on 2D environments without interactivity or dynamic problem-solving capabilities. This limitation perpetuates a lack of meaningful progress in visual-spatial reasoning.

\textbf{Contributions.}\hspace{1em} To bridge this gap, we introduce iVISPAR (Interactive Visual-Spatial Reasoning), a novel benchmark designed to systematically evaluate VLMs as agents in dynamic environments. iVISPAR is built around the sliding tile puzzle, a well-established problem in developmental psychology that demands logical planning, spatial awareness, and multi-step problem-solving. As part of our contributions, we introduce the Sliding Geom Puzzle, a variant that replaces traditional numbered tiles with geometric objects distinguished by their color and shape, adding an additional layer of visual reasoning.

Notably, iVISPAR is grounded in a well-studied, formalized problem with access to optimal solutions, ensuring a robust framework for evaluation.\footnote{The formalization is achieved through the adaptation of the sequential generalized sliding-tile puzzle, as described in Section~\ref{slidingtilepuzzle}. Optimal solutions are computed using the A* algorithm, detailed in Section \ref{sec:baselines}.} The benchmark supports scalable task complexity by adjusting factors such as board size, the number of tiles, and solution paths, ranging from simple configurations to NP-complete challenges that surpass baseline human performance.

Leveraging a prompt-based API, iVISPAR enables VLMs to interact with a simulated environment through an iterative action-perception loop. Experimentation results demonstrate that while state-of-the-art VLMs can handle basic spatial reasoning tasks, they face significant difficulties with more complex scenarios, especially in 3D environments. By contrasting their performance against optimal solutions and human baselines, we highlight the persistent gap between current VLM capabilities and human-level spatial reasoning.

Our contributions are threefold: (i) a novel interactive benchmark that systematically evaluates visual-spatial reasoning in VLMs; (ii) a scalable task design rooted in a formalized problem with optimal solutions; and (iii) empirical insights into the strengths and limitations of VLMs across varying task complexities and modalities. iVISPAR lays the foundation for advancing VLM research toward overcoming critical gaps in reasoning and alignment capabilities.
