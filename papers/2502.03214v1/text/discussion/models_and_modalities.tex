Our results show that VLMs perform worst on 3D vision, with performance generally close to the random baseline, highlighting the heightened difficulty of processing spatial relationships in three dimensions (Figure \ref{fig:results_bar_plot}). Environments with 2D vision are more manageable for VLMs compared to both 3D vision and text-based spatial reasoning, likely due to finetuning on 2D spatial reasoning tasks during training. Sonnet-3.5 consistently outperforms other VLMs across all modalities, while GPT-4o stands out as an outlier, performing better in 3D and 2D vision than in text-based environments. Figure \ref{fig:stacked_bar_plot_win_percentage} ranks models by overall performance, showing that closed-source models outperform open-source models. 

Performance challenges also differ between modalities: while vision struggles with high geometric complexity but handles sequential reasoning better, text-based representations struggle with long sequences but are relatively unaffected by the number of geoms on the board (Figure \ref{fig:cumulative_heatmap}).