Recent advancements in Vision-Language Models (VLMs) have prompted a surge in evaluations, yet many lack comprehensive benchmarking approaches. Existing studies primarily rely on question-answering tests or visual-spatial reasoning tasks, highlighting the need for more systematic evaluations tailored to the unique strengths of VLMs. For instance, \citet{DBLP:journals/corr/abs-2406-14852} proposed SpatialEval, a benchmark encompassing diverse spatial reasoning tasks such as relationship understanding, navigation, and counting. Their findings reveal significant challenges for both language and vision-language models, with VLMs often underperforming compared to LLMs when textual clues are sufficient. Similarly, \citet{DBLP:journals/corr/abs-2410-16162} introduced Sparkle, a dataset focusing on core 2D spatial capabilities—direction comprehension, distance estimation, and localization—with fine-tuning experiments showing improved performance on composite spatial reasoning tasks. However, these works are primarily constrained to 2D environments and do not incorporate agency or explore advanced multi-modal comparisons in 3D scenarios.

Several benchmarks in non-VLM domains focus on physical simulation or interactive environments, offering insights into related tasks but presenting challenges for VLM evaluation. For example, ThreeDWorld \cite{DBLP:journals/corr/abs-2310-03249} provides a high-fidelity physics environment with tasks emphasizing spatial-temporal reasoning, such as path planning. However, its interaction framework is overly complex, and the lack of a dedicated language API limits its suitability for VLM evaluation. \citet{DBLP:journals/corr/abs-2308-09778} proposed GSR-BENCH, a benchmark evaluating spatial relationships using multimodal models, but their work primarily focuses on grounded relationships and does not extend to agent-based tasks or dynamic reasoning.

Benchmarks targeting VLMs often prioritize tasks that do not explicitly focus on spatial reasoning. For instance, \citet{DBLP:journals/tmlr/YamadaBLKY24} evaluated LLMs on textual representations of spatial relationships and navigation tasks across various grid structures, revealing gaps in their implicit understanding of spatial structures. Similarly, \citet{DBLP:conf/acl/RizviZG24} introduced SpaRC and SpaRP, which focus on textual spatial reasoning chains and path generation. While these works highlight interesting patterns in reasoning, they lack the integration of visual modalities necessary for real-world multimodal tasks.

Path planning benchmarks, such as those presented in \citet{DBLP:journals/corr/abs-2310-03249}, test spatial-temporal reasoning using textual inputs, with visual components relegated to supplementary material. Other works, such as \citet{DBLP:conf/emnlp/ZhangCHW0THHM0Z24}, leverage synthetic abstract images for visual reasoning but fail to explore spatial agency or complex multi-step planning. Earlier multimodal benchmarks, like ShapeWorld \cite{DBLP:journals/corr/KuhnleC17}, demonstrated the value of synthetic data for controlled evaluations but were limited to abstract tasks and did not address dynamic interactions or real-world spatial reasoning.

Despite these contributions, key gaps remain. Few studies systematically investigate the interplay between textual and visual reasoning in dynamic environments, and evaluations of detailed, multi-step planning are rare. While benchmarks such as SpatialRGPT \cite{DBLP:journals/corr/abs-2406-13246} introduced methods for integrating 3D spatial information into VLMs via depth cues, they lack interactivity and agency. Similarly, \citet{DBLP:journals/corr/abs-2410-16162} demonstrated that fine-tuning on core 2D spatial tasks improves generalization but did not extend to 3D or agent-based reasoning. Multimodal Self-Instruct \cite{DBLP:conf/emnlp/ZhangCHW0THHM0Z24} exposed gaps in abstract image reasoning through synthetic datasets but did not evaluate models' ability to reason dynamically in grounded settings.

% This work seeks to address these shortcomings by introducing iVISPAR (Interactive Visual-Spatial Reasoning), a benchmark that integrates vision and text modalities with interactive, agent-based tasks in 3D environments. Unlike existing works, iVISPAR enables multi-modal evaluations that incorporate agency, planning, and dynamic reasoning, offering a new framework to systematically evaluate the spatial reasoning capabilities of VLMs.
iVISPAR builds around the sliding-tile puzzle, which has been established as a foundational testbed for spatial reasoning by recent works. \citet{DBLP:journals/corr/abs-2410-14038} introduced Sliding Puzzles Gym, extending the puzzle to varying grid sizes and observation spaces to evaluate representation learning in reinforcement learning agents. \citet{DBLP:conf/aaai/GozonY24} analyzed generalized sliding-tile puzzles, providing theoretical insights into NP-complete solutions and approximation algorithms. While these works focus on reinforcement learning and theoretical problem-solving, our approach builds on their foundations by integrating multimodal inputs, agency, and interaction, enabling systematic evaluation of spatial reasoning and planning in vision-language models.