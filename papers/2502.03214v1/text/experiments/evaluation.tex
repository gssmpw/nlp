Agent performance is evaluated through two primary metrics: the fraction of solved environments and mean step-deviation from the optimal path 

\textbf{Mean step-deviation from optimal path} measures the deviation from optimal behavior during problem-solving. At each step \( t \), the shortest path solution from the current board state to the goal, computed by A*, is used to assess how efficiently the agent progresses. Formally, step-deviation from optimal path is expressed as:  $R(t) = d(s_t, s^*) - d(s_0, s^*)$ where \( d(s, s^*) \) represents the shortest path from state \( s \) to the goal \( s^* \). This metric quantifies how much further the agent remains from the goal compared to the initial state. A regret value of zero indicates that the agent follows an optimal trajectory, while positive regret reflects inefficiencies or unnecessary detours. By capturing performance even in unsolved environments, this approach provides insights into agent behavior under varying complexities.

To gain deeper insights, we analyze the most common error patterns exhibited by agents. This allows us to identify model weaknesses, recurring failure cases, and patterns of suboptimal decision-making.

