
\begin{table*}[!t]
\small
\centering
\begin{tabular}{@{}l|cc|ccc|cc|c@{}}
\toprule
\bfseries Model & \bfseries \#Params & \bfseries Training Data & \bfseries SIM-O$\uparrow$ & \bfseries SIM-R$\uparrow$ & \bfseries WER$\downarrow$ & \bfseries CMOS$\uparrow$& \bfseries SMOS$\uparrow$ & \bfseries RTF$\downarrow$ \\      
\midrule
GT & - & - & 0.68 & - & 1.94\% & +0.12 & 3.92 & - \\
\midrule
VALL-E 2$^{*}$ & 0.4B & LibriHeavy & 0.64 & 0.68 & 2.44\% & - & - & - \\
VoiceBox$^{\dag}$ & 0.4B & Collected (60kh) &  0.64 & 0.67 & 2.03\% & -0.20 & 3.81 & 0.340 \\
DiTTo-TTS$^{*}$ & 0.7B & Collected (55kh) & 0.62 & 0.65 & 2.56\% & - & - & - \\
% \textit{MELLE}$^{*}$ & LibriHeavy & 0.63 & 0.66 & 2.10\% & - & - &\\
NaturalSpeech 3$^{\dag}$ & 0.5B & LibriLight & 0.67 & 0.76 & \bfseries 1.81\% & -0.10 & 3.95 & 0.296\\
CosyVoice & 0.4B & Collected (172kh) & 0.62 & - & 2.24\% & -0.18 & 3.93 & 1.375\\
MaskGCT & 1.0B & Emilia (100kh) & 0.69 & - & 2.63\% & - & - & - \\
F5-TTS & 0.3B & Emilia (100kh) & 0.66 & - & 1.96\% & -0.12 & 3.96 & 0.307 \\
\midrule
MegaTTS 3 & 0.3B & LibriLight & \bfseries 0.71 & \bfseries 0.78 & \bfseries 1.82\%  & \bfseries 0.00 & \bfseries 3.98 & 0.188 \\
MegaTTS 3-accelerated & 0.3B & LibriLight & 0.70 & \bfseries0.78 & 1.86\%  & -0.03  & 3.96 & \bfseries 0.124 \\

\bottomrule
\end{tabular}
\caption{Zero-shot TTS results on the LibriSpeech test-clean set following NaturalSpeech 3~\citep{ju2024naturalspeech}. $^{*}$ means the results are obtained from the paper. $^{\dag}$ means the
results are obtained from the authors. \#Params denotes the number of parameters. RTF denotes the real-time factor.}
\label{table:en_zs_tts}
\end{table*}

\begin{table}[!t]
\small
\centering
\begin{tabular}{@{}l|ccc@{}}
\toprule
\bfseries Model & \bfseries \#Params & \bfseries SIM-O$\uparrow$ & \bfseries WER$\downarrow$  \\       
\midrule
GT              & - &  0.69 & 2.23\%   \\
\midrule
CosyVoice       & 0.3B & 0.66 & 3.59\% \\
E2 TTS          & 0.3B & 0.69 & 2.95\% \\
F5-TTS          & 0.3B & 0.66 & 2.42\% \\
\midrule
MegaTTS 3           & 0.3B & \bfseries 0.70 & \bfseries 2.31\% \\
\bottomrule
\end{tabular}
\caption{Zero-shot TTS results on the LibriSpeech-PC test-clean set following F5-TTS~\citep{chen2024f5}. \#Params denotes the number of parameters.}
\label{table:en_zs_tts_valle}
\end{table}

\section{Experiments} 
In this subsection, we describe the datasets, training, inference, and evaluation metrics. We provide the model configuration and detailed hyper-parameter setting in Appendix~\ref{app:model_config}.

\subsection{Experimental setup}
\label{Experimental_Setup}
\paragraph{Datasets.} We train MegaTTS 3 on the LibriLight~\citep{kahn2020libri} dataset, which contains 60k hours of unlabeled speech derived from LibriVox audiobooks. All speech data are sampled at 16KHz. We transcribe the speeches using an internal ASR system and extract the predefined speech-text alignment using the external alignment tool~\citep{mcauliffe2017montreal}. We utilize three benchmark datasets: 1) the librispeech~\citep{panayotov2015librispeech} test-clean set following NaturalSpeech 3~\citep{ju2024naturalspeech} for zero-shot TTS evaluation; 2) the LibriSpeech-PC test-clean set following F5-TTS~\citep{chen2024f5} for zero-shot TTS evaluation; 3) the L2-arctic dataset~\citep{zhao2018l2arctic} following~\citep{melechovsky2022accented,liu2024controllable} for accented TTS evaluation.

\paragraph{Training and Inference.} 
We train the WaveVAE model and MegaTTS 3 on 8 NVIDIA A100 GPUs. The batch sizes, optimizer settings, and learning rate schedules are described in Appendix~\ref{app:model_config}. It takes 2M steps for the WaveVAE model's training and 1M steps for MegaTTS 3's training until convergence. The pre-training of MegaTTS 3 requires 800k steps and PeRFlow distillation requires 200k steps.
\paragraph{Objective Metrics.} 1) For zero-shot TTS, we evaluate speech intelligibility using the word error rate (WER) and speaker similarity using SIM-O~\citep{ju2024naturalspeech}. To measure SIM-O, we utilize the WavLM-TDCNN speaker embedding model\footnote{\url{https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification}} to calculate the cosine similarity score between the generated samples and the prompt. As SIM-R~\citep{le2023Voicebox} is not comparable across baselines using different acoustic tokenizers, we recommend focusing on SIM-O in our experiments. The similarity score is in the range of $\left[-1,1\right]$, where a higher value indicates greater similarity. In terms of WER, we use the publicly available HuBERT-Large model~\citep{hsu2021hubert}, fine-tuned on the 960-hour LibriSpeech training set, to transcribe the generated speech. The WER is calculated by comparing the transcribed text to the original target text. All samples from the test set are used for the objective evaluation; 2) For accented TTS, we evaluate the Mel Cepstral Distortion (MCD) in dB level and the moments (standard deviation ($\sigma$), skewness ($\gamma$) and kurtosis ($\kappa$))~\citep{andreeva2014differences,niebuhr2019measuring} of the pitch distribution to evaluate whether the model accurately captures accent variance. 

\paragraph{Subjective Metrics.} We conduct the MOS (mean opinion score) evaluation on the test set to measure the audio naturalness via Amazon Mechanical Turk. We keep the text content and prompt speech consistent among different models to exclude other interference factors. We randomly choose 40 samples from the test set of each dataset for the subjective evaluation, and each audio is listened to by at least 10 testers. We analyze the MOS in three aspects: CMOS (quality, clarity, naturalness, and high-frequency details), SMOS (speaker similarity in terms of timbre reconstruction and prosodic pattern), and ASMOS (accent similarity). We tell the testers to focus on one corresponding aspect and ignore the other aspect when scoring.


\subsection{Results of Zero-Shot Speech Synthesis}
\label{exp_zero-shot}


\paragraph{Evaluation Baselines.} We compare the zero-shot speech synthesis performance of MegaTTS 3 with 11 strong baselines, including: 1) VALL-E 2~\citep{chen2024vall}; 2) VoiceBox~\citep{le2023Voicebox}; 3) DiTTo-TTS~\citep{lee2024ditto}; 4) NaturalSpeech 3~\citep{ju2024naturalspeech}; 5) CosyVoice~\citep{du2024cosyvoice}; 6) MaskGCT~\citep{wang2024maskgct}; 7) F5-TTS~\citep{chen2024f5}; 8) E2 TTS~\citep{eskimez2024e2}. Explanation and details of the selected baseline systems are provided in Appendix~\ref{app:detail_zs_tts_baseline}.



\paragraph{Analysis} 
As shown in Table~\ref{table:en_zs_tts}, we can see that 1) MegaTTS 3 achieves state-of-the-art SIM-O, SMOS, and WER scores, comparable to NaturalSpeech 3 (the counterpart with forced alignment), and significantly surpasses other baselines without explicit alignments. The improved SIM-O and SMOS suggest that the proposed sparse alignment effectively simplifies the text-to-speech mapping challenge like predefined forced duration information, allowing the model to focus more on learning timbre information. And the improved WER indicates that MegaTTS 3 also enjoys strong robustness; 2) MegaTTS 3 significantly surpasses all baselines in terms of CMOS, demonstrating the effectiveness of the proposed sparse alignment strategy; 3) After the PeRFlow acceleration, the student model of MegaTTS 3 shows on par quality with the teacher model and enjoys fast inference speed. We also conduct the experiments on the LibriSpeech-PC test-clean set provided by F5-TTS and the results are shown in Table~\ref{table:en_zs_tts_valle}, which also demonstrates that our method achieves state-of-the-art performance in terms of speaker similarity and speech intelligibility. The duration controllability of MegaTTS 3 is verified in Appendix~\ref{app:dur_contol}. In the demo page, we also demonstrate that our method can maintain high naturalness even when the performance of the duration predictor is suboptimal (while MegaTTS 3 with forced alignment fails).


\begin{table*}[!t]
\small
\centering
\begin{tabular}{@{}l|cccc|ccc@{}}
\toprule
\bfseries Model & \bfseries MCD (dB) $\downarrow$ & \bfseries $\sigma$ $\uparrow$ & \bfseries $\gamma$ $\downarrow$ & \bfseries $\kappa$ $\downarrow$ & \bfseries ASMOS $\uparrow$ & \bfseries CMOS $\uparrow$ & \bfseries SMOS $\uparrow$ \\       
\midrule
GT      &   - & 45.1 & 0.591 & 0.783 & 4.03 & +0.09 & 3.95 \\

CTA-TTS & 5.98  & 41.1 & 0.602 & 0.799 & 3.72 & -0.60 & 3.64 \\

MegaTTS 3   & \bfseries 5.69 & \bfseries 42.3  & \bfseries 0.601  & \bfseries 0.790  & \bfseries 3.84 & \bfseries +0.00 & \bfseries 3.89 \\

\bottomrule
\end{tabular}
\caption{The objective and subjective experimental results for accented TTS. MCD (dB) denotes the Mel Cepstral Distortion at the dB level. $\sigma$, $\gamma$, and $\kappa$ are the standard deviation, skewness, and kurtosis of the pitch distribution.}
\label{table:accent-tts-result}
\end{table*}



\begin{table}[!t]
\small
\centering
\begin{tabular}{@{}l|cccccc@{}}
\toprule
\bfseries Method & \bfseries MCD$\downarrow$ & \bfseries GPE$\downarrow$ & \bfseries VDE$\downarrow$ & \bfseries FFE$\downarrow$ \\       
\midrule
NaturalSpeech 3      &  4.45  & 0.44 & 0.33 & 0.37\\
Ours w/ F.A. &  4.48   & 0.44 & 0.35 & 0.40 \\
\midrule
Ours w/ S.A. & \bfseries 4.42 & \bfseries 0.31 & \bfseries 0.29 & \bfseries 0.34 \\

\bottomrule
\end{tabular}
\caption{Comparisons about prosodic naturalness metrics on LibriSpeech test-clean set. ``F.A.'' denotes forced alignment and ``S.A.'' denotes sparse alignment.}
\label{table:expressiveness_exp_40}
\end{table}

\subsection{Experiments of Prosodic Naturalness}
\label{exp:prosodic_naturalness}
We also measure the objective metrics MCD, SSIM, STOI, GPE, VDE, and FFE following InstructTTS~\citep{yang2024instructtts} to evaluate the prosodic naturalness of our method. The results are presented in Table~\ref{table:expressiveness_exp_40}. Specifically, our method with sparse alignment (Ours w/ S.A.) achieves the best performance across all metrics, with an MCD of 4.42, GPE of 0.31, VDE of 0.29, and FFE of 0.34. These results indicate a significant improvement in prosodic naturalness compared to the baseline NaturalSpeech 3 and our method with forced alignment (Ours w/ F.A.), further validating the effectiveness of our sparse alignment strategy. Our method provides a noval and effective solution for speech synthesis applications that require high robustness and exceptional expressiveness, such as audiobook narration and virtual assistants.

\subsection{Results of Accented TTS}
In this subsection, we evaluate the accented TTS performance of our model on the L2-ARCTIC dataset~\citep{zhao2018l2arctic}. This corpus includes recordings from non-native speakers of English whose first languages are Hindi, Korean, etc. In this experiment, we focus on verifying whether our model and baseline can synthesize natural speech with different accent types (standard English or English with specific accents) while maintaining consistent vocal timbre. We compare our MegaTTS 3 model with CTA-TTS~\citep{liu2024controllable}. More details of the baseline model are provided in Appendix~\ref{app:detail_accented_tts_experiment}. 1) First, we evaluate whether the models can synthesize high-quality speeches with accents. As shown in Table~\ref{table:accent-tts-result}, our MegaTTS 3 model significantly outperforms the CTA-TTS baseline in terms of the subjective accent similarity MOS core, the MCD (dB) values, and the statistical moments ($\sigma$, $\gamma$, and $\kappa$) of pitch distributions. These results demonstrate the superior accent learning capability of MegaTTS 3 compared to the baseline system. Besides, the MegaTTS 3 model achieves higher CMOS and SMOS scores compared to CTA-TTS, indicating a significant improvement in speech quality and speaker similarity; 2) Secondly, we evaluate whether the models can accurately control the accent types of the generated speeches. We follow CTA-TTS to conduct the intensity classification experiment~\citep{liu2024controllable}. At
run-time, we generate speeches with two accent types, and the listeners are instructed to classify the perceived accent categories, including ``standard'' and ``accented''. Figure~\ref{exp:accent_confusion_matrices} shows that our MegaTTS 3 significantly surpasses CTA-TTS in terms of accent controllability.


\begin{figure}[!t]
  \centering
    \includegraphics[scale=0.50]{figures/vis/accent_matrix.pdf}
  \caption{The confusion matrices between the perceived and intended accent categories of synthesized speech. The X-axis and Y-axis represent the intended and perceived categories, respectively.}
  \label{exp:accent_confusion_matrices}
\end{figure}





\begin{table*}[!ht]
\centering
\small
\begin{tabular}{@{}l|ccccccccc@{}}
\toprule
\textbf{Models} & \textbf{Tokens/s} & \textbf{Latent Layer} & \textbf{Type} & \textbf{PESQ$\uparrow$} & \textbf{STOI$\uparrow$} & \textbf{ViSQOL$\uparrow$} & \textbf{MCD$\downarrow$} & \textbf{UTMOS$\uparrow$} \\
\midrule
Encodec &  600 & 8 & Discrete & 3.16 & 0.94 & 4.31 & 1.63 & 3.07 \\
DAC &  450 & 9 & Discrete & \textbf{4.13} & \textbf{0.97} & \underline{4.68} & \underline{1.05} & 4.01 \\
WavTokenizer &  75 & 1 & Discrete & 2.55 & 0.88 & 3.83 & 1.99 & 4.07 \\
X-codec2 &  50 & 1 & Discrete & 3.03 & 0.91 & 4.12 & 1.72 & \textbf{4.13} \\
\midrule
WaveVAE & 25 & 1 & Continuous & \underline{3.84} & \underline{0.96} & \textbf{4.71} & \textbf{1.03} & \underline{4.10} \\
\bottomrule
\end{tabular}
\caption{Comparison of the reconstruction quality. The sampling rate are set to 16 kHz.  \textbf{Bold} and \underline{Underline} values indicate the best and second best results. ``Tokens/s'' means how many tokens a one-second speech will be compressed into.}
\label{app:table_recon_speech_compression}
\end{table*}

\begin{table}[!ht]
\small
\centering
\begin{tabular}{@{}l|cc@{}}
\toprule
\bfseries Setting & \bfseries SIM-O$\uparrow$ & \bfseries WER$\downarrow$ \\       
\midrule
Ours   & \bfseries 0.71           & \bfseries 1.82\%                 \\
\midrule
\textit{w/ Encodec}  & 0.56           & 2.24\%                 \\
\textit{w/ DAC}  &  0.64 & 1.93\%        \\
\bottomrule
\end{tabular}
\caption{Comparison of zero-shot TTS performance of MegaTTS 3 using different speech compression models on the LibriSpeech test-clean set.}
\label{app:table_zs_tts_different_codec}
\end{table}

\subsection{Evaluation of WaveVAE}
\label{app:evaluation_speech_compression}
First, we evaluate the reconstruction quality of the WaveVAE model, with results presented in Table~\ref{app:table_recon_speech_compression}. We report the objective metrics, including Perceptual Evaluation of Speech Quality (PESQ), Virtual Speech Quality Objective Listener (ViSQOL), and Mel-Cepstral Distortion (MCD). We select the following codec models as baselines: 1) EnCodec~\citep{defossez2022high}, a representative and pioneering work in the field of speech codec; 2) DAC~\citep{kumar2024high}, a high-bitrate audio codec model with high reconstruction quality; 3) WavTokenizer~\citep{ji2024wavtokenizer}, a low-bitrate speech codec model that focuses more on perceptual reconstruction quality; 4) X-codec2~\citep{ye2025llasa}, a low-bitrate speech codec model, leveraging the representations of a pre-trained model to further enhance overall quality. The results demonstrates that, despite applying higher compression rate, our solution achieves superior performance on various reconstruction metrics, such as MCD and ViSQOL. 

Second, to demonstrate the impact of different speech compression models on the overall performance of the TTS system, we extracted the latents from Encodec and DAC, respectively, for training our MegaTTS 3 model. We report the experimental results in Table~\ref{app:table_zs_tts_different_codec}. It can be seen that our method outperforms ``w/ DAC'' and ``w/ Encodec'', due to the fact that the latent space of our speech compression model is more compact (only 25 tokens per second). The results demonstrate the importance of our WaveVAE, a high-compression, high-reconstruction-quality speech codec model, for TTS systems. This conclusion is also verified by a previous work~\citep{lee2024ditto}, which shows compact target latents facilitate learning in diffusion models.


\subsection{Ablation Studies}
\label{exp:ablation_studies}
\begin{table}[!t]
\small
\centering
\begin{tabular}{@{}l|cc|cc@{}}
\toprule
\bfseries Setting & \bfseries SIM-O$\uparrow$ & \bfseries WER$\downarrow$ & \bfseries CMOS$\uparrow$ & \bfseries SMOS$\uparrow$ \\       
\midrule
Ours & 0.71 & 1.82$\%$ & 0.00 & 3.94 \\
\midrule
\textit{w/ F.A.}  & 0.70 & 1.80\% & -0.17 & 3.94 \\
\textit{w/o A.}        & 0.67 & 2.14\% & -0.12 & 3.88 \\
\midrule
\textit{w/ CFG}       & 0.68 & 1.79\% & -0.02 & 3.89 \\
\textit{w/o CFG}              & 0.43 & 6.85\% & -0.56 & 3.35  \\
\bottomrule
\end{tabular}
\caption{Ablation studies of alignment strategies and CFG mechanisms on the LibriSpeech test-clean set.}
\label{table:ablation_alignments_cfg}
\end{table}

We test the following four settings: 1) \textit{w/ FA}, which replaces the sparse alignment in MegaTTS 3 with forced alignment used in ~\citep{le2023Voicebox,shen2023naturalspeech}; 2) \textit{w/o A.}, we do not use the predefined alignments and modeling the duration information implicitly; 3) \textit{w/ CFG}, we use the standard CFG following the common practice in Diffusion-based TTS; 4) \textit{w/o CFG}, we do not use the CFG mechanism. All tests follow the experimental setup described in Section~\ref{exp_zero-shot}. The results are shown in Table~\ref{table:ablation_alignments_cfg}. For settings 1) and 2), it can be observed that both forced alignment and sparse alignment can enhance the performance of speech synthesis models. However, compared to forced alignment, sparse alignment does not constrain the model's search space, leading to a prosodic naturalness (see Section~\ref{exp:prosodic_naturalness}). Therefore, the sparse alignment strategy achieves $+0.17$ CMOS compared to the forced alignment strategy. For setting 3), compared with the standard CFG, our multi-condition CFG performs slightly better as it allows for flexible control over the weights between the text prompt and the speaker prompt. Setting 4) proves that the CFG mechanism is crucial for MegaTTS 3. Additionally, we visualize the attention score matrices from different transformer layers in MegaTTS 3 in Appendix~\ref{app:vis_diff_attn}, leading to some interesting observations.


% \begin{figure*}[t]
% \centering
% \begin{minipage}{0.45\linewidth}
%     \centering
% \includegraphics[width=1\linewidth]{figures/vis/training_procedure_simo.pdf}
% \end{minipage}
% \centering
% \hspace{1.0em} 
% \begin{minipage}{0.45\linewidth}
%     \centering
%     \includegraphics[width=1\linewidth]{figures/vis/training_procedure_wer.pdf}
% \end{minipage}
% \centering
% \caption{The visualization for effects of different speech-text alignment strategies on MegaTTS 3 training.}
% \label{exp:vis_training_process}
% \end{figure*}