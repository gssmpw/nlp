\appendix

\section{Detailed Experimental Settings}

\subsection{Model Configuration}
\label{app:model_config}

\begin{itemize}
\item \textbf{The WaveVAE model} consists of a VAE encoder, a wave decoder, and discriminators; The VAE encoder follows the architecture used in~\citep{ji2024wavtokenizer}. The wave decoder consists of a convolutional upsampler and a Hifi-GAN decoder~\citep{kong2020hifi}. The latent channel size is set to 32. The weight of the KL loss is set to $1\times10^{-3}$, which only imposes a slight KL penalty on the learned latent. In training, we use batches of fixed length, consisting of 72,000 wavform frames, with a batch size set to 40 for each GPU. We use the Adam optimizer with a learning rate of $1\times10^{-4}$, $\beta_{1} = 0.9$, $\beta_{2} = 0.999$, and 10K warmup steps.

\item \textbf{The MegaTTS 3 model} use the standard transformer block from LLAMA~\citep{dubey2024llama} as the basic structure, which comprises a 24-layer Transformer with 16 attention heads and 1024 embedding dimensions. It contains 339M parameters in total. We adopt the Rotary Position Embedding (RoPE)~\citep{su2024roformer} as the positional embedding following the common practice in LLAMA implementations. For simplicity, we do not use the phoneme encoder and style encoder like previous works. We only use a linear projection layer to transform these features to the same dimension. During training, we use 8 A100 80GB GPUs with a batch size of 10K latent frames per GPU for 1M steps. We use the Adam optimizer with a learning rate of $5\times10^{-5}$, $\beta_{1} = 0.9$, $\beta_{2} = 0.999$, and 10K warmup steps. In zero-shot TTS experiments, we set the text guidance scale $\alpha_{txt}$ and the speaker guidance scale $\alpha_{spk}$ to 2.5 and 3.5, respectively. In accented TTS experiments, we set $\alpha_{spk}=6.5$, $\alpha_{txt}=1.5$ to generate the accented speech and set $\alpha_{spk}=2.0$, $\alpha_{txt}=5.0$ to generate the speech with standard English.

\end{itemize}



\subsection{Random Seeds}
\label{app:random_seeds}
We ran objective experiments 10 times with 10 different random seeds and obtained the averaged results. The chosen random seeds are [4475, 5949, 6828, 6744, 3954, 3962, 6837, 1237, 3824, 3163].


\subsection{Sampling Strategy}
For MegaTTS 3, we applied the Euler sampler with a fixed step size following the common practice in flow ODE sampling. We use 25 and 8 sampling steps for \textit{MegaTTS 3} and \textit{MegaTTS 3-accelerated}, respectively.

\subsection{Details about Zero-Shot TTS Baselines}
\label{app:detail_zs_tts_baseline}
In this subsection, we provide the details about the baselines in our zero-shot TTS experiments:

\begin{itemize}

\item \textbf{VALL-E 2}~\citep{chen2024vall}, based on VALL-E, introduces Repetition Aware Sampling to stabilize the decoding process and proposes the Grouped Code Modeling to effectively address the challenges of long sequence modeling.

\item \textbf{VoiceBox}~\citep{le2023Voicebox} is a non-autoregressive flow-matching model designed to infill mel-spectrograms based on provided speech context and text. We obtained the samples by contacting the authors.


\item \textbf{DiTTo-TTS}~\citep{lee2024ditto} addresses
the challenge of text-speech alignment via cross-attention mechanisms with the prediction of the total length of speech representations. We directly obtain the results of objective evaluations from their paper.

\item \textbf{NaturalSpeech 3}~\citep{ju2024naturalspeech} designs a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details and propose a factorized diffusion model to generate attributes in each subspace following its corresponding prompt. We obtained the samples by contacting the authors.

\item \textbf{CosyVoice}~\citep{du2024cosyvoice} utilizes an LLM for text-to-token generation and a conditional flow matching model for token-to-speech synthesis. We use the official code and the model snapshot named ``CosyVoice-300M'' in our experiments\footnote{\url{https://github.com/FunAudioLLM/CosyVoice}}.


\item \textbf{MaskGCT}~\citep{wang2024maskgct} proposes a fully non-autoregressive codec-based TTS model that eliminates the need for explicit alignment information between text
and speech supervision, as well as phone-level duration prediction. We directly obtain the results of objective evaluations from their paper.

\item \textbf{F5-TTS}~\citep{chen2024f5} proposes a fully non-autoregressive text-to-speech system
based on flow matching with Diffusion Transformer (DiT). We use the official code and pretrained model in our experiments\footnote{\url{https://github.com/SWivid/F5-TTS}}.

\item \textbf{E2 TTS}~\citep{eskimez2024e2} proposes an easy non-autoregressive zero-shot TTS system, that offers human-level naturalness and state-of-the-art speaker similarity and intelligibility. We use the code implemented by F5-TTS authors in our experiments\footnote{\url{https://github.com/SWivid/F5-TTS}}.
\end{itemize}

The evaluation is conducted on a server with 1 NVIDIA V100 GPU and batch size 1. RTF denotes the real-time factor, i.e., the seconds required for the system (together with the vocoder) to synthesize one-second audio.


\subsection{Details about the Accented TTS Baseline}
\label{app:detail_accented_tts_experiment}
CTA-TTS~\citep{liu2024controllable} is a TTS framework that uses a phoneme recognition model to quantify the accent intensity in phoneme level for accent intensity control. CTA-TTS first trains the phoneme recognition model on the standard pronunciation LibriSpeech dataset, and then uses the output probability distribution of the model to assess the accent intensity and create accent labels on the accented L2Arctic dataset. These labels were input into the TTS model to enable control over accent intensity.

Systems like CTA-TTS require precise accent annotations during training, so we trained them on the L2-ARCTIC dataset. However, our model does not require accent annotations and learns different accent patterns from large-scale data, using only the multi-condition CFG mechanism to achieve accent intensity control. Therefore, we directly compare the zero-shot results of our model with the baselines, which is a more challenging task.

\subsection{Details in Subjective Evaluations}
\label{details_subjective_evaluation}
We conduct evaluations of audio quality, speaker similarity, and accent similarity on Amazon Mechanical Turk (MTurk). We inform the participants that the data will be utilized for scientific research purposes. For each dataset, 40 samples are randomly selected from the test set, and the TTS systems are then used to generate corresponding audio samples. Each audio sample is listened to by a minimum of 10 listeners. For CMOS, following the approach of ~\citet{loizou2011speech}, listeners are asked to compare pairs of audio generated by systems A and B and indicate their preference between the two. They are then asked to choose one of the following scores: 0 indicating no difference, 1 indicating a slight difference, 2 indicating a significant difference and 3 indicating a very large difference. We instruct listeners to ``\textit{Please focus on speech quality, particularly in terms of clarity, naturalness, and high-frequency details, while disregarding other factors}''. For SMOS and ASMOS, each participant is instructed to rate the sentence on a 1-5 Likert scale based on their subjective judgment. For speaker similarity evaluations (SMOS), we instruct listeners to ``\textit{Please focus solely on the timbre and prosodic similarity between the reference speech and the generated speech, while disregarding differences in content, grammar, audio quality, and other factors}''. For accent similarity evaluations (ASMOS), we instruct listeners to ``\textit{Please focus solely on the accent similarity between the ground-truth speech and the generated speech, while disregarding other factors}''. The screenshots of instructions for testers are shown in Figure~\ref{screenshots_subjective_evaluations}. Additionally, we insert audio samples with known quality levels (e.g., reference recordings with no artifacts or intentionally corrupted audio with noticeable distortions) into the evaluation set to verify whether evaluators are attentive and professional. We also randomly repeat some audio clips in the evaluation set to check whether evaluators provide consistent ratings for the same sample. If large deviations in scores (larger than 1.0) for repeated clips occurs, we will select a new rater to evaluate this audio clip. We paid \$8 to participants hourly and totally spent about \$500 on participant compensation.

\begin{figure*}[!ht]
    \centering
	\begin{minipage}{0.85\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{figures/vis/CMOS.png}
		\caption*{(a)  Screenshot of CMOS testing.}
	\end{minipage}
	\centering
	\begin{minipage}{0.85\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{figures/vis/SMOS.png}
		\caption*{(b) Screenshot of SMOS testing.}
	\end{minipage}
	\centering
	\begin{minipage}{0.85\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{figures/vis/ASMOS.png}
		\caption*{(c) Screenshot of ASMOS testing.}
	\end{minipage}
	\centering
	\caption{Screenshots of subjective evaluations.}
	\label{screenshots_subjective_evaluations}
\end{figure*}

\section{Classifier-Free Guidance Used in Zero-Shot TTS}
\label{app:CFG_in_zs_tts}
Classifier-Free Guidance (CFG)~\citep{ho2022classifier} is a technique that balances sample fidelity and mode coverage in diffusion models by combining the score estimates from both a conditional and an unconditional model. The unconditional model is trained alongside the conditional model by randomly omitting the conditioning variable $c$ with a certain probability, allowing the same model to provide score estimates for both $p(x)$ and $p(x|c)$. In large-scale zero-shot TTS, VoiceBox~\citep{le2023Voicebox} and NaturalSpeech 2~\citep{shen2023naturalspeech} achieve CFG mechanism by dropping the text and prompt speech features. However, these works overlook that text and timbre should be controlled separately. Inspired by VoiceLDM~\citep{lee2024voiceldm} that introduces separate control of environmental conditions and speech contents, a concurrent work~\citep{yang2024dualspeech} proposes separately controlling the speaker fidelity and text intelligibility. However, this work is limited to improving the audio quality of TTS and does not explore the impact of CFG on accent.


\section{Details of PeRFlow Training Procedure}
\label{app:details_perflow_training}
Once the pretrained ODE solver of the teacher model $\phi_{\theta}$ is available, we perform the PeRFlow technique to train an accelerated solver in real time. When training, we only consider the shortened segments of the ODE trajectories, reducing the computational load of inference for the teacher model at each training step, and accelerating the training process. 

At each training step, given a data sample $z_1$ and a sample $z_0$ drawn from the source distribution (in this case, $z_0 \sim \mathcal{N}(0, I)$, i.e., Gaussian distribution), we randomly select a time window $(t_{k-1}, t_{k}]$ and compute the standpoint of the segmented probability path $z_{t_{k-1}} = \sqrt{1 - \sigma^2(t_{k-1})} z_1 + \sigma(t_{k-1}) z_0$, where $K$ is a hyperparameter indicating the total number of segments, $k\in\{1,\cdots,K\}$, $t_k = k / K$, and $\sigma(t)$ is the noise schedule. The teacher solver only needs to infer the endpoint of this segmented path, $\hat{z}_{t_k} = \phi_{\theta}(z_{t_{k-1}}, t_{k-1}, t_{k})$, with a remarkably smaller number of iterations $\widehat{T}$, comparing to that of a full trajectory, $T$. Finally, the student model is optimized on the segmented trajectory from $z_{t_{k-1}}$ to $\hat{z}_{t_k}$. We set $T$ to 25 and $\widehat{T}$ to 8, achieving a non-negligible acceleration of the training process.


\section{Details about Data and Model Scaling Experiments}
\label{app:data_model_scaling}

\paragraph{Training Corpus.} The data/model scalability is crucial for practical TTS systems. To evaluate the scalability of MegaTTS 3 in Section~\ref{exp:ablation_studies}, we construct a 600kh internal multilingual training corpus comprising both English and Chinese speech. Most of the audiobook recordings are crawled from YouTube and online podcasts like novelfm\footnote{\url{https://novelfm.changdunovel.com/}}. We also include the academic datasets like LibriLight~\citep{kahn2020libri}, WenetSpeech~\citep{zhang2022wenetspeech}, and GigaSpeech~\citep{chen2021gigaspeech}. Since the crawled corpus may contain unlabelled speeches. We transcribe them using an internal ASR model. 

\paragraph{Test Set.} Most prior studies of zero-shot TTS evaluate performances using the reading-style LibriSpeech test set, which may be different from real-world speech generation scenarios. In section~\ref{exp:ablation_studies}, we evaluate our model using the test sets collected from various sources, including: 1) CommonVoice~\citep{ardila2019common}, a large voice corpus containing noisy speeches from various scenarios; 2) RAVDESS~\citep{livingstone2018ryerson}, an emotional TTS dataset featuring 8 emotions and 2 emotional intensity. We follow~\citet{ju2024naturalspeech} and use strong-intensity samples to validate the model’s ability to handle emotional variance; 3) LibriTTS~\citep{zen2019libritts}, a high-quality speech corpus; 4) we collect samples from videos, movies, and animations to test whether our model can simulate timbres with distinctly strong individual characteristics. The test set consists of 40 audio samples extracted from each source.

\paragraph{Experimental Setup} We scale up MegaTTS 3 from 0.5B to 7.0B following the hyper-parameter settings in Qwen 2~\citep{yang2024qwen2}. In this experiment, we only increase the parameters of the MegaTTS 3 model to verify its scalability. The parameters of the speech compression VAE remained unchanged. In theory, expanding the parameters of both models could yield the optimal results, which we leave for future work.

\paragraph{Speech-Text Alignment Labels for Large-Scale Data.} Training an MFA model directly on a 600k-hour dataset is impractical. Therefore, we randomly sampled a 10k-hour subset from the dataset to train a robust MFA model, which is then used to align the full dataset. Since data processing inherently requires some alignment model (such as an ASR model) for speech segmentation, using a pretrained MFA model for alignment extraction does not limit the system's data scalability.


\begin{table}[!t]
\small
\centering
\begin{tabular}{@{}l|cc@{}}
\toprule
\bfseries Setting & \bfseries SIM-O$\uparrow$ & \bfseries WER$\downarrow$ \\       
\midrule
2kh   & 0.52           & 4.27\%                 \\
40kh  & 0.63           & 2.98\%                 \\
200kh & 0.65           & 2.34\%                 \\
600kh & \bfseries 0.66 & \bfseries 2.10\%       \\
\midrule
0.5B & 0.66           & 2.10\%                  \\
1.5B & 0.72           & 1.98\%                  \\
7.0B & \bfseries 0.74 & \bfseries 1.90\%        \\
\bottomrule
\end{tabular}
\caption{Results of data and model scaling experiments.}
\label{table:ablation_scalability}
\end{table}
\paragraph{Results} We evaluate the effectiveness of data and model scaling for the proposed MegaTTS 3 model. In this experiment, we train models with 0.5B parameters on multilingual internal datasets with data sizes of 2kh, 40kh, 200kh, and 600kh, respectively. We also train models with 0.5B, 1.5B, and 7.0B parameters on the 600kh dataset. We evaluate the zero-shot TTS performance in terms of speaker similarity (Sim-O) and speech intelligibility (WER) on an internal test set consisting of 400 speech samples from various sources. Based on Table~\ref{table:ablation_scalability}, we conclude that: 1) as the data size increases from 2kh to 600kh, both the model's speaker similarity and speech intelligibility improve consistently, demonstrating strong data scalability of our model; 2) as the model size scales from 0.5B to 7.0B parameters, SIM-O improves by 12.1\% and WER decreases by 9.52\%, validating the model scalability of MegaTTS 3. Additionally, we find that increasing the model parameters enhances its para-linguistic capabilities, with specific audio examples available on the demo page.


\begin{figure*}[!ht]
\centering
\begin{minipage}{0.49\linewidth}
    \centering
\includegraphics[width=1\linewidth]{figures/vis/vis_dur_control_1.png}
\caption{Sentence-level duration control.}
\label{app:vis_sent_dur_control}
\end{minipage}
\centering
\begin{minipage}{0.49\linewidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/vis/vis_dur_control_2.png}
    \caption{Phoneme-level duration control.}
    \label{app:vis_ph_dur_control}
\end{minipage}
\centering
\end{figure*}

\section{Duration Controllability of MegaTTS 3}
\label{app:dur_contol}
In this section, we aim to verify MegaTTS 3's duration control capabilities through case studies. We randomly selected a speech prompt from the test set and used the sentence ``Notably, raising questions about both the size of the perimeter and efforts to sweep and secure.'' as the target sentence to generate speeches. In the generation process, we first control the sentence-level duration by multiplying the time coordinates of the phoneme anchors described in Section~\ref{method:sec_3_2} by a fixed value. As shown in Figure~\ref{app:vis_sent_dur_control}, our MegaTTS 3 demonstrates good sentence-level duration control. Moreover, our MegaTTS 3 is also capable of fine-grained phoneme-level duration control. As illustrated in Figure~\ref{app:vis_ph_dur_control}, we multiplied the anchor coordinates of the phoneme within the red box by a fixed value while keeping the relative positions of other phoneme anchors unchanged. The figure shows that our MegaTTS 3 also exhibits good fine-grained phoneme-level duration controllability.




\begin{figure*}[!ht]
\centering
\begin{minipage}{0.98\linewidth}
    \centering
\includegraphics[width=1\linewidth]{figures/vis/layer8_diff_timestep.png}
    \caption*{(a) Layer 8 with different timesteps.}
\end{minipage}
\begin{minipage}{0.98\linewidth}
    \centering
\includegraphics[width=1\linewidth]{figures/vis/layer16_diff_timestep.png}
    \caption*{(b) Layer 16 with different timesteps.}
\end{minipage}
\begin{minipage}{0.98\linewidth}
    \centering
\includegraphics[width=1\linewidth]{figures/vis/layer27_diff_timestep.png}
    \caption*{(c) Layer 27 with different timesteps.}
\end{minipage}
\centering
\caption{Visualization of Attention Matrices from different layers in MegaTTS 3.}
\label{app:vis_attn}
\end{figure*}



\section{Visualization of Attention Matrices}
\label{app:vis_diff_attn}
We visualize the attention matrices from all layers in the 1.4B MegaTTS 3 model, using 8 sampling steps. From Figure~\ref{app:vis_attn}, we observe: 1) within the same layer, despite different timesteps, the attention matrices remain identical. In other words, the function of each layer stays consistent across timesteps; 2) the functions of the transformer layers can be categorized into three types. As shown in Figure~\ref{app:vis_attn} (a), the bottom layers handle text and audio feature extraction; in Figure~\ref{app:vis_attn} (b), the middle layers focus on speech-text alignment; and in Figure~\ref{app:vis_attn} (c), the top layers refine the target latent features.



\begin{table*}[!ht]
\small
\centering
\begin{tabular}{@{}l|cccccc@{}}
\toprule
\bfseries Method & \bfseries MCD$\downarrow$ & \bfseries SSIM$\uparrow$ & \bfseries STOI$\uparrow$ & \bfseries GPE$\downarrow$ & \bfseries VDE$\downarrow$ & \bfseries FFE$\downarrow$ \\       
\midrule
Ours w/ Sparse Alignment & \bfseries 4.56 & \bfseries 0.52  & \bfseries 0.62 & \bfseries 0.34 & \bfseries 0.30 & \bfseries 0.35 \\
Ours w/ Forced Alignment &  4.62   & 0.45 & \bfseries 0.62 & 0.42 & 0.34 & 0.40 \\
Ours w/ Standard CFG & 4.59 & 0.51  & 0.61 & 0.36 & 0.32 & 0.37 \\
Ours w/ Standard AR Duration & 4.58 & 0.50  & \bfseries 0.62 & 0.36 & 0.31 & 0.36 \\
\bottomrule
\end{tabular}
\caption{Comparisons about ``expressiveness'' metrics on the LibriSpeech test-clean set.}
\label{table:expressiveness_exp_libritest_all}
\end{table*}


\begin{table*}[!ht]
\small
\centering
\begin{tabular}{@{}l|cc@{}}
\toprule
\bfseries Model - with Longer Texts & \bfseries WER$\downarrow$ & \bfseries SIM-O$\uparrow$ \\       
\midrule
VoiceCraft      &  12.81\% & 0.62  \\
CosyVoice       &  5.52\%  & 0.68 \\
MegaTTS 3           &  \bfseries 2.39\%  & \bfseries 0.70 \\
\midrule
\midrule
\bfseries Model - with Short Texts & \bfseries WER$\downarrow$ & \bfseries SIM-O$\uparrow$ \\  
\midrule
VoiceCraft       &  4.07\%   & 0.58 \\
CosyVoice      &  2.24\%   & 0.62 \\
MegaTTS 3           &  \bfseries 1.82\%   & \bfseries 0.71  \\
\bottomrule
\end{tabular}
\caption{Comparisons with longer texts.}
\label{table:cmp_longer_samples}
\end{table*}


\section{About Different Lengths of Context}
An imbalanced distribution of prompt and target lengths during training can lead to unstable generation performance during inference. For example, if the majority of the sampled data during training consists of 20-second targets, the generation performance for audio with a 40-second target will be worse than that of 20-second targets in inference. To solve the imbalanced distribution issue, we recommend using the following multi-sentence data sampling strategy: we concatenate all audio recordings of the same speaker in the dataset in time order, and then randomly extract audio segments of length $t\sim U(t_{min}, t_{max})$ from the concatenated audio, where $t_{min}$ is the minimum sampling time and $t_{max}$ is the maximum sampling time. Then, following Section~\ref{method:main_arch}, we randomly divide the sampled sequence into a prompt region and a target region. Although we do not use this strategy in our experiments in order to make a fair comparison with other methods, this strategy is effective in practical scenarios.




\section{Experiments of Prosodic Naturalness for Zero-Shot TTS}
\label{app:exp_expressivenees_zs_tts}

We also conduct the ablation studies using the objective metrics MCD, SSIM, STOI, GPE, VDE, and FFE following InstructTTS~\citep{yang2024instructtts} to evaluate the prosodic naturalness of our proposed method. We conduct experiments on the LibriSpeech test-clean 2.2-hour subset (following the setup in VALL-E 2 and Voicebox). The results are shown in the Table below. We compare MegaTTS 3 with the following baselines: 1) ``Ours w/ Forced Alignment'', we replace the sparse alignment with the forced alignment; 2) ``Ours w/ Standard CFG'', we replace the multi-condition CFG with standard CFG; 3) ``Ours w/ Standard AR Duration'', we replace the duration from F-LM with the duration from standard AR duration predictor following SimpleSpeech 2~\citep{yang2024simplespeech2}. The results in Table~\ref{table:expressiveness_exp_libritest_all} show that sparse alignment brings significant improvements, and both multi-condition CFG and F-LM duration contribute positively to the performance.


\section{Experiments with Longer Samples}
\label{app:exp_longer_samples}
To directly compare MegaTTS 3's robustness to long sequences against other AR models, we have conducted experiemnts for a test set with longer samples. Specifically, we randomly select 10 sentences, each containing more than 50 words. For each speaker in the LibriSpeech test-clean set, we randomly chose a 3-second clip as a prompt, resulting in 400 target samples in total. To make our results more convincing, we include strong-performing TTS models, VoiceCraft~\citep{peng2024voicecraft} and CosyVoice (AR+NAR)~\citep{du2024cosyvoice}, as our baselines. The results for longer samples are presented in Table~\ref{table:cmp_longer_samples}. As shown, compared to the baseline systems, MegaTTS 3 does not exhibit a significant decline in speech intelligibility when generating longer sentences, illustrating the effectiveness of the combination of F-LM and MegaTTS 3.


\begin{table*}[!ht]
\small
\centering
\begin{tabular}{@{}l|cccc@{}}
\toprule
\bfseries Model & \bfseries WER$\downarrow$ & \bfseries Substitution$\downarrow$ & \bfseries Deletion$\downarrow$& \bfseries Insertion$\downarrow$ \\       
\midrule
E2-TTS       &  8.49\%   & 3.65\% & 4.75\% & 0.09\% \\
F5-TTS       &  4.28\%   & \bfseries 1.78\% & 2.28\% & 0.22\% \\
MegaTTS 3        &  \bfseries 3.95\%   & 1.80\% & \bfseries 2.07\% & \bfseries 0.08\% \\
\bottomrule
\end{tabular}
\caption{Comparisons with hard sentences. The results of the baselines are infered from offical
checkpoints.}
\label{table:cmp_hard_sentences}
\end{table*}


\section{Experiments with Hard Sentences}
The transcriptions on the LibriSpeech test-clean set are relatively simple since they come from audiobooks. To further indicate the speech intelligibility of different methods, we evaluate our model on the challenging set containing 100 difficult textual patterns from ELLA-V~\citep{song2024ella}. Since the speech prompts used by ELLA-V are not publicly available, we randomly sample 3-second-long speeches in the LibriSpeech test-clean set as speech prompts. For this evaluation, we used the official checkpoint of F5-TTS~\citep{chen2024f5} and the E2-TTS~\citep{eskimez2024e2} inference API provided on F5-TTS's Hugging Face page. We employ Whisper-large-v3 for WER calculation. Based on the results presented in Table~\ref{table:cmp_hard_sentences}, our model shows stronger robustness against hard transcriptions.

\section{Additional Details for Multi-Condition CFG}
\label{app:additional_detials_for_mt_cfg}
In Section~\ref{method:sec_3_2}, regarding the multi-condition CFG technique, the experimental setup for the preliminary experiment for accent control is: fixing $\alpha_{spk}$ at 2.5 and varying $\alpha_{txt}$ from 1.0 to 6.0. Specifically, as $\alpha_{txt}$ increases from 1.0 to 1.5, the generated speeches contains improper pronunciations and distortions. When $\alpha_{txt}$ ranges from 1.5 to 2.5, the pronunciations align with the speaker's accent. Finally, once $\alpha_{txt}$ exceeds 4.0, the generated speech converges toward the standard pronunciation of the target language. Notably, the optimal values for parameters $\alpha_{txt}$ and $\alpha_{spk}$ may vary across different models. The values reported here are specific to the model used in our experiments.



\section{Ethics Statement}
\label{sec:ethics_statement}
The proposed model, MegaTTS 3, is designed to advance zero-shot TTS technologies, making it easier for users to generate personalized speech. When used responsibly and legally, this technique can enhance applications such as movies, games, podcasts, and various other services, contributing to increasing convenience in everyday life. However, we acknowledge the potential risks of misuse, such as voice cloning for malicious purposes. To mitigate this risk, solutions like building a corresponding deepfake detection model will be considered. Additionally, we plan to incorporate watermarks and verification methods for synthetic audio to ensure ethical use in real-world applications. Restrictions will also be included in the licensing of our project to further prevent misuse. By addressing these ethical concerns, we aim to contribute to the development of responsible and beneficial AI technologies, while remaining conscious of the potential risks and societal impact.


\section{Reproducibility Statement}
\label{sec:reproducibility_statement}
We have taken several steps to ensure the reproducibility of the experiments and results presented in this paper: 1) the architecture and algorithm of the MegaTTS 3 model are described in Section~\ref{method} and and relevant hyperparameters are fully described in Appendix~\ref{app:model_config}; 2) The evaluation metrics, including WER, SIM-O, MCD (dB), the moments of the pitch distribution, alignment error, CMOS, SMOS, and ASMOS, are described in detail in Section~\ref{Experimental_Setup}; 3) For most of the key experiments, we utilize publicly available datasets such as LibriLight, LibriSpeech, and L2Arctic. The selection of the test sets is identical to that used in previous zero-shot TTS research. However, as the publicly available datasets are insufficient for our data scaling experiments, we construct a larger dataset, which is described in detail in Appendix~\ref{app:data_model_scaling}; 4) To ensure reproducibility of the results, we have carefully set random seeds in our experiments and the random seeds are provided in Appendix~\ref{app:random_seeds}. All objective results reported are based on the average performance across multiple runs.


% \begin{table*}[thb]
% 	\centering
% 	\setlength\tabcolsep{6pt}
% 		\begin{tabular}{lcccc}
% 			\toprule
% 			\multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{\emph{test-zh}}} & \multicolumn{2}{c}{\textbf{\emph{test-en}}}  \\
% 			\cmidrule(r){2-3} \cmidrule(r){4-5}
% 			& \textbf{CER (\%)~$\downarrow$} & \textbf{SIM-O~$\uparrow$} & \textbf{WER (\%)~$\downarrow$} & \textbf{SIM-O~$\uparrow$}\\
% 			\midrule
% 			\textbf{Human} & 1.26 & 0.755 & 2.14 & 0.734   \\
% 			\midrule
% 			\textbf{FireRedTTS} & 1.51 & 0.635 & 3.82 & 0.460  \\
% 			\textbf{MaskGCT} & 2.27 & \underline{0.774} & 2.62 & \underline{0.714}\\

% 			\textbf{F5-TTS (32 NFE)} & 1.56 & 0.741 & \underline{1.83} & 0.647\\
%             \textbf{Llasa-8B} & 1.59 & 0.684 & 2.97 & 0.574\\
%             \textbf{Spark-TTS} & \bfseries{1.20} & 0.672 & 1.98 & 0.584\\
%             \textbf{CosyVoice 2} & 1.45 & 0.748 & 2.57 & 0.652 \\
%             \midrule
%             \textbf{MegaTTS 3} & \underline{1.36} & \bfseries{0.790} & \bfseries 1.82 & \bfseries 0.773 \\
            
%             \bottomrule
% 	\end{tabular}
% 	\caption{Results of MegaTTS 3 and recent open-sourced TTS models on the SEED test sets.}
% 	\label{tab:comapre}
% \end{table*}