\section{Introduction}
In recent years, neural codec language models~\citep{wang2023neural,zhang2023speak,song2024ella,xin2024rall} and large-scale diffusion models ~\citep{shen2023naturalspeech,le2023Voicebox,lee2024ditto,eskimez2024e2,ju2024naturalspeech,yang2024simplespeech,yang2024simplespeech2} have brought considerable advancements to the field of speech synthesis. Unlike traditional text-to-speech (TTS) systems~\citep{shen2018natural,jia2018transfer,li2019neural,kim2020glow,ren2019fastspeech,kim2021conditional,kim2022guided}, these models are trained on large-scale, multi-domain speech corpora, which contributes to notable improvements in the naturalness and expressiveness of synthesized audio. Given only seconds of speech prompt, they can synthesize identity-preserving speech in a zero-shot manner.

To generate high-quality speech with clear and expressive pronunciation, a TTS model must establish an alignment mapping from text to speech signals~\citep{kim2020glow,tan2021survey}. However, from the perspective of speech-text alignment, current solutions suffer from the following issues:


\begin{itemize}
    \item \textbf{Models with implicit speech-text alignment} achieve the soft alignment paths through attention mechanisms~\citep{wang2023neural,chen2024vall,du2024cosyvoice}. These models can be categorized into: 1) autoregressive codec language models (AR LM), which are inefficient and lack robustness. The lengthy discrete speech codes, which typically require a bit rate of 1.5 kbps~\citep{kumar2024high,wu2024towards}, impose a significant burden on these autoregressive language models; 2) diffusion-based models without explicit duration modeling~\citep{lee2024ditto,eskimez2024e2,lovelace2023simple,gao2023e3,cambara2024mapache,yang2024simplespeech,yang2024simplespeech2}, which significantly speeds up the speech generation process. However, when compared with methods that adopt forced alignment, these models exhibit a decline in speech intelligibility. Besides, these methods cannot provide fine-grained control over the duration of specific pronunciations and can only adjust the overall speech rate.

    \item \textbf{Predefined alignment-based methods} have prosodic naturalness constraints of forced alignments. During training, alignment paths~\citep{ren2020fastspeech,kim2020glow} are directly introduced into their models~\citep{le2023Voicebox,shen2023naturalspeech,ju2024naturalspeech} to reduce the complexity of text-to-speech generation, which achieves higher intelligibility. Nevertheless, they suffer from the following limitations: 1) predefined alignments constrain the model's search space to produce more natural-sounding speech~\citep{anastassiou2024seed,chen2024vall}; 2) the overall naturalness is highly dependent on the performance of duration models.
\end{itemize}

Intuitively, we can integrate the two aforementioned diffusion-based methods to pursue optimal performance. To be specific, we propose a novel sparse speech-text alignment strategy to enhance the latent diffusion transformer (DiT), termed MegaTTS 3. In our approach, phoneme tokens are sparsely distributed within the corresponding forced alignment regions to provide coarse pronunciation information that is then refined by the latent DiT model. Experimental results demonstrate that MegaTTS 3 achieves nearly state-of-the-art speech intelligibility and speaker similarity on the LibriSpeech test-clean set~\citep{panayotov2015librispeech} with only 8 sampling steps, while also exhibiting high speech naturalness. The main contributions of this work are summarized as follows:

\begin{itemize}
\item We design a sparse alignment enhanced latent diffusion transformer model, which effectively integrates the strengths of the two aforementioned speech-text alignment approaches. Notably, our model also demonstrates greater robustness to duration prediction errors compared to methods with forced alignment. 

\item To achieve higher generation quality and more flexible control, we propose a multi-condition CFG strategy to adjust the guidance scales for speaker timbre and text content separately. Furthermore, we discover that the text guidance scale can also be used to modulate the intensity of personal accents, offering a new direction for enhancing speech expressiveness.

\item We successfully reduce the inference steps from 25 to 8 with the piecewise rectified flow (PeRFLow) technique, achieving highly efficient zero-shot TTS with minimal quality degradation. We also visualize the attention matrices across various layers of MegaTTS 3 and obtain insightful findings in Appendix~\ref{app:vis_diff_attn}.

\end{itemize}