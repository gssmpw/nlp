\section{Background}

\paragraph{Zero-shot TTS.} 
Zero-shot TTS~\citep{casanova2022yourtts,wang2023neural,zhang2023speak,shen2023naturalspeech,le2023Voicebox,jiang2024mega,liu2024autoregressive,lee2024ditto,li2024styletts,lee2023hierspeech++,ju2024naturalspeech,meng2024autoregressive,chen2024f5} aims to synthesize unseen voices with speech prompts. Among them, neural codec language models~\citep{chen2024vall} are the first that can autoregressively synthesize speech that rivals human recordings in naturalness and expressiveness. However, they still face several challenges, such as the lossy compression in discrete audio tokenization and the time-consuming nature of autoregressive generation. To address these issues, some subsequent works explore solutions based on continuous vectors and non-autoregressive diffusion models~\citep{shen2023naturalspeech,le2023Voicebox,lee2024ditto,eskimez2024e2,yang2024simplespeech,yang2024simplespeech2,chen2024f5}. These works can be categorized into two main types: 1) the first type directly models speech-text alignments using attention mechanisms without explicit duration modeling~\citep{lee2024ditto,eskimez2024e2}. Although these models perform well in terms of generation speed and quality, their robustness, especially in challenging cases, still requires enhancement. The second category~\citep{shen2023naturalspeech,le2023Voicebox} utilizes predefined alignments to simplify alignment learning. However, the search space of the generated speech of these models is limited by predefined alignments. To address these limitations, we propose a sparse alignment mechanism to reduce the constraints of predefined alignment-based methods while also reducing the difficulty of speech-text alignment learning.

\begin{figure*}[!t]
	\centering
	\includegraphics[width=0.95\textwidth]{figures/Main_Arch.pdf}
	\caption{(a) The WaveVAE model; (b) Overview of our model. We insert the sparse alignment anchors into the latent vector sequence to provide coarse alignment information. The transformer blocks in MegaTTS 3 will automatically build fine-grained alignment paths.}
	\label{fig:arch_overview}
\end{figure*}

\paragraph{Accented TTS.} While accented TTS is not yet mainstream in the field of speech synthesis, it offers valuable potential for customized TTS services, by enhancing the expressiveness of speech synthesis systems and improving listeners' comprehension of speech content~\citep{tan2021survey,melechovsky2022accented,badlani2023multilingual,zhou2024multi,shah2024parrottts,ma2023accent,inoue2024macst,zhong2024accentbox}. With the emergence of conversational AI systems, accented TTS technology has even broader application scenarios. In this paper, we focus on a specific task of accented TTS: adjusting the accent intensity of speakers to make them sound like native English speakers or accented speakers who use English as a second language~\citep{liu2024controllable}. Unlike previous work, our approach does not require paired data or accurate accent labels; instead, it allows for flexible control over the accent intensity using the proposed multi-condition CFG mechanism. In addition, we describe the CFG mechanism used in zero-shot TTS systems in Appendix~\ref{app:CFG_in_zs_tts}.