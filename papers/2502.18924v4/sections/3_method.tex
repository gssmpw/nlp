\section{Method}
\label{method}
This section introduces MegaTTS 3. To begin with, we describe the architecture design of MegaTTS 3. Then, we provide detailed explanations of the sparse alignment mechanism, the piecewise rectified flow acceleration technique, and the multi-condition classifier-free guidance strategy.

\subsection{Architecture}
\label{method:main_arch}
\paragraph{WaveVAE.} As shown in Figure~\ref{fig:arch_overview} (a), given a speech waveform $s$, the VAE encoder $E$ encodes $s$ into a latent vector $z$, and the wave decoder $D$ reconstructs the waveform $x = D(z) = D(E(s))$. To reduce the computational burden of the model and simplify speech-text alignment learning, the encoder $E$ downsamples the waveform by a factor of $d$ in length. The encoder $E$ is similar to the one used in~\citet{ji2024wavtokenizer}, and the decoder $D$ is based on~\citet{kong2020hifi}. We also adopt the multi-period discriminator (MPD), multi-scale discriminator (MSD), and multi-resolution discriminator (MRD) ~\citep{kong2020hifi,jang2021univnet} to model the high-frequency details in waveforms, which ensure perceptually high-quality reconstructions. The training loss of the speech compression model can be formulated as $\mathcal{L} = \mathcal{L}_{\mathrm{rec}} + \mathcal{L}_{\mathrm{KL}} + \mathcal{L}_{\mathrm{Adv}}$, where $\mathcal{L}_{\mathrm{rec}}=\|s-\hat{s}\|^2$ is the spectrogram reconstruction loss, $\mathcal{L}_{\mathrm{KL}}$ is the slight KL-penalty loss~\citep{rombach2022high}, and $\mathcal{L}_{\mathrm{Adv}}$ is the LSGAN-styled adversarial loss~\citep{mao2017least}. After training, a one-second speech clip can be encoded into 25 vector frames. For more details, please refer to Appendix~\ref{app:model_config} and~\ref{app:evaluation_speech_compression}.

\paragraph{Latent Diffusion Transformer with Masked Speech Modeling.}
The latent diffusion transformer is used to predict speech that matches the style of the given speaker and the content of the provided text. Given the random variables $Z_{0}$ sampled from a standard Gaussian distribution $\pi_{0}$ and $Z_{1}$ sampled from the latent space given by the speech compression model with data density $\pi_{1}$, we adopt the rectified flow~\citet{liu2022flow} to implicitly learn the transport map $T$, which yields $Z_{1} := T(Z_{0})$. The rectified flow learns $T$ by constructing the following ordinary differential equation (ODE):
\begin{equation}
    \small
    \mathrm{d}Z_t = v(Z_t, t)\,\mathrm{d}t,
    \label{eq:1}
\end{equation}
where $t\in[0,1]$ denotes time and $v$ is the drift force. Equation~\ref{eq:1} converts $Z_{0}$ from $\pi_{0}$ to $Z_{1}$ from $\pi_{1}$. The drift force $v$ drives the flow to follow the direction $(Z_{1}-Z_{0})$. The latent diffusion transformer,  parameterized by $\theta$, can be trained by estimating $v(Z_{t}, t)$ with $v_{\theta}(Z_{t}, t)$ through minimizing the least squares loss with respect to the line directions $(Z_{1}-Z_{0})$:
\begin{equation}
    \small
    \min_v \int_0^1 \mathbb{E} \left[ \| (Z_1 - Z_0) - v(Z_t, t) \|^2 \right] \, \mathrm{d}t.
    \label{eq:2}
\end{equation}
We use the standard transformer block from LLAMA~\citep{dubey2024llama} as the basic structure for MegaTTS 3 and adopt the Rotary Position Embedding (RoPE)~\citep{su2024roformer} as the positional embedding. During training, we randomly divide the latent vector sequence into a prompt region $z_{prompt}$ and a masked target region $z_{target}$, with the proportion of $z_{prompt}$ being $\gamma \sim U(0.1, 0.9)$. We use $v_{\theta}$ to predict the masked target vector $\hat{z}_{target}$ conditioned on $z_{prompt}$ and the phoneme embedding $p$, denoted as $v_{\theta}(\hat{z}_{target}|z_{prompt}, p)$. The loss is calculated using only the masked region $z_{target}$. MegaTTS 3 learns the average pronunciation from $p$ and the specific characteristics such as timbre, accent, and prosody of the corresponding speaker from $z_{prompt}$.

%This means that when the longest speech length during training is 60 seconds, the model can generate 54 seconds of target speech using just a 6-second prompt, which is sufficient to cover most speech synthesis scenarios. 


% The phoneme embeddings $p$ are concatenated with $z_{prompt}$ to serve as input.

\subsection{Sparse Alignment Enhanced Latent Diffusion Transformer (MegaTTS 3)}
\label{method:sec_3_2}
In this subsection, we describe the sparse alignment strategy as the foundation of MegaTTS 3, followed by the piecewise rectified flow and multi-condition CFG strategies to further enhance MegaTTS 3's capacity.

\paragraph{Sparse Alignment Strategy.}
Letâ€™s first analyze the reasons behind the characteristics of different speech-text alignment modeling methods in depth. Implicitly modeling speech-text alignment is a relatively challenging task, which consequently leads to suboptimal speech intelligibility, particularly in hard cases. On the other hand, employing predefined hard alignment paths constrains the model's search space to produce more natural-sounding speech. The characteristics of these systems motivate us to design an approach that combines the advantages of both: we first provide a rough alignment to MegaTTS 3 and then use attention mechanisms in Transformer blocks to construct the fine-grained implicit alignment path. The visualizations of the implicit alignment paths are included in Appendix~\ref{app:vis_diff_attn}. In specific, denote the latent speech vector sequence as $z=[z_1, z_2, \cdots, z_n]$, the phoneme sequence as $p=[p_1, p_2, \cdots, p_m]$, and the phoneme duration sequence as $d=[d_1, d_2, \cdots, d_m]$, where $n$, $m$ is the length of the sequence. The length of the speech vector that corresponds to a phoneme $p_i$ is the duration $d_i$. Given $d=[2, 2, 3]$, the hard speech-text alignment path can be denoted as $a=[p_1, p_1, p_2, p_2, p_3, p_3, p_3]$. To construct the rough alignment $\tilde{a}$, we randomly retain only one anchor for each phoneme: $\tilde{a} = [\underline{M}, p_1, p_2, \underline{M}, \underline{M}, \underline{M}, P_3]$, where $\underline{M}$ represents the mask token. $\tilde{a}$ is downsampled with convolution layers to match the length of the latent sequence $z$. Then, we directly concatenate the downsampled $\tilde{a}$ and $z$ along the channel dimension. The anchors in $\tilde{a}$ provide MegaTTS 3 with approximate positional information for each phoneme, simplifying the learning process of speech-text alignment. At the same time, the rough alignment information does not limit MegaTTS 3's search space and also enables fine-grained control over each phoneme's duration.


\paragraph{Piecewise Rectified Flow Acceleration.}
We adopt Piecewise Rectified Flow (PeRFlow)~\citep{yan2024perflow} to distill the pretrained MegaTTS 3 model into a more efficient generator.
Although our MegaTTS 3 is non-autoregressive in terms of the time dimension, it requires multiple iterations to solve the Flow ODE. The number of iterations (i.e., number of function evaluations, NFE) has a great impact on inference efficiency, especially when the model scales up further. Therefore, we adopt the PeRFlow technique to further reduce NFE by segmenting the flow trajectories into multiple time windows. Applying reflow operations within these shortened time intervals, PeRFlow eliminates the need to simulate the full ODE trajectory for training data preparation, allowing it to be trained in real-time alongside large-scale real data during the training process. Given number of windows $K$, we divide the time $t\in[0,1]$ into $K$ time windows $\{ (t_{k-1}, t_{k}] \}^{K}_{k=1}$. Then, we randomly sample $k\in\{1,\cdots,K\}$ uniformly. We use the startpoint of the sampled time window $z_{t_{k-1}} = \sqrt{1 - \sigma^2(t_{k-1})} z_1 + \sigma(t_{k-1}) \epsilon$ to solve the endpoint of the time window $\hat{z}_{t_k} = \phi_{\theta}(z_{t_{k-1}}, t_{k-1}, t_{k})$, where $\epsilon \sim \mathcal{N}(0, I) $ is the random noise, $\sigma(t)$ is the noise schedule, and $\phi_{\theta}$ is the ODE solver of the teacher model. Since $z_{t_{k-1}}$ and $\hat{z}_{t_{k}}$ is available, the student model $\hat{\theta}$ can be trained via the following objectives:
\begin{equation}
    \small
    \ell = \left\lVert v_{\hat{\theta}}(z_t, t) - \frac{\hat{z}_{t_k} - z_{t_{k-1}}}{t_k - t_{k-1}} \right\lVert^2,
    \label{eq:3}
\end{equation}
where $v_{\hat{\theta}}$ is the estimated drift force with parameter $\hat{\theta}$ and $t$ is uniformly sampled from $(t_{k-1},t_{k}]$. We provide details of PeRFlow training for MegaTTS 3 in Appendix~\ref{app:details_perflow_training}.

\paragraph{Multi-condition Classifier-Free Guidance (CFG).}
We employ classifier-free guidance approach~\citep{ho2022classifier} to steer the model $g_{\theta}$'s output towards the conditional generation $g_{\theta}(z_t,c)$ and away from the unconditional generation $g_{\theta}(z_t,\varnothing)$:
\begin{equation}
    \small
    \hat{g}_{\theta}(z_t, c) = g_{\theta}(z_t, \varnothing) + \alpha \cdot \left[ g_{\theta}(z_t, c) - g_{\theta}(z_t, \varnothing) \right],
    \label{eq:4}
\end{equation}
where $c$ denotes the conditional state, $\varnothing$ denotes the unconditional state, and $\alpha$ is the guidance scale selected based on experimental results. Unlike standard classifier-free guidance, MegaTTS 3's conditional states $c$ consist of two components: phoneme embeddings $p$ and the speaker prompt $z_{prompt}$. In the experiments, as the text guidance scale increases, we observe that the pronunciation changes according to the following pattern: 1) starting with improper pronunciation; 2) then shifting to pronouncing with the current speaker's accent; 3) and finally approaching the standard pronunciation of the target language. The detailed experimental setup is described in Appendix~\ref{app:additional_detials_for_mt_cfg}. This allows us to use the text guidance scale $\alpha_{txt}$ to control the accent intensity. At the same time, the speaker guidance scale $\alpha_{spk}$ should be a relatively high value to ensure a high speaker similarity. Therefore, we adopt the multi-condition classifier-free guidance technique to separately control $\alpha_{txt}$ and $\alpha_{spk}$:
\begin{equation}
    \small
    \begin{split}
    \hat{g}_{\theta}(z_t, p, z_{prompt}) = & \alpha_{spk} \left[ g_{\theta}(z_t, p, z_{prompt}) - g_{\theta}(z_t, p, \varnothing) \right] \\
    & + \alpha_{txt} \left[ g_{\theta}(z_t, p, \varnothing) - g_{\theta}(z_t, \varnothing, \varnothing) \right] \\ 
    & + g_{\theta}(z_t, \varnothing, \varnothing) \\
    \end{split}
    \label{eq:5}
\end{equation}
In training, we randomly drop condition $z_{prompt}$ with a probability of $p_{spk} = 0.10$. Only when $z_{prompt}$ is dropped, we randomly drop condition $p$ with a probability of 50\%. Therefore, our model is able to handle all three types of conditional inputs described in Equation~\ref{eq:5}. We select the guidance scale $\alpha_{spk}$ and $\alpha_{txt}$ based on experimental results.