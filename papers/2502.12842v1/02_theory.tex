\section{Theoretical Background}


\subsection{Characteristics of effective feedback} \label{sec:Characteristics_feedback}

Focusing on the learning process, formative assessment tries to continuously adapt the teaching to the needs of the students \citep{filsecker2012repositioning}. It has been, despite some critics \citep{bennett2010cognitively}, identified as one of the most significant influencing factors for effective learning \citep{hattie2008visible}. 
Feedback is a key part of formative assessment \citep{brookhart2008feedback}, helping to bridge the gap between current understanding and learning goals \citep{sadler1989formative}. \citet{shute2008focus} defined formative feedback as ``information communicated to the learner that is intended to modify his or her thinking or behavior to improve learning''. According to the model of \cite{hattie2007power}, effective feedback answers the following the questions ``Where am I going?'' \textit{(Feed Up)}, ``How am I going?''  \textit{(Feed Back)}, and ``Where to go next?''  \textit{(Feed Forward)}. Each feedback question again can be applied on four levels: The task level, the process level, the self-regulation level and the self level (see \cite{hattie2007power}).

To guide students towards their academic goals, content-related feedback is fundamental. Firstly, \textit{Feed Up} involves clearly summarizing the objectives of the student, establishing a clear goal, and providing a sense of direction \citep{hattie2007power}. 
Secondly, \textit{Feed Back} focuses on the student's current performance, highlighting errors in context. This helps students understand their situation and identify specific areas for improvement \citep{hattie2007power}.
Lastly, \textit{Feed Forward} offers guidance on the next steps. Overall, effective feedback facilitates the learning process, by identifying errors but also recalling the objectives and providing hints for solutions or giving explanations to address identified challenges \citep{hattie2007power, ossenberg2019attributes}.

Next to the content-related factors, the tone, linguistic clarity,  appropriate use of technical terms, and the length of the text play a crucial role in the effectiveness of feedback. 
The tone should be encouraging while avoiding excessive positivity \citep{kluger1996effects}. This approach creates a supportive learning environment and ensures that the feedback remains constructive \citep{brookhart2017give}. A well-balanced tone motivates students while preserving the critical insights essential for their growth.
%
Scientific language often creates a significant barrier to understanding scientific concepts \citep{gabel1999improving, cassels1983meaning}. Therefore, feedback needs to be specific \citep{moreno2004decreasing}, straightforward, and easily understandable \citep{brookhart2017give, ossenberg2019attributes, Mory2004}. Using simple sentence structures and clear language tailored to students helps keep feedback focused and manageable, avoiding information overload.
%
Furthermore, using technical terms adequately is essential for effective feedback. While scientific language can enhance learning \citep{fazio2019science}, it can also be challenging for students to understand \citep{vladuvsic2016understanding, hamnell2023scientific}. To balance this, feedback should include relevant terms without overwhelming students, promoting both understanding and educational value.
%
Overall, regarding the length, feedback should be as simple and focused as possible \citep{kulhavy1985conjoint}. However, \citet{van2015effects} found in their meta-analysis that more elaborate feedback led to higher learning outcomes than (overly) simplified feedback.

%A broad and comprehensive feedback model, such as the one presented, should not be interpreted as requiring all aspects simultaneously, since conflicting aspects may need careful balancing by educators to avoid overstraining students. Consequently, tailoring the feedback to specific purposes has to ensure its effectiveness without overwhelming learners - especially if the feedback is provided instantly to achieve high effectiveness to the learner \citep{azevedo1995meta}; condensed feedback might be more effective than a comprehensive one.

%This broad and comprehensive model of feedback shouldn’t be understood as aspects that all have to be met in order to generate effective feedback. In contrast, these feedback aspects might even contradict each other. For example, it is a common understanding that one should provide elaborate feedback \citep{hattie2007power, narciss2004design},  but, as stated above, on the other hand, feedback should be delivered as simple as possible and in manageable units.  Especially in complex reasoning tasks, e.g., planning and conducting an experiment, these somewhat contradicting requirements have to be carefully assessed and balanced by the educator.

In the field of science education, teachers often favor summative assessment for inquiry-based science learning \citep{grob2017formative, zlabkova2024development}. But, in line with findings from other fields research, especially formative feedback can enhance students' understanding in science education \citep{schultz2019effects} and of scientific inquiry, like experimentation \citep{grangeat2024introduction}. 




\subsection{Automated computer-based feedback} 

%%Allgemein über Automated computer-based feedback
The effectiveness of automated, computer-based feedback is known for decades in educational research, as it shows some benefits to human-generated feedback. One central advantage is the instance generation of feedback. Especially for low-achieving learners this immediate feedback has proven to be effective \citep{mason2001providing}. In their meta-analysis of the effects of feedback in computer-based instruction, \citet{azevedo1995meta} found effects of .80 for immediate feedback and .35 for delayed feedback across all learners. 
Another advantage can be the lower bias that is perceived when using automated feedback \citep{kluger1996effects}. 
However, one-to-one tutoring is generally very effective \citep{bloom19842}, and computer-based feedback can have difficulties mimicking the adaptiveness of human tutoring.
Unlike conventional computer-based feedback systems, LLM-based systems can offer a higher degree of interactivity, thus enabling a more adaptive and engaging learning experience.


% However, students seem to prefer human-generated feedback over computer-generated feedback \citep{nazaretsky2024ai}.

%%%
%Our agent is able to combine the advantages of computer-based feedback - instant and neutral -, while mitigating remaining problems of classical computer-based problems like missing adaptiveness. We develop a feedback system which 

%We aim to evaluate an LLM agent compared to feedback from human experts and science teachers. By doing so we can demonstrate that our feedback agent might be as effective as human experts regarding central categories of effective feedback while the LLM agent additionally integrates the advantages of automated computer-based feedback - instant, neutral and adaptive feedback.

%LLM-based Feedback
%Name systems which have limitations but also which have great possibilities -> look into previous and latter sections. % Kathrin connection



\subsection{LLM-based feedback}

%Das folgende habe ich mal rausgenommen da es viel der Intro wiederholt.
%Advancements in large language models (LLMs) like GPT-4 \cite{gpt4} have opened new possibilities for integrating intelligent systems into educational environments \cite{kasneci2023chatgpt}, particularly in the domain of feedback generation. LLMs can produce personalized, context-aware feedback that addresses individual student needs, thereby enhancing the learning experience. To fully harness the potential of these models, it is essential to integrate feedback generation with pedagogical design and adapt theoretical frameworks accordingly, as emphasized by Stamper et al. \cite{stamper2024enhancing}. 


LLM-generated feedback has been explored in various educational fields. In mathematics education, \citet{nguyen2023evaluating} evaluated the effectiveness of GPT-3.5 in addressing student errors related to decimal numbers and found it generally effective in providing appropriate feedback. For writing instruction, \citet{sessler2023peer} utilized GPT-3.5 to generate instant feedback, demonstrating its utility in supporting writing development through constructive critiques. A significant focus has been placed on programming education, where several studies have investigated the capabilities of LLMs. \citet{gabbay2024combining} found that while GPT-3.5 and GPT-4 are effective at detecting errors in code assignments within MOOCs, they often fall short in providing accurate or actionable feedback. \citet{estevez2024evaluation} observed limitations in LLMs' ability to evaluate exercises involving concurrency errors, highlighting challenges in understanding complex programming concepts. Additionally, \citet{koutcheme2024open} noted a tendency for GPT-4 to provide overly positive feedback in introductory programming courses, potentially overlooking critical issues that require attention.

While these advancements are promising, research on LLM-based feedback generation in science education is still developing. From a theoretical standpoint, \citet{zhai2023ai} underscore the importance of embracing AI's potential in formative assessment within science education, advocating for constructive dialogue rather than dismissing its impact. In an earlier review, \citet{zhai2020applying} examined machine learning-based science assessments across three axes -technical implementation, validity, and pedagogical features - and found that most studies concentrated on validity, often neglecting technical and pedagogical aspects.

Recent efforts have begun to explore the application of language models in science education assessment. \citet{wu2023matching} applied a pre-trained BERT model with zero-shot prompting to score students' written responses, demonstrating the feasibility of using LLMs for assessment purposes. Similarly, \citet{latif2024fine} compared a fine-tuned BERT model with GPT-3.5 for automatically scoring student answers, finding that GPT-3.5 significantly outperforms BERT in terms of scoring accuracy. However, these studies primarily focus on scoring student responses rather than generating detailed feedback that can guide learning.
\citet{guo2024using} developed a multi-agent system to generate feedback on science education items. However, the study has notable limitations. It does not compare the system’s feedback to that of real-world teachers, leaving its alignment with practical classroom feedback untested. Additionally, the evaluation focuses solely on over-praise and over-inference, neglecting other critical pedagogical aspects such as clarity or specificity. This narrow scope, combined with limited validation for diverse classroom contexts, raises questions about its broader applicability in education.

There remains a significant gap in research regarding the generation of feedback texts for hands-on student experimentation in science education, particularly in benchmarking against real-world teacher feedback and validating critical feedback quality dimensions. Addressing this gap, our study analyzes the performance of LLM-generated feedback on students' experimentation protocols, comparing it to feedback from practicing teachers and science education experts. Our evaluation spans multiple dimensions and incorporates real-world student data.

% we contribute to a deeper understanding of how LLMs can be effectively utilized in science education to provide meaningful, personalized feedback that aligns with pedagogical best practices.


%Feedback with LLMs in education (often Language Learning, no criteria-based feedback)

%Feedback in Science Education (often only assessment? Scientific argumentation)

%--> For experimentation there are well defined error, the feedback needs to make use of them

%% Dann zu Generative AI
%With the advent of using generative AI for automated, computer-based feedback


%LLMs in Science Edu: 

%\cite{zhai2023ai} highlight the importance to discuss the potential of AI in formative assessment in science education, instead of rejecting its impact.

%\cite{zhai2020applying}, " reviewed 49 articles regarding. ML-based science assessment through a triangle framework with technical, validity, and pedagogical features on three vertices", "majority of the studies focused on the validity vertex,"

%\cite{latif2024fine} use  hybrid neural network combing BERT, Attention, Graph Embeddings and LSTMs, automatic scoring of students’ written responses in science education, task:"analyze and interpret data to determine whether substances are the same based on characteristic properties"
% Das Paper ist so eine Katastrophe, aaaah

%\cite{wu2023matching} use a pre-trained BERT to provide scores on students’ written responses using zero-shot prompting in science education.

%LLMs in General education:
%\cite{stamper2024enhancing} analyze previous research on feedback generation in ITS "To fully utilize the potential of generative AI in the educational context, it is essential to approach its integration with pedagogical desing", "grounding the current LLM-based feedback generation in theoretical frameworks and evidence-based approaches"

%\cite{nguyen2023evaluating} feedback generation for math, decimal numbers, finding ChatGPT was overall effective at addressing the errors of the students in its feedback.


%\cite{sessler2023peer} feedback generation for writing, using GPT-3.5 and zero-shot prompting

%for programming:

%\cite{gabbay2024combining} GPT-3.5 and GPT-4 for feedback on code assignments in MOOCs: "The findings point to effective error detection, yet the feedback is often inaccurate"

%\cite{estevez2024evaluation} feedback on student exercises in a university programming course. GPT 3.5 and Bard PaLM, " limitations in their ability to evaluate these particular exercises effectively, specifically finding typical concurrency errors." 

%\cite{koutcheme2024open} introductory programming course, GPT-4, bias toward positively rating feedback, several leading open-source LLMs by using GPT-4 to evaluate the feedback
