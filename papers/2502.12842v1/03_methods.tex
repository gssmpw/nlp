\section{Methodology}

In our study, we conduct a multidimensional comparison of feedback generated by our LLM agent for student errors in real-world experimentation protocol to feedback texts given by teachers and experts in science education. In the following, we introduce our used rating scheme, explain the data collection and automated feedback generation and the analyzes we conducted to evaluate the LLM-generated feedback texts.

\subsection{The Research Context: Students' Errors in Experimentation Protocols}

Planning and conducting investigations, along with analyzing and interpreting data, are considered key scientific practices essential for developing scientific literacy \citep{NationalResearchCouncil2012}. These practices are effectively fostered through inquiry-based learning methods, particularly student-centered experimentation. However, while experimentation is central to scientific inquiry, it demands high levels of scientific reasoning and literacy from students.

Students frequently encounter challenges in planning and conducting experiments, which often result in errors. These errors have been well-documented and empirically confirmed in numerous studies \citep{kranz2023learners}. Common errors include formulating hypotheses that focus on expected observations rather than the dependent variable \citep{baur2018fehler} or neglecting to include control trials \citep{dasgupta2014development, germann1996identifying}.

Building on these documented errors, in a previous study, we developed and validated an LLM-based system to identify 16 common student errors related to experimentation \citep{bewersdorff2023assessing}. 
%For the current study, we analyzed [insert total number] student errors identified in 65 structured experimentation protocols. 
Each protocol included sections for `Hypothesis', `Materials', `Sketch of the Experimental Setup', `Description of the Implementation', `Observation', and `Result'. The protocols were collected from 37 sixth to eighth-grade students attending secondary schools in Germany and are publicly available in the supplementary material of our previous work (see \citet{bewersdorff2023assessing}). 
%To include participants with good, average, and poor performance students were selected based on their summed grades in mathematics, German, and science.
%
The student protocols were based on two experimental contexts:

\begin{itemize}

\item \textbf{Yeast Experiment} Students were tasked with determining the conditions necessary for yeast to produce carbon dioxide (``Find out what yeast needs to produce carbon dioxide''). 
%A common error in this context was formulating a hypothesis without specifying the dependent variable, such as stating "Yeast will produce bubbles when mixed with sugar," without indicating that carbon dioxide production is being measured.

\item \textbf{Pine Cone Experiment} Students explored the factors that cause pine cone scales to close (``Find out what triggers cone scales to close''). 
%An example of an error here was the omission of control variables, where students might not account for environmental factors like humidity or temperature that could affect the results.

\end{itemize}

In total, the 65 experimentation protocols from the two experimental contexts contained 159 errors. We used this dataset as a basis for our study.



\subsection{Feedback collection from human participants}

From the protocol dataset, we selected 40 protocols to create a test dataset containing a total of 109 student errors. The size of the dataset is limited by the inherent challenges of collecting data on student experimentation. Despite its modest size, the dataset was carefully curated to capture a wide range of error types and reflect realistic classroom scenarios \citep{baur2018fehler}. The data was collected from a sample of 37 students in grades six through eight (ages 10–12) across multiple schools. Efforts were made to ensure an approximately equal distribution of genders (boys and girls). Additionally, the sample was designed to ensure diversity regarding performance by including students with high, average, and low performance levels in mathematics, science, and German.

For each error, we collected two human feedback texts to serve as benchmarks for our LLM agent. To achieve this, we engaged 11 science teachers (6 in-service teachers and 5 pre-service teachers in their final semester) and 5 experts in science education who are currently working as university researchers. These two sources provided both real-world feedback from teachers and additional benchmark feedback from domain experts in science education. This selected sample serves as a preliminary foundation for investigating the potential of LLMs in providing feedback on scientific inquiry tasks.

All experts were provided with the student protocol, a list of the identified errors, and general feedback guidelines based on our rating scheme (see Section \ref{sec:rating_scheme}). They also received detailed information about the structure of an experimentation protocol and the tasks assigned to the students. The participation was voluntary, all teachers received a €10 voucher for their support.
% 
% After data collection, we obtained two feedback texts for each error: one written by a teacher and one by an expert in science education.


\begin{figure}[htbp]
    \centering
    \begin{tcolorbox}[colback=gray!5!white, colframe=gray!75!black, width=\textwidth, arc=1mm, auto outer arc, boxrule=0.2mm, title=Prompt for automated feedback generation on experimental protocols]
    \begin{small}
        %\textbf{Context} \\
        You are a science teacher. You will give feedback for students in science class that learn about experimentation. \\

        %\textbf{Protocol} \\
        \textless Task\textgreater{}  + \textless Student Protocol\textgreater \\

        %\textbf{Error} \\
        \textless Occurred Error\textgreater \\

        %\textbf{Instructions} \\
        Follow the guidelines step by step. \\
        1. Reformulate the problem with your own words specific to the given protocol. Do not just repeat the protocol. Write around 100 words. \\
        2. Take the output of step 1 and answer the question: ``why does this specific protocol contain an error?". Be as concrete as possible. Write a new denser feedback containing all information from 1 and 2. \\
        3. Take the output of step 2, answer the question: ``what could the student do to fix the error?" Be as concrete as possible and stay as close at the original protocol as possible. Write a new denser feedback containing all information from 2 and 3. \\
        4. Reformulate the outcome of step 2 and 3 into a feedback that you as a teacher could give the student. Write here around 200 words. Remember speaking to a student. Be very precise. \\
        5. Reformulate the outcome of step 4 in 100 words. Do not remove any content, just write denser. \\
        
        % \textbf{Format} \\
        Return a JSON with keys 1, 2, etc.
    \end{small}
    \end{tcolorbox}
    \caption{Prompt to generate feedback texts on all occurred error in the student experimental protocols.}\label{fig:prompt}
\end{figure}

\subsection{Technical background of the LLM agent}
For automatic feedback generation, we exploited a zero-shot approach, optimizing the prompt for our LLM agent with clear instructions but without requiring prior task-specific examples.
The model was role-prompted to act as a science teacher. For the context, it was then provided with the student's task and the relevant section of the protocol. For example, if the error related to the hypothesis, only the first part of the protocol was included. For errors associated with the result, the entire student-written protocol was provided. The model also received a definition of the specific error identified. Finally, it was instructed to follow a step-by-step approach and return its output in the defined format. The complete prompt is shown in Figure \ref{fig:prompt}.

The prompt was created and refined using 15 protocols selected as train dataset including 50 error of the real-world dataset. The 40 test protocols later used for the comparison with the humans were not considered for the prompt adjustments.
We generated all feedback texts using the OpenAI pipeline, leveraging GPT-3.5 (\texttt{gpt-3.5-turbo-0125}) as baseline LLM. At the time of running the experiments, this version was the most cost-effective option, making it the most practical choice for a classroom setting. 
Notably, since then, the newer GPT-4o Mini model (\texttt{gpt-4o-mini}) has become available, offering even lower costs and improved reasoning capabilities \cite{sessler2024benchmarking}, which could further enhance the feasibility and effectiveness of automated feedback generation in educational contexts.



\subsection{Feedback rating scheme} \label{sec:rating_scheme}

After collecting the feedback tests from humans and our LLM agent, we used a multi-dimensional rating scheme to evaluate them.
Drawing from the theoretical foundations of effective feedback in science education (see \ref{sec:Characteristics_feedback}), we derived a rating scheme based on six dimensions for our analysis. 

According to \citet{hattie2007power}, feedback comprises three core components: \textit{Feed Up}, \textit{Feed Back}, and \textit{Feed Forward}. \textit{Feed Up} clarifies the current goals and learning objectives, ensuring that students understand what is expected of them. \textit{Feed Back} identifies specific errors within the context of the current work, providing students with clear insights into their mistakes. \textit{Feed Forward} suggests possible courses of action to address the identified errors, guiding students on how to improve and proceed with their learning.

In addition to these components, the format of the feedback plays a crucial role in its effectiveness (\ref{sec:Characteristics_feedback}). Therefore, we included three language-related dimensions: \textit{Constructive Tone}, \textit{Linguistic Clarity}, and \textit{Technical Terminology}. A \textit{Constructive Tone} ensures that the feedback is delivered in an encouraging and supportive manner without being overly positive, fostering a positive learning environment. \textit{Linguistic Clarity} involves the use of clear and straightforward language that is appropriate for students in grades 6 to 8, making the feedback easily understandable. Lastly, \textit{Technical Terminology} refers to the appropriate use of subject-specific terms to enhance understanding and ensure that the feedback is both precise and relevant to the scientific context.

Each of these six dimensions was evaluated using a 5-point Likert scale, ranging from \textit{Strongly Disagree} to \textit{Strongly Agree}. This quantitative approach enables us to systematically rate and compare the quality of feedback provided by both human experts in the field of science education and the LLM agent. Table \ref{tab:criteria} provides an overview of the dimensions and their corresponding descriptions.

\begin{table}[htbp]
    \centering
    \caption{Overview of the multidimensional criteria applied to assess the feedback, categorized into content-related (C) and language-related (L) aspects.}
    \label{tab:criteria}
    \begin{tabular}{lcl}
    \toprule
    Criteria & Type & Description  \\
    \midrule
         Feed Up & C & Summarizes the goal to be achieved  \\
         Feed Back & C & Describes the error in the context of the current situation  \\
         Feed Forward & C & Indicates solutions or explains methods to address errors   \\
         Constructive Tone & L & Encouraging without being overly positive  \\
         Linguistic Clarity & L & Uses clear sentence structures suitable for grades 6–8  \\
         Technical Terminology & L & Appropriately employs technical terms for grades 6–8  \\
    \bottomrule
    \end{tabular}
    
\end{table}

In addition to the rating scheme, we also measured the length of the feedback text (i.e. the number of words) to analyze its comprehensiveness and conciseness, since it is important to not overwhelm the students by too verbose feedback \citep{kulhavy1985conjoint}.



\subsection{Expert feedback ratings}
% Final analysis

After the data collecting phase, we presented the three feedback texts (generated by teacher, expert and LLM agent) blinded and shuffled to four human raters, that are professionals in science inquiry as well as in teaching and learning. The criteria for selection as an rater included: 1) a PhD in a natural science or science education, 2) several years of teaching experience with pre-service science teachers, and 3) publications on experimentation as a method of scientific inquiry. % Wie beschreiben wir Armin, Susanne, Pizzi und Pitt? Students perception in experimentation
The raters were provided with a batch of the original protocols and the task to the assess the texts based on the six aspects in Table \ref{tab:criteria}. 

% on a five-point Likert scale from \textit{strongly agree} to \textit{strongly disagree}.

\subsection{Analysis}

To thoroughly compare the feedback texts written by teachers, experts, and the LLM agent, we conducted several analyses, using human-generated feedback as the benchmark for evaluating the agent's performance. First, we assessed the overall performance by averaging all six categories of the rating scheme into a single overall score and analyzing the distribution of these scores across the three groups, providing a general impression of performance (Section \ref{sec:overall_scoring}).

Next, we conducted a detailed aspect-level analysis to examine each dimension of the rating scheme individually. By comparing the mean and variance of scores for each category, we identified significant differences among the feedback sources—teachers, experts, and agent—using independent t-tests (Section \ref{sec:multidim}).
Since feedback length is also a critical factor for adequacy, particularly because LLMs tend to produce verbose responses, we analyzed the number of words in each feedback text to investigate this possible challenge (Section \ref{sec:length}).

To explore the alignment between feedback from the LLM agent, teachers, and experts, we performed correlation analyzes using Spearmans correlation coefficient $\rho$. This helped us determine whether they excelled or struggled with the same texts or showed strengths on distinct examples (Section \ref{sec:corr_analysis}).
Finally, to gain deeper insight into the challenges faced by the AI system, we closely examined an illustrative example where the LLM agent failed to provide valuable feedback while humans succeeded. This qualitative analysis highlights specific areas where the AI system still lags behind, providing a clearer understanding of the remaining issues (Section \ref{sec:qualitative_example}).

