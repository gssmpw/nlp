\section{Related Works}
% \textcolor{red}{have to add something here}
Skin cancer analysis has had various advancements over the years, which have been influential in designing our model's architecture. We highlight and compare other approaches with ours in the following section.

\subsubsection{Skin Lesion Segmentation}
The evolution from thresholding and active contour models ~\cite{hemalatha2018active, ravichandran2009color,yogarajah2010dynamic} to deep neural networks has revolutionized skin lesion segmentation, as seen in the 19-layer fully convolutional network employing Jaccard distance loss~\cite{nasr2019dense} and star shape priors for global structure preservation~\cite{mirikharaji2018star}. With inspiration from~\cite{centernet}, \ours advances segmentation by introducing 2D Gaussian splatting and dual-task learning, ensuring robust and consistent mask generation without reliance on specific priors or loss functions.

\subsubsection{Skin Image Classification}
The advent of deep neural networks has significantly enhanced skin image classification, transitioning from manual feature engineering~\cite{hagerty2019deep} to automated methods like attention residual learning~\cite{zhang2018skin}. \ours employs Vision Transformers (ViT) to enhance classification by capturing long-range dependencies and global context in skin images.

\subsubsection{Multi-task Learning}
Initial multi-task learning approaches like MB-DCNN~\cite{mb_dcnn} and MT-TransUNet~\cite{yu2016automated} showcased the benefits of integrating segmentation and classification tasks but often relied on separate or token-based methods. \ours advances multi-task learning with an end-to-end framework that enhances task interdependence through dual-task consistency and robust mask generation.

\subsubsection{Advanced Vision Transformers in Dermatology}
Vision Transformers (ViTs)~\cite{vit}, originally developed for natural language processing, have redefined the analysis of dermatological images by capturing long-range dependencies and global context. \ours builds on ViTs within the Transformer UNet architecture, leveraging their strengths for dermatological image analysis.

\subsubsection{Task Regularization for Enhanced Learning}
Consistency regularization has proven effective in semi-supervised learning contexts, as demonstrated by MT-TransUNet~\cite{dtc,robust}, which aligns segmentation and classification through dual-task and attended region consistency losses. \ours enhances this strategy with 2D Gaussian splatting and dual-task consistency loss, achieving robust performance without heavy dependence on large labeled datasets.

The development of \ours model stands as a testament to the confluence of deep learning innovations, transformer architectures, and consistency regularization strategies in the domain of skin lesion analysis and pushing the boundaries for obtaining robust and reliable results.
% By incorporating detailed technical advancements from seminal works like MT-TransUNet~\cite{chen2021mt}, this literature review not only situates GS-TransUNet within the current scientific discourse but also highlights its contributions to advancing automated dermatological diagnosis.

%