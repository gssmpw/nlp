
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

% En Yu's settings
\definecolor{linkColor}{rgb}{0.18,0.39,0.62}
% \usepackage[colorlinks=true,linkcolor=linkColor,citecolor=linkColor,filecolor=linkColor,urlcolor=linkColor]{hyperref}       % hyperlinks
% \usepackage{url}            % simple URL typesetting

\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{officeblue}{RGB}{0,102,204}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{mybrickred}{RGB}{182,50,28}
\definecolor{nick_orange}{RGB}{255, 127, 80}
\definecolor{myred}{rgb}{0.992,0.9576,0.932}
\definecolor{mydred}{rgb}{0.992,0.915,0.892}
\definecolor{mypink}{rgb}{1,0.95,0.962}
\definecolor{myyellow}{rgb}{0.99,1,0.78}
\definecolor{myredd}{rgb}{0.992,0.9076,0.63}
\definecolor{mydredd}{rgb}{0.96,0.72,0.72}
\definecolor{mypinkd}{rgb}{0.98,0.75,0.952}

% \usepackage{graphicx} % 加载插图
% \usepackage{subcaption} % 加载 subcaption 包
\usepackage{threeparttable}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{xspace}

% \PassOptionsToPackage{table}{xcolor}
% \usepackage[table]{xcolor}
\usepackage{colortbl}

\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{subcaption} % 加载 subcaption 包
\usepackage{wrapfig}
\usepackage{enumitem}
\setitemize{itemsep=10pt,topsep=0pt,parsep=0pt,partopsep=0pt}
\pdfminorversion=4

\usepackage{pifont}% http://ctan.org/pkg/pifont

\def\eg{{\it{e.g.}}}
\def\etal{{\it{et al.}}}
\def\ie{{\it{i.e.}}}
\def\etc{{\it{etc}}}

\newcommand{\worldwideweb}{\raisebox{-1.5pt}{\includegraphics[height=1.05em]{Figs/icons/internet-icon.pdf}}\xspace}
\newcommand{\fox}{\raisebox{-1.5pt}{\includegraphics[height=1.25em]{Figs/icons/fox.png}}\xspace}

\title{Unhackable Temporal Rewarding for Scalable Video MLLMs}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{En Yu$^1$\textsuperscript{,\P} \quad Kangheng Lin$^2$\textsuperscript{,\P} \quad Liang Zhao$^3$\textsuperscript{,\P} \quad Yana Wei$^4$ \quad Zining Zhu$^5$ \quad Haoran Wei$^3$ \\
\textbf{Jianjian Sun$^3$ \quad Zheng Ge$^3$  \quad Xiangyu Zhang$^3$ \quad Jingyu Wang$^2$$\footnotemark[2]$ \quad Wenbing Tao$^1$$\footnotemark[2]$} \\ 
$^1$Huazhong University of Science and Technology\\
$^2$Beijing University of Posts and Telecommunications \quad $^3$StepFun \\
$^4$Johns Hopkins University \quad $^5$University of Chinese Academy of Sciences \\ \\
% \texttt{\{yuen, wenbingtao\}@hust.edu.cn} \\
 % \texttt{\{wangpeiyi9979, nlp.lilei\}@gmail.com}  \\
 % \texttt{{li.14042}@osu.edu} \quad \texttt{szf@pku.edu.cn}
 {\quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \fox \ \ Project Page: \href{https://Ahnsun.github.io/UTR/}{{\tt\text{Video-UTR}}}}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[2]{Corresponding authors, \textsuperscript{\P} Core contribution}
\renewcommand{\thefootnote}{\arabic{footnote}}


\maketitle
\vspace{-5.5mm}
\begin{abstract}
In the pursuit of superior video-processing MLLMs, we have encountered a perplexing paradox: the “\textit{anti-scaling law}”, where more data and larger models lead to worse performance. This study unmasks the culprit: \textit{“\textbf{temporal hacking}”}, a phenomenon where models shortcut by fixating on select frames, missing the full video narrative. In this work, we systematically establish a comprehensive theory of temporal hacking, defining it from a \textit{reinforcement learning} perspective, introducing the \textit{\textbf{T}emporal \textbf{P}erp\textbf{l}exity (\textbf{TPL})} score to assess this misalignment, and proposing the \textit{\textbf{U}nhackable \textbf{T}emporal \textbf{R}ewarding (\textbf{UTR})} framework to mitigate the temporal hacking. Both theoretically and empirically, TPL proves to be a reliable indicator of temporal modeling quality, correlating strongly with frame activation patterns. Extensive experiments reveal that UTR not only counters temporal hacking but significantly elevates video comprehension capabilities. This work not only advances video-AI systems but also illuminates the critical importance of aligning proxy rewards with true objectives in MLLM development.

 \end{abstract}

\section{Introduction}
\label{intro}

The pursuit of artificial intelligence that emulates human-like reasoning has increasingly emphasized the role of System 2 cognitive processes—deliberate~\citep{r1, o1}, structured~\citep{gpt4o}, and temporally-aware reasoning~\citep{merlin,fei2024video}—in advancing multimodal large language models (MLLMs). While early MLLMs like GPT-4V~\citep{gpt4v}, LLaVAs~\citep{llava, llava1p5} demonstrated remarkable capabilities in static image understanding, their application to video understanding remains constrained by the inherent complexity of spatiotemporal dynamics, long-range context dependencies, and multimodal alignment. This motivates researchers to develop powerful video MLLMs for the open-source community.

The dominant paradigm in video foundation model construction relies on contrastive~\citep{tong2022videomae, feichtenhofer2022masked, Internvideo2} or generative learning~\citep{videollama2, llavanext-video} from extensive video-text pair datasets. However, recent studies have unveiled a counterintuitive “\textit{anti-scaling law}” phenomenon~\citep{pllava}. Practically, increased data volume~\citep{wang2024tarsier} or model parameters~\citep{pllava} leads to performance degradation. Our analysis in Figure~\ref{fig:tp_vs_bmk} also shows that adding more training data decreases temporal modeling performance due to the dilution of high-quality samples. Further investigation reveals models often infer entire captions from a few key frames, typically just the initial (Figure~\ref{fig:attnmap}) or last one (Figure~\ref{fig:temporal_hacking}).
This suggests that current methodologies inadvertently promote a form of \textit{shortcut learning}. Critically, this issue resists resolution through mere data and parameter scaling. Such approaches may, in fact, exacerbate the problem. 

We propose to reframe this issue through the lens of \textit{reinforcement learning} (RL)~\citep{sutton2018reinforcement}. 
The generative modeling of MLLMs on video-text pairs can be formulated as a sequential decision-making process where the model’s policy aims to maximize the expected reward of generating highly relevant text conditioned on video frame context.
This formulation necessitates a critical examination: \textit{Does our proxy reward function (video-text or video-caption pair) adequately approximate the true reward (video-language alignment) we aim to optimize?}
Empirical evidence suggests a significant misalignment. We observe a manifestation of reward hacking~\citep{skalse2022defining} --- termed “\textbf{temporal hacking}” in the context of video LLMs. This predicament mirrors a boat in a racing game, furiously spinning in circles to collect “power-ups” while never advancing towards the finish line~\citep{boat}. 

Escaping the vortex of temporal reward hacking requires a shift in strategy, not merely increased effort. That is, \textbf{\textit{employing a more suitable proxy reward is key}} to overcoming this challenge. To this end, we first investigate the causes of temporal reward hacking and introduce a novel metric, \textit{\textbf{T}emporal \textbf{P}erp\textbf{l}exity (\textbf{TPL})} score, to quantify its severity. Experiments reveal a striking correlation between TP scores and models’ temporal modeling capabilities, with higher TPL scores consistently associated with the activation of more video frames. Our analysis further leads to the proposal of two key principles for designing an effective proxy reward function for video MLLMs: \textit{high frame information density} and \textit{high inter-frame information dynamics}. Guided by these two principles, we further propose an \textit{\textbf{U}nhackable \textbf{T}emporal \textbf{R}eward \textbf{(UTR)}}. UTR leverages \textit{spatiotemporal attributes} and \textit{bidirectional queries} to model video-language alignment. Comprehensive experiments validate that UTR, as an automated and scalable method, effectively achieves unhackable temporal modeling by guiding the model’s observational tendencies across all frames.

Our contributions are threefold:
\begin{itemize}
    \item We provide a novel RL perspective on the video MLLM unscaling phenomenon, systematically establishing \textit{“\textbf{temporal hacking}”} theory as its first comprehensive explanation.
    
    \item We design the \textit{\textbf{T}emporal \textbf{P}erp\textbf{l}exity (\textbf{TPL})} score, and through extensive experiments, TPL has demonstrated a high correlation with the true performance of the model, providing a reliable reference metric for mitigating temporal hacking.
    
    \item Through a series of theoretical and experimental analyses, we propose \textit{two principles} to guide the design of proxy rewards for video-language modeling and further propose \textit{\textbf{U}nhackable \textbf{T}emporal \textbf{R}ewarding (\textbf{UTR})}. Extensive experiments and analyses substantiate the effectiveness of UTR, offering crucial insights into video MLLM temporal modeling.
\end{itemize}

\begin{figure*}[t]
\centering
\includegraphics[width=1.0\linewidth]{Figs/temporal_hacking.pdf}
\vspace{-12mm}
\caption{\textbf{Illustration of temporal hacking.} We select a scene from the \textit{Zootopia} to vividly illustrate the phenomenon of temporal hacking, where the fox is named \textbf{\textit{\textcolor{nick_orange}{Nick}}} and the rabbit is named \textbf{\textit{\textcolor{gray}{Judy}}}. Humans watch videos frame by frame, gradually building an understanding of the content, following a “flow” similar to a Markov process. In contrast, MLLMs process the entire video and its content at once, which can cause them to take shortcuts by focusing only on the most relevant frames.}
\label{fig:temporal_hacking}
\vspace{-6mm}
\end{figure*}

\section{Background \& Example Analysis}
\label{background&example}

\subsection{What is Temporal Hacking?}

\textbf{Reward hacking}~\citep{skalse2022defining, yuan2019novel}, also known as reward exploitation or reward gaming, refers to a phenomenon in reinforcement learning (RL) where an agent discovers a way to maximize its reward signal without actually achieving the intended goal of the task designer. Specifically, we first define a sequential decision problem $M = (S, A, P, R, \gamma)$, typically formalized as a Markov decision process (MDP), where $S$ is the state space, $A$ is the action space, $P: S \times A \times S \rightarrow [0, 1]$ is the transition probability function, $R: S \times A \times S \rightarrow \mathbb{R}$ is the reward function, and $\gamma \in [0, 1]$ is the discount factor. The goal of RL is to find a policy $\pi: S \rightarrow A$ that maximizes the expected cumulative discounted reward:
\vspace{-5mm}

\begin{equation}
\begin{aligned}
&\ J(\pi) = \mathbb{E}_{\tau \sim \pi} \left[ \sum\nolimits_{t=0}^{\infty} \gamma^t R(s_t, a_t) \right], \\
&\ \pi^{*} = \arg\max_{\pi} \mathbb{E}_{\tau \sim \pi} \left[\sum\nolimits_{t=0}^{\infty} \gamma^t R(s_t, a_t) \right], \\
\end{aligned}
\label{eq1}
\vspace{-3mm}
\end{equation}

where $\tau = (s_0, a_0, s_1, a_1, ...)$ is a trajectory generated by following policy $\pi$. $\pi^{*}$ is the optimal policy obtained under the current reward function. Reward hacking occurs when there exists a policy $\pi_h$ (generally $\pi_h = \pi^{*}$ ) such that:
\vspace{-5mm}

\begin{equation}
J(\pi_h) > J(\hat{\pi}), \ \ \text{\textit{however}}, \ \  K(\pi_h) \ll K(\hat{\pi}),
\label{eq2}
\vspace{-3mm}
\end{equation}

where $\hat{\pi}$ is the optimal policy for achieving the intended task, and $K$ denotes the true performance of the policy model in the intended task. In essence, reward hacking indicates an optimization misalignment, leading to policies that achieve high proxy rewards ($J(\pi_h)$) but fail to accomplish the true reward objectives ($K(\pi_h)$).

\textbf{From reward hacking to temporal hacking.} 
Autoregressive video-language modeling~\citep{li2023videochat, video-llama, llavanext-video}, aims to replicate human video comprehension. As illustrated in Figure~\ref{fig:temporal_hacking}, humans sequentially access each video frame, incrementally building an understanding by integrating all prior information~\citep{coltheart1980persistences}. Similarly, the model progressively generates tokens for each frame with the preceding video context conditioned. It is natural to represent this task as a sequential Markov decision process from an RL perspective.

Particularly, given a video frame sequence $ V = \{v_t\}_{t=1}^{T} $(where $T$ is the total number of frames) and a specific time step $t$, the sequence of preceding frames $V_{1:t}$ constitutes the state space, and the corresponding text token $x_t$ forms the action space. During training, the policy $\pi$ sequentially generates tokens $x_t$ conditioned on state $V_{1:t}$. The generated tokens’ quality and relevance to $V_{1:t}$ are evaluated by a reward function $R$, typically measured through the next token’s cross-entropy~\citep{gpt1, gpt2}. The objective can be formalized as:
\vspace{-5mm}

\begin{equation}
J(\pi) = \mathbb{E}_{\tau \sim \pi} \left[ \sum\nolimits_{t=1}^{T} \gamma^t R(V_{1:t}, x_t) \right].
\label{eq3}
\vspace{-3mm}
\end{equation}

By optimizing the policy model based on this objective function $J$, we obtain an optimal policy model $\pi^{*}$ under the current reward function. However, as shown in Figure~\ref{fig:temporal_hacking} and previous works~\citep{pllava,wang2024tarsier}, $\pi^{*}$ often fails to generate text that accurately aligns with video content and user instructions. Instead, the model may optimize the objective by \textit{accessing only a limited number of frames}, leading to shortcut learning. 
% adding some extra descriptions to reward hacking.
This issue, termed \textit{temporal hacking} in this paper, reflects the discrepancy between proxy and true objectives as described by Eq.~\ref{eq2}.

We provide an illustrative example in Figure~\ref{fig:temporal_hacking}, where it can be observed that the model, through temporal hacking, has identified a ``simpler" version of the true reward by focusing only on the last two frames of the video. This learned proxy reward can be highly dangerous in certain situations, leading to completely erroneous video understanding.

\subsection{What causes Temporal Hacking?}
\label{analysis_th}

In this section, we will analyze the causes of temporal hacking phenomenon in video-language modeling from both theoretical and experimental perspectives.

\textbf{Theoretical perspectives.} In reward hacking theory~\citep{skalse2022defining, yuan2019novel}, misalignment between proxy and true objectives ($J(\pi) \neq K(\pi)$) leads to shortcut learning. For video-language modeling, the true objective is to generate spatially and temporally comprehensive descriptions that align with human understanding of the video. However, in practice, the surrogate objective rewards consistency between model predictions and human-annotated captions~\citep{wang2024tarsier} or curated internet content~\citep{Bain21,wang2023internvid}. This discrepancy can result in suboptimal model behavior.

Ideally, as illustrated in Eq.~\ref{eq3}, trajectories $\tau = (V_{1:1}, x_1, ..., V_{1:t}, x_t, ...)$ propagates along every frame in the temporal sequence, implicitly necessitating a textual descriptions comprehensively describe each frame. However, due to frame redundancy and annotation costs, the text is often conditioned only on a subset of frames or aggregated information from multiple frames, especially in some \textit{static} or \textit{low-motion} scenarios. It is particularly challenging to provide a distinct description for each frame.
% we should also discuss how low-motion video also causes reward hacking
Consequently, the policy’s trajectory becomes $\tau = (V_{1:1}, x_1, ..., V_{k:t}, x_t, ...)$ where $V_{k:t}$ represents any frame set satisfying description $x_t$, and is a subset of $V_{1:t}$. The resultant surrogate objective can be expressed as:
\vspace{-5mm}

\begin{equation}
% \vspace{-4mm}
J(\pi) = \mathbb{E}_{\tau \sim \pi} \left[ \sum\nolimits_{t=1, 1 \leq k \leq t}^{T} \gamma^t R(V_{k:t}, x_t) \right].
\label{eq4}
\vspace{-3mm}
\end{equation}

As illustrated in Figure~\ref{fig:temporal_hacking}, optimizing such a proxy is insufficient and prone to deviate from the true objective of comprehensive video understanding. This reward hacking can be quantified by subtracting Eq.~\ref{eq3} from Eq.~\ref{eq4}, yielding $\Delta \mathcal{R}$:
\vspace{-5mm}

\begin{equation} 
\Delta \mathcal{R} = \sum\nolimits_{t=1, 1 \leq k \leq t}^{T} \gamma^t \left( R(V_{1:t}, x_t) - R(V_{k:t}, x_t) \right).
\label{eq5}
\vspace{-3mm}
\end{equation}

From Eq.~\ref{eq5}, it is evident that as $t$ increases, or as the average subset size $k$ increases (indicating that video descriptions can be condensed to fewer frames), the reward gap widens. This elucidates the observed “anti-scaling law” phenomenon in existing video-language models, where performance degrades as video length increases.

\begin{figure*}[t!]
    \centering
    \subfigure[]{
    % width=0.312
        \includegraphics[width=0.38\columnwidth]{Figs/temporal_perxility.pdf}
        \label{fig:tp_vs_bmk}
    }
    \subfigure[]{
    % width=0.632
	\includegraphics[width=0.56\columnwidth]{Figs/attn.pdf}
        \label{fig:attnmap}
    }
    \vspace{-5mm}
        \caption{\textbf{Analysis of the temporal hacking.} \textbf{(a)} shows the relationship between temporal perplexity and true performance. The size of the radius of the circle represents the amount of data. \textbf{(b)} visualizes the attention map illustrating which specific frames the model’s output focuses on.}
        \vspace{-6mm}
        \label{fig:anaysis_tp}
\end{figure*}

\textbf{Experimental perspectives.} To shed light on reward hacking, we propose an extreme perspective to probe $\Delta \mathcal{R}$. We leverage perplexity~\citep{li2024superfiltering} $\mathcal{R}_{ppl}$ to model the cumulative reward between video context and its textual description. Higher similarity correlates with greater cumulative reward and lower model perplexity.
We simulate the true cumulative reward using a fully sampled video sequence as video context. To model an extreme case of proxy cumulative reward, we use a single, randomly sampled keyframe to represent the entire video context (\ie, $k = t$). This simulates a scenario where the model attempts to describe the whole video based on minimal information. The difference between these two rewards is defined in this paper as \textit{temporal perplexity (TPL, defined as $\mathcal{T}_{tpl}$)} or \textit{temporal hackability}. Formally,
\vspace{-5mm}

\begin{equation} 
\mathcal{T}_{tpl} = - \left(\mathcal{R}_{ppl}(V_{1:T}, x_T) - \mathcal{R}_{ppl}(V_{T:T}, x_T)\right).
\label{eq:tp} 
\vspace{-3mm}
\end{equation}

In practice, to avoid distributional shift, we utilize our own MLLM model, trained on the full set of video data, to calculate perplexity. We record the mean negative log-likelihood (NLL) loss across all text tokens for each sample (\ie,  the logarithm of perplexity) to represent $R_{\text{ppl}}$.

By combining Eq.~\ref{eq5} and Eq.~\ref{eq:tp}, we can intuitively infer that, under the same training setup, a lower TPL score indicates a larger $\Delta R$, which in turn leads to a more severe occurrence of temporal hacking. To prove this, we conduct two experiments as shown in Figure~\ref{fig:anaysis_tp} for in-depth analysis of the relation between TPL score and temporal hacking.

% To investigate the impact of temporal perplexity on video-language modeling,

Specifically, we first fine-tuned models using subsets from VideoChat2~\citep{mvbench} data with varying $\mathcal{T}_{tpl}$ ranges and then mixed the data with different TPL. Intuitively, \textit{higher average TPL scores indicate a reduced likelihood of reward hacking, thereby leading to superior video comprehension performance.} Figure~\ref{fig:tp_vs_bmk} corroborates this, showing a significant correlation between video performance and TPL scores across multiple benchmarks, indicating that temporal perplexity effectively measures $\Delta \mathcal{R}$ and even reward hacking. Furthermore, we can also observe that when the TPL score is low, increasing the amount of data does not lead to performance gains, indicating the occurrence of the anti-scaling law phenomenon.

Then we delved deeper by analyzing attention maps of models on identical video-text pairs. Figure~\ref{fig:attnmap} illustrates that models trained on data with higher average-$\mathcal{T}_{tpl}$ activate more frames during inference on these well-described data. Conversely, models with lower-$\mathcal{T}_{tpl}$, due to severe reward hacking and inferior video modeling, activate fewer frames. These experiments demonstrate that our TPL score can effectively reflect the extent of temporal hacking, providing a reliable metric for exploring strategies to address this issue.

\section{Unhackable Temporal Rewarding}
\label{USTM}

\subsection{How to Mitigate Temporal Hacking?}


Section~\ref{background&example} introduces, defines, and analyzes the concept of \textit{temporal hacking}. A novel metric, \textit{temporal perplexity (TPL score)}, is proposed to assess whether the issue of temporal hacking arises in video-language modeling. At this point, the next important question arises: \textit{How can temporal hacking be mitigated or prevented?} Building upon the aforementioned analysis, we first propose two principles to guide the design of an \textbf{unhackable} reward in video-language temporal modeling:

\quad \textit{\textbf{Principle I\label{p1}: High frame information density.} 
% The content of the video text should have an unique correspondence with as many frames as possible.}
The content of the video text should uniquely correspond to as many frames as possible.}

\quad \textit{\textbf{Principle II\label{p2}: High inter-frame information dynamics.} 
% The description corresponding to different frames should exhibit strong correlations and changes continually.}
% The descriptions corresponding to different frames should exhibit strong correlations and continual changes.}
Descriptions for different frames should be coherent and reflect temporal variations and event progression.}
% \quad \textit{\textbf{Principle \mathbb{II}\label{p2}: High inter-frame information variability.} The information corresponding to consecutive frames should emphasize dynamic variability as much as possible.}

The Principle~\hyperref[p1]{I}, as delineated by Eq.~\ref{eq5}, aims to mitigate the $\Delta \mathcal{R}$ by reducing $k$ as discussed in Section~\ref{analysis_th}. This can be accomplished by ensuring each frame of the video is uniquely described. The Principle~\hyperref[p2]{II} emphasizes continuous dynamics, not only to further reduce $k$ and $\Delta \mathcal{R}$, but also to ensure the continuity of policy state transitions in Eq.~\ref{eq3}, thereby enhancing the model’s understanding of real-world physical laws.

Current temporal modeling approaches predominantly focus on maximizing the relevance and consistency of video information (Principle~\hyperref[p2]{II}). However, addressing Principle~\hyperref[p1]{I} remains challenging due to high frame rates and inter-frame redundancy, complicating textual descriptions of individual frames. Advanced techniques such as InternVID~\citep{wang2023internvid} and COSMO~\citep{wang2024cosmo} ameliorate information density to some extent through video interleave formats, yet they still struggle with the high information density of frames and fail to effectively model spatiotemporal dynamics, thus not fully addressing Principle~\hyperref[p2]{II}. Additionally, methods like COSA~\citep{chen2023cosa}, which concatenate image-text pairs to create video data, fail to establish spatiotemporal relationships between frames, entirely violating Principle~\hyperref[p2]{II}.

To simultaneously satisfy the two proposed principles, we further propose the 

\textit{\textbf{U}nhackable \textbf{T}emporal \textbf{R}ewarding \textbf{(UTR)}} to boostrap the video-language modeling. 


\begin{figure*}[t]
\centering
\includegraphics[width=1.0\linewidth]{Figs/UTR.pdf}
\caption{\textbf{Overall pipeline of Unhackable Temporal Rewarding (UTR)}. UTR begins by using a mixture of expert models to extract unique spatiotemporal attributes and employs a tracking algorithm to construct multiple subject trajectories based on confidence levels (data modeling, top). It then performs bidirectional querying of temporal and spatial attributes to generate dialogue data (task modeling, bottom), thereby learning spatiotemporal dynamics.}
\label{fig:UTR}
\vspace{-5mm}
\end{figure*}

\subsection{Unhackable Temporal Rewarding}
\label{UTR}

As validated in Section~\ref{background&example}, suboptimal proxy rewards easily lead to temporal hacking in models. To address this, we propose a novel temporal rewarding method adhering to the aforementioned principles. Our approach, illustrated in Figure~\ref{fig:UTR}, extracts spatiotemporal attributes from video frames (row 1) and uniformly queries them (row 2) to model video-language alignment. This automated and scalable method achieves unhackable temporal modeling by guiding the model’s observational tendencies across all frames.

\textbf{Spatiotemporal attributes are key to representing unique video frame content.} Mitigating temporal hacking is challenging due to high frame rates and information redundancy in videos as mentioned before. We propose extracting \textit{spatiotemporal attributes} (e.g., trajectory, identity, action) to capture relatively independent information from each frame. This approach offers two advantages:

\begin{itemize}[leftmargin=2.5mm]
\setlength{\itemsep}{2pt}
%
\item \textit{Frame-to-frame variations in attributes, especially positional coordinates, enable modeling of frame-specific information, increasing information density (aligning with \textbf{Principle \hyperref[p1]{I})}}.

\item \textit{These attributes function as queries to link information across the video, facilitating learning of spatiotemporal dynamics (aligning with \textbf{Principle \hyperref[p2]{II})}}.

\end{itemize}

Specifically, given a video frame sequence $V = \{v_t\}_{t=1}^{T}$ with the same meaning in Eq.~\ref{eq3}, we extract the attribute information of subjects from each frame as follows:
\vspace{-5mm}

\begin{equation} 
X_{t} = \{x_t^{loc}, x_t^{app}, x_t^{act}\} = F(v_t),
\label{eq7}
\vspace{-3mm}
\end{equation}

where $x_t^{loc}, x_t^{app}, x_{t}^{act}$ indicate the location, appearance, and action information of subjects in frame $v_t$, respectively. Function $F$ extracts this information, using labeled data or specialized models such as GRiT~\citep{wu2022grit} and Grounding DINO~\citep{liu2023grounding}. We then organize this subject information into trajectories corresponding to each subject:
\vspace{-5mm}

\begin{equation} 
\{Y_{i}\}_{i=1}^{N} = \left\{\{y_{i, t}^{tr}, y_{i, t}^{id}, y_{i, t}^{act}\}_{t=1}^{T}\right\}_{i = 1}^{N}  = A(\{X_t\}_{t=1}^{T}),
\label{eq8} 
\vspace{-3mm}
\end{equation}

where $Y_{i}$ is the trajectory of subject $i$ and $N$ is the number of subjects in the video. To be specific, $y_{i, t}^{tr}, y_{i, t}^{id}, y_{i, t}^{act}$ indicate the trajectory, identity, and action information of subject $i$ in frame $v_t$, respectively. Function $A$ associates subjects across frames to form trajectories and identities, typically using tracking algorithms like ByteTrack~\citep{zhang2022bytetrack}.

\textbf{Bidirectional querying explicitly models spatiotemporal dynamics.} Previous methods~\citep{wang2023internvid,wang2024cosmo} modeled relatively dense information by interleaving text with selected frames, yet they neglected the critical spatiotemporal dynamics. Inspired by Merlin~\citep{merlin}, we propose a bidirectional querying mechanism that uses any temporal or spatial attribute to query global spatiotemporal attributes. This approach offers two benefits: 

\begin{itemize}[leftmargin=2.5mm]
\setlength{\itemsep}{2pt}

    \item \textit{Explicit modeling of spatiotemporal attributes forces the model to read each frame, aligning with \textbf{Principle \hyperref[p1]{I}.}}
    \item \textit{The arbitrariness of querying across time and space enhances the model’s understanding of spatiotemporal dynamics, and the stronger this arbitrariness, the deeper the understanding, aligning with \textbf{Principle \hyperref[p1]{II}.}}
    
\end{itemize}

Particularly, we randomly sample the information of one or more subjects as query attributes and select several frames as query frames. The model must predict the complete subject information based on the provided query data. Formally,
\vspace{-5mm}

\begin{equation}
P(Y|V,Y_{q}) \sim P\left(\{y_{s_i}\}_{i=1}^N|\{v_t\}_{t=1}^T, \{y_{s_i,t_j}\}_{i,j}\right),
\label{eq:bidirectional}
\vspace{-3mm}
\end{equation}

where $\{s_i\}_{i=1}^N \subseteq \{1,2,\ldots,N\}$ represents sampled subject identities, $\{t_j\}_{j=1}^M \subseteq \{1,2,\ldots,T\}$ indicates selected query frames, and $y_{s_i,t_j}$ denotes attribute information of selected subjects sampled from $Y$, which can be location, appearance, and action description. 

Notably, the random selection of query frames $\{t_j\}_{j=1}^M$ ensures the model utilizes query information from any part of the video—beginning, middle, or end—as cues to trace the entire trajectory. This approach not only compels the model to fully observe and comprehend the entire video, avoiding shortcuts like relying solely on initial or final frames, but also enhances its understanding of time-dependent physical laws. By necessitating the model to infer states across various temporal intervals, it implicitly learns concepts such as momentum, velocity, and acceleration, thereby strengthening its grasp of fundamental spatiotemporal dynamics.

\section{Empirical Result Details}
\label{exp}

\subsection{Experiment Settings}
\label{exp_setting}

\textbf{Datasets.} We primarily construct UTR-Data using several existing open-source video datasets, namely HowTo100M~\citep{miech2019howto100m}, MeViS~\citep{ding2023mevis}, and LaMOT~\citep{li2024lamot}. To extract subject attributions from each video frame, we use the region-to-text detector GRiT~\citep{wu2022grit}. Subsequently, we apply the ByteTrack~\citep{zhang2022bytetrack} tracking algorithm to construct attribution trajectories. Further details can be found in the Appendix~\ref{exp_detail}.

\textbf{Implementation Details.} To apply our UTR modeling strategy within the current video MLLM, we have developed a novel video MLLM, \ie, \textbf{\textit{Video-UTR}}. For the specific Video-UTR pipeline, we follow the general architecture in LLaVA-NeXT-Video~\citep{llavanext-video}, which consists of a vision encoder, SigLIP-L~\citep{zhai2023sigmoid}, a large language model, QWen-2~\citep{yang2024qwen2}, and a modality alignment projector, 2-layer GeLU-MLP. The training process consists of two stages. (1) \textit{Stage I}: Modality alignment, where only the projector is trained using the $558K$ LLaVA~\citep{llava} dataset. (2) \textit{Stage II}: Multi-task joint training, where the LLM is trained with various task datasets including video instruction-following data. Here, we mainly apply our \textbf{\textit{UTR}} in the \textit{Stage II}, which combines the constructed task data based on UTR with LLaVA-NeXT SFT data. Further details about the training settings can be found in the Appendix~\ref{exp_detail}.

\subsection{General Comprehension Evaluation}
\label{general_sota}

To showcase the generality and effectiveness of the proposed paradigm, we evaluated Video-UTR across various understanding benchmarks. Using the standard MLLM evaluation framework and the LLMs-Eval tool~\citep{zhang2024lmms}, we assessed major image and video understanding tasks. Results are shown in Tables~\ref{tab:general_video} and \ref{tab:general_image}. For video understanding, we focused on three general benchmarks: MVBench~\citep{mvbench}, TempCompass~\citep{tempcompass}, and VideoMME~\citep{videomme}, as well as four video QA benchmarks: MVSD-QA~\citep{mvsd}, MSRVTT-QA~\citep{msrvvt}, TGIF-QA~\citep{tgif}, and ActivityNet-QA~\citep{activitynet}. For image understanding, we reported scores from popular benchmarks like MM-Vet~\citep{mmvet}, MMBench~\citep{mmbench}, MMMU~\citep{mmmu}, MME~\citep{mme}, LLaVA-wild~\citep{llava}, SEED~\citep{seed}, AI2D~\citep{ai2d}, and RealWorldQA~\citep{grok}. For fairness, we used results from original papers.

\textbf{Video Understanding.} Table~\ref{tab:general_video} shows that Video-UTR outperforms other video MLLMs on most benchmarks, ranking first in 4 out of 7 tasks, highlighting its strong video understanding capabilities. Its high scores on MVBench ($58.78\%$), TempCompass ($59.67\%$), and VideoMME ($52.63\%$) demonstrate its ability to handle complex tasks like temporal reasoning, identifying differences, locating objects, tracking motion, and interpreting dynamic scenes. Additionally, its performance on four video QA benchmarks reflects exceptional understanding, particularly in managing temporally sensitive information. Remarkably, Video-UTR achieves these results using only about $1.1M$ video samples, a much smaller dataset compared to other models of similar performance, showcasing the efficiency and effectiveness of our UTR approach.

\textbf{Image Understanding.} Table~\ref{tab:general_image} shows that Video-UTR, despite being a video MLLM, delivers highly competitive performance compared to image-level MLLMs. For instance, on key benchmarks, Video-UTR matches or outperforms top image MLLMs like LLaVA-1.5~\citep{llava} ($39.6\%$ vs. $35.4\%$ on MM-Vet) and the stronger LLaVA-NeXT-Img~\citep{llavanext-video} ($76.6\%$ vs. $72.1\%$ on MMBench). It also performs well on hallucination benchmarks, achieving $88.9\%$ on POPE, and excels in image QA, with $63.7\%$ on RealWorldQA, showing its ability to avoid misidentification and misalignment with irrelevant image details. These results demonstrate that UTR not only helps video MLLMs overcome temporal hacking but also enhances their ability to analyze and understand images effectively.

\input{Tabs/video_benchmark}
\input{Tabs/general_benchmark}

\subsection{Abalation Study about UTR}
\label{ablation}

\textbf{Effectiveness of each component of UTR.} In this ablation study, we evaluate the impact of removing the two key components of UTR: data modeling (UTR-Data) and task modeling (Bidirectional Querying) from Video-UTR. We focus on three major video and image understanding benchmarks. As shown in Table~\ref{tab:ablation}, removing UTR-Data and Bidirectional Querying leads to a significant drop in performance on video understanding tasks, emphasizing their importance in handling complex video reasoning tasks. Notably, removing UTR-Data causes a more consistent and pronounced decline across all benchmarks, including both image and video tasks. This underscores the critical role of \textit{data modeling} in UTR, as it directly aligns with the \textit{two principles} we proposed.

At the same time, to eliminate the potential influence of video data volume, we also add an equivalent amount of VideoChat2~\citep{mvbench} data. It can be observed that the additional video data did not result in further gains, which further underscores the importance of data modeling. Video data constructed in an improper manner will inevitably lead to temporal hacking, thus hindering the improvement of true video understanding performance.

\textbf{Ablation on the scalability of Video-UTR.} In Section~\ref{analysis_th}, we identify that the ``anti-scaling law" phenomenon observed in current video MLLMs is due to the issue of temporal hacking. To address this, we propose UTR as a mitigation strategy. In this experiment, we will demonstrate whether video MLLMs, with the integration of UTR, can exhibit scalability. As shown in Table~\ref{tab:ablation_scale_data} and Table~\ref{tab:ablation_scale_frame}, thanks to the incorporation of UTR, Video-UTR demonstrates a certain degree of scalability in the size of video data. Specifically, the larger the volume of video data, the better the model’s performance. Additionally, we observe that under the condition of unchanged video content, an increased number of video frames does not negatively impact the model’s performance. This scalability is advantageous for further exploring the better performance of Video-UTR in the future.

\input{Tabs/ablation}

\begin{table}[t]
  \centering

\begin{minipage}[t]{0.48\textwidth}
\makeatletter\def\@captype{table}
\input{Tabs/scalable_data}
\end{minipage}
\begin{minipage}[t]{0.49\textwidth}
\makeatletter\def\@captype{table}
\input{Tabs/scalable_frame}
\end{minipage}
\vspace{-5mm}
\end{table}


\subsection{Spatial-temporal Understanding of Video-UTR}
\label{exp_st}

\input{Tabs/ida_bench}


Spatial and temporal comprehension are equally important for multimodal video understanding. Here, we evaluate Video-UTR’s performance in these two areas using the latest benchmark, MM-ID, in a zero-shot setting. MM-ID tests a model’s ability to recognize identities across four increasingly complex levels, focusing on matching and locating objects with different identities across frames. As shown in Table~\ref{tab:ida_bench}, Video-UTR achieved highly competitive scores on both the matching and location sub-metrics without any MM-ID training data. Moreover, it outperformed methods using significantly larger datasets, further demonstrating the strength of the UTR approach. By leveraging spatiotemporal attribute modeling, UTR effectively enables the model to learn both spatial and temporal aspects.

In addition, we rigorously assess the performance of our Video-UTR on the recently introduced and exceptionally demanding long-range video understanding benchmark, MVBench-Video. As illustrated in Table~\ref{tab:mmbench_video}, our Video-UTR exhibits remarkable competitiveness on the performance of video perception and reasoning, surpassing a multitude of models with 34B parameters and even larger architectures, despite its compact 7B parameter size.


\subsection{In-depth Analysis about the TPL Score}
\label{exp_tp}

In Section~\ref{analysis_th}, we design a novel metric, temporal perplexity (TPL score), to measure the alignment degree between the proxy reward and the true reward. In this part, we aim to elucidate the correlation between TP scores and true rewards more intuitively from the perspective of video data quality. Specifically, we randomly select 100 video-text pairs from WebVid~\citep{Bain21} and calculate their temporal perplexity based on the definition in Eq.~\ref{eq:tp}. Then we pick two representative examples to illustrate the relationship between temporal perplexity (TPL) and the quality of video-text pairs.

As shown in Figure~\ref{fig:tpl_data}, it can be observed that \textit{higher TPL score indicates a higher information density in the video or a more detailed description}. In this scenario, the model struggles to describe the entire video using just a single frame. Conversely, if the model’s performance based on a single frame is nearly as good as when using all frames, it either suggests that the video’s dynamics are negligible, making it almost like an image, or the textual description is so sparse that additional video information does not significantly improve modeling. The result aligns with our discussion in Section~\ref{analysis_th} and the analysis in Figure~\ref{fig:anaysis_tp}. This case study demonstrates that the TPL score can serve as a useful metric for filtering high-quality video-text pair data. Please refer to Appendix~\ref{add_exp} for more in-depth investigation. Furthermore, for a more comprehensive and in-depth analysis and discussion of the UTR methodology, please refer to Appendix~\ref{add_disscusion}. 


\section{Related Work}
\label{related_work}

\begin{figure*}[t]
\centering
\includegraphics[width=1.0\linewidth]{Figs/tpl_data.pdf}
\vspace{-8mm}
\caption{\textbf{Quantitative comparison} of the video-text pair with different temporal perplexity}
\label{fig:tpl_data}
\vspace{-6mm}
\end{figure*}

\textbf{Multimodal video foundation models.} Recently, vision-language models~\citep{llava, minigpt4, chatspot, wei2024small, wei2024vary} have demonstrated versatile visual understanding through visual instruction tuning~\citep{llava, minigpt4}. However, real-world video comprehension in multimodal models is still in its early stages. Due to the absence of powerful video encoders in the community, existing mainstream video MLLMs~\citep{llavanext-video, video-llama, videollama2} still rely on established images encoder, \ie, CLIP~\citep{clip}, to extract visual information frame by frame. Subsequently, they integrate temporal modeling techniques, \eg, Q-former~\citep{video-llama}, 3D Conv~\citep{videollama2}, and Pooling~\citep{llavanext-video, pllava}, to compress the visual tokens before feeding them with language tokens into LLMs. 

In addition to advancing the design of powerful temporal modules, recent works have increasingly acknowledged the pivotal role of \textit{video-language modeling} in video comprehension. Some works try to design filtering mechanisms~\citep{wang2024tarsier} to obtain high-quality video data with fine-grained description, while others aim to construct appropriate data structures~\citep{wang2023internvid, wang2024cosmo, chen2023cosa} and task formats~\citep{merlin} to enhance modeling performance. In this work, we systematically present how to design effective video-language modeling from a reinforcement learning perspective and propose guiding principles along with example frameworks.

\textbf{Reward hacking theory} was firstly introduced in the field of RL as a special case of Goodhart's Law~\citep{goodhart1984monetary, leike2018scalable}, and later explored in the context of AI alignment~\citep{leike2017ai}. ~\citep{rewardhacking} formalizes reward hacking by identifying types of reward mis-specifications that lead to it. Subsequent works~\citep{pan2022effects, laidlaw2024preventing} try to deal with reward hacking from different aspects. On the other hand, reward hacking is not exclusive to reinforcement learning, it also occurs in the optimization of pretrained visual generation models, where approaches often optimize towards a reward model by directly backpropagating gradients from a differentiable reward model~\citep{li2024reward, zhang2024large}. In this work, we transfer the concept of reward hacking to video-language modeling and establish a novel \textit{temporal hacking} theory to explain the shortcut learning in the existing video MLLM.

\section{Conclusion}
\label{conclusion}

In this work, we propose the theory of \textbf{\textit{temporal hacking}} from a reinforcement learning perspective to explain shortcut learning in video MLLMs. We introduce a novel metric, \textit{\textbf{T}emporal \textbf{P}erp\textbf{l}exity (\textbf{TPL})}, to quantify the severity of temporal hacking. Through extensive experiments, we use the TPL score to analyze the causes and features of temporal hacking, leading to the development of two guiding \textit{principles} for video-language modeling. We further propose \textit{\textbf{U}nhackable \textbf{V}ideo-Language \textbf{M}odeling (\textbf{UTR})} and build a powerful video MLLM, \ie, \textit{\textbf{Video-UTR}}. We hope this work offers a new perspective and insights to help the community build more robust video-AI systems.

\section*{Acknowledgements}

This work was supported by the National Natural Science Foundation of China under Grant 62176096.


























% \subsubsection*{Acknowledgments}
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.


\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage
\appendix
\input{appendix}


\end{document}
