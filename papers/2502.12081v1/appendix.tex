\section{Appendix}
\label{appendix}

In this appendix, we provide additional details about \textit{temporal hacking} and our \textit{\textbf{U}nhackable \textbf{T}emporal \textbf{R}ewarding (\textbf{UTR})}, which were omitted due to the 10-page limit of the main paper. Specifically, Section~\ref{exp_detail} elaborates on the dataset and training settings of Video-UTR. Section~\ref{add_exp} presents additional experiments to analyze UTR’s characteristics. Section~\ref{more_cases} offers more qualitative examples to demonstrate the capabilities of Video-UTR, and Section~\ref{add_disscusion} provides further discussion of existing approaches.


\section{Additional Details about Experimental Setting}
\label{exp_detail}

\textbf{Additional information of the datasets.} In Section~\ref{UTR} of the manuscript, we introduced how we established the unhackable temporal rewarding (UTR) including \textit{data modeling (UTR-Data) and \textit{task modeling (Bidirectional Querying)}}. Now, in this section, we go into greater detail about how we collected and built the UTR-Data and how we constructed task conversation. To start, we provide an overview of our collected data in Table~\ref{tab:train_data}, and then dive into the step-by-step process of how it was constructed.

\begin{table}[h]
\centering                         % 控制表格居中
\renewcommand{\arraystretch}{1.4}  % 控制行高
\caption{\textbf{Training Data Statistics.} We first build our UTR-Data mainly based on sampled HowTo100M, MeViS, and LaMOT. Then we mix UTR-Data with several existing video conversation data, \ie, LLaVA-NeXT-SFT and VideoChat2.}
\setlength{\tabcolsep}{1.5mm}      % 控制列间距
  \resizebox{1.0\columnwidth}{!}{
\small                             % 控制字体大小
\begin{tabular}{c | c |c c c | c}
\toprule
\textbf{Modality} & \multicolumn{1}{c|}{\textbf{Dataset}} & \textbf{Original} & \textbf{Used} & \textbf{Ratio\%} & Training Stage \\ 
\hline
 & HowTo100M~\citep{miech2019howto100m} & 100M & 50K & 0.05\% & \text{Stage II} \\
 & MeViS~\citep{ding2023mevis}       & 443K & 90K & 20.3\% & \text{Stage II}\\
 & LaMOT~\citep{li2024lamot} & 2.44M  & 225K & 10.5\% & Stage II \\ 
\multirow{-4}{*}{Video-Text} & VideoChat2~\citep{mvbench} & 2M & 100K & 5\% & Stage II \\ 
\midrule
% \rowcolor{gray!10}
 & BLIP-558K~\citep{llava} & 558K & 558K & 100\% & Stage I\\
% \rowcolor{gray!10}
\multirow{-2}{*}{Image-Text} & LLaVA-NeXT-SFT~\citep{llavanext-video} & 790K & 790K & 100\% & Stage II                     \\ 
\hline
% \rowcolor{alicegreen}
% \multirow{2}{*}{Vision-Language} & \multirow{2}{*}{Total} &  & & \\
% & & & & \\
Vision-Language & Total & 106.231M & 1.813M & 1.71\% & Stage I \& II  \\
\bottomrule
\end{tabular}
\vspace{3mm}
}
\label{tab:train_data}
\end{table}

Specifically, we follow the steps below to pre-process the raw video data to construct UTR-Data

\noindent (1) Randomly sample the fixed number ($16$, $24$ or $32$) frames at a certain frame (gap = 3,4 or 5) or random interval to form a video clip each time.

\noindent (2) Extract all spatiotemporal attribution trajectories containing their category, identity, action and bounding boxes in each video clip. This can be accomplished through expert models, \eg, GRiT~\citep{wu2022grit}, Grounding DINO~\citep{liu2023grounding}, and ByteTrack~\citep{zhang2022bytetrack} or directly obtained from the annotations provided by datasets.

\noindent (3) Remove the trajectory containing too small objects (smaller than $1/32$ of the image size).

\noindent (4) Random select observation (spatial or temporal attributions in the randomly selected frame) as the query to conduct bidirectional querying task modeling.

\noindent (5) Compose the task format as the following: 

\texttt{\textit{\textbf{Question}: System prompt + query question}}. 

\texttt{\textit{\textbf{Answer}: query answer, cat1<idi>Frame1:<box>;Frame2:<box>;...</idi>}},

where \texttt{<query question, query answer>} is the question-answer pair that is designed based on the selected querying attributes. 

\textbf{Additional Training Setting Details.} As stated in the manuscript, Video-UTR follows a two-stage training procedure. In this part, we will provide a detailed overview of our training settings, including the hardware used for training, the duration, and the training hyperparameters. All information are recoderd in Table~\ref{tab:hyperparam}.

\begin{table}[t]
    \centering
    \caption{\textbf{Training hyperparameters of Video-UTR}. The hyperparameter placed in the middle indicates that this hyperparameter is used in both stages.}
    % \tablestyle{1pt}{1.0}
    \begin{tabular}{l cc}
         \toprule
         \textbf{Configuration}            & \textbf{Stage I} & \textbf{Stage II} \\
         \midrule
         Machine                  & \multicolumn{2}{c}{NVIDIA Tesla A800 80GB GPU x 64}\\
         Training hours           & 1 hour   & 20 hours\\
         \midrule
         ViT init.                & SigLIP-so400m-patch14-384 & Video-UTR Stage I\\
         LLM init.                & Qwen2-7B-Instruct & Video-UTR Stage I \\
         Projection init.         & random & Video-UTR Stage I \\
         Image resolution         & $384^2$ & $384^2$ \\
         ViT sequence length      & 2048 & 2048 \\
         LLM sequence length      & 32K & 32K\\
         Video Frame length       & 1 & 32 \\
         Optimizer                & \multicolumn{2}{c}{AdamW} \\
         Optimizer hyperparameter & \multicolumn{2}{c}{$\beta_{2}=0.95, eps=1e^{-8}$} \\
         Peak learning rate       & \multicolumn{2}{c}{Vision Tower:  $2e^{-6}$;  LLM: $1e^{-5}$} \\
         Minimum learning rate    & \multicolumn{2}{c}{0} \\
         ViT learning rate decay  & 0.9 & 0 \\
         ViT Drop path rate       & \multicolumn{2}{c}{0} \\
         Learning rate schedule   & \multicolumn{2}{c}{cosine decay} \\
         Weight decay             & \multicolumn{2}{c}{0.05} \\
         Gradient clip            & \multicolumn{2}{c}{1.0} \\
         Training steps           & 1k & 5k \\
         Warm-up ratio            & 0.003 & 0.003 \\
         Global batch size        & 512 & 256 \\
         Gradient Acc.            & 1 & 4 \\
         Numerical precision      & \multicolumn{2}{c}{$\mathtt{bfloat16}$} \\
         Optimizer sharding       & \multicolumn{2}{c}{\ding{51}} \\
         Activation checkpointing & \multicolumn{2}{c}{\ding{55}} \\
         Model parallelism        & \multicolumn{2}{c}{\ding{55}} \\
         Pipeline parallelism     & \multicolumn{2}{c}{\ding{55}} \\
         \bottomrule
    \end{tabular}
    \label{tab:hyperparam}
% \vspace{-4mm}
\end{table}

\textbf{Additional Testing Setting Details.} In the inference and evaluation phase, we essentially follow the settings of PLLaVA~\citep{pllava} and LLaVA-NeXT-Video~\citep{llavanext-video}, including the system prompt for inference, the number of frames extracted, and so on, while conducting evaluations on the existing video benchmark. Specifically, as illustrated in Table~\ref{tab:text_setting}, we mainly use the uniform frame sampling for frame selection. For answer selection based on GPT score, we mainly use the gpt-3.5-turbo-0613 version to evaluate the responses of our model.

\begin{table}[t]
\centering                         % 控制表格居中
\renewcommand{\arraystretch}{1.4}  % 控制行高
\caption{\textbf{Video benchmark evaluation setting.} We report some detailed setting during evaluation. MCQ: Multi-choice question. QA: Question-answer.}
\setlength{\tabcolsep}{1.5mm}      % 控制列间距
  \resizebox{1.0\columnwidth}{!}{
\large                          % 控制字体大小
\begin{tabular}{c | c |c |c | c}
\toprule
\textbf{Benchmark} & \multicolumn{1}{c|}{\textbf{Evaluation type}} & \textbf{Prompts} & \textbf{Input frames} & \textbf{Answer selection} \\ 
\hline
 & MCQ & \textbf{\texttt{Question}} + ``Please directly give the best option:" & 32 & \\
 & Yes or No       &  \textbf{\texttt{Question}} + ``Please answer yes or no:" & 32 & \\
 & Caption Matching &  \textbf{\texttt{Question}} + ``Please directly give the best option:"  & 32 &  \\ 
\multirow{-4}{*}{\makecell{Temoral\\Compass}} & Captioning &  \textbf{\texttt{Question}} & 32 & \multirow{-4}{*}{GPT score} \\ 
\midrule
% \rowcolor{gray!10}
 % & BLIP-558K~\citep{llava} & 558K & 558K & Stage I\\
% \rowcolor{gray!10}
MVBench & Video - MCQ & "Question" +  \textbf{\texttt{Question}} +``Option:" + \textbf{\texttt{Options}} + ``Only give the best option." & 32 & Option matching                     \\ 
\hline
% \rowcolor{alicegreen}
% \multirow{2}{*}{Vision-Language} & \multirow{2}{*}{Total} &  & & \\
& & & & \\
VideoMME & Video-MCQ &   \multirow{-2}{*}{\makecell{These are the frames of a video. Select the best answer to the following multiple-choice \\ question based on the video. Respond with only the letter (A, B, C, or D) of the correct option.}} & 32 & Option matching  \\
\hline
MSVD & Video QA&   \textbf{\texttt{Question}}& 32& GPT Score\\
\hline
ActivityNetQA & Video QA &    \textbf{\texttt{Question}}+ ``Answer the question using a single word or phrase." & 32& GPT Score\\
\hline
TGIFQA & Video QA & \textbf{\texttt{Question}}& 32& GPT Score\\
\hline
VideoChatGPT & Video QA &  \textbf{\texttt{Question}}& 32& GPT Score\\
\bottomrule
\end{tabular}
% \vspace{3mm}
}
\label{tab:text_setting}
\end{table}



\section{Additional Experimental Analysis}
\label{add_exp}

\begin{wrapfigure}{r}{9.5cm} 
    \vspace{-0.2cm} 
    \centering
    \footnotesize
    %		\capbtabbox{
    %			\resizebox{0.9\textwidth}{!}{
    \includegraphics[width=0.67\columnwidth]{Figs/mean_loss.pdf}
    %			}
    %		}
    \vspace{-5mm} 
    {
        \caption{\textbf{Quantitative TPL statistic} of VideoChat2. \vspace{-0.2cm} }
        \label{fig: tp_videochat2}
    }
    % \vspace{-3mm} 
\end{wrapfigure}

\textbf{More analysis about temporal perplexity (TPL).} In the Section~\ref{exp_tp}, we present a case study to illustrate the relationship between the proposed TPL score and data quality, where a higher TPL score indicates better data quality. In this part, we further present the relationship between TPL and data quality from a quantitative statistical perspective. Specifically, we calculate the TPL score for different data subsets in VideoChat2~\citep{mvbench} and computed their average values. The results are shown in Figure~\ref{fig: tp_videochat2}. We can observe that the TPL distribution for the YouCook2~\citep{zhou2018towards} and TextVR~\citep{wu2025large} subset is relatively high. This suggests that these two data subsets are of relatively high quality. As we know, these datasets, such as YouCook2, contain a large amount of first-person perspective and high-motion video data. These videos are rich in high information density and dynamic content, which is beneficial for the model’s temporal modeling. The results further prove that TPL provides a reference for selecting high-quality data from VideoChat2. Based on the TPL distribution, sampling more reasoning data is likely to be more beneficial for achieving better video-language modeling.


\begin{figure*}[t!]
\centering
\includegraphics[width=1.0\linewidth]{Figs/attn_plus1.pdf}
% \vspace{-5mm}
\caption{\textbf{Output attention visualization.} We compute the average output layer attention of the tokens generated by the model for each frame in the QA task and visualized the results.}
\label{fig:attn_plus1}
\vspace{-4mm}
\end{figure*}


\begin{figure*}[t!]
\centering
\includegraphics[width=1.0\linewidth]{Figs/attn_plus2.pdf}
% \vspace{-5mm}
\caption{\textbf{Video-text input attention visualization.} The left is the attention map of the model with low TPL while the right is the attention map with high TPL score.}
\label{fig:attn_plus2}
\vspace{-4mm}
\end{figure*}

\textbf{More attention visualization analysis.} In Figure~\ref{fig:attnmap}, we present the attention map visualizations of frame tokens under model responses at different TPL levels. In this part, we further provide more detailed attention analysis. As shown in Figure~\ref{fig:attn_plus1} \&~\ref{fig:attn_plus2}, we conduct two forms of attention visualization. The first involves video QA, visualizing the attention values between the answer content and the tokens of each video frame. The second form calculates the self-attention when inputting the video-text pair into the model simultaneously. From both visualization results, we can observe that our Video-UTR, while achieving a higher TPL score, clearly attends to more frames, thereby avoiding the loss of crucial details in the video and making the answers more accurate and detailed.

\begin{figure*}[t!]
\centering
\includegraphics[width=1.0\linewidth]{Figs/demo_case.pdf}
% \vspace{-5mm}
\caption{\textbf{Qualitative examples visualization} of Video-UTR. Please note that we only display the most important frames from the full video (32 frames) to conserve space.}
\label{fig:demo_case}
% \vspace{-4mm}
\end{figure*}



\section{Additional Qualitative Analysis}
\label{more_cases}

In this section, to more intuitively demonstrate the unhackable capability of Video-UTR, we present several subjective video Q\&A cases, as shown in Figure~\ref{fig:demo_case}. Compared to our baseline, LLaVA-Next-Video, our Video-UTR demonstrates a more accurate video understanding capability, specifically by better comprehending user queries, focusing on more video details, and providing more precise and less hallucinated responses. These results further validate the effectiveness of our proposed UTR modeling.


\section{More Disscusions}
\label{add_disscusion}

\subsection{Dependency on Expert Models for UTR}
\label{add_utr}

As introduced in Section~\ref{UTR} of the main text, UTR leverages existing expert models to extract spatiotemporal attribute cues, which serve as the foundation for data modeling and task modeling. Therefore, the capability of the selected expert models and the quality of the extracted attributes are critical variables that significantly influence the effectiveness of UTR modeling. In this part, we will delve into the significance of selecting expert models, the selection criteria, the details of attribute extraction, and the validation of attribute quality.

\textbf{The importance and rationale behind selecting expert models for attribute extraction.} The use of expert models to support MLLM training has become a widely adopted strategy in the current development stage. Notable implementations include models such as PaLI-X~\citep{chen2023pali}, Qwen-VL~\citep{qwen}, InternVL~\citep{chen2024internvl}, and LLaMA3.2-Vision~\citep{llama3.2}, which integrate domain-specific expert models spanning areas like detection, grounding, and OCR to scale up training data annotation. The effectiveness of this approach has been well-validated through extensive empirical studies. Fundamentally, these pipelines operate as a distillation process, transferring knowledge from expert models to MLLMs to enhance specific capabilities, such as fine-grained perception. In line with this paradigm, our proposed UTR framework employs expert models to extract spatiotemporal attributes from video data, thereby strengthening the spatiotemporal perception abilities of video MLLMs. This improvement is substantiated by the empirical results presented in Table~\ref{exp_st} of our manuscript.

\textbf{Extraction and filtering of high-quality attributes.} To select specific expert models, we conducted a systematic evaluation based on existing benchmarks, \eg, COCO~\citep{coco}, Lvis~\citep{gupta2019lvis}, VG~\citep{krishna2017visual}, \etc., of the performance of various options, such as GRiT~\citep{wu2022grit} and GroundingDINO~\citep{liu2023grounding}, to identify the most suitable candidates. For the proposed spatiotemporal attributes—including bounding boxes, captions, identities, and actions, as illustrated in Figure~\ref{fig:UTR} of our manuscript—we implemented a multi-stage selection and filtering process. \textbf{First}, we filtered the attributes based on the \textit{confidence scores} provided by the expert models. \textbf{Next}, we applied a multi-object tracking algorithm, \ie, ByteTrack~\citep{zhang2022bytetrack}, to analyze contextual correlations within the video content. This analysis included examining factors such as the Intersection over Union (IoU) of bounding boxes across frames and trajectory continuity metrics, ensuring that trajectory lengths exceeded predefined thresholds. This comprehensive process ensures the reliability and consistency of the extracted attribute trajectories, thereby enhancing their overall quality and utility.

\begin{wraptable}{r}{0.57\textwidth}
    \caption{\textbf{Human validation} of extracted attributes.}
    \vspace{-1em}
    \label{tab:human_quality}
    \begin{center}        
    \def\arraystretch{1}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|ccc}
        \toprule
        \bf Validation & \bf Location & \bf Description & \bf Consistency \\
        \hline
        Human & 2.98 & 2.23 & 2.57\\
        \bottomrule
    \end{tabular}
    \vspace{-20em}
    }
    \end{center}
\end{wraptable}

\textbf{Human validation of the extracted attributes.} To further validate the effectiveness of the extracted spatiotemporal attributes from video data, we conducted a human evaluation experiment. Specifically, 100 data samples generated using our UTR pipeline were randomly selected for assessment by human evaluators.
Human annotators will score these data based on three criteria: the accuracy of the subject bounding box, the correctness of the attribute descriptions, and the consistency of the attribute trajectories, using a scoring range of 1 to 3. The results is shown in Table~\ref{tab:human_quality}. We can observe that the average quality score of the extracted attributes is quite high, indicating a strong level of reliability. The results of this evaluation highlight the robustness and high quality of both the extracted spatiotemporal attributes and the constructed data, confirming the reliability of our pipeline.

\subsection{Consistency of TPL with Human Judgment}
\label{add_tpl}

\begin{table}[h]
    \caption{\textbf{Consistency between TPL score and human judgment}.}
    \vspace{-1em}
    \label{tab:tpl_consis}
    \begin{center}        
    % \def\arraystretch{1}
    \resizebox{0.9\linewidth}{!}{
    \begin{tabular}{c|ccc ccc cc}
        \toprule
          \multirow{2}{*}{\textbf{Validation}} & \multicolumn{2}{c}{High} &&\multicolumn{2}{c}{Medium} &&\multicolumn{2}{c}{Low}\\
    \cmidrule{2-3}  \cmidrule{5-6}  \cmidrule{8-9}
    
        & \bf Richness & \bf Relevance  && \bf Richness & \bf Relevance &&  \bf Richness & \bf Relevance\\
        \hline
        TPL level & 3 & 3 && 2 & 2 && 1 & 1\\
        Human & 2.85 & 2.76 && 2.15 & 1.85 && 1.61 & 1.64\\
        \bottomrule
    \end{tabular}
    \vspace{-10em}
    }
    \end{center}
        \vspace{-2em}
\end{table}

In Section~\ref{exp_tp}, we point out that TPL not only reflects the degree of temporal hacking in the video-language modeling process, but it can also serve as a high-order metric to indicate the quality of video-text pairs. In this part, we plan to further explore this issue by examining the consistency of TPL with human judgment, highlighting the reliability of TPL score as a data filtering metric.

Specifically, we first randomly select 100 video-text pairs from VideoChat2~\citep{li2023videochat} and calculate their temporal perplexity based on the definition in Eq.~\ref{eq:tp}. Next, we sort the data by their TPL values and divide it into three groups: high, medium, and low. We then invite several human annotators to rate these sampled video-text pairs on a scale of 1 to 3. The criteria for scoring includes two aspects, \ie, the richness of the video-text information (considering both information density and dynamics) and the relevance of the video to the text. Based on the annotators’ scores, the consistency can be evaluated based on the average human ratings and their alignment with the level categories. Before the human annotators begin the annotation process, we provide a detailed annotation guideline, which explains the scoring criteria and standards comprehensively and includes relevant references. Specifically, the two composite metrics, \ie, richness and relevance, are defined as follows:

\begin{itemize}[leftmargin=2.5mm]
\setlength{\itemsep}{2pt}

    \item \textbf{Richness:}
        \begin{itemize}[leftmargin=2.5mm]
        \setlength{\itemsep}{2pt}
            \item Frame information density, referring to the degree to which each frame corresponds to an independent description.
            \item Level of descriptive detail, referring to the richness of details included from the video.
            \item The richness of motion information, including the extent of motion in both the subject and the scene.
        \end{itemize}
    
    \item \textbf{Relevance:}
        \begin{itemize}[leftmargin=2.5mm]
        \setlength{\itemsep}{2pt}
            \item The relevance between the video and the text, specifically, the degree to which the description corresponds to the video content.
            \item The relevance of the video context, specifically, the extent to which the descriptions of relevant subjects in the video reflect dynamic changes.
        \end{itemize}
    
\end{itemize}

Follow the above standards, annotators conducted the validation of the sampled cases and ultimately summarized their assessments into scores for two composite metrics. The results is shown in Table~\ref{tab:tpl_consis}. We can observe that the groupings based on TPL scores and those based on human judgments are generally consistent. This indicates that our proposed TPL score is a reliable metric for filtering high-quality video-text pair data.

\subsection{Failure Case Analysis of UTR}
\label{failure_case}

\begin{figure*}[t!]
\centering
\includegraphics[width=1.0\linewidth]{Figs/Failure_case.pdf}
% \vspace{-5mm}
\caption{\textbf{Failure case visualization} of Video-UTR. We select two representative failure cases from the VideoMME~\citep{videomme} benchmark.}
\label{fig:failure_case}
\vspace{-4mm}
\end{figure*}

Although our proposed UTR significantly mitigates temporal hacking from both data modeling and task modeling perspectives, it still has limitations in some situation, and we identify several representative examples on the VideoMME~\citep{videomme} benchmark. As illustrated in Figure~\ref{fig:failure_case}, the top case shows that Video-UTR does not perform as well on certain knowledge-oriented Video MCQ tasks. This type of question tests the inherent knowledge base of large language models, so our UTR method does not result in a significant improvement. The bottom case illustrates that in scenarios where the answer can be determined by analyzing a single frame or a few frames, our UTR method does not demonstrate a significant advantage. Placing more emphasis on the overall video content does not provide notable benefits in addressing such questions.

The aforementioned failure cases analysis also highlights the need to design better video understanding benchmarks that can more reasonably and reliably evaluate the ability of video MLLMs to observe and comprehend the overall video content, rather than relying heavily on the inherent capabilities of LLMs.

\subsection{Limitaion and Furture Work.}

\textbf{Limitation of Unhackable Temporal Hacking.} Although our proposed UTR significantly mitigates temporal hacking from both data modeling and task modeling perspectives, it has a noticeable limitation in terms of its reliance on expert model accuracy. Since UTR modeling is based on extracted subject attributes, the quality of these attributes—such as positional accuracy, precise descriptions of the subject’s appearance and actions, and the accuracy of trajectory associations—directly impacts the overall performance of the final model. Therefore, improving the quality of these extracted subject attributes represents a highly valuable direction for future improvement. 

\textbf{Future work.} On the other hand, seamlessly integrating the constructed attribute trajectories into dialogues poses yet another challenging issue. Exploring whether a single multimodal large language model~\citep{merlin, dreamllm, peng2024dreambench++} can be utilized to handle the entire data processing and task construction pipeline is a highly promising research direction. In addition, the establishment of a more explicit temporal rewarding mechanism represents a valuable research direction. Leveraging reinforcement learning algorithms, \eg, DPO~\citep{rafailov2024direct, zhu2025perpo}, PPO~\citep{schulman2017proximal} and GRPO~\citep{r1}, for post-training enhancement of the model's video comprehension and reasoning capabilities constitutes a key focus for future research.