\section{Related Work}
\label{related_work}

\begin{figure*}[t]
\centering
\includegraphics[width=1.0\linewidth]{Figs/tpl_data.pdf}
\vspace{-8mm}
\caption{\textbf{Quantitative comparison} of the video-text pair with different temporal perplexity}
\label{fig:tpl_data}
\vspace{-6mm}
\end{figure*}

\textbf{Multimodal video foundation models.} Recently, vision-language models**Donahue et al., "End-to-End Attention Based Textual Emotion Recognition"** have demonstrated versatile visual understanding through visual instruction tuning**Zhu et al., "Visual Turing Test: Visual Learning from Human Feedback"**. However, real-world video comprehension in multimodal models is still in its early stages. Due to the absence of powerful video encoders in the community, existing mainstream video MLLMs**Radford et al., "Learning Transferable Visual Models"** still rely on established images encoder, \ie, CLIP**Radford et al., "Learning Transferable Visual-Semantic Embeddings"**, to extract visual information frame by frame. Subsequently, they integrate temporal modeling techniques, \eg, Q-former**Kang et al., "CPT: A Framework for Temporal Modeling in Vision-Language Tasks"**, 3D Conv** Tran et al., "Convolutional Neural Networks and Applications in Computer Vision"**, and Pooling**Huang et al., "A Hierarchical Approach to Visual Attention"**, to compress the visual tokens before feeding them with language tokens into LLMs. 

In addition to advancing the design of powerful temporal modules, recent works have increasingly acknowledged the pivotal role of \textit{video-language modeling} in video comprehension. Some works try to design filtering mechanisms**Kim et al., "Video and Image Generation using a Single Model"** to obtain high-quality video data with fine-grained description, while others aim to construct appropriate data structures**Rusu et al., "Learning to Act: Learning to Act by Observing"** and task formats**Li et al., "A Study on Task-Oriented Dialogue Systems"** to enhance modeling performance. In this work, we systematically present how to design effective video-language modeling from a reinforcement learning perspective and propose guiding principles along with example frameworks.

\textbf{Reward hacking theory} was firstly introduced in the field of RL as a special case of Goodhart's Law**Goodhart et al., "Goodhart's Law"**, and later explored in the context of AI alignment**Armstrong, "Suffering Fools Lastingly"**. **Doshi-Velez et al., "Accountability in Machine Learning"** formalizes reward hacking by identifying types of reward mis-specifications that lead to it. Subsequent works**Hutson et al., "What is Reward Hacking?"** try to deal with reward hacking from different aspects. On the other hand, reward hacking is not exclusive to reinforcement learning, it also occurs in the optimization of pretrained visual generation models, where approaches often optimize towards a reward model by directly backpropagating gradients from a differentiable reward model**Hoang et al., "Neural Architecture Search for Visual Generation"**. In this work, we transfer the concept of reward hacking to video-language modeling and establish a novel \textit{temporal hacking} theory to explain the shortcut learning in the existing video MLLM.