[
  {
    "index": 0,
    "papers": [
      {
        "key": "llava",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual instruction tuning"
      },
      {
        "key": "minigpt4",
        "author": "Deyao Zhu and Jun Chen and Xiaoqian Shen and Xiang Li and Mohamed Elhoseiny",
        "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"
      },
      {
        "key": "chatspot",
        "author": "Zhao, Liang and Yu, En and Ge, Zheng and Yang, Jinrong and Wei, Haoran and Zhou, Hongyu and Sun, Jianjian and Peng, Yuang and Dong, Runpei and Han, Chunrui and others",
        "title": "Chatspot: Bootstrapping multimodal llms via precise referring instruction tuning"
      },
      {
        "key": "wei2024small",
        "author": "Wei, Haoran and Kong, Lingyu and Chen, Jinyue and Zhao, Liang and Ge, Zheng and Yu, En and Sun, Jianjian and Han, Chunrui and Zhang, Xiangyu",
        "title": "Small language model meets with reinforced vision vocabulary"
      },
      {
        "key": "wei2024vary",
        "author": "Wei, Haoran and Kong, Lingyu and Chen, Jinyue and Zhao, Liang and Ge, Zheng and Yang, Jinrong and Sun, Jianjian and Han, Chunrui and Zhang, Xiangyu",
        "title": "Vary: Scaling up the vision vocabulary for large vision-language model"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "llava",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual instruction tuning"
      },
      {
        "key": "minigpt4",
        "author": "Deyao Zhu and Jun Chen and Xiaoqian Shen and Xiang Li and Mohamed Elhoseiny",
        "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "llavanext-video",
        "author": "Zhang, Yuanhan and Li, Bo and Liu, haotian and Lee, Yong jae and Gui, Liangke and Fu, Di and Feng, Jiashi and Liu, Ziwei and Li, Chunyuan",
        "title": "LLaVA-NeXT: A Strong Zero-shot Video Understanding Model"
      },
      {
        "key": "video-llama",
        "author": "Zhang, Hang and Li, Xin and Bing, Lidong",
        "title": "Video-llama: An instruction-tuned audio-visual language model for video understanding"
      },
      {
        "key": "videollama2",
        "author": "Cheng, Zesen and Leng, Sicong and Zhang, Hang and Xin, Yifei and Li, Xin and Chen, Guanzheng and Zhu, Yongxin and Zhang, Wenqi and Luo, Ziyang and Zhao, Deli and others",
        "title": "VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "clip",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
        "title": "Learning transferable visual models from natural language supervision"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "video-llama",
        "author": "Zhang, Hang and Li, Xin and Bing, Lidong",
        "title": "Video-llama: An instruction-tuned audio-visual language model for video understanding"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "videollama2",
        "author": "Cheng, Zesen and Leng, Sicong and Zhang, Hang and Xin, Yifei and Li, Xin and Chen, Guanzheng and Zhu, Yongxin and Zhang, Wenqi and Luo, Ziyang and Zhao, Deli and others",
        "title": "VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "llavanext-video",
        "author": "Zhang, Yuanhan and Li, Bo and Liu, haotian and Lee, Yong jae and Gui, Liangke and Fu, Di and Feng, Jiashi and Liu, Ziwei and Li, Chunyuan",
        "title": "LLaVA-NeXT: A Strong Zero-shot Video Understanding Model"
      },
      {
        "key": "pllava",
        "author": "Xu, Lin and Zhao, Yilin and Zhou, Daquan and Lin, Zhijie and Ng, See Kiong and Feng, Jiashi",
        "title": "Pllava: Parameter-free llava extension from images to videos for video dense captioning"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "wang2024tarsier",
        "author": "Wang, Jiawei and Yuan, Liping and Zhang, Yuchen",
        "title": "Tarsier: Recipes for Training and Evaluating Large Video Description Models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "wang2023internvid",
        "author": "Wang, Yi and He, Yinan and Li, Yizhuo and Li, Kunchang and Yu, Jiashuo and Ma, Xin and Li, Xinhao and Chen, Guo and Chen, Xinyuan and Wang, Yaohui and others",
        "title": "Internvid: A large-scale video-text dataset for multimodal understanding and generation"
      },
      {
        "key": "wang2024cosmo",
        "author": "Wang, Alex Jinpeng and Li, Linjie and Lin, Kevin Qinghong and Wang, Jianfeng and Lin, Kevin and Yang, Zhengyuan and Wang, Lijuan and Shou, Mike Zheng",
        "title": "COSMO: COntrastive Streamlined MultimOdal Model with Interleaved Pre-Training"
      },
      {
        "key": "chen2023cosa",
        "author": "Chen, Sihan and He, Xingjian and Li, Handong and Jin, Xiaojie and Feng, Jiashi and Liu, Jing",
        "title": "Cosa: Concatenated sample pretrained vision-language foundation model"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "merlin",
        "author": "Yu, En and Zhao, Liang and Wei, Yana and Yang, Jinrong and Wu, Dongming and Kong, Lingyu and Wei, Haoran and Wang, Tiancai and Ge, Zheng and Zhang, Xiangyu and others",
        "title": "Merlin: Empowering multimodal llms with foresight minds"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "goodhart1984monetary",
        "author": "Goodhart, Charles Albert Eric",
        "title": "Monetary theory and practice: The UK experience"
      },
      {
        "key": "leike2018scalable",
        "author": "Leike, Jan and Krueger, David and Everitt, Tom and Martic, Miljan and Maini, Vishal and Legg, Shane",
        "title": "Scalable agent alignment via reward modeling: a research direction"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "leike2017ai",
        "author": "Leike, Jan and Martic, Miljan and Krakovna, Victoria and Ortega, Pedro A and Everitt, Tom and Lefrancq, Andrew and Orseau, Laurent and Legg, Shane",
        "title": "AI safety gridworlds"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "rewardhacking",
        "author": "DeepMind",
        "title": "Specification Gaming: The Flaw in the Reward"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "pan2022effects",
        "author": "Pan, Alexander and Bhatia, Kush and Steinhardt, Jacob",
        "title": "The effects of reward misspecification: Mapping and mitigating misaligned models"
      },
      {
        "key": "laidlaw2024preventing",
        "author": "Laidlaw, Cassidy and Singhal, Shivam and Dragan, Anca",
        "title": "Preventing reward hacking with occupancy measure regularization"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "li2024reward",
        "author": "Li, Jiachen and Feng, Weixi and Chen, Wenhu and Wang, William Yang",
        "title": "Reward guided latent consistency distillation"
      },
      {
        "key": "zhang2024large",
        "author": "Zhang, Yinan and Tzeng, Eric and Du, Yilun and Kislyuk, Dmitry",
        "title": "Large-scale reinforcement learning for diffusion models"
      }
    ]
  }
]