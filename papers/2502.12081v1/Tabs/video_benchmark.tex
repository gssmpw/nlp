\begin{table*}[t!]
\renewcommand{\arraystretch}{1.2}
  \centering
  \caption{\textbf{General Video Understanding Performance Comparsion} on 7 benchmarks, Video-UTR outperforms competitors in 4 out of 7 benchmarks and ranks second on the others, despite these competitors using larger training datasets or more parameters. Several benchmark names are abbreviated due to space limits. TempC: Tempcompass, ANet-QA: ActivityNet-QA. And Acc indicates Accuracy. The best results are \textbf{bold} and the second-best results are \underline{underlined}. $*$ indicates metrics reproduced by ourselves for evaluation cause original paper does not report.}
  % \vspace{-3mm}
  \setlength{\tabcolsep}{1.05mm}{
  \resizebox{1.0\columnwidth}{!}{
    \begin{threeparttable} %添加此处
    % \scriptsize
    % \small
    \begin{tabular}{l c c | c  c c c c c c c c c c}
    \toprule
 	\multirow{2}{*}{Methods} & \multirow{2}{*}{LLM} & \text{Data} & \multirow{2}{*}{\textbf{MVBench}} & \multirow{2}{*}{\textbf{TempC}}  & \multirow{2}{*}{\textbf{VideoMME}} &  \multicolumn{2}{c} {\textbf{MSVD-QA}} &  \multicolumn{2}{c} {\textbf{MSRVVT-QA}} &  \multicolumn{2}{c} {\textbf{TGIF-QA}} &  \multicolumn{2}{c} {\textbf{ANet-QA}} \\
   \cmidrule(rl){7-8} \cmidrule(rl){9-10} \cmidrule(rl){11-12} \cmidrule(rl){13-14}

    & & \text{Scale} & & & & \text{Acc} & \text{Score}  & \text{Acc} & \text{Score} & \text{Acc} & \text{Score} & \text{Acc} & \text{Score}\\
    \midrule
     % \multicolumn{11}{c} {LLaVA-NEXT-7B} \\
     % \midrule
    VideoChat~(\citeyear{li2023videochat}) & Vicuna-7B & $765K$ & 35.5 & $-$ & $-$ & 56.3 & 2.8 & 45.0 & 2.5 & 34.4 & 2.3 &  $-$ & 2.2 \\
    VideoChat2 (\citeyear{mvbench}) & Vicuna-7B & $1.9M$ & 51.1 & 38.5 &  $-$ & 70.0 & 3.9 & 54.1 & 3.3 & $-$ & $-$ & 49.1 & \underline{3.3} \\
    Video-ChatGPT (\citeyear{maaz2023video}) & Vicuna-7B& $765K$ & 32.7& 31.8&  $-$ & 51.6 & 2.5 & 29.6 & 1.8 & $-$ & $-$ & 12.4 & 1.1 \\
    Video-LLaVA (\citeyear{lin2023video}) & Vicuna-7B & $765K$ & 34.1 & 34.8& 39.9& 64.9 & 3.3 & 49.3 & 2.8 & 51.4 &3.0 & 35.2 & 2.7 \\
    VideoLLaMA2 (\citeyear{videollama2}) & LLaMA2-7B & $13.4M$ & 54.6 & $-$ & 46.6 & 70.9&\underline{3.8} & $-$ & $-$ & $-$ & $-$ & 50.2& \underline{3.3}\\
    PLLaVA (\citeyear{pllava}) & LLaMA2-7B & $1M$ & 46.6& $-$ &$-$ & \bf 76.6& \bf 4.1& \bf 62.0& \underline{3.5}& \bf 77.5& \bf 4.1& \underline{56.3}& 
 \bf3.5 \\
    LLaVA-NeXT-Video (\citeyear{llavanext-video}) & Qwen2-7B & $860K$ & 54.6 & $-$& 33.7 & 67.8& 3.5 & $-$& $-$& $-$& $-$& 53.5& 3.2\\
    LLaVA-OneVision(\citeyear{li2024llava}) & Qwen2-7B & $1.6M$ & \underline{56.7} & \underline{$59.0^{*}$}& \bf 58.2 & $65.3^{*}$ & \underline{$3.8^{*}$} & $43.3^{*}$ & $3.0^{*}$ & $52.8^{*}$ & $3.4^{*}$ & \bf$56.6^{*}$& \underline{$3.3^{*}$} \\
    \midrule
    \rowcolor{mydred}
    Video-UTR (\textbf{Ours}) & Qwen2-7B & $1.1M$ & \bf 58.8& \bf 59.7& \underline{52.6}&  \underline{73.5}& \bf 4.1& \underline{58.3}& \bf 3.6& \underline{56.4}& \underline{3.6}& 55.0 & 3.2\\
    \bottomrule
    \end{tabular}
    \end{threeparttable}}
}
  \label{tab:general_video}%
  \vspace{-3mm}
\end{table*}%