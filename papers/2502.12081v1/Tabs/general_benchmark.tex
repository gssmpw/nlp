\begin{table*}[t!]
\renewcommand{\arraystretch}{1.2}
  \centering
  \caption{\textbf{General Image Understanding Performance Comparision} on 9 benchmarks, Video-UTR achieves performance comparable to, or even surpassing, that of pure image-level MLLMs. LLaVA\textsuperscript{W}: LLaVA in the wild. The best results are \textbf{bold} and the second-best results are \underline{underlined}.}
  % \vspace{-3mm}
  \setlength{\tabcolsep}{1.05mm}{
  \resizebox{0.97\columnwidth}{!}{
    \begin{threeparttable} %添加此处
    % \scriptsize
    % \small
    \begin{tabular}{l c | c c c c c c c c c}
    \toprule

 	\multirow{2}{*}{Methods} & \multirow{2}{*}{LLM} & \multirow{2}{*}{\textbf{MM-Vet}} & \multirow{2}{*}{\textbf{MMBench}} & \multirow{2}{*}{\textbf{MMMU}} &  \multirow{2}{*}{\textbf{MME}}  & \multirow{2}{*}{\textbf{LLaVA}\textsuperscript{W}} & \multirow{2}{*}{\textbf{POPE}} & \multirow{2}{*}{\textbf{SEED}} & \multirow{2}{*}{\textbf{AI2D}} & \multirow{2}{*}{\textbf{RealWorldQA}} \\

    & & & & & & & & &\\
    \midrule
    \textit{Image-level MLLM} &&&&&&&&&\\
     InstructBLIP~(\citeyear{instructblip}) & Vicuna-7B & 33.1 & 36.0& 30.6   & 1137.1  & 59.8   &  86.1 & 53.4 & 40.6 & 36.9\\ 
     Qwen-VL-Chat~(\citeyear{bai2023qwen}) & Qwen-7B & \bf 47.3 &60.6 &  37.0    & 1467.8  &  67.7  &  74.9 &  58.2 & 63.0 & 49.3\\
     % \multicolumn{11}{c} {LLaVA-v1.5-7B} \\
     % \midrule
      LLaVA-v1.5-7B~(\citeyear{llava1p5}) & Vicuna-7B & 30.5 & 64.3&  35.7   & 1510.7 & 61.8  & 86.1 & 58.6 & 55.5 & 54.8\\
     LLaVA-v1.5-13B & Vicuna-13B& 35.4 & 67.7 &  37.0   & 1531.3 & 66.1   & 88.4  &   61.6 & 61.1 & 55.3 \\
      ShareGPT4V~(\citeyear{chen2023sharegpt4v}) & Vicuna-7B & 37.6 & 68.8 & 37.2 & \underline{1567.4} & 72.6 & 86.6 & \underline{69.7} & 58.0 & 54.9 \\
     LLaVA-NeXT-Img~(\citeyear{llavanext-video}) & LLaMA3-8B & \underline{44.4} & \underline{72.1} & 41.7 & 1551.5 & 63.1 & 87.1 & $-$ & 71.6 & 60.0 \\ 
     \midrule
     \textit{Video-level MLLM} &&&&&&&&&\\
     % \multicolumn{11}{c} {LLaVA-NEXT-7B} \\
     % \midrule
     LLaMA-VID~(\citeyear{llamavid}) & Vicuna-7B & $-$ & 66.6 & $-$  & 1521.4 & $-$ & 86.0  & 59.9 & $-$ & $-$ \\
     Video-LLaVA~(\citeyear{lin2023video}) & Vicuna-7B & 32.0 & 60.9 & $-$  & $-$ & \underline{73.1} & 84.4  & $-$ & $-$ & $-$ \\
    LLaVA-NeXT-Video~(\citeyear{llavanext-video}) & QWen2-7B & 42.9 & 74.5 & \underline{42.6}  & 1580.1 & \bf 75.9 & \underline{88.7}  & 74.6 & \underline{71.9} & \underline{60.1} \\
    \rowcolor{mydred}
    Video-UTR (\textbf{Ours}) & Qwen2-7B & 39.6 & \bf 76.6 & \bf 43.4  & \bf 1583.6 & 69.4 & \bf88.9  &  \bf 74.7 & \bf 72.1 & \bf63.7\\
    \bottomrule
    \end{tabular}

    \end{threeparttable}}
}
  \label{tab:general_image}%
\vspace{-6mm}
\end{table*}%