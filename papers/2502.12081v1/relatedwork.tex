\section{Related Work}
\label{related_work}

\begin{figure*}[t]
\centering
\includegraphics[width=1.0\linewidth]{Figs/tpl_data.pdf}
\vspace{-8mm}
\caption{\textbf{Quantitative comparison} of the video-text pair with different temporal perplexity}
\label{fig:tpl_data}
\vspace{-6mm}
\end{figure*}

\textbf{Multimodal video foundation models.} Recently, vision-language models~\citep{llava, minigpt4, chatspot, wei2024small, wei2024vary} have demonstrated versatile visual understanding through visual instruction tuning~\citep{llava, minigpt4}. However, real-world video comprehension in multimodal models is still in its early stages. Due to the absence of powerful video encoders in the community, existing mainstream video MLLMs~\citep{llavanext-video, video-llama, videollama2} still rely on established images encoder, \ie, CLIP~\citep{clip}, to extract visual information frame by frame. Subsequently, they integrate temporal modeling techniques, \eg, Q-former~\citep{video-llama}, 3D Conv~\citep{videollama2}, and Pooling~\citep{llavanext-video, pllava}, to compress the visual tokens before feeding them with language tokens into LLMs. 

In addition to advancing the design of powerful temporal modules, recent works have increasingly acknowledged the pivotal role of \textit{video-language modeling} in video comprehension. Some works try to design filtering mechanisms~\citep{wang2024tarsier} to obtain high-quality video data with fine-grained description, while others aim to construct appropriate data structures~\citep{wang2023internvid, wang2024cosmo, chen2023cosa} and task formats~\citep{merlin} to enhance modeling performance. In this work, we systematically present how to design effective video-language modeling from a reinforcement learning perspective and propose guiding principles along with example frameworks.

\textbf{Reward hacking theory} was firstly introduced in the field of RL as a special case of Goodhart's Law~\citep{goodhart1984monetary, leike2018scalable}, and later explored in the context of AI alignment~\citep{leike2017ai}. ~\citep{rewardhacking} formalizes reward hacking by identifying types of reward mis-specifications that lead to it. Subsequent works~\citep{pan2022effects, laidlaw2024preventing} try to deal with reward hacking from different aspects. On the other hand, reward hacking is not exclusive to reinforcement learning, it also occurs in the optimization of pretrained visual generation models, where approaches often optimize towards a reward model by directly backpropagating gradients from a differentiable reward model~\citep{li2024reward, zhang2024large}. In this work, we transfer the concept of reward hacking to video-language modeling and establish a novel \textit{temporal hacking} theory to explain the shortcut learning in the existing video MLLM.