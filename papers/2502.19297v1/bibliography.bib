
% The citations apart from the template start here
@article{temp,
    doi = {10.1371/journal.pone.0172395},
    author = {Tampuu, Ardi AND Matiisen, Tambet AND Kodelja, Dorian AND Kuzovkin, Ilya AND Korjus, Kristjan AND Aru, Juhan AND Aru, Jaan AND Vicente, Raul},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Multiagent cooperation and competition with deep reinforcement learning},
    year = {2017},
    month = {04},
    volume = {12},
    url = {https://doi.org/10.1371/journal.pone.0172395},
    pages = {1-15},
    abstract = {Evolution of cooperation and competition can appear when multiple adaptive agents share a biological, social, or technological niche. In the present work, we study how cooperation and competition emerge between autonomous agents that learn by reinforcement while using only their raw visual input as the state representation. In particular, we extend the Deep Q-Learning framework to multiagent environments to investigate the interaction between two learning agents in the well-known video game Pong. By manipulating the classical rewarding scheme of Pong we show how competitive and collaborative behaviors emerge. We also describe the progression from competitive to collaborative behavior when the incentive to cooperate is increased. Finally, we show how learning by playing against another adaptive agent, instead of against a hard-wired algorithm, results in more robust strategies. The present work shows that Deep Q-Networks can become a useful tool for studying decentralized learning of multiagent systems coping with high-dimensional environments.},
    number = {4},

}

@inproceedings{dreduction,
  title={Using dimensionality reduction to exploit constraints in reinforcement learning},
  author={Bitzer, Sebastian and Howard, Matthew and Vijayakumar, Sethu},
  booktitle={2010 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  address={Taipei, Taiwan},
  publisher={IEEE},
  pages={3219--3225},
  year={2010},
  organization={IEEE}
}


@incollection{van2012rrlsurvey,
  title={Solving relational and first-order logical markov decision processes: A survey},
  author={van Otterlo, Martijn},
  booktitle={Reinforcement learning: State-of-the-art},
  pages={253--292},
  year={2012},
  address={Berlin, Heidelberg},
  publisher={Springer}
}

@inproceedings{kvarnstrom2011planning,
  title={Planning for loosely coupled agents using partial order forward-chaining},
  author={Kvarnstr{\"o}m, Jonas},
  booktitle={Proceedings of the International Conference on Automated Planning and Scheduling},
  publisher={ICAPS},
  address={Freiburg, Germany},
  volume={21},
  pages={138--145},
  year={2011}
}

@inproceedings{veloso1990nonlinear,
  title={Nonlinear planning with parallel resource allocation},
  author={Veloso, Manuela M and P{\'e}rez, M Alicia and Carbonell, Jaime G},
  booktitle={Proceedings of the DARPA Workshop on Innovative Approaches to Planning, Scheduling, and Control},
  pages={207--212},
  year={1990},
  organization={Morgan Kaufmann San Diego, CA}
}
  % Citation for Markov Games
@incollection{markovgame,
title = {Markov games as a framework for multi-agent reinforcement learning},
editor = {William W. Cohen and Haym Hirsh},
booktitle = {Machine Learning Proceedings 1994},
publisher = {Morgan Kaufmann},
address = {San Francisco (CA)},
pages = {157-163},
year = {1994},
isbn = {978-1-55860-335-6},
doi = {https://doi.org/10.1016/B978-1-55860-335-6.50027-1},
url = {https://www.sciencedirect.com/science/article/pii/B9781558603356500271},
author = {Michael L. Littman},
abstract = {In the Markov decision process (MDP) formalization of reinforcement learning, a single adaptive agent interacts with an environment defined by a probabilistic transition function. In this solipsis-tic view, secondary agents can only be part of the environment and are therefore fixed in their behavior. The framework of Markov games allows us to widen this view to include multiple adaptive agents with interacting or competing goals. This paper considers a step in this direction in which exactly two agents with diametrically opposed goals share an environment. It describes a Q-learning-like algorithm for finding optimal policies and demonstrates its application to a simple two-player game in which the optimal policy is probabilistic.}
}

@article{bellman1956dynamic,
  title={Dynamic Programming},
  author={Bellman, Richard and RAND Corp Santa Monica CA},
  year={1956}
}
@article{sampleefficientMARL,
    doi = {10.1371/journal.pone.0291545},
    author = {Kim, Jung In AND Lee, Young Jae AND Heo, Jongkook AND Park, Jinhyeok AND Kim, Jaehoon AND Lim, Sae Rin AND Jeong, Jinyong AND Kim, Seoung Bum},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Sample-efficient multi-agent reinforcement learning with masked reconstruction},
    year = {2023},
    month = {09},
    volume = {18},
    url = {https://doi.org/10.1371/journal.pone.0291545},
    pages = {1-14},
    abstract = {Deep reinforcement learning (DRL) is a powerful approach that combines reinforcement learning (RL) and deep learning to address complex decision-making problems in high-dimensional environments. Although DRL has been remarkably successful, its low sample efficiency necessitates extensive training times and large amounts of data to learn optimal policies. These limitations are more pronounced in the context of multi-agent reinforcement learning (MARL). To address these limitations, various studies have been conducted to improve DRL. In this study, we propose an approach that combines a masked reconstruction task with QMIX (M-QMIX). By introducing a masked reconstruction task as an auxiliary task, we aim to achieve enhanced sample efficiency—a fundamental limitation of RL in multi-agent systems. Experiments were conducted using the StarCraft II micromanagement benchmark to validate the effectiveness of the proposed method. We used 11 scenarios comprising five easy, three hard, and three very hard scenarios. We particularly focused on using a limited number of time steps for each scenario to demonstrate the improved sample efficiency. Compared to QMIX, the proposed method is superior in eight of the 11 scenarios. These results provide strong evidence that the proposed method is more sample-efficient than QMIX, demonstrating that it effectively addresses the limitations of DRL in multi-agent systems.},
    number = {9},

}


@inproceedings{sampleGene,
 author = {Li, Gen and Chi, Yuejie and Wei, Yuting and Chen, Yuxin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {15353--15367},
 publisher = {Curran Associates, Inc.},
 title = {Minimax-Optimal Multi-Agent RL in Markov Games With a Generative Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/62b4fea131cfd5b7504eae356b75bbd8-Paper-Conference.pdf},
 volume = {35},
 address={New Orleans},
 year = {2022}
}

@article{weld1994pocl,
  title={An introduction to least commitment planning},
  author={Weld, Daniel S},
  journal={AI magazine},
  volume={15},
  number={4},
  pages={27--27},
  year={1994}
}
@inproceedings{kokel2021reprel,
  title={Reprel: Integrating relational planning and reinforcement learning for effective abstraction},
  author={Kokel, Harsha and Manoharan, Arjun and Natarajan, Sriraam and Ravindran, Balaraman and Tadepalli, Prasad},
  booktitle={Proceedings of the International Conference on Automated Planning and Scheduling},
  address={Guangzhou, China},
  publisher={ICAPS},
  volume={31},
  pages={533--541},
  year={2021}
}

@inproceedings{maddpg,
author = {Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, Pieter and Mordatch, Igor},
title = {Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {6382–6393},
numpages = {12},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@InProceedings{Qmix,
  title = 	 {{QMIX}: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning},
  author =       {Rashid, Tabish and Samvelyan, Mikayel and Schroeder, Christian and Farquhar, Gregory and Foerster, Jakob and Whiteson, Shimon},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {4295--4304},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  address={Stockholm, Sweden},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/rashid18a/rashid18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/rashid18a.html},
  abstract = 	 {In many real-world settings, a team of agents must coordinate their behaviour while acting in a decentralised way. At the same time, it is often possible to train the agents in a centralised fashion in a simulated or laboratory setting, where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a network that estimates joint action-values as a complex non-linear combination of per-agent values that condition only on local observations. We structurally enforce that the joint-action value is monotonic in the per-agent values, which allows tractable maximisation of the joint action-value in off-policy learning, and guarantees consistency between the centralised and decentralised policies. We evaluate QMIX on a challenging set of StarCraft II micromanagement tasks, and show that QMIX significantly outperforms existing value-based multi-agent reinforcement learning methods.}
}


@inproceedings{prr,
author = {Wen, Ying and Yang, Y and Luo, R and Wang, J and Pan, Wei},
year = {2019},
month = {05},
pages = {},
title = {Probabilistic recursive reasoning for multi-agent reinforcement learning}
}



@article{ctde,
  title={Multi-agent reinforcement learning as a rehearsal for decentralized planning},
  author={Landon Kraemer and Bikramjit Banerjee},
  journal={Neurocomputing},
  year={2016},
  volume={190},
  pages={82-94},
  url={https://api.semanticscholar.org/CorpusID:207113073}
}

@inbook{maven,
author = {Mahajan, Anuj and Rashid, Tabish and Samvelyan, Mikayel and Whiteson, Shimon},
title = {MAVEN: Multi-Agent Variational Exploration},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Centralised training with decentralised execution is an important setting for cooperative deep multi-agent reinforcement learning due to communication constraints during execution and computational tractability in training. In this paper, we analyse value-based methods that are known to have superior performance in complex environments [43]. We specifically focus on QMIX [40], the current state-of-the-art in this domain. We show that the representational constraints on the joint action-values introduced by QMIX and similar methods lead to provably poor exploration and suboptimality. Furthermore, we propose a novel approach called MAVEN that hybridises value and policy-based methods by introducing a latent space for hierarchical control. The value-based agents condition their behaviour on the shared latent variable controlled by a hierarchical policy. This allows MAVEN to achieve committed, temporally extended exploration, which is key to solving complex multi-agent tasks. Our experimental results show that MAVEN achieves significant performance improvements on the challenging SMAC domain [43].},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {684},
numpages = {12}
}

  @INPROCEEDINGS{nlpIntro,
  author={Sharma, Akanksha Rai and Kaushik, Pranav},
  booktitle={2017 International Conference on Computing, Communication and Automation (ICCCA)}, 
  title={Literature survey of statistical, deep and reinforcement learning in natural language processing}, 
  year={2017},
  volume={},
  number={},
  pages={350-354},
  doi={10.1109/CCAA.2017.8229841}}

@ARTICLE{drivingIntro,
  author={Kiran, B Ravi and Sobh, Ibrahim and Talpaert, Victor and Mannion, Patrick and Sallab, Ahmad A. Al and Yogamani, Senthil and Pérez, Patrick},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={Deep Reinforcement Learning for Autonomous Driving: A Survey}, 
  year={2022},
  volume={23},
  number={6},
  pages={4909-4926},
  doi={10.1109/TITS.2021.3054625}}


@Article{marlRobotics,
AUTHOR = {Orr, James and Dutta, Ayan},
TITLE = {Multi-Agent Deep Reinforcement Learning for Multi-Robot Applications: A Survey},
JOURNAL = {Sensors},
VOLUME = {23},
YEAR = {2023},
NUMBER = {7},
ARTICLE-NUMBER = {3625},
URL = {https://www.mdpi.com/1424-8220/23/7/3625},
PubMedID = {37050685},
ISSN = {1424-8220},
ABSTRACT = {Deep reinforcement learning has produced many success stories in recent years. Some example fields in which these successes have taken place include mathematics, games, health care, and robotics. In this paper, we are especially interested in multi-agent deep reinforcement learning, where multiple agents present in the environment not only learn from their own experiences but also from each other and its applications in multi-robot systems. In many real-world scenarios, one robot might not be enough to complete the given task on its own, and, therefore, we might need to deploy multiple robots who work together towards a common global objective of finishing the task. Although multi-agent deep reinforcement learning and its applications in multi-robot systems are of tremendous significance from theoretical and applied standpoints, the latest survey in this domain dates to 2004 albeit for traditional learning applications as deep reinforcement learning was not invented. We classify the reviewed papers in our survey primarily based on their multi-robot applications. Our survey also discusses a few challenges that the current research in this domain faces and provides a potential list of future applications involving multi-robot systems that can benefit from advances in multi-agent deep reinforcement learning.},
DOI = {10.3390/s23073625}
}
@INPROCEEDINGS{marlTraffic,
  author={Prabuchandran K.J. and Hemanth Kumar A.N and Bhatnagar, Shalabh},
  booktitle={17th International IEEE Conference on Intelligent Transportation Systems (ITSC)}, 
  title={Multi-agent reinforcement learning for traffic signal control}, 
  year={2014},
  volume={},
  number={},
  pages={2529-2534},
  doi={10.1109/ITSC.2014.6958095}}

@ARTICLE{marlResource,
  author={Cui, Jingjing and Liu, Yuanwei and Nallanathan, Arumugam},
  journal={IEEE Transactions on Wireless Communications}, 
  title={Multi-Agent Reinforcement Learning-Based Resource Allocation for UAV Networks}, 
  year={2020},
  volume={19},
  number={2},
  pages={729-743},
  doi={10.1109/TWC.2019.2935201}}


@inproceedings{vdn,
author = {Sunehag, Peter and Lever, Guy and Gruslys, Audrunas and Czarnecki, Wojciech Marian and Zambaldi, Vinicius and Jaderberg, Max and Lanctot, Marc and Sonnerat, Nicolas and Leibo, Joel Z. and Tuyls, Karl and Graepel, Thore},
title = {Value-Decomposition Networks For Cooperative Multi-Agent Learning Based On Team Reward},
year = {2018},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We study the problem of cooperative multi-agent reinforcement learning with a single joint reward signal. This class of learning problems is difficult because of the often large combined action and observation spaces. In the fully centralized and decentralized approaches, we find the problem of spurious rewards and a phenomenon we call the "lazy agent'' problem, which arises due to partial observability. We address these problems by training individual agents with a novel value-decomposition network architecture, which learns to decompose the team value function into agent-wise value functions.},
booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {2085–2087},
numpages = {3},
keywords = {value-decomposition, collaborative, reinforcement learning, neural networks, dqn, q-learning, multi-agent},
location = {Stockholm, Sweden},
series = {AAMAS '18}
}

@inproceedings{coma,
author = {Foerster, Jakob N. and Farquhar, Gregory and Afouras, Triantafyllos and Nardelli, Nantas and Whiteson, Shimon},
title = {Counterfactual Multi-Agent Policy Gradients},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {Many real-world problems, such as network packet routing and the coordination of autonomous vehicles, are naturally modelled as cooperative multi-agent systems. There is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems. To this end, we propose a new multi-agent actor-critic method called counterfactual multi-agent (COMA) policy gradients. COMA uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents' policies. In addition, to address the challenges of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent's action, while keeping the other agents' actions fixed. COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass. We evaluate COMA in the testbed of StarCraft unit micromanagement, using a decentralised variant with significant partial observability. COMA significantly improves average performance over other multi-agent actor-critic methods in this setting, and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {363},
numpages = {9},
location = {New Orleans, Louisiana, USA},
series = {2018AAAI'18/IAAI'18/EAAI'18}
}

@InProceedings{icarte2018rm,
  title = 	 {Using Reward Machines for High-Level Task Specification and Decomposition in Reinforcement Learning},
  author =       {Icarte, Rodrigo Toro and Klassen, Toryn and Valenzano, Richard and McIlraith, Sheila},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2107--2116},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  address = {Stockholm, Sweden},
  pdf = 	 {http://proceedings.mlr.press/v80/icarte18a/icarte18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/icarte18a.html},
  abstract = 	 {In this paper we propose Reward Machines {—} a type of finite state machine that supports the specification of reward functions while exposing reward function structure to the learner and supporting decomposition. We then present Q-Learning for Reward Machines (QRM), an algorithm which appropriately decomposes the reward machine and uses off-policy q-learning to simultaneously learn subpolicies for the different components. QRM is guaranteed to converge to an optimal policy in the tabular case, in contrast to Hierarchical Reinforcement Learning methods which might converge to suboptimal policies. We demonstrate this behavior experimentally in two discrete domains. We also show how function approximation methods like neural networks can be incorporated into QRM, and that doing so can find better policies more quickly than hierarchical methods in a domain with a continuous state space.}
}

@inproceedings{wang2010rpomdp,
  title={Relational partially observable MDPs},
  author={Wang, Chenggang and Khardon, Roni},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={24},
  address={Atlanta, Georgia, USA},
  publisher={AAAI Press},
  pages={1153--1158},
  year={2010}
}

@inproceedings{benchmark_marl,
 author = {Papoudakis, Georgios and Christianos, Filippos and Sch\"{a}fer, Lukas and Albrecht, Stefano},
 booktitle = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks},
 editor = {J. Vanschoren and S. Yeung},
 pages = {},
 publisher = {Curran},
 title = {Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in Cooperative Tasks},
 url = {https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/a8baa56554f96369ab93e4f3bb068c22-Paper-round1.pdf},
 volume = {1},
 year = {2021}
}

@article{hmarl,
author = {Ghavamzadeh, Mohammad and Mahadevan, Sridhar and Makar, Rajbala},
title = {Hierarchical Multi-Agent Reinforcement Learning},
year = {2006},
issue_date = {September 2006},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {13},
number = {2},
issn = {1387-2532},
url = {https://doi.org/10.1007/s10458-006-7035-4},
doi = {10.1007/s10458-006-7035-4},
abstract = {In this paper, we investigate the use of hierarchical reinforcement learning (HRL) to speed up the acquisition of cooperative multi-agent tasks. We introduce a hierarchical multi-agent reinforcement learning (RL) framework, and propose a hierarchical multi-agent RL algorithm called Cooperative HRL . In this framework, agents are cooperative and homogeneous (use the same task decomposition). Learning is decentralized, with each agent learning three interrelated skills: how to perform each individual subtask, the order in which to carry them out, and how to coordinate with other agents. We define cooperative subtasks to be those subtasks in which coordination among agents significantly improves the performance of the overall task. Those levels of the hierarchy which include cooperative subtasks are called cooperation levels . A fundamental property of the proposed approach is that it allows agents to learn coordination faster by sharing information at the level of cooperative subtasks , rather than attempting to learn coordination at the level of primitive actions. We study the empirical performance of the Cooperative HRL algorithm using two testbeds: a simulated two-robot trash collection task, and a larger four-agent automated guided vehicle (AGV) scheduling problem. We compare the performance and speed of Cooperative HRL with other learning algorithms, as well as several well-known industrial AGV heuristics. We also address the issue of rational communication behavior among autonomous agents in this paper. The goal is for agents to learn both action and communication policies that together optimize the task given a communication cost. We extend the multi-agent HRL framework to include communication decisions and propose a cooperative multi-agent HRL algorithm called COM-Cooperative HRL . In this algorithm, we add a communication level to the hierarchical decomposition of the problem below each cooperation level . Before an agent makes a decision at a cooperative subtask , it decides if it is worthwhile to perform a communication action. A communication action has a certain cost and provides the agent with the actions selected by the other agents at a cooperation level . We demonstrate the efficiency of the COM-Cooperative HRL algorithm as well as the relation between the communication cost and the learned communication policy using a multi-agent taxi problem.},
journal = {Autonomous Agents and Multi-Agent Systems},
month = {sep},
pages = {197–229},
numpages = {33},
keywords = {Cooperative multi-agent systems, Communication, Hierarchical reinforcement learning, Coordination}
}

@inproceedings{dfoci,
author = {Natarajan, Sriraam and Tadepalli, Prasad and Altendorf, Eric and Dietterich, Thomas G. and Fern, Alan and Restificar, Angelo},
title = {Learning First-Order Probabilistic Models with Combining Rules},
year = {2005},
isbn = {1595931805},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1102351.1102428},
doi = {10.1145/1102351.1102428},
abstract = {First-order probabilistic models allow us to model situations in which a random variable in the first-order model may have a large and varying number of parent variables in the ground ("unrolled") model. One approach to compactly describing such models is to independently specify the probability of a random variable conditioned on each individual parent (or small sets of parents) and then combine these conditional distributions via a combining rule (e.g., Noisy-OR). This paper presents algorithms for learning with combining rules. Specifically, algorithms based on gradient descent and expectation maximization are derived, implemented, and evaluated on synthetic data and on a real-world task. The results demonstrate that the algorithms are able to learn the parameters of both the individual parent-target distributions and the combining rules.},
booktitle = {Proceedings of the 22nd International Conference on Machine Learning},
pages = {609–616},
numpages = {8},
location = {Bonn, Germany},
series = {ICML '05}
}

@inproceedings{Diettrich2000,
 author = {Dietterich, Thomas},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 pages = {},
 publisher = {MIT Press},
 title = {State Abstraction in MAXQ Hierarchical Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e5a4d6bf330f23a8707bb0d6001dfbe8-Paper.pdf},
 volume = {12},
 year = {1999},
 address = {Denver}
}

@inproceedings{holler2020hddl,
  title={HDDL: An extension to PDDL for expressing hierarchical planning problems},
  author={H{\"o}ller, Daniel and Behnke, Gregor and Bercher, Pascal and Biundo, Susanne and Fiorino, Humbert and Pellier, Damien and Alford, Ron},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  publisher={AAAI},
  address={New York City},
  pages={9883--9891},
  year={2020}
}

@article{mcdermott1998pddl,
  title={PDDL-the planning domain definition language},
  author={McDermott, Drew and Ghallab, Malik and Howe, Adele and Knoblock, Craig and Ram, Ashwin and Veloso, Manuela and Weld, Daniel and Wilkins, David},
  year={1998},
  publisher={New Haven, CT}
}
@inproceedings{iql,
author = {Tan, Ming},
title = {Multi-Agent Reinforcement Learning: Independent versus Cooperative Agents},
year = {1993},
isbn = {1558603077},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the Tenth International Conference on International Conference on Machine Learning},
pages = {330–337},
numpages = {8},
location = {Amherst, MA, USA},
series = {ICML'93}
}

@article{dqn,
  title={Human-level control through deep reinforcement learning},
  author={Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Andrei A. Rusu and Joel Veness and Marc G. Bellemare and Alex Graves and Martin A. Riedmiller and Andreas Kirkeby Fidjeland and Georg Ostrovski and Stig Petersen and Charlie Beattie and Amir Sadik and Ioannis Antonoglou and Helen King and Dharshan Kumaran and Daan Wierstra and Shane Legg and Demis Hassabis},
  journal={Nature},
  year={2015},
  volume={518},
  pages={529-533},
  url={https://api.semanticscholar.org/CorpusID:205242740}
}
@inproceedings{shop,
  title={SHOP: Simple Hierarchical Ordered Planner},
  author={Dana S. Nau and Yue Cao and Amnon Lotem and Hector Mu{\~n}oz-Avila},
  booktitle={International Joint Conference on Artificial Intelligence},
  year={1999},
  address={Stockholm, Sweden},
  publisher={IJCAI},
  pages={968--973},
  url={https://api.semanticscholar.org/CorpusID:2329216}
}

@inproceedings{shapley1953,
  title={Stochastic games},
  author={Shapley, Lloyd S},
  journal={Proceedings of the national academy of sciences},
  volume={39},
  number={10},
  pages={1095--1100},
  year={1953},
  publisher={National Acad Sciences}
}

@article{oliehoek2008decpomdp,
  title={Optimal and approximate Q-value functions for decentralized POMDPs},
  author={Oliehoek, Frans A and Spaan, Matthijs TJ and Vlassis, Nikos},
  journal={Journal of Artificial Intelligence Research},
  volume={32},
  pages={289--353},
  year={2008}
}


@inproceedings{samvelyan2019starcraft2,
 author = {Ellis, Benjamin and Cook, Jonathan and Moalla, Skander and Samvelyan, Mikayel and Sun, Mingfei and Mahajan, Anuj and Foerster, Jakob and Whiteson, Shimon},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {37567--37593},
 publisher = {Curran Associates, Inc.},
 title = {SMACv2: An Improved Benchmark for Cooperative Multi-Agent Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/764c18ad230f9e7bf6a77ffc2312c55e-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 address = {New Orleans},
 year = {2023}
}

@inproceedings{mordatch2018mpe,
  title={Emergence of grounded compositional language in multi-agent populations},
  author={Mordatch, Igor and Abbeel, Pieter},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  number={1},
  year={2018}
}

@article{albrecht2015lbf,
  title={A game-theoretic model and best-response learning method for ad hoc coordination in multiagent systems},
  author={Albrecht, Stefano V and Ramamoorthy, Subramanian},
  journal={arXiv preprint arXiv:1506.01170},
  year={2015}
}

@inproceedings{tenenberg1986planning,
  title={Planning with abstraction},
  author={Tenenberg, Josh},
  booktitle={Proceedings of the Fifth AAAI National Conference on Artificial Intelligence},
  pages={76--80},
  year={1986}
}

@book{intro_rl_sutton_barto,
author = {Sutton, Richard S. and Barto, Andrew G.},
title = {Reinforcement Learning: An Introduction},
year = {2018},
isbn = {0262039249},
publisher = {A Bradford Book},
address = {Cambridge, MA, USA},
abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence. Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics. Like the first edition, this second edition focuses on core online learning algorithms, with more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.}
}

@InProceedings{trpo,
  title = 	 {Trust Region Policy Optimization},
  author = 	 {Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1889--1897},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/schulman15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/schulman15.html},
  abstract = 	 {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.}
}


@InProceedings{a3c,
  title = 	 {Asynchronous Methods for Deep Reinforcement Learning},
  author = 	 {Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1928--1937},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/mniha16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/mniha16.html},
  abstract = 	 {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.}
}

@article{reinforce,
author = {Williams, Ronald J.},
title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
year = {1992},
issue_date = {May 1992},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {8},
number = {3–4},
issn = {0885-6125},
url = {https://doi.org/10.1007/BF00992696},
doi = {10.1007/BF00992696},
abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate reinforcement tasks and certain limited forms of delayed reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
journal = {Mach. Learn.},
month = {may},
pages = {229–256},
numpages = {28},
keywords = {Reinforcement learning, mathematical analysis, gradient descent, connectionist networks}
}

@article{rrl,
  title={Relational reinforcement learning},
  author={D{\v{z}}eroski, Sa{\v{s}}o and DeRaedt, Luc and Driessens, Kurt},
  journal={Machine learning},
  volume={43},
  pages={7--52},
  year={2001},
  publisher={Springer}
}

@inproceedings{christianos2020rware,
 author = {Christianos, Filippos and Sch\"{a}fer, Lukas and Albrecht, Stefano},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {10707--10717},
 publisher = {Curran Associates, Inc.},
 title = {Shared Experience Actor-Critic for Multi-Agent Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper/2020/file/7967cc8e3ab559e68cc944c44b1cf3e8-Paper.pdf},
 address={Virtual},
 volume = {33},
 year = {2020}
}


@InProceedings{rmg,
author="Finzi, Alberto
and Lukasiewicz, Thomas",
editor="Alferes, J{\'o}se J{\'u}lio
and Leite, Jo{\~a}o",
title="Relational Markov Games",
booktitle="Logics in Artificial Intelligence",
year="2004",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="320--333",
abstract="Towards a compact and elaboration-tolerant first-order representation of Markov games, we introduce relational Markov games, which combine standard Markov games with first-order action descriptions in a stochastic variant of the situation calculus. We focus on the zero-sum two-agent case, where we have two agents with diametrically opposed goals. We also present a symbolic value iteration algorithm for computing Nash policy pairs in this framework.",
isbn="978-3-540-30227-8"
}

@book{MARLBook,
  author = {Stefano V. Albrecht and Filippos Christianos and Lukas Sch\"afer},
  title = {Multi-Agent Reinforcement Learning: Foundations and Modern Approaches},
  publisher = {MIT Press},
  address={Boston},
  year = {2024},
  url = {https://www.marl-book.com}
}

@article{StaRAIBook,
  title={Statistical relational artificial intelligence: Logic, probability, and computation},
  author={Raedt, Luc De  and Kersting, Kristian and Natarajan, Sriraam and Poole, David},
  journal={Synthesis lectures on artificial intelligence and machine learning},
  volume={10},
  number={2},
  pages={1--189},
  year={2016},
  publisher={Springer}
}

@book{SRLBook,
  title={Introduction to statistical relational learning},
  author={Getoor, Lise and Taskar, Ben},
  year={2007},
  address={Boston},
  publisher={MIT press}
}

@inproceedings{TaskableRL,
  title={Symbolic plans as high-level instructions for reinforcement learning},
  author={Illanes, Le{\'o}n and Yan, Xi and Icarte, Rodrigo Toro and McIlraith, Sheila A},
  booktitle={Proceedings of the international conference on automated planning and scheduling},
  volume={30},
  pages={540--550},
  year={2020},
  publisher={ICAPS},
  address={Online}
}

@inproceedings{maxq,
  title={The MAXQ Method for Hierarchical Reinforcement Learning.},
  author={Dietterich, Thomas G},
  booktitle={ICML},
  volume={98},
  pages={118--126},
  year={1998},
  publisher={ICML},
  address={Madison, Wisconsin}
}

@article{options,
  title={Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning},
  author={Sutton, Richard S and Precup, Doina and Singh, Satinder},
  journal={Artificial intelligence},
  volume={112},
  number={1-2},
  pages={181--211},
  year={1999},
  publisher={Elsevier}
}

@inproceedings{hams,
  title={Reinforcement learning with a hierarchy of abstract models},
  author={Singh, Satinder P},
  booktitle={Proceedings of the tenth national conference on Artificial intelligence},
  address={San Jose, California},
  publisher={AAAI},
  pages={202--207},
  year={1992}
}

@article{kokel2022Journal,
  title={RePReL: a unified framework for integrating relational planning and reinforcement learning for effective abstraction in discrete and continuous domains},
  author={Kokel, Harsha and Natarajan, Sriraam and Ravindran, Balaraman and Tadepalli, Prasad},
  journal={Neural Computing and Applications},
  volume={35},
  number={23},
  pages={16877--16892},
  year={2023},
  publisher={Springer}
}
@article{pddl,
  title={PDDL-the planning domain definition language},
  author={McDermott, Drew and Ghallab, Malik and Howe, Adele and Knoblock, Craig and Ram, Ashwin and Veloso, Manuela and Weld, Daniel and Wilkins, David},
  year={1998},
  publisher={New Haven, CT}
}

@book{liftedbook,
  title={An Introduction to Lifted Probabilistic Inference},
  author={Van den Broeck, Guy and Kersting, Kristian and Natarajan, Sriraam and Poole, David},
  year={2021},
  address={Boston},
  publisher={MIT Press}
}

@misc{unityml,
      title={Unity: A General Platform for Intelligent Agents}, 
      author={Arthur Juliani and Vincent-Pierre Berges and Ervin Teng and Andrew Cohen and Jonathan Harper and Chris Elion and Chris Goy and Yuan Gao and Hunter Henry and Marwan Mattar and Danny Lange},
      year={2020},
      eprint={1809.02627},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1809.02627}, 
}

@inproceedings{littman1994markov,
author = {Littman, Michael L.},
title = {Markov games as a framework for multi-agent reinforcement learning},
year = {1994},
isbn = {1558603352},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the Eleventh International Conference on International Conference on Machine Learning},
pages = {157–163},
numpages = {7},
location = {New Brunswick, NJ, USA},
series = {ICML'94}
}

@article{zhang2021multi,
  title={Multi-agent reinforcement learning: A selective overview of theories and algorithms},
  author={Zhang, Kaiqing and Yang, Zhuoran and Ba{\c{s}}ar, Tamer},
  journal={Handbook of reinforcement learning and control},
  pages={321--384},
  year={2021},
  volume={325},
  publisher={Springer}
}

@article{starai_nesy,
title = {From statistical relational to neurosymbolic artificial intelligence: A survey},
journal = {Artificial Intelligence},
volume = {328},
pages = {104062},
year = {2024},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2023.104062},
url = {https://www.sciencedirect.com/science/article/pii/S0004370223002084},
author = {Giuseppe Marra and Sebastijan Dumančić and Robin Manhaeve and Luc {De Raedt}},
keywords = {Neurosymbolic AI, Statistical relational AI, Learning and reasoning, Probabilistic logics},
abstract = {This survey explores the integration of learning and reasoning in two different fields of artificial intelligence: neurosymbolic and statistical relational artificial intelligence. Neurosymbolic artificial intelligence (NeSy) studies the integration of symbolic reasoning and neural networks, while statistical relational artificial intelligence (StarAI) focuses on integrating logic with probabilistic graphical models. This survey identifies seven shared dimensions between these two subfields of AI. These dimensions can be used to characterize different NeSy and StarAI systems. They are concerned with (1) the approach to logical inference, whether model or proof-based; (2) the syntax of the used logical theories; (3) the logical semantics of the systems and their extensions to facilitate learning; (4) the scope of learning, encompassing either parameter or structure learning; (5) the presence of symbolic and subsymbolic representations; (6) the degree to which systems capture the original logic, probabilistic, and neural paradigms; and (7) the classes of learning tasks the systems are applied to. By positioning various NeSy and StarAI systems along these dimensions and pointing out similarities and differences between them, this survey contributes fundamental concepts for understanding the integration of learning and reasoning.}
}

@inproceedings{tadepalli2004relational,
  title={Relational reinforcement learning: An overview},
  author={Tadepalli, Prasad and Givan, Robert and Driessens, Kurt},
  booktitle={Proceedings of the ICML},
  volume={4},
  publisher={ICML},
  address={Banff, Alberta},  
  pages={1--9},
  year={2004}
}

@book{rrl_survey_ottolo,
title = "A Survey of Reinforcement Learning in Relational Domains",
abstract = "Reinforcement learning has developed into a primary approach for learning control strategies for autonomous agents. However, most of the work has focused on the algorithmic aspect, i.e. various ways of computing value functions and policies. Usually the representational aspects were limited to the use of attribute-value or propositional languages to describe states, actions etc. A recent direction - under the general name of relational reinforcement learning - is concerned with upgrading the representation of reinforcement learning methods to the first-order case, being able to speak, reason and learn about objects and relations between objects. This survey aims at presenting an introduction to this new field, starting from the classical reinforcement learning framework. We will describe the main motivations and challenges, and give a comprehensive survey of methods that have been proposed in the literature. The aim is to give a complete survey of the available literature, of the underlying motivations and of the implications if the new methods for learning in large, relational and probabilistic environments.",
keywords = "METIS-227385, IR-53976, EWI-1879",
author = "{van Otterlo}, M.",
note = "Imported from HMI",
year = "2005",
language = "Undefined",
series = "CTIT Technical Report Series",
publisher = "Centre for Telematics and Information Technology (CTIT)",
number = "05-31",
address = "Netherlands",
}

@article{rrl_guidance,
title = {Relational reinforcement learning with guided demonstrations},
journal = {Artificial Intelligence},
volume = {247},
pages = {295-312},
year = {2017},
note = {Special Issue on AI and Robotics},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2015.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0004370215000284},
author = {David Martínez and Guillem Alenyà and Carme Torras},
keywords = {Active learning, Learning guidance, Planning excuse, Reinforcement learning, Robot learning, Teacher demonstration, Teacher guidance},
abstract = {Model-based reinforcement learning is a powerful paradigm for learning tasks in robotics. However, in-depth exploration is usually required and the actions have to be known in advance. Thus, we propose a novel algorithm that integrates the option of requesting teacher demonstrations to learn new domains with fewer action executions and no previous knowledge. Demonstrations allow new actions to be learned and they greatly reduce the amount of exploration required, but they are only requested when they are expected to yield a significant improvement because the teacher's time is considered to be more valuable than the robot's time. Moreover, selecting the appropriate action to demonstrate is not an easy task, and thus some guidance is provided to the teacher. The rule-based model is analyzed to determine the parts of the state that may be incomplete, and to provide the teacher with a set of possible problems for which a demonstration is needed. Rule analysis is also used to find better alternative models and to complete subgoals before requesting help, thereby minimizing the number of requested demonstrations. These improvements were demonstrated in a set of experiments, which included domains from the international planning competition and a robotic task. Adding teacher demonstrations and rule analysis reduced the amount of exploration required by up to 60% in some domains, and improved the success ratio by 35% in other domains.}
}

@article{hrl_survey,
  title={Hierarchical reinforcement learning: A comprehensive survey},
  author={Pateria, Shubham and Subagdja, Budhitama and Tan, Ah-hwee and Quek, Chai},
  journal={ACM Computing Surveys (CSUR)},
  volume={54},
  number={5},
  pages={1--35},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inproceedings{hsd,
author = {Yang, Jiachen and Borovikov, Igor and Zha, Hongyuan},
title = {Hierarchical Cooperative Multi-Agent Reinforcement Learning with Skill Discovery},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Human players in professional team sports achieve high level coordination by dynamically choosing complementary skills and executing primitive actions to perform these skills. As a step toward creating intelligent agents with this capability for fully cooperative multi-agent settings, we propose a two-level hierarchical multi-agent reinforcement learning (MARL) algorithm with unsupervised skill discovery. Agents learn useful and distinct skills at the low level via independent Q-learning, while they learn to select complementary latent skill variables at the high level via centralized multi-agent training with an extrinsic team reward. The set of low-level skills emerges from an intrinsic reward that solely promotes the decodability of latent skill variables from the trajectory of a low-level skill, without the need for hand-crafted rewards for each skill. For scalable decentralized execution, each agent independently chooses latent skill variables and primitive actions based on local observations. Our overall method enables the use of general cooperative MARL algorithms for training high level policies and single-agent RL for training low level skills. Experiments on a stochastic high dimensional team game show the emergence of useful skills and cooperative team play. The interpretability of the learned skills show the promise of the proposed method for achieving human-AI cooperation in team sports games.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1566–1574},
numpages = {9},
keywords = {hierarchical learning, multi-agent learning, option discovery},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{tadepalli_hrrl,
  title={Function approximation in hierarchical relational reinforcement learning},
  author={Roncagliolo, Silvana and Tadepalli, Prasad},
  booktitle={Proceedings of the ICML-2004 Workshop on Relational Reinforcement Learning},
  pages={1--5},
  publisher={ICML},
  address = {Banff, Alberta},
  year={2004}
}

@INPROCEEDINGS{htn_mtrl,
  author={Hu, Yuyong and Zhuo, Hankz Hankui},
  booktitle={2024 5th International Conference on Computer Engineering and Application (ICCEA)}, 
  title={Multi-Task Reinforcement Learning with Cost-based HTN Planning}, 
  year={2024},
  volume={},
  number={},
  address={Hangzhou, China},
  pages={155-160},
  publisher={IEEE},
  keywords={Adaptation models;Computational modeling;Reinforcement learning;Multitasking;Planning;Resource management;Task analysis;Multi-Task Reinforcement Learning;Planning;Hierarchical Task Network;Pre-trained Language Models},
  doi={10.1109/ICCEA62105.2024.10603549}
}

@inproceedings{sbert,
    title = "Sentence-{BERT}: Sentence Embeddings using {S}iamese {BERT}-Networks",
    author = "Reimers, Nils  and
      Gurevych, Iryna",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1410",
    doi = "10.18653/v1/D19-1410",
    pages = "3982--3992",
    abstract = "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.",
}

@inproceedings{marrl,
  title={Multi-agent relational reinforcement learning: Explorations in multi-state coordination tasks},
  author={Croonenborghs, Tom and Tuyls, Karl and Ramon, Jan and Bruynooghe, Maurice},
  booktitle={Learning and Adaption in Multi-Agent Systems: First International Workshop, LAMAS 2005, Utrecht, The Netherlands, July 25, 2005, Revised Selected Papers},
  pages={192--206},
  year={2006},
  publisher={LAMAS},
  address={Utrecht, The Netherlands},
  organization={Springer}
}

@article{haven, title={HAVEN: Hierarchical Cooperative Multi-Agent Reinforcement Learning with Dual Coordination Mechanism}, volume={37}, url={https://ojs.aaai.org/index.php/AAAI/article/view/26386}, DOI={10.1609/aaai.v37i10.26386}, abstractNote={Recently, some challenging tasks in multi-agent systems have been solved by some hierarchical reinforcement learning methods. Inspired by the intra-level and inter-level coordination in the human nervous system, we propose a novel value decomposition framework HAVEN based on hierarchical reinforcement learning for fully cooperative multi-agent problems. To address the instability arising from the concurrent optimization of policies between various levels and agents, we introduce the dual coordination mechanism of inter-level and inter-agent strategies by designing reward functions in a two-level hierarchy. HAVEN does not require domain knowledge and pre-training, and can be applied to any value decomposition variant. Our method achieves desirable results on different decentralized partially observable Markov decision process domains and outperforms other popular multi-agent hierarchical reinforcement learning algorithms.}, number={10}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Xu, Zhiwei and Bai, Yunpeng and Zhang, Bin and Li, Dapeng and Fan, Guoliang}, year={2023}, month={Jun.}, pages={11735-11743} }

@inproceedings{alma,
 author = {Iqbal, Shariq and Costales, Robby and Sha, Fei},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {7155--7166},
 publisher = {Curran Associates, Inc.},
 title = {ALMA: Hierarchical Learning for Composite Multi-Agent Tasks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/2f27964513a28d034530bfdd117ea31d-Paper-Conference.pdf},
 volume = {35},
 year = {2022},
 address = {New Orleans}
}

@inproceedings{marl_ilp,
  title={Towards relational multi-agent reinforcement learning via inductive logic programming},
  author={Li, Guangxia and Xiao, Gang and Zhang, Junbo and Liu, Jia and Shen, Yulong},
  booktitle={International Conference on Artificial Neural Networks},
  pages={99--110},
  year={2022},
  address={Bristol, UK},
  publisher={ICANN},
  organization={Springer}
}

@article{FODDs,
  title={First order decision diagrams for relational MDPs},
  author={Wang, Chenggang and Joshi, Saket and Khardon, Roni},
  journal={Journal of Artificial Intelligence Research},
  volume={31},
  pages={431--472},
  year={2008}
}

@misc{DasFittedQ,
      title={Fitted Q-Learning for Relational Domains}, 
      author={Srijita Das and Sriraam Natarajan and Kaushik Roy and Ronald Parr and Kristian Kersting},
      year={2020},
      eprint={2006.05595},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2006.05595}, 
}