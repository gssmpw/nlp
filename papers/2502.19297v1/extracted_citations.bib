@misc{DasFittedQ,
      title={Fitted Q-Learning for Relational Domains}, 
      author={Srijita Das and Sriraam Natarajan and Kaushik Roy and Ronald Parr and Kristian Kersting},
      year={2020},
      eprint={2006.05595},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2006.05595}, 
}

@book{MARLBook,
  author = {Stefano V. Albrecht and Filippos Christianos and Lukas Sch\"afer},
  title = {Multi-Agent Reinforcement Learning: Foundations and Modern Approaches},
  publisher = {MIT Press},
  address={Boston},
  year = {2024},
  url = {https://www.marl-book.com}
}

@InProceedings{Qmix,
  title = 	 {{QMIX}: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning},
  author =       {Rashid, Tabish and Samvelyan, Mikayel and Schroeder, Christian and Farquhar, Gregory and Foerster, Jakob and Whiteson, Shimon},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {4295--4304},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  address={Stockholm, Sweden},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/rashid18a/rashid18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/rashid18a.html},
  abstract = 	 {In many real-world settings, a team of agents must coordinate their behaviour while acting in a decentralised way. At the same time, it is often possible to train the agents in a centralised fashion in a simulated or laboratory setting, where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a network that estimates joint action-values as a complex non-linear combination of per-agent values that condition only on local observations. We structurally enforce that the joint-action value is monotonic in the per-agent values, which allows tractable maximisation of the joint action-value in off-policy learning, and guarantees consistency between the centralised and decentralised policies. We evaluate QMIX on a challenging set of StarCraft II micromanagement tasks, and show that QMIX significantly outperforms existing value-based multi-agent reinforcement learning methods.}
}

@book{SRLBook,
  title={Introduction to statistical relational learning},
  author={Getoor, Lise and Taskar, Ben},
  year={2007},
  address={Boston},
  publisher={MIT press}
}

@article{StaRAIBook,
  title={Statistical relational artificial intelligence: Logic, probability, and computation},
  author={Raedt, Luc De  and Kersting, Kristian and Natarajan, Sriraam and Poole, David},
  journal={Synthesis lectures on artificial intelligence and machine learning},
  volume={10},
  number={2},
  pages={1--189},
  year={2016},
  publisher={Springer}
}

@inproceedings{TaskableRL,
  title={Symbolic plans as high-level instructions for reinforcement learning},
  author={Illanes, Le{\'o}n and Yan, Xi and Icarte, Rodrigo Toro and McIlraith, Sheila A},
  booktitle={Proceedings of the international conference on automated planning and scheduling},
  volume={30},
  pages={540--550},
  year={2020},
  publisher={ICAPS},
  address={Online}
}

@inproceedings{alma,
 author = {Iqbal, Shariq and Costales, Robby and Sha, Fei},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {7155--7166},
 publisher = {Curran Associates, Inc.},
 title = {ALMA: Hierarchical Learning for Composite Multi-Agent Tasks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/2f27964513a28d034530bfdd117ea31d-Paper-Conference.pdf},
 volume = {35},
 year = {2022},
 address = {New Orleans}
}

@inproceedings{dreduction,
  title={Using dimensionality reduction to exploit constraints in reinforcement learning},
  author={Bitzer, Sebastian and Howard, Matthew and Vijayakumar, Sethu},
  booktitle={2010 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  address={Taipei, Taiwan},
  publisher={IEEE},
  pages={3219--3225},
  year={2010},
  organization={IEEE}
}

@inproceedings{hams,
  title={Reinforcement learning with a hierarchy of abstract models},
  author={Singh, Satinder P},
  booktitle={Proceedings of the tenth national conference on Artificial intelligence},
  address={San Jose, California},
  publisher={AAAI},
  pages={202--207},
  year={1992}
}

@article{haven, title={HAVEN: Hierarchical Cooperative Multi-Agent Reinforcement Learning with Dual Coordination Mechanism}, volume={37}, url={https://ojs.aaai.org/index.php/AAAI/article/view/26386}, DOI={10.1609/aaai.v37i10.26386}, abstractNote={Recently, some challenging tasks in multi-agent systems have been solved by some hierarchical reinforcement learning methods. Inspired by the intra-level and inter-level coordination in the human nervous system, we propose a novel value decomposition framework HAVEN based on hierarchical reinforcement learning for fully cooperative multi-agent problems. To address the instability arising from the concurrent optimization of policies between various levels and agents, we introduce the dual coordination mechanism of inter-level and inter-agent strategies by designing reward functions in a two-level hierarchy. HAVEN does not require domain knowledge and pre-training, and can be applied to any value decomposition variant. Our method achieves desirable results on different decentralized partially observable Markov decision process domains and outperforms other popular multi-agent hierarchical reinforcement learning algorithms.}, number={10}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Xu, Zhiwei and Bai, Yunpeng and Zhang, Bin and Li, Dapeng and Fan, Guoliang}, year={2023}, month={Jun.}, pages={11735-11743} }

@article{hmarl,
author = {Ghavamzadeh, Mohammad and Mahadevan, Sridhar and Makar, Rajbala},
title = {Hierarchical Multi-Agent Reinforcement Learning},
year = {2006},
issue_date = {September 2006},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {13},
number = {2},
issn = {1387-2532},
url = {https://doi.org/10.1007/s10458-006-7035-4},
doi = {10.1007/s10458-006-7035-4},
abstract = {In this paper, we investigate the use of hierarchical reinforcement learning (HRL) to speed up the acquisition of cooperative multi-agent tasks. We introduce a hierarchical multi-agent reinforcement learning (RL) framework, and propose a hierarchical multi-agent RL algorithm called Cooperative HRL . In this framework, agents are cooperative and homogeneous (use the same task decomposition). Learning is decentralized, with each agent learning three interrelated skills: how to perform each individual subtask, the order in which to carry them out, and how to coordinate with other agents. We define cooperative subtasks to be those subtasks in which coordination among agents significantly improves the performance of the overall task. Those levels of the hierarchy which include cooperative subtasks are called cooperation levels . A fundamental property of the proposed approach is that it allows agents to learn coordination faster by sharing information at the level of cooperative subtasks , rather than attempting to learn coordination at the level of primitive actions. We study the empirical performance of the Cooperative HRL algorithm using two testbeds: a simulated two-robot trash collection task, and a larger four-agent automated guided vehicle (AGV) scheduling problem. We compare the performance and speed of Cooperative HRL with other learning algorithms, as well as several well-known industrial AGV heuristics. We also address the issue of rational communication behavior among autonomous agents in this paper. The goal is for agents to learn both action and communication policies that together optimize the task given a communication cost. We extend the multi-agent HRL framework to include communication decisions and propose a cooperative multi-agent HRL algorithm called COM-Cooperative HRL . In this algorithm, we add a communication level to the hierarchical decomposition of the problem below each cooperation level . Before an agent makes a decision at a cooperative subtask , it decides if it is worthwhile to perform a communication action. A communication action has a certain cost and provides the agent with the actions selected by the other agents at a cooperation level . We demonstrate the efficiency of the COM-Cooperative HRL algorithm as well as the relation between the communication cost and the learned communication policy using a multi-agent taxi problem.},
journal = {Autonomous Agents and Multi-Agent Systems},
month = {sep},
pages = {197–229},
numpages = {33},
keywords = {Cooperative multi-agent systems, Communication, Hierarchical reinforcement learning, Coordination}
}

@article{hrl_survey,
  title={Hierarchical reinforcement learning: A comprehensive survey},
  author={Pateria, Shubham and Subagdja, Budhitama and Tan, Ah-hwee and Quek, Chai},
  journal={ACM Computing Surveys (CSUR)},
  volume={54},
  number={5},
  pages={1--35},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inproceedings{hsd,
author = {Yang, Jiachen and Borovikov, Igor and Zha, Hongyuan},
title = {Hierarchical Cooperative Multi-Agent Reinforcement Learning with Skill Discovery},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Human players in professional team sports achieve high level coordination by dynamically choosing complementary skills and executing primitive actions to perform these skills. As a step toward creating intelligent agents with this capability for fully cooperative multi-agent settings, we propose a two-level hierarchical multi-agent reinforcement learning (MARL) algorithm with unsupervised skill discovery. Agents learn useful and distinct skills at the low level via independent Q-learning, while they learn to select complementary latent skill variables at the high level via centralized multi-agent training with an extrinsic team reward. The set of low-level skills emerges from an intrinsic reward that solely promotes the decodability of latent skill variables from the trajectory of a low-level skill, without the need for hand-crafted rewards for each skill. For scalable decentralized execution, each agent independently chooses latent skill variables and primitive actions based on local observations. Our overall method enables the use of general cooperative MARL algorithms for training high level policies and single-agent RL for training low level skills. Experiments on a stochastic high dimensional team game show the emergence of useful skills and cooperative team play. The interpretability of the learned skills show the promise of the proposed method for achieving human-AI cooperation in team sports games.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1566–1574},
numpages = {9},
keywords = {hierarchical learning, multi-agent learning, option discovery},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{kokel2021reprel,
  title={Reprel: Integrating relational planning and reinforcement learning for effective abstraction},
  author={Kokel, Harsha and Manoharan, Arjun and Natarajan, Sriraam and Ravindran, Balaraman and Tadepalli, Prasad},
  booktitle={Proceedings of the International Conference on Automated Planning and Scheduling},
  address={Guangzhou, China},
  publisher={ICAPS},
  volume={31},
  pages={533--541},
  year={2021}
}

@article{kokel2022Journal,
  title={RePReL: a unified framework for integrating relational planning and reinforcement learning for effective abstraction in discrete and continuous domains},
  author={Kokel, Harsha and Natarajan, Sriraam and Ravindran, Balaraman and Tadepalli, Prasad},
  journal={Neural Computing and Applications},
  volume={35},
  number={23},
  pages={16877--16892},
  year={2023},
  publisher={Springer}
}

@book{liftedbook,
  title={An Introduction to Lifted Probabilistic Inference},
  author={Van den Broeck, Guy and Kersting, Kristian and Natarajan, Sriraam and Poole, David},
  year={2021},
  address={Boston},
  publisher={MIT Press}
}

@inproceedings{maddpg,
author = {Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, Pieter and Mordatch, Igor},
title = {Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {6382–6393},
numpages = {12},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@inproceedings{marl_ilp,
  title={Towards relational multi-agent reinforcement learning via inductive logic programming},
  author={Li, Guangxia and Xiao, Gang and Zhang, Junbo and Liu, Jia and Shen, Yulong},
  booktitle={International Conference on Artificial Neural Networks},
  pages={99--110},
  year={2022},
  address={Bristol, UK},
  publisher={ICANN},
  organization={Springer}
}

@inproceedings{marrl,
  title={Multi-agent relational reinforcement learning: Explorations in multi-state coordination tasks},
  author={Croonenborghs, Tom and Tuyls, Karl and Ramon, Jan and Bruynooghe, Maurice},
  booktitle={Learning and Adaption in Multi-Agent Systems: First International Workshop, LAMAS 2005, Utrecht, The Netherlands, July 25, 2005, Revised Selected Papers},
  pages={192--206},
  year={2006},
  publisher={LAMAS},
  address={Utrecht, The Netherlands},
  organization={Springer}
}

@inproceedings{maxq,
  title={The MAXQ Method for Hierarchical Reinforcement Learning.},
  author={Dietterich, Thomas G},
  booktitle={ICML},
  volume={98},
  pages={118--126},
  year={1998},
  publisher={ICML},
  address={Madison, Wisconsin}
}

@article{options,
  title={Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning},
  author={Sutton, Richard S and Precup, Doina and Singh, Satinder},
  journal={Artificial intelligence},
  volume={112},
  number={1-2},
  pages={181--211},
  year={1999},
  publisher={Elsevier}
}

@article{rrl,
  title={Relational reinforcement learning},
  author={D{\v{z}}eroski, Sa{\v{s}}o and DeRaedt, Luc and Driessens, Kurt},
  journal={Machine learning},
  volume={43},
  pages={7--52},
  year={2001},
  publisher={Springer}
}

@article{rrl_guidance,
title = {Relational reinforcement learning with guided demonstrations},
journal = {Artificial Intelligence},
volume = {247},
pages = {295-312},
year = {2017},
note = {Special Issue on AI and Robotics},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2015.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0004370215000284},
author = {David Martínez and Guillem Alenyà and Carme Torras},
keywords = {Active learning, Learning guidance, Planning excuse, Reinforcement learning, Robot learning, Teacher demonstration, Teacher guidance},
abstract = {Model-based reinforcement learning is a powerful paradigm for learning tasks in robotics. However, in-depth exploration is usually required and the actions have to be known in advance. Thus, we propose a novel algorithm that integrates the option of requesting teacher demonstrations to learn new domains with fewer action executions and no previous knowledge. Demonstrations allow new actions to be learned and they greatly reduce the amount of exploration required, but they are only requested when they are expected to yield a significant improvement because the teacher's time is considered to be more valuable than the robot's time. Moreover, selecting the appropriate action to demonstrate is not an easy task, and thus some guidance is provided to the teacher. The rule-based model is analyzed to determine the parts of the state that may be incomplete, and to provide the teacher with a set of possible problems for which a demonstration is needed. Rule analysis is also used to find better alternative models and to complete subgoals before requesting help, thereby minimizing the number of requested demonstrations. These improvements were demonstrated in a set of experiments, which included domains from the international planning competition and a robotic task. Adding teacher demonstrations and rule analysis reduced the amount of exploration required by up to 60% in some domains, and improved the success ratio by 35% in other domains.}
}

@book{rrl_survey_ottolo,
title = "A Survey of Reinforcement Learning in Relational Domains",
abstract = "Reinforcement learning has developed into a primary approach for learning control strategies for autonomous agents. However, most of the work has focused on the algorithmic aspect, i.e. various ways of computing value functions and policies. Usually the representational aspects were limited to the use of attribute-value or propositional languages to describe states, actions etc. A recent direction - under the general name of relational reinforcement learning - is concerned with upgrading the representation of reinforcement learning methods to the first-order case, being able to speak, reason and learn about objects and relations between objects. This survey aims at presenting an introduction to this new field, starting from the classical reinforcement learning framework. We will describe the main motivations and challenges, and give a comprehensive survey of methods that have been proposed in the literature. The aim is to give a complete survey of the available literature, of the underlying motivations and of the implications if the new methods for learning in large, relational and probabilistic environments.",
keywords = "METIS-227385, IR-53976, EWI-1879",
author = "{van Otterlo}, M.",
note = "Imported from HMI",
year = "2005",
language = "Undefined",
series = "CTIT Technical Report Series",
publisher = "Centre for Telematics and Information Technology (CTIT)",
number = "05-31",
address = "Netherlands",
}

@inproceedings{sampleGene,
 author = {Li, Gen and Chi, Yuejie and Wei, Yuting and Chen, Yuxin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {15353--15367},
 publisher = {Curran Associates, Inc.},
 title = {Minimax-Optimal Multi-Agent RL in Markov Games With a Generative Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/62b4fea131cfd5b7504eae356b75bbd8-Paper-Conference.pdf},
 volume = {35},
 address={New Orleans},
 year = {2022}
}

@article{sampleefficientMARL,
    doi = {10.1371/journal.pone.0291545},
    author = {Kim, Jung In AND Lee, Young Jae AND Heo, Jongkook AND Park, Jinhyeok AND Kim, Jaehoon AND Lim, Sae Rin AND Jeong, Jinyong AND Kim, Seoung Bum},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Sample-efficient multi-agent reinforcement learning with masked reconstruction},
    year = {2023},
    month = {09},
    volume = {18},
    url = {https://doi.org/10.1371/journal.pone.0291545},
    pages = {1-14},
    abstract = {Deep reinforcement learning (DRL) is a powerful approach that combines reinforcement learning (RL) and deep learning to address complex decision-making problems in high-dimensional environments. Although DRL has been remarkably successful, its low sample efficiency necessitates extensive training times and large amounts of data to learn optimal policies. These limitations are more pronounced in the context of multi-agent reinforcement learning (MARL). To address these limitations, various studies have been conducted to improve DRL. In this study, we propose an approach that combines a masked reconstruction task with QMIX (M-QMIX). By introducing a masked reconstruction task as an auxiliary task, we aim to achieve enhanced sample efficiency—a fundamental limitation of RL in multi-agent systems. Experiments were conducted using the StarCraft II micromanagement benchmark to validate the effectiveness of the proposed method. We used 11 scenarios comprising five easy, three hard, and three very hard scenarios. We particularly focused on using a limited number of time steps for each scenario to demonstrate the improved sample efficiency. Compared to QMIX, the proposed method is superior in eight of the 11 scenarios. These results provide strong evidence that the proposed method is more sample-efficient than QMIX, demonstrating that it effectively addresses the limitations of DRL in multi-agent systems.},
    number = {9},

}

@article{starai_nesy,
title = {From statistical relational to neurosymbolic artificial intelligence: A survey},
journal = {Artificial Intelligence},
volume = {328},
pages = {104062},
year = {2024},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2023.104062},
url = {https://www.sciencedirect.com/science/article/pii/S0004370223002084},
author = {Giuseppe Marra and Sebastijan Dumančić and Robin Manhaeve and Luc {De Raedt}},
keywords = {Neurosymbolic AI, Statistical relational AI, Learning and reasoning, Probabilistic logics},
abstract = {This survey explores the integration of learning and reasoning in two different fields of artificial intelligence: neurosymbolic and statistical relational artificial intelligence. Neurosymbolic artificial intelligence (NeSy) studies the integration of symbolic reasoning and neural networks, while statistical relational artificial intelligence (StarAI) focuses on integrating logic with probabilistic graphical models. This survey identifies seven shared dimensions between these two subfields of AI. These dimensions can be used to characterize different NeSy and StarAI systems. They are concerned with (1) the approach to logical inference, whether model or proof-based; (2) the syntax of the used logical theories; (3) the logical semantics of the systems and their extensions to facilitate learning; (4) the scope of learning, encompassing either parameter or structure learning; (5) the presence of symbolic and subsymbolic representations; (6) the degree to which systems capture the original logic, probabilistic, and neural paradigms; and (7) the classes of learning tasks the systems are applied to. By positioning various NeSy and StarAI systems along these dimensions and pointing out similarities and differences between them, this survey contributes fundamental concepts for understanding the integration of learning and reasoning.}
}

@inproceedings{tadepalli2004relational,
  title={Relational reinforcement learning: An overview},
  author={Tadepalli, Prasad and Givan, Robert and Driessens, Kurt},
  booktitle={Proceedings of the ICML},
  volume={4},
  publisher={ICML},
  address={Banff, Alberta},  
  pages={1--9},
  year={2004}
}

@inproceedings{tadepalli_hrrl,
  title={Function approximation in hierarchical relational reinforcement learning},
  author={Roncagliolo, Silvana and Tadepalli, Prasad},
  booktitle={Proceedings of the ICML-2004 Workshop on Relational Reinforcement Learning},
  pages={1--5},
  publisher={ICML},
  address = {Banff, Alberta},
  year={2004}
}

@article{zhang2021multi,
  title={Multi-agent reinforcement learning: A selective overview of theories and algorithms},
  author={Zhang, Kaiqing and Yang, Zhuoran and Ba{\c{s}}ar, Tamer},
  journal={Handbook of reinforcement learning and control},
  pages={321--384},
  year={2021},
  volume={325},
  publisher={Springer}
}

