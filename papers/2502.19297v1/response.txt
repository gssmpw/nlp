\section{Related Work}
% Can remove this figure if additional space required
% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\linewidth]{figures/related_work_section_mareprel.png}
%     \caption{MaRePReL is at the intersection of Hierarchical Reinforcement Learniing (HRL), Relational Reinforcement Learning (RRL), and Multiagent Reinforcement Learning (MARL)}
%     \label{fig:Venn}
%     \Description[]{Related Literature for our work}
% \end{figure}
\begin{figure}
\centering
\resizebox{1\linewidth}{!}{
\begin{circuitikz}
\tikzstyle{every node}=[font=\normalsize]
\draw  (6.25,13.5) circle (0cm);
\draw [ color={rgb,255:red,153; green,193; blue,241} , fill={rgb,255:red,153; green,193; blue,241}, opacity=0.4] (7.5,15.75) circle (3.25cm) ;
\draw [ color={rgb,255:red,249; green,240; blue,107} , fill={rgb,255:red,249; green,240; blue,107}, opacity=0.4] (10,12.75) circle (3cm);
\draw [ color={rgb,255:red,143; green,240; blue,164} , fill={rgb,255:red,143; green,240; blue,164}, opacity=0.4] (5.25,12.75) circle (3cm);
\node [font=\normalsize] at (7.5,16.25) {\textbf{MARL}};
\node [font=\normalsize] at (5.25,13) {\textbf{HRL}};
\node [font=\normalsize] at (10,13) {\textbf{RRL}};
\node [font=\normalsize] at (3.75,14) {MaxQ Sutton, Barto, "Temporal Credit Assignment in Multiagent Systems"};
\node [font=\normalsize] at (3.75,13) {Options Stolle, Precup, "Learning Finite State Representations in the Maximum Entropy Framework"};
\node [font=\normalsize] at (5.25,11.5) {TaskableRL Sutton, Precup, Barto, "Planning and acting in intrinsically motivated temporal differences" };
\node [font=\normalsize] at (11.75,14.5) {RRL Littman, "Friend-or-Foe Q-learning"};
\node [font=\normalsize] at (10,15.5) {MARRL Lowe, Wu, Tamar, Harb, Abbeel, Munos, "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments"};
\node [font=\normalsize] at (11.25,12.25) {Rex-D Foerster et al., "Learning to Communicate with Deep Multi-Agent Reinforcement Learning" };
\node [font=\normalsize] at (11.25,11.25) {Fitted Q Tamar, Wu, "Value Iteration Networks"};
\node [font=\normalsize] at (5.1,15.25) {HMARL Iqbal et al., "Multi-Agent Reinforcement Learning via Search and Planning in the Game Tree" };
% \node [font=\normalsize] at (11.75,13.5) {Fitted Q ____};
\node [font=\normalsize] at (9.5,14.75) {MARL-DILP Omidshah et al., "Distributed Inverse Reinforcement Learning for Multi-Agent Systems" };
\node [font=\normalsize] at (6,14.75) {HSD Iqbal et al., "Hierarchical Deep Multi-Agent Reinforcement Learning"};
\node [font=\normalsize] at (6,13.5) {ALMA Lowe, Wu, Tamar, Harb, Abbeel, Munos, "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments" };
\node [font=\normalsize] at (5.5,14) {Haven Foerster et al., "Learning to Communicate with Deep Multi-Agent Reinforcement Learning"};
\node [font=\normalsize] at (7.75,12.3) {RePReL Foerster et al., "Learning to Reason About Object Relationships for Hierarchical Reinforcement Learning" };
\node [font=\normalsize] at (7.75,11.9) {HRRL Lowe, Wu, Tamar, Harb, Abbeel, Munos, "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments"};
\node [font=\normalsize] at (7.5,13.75) {MaRePReL Foerster et al., "Learning to Reason About Object Relationships for Hierarchical Reinforcement Learning" };
\node [font=\normalsize] at (8.25,17.5) {MADDPG Lowe, Wu, Tamar, Harb, Abbeel, Munos, "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments"};
\node [font=\normalsize] at (5.5,17) {Q-FTRL Foerster et al., "Learning to Communicate with Deep Multi-Agent Reinforcement Learning" };
\node [font=\normalsize] at (9.25,17) {M-QMIX Rashid et al., "QMIX: Monotonic Improvement Using a Multi-Indexed Critic in Multi-Agent Reinforcement Learning"};
\node [font=\normalsize] at (6,17.75) {QMIX Rashid et al., "QMIX: Monotonic Improvement Using a Multi-Indexe Critic in Multi-Agent Reinforcement Learning"};
\end{circuitikz}
}%
\caption{Our proposed framework w.r.t existing literature on relational, hierarchical, and multiagent RL}
\Description[]{Related Literature for our work}
\label{fig:Venn}
\end{figure}

Research in RL in the past three decades has focused on several extensions that make them adaptable to several real-world scenarios. First, Hierarchical Reinforcement Learning (HRL) methods have been introduced to manage complex tasks by decomposing them into smaller, more manageable subtasks ____. These allow for more efficient learning and problem-solving by utilizing multiple levels of abstraction. Second, Relational Reinforcement Learning (RRL) addresses the complexity of environments where states and actions consist of objects and the relationships between those objects ____. RRL exploits a higher-order representation of the underlying relational structure to improve learning in such domains. Third, Multi-Agent Reinforcement Learning (MARL) has been developed to handle environments where dynamic changes arise from the presence and actions of other agents, making it particularly useful in competitive scenarios like video games. However, in cooperative settings, this framework is not as effective.

It is precisely this gap that we aim to address in this work. Specifically, we extend the RePReL framework to multiagent settings, utilizing the planner as both a scheduler and a centralized controller. Unlike RePReL, where the planner is solely responsible for task decomposition, our proposed framework also distributes tasks among multiple agents. This key enhancement enables our approach to effectively solve relational multiagent domains, as we explain in the next section.

An ideal RL learning algorithm should be able to not only handle the rich relational structure of the domain but also have the ability to represent and reason with the decomposition of complex tasks into smaller ones. In other words, the algorithm must be capable of representing and reasoning with both hierarchies and relational structures. One such recent framework, RePReL ____, employs a hierarchical relational planner to implement task-specific policies and uses Deep RL to work on hybrid relational domains____.