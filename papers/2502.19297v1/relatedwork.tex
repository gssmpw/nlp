\section{Related Work}
% Can remove this figure if additional space required
% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\linewidth]{figures/related_work_section_mareprel.png}
%     \caption{MaRePReL is at the intersection of Hierarchical Reinforcement Learniing (HRL), Relational Reinforcement Learning (RRL), and Multiagent Reinforcement Learning (MARL)}
%     \label{fig:Venn}
%     \Description[]{Related Literature for our work}
% \end{figure}
\begin{figure}
\centering
\resizebox{1\linewidth}{!}{
\begin{circuitikz}
\tikzstyle{every node}=[font=\normalsize]
\draw  (6.25,13.5) circle (0cm);
\draw [ color={rgb,255:red,153; green,193; blue,241} , fill={rgb,255:red,153; green,193; blue,241}, opacity=0.4] (7.5,15.75) circle (3.25cm) ;
\draw [ color={rgb,255:red,249; green,240; blue,107} , fill={rgb,255:red,249; green,240; blue,107}, opacity=0.4] (10,12.75) circle (3cm);
\draw [ color={rgb,255:red,143; green,240; blue,164} , fill={rgb,255:red,143; green,240; blue,164}, opacity=0.4] (5.25,12.75) circle (3cm);
\node [font=\normalsize] at (7.5,16.25) {\textbf{MARL}};
\node [font=\normalsize] at (5.25,13) {\textbf{HRL}};
\node [font=\normalsize] at (10,13) {\textbf{RRL}};
\node [font=\normalsize] at (3.75,14) {MaxQ \citep{maxq}};
\node [font=\normalsize] at (3.75,13) {Options \citep{options}};
\node [font=\normalsize] at (5.25,11.5) {TaskableRL \citep{TaskableRL}};
\node [font=\normalsize] at (11.75,14.5) {RRL \citep{rrl}};
\node [font=\normalsize] at (10,15.5) {MARRL \citep{marrl}};
\node [font=\normalsize] at (11.25,12.25) {Rex-D \citep{rrl_guidance}};
\node [font=\normalsize] at (11.25,11.25) {Fitted Q \citep{DasFittedQ}};
\node [font=\normalsize] at (5.1,15.25) {HMARL \citep{hmarl}};
% \node [font=\normalsize] at (11.75,13.5) {Fitted Q \citep{fitted_q}};
\node [font=\normalsize] at (9.5,14.75) {MARL-DILP \citep{marl_ilp}};
\node [font=\normalsize] at (6,14.75) {HSD \citep{hsd}};
\node [font=\normalsize] at (6,13.5) {ALMA \citep{alma}};
\node [font=\normalsize] at (5.5,14) {Haven \citep{haven}};
\node [font=\normalsize] at (7.75,12.3) {RePReL \citep{kokel2021reprel}};
\node [font=\normalsize] at (7.75,11.9) {HRRL \citep{tadepalli_hrrl}};
\node [font=\normalsize] at (7.5,13.75) {MaRePReL};
\node [font=\normalsize] at (8.25,17.5) {MADDPG \citep{maddpg}};
\node [font=\normalsize] at (5.5,17) {Q-FTRL \citep{sampleefficientMARL}};
\node [font=\normalsize] at (9.25,17) {M-QMIX \citep{sampleGene}};
\node [font=\normalsize] at (6,17.75) {QMIX \citep{Qmix}};
\end{circuitikz}
}%
\caption{Our proposed framework w.r.t existing literature on relational, hierarchical, and multiagent RL}
\Description[]{Related Literature for our work}
\label{fig:Venn}
\end{figure}

Research in RL in the past three decades has focused on several extensions that make them adaptable to several real-world scenarios. First, Hierarchical Reinforcement Learning (HRL) methods have been introduced to manage complex tasks by decomposing them into smaller, more manageable subtasks \citep{hams}. These allow for more efficient learning and problem-solving by utilizing multiple levels of abstraction. Second, Relational Reinforcement Learning (RRL) addresses the complexity of environments where states and actions consist of objects and the relationships between those objects \citep{rrl}. RRL exploits a higher-order representation of the underlying relational structure to improve learning in such domains. Third, Multi-Agent Reinforcement Learning (MARL) has been developed to handle environments where dynamic changes arise from the presence and actions of other agents, making it particularly useful in competitive or cooperative multi-agent settings \citep{MARLBook}. Before introducing our framework, which addresses all three challenges, namely hierarchies, relational structures, and multi-agent domains, we review the relevant literature for these three RL extensions.

\textbf{Hierarchical Reinforcement Learning (HRL)} algorithms have been developed to tackle the complexity of long-horizon tasks by breaking them down into smaller, more manageable subtasks. Frameworks such as the Options  \citep{options} and MAXQ \citep{maxq} facilitate the learning of hierarchical policies across multiple levels of abstraction. By exploiting temporal abstraction, HRL transforms the original long-horizon task into a sequence of shorter-horizon subtasks, where each subtask represents a high-level action that spans a longer period than the lower-level actions carried out by agents deeper in the hierarchy. This hierarchical structure enhances the agentâ€™s ability to operate effectively over extended time horizons and significantly improves learning efficiency \citep{hrl_survey}. %Moreover, integrating automated planning techniques with reinforcement learning can further optimize HRL by guiding the learning of lower-level policies in a more structured and efficient manner \citep{TaskableRL}.

\textbf{Multiagent Reinforcement Learning (MARL)} extends reinforcement learning to systems with multiple agents, where they interact with the environment to maximize cumulative rewards \citep{MARLBook}. However, MARL introduces its own unique set of challenges. The first challenge is the curse of dimensionality, where the increasing number of agents leads to an exponential increase in the sizes of state and action spaces. The second challenge is the non-stationary nature of the environment %i.e., the evolution of the environment independently for an agent regardless of the action taken 
due to the actions taken by the other agents. The final issue for MARL is the sample inefficiency due to the large amount of data required to train such agents \citep{zhang2021multi}.

Numerous solutions have been proposed to address the challenges outlined above, falling into two main categories: those adapting the underlying architectures for the RL agents and those considering the overall tasks performed by the agents. In the former category, methods use function approximation techniques to combat the curse of dimensionality \citep{dreduction}. In addition, Centralized Training, and Decentralized Execution (CTDE) methods such as QMIX and MADDPG address the non-stationary nature of the environment \citep{Qmix, maddpg}. Generative modeling or mask reconstruction algorithms \citep{sampleGene, sampleefficientMARL} also fall in this category. In the latter category, hierarchical approaches such as HMARL \citep{hmarl} and HSD \citep{hsd} utilize task decomposition and hierarchical structures in multiagent settings to define task abstractions and improve sample inefficiency by filtering out irrelevant parts of the state space. Additionally, the structured task hierarchies introduced in such methods can facilitate agent communication to address the non-stationary nature of different multiagent environments. While powerful in standard propositional (and continuous) settings, these methods do not address the challenge of a rich, relational structure in the environment.

\textbf{Relational Reinforcement Learning (RRL)} considers the task of learning in environments where states and actions involve relationships between objects and their properties, i.e., relational domains \citep{rrl}. In these domains, RL agents must explicitly learn to reason about and exploit the relationships between objects ~\citep{tadepalli2004relational}. 
%In the past two decades, 
Previously, several works have demonstrated the need for a rich relational representation to be explicitly used inside the learning algorithms as against simply grounding all the objects and obtaining a feature-based representation~\cite{StaRAIBook,liftedbook}. A key advantage of relational representations is their ability to support abstractions and facilitate generalization and effective transfer across tasks \citep{SRLBook, starai_nesy, rrl_survey_ottolo}.  However, finding optimal policies in many relational domains is intractable even for moderately large problems \citep{tadepalli2004relational}. To mitigate this issue, algorithms that incorporate guidance and domain knowledge as constraints on the policy space have been developed ~\citep{rrl_guidance}. 

\textbf{Planning and RL integration} have been explored to exploit the power of hierarchical planning with deep RL enabling the use of HRL in continuous domains. While Taskable RL \citep{TaskableRL} demonstrated significant performance improvement, the underlying planner was still propositional, limiting their applicability to relational problems with varying numbers of objects and relations.

An ideal RL learning algorithm should be able to not only handle the rich relational structure of the domain but also have the ability to represent and reason with the decomposition of complex tasks into smaller ones. In other words, the algorithm must be capable of representing and reasoning with both hierarchies and relational structures. 
One such recent framework, RePReL \citep{kokel2021reprel}, employs a hierarchical relational planner to implement task-specific policies and uses Deep RL to work on hybrid relational domains~\citep{kokel2021reprel,kokel2022Journal}. To interface the higher-level planner with the Deep RL, a {\em hand-crafted} abstract reasoner is employed to lift the reasoning process and construct smaller lower-level Markov Decision Processes (MDPs) that can be solved efficiently. This approach has been demonstrated to be successful in domains with varying numbers of objects, complex task structures, and continuous state-action spaces. 

While the RePReL framework successfully handled relations and hierarchies in continuous spaces, it can not handle multiagent systems. More precisely, given the three-pronged challenge of complex task structures, rich object-centric environments, and multiagent 
domains, several advances have been made in each of these specific directions. Also, in the recent past, methods that arise from the combinations of these methods -- for instance, HRL with RRL~\citep{tadepalli_hrrl, maxq, kokel2021reprel}, MARL with RRL~\citep{marrl, marl_ilp}, HRL with MARL~\citep{hmarl, hsd, haven, alma} -- have been proposed. However, no significant research encompasses all three of these challenges (see Figure~\ref{fig:Venn}).

It is precisely this gap that we aim to address in this work. Specifically, we extend the RePReL framework to multiagent settings, utilizing the planner as both a scheduler and a centralized controller. Unlike RePReL, where the planner is solely responsible for task decomposition, our proposed framework also distributes tasks among multiple agents. This key enhancement enables our approach to effectively solve relational multiagent domains, as we explain in the next section.