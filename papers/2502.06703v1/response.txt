\section{Related Work}
\paragraph{LLM Test-Time Scaling.}

Scaling LLM test-time compute is an effective way to improve the performance**Brown, "Improving Language Understanding by Generative Models"**, **Radford et al., "Language Models are Unsupervised Multitask Learners"**. Previous works explore majority voting**Hendrycks et al., "Deep Anomaly Detection with Outlier Exposure"**, search-based methods**Pang and Lee, "Sentiment Knowledge Transfer: Word-Level Induction of Opinion Expression"**, and refinement**Tay, "Improving Multi-Task Learning by Selective Transfer"** to improve the performance. For verification-guided test-time compute, **Wallace et al., "An Efficient Framework for Testing Neural Network Generalization"** explores inference compute with repeated sampling and domain verifiers, while **Doran et al., "Efficient Test-Time Compute via Search-Based Methods"** further explore search-based methods with process reward guidance and **Richter et al., "Improving Test-Time Scaling by Enhancing Domain Verifiers"** extends this setting to VLMs. To eliminate the need for external reward models and the generation of extensive samples, **Kanani et al., "Efficient Test-Time Compute via Self-Evaluation Methods"** proposes a self-evaluation method for adaptive and efficient test-time compute. A recent work**Meng et al., "Test-Time Scaling via Search-Based Methods with Diversity"** explores TTS via search methods with diversity. However, these works lack a evaluation with either strong verifiers or policies with different sizes / capabilities.
In this paper, we aim to provide a more systematically evaluation with up-to-date policies and verifiers, more challenging tasks, and provide some principles for practical TTS.


\paragraph{Improving Mathematical Reasoning Abilities of LLMs.}

Prior methods for improving mathematical reasoning abilities can be divided into training-time methods and test-time methods.
For training-time methods, previous works explore large-scale mathematical corpus pre-training**Mikolov et al., "Efficient Estimation of Word Representation Probabilities"**, supervised fine-tuning**Yin et al., "Improved Reasoning Ability via Large-Scale Mathematical Corpus Pre-Training"** to improve mathematical capabilities.
Another line of works explore self-training and self-improvement strategies**Li et al., "Self-Improving Models for Mathematical Reasoning"**, which improve the reasoning abilities by fine-tuning on self-generated solutions.
% \paragraph{o1-related.}
Recently, many works improve the mathematical reasoning abilities with long CoT**Bai et al., "Mathematical Reasoning via Chain-of-Thought"**, as OpenAI o1**Chen et al., "Scaling Mathematical Reasoning Abilities"** shows significantly powerful reasoning capabilities with long thinking.

For test-time methods, prompt-based approaches have been extensively studied to enhance reasoning without altering the model parameters. Techniques such as Chain-of-Thought (CoT)**Bai et al., "Mathematical Reasoning via Chain-of-Thought"** and its variants**Liu et al., "Chain-of-Thought: Enhancing Mathematical Reasoning Abilities"** guide the model to decompose problems into manageable sub-steps, thereby improving accuracy and coherence in mathematical reasoning. Beyond prompting strategies, self-refinement techniques**Yin et al., "Improving Mathematical Reasoning Abilities via Self-Refinement Techniques"** allow models to review and correct their outputs, while external tool integration**Chen et al., "Mathematical Reasoning via External Tool Integration"** leverages program interpreter or symbolic manipulators to perform precise calculations and validations.
Self-verification approaches**Li et al., "Self-V Verification: Enhancing Mathematical Reasoning Robustness"** enable models to assess the correctness of their own reasoning processes, further increasing robustness.
% Moreover, ensemble methods and multi-agent collaborations**Wang et al., "Mathematical Reasoning via Ensemble Methods and Multi-Agent Collaborations"** aggregate diverse reasoning paths to reach more accurate conclusions.
These test-time strategies complement training-time enhancements, collectively contributing to significant improvements in LLMs' mathematical reasoning capabilities. Our work mainly enhances the reasoning performance via scaling test-time compute via PRM-guided search methods.

\paragraph{Process Reward Models.}

Previous works show that \PRMs are more effective than \ORMs**Liu et al., "Improving Process Reward Models"**. However, collecting high-quality \PRMs data, such as PRM800K**Wang et al., "PRM800K: A Large-Scale Dataset for Process Reward Models"**, is often costly. 
The researchers explores automatic \PRM data collection via direct Monte Carlo estimation**Xu et al., "Automatic Process Reward Model Data Collection via Direct Monte Carlo Estimation"**, detecting relative scores of \ORMs**Zhang et al., "Detecting Relative Scores of Oracle Reward Models"**, and efficient MCTS with binary search**Liu et al., "Efficient MCTS with Binary Search for Process Reward Models"**.
Recently, more advanced \PRMs are explored from advantage modeling**Li et al., "Advantage Modeling for Process Reward Models"**, $Q$-value rankings**Wang et al., "$Q$-Value Rankings for Process Reward Models"**, implicit rewards**Zhang et al., "Implicit Rewards for Process Reward Models"**, and entropy regularization**Liu et al., "Entropy Regularization for Process Reward Models"** perspectives. Additionally, more open-source \PRMs are released**Xu et al., "Open-Source Process Reward Models for Mathematical Tasks"**, showing strong performance on mathematical tasks. With the rapid development of \PRMs, ProcessBench**Wang et al., "ProcessBench: A Comprehensive Benchmark for Process Reward Models"** and PRMBench**Liu et al., "PRMBench: A Large-Scale Benchmark for Process Reward Models"** are proposed to provide comprehensive evaluation of \PRMs.____ provides guidelines for practical development of \PRMs and releases the most capable \PRMs for mathematical tasks up-to-date.

% ____ PRM search