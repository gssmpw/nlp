\section{Related Work}
\paragraph{LLM Test-Time Scaling.}

Scaling LLM test-time compute is an effective way to improve the performance~\citep{o1}. Previous works explore majority voting~\citep{Self-Consistency}, search-based methods~\citep{ToT, xie2024self, ARGS, wan2024alphazero}, and refinement~\citep{RISE} to improve the performance. For verification-guided test-time compute, \citet{brown2024large} explores inference compute with repeated sampling and domain verifiers, while \citet{MindStar, wu2024inference, snell2024scaling} further explore search-based methods with process reward guidance and \citet{xiyao2024scaling} extends this setting to VLMs. To eliminate the need for external reward models and the generation of extensive samples, \citet{manvi2024adaptive} proposes a self-evaluation method for adaptive and efficient test-time compute. A recent work~\citep{huggingface2024scaling} explores TTS via search methods with diversity. However, these works lack a evaluation with either strong verifiers or policies with different sizes / capabilities.
In this paper, we aim to provide a more systematically evaluation with up-to-date policies and verifiers, more challenging tasks, and provide some principles for practical TTS.


\paragraph{Improving Mathematical Reasoning Abilities of LLMs.}

Prior methods for improving mathematical reasoning abilities can be divided into training-time methods and test-time methods.
For training-time methods, previous works explore large-scale mathematical corpus pre-training~\citep{GPT-4, Llemma, DeepSeekMath} and supervised fine-tuning~\citep{Wizardmath, MetaMath, ToRA, MathScale, DART-Math, Skywork-Math} to improve mathematical capabilities.
Another line of works explore self-training and self-improvement strategies~\citep{STaR, ReST, ReFT, V-STaR, Quiet-STaR, ReST-MCTS*, setlur2024rl, kumar2024training, PRIME}, which improve the reasoning abilities by fine-tuning on self-generated solutions.
% \paragraph{o1-related.}
Recently, many works improve the mathematical reasoning abilities with long CoT~\citep{o1-Journey1, o1-Journey2, k0-math, DeepSeek-R1, QwQ, Skywork-o1, Marco-o1}, as OpenAI o1~\citep{o1} shows significantly powerful reasoning capabilities with long thinking.

For test-time methods, prompt-based approaches have been extensively studied to enhance reasoning without altering the model parameters. Techniques such as Chain-of-Thought (CoT)~\citep{CoT} and its variants~\citep{ToT, CoMAT} guide the model to decompose problems into manageable sub-steps, thereby improving accuracy and coherence in mathematical reasoning. Beyond prompting strategies, self-refinement techniques~\citep{Self-Refine} allow models to review and correct their outputs, while external tool integration~\citep{PAL, PoT} leverages program interpreter or symbolic manipulators to perform precise calculations and validations.
Self-verification approaches~\citep{Self-Verification} enable models to assess the correctness of their own reasoning processes, further increasing robustness.
% Moreover, ensemble methods and multi-agent collaborations~\citep{ensemble_methods, multi_model} aggregate diverse reasoning paths to reach more accurate conclusions.
These test-time strategies complement training-time enhancements, collectively contributing to significant improvements in LLMs' mathematical reasoning capabilities. Our work mainly enhances the reasoning performance via scaling test-time compute via PRM-guided search methods.

\paragraph{Process Reward Models.}

Previous works show that \PRMs are more effective than \ORMs~\citep{uesato2022solving, PRM800K}. However, collecting high-quality \PRMs data, such as PRM800K~\citep{PRM800K}, is often costly. 
The researchers explores automatic \PRM data collection via direct Monte Carlo estimation~\citep{Math-Shepherd}, detecting relative scores of \ORMs~\citep{AutoPSV}, and efficient MCTS with binary search~\citep{OmegaPRM}.
Recently, more advanced \PRMs are explored from advantage modeling~\citep{PAV}, $Q$-value rankings~\citep{PQM}, implicit rewards~\citep{Implicit-PRM}, and entropy regularization~\citep{ER-PRM} perspectives. Additionally, more open-source \PRMs are released~\citep{RLHFlow, Skywork-o1, ER-PRM, PQM, Implicit-PRM, PRMLessons}, showing strong performance on mathematical tasks. With the rapid development of \PRMs, ProcessBench~\citep{ProcessBench} and PRMBench~\citep{PRMBench} are proposed to provide comprehensive evaluation of \PRMs.~\citet{PRMLessons} provides guidelines for practical development of \PRMs and releases the most capable \PRMs for mathematical tasks up-to-date.

% \citet{ma2023let} PRM search