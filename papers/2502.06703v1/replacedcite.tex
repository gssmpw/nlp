\section{Related Work}
\paragraph{LLM Test-Time Scaling.}

Scaling LLM test-time compute is an effective way to improve the performance____. Previous works explore majority voting____, search-based methods____, and refinement____ to improve the performance. For verification-guided test-time compute, ____ explores inference compute with repeated sampling and domain verifiers, while ____ further explore search-based methods with process reward guidance and ____ extends this setting to VLMs. To eliminate the need for external reward models and the generation of extensive samples, ____ proposes a self-evaluation method for adaptive and efficient test-time compute. A recent work____ explores TTS via search methods with diversity. However, these works lack a evaluation with either strong verifiers or policies with different sizes / capabilities.
In this paper, we aim to provide a more systematically evaluation with up-to-date policies and verifiers, more challenging tasks, and provide some principles for practical TTS.


\paragraph{Improving Mathematical Reasoning Abilities of LLMs.}

Prior methods for improving mathematical reasoning abilities can be divided into training-time methods and test-time methods.
For training-time methods, previous works explore large-scale mathematical corpus pre-training____ and supervised fine-tuning____ to improve mathematical capabilities.
Another line of works explore self-training and self-improvement strategies____, which improve the reasoning abilities by fine-tuning on self-generated solutions.
% \paragraph{o1-related.}
Recently, many works improve the mathematical reasoning abilities with long CoT____, as OpenAI o1____ shows significantly powerful reasoning capabilities with long thinking.

For test-time methods, prompt-based approaches have been extensively studied to enhance reasoning without altering the model parameters. Techniques such as Chain-of-Thought (CoT)____ and its variants____ guide the model to decompose problems into manageable sub-steps, thereby improving accuracy and coherence in mathematical reasoning. Beyond prompting strategies, self-refinement techniques____ allow models to review and correct their outputs, while external tool integration____ leverages program interpreter or symbolic manipulators to perform precise calculations and validations.
Self-verification approaches____ enable models to assess the correctness of their own reasoning processes, further increasing robustness.
% Moreover, ensemble methods and multi-agent collaborations____ aggregate diverse reasoning paths to reach more accurate conclusions.
These test-time strategies complement training-time enhancements, collectively contributing to significant improvements in LLMs' mathematical reasoning capabilities. Our work mainly enhances the reasoning performance via scaling test-time compute via PRM-guided search methods.

\paragraph{Process Reward Models.}

Previous works show that \PRMs are more effective than \ORMs____. However, collecting high-quality \PRMs data, such as PRM800K____, is often costly. 
The researchers explores automatic \PRM data collection via direct Monte Carlo estimation____, detecting relative scores of \ORMs____, and efficient MCTS with binary search____.
Recently, more advanced \PRMs are explored from advantage modeling____, $Q$-value rankings____, implicit rewards____, and entropy regularization____ perspectives. Additionally, more open-source \PRMs are released____, showing strong performance on mathematical tasks. With the rapid development of \PRMs, ProcessBench____ and PRMBench____ are proposed to provide comprehensive evaluation of \PRMs.____ provides guidelines for practical development of \PRMs and releases the most capable \PRMs for mathematical tasks up-to-date.

% ____ PRM search