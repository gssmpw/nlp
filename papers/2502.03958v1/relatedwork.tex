\section{Related work}
We broadly categorize the related work for solving \eqref{eqn:basic_opt} into algorithms that consider smooth ($g=0$) and non-smooth ($g\neq 0$) losses.

{\bf  Smooth FL problems. }
FedAvg was originally proposed in \cite{mcmahan2017communication} for federated
learning of deep networks, but without convergence analysis. A comprehensive analysis of FedAvg for strongly convex problems was conducted in \cite{stich2018local} under the assumption of homogeneous data distribution among clients. %
However, when data is heterogeneous, the use of local updates in FedAvg leads to the issue of client drift. As highlighted in~\cite{zhang2021fedpd}, without strong assumptions on the problem structure, the behavior of FedAvg can become highly erratic due to the impact of client drift. 
The impact of client drift on FedAvg was theoretically analyzed in~\cite{li2019convergence,zhang2021fedpd} under the assumption of bounded data heterogeneity. While \cite{li2019convergence} focused on strongly convex problems,  \cite{zhang2021fedpd} explored non-convex problems. 
%
Another line of work attempts to reduce or eliminate client drift by modifying FedAvg at the algorithmic level~\cite{li2020federated-fedprox,c1, karimireddy2020scaffold,pathak2020fedsplit,karimireddy2020mime}.  
% 
{For example, \cite{li2020federated-fedprox} and \cite{c1} penalized the difference between the local models on clients and the global model on the server to ensure that the local models remain close, thereby reducing client drift. Both \cite{li2020federated-fedprox} and \cite{c1} established convergence for non-convex problems.} Scaffold \cite{karimireddy2020scaffold} and Mime \cite{karimireddy2020mime} tackle client drift by designing control variates to correct the local direction during the local updates, achieving convergence for non-convex problems.  
%
A drawback of these approaches is their need to communicate also the control variates, which increases the overall communication cost.  
%
In contrast, Fedsplit \cite{pathak2020fedsplit} adopts Peaceman-Rachford splitting \cite{he2014strictly} to address the client drift through a consensus reformulation of the original problem, exchanging only one local model per communication round for convex problems.
%
None of the methods discussed above handles composite FL problems.


{\bf Composite FL problems.} 
Compared to the extensive research on smooth FL problems,  composite problems have received significantly less attention. An effort to bridge this gap is the Federated Dual Averaging (FedDA) introduced in~\cite{yuan2021federated}. In FedDA, each client employs dual averaging~\cite{nesterov2009primal} during the local updates,  while the server averages the local models in the dual space and applies the proximal step. 
Convergence was established for convex problems, with the ability to handle general loss functions as long as the gradients are bounded. However, under data heterogeneity, the convergence analysis is limited to quadratic loss functions.
%
The Fast Federated Dual Averaging (Fast-FedDA) algorithm~\cite{bao2022fast} incorporates weighted summation of past gradient and model information during the local updates, but it introduces additional communication overhead.  While Fast-FedDA achieves convergence for general loss functions, it still relies on the assumption of bounded heterogeneity and can only handle strongly convex problems.
%
Federated Douglas-Rachford (FedDR) was introduced in \cite{tran2021feddr} and is able to avoid the bounded heterogeneity assumption. A subsequent development, FedADMM \cite{wang2022fedadmm}, utilizes FedDR to solve the dual problem of \eqref{eqn:basic_opt} and was demonstrated to have identical performance to FedDR. 
%
For both FedDR and FedADMM, convergence is established for non-convex problems and the local updates implement an inexact evaluation of the proximal operator of the smooth loss with adaptive accuracy. However, to ensure convergence, the accuracy needs to increase by every iteration, resulting in an impractically large number of local updates.

{The paper~\cite{c3} utilizes the mini-batch stochastic proximal point (MSPP) method as the local
stochastic optimal oracle for FedProx~\cite{li2020federated-fedprox}. The proximal term is used to mitigate client drift, but to guarantee convergence, the local subproblem needs to be solved exactly, which implies a large number of local updates. The paper~\cite{c3} established convergence 
for Lipschitz-continuous and weakly convex loss functions.
%under conditions where the loss function is Lipschitz continuous and %weakly convex.
The work~\cite{c2} investigated differential privacy in non-convex federated learning. It proposed two variance-reduced algorithms for solving composite problems under the proximal Polyak-{\L}ojasiewicz (PL) inequality and general non-convexity, respectively. However, the algorithms in~\cite{c2} utilize a single local update, leading to frequent communication between clients and the server, and
assume that the smooth part of the composite loss is Lipschitz continuous. }