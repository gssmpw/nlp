\newpage
\appendix
\onecolumn
\section*{Contents of the Appendix}
The Appendix is organized as follows:
\begin{itemize}[leftmargin=*]
  \vspace{-0.5em}
% \setlength\itemsep{-0.1em}
\item
    In \cref{sec:appendix_symbol}, we summarize the symbols and notation used in this paper.
\item Preliminaries on single-step RLHF can be found in \cref{prelimi:singlestep}.
\item In \cref{sec:additional_relatedwork}, we add additional related work and elaborate on the difference and contribution of our work. 
\item In \cref{sec:proof}, we provide the proofs for the theoretical results.
\item \cref{app:implement_forb} shows the implementation of Algorithm~\ref{alg:forb} with updates over policies.
\item \cref{sec:appendix_exp} provides an overview of the MT-bench 101 benchmark in the experiment.
\end{itemize}

\section{Symbols and Notation}
\label{sec:appendix_symbol}
We include the core symbols and notation in \cref{table:symbols_and_notations} to facilitate the understanding of our work.

\begin{table}[ht]
\caption{Core symbols and notations used in this paper.
% \grig{Will this be in the main paper or move to the supplementary?}\zhenyu{supplementary, many symbols are not used in the text} \grig{Great; then please move it in the supplementary.}
}
\label{table:symbols_and_notations}
\small
\centering
\begin{tabular}{c | c | c}
\toprule
Symbol & Dimension(s) $\&$ range & Definition \\
\midrule
$x_h$ & - & Prompt at step $h$
\\
$a_h$ & - & Answer (action) at step $h$
\\
$s_h$ & - & State at step $h$ 
\\
$s_1(s_h)$ & - &  The only initial state that can lead to $s_h$
\\
$\pi$ &  & Language model (policy)
\\
$\initial$ &  & Initial distribution of state $s_1$
\\
 $d_h^\pi(s,a)$ & $[0,1]$ &  Occupancy measure of $\pi$ at stage $h$
\\
$f$ &  & Transition function  
\\
$\mathrm{Pr}(s_h=s,a_h=a)$ &  $[0,1]$ & Joint probability of $s_h=a$ and $a_h=a$
\\
$o$ &  $\{0,1\}$ & Preference oracle \\
$\mathbb{P}([s,a],[s^\prime,a^\prime)]$ & $[0,1]$ & Winning probability of $[s,a]$ against $[s^\prime,a^\prime)]$
\\
$D(p \| q)$ &  & KL divergence of two probability distributions $p$ and $q$ 
\\ 
$\mathbb{D}(p \| q)$ &  &
Bregman Divergences between two points $q$ and $p$.
\\
$\mathcal{D}_t$ &  & Dataset buffet at iteration t
\\
$\Delta_{\mathcal{X}}$ & $[0,1]^{\abs{\mathcal{X}}}$ & Set of probability distributions over the set $\mathcal{X}$
\\
% define a preference oracle as $o([x_1,a_1],[x_1,a_1^\prime]) \in \{0,1\}$, which can provide preference feedback with 0-1 scores for $[x_1,a_1]$ and $[x_1,a_1^\prime]$.  
% We denote $\mathbb{P}([x_1,a_1]\succ [x_1,a_1^\prime]) = \mathbb{E}[o([x_1,a_1] \succ [x_1,a_1^\prime])]$ as the probability of the answer $a_1$ outperforms $a_1'$. Moreover, we have $\mathbb{P}([x_1,a_1]\succ [x_1,a_1^\prime]) = 1 - \mathbb{P}([x_1,a_1^\prime]\succ [x_1,a_1])$.
% $\mathcal{N}(\mu,\sigma) $ & - & Gaussian distribution of mean $\mu$ and variance $\sigma$
% \\
\midrule
$\mathcal{O}$, $o$, $\Omega$ and $\Theta$ & - & Standard Bachmannâ€“Landau order notation\\
\midrule
\end{tabular}
\end{table}
We additionally use a compact notation for representing the Bellman flow constraints. We denote by $E \in \mathbb{R}^{\abs{\mathcal{S}}\times\abs{\mathcal{A}}\abs{\mathcal{S}}}$ the matrix such that $(E z)(s, a) = z(s)$ for all vectors $z \in \mathbb{R}^{\abs{\mathcal{S}}}$. Additionally, we denote by $F$ the matrix such that $(F z)(s,a) = \sum_{s'}f(s'|s,a)z(s')$ for all vectors $z \in \mathbb{R}^{\abs{\mathcal{S}}}$.

\section{Preliminary on Single-step RLHF}
\label{prelimi:singlestep}
% \subsection{Single-step RLHF}
In this section, we review the earlier methods in single-step RLHF. Classical RLHF methods~\citep{ziegler2019fine,ouyang2022training} assume that the preference oracle can be expressed by an underlying Bradley-Terry (BT) reward model~\citep{bradley1952rank}, i.e., 
$$\mathbb{P}([x_1,a_1]\succ [x_1,a_1']) = \sigmoid(r(x_1,a_1)-r(x_1,a_1'))\,.$$ Thus, one can first learn a reward model and optimize the policy based on the following KL-constrained RL objective with PPO:
$$
\pi^\star = \argmax_{\pi} \mathbb{E}
_{x_1 \sim \initial,a_1 \sim \pi(\cdot|x_1)}
(r(x_1,a_1) -\beta D(\pi(\cdot|x_1) || 
\pi_{\rm ref}
(\cdot|x_1)) )\,,
$$
where $\beta$ is a parameter controlling the deviation from the reference model $\pi_{\rm ref}$. Another line of work, e.g., DPO~\citep{rafailov2023direct} avoids explicit reward modeling and optimizes the following objective over pair-wise preference data ${(x_1,a_1^w,a_1^l})$.
$$
\pi^\star = \argmax_{\pi} \mathbb{E}
_{(x_1,a_1^w,a_1^l) \sim \mathcal{D} }
\Bigg[\log \sigmoid \left(\beta \log\frac{\pi (a_1^w|x_1)}{\pi_{\rm ref}(a_1^w|x_1)} -
\beta \log\frac{\pi (a_1^l|x_1)}{\pi_{\rm ref}(a_1^l|x_1)} 
\right)
\Bigg]\,.
$$
More recently, several studies ~\citep{swamyminimaximalist,munos2024nash,wu2024self,zhang2024iterative,rosset2024direct} have circumvented the Bradley-Terry (BT) assumption by directly modeling the general oracle $\mathbb{P}$, avoiding the reliance on the reward model which is transitive. Specifically, the goal is to identify the Nash equilibrium (or von Neumann winner) of the following two-player constant-sum game:
\begin{equation*}
\begin{split}
     (\pi^*, \pi^*)
    = &
    \arg \max_{\pi}\min_{\pi'}
    \mathbb{E}_{x_1 \sim \initial,a_1 \sim \pi(\cdot|x_1),a_1^\prime \sim \pi^\prime(\cdot|x_1)}
     \mathbb{P}([x_1,a_1] \succ [x_1,a_1^\prime])\,.
\end{split}
\end{equation*}




\section{Additional discussion on related work}
\label{sec:additional_relatedwork}
\subsection{Related work on two-player markov game \& optimistic online gradient descent}
Two-player Markov games have been widely studied since the seminal work \citep{shapley1953stochastic}. Particularly relevant to our work is the research line on policy gradient algorithms for two-player Markov games such as \citet{daskalakis2020independent,wei2021last,alacaoglu2022natural}.
% \textbf{optimistic online gradient descent} 
Our \oomdmethod{} is strictly related to the idea of optimistic online gradient descent \citep{popov1980modification,chiang2012online,rakhlin2013online} originally proposed in online learning to achieve small regret in case of slow varying loss sequences. Our update that uses only one projection per update was proposed in \citet{joulani17a}. The name of our method is due to a similar algorithm introduced in the context of variational inequalities by \citet{malitsky2020forward}.
\rebuttal{
\subsection{Related work on token-level preference optimization}
A line of work formulates the alignment of contextual bandit problems in LLMs (Example.\ref{example:1}) from token-level MDPs perspective~\citep{rafailov2024r, zengtoken, liu2024tis}. In \citet{rafailov2024r}, by defining the reward at each token before the terminal token as the generation likelihood and using the maximum entropy RL objective, the authors derive the original objective of DPO from a new perspective that incorporates token-level rewards. \citet{zengtoken} assume that the reward for a response can be decomposed into token-level rewards at each token. Then they design a token-level objective function based on Trust Region Policy Optimization, adding token-level KL divergence constraints to the DPO objective in the final algorithm. More recently, ~\citet{liu2024tis} study how the difference in average rewards between chosen and rejected responses affects the optimization stability, designing a new algorithm where importance sampling weights are assigned to each token-level reward.
There are two main differences between the multi-step alignment approach in our work and those in previous work. First, while \citet{rafailov2024r, zengtoken, liu2024tis} develop alignment methods based on the Bradley-Terry model with transitive rewards, our framework is motivated by a two-player game with relative rewards. Secondly, although \citet{rafailov2024r, zengtoken, liu2024tis} formulate the alignment process as an MDP, their final objective is tailored to a contextual bandit problem in LLMs. In contrast, our objective is designed for a multi-step alignment problem, suited for multi-turn conversation or chain-of-thought reasoning.
\subsection{Discussion on the difference from SPPO}
Next, we elaborate on the difference with SPPO~\citep{wu2024self} below: Firstly, the theoretical analysis of the proposed MPO differs from that of SPPO due to differences in the settings. SPPO considers the contextual bandit problem and builds its analysis based on the game matrix from~\citet{freund1999adaptive}. In our case, however, we frame the problem as a Markov game and employ a distinct theoretical analysis apart from~\citet{freund1999adaptive}. Specifically, in our proof, we (i) use the performance difference lemma to rewrite the global regret as weighted average of local regrets and (ii) control the local regrets with multiplicative weights updates. Secondly, a new algorithm, OMPO, is developed in this work with a novel theoretical guarantee. In the case where the horizon $H = 1$, the update of OMPO reduces to $$\pi^{t+1}(a|s) \propto \pi^t(a|s) \exp{[\beta (2\mathbb{P}(a\succ \pi^t(\cdot|s)) - \mathbb{P}(a\succ \pi^{t-1}(\cdot|s)))]},$$ while the update of SPPO is $$ \pi^{t+1}(a|s) \propto \pi^t(a|s) \exp{[\beta (\mathbb{P}(a\succ \pi^t(\cdot|s)))]}.$$ As a result, OMPO enables \( \mathcal{O}(\epsilon^{-1}) \) policy updates to converge to an \( \epsilon \)-approximate Nash equilibrium instead of \( \mathcal{O}(\epsilon^{-2}) \), according to our theoretical analysis.
}


\section{Proofs}
\label{sec:proof}
\subsection{Proof of \cref{lemma:bellman}}
\label{proof:bellmam}
\begin{proof}
By the definition of the state action value function for the policy pair 
$(\pi,\pi^\prime)$
we have that 
\begin{equation*}
\begin{split}
& Q^{\pi,\pi^\prime}_h(s,a,s^\prime,a^\prime) = 
r(s,a,s^\prime,a^\prime)
% \\ & \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
+
\mathbb{E}
% _{\pi,\pi^\prime}
% {\substack{(a_{h+1},\dots, s_H,a_H) \sim \pi \\  
% a_{h+1}^\prime ,\dots, s_H^\prime,a_H^\prime) \sim \pi^\prime }}
% \left(
\Big[ 
\sum_{h'=h+1}^{H}  r(s_{h'},a_{h'},s_{h'}^\prime,a_{h'}^\prime)
% \right)
\Big]
\,.
\end{split}
\end{equation*}
Now, using tower property of the expectation we have that
\begin{align*}
&Q_h^{\pi,\pi^\prime}(s,a,s^\prime,a^\prime) \\
&= 
r(s,a,s^\prime,a^\prime)
% \\ & \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
+
\mathbb{E}_{s''\sim f(\cdot|s,a),\bar{s}\sim f(\cdot|s',a')}\Big[\mathbb{E}
% _{\pi,\pi^\prime}
% {\substack{(a_{h+1},\dots, s_H,a_H) \sim \pi \\  
% a_{h+1}^\prime ,\dots, s_H^\prime,a_H^\prime) \sim \pi^\prime }}
% \left(
\Big[ 
\sum_{h'=h+1}^{H} r(s_{h'},a_{h'},s_{h'}^\prime,a_{h'}^\prime) | s_{h+1} = s'', s'_{h+1} = \bar{s}
% \right)
\Big]\Big]
\\
&=
r(s,a,s^\prime,a^\prime)
% \\ & \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
+
 \mathbb{E}_{s''\sim f(\cdot|s,a),\bar{s}\sim f(\cdot|s',a')}\Big[V^{\pi,\pi'}(s'',\bar{s})\Big],
\end{align*}
where the last equality follows from the definition of the state value function.
\end{proof}

\subsection{Proof of \cref{lemma:valuediff}}
\label{proof:valuedifflemma}
\begin{proof}
\if 0
We write down the difference of value function at step 0 as follows:
\begin{equation}
\begin{split}
 &   V^{\pi, \overline{\pi}}(s_1,s_1) - 
        V^{\pi^\prime, \overline{\pi}}(s_1,s_1)
    \\ &      = \mathbb{E}_{a_1 \sim \pi(\cdot|s_1),
    \overline{a}_0 \sim \overline{\pi}(\cdot|s_1)}
     Q^{\pi,\overline{\pi}}(s_1,a_1,s_1,\overline{a}_0)
     -
     \mathbb{E}_{a_1 \sim \pi^\prime(\cdot|s_1),
    \overline{a}_0 \sim \overline{\pi}(\cdot|s_1)}
     Q^{\pi^\prime,\overline{\pi}}(s_1,a_1^\prime,s_1,\overline{a}_0)
     \\ & =\mathbb{E}_{\overline{a}_0 \sim \overline{\pi}(\cdot|s_1)} \left[
     \langle  \pi(\cdot|s_1 ), Q^{\pi,\overline{\pi}}(s_1, \cdot,s_1,\overline{a}_0)
     \rangle
     -
        \langle  \pi^\prime(\cdot|s_1 ), Q^{\pi^\prime,\overline{\pi}}(s_1, \cdot,s_1,\overline{a}_0)
     \rangle
     \right]
     \\ &= 
     \mathbb{E}_{\overline{a}_0 \sim \overline{\pi}(\cdot|s_1)} \left[
     \langle  \pi(\cdot|s_1 ) -\pi^\prime(\cdot|s_1 ) , Q^{\pi,\overline{\pi}}(s_1, \cdot,s_1,\overline{a}_0)
     \rangle \right]
          \\ &  \hspace{5cm} +  \underset{\vardiamond}{\underbrace{\mathbb{E}_{\overline{a}_0 \sim \overline{\pi}(\cdot|s_1)}   \left[
        \langle  \pi^\prime(\cdot|s_1 ), 
        Q^{\pi,\overline{\pi}}(s_1, \cdot,s_1,\overline{a}_0)-Q^{\pi^\prime,\overline{\pi}}(s_1, \cdot,s_1,\overline{a}_0) 
     \rangle      \right] }}
\end{split}
\end{equation}
The term $\vardiamond$ can be written as follows:
\begin{equation}
    \begin{split}
  \vardiamond   &= 
    \mathbb{E}_{\overline{a}_0 \sim \overline{\pi}(\cdot|s_1),
    a_1^\prime  \sim \pi^\prime(\cdot|s_1)
    }   \left[
        Q^{\pi,\overline{\pi}}(s_1, a_1^\prime ,s_1,\overline{a}_0)-Q^{\pi^\prime,\overline{\pi}}(s_1, a_1^\prime,s_1,\overline{a}_0) 
     \rangle      \right]
    \\  &  = 
     \mathbb{E}_{\overline{a}_0 \sim \overline{\pi}(\cdot|s_1),
    a_1^\prime  \sim \pi^\prime(\cdot|s_1)
    }  \left[r(s_1, a_1^\prime ,s_1,\overline{a}_0)
    +\gamma \mathbb{E}_{s_1^\prime, \overline{s}_1} V^{\pi,\overline{\pi}}(s_1^\prime,\overline{s}_1)
    -
    r(s_1, a_1^\prime ,s_1,\overline{a}_0)
    -\gamma \mathbb{E}_{s_1^\prime, \overline{s}_1} 
    V^{\pi^\prime,\overline{\pi}}(s_1^\prime,\overline{s}_1)
    \right]
    \\  &  = 
     \gamma   \mathbb{E}_{\overline{a}_0 \sim \overline{\pi}(\cdot|s_1),
    a_1^\prime  \sim \pi^\prime(\cdot|s_1)
    }  
     \mathbb{E}_{s_1^\prime \sim f(\cdot|[s_1,a_1^\prime]), \overline{s}_1 \sim f(\cdot|[s_1,\overline{a}_0])} 
    \underset{\clubsuit}{\underbrace{  
        \left[
 V^{\pi,\overline{\pi}}(s_1^\prime,\overline{s}_1)
    -
   V^{\pi^\prime,\overline{\pi}}(s_1^\prime,\overline{s}_1)
       \right]
   }}
    \end{split}
\end{equation}
Next, we expand $\clubsuit$ as follows:
\begin{equation}
    \begin{split}
 \clubsuit  
 & = 
\mathbb{E}_{a_1^\prime \sim \pi(\cdot|s_1^\prime),
    \overline{a}_1 \sim \overline{\pi}(\cdot|\overline{s}_1)}
     Q^{\pi,\overline{\pi}}(s_1^\prime,a_1^\prime,s_1^\prime,\overline{a}_1)
     -
     \mathbb{E}_{a_1 \sim \pi^\prime(\cdot|s_1^\prime),
    \overline{a}_1 \sim \overline{\pi}(\cdot|\overline{s}_1)}
     Q^{\pi^\prime,\overline{\pi}}(s_1^\prime,a_1^\prime,s_1^\prime,\overline{a}_1)
\\ & =
    \mathbb{E}_{\overline{a}_1 \sim \overline{\pi}(\cdot|\overline{s}_1)} \left[
     \langle  \pi(\cdot|s_1^\prime ), Q^{\pi,\overline{\pi}}(s_1^\prime, \cdot,s_1^\prime,\overline{a}_1)
     \rangle
     -
        \langle  \pi^\prime(\cdot|s_1^\prime ), Q^{\pi^\prime,\overline{\pi}}(s_1^\prime, \cdot,s_1^\prime,\overline{a}_1)
     \rangle
     \right]
\\ &= 
     \mathbb{E}_{\overline{a}_1 \sim \overline{\pi}(\cdot|\overline{s}_1)} \left[
     \langle  \pi(\cdot|s_1^\prime ) -\pi^\prime(\cdot|s_1^\prime ) , Q^{\pi,\overline{\pi}}(s_1^\prime, \cdot,s_1^\prime,\overline{a}_1)
     \rangle \right]
\\ &  \hspace{5cm} + \underset{\spadesuit}{\underbrace{   \mathbb{E}_{\overline{a}_1 \sim \overline{\pi}(\cdot|\overline{s}_1)}   \left[
        \langle  \pi^\prime(\cdot|s_1^\prime ), 
        Q^{\pi,\overline{\pi}}(s_1^\prime, \cdot,s_1^\prime,\overline{a}_1)-Q^{\pi^\prime,\overline{\pi}}(s_1^\prime, \cdot,s_1^\prime,\overline{a}_0) 
     \rangle      \right]}}
    \end{split}
\end{equation}
Similarly, the term $\spadesuit$ can be expressed as follows:
\begin{equation}
\begin{split}
    \spadesuit &= 
    \mathbb{E}_{\overline{a}_1 \sim \overline{\pi}(\cdot|\overline{s}_1),
    a_1^\prime \sim \pi^\prime(\cdot|s_1^\prime)
    }   \left[
        Q^{\pi,\overline{\pi}}(s_1^\prime, a_1^\prime,s_1^\prime,\overline{a}_1)-Q^{\pi^\prime,\overline{\pi}}(s_1^\prime, a_1^\prime,s_1^\prime,\overline{a}_0) 
    \right]
    \\  &  = 
     \mathbb{E}_{\overline{a}_1 \sim \overline{\pi}(\cdot|\overline{s}_1),
    a_1^\prime  \sim \pi^\prime(\cdot|s_1^\prime)
    }  \left[r(s_1^\prime, a_1^\prime ,s_1^\prime,\overline{a}_1)
    +\gamma \mathbb{E}_{s_2^\prime, \overline{s}_2} V^{\pi,\overline{\pi}}(s_2^\prime,\overline{s}_2)
    -
    r(s_1^\prime, a_1^\prime ,s_1^\prime,\overline{a}_1)
    -\gamma \mathbb{E}_{s_2^\prime, \overline{s}_2} 
    V^{\pi^\prime,\overline{\pi}}(s_2^\prime,\overline{s}_2)
    \right]
    \\  &  = 
     \gamma   \mathbb{E}_{\overline{a}_1 \sim \overline{\pi}(\cdot|\overline{s}_1),
    a_1^\prime  \sim \pi^\prime(\cdot|s^\prime_1)
    }  
     \mathbb{E}_{s_2^\prime \sim f(\cdot|[s_1^\prime,a_1^\prime]), \overline{s}_2 \sim f(\cdot|[s_1^\prime,\overline{a}_1])} 
        \left[
 V^{\pi,\overline{\pi}}(s_2^\prime,\overline{s}_2)
    -
   V^{\pi^\prime,\overline{\pi}}(s_2^\prime,\overline{s}_2)
       \right]
\end{split}
\end{equation}
Thus, the proof can be completed by recursively performing such step over $H$.
\fi
 Let us consider the Bellman equation in vectorial form for the policy pair $(\pi', \bar{\pi})$, that is
\begin{equation*}
    r_h + F V_{h+1}^{\pi', \bar{\pi}} = Q_h^{\pi', \bar{\pi}},
\end{equation*}
where $F$ denoted the transition matrix induced by the transition function $f:\mathcal{S}^2\times\mathcal{A}\rightarrow \Delta_{\mathcal{S}\times \mathcal{S}}$.
Now, multiplying by the occupancy measure of the policy pair $(\pi, \bar{\pi})$ at stage $h$ we obtain
\begin{equation*}
\innerprod{d^{\pi, \bar{\pi}}_h}{r_h} + \innerprod{d^{\pi, \bar{\pi}}_h}{ F V_{h+1}^{\pi', \bar{\pi}}} = \innerprod{d_h^{\pi, \bar{\pi}}}{Q_h^{\pi', \bar{\pi}}}.
\end{equation*}
At this point, using the Bellman flow constraints \cite{Puterman:1994}, it holds that
\begin{equation*}
     F^T d_h^{\pi, \bar{\pi}} = E^T d_{h+1}^{\pi, \bar{\pi}},
\end{equation*}
where $E \in \mathbb{R}^{\abs{\mathcal{S}}^2\abs{\mathcal{A}} \times \abs{\mathcal{S}}^2}$ such that $(E^T V)(s,a) = V(s) $ for all $V \in \mathbb{R}^{\abs{\mathcal{S}}^2}$.
Plugging this equality in the Bellman equation above we obtain
\begin{equation*}
\innerprod{d^{\pi, \bar{\pi}}_h}{r_h} + \innerprod{d_{h+1}^{\pi, \bar{\pi}}}{ E V_{h+1}^{\pi', \bar{\pi}}} = \innerprod{d_h^{\pi, \bar{\pi}}}{Q_h^{\pi', \bar{\pi}}}.
\end{equation*}
Now, subtracting on both sides $\innerprod{d_{h}^{\pi, \bar{\pi}}}{ E V_{h}^{\pi', \bar{\pi}}}$ and rearranging,  it holds that
\begin{equation*} \innerprod{d^{\pi, \bar{\pi}}_h}{r_h} + \innerprod{d_{h+1}^{\pi, \bar{\pi}}}{ E V_{h+1}^{\pi', \bar{\pi}}} - \innerprod{d_{h}^{\pi, \bar{\pi}}}{ E V_{h}^{\pi', \bar{\pi}}} = \innerprod{d_h^{\pi, \bar{\pi}}}{Q_h^{\pi', \bar{\pi}} - E V_h^{\pi', \bar{\pi}}}.
\end{equation*}
After this, taking sum from $h=1$ to $H$ and recognizing that for all policy pairs $(\pi,\pi')$ it holds that $V^{\pi,\pi'}_{H+1}=0$, it holds that
\begin{equation*} \sum^H_{h=1}\innerprod{d^{\pi, \bar{\pi}}_h}{r_h} - \innerprod{d_{1}^{\pi, \bar{\pi}}}{ E V_{1}^{\pi', \bar{\pi}}} = \sum^H_{h=1}\innerprod{d_h^{\pi, \bar{\pi}}}{Q_h^{\pi', \bar{\pi}} - E V_h^{\pi', \bar{\pi}}}.
\end{equation*}
Then, notice that for all policies $\pi, \bar{\pi}$ it holds that 
% $d_{1}^{\pi, \bar{\pi}} = \initial$ and that 
$\sum^H_{h=1}\innerprod{d^{\pi, \bar{\pi}}_h}{r_h} = \innerprod{\initial}{V^{\pi, \bar{\pi}}}$. Plugging in these observations, we get
\begin{equation*}
\innerprod{\initial}{V^{\pi, \bar{\pi}} -V^{\pi', \bar{\pi}}} =
\sum^H_{h=1}\innerprod{d_h^{\pi, \bar{\pi}}}{Q_h^{\pi', \bar{\pi}} - E V_h^{\pi', \bar{\pi}}}.
\end{equation*}
Therefore, expanding the expectation, and noticing that $d_h^{\pi, \bar{\pi}}(s,a,s',a'|s_1) = d_h^\pi(s,a|s_1)d_h^{\bar{\pi}}(s',a'|s_1)$ for all $h,s,a,s',a'$ and conditioning $s_1$, we get that
\begin{align*}
   & \innerprod{\initial}{V^{\pi, \bar{\pi}} -V^{\pi', \bar{\pi}}} 
    \\ &= \mathbb{E}_{s_1\sim\initial}\sum^H_{h=1}\mathbb{E}_{s \sim d_h^\pi |s_1}\bs{\innerprod{\mathbb{E}_{s',a' \sim d_h^{\bar{\pi}}|s_1}Q_h^{\pi', \bar{\pi}}(s,\cdot,s',a')}{\pi_h(\cdot|s,s_1) - \pi_h'(\cdot|s,s_1)}}.
\end{align*}
% $\heartsuit\varheart\diamondsuit\vardiamond\clubsuit\spadesuit$
\end{proof}

\subsection{Proof of \cref{thm:converge}}
\label{proof:converge}
\begin{proof}
We set $\bar{\pi}^T_h(a_h|s_h) = \frac{\sum^T_{t=1} d^{\pi^t}_h(s_h,a_h)}{\sum^T_{t=1} d^{\pi^t}_h(s_h)}$, where $d(s)$ is the marginal distribution of $d(s,a)$ on state $s$, and $\bar{\pi}^T = (\bar{\pi}^T_h)_{h=1}^H$. We shows that $d^{\bar{\pi}^T}_h = \frac{1}{T}\sum^T_{t=1} d^{\pi^t}_h$ by induction. $h=1$ holds by definition. Assuming on step $h$, the equation holds, we have
\begin{align*}
    d^{\bar{\pi}^T}_{h+1}(s_{h+1},a_{h+1}) &= d^{\bar{\pi}^T}_{h+1}(s_{h+1}) \bar{\pi}^T_{h+1}(a_{h+1}|s_{h+1})\\
     &= \sum_{s_h, a_h\sim \bar{\pi}^T_h(\cdot|s_h)} d^{\bar{\pi}^T}_h(s_h,a_h) f(s_{h+1}|s_h,a_h) \bar{\pi}^T_{h+1}(a_{h+1}|s_{h+1})\\
     &=\sum_{s_h, a_h\sim \bar{\pi}^T_h(\cdot|s_h)}  \frac{1}{T}\sum^T_{t=1} d^{\pi^t}_h(s_h,a_h) f(s_{h+1}|s_h,a_h) \bar{\pi}^T_{h+1}(a_{h+1}|s_{h+1})\\
     &=\frac{1}{T}\sum_{t=1}^T  d_{h+1}^{\pi^t}(s_{h+1})\bar{\pi}^T_{h+1}(a_{h+1}|s_{h+1}) \\
     &=\frac{1}{T} \sum^T_{t=1} d^{\pi^t}_{h+1}(s_{h+1},a_{h+1}),
\end{align*}
where the last equation holds by definition of $\bar{\pi}^T_{h+1}$. 
Therefore, $h+1$ holds, and the $\bar{\pi}^T$ satisfy all equations for $h\in [H]$.

Using the value difference Lemma \ref{lemma:valuediff} we have that for any $\pi^\star \in \Pi$
    \begin{align*}
&
    \innerprod{\initial}{V^{\pi^\star, \pi^t} -V^{\pi^t, \pi^t}} 
  \\& =
\mathbb{E}_{s_1\sim\initial}\sum^H_{h=1}\mathbb{E}_{s \sim d_h^{\pi^\star}|s_1}\bs{\innerprod{\mathbb{E}_{s',a' \sim d_h^{\pi^t}|s_1}Q_h^{\pi^t, \pi^t}(s,\cdot,s',a')}{\pi_h^\star(\cdot|s) - \pi_h^t(\cdot|s)}}.
\end{align*}
Therefore, summing over $t$ from $t=1$ to $T$ we obtain
\begin{equation*}
\begin{split}
% \scalemath{0.9}{
 &   \sum^T_{t=1}\innerprod{\initial}{V^{\pi^\star, \pi^t} -V^{\pi^t, \pi^t}}
  \\& = \mathbb{E}_{s_1\sim\initial}\sum^H_{h=1}\mathbb{E}_{s \sim d_h^{\pi^\star}|s_1}\bs{\sum^T_{t=1}\innerprod{\mathbb{E}_{s',a' \sim d_h^{\pi^t}|s_1}Q_h^{\pi^t, \pi^t}(s,\cdot,s',a')}{\pi_h^\star(\cdot|s) - \pi_h^t(\cdot|s)}}.
    % }
\end{split}
\end{equation*}
Therefore, we need to control the local regrets at each state $s$ with loss $\ell^t_h(s,s_1) := \mathbb{E}_{s',a' \sim d_h^{\pi^t}|s_1}Q_h^{\pi^t, \pi^t}(s,\cdot,s',a')$. To this end, we can invoke a standard convergence result for online mirror descent \cite[Theorem 6.10]{orabona2023onlinelearning} we obtain that at each state we have 
\begin{equation*}
    \sum^T_{t=1} \innerprod{\ell^t_h(s,s_1)}{\pi^\star(\cdot|s) - \pi^t(\cdot|s)} \leq \frac{D(\pi^\star(\cdot|s), \pi^1(\cdot|s))}{ \beta} +  \beta \sum^T_{t=1} \norm{\ell^t_h(s,s_1)}^2_{\infty}.
\end{equation*}
Now, noticing that we have $\norm{\ell^t_h(s,s_1)}_{\infty} \leq H$ it holds that
\begin{equation*}
    \sum^T_{t=1} \innerprod{\ell^t_h(s)}{\pi_h^\star(\cdot|s) - \pi_h^t(\cdot|s)} \leq \frac{D(\pi_h^\star(\cdot|s), \pi_h^1(\cdot|s))}{ \beta} +  \beta T H^2.
\end{equation*}
Finally, using the assumption that $\pi^1(a|s) \geq \underline{\pi} $ for all $s,a \in \mathcal{S}\times\mathcal{A}$ it holds that $D(\pi^\star(\cdot|s), \pi^1(\cdot|s)) \leq \log \underline{\pi}^{-1}$. Therefore, choosing $ \beta = \sqrt{\frac{\log{\underline{\pi}^{-1}}}{TH^2}}$ it holds that 
\begin{equation*}
    \sum^T_{t=1} \innerprod{\ell^t_h(s,s_1)}{\pi^\star(\cdot|s) - \pi^t(\cdot|s)} \leq 2 H \sqrt{ T \log \underline{\pi}^{-1}}.
\end{equation*}
Thus, we conclude that
\begin{equation*}
\sum^T_{t=1}\innerprod{\initial}{V^{\pi^\star, \pi^t} -V^{\pi^t, \pi^t}} \leq 2 H^2\sqrt{ T \log \underline{\pi}^{-1}}.
\end{equation*}
By the antisimmetry of the game, the same proof steps 
\begin{equation*}
\sum^T_{t=1}\innerprod{\initial}{V^{\pi^t, \pi^t} - V^{\pi^t, \bar{\pi}^\star} } \leq 2 H^2\sqrt{ T \log \underline{\pi}^{-1}}.
\end{equation*}
Therefore, it holds that for all $\pi^\star, \bar{\pi}^\star \in \Pi$
\begin{equation*}
\sum^T_{t=1}\innerprod{\initial}{V^{\pi^\star, \pi^t} - V^{\pi^t, {\pi}^\star} } \leq 4 H^2\sqrt{ T \log \underline{\pi}^{-1}}.
\end{equation*}
Then, define $\bar{\pi}^T$ the trajectory level mixture policy as in \cite{swamyminimaximalist}, i.e. such that $d_h^{\bar{\pi}^T} = \frac{1}{T}\sum^T_{t=1} d_h^{\pi^t}$ for all stages $h \in [H]$. This implies that $V^{\bar{\pi}^T,\pi^\star} = \frac{1}{T}\sum^T_{t=1} V^{\pi^t,\pi^\star}$, and $V^{\pi^\star,\bar{\pi}^T} = \frac{1}{T}\sum^T_{t=1}V^{\pi^\star,\pi_t}$. 

Therefore, we have that
\begin{align*}
    \innerprod{\initial}{V^{\pi^\star, \bar{\pi}^T} - V^{\bar{\pi}^T, \bar{\pi}^\star} } \leq 4 H^2 \sqrt{\frac{   \log \underline{\pi}^{-1}}{T}}.
\end{align*}
Finally, selecting $\pi^\star = \innerprod{\initial}{\argmax_{\pi\in\Pi} V^{\pi, \bar{\pi}^T}}$ and $\bar{\pi}^\star = \innerprod{\initial}{\argmin_{\pi\in\Pi} V^{\bar{\pi}^T,\pi}}$, we obtain that
\begin{equation*}
\max_{\pi\in\Pi}\innerprod{\initial}{ V^{\pi, \bar{\pi}^T}} - \min_{\pi\in\Pi}\innerprod{\initial}{ V^{\bar{\pi}^T, \pi}} \leq 4 H^2 \sqrt{\frac{  \log \underline{\pi}^{-1}}{T}}.
\end{equation*}

This implies that
\begin{equation*}
\innerprod{\initial}{ V^{\bar{\pi}^T, \bar{\pi}^T}} - \min_{\pi\in\Pi}\innerprod{\initial}{ V^{\bar{\pi}^T, \pi}} \leq 4 H^2\sqrt{\frac{  \log \underline{\pi}^{-1}}{T}},
\end{equation*}
and
\begin{equation*}
\max_{\pi\in\Pi}\innerprod{\initial}{ V^{\pi, \bar{\pi}^T}} - \innerprod{\initial}{ V^{\bar{\pi}^T, \bar{\pi}^T}} \leq 4 H^2 \sqrt{\frac{ \log \underline{\pi}^{-1}}{T}},
\end{equation*}
Therefore, setting $T = \frac{16 H^4 \log\underline{\pi}^{-1}}{ \epsilon^2}$ we obtain an $\epsilon$-approximate Nash equilibrium.
\end{proof}
% \begin{lemma}[Peformance difference lemma]
% \begin{equation}
%     \begin{split}
%         V^{\pi,\pi^t}(s_1,s_1) - 
%     V^{\pi_t,\pi^t}(s_1,s_1) &=
%     \\ &=
%     \end{split}
% \end{equation}

    
% \end{lemma}
% \label{proof:converge}
% Define the loss at $t$ step as $\ell_t(\pi) :=  - V^{\pi,\pi^t}(s_1,s_1)$.
% Then 
% \begin{align}
%     \mathsf{Reg}(T) &= \max_{\pi \in \Pi} \sum_{t=1}^T \ell_t(\pi_t) - \ell_t(\pi) \\
%     % &= \max_{\pi \in \Pi} \sum_{t=1}^T J(\pi, r_t) - J(\pi_t, r_t) \\
%     &= \max_{\pi \in \pi} \sum_{t=1}^T V^{\pi,\pi^t}(s_1,s_1) - 
%     V^{\pi_t,\pi^t}(s_1,s_1)
%     \\
%     &= \frac{1}{1-\gamma}
%     \max_{\pi \in \pi} H \mathbb{E}_{\substack{h \sim \mathsf{Unif}([H])
%     \\
%     {a}_h^\prime \sim \pi_t(\cdot|s_h), f
%     % (\overline{s}_h,\overline{a}_h) \sim \pi^t, f
%     \\
%     % (s_h,a_h) \sim \pi, f \\
%     (s_h,a_h) \sim \pi, f
%     }} \left[ \sum_{t=1}^T Q^{\pi,\pi^t}(s_h,a_h,{s}_h,{a}_h) - Q^{\pi_t,\pi^t}(s_h,a_h,{s}_h,{a}^\prime_h) \right]\,,
%     % \tag{By the finite-horizon PDL}
% \end{align}
% % \begin{align}
% %     \mathsf{Reg}(T) &= \max_{\pi \in \Pi} \sum_{t=1}^T \ell_t(\pi_t) - \ell_t(\pi) \\
% %     % &= \max_{\pi \in \Pi} \sum_{t=1}^T J(\pi, r_t) - J(\pi_t, r_t) \\
% %     &= \max_{\pi \in \Pi} \sum_{t=1}^T V^{\pi_t,\pi_{t-1}}(s_1,s_1) - 
% %     V^{\pi,\pi_{t-1}}(s_1,s_1)
% %     \\
% %     &= \frac{1}{1-\gamma}
% %     \max_{\pi \in \Pi} H \mathbb{E}_{\substack{h \sim \mathsf{Unif}([H])
% %     \\
% %     (\overline{s}_h,\overline{a}_h) \sim \pi_{t-1}, f
% %     \\
% %     (s_h,a_h) \sim \pi, f \\
% %     (s_h^\prime,a_h^\prime) \sim \pi^t, f
% %     }} \left[ \sum_{t=1}^T Q^{\pi_t,\pi_{t-1}}(s_h^\prime,a_h^\prime,\overline{s}_h,\overline{a}_h) - Q^{\pi,\pi_{t-1}}(s_h,a_h,\overline{s}_h,\overline{a}_h) \right]\,,
% %     % \tag{By the finite-horizon PDL}
% % \end{align}
% where the last equality is by finite-horizon performance difference lemma.
% % ~\url{https://wensun.github.io/CS4789_data/PDL.pdf}.
% % \yongtao{Fix citation}

% Next, we define 
% $$l_t(a_h) = 
%  Q^{\pi,\pi^t}(s_h,a_h,{s}_h,{a}_h^\prime)
% $$
% $$L_t(a_h) = \sum_{\hat{t}=1}^t l_t(a_h).$$
% $$\Phi_t = \frac{1}{ \beta} \ln (\sum_{i=1}^N \exp (-  \beta L_t(i)))$$
% Next, we can write \cref{eq:update}, i.e.,:
% $\pi_{t+1}(a_h|s_h) \propto  \pi^t(a_h|s_h) \exp\{{ \beta_{t} \mathbb{E}_{a_h^\prime \sim \pi^t(\cdot|s_h)} Q^{\pi^t,\pi^t}(s_h,a_h,s_h,a_h^\prime) }\} = .$

% Then
% \begin{equation}
%     \begin{split}
%     \Phi_t  - \Phi_{t-1}     & = \frac{1}{ \beta} \ln (
% \frac{
% a_h))
% }{
% a_h))}   )
% \\  &=
% \frac{1}{ \beta} \ln (
% \frac{
% \sum_{i=1}^N \exp (-  \beta L_t(a_h))
% }{
% \sum_{i=1}^N \exp (-  \beta L_{t-1}(a_h))}   )
%     \end{split}
% \end{equation}



% ---------------------------------------------------------------------------------------------------

% ---------------------------------------------------------------------------------------------------

% ---------------------------------------------------------------------------------------------------

% \yongtao{Previous version: }

% Given a fixed state $s_h$, by theorem 1 in \citet{freund1999adaptive}, the sequence of policies $\pi_1(\cdot|s_h), \pi_2(\cdot|s_h), \dots, \pi_T(\cdot|s_h)$ produced by \cref{eq:update} satisfies:
% \begin{align*}
%     \sum_{t=1}^{T} 
%     \mathbb{E}
%     _{\substack{a_h \sim \pi_t(\cdot|s_h) \\  a_h^\prime \sim \pi_t(\cdot|s_h) }}
%     1-  \beta Q^{\pi_t,\pi_t}(s_h,a_h,s_h,a_h^\prime)
%     & \le 
%     \min_{\pi}
%     \bigg[
%     \frac{1}{1-e^{-1}}
%     \sum_{t=1}^{T} 
%     \mathbb{E}
%     _{\substack{a_h \sim \pi_t(\cdot|s_h) \\  a_h^\prime \sim \pi(\cdot|s_h) }}
%         1-  \beta
%     Q^{\pi_t,\pi_t}(s_h,a_h,s_h,a_h^\prime)
%     +
%     \frac{\mathbb{D}_\mathrm{KL}(\pi \| \pi_0)[s_h]}{1-e^{-1}} 
%     \bigg].
% \end{align*}
% Next
% \begin{equation*}
% \begin{split}
%   1-  \beta  \frac{T(1-\gamma^{H-h+1})}{2(1-\gamma)} 
%     &\le 
%         \min_{\pi}
%     \bigg[
%     \frac{1}{1-e^{-1}}
%         \sum_{t=1}^{T} 
%         \mathbb{E}
%     _{\substack{a_h \sim \pi_t(\cdot|s_h) \\  a_h^\prime \sim \pi(\cdot|s_h) }}
%         1-  \beta
%     Q^{\pi_t,\pi_t}(s_h,a_h,s_h,a_h^\prime)
%     +
%     \frac{\mathbb{D}_\mathrm{KL}(\pi \| \pi_0)[s_h]}{1-e^{-1}} 
%     \bigg]
% % \\  & =
% %         \min_{\pi}
% %     \bigg[
% %     \frac{ T }{1-e^{-1}}
% %     \mathbb{E}
% %     _{\substack{a_h \sim \bar{\pi}_T(\cdot|s_h) \\  a_h^\prime \sim \pi(\cdot|s_h) }}
% %         1-  \beta
% %     Q^{\bar{\pi}_T,\bar{\pi}_T}(s_h,a_h,s_h,a_h^\prime)
% %     +
% %     \frac{\mathbb{D}_\mathrm{KL}(\pi \| \pi_0)[s_h]}{1-e^{-1}} 
% %     \bigg]
% \\ & = 
%     \min_{\pi}
%     \bigg[
%     \frac{ T }{1-e^{-1}}
%    1-  \beta  V^{\bar{\pi}_T,\pi}(s_h,s_h)
%     +
%     \frac{\mathbb{D}_\mathrm{KL}(\pi \| \pi_0)[s_h]}{1-e^{-1}} 
%     \bigg]
%     \yongtao{todo}
% \\ & \le
%     \min_{\pi}
%     \bigg[
%     \frac{ T }{1-e^{-1}}
%     1-  \beta V^{\bar{\pi}_T,\pi}(s_h,s_h)
%     +
%     \frac{\norm{\log \pi_0(\cdot|s_h)}_\infty}{1-e^{-1}} 
%     \bigg] 
% \end{split}
% \end{equation*}
% By rearranging the inequality above, we get:
% \begin{equation*}
% \begin{split} 
%     \frac{(1-\gamma^{H-h}) (1-e^{-1})}{2(1-\gamma)  \beta} &\le
%     \min_{\pi}
%     \bigg[
%     V^{\bar{\pi}_T,\pi}(s_h,s_h)
%     +
%     \frac{\norm{\log \pi_0(\cdot|s_h)}_\infty}{ T} 
%     \bigg]
%     \\ & 
% \end{split}
% \end{equation*}
% By selecting  step-size $ \beta= \frac{\norm{\log \pi_0(\cdot|s_h)}_\infty}{\sqrt{T}}$, setting $h=0$, and using Taylor's expansion $\frac{1-e^{-1}}{2  \beta} = \frac{1}{2} - \frac{ \beta}{4} + \mathcal{O}( \beta^2)$, we get:
% \begin{equation*}
% \begin{split}
%     \frac{(1-\gamma^{H})}{(1-\gamma)} \bigg[\frac{1}{2} - \frac{\norm{\log \pi_0(\cdot|s_h)}_\infty}{4\sqrt{T}} + \mathcal{O}(T^{-1}) \bigg] &\le
%     \min_{\pi}
%     \bigg[
%     V^{\bar{\pi}_T,\pi}(s_1,s_1)
%     \bigg]
%     +
%    \sqrt{ \frac{\norm{\log \pi_0(\cdot|s_1)}_\infty}{ T} }
%     \\ & 
% \end{split}
% \end{equation*}
% Lastly, by noticing that $\frac{(1-\gamma^{H})}{2(1-\gamma)}$ is the value of the symmetric two-player constant-sum game, we can show that the mixture policy $\bar{\pi}_T$ is close to the Nash equilibrium. The duality gap can be bounded as follows:
% \begin{equation}
% \begin{split}
%      &   \max_\pi V^{\pi,\bar{\pi}_T}(s_1,s_1) -  
%       \min_\pi V^{\bar{\pi}_T,\pi}(s_1,s_1)  
%       \\ & 
%       =\max_\pi \bigg[ \frac{(1-\gamma^{H})}{(1-\gamma)}
%       - V^{\bar{\pi}_T,\pi}(s_1,s_1)
%       \bigg]
%       - \min_\pi V^{\bar{\pi}_T,\pi}(s_1,s_1)
%        \\ & =
%        2 \bigg[ \frac{(1-\gamma^{H})}{2(1-\gamma)}
%              - \min_\pi V^{\bar{\pi}_T,\pi}(s_1,s_1)
%       \bigg]
%       \\ & = \mathcal{O}(\frac{1}{\sqrt{T}})
%       \,.
% \end{split}
% \end{equation}
% This completes the proof.

% \subsection{Some auxiliary lemmas}
% \begin{lemma}[Lemma 3.1 in \citet{shani2024multi}]
% \label{lemma:kl}
%     Let $\pi,\pi'$ be two policies, then: $$\KL{\pi}{\pi'} = \EE[s_1\dots s_H \sim \pi] \brk[s]*{\sum_{h=1}^H \KLshort{\pi}{\pi'}{s_h}}.$$
% \end{lemma}

% \subsection{Convergence at last iterate (
% \yongtao{does not work now, unless using mixture policy at the mirror descent optimization)}}
% \begin{theorem}[Convergence of MSPPO at last iterate]
% Denote by $\pi^\star$ the Nash equilibrium of the preference model in \cref{equ:minmaxgame_0}, by running the update in \cref{eq:update}, at every iteration $t$ we have
% that 
% \begin{equation}
% \mathbb{D}_\mathrm{KL}(\pi^\star || \pi_t) \le 
% \end{equation}
% \end{theorem}
% \begin{proof}
% At each iteration step $t$ and state $s_h$, we 
% apply \citet[Lemma 2]{munos2024nash} with $\pi^{+} = \piT[t+1]$, $\pi^{-} = \piT, \pi = \pi^\star$ and the vector $\delta =  \betaT 
% % \QReg^{\piT, t}(s_h, y)
% \mathbb{E}_{a_h^\prime \sim \pi_t(\cdot|s_h)} Q^{\pi_t,\pi_t}(s_h,\cdot,s_h,a_h^\prime) $, we get:
%     \begin{equation}
%     \begin{split}
%          \KLshort{\pi^\star}{\piT[t+1]}{s_h}
%         & 
%         \le
%         \KLshort{\pi^\star}{\piT}{s_h} + 2  \betaT^2 \norm{ \mathbb{E}_{a_h^\prime \sim \pi_t(\cdot|s_h)} Q^{\pi_t,\pi_t}(s_h,\cdot,s_h,a_h^\prime) }_\infty^2
%         \\
%         &
%         \qquad
%         +
%          \betaT \inner{\piT(\cdot \mid s_h) - \pi^\star(\cdot \mid s_h) ,  \mathbb{E}_{a_h^\prime \sim \pi_t(\cdot|s_h)} Q^{\pi_t,\pi_t}(s_h,\cdot,s_h,a_h^\prime)}. 
%     \end{split}
%     \label{equ:kldifference}
%     \end{equation}
% By substituting \cref{equ:kldifference} into \cref{lemma:kl}, we get
%     \begin{align*}
%         \KL{\pi^\star}{\piT[t+1]}
%         &
%         =
%         \EE[s_1\dots s_H \sim \pi^\star] \brk[s]*{\sum_{h=1}^H \KLshort{\pi^\star}{\piT[t+1]}{s_h} }
%         \\
%         &
%         \le
%         \EE[\pi^\star] \brk[s]*{\sum_{h=1}^H \KLshort{\pi^\star}{\piT}{s_h} } 
%         + 
%         2  \betaT^2 \EE[\pi^\star] \brk[s]*{\sum_{h=1}^H \norm{ \mathbb{E}_{a_h^\prime \sim \pi_t(\cdot|s_h)} Q^{\pi_t,\pi_t}(s_h,\cdot,s_h,a_h^\prime)
%         }_\infty^2}
%         \\
%         &
%         \qquad
%         +
%          \betaT \EE[ \pi^\star] \brk[s]*{\sum_{h=1}^H \inner{\piT(\cdot \mid s_h) - \pi^\star(\cdot \mid s_h) , \mathbb{E}_{a_h^\prime \sim \pi_t(\cdot|s_h)} Q^{\pi_t,\pi_t}(s_h,\cdot,s_h,a_h^\prime)}}
%         \\
%         &
%         \le
%         \EE[ \pi^\star] \brk[s]*{\sum_{h=1}^H \KLshort{\pi^\star}{\piT}{s_h} } 
%         + 
%         2  \betaT^2 \EE[ \pi^\star] \brk[s]*{\sum_{h=1}^H \norm{ \mathbb{E}_{a_h^\prime \sim \pi_t(\cdot|s_h)} Q^{\pi_t,\pi_t}(s_h,\cdot,s_h,a_h^\prime)
%         }_\infty^2}
%         \\
%         &
%         \qquad
%         +
%         V^{\pi_t, \pi^\star}(s_1,s_1) - 
%         V^{\pi^\star, \pi^\star}(s_1,s_1)
%        \\
%         &
%         \le
%         \EE[ \pi^\star] \brk[s]*{\sum_{h=1}^H \KLshort{\pi^\star}{\piT}{s_h} } 
%         + 
%         2  \betaT^2 \EE[ \pi^\star] \brk[s]*{\sum_{h=1}^H \norm{ \mathbb{E}_{a_h^\prime \sim \pi_t(\cdot|s_h)} Q^{\pi_t,\pi_t}(s_h,\cdot,s_h,a_h^\prime)
%         }_\infty^2}
%         \\
%         & =  \KL{\pi^\star}{\piT[t]} +   2  \betaT^2 \EE[ \pi^\star] \brk[s]*{\sum_{h=1}^H \norm{ \mathbb{E}_{a_h^\prime \sim \pi_t(\cdot|s_h)} Q^{\pi_t,\pi_t}(s_h,\cdot,s_h,a_h^\prime)
%         }_\infty^2}
%         \,,
%     \end{align*}
% where the second inequality is by \cref{lemma:valuediff}, the last inequality is by the optimality of $\pi^\star$. Next, by recursively unrolling the above relation along the iteration step $t$, we get:
% \begin{equation}
% \KL{\pi^\star}{\piT[t+1]} \le 
% \KL{\pi^\star}{\pi_1}
% +
% \sum_{\hat{t}=1}^t
% 2  \beta_{\hat{t}}^2 \EE[ \pi^\star] \brk[s]*{\sum_{h=1}^H \norm{ \mathbb{E}_{a_h^\prime \sim \pi_{\hat{t}}(\cdot|s_h)} Q^{\pi_{\hat{t}},\pi_{\hat{t}}}(s_h,\cdot,s_h,a_h^\prime)
%         }_\infty^2}
% \end{equation}

% \end{proof}

%  \subsection{Connection to TRPO}
%  % \yongtao{General preference optimization contains secretly a Q function}
 
% In standard RL, we define the state-action function $Q^{\pi}$
%  for a policy $\pi$:
% \begin{equation}
%     {\small
%     \begin{aligned}
%     Q^{\pi}(s_h,a_h) & = \mathbb{E}_{\pi}\left[\sum_{k=0}^H
%     r(s_{h+k},a_{h+k})
%     \bigg|s_h, a_h \right]
%     \end{aligned}}\label{QVA}
% \end{equation}

%  Objective at step $h$, iteration $t$:
%  \begin{equation}
%     \begin{split}
% \pi_{t+1}=\argmax_{\pi} \ &\mathbb{E}_{a_h\sim \pi(\cdot|s_h)}\big[Q^{\pi_t}(s_h, a_h)-\beta D_{\mathrm{KL}}\left(\pi(\cdot|s_h)||\pi_t(\cdot|s_h)\right)\big].
%     \end{split}\label{rpo_obj}
% \end{equation}

%  \begin{lemma}
%     The constrained problem in Eq.~\ref{rpo_obj} has the closed-form solution:
% \begin{equation}
% \begin{aligned}
%     \pi_{t+1}(a_h|s_h) \propto
%      % \frac{
%      \pi_t(a_h|s_h)\exp\left(\frac{1}{\beta}Q^{\pi_t}(s_h,a_h)\right)
%      % }{Z(s_h;\beta)}
%      ,
% \end{aligned}
% \end{equation}
% % where $Z(s_h;\beta) = \mathbb{E}_{z\sim \pi_t(\cdot|s_h)}e^{\frac{1}{\beta}Q^{\pi_t}(s_h,z)}$
% % is the partition function.
% \end{lemma}

% \section{connection to soft actor critic~\cite{haarnoja2018soft}}
% \begin{equation}
%     (\pi^*, \pi^*)
%     =
%     \arg \max_{\pi}\min_{\pi'}
%     \mathbb{E}_{x \sim \mathcal{X}}\Big[ 
%     \mathbb{E}_{y \sim \pi(\cdot|x), y' \sim \pi'(\cdot|x)}
%     \big[
%     \mathbb{P}(y \succ y' | x)
%     \big]
%     \Big],
% \end{equation}
% We can assume that there exists an implicit relative reward corresponding to the general preference:
% $$
% \mathbb{P}(y>y' | x) = 
% \sigma(R(x,y,y'))=
% \frac{\exp(R(x,y,y'))}{1+\exp(R(x,y,y'))},
% $$
% where $\sigma$ is the sigmoid function, $R(x,y,y')$ is assigned with high value if $y$ is better than $y'$.
% We can further define such reward as the summation of the rewards at each step:
% $$R(x,y,y'): = \sum_{h=0}^H r(s_h,s_h',a_h,a_h')$$ 
% where
% $r(s_h,s_h',a_h,a_h') = 0 $ if $h \le H$, otherwise it is a high value when $s_{H+1}$ is better than $s_{H+1}'$.
% % r(s_h,a_h,a_h') = 
% % \begin{cases}
% % 			\text{some high value}, & \text{if $l=L$}\\
% %             0, & \text{otherwise}
% % 		 \end{cases}
% % $$
% It can be written as 
% $$
% \prod_{l=0}^L \pi_{t+1}(a_h|s_h) \propto 
% \prod_{l=0}^L \pi^t(a_h|s_h)
% \exp \left\{ \beta \left[\sum_{h=0}^H r(s_h,a_h)-\mathbb{E}
% % _{\hat{a_h} \sim \pi_t(\cdot|s_h)}
% \sum_{h=0}^H r(s_h,\hat{a_h})
% \right] \right\}
% $$
% Equivalently:
% $$
% \prod_{l=0}^L \pi_{t+1}(a_h|s_h) \propto 
% \prod_{l=0}^L \pi^t(a_h|s_h)
% \exp \left\{ \beta \left[\sum_{h=0}^H r(s_h,a_h)-
% \mathbb{E}
% % _{\hat{a}_0\sim \pi_t(\cdot|s_1)}
% Q^{\pi_t}(s_1,\hat{a}_0)
% \right] \right\}
% $$

% $$
% \prod_{l=0}^L \pi_{t+1}(a_h|s_h) \propto 
% \exp \left\{ \beta \left[\sum_{h=0}^H r(s_h,a_h)-
% \mathbb{E}
% % _{\hat{a}_0\sim \pi_t(\cdot|s_1)}
% Q^{\pi_t}(s_1,\hat{a}_0) 
% +
%  \log \pi^t(a_h|s_h)
% \right] \right\}
% $$

% \noindent\rule{\textwidth}{1pt}
% Soft actor critic~\cite{haarnoja2018soft}:

% Soft policy evaluation:
% \begin{equation}
%     \begin{split}
% Q^{\pi_t}(s_h,a_h) &= r(s_h,a_h) + \mathbb{E}_{s_{H+1},a_{H+1}} \left[ Q^{\pi_t}(s_{H+1},a_{H+1})  - \log \pi_t(a_{H+1}|s_{H+1})
% \right]
% \\ & 
%  =r(s_h,a_h) + \mathbb{E}_{a_{H+1}} \left[ Q^{\pi_t}(s_{H+1},a_{H+1})  - \log \pi_t(a_{H+1}|s_{H+1})
% \right]
%     \end{split}
% \end{equation}

% \yongtao{it is $-\log \pi_t(a_{H+1}|s_{H+1})$ instead of 
% $\log \pi_t(a_{l}|s_{l})$}

% Soft policy improvement: 
% $$\pi_{t+1}(a_h|s_h) \propto \exp(Q^{\pi_t}(s_h,a_h))$$
% Aggregating $L$ step:
% $$\prod_{l=0}^L \pi_{t+1}(a_h|s_h) \propto \exp(\sum_{h=0}^H Q^{\pi_t}(s_h,a_h))$$
% % \noindent\rule{\textwidth}{1pt}
\subsection{Proof of Theorem~~\ref{thm:converge_fast}}
\label{proof:converge_fast}
\begin{proof}
The optimization problem
$$
\argmax_{d\in\tilde{\mathcal{F}}} \min_{d'\in\tilde{\mathcal{F}}} \mathbb{E}_{s_1 \sim \initial}\sum^H_{h=1}\sum_{s,a,s',a'} d_h(s,a | s_1) r(s,a,s',a') d_h'(s',a'|s_1)
$$
can be carried out individually over possible initial states. That is for each $s_1 \in \mathrm{supp}(\initial)$ we aim at solving
$$
\argmax_{d\in\mathcal{F}_{s_1}} \min_{d'\in\mathcal{F}_{s_1}} \sum^H_{h=1}\sum_{s,a,s',a'} d_h(s,a | s_1) r(s,a,s',a') d_h'(s',a'|s_1)
$$
\if 0
For the proof convenience we consider two copies of the $\pi$ iterates denoted by $x$ and $y$. $x$ denotes the policy of the minimizing player and $y$ the policy of the maximizing player. As explained in \cite{swamyminimaximalist} the antisymmetry of the game implies that both updates can be implemented using only one update.
Invoking, \cite[Theorem 1]{perolat2015approximate} for $p = + \infty$ and $\rho = \initial$, we obtain
\begin{align*}
\innerprod{\initial}{\max_{y} V^{x^N,y} - \min_x \max_y V^{x,y}} \leq \frac{1}{(1 - \gamma)^2} \max_{n \in [N]} \epsilon_x^n + \frac{2 \gamma^N}{(1 - \gamma)^2}
\end{align*}
and for the other player
\begin{align*}
\innerprod{\initial}{\min_x \max_y V^{x,y} - \min_{x} V^{x,y^N} } \leq \frac{1}{(1 - \gamma)^2} \max_{n \in [N]} \epsilon_y^n + \frac{2 \gamma^N}{(1 - \gamma)^2}
\end{align*}
where, denoting $Q^n(s,a,a') := \mathbb{E}_{s' \sim d^{x^n}} Q^{x^n,y^n}(s,a,s',a') $ \footnote{which is also equal to $\mathbb{E}_{s' \sim d^{y^n}} Q^{x^n,y^n}(s,a,s',a')$ by the game symmetry}, we define the errors as $$\epsilon_x^n = \max_{s\in \mathcal{S}} \bs{\max_ y \sum_{a,a'} x^n(a|s)^T Q^n(s, a, a') y(a'|s) - \min_x \max_ y \sum_{a,a'} x(a|s)^T Q^n(s, a, a') y(a'|s)}, $$
and
$$\epsilon_y^n = \max_{s\in \mathcal{S}} \bs{\min_x \max_ y \sum_{a,a'} x(a|s)^T Q^n(s, a, a') y(a'|s) - \min_x \sum_{a,a'} x(a|s)^T Q^n(s, a, a') y^n(a'|s)}. $$
Now, we show how to control $\epsilon^n = \epsilon^n_x + \epsilon^n_y$ which can be done with the proof for a single stage analysis of the \oomdmethod{} algorithm.
\if 0
\begin{align*}
V_1^{x,x_t}(s_1,s_1) - V_1^{x^t,x^t}(s_1,s_1) &= \sum^H_{h=1}\mathbb{E}_{(s, a, s', a') \sim d^{x, x^t}_h}\left[ Q_h^{x^t,x^t}(s,a,s',a') - V_h^{x^t,x^t}(s,s')\right] \\
&= \sum^H_{h=1}\mathbb{E}_{(s, a) \sim d^{x}_h} \mathbb{E}_{(s', a') \sim d^{x^t}_h}\left[ Q_h^{x^t,x^t}(s,a,s',a') - V_h^{x^t,x^t}(s,s')\right] \\&\text{(Since the two policies generates the trajectories independently)}
\\&= \sum^H_{h=1}\mathbb{E}_{(s, a) \sim d^{x}_h} \left[ \mathbb{E}_{(s', a') \sim d^{x^t}_h}[Q_h^{x^t,x^t}(s,a,s',a')] - \mathbb{E}_{s' \sim d^{x^t}_h}[V_h^{x^t,x^t}(s,s')]\right] \\&=
\sum^H_{h=1}\mathbb{E}_{s \sim d^{x}_h} \left[ \langle \mathbb{E}_{(s', a') \sim d^{x^t}_h}[Q_h^{x^t,x^t}(s,\cdot,s',a')], x(\cdot|s) - x^t(\cdot|s) \rangle \right] \\&=
\sum^H_{h=1} \mathbb{E}_{s \sim d^{x}_h} \left[ \langle \theta^t(s,\cdot), x(\cdot|s) - x^t(\cdot|s) \rangle \right]
\end{align*}
\fi
We give for granted that we are in the outer loop indexed by $n$, so we abbreviate $x^t_n$ and $y^t_n$ by $x^t$, $y^t$. Noticed that in Algorithm~\ref{alg:forb} we wrote a single policy update, i.e. for $\pi^t_n$. In Theorem~\ref{thm:same_updates} we show that  $x^t_n$ and $y^t_n$ are in fact identical to $\pi^t_n$.
\fi
To this end for any $s_1$, we consider $\phi^t_h \in \mathcal{F}$ and $\psi^t_h \in \mathcal{F}$ which are generated by the following updates
\begin{align*}
            % \theta_{t+1}
            \phi_h^{t+1} = \argmax_{\phi \in \mathcal{F}_{s_1}} 
            \beta \innerprod{\phi}{
            % Q^{\pi^t,\pi^t}(s_h,a_h,s_h,a_h^\prime)
            2 \mathbb{E}_{s',a' \sim \psi^t}r_h(\cdot,\cdot,s',a') - \mathbb{E}_{s',a' \sim \psi^{t-1}}r_h(\cdot,\cdot,s',a')} - \mathbb{D}(\phi, \phi_h^t),
        \end{align*}
        and
        \begin{align*}
            % \theta_{t+1}
            \psi_h^{t+1} = \argmin_{\psi \in \mathcal{F}_{s_1}} 
            \beta \innerprod{\psi}{
            % Q^{\pi^t,\pi^t}(s_h,a_h,s_h,a_h^\prime)
            2 \mathbb{E}_{s',a' \sim \phi^t}r_h(s',a',\cdot,\cdot) - \mathbb{E}_{s',a' \sim \phi^{t-1}}r_h(s',a',\cdot,\cdot)} + \mathbb{D}(\psi, \psi_h^t),
        \end{align*}
        In order to prove convergence to an $\epsilon$-approximate Nash equilibrium, we need to control the quantity
\begin{align*}
\mathrm{Gap}_{s_1} = \frac{1}{T}\sum^H_{h=1}\sum^T_{t=1}\innerprod{\theta_h^t}{\phi_h^t - \phi_h^\star} + \frac{1}{T}\sum^H_{h=1}\sum^T_{t=1}\innerprod{\zeta_h^t}{\psi_h^t - \psi_h^\star},
\end{align*}
for $\theta_h^t(s,a) = \sum_{s',a'} \psi_h^t(s',a') r_h(s,a,s',a')$ and $\zeta_h^t(s',a') = - \sum_{s,a} \phi_h^t(s,a) r_h(s,a,s',a')$.
At this point, we bound the local regret term with the \oomdmethod{}  update. We have that for any $\phi_h \in \mathcal{F}$
\begin{align*}
 \beta \innerprod{2 \theta_h^{t} - \theta_h^{t-1}}{\phi_h - \phi_h^{t+1}} &=  \beta \innerprod{ \theta_h^{t} - \theta_h^{t+1}}{\phi_h - \phi_h^{t+1}} \\&\phantom{=} +
 \beta \innerprod{ \theta_h^{t}+ \theta^{t+1}_h - \theta^{t-1}_h}{\phi_h - \phi_h^{t+1}} \\
&=  \beta \innerprod{ \theta^{t}_h - \theta^{t+1}_h}{\phi_h - \phi_h^{t+1}} \\
&\phantom{=}+  \beta \innerprod{ \theta^{t}_h - \theta^{t-1}_h}{\phi_h - \phi^t_h} \\
&\phantom{=}+  \beta \innerprod{ \theta_h^{t} - \theta_h^{t-1}}{\phi^t_h - \phi^{t+1}_h} \\
&\phantom{=}+ \beta \innerprod{ \theta^{t+1}_h}{\phi_h - \phi_h^{t+1}}.
\end{align*}
At this point, we work on the third summand above
\begin{align*}
 \beta \innerprod{ \theta^{t}_h - \theta^{t-1}_h}{\phi^t_h - \phi_h^{t+1}} \leq  \beta^2 \lambda \norm{\theta^t_h - \theta^{t-1}_h}_{\infty}^2 + \frac{1}{4 \lambda}\norm{\phi^t_h - \phi_h^{t+1}}_{1}^2.
\end{align*}
In addition, we have that$
\norm{\theta^t_h - \theta^{t-1}_h}_{\infty} \leq \norm{\psi_h^{t} - \psi_h^{t-1}}_1$
and we can apply the $1/\lambda$ strong convexity of $\mathbb{D}$, we obtain
\begin{align*}
 \beta \innerprod{ \theta_h^{t} - \theta_h^{t-1}}{\phi_h^t - \phi_h^{t+1}} \leq   \lambda \beta^2 \norm{\psi_h^{t} - \psi_h^{t-1}}^2_1 + \frac{1}{2}\mathbb{D}(\phi_h^{t+1},\phi_h^t).
\end{align*}
On the other hand, by the three point identity we have that for all $\phi \in \mathcal{F}$
\begin{equation*}
\mathbb{D}(\phi_h,\phi_h^{t+1}) = \mathbb{D}(\phi_h,\phi_h^{t}) - \mathbb{D}(\phi_h^{t+1},\phi_h^{t}) + \innerprod{\nabla \mathbb{D}(\phi_h^{t+1}, \phi_h^t)}{\phi_h^{t+1} - \phi_h}
\end{equation*}
Then, using the property of the update rule, we obtain that 
\begin{equation*}
 \innerprod{\nabla \mathbb{D}(\phi_h^{t+1}, \phi_h^t)}{\phi_h^{t+1} - \phi_h} \leq  \beta \innerprod{2 \theta^{t}_h - \theta_h^{t-1}}{\phi_h - \phi_h^{t+1}}.
\end{equation*}
Putting all the pieces together we have that
\begin{align*}
\mathbb{D}(\phi_h,\phi_h^{t+1}) &\leq \mathbb{D}(\phi_h,\phi_h^{t}) - \mathbb{D}(\phi_h^{t+1},\phi_h^{t}) +  \beta \innerprod{2 \theta^{t}_h - \theta^{t-1}_h}{\phi_h - \phi_h^{t+1}(\cdot|s)} \\
&\leq \mathbb{D}(\phi_h,\phi_h^{t}) - \mathbb{D}(\phi_h^{t+1},\phi_h^{t}) \\
&\phantom{=} +  \beta \innerprod{ \theta_h^{t} - \theta_h^{t+1}}{\phi_h - \phi_h^{t+1}} \\
&\phantom{=}+  \beta \innerprod{ \theta_h^{t}- \theta_h^{t-1}}{\phi_h - \phi_h^{t}} \\
&\phantom{=}+  \beta^2 \norm{\psi_h^{t} - \psi_h^{t-1}}^2_1 + \frac{1}{2}\mathbb{D}(\phi_h^{t+1},\phi_h^{t}) \\
&\phantom{=}+ \beta \innerprod{ \theta_h^{t+1}}{\phi_h - \phi_h^{t+1}}.
\end{align*}
Now, rearranging the terms we get
\begin{align*}
 \beta \innerprod{ \theta^{t+1}_h}{\phi_h - \phi_h^{t+1}}
&\leq \mathbb{D}(\phi_h,\phi_h^{t}) - \mathbb{D}(\phi_h,\phi_h^{t+1}) - \frac{1}{2}\mathbb{D}(\phi_h^{t+1},\phi_h^{t}) \\
&\phantom{=} +  \beta \innerprod{ \theta_h^{t} - \theta_h^{t+1}}{\phi_h - \phi_h^{t+1}} \\
&\phantom{=}+  \beta \innerprod{ \theta_h^{t} - \theta_h^{t-1}}{\phi_h - \phi_h^{t}} \\
&\phantom{=}+  \beta^2 \lambda \norm{\psi^{t}_h - \psi_h^{t-1}}^2_1. 
\end{align*}
Now, denoting $\Phi_\phi^t := \mathbb{D}(\phi_h,\phi_h^{t}) +   \beta \innerprod{ \theta^{t}_h - \theta^{t-1}_h}{\phi_h- \phi_h^{t}}$ and summing over $t$ we obtain
\begin{align*}
 \beta \sum^T_{t=1}\innerprod{ \theta^{t}_h}{\phi_h - \phi_h^{t}}
&\leq \sum^T_{t=1} \Phi_\phi^{t-1} - \Phi_\phi^t - \frac{1}{2} \sum^T_{t=1} \mathbb{D}(\phi_h^t, \phi_h^{t-1}) +   \beta^2 \lambda \sum^T_{t=1} \norm{\psi_h^{t-1}- \psi_h^{t-2}}^2_1.
\end{align*}
Similarly we get
\begin{align*}
 \beta \sum^T_{t=1}\innerprod{ \zeta^{t}(s, \cdot)}{\psi^t_h - \psi_h^{t}}
&\leq \sum^T_{t=1} \Phi_\psi^{t-1} - \Phi_\psi^t - \frac{1}{2} \sum^T_{t=1} \mathbb{D}(\psi_h^t, \psi_h^{t-1}) + \beta^2 \lambda \sum^T_{t=1} \norm{\phi_h^{t-1} - \psi_h^{t-2}}^2_1.
\end{align*}
Now, using $1/\lambda$ strong convexity of $\mathbb{D}$ and summing the two terms we have that
\begin{align*}
    \beta T \mathrm{Gap}_{s_1,h} &\leq \Phi^0 - \Phi^{T-1} - \frac{1}{2}\sum^T_{t=1} (\mathbb{D}(\psi_h^t, \psi_h^{t-1}) + \mathbb{D}(\phi_h^t, \phi_h^{t-1})) \\& \qquad + 2  \beta^2 \lambda\sum^T_{t=1} (\mathbb{D}(\psi_h^{t-1}, \psi_h^{t-2}) + \mathbb{D}(\phi_h^{t-1}, \phi_h^{t-2})),
\end{align*}
with $\Phi^t = \Phi^t_\phi + \Phi^t_\psi$. At this point, setting $ \beta \leq \frac{1}{\sqrt{2\lambda} }$, we obtain a telescopic sum
\begin{align*}
   & \beta T \mathrm{Gap}_{s_1, h} \\ &\leq \Phi^0 - \Phi^{T-1} - \frac{1}{2}\sum^T_{t=1} (\mathbb{D}(\psi_h^t, \psi_h^{t-1}) + \mathbb{D}(\phi_h^t, \phi_h^{t-1}) - \mathbb{D}(\psi_h^{t-1}, \psi_h^{t-2}) - \mathbb{D}(\phi_h^{t-1}, \phi_h^{t-2})) \\
    &\leq \Phi^0 - \Phi^{T-1} + \frac{1}{2}\br{\mathbb{D}(\psi_h^1, \psi_h^0) + \mathbb{D}(\phi_h^1,\phi_h^0)}.
\end{align*}
Now recalling that by assumption the occupancy measure of the reference policy is lower bounded, i.e. $d^{\pi^1} \geq \underline{d}$, we can upper bound $\Phi^0 - \Phi^T \leq 2 \log \underline{d}^{-1} + 8  \beta$ that allows to conclude that for all $n \in [N]$ and setting $\psi_h^0 = \psi_h^1$ and $\phi^1_h = \phi^0_h$, 
\begin{align*}
    \mathrm{Gap}_{s_1,h} &\leq \frac{2 \log \underline{d}^{-1} + 8 \beta}{\beta T} \leq \frac{10\log \underline{d}^{-1} }{\beta T}.
\end{align*}
\if 0
At this point noticing that the above result implies that $$\max_{n \in [N]} \epsilon^n_x + \max_{n\in[N]} \epsilon^n_y \leq \frac{3 \log \abs{\mathcal{A}} + 4}{T}.$$
Therefore, plugging in into \cite[Theorem 1]{perolat2015approximate} we obtain
\begin{equation*}
    \innerprod{\initial}{\max_{y} V^{x^N,y} - \min_x V^{x,y^N}} \leq \frac{3 \log(\abs{\mathcal{A}})+4}{(1 - \gamma)^2 T}  + \frac{2 \gamma^N}{(1 - \gamma)^2}
\end{equation*}
Therefore setting $N = \mathcal{O}\br{\log (\frac{(1 - \gamma)^2}{2 \epsilon})}$ and $T = \frac{3 \log(\abs{\mathcal{A}})+4}{\epsilon (1 - \gamma)^2}$ ensures that
\fi
Now, notice that $\mathrm{Gap}$ can be rewritten as 
\begin{align*}
& \mathrm{Gap}_{s_1} = 
\sum^H_{h=1} \mathrm{Gap}_{s_1,h} \\ &  =  \frac{1}{T}\sum^T_{t=1} \sum^H_{h=1} \sum_{s,a,s',a'}\psi^\star_h(s',a')r_h(s,a,s',a')\phi^t_h(s,a) 
\\ & \hspace{5em} - \frac{1}{T}\sum^T_{t=1} \sum^H_{h=1} \sum_{s,a,s',a'}\psi^t_h(s',a')r_h(s,a,s',a')\phi^\star_h(s,a) \\
& = \sum^H_{h=1} \sum_{s,a,s',a'}\psi^\star_h(s',a')r_h(s,a,s',a')\frac{1}{T}\sum^T_{t=1} \phi^t_h(s,a) 
\\ & \hspace{5em} -  \sum^H_{h=1} \sum_{s,a,s',a'}\frac{1}{T}\sum^T_{t=1}\psi^t_h(s',a')r_h(s,a,s',a')\phi^\star_h(s,a) \\
&= \sum^H_{h=1} \sum_{s,a,s',a'}\psi^\star_h(s',a')r_h(s,a,s',a') \bar{\phi}_h(s,a) -  \sum^H_{h=1} \sum_{s,a,s',a'}\bar{\psi}_h(s',a')r_h(s,a,s',a')\phi^\star_h(s,a)\,. %\\&
%= \innerprod{\initial}{\max_{\psi} V^{\pi_{\bar{\phi}},\psi} - \min_\phi V^{\phi,\pi_{\bar{\psi}}}},
\end{align*}
At this point, let us define $\pi^\mathrm{out}_{\phi}(a|s) = \frac{\bar{\phi}(s,a)}{\sum_a \bar{\phi}(s,a)}$ and $\pi^\mathrm{out}_{\psi}(a|s) = \frac{\bar{\psi}(s,a)}{\sum_a \bar{\psi}(s,a)}$. For such policies and by appropriate choice for $\psi^\star$ and $\phi^\star$ it follows that
$$\mathrm{Gap}_{s_1} = \max_{\psi} V^{\pi^{\mathrm{out}}_{\phi},\psi}(s_1) - \min_\phi V^{\phi,\pi^{\mathrm{out}}_{\psi}}(s_1).$$
 By the bound on $\mathrm{Gap}_{s_1}$ for each $s_1 \in \mathrm{supp}(\initial)$, it follows that
\begin{equation*}
    \innerprod{\initial}{\max_{\psi} V^{\pi^{\mathrm{out}}_{\phi},\psi} - \min_\phi V^{\phi,\pi^{\mathrm{out}}_{\psi}}}= \mathbb{E}_{s_1 \sim \initial} \mathrm{Gap}_{s_1} \leq \frac{10 H \log \underline{d}^{-1} }{\beta T},
\end{equation*}
therefore $T \geq \frac{10 H \log \underline{d}^{-1} }{\beta \epsilon}$. The proof is concluded invoking \cref{thm:same_updates} that ensures that the policies $\pi^{\mathrm{out}}_{\psi}$ and $\pi^{\mathrm{out}}_{\phi}$ coincide.
%%%%OLD PROOFS Follows
\if 0
In particular, at every state we need to control the quantity
\begin{align*}
\epsilon^n = \frac{1}{T}\sum^T_{t=1}\innerprod{\theta^t(s,\cdot)}{x^t(\cdot|s) - x^\star(\cdot|s)} + \frac{1}{T}\sum^T_{t=1}\innerprod{\zeta^t(s,\cdot)}{y^t(\cdot|s) - y^\star(\cdot|s)}
\end{align*}
for $\theta^t(s,a) = \sum_{a'} y^t(a'|s) Q^n(s,a,a')$ and $\zeta^t(s,a') = - \sum_{a} x^t(a|s) Q^n(s,a,a')$.
At this point, we bound the local regret term with the \oomdmethod{}  update.
\begin{align*}
 \beta \innerprod{2 \theta^{t}(s,\cdot) - \theta^{t-1}(s, \cdot)}{x(\cdot|s) - x^{t+1}(\cdot|s)} &=  \beta \innerprod{ \theta^{t}(s,\cdot) - \theta^{t+1}(s, \cdot)}{x(\cdot|s) - x^{t+1}(\cdot|s)} \\&\phantom{=} +
 \beta \innerprod{ \theta^{t}(s,\cdot) + \theta^{t+1}(s,\cdot) - \theta^{t-1}(s, \cdot)}{x(\cdot|s) - x^{t+1}(\cdot|s)} \\
&=  \beta \innerprod{ \theta^{t}(s,\cdot) - \theta^{t+1}(s, \cdot)}{x(\cdot|s) - x^{t+1}(\cdot|s)} \\
&\phantom{=}+  \beta \innerprod{ \theta^{t}(s,\cdot) - \theta^{t-1}(s, \cdot)}{x(\cdot|s) - x^{t}(\cdot|s)} \\
&\phantom{=}+  \beta \innerprod{ \theta^{t}(s,\cdot) - \theta^{t-1}(s, \cdot)}{x^t(\cdot|s) - x^{t+1}(\cdot|s)} \\
&\phantom{=}+ \beta \innerprod{ \theta^{t+1}(s, \cdot)}{x(\cdot|s) - x^{t+1}(\cdot|s)}
\end{align*}
At this point, we work on the third summand above
\begin{align*}
 \beta \innerprod{ \theta^{t}(s,\cdot) - \theta^{t-1}(s, \cdot)}{x^t(\cdot|s) - x^{t+1}(\cdot|s)} \leq  \beta^2 \norm{\theta^t(s, \cdot) - \theta^{t-1}(s,\cdot)}_{\infty}^2 + \frac{1}{4}\norm{x^t(\cdot|s) - x^{t+1}(\cdot|s)}_{1}^2
\end{align*}
Now, at this point we have that
\begin{align*}
\norm{\theta^t(s, \cdot) - \theta^{t-1}(s,\cdot)}_{\infty} &\leq \frac{1}{1 - \gamma} \norm{y^{t}(\cdot|s) - y^{t-1}(\cdot|s)}_1
\end{align*}
So with an additional application of the Pinkser's inequality we obtain
\begin{align*}
 \beta \innerprod{ \theta^{t}(s,\cdot) - \theta^{t-1}(s, \cdot)}{x^t(\cdot|s) - x^{t+1}(\cdot|s)} \leq  \frac{ \beta^2}{(1 - \gamma)^2} \norm{y^{t}(\cdot|s) - y^{t-1}(\cdot|s)}^2_1 + \frac{1}{2}D(x^{t+1}(\cdot|s),x^t(\cdot|s)).
\end{align*}
On the other hand, by the three point identity we have that at each $s \in \mathcal{S}$ and for all $x \in \Pi$
\begin{equation*}
D(x(\cdot|s),x^{t+1}(\cdot|s)) = D(x(\cdot|s),x^{t}(\cdot|s)) - D(x^{t+1}(\cdot|s),x^{t}(\cdot|s)) + \innerprod{\nabla D(x^{t+1}(\cdot|s), x^t(\cdot|s))}{x^{t+1}(\cdot|s) - x(\cdot|s)}
\end{equation*}
Then, using the property of the update rule, we obtain that 
\begin{equation*}
\innerprod{\nabla D(x^{t+1}(\cdot|s), x^t(\cdot|s))}{x^{t+1}(\cdot|s) - x(\cdot|s)} \leq  \beta \innerprod{2 \theta^{t}(s,\cdot) - \theta^{t-1}(s,\cdot)}{x(\cdot|s) - x^{t+1}(\cdot|s)}.
\end{equation*}
Putting all the pieces together we have that
\begin{align*}
D(x(\cdot|s),x^{t+1}(\cdot|s)) &\leq D(x(\cdot|s),x^{t}(\cdot|s)) - D(x^{t+1}(\cdot|s),x^{t}(\cdot|s)) +  \beta \innerprod{2 \theta^{t}(s,\cdot) - \theta^{t-1}(s,\cdot)}{x(\cdot|s) - x^{t+1}(\cdot|s)} \\
&\leq D(x(\cdot|s),x^{t}(\cdot|s)) - D(x^{t+1}(\cdot|s),x^{t}(\cdot|s)) \\
&\phantom{=} +  \beta \innerprod{ \theta^{t}(s,\cdot) - \theta^{t+1}(s, \cdot)}{x(\cdot|s) - x^{t+1}(\cdot|s)} \\
&\phantom{=}+  \beta \innerprod{ \theta^{t}(s,\cdot) - \theta^{t-1}(s, \cdot)}{x(\cdot|s) - x^{t}(\cdot|s)} \\
&\phantom{=}+  \frac{ \beta^2}{(1 - \gamma)^2} \norm{y^{t}(\cdot|s) - y^{t-1}(\cdot|s)}^2_1 + \frac{1}{2}D(x^{t+1}(\cdot|s),x^{t}(\cdot|s)) \\
&\phantom{=}+ \beta \innerprod{ \theta^{t+1}(s, \cdot)}{x(\cdot|s) - x^{t+1}(\cdot|s)}
\end{align*}
Now, rearranging the terms we get
\begin{align*}
 \beta \innerprod{ \theta^{t+1}(s, \cdot)}{x(\cdot|s) - x^{t+1}(\cdot|s)}
&\leq D(x(\cdot|s),x^{t}(\cdot|s)) - D(x(\cdot|s),x^{t+1}(\cdot|s)) - \frac{1}{2}D(x^{t+1}(\cdot|s),x^{t}(\cdot|s)) \\
&\phantom{=} +  \beta \innerprod{ \theta^{t}(s,\cdot) - \theta^{t+1}(s, \cdot)}{x(\cdot|s) - x^{t+1}(\cdot|s)} \\
&\phantom{=}+  \beta \innerprod{ \theta^{t}(s,\cdot) - \theta^{t-1}(s, \cdot)}{x(\cdot|s) - x^{t}(\cdot|s)} \\
&\phantom{=}+  \frac{ \beta^2}{(1 - \gamma)^2} \norm{y^{t}(\cdot|s) - y^{t-1}(\cdot|s)}^2_1 
\end{align*}
Now, denoting $\Phi_x^t := D(x(\cdot|s),x^{t}(\cdot|s)) +   \beta \innerprod{ \theta^{t}(s,\cdot) - \theta^{t-1}(s, \cdot)}{x(\cdot|s) - x^{t}(\cdot|s)}$ and summing over $t$ we obtain
\begin{align*}
 \beta \sum^T_{t=1}\innerprod{ \theta^{t}(s, \cdot)}{x(\cdot|s) - x^{t}(\cdot|s)}
&\leq \sum^T_{t=1} \Phi_x^{t-1} - \Phi_x^t - \frac{1}{2} \sum^T_{t=1} D(x^t, x^{t-1}) +  \frac{ \beta^2}{(1 - \gamma)^2} \sum^T_{t=1} \norm{y^{t-1}(\cdot|s) - y^{t-2}(\cdot|s)}^2_1 
\end{align*}
Similarly we get
\begin{align*}
 \beta \sum^T_{t=1}\innerprod{ \zeta^{t}(s, \cdot)}{y(\cdot|s) - y^{t}(\cdot|s)}
&\leq \sum^T_{t=1} \Phi_y^{t-1} - \Phi_y^t - \frac{1}{2} \sum^T_{t=1} D(y^t, y^{t-1}) +  \frac{ \beta^2}{(1 - \gamma)^2} \sum^T_{t=1} \norm{x^{t-1}(\cdot|s) - x^{t-2}(\cdot|s)}^2_1 
\end{align*}
Now, using Pinkser's inequality and summing the two terms we have that
\begin{align*}
    T \epsilon^n &\leq \Phi^0 - \Phi^{T-1} - \frac{1}{2}\sum^T_{t=1} (D(y^t, y^{t-1}) + D(x^t, x^{t-1})) + \frac{2  \beta^2}{(1 - \gamma)^2} \sum^T_{t=1} (D(y^{t-1}, y^{t-2}) + D(x^{t-1}, x^{t-2}))
\end{align*}
with $\Phi^t = \Phi^t_x + \Phi^t_y$. At this point, setting $ \beta = \frac{1 - \gamma}{2}$, we obtain a telescopic sum
\begin{align*}
    T \epsilon^n &\leq \Phi^0 - \Phi^{T-1} - \frac{1}{2}\sum^T_{t=1} (D(y^t, y^{t-1}) + D(x^t, x^{t-1}) - D(y^{t-1}, y^{t-2}) - D(x^{t-1}, x^{t-2})) \\
    &\leq \Phi^0 - \Phi^{T-1} + \frac{1}{2}\br{D(y^1, y^0) + D(x^1,x^0)}
\end{align*}
Now, we can upper bound $\Phi^0 - \Phi^T \leq 2 \log \abs{\mathcal{A}} + \frac{8  \beta}{1 - \gamma}$ that allows to conclude that for all $n \in [N]$
\begin{align*}
    \epsilon^n &\leq \frac{3 \log \abs{\mathcal{A}} + 4}{T}.
\end{align*}
At this point noticing that the above result implies that $$\max_{n \in [N]} \epsilon^n_x + \max_{n\in[N]} \epsilon^n_y \leq \frac{3 \log \abs{\mathcal{A}} + 4}{T}.$$
Therefore, plugging in into \cite[Theorem 1]{perolat2015approximate} we obtain
\begin{equation*}
    \innerprod{\initial}{\max_{y} V^{x^N,y} - \min_x V^{x,y^N}} \leq \frac{3 \log(\abs{\mathcal{A}})+4}{(1 - \gamma)^2 T}  + \frac{2 \gamma^N}{(1 - \gamma)^2}
\end{equation*}
Therefore setting $N = \mathcal{O}\br{\log (\frac{(1 - \gamma)^2}{2 \epsilon})}$ and $T = \frac{3 \log(\abs{\mathcal{A}})+4}{\epsilon (1 - \gamma)^2}$ ensures that
\begin{equation*}
    \innerprod{\initial}{\max_{y} V^{x^N,y} - \min_x V^{x,y^N}} \leq 2 \epsilon.
\end{equation*}
which implies that $(x^N,y^N)$ is a $2 \epsilon$-approximate Nash equilibrium.
\fi
\end{proof}

\subsection{Proof of Theorem~~\ref{thm:same_updates}}
\label{proof:same_updates}
\begin{proof}
Let us consider two players performing the following updates
\begin{align*}
            % \theta_{t+1}
            \phi_h^{t+1} = \argmax_{\phi \in \mathcal{F}_{s_1}} 
            \beta \innerprod{\phi}{
            % Q^{\pi^t,\pi^t}(s_h,a_h,s_h,a_h^\prime)
            2 \mathbb{E}_{s',a' \sim \psi^t}r_h(\cdot,\cdot,s',a') - \mathbb{E}_{s',a' \sim \psi^{t-1}}r_h(\cdot,\cdot,s',a')} - \mathbb{D}(\phi, \phi_h^t),
        \end{align*}
        and
        \begin{align*}
            % \theta_{t+1}
            \psi_h^{t+1} = \argmin_{\psi \in \mathcal{F}_{s_1}} 
            \beta \innerprod{\psi}{
            % Q^{\pi^t,\pi^t}(s_h,a_h,s_h,a_h^\prime)
            2 \mathbb{E}_{s',a' \sim \phi^t}r_h(s',a',\cdot,\cdot) - \mathbb{E}_{s',a' \sim \phi^{t-1}}r_h(s',a',\cdot,\cdot)} + \mathbb{D}(\psi, \psi_h^t).
        \end{align*}
        The goal is to proof that the iterates generated by the two updates are identical. We will prove this fact by induction. The base case holds by initialization which gives $\phi_h^0=\psi^0_h$ for all $h \in [H]$.
        Then, let us assume by the induction step that $\psi^t_h = \phi^t_h$ for all $h \in [H]$, then 
        \begin{align*}
      % \scalemath{0.95}{
            % \theta_{t+1}
           & \phi_h^{t+1} \\&= \argmax_{\phi \in \mathcal{F}_{s_1}} 
            \beta \innerprod{\phi}{
            % Q^{\pi^t,\pi^t}(s_h,a_h,s_h,a_h^\prime)
            2 \mathbb{E}_{s',a' \sim \psi^t}r_h(\cdot,\cdot,s',a') - \mathbb{E}_{s',a' \sim \psi^{t-1}}r_h(\cdot,\cdot, s',a')} - \mathbb{D}(\phi, \phi_h^t) \\
            &= \argmax_{\phi \in \mathcal{F}_{s_1}} 
            \beta \innerprod{\phi}{
            % Q^{\pi^t,\pi^t}(s_h,a_h,s_h,a_h^\prime)
            - 2 \mathbb{E}_{s',a' \sim \psi^t}r_h(s',a',\cdot,\cdot) + \mathbb{E}_{s',a' \sim \psi^{t-1}}r_h(s',a',\cdot,\cdot)} - \mathbb{D}(\phi, \phi_h^t) + \beta \innerprod{\phi}{\mathbf{1}}\\&\text{(Antisymmetric Reward)}\\
            &= \argmax_{\phi \in \mathcal{F}_{s_1}} 
            \beta \innerprod{\phi}{
            % Q^{\pi^t,\pi^t}(s_h,a_h,s_h,a_h^\prime)
            - 2 \mathbb{E}_{s',a' \sim \psi^t}r_h(s',a',\cdot,\cdot) +\mathbb{E}_{s',a' \sim \psi^{t-1}}r_h(s',a',\cdot,\cdot)} - \mathbb{D}(\phi, \phi_h^t) + \beta\\&\text{(Normalization of $\phi$)}\\
            &= \argmax_{\phi \in \mathcal{F}_{s_1}} 
            \beta \innerprod{\phi}{
            % Q^{\pi^t,\pi^t}(s_h,a_h,s_h,a_h^\prime)
            - 2 \mathbb{E}_{s',a' \sim \psi^t}r_h(s',a',\cdot,\cdot) +\mathbb{E}_{s',a' \sim \psi^{t-1}}r_h(s',a',\cdot,\cdot)} - \mathbb{D}(\phi, \phi_h^t) \\&\text{($\beta$ does not depend on $\phi$)}\\
            &= \argmax_{\phi \in \mathcal{F}_{s_1}} 
            \beta \innerprod{\phi}{
            % Q^{\pi^t,\pi^t}(s_h,a_h,s_h,a_h^\prime)
            - 2 \mathbb{E}_{s',a' \sim \phi^t}r_h(s',a',\cdot,\cdot) +\mathbb{E}_{s',a' \sim \phi^{t-1}}r_h(s',a',\cdot,\cdot)} - \mathbb{D}(\phi, \psi_h^t) \\&\text{(Inductive Hypothesis)}
            % \\ & =
            %  \argmax_{d \in \mathcal{F}_{s_1}} 
            % \beta \innerprod{d}{
            % % Q^{\pi^t,\pi^t}(s_h,a_h,s_h,a_h^\prime)
            % 2 \mathbb{E}_{s',a' \sim d^t_h}r(\cdot,\cdot,s',a') - \mathbb{E}_{s',a' \sim d^{t-1}_h}r(\cdot,\cdot,s',a')} - \mathbb{D}(d, d_h^t).
            \\&= \argmin_{\psi \in \mathcal{F}_{s_1}} 
            \beta \innerprod{\psi}{
            2 \mathbb{E}_{s',a' \sim \phi^t}r_h(s',a',\cdot,\cdot) - \mathbb{E}_{s',a' \sim \phi^{t-1}}r_h(s',a',\cdot,\cdot)} + \mathbb{D}(\psi, \psi_h^t) 
            \\&\text{(Renaming the optimization variable and $\argmax_x f(x) = \argmin_x - f(x)$)}
            \\
            & = \psi^{t+1}_h.
            % }
        \end{align*}
\if 0
\yongtao{x is defined as the input prompt in the main body, and y is typically the output in other papers, can we use other notations? i.e.,$(\phi,\psi)$ or $(\varphi,\psi)$}
For an antisymmetric $Q$ function both players always play the same policy. We prove the statement by induction.
For the base step, we have that the FoRB update coincides with OMD considered by \cite{swamyminimaximalist} for which they proved that the two updates coincide.
Now we, assume that $x^t = y^t = \pi^t$ and we target to prove that the same equality holds at time step $t+1$.
We have that
\begin{align*}
    x^{t+1}(a'|s') &= x^t(a'|s') \exp( \beta(2 \mathbb{E}_{s\sim d^{x^k}, a\sim x^t(\cdot|s)}Q(s,a,s',a') - \mathbb{E}_{s\sim d^{x^k}, a\sim x^t(\cdot|s)}Q(s,a,s',a') )) \\
    &= x^t(a'|s') \exp( \beta(-2 \mathbb{E}_{s\sim d^{x^k}, a\sim x^t(\cdot|s)}Q(s',a',s,a) + \mathbb{E}_{s\sim d^{x^k}, a\sim x^t(\cdot|s)}Q(s',a',s,a) )) \\
    &= y^t(a'|s') \exp( \beta(-2 \mathbb{E}_{s\sim d^{y^k}, a\sim y^t(\cdot|s)}Q(s',a',s,a) + \mathbb{E}_{s\sim d^{y^k}, a\sim y^t(\cdot|s)}Q(s',a',s,a) )) \\
    &= y^{t+1}(a'|s')
\end{align*}
\fi


\end{proof}


\section{Implementation of Algorithm~\ref{alg:forb} with updates over policies.}
\label{app:implement_forb}
In this section, we explain how the update in Algorithm~\ref{alg:forb} for different choices of $\mathbb{D}$. In both cases, we will derive an update that can be summarized by following template.
Let us define $r_{h}^t(s,a) = \mathbb{E}_{s',a' \sim d^t_h} r(s,a,s',a')$ and $r_h^{t-1}(s,a) = \mathbb{E}_{s',a' \sim d^{t-1}_h} r(s,a,s',a')$
\begin{itemize}
    \item Compute the $Q^t_{h}$ function corresponding to the reward function $2r_h^t - r_h^{t-1}$ minimizing a loss function that depends on the choice of $\mathbb{D}$.
    \item Update the policy as 
    \begin{align*}
        \pi^{t+1}_h(a|s) \propto \pi^t_h(a|s) \exp\br{\beta Q^t_{h}(s,a)}.
    \end{align*}
\end{itemize}
Finally, in \cref{sebsec:approx_logistic_bellman_error} we show that for $\mathbb{D}$ being the conditional relative entropy and for $\beta$ small enough the value function $Q^t_{h}$ is well approximated by the standard Bellman equations.
\begin{remark}
Both choices of the Bregman divergence are $1$ strongly convex so \cref{thm:converge_fast} applies with $\lambda = 1$.
\end{remark}
In the following we consider a generic reward function $\tilde{r}$. In our setting, we will apply the following results for $\tilde{r}_h = 2r_{h}^t - r_{ h}^{t-1} $ in order to implement the updates of \cref{alg:forb} for the different values of $h$ and $t$.
\subsection{$\mathbb{D}$ chosen as the sum of conditional and relative entropy}
In this section, we explain how to implement the occupancy measure update in Algorithm~\ref{alg:forb} over policies. We use the machinery for single agent MDPs introduced in \cite{bas2021logistic}.
In particular, we consider the Bregman divergence given by the sum of the relative entropy $D(d,d') = \sum_{s,a}d(s,a) \log \br{\frac{d(s,a)}{d'(s,a)}}$ and of the conditional relative entropy given, i.e. $H(d,d') = \sum_{s,a}d(s,a) \log \br{\frac{\pi_d(a|s)}{\pi_{d'}(a|s)}}$ with $\pi_d(a|s) = d(s,a)/\sum_{a}d(s,a)$. Under this choice for $\mathbb{D}$, the update of Algorithm~\ref{alg:forb} for particular values of $h,t,s_1$ corresponds to the solution of the following optimization program
\begin{align}
d^{t+1}_h = \argmax_{d\in\Delta^H} &\sum^H_{h=1}\innerprod{d_h}{\tilde{r}_h} - \frac{1}{\beta} D(d_h,d_h^t) - \frac{1}{\beta} H(d_h,d_h^t), \nonumber \\
&\text{s.t.} \quad E^T d_h = F^T d_{h-1} \quad \forall h \in [H] \label{eq:update_app} \tag{\textcolor{black}{Update I}}.
\end{align}
\begin{theorem} \label{thm:updateV}
    The policy $\pi^{t+1}_h$ with occupancy measure $d^{t+1}_{h}$ defined in \cref{eq:update_app} can be computed as follows 
    \begin{align*}
        \pi^{t+1}_h(a|s) \propto \pi^t_h(a|s) \exp\br{\beta Q^t_{h}(s,a)},
    \end{align*}
    where $Q^t_{h}$ is the minimizer of the following loss
    \begin{align*}\frac{1}{\beta}\sum^H_{h=1} \log \sum_{s,a} \mu_h^t(s,a) \exp\br{\beta(2 \tilde{r}_h + PV_{h+1} - Q_h)(s,a)} + \innerprod{\initial}{V_1},
    \end{align*}
    while $V^t_{h+1}$ is given by the following closed form.
    \begin{align*}
        %&Q^t_{h, 2r^t - r^{t-1}} = 2r^t - r^{t-1} + F V^t_{h+1, 2r^t - r^{t-1}} \\
        V^t_{h+1}(s) = \frac{1}{\beta}\log \sum_a \pi^t_h(a|s) \exp(\beta Q^t_{h+1}(s,a)).
    \end{align*}
\end{theorem}

\begin{proof}
Let us introduce an auxiliary variable $\mu_h = d_h$ for all $h \in [H]$, then we can rewrite the optimization program as
\begin{align*}
\argmax_{d\in\Delta^H }\max_{\mu\in\Delta^H} &\sum^H_{h=1}\innerprod{\mu_h}{\tilde{r}_h} - \frac{1}{\beta} D(\mu_h,\mu_h^t) - \frac{1}{\beta} H(d_h,d_h^t), \\
&\text{s.t.} \quad E^T d_h = F^T \mu_{h-1} \quad \forall h \in [H], \\
&\text{s.t.} \quad \mu_h = d_h \quad \forall h \in [H].
\end{align*}
Then, by Lagrangian duality we have that
\begin{align*}
\max_{d\in\Delta^H}&\max_{\mu\in\Delta^H} \min_{Q, V} \sum^H_{h=1}\innerprod{\mu_h}{\tilde{r}} - \frac{1}{\beta} D(\mu_h,\mu_h^t) - \frac{1}{\beta} H(d_h,d_h^t) \\&+ \innerprod{- E^T d_h + F^T \mu_{h-1}}{V_h} + \innerprod{Q_h}{d_h - \mu_h} \\
&=\max_{d\in\Delta^H}\max_{\mu\in\Delta^H} \min_{Q, V} 
\sum^H_{h=1}\innerprod{\mu_h}{\tilde{r} + F V_{h+1} - Q_h} + \innerprod{d_h}{Q_h - E V_h} \\&\phantom{=}- \frac{1}{\beta} D(\mu_h,\mu_h^t) - \frac{1}{\beta} H(d_h,d_h^t) \\&\phantom{=}+ \innerprod{\initial}{V_1} = \mathcal{L}^\star\,.
\end{align*}
Then, by Lagrangian duality, we have that the objective is unchanged by swapping the min and max
\begin{align*}
\mathcal{L^\star}&=\min_{Q, V}  \max_{d\in\Delta^H}\max_{\mu\in\Delta^H} 
\sum^H_{h=1}\innerprod{\mu_h}{\tilde{r}_h + F V_{h+1} - Q_h} + \innerprod{d_h}{Q_h - E V_h} \\&- \frac{1}{\beta} D(\mu_h,\mu_h^t) - \frac{1}{\beta} H(d_h,d_h^t) + \innerprod{\initial}{V_1}\,.
\end{align*}
The inner maximization is solved by the following values
\begin{align*}
    \mu^{+}_h(Q,V) &\propto \mu^t_h \odot \exp \br{\beta(\tilde{r}_h + F V_{h+1} - Q_h)}, \\
    \pi^{+}_h(Q,V;s) & \propto \pi^t_h(\cdot|s) \odot \exp \br{\beta(Q_h(s,\cdot) - V_h(s))},
\end{align*}
where $\odot$ denotes the elementwise product between vectors. Then, replacing these values in the Lagrandian and parameterizing the functions $V_h$ by the functions $Q_h$ to ensure normalization of the policy, i.e. $V_h(s) = \frac{1}{\beta} \log \sum_a \pi^t_h(a|s)\exp(\beta Q_h(s,a))$ we have that 
\begin{align*}
\mathcal{L}^\star = \min_{Q} \frac{1}{\beta}\sum^H_{h=1} \log \sum_{s,a} \mu_h^t(s,a) \exp\br{\beta(\tilde{r}_h + FV_{h+1} - Q_h)(s,a)} + \innerprod{\initial}{V_1}.
\end{align*}
Therefore, denoting \begin{align*}
    Q^t_h &= \argmin_{Q} \frac{1}{\beta}\sum^H_{h=1} \log \sum_{s,a} \mu_h^t(s,a) \exp\br{\beta(\tilde{r}_h + FV_{h+1} - Q_h)(s,a)} + \innerprod{\initial}{V_1},
\end{align*} and $V^t_h = \frac{1}{\beta} \log \sum_a \pi^t_h(a|s)\exp(\beta Q^t_h(s,a))$, we have that the policy $\pi^{t+1}_h(\cdot|s) = \pi^{+}_h(Q^t,V^t;s) $ has occupancy measure equal to $d^{t+1}_h$ for all $h\in[H]$.
This is because by the constraints of the problem we have that $d^{t+1}_h$ satisfies the Bellman flow constraints and that the policy $\pi^{t+1}_h$ satisfies $\pi^{t+1}_h(a|s) = d^t_h(s,a)/\sum_{a}d^t_h(s,a)$.
\end{proof}
\subsection{$\mathbb{D}$ chosen as conditional relative entropy \cite{neu2017unified}}
In this section, we study the update considering $\mathbb{D}$ chosen as sum of the conditional relative entropy over the stages $h'$ s.t. $1 \leq h' \leq h$, i.e. we study the following update.\footnote{The sum over previous stages is taken to ensure $1$-strong convexity. Indeed, it holds that $\sum^h_{h'=1} H(d_{h'},d'_{h'}) \geq D(d_h,d'_h) \geq \frac{1}{2}\norm{d_h - d'_h}^2_1$. The first inequality is proven in \citet[Lemma 7]{neu2021online}. }
\begin{align}
d^{t+1} = \argmax_{d\in\Delta^H} &\sum^H_{h=1}\br{\innerprod{d_h}{\tilde{r}_h} - \frac{1}{\beta} \sum^h_{h'=1}H(d_{h'},d_{h'}^t)}, \nonumber \\
&\text{s.t.} \quad E^T d_h = F^T d_{h-1} \quad \forall h \in [H].\label{eq:update2}
\end{align}
\begin{theorem} \label{thm:updateV_bis}
    The policy $\pi^{t+1}_h$ with occupancy measure $d^{t+1}_{h}$ defined in \cref{eq:update2} can be computed as follows 
    \begin{align*}
        \pi^{t+1}_h(a|s) \propto \pi^t_h(a|s) \exp\br{\frac{\beta}{H-h+1}(Q^t_{h}(s,a))},
    \end{align*}
    where $Q^t_{h}$ and $V^t_{h+1}$ satisfies the following recursion
    \begin{align*}
        &Q^t_{h} = \tilde{r}_h + F V^t_{h+1} \\
        &V^t_{h+1}(s) = \frac{H-h+1}{\beta}\log \sum_a \pi^t_h(a|s) \exp\br{\frac{\beta}{H-h+1} Q^t_{h+1}(s,a)}.
    \end{align*}
\end{theorem}
    \begin{remark}
    The above recurrencies are sometimes called soft Bellman equations \cite{ziebart2010modeling,fox2015taming}.
\end{remark}
\if 0
\begin{remark}
Using the expression of the conditional relative entropy $H$ we obtain
\begin{align*}
d^{t+1} = \argmax_{d\in\Delta^H} &\sum^H_{h=1}\br{\sum_{s,a}d_h(s,a)\br{2 r^t_h(s,a) - r^{t-1}_h(s,a) - \frac{1}{\beta}\log \frac{\pi_{d_h}(a|s)}{\pi_{d^t_h}(a|s)}} - \frac{1}{\beta} \sum^{h-1}_{h'=1} H(d_{h'}, d^t_{h'})},\nonumber \\
&\text{s.t.} \quad E^T d_h = F^T d_{h-1} \quad \forall h \in [H].
\end{align*}
This is equivalent to solve a Markov decision process with policy dependent reward i.e. $2r^t_h(s,a) - r^{t-1}_h(s,a) - \frac{1}{\beta}\log \frac{\pi_{d_h}(a|s)}{\pi_{d^t_h}(a|s)}$ which can be solved by a backward recursion with the soft Bellman equations \cite[Corollary 6.8]{ziebart2010modeling}. The following theorem slightly generalizes their result for the case of $\pi^t_h$ being non uniform.
\if 0
The policy $\pi^{t+1}_h$ with occupancy measure $d^{t+1}_h$ computed as in Equation~\ref{eq:update2} can be computed as follows 
    \begin{align*}
        \pi^{t+1}_h(a|s) \propto \pi^t_h(a|s) \exp\br{\beta(Q^t_{h, 2r^t - r^{t-1}}(s,a))}
    \end{align*}
    where $Q^t_{h, 2r^t - r^{t-1}}$ satisfies the following Bellman equations
    \begin{align*}
        &Q^t_{h, 2r^t - r^{t-1}}(s,a) = 2r_h^t(s,a) - r_h^{t-1}(s,a) + F V^t_{h+1, 2r^t - r^{t-1}}(s,a) \\
        &V^t_{h, 2r^t - r^{t-1}}(s) = \max_{\pi} \sum_a \pi(a|s) \br{Q^t_{h, 2r^t - r^{t-1}}(s,a) - \frac{1}{\beta} \log \frac{\pi_{d_h}(a|s)}{\pi_{d^t_h}(a|s)}} \\&=
        \log \sum_{a} \pi_{d^t_h}(a|s) \exp\br{\beta Q^t_{h, 2r^t - r^{t-1}}(s,a)}
    \end{align*}
\fi
\end{remark}
\fi
\begin{proof}
Let us introduce an auxiliary variable $\mu_h = d_h$ for all $h \in [H]$, then we can rewrite the optimization program as
\begin{align*}
\argmax_{d\in\Delta^H }\max_{\mu} &\sum^H_{h=1}\br{\innerprod{\mu_h}{\tilde{r}_h} - \frac{1}{\beta} \sum^h_{h'=1}H(d_{h'},d_{h'}^t)} \\
&\text{s.t.} \quad E^T d_h = F^T \mu_{h-1} \quad \forall h \in [H] \\
&\text{s.t.} \quad \mu_h = d_h \quad \forall h \in [H].
\end{align*}
Notice that importantly, we do not constraint the variable $\mu$. Then, by Lagrangian duality we have that
\begin{align*}
\max_{d\in\Delta^H}&\max_{\mu} \min_{Q, V} \sum^H_{h=1}\innerprod{\mu_h}{\tilde{r}_h} - \frac{1}{\beta} \sum^h_{h'=1}H(d_{h'},d_{h'}^t) \\&+ \innerprod{- E^T d_h + F^T \mu_{h-1}}{V_h} + \innerprod{Q_h}{d_h - \mu_h} \\
&=\max_{d\in\Delta^H}\max_{\mu} \min_{Q, V} 
\sum^H_{h=1}\innerprod{\mu_h}{\tilde{r}_h + F V_{h+1} - Q_h} + \innerprod{d_h}{Q_h - E V_h} \\&\phantom{=} - \frac{1}{\beta} \sum^h_{h'=1}H(d_{h'},d_{h'}^t) + \innerprod{\initial}{V_1}
\\
&=\min_{Q, V} \max_{d\in\Delta^H}\max_{\mu} 
\sum^H_{h=1}\innerprod{\mu_h}{\tilde{r}_h + F V_{h+1} - Q_h} + \innerprod{d_h}{Q_h - E V_h} \\&\phantom{=} - \frac{H -h + 1}{\beta}H(d_{h},d_{h}^t) + \innerprod{\initial}{V_1} = \tilde{\mathcal{L}}^\star,
\end{align*}
where the last equality holds by Lagrangian duality and by $\sum^H_{h=1} \sum^h_{h'=1} H(d_{h'}, d^t_{h'}) = \sum^H_{h=1} (H -h +1 ) H(d_{h'}, d^t_{h'})$.
Now since $\mu$ is unconstrained we have that $\max_{\mu} \sum^H_{h=1}\innerprod{\mu_h}{\tilde{r}_h + F V_{h+1} - Q_h} $ is equivalent to impose the constraint $\tilde{r}_h + F V_{h+1} = Q_h$ for all $h \in [H]$. Moreover, as in the proof of \cref{thm:updateV} the optimal $d_h$ needs to satisfies that $\pi_{d_h}(a|s) = d_h(s,a)/\sum_a d_h(s,a) $ is equal to
$\pi^{+}_h(Q,V;s) = \pi^t_h(\cdot|s) \odot \exp \br{\frac{\beta}{H-h+1}(Q_h(s,\cdot) - V_h(s))}$
for $V_h(s)= \frac{H-h+1}{\beta} \log \sum_a \pi^t_h(a|s)\exp(\frac{\beta}{H-h+1} Q_h(s,a))$. Plugging in, these facts in the expression for $\tilde{\mathcal{L}}^\star$, we have that
\begin{align*}
    \tilde{\mathcal{L}}^\star = \min_Q \innerprod{\initial}{V_1} \quad \text{s.t.}~~~ \tilde{r}_h + F V_{h+1} = Q_h \quad \forall h \in [H].
\end{align*}
Since the above problem as only one feasible point, we have that the solution is the sequence $Q^t_h$ satisfying the recursion $\tilde{r}_h + F V^t_{h+1} = Q^t_h$ with $V^t_h(s)= \frac{H-h+1}{\beta} \log \sum_a \pi^t_h(a|s)\exp(\frac{\beta}{H-h+1} Q^t_h(s,a))$.
\end{proof}

\subsection{Approximating soft Bellman equations by standard Bellman equations.}
\label{sebsec:approx_logistic_bellman_error}
Unfortunately, implementing the update for the $V$ value as in Theorem~\ref{thm:updateV} is often numerically instable. In this section, we show a practical approximation which is easy to implement and shown to be accurate for $\beta$ sufficiently small.
\begin{theorem}\label{thm:approx}
Let us denote $\beta_h = \frac{\beta}{H-h+1}$ and let us assume that the values $Q^t_h$ generated by the soft Bellman equations in \cref{thm:updateV_bis} are uniformly upper bounded by $Q_{\max}$, and let us choose $\beta_h \leq \frac{1}{Q_{\max}}$ for all $h\in[H]$. Then, it holds that
\begin{align*}
\innerprod{\pi^t_h(\cdot|s)}{Q^t_h(s,\cdot)} \leq \frac{1}{\beta_h} \log \sum_a \pi^t_h(a|s) \exp(\beta_h Q^t_h(s,a)) \leq \innerprod{\pi^t_h(\cdot|s)}{Q^t_h(s,\cdot)} + \beta_h Q^2_{\max}\,.
\end{align*}
\end{theorem}
\begin{proof}
\begin{align*}
\frac{1}{\beta_h} \log \sum_a \pi^t_h(a|s) \exp(\beta_h Q^t_h(s,a)) &\geq \frac{1}{\beta_h} \sum_{a}\pi^t_h(a|s) \log \exp (\beta_h Q^t_h(s,a) )
    \\ &= \innerprod{\pi^t_h(\cdot|s)}{Q^t_h(s,\cdot)},
\end{align*}
where the above inequality holds for Jensen's. For the upper bound, we first use the inequality $e^x \leq 1 + x + x^2$ for $x \leq 1$ we have that
\begin{align*}
&\frac{1}{\beta_h} \log \sum_a \pi^t_h\exp(\beta_h Q^t_h(s,a)) \\&\leq \frac{1}{\beta_h} \log \sum_a \pi^t_h (1 + \beta_h Q^t_h(s,a) + \beta_h^2 Q_{\max}^2 ) \quad \text{ 
 (Using $Q^t_h(s,a) \leq Q_{\max}$)} \\
&= \frac{1}{\beta_h} \log (1 + \beta_h \sum_a \pi^t_h(a|s) Q^t_h(s,a) + \beta_h^2 Q_{\max}^2 ) \\& \leq 
\frac{1}{\beta_h} \br{ \sum_a \pi^t_h(a|s) \beta_h Q^t_h(s,a) + \beta_h^2 Q_{\max}^2 } \quad \text{   (Using $\log(1 + x) \leq x$)}  \\&
\leq \innerprod{\pi^t_h(\cdot|s)}{Q^t_h(s,\cdot)} + \beta_h Q_{\max}^2 .
\end{align*}
\end{proof}
\begin{remark}
    Given this result, in the implementation for deep RL experiment, i.e. Algorithm~\ref{alg:forbprac} we compute the standard $Q$ value satisfying the standard Bellman equations (given in \cref{lemma:bellman}) rather than the soft Bellman equation in \cref{thm:updateV}. In virtue of \cref{thm:approx}, the approximation is good for $\beta$ reasonably small.
\end{remark}
%ATTEMPT TO GET LAST ITERATE BUT IT DIDN'T WORK BECAUSE WE DO NOT HAVE SMOOTHNESS IN THE NORM.
%WE WILL TRY WITH SMOOTHNESS IN KL.
\if 0
\section{Last iterate guarantees for the $\mu$-regularized preference model}
For our last iterate result, we consider the $\mu$-regularized preference model by \cite{munos2024nash,shani2024multi} lifted in the occupancy measure space.
In particular, we consider finding a saddle point of the following problem
\begin{align}
    (d^\star_\mu, d^\star_\mu) &= \argmax_{d\in\tilde{\mathcal{F}}} \min_{d'\in\tilde{\mathcal{F}}} \mathbb{E}_{s_1 \sim \initial}\sum^H_{h=1}\sum_{s,a,s',a'}  d_h(s,a | s_1)r(s,a,s',a') d_h'(s',a'|s_1) - \mu \mathbb{E}_{s_1 \sim \initial} \mathbb{D}(d(\cdot,\cdot|s_1),d_{\mathrm{ref}}(\cdot,\cdot|s_1)) \nonumber \\&\phantom{=}
    + \mu \mathbb{E}_{s_1 \sim \initial} \mathbb{D}(d'(\cdot,\cdot|s_1),d_{\mathrm{ref}}(\cdot,\cdot|s_1)) 
    \,, \label{eq:alignment_game} \tag{Alignment-Game}
\end{align}
Crucial to show the exponential convergence rate for the last iterate is to realize that the above saddle point problem is relative strongly convex-strongly concave with respect to the Bregman divergence $\mathbb{D}(\cdot,\cdot)$.
\begin{definition}
\textbf{Relative strong convexity}
A function $f:\mathcal{X}\rightarrow\mathbb{R}$ is said to be $\mu$-relative strongly convex  in $x$ relative to the divergence $\mathbb{D}(\cdot,\cdot)$ if it satisfies the following inequality
\begin{equation*}
    f(x) \geq f(x') + \innerprod{\nabla f(x')}{x - x'} + \mu \mathbb{D}(x,x').
\end{equation*}
\end{definition}
The following Lemma shows that the objective in \ref{eq:alignment_game} is relative strongly convex-strongly concave.
\begin{lemma}
The min max problem in \ref{eq:alignment_game} is $\mu$- strongly convex in $d'$ and strongly concave in $d$ with respect to the divergence $\mathbb{D}(\cdot,\cdot)$.
\end{lemma}
\begin{proof}
We show the strong convexity with respect to $d$. The strong concavity with respect to the variable $d'$ holds analogously.
Let us consider a fixed $s_1$ and let us consider the function
$$
f(d; r) = \sum^H_{h=1}\sum_{s,a} d_h(s,a|s_1) r(s,a) + \mu \mathbb{D}(d(\cdot,\cdot|s_1),d_{\mathrm{ref}}(\cdot,\cdot|s_1)) 
:= \innerprod{d}{r} + \mu \mathbb{D}(d,d_{\mathrm{ref}})
$$
for a general $r \in \mathbb{R}^{\abs{\mathcal{S}}\abs{\mathcal{A}}}$.
For this specific choice of $f$, we have that
\begin{align*}
f(d';r) + \innerprod{\nabla f (d',r)}{d - d'} + \mu \mathbb{D}(d,d') &= \innerprod{d'}{r} + \mu \mathbb{D}(d',d_{\mathrm{ref}}) + \innerprod{r +\mu (\nabla \phi(d') - \nabla \phi(d_{\mathrm{ref}}))}{d - d'} + \mu \mathbb{D}(d,d') \\
&= \innerprod{d}{r} + \mu \mathbb{D}(d',d_{\mathrm{ref}}) + \mu \innerprod{ \nabla \phi(d') - \nabla \phi(d_{\mathrm{ref}})}{d - d'} + \mu \mathbb{D}(d,d') \\
\end{align*}
where $\mathbf{\phi}(d)$ is the distance generating function of the Bregman divergence $\mathbb{D}$ that is
\begin{equation*}
    \mathbb{D}(d,d') = \mathbf{\phi}(d) - \mathbf{\phi}(d') - \innerprod{\nabla \phi(d')}{d - d'}.
\end{equation*}
At this point, using the three point identity, we have that
\begin{equation*}
\innerprod{\nabla \phi(d') - \nabla\phi(d_{\mathrm{ref})}}{d' - d} = \mathbb{D}(d',d_{\mathrm{ref}}) + \mathbb{D}(d,d') - \mathbb{D}(d,d_{\mathrm{ref}})
\end{equation*}
Plugging in we have that
\begin{align*}
f(d';r) + \innerprod{\nabla f (d',r)}{d - d'} + \mu \mathbb{D}(d,d') &= \innerprod{d}{r} + \mu \mathbb{D}(d',d_{\mathrm{ref}}) - \mu \br{\mathbb{D}(d',d_{\mathrm{ref}}) + \mathbb{D}(d,d') - \mathbb{D}(d, d_{\mathrm{ref}})} + \mu \mathbb{D}(d,d') \\
&= \innerprod{d}{r}  + \mu \mathbb{D}(d,d_{\mathrm{ref}}) \\&= f(d,r) 
\end{align*}
Finally, taking the expectations over $s_1$ on both sides it concludes the proof.
\end{proof}
Another useful proof is to show Lipshitzness of the gradients of the function $f$.
In particular, notice that
\begin{align*}
\nabla f(d,r) = r + \mu \nabla \phi(d) - \mu \nabla \phi(d_{\mathrm{ref}})
\end{align*}
Therefore,
\begin{align*}
    \norm{\nabla f(d,r) - \nabla f(d',r)}_{\infty} = \mu \norm{\nabla \phi(d,r) - \nabla \phi(d',r)}_{\infty}
\end{align*}
At this point, we can 
formulate the constraints as an indicator function defined as follows
\begin{equation*}
    h_{\widetilde{\mathcal{F}}}(d) = \begin{cases}
& 0 ~~~~~ \text{if} ~~~~~~ d \in \widetilde{\mathcal{F}} \\
& +\infty ~~~~~ \text{if} ~~~~~~ d \notin \widetilde{\mathcal{F}} \\
    \end{cases}
\end{equation*}
With this notation in place, we can rewrite \ref{eq:alignment_game} the game as follows
\begin{align*}
(d^\star_\mu, d^\star_\mu) &= \argmax_{d\in\Delta} \min_{d'\in\Delta} \mathbb{E}_{s_1 \sim \initial}\sum^H_{h=1}\sum_{s,a,s',a'}  d_h(s,a | s_1)r(s,a,s',a') d_h'(s',a'|s_1) - \mu \mathbb{E}_{s_1 \sim \initial} \mathbb{D}(d(\cdot,\cdot|s_1),d_{\mathrm{ref}}(\cdot,\cdot|s_1)) \nonumber \\&\phantom{=}
    + \mu \mathbb{E}_{s_1 \sim \initial} \mathbb{D}(d'(\cdot,\cdot|s_1),d_{\mathrm{ref}}(\cdot,\cdot|s_1)) - h_{\widetilde{\mathcal{F}}}(d) + h_{\widetilde{\mathcal{F}}}(d')
\end{align*}
At this point, we can analyze OMPO in this setting applying the following result from the optimization literature \cite{jiang2022generalized}.
\fi
\section{Supplementary material on experiment}
\begin{algorithm}[t]
% \setcounter{ALC@unique}{0}
\caption{\oomdmethod{} (Practical version)}
\label{alg:forbprac}
\begin{algorithmic} %[1]
    \STATE \textbf{input}:
    reference policy $\pi^1$,
    preference oracle $\mathbb{P}$,
    learning rate $\beta$, number of generated samples $K$, 
    % {\color{ForestGreen}{
    horizon $H$, total iteration $T$, tunable bias term $\tau$.
    % }}
    \FOR{$t=1,2,\dots, T $}
        \STATE Generates response by sampling $s_1^1 \sim \initial$ and $a_h^1 \sim   \pi^t(\cdot|s_h^1)$ for $h$ $\in [H]$. 
        \STATE Clear the dataset buffer $\mathcal{D}_t$.
        \FOR{$
        {h=1,2,\dots, H}$}
        \STATE Set $s_{h}^{K} =,\dots,=s_{h}^{2}=s_{h}^{1} $.
        \STATE Generate $K-1$ conversations by sampling $a_{\hat{h}}^{2:K} \sim  \pi^t(\cdot|s_{\hat{h}}^{2:K})$ for $\hat{h}$ $\in [h,H]$. 
        \STATE Estimate $\mathbb{E}_{a_h^{k^\prime}} Q^{\pi^t,\pi^t}(s_h^1,a_h^{k},s_h^1,a_h^{k'})  \forall k,k^\prime \in [K]$  
        % by rollouts of the policies.
        via \cref{equ:q_estimate}.
         % with query to the oracle $\mathbb{P}$
        \IF{$t>1$}
        \STATE Estimate $
       \mathbb{E}_{a_h^{k^\prime}} Q^{\pi^{t},\pi^{t-1}}(s_h^1,a_h^{k},s_h^1,a_h^{k'})\quad
        \forall k,k^\prime \in [K]$
        % by rollouts of the policies.
        via \cref{equ:q_estimate}.
        \STATE Add
        $
          \scalemath{0.8}{
        \{
        (s_h^1, a_h^k,
        \mathbb{E}_{a_h^{k^\prime}}Q^{\pi^t,\pi^t}(s_h^1,a_h^k,s_h^1,a_h^{k^\prime}) ,
        \mathbb{E}_{a_h^{k^\prime}}Q^{\pi^{t},\pi^{t-1}}(s_h^1,a_h^k,s_h^1,a_h^{k^\prime})
        \}
        _{k\in[K]}}
        $
        into  $\mathcal{D}_t$.
        \ELSE
        \STATE 
        Add
        $
          \scalemath{0.8}{
        \{
        (s_h^1, a_h^k,
        \mathbb{E}_{a_h^{k^\prime}}Q^{\pi^t,\pi^t}(s_h^1,a_h^k,s_h^1,a_h^{k^\prime})
        \}
        }
        $
        into  $\mathcal{D}_t$.
      \ENDIF
        \ENDFOR
    \IF{$t>1$}
        \STATE Optimize $\pi_{{t+1}}$ over $\mathcal{D}_t$ according to 
                % \scalebox{0.5}{
        \begin{align*}
        \scalemath{1}{
            \pi^{t+1}
            \leftarrow 
            \argmin_{\pi}
            \mathbb{E}
            \bigg(
            \log \bigg(\frac{\pi(a_h^k|s_h^1)}{\pi^t(a_h^k|s_h^1)}\bigg)
            -
            \beta \bigg(2\mathbb{E}_{a_h^{k^\prime}}Q^{\pi^t,\pi^t}(s_h^1,a_h^k,s_h^1,a_h^{k^\prime})
            - 
            \mathbb{E}_{a_h^{k^\prime}}Q^{\pi^{t},\pi^{t-1}}(s_h^1,a_h^k,s_h^1,a_h^{k^\prime}
            ) 
            - \tau
            \bigg)
            \bigg)^2.}
        \end{align*}
        % }
    \ELSE
        \STATE     Optimize $\pi_{{t+1}}$ over $\mathcal{D}_t$ according to 
                % \scalebox{0.5}{
        \begin{align*}
        \scalemath{1}{
            % \theta_{t+1}
            \pi^{t+1}
            \leftarrow 
            \argmin_{\pi}
            \mathbb{E}
            % _{
            % ( s_h^1, a_h^k,\mathbb{E}_{a_h^{k^\prime}}Q^{\pi^t,\pi^t}(s_h^1,a_h^k,s_h^1,a_h^{k^\prime}) )\sim \mathcal{D}_t}
            \bigg(
            \log \bigg(\frac{\pi(a_h^k|s_h^1)}{\pi^t(a_h^k|s_h^1)}\bigg)
            -
            \beta \bigg(
            % Q^{\pi^t,\pi^t}(s_h,a_h,s_h,a_h^\prime)
        \mathbb{E}_{a_h^{k^\prime}}Q^{\pi^t,\pi^t}(s_h^1,a_h^k,s_h^1,a_h^{k^\prime}) 
            % -\frac{(1-\gamma^{H-h+1})}{2(1-\gamma)}
            -\frac{H-h+1}{2}
            \bigg)
            \bigg)^2.}
        \end{align*}
    \ENDIF
    \ENDFOR
    \STATE \textbf{output}: $\pi^{T+1}$
  \end{algorithmic}
\end{algorithm}

\label{sec:addexp}
\subsection{Experiment in MT-bench 101}
\label{sec:appendix_exp}
The tasks in MT-bench 101 include 
Context Memory (CM), Anaphora Resolution (AR), Separate Input (SI), Topic Shift (TS), Content Confusion (CC), Content Rephrasing (CR), Format Rephrasing (FR), Self-correction (SC), Self-affirmation (SA), Mathematical Reasoning (MR), General Reasoning (GR), Instruction Clarification (IC), and Proactive Interaction (PI). We list the description of each task in \cref{tab:task}. The default evaluation mode of MT-bench 101 is that the GPT model requires to access the conversation based on the given ground truth of previous steps, provided in MT-bench 101. However, in our problem setting, the answers among the conversation is also generated by the model. We use ``gpt-4o-mini-2024-07-18'' to evaluate the conversation.
The maximum output length and maximum sequence length of gpt-4o are set as 4096. We use a batch size of 8 with a temperature of 0.8. We use the same prompt for gpt-4o as in \citet{bai2024mt}. Our experiment is conducted on 4 H200 GPUs. We use the PyTorch platform and the Transformer Reinforcement Learning (TRL) for fine-tuning. The $\gamma$ is selected as zero. Each method is trained with epochs number selected from $ \{1, 2\} $, learning rates from $\{5e\text{-}6, 5e\text{-}7\} $, and $\beta$ values from $ \{0.1, 0.01, 0.001\}$. The final model is chosen based on the highest winning rate against the base model, as determined by the PairRM model. We use full-parameter fine-tuning for all methods with bf16 precision. A batch size of 64 is used. The maximum output length and maximum prompt length during training are both set as 2048. We use AdamW optimizer~\citep{loshchilov2018decoupled} and cosine learning rate schedule~\citep{loshchilov2017sgdr} with a warmup ratio of 0.1.
\begin{table*}[h]
\caption{A detailed description of each task in MT-bench 101 (taken from \citet{bai2024mt}.)}
\vspace{3mm}
\centering
\setlength{\abovecaptionskip}{0.15cm}  
\setlength{\belowcaptionskip}{-0.3cm}  
\resizebox{1\textwidth}{!}{
\begin{tabular}{l|c|l}
\toprule
\textbf{Task} & \textbf{Abbr.} & \textbf{Description}  \\ \midrule
Context Memory & CM & Recall early dialogue details to address the user's current question. \\ \midrule
Anaphora Resolution & AR & Identify pronoun referents throughout a multi-turn dialogue. \\ 
Separate Input & SI & The first turn outlines the task requirements and the following turns specify the task input. \\ \midrule
Topic Shift & TS & Recognize and focus on the new topic when users unpredictably switch topics. \\ 
Content Confusion & CC & Avoid interference from similar-looking queries with distinct meanings in the dialogue's history. \\ \midrule
Content Rephrasing & CR & Rephrase the content of the last response according to the user's newest requirement. \\ 
Format Rephrasing & FR & Rephrase the format of the last response according to the user's newest requirement. \\ \midrule
Self-correction & SC & Recorrect the last response according to the user feedback. \\ 
Self-affirmation & SA & Preserve the last response against inaccurate user feedback. \\ \midrule
Mathematical Reasoning & MR & Collaboratively solve complex mathematical problems with users across dialogue turns.\\ 
General Reasoning & GR & Collaboratively solve complex general reasoning problems with users across dialogue turns. \\ \midrule
Instruction Clarification & IC & Seek clarification by asking further questions on ambiguous user queries. \\
Proactive Interaction & PI & Propose questions in reaction to user statements to spark their interest to continue the dialogue. \\
\bottomrule
\end{tabular}
}
% \vspace{-1mm}
\label{tab:task}
\end{table*}

\newpage

\rebuttal{Next, we provide the comparison between the proposed \method{} and IPO~\citep{azar2024general}, which also uses the squared loss and bypasses the BT model assumption. We run both IPO and MPO for one iteration.
% as we observe that IPO is over-fitting in the second iteration. 
The results in \cref{tab:compare_ipi} show that \method{} achieves a higher average score than IPO.}
\begin{table}[!h]
\caption{\rebuttal{Comparison between \method{} and IPO in MT-BENCH 101 dataset.}}
\vspace{1mm}
\centering
\setlength\tabcolsep{4pt}
\renewcommand{\arraystretch}{1.5}
% \scalebox{0.5}{
\resizebox{1\textwidth}{!}{
% \small
\rebuttal{
\begin{tabular}{c|c|c|cc|cc|cc|cc|cc|cc}
\toprule
\multirow{3}{*}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{}} & \multicolumn{5}{|c}{\textbf{Perceptivity}} & \multicolumn{6}{|c}{\textbf{Adaptability}} & \multicolumn{2}{|c}{\textbf{Interactivity}} \\
 % & \multicolumn{1}{c}{\textbf{}} & \multicolumn{1}{|c}{\textbf{Memory}} & \multicolumn{2}{|c}{\textbf{Understanding}} & \multicolumn{2}{|c}{\textbf{Interference}} & \multicolumn{2}{|c}{\textbf{Rephrasing}} & \multicolumn{2}{|c}{\textbf{Reflection}} & \multicolumn{2}{|c}{\textbf{Reasoning}} & \multicolumn{2}{|c}{\textbf{Questioning}} \\
 & \multicolumn{1}{c}{\textbf{Avg.}} & \multicolumn{1}{|c}{\textbf{CM}}     & \multicolumn{1}{|c}{\textbf{SI}} & \multicolumn{1}{c}{\textbf{AR}} & \multicolumn{1}{|c}{\textbf{TS}} & \multicolumn{1}{c}{\textbf{CC}} & \multicolumn{1}{|c}{\textbf{CR}} & \multicolumn{1}{c}{\textbf{FR}} & \multicolumn{1}{|c}{\textbf{SC}} & \multicolumn{1}{c}{\textbf{SA}} & \multicolumn{1}{|c}{\textbf{MR}} & \multicolumn{1}{c}{\textbf{GR}} & \multicolumn{1}{|c}{\textbf{IC}} & \multicolumn{1}{c}{\textbf{PI}} \\
 \midrule
 % GPT4    &  \\
 %  \midrule
Base (Mistral-7B-Instruct)             &  6.223 & 
7.202 & 7.141 & 7.477 & 7.839 & 8.294 & 6.526 & 6.480 & 4.123 & 4.836 & 4.455 & 5.061 & 5.818 & 5.641                            \\ \midrule
IPO  
 & 6.498  & 7.518  & 7.480  & 7.759  & 7.952  & 8.652  & 6.892  & 6.768  & 4.390  & 5.185  & 4.313  & 5.378  & 6.146  & 6.044 \\ 
\midrule
\method{}     &      6.630 & 7.624 & {7.846} & 8.085 & 8.398 & 8.947 & 7.105 & 7.286 & 4.208 & 4.993 & 4.377 & 5.264 & 6.179 & 5.873
\\ \midrule
\bottomrule
\end{tabular}}
}
\label{tab:compare_ipi}
\end{table}

\rebuttal{
We now present an ablation study to evaluate the benefits of incorporating terminal rewards. Using MPO, we compare two approaches for optimizing \(a_h\): one computes the preference signal based on the terminal state \(s_{H+1}\), while the other uses the immediate next state \(s_h\). The results within one iteration for the MT-Bench 101 dataset are shown in \cref{tab:compare_terminal}, and those for the GSM/Math experiments are provided in \cref{tab:compare_terminal_gsm}. Our findings reveal that using the terminal state \(s_{H+1}\) performs worse than using the immediate state \(s_h\) in MT-Bench 101. In contrast, the difference in performance is negligible in the GSM/Math tasks. The underlying reason is that in multi-turn conversational datasets, especially when adjacent questions are not closely related, relying on preferences derived from the terminal state can introduce noise. However, in math and reasoning tasks, the terminal state often captures the final answer, making it more critical. Moreover, using \(s_{H+1}\) for preference signals is significantly more computationally expensive than using \(s_h\), due to the extended sequence length. Consequently, we conclude that adapting the choice of terminal preference or intermediate preference on the task's characteristics is crucial for balancing performance and efficiency.
}

\subsection{Experiment in math-reasoning task}
Our experiment is conducted on 4 A100 GPUs.
For both \method{} and \oomdmethod{}, we perform full-parameter finetuning for 1 epoch with learning rate $5e^{-7}$ and $\beta$ tuned in the range of $\{0.1,0.01,0.001\}$, we set the $\log z$  as 0.5. The final state with the answer is important in this task so we only use the terminal reward (see \cref{tab:compare_terminal_gsm} for comparison). We run two iterations for both methods.
% $S_{H+1}$ in the calculation of the $Q$ function and ignore comparisons with all previous states.
We use AdamW optimizer~\citep{loshchilov2018decoupled} and cosine learning rate schedule~\citep{loshchilov2017sgdr} with a warmup ratio of 0.1.
\begin{table}[!h]
\tabcolsep=0.1cm
	% \renewcommand\arraystretch{2}
    \centering
    \vspace{3mm}
    \caption{ \rebuttal{
    Ablation on terminal reward in MATH and GSM8K dataset.}}
    \rebuttal{
\begin{tabular}{c|c|c}
\toprule
 Method &   GSM8K &     Math              \\
\midrule
 Base (Qwen2-7B-Instruct)    &  0.8559  & 0.5538   \\
 \method{}  (intermediate reward)
 %($s_{h}$) 
 &  0.8734 &  0.5720   \\
\method{} 
%($s_{H+1}$)   
(terminal reward)
&  0.8734 &  0.5734   \\
\bottomrule
\end{tabular}
}
\label{tab:compare_terminal_gsm}
\end{table}


\begin{table}[t]
\caption{\rebuttal{Ablation on terminal reward in MT-BENCH 101 dataset.}}
\vspace{3mm}
\centering
\setlength\tabcolsep{4pt}
\renewcommand{\arraystretch}{1.5}
% \scalebox{0.5}{
\resizebox{1\textwidth}{!}{
% \small
\rebuttal{
\begin{tabular}{c|c|c|cc|cc|cc|cc|cc|cc}
\toprule
\multirow{3}{*}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{}} & \multicolumn{5}{|c}{\textbf{Perceptivity}} & \multicolumn{6}{|c}{\textbf{Adaptability}} & \multicolumn{2}{|c}{\textbf{Interactivity}} \\
 % & \multicolumn{1}{c}{\textbf{}} & \multicolumn{1}{|c}{\textbf{Memory}} & \multicolumn{2}{|c}{\textbf{Understanding}} & \multicolumn{2}{|c}{\textbf{Interference}} & \multicolumn{2}{|c}{\textbf{Rephrasing}} & \multicolumn{2}{|c}{\textbf{Reflection}} & \multicolumn{2}{|c}{\textbf{Reasoning}} & \multicolumn{2}{|c}{\textbf{Questioning}} \\
 & \multicolumn{1}{c}{\textbf{Avg.}} & \multicolumn{1}{|c}{\textbf{CM}}     & \multicolumn{1}{|c}{\textbf{SI}} & \multicolumn{1}{c}{\textbf{AR}} & \multicolumn{1}{|c}{\textbf{TS}} & \multicolumn{1}{c}{\textbf{CC}} & \multicolumn{1}{|c}{\textbf{CR}} & \multicolumn{1}{c}{\textbf{FR}} & \multicolumn{1}{|c}{\textbf{SC}} & \multicolumn{1}{c}{\textbf{SA}} & \multicolumn{1}{|c}{\textbf{MR}} & \multicolumn{1}{c}{\textbf{GR}} & \multicolumn{1}{|c}{\textbf{IC}} & \multicolumn{1}{c}{\textbf{PI}} \\
 \midrule
 % GPT4    &  \\
 %  \midrule
Base (Mistral-7B-Instruct)             &  6.223 & 
7.202 & 7.141 & 7.477 & 7.839 & 8.294 & 6.526 & 6.480 & 4.123 & 4.836 & 4.455 & 5.061 & 5.818 & 5.641                            \\ \midrule
\method{}  (intermediate reward)  &      6.630 & 7.624 & {7.846} & 8.085 & 8.398 & 8.947 & 7.105 & 7.286 & 4.208 & 4.993 & 4.377 & 5.264 & 6.179 & 5.873
\\
\midrule
\method{}  (terminal reward)  & 6.459 & 7.536 & 7.328 & 7.643 & 8.084 & 8.518 & 6.847 & 6.883 & 4.357 & 4.863 & 4.403 & 5.542 & 6.034 & 5.924 
\\ \midrule
\bottomrule
\end{tabular}}
}
\label{tab:compare_terminal}
\end{table}



\rebuttal{
\section{Motivation of considering intermediate reward}
\label{sec:motication_turnreward}
In this section, we elaborate on the motivation for considering intermediate rewards at each turn instead of only terminal rewards.
}

\rebuttal{
In multi-turn conversation tasks, such as MT-bench 101~\citep{bai2024mt}, the user asks questions $x_1$, $x_2$, $x_3$, and receives answers $a_1$, $a_2$, $a_3$. When $x_2$ is not closely related to $x_1$, aligning the first step using feedback among different $a_1$ is much more helpful than using the sequence $[a_1, x_2, a_2]$, where $x_2, a_2$ can be considered as noise.}

\rebuttal{
In mathematical reasoning tasks, as mentioned in~\citet{lai2024step}, some cases yield correct final answers but contain errors in intermediate reasoning steps. Consequently, \citet{lai2024step} filter out such samples using GPT-4. For example, consider a case where the reasoning steps yield a correct final answer but include an error: $[a_1^{\text{correct}}, a_2^{\text{wrong}}, a_3^{\text{correct}}]$, where $a_2^{\text{wrong}}$ is incorrect while all of the other steps and the final answer $a_3^{\text{correct}}$ is correct. When there is another response, $[a_1^{\text{correct}}, a_2^{\text{correct}}, a_3^{\text{correct}}]$ with all correct steps, using only terminal signal for aligning step 2 might not guarantee that $a_2^{\text{correct}} \succ  a_2^{\text{wrong}}$ because both of final answers are correct, especially when there is only an incorrect step among long reasoning steps. In contrast, an intermediate signal would clearly indicate $a_2^{\text{correct}} \succ  a_2^{\text{wrong}}$, accurately reflecting the quality of the intermediate steps.
% In mathematical reasoning tasks, as noted in [4], some cases yield correct final answers but contain errors in intermediate reasoning steps. Consequently, [4] filters out such samples using GPT-4.
%     For example, consider a case where the reasoning steps yield a correct final answer but include an error: $[a_1^{\text{correct}}, a_2^{\text{wrong}}, a_3^{\text{correct}}, a_4^{\text{correct}}]$, where $a_2^{\text{wrong}}$ is incorrect, but the final answer $a_4^{\text{correct}}$ is correct. Due to the randomness of LLM sampling, itâ€™s likely that another response, $[a_1^{\text{correct}}, a_2^{\text{correct}}, a_3^{\text{wrong}}, a_4^{\text{wrong}}]$, could also be generated, in which the wrong $a_3^{\text{wrong}}$ leads to an incorrect final answer $a_4^{\text{wrong}}$.
%     Thus, when aligning the second step, using a terminal signal based on the entire conversation could imply $s_2^{\text{wrong}} \succ  s_2^{\text{correct}}$. In contrast, an intermediate signal would clearly indicate $s_2^{\text{correct}} \succ  s_2^{\text{wrong}}$, accurately reflecting the quality of the intermediate steps.
In practice, if the final signal is important, e.g., in math reasoning task, then we can use only the terminal reward or the average of terminal reward and intermediate reward, otherwise one can just use the intermediate reward, which is cheaper to collect as compared to assigning reward until the terminal state.
}
