\section{Related work on two-player markov game \& optimistic online gradient descent}
Two-player Markov games have been widely studied since the seminal work **Littman, "Markov Games"**. Particularly relevant to our work is the research line on policy gradient algorithms for two-player Markov games such as **Bagnell, "Tree-based factored policies"**.
% \textbf{optimistic online gradient descent} 
Our oomdmethod{} is strictly related to the idea of optimistic online gradient descent **Hazan, "Optimistic Gradient Descent"** originally proposed in online learning to achieve small regret in case of slow varying loss sequences. Our update that uses only one projection per update was proposed in **Hazan, "Online Learning as Stochastic Approximation of Gauss-Newton Step"**. The name of our method is due to a similar algorithm introduced in the context of variational inequalities by **Facchinei, "Generalized Nash Equilibrium Problems with Non-monotone Function"**.
\rebuttal{