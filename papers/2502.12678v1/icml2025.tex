%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{booktabs} % for professional tables
\usepackage{dsfont}
\usepackage{enumitem}
\usepackage{makecell}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\input{math_commands.tex}

% Attempt to make hyperref and algorithmic work together better:
% \newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{algorithm}
% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

%%my
\usepackage{subfigure}
% \usepackage{subfig}
\usepackage{amsthm}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{assumption}{Assumption}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{claim}{Claim}
\newtheorem{definition}{Definition}
\newtheorem{hypothesis}{Hypothesis}
\newtheorem{example}[theorem]{Example}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
\providecommand{\rebuttal}[1]{\textcolor{black}{#1}}

%short:
\crefname{ineq}{inequ.}{inequ.}
\crefname{equation}{Eq.}{Eqs.}
\creflabelformat{ineq}{#2{\upshape(#1)}#3} 
\crefname{theorem}{Thm.}{Thm.}
\crefname{proposition}{Prop.}{Prop.}
\crefname{claim}{Claim}{Claims}
\crefname{algorithm}{Alg.}{Alg.}
\crefname{definition}{Def.}{Def.}
\crefname{lemma}{Lemma}{Lemmas}
\crefname{example}{Example}{Examples}
\crefname{appendix}{Appx.}{Appx.}
\crefname{figure}{Fig.}{Fig.}
\crefname{table}{Tab.}{Tab.}
\crefname{section}{Sec.}{Sec.}
\crefname{assumption}{Asm.}{Asm.}
\creflabelformat{ineq}{#2{\upshape(#1)}#3} 

\usepackage{bm}
\usepackage{multirow}
% \usepackage[dvipsnames]{xcolor}% colors
% \definecolor{ytcolor}{rgb}{1.0, 0.49, 0.0}
% \newcommand{\yongtao}[1]{\textcolor{ytcolor}{(yt: #1)}}
% \newcommand{\red}[1]{\textcolor{red}{#1}}
% \newcommand{\blue}[1]{\textcolor{blue}{#1}}
% \newcommand{\yihang}[1]{\textcolor{purple}{(Yihang: #1)}}
\usepackage{xcolor,colortbl}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\definecolor{tabblue}{HTML}{1F77B4}
\definecolor{taborange}{HTML}{FF7F0E}
\definecolor{tabred}{HTML}{D62728}
\definecolor{tabcyan}{HTML}{17BECF}
\definecolor{tabgreen}{HTML}{2CA02C}
%%%my






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \theoremstyle{plain}
% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{proposition}[theorem]{Proposition}
% \newtheorem{lemma}[theorem]{Lemma}
% \newtheorem{corollary}[theorem]{Corollary}
% \theoremstyle{definition}
% \newtheorem{definition}[theorem]{Definition}
% \newtheorem{assumption}[theorem]{Assumption}
% \theoremstyle{remark}
% \newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}
\newcommand\scalemath[2]{\scalebox{#1}{\mbox{\ensuremath{\displaystyle #2}}}}

\newcommand{\mytitle}{Multi-Step Alignment as Markov Games: \\ An Optimistic Online Gradient Descent Approach with Convergence Guarantees}
% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Multi-Step Alignment as Markov Games}

\begin{document}

\twocolumn[
% \icmltitle{Multi-Step Preference Optimization via Two-Player Markov Games}
% \icmltitle{Optimistic Online Gradient Descent for Markov Games in Multi-Step Alignment}

\icmltitle{\mytitle}

% \icmltitle{Multi-Step Alignment as Markov game: an Optimistic Online Gradient Descent Approach}
% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}
% \icmlsetsymbol{equalc}{'}

\begin{icmlauthorlist}
\icmlauthor{$\quad $ Yongtao Wu}{equal,yyy}
\icmlauthor{$\quad  $ Luca Viano}{equal,yyy}
\icmlauthor{$\quad  $ Yihang Chen}{comp}
\icmlauthor{$\quad  $ Zhenyu Zhu}{yyy}
\icmlauthor{$\quad  $ Kimon Antonakopoulos}{yyy}
\icmlauthor{$\quad  $ Quanquan Gu}{comp,equalc}
\icmlauthor{$\quad  $ Volkan Cevher}{yyy,equalc}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{EPFL}
\icmlaffiliation{comp}{UCLA}
\icmlaffiliation{equal}{Equal contribution}
\icmlaffiliation{equalc}{Equal mentorship}

\icmlcorrespondingauthor{}{yongtao.wu@epfl.ch}
% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention the equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
% \printAffiliationsAndNotice{}

\begin{abstract}
Reinforcement Learning from Human Feedback (RLHF) has been highly successful in aligning large language models with human preferences. While prevalent methods like DPO have demonstrated strong performance, they frame interactions with the language model as a bandit problem, which limits their applicability in real-world scenarios where multi-turn conversations are common. Additionally, DPO relies on the Bradley-Terry model assumption, which does not adequately capture the non-transitive nature of human preferences. In this paper, we address these challenges by modeling the alignment problem as a two-player constant-sum Markov game, where each player seeks to maximize their winning rate against the other across all steps of the conversation. Our approach Multi-step Preference Optimization (\method{}) is built upon the natural actor-critic framework~\citep{peters2008natural}. We further develop \oomdmethod{} based on the optimistic online gradient descent algorithm~\citep{rakhlin2013online,joulani17a}.
Theoretically, we provide a rigorous analysis for both algorithms on convergence and show that \oomdmethod{} requires $\mathcal{O}(\epsilon^{-1})$ policy updates to converge to an $\epsilon$-approximate Nash equilibrium. We also validate the effectiveness of our method on multi-turn conversations dataset and math reasoning dataset.
% in MT-bench-101.
\end{abstract}

\section{Introduction}
 
% There are several types of alignment problems in LLMs that can be cast as multi-step alignment: 
% \begin{itemize}
%     \item Autoregressive next token generation ~\cite{rafailov2024r,zengtoken}
%     \begin{itemize}
%         \item token-level
%         \item only preference data at the final step
%     \end{itemize}
%     \item   Reasoning (math), chain of thought ~\cite{hwang2024self,zhang2024chain,wang2024self}
%     \begin{itemize}
%         \item sentence-level
%         \item preference data at each step or at the final step.
%     \end{itemize}
%     \item Multi-round conversation~\cite{shani2024multi}
%     \begin{itemize}
%         \item sentence-level
%         \item preference data at each step or at the final step.
%     \end{itemize}
% \end{itemize}


In recent years, the integration of large language models (LLMs)~\citep{brown2020language,achiam2023gpt,team2023gemini,dubey2024llama} into various applications has highlighted the need for advanced preference alignment methods~\citep{ziegler2019fine,stiennon2020learning,bai2022training,ouyang2022training,rafailov2023direct}. As models increasingly engage in complex decision making or reasoning scenarios, e.g., GPT-4o and o1\footnote{https://openai.com/o1}, the ability to align their outputs with user preferences has received more attention. However, existing works on reinforcement learning from human feedback (RLHF) focus mostly on one-step preference~\citep{rafailov2023direct,meng2024simpo,munos2024nash,azar2024general,zhang2024iterative,wu2024self}, which neglects indispensable intermediate preferences within the answer and limits the model's alignment ability. 
For example, in multi-round conversations, alignment must occur at each turn to meet user needs. Similarly, in mathematical reasoning with chain-of-thought prompting, step-by-step validation is essential to ensure accuracy in the final result. The reliance on final-output feedback in most existing RLHF methods~\citep{wang2023rlhf,shani2024multi} neglects these intermediate steps, highlighting the need for multi-step preference optimization to enhance alignment capabilities.\looseness-1

% For example, the multi-round conversation requires preferences at multiple turns in the conversation to satisfy the user's need. 
% Moreover, the mathematical reasoning by chain-of-thought requires a step-by-step evaluation of correctness to achieve the final accuracy.
% The previous RLHF methods, which depend only on the final output, neglect the intermediate reasoning steps that lead to the correct output. Therefore, the multi-step preference optimization needs to be taken into account. 

Meanwhile, earlier alignment methods e.g., DPO and its variants step-DPO~\citep{lai2024step,lu2024step}, typically model the pairwise preference by the Bradley-Terry (BT) model~\citep{bradley1952rank}, which assigns a score for each answer based on its preference. This assumption of the model cannot capture the non-transitive preference, which is often observed in the averaged human preferences from the population~\citep{tversky1969intransitivity,gardner1970mathematical}. While a recent line of work has modeled the alignment process under the framework of general preference\rebuttal{~\citep{azar2024general,munos2024nash,wu2024self,rosset2024direct}}, and thus bypasses the BT model assumption, the challenge of multi-step preference optimization remains underexplored.

In this paper, we first address this gap by formulating multi-step general preference optimization within the framework of two-player Markov games~\citep{shapley1953stochastic}, where each player seeks to maximize their winning rate against the other across all steps of the conversation. Next, we introduce Multi-step Preference Optimization (\method{}) drawing on insights from the natural actor-critic framework~\citep{peters2008natural}. We further develop \oomdmethod{} which leverages the optimistic online gradient descent algorithm and benefits from improved theoretical guarantees~\citep{rakhlin2013online,joulani17a}.
Theoretically, we provide rigorous analysis for both algorithms on the convergence to Nash equilibrium. 
Empirically, we demonstrate the effectiveness of our approach through experiments on multi-turn conversation task and math reasoning task. We firmly believe that our framework and approach can enhance the responsiveness of LLMs to user feedback. Based on our discussions above, we summarize the contributions as follows:\looseness-1
\begin{itemize}[leftmargin=*]
    \item We formulate multi-step preference optimization as a two-player partially observable Markov game. Unlike \citet{wang2023rlhf,swamyminimaximalist,shani2024multi} who focus on the preference feedback at the final state, we assume that the preference signal is received at each step. Such feedback allows the model to better identify which steps are correct or erroneous, potentially enhancing learning efficiency and accuracy.
    \item We propose Multi-step Preference Optimization (\method{}) based on the natural actor-critic framework and Optimistic Multi-step Preference Optimization (\oomdmethod{}), built upon the optimistic online gradient descent. 
    Theoretically, we show that \oomdmethod{}
 requires $\mathcal{O}(\epsilon^{-1})$ policy updates to converge to an $\epsilon$-approximate Nash equilibrium, compared to $\mathcal{O}(\epsilon^{-2})$ by the algorithms provided in~\citet{wang2023rlhf,swamyminimaximalist,shani2024multi}.
   Our result cannot be trivially extended by~\citet{alacaoglu2022natural} due to the partially observable nature of Markov game. Interestingly, we bypass this difficulty by deriving our \oomdmethod{} that parameterizes the game over occupancy measures.\looseness=-1
    \item We provide practical implementations of both \method{} and \oomdmethod{} for LLM alignment. Numerical results demonstrate considerable improvements on multi-turn conversation datasets and math reasoning datasets.
    % Numerical results show that the proposed methods achieve considerable improvement on multi-turn conversation datasets and math reasoning datasets.
    % , compared to the multi-step variant of DPO.
\end{itemize}

% The remaining part of this paper is organized as follows: \cref{sec:relatedwork}{} provides a comprehensive review and discussion of related work. In \cref{sec:preliminary}, we 
% % give a brief preliminary on single-step RLHF, and 
% introduce the problem setting for the investigated multi-step RLHF. \cref{sec:nac} and \cref{sec:forb} introduce the proposed \method{} and \oomdmethod{} and provide a theoretical convergence analysis. Experimental results are present in \cref{sec:exp}. Conclusion, limitation, and future work are discussed in \cref{sec:conclusion}. 


\section{Related work}
\label{sec:relatedwork}
In this section, we present a brief summary of the related literature, while a more comprehensive discussion is deferred to \cref{sec:additional_relatedwork} due to the limited space.

\textbf{RLHF under Bradley-Terry model.}
Over the years, significant strides have been made towards developing RLHF algorithms from various perspectives under the Bradley-Terry (BT) model~\cite{bradley1952rank}. Earlier RLHF pipelines usually included supervised fine-tuning, learning a reward model, and reinforcement learning optimization with PPO~\citep{ziegler2019fine,stiennon2020learning,bai2022training,ouyang2022training}. Due to the instability and scaling issues of such a pipeline, direct alignment methods such as DPO have been proposed to bypass the training of the reward model~\citep{rafailov2023direct}. Several follow-up methods, such as generalized preference optimization~\citep{tang2024generalized}, use offline preference data to directly optimize pairwise preferences against a fixed opponent. A number of works have proposed reference-model-free method~\citep{meng2024simpo,hong2024orpo}. In \citet{meng2024simpo}, the impact of sequence length is mitigated by averaging the likelihood over the length of the sequence.
In the multi-step scenario, several multi-step variants of DPO are introduced in the math reasoning task. \citet{lu2024step} initiate from an intermediate step in a correct reasoning process and increase the temperature to produce a flawed reasoning path leading to an incorrect answer. Meanwhile, \citet{lai2024step} leverage GPT-4 to detect the first incorrect step in a multi-step reasoning trajectory, then regenerate from that point to obtain the correct path. Together, these serve as the pair of samples for DPO.

% Given the fact that only preference feedback can be directly collected in training the language models, recent work has attempted to optimize for preference feedback without explicitly learning a reward function. 
% Direct Preference Optimization (DPO, \citet{rafailov2023direct}) models the pairwise preference by the Bradley-Terry model~\cite{bradley1952rank}, which measures how well a specific reward function aligns with the preference feedback. Several follow-ups, such as Identity Preference Optimization (IPO, \citealt{azar2024general}) and Generalized
% Preference Optimization (GPO, \citealt{tang2024generalized}) use offline preference data to directly optimize pairwise preference against a fixed opponent.  


\textbf{RLHF under general preferences.}
The reward model in the BT model inherently implies transitivity in preferences. However, human preferences, especially the resulting averaged human preferences from populations, are usually nontransitive~\citep{tversky1969intransitivity,gardner1970mathematical}. To this end, \citet{azar2024general} outline a general framework for RLHF starting from general preference optimization and shows that DPO is a special case with the assumption of BT model. They further proposed IPO without such an assumption. Subsequently, \citet{munos2024nash} try to solve the alignment of non-transitive general preferences using two-player Nash learning in a bandit setting. In their work, preferences are regularized through KL divergence to a reference policy, and they prove the convergence of the last iterative. In \citet{swamyminimaximalist}, multi-step alignment is considered while preference signals are only applied at the final step. \citet{swamyminimaximalist} do not demonstrate the effectiveness of this framework in large language models.
\citet{wu2024self} propose SPPO, studying bandit alignment under general preferences. They introduce a novel loss function that increases the log-likelihood of the selected response while decreasing that of the rejected response, in contrast to DPO. 
% \yongtao{\citet{rosset2024direct}}
\citet{rosset2024direct} start with the Nash learning framework and propose Online DPO, which is an iterative version of DPO. 
\citet{wang2023rlhf} provide theoretical analysis on multi-step RLHF under general preference while practice application is not explored. In \citet{wang2023rlhf}, the preference signal is given for the entire trajectory of an MDP while in this paper it is step-wise. 
\citet{shani2024multi} study multi-step alignment under general preferences. However, unlike their approach where only preferences at the final states are considered, our work is built on a two-player Markov game which assumes that human preference is received at each step. 
% rather than only at the final step. 
Additionally, we leverage the optimistic online gradient descent to achieve a better convergence rate than \citet{wang2023rlhf,shani2024multi}, and utilize Monte Carlo estimation with a small-scale pairwise reward model, avoiding the need for an additional function approximator for the critic network. \looseness-1



% In this paper, we consider the second type with per-step preference data. The most closed work ~\cite{shani2024multi} has studied multi-step alignment for the third type with final step general preference. Specifically, they would like to  identify the Nash equilibrium (or von Neumann winner) of the following two-player constant-sum game 
% \begin{equation}
% \begin{split}
%      (\pi^*, \pi^*)
%     = &
%     \arg \max_{\pi}\min_{\pi'}
%     \mathbb{E}_{s_1 \sim \initial}
%      \mathbb{P}(s_{H+1} \succ s_{H+1}^\prime)
% \end{split}
% \label{minmax:lastpreference}
% \end{equation}
% \section{Problem setting}
\section{Problem setting: Multi-step RLHF as two-player Markov games}
\label{sec:preliminary}
\subsection{Notation}
We define the prompt to the language model as $x$ and the answer from the language model as $a$. For a multi-turn conversation with turn $H$, the prompts and the answers are denoted by $x_h \text{ and } a_h, \forall h \in [H]$.
% a sequence $(x_1,\dots,x_H)$
% and the answers are denoted by a sequence $(a_1,\dots,a_H)$.
The concatenation of a prompt $x$ and an answer $a$ is denoted by $[x,a]$ and can be generalized to the concatenation of multiple prompts and answers, e.g., $[x_1,a_1,\dots,x_{H},a_{H}]$. 
For any two sentences, e.g., $[x,a]$ and $[x^\prime,a^\prime]$, we define a preference oracle as $o([x,a] \rebuttal{\succ} [x^\prime,a^\prime]) \in \{0,1\}$, which can provide preference feedback with 0-1 scores, where 1 means the conversation $[x,a]$ is preferred and 0 otherwise.
We denote $\mathbb{P}([x,a]\succ [x^\prime,a^\prime]) = \mathbb{E}[o([x,a] \succ [x^\prime,a^\prime])]$ as the probability that the conversation $[x,a]$ is preferred over $[x^\prime,a^\prime]$. Moreover, we have $\mathbb{P}([x,a]\succ[x^\prime,a^\prime]) = 1 - \mathbb{P}([x^\prime,a^\prime]\succ [x,a])$.
An autoregressive language model is denoted by $\pi(a|x)$ which receives input $x$ and generates answer $a$. We denote the KL divergence of two probability distributions $p$ and $q$ by $D(p \| q)$. The Bregman Divergences between two points are denoted by $\mathbb{D}(p \| q)$.
The sigmoid function is defined by $\sigma(z):=\frac{1}{1+e^{-z}}$. Detailed definitions for the notations are summarized in \cref{sec:appendix_symbol}.\looseness=-1
% The concatenation of multiple sentences $a_1 \dots a_h$ is denoted by $[a_1,\dots,a_h]$.
% Define $x$ as the input prompt. Define $[a_1,\cdots,a_h]$ as the model's output, where each $a_h$ corresponds to a reasoning step. 

% \subsection{Definition of value function $V$, action-value function $Q$, Preference oracle $\mathbb{P}$, and reward $r$}

\subsection{Problem formulation of multi-step RLHF}
In this section, we introduce the problem setting for multi-step RLHF and we defer the preliminaries on single-step RLHF to \cref{prelimi:singlestep}. Specifically, we can cast the multi-step alignment process as a finite-horizon Markov Decision Process (MDP). We define $s_h = [x_1,a_1,\dots,x_{h-1},a_{h-1},x_{h}]$ as the state at $h> 1$.
We define the action $a_h$ as the answer given $s_h$. Particularly, we have $s_1= x_1$. The prompt in the next state is sampled under the transition $x_{h+1} \sim f(\cdot|s_{h},a_{h})$, which is equivalent to $s_{h+1} \sim f(\cdot|s_{h},a_{h})$. The equivalence comes from the fact $s_{h+1} =[s_{h},a_{h},x_{h+1}]$ by using the concatenation operator between sentences.
The terminal state is $s_{H+1}$. Our setting covers a number of alignment problems, and we list some examples below.
\begin{example}[Single-step alignment]
\label{example:1}
In single-step alignment, a language model receives one prompt and outputs one answer. Our framework covers the single-step alignment by dissecting the answer into single tokens. Specifically, we set $x_1$ as the prompt, $x_2,\dots,x_{H+1}$ as empty sentences, and the answer $a_h$ at each turn consists of only one token. Then the horizon $H$ is the number of tokens in the answer. The transition between each state is deterministic. 
\end{example}

\begin{example}[Chain-of-thought reasoning alignment] In the chain-of-thought reasoning, the horizon $H$ denotes the number of reasoning steps, where $x_1$ is the initial prompt and $x_2,\dots,x_{H+1}$ are empty. 
Each $a_h$ corresponds to a reasoning step. The transition between each state is deterministic.
\end{example}

\begin{example}[Mutli-turn conversation alignment] In multi-turn conversation, the horizon $H$ denotes the total number of turns in the conversation. In the $h$-th turn, $x_h$ is the prompt, and $a_h$ is the answer. The prompt in the terminal state, $x_{H+1}$, is an empty sentence.
The transition between each state can be deterministic or stochastic.
\end{example}


% We consider the second type with general preference per-step. 
 Next, we define the pair-wise reward function of two state-action pairs as the preference of two trajectories:
$$ r(s_h,a_h,s_h',a_h') = \mathbb{P}([s_h,a_h] \succ [s_h^\prime,a_h^\prime])\,.$$
% = \sigma(r([s_h,a_h],[s_h^\prime,a_h^\prime]))$$
% where $r$ is a \textit{pair-wise} reward function that will assign a high value if $[s_h,a_h]$ is better than $[s_h^\prime,a_h^\prime]$.
% Additionally, the preference for two actions in a state can be expressed based on the definition above:
% $$\mathbb{P}(a_h,a_h^\prime|s_h) = \mathbb{P}([s_h,a_h],[s_h,a_h^\prime]) =  \sigma(r([s_h,a_h],[s_h,a_h^\prime])).$$
Upon this point, we can define the MDP as a tuple $\mathcal{M}= (\mathcal{S},\mathcal{A},f,r,\initial,H)$, where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, $H$ is the horizon (total steps),  the initial state distribution $\initial$ is a distribution over the initial prompt $x_1$. Note that in a two-player game environment, each state in $\mathcal{S}$ is a pair of $s_h$ and $s_h^\prime$ generated by two policies.
Our goal is to identify the Nash equilibrium (or von Neumann winner) of the following two-player constant-sum Markov game: 
\begin{equation}
\begin{split}
\scalemath{0.85}{
     (\pi^*, \pi^*)
    = 
    \arg \max_{\pi}\min_{\pi'}
    \mathbb{E}_{s_1 \sim \initial, s_h,a_h, s_h^\prime, a_h^\prime}
    \Big[ 
    % \mathbb{E}
    % _{(s_h,a_h) \sim \pi, (s_h^\prime,a_h^\prime) \sim \pi'}
    % \big[
    \sum_{h=1}^{H} r(s_h,a_h, s_h^\prime,a_h^\prime)
    % \big]
    \Big],
}
\end{split}
\label{minmax:eachpreference} \tag{Game}
\end{equation}
where  $s_1=s_1^\prime=x_1, a_h \sim \pi(\cdot|s_{h})$, $a_h^\prime \sim \pi^\prime(\cdot|s_{h}^\prime)$, $s_h \sim f(\cdot|s_{h-1},a_{h-1})$, $s_h^\prime \sim f(\cdot|s_{h-1}^\prime,a_{h-1}^\prime)$. 

Here we make a few remarks on the benefit of incorporating human preferences at each step. \rebuttal{More detail on the motivation can be found at \cref{sec:motication_turnreward}.}
\begin{remark}
% Let $s_1$ denote the question particularly those framed with chain-of-though prompting, i.e., ``answer this question step by step''. 
% If two answers of length $H$ ($s_{H+1}$ and $s_{H+1}^\prime$) are globally similar but differ in the early reasoning steps (e.g., $s_1$ and $s_2$ are better than $s_1^\prime$ and $s_2^\prime$),
If two conversations of  $H$ turns, $s_{H+1}$ and $s_{H+1}^\prime$, are globally similar but differ in the early turns (e.g., $s_2$ are better than $s_2^\prime$),
more credit should be assigned to $s_{H+1}$, encouraging the model to align with it. This follows the principle that humans typically master simpler and earlier tasks before progressing to more complex ones. 
\end{remark}

\begin{remark} From a practical standpoint, including per-step preference data generates a richer dataset for training, helping the model learn which reasoning steps are correct or wrong. This incremental feedback can enhance overall performance by reinforcing the importance of foundational steps in reasoning.
\end{remark}

Next, we present some additional notation. We define the \textit{pair-wise} value function as follows
\begin{equation*}
\begin{split}
& V_h^{\pi,\pi^\prime}(s,s^\prime) = 
\mathbb{E}
% _{\pi,\pi^\prime,f}
% {\substack{(a_h,\dots,s_H,a_H) \sim \pi \\  
% (a_h^\prime,\dots, s_{H}^\prime, a_H^\prime) \sim \pi^\prime }}
% \left(
\Big[ 
\sum_{\hat{h}=h}^{H} r(s_{\hat{h}},a_{\hat{h}},
s_{\hat{h}}^\prime,a_{\hat{h}}^\prime)
% \right)
| s_h =s, s^\prime_h = s'\Big]
\,,
\end{split}
\end{equation*}
where  $a_{\hat{h}} \sim \pi_{\hat{h}}(\cdot|s_{\hat{h}})$, $a_{\hat{h}}^\prime \sim \pi_{\hat{h}}^\prime(\cdot|s_{\hat{h}}^\prime)$, $s_{\hat{h}+1} \sim f(\cdot|s_{\hat{h}},a_{\hat{h}})$, and $s_{\hat{h}+1}^\prime \sim f(\cdot|s_{\hat{h}}^\prime,a_{\hat{h}}^\prime)$. We will often denote $V^{\pi,\pi'}_1$ omitting the subscript, i.e., as $V^{\pi,\pi'}$. Moreover, notice that we consider potentially non stationary policies, i.e. they are indexed by $h$. We denote by $\pi$ the non stationary policy and by $\pi_h$ the distribution over actions at step $h$ corresponding to the non stationary policy $\pi$. 
\looseness=-1

We define the \textit{pair-wise} Q-function as follows:
\vspace{-2mm}
\begin{equation*}
\begin{split}
\scalemath{0.90}{
Q^{\pi,\pi^\prime}_h(s,a,s^\prime,a^\prime) = 
r(s,a,s^\prime,a^\prime)
+
\mathbb{E}
\Big[ 
\sum_{\hat{h}=h+1}^{H}  r(s_{\hat{h}},a_{\hat{h}},s_{\hat{h}}^\prime,a_{\hat{h}}^\prime)
% \right)
\Big]
\,,
}
\end{split}
\end{equation*}
where $s_{\hat{h}+1} \sim f(\cdot|s_{\hat{h}},a_{\hat{h}})$ and $s'_{\hat{h}+1} \sim f(\cdot|s^\prime_{\hat{h}},a^\prime_{\hat{h}})$.
% \begin{equation*}
% V^{\pi,\pi^\prime}(s_h,s_h^\prime) = 
% \begin{cases}
% \mathbb{E}_{a_h \sim \pi(\cdot|s_h), a_h^\prime \sim \pi^\prime(\cdot|s_h^\prime))}( \mathbb{P}([s_h,a_h],[s_h^\prime,a_h^\prime]) + \gamma V^{\pi,\pi^\prime}([s_h,a_h],[s_h^\prime,a_h^\prime])), &h < H+1\\
% \mathbb{P}(s_{H+1},s_{H+1}^\prime), & h = H+1
% \end{cases}
% \end{equation*}
% There are two special cases that we are interested in. One is $V^{\pi,\pi}(s_h,s_h^\prime)$, and the interpretation of this value function is whether starting from $s_h$ will result in better preference in the future than starting from $s_h^\prime$. Another one is $V^{\pi,\pi^\prime}(s_h,s_h)$, indicating whether using $\pi$ can receive more preference than $\pi^\prime$ when starting from the same state $s_h$.
% We define the Bellman
% operator Bh for any function V : S â†’ R as
\begin{lemma} (\rebuttal{Adapted from \cite{Puterman:1994}})
 The pair-wise value function and pair-wise Q-value function satisfy the  Bellman equation, i.e., for all $h \in [H]$: 
 $Q_h^{\pi,\pi^\prime}(s,a,s^\prime,a^\prime) 
= r(s,a,s^\prime,a^\prime) +   \mathbb{E}_{\hat{s}\sim f(\cdot|s,a),\bar{s}\sim f(\cdot|s^\prime,a^\prime)} [V_{h+1}^{\pi,\pi^\prime}(\hat{s},\bar{s})]\,$ and 
$
 V_h^{\pi,\pi^\prime}(s,s^\prime) = \mathbb{E}_{a \sim \pi_h(\cdot|s), a^\prime \sim \pi_h^\prime(\cdot|s^\prime)} 
Q_h^{\pi,\pi^\prime}(s,a,s^\prime,a^\prime).
$
% $$
% \scalemath{0.8}{
% Q_h^{\pi,\pi^\prime}(s,a,s^\prime,a^\prime) 
% = r(s,a,s^\prime,a^\prime) +   \mathbb{E}_{\hat{s}\sim f(\cdot|s,a),\bar{s}\sim f(\cdot|s^\prime,a^\prime)} [V_{h+1}^{\pi,\pi^\prime}(\hat{s},\bar{s})]\,.
% }
% $$
% $$
% \scalemath{0.85}{
% V_h^{\pi,\pi^\prime}(s,s^\prime) = \mathbb{E}_{a \sim \pi_h(\cdot|s), a^\prime \sim \pi_h^\prime(\cdot|s^\prime)} 
% Q_h^{\pi,\pi^\prime}(s,a,s^\prime,a^\prime).
% }
% $$   
\label{lemma:bellman}
\end{lemma}
By \cref{lemma:bellman}, we can rewrite \ref{minmax:eachpreference} as follows:
\begin{equation}
\begin{split}
     (\pi^*, \pi^*)
    = &
    \arg \max_{\pi}\min_{\pi'}
    \mathbb{E}
    \Big[ 
    % \mathbb{E}
    % _{(s_h,a_h) \sim \pi, (s_h^\prime,a_h^\prime) \sim \pi'}
    % \big[
    \sum_{h=1}^{H} r(s_h,a_h, s_h^\prime,a_h^\prime)
    % \big]
    \Big]
% \\ = & \arg \max_{\pi}\min_{\pi'}\mathbb{E}_{s_1 \sim \initial}
%     \mathbb{E}_{a_1 \sim \pi(\cdot|s_1), a_1^\prime \sim \pi^\prime(\cdot|s_1)}
%     Q^{\pi,\pi^\prime}(s_1,a_1,s_1,a_1^\prime) 
 \\     = &
     \arg \max_{\pi}\min_{\pi'}\mathbb{E}_{s_1 \sim \initial}
     V^{\pi,\pi^\prime}(s_1,s_1) \,.
\end{split}
    \label{equ:minmaxgame_0}
\end{equation}
Given the above notation, we can formalize our objective. We look for a policy $\pi$ satisfying the following definition of approximate equilibrium.
\begin{definition}[\textbf{$\epsilon$-approximate Nash equilibrium}]
    A policy $\pi$ is said to be an approximate Nash equilibrium if 
    it holds 
    that:
    % \looseness-1
    \begin{equation*}
\innerprod{\initial}{ V^{\pi,\pi}} - \min_{\bar{\pi}\in\Pi} \innerprod{\initial}{ V^{\pi, \bar{\pi}}} \leq \epsilon,
\end{equation*}
and
\begin{equation*}
\max_{\bar{\pi}\in\Pi}\innerprod{\initial}{ V^{\bar{\pi}, \pi}} - \innerprod{\initial}{ V^{\pi,\pi}} \leq \epsilon.
\end{equation*}
\end{definition}

\begin{definition}[Occupancy measures]\label{def:occupancy}
    Given the policy $\pi$, the occupancy measure of $\pi$, is defined at stage $h$ as $d_h^\pi(s,a) = \mathrm{Pr}(s_h=s,a_h=a)$
    where $s_1=x_1\sim\initial, a_h\sim \pi_h(\cdot|s_h), s_h\sim f(\cdot|s_{h-1},a_{h-1})$.
    We also define $d_h^\pi(s,a)|{s}_1 = \mathrm{Pr}(s_h=s,a_h=a | s_1 = {s}_1)$ . 
    In addition, given the policies $\pi,\bar{\pi}$, the occupancy measure of $(\pi,\bar{\pi})$ at stage $h$ is defined as $d^{\pi, \bar{\pi}}_h(s,a,s^\prime,a^\prime) =  \mathrm{Pr}(s_h=s,a_h=a,s_h^\prime=s^\prime,a_h^\prime=a^\prime)$,
    where  $s_1=s_1^\prime=x_1\sim \initial, a_{h} \sim \pi(\cdot|s_{h})$, $a_{h}^\prime \sim \pi^\prime(\cdot|s_{h}^\prime)$, $s_h \sim f(\cdot|s_{h-1},a_{h-1})$, and $s_{h}^\prime \sim f(\cdot|s_{h-1}^\prime,a_{h-1}^\prime)$. 
\end{definition}
{\bf Remark}: The value function at the initial state can be represented as an inner product between the reward function and the occupancy measure, i.e., $V^{\pi,\bar{\pi}} = 
\sum^H_{h=1}\left\langle r, d_h^{\pi,\bar{\pi}}\right\rangle$.
Given the structure of the game where the sequences of sentences and answers are generated independently by the two agents given the initial state $s_1$, the occupancy measure at each step can be factorized as the product of the two agents occupancy measures given $s_1$. In particular, we have $d_h^{\pi,\bar{\pi}}(s,a,s',a' )| s_1 = d_h^{\pi}(s,a) | s_1 \cdot d_h^{\bar{\pi}}(s',a' )| s_1$ for all $h,s,a,s',a'$.
\if 0
\yihang{Does $d^{\pi,\bar{\pi}} = d^\pi\cdot d^{\bar{\pi}}$ holds? Used in Lemma 2. Admittedly, we have $\mathbb{P}(s_h=s,a_h=a,s_h^\prime=s^\prime,a_h^\prime=a^\prime) = \mathbb{P}(s_h=s,a_h=a) \mathbb{P}(s_h^\prime=s^\prime,a_h^\prime=a^\prime) $ by the independence.}\textcolor{blue}{Well spotted, we have a problem in the infinite horizon, I am changing the proofs for the finite horizon setting.}
\fi


\section{Method}
% Let us now introduce our method Multi-step Preference Optimization (\method{}). 
We first develop our method Multi-Step Preference Optimization ($\method{}$) based on the natural actor-critical framework~\citep{peters2008natural,alacaoglu2022natural} in \cref{sec:nac}. Next, we introduce Optimistic Multi-Step Preference Optimization, dubbed \oomdmethod{}, in \cref{sec:forb}. The framework is inspired by the idea of optimism used in online learning and in min-max optimization with improved theoretical guarantees~\citep{popov1980modification,chiang2012online,rakhlin2013online}. 
% This algorithm benefits from improved theoretical guarantees.\looseness-1
\subsection{\method{} with natural actor-critic}
\label{sec:nac}
This section presents our first method to find an approximate solution to \ref{minmax:eachpreference}. 
In order to find an $\epsilon$-approximate Nash equilibrium, the MPO method builds upon the next lemma which decomposes the difference of two value functions to the $Q$ function at each step. Lemma~\ref{lemma:valuediff} is the extension of \citet{kakade2002approximately} to the multi-agent setting where the dynamics are controlled independently by each player but the reward depends on the joint-state action tuple. In \citet{kakade2002approximately}, the $Q$ function is a function of only one state-action pair while in our setting the $Q$ function is based on two state-action pairs. 
 \begin{lemma}[Value difference lemma (\rebuttal{Adapted from \citet{kakade2002approximately}})]
 \label{lemma:valuediff}
\if 0
For any initial state $s_1 \in \initial$, we have:
\begin{equation}
    \begin{split}
        V^{\pi, \overline{\pi}}(s_1,s_1) - 
        V^{\pi^\prime, \overline{\pi}}(s_1,s_1)
        =
        \mathbb{E}
        _{\substack{(
        (\overline{s}_h,\overline{a}_h) \sim \overline{\pi}, f
        \\  
        s_h^\prime \sim \pi^\prime, f
        }} 
        \sum_{h=1}^H \langle \pi(\cdot|s_h^\prime)-\pi^\prime(\cdot|s_h^\prime), 
        Q^{\pi,\overline{\pi}}(s_h^\prime,\cdot,\overline{s}_h,\overline{a}_h)
        \rangle\,.
    \end{split}
\end{equation}
\fi
% For a finite horizon MDP with initial distribution $\initial$ it holds that:
%     $$\bm{W}^{[3]}_{(1)} = \bm{W}^{[3]}_{(1)}\bm{W}^{[3]}_{(1)}\bm{W}^{[3]}_{(1)}\bm{W}^{[3]}_{(1)}\bm{W}^{[3]}_{(1)}\bm{C} \bigg\{\bm{A} \odot \bigg[\bigg(\bm{A} \odot \Big\{\Big(\bm{A} \odot \bm{B}\Big) \bm{S}\Big\} \bigg)\bm{S}\bigg]\bigg\}^T$$
For a finite horizon MDP with initial distribution $\initial$ it holds that:
%  \begin{equation}
%     \label{eq:word}
%     \resizebox{0.45\textwidth}{!}{% 
%     $
%     \begin{split}
%         quality(w_{i}) = & \log(1 + \operatorname{similarity}(a,b)) + \log(1 + familiarity(w_{i})) \\ 
%         & + \log(1+familiarity(\min{(e,3)})) +
%         \log(1 + expectedness(w_i|w_{i-1})) \\
%         & - \log(1+tts\textnormal{-}experience(subject)) + 
%         quality(w_{i-1}) + \mathcal{A}% 
%     \end{split} 
%     $
% }
% \end{equation} 
\begin{equation*}
\resizebox{0.45\textwidth}{!}{% 
    $
    \begin{split}
% \scalemath{0.9}{
  &   \innerprod{\initial}{V^{\pi, \bar{\pi}} -V^{\pi^\prime, \bar{\pi}}} 
=\mathbb{E}_{s_1\sim\initial}\sum^H_{h=1}\mathbb{E}_{s \sim d_h^\pi |s_1} 
\bigg[ 
% \bs{
% \right.
\\ &
% \left.
\left\langle
\mathbb{E}_{s',a' \sim d_h^{\bar{\pi}}|s_1}
% \innerprod{
Q_h^{\pi', \bar{\pi}}(s,\cdot,s',a')
,
% }
{\pi_h(\cdot|s,s_1) - \pi_h'(\cdot|s,s_1)}
\right\rangle
\bigg]\,.
\end{split}
$
}
\end{equation*}
\end{lemma}
The proof can be found at \cref{proof:valuedifflemma}. 
In our setting, the initial state $s_1$ is a deterministic function of the state $s$ so we can remove $s_1$ from the conditioning in the policy\footnote{
This is motivated by practical LLM training, where system prompts such as ``user'' and ``assistant'' are inserted before every $x_h$ and $a_h$, respectively. As a result, one can infer a unique $s_1$ for every $s$.
The conditioning of the policy on the initial state might appear unusual at the first glance but it is in fact common in the setting of Contextual MDPs (see for example \cite{levy2023efficient}). Indeed, the initial state $s_1$ could be interpreted as a context and we optimize over policies that depend on both the initial context and the current state.
}. To highlight this fact we denote as $s_1(s)$ the only initial state that can lead to $s$.
By setting $\pi^\prime = \overline{\pi}=\pi^t$ in \cref{lemma:valuediff} and $\pi = \pi^\star$ and summing from $t=1$ to $T$ we obtain: \looseness-1
\begin{equation*}
    % \resizebox{0.45\textwidth}{!}{% 
    % $
\begin{split}
  &  \sum^T_{t=1}\innerprod{\initial}{V^{\pi^\star, \pi^t} -V^{\pi^t, \pi^t}}  =  \mathbb{E}_{s_1\sim\initial}\sum^H_{h=1}\sum^T_{t=1} \mathbb{E}_{s \sim d_h^{\pi^\star}|s_1}
  \\ & \bs{\innerprod{\mathbb{E}_{s^\prime,a^\prime \sim d_h^{\pi^t}|s_1}Q_h^{\pi^t, \pi^t}(s,\cdot,s',a')}{\pi_h^\star(\cdot|s) - \pi_h^t(\cdot|s)}}\,.
\end{split}
    % $
% }
\end{equation*}

% \begin{equation*}
% \begin{split}
% % \scalemath{0.85}{
%   &  \sum^T_{t=1}\innerprod{\initial}{V^{\pi^\star, \pi^t} -V^{\pi^t, \pi^t}}  = 
%   \\ &  \mathbb{E}_{s_1\sim\initial}\sum^H_{h=1}\sum^T_{t=1} \mathbb{E}_{s \sim d_h^{\pi^\star}|s_1}\bs{\innerprod{\mathbb{E}_{s^\prime,a^\prime \sim d_h^{\pi^t}|s_1}Q_h^{\pi^t, \pi^t}(s,\cdot,s',a')}{\pi_h^\star(\cdot|s) - \pi_h^t(\cdot|s)}}\,.
%     % }
% \end{split}
% \end{equation*}

Since the sum over $t$ commutes with the expectation, we see that we can decompose the global regret $\sum^T_{t=1}\innerprod{\initial}{V^{\pi^\star, \pi^t} -V^{\pi^t, \pi^t}}$ into a weighted sum of local regrets at each stage $h \in [H]$. 
% i.e., $\scalemath{0.9}{\mathbb{E}_{s \sim d_h^{\pi^\star}|s_1}\bs{\sum^T_{t=1} \innerprod{\mathbb{E}_{s^\prime,a^\prime \sim d_h^{\pi^t}|s_1}Q_h^{\pi^t, \pi^t}(s,\cdot,s^\prime,a^\prime)}{\pi_h^\star(\cdot|s) - \pi_h^t(\cdot|s)}}}$.
Therefore, we can control the global regret implementing at each state online mirror descent updates (\citealt{warmuth1997continuous}, \citealt[Chapter 6]{orabona2023onlinelearning}, \citealt{cesa2006prediction}), i.e., implementing the following update:
\vspace{-2mm}
\begin{equation*}
    \begin{split}
      &  \pi_h^{t+1}(\cdot|s) = \argmax_{\pi}
        \langle \pi(\cdot|s), 
         \mathbb{E}_{s^\prime, a^\prime \sim d_h^{\pi^t}|s_1(s)} 
          \\ &
          \qquad \qquad  \qquad 
         Q_h^{\pi^t,\pi^t}(s,\cdot,s',a')
        \rangle - \beta {D}(\pi(\cdot|s)||\pi_h^t(\cdot|s))\,,
    \end{split}
\vspace{-2mm}
\end{equation*}
where $\beta$ is a learning rate. The solution has the following form:
$
 \pi_h^{t+1}(a|s)
\propto  \pi_h^t(a|s) 
      \exp  \{ \beta \mathbb{E}_{s^\prime, a^\prime \sim d_h^{\pi^t}|s_1(s)} Q_h^{\pi^t,\pi^t}(s,a,s^\prime,a^\prime) \},$
 % \begin{equation}
 %     \begin{split}
 %     &
 %     \pi_h^{t+1}(a|s)
 %     \\ & \propto  \pi_h^t(a|s) 
 %      \exp\{{\beta \mathbb{E}_{s^\prime, a^\prime \sim d_h^{\pi^t}|s_1(s)} Q_h^{\pi^t,\pi^t}(s,a,s^\prime,a^\prime) }\},
 %     \label{eq:update}
 %     \end{split}
 %     \end{equation}
which corresponds to natural actor-critic \citep{peters2008natural} that utilizes a softmax-based method for updating policies. The number of policy updates needed by the ideal version of \method{} (see \cref{alg:mspo_theory}) can be bounded as follows and the proof can be found at \cref{proof:converge}.

\begin{algorithm}[t]
% \setcounter{ALC@unique}{0}
\caption{\method{} (Theoretical Version)}
\label{alg:mspo_theory}
\begin{algorithmic}[1]
    \STATE \textbf{input}:
    reference policy $\pi^1$,
    preference oracle $\mathbb{P}$,
    learning rate $\beta = \sqrt{\frac{\log{\underline{\pi}^{-1}}}{TH^2}}$, 
    % {\color{ForestGreen}{
    total iteration $T$ 
    % }}
    \FOR{$t=1,2,\dots, T $}
        \STATE 
        \hspace{1em}
         \vspace{-5mm}
        \begin{align*}
        \scalemath{0.8}{
            \pi_h^{t+1}(a|s)
            \propto 
            \pi_h^t(a|s)
            \exp\bs{\beta
        \mathbb{E}_{s^\prime,a^\prime \sim d_h^{\pi^t}|s_1(s)}Q_h^{\pi^t,\pi^t}(s,a,s^\prime,a^\prime)}
        }
        \end{align*}
        % \begin{align*}
        % \forall h \in [H], ~~~\forall s,a.
        % \end{align*}
        % }
    \ENDFOR
    \STATE \textbf{output}: $\bar{\pi}^T$ (s.t. $d_h^{\bar{\pi}^T} = \frac{1}{T}\sum^T_{t=1} d_h^{\pi^t} $, $~ \forall h \in [H]$.). 
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[t]
% \setcounter{ALC@unique}{0}
\caption{\method{} (Practical version)}
\label{alg:so}
\begin{algorithmic}[1]
    \STATE \textbf{input}:
    reference policy $\pi^1$,
    preference oracle $\mathbb{P}$,
    learning rate $\beta$, number of generated samples $K$, 
    % {\color{ForestGreen}{
    horizon $H$, total iteration $T$.
    % }}
    \FOR{$t=1,2,\dots, T $}
        \STATE Generates response by sampling $s_1^1 \sim \initial$ and $a^1_h \sim   \pi^t(\cdot|s_h^1)$ for $h$ $\in [H]$. 
        \STATE Clear the dataset buffer $\mathcal{D}_t$.
        \FOR{$
        {h=1,2,\dots, H}$}
        \STATE Set $s_{h}^{K} =,\dots,=s_{h}^{2}=s_{h}^{1} $.
        \STATE Generate $K-1$ conversations by sampling $a_{\hat{h}}^{2:K} \sim  \pi^t(\cdot|s_{\hat{h}}^{2:K})$ for $\hat{h}$ $\in [h,H]$. 
        \STATE Estimate $\mathbb{E}_{a_h^{k^\prime}} Q^{\pi^t,\pi^t}(s_h^1,a_h^{k},s_h^1,a_h^{k'}), \forall k,k^\prime \in [K]$ via \cref{equ:q_estimate} with query to $\mathbb{P}$.
        \STATE
        Fill out $\mathcal{D}_t$ with the following data pair  $
        % \mathcal{D}_t = 
        % \{
        \bigg\{
        (s_h^1, a_h^k, \mathbb{E}_{a_h^{k^\prime}}Q^{\pi^t,\pi^t}(s_h^1,a_h^k,s_h^1,a_h^{k^\prime}) 
        \bigg\}
        _{k\in[K]}, 
        $
        % add to  $\mathcal{D}_t$
        \label{line:select}
        \ENDFOR
        \STATE Optimize $\pi_{{t+1}}$ over $\mathcal{D}_t$ according to 
                % \scalebox{0.5}{
        % \begin{align*}
        % \scalemath{0.9}{
        $
            \pi^{t+1}
            \leftarrow 
            \argmin_{\pi}
            \mathbb{E}
            \bigg(
            \log \bigg(\frac{\pi(a_h^k|s_h^1)}{\pi^t(a_h^k|s_h^1)}\bigg)
            -
            \beta \bigg(\mathbb{E}_{a_h^{k^\prime}}Q^{\pi^t,\pi^t}(s_h^1,a_h^k,s_h^1,a_h^{k^\prime}) 
            -\frac{H-h+1}{2}
            \bigg)
            \bigg)^2.
            $
        %     }
        %     \label{eqn:opt}
        % \end{align*}
    \ENDFOR
    \STATE \textbf{output}: $\pi^{T+1}$
  \end{algorithmic}
\end{algorithm}

\begin{theorem} Consider~\cref{alg:mspo_theory} and assume that the reference policy is uniformly lower bounded by $\underline{\pi}$, then there exists a policy $\bar{\pi}^T$ such that
$d^{\bar{\pi}^T}_h = \frac{1}{T}\sum^T_{t=1} d^{\pi^t}_h,\forall h\in [H]$, and it holds that for $T = \frac{16 H^4 \log \underline{\pi}^{-1}}{ \epsilon^2}$ the policy pair $(\bar{\pi}^T, \bar{\pi}^T)$ is an $\epsilon$-approximate Nash equilibrium. Therefore,~\cref{alg:mspo_theory} outputs an $\epsilon$-approximate Nash equilibrium after $\frac{16 H^4 \log \underline{\pi}^{-1}}{ \epsilon^2}$ policy updates.
\label{thm:converge}
\end{theorem}
\begin{remark}
    The above result generalizes the $\mathcal{O}(H^2\epsilon^{-2})$ bound on the policy updates proven in \citet{swamyminimaximalist} in the setting of terminal-only reward. The additional $H^2$ factor in our theorem is due to considering rewards that are not terminal-only.  In \cref{thm:converge_fast} we show that~\cref{alg:forb} improves the number of policy updates needed to converge to an $\epsilon$-approximate Nash equilibrium to $\mathcal{O}(H \epsilon^{-1})$. 
\end{remark}
% \subsection{Convergence of MSPO}
\textbf{Practical relaxations.} For the above theorem, \method{} requires the access of the $Q$ function, which is unknown. Next, we are going to develop a practical algorithm to efficiently estimate the $Q$ function and implement \cref{alg:mspo_theory}. Equivalently, the update in \cref{alg:mspo_theory} can be written as
 \begin{equation}
     \begin{split}
     \scalemath{0.9}{
          \pi_h^{t+1}(a|s) = \frac{\pi_h^t(a|s) \exp\{{\beta \mathbb{E}_{s^\prime, a^\prime \sim d_h^{\pi^t}|s_1(s)} Q_h^{\pi^t,\pi^t}(s,a,s',a^\prime)}\}}{Z_h^t(s)}\,,
          }
     \end{split}
     \label{eq:update_equal}
     \end{equation}
where $Z_h^t(s)$ is the partition function. Next, we express \cref{eq:update_equal} as follows:
 \begin{equation*}
     \begin{split}
     \scalemath{0.85}{
          \log \frac{\pi_h^{t+1}(a|s)}{\pi_h^t(a|s)} = \beta \mathbb{E}_{s^\prime, a^\prime \sim d_h^{\pi^t}|s_1(s)} Q_h^{\pi^t,\pi^t}(s,a,s',a^\prime) - \log Z_h^t(s)\,.}
     \end{split}
     % \label{eq:rewrite}
     \end{equation*}
Next, following~\citet{wu2024self}, we approximate the equation above with an approximate solution of the following optimization program:
 \begin{align*}
 % \scalemath{0.85}{
      &    \pi^{t+1}
          % (a_h|s_h)
          = \argmin_{\pi} \sum^H_{h=1} \mathbb{E}          _{\substack{ s_1 \sim \initial \\
          (s_h,a_h) \sim d^{\pi^t}_h|s_1
          }}
          \bigg[
        \log   \frac{\pi(a_h|s_h)}{\pi_h^t(a_h|s_h)} 
      \\  & - 
        (\mathbb{E}_{s^\prime, a^\prime \sim d_h^{\pi^t}|s_1} Q_h^{\pi^t,\pi^t}(s_h,a_h,s',a^\prime) - \log Z_h^t(s_h))
          \bigg]^2\,.
          % }
     \end{align*}
 Unfortunately, solving the above minimization exactly is out of hope. The first difficulty is the efficient estimation of  $\mathbb{E}_{s',a' \sim d_h^{\pi^t}|s_1}Q_h^{\pi^t,\pi^t}(s_h,a_h,s',a')$. In particular, since $s^\prime$ and $s$ are sampled from the same distribution, we will sample $a^\prime$ from the state $s_h$ and use the Monte Carlo estimator:
 \vspace{-3mm}
 % \looseness-1
 \begin{equation}
   \begin{split}
&\mathbb{E}_{a'\sim \pi^t(\cdot|s_h)}Q_h^{\pi^t,\pi^t}(s_h,a_h,s_h,a')
\\ &\approx  \frac{1}{K} \sum_{k=1}^K \sum_{\hat{h} = h}^H  \mathbb{P}([s_{\hat{h},k},a_{\hat{h},k}],[s_{\hat{h},k}^\prime,a_{\hat{h},k}^\prime]) \,,
\label{equ:q_estimate}
\vspace{-5mm}
   \end{split} 
\end{equation}
where the sequences $\bc{(s_{\hat{h},k},a_{\hat{h},k}, s'_{\hat{h},k},a'_{\hat{h},k})}^H_{\hat{h}=h}$ for
% each $\hat{h}\in[H]$ and 
$k \in [K]$ are generated by rollouts of the policies pair $(\pi^t, \pi^t)$.
The second difficulty is $Z_h^t(s)$, which is difficult to compute for large action spaces. In all states $s$, we replace $\log Z_h^t(s)$ with $\beta\frac{H-h+1}{2}$.
Such heuristic is motivated by the following observation: If the preference between $a_h$ and $a_h^\prime$ in \cref{equ:q_estimate} results in a tie, then with such $\log Z_h^t(s)$, the solution of \cref{equ:q_estimate} is $\pi^{t+1} = \pi^t$, leaving the model unchanged.
% \end{remark}
In summary, we provide a practical version of \method{} in \cref{alg:so}. In practice, we used a stationary policy that we find to be sufficient to obtain convincing results.

 % \yongtao{The challenge of the first method is how to estimate ``$\mathbb{P}(a_h \succ \pi^t | s_h) $''? i.e., how to estimate the Q function (same problem in standard RL).  The second method does not have this problem since it is based on single-step preference (two-player game at each step?)}


% We subtract $\gamma V^{\pi,\pi^\prime}([s_h,a_h],[s_h^\prime,a_h^\prime])$ from $Q^{\pi^t,\pi^t}(s_h,a_h,s_h,a_h^\prime)$, then we will just use $\mathbb{P}([s_h,a_h],[s_h^\prime,a_h^\prime])$. 
% We define the winning probability of $a_h$ against a distribution of responses $\pi$ as follows:
% $$
% \mathcal{P}(a_h \succ \pi | s_h) = \mathbb{E}_{a_h^\prime \sim \pi(\cdot|s_h)} P([s_h,a_h],[s_h,a_h^\prime]).
% $$
% Use the following update:
% \begin{equation*}
%     \begin{split}
%         \pi^{t+1}(a_h|s_h) &\propto \pi^t(a_h|s_h) \exp\{{\beta \mathbb{P}(a_h>\pi^t | s_h)}\}
%         \\ &\propto \pi^t(a_h|s_h) \exp\{{\beta 
%       \mathbb{E}_{a_h^\prime \sim \pi^t(\cdot|s_h)} \mathbb{P}([s_h,a_h],[s_h,a_h^\prime])
%         }\} \,.
%     \end{split}
% \end{equation*}

 % \textbf{Method 2.}
 % Use two independent policy gradients~\citep{daskalakis2020independent}, and then use sampling to approximate Q (ReMAX)~\cite{li2024remax}


\begin{algorithm}[t]
% \setcounter{ALC@unique}{0}
\caption{\oomdmethod{} (Theory Version) 
}
\label{alg:forb}
\begin{algorithmic}[1]
    \STATE \textbf{input}:
    occupancy measure of reference policy $\pi^1$ denoted as $d^1$,
    preference oracle $\mathbb{P}$ (i.e. reward function $r$),
    learning rate $\beta$, Bregman divergence $\mathbb{D}$,
    % {\color{ForestGreen}{
 iteration $T$%, outer iteration $N$.
    % }}
    %\For{$n=1,2,\dots, N$}
    %     \State Initialize $\pi^{n,1}, \pi^{n,0}$ as the reference policy.
    \FOR{$t=1,2,\dots, T $}
        %\State Compute $Q^{\pi^n,\pi^n}(s,a,s',a')$ for all $s,a,s',a'$.
        % \State Update
                % \scalebox{0.5}{
       \STATE 
      \hspace{-1em}
         \vspace{-5mm}
       \begin{align*}
        % \scalemath{0.85}{
          &  d_h^{t+1} = \argmax_{d \in \mathcal{F}_{s_1}} 
            \beta \left\langle{d}, 
            2 \mathbb{E}_{s',a' \sim d_h^t}r(\cdot,\cdot,s',a')
            \right.
            \\ & 
            \left. \qquad  \qquad 
            - \mathbb{E}_{s',a' \sim d_h^{t-1}}r(\cdot,\cdot,s',a')
            \right \rangle - \mathbb{D}(d, d_h^t).
            % ~~~\forall h \in [H] ~~\forall s_1.
            % }
        \end{align*}
        % }
    \ENDFOR
    \STATE  $\pi_h^{\mathrm{out}}(a|s) = \frac{\bar{d}_h(s,a|s_1)}{\sum_a\bar{d_h}(s,a|s_1)}$ with $\bar{d}_h = T^{-1} \sum^T_{t=1} d_h^t$ for all $h \in [H]$ for the unique $s_1$ from which $s$ is reachable.
    \STATE \textbf{Output : } $\pi^{\mathrm{out}} $
  \end{algorithmic}
\end{algorithm}

\subsection{Optimistic \method{}: \oomdmethod{}}
\label{sec:forb}
In this section, we propose an alternative algorithm based on the optimistic gradient descent method
\footnote{The same update we use can also be seen as the Forward-Reflected-Backward (FoRB) update proposed in~\citet{malitsky2020forward} for variational inequalities. This point of view is taken by \citet{alacaoglu2022natural}  to solve zero-sum Markov game.} by reformulating the optimization problem over occupancy measures. Here, we show that optimistic online mirror descent with one projection \citep{joulani17a} with an appropriately chosen regularizer can be used to solve approximately the following program, which corresponds to \ref{minmax:eachpreference} lifted to the space of conditional occupancy measures.
% \vspace{-1mm}
\begin{align*}
(d^\star, d^\star) &= \argmax_{d\in\tilde{\mathcal{F}}} \min_{d'\in\tilde{\mathcal{F}}} \mathbb{E}_{s_1 \sim \initial}\sum^H_{h=1}\sum_{s,a,s',a'}  d_h(s,a | s_1) 
\\ & \qquad \qquad \qquad \qquad  \quad   r(s,a,s',a') d_h'(s',a'|s_1)\,,
\end{align*}
where $\tilde{\mathcal{F}}$ is the product set of the Bellman flow constraints for a particular initial state, i.e. $\tilde{\mathcal{F}} = \times_{s_1 \in \mathrm{supp}(\initial)}\mathcal{F}_{s_1}$. We also introduce the Bellman flow constraints for a specific initial state $\mathcal{F}_{s_1} = \bigg\{ d = (d_1, \dots, d_H) : \sum_a d_{h+1}(s,a) = \sum_{s',a'} f(s|s',a') d_h(s',a'), d_1(s)= \mathds{1}\bc{s=s_1} \bigg\}$. 
%We can find a solution of the above saddle point problem solving for every initial state, i.e. $(d^\star|s_1, d^\star|s_1) = \argmax_{d\in\mathcal{F}|s_1} \min_{d'\in\mathcal{F}|s_1} \sum^H_{h=1}\sum_{s,a,s',a'} d_h(s,a | s_1) r(s,a,s',a') d_h'(s',a'|s_1)$ where the sum now can be restricted to only the state action tuples reachable from the state $s_1$. We also introduced the Bellman flow constraints for a specific initial state $\mathcal{F}_{s_1} = \bc{d = (d_1, \dots, d_H) : \sum_a d_{h+1}(s,a) = \sum_{s',a'} f(s|s',a') d_h(s',a'), d_1(s)= \mathds{1}\bc{s=s_1}}$.
\if 0
Specifically, in 
 \oomdmethod{}, we replace the update in \cref{eq:update} with  
\begin{align*}
        \scalemath{0.85}{
      \pi^{t+1}(a_h|s_h)
     \propto  \pi^t(a_h|s_h) \exp
     \bs{
     \beta_t \left(2 \mathbb{E}_{a_h^\prime \sim \pi^t(\cdot|s_h)} Q^{\pi^t,\pi^t}(s_h,a_h,s_h,a_h^\prime) 
     - 
     \mathbb{E}_{a_h^\prime \sim \pi^t(\cdot|s_h)} Q^{\pi^{t-1},\pi^{t-1}}(s_h,a_h,s_h,a_h^\prime) 
     \right)
     },
}
\end{align*}
\fi
The policy pair $(\pi^\star, \pi^\star)$ solution of \ref{minmax:eachpreference} can be retrieved from the occupancy measure pair $(d^\star, d^\star)$ as $\pi^\star(a|s) = \frac{d^\star(s,a|s_1)}{\sum_a d^\star(s,a|s_1)}$. 
%We remark that since $s_1$ is a deterministic function of $s$ it can be removed from the conditioning of the policy. Therefore the optimal policy can be extracted also from the ocuupancy measure without conditioning on $s_1$, i.e. $\pi^\star(a|s) = \frac{\tilde{d^\star}(s,a|s_1)}{\sum_a \tilde{d^\star}(s,a|s_1)} = \frac{\tilde{d^\star}(s,a|s_1)\initial(s_1)}{\sum_a \tilde{d^\star}(s,a|s_1)\initial(s_1)} = \frac{d^\star(s,a)}{\sum_a d^\star(s,a)}$. We used that $d^\star(s,a) =\tilde{d^\star}(s,a|s_1)\initial(s_1)$ since there exists only one initial state for which $\tilde{d^\star}(s,a|s_1) \neq 0$. It follows that we can then look at this formulation over occupancy measures
%\begin{align*}
%(d^\star, d^\star) &= \argmax_{d\in\mathcal{F}} \min_{d'\in\mathcal{F}}\sum^H_{h=1}\sum_{s,a,s',a'} d_h(s,a) r(s,a,s',a') d_h'(s',a')
%\end{align*}
%where we used $\mathcal{F} = \bc{d = (d_1, \dots, d_H) : \sum_a d_{h+1}(s,a) = \sum_{s',a'} f(s|s',a') d_h(s',a'), d_1 = \initial}$\,.
Our idea is to apply the optimistic algorithm from \citet{joulani17a} to the reformulation of \ref{minmax:eachpreference} over occupancy measures. We present the resulting algorithm, i.e., \oomdmethod{}, in \cref{alg:forb}.
\begin{remark}
    In a partially observable Markov
game, lifting the problem to the occupancy measures turns out to be fundamentally important for enabling each agent to learn a policy conditioned only on their own state. This is different from the standard literature on Markov Games \citep{daskalakis2020independent,wei2021last,alacaoglu2022natural}, which assumes that both agents share a common state.
\end{remark}

As the next theorem shows, in the ideal case where the updates can be computed exactly, \cref{alg:forb} finds an $\epsilon$-approximate Nash equilibrium using fewer updates compared to \cref{alg:mspo_theory} and to \citet[Alg. 1]{swamyminimaximalist}. The proof can be found at \cref{proof:converge_fast}.
 \begin{theorem} [Convergence of \oomdmethod{}] Consider~\cref{alg:forb} and let us assume that the occupancy measure of the reference policy is uniformly lower bounded by $\underline{d}$. Moreover, let $\mathbb{D}$ be $1/\lambda$ strongly convex, i.e. $\mathbb{D}(p||q) \geq \frac{\norm{p-q}^2_1}{2\lambda}$. Then, by setting $T = \frac{10 H \log \underline{d}^{-1} }{\beta \epsilon}$ and $\beta \leq \frac{1}{\sqrt{2 \lambda}}$, we ensure that $(\pi^{\mathrm{out}}, \pi^\mathrm{out})$, i.e., the output of~\cref{alg:forb} is an $\epsilon$-approximate Nash equilibrium. Therefore, we need at most $\frac{10 H \log \underline{d}^{-1}  }{\beta \epsilon}$ policy updates.
\label{thm:converge_fast}
\end{theorem}
In addition, not only \citet[Alg. 1]{swamyminimaximalist} but also \oomdmethod{} can be implemented using only one player since in a constant sum game, the max and min player produce the same iterates. The result is formalized as follows and the proof is deferred to \cref{proof:same_updates}.

\begin{table*}[!ht]
\caption{Evaluation results on MT-bench-101 dataset. Mistral-7B-Instruct is selected as the base model. We can observe that both of the proposed algorithms \method{} and \oomdmethod{} considerably outperform the baseline in terms of the score (the higher the better).}
% \vspace{1mm}
\centering
\setlength\tabcolsep{4pt}
\renewcommand{\arraystretch}{1.5}
% \scalebox{0.5}{
\resizebox{1\textwidth}{!}{
% \small
\begin{tabular}{c|c|c|cc|cc|cc|cc|cc|cc}
\toprule
\multirow{3}{*}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{}} & \multicolumn{5}{|c}{\textbf{Perceptivity}} & \multicolumn{6}{|c}{\textbf{Adaptability}} & \multicolumn{2}{|c}{\textbf{Interactivity}} 
\\
 % & \multicolumn{1}{c}{\textbf{}} & \multicolumn{1}{|c}{\textbf{Memory}} & \multicolumn{2}{|c}{\textbf{Understanding}} & \multicolumn{2}{|c}{\textbf{Interference}} & \multicolumn{2}{|c}{\textbf{Rephrasing}} & \multicolumn{2}{|c}{\textbf{Reflection}} & \multicolumn{2}{|c}{\textbf{Reasoning}} & \multicolumn{2}{|c}{\textbf{Questioning}} \\
 & \multicolumn{1}{c}{\textbf{Avg.}} & \multicolumn{1}{|c}{\textbf{CM}}     & \multicolumn{1}{|c}{\textbf{SI}} & \multicolumn{1}{c}{\textbf{AR}} & \multicolumn{1}{|c}{\textbf{TS}} & \multicolumn{1}{c}{\textbf{CC}} & \multicolumn{1}{|c}{\textbf{CR}} & \multicolumn{1}{c}{\textbf{FR}} & \multicolumn{1}{|c}{\textbf{SC}} & \multicolumn{1}{c}{\textbf{SA}} & \multicolumn{1}{|c}{\textbf{MR}} & \multicolumn{1}{c}{\textbf{GR}} & \multicolumn{1}{|c}{\textbf{IC}} & \multicolumn{1}{c}{\textbf{PI}} \\
 \midrule
 % GPT4    &  \\
 %  \midrule
Base (Mistral-7B-Instruct)             &  6.223 & 
7.202 & 7.141 & 7.477 & 7.839 & 8.294 & 6.526 & 6.480 & 4.123 & 4.836 & 4.455 & 5.061 & 5.818 & 5.641                            \\  \midrule  DPO  (iter=1)                  &     6.361 & 7.889 & 6.483 & 7.699 & 8.149 & 8.973 & 7.098 & 7.423 & 3.448 & \textbf{6.123} & 3.421 & 4.492 & 5.639 & 5.858    
\\  
 DPO  (iter=2)    &  6.327 & 7.611 & 6.206 & 8.106 & 8.052 & 9.111 & 6.670 & 7.153 & 3.494 & 5.884 & 3.360 & 4.691 & 5.837 & 6.078  \\
DPO  (iter=3)    & 5.391 & 6.019 & 4.521 & 6.890 & 6.631 & 8.177 & 5.437 & 5.723 & 3.448 & 5.295 & 3.142 & 4.015 & 5.256 & 5.529  \\ 
SPPO  (iter=1)                  & 6.475 & 7.432 & 7.464 & 7.714 & 8.353 & 8.580 & 6.917 & 6.714 & 4.136 & 5.055 & 4.403 & 5.400 & 6.036 & 5.966     \\  SPPO  (iter=2)      &6.541 & 7.516 & 7.496 & 7.808 & 8.313 & 8.731 & 7.077 & 6.867 & 4.136 & 5.281 & 4.488 & 5.477 & 6.098 & 5.751   \\  
 SPPO  (iter=3)      &6.577 & 7.575 & 7.547 & 7.944 & 8.365 & 8.797 & 7.040 & 6.865 & 4.442 & 5.185 & 4.346 & 5.394 & 6.092 & 5.906  \\  
Step-DPO   (iter=1)                 &    6.433 & 7.463 & 7.054 & 7.790 & 8.157 & 8.593 & 6.827 & 6.748 & 4.234 & 4.849 & 4.236 & 5.519 & 5.982 & 6.171                          \\  
Step-DPO   (iter=2)     &           6.553 & 7.616 & 7.043 & 7.925 & 8.147 & 8.662 & 6.790 & 6.878 & 4.331 & 5.048 & 4.366 & \textbf{5.734} & \textbf{6.391} & 6.254 \\ 
Step-DPO   (iter=3)     &6.442 & 7.665 & 7.023 & 7.767 & 8.016 & 8.589 & 6.723 & 6.581 & 4.305 & 5.014 & 4.153 & 5.453 & 6.202 & \textbf{6.257} \\
\midrule
% \textbf{MSPO} (iter=1)             &   6.494 & 7.566 & 7.247 & 7.712 & 8.016 & 8.601 & 6.869 & 6.624 & 4.519 & 5.110 & 4.443 & 5.447 & 6.138 & 6.135     
%   \\   
% \textbf{MSPO}  (iter=2)               &   6.576 & 7.590 & 7.318 & 7.836 & 8.418 & 8.724 & 6.947 & 6.914 & 4.390 & 4.925 & 4.326 & 5.735 & 6.178 & 6.194                           \\ 
% \textbf{MSPO}  (iter=3)    
% &  6.606 & 7.690 & 7.465 & 8.003 & 8.482 & 8.888 & 7.000 & 6.973 & 4.325 & 4.555 & 4.600 & 5.631 & 6.238 & 6.032 \\
\method{} (iter=1)     &      6.630 & 7.624 & \textbf{7.846} & 8.085 & 8.398 & 8.947 & 7.105 & 7.286 & 4.208 & 4.993 & 4.377 & 5.264 & 6.179 & 5.873 \\
\method{}  (iter=2)               &   6.735 & 7.838 & 7.723 & 8.196 & \textbf{8.590} & 9.027 & 7.347 & 7.209 & 4.240 & 5.137 & 4.469 & 5.531 & 6.181 & 6.061                       \\ 
\method{}  (iter=3)    &
6.733 & \textbf{7.868} & 7.686 & \textbf{8.289} & 8.510 & 9.078 & 7.330 & 7.529 & \textbf{4.461} & 4.829 & 4.225 & 5.366 & 6.198 & 6.155
\\
\oomdmethod  (iter=2)    &6.736 & 7.733 & 7.723 & 8.257 & 8.478 & 9.122 & 7.300 & 7.421 & 4.123 & 5.288 & \textbf{4.506} & 5.513 & 6.179 & 5.923\\
\oomdmethod  (iter=3) &  \textbf{6.776} & 7.649 & 7.792 & 8.281 & 8.578 & \textbf{9.136} & \textbf{7.424} & \textbf{7.635} & 4.377 & 5.308 & 4.312 & 5.455 & 6.187 & 5.954\\
\bottomrule
\end{tabular}}
\label{tab:mtbench}
\end{table*}

\begin{theorem}\label{thm:same_updates}
Consider a constant sum two-player Markov games with reward such that $r(s,a,s',a') = 1 - r(s',a',s,a)$, then for each $s_1 \in \mathrm{supp}(\initial)$ the updates for $d$ in \cref{alg:forb} coincides with the updates for the min player that uses the updates
\vspace{-2mm}
       \begin{align*}
        % \scalemath{0.85}{
          &  d_h^{t+1} = \argmax_{d \in \mathcal{F}_{s_1}} 
            \beta \left\langle{d}, 
            2 \mathbb{E}_{s',a' \sim d_h^t}r(\cdot,\cdot,s',a')
            \right.
            \\ & 
            \left. \qquad  \qquad 
            - \mathbb{E}_{s',a' \sim d_h^{t-1}}r(\cdot,\cdot,s',a')
            \right \rangle - \mathbb{D}(d, d_h^t).
            % }
        \end{align*}
    % \begin{align*}
    %     % \scalemath{0.95}{
    % &  d_h^{t+1}(a|s) =   \argmin_{d \in \mathcal{F}_{s_1}} 
    %         \beta \left \langle {d} 
    %         ,
    %         % {
    %         2 \mathbb{E}_{s',a' \sim d^t_h}r(s',a',\cdot,\cdot) 
    %         \right. \\ &
    %         \left. - \mathbb{E}_{s',a' \sim d_h^{t-1}}r(s',a',\cdot,\cdot)
    %         \right \rangle
    %         + \mathbb{D}(d, d_h^t) \,.
    %         % }
    %                 \end{align*}
\end{theorem}
\vspace{-3mm}
Furthermore, we can avoid the projection over the set $\mathcal{F}$ implementing this update on the policy space (see  Appendix~\ref{app:implement_forb}). We achieve such results following the techniques developed in \citet{bas2021logistic,viano2022proximal}.

For the first iteration, we initialize $d_h^0$ to be equal to $d_h^1$ for all $h$. That is, at the first iteration, we use the same update rule as in \method{}. After the first iteration, we apply similar techniques as in $\method$ by estimating the $Q$ function and we use a tunable parameter to approximate the $\log Z $ term. We illustrate the practical algorithm in \cref{alg:forbprac}.




\section{Experiments}
In this section, we provide several numerical results while additional detail on the dataset, experimental set-up, and ablation studies are deferred to \cref{sec:addexp}.
\label{sec:exp}
\subsection{\rebuttal{Tabular experiment}}
\rebuttal{
% The setting of our large-scale experiments does not match the assumptions under which \Cref{thm:converge_fast} is proven. In particular, in the large-scale experiments the state action value functions can not be computed exactly.
First, we consider a synthetic experiment in which the state action functions can be computed exactly for both \oomdmethod{} and \method{}. We generate $10$ random gridworlds with a number of states and actions sample uniformly from the intervals $[1,100]$ and $[2,10]$. We plot the exploitability computed as $\innerprod{\initial}{\max_{\pi} V^{\pi, \pi^k} - V^{\pi^k\pi^k}},$ which is a standard metric to evaluate the distance from a Nash equilibrium. In particular, when $(\pi^k,\pi^k)$ is a Nash equilibrium, the exploitability is $0$. We can see that \oomdmethod{} achieves very low exploitability after $100$ updates while $2000$ updates are needed by \method{}. In this case, where the $Q$ functions can be computed exactly, we can appreciate the faster convergence rate of \oomdmethod{} as described by \Cref{thm:converge_fast}.
} 


% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.3\textwidth]{figures/exploitability.pdf}
%      \label{subfig:exploitability}
%     \vspace{-5mm}
% \caption{\rebuttal{Results in the tabular experiments. Curves are averages across $10$ different randomly generated environments. The error bars report one standard deviation.}}
% \end{figure}
\begin{figure*}
    \centering
    \subfigure[Results in the tabular experiments.]{\includegraphics[width=0.29\textwidth]{figures/exploitability_grid2.pdf}
    }
    \subfigure[Radar chart on different categories.]{\includegraphics[width=0.38\textwidth]{figures/radar_crop.pdf}
        \label{subfig:radar}
    }
    \subfigure[Winning rate against the base model.]{
\includegraphics[width=0.28\textwidth]{figures/fig_winning_rate_comparison.pdf}
     \label{subfig:winrate}
    }
    \vspace{-4mm}
\caption{(a): Results in the tabular experiments. Curves are averages across $10$ different randomly generated environments. The error bars report one standard deviation. (b): Result of \oomdmethod{} on the MT-bench-101 dataset;
% , showing that applying \oomdmethod{} leads to improvements across various categories. \textbf{Right}: 
(c) Winning rate against the base model with different approximations for the $Q$ functions in \method{}.
\rebuttal{When optimizing $a_h$ at the $h$ step, only considering the preference of $s_{h}$ is sufficient compared to using $s_{h},\dots,s_{H+1}$.}}
\end{figure*}

% \begin{figure*}[!tbh]
%     \subfloat[Convergence curve.]{\label{f1}\includegraphics[width=0.3\textwidth]{figures/radar.pdf}}
%     % \qquad
% \vspace{0.4mm}
%     \subfloat[Weight movement.]{\label{f2}\includegraphics[width=0.3\textwidth]{figures/fig_winning_rate_comparison.pdf}}
% \vspace{-3mm}
% \caption{..
% }
% \end{figure*}
\subsection{Experiment on multi-turn conversation dataset}
In this section, we test the proposed algorithms with multi-turn conversations in MT-bench-101~\citep{bai2024mt}.
% , which is a dataset that includes multi-turn conversations on 13 distinct tasks up to 7 turns.
% , see detailed description at \cref{sec:appendix_exp}.
We choose Mistral-7B-Instruct-v0.2 as the base model~\citep{jiang2023mistral}.
% ~\footnote{{https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2}}
We use a pre-trained PairRM
\footnote{{https://huggingface.co/llm-blender/PairRM}}
% , with 0.4B parameters, 
as the preference oracle. Specifically, given two conversations $[s_h,a_h]$ and $[s_h^\prime,a_h^\prime]$, PairRM will return a score that indicates the probability that $[s_h,a_h]$ is better than $[s_h^\prime,a_h^\prime]$, which can be used to considered as the preference oracle $\mathbb{P}$ defined in the previous section.
We select iterative DPO~\citep{dong2024rlhf}, iterative SPPO~\citep{wu2024self}, and iterative Step-DPO as our baselines. For both iterative DPO and iterative SPPO, we sample $K=5$ complete conversations starting from $s_1$, and estimate the winning rate
$\mathbb{P}([s_{H+1}^k,a_{H+1}^k] \succ (s_{H+1}^{k^\prime},a_{H+1}^{k^\prime}] )\,\forall k,k^\prime \in [K]$. Then we select both the best and worst conversations according to their winning rates against others, which is defined as $\frac{1}{K}\sum_{k^\prime=1}^K \mathbb{P}([s_{H+1}^k,a_{H+1}^k] \succ [s_{H+1}^{k^\prime},a_{H+1}^{k^\prime}] ) $ for the conversation $[s_{H+1}^k,a_{H+1}^k]$. Such a pair is used to train DPO while the winning rate is used to train SPPO. 
For both Step-DPO, \method{}, and \oomdmethod{}, we do the same strategy with starting at $s_h$. In \method{} and \oomdmethod{}, we estimate ${Q}(s_h, a_h, s_h, a_h^\prime)$ by  $ \mathbb{P}([s_h, a_h], [s_h, a_h^\prime])$ to enhance the efficiency.
% Based on this, we select both the best and worst answers according to their winning rates, treating these as the preference pair for all methods. For both \method{} and \oomdmethod{}, we estimate the \( Q \)-function at each step using the current preferences, i.e., ${Q}(s_h, a_h, s_h, a_h^\prime) = \mathbb{P}([s_h, a_h], [s_h, a_h^\prime])$ to enhance the efficiency.
For \oomdmethod{}, the $Q^{\pi^t,\pi^{t-1}}$ term is estimated by calculating the winning rate between two answers (the best and the worst) generated by the current policy $\pi^t$ and the five answers previously generated by $\pi^{t-1}$. Each round of dialogue is rated on a scale of 1 to 10 by GPT-4o mini, with the mean score reported for each dialogue. All methods are run for a total of 3 iterations. The results are summarized in \cref{tab:mtbench}, showing significant improvements over the baselines with the proposed \method{} and \oomdmethod{} approaches. In~\cref{subfig:radar}, we present the Radar chart on different categories and we can see that the proposed \oomdmethod{} leads to improvements generally along the iterations. \cref{subfig:winrate} shows that using the entire trajectory to estimate the $Q$ function can lead to subtle improvement at the first two iterations while it finally achieves a similar winning rate when compared to the one that only use one step. \looseness-1


\subsection{\rebuttal{Experiment on math reasoning dataset}}
\rebuttal{
As discussed in \cref{sec:preliminary}, our framework can also cover the alignment of chain-of-thought reasoning. In this section, we validate the proposed methods in 
% we validate the proposed methods for math reasoning tasks. 
% We select
two widely used math reasoning datasets: MATH~\cite{hendrycksmath2021} and GSM8K~\cite{cobbe2021training}. We use Qwen2-7B-Instruct as the base model and follow the same evaluation procedure as in \citet{lai2024step}. We adopt the dataset for alignment from \citet{lai2024step}, which contains 10795 samples of augmented mathematical problems from MetaMath~\citep{yu2024metamath} and MMIQC~\citep{liu2024augmenting}\footnote{\rebuttal{https://huggingface.co/datasets/xinlai/Math-Step-DPO-10K}}. For both \method{} and \oomdmethod{}, we select the Llama-3-based model\footnote{\rebuttal{https://huggingface.co/RLHFlow/pair-preference-model-LLaMA3-8B}} as the preference oracle. For Step-DPO, we implement two versions. The first version is using the Llama-3-based model as the preference oracle and follows the same procedure as \method{} and \oomdmethod{}. The second version is using the checkpoint provided in \citet{lai2024step}.
% The experiment
% is conducted on 4 A100-SXM4-80GB GPUs. 
The result is provided in \cref{tab:mathgsm}, showing that the proposed methods achieve performance comparable to Step-DPO~\citep{lai2024step}.
% Notably, \method{} and \oomdmethod{} do not require the ground truth label of the dataset during fine-tuning while \citet{lai2024step} requires it. Additionally, MPO and OMPO need only a Llama3-based reward model to compare the two answers. Step-DPO~\citep{lai2024step} requires GPT-4 to identify the incorrect reasoning step in an answer, which is a considerably more difficult task than comparison.
}

% \begin{table}
% \centering
% \resizebox{0.3\columnwidth}{!}{%
% \begin{tabular}{r|lll}
% \multicolumn{1}{r}{}
% & \multicolumn{1}{l}{Heading 1}
% & \multicolumn{1}{l}{Heading 2}
% & \multicolumn{1}{l}{Heading 3} \\ \cline{2-4}
% Row 1 & Cell 1,1 & Cell 1,2 & Cell 1,3 \\
% Row 2 & Cell 2,1 & Cell 2,2 & Cell 2,3
% \end{tabular}%
% }
% \end{table}

\begin{table}[!t]
% \tabcolsep=0.1cm
	% \renewcommand\arraystretch{2}
    \centering
    \caption{ \rebuttal{Performance of math reasoning on MATH and GSM8K dataset across various models.
    \method{} and \oomdmethod{} achieve comparable performance comparable to Step-DPO~\citep{lai2024step} without requiring the ground truth label of the dataset during fine-tuning while \citet{lai2024step} requires. Additionally, \method{} and \oomdmethod{} only need access to a Llama-3-based reward model (RM) to compare two answers whereas Step-DPO \cite{lai2024step} requires GPT-4 to locate the identify the incorrect reasoning step in an answer, which is a considerably more difficult task than comparison.
    }}
        \resizebox{1\columnwidth}{!}{%
\begin{tabular}{c|c|c|c|c|c}
\toprule
 Method & 
 % \shortstack{ Additional info on \\ incorrect step }
 \makecell{Additional info on\\incorrect step}
 &  \makecell{Auxiliary Autoregressive \\ Language Model}
   &  Average  &  GSM8K &     Math              \\
\midrule
 Base (Qwen2-7B-Instruct)   & - &-& 0.7049 &  0.8559  & 0.5538   \\
Step-DPO~\citep{lai2024step}   & \textcolor{tabgreen}{\textbf{\cmark}} &\textcolor{tabgreen}{\textbf{\cmark}} 
 (Require GPT-4) &0.7258 &  0.8680 & \textbf{0.5836}    \\
  Step-DPO        & 
  \textcolor{red}
  {\textbf{\xmark}} &\textcolor{red}
  {\textbf{\xmark}} (Require Llama-3 RM)
  &0.7184 & 0.8749  & 0.5618  \\   
% \method{} (iter=1)     &0.7260 &  0.8734 &  0.5734   \\
\method{}    &\textcolor{red}
  {\textbf{\xmark}} &\textcolor{red}
  {\textbf{\xmark}} (Require Llama-3 RM)& 0.7260&  0.8734 & 	0.5786   \\
% \oomdmethod{} (iter=1)     &   0.8734 &  0.5734     \\
\oomdmethod{}  &\textcolor{red}
  {\textbf{\xmark}}  &\textcolor{red}
  {\textbf{\xmark}} (Require Llama-3 RM)&	\textbf{0.7283}  &  \textbf{0.8779}	& 0.5786     \\
\bottomrule
\end{tabular}%
}
\label{tab:mathgsm}
\end{table}


\section{Conclusion}
\label{sec:conclusion}
This work presents a novel framework to enhance the preference alignment of LLMs in multi-step setting by casting the alignment process as a two-player Markov game. Based on natural actor-critic and optimistic online gradient descent, novel algorithms are introduced with theoretical analysis and empirical results.  However, the limitation of this work includes the finite-horizon assumption in our theoretical framework and future work should explore extending the theoretical framework to infinite-horizon settings.
% These findings suggest that our method can significantly improve alignment with human values in more complex, multi-step scenarios.
% , which may not fully capture real-world conversations or reasoning processes that often span with different steps instead of a fixed step $H$. 

% Additionally, our practical algorithm requires querying a preference oracle, 
% often modeled by another advanced language model,
% which may limit its applicability in cases where such preference oracles are unavailable or when collecting human feedback is costly. 
% Future work should explore extending the theoretical framework to infinite-horizon settings and finding more scalable methods for gathering preference feedback.
\looseness-1
% \subsection{Ablation study}
% \begin{itemize}
%     \item Using step-wise reward (gamma=1) vs Using one step reward (gamma=0)
%     \item .
% \end{itemize}


% \newpage
% \section{Related work}
% \subsection{singlestep environment}
% From r to Q~\citep{rafailov2024r}

% Token-level Direct Preference Optimization~\citep{zengtoken}

% Alignment use soft actor-critic~\cite{singh2024dipper}



% \subsection{multistep environment without reward (reasoning)}
% \label{sec:related_reasoning}
% Chain of Preference Optimization: \cite{zhang2024chain}, this paper does not consider self-play and general preference, we should build our method on top of it.

% % Self-Training DPO~\cite{wang2024self}

% Step-DPO \cite{lai2024step}, 

% Q*: Improving Multi-step Reasoning \cite{wang2024q}


% Iterative reasoning preference optimization 
% \cite{pang2024iterative}

% \subsection{multistep environment with reward}
% ~\cite{song2024trial}, very similar to the extension of SPIN to multistep fine-tuning

% ~\cite{shi2024direct} replace kl divergence with  state-action occupancy 

% ~\cite{xiong2024watch}, a bit similar to the chain of reference optimization in previous section


% \subsection{two-player game RL}
% \cite{zhong2022pessimistic}

% \cite{daskalakis2020independent}


% \begin{minipage}{0.8\linewidth} 
% \end{minipage}2





\section*{Impact Statement}
In this work, we propose novel algorithms for multi-step alignment in LLMs and establish their theoretical guarantees. Our contributions aim to advance the alignment of LLMs with human values, thereby improving their trustworthiness and societal utility. We do not create any new benchmarks for human preferences nor solicit human preferences
for this study. As such, we do not expect any potential violations of ethical standards, including
those concerning the use of human data. Our contributions are primarily methodological and theoretical analysis of the convergence, and we have taken care to ensure that our work complies with
all relevant ethical guidelines. 

\section*{Acknowledgements}
%zhenyu
Part of this work was done during Yongtao Wu and Zhenyu Zhu's visit at UCLA. This work was supported by Hasler Foundation Program: Hasler Responsible AI (project number 21043).
%yongtao kimon
This work was supported by the Swiss National Science Foundation (SNSF) under grant number 200021\_205011.
%luca
This work is funded (in part) through a PhD fellowship of the Swiss Data Science Center, a joint venture between EPFL and ETH Zurich.
%volkan
This research was sponsored by the Army Research Office and was accomplished under Grant Number W911NF-24-1-0048. 
%TODO yihang chen, 
% quanquan gu
Gu is partially supported by the National Science Foundation IIS-2008981, CPS-2312094, DMS-2323113, IIS-2403400 and the Sloan Research Fellowship.

% Our focus is on algorithmic innovations in RLHF, without introducing new benchmarks or collecting human preference data for this study. As a result, our research avoids potential ethical concerns related to the handling of human data or the solicitation of subjective input.

% The primary contributions of this work are methodological, centered on the theoretical analysis of algorithmic convergence. We have rigorously ensured compliance with all relevant ethical standards and guidelines, and we do not foresee any immediate negative societal impacts arising from this research. Instead, we believe this work can serve as a foundation for further exploration of alignment techniques, contributing positively to the responsible development of AI systems.

% Authors are \textbf{required} to include a statement of the potential 
% broader impact of their work, including its ethical aspects and future 
% societal consequences. This statement should be in an unnumbered 
% section at the end of the paper (co-located with Acknowledgements -- 
% the two may appear in either order, but both must be before References), 
% and does not count toward the paper page limit. In many cases, where 
% the ethical impacts and expected societal implications are those that 
% are well established when advancing the field of Machine Learning, 
% substantial discussion is not required, and a simple statement such 
% as the following will suffice:

% ``This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.''

% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.

\bibliography{icml2025}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{appendix}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
