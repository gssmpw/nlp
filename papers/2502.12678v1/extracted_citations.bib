@inproceedings{alacaoglu2022natural,
  title={A natural actor-critic framework for zero-sum Markov games},
  author={Alacaoglu, Ahmet and Viano, Luca and He, Niao and Cevher, Volkan},
  booktitle={International Conference on Machine Learning},
  pages={307--366},
  year={2022},
  organization={PMLR}
}

@InProceedings{chiang2012online,
  title = 	 {Online Optimization with Gradual Variations},
  author = 	 {Chiang, Chao-Kai and Yang, Tianbao and Lee, Chia-Jung and Mahdavi, Mehrdad and Lu, Chi-Jen and Jin, Rong and Zhu, Shenghuo},
  booktitle = 	 {Proceedings of the 25th Annual Conference on Learning Theory},
  pages = 	 {6.1--6.20},
  year = 	 {2012},
  editor = 	 {Mannor, Shie and Srebro, Nathan and Williamson, Robert C.},
  volume = 	 {23},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Edinburgh, Scotland},
  month = 	 {25--27 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v23/chiang12/chiang12.pdf},
  url = 	 {https://proceedings.mlr.press/v23/chiang12.html},
  abstract = 	 {We study the online convex optimization problem, in which an online algorithm has to make repeated decisions with convex loss functions and hopes to achieve a small regret. We consider a natural restriction of this problem in which the loss functions have a small deviation, measured by the sum of the distances between every two consecutive loss functions, according to some distance metrics. We show that for the linear and general smooth convex loss functions, an online algorithm modified from the gradient descend algorithm can achieve a regret which only scales as the square root of the deviation. For the closely related problem of prediction with expert advice, we show that an online algorithm modified  from the multiplicative update algorithm can also achieve a similar regret bound for a different measure of deviation. Finally, for loss functions which are strictly convex, we show that an online algorithm modified from the online Newton step algorithm can achieve a regret which is only logarithmic in terms of the deviation, and as an application, we can also have such a logarithmic regret for the portfolio management problem.}
}

@article{daskalakis2020independent,
  title={Independent policy gradient methods for competitive reinforcement learning},
  author={Daskalakis, Constantinos and Foster, Dylan J and Golowich, Noah},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={5527--5540},
  year={2020}
}

@InProceedings{joulani17a,
  title = 	 {A Modular Analysis of Adaptive (Non-)Convex Optimization: Optimism, Composite Objectives, and Variational Bounds},
  author = 	 {Joulani, Pooria and György, András and Szepesvári, Csaba},
  booktitle = 	 {Proceedings of the 28th International Conference on Algorithmic Learning Theory},
  pages = 	 {681--720},
  year = 	 {2017},
  editor = 	 {Hanneke, Steve and Reyzin, Lev},
  volume = 	 {76},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {15--17 Oct},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v76/joulani17a/joulani17a.pdf},
  url = 	 {https://proceedings.mlr.press/v76/joulani17a.html},
  abstract = 	 {Recently, much work has been done on extending the scope of online learning and incremental stochastic optimization algorithms. In this paper we contribute to this effort in two ways: First, based on a new regret decomposition and a generalization of Bregman divergences, we provide a self-contained, modular analysis of the two workhorses of online learning: (general) adaptive versions of Mirror Descent (MD) and the Follow-the-Regularized-Leader (FTRL) algorithms. The analysis is done with extra care so as not to introduce assumptions not needed in the proofs and allows to combine, in a straightforward way, different algorithmic ideas (e.g., adaptivity, optimism, implicit updates) and learning settings (e.g., strongly convex or composite objectives). This way we are able to reprove, extend and refine a large body of the literature, while keeping the proofs concise. The second contribution is a byproduct of this careful analysis: We present algorithms with improved variational bounds for smooth, composite objectives, including a new family of optimistic MD algorithms with only one projection step per round. Furthermore, we provide a simple extension of adaptive regret bounds to practically relevant non-convex problem settings with essentially no extra effort.}
}

@article{malitsky2020forward,
  title={A forward-backward splitting method for monotone inclusions without cocoercivity},
  author={Malitsky, Yura and Tam, Matthew K},
  journal={SIAM Journal on Optimization},
  volume={30},
  number={2},
  pages={1451--1472},
  year={2020},
  publisher={SIAM}
}

@article{popov1980modification,
  title={A modification of the Arrow-Hurwitz method of search for saddle points},
  author={Popov, Leonid Denisovich},
  journal={Mat. Zametki},
  volume={28},
  number={5},
  pages={777--784},
  year={1980}
}

@inproceedings{rakhlin2013online,
  title={Online learning with predictable sequences},
  author={Rakhlin, Alexander and Sridharan, Karthik},
  booktitle={Conference on Learning Theory},
  pages={993--1019},
  year={2013},
  organization={PMLR}
}

@article{shapley1953stochastic,
  title={Stochastic games},
  author={Shapley, Lloyd S},
  journal={Proceedings of the national academy of sciences},
  volume={39},
  number={10},
  pages={1095--1100},
  year={1953},
  publisher={National Acad Sciences}
}

@inproceedings{wei2021last,
  title={Last-iterate convergence of decentralized optimistic gradient descent/ascent in infinite-horizon competitive Markov games},
  author={Wei, Chen-Yu and Lee, Chung-Wei and Zhang, Mengxiao and Luo, Haipeng},
  booktitle={Conference on Learning Theory},
  pages={4259--4299},
  year={2021},
  organization={PMLR}
}

