[
  {
    "index": 0,
    "papers": [
      {
        "key": "wortsman2022model",
        "author": "Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and others",
        "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time"
      },
      {
        "key": "ilharco2022editing",
        "author": "Ilharco, Gabriel and Ribeiro, Marco Tulio and Wortsman, Mitchell and Gururangan, Suchin and Schmidt, Ludwig and Hajishirzi, Hannaneh and Farhadi, Ali",
        "title": "Editing models with task arithmetic"
      },
      {
        "key": "goddard2024arcee",
        "author": "Goddard, Charles and Siriwardhana, Shamane and Ehghaghi, Malikeh and Meyers, Luke and Karpukhin, Vlad and Benedict, Brian and McQuade, Mark and Solawetz, Jacob",
        "title": "Arcee's MergeKit: A Toolkit for Merging Large Language Models"
      },
      {
        "key": "jin2022dataless",
        "author": "Jin, Xisen and Ren, Xiang and Preotiuc-Pietro, Daniel and Cheng, Pengxiang",
        "title": "Dataless knowledge fusion by merging weights of language models"
      },
      {
        "key": "matena2022merging",
        "author": "Matena, Michael S and Raffel, Colin A",
        "title": "Merging models with fisher-weighted averaging"
      },
      {
        "key": "yadav2024ties",
        "author": "Yadav, Prateek and Tam, Derek and Choshen, Leshem and Raffel, Colin A and Bansal, Mohit",
        "title": "Ties-merging: Resolving interference when merging models"
      },
      {
        "key": "yu2024language",
        "author": "Yu, Le and Yu, Bowen and Yu, Haiyang and Huang, Fei and Li, Yongbin",
        "title": "Language models are super mario: Absorbing abilities from homologous models as a free lunch"
      },
      {
        "key": "roberts2024pretrained",
        "author": "Roberts, Nicholas and Guo, Samuel and Gao, Zhiqi and GNVV, Satya Sai Srinath Namburi and Cromp, Sonia and Wu, Chengjun and Duan, Chengyu and Sala, Frederic",
        "title": "Pretrained Hybrids with MAD Skills"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "wortsman2022model",
        "author": "Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and others",
        "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "ilharco2022editing",
        "author": "Ilharco, Gabriel and Ribeiro, Marco Tulio and Wortsman, Mitchell and Gururangan, Suchin and Schmidt, Ludwig and Hajishirzi, Hannaneh and Farhadi, Ali",
        "title": "Editing models with task arithmetic"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "jin2022dataless",
        "author": "Jin, Xisen and Ren, Xiang and Preotiuc-Pietro, Daniel and Cheng, Pengxiang",
        "title": "Dataless knowledge fusion by merging weights of language models"
      },
      {
        "key": "matena2022merging",
        "author": "Matena, Michael S and Raffel, Colin A",
        "title": "Merging models with fisher-weighted averaging"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "yadav2024ties",
        "author": "Yadav, Prateek and Tam, Derek and Choshen, Leshem and Raffel, Colin A and Bansal, Mohit",
        "title": "Ties-merging: Resolving interference when merging models"
      },
      {
        "key": "yu2024language",
        "author": "Yu, Le and Yu, Bowen and Yu, Haiyang and Huang, Fei and Li, Yongbin",
        "title": "Language models are super mario: Absorbing abilities from homologous models as a free lunch"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "wortsman2022model",
        "author": "Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and others",
        "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time"
      },
      {
        "key": "ilharco2022editing",
        "author": "Ilharco, Gabriel and Ribeiro, Marco Tulio and Wortsman, Mitchell and Gururangan, Suchin and Schmidt, Ludwig and Hajishirzi, Hannaneh and Farhadi, Ali",
        "title": "Editing models with task arithmetic"
      },
      {
        "key": "goddard2024arcee",
        "author": "Goddard, Charles and Siriwardhana, Shamane and Ehghaghi, Malikeh and Meyers, Luke and Karpukhin, Vlad and Benedict, Brian and McQuade, Mark and Solawetz, Jacob",
        "title": "Arcee's MergeKit: A Toolkit for Merging Large Language Models"
      },
      {
        "key": "jin2022dataless",
        "author": "Jin, Xisen and Ren, Xiang and Preotiuc-Pietro, Daniel and Cheng, Pengxiang",
        "title": "Dataless knowledge fusion by merging weights of language models"
      },
      {
        "key": "matena2022merging",
        "author": "Matena, Michael S and Raffel, Colin A",
        "title": "Merging models with fisher-weighted averaging"
      },
      {
        "key": "roberts2024pretrained",
        "author": "Roberts, Nicholas and Guo, Samuel and Gao, Zhiqi and GNVV, Satya Sai Srinath Namburi and Cromp, Sonia and Wu, Chengjun and Duan, Chengyu and Sala, Frederic",
        "title": "Pretrained Hybrids with MAD Skills"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "wortsman2022model",
        "author": "Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and others",
        "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "ilharco2022editing",
        "author": "Ilharco, Gabriel and Ribeiro, Marco Tulio and Wortsman, Mitchell and Gururangan, Suchin and Schmidt, Ludwig and Hajishirzi, Hannaneh and Farhadi, Ali",
        "title": "Editing models with task arithmetic"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "jin2022dataless",
        "author": "Jin, Xisen and Ren, Xiang and Preotiuc-Pietro, Daniel and Cheng, Pengxiang",
        "title": "Dataless knowledge fusion by merging weights of language models"
      },
      {
        "key": "matena2022merging",
        "author": "Matena, Michael S and Raffel, Colin A",
        "title": "Merging models with fisher-weighted averaging"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "yadav2024ties",
        "author": "Yadav, Prateek and Tam, Derek and Choshen, Leshem and Raffel, Colin A and Bansal, Mohit",
        "title": "Ties-merging: Resolving interference when merging models"
      },
      {
        "key": "yu2024language",
        "author": "Yu, Le and Yu, Bowen and Yu, Haiyang and Huang, Fei and Li, Yongbin",
        "title": "Language models are super mario: Absorbing abilities from homologous models as a free lunch"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "roberts2024pretrained",
        "author": "Roberts, Nicholas and Guo, Samuel and Gao, Zhiqi and GNVV, Satya Sai Srinath Namburi and Cromp, Sonia and Wu, Chengjun and Duan, Chengyu and Sala, Frederic",
        "title": "Pretrained Hybrids with MAD Skills"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "wan2024knowledge",
        "author": "Wan, Fanqi and Huang, Xinting and Cai, Deng and Quan, Xiaojun and Bi, Wei and Shi, Shuming",
        "title": "Knowledge fusion of large language models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "roberts2024pretrained",
        "author": "Roberts, Nicholas and Guo, Samuel and Gao, Zhiqi and GNVV, Satya Sai Srinath Namburi and Cromp, Sonia and Wu, Chengjun and Duan, Chengyu and Sala, Frederic",
        "title": "Pretrained Hybrids with MAD Skills"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "wan2024knowledge",
        "author": "Wan, Fanqi and Huang, Xinting and Cai, Deng and Quan, Xiaojun and Bi, Wei and Shi, Shuming",
        "title": "Knowledge fusion of large language models"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "fedus2022switch",
        "author": "Fedus, William and Zoph, Barret and Shazeer, Noam",
        "title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity"
      },
      {
        "key": "shazeer2017outrageously",
        "author": "Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff",
        "title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer"
      },
      {
        "key": "zhang2022mixtureattentionheadsselecting",
        "author": "Xiaofeng Zhang and Yikang Shen and Zeyu Huang and Jie Zhou and Wenge Rong and Zhang Xiong",
        "title": "Mixture of Attention Heads: Selecting Attention Heads Per Token"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "komatsuzaki2022sparse",
        "author": "Komatsuzaki, Aran and Puigcerver, Joan and Lee-Thorp, James and Ruiz, Carlos Riquelme and Mustafa, Basil and Ainslie, Joshua and Tay, Yi and Dehghani, Mostafa and Houlsby, Neil",
        "title": "Sparse upcycling: Training mixture-of-experts from dense checkpoints"
      },
      {
        "key": "jiang2024mixtral",
        "author": "Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others",
        "title": "Mixtral of experts"
      },
      {
        "key": "dou2024loramoe",
        "author": "Dou, Shihan and Zhou, Enyu and Liu, Yan and Gao, Songyang and Shen, Wei and Xiong, Limao and Zhou, Yuhao and Wang, Xiao and Xi, Zhiheng and Fan, Xiaoran and others",
        "title": "LoRAMoE: Alleviating World Knowledge Forgetting in Large Language Models via MoE-Style Plugin"
      },
      {
        "key": "dai2024deepseekmoe",
        "author": "Dai, Damai and Deng, Chengqi and Zhao, Chenggang and Xu, RX and Gao, Huazuo and Chen, Deli and Li, Jiashi and Zeng, Wangding and Yu, Xingkai and Wu, Y and others",
        "title": "Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "sukhbaatar2024branchtrainmixmixingexpertllms",
        "author": "Sainbayar Sukhbaatar and Olga Golovneva and Vasu Sharma and Hu Xu and Xi Victoria Lin and Baptiste Rozi\u00e8re and Jacob Kahn and Daniel Li and Wen-tau Yih and Jason Weston and Xian Li",
        "title": "Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "gururangan2023scaling",
        "author": "Gururangan, Suchin and Li, Margaret and Lewis, Mike and Shi, Weijia and Althoff, Tim and Smith, Noah A and Zettlemoyer, Luke",
        "title": "Scaling expert language models with unsupervised domain discovery"
      },
      {
        "key": "li2022branch",
        "author": "Li, Margaret and Gururangan, Suchin and Dettmers, Tim and Lewis, Mike and Althoff, Tim and Smith, Noah A and Zettlemoyer, Luke",
        "title": "Branch-train-merge: Embarrassingly parallel training of expert language models"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "sukhbaatar2024branchtrainmixmixingexpertllms",
        "author": "Sainbayar Sukhbaatar and Olga Golovneva and Vasu Sharma and Hu Xu and Xi Victoria Lin and Baptiste Rozi\u00e8re and Jacob Kahn and Daniel Li and Wen-tau Yih and Jason Weston and Xian Li",
        "title": "Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "kang2024self",
        "author": "Kang, Junmo and Karlinsky, Leonid and Luo, Hongyin and Wang, Zhen and Hansen, Jacob and Glass, James and Cox, David and Panda, Rameswar and Feris, Rogerio and Ritter, Alan",
        "title": "Self-MoE: Towards Compositional Large Language Models with Self-Specialized Experts"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "hu2021lora",
        "author": "Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu",
        "title": "Lora: Low-rank adaptation of large language models"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "fedus2022switch",
        "author": "Fedus, William and Zoph, Barret and Shazeer, Noam",
        "title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity"
      },
      {
        "key": "shazeer2017outrageously",
        "author": "Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff",
        "title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer"
      },
      {
        "key": "zhang2022mixtureattentionheadsselecting",
        "author": "Xiaofeng Zhang and Yikang Shen and Zeyu Huang and Jie Zhou and Wenge Rong and Zhang Xiong",
        "title": "Mixture of Attention Heads: Selecting Attention Heads Per Token"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "komatsuzaki2022sparse",
        "author": "Komatsuzaki, Aran and Puigcerver, Joan and Lee-Thorp, James and Ruiz, Carlos Riquelme and Mustafa, Basil and Ainslie, Joshua and Tay, Yi and Dehghani, Mostafa and Houlsby, Neil",
        "title": "Sparse upcycling: Training mixture-of-experts from dense checkpoints"
      },
      {
        "key": "jiang2024mixtral",
        "author": "Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others",
        "title": "Mixtral of experts"
      },
      {
        "key": "dou2024loramoe",
        "author": "Dou, Shihan and Zhou, Enyu and Liu, Yan and Gao, Songyang and Shen, Wei and Xiong, Limao and Zhou, Yuhao and Wang, Xiao and Xi, Zhiheng and Fan, Xiaoran and others",
        "title": "LoRAMoE: Alleviating World Knowledge Forgetting in Large Language Models via MoE-Style Plugin"
      },
      {
        "key": "dai2024deepseekmoe",
        "author": "Dai, Damai and Deng, Chengqi and Zhao, Chenggang and Xu, RX and Gao, Huazuo and Chen, Deli and Li, Jiashi and Zeng, Wangding and Yu, Xingkai and Wu, Y and others",
        "title": "Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "sukhbaatar2024branchtrainmixmixingexpertllms",
        "author": "Sainbayar Sukhbaatar and Olga Golovneva and Vasu Sharma and Hu Xu and Xi Victoria Lin and Baptiste Rozi\u00e8re and Jacob Kahn and Daniel Li and Wen-tau Yih and Jason Weston and Xian Li",
        "title": "Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"
      },
      {
        "key": "li-etal-2024-pedants",
        "author": "Li, Zongxia  and\nMondal, Ishani  and\nNghiem, Huy  and\nLiang, Yijun  and\nBoyd-Graber, Jordan Lee",
        "title": "{PEDANTS}: Cheap but Effective and Interpretable Answer Equivalence"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "gururangan2023scaling",
        "author": "Gururangan, Suchin and Li, Margaret and Lewis, Mike and Shi, Weijia and Althoff, Tim and Smith, Noah A and Zettlemoyer, Luke",
        "title": "Scaling expert language models with unsupervised domain discovery"
      },
      {
        "key": "li2022branch",
        "author": "Li, Margaret and Gururangan, Suchin and Dettmers, Tim and Lewis, Mike and Althoff, Tim and Smith, Noah A and Zettlemoyer, Luke",
        "title": "Branch-train-merge: Embarrassingly parallel training of expert language models"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "sukhbaatar2024branchtrainmixmixingexpertllms",
        "author": "Sainbayar Sukhbaatar and Olga Golovneva and Vasu Sharma and Hu Xu and Xi Victoria Lin and Baptiste Rozi\u00e8re and Jacob Kahn and Daniel Li and Wen-tau Yih and Jason Weston and Xian Li",
        "title": "Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "kang2024self",
        "author": "Kang, Junmo and Karlinsky, Leonid and Luo, Hongyin and Wang, Zhen and Hansen, Jacob and Glass, James and Cox, David and Panda, Rameswar and Feris, Rogerio and Ritter, Alan",
        "title": "Self-MoE: Towards Compositional Large Language Models with Self-Specialized Experts"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "hu2021lora",
        "author": "Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu",
        "title": "Lora: Low-rank adaptation of large language models"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "liu2024csrec",
        "author": "Liu, Xiaoyu and Yuan, Jiaxin and Zhou, Yuhang and Li, Jingling and Huang, Furong and Ai, Wei",
        "title": "Csrec: Rethinking sequential recommendation from a causal perspective"
      }
    ]
  }
]