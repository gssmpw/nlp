\section{Background and Related Work}
% This paper is related to previous work on dense and MoE model merging.

\subsection{Dense Model Merging}

% Model merging methods combine multiple domain experts into a single model to obtain diverse capabilities \cite{wortsman2022model, ilharco2022editing, goddard2024arcee, jin2022dataless, matena2022merging, yadav2024ties, yu2024language, roberts2024pretrained}. Most merging methods focus on homogeneous dense expert models into another dense model of the same architecture: average merging \cite{wortsman2022model} applies averaging over the model parameters; task vector merging \cite{ilharco2022editing} first extracts the task vector (the difference of parameters between the base and the experts) and adds the unweighted sum of the task vectors back to the dense model with a scaling term. Other works explore how to determine the weights of the task vector instead of an unweighted sum \cite{jin2022dataless, matena2022merging}. Dare and Ties \cite{yadav2024ties, yu2024language}, two SoTA merging methods, choose to trim the task vector to resolve parameter interference: in Dare, the task vector is randomly trimmed and rescaled by a fixed amount;
% in the Ties method, on the other hand, the authors
% set the vector parameter to 0 by magnitude and adjust the sign of each parameter to reduce the sign conflict.

Dense merging methods combine multiple dense models into one to achieve diverse capabilities \cite{wortsman2022model, ilharco2022editing, goddard2024arcee, jin2022dataless, matena2022merging, roberts2024pretrained}. Most approaches focus on merging homogeneous dense models into another dense model. For example, average merging~\cite{wortsman2022model} averages model parameters, while task vector merging~\cite{ilharco2022editing} adds the unweighted sum of task vectors (the difference between base and expert parameters) back to the dense model with scaling. Other work determines task vector weights instead of using an unweighted sum \cite{jin2022dataless, matena2022merging}. SoTA methods like Dare and Ties \cite{yadav2024ties, yu2024language} trim the task vector to resolve parameter interference: Dare trims the task vector randomly and rescales, while Ties sets vector parameters to zero by magnitude and adjusts signs to reduce conflicts.

% In addition to homogeneous model merging, \citet{roberts2024pretrained} proposes an approach to merge heterogeneous models to a dense model by incorporating the projectors. \citet{wan2024knowledge} applies the knowledge distillation to fuse heterogeneous models.
% Compared with previous merging works for dense models, our paper focuses on merging experts to an MoE model. For homogeneous experts, we explore a more efficient merging method and merging without fine-tuning. We are the first work to propose a pipeline for merging heterogeneous models into an MoE.

In addition to homogeneous model merging, \citet{roberts2024pretrained} propose merging heterogeneous models into a dense model using projectors, while \citet{wan2024knowledge} apply knowledge distillation to fuse heterogeneous models. 
In this work, we introduce a more efficient method for merging experts with limited or no further fine-tuning and, unlike previous work focusing on dense models, we explore merging homogeneous and heterogeneous experts into an MoE model. 
%Additionally, we propose 
%For homogeneous experts, we propose a more efficient merging method without fine-tuning. We are also the first to introduce a pipeline for merging heterogeneous models into an MoE.

\subsection{MoE Training and Merging}

% MoE architectures allows for faster training given a fixed computational budget, and faster inference given a fixed number of parameters.  They do so by introducing Sparse MoE layers, where a router mechanism dispatches tokens to the top-K expert FFNs in parallel (where k is usually 1 or 2)  \cite{fedus2022switch, shazeer2017outrageously, zhang2022mixtureattentionheadsselecting}. In terms of MoE training, most work chooses to train the whole MoE on all domain data to obtain capabilities across multiple tasks \cite{komatsuzaki2022sparse, jiang2024mixtral, dou2024loramoe, dai2024deepseekmoe}, but full communication during training on different GPUs is costly \cite{sukhbaatar2024branchtrainmixmixingexpertllms}. To address this challenge, inspired by the Branch-Train-Merge (BTM) method \cite{gururangan2023scaling, li2022branch}, where they average the final model output distributions from different experts, the Branch-Train-Mix (BTX) method \cite{sukhbaatar2024branchtrainmixmixingexpertllms} first branches the base model to multiple copies, trains the base model in different domains, and merges the experts into a unified MoE. Another recent work, Self-MoE \cite{kang2024self}, uses the LoRa style \cite{hu2021lora} to fine-tune each expert on self-generated data and then combine all trained adapters into a base model for a LoRa MoE. 

MoE architectures enable quicker inference with a certain parameter count by introducing Sparse MoE layers, where a router mechanism assigns tokens to the top-$K$ expert FFNs (usually 1 or 2) in parallel \cite{fedus2022switch, shazeer2017outrageously, zhang2022mixtureattentionheadsselecting}. Most MoE training approaches, known as upcycling, train the entire model from scratch to handle multiple tasks \cite{komatsuzaki2022sparse, jiang2024mixtral, dou2024loramoe, dai2024deepseekmoe}. These methods first initialize the MoE model from a pretrained base model and then train it on the entire dataset. However, due to the costly communication between GPUs, the upcycling method introduces significant computational overhead \cite{sukhbaatar2024branchtrainmixmixingexpertllms, li-etal-2024-pedants}. To address this, methods like Branch-Train-Merge (BTM) \cite{gururangan2023scaling, li2022branch} average model outputs from different experts, while Branch-Train-Mix (BTX) \cite{sukhbaatar2024branchtrainmixmixingexpertllms} branches the base model, trains each on different domains, and merges them into a unified MoE. 
BTX is shown to be more effective than BTM as well as dense CPT and MoE upcycling baselines.
%VS: do we need to explain upcycling?
Another recent approach, Self-MoE \cite{kang2024self}, uses low-rank adaptation (LoRA) \cite{hu2021lora} to fine-tune experts on generated synthetic data \cite{liu2024csrec} and combines trained adapters into an MoE.
To our knowledge, we are the first to introduce a framework for merging heterogeneous models into an MoE.
