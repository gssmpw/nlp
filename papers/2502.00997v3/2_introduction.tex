\section{Introduction}
\label{sec:intro}

Large language models (LLMs) pretrained on a wide-variety of corpora have achieved notable success in multiple tasks \cite{touvron2023llama, openai2023gpt4, brown2020language, liu2024large}. With significant progress, there is increasing interest in how to continuously improve the performance of LLMs in new domains, including math \cite{yu2023metamath}, code \cite{roziere2023code}, Wikipedia knowledge \cite{shao2024assisting}, or legal domains \cite{cui2023chatlaw}. One straightforward approach is through continual pretraining (CPT) on domain-specific data, which, however, is challenging for multiple target domains, as it can cause catastrophic forgetting on previously learned tasks~\citep{luo2023empirical}. 


% One possible solution to prevent catastrophic forgetting is to CPT with data replay, including the pretraining data, or add the regularization term \cite{chen2021overcoming, lubana2022quadratic}, but in the realistic case, it is hard to find the appropriate data mix or the regularization term for a specific domain. 

% One state-of-the-art (SoTA) solution to avoiding catastrophic forgetting is to perform model expansion, in which the model blocks are expanded and fine-tuned \cite{wu2024llama, sukhbaatar2024branchtrainmixmixingexpertllms, bafghi2024parameter}. There are two lines of work for model expansion: dense model expansion and the Mixture-of-Expert (MoE) expansion. Dense block-expanded models \cite{wu2024llama, bafghi2024parameter} activate all its parameters during inference, therefore increasing cost during inference time. MoE expansion methods, on the other hand, in which multiple fine-tuned dense experts are combined into a unified MoE, has the advantage of a smaller inference cost and flexible experimentation through parallel training \cite{sukhbaatar2024branchtrainmixmixingexpertllms, kang2024self}.

%One state-of-the-art (SoTA) solution
An alternative approach is Mixture-of-Experts (MoE) merging, where dense experts are first CPT-ed in parallel for each domain and then merged into a unified MoE model, usually by keeping feedforward neural network (FFN) layers separate and averaging non-FFN layers~\cite{sukhbaatar2024branchtrainmixmixingexpertllms, kang2024self}.
Compared with dense models of similar size, the MoE model uses just a subset of parameters during inference by learning to route tokens to the top few experts, thus reducing inference costs.
%, which learns to route tokens to the corresponding expert layer. 
Unlike training an MoE model from scratch, MoE merging offers modularity, as individual experts are domain-specialized, and is substantially less expensive, as CPT-ing experts in parallel requires less compute than training the entire MoE on large datasets from the beginning \cite{sukhbaatar2024branchtrainmixmixingexpertllms}.

%In this approach, multiple dense experts are CPT-ed on different domains and unified into an MoE model by merging the non-feedforward neural network (FFN) layers \cite{sukhbaatar2024branchtrainmixmixingexpertllms, kang2024self}. Compared to a dense model of similar size, during inference, the MoE model selects one or two FFN experts for each decoder layer using a router network positioned between the attention and FFN layers, reducing inference costs. Additionally, unlike training an MoE model from scratch, the MoE merging method bypasses the need for training the entire MoE on large datasets from the beginning, thus saving significant computational resources \cite{sukhbaatar2024branchtrainmixmixingexpertllms}.


% The current SoTA MoE expansion method is Branch-Train-Mix \cite{sukhbaatar2024branchtrainmixmixingexpertllms}. In BTX, a base dense model is first branched into multiple expert models using CPT on domain data, and then merged into a single unified MoE model. BTX merges the non-FFN layers: for each decoder layer of the merged MoE model, the attention and embedding layers are averaged from all experts' corresponding layers, and the MoE FFNs layers are copied from FFNs for each expert. Between attention and FFNs layers, they add the router network, which is a multilayer perceptron (MLP) for token-level routing, choosing only 1 or 2 experts (FFNs) to use for this layer.

% However, there are still two limitations to BTX. First, it utilizes simple averaging for attention layer merging. This straightforward merging method may lead to model parameter interference for homogeneous (same architecture) expert merging and is not suitable for heterogeneous (different architecture) expert merging due to different attention shapes or layer number. Second, to learn an effective router, it requires substantial post-merge fine-tuning for all parameters in the MoE. Fine-tuning the newly merged MoE is costly, and in realistic cases, the training data for the experts may not be public, such as CodeLLama or Mistral \cite{roziere2023code, jiang2023mistral}, where fine-tuning is also not practical.

In this paper, we investigate how to effectively merge different domain expert models into a unified MoE model.
The current state-of-the-art (SoTA) MoE merging approach, such as Branch-Train-Mix (BTX)~\citep{sukhbaatar2024branchtrainmixmixingexpertllms} assumes experts are branched from the same ancestor model and merges experts by simply unweighted averaging the non-FFN layers. 
However, as experts diverge in the parameter space, for example by branching from different ancestors or by training on aggressively different data, unweighted averaging may not effectively handle parameter interference such as sign conflicts~\cite{yu2024language, yadav2024ties}. As a result, the merged MoE may underperform and will require a significant amount of additional fine-tuning to recover in performance, which is both expensive and could be impractical when the experts' training data is not publicly available.
Furthermore, existing MoE merging methods cannot be directly used to merge heterogeneous experts with different architectures, which could be the case in practice, as increasingly more experts are provided by separate teams, such as CodeLlama~\cite{roziere2023code} and Olmo~\cite{groeneveld2024olmo}.
Therefore, it is still an open question how to effectively merge homogeneous and heterogeneous experts into an MoE combining the benefits of each. 
%However, there are still key limitations to current MoE merging method. They utilizes the simple averaging to merge the non-FFN layers (attention, embedding), which may lead to model parameter interference for homogeneous (same architecture) expert merging and is not suitable for heterogeneous (different architecture) expert. Furthermore, to learn an effective router, it requires substantial post-merge fine-tuning for all parameters in the MoE. Fine-tuning the newly merged MoE is costly, and in realistic cases, the training data for the experts may not be public, such as CodeLLama or Mistral \cite{roziere2023code, jiang2023mistral}, where fine-tuning is also not practical.

% We propose solutions to each one of these limitations. For Limitation 1, we propose to replace simple averaging with two more advanced model merging methods, Dare \& Ties \cite{yadav2024ties, yu2024language}, to solve parameter interference for homogeneous models that have seen different data. We also propose a new MoE architecture to merge heterogeneous models to extend the applicable range of the MoE expansion pipeline. For Limitation 2, to reduce the amount of fine-tuning post-merge, we propose a routing heuristic to route token sequences to experts without using the training data used during expert training. We also explore the effects of the combination of different merging options and routing heuristics.  Our extensive experiments demonstrate that our solutions further improve performance across various tasks in different settings.

To enable the use of diverse expert models, our work addresses the above limitations via new MoE merging methodologies for both homogeneous and heterogeneous experts. In summary, our work introduces three main contributions:
% First, we utilize advanced merging methods that tackle parameter interference to replace unweighted averaging in homogeneous expert merging.
% To address low-resource scenarios where no MoE fine-tuning is possible, we design a perplexity-based heuristic to route token sequences to individual domain experts.
% Finally, we propose a novel method to merge experts with different architectures into an MoE, which learns to route token sequences to the individual experts.  

\begin{itemize}[leftmargin=*]
    \item We utilize advanced merging methods that address parameter interference, demonstrating their superiority over unweighted averaging in homogeneous expert merging, particularly in scenarios with limited resources for post-merging MoE fine-tuning.
    \item We propose a perplexity-based heuristic for routing token sequences to domain-specific experts in low-resource environments where MoE fine-tuning is not feasible.
    \item We develop a novel approach to merge experts with different architectures into a single MoE, which learns to route token sequences dynamically to the appropriate expert. 
\end{itemize}

Through extensive experiments and ablation studies across benchmarks in mathematical reasoning, programming, and general knowledge, we show that our proposed methodologies outperform previous state-of-the-art methods and extend the practical applications of MoE merging.

