\section{Conclusion}

% In this paper, we propose novel methods to resolve the challenges in the current MoE merging pipeline. For homogeneous experts, our framework replaces the average merging in non-FFN layers to more advanced merging methods to mitigate the parameter interference. We also explore how to merge models to an MoE without post-merge fine-tuning. For heterogeneous experts, we propose a novel method with projectors and sequence-level routing networks to combine models with different architectures to an MoE. Our extensive empirical evaluations show that the proposed solution can significantly increase MoE performance across multiple datasets in each setting.

In this paper, we propose novel methods to address challenges in the current MoE merging literature. For homogeneous experts, we replace average merging in non-FFN layers with more advanced methods to reduce parameter interference. We also explore merging models into an MoE without post-merge fine-tuning. For heterogeneous experts, we introduce a method using projectors and sequence-level routing networks to combine models with different architectures. Extensive empirical evaluations show that our approach significantly improves MoE performance across multiple datasets.

\section{Limitation}

One of the limitations of the proposed merging methods with heterogeneous experts is that the merged MoE model has more parameters when the BTX merging, since we do not merge the attention layers. For example, for our 4 $\times$ 1B \llama MoE, the total parameter number is about 3.7 billion due to the non-FFNs layer merging but the total parameter number of the MoE after the heterogeneous merging method is near 4 billion. More parameters represent more costly fine-tuning and inference.

For our homogeneous merging method, we replace simple averaging with a more advanced merging method: Dare and Ties and fine-tune MoE models. There are still other merging methods, such as fisher merging \cite{matena2022merging} or Regmean \cite{jin2022dataless} methods. However, in the Ties and Dare paper \cite{yadav2024ties, yu2024language}, they have demonstrated the superiority of proposed merging methods over Regmean and finisher merging, so we leave the exploration of other merging methods to future work.

Moreover, using routing heuristics to process the input sequence introduces additional inference costs, as we first need to use the expert model to calculate the perplexity (PPL) or gradient. However, our routing heuristic requires only one additional forward pass, and considering the multiple forward passes during inference (forward pass number = the generate token number), the computational overhead for our method to enhance MoE performance without fine-tuning is minimal.

For all MoE fine-tuning, we utilize only the cross-entropy loss to do the auto-regression on the training data. Previous works showed that the load-balancing loss \cite{fedus2022switch, sukhbaatar2024branchtrainmixmixingexpertllms} may be beneficial to resolve the ``dead'' experts. From our routing analysis for the merged MoEs, we observe that merging with homogeneous experts gets the desirable patterns, where most tokens in one specific domain are gated to the corresponding expert. However, for heterogeneous experts, due to the different architecture and tokenizer of the math expert, the math expert does not get the highest routing probability in evaluating on GSM8K and MATH datasets. For the next step, we may need to add the load balancing loss for the fine-tuning of MoE with heterogeneous experts to develop more robust models \cite{zhou2024explore} and observe whether the routing patterns are more efficient.

Due to limitations of computation resources, we only experimented with three domains and 1b LLMs. Incorporating larger models and more domains, such as legal, medical, or multilingual, can benefit future studies. Furthermore, our method can be extended to multimodal MoE by incorporating vision audio or graph experts \cite{wang2024mementos, wang2024enhancing, li2024uni, zhu2024multimodal}.

In addition to directly merging models with different architectures with additional projectors, there is another direction to first distill the knowledge of experts to student models with the same architecture \cite{wan2024knowledge, zhou2024teaching, li2025benchmarkevaluationsapplicationschallenges, zhou2023scalable, zhou2024multi} and merge student models together to an MoE. We leave the exploration of this direction to future work. 

\section*{Acknowledgments}
We would like to thank the anonymous reviewers as well
as Saleh Soltan, He Xie, Venkatesh Elango, Wael Hamza, Paiheng Xu, and Xiyao Wang for providing helpful comments and suggestions. 