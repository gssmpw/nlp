\section{Implementation Details}
\label{sec:implement}
For our Base-1B models, we utilize the Llama-2 architecture \cite{wu2024llama} with layer number 24 and hidden dimension 2048. The open-source TinyLlama-1.1B model contains 22 layers and the hidden dimension is 2048. For the open-source Olmo-1B model, it has 16 layers and the hiddn dimension is 2048.

In our experiments, we use top-2 routing for MoE models. For Dare-merging and Ties merging (both dense and MoE), we set the scaling term $\lambda$ to $\frac{1}{3}$ and the retain ratio $p$ of the model parameters of two methods are set to $80\%$ to gain the optimal performance, according to our preliminary exploration. For inference, we set the temperature to 0.0 for greedy decoding, and the maximal number of generated tokens is 512. For CPT and fine-tuning of MoE and dense models, we set the learning rate to 1e-5 and the weight decay is 0.01.

\section{Data mixture}
\label{sec:data_mix}

In Table \ref{table:data_mix}, we present the data ratios to CPT or fine-tune the dense or MoE models.
For fine-tuning the MoE model, we sample datasets that are used to train all experts and the base model with the same probabilities as described in \citet{sukhbaatar2024branchtrainmixmixingexpertllms}.

\begin{table}[!htb]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lccccc}
\toprule
                & Base    & Math    & Code    & Knowledge & Finetune MoE \\ \midrule
Wiki1           & 0.85\%  & 0.17\%  & 0.17\%  & 8.00\%    & 1.11\%       \\ \midrule
Wiki2           & 0.00\%  & 0.00\%  & 0.00\%  & 8.00\%    & 0.82\%       \\ \midrule
Arxiv           & 9.37\%  & 1.87\%  & 1.87\%  & 7.94\%    & 3.94\%       \\ \midrule
CommonCrawl     & 27.92\% & 5.58\%  & 5.58\%  & 23.65\%   & 11.74\%      \\ \midrule
C4              & 54.60\% & 10.93\% & 10.93\% & 46.26\%   & 22.97\%      \\ \midrule
StackExchange   & 7.26\%  & 1.45\%  & 1.45\%  & 6.15\%    & 3.05\%       \\ \midrule
Open   Web Math & 0.00\%  & 80.00\% & 0.00\%  & 0.00\%    & 24.13\%      \\ \midrule
GitHub          & 0.00\%  & 0.00\%  & 80.00\% & 0.00\%    & 32.25\% \\
\bottomrule
\end{tabular}
}
\caption{\label{table:data_mix} Data source and weights for CPT or fine-tune MoE or dense models. Wiki1 represents the first half of Wikipedia data for pretraining the base model and Wiki2 represents the second half of Wikipedia data for CPT the knowledge expert.}
\end{table}


\section{Task Vector Routing Heuristic}
\label{sec:task_vector_routing}

Our second approach is to identify the input domain and assign the input to experts trained in that domain. The core idea is that an expert's task vector, defined as the difference between its parameters and the base model, represents the cumulative gradient of the base model on the expert's training data. For a given input, we first compute the base model's gradient on that input and compare it to the task vectors of each expert. A higher similarity between the gradient and a task vector suggests the input is closer to the expert's training data.

With the task vectors $\tau_1, \tau_2, \dots, \tau_l$ for $l$ experts and inference input $x_{inf}$, the loss function $\mathcal{L}$ and the base model parameters $\theta_b$, we first compute the gradient ($g_{inf}$) of the loss function with respect to the base model parameters as: $g_{inf} = \nabla_{\theta_b}\mathcal{L}(x_{inf})$.

The routing heuristic decides the experts and weights with the cosine similarity (Sim) as below:
\begin{equation*}
    \resizebox{\columnwidth}{!}{%
        $\alpha = \text{SoftMax}(\text{top-K}(\text{Sim}(g_{inf}, \tau_1), \dots, \text{Sim}(g_{inf}, \tau_l)))$%      
        }
\end{equation*}

% \section{Details of Model Parameter Similarity Measurement}
% \label{sec:simi_detail}

% To quantify the degree of domain specialization in the model layers, we first extract the task vectors for each layer from our Math and Code \llama models. We then concatenate the task vectors from the attention layers and FFNs into two long vectors. Next, we calculate the cosine similarity between the two concatenated task vectors. The cosine similarity for the task vectors of the FFNs and self-attention layers is visualized separately in Figure \ref{fig:simi_task_vector}.

% \begin{figure}[!htb]
%      \centering
%      \includegraphics[width=\linewidth]{figure/task_vector_simi.pdf}
%      \caption{\textbf{Similarity of task vector for attention and FFNs layers for Math and Code \llama experts.} We average the similarity of attentions or FFNs in one decoder layers as the overall similarity for each layer.}
%      \label{fig:simi_task_vector}
% \end{figure}

\section{Supplementary Results}
\label{sec:supp_routing}

In this section, we present the supplementary analysis of the routing probability for each research question.

\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{0.48\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/routing_probabilities_MBPP.pdf}
        \caption{MBPP}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/routing_probabilities_Human_eval.pdf}
        \caption{HumanEval}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/routing_probabilities_NQ.pdf}
        \caption{Natural Questions}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/routing_probabilities_TriviaQA.pdf}
        \caption{TriviaQA}
    \end{subfigure}
    \caption{Routing probability of experts on MBPP, HumanEval, Natural Questions and TriviaQA for different merging methods.}
    \label{fig:supp_routing_prob}
\end{figure}


\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{0.48\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/hetero_routing_probabilities_MBPP.pdf}
        \caption{MBPP}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/hetero_routing_probabilities_Human_eval.pdf}
        \caption{HumanEval}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/hetero_routing_probabilities_NQ.pdf}
        \caption{Natural Questions}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/hetero_routing_probabilities_TriviaQA.pdf}
        \caption{TriviaQA}
    \end{subfigure}
    \caption{Routing probability of experts on MBPP, HuamnEval, Natural Questions and TriviaQA for the MoE w/ Olmo and MoE w/ TinyLlama.}
    \label{fig:supp_hetero_routing_prob}
\end{figure}


\begin{figure}[!t]
    % \begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{0.48\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/heuristic_routing_probabilities_GSM8K.pdf}
        \caption{GSM8K}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/heuristic_routing_probabilities_MATH.pdf}
        \caption{MATH}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/heuristic_routing_probabilities_MBPP.pdf}
        \caption{MBPP}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/heuristic_routing_probabilities_Human_eval.pdf}
        \caption{HumanEval}
    \end{subfigure}
%     \caption{\label{fig:routing_heuristic} Routing probability of the proposed heuristics for MATH, GSM8K, MBPP and HumanEval dataset.}
% \end{figure}

    \centering
    \begin{subfigure}[b]{0.48\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/heuristic_routing_probabilities_NQ.pdf}
        \caption{Natural Questions}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/heuristic_routing_probabilities_TriviaQA.pdf}
        \caption{TriviaQA}
    \end{subfigure}
    \caption{\label{fig:routing_heuristic} Routing probability of tow routing heuristics for each dataset.}
\end{figure}

\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{0.48\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/result_token_mbpp.pdf}
        \caption{MBPP}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/result_token_human_eval.pdf}
        \caption{HumanEval}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/result_token_gsm8k.pdf}
        \caption{GSM8K}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/result_token_math.pdf}
        \caption{MATH}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/result_token_nq.pdf}
        \caption{Natural Questions}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/result_token_triviaqa.pdf}
        \caption{TriviaQA}
    \end{subfigure}
    \caption{Performance with varied fine-tuning token numbers across different datasets.}
    \label{fig:result_token}
\end{figure}


% \begin{table}[!htb]
% \centering
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{lccccccc}
% \hline
% Method             & MBPP           & HumanEval      & MATH          & GSM8K         & NQ            & TriviaQA       & Avg.           \\ \hline             
% Upcycling & 18.4            & 12.2           &  7.8         &    12.2       & 8.4  & 37.3          & 16.1           \\ \hline
% \end{tabular}
% }
% \caption{\label{table:upcycling_results} Results of MoE with sparse upcycling method.}
% \end{table}

For the calculation of training cost for each method, we will use the product of the number of model parameters and the number of training tokens as a metric for training cost. We present the training costs for each method featured in Tables \ref{table:homo_results}, \ref{table:moe_wo_finetune}, and \ref{table:hetero_results}.


\begin{table}[!htb]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lc}
\toprule
\textbf{Method} & \textbf{Training Cost (\# B parameters × \# B tokens)} \\
\midrule
Base-1B & 0 \\
Code Expert & 100 \\
Math Expert & 100 \\
Knowledge Expert & 100 \\
Random Routing & 300 \\
Router Fine-Tuning & 300 \\
BTX Merging & 448 (3 × 100 + 3.7 × 40) \\
Ties Merging & 448 \\
Dare Merging & 448 \\
Model Upcycling & 1258 (3.7 × 340) \\
\bottomrule
\end{tabular}
}
\caption{\label{table:1_training_cost} Training cost of methods in Table \ref{table:homo_results}}
\end{table}



\begin{table}[!htb]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lc}
\toprule
\textbf{Method} & \textbf{Training Cost (\# B parameters × \# B tokens)} \\
\midrule
Dare & 100 \\
Ties & 100 \\
Merge Attention & 100 \\
Separate Attention & 100 \\
\bottomrule
\end{tabular}
}
\caption{\label{table:3_training_cost} Training cost of methods in Table \ref{table:moe_wo_finetune}}
\end{table}


\begin{table}[!htb]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lc}
\toprule
\textbf{Method} & \textbf{Training Cost (\# B parameters × \# B tokens)} \\
\midrule
Base-1B & 0 \\
Base TinyLlama & 0 \\
Base Olmo & 0 \\
Code Expert & 100 \\
Math TinyLlama & 100 \\
Math Olmo & 100 \\
Knowledge Expert & 100 \\
3-expert MoE & 312 (2 × 100 + 2.8 × 40) \\
(Ours) MoE w/ Math Olmo & 448 \\
(Ours) MoE w/ Math TinyLlama & 448 \\
\bottomrule
\end{tabular}
}
\caption{\label{table:4_training_cost} Training cost of methods in Table \ref{table:hetero_results}}
\end{table}