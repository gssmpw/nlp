\section{Methodology}


\begin{figure}[!t]
     \centering
     \includegraphics[width=\columnwidth]{figure/moe_homo.pdf}
     \caption{\textbf{Overview of the proposed MoE framework for homogeneous model merging}. We replace averaging with Dare or Ties merging to reduce parameter interference. Additionally, we introduce novel routing heuristics to enhance performance without fine-tuning.}
     \label{fig:moe_homo}
\end{figure}

We define our research problem as follows: Given $l$ dense expert models with parameters $[\theta_1, \theta_2, \dots, \theta_l]$, each pretrained on different domains, we aim to propose an efficient merging method to combine these dense models into an MoE with parameters $\theta_m = \text{Merge}(\theta_1, \theta_2, \dots, \theta_l)$ , optimizing performance across all domains.

We now present our approach for MoE merging with homogeneous and heterogeneous expert models. 
%To address current limitations in MoE merging, we propose separate frameworks for homogeneous and heterogeneous experts.
First, for MoE merging with homogeneous experts (Section~\ref{s:homogeneous-model-merging}), we propose replacing existing averaging with more advanced merging methods to deal with parameter interference, and introduce sequence-level routing heuristics to enhance MoE performance without post-merge fine-tuning.
Second, we introduce a novel framework for MoE merging with heterogeneous experts (Section~\ref{sec:merge_with_hetero}), which uses projectors to unify expert inputs and outputs, and a sequence-level router. 

\subsection{Homogeneous Model Merging}
\label{s:homogeneous-model-merging}

First, we describe the basic merging setup (Section~\ref{ss:standard-merging-btx}) and then summarize our extensions to resolve parameter interference (Section~\ref{ss:replacing-average-merging}) and address the need for MoE fine-tuning (Section~\ref{sec:merging_wo_ft}). The overall pipeline is visualized in Figure \ref{fig:moe_homo}. 


\subsubsection{Merging Setup}
\label{ss:standard-merging-btx}

Our merging setup is similar to the BTX~\cite{sukhbaatar2024branchtrainmixmixingexpertllms}, where it merges all non-FFN layers (embedding, attention, normalization, and head) of experts by unweighted averaging and keeps the FFNs separate.
As in standard MoE architectures, a router network, implemented as a Multilayer Perceptron (MLP), is inserted between the attention and FFN layers for token-level routing, selecting the top $K$ (usually 1 or 2) experts for each layer among all $l$ experts. The output of FFN layers $\text{FF}_{MoE}(v)$ of token embedding $v$ is formulated as:
\begin{equation*}
    \text{FF}_{MoE}(v) = \sum_{i=1}^K \text{SoftMax}(\text{top-K}(\theta_r v))\text{FF}_i(v)
\end{equation*}
where $\theta_r$ is the parameter of the router network and $\text{FF}_{i}(v)$ is the output of each FFN experts for token $v$.
After merging experts into a single MoE, BTX fine-tunes all parameters, including the router parameters on a mix of training data from all experts.

\subsubsection{Addressing Parameter Interference}
\label{ss:replacing-average-merging}
The major pitfall of the unweighted merging is that there exists parameter interference, as explored in the previous work on dense model merging \cite{yu2024language, yadav2024ties}. As suggested in Figure \ref{fig:model_interference}, when influential parameters (large magnitude parameters) in the task vector merge with redundant parameters (small magnitude parameters) or parameters with sign conflict, simple averaging will output a small magnitude parameter, which may reduce the effect of the original task vector.

\begin{figure}[!htb]
     \centering
     \includegraphics[width=\linewidth]{figure/model_interference.pdf}
     \caption{Different types of parameter interference and merged outputs produced by simple averaging.}
     \label{fig:model_interference}
\end{figure}

%Our overall pipeline for merging homogeneous experts still follows the BTX pipeline: branch the seed model, CPT seed models in parallel on different domain data, merge the non-FFN layers of all CPTed models, and fine-tune the MoE on a small scale of data. 
In contrast to BTX, we mitigate model interference by employing previous SoTA methods in this MoE setup, namely Dare and Ties. First, we calculate the task vector $\tau_i = \theta_{b} - \theta_i$ with the base model parameter $\theta_b$ and the parameter $\theta_i$ for the model CPTed on domain $i$. 
For Ties merging, we first drop the bottom $(100 - p) \%$ of the redundant parameters (smallest magnitude) by resetting them to 0. For each parameter, we determine the sign with the highest total magnitude in all task vectors and sum all task vectors together to $\tau_m$ but only by keeping the parameter values whose signs are the same as the determined sign.
For Dare merging, we randomly drop the $(100 - p) \%$ parameters. We rescale each task vector with $\tau_i = \frac{\tau_i}{0.01p}$. We sum all task vectors to $\tau_m$.
Finally, we add the summed task vector back to the base model with the scaling term $\lambda$ and obtain the merged layer parameters: $\theta_m = \theta_b + \lambda \cdot \tau_m$. We expect that the drop operation in both methods will address the parameter interference issue, as revealed in dense model merging, and produce a consistent performance boost \cite{yu2024language, yadav2024ties}. 

Similar to BTX, after combining each expert model into an MoE, we fine-tune all parameters in the MoE in the fine-tuning stage. By addressing parameter interference, our approach achieves performance improvements over BTX especially in earlier stages of fine-tuning. Next, we describe how to further reduce the fine-tuning needs.


\subsubsection{Reducing Fine-Tuning Needs}
\label{sec:merging_wo_ft}
% From the previous parts, we propose the solution for the first limitation (use unweighted averaging merging) of the current MoE expansion method. 
Fine-tuning MoEs is expensive due to the communication cost between GPUs \cite{sukhbaatar2024branchtrainmixmixingexpertllms}. Previous MoE merging methods require substantial fine-tuning of the MoE parameters to train the router network. In this section, we propose two techniques to reduce reliance on MoE fine-tuning, namely a perplexity-based routing and separating the attention layers.

% The overall pipeline of MoE after merging is suggested as Figure \ref{fig:moe_homo}. However, we use the routing heuristic to replace the router network in the figure. Moreover, we also propose two merging options to serve as the merged layers. For each input, we first use the routing heuristic to determine which experts to use and the corresponding weights, then feed the input to the decided experts, and combine the expert outputs with the computed weights from the routing heuristic.

The overall MoE pipeline after merging is illustrated in Figure \ref{fig:moe_homo}, but we replace the router network with our routing heuristic to determine the expert selection. Additionally, we separate attention layers without merging them. For each input, the routing heuristic selects the appropriate experts and assigns their weights. The input is then processed by the chosen experts, and their outputs are combined using weights.

\paragraph{Routing Heuristics}

% As discussed in Section \ref{sec:intro}, our aim is to develop the routing heuristic to replace the previous routing network without touching the training data. With only the inference input, we propose two routing heuristics: task vector routing and perplexity (PPL) routing. Moreover, we notice that it is usually challenging to develop the token-level routing method with certain heuristics, so all two heuristics mentioned below are sequence-level heuristics, which means that we route all tokens to the same experts.

Our goal is to develop routing heuristics that replace the routing network without accessing the training data. We propose a sequence-level heuristics: perplexity (PPL) routing with only access to the inference sentence.


Our approach assesses the confidence of expert models by utilizing perplexity (PPL) to estimate their uncertainty. We then select the experts with the lowest PPL values, indicating higher confidence \cite{jelinek1977perplexity}.
Formally, with the inference input $x_{inf}$ with $t$ tokens and the expert parameter $\theta_i$ for expert $i$, we compute the PPL value $\text{PPL}(x_{inf}, \theta_i)$ as below:
\begin{equation*}
\resizebox{\columnwidth}{!}{%
    $\text{PPL}(x_{inf} \mid \theta_i) = \exp\left(-\frac{1}{t} \sum_{j=1}^{t} \log P(x_j \mid x_{<j}, \theta_i)\right)
    $%
    }
\end{equation*}
where $P(x_j \mid x_{<j}, \theta_i)$ is the probability assigned by model $\theta_i$ on $j$-th token, given previous tokens.

Since a higher PPL indicates greater uncertainty, we use the reciprocal of PPL values to represent the model's confidence. With the top-K routing, the selected experts and their weights $\alpha$ can be computed as follows:

\begin{equation*}
    \resizebox{\columnwidth}{!}{%
        $\alpha = \text{SoftMax}(\text{top-K}(\frac{1}{\text{PPL}(x_{inf} \mid \theta_1)}, \dots, \frac{1}{\text{PPL}(x_{inf} \mid \theta_l)}))$%      
        }
\end{equation*}

Additionally, we also propose another routing heuristic based on the task vector and we present the details of this heuristic in Appendix \ref{sec:task_vector_routing}. With the routing heuristics and the corresponding computed weights from the heuristic, we will present the detailed merging process to form the MoE without further fine-tuning.

\paragraph{Separating attention layers}

% We consider two merging options to form the MoE models. The first option is the same as the BTX pipeline \cite{sukhbaatar2024branchtrainmixmixingexpertllms}, where we merge all non-FFN layers, including embedding, attention, normalization, and head layers. However, this merging method without fine-tuning will cause inconsistency between attention output and FFNs output. With $l$ dense experts, the merged attention layers are influenced by all $l$ task vectors, which means that the output of the attention layers is affected by all task vectors. However, since we apply the top-k routing method, the output of FFNs is influenced only by $k$ task vectors. This discrepancy between the number of task vectors affecting the attention layers and the FFNs results in inconsistent output.

%We consider two options for merging to form the MoE models. The first follows the BTX pipeline \cite{sukhbaatar2024branchtrainmixmixingexpertllms}, where all non-FFN layers (embedding, attention, normalization, and head) are merged. 
We hypothesize that by merging attention layers, BTX creates inconsistency between the attention and FFN outputs. Specifically, the merged attention layers are influenced by all $l$ task vectors from the dense experts, while the top-k routing method limits the FFN output to only $k$ task vectors, leading to mismatched outputs.
% To overcome this challenge, for our second merging option, we do not merge the attention layer but only merge the embedding, normalization, and head layers where the parameters do not change much during the fine-tuning from our preliminary analysis. In this way, the selected attention layers and the FFN layers come from the same expert, eliminating the discrepancy caused by the inconsistent number of task vectors.
To address this, we consider keeping experts' attention layers as separate, similar to FFN. This ensures that both the attention and FFN layers come from the same expert, eliminating discrepancies from inconsistent task vector counts.

\subsection{Heterogeneous Model Merging}
\label{sec:merge_with_hetero}
This section describes how to merge models with different architectures into a unified MoE. 
Previous MoE merging techniques cannot be directly used in this setting, as it is not possible to merge non-FFN networks layer by layer when experts have different numbers of layers or different layer shapes. 
To resolve this challenge, we propose a new merging method, which introduces projector layers and sequence-level routing as shown in Figure~\ref{fig:moe_hetero}. 
%We only merge the embedding and head layers but keep all other layers, use a router network to choose the expert for each input, and add projectors to translate the embedding output to be compatible with expert hidden dimension. 
%Figure \ref{fig:moe_hetero} shows an overview of our framework with heterogeneous models.


% \begin{figure}[!htb]
%      \centering
%      \includegraphics[width=\linewidth]{figure/model_wo_finetune.pdf}
%      \caption{\textbf{Overview of the proposed MoE framework without further fine-tuning}}
%      \label{fig:moe_hetero}
% \end{figure}

\begin{figure}[!htb]
     \centering
     \includegraphics[width=\linewidth]{figure/hetero_moe.pdf}
     \caption{\textbf{Overview of the proposed MoE framework for heterogeneous experts.} Each color represents one heterogeneous expert. $n_1, \cdots, n_4$ refers to the number of layers in each expert.}
     \label{fig:moe_hetero}
\end{figure}

First, we denote the hidden dimension of all $l$ experts as $d_1, d_2\dots, d_l$, and the maximum dimension among them is $d_m$. Suppose that we have a vocabulary $\mathcal{V}$ and an input sentence with tokens $[v_1, v_2 \dots, v_t]$. For the shared embedding layer $\mathcal{M}_e$, it maps the token $v_i$ in the sentence to embedding $e_i \in \mathbb{R}^{d_m}$ and the shared head layer is the network $\mathcal{M}_{h}: \mathbb{R}^{d_m} \rightarrow \mathbb{R}^{\vert \mathcal{V} \vert}$, which maps the weighted sum of projectors back to the probability distribution of tokens in the vocabulary. The embedding and head layer parameters are initialized from an averaging of the embedding and head layers of each expert. For experts with a hidden dimension less than $d_m$, we add padding zeros for their embedding and head layers before averaging. 

% For the router network, since we do not merge attention layers due to heterogeneous experts, we have to route every token in the input to the same expert. Otherwise, the attention layers cannot perform the self-attention operation because the attention operation requires the attention layers to access every token. Hence, we first do the averaging for each token embedding and use router to determine which experts to apply and the corresponding weights. Formally, for the top-$k$ routing and router parameters $\theta_r$, the router computes the model weights as below:

Since we do not merge attention layers due to heterogeneous experts, all tokens must be routed to the same expert. Otherwise, the attention layers cannot perform self-attention, as they require access to every token. Hence, we average the token embeddings and use the router to perform the sequence-level routing. Formally, for top-$K$ routing with router parameters $\theta_r$, the router computes the model weights as follows:
\begin{equation*}
    \alpha = \text{SoftMax}(\text{top-K}(\theta_r \text{avg}(e_1, e_2, \dots, e_t)))
\end{equation*}

For projectors: Proj-in and Proj-out, for each expert, randomly initialized MLP layers, they project the embedding outputs to the dimension of each expert, and project the expert output back to the maximum dimension. For $i$-th expert, we define:
\begin{equation*}
\resizebox{\columnwidth}{!}{%
$
\text{Proj-in layer}: \mathbb{R}^{d_m} \rightarrow \mathbb{R}^{d_i}, \quad \text{Proj-out layer}: \mathbb{R}^{d_i} \rightarrow \mathbb{R}^{d_m}
$
}
\end{equation*}

% After we use the chosen $k$ experts to process the input tokens and the Proj-out layer to translate the expert output to the representation $r_i$ with dimension $d_m$, we combine the translated representations with the weights from the router, which is $r_{combined} = \sum_{i=1}^k \alpha_i r_i$.
% Finally, we feed the combined representation $r_{combined}$ to head layer to obtain the token probability. Note that after merging to the MoE model with heterogeneous experts, we also fine-tune all the parameters during the fine-tuning stage. For tokenizer selection, we simply chose an arbitrary tokenizer, followed the previous work \cite{roberts2024pretrained}, and update the parameters for our embedding and LM head layers during the fine-tuning stage. 

After using the selected $K$ experts to process the input sequences and translating their outputs to the representation $r_i$ via the Proj-out layer (with dimension $d_m$), we combine the representations using the router's weights: $\sum_{i=1}^K \alpha_i r_i$. The combined representation is then fed into the head layer to obtain the token probabilities. 

After merging the heterogeneous experts into the MoE model, we choose an arbitrary tokenizer from one expert, following previous work \cite{roberts2024pretrained} and fine-tune all parameters.