% The Mixture-of-Experts (MoE) merging method, where multiple dense expert models are trained in parallel and merged into a unified Mixture-of-Experts (MoE) model, has shown notable success in enhancing large language model (LLM) performance across new domains. 
% However, the key limitation remains: the method employs unweighted averaging for merging non-FFN layers from the homogeneous experts (same architectures) and is inapplicable for heterogeneous experts (different architectures). Moreover, the method requires substantial fine-tuning post-merging, increasing computation costs and making it impractical when training data is inaccessible.

% We propose to alleviate these issues with the corresponding solutions: for homogeneous expert merging, we replace simple averaging with more sophisticated merging methods to reduce parameter interference. Furthermore, we develop two routing heuristics that select experts and merging strategies to form the MoE without fine-tuning. For heterogeneous expert merging, we introduce a novel MoE architecture that includes additional projectors and a sequence-level routing network to unify the outputs from experts with different architectures.
% The experiments demonstrate the superiority of our methods in merging both homogeneous and heterogeneous experts across multiple tasks in each setting.

The recent success of specialized Large Language Models (LLMs) in domains such as mathematical reasoning and coding has led to growing interest in methods for merging these expert LLMs into a unified Mixture-of-Experts (MoE) model, with the goal of enhancing performance in each domain while retaining effectiveness on general tasks. 
However, the effective merging of expert models remains an open challenge, especially for models with highly divergent weight parameters or different architectures. 
State-of-the-art MoE merging methods only work with homogeneous model architectures and rely on simple unweighted averaging to merge expert layers, which does not address parameter interference and requires extensive fine-tuning of the merged MoE to restore performance. 
To address these limitations, this paper introduces new MoE merging techniques, including strategies to mitigate parameter interference, routing heuristics to reduce the need for MoE fine-tuning, and a novel method for merging experts with different architectures. Extensive experiments across multiple domains demonstrate the effectiveness of our proposed methods, reducing fine-tuning costs, improving performance over state-of-the-art methods, and expanding the applicability of MoE merging.

% Model-generated shortened version of the abstract:


%Creating Mixture-of-Experts (MoE) models via parallel training, where multiple expert models are trained in parallel and merged, has shown promise in improving large language model (LLM) performance across new domains. However, key challenges persist: the method utilizes the unweighted averaging in non-FFN layer merging, which leads to parameter interference in homogeneous experts and is unsuitable for heterogeneous experts. Furthermore, it requires significant fine-tuning post-merging increases computational cost and is impractical without access to training data.
%We address these issues with the corresponding solutions. In homogeneous expert merging, we introduce improved merging strategies to reduce interference. Additionally, we propose two routing heuristics that enable fine-tuning-free merging by optimizing expert selection and merging consistency. In heterogeneous expert merging, we propose a novel MoE architecture with projectors and a sequence-level routing network. Experiments demonstrate the effectiveness of this framework in both homogeneous and heterogeneous expert settings across multiple tasks.
