\section{Experiments Setup and Model Analysis}
\label{sec:experiments}
Through our extensive empirical analysis, we aim to evaluate our frameworks in the settting of homogeneous experts and heterogeneous experts.
% \begin{itemize}[leftmargin=*]
%     \item \textbf{RQ1}: Will more advanced merging methods mitigate parameter interference and bring more performance improvement for homogeneous model merging?
%     \item \textbf{RQ2}: Will our merging methodology for heterogeneous experts obtain an effective MoE?
%     \item \textbf{RQ3}: How well do our routing heuristics and merging options work for the merged MoE without fine-tuning? 
% \end{itemize}

\subsection{Evaluation Dataset}
We evaluate our proposed methodology on 6 datasets from three domains, as in the previous work \cite{sukhbaatar2024branchtrainmixmixingexpertllms}. For math reasoning, we choose GSM8K (8-shot) and MATH (4-shot) \cite{cobbe2021training, hendrycks2021measuring}. For code generation, we choose MBPP (0-shot) and HumanEval (0-shot) \cite{chen2021evaluating, austin2021program}. For world knowledge, we choose Natural Questions (NQ, 5-shot) and TriviaQA (5-shot) \cite{kwiatkowski2019natural, joshi2017triviaqa}.

\subsection{Model Configuration}
\label{sec:model_pretraining}
This section describes the base model and experts discussed in our experiments:

\begin{itemize}[leftmargin=*]
\item \textbf{Base Model (\llamab}): This is our base model with 1B parameters and Llama-like architecture. We pretrain \llamab from scratch with 250 billion (250B) tokens from the following datasets from the RedPajama dataset~\cite{together2023redpajama}: Arxiv, CommonCrawl, C4, StackExchange data and the first half of the WikiPedia data in the RedPajama dataset. 
    \item \textbf{Math \llama}: We CPT the Base model on the OpenWebMath data for 100B tokens ~\cite{paster2023openwebmath}.
    \item \textbf{Code \llama}: We use the GitHub data in RedPajama to CPT the Base model for 100B tokens.
    \item \textbf{Knowledge \llama}: We CPT the \llamab model on the second half of the Wikipedia data in the RedPajama dataset for 100B tokens.
    \item \textbf{Math TinyLlama and Math Olmo}: We CPT the TinyLlama-1.1B model \cite{zhang2024tinyllama} and Olmo-1B model \cite{groeneveld2024olmo} on the same data mixture of the Math \llama.
    \item \textbf{Mixture of Experts (MoE)}: 
    % For homogeneous model merging experiments, we merge three experts (Math \llama, Code \llama and Knowledge \llama) and one base model (\llamab) above to an MoE model. For heterogeneous model merging experiments, we merge Code \llama, Knowledge \llama, \llamab and Math TinyLlama or Math Olmo to an MoE with method proposed in Section \ref{sec:merge_with_hetero}.  For MoE fine-tuning we utilize all the data sources for the base and expert models and fine-tune the MoE for other 40B tokens. The detailed sampling ratio for pretraining and fine-tuning can be found in Appendix \ref{sec:data_mix}.
    For homogeneous model merging, we combine three experts (Math \llama, Code \llama, Knowledge \llama) and one base model (\llamab) into an MoE. For heterogeneous merging, we combine Code \llama, Knowledge \llama, \llamab, and either Math TinyLlama or Math Olmo. MoE fine-tuning is performed on all data sources from the base and expert models, using an additional 40B tokens. Detailed sampling ratios for pretraining and fine-tuning are provided in Appendix \ref{sec:data_mix}.
\end{itemize}

We present the details of model architecture for each expert in Appendix \ref{sec:implement}.

% For merging experiments with heterogeneous model architectures, we choose the open-source pretrained TinyLlama-1.1B model \cite{zhang2024tinyllama} and the pretrained Olmo-1B model \cite{groeneveld2024olmo}. We CPT the TinyLlama and Olmo models on the same data mixture of the Math \llama and obtain another two math experts with different architectures. We name these two models as \textbf{Math Olmo} and \textbf{Math TinyLlama} respectively.

\subsection{Baseline Methods}
\label{sec:baseline}
To demonstrate the effectiveness of our methodology, we compare the performance of the merged 4-expert MoE models with several other baselines.

\begin{itemize}[leftmargin=*]
    \item \textbf{Base \& Experts}: The dense base and expert models in Section \ref{sec:model_pretraining}.
    % \item \textbf{Dense}: The dense Llama2 model CPTed on the same training data (340B tokens) as the MoE model, including data in the pretraining stage and the final fine-tuning stage.
    
    % \item \textbf{Sparse Upcycling} \cite{komatsuzaki2022sparse}: This baseline initializes from the base model and make 4 identical copies of FFNs layers to make the MoE model. Then we CPT the initialized MoE model on the same training data (340B tokens) as the proposed pipeline.
    
    % \yz{yz: will add these two baselines once we get the evaluation number.}
    
    \item \textbf{BTX} \cite{sukhbaatar2024branchtrainmixmixingexpertllms}: The MoE model derived from the BTX pipeline with average merging and post-merge fine-tuning.
    \item \textbf{Random Routing}: The average merged MoE with randomly initialized router.
    \item \textbf{Router Fine-tuning}: The MoE model derived from the BTX pipeline but only fine-tune the parameters in the router network.
    \item \textbf{3-expert MoE}: 
    % To show the functionality of Math Olmo or Math TinyLlama in heterogeneous expert merging. We prepare the 3-expert MoE models (Base, Knowledge \llama, Code \llama) fine-tuned on the same data source (including math) or only code- and knowledge-related data. We merge the models to MoE with BTX method. We name is as \textbf{3-expert MoE (same data)} and \textbf{3-expert (w/o) math} respectively.
    To demonstrate the functionality of Math Olmo or TinyLlama in heterogeneous expert merging, we prepare 3-expert MoE models (Base, Knowledge \llama, Code \llama), fine-tuned either on the full data source (including math) or only on code- and knowledge-related data. We merge these models using the BTX method, naming them \textbf{3-expert MoE (same data)} and \textbf{3-expert MoE (w/o math)}.
    \item  \textbf{Dare Dense} \cite{yu2024language}, \textbf{Ties Dense} \cite{yadav2024ties}: Advanced dense model merging method. We apply Dare or Ties to merge four LMs to one dense model.
\end{itemize}

The details of the model configuration of the baseline methods are included in Appendix \ref{sec:implement}.

\subsection{Similarity of Model Parameters}

Before presenting the performance of our proposed methodology, we first analyze the similarities in model parameters across different experts to demonstrate the necessity for alternatives to average merging. Previous work assumes that parameters in attention layers are less domain-specialized, leading to the use of simple averaging when combining non-FFN layers \cite{sukhbaatar2024branchtrainmixmixingexpertllms}. Our analysis aims to verify whether this assumption holds true for experts trained on different domains.

% In Figure \ref{fig:simi_task_vector} of Appendix \ref{sec:simi_detail}, we visualize the similarity between task vectors from our Math and Code \llama  for both self-attention and FFN layers. 

To quantify the degree of domain specialization in the model layers, we first extract the task vectors for each layer from our Math and Code \llama models. We then concatenate the task vectors from the attention layers and FFNs into two long vectors. Next, we calculate the cosine similarity between the two concatenated task vectors. The cosine similarity for the task vectors of the FFNs and self-attention layers is visualized separately in Figure \ref{fig:simi_task_vector}.

\begin{figure}[!htb]
     \centering
     \includegraphics[width=\linewidth]{figure/task_vector_simi.pdf}
     \caption{\textbf{Similarity of task vector for attention and FFNs layers for Math and Code \llama experts.} We average the similarity of attentions or FFNs in one decoder layers as the overall similarity for each layer.}
     \label{fig:simi_task_vector}
\end{figure}

We observe that the task vectors from both layers exhibit low similarity, suggesting that the assumption of similar attention layers does not consistently hold and parameter interference may occur. This analysis demonstrates the need for more advanced merging methods, rather than averaging, for homogeneous model merging. 
% More details on similarity measurement can be found in Appendix \ref{sec:simi_detail}.

% Before presenting the performance of our proposed methodology, we first analyze the similarities of model parameters between different experts to show the necessity of replacing the average merging. Previous work holds the assumption that the parameters in self-attention layers are less domain specialized, so they use simple averaging to combine attention layers \cite{sukhbaatar2024branchtrainmixmixingexpertllms}. Our analysis will verify whether this assumption holds for our experts from different domains.

% To quantify how much domain specialized for the model layers, we first extract the task vector of our Math and Code \llama for each model layer. Then, we calculate the similarity between two task vectors. We visualize the cosine similarity of the task vector for the FFNs and the self-attention layers in Figure \ref{fig:simi_task_vector}, respectively.

% \begin{figure}[!htb]
%      \centering
%      \includegraphics[width=\linewidth]{figure/task_vector_simi.pdf}
%      \caption{\textbf{Similarity of task vector for attention and FFNs layers for Math and Code \llama experts.} We average the similarity of attentions or FFNs in one decoder layers as the overall similarity for each layer.}
%      \label{fig:simi_task_vector}
% \end{figure}
