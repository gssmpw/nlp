\section{Results}

\subsection{Homogeneous Model Merging}

\subsubsection{Averaging vs. Dare / Ties}
\label{sec:rq1}
% We merge 4 models (Base, Math \llama, Code \llama, Knowledge \llama) with the same architecture into a unified MoE with different merging methods and fine-tune it as described in Section \ref{sec:model_pretraining}. 

\textbf{Replacing simple averaging with Dare or Ties merging obtains better performance.} \quad
In this section, we demonstrate the superiority of our proposed Ties and Dare merging MoE over the BTX merging method.
We present the performance of MoE models with \textbf{Dare merging} or \textbf{Ties merging} on non-FFN layers and other baselines in Table \ref{table:homo_results}. The details of training cost for each method are presented in Table \ref{table:1_training_cost} in Appendix.

\begin{table}[!htb]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lccccccc}
\hline
Method             & MBPP           & HumanEval      & MATH          & GSM8K         & NQ            & TriviaQA       & Avg.           \\ \hline
\multicolumn{8}{c}{\textbf{Dense Model}}                                                                                               \\ \hline
\llamab & 4.60            & 3.04           & 2.42          & 1.44          & \textbf{6.61} & 26.72          & 7.47           \\ \hline
Code \llama        & \textbf{10.2}  & \textbf{8.53}  & 2.42          & 2.57          & 3.11          & 16.70           & 7.26           \\ \hline
Math \llama        & 9.80            & 6.71           & \textbf{7.81} & \textbf{6.36} & 5.48          & 19.86          & \textbf{9.34}  \\ \hline
Knowledge \llama   & 3.60            & 4.26           & 2.62          & 2.04          & 5.65          & \textbf{28.71} & 7.81           \\ \hline
\multicolumn{8}{c}{\textbf{MoE Merging}}                                                                                                 \\ \hline
Random Routing     & 4.00           & 6.10           & 2.78          & 2.05          & 4.86          & 21.75          & 6.92           \\ \hline
Router Fine-tuning & 3.60           & 6.71           & 2.42          & 2.96          & 5.82          & 25.98          & 7.92           \\ \hline
BTX merging        & 12.40          & 11.58          & 6.74          & 7.73          & \textbf{6.78} & 25.10           & 11.72          \\ \hline
Ties merging       & 14.20          & \textbf{11.98} & 6.74          & 7.81          & 6.72          & 27.66          & 12.52          \\ \hline
Dare merging       & \textbf{14.20} & 10.98          & \textbf{6.82} & \textbf{7.96} & 6.50           & \textbf{30.68} & \textbf{12.86} \\ \hline
\multicolumn{8}{c}{\textbf{MoE from Scratch}}                                                                                                \\ \hline
MoE Upcycling & 18.40  & 12.20 & 7.80 & 12.21 & 8.37  &  37.33 & 16.05 \\
\hline
\end{tabular}
}
\caption{\label{table:homo_results} \textbf{Performance of proposed Dare and Ties merged MoE and other baselines across six datasets.} The best performance of Dense and MoE model is marked in bold. Results of Dare and Ties merged MoE outperform the BTX MoE and other baseline methods.}
\end{table}

% Regarding dense model performance, from Table \ref{table:homo_results}, we find that individual expert LLMs achieve the best performance in their respective domain in most cases, as expected. However, CPTed \llama models also suffer from catastrophic forgetting. For example, both Code and Math \llama behaved worse than the \llamab model on the TriviaQA and NQ datasets.

From Table \ref{table:homo_results}, we see that individual experts generally achieve the best performance in their respective domains, as expected. However, CPTed \llama models experience catastrophic forgetting. For instance, both Code and Math \llama perform worse than \llamab on the TriviaQA and NQ datasets.


% The results of the MoEs in Table \ref{table:homo_results} suggest that by using the Ties or Dare merging to replace the average merging method, the proposed MoE performance exceeds the MoE from the BTX pipeline in almost all datasets and obtains a relative improvement of 6.94\% and 9.72\% compared to the BTX method in average performance. This observation implies that advanced merging methods mitigate the interference of model weights during merging and increase performance. 

The results in Table \ref{table:homo_results} show that using Ties or Dare merging significantly improves MoE performance over the BTX pipeline across almost all datasets, with a relative improvement of 6.94\% and 9.72\% in average performance. This suggests that advanced merging methods reduce weight interference and enhance performance.

% In addition to these baseline results, we include the results of MoE sparse upcycling \cite{komatsuzaki2022sparse} in the last row as a reference. In this approach, the MoE model is initialized from the base model by creating four identical copies of the FFN layers. We then CPT the initialized MoE model on the same training data (340B tokens) as used in the proposed pipeline. Note that we do not compare our results with the upcycling method, since that method pretrains the entire MoE on all data with significantly higher additional cost.

As a reference, we include the results of MoE sparse upcycling \cite{komatsuzaki2022sparse} in the last row of Table \ref{table:homo_results}. This approach initializes the MoE model by creating four identical copies of the FFN layers from the base model and then CPT on the same 340B tokens used in our pipeline. However, we do not directly compare our results with the upcycling method, as it involves pretraining the entire MoE on all data, incurring significantly higher costs.
We also visualize the average performance for each merging method with different fine-tuning token numbers in Figure \ref{fig:result_token} in Appendix \ref{sec:supp_routing}.
In Figure \ref{fig:result_token}, we observe that the Dare and Ties merging MoE models consistently outperform the BTX merging MoE throughout fine-tuning, especially in the earlier stages of fine-tuning. 
% There is more performance gain when fine-tuning with a smaller number of tokens, which follows our expectation since we regard Dare or Ties-merging methods as a better initialization method and have a larger effect during the early stage of training. This observation implies that more advanced merging techniques should be preferred over unweighted average especially when the fine-tuning budget is limited.  

\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{0.48\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/routing_probabilities_GSM8K.pdf}
        \caption{GSM8K}
        \label{fig:gsm8k_route}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/routing_probabilities_MATH.pdf}
        \caption{MATH}
        \label{fig:math_route}
    \end{subfigure}
    \vspace{-0.5em}
    \caption{Routing probability of experts on GSM8K and MATH for different merging methods.}
    \label{fig:routing_prob}
\end{figure}


\noindent \textbf{MoE with Dare or Ties merging routes more tokens to domain experts.}\quad
To further explore the effectiveness of Dare and Ties merging MoE, we evaluate MoEs on multiple benchmarks and calculate the routing probability averaged from each layer and token. We visualize the routing probability of each method of two math datasets (MATH and GSM8K) in Figure \ref{fig:routing_prob} and for other datasets, we put the results in Figure \ref{fig:supp_routing_prob} in Appendix \ref{sec:supp_routing}.


Compared to MoEs with BTX merging, where the base model accepts the most routing decisions, the Dare and Ties merging method routes tokens to domain experts more frequently, as suggested in Figure \ref{fig:routing_prob}. For example, for the GSM8K dataset, the routing probability for math expert increases from 0.28 to 0.35 or 0.46 when replacing simple averaging with the Ties or Dare merging. This finding suggests that the more effective MoE with the more advanced merging method should be attributed to more optimized routing decisions.


\subsubsection{Merging without Fine-tuning}

In this part, we will evaluate our proposed routing heuristics in Section \ref{sec:merging_wo_ft} for MoE without fine-tuning.
Before we evaluate the overall performance of each benchmark, we will first examine the routing decision with our proposed heuristics. We present the routing probability for PPL routing heuristics for each dataset in Table \ref{table:routing_heuristic}.

\begin{table}[!htb]
\centering
\resizebox{0.8\columnwidth}{!}{%
\begin{tabular}{lcccc}
\hline
% \multicolumn{5}{c}{Heuristic 1: Task Vector Routing}     \\ \hline
Benchmark      & Base       & Code          & Math          & Knowledge     \\ \hline
GSM8K     & 23\%       & 2\%           & \textbf{43\%} & {\ul 32\%}    \\ \hline
MATH      & 22\%       & 2\%           & \textbf{49\%} & {\ul 27\%}    \\ \hline
MBPP      & 19\%       & {\ul 22\%}    & \textbf{44\%} & 15\%          \\ \hline
HumanEval & 5\%        & {\ul 43\%}    & \textbf{45\%} & 7\%           \\ \hline
NQ        & {\ul 43\%} & 4\%           & 10\%          & \textbf{43\%} \\ \hline
TriviaQA  & {\ul 50\%} & 0\%           & 0\%           & \textbf{50\%} \\ \hline
\end{tabular}
}
\caption{\label{table:routing_heuristic} \textbf{Routing probability of PPL routing for each dataset.} The largest probability are in bold, and the second-largest are underlined.}
\end{table}

\noindent \textbf{Routing heuristic effectively assigns tokens to the corresponding experts.} \quad 
Table \ref{table:routing_heuristic} demonstrates that PPL routing generally achieves the desired routing patterns, effectively directing inputs from a specific domain to the specialized expert models, except in the case of the MBPP dataset. Since our heuristics rely solely on inference inputs without fine-tuning, they can be considered reliable strategies. We also visualize the routing probability for both PPL and task vector routing heuristics for each dataset in Figure \ref{fig:routing_heuristic} in Appendix \ref{sec:supp_routing}. We find that PPL routing consistently produces better results than the task vector routing.

Next, we evaluate the performance on each dataset with different combinations of merging methods and routing heuristics, compared to the baseline methods. We prepare three dense fine-tuning baselines: \textbf{Dare Dense}, \textbf{Ties Dense} and \textbf{Random Routing} (details in Section \ref{sec:baseline}). We also evaluate the ablation methods: merging attention layers without separation and task vector routing. We present the results of each method across datasets in Table \ref{table:moe_wo_finetune}. The details of training cost for each method are presented in Table \ref{table:3_training_cost} in Appendix.

\begin{table}[!htb]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{llccccccc}
\hline
Merging      & Routing     & MBPP         & HumanEval     & MATH          & GSM8K         & NQ            & TriviaQA       & Avg.          \\ \hline
\multicolumn{9}{c}{\textbf{Dense Merging}}                                                                                                          \\ \hline
Dare       & N/A         & 6.20         & 6.70          & 2.22          & 2.27          & 4.80          & 20.45          & 7.11          \\ \hline
Ties        & N/A         & 6.00         & 6.70          & 2.48          & 2.19          & 3.62          & 20.86          & 6.98          \\ \hline
\multicolumn{9}{c}{\textbf{MoE Merging}}                                                                                                            \\ \hline
Merge attention     & random      & 4.00         & 6.10          & 2.78          & 2.05          & 4.86          & 21.75          & 6.92          \\ \hline
Merge attention     & task vector & 6.60          & 4.87          & 3.06          & 1.44          & \textbf{6.05} & 21.39          & 7.24          \\ \hline
Merge attention     & PPL         & 6.40          & 4.87          & 2.86          & 1.13          & 5.93          & 22.71          & 7.32          \\ \hline
Separate attention & task vector & 4.00            & 7.32          & \textbf{2.98} & 2.5           & 5.37          & 20.11          & 7.05          \\ \hline
Separate attention & PPL         & \textbf{6.80} & \textbf{7.92} & 2.88          & \textbf{2.95} & 4.74          & \textbf{23.21} & \textbf{8.08} \\ \hline
\end{tabular}
}
\caption{\label{table:moe_wo_finetune} \textbf{Performance of proposed merging and routing methods for MoE without substantial fine-tuning and other baselines across six datasets.} Separating attention layers and perplexity routing heuristics get the best average performance.}
\end{table}

% \noindent \textbf{Proposed MoE method without fine-tuning perform better then dense merging baseline.} \quad From the results in Table \ref{table:moe_wo_finetune}, we find that with the perplexity routing heuristic and the option of separating the attention layers, we get the best average result among all baseline methods. Compared to Random Routing and SoTA dense merging method: Dare, our best methods: PPL routing + separating the attention layers bring a relative improvement of 16.8\% and 13.6\%, respectively, on the average performance. Better results with PPL routing than with task vector routing align with the observation in Figure \ref{fig:routing_heuristic}, where PPL routing routes input more precisely to the desired experts. 
% Moreover, better result for merging without attention layers also follows our expectation that this merging option eliminates the inconsistency of task vector effect as suggested in Section \ref{sec:merging_wo_ft}.

\noindent \textbf{Proposed MoE method without fine-tuning outperforms the dense merging baseline.} \quad From Table \ref{table:moe_wo_finetune}, we observe that using the PPL routing heuristic and separating attention layers achieves the best average results among all baseline methods. Compared to Random Routing and the SoTA dense merging method (Dare), our best method - PPL routing + separating attention layers - yields relative improvements of 16.8\% and 13.6\%, respectively. The superior performance of PPL routing aligns with Figure \ref{fig:routing_heuristic} in Appendix \ref{sec:supp_routing}, where PPL routing more accurately directs input to the appropriate experts. 
Moreover, the better results of separating attention layers support our expectation that this approach resolves the inconsistency of task vector counts, as discussed in Section \ref{sec:merging_wo_ft}.


\subsection{Heterogeneous Model Merging}

% We prepare two MoE models with heterogeneous model merging for this part. We use 3 models (\llamab, Code \llama, and Knowledge \llama) with the same architecture and one Math Olmo model for the first MoE. For the second MoE, we merge 3 models and one Math TinyLlama model. 
% For the tokenizer, we choose the \llamab tokenizer due to the majority of experts. 
% We also prepare two 3-expert MoE models (\llamab, Code \llama, and Knowledge \llama) as the baseline method to show the functionality of Olmo or TinyLlama experts. One 3-expert MoE is fine-tuned on the same data source, and the other one is fine-tuned only on the data without math data. Both baselines are fine-tuned with 100B tokens. 

\textbf{MoE merged with heterogeneous models outperforms the corresponding experts.} \quad
After showing the superiority of our homogeneous model merging method, our next question is whether the proposed heterogeneous expert merging is also effective.
We present the performance of the dense, MoE and baseline methods in Table \ref{table:hetero_results}. The details of training cost for each method are presented in Table \ref{table:4_training_cost} in Appendix.

\begin{table}[!htb]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lccccccc}
\toprule
Method                                                           & MBPP           & HumanEval      & MATH          & GSM8K         & NQ            & TriviaQA      & Avg.           \\ \hline
\multicolumn{8}{c}{\textbf{Dense Model}}                                                                                                                                            \\ \hline
\llamab                                                      & 4.60           & 3.04           & 2.42          & 1.44          & 6.61          & 26.72         & 7.47           \\ \hline
Base TinyLlama                                                   & 5.40           & 5.27           & 2.26          & 2.2           & 8.53          & 34.27         & 9.66           \\ \hline
Base Olmo                                                        & 2.80           & 2.64           & 2.46          & 2.42          & 6.16          & 29.21         & 7.62           \\ \hline
Code \llama                                                       & 10.20          & 8.53           & 2.42          & 2.57          & 3.11          & 16.7          & 7.26           \\ \hline
Math TinyLlama                                                   & 15.60          & 9.76           & 4.18          & 5.91          & 6.05          & 21.12         & 10.44          \\ \hline
Math Olmo                                                        & 0.00           & 0.00           & 4.82          & 5.08          & 3.61          & 11.25         & 4.13           \\ \hline
Knowledge \llama                                                  & 3.60            & 4.26           & 2.62          & 2.04          & 5.65          & 28.71         & 7.81           \\ \hline

\multicolumn{8}{c}{\textbf{Homogeneous Expert Merging}}                                                                                                                                              \\ \hline
% BTX merging        & 12.40          & 11.58          & 6.74          & 7.73          & 6.78 & 25.1           & 11.72          \\ \hline
\begin{tabular}[c]{@{}l@{}}3-expert MoE \\ (same data)\end{tabular}  & 9.14           & 10.8           & 4.42          & 5.16          & 6.95          & 26.78         & 10.54          \\ \hline
\begin{tabular}[c]{@{}l@{}}3-expert MoE \\ (w/o math)\end{tabular}   & 12.00             & 9.76           & 2.38          & 1.74          & 6.22          & \textbf{33.20} & 10.88          \\ \hline


\multicolumn{8}{c}{\textbf{Heterogeneous Expert merging}}                                                                                                                                              \\ \hline
\begin{tabular}[c]{@{}l@{}}(Ours) MoE w/ \\ Math Olmo\end{tabular}      & 13.60           & 10.98          & 4.86          & 6.14          & 5.43 & 26.01         & 11.17          \\ \hline

\begin{tabular}[c]{@{}l@{}} (Ours) MoE w/ \\ Math TinyLlama\end{tabular} & \textbf{15.80} & \textbf{11.59} & \textbf{5.42} & \textbf{6.29} & \textbf{8.25} & 32.71         & \textbf{13.34} \\ \bottomrule
\end{tabular}
}
\caption{\label{table:hetero_results} \textbf{Performance of proposed heterogeneous merged MoE and other baselines.} The merged MoE is comparable or outperform the dense or 3-expert baselines on the benchmark from the corresponding domain.}
\end{table}

% From Table \ref{table:hetero_results}, compared to the domain expert models, our merged heterogeneous MoE models are comparable to or exceed the expert results in their corresponding domains. For example, MoE with Math Olmo and Math TinyLlama achieves the performance of 6.14\% and 6.29\% on GSM8K datasets and our CPTed dense model: Math Olmo and Math TinyLlama obtain 5.91\% and 5.08\% accuracy. For average performance, our merging framework also brings a relative improvement of 43.02\% and 27.78\% between the best-performed experts and the merged MoE models for MoE with Olmo and TinyLlama, respectively.
% As for the MoE baseline: 3-expert MoE, both heterogeneous merged MoEs outperform their performance,  especially in the math domains, demonstrating the functionality of including the math expert, which suggests the effectiveness of our merged pipeline. 

Table \ref{table:hetero_results} shows that our merged MoE models are comparable to or outperform the domain expert models in their respective domains. For instance, the MoE merged with Math Olmo and Math TinyLlama achieves 6.14\% and 6.29\% accuracy on GSM8K, compared to 5.91\% and 5.08\% for their dense counterparts. On average, our MoEs with Olmo and TinyLlama improves performance by 43.02\% and 27.78\% relative to the best dense experts, respectively. Both MoEs with heterogeneous experts also outperform the 3-expert MoE baseline, particularly in math, highlighting the effectiveness of including math experts in the pipeline.

\noindent \textbf{MoE merged with heterogeneous experts show the desired routing patterns in most cases.} \quad We also perform a similar routing analysis as described in Section \ref{sec:rq1}. We visualize the routing probability of two MoEs when evaluating on GSM8K and MATH datasets in Figure \ref{fig:hetero_routing_prob} and for other datasets, we visualize the results in Figure \ref{fig:supp_hetero_routing_prob} in Appendix \ref{sec:supp_routing}.

\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{0.48\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/hetero_routing_probabilities_GSM8K.pdf}
        \caption{GSM8K}
        \label{fig:hetero_gsm8k_route}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/hetero_routing_probabilities_MATH.pdf}
        \caption{MATH}
        \label{fig:hetero_math_route}
    \end{subfigure}
    \caption{Routing probability of experts on GSM8K and MATH for the MoE w/ Olmo and MoE w/ TinyLlama.}
    \label{fig:hetero_routing_prob}
\end{figure}

% From the routing analysis in Figures \ref{fig:hetero_routing_prob} and \ref{fig:supp_hetero_routing_prob}, for the coding and knowledge datasets, most of the tokens will be routed to the corresponding experts.
% However, unlike the homogeneous model merging where the math expert shares the highest routing probability in math datasets, the math expert (Olmo or TinyLlama) obtains the second highest routing probability. This discrepancy should be attributed to the difference between the embedding output from MoE models and expert models. Since the embedding layer of MoE is merged from 3 \llama models and 1 other model, the embedding output of MoE is expected to be closer to the embedding output of \llama models. Therefore, the router network is more likely to route the inputs to the \llama model. Adding the load balancing loss may be the possible solution to this caveat \cite{sukhbaatar2024branchtrainmixmixingexpertllms, fedus2022switch}, which ensures a more uniform distribution of the routing load, and we leave that exploration to future work.

As shown in Figures \ref{fig:hetero_routing_prob} and \ref{fig:supp_hetero_routing_prob}, most tokens in the coding and knowledge datasets are routed to the corresponding experts. However, unlike homogeneous model merging where the math expert has the highest routing probability for math datasets, Math Olmo or Math TinyLlama ranks second. This discrepancy is likely due to the difference in embedding outputs between the MoE and expert models. Since the MoE's embedding layer is merged from 3 \llama models and 1 other model, its output is closer to that of the \llama models, making the router more likely to select them. Adding a load balancing loss is a possible solution to address this issue \cite{sukhbaatar2024branchtrainmixmixingexpertllms, fedus2022switch}, ensuring a more uniform routing distribution. We leave this for future exploration