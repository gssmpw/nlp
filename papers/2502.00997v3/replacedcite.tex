\section{Background and Related Work}
% This paper is related to previous work on dense and MoE model merging.

\subsection{Dense Model Merging}

% Model merging methods combine multiple domain experts into a single model to obtain diverse capabilities ____. Most merging methods focus on homogeneous dense expert models into another dense model of the same architecture: average merging ____ applies averaging over the model parameters; task vector merging ____ first extracts the task vector (the difference of parameters between the base and the experts) and adds the unweighted sum of the task vectors back to the dense model with a scaling term. Other works explore how to determine the weights of the task vector instead of an unweighted sum ____. Dare and Ties ____, two SoTA merging methods, choose to trim the task vector to resolve parameter interference: in Dare, the task vector is randomly trimmed and rescaled by a fixed amount;
% in the Ties method, on the other hand, the authors
% set the vector parameter to 0 by magnitude and adjust the sign of each parameter to reduce the sign conflict.

Dense merging methods combine multiple dense models into one to achieve diverse capabilities ____. Most approaches focus on merging homogeneous dense models into another dense model. For example, average merging____ averages model parameters, while task vector merging____ adds the unweighted sum of task vectors (the difference between base and expert parameters) back to the dense model with scaling. Other work determines task vector weights instead of using an unweighted sum ____. SoTA methods like Dare and Ties ____ trim the task vector to resolve parameter interference: Dare trims the task vector randomly and rescales, while Ties sets vector parameters to zero by magnitude and adjusts signs to reduce conflicts.

% In addition to homogeneous model merging, ____ proposes an approach to merge heterogeneous models to a dense model by incorporating the projectors. ____ applies the knowledge distillation to fuse heterogeneous models.
% Compared with previous merging works for dense models, our paper focuses on merging experts to an MoE model. For homogeneous experts, we explore a more efficient merging method and merging without fine-tuning. We are the first work to propose a pipeline for merging heterogeneous models into an MoE.

In addition to homogeneous model merging, ____ propose merging heterogeneous models into a dense model using projectors, while ____ apply knowledge distillation to fuse heterogeneous models. 
In this work, we introduce a more efficient method for merging experts with limited or no further fine-tuning and, unlike previous work focusing on dense models, we explore merging homogeneous and heterogeneous experts into an MoE model. 
%Additionally, we propose 
%For homogeneous experts, we propose a more efficient merging method without fine-tuning. We are also the first to introduce a pipeline for merging heterogeneous models into an MoE.

\subsection{MoE Training and Merging}

% MoE architectures allows for faster training given a fixed computational budget, and faster inference given a fixed number of parameters.  They do so by introducing Sparse MoE layers, where a router mechanism dispatches tokens to the top-K expert FFNs in parallel (where k is usually 1 or 2)  ____. In terms of MoE training, most work chooses to train the whole MoE on all domain data to obtain capabilities across multiple tasks ____, but full communication during training on different GPUs is costly ____. To address this challenge, inspired by the Branch-Train-Merge (BTM) method ____, where they average the final model output distributions from different experts, the Branch-Train-Mix (BTX) method ____ first branches the base model to multiple copies, trains the base model in different domains, and merges the experts into a unified MoE. Another recent work, Self-MoE ____, uses the LoRa style ____ to fine-tune each expert on self-generated data and then combine all trained adapters into a base model for a LoRa MoE. 

MoE architectures enable quicker inference with a certain parameter count by introducing Sparse MoE layers, where a router mechanism assigns tokens to the top-$K$ expert FFNs (usually 1 or 2) in parallel ____. Most MoE training approaches, known as upcycling, train the entire model from scratch to handle multiple tasks ____. These methods first initialize the MoE model from a pretrained base model and then train it on the entire dataset. However, due to the costly communication between GPUs, the upcycling method introduces significant computational overhead ____. To address this, methods like Branch-Train-Merge (BTM) ____ average model outputs from different experts, while Branch-Train-Mix (BTX) ____ branches the base model, trains each on different domains, and merges them into a unified MoE. 
BTX is shown to be more effective than BTM as well as dense CPT and MoE upcycling baselines.
%VS: do we need to explain upcycling?
Another recent approach, Self-MoE ____, uses low-rank adaptation (LoRA) ____ to fine-tune experts on generated synthetic data ____ and combines trained adapters into an MoE.
To our knowledge, we are the first to introduce a framework for merging heterogeneous models into an MoE.