%
% 1. Evaluation of LLMs is challenging, many different benchmarks
One of the most celebrated aspects of state of the art large language models (LLMs) is that they can solve open-ended, complex tasks across many different application domains such as coding, healthcare and scientific discovery~\cite{bubeck2023sparks, mozannar2022reading, haupt2023ai, romera2023mathematical}. 
%
However, this is crucially what also makes the evaluation and comparison of LLMs very challenging---it is very difficult, if not impossible, to create a single benchmark.
%
As a consequence, in recent years, there has been a flurry of papers introducing different benchmarks~\cite{bach2022promptsource,wei2022finetuned,talmor2019commonsense,mishra2022cross,chen2021evaluating,liang2023holistic,longpre2023flan,hendryckstest2021,wang2022self,ouyang2022training,wang2023aligning,chiang2024chatbot,taori2023stanford,zheng2023judging,li2023generative,li2023prd,boubdir2023elo,singhal2023large}.
%
In fact, one of the flagship conferences in machine learning has even created a separate datasets and benchmarks track!

%
% 2. Uncertainty is overlooked
In this context, it is somehow surprising that, in comparison, there has been a paucity of work understanding, measuring or controlling for the different sources of uncertainty present in the evaluations and comparisons of LLMs based on these benchmarks~\cite{miller2024adding,madaan2024quantifying,dubey2024llama,saadfalcon2023ares,boyeau2024autoeval,chatzi2024prediction,dorner2024limits,gera2024justrank}.
%
In our work, we focus on one source of uncertainty that has been particularly overlooked, the uncertainty in the outputs of the LLMs under comparison.

%
% 3. Autoregressive process underpinning LLMs output randoms response
Given an input prompt, LLMs generate a sequence of tokens\footnote{Tokens are the units that make up sentences and paragraphs, \eg, (sub-)words, numbers, and special end-of-sequence tokens.} as output using an autoregressive process~\cite{bengio2000neural,radford2019language}. 
%
At each time step, they first use a neural network to map the prompt and the (partial) sequence of tokens generated so far to a token distribution. 
%
Then, they use a sampler to draw the next token at random from the token distribution.\footnote{If an LLM is forced to output tokens deterministically, multiple lines of evidence suggest that its performance worsens~\citep{holtzman2020the}.}
%
Finally, they append the next token to the (partial) sequence of tokens, and continue until a special end-of-sequence token is sampled.
%
To understand why, in the context of LLM evaluation and ranking, the above autoregressive process may lead to inconsistent conclusions, we will use a stylized example.

%
% 4. Example showing independent noise can lead to counterintuitive results (copy of 
% the same model or very similar models).
Consider we are given three LLMs $m_1$, $m_2$ and $m_3$, and we need to rank them according to their ability to answer correctly two types of input prompts, $q$ and $q'$, picked uniformly at random.
%
Moreover, assume that the true probability that each LLM answers correctly each type of input prompt is given by:
%
\begin{table}[h]
    \centering
    \begin{tabular}{lccc}
        \toprule
         & $m_1$ & $m_2$ & $m_3$ \\
        \midrule
        $q$           & 0.4             & 0.48           & 0.5             \\
        $q'$           & 1           & 0.9         & 0.89              \\
        \bottomrule
 \end{tabular}
 \vspace{-8mm}
 \caption*{}
 \end{table}

Then, one may argue that $m_1$ is the best LLM, followed closely by $m_3$, and $m_2$ is the worst, because the average probabilities that they answer a query picked uniformly at random correctly are $0.7$, $0.695$ and $0.69$, respectively.
%
However, if we conduct pairwise comparisons between outputs by two different LLMs to the same input prompt, as commonly done in practice, we may instead argue that $m_3$ is the best LLM, followed by $m_2$, and $m_1$ is the worst, because the probability that an LLM is preferred over others---the win-rates---are $0.16225$, $0.15675$, and $0.1545$, respectively.\footnote{Refer to Appendix~\ref{app:example-ranking} for the detailed calculation of the average win-rates.}
%
In our work, we argue that controlling for the randomization of the autoregressive processes underpinning the LLMs under comparison can, at least in certain cases, avoid such inconsistencies and lead to more intuitive conclusions.
%
Along the way, we also show that it can reduce the number of samples required to reliably compare the performance of LLMs. 

\xhdr{Our contributions}
%
% 5. Causal model of coupled autoregressive generation
Our key idea is to couple the autoregressive processes underpinning a set of LLMs under comparison, particularly their samplers, by means of sharing the same source of randomness. 
%
To this end, we treat the sampler of each LLM as a causal mechanism that receives as input the distribution of the next token and the same set of noise values, which determine the sampler'{}s (stochastic) state.
%
By doing so, at each time step of the generation, we can expect that, if different LLMs map the prompt and the (partial) sequence of tokens generated so far to the same token distribution, they will sample the same next token.
%
Loosely speaking, in the context of LLM evaluation and ranking, coupled autoregressive generation ensures that no LLM will have better luck than others.
%
% 6. Theoretical results
More formally, on evaluations based on benchmark datasets, we show that the difference in average performance of each pair of LLMs under comparison is asymptotically the same under coupled and vanilla autoregressive generation, but coupled autoregressive generation provably leads to a reduction in the required sample size.
%
On evaluations based on (human) pairwise comparisons, we show that the win-rates of the LLMs under comparison can be asymptotically different under coupled and vanilla autoregressive generation and, perhaps surprisingly, the resulting rankings can actually differ.
%
This suggests that the apparent advantage of an LLM over others in existing evaluation protocols may not be genuine but rather confounded by the randomness inherent to the generation process.

%
% 7. Experiments
To illustrate and complement our theoretical results, we conduct experiments with several LLMs of the \texttt{Llama} family, namely \texttt{Llama-3.1-8B-Instruct}, \texttt{Llama-3.2-\{1B, 3B\}-Instruct}, and \texttt{Llama-3.1-8B-Instruct-\{AWQ-INT4, bnb-4bit, bnb-8bit\}}.
%
% 8. MMLU results
% 
We find that, across multiple knowledge areas from the popular MMLU benchmark dataset, 
% coupled autoregressive generation leads to a reduction of up to $40$\% in the required number of samples.
coupled autoregressive leads to a reduction of up to $40$\% in the required number of samples to reach the same conclusions as vanilla autoregressive generation.
%
% 9. LMSYS results
%
Further, using data from the LMSYS Chatbot Arena platform, we find that the win-rates derived from pairwise comparisons by a strong LLM differ under coupled and vanilla autoregressive generation.
%
We conclude with a comprehensive discussion of the limitations of our theoretical results and experiments, including additional avenues for future work. An open-source implementation of coupled autoregressive generation is available at \url{https://github.com/Networks-Learning/coupled-llm-evaluation}.
% \footnote{We provide the code used in our experiments as supplementary 
% material. We will publicly release it with the final version of the paper.}

% 10. Further related work
\xhdr{Further related work}
%
% Our work builds upon a very recent work on counterfactual token generation by Chatzi et al.~\citep{chatzi2024counterfactual}, which also treats the sampler of an LLM as a causal mechanism.
%
% However, their focus is different to ours, they augment a single LLM with the ability to conduct counterfactual reasoning about alternatives to their own outputs.
%
Our work builds upon a very recent work on counterfactual token generation by~\citet{chatzi2024counterfactual}, which also treats the sampler of an LLM as a causal mechanism.
%
However, their focus is different to ours; they augment a single LLM with the ability to reason counterfactually about alternatives to its own outputs if individual tokens had been different.
% conduct counterfactual reasoning about alternatives to its own outputs. 
% 
Our work also shares technical elements with a recent work by~\citet{ravfogel2024counterfactual}, which develops a causal model to generate counterfactual strings resulting from interventions within (the network of) an LLM. 
% However, their work does not relate to model evaluation and, thus, is complementary to ours.
However, their work does not study counterfactual generation for the purposes of model evaluation.
% and, thus, is complementary to ours.
% 
In this context, it is also worth pointing out that the specific class of causal models used in the aforementioned works and our work, called the Gumbel-max structural causal model~\cite{oberst2019counterfactual}, has also been used to enable counterfactual reasoning in Markov decision processes~\citep{tsirtsis2021counterfactual}, temporal point processes~\citep{noorbakhsh2022counterfactual}, and expert predictions~\citep{benz2022counterfactual}.
% In this context, it is also worth pointing out that the specific class of causal mechanisms used by Chatzi et al. and our work, called Gumbel-max structural causal model~\cite{oberst2019counterfactual}, has been also used to enable counterfactual reasoning in string generation~\cite{ravfogel2024counterfactual}, Markov decision processes~\citep{tsirtsis2021counterfactual}, temporal point processes~\citep{noorbakhsh2022counterfactual}, and expert predictions~\citep{benz2022counterfactual}.

Our work also builds upon the rapidly increasing literature on evaluation and comparison of LLMs~\cite{chang2024asurvey}. 
%
Within this literature, LLMs are evaluated and compared using: 
%
(i) benchmark datasets with manually hand-crafted inputs and ground-truth outputs~\cite{bach2022promptsource,wei2022finetuned,talmor2019commonsense,mishra2022cross,chen2021evaluating,liang2023holistic,longpre2023flan} and (ii) the level of alignment with human preferences, as elicited by means of pairwise comparisons~\cite{taori2023stanford,zheng2023judging,li2023generative,li2023prd,boubdir2023elo,singhal2023large,chiang2024chatbot}.
%
However, it has become increasingly clear that oftentimes rankings derived from benchmark datasets do not match those derived from human preferences~\cite{zheng2023judging,li2023generative,li2023prd,chiang2023vicuna,chiang2024chatbot}.
%
Within the literature on ranking LLMs from pairwise comparisons, most studies use the Elo rating system~\cite{askell2021general,dettmers2024qlora,bai2022training,wu2023chatarena,lin2023llm}, originally introduced for chess tournaments~\cite{elo1966uscf}. 
%
However, Elo-based rankings are sensitive to the order of pairwise comparisons, as newer comparisons have more weight than older ones, which leads to unstable rankings~\cite{boubdir2023elo}.
%
To address this limitation, several studies have instead used the Bradley-Terry model~\cite{chiang2024chatbot,boyeau2024autoeval}, which weighs pairwise comparisons equally regardless of their order. % , thus resulting in more stable rankings.
%
Nevertheless, both the Elo rating system and the Bradley-Terry model have faced criticism, 
%
as pairwise comparisons often fail to satisfy the fundamental axiom of transitivity, 
%
upon which both approaches rely~\cite{boubdir2023elo,bertrand2023limitations}. 
%
Recently, several studies have used the win-rate~\cite{zheng2023judging,chiang2024chatbot,boyeau2024autoeval}, which weighs comparisons equally regardless of their order and does not require the transitivity assumption.
%
In our work, we focus on win-rates. However, we believe that it may be possible to extend our theoretical and empirical results to rankings based on Elo ratings and the Bradley-Terry model.