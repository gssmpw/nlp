\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{./img/Counterfactual_LLM_Eval_final.pdf}
    \caption{{\bf Example of coupled autoregressive generation for \texttt{Llama 1B} and \texttt{Llama 8B}.} Boxes represent endogenous random variables and circles represent exogenous random variables. The value of each endogenous variable is given by a function of the values of its ancestors in the causal graph, as defined by Eq.~\ref{eq:SCM}. The value of the coupled noise variable $U_1$ (purple) is sampled independently from a given distribution $P_U$, and it determines the stochastic state of the samplers used by both \texttt{Llama 1B} and \texttt{Llama 8B} during the generation of token $T_1$.}
    \label{fig:scm}
\end{figure}

Let $V$ denote a vocabulary (set) of tokens, including an end-of-sequence token $\bot$, 
%
$V^*=V\cup V^2 \cup \dots \cup V^K$ be the set of sequences of tokens up to length $K$, 
%
and $\varnothing$ be the empty token.\footnote{Here, $V^j$ denotes the set of all sequences of length $j$ that can be constructed from the tokens in $V$. We restrict our attention to sequences of finite length $(\leq K)$ because, in practice, the context window of LLMs is finite.}
%
An LLM $m \in \Mcal$ takes as input a prompt sequence $s_q \in V^*$ and responds with an output sequence $s \in V^*$, generated using an autoregressive process. 
%
At each time step $i \in [K]$ of the process, the LLM first takes as input the concatenation of the prompt sequence $s_q$ and the (partial) output sequence $s_{i-1}$, and generates a distribution over tokens $d_{i} \in \Delta(V)$. 
%
Then, it samples the next token $t_{i} \sim d_{i}$ from the distribution $d_{i}$ and creates the output sequence $s_{i} = s_{i-1} \circ t_{i}$, where $\circ$ denotes the concatenation of a token or sequence with another sequence.
%
If $t_{i} = \bot$, it terminates and returns $s = s_i$, otherwise, it continues to the next step $i+1$ in the generation. 
%
Once the process is completed, the output sequence $s$ is assigned a score $r$, which is subsequently used for model evaluation.

Following the recent work by~\citet{chatzi2024counterfactual}, we augment the above autoregressive process using a structural causal model (SCM)~\citep{pearl2009causality, peters2017elements}, which we denote as $\Ccal$. 
%
The SCM $\Ccal$ is defined by the following structural equations:\footnote{We use capital letters to denote random variables and lowercase letters to denote their realizations.}
%
\begin{equation}\label{eq:SCM}
\begin{split}
           S_0&=S_q,
    \quad   D_{i} 
    %=f_D(S_{i-1};\theta) 
 =\begin{cases}
        f_D(S_{i-1},M)
        % ;\theta) &
        & \text{if} \,\, \texttt{last}(S_{i-1})\neq \bot,\\
        P_\varnothing & \text{otherwise}
    \end{cases},
    \quad  T_{i} = \begin{cases}
        f_T( D_{i}, U_{i}) & \text{if} \,\, D_{i} \neq P_\varnothing,\\
        \varnothing & \text{otherwise}
    \end{cases},
        \\[2ex] 
    S_{i} &=  
    % f_{S}(S_{i-1}, T_{i}) = 
    S_{i-1} \circ T_{i}, 
    \quad S=S_K,
    \quad \text{and } R=f_R(S, Z).
\end{split}
\end{equation}
%
In the above equations, $M,S_q,\Ub = (U_i)_{i \in \{1, \ldots, K\}}$, and $Z$ are independent exogenous random variables, 
%
with $M \sim P_{M}$, $S_q \sim P_{Q}$, $U_i \sim P_{U}$, and $Z \sim P_{Z}$.
%
Moreover, $f_D$, $f_T$ and $f_R$ are given functions,
%
$P_\varnothing$ denotes the point mass distribution on $\varnothing$, and $\texttt{last}(S_{i-1})$ denotes the last token of the sequence $S_{i-1}$. 
%
Here, the function $f_D$ maps an input sequence $S_{i-1}$ to a distribution $D_i$ for the next token, using the architecture and network weights of the LLM $M$, 
%
the function $f_T$ and distribution $P_U$ specify the sampling mechanism that is used to sample the next token at each step of the generation process, following the distribution $D_i$,
%
and the function $f_R$ and distribution $P_Z$ specify the exact scoring process by which the score $R$ is assigned to an output sequence $S$ during the evaluation of the LLM $M$.
%

Throughout the paper, we focus on sampling mechanisms that satisfy counterfactual stability~\citep{oberst2019counterfactual,tsirtsis2021counterfactual,chatzi2024counterfactual}---an intuitive form of consistency between the next token $T_i$, its distribution $D_i$, and the corresponding noise variable $U_i$.\footnote{The default categorical sampler in \texttt{PyTorch}~\citep{paszke2019pytorch}, one of the most popular libraries used by state of the art LLMs, is an implementation of the Gumbel-Max SCM~\citep{oberst2019counterfactual}, which satisfies counterfactual stability. 
For a formal definition of counterfactual stability, refer to Appendix~\ref{app:stability}.} 
%
Moreover, we allow the score $R$ to be observable or unobservable, and its semantic meaning and support of its distribution to vary depending on the evaluation protocol. For example, in multiple-choice questions~\citep{hendrycks2021measuring}, $R\in\{0,1\}$ may represent whether an LLM outputs a correct ($R=1$) or an incorrect ($R=0$) response. In pairwise comparisons~\citep{chiang2024chatbot}, $R\in\RR^+$ may represent the level of user'{}s satisfaction with the response provided by an LLM. 
%
In this context, the noise variable $Z$ models any potential sources of uncertainty in the scoring process, \eg, uncertainty in users' preferences~\citep{thurstone1927law,bradley1952rank,luce1959individual}.

Building upon the above causal model, we can now formally express what it means to sample (and evaluate) output sequences by different LLMs using the same source of randomness,\footnote{In our work, we implicitly assume that different LLMs share the same vocabulary $V$, however, in practice, this may not hold if the LLMs use different tokenizers. Refer to Section~\ref{sec:discussion} for further discussion on this point.} a process we refer to as {\bf coupled autoregressive generation}. 
%
Consider a specific model $m$, a prompt $s_q$, and fixed noise values $\ub$ and $z$. It is easy to see that specifying these values is sufficient to (deterministically) specify and compute the exact value of the output sequence $S$ and its score $R$ using the autoregressive generation and scoring process given by Eq.~\ref{eq:SCM}. 
%
Then, we can formally express the coupled output sequences by two models $m$ and $m'$ and their corresponding scores as the result of \emph{interventions} $do(M=m)$ and $do(M=m')$, respectively, where the $do(\cdot)$ operator forcibly sets the value of $M$ while keeping the prompt $s_q$ and the noise values $\ub$, $z$ fixed~\citep{pearl1994probabilistic}. 
% 
In what follows, we denote the respective scores $R_m (\ub, s_q, z)$ and $R_{m'} (\ub, s_q, z)$, following standard notation~\citep{pearl2009causality}.
% 
For an illustration of coupled autoregressive generation against independent autoregressive generation---the vanilla generation approach---refer to Figure~\ref{fig:scm}.

In practice, one run of coupled autoregressive generation consists of two or more runs of autoregressive generation with the same prompt $s_q$ and
% function $f_T$ and 
noise values $\ub$ and $z$, one per LLM.\footnote{In practice, we may not always have control over the noise value $z$ (\eg, when the scoring process is performed by an end user). 
%
However, even in such cases, we can still implement coupled autoregressive generation if the scoring processes occur simultaneously for each run, such as in pairwise comparisons.}
%
From a causal perspective, we can view these runs as realizations of possible worlds where everything is equal except for the (architecture and network weights of the) LLM.
%
Or we can also view one of these runs as a realization of the factual world and the 
other runs as realizations of different counterfactual worlds. 
%
Consequently, this lends support to attribute any difference in the scores $R_m (\ub, s_q, z)$ across models $m \in \Mcal$ to the models' architectures and weights rather than the randomness in their autoregressive generation processes. 
%
In the following sections, we will investigate both theoretically and empirically the differences between coupled and independent autoregressive generation in the context of evaluations based on benchmark datasets and pairwise comparisons.