In this section, we focus on the evaluation and comparison of LLMs based on benchmark datasets, \eg, multiple-choice questions~\citep{hendrycks2021measuring}, and theoretically investigate under which conditions coupled autoregressive generation requires fewer samples than independent autoregressive generation to reliably estimate the competitive advantage of one LLM over another.

% Introduce reward
Given a benchmark dataset characterized by an input prompt distribution $P_Q$, for each prompt $s_q \sim P_Q$,
%
let $\textsc{c}(s_q) \subset V^*$ denote the set of correct output sequences.\footnote{In multiple-choice questions, $\textsc{c}(s_q)$ may consist of all sequences that include the correct choice.} %and, in mathematical reasoning questions, it may consist of all sequences that end with the numerical value that matches the correct response.} 
In what follows, for ease of exposition, we consider binary scores
%
% \begin{equation}\label{eq:reward_bench}
    $R_m(\ub, s_q) = \one\left\{S_m(\ub, s_q) \in \textsc{c}(s_q)\right\} \in \{0, 1\}$,
% \end{equation}
%
where $S_m(\ub, s_q)$ denotes the output sequence of a model $m$ given a prompt $s_q$ under a realized sequence of noise values $\ub$ and $\one\{\cdot\}$ is the indicator function.\footnote{Our theoretical results can be extended to real-valued scores in a bounded interval.}

% Introduce different expressions for coupled and independent autoregressive generation + comment on U, U'
The standard approach to compare the performance of any pair of LLMs $m, m' \in \Mcal$ using a benchmark dataset reduces to estimating the difference in their expected score, \ie,
%
\begin{align}
    \EE_{\Ub\sim P_{\Ub}, \Ub'\sim P_{\Ub}, S_q\sim P_Q} [&R_{m}(\vertarrowbox{\Ub}{}, S_q) - R_{m'}(\vertarrowbox{\Ub}{}', S_q)], \label{eq:independent-generation-difference} \\[-4ex]
    &\text{\small \hspace{1mm} Independent generation} \nn
\end{align}
%
where note that we use different noise variables $\Ub$ and $\Ub'$ for each LLM because, in the standard approach, each LLM generates outputs to each query independently (\ie, using independent autoregressive generation).
%
At first, one may think that, in this context, coupled autoregressive generation will not be helpful. 
%
Under coupled autoregressive generation, the difference in the expected score adopts the following form:
%
\begin{align}
\EE_{\Ub\sim P_{\Ub},S_q \sim P_Q} [R_m&(\vertarrowbox{\Ub}{},S_q)-R_{m'}(\vertarrowbox{\Ub}{},S_q)]. \label{eq:coupled-generation-difference} \\[-4ex] &\text{\small Coupled generation} \nn
\end{align}
%
Therefore, based on the linearity of expectation and the fact that, under independent generation, both $\Ub$ and $\Ub'$ are sampled from the same distribution $P_{\Ub}$, it is easy to see that Eqs.~\ref{eq:independent-generation-difference} and~\ref{eq:coupled-generation-difference} are equivalent. 
%
However, as we will show next, coupled autoregressive generation allows us to reliably estimate the difference in the two LLMs' scores from finite samples faster.
%
More formally, we first start by characterizing the relation between the variances of the difference of scores between LLMs using the following proposition:\footnote{All proofs can be found in Appendix~\ref{sec:proofs}.}
%
\begin{proposition}\label{prop:variance}
    For any pair of LLMs $m, m' \in \Mcal$, it holds that
    \begin{equation}\label{eq: variance shared noise}
        \textnormal{Var}[R_m(\Ub,S_q)-R_{m'}(\Ub',S_q) ]
        = \textnormal{Var}[R_m(\Ub,S_q)-R_{m'}(\Ub,S_q) ] + 2\cdot  \Cov[R_m(\Ub,S_q),R_{m'}(\Ub,S_q)]
    \end{equation}
\end{proposition}
%
% Explain the intuition behind the proposition. 
This result immediately implies that, if the scores achieved by the LLMs under comparison are positively correlated, \ie, the LLMs tend to generate a (in-)correct output sequence on the same prompts under the same noise values, then the variance of the difference in scores is lower under coupled generation than under independent generation, and thus we can expect a reduction in the sample size required to obtain equivalent estimation errors.
%
In what follows, we will analyze two canonical settings in which this condition holds and, in Section~\ref{sec:experiments}, we will provide empirical evidence that, in a well-known benchmark dataset, this condition also holds.

% We identify conditions under which the covariance is positive.
In the first canonical setting, the correct response to each prompt is one of two given single-token sequences, the LLMs $m$ and $m'$ under comparison always output a response that is either of these two sequences, and the sampling mechanism used by the LLMs satisfies counterfactual stability.
%
While this setting may seem restrictive, it is found in real-world scenarios. 
%
For example, think of true/false questions (or multiple-choice questions with two options) and evaluation protocols in which the LLMs are explicitly instructed to always output true/false (or one of the two options) via their system  prompt.\footnote{Here, our goal is to illustrate that there exist natural conditions under which coupled autoregressive generation is provably beneficial in comparison to independent autoregressive generation. However, in practice, in this canonical setting, one could directly use the LLMs' probabilities for the two tokens in each prompt to estimate the average difference of scores exactly.} 
%
The following proposition~shows that the variance of the difference in scores is lower under coupled autoregressive generation:
% Proposition 2 + discussion
\begin{proposition} \label{prop:var_stability}
    Consider a benchmark dataset such that $\textsc{c}(s_q) \subsetneq \{t_1, t_2\}$ for all $s_q \sim P_Q$, where $t_1$ and $t_2$ are two single-token sequences. Let $m$ and $m'$ be two LLMs that assign positive probability to the sequences $t_1$ and $t_2$ and zero probability to any other sequence.
    %
    If the sampling mechanism defined by $f_T$ and $P_U$ satisfies counterfactual stability, then, it holds that
    %
    \begin{equation}\label{eq:binary_var}
        \textnormal{Var}[R_m(\Ub,S_q)-R_{m'}(\Ub',S_q)] > \textnormal{Var}[R_m(\Ub,S_q)-R_{m'}(\Ub,S_q)]. 
    \end{equation}
    %
%     where the inequality is strict if the probabilities assigned by the LLMs to 
% the two sequences are strictly less than $1$.
\end{proposition}

In the second canonical setting, the correct response to each prompt is a single-token sequence, the LLMs $m$ and $m'$ under comparison always output a single-token response, and the sampling mechanism used by the LLMs is given by the Gumbel-Max SCM\footnote{The Gumbel-Max SCM is defined as $f_T(D_i, U_i) = \argmax_{t\in V} \left\{\log\left(D_{i,t}\right) + U_{i,t}\right\}$, where $U_{i,t} \sim \text{Gumbel}(0,1)$ are i.i.d. noise variables associated with each token~\citep{chatzi2024counterfactual}.}.
%
Similarly as in the first canonical setting, this second setting is also found in real-world scenarios, particularly taking into account that the default categorical sampler in the library \texttt{PyTorch}~\citep{paszke2019pytorch} implements the Gumbel-Max SCM.
%
The following proposition shows that, as long as the model $m'$ is \emph{similar enough} to $m$, the variance of the difference in scores is lower under coupled generation:
% Proposition 3 + discussion
\begin{proposition}\label{prop:var_gumbel}
    Consider a benchmark dataset such that $|\textsc{c}(s_q)|=1$ for all $s_q \sim P_Q$. Let $m$ be an LLM that assigns positive probability to every single-token sequence and zero probability to any other sequence. 
    %
    If the sampling mechanism defined by $f_T$ and $P_U$ is given by the Gumbel-Max SCM, then, there exists a constant $\varepsilon(m)>0$ such that, for every LLM $m'$ that assigns positive probability to every single-token sequence and zero probability to any other sequence and satisfies $d(m,m')=\sup_{s_q} \norm{f_D(s_q,m)-f_D(s_q, m')}_\infty < \varepsilon(m)$, it holds that
    \begin{equation*}
        \textnormal{Var}[R_m(\Ub,S_q)-R_{m'}(\Ub',S_q)]
        > \textnormal{Var}[R_m(\Ub,S_q)-R_{m'}(\Ub,S_q)].
    \end{equation*}
    %
\end{proposition}
%
Based on the above proposition, we hypothesize that coupled autoregressive generation will reduce the number of samples required to reliably compare the performance of LLMs whenever these are sufficiently \emph{similar}, \eg, whenever we compare fine-tuned or quantized versions of the same pre-trained LLM.