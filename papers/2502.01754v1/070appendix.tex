\section{Formal Definition of Counterfactual Stability}\label{app:stability}

Counterfactual stability is a desirable property of SCMs~\citep{oberst2019counterfactual} that has previously been used in the context of autoregressive generation of LLMs~\cite{chatzi2024counterfactual}. In the following, we provide its formal definition along with a simple example to explain the intuition behind it. Throughout this section, $\PP^{\Ccal \,;\, do(\cdot)}$ denotes the probability of the interventional distribution entailed by an SCM $\Ccal$ under an intervention $do(\cdot)$.
% and $do(\cdot)$ denotes interventions to random variables. 
Moreover, $\PP^{\Ccal \,|\, \star \,;\, do(\cdot)}$ denotes the probability of the counterfactual distribution entailed by an SCM $\Ccal$ under an intervention $do(\cdot)$ given that an observed event $\star$ has already occurred.

\begin{definition}
    A sampling mechanism defined by $f_T$ and $P_U$ satisfies counterfactual stability if for all LLMs $m,m'\in \Mcal$, $i\in\{1,2,\ldots,K\}$ and tokens $t_1,t_2 \in V$ with $t_1\neq t_2$, the condition 
    \begin{equation}\label{eq:stability_condition}
        \frac{\PP^{\Ccal \,;\, do(M=m')}[T_i=t_1 \given D_i]}{\PP^{\Ccal \,;\, do(M=m)}[T_i=t_1 \given D_i]} \geq \frac{\PP^{\Ccal \,;\, do(M=m')}[T_i=t_2 \given D_i]}{\PP^{\Ccal \,;\, do(M=m)}[T_i=t_2 \given D_i]}
    \end{equation}
    implies that $\PP^{\Ccal \,|\, D_i,M=m,T_i=t_1 \,;\, do(M=m')}[T_i=t_2]=0$.
\end{definition}

The property of counterfactual stability has an intuitive interpretation that can be best understood via a simple example. Assume that the vocabulary contains $2$ tokens ``A'' and ``B'' and, using LLM $m$, the next-token distribution at a time step $i$ assigns values $0.6$, $0.4$ to the two tokens, respectively. Moreover, the realized noise value $\ub_i$ is such that the token ``A'' is sampled.
% , that is, $\text{``A''}=f_T(d_i, u_i)$.
Now, consider that, while keeping the noise value $\ub_i$ fixed, we change the LLM to $m'$, resulting in a next-token distribution that assigns values $0.7$, $0.3$ to the two tokens, respectively. Counterfactual stability ensures that, since the noise value $\ub_i$ led to ``A'' being sampled under $m$ at $0.6$ to $0.4$ odds, the same value
cannot lead to ``B'' being sampled under $m'$ where its relative odds are lower (\ie, $0.3$ to $0.7$).
% also leads to ``A'' being sampled under $m'$ where its relative odds are higher (\ie, $0.7$ to $0.3$).

\section{Proofs} \label{sec:proofs}

\subsection{Proof of Proposition~\ref{prop:variance}}
 We can rewrite the variance of the difference in scores under independent generation in terms of the variance of the difference in scores under coupled generation as follows:
 %
    \begin{align*}
        \Var[R_m(\Ub,S_q)-R_{m'}(\Ub',S_q)] ] 
        &= \Var[R_m(\Ub,S_q)-R_{m'}(\Ub,S_q) + R_{m'}(\Ub,S_q) -R_{m'}(\Ub',S_q)] ] \nonumber
        \\ &= \Var[R_m(\Ub,S_q)-R_{m'}(\Ub,S_q)] + \Var[R_{m'}(\Ub,S_q)-R_{m'}(\Ub',S_q)] \nn
            \\ &+ 2 \cdot \Cov[R_m(\Ub,S_q)-R_{m'}(\Ub,S_q), R_{m'}(\Ub,S_q)-R_{m'}(\Ub',S_q) ]. 
    \end{align*}
%
For the variance of the difference in scores for the same LLM under independent noise values, we have that
%
    \begin{align*}
        \Var[R_{m'}(\Ub,S_q)-R_{m'}(\Ub',S_q)]
        & \overset{(a)}{=} \EE[(R_{m'}(\Ub,S_q)-R_{m'}(\Ub',S_q))^2] -
         \EE[R_{m'}(\Ub,S_q)-R_{m'}(\Ub',S_q)]^2 % \label{eq:defvar1}
        \\& \overset{(b)}{=} \EE[R_{m'}(\Ub,S_q)^2-2\cdot R_{m'}(\Ub,S_q)R_{m'}(\Ub',S_q) + R_{m'}(\Ub',S_q)^2] % \label{eq:zeroterm}
        \\& \overset{(c)}{=} 2 \cdot \EE[R_{m'}(\Ub,S_q)^2] - 2 \cdot \EE[R_{m'}(\Ub,S_q)R_{m'}(\Ub',S_q)], % \label{eq:rewrite}
    \end{align*}
%
where (a) holds by the definition of variance, (b) is due to the subtraction term being $0$, and (c) is due to the linearity of expectation.
%
Further, for the covariance of the difference in scores under independent generation and the difference in scores under coupled generation, we have that
%
\begingroup
\allowdisplaybreaks
\begin{align*}
    \Cov[R_m(\Ub,S_q)-R_{m'}&(\Ub,S_q), R_{m'}(\Ub,S_q)-R_{m'}(\Ub',S_q)]\nonumber
    \\ % \begin{split}\label{eq:covdef}
    &\overset{(a)}{=} \EE[\left(R_m(\Ub,S_q)-R_{m'}(\Ub,S_q)\right) \cdot \left( R_{m'}(\Ub,S_q)-R_{m'}(\Ub',S_q) \right)] 
    \\ &\quad \quad - \EE[R_m(\Ub,S_q)-R_{m'}(\Ub,S_q)]\cdot \EE[R_{m'}(\Ub,S_q)-R_{m'}(\Ub',S_q)]
    % \end{split}
    \\ % \begin{split}\label{eq:zeroexp}
    &\overset{(b)}{=} \EE[R_m(\Ub,S_q)R_{m'}(\Ub,S_q)] - \EE[R_m(\Ub,S_q)R_{m'}(\Ub',S_q)]
    \\ &\quad \quad - \EE[R_{m'}(\Ub,S_q)R_{m'}(\Ub,S_q)] + \EE[R_{m'}(\Ub,S_q)R_{m'}(\Ub',S_q)] 
    \\ % \end{split}
    & \overset{(c)}{=} \Cov[R_m(\Ub,S_q),R_{m'}(\Ub,S_q)] - \EE[R_{m'}(\Ub,S_q)^2] + \EE[R_{m'}(\Ub,S_q)R_{m'}(\Ub',S_q)]] % \label{eq:covfinal}
\end{align*}
\endgroup
where (a) and (c) hold by the definition of covariance and (b) is due to the last term being zero and by the expansion of the first term.

Putting all the above results together, it follows that
%
\begin{align*}
    \Var[R_m(\Ub,S_q)-R_{m'}(\Ub',S_q)] ]
            % \\ \begin{split}
        & = \Var[R_m(\Ub,S_q)-R_{m'}(\Ub,S_q)] 
        + 2\cdot \Cov\left[R_m(\Ub,S_q),R_{m'}(\Ub,S_q)\right] \\
         & \quad+ 2\cdot \EE[R_{m'}(\Ub,S_q)R_{m'}(\Ub',S_q)] 
         - 2\cdot \EE[R_{m'}(\Ub,S_q)^2] 
         + 2 \cdot \EE[R_{m'}(\Ub,S_q)^2] \\
         & \quad - 2 \cdot \EE[R_{m'}(\Ub,S_q)R_{m'}(\Ub',S_q)]
       %  \end{split}
        \\&= \Var[R_m(\Ub,S_q)-R_{m'}(\Ub,S_q)] 
        + 2\cdot \Cov[R_m(\Ub,S_q),R_{m'}(\Ub,S_q)] 
\end{align*}
which concludes the proof.


\subsection{Proof of Proposition~\ref{prop:var_stability}}

Due to Proposition~\ref{prop:variance}, to show that Eq.~\ref{eq:binary_var} holds, it suffices to show that the covariance between the scores of the different LLMs under coupled generation is non-negative, \ie, $\Cov[R_m(\Ub,S_q),R_{m'}(\Ub,S_q)]\geq 0$.

To this end, we first rewrite the covariance as
    \begin{multline}
            \Cov[R_m(\Ub,S_q),R_{m'}(\Ub,S_q)] = \PP[R_m(\Ub,S_q)=1, R_{m'}(\Ub,S_q)=1] -\PP[R_m(\Ub,S_q)=1]\cdot \PP[R_{m'}(\Ub,S_q)=1] \\ \label{eq:covariance_rewrite}
            =\sum_{s_q} \PP[S_q=s_q] \cdot \left( \PP[R_m(\Ub,s_q)=1, R_{m'}(\Ub,s_q)=1]-\PP[R_m(\Ub,s_q)=1]\cdot \PP[R_{m'}(\Ub,s_q)=1] \right) 
        % \end{split}
    \end{multline}
%
Next, we note that the event $R_m(\Ub,s_q)=1$ is equivalent to LLM $m$ sampling the ground truth token for prompt $s_q$. 
%
Without loss of generality, assume $t_1$ is the ground truth token, \ie, $\textsc{c}(s_q)=t_1$. 
%
Then, since only tokens $\{t_1, t_2\}$ have positive probability under $m$ and $m'$, it must hold that either (i) one LLM assigns a greater probability to $t_1$ and the other LLM assigns a greater probability to $t_2$, 
%
or (ii) both LLMs assign the same probabilities. 
%
Further, since the sampling mechanism defined by $f_T$ and $P_U$ satisfies counterfactual stability, we have that the condition in Eq.~\ref{eq:stability_condition} holds in both (i) and (ii) and, under coupled generation, the LLM with greater (or equal) probability for $t_1$ will always sample $t_1$ when the LLM with lower (or equal) probability does. 
%
This implies that
%
\begin{equation}
    \PP[R_m(\Ub,s_q)=1, R_{m'}(\Ub,s_q)=1] = \min\{ \PP[R_m(\Ub,s_q)=1], \PP[R_{m'}(\Ub,s_q)=1]\}
\end{equation}
%
Finally, since it holds that
%
\begin{equation}
    \min\{ \PP[R_m(\Ub,s_q)=1], \PP[R_{m'}(\Ub,s_q)=1]\} \geq \PP[R_m(\Ub,s_q)=1] \PP[R_{m'}(\Ub,s_q)=1]
\end{equation}
because $\PP[R_m(\Ub,s_q)=1] \in (0,1)$ and $\PP[R_{m'}(\Ub,s_q)=1]\in(0,1)$ by assumption,
% (with strict inequality holding if in $(0,1)$),
we can conclude from Eq.~\ref{eq:covariance_rewrite} that
    \begin{equation}
        \begin{split}
            &\Cov[R_m(\Ub,S_q),R_{m'}(\Ub,S_q)] > 0.
        \end{split}
    \end{equation}
%
% Additionally, we can also conclude that, if $\PP[R_m(\Ub,s_q)=1] \in (0,1)$ 
% and $\PP[R_{m'}(\Ub,s_q)=1]\in (0,1)$, the above inequality is strict.
    
\subsection{Proof of Proposition~\ref{prop:var_gumbel}}
%
Using Proposition~\ref{prop:variance}, we have that
%
    \begin{align*}
            \Cov[R_m(\Ub,S_q),R_{m'}(\Ub,S_q)] &= \EE[R_m(\Ub,S_q)\cdot R_{m'}(\Ub,S_q)]-\EE[R_m(\Ub,S_q)]\cdot  \EE[R_{m'}(\Ub,S_q)]\\
            &= \underbrace{\PP[R_m(\Ub,S_q)=1, R_{m'}(\Ub,S_q)=1
            ]}_{(i)}-\underbrace{\PP[R_m(\Ub,S_q)=1) \cdot \PP[R_{m'}(\Ub,S_q)=1]}_{(ii)}.
    \end{align*}
%
In the remainder of the proof, we will bound each term (i) and (ii) separately and, since $|\textsc{c}(s_q)|=1$ for all $s_q \sim P_{Q}$, assume without loss of generality that the correct token is single-token sequence $t_1$.

To bound the term (ii), first note that, using the definition of the Gumbel-Max SCM, we have that, for each $k \in \{2, \ldots, |V|\}$, it holds that
%
\begin{align*}
            R_{m}(\Ub,s_q)=1 &\iff U_1 + \log ( [f_D(s_q,m)]_{t_1} ) \geq U_k + \log ( [f_D(s_q, m)]_{t_k} ), \\
            R_{m'}(\Ub,s_q)=1 &\iff U_1 + \log ( [f_D(s_q,m')]_{t_1} ) \geq U_k + \log ( [f_D(s_q, m')]_{t_k} ).
\end{align*}
%
Next, let $\varepsilon^*>0$ be an arbitrary constant that we will determine later such that
%
\begin{equation} \label{eq:bound}
    |\log ( [f_D(S_q,m)]_{t_k}) -\log ([f_D(S_q,m')]_{t_k}) |\leq \varepsilon^*,
\end{equation}
%
and note that since, by assumption, $D_{t_k} > 0$ for all $k \in \{1, \ldots, |V|\}$, any bound on the absolute difference of log-probabilities $|\log ( [f_D(S_q,m)]_{t_k}) -\log ([f_D(S_q,m')]_{t_k}) |$ uniformly implies a bound on the difference of probabilities $|[f_D(S_q,m)]_{t_k} -[f_D(S_q,m')]_{t_k} |$ and vice versa. 
%
For simplicity, we prove the result in the log-domain.

Now, using the bound defined by Eq.~\ref{eq:bound}, we have that
%
\begin{multline*}
        \bigcap_{k\neq1} \left\{ U_1 + \log ( [f_D(S_q, m')]_{t_1} ) \geq U_k + \log ( [f_D(S_q, m')]_{t_k} ) \right\}\\
            \subset \bigcap_{k\neq1} \left\{U_1 + \log ( [f_D(S_q, m)]_{t_1} )+\varepsilon^* \geq U_k + \log ( [f_D(S_q, m)]_{t_k} )-\varepsilon^* \right\},
\end{multline*}
%
and we can then bound the term (ii) as follows:
    \begin{align*}
            \PP[R_m(\Ub,S_q)=1] &\cdot \PP[R_{m'}(\Ub,S_q)=1] \\
            &=
            \PP[\cap_{k\neq1} \{U_1 + \log ( [f_D(S_q,m)]_{t_1} ) \geq U_k + \log ( [f_D(S_q,m)]_{t_k} )\}]\\
            & \qquad \times \PP[\cap_{k\neq1}\{ U_1 + \log ( [f_D(S_q,m')]_{t_1} ) \geq U_k + \log ( [f_D(S_q,m')]_{t_k} ) \}]\\
            &\leq \PP[\cap_{k\neq1} \{U_1 + \log ( [f_D(S_q,m)]_{t_1} ) \geq U_k + \log ( [f_D(S_q,m)]_{t_k} )\}]\\
            & \qquad \times \PP[\cap_{k\neq1} \{U_1 + \log ( [f_D(S_q,m)]_{t_1} )+\varepsilon^* \geq U_k + \log ( [f_D(S_q,m)]_{t_k} )-\varepsilon^*\}].
    \end{align*}
%
To bound the term (i), first note that, using the bound defined by Eq.~\ref{eq:bound}, we have that
    \begin{multline*}
        \bigcap_{k\neq1} \left\{U_1 + \log ( [f_D(S_q,m')]_{t_1} ) \geq U_k + \log ( [f_D(S_q,m')]_{t_k} ) \right\}\\
         \supset \bigcap_{k\neq1} \left\{U_1 + \log ( [f_D(S_q,m)]_{t_1} )-\varepsilon^* \geq U_k + \log ( [f_D(S_q,m)]_{t_k} )+\varepsilon^* \right\}.
    \end{multline*} 
%        
Thus, we can bound the term (i) as follows:
%
    \begin{align*}
        \PP[R_m(\Ub,S_q)=1, R_{m'}(\Ub,S_q)=1] &=\PP\Big[\cap_{k\neq1} \{U_1 + \log ( [f_D(S_q,m)]_{t_1} ) \geq U_k + \log ([f_D(S_q,m)]_{t_k} )\}\\
            & \qquad \cap \{ U_1 + \log ( [f_D(S_q,m')]_{t_1} ) \geq U_k + \log ( [f_D(S_q,m')]_{t_k} ) \}\Big] \\
            & \geq \PP\Big[\cap_{k\neq1} \{U_1 + \log ( [f_D(S_q,m)]_{t_1} ) \geq U_k + \log ( [f_D(S_q,m)]_{t_k} )\}\\
            & \qquad \cap \{ U_1 + \log ( [f_D(S_q,m)]_{t_1} ) \geq U_k + \log ( [f_D(S_q,m)]_{t_k} ) +2\varepsilon^* \}\Big] \\
            & \overset{(a)}{=} \sum_{s_q} \PP[S_q=s_q]\cdot \PP[\cap_{k\neq1} \{ U_1 + \log ( [f_D(S_q,m)]_{t_1} ) \\
            & \geq U_k + \log ( [f_D(S_q,m)]_{t_k} ) +2\varepsilon^* \}],
    \end{align*}
where (a) follows from the fact that
%
\begin{multline*}
      \left\{ U_1 + \log ( [f_D(S_q,m)]_{t_1} ) \geq U_k + \log ( [f_D(S_q,m)]_{t_k} ) +2\varepsilon^* \right\} \\
      \subset \left\{ U_1 + \log ( [f_D(S_q,m)]_{t_1} ) \geq U_k + \log ( [f_D(S_q,m)]_{t_k} ) \right\}.
\end{multline*}
%
Now, note that, for $k \in \{2, \ldots, |V|\}$, the variable $X_k \equiv U_1-U_k \sim \text{Logistic}(0,1)$ (for $k=1$, define $X_k \equiv 0$). 
%
Therefore, we can rewrite the bound for (i) as
%  
\begin{multline*}
      \PP[R_m(\Ub,S_q)=1, R_{m'}(\Ub,S_q)=1]\\
      \geq\sum_{s_q} \PP[S_q=s_q] \cdot \prod_{k\neq1}\cdot \PP[ \{ X_k \geq \log ( [f_D(S_q,m)]_{t_k} )-\log ( [f_D(S_q,m)]_{t_1} ) +2\varepsilon^* \}]
\end{multline*}
%
and we can rewrite the bound for (ii) as
%
\begin{multline*}
    \PP[R_m(\Ub,S_q)=1] \PP[R_{m'}(\Ub,S_q)=1]\leq \\
    \sum_{s_q}\PP[S_q=s_q]\cdot 
            \left\{
            \prod_{k\neq1}
            \PP[ \{ X_k \geq \log ( [f_D(S_q,m)]_{t_k} )-\log ( [f_D(S_q,m)]_{t_k} ) - 2\varepsilon^* \} ]
            \right\}\\
            \times \PP[\cap_{k\neq1} \{X_k \geq \log ( [f_D(S_q,m)]_{t_k} )-\log ( [f_D(S_q,m)]_{t_k} )\}].
\end{multline*}
%
As a consequence, to prove that $\PP[R_m(\Ub,S_q)=1, R_{m'}(\Ub,S_q)=1] > \PP[R_m(\Ub,S_q)=1] \PP[R_{m'}(\Ub,S_q)=1]$, it suffices to show that
    \begin{multline}\label{eq:condition-covariance-binary-scores}
            \sum_{s_q}P[S_q=s_q] \prod_{k\neq1}\cdot \PP[ \{ X_k \geq \log ( [f_D(S_q,m)]_{t_k} )-\log ( [f_D(S_q,m)]_{t_1} ) +2\varepsilon^* \}]\\
            > \sum_{s_q}\PP[S_q=s_q] \prod_{k\neq1}\cdot \PP[ \{ X_k \geq \log ( [f_D(S_q,m)]_{t_k} )-\log ( [f_D(S_q,m)]_{t_1} ) - 2\varepsilon^* \}]
            \\
            \times \PP[\cap_{k\neq1} \{X_k \geq \log ( [f_D(S_q,m)]_{t_k} )-\log ( [f_D(S_q,m)]_{t_1} )\}]
    \end{multline}
%
To do so, note that Eq.~\ref{eq:condition-covariance-binary-scores} holds trivially for $\varepsilon^*=0$ since
%
\begin{equation*}
      \PP[\cap_{k\neq1} \{X_k \geq \log ( [f_D(S_q,m)]_{t_k} )-\log ( [f_D(S_q,m)]_{t_1} )\}] < 1,
\end{equation*}
%
which is a fixed term independent of $m'$. Since all terms in Eq.~\ref{eq:condition-covariance-binary-scores} are continuous in $\varepsilon^*$, there exists $\varepsilon^*(m) >0 $, possibly dependent of $m$ but independent of $m'$, such that Eq.~\ref{eq:condition-covariance-binary-scores} holds if
\begin{equation*}
    \sup_{s_q} \norm{\log(f_D(s_q,m))-\log(f_D(s_q, m'))}_\infty < \varepsilon^*(m).
\end{equation*}
Since by assumption $D_t>0$ for all $t\in V$, there exists $\varepsilon(m)>0$ in probability space such that Eq.~\ref{eq:condition-covariance-binary-scores} holds if
\begin{equation*}
    \sup_{s_q} \norm{f_D(s_q,m)-f_D(s_q, m')}_\infty < \varepsilon(m).
\end{equation*}
%
This concludes the proof.

\subsection{Proof of Proposition~\ref{prop:gap_win_rates_stability}}

% We first compute the win-rates under coupled autoregressive generation.
%
Under coupled autoregressive generation, if the LLM $m$ samples the preferred token $t_+$, then the LLM $m'$ must also sample $t_{+}$ because $t_{+}$ is more likely under $m'$ than under $m$ and the sampling mechanism defined by $f_T$ and $P_U$ satisfies counterfactual stability. 
%
This implies that the win-rate achieved by $m$ against $m'$ is
%
    \begin{equation}\label{eq:prop_greater_shared}
            \EE_{\Ub\sim P_{\Ub}} [\one\{R_m(\Ub,s_q)>R_{m'}(\Ub,s_q)\}] =
            \PP[f_T(f_D(s_q, m),\Ub)= t_+, f_T(f_D(s_q, m'),\Ub)= t_-]=0 
    \end{equation}
and that
    \begin{equation}\label{eq:prop_equal_probs+}
            \PP[f_T(f_D(s_q, m),\Ub)= t_+, f_T(f_D(s_q, m'),\Ub)= t_+]= \PP[f_T(f_D(s_q, m),\Ub)= t_+] =p_m.
    \end{equation}
%
Using the same reasoning, if the LLM $m'$ samples the non-preferred token $t_{-}$, then, $m$ must also sample $t_{-}$ because $t_{-}$ is more likely under $m$ than under $m'$. This implies that
%
    \begin{equation}\label{eq:prop_equal_probs-}
            \PP[f_T(f_D(s_q, m),\Ub)= t_-, f_T(f_D(s_q, m'),\Ub)= t_-]= \PP[f_T(f_D(s_q, m'),\Ub)= t_-] = 1-p_{m'}
    \end{equation} 
%
Then, from Eq.~\ref{eq:prop_equal_probs+} and Eq.~\ref{eq:prop_equal_probs-}, we can conclude that
%
    \begin{equation}\label{eq:prop_equal_shared}
        \begin{aligned}
            \EE_{\Ub\sim P_{\Ub}} [\one\{R_m(\Ub,s_q)=R_{m'}(\Ub,s_q)\}] = p_m + (1-p_{m'})
        \end{aligned}
    \end{equation}
%
Finally, from Eq.~\ref{eq:prop_greater_shared} and Eq.~\ref{eq:prop_equal_shared}, we can conclude that the win-rate achieved by $m'$ against $m$ is
%
\begin{multline*}
    \EE_{\Ub\sim P_{\Ub}} [\one\{R_m(\Ub,s_q)<R_{m'}(\Ub,s_q)\}] \\
        = 1 - \EE_{\Ub\sim P_{\Ub}} [\one\{R_m(\Ub,s_q)>R_{m'}(\Ub,s_q)\}] -\EE_{\Ub\sim P_{\Ub}} [\one\{R_m(\Ub,s_q)=R_{m'}(\Ub,s_q)\}] = p_{m'}-p_m.
\end{multline*}

% Next, we compute the win-rates under independent autoregressive generation.
%
Under independent autoregressive generation, the LLMs $m$ and $m'$ sample tokens independently from each other, \ie, $f_T(f_D(s_q, m),\Ub) \perp f_T(f_D(s_q, m'),\Ub')$. 
%
Thus, we can factorize all joint probabilities when computing the win-rates and obtain
%
\begin{align*}\nonumber
    % \begin{split}
        \EE_{\Ub, \Ub'\sim P_{\Ub}} [\one\{R_m(\Ub,s_q)>R_{m'}(\Ub',s_q)\}] &=\PP[f_T(f_D(s_q, m),\Ub)= t_+] \cdot \PP[f_T(f_D(s_q, m'),\Ub')= t_-] \\ &=
        p_m \cdot (1-p_{m'})
    % \end{split}
\end{align*}
and
\begin{equation}\nonumber
    \begin{split}
        \EE_{\Ub, \Ub'\sim P_{\Ub}} & [\one\{R_m(\Ub,s_q)<R_{m'}(\Ub',s_q)\}] 
         = p_{m'} \cdot (1-p_m).
        % we do not write it the = rates in the proposition 
        %\\
        % \text{and} \quad \EE_{\Ub\sim P_{\Ub}, \Ub'\sim P_{\Ub}, S_q\sim P_{Q}} & [\one\{R_m(\Ub,S_q)=R_{m'}(\Ub',S_q)\}] 
        %  = p_m(p_m+\delta) + (1-p_m-\delta)(1-p_m)
    \end{split}
\end{equation}


\subsection{Proof of Proposition~\ref{prop:gumbel_ties}}

We follow the notations and technique of Proposition~\ref{prop:var_gumbel}.
Fix query $s_q$ and consider first the case of independent autoregressive generation. Since each LLM can only assign a non-zero probability to single-token sequences, we have:
\begin{equation*}
    \begin{split}
        \PP[R_m(\Ub, s_q)=R_{m'}(\Ub', s_q)]&=\sum_{k=1}^{|V|}\PP[f_T(f_D(s_q,m),\Ub)=t_k] \PP[f_T(f_D(s_q,m'),\Ub)=t_k]\\
        &<\sum_{k=1}^{|V|}\PP[f_T(f_D(s_q,m),\Ub)=t_k],
    \end{split}
\end{equation*}
%
In the case of coupled autoregressive generation, since
\begin{equation*}
    \PP\left[\{f_T(f_D(s_q,m),\Ub)=t_k\} \cap \{f_T(f_D(s_q,m),\Ub)=t_j \}\right]=0,\; i\neq j,
\end{equation*}
%
we obtain:
\begin{equation*}
    \begin{split}
        &\PP[R_m(\Ub, s_q)=R_{m'}(\Ub, s_q)]\\
        &=\PP\left[\cup_i \{ f_T(f_D(s_q,m),\Ub)=t_k, f_T(f_D(s_q,m'),\Ub)=t_k \} \right]\\
        &=\sum_k\PP[\{f_T(f_D(s_q,m),\Ub)=t_k, f_T(f_D(s_q,m'),\Ub)=t_k \}]\\
        &=\sum_k \PP[f_T(f_D(s_q,m),\Ub)=t_k]\PP[f_T(f_D(s_q,m'),\Ub)=t_k|f_T(f_D(s_q,m),\Ub)=t_k].\\
    \end{split}
\end{equation*}
We now follow \cite{9729603} and expand the posterior Gumbels, $\PP[f_T(f_D(s_q,m'),\Ub)=t_k|f_T(f_D(s_q,m),\Ub)=t_k]$, as truncated Gumbel distributions. In particular, we leverage the fact that
\begin{equation}\label{eq:max of gumbels}
    \max_{t\in V}\{U_t+\log([f_D(s_q,\bullet)]_{t})\} \sim \text{Gumbel}(0,1),
\end{equation}
and that a Gumbel distribution, with parameter $\log (\theta)$, truncated at $b\sim \text{Gumbel}(0,1)$  can be sampled as
\begin{equation}\label{eq: truncated gumbel}
    -\log(\exp(-b)-\log(\eta)/\theta),\; \eta \sim U(0,1).
\end{equation}
Furthermore, by assumption, $D_{t_k} > 0$ for all $k \in \{1, \ldots, |V|\}$, so that any bound on the absolute difference of log-probabilities $|\log ( [f_D(s_q,m)]_{t_k}) -\log ([f_D(s_q,m')]_{t_k}) |$ uniformly implies a bound on the difference of probabilities $|[f_D(s_q,m)]_{t_k} -[f_D(s_q,m')]_{t_k} |$ and vice versa. Using the bound 
\begin{equation*}
    |\log([f_D(s_q,m)]_{t_k})-\log([f_D(s_q,m')]_{t_k})|\leq \varepsilon^*
\end{equation*}
and the Gumbel properties in Eq.~\ref{eq:max of gumbels} and Eq.~\ref{eq: truncated gumbel}, we obtain:
%
\begin{align}
        &\PP[R_m(\Ub, s_q)=R_{m'}(\Ub, s_q)] \nn \\
        &=\sum_k \PP[f_T(f_D(s_q,m),\Ub)=t_k] \nn \\
            & \qquad \quad \times \PP\Bigg[
            \bigcap_k \Big\{
            \log( [f_D(s_1, m')]_{t_k}) - \log([f_D(s_1, m)]_{t_k})-\log(-\log (\eta_k)) \nn \\
            & \qquad \qquad \qquad \qquad \geq \log([f_D(s_1, m')]_{t_j}) - \log([f_D(s_1, m)]_{t_j})-\log(-\log(\eta_k) -\log(\eta_j)/[f_D(s_1,m')]_{t_j})
            \Big\}
            \Bigg] \nn \\
            &\geq
            \sum_k \PP[f_T(f_D(s_q,m),\Ub)=t_k] \nn \\
            &\qquad \quad \times\PP\left[
            \cap_k \{
            -\log(-\log (\eta_k)) \geq -2\varepsilon^* -\log(-\log(\eta_k) -\log(\eta_j)/[f_D(s_1,m')]_{t_j})
            \}
            \right] \label{eq:final-bound}
\end{align}
where $\eta_k\sim \text{U}(0,1)$ are independently distributed uniform random variables.
%
Now, note that the claim holds for $\varepsilon^*=0$ since, in that case, we have that
%
\begin{equation*}
   \PP\left[
        \bigcap_k \left\{
        -\log(-\log (\eta_k)) \geq -\log(-\log(\eta_k) -\log(\eta_k)/[f_D(s_1,m')]_{t_k})
        \right\} \right]=1,
\end{equation*}
%
using that $x\mapsto -\log(x)$ is strictly decreasing. 
%
Since all terms in Eq.~\ref{eq:final-bound} are continuous in $\varepsilon^*$, there exists $\varepsilon^*(m) >0 $, possibly dependent on $m$ but independent of $m'$, such that 
\begin{equation}\label{eq:prop5 claim prob}
    \PP[R_m(\Ub, s_q)=R_{m'}(\Ub, s_q)] > \PP[R_m(\Ub, s_q)=R_{m'}(\Ub', s_q)]
\end{equation}
holds if
\begin{equation*}
    \sup_{s_q} \norm{\log(f_D(s_q,m))-\log(f_D(s_q, m'))}_\infty < \varepsilon^*(m).
\end{equation*}
Since by assumption $D_t>0$ for all $t\in V$, there exists $\varepsilon(m)>0$ in probability space such that Eq.~\ref{eq:prop5 claim prob} holds if

\begin{equation*}
    \sup_{s_q} \norm{f_D(s_q,m)-f_D(s_q, m')}_\infty < \varepsilon(m).
\end{equation*}
%
This concludes the proof.






\subsection{Calculation of average win-rates in the example used in Sections~\ref{sec:intro} and~\ref{sec:pairwise}} \label{app:example-ranking}
%
In this section, we provide detailed calculations of the win-rates for the example in Sections~\ref{sec:intro} and~\ref{sec:pairwise}. Recall that in this example, we are given three LLMs $m_1$, $m_2$ and $m_3$, and we need to rank them according to their ability to answer correctly two types of input prompts, $q$ and $q'$, picked uniformly at random.
%
We assume that the true probability that each LLM answers correctly each type of input prompt is given by:
%
\begin{table}[H]
    \centering
    \begin{tabular}{lccc}
        \toprule
         & $m_1$ & $m_2$ & $m_3$ \\
        \midrule
        $q$           & $p_1=0.4$             & $p_2=0.48$           & $p_3=0.5$             \\
        $q'$           & $p'_1=1$           & $p'_2=0.9$         & $p'_3=0.89$              \\
        \bottomrule
 \end{tabular}
 \vspace{-8mm}
 \caption*{}
 \end{table}

Using Proposition~\ref{prop:gap_win_rates_stability}, 
% under the assumption of counterfactual stability, 
the win-rates under independent autoregressive generation are given, for each LLM $m_k$, by:
\begin{equation}
    \begin{split}
       \frac{1}{2}\sum_{j\neq k}\EE_{\Ub, \Ub' \sim P_{\Ub}, S_q\sim P_Q}[\one\{R_{m_k}(\Ub, S_q)>R_{m_j}(\Ub', S_q)\}] & =\frac{\sum_{j\neq k}p_k(1-p_j) + \sum_{j\neq k}p'_k(1-p'_j)}{4}.
    \end{split}
\end{equation}
%
Substituting the numerical values we obtain:

 \begin{equation}
    \begin{split}
       \frac{1}{2}\sum_{j\neq 1}\EE_{\Ub, \Ub' \sim P_{\Ub}, S_q\sim P_Q}[\one\{R_{m_1}(\Ub, S_q)>R_{m_j}(\Ub', S_q)\}] & =0.1545,\\
       \frac{1}{2}\sum_{j\neq 2}\EE_{\Ub, \Ub' \sim P_{\Ub}, S_q\sim P_Q}[\one\{R_{m_2}(\Ub, S_q)>R_{m_j}(\Ub', S_q)\}] & =0.15675,\\
       \frac{1}{2}\sum_{j\neq 3}\EE_{\Ub, \Ub' \sim P_{\Ub}, S_q\sim P_Q}[\one\{R_{m_3}(\Ub, S_q)>R_{m_j}(\Ub', S_q)\}] & =0.16225\\
    \end{split}
\end{equation}

Similarly, using Proposition~\ref{prop:gap_win_rates_stability}, the win-rates using coupled autoregressive generation can be written, for each LLM $m_k$, as:
\begin{equation}
    \begin{split}
       \frac{1}{2}\sum_{j\neq k}\EE_{\Ub \sim P_{\Ub}, S_q\sim P_Q}[\one\{R_{m_k}(\Ub, S_q)>R_{m_j}(\Ub, S_q)\}] & =\frac{\sum_{j\neq k}(p_k-p_j)_{+} + \sum_{j\neq k}(p'_k-p'_j)_{+}}{4},
    \end{split}
\end{equation}
where $(\bullet)_{+}=\max(0, \bullet)$ denotes the positive part. Substituting the numerical values we obtain:

 \begin{align*}
       \frac{1}{2}\sum_{j\neq 1}\EE_{\Ub \sim P_{\Ub}, S_q\sim P_Q}[\one\{R_{m_1}(\Ub, S_q)>R_{m_j}(\Ub, S_q)\}] & = 
       % \frac{(p_1-p_2)_{+} +(p_1-p_3)_{+} +(p'_1-p'_2)_{+} +(p'_1-p'_3)_{+}}{4}
       0.0525, \\
       \frac{1}{2}\sum_{j\neq 2}\EE_{\Ub \sim P_{\Ub}, S_q\sim P_Q}[\one\{R_{m_2}(\Ub, S_q)>R_{m_j}(\Ub, S_q)\}] &=
       % \frac{(p_2-p_1)_{+} +(p_2-p_3)_{+} +(p'_2-p'_1)_{+} +(p'_2-p'_3)_{+}}{4}=
       0.0225, \\
       \frac{1}{2}\sum_{j\neq 3}\EE_{\Ub \sim P_{\Ub}, S_q\sim P_Q}[\one\{R_{m_3}(\Ub, S_q)>R_{m_j}(\Ub, S_q)\}] &=
       % \frac{(p_3-p_2)_{+} +(p_3-p_1)_{+} +(p'_3-p'_2)_{+} +(p'_3-p'_1)_{+}}{4}=
       0.03.
\end{align*}






\newpage

\section{Additional Experimental Details} \label{app:add_exp}
\input{072experimentaldetails}

\clearpage
\newpage

% \section{Additional Experimental Results} \label{app:exp_results}
\input{073additional-experiments}
