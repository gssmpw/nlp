\section{PRELIMINARIES}

\subsection{VLN Background}

% Given an environment that can be represented as a graph \(G=(V,E)\), where \(V\) is the nodes representing different locations and \(E\) is the edges representing the navigable path between nodes. Each node \(v \in V\) corresponds to a physical location and has visual observations \( o_t\) associated with it at time \(t\), where \( o_t\) has RGBD images \( o_t^{\text{rgb}} \in \mathbb{R}^{H \times W \times 3} \) and \( o_t^{\text{depth}} \in \mathbb{R}^{H \times W} \). The agent receives a natural language instruction \( L = \{ l_1, l_2, \dots, l_n \} \), where \( l_i \) are tokens (words) in the instruction. The instruction tells the agent how to navigate from the starting location \( v_{\text{start}} \) to the goal location \( v_{\text{goal}} \).
As the navigation graph assumption cannot reflect the challenges a deployed system would experience in a real world environment. This paper focuses on the VLN-CE, an agent tasked with navigating through a continuous 3D environment based on natural language instructions. The environment represented as a continuous 3D spacese \(E\), where the agent’s position at any time \( t \) is given by its 3D coordinates \( \mathbf{x}_t = (x_t, y_t, z_t) \in \mathcal{E} \), where \( x_t \), \( y_t \), and \( z_t \) represent the agent’s location in a continuous space.
At each position \( \mathbf{x}_t \), the agent perceives its surroundings through visual observations \( o_t\), where \( o_t\) has RGBD images \( o_t^{\text{rgb}} \in \mathbb{R}^{H \times W \times 3} \) and \( o_t^{\text{depth}} \in \mathbb{R}^{H \times W} \).The agent is provided with a natural language instruction \( L = \{ l_1, l_2, \dots, l_n \} \), where \( l_i \) are tokens (words) in the instruction. This instruction guides the agent from a start position \( \mathbf{x}_{\text{start}} \in \mathcal{E} \) to the goal position \( \mathbf{x}_{\text{goal}} \in \mathcal{E} \) with discrete low-level actions.

\subsection{Cross-modal Planning with Topological Map}
\noindent\textbf{Waypoint Prediction Network:} Let \( \mathcal{P}_t = \{p_1, p_2, \dots, p_n\} \) represent the 3D waypoint positions at time step \( t \), where each \( p_i \in \mathbb{R}^3 \). Similarly, let \( \mathcal{V}_t = \{v_1, v_2, \dots, v_n\} \) denote the corresponding \( d \)-dimensional visual features. At each time step, the visual encoders process the panoramic input to generate \( \mathcal{V}_t \), and a Transformer operates on \( \mathcal{V}_t \) to establish spatial and contextual relationships among the neighboring sectors, enriching the visual feature representation and informing the generation of candidate waypoints \( \mathcal{P}_t \), where each waypoint is associated with a direction encoded in \( v_i \). The agent selects the most promising waypoint \( p_i \) based on its visual feature and spatial position, simplifying navigation by moving directly toward the chosen waypoint.

\noindent\textbf{Topological Navigation Policy:} To enable effective backtracking and planning in the continuous environment, we follow the previous SoTA method ETPNav~\cite{an2024etpnav} on VLN-CE and perform language-guided navigation based on topological mapping. The environment is represented as a graph-based topo map $G_t = \{N_t, E_t\}$ keeps track of all observed nodes along the path $\Gamma'$. Given $\Gamma'$, we initialize $G_t$ by deriving its corresponding sub-graph from the predefined graph $G^*$. The nodes $N_t$ are divided into three categories: 

\begin{itemize}
    \item \textbf{Visited Node} is the agent has already visited
    \item \textbf{Current Node} is where the agent is currently located
    \item \textbf{Ghost Node} is a hypothetical node representing an uncertain or predicted location in the environment, not yet confirmed
    % \item \textbf{Waypoints} are predicted possibly accessible locations near the agent
    
\end{itemize}

The edges $E_t$ record the Euclidean distances among all adjacent nodes. The feature vectors $V^p_t$ are mapped onto the nodes as their visual representations. Taking time step $t$ as an example, $V^p_t$ are first fed into a panoramic encoder~\cite{an2023etpnav} to obtain contextual view embeddings $\hat{V}^p_t$. \textbf{Visited Node} and \textbf{Current Node} have been visited and can access panoramas, they are represented by an average of panoramic view embeddings. \textbf{Ghost Node} is partially observed and therefore is represented by accumulated embeddings of views from which \textbf{Ghost Node} can be observed. $G_t$ is equiped with a global action space $A^G$ for long-term planning, which consists of all observed nodes.

The graph $G^*$ is updated continuously based on the agent’s predictions and spatial relationships between nodes. If a visited node is localized, the input waypoint is deleted, and an edge is added between the current node and the localized visited node. If a ghost node is localized, the position and visual representation of the input waypoint are accumulated to the localized ghost node. This means that the ghost node’s position and features are updated based on the accumulated observations of the waypoint. If no node is localized, the input waypoint is added to the graph as a new ghost node. This newly added ghost node will remain unconfirmed until future localization attempts.
To ensure the graph $G^*$ remains efficient, nodes that are too close together or redundant are pruned. If the distance between nodes \( v_i \) and \( v_j \) is less than a threshold \( \epsilon \), then 
prune \(v_i\) \text{ if } \( d(v_i, v_j) \) < \( \epsilon \), where \( d(v_i, v_j) \) is a distance function between two nodes.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{METHODS}

\begin{figure*}[t]
      \centering
      \includegraphics[width=.99\linewidth]{figures/method.jpg}
      \vspace{-10pt}
      \caption{Multi-view Information Gathering emphasizes more informative features for the current context, enabling adaptive selection of the visual representations from multiple viewpoints (\textbf{A} and \textbf{B}). The navigation policy identifies the optimal next viewpoint in the topological graph (selecting \textbf{C} as the next viewpoint after \textbf{A}). This prediction is based not only on the robot's current observation at \textbf{A}, but also on previous, unobstructed views (from \textbf{B}), allowing the robot to mitigate occlusions and plan more robust navigation strategies.}
      \label{teaser}
    \vspace{-15pt}
\end{figure*}

\subsection{Scaling up Waypoint Prediction Network Training}

The first challenge posed by the ground-level viewpoint is the substantial degradation in waypoint prediction performance. This is not only due to the downward-shifted line of sight, which limits the visual field, but also the inherently low generalizability of the waypoint predictor in complex, real-world environments. Fig.~\ref{teaser} illustrates the candidate waypoints predicted by a waypoint predictor re-trained exclusively on the R2R dataset using low line-of-sight visual inputs, denoted by red crosses. Despite being re-trained to account for the robot's lower viewpoint, the predictions exhibit suboptimal performance. To solve this problem, we follow ScaleVLN~\cite{wang2023scaling} and construct a large waypoint prediction dataset in 800 scans from HM3D~\cite{ramakrishnan2021habitat}, 491 scans from Gibson~\cite{xia2018gibson}, and 61 scans from MP3D~\cite{chang2017matterport3d} under low-angle observation. Despite heuristically sampled viewpoints can also estimate feasible navigation paths based on depth information, it often lack the flexibility to handle complex, language-guided navigation tasks and oversimplify the navigation process by focusing primarily on depth cues, neglecting the rich semantic and visual information.

Specifically, we adopt the connectivity graph constructed in ScaleVLN~\cite{wang2023scaling} and discretize the environments into undirected graphs. At each node of the graphs, we annotate the distance and orientation of the connected nodes as ground truth supervision for the Waypoint Prediction Network. This provides 212924 training samples in total. Compared to the original training data for the waypoint prediction network~\cite{hong2022bridging}, this raised $\times 22.02$ in training data amount. Moreover, we set the rendering height to 80 cm from the ground in the Habitat simulator, and captured the depth images from ground-level observation at each node. 

\subsection{Multi-view Information Gathering}

The second challenge posed by the ground-level viewpoint is the discrepancy between the oracle and the agent's local observation, caused by environmental obstructions. This creates difficulties for the agent when attempting to predict the next action based on limited local observations. 
%Due to the restricted field of view at ground level, there is often a significant mismatch between the oracle, which represents a global, unobstructed viewpoint, and the robot's local observations, which can be occluded by objects in the environment. This constraint complicates the agent’s ability to accurately predict its next action based purely on its current state. For example, obstacles like furniture at lower viewpoints may obscure parts of the surrounding environment, leading to incomplete observations. 
In Figure~\ref{teaser}, the robot’s observation at position \textbf{A} is limited, while historical data from position \textbf{B} offers an unobstructed view. This disparity between local observations and the oracle’s ideal perspective becomes critical when the agent is tasked with selecting the next action or viewpoint in its decision-making.

%Local observations provide no meaningful information about the environment and can be considered highly noisy, the previous method that averaged these incomplete/noisy features with relevant, clear features from previous viewpoints dilutes the useful information. To overcome this problem, 
We propose to adaptively gather information from previous unobstructed angles along the trajectory from the previous SoTA method ETPNav~\cite{an2024etpnav} on VLN-CE. As illustrated in Figure~\ref{teaser}, during the update of the topo map with the predicted waypoints, we introduce a trainable transformer encoder layer that adaptively selects the optimal visual representation $\tilde{v}_g$ for each ghost node g. At each time step t, the visual representations $\mathcal{V^p}_t = \{v^p_1, v^p_2, \dots, v^p_n\}$ are processed through the trainable transformer encoder layer, which applies self-attention to capture dependencies between the visual features:
\[
V'_t = \text{SelfAttn}(V^p_t)
\]
Here, $V'_t \in \mathbb{R}^{n \times d}$ is the output matrix of the transformer, which incorporates the contextual relationships between the visual features. Instead of averaging the visual features, the transformer encoder layer uses a learned attention mechanism. Specifically, after applying the transformer block, the layer computes a set of learned weights $\mathcal{W} = \{w_1, w_2, w_e\}$for each input feature:

This generates attention weights, $W \in \mathbb{R}^{n \times 1}$, used to select the most relevant feature representations.
The final representation for the ghost node is computed as a weighted sum of the transformed features $v'_i$, where the weights are derived from the attention mechanism:
\[
\tilde{v}_g = \sum_{i=1}^{n} \text{Softmax}(\text{Linear}(v'_i)) v'_i
\]
Thus, the transformer encoder layer hence then learns to emphasize more informative features for the current context, enabling adaptive selection of the visual representations from multiple viewpoints (\textbf{A} and \textbf{B}). As shown in Figure~\ref{teaser}, the navigation policy identifies the optimal next viewpoint in the topological graph (selecting \textbf{C} as the next viewpoint after \textbf{A}). This prediction is based not only on the robot's current observation at \textbf{A}, but also on previous, unobstructed views (from \textbf{B}), allowing the robot to mitigate occlusions and plan more robust navigation strategies.
