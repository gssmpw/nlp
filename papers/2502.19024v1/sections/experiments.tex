
\section{EXPERIMENTS}
\vspace{-5pt}
\subsection{Experiment Setup}

\input{tables/tab1}


\input{tables/tab_abl}
In this study, we aim to evaluate the performance of serval VLN models under varying line-of-sight perspectives, transitioning from a high line-of-sight perspective (representative of human vision) to a low line-of-sight perspective (representative of small quadruped robots). This evaluation is designed to identify the performance gap caused by viewpoint discrepancies and to assess the limitations of existing VLN models in low line-of-sight scenarios. Our implementation is based on the Habitat simulator~\cite{savva2019habitat} and uses the Matterport3D (MP3D) dataset~\cite{chang2017matterport3d}, which offers photo-realistic 3D environments with both panoramic and line-of-sight variations, effectively simulating real-world conditions. We employed a two-stage training process, with the first stage involving learning on a scaled dataset generated from the HM3D, Gibson, and MP3D datasets for waypoint prediction, and the second stage training on navigation task-specific data (R2R~\cite{krantz2020beyond}). The learning rate was set to 0.0001 with a batch size of 32. Our approach was benchmarked against multiple baselines, including Seq2Seq~\cite{krantz2020beyond}, CMA (mono)~\cite{krantz2020beyond}, BEVBert~\cite{an2022bevbert}, and ETP~\cite{an2023etpnav}. Evaluation metrics included Trajectory Length (TL), Navigation Error (NE), Overall Success Rate (OSR), Success Rate (SR), and Success weighted by Path Length (SPL), which collectively provided a comprehensive assessment of model performance. In our evaluation, a navigation attempt was considered successful if the robot reached within 3 meters of the target location. Additionally, we deployed our proposed method on a Xiaomi Cyberdog for real-world tests, comparing its performance against two monocular methods and two panoramic methods, to demonstrate its robustness in diverse environments.
% In this study, we explore the performance of Vision-and-Language Navigation (VLN) models when transitioning from a high line-of-sight perspective (similar to that of a human) to a low line-of-sight perspective (akin to small quadruped robots) in both simulated and real-world environments. Our experimental design aims to assess the performance gap arising from viewpoint discrepancies, highlighting the limitations of existing models in low line-of-sight scenarios. We conducted our experiments in the simulated environment provided by the Habitat simulator \cite{savva2019habitat}, using the Matterport3D (MP3D) dataset \cite{chang2017matterport3d}, which provides photo-realistic 3D environments with both panoramic and line-of-sight variations, closely mimicking real-world conditions. Our method was benchmarked against multiple baselines, including Seq2Seq\cite{krantz2020beyond}, CMA (mono)\cite{krantz2020beyond}, CMA (pano)\cite{hong2022bridging}, Recurrent-BERT\cite{hong2022bridging}, BEVBert\cite{an2022bevbert}, and ETP\cite{an2023etpnav}. Additionally, we deployed our approach on a Xiaomi Cyberdog in real-world tests, comparing its performance to two monocular methods and two panoramic methods.
% \GZ{Rewrite this paragraph, including the following main points: (1) Introduction to the purpose of the following experiments. (2) Implementation details (training details: lr, two-stage; dataset \& benchmark details ...) (3) baseline explanation (4) evaluation metrics.}



% \begin{table*}[th]
% \caption{
% The performance gap in VLN models trained exclusively on high line-of-sight perspective data that testing with low line-of-sight agents.
% N: Navigation Policy || WP: Waypoint Predictor
% }
% \label{}
% \centering
% \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c|c}
% \hline
%   && &  \multicolumn{12}{c}{Changing in Viewpoint} \\
% \hline
%  && & \multicolumn{6}{c|}{Val\_seen} & \multicolumn{6}{c}{Val\_unseen} \\
% \hline
%   && & TL& NE$\downarrow$ & nDTW$\uparrow$& OSR$\uparrow$& SR$\uparrow$& SPL$\uparrow$& TL& NE$\downarrow$& nDTW$\uparrow$& OSR$\uparrow$& SR$\uparrow$&SPL$\uparrow$\\
% \hline
% \multirow{4}{*}{Monocular}  & \multirow{2}{*}{seq2seq} & H& 9.37& 7.02& 0.54& 0.46& 0.33& 0.31& 9.32& 7.77& 0.47& 0.47& 0.25& 0.22\\ \cline{3-15}
%  & & D& 9.15& 9.58& 0.30& 0.17& 0.06& 0.04& 9.23& 9.35& 0.30& 0.17& 0.05&0.04\\
% \cline{2-15}
%   & \multirow{2}{*}{CMA(mono)} & H& 9.26& 7.12& 0.54& 0.46& 0.37& 0.35& 8.64& 7.37& 0.51& 0.40& 0.32& 0.30\\ \cline{3-15}
%  & & D& 10.29& 7.69& 0.45& 0.36& 0.23& 0.20& 7& 7.75& 0.43& 0.32& 0.19&0.16\\
% \hline
% \multirow{10}{*}{Panoramic}  & \multirow{2}{*}{CMA(pano)} & H& 11.47& 5.20& 0.61& 0.61& 0.51& 0.45& 10.9& 6.2& 0.55& 0.52& 0.41& 0.36\\ \cline{3-15}
%  & & D& 12.49& 8.43& 0.31& 0.33& 0.19& 0.13& 15.05& 8.41& 0.31& 0.33& 0.18&0.13\\ 
% \cline{2-15}
%   & \multirow{2}{*}{RecurrentBert} & H& 12.5& 5.02& 0.58& 0.59& 0.50& 0.44& 12.23& 5.74& 0.54& 0.53& 0.44& 0.39\\ \cline{3-15}
%  & & D& 16.38& 7.41& 0.37& 0.37& 0.26& 0.19& 16.41& 7.32& 0.35& 0.37& 0.23&0.16\\ 
% \cline{2-15}
%   & \multirow{2}{*}{BEVBert}& H& 12.35 & 3.22 & 0.70& 0.78& 0.71& 0.63& -& 4.70& -& 0.67& 0.59& 0.50\\ \cline{3-15}
%  & & D& 22.77 & 7.55 & 0.32 & 0.38 & 0.28 & 0.18 & 22.25 & 7.58 & 0.31 & 0.37 & 0.27 & 0.17\\ 
% \cline{2-15}
%   & \multirow{2}{*}{ETP(DUET)} & H& 11.78& 3.95& -& 0.72& 0.66& 0.59& 11.99& 4.71& -& 0.65& 0.57& 0.49\\ \cline{3-15}
%  & & D& 22.32 & 8.53 & 0.26 & 0.32 & 0.22 & 0.13 & 21.79 & 8.14 & 0.26 & 0.32 & 0.21 & 0.11\\ \hline

% \end{tabular}
% \end{table*}
\subsection{Comparison on Simulated Environments}
\noindent\textbf{Impact of Changing Line-of-Sight:} We evaluated VLN models trained exclusively on high line-of-sight data (approximately 1.7 meters, simulating a human perspective) and tested them under both high and low line-of-sight conditions. This comparison revealed a substantial performance gap between the two settings, illustrating the difficulty of applying models trained on high line-of-sight visual data to small quadruped robots. As shown in Table~\ref{tab:policy}, models M\#1 and M\#5 scored 13\% lower SR scores, respectively, for the CMA and RecurrentBert. Comparing M\#3 to M\#9 and M\#4 to M\#11, the SR scores are reduced by 32\% and 36\% for BEVBert and ETPNav, respectively. The disparity between these two perspectives, particularly in landmark recognition, depth perception, and spatial awareness, leads to navigation errors, underscoring that models trained on high line-of-sight data are not directly transferable to small robots with a pronounced downward-shifting line of sight. Especially for BEVBert(M\#9, M\#10) and ETPNav(M\#11, M\#12), as they heavily rely on depth information for spatial accessibility to predict waypoint~\cite{an2023etpnav, an2022bevbert}. To address this, we re-trained exclusively on the same model configuration using low line-of-sight visual inputs and there remains a noticeable drop in performance when compared to high line-of-sight tasks that are shown in Table~\ref{tab:policy}(M\#6, M\#8, M\#10 and M\#12). This highlights the inherent limitations of re-training the model on low-perspective data without introducing additional architectural adjustments or compensatory mechanisms. The reduced performance underscores that the discrepancy between viewpoints introduces a domain gap, which cannot be bridged solely through data re-training. This finding aligns with the results discussed in Section III, Part B, which highlighted the mismatch between the oracle and local observations due to occlusions. This reflects a critical limitation in the design of existing VLN datasets, which primarily focus on high-level, human-like visual data, leaving a performance gap when applied to low perspectives. In M\#13 we compared our GVNav with current state-of-the-art methods on the R2R-CE dataset. The results demonstrate that our model outperforms the existing models on all splits in terms of NE: 0.26-0.72, nDTW: 1\%-5\%, OSR: 2\%-5\%, SR: 3\%-8\%, and SPL: 2\%-7\%. 

\input{tables/tab3}

\noindent\textbf{Navigator vs. Waypoint Predictor}:
Table~\ref{tab:wp} presents ablation experiments designed to isolate the contributions of the waypoint predictor and the navigator as individual components. The experiments demonstrate that under challenging low line-of-sight conditions, the waypoint predictor has a more pronounced impact on performance compared to the navigator. We tested ETPnav in VLN-CE R2R and freeze/re-train individual components including the navigation policy and waypoint prediction networks. The results show that Re-training the waypoint predictor significantly improves the modelâ€™s generalization and navigation accuracy, from 21\% SR to 39\% SR, even when the navigator remains frozen. In contrast, re-training the navigator without updating the waypoint predictor yields only marginal performance improvements, from 21\% SR to 32\% SR as the static waypoints limit the agent's ability to navigate effectively under low-visibility conditions. To address this limitation, we constructed a larger waypoint prediction dataset, allowing for more comprehensive training of the waypoint prediction network. We use the same evaluation metric as Hong et.al~\cite{hong2022bridging}, where $\mid$$\Delta$$\mid$ measures the difference in number of target waypoints and predicted waypoints. \%Open measures the ratio of predicted waypoints that is in open space, which is the most important factor. dC and dH are the Chamfer distance and the Hausdorff distance, respectively. As shown in Table~\ref{tab:scale}, scaling up the training dataset by 22.02 times results in a 7.99\% increase in open space (1st and 4th element in Table~\ref{tab:scale}), demonstrating the effectiveness of expanding waypoint prediction training data for low line-of-sight navigation tasks.

\input{tables/tab4}


%& $\mid$$\Delta$$\mid$ & \%~Open$\uparrow$ & dC$\downarrow$ & dH$\downarrow$ & $\mid$$\Delta$$\mid$ & \%Open$\uparrow$ & dC$\downarrow$ & dH$\downarrow$\\
% \noindent\textbf{\textcolor{blue}{Final Performance in Simulation}}
% \subsection{Comparison on Simulated Environments}
% \begin{table*}[th]
% \caption{Models are retrained specifically for low line-of-sight perspective visual input, there was still a noticeable drop in performance compared to high-perspective tasks}
% \label{table_example}
% \centering
% \begin{tabular}{c||c|c|c|c|c|c|c|c|c|c|c|c|c}
% \hline
%  & &  \multicolumn{11}{|c}{0.30m height - Retrained} \\
% \hline
% & & \multicolumn{6}{|c|}{Val\_seen} & \multicolumn{6}{|c}{Val\_unseen} \\
% \hline
%  & & TL& NE$\downarrow$ & nDTW$\uparrow$& OSR$\uparrow$& SR$\uparrow$& SPL$\uparrow$& TL& NE$\downarrow$& nDTW$\uparrow$& OSR$\uparrow$& SR$\uparrow$&SPL$\uparrow$\\
% \hline
% \multirow{2}{*}{Monocular} & seq2seq\cite{krantz2020beyond} & 7.96 & 6.82 & 0.53 & 0.39 & 0.21 & 0.2 & 7.66 & 7.11 & 0.51 & 0.34 & 0.19 & 0.18\\
% \cline{2-14}
%  & CMA(mono)\cite{krantz2020beyond} & 7.99& 6.43& 0.53& 0.41& 0.24& 0.21& 7.16 & 7.41 & 0.53& 0.36 & 0.22 & 0.18\\
% \hline
% \multirow{5}{*}{Panoramic} & CMA(pano)\cite{hong2022bridging} & 13.24 & 5.99  & 0.53 & 0.58 & 0.47 & 0.37 & 15.53 & 6.77 & 0.44 & 0.44& 0.31 & 0.23\\
% \cline{2-14}
%  & RecurrentBert\cite{hong2022bridging} & 14.13& 5.31& 0.54& 0.57& 0.45& 0.37& 14.45& 5.99& 0.48& 0.48& 0.37&0.26 \\
% \cline{2-14}
%  & BEVBert\cite{an2023bevbert} & 14.05 & 5.02 & 0.58 & 0.61 & 0.51 & 0.43 & 15.23 & 5.61 & 0.53 & 0.57 & 0.47 & 0.38\\
% \cline{2-14}
%  & ETP(DUET)\cite{an2024etpnav} & 12.35 & 4.6 & 0.62 & 0.68 & 0.58 & 0.50 & 12.73& 5.15& 0.57& 0.60& 0.52& 0.43\\
% \hline
% \end{tabular}
% \end{table*}


\subsection{Comparison on Real-world Environments}


% \begin{table*}[th]
% \caption{We compared our approach with other method that deployed on Xiaomi Cyberdog, they are tested in 4 environments.}
% \label{table_example}
% \centering
% \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c}
% \hline
%  \multicolumn{17}{c}{Real Environments}\\
% \hline
% & \multicolumn{4}{c|}{Gaming Room} & \multicolumn{4}{c|}{Kitchen} &  \multicolumn{4}{c}{Lab} & \multicolumn{4}{|c}{Office Area}\\
% \hline
% & TL& NE$\downarrow$ & nDTW$\uparrow$& OSR$\uparrow$& SR$\uparrow$& SPL$\uparrow$& TL& NE$\downarrow$& nDTW$\uparrow$& OSR$\uparrow$& SR$\uparrow$&SPL$\uparrow$\\
% \hline
%  seq2seq\cite{krantz2020beyond} & - & - & 0 & 0 & - & - & 0 & 0 & - & - & 0 & 0 & - & - & 0 & 0 \\
% \cline{1-17}
%  CMA(mono)\cite{hong2022bridging} & - & - & 0 & 0 & - & - & 0 & 0 & - & - & 0 & 0 & - & - & 0 & 0 \\ 
% \cline{1-17}
%  BEVBert\cite{an2023bevbert} & & & & & & & & &  & & & & & & &\\
% \cline{1-17}
%  ETP(DUET)\cite{an2024etpnav} & & & & & & & & &  & & & & & & &\\
% \cline{1-17}
%  Ours & & & & & & & & &  & & & & & & &\\
% \cline{1-17}
% \end{tabular}
% \end{table*}

We deployed our GVNav approach on a Xiaomi Cyberdog to demonstrate its capability to navigate in real-world environments based on given instructions. Our pipeline allows the robot to perform low-level point navigation, effectively enabling it to navigate in unseen environments without prior mapping. The hardware was upgraded with an Intel RealSense D455 camera for more accurate depth sensing. We integrated a 360Â° TTL programmable gear motor to rotate the camera in precise 30Â° increments, capturing 12 images to form a full panoramic view. These images were fed into our navigation model for processing. All models, including CLIP, the waypoint predictor, and our navigation policy, were executed in real-time on a laptop equipped with an NVIDIA RTX 3080 Mobile GPU (16 GB VRAM).

We evaluated our method in four distinct environments: a gaming room, a kitchen, a laboratory, and an office area, with 25 unique instructions provided for each scene. As shown in Table~\ref{tab:tab3}, our approach was successfully deployed on a real robot for Vision-and-Language Navigation (VLN) tasks in real-world settings with a low line-of-sight, and the robot effectively navigated through these diverse environments. In addition to the metrics outlined in Table~\ref{tab:tab3}, our approach outperformed other methods in both simulated and real-world environments under low line-of-sight conditions.The gaming room represented a particularly challenging environment due to its cluttered layout and limited open space. In contrast, the kitchen was a smaller but relatively open area with few branching paths. The laboratory offered a more spacious and less cluttered environment, while the office area presented a highly expansive space with numerous branching paths. These diverse environments allowed for a thorough evaluation of the robustness and adaptability of our method in various spatial and navigational complexities. 

% To evaluate performance, we used a tape measure to calculate Trajectory Length (TL) and Navigation Error (NE). Additionally, we calculated the Overall Success Rate (OSR) and Success Rate (SR) to measure task completion. In our evaluation, a navigation attempt was considered successful if the robot reached within 2 meters of the target location.






% \begin{table*}[th]
% \caption{An Example of a Table}
% \label{table_example}
% \centering
% \begin{tabular}{c||c|c|c|c|c|c|c|c|c|c|c|c|c}
% \hline
%  & &  \multicolumn{12}{|c}{1.25m height - default} \\
% \hline
% & & \multicolumn{6}{|c|}{Val\_seen} & \multicolumn{6}{|c}{Val\_unseen} \\
% \hline
%  & & TL& NE& nDTW& OSR& SR& SPL& TL& NE& nDTW& OSR& SR&SPL\\
% \hline
% \multirow{3}{*}{Monocular} & seq2seq & & & & & & & & & & & & \\
% \cline{2-14}
%  & CMA(mono) & & & & & & & & & & & & \\
% \cline{2-14}
%  & WS-MGmap & & & & & & & & & & & & \\
% \hline
% \multirow{5}{*}{Panoramic} & CMA(pano) & & & & & & & & & & & & \\
% \cline{2-14}
%  & RecurrentBert & & & & & & & & & & & & \\
% \cline{2-14}
%  & GridMM & & & & & & & & & & & & \\
% \cline{2-14}
%  & BEVBert & & & & & & & & & & & & \\
% \cline{2-14}
%  & ETP(DUET) & & & & & & & & & & & & \\
% \hline
% \end{tabular}
% \end{table*}

% \begin{table*}[th]
% \caption{An Example of a Table}
% \label{table_example}
% \centering
% \begin{tabular}{c||c|c|c|c|c|c|c|c|c|c|c|c|c}
% \hline
%  & &  \multicolumn{12}{|c}{0.30m height - Zeroshot} \\
% \hline
% & & \multicolumn{6}{|c|}{Val\_seen} & \multicolumn{6}{|c}{Val\_unseen} \\
% \hline
%  & & TL& NE& nDTW& OSR& SR& SPL& TL& NE& nDTW& OSR& SR&SPL\\
% \hline
% \multirow{3}{*}{Monocular} & seq2seq & & & & & & & & & & & & \\
% \cline{2-14}
%  & CMA(mono) & & & & & & & & & & & & \\
% \cline{2-14}
%  & WS-MGmap & & & & & & & & & & & & \\
% \hline
% \multirow{5}{*}{Panoramic} & CMA(pano) & & & & & & & & & & & & \\
% \cline{2-14}
%  & RecurrentBert & & & & & & & & & & & & \\
% \cline{2-14}
%  & GridMM & & & & & & & & & & & & \\
% \cline{2-14}
%  & BEVBert & & & & & & & & & & & & \\
% \cline{2-14}
%  & ETP(DUET) & & & & & & & & & & & & \\
% \hline
% \end{tabular}
% \end{table*}

\begin{figure}[t]
      \centering
      \includegraphics[width=0.95\linewidth]{figures/video.pdf}
      %\includegraphics[scale=1.0]{figurefile}
      \vspace{-10pt}
      \caption{Real-world demo of our proposed Ground-View approach, for vision-and-language navigation. Given the human instruction, GVNav only takes 12 RGBD images as input and outputs a predicted waypoint for robotic execution.}
      \label{demo}
      \vspace{-15pt}
\end{figure}



% \begin{table*}[th]
% \caption{Waypoint predictor trained with $\times 22.02$ in training data amount, the results shows that the performance of waypoint prediction are increased around 5-8\% compare to the baseline}
% \label{table_example}
% \begin{center}
% \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c}
% \hline
% \multirow{6}{*}{Waypoint Predictor}& \multirow{2}{*}{\#}& \multirow{2}{*}{Height}& \multicolumn{4}{c}{MP3D Train}& \multicolumn{4}{|c}{MP3D Val\_Unseen}\\
% \cline{4-11}
% & & & $\mid$$\Delta$$\mid$ & \% Open$\uparrow$ & dC$\downarrow$ & dH$\downarrow$ & $\mid$$\Delta$$\mid$ & \%Open$\uparrow$ & dC$\downarrow$ & dH$\downarrow$\\
% \hline
%  & 1& Baseline(1.25m)\cite{hong2022bridging}& 1.3& 82.56& 1.12& 2.13& 1.4& 79.86& 1.07&2\\
%  & 2& Retrained(0.3m)& 1.29& 84.23& 1.1& 2.11& 1.36& 81.91& 1.06&2\\
%  & 3& Scaled(RGBD)& 1.29& 88.7& \textbf{1.05}& \textbf{2.01}& 1.38& 87.16& \textbf{0.99}& \textbf{1.9}\\
%  & 4& Scaled(Depth Only)& 1.33 & \textbf{90.02} & 1.06 & 2.04 & 1.42 & \textbf{89.90} & 1 & 1.92 \\\hline
% \end{tabular}
% \end{center}
% \end{table*}



% \begin{table*}[th]
% \caption{Ablation experiments on the navigator and waypoint predictor }
% \label{table_example}
% \centering
% \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c|c}
% \hline
%   \multicolumn{15}{c}{Waypoint Prediction Methods}\\
% \hline
%  \multirow{2}{*}{N} & \multirow{2}{*}{WP} & & \multicolumn{6}{c|}{Val\_seen} & \multicolumn{6}{c}{Val\_unseen} \\
% \cline{3-15}
%   & & & TL& NE$\downarrow$ & nDTW$\uparrow$& OSR$\uparrow$& SR$\uparrow$& SPL$\uparrow$& TL& NE$\downarrow$& nDTW$\uparrow$& OSR$\uparrow$& SR$\uparrow$&SPL$\uparrow$\\ \hline
% \multirow{5}{*}{Freeze}  & \multirow{5}{*}{Freeze} & CMA\cite{hong2022bridging} & 12.49& 8.43& 0.31& 0.33& 0.19& 0.13& 15.05& 8.41& 0.31& 0.33& 0.18& 0.13\\ \cline{3-15}
%  & & RecurrentBert\cite{hong2022bridging} & 16.38& 7.41& 0.37& 0.37& 0.26& 0.19& 16.41& 7.32& 0.35& 0.37& 0.23&0.16\\
% \cline{3-15}
%  & & BEVBert\cite{an2023bevbert} & 22.77 & 7.55 & 0.32 & 0.38 & 0.28 & 0.18 & 22.25& 7.58& 0.31& 0.37& 0.27&0.17\\
% \cline{3-15}
%  & & ETP(DUET)\cite{an2024etpnav} & 22.32 & 8.53 & 0.26 & 0.33 & 0.22 & 0.13 & 21.79& 8.14& 0.26& 0.32& 0.21& 0.12\\ \hline 
% \multirow{5}{*}{Freeze}  & \multirow{5}{*}{Re-trained} & CMA\cite{hong2022bridging} & 14.05& 7.75& 0.42& 0.47& 0.29& 0.24& 13.33& 7.83& 0.40& 0.40& 0.26& 0.21\\ \cline{3-15}
%  & & RecurrentBert\cite{hong2022bridging} & 14.27& 6.31& 0.48& 0.50& 0.38& 0.32& 14.3& 6.78& 0.44& 0.43& 0.31&0.26\\
% \cline{3-15}
%  & & BEVBert\cite{an2023bevbert}& 16.00 & 4.96 & 0.54 & 0.69 & 0.54 & 0.42 & 17.06 & 6.26 & 0.44 & 0.61 & 0.44 & 0.32\\
% \cline{3-15}
%  & & ETP(DUET)\cite{an2024etpnav} & 18.63 & 6.45 & 0.41 & 0.58 & 0.38 & 0.25 & 19.67& 6.57& 0.37& 0.56& 0.39& 0.24\\ \hline 
%  \multirow{5}{*}{Re-trained}  & \multirow{5}{*}{Freeze} & CMA\cite{hong2022bridging} & 15.98 & 8.76 & 0.47& 0.47 & 0.26 & 0.19 & 12.50 & 7.73 & 0.37 & 0.37 & 0.25 & 0.19\\ \cline{3-15}
%  & & RecurrentBert\cite{} & 10.91 & 5.70 & 0.57 & 0.45 & 0.38 & 0.34 & 10.64 & 6.22 & 0.51 & 0.36 & 0.30 & 0.26\\
% \cline{3-15}
%  & & BEVBert & 19.50 & 7.35 & 0.37 & 0.35 & 0.29 & 0.20 &  & & & & &\\
% \cline{3-15}
%  & & ETP(DUET) & 16.39 & 7.08 & 0.39 & 0.35 & 0.30 & 0.22 & 16.61& 6.61& 0.40& 0.36& 0.32& 0.23\\ \hline 
%  \multirow{5}{*}{Re-trained}  & \multirow{5}{*}{Re-trained} & CMA &13.24 & 5.99  & 0.53 & 0.58 & 0.47 & 0.37 & 15.53 & 6.77 & 0.44 & 0.44& 0.31 & 0.23 \\ \cline{3-15}
%  & & RecurrentBert & 14.13& 5.31& 0.54& 0.57& 0.45& 0.37& 14.45& 5.99& 0.48& 0.48& 0.37&0.26\\
% \cline{3-15}
%  & & BEVBert & 14.05 & 5.02 & 0.58 & 0.61 & 0.51 & 0.43 & 15.23 & 5.61 & 0.53 & 0.57 & 0.47 & 0.38\\
% \cline{3-15}
%  & & ETP(DUET) & 12.35 & 4.6 & 0.62 & 0.68 & 0.58 & 0.50 & 12.73& 5.15& 0.57& 0.60& 0.52& 0.43\\ \hline 

% \end{tabular}
% \end{table*}


\input{tables/tab_example}
% \begin{table*}[th]
% \caption{An Example of a Table}
% \label{table_example}
% \centering
% \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c|c}
% \hline
%  \multicolumn{15}{c}{Simulator}\\
% \hline
% & \multicolumn{7}{c|}{Val\_seen} & \multicolumn{7}{c}{Val\_unseen}\\
% \hline
% & TL& NE$\downarrow$ & nDTW$\uparrow$& OSR$\uparrow$& Co$\uparrow$ &SR$\uparrow$& SPL$\uparrow$& TL& NE$\downarrow$& nDTW$\uparrow$& OSR$\uparrow$& Co$\uparrow$ &SR$\uparrow$&SPL$\uparrow$\\
% \hline
%  seq2seq & 9.15 & 9.58 & 0.30 & 0.17 & - & 0.06 & 0.04 & 9.23 & 9.35 & 0.30 & 0.17 & - & 0.05 & 0.04\\
% \cline{1-15}
%  CMA(mono) & 7.96 & 6.82 & 0.53 & 0.39 & - & 0.21 & 0.20 & 7.66 & 7.11 & 0.51 & 0.34 & - & 0.19 & 0.18\\ 
% \cline{1-15}
%  CMA(pano)& 13.24 & 5.99  & 0.53 & 0.58 & - &0.47 & 0.37 & 15.53 & 6.77 & 0.44 & 0.44& - &0.31 & 0.23 \\
% \cline{1-15}
%  RecurrentBert & 14.13& 5.31& 0.54& 0.57& - &0.45& 0.37& 14.45& 5.99& 0.48& 0.48& - &0.37&0.26 \\
% \cline{1-15}
%  BEVBert & 14.05 & 5.02 & 0.58 & 0.61 & 0.08 & 0.51 & 0.43 & 15.23 & 5.61 & 0.53 & 0.57 & 0.10 & 0.47 & 0.38\\
% \cline{1-15}
%  ETP(DUET) & 12.35 & 4.61 & 0.62 & 0.68 & 0.11 & 0.58 & 0.50 & 12.73 & 5.15 & 0.57 & 0.60 & 0.08 & 0.52 & 0.43\\
% \cline{1-15}
%  Ours(GV-VLN)& \textbf{12.34} & \textbf{3.88} & \textbf{0.66} & \textbf{0.70} & \textbf{0.05} & \textbf{0.64} & \textbf{0.56}& 13.76 & \textbf{4.89} & \textbf{0.58} & \textbf{0.62} & \textbf{0.05} & \textbf{0.55} & \textbf{0.45}\\
% \cline{1-15}
% \end{tabular}
% \end{table*}

