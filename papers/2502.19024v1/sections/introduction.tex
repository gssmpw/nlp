\section{INTRODUCTION}
\vspace{-3pt}
Vision-and-Language Navigation (VLN) is a challenging cross-domain research field that requires an agent to interpret natural language instructions from humans and navigate in unseen environments by executing a sequence of actions. 

There have been significant advancements in understanding and aligning vision, language, and action in navigation tasks~\cite{anderson2018vision,qi2020reverie, ku2020room, hong2022bridging, wang2023scaling}, nevertheless, the effectiveness of these developments is limited when applied to practical scenarios, as they are primarily designed in discrete environments, where the agent can only navigate on predefined navigation graph by teleporting between adjacent nodes. Therefore, Krantz et al.~\cite{krantz2020beyond} proposed a benchmark that sets the VLN task in a continuous photo-realistic reconstructing 3D environment where visual agents are required to execute low-level discrete actions.
Irshad et al.\cite{irshad2021hierarchical} introduced a hierarchical model to better simulate real robotic actions by estimating the agent's linear and angular velocities as continuous actions within the Robo-VLN environment. Recently, based on the close-to-human~\cite{anderson2018vision} performance of VLN tasks both in discrete settings~\cite{wang2023scaling} (over 80\% successful rate) and continuous settings~\cite{an2022bevbert} (over 60\% successful rate), researchers are extending the VLN task into real robot experiments ~\cite{zhang2024navid, wang2024sim, li2024human, yokoyama2024vlfm}. However, a significant performance gap between simulation and real-world deployment has been identified.

One of the primary reasons for this gap is the mismatch of panoramic observation in VLN research and monocular observation on real robots. Most existing Sim-to-Real VLN models rely on monocular RGBD cameras as visual sensors, limiting the agent’s field of view and preventing panoramic observation. This restricted visual input hinders the agent’s ability to perceive the environment and make informed decisions. Zhang et al.~\cite{zhang2024vision} demonstrated that panoramic visual input significantly outperforms monocular input across various performance metrics, further emphasizing the limitations of monocular sensors in real-world applications.

%形象的例子（人视角狗视角区别）
\begin{figure}[t]
      \centering
      \includegraphics[width=.99\linewidth]{figures/teaser1.pdf}
      % \includegraphics[scale=0.98]{figures/teaser1.pdf}
      \vspace{-25pt}
      \caption{There is a significant viewpoint height discrepancy between humans and the robot dog (Up: human, Down: dog). Humans typically have a much higher line of sight compared to the robot dog. Our waypoint prediction network could provide robust prediction under a low line of sight.}
      \label{view}
\vspace{-20pt}
\end{figure}

Moreover, in real-world applications where humans issue commands and robots execute actions, there is often a significant discrepancy in viewpoint height between humans and most robots, such as a robot dog. Humans typically have a much higher line of sight, allowing them to observe a broader and more comprehensive view of the environment. In contrast, the robot dog’s lower viewpoint limits its field of vision, focusing more on ground-level obstacles and localized surroundings. This height disparity introduces an information asymmetry: humans issue commands based on a global understanding of the environment, while the robot dog, constrained by its limited perspective, makes decisions based on partial, localized information. This mismatch can lead to errors in command interpretation, particularly in complex environments where the robot lacks sufficient information to execute tasks accurately such as shown in Figure~\ref{view}, and this mismatch can not be solved by simply raise the height of the dog's sensor since it will decrease the passability in constrained environments, potentially impeding its ability to maneuver through narrow spaces or under obstacles.

To the best of our knowledge, current VLN research has not adequately addressed the impact of this visual information gap on performance, which poses practical challenges in applications involving various forms, such as assistive robots autonomous vehicles. Exploring these gaps is crucial for improving VLN tasks in real-world robots, which vary in shape and visual perspective. In this paper, we identify several challenges in deploying VLN systems on real robots, using the Xiaomi Cyberdog—a typical small dog-shaped robot with a low line of sight—as a case study:
(1) VLN methods are navigating through panoramic observation, but most of the robots are constructed with monocular RGBD cameras as visual sensors. 
(2) There are numerous visual domain variances to transfer the VLN model from a simulator to the real world:
Firstly, small-sized dog-like robots such as Unitree Go1 and Xiaomi Cyberdog are only around 30cm in height. The reduction in the height of the viewpoint leads to a different understanding of landmarks.
Secondly, ground-level viewpoint also results in a significant performance drop in depth-only waypoint prediction used by most VLN-CE approaches~\cite{an2022bevbert, wang2023gridmm, an2023etpnav}.
Thirdly, instructions in existing datasets are primarily designed based on human's line of sight, which are not always suitable for quadruped robots.
(3) The generalizability of waypoint prediction has been underestimated in VLN-CE R2R benchmarks. The waypoint prediction does not perform well in more complex real environments.
%In this paper, we mainly focus on those three gaps. 
Our contribution includes:
\begin{enumerate}
\item We assessed the impacts of notable differences in visual information between human-issued instructions and robot dogs' execution by reconstructing the Xiaomi Cyberdog with a programmable motor to spin an RGBD camera to get panoramic visual input.
\item We assessed the impact of waypoint prediction on ground-level viewpoint between depth-only and RGBD waypoint predictions. We further transfer the connectivity graphs from public 3D scans as extra data to power up the generalization ability of waypoint predictors in real-world complex environments. 
\item We proposed an adaptive information-gathering module to handle obstruction in local observation by assigning appropriate weights to identical features across different viewpoints, significantly enhancing performance in both simulated environments and real-world deployments with quadruped robots.
\end{enumerate}