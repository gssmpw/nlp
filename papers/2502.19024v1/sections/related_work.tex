\section{RELATED WORK}

\subsection{Vision-and-Language Navigation}
% A large endeavor of learning to navigate in unvisited environments following human instructions is established over discretized simulated scenes with pre-defined navigation graphs~\cite{anderson2018vision, anderson2020rxr, qi2020reverie, thomason2020cvdn}. To aid the learning process of aligning language and visual observations for decision-making, Fried et al.~\cite{fried2018speaker} introduced the concept of navigation through panoramic actions, enabling the agent to teleport between adjacent nodes on the graph by selecting an image-oriented towards the target node. Following this, research in VLN is constantly pushing the limit of model performance toward human~\cite{ma2019self, wang2019reinforced, fried2018speaker, tan2019envdrop, ke2019tactical, fu2020counterfactual, qi2020object, hong2020graph, hao2020towards,li2019robust,hong2020recurrent,majumdar2020improving,chen2022think,wang2023gridmm}. Recently, Wang et al.\cite{wang2023scaling} pushed up the performances to 80\% single-run success rate on the well-recognized R2R-VLN benchmark\cite{anderson2018r2r}. However, the effectiveness of these developments is limited due to the high-level panoramic action space when applied to practical scenarios. Therefore, Krantz et al.\cite{krantz2020beyond} proposed a benchmark that transfers the VLN task from a discrete to a continuous environment that closely mimics real-world scenarios. However, directly transferring VLN methods in a continuous environment leads to a significant performance drop~\cite{irshad2021hierarchical,irshad2022semantically,raychaudhuri2021language}. To benefit from the simplicity of learning cross-modal alignment in a discrete environment, \cite{hong2022bridging, krantz2020navgraph, krantz2021waypoint} propose waypoint models to bridge the gap between VLN and VLN-CE. Moreover, \cite{hong2022bridging} indicates the proposed waypoint direction and step size would largely affect the decision-making process of VLN policy. In this work, we strive to optimize waypoint prediction under low sight-of-line.

In recent years, significant efforts have been devoted to enabling navigation in previously unvisited environments based on human instructions. This research is often conducted within discretized simulated scenes that utilize predefined navigation graphs~\cite{anderson2018vision, anderson2020rxr, qi2020reverie, thomason2020cvdn}. To facilitate the alignment of language and visual cues for decision-making, Fried et al.~\cite{fried2018speaker} introduced the concept of navigation through panoramic actions. This method allows the agent to teleport between adjacent nodes on the graph by selecting an image oriented toward the target node. Building on this foundation, research in VLN has made steady progress in improving model performance towards human-level capabilities~\cite{ma2019self, wang2019reinforced, fried2018speaker, tan2019envdrop, ke2019tactical, fu2020counterfactual, qi2020object, hong2020graph, hao2020towards,li2019robust,hong2020recurrent,majumdar2020improving,chen2022think,wang2023gridmm}. Recently, Wang et al.~\cite{wang2023scaling} achieved an 80\% single-run success rate on the widely recognized R2R-VLN benchmark~\cite{anderson2018r2r}. However, these advancements remain constrained by the limitations of the high-level panoramic action space when applied to real-world scenarios. To address this, Krantz et al.~\cite{krantz2020beyond} proposed a benchmark that shifts the VLN task from a discrete to a continuous environment, more closely resembling real-world settings. Despite this shift, directly transferring VLN methods into continuous environments has resulted in substantial performance declines~\cite{irshad2021hierarchical, irshad2022semantically, raychaudhuri2021language}. To overcome these challenges, several studies~\cite{hong2022bridging, krantz2020navgraph, krantz2021waypoint} have introduced waypoint models that bridge the gap between VLN and VLN-CE, maintaining the simplicity of learning cross-modal alignment in discrete environments. Notably, Hong et al.~\cite{hong2022bridging} highlight that the choice of waypoint direction and step size significantly impacts VLN policy decision-making. In this work, we aim to optimize waypoint prediction under conditions of limited line-of-sight.

\subsection{Vision-and-Language Navigation in Real Environments}

Recently, researchers have been trying to extend the VLN task in real robots. Navid~\cite{zhang2024navid} proposed a video-based large vision language model (VLM), it only requires an on-the-fly video stream from a monocular RGB camera equipped on the robot to output the next-step action with human instructions, to showcase the capability of VLMs to achieve state-of-the-art level navigation performance without any maps, odometers, or depth inputs. Wang et al.~\cite{wang2024sim} propose an approach to endow the monocular robots with panoramic traversability perception and panoramic semantic understanding.  This method transfers the high-performance panoramic VLN models to the common monocular robots and tested in real robots. Li et al.~\cite{li2024human} extended traditional VLN by incorporating dynamic human activities and relaxing key assumptions, and introduced a Human-Aware 3D (HA3D) simulator and also tested in a real robot. However, none of them indicates the performance dropping by generalization gap in different height field of view.

% \section{RELATED WORK}
% %加别的机器狗任务，和别人的不一样（不限定于VLN）
% \subsection{Vision-and-Language Navigation}
% Based on the setting of Vision-and-Language Navigation, the agent must be able to associate time-sequenced visual observations with corresponding instructions to make informed decisions.  %To aid this learning process, Fried et al.\cite{fried_speaker-follower_2018} utilised the connectivity graphs designed for Matterport3D environments and introduced the concept of navigation through panoramic actions, enabling the agent to teleport between adjacent nodes (waypoints) on the graph by selecting an image-oriented toward the target node \cite{chang_matterport3d_2017}. 
% Building on connectivity graph approach\cite{fried2018speaker}, one of the most advanced subsequent research such as  \cite{hao2020towards,li2019robust,hong2020recurrent,majumdar2020improving,chen2022think,wang2023gridmm} has been implemented by leveraging the connectivity graph and operating within a high-level action space (teleporting). Recently, Wang et al.\cite{wang2023scaling} pushed up the performances to 80\% single-run success rate on the R2R test split in discrete environments. However, the effectiveness of these developments is limited when applied to more practical, real-world scenarios. 
% %Recent crucial platforms were developed such as Gibson \cite{xia_gibson_2018}, iGibson \cite{shen_igibson_2021}, MP3D \cite{chang_matterport3d_2017}, Habitat \cite{savva_habitat_2019}, CHALET \cite{yan_chalet_2019}, AI2THOR \cite{kolve_ai2-thor_2022} and House3D\cite{wu_building_2018} that offering both synthetic and photo-realistic scenes. These environments provide a more complex and realistic setting for studying agent behaviors, especially in the context of VLN. 
% Krantz et al.\cite{krantz2020beyond} proposed a benchmark that sets the VLN task in a continuous photo-realistic reconstructing 3D environment where visual agents are required to execute low-level actions to follow navigation directions by natural language instructions. This adaptation closely mimics real-world scenarios.%this benchmark transfers the discrete paths in the R2R\cite{anderson_vision-and-language_2018} (and the RxR\cite{ku_room-across-room_2020}) dataset to continuous trajectories-based and reconstructs the photo-realistic environments in the MP3D simulator to the Habitat simulator \cite{savva_habitat_2019}. 
% %This adaptation allows for a more fluid and naturalistic navigation experience, 
% \GZ{List works on de, explain the problems for de in real, introduce ce.}
% \subsection{VLN-CE with Waypoint Prediction}
% Further enhancing the capabilities of agents in VLN-CE such as \cite{irshad2021hierarchical,irshad2022semantically,raychaudhuri2021language}  to further refine agent navigation in continuous spaces. In addition, Hong et al.\cite{hong2022bridging} propose a predictor to generate a set of candidate waypoints during navigation, so that agents designed with high-level actions can be transferred to and trained in continuous environments. This approach has revealed a substantial performance gap between agents operating in discrete versus continuous environments, underscoring the complexity and challenges posed by continuous settings and the need for more sophisticated models to achieve comparable levels of proficiency. Inspired by this approach and SLAM\cite{fuentes2015visual}, State-of-The-Art methods in recent years were proposed and performed well such as \cite{konolige2011navigation, irshad2022semantically, georgakis2022cross, chaplot2020neural,wang2021structured,chen2021topological,kwon2021visual, an2023bevbert, konolige2011navigation,blanco2008toward}. Even 
% An et al.\cite{an_bevbert_2023} propose learnable topo-metric maps that utilise new spatial-awared map-based pre-training paradigm for use in VLN, but hese methods can not implicitly correlate incomplete, duplicate observations within the panoramas hence impair an agent’s spatial understanding and still not mitigate such a generalization gap because of low-height field of view of a robot. 
% %\subsection{Visual Representation in Vision-Language Pre-training} Existing approaches for VLP fall into image-based, object-based, and grid-based. Image-based methods [51] extract an overall feature for an image, yet neglect details, thus drawback on fine-grained language grounding. Object-based methods [5, 52] represent an image with dozens of objects identified by external detectors [53,54]. The challenge is that objects can be redundant and limited in predefined categories. Grid-based methods [55,56] directly use image grid features for pre-training, thus enabling multi-grained vision-language alignments. Most VLN pre-training are image-based [11, 12, 14], which rely on discrete panoramas. We introduce gridbased methods into VLN through metric maps, where the model can learn via multi-grained room layouts. 

% %\subsection{Mappings for Vision-and-Language Navigation}
% %Works on navigation have a long tradition of using SLAM\cite{fuentes2015visual} to construct metric maps \cite{chaplot2020object,narasimhan2020seeing}. 

% \subsection{Vision-and-Language Navigation in Real Environments}

% Recently, based on the considerable performance of VLN tasks in continuous photo-realistic reconstructing 3D environments. Researchers is trying to extend the VLN task in real robot. Navid \cite{zhang2024navid} proposed a video-based large vision language model (VLM), it only requires an on-the-fly video stream from a monocular RGB camera equipped on the robot to output the next-step action with human instructions, to showcase the capability of VLMs to achieve state-of-the-art level navigation performance without any maps, odometers, or depth inputs. Wang et al.\cite{wang2024sim} propose a approach to endow the monocular robots with panoramic traversability perception and panoramic semantic understanding.  This method transfers the high-performance panoramic VLN models to the common monocular robots and tested in real robot. Li et al.\cite{li2024human} extended traditional VLN by incorporating dynamic human activities and relaxing key assumptions, and introduced a Human-Aware 3D (HA3D) simulator and also tested in a real robot. However, none of them indicates the performance dropping by generalization gap in different height field of view.