\section{Implementation Details}

% \begin{CJK}{UTF8}{gbsn}中文写在这里面\end{CJK}

In this section, we provide implementation details of all the methods used in Section \ref{sec:Experiments}. Except from Large Language Models (LLMs), all the other methods are trained on 300 epochs, with an early stopping of 5. We use Adam optimizer to update model parameters. The experiments are conducted on a linux server with Ubuntu 20.04, trained on a single NVIDIA RTX A5000 GPU with 24GB memory. All the methods are trained on train set, the hyperparameters are searched on validation set, where the search space is given by:

\begin{itemize}
    \item Hidden Dimension: \{16, 32, 64, 128\},
    \item Learning Rate: \{5e-6, 1e-5, 2e-5, 3e-5, 5e-5, 1e-4\},
    \item Weight Decay: \{1e-5, 1e-4\},
    \item Batch Size: \{16, 32\},
\end{itemize}

For the task of parody detection, the threshold for each dataset is the same for all the methods. Specific, we let the threshold be $0.9415$ for \textit{Alibaba-Math}, $0.9526$ for \textit{BridePrice}, $0.9691$ for \textit{DrinkWater}, $0.9387$ for \textit{CS2}, $0.9262$ for \textit{CampusLife}, $0.9406$ for \textit{Tiktok-Trump}, $0.8768$ for \textit{Reddit-Trump}

Prior to feeding the data into the model, we utilize over sampling with replacement for parody detection, and use Synthetic Minority Over-sampling Technique (SMOTE) \citep{chawla2002smote} for sentiment classification to balance the training data.

Apart from these common settings, we introduce the detailed implementations of each specific model as follows.

\textbf{BoW+MLP} \citep{BoW}
Bag of Words (BoW) is a kind of word embedding method. In this study, the BoW model implemented in Word2Vec \citep{BoW}, aiming to predict a target word based on its surrounding context words. Before using Bag of Words, we standardize text input, remove unnecessary whitespace variations, tokenization text into individual words, and filter out high-frequency words that may not contribute much meaning. Next, we use Bag of Words in Word2Vec to get the word embedding, setting vector size to 50, window to 10, min count to 1, epochs to 50.

Multi-Layer Perceptrons (MLP) is a kind of feedforward neural network. In our study, we employ a three-layer MLP, with a dropout rate set to 0.3 and ReLU as the activation function. 

\textbf{Skip-gram+MLP} \citep{Skip-gram}
Skip-gram is a word embedding method which learns word representations by predicting context words given a target word. Before using Skip-gram, we standardize text input, avoid unnecessary whitespace variations, the text is tokenized into individual words, and filter out high-frequency words that may not contribute much meaning. Then we use Skip-gram in Word2Vec, setting vector size to 50, window to 10, min count to 1, epochs to 50.
The part of MLP is the same as in BoW+MLP.

\textbf{RoBERTa+MLP} \citep{RoBERTa}
RoBERTa ( Robustly Optimized BERT Pretraining Approach ) is an advanced variant of BERT. The part of Next sentence prediction (NSP) is removed from RoBERTa's pre-training objective. To obtain embedding of textual data, we use mean embedding method to compute the average of token embedding from last hidden state. Setting max length to 256, batch size to 32. The part of MLP is the same as in BoW+MLP.

\textbf{BNS-Net} \citep{BNS-Net}
The propagation mechanism in BNS-Net is defined as:$H = f(X, U, W)$, where $X$ represents the textual features, $U$ denotes user embeddings, and $W$ is the weight matrix. The Behavior Conflict Channel (BCC) applies a Conflict Attention Mechanism (CAM) to extract inconsistencies in behavioral patterns, while the Sentence Conflict Channel (SCC) leverages external sentiment knowledge (e.g., SenticNet) to detect implicit and explicit contradictions. BNS-Net is trained using a multi-task loss function, which combines sarcasm classification and sentiment inconsistency modeling:
$L = \lambda_1 J_{\text{sar}} + \lambda_2 J_{\text{imp}} + \lambda_3 J_{\text{exp}} + \lambda_4 J_{\text{balance}}$, where: sar is the sarcasm classification loss,imp and exp correspond to implicit and explicit sentiment contradiction losses. Balance is a balancing term to mitigate bias toward dominant classes. The balancing coefficients used in experiments are: $\lambda_1 = 1.0$, $\quad \lambda_2 = 0.5$, $\quad \lambda_3 = 0.5$, $\quad \lambda_4 = 0.2$.

\textbf{DC-Net} \citep{DC-Net}
The Dual-Channel Network is a dual-channel architecture to realize sarcasm detection by capturing the contrast between literal sentiment and implied sentiment. The model consists of Decomposer, literal channel, implied channel and analyzer. Prior to feeding data into DC-Net, we utilize the opinion lexicon from nltk 3.9.1 to identify the positive and negative word in our datasets. Following the methodology outlined in the original paper, it needs to use GLOVE to obtain the embedding and vocabulary. To generate the literal and implied sentiment labels, we leverage the parody label along with the counts of positive and negative words. These labels are then processed separately in the two channels. Finally the analyzer measure the conflicts between the channels. In our datasets, we follow the original paper and set all of the loss contributions \(\lambda_1\), \(\lambda_2\), \(\lambda_3\) of our DC-Net model are set to 1.

\textbf{QUIET} \citep{QUIET}
The Quantum Sarcasm Model detects sarcasm in text by using quantum-inspired techniques. It converts text and context inputs into dense vector representations through an embedding layer. These embeddings undergo quantum encoding, where sine and cosine functions simulate quantum amplitude and phase encoding, capturing complex relationships. The encoded features are averaged to reduce dimensionality, then passed through a hidden layer with ReLU activation. A sigmoid output layer predicts whether a comment is sarcastic or not. The model addresses class imbalance with class weights and evaluates performance using precision, recall, and F1-score. This single-modality model applies quantum-inspired methods to enhance feature transformation for sarcasm detection.


\textbf{SarcPrompt} \citep{SarcPrompt}
is a prompt-tuning method for sarcasm recognition that enhances PLMs by incorporating prior knowledge of contradictory intentions. The framework comprises two key components: (1) Prompt Construction. (2) Verbalizer Engineering. In our implementation, we adopt the question prompt approach and design bilingual templates tailored to Chinese and English datasets. For Chinese parody detection, we construct the prompt as " \{COMMENT\} \ch{这段话是在反串吗？} \{MASK\}.". For English datasets, we design"\{COMMENT\} Are you parody? \{MASK\}." To enhance model interpretability and alignment with domain knowledge, we employ a verbalizer as paper, where domain-specific label words are mapped based on dataset statistics. In parody detection, we use words like "\ch{反串}", "\ch{是}", "parody", "no". In sentiment classification, we use words like "\ch{支持}", "\ch{反对}", "support", "oppose". The total loss combines cross-entropy (classification) and contrastive losses (enhancing intra-class consistency): $L(\theta) = \lambda_1 L_{\text{sarc}}(\theta) + \lambda_2 L_{\text{con}}(\theta)$, where \(\lambda_1 = 1\) and \(\lambda_2\) is selected from \{0.05, 0.1, 0.2, 0.5, 1\} via validation, following the original paper's hyperparameter selection.

\textbf{GCN} \citep{GCN}
All Graph Neural Networks (GNNs), including GCN, GAT, and GraphSAGE, are implemented using PyTorch Geometric \citep{pyg}, with the version specified as 2.6.1. For the GCN, we set the number of graph convolution layers to 2, the size of the hidden embedding to 64, and the dropout rate to 0.5. Additionally, we incorporate residual connections \citep{residual} and layer normalization \citep{layer_norm} to enhance model performance, as suggested by \citet{classic_gnn_strong}.

\textbf{GAT} \citep{GAT}
In GAT, we adopt the same configuration as in Graph Convolutional Networks (GCN), utilizing 2 graph convolution layers, a hidden embedding size of 64, and a dropout rate of 0.5. Additionally, we set the number of attention heads to 8.

\textbf{GraphSAGE} \citep{GraphSAGE}
In GraphSAGE, we adopt the same configuration as in Graph Convolutional Networks (GCN), utilizing 2 graph convolution layers, a hidden embedding size of 64, and a dropout rate of 0.5. Additionally, we set the neighborhood size to 5.

\textbf{LLMs}
we employ a variety of LLMs from different companies to perform parody detection and sentiment classification, which include ChatGPT-4o (and 4o-mini) \citep{GPT4}, ChatGPT-o1-mini \citep{ChatGPT-o1}, ChatGPT-o3-mini \citep{ChatGPT-o3} Claude 3.5 \citep{Claude}, Qwen 2.5 \citep{Qwen2.5}, DeepSeek-V3 \citep{DeepSeek}, and DeepSeek-R1 \citep{DeepSeek-R1}.They require different kinds of input formats, objects and parameters. Except reasoning model, we set temperature to 0, which reasoning model not support this object. For reasoning model, they have to use more and more tokens to complete the reasoning procedure before outputting the content. To optimize model performance, we design task-specific prompts, ensuring that each LLM receives input formulations tailored to the characteristics of parody detection and sentiment analysis. For example, in parody detection, we design the prompt as \textit{``You are a helpful assistant trained to classify whether a statement is parody or not.''} in the system role, and \textit{``Determine whether the following comment is parody:\{text\}\texttt{\textbackslash n} Directly output 1 for parody, 0 for non-parody.''} in the user role. In particular, ChatGPT o1-mini doesn't have the system role, so we input all in the user role.



% Mention hyperparameter search space

% Mention up-sampling strategies

% 