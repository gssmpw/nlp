\section{Additional Results}

This section introduces additional results in our experiments. We introduce more results of the influence of context to parody detection in Section \ref{apd:impact_context} and the influence of parody to sentiment classification in Section \ref{apd:impact_parody}. Then, we show the performance comparison of reasoning LLMs and non-reasoning LLMs in Section \ref{apd:reasoning_llms}. Last, we investigate the impact of train ratio of embedding-based models compared with LLMs in Section \ref{apd:train_ratio}.

\subsection{Influence of Context to Parody Detection}\label{apd:impact_context}
\input{images/compare_context}

Figure \ref{fig:apd_impact_context} illustrates the detailed results of the performance comparison of the F1 score in parody detection with and without context across seven datasets. Generally, contextual information significantly enhances model performance on most datasets and methods. For instance, on \textit{Alibaba-Math}, the performance of ChatGPT4o improves from $15.9$ to $19.54$, while on \textit{BridePrice}, the performance of RoBERTa+MLP increases from $19.17$ to $32.50$. These results indicate that contextual information is beneficial for parody detection. This finding aligns with the results in \citet{dialogue_bamman, dialogue_wang}, which show that providing dialogue as context significantly improves model performance in sarcasm detection.

However, although contextual information significantly improves model performance on most datasets, there are still some datasets where context does not enhance or even decreases model performance. For example, on \textit{Tiktok-Trump}, the model performance decreases, and on \textit{CampusLife}, the performance remains similar after adding contextual information. This suggests that contextual information may not always contribute to improving model performance in parody detection.

\subsection{Influence of Parody to Sentiment Classification}\label{apd:impact_parody}
\input{images/ORP_sentiment}

Figure \ref{fig:apd_impact_parody} presents the detailed model performance of comment sentiment classification on parody and non-parody comments across seven datasets. In the \textit{DrinkWater} dataset, large language models (LLMs) such as ChatGPT-4o-mini (F1-score: 51.42) and Qwen2.5 (F1-score: 47.00) achieve competitive performance compared to embedding-based methods like Bag of Words (BoW) (F1-score: 48.21), Skip-gram (F1-score: 47.11), and RoBERTa (F1-score: 44.93) when parody is not present. However, for parody comments, the performance of LLMs degrades significantly, falling below that of embedding-based approaches. For instance, ChatGPT-4o drops from an F1-score of 48.7 to 19.04, and ChatGPT-4o-mini declines from 51.42 to 15.53, whereas embedding-based methods exhibit greater robustness, with BoW decreasing from 48.21 to 36.21, Skip-gram from 47.11 to 32.35, and RoBERTa from 44.93 to 33.83. Overall, these results indicate that parody presents substantial challenges for sentiment classification, and LLMs struggle to maintain their advantage over traditional embedding-based methods in this context.

\subsection{Reasoning LLMs in Parody Detection}\label{apd:reasoning_llms}
\input{images/LLMCompare}

We present the details of reasoning LLMs in parody detection across six datasets in Figure \ref{fig:apd_reasoning_llms}. Our findings indicate that reasoning LLMs do not exhibit a performance advantage compared to non-reasoning LLMs. For instance, ChatGPT-o1-mini and ChatGPT-o3-mini underperform relative to ChatGPT4o-mini on the \textit{CampusLife} and \textit{Tiktok-Trump} datasets. Additionally, DeepSeek-R1 significantly underperforms compared to DeepSeek-V3 across all datasets. 

These results suggest that reasoning does not enhance LLM performance in parody detection. We speculate that this may be due to the nature of parody, which often relies on indirect or subtle cues related to tone, context, and nuance rather than direct logical inference. In such cases, non-reasoning LLMs, which excel at identifying statistical patterns and linguistic structures, may be more effective at detecting parody than reasoning LLMs that focus excessively on logical steps or detailed analysis.

\subsection{Impact of Supervision Ratio}\label{apd:train_ratio}

\input{images/fanchuan_image}
\input{images/sentiment_image}
\input{images/user_sentiment_image}

The embedding-based methods used in our experiments require explicit training on labeled data, whereas LLMs like RoBERTa do not require such training once pre-trained. Therefore, the performance of embedding-based models depends on the size and quality of the training set. To explore this, we investigate how varying the training ratio influences model performance by gradually increasing the training set size while keeping the test set constant. The results for RoBERTa+MLP under different train ratio are presented in Figures \ref{fig:train_ratio_parody_detection}, \ref{fig:train_ratio_comment_senti}, and \ref{fig:train_ratio_user_senti} for parody detection, comment sentiment classification, and user sentiment classification. In all tasks, we observe that the performance increases monotonically with the training ratio, highlighting the benefit of additional training data for embedding-based methods.

In addition, on the \textit{BridePrice} dataset, only $10\%$ supervision is enough for RoBERTa to outperform all LLMs in parody detection, indicating a limitation of LLMs in domain-specific tasks. This suggests that fine-tuned models like RoBERTa perform better with minimal supervision in specialized contexts. In contrast, on the \textit{CampusLife} dataset, RoBERTa's performance consistently falls below that of all LLMs, regardless of the training ratio. This suggests that LLMs are more effective in tasks requiring generalizable knowledge and flexibility, such as parody detection in diverse, context-rich domains. These results demonstrate that LLMs remain powerful in specific areas requiring flexibility in adapting to diverse linguistic contexts and nuanced understanding, while embedding-based models like RoBERTa excel in more targeted, domain-specific tasks.
