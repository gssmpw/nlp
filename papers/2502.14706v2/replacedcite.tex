\section{Related work}
\paragraph{Self-play for agents in games}
Self-play RL ____ has been a core ingredient in creating effective agents across a wide range of complex games. Notable examples include superhuman gameplay in two-player zero-sum games like Chess and Go ____, expert human-level play in Stratego ____ and Starcraft ____, as well many-player games that require some level of cooperation like Diplomacy ____ and Gran Turismo____. These successes have demonstrated the effectiveness of self-play, particularly in the large-data, large-compute regime. However, the majority of its successes are in variants of zero-sum games whereas driving tasks are likely general-sum and feature many-agent interaction.

\paragraph{RL for driving agents}
Reinforcement learning has been explored for the design of autonomous driving agents, though state-of-the-art agents are currently far below the human rate of between $800000$ km per police-reported traffic crash in the United States____ or as much as $1$ crash per $24800$ km in more challenging domains such as San Francisco____. These agents are frequently trained in simulators built atop large open-source driving datasets____ such as Waymo Open Motion ____, ____, ____ though there are also procedurally generated____ and non-data-driven simulators____. These datasets collectively add up to tens of thousands of hours of available data and are often used to train RL agents in \emph{log-replay} mode, a setting in which only one agent is learning and the remainder are either replaying human trajectories or executed hand-coded policies. The complexity of scaling RL in these settings has led to the creation of batched simulators ____, ____ whose high throughput helps ameliorate issues of sample complexity. Many works have explored ways to use these simulators to learn high-quality reinforcement learning agents through RL including uses of self-play ____. Our work is mostly distinct from these by the scale of training and a significantly lower crash and off-road rate than has previously been observed. 

% \paragraph{Unsupervised Environment Design}

% What is different about our approach: We use self-play PPO agents to as an ingredient to filter possible scenarios and focus on \textit{finding interesting edge cases}, rather then directly training an agent on them.