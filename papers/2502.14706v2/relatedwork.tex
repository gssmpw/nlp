\section{Related work}
\paragraph{Self-play for agents in games}
Self-play RL \cite{5392560, tesauro1995temporal} has been a core ingredient in creating effective agents across a wide range of complex games. Notable examples include superhuman gameplay in two-player zero-sum games like Chess and Go \cite{silver2018general}, expert human-level play in Stratego \cite{perolat2022mastering} and Starcraft \cite{starcraft}, as well many-player games that require some level of cooperation like Diplomacy \cite{DBLP:conf/iclr/Bakhtin0LGJFMB23} and Gran Turismo~\cite{gtsophy}. These successes have demonstrated the effectiveness of self-play, particularly in the large-data, large-compute regime. However, the majority of its successes are in variants of zero-sum games whereas driving tasks are likely general-sum and feature many-agent interaction.

\paragraph{RL for driving agents}
Reinforcement learning has been explored for the design of autonomous driving agents, though state-of-the-art agents are currently far below the human rate of between $800000$ km per police-reported traffic crash in the United States~\cite{stewart2023overview} or as much as $1$ crash per $24800$ km in more challenging domains such as San Francisco~\cite{flannagan2023establishing}. These agents are frequently trained in simulators built atop large open-source driving datasets~\cite{gulino2024waymax,nocturne,kazemkhani2024gpudrive} such as Waymo Open Motion \citep[WOMD]{ettinger2021large}, \citep[NuScenes]{caesar2020nuscenes}, \citep[ONE-Drive]{onedrive} though there are also procedurally generated~\cite{li2022metadrive} and non-data-driven simulators~\cite{carla17}. These datasets collectively add up to tens of thousands of hours of available data and are often used to train RL agents in \emph{log-replay} mode, a setting in which only one agent is learning and the remainder are either replaying human trajectories or executed hand-coded policies. The complexity of scaling RL in these settings has led to the creation of batched simulators \citep[GPUDrive]{kazemkhani2024gpudrive}, \citep[Waymax]{gulino2024waymax} whose high throughput helps ameliorate issues of sample complexity. Many works have explored ways to use these simulators to learn high-quality reinforcement learning agents through RL including uses of self-play \cite{copo,nocturne,closed_loop_driving,closed_loop_v2,aspDrive}. Our work is mostly distinct from these by the scale of training and a significantly lower crash and off-road rate than has previously been observed. 

% \paragraph{Unsupervised Environment Design}

% What is different about our approach: We use self-play PPO agents to as an ingredient to filter possible scenarios and focus on \textit{finding interesting edge cases}, rather then directly training an agent on them.