\documentclass[dvipsnames]{article}
\usepackage{subcaption} % Include this in the preamble if not already included

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{gensymb}
\usepackage{booktabs} % for professional tables
\newcommand{\defeq}{\vcentcolon=}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{enumitem}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% accepted
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{float}
\usepackage{enumitem}
\usepackage{amsthm}
\usepackage{stix}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% Define a command for green text
\newcommand{\dg}[1]{\textcolor[rgb]{0,0.5,0}{#1}}

% Define a command for dark red text
\newcommand{\dr}[1]{\textcolor[rgb]{0.6,0,0}{#1}}
\newcommand{\tododc}[1]{\textcolor{Red}{[todo(dc): #1]}}
\newcommand{\todoev}[1]{\textcolor{Orange}{[todo(ev): #1]}}
\newcommand{\todokj}[1]{\textcolor{Blue}{[todo(kj): #1]}}
% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
%\icmltitlerunning{Building reliable sim agents}

\begin{document}

\twocolumn[
\icmltitle{Building reliable sim driving agents by scaling self-play
}
% possible titles
%\icmltitle{A different point of view: self-play PPO ... at scale}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
%\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Daphne Cornelisse}{yyy}
\icmlauthor{Aarav Pandya}{yyy}
\icmlauthor{Kevin Joseph}{yyy}
\icmlauthor{Joseph Suárez}{puffer}
\icmlauthor{Eugene Vinitsky}{yyy}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{NYU Tandon School of Engineering}
\icmlaffiliation{puffer}{Puffer.ai}

\icmlcorrespondingauthor{Daphne Cornelisse}{cornelisse.daphne@nyu.edu}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Simulation agents are essential for designing and testing systems that interact with humans, such as autonomous vehicles (AVs). These agents serve various purposes, from benchmarking AV performance to stress-testing system limits, but all applications share one key requirement: reliability. To enable systematic experimentation, a simulation agent must behave as intended. It should minimize actions that may lead to undesired outcomes, such as collisions, which can distort the signal-to-noise ratio in analyses. As a foundation for reliable sim agents, we propose scaling self-play to thousands of scenarios on the Waymo Open Motion Dataset under semi-realistic limits on human perception and control. Training from scratch on a single GPU, our agents nearly solve the full training set within a day. They generalize effectively to unseen test scenes, achieving a 99.8\% goal completion rate with less than 0.8\% combined collision and off-road incidents across 10,000 held-out scenarios. Beyond in-distribution generalization, our agents show partial robustness to out-of-distribution scenes and can be fine-tuned in minutes to reach near-perfect performance in those cases. We open-source the pre-trained agents and integrate them with a batched multi-agent simulator. Demonstrations of agent behaviors can be found at \url{https://sites.google.com/view/reliable-sim-agents}. 
\end{abstract}

\label{submission}

\section{Introduction}

Simulation agents are a core part of safely developing and testing systems that interact with humans, such as autonomous vehicles (AVs). In the context of self-driving, these agents, also referred to as road user behavior models, serve two primary purposes: establishing benchmarks for AV behavior \cite{engstrom2024modeling}, and representing other road users in simulators to enable statistical safety testing in both nominal and rare, long-tail scenarios \cite{corso2021survey, montali2024waymo}. While each use case brings particular requirements, \textit{reliability} is an important one that they share.

A reliable simulation agent consistently behaves as intended by the designer, minimizing unintended actions. For instance, agents designed to stress-test AVs should reliably initiate realistic near-collision events, generating safety-critical scenarios to provide meaningful information about the system's behavior in edge cases. Conversely, nominal agents should focus on replicating typical road behavior to simplify experiments that vary other environmental factors, such as weather. In either case, unreliable sim agents introduce \textit{noise} into the evaluation process by producing trajectories that crash too infrequently in the stress-test case and too frequently in the nominal case.

How can we build sim agents that are \textit{close enough}\footnote{Here, close enough is emphasized because what constitutes an acceptable model of human behavior depends highly on the use case.} to reality while maximizing designer specifications i.e. reliability? One approach relies on generative models, which have shown remarkable progress in producing diverse, human-like behaviors through imitation learning from demonstrations \cite{xu2023bits, DBLP:conf/iclr/PhilionPF24, huang2024versatile}. However, whether they meet the reliability standards of a fully automated AV development pipeline is uncertain. This is highlighted by the top-performing models in the Waymo Open Sim Agent Challenge \citep[WOSAC]{montali2024waymo}, a well-known benchmark for realistic nominal road user behavior. While state-of-the-art models in the 2024 challenge closely replicate logged human trajectories and achieve high scores on various distributional metrics, they still fall short in critical areas. Ground-truth human trajectories in the dataset rarely or never involve collisions or off-road movements, yet the top submissions (1st and 2nd place) frequently display such unintended behaviors. Specifically, simulated agents collide with others in 5–6\% of scenarios and go off-road in 6–12\% of cases \citep[BehaviorGPT, VBD]{zhou2024behaviorgpt, huang2024versatile}. 


\begin{figure*}[!htbp]  
    \centering
    \includegraphics[width=1\textwidth]{Figures/hero_figure_v3.pdf}  
    \caption{\textbf{Overview of approach.} \textit{Left}: We define several criteria to guide the learning of simulation agents through rewards. The reward function is a weighted combination of these criteria: $r(o^i_t )= \sum_i c_i \cdot \mathbb{I}[\text{criteria}_i]$. Here, we focus on achieving goal-directed nominal sim agent behavior—ensuring agents stay on the road and avoid collisions while navigating to a target position. \textit{Right}: Over 24 hours on a single GPU, we iterate through 10,000 scenarios (green curve) from the Waymo Open Motion Dataset in GPUDrive \cite{kazemkhani2024gpudrive}, reaching near-perfect performance (blue curve, reliability) on the defined criteria after 2 billion agent steps by self-play PPO. The example scenarios illustrate agent behavior at different stages of training. Initially, agents display random behavior and frequently collide with each other and the road edges (marked in orange and red), but their behavior becomes streamlined over many iterations.}
    \label{fig:hero_figure}
\end{figure*}

% \begin{figure*}[htbp]
%     \centering
%     % Subfigure 1
%     \begin{subfigure}[t]{0.32\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Figures/viz_rollouts_1.pdf}
%         \caption{\textbf{Example 1.}}
%         \label{fig:example_1}
%     \end{subfigure}
%     \hfill
%     % Subfigure 2
%     \begin{subfigure}[t]{0.32\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Figures/viz_rollouts_2.pdf}
%         \caption{\textbf{Example 2.}}
%         \label{fig:example_2}
%     \end{subfigure}
%     \hfill
%     % Subfigure 3
%     \begin{subfigure}[t]{0.32\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Figures/viz_rollouts_3.pdf}
%         \caption{\textbf{Example 3.}}
%         \label{fig:example_3}
%     \end{subfigure}

%     \caption{\textbf{Example rollouts}. Rollouts with the learned policy on sampled scenarios from the WOMD test set.}
%     \label{fig:scaling_laws_combined}
% \end{figure*}

This limits the scalability of AV evaluation and development, especially as generative models are increasingly used to create rare safety-critical scenarios underrepresented in real-world data \cite{mahjourian2024unigen}. When trajectories deviate unpredictably, researchers or engineers must find out: is the observed outcome a signal or an artifact of simulator noise? For instance, if 1 in 10 scenarios reflects unintended behavior, distinguishing meaningful failures from artifacts becomes a time-consuming task. As such, making sim agents more reliable seems a key pillar to further scale AV evaluation and development.

The question becomes: how can we close this reliability gap in state-of-the-art sim agents? Assuming we can precisely define what the agent should adhere to (e.g. stay on the road), there is reason to believe that self-play reinforcement learning (RL) could be a piece of the puzzle. Evidence from a broad body of recent literature on games shows that self-play RL, combined with well-defined criteria (e.g. maximize score X) can produce agents capable of perfect, superhuman, gameplay in the large compute and data regime \cite{silver2018general, DBLP:conf/iclr/Bakhtin0LGJFMB23, openai5}.

We systematically study whether self-play at scale improves the reliability of sim agents. Specifically, we ask:
\vspace{-3mm}  
\begin{enumerate}[noitemsep]
    \item How does the \textit{reliability} (as measured by performance on the test set of metric X) of sim agents through self-play scale as a function of the data available?
    \item How well do these agents generalize to unseen scenarios and out-of-distribution events?
\end{enumerate}
To investigate these questions, we train agents via self-play using a semi-realistic human perception framework in a data-driven simulator \cite{kazemkhani2024gpudrive}. We evaluate performance across thousands of scenarios from the Waymo Open Motion Dataset \cite{ettinger2021large}. Our key finding is that self-play PPO scales effectively with on-policy data and compute. After sufficient training, models generalize well to 10,000 unseen test traffic scenarios, virtually closing the train-test gap.

At scale, self-play PPO sim agents consistently achieve the specified criteria (Section \ref{sec:task_definition}): staying on the road, avoiding collisions, and reaching a target position. This establishes a flexible framework where agents can be tuned to achieve specific collision rates, enabling both nominal and safety-critical traffic simulation. By improving the reliability standards of sim agents, our approach supports the continued scaling and automation of AV development and evaluation pipelines.

Finally, we take a first step toward fine-tuning these agents for behaviors underrepresented in the dataset, a useful capability for safety-critical applications. 

To facilitate further research, we open-source the pre-trained agents at \url{www.github.com/Emerge-Lab/gpudrive}, allowing others to reproduce our results and seamlessly use these sim agents in GPUDrive.

% We ask the following questions
% \begin{enumerate}
%     \item How does the reliability (as measured by X) of sim agents through self-play scale as a function of the data available?
%     \item How robust are these agents to particular perturbations in the scenarios?
%     \item When we combine self-play PPO with human-like constraints on perception and control, what kind of agent behaviors emerge? [maybe not this paper? we don't have baselines]
% \end{enumerate}


% We propose scaling self-play with a semi-realistic framework for human perception to thousands of scenarios \cite{kazemkhani2024gpudrive} on the Waymo Open Motion Dataset \cite{ettinger2021large} as a possible pillar for reliable sim agent design.

% Our key insight is that self-play PPO scales effectively with on-policy data and compute. After a sufficient number of training steps, models trained via self-play PPO in a data-driven simulator demonstrate effective generalization to the unseen test traffic scenarios, effectively closing the train-test gap.


% \tododc{rewrite intro, this intro is too focused on edge cases} Edge cases are key to understanding system performance in safety-critical domains, such as autonomous driving. These scenarios, which test a system's limits, are rare and become even more scarce as system performance improves \cite{liu2024curse}. Consequently, capturing meaningful signals from real-world events is challenging. Simulators, especially when paired with generative models trained on human data, offer a scalable way to evaluate system performance in challenging safety-critical scenarios \cite{huang2024versatile, mahjourian2024unigen}.

% While generative models excel at producing diverse scenarios, they introduce a critical challenge: it is often unclear if these generated scenarios are \textit{solvable} under realistic conditions. For instance, if an autonomous vehicle (AV) crashes in a generated scenario, it may stem from an unsolvable setup, such as overlapping trajectories from other agents or missing information, rather than a system flaw. This uncertainty can lead to wasted time and resources, as researchers are tasked to distinguish between meaningful failures and artifacts of the simulator or generative model. For example, 6\% of scenarios generated from current state-of-the-art generative models, such as VBD and UniGen [cite], still contain unintended agent collisions. This limits the scalability and reliability of AV evaluation and development pipelines. If we could trust that scenarios are solvable, that would make the evaluation process more efficient.

% In this paper, we draw inspiration from the PAIRED method \cite{DBLP:conf/nips/0001JVBRCL20} to take a step towards automated generation of evaluation scenarios that are both challenging and solvable. We propose a framework for building agents that can be used to filter real and generated scenarios and narrow down possible failure modes for systems deployed alongside humans. Our key idea is to combine self-play PPO with human-like constraints on perception and control to create agents that can serve as a benchmark for solvability. These agents, while not replicating human behavior, use restricted fields of view and realistic control outputs to:
% \begin{itemize}[noitemsep]
%     \item Filter solvable scenarios: This becomes increasingly important as reliance on generated scenarios grows.
%     \item Analyze system safety: Post-crash analysis can reveal whether failures arise from limited information or system deficiencies.
% \end{itemize}
% Figure \ref{fig:example_use_case} illustrates an example use case, showcasing how our agents can distinguish between solvable and unsolvable scenarios.

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=\textwidth]{Figures/example_use_case_self_play_RL_1015.pdf}
%     \caption{Example use case for self-play PPO agents with human-like observations and control for Autonomous Driving Systems.}
%     \label{fig:example_use_case}
% \end{figure*}


% has emerged as a paradigm for continually improving agent performance in an automated way. UED leverages parameterized environments to automatically generate a distribution of valid, solvable scenarios that become more challenging over time. By focusing on the feasibility of these scenarios, methods such as PAIRED ensure that the generated games are both increasingly challenging and solvable. 

% \paragraph{Contributions} \tododc{after results}
% \begin{itemize}
%     \item State-of-the-art performance under partial observability: We achieved near-perfect performance on 100k Waymo Open Motion Dataset (WOMD) scenarios using self-play PPO from scratch. (todo: quantify after results are in)
%     \item Minimal and realistic agent assumptions: Our framework is grounded in assumptions that mirror real-world (human) road conditions and can be easily configured. This ensures the agent’s functionality aligns with realistic operational settings and they can be used to test if scenarios are solvable, a step in the direction of automated AV evaluation and development.
%     \item Reliable foundation for sim agent finetuning: Our framework offers a reliable starting point for blending human-like behavior realism.
% \end{itemize}

\section{Method}

% \subsection{Desiderata} A benchmark for evaluating the effectiveness of self-play reinforcement learning (RL) in constructing reactive simulation agents for safety-critical tasks involving humans should meet two key criteria: (1) a strong multi-agent component to capture the interactivity and coordination challenges, and (2) a ground truth to assess how well the resulting agents approximate the behavior of real human road users.

\subsection{Dataset and simulator}
We conduct our experiments in GPUDrive, a data-driven, multi-agent, GPU-accelerated simulator \cite{kazemkhani2024gpudrive}. GPUDrive contains $K =$ 160,147 real-world traffic scenarios from the Waymo Open Motion Dataset \citep[WOMD]{ettinger2021large}. Each scenario $k \in K$ comprises a static road graph, $R_k$, and a time series of \textit{joint} logged human trajectories:
\begin{align}
    \mathcal{S}_k = \{(\mathbf{s}_t, \mathbf{A}_t)_{t=0}^{T=90}, R_k \}
\end{align}
where $\mathbf{s}_t \in \mathbb{R}^{(1, F)}$ represents the world state represented as $F$ features at time $t$, and $\mathbf{A}_t \in \mathbb{R}^{(N, 2)}$ represents the action matrix for all $N$ agents in the scene. The joint agent demonstrations are 9 seconds long and discretized at 10Hz. 

\begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth]{Figures/sim_state_and_obs.pdf}
    \caption{\textbf{Sample scenario state with corresponding agent observation}. \textit{Left}: Example scenario from the Waymo Open Motion Dataset rendered in GPUDrive as shown from a bird's eye view. The boxes ($\textcolor{RoyalBlue}{\hrectangle}$) indicate controlled agents and the circles ($\textcolor{RoyalBlue}{\odot}$) indicate the goal positions for every controlled agent. \textit{Right}: Scene view from the agent in the center ($\textcolor{Periwinkle}{\hrectangle}$). Agents see a subset of the road points within a configurable radius (here $r_o = 50$ meters) and their corresponding types and segment length. Road types are road edges ($\textcolor{Black}{\bullet}$) and road lanes ($\textcolor{Gray}{\bullet}$) They can also view the relative position and velocity of the other agents in the scene ($\textcolor{YellowOrange}{\hrectangle}$). Agents in gray are static throughout the episode as they are parked cars but this information is not visible to the agent i.e. the agent does not know that the gray cars are guaranteed not to move and consequently all cars are orange in the agent observation view.}
    \label{fig:state_and_obs_space}
\end{figure}

% issues with current motivation
% 1) Our self-play agents still collide...
% Experiment: Design scenario where collision could not have been avoided, show that self-play agent collides and can be used 


\subsection{Task definition and measuring performance}
\label{sec:task_definition}

\subsubsection{Task definition}
We aim to systematically study how the reliability of simulation agents trained via self-play scales with data. To do this, we design a task with well-defined metrics such that experimental results are easy to interpret. Given a traffic scenario \(\mathcal{S}_k\) with \(N\) controlled agents we task every agent to navigate to a designated goal position while satisfying two criteria: (1) avoiding collisions with other agents and (2) staying on the road.  

To obtain valid goals, we use the endpoints \((x^i_T, y^i_T)\) (marked by \(\textcolor{RoyalBlue}{\odot}\) in Figure \ref{fig:state_and_obs_space}) from the WOMD. Agents are initialized from the starting positions \((x^i_0, y^i_0)\) of the WOMD. Given how the WOMD dataset is collected and processed, we know that the human road users in the dataset must have successfully reached their endpoints within 9 seconds (or 91 steps). As such, we assume that, in principle, all agents should be capable of doing the same. To reflect this, a scenario is considered solved when \textit{all controlled agents} reach their target positions within 91 steps while adhering to the specified criteria. 

\subsubsection{Metrics}
\label{sec:metrics}

We use four \textit{scene-based metrics} to quantify performance:
\vspace{-4mm}
\begin{itemize}[noitemsep]
    \item \underline{Goal achieved} \textcolor{RoyalBlue}{↑}: Percentage of agents that reached their target position within $T=91$ steps.
    \item \underline{Collided} \textcolor{BrickRed}{↓}: Percentage per scenario indicating objects that collided, at any point in time, with any other object, i.e. when the agent bounding boxes touch.
    \item \underline{Off-road}  \textcolor{RedOrange}{↓}: Percentage of agents per scenario that went off the road or touched a road edge, at any point in time.
    \item \underline{Other} \textcolor{lightgray}{↓}: Percentage of agents per scenario that did not collide or go off-road but also did not reach the goal position.
\end{itemize}
The \underline{Collided} and \underline{Off-road} metrics align with the Waymo Open Sim Agent Challenge and Waymax \citep{montali2024waymo, gulino2024waymax}. Specifically, \underline{Collided} is part of the ``object interaction metrics" category and the off-road events are part of the ``map-based metrics'' category. Under the assumption that human road users have near zero collision and off-road events, we can meaningfully compare our scores to the top submissions \cite{huang2024versatile, zhou2024behaviorgpt} \footnote{Technically, WOSAC frames this as a distribution-matching problem: metrics are first computed as event counts, which are then compared to the distribution of log replay trajectories across several rollouts.}. 

The \underline{Goal achieved} metric is not directly reported in WOSAC, making it less comparable. The most similar metric is the Route Progress Ratio used in Waymax \cite{gulino2024waymax}, which measures how far an agent travels along the logged trajectory. However, since our focus is not on mimicking logged trajectories but on precisely reaching a particular goal, a binary metric is, in our case, a more meaningful indicator of performance. However, reaching the goal roughly corresponds to a Route Progress Ratio of $100\%$.

\textit{Agent-based metrics}: Since the scene-based metrics are biased towards scenes with a small number of agents (one agent colliding in a scene with 2 agents vs. 10 scenes provides a fraction of 1/2 vs 1/10th), we also report the metrics above in \textit{agent-based} way, where we aggregate the counts across the whole dataset and then divide them by the number of total agents. 

In both cases, the ceiling for this task is 100\% \underline{Goal achieved}, 0\% \underline{Collided}, and 0\% \underline{Off-road}. 

\subsection{State and observation space}

This section outlines the design choices and parameterization of the observation \( \mathbf{o}_{t}^i \) for agent \( i \) at time \( t \). We make these choices to reflect semi-realistic limits on human perception. The observation encodes the agent’s partial view of the scenario state \( s_t \), capturing the information necessary for decision-making. In this work, we model the RL problem as a Partially Observed Stochastic Game \citep[][POSG]{posg04}, where agents make simultaneous decisions under partial observability. We further make the following design choices for our agents:  

\paragraph{Relative coordinate frame} All agent information is presented in an ego-centric coordinate frame to align with human-like perception.

\paragraph{Observation radius} The observation radius \( r_o \) determines the visible area around the agent. For our experiments, we set \( r_o = 50 \) meters, as illustrated in Figure \ref{fig:state_and_obs_space}.

\paragraph{No history} Agents only receive information from the current timestep.

\paragraph{Road graph} We reduce the full road graph, which consists of up to 10,000 sparsely distributed road points, in dimension for computational efficiency. To reduce the number of points corresponding to straight lines, we run the polyline reduction threshold of the polyline decimation algorithm \cite{visvalingam2017line} in GPUDrive to 0.1 which roughly cuts the number of points by a factor of 10. We also cap the maximum visible road points at 200, selecting 200 points from those in the view radius in a random order if there are more than $200$ points, creating a sparse view of the local road graph. Empirical results show this is sufficient for agents to navigate toward goals without going off the road or causing collisions.

\paragraph{Normalization} Features are normalized to be between -1 and 1 by the minimum and maximum value in their respective category. Details are found in Tables \ref{tab:ego_feature_dimensions}, \ref{tab:rg_feature_dimensions}, and \ref{tab:partner_feature_dimensions}.

 A complete overview of the observation features is provided in Appendix \ref{appendix:feature_details}.

\subsection{Action space and dynamics model}

To align with the control outputs of real human road users more closely, we take the action for every agent $i$ to be a vector of the following discrete random variables:
\begin{align}
    \mathbf{a}^i_t = (\tilde{a}, \tilde{s})
\end{align}
where acceleration actions are 7 actions defined over an evenly spaced grid between $[-4, 4 ]$ and the steering wheel angle are 13 actions defined over an evenly spaced grid between $[- \pi, \pi]$. The bounds are set to reflect the kinematic constraints of real driving. We assume that the random variables $\tilde{a}, \tilde{s}$ are not independent (e.g. sharp turns are less likely at high acceleration) and model the conditional joint probability mass function (pmf) of the two discrete random variables, where we condition on the current observation of agent $i$ at time step $t$:
\begin{align}
    \pi_{\tilde{a}, \tilde{s}}(a, s \mid \mathbf{o}_t^i) \defeq P(\tilde{a} = a, \tilde{s} = s\mid \mathbf{o}_t^i)
\end{align}
the conditional pmf $\pi_{\theta}$ describes the behavior under the assumption that $\mathbf{o}_t^i$ takes a fixed set of values. The total joint action space contains $7 \times 13 = 91$ actions. With these actions, agents are stepped in the simulator using an Ackermann bicycle model \cite{rajamani2011vehicle}.

\subsection{Reward function}

We define the individual agent rewards as follows:  
\begin{align}  
r(\mathbf{o}^i_t, \mathbf{a}^i_t) &= w_{\text{Goal achieved}} \cdot \mathbb{I}[\text{Goal achieved}] \\
&- w_{\text{Collided}} \cdot \mathbb{I}[\text{Collided}] \\
&- w_{\text{Offroad}} \cdot \mathbb{I}[\text{Offroad}]    
\end{align}  
Here, \( \mathbb{I}[.] \) is an indicator function that equals 1 if the condition is true and 0 otherwise. We assign weights \( w_{\text{Goal Achieved}} = 1.0 \), \( w_{\text{Collided}} = 0.75 \), and \( w_{\text{Offroad}} = 0.75 \). An agent achieves the goal position when it reaches within a 2-meter radius of the target \((x, y)\). Once an agent reaches its goal, we remove it from the scene. This latter choice is made as it is ill-defined what an agent should do after it reaches its goal. 

% Under our discount factor of $.99$, an agent will only be willing to accept a collision or off-road event if it would allow them to achieve their goal over $70$ steps faster. Note that such scenarios do occur in the dataset infrequently and this particular combination of collision weights and discount factor may occasionally make colliding an optimal behavior. An agent achieves the goal position when it reaches within a 2-meter radius of the target \((x, y)\). Once an agent reaches its goal, we remove it from the scene. This latter choice is made as it is ill-defined what an agent should do after it reaches its goal. 

\subsection{Collision behavior}

During training and testing, we allow agents to continue the episode even after going off-road or colliding with another agent in the scene. Agents receive a penalty for each collision or off-road event, allowing them to accrue multiple penalties throughout an episode. A detailed discussion on can be found in Appendix \ref{appendix:collision_behavior}.

\subsection{Models}

We use a neural network with an encoder and a shared embedding, as illustrated in Figure \ref{fig:network_architecture}. The flat observation vector is first decomposed into three modalities: the dense ego state, the sparse road graph, and the sparse partner observations. Each modality is processed independently. Inspired by the late fusion approach in Wayformer \citep{DBLP:conf/icra/NayakantiAZGRS23}, we then concatenate the outputs, apply max pooling, and pass the result through a shared embedding. This hidden embedding is fed into separate actor and critic heads, each implemented as a single feedforward layer. The model only has $\approx 50,000$ trainable parameters.

\begin{figure}[htb]  
    \centering
    \includegraphics[width=\linewidth]{Figures/network_architecture.pdf} 
    \caption{\textbf{Network architecture}. The relative observation vector $o_t^i$ is first decomposed into its separate modalities: the ego state (i.e. the agent's information about itself and its goals), the visible portion of the road graph, and the speeds, yaws, and relative positions of the other agents in the scene. These modalities are first processed separately. Their outputs are combined and max pooled, then processed together. The hidden layer is finally fed into an actor and a critic head.}
    \label{fig:network_architecture}
\end{figure}

\subsection{Training}

\paragraph{Self-play PPO} In each scenario, we control up to $N=64$ agents using a shared, decentralized policy $\pi_{\theta}$. Actions are independently sampled from the policy based on the ego views of each agent $i$ during every step in the rollout: $\mathbf{a}_t^i \sim \pi_{\theta}(\cdot \mid \mathbf{o}_t^i)$. We train agents using Proximal Policy Optimization \citep[][PPO]{ppoSchulman17} using batches of $S = 800$ distinct scenarios, with the set of training scenarios uniformly resampled every 2 million steps. Initially, agents exhibit random behavior and crash frequently. Over time, the agents' behavior becomes more streamlined, creating smooth trajectories with high rates of reaching the goals. 

% Figure \ref{fig:self-play-ppo} and videos on the project page illustrate this process.

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=\linewidth]{Figures/self-play-hero.pdf}
%     \caption{\textbf{Self-play leads to effective goal-conditioned agents in the million/billion sample regime}. \textit{Left}: Example scenario at initialization. All agents are controlled by a shared policy $\pi$. The color of the boxes indicates the state the agent is in: $\textcolor{Red}{\hrectangle}$ indicates agents that are in collision with other agents, $\textcolor{Orange}{\hrectangle}$ are in collision with a road edge, $\textcolor{RoyalBlue}{\hrectangle}$ agents are not in collision. Initially, all agents are controlled by the same, random policy.  \textit{Right}: Through trial and error, the policy converges to a conditional distribution that leads to well-behaved, goal-reaching agents.}
%     \label{fig:self-play-ppo}
% \end{figure}
\section{Related work}

\paragraph{Self-play for agents in games}
Self-play RL \cite{5392560, tesauro1995temporal} has been a core ingredient in creating effective agents across a wide range of complex games. Notable examples include superhuman gameplay in two-player zero-sum games like Chess and Go \cite{silver2018general}, expert human-level play in Stratego \cite{perolat2022mastering} and Starcraft \cite{starcraft}, as well many-player games that require some level of cooperation like Diplomacy \cite{DBLP:conf/iclr/Bakhtin0LGJFMB23} and Gran Turismo~\cite{gtsophy}. These successes have demonstrated the effectiveness of self-play, particularly in the large-data, large-compute regime. However, the majority of its successes are in variants of zero-sum games whereas driving tasks are likely general-sum and feature many-agent interaction.

\paragraph{RL for driving agents}
Reinforcement learning has been explored for the design of autonomous driving agents, though state-of-the-art agents are currently far below the human rate of between $800000$ km per police-reported traffic crash in the United States~\cite{stewart2023overview} or as much as $1$ crash per $24800$ km in more challenging domains such as San Francisco~\cite{flannagan2023establishing}. These agents are frequently trained in simulators built atop large open-source driving datasets~\cite{gulino2024waymax,nocturne,kazemkhani2024gpudrive} such as Waymo Open Motion \citep[WOMD]{ettinger2021large}, \citep[NuScenes]{caesar2020nuscenes}, \citep[ONE-Drive]{onedrive} though there are also procedurally generated~\cite{li2022metadrive} and non-data-driven simulators~\cite{carla17}. These datasets collectively add up to tens of thousands of hours of available data and are often used to train RL agents in \emph{log-replay} mode, a setting in which only one agent is learning and the remainder are either replaying human trajectories or executed hand-coded policies. The complexity of scaling RL in these settings has led to the creation of batched simulators \citep[GPUDrive]{kazemkhani2024gpudrive}, \citep[Waymax]{gulino2024waymax} whose high throughput helps ameliorate issues of sample complexity. Many works have explored ways to use these simulators to learn high-quality reinforcement learning agents through RL including uses of self-play \cite{copo,nocturne,closed_loop_driving,closed_loop_v2,aspDrive}. Our work is mostly distinct from these by the scale of training and a significantly lower crash and off-road rate than has previously been observed. 

% \paragraph{Unsupervised Environment Design}

% What is different about our approach: We use self-play PPO agents to as an ingredient to filter possible scenarios and focus on \textit{finding interesting edge cases}, rather then directly training an agent on them.

\section{Results}

\begin{table*}[htbp]
\centering
\begingroup
\setlength{\arrayrulewidth}{1pt} % Adjust table rule thickness only for this table
\renewcommand{\toprule}{\specialrule{0.2pt}{0pt}{0pt}} % Thicker top rule
\renewcommand{\midrule}{\specialrule{0.2pt}{0pt}{0pt}} % Medium mid rule
\renewcommand{\bottomrule}{\specialrule{0.2pt}{0pt}{0pt}} % Thicker bottom rule
\fontsize{2}{3.5}\selectfont % Apply fixed font size only within this group
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lcccc@{}} 
\toprule
\textbf{Dataset} & \textbf{Goal achieved} \textcolor{RoyalBlue}{↑} & \textbf{Collided} \textcolor{BrickRed}{↓} & \textbf{Off-road} \textcolor{RedOrange}{↓} & \textbf{Other} \textcolor{lightgray}{↓} \\
\midrule
Train  & 99.84 ± 1.27 & 0.38 ± 2.91 & 0.26 ± 2.17 &  0.13 ± 1.14 \\ 
Test   &  99.81 ± 1.53 & 0.44 ± 3.17 & 0.31 ± 2.59 & 0.14 ± 1.16 \\ 
\bottomrule
\end{tabular}%
}
\endgroup
\caption{Aggregate scene-based performance in \% across $N=10,000$  randomly sampled train and test traffic scenarios from the Waymo Open Motion Dataset (mean $\pm$ std). Metrics are defined in section \ref{sec:metrics}.}
\label{tab:aggregate_perf_best_policy}
\end{table*}

\subsection{Scaling with data}
\label{sec:scaling_with_data}

\begin{figure*}[!htbp]  
    \centering
    \includegraphics[width=\textwidth]{Figures/scaling_laws_point_02_19_17_35_simple.pdf}  
    \caption{\textbf{Scaling with data}. Average performance with standard errors on 10,000 unseen scenarios from the WOMD validation set as a function of the training dataset size. The striped lines indicate optimal performance.}
    \label{fig:scaling_laws}
\end{figure*}

\paragraph{Solving the full Waymo Open Motion Dataset under partial observability}
We investigate whether agents with a partial view of the environment can solve all scenarios in the Waymo Open Motion Dataset. Our results show that nearly all scenarios can be solved successfully. After $1$ billion training steps, agents achieve a goal-reaching rate of $99.84$\%, a collision rate of $0.38$\%, and an off-road rate of $0.26$\% on the training dataset. Furthermore, as depicted in Figure \ref{fig:train_performance_log}, zooming in on the final four hours of training suggests that metrics exhibit a continued, albeit gradual, improvement, indicating that performance can be further increased with additional training. This training run took $24$ hours on a single NVIDIA A100 GPU.\footnote{These metrics are computed in alignment with the way they are defined in WOSAC, but it should be noted that this is an over-optimistic metric as it includes many agents that simply need to remain in place as they are initialized right next to their goals. This initialization mode can be reproduced in the simulator by setting \texttt{init\_mode = all\_objects}. Excluding such agents, the performance metrics are: $99.40$\% goal-reaching rate, $0.5$\% collision rate, and $0.6$\% offroad rate. The latter initialization mode, referred to as \texttt{init\_mode = all\_non\_trivial}, only controls agents that must drive more than 2 meters before reaching their goal and is used during training.}

% \begin{table}[htbp]
% \centering
% \small % Adjusts the font size
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{@{}ccc@{}} 
% \toprule 
% Total collisions during training &  Total driving experience & Costs \\
% \midrule
% 10.0  &  &  \\
% \bottomrule
% \end{tabular}%
% }
% \caption{Fun training facts \textcolor{red}{todo}}
% \label{tab:fun_facts}
% \end{table}

The agent-based metrics are similar to the scene-based metrics reported above: a goal rate of $99.72$\%, a collision rate of $0.26$\%, and an off-road rate of $0.35\%$. Sample rollouts with the best-trained policy are shown in Figures \ref{fig:3d_examples}, \ref{fig:2d_examples} and on the project page.

\begin{figure*}[!htbp]
\centering \includegraphics[width=1\textwidth]{Figures/training_curves.pdf}
\caption{\textbf{Batch performance throughout training.} \textit{Left}: Average reward per agent (maximum of 1) as a function of wall-clock time. We train agents for at most 24 hours. \textit{Center}: Goal achievement rate per batch as a function of global steps (2 billion steps generated in 24 hours). \textit{Right}: Percentage of agents that collide with another agent (red) or with a road edge (orange). All curves are smoothed using a rolling window of 250 steps. The inset figures show a zoomed-in view of the final four hours of the run, with the y-axes displayed on a logarithmic scale. The red annotations on the insets indicate the minimum and maximum values within the zoomed-in window. Note that the metrics reported during training are by excluding trivial agents, we only control agents that have to drive for more than 2 meters to reach their goal destination.} 
\label{fig:train_performance_log} 
\end{figure*}

\paragraph{Effective generalization to unseen scenarios with sufficient data}
We conduct experiments with 100, 1,000, 10,000, and 100,000 unique training scenarios to assess how self-play performance scales with the diversity of training scenes. Table \ref{tab:aggregate_perf_best_policy} summarizes the results. We find no significant train-test gap when training with 10,000 scenarios or more, indicating the model generalizes well to new, unseen situations. Figure \ref{fig:scaling_laws} shows the key metrics as a function of training dataset size. Notably, with 10,000 training scenarios, the model reaches nearly the ceiling of our benchmark, achieving a 99.81\% goal-reaching rate, 0.44\% collision rate, and 0.31\% off-road rate on 10,000 held-out test scenarios. 

\subsection{Distribution of errors and remaining failure modes}
\label{sec:failure_modes}
We analyze scenarios that are not perfectly solved, defined as those with a collision rate or off-road rate greater than 0, or where at least one agent fails to reach its goal. A selection of failure modes can be viewed on the \href{https://sites.google.com/view/reliable-sim-agents/home}{project page}. Together, these account for $8.95$\% of the test dataset ($896$ out of $10,000$ scenarios). Figure \ref{fig:error_distributions} shows the histogram of error distributions, revealing that most unsolved scenarios have only a small error rate. We compute the Pearson correlation between off-road fractions and collision rates to examine potential relationships between failure modes. The result, $\rho = 0.0135$, is not significant at $\alpha = 0.05$, indicating no meaningful correlation between these two metrics in the unsolved scenarios and suggesting that errors are spread across scenarios.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.9\linewidth]{Figures/eror_distribution_01_30_21_06.pdf}
    \caption{\textbf{Probability distribution function for each type of error for scenes that are not fully solved.}. \textit{Left}: Percentage of agents that collided. \textit{Middle}: Percentage of agents that went off road. \textit{Right}: Percentage of agents that neither failed nor reached its goal. Note that almost all scenes contain just a single failure.}
    \label{fig:error_distributions}
\end{figure}


Additionally, we analyze the top $0.5$\% failure modes in each category (collision rates, off-road rates, and agents that did not reach the goal position) of the test set. This analysis provides information about challenging aspects of these scenarios. The key takeaways are as follows.


\subsubsection{Rare map layouts and objects}

High off-road rates occur in scenarios with rarely occurring road structures. One example of this is roundabouts. A large fraction ($15$\%) of the top fraction of collision rates was in roundabout scenes. The rest included road layouts that are simply harder to navigate, such as tight corners, narrow lane entries, parking lots, etc. Larger vehicles especially struggle with such maps. This coupled with multiple vehicles crowding leads to some of them going off-road.

\subsubsection{Coordination}

High collision rates occur in intersections, speedy highways, and crowded scenes where sophisticated interaction is required (eg: letting another agent pass before you, making space for another agent to overtake, etc). Crowding and interaction coupled with rare map layouts compound the difficulty of the scene and lead to a higher collision and off-road rate.

\subsubsection{Out of time}

Some agents have goals further away than others. Having a finite horizon of $91$ steps means trying to squeeze past agents and narrow lanes when it is very hard to. This leads to a higher collision and off-road rate compared to scenes with closer goals. This can also compound difficulty in scenes with the aforementioned properties. 


\subsection{Extrapolative generalization and fast fine-tuning}
\label{sec:finetuning}

\begin{figure*}[htbp]  
    \centering
    \includegraphics[width=\textwidth]{Figures/finetuning_agent_experiment.pdf}  
    \caption{\textbf{Fine-tuning agent behaviors} \underline{1:} In most scenarios, agent target positions are located in front of them. The figure shows a typical example from the dataset with rollouts from the trained policy. \underline{2:} Fewer than 2\% of agent goals require backward driving or a U-turn. To evaluate agent performance in such out-of-distribution cases, we create hand-designed scenarios where goals are placed behind agents. As expected, performance drops significantly (by 50\%), as agents struggle to reach these goals. In this scene, no agent achieves its new goal. \underline{3:} To address this, we fine-tune a model pre-trained on 10,000 WOMD scenarios using the 13 hand-designed cases. Within 15 minutes, agents successfully learn to navigate to the goals behind them. \underline{4:} A rollout of the fine-tuned model demonstrates its ability to handle the altered scenario. Each agent executes a U-turn to get to its goal.}
    \label{fig:finetuning_steps}
\end{figure*}


\subsubsection{Navigating backwards} 
Beyond generalization to within distribution scenarios, as reported in Section \ref{sec:scaling_with_data}, we are interested in agent performance in out-of-distribution events. This is useful to know as researchers may typically manipulate scenarios or make them harder in some way to test the limits of AV systems. Where do these agents break, and how easily can they be finetuned? Driving backward, or navigating to goals behind agents is one such behavior that is rarely observed in the data. To quantify this, we analyzed the full training dataset ($\approx 129,000$ scenes) or about $4.2$ million controllable agents. Of these, we found approximately $30,000$ agents ($0.73$\%) making a U-turn, and $47,000$ agents ($1.13$\%) driving in reverse (see Appendix~\ref{appendix:ood} for the exact definition of these events). Further, most agents driving in reverse were simply pulling out of park, with goals immediately behind them, We observed a distinct lack of goals where the agent needs to execute a complex U-turn, making it plausibly out of distribution. We then hand-designed 13 scenarios from the test dataset with a total of 27 agents across all scenes, placing goals behind agents. This was done by setting the new goal for each agent to $(x_f - x_i, y_f - y_i)$, where $(x_i, y_i)$ is the initial position, and $(x_f, y_f)$ is the original goal. We chose the scenes in such a way that doing this process for all controlled agents results in valid and reachable goals. Figure \ref{fig:finetuning_steps}.2 illustrates an example of such a scene.

We summarize the results in Table \ref{tab:goals_behind_agents_perf}. We can see that, whereas the agent performance in the original scenarios is 100\%, the performance drops to 53.5\% goal-reaching rate when we place goals behind the agents. Unsurprisingly, agent exhibit poor performance on events that are extremely rare or entirely unobserved in the training scenarios.

\begin{table}[htbp]
\centering
\small % Adjusts the font size
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lcccc@{}} 
\toprule
\textbf{Class} & \textbf{Goal achieved} \textcolor{RoyalBlue}{\textuparrow} & \textbf{Collided} \textcolor{BrickRed}{\textdownarrow} & \textbf{Off-road} \textcolor{RedOrange}{\textdownarrow} & \textbf{Other} \textcolor{lightgray}{\textdownarrow} \\
\midrule
Altered  & 53.5 $\pm$ 38.4 & 10.0 $\pm$ 8.3 & 6.7 $\pm$ 16.1 & 41.7 $\pm$ 32.0 \\
Original & 100.0 $\pm$ 0.0 & 0.0 $\pm$ 0.0 & 0.0 $\pm$ 0.0 & 0.0 $\pm$ 0.0 \\
\bottomrule
\end{tabular}%
}
\caption{Aggregate performance comparison between Altered and Original goal positions (mean $\pm$ std).}
\label{tab:goals_behind_agents_perf}
\end{table}


% \begin{figure}[htb]
%     \centering
%     \includegraphics[width=\linewidth]{Figures/ood_goals.pdf}
%     \caption{\textbf{Placing target positions behind agents}. An example scenario is where we place the goal position behind agents. This is something we have not seen in the dataset; most agents must drive ahead to reach their target positions.}
%     \label{fig:example_hand_desiged_exp}
% \end{figure}

\subsubsection{Fast finetuning}

As a proof of concept, we demonstrate how self-play reinforcement learning enables rapid fine-tuning of a model to learn new behaviors, such as navigating backward, using only a few samples. Figure \ref{fig:finetuning_steps} provides an overview of our approach. Initially, introducing an out-of-distribution scenario—where goals are positioned behind agents—leads to a drop in performance ($1 \rightarrow 2$). To address this, we take the 13 hand-designed scenarios and fine-tune the policy that was pre-trained on 10,000 WOMD scenarios ($3$). The model starts with a low goal-reaching rate but quickly adapts, achieving 100\% success within 15 minutes of training. After fine-tuning, agents can reliably reach goals behind them ($4$). An accompanying video of before and after finetuning is shared at \underline{\href{https://sites.google.com/view/reliable-sim-agents/home}{the project page}}.



% \subsection{Self-play agent behaviors}

% \tododc{Stretch goal; everything else first}

% \subsubsection{Quantitative analysis}

% - average episode length self-play agents vs. data

% \subsubsection{Qualitative analysis}

% 1) interesting crash behavior
% 2) nifty dodges

% point to a webpage

% limitations: 1) We don't know how "diverse" WOMD actually is

\section{Discussion}

Our results lead us to three main conclusions:
\paragraph{1. Self-play at scale reliably achieves well-defined criteria in unseen scenarios.}  
Our findings suggest that self-play RL scales effectively with available data (Section \ref{sec:scaling_with_data}), achieving state-of-the-art performance on the Waymo Open Motion Dataset (WOMD) with  no generalization gap. To the best of our knowledge, this is the first demonstration of this level of performance on WOMD. Compared to state-of-the-art supervised models, such as VBD \cite{huang2024versatile} and BehaviorGPT \cite{zhou2024behaviorgpt}, our approach reduces collision and off-road rates by at least 15 $\times$.  
\paragraph{2. Rare events remain a challenge.}  
Agents struggle with rare or out-of-distribution scenarios, such as goals placed behind them (Section \ref{sec:failure_modes}) or navigating roundabouts. In these cases, performance drops significantly, indicating that performance on uncommon situations remains a key limitation.  
\paragraph{3. Fine-tuning quickly improves performance in unseen scenarios.}  
Fine-tuning on a small subset of hand-designed cases can improve agent performance. In our experiments, fine-tuning a pre-trained model for just a few minutes enables agents to achieve near-perfect goal-reaching rates on previously difficult tasks (Section \ref{sec:finetuning}).  
\subsection{Limitations and open questions}  

Our results represent a small step towards more reliable sim agents. We highlight three limitations of our work.

\paragraph{1. Are these agents reliable enough?}  
Despite achieving near-perfect performance in many cases, failures still occur in 8\% of scenarios (862 out of 10,000), even if the \textit{fraction} of unintended behaviors per scene is tiny. This falls short of the reliability needed for fully automated AV pipelines. A key open question is how to further improve within-scene reliability to meet the high standards of automated pipelines.

\paragraph{2. Limited agent diversity and horizon.}  
Our benchmark, build atop the Waymo Open Motion Dataset, consists of short-horizon scenarios that are only 9 seconds long. Furthermore, we excluded pedestrians, cyclists, and traffic lights. Expanding the scope of evaluation to include longer scenarios with several types of road users is an interesting direction for future work.

\paragraph{3. Reliable and human-like.}  
Our agents are trained to optimize performance over given criteria above maximizing human likeness, making it unclear how closely they resemble real road users. An interesting direction for future work is balancing reliability with realism, ensuring agents not only meet performance standards but also accurately reflect human driving behavior across diverse scenarios.

\subsection{Concluding thoughts}

In summary, the application of self-play reinforcement learning has enabled state-of-the-art crash rates for end-to-end methods. Our agents crash on the order of once every 30 minutes, which, while well below human capabilities, represents a meaningful increase over baselines. Furthermore, the resultant policies appear to generalize well, even somewhat to out-of-distribution scenes, and form a base that can be rapidly fine-tuned to solve new scenes. As our agents may be independently interesting to use as part of other simulators or in autonomous vehicle test cases, we open-source our agents at \url{www.github.com/Emerge-Lab/gpudrive}.

We demonstrated the potential of scaling self-play to develop agents that can be precisely controlled to meet specific criteria in autonomous driving. While not explored in this paper, we anticipate that our findings extend to other domains such as neuroscience, where agent-based modeling is gaining momentum \cite{aldarondo2024virtual, johnson2024understanding, castro2025discovering}. In neuroscience, researchers are increasingly using physics-based simulators to create digital twins of animals, enabling cost-effective and controlled experimentation. For these agents to be useful models of animal behavior, reliability and robustness appear essential. A rodent foraging model, for example, should not exhibit free movement. We hope our work contributes to the improvement of agent-based modeling, helping to enhance controllability and robustness across different domains.

\section{Impact Statement}

This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of the work, none of which we feel must be specifically highlighted here.

\section*{Acknowledgments}
This work is funded by the C2SMARTER Center through a grant from the U.S. DOT’s University Transportation Center Program. The contents of this report reflect the views of the authors, who are responsible for the facts and the accuracy of the information presented herein. The U.S. Government assumes no liability for the contents or use thereof. This work was also supported in part through the NYU IT High-Performance Computing resources, services, and staff expertise.

% Future work
% \begin{itemize}
%     \item Add occlusion closer to how humans perceive
% \end{itemize}

%A more practical contribution of this paper is how a relatively simple observation space can be used from a complex simulation environment.

\bibliography{paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

% \section{Dataset and simulator}
% \label{appendix:data_and_simulator}

% \subsection{Waymo open motion dataset}

% - We currently only use scenarios without traffic light states


% \subsection{Mechanisms to resolve bugs / unsolvable trajectories}




\section{Observation features and design choices}
\label{appendix:feature_details}

The observation at time step $t$ for agent $i$, $\mathbf{o}_i^t$, is multi-modal and consists of three types of information: the ego state, the visible view of the scene, and the partner observation. We set the maximum number of agents per scenario throughout the experiments, $N=64$. We limit agents to vehicles. A given agent's observation is provided as a flattened vector of $\sim 3000$ elements.

\begin{table}[htbp]
    \centering
    \caption{Ego state features and dimensions provided in the observation $o_t^i$.}
    \begin{tabular}{@{}p{0.2\textwidth}p{0.1\textwidth}p{0.65\textwidth}@{}} 
        \toprule
        Feature & Dimension & Description \\ \midrule
        Speed & 1 & The speed of the agent \\
        Vehicle length & 1 & Length of the agents' bounding box \\
        Vehicle width & 1 & Width of the agents' bounding box \\
        Relative goal position & 2 & Distance from agent to the target position in the $x$ and $y$ axis \\
        Collision state & 1 & Whether the agent is in collision (1) or not (0) \\ 
        \bottomrule
    \end{tabular}
    \label{tab:ego_feature_dimensions}
\end{table}

\begin{table}[htbp]
    \centering
    \caption{Visible view or road graph features and dimensions provided in the observation $o_t^i$. The road graph consists of a sampled set of $R$ nearest road points, where $R$ is set to 200 in the experiments.}
    \begin{tabular}{@{}p{0.2\textwidth}p{0.1\textwidth}p{0.65\textwidth}@{}} 
        \toprule
        Feature & Dimension & Description \\ \midrule
        $x$ & $1 \cdot R$  & Relative x coordinate of the road point  \\
        $y$ & $1 \cdot R$ & Relative y coordinate of the road point \\
        Segment length & $1 \cdot R$ & Length of the road segment associated with the ($x, y$) coordinate \\
        Segment width & $1 \cdot R$ & Width of the road segment associated with the ($x, y$) coordinate \\
        Segment height & $1 \cdot R$ & Height of the road segment associated with the ($x, y$) coordinate \\
        Segment orientation & $1 \cdot R$ & Angle between the segment midpoint and the ego agent  \\
        Type & $1 \cdot R$ & Integer indicating the type of the road point. Existing types are: Road edge (impassable; boundary of the road), road lane, road line, stop sign, crosswalk, and speed bump. Integers are one-hot encoded during training, which multiplies the feature dimension by the total number of classes. \\
        \bottomrule
    \end{tabular}
    \label{tab:rg_feature_dimensions}
\end{table}


\begin{table}[htbp]
    \centering
    \caption{Partner (``the other``) agent features and dimensions provided in the observation $o_t^i$. Partner information is visible within the observation radius.}
    \begin{tabular}{@{}p{0.2\textwidth}p{0.1\textwidth}p{0.65\textwidth}@{}} 
        \toprule
        Feature & Dimension & Description \\ \midrule
        Speed & $1 \cdot N - 1$ & The speed of the other agents \\
        ($\mathbf{x}, \mathbf{y}$) & $2 \cdot N-1$ & Relative positions of the other $N-1$ agents in the scene. Information is only provided if the partner agents are within the observation radius of the ego agent, and are left as zero otherwise.  \\
        ($\mathbf{\theta}_x, \mathbf{\theta}_y$) & $2 \cdot N-1$ & Relative orientation of the other $N-1$ agents in the scene. Information is only provided if the partner agents are within the observation radius of the ego agent, and are left as zero otherwise.  \\
        $(w, l, h)$ & $3 \cdot N - 1$ & The width, length, and height of the bounding boxes of the other agents. \\
        \bottomrule
    \end{tabular}
    \label{tab:partner_feature_dimensions}
\end{table}

\newpage
\section{Additional figures}

\subsection{Sample rollouts}

\begin{figure}[htbp]
    \centering
    % Subfigure 1
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/tfrecord-00000-of-01000_1_3d.png}
        \caption{}
        \label{fig:example_1}
    \end{subfigure}
    \hfill
    % Subfigure 2
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/tfrecord-00000-of-01000_102_3d.png}
        \caption{}
        \label{fig:example_2}
    \end{subfigure}
    \hfill
    % Subfigure 3
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/tfrecord-00000-of-01000_114_3d.png}
        \caption{}
        \label{fig:example_3}
    \end{subfigure}
\caption{Example rollouts with the best-trained policy. Agents controlled by the trained policy are shown in blue, while static agents are colored in grey.}
\label{fig:3d_examples}
\end{figure}


\begin{figure}[htbp]
    \centering
    % Subfigure 1
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/viz_rollouts_1.pdf}
        \caption{}
        \label{fig:example_1}
    \end{subfigure}
    \hfill
    % Subfigure 2
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/viz_rollouts_2.pdf}
        \caption{}
        \label{fig:example_2}
    \end{subfigure}
    \hfill
    % Subfigure 3
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/viz_rollouts_3.pdf}
        \caption{}
        \label{fig:example_3}
    \end{subfigure}
\caption{Example rollouts with the best-trained policy. Agents controlled by the trained policy are shown in blue, while static agents are colored in grey.}
\label{fig:2d_examples}
\end{figure}



\newpage
\section{Considerations for learning sim agents through self-play PPO}

\subsection{Collision behavior}
\label{appendix:collision_behavior}

GPUDrive supports three types of collision behaviors: \textit{ignore}, \textit{remove}, and \textit{stop}. Each of these has different effects on the types of behaviors agents learn over time. We briefly outline some things to be aware of below, which might be useful for future experiments. 

\paragraph{Ignoring collisions} When collision behavior is ignored, the agent is not terminated when it collides with another agent or touches a road edge. As such, it can proceed to the goal and collide within a single episode. To discourage collisions, it seems reasonable to give agents a penalty. However, since, in most scenarios, the probability of getting negative signals in an episode with random behavior (e.g. hitting a road edge) is significantly larger than the probability of receiving a positive signal (getting to the goal), the value function may become overly pessimistic because the majority of the advantages the agent is receiving will be negative, and as such the probability of actions that lead to these negative advantages, such as higher acceleration, will be decreased. This can lead to a behavior where agents freeze (they learn to stay on the road) and do not head towards the goal. This can be avoided by ensuring that agents receive enough positive signals along with negative ones, especially early on during learning. This can be achieved by sufficient exploration through a large enough entropy coefficient.

\paragraph{Removing agents at collision} Another option is to simply terminate agents whenever they do something that is not desired (in our case colliding) without assigning penalties (giving negative rewards). This means that the goal can only be achieved if the agent does not do something bad. Since the penalty in this case is implicit, the value function can not become overly pessimistic. Instead, the advantages will be 0 most of the time early on in training. Once the first positive signals are achieved by accident (which is inevitable given the small maps of the WOMD and a high enough entropy coefficient), the probability of the right action sequences will be increased until all agents hit their goals without colliding or going off-road.

\begin{table}[htbp]
    \centering
    \caption{Overview of collision behaviors}
    \begin{tabular}{@{}p{0.15\textwidth}p{0.4\textwidth}p{0.4\textwidth}@{}} 
        \toprule
        Collision behavior & Pro's & Caveats \\ \midrule
        Ignore & 1) Agents receive a diverse range of observations & 1) Value function can become overly pessimistic; 2) Large exploration space: A large set of possible states leads to agents seeing lots of useless observations during exploration \\
        Stop & 1) Closest to the real-world effect of collisions & 1) Introduces extra challenge during learning: drive around other stopped agents \\
        Remove & 1) Simple & 1) Might lead to unrealistic behavior when used and agents are not removed from the scene; 2) Might be difficult to reach certain states because the agents are always removed upon collisions; 3) Number of completed episodes is large in the beginning since most agents are terminated within 10 steps and subsequently a lot of \texttt{reset()} calls early on in training, which decreases the SPS. \\
        \bottomrule
    \end{tabular}
    \label{tab:collision_behaviors}
\end{table}


\section{Analyses.}

\subsection{Detecting out of distribution events}
\label{appendix:ood}
\begin{enumerate}
    \item \textbf{U-turn}: For each time step $t$ where the agent is valid, we check the condition: abs(heading[t] - heading[initial]) > 150\degree.
    \item \textbf{Driving in reverse}: For each time step $t$ where the agent is valid, calculate the direction of its velocity vector and subtract it from its heading angle. If the absolute difference is greater than a threshold (150\degree), it is driving in reverse. Note: We only detect driving in reverse if it occurs for more than a threshold (10) consecutive steps, and above a minimum magnitude velocity (0.5 km/hr). 
\end{enumerate}

% \begin{figure*}[htbp]
%     \centering
%     % Subfigure 1
%     \begin{subfigure}[t]{0.32\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Figures/original_rollout.pdf}
%         \caption{\textbf{Original Goals}}
%         \label{fig:original_rollout}
%     \end{subfigure}
%     \hfill
%     % Subfigure 2
%     \begin{subfigure}[t]{0.32\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Figures/altered_rollout.pdf}
%         \caption{\textbf{Altered Goals}}
%         \label{fig:altered_rollout}
%     \end{subfigure}
%     \hfill
%     % Subfigure 3
%     \begin{subfigure}[t]{0.32\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Figures/finetuned_rollout.pdf}
%         \caption{\textbf{Altered Goals (Finetuned)}}
%         \label{fig:finetuned_rollout}
%     \end{subfigure}

%     \caption{\textbf{Rollouts for Navigating Backward}. Using the learned and finetuned policy on a hand-designed scene. \tododc{re-generate}}
%     \label{fig:rollout_comparison}
% \end{figure*}

% \subsection{Effect of initialization modes}

% report different initialization modes and trivial agents

\section{PPO implementation details.}

\subsection{Hyperparameters}

Table \ref{tab:PPO_hparams} reports the hyperparameters used for the results in our experiments.

\begin{table}[htbp]
    \centering
    \caption{PPO Algorithm Hyperparameters}
    \begin{tabular}{@{}p{0.25\textwidth}p{0.1\textwidth}p{0.55\textwidth}@{}} 
        \toprule
        Parameter & Value & Description \\ \midrule
        \texttt{total\_timesteps} & 1,000,000,000 & Total number of timesteps for training. \\
        \texttt{batch\_size} & 524,288 & Number of timesteps collected in each rollout. \\
        \texttt{minibatch\_size} & 16,384 & Number of timesteps in each minibatch for gradient updates. \\
        \texttt{learning\_rate} & 3e-4 & Initial learning rate for the optimizer. \\
        \texttt{anneal\_lr} & \texttt{false} & Whether to anneal the learning rate over time. \\
        \texttt{gamma} & 0.99 & Discount factor for future rewards. \\
        \texttt{gae\_lambda} & 0.95 & Lambda parameter for Generalized Advantage Estimation. \\
        \texttt{update\_epochs} & 2 & Number of epochs to update the policy network per rollout. \\
        \texttt{norm\_adv} & \texttt{true} & Whether to normalize advantages during training. \\
        \texttt{clip\_coef} & 0.2 & PPO clipping coefficient for policy updates. \\
        \texttt{clip\_vloss} & \texttt{false} & Whether to clip the value loss. \\
        \texttt{vf\_clip\_coef} & 0.2 & Clipping coefficient for value function updates. \\
        \texttt{ent\_coef} & 0.0001 & Entropy regularization coefficient to encourage exploration. \\
        \texttt{vf\_coef} & 0.5 & Coefficient for the value function loss in the total loss. \\
        \texttt{max\_grad\_norm} & 0.5 & Maximum norm for gradient clipping. \\
        \texttt{target\_kl} & \texttt{null} & Target KL divergence for policy updates (unused if null). \\
        \texttt{collision\_weight} & -0.75 & Penalty weight for collision events. \\
        \texttt{off\_road\_weight} & -0.75 & Penalty weight for off-road events. \\
        \texttt{goal\_achieved\_weight} & 1.0 & Reward weight for achieving the goal. \\
        \bottomrule
    \end{tabular}
    \label{tab:PPO_hparams}
\end{table}


\section{Compute resources}

Experiments were run on either a single NVIDIA A100 or an RTX4080 device for 12-36 hours per experiment. Including hyperparameter tuning and experimentation, all runs combined for this paper took approximately 5 GPU days.


% \subsection{Rollouts}

% Let $\mathbf{o}_t \in \mathbb{R}^{(W \times M, \mathcal{O})}$ where $W$ is the number of worlds, $M$ is the maximum number of agents to be controlled and $\mathcal{O}$ the observation dimension. All worlds are reset at initialization:

% $$\mathbf{o}_0 = \texttt{env.reset()}$$

% Then $\mathbf{o}_0$ is used to generate actions:

% $$\mathbf{a}_0 \sim \pi_{\theta}(\mathbf{o}_0), \qquad \mathbf{a}_0 \in \mathbb{R}^{(W \times M)}$$

% With the generated actions, we step the environment/simulator, and time is incremented by one:

% $$
% \mathbf{o}_1, \mathbf{r}_1, \mathbf{d}_1, \mathbf{i}_1 = \texttt{env.step}(\mathbf{a}_0)
% $$

% The following tuple is stored in the buffer for later to update the model:

% $$
% (\mathbf{o}_0, \mathbf{a}_0, \mathbf{r}_1, \mathbf{d}_1, \mathbf{i}_1)
% $$

% Generally, we store:

% $$
% (\mathbf{o}_t, \mathbf{a}_t, \mathbf{r}_{t+1}, \mathbf{d}_{t+1}, \mathbf{i}_{t+1})
% $$

% \subsection{Handling multiple agents}

% Environments in \texttt{gpudrive} are initialized from WOMD scenarios. Every scenario may have a different number of agents that we control. To explain how this is handled in the implementation, consider the following examples. Every time step, we store: 

% $$
% (\mathbf{o}_t, \mathbf{a}_t, \mathbf{r}_{t+1}, \mathbf{d}_{t+1}, \mathbf{m}_{t})
% $$

% where $\mathbf{m}_{t}$ is a mask that determines which indices to exclude from training; \texttt{1} for living agents and \texttt{0} otherwise. 

% \paragraph{Example 1. One world, two agents} Let $W=1$ and $N=2$. We highlight information from living agents in green and dead agents in red. 

% At $t=0$:
% \begin{itemize}
%     \item Initialize agent mask: $\mathbf{m}_{0} = [\dg{1, 1}]$
%     \item Initialize environment: 
%     $$
%     \begin{bmatrix}
%         \dg{o_0^0} \\
%         \dg{o_0^1} \\ 
%     \end{bmatrix} = \texttt{env.reset()}
%     $$
%     \item Sample actions and step environment
%     $$\begin{bmatrix}
%         \dg{a_0^0} \\
%         \dg{a_0^1} \\ 
%     \end{bmatrix}  \sim \pi_{\theta} \left(
%     \begin{bmatrix}
%         \dg{o_0^0} \\
%         \dg{o_0^1} \\ 
%     \end{bmatrix} \right)$$
%     \begin{align*}
%     \begin{bmatrix}
%         \dg{o_1^0} \\
%         \dg{o_1^1} \\ 
%     \end{bmatrix}, 
%     \begin{bmatrix}
%           \dg{r_1^0 = 1} \\
%           \dg{r_1^1 = 0} \\ 
%     \end{bmatrix},
%     \begin{bmatrix}
%           \dg{d_1^0 = 1} \\
%           \dg{d_1^1 = 0} \\ 
%     \end{bmatrix}
%     \begin{bmatrix}
%           \dg{i_1^0} \\
%           \dg{i_1^1} \\ 
%     \end{bmatrix} 
%     = \texttt{env.step} \left(
%     \begin{bmatrix}
%         \dg{a_0^0} \\
%         \dg{a_0^1} \\ 
%     \end{bmatrix} \right)
% \end{align*}
% \item Store in buffer
%     \begin{itemize}
%         \item \textbf{agent 0}: $(\dg{o_0^0, a_0^0, r_1^0=1, d_1^0=1, m_0^0=1})$
%         \item \textbf{agent 1}: $(\dg{o_0^1, a_0^1, r_1^1=0, d_1^0=0, m_0^1=1})$
%     \end{itemize}
% \item Update live agent mask: Before the next iteration, we update the live agent mask:
% \begin{align*}
%     \mathbf{m}_1 &= (\mathbf{m}_0[\mathbf{d}_1 == 1]) = \dr{0} \\
%                  &= [\dr{0}, \dg{1}]
% \end{align*}
% \end{itemize}

% At $t=1$:
% Now agent $i=0$ is dead and will be masked out from $t=1$ onwards. We repeat the steps above and eventually store

% Store in buffer
% \begin{itemize}
%     \item \textbf{agent 0}: $(\dr{o_1^0, a_1^0, r_1^1=1, d_2^0=1, m_1^0=0})$
%     \item \textbf{agent 1}: $(\dg{o_1^1, a_1^1, r_2^1=0, d_2^1=0, m_2^1=1})$
% \end{itemize}

% \subsection{Asynchronous resets in multi-agent environment}

% TODO

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
