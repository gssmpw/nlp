\section{System Design}

We introduce \tool, an automated fact sheet generation system, as shown in \cref{fig:overview}. \tool applies the concept of an AI chain to generate fact sheets based on data and user requests. Users can further customize the fact sheet, such as by adding new facts or rearranging content. In this section, we first highlight the AI worker profiling and workflow, followed by a detailed description of the fact sheet generation process. Lastly, we present the \tool’s interface for generating and customizing fact sheets.

\subsection{AI Worker Profiling \& Workflow}
\label{sec:agent}

While simple prompting techniques can effectively handle straightforward, isolated tasks, such as visualization code generation, they face challenges when applied to more complex, integrated tasks. First, LLMs often lack domain-specific knowledge, which can result in outputs that do not meet the nuanced requirements of specialized tasks. Second, providing an abstract task without thorough step-by-step planning can disrupt the overall process, as LLMs may fail to propose the necessary sequence of actions to achieve the desired outcome. Finally, there can be incompatibilities between subtasks, where outputs from one stage may not align with the inputs required by the next, leading to pipeline failures and hindering task completion. To address these issues, we adopted the AI chain approach for generating fact sheets from tabular datasets. This approach leverages specialized AI workers, known as agents, each designed to handle specific subtasks more efficiently. These agents possess unique knowledge and capabilities tailored to their specific tasks, allowing them to focus on their assigned roles with greater precision, thereby improving the overall quality of the output. We define each worker with the following attributes:


\textit{Overall Goal \& Specific Role.} To simulate the positive impact on work outcomes and productivity, we provide overall goals before detailing individual tasks, mirroring real-world scenarios~\cite{barrick2013theory}. The workers share common objectives while working on specific parts of the pipeline, stimulating goal-directed behaviors~\cite{sioutis2006agent}. The specific role of each worker in the AI chain is then defined to achieve the overall objective.

\textit{Persona.} Persona refers to how each individual behaves and thinks~\cite{10.1145/3613904.3642406}, which has been shown to be crucial in optimizing the effectiveness of AI workers~\cite{10.1145/3640794.3665887}. While LLMs do not have a default persona, we manually define the persona of each worker, including the characteristic traits, motivation, and expertise necessary for successfully completing assigned tasks. For example, the Fact Composer is specified to be creative and analytical, while the Data Extractor is meticulous and knowledgeable. Further details of the AI workers we defined can be found in Section~\ref{sec:factideas}.


\textit{Input \& Output.} We provide a list of input and output variables with their descriptions, formats, and data types that the agent receives or returns. This improves the reliability of the overall pipeline. Additionally, we include few-shot examples following this input and output format, which further enhances the reliability and alignment of the output~\cite{song2023llm}.

\textit{Knowledge Base.} This refers to specialized domain knowledge that supplements the general capabilities of LLMs. For each task assigned to a worker, we compile and refine relevant datasets and prior research to build a tailored Knowledge Base. This allows the agent to perform optimally in its specialized task, avoiding the hallucination effects of LLMs when dealing with complex user requests~\cite{martino2023knowledge}.

\textit{Instructions.} Research has shown the importance of sub-step guidance over abstract instruction for task performance accuracy~\cite{madaan2022language}. Therefore, we include step-by-step procedures that each worker follows to achieve its objectives.

Our AI workers utilize a decentralized communication model, where interaction occurs directly between related agents~\cite{cheng2024prompt}. Upon completing a task, an agent forwards its output to the next designated agent in the workflow. To maintain consistency, all input and output data are standardized in JSON format, adhering to REST API protocols. For transmitting object files, such as images or data files, these are stored as block objects, with the directory path provided for file retrieval. 


\subsection{Fact Sheet Generation}

Upon receiving the dataset and, optionally, a user request, the fact sheet generation process begins. \tool first analyzes the dataset to create a dataset representation and then utilizes the AI workers described in \cref{sec:factideas} to generate the fact sheet structure and visual components. Following this, we arrange the fact sheet contents on the canvas and export the initial fact sheet.


\subsubsection{Dataset Representation}
\label{sec:anomysation}

Given that LLMs process input as textual prompts, our goal is to identify the optimal method for capturing and conveying the essential details of the input dataset. Traditional approaches often involve feeding the entire dataset to the LLM, but this method faces significant limitations. Firstly, LLMs have token constraints, making it impractical to handle large datasets. Additionally, directly exposing the entire dataset can compromise privacy and intellectual property. Therefore, our approach focuses on incorporating sufficient dataset information into the prompts to guide LLMs in making accurate decisions without directly providing the dataset. We achieve this by presenting the dataset using SQL table creation commands, column statistics, and example rows. An example of dataset representation for the Global Population dataset with 3 columns (Country, Population, Continent) is shown in \cref{fig:overview}.

\textit{SQL Table Creation Command.} To capture the table name and column names along with their corresponding data types, we use SQL table creation command syntax. This approach also aids our AI workers in better formulating and generating SQL query syntax in subsequent processes. 

\textit{Column Statistics.} For each column, we generate statistics that summarize the values it contains. For numerical columns, this includes the maximum, minimum, median, average, and the 25th and 75th percentiles. For string columns, we count the number of unique values and identify the top five most frequent occurrences. These statistics provide LLMs with a comprehensive overview of the data range.

\textit{Anonymized Data Examples.} We include anonymized sample rows from the dataset to help LLMs understand the expected data values in each column. The number of rows presented depends on the token limits, with more columns requiring more tokens. Given that the input data may contain private information or intellectual property, anonymization is crucial to protect the confidentiality of the data~\cite{lin2024promptcrypt}. We employ a format-preserving encryption technique that maintains semantic integrity, preserving the semantic meaning and ordinal relationships of the data during anonymization. This allows LLMs to comprehend the underlying data structure without directly accessing the original dataset. The anonymization process involves classifying each column by its data type—nominal, ordinal, discrete, or continuous—and applying a tailored anonymization method.

\begin{itemize}

\item \textit{Nominal values} are qualitative and lack a specific order or comparability. To anonymize these values while preserving meaning, we substitute them with semantically equivalent terms. For instance, if the column refers to country names, ``France'' might be replaced by ``Italy.'' We use the column's entity type to guide this substitution, generating random values for each entity type and mapping them to unique values in the data.
\item \textit{Ordinal values} have a specific order, and maintaining this order during substitution is crucial to avoid inconsistencies in later analysis. For example, in a letter grading system where grades are A, B, C, D, and F, substituting these grades with random values could lead to confusion, such as when querying how many students received a grade better than C. To prevent this, we first extract all valid values from the dataset and ensure that the anonymized values are selected from this pool.
\item \textit{Discrete numerical values} are anonymized using a mathematical transformation that maintains the ordinal relationships between values. For example, if the original values are integers representing counts, the transformed values will still reflect the same relative magnitudes, ensuring consistency in any subsequent analysis.
\item \textit{Continuous values} such as those found in time series data, are treated with a different approach. We anonymize these by generating new random values within the original range, ensuring that the data remains realistic and usable while masking the exact figures.
\end{itemize}

 
\subsubsection{Fact Sheet Content Generation}
\label{sec:factideas}

After anonymizing the dataset, we feed the data into our AI chain to generate the fact sheet content. Users can provide additional requests in natural language to specify their requirements for the dataset. This feature is particularly useful when users are interested in covering only a specific portion of the dataset rather than the entire dataset. For example, a user may request data within a limited time range or focus on specific columns. This section discusses how each agent handles its tasks in the pipeline.

\textit{Fact Idea Composer.} 
The primary task of this agent is to generate fact ideas from the dataset. To enhance the agent's capability, we incorporated expert knowledge from a prior study that categorizes facts into eleven distinct types such as Distribution, Categorization, Trend and Rank~\cite{wang2019datashot}. Additionally, we conducted a design session with three senior data scientists, providing each with ten distinct datasets and asking them to suggest interesting facts for a fact sheet. We refined these suggestions by removing duplicates and resolving ambiguities, integrating the refined list of fact ideas into the Fact Idea Composer's knowledge base\footnote{\url{https://anonymous.4open.science/r/factflow-public-2BF8/knowledgeBase}}. This ensures that the generated fact ideas align closely with expert expectations and are grounded in the actual data context. The agent is instructed to generate all potential facts and rank them based on relevance and significance, filtering out less important or redundant facts. The Fact Composer Agent employs the self-consistency prompting technique~\cite{cheng2024relic}, using multi-turn communication to ensure output accuracy. The agent's output consists of a list of fact ideas, each categorized by fact type and content.

\textit{Data Extractor.}
Since each fact represents only a portion of the original dataset, \tool involves the Data Extractor to retrieve the data relevant to the fact. This agent receives a fact idea, additional user requests, and the dataset representation to generate an appropriate SQL command to query the original dataset. To improve the accuracy of converting natural language to SQL (NL2SQL), we added into the agent's knowledge base the Spider dataset~\cite{yu-etal-2018-spider}, which is a well-known dataset containing the natural language questions and its corresponding SQL command. Given the challenges LLMs face in code generation, particularly in terms of robustness and accuracy~\cite{yu2023llm}, we apply the Code Generation with Advising and Validation approach~\cite{chen2023seed}. The agent first lists recommendations for code generation, considering data types and required facts. It then generates an executable SQL command to query the data locally. In the validation step, the agent presents the data representation obtained from the initial query, along with the SQL command, to the LLMs to refine any syntactical errors. Upon completion, the agent outputs the filtered dataset relevant to the fact idea.


\textit{Data Visualizer.}  
After filtering the data, each fact idea is presented through a visualization. The Data Visualizer uses the dimensions of the filtered dataset provided by the previous agent to recommend the most appropriate visualisation type and visual encoding channels  To ensure the fact sheet's accessibility to a wide range of users, \tool supports widely recognized visualization types such as line charts, bar charts, scatter plots, pie charts, and area plots. We limit the number of encoded dimensions to three, resulting in a 2D chart with color as the maximum additional encoding. Unlike previous approaches that relied on direct code generation for creating graphs, we implemented a parameter-driven approach. In this method, a function generates different types of visualizations based on input parameters provided by the agent. These parameters may include chart type, data dimensions, axis labels, color schemes, and more. To enhance the Data Visualizer's capability in generating accurate and relevant visualizations, we documented the function’s input and output parameters. This documentation serves as a knowledge base, guiding the agent in appropriately configuring the parameters to produce the desired visual outputs.


\textit{Fact Writer.}
The Fact Writer leverages the processed dataset and visualizations from previous agents to generate factual statements that are well-aligned with the context provided by the charts. Additionally, the agent aims to clarify the underlying rationale of the presented data. While some inquiries related to the data can be directly addressed using the processed data from previous steps, others require a broader knowledge base for comprehensive explanation. Therefore, we instruct the agent to generate questions regarding the causal relationships in the presented data. The Fact Writer uses LLMs to create additional questions that explore the causation effects of external contexts, such as related historical events or geographical information. These questions help explain the key data points highlighted in the chart or explain what the fact implies. For example, if the dataset shows that France is the most visited country for tourism, the agent might ask, \textit{``Why is France the most visited country for tourism?''} to gain deeper insights and present them. This enriched content significantly enhances the viewer's understanding and interpretation of the charts, especially when dealing with complex data that might be unfamiliar to the audience.


\textit{Factsheet Organizer.}
The primary goal of this agent is to structure the fact sheet in the most logical order to enhance the learnability of the presented information. Our fact sheet contains different sections, each presenting a key topic with a list of relevant charts to support the idea. Using the list of facts and generated material from other workers, the Fact Sheet Organizer finds relevancy between facts to form the story flow with several key topics. After that, the agent sorts the list of facts into these topics based on the relevancy and outputs the structure of the fact sheet in JSON format for fact sheet generation.


\begin{algorithm}
\caption{Split Sections Into Columns}
\label{alg:split_sections}
\begin{algorithmic}[1]
\Require A list of $sections$
\Ensure A boolean list for column layout
\Function{split\_columns}{$sections$}
    \State $best\_diff \gets \infty$, $best\_layout \gets \text{None}$
    \For{$perm$ in $\text{permutations}(sections)$}
        \For{$comb$ in $\text{powerset}(\text{range}(1, \text{len}(perm)))$}
            \State $left\_score \gets \text{calculate\_score}(perm[0])$
            \State $right\_score \gets 0$
            \For{$i \gets 1$ \textbf{to} $\text{len}(perm) - 1$}
                \If{$i$ in $comb$}
                    \State $left\_score \gets left\_score + \text{calculate\_score}(perm[i])$
                \Else
                    \State $right\_score \gets right\_score + \text{calculate\_score}(perm[i])$
                \EndIf
            \EndFor
            \State $diff \gets \text{abs}(left\_score - right\_score)$
            \If{$diff < best\_diff$}
                \State $best\_diff \gets diff$, $best\_layout \gets (perm, comb)$
            \EndIf
        \EndFor
    \EndFor
    \State Convert $best\_layout$ to boolean list $result$
    \State \Return $result$
\EndFunction
\end{algorithmic}
\end{algorithm}


\subsubsection{Fact Sheet Layout}
\label{sec:layout}

\begin{figure*}% specify a combination of t, b, p, or h for top, bottom, on its own page, or here
  \centering % avoid the use of \begin{center}...\end{center} and use \centering instead (more compact)
  \includegraphics[width=0.8\linewidth]{figs/customisation.pdf}
  \caption{%
  	Our \tool interface: a) User can upload dataset and provide any additional natural language request to generate the fact sheet b) Based on the initial fact sheet, users can manually add, remove and edit the content, as well as generating a new fact using natural language request.
  }
  \label{fig:customisation}
    \vspace{-4mm}
\end{figure*}

After establishing the structure of the fact sheet, we proceed to design its layout. We advocate for a dual-column layout to enhance readability and minimize eye movement~\cite{shrestha2008eye}. The layout consists of multiple sections within each column, arranged vertically. Our objectives in developing the layout are twofold: 
First, to preserve the original logical order of the sections as outlined in the fact sheet structure, thereby enhancing its storytelling attributes. Second, to achieve a balanced distribution of content between the two columns. This balance not only contributes to the aesthetic appeal of the fact sheet but also ensures content readability. A key challenge is dividing sections into two columns, given the varying sizes of each section due to differing numbers of facts. To address this, we assign a fixed height to each fact and calculate the cumulative height of each section. We then apply an optimization algorithm to distribute sections across the columns, as described in \cref{alg:split_sections}. This algorithm processes an array of section heights in logical order and outputs a boolean array, assigning sections to the left column as ``True'' and to the right column as ``False''. The Introduction section is always placed at the top left, so the first array value is set to ``True'' by default. \tool then uses this layout structure to iteratively assign sections to the columns, generating the complete fact sheet.



\subsection{Fact Sheet Customization} 
\label{sec:customisation}

In developing our automated pipeline for fact sheet generation, we emphasize the synergy between human input and AI automation, aiming to enhance human-AI collaboration with AI as an assistant in various tasks. To support this, we provide an interface that allows users to create and customize AI-generated content within the fact sheet. Users start by uploading their dataset on the Upload page (\cref{fig:customisation}a) and can specify their interests. We also offer two sample datasets for experimentation.

Upon clicking Begin, \tool automates the fact sheet generation and directs users to the Editor page (\cref{fig:customisation}b), where they can dynamically tailor the content to their needs. The interface allows users to add, remove, and rearrange sections according to their preferences. For each visualisation, users can delete, rearrange or move them across sections. \tool supports adding new charts based on natural language request, processing this as new fact ideas through the AI chain to generate corresponding charts and textual content. This new fact will be presented in appropriate section automatically. After customization, users can export the fact sheet as a PDF. The generated content will be stored with the unique ID, where user can further continue to modify the same fact sheet in the future.

\subsection{Implementation}

\begin{figure*}[!h]% specify a combination of t, b, p, or h for top, bottom, on its own page, or here
  \centering
   \label{fig:showcase}
  \begin{subfigure}[t]{0.50\textwidth}
  	\centering
  	\includegraphics[width=\textwidth]{figs/factsheet-1.pdf}
  	\caption{Fact sheet about startup failures after the tide of ``new economics'' in China.}
  	\label{fig:showcase1}
  \end{subfigure}%
  \begin{subfigure}[t]{0.49\textwidth}
  	\centering
  	\includegraphics[width=\textwidth]{figs/factsheet-2.pdf}
  	\caption{Fact sheet about university major and employment opportunities.}
  	\label{fig:showcase2}
  \end{subfigure}%
  \\%

  \subfigsCaption{Two fact sheets that automatically generated by \tool based on real-world datasets. }
    \vspace{-4mm}
\end{figure*}

\tool is implemented in Python 3.8, utilizing the Matplotlib library for generating visualizations and the FPDF library for creating and storing fact sheets. To facilitate seamless communication with the GPT-4 model from OpenAI, we establish a REST API interface. For public accessibility, \tool is deployed as a web application using Flask.
