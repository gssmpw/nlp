
\subsection{User Evaluation}

We conducted a user study to assess the quality of the fact sheets generated by \tool and the usability of our fact sheet customization interface. First, to evaluate the quality of the fact sheets created by \tool, we involved data workers from industry and non-data workers to determine the tool's versatility in catering to both data professionals and general audiences. We presented the automatically generated fact sheets from \tool and two state-of-the-art baselines to the participants and collected their feedback. Second, we assessed the quality of customized fact sheets and the user experience during the customization process using \tool. This involved a user study where participants actively customized their fact sheets with \tool, followed by feedback collection.

\subsubsection{Research Questions}
This study aimed to answer the following research questions:
\begin{itemize}
    \item \textbf{RQ1}: How does the content and visual presentation of fact sheets generated by \tool compare to those produced by existing approaches?
    \item \textbf{RQ2}: How does a user's experience level with data analytics influence their evaluation of fact sheets?
    \item \textbf{RQ3}: What is the quality of customized fact sheets created using \tool, and how do users perceive the experience of customizing fact sheets with this tool?
\end{itemize}

    \begin{figure*}[tb]% specify a combination of t, b, p, or h for top, bottom, on its own page, or here
  \centering % avoid the use of \begin{center}...\end{center} and use \centering instead (more compact)
  \includegraphics[width=0.95\linewidth]{figs/exp1.pdf}
    \caption{Radar charts showing the quality ratings of fact sheets produced by \tool compared to the Calliope and ChatGPT Plus baselines, evaluated by three participant groups: N1-N6 (novices), J1-J6 (junior-level data workers), S1-S6 (senior data workers), and ALL (all participants).}
  \label{fig:part1}
    \vspace{-4mm}
\end{figure*}

\subsubsection{Experimental Setup}

    \textbf{Participants.} We recruited eighteen participants (8 males, 10 females, aged 23-46 years, Mean=30.94, SD=1.43) to join our 2-phase study. All were proficient English speakers with at least a bachelor's degree. Participants were divided into three groups based on their experience with data visualizations: Group 1 (N1-N6) with no prior experience with data work, Group 2 (J1-J6) with 1 to 5 years of work experience, and Group 3 (S1-S6) with over 5 years of experience. This division aimed to analyze the impact of work experience on expectations of fact sheet quality and the user experience with \tool. Each participant received a AU\$50 Amazon gift card for their participation.


    \textbf{Datasets.} We used two different datasets to generate fact sheets for this experiment. First, we selected the CarSales dataset (as used in previous research~\cite{shi2020calliope}), comprising 4 columns (Brand, Type, Sale, and Year) and 275 rows, with each row representing a car sale record. Second, we used the Movies dataset, which includes 7 columns (Movie, Studio, Type, Worldwide \$m, Domestic \$m, Overseas \$m, and Year) and 198 rows, with each row representing a movie. This dataset was chosen for its diversity in data types (categorical, temporal, and numerical), enabling the extraction of a wide range of facts. All participants were unfamiliar with these datasets before the user study.

    \textbf{Baselines.} We selected two baselines that support automatic fact sheet generation and generated two fact sheets each for the CarSales and Movies datasets. The first baseline, \textbf{Calliope}~\cite{shi2020calliope}, is a state-of-the-art tool for producing high-quality narrative flows in fact sheets. We used Calliope to benchmark improvements in fact sheet generation. The second baseline, \textbf{ChatGPT Plus} with the latest GPT-4 model, is recognized for its capability to interpret data files and generate graphical visualizations~\cite{openai_2023}. We used ChatGPT Plus to demonstrate the advancements of our AI chain approach compared to simple prompting techniques. We uploaded the datasets to ChatGPT Plus and requested fact sheets be generated. Fact sheets from \tool were generated using our public platform. Each fact sheet was automatically generated with no further modification or changes. All generated fact sheets from \tool and the baselines are provided in our supplementary material\footnote{\url{https://anonymous.4open.science/r/factflow-public-2BF8/Experiment/}}.

    \textbf{Procedures.} We conducted a 60-minute Zoom session with each participant, initially introducing them to the datasets and the concept of fact sheets. After this introduction, we displayed six automatically generated fact sheetsâ€”two each from \tool, Calliope, and ChatGPT Plus, derived from the Movies and CarSales datasets. Participants were given five minutes to review each fact sheet. To maintain fairness, the presentation order of the fact sheets was counterbalanced, and participants were not informed which tool produced which fact sheet. We then collected both quantitative and qualitative feedback for each fact sheet, ensuring an unbiased comparison and comprehensive evaluation.

    After collecting feedback on the generated fact sheets, we introduced participants to our customization platform through a 3-minute tutorial video. This video used the CarSales dataset to showcase \tool's capabilities, such as adding or deleting facts and sections, rearranging components, and editing text as detailed in \cref{sec:customisation}. Following the tutorial, participants were given a \tool-generated preliminary fact sheet based on the Movies dataset, with 20 minutes allocated for customization according to their preferences and dataset insights. Subsequently, we collected both quantitative and qualitative feedback through a structured questionnaire.

    \textbf{Metrics.} Similar to Calliope's evaluation~\cite{shi2020calliope}, we assessed fact sheet quality based on Content (Insightfulness, Comprehensiveness, Effectiveness, Coherence, and Reasonableness) and Visuals (Ease of understanding, Aesthetic appeal, and Expressiveness). We removed the System aspects as we only evaluated the quality of generated fact sheets. For each metric, we used a 5-point Likert scale (1 being the lowest, 5 being the highest). Additionally, participants were asked to rate their most preferred fact sheet and justify their choice, as well as provide suggestions for improvement for each fact sheet.
    
    We assessed the quality of our \tool customization platform using three metrics: (i) \textbf{Accuracy}, which measures how well the generated facts reflect the dataset's data; (ii) \textbf{Alignment}, which assesses the extent to which the customized facts match the users' ideas and requests; and (iii) \textbf{Aesthetic}, which evaluates the visual appeal and design quality of the fact sheets. Participants rated each metric on a 5-point Likert scale (1 being the lowest, 5 being the highest). To measure the usability of \tool, we used the System Usability Scale (SUS)~\cite{bangor2008empirical}, which consists of 10 questions each rated on a 5-point scale (1 for strongly disagree, 5 for strongly agree). Finally, we invited participants to share their thoughts on the advantages and disadvantages of using \tool, their experiences with AI-assisted customization, and how they see such tools fitting into their daily data workflows.


\begin{figure*}[tb]% specify a combination of t, b, p, or h for top, bottom, on its own page, or here
  \centering % avoid the use of \begin{center}...\end{center} and use \centering instead (more compact)
\includegraphics[width=0.8\textwidth]{figs/part2.pdf}
  	\caption{A) Bar chart showing ratings from different user groups on Accuracy, Alignment, Aestheticness for the customized fact sheet.
   B) Bar chart displaying average SUS feedback from participants on the use of the \tool platform.}

  \label{fig:part2}
    \vspace{-7mm}
\end{figure*}
 
\subsubsection{Results}
We summarize feedback from 18 participants to answer the research questions, starting with quantitative assessments of \tool and baseline tools, followed by user preferences for fact sheets and their reasons. We also discuss suggestions to enhance \tool's fact sheet quality. After that, we detail user evaluations of the customized fact sheets and their experiences with \tool, alongside a discussion on the potential benefits of AI-assisted tools in data workflow optimization.

\underline{Fact Sheet Quality Ratings:} As depicted in \cref{fig:part1}, our quantitative analysis of participant feedback highlights \tool's impressive performance, with an overall average rating of 4.63 out of 5. This rating is significantly higher than the scores attained by Calliope and ChatGPT Plus, which received average ratings of 2.37 and 2.84, respectively. Remarkably, \tool surpassed these tools in every user group category, including users with no experience (N1-N6), junior data workers (J1-J6), and senior data workers (S1-S6). Our tool demonstrated particularly strong performance in the criteria of being \emph{Insightful} (4.78) and \emph{Effective} (4.78). The comparison between the ratings from different participant groups for \tool revealed that junior data workers gave it the highest rating of 4.71. This was closely followed by users with no prior experience, who rated it at 4.63, and senior data workers, who provided a slightly lower rating of 4.54.


\underline{Most Preferred Fact Sheet and Rationale:} The collected feedback indicates a strong preference for \tool among all participants, who commended its comprehensive and logical presentation of data. Participants N1, N5, N6, J1, and J2 were impressed by the coherent story that \tool narrated, with N1 describing it as \emph{``the most comprehensive one''} and N6 noting its storytelling capability that allowed them to have a deep understanding of the dataset with minimal effort. In terms of design and layout, participants praised its consistency and structure (N3, J4, J5). N3 admired \emph{``the consistency of fonts, colors, and visualization size,''} while J4 and J5 acknowledged its structured layout, with J5 noting that the consistency of layout helped to improve the overall readability of the fact sheet. Visual appeal and clarity were also highlighted as significant strengths (S1, S5, S6). S5 mentioned that it \emph{``uses more appropriate chart types and is more visually appealing,''} reinforcing S1's sentiment, who found that the \emph{``layout and design of the visualization are visually appealing.''} Furthermore, several participants, such as S3, suggested \tool's potential role in real-world applications, especially in explaining business-related data. The clarity with which \tool delivers information was emphasized by S4 as \emph{``understandable and very clear,''} highlighting the fact sheet's effectiveness in communication. Collectively, the feedback illustrates that \tool excels in providing a logical, comprehensive, and visually appealing representation of data, structured in a way that is both engaging and accessible for various users, from business analysts to those less skilled in data analysis. 

For the baseline tools, participants mentioned that they appreciated the color palette and layout of different facts from Calliope. For improvement, they suggested introducing diverse visual elements to comprehensively cover necessary information, optimizing chart types and designs for clarity, and ensuring a professional and uncluttered layout. Additionally, the language content can be utilized to provide causal explanations for data trends. For ChatGPT Plus's baseline, while the majority of participants praised the content for being easy to understand, the focus for further improvements is on visualizing key information succinctly, simplifying data representation, and generating deeper insights, possibly through more advanced analysis of the provided data.

\begin{mdframed}[linecolor=black, linewidth=1pt]
    \textbf{Answer to RQ1}: \tool outperforms baseline methods in generating fact sheet content that is not only more insightful and comprehensive but also more coherent and logically sound. Additionally, the tool excels in creating designs that are easier to comprehend and more aesthetically appealing and expressive.
\end{mdframed}

 \underline{Improvements:} To enhance \tool, the feedback indicates a need for brevity and visual clarity, with participants suggesting more concise content for quicker comprehension (N1, N2, N4). Additionally, participants advocated for more diverse visual elements, such as additional chart types and images, to increase engagement (N3, N6, J4). Some critiques focused on improving visual appearance through larger text sizes and distinct color use to differentiate data points (J6, S6). Finally, S2 and S3 suggested more intricate data representation, including forecasting and statistical analysis, from their experience.

\underline{Fact Sheet Customization Quality.} As shown in \cref{fig:part2}a, \tool received an average rating of 4.11 out of 5, reflecting robust user satisfaction. \tool scored 4.39 for Accuracy, indicating the robustness of our approach to query and present data from the dataset. The Alignment and Aesthetic Appeal received scores of 3.83 and 4.11, respectively, suggesting that \tool effectively resonates with user needs and aesthetic preferences. When analyzing the differences in results from different participant groups, all groups agreed on the aesthetics and accuracy of the modified fact sheet. Interestingly, there were differences in ratings regarding alignment for these user groups. By analyzing the commands issued by users to request new facts, we noticed that more experienced participants tended to have more complex requests. Users with low data experience made simple queries such as \emph{``Show me the top 5 dramas with the highest revenue this century''} or \emph{``The proportion of movies by type from Fox Studio,''} which \tool handled effectively. However, a request from a senior data scientist, \emph{``Predict the future trends of different studios in choosing the most profitable movie genre,''} posed challenges to \tool in terms of predictive modeling capabilities for forecasting future trends, which can be considered as an area for future work.

\begin{mdframed}[linecolor=black, linewidth=1pt]
    \textbf{Answer to RQ2}: The experience level of a participant is proportional to their expectations of fact sheet quality, as well as the complexity of their requests.
\end{mdframed}

    \underline{SUS Form Result.} \cref{fig:part2}b shows the usability feedback of \tool, resulting in a score of 73.45 out of 100. Specific feedback highlighted \tool's intuitiveness, with scores of 4.22 for ease of use and 4.33 for the ability to learn quickly. The integration quality received a positive 4.11, while complexity was deemed low, with a score of 1.72. Collectively, these metrics underscore the high usability of \tool.

    \underline{Qualitative Analysis.} We inquired about participants' most favored aspects of \tool and their recommendations for user experience improvements. Participant S1 and J1 praised \tool's innovative approach, while N2 highlighted the novelty of creating charts via commands as \textit{``quite new.''} Additionally, N4 appreciated the tool's performance, noting its \textit{``fast and responsive''} nature. Features such as the drag-and-drop functionality and PDF export capability received positive feedback for their usefulness in data sharing and presentation, with N6 emphasizing their value. To enhance user support, suggestions were made for integrating a chatbot for immediate assistance (N1) and providing on-screen tutorials to facilitate a smoother onboarding experience (N3). An undo feature was proposed by S6 to offer more control over the data visualization process. Meanwhile, N4 expressed a desire for customizable formats to suit various presentation contexts.


    \begin{mdframed}[linecolor=black, linewidth=1pt]
        \textbf{Answer to RQ3}: The customization module of \tool received high ratings for usability and output quality, showcasing its effective customization capabilities.
    \end{mdframed}