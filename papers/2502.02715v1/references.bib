    @ARTICLE{159342,
      author={},
      journal={IEEE Std 610.12-1990}, 
      title={IEEE Standard Glossary of Software Engineering Terminology}, 
      year={1990},
      volume={},
      number={},
      pages={1-84},
      keywords={Terminology;Software engineering;Standards;glossary;terminology;dictionary;Software engineering;Definitions},
      doi={10.1109/IEEESTD.1990.101064}}


    @misc{idoft2024,
  title = {International Dataset of Flaky Tests ({IDoFT})},
  howpublished = {\url{https://github.com/TestingResearchIllinois/idoft}},

}

    @inproceedings{WMD+20,
    author = {Wang, Yuqing and Mäntylä, Mika and Demeyer, Serge and Wiklund, Kristian and Eldh, Sigrid and Kairi, Tatu},
    year = {2020},
    month = {01},
    pages = {27-38},
    title = {Software Test Automation Maturity: A Survey of the State of the Practice},
    doi = {10.5220/0009766800270038}
    }
@inproceedings{Lam2019,
author = {Lam, Wing and Godefroid, Patrice and Nath, Suman and Santhiar, Anirudh and Thummalapenta, Suresh},
title = {Root causing flaky tests in a large-scale industrial setting},
year = {2019},
isbn = {9781450362245},
publisher = {Association for Computing Machinery},
doi = {10.1145/3293882.3330570},
abstract = {In today’s agile world, developers often rely on continuous integration pipelines to help build and validate their changes by executing tests in an efficient manner. One of the significant factors that hinder developers’ productivity is flaky tests—tests that may pass and fail with the same version of code. Since flaky test failures are not deterministically reproducible, developers often have to spend hours only to discover that the occasional failures have nothing to do with their changes. However, ignoring failures of flaky tests can be dangerous, since those failures may represent real faults in the production code. Furthermore, identifying the root cause of flakiness is tedious and cumbersome, since they are often a consequence of unexpected and non-deterministic behavior due to various factors, such as concurrency and external dependencies.  As developers in a large-scale industrial setting, we first describe our experience with flaky tests by conducting a study on them. Our results show that although the number of distinct flaky tests may be low, the percentage of failing builds due to flaky tests can be substantial. To reduce the burden of flaky tests on developers, we describe our end-to-end framework that helps identify flaky tests and understand their root causes. Our framework instruments flaky tests and all relevant code to log various runtime properties, and then uses a preliminary tool, called RootFinder, to find differences in the logs of passing and failing runs. Using our framework, we collect and publicize a dataset of real-world, anonymized execution logs of flaky tests. By sharing the findings from our study, our framework and tool, and a dataset of logs, we hope to encourage more research on this important problem.},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA 2019)},
pages = {101–111},
numpages = {11},
keywords = {debugging, flaky tests, regression testing},
location = {Beijing, China},
}
    
    @article{LVL23,
    author = {Luccioni, Alexandra Sasha and Viguier, Sylvain and Ligozat, Anne-Laure},
    title = {Estimating the carbon footprint of {BLOOM}, a {176B} parameter language model},
    year = {2024},
    issue_date = {January 2023},
    publisher = {JMLR.org},
    volume = {24},
    number = {1},
    issn = {1532-4435},
    abstract = {Progress in machine learning (ML) comes with a cost to the environment, given that training ML models requires computational resources, energy and materials. In the present article, we aim to quantify the carbon footprint of BLOOM, a 176-billion parameter language model, across its life cycle. We estimate that BLOOM's final training emitted approximately 24.7 tonnes of CO2eq if we consider only the dynamic power consumption, and 50.5 tonnes if we account for all processes ranging from equipment manufacturing to energy-based operational consumption. We also carry out an empirical study to measure the energy requirements and carbon emissions of its deployment for inference via an API endpoint receiving user queries in real-time. We conclude with a discussion regarding the difficulty of precisely estimating the carbon footprint of ML models and future research directions that can contribute towards improving carbon emissions reporting.},
    journal = {J. Mach. Learn. Res.},
    month = {Mar.},
    articleno = {253},
    numpages = {15},
    keywords = {carbon footprint, language modeling, life cycle assessment, machine learning}
    }
    

@INPROCEEDINGS{rahman2024,
  author={Rahman, Shanto and Baz, Abdelrahman and Misailovic, Sasa and Shi, August},
  booktitle={2024 IEEE Conference on Software Testing, Verification and Validation (ICST)}, 
  title={Quantizing Large-Language Models for Predicting Flaky Tests}, 
  year={2024},
  volume={},
  number={},
  pages={93-104},
  keywords={Training;Software testing;Quantization (signal);Codes;Computational modeling;Source coding;Artificial neural networks;Flaky Test Categorization;Large-Language Models;Quantization},
  doi={10.1109/ICST60714.2024.00018}}
    
    @INPROCEEDINGS{JV09,
      author={Juristo, Natalia and Vegas, Sira},
      booktitle={2009 3rd International Symposium on Empirical Software Engineering and Measurement}, 
      title={Using differences among replications of software engineering experiments to gain knowledge}, 
      year={2009},
      volume={},
      number={},
      pages={356-366},
      keywords={Software engineering;Packaging;Software measurement;Programming;Gain measurement;Physics;Chemistry;Humans},
      doi={10.1109/ESEM.2009.5314236}}
    
    @INPROCEEDINGS{CMFB24,
      author={Castaño, Joel and Martínez-Fernández, Silverio and Franch, Xavier and Bogner, Justus},
      booktitle={2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR)}, 
      title={Analyzing the Evolution and Maintenance of ML Models on Hugging Face}, 
      year={2024},
      volume={},
      number={},
      pages={607-618},
      keywords={Analytical models;Adaptation models;Technological innovation;Biological system modeling;Hafnium;Predictive models;Market research;repository mining;software evolution;maintenance},
      doi={}}

    @inproceedings{Bromley1993,
author = {Bromley, Jane and Guyon, Isabelle and LeCun, Yann and S\"{a}ckinger, Eduard and Shah, Roopak},
title = {Signature verification using a "Siamese" time delay neural network},
year = {1993},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This paper describes an algorithm for verification of signatures written on a pen-input tablet. The algorithm is based on a novel, artificial neural network, called a "Siamese" neural network. This network consists of two identical sub-networks joined at their outputs. During training the two sub-networks extract features from two signatures, while the joining neuron measures the distance between the two feature vectors. Verification consists of comparing an extracted feature vector with a stored feature vector for the signer. Signatures closer to this stored representation than a chosen threshold are accepted, all other signatures are rejected as forgeries.},
booktitle = {Proceedings of the 6th International Conference on Neural Information Processing Systems},
pages = {737–744},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'93}
}


    @article{Chawla2002,
author = {Chawla, Nitesh and Bowyer, Kevin and Hall, Lawrence and Kegelmeyer, W.},
year = {2002},
month = {06},
pages = {321-357},
title = {SMOTE: Synthetic Minority Over-sampling Technique},
volume = {16},
journal = {J. Artif. Intell. Res. (JAIR)},
doi = {10.1613/jair.953}
}
    @ARTICLE{WHC+24,
      author={Wang, Junjie and Huang, Yuchao and Chen, Chunyang and Liu, Zhe and Wang, Song and Wang, Qing},
      journal={IEEE Transactions on Software Engineering}, 
      title={Software Testing With Large Language Models: Survey, Landscape, and Vision}, 
      year={2024},
      volume={50},
      number={4},
      pages={911-936},
      keywords={Software testing;Task analysis;Computational modeling;Codes;Software systems;Natural language processing;Reviews;Pre-trained large language model;software testing;LLM;GPT},
      doi={10.1109/TSE.2024.3368208}}
    
    @inproceedings{Har24,
    author = {Harman, Mark},
    title = {The Role of Software Measurement in Assured {LLM}-Based {Software Engineering}},
    year = {2024},
    isbn = {9798400717017},
    publisher = {Association for Computing Machinery},
    doi = {10.1145/3661167.3661269},
    abstract = {Assured Large Language Model Software Engineering (Assured LLMSE) addresses the twin challenges: 1. Ensuring LLM-generated code does not regress the properties of the original code 2. Quantifying the improvement over the original archived by the improve code in a verifiable and measurable way. In so doing, the Assured LLMSE approach tackles the problem of LLMs’ tendency to hallucinate, as well as providing confidence that generated code improves an existing code base. Software testing and measurement play critical roles in this improvement process: testing is the guard against regression, while measurement provides the quantifiable assurance of improvement. Assured LLMSE takes its inspiration from previous work on genetic improvement, for which software measurement also plays a central role. In this keynote we outline the Assured LLMSE approach, highlighting the role of software measurement in the provision of quantifiable, verifiable assurances for code that originates from LLM–based inference. This paper is an outline of the content of the keynote by Mark Harman at the 28th International Conference on Evaluation and Assessment in Software Engineering. This is joint work with Nadia Alshahwan, Andrea Aquino, Jubin Chheda, Anastasia Finegenova, Inna Harper, Mitya Lyubarskiy, Neil Maiden, Alexander Mols, Shubho Sengupta, Rotem Tal, Alexandru Marginean, and Eddy Wang.},
    booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
    pages = {4},
    numpages = {1},
    keywords = {Automated Code Generation, CodeLlama, Genetic Improvement (GI), Large Language Models (LLMs), Llama, Search Based Software Engineering (SBSE)},
    location = {Salerno, Italy},
    series = {EASE '24}
    }
    
    @ARTICLE{SNET24,
      author={Schäfer, Max and Nadi, Sarah and Eghbali, Aryaz and Tip, Frank},
      journal={IEEE Transactions on Software Engineering}, 
      title={An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation}, 
      year={2024},
      volume={50},
      number={1},
      pages={85-105},
      keywords={Training;Test pattern generators;Documentation;Codes;Source coding;Software;Electronic mail;Test generation;JavaScript;language models},
      doi={10.1109/TSE.2023.3334955}}
    
    @inproceedings{ZMM06,
    author = {Zannier, Carmen and Melnik, Grigori and Maurer, Frank},
    title = {On the success of empirical studies in the international conference on software engineering},
    year = {2006},
    isbn = {1595933751},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    doi = {10.1145/1134285.1134333},
    abstract = {Critiques of the quantity and quality of empirical evaluations in software engineering have existed for quite some time. However such critiques are typically not empirically evaluated. This paper fills this gap by empirically analyzing papers published by ICSE, the prime research conference on Software Engineering. We present quantitative and qualitative results of a quasi-random experiment of empirical evaluations over the lifetime of the conference. Our quantitative results show the quantity of empirical evaluation has increased over 29 ICSE proceedings but we still have room to improve the soundness of empirical evaluations in ICSE proceedings. Our qualitative results point to specific areas of improvement in empirical evaluations.},
    booktitle = {Proceedings of the 28th International Conference on Software Engineering},
    pages = {341–350},
    numpages = {10},
    keywords = {empirical evaluation},
    location = {Shanghai, China},
    series = {ICSE '06}
    }
    
    
    
    @ARTICLE {Fatima2023,
    author = {S. Fatima and T. A. Ghaleb and L. Briand},
    journal = {IEEE Transactions on Software Engineering},
    title = {Flakify: A Black-Box, Language Model-Based Predictor for Flaky Tests},
    year = {2023},
    volume = {49},
    number = {04},
    issn = {1939-3520},
    pages = {1912-1927},
    abstract = {Software testing assures that code changes do not adversely affect existing functionality. However, a test case can be flaky, i.e., passing and failing across executions, even for the same version of the source code. Flaky test cases introduce overhead to software development as they can lead to unnecessary attempts to debug production or testing code. Besides rerunning test cases multiple times, which is time-consuming and computationally expensive, flaky test cases can be predicted using machine learning (ML) models, thus reducing the wasted cost of re-running and debugging these test cases. However, the state-of-the-art ML-based flaky test case predictors rely on pre-defined sets of features that are either project-specific, i.e., inapplicable to other projects, or require access to production code, which is not always available to software test engineers. Moreover, given the non-deterministic behavior of flaky test cases, it can be challenging to determine a complete set of features that could potentially be associated with test flakiness. Therefore, in this article, we propose Flakify, a black-box, language model-based predictor for flaky test cases. Flakify relies exclusively on the source code of test cases, thus not requiring to (a) access to production code (black-box), (b) rerun test cases, (c) pre-define features. To this end, we employed CodeBERT, a pre-trained language model, and fine-tuned it to predict flaky test cases using the source code of test cases. We evaluated Flakify on two publicly available datasets (FlakeFlagger and IDoFT) for flaky test cases and compared our technique with the FlakeFlagger approach, the best state-of-the-art ML-based, white-box predictor for flaky test cases, using two different evaluation procedures: (1) cross-validation and (2) per-project validation, i.e., prediction on new projects. Flakify achieved F1-scores of 79% and 73% on the FlakeFlagger dataset using cross-validation and per-project validation, respectively. Similarly, Flakify achieved F1-scores of 98% and 89% on the IDoFT dataset using the two validation procedures, respectively. Further, Flakify surpassed FlakeFlagger by 10 and 18 percentage points (pp) in terms of precision and recall, respectively, when evaluated on the FlakeFlagger dataset, thus reducing the cost bound to be wasted on unnecessarily debugging test cases and production code by the same percentages (corresponding to reduction rates of 25% and 64%). Flakify also achieved significantly higher prediction results when used to predict test cases on new projects, suggesting better generalizability over FlakeFlagger. Our results further show that a black-box version of FlakeFlagger is not a viable option for predicting flaky test cases.},
    keywords = {codes;predictive models;production;computational modeling;software testing;software;feature extraction},
    doi = {10.1109/TSE.2022.3201209},
    publisher = {IEEE Computer Society},
    address = {Los Alamitos, CA, USA},
    month = {apr}
    }
    
    @INPROCEEDINGS{Alshammari21,
  author={Alshammari, Abdulrahman and Morris, Christopher and Hilton, Michael and Bell, Jonathan},
  booktitle={2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)}, 
  title={FlakeFlagger: Predicting Flakiness Without Rerunning Tests}, 
  year={2021},
  volume={},
  number={},
  pages={1572-1584},
  abstract={When developers make changes to their code, they typically run regression tests to detect if their recent changes (re) introduce any bugs. However, many tests are flaky, and their outcomes can change non-deterministically, failing without apparent cause. Flaky tests are a significant nuisance in the development process, since they make it more difficult for developers to trust the outcome of their tests, and hence, it is important to know which tests are flaky. The traditional approach to identify flaky tests is to rerun them multiple times: if a test is observed both passing and failing on the same code, it is definitely flaky. We conducted a very large empirical study looking for flaky tests by rerunning the test suites of 24 projects 10,000 times each, and found that even with this many reruns, some previously identified flaky tests were still not detected. We propose FlakeFlagger, a novel approach that collects a set of features describing the behavior of each test, and then predicts tests that are likely to be flaky based on similar behavioral features. We found that FlakeFlagger correctly labeled as flaky at least as many tests as a state-of-the-art flaky test classifier, but that FlakeFlagger reported far fewer false positives. This lower false positive rate translates directly to saved time for researchers and developers who use the classification result to guide more expensive flaky test detection processes. Evaluated on our dataset of 23 projects with flaky tests, FlakeFlagger outperformed the prior approach (by F1 score) on 16 projects and tied on 4 projects. Our results indicate that this approach can be effective for identifying likely flaky tests prior to running time-consuming flaky test detectors.},
  keywords={Computer bugs;Detectors;Testing;Software engineering;regression testing;flaky tests;reliability},
  doi={10.1109/ICSE43902.2021.00140},
  ISSN={1558-1225},
  month={May},}

    @inproceedings{SDP24,
    author = {Sallou, June and Durieux, Thomas and Panichella, Annibale},
    title = {Breaking the Silence: the Threats of Using LLMs in Software Engineering},
    year = {2024},
    isbn = {9798400705007},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3639476.3639764},
    doi = {10.1145/3639476.3639764},
    abstract = {Large Language Models (LLMs) have gained considerable traction within the Software Engineering (SE) community, impacting various SE tasks from code completion to test generation, from program repair to code summarization. Despite their promise, researchers must still be careful as numerous intricate factors can influence the outcomes of experiments involving LLMs. This paper initiates an open discussion on potential threats to the validity of LLM-based research including issues such as closed-source models, possible data leakage between LLM training data and research evaluation, and the reproducibility of LLM-based findings. In response, this paper proposes a set of guidelines tailored for SE researchers and Language Model (LM) providers to mitigate these concerns. The implications of the guidelines are illustrated using existing good practices followed by LLM providers and a practical example for SE researchers in the context of test case generation.},
    booktitle = {Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
    pages = {102–106},
    numpages = {5},
    location = {Lisbon, Portugal},
    series = {ICSE-NIER'24}
    }
    
    
    
  @ARTICLE{b2,
  author={Kang, Sungmin and Yoon, Juyeon and Askarbekkyzy, Nargiz and Yoo, Shin},
  journal={IEEE Transactions on Software Engineering}, 
  title={Evaluating Diverse Large Language Models for Automatic and General Bug Reproduction}, 
  year={2024},
  volume={50},
  number={10},
  pages={2677-2694},
  abstract={Bug reproduction is a critical developer activity that is also challenging to automate, as bug reports are often in natural language and thus can be difficult to transform to test cases consistently. As a result, existing techniques mostly focused on crash bugs, which are easier to automatically detect and verify. In this work, we overcome this limitation by using large language models (LLMs), which have been demonstrated to be adept at natural language processing and code generation. By prompting LLMs to generate bug-reproducing tests, and via a post-processing pipeline to automatically identify promising generated tests, our proposed technique Libro could successfully reproduce about one-third of all bugs in the widely used Defects4J benchmark. Furthermore, our extensive evaluation on 15 LLMs, including 11 open-source LLMs, suggests that open-source LLMs also demonstrate substantial potential, with the StarCoder LLM achieving 70% of the reproduction performance of the closed-source OpenAI LLM code-davinci-002 on the large Defects4J benchmark, and 90% of performance on a held-out bug dataset likely not part of any LLM's training data. In addition, our experiments on LLMs of different sizes show that bug reproduction using Libro improves as LLM size increases, providing information as to which LLMs can be used with the Libro pipeline.},
  keywords={Computer bugs;Codes;Pipelines;Large language models;Debugging;Java;Computational modeling;Test generation;natural language processing;software engineering},
  doi={10.1109/TSE.2024.3450837},
  ISSN={1939-3520},
  month={Oct},}

    @article{bano2023LLMsoftware,
    author = {Bano, Muneera and Hoda, Rashina and Zowghi, Didar and Treude, Christoph},
    title = {Large language models for qualitative research in software engineering: exploring opportunities and challenges},
    year = {2023},
    issue_date = {May 2024},
    publisher = {Kluwer Academic Publishers},
    address = {USA},
    volume = {31},
    number = {1},
    issn = {0928-8910},
    doi = {10.1007/s10515-023-00407-8},
    abstract = {The recent surge in the integration of Large Language Models (LLMs) like ChatGPT into qualitative research in software engineering, much like in other professional domains, demands a closer inspection. This vision paper seeks to explore the opportunities of using LLMs in qualitative research to address many of its legacy challenges as well as potential new concerns and pitfalls arising from the use of LLMs. We share our vision for the evolving role of the qualitative researcher in the age of LLMs and contemplate how they may utilize LLMs at various stages of their research experience.},
    journal = {Automated Software Engg.},
    month = {dec},
    numpages = {12},
    keywords = {Software engineering, Qualitative research, LLMs, Large language models}
    }

@inproceedings{riddhi2025,
  author    = {Riddhi More and Jeremy S. Bradbury},
  title     = {Assessing Data Augmentation-Induced Bias in Training and Testing of Machine Learning Models},
  booktitle = {Proceedings of the 1st International Workshop on
Fairness in Software Systems (FAIRNESS 2025)},
  month     = {Mar.},
  year      = {2025},
  location   = {Montreal, PQ, Canada}
}

    
    @Article{pan2021CodeBERTSurvey,
    AUTHOR = {Pan, Cong and Lu, Minyan and Xu, Biao},
    TITLE = {{An Empirical Study on Software Defect Prediction Using CodeBERT Model}},
    JOURNAL = {Applied Sciences},
    VOLUME = {11},
    YEAR = {2021},
    NUMBER = {11},
    ARTICLE-NUMBER = {4793},
    ISSN = {2076-3417},
    ABSTRACT = {Deep learning-based software defect prediction has been popular these days. Recently, the publishing of the CodeBERT model has made it possible to perform many software engineering tasks. We propose various CodeBERT models targeting software defect prediction, including CodeBERT-NT, CodeBERT-PS, CodeBERT-PK, and CodeBERT-PT. We perform empirical studies using such models in cross-version and cross-project software defect prediction to investigate if using a neural language model like CodeBERT could improve prediction performance. We also investigate the effects of different prediction patterns in software defect prediction using CodeBERT models. The empirical results are further discussed.},
    DOI = {10.3390/app11114793}
    }
    
    
@article{Wang2023SurveyLLMSoftware,
author = {Wang, Junjie and Huang, Yuchao and Chen, Chunyang and Liu, Zhe and Wang, Song and Wang, Qing},
title = {Software Testing With Large Language Models: Survey, Landscape, and Vision},
year = {2024},
issue_date = {April 2024},
publisher = {IEEE Press},
volume = {50},
number = {4},
issn = {0098-5589},
doi = {10.1109/TSE.2024.3368208},
abstract = {Pre-trained large language models (LLMs) have recently emerged as a breakthrough technology in natural language processing and artificial intelligence, with the ability to handle large-scale datasets and exhibit remarkable performance across a wide range of tasks. Meanwhile, software testing is a crucial undertaking that serves as a cornerstone for ensuring the quality and reliability of software products. As the scope and complexity of software systems continue to grow, the need for more effective software testing techniques becomes increasingly urgent, making it an area ripe for innovative approaches such as the use of LLMs. This paper provides a comprehensive review of the utilization of LLMs in software testing. It analyzes 102 relevant studies that have used LLMs for software testing, from both the software testing and LLMs perspectives. The paper presents a detailed discussion of the software testing tasks for which LLMs are commonly used, among which test case preparation and program repair are the most representative. It also analyzes the commonly used LLMs, the types of prompt engineering that are employed, as well as the accompanied techniques with these LLMs. It also summarizes the key challenges and potential opportunities in this direction. This work can serve as a roadmap for future research in this area, highlighting potential avenues for exploration, and identifying gaps in our current understanding of the use of LLMs in software testing.},
journal = {IEEE Trans. Softw. Eng.},
month = apr,
pages = {911–936},
numpages = {26}
}
    
    @article{Ott19,
      title={Exploring the applicability of low-shot learning in mining software repositories},
      author={Ott, Jordan and Atchison, Abigail and Linstead, Erik J},
      journal={Journal of Big Data},
      volume={6},
      number={1},
      pages={1--10},
      year={2019},
      publisher={SpringerOpen},
    DOI = {https://doi.org/10.1186/s40537-019-0198-z}
    }
    
    
    @misc{Khaj2023,
          title={Evaluating few shot and Contrastive learning Methods for Code Clone Detection}, 
          author={Mohamad Khajezade and Fatemeh Hendijani Fard and Mohamed S. Shehata},
          year={2023},
          eprint={2204.07501},
          archivePrefix={arXiv},
          primaryClass={cs.SE}
    }
@Article{Khaj2023,
author={Khajezade, Mohamad
and Fard, Fatemeh H.
and Shehata, Mohamed S.},
title={Evaluating few-shot and contrastive learning methods for code clone detection},
journal={Empirical Software Engineering},
year={2024},
month={Oct},
day={09},
volume={29},
number={6},
pages={163},
abstract={Code Clone Detection (CCD) is a software engineering task that is used for plagiarism detection, code search, and code comprehension. Recently, deep learning-based models have achieved an F1-Score (a metric used to assess classifiers) of {\$}{\$}{\backslash}sim {\$}{\$}95{\%} on the CodeXGLUE benchmark. These models require many training data, mainly fine-tuned on Java or C++ datasets. However, no previous study evaluates the generalizability of these models where a limited amount of annotated data is available.},
issn={1573-7616},
doi={10.1007/s10664-024-10441-z},
url={https://doi.org/10.1007/s10664-024-10441-z}
}


    
    @inproceedings{Colavito2023FSL,
      title={Few-Shot Learning for Issue Report Classification},
      author={Colavito, Giuseppe and Lanubile, Filippo and Novielli, Nicole},
      booktitle={2023 IEEE/ACM 2nd International Workshop on Natural Language-Based Software Engineering (NLBSE)},
      pages={16--19},
      year={2023},
      organization={IEEE}
    }
    
    @inproceedings{Bertram2022FSL,
      title={Neural Language Models and Few Shot Learning for Systematic Requirements Processing in MDSE},
      author={Bertram, Vincent and Bo{\ss}, Miriam and Kusmenko, Evgeny and Nachmann, Imke Helene and Rumpe, Bernhard and Trotta, Danilo and Wachtmeister, Louis},
      booktitle={Proceedings of the 15th ACM SIGPLAN International Conference on Software Language Engineering},
      pages={260--265},
      year={2022}
    }
    
    @article{Liu2023SDP,
      title={Semantic feature learning for software defect prediction from source code and external knowledge},
      author={Liu, Jingyu and Ai, Jun and Lu, Minyan and Wang, Jie and Shi, Haoxiang},
      journal={Journal of Systems and Software},
      pages={111753},
      year={2023},
      publisher={Elsevier}
    }
    
    @article{Brown2020FSL,
      title={Language models are few-shot learners},
      author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie, and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda, and others},
      journal={Advances in neural information processing systems},
      volume={33},
      pages={1877--1901},
      year={2020}
    }
    
    @article{Xu2022Pretrain,
      title={A Survey on Pretrained Language Models for Neural Code Intelligence},
      author={Xu, Yichen and Zhu, Yanqiao},
      journal={arXiv preprint arXiv:2212.10079},
      year={2022}
    }
    
    @inproceedings{BuiCodeT5,
        title = "Detect-Localize-Repair: A Unified Framework for Learning to Debug with {C}ode{T}5",
        author = "Bui, Nghi  and
          Wang, Yue  and
          Hoi, Steven C.H.",
        editor = "Goldberg, Yoav  and
          Kozareva, Zornitsa  and
          Zhang, Yue",
        booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
        month = dec,
        year = "2022",
        address = "Abu Dhabi, United Arab Emirates",
        publisher = "Association for Computational Linguistics",
        url = "https://aclanthology.org/2022.findings-emnlp.57",
        doi = "10.18653/v1/2022.findings-emnlp.57",
        pages = "812--823",
        abstract = "Automated software debugging is a crucial task for improving the productivity of software developers. Many neural-based techniques have been proven effective for debugging-related tasks such as bug localization and program repair (or bug fixing). However, these techniques often focus only on either one of them or approach them in a stage-wise manner, ignoring the mutual benefits between them. In this work, we propose a novel unified Detect-Localize-Repair framework based on a pretrained programming language model CodeT5 to seamlessly address these tasks, named CodeT5-DLR. Specifically, we propose three objectives to adapt the generic CodeT5 for debugging: a bug detection objective to determine whether a given code snippet is buggy or not, a bug localization objective to identify the buggy lines, and a program repair objective to translate the buggy code to its fixed version. We evaluate it on each of these tasks and their combined setting on two newly collected line-level debugging datasets in Java and Python. Extensive results show that our model significantly outperforms existing baselines from both NLP and software engineering domains.",
    }
    @inbook{Zhou2019,
author = {Zhou, Yaqin and Liu, Shangqing and Siow, Jingkai and Du, Xiaoning and Liu, Yang},
title = {Devign: effective vulnerability identification by learning comprehensive program semantics via graph neural networks},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Vulnerability identification is crucial to protect the software systems from attacks for cyber security. It is especially important to localize the vulnerable functions among the source code to facilitate the fix. However, it is a challenging and tedious process, and also requires specialized security expertise. Inspired by the work on manually-defined patterns of vulnerabilities from various code representation graphs and the recent advance on graph neural networks, we propose Devign, a general graph neural network based model for graph-level classification through learning on a rich set of code semantic representations. It includes a novel Conv module to efficiently extract useful features in the learned rich node representations for graph-level classification. The model is trained over manually labeled datasets built on 4 diversified large-scale open-source C projects that incorporate high complexity and variety of real source code instead of synthesis code used in previous works. The results of the extensive evaluation on the datasets demonstrate that Devign outperforms the state of the arts significantly with an average of 10.51\% higher accuracy and 8.68\% F1 score, increases averagely 4.66\% accuracy and 6.37\% F1 by the Conv module.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {915},
numpages = {11}
}

    @inproceedings{Siddiq2024LLMSoftware,
author = {Siddiq, Mohammed Latif and Da Silva Santos, Joanna Cecilia and Tanvir, Ridwanul Hasan and Ulfat, Noshin and Al Rifat, Fahmid and Carvalho Lopes, Vin\'{\i}cius},
title = {{Using Large Language Models to Generate JUnit Tests: An Empirical Study}},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3661167.3661216},
abstract = {A code generation model generates code by taking a prompt from a code comment, existing code, or a combination of both. Although code generation models (e.g., GitHub Copilot) are increasingly being adopted in practice, it is unclear whether they can successfully be used for unit test generation without fine-tuning for a strongly typed language like Java. To fill this gap, we investigated how well three models (Codex, GPT-3.5-Turbo, and StarCoder) can generate unit tests. We used two benchmarks (HumanEval and Evosuite SF110) to investigate the effect of context generation on the unit test generation process. We evaluated the models based on compilation rates, test correctness, test coverage, and test smells. We found that the Codex model achieved above 80\% coverage for the HumanEval dataset, but no model had more than 2\% coverage for the EvoSuite SF110 benchmark. The generated tests also suffered from test smells, such as Duplicated Asserts and Empty Tests.},
booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
pages = {313–322},
numpages = {10},
keywords = {junit, large language models, test generation, test smells, unit testing},
location = {Salerno, Italy},
series = {EASE '24}
}


@inproceedings{Wei2022,
title = "{Preempting Flaky Tests via Non-Idempotent-Outcome Tests}",
abstract = "Regression testing can greatly help in software development, but it can be seriously undermined by flaky tests, which can both pass and fail, seemingly nondeterministically, on the same code commit. Flaky tests are an emerging topic in both research and industry. Prior work has identified multiple categories of flaky tests, developed techniques for detecting these flaky tests, and analyzed some detected flaky tests. To proactively detect, i.e., preempt, flaky tests, we propose to detect non-idempotent-outcome (NIO) tests, a novel category related to flaky tests. In particular, we run each test twice in the same test execution environment, e.g., run each Java test twice in the same Java Virtual Machine. A test is NIO if it passes in the first run but fails in the second. Each NIO test has side effects and 'self-pollutes' the state shared among test runs. We perform experiments on both Java and Python open-source projects, detecting 223 NIO Java tests and 138 NIO Python tests. We have inspected all 361 detected tests and opened pull requests that fix 268 tests, with 192 already accepted, only 6 rejected, and the remaining 70 pending.",
author = "Anjiang Wei and Pu Yi and Zhengxi Li and Tao Xie and Darko Marinov and Wing Lam",
year = "2022",
doi = "10.1145/3510003.3510170",
language = "English (US)",
series = "Proceedings - International Conference on Software Engineering",
publisher = "IEEE Computer Society",
pages = "1730--1742",
booktitle = "Proceedings - 2022 ACM/IEEE 44th International Conference on Software Engineering, ICSE 2022",
}
@INPROCEEDINGS{LOS+19,
  author={Lam, Wing and Oei, Reed and Shi, August and Marinov, Darko and Xie, Tao},
  booktitle={2019 12th IEEE Conference on Software Testing, Validation and Verification (ICST) }, 
  title={{iDFlakies: A Framework for Detecting and Partially Classifying Flaky Tests}}, 
  year={2019},
  volume={},
  number={},
  pages={312-322},
  abstract={Regression testing is increasingly important with the wide use of continuous integration. A desirable requirement for regression testing is that a test failure reliably indicates a problem in the code under test and not a false alarm from the test code or the testing infrastructure. However, some test failures are unreliable, stemming from flaky tests that can nondeterministically pass or fail for the same code under test. There are many types of flaky tests, with order-dependent tests being a prominent type. To help advance research on flaky tests, we present (1) a framework, iDFlakies, to detect and partially classify flaky tests; (2) a dataset of flaky tests in open-source projects; and (3) a study with our dataset. iDFlakies automates experimentation with our tool for Maven-based Java projects. Using iDFlakies, we build a dataset of 422 flaky tests, with 50.5% order-dependent and 49.5% not. Our study of these flaky tests finds the prevalence of two types of flaky tests, probability of a test-suite run to have at least one failure due to flaky tests, and how different test reorderings affect the number of detected flaky tests. We envision that our work can spur research to alleviate the problem of flaky tests.},
  keywords={Tools;Testing;Java;Open source software;Reliability;Concurrent computing;Servers;regression testing;flaky tests;order dependent tests},
  doi={10.1109/ICST.2019.00038},
  ISSN={2159-4848},
  month={April},}


@inproceedings{Luo2014,
author = {Luo, Qingzhou and Hariri, Farah and Eloussi, Lamyaa and Marinov, Darko},
title = {{An Empirical Analysis of Flaky Tests}},
year = {2014},
isbn = {9781450330565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/2635868.2635920},
abstract = {Regression testing is a crucial part of software development. It checks that software changes do not break existing functionality. An important assumption of regression testing is that test outcomes are deterministic: an unmodified test is expected to either always pass or always fail for the same code under test. Unfortunately, in practice, some tests often called flaky tests—have non-deterministic outcomes. Such tests undermine the regression testing as they make it difficult to rely on test results. We present the first extensive study of flaky tests. We study in detail a total of 201 commits that likely fix flaky tests in 51 open-source projects. We classify the most common root causes of flaky tests, identify approaches that could manifest flaky behavior, and describe common strategies that developers use to fix flaky tests. We believe that our insights and implications can help guide future research on the important topic of (avoiding) flaky tests.},
booktitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {643–653},
numpages = {11},
keywords = {non-determinism, flaky tests, Empirical study},
location = {Hong Kong, China},
series = {FSE 2014}
}


@article{barbosa2023,
author = {Barbosa, Keila and Ferreira, Ronivaldo and Pinto, Gustavo and d'Amorim, Marcelo and Miranda, Breno},
title = {Test Flakiness Across Programming Languages},
year = {2023},
issue_date = {April 2023},
publisher = {IEEE Press},
volume = {49},
number = {4},
issn = {0098-5589},
url = {https://doi.org/10.1109/TSE.2022.3208864},
doi = {10.1109/TSE.2022.3208864},
abstract = {Regression Testing (RT) is a quality-assurance practice commonly adopted in the software industry to check if functionality remains intact after code changes. Test flakiness is a serious problem for RT. A test is said to be flaky when it non-deterministically passes or fails on a fixed environment. Prior work studied test flakiness primarily on Java programs. It is unclear, however, how problematic is test flakiness for software written in other programming languages. This paper reports on a study focusing on three central aspects of test flakiness: concentration, similarity, and cost. Considering concentration, our results show that, for any given programming language that we studied (C, Go, Java, JS, and Python), most issues could be explained by a small fraction of root causes (5/13 root causes cover 78.07% of the issues) and could be fixed by a relatively small fraction of fix strategies (10/23 fix strategies cover 85.20% of the issues). Considering similarity, although there were commonalities in root causes and fixes across languages (e.g., concurrency and async wait are common causes of flakiness in most languages), we also found important differences (e.g., flakiness due to improper release of resources are more common in C), suggesting that there is opportunity to fine tuning analysis tools. Considering cost, we found that issues related to flaky tests are resolved either very early once they are posted (<inline-formula><tex-math notation="LaTeX">$< $</tex-math><alternatives><mml:math><mml:mo><</mml:mo></mml:math><inline-graphic xlink:href="miranda-ieq1-3208864.gif"/></alternatives></inline-formula>10 days), suggesting relevance, or very late (<inline-formula><tex-math notation="LaTeX">$>$</tex-math><alternatives><mml:math><mml:mo>></mml:mo></mml:math><inline-graphic xlink:href="miranda-ieq2-3208864.gif"/></alternatives></inline-formula>100 days), suggesting irrelevance.},
journal = {IEEE Trans. Softw. Eng.},
month = {apr},
pages = {2039–2052},
numpages = {14}
}


@inproceedings{Shi2019,
author = {Shi, August and Lam, Wing and Oei, Reed and Xie, Tao and Marinov, Darko},
title = {{iFixFlakies
          }: a framework for automatically fixing order-dependent flaky tests},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3338906.3338925},
abstract = {Regression testing provides important pass or fail signals that developers use to make decisions after code changes. However, flaky tests, which pass or fail even when the code has not changed, can mislead developers. A common kind of flaky tests are order-dependent tests, which pass or fail depending on the order in which the tests are run. Fixing order-dependent tests is often tedious and time-consuming.  We propose iFixFlakies, a framework for automatically fixing order-dependent tests. The key insight in iFixFlakies is that test suites often already have tests, which we call helpers, whose logic resets or sets the states for order-dependent tests to pass. iFixFlakies searches a test suite for helpers that make the order-dependent tests pass and then recommends patches for the order-dependent tests using code from these helpers. Our evaluation on 110 truly orderdependent tests from a public dataset shows that 58 of them have helpers, and iFixFlakies can fix all 58. We opened pull requests for 56 order-dependent tests (2 of 58 were already fixed), and developers have already accepted pull requests for 21 of them, with all the remaining ones still pending.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {545–555},
numpages = {11},
keywords = {patch generation, order-dependent test, flaky test, automated fixing},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}
    @inproceedings{Akli2023,
      title={Flakycat: predicting flaky tests categories using few-shot learning},
      author={Akli, Amal and Haben, Guillaume and Habchi, Sarra and Papadakis, Mike and Le Traon, Yves},
      booktitle={2023 IEEE/ACM International Conference on Automation of Software Test (AST)},
      pages={140--151},
      year={2023},
      organization={IEEE}
    }
    
    @article{Wang2020,
      title={Generalizing from a few examples: A survey on few-shot learning},
      author={Wang, Yaqing and Yao, Quanming and Kwok, James T and Ni, Lionel M},
      journal={ACM computing surveys (csur)},
      volume={53},
      number={3},
      pages={1--34},
      year={2020},
      publisher={ACM New York, NY, USA}
    }
    
    @inproceedings{Guo2020CodeBERT,
        title = "{C}ode{BERT}: {A Pre-Trained Model for Programming and Natural Languages}",
        author = "Feng, Zhangyin  and
          Guo, Daya  and
          Tang, Duyu  and
          Duan, Nan  and
          Feng, Xiaocheng  and
          Gong, Ming  and
          Shou, Linjun  and
          Qin, Bing  and
          Liu, Ting  and
          Jiang, Daxin  and
          Zhou, Ming",
        editor = "Cohn, Trevor  and
          He, Yulan  and
          Liu, Yang",
        booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
        month = Nov,
        year = "2020",
        address = "Online",
        publisher = "Association for Computational Linguistics",
        doi = "10.18653/v1/2020.findings-emnlp.139",
        pages = "1536--1547",
        abstract = "We present CodeBERT, a bimodal pre-trained model for programming language (PL) and natural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as natural language code search, code documentation generation, etc. We develop CodeBERT with Transformer-based neural architecture, and train it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables us to utilize both {``}bimodal{''} data of NL-PL pairs and {``}unimodal data, where the former provides input tokens for model training while the latter helps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NLPL probing.",
    }
    
    @inproceedings{Wang2021CodeT5,
        title = "{C}ode{T}5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation",
        author = "Wang, Yue  and
          Wang, Weishi  and
          Joty, Shafiq  and
          Hoi, Steven C.H.",
        editor = "Moens, Marie-Francine  and
          Huang, Xuanjing  and
          Specia, Lucia  and
          Yih, Scott Wen-tau",
        booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
        month = nov,
        year = "2021",
        address = "Online and Punta Cana, Dominican Republic",
        publisher = "Association for Computational Linguistics",
        url = "https://aclanthology.org/2021.emnlp-main.685",
        doi = "10.18653/v1/2021.emnlp-main.685",
        pages = "8696--8708",
        abstract = "Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at \url{https://github.com/salesforce/CodeT5}.",
    }
    
    @article{Chen2021LLpretrain,
      title={Evaluating large language models trained on code},
      author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
      journal={arXiv preprint arXiv:2107.03374},
      year={2021}
    }
    
    @misc{CodeT5,
    title={CodeTF},
    author={Salesforce},
    howpublished={GitHub},
    year={2024},
    url={https://github.com/salesforce/CodeTF},
    note={Retrieved January 31, 2024}
    }
    
    @article{Trev2023,
        author = {Treviso, Marcos and Lee, Ji-Ung and Ji, Tianchu and Aken, Betty van and Cao, Qingqing and Ciosici, Manuel R. and Hassid, Michael and Heafield, Kenneth and Hooker, Sara and Raffel, Colin and Martins, Pedro H. and Martins, André F. T. and Forde, Jessica Zosa and Milder, Peter and Simpson, Edwin and Slonim, Noam and Dodge, Jesse and Strubell, Emma and Balasubramanian, Niranjan and Derczynski, Leon and Gurevych, Iryna and Schwartz, Roy},
        title = "{Efficient Methods for Natural Language Processing: A Survey}",
        journal = {Transactions of the Association for Computational Linguistics},
        volume = {11},
        pages = {826-860},
        year = {2023},
        month = {07},
        abstract = "{Recent work in natural language processing (NLP) has yielded appealing results from scaling model parameters and training data; however, using only scale to improve performance means that resource consumption also grows. Such resources include data, time, storage, or energy, all of which are naturally limited and unevenly distributed. This motivates research into efficient methods that require fewer resources to achieve similar results. This survey synthesizes and relates current methods and findings in efficient NLP. We aim to provide both guidance for conducting NLP under limited resources, and point towards promising research directions for developing more efficient methods.}",
        issn = {2307-387X},
        doi = {10.1162/tacl_a_00577},
        url = {https://doi.org/10.1162/tacl\_a\_00577},
        eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00577/2143614/tacl\_a\_00577.pdf},
    }
    
    
    @INPROCEEDINGS{banane2022,
      author={Banane, Mouad and Erraissi, Allae},
      booktitle={2022 International Conference on Decision Aid Sciences and Applications (DASA)}, 
      title={A comprehensive study of Natural Language processing techniques Based on Big Data}, 
      year={2022},
      volume={},
      number={},
      pages={1492-1497},
      keywords={Correlation;Databases;Semantics;Big Data;Syntactics;Natural language processing;Real-time systems;Natural Language processing;Big Data;NLP;NoSQL},
      doi={10.1109/DASA54658.2022.9765270}}
    
    @InProceedings{Zhu2015,
        title = {Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books},
        author = {Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
        booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
        month = {December},
        year = {2015}
    }
    
    @article{Zhao2023FSL,
    title = {Test case classification via few-shot learning},
    journal = {Information and Software Technology},
    volume = {160},
    pages = {107228},
    year = {2023},
    issn = {0950-5849},
    doi = {https://doi.org/10.1016/j.infsof.2023.107228},
    url = {https://www.sciencedirect.com/science/article/pii/S0950584923000824},
    author = {Yuan Zhao and Sining Liu and Quanjun Zhang and Xiuting Ge and Jia Liu},
    keywords = {Test case classification, Few-shot learning, Attention mechanism},
    abstract = {Context:
    Crowdsourced testing can reduce testing costs and improve testing efficiency. However, crowdsourced testing generates massive test cases, requiring testers to select high-quality test cases for execution. Consequently, crowdsourced test cases require much effort to perform labeling due to the costly manual labor and domain knowledge.
    Objective:
    Existing methods usually fail to consider the crowdsourced testing scenarioâs inadequate and imbalanced data issues. We aim to effectively and efficiently classify many crowdsourced test cases for developers to alleviate manual efforts.
    Method:
    In this paper, we propose a test case classification approach based on few-shot learning and test case augmentation to address the limitations mentioned above. The proposed approach generates new test cases by the large pre-trained masked language model and extracts embedding representation by training word embedding models. Then a Bidirectional Long Short-Term Memory (BiLSTM)-based classifier is designed to perform test case classification by extracting the in-depth features. Besides, we also apply the attention mechanism to assign high weights to words that represent the test case category by lexicon matching.
    Results:
    To verify the effectiveness of the classification framework, we select 1659 test cases from three crowdsourced testing projects to conduct in-usability evaluation experiments. The experimental results show that the proposed approach has a higher accuracy and precision rate than existing classification methods.
    Conclusion:
    It can be concluded that (1) the proposed approach is an effective test case classification technique for crowdsourced testing; (2) the proposed approach is practical to help developers select high-quality test cases quickly and effectively.}
    }
    
    @article{Jacob2018BERT,
      author       = {Jacob Devlin and
                      Ming{-}Wei Chang and
                      Kenton Lee and
                      Kristina Toutanova},
      title        = {{BERT:} {Pre-training of Deep Bidirectional Transformers for Language}
                      Understanding},
      journal      = {CoRR},
      volume       = {abs/1810.04805},
      year         = {2018},
      eprinttype    = {arXiv},
      eprint       = {1810.04805},
      timestamp    = {Tue, 30 Oct 2018 20:39:56 +0100},
      biburl       = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
      bibsource    = {dblp computer science bibliography, https://dblp.org}
    }
@ARTICLE{IEEE1990,
  author={},
  journal={IEEE Std 610.12-1990}, 
  title={IEEE Standard Glossary of Software Engineering Terminology}, 
  year={1990},
  volume={},
  number={},
  pages={1-84},
  doi={10.1109/IEEESTD.1990.101064}
}

@inproceedings{Wang2020,
  author = {Wang, Yuqing and Mäntylä, Mika and Demeyer, Serge and Wiklund, Kristian and Eldh, Sigrid and Kairi, Tatu},
  year = {2020},
  month = {01},
  pages = {27-38},
  title = {Software Test Automation Maturity: A Survey of the State of the Practice},
  doi = {10.5220/0009766800270038}
}


@ARTICLE{Wang2024,
  author = {Junjie Wang and Yuchao Huang and Chunyang Chen and Zhe Liu and Song Wang and Qing Wang},
  journal = {IEEE Transactions on Software Engineering}, 
  title = {Software Testing With Large Language Models: Survey, Landscape, and Vision}, 
  year = {2024},
  volume = {50},
  number = {4},
  pages = {911-936},
  doi = {10.1109/TSE.2024.3368208}
}

@ARTICLE{Schäfer2024,
  author = {Max Schäfer and Sarah Nadi and Aryaz Eghbali and Frank Tip},
  journal = {IEEE Transactions on Software Engineering}, 
  title = {An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation}, 
  year = {2024},
  volume = {50},
  number = {1},
  pages = {85-105},
  doi = {10.1109/TSE.2023.3334955}
}




@article{Ott2019,
  author = {Jordan Ott and Abigail Atchison and Erik J. Linstead},
  title = {Exploring the applicability of low-shot learning in mining software repositories},
  journal = {Journal of Big Data},
  volume = {6},
  number = {1},
  pages = {1-10},
  year = {2019},
  doi = {10.1186/s40537-019-0198-z}
}

@inproceedings{Yildirim23,
author = {Yildirim, Savas and Cevik, Mucahit and Ba\c{s}ar, Ay\c{s}e},
title = {Few-shot Learning Approaches to Software Requirement Quality Prediction},
year = {2023},
abstract = {Computer-aided processes can be used to improve the quality of requirements in software projects. These processes, in principle, can be divided into two stages: requirement quality evaluation, essentially a classification task, and requirement correction sugges-tions, fundamentally a language generation problem. This study particularly focuses on the initial phase –requirement quality evalu-ation, for which we develop a machine learning infrastructure. Our approach aligns itself with the universally recognized assumption that ideal requirements should be uncomplicated, precise, atomic, and succinct. We have a two-step solution to evaluate the qual-ity of requirements. The first s tep e mploys f ew-shot techniques, addressing the issue of limited access to labeled data in practi-cal contexts. The second step involves adapting a language model with domain-specific data, given that language models trained on general-purpose texts might struggle to accurately capture patterns and meanings in specific d omains. O ur p ipeline p rimarily lever-ages contrastive learning objectives in both steps of our approach. Furthermore, we consider various robust baselines for compara-tive analysis. Numerical results illustrate that our quality detection model effectively discriminates between good quality and bad qual-ity requirements, thereby providing valuable input for subsequent requirement generation and suggestion models.},
booktitle = {Proceedings of the 33rd Annual International Conference on Computer Science and Software Engineering (CASCON '23)},
pages = {155–160},
numpages = {6},
keywords = {Few-shot Learning, Contrastive Learning, Domain Adaption, Software Requirement Specifications, Transformer Models},
location = {Las Vegas, NV, USA},
}

@inproceedings{Ahmed23,
author = {Ahmed, Toufique and Devanbu, Premkumar},
title = {Few-shot training {LLMs} for project-specific code-summarization},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
doi = {10.1145/3551349.3559555},
abstract = {Very large language models (LLMs), such as GPT-3 and Codex have achieved state-of-the-art performance on several natural-language tasks, and show great promise also for code. A particularly exciting aspect of LLMs is their knack for few-shot and zero-shot learning: they can learn to perform a task with very few examples. Few-shotting has particular synergies in software engineering, where there are a lot of phenomena (identifier names, APIs, terminology, coding patterns) that are known to be highly project-specific. However, project-specific data can be quite limited, especially early in the history of a project; thus the few-shot learning capacity of LLMs might be very relevant. In this paper, we investigate the use few-shot training with the very large GPT (Generative Pre-trained Transformer) Codex model, and find evidence suggesting that one can significantly surpass state-of-the-art models for code-summarization, leveraging project-specific training.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering (ASE '22)},
articleno = {177},
numpages = {5},
keywords = {code summarization, deep learning, large language model},
location = {Rochester, MI, USA},
}





@inproceedings{Chaaben23,
  author = {Meriem Ben Chaaben and Lola Burgueño and Houari Sahraoui},
  title = {Towards Using Few-Shot Prompt Learning for Automating Model Completion},
  booktitle = {Proceedings of the 45th International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER '23)},
  year = {2023},
  doi = {10.1109/ICSE-NIER58687.2023.00008}
}

    
    @article{raffel2020transferLearning,
    author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
    title = {Exploring the limits of transfer learning with a unified text-to-text transformer},
    year = {2020},
    issue_date = {January 2020},
    publisher = {JMLR.org},
    volume = {21},
    number = {1},
    issn = {1532-4435},
    abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pretraining objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
    journal = {J. Mach. Learn. Res.},
    month = {jan},
    articleno = {140},
    numpages = {67},
    keywords = {transfer learning, natural language processing, multi-task learning, attention based models, deep learning}
    }
@inproceedings{Vaswani2017Attention,
  title={Attention Is All You Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5998--6008},
  year={2017},
  publisher={Curran Associates, Inc.}
}
    @article{Parry2021,
author = {Parry, Owain and Kapfhammer, Gregory M. and Hilton, Michael and McMinn, Phil},
title = {A Survey of Flaky Tests},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1049-331X},
doi = {10.1145/3476105},
abstract = {Tests that fail inconsistently, without changes to the code under test, are described as flaky. Flaky tests do not give a clear indication of the presence of software bugs and thus limit the reliability of the test suites that contain them. A recent survey of software developers found that 59\% claimed to deal with flaky tests on a monthly, weekly, or daily basis. As well as being detrimental to developers, flaky tests have also been shown to limit the applicability of useful techniques in software testing research. In general, one can think of flaky tests as being a threat to the validity of any methodology that assumes the outcome of a test only depends on the source code it covers. In this article, we systematically survey the body of literature relevant to flaky test research, amounting to 76 papers. We split our analysis into four parts: addressing the causes of flaky tests, their costs and consequences, detection strategies, and approaches for their mitigation and repair. Our findings and their implications have consequences for how the software-testing community deals with test flakiness, pertinent to practitioners and of interest to those wanting to familiarize themselves with the research area.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {oct},
articleno = {17},
numpages = {74},
keywords = {Flaky tests, software testing}
}
    
    
    @misc{OpenAI2023,
          title={{GPT-4} Technical Report}, 
          author={OpenAI},
          year={2023},
          eprint={2303.08774},
          archivePrefix={arXiv},
          primaryClass={cs.CL}
    }
    
    @inproceedings{c4Corpus,
        title = "{C}4{C}orpus: Multilingual Web-size Corpus with Free License",
        author = "Habernal, Ivan  and
          Zayed, Omnia  and
          Gurevych, Iryna",
        editor = "Calzolari, Nicoletta  and
          Choukri, Khalid  and
          Declerck, Thierry  and
          Goggi, Sara  and
          Grobelnik, Marko  and
          Maegaard, Bente  and
          Mariani, Joseph  and
          Mazo, Helene  and
          Moreno, Asuncion  and
          Odijk, Jan  and
          Piperidis, Stelios",
        booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)",
        month = may,
        year = "2016",
        address = "Portoro{\v{z}}, Slovenia",
        publisher = "European Language Resources Association (ELRA)",
        url = "https://aclanthology.org/L16-1146",
        pages = "914--922",
        abstract = "Large Web corpora containing full documents with permissive licenses are crucial for many NLP tasks. In this article we present the construction of 12 million-pages Web corpus (over 10 billion tokens) licensed under CreativeCommons license family in 50+ languages that has been extracted from CommonCrawl, the largest publicly available general Web crawl to date with about 2 billion crawled URLs. Our highly-scalable Hadoop-based framework is able to process the full CommonCrawl corpus on 2000+ CPU cluster on the Amazon Elastic Map/Reduce infrastructure. The processing pipeline includes license identification, state-of-the-art boilerplate removal, exact duplicate and near-duplicate document removal, and language detection. The construction of the corpus is highly configurable and fully reproducible, and we provide both the framework (DKPro C4CorpusTools) and the resulting data (C4Corpus) to the research community.",
    }