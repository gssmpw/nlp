\section{Related Work}
\label{sec:Related Work}

Detecting and classifying flaky tests are critical for maintaining the reliability of software test suites. Various techniques have been developed to address these tasks, each with its strengths and limitations.
\vspace{2mm}
\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.9\textwidth]{FXBModel.png} % Adjust the image width as needed
    \caption{Architecture of FlakyXbert}
    \label{fig:flakyxbert}
\end{figure*}

\subsection{Detection Techniques}

One of the prominent tools for flaky test detection is FlakeFlagger. **Zhang, “FlakeFlagger: A Machine Learning-Based Approach for Detecting Flaky Tests”**. FlakeFlagger employs machine learning models to predict flaky tests by analyzing features extracted from test execution logs and results. This approach has achieved promising results across multiple datasets by identifying patterns and anomalies indicative of flakiness. However, FlakeFlagger relies on extensive feature engineering and access to detailed execution logs, which may not always be available.

Another innovative approach is Flakify, a black-box, language model-based predictor for flaky tests that relies exclusively on the source code of test cases. Flakify utilizes CodeBERT. **Liu et al., “CodeBERT: Pre-trained Language Model for Source Code Understanding”**. This eliminates the need for access to production code and rerunning test cases, making it a practical solution for various settings. However, the dependency on source code alone might limit its accuracy in certain contexts.

Shanto Rahman et al., **Rahman et al., “Quantizing Large Language Models for Flaky Test Prediction”** have introduced a method for quantizing large language models to predict flaky tests. Quantization aims to optimize model size and performance, allowing it to operate with reduced computational resources while maintaining high accuracy. This approach is particularly beneficial for reducing computational costs during prediction. To address potential accuracy losses due to quantization, Rahman et al. integrated additional classifiers, such as random forests, to enhance performance post-quantization.



\subsection{Classification Techniques}

Classifying flaky tests into specific categories is crucial for understanding their root causes and effectively addressing them. FlakyCat is a technique that categorizes flaky tests based on their root causes, including asynchronous waits, concurrency issues, and test order dependencies. By using few-shot learning, FlakyCat can classify tests with minimal labeled data, making it practical for various projects. This approach allows developers to pinpoint the specific type of flakiness and address it accordingly.

In addition to detection, Rahman’s work extends to the classification of flaky tests. By leveraging large language models, their approach aims to categorize flaky tests into meaningful groups to facilitate debugging and resolution. This classification helps in understanding the patterns and causes of flakiness across different projects, providing valuable insights for improving test reliability.