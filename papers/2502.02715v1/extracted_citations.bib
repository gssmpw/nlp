@inproceedings{Akli2023,
      title={Flakycat: predicting flaky tests categories using few-shot learning},
      author={Akli, Amal and Haben, Guillaume and Habchi, Sarra and Papadakis, Mike and Le Traon, Yves},
      booktitle={2023 IEEE/ACM International Conference on Automation of Software Test (AST)},
      pages={140--151},
      year={2023},
      organization={IEEE}
    }

@INPROCEEDINGS{Alshammari21,
  author={Alshammari, Abdulrahman and Morris, Christopher and Hilton, Michael and Bell, Jonathan},
  booktitle={2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)}, 
  title={FlakeFlagger: Predicting Flakiness Without Rerunning Tests}, 
  year={2021},
  volume={},
  number={},
  pages={1572-1584},
  abstract={When developers make changes to their code, they typically run regression tests to detect if their recent changes (re) introduce any bugs. However, many tests are flaky, and their outcomes can change non-deterministically, failing without apparent cause. Flaky tests are a significant nuisance in the development process, since they make it more difficult for developers to trust the outcome of their tests, and hence, it is important to know which tests are flaky. The traditional approach to identify flaky tests is to rerun them multiple times: if a test is observed both passing and failing on the same code, it is definitely flaky. We conducted a very large empirical study looking for flaky tests by rerunning the test suites of 24 projects 10,000 times each, and found that even with this many reruns, some previously identified flaky tests were still not detected. We propose FlakeFlagger, a novel approach that collects a set of features describing the behavior of each test, and then predicts tests that are likely to be flaky based on similar behavioral features. We found that FlakeFlagger correctly labeled as flaky at least as many tests as a state-of-the-art flaky test classifier, but that FlakeFlagger reported far fewer false positives. This lower false positive rate translates directly to saved time for researchers and developers who use the classification result to guide more expensive flaky test detection processes. Evaluated on our dataset of 23 projects with flaky tests, FlakeFlagger outperformed the prior approach (by F1 score) on 16 projects and tied on 4 projects. Our results indicate that this approach can be effective for identifying likely flaky tests prior to running time-consuming flaky test detectors.},
  keywords={Computer bugs;Detectors;Testing;Software engineering;regression testing;flaky tests;reliability},
  doi={10.1109/ICSE43902.2021.00140},
  ISSN={1558-1225},
  month={May},}

@inproceedings{Guo2020CodeBERT,
        title = "{C}ode{BERT}: {A Pre-Trained Model for Programming and Natural Languages}",
        author = "Feng, Zhangyin  and
          Guo, Daya  and
          Tang, Duyu  and
          Duan, Nan  and
          Feng, Xiaocheng  and
          Gong, Ming  and
          Shou, Linjun  and
          Qin, Bing  and
          Liu, Ting  and
          Jiang, Daxin  and
          Zhou, Ming",
        editor = "Cohn, Trevor  and
          He, Yulan  and
          Liu, Yang",
        booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
        month = Nov,
        year = "2020",
        address = "Online",
        publisher = "Association for Computational Linguistics",
        doi = "10.18653/v1/2020.findings-emnlp.139",
        pages = "1536--1547",
        abstract = "We present CodeBERT, a bimodal pre-trained model for programming language (PL) and natural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as natural language code search, code documentation generation, etc. We develop CodeBERT with Transformer-based neural architecture, and train it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables us to utilize both {``}bimodal{''} data of NL-PL pairs and {``}unimodal data, where the former provides input tokens for model training while the latter helps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NLPL probing.",
    }

@INPROCEEDINGS{rahman2024,
  author={Rahman, Shanto and Baz, Abdelrahman and Misailovic, Sasa and Shi, August},
  booktitle={2024 IEEE Conference on Software Testing, Verification and Validation (ICST)}, 
  title={Quantizing Large-Language Models for Predicting Flaky Tests}, 
  year={2024},
  volume={},
  number={},
  pages={93-104},
  keywords={Training;Software testing;Quantization (signal);Codes;Computational modeling;Source coding;Artificial neural networks;Flaky Test Categorization;Large-Language Models;Quantization},
  doi={10.1109/ICST60714.2024.00018}}

