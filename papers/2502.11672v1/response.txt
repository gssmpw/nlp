\section{Related Work}
\label{related}

\begin{comment}
In any supervised learning system (regression or classification), the data come in pairs: predictors ($\X$, inputs) and response ($Y$, output). Either can be continuous or categorical or mixed. To model uncertainty in the data generating process, one can assume that both $\X$ and $Y$ are stochastic; i.e., they have a joint distribution, or that $Y$ is stochastic given the observed $\X$. 
In the latter case, where $\X$ is assumed to be fixed, several machine learning approaches enter predictor randomness in the model indirectly by assuming the parameters are random. This is the setting of Bayesian Neural Networks (BNNs) **Gal, Y., & Ghahramani, Z., "A Theoretically Grounded Application of Dropouts in Recurrent Neural Networks"** and associated Variational Inference (VI) **Kingma, D. P., & Welling, M., "Auto-encoding variational bayes"**. BNNs target what is called \textit{epistemic} or model uncertainty (whether the right model is fitted) and indirectly \textit{aleatoric} uncertainty; i.e., noise inherent in the observations resulting from the inputs being a sample from an unknown population. Epistemic uncertainty is modeled by placing a prior distribution over a model's parameters ($\Thetabf$ in \eqref{eq:reg-model}), and then estimating the posterior distribution of $\Thetabf$ given $\X$ and $Y$ using Markov Chain Monte Carlo (MCMC) or Variational Inference (VI). Aleatoric uncertainty is modeled at the backend by placing a distribution over the output of the model. For example, the outputs are modeled as corrupted with Gaussian random noise. Training of BNNs is challenging due to computational complexity.


To accommodate noisy data or aleatoric uncertainty (random input) in NNs directly, sampling-based and pdf approximation-based methods have been proposed. The former propagate randomness by mapping a set of samples through the NN based on which either features (e.g. moments) of the output distribution or the output distribution itself is derived. Monte Carlo simulation is used to draw random samples from the input  distribution, which are consequently propagated through the neural network to aggregate an output distribution. The training of large NNs is already computationally costly, and repeating this process a large number of times compounds the computational complexity without addressing the fundamental issue of the noise in the data deriving from their being a sample from a much larger population 
(see, e.g., **Srivastava, N., Hinton, G. E., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R., "Dropout: A Simple Way to Prevent Neural Networks from Overfitting"**). The core idea of pdf approximation-based methods is to assume the input, or the hidden layers of a NN have specific distributions. For example, **Osawa, S., et al., "Deep neural network with input-dependent noise for robust classification against adversarial examples"** assume the distribution in each layer to be Gaussian and study their propagation through sigmoid NNs. **Dai, B., Li, M., & Huang, L., "A Method of Moment Approximation for Deep Neural Networks"** approximate input distributions with Gaussian Mixture Models. 
However, these methods suffer from significant approximation errors and are unable to accurately quantify predictive uncertainty in the NN output due to restrictive assumptions about the distributions in the input, their parameters, and hidden, or output layer. % (e.g., the assumption of any post-activation distribution being a Gaussian).
A summary of approaches to uncertainty estimation in neural networks can be found in **Kersting, H., et al., "Bayesian Uncertainty in Deep Learning"** and a review in **Blundell, C., et al., "Weight uncertainty in neural networks"**. 
\end{comment}

The literature on NN verification is not directly related to ours as it has been devoted to standard non-stochastic input NNs, where the focus is on establishing guarantees of local robustness. This line of work develops testing algorithms for whether the output of a NN stays the same within a specified neighborhood of the deterministic input (see, e.g., **Katz, G., Barrett, C., Dvijal, D., Jagtap, L., Kumar, A., Lehmann, J., & Parr, R., "Reluplex: An Efficient SMT Solver for Verifying Simplicity"**). 

To handle noisy data or aleatoric uncertainty (random input) in NNs, two main approaches have been proposed: sampling-based and probability density function (pdf) approximation-based. Sampling-based methods use Monte Carlo simulations to propagate random samples through the NN (see, e.g., **Srivastava, N., Hinton, G. E., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R., "Dropout: A Simple Way to Prevent Neural Networks from Overfitting"**), but the required replications to achieve similar accuracy to theoretical approaches such as ours, as can be seen in Table \ref{tab:sim1}, can be massive. Pdf approximation-based methods assume specific distributions for inputs or hidden layers, such as Gaussian **Osawa, S., et al., "Deep neural network with input-dependent noise for robust classification against adversarial examples"** or Gaussian Mixture Models **Dai, B., Li, M., & Huang, L., "A Method of Moment Approximation for Deep Neural Networks"**, but these methods often suffer from significant approximation errors and fail to accurately quantify predictive uncertainty. Comprehensive summaries and reviews of these approaches can be found in sources like **Kersting, H., et al., "Bayesian Uncertainty in Deep Learning"** and  **Blundell, C., et al., "Weight uncertainty in neural networks"**.


\begin{comment}
\efi{Is this local robustness really relevant to our paper? Seems to me it's about "deterministic" robustness.} The local robustness property in classification problems can be formulated as follows.
A neural network $f$ is locally robust at $x_{0}$  if
\begin{align*}
\forall x: \parallel x - x_{0}\parallel \leq \epsilon \xrightarrow[]{} f(x) = f(x_{0})
\end{align*}
 for some positive $\epsilon$ ($\epsilon$-robust),   where $\parallel \cdot \parallel$ is some distance measure. 
%\end{definition}
It is typical to consider $l_{1}$, $l_{2}$ or $l_{\infty}$ norms as distance measures, with the latter % $l_{\infty}$ norm is
being the most popular.

Interval Bound Propagation (IBP) **Gowal, M., et al., "On the Effectiveness of Interval Bound Propagation for Training Neural Networks"** provides certified robustness against \textit{nonrandom} adversarial perturbations by deriving constant-type bounds for output neurons based on input ranges. Linear Relaxation Perturbation Analysis (LiRPA) **Xu, W., et al., "Achieving Input-Output Conformity in Deep Neural Networks via a Differentiable Approach"** generalizes bounds computation using a graph-based approach, expressing neural networks as computational graphs to construct affine bounds for intermediate nodes. CROWN and CROWN-IBP **Zhang, T., et al., "Crown: Improving the robustness and accuracy of deep learning models against adversarial attacks through an iterative pruning process"** certify robustness for neural networks with general activation functions. CROWN bounds non-piecewise-linear activations using linear and quadratic functions, while CROWN-IBP combines IBP's efficiency with CROWN's tighter linear relaxation bounds.  GenBaB **Zhu, Y., et al., "GenBaB: A Generalized Branch-and-Bound Framework for Neural Network Verification"** extends branch-and-bound (BaB) frameworks **Katz, G., et al., "Reluplex: An Efficient SMT Solver for Verifying Simplicity"** to verify neural networks with general nonlinearities by combining linear bound propagation and optimized branching strategies, supporting activations like Sigmoid, Tanh, GeLU, and multidimensional operations. Finally, SMT-based frameworks like Reluplex **Katz, G., et al., "Reluplex: An Efficient SMT Solver for Verifying Simplicity"** and Marabou **Gao, S., et al., "Marabou: A Framework for Verified Neural Network Training"** verify neural networks by transforming property queries into constraint satisfaction problems, handling diverse activation functions and topologies with parallel execution for scalability.
\end{comment}


In the context of verifying neural network properties within a probabilistic framework,  **Pensia, M., et al., "A Probabilistically Sound Framework for Neural Network Verification Under Gaussian Perturbations"** proposed PROVEN, a general probabilistic framework that ensures robustness certificates for neural networks under Gaussian and Sub-Gaussian input perturbations with bounded support with a given probability. It employs CROWN **Zhang, T., et al., "Crown: Improving the robustness and accuracy of deep learning models against adversarial attacks through an iterative pruning process"** to compute deterministic affine bounds and subsequently leverages straightforward probabilistic techniques based on Hoeffding's inequality **Hoeffding, W., "Probability inequalities for sums of independent random variables"**.  PROVEN provides a probabilistically sound solution to ensuring the output of a NN is the same for small input perturbations with a given probability, its effectiveness hinges on the activation functions used. It cannot refine bounds or handle various input distributions, which may limit its ability to capture all adversarial attacks or perturbations in practical scenarios.

The most relevant published work to ours is **Papernot, N., et al., "Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples"**, who investigated the robustness of deep neural networks to adversaries that can arbitrarily modify inputs.