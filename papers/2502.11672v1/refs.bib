@article{vanBuurenetal2011,
    author   = {van Buuren, Stef and Groothuis-Oudshoorn, Karin},
    year     = {2011},
    title    = {mice: Multivariate Imputation by Chained Equations in R},
    journal  = {Journal of Statistical Software},
    volume   = {45},
    number   = {3},
    pages    = {1--67},
    doi      = {10.18637/jss.v045.i03},
    url      = {https://www.jstatsoft.org/index.php/jss/article/view/v045i03}
}

@article{Efronetal_2004,
  title={Least Angle Regression},
  author={Bradley Efron and Trevor Hastie and Iain Johnstone and Robert Tibshirani},
  journal={The Annals of Statistics},
  pages={407-451},
  year={2004},
  publisher={Institute of Mathematical Statistics},
  url={https://www.jstor.org/stable/3448465}
}

@article{dataset-diabetes,
    author    = {Efron, Bradley and Hastie, Trevor and Johnstone, Iain and Tibshirani, Robert},
    year      = {2004},
    title     = {Least angle regression},
    journal   = {The Annals of Statistics},
    volume    = {32},
    number    = {2},
    pages     = {407--499},
    publisher = {Institute of Mathematical Statistics},
    url       = {https://www.jstor.org/stable/3448465}
}

@article{dataset-iris,
    author  = {FISHER, R. A.},
    year    = {1936},
    title   = {THE USE OF MULTIPLE MEASUREMENTS IN TAXONOMIC PROBLEMS},
    journal = {Annals of Eugenics},
    volume  = {7},
    number  = {2},
    pages   = {179-188},
    doi     = {10.1111/j.1469-1809.1936.tb02137.x},
    url     = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-1809.1936.tb02137.x}
}

@misc{dataset-wine,
    author    = {Lichman, M},
    year      = {2013},
    title     = {Wine Quality},
    publisher = {Irvine, CA: University of California, School of Information and Computer Science},
    note      = {UCI Machine Learning Repository [\url{https://archive.ics.uci.edu/ml}]}
}

@article{Abdelazizetal2015,
title = "Uncertainty propagation through deep neural networks",
abstract = "In order to improve the ASR performance in noisy environments, distorted speech is typically pre-processed by a speech enhancement algorithm, which usually results in a speech estimate containing residual noise and distortion. We may also have some measures of uncertainty or variance of the estimate. Uncertainty decoding is a framework that utilizes this knowledge of uncertainty in the input features during acoustic model scoring. Such frameworks have been well explored for traditional probabilistic models, but their optimal use for deep neural network (DNN)-based ASR systems is not yet clear. In this paper, we study the propagation of observation uncertainties through the layers of a DNN-based acoustic model. Since this is intractable due to the nonlinearities of the DNN, we employ approximate propagation methods, including Monte Carlo sampling, the unscented transform, and the piecewise exponential approximation of the activation function, to estimate the distribution of acoustic scores. Finally, the expected value of the acoustic score distribution is used for decoding, which is shown to further improve the ASR accuracy on the CHiME database, relative to a highly optimized DNN baseline.",
keywords = "Deep Neural Networks, Noise-robust ASR, Observation Uncertainty, Uncertainty Propagation",
author = "Abdelaziz, {Ahmed Hussen} and Shinji Watanabe and Hershey, {John R.} and Emanuel Vincent and Dorothea Kolossa",
note = "Publisher Copyright: Copyright {\textcopyright} 2015 ISCA.; 16th Annual Conference of the International Speech Communication Association, INTERSPEECH 2015 ; Conference date: 06-09-2015 Through 10-09-2015",
year = "2015",
language = "English",
volume = "2015-January",
pages = "3561--3565",
journal = "Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH",
issn = "2308-457X"
}

@inproceedings{Aligator18, 
  author    = {Andreas Humenberger and
               Maximilian Jaroschek and
               Laura Kov{\'{a}}cs},
  title     = {{Aligator.jl - {A} Julia Package for Loop Invariant Generation}},
  booktitle = {CICM},
  pages     = {111--117},
  year      = {2018},
 }

@misc{Arbeletal2023,
      title={A Primer on Bayesian Neural Networks: Review and Debates}, 
      author={Julyan Arbel and Konstantinos Pitas and Mariia Vladimirova and Vincent Fortuin},
      year={2023},
      eprint={2309.16314},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2309.16314}, 
}

@Article{Atkeson_Ohanian_2001,
  author={Andrew Atkeson and Lee E. Ohanian},
  title={{Are Phillips curves useful for forecasting inflation?}},
  journal={Quarterly Review},
  year=2001,
  volume={25},
  number={Win},
  pages={2-11},
  month={},
  keywords={Forecasting; Phillips curve; Inflation (Finance)},
  doi={},
  abstract={This study evaluates the conventional wisdom that modern Phillips curve-based models are useful tools for forecasting inflation. These models are based on the non-accelerating inflation rate of unemployment (the NAIRU). The study compares the accuracy, over the last 15 years, of three sets of inflation forecasts from NAIRU models to the naive forecast that at any date inflation will be the same over the next year as it has been over the last year. The conventional wisdom is wrong; none of the NAIRU forecasts is more accurate than the naive forecast. The likelihood of accurately predicting a change in the inflation rate from these three forecasts is no better than the likelihood of accurately predicting a change based on a coin flip. The forecasts include those from a textbook NAIRU model, those from two models similar to Stock and Watson's, and those produced by the Federal Reserve Board.},
  url={https://ideas.repec.org/a/fip/fedmqr/y2001iwinp2-11nv.25no.1.html}
}

@article{Askey,
 author = {Richard Askey and James Wilson},
 journal = {Memoirs of the American Mathematical Society, Volume 54, Number 319},
 number = {319},
 title = {Some basic hypergeometric orthogonal polynomials that generalize Jacobi polynomials},
 volume = {53},
 year = {1985}
}

@book{Baier2008,
 author = {Baier, Christel and Katoen, Joost-Pieter},
 title = {Principles of Model Checking (Representation and Mind Series)},
 year = {2008},
 isbn = {026202649X, 9780262026499},
 publisher = {The MIT Press},
} 


@inproceedings{BartheKOB12,
  author    = {Gilles Barthe and
               Boris K{\"{o}}pf and
               Federico Olmedo and
               Santiago Zanella B{\'{e}}guelin},
  title     = {Probabilistic relational reasoning for differential privacy},
  booktitle = {Proc. of {POPL} 2012: the proceedings of the 39th {ACM} 
                    {SIGPLAN-SIGACT} Symposium on Principles of Programming Languages},
  pages     = {97--110},
  year      = {2012},
  publisher = {{ACM}},
  doi       = {10.1145/2103656.2103670}
}

@inproceedings{BartheGB12,
  author    = {Gilles Barthe and
               Benjamin Gr{\'{e}}goire and
               Santiago Zanella B{\'{e}}guelin},
  title     = {Probabilistic Relational Hoare Logics for Computer-Aided Security Proofs},
  booktitle = {Proc. of {MPC} 2012: the 11th International Conference on Mathematics of Program Construction},
  pages     = {1--6},
  year      = {2012},
  eries    = {LNCS},
  volume    = {7342},
  publisher = {Springer},
  doi       = {10.1007/978-3-642-31113-0}
}

@book{barthe2020,
  title     = {Foundations of Probabilistic Programming},
  author    = {Barthe, Gilles and Katoen, Joost-Pieter and Silva, Alexandra},
  year      = {2020},
  publisher = {Cambridge University Press}
}

@inproceedings{BartocciKS20,
  author    = {Ezio Bartocci and
               Laura Kov{\'{a}}cs and
               Miroslav Stankovic},
  title     = {Mora - Automatic Generation of Moment-Based Invariants},
  booktitle = {Proc. of {TACAS} 2020: the 26th International Conference on Tools and Algorithms for the Construction and Analysis of Systems},
  pages     = {492--498},
  series    = {LNCS},
  volume    = {12078},
  publisher = {Springer},
  year      = {2020},
  doi       = {10.1007/978-3-030-45190-5}
}

@article{Bartoccietal2019,
  author    = {Ezio Bartocci and
               Laura Kov{\'{a}}cs and
               Miroslav Stankovic},
  title     = {Automatic Generation of Moment-Based Invariants for Prob-Solvable
               Loops},
  journal   = {CoRR},
  volume    = {abs/1905.02835},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.02835},
  archivePrefix = {arXiv},
  eprint    = {1905.02835},
  timestamp = {Mon, 27 May 2019 13:15:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-02835.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Barthe2016,
    author = {Barthe, Gilles and Espitau, Thomas and Fioriti, Luis Mar{\'{i}}a Ferrer and Hsu, Justin},
    title     = {Synthesizing Probabilistic Invariants via Doob's Decomposition},
    booktitle = {Proc. of {CAV} 2016: the 28th International Conference on Computer Aided Verification},
    series    = {LNCS},
    pages     = {43--61},
    volume    = {9779},
    publisher = {Springer},
    doi       = {10.1007/978-3-319-41528-4},
abstract = {When analyzing probabilistic computations, a powerful approach is to first find a martingale---an expression on the program variables whose expectation remains invariant---and then apply the optional stopping theorem in order to infer properties at termination time. One of the main challenges, then, is to systematically find martingales. We propose a novel procedure to synthesize martingale expressions from an arbitrary initial expression. Contrary to state-of-the-art approaches, we do not rely on constraint solving. Instead, we use a symbolic construction based on Doob's decomposition. This procedure can produce very complex martingales, expressed in terms of conditional expectations. We show how to automatically generate and simplify these martingales, as well as how to apply the optional stopping theorem to infer properties at termination time. This last step typically involves some simplification steps, and is usually done manually in current approaches. We implement our techniques in a prototype tool and demonstrate our process on several classical examples. Some of them go beyond the capability of current semi-automatic approaches.},
file = {:Users/miroslav/Documents/all drive/Documents/Papers/Gilles Barthe, Thomas Espitau, Luis Mar{\'{i}}a Ferrer Fioriti, Justin Hsu - Synthesizing Probabilistic Invariants via Doob's Decomposition. CAV (1) 2016.pdf:pdf},
year = {2016}
}

@misc{BartocciKovacsStankovic2021,
      title={MORA -- Automatic Generation of Moment-Based Invariants}, 
      author={Ezio Bartocci and Laura Kovacs and Miroslav Stankovic},
      year={2021},
      eprint={2103.03908},
      archivePrefix={arXiv},
      primaryClass={cs.FL}
}

@inproceedings{BartocciKS19,
  author    = {Ezio Bartocci and
               Laura Kov{\'{a}}cs and
               Miroslav Stankovic},
  title     = {Automatic Generation of Moment-Based Invariants for Prob-Solvable Loops},
  booktitle = {Proc. of {ATVA} 2019: the 17th International Symposium on Automated Technology for Verification and Analysis},
  series    = {LNCS},
  volume    = {11781},
  publisher = {Springer},
  year      = {2019},
  doi       = {10.1007/978-3-030-31784-3\_15}
}


@InProceedings{Berzins_2023,
  title = 	 {Polyhedral Complex Extraction from {R}e{LU} Networks using Edge Subdivision},
  author =       {Berzins, Arturs},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {2234--2244},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/berzins23a/berzins23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/berzins23a.html},
  abstract = 	 {A neural network consisting of piecewise affine building blocks, such as fully-connected layers and ReLU activations, is itself a piecewise affine function supported on a polyhedral complex. This complex has been previously studied to characterize theoretical properties of neural networks, but, in practice, extracting it remains a challenge due to its high combinatorial complexity. A natural idea described in previous works is to subdivide the regions via intersections with hyperplanes induced by each neuron. However, we argue that this view leads to computational redundancy. Instead of regions, we propose to subdivide edges, leading to a novel method for polyhedral complex extraction. A key to this are sign-vectors, which encode the combinatorial structure of the complex. Our approach allows to use standard tensor operations on a GPU, taking seconds for millions of cells on a consumer grade machine. Motivated by the growing interest in neural shape representation, we use the speed and differentiablility of our method to optimize geometric properties of the complex. The code is available at https://github.com/arturs-berzins/relu_edge_subdivision.}
}

@INPROCEEDINGS{Bibietal2018,
  author={Bibi, Adel and Alfadly, Modar and Ghanem, Bernard},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={Analytic Expressions for Probabilistic Moments of PL-DNN with Gaussian Input}, 
  year={2018},
  volume={},
  number={},
  pages={9099-9107},
  keywords={Perturbation methods;Visualization;Training;Probability density function;Probabilistic logic;Task analysis;Robustness},
  doi={10.1109/CVPR.2018.00948}}


@book{Billingsley2012,   
    author = {Patrick Billingsley},  year = {2012},   
    title = {Probability and Measure},   
    publisher = {Wiley},   
    edition = {Anniversary Edition}, 
}

@article{abs-1810-09538,
  author    = {Eli Bingham and
               Jonathan P. Chen and
               Martin Jankowiak and
               Fritz Obermeyer and
               Neeraj Pradhan and
               Theofanis Karaletsos and
               Rohit Singh and
               Paul A. Szerlip and
               Paul Horsfall and
               Noah D. Goodman},
  title     = {Pyro: Deep Universal Probabilistic Programming},
  journal   = {CoRR},
  volume    = {abs/1810.09538},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.09538},
  archivePrefix = {arXiv},
  eprint    = {1810.09538},
}

@article{Biswas2010,
  publisher = {{IOP} Publishing},
  author = {Parthapratim Biswas and Arun K Bhattacharya},
  title = {{Function Reconstruction as a Classical Moment Problem: A Maximum Entropy Approach}},
  journal = {Journal of Physics A: Mathematical and Theoretical},
  year = {2010},
   pages = {1--19},
  number={405003},
  volume={43}
  
}

@article{BlinnikovMoessner1998,
	author = {{Blinnikov, S.} and {Moessner, R.}},
	title = {Expansions for nearly Gaussian distributions},
	DOI= "10.1051/aas:1998221",
	url= "https://doi.org/10.1051/aas:1998221",
	journal = {Astron. Astrophys. Suppl. Ser.},
	year = 1998,
	volume = 130,
	number = 1,
	pages = "193-205",
}


@misc{Boetiusetal2024,
      title={Probabilistic Verification of Neural Networks using Branch and Bound}, 
      author={David Boetius and Stefan Leue and Tobias Sutter},
      year={2024},
      eprint={2405.17556},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.17556}, 
}

@inproceedings{Bouissouetal2016,
  title={Uncertainty propagation using probabilistic affine forms and concentration of measure inequalities},
  author={Bouissou, Olivier and Goubault, Eric and Putot, Sylvie and Chakarov, Aleksandar and Sankaranarayanan, Sriram},
  booktitle={International Conference on Tools and Algorithms for the Construction and Analysis of Systems},
  pages={225--243},
  year={2016},
  organization={Springer}
}


@article{Brennetal2017,
  title={{A Revisit of the {G}ram-{C}harlier and {E}dgeworth Series Expansions}},
  author={Brenn, Torgeir and Anfinsen, Stian Normann},
  journal={Preprint},
  pages={1--12},
  year={2017},
  url={https://hdl.handle.net/10037/11261}
}

@article{Buneletal_2019,
  title={Branch and Bound for Piecewise Linear Neural Network Verification},
  author={Rudy Bunel and Jingyue Lu and Ilker Turkaslan and Philip H. S. Torr and Pushmeet Kohli and M. Pawan Kumar},
  journal={ArXiv},
  year={2019},
  volume={abs/1909.06588},
  url={https://api.semanticscholar.org/CorpusID:202577669}
}

@article{CameronMartin1947,
 ISSN = {0003486X},
 URL = {http://www.jstor.org/stable/1969178},
 author = {R. H. Cameron and W. T. Martin},
 journal = {Annals of Mathematics},
 number = {2},
 pages = {385--392},
 publisher = {Annals of Mathematics},
 title = {The Orthogonal Development of Non-Linear Functionals in Series of Fourier-Hermite Functionals},
 volume = {48},
 year = {1947}
}

@book{CasellaBerger2001,
  author = {George Casella and Roger L. Berger},
  year = {2001},
  title = {Statistical Inference},
  publisher = {Cengage Learning},
  edition = {2}
}

@article{Chakarov2014,
abstract = {Indonesia is recognized as a nurse exporting country, with policies that encourage nursing professionals to emigrate abroad. This includes the country's adoption of international principles attempting to protect Indonesian nurses that emigrate as well as the country's own participation in a bilateral trade and investment agreement, known as the Indonesia–Japan Economic Partnership Agreement that facilitates Indonesian nurse migration to Japan. Despite the potential trade and employment benefits from sending nurses abroad under the Indonesia–Japan Economic Partnership Agreement, Indonesia itself is suffering from a crisis in nursing capacity and ensuring adequate healthcare access for its own populations. This represents a distinct challenge for Indonesia in appropriately balancing domestic health workforce needs, employment, and training opportunities for Indonesian nurses, and the need to acknowledge the rights of nurses to freely migrate abroad. Hence, this article reviews the complex operational and ethical issues associated with Indonesian health worker migration under the Indonesia–Japan Economic Partnership Agreement. It also introduces a policy proposal to improve performance of the Indonesia–Japan Economic Partnership Agreement and better align it with international principles focused on equitable health worker migration.},
author = {Chakarov, Aleksandar and Sankaranarayanan, Sriram},
doi = {10.13140/2.1.3885.0247},
file = {:Users/miroslav/Documents/all drive/Documents/Papers/  Aleksandar Chakarov, Sriram Sankaranarayanan - Expectation Invariants for Probabilistic Program Loops as Fixed Points.pdf:pdf},
issn = {16113349},
journal = {Static Analysis Symposium},
pages = {85--100},
title = {{Expectation Invariants for Probabilistic Program Loops as Fixed Points (REPORT)}},
year = {2014}
}

@inbook{Chatterjeeetal2020,
	chapter={Termination Analysis of Probabilistic Programs with Martingales},
	author={Krishnendu Chatterjee and Hongfei Fu and Petr Novotn{\'{y}}},
	doi={10.1017/9781108770750.008},
	title={Foundations of Probabilistic Programming},
	publisher={Cambridge University Press},
	address = {Cambridge},
	year={2020},
	pages={221–258}
}

@inproceedings{Chen2015,
  author    = {Yu{-}Fang Chen and
                     Chih{-}Duo Hong and
                     Bow{-}Yaw Wang and
                     Lijun Zhang},
  title     = {Counterexample-Guided Polynomial Loop Invariant Generation by Lagrange Interpolation},
  booktitle = {Proc. of {CAV} 2015: the  27th International Conference on Computer Aided Verification},
  pages     = {658--674},
  year      = {2015},
  series    = {LNCS},
  volume    = {9206},
  publisher = {Springer},
  doi       = {10.1007/978-3-319-21690-4},
  abstract = {We apply multivariate Lagrange interpolation to synthesize polynomial quantitative loop invariants for probabilistic programs. We reduce the computation of an quantitative loop invariant to solving constraints over program variables and unknown coefficients. Lagrange interpolation allows us to find constraints with less unknown coefficients. Counterexample-guided refinement furthermore generates linear constraints that pinpoint the desired quantitative invariants. We evaluate our technique by several case studies with polynomial quantitative loop invariants in the experiments.},
file = {:Users/miroslav/Documents/all drive/Documents/Papers/Yu-Fang Chen, Chih-Duo Hong, Bow-Yaw Wang, Lijun Zhang - Counterexample-Guided Polynomial Loop Invariant Generation by Lagrange Interpolation. CAV (1) 2015.pdf:pdf},
}

@inproceedings{Chenetal12,
  author    = {Xin Chen and
               Erika {\'{A}}brah{\'{a}}m and
               Sriram Sankaranarayanan},
  title     = {Taylor Model Flowpipe Construction for Non-linear Hybrid Systems},
  booktitle = {Proc. of {IEEE} {RTSS}},
  year      = {2012},
  doi       = {10.1109/RTSS.2012.70},
}

@book{Chihara1978,
    author = {Theodore S. Chihara},
    title = {An Introduction to Orthogonal Polynomials},
    publisher = {Gordon and Breach, Science Publishers},
    year = 1978
}

@article{Chorin1974, 
title={Gaussian fields and random flow}, 
volume={63}, 
DOI={10.1017/S0022112074000991}, 
number={1}, 
journal={Journal of Fluid Mechanics}, 
publisher={Cambridge University Press}, 
author={Chorin, Alexandre Joel}, 
year={1974}, 
pages={21–32}
}

@inproceedings{ColonSS03,
  author    = {Michael Col{\'{o}}n and
               Sriram Sankaranarayanan and
               Henny Sipma},
  title     = {Linear Invariant Generation Using Non-linear Constraint Solving},
  booktitle = {Proc. of {CAV} 2003: the 15th International Conference on Computer Aided Verification},
  pages     = {420--432},
  year      = {2003},
  series    = {LNCS},
  volume    = {2725},
  publisher = {Springer},
  doi       = {10.1007/b11831}
}

@book{Cramer1957,
publisher = {Princeton Univ. Press},
year = {1957},
title = {Mathematical Methods of Statistics},
language = {eng},
address = {Princeton, NJ},
author = {Harald Cram\'{e}r},
URL = {http://www.jstor.org/stable/j.ctt1bpm9r4}
}

@article{Cybenko1989,
	abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
	author = {Cybenko, G. },
	date = {1989/12/01},
	doi = {10.1007/BF02551274},
	id = {Cybenko1989},
	isbn = {1435-568X},
	journal = {Mathematics of Control, Signals and Systems},
	number = {4},
	pages = {303--314},
	title = {Approximation by superpositions of a sigmoidal function},
	url = {https://doi.org/10.1007/BF02551274},
	volume = {2},
	year = {1989},
	bdsk-url-1 = {https://doi.org/10.1007/BF02551274}}

@inproceedings{DehnertJK017,
  author    = {Christian Dehnert and
               Sebastian Junges and
               Joost-Pieter Katoen and
               Matthias Volk},
  title     = {A Storm is Coming: {A} Modern Probabilistic Model Checker},
  booktitle = {Proc. of {CAV} 2017: the 29th International Conference on Computer Aided Verification},
  pages     = {592--600},
  series    = {LNCS},
  volume    = {10427},
  publisher = {Springer},
  year      = {2017},
  doi       = {10.1007/978-3-319-63390-9}
}

@article {Denamieletal2020,
      author = "Cl\'ea Denamiel and Xun Huan and Jadranka Šepić and Ivica Vilibi\'c",
      title = "Uncertainty Propagation Using Polynomial Chaos Expansions for Extreme Sea Level Hazard Assessment: The Case of the Eastern Adriatic Meteotsunamis",
      journal = "Journal of Physical Oceanography",
      year = "2020",
      publisher = "American Meteorological Society",
      address = "Boston MA, USA",
      volume = "50",
      number = "4",
      doi = "10.1175/JPO-D-19-0147.1",
      pages=      "1005 - 1021",
      url = "https://journals.ametsoc.org/view/journals/phoc/50/4/jpo-d-19-0147.1.xml"
}

@article{Deshpandeetal2022,
title = {Probabilistic deep learning for real-time large deformation simulations},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {398},
pages = {115307},
year = {2022},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2022.115307},
url = {https://www.sciencedirect.com/science/article/pii/S004578252200411X},
author = {Saurabh Deshpande and Jakub Lengiewicz and Stéphane P.A. Bordas},
keywords = {Convolutional neural network, Bayesian inference, Bayesian deep learning, Large deformations, Finite element method, Real-time simulations},
abstract = {For many novel applications, such as patient-specific computer-aided surgery, conventional solution techniques of the underlying nonlinear problems are usually computationally too expensive and are lacking information about how certain can we be about their predictions. In the present work, we propose a highly efficient deep-learning surrogate framework that is able to accurately predict the response of bodies undergoing large deformations in real-time. The surrogate model has a convolutional neural network architecture, called U-Net, which is trained with force–displacement data obtained with the finite element method. We propose deterministic and probabilistic versions of the framework. The probabilistic framework utilizes the Variational Bayes Inference approach and is able to capture all the uncertainties present in the data as well as in the deep-learning model. Based on several benchmark examples, we show the predictive capabilities of the framework and discuss its possible limitations.}
}

@article{Dharmani2018,
	author = {{Dharmani, Bhaveshkumar C}},
	title = {Multivariate generalized Gram–Charlier series in vector notations},
	DOI= "10.1007/s10910-018-0878-5",
	url= "https://doi.org/10.1007/s10910-018-0878-5",
	journal = {Journal of Mathematical Chemistry},
	year = 2018,
	volume = 56,
	pages = "1631–1655",
}

@article{Dijkstra75,
  author    = {Edsger W. Dijkstra},
  title     = {Guarded Commands, Nondeterminacy and Formal Derivation of Programs},
  journal   = {Commun. {ACM}},
  volume    = {18},
  number    = {8},
  pages     = {453--457},
  year      = {1975},
  doi       = {10.1145/360933.360975}
}

@inproceedings{Dwork06,
  author    = {Cynthia Dwork},
  title     = {Differential Privacy},
  booktitle = {Proc. of {ICALP} 2006: the 33rd International Colloquium on Automata, Languages and Programming},
  pages     = {1--12},
  doi       = {10.1007/11787006\_1},
  series    = {LNCS},
  volume    = {4052},
  publisher = {Springer},
  year      = {2006},
  doi       = {10.1007/11787006}
}

@article{Efromovich2010,
author = {Efromovich, Sam},
title = {Orthogonal series density estimation},
journal = {WIREs Computational Statistics},
volume = {2},
number = {4},
pages = {467-476},
keywords = {adaptation, asymptotic, indirect data, nonparametric, Oracle, small sample},
doi = {https://doi.org/10.1002/wics.97},
url = {https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wics.97},
eprint = {https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/wics.97},
abstract = {Abstract Orthogonal series density estimation is a powerful nonparametric estimation methodology that allows one to analyze and present data at hand without any prior opinion about shape of an underlying density. The idea of construction of an adaptive orthogonal series density estimator is explained on the classical example of a direct sample from a univariate density. Data-driven estimators, which have been used for years, as well as recently proposed procedures, are reviewed. Orthogonal series estimation is also known for its sharp minimax properties which are explained. Furthermore, applications of the orthogonal series methodology to more complicated settings, including censored and biased data as well as estimation of the density of regression errors and the conditional density, are also presented. Copyright © 2010 John Wiley \& Sons, Inc. This article is categorized under: Statistical and Graphical Methods of Data Analysis > Density Estimation},
year = {2010}
}


@article{Ernstetal2012,
	author = {{Ernst, Oliver G.} and {Mugler, Antje} and {Starkloff, Hans-J\"org} and {Ullmann, Elisabeth}},
	title = {On the convergence of generalized polynomial chaos
          expansions},
	DOI= "10.1051/m2an/2011045",
	url= "https://doi.org/10.1051/m2an/2011045",
	journal = {ESAIM: M2AN},
	year = 2012,
	volume = 46,
	number = 2,
	pages = "317-339",
	month = "",
}

@book{Feller1971,
author = {William Feller},
title = {An introduction to probability theory and its applications},
publisher = {John Wiley {\&} Sons},
year = {1971},
edition={2},
volume={II},
address = {New York-London-Sydney}
}

@book{Kolossa2006,
author = {J. E. Kolossa},
title = {Series Approximation Methods in Statistics},
publisher = {Springer},
year = {2006},
edition={3},
address = {New York, NY}
}

@inproceedings{Fawzietal2018, 
author = {Fawzi, Alhussein and Fawzi, Hamza and Fawzi, Omar}, title = {Adversarial vulnerability for any classifier}, 
year = {2018}, 
publisher = {Curran Associates Inc.}, 
address = {Red Hook, NY, USA}, abstract = {Despite achieving impressive performance, state-of-the-art classifiers remain highly vulnerable to small, imperceptible, adversarial perturbations. This vulnerability has proven empirically to be very intricate to address. In this paper, we study the phenomenon of adversarial perturbations under the assumption that the data is generated with a smooth generative model. We derive fundamental upper bounds on the robustness to perturbations of any classification function, and prove the existence of adversarial perturbations that transfer well across different classifiers with small risk. Our analysis of the robustness also provides insights onto key properties of generative models, such as their smoothness and dimensionality of latent space. We conclude with numerical experimental results showing that our bounds provide informative baselines to the maximal achievable robustness on several datasets.}, booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems}, pages = {1186–1195}, numpages = {10}, location = {Montr\'{e}al, Canada}, series = {NIPS'18} }

@inproceedings{Feng2017,
  author    = {Yijun Feng and Lijun Zhang and David N. Jansen and Naijun Zhan and Bican Xia},
  title     = {Finding Polynomial Loop Invariants for Probabilistic Programs},
  booktitle = {Proc. of {ATVA} 2017: the 15th International Symposium on Automated Technology for Verification and Analysis},
  series    = {LNCS},
  volume    = {10482},
  publisher = {Springer},
  pages     = {400--416},
  year      = {2017},
  abstract = {Quantitative loop invariants are an essential element in the verification 
of probabilistic programs. Recently, multivariate Lagrange interpolation has been applied to synthesizing polynomial invariants. In this paper, we propose an alternative approach. First, we fix a polynomial template as a candidate of a loop invariant. Using Stengle's Positivstellensatz and a transformation to a sum-of-squares problem, we find sufficient conditions on the coefficients. Then, we solve a semidefinite programming feasibility problem to synthesize the loop invariants. If the semidefinite program is unfeasible, we backtrack after increasing the degree of the template. Our approach is semi-complete in the sense that it will always lead us to a feasible solution if one exists and numerical errors are small. Experimental results show the efficiency of our approach.},
file = {:Users/miroslav/Documents/all drive/Documents/Papers/Yijun Feng, Lijun Zhang, David N. Jansen, Naijun Zhan, Bican Xia - Finding Polynomial Loop Invariants for Probabilistic Programs. ATVA 2017.pdf:pdf},
}

@inproceedings{
    Ferrarietal_2022,
    title={Complete Verification via Multi-Neuron Relaxation Guided Branch-and-Bound},
    author={Claudio Ferrari and Mark Niklas Mueller and Nikola Jovanovi{\'c} and Martin Vechev},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=l_amHf1oaK}
}

@article{Filipovicetal2013,
title = {Density approximations for multivariate affine jump-diffusion processes},
journal = {Journal of Econometrics},
volume = {176},
number = {2},
pages = {93-111},
year = {2013},
issn = {0304-4076},
doi = {https://doi.org/10.1016/j.jeconom.2012.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0304407613000596},
author = {Damir Filipović and Eberhard Mayerhofer and Paul Schneider},
keywords = {Affine processes, Asymptotic expansion, Density approximation, Orthogonal polynomials},
abstract = {We introduce closed-form transition density expansions for multivariate affine jump-diffusion processes. The expansions rely on a general approximation theory which we develop in weighted Hilbert spaces for random variables which possess all polynomial moments. We establish parametric conditions which guarantee existence and differentiability of transition densities of affine models and show how they naturally fit into the approximation framework. Empirical applications in option pricing, credit risk, and likelihood inference highlight the usefulness of our expansions. The approximations are extremely fast to evaluate, and they perform very accurately and numerically stable.}
}

@article{Fisher_1936,
  title={THE USE OF MULTIPLE MEASUREMENTS IN TAXONOMIC PROBLEMS},
  author={Rory A. Fisher},
  journal={Annals of Human Genetics},
  year={1936},
  volume={7},
  pages={179-188},
  url={https://onlinelibrary.wiley.com/doi/10.1111/j.1469-1809.1936.tb02137.x}
}

@inproceedings{Floyd67,
  author    = {Robert W. Floyd},
  title     = {Assigning Meanings to Programs},
  booktitle = {Mathematical Aspects of Computer Science.
               Proceedings of Symposium on Applied Mathematics},
  pages     = {19–32},
  year      = {1967},
  volume    = {19},
  publisher = {American Mathematical Society, Providence}
}

@article{Fooetal2007,
   author    =  "J. Foo and Z. Yosibash and G. E. Karniadakis",
   title     =  "Stochastic simulation of riser-sections with uncertain measured pressure loads and/or uncertain material properties",
   year      =  "2007",
   journal   =  "Comput. Methods Appl. Mech. Eng.",
   volume    =  "196",
   pages     =  "4250--4271"
}


@article{Formaggiaetal2013,
   author    =  "L. Formaggia and A. Guadagnini and I. Imperiali and V. Lever and G. Porta and M. Riva and A. Scotti and L. Tamellini",
   title     =  "Global sensitivity analysis through polynomial chaos expansion of a basin-scale geochemical compaction model",
   year      =  "2013",
   journal   =  "Comput. Geosci.",
   volume    =  "17",
   pages     =  "25--42"
}

@inproceedings{FremontDGYSS19,
  author    = {Daniel J. Fremont and
               Tommaso Dreossi and
               Shromona Ghosh and
               Xiangyu Yue and
               Alberto L. Sangiovanni{-}Vincentelli and
               Sanjit A. Seshia},
  title     = {Scenic: a language for scenario specification and scene generation},
  booktitle = {Proc. of PLDI 2019: the 40th {ACM} {SIGPLAN} Conference on Programming Language Design and Implementation},
  pages     = {63--78},
  year      = {2019},
  publisher = {{ACM}},
  doi       = {10.1145/3314221}
}

@inproceedings{GalGhahramani2016,
  title={Dropout as a bayesian approximation: Representing model uncertainty in deep learning},
  author={Gal, Yarin and Ghahramani, Zoubin},
  booktitle={international conference on machine learning},
  pages={1050--1059},
  year={2016},
  organization={PMLR}
}

@article{Gawlikowskietal2023,
author={Jakob Gawlikowski and Cedrique Rovile Njieutcheu Tassi and Mohsin Ali and Jongseok Lee and Matthias Humt and Jianxiang Feng and Anna Kruspe and Rudolph Triebel and Peter Jung and Ribana Roscher and  Muhammad Shahzad and Wen Yang and Richard Bamler and Xiao Xiang Zhu},
title = {A survey of uncertainty in deep neural networks},
journal = {Artificial Intelligence Review},
volume= {56},
pages = {1513–1589},
year = {2023}, 
url = {https://doi.org/10.1007/s10462-023-10562-9}
}

@inproceedings{Gehretal2016,
  author    = {Timon Gehr and
               Sasa Misailovic and
               Martin T. Vechev},
  title     = {{PSI:} Exact Symbolic Inference for Probabilistic Programs},
  booktitle = {Proc. of {CAV}
               2016: the 28th International Conference on   Computer Aided Verification},
  pages     = {62--83},
  year      = {2016},
  series    = {LNCS},
  volume    = {9779},
  publisher = {Springer},
  doi       = {10.1007/978-3-319-41528-4\_4}
}

@inproceedings{Gehretal2018,
  author={Gehr, Timon and Mirman, Matthew and Drachsler-Cohen, Dana and Tsankov, Petar and Chaudhuri, Swarat and Vechev, Martin},
  booktitle={2018 IEEE Symposium on Security and Privacy (SP)}, 
  title={AI2: Safety and Robustness Certification of Neural Networks with Abstract Interpretation}, 
  year={2018},
  volume={},
  number={},
  pages={3-18},
  keywords={Robustness;Biological neural networks;Cats;Neurons;Safety;Perturbation methods;Reliable Machine Learning;Robustness;Neural Networks;Abstract Interpretation},
  doi={10.1109/SP.2018.00058}
}

@inproceedings{Gehretal2020, 
title = {$\lambda${P}{S}{I}: {Exact} {Inference} for {Higher}-{Order} {Probabilistic} {Programs}}, 
isbn = {978-1-4503-7613-6}, 
url = {https://dl.acm.org/doi/10.1145/3385412.3386006}, 
doi = {10.1145/3385412.3386006}, booktitle = {Proceedings of the 41st {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}}, 
publisher = {ACM}, 
author = {Gehr, Timon and Steffen, Samuel and Vechev, Martin}, 
month = jun, year = {2020} 
}

@MastersThesis{Gerhold02,
  author = 	 {Stefan Gerhold},
  title = 	 {{Uncoupling Systems of Linear Ore Operator Equations}},
  school = 	 {RISC-Linz},
  year = 	 {2002},
}

@article{Ghanem1998,
  title={Probabilistic Characterization of Transport in Heterogeneous Media},
  author={Ghanem, Roger},
  journal={Computer Methods in Applied Mechanics and Engineering},
  volume={158},
  pages={199–220},
  year={1998},
  url={doi:10.1016/s0045-7825(97)00250-8}
}

@article{GhanemDham1998,
  title={Stochastic Finite Element Analysis for Multiphase Flow in Heterogeneous Porous Media},
  author={Ghanem, R.  and Dham, S.},
  journal={Transport in Porous Medias},
  volume={32},
  pages={239–262},
  year={1998},
  url={https://doi.org/10.1023/A:1006514109327}
}

@book{GhanemSpanos1991,
author = {Roger G. Ghanem and Pol D. Spanos},
title = {Stochastic Finite Elements: A Spectral Approach},
publisher = {Springer},
year = {1991},
address = {New York, NY}
}

@article{Ghahramani15,
  author    = {Zoubin Ghahramani},
  title     = {Probabilistic machine learning and artificial intelligence},
  journal   = {Nature},
  volume    = {521},
  number    = {7553},
  pages     = {452--459},
  year      = {2015},
  url       = {https://doi.org/10.1038/nature14541},
  doi       = {10.1038/nature14541},
  timestamp = {Wed, 14 Nov 2018 10:30:42 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/nature/Ghahramani15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Ghahramani2015,
author    = {Zoubin Ghahramani}, 
title     = {Probabilistic machine learning and artificial intelligence}, 
journal   = {Nature},
pages     = {452--459},
volume    = {521},
publisher = {Nature Publishing Group},
year      = {2015},
doi       = {10.1038/nature14541}
}

@inproceedings{WuIZTDKRAJBHLWZKKB24,
  author       = {Haoze Wu and
                  Omri Isac and
                  Aleksandar Zeljic and
                  Teruhiro Tagomori and
                  Matthew L. Daggitt and
                  Wen Kokke and
                  Idan Refaeli and
                  Guy Amir and
                  Kyle Julian and
                  Shahaf Bassan and
                  Pei Huang and
                  Ori Lahav and
                  Min Wu and
                  Min Zhang and
                  Ekaterina Komendantskaya and
                  Guy Katz and
                  Clark W. Barrett},
  editor       = {Arie Gurfinkel and
                  Vijay Ganesh},
  title        = {Marabou 2.0: {A} Versatile Formal Analyzer of Neural Networks},
  booktitle    = {Computer Aided Verification - 36th International Conference, {CAV}
                  2024, Montreal, QC, Canada, July 24-27, 2024, Proceedings, Part {II}},
  series       = {Lecture Notes in Computer Science},
  volume       = {14682},
  pages        = {249--264},
  publisher    = {Springer},
  year         = {2024},
  url          = {https://doi.org/10.1007/978-3-031-65630-9\_13},
  doi          = {10.1007/978-3-031-65630-9\_13},
  timestamp    = {Mon, 09 Dec 2024 22:46:02 +0100}
}

@article{Giraldietal2017,
   author    =  "L. Giraldi and O. P. Le Ma{\^{\i}}tre and K. T. Mandli and C. N. Dawson and I. Hoteit and O. M. Knio",
   title     =  "Bayesian inference of earthquake parameters from buoy data using a polynomial chaos-based surrogate",
   year      =  "2017",
   journal   =  "Comput. Geosci.",
   volume    =  "21",
   pages     =  "683--699"
}


@article{Goetze1983,
  title={Asymptotic expansions for sums of weakly dependent random vectors},
  author={Friedrich G{\"o}tze and Christian Hipp},
  journal={Zeitschrift f{\"u}r Wahrscheinlichkeitstheorie und Verwandte Gebiete},
  year={1983},
  volume={64},
  pages={211-239}
}


@article{Goodfellowetal2014,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@article{Goodfellowetal2014b,
  title={Explaining and Harnessing Adversarial Examples},
  author={Ian J. Goodfellow and Jonathon Shlens and Christian Szegedy},
  journal={CoRR},
  year={2014},
  volume={abs/1412.6572},
  url={https://api.semanticscholar.org/CorpusID:6706414}
}

@article{Gowaletal_2018,
  title={On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models},
  author={Sven Gowal and Krishnamurthy Dvijotham and Robert Stanforth and Rudy Bunel and Chongli Qin and Jonathan Uesato and Relja Arandjelovi{\'c} and Timothy A. Mann and Pushmeet Kohli},
  journal={ArXiv},
  year={2018},
  volume={abs/1810.12715},
  url={https://api.semanticscholar.org/CorpusID:53112003}
}

@article{Graves2011,
  title={Practical variational inference for neural networks},
  author={Graves, Alex},
  journal={Advances in neural information processing systems},
  volume={24},
  year={2011}
}

@article{Gretz2013,
abstract = {Introduction: Interest on the impact of variant histology in bladder cancer prognosis is increasing. Although squamous differentiation is the most well characterized, only recently have less common variants gained increased recognition. We assessed whether squamous differentiation conferred a worse prognosis than nonvariant urothelial bladder cancer in a contemporary cohort of patients treated with radical cystectomy given the increased awareness of other less common variants. Methods: We identified patients with squamous differentiation or nonvariant histology on transurethral resection of bladder tumor and/or cystectomy pathology during a 10-year period. Disease specific and overall survival were evaluated using Kaplan-Meier methodology. Cox regression was used to assess variables associated with mortality. Results: Between 2003 and 2013, 934 patients underwent cystectomy for urothelial bladder cancer. Overall 617 nonvariant and 118 squamous differentiation cases were identified, and the remainder was nonsquamous differentiation variant histology. Overall 75{\%} of patients with squamous differentiation had muscle invasive disease at diagnosis compared with 59{\%} of those with nonvariant histology (p=0.002). Nonorgan confined disease at cystectomy was more common in patients with squamous differentiation (57{\%} vs 44{\%}, p=0.009). Among cases on neoadjuvant chemotherapy 20{\%} (9 of 45) of nonvariant and 13{\%} (1 of 8) of squamous differentiation were pT0N0 (p=0.527). Median followup was 52 months. Adjusted for demographics, pathological stage and chemotherapy, squamous differentiation was not associated with an increased risk of disease specific (HR 1.35, 95{\%} CI 0.90-2.04, p=0.150) or all cause mortality (HR 0.90, 95{\%} CI 0.60-1.25, p=0.515). Conclusions: In a contemporary cohort of urothelial bladder cancer with recognition and characterization of less commonly described variants, squamous differentiation is not associated with a worse disease specific and all cause mortality when compared to a pure nonvariant cohort. {\textcopyright} 2015 American Urological Association Education and Research, Inc.},
author = {Gretz, Friedrich and Katoen, Joost-pieter and Mciver, Annabelle},
doi = {10.1007/978-3-642-40196-1},
file = {:Users/miroslav/Documents/all drive/Documents/Papers/Friedrich Gretz, Joost-Pieter Katoen, Annabelle McIver - Prinsys - On a Quest for Probabilistic Loop Invariants. QEST 2013.pdf:pdf},
isbn = {978-3-642-40195-4},
issn = {23520779 (ISSN)},
number = {August},
title = {{Quantitative Evaluation of Systems}},
url = {http://link.springer.com/10.1007/978-3-642-40196-1},
volume = {8054},
year = {2013}
}

@inproceedings{GretzKM13,
  author    = {Friedrich Gretz and Joost-Pieter Katoen and Annabelle McIver},
  title     = {Prinsys - On a Quest for Probabilistic Loop Invariants},
  booktitle = {Proc. of {QEST} 2013: the 10th International Conference on Quantitative Evaluation of Systems},
  pages     = {193--208},
  year      = {2013},
  series    = {LNCS},
  volume    = {8054},
  publisher = {Springer},
  doi       = {10.1007/978-3-642-40196-1\_17}
}

@misc{guardian2018,
  author = {Edward Helmore},
  title = {{Uber shuts down self-driving operation in Arizona after fatal crash}},
  howpublished = "\href{https://www.theguardian.com/technology/2018/may/23/uber-shuts-down-self-driving-operation-in-arizona-two-months-after-fatal-crash}{The Guardian, May 23}",
  year = {2018}
}

@InProceedings{HafizBhat2020,
author="Hafiz, Abdul Mueed
and Bhat, Ghulam Mohiuddin",
editor="Tuba, Milan
and Akashe, Shyam
and Joshi, Amit",
title="A Survey of Deep Learning Techniques for Medical Diagnosis",
booktitle="Information and Communication Technology for Sustainable Development",
year="2020",
publisher="Springer Singapore",
address="Singapore",
pages="161--170",
abstract="With the advent of new technologies in artificial intelligence and machine learning, the medical community has taken a strong notice of the potential of these technologies for addressing automation. Deep learning is one of these technologies which has been chosen by the research community for advancing its medical applications. This survey paper serves the research community twofold. First, it gives researchers an introduction to the basic technologies involved in deep learning. Second, it gives the readers insight into the state of the art in the field of medical applications of deep learning, particularly for medical imaging technologies.",
isbn="978-981-13-7166-0"
}


@inproceedings{HahnHWZ10,
  author = {Ernst Moritz Hahn and Holger Hermanns and Bj{\"{o}}rn Wachter and Lijun Zhang},
  title = {{PASS:} Abstraction Refinement for Infinite Probabilistic Models},
  booktitle = {Proc. of {TACAS} 2010: the 16th International Conference on Tools and Algorithms for the Construction and Analysis of Systems},
  pages     = {353--357},
  year      = {2010},
  series    = {LNCS},
  volume    = {6015},
  publisher = {Springer},
  doi       = {10.1007/978-3-642-12002-2}
}

@article{Hald2000,
  title={{The Early History of the Cumulants and the Gram-Charlier Series}},
  author={Hald, Anders},
  journal={International Statistical Review},
  volume={68},
  number={2},
  pages={137--153},
  year={2000},
  publisher={Wiley Online Library}
}


@article{Herman90,
  author    = {Ted Herman},
  title     = {Probabilistic Self-Stabilization},
  journal   = {Inf. Process. Lett.},
  volume    = {35},
  number    = {2},
  pages     = {63--67},
  year      = {1990},
  doi       = {10.1016/0020-0190(90)90107-9}
}

@article{Herve2009,
  title={The Nagaev-Guivarc'h method via the Keller-Liverani theorem},
  author={Loic Herv{\'e} and Franccoise Pene},
  journal={Bulletin de la Soci{\'e}t{\'e} Math{\'e}matique de France},
  year={2009},
  volume={138},
  pages={415-489}
}

@article{HienKleiber1997,
title = {Stochastic finite element modelling in linear transient heat transfer},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {144},
number = {1},
pages = {111-124},
year = {1997},
issn = {0045-7825},
doi = {https://doi.org/10.1016/S0045-7825(96)01168-1},
url = {https://www.sciencedirect.com/science/article/pii/S0045782596011681},
author = {Tran Duong Hien and Michał Kleiber},
abstract = {A stochastic heat transfer variational principle is suggested for transient and steady-state heat problems. The non-statistical formulation is based on the combination of the second-order perturbation technique and second-moment analysis. The principle allows incorporation of system uncertainties into the conventional finite element equations, which are solved for the first two probabilistic moments of the nodal random temperature field. Computational aspects of the problem are discussed. Numerical algorithms have been worked out and easily adapted to an existing finite element code. The formalism presented may be directly employed for a wide class of field problems.}
}

@inproceedings{HintonvanCamp1993, 
author = {Hinton, Geoffrey E. and van Camp, Drew}, title = {Keeping the neural networks simple by minimizing the description length of the weights}, 
year = {1993}, 
isbn = {0897916115}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, 
url = {https://doi.org/10.1145/168304.168306}, 
doi = {10.1145/168304.168306}, booktitle = {Proceedings of the Sixth Annual Conference on Computational Learning Theory}, 
pages = {5–13}, 
numpages = {9}, 
location = {Santa Cruz, California, USA}, 
series = {COLT '93} 
}

@article{Hoare69,
  author    = {C. A. R. Hoare},
  title     = {An Axiomatic Basis for Computer Programming},
  journal   = {Commun. {ACM}},
  volume    = {12},
  number    = {10},
  pages     = {576--580},
  year      = {1969},
  doi       = {10.1145/363235.363259}
}

@article{Hoeffding1963,
 ISSN = {01621459, 1537274X},
 URL = {http://www.jstor.org/stable/2282952},
 abstract = {Upper bounds are derived for the probability that the sum S of n independent random variables exceeds its mean ES by a positive number nt. It is assumed that the range of each summand of S is bounded or bounded above. The bounds for $\Pr \{S - ES \geq nt \}$ depend only on the endpoints of the ranges of the summands and the mean, or the mean and the variance of S. These results are then used to obtain analogous inequalities for certain sums of dependent random variables such as U statistics and the sum of a random sample without replacement from a finite population.},
 author = {Wassily Hoeffding},
 journal = {Journal of the American Statistical Association},
 number = {301},
 pages = {13--30},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Probability Inequalities for Sums of Bounded Random Variables},
 urldate = {2025-01-30},
 volume = {58},
 year = {1963}
}

@book{Hollanderetal2013,
author = {Myles Hollander and Douglas A. Wolfe and Eric Chicken},
title = {Nonparametric Statistical Methods},
publisher = {John Wiley {\&} Sons},
year = {2013},
edition={3},
address = {New York-London-Sydney}
}

@article{Horniketal1989,
title = {Multilayer feedforward networks are universal approximators},
journal = {Neural Networks},
volume = {2},
number = {5},
pages = {359-366},
year = {1989},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(89)90020-8},
url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
author = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
keywords = {Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks},
abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.}
}

@INPROCEEDINGS{Hosseinietal2017,
  author={Hosseini, Hossein and Xiao, Baicen and Poovendran, Radha},
  booktitle={2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA)}, 
  title={Google's Cloud Vision API is Not Robust to Noise}, 
  year={2017},
  volume={},
  number={},
  pages={101-105},
  keywords={Noise measurement;Google;Robustness;Image restoration;Perturbation methods;Biological system modeling;Gaussian noise;Google Cloud Vision API;machine learning;Image Noise;adversarial machine learning},
  doi={10.1109/ICMLA.2017.0-172}}

@article{Houetal2006,
   author    =  "T. Y. Hou and W. Luo and B. Rozovskii and H.-M. Zhou",
   title     =  "Wiener chaos expansions and numerical solutions of randomly forced equations of fluid mechanics",
   year      =  "2006",
   journal   =  "J. Comput. Phys.",
   volume    =  "216",
   pages     =  "687--706"
}

@article{Huang2014,
author = {Huang, Zhenqi and Fan, Chuchu and Mereacre, Alexandru and Mitra, Sayan and Kwiatkowska, Marta},
doi = {10.1007/978-3-319-08867-9_25},
file = {:Users/miroslav/Documents/all drive/Documents/Papers/Zhenqi Huang, Chuchu Fan, Alexandru Mereacre, Sayan Mitra, and Marta Kwiatkowska - Invariant verification of nonlinear hybrid automata networks of cardiac cells.pdf:pdf},
isbn = {9783319088662},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Biological networks,hybrid systems,invariants,verification},
pages = {373--390},
title = {{Invariant verification of nonlinear hybrid automata networks of cardiac cells}},
volume = {8559 LNCS},
year = {2014}
}

@article{HuellermeierWaegeman2021,
title= {Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods},
   author = {H\"ullermeier, Eyke and Waegeman, Willem},
journal= {Machine Learning},
year = {2021},
volume = {110}, 
number = {3},
pages = {457--506},
abstract = {The notion of uncertainty is of major importance in machine learning and constitutes a key element of machine learning methodology. In line with the statistical tradition, uncertainty has long been perceived as almost synonymous with standard probability and probabilistic predictions. Yet, due to the steadily increasing relevance of machine learning for practical applications and related issues such as safety requirements, new problems and challenges have recently been identified by machine learning scholars, and these problems may call for new methodological developments. In particular, this includes the importance of distinguishing between (at least) two different types of uncertainty, often referred to as aleatoric and epistemic. In this paper, we provide an introduction to the topic of uncertainty in machine learning as well as an overview of attempts so far at handling uncertainty in general and formalizing this distinction in particular.},
doi = {10.1007/s10994-021-05946-3},
url = {https://doi.org/10.1007/s10994-021-05946-3}
}

@ARTICLE{Innocentinietal2018,
AUTHOR={Innocentini, Guilherme C. P. and Hodgkinson, Arran and Radulescu, Ovidiu}, TITLE={Time Dependent Stochastic mRNA and Protein Synthesis in Piecewise-Deterministic Models of Gene Networks},   JOURNAL={Frontiers in Physics},      
VOLUME={6},           
YEAR={2018},        
URL={https://www.frontiersin.org/articles/10.3389/fphy.2018.00046},       	
DOI={10.3389/fphy.2018.00046},      
ISSN={2296-424X}
}

@article{Jakemanetal2019,
title = {Polynomial chaos expansions for dependent random variables},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {351},
pages = {643-666},
year = {2019},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2019.03.049},
url = {https://www.sciencedirect.com/science/article/pii/S0045782519301884},
author = {John D. Jakeman and Fabian Franzelin and Akil Narayan and Michael Eldred and Dirk Plfüger},
keywords = {Uncertainty quantification, Nataf transformation, Polynomial chaos expansion, Leja sequence, Interpolation, Quadrature},
abstract = {Polynomial chaos expansions (PCE) are well-suited to quantifying uncertainty in models parameterized by independent random variables. The assumption of independence leads to simple strategies for building multivariate orthonormal bases and for sampling strategies to evaluate PCE coefficients. In contrast, the application of PCE to models of dependent variables is much more challenging. Three approaches can be used to construct PCE of models of dependent variables. The first approach uses mapping methods where measure transformations, such as the Nataf and Rosenblatt transformation, can be used to map dependent random variables to independent ones; however we show that this can significantly degrade performance since the Jacobian of the map must be approximated. A second strategy is the class of dominating support methods. In these approaches a PCE is built using independent random variables whose distributional support dominates the support of the true dependent joint density; we provide evidence that this approach appears to produce approximations with suboptimal accuracy. A third approach, the novel method proposed here, uses Gram–Schmidt orthogonalization (GSO) to numerically compute orthonormal polynomials for the dependent random variables. This approach has been used successfully when solving differential equations using the intrusive stochastic Galerkin method, and in this paper we use GSO to build PCE using a non-intrusive stochastic collocation method. The stochastic collocation method treats the model as a black box and builds approximations of the input–output map from a set of samples. Building PCE from samples can introduce ill-conditioning which does not plague stochastic Galerkin methods. To mitigate this ill-conditioning we generate weighted Leja sequences, which are nested sample sets, to build accurate polynomial interpolants. We show that our proposed approach, GSO with weighted Leja sequences, produces PCE which are orders of magnitude more accurate than PCE constructed using mapping or dominating support methods.}
}

@article{Jasouretal2021,
  author    = {Ashkan Jasour and
               Allen Wang and
               Brian C. Williams},
  title     = {Moment-Based Exact Uncertainty Propagation Through Nonlinear Stochastic
               Autonomous Systems},
  journal   = {CoRR},
  volume    = {abs/2101.12490},
  year      = {2021},
  url       = {https://arxiv.org/abs/2101.12490},
  eprinttype = {arXiv},
  eprint    = {2101.12490},
  timestamp = {Tue, 02 Feb 2021 09:52:17 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2101-12490.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{Jietal2020,
      title={Uncertainty Propagation in Deep Neural Network Using Active Subspace}, 
      author={Weiqi Ji and Zhuyin Ren and Chung K. Law},
      year={2020},
      eprint={1903.03989},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1903.03989}, 
}

@article{KaminskiKM19,
  author    = {Benjamin Lucien Kaminski and
               Joost-Pieter Katoen and
               Christoph Matheja},
  title     = {On the hardness of analyzing probabilistic programs},
  journal   = {Acta Inf.},
  volume    = {56},
  number    = {3},
  pages     = {255--285},
  year      = {2019},
  doi       = {10.1007/s00236-018-0321-1}
}

@InProceedings{Katzetal_2017,
author="Katz, Guy
and Barrett, Clark
and Dill, David L.
and Julian, Kyle
and Kochenderfer, Mykel J.",
editor="Majumdar, Rupak
and Kun{\v{c}}ak, Viktor",
title="Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks",
booktitle="Computer Aided Verification",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="97--117",
abstract="Deep neural networks have emerged as a widely used and effective means for tackling complex, real-world problems. However, a major obstacle in applying them to safety-critical systems is the great difficulty in providing formal guarantees about their behavior. We present a novel, scalable, and efficient technique for verifying properties of deep neural networks (or providing counter-examples). The technique is based on the simplex method, extended to handle the non-convex Rectified Linear Unit (ReLU) activation function, which is a crucial ingredient in many modern neural networks. The verification procedure tackles neural networks as a whole, without making any simplifying assumptions. We evaluated our technique on a prototype deep neural network implementation of the next-generation airborne collision avoidance system for unmanned aircraft (ACAS Xu). Results show that our technique can successfully prove properties of networks that are an order of magnitude larger than the largest networks verified using existing methods.",
isbn="978-3-319-63387-9"
}




@InProceedings{Katzetal_2019,
author="Katz, Guy
and Huang, Derek A.
and Ibeling, Duligur
and Julian, Kyle
and Lazarus, Christopher
and Lim, Rachel
and Shah, Parth
and Thakoor, Shantanu
and Wu, Haoze
and Zelji{\'{c}}, Aleksandar
and Dill, David L.
and Kochenderfer, Mykel J.
and Barrett, Clark",
editor="Dillig, Isil
and Tasiran, Serdar",
title="The Marabou Framework for Verification and Analysis of Deep Neural Networks",
booktitle="Computer Aided Verification",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="443--452",
abstract="Deep neural networks are revolutionizing the way complex systems are designed. Consequently, there is a pressing need for tools and techniques for network analysis and certification. To help in addressing that need, we present Marabou, a framework for verifying deep neural networks. Marabou is an SMT-based tool that can answer queries about a network's properties by transforming these queries into constraint satisfaction problems. It can accommodate networks with different activation functions and topologies, and it performs high-level reasoning on the network that can curtail the search space and improve performance. It also supports parallel execution to further enhance scalability. Marabou accepts multiple input formats, including protocol buffer files generated by the popular TensorFlow framework for neural networks. We describe the system architecture and main components, evaluate the technique and discuss ongoing work.",
isbn="978-3-030-25540-4"
}




@InProceedings{Karimietal2022,
author="Karimi, Ahmad
and Moosbrugger, Marcel
and Stankovi{\v{c}}, Miroslav
and Kov{\'a}cs, Laura
and Bartocci, Ezio
and Bura, Efstathia",
editor="{\'A}brah{\'a}m, Erika
and Paolieri, Marco",
title="Distribution Estimation for Probabilistic Loops",
booktitle="Quantitative Evaluation of Systems",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="26--42",
abstract="We present an algorithmic approach to estimate the value distributions of random variables of probabilistic loops whose statistical moments are (partially) known. Based on these moments, we apply two statistical methods, Maximum Entropy and Gram-Charlier series, to estimate the distributions of the loop's random variables. We measure the accuracy of our distribution estimation by comparing the resulting distributions using exact and estimated moments of the probabilistic loop, and performing statistical tests. We evaluate our method on several probabilistic loops with polynomial updates over random variables drawing from common probability distributions, including examples implementing financial and biological models. For this, we leverage symbolic approaches to compute exact higher-order moments of loops as well as use sampling-based techniques to estimate moments from loop executions. Our experimental results provide practical evidence of the accuracy of our method for estimating distributions of probabilistic loop outputs.",
isbn="978-3-031-16336-4"
}

@inproceedings{Katoen2010,
author    = {Katoen, Joost Pieter and McIver, 
             Annabelle K. and Meinicke, Larissa A. 
              and Morgan, Carroll C.},
title     = {Linear-invariant generation for 
             probabilistic programs: Automated 
             support for proof-based methods},
booktitle = {Proc. of {SAS} 2010: the 17th International 
             Symposium on Static Analysis},
pages     = {390--406},
volume    = {6337},
series    = {LNCS},
year      = {2010},
abstract  = {We present a constraint-based method for 
             automatically generating quantitative invariants 
             for linear probabilistic programs, and we show 
             how it can be used, in combination with proof-based 
             methods, to verify properties of probabilistic programs 
             that cannot be analysed using existing automated methods. 
             To our knowledge, this is the first automated method 
             proposed for quantitative-invariant generation.},
doi       = {10.1007/978-3-642-15769-1\_24},
file = {:Users/miroslav/Documents/all drive/Documents/Papers/Joost-Pieter Katoen, Annabelle McIver, Larissa Meinicke, Carroll C. Morgan - Linear-Invariant Generation for Probabilistic Programs.pdf:pdf},
keywords = {Probabilistic programs,invariant generation,quantitative program logic,verification}
}

@article{KatoenZHHJ11,
  author    = {Joost-Pieter Katoen and
               Ivan S. Zapreev and
               Ernst Moritz Hahn and
               Holger Hermanns and
               David N. Jansen},
  title     = {The ins and outs of the probabilistic model checker {MRMC}},
  journal   = {Perform. Eval.},
  volume    = {68},
  number    = {2},
  pages     = {90--104},
  year      = {2011},
  doi       = {10.1016/j.peva.2010.04.001}
}

@book{Kauers11, 
  author    = {Manuel Kauers and
               Peter Paule},
  title     = {{The Concrete Tetrahedron - Symbolic Sums, Recurrence Equations, Generating
               Functions, Asymptotic Estimates}},
  series    = {Texts {\&} Monographs in Symbolic Computation},
  publisher = {Springer},
  year      = {2011},
  url       = {https://doi.org/10.1007/978-3-7091-0445-3},
  doi       = {10.1007/978-3-7091-0445-3},
  isbn      = {978-3-7091-0444-6},
  timestamp = {Tue, 16 May 2017 14:24:23 +0200},
  biburl    = {https://dblp.org/rec/bib/series/tmsc/KauersP11},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{KendallGal2017, 
author = {Kendall, Alex and Gal, Yarin}, title = {What uncertainties do we need in Bayesian deep learning for computer vision?}, year = {2017}, isbn = {9781510860964}, publisher = {Curran Associates Inc.}, address = {Red Hook, NY, USA}, abstract = {There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model - uncertainty which can be explained away given enough data. Traditionally it has been difficult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the benefits of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks.}, booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems}, pages = {5580–5590}, numpages = {11}, location = {Long Beach, California, USA}, series = {NIPS'17} }


@book{KendallStuart1977,
    title={{The Advanced Theory of Statistics. Volume 1: Distribution Theory}},
    author={Maurice Kendall and Alan Stuart},
    year={1977},
    address = {New York, NY},
	publisher={Macmillan}
}

@misc{KingmaWelling2022,
      title={Auto-Encoding Variational Bayes}, 
      author={Diederik P Kingma and Max Welling},
      year={2022},
      eprint={1312.6114},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1312.6114}, 
}

@misc{Klinkenbergetal2023,
      title={Exact Bayesian Inference for Loopy Probabilistic Programs}, 
      author={Lutz Klinkenberg and Christian Blumenthal and Mingshuai Chen and Joost-Pieter Katoen},
      year={2023},
      eprint={2307.07314},
      archivePrefix={arXiv},
      primaryClass={cs.PL}
}

@article{Knio2006,
	doi = {10.1016/j.fluiddyn.2005.12.003},
	url = {https://doi.org/10.1016/j.fluiddyn.2005.12.003},
	year = 2006,
	month = {sep},
	publisher = {{IOP} Publishing},
	volume = {38},
	number = {9},
	pages = {616--640},
	author = {O M Knio and O P Le Ma{\^{\i}}tre},
	title = {Uncertainty propagation in {CFD} using polynomial chaos decomposition},
	journal = {Fluid Dynamics Research},
	abstract = {Uncertainty quantification in CFD computations is receiving increased interest, due in large part to the increasing complexity of physical models, and the inherent introduction of random model data. This paper focuses on recent application of PC methods for uncertainty representation and propagation in CFD computations. The fundamental concept on which polynomial chaos (PC) representations are based is to regard uncertainty as generating a new set of dimensions, and the solution as being dependent on these dimensions. A spectral decomposition in terms of orthogonal basis functions is used, the evolution of the basis coefficients providing quantitative estimates of the effect of random model data. A general overview of PC applications in CFD is provided, focusing exclusively on applications involving the unreduced Navier–Stokes equations. Included in the present review are an exposition of the mechanics of PC decompositions, an illustration of various means of implementing these representations, and a perspective on the applicability of the corresponding techniques to propagate and quantify uncertainty in Navier–Stokes computations.}
}

@InProceedings{Kofnovetal2022,
author="Kofnov, Andrey
and Moosbrugger, Marcel
and Stankovi{\v{c}}, Miroslav
and Bartocci, Ezio
and Bura, Efstathia",
editor="{\'A}brah{\'a}m, Erika
and Paolieri, Marco",
title="Moment-Based Invariants for Probabilistic Loops with Non-polynomial Assignments",
booktitle="Quantitative Evaluation of Systems",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="3--25",
abstract="We present a method to automatically approximate moment-based invariants of probabilistic programs with non-polynomial updates of continuous state variables to accommodate more complex dynamics. Our approach leverages polynomial chaos expansion to approximate non-linear functional updates as sums of orthogonal polynomials. We exploit this result to automatically estimate state-variable moments of all orders in Prob-solvable loops with non-polynomial updates. We showcase the accuracy of our estimation approach in several examples, such as the turning vehicle model and the Taylor rule in monetary policy.",
isbn="978-3-031-16336-4"
}


@article{Kofnovetal2024,
author = {Kofnov, Andrey and Moosbrugger, Marcel and Stankovi\v{c}, Miroslav and Bartocci, Ezio and Bura, Efstathia},
title = {Exact and Approximate Moment Derivation for Probabilistic Loops With Non-Polynomial Assignments},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-3301},
url = {https://doi.org/10.1145/3641545},
doi = {10.1145/3641545},
abstract = {Many stochastic continuous-state dynamical systems can be modeled as probabilistic programs with nonlinear non-polynomial updates in non-nested loops. We present two methods, one approximate and one exact, to automatically compute, without sampling, moment-based invariants for such probabilistic programs as closed-form solutions parameterized by the loop iteration. The exact method applies to probabilistic programs with trigonometric and exponential updates and is embedded in the Polar tool. The approximate method for moment computation applies to any nonlinear random function as it exploits the theory of polynomial chaos expansion to approximate non-polynomial updates as the sum of orthogonal polynomials. This translates the dynamical system to a non-nested loop with polynomial updates, and thus renders it conformable with the Polar tool that computes the moments of any order of the state variables. We evaluate our methods on an extensive number of examples ranging from modeling monetary policy to several physical motion systems in uncertain environments. The experimental results demonstrate the advantages of our approach with respect to the current state-of-the-art.},
journal = {ACM Trans. Model. Comput. Simul.},
month = {jan},
keywords = {Probabilistic programs, Prob-solvable loops, Polynomial Chaos Expansion, Non-linear updates, Trigonometric updates, Exponential updates, Stochastic dynamical systems}
}

@book{KolmogorovFomin1976,
author = {Kolmogorov, A.N. and Fomin, S.V.},
title = {Elements of the Theory of Functions and Functional Analysis},
publisher = {Nauka},
year = {1976},
edition = {4},
address = {Moscow}
}


@book{KolmogorovFomin1957,
publisher = {Graylock Press},
booktitle = {Elements of the theory of functions and functional analysis : 1. Metric and normed spaces / transl. from the 1. (1954) Russian ed. by Leo F. Boron},
year = {1957},
title = {Elements of the theory of functions and functional analysis  : 1. Metric and normed spaces / transl. from the 1. (1954) Russian ed. by Leo F. Boron},
language = {eng},
address = {Rochester, NY},
author = {Kolmogorov, Andrej Nikolaevič and Fomin, Sergej V},
keywords = {Normierter Raum},
}

@inproceedings{Kovacs2008,
author = {Kovacs, Laura},
file = {:Users/miroslav/Documents/PhD/References/Kovacs2008{\_}ReasoningAlgebraicallyAboutPSolvableLoops.pdf:pdf},
pages = {249--264},
title = {{Reasoning Algebraically About P-Solvable Loops.pdf}},
year = {2008},
booktitle={TACAS}
}

@article{Kozen81,
  author    = {Dexter Kozen},
  title     = {Semantics of Probabilistic Programs},
  journal   = {J. Comput. Syst. Sci.},
  volume    = {22},
  number    = {3},
  pages     = {328--350},
  year      = {1981},
  doi       = {10.1016/0022-0000(81)90036-2}
}

@article{Krapfetal2024, 
title={Piecewise Linear Transformation – Propagating Aleatoric Uncertainty in Neural Networks}, 
volume={38}, url={https://ojs.aaai.org/index.php/AAAI/article/view/30029}, DOI={10.1609/aaai.v38i18.30029}, abstractNote={Real-world data typically exhibit aleatoric uncertainty which has to be considered during data-driven decision-making to assess the confidence of the decision provided by machine learning models. To propagate aleatoric uncertainty represented by probability distributions (PDs) through neural networks (NNs), both sampling-based and function approximation-based methods have been proposed. However, these methods suffer from significant approximation errors and are not able to accurately represent predictive uncertainty in the NN output. In this paper, we present a novel method, Piecewise Linear Transformation (PLT), for propagating PDs through NNs with piecewise linear activation functions (e.g., ReLU NNs). PLT does not require sampling or specific assumptions about the PDs. Instead, it harnesses the piecewise linear structure of such NNs to determine the propagated PD in the output space. In this way, PLT supports the accurate quantification of predictive uncertainty based on the criterion exactness of the propagated PD. We assess this exactness in theory by showing error bounds for our propagated PD. Further, our experimental evaluation validates that PLT outperforms competing methods on publicly available real-world classification and regression datasets regarding exactness. Thus, the PDs propagated by PLT allow to assess the uncertainty of the provided decisions, offering valuable support.}, number={18}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Krapf, Thomas and Hagn, Michael and Miethaner, Paul and Schiller, Alexander and Luttner, Lucas and Heinrich, Bernd}, year={2024}, month={Mar.}, pages={20456-20464} }

@InProceedings{Kura19,
author="Kura, Satoshi
and Urabe, Natsuki
and Hasuo, Ichiro",
editor="Vojnar, Tom{\'a}{\v{s}}
and Zhang, Lijun",
title="Tail Probabilities for Randomized Program Runtimes via Martingales for Higher Moments",
booktitle="Tools and Algorithms for the Construction and Analysis of Systems",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="135--153",
}

@book{Kutateladze_1996,
author = {Kutateladze, Semen},
year = {1996},
month = {01},
pages = {},
title = {Fundamentals of Functional Analysis},
series    = {Kluwer Texts in the Mathematical Sciences},
volume = {12},
isbn = {978-90-481-4661-1},
doi = {10.1007/978-94-015-8755-6}
}


@inproceedings{KwiatkowskaNP11,
  author    = {Marta Z. Kwiatkowska and
               Gethin Norman and
               David Parker},
  title     = {{PRISM} 4.0: Verification of Probabilistic Real-Time Systems},
  booktitle = {Proc. of {CAV} 2011: the 23rd International Conference on Computer Aided Verification},
  pages     = {585--591},
  year      = {2011},
  series    = {LNCS},
  volume    = {6806},
  publisher = {Springer},
  doi       = {10.1007/978-3-642-22110-1},
}

@article{Lasserre2021,
author = {Lasserre, Jean B.},
title = {Simple formula for integration of polynomials on a simplex},
year = {2021},
issue_date = {Jun 2021},
publisher = {BIT Computer Science and Numerical Mathematics},
address = {USA},
volume = {61},
number = {2},
issn = {0006-3835},
url = {https://doi.org/10.1007/s10543-020-00828-x},
doi = {10.1007/s10543-020-00828-x},
abstract = {We show that integrating a polynomial f of degree t on an arbitrary simplex (with respect to Lebesgue measure) reduces to evaluating t homogeneous related Bombieri polynomials of degree j=1,2,…,t, each at a unique point ξj of the simplex. This new and very simple formula could be exploited in finite (and extended finite) element methods, as well as in applications where such integrals must be evaluated. A similar result also holds for a certain class of positively homogeneous functions that are integrable on the canonical simplex.},
journal = {BIT},
month = {jun},
pages = {523–533},
numpages = {11},
keywords = {Numerical integration, Simplex, Laplace transform, 65D30, 78M12, 44A10}
}


@article{Lebazetal2016,
	title={{Reconstruction of a Distribution From a Finite Number of Its Moments: A Comparative Study in the Case of Depolymerization Process}},
	author={Lebaz, Noureddine and Cockx, Arnaud and Sp{\'e}randio, Mathieu and Morchain, J{\'e}r{\^o}me},
	journal={Computers \& Chemical Engineering},
	pages={326--337},
	volume={84},
	year={2016},
	publisher={Elsevier}
}


@book{Lin92,
 author = {G. L. Lin}, 
 title = {Characterizations of Distributions via Moments},
 year = {1992},
 publisher = {Indian Statistical Institute}
} 

@article{Makino2003,
    author = {Kyoko Makino and Martin Berz},
    title = {Taylor models and other validated functional inclusion methods},
    journal = {Int. J. Pure Appl. Math},
    year = {2003},
}

@article{Marzarietal2024, 
title={Enumerating Safe Regions in Deep Neural Networks with Provable Probabilistic Guarantees}, volume={38}, url={https://ojs.aaai.org/index.php/AAAI/article/view/30134}, DOI={10.1609/aaai.v38i19.30134}, abstractNote={Identifying safe areas is a key point to guarantee trust for systems that are based on Deep Neural Networks (DNNs). To this end, we introduce the AllDNN-Verification problem: given a safety property and a DNN, enumerate the set of all the regions of the property input domain which are safe, i.e., where the property does hold. Due to the #P-hardness of the problem, we propose an efficient approximation method called ε-ProVe. Our approach exploits a controllable underestimation of the output reachable sets obtained via statistical prediction of tolerance limits, and can provide a tight —with provable probabilistic guarantees— lower estimate of the safe areas. Our empirical evaluation on different standard benchmarks shows the scalability and effectiveness of our method, offering valuable insights for this new type of verification of DNNs.}, number={19}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Marzari, Luca and Corsi, Davide and Marchesini, Enrico and Farinelli, Alessandro and Cicalese, Ferdinando}, year={2024}, month={Mar.}, pages={21387-21394} }
@article{McCullochPitts1943, 
title={A logical calculus of the ideas immanent in nervous activity}, 
volume={5}, 
DOI={10.1007/BF02478259}, 
number={4}, 
journal={The bulletin of mathematical biophysics}, 
author={McCulloch, Warren S. and Pitts, Walter}, 
year={1943}, 
pages={115-133}
}


@book{McIverM05,
  author    = {Annabelle McIver and
               Carroll Morgan},
  title     = {Abstraction, Refinement and Proof for Probabilistic Systems},
  series    = {Monographs in Computer Science},
  publisher = {Springer},
  year      = {2005},
  doi       = {10.1007/b138392}
}

@article{MeechamJeng1968, 
title={Use of the {W}iener—{H}ermite expansion for nearly normal turbulence}, 
volume={32}, 
DOI={10.1017/S0022112068000698}, 
number={2}, 
journal={Journal of Fluid Mechanics}, 
publisher={Cambridge University Press}, 
author={Meecham, William C. and Jeng, Dah-Teng}, 
year={1968}, 
pages={225–249}
}


@article{Moosbruggeretal2022, 
author = {Moosbrugger, Marcel and Stankovi\v{c}, Miroslav and Bartocci, Ezio and Kov\'{a}cs, Laura}, 
title = {This is the Moment for Probabilistic Loops}, 
year = {2022}, 
issue_date = {October 2022}, 
publisher = {Association for Computing Machinery}, 
address = {New York, NY, USA}, 
volume = {6}, 
number = {OOPSLA2}, 
url = {https://doi.org/10.1145/3563341}, 
doi = {10.1145/3563341}, 
abstract = {We present a novel static analysis technique to derive higher moments for program variables for a large class of probabilistic loops with potentially uncountable state spaces. Our approach is fully automatic, meaning it does not rely on externally provided invariants or templates. We employ algebraic techniques based on linear recurrences and introduce program transformations to simplify probabilistic programs while preserving their statistical properties. We develop power reduction techniques to further simplify the polynomial arithmetic of probabilistic programs and define the theory of moment-computable probabilistic loops for which higher moments can precisely be computed. Our work has applications towards recovering probability distributions of random variables and computing tail probabilities. The empirical evaluation of our results demonstrates the applicability of our work on many challenging examples.}, 
journal = {Proc. {ACM} Program. Lang.}, 
month = {oct}, 
articleno = {178}, 
numpages = {29}, 
keywords = {Higher Moments, Distribution Recovery, Linear Recurrences, Probabilistic Programs} 
}

@book{MotwaniRaghavan1995,
  author    = {Rajeev Motwani, Prabhakar Raghavan}, 
  title     = {Randomized Algorithms},
  publisher = {Cambridge University Press},
  year      = {1995},
  isbn      = {3257227892}
}

@article{Munkhammar_etal_2017,
    doi = {10.1371/journal.pone.0174573},
    author = {Munkhammar, Joakim AND Mattsson, Lars AND Rydén, Jesper},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Polynomial probability distribution estimation using the method of moments},
    year = {2017},
    month = {04},
    volume = {12},
    url = {https://doi.org/10.1371/journal.pone.0174573},
    pages = {1-14},
    number = {4},

}

@ARTICLE{Muehlpfordtetal2018,
  author={Mühlpfordt, Tillmann and Findeisen, Rolf and Hagenmeyer, Veit and Faulwasser, Timm},
  journal={IEEE Control Systems Letters}, 
  title={Comments on Truncation Errors for Polynomial Chaos Expansions}, 
  year={2018},
  volume={2},
  number={1},
  pages={169-174},
  doi={10.1109/LCSYS.2017.2778138}}


@article{Neher2007,
    author = {Neher, M. and  Jackson, K. R. and  Nedialkov, N. S.},
    title = {On Taylor Model Based Integration of ODEs},
    journal = {SIAM Journal on Numerical Analysis},
    pages = {236-262},
    year = {2007},
    doi = {10.1137/050638448},
}

@article{NoviInverardi:2006,
 author = {Novi Inverardi, P. L. and Tagliani, A.},
 title = {Discrete Distributions from Moment Generating Function},
 journal = {Appl. Math. Comput.},
 issue_date = {November, 2006},
 volume = {182},
 number = {1},
 year = {2006},
 issn = {0096-3003},
 pages = {200--209},
 numpages = {10},
 doi = {10.1016/j.amc.2006.03.048},
 acmid = {2618577},
 publisher = {Elsevier Science Inc.},
 address = {New York, NY, USA},
} 

@misc{Pintzetal2022,
      title={A Survey on Uncertainty Toolkits for Deep Learning}, 
      author={Maximilian Pintz and Joachim Sicking and Maximilian Poretschkin and Maram Akila},
      year={2022},
      eprint={2205.01040},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2205.01040}, 
}

@article{ProvostHa2009,
title = {On the inversion of certain moment matrices},
journal = {Linear Algebra and its Applications},
volume = {430},
number = {10},
pages = {2650-2658},
year = {2009},
note = {Special Issue devoted to selected papers presented at the 16th International Workshop on Matrices and Statistics (IWMS-2007)},
issn = {0024-3795},
doi = {https://doi.org/10.1016/j.laa.2008.10.024},
url = {https://www.sciencedirect.com/science/article/pii/S0024379508005223},
author = {Serge B. Provost and Hyung {Tae Ha}},
keywords = {Matrix inversion, Moments, Density approximation, Orthogonal polynomials, Patterned matrices},
abstract = {An explicit representation of the elements of the inverses of certain patterned matrices involving the moments of nonnegative weight functions is derived in this paper. It is shown that a sequence of monic orthogonal polynomials can be generated from a given weight function in terms of Hankel-type determinants and that the corresponding matrix inverse can be expressed in terms of their associated coefficients and orthogonality factors. This result enables one to obtain an explicit representation of a certain type of approximants which apply to a wide class of positive continuous functions. Convenient expressions for the coefficients of standard classical orthogonal polynomials such as Legendre, Jacobi, Laguerre and Hermite polynomials are also provided. Several examples illustrate the results.}
}

@article{Rahman2018,
title = {A polynomial chaos expansion in dependent random variables},
journal = {Journal of Mathematical Analysis and Applications},
volume = {464},
number = {1},
pages = {749-775},
year = {2018},
issn = {0022-247X},
doi = {https://doi.org/10.1016/j.jmaa.2018.04.032},
url = {https://www.sciencedirect.com/science/article/pii/S0022247X18303305},
author = {Sharif Rahman},
keywords = {Uncertainty quantification, Multivariate orthogonal polynomials, Fourier series},
abstract = {This paper introduces a new generalized polynomial chaos expansion (PCE) comprising measure-consistent multivariate orthonormal polynomials in dependent random variables. Unlike existing PCEs, whether classical or generalized, no tensor-product structure is assumed or required. Important mathematical properties of the generalized PCE are studied by constructing orthogonal decomposition of polynomial spaces, explaining completeness of orthogonal polynomials for prescribed assumptions, exploiting whitening transformation for generating orthonormal polynomial bases, and demonstrating mean-square convergence to the correct limit. Analytical formulae are proposed to calculate the mean and variance of a truncated generalized PCE for a general output variable in terms of the expansion coefficients. An example derived from a stochastic boundary-value problem illustrates the generalized PCE approximation in estimating the statistical properties of an output variable for 12 distinct non-product-type probability measures of input variables.}
}


@InProceedings{Raghuetal_2017,
  title = 	 {On the Expressive Power of Deep Neural Networks},
  author =       {Maithra Raghu and Ben Poole and Jon Kleinberg and Surya Ganguli and Jascha Sohl-Dickstein},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {2847--2854},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/raghu17a/raghu17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/raghu17a.html},
  abstract = 	 {We propose a new approach to the problem of neural network expressivity, which seeks to characterize how structural properties of a neural network family affect the functions it is able to compute. Our approach is based on an interrelated set of measures of expressivity, unified by the novel notion of trajectory length, which measures how the output of a network changes as the input sweeps along a one-dimensional path. Our findings show that: (1) The complexity of the computed function grows exponentially with depth (2) All weights are not equal: trained networks are more sensitive to their lower (initial) layer weights (3) Trajectory regularization is a simpler alternative to batch normalization, with the same performance.}
}

@article{Rao_1962,
author = {R. Ranga Rao},
title = {{Relations between Weak and Uniform Convergence of Measures with Applications}},
volume = {33},
journal = {The Annals of Mathematical Statistics},
number = {2},
publisher = {Institute of Mathematical Statistics},
pages = {659 -- 680},
year = {1962},
doi = {10.1214/aoms/1177704588},
URL = {https://doi.org/10.1214/aoms/1177704588}
}



@Manual{RizzoSzekely2022,
    title = {energy: E-Statistics: Multivariate Inference via the Energy of Data},
    author = {Maria Rizzo and Gabor Szekely},
    year = {2022},
    note = {R package version 1.7-11},
    url = {https://CRAN.R-project.org/package=energy},
  }


@Book{Rudin1976,
author = { Rudin, Walter },
title = { Principles of mathematical analysis},
edition = { 3d ed. },
isbn = { 007054235 },
publisher = { McGraw-Hill New York },
pages = { x, 342 p. ; },
year = { 1976 },
type = { Book },
url = { http://www.loc.gov/catdir/toc/mh031/75017903.html },
language = { English },
subjects = { Mathematical analysis. },
life-dates = { 1976 -  },
catalogue-url = { https://nla.gov.au/nla.cat-vn298857 },
}


@book{rudin1986,
  abstract = {This is an advanced text for the one- or two-semester course in analysis taught primarily to math, science, computer science, and electrical engineering majors at the junior, senior or graduate level.},
  added-at = {2009-08-06T15:16:38.000+0200},
  author = {Rudin, Walter},
  biburl = {https://www.bibsonomy.org/bibtex/229da3cf9192185322b3412cbd84926d6/chato},
  citeulike-article-id = {370342},
  citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0070542341},
  citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0070542341},
  citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0070542341},
  citeulike-linkout-3 = {http://www.amazon.co.uk/exec/obidos/ASIN/0070542341/citeulike00-21},
  citeulike-linkout-4 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0070542341},
  citeulike-linkout-5 = {http://www.worldcat.org/isbn/0070542341},
  citeulike-linkout-6 = {http://books.google.com/books?vid=ISBN0070542341},
  citeulike-linkout-7 = {http://www.amazon.com/gp/search?keywords=0070542341\&index=books\&linkCode=qs},
  citeulike-linkout-8 = {http://www.librarything.com/isbn/0070542341},
  howpublished = {Hardcover},
  interhash = {d61b982cc29cc3a417774eaa75e9270b},
  intrahash = {29da3cf9192185322b3412cbd84926d6},
  isbn = {0070542341},
  keywords = {math},
  month = May,
  posted-at = {2005-10-29 15:39:35},
  priority = {0},
  publisher = {McGraw-Hill Science/Engineering/Math},
  timestamp = {2009-08-06T15:16:54.000+0200},
  title = {Real and Complex Analysis},
  year = 1986
}


@article{Sankaranarayanan2020,
  title={Quantitative analysis of programs with probabilities and concentration of measure inequalities},
  author={Sankaranarayanan, Sriram},
  journal={Foundations of Probabilistic Programming},
  pages={259},
  year={2020}
}


@book{Schoutens2000,
author = {Schoutens, Wim},
title = {Stochastic Processes and Orthogonal Polynomials},
publisher = {Springer},
year = {2000},
series = {Lecture Notes in Statistics},
address = {New York}
}

@article{Schmidhuber2015,
title = {Deep learning in neural networks: An overview},
journal = {Neural Networks},
volume = {61},
pages = {85-117},
year = {2015},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2014.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0893608014002135},
author = {Jürgen Schmidhuber},
keywords = {Deep learning, Supervised learning, Unsupervised learning, Reinforcement learning, Evolutionary computation},
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks.}
}

@inproceedings{SelyuninRBISG15,
  author    = {Konstantin Selyunin and
               Denise Ratasich and
               Ezio Bartocci and
               Md. Ariful Islam and
               Scott A. Smolka and
               Radu Grosu},
  title     = {Neural Programming: Towards adaptive control in Cyber-Physical Systems},
  booktitle = {Proc. of {CDC} 2015; the 54th {IEEE} Conference on Decision and Control},
  pages     = {6978--6985},
  publisher = {IEEE},
  year      = {2015},
  doi       = {10.1109/CDC.2015.7403319}
}

@article{Shietal2024,
  title={Neural Network Verification with Branch-and-Bound for General Nonlinearities},
  author={Zhouxing Shi and Qirui Jin and Zico Kolter and Suman Jana and Cho-Jui Hsieh and Huan Zhang},
  journal={ArXiv},
  year={2024},
  volume={abs/2405.21063},
  url={https://arxiv.org/pdf/2405.21063}
}



@article{Sickingetal2022,
  title={Tailored Uncertainty Estimation for Deep Learning Systems},
  author={Joachim Sicking and Maram Akila and Jan David Schneider and Fabian Huger and Peter Schlicht and Tim Wirtz and Stefan Wrobel},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.13963},
  url={https://api.semanticscholar.org/CorpusID:248475977}
}

@book{Silverman1998,
author = {Silverman, B.W.},
title = {Density Estimation for Statistics and Data Analysis},
publisher = {Routledge},
year = {1998},
edition = {1},
doi = {https://doi.org/10.1201/9781315140919}
}

@article{SonDu2020,
author = {Son, Jeongeun and Du, Yuncheng},
title = {Probabilistic surrogate models for uncertainty analysis: Dimension reduction-based polynomial chaos expansion},
journal = {International Journal for Numerical Methods in Engineering},
volume = {121},
number = {6},
pages = {1198-1217},
keywords = {bivariate dimension reduction, generalized polynomial chaos, high-dimensional integration, parametric uncertainty, stochastic ordinary differential equation},
doi = {https://doi.org/10.1002/nme.6262},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nme.6262},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/nme.6262},
abstract = {Summary This paper presents an approach for efficient uncertainty analysis (UA) using an intrusive generalized polynomial chaos (gPC) expansion. The key step of the gPC-based uncertainty quantification (UQ) is the stochastic Galerkin (SG) projection, which can convert a stochastic model into a set of coupled deterministic models. The SG projection generally yields a high-dimensional integration problem with respect to the number of random variables used to describe the parametric uncertainties in a model. However, when the number of uncertainties is large and when the governing equation of the system is highly nonlinear, the SG approach-based gPC can be challenging to derive explicit expressions for the gPC coefficients because of the low convergence in the SG projection. To tackle this challenge, we propose to use a bivariate dimension reduction method (BiDRM) in this work to approximate a high-dimensional integral in SG projection with a few one- and two-dimensional integrations. The efficiency of the proposed method is demonstrated with three different examples, including chemical reactions and cell signaling. As compared to other UA methods, such as the Monte Carlo simulations and nonintrusive stochastic collocation (SC), the proposed method shows its superior performance in terms of computational efficiency and UA accuracy.},
year = {2020}
}


@inproceedings{Srirametal2020,
 author = {Sankaranarayanan, Sriram and Chou, Yi and Goubault, Eric and Putot, Sylvie},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {17502--17513},
 publisher = {Curran Associates, Inc.},
 title = {Reasoning about Uncertainties in Discrete-Time Dynamical Systems using Polynomial Forms.},
 url = {https://proceedings.neurips.cc/paper/2020/file/ca886eb9edb61a42256192745c72cd79-Paper.pdf},
 volume = {33},
 year = {2020}
}


@article{Stankovic1996,
title = {Taylor Expansion for Generalized Functions}, 
journal = {Journal of Mathematical Analysis and Application 203, 31-37}, 
year = {1996},
author = {B. Stankovi{\'{c}}}, 
}


@article{SteinhardtRuss2012,
  title={Finite-time regional verification of stochastic non-linear systems},
  author={Steinhardt, Jacob and Tedrake, Russ},
  journal={The International Journal of Robotics Research},
  volume={31},
  number={7},
  pages={901--923},
  year={2012},
  publisher={SAGE Publications Sage UK: London, England}
}


@techreport{StockWatson2008,
 title = "Phillips Curve Inflation Forecasts",
 author = "Stock, James H and Watson, Mark W",
 institution = "National Bureau of Economic Research",
 type = "Working Paper",
 series = "Working Paper Series",
 number = "14322",
 year = "2008",
 month = "September",
 doi = {10.3386/w14322},
 URL = "http://www.nber.org/papers/w14322",
 abstract = {This paper surveys the literature since 1993 on pseudo out-of-sample evaluation of inflation forecasts in the United States and conducts an extensive empirical analysis that recapitulates and clarifies this literature using a consistent data set and methodology.  The literature review and empirical results are gloomy and indicate that Phillips curve forecasts (broadly interpreted as forecasts using an activity variable) are better than other multivariate forecasts, but their performance is episodic, sometimes better than and sometimes worse than a good (not naïve) univariate benchmark.  We provide some preliminary evidence characterizing successful forecasting episodes.},
}

@article{SteinhardtT12,
  author    = {Jacob Steinhardt and
               Russ Tedrake},
  title     = {Finite-time regional verification of stochastic non-linear systems},
  journal   = {Int. J. Robotics Res.},
  volume    = {31},
  number    = {7},
  pages     = {901--923},
  year      = {2012},
  doi       = {10.1177/0278364912444146}
}

@article{Sudjianto2020,
  title={Unwrapping The Black Box of Deep ReLU Networks: Interpretability, Diagnostics, and Simplification},
  author={A. Sudjianto and William Knauth and Rahul Singh and Zebin Yang and Aijun Zhang},
  journal={ArXiv},
  year={2020},
  volume={abs/2011.04041},
  url={https://api.semanticscholar.org/CorpusID:226281525}
}


@book{Suetin1979,
author = {Suetin, P.K.},
title = {Classical orthogonal polynomials},
publisher = {Nauka},
year = {1979},
edition = {2},
address = {Moscow}
}


@article{SZABLOWSKI_2015,
title = {A few remarks on orthogonal polynomials},
journal = {Applied Mathematics and Computation},
volume = {252},
pages = {215-228},
year = {2015},
issn = {0096-3003},
doi = {https://doi.org/10.1016/j.amc.2014.11.112},
url = {https://www.sciencedirect.com/science/article/pii/S0096300314016452},
author = {Paweł J. Szabłowski},
keywords = {Moment problem, Moment matrix, Cholesky decomposition, Hankel matrices, Connection coefficients, Linearization coefficients},
}

@book{Szego1939,
    author = {Gábor Szegő},
    title = {Orthogonal Polynomials},
    publisher = {American Mathematical Society} ,
    year = 1939
}

@inproceedings{SzekelyRizzo2004,
  title={TESTING FOR EQUAL DISTRIBUTIONS IN HIGH DIMENSION},
  author={G{\'a}bor J. Sz{\'e}kely and Maria L. Rizzo},
  year={2004},
  volume       = "5",
  series       = "InterStat",
  month        = "November"
}

@article{Taylor1993,
  author={Taylor, John B.},
  title={{Discretion versus policy rules in practice}},
  journal={Carnegie-Rochester Conference Series on Public Policy},
  year=1993,
  volume={39},
  number={1},
  pages={195-214},
  month={December},
  keywords={},
  doi={},
  abstract={No abstract is available for this item.},
  url={https://ideas.repec.org/a/eee/crcspp/v39y1993ip195-214.html}
}


@inproceedings{Tekel_Cohen,
author = {J. Tekel and L. Cohen},
title = {{Constructing and estimating probability distributions from moments}},
volume = {8391},
booktitle = {Automatic Target Recognition XXII},
editor = {Firooz A. Sadjadi and Abhijit Mahalanobis},
organization = {International Society for Optics and Photonics},
publisher = {SPIE},
pages = {83910E},
keywords = {moments, probability, orthogonal polynomials},
year = {2012},
doi = {10.1117/12.919443},
URL= {https://doi.org/10.1117/12.919443}
}

@article{TranHSBMB17,
  author    = {Dustin Tran and
               Matthew D. Hoffman and
               Rif A. Saurous and
               Eugene Brevdo and
               Kevin Murphy and
               David M. Blei},
  title     = {Deep Probabilistic Programming},
  journal   = {CoRR},
  volume    = {abs/1701.03757},
  year      = {2017},
  url       = {http://arxiv.org/abs/1701.03757},
  archivePrefix = {arXiv},
  eprint    = {1701.03757},
}

@incollection{Triebel2001,
  doi = {10.1007/978-3-0348-8257-6\_8},
  year = {2001},
  publisher = {Birkh\"{a}user Basel},
  author = {Hans Triebel},
  title = {Taylor expansions of distributions},
  booktitle = {The Structure of Functions}
}

@article{Vasicek1977,
title = {An equilibrium characterization of the term structure},
journal = {Journal of Financial Economics},
volume = {5},
number = {2},
pages = {177-188},
year = {1977},
issn = {0304-405X},
doi = {https://doi.org/10.1016/0304-405X(77)90016-2},
url = {https://www.sciencedirect.com/science/article/pii/0304405X77900162},
author = {Oldrich Vasicek}
}

@article{Wangetal_2022,
author = {Wang, Zi and Albarghouthi, Aws and Prakriya, Gautam and Jha, Somesh},
title = {Interval universal approximation for neural networks},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {POPL},
url = {https://doi.org/10.1145/3498675},
doi = {10.1145/3498675},
abstract = {To verify safety and robustness of neural networks, researchers have successfully applied abstract interpretation, primarily using the interval abstract domain. In this paper, we study the theoretical power and limits of the interval domain for neural-network verification. First, we introduce the interval universal approximation (IUA) theorem. IUA shows that neural networks not only can approximate any continuous function f (universal approximation) as we have known for decades, but we can find a neural network, using any well-behaved activation function, whose interval bounds are an arbitrarily close approximation of the set semantics of f (the result of applying f to a set of inputs). We call this notion of approximation interval approximation. Our theorem generalizes the recent result of Baader et al. from ReLUs to a rich class of activation functions that we call squashable functions. Additionally, the IUA theorem implies that we can always construct provably robust neural networks under ℓ∞-norm using almost any practical activation function. Second, we study the computational complexity of constructing neural networks that are amenable to precise interval analysis. This is a crucial question, as our constructive proof of IUA is exponential in the size of the approximation domain. We boil this question down to the problem of approximating the range of a neural network with squashable activation functions. We show that the range approximation problem (RA) is a Δ2-intermediate problem, which is strictly harder than NP-complete problems, assuming coNP⊄NP. As a result, IUA is an inherently hard problem: No matter what abstract domain or computational tools we consider to achieve interval approximation, there is no efficient construction of such a universal approximator. This implies that it is hard to construct a provably robust network, even if we have a robust network to start with.},
journal = {Proc. ACM Program. Lang.},
month = {jan},
articleno = {14},
numpages = {29},
keywords = {Abstract Interpretation, Universal Approximation}
}



@article{WanKarniadakis2005,
   author    =  "X. Wan and G. E. Karniadakis",
   title     =  "An adaptive multi-element generalized polynomial chaos method for stochastic differential equations",
   year      =  "2005",
   journal   =  "J. Comput. Phys.",
   volume    =  "209",
   pages     =  "617--642"
}


@InProceedings{Wengetal2019,  
title =  {{PROVEN}: Verifying Robustness of Neural Networks with a Probabilistic Approach},  
author =       {Weng, Lily and Chen, Pin-Yu and Nguyen, Lam and Squillante, Mark and Boopathy, Akhilan and Oseledets, Ivan and Daniel, Luca},  
booktitle =  {Proceedings of the 36th International Conference on Machine Learning},  
pages =  {6727--6736},  year =  {2019},  
editor =  {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},  volume =  {97},  
series =  {Proceedings of Machine Learning Research},  
month =  {09--15 Jun},  
publisher =    {PMLR},  
pdf =  {http://proceedings.mlr.press/v97/weng19a/weng19a.pdf},  url =  {https://proceedings.mlr.press/v97/weng19a.html},  abstract =  {We propose a novel framework PROVEN to \textbf{PRO}babilistically \textbf{VE}rify \textbf{N}eural network’s robustness with statistical guarantees. PROVEN provides probability certificates of neural network robustness when the input perturbation follow distributional characterization. Notably, PROVEN is derived from current state-of-the-art worst-case neural network robustness verification frameworks, and therefore it can provide probability certificates with little computational overhead on top of existing methods such as Fast-Lin, CROWN and CNN-Cert. Experiments on small and large MNIST and CIFAR neural network models demonstrate our probabilistic approach can tighten up robustness certificate to around $1.8 \times$ and $3.5 \times$ with at least a $99.99%$ confidence compared with the worst-case robustness certificate by CROWN and CNN-Cert.}}

@misc{wine_109,
  author       = {Aeberhard, Stefan and Forina, M.},
  title        = {{Wine}},
  year         = {1992},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C5PC7J}
}

@Article{Wu2005,
  author={Wu, Hsin-i},
  title={{A case study of type 2 diabetes self-management}},
  journal={PubMed},
  year=2005,
  volume={4},
  number={Win},
  pages={4},
  month={},
  publisher={BioMed Central},
  doi={},
  
}

@InProceedings{Wuetal_2024,
author="Wu, Haoze
and Isac, Omri
and Zelji{\'{c}}, Aleksandar
and Tagomori, Teruhiro
and Daggitt, Matthew
and Kokke, Wen
and Refaeli, Idan
and Amir, Guy
and Julian, Kyle
and Bassan, Shahaf
and Huang, Pei
and Lahav, Ori
and Wu, Min
and Zhang, Min
and Komendantskaya, Ekaterina
and Katz, Guy
and Barrett, Clark",
editor="Gurfinkel, Arie
and Ganesh, Vijay",
title="Marabou 2.0: A Versatile Formal Analyzer of Neural Networks",
booktitle="Computer Aided Verification",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="249--264",
abstract="This paper serves as a comprehensive system description of version 2.0 of the Marabou framework for formal analysis of neural networks. We discuss the tool's architectural design and highlight the major features and components introduced since its initial release.",
isbn="978-3-031-65630-9"
}



@book{Xiu2010,
 ISBN = {9780691142128},
 URL = {http://www.jstor.org/stable/j.ctv7h0skv},
 abstract = {The@ first graduate-level textbook to focus on fundamental aspects of numerical methods for stochastic computations, this book describes the class of numerical methods based on generalized polynomial chaos (gPC). These fast, efficient, and accurate methods are an extension of the classical spectral methods of high-dimensional random spaces. Designed to simulate complex systems subject to random inputs, these methods are widely used in many areas of computer science and engineering.The book introduces polynomial approximation theory and probability theory; describes the basic theory of gPC methods through numerical examples and rigorous development; details the procedure for converting stochastic equations into deterministic ones; using both the Galerkin and collocation approaches; and discusses the distinct differences and challenges arising from high-dimensional problems. The last section is devoted to the application of gPC methods to critical areas such as inverse problems and data assimilation.Ideal for use by graduate students and researchers both in the classroom and for self-study,Numerical Methods for Stochastic Computations provides the required tools for in-depth research related to stochastic computations.The first graduate-level textbook to focus on the fundamentals of numerical methods for stochastic computations Ideal introduction for graduate courses or self-study Fast, efficient, and accurate numerical methods Polynomial approximation theory and probability theory included Basic gPC methods illustrated through examples},
 author = {Dongbin Xiu},
 publisher = {Princeton University Press},
 title = {Numerical Methods for Stochastic Computations: A Spectral Method Approach},
 urldate = {2022-04-28},
 year = {2010}
}



@article{XiuKarniadakis2002a,
author = {Xiu, Dongbin and Karniadakis, George},
title = {The {W}iener-{A}skey Polynomial Chaos for Stochastic Differential Equations},
year = {2002},
issue_date = {2002},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
volume = {24},
number = {2},
issn = {1064-8275},
doi = {10.1137/S1064827501387826},
journal = {SIAM J. Sci. Comput.},
month = Feb,
pages = {619–644},
}

@article{XiuKarniadakis2002b,
title = {Modeling uncertainty in steady state diffusion problems via generalized polynomial chaos},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {191},
number = {43},
pages = {4927-4948},
year = {2002},
issn = {0045-7825},
doi = {https://doi.org/10.1016/S0045-7825(02)00421-8},
url = {https://www.sciencedirect.com/science/article/pii/S0045782502004218},
author = {Dongbin Xiu and George E Karniadakis},
keywords = {Uncertainty, Random diffusion, Polynomial chaos},
abstract = {We present a generalized polynomial chaos algorithm for the solution of stochastic elliptic partial differential equations subject to uncertain inputs. In particular, we focus on the solution of the Poisson equation with random diffusivity, forcing and boundary conditions. The stochastic input and solution are represented spectrally by employing the orthogonal polynomial functionals from the Askey scheme, as a generalization of the original polynomial chaos idea of Wiener [Amer. J. Math. 60 (1938) 897]. A Galerkin projection in random space is applied to derive the equations in the weak form. The resulting set of deterministic equations for each random mode is solved iteratively by a block Gauss–Seidel iteration technique. Both discrete and continuous random distributions are considered, and convergence is verified in model problems and against Monte Carlo simulations.}
}


@article{XiuKarniadakis2003a,
title = {A new stochastic approach to transient heat conduction modeling with uncertainty},
journal = {International Journal of Heat and Mass Transfer},
volume = {46},
number = {24},
pages = {4681-4693},
year = {2003},
issn = {0017-9310},
doi = {https://doi.org/10.1016/S0017-9310(03)00299-0},
url = {https://www.sciencedirect.com/science/article/pii/S0017931003002990},
author = {Dongbin Xiu and George E Karniadakis},
keywords = {Uncertainty, Stochastic modeling, Polynomial chaos, Transient heat conduction, Random medium},
abstract = {We present a generalized polynomial chaos algorithm for the solution of transient heat conduction subject to uncertain inputs, i.e. random heat conductivity and capacity. The stochastic input and solution are represented spectrally by the orthogonal polynomial functionals from the Askey scheme, as a generalization of the original polynomial chaos idea of Wiener [Am. J. Math. 60 (1938) 897]. A Galerkin projection in random space is applied to derive the equations in the weak form. The resulting set of deterministic equations is subsequently discretized by the spectral/hp element method in physical space and integrated in time. Numerical examples are given and the convergence of the chaos expansion is demonstrated for a model problem.}
}

@article{XiuKarniadakis2003b,
title = {Modeling uncertainty in flow simulations via generalized polynomial chaos},
journal = {Journal of Computational Physics},
volume = {187},
number = {1},
pages = {137-167},
year = {2003},
issn = {0021-9991},
doi = {https://doi.org/10.1016/S0021-9991(03)00092-5},
url = {https://www.sciencedirect.com/science/article/pii/S0021999103000925},
author = {Dongbin Xiu and George Em Karniadakis},
keywords = {Polynomial chaos, Uncertainty, Fluids, Stochastic modeling},
abstract = {We present a new algorithm to model the input uncertainty and its propagation in incompressible flow simulations. The stochastic input is represented spectrally by employing orthogonal polynomial functionals from the Askey scheme as trial basis to represent the random space. A standard Galerkin projection is applied in the random dimension to obtain the equations in the weak form. The resulting system of deterministic equations is then solved with standard methods to obtain the solution for each random mode. This approach can be considered as a generalization of the original polynomial chaos expansion, first introduced by Wiener [Am. J. Math. 60 (1938) 897]. The original method employs the Hermite polynomials (one of the 13 members of the Askey scheme) as the basis in random space. The algorithm is applied to micro-channel flows with random wall boundary conditions, and to external flows with random freestream. Efficiency and convergence are studied by comparing with exact solutions as well as numerical solutions obtained by Monte Carlo simulations. It is shown that the generalized polynomial chaos method promises a substantial speed-up compared with the Monte Carlo method. The utilization of different type orthogonal polynomials from the Askey scheme also provides a more efficient way to represent general non-Gaussian processes compared with the original Wiener–Hermite expansions.}
}

@InProceedings{Xiuetal2003,
author="Xiu, Dongbin
and Lucor, Didier
and Su, C.-H.
and Em Karniadakis, George",
editor="Sloot, Peter M. A.
and Abramson, David
and Bogdanov, Alexander V.
and Gorbachev, Yuriy E.
and Dongarra, Jack J.
and Zomaya, Albert Y.",
title="Performance Evaluation of Generalized Polynomial Chaos",
booktitle="Computational Science --- ICCS 2003",
year="2003",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="346--354",
abstract="In this paper we review some applications of generalized polynomial chaos expansion for uncertainty quantification. The mathematical framework is presented and the convergence of the method is demonstrated for model problems. In particular, we solve the first-order and second-order ordinary differential equations with random parameters, and examine the efficiency of generalized polynomial chaos compared to Monte Carlo simulations. It is shown that the generalized polynomial chaos can be orders of magnitude more efficient than Monte Carlo simulations when the dimensionality of random input is low, e.g. for correlated noise.",
isbn="978-3-540-44864-8"
}


@article{Xuetal_2020,
  title={Automatic Perturbation Analysis on General Computational Graphs},
  author={Kaidi Xu and Zhouxing Shi and Huan Zhang and Minlie Huang and Kai-Wei Chang and Bhavya Kailkhura and X. Lin and Cho-Jui Hsieh},
  journal={ArXiv},
  year={2020},
  volume={abs/2002.12920},
  url={https://api.semanticscholar.org/CorpusID:211572699}
}

@article{Yurtseveretal2020,
title={ A Survey of Autonomous Driving: Common Practices and Emerging Technologies },
author={Ekim Yurtsever and Jacob Lambert and Alexander Carballo and Kazuya Takeda },
journal={ IEEE Access },
year={ 2020 },
publisher={ IEEE },
volume={ 8 },
pages={ 58443-58469 },
doi={ 10.1109/ACCESS.2020.2983149 },  
}

@book{Ziegler_1995,
  address = {New York},
  author = {Ziegler, Günter M.},
  booktitle = {Graduate texts in mathematics, 152},
  description = {Lectures on Polytopes},
  publisher = {Springer-Verlag},
  title = {Lectures on polytopes},
  year = 1995
}


@inproceedings{Zhangetal2018, 
author = {Zhang, Huan and Weng, Tsui-Wei and Chen, Pin-Yu and Hsieh, Cho-Jui and Daniel, Luca}, title = {Efficient neural network robustness certification with general activation functions}, year = {2018}, publisher = {Curran Associates Inc.}, address = {Red Hook, NY, USA}, abstract = {Finding minimum distortion of adversarial examples and thus certifying robustness in neural network classifiers for given data points is known to be a challenging problem. Nevertheless, recently it has been shown to be possible to give a non-trivial certified lower bound of minimum adversarial distortion, and some recent progress has been made towards this direction by exploiting the piece-wise linear nature of ReLU activations. However, a generic robustness certification for general activation functions still remains largely unexplored. To address this issue, in this paper we introduce CROWN, a general framework to certify robustness of neural networks with general activation functions for given input data points. The novelty in our algorithm consists of bounding a given activation function with linear and quadratic functions, hence allowing it to tackle general activation functions including but not limited to four popular choices: ReLU, tanh, sigmoid and arctan. In addition, we facilitate the search for a tighter certified lower bound by adaptively selecting appropriate surrogates for each neuron activation. Experimental results show that CROWN on ReLU networks can notably improve the certified lower bounds compared to the current state-of-the-art algorithm Fast-Lin, while having comparable computational efficiency. Furthermore, CROWN also demonstrates its effectiveness and flexibility on networks with general activation functions, including tanh, sigmoid and arctan.}, booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems}, pages = {4944–4953}, numpages = {10}, location = {Montr\'{e}al, Canada}, series = {NIPS'18},
url = {https://dl.acm.org/doi/abs/10.5555/3327345.3327402}}


@inproceedings{Zhangetal2020,
title={Towards Stable and Efficient Training of Verifiably Robust Neural Networks},
author={Huan Zhang and Hongge Chen and Chaowei Xiao and Sven Gowal and Robert Stanforth and Bo Li and Duane Boning and Cho-Jui Hsieh},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=Skxuk1rFwB}
}

@article{ZhangShin2021,
title = {An adaptive Gaussian mixture method for nonlinear uncertainty propagation in neural networks},
journal = {Neurocomputing},
volume = {458},
pages = {170-183},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221009024},
author = {Bin Zhang and Yung C. Shin},
keywords = {Neural networks, Nonlinear uncertainty propagation, Gaussian mixture model, Dynamic systems},
abstract = {Using neural networks to address data-driven problems often entails dealing with uncertainties. However, the propagation of uncertainty through a network’s nonlinear layers is usually a bottleneck, since the existing techniques designed to transmit Gaussian distributions via moment estimation are not capable of predicting non-Gaussian distributions. In this study, a Gaussian-mixture-based uncertainty propagation scheme is proposed for neural networks. Given that any input uncertainty can be characterized as a Gaussian mixture with a finite number of components, the developed scheme actively examines each mixture component and adaptively split those whose fidelity in representing uncertainty is deteriorated by the network’s nonlinear activation layers. A Kullback–Leibler criterion that directly measures the nonlinearity-induced non-Gaussianity in post-activation distributions is derived to trigger splitting and a set of high-precision Gaussian splitting libraries is established. Four uncertainty propagation examples on dynamic systems and data-driven applications are demonstrated, in all of which the developed scheme exhibited exemplary fidelity and efficiency in predicting the evolution of non-Gaussian distributions through both recurrent and multi-layer neural networks.}
}