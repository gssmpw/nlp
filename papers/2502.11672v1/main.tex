%\documentclass[acmsmall,screen,anonymous,review]{acmart}
\documentclass{article}
%\usepackage[nohyperref]{icml2025}
\usepackage[nohyperref,accepted]{icml2025}
\usepackage{graphicx,subcaption}
\usepackage{microtype}
\usepackage{hyperref}
%\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{algorithm}
%\usepackage{algorithmic}

\usepackage{amsmath,bm,esint}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage[capitalize,noabbrev]{cleveref}

%\documentclass[acmlarge,screen,nonacm]{acmart}

%\usepackage{algpseudocode}
\usepackage{comment}

\usepackage{tikz}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\usepackage[font=small,labelfont=bf]{caption}
\usepackage[clock]{ifsym}

%\usepackage{comment}



%\usepackage[ruled]{algorithm2e} % For algorithms




%\usepackage{setspace}
%\usepackage{algpseudocode}


\usepackage{blkarray, bigstrut}
\usepackage{xparse}
%\usepackage{xcolor}
\usepackage{rotating}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{tablefootnote}

\usepackage{booktabs}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}

\usepackage{threeparttable}
\usepackage{pifont}

\usepackage{array}
\usepackage{colortbl}
\usepackage{wrapfig}

\usepackage{lstautogobble}

\definecolor{light-gray}{gray}{0.95}
\definecolor{tbl-row-color}{gray}{0.95}
\definecolor{pastelred}{rgb}{1.0, 0.41, 0.38}
\definecolor{radicalred}{rgb}{1.0, 0.21, 0.37}
\usepackage{listings}

\definecolor{keywordsColor}{RGB}{134, 14, 11}
\definecolor{commentsColor}{RGB}{54, 54, 54}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\interior}[1]{%
  {\kern0pt#1}^{\mathrm{o}}%
}

\DeclareMathOperator{\sign}{sign}

\newcommand{\E}{\mathbb{E}}
%\newcommand{\Es}{\mbox{\scriptsize{E}}}
\newcommand{\var}{\mathbb{V}\mathrm{ar}} %% Use for variances
\newcommand{\cov}{\mathbb{C}\mathrm{ov}}
\newcommand{\pr}{\mathbb{P}}

\newcommand{\tr}{\operatorname{tr}}
\newcommand{\vecc}{\operatorname{vec}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\unvecc}{\operatorname{unvec}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\spn}{\operatorname{span}}
\newcommand{\spc}{{\mathcal S}}
\newcommand{\real}{{\mathbb R}}
\newcommand{\nat}{{\mathbb N}}

\newcommand{\gc}{\mathrm{GC}}
\newcommand{\mm}{\mathrm{MM}}
\newcommand{\ks}{\mathrm{KS}}

%%%%% Boldface letters %%%%%%%%%%%

\newcommand{\A}{{\mathbf A}}
\newcommand{\X}{{\mathbf X}}
\newcommand{\Y}{{\mathbf Y}}
\newcommand{\y}{{\mathbf y}}
\newcommand{\pY}{{\widetilde{\mathbf Y}}}
\newcommand{\D}{{\mathbf D}}
\newcommand{\Z}{{\mathbf Z}}
\newcommand{\z}{{\mathbf z}}
\newcommand{\V}{{\mathbf V}}
\newcommand{\U}{{\mathbf U}}
\newcommand{\W}{{\mathbf W}}
\newcommand{\Hb}{{\mathbf H}}
\newcommand{\Eb}{{\mathbf E}}
\newcommand{\Sb}{{\mathbf S}}
\newcommand{\Pb}{{\mathbf P}}
\newcommand{\F}{{\mathbf F}}
\newcommand{\M}{{\mathbf M}}
\newcommand{\cb}{{\mathbf c}}

\newcommand{\x}{{\mathbf x}}

\newcommand{\f}{{\mathbf f}}
\newcommand{\ub}{{\mathbf u}}
\newcommand{\w}{{\mathbf w}}
\newcommand{\vb}{{\mathbf v}}
\newcommand{\mbf}{\mathbf{m}}
\newcommand{\hbf}{{\mathbf h}}

%%%% Calligraphic 
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Pcal}{\mathcal{P}}
\newcommand{\Qcal}{\mathcal{Q}}

%%%%% Boldface Greek %%%%%%%%%%%

\newcommand{\phibf}{{\bm \phi}}
\newcommand{\psibf}{{\bm \psi}}
\newcommand{\zetabf}{{\bm \zeta}}

\newcommand{\Lambdabf}{{\bm \Lambda}}


\newcommand{\bb}{{\mathbf b}}
\newcommand{\0}{{\mathbf 0}}

\newcommand{\epsilonbf}{{\bm \epsilon}}
\newcommand{\Thetabf}{{\bm \Theta}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{Assumption}[theorem]{Assumption}
\newtheorem{condition}{Condition}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}
%\newtheorem*{example*}{Example}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% See https://en.wikibooks.org/wiki/LaTeX/Colors for color definitions

\definecolor{BrickRed}{HTML}{B6321C}
\definecolor{BlueViolet}{HTML}{473992}
\definecolor{Maroon}{HTML}{AF3235}
\definecolor{ForestGreen}{HTML}{009B55}


\newcommand{\efi}[1]{{\color{radicalred}#1}}
\newcommand{\andrey}[1]{{\color{teal}#1}}
\newcommand{\marcel}[1]{{\color{BlueViolet}#1}}
\newcommand{\miroslav}[1]{{\color{Maroon}#1}}
\newcommand{\ezio}[1]{{\color{blue}#1}}
\newcommand{\new}[1]{{\color{ForestGreen}#1}}
\newcommand{\daniel}[1]{{\bf\color{red!70!black}#1}}

\newcommand{\revision}[1]{{\color{red}#1}}

\icmltitlerunning{Exact Upper and Lower Bounds for the Output Distribution of NNs with Random Inputs}
\begin{document}


\twocolumn[
\icmltitle{Exact Upper and Lower Bounds for the Output Distribution \\of Neural Networks with Random Inputs}

\icmlsetsymbol{equal}{*}
\begin{icmlauthorlist}
\icmlauthor{Andrey Kofnov}{math}
\icmlauthor{Daniel Kapla}{math}
\icmlauthor{Ezio Bartocci}{comp}
\icmlauthor{Efstathia Bura}{math}
\end{icmlauthorlist}
\icmlaffiliation{math}{Institute of Statistics and Mathematical Methods in Economics, Faculty of Mathematics and Geoinformation, TU Wien, Vienna, Austria}
\icmlaffiliation{comp}{Faculty of Informatics, TU Wien, Vienna, Austria}
\icmlcorrespondingauthor{Andrey Kofnov}{andrey.kofnov@tuwien.ac.at
}
\icmlkeywords{Neural Networks, Uncertainty Propagation, Predictive Output Distribution, Guaranteed Bounds}
\vskip 0.3in
]
\printAffiliationsAndNotice{}
%]

\begin{abstract} 
%Lack of uncertainty quantification prevents neural networks from being deployed in safety-critical applications despite their potential. 
We derive exact upper and lower bounds for the cumulative distribution function (cdf)  of the output of a neural network over its entire support subject to noisy (stochastic) inputs. 
The upper and lower bounds converge to the true cdf over its domain as the resolution increases. 
Our method applies to any feedforward NN using continuous monotonic piecewise differentiable activation functions (e.g.,  ReLU, tanh and softmax) and convolutional NNs, which were beyond the scope of competing approaches. The novelty and an instrumental tool of our approach is to bound general NNs with ReLU NNs. %any continuous function of a random vector can be approximated with a ReLU NN and subsequently compute guaranteed upper and lower bounds on its cdf. 
The ReLU NN based bounds are then used to derive upper and lower bounds of the cdf of the NN output. 
%(a) NNs can approximate any continuous cdf with bounded support and that (b) upper and lower bounds to cdfs can be computed using two ReLU NNs. 
Experiments demonstrate that our method delivers guaranteed bounds of the predictive output distribution over its support, thus providing exact error guarantees, in contrast to competing approaches. 
%Probabilistic robustness verification evaluates how vulnerable a neural network is to changes in input data that can result in incorrect outputs in real-world situations, where inputs are subject to uncertainty and randomness. Instead of testing the network against a predefined set of inputs, probabilistic methods consider a distribution over possible inputs and evaluate how well the network performs across this distribution. We present a new approach to computing highly accurate estimators and derive exact bounds on the true distribution of output of a broad class of widely used neural networks subject to given random perturbations in input nodes.
\end{abstract}
%\maketitle



\section{Introduction}

%Neural Network (NN) based machine learning methods have been very successful in image recognition, natural language processing, speech recognition, robotics, strategic games, etc. \cite{Schmidhuber2015} offers a historical survey of neural networks, starting from 1943 \cite{McCullochPitts1943}. %The first neural network (NN) was proposed in 1943 by McCulloch & Pitts [4]. The field expanded until the early 1960s, when the existing algorithms proved inefficient and unstable. The invention of backpropagation in 1970 [5,6] led to a second wave of ML applications as it became possible to build more extensive NNs and train them to recognize nonlinear relationships in data (e.g. [7]). Even though the development of ML algorithms continued, the enthusiasm about them soon dwindled again, because they rarely showed significant performance gain, and computing resources were not sufficient to solve larger problems. Furthermore, big amounts of labelled data which are mandatory for most data-driven ML approaches were hard to come by (note that this was before the advent of the world wide web). Three significant developments around 2010 started the third wave of artificial intelligence, which continues to the present: computing capabilities were vastly expanded due to massive parallel processing in graphical processing units (GPUs), convolutional neural networks (CNN) allowed much more efficient analysis of massive (image) datasets, and large benchmark datasets were made available on the internet. Highly complex NNs with greater than 106 parameters enabled a breakthrough in image recognition [8], soon followed by remarkable success stories in speech recognition [9], gaming [10,11], and video analysis and prediction [12,13]. 
Increased computational power, availability of large datasets, and the rapid development of new NN architectures contribute to the ongoing success of neural network (NN) based learning in image recognition, natural language processing, speech recognition, robotics, strategic games, etc.
A limitation of NN machine learning (ML) approaches is that it is difficult to infer the uncertainty of the predictions from their results: there is no internal mechanism in NN learning to assess how trustworthy the network outputs are with \textit{ not seen} data. A NN is a model of the form: 
\begin{align}\label{eq:reg-model} 
%Y&=\mathcal{W}(X) = f(X, \Thetabf) + \epsilon_{Y}, \\ 
Y&=f(X, \Thetabf),
\end{align}
where $Y$ is the output and $X$ the input (typically multivariate), %$\mathcal{W}(X)$ is an assumed unknown true relationship between $X$ and $Y$, 
and $f$ is a \textit{known} function modeling the relationship between $X$ and $Y$ parametrized by $\Theta$. Model \eqref{eq:reg-model} incorporates uncertainty neither in $Y$ nor in $X$ and NN fitting is a numerical algorithm for minimizing a loss function. %(typically, minimize some function of error term $\epsilon_{Y}$ between $Y$ and $\widetilde{Y}$) applied to sequential applications of known functions on affine transformations of observed data. 
%Attempts to introduce stochasticity to NNs include variational auto-encoders (VAE) \cite{KingmaWelling2022} and generative adversarial networks (GAN) \cite{Goodfellowetal2014}. Both of these are \textit{generative} models in that they learn the data distributions from training samples and use generators to produce novel samples that match the characteristics of the training data. 
Lack of uncertainty quantification, such as assessment mechanisms for prediction accuracy beyond the training data, prevents neural networks, despite their potential, from being deployed in safety-critical applications ranging from medical diagnostic systems (e.g., \cite{HafizBhat2020}) to cyber-physical systems such as autonomous vehicles, robots or drones (e.g., \cite{Yurtseveretal2020}). Also, the deterministic nature of NNs renders them highly vulnerable to not only adversarial noise but also to even small perturbations in inputs  (\cite{Bibietal2018,Fawzietal2018, Goodfellowetal2014,Goodfellowetal2014b,Hosseinietal2017}). %(Fawzi et al., 2016) show random perturbations can indeed fool VGG networks; (Hosseini et al., 2017) show they fool the Google Cloud Vision API by random Gaussian noises, suggesting random perturbation can result in successful attacks. 

Uncertainty in modeling is typically classified as \textit{epistemic} or \textit{systematic}, which derives from lack of knowledge of the model, and \textit{aleatoric} or \textit{statistical},  which reflects the inherent randomness in
the underlying process being modeled (see, e.g., \cite{HuellermeierWaegeman2021}). The \textit{universal approximation theorem} (UAT) \cite{Cybenko1989,Horniketal1989} states that a neural network with one hidden layer can approximate any continuous function for inputs within a specific range by increasing the number of neurons. In the context of NNs, epistemic uncertainty is of secondary importance to aleatoric uncertainty. Herein, we focus on studying the effect of random inputs on the output distribution of NNs and derive uniform upper and lower bounds for the cumulative distribution function (cdf) of the outputs of a NN subject to noisy (stochastic) input data. 

We evaluate our proposed framework on three benchmark  datasets (Iris \cite{Fisher_1936}, Wine \cite{wine_109} and Diabetes \cite{Efronetal_2004})
and demonstrate the efficacy of our approach to bound the cdf of the NN output subject to Gaussian and Gaussian mixture inputs.  %The computation time is \efi{competitive? }. 
We demonstrate that our bounds cover the true underlying cdf over its entire support. In contrast, the directly competing approach of \citet{Krapfetal2024}, as well as high-sample Monte-Carlo simulations, produce estimates outside the bounds over several areas of the output range. 


\section{Statement of the Problem}

\begin{comment}
\paragraph{Notation:}

\begin{enumerate}

\item $X$ - datapoint (belongs to the dataset) of predictors
\item $Y$ - True output variable.
\item $f(\cdot) = f_{L}(\cdot)$ - function which represents a NN
\item $\widetilde{Y} = f(X)$ - prediction of $Y$, result of application of a NN to a dataset
\item $n_{0}$ - dimensionality of input vector $X$ = number of neurons in layer $0$
\item $n_{L}$ - dimensionality of output vector $Y$ = number of neurons in layer $L$
\item Layer $L$ can be a rescaling function ($sigmoid$, $softmax$), a linear function, or a typical activation function (ReLU, $tanh$, etc) - without loss of generality
\item $L + 1$-layer - application of a decision function (for example, argmax)
\item $\W_{l}$ - matrix of weights which is applied to the output of $l-1$-th layer in the $l$-th layer
\item $\bb_{l}$ - vector of biases which is added to the weighted sum of outputs of $l-1$-th layer in the $l$-th layer
\item $\Thetabf$ - set of parameters of the NN (weights and biases)
\item $\sigma^{l}$ - activation function which is applied in the $l$-th layer component-wise. 
\item $\X$ - random input vector distributed as $F_{\X}$
\item $\pY$ - random output vector distributed as $F_ {\pY}$
\end{enumerate}


\begin{enumerate}

\item $X$ - datapoint (belongs to the dataset) of predictors
\item $Y$ - True output variable.
\item $f(\cdot) = f_{L}(\cdot)$ - function which represents a NN
\item $\widetilde{Y} = f(X)$ - prediction of $Y$, result of application of a NN to a dataset
\item $n_{0}$ - dimensionality of input vector $X$ = number of neurons in layer $0$
\item $n_{L}$ - dimensionality of output vector $Y$ = number of neurons in layer $L$
\item Layer $L$ can be a rescaling function ($sigmoid$, $softmax$), a linear function, or a typical activation function (ReLU, $tanh$, etc) - without loss of generality
\item $L + 1$-layer - application of a decision function (for example, argmax)
\item $\W_{l}$ - matrix of weights which is applied to the output of $l-1$-th layer in the $l$-th layer
\item $\bb_{l}$ - vector of biases which is added to the weighted sum of outputs of $l-1$-th layer in the $l$-th layer
\item $\Thetabf$ - set of parameters of the NN (weights and biases)
\item $\sigma^{l}$ - activation function which is applied in the $l$-th layer component-wise. 
\item $\X$ - random input vector distributed as $F_{\X}$
\item $\pY$ - random output vector distributed as $F_ {\pY}$
\end{enumerate}

%The project aims to quantify the distribution of the output of a neural network (NN) resulting from randomness in the input. 
\end{comment}

A neural network is a mathematical model that produces outputs from inputs.  The input is typically a vector of predictor variables, $\X \in \real^{n_0}$, and the output $Y$, is univariate or multivariate, continuous or categorical.  %$f: \real^{n} \rightarrow \{0,1,\ldots,k-1\}$ 

%$\{X_{tr}, Y_{tr}\} \in (\real^{p \times n}, \{0,1,\ldots,k-1\}^{p \times 1})$ to solve the $k-$class classification problem, where $p$ is a size of the dataset. 
%The NN $f$ has $L$ consecutive hidden layers characterized by a vector-valued  $\sigma_{l}$ consisting of $n_{l}$ continuous monotonically increasing activation functions, %(until otherwise mentioned, further we consider only ReLU functions) intercepts $b_{l} \in \real^{n_{l+1}, 1}$ and weight matrices $W_{l} \in \real^{n_{l+1}, n_{l}}, l \in \{0,\ldots,L\}$. 
 
 
% are pre-trained vectors of intercepts and matrices of weighted, respectively. 
A \emph{feedforward NN} with $L$ layers from $\real^{n_{0}} \to \real^{n_L}$ is a composition of $L$ functions,
\begin{align}\label{MLP}
	     f_L(\x; \Thetabf) &= f^{(L)}\circ f^{(L-1)} \circ ... \circ f^{(1)} (\x),
	     %&= f^{(N)}(\ldots, f^{(2)}(f^{(1)}(\xn; \W_1, \bb_1); \W_2, \bb_2); \W_N, \bb_N),
\end{align}
where the $l$-th layer is given by
\begin{displaymath}\label{eq:layer}
		f^{(l)}(\x; \W^{(l)}, \bb^{(l)}) = \sigma^{(l)}( \W^{(l)}\x + \bb^{(l)}),
\end{displaymath}
with weights $\W^{(l)}\in\mathbb{R}^{n_{l}\times n_{l-1}}$, bias terms $\bb^{(l)}\in\mathbb{R}^{n_l}$, and a non-constant, continuous 
activation function $\sigma^{(l)}:\real \to \real$ that is applied component-wise. The NN parameters are collected in  $\Thetabf = (\text{vec}(\W_1)$, $\bb_1,\ldots$, $\text{vec}(\W_L)$, $\bb_L) \in \real^{\sum_{l=1}^L (n_{l-1}\cdot n_l +n_l)}$. \footnote{The operation $\text{vec}:\real^{n_{l-1} \times n_l} \to \real^{n_{l-1}\cdot n_l}$ stacks the columns of a matrix one after another.} 
%In general, an \texttt{MLP} allows for multi-dimensional output, but for the sake of simplicity we only consider univariate responses and therefore only univariate outputs in the following.
The first layer that receives the input $\x$ is called the \emph{input layer}, and the last layer is the  \emph{output layer}. All other layers are called \emph{hidden}. %A common convention is to only count the hidden in  the number of layers in NN, so that an $L$ layer NN consists of $L+1$ layers. 
%A widely used activation function is the so called \texttt{ReLU} (Rectified Linear Unit) given by
%\begin{equation*}
 %   \phi_{\texttt{ReLU}}(x) = \max(0, x).
%\end{equation*}
%The \texttt{ReLu} activation function will be used throughout this paper. Other popular choices include sigmoid functions like the tangens-hyperbolicus.
For categorical outputs, %$y \in \{1,\ldots,n_{L}\}$, 
the class label is assigned by a final application of a decision function, such as $\mathrm{arg\,max}$. 
%the NN is $f_{L+1}=D\circ f_L $, where  $D: \real^{n_{L}} \rightarrow \{1,\ldots,n_{L}\}$ is a decision function, such as $arg\,max$, that assigns the class label $k$ with the highest probability.


Despite not being typically acknowledged, % are inherently deterministic in that they do not incorporate  randomness in the data. %They differ from statistical models in that they do not incorporate a source of randomness in the data. %In standard NN training, randomness occurs from stochastic gradient descent. 
the training data in NNs are drawn from larger populations, and hence they contain only limited information about the corresponding population. We incorporate the uncertainty associated with the observed data assuming that they are random draws from an unknown distribution of bounded support. That is, the data are comprised of $m$ draws from the joint distribution of $(\X,Y)$, and the network is trained on observed $(\x_i,y_i)$, $\x_i=(x_{i1}, x_{i2}, \ldots, x_{i n_0})$, and $y_i$, $i=1,\ldots, $$m$. \footnote{We use the convention of denoting random quantities with capital letters and their realizations (observed) by lowercase letters.}%. We establish robustness guarantees in a given neural network to random input perturbation. Specifically, 
A NN with $L$ layers and $n_{l}$ neurons at each layer, $l=1,\ldots, L$,  is trained on the observed $(\x_i,y_i)$, $i=1,\ldots, m$, to produce $m$ outputs $\tilde{y}_i$, and the vector of the NN parameters, $\Thetabf = \left(\vecc(\W_1), \bb_1,\ldots, \vecc(\W_{L}), \bb_{L}\right)$, is obtained. %$ \in \real^{\sum_{j=0}^L n_{l-1}n_l +n_l}$.
$\Thetabf$ uniquely identifies the trained NN. Given $\Thetabf$, we aim to quantify the robustness of the corresponding NN, to perturbations in the input variables. For this, we let 
\begin{align}\label{eq:X_distr}
    \X \sim F_{\X},
\end{align}
where $\X \in \real^{n_{0}}$ stands for the randomly perturbed input variables with cdf $F_{\X}$ and probability density function (pdf) $\phi(\x)$ that is piecewise continuous and bounded on a compact support. 
%which contains the set of the observed input values and whose  either admits a neural network form, or is a piece-wise polynomial (Beta, Uniform, Dirichlet, etc.).
We study the \textit{propagation of uncertainty} (effect of the random perturbation) in the NN by deriving upper and lower bounds of the cdf $F_{\pY}(y)=\pr(\pY \le y)$ of the \textit{random} output, $\pY=f_L(\X\mid \Thetabf)$.\footnote{The notation $f_L(\X\mid \Thetabf)$ signifies that $\Thetabf$, equivalently the NN, is fixed and only $\X$ varies.} 


\paragraph{Our contributions:}
\begin{enumerate} 
\item We develop a method to compute the exact cdf of the output of ReLU NNs with random input pdf, which is a piecewise polynomial over a compact hyperrectangle. 
%domain, specifically a union of convex bounded polytopes. 
This result, which can be viewed as a stochastic analog to the Stone-Weierstrass theorem, \footnote{A significant corollary to the Stone-Weierstrass theorem is that any continuous function defined on a compact set can be uniformly approximated as closely as desired by a polynomial.\label{foot:Stone-Weierstrass}}  significantly contributes to the characterization of the distribution of the output of NNs with piecewise-linear activation functions under any input continuous pdf.
\item We derive \textit{guaranteed} upper and lower bounds of the NN output distribution resulting from random input perturbations on a fixed support. This provides \textit{exact} upper and lower bounds for the output cdf provided the input values fall within the specified support. No prior knowledge about the true cdf is required to guarantee the validity of our bounds. 
%\item Our approach applies to all NNs with inputs of any multivariate continuous distribution on a bounded support with arbitrary dependency structure.
%while the joint continuity is preserved.
\item We show the convergence of our bounds to the true cdf; that is, our bounds can be refined to arbitrary accuracy.

\item We provide a constructive proof that any feedforward NN with continuous monotonic \textit{piecewise differentiable}\footnote{A wide class of the most common continuous functions, except for rare cases as the Weierstrass function.} activation functions can be both upper and lower approximated by a fully connected ReLU network, achieving any desired level of accuracy. Moreover, we enable the incorporation of multivariate operations such as $\max$, $\mathrm{product}$ and $\mathrm{softmax}$, as well as some non-monotonic functions such as $|x|$ and $x^{n}, n \in \nat$.
\item We prove a new \textit{universal \textbf{distribution} approximation theorem} (UDAT), which states that we can estimate the cdf of the output of any continuous function of a random variable (or vector) that has a continuous distribution supported on a compact hyperrectangle, achieving any desired level of accuracy.

%\item If the cumulative probability of an output point falls outside the upper and lower bounds then it originates from a different input distribution than the assumed and can correspond to a potential input anomaly. 
%\item Thus, we develop a tool that not only computes approximation bounds for the predictive output distribution, but may also be able to identify potentially unusual or \textit{adversarial} inputs.  
\end{enumerate}





\section{Our Approximation Approach}

We aim to estimate the cdf $F_{\pY}(y)$ of the output $\pY=f_L(\X \mid \Thetabf)$ of the NN in \eqref{MLP} under \eqref{eq:X_distr}; i.e., subject to random perturbations of the input $\X$. %The randomness of $\pY$ derives from $\X$. 
We do so by computing upper and lower bounds of $F_{\pY}$; that is, we compute $\overline{F}_{\pY}$, $\underline{F}_{\pY}$ such that 
\begin{align}
    \underline{F}_{\pY}(y)\le F_{\pY}(y) \le \overline{F}_{\pY}(y), \, \forall y
\end{align}
We refer to the NN in \eqref{MLP} as \textit{prediction NN} when needed for clarity. We estimate the functions $\overline{F}_{\pY}$, $\underline{F}_{\pY}$ on a ``superset'' of the output domain of the prediction NN \eqref{MLP} via an integration procedure. The cdf of $\pY$ is given by
\begin{align}\label{cdf_expr}
    F_{\pY}(y)&=\pr(\pY \le y)=\int_{\{\pY \le y\}} \phi(\x) d\x,
\end{align}
where $\phi(\x)$ is the pdf of $\X$.
To bound $F_{\pY}$, we bound $\phi$ by its upper ($\overline{\phi}$) and lower ($\underline{\phi}$) estimates on the bounded support of $\phi$ as described in Section \ref{sec:cdfapprox}. If $\phi$ is a piecewise polynomial, then \ref{cdf_expr} can be computed exactly for a ReLU prediction network, as we show in Section \ref{sec:cdf_relu_polynomial}. Once  $\overline{\phi}$, $\underline{\phi}$ are estimated, then
\begin{align}
\underline{F}_{\widetilde{Y}}(y)= &\int_{\{\pY \le y\}}  \underline{\phi}(\x) dx \le F_{\pY }(y) \le \\&\int_{\{\pY  \le y\}}  \overline{\phi}(\x) dx=\overline{F}_{\pY} (y)  \notag
\end{align}

\begin{remark}
$\underline{F}_{\pY }(y)$ and $\overline{F}_{\pY }(y)$ are not always true cdfs since we allow the lower estimator not to achieve 1, while the upper bound is allowed to take the smallest value greater than 0.
\end{remark}

%\paragraph{Estimation Details}


\subsection{Exact cdf evaluation for a fully connected NN with ReLU activation function}\label{sec:cdf_relu_polynomial}

\begin{figure}
    \centering
\includegraphics[scale=0.17]{figures_c/Iris_Exact_Empirical_CDF.pdf} 
    \caption{Exact CDF of the ReLU neural network outcome for the class \textit{Setosa} in the Iris problem, assuming Beta-distributed inputs.}
    \label{fig:Iris_cdf}
\end{figure}


% 
\begin{definition}[Almost disjoint sets]
    We say that sets $\mathcal{A}$ and $\mathcal{B}$ are almost disjoint with respect to measure $\alpha$, if $\alpha(\mathcal{A} \cap \mathcal{B}) = 0$.
\end{definition}


\begin{definition}[Closed halfspace] A $n_{0}$-dimensional closed halfspace is a set $H = \{ x \in \real^{n_{0}} | \vb^{T}x \leq c\}$ for $c \in \real$ and some $\vb \in \real^{n_{0}}$, which is called the normal of the halfspace.
\end{definition}

It is known that a convex polytope can be represented as an intersection of halfspaces, called $\mathcal{H}$-representation (\citet{Ziegler_1995}).

\begin{definition}[$\mathcal{H}$-polytope] A $n_{0}$-dimensional $\mathcal{H}$-polytope $\mathcal{C} = \bigcap\limits_{j = 1}^{h} H_{i}$ is the intersection of finitely many closed halfspaces.
\end{definition}

\begin{definition}[Simplex]  A $n_{0}$-dimensional simplex is a $n_{0}$-dimensional polytope with $n_{0}+1$ vertices.
\end{definition}

\begin{definition}[Piecewise polynomial]
    %Let $K \subset \real^{n_{0}}$ be a compact rectangle.  The function $p: K \rightarrow \real^{n_{L}}$ is a piecewise polynomial, if there exists such a finite partition of $K$ into almost disjoint sets of simplices $K = \bigcup\limits_{i=1}^{q}k_{i}$, and there exist standard polynomials $p_{i}: \real^{n_{0}} \rightarrow \real^{n_{L}}$ such that $p(x) = p_{i}(x)$ for all $x \in k_{i}$ and all $i = 1,\ldots,q$.

    A function $p: K \rightarrow \real^{n_{L}}$ is a \emph{piecewise polynomial}, if there exists a finite set of $n_{0}$-simplices such that $K = \bigcup\limits_{i = 1}^{q}k_{i}$ and the function $p$ constrained to the interior $\interior{k}_{i}$ of $k_{i}$ is a polynomial; that is, $p\big|_{\interior{k}_{i}} : \interior{k}_{i} \rightarrow \real^{n_{L}}$ is a polynomial for all $i = 1,\ldots,q$.
\end{definition}
%\begin{remark}
%    We only consider simplices which are almost disjoint, that means for $k_i\cap k_j$ has Lebesque measure zero. Moreover, the domain $K$ is compact in $\real^{n_0}$ since it is bounded and closed.
%\end{remark}
\begin{remark}
We do not require piecewise polynomials to be continuous everywhere on the hyperrectangle. Specifically, we allow discontinuities at the borders of simplices. However, the existence of left and right limits of the function at every point on the bounded support is guaranteed by the properties of standard polynomials.
\end{remark}

\citet{Raghuetal_2017} showed that  ReLU deep networks divide the input domain into activation patterns (see \citet{Sudjianto2020}) that are disjoint convex polytopes $\{\mathcal{C}_{j}\}$ over which the output function is locally represented as the affine transformation $f_L(\x) =  NN^{j}(\x) = \mathbf{c}^{j} + \V^{j}\x $ for $\x \in \{\mathcal{C}_{j}\}$, the number of which grows at the order $\mathcal{O}((\max\{n_{l}\}_{l=1,\ldots,L})^{n_{0} L})$. \citet{Sudjianto2020} outline an algorithm for extracting the full set of polytopes and determining local affine transformations, including the coefficients   $\mathbf{c}^{j}, \V^{j}$ for all $\{\mathcal{C}_{j}\}$, by propagating through the layers. For our computations, we utilize a recent GPU-accelerated algorithm from \citet{Berzins_2023}.

We aim to derive a superset of the range of the network output. For this, we exploit the technique of Interval Bound Propagation (IBP), based on ideas from \cite{Gowaletal_2018, Wangetal_2022,Gehretal2018}. Propagating the $n_{0}$-dimensional box through the network leads to the superset of the range of the network output. We compute the cdf of the network's output at each point of a grid of the superset of the output range.

\begin{theorem}[Exact cdf of ReLU NN w.r.t. piecewise polynomial pdf]\label{thm::exact_cdf}

Let $\pY: \real^{n_{0}} \rightarrow \real^{n_{L}}$ %with $L \in \nat$ 
be a feed-forward ReLU neural network, which splits the input space into a set of almost disjoint polytopes $\{\mathcal{C}_{j}\}_{j=1}^{q_{Y}}$ with local affine transformations $\pY(\x) = NN^{j}(\x)$ for $\x \in \mathcal{C}_{j}$. Let $\phi(\x)$ denote the pdf of the random vector $\X$ that is a piecewise polynomial with local polynomials, $\phi(\x) = \phi_{i}(\x)$ for all $\x \in \interior{k}_{i}$ over an almost disjoint set of simplices $\{k_{i}\}_{i=1}^{q_\phi}$, and a compact hyperrectangle support $K \subset \real^{n_{0}}$. Then, the cdf of $\pY$ is
\begin{align*}
F_{\pY}(\y) = \pr\left[\pY \leq \y\right] &= \sum\limits_{i=1}^{q_{\phi}}\sum\limits_{j=1}^{q_{Y}}\mathcal{I}\left[\phi_{i}(\x) ;\mathcal{C}^{r}_{j,i}\right]\\
&=\sum\limits_{i=1}^{q_{\phi}}\sum\limits_{j=1}^{q_{Y}}\sum\limits_{s=1}^{S_{i,j}}\mathcal{I}\left[\phi_{i}(\x) ;\mathcal{T}_{i,j,s}\right],
\end{align*}
where $\mathcal{I}\left[\phi_{i}(\x) ;\mathcal{T}_{i,j,s}\right]$ is the integral of the local polynomial $\phi_{i}(\x)$ over the simplex $\mathcal{T}_{i,j,s}$ such that the reduced polytope
\begin{align}\label{eq:reduced_polytope}
\mathcal{C}^{r}_{j,i} = \mathcal{C}_{j} \cap k_{i} \cap \{\x: NN^{j}(\x) \leq \y\} = \bigcup\limits_{s=1}^{S_{i,j}}\mathcal{T}_{i,j,s}
\end{align}
is defined by the intersection of polytopes $\mathcal{C}_{j}$ and $k_{i}$, and an intersection of halfspaces 
\begin{align*}
\{\x: NN^{j}(\x) \leq \y\} = \bigcap\limits_{t = 1}^{n_{L}} \left\{\x: NN^{j}_{t}(\x) \leq y_{t}\right\}.
\end{align*}
\end{theorem}

Theorem \ref{thm::exact_cdf} is shown in Appendix \ref{app:thm_proof_Relu_cdf}. The proof  relies on the algorithm for evaluating the integral of a polynomial over a simplex as described in \citet{Lasserre2021}. The right-hand side of (\ref{eq:reduced_polytope}) results from the Delaunay triangulation, dividing the reduced polytope $\mathcal{C}^{r}_{j,i}$ into 
$S_{i,j}$ almost disjoint simplices. 

\begin{remark}
Theorem \ref{thm::exact_cdf} is a tool for approximating the output cdf of any feedforward neural network with piecewise linear activation functions on a compact domain, given random inputs with arbitrary continuous distribution at any desired degree of accuracy (see \ref{foot:Stone-Weierstrass}).\\
\end{remark}

\begin{comment}
$$
NN(L,n_0,\ldots,n_{L+1}) =\{b_1, W_1, \ldots b_{L+1}, W_{L+1} \}$$
be a NN with $L$ hidden layers,
intercepts $b_{l} \in \real^{n_{l}, 1}$ and weight matrices $W_{l} \in \real^{n_{l}, n_{l-1}}, l \in \{1,\ldots,L+1\}$, where $\{n_{l}\}_{l = 1}^{L}$ are numbers of neurons with with ReLU activation functions in each hidden layer, 
\end{comment}




\begin{example}\label{exmp:Iris_cdf}
We compute the output cdf of a 3-layer, 12-neuron  fully connected ReLU neural network with the last (before $\mathrm{softmax}$) linear 3-neuron layer trained on the Iris dataset~\cite{Fisher_1936}. The Iris dataset consists of 150 samples of iris flowers from three different species: Setosa, Versicolor, and Virginica. Each sample includes four features: Sepal Length, Sepal Width, Petal length, and Petal width. We focused on two features, \textit{Sepal Length} and \textit{Sepal Width} (scaled to $[0,1]$), classifying objects into three classes. Specifically, we recovered the distribution of the first component (class \textit{Setosa}) before applying the $\mathrm{softmax}$ function, assuming Beta-distributed inputs with parameters $(2,2)$ and $(3,2)$. The exact cdf is plotted in purple in Figure~\ref{fig:Iris_cdf}, with additional details provided in Appendix~\ref{app:iris}. The agreement with the empirical cdf is almost perfect.
\end{example}


\subsection{Algorithm for Upper and Lower Approximation of the Neural Network using ReLU activation functions.}\label{sec:RelU_approx_algorithm}



\begin{theorem}\label{thm::relu_approx}

Let $\widetilde{Y}$ be a feedforward neural network with $L$  layers of arbitrary width with continuous non-decreasing piecewise differentiable activation functions at each node. There exist sequences of fully connected ReLU neural networks $\{\overline{Y}_{n}\}$, $\{\underline{Y}_{n}\}$ with $L$  layers, which are monotonically decreasing and increasing, respectively, such that for any $\epsilon >0$ and any compact hyperrectangle $K \subset \real^{n_{0}}$, one can find $N \in \nat$ such that for all $n \geq N $
\begin{align*}
0 \leq  \widetilde{Y} (\x)  - \underline{Y}_{n}(\x) < \epsilon, \quad
0 \leq  \overline{Y}_{n}(\x) - \widetilde{Y} (\x)  < \epsilon
\end{align*}
for all $\x \in K$.
\end{theorem}


We develop an approach to approximate a neural network's activation functions and, consequently, the output of the neural network, using the ReLU activation function to create piecewise linear upper and lower bounds. These bounds approximate the true activation function, allowing a more analyzable neural network. This method is a constructive proof of Theorem \ref{thm::relu_approx} and applies to any fully connected or convolution (CNN) feedforward neural network with non-decreasing continuous piecewise differentiable activation functions that is analyzed on a hyperrectangle.
The key features of our approach are:
\begin{itemize}
\item %\textbf{Convexity and Concavity:} 
\textbf{Local adaptability:} The algorithm adapts to the curvature of the activation function, providing an adaptive approximation scheme depending on whether the function is locally convex or concave.

\item \textbf{Streamlining:} By approximating the network with piecewise linear functions, the complexity of analyzing the network output is significantly reduced.

%\item \textbf{ReLU Focus:} The algorithm, optimized for ReLU activations, \efi{easily extends to other piecewise linear functions: I thought you can approximate any continuous differentiable monotone activation function \andrey{Probably, bad text from my side. I meant that we can easily use another type of piecewise linear activation instead ReLU. Maybe we can drop this.}}   and is essential for efficiently analyzing neural networks or bounding their behavior, especially in safety-critical applications where universal affine approximations may be overly coarse.
\end{itemize}

How it works:
%\begin{itemize}
%  \item[a.] 
\paragraph
{Input/Output range evaluation:}
    Using IBP \cite{Gowaletal_2018}, we compute supersets of the input and output ranges of the activation function for every neuron and every layer. 
    %\item[b.] %
    \paragraph{Segment Splitting:} First, input intervals are divided into macro-areas based on inflection points (different curvature areas) and non-differentiable points (e.g., $0$ for ReLU). Next, these macro-areas are subdivided into intervals based on user-specified points or their predefined number within each range.
The algorithm utilizes knowledge about the behavior of the activation function and differentiates between concave and convex regions of the activation function, which impacts how the approximations are constructed and how to choose the points of segment splitting.  A user defines the number of splitting segments and the algorithm ensures the resulting disjoint sub-intervals are properly ordered and on each sub-interval the function is either concave or convex. If the function is linear in a given area, it remains unchanged, with the upper and lower approximations equal to the function itself.
%\item[c.] %\paragraph
\paragraph{Upper and Lower Approximations:} The method constructs tighter upper and lower bounds through the specific choice of points and subsequent linear interpolation. It calculates new points within each interval (one per interval) and uses them to refine the approximation, ensuring the linear segments closely follow the curvature of the activation function.

%\end{itemize}



The method guarantees that the piecewise linear approximation of the activation function for each neuron (a) is a non-decreasing function, and (b) 
the output domain remains the same. %takes the values from the same range as the original activation functions (maintains the support). 

To see this, consider a neuron of a layer. For upper (lower) approximation on a convex (concave) segment, we choose a midpoint $a_{k'} = (a_{k} + a_{k+1}) / 2$ for each subinterval $\left[ a_{k}, a_{k+1}\right]$ and compute a linear interpolation, as follows:
\begin{flalign*}
&\kappa_{1} = \frac{f(a_{k'}) - f(a_{k})}{a_{k'} - a_{k}}, \hspace{0.5cm}\kappa_{2} = \frac{f(a_{k+1}) - f(a_{k'})}{a_{k+1} - a_{k'}},\\
&\widetilde{f}(\tau) = f(a_{k}) + (\tau - a_{k})\kappa_{1},  \hspace{0.45cm}\tau \in \left[ a_{k}, a_{k'}\right]\\
 &\widetilde{f}(\tau) = f(a_{k'}) + (\tau - a_{k'})\kappa_{2}, \hspace{0.3cm}\tau \in \left[ a_{k'}, a_{k+1}\right]
\end{flalign*}
For upper (lower) approximation on a concave (convex) segment, we compute derivatives and look for tangent lines at border points of the sub-interval $\left[ a_{k}, a_{k+1}\right]$. We choose a point $a_{k'}: a_{k} \leq  a_{k'} \leq a_{k+1}$ to be the intersection of the tangent lines. The %piecewise tangent line of the 
original function is approximated by the following two tangent line segments: %defines the corresponding approximation:
\begin{flalign*}
    &a_{k'} = \frac{f(a_{k}) - f(a_{k+1}) - (f_{+}'(a_{k})a_{k} - f_{-}'(a_{k+1})a_{k+1})}{f_{-}'(a_{k+1}) - f_{+}'(a_{k})}\\
    &\widetilde{f}(\tau) = 
        f(a_{k}) + f_{+}'(a_{k})(\tau - a_{k}),\hspace{1.15cm}\tau \in \left[ a_{k}, a_{k'}\right]
        \\
        &\widetilde{f}(\tau) = f(a_{k+1}) + f_{-}'(a_{k+1})(\tau - a_{k+1}),\hspace{0.10cm}\tau \in \left[ a_{k'}, a_{k+1}\right],
\end{flalign*}
where $f_{-}'(\cdot), f_{+}'(\cdot)$ are left and right derivatives, respectively.


For monotonically increasing functions, this procedure guarantees that the constructed approximators are exact upper and lower approximations. Moreover, decreasing the step size (increasing the number of segments) reduces the error at each point, meaning that the sequences of approximators for the activation functions at each node are monotonic, that is $\overline{f}_{n+1}(x) \leq \overline{f}_{n}(x), \underline{f}_{n+1}(x) \geq \underline{f}_{n}(x)$ for all $x$ in the IBP domain. This procedure of piecewise linear approximation of each activation function in each neuron is equivalent to the construction of a one-layer ReLU network for a given neuron which will approximate it. By the UAT, for any continuous activation function $\sigma_{l,i}$ at each neuron and any positive $\epsilon_{l,i}$ we can always find a ReLU network-approximator $NN_{l, i}$, such that $|NN_{l, i}(x) - \sigma_{l,i}(x)| < \epsilon_{l,i}$ for all $x$ in the  input domain, defined by the IBP. To find such an approximating network we need to choose the corresponding number of splitting segments of the IBP input region. Additionally, uniform convergence is preserved by Dini's theorem, which states that a monotonic sequence of continuous functions that converges pointwise on a compact domain to a continuous function also converges uniformly. Moreover, the approximator always stays within the range of the limit function, ensuring that the domain in the next layer remains unchanged and preserves its uniform convergence. 
\begin{figure}[t]
\begin{subfigure}[t]{0.5\textwidth}
  \centering
\includegraphics[width=0.9\textwidth]{figures_c/Iris_Tanh_5p_1.pdf}
\end{subfigure}%
\hfill
\begin{subfigure}[t]{0.5\textwidth}
  \centering
\includegraphics[width=0.85\textwidth]{figures_c/Iris_Tanh_10p_1.pdf}
\end{subfigure}
\caption{\textit{Tanh} NN output for the \textit{Setosa} class  (blue) in the Iris dataset, and its ReLU NN upper (red) and lower (green) approximations with 5 (upper panel) and 10 (lower panel) segments of bounding per convex section.}
\label{fig:Iris_tanh}
\end{figure}

\paragraph{Transformation into ReLU-Equivalent Form:}
    The approximating piecewise linear upper-lower functions are converted into a form that mimics the ReLU function's behavior; i.e., $\tilde{f}(x)=W^{(2)}\mathrm{ReLU}(W^{(1)}x +b^{(1)})+b^{(2)}$. This involves creating new weighting coefficients and intercepts that replicate the ReLU’s activation pattern across the approximation intervals.

Specifically, we receive a set of intervals $\{\left[x_{i-1}, x_{i}\right]\}_{i=1}^{n}$ with the corresponding set of parameters of affine transformations $\{\left(a_{i}, b_{i}\right)\}_{i=1}^{n}$: $$\widetilde{f}(\tau) = b_{i} + a_{i}\tau, \hspace{0.5cm} \tau \in \left[x_{i-1}, x_{i}\right],$$
and additionally set $a_{0} = 0$. Then, the corresponding ReLU-equivalent definition of approximation $\widetilde{f}$ on $\left[x_{0}, x_{n}\right]$ is
\begin{flalign*}
&\widetilde{f}(\tau) = x_{0}a_{1} + b_{1} + \sum\limits_{i = 1}^{n} \xi_{i}\mathrm{ReLU}(|a_{i} - a_{i-1}|(\tau - x_{i-1})), \\ 
&\xi_{i} = \sign(a_{i} - a_{i-1}).
\end{flalign*}

\paragraph{Overall Neural Network Approximation:} The entire NN is approximated by applying the above techniques to each neuron, layer by layer, and then merging all intermediate weights and biases. For each neuron, both upper and lower approximations are generated, capturing the range of possible outputs under different inputs. To ensure the correct propagation of approximators, to create an upper approximation, we connect the upper approximation of the external layer with the upper approximation of the internal subnetwork if the internal subnetwork has a positive coefficient, or with the lower approximation if it has a negative coefficient. The reverse applies to the lower approximation. This leads to a composition of uniformly convergent sequences, guaranteeing the overall uniform convergence of the final estimator to the original neural network.
The final output is a set of piecewise linear approximations that bound the output of the original neural network, which can then be used for further analysis or verification. The proof sketch is in Appendix \ref{app:thm_proof_Relu_est}.


\begin{example}\label{exmp:Iris_tanh_apprx}
Using the same setup as in Example \ref{exmp:Iris_cdf}, we train a fully connected neural network with 3 layers of 12 neurons each with $\mathrm{tanh}$ activation followed by 1 layer of 3 output neurons with linear activation % $\left[3 \times 12\right]$ \textit{tanh} + $\left[1 \times 3\right]$ linear  
on the Iris dataset, focusing on the re-scaled features \textit{Sepal Length} and \textit{Sepal Width}. We construct upper and lower approximations of the network's output by  ReLU neural networks with linear output layer. Two approximations are performed: with 5 and 10 segments of bounding per convex section at each node (upper and lower panels of Figure \ref{fig:Iris_tanh}, respectively). Notably, with 10 segments, the original network and its approximations are nearly indistinguishable. %Additional details are provided in Appendix~\ref{app:iris}.
\end{example} 

This procedure applies to various non-monotonic functions, such as monomials \( x^n, n \in \mathbb{N} \). These functions can be attained by a sequence of transformations that ensure monotonicity at all intermediate steps. These transformations can be represented as a subnetwork. %, as illustrated in Appendix~\ref{app:NN_square}. 
Furthermore, multivariate functions like $\mathrm{softmax}$ and the product operation also have equivalent subnetworks with continuous monotonic transformations, as shown in Appendix~\ref{app:NN_equivalent}.


\subsection{Application to an arbitrary function on a compact domain}\label{sec:cdfapprox}

We present the \textit{universal distribution approximation theorem}, which may serve as a starting point for further research in the field of stochastic behavior of functions and the neural networks that describe them.

\begin{theorem}[Universal distribution approximation theorem]\label{thm:UDAT}
Let $\X$ be a random vector with continuous pdf $\phi(\x)$ supported over a compact hyperrectangle $K \subset \real^{n_{0}}$. Let $Y = \mathcal{W}(\x)$ be a continuous function of $\X$ with domain $K$, and let $F(y)$ denote its cdf. Then, there exist sequences of cdf bounds $\{\underline{F}_{n}\}$,  $\{\overline{F}_{n}\}$, $n=1,2,\ldots$, %\efi{is $n$ the grid size then? \andrey{Implicitly, number of segments. We can call it as a grid size.}} 
such that  
\begin{align*}
\underline{F}_{n}(y) \leq  F(y) \le \overline{F}_{n}(y) 
\end{align*}
for all $y \in \{\mathcal{W}(\x): \x \in K\}$
and 
\begin{align*}
\underline{F}_{n} (y) \rightarrow F(y), \quad 
\overline{F}_{n} (y) \rightarrow F(y) 
\end{align*}
for all $y$ where $F(y)$ is continuous. Moreover, if $\mathcal{W}(\X)$ is nowhere locally constant almost surely, that is           \begin{align}\label{cont_contidion}
\int\limits_{\{\mathcal{W}(\x) = y\}} \phi(\x)d\x = 0
\end{align}
for all $y \in \{\mathcal{W}(\x): \x \in K\}$,
then both bounds $\underline{F}_{n}, \overline{F}_{n}$ converge uniformly to the true cdf $F$. The sequence $\{\underline{F}_{n}\}$,  $\{\overline{F}_{n}\}$ can be constructed by bounding the distributions of sequences of ReLU NNs. 
    
\end{theorem}

The proof is presented in Appendix \ref{app:thm_proof_UDAT}. It leverages the UAT \cite{Cybenko1989} to approximate the function $\mathcal{W}(\x)$ and input pdf $\phi(\x)$ with ReLU NNs to arbitrary accuracy. Cdf bounds are then computed over polytope intersections, with greater NN complexity yielding more simplices for finer local affine approximations of the pdf.

To bound the cdf of a given NN with respect to a specified input pdf, we construct upper and lower bounding ReLU NNs to approximate the target NN. Next, we subdivide the resulting polytopes of the bounding ReLU NNs into simplices as much as needed to achieve the desired accuracy and locally approximate the input pdf with constant values (the simplest polynomial form) on these simplices, transforming the problem into the one described in Section \ref{sec:cdf_relu_polynomial}.

\section{Experiments}

In numerical experiments we cosider three datasets, Diabetes \cite{Efronetal_2004}, Iris \cite{Fisher_1936} and, Wine \cite{wine_109}\footnote{All three datasets are provided by the Python package \href{https://scikit-learn.org/stable/}{\texttt{scikit-learn}}}.

For all of the experiments, we compare our guaranteed bounds for the output cdf with cdf estimates obtained via a Monte Carlo (MC) simulation ($100$ million samples), as well as with the ``Piecewise Linear Transformation'' (PLT) method of \citet{Krapfetal2024}. %As the true output cdf is unknown, we set up the experiments in comparison with the guaranteed bounds computed by our method. 
Given that the true cdf is contained within these limits, the experiments assess the tightness of our bounds compared to both MC and PLT by tallying the number of ``out of bounds'' instances. Essentially, achieving very tight bounds makes it challenging to stay within those limits, whereas even imprecise estimates can fall within broader bounds.

Our experimental setup is based on small pre-trained (fixed) neural networks. For the Diabetes dataset we trained a ReLU network with $3$ fully connected layers with $32$, $16$, and $8$ neurons, respectively. The univariate output has no activation. Training was performed on $70\%$ of the data, randomly selected. The remaining $30\%$, consisting of $73$ observations, comprise the test set. As there is no randomness in the observations, we added univariate normal noise to one randomly selected feature of every observation (different features in different observations). The standard deviation of the Gaussian noise is set to the sample standard deviation of the selected feature within the test set.

For both, Iris and Wine, we replicated the exact experimental setup from \cite{Krapfetal2024} using the same test sets, Gaussian Mixtures as randomness as well as the same pre-trained networks\footnote{GitHub page \url{https://github.com/URWI2/Piecewise-Linear-Transformation}, accessed Jan 25, 2025}. The $1$ to $3$ dimensional Gaussian mixtures were computed by first deleting  $25\%$ or $50\%$ of the test dataset. Subsequently, $50$ new observations are imputed using MICE \cite{vanBuurenetal2011}.% which where the per observation bases to compute a Gaussian Mixture by means of Gaussian kernel density estimation. 
The only difference in the experimental setup is the MC estimate, which we recomputed with a higher sample count. Here, the MC estimate does not play the role of the ``ground truth'' as in their experiments, but as another estimate.

We summarize our results in Table~\ref{tab:sim1}. We observe a high ratio of ``out of bounds'' samples for both MC and PLT compared to the number of grid points the cdf was evaluated at. This has two components:  (a) In regions where the cdf is very flat, we obtain very tight bounds leading to small errors in a bucketed estimation approach easily falling outside of these tight bounds;  (b) due to either pure random effect in the case of MC or numerical estimation inaccuracies in case of PLT, it produces estimates outside the bounds. We note, however, that in these examples, especially as regards PLT, a ``coarse grid'' can cause inaccuracies in areas where the pdf is volatile despite that their estimator targets the pdf directly and it should be more accurate than ours (as we target bounding the cdf instead).

\begin{table*}[!ht]
\centering
\caption{Comparison of our approach (guaranteed upper and lower bounds) with pointwise estimators from Monte-Carlo simulations and PLT \cite{Krapfetal2024}. Under column headed by $n_{0}$ are numbers of input variables, \textit{\# tests} gives the number of observations deleted, %with $n_0$ dimensional uncertainty, 
under \textit{U/L-dist (std)} are the mean distance (standard deviation) upper and lower bounds, under $\mathrm{OOB}_{\mathrm{MC}}$ (median) and $\mathrm{OOB}_{\mathrm{PLT}}$ (median) are the average (standard deviation) of the median of number of points %out of evaluated grid points 
outside our bounds for Monte-Carlo simulations and PLT, respectively.}
\label{tab:sim1}
\vspace{0.1in}
%\resizebox{\columnwidth}{!}{%
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l  c  c  c  c  c  c  c}
    \toprule
    dataset & \%unc & $n_0$ & \# tests & U/L-dist (std)
        & $\mathrm{OOB}_{\mathrm{MC}}$ (median)
        & $\mathrm{OOB}_{\mathrm{PLT}}$ (median) & Grid size\\
    \midrule
    diabetes & - & 1 & 73 & 0.01337 (0.005) & 7 (7) & 18 (19) & 20 \\
    \hline
            &      &  1  &   25   & 0.00988 (0.016) & 1093 (1080) & 1604 (2148) & 8000 \\
    iris    &  25  &  2  &   10   & 0.01236 (0.021) &  793  (697) & 1528 (2162) & 8000 \\
            &      &  3  &   --   &   ---           &  ---        &  ---        & 8000 \\
    \hline
            &      &  1  &   20   & 0.05534 (0.077) &  305   (30) &  500    (4) & 8000 \\
    iris    &  50  &  2  &   23   & 0.04709 (0.090) &  980 (1016) & 1312 (2031) & 8000 \\
            &      &  3  &    8   & 0.06864 (0.108) &  779  (599) & 1150 (1000) & 8000 \\
    \hline
            &      &  1  &   32   & 0.02014 (0.055) & 1066 (1242) & 1708 (2172) & 8000 \\
    wine    &  25  &  2  &    8   & 0.00909 (0.017) & 1188 (1347) & 1839 (2275) & 8000 \\
            &      &  3  &    2   & 0.00008 (0.000) & 1324 (1324) & 2516 (2516) & 8000 \\
    \hline
            &      &  1  &   27   & 0.07590 (0.109) &  757  (273) & 1027   (45) & 8000 \\
    wine    &  50  &  2  &   21   & 0.04214 (0.079) & 1031 (1193) & 1304 (2057) & 8000 \\
            &      &  3  &   13   & 0.06612 (0.102) &  550  (128) &  797   (40) & 8000 \\
    \bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
%}
\vspace{0.1in}
\end{table*}


\section{Related Work}
\label{related}

\begin{comment}
In any supervised learning system (regression or classification), the data come in pairs: predictors ($\X$, inputs) and response ($Y$, output). Either can be continuous or categorical or mixed. To model uncertainty in the data generating process, one can assume that both $\X$ and $Y$ are stochastic; i.e., they have a joint distribution, or that $Y$ is stochastic given the observed $\X$. 
In the latter case, where $\X$ is assumed to be fixed, several machine learning approaches enter predictor randomness in the model indirectly by assuming the parameters are random. This is the setting of Bayesian Neural Networks (BNNs) (see, e.g., \cite{Arbeletal2023,Deshpandeetal2022,GalGhahramani2016,KendallGal2017,Ghahramani2015}) and associated Variational Inference (VI) \cite{HintonvanCamp1993,Graves2011}. BNNs target what is called \textit{epistemic} or model uncertainty (whether the right model is fitted) and indirectly \textit{aleatoric} uncertainty; i.e., noise inherent in the observations resulting from the inputs being a sample from an unknown population. Epistemic uncertainty is modeled by placing a prior distribution over a model's parameters ($\Thetabf$ in \eqref{eq:reg-model}), and then estimating the posterior distribution of $\Thetabf$ given $\X$ and $Y$ using Markov Chain Monte Carlo (MCMC) or Variational Inference (VI). Aleatoric uncertainty is modeled at the backend by placing a distribution over the output of the model. For example, the outputs are modeled as corrupted with Gaussian random noise. Training of BNNs is challenging due to computational complexity.


To accommodate noisy data or aleatoric uncertainty (random input) in NNs directly, sampling-based and pdf approximation-based methods have been proposed. The former propagate randomness by mapping a set of samples through the NN based on which either features (e.g. moments) of the output distribution or the output distribution itself is derived. Monte Carlo simulation is used to draw random samples from the input  distribution, which are consequently propagated through the neural network to aggregate an output distribution. The training of large NNs is already computationally costly, and repeating this process a large number of times compounds the computational complexity without addressing the fundamental issue of the noise in the data deriving from their being a sample from a much larger population 
(see, e.g., \citet{Abdelazizetal2015,Jietal2020}). The core idea of pdf approximation-based methods is to assume the input, or the hidden layers of a NN have specific distributions. For example, \citet{Abdelazizetal2015} assume the distribution in each layer to be Gaussian and study their propagation through sigmoid NNs. \citet{ZhangShin2021} approximate input distributions with Gaussian Mixture Models. 
However, these methods suffer from significant approximation errors and are unable to accurately quantify predictive uncertainty in the NN output due to restrictive assumptions about the distributions in the input, their parameters, and hidden, or output layer. % (e.g., the assumption of any post-activation distribution being a Gaussian).
A summary of approaches to uncertainty estimation in neural networks can be found in \cite{Sickingetal2022} and a review in \cite{Gawlikowskietal2023}. 
\end{comment}

The literature on NN verification is not directly related to ours as it has been devoted to standard non-stochastic input NNs, where the focus is on establishing guarantees of local robustness. This line of work develops testing algorithms for whether the output of a NN stays the same within a specified neighborhood of the deterministic input (see, e.g., \citet{Gowaletal_2018,Xuetal_2020,Zhangetal2018,Zhangetal2020,Shietal2024,Buneletal_2019,Ferrarietal_2022,Katzetal_2017,Katzetal_2019,Wuetal_2024}). 

To handle noisy data or aleatoric uncertainty (random input) in NNs, two main approaches have been proposed: sampling-based and probability density function (pdf) approximation-based. Sampling-based methods use Monte Carlo simulations to propagate random samples through the NN (see, e.g., \citet{Abdelazizetal2015,Jietal2020}), but the required replications to achieve similar accuracy to theoretical approaches such as ours, as can be seen in Table \ref{tab:sim1}, can be massive. Pdf approximation-based methods assume specific distributions for inputs or hidden layers, such as Gaussian \cite{Abdelazizetal2015} or Gaussian Mixture Models \cite{ZhangShin2021}, but these methods often suffer from significant approximation errors and fail to accurately quantify predictive uncertainty. Comprehensive summaries and reviews of these approaches can be found in sources like \citet{Sickingetal2022} and  \citet{Gawlikowskietal2023}.


\begin{comment}
\efi{Is this local robustness really relevant to our paper? Seems to me it's about "deterministic" robustness.} The local robustness property in classification problems can be formulated as follows.
A neural network $f$ is locally robust at $x_{0}$  if
\begin{align*}
\forall x: \parallel x - x_{0}\parallel \leq \epsilon \xrightarrow[]{} f(x) = f(x_{0})
\end{align*}
 for some positive $\epsilon$ ($\epsilon$-robust),   where $\parallel \cdot \parallel$ is some distance measure. 
%\end{definition}
It is typical to consider $l_{1}$, $l_{2}$ or $l_{\infty}$ norms as distance measures, with the latter % $l_{\infty}$ norm is
being the most popular.

Interval Bound Propagation (IBP) \cite{Gowaletal_2018} provides certified robustness against \textit{nonrandom} adversarial perturbations by deriving constant-type bounds for output neurons based on input ranges. Linear Relaxation Perturbation Analysis (LiRPA) \cite{Xuetal_2020} generalizes bounds computation using a graph-based approach, expressing neural networks as computational graphs to construct affine bounds for intermediate nodes. CROWN and CROWN-IBP \cite{Zhangetal2018,Zhangetal2020} certify robustness for neural networks with general activation functions. CROWN bounds non-piecewise-linear activations using linear and quadratic functions, while CROWN-IBP combines IBP's efficiency with CROWN's tighter linear relaxation bounds.  GenBaB \cite{Shietal2024} extends branch-and-bound (BaB) frameworks \cite{Buneletal_2019,Ferrarietal_2022} to verify neural networks with general nonlinearities by combining linear bound propagation and optimized branching strategies, supporting activations like Sigmoid, Tanh, GeLU, and multidimensional operations. Finally, SMT-based frameworks like Reluplex \cite{Katzetal_2017} and Marabou \cite{Katzetal_2019,Wuetal_2024} verify neural networks by transforming property queries into constraint satisfaction problems, handling diverse activation functions and topologies with parallel execution for scalability.
\end{comment}


In the context of verifying neural network properties within a probabilistic framework,  \cite{Wengetal2019} proposed PROVEN, a general probabilistic framework that ensures robustness certificates for neural networks under Gaussian and Sub-Gaussian input perturbations with bounded support with a given probability. It employs CROWN \cite{Zhangetal2018,Zhangetal2020} to compute deterministic affine bounds and subsequently leverages straightforward probabilistic techniques based on Hoeffding's inequality \cite{Hoeffding1963}.  PROVEN provides a probabilistically sound solution to ensuring the output of a NN is the same for small input perturbations with a given probability, its effectiveness hinges on the activation functions used. It cannot refine bounds or handle various input distributions, which may limit its ability to capture all adversarial attacks or perturbations in practical scenarios.

The most relevant published work to ours we could find in the literature is \citet{Krapfetal2024}. They propagate input densities through NNs with piecewise linear activations like ReLU, without needing sampling or specific assumptions beyond bounded support. Their method calculates the propagated pdf in the output space using the ReLU structure. They estimate the output pdf, which is shown to be very close to a pdf estimate obtained by Monte Carlo simulations. Despite its originality, the approach has drawbacks, as they compare histograms rather than the actual pdfs in their experiments. Theorem 5 (App. C) in \cite{Krapfetal2024} suggests approximating the distribution with fine bin grids and input subdivisions, but this is practically difficult. Without knowledge of the actual distribution, it is challenging to define a sufficiently ``fine'' grid. In contrast, we compute exact bounds of the true output cdf over its entire support (at any point, no grid required), representing the maximum error over its support, and show %pointwise 
convergence to the true cdf. %, with uniform convergence when the output cdf is continuous.
\citet{Krapfetal2024} use a piecewise constant approximation for input pdfs, which they motivate by their Lemma 3 (App. C) to deduce that exact propagation of piecewise polynomials through a neural network cannot be done. 
We demonstrate that it is feasible and provide a method for exact integration over polytopes. Additionally, their approach is limited to networks with piecewise linear activations, excluding locally nonlinear functions. In contrast, our method adapts to CNNs and any NN with continuous, monotonic piecewise differentiable activations.

\section{Conclusion}

We develop a novel method to analyze the probabilistic behavior of the output of 
a neural network subject to noisy (stochastic) inputs. We formulate an algorithm to compute bounds (upper and lower) for the cdf of a neural network's output and prove that the bounds are guaranteed and that they converge uniformly to the true cdf. 

Our approach enhances deterministic local robustness verification using non-random function approximation. By bounding intermediate neurons with piecewise affine transformations and known ranges of activation functions evaluated with IBP \cite{Gowaletal_2018}, we achieve more precise functional bounds.  These bounds converge to the true functions of input variables as local linear units increase.  

Our method addresses neural networks with continuous monotonic piecewise differentiable activation functions using tools like Marabou  \cite{WuIZTDKRAJBHLWZKKB24}, originally designed for piecewise linear functions.
 While the current approach analyzes the behavior of  NNs on a compact hyperrectangle, we can easily extend our theory to unions of bounded polytopes. %, which is a composition of convex polytopes. 
 In future research,  we plan to bound the cdf of a neural network where the input admits arbitrary distributions with bounded piecewise continuous pdf supported on arbitrary compact sets. Moreover, we intend to improve the algorithmic performance so that our method applies to larger networks.
 
 
 %We plan to create a tool to bound the cdf of the neural network with respect to arbitrary input distribution with bounded piecewise continuous pdf with an arbitrary bounded support.

%\efi{
%Deterministic verification algorithms ensure reliability by focusing on the worst-case scenario, often disqualifying high-performing models due to rare "non-robustness." We instead adopt a probabilistic approach, relaxing local robustness conditions and allowing models with low probabilities of unstable behavior to be considered. 

% Our approach also enhances deterministic local robustness verification using non-random function approximation. By bounding intermediate neurons with piecewise affine transformations and known activation functions evaluated with IBP, we achieve more precise functional bounds. These bounds converge to the true functions of input variables as local linear units increase. Our method addresses neural networks with continuous monotonic activation functions using tools like Marabou, originally designed for piecewise linear functions.

%By bounding all intermediate neurons via piecewise affine transformations, leveraging known activation functions at each neuron and supersets of their domains evaluated with IBP, we achieve more precise functional representations of these bounds. These representations uniformly converge to the true functions (within the network) of the input variables as the number of local linear units increases. Our results demonstrate that with proper estimation with our approach, neural networks with continuous monotonic activation functions—beyond piecewise linear ones—can be effectively addressed by methods like Marabou, which were initially designed for piecewise linear activation functions.

%Although deterministic verification algorithms aimed at addressing the problem of local robustness in neural networks provide reliability guarantees, they are designed to identify the worst-case scenario without considering how often or in which points this "worst-case scenario" might occur. As a result, they may disqualify a large number of models that exhibit high performance across other objective criteria, despite having "non-robustness" in a very small number of scenarios (which occur with extremely low probability). In contrast to the deterministic approach, we relax the condition of local robustness to a probabilistic one, allowing models with a low probability of unstable behavior to be considered.
%}




\clearpage

\bibliography{refs}
\bibliographystyle{icml2025}


% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Proof of Theorems}

\subsection{Theorem \ref{thm::exact_cdf}}\label{app:thm_proof_Relu_cdf}
Suppose the activation function in the prediction NN \eqref{MLP} is ReLU and $n_{0}$ and $n_{L}$ are the number of input and output neurons, respectively.
To compute  the cdf of $\pY = f_{L}(\x)$  at $y$, $F_{\pY}(y) = \Pr[f_{L}(\x) \leq y]$, we compute the sum of local probabilities $\pr[NN^{j}(\x) \leq y \mid \x \in \mathcal{C}_{j}]$, each of which is the integral of the pdf of the input over the given polytopes subject to $NN^{j}(\x) \leq y$.

$\mathcal{C}_{j}$ is a convex polytope and  can be represented as the intersection of halfspaces. The set $\{\x: NN^{j}(\x) = \cb^{j} + \V^{j}\x \leq y\}$ is defined as the intersection of halfspaces $\bigcap\limits_{t = 1}^{n_{L}} \left\{\x: NN^{j}_{t}(\x) = c^{j}_{t} + \sum_{z=1}^{n_{0}}x_{z}v^{j}_{t, z} \leq y_{t}\right\}$, which when intersected with $\mathcal{C}_{j}$ defines the reduced complex polytope $\mathcal{C}^{r}_{j,i}$. The desired local probability, $\pr[NN^{j}(\x) \leq y \mid \x \in \mathcal{C}_{j}]$, can be found as the integral of the pdf of $\X$ over the reduced polytope.

Using Delaunay triangulation one can decompose any convex polytope $\mathcal{C}^{r}_{j,i}$ into the disjoint set of simplices $\mathcal{T}_{i,j,s}$. This triangulation allowes us to compute the integral over the polytope as a sum of integrals over each simplex. Assuming that the pdf of the input is a piecewise polynomial, allows us to use the algorithm from \cite{Lasserre2021} to compute exact integrals over all of simplices. The sum of all these local integrals (probabilities) is the exact cdf value at point $y$.


\subsection{Theorem \ref{thm::relu_approx}}\label{app:thm_proof_Relu_est}
We provide a sketch of the proof of Theorem~\ref{thm::relu_approx}. 

One can verify that both bounds of every neuron in every layer are continuous functions on a bounded domain. The domain, of these bounds is the same as the domain of the activation function itself, and converge pointwise to the activation function. % Increasing the number of approximation intervals leads to reduction of approximation errors.

The sequence of upper/lower bounds is monotonic decreasing/increasing and bounded by the activation function, and thus they converge. Since their domains are compact, application of Dini's theorem obtains uniform convergence.
% Due to the Dini' theorem, monotonic sequence of continuous function which converges pointwise to the continuous function on a compact domain converges uniformly to the limit.

Hence, all estimators of all neurons converge uniformly and preserve the domain of the target activation functions.%, as well as the range in all subsequent layers uniform convergence of all neurons in all subsequent layers.
Also the composition of uniformly convergent functions on bounded intervals, preserved the convergence, resulting in the convergence of the sequence of the networks, as a composition of those estimates.

%The composition of uniform convergent functions on finite intervals preserves the convergence, resulting the convergence of the sequence of networks estimates.

%The upper and lower position of the estimators are obvious due to the construction and the monotonicity and continuity of the activation functions.


\subsection{Theorem \ref{thm:UDAT}}\label{app:thm_proof_UDAT}
According to the UAT, for any $\epsilon > 0$ there exist one-layer networks $\widetilde{Y}$, $\widetilde{\phi}$ with ReLU activation function, such that
\begin{align*}
    \sup\limits_{\x \in K}\parallel \mathcal{W}(\x) - \widetilde{Y}(\x) \parallel < \epsilon, \qquad
    \sup\limits_{\x \in K}\parallel \phi(\x) - \widetilde{\phi}(\x) \parallel < \epsilon.
\end{align*}
Define $\underline{Y}_{n}(\x) = \widetilde{Y}(\x) - \epsilon$, which is also a neural network. Similarly, for $\overline{Y}_{n}$, $\underline{\phi}_{n}$, $\overline{\phi}_{n}$. Then, \begin{align*}
    \mathcal{W}(\x) - 2\epsilon \leq \underline{Y}_{n}(\x) = \widetilde{Y}(\x) - \epsilon \leq \mathcal{W}(\x) \leq \widetilde{Y}(\x) + \epsilon = \overline{Y}_{n}(\x) < \mathcal{W}(\x) + \epsilon \\
    \phi(\x) - 2\epsilon < \underline{\phi}_{n}(\x) = \widetilde{\phi}(\x) - \epsilon \leq \phi(\x) \leq \widetilde{\phi}(\x) + \epsilon = \overline{\phi}_{n}(\x) < \phi(\x) + 2\epsilon,
\end{align*}
% Note, that for all $\x \in K$, $\underline{Y}_{n}(\x) \Rightarrow \widetilde{Y}(\x)-\epsilon$, $\overline{Y}(\x)_{n}(\x) \Rightarrow \widetilde{Y}(\x)+\epsilon$ with $n \rightarrow \infty$, and that is why, $\underline{Y}_{n}(\x) \Rightarrow Y(\x)$, $\overline{Y}_{n}(\x) \Rightarrow Y(\x)$ with $\epsilon \rightarrow 0$ and $n \rightarrow \infty$. In the same manner, $\underline{\phi}_{n}(\x) \Rightarrow \phi(\x)$, $\overline{\phi}_{n}(\x) \Rightarrow \phi(\x)$ with $\epsilon \rightarrow 0$ and $n \rightarrow \infty$.
which proves that, as $\epsilon \to 0$,   $\underline{Y}_{n}, \overline{Y}_{n}\to \mathcal{W}$ and $\underline{\phi}_{n}, \overline{\phi}_{n} \to \phi$, uniformly on a compact domain while guaranteeing to be upper/lower bounds.

Let
\begin{align*}
    \overline{F}_{n}(y) = \min \left[1, \int\limits_{\{\x: \x \in K \cap \underline{Y}_{n}(\x) \leq y\}}\overline{\phi}_{n}(\x)d\x \right], \quad 
    \underline{F}_{n}(y) = 
    \max \left[0, \int\limits_{\{\x: \x \in K \cap \overline{Y}_{n}(\x) \leq y\}}\underline{\phi}_{n}(\x)d\x \right]
\end{align*}
The limit cdf is 
\begin{align*}
    F(y) &= \int\limits_{\{\x: \x \in K \cap Y(\x) \leq y\}}\phi(\x)d\x 
\end{align*}
Since $\underline{\phi}_{n}(\x) \leq \phi(\x) \leq \overline{\phi}_{n}(\x)$  and $\underline{Y}_{n}(\x) \leq \mathcal{W}(\x) \leq \overline{Y}_{n}(\x)$ for any $\x \in K$, %the embeddings 
$\{\x: \x \in K \cap \underline{Y}_{n}(\x) \leq y\} \supseteq \{\x: \x \in K \cap \mathcal{W}(\x) \leq y\}$ and $\{\x: \x \in K \cap \overline{Y}_{n}(\x) \leq y\} \subseteq \{\x: \x \in K \cap \mathcal{W}(\x) \leq y\}$. Since $0 \leq F(y) \leq 1$ for all $y$, 
\begin{align*}
     \underline{F}_{n}(y) \leq F(y) \leq  \overline{F}_{n}(y)
\end{align*}
for all $y \in \{\mathcal{W}(\x): \x \in K\}$.

Now let us fix an arbitrary $y = \mathcal{W}(\x)$ for $\x\in K$, such that $y$ is a continuity point of $F$.
\begin{align*}
    \overline{F}_{n}(y) - F(y) &\leq \int\limits_{\{\x: \x \in K \cap \underline{Y}_{n}(\x) 
    \leq y\}}\overline{\phi}_{n}(\x)dx - \int\limits_{\{\x: \x \in K \cap \mathcal{W}(\x) \leq y\}}\phi(\x)d\x \\
    &= \underbrace{\int\limits_{\{\x: \x \in K \cap \underline{Y}_{n}(\x) \leq y\}}(\overline{\phi}_{n}(\x) - \phi(\x))dx}_{A} + \underbrace{\int\limits_{\{\x: \x \in K \cap \underline{Y}_{n}(\x) 
    \leq y\}}\phi(\x)dx - \int\limits_{\{\x: \x \in K \cap \mathcal{W}(\x) \leq y\}}\phi(\x)dx}_{B}
\end{align*}
\begin{align*}
     F(y) - \underline{F}_{n}(y) &\leq \int\limits_{\{\x: \x \in K \cap Y \leq y\}}\phi(\x)dx -\int\limits_{\{\x: \x \in K \cap \overline{Y}_{n}(x) 
    \leq y\}}\underline{\phi}_{n}(\x)d\x   \\
    &= \underbrace{\int\limits_{\{\x: \x \in K \cap \overline{Y}_{n}(\x) \leq y\}}( \phi(\x) - \underline{\phi}_{n}(\x))dx}_{C} + \underbrace{\int\limits_{\{\x: \x \in K \cap Y \leq y\}}\phi(\x)d\x - \int\limits_{\{\x: \x \in K \cap \overline{Y}_{n}(\x) 
    \leq y\}}\phi(\x)d\x}_{D}
\end{align*}
The left integrals in both equations (A and C) converge to zero due to the uniform convergence to zero of the integrands over the whole set $K$. The second differences (B and D) converge to zero, since the superset $\{\x: \x \in K \cap \underline{Y}_{n}(\x) \leq y\}$ and subset $\{\x: \x \in K \cap \overline{Y}_{n}(\x) \leq y\}$ of the limits of integrals, respectively, converge to the true limit set $\{\x: \x \in K \cap \mathcal{W}(\x) \leq y\}$ due to continuity. 
% as the function $\overline{Y}_n\to\mathcal{W}$ \efi{their pre-domains converge as well due to continuous.}

%Indeed, assume that the subset $K_{1}$ is such that $\mathcal{W}(\x) \leq y$, $\overline{Y}_{n_{1}}(\x) > y$ for some $n_{1} \in \nat$ and for all $\x \in K_{1}$ and there is no ball $U \in K_{1}$, such that $\mathcal{W}(\x) = y$ for all $\x \in U$ and $\int_{ U}\phi(\x)d\x > 0$. Since $\mathcal{W}(\x)$ is continuous, $K_{1}$ is a compact and there is $\delta = \sup\limits_{\x \in K_{1}}(y - \mathcal{W}(\x))$. Then, since $K_{1} \subseteq K$, and $\overline{Y}_{n} \Rightarrow Y$,  
%$\delta \leq \sup\limits_{\x \in K}(\overline{Y}_{n_{1}}(\x) - Y(\x))$. 
%Due to the uniform convergence, we always can find $n_{2} > n_{1}$ and $\overline{Y}_{n_{2}}(\x)$, such that error will be lower that $\delta$ over the whole $K$, and, in particular $K_{1}$.

%Due to the continuity of $\overline{Y}_{n_{2}}$ and its non-increasing behavior: $\overline{Y}_{n_{2}}(\x) \leq \overline{Y}_{n_{1}}(\x)$ for all $\x \in K$, we can observe the convergence of sets:
%\begin{align*}
%    \{\x: \x \in K \cap \overline{Y}_{n_{1}}(\x) \leq y\} \subseteq \{\x: \x \in K \cap \overline{Y}_{n_{2}}(\x) \leq y\} \subseteq \{\x: \x \in K \cap Y(\x) \leq y\}
%\end{align*}

We have proven pointwise convergence for every  point of continuity of the limiting cdf; that is, convergence in distribution. 

Requiring \ref{cont_contidion} means that the limiting distribution has no point mass; i.e., it is continuous. We can then apply Polya's theorem (\cite{Rao_1962})  that the convergence of both bounds is uniform since the sequences converge in distribution to random variables with continuous cdf. 


\section{Subnetwork equivalent for specific functions for upper/lower approximation}\label{app:NN_equivalent}

%\section{Appendix section: Subnetwork equivalent for specific functions for upper/lower approximation}

\subsection{Square}\label{app:NN_square}

The quadratic function $x^2$ is not monotone on an arbitrary interval.
But $x^2$ is monotonic on $\real_{\geq 0}$. We can modify the form of the function by representing it as a subnetwork to be a valid set of sequential monotonic operations.
Since $x^2 = |x|^2$, $x \in \real$, and the output range of $|x|$ is exactly $\real_{\geq 0}$, we  represent $|x|$ as a combination of monotonic $\mathrm{ReLU}$ functions, $|x| = \mathrm{ReLU}(x) + \mathrm{ReLU}(-x)$.
The resulting subnetwork is drawn in Figure~\ref{fig:NN_square}.


\begin{figure}[!ht]
    \centering
    \begin{subcaptionblock}{\textwidth}
        \begin{minipage}{0.44\textwidth} % First column (one big figure)
        \hspace{1cm}
        \includegraphics[width=0.75\textwidth, height=0.43\textwidth]{figures_c/NN_square.pdf}
            \caption{Subnetwork equivalent to operation of taking a square.}
            \label{fig:NN_square}
        \end{minipage}
        \hfill
        \begin{minipage}{0.5\textwidth} % Second column (two small figures)
        \hspace{1cm}
            \includegraphics[width=0.9\textwidth, height=0.6\textwidth]{figures_c/NN_product.pdf}
            \caption{Subnetwork equivalent to the product operation.}
            \label{fig:NN_product}
        \end{minipage}
    \end{subcaptionblock}
\end{figure}



\subsection{Product of two values}

To find a product of two values $x_{1}$ and $x_{2}$ one can use the formula $x_1 \cdot x_2 = 0.5 \cdot ((x_{1} + x_{2})^{2} - x_{1}^{2} - x_{2}^{2})$.
 This leads us to the feedforward network structure in Figure~\ref{fig:NN_product}.


\subsection{Maximum of two values}

The maximum operation can be expressed via a subnetwork with ReLU activation functions only, as follows.
Observing that $\max\{x_{1}, x_{2}\} = 0.5 \cdot (x_{1} +  x_{2} + |x_{1} - x_{2}|)$ results in the corresponding network structure in Figure~\ref{fig:NN_max}.

\begin{figure}[!h]
\centering
\includegraphics[width=0.5\textwidth, height=0.3\textwidth]{figures_c/NN_max.pdf}
\caption{Subnetwork equivalent to a maximum of two values.}
\label{fig:NN_max}
\end{figure}


\subsection{Softmax}

The function $\mathrm{softmax}$ transforms a vector of real numbers to a probability distribution. That is, if $\x = (x_{1}, \ldots, x_{n}) \in  \real^n$, then there is a multivariate function $SfMax: \real^n \xrightarrow[]{} \real^n$, so that% component-wise application is
\[ SfMax_{i} = \mathrm{softmax}(x_{i}) = \frac{e^{x_{i}}}{\sum\limits_{j = i}^{n}e^{x_{j}}}\]
Then, $log(SfMax_{i}) = x_{i} - \log \sum\limits_{j = i}^{n}e^{x_{j}}$, which is a composition of monotonic functions. This leads to the feedforward network structure in Figure~\ref{fig:NN_softmax}.


\begin{figure}[!ht]
\centering
\includegraphics[width=0.5\textwidth, height=0.3\textwidth]{figures_c/NN_softmax.pdf}
\caption{Subnetwork equivalent to one softmax node.}
\label{fig:NN_softmax}
\end{figure}


\section{Description of Iris Experiments}\label{app:iris}

We trained a fully connected $\left[3 \times 12\right]$ ReLU NN with a final $\left[1 \times 3\right]$ linear layer, as well as a fully connected $\left[3 \times 12\right]$ $\tanh$ NN with the same final $\left[1 \times 3\right]$ linear layer, on the Iris dataset. The networks classify objects into three classes: \textit{Setosa}, \textit{Versicolor}, and \textit{Virginica}, using two input features: \textit{Sepal Length} and \textit{Sepal Width}. The allocation of the data for these two variables in the three classes is shown in Figure~\ref{fig:Iris_dataset}. The input data were rescaled to be within the interval $\left[0, 1\right]$.  

\paragraph{Experiment 1: ReLU-Based Network with Random Inputs.}
The ReLU network was pre-trained. We next introduced randomness to the input variables by modeling them as Beta-distributed with parameters $(2,2)$ and $(3,2)$, respectively. The pdfs of these input distributions are shown in Figure~\ref{fig:Beta_distr}. The first is symmetric about 0.5 and the second is left-skewed.

In our first experiment, Example~\ref{exmp:Iris_cdf}, we computed the exact cdf of the first output neuron (out of three) in the ReLU network before applying the softmax function. Due to the presence of a final linear layer, the output may contain negative values. To validate our computation, we compared it against a conditional \textit{ground truth} obtained via extensive Monte Carlo simulations, where the empirical cdf was estimated using $10^5$ samples. As shown in Figure~\ref{fig:Iris_cdf}, both cdf plots coincide. The cdf values were computed at 100 grid points across the estimated support of the output, determined via the IBP procedure.  

For further comparison, Figure~\ref{fig:Iris_PDF} presents an approximation of the output pdf based on the previously computed cdf values. This is compared to a histogram constructed from Monte Carlo samples. Additionally, we include a Gaussian kernel density estimation (KDE) plot obtained from the sampled data using a smoothing parameter of $h = 0.005$. The results indicate that our pdf approximation better represents the underlying distribution compared to KDE and tracks the histogram more closely.  

\paragraph{Experiment 2: Bounding a Tanh-Based Network with ReLU Approximations.}
In our second experiment, we used a pre-trained NN with a similar structure but replaced the ReLU activation functions in the first three layers with \textit{tanh}. To approximate this network, we constructed two bounding fully connected NNs—one upper and one lower—using only ReLU activations.  

We conducted computations in two regimes: one using 5 segments and another using 10 segments, into which both convex and concave regions of the \textit{tanh} activation function at each neuron in the first three layers were divided. Over each segment, we performed piecewise linear approximations according to the procedure described in Section~\ref{sec:RelU_approx_algorithm} and combined these approximations into three-layer ReLU networks with an additional final linear layer for both upper and lower bounds.  The results of these approximations are shown in Figure~\ref{fig:Iris_tanh}. In the 10-segment regime, both the upper and lower approximations closely align with the original NN's output.  



\begin{figure}[h]
\centering
\begin{subcaptionblock}{\textwidth}
\begin{minipage}{0.48\textwidth} % First column (one big figure)
\includegraphics[width=\textwidth, height=0.35\textheight]{figures_c/Iris_Dataset.pdf}
\caption{Plot of \textit{Sepal Width} vs \textit{Sepal Length} with class indication.}
\label{fig:Iris_dataset}
\end{minipage}
\begin{minipage}{0.48\textwidth} % Second column (two small figures)
\includegraphics[width=\textwidth]{figures_c/Beta_PDF.pdf}
\caption{Plots of the $Beta(2,2)$ and $Beta(3,2)$ pdfs, resp.}
\label{fig:Beta_distr}
\vspace{3mm} % Space between figures
\includegraphics[width=\textwidth]{figures_c/Estimated_pdf.pdf}
\caption{Estimated output pdf compared with KDE with smoothing parameter $h = 0.005$ and histogram of MC simulations.}
\label{fig:Iris_PDF}
\end{minipage}
\end{subcaptionblock}
\caption{Iris Dataset}
\label{}
\end{figure}

%\efi{What does "Outcome of the first element of the NN" mean?}


\end{document}

