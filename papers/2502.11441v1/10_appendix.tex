\appendix
\newpage
\section{Evaluation Metrics Details}
\label{appendix:evaluationMetrics}This section provides details on the metrics used to assess the effectiveness of unlearning. These metrics capture different aspects of model performance, including lexical similarity, semantic consistency, confidence in predictions, and factual correctness.

\noindent
\textbf{ROUGE} measures how closely the model's output aligns with the ground truth at the word level. Specifically, we use ROUGE-L recall~\citep{lin2004rouge}, which considers the longest common subsequence between the model’s generated output $g(x;\theta_u)$ and the correct answer $y$. This metric is useful for evaluating whether the model retains relevant content after unlearning.

\noindent
\textbf{Probability} quantifies the likelihood that the model correctly predicts the ground truth answer. Following~\citet{maini2024tofu}, we compute the normalized conditional probability of the ground truth, defined as $P(y|x) = \frac{1}{T} \sum_{t=1}^{T} p(y_t|x \circ y_{<t}; \theta_u)$. A lower probability after unlearning indicates reduced model confidence in generating the forgotten content.

\noindent
\textbf{Cosine Similarity} assesses the semantic consistency of model outputs before and after unlearning. Inspired by semantic textual similarity tasks~\citep{cer-etal-2017-semeval}, we embed the outputs using Sentence-BERT~\citep{reimers-2019-sentence-bert} and compute their cosine similarity. We set a lower bound of 0, defining the metric as $\max(\text{Cos}(g(x;\theta), g(x;\theta_u)), 0)$. Lower similarity scores indicate greater divergence in output, often due to additional or altered information introduced post-unlearning.

\noindent
\textbf{Entailment Score} evaluates the factual correctness of generated responses relative to the ground truth. This metric is based on Natural Language Inference (NLI), where a pre-trained NLI model~\citep{sileo2023tasksource} determines whether the model's output logically follows from the reference answer~\citep{liu2024learning}. The final score represents the proportion of outputs classified as “entailment.” Higher values indicate better factual alignment, particularly for retained knowledge, while lower scores suggest effective forgetting of targeted information.

These metrics collectively provide a comprehensive evaluation of the unlearning process by measuring its impact on both forgotten and retained knowledge.
\section{Overview of Unlearning Methods}
\label{appendix:overviewUnlearningMethods}
This section provides a detailed explanation of the unlearning methods discussed in the main text, describing their underlying principles and mathematical formulations.

\subsection{Gradient Ascent (GA)}
Gradient Ascent (GA) directly modifies the model’s behavior by applying optimization in the reverse direction of standard training. The objective function for GA is defined as:
\begin{equation}\label{eq:GA}
\resizebox{0.81\hsize}{!}{$
\mathcal{L}_{\text{GA}}(\mathcal{D}_{\text{F}};\theta) 
= - \mathbb{E}_{(x,y) \sim \mathcal{D}_{\text{F}}} \left[ -\log p(y|x; \theta) \right]. 
$}
\end{equation}
\subsection{Negative Preference Optimization (NPO)}
Negative Preference Optimization (NPO) treats unlearning as a preference optimization problem by discouraging responses associated with the forget set. It adapts Direct Preference Optimization (DPO) by treating answers in the forget set as undesirable and excluding positive terms from the DPO loss. The loss function for NPO is given by:
\begin{equation}\label{eq:NPO}
\resizebox{0.81\hsize}{!}{$
\mathcal{L}_{\text{NPO}}(\mathcal{D}_{\text{F}};\theta) 
= -\frac{2}{\beta} \mathbb{E}_{(x,y)\sim \mathcal{D}_{\text{R}}} \left[ \log \sigma \left(-\beta \log \frac{p(y|x;\theta)}{p(y|x;\theta_{\text{ref}})} \right) \right],
$}
\end{equation}
where $\beta$ is a hyperparameter, and $\theta_{\text{ref}}$ represents the reference model, typically the initial model before unlearning. NPO dynamically adjusts gradient weights, making it an adaptive form of GA.

\subsection{Direct Preference Optimization (DPO)}
Direct Preference Optimization (DPO) formalizes unlearning as a preference ranking problem by contrasting the probabilities of retaining and forgetting knowledge. In this approach, responses from the forget set are treated as negative examples, while predefined rejection responses are treated as positive.

\subsection{IDK Fine-tuning (IDK)}
IDK Fine-tuning reframes unlearning as an instruction-tuning task by relabeling forget set queries with predefined rejection templates. This ensures that the model responds with a standardized “I don’t know” response instead of recalling forgotten information. The objective function is:
\begin{equation}\label{eq:IDK}
\resizebox{0.81\hsize}{!}{$
\mathcal{L}_{\text{IDK}}(\mathcal{D}_{\text{F}},\mathcal{D}_{\text{IDK}};\theta) 
= \mathbb{E}_{x \sim \mathcal{D}_{\text{F}}, y \sim \mathcal{D}_{\text{IDK}}}  \left[ -\log p(y|x; \theta) \right].$}
\end{equation}
where $\mathcal{D}_{\text{IDK}}$ contains multiple rejection templates. By fine-tuning on these templates, the model systematically replaces knowledge recall with a controlled rejection response.
\subsection{Regularization Loss}
While the aforementioned losses focus solely on unlearning, a robust method must also preserve the model’s utility. To achieve this, a regularization loss is applied to the retain set, ensuring that useful knowledge remains intact.

\textbf{Gradient Descent (GD)} directly applies the standard prediction loss to the retain set, reinforcing learned knowledge:
\begin{equation}\label{eq:GD}
\resizebox{0.81\hsize}{!}{$
\mathcal{L}_{\text{GD}}(\mathcal{D}_{\text{R}};\theta) = \mathbb{E}_{{(x,y) \sim \mathcal{D}_{\text{R}}}} \left[ -\log p(y|x; \theta) \right].$}
\end{equation}

\textbf{Kullback-Leibler Divergence (KL)} maintains consistency between the unlearned and reference model predictions by minimizing KL divergence on the retain set:
\begin{equation}\label{eq:KL}
\resizebox{0.86\hsize}{!}{$
\mathcal{L}_{\text{KL}}(\mathcal{D}_{\text{R}};\theta) = \mathbb{E}_{(x,y) \sim \mathcal{D}_{\text{R}}} \left[\text{KL}(p(y|x; \theta) \Vert p(y|x; \theta_{\text{ref}}))\right].$}
\end{equation}

By combining different unlearning objectives with regularization losses, we obtain seven baseline methods: GA+GD, GA+KL, NPO+GD, NPO+KL, DPO+GD, DPO+KL, IDK+GD, and IDK+KL.

\section{Further Implementation Details}
All experiments are conducted on two NVIDIA RTX 6000 Ada GPUs. We utilize DeepSpeed with ZeRO3 to reduce memory costs. The AdamW optimizer is employed with a weight decay of 0.01, and all experiments use an effective batch size of 32.
To ensure a fair comparison across different unlearning methods, we adjust training epochs and the learning rate to maintain a Forget Efficacy within the range of 0.65 to 0.75. This range is selected to establish a common baseline for model utility across methods, ensuring that comparisons are not skewed by differences in the extent of forgetting. 
\label{appendix:implementationDetails}
\input{table/realworld_hyperparameters}
\input{table/tofu_hyperparameters}

\section{Detailed Explanation of Syntactically Similar Neighbor Set Construction}
\label{appendix:dataset_construction}

\paragraph{Definition of Syntactic Similarity.} We define syntactic similarity based on the Levenshtein similarity score. Specifically, we consider two questions to be syntactically similar if their Levenshtein similarity is at least 0.75. Conversely, if the similarity is 0.4 or lower, they are deemed syntactically different. These thresholds ensure a clear distinction between syntactically similar and different questions while allowing for slight variations in wording.

\paragraph{Ensuring Syntactic Distinctness in Other Neighbor Sets.} The syntactically similar neighbor set is the only set where elements share syntactic structures with the forget set. To ensure differentiation, all other neighbor sets (i.e., domain neighbor and entity neighbor sets) consist of questions classified as syntactically different (Levenshtein similarity $\leq 0.4$) from those in the forget set. This ensures that these sets are semantically related but do not overlap structurally with the forget set.

\paragraph{Clustering Criteria.} 
Each syntactic cluster is formed such that all elements within it are syntactically similar (Levenshtein similarity $\geq 0.75$). To ensure meaningful groupings, we define a cluster as valid only if it contains at least three elements. This criteria ensure that syntactically similar neighbor sets are well-defined and systematically constructed across both scenarios while maintaining clear distinctions from other neighbor sets.
\input{table/appendix_datastatistics_forget}
\input{table/appendix_datastatistics_neighbors}

\section{Detailed Prompts}
\input{table/appendix_prompt_masking}
\input{table/appendix_prompt_entity_qa}
\input{table/appendix_prompt_structure_qa}

\section{Detailed Forget Quality and Model Utility for Each Method in Each Experiment}
\label{appendix:detailedResultsPerMethods}
\input{table/appendix_realworld_no_regularization}
\input{table/appendix_tofu_no_regularization}
\input{table/appendix_realworld_yes_regularization_domain}
\input{table/appendix_realworld_yes_regularization_entity}
\input{table/appendix_realworld_yes_regularization_syntactic}
\input{table/appendix_realworld_paraphrase}
\input{table/appendix_realworld_domain_effects}