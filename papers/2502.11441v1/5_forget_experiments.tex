
\section{How does Performance Degradation Vary across Different Neighbor Sets?}
\label{sec:problem}
This section investigates how performance degradation after unlearning varies across different neighbor sets.
First, we examine which neighbor sets experience the most significant performance degradation. (Section~\ref{sec:5_1}) If similar syntactic structures sets are the most vulnerable to forgetting, we further examine whether domain differences within these structures lead to varying effects. (Section~\ref{sec:5_2}) We then examine the robustness of these forgetting patterns when questions are paraphrased. (Section~\ref{sec:5_3}) Finally, we analyze gradient updates during unlearning to understand the underlying mechanisms driving the observed patterns. (Section~\ref{sec:5_4})
% First, we examine which neighbor sets experience the most significant performance degradation. (Section~\ref{sec:5_1}) Finding that syntactically similar sets show the highest vulnerability, we then investigate whether domain differences within syntactic similarities create varying effects. (Section~\ref{sec:5_2}) We then examine the robustness of these forgetting patterns when questions are paraphrased. (Section~\ref{sec:5_3}) Finally, we analyze gradient updates during unlearning to understand the underlying mechanisms driving the observed patterns. (Section~\ref{sec:5_4})

\subsection{Analyzing Performance Drops Across Neighbor Sets}
\label{sec:5_1}
\vspace{2pt}
\textbf{Syntactically Similar Neighbor Set Experiences Higher Forgetting.} Across both real-world scenario and  TOFU evaluations (Figure~\ref{fig:real-world_main} and Figure~\ref{fig:tofu_main}), $\mathcal{N}_{\text{syntactically}}$ consistently demonstrates a higher utility drop compared to both $\mathcal{N}_{\text{domain}}$ and $\mathcal{N}_{\text{entity}}$. The greater utility drop suggests that syntactic similarity plays a crucial role in the forgetting phenomenon.  When the model is unlearning specific data, it appears to struggle more with retaining information that shares similar sentence structures, regardless of the specific domain or entities involved.

\noindent \textbf{No Significant Difference among Existing Neighbor Sets.} In the real-world scenario, a notable observation is the lack of significant performance differences between $\mathcal{N}_{\text{domain}}$ and $\mathcal{N}_{\text{entity}}$. As depicted in Figure~\ref{fig:real-world_main}, both sets exhibit similar RUD across all methods. Our results show that, despite different ways of defining neighbor sets in previous studies~\citep{choi2024opt, closerlookat}, the impact caused by unlearning is similar across them.

\subsection{Exploring Domain Effects on Forgetting in Syntactically Similar Cases}
\label{sec:5_2}
To examine the domain-specific effects of unlearning in syntactically similar cases, we conduct experiments in real-world scenario across five distinct categories: \textit{Human, Company, Creative Works, Fictional Character,} and \textit{Product}. This analysis builds on our previous findings that syntactically similar neighbor sets exhibit more pronounced forgetting than those based on domain or entity similarity.

Prior studies~\cite{rwku,maini2024tofu} define data distributions similar to $\mathcal{D}_f$ using either entity similarity or professional domain association, assuming that these sets would experience the most significant forgetting. Under this assumption, the Human category would be expected to exhibit the highest degree of forgetting, as entities within this category tend to be closely related. However, as shown in Figure~\ref{fig:variousfig}, our results reveal the opposite trend—across all evaluated methods except for DPO, non-human categories exhibit substantially higher forgetting rates. This finding challenges the conventional assumption that neighbor sets constructed based on entity or domain similarity necessarily lead to higher forgetting.
\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figure/03_various_entity_types.pdf}
    \caption{Relative Utility Drop across different entity categories (Human, Company, Creative Works, Fictional Character, and Product) for various unlearning methods.}
    \label{fig:variousfig}
\end{figure}
\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figure/02_paraphrase_experiments.pdf}
    \caption{Relative Utility Drop for syntactically similar and different neighbor sets across different unlearning methods, measured over three paraphrases per question. A larger drop indicates higher semantic forgetting.}
    \label{fig:paraphrasefig}
\end{figure}
\subsection{Robustness of Forgetting Patterns in Paraphrased Scenarios}
\label{sec:5_3}
Our previous experiments reveal that syntactically similar neighbor sets experience higher levels of forgetting compared to other neighbor sets. To validate the robustness of this finding, we investigate whether this performance gap persists even when questions are paraphrased. 

Specifically, we generate paraphrased versions for each question for syntactically similar and different neighbor sets using GPT-4o Then, we filter out cases where the pre-unlearning model fails to provide correct answers, ensuring that each question has three valid paraphrases. We then measure the RUD for these paraphrased questions using the post-unlearning model and compare the forgetting rates across the two groups.

Figure~\ref{fig:paraphrasefig} shows that even after paraphrasing, syntactically similar neighbors exhibit greater utility drops than dissimilar neighbors. This suggests that the model's increased forgetting isn't solely due to shared syntax, but also reflects a sensitivity to underlying semantic relationships. The consistent performance gap after paraphrasing reinforces the role of syntactic similarity in forgetting, highlighting its influence beyond surface-level wording.





% Specifically, we generate three paraphrased versions for each question in both syntactically similar and different neighbor sets. We then measure the RUD for these paraphrased questions and compare the forgetting rates across the two groups.


\subsection{Gradient Analysis}
\label{sec:5_4}
To further investigate the underlying mechanisms behind the forgetting patterns observed in syntactically similar and dissimilar neighbor sets, we analyze the gradient updates during the unlearning process. Our primary goal is to understand how the model's gradient norms evolve when encountering different types of neighbors, particularly whether syntactically similar instances influence each other more strongly than dissimilar ones.

In our experimental setup, we perform gradient ascent on a syntactically similar set and track the changes in gradient norms as the model encounters other syntactically similar or syntactically different instances. Specifically, we measure the Frobenius norm of the model’s weight gradients at each unlearning step, comparing how the gradients behave when interacting with different types of data points.


\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figure/04_gradient_norm_plot.pdf}
    \caption{Frobenius norm of model weight gradients across unlearning steps. The gradient norms for syntactically similar instances (red) increase more steeply than those for syntactically different instances (blue).}
    \label{fig:gradientanalysis}
\end{figure}
As shown in Figure~\ref{fig:gradientanalysis}, the gradient norms of syntactically similar instances exhibit a steeper increase over unlearning steps compared to syntactically different instances. Notably, the initial gap between their gradient norms at the first checkpoint widens progressively as unlearning proceeds. This suggests that forgetting syntactically similar knowledge amplifies gradient updates in a way that reinforces the distinction between similar and dissimilar instances.
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figure/05_heatmap_figure.pdf}
    \caption{Relative utility drop (\%) averaged across all unlearning methods (GA, DPO, NPO, and IDK) under different retain set configurations using GD (left) and KL (right) regularization. The x-axis represents the type of train retain set, while the y-axis represents the type of test retain set. A higher value (darker color) indicates better utility retention. Detailed relative utility drop results for each individual unlearning method can be found in Appendix~\ref{appendix:detailedResultsPerMethods}.}
    \label{fig:regularizationheapmap}
    %\vspace{-3mm}
\end{figure*}