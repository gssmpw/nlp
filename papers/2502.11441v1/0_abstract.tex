%<고칠점>
% - 기존 연구방식이 왜 문제인지 보충
% - retain set이 뭔지 모름. 설명
% - retain set challenge 설명
%</고칠점>
\begin{abstract}
\begin{comment}
Large language models (LLMs) raise concerns about retaining unauthorized content from training data, leading to issues like privacy violations. LLM unlearning aims to mitigate these risks by removing specific information while preserving overall model performance. Existing research primarily focuses on methods for effective forgetting, but lacks a comprehensive understanding of the retain set itself. The retain set refers to the portion of the training data not targeted for unlearning, and preserving its performance is crucial for practical unlearning.  A key challenge hindering wider adoption of unlearning is the significant performance degradation observed on the retain set, making it essential to understand which parts of this set are most vulnerable. This paper investigates the impact of unlearning on various subsets of the retain set, focusing on entity unlearning as a case study. We introduce the Syntactically Similar Neighbor Set, comprising questions with similar syntactic structures to the data being unlearned, and demonstrate that it experiences the most significant performance degradation during unlearning. Furthermore, we find that using this set for regularization during unlearning not only best preserves performance on syntactically similar data but also achieves comparable or superior performance across other data subsets. Our findings suggest that syntactic similarity is a more crucial factor than previously considered domain or entity relationships for effective and practical LLM unlearning.
\end{comment}
Large language models (LLMs) risk retaining unauthorized or sensitive information from their training data, which raises privacy concerns. LLM unlearning seeks to mitigate these risks by selectively removing specified data while maintaining overall model performance. However, most existing work focus on methods to achieve effective forgetting and does not provide a detailed analysis of the retain set, the portion of training data that is not targeted for removal. 
In this paper, we investigate the effects of unlearning on various subsets of the retain set through a case study on entity unlearning. We introduce the \textit{Syntactically Similar Neighbor Set}, a group of queries that share similar syntactic structures with the data targeted for removal, and show that this subset suffers the greatest performance drop during unlearning. Moreover, when used for regularization, this set not only preserves performance on syntactically similar queries but also delivers comparable or improved results across other data subsets. Our results highlight that syntactic similarity is a critical factor, potentially more so than domain or entity relationships, in achieving effective and practical LLM unlearning.
\end{abstract}


