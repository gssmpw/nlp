\section{Preliminaries}
\subsection{Language Model Unlearning}
Let a LLM be parameterized by $\bm{\theta}$ and trained on a dataset $\mathcal{D}$, which consists of a forget set $\mathcal{D}_f$ and a retain set $\mathcal{D}_r = \mathcal{D} \setminus \mathcal{D}_f$. The goal of unlearning is to obtain a new set of parameters $\bm{\theta'}$ that removes knowledge from $\mathcal{D}_f$ while preserving performance on $\mathcal{D}_r$.
\subsection{Entity Unlearning}
Entity unlearning~\citep{maini2024tofu, rwku} aims to remove knowledge associated with specific entities from the LLM. Let $\mathcal{E} = \{e_1, ..., e_n\}$ represent the set of entities to be forgotten, where each entity $e_i$ is represented by a collection of question-answer pairs: $e_i = \{(q_{i,1}, a_{i,1}), ..., (q_{i,m}, a_{i,m})\}$. Thus, the forget set can be expressed as $\mathcal{D}_f = \bigcup_{i=1}^{n} \bigcup_{j=1}^{m} (q_{i,j}, a_{i,j})$.

\subsection{Evaluating Retain Set Preservation}
\label{subsec:EvaluatingRetainSetPreservation}
Since $\mathcal{D}_r$ comprises the entire training set except for $\mathcal{D}_f$, evaluating all of $\mathcal{D}_r$ is impractical. Prior work~\cite{maini2024tofu, rwku} addresses this challenge through two main approaches. First, they assess performance on general knowledge benchmarks such as MMLU~\citep{hendrycks2021measuring} to ensure broad knowledge retention. Second, they evaluate on neighbor sets, which are subsets of $\mathcal{D}_r$ that are expected to be most affected by the unlearning process. These sets are constructed based on the assumption that data points similar to the forget set are more likely to be impacted during unlearning.
Previous work has identified two primary types of neighbor sets:

\noindent
\textbf{Domain Neighbor Set} ($\mathcal{N}_{\text{domain}}$): Instances related to the same professional domain as the forget set~\citep{closerlookat, maini2024tofu}. For example, if $\mathcal{D}_f$ consists of data about J.K. Rowling, $\mathcal{N}_{\text{domain}}$ may include information about other authors such as Ian McEwan.

\noindent
\textbf{Entity Neighbor Set} ($\mathcal{N}_{\text{entity}}$): Instances involving entities closely associated with those in $\mathcal{D}_f$~\citep{rwku, choi2024opt}. For example, if J.K. Rowling is in $\mathcal{D}_f$, then $\mathcal{N}_{\text{entity}}$ may include information about Daniel Radcliffe, the lead actor in the Harry Potter films.

Expanding on the concept of neighbor sets, we propose a new type of neighbor set based on syntactic similarity. 
%Existing neighbor sets primarily rely on topical or entity relationships, but we observe that performance degradation often extends to syntactically similar instances. 
While existing neighbor sets rely mainly on topical or entity relationships, we observe that performance degradation can also affect instances that share similar syntactic structures. 
We define the \textbf{Syntactically Similar Neighbor Set} ($\mathcal{N}_{\text{syntactically}}$) as a subset of $\mathcal{D}_r$ containing questions with syntactic structures similar to those of $\mathcal{D}_f$. For example, if $\mathcal{D}_f$ contains multiple instances of the form “When was X born?”, $\mathcal{N}_{\text{syntactically}}$ consists of similarly structured questions.

To construct $\mathcal{N}_{\text{syntactically}}$, we use a two-step process that quantifies syntactic similarity between sentences. First, we perform entity masking using GPT-4o~\citep{hurst2024gpt4o} to replace named entities such as person names, dates, and organization names. This allows us to focus on the structural aspects of the sentences while minimizing the influence of specific entities. Let $s_1'$ and $s_2'$ represent the masked versions of sentences $s_1$ and $s_2$, respectively.
Next, we define the Levenshtein similarity based on the Levenshtein distance between the masked sentences. The Levenshtein distance~\cite{LevenshteinDistance} measures the minimum number of edit operations (insertions, deletions, or substitutions) needed to transform one string into another. We normalize this distance into a similarity score using:
\begin{equation}
\label{eq:LevenshteinSimilarity}
\resizebox{0.89\hsize}{0.055\hsize}{$
\text{LevenshteinSimilarity}(s_1, s_2) = 1 - \frac{\text{LevenshteinDistance}(s_1', s_2')}{\max(\text{len}(s_1'), \text{len}(s_2'))}$}
\end{equation}
\input{table/algorithm}
