\section{Related Work}
LLM unlearning~\citep{jang2023knowledgeunlearning, yao2023llmunlearningsurvey, lynch2024eight} has gained significant attention as a method to enhance privacy. Various approaches~\citep{sinha2024unstar, zhang2024npo} have been proposed to ensure that models effectively erase specific information while maintaining overall performance. A key challenge in unlearning is assessing whether knowledge unrelated to the forget set is inadvertently affected. To evaluate this, researchers commonly examine general knowledge~\citep{hendrycks2021measuring, cobbe2021training} as well as a designated subset of the retain set that shares a similar distribution with the forget set but excludes the targeted information. These subsets, often referred to as neighbor sets~\citep{closerlookat}, help determine the extent of unintended degradation in model performance.

In hazardous knowledge unlearning, prior work has leveraged domain-relevant general knowledge as a benchmark. For instance,~\citet{li2024wmdp} employs general biology knowledge to assess the impact of bioweapon-related unlearning and general computer security knowledge to evaluate the removal of information related to Attacking Critical Infrastructure. For entity unlearning~\citep{maini2024tofu, rwku}, previous studies have used entities from similar professions or those closely linked to the target entity as neighbor sets. While these approaches provide an initial framework, they lack a systematic investigation of which aspects of the retain set suffer the most from unlearning. Our study addresses this gap by systematically investigating the impact of unlearning on different types of neighbor sets more clearly and identifying which knowledge components experience the highest degree of forgetting.