\section{What is the Optimal Neighbor Set for Effective Regularization?}
\label{sec:solution}
To preserve model utility during unlearning, regularization losses on a subset of the retain set are commonly employed during the unlearning process~\cite{closerlookat,maini2024tofu}. Based on the findings of the previous section, we aim to identify the optimal configuration of the retain set used for regularization, to optimize model utility while ensuring successful forgetting, specifically from a data perspective.

\paragraph{Regularization loss.} It encourages the unlearned model parameters $\bm \theta$ to preserves model utility. A typical unlearning objective function, computed on a subset of $\mathcal{D}_{\text{R}}$, is formulated as follows:
\begin{equation}
    \resizebox{0.71\hsize}{!}{$
    \underset{\bm \theta}{\min} \mathcal{L}(\bm \theta) = \underset{\bm \theta}{\min} - \mathcal{L}_f(\bm \theta) +  \mathcal{L}_{\text{R}}(\theta;\mathcal{D}_{\text{R}}).$}
    \label{eq:previous_objective}
\end{equation}
Our analysis considers two primary regularization approaches: Gradient Descent (GD) and Kullback-Leibler Divergence (KL). A comprehensive explanation of these methods is provided in Appendix~\ref{appendix:overviewUnlearningMethods}.
\paragraph{Setup.} To determine the optimal train retain set configuration, we conduct comprehensive experiments examining nine different combinations of train and test retain sets, using $\mathcal{N}_{\text{domain}}$, $\mathcal{N}_{\text{entity}}$, and $\mathcal{N}_{\text{syntactically}}$ for both training and evaluation. For each train retain set, we apply different unlearning methods (GA, DPO, NPO, and IDK) with regularization loss and report the average RUD across test retain sets.

\paragraph{Results.} We visualize the results separately for GD and KL regularization in Figure~\ref{fig:regularizationheapmap}. The results reveal two key findings:

\noindent
\textbf{1) Training with $\mathcal{N}_{\text{syntactically}}$ effectively preserves performance on $\mathcal{N}_{\text{syntactically}}$.} In both GD and KL regularization heatmaps, the bottom row (Test Retain Set: Syntactically Similar) shows that training with $\mathcal{N}_{\text{syntactically}}$ preserves utility best, with average differences of 14.7\% point and 7.35\% point compared to other training sets, respectively.

\noindent
\textbf{2) Training with $\mathcal{N}_{\text{syntactically}}$ contributes to robust performance across various neighbor sets.} Beyond preserving performance on syntactically similar data, training with $\mathcal{N}_{\text{syntactically}}$ also yields competitive results when evaluated on $\mathcal{N}_{\text{entity}}$ and $\mathcal{N}_{\text{domain}}$. In many cases, it surpasses or closely matches the performance achieved by training with other neighbor sets. These findings emphasize the role of syntactically similar examples in reducing utility loss while unlearning.