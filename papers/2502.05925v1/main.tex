%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables


\usepackage{times}
\usepackage{soul}
\usepackage{url}
% \usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}
\usepackage{subcaption}
\usepackage{adjustbox}
\usepackage{float}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Sign-Symmetry Learning Rules are Robust Fine-Tuners}

\begin{document}

\twocolumn[
\icmltitle{Sign-Symmetry Learning Rules are Robust Fine-Tuners}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Aymene Berriche}{equal,comp}
\icmlauthor{Mehdi Zakaria Adjal}{equal,comp}
\icmlauthor{Riyadh Baghdadi}{comp}
\end{icmlauthorlist}

% \icmlaffiliation{yyy}{Université Grenoble Alpes, Grenoble, France}
\icmlaffiliation{comp}{New York University Abu Dhabi, Abu Dhabi, United Arab Emirates}
% \icmlaffiliation{sch}{Université Paris Cité, Paris, France}

\icmlcorrespondingauthor{Aymene Berriche}{ab11904atnyudotedu}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Learning Theory, Bio-plausible Neural Network, Sign-symmetry, Adversarial Robustness, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}

Backpropagation (BP) has long been the predominant method for training neural networks due to its effectiveness. However, numerous alternative approaches, broadly categorized under feedback alignment, have been proposed—many of which are motivated by the search for biologically plausible learning mechanisms. Despite their theoretical appeal, these methods have consistently underperformed compared to BP, leading to a decline in research interest. In this work, we revisit the role of such methods and explore how they can be integrated into standard neural network training pipelines. Specifically, we propose fine-tuning BP-pre-trained models using sign-symmetry learning rules and demonstrate that this approach not only maintains performance parity with BP but also enhances robustness. Through extensive experiments across multiple tasks and benchmarks, we establish the validity of our approach. Our findings introduce a novel perspective on neural network training and open new research directions for leveraging biologically inspired learning rules in deep learning.


% Backpropagation (BP) has long been the cornerstone of deep neural network training. While neural networks trained with backpropagation typically have high accuracy and precision, they suffer from limitations in their robustness to adversarial perturbation. Biologically plausible (bio-plausible) learning rules, on the other hand, are more robust. Yet, they typically underperform in terms of accuracy and precision, which has limited their widespread adoption. In this work, we aim to bridge this gap. We propose a novel approach that combines backpropagation pre-training with bio-plausible Sign-Symmetry learning rules for fine-tuning neural networks. We explore the effectiveness of this approach in two tasks, image classification, and image retrieval, then demonstrate that it improves robustness against gradient-based adversarial attacks while offering comparable accuracy and precision compared to the use of backpropagation alone. These findings show the benefit of mixing backpropagation and bio-plausible learning rules, suggesting the need for further research by the community to evaluate this approach on other tasks.
\end{abstract}


\section{Introduction}
The human cerebral cortex learns through updating synapses based on input signals that activate multiple regions involved in the learning process~\cite{hebb2005organization,markram1997regulation,bliss1973long}. The complex multilayered structure of our brain's neural network makes it challenging to determine the exact mechanism responsible for learning in the brain~\cite{lillicrap2020backpropagation}. A common way to address this problem in Deep Artificial Neural Networks is through a credit assignment algorithm responsible for updating synaptic connections based on feedback signals. These algorithms typically differ in the way the feedback signal is backpropagated~\cite{lansdell2019learning}. The most widely used credit assignment algorithm is Backpropagation (BP)~\cite{rumelhart1986learning}. While BP has been an indispensable tool for building effective neural networks, studies have shown that it is not robust to perturbation attacks~\cite{szegedy2013intriguing}. Adversarial perturbation is a phenomenon commonly observed in neural networks, wherein carefully crafted inputs can deceive models into making incorrect predictions. An attacker can exploit the weaknesses of a trained neural network by introducing subtle perturbations to an input image that are not perceivable to the human eye but significantly alter the model's output. This alteration could be as simple as adding a small amount of noise to the image (e.g., slightly changing pixel values). Such modifications can result in the model's accuracy dropping dramatically, even though the altered inputs are nearly indistinguishable from legitimate inputs~\cite{zhang2019limitations}. Bio-plausible learning rules generally refer to training methods that closely mimic the mechanisms observed in biological systems. In this approach, properties found in natural learning mechanisms are applied to artificial neural networks, aligning them with their natural counterparts. Examples of these learning rules include Feedback Alignment (FA)~\cite{lillicrap2014random} and Sign-Symmetry rules, such as Fixed Random-magnitude Sign-concordant Feedback (frSF), Batchwise Random Magnitude Sign-concordant Feedbacks (brSF), and Uniform Sign-concordant Feedback (uSF)~\cite{liao2016important}. These methods typically underperform in terms of accuracy and precision when compared to backpropagation, indeed, multiple studies have demonstrated that the performance disparity between BP and FA can be substantial. For example, ~\cite{bartunov2018assessing} showed that, in the image classification task, BP achieves a top-1 error rate reaching 71.43\% outperforming the 93.03\% achieved by FA (on ImageNet), this performance gap compared to backpropagation has significantly limited their widespread adoption.
In this work, we aim to bridge the gap between BP and bio-plausible methods. We propose to reconsider the integration of bio-plausible methods when training deep neural networks through backpropagation. 

We propose a novel approach where neural networks are pre-trained using backpropagation and fine-tuned with bio-plausible learning rules, specifically Sign-Symmetry. We evaluate this method on image classification and hashing-based image retrieval, showing improved robustness against gradient-based adversarial attacks while maintaining performance close to backpropagation. This robustness stems from bio-plausible learning algorithms' independence from precise gradient computation. Our experiments span various backbones (AlexNet, VGG16, ResNet-18), learning rules (uSF, frSF, brSF), attacks (two per task), and datasets (at least three per task). Results consistently highlight the benefits of combining backpropagation and sign symmetry, encouraging further exploration of this approach across other tasks.

Our hypothesis suggests that while BP demonstrates superior capability in representation learning during the pre-training phase (which has already been shown in \cite{bartunov2018assessing}), both BP and Sign-Symmetry methods achieve comparable results in task adaptation (transfer learning) during fine-tuning. Furthermore, Sign-Symmetry methods exhibit an inherent advantage in robustness. The contributions of this paper are as follow:

\begin{itemize}
    \item We propose a novel training approach for deep neural networks that combines backpropagation for pre-training with fine-tuning using sign-symmetry learning rules.
    \item We demonstrate that this method significantly enhances robustness against gradient-based adversarial attacks while maintaining performance comparable to standard BP.
    \item We further validate the specific impact of sign-symmetry on attacks that exploit knowledge of model weights by benchmarking against black-box attacks.
    \item Finally, our findings open new research directions for exploring the broader implications of this training strategy across various neural network properties.
\end{itemize}


\begin{figure*}[h!]
    \centering
    \includegraphics[width=\linewidth]{assets/weight_transport.pdf}
    \caption{Credit assignment methods can be ranked based on the amount of weight transport each method employs. Backpropagation is known to use full-weight transport, as the algorithm utilizes the same forward weight matrix to conduct the backward pass. Uniform Sign-concordant Feedback (uSF) uses the sign of the weight matrix as a backward matrix, thus using less information than BP. Further Fixed Random Magnitude Sign-Concordant Feedback (frSF) and Batchwise Random Magnitude Sign-concordant Feedback (brSF) add approximation to the sign of the weight matrix, which introduces more randomness and uses less weight information. Finally, Feedback Alignment (FA) uses no weight transport, relying instead on a fixed random matrix to conduct the backward pass.}
    \label{fig:WTRanking}
\end{figure*}


\section{Related Work}

\subsection{Backpropagation and Bio-plausible Learning Rules}
Backpropagation, while efficient, is often criticized for its biological implausibility, especially regarding the weight transport problem~\cite{grossberg1987competitive,crick1989recent,schwartz1993computational}. Unlike the brain, which uses asymmetrical feedback signals, backpropagation employs identical weights for forward and backward passes~\cite{lillicrap2020backpropagation}. Efforts to create biologically plausible credit assignment methods focus on reducing weight transport. Figure~\ref{fig:WTRanking} illustrates methods relevant to this study, categorized by their dependence on weight symmetry.

Backpropagation relies entirely on a symmetrical feedback structure, while Sign-Symmetry~\cite{liao2016important} employs weight sign matrices, reducing the extent of weight transport. Variants include Uniform Sign-Concordant Feedback (uSF), Fixed Random Magnitude Sign-Concordant Feedback (frSF), and Batchwise Random Magnitude Sign-Concordant Feedback (brSF). These approximate gradients enable learning without temporally synchronized gradients~\cite{czarnecki2017understanding}. Feedback Alignment (FA) \cite{lillicrap2016random} uses fixed random feedback matrices, avoids weight transport entirely and demonstrates that symmetry is unnecessary for training. This mechanism aligns forward synaptic connections with synthetic feedback, making errors derived by feedforward weights converge toward those calculated by synthetic backward matrices~\cite{lillicrap2020backpropagation}. Additionally, biologically plausible methods could overcome backpropagation's sequential nature, which limits computational efficiency. Techniques like Target Propagation~\cite{le1986learning,hinton2007backpropagation,bengio2014auto,lee2015difference} use local updates but perform poorly at scale~\cite{bartunov2018assessing}, discouraging further exploration.

% \subsection{The Robustness of Bio-plausible Learning Rules}
% When it comes to assessing the robustness of bio-plausible learning rules, relatively little work has been conducted in this area. In the study by~\cite{sanfiz2021benchmarking}, the authors evaluated the robustness of bio-plausible credit assignment methods, including FA, uSF, frSF, and others. Their findings demonstrated that these methods exhibit enhanced performance under certain conditions. The study employed various attack methods, including white-box (gradient-based) attacks such as FGSM~\cite{goodfellow2014explaining}, PGD~\cite{madry2017towards}, APGD~\cite{zimmermann2019comment}, and TPGD~\cite{zhang2019theoretically}. For black-box attacks, the researchers utilized the Few-pixel attack~\cite{su2019one} and Square attack~\cite{andriushchenko2020square}. Results from the white-box attacks were particularly useful in highlighting the differences in robustness between backpropagation (BP) and bio-plausible learning rules. However, black-box attacks yielded similar results across the methods and did not provide sufficient evidence to claim any general tendencies in robustness.
\section{Background}
\subsection{Learning Algorithms}

In the following, we consider a fully connected neural network with \(L\) layers. \(W_l\) is the weight matrix for layer \(l\). \(\mathbf{a}_l\) denotes the pre-activation of layer \(l\), and satisfies \(\mathbf{h}_l = \mathbf{f}(\mathbf{a}_l)\), where \(\mathbf{h}_l\) is the activation vector for layer \(l\) and \(\mathbf{f}\) is a non-linearity. The network final output is denoted \(\hat{y}\). We calculate the error using the squared error \(E =\frac{1}{2} \sum_k\left(y_k-\hat{y}_k\right)^2\).

\paragraph{The Backpropagation algorithm} In backpropagation, we compute the exact error for each parameter of the network and update its value using the negative of its gradient, given in vector/matrix notation: \cite{lillicrap2020backpropagation}:
\begin{equation}
    \Delta W_{l}=-\eta \frac{\partial E}{\partial W_{l}} = -\eta \boldsymbol{\delta}_l \mathbf{h}_{l-1}^{\top}
    \label{eq:bp_weight_update}
\end{equation}

Where \(\eta\) is the learning rate and \(\delta\) is referred to as the error signal and is computed recursively via the chain rule:

\begin{equation}
    \boldsymbol{\delta}_l=\left(W_{l+1}^{\top} \boldsymbol{\delta}_{l+1}\right) \circ \mathbf{f}^{\prime}\left(\mathbf{a}_l\right).
\end{equation}
Where \(\circ\) is the Hadamard product. This equation demonstrates the full symmetry of backpropagation, as the full weight matrix is used to calculate the backward update.

\paragraph{Feedback Alignment} In Feedback Alignment~\cite{lillicrap2014random}, the update of the synaptic connections is similar to BP given by \eqref{eq:bp_weight_update}, but instead of using the weight matrix to perform the backward update, a synthetic fixed random matrix (note that,  the exact distribution is often not critical, as long as the feedback matrices remain fixed after initialization and provide a roughly consistent direction for error propagation) is defined for each layer which we denote \(B_l\). The error signal error in FA is then given by:
\begin{equation}
        \boldsymbol{\delta}_l=\left(B_{l+1} \boldsymbol{\delta}_{l+1}\right) \circ \mathbf{f}^{\prime}\left(\mathbf{a}_l\right).
\end{equation}

FA introduces an unconventional method of updating synaptic connections compared to BP and employs a much simpler approach to circumvent the weight transport problem. This algorithm has been criticized for its lower performance compared to BP~\cite{bartunov2018assessing}. 
Sign-Symmetry methods try to mitigate this problem and therefore show more promising results.

\paragraph{Sign-Symmetry} The Sign-Symmetry algorithm~\cite{liao2016important} shares similarities with Feedback Alignment in that it tries to mitigate the biological problem of weight transport and introduces symmetrical sign-sharing in feedback (instead of a random matrix, it uses the sign matrix of the feedforward weights). This algorithm strikes a balance between weight transport considerations and biological plausibility, leveraging benefits from both principles' benefits. 
Let's denote by $V$ the feedback weight matrix used in Sign-Symmetry. In backpropagation, the backward pass is done using the matrix of weights, i.e. $V = W^T$. In Sign-Symmetry, $V$ is defined through multiple variants of \textit{asymmetric backpropagation}, while \citeauthor{liao2016important} have explored various choices of asymmetric feedback, we only introduce the ones that are relevant to our work:

\begin{enumerate}
    \item Uniform Sign-concordant Feedback (uSF): \(V = ~\mathrm{sign}(W^\top)\)
    \item Fixed Random-magnitude Sign-concordant Feedback (frSF): \(V = M \circ \mathrm{sign}(W^\top)\), where \(M\) is defined as a matrix of uniform random numbers and is initialized once and fixed through the training.
    \item Batchwise Random Magnitude Sign-concordant Feedbacks (brSF): This is a variation of frSF where the magnitude matrix \(M\) is redrawn after each parameter update.
\end{enumerate}

In this work, we focus on sign symmetry methods, exploring their potential to enhance robustness while maintaining performance when used as fine-tuners.


\subsection{Adversarial Robustness}  
Adversarial robustness quantifies a machine learning model's ability to withstand inputs intentionally perturbed to induce erroneous predictions. Evaluating and enhancing this property is of paramount importance, particularly in applications where reliability and trustworthiness are critical. Adversarial attacks are broadly categorized based on the level of access and information available to the attacker:  

\subsubsection*{White-Box Attacks}  
White-box attacks leverage access to the gradient information of the model to craft perturbations that maximize the classification loss. The perturbations are tailored to deceive the model effectively while remaining imperceptible to human observers.  

\paragraph{Classification Attacks}  
Adversarial classification attacks target discriminative models to alter their predictions through carefully crafted input perturbations. Prominent examples include Fast Gradient Sign Method (FGSM)~\cite{goodfellow2014explaining}.  
    This method generates adversarial examples through a single-step optimization of the loss function:  
    \begin{equation}  
        x^{\mathrm{adv}} = x + \epsilon \cdot \operatorname{sign}(\nabla_x \mathcal{L}(f(x), y^*))  
    \end{equation}  
    where \(x\) is the input, \(y^*\) the true label, \(f(x)\) the model prediction, and \(\mathcal{L}\) the loss function. The scalar \(\epsilon\) controls the magnitude of perturbation. While computationally efficient, FGSM suffers from suboptimal performance in scenarios requiring fine-grained adversarial modifications. Projected Gradient Descent (PGD)~\cite{madry2017towards} is an iterative variant of FGSM. It requires a step size \(\alpha\) and a number of steps \(k\) as hyperparameters. The update is given by:

\begin{equation*}
    x_t^{\mathrm{adv}} = \left\{\begin{array}{ll}
         x_0& ,t=0 \\
         \operatorname{Clip}_x^\epsilon\left(x_{t-1}^{\mathrm{adv}}+\alpha \operatorname{sign}\left(\nabla_{x_{t-1}^{\mathrm{adv}}} \mathcal{L}\left(f\left(x_{t-1}^{\mathrm{adv}}\right) y^*\right)\right)\right)&, t\leq k 
    \end{array}\right.
\end{equation*}

where \( \operatorname{Clip}_x^\epsilon\left(\cdot\right)\) is a function that ensures the input values remain within the \(\epsilon\)-ball centered on the original image \(x\). 

\paragraph{Hashing Attacks}  
Hashing-based models, commonly used in information retrieval tasks, are also susceptible to adversarial perturbations. The goal in this context is to perturb the input such that the resulting hash code differs significantly from the original, thereby degrading retrieval accuracy. Hamming Attack Gradient (HAG)~\cite{yang2018adversarial}:  
    HAG maximizes the Hamming distance between hash codes of original and adversarial examples. Due to the discrete nature of hash codes, surrogate gradients are employed to approximate their effects, enabling effective optimization. Smart Discrete Hashing Attack (SDHA)~\cite{lu2021smart}:  
    SDHA improves upon HAG by focusing on specific hash code dimensions that significantly impact retrieval rankings. 


In both classification and hashing scenarios, white-box adversarial attacks exploit detailed gradient information, making them highly effective but reliant on substantial model transparency.  


\subsubsection*{Black-Box Attacks}  
Black-box adversarial attacks operate under the constraint of limited information about the target model. Unlike white-box attacks, these methods do not rely on gradients or knowledge of the model architecture, instead using input-output queries to infer the model's behavior and craft adversarial examples. Boundary Attack~\cite{brendel2017decision} is one example of black-box attacks, a decision-based approach that begins with a high-perturbation adversarial example and iteratively refines it to minimize the perturbation while ensuring it remains adversarial. This method requires no access to gradients or model internals, making it a pure black-box strategy. HopSkipJump Attack (HSJA)~\cite{chen2020hopskipjump} is another example, a query-efficient black-box attack that estimates gradients indirectly through binary input-output feedback. HSJA is highly efficient, requiring significantly fewer queries compared to other black-box methods, making it well-suited for scenarios where query budgets are limited. Its effectiveness stems from its ability to approximate gradient-based optimization without access to actual gradient information.  


\section{Method}

\paragraph{Tasks} We apply our proposed training method to two tasks: image classification, and hashing-based image retrieval.
%where we consider both performance measured by accuracy and the robustness of each training approach, we also test our method on
Since the first is well known, we will provide more details about the second.
This task involves mapping high-dimensional image data to compact binary codes that preserve semantic similarity~\cite{hussain2022efficient,xia2014supervised}. The process consists of two main steps: first is to learn a hash function to map input images to continuous encodings while preserving semantic relevance~\cite{luo2022image}, and the second is to generate binary codes by binarizing the hidden representations using a sign function~\cite{yang2022deep}. Retrieval is then performed by comparing binary codes using Hamming distance, enabling efficient matching~\cite{fang2021scalable}.

\paragraph{Learning Method and Model Architectures} We propose to use backpropagation for pre-training followed by the use of a Sign-Symmetry credit assignment method for fine-tuning. We use a CNN-based backbone model that was pre-trained on ImageNet (\texttt{IMAGENET1K\_V1}) using backpropagation (e.g., RestNet-18).
We then append a task-specific fully connected layer to adapt the network to the particular task, either a hash layer or a classification layer, with its parameters trained from scratch. Subsequently, we fine-tune all network parameters using a Sign-Symmetry method (e.g., uSF). We do not freeze the weights of the backbone during fine-tuning. We use the hashing loss HyP\(^2\) \cite{xu2022hyp2} to fine-tune the task of image retrieval, setting the hashing size \(k\) to 32, while we use the Cross-Entropy loss function for image classification.

\section{Experimental Setup}


\paragraph{Models}
%We employ pre-trained backbone networks that were initially trained on the \texttt{IMAGENET1K\_V1} dataset. To maintain reasonable experiment durations,
We evaluate our method using three different backbone architectures: AlexNet~\cite{krizhevsky2012imagenet}, ResNet-18~\cite{he2016deep}, and VGG-16~\cite{simonyan2014very}. These pre-trained backbone models were obtained from the torchvision model repository. The motivation behind using these backbones is that they already have established biologically plausible definitions for their respective architectures. In our approach, we propose a new method for training neural networks rather than a biologically plausible version of AlexNet or VGG-16 (work that has already been done). 

\textbf{Datasets} We evaluate our approach across varying complexity levels, following established protocols. For image classification, we use CIFAR-10~\cite{krizhevsky2009learning}, MS-COCO~\cite{lin2014microsoft}, NUS WIDE~\cite{chua2009nus}, and ImageNet~\cite{deng2009imagenet} dataset. Adversarial attacks were tested on 5,000 images per dataset, based on the method from~\cite{vuyyuru2020biologically}.

For hashing-based image retrieval, we use CIFAR-10, NUS WIDE~\cite{chua2009nus}, MS-COCO~\cite{lin2014microsoft}, and ImageNet100. CIFAR-10 has 500 images per class for training, and 100 for testing/validation. NUS-WIDE is filtered to 148,332 images, MS-COCO to 20,000, and ImageNet100 contains 13,000 training and 5,000 testing/validation images. The remaining images are used as the query database.

\paragraph{Training} Pre-trained backbones, initially trained on ImageNet, were loaded through torchvision. For fine-tuning in both tasks, the same set of hyperparameters was employed. We used ADAM as the optimizer with \(\beta_1 = 0.9\) and \(\beta_2 = 0.999\), and a weight decay of 0.0005. The learning rate was set to \(10^{-5}\) for all layers except the classification/hashing layer, which was assigned a learning rate of \(10^{-4}\). We fine-tuned each dataset for 20 epochs using a batch size of 32, apart from ImageNet and NUS WIDE, that have been fine-tuned for 10 epochs.

\paragraph{Evaluation Metrics}
%To generalize our findings on the performance of this training approach, we evaluate the method on another demanding task: hashing-based image retrieval. This task heavily relies on high-quality internal representations, making it an ideal benchmark to assess our method.
We assess performance using two metrics: accuracy for the classification task and mean average precision (mAP) for the hashing-based image retrieval task, which are both widely used for the tasks we consider~\cite{singh2022learning,berriche2024leveraging,bartunov2018assessing}.
For the task of image retrieval, we follow the practices established in the literature~\cite{cao2017hashnet,schwengber2023deep,berriche2024leveraging} and compute the \texttt{mAP@k} with \(k = 5000\) for CIFAR-10, NUS-WIDE, and MS COCO datasets, and with \(k = 1000\) for ImageNet, The mAP metric is defined as follows:
\begin{equation}
\texttt{mAP@k} = \frac{1}{|Q|} \sum_{q \in Q} AP_k(q)
\end{equation}
where \(Q\) represents the set of queries, and \(AP_k\) denotes the average precision of the first \(k \leq n\) retrieved entries..


\paragraph{Adverserial Robustness Evaluation Setup}
Following prior studies on adversarial robustness~\cite{athalye2018obfuscated,carlini2019evaluating,uesato2018adversarial,yuan2023semantic} and the robustness of bio-inspired methods~\cite{vuyyuru2020biologically}, we adopt a similar experimental setup with some adjustments. Adversarial attacks experiments for classification models were implemented using Foolbox~\cite{rauber2017foolbox}, using the \(L_\infty\) variants of FGSM and PGD. FGSM is a single-step attack, while PGD is run for 5 iterations with a step size \(\alpha = \epsilon / 3\). Perturbation magnitude \(\epsilon\) varies from 0 to 0.5, then the robust accuracy is recorded. To demonstrate that Sign-Symmetry learning rules specifically enhance robustness against gradient-based attacks while leaving models as vulnerable as the classical BP fine-tuned ones to gradient-free adversarial strategies, we conduct similar experiments on black-box attacks where we use HopSkipJump (HSJA) \cite{chen2020hopskipjump} and Boundary Attack (BA) \cite{brendel2017decision}. 

For hashing-based image retrieval, robustness is assessed against HAG and SDHA non-targeted attacks, using 5 iterations and perturbation magnitudes ranging from 0.001 to 0.5. These settings prioritize computational efficiency over optimal attack strength, focusing on the comparative robustness of learning methods under adversarial perturbations.

As gradient-based attacks rely on model gradients to craft adversarial samples, sign symmetry methods with approximate gradients make fine-tuned models inherently harder to fool using such attacks.


\section{Results}
We compared our proposed approach and the classical fine-tuning approach where backpropagation is used to fine-tune all the parameters of the model. We evaluate on different backbones and datasets. We compare the performance and robustness of the models in each case (performance is measured using the top-1 accuracy for image classification and the mean average precision for image retrieval).

\subsection{Performance}

\paragraph{Image Classification} We first measure the accuracy for the classification task. Table~\ref{tab:performance_classification} shows the top-1 accuracy for each credit assignment method used for fine-tuning (BP, frSF, brSF, and uSF), benchmarked on four datasets (CIFAR10, MS COCO, NUS WIDE, and ImageNet) and using different backbones (AlexNet, VGG-16, and Resnet-18).
The results indicate that Sign-Symmetry methods can perform comparably to, or outperform BP in different experiment settings, such as AlexNet-CIFAR10, AlexNet-ImageNet, and ResNet18-CIFAR10. Generally, uSF and frSF are the two methods that show the best results in classification. 
\begin{table}[htb]
    \centering
    \caption{Accuracy measurements for credit assignment methods across various backbones and datasets. Bold values indicate the highest accuracy for each backbone-dataset combination. Underlined values denote the second-highest accuracy or two equally highest values. If three or more methods share the highest accuracy, no values are highlighted. Similarly, if two or more methods share the second-highest accuracy, none are underlined.}
    \vspace{0.1in}
    \adjustbox{max width= \linewidth}{
        \begin{tabular}{llcccc}
            \toprule
            Backbone & Dataset & BP & uSF & frSF & brSF \\
            \midrule
                     & CIFAR10  & 90.62 & 90.62 & \textbf{93.75} & 87.5 \\
            AlexNet  & MS COCO  & 62.5 & 65.62 & \textbf{71.87} & 65.62 \\
                    & NUS WIDE  & \textbf{90.62} & \underline{87.5} & 84.37 & 84.37 \\
                     & ImageNet & \textbf{87.5}  & 84.37 & 84.37 & 84.37 \\   
            \midrule
                     & CIFAR10  & \underline{93.75} & \textbf{96.87} & 90.62 & 90.62 \\
            ResNet-18 & MS COCO  & \underline{53.12} & 65.62 & 65.62 & 65.62 \\
                    & NUS WIDE  & \underline{78.12} & 84.37 & 84.37 & 84.37 \\
                     & ImageNet & \underline{87.5}  & 84.37 & \underline{87.5} & 84.37 \\
            \midrule
                     & CIFAR10  & 100.0 & 100.0 & 100.0 & \underline{90.62} \\
            VGG16    & MS COCO  & \textbf{65.62} & 59.37 & 56.25 & \underline{62.5} \\
                    & NUS WIDE  & \underline{84.37} & 81.25 & 81.25 & \underline{84.37} \\
                     & ImageNet & \underline{96.87} & \underline{96.87} & 87.5 & 87.5 \\
            \bottomrule
        \end{tabular}
    }
    \label{tab:performance_classification}
\end{table}
\vspace{-0.5cm}

\paragraph{Hashing-based Image Retrieval} We asses hashing-based image retrieval models fine-tuned using the following credit assignment methods (BP, frSF, and uSF), on four datasets (CIFAR10, MS-COCO, NUSWIDE, and ImageNet) and using the following backbones (AlexNet, VGG-16, and Resnet-18). Table~\ref{tab:performance_hashing} presents the results.

Our findings indicate that Sign-Symmetry methods perform comparably to BP across most settings. Notably, uSF and frSF outperformed BP in 6 out of 12 benchmarks. In the remaining cases where BP demonstrated superior performance, all of the Sign-Symmetry methods consistently achieved results close to BP, highlighting the stability and consistency of such methods.
A particular promising result was observed on the ImageNet dataset, which is widely recognized in the image retrieval literature as challenging due to its diversity and complexity. In this context, Sign-Symmetry methods outperformed BP in 2 out of 3 setups.
These results suggest that Sign-Symmetry methods offer competitive performance and in many cases outperform BP. 

\begin{table}[htb]
    \caption{Mean Average Precision (mAP) values recorded for each benchmark dataset. For all the datasets used for the hashing task, $k = 5000$ except for ImageNet, where $k = 1000$. The HyP$^2$ loss function was utilized, and results are provided for four different backbone architectures: VGG16, AlexNet, and ResNet-18. The higher the better; bold indicates top values and underlined values represent the second best. "-" indicates missing results.}
    \vspace{0.1in}
    \centering
    \adjustbox{max width =\linewidth}{
        \begin{tabular}{llcccc}
            \toprule
            Backbone & Dataset & BP & frSF & uSF & brSF \\
            \midrule
                    & CIFAR10      & \textbf{81.1}      & 79.0           & 79.2 & \underline{81.0} \\
            AlexNet & MSCOCO       & \textbf{74.1}      & \underline{71.5} & 71.4 & 71.1 \\
                    & NUS WIDE     & 82.5               & \textbf{82.7}  & \underline{82.6} & - \\
                    & ImageNet100  & 56.9               & 58.0 & \underline{58.6} & \textbf{60.2} \\
                    \midrule
                    & CIFAR10    & \underline{80.4}      & 75.6           & 77.2 & \textbf{85.4} \\
         ResNet-18  & MSCOCO     & \underline{74.1}              & \textbf{74.4} & 71.6 & 73.3 \\
                      & NUS WIDE   & \textbf{84.2}      & \underline{83.6} & 83.8 & - \\
                      & ImageNet100 & \textbf{67.9}      & 64.8           & 57.1 & \underline{66.1} \\
              \midrule
                    & CIFAR10      & \textbf{85.2}      & 85.0           & \underline{85.1} & 81.0 \\
             VGG16  & MSCOCO       & 82.0               & \underline{82.7} & \textbf{82.8} & 78.6 \\
                    & NUS WIDE     & 85.5               & \underline{86.2} & \textbf{86.4} & - \\
                    & ImageNet100  & 77.5               & \underline{79.6} & \textbf{80.6} & 77.6 \\
            \bottomrule
        \end{tabular}
        }
    \label{tab:performance_hashing}
\end{table}

\subsection{Adverserial Robustness}

\subsubsection{Whitebox Attacks ( Gradient-Based )}

\paragraph{Image Classification} We evaluate the robustness of the image classification models using two gradient-based attacks: FGSM and PGD. We benchmarked the accuracy of the fine-tuned models on three datasets (CIFAR10, MS COCO, and ImageNet), using two backbone architectures (AlexNet and VGG-16). For each backbone-dataset configuration, we compared Sign-Symmetry methods to backpropagation and recorded the robust accuracy for each perturbation distance \(\epsilon \in \{0, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5\}\).

All backbones exhibit similar behavior under adversarial attacks, with Sign-Symmetry methods showing consistently greater robustness than BP across configurations. This trend is apparent in most results, for instance, ResNet18 results in Figure~\ref{fig:resnet18_results} show that the robustness gap can surpace 50\% for both types of attack. Among Sign-Symmetry variants, brSF and frSF perform best, with frSF demonstrating the most stable performance (full results for AlexNet are provided in~\autoref{app:add-res}, Figures~\ref{fig:alexnet_results}-\ref{fig:vgg16_results}).

\begin{figure*}[p]
    \centering
    \includegraphics[width=0.9\textwidth]{plots/classification/resnet18_fgsm_pgd.pdf}
    \caption{Accuracy of ResNet18 under FGSM (top row) and PGD (bottom row) adversarial attacks. Note the gradual decline in Sign-Symmetry methods compared to BP's sharp drop.}
    \label{fig:resnet18_results}
\end{figure*}


Most notably, we observe that BP accuracy decreases drastically compared to Sign-Symmetry methods, which exhibit a more gradual decline. In ResNet18 experiments (Figure~\ref{fig:resnet18_results}), BP's accuracy drops to 0 rapidly at \(\epsilon=0.1\). The difference in accuracy between BP and Sign-Symmetry methods can reach up to 71.88\% in multiple configurations (\autoref{app:add-res}).


\paragraph{Hashing-based Image Retrieval} We have measured the performance of deep hashing models when fine-tuned using either BP or Sign-Symmetry methods (frSF, brSF, uSF). The mean average precision was measured while varying the perturbation distance \(\epsilon\) from 0 to 0.5. This was done for both HAG and SDHA attacks, and for each experiment combination, as shown in Figures~\ref{fig:vgg16_results_hash} (see Alexnet results in ~\cref{fig:alexnet_results_hash}, \autoref{app:add-res}).


% Figure 2: VGG16 backbone
\begin{figure*}[p]
\centering
% Include the VGG16 results here (HAG and SDHA for all datasets)
\includegraphics[width=0.9\textwidth]{plots/hashing/vgg16_hashing_robustness_plots.pdf}
\caption{Adversarial robustness of VGG-16 under HAG and SDHA attacks. Trends mirror AlexNet: BP performance collapses sharply (e.g., mAP@5000 drops by 28.65\% on CIFAR-10 at \(\epsilon=0.5\)), while Sign-Symmetry methods retain robustness across all datasets. Metrics: mAP@5000 for CIFAR-10/MSCOCO, mAP@1000 for ImageNet.}
\label{fig:vgg16_results_hash}
\end{figure*}

The robustness of AlexNet and VGG-16 under adversarial attacks reveals critical insights. As can be seen in figure~\ref{fig:vgg16_results_hash} BP collapses entirely on ImageNet (mAP \(\approx\) 12\% at \(\epsilon=0.5\)), while sign symmetry methods retain \(\sim\)40\% mAP. Similar trends hold for CIFAR-10 and MS COCO. Both attacks (HAG and SDHA) highlight the superiority of Sign-Symmetry fine-tuning, particularly at large \(\epsilon\) (\(\geq 0.3\)). The gap between BP and Sign-Symmetry methods grows with model complexity (e.g., VGG-16 shows larger drops than AlexNet ~\cref{fig:alexnet_results_hash} in \autoref{app:add-res}).

\subsubsection{Black-Box Attacks} 
\begin{table}[htb]
    \centering
    \caption{Accuracy measurements for Boundary Attack (BA) across various backbones and datasets. Bold values indicate the highest accuracy for each backbone-dataset combination, and underlined values represent the second best. The higher the better.}
    \vspace{0.1in}
    \adjustbox{max width=\linewidth}{
        \begin{tabular}{llcccc}
            \toprule
            Backbone & Dataset & BP & uSF & frSF & brSF \\
            \midrule
                     & CIFAR10  & 6.25  & 6.25  & \underline{9.37} & \textbf{12.5} \\
            AlexNet  & MS COCO  & \underline{65.6} & 62.5  & \textbf{71.8} & \underline{65.6} \\
                     & ImageNet & 6.25  & 6.2  & 3.1  & \textbf{28.1} \\
            \midrule
                     & CIFAR10  & \textbf{21.87}  & \underline{15.62}  & \underline{15.62} & \underline{15.62} \\
            VGG16    & MS COCO  & 56.2  & \underline{65.6}  & 59.3 & \textbf{62.5} \\
                     & ImageNet & \textbf{12.5}  & \underline{9.3} & 0.0  & 6.2 \\
            \midrule
                     & CIFAR10  & 9.38  & 6.25  & \underline{18.75} & \textbf{18.75} \\
            ResNet-18 & MS COCO  & \textbf{68.7}  & 18.7 & \underline{65.6} & 25.0 \\
                     & ImageNet & 0.0 & \underline{9.3} & \textbf{15.6} & \underline{9.3} \\
            \bottomrule
        \end{tabular}
    }
    \label{tab:BA_performance}
\end{table}

\begin{table}[htb]
    \centering
    \caption{Accuracy measurements for HopSkipJump Attack (HSJA) across various backbones and datasets.}
    \vspace{0.1in}
    \adjustbox{max width=\linewidth}{
        \begin{tabular}{llcccc}
            \toprule
            Backbone & Dataset & BP & uSF & frSF & brSF \\
            \midrule
                     & CIFAR10  & \underline{12.5} & 9.37 & \textbf{12.5}  & 6.25 \\
            AlexNet  & MS COCO  & 68.7  & 68.7  & \textbf{68.75}  & \underline{62.5} \\
                     & ImageNet & 3.1  & \underline{9.3}  & 6.2  & \textbf{12.5} \\
            \midrule
                     & CIFAR10  & \underline{18.7} & 15.6  & \textbf{21.8}  & 15.6 \\
            VGG16    & MS COCO  & \textbf{68.7}  & 34.3  & \underline{18.7} & 3.1 \\
                     & ImageNet & 3.12  & \textbf{6.2}  & \underline{3.15} & 3.1 \\
            \midrule
                     & CIFAR10  & 15.6  & \textbf{18.7} & \textbf{18.7}  & \underline{12.5} \\
            ResNet-18 & MS COCO  & 59.3 & \textbf{68.7} & \textbf{68.7} & \underline{62.5} \\
                     & ImageNet & 3.1 & 3.1 & \underline{6.2} & \textbf{9.3} \\
            \bottomrule
        \end{tabular}
    }
    \label{tab:HSJA_performance}
\end{table}

The results of the experiments presented in tables~\ref{tab:BA_performance} and~\ref{tab:HSJA_performance} indicate that there's no explicit tendency for any of the methods. However, this is expected since black-box attacks do not depend on the nature of the learning algorithm to perform their attack. The other positive takeaway from this experiment is that Sign-Symmetry still performs similarly to BP, which highlights no disadvantage or instability when using it.

\section{Discussion}

Sign-Symmetry methods can be thought of as robust finetuners, this robustness is attributed to the use of approximate gradients during backpropagation, which obfuscates the use of exact gradients by gradient-based adversarial attacks, making the task more challenging for such optimization techniques.
While previous work by~\cite{bartunov2018assessing} has demonstrated the performance limitations of bio-plausible methods compared to backpropagation, no study has yet considered the use of bio-plausible methods along with backpropagation, potentially benefiting from both the effective representation learning of BP and the robustness enhancement of bio-inspired methods.
Our research demonstrates that through fine-tuning pre-trained models using bio-plausible methods, we obtain models that achieve a performance comparable to BP while being more robust. Our findings also indicate that Sign-Symmetry methods, when used in fine-tuning, are very effective in terms of performance and offer the greatest potential for results comparable to BP. Among Sign-Symmetry methods, frSF emerged as the most performant and stable learning method.
While our research has shown the effectiveness of the proposed approach, it is also important to investigate the direct impact of bio-plausible learning on the robustness of BP-trained methods. We suggest the use of a few fine-tuning steps using a Sign-Symmetry method as a corrective measure for robustness in BP-trained models. To fully explore this approach, more experiments need to be conducted with appropriate settings across various architectures.
It should be noted that adversarial attacks are generally designed to target models trained using BP, especially white-box attacks that rely on the model's gradient. Given this, it would be more equitable to compare BP's robustness with attacks specifically designed for bio-plausible learning methods. This observation opens another avenue of research: developing attacks tailored to bio-plausible methods.

\section{Conclusion}
In our work, we propose a novel way of training neural networks, that enhances robustness to adversarial attacks while keeping a similar performance to traditional methods. Through our proposed approach, we narrow the gap between bio-plausible learning methods and backpropagation. By integrating backpropagation with Sign-Symmetry methods, we have demonstrated the potential of achieving high robustness while maintaining performance comparable to BP. Improvements in robustness against adversarial attacks were significant and this was achieved on both tasks, image classification and hashing-based image retrieval. These results hold important implications for the field of deep learning, suggesting that biologically inspired learning rules can address some of the limitations of backpropagation, particularly in terms of robustness, without a significant degradation in performance. The observed improvements across various architectures, including AlexNet, VGG-16, and ResNet-18, and datasets of increasing complexity, such as CIFAR-10, ImageNet-100, ImageNet-1000, MS-COCO, and NUS-WIDE, further support the broad applicability and efficacy of this approach. Future research could investigate the application of this hybrid learning approach to a wider range of neural network architectures and tasks. Moreover, understanding the underlying mechanisms that contribute to the increased robustness observed in Sign-Symmetry methods could lead to the development of new biologically inspired learning rules that further enhance both performance and robustness.

% \section*{Impact Statement}
% This paper presents work whose goal is to advance the field
%  of Machine Learning. There are many potential societal
%  consequences of our work, none which we feel must be
%  specifically highlighted here.
 
\bibliography{references}
\bibliographystyle{icml2025}

\appendix

\flushbottom
\twocolumn[
\section{Additional Experimental Results}
\label{app:add-res}
The following plots display the results of other backbones under adversarial attacks for both classification and hashing tasks, as referenced in the main text.]

\begin{figure*}[p]
    \centering
    \includegraphics[width=0.9\textwidth]{plots/classification/alexnet_fgsm_pgd.pdf}
    \caption{Accuracy of AlexNet under FGSM (top row) and PGD (bottom row) adversarial attacks across different datasets. Higher $\epsilon$ values indicate stronger perturbations.}
    \label{fig:alexnet_results}
\end{figure*}

\begin{figure*}[p]
    \centering
    \includegraphics[width=0.9\textwidth]{plots/classification/vgg16_fgsm_pgd.pdf}
    \caption{Accuracy of VGG16 under FGSM (top row) and PGD (bottom row) adversarial attacks across different datasets. The robustness gap between methods is particularly pronounced at larger $\epsilon$ values.}
    \label{fig:vgg16_results}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Figure 1: AlexNet backbone

\begin{figure*}[p]
\centering
% Include the AlexNet results here (HAG and SDHA for all datasets)
\includegraphics[width=0.9\textwidth]{plots/hashing/alexnet_hashing_robustness_plots.pdf}
\caption{Adversarial robustness of AlexNet under HAG and SDHA attacks. Results are shown for CIFAR-10, MSCOCO , and ImageNet on mAP@5000. Performance degrades with increasing perturbation strength (\(\epsilon\)) for BP fine-tuning, while Sign-Symmetry methods (frSF, brSF, uSF) maintain stability.}
\label{fig:alexnet_results_hash}
\end{figure*}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
