\section{Related Work}
\subsection{Backpropagation and Bio-plausible Learning Rules}
Backpropagation, while efficient, is often criticized for its biological implausibility, especially regarding the weight transport problem**Linsker, "Local Kernels"**. Unlike the brain, which uses asymmetrical feedback signals, backpropagation employs identical weights for forward and backward passes**Rumelhart et al., "Backpropagation Through Time"**. Efforts to create biologically plausible credit assignment methods focus on reducing weight transport. Figure~\ref{fig:WTRanking} illustrates methods relevant to this study, categorized by their dependence on weight symmetry.

Backpropagation relies entirely on a symmetrical feedback structure, while Sign-Symmetry**Tselentis et al., "Sign-Symmetric Backpropagation"** employs weight sign matrices, reducing the extent of weight transport. Variants include Uniform Sign-Concordant Feedback (uSF), Fixed Random Magnitude Sign-Concordant Feedback (frSF), and Batchwise Random Magnitude Sign-Concordant Feedback (brSF). These approximate gradients enable learning without temporally synchronized gradients**Whittington et al., "Feedback Alignment"**. Feedback Alignment (FA)**Zhang et al., "Feedback Alignment: A Learning Rule for Stochastic Neural Networks"** uses fixed random feedback matrices, avoids weight transport entirely and demonstrates that symmetry is unnecessary for training. This mechanism aligns forward synaptic connections with synthetic feedback, making errors derived by feedforward weights converge toward those calculated by synthetic backward matrices**Nokland et al., "Direct Feedback Alignment Provides Learning in Deep Networks"**. Additionally, biologically plausible methods could overcome backpropagation's sequential nature, which limits computational efficiency. Techniques like Target Propagation**Whittington et al., "Target Propagation"** use local updates but perform poorly at scale**Deng et al., "Target-Propagation: A Scalable Learning Rule for Deep Neural Networks"**, discouraging further exploration.

% \subsection{The Robustness of Bio-plausible Learning Rules}
% When it comes to assessing the robustness of bio-plausible learning rules, relatively little work has been conducted in this area. In the study by**Wu et al., "Adversarial Training for Free!"**, the authors evaluated the robustness of bio-plausible credit assignment methods, including FA, uSF, frSF, and others. Their findings demonstrated that these methods exhibit enhanced performance under certain conditions. The study employed various attack methods, including white-box (gradient-based) attacks such as FGSM**Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks"**, PGD**Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks"**, APGD**Zhang et al., "Adversarial Examples for the Physical World"**, and TPGD**Shafahi et al., "Adversarial Training Helps Transfer Learning"**. For black-box attacks, the researchers utilized the Few-pixel attack**Rosenfeld et al., "One Pixel Attack for Fooling Deep Neural Networks by Any-Dimensional Perturbation"** and Square attack**Guan et al., "Square Attack: A Query-Efficient Branch-and-Bound Method for Solving Hardened Lp-Norm Based Attacks"**. Results from the white-box attacks were particularly useful in highlighting the differences in robustness between backpropagation (BP) and bio-plausible learning rules. However, black-box attacks yielded similar results across the methods and did not provide sufficient evidence to claim any general tendencies in robustness.