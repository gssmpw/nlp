\section{Introduction}
Machine learning (ML) approaches based on either classical dimension reduction methods like the proper orthogonal decomposition (POD)~\cite{lumley1967structure} or autoencoders are frequently used to reduce the dimension of fluid flows in data-driven surrogate models~\cite{lusch:2018, Swischuk.2019, eivazi:2020, Agostini20, GANTI2020104626, wu:2021}.
An autoencoder passes the input through a dimension-reducing encoder and subsequently a dimension-increasing decoder. It aims to learn a lower-dimensional representation of the input by forcing the output of the network to be close to the input.  When considering two- or three-dimensional data, which is often the case in physics, autoencoders are usually used with convolutional layers to capture local dependencies in the data. 
To predict the temporal dynamics of unsteady physical phenomena, autoencoders can be combined with networks suitable for time series prediction like a long short-term memory (LSTM) network~\cite{hochreiter:1997}, a transformer~\cite{vaswani:2017} and other ML frameworks such as the sparse identification of nonlinear dynamics (SINDy)~\cite{brunton2016discovering} or approximations to the Koopman operator~\cite{Koopman:1931}. The dynamics are then usually learned in the obtained lower-dimensional latent space of the autoencoder~\cite{lusch:2018, eivazi:2020, wu:2021, GUPTA2022105239, Hemmasian:2023, ANDO2023106047, ZHANG2023105883, solera-rico:2024, schwarz:2024}.

By using nonlinear activation functions, an autoencoder performs nonlinear dimension reduction in contrast to the linear POD framework. Due to this, autoencoders usually achieve better reconstruction accuracy and a more compact latent space compared to POD. This comes, however, at the cost of losing interpretability, as the latent variables of the autoencoder are generally not disentangled or not orthogonal. 
The difficulty is that the influence of individual latent variables is hard to assess if the change in one latent variable inevitably forces a change in another latent variable due to entanglement.
%
Therefore, several studies have recently focused on obtaining disentangled (orthogonal) latent variables in autoencoders to gain interpretability~\cite{eivazi:2022, kang:2022, solera-rico:2024}. 
There are several strategies to achieve this goal.
The $\beta$-variational autoencoder ($\beta$-VAE)~\cite{Higgins2016betaVAELB}, which is based on the variational autoencoder (VAE)~\cite{Kingma:2013}, is probably the most used option. It is a probabilistic framework that aims at normally distributed latent space variables.  
A hyperparameter $\beta>0$ in the loss function controls the importance of reconstructive capability, typically in terms of the mean squared error, relative to a Kullback-Leibler divergence term that assesses the closeness of the normally distributed latent space variables to the standard normal distribution. 
%
In contrast, the orthogonal autoencoder (OAE)~\cite{wang:2019}, originally introduced for clustering tasks, enforces orthogonality of the latent variables in the loss function in a deterministic manner. 
Again, there is a hyperparameter ($\lambda>0$) that controls the importance of orthogonality in comparison to the reconstruction quality in the training process. 
In~\cite{CACCIARELLI2022107853}, the OAE was used to provide uncorrelated latent variables for a statistical process control framework. 

Work on disentangling the latent variables in the context of low-dimensional representations of fluid dynamic fields has been focusing on $\beta$-VAEs. 
Eivazi et al.~\cite{eivazi:2022} proposed using a $\beta$-VAE for the extraction of near-orthogonal modes of turbulent flows. More recently, Solera-Rico et al.~\cite{solera-rico:2024} combined this approach with a transformer to model the dynamics of fluid flows in the obtained disentangled latent space. 
Kang et al.~\cite{kang:2022} employ a $\beta$-VAE in combination with a Gaussian process regression to construct a reduced-order model (ROM) for transonic flows. After dimension reduction, the input parameters of the full-order simulation are mapped to the disentangled latent space using  regression techniques similar to other works~\cite{Swischuk.2019,Agostini20,Pache22,lazzara:2022, DIASRIBEIRO2023105949} to obtain a more interpretable surrogate model. They demonstrate the ability of the $\beta$-VAE to use only a few physics-aware latent variables, when trained with a larger than needed latent space dimension. Notice that the phenomenon of VAEs having inactive latent variables was reported before, e.g.,~\cite{bowman2016generating, burda2016importanceweightedautoencoders, ladder_vae}.


In this work, we compare the established $\beta$-VAE with two non-probabilistic frameworks, the OAE and an approach we call uncorrelated autoencoder (UAE), which directly uses the correlation matrix of the latent variables to enforce the desired property of a disentangled latent space. 
Slightly similar proposals to complement the loss function have recently been used for an image style transfer application, where correlations between different channels of encoded feature maps are reduced~\cite{KIM2021148}, as well as for the uncorrelated sparse autoencoder, pursuing other encoding goals~\cite{Savargaonkar:2024}.
  
Two data sets are used in this work. The first example is a benchmark example for periodic flows that has already been used in publications of similar strategies for disentangled latent space ROM. The second case contains aircraft ditching load data and is used as an industrial application in this work. 
Ditching is the emergency landing on water, and its analysis is critical for the certification of a commercial aircraft. To reduce the computational effort, simulation-based ditching analysis is usually performed using a one-way coupling between the fluid and the structure, i.e., in a first step the hydrodynamic loads are calculated and subsequently the structural analysis is performed. 
In the long-term, the current research aims to include approximate deformations to the calculation of the hydrodynamic loads with ML. 
First steps towards the spatio-temporal prediction of ditching loads using combinations of convolutional autoencoder (CAE) with LSTM networks and Koopman operator approaches that can include deformation caused changes of the loads are documented in~\cite{schwarz:2024}.  
Interpretabilty of the data-driven surrogate model is obviously critical in an industrial process. The goal within this work is to advance the employed ML methods in this regard.
To the best of our knowledge, this is the first work for applications related to fluid dynamics that applies a loss function focusing on the correlation matrix of the latent variables and compares the OAE to the $\beta$-VAE and the UAE.

The remainder of the paper is structured as follows: In Sec.~\ref{sec:2}, the used autoencoder frameworks are briefly presented. Results on both datasets are shown in Sec.~\ref{sec:3} and conclusions are drawn in Sec.~\ref{sec:4}.   

