\section{Methods}
\label{sec:2}

An autoencoder comprises an encoder and a decoder, as illustrated in Fig.~\ref{fig:ae}. The encoder maps the input $\mathbf{x}\in\mathbb{R}^n$ to a usually lower dimensional latent space representation $\mathbf{z}\in\mathbb{R}^m$ and the decoder subsequently maps  $\mathbf{z}$ to the full dimension to obtain a reconstruction of the input, $\tilde{\mathbf{x}}\in\mathbb{R}^n$. Encoder and decoder usually have symmetric structure. 
All models in this work are based on a CAE, as the considered physical data is two-dimensional in space and convolutional layers are usually better than fully connected layers at capturing spatial dependencies.
\begin{figure}[!h]
     \centering
     \includegraphics{figures/ae.pdf}
    \caption{Basic autoencoder scheme.}
    \label{fig:ae}
\end{figure}

  A standard autoencoder reconstruction loss comparing input $\mathbf{x}$ and the reconstruction $\tilde{\mathbf{x}}$, i.e., mean squared error (MSE), is considered for all models so that encoder and decoder can be used to decrease or increase the spatial dimension of the data at hand. 
%
In this work, the goal of the reduced order models is to obtain disentangled latent variables $\mathbf{z}$ while keeping the reconstruction error as low as possible. To this end, the three models considered, i.e., $\beta$-VAE, OAE and UAE, each use a different second term in the loss function that acts in the latent space to effect the disentanglement. 

\subsection{Orthogonal Autoencoder (OAE)}
The orthogonal autoencoder (OAE) was introduced by Wang et al.~\cite{wang:2019} for clustering tasks. In addition to the reconstruction loss, the OAE enforces the orthogonality of the latent variables within the loss function for a mini batch of size $b$
\begin{align}
    \mathrm{Loss_{OAE}}(\mathbf{X},\tilde{\mathbf{X}}) =
    \frac{1}{b\cdot n}\lVert \mathbf{X} - \tilde{\mathbf{X}} \rVert^2_\mathrm{F} + 
    \frac{\lambda}{m^2}\lVert \mathbf{Z}^T \mathbf{Z} - \mathbf{I} \rVert^2_\mathrm{F}, \quad \lambda>0,
    \label{eq:oae_loss}
\end{align}
where $\mathbf{X}\in \mathbb{R}^{b\times n}$ contains the inputs for one mini batch, $\mathbf{Z} \in \mathbb{R}^{b\times m}$ is a matrix containing the latent vectors of a mini batch as rows and $\mathbf{I} \in \mathbb{R}^{m\times m}$.

\subsection{Uncorrelated Autoencoder (UAE)}
To measure the independence of the latent variables, the Pearson correlation matrix $\mathbf{R} \in \mathbb{R}^{m\times m}$ of the latent vectors, and its determinant $\det(\mathbf{R})$ are often considered~\cite{eivazi:2022, kang:2022, CACCIARELLI2022107853, solera-rico:2024}. The matrix is computed as follows
\begin{align*}
    \mathbf{R}_{ij} = \frac{\sum^K_{k=1}(z_i^k-\bar{z_{i}})(z_j^k-\bar{z_j})}{\sqrt{\sum^K_{k=1}(z_i^k-\bar{z_{i}})^2\sum^K_{k=1}(z_j^k-\bar{z_{j}})^2}}, \quad 1\le i,j \le m,
\end{align*}
where $K$ is the number of considered training or validation examples used for the calculation, $z_i^k$ denotes the value of the $i$th latent variable on the $k$th data example, and $\bar{z_{i}}$ is the mean value of the $i$th latent variable over all $k$ examples.
The matrix $\mathbf{R}$ is symmetric and has unit values on the diagonal. 
A correlation matrix close to the identity matrix and thus a determinant close to one corresponds to a good level of disentanglement. 

The second non-probabilistic approach considered here, which we term uncorrelated autoencoder (UAE), uses this criterion directly in the loss function. The employed loss function is similar to the correlation losses in~\cite{KIM2021148} and~\cite{Savargaonkar:2024}, where the absolute values of the non-diagonal entries of the correlation matrix are forced to be small. The present study follows the OAE method and uses the squared entries
\begin{align}
    \mathrm{Loss_{UAE}}(\mathbf{X},\tilde{\mathbf{X}}) =  \frac{1}{b\cdot n}\lVert \mathbf{X} - \tilde{\mathbf{X}} \rVert^2_\mathrm{F} + 
    \frac{\nu}{m^2}\lVert \mathbf{R} - \mathbf{I} \rVert^2_\mathrm{F}, \quad \nu>0,
    \label{eq:our_loss}
\end{align}
where $\mathbf{R} \in \mathbb{R}^{m\times m}$ is the Pearson correlation matrix of the latent vectors of a mini batch of size $b$ and $\mathbf{I} \in \mathbb{R}^{m\times m}$.
%
The UAE therefore focuses directly on the two properties of interest, reconstruction quality and disentanglement, without forcing the latent space to follow a normal distribution as the ($\beta$-)VAE presented next.  

\subsection{Variational Autoencoder (VAE)}
The latent space representations in a VAE are forced to follow a probability distribution, which is usually assumed to be normally distributed: $\mathbf{z}\sim \mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})$, with mean $\boldsymbol{\mu}\in \mathbb{R}^m$ 
and assumed diagonal covariance matrix $\boldsymbol{\Sigma}\in\mathbb{R}^{m\times m}$ where the diagonal entries are $(\sigma_i)^2, i\in \{1,\dots,m\}$. 
 The encoder provides the two outputs $\boldsymbol{\mu}$ and 
$\log (\boldsymbol{\sigma}^2)$, as illustrated in Fig.~\ref{fig:vae}, 
where for stability reasons the $\log$-variance $\log (\boldsymbol{\sigma}^2)$ is returned. 
Using $\boldsymbol{\sigma} = \exp(0.5\cdot\log (\boldsymbol{\sigma}^2))$ and  generating $\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0},\mathbf{I})$, the latent variables $\mathbf{z} = \boldsymbol{\mu} + \boldsymbol{\sigma} \odot \boldsymbol{\epsilon}$ are computed, which are the input for the decoder. This is the \emph{reparameterization trick}~\cite{Kingma:2013} to enable classical backpropagation, as $\mathbf{z}$ can be considered a deterministic variable during training. The gradient with respect to the encoder can otherwise cause problems \cite{Kingma:2013}.
\begin{figure}[!h]
     \centering
    \includegraphics{figures/vae.pdf}
    \caption{Scheme of a VAE.}
    \label{fig:vae}
\end{figure}
The loss function of a VAE is composed of a reconstruction loss, as well as a term that forces the posterior distribution in the latent space to be close to the chosen prior distribution. In this work, these are the MSE and the Kullback-Leibler (KL) divergence, which forces the posterior $\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})$ to be close to the prior $\mathcal{N}(\mathbf{0},\mathbf{I})$. The formula for the loss is
\begin{align}
    \mathrm{Loss_{VAE}}(\mathbf{X},\tilde{\mathbf{X}}) 
     =\frac{1}{b\cdot n}\lVert \mathbf{X} - \tilde{\mathbf{X}} \rVert^2_\mathrm{F} + \frac{\beta}{b\cdot 2}\sum_{i=1}^b\sum_{j=1}^m (\sigma_{ij}^2+\mu_{ij}^2 - 1 - \log(\sigma_{ij}^2)), \quad \beta>0,
    \label{eq:vae_loss}
\end{align}
where $b$ is the mini batch size, $\mathbf{X}\in \mathbb{R}^{b\times n}$ contains the inputs for one mini batch, and $\mu_{ij}$ and $\sigma^2_{ij}$ denote the $j$th component of mean and variance for the $i$th example in the mini batch. 
For $\beta= 1$, this is referred to as the VAE \cite{Kingma:2013} and $\beta\neq 1$ denotes the $\beta$-VAE~\cite{Higgins2016betaVAELB}.


The latent space obtained from VAEs or $\beta$-VAEs is usually continuous. That is a useful feature for the generation of new data, for which samples of the probability distribution in the latent space are decoded. 
Because $\boldsymbol{\epsilon}\sim \mathcal{N}(\mathbf{0},\mathbf{I})$ is generated every time the VAE is used to encode data, the trained model is not deterministic. 
Mind that in the work of Solera-Rico et al.~\cite{solera-rico:2024}, the trained $\beta$-VAE is applied in a deterministic manner, as only the mean values $\boldsymbol{\mu}$ are used by the models that learn the dynamics of fluid flows. 
Using a deterministic model from the beginning could, however, be the simpler approach as the training of VAEs can be challenging. 


A common challenge is the so-called \emph{posterior collapse}, in which the posterior matches the prior distribution and the decoder ignores the latent vector $\mathbf{z}$~\cite{bowman2016generating, ladder_vae, lucas2019understanding}. In that case, the model does not learn meaningful representations of the inputs $\mathbf{x}$. Although techniques exist to mitigate this problem, such as including a warm-up phase for the reconstruction loss in the beginning of the training in which $\beta$ is initially set to 0 and then gradually increased~\cite{bowman2016generating, ladder_vae}, or using a different prior distribution, e.g., \cite{NIPS2017_7a98af17, davidson2022hypersphericalvariationalautoencoders}, this requires additional consideration for the training of VAEs. 

On the contrary, the collapse or inactiveness of several \emph{single} latent variables $z_i$, which do not help the reconstruction, is a beneficial feature for a disentangled latent space~\cite{burgess2018understandingdisentanglingbetavae} and for surrogate modeling, as this might help to identify the physics-aware latent variables~\cite{kang:2022}.

\subsection{Model Comparison}
The following section attempts to explain the similarities and differences between the three loss function of the three autoencoder models.
Let $\mathbf{Z}\in\mathbb{R}^{b\times m}$ contain the $b$ latent space vectors of a mini batch, and $\mathbf{z}_j$, $j=1,\ldots,m$ denote the columns of $\mathbf{Z}$. By initialization of the networks, for which typically a uniform distribution centered at zero is used, and the loss functions used, we can assume that all $m$ latent space features have approximately zero mean and a nonzero variance. If the features have zero mean, $\|\mathbf{z}_j\|^2=\langle\mathbf{z}_j,\mathbf{z}_j\rangle=b\textsf{Var}(z_j^k)$ for all $j=1,\ldots,m$.
The OAE enforces nearly orthonormal columns of $\mathbf{Z}$, which also affects the variance, as the columns are scaled to have approximately unit length.
The KL divergence in the ($\beta$-)VAE shifts the latent variables towards zero mean values with unit variances.
A combination of both, i.e., latent variables with a zero mean and (scaled) orthonormal columns, results in the minimization of the latent space loss function employed by the UAE:
with $i\neq j$, $\mathbf{z}_i^T\mathbf{z}_j=0$ and $\bar{z}_{i}=\bar{z}_{j}=0$. Then $\sum^b_{k=1}(z_i^k-\bar{z}_{i})(z_j^k-\bar{z}_j) = \sum^b_{k=1}z_i^kz_j^k=0 \Rightarrow \mathbf{R}_{i,j}=0$.
Vice versa, an identity correlation matrix $\mathbf{R}=\mathbf{I}$ is obtained exactly, if the columns of $\mathbf{Z}$ shifted by their mean are nonzero orthogonal. By assumption the mean will be approximately zero, thus it makes sense to directly minimize the UAE loss.

\subsection{Analysis of Latent Variables}
Having disentangled latent variables, the impact of the different variables may be analyzed. This can either be done by decoding latent vectors and analyzing the reconstruction, or solely in the latent space by encoding data from the full dimensional space. 

Eivazi et al.~\cite{eivazi:2022} use the former approach in conjunction with a $\beta$-VAE. They map full order data to the latent space and set all components except the component $z_i$ to zero. Subsequently, the resulting vectors are decoded to obtain the $i$th modes, and the whole procedure is repeated for all latent variables. Finally, the obtained energy from those reconstructions is calculated and the mode providing the largest energy is identified. If the largest energy is obtained using the $j$th latent variable, this $j$th mode is ranked as the first mode. This procedure is then repeated to determine the second mode in the ranking, but without setting $z_j$ to zero, and so on.
%
 Kang et al.~\cite{kang:2022} perform their analysis of the $\beta$-VAE latent variables completely in the latent space, which avoids the application of the decoder. They calculate the KL divergence of each latent variable to observe which variables contain useful information. Additionally, they consider the standard deviation of each latent variable to assess, which variables are active [inactive] and thus change [do not change] for varying inputs.  

\subsubsection{Mode Generation}
Single latent variables can be associated with modes. To this end, the latent variables are ranked as described above to identify the most active latent variables. {Inspired by Kang et al.~\cite{kang:2022}, this ranking is performed inside the latent space using the standard deviation and KL divergence of the latent variables.} 
The spatial modes are generated by decoding the $i$th unit vector of the latent space dimension for some of the highest ranked $i$ ~\cite{eivazi:2020, solera-rico:2024}. 
However, there is a non-linear behavior in the output when changing the value of $z_i$, cf. supplementary information of~\cite{solera-rico:2024}. 
%
 For this reason, when analyzing the impact of a latent variable, we do not look at a single value for that latent variable, but rather at a range of values while keeping the other latent variables fixed.