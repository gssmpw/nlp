\section{Related Work}
\subsection{Text-guided 3D Generation}

Early works in text-to-3D generation, such as **Carfagno et al., "CLIP-forge: Text-to-3D Generative Model"** , **Li et al., "Dream Fields: Text-to-3D Scene Reconstruction"** , **Tessone et al., "Text2Mesh: Text-to-3D Mesh Generation"** , **Kulkarni et al., "CLIP-NeRF: Text-to-3D Neural Radiance Field"** , and **Huang et al., "CLIP-mesh: Text-to-3D Mesh Reconstruction"** employed CLIP as a guidance mechanism for 3D generation. However, **Park et al., "DreamFusion: Score Distillation Sampling Loss for Text-to-3D Generation"** introduced the Score Distillation Sampling (SDS) loss, significantly advancing the quality of 3D models with the aid of 2D diffusion guidance. **Wang et al., "Magic3D: Two-stage Optimization for Text-to-3D Generation"** improved the quality of generated models by employing a two-stage optimization process, progressing from coarse to fine. **Kwon et al., "Fantasia3D: Geometry and Texture Optimization for 3D Models"** prioritized the optimization of geometry and texture in 3D models, while **Kim et al., "ProlificDreamer: Variational Score Distillation for Text-to-3D Generation"** enhanced the diversity of SDS loss and addressed out-of-distribution issues by introducing Variational Score Distillation. Similarly, **Chen et al., "Score Jacobian Chaining (SJC): 2D Diffusion-based Text-to-3D Generation"** proposed a method for 3D generation using 2D diffusion, leveraging the Perturb-and-Average Scoring (PAAS) technique to iteratively optimize 3D structures. Additionally, other works utilized 3DGS as a 3D representation to achieve rapid and high-fidelity model generation. **Lee et al., "DreamGaussian: Initializing 3D Gaussians with Random Positions"** initialized 3D Gaussians by randomly assigning positions within a sphere. However, this approach introduced a bias, favoring spherical symmetry in generated structures. In contrast, methods such as **Kang et al., "GaussianDreamer: Pre-trained Text-to-3D Generation Models for Gaussian Initialization"** , **Zhou et al., "GSGEN: Guided 3D Generation with Scene Graphs"** , and **Choi et al., "GaussianDiffusion: Gaussian-based Diffusion Model for Text-to-3D Generation"** employed pre-trained 3D generation models to initialize 3D Gaussians, offering a more versatile approach.

% Despite these advancements, these methods face challenges in maintaining consistency in 3D outputs for complex scenes. Our work addresses this limitation by converting text into scene graphs to guide the generation process, facilitating the efficient generation of physically plausible composite 3D scenes.

\subsection{Complex Scene Generation}
Early methods **Gao et al., "Scene Graph-based 3D Scene Synthesis"** for synthesizing 3D scenes used scene graphs to define objects and organize spatial relationships. **Wang et al., "Compositional NeRF: Neural Radiance Field with Compositional Scenes"** used compositional NeRF for scene representation, while **Kim et al., "Set-the-Scene: Style-consistent, Disentangled Scene Generation"** developed a style-consistent, disentangled NeRF-based framework for scene generation. **Chen et al., "Text2Room: Text-to-3D Room Generation with 2D Views"** and **Zhou et al., "Text2NeRF: Text-to-3D Scene Reconstruction with Neural Radiance Fields"** generated 2D views from text and extrapolated these views to construct 3D scenes but struggled to maintain scene coherence. **Huang et al., "VP3D: Volumetric Prediction for Compositional 3D Generation"** and **Lee et al., "CompGS: Compositional Scene Generation with Layout Guidance"** achieved compositional 3D generation through layout guidance from 2D views. **Wang et al., "CG3D: Compositional 3D Generation with Gravity and Contact Constraints"** incorporated gravity and contact constraints during compositional generation to produce physically realistic outcomes.

With the rise of large language models (LLMs), new inspirations for scene layout have emerged. Methods such as **Kim et al., "SceneCraft: Scene Layout Synthesis with Large Language Models"** , **Chen et al., "Holodeck: Text-to-3D Scene Generation with Vision-Language Models"** and **Zhou et al., "LayoutGPT: Layout Generation with Pre-trained Language Models"** utilized LLMs or vision-language models (VLMs) to generate complex 3D scenes from the textual descriptions. Nonetheless, due to the hallucination issues inherent in large models, layout confusion can arise in intricate spatial environments. **Huang et al., "Gala3D: Coarse Layout Priors and Optimization for Physically Plausible Scenes"** utilized coarse layout priors from LLMs and refined the layout through optimization to achieve more structured and coherent scene arrangements.
% Our work addresses these limitations by constructing initial layouts guided by scene graphs and emphasizing physical layout constraints during training, facilitating the efficient creation of physically plausible composite scenes.