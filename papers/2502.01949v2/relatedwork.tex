\section{Related Work}
\subsection{Text-guided 3D Generation}

Early works in text-to-3D generation, such as CLIP-forge \cite{sanghi2022clip}, Dream Fields \cite{jain2022zero}, Text2Mesh \cite{michel2022text2mesh}, CLIP-NeRF \cite{wang2022clip}, and CLIP-mesh \cite{mohammad2022clip} employed CLIP as a guidance mechanism for 3D generation. However, DreamFusion \cite{poole2022dreamfusion} introduced the Score Distillation Sampling (SDS) loss, significantly advancing the quality of 3D models with the aid of 2D diffusion guidance. Magic3D \cite{lin2023magic3d} improved the quality of generated models by employing a two-stage optimization process, progressing from coarse to fine. Fantasia3D \cite{chen2023fantasia3d} prioritized the optimization of geometry and texture in 3D models, while ProlificDreamer \cite{wang2024prolificdreamer} enhanced the diversity of SDS loss and addressed out-of-distribution issues by introducing Variational Score Distillation. Similarly, Score Jacobian Chaining (SJC) \cite{wang2023score} proposed a method for 3D generation using 2D diffusion, leveraging the Perturb-and-Average Scoring (PAAS) technique to iteratively optimize 3D structures. Additionally, other works utilized 3DGS as a 3D representation to achieve rapid and high-fidelity model generation. DreamGaussian \cite{tang2023dreamgaussian} initialized 3D Gaussians by randomly assigning positions within a sphere. However, this approach introduced a bias, favoring spherical symmetry in generated structures. In contrast, methods such as GaussianDreamer \cite{yi2023gaussiandreamer}, GSGEN \cite{chen2024text}, and GaussianDiffusion \cite{li2023gaussiandiffusion} employed pre-trained 3D generation models to initialize 3D Gaussians, offering a more versatile approach.
% Despite these advancements, these methods face challenges in maintaining consistency in 3D outputs for complex scenes. Our work addresses this limitation by converting text into scene graphs to guide the generation process, facilitating the efficient generation of physically plausible composite 3D scenes.

\subsection{Complex Scene Generation}
Early methods \cite{chang2014learning} for synthesizing 3D scenes used scene graphs to define objects and organize spatial relationships. Giraffe \cite{niemeyer2021giraffe} used compositional NeRF for scene representation, while Set-the-Scene \cite{cohen2023set} developed a style-consistent, disentangled NeRF-based framework for scene generation. Text2Room \cite{hollein2023text2room} and Text2NeRF \cite{zhang2024text2nerf} generated 2D views from text and extrapolated these views to construct 3D scenes but struggled to maintain scene coherence. VP3D \cite{chen2024vp3d} and CompGS \cite{ge2024compgs} achieved compositional 3D generation through layout guidance from 2D views. CG3D \cite{vilesov2023cg3d} incorporated gravity and contact constraints during compositional generation to produce physically realistic outcomes. 

With the rise of large language models (LLMs), new inspirations for scene layout have emerged. Methods such as SceneCraft \cite{kumaran2023scenecraft}, Holodeck \cite{yang2024holodeck} and LayoutGPT \cite{feng2024layoutgpt} utilized LLMs or vision-language models (VLMs) to generate complex 3D scenes from the textual descriptions. Nonetheless, due to the hallucination issues inherent in large models, layout confusion can arise in intricate spatial environments. Gala3D \cite{zhou2024gala3d} utilized coarse layout priors from LLMs and refined the layout through optimization to achieve more structured and coherent scene arrangements. 
% Our work addresses these limitations by constructing initial layouts guided by scene graphs and emphasizing physical layout constraints during training, facilitating the efficient creation of physically plausible composite scenes.