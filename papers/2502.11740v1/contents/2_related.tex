\section{Related Work}







\subsection{Visual Knowledge Forgetting in MLLMs}
Catastrophic forgetting, a persistent challenge in continual learning, 
occurs when a model forgets previous knowledge while learning new tasks, reducing its performance on earlier tasks \cite{wang2023orthogonal}. 
This issue has gained attention in LLMs due to the growing need for continual pre-training and instruction tuning \cite{wu2024continual,luo2023empirical}. 
MLLMs, which integrate multiple modalities through feature encoders projecting inputs into the LLMâ€™s token space, are also prone to catastrophic forgetting \cite{zhai2023investigating}. 
While several methods have attempted to address this issue by adapting continual learning techniques, 
ranging from fine-tuning and task-orthogonal gradient descent to knowledge distillation and replay-based strategies \cite{shi2024continual, wu2024continual, zhu2024model, zheng2024beyond}, 
these approaches often fall short of preserving rich visual representations. 
For instance, fine-tuning MLLMs on new tasks tends to overfit textual instructions, inadvertently suppressing visual features, and even parameter-efficient adaptations like LoRA have been shown to suffer from forgetting \cite{fawi2024curlora, liu2024learning}. 
Model Tailor \cite{zhu2024model} addresses forgetting by adapting the LLM backbone across reasoning tasks but neglects the critical visual knowledge forgetting problem,
which may lead to visual hallucination or deficiency problems while generalizing to various tasks \cite{zhai2023investigating}. 
In contrast, our method offers a more principled and synchronized approach to instruction tuning that jointly optimizes the alignment between the visual encoder and the LLM, 
effectively preserving pre-trained visual knowledge while mitigating the degradation of visual representation learning in previous works.


\subsection{Information Theory in LLMs}
The Information Bottleneck (IB) method \cite{tishby2000information} in Large Language Models (LLMs) 
focuses on compressing input data while preserving information relevant to the target output \cite{deletang2023language,valmeekam2023llmzip,wei2024diff,wu2022context}. 
Previous works employ IB to extract robust task-specific features \cite{zhang2022improving,wu2024infoprompt} and enable feature attribution \cite{li2022explanation,jiang2020inserting}
However such methods focus on information compression in language models \cite{yang2025exploring}, while the unique visual knowledge forgetting problem cannot be directly viewed and interpreted.
In addition, existing information-theoretic transfer learning methods \cite{tseng2024semantic,wu2024infoprompt,ling2024convergence} cannot be directly applied to MLLMs, 
where the multimodal knowledge is entangled and fused by a dominant LLM. 
In contrast to these prior works, our approach leverages effective rank to quantify and mitigate the over-compression of visual representations in MLLMs. 
Specifically, our modality-decoupled gradient descent (MDGD) method explicitly decouples visual optimization from task-specific alignment, which cannot be enabled by existing IB-based methods for LLMs.











