\begin{table*}[ht]
  \centering
  \small
  \begin{tabular}{lcccccccccc}
    \toprule
    \multirow{2}{*}{Method} & \multirow{2}{*}{\#Params} & \multicolumn{6}{c}{Pre-trained tasks} & \multicolumn{1}{c}{Target task} & \multicolumn{2}{c}{Metrics} \\
    \cmidrule(lr){3-8} \cmidrule(lr){9-9}  \cmidrule(lr){10-11}
                    &        & \textbf{GQA}              & \textbf{VizWiz}                & \textbf{SQA}                   & \textbf{TextVQA} & \textbf{POPE} & \textbf{MMBench} & \textbf{Flickr30k} & \textbf{Avg} & \textbf{Hscore} \\
    \midrule
    \textbf{Zero-shot}       & --      & 61.94            & 50.00                 & 66.80                 & 58.27 & 85.90 & 64.30 & 3.5 & 55.82  & 59.86 \\
    \midrule
    \textbf{Fine-tune}       & 1.2B   & 56.26            & 44.45                 & 28.34                 & 38.98 & 38.40 & 50.56 & \textbf{78.82} & 47.97 & 45.26 \\
    \textbf{LoRA}            & 29M   & 17.74            & 40.63                 & 5.38                  & 30.48 & 2.40  & 9.55  & 64.18 & 24.33 & 20.49 \\
    \textbf{Model Tailor}    & 273M   & 52.49            & 42.28                 & \underline{67.15}     & 43.89 & 82.88 & 63.40 & \underline{75.40} & 61.07 & 59.85 \\
    \midrule
    \textbf{MDGD}             & 1.2B   & \underline{67.71}  & \underline{48.18} & \textbf{69.05}         & \underline{57.32} & \textbf{85.12} & \underline{65.43} & 73.47 & \textbf{66.61} & \textbf{66.03} \\
    ~~w/o visual align     & 1.2B   & 57.64           & 36.95                 & 53.96                 & 32.84 & 30.43 & 56.66 & 65.58 & 47.72 & 46.19 \\
    \textbf{MDGD-GM } & 124M   & \textbf{69.89}  & \textbf{51.22}        & 65.87                 & \textbf{58.18} & \underline{84.39} & \textbf{66.42} & 64.18  & \underline{65.74} & \underline{65.86} \\
    \bottomrule
    \toprule

    \multirow{2}{*}{Method} & \multirow{2}{*}{\#Params} & \multicolumn{6}{c}{Pre-trained tasks} & \multicolumn{1}{c}{Target task} & \multicolumn{2}{c}{Metrics} \\
    \cmidrule(lr){3-8} \cmidrule(lr){9-9}  \cmidrule(lr){10-11}
    &                       & \textbf{GQA} & \textbf{VizWiz} & \textbf{SQA} & \textbf{TextVQA} & \textbf{POPE} & \textbf{MMBench} & \textbf{OKVQA} & \textbf{Avg} & \textbf{Hscore} \\
    \midrule
    \textbf{Zero-shot}       & --     & 61.94 & 50.00 & 66.80 & 58.27 & 85.90 & 64.30 & 0.14 & 55.34 & 59.58 \\
    \midrule
    \textbf{Fine-tune}     & 1.2B  & 62.98 & 40.59 & 59.84 & 48.38 & 71.42 & 51.98 & \underline{69.10} & 57.76 & 56.79 \\
    \textbf{LoRA}            & 29M   & 63.44 & 41.61 & 51.29 & 48.02 & 75.27 & 37.31 & \textbf{71.46} & 55.49 & 54.12 \\
    \textbf{Model Tailor}    & 273M   & 60.39              & \textbf{46.49} & \textbf{69.51} & \textbf{54.88} & \textbf{85.44} & \underline{63.32} & 38.10 & 59.73 & 61.48 \\
    \midrule
    \textbf{MDGD}          & 1.2B     & \textbf{66.55}     & 42.72 & 64.60 & 52.54 & \underline{85.17} & 61.73 & 62.29 & \underline{62.23} & \underline{62.22} \\
    ~~w/o visual align & 1.2B  & \underline{66.39} & 39.89 & 60.19 & 52.40 & 84.92 & 62.97 & 62.39 & 61.31 & 61.22 \\
    \textbf{MDGD-GM}  & 124M  & 66.02              & \underline{43.97} & \underline{67.91} & \underline{52.80} & 84.70 & \textbf{63.97} & 61.04 & \textbf{62.92} & \textbf{63.07} \\
    \bottomrule
  \end{tabular}
  \caption{
  Performance on various pre-trained tasks of LLaVA-1.5 models fine-tuned on Flickr30K and OKVQA. 
  We report the best performance for each task in a \textbf{bold font} while the second best performance \underline{underlined}. 
  }
  \label{tab:llava-main}
  \vspace{-.8cm}
\end{table*}
