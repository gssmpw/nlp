



    
    


\section{Preliminary}\label{sec:prelim}
\noindent\textbf{Task Definition.}
Given an MLLM $\pi_{\theta}$ and instruction-tuning dataset $D$, the image prompt $I\in \Omega$ is encoded by a visual encoder $f$ 
into a sequence of $M$ visual tokens $f(I)=X^v=(x_1^v,x_2^v,\dots,x_M^v)$.
During instruction tuning,
the textual instructions $T\in D$ are tokenized as $X^l=(x_1^l,x_2^l,\dots,x_N^l)$ using the tokenizer of the backbone LLM, 
which is querying the MLLM to generate textual responses conditioned on the multimodal inputs,
\begin{equation}
    \hat{y}_k \sim \pi_{\theta}(\cdot \mid X^v, X^l, y_{<k}).
\end{equation}
Therefore, the learning objective of visual instruction-tuning for $K$ samples is to 
maximize the average log-likelihood of the ground truth answer tokens $y =(y_1, y_2, \dots, y_T)$ of each sample,
\begin{equation}\label{eq:task_loss}
    \mathcal{L}_{vl}(\theta) = 
    -  \sum_{t=1}^{T} 
    \log \pi_\theta \left( y_t \mid X^v, X^l, y_{<t} \right),
\end{equation}
where multimodal instructions $X^v$ and $X^l$ both serve as generation conditions.


\vspace{1em}\noindent\textbf{An Information Bottleneck Perspective on Visual Knowledge Forgetting.}
In multimodal models, the information bottleneck~\cite{mai2022multimodal} (IB) framework provides a powerful lens to understand how representations are formed. 
In our setting, the IB principle seeks a representation \( Z \) that is maximally informative about the output \( y \) while discarding irrelevant details from the inputs. 
For an MLLM that processes visual inputs \( X^v \) and textual inputs \( X^l \), a full IB objective might take the form:
\begin{equation}\label{eq:ib_vision}
    \min_{\theta} \quad \mathcal{L}_{\text{IB}}^{\text{vision}}(\theta) = - I(y; Z) + \beta\, I(X^v; Z).
\end{equation}
where \( I(\cdot;\cdot) \) denotes mutual information and \(\beta\) controls the trade-off between predictive power and compression.
This formulation explicitly highlights the risk of discarding visual details when the model is optimized primarily to predict \( y \).

\vspace{1em}\noindent\textbf{Effective Rank as a Measure of Representation Richness.}
To quantify the information content retained in a representation, we use the effective rank metric~\cite{roy2007effective}. Given a representation matrix \( Z \) whose singular values are \( \{\sigma_i\} \), the effective rank is defined as:
\begin{equation}\label{eq:erank}
    \text{erank}(Z) = \exp\Biggl(-\sum_{i} p_i \log p_i\Biggr), \quad \text{with} \quad p_i = \frac{\sigma_i}{\sum_j \sigma_j}.
\end{equation}
This measure, based on the entropy of the singular value distribution, captures the “richness” or intrinsic dimensionality of \( Z \). A higher effective rank indicates that the representation spans a larger subspace, whereas a lower effective rank implies that the representation has been overly compressed.




\section{Visual Forgetting in MLLM Instruction-tuning}\label{sec:visual_forget}
Building on the IB objective Eq. (\ref{eq:ib_vision}) introduced in Section~\ref{sec:prelim}, we examine how instruction tuning affects the richness of visual representations. 
Let the pre-trained MLLM induce a latent representation,  
$$Z \sim p(\cdot\mid X^v, X^l),$$
where $Z$ is is decomposed into modality-specific components, $Z = (Z^v, Z^l)$
with \( Z^v \) captures the visual features extracted from \( X^v \), and \( Z^l \) encapsulates the textual features from \( X^l \).
Define the \emph{pre-trained} visual representation space as,
\[
\mathcal{Z}_0^v = \left\{ Z^v_\phi : Z \sim p_\phi(\cdot \mid X^v), \quad X^v \in \Omega \right\}.
\]
During instruction tuning, the model is optimized primarily to predict the target $y$. As described in Eq. (\ref{eq:ib_vision}), the IB objective introduces a trade-off between retaining visual information \( I(X^v; Z) \) and ensuring that \( Z \) remains predictive of \( y \) via \( I(y; Z) \)~\cite{jiang2024correlation}. 
In practice, however, instruction-tuning datasets are predominantly text-driven; thus, the learned visual representation $Z^v$ receives only indirect and often weaker supervision~\cite{wang2024mdpo}.

Let the tuned model’s latent representation be $Z_\theta \sim p_\theta(\cdot\mid X^v, X^l),$
and denote the corresponding visual representation space by,
\[
\mathcal{Z}_\theta^v = \left\{ Z^v_\theta : Z \sim p_\theta(\cdot\mid X^v, X^l), \quad (X^v, X^l) \in D \right\},
\]
where \( D \) is the instruction-tuning dataset.
To measure the richness of the visual representation, we employ the effective rank metric from Eq.~(\ref{eq:erank}). 
A higher effective rank indicates that the representation spans a broader subspace, whereas a lower effective rank signals more aggressive compression.  

\vspace{1em}\noindent\textbf{The Visual Forgetting Problem.} 
During instruction tuning, the visual representation undergoes significant compression as the model prioritizes textual supervision. 
This reduction occurs because the model effectively sacrifices part of $I(X^v; Z)$ to focus on $I(y; Z)$, thereby reducing the effective dimensionality of the visual features. 
As a result, the model progressively loses its ability to retain and utilize rich visual information, leading to a phenomenon we define as \textbf{\textit{visual forgetting}}.
Empirically, in Figure~\ref{fig:intro} we observe,
\begin{equation}\label{eq:erank_degradation}
    \text{erank}(\mathcal{Z}_\theta^v) < \text{erank}(\mathcal{Z}_0^v).
\end{equation}
This indicates that the tuned visual representation is compressed relative to the pre-trained space, making it harder for the model to leverage visual information effectively.
In RQ3 (Section~\ref{sec:repre}), we validate such empirical observations and demonstrate that our method helps to preserve effective ranks in the visual representation learning of MLLMs.












