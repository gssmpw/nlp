\section{MDGD: Modality-Decoupled Gradient Regularization and Descent}
Motivated by the visual forgetting problem caused by the degradation of multimodal encoding in Eq.~\eqref{eq:erank_degradation}, we introduce a modality-decoupling gradient regularization (\textbf{MDGD}) to approximate orthogonal gradients between visual understanding drift and downstream task optimization. Specifically, leveraging modality-decoupled gradients $\Bar{g}_\theta$ and $\Bar{g}_\phi$ derived from the current MLLM and a pre-trained MLLM respectively, we propose a gradient regularization term $\Tilde{g}_\theta$ for more efficient multimodal instruction tuning, which promotes the alignment of downstream tasks while mitigating visual forgetting \cite{zhu2024model}. Since MDGD requires the estimation of parameter gradients, we could not directly apply parameter-efficient fine-tuning methods (\emph{e.g.}, LoRA \cite{hu2021lora}). Thus, we alternatively formulate the regularization as a gradient mask $M_{\Tilde{g}_\theta}$, which allows efficient fine-tuning only on a subset of masked model parameters.

\subsection{Modality Decoupling}
Based on the information bottleneck objective in Eq.~\eqref{eq:ib_vision}, the objective encourages the model to maximize $I(y; Z)$ while compressing $I(X^v; Z)$ \cite{tishby2000information, alemi2016deep}. 
In practice, this compression may discard useful visual details, leading to visual forgetting. To mitigate such compression and preserve the pre-trained visual knowledge, we follow the KL divergence loss
$D_{\text{KL}}\Bigl(\mu_\phi(X^v) \,\Big\|\, \pi_\theta (X^v)\Bigr)$
to constrain the current model’s visual representation $\pi_\theta(X^v)$ to remain close to the pre-trained distribution $\mu_\phi(X^v)$, 
thereby preserving the mutual information $I(X^v; Z)$ that would otherwise be reduced by the compression \cite{hinton2015distilling, lopez2018information}. 
However, since MLLMs cannot directly track the distributions of image tokens, we instead introduce an auxiliary loss function
\begin{equation}\label{eq:visual_loss}
    \mathcal{L}_v(\phi,\theta) = \|\mu(X^v|\phi) - \pi(X^v|\theta)\|_1,
\end{equation}
which approximates the KL divergence loss \cite{zhu2022wdibs,zhu2017unpaired} by penalizing discrepancies between the pre-trained visual representation and that obtained during instruction tuning. 

In the MLLM instruction tuning, the visual output tokens (e.g., $\{z^{vl}_k\}_{k=1}^M$) are encoded as latent representations. 
Such visual encoding cannot be directly supervised by any learning objective but is learned through textual gradient propagation of the negative log-likelihood loss in downstream tasks. 
To approximate the visual optimization direction, we derive the gradients of $\mathcal{L}_v(\phi,\theta)$ for both the pre-trained MLLM $\pi_\phi$ and the current MLLM $\pi_\theta$:
\begin{align*}
    h_{\phi} &= \nabla_{\phi}\mathcal{L}_v(\phi) = \boldsymbol{\lambda}(\phi,\theta) \cdot \nabla_\phi \mu(X^v|\phi), \\
    h_{\theta} &= \nabla_{\theta}\mathcal{L}_v(\theta) = -\boldsymbol{\lambda}(\phi,\theta) \cdot \nabla_\theta \pi(X^v|\theta),
\end{align*}
where $\boldsymbol{\lambda}(\phi,\theta) = \text{sign}\left( \mu(X^v|\phi) - \pi(X^v|\theta) \right)$.
Intuitively, when the MLLM's visual understanding drift causes visual forgetting, we further derive the orthogonal task gradients $\Bar{g}_\phi$ and $\Bar{g}_\theta$:
\begin{align}\label{eq:orth}
    \Bar{g}_\phi &= \nabla_{\phi}\mathcal{L}_{vl}(\phi) - \frac{\nabla_{\phi}\mathcal{L}_{vl}(\phi)^\top h_{\phi}}{\|h_{\phi}\|^2} \cdot h_{\phi}, \\
    \Bar{g}_\theta &= \nabla_{\theta}\mathcal{L}_{vl}(\theta) - \frac{\nabla_{\theta}\mathcal{L}_{vl}(\theta)^\top h_{\theta}}{\|h_{\theta}\|^2} \cdot h_{\theta},
\end{align}
which enables \textbf{modality decoupling} of the downstream task loss gradient in Eq.\eqref{eq:task_loss} orthogonal to the visual understanding drift
for the pretrained MLLM $\Bar{g}_{\phi} \perp h_{\phi}$ and current MLLM $\Bar{g}_{\theta} \perp h_{\theta}$.

\input{contents/figure/fig-opt}
\input{contents/alg}





\subsection{Regularized Gradient Descent}
The auxiliary loss in Eq.~\eqref{eq:visual_loss} preserves the visual representation at a distribution level via the feature alignment auxiliary loss in Eq.~\eqref{eq:visual_loss}. 
However, the information bottleneck framework indicates that the gradient component compressing $I(X^v; Z)$ (\emph{i.e.}, $\nabla_\theta I(X^v; Z)$), 
can harm visual preservation by reducing the effective rank of the features \cite{achille2018information,lee2021compressive}.

To address this compression-induced drift, we incorporate an orthogonal gradient as a regularize. 
Motivated by multi-task orthogonal gradient optimization \cite{yu2020gradient, zhu2022gradient, dong2022gdod}, 
we leverage the gradient $\Bar{g}_\phi$ from the pre-trained model $\mu_\phi$, which reflects the accumulated visual drift and approximates a global orthogonal learning effect in the downstream task. 
We then project the current model’s gradient onto this direction:
\begin{equation}\label{eq:gd}
    \Tilde{g}_\theta = \frac{\Bar{g}_\theta^\top \Bar{g}_\phi}{\|\Bar{g}_\phi\|^2}\cdot \Bar{g}_\phi.
\end{equation}

In addition, to prevent discrepancies between the regularization and task gradients, we include the feature alignment auxiliary loss (Eq.~\eqref{eq:visual_loss}) in the overall objective. The final parameter update is:
\begin{equation}\label{eq:opt-gd}
    \pi_\theta \leftarrow \pi_\theta - \nabla_\theta\mathcal{L}_{vl}(\theta) - \nabla_\theta\mathcal{L}_v(\theta) - \Tilde{g}_\theta.
\end{equation}

\subsection{Enabling Parameter-efficient Fine-tuning of MDGD via Gradient Masking}
Parameter-efficient fine-tuning (PEFT) methods, such as adapters \cite{houlsby2019parameter} and LoRA \cite{hu2021lora}, aim to reduce the computational cost and memory usage when fine-tuning models on downstream tasks under practical constraints \cite{han2024parameter}. 
However, due to the requirement of directly estimating gradient directions on the pre-trained model parameters, MDGD cannot be directly applied to these PEFT methods, which introduce additional model parameters whose gradients are separate from the original model weights. 

To address this challenge, we propose a variant, MDGD-GM, by formulating the gradient regularization term in Eq.~\eqref{eq:gd} as gradient masking that selects model weights with efficient gradient directions. Specifically, we define the gradient mask as
\begin{equation}\label{eq:masking}
    M_{\Tilde{g}_\theta} = \mathbf{1}\left\{\frac{\Bar{g}_\theta^\top \Bar{g}_\phi}{\|\Bar{g}_\phi\| \|\Bar{g}_\theta\|} \geq T_\alpha \right\},
\end{equation}
where $T_\alpha$ is determined by a percentile $\alpha$ of trainable parameters with the highest similarity scores between $\Bar{g}_\theta$ and $\Bar{g}_\phi$. Consequently, the optimization in Eq.~\eqref{eq:opt-gd} is reformulated as
\begin{equation}\label{eq:opt-mask}
    \pi_\theta \leftarrow \pi_\theta - M_{\Tilde{g}_\theta} \cdot \left(\nabla_\theta\mathcal{L}_{vl}(\theta) + \nabla_\theta\mathcal{L}_v(\theta)\right).
\end{equation}
We summarize and illustrate the optimization process of MDGD and MDGD-GM in Algorithm~\ref{alg}.
