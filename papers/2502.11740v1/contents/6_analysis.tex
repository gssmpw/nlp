
\input{contents/figure/tsne-llava}
\input{contents/figure/tsne-minicpm}
\input{contents/figure/llava-curve}
\input{contents/figure/erank-eval}

\subsection{Ablation Study (RQ2)}
\subsubsection{Ablation study on visual alignment.}
We compare MDGD with its two variants, MDGD w/o visual align and MDGD-GM.
MDGD w/o visual align enables MDGD without including visual representation loss $\mathcal{L}_v(\phi,\theta)$ Eq.\eqref{eq:visual_loss}, 
to understand the effect of directly optimizing to reduce the visual representation discrepancy between the current model and pre-trained model.
We observe that MDGD w/o visual align maintains relatively comparable  performance to MDGD on OKVQA and PathQA,
due to the reduced need for visual representation adaptation in such visual question-answering tasks.
In contrast, tasks like image captioning on Flickr30K and TextCaps benefit from feature alignment regularization, 
which directly mitigates visual understanding drift in the MLLM.

\subsubsection{Ablation study on gradient masking.}
The other variant, MDGD-GM, leverages gradient masking to enable parameter-efficient fine-tuning (PEFT).
We observe the PEFT variant of MDGD consistently achieves comparable performance across all tasks and backbone MLLMs,
which only fine-tunes a subset of 10\% original MLLM parameters used for direct fine-tuning and original MDGD. 
Different from conventional PEFT methods such as adapters, 
MDGD and its variants do not introduce additional parameters to the original model architecture, 
enabling continuous and incremental learning in an online setting \citep{maltoni2019continuous,gao2023llama}.

\subsection{Representation Study (RQ3)} \label{sec:repre}
\subsubsection{T-SNE Analysis on Visual Representation}
To analyze the learning of visual and multimodal representation distributions in MLLMs, 
we create T-SNE \cite{van2008visualizing} plots to visualize the feature distributions extracted from pre-trained MLLMs, 
as well as MLLMs after standard fine-tuning and MDGD
We illustrate the distributions of the multimodal features $z^{vl}$ extracted from the last token of the multimodal instruction tokens, 
and the visual features $\pi_\theta(X^v)$ extracted from the last token of the input image tokens.
We observe a consistent visual understanding drift in the MLLMs' visual representation spaces after standard fine-tuning on Flickr30K and OKVQA with LLaVA (Figure~\ref{fig:tsne-llava}b and \ref{fig:tsne-llava}d), as well as PathVQA and TextCaps with MiniCPM (Figure~\ref{fig:tsne-cpm}b and \ref{fig:tsne-cpm}d).
By employing MDGD to mitigate visual forgetting, we observe that visual understanding drift is effectively reduced, 
allowing the fine-tuned MLLM to retain pre-trained visual capabilities and preserve visual information.

We further observe a distributional discrepancy in the multimodal 
representation $z^{vl}$ of LLaVA (Figures~\ref{fig:tsne-llava}a and \ref{fig:tsne-llava}c) 
between MDGD and the pre-trained MLLM. 
This discrepancy arises from the alignment of the MLLM to the target task through multimodal instructions, 
demonstrating effective adaptation to the downstream task of the LLaVA model.
In addition, we also observe such multimodal distribution discrepancy reduces in a smaller MLLM, MiniCPM.
This observation aligns with our findings on MiniCPM in Section~\ref{sec:main-results}, 
where we noted limited effects in model adaptation to downstream tasks. 
However, applying MDGD to MiniCPM mitigates visual forgetting by preventing degradation of both image and multimodal encodings into lower-rank representation spaces.


\subsubsection{Effective Rank Analysis on Visual Representation}
To quantitatively analyze the visual forgetting problem (in Section~\ref{sec:visual_forget}) described in Eq.~\eqref{eq:erank_degradation},
we calculate effective ranks of the visual representations extracted from the last hidden layer on the position of image tokens in individual MLLMs.
We show the comparison results of LLaVA models in Figure~\ref{fig:erank-eval1} and MiniCPM models in Figure~\ref{fig:erank-eval2}.
We observe that with both the backbone models of LLaVA and MiniCPM, 
directly fine-tuning the pre-trained models on downstream tasks can lead to a consistent reduction of effective ranks in visual representations.
Such observations validate the hypothesis in Section~\ref{sec:visual_forget} regarding the potential visual forgetting problem in MLLM instruction tuning.
In addition, we can observe that MDGD achieves consistent improvements in effective ranks compared with the standard fine-tuning method for both backbone MLLMs across various pre-trained tasks.
In Figure~\ref{fig:erank-eval1}, we observe that MDGD achieves comparable or even better effective ranks on pre-trained tasks, compared with the pre-trained LLaVA model.
However, MDGD on MiniCPM in Figure~\ref{fig:erank-eval2} also suffers from the visual representation degradation problem, while MDGD consistently alleviates the problem.
Such observation suggests a higher risk of visual forgetting in smaller-scale MLLMs.



\subsection{Sensitivity Study (RQ4)}
We evaluate the learning curves of MDGD and MDGD-GM compared with standard fine-tuning in Figure~\ref{fig:learning}(a),
where we observe that MDGD and MDGD-GM achieve comparable training efficiency compared with the standard fine-tuning method.
We also investigate the sensitivity of gradient cosine similarity between $\Bar{g}_\theta$ and $\Bar{g}_\phi$ in Figure~\ref{fig:learning}(b) and the representation loss in Figure~\ref{fig:learning}(c),
with respect to the gradient masking ratio in MDGD-GM.
In Figure~\ref{fig:learning}(b), we observe that MDGD-GM with lower gradient masking ratios can better align the modality-decoupled learning gradients between the target model and the pre-trained model,
while MDGD-GM maintains over 70\% alignment with 50\% gradient masking.
In Figure~\ref{fig:learning}(c), we show that MDGD-GM with 50\% gradient masking still effectively alleviates the visual representation degradation problem by reducing the visual representation discrepancy $\mathcal{L}_v$,
while learning with a more active gradient can achieve better alignment.
