


\begin{algorithm}[ht]
\caption{MDGD: Modality Decoupled Gradients Descent }
\label{alg}
\begin{algorithmic}[1]
\State \textbf{Inputs:} 
Pre-trained MLLM $\mu_\phi$, current MLLM $\pi_\theta$, instruction-tuning dataset $D$, and learning rate $\eta$
\State \textbf{Outputs:} The optimized model weights of $\pi_\theta$
\State \textbf{Initialize} $\pi_\theta \leftarrow \mu_\phi$
\For{Receive minibatch $D_i\subset D$}
    \State Calculate $\mathcal{L}_{vl}(\phi)$ of $\mu_\phi$, based on Eq.\eqref{eq:task_loss};
    \State Calculate $\mathcal{L}_{vl}(\theta)$ of $\pi_\theta$, based on Eq.\eqref{eq:task_loss};
    \State Extract visual encodings of $\mu(X^v|\phi)$;
    \State Extract visual encodings of $\pi(X^v|\theta)$;
    \State Calculate $\mathcal{L}_v(\phi,\theta)$, based on Eq.\eqref{eq:visual_loss};
    \State Derive orthogonal task gradients $\Bar{g}_\phi$ and $\Bar{g}_\theta$, according to Eq.\eqref{eq:orth};
    \If{Parameter-efficient fine-tuning}
        \State Calculate $M_{\Tilde{g}_\theta}$,based on Eq.\eqref{eq:masking};
        \State Update the model following Eq.\eqref{eq:opt-mask}.
    \Else
        \State Calculate $\Tilde{g}_\theta$, based on Eq.\eqref{eq:gd};
        \State Update the model following Eq.\eqref{eq:opt-gd}.
    \EndIf
\EndFor
\end{algorithmic}
\end{algorithm}
