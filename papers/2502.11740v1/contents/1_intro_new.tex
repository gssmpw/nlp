\section{Introduction}
Multimodal large language models (MLLMs) enhanced visual understanding and reasoning by pre-training on large-scale multimodal datasets with comprehensive visual descriptions that integrats textual and visual knowledge  \cite{liu2024visual,yao2024minicpmvgpt4vlevelmllm,li2023blip,bai2023qwen,liu2024multimodal}. 
These models achieve strong performance across various vision-language tasks, 
such as visual question answering \cite{jin2024rjua}, multimodal reasoning \cite{zhang2024large,jiang2024killing,yan2024list}, multimodal recognition \cite{shenoy2024lumos,wu2024visual}, 
personalized multimodality \cite{wu2024personalized}, and document intelligence \cite{jin2024rjua, shenoy2024lumos}.
However, adapting pre-trained MLLMs to downstream tasks via instruction-tuning  \cite{wu2024commit,li2024vision,li2023fine,panagopoulou2023x,liu2024multimodal} presents a critical challenge of visual forgetting. 
Unlike pre-training, where models receive rich visual-text alignment, instruction-tuning is often text-driven with limited direct visual supervision. 
This shift in training focus leads to the degradation of pre-trained visual encoding \cite{zhou2024mitigating,niu2024text,wu2024commit,ko2023large}, 
negatively impacting model generalizability across downstream tasks that require strong visual knowledge \cite{bai2024hallucination,huang2024visual}. 
Addressing this challenge is essential for ensuring MLLMs retain their visual capabilities while aligning with new tasks efficiently.

While several approaches have attempted to mitigate catastrophic forgetting in neural networks through direct fine-tuning and continual learning methods \cite{shi2024continual, wu2024continual, zhu2024model, zheng2024beyond}, 
these methods often overlook the unique challenge of preserving visual knowledge in multimodal large language models (MLLMs). 
Directly fine-tuning MLLMs on new tasks often leads to overfitting to textual instructions while inadvertently suppressing visual representations \citep{zhai2023investigating}.
Existing continual learning strategies such as regularization and replay methods tend to focus on retaining language-based knowledge, 
neglecting the critical trade-off between compressing visual representations and aligning them with task-specific instructions \cite{zhou2024mitigating, niu2024text, wu2024commit, ko2023large}, 
leading to the degradation of pre-trained visual knowledge.
Task-orthogonal gradient descent techniques have shown promise in disentangling gradients for multi-task optimization.
However, their practical application in MLLMs poses unique challenges. 
MLLMs are pre-trained on vast and heterogeneous multimodal datasets \cite{liu2024visual, li2023blip, bai2023qwen}, 
where it is challenging to precisely isolate task-specific gradients, 
causing the components critical for visual understanding to become entangled with other features.
\input{contents/figure/figure-intro}

To gain a fundamental view of the challenge of visual knowledge forgetting in MLLM instruction tuning, 
we adopt an information bottleneck (IB) perspective that characterizes the trade-off between retaining input information and ensuring output predictiveness \cite{tishby2000information}.
To investigate the degradation of crucial pre-trained visual knowledge, we introduce a novel perspective that leverages effective rank to quantify the richness of the encoded visual representation from MLLMs.
Specifically, we illustrate the visual forgetting problem in Figure~\ref{fig:intro}, where we observe a consistent effective rank reduction problem caused by MLLM instruction tuning.
Based on this view, we propose a modality-decoupled gradient descent (MDGD) method, which disentangles the optimization of visual understanding from task-specific alignment,
MDGD regulates gradient updates to maintain the effective rank of visual representations compared with pre-trained MLLMs,
while mitigating the over-compression effects described by the information bottleneck. 
Intuitively, visual forgetting occurs due to the shift from rich multimodal pre-training to instruction-tuning, 
where text-based supervision dominates without direct visual supervision. 
By explicitly decoupling the task-specific alignment with visual representation learning, MDGD preserves expressive and robust visual features. 
To further improve efficiency in instruction-tuning, we introduce a memory-efficient fine-tuning strategy using gradient masking, 
which selectively updates a subset of model parameters for parameter-efficient fine-tuning (PEFT).
This approach reduces computational overhead while ensuring that crucial pre-trained visual representations are retained.


We summarize our contributions as follows: 
\begin{itemize}
\item We analyze the visual knowledge forgetting problem in MLLM instruction tuning and frame the problem through the lens of effective rank and information bottleneck theory.
\item We propose MDGD, which decouples visual optimization from task-specific alignment to preserve visual representations and introduces a PEFT variant MDGD-GM to reduce computational overhead through gradient masking. 
\item We conduct comprehensive experiments on various MLLMs and downstream tasks, demonstrating that MDGD effectively mitigates visual forgetting while enabling strong adaptation to new tasks.
\end{itemize}




