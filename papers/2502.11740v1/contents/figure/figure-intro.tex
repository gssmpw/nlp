\begin{figure}[btp]
    \centering
    \subfigure[LLaVA on OKVQA]{%
        \includegraphics[width=0.48\linewidth]{contents/figure/imgToken-erank--LLaVA-OKVQA.pdf} %
        \label{fig:erank-token1}
    }
    \hfill
    \subfigure[LLaVA on POPE]{%
        \includegraphics[width=0.48\linewidth]{contents/figure/imgToken-erank--LLaVA-POPE.pdf} %
        \label{fig:erank-token2}
    }
    
    
    \subfigure[MiniCPM on PathVQA]{%
        \includegraphics[width=0.48\linewidth]{contents/figure/imgToken-erank--MiniCPM-PathVQA.pdf} %
        \label{fig:erank-token3}
    }
    \hfill
    \subfigure[MiniCPM on POPE]{%
        \includegraphics[width=0.48\linewidth]{contents/figure/imgToken-erank--MiniCPM-POPE.pdf} %
        \label{fig:erank-token4}
    }

    
    \caption{The top-10 image tokens with the highest effective ranks on OKVQA and POPE encoded by LLaVA, and PathVQA and POPE encoded by MiniCPM. 
    We compare pretrained, finetuned, and MDGD-finetuned models. 
    Effective rank \cite{wei2024diff} quantifies representation richness, and we novelly use it to analyze visual degradation in instruction-tuned MLLMs. Results show that MDGD preserves higher effective rank, mitigating visual forgetting.}
    \label{fig:intro}
    \vspace{-1em}
\end{figure}
