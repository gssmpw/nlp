
\input{contents/tabs/llava-main}
\section{Experiments}
In this section, we conduct experiments on various datasets and backbone MLLMs to investigate the following research questions:
\begin{enumerate}
    \item \textbf{RQ1 (Overall Performance)} Can MDGD prevent visual forgetting while improving downstream tasks?
    \item \textbf{RQ2 (Ablation Study)} How do the visual alignment and gradient masking affect the MDGD's performance?
    \item \textbf{RQ3 (Representation Learning)} How does MDGD benefit effective multimodal representation learning in MLLMs?
    \item \textbf{RQ4 (Sensitivity Study)} How does gradient masking ratio $\alpha$ affects the learning of MDGD?
\end{enumerate}


\noindent\textbf{Datasets}
To evaluate the effectiveness of MDGD in mitigating catastrophic forgetting, we used two models of different sizes. 
Our experimental design follows the settings from the work of \citet{zhu2024model}. 
For each model, datasets were categorized into two types: 
\textbf{pre-trained tasks}, which assess the model's ability to retain inherent knowledge after fine-tuning, 
and \textbf{fine-tuning tasks}, consisting of unseen datasets used to test adaptability. 
After fine-tuning, we evaluated performance on both task types to measure forgetting and generalization. 
Below, we detail the datasets used for each model. 
\textbf{LLaVA-1.5 (Vicuna-7B) \citep{liu2024improved}}: This model has 7 billion parameters. In line with \citet{liu2024improved}, we used the following datasets:
\begin{itemize}
    \item \textbf{Pre-trained Tasks}: VQAv2 \citep{goyal2017making}, GQA \citep{hudson2019gqa}, VizWiz \citep{gurari2018vizwiz}, SQA \citep{lu2022learn}, TextVQA \citep{singh2019towards}, POPE \citep{li2023evaluating}, and MM-Bench \citep{liu2023mmbench}.
        
    \item \textbf{Fine-tuning}: Flickr30k \citep{young2014image} and OKVQA \citep{marino2019ok}, which were not encountered in the pre-training stage.
\end{itemize}
\textbf{MiniCPM-V-2.0 \citep{yao2024minicpmvgpt4vlevelmllm}}: This model has 2.8 billion parameters. We evaluated its performance on:
\begin{itemize}
    \item \textbf{Pre-trained Tasks}: VizWiz, OKVQA, A-OKVQA \cite{schwenk2022okvqa}, Text-VQA, IconQA \cite{lu2021iconqa}, POPE, and MM-Bench.
    \item \textbf{Fine-tuning}: TextCaps \citep{sidorov2020textcaps} and PathVQA \citep{he2020pathvqa}, which were not part of its pre-training exposure.
\end{itemize}



\noindent\textbf{Baselines} We compare our approach against several baselines: 
\begin{itemize}
\item \textbf{Standard Fine-Tuning.} For a fair comparison, we follow the setting of Model-Tailor \cite{zhu2024model}, where LLaVA-1.5 is fine-tuned on the last 6 layers and its feature adapter, with a total of 1.2B parameters.
MiniCPM is fine-tuned on the last 8 layers and its feature resampler, with 517M parameters. 
\item \textbf{LoRA-based Fine-Tuning \citep{hu2021lora}.} LoRA introduces low-rank matrices to update only a small subset of parameters, reducing memory consumption and computational cost. In our experiments, LLaVA-1.5 and MiniCPM are fine-tuned by modifying the query and key projection layers within the attention mechanism. 
\item \textbf{Model Tailor \citep{zhu2024model}.} This baseline employs a hybrid strategy that mitigates catastrophic forgetting by identifying and adjusting the most critical parameters for adaptation. It has been evaluated through experiments on multimodal large language models (MLLMs). As the method is not open source, we report only the original results of the LLaVA-1.5 experiments provided in the original paper as a baseline.
\end{itemize}


\noindent\textbf{Implementation Details}
We use the official Huggingface implementations of the LLaVA-1.5 and the MiniCPM-V-2.0 models and their LoRA adapters. 
For model fine-tuning, we use BFloat16 precision for memory-efficient training. 
Experiments are conducted using 2 NVIDIA A100-SXM4-80GB GPUs.

\subsection{Overall Performance (RQ1)}\label{sec:main-results}
\subsubsection{Larger MLLM adapts better to downstream tasks but is more prone to visual forgetting.}
We study the visual forgetting problem on the LLaVA-1.5 MLLM which contains 7B model parameters 
and report performance comparison results in Table~\ref{tab:llava-main}.
We observe that the pre-trained LLaVA enables efficient instruction tuning on target tasks,
where the zero-shot performance is near zero.
When the model is fine-tuned on the image caption task, Flickr30K, which largely differs from the pre-trained tasks of visual question-answering,
the model can learn a degraded multimodal representation,
which causes visual forgetting in its projected visual representation space (in Section~\ref{sec:visual_forget}) 
and its average performance on pre-trained tasks drops 33.63\% compared with zero-shot performance. 
Fine-tuning on visual question-answering task OKVQA, which is similar to the pre-trained tasks, can also cause a 13.44\% performance drop,
due to the limited image-text pairs existing in the downstream task, which potentially leads to MLLM's visual understanding drift.

\subsubsection{Smaller MLLM also experiences visual forgetting while limited in downstream task improvements.}
To validate the observation on a smaller MLLM, we report the comparison results of MiniCPM-V-2.0 with 2.8B model parameters in Table~\ref{tab:minicpm}.
We observe that compared with the LLaVA MLLM, MiniCPM suffers from less prominent visual forgetting.
The average performance drop of the model limits to 6.28\% and 4.25\% when fine-tuning on PathVQA and TextCaps, respectively.
We attribute this observation to MiniCPM learning a more compact and constrained visual representation space during pre-training, 
causing the visual representations of target task images to be less aligned with those of the pre-trained MLLM. 
Consequently, MiniCPM exhibits limited improvement in downstream tasks, 
as its restricted ability to acquire additional visual knowledge leads to ineffective instruction tuning.

\subsubsection{MDGD prevents visual forgetting while maintaining downstream task improvements.}
By employing MDGD in MLLM instruction tuning,
we observe the LLaVA's average performance drop on pre-trained tasks reduces to 3.59\% when fine-tuned on OKVQA
and also achieves a 1.45\% improvement when fine-tuned on Flickr30K,
which demonstrates the efficiency of MDGD in mitigating visual forgetting.
For the smaller MLLM, MiniCPM, MDGD achieves comparable downstream task improvements with direct fine-tuning,
while completely eliminating visual forgetting in the pre-trained tasks.
MDGD and its variants consistently achieve the best average performance for both MLLMs,
demonstrating its great potential for incremental learning on individual downstream tasks.

\subsubsection{Comparison with baseline methods.}
In Table~\ref{tab:llava-main}, we compare MDGD with LoRA fine-tuning and Model Tailor \cite{zhu2024model} on LLaVA-1.5, 
which are designed for parameter-efficient fine-tuning.
We observe that LoRA fine-tuning can suffer from significant visual forgetting on Flickr30K and OKVQA.
Since LoRA introduces additional representation projections in intermediate layers,
the pre-trained multimodal representations can be projected into a lower-rank subspace leading to visual forgetting (in Section~\ref{sec:visual_forget}),
due to the limitation of image-text pairs in the target dataset.
Model Tailor is designed for MLLM anti-forgetting, 
which identifies ``patches'' of sub-model parameters significantly affected by fine-tuning on the target task.
However, since the method is not specifically designed for MLLMs, 
the unique challenge of visual forgetting cannot be effectively mitigated while maintaining robust performance on the target task.Thus, we observe that Model Tailorâ€™s performance is sensitive to the target task datasets (e.g., better on Flickr30K than OKVQA), 
whereas MDGD consistently outperforms Model Tailor in terms of both average task scores and H-scores across the two datasets.
In Table~\ref{tab:minicpm}, we also report the results of the MDGD comparison with LoRA fine-tuning on MiniCPM.
We observe consistent improvements on the average task performance of MDGD when fine-tuned on PathVQA and TextCaps,
especially MDGD achieves 2.43\% and 1.83\% on the pre-trained tasks of PathVQA and TextCaps, respectively,
which demonstrates the effectiveness of MDGD in mitigating visual forgetting. 

\input{contents/tabs/minicpm-main}
