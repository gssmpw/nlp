\documentclass[10pt]{article}

% \usepackage{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
\usepackage[preprint]{tmlr}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\input{mathdef}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx, color}

\usepackage[noorphans]{quoting}
\usepackage[square,sort,comma,numbers]{natbib}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage{booktabs}

\usepackage{caption}

\usepackage[most]{tcolorbox}

\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\SetKwInput{KwInput}{Input}
\SetKwInput{KwOutput}{Output}

\newenvironment{prooftn}[1]{%
\renewcommand{\proofname}{Proof of Theorem {#1}}\proof}{\endproof}
\newenvironment{proofln}[1]{%
\renewcommand{\proofname}{Proof of Lemma {#1}}\proof}{\endproof}
\newenvironment{proofpn}[1]{%
\renewcommand{\proofname}{Proof of Proposition {#1}}\proof}{\endproof}
\newenvironment{sketch}{%
\renewcommand{\proofname}{Proof Sketch}\proof}{\endproof}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\allowdisplaybreaks

\newcommand*{\lbb}{\{\mskip-5mu\{}
\newcommand*{\rbb}{\}\mskip-5mu\}}

\usepackage{listings}
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    commentstyle=\color{olive},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{gray},
    numbers=left,
    stepnumber=1,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    frame=single,
    tabsize=2,
    captionpos=b,
    breaklines=true,
    breakatwhitespace=true,
    escapeinside={(*@}{@*)}
}


\title{Solving the Cold Start Problem on One's Own as an End User via Preference Transfer}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

\author{\name Ryoma Sato \email rsato@nii.ac.jp \\
      \addr National Institute of Informatics}

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

% \def\month{MM}  % Insert correct month for camera-ready version
% \def\year{YYYY} % Insert correct year for camera-ready version
% \def\openreview{\url{https://openreview.net/forum?id=XXXX}} % Insert correct link to OpenReview for camera-ready version


\begin{document}

\maketitle

\begin{abstract}
  We propose a new approach that enables end users to directly solve the cold start problem by themselves. The cold start problem is a common issue in recommender systems, and many methods have been proposed to address the problem on the service provider's side. However, when the service provider does not take action, users are left with poor recommendations and no means to improve their experience. We propose an algorithm, \textsc{Pretender}, that allows end users to proactively solve the cold start problem on their own. \textsc{Pretender} does not require any special support from the service provider and can be deployed independently by users. We formulate the problem as minimizing the distance between the source and target distributions and optimize item selection from the target service accordingly. Furthermore, we establish theoretical guarantees for \textsc{Pretender} based on a discrete quadrature problem. We conduct experiments on real-world datasets to demonstrate the effectiveness of \textsc{Pretender}.
\end{abstract}

\section{Introduction}

Recommender systems have become an essential component of many online services, such as e-commerce \cite{linden2003amazon, mcauley2013hidden}, social media \cite{weng2010twitterrank, chitra2020analyzing}, and video streaming platforms \cite{bell2007lessons,uribe2016netflix,steck2018calibrated}. These systems analyze user preferences and recommend items that the user may like. A common problem in recommender systems is the cold start problem, where the system cannot provide good recommendations for new users due to a lack of historical data. Many methods have been proposed to address the cold start problem, such as content-based filtering \cite{mooney2000content, schein2002methods, lam2008addressing}, utilizing side information \cite{zhao2016connecting, park2009pairwise, lin2013addressing}, meta learning \cite{vartak2017meta, lee2019melu, lu2020meta}, and active learning \cite{zhou2011functinoal, christakopoulou2016towards}.

However, all of these methods are designed to be implemented on the service provider's side, and require the service provider to take action. When the service provider does not take action, e.g., due to lack of resources, lack of incentives, or simple negligence, users are left with poor recommendations and no immediate way to improve their experience. This can be particularly frustrating for users who are eager to adopt a new service but struggle to discover relevant items due to the cold start problem.

In this paper, we propose a new problem setting that enables users to address the cold start problem on their own. We consider a scenario where a user has been using a source service (e.g., Netflix) for a long time and has a history of preferences for items. The user has just started using a target service (e.g., Hulu) but lacks any preference history on that platform. The user receives good recommendations from the source service but poor recommendations from the target service due to the cold start problem. The user wants to transfer the preferences from the source service to the target service so that the user can enjoy good recommendations from the target service as well. If the target service offers a built-in functionality to import preferences, this problem can be easily solved. However, in many cases, such functionality is not provided, leaving the user with no direct means to improve their recommendations. To address this, we propose an algorithm, \textsc{Pretender}, that enables users to overcome the cold start problem independently, even when neither the source nor the target service provides dedicated support for preference transfer.

We formulate the problem as minimizing the distance between the source and target distributions and optimize the selection of items from the target service accordingly. A key strength of our method is that it provides strong theoretical guarantees even when the way the target service uses the data is unknown. We prove this by solving a new quadrature problem that arises in the optimization process, which is of independent interest. We conduct experiments on real-world datasets to demonstrate the effectiveness of \textsc{Pretender}. Our experiments show that \textsc{Pretender} can transfer the preferences effectively.

The contributions of this paper are as follows:

\begin{itemize}
  \item We propose a new problem setting that enables users to independently address the cold start problem on their own.
  \item We propose \textsc{Pretender}, an algorithm that allows users to overcome the cold start problem with theoretical guarantees, even in the absence of support from the service provider.
  \item We conduct experiments on real-world datasets to validate the effectiveness of \textsc{Pretender}.
\end{itemize}

\section{Problem Setting} \label{sec: problem_setting}

Suppose we are an end user of the source service (e.g., Netflix) and the target service (e.g., Hulu). We have been using the source service for a long time and have a history of preferences for items (e.g., thumbs-up or thumbs-down for videos). We have just started using the target service and lack any preference history on that platform. We receive poor recommendations from the target service due to the cold start problem. Our goal is to transfer our preferences from the source service to the target service so that we can enjoy good recommendations from the target service as well. The problem is formally defined as follows:

\begin{tcolorbox}[colframe=gray!20,colback=gray!20,sharp corners]
  \textbf{Problem (Preference Transfer).} \\
  \textbf{Input}: Sets of source items $I_S$ and target items $I_T$. Features $\boldx_i$ of items $i \in I_S \cup I_T$. A set $\mathcal{D}_S = \{(i, y_i)\} \subset I_S \times \{0, 1\}$ of user preferences on source items. A positive integer $K \in \mathbb{Z}_+$ representing the number of target items to interact with. \\ 
  \textbf{Output}: A set  $\mathcal{D}_T = \{(i, y_i)\} \subset I_T \times \{0, 1\}$ with $|\mathcal{D}_T| = K$ such that clicking items following $\mathcal{D}_T$ results in good recommendations from the target service.
\end{tcolorbox}

A method outputs a set of preferences for the target items, denoted as $\mathcal{D}_T = \{(i, y_i)\}$, Following this output, the end user clicks thumbs up for each item $i \in \{i \in I_T \mid (i, 1) \in \mathcal{D}_T\}$ and thumbs down for each item $i \in \{i \in I_T \mid (i, 0) \in \mathcal{D}_T\}$. A good method should output $\mathcal{D}_T$ such that the user can enjoy good recommendations from the target service after this process. Note that the clicking process (and thus the entire process) may be automated by a Web agent.

This problem presents three main challenges:

\begin{itemize}
\item \textbf{Items are not shared between services.} A naive approach might simply ``copy'' the history to the target service, but this approach fails because the corresponding items may not exist in the target service, i.e., $I_S \neq I_T$.
\item \textbf{Clicking many items is tedious.} The user may not want to interact with many items just for preference transfer, i.e., $K$ is small. Even when the clicking process is automated, too large $K$ may take a long time and/or impose a heavy load on the target service.
\item \textbf{The target service's use of the data is unknown.} The target service may process preference data $\mathcal{D}_T$ differently from the source service or in an unexpected manner, making it difficult to predict how the transferred preferences will influence recommendations.
\end{itemize}

\section{Pretender}

We propose \textsc{Pretender} (\underbar{PRE}ference \underbar{T}ransfer by \underbar{END} us\underbar{ER}s) to solve the preference transfer problem. We first describe the gerneral framework and then provide variants for specific settings.


\subsection{Formulating the Problem as Distance Minimization} \label{sec: formulation}

The goal of \textsc{Pretender} is to select items from the target service such that its empirical distribution $\mu_T^{\mathcal{D}_T}$ is close to the source distribution $\mu_S$, which are defined as \begin{align}
  \mu_T^{\mathcal{D}_T} = \frac{1}{K} \sum_{(i, y_i) \in \mathcal{D}_T} \delta_{(\boldx_i, y_i)}, \qquad  \mu_S = \frac{1}{|\mathcal{D}_S|} \sum_{(i, y_i) \in \mathcal{D}_S} \delta_{(\boldx_i, y_i)}
\end{align} where $\delta_x$ denotes the Dirac measure at $x$. We use the integral probability metric (IPM) \cite{muller1997integral,sriperumbudur2009note} to quantify the discrepancy between the distributions, which is defined for a function class $\mathcal{F}$ as \begin{align}
  \text{IPM}_\mathcal{F}(\mu, \nu) = \sup_{f \in \mathcal{F}} \int f \, d\mu - \int f \, d\nu. \label{eq: ipm}
\end{align} When $\mathcal{F}$ is the class of functions with a reproducing kernel hilbert space (RKHS) norm at most 1, the IPM is equivalent to the maximum mean discrepancy (MMD) \cite{gretton2012kernel}, and when $\mathcal{F}$ is the class of 1-Lipschitz functions, the IPM corresponds to the $1$-Wasserstein distance \cite{peyre2019computational, villani2009optimal}. \textsc{Pretender} selects items such that $\text{IPM}_\mathcal{F}(\mu_T, \mu_S)$ is small, ensuring that the target distribution closely aligns with the source distribution.

This formulation addresses the third challenge. Suppose the target service uses an unknown model $f_T(\cdot; \theta)$ and unknown loss function $\ell_T(\cdot, \cdot)$ to train the model. We only know that the loss $\ell_T(f_T(\boldx; \theta), y)$ is $L$-Lipschitz in $(\boldx, y)$. Then, the model incurs the following loss on the source preferences: \begin{align}
  (\text{loss on the source data}) &= \frac{1}{|\mathcal{D}_S|} \sum_{(i, y_i) \in \mathcal{D}_S} \ell_T(f_T(\boldx_i; \theta), y_i) \\
  &= \int \ell_T(f_T(\boldx; \theta), y) \, d\mu_S(\boldx, y) \\
  &\leq \int \ell_T(f_T(\boldx; \theta), y) \, d\mu_T^{\mathcal{D}_T}(\boldx, y) + L \cdot W_1(\mu_T^{\mathcal{D}_T}, \mu_S) \\
  &= (\text{training loss on the target data}) + L \cdot W_1(\mu_T^{\mathcal{D}_T}, \mu_S)
\end{align} where $W_1$ denotes the $1$-Wasserstein distance. The inequality follows from the definition of IPM (Eq. (\ref{eq: ipm})). Therefore, if we minimize the Wasserstein distance $W_1(\mu_T, \mu_S)$ between the source and target distributions, and the target service effectively minimizes the training loss on the target data, the model trained on the target preferences will also generalize well to the source preferences, thereby accurately reflecting the user's preferences

The crux of this approach is that its guarantee is agnostic to the model, loss function, and the training method the target service employs, which are typically not known to the user. Regardless of how the target service uses the data, the user can ensure that the recommendation model trained on the target preferences reflects the source preferences as long as the distributions are close.

The use of IPM also addresses the first challenge. IPM exploits the geometry of the data space through the smoothnessof the function class $\mathcal{F}$ (e.g., Lipschitz continuity and a small RKHS norm) and can be used even when the items are not shared between the services. This is in a stark contrast to other discrepancy measures such as KL divergence and Hellinger distance.

Although we have formulated the problem as distance minimization between distributions, this problem remains challenging because selection of items is combinatorial. This is in contrast to other minimization problems, where optimization is performed over weights and/or coordinates of data points, making the problem continuous and often convex. However, end users cannot ``thumbs up 0.2 points'' or alter the features of the items in the service, meaning that we cannot sidesitep the combinatorial nature of the problem. We address this challenge in the following.

\subsection{Optimization} \label{sec: optimization}

The optimization framework of \textsc{Pretender} is as follows:

\begin{enumerate}
  \item (\textbf{Item Preparation}) Prepare the set of labeled target items $J_T = \{(i, y) \mid i \in I_T, y \in \{0, 1\}\}$. We rearrange the items such that they are indexed by $1, 2, \ldots, 2m$ and redefine $J_T = \{(i_j, y_j) \mid j \in [2m]\}$, where $m = |I_T|$ is the number of items in the target service.
  \item (\textbf{Continuous Optimization}) Optimize the weights $\boldw \in \left[0, \frac{1}{K}\right]^{2m} \cap \Delta_{2m}$ of the items in the target service such that the weighted empirical distribution $\mu_T^\boldw = \sum_{j = 1}^{2m} \boldw_i \delta_{(\boldx_{i_j}, y_j)}$ is close to the source distribution $\mu_S$, where $\Delta_d$ denotes the $(d-1)$-dimensional probability simplex. This is achieved by solving the following optimization problem: \begin{align}
    \begin{split}
    &\min_{\boldw \in \mathbb{R}^{2m}} \mathcal{D}\left(\sum_{j = 1}^{2m} \boldw_i \delta_{(\boldx_{i_j}, y_j)}, \mu_S\right), \\
    &\quad \text{s.t.} \quad \sum_{j = 1}^{2m} \boldw_j = 1, \qquad 0 \le \boldw_j \le \frac{1}{K} ~(j = 1, 2, \ldots, 2m). \label{eq: continuous_optimization}
    \end{split}
  \end{align}
  \item (\textbf{Random Selection}) Sample items according to the optimized weights. For each $j \in [2m]$, sample $I_j \sim \text{Bernoulli}(K w_j)$ for $j \in [2m]$, and define the selected set as $\hat{\mathcal{D}}_T = \{(i_j, y_j) \mid I_j = 1\}$.
  \item (\textbf{Postprocessing}) If $|\hat{\mathcal{D}}_T| < K$, greedily insert additional items, and if $|\hat{\mathcal{D}}_T| > K$, greedily remove items to obtain the final preference set $\mathcal{D}_T$ satisfying $|\mathcal{D}_T| = K$.
\end{enumerate}

\textbf{Item Preparation}. We  construct the set of labeled target items as $J_T = \{(i_j, y_i) \mid j \in [2m]\}$. Since the user can choose the label for each item (e.g., thumbs up or thumbs down), we include both possible labels $(i, 0)$ and $(i, 1)$ in the candidate set, making the total number of items $2m$.

\textbf{Continuous Optimization}. We first solve the continuous relaxation of the problem. This problem is continuous and convex for MMD and the Wasserstein distance. Therefore, we can employ standard methods such as the Frank-Wolfe algorithm \cite{jaggi2013revisiting} or the projected subgradient descent algorithm \cite{boyd2003subgradient} to solve the problem. By standard results in convex optimization, we can obtain $\hat{\boldw}$ such that \begin{align}
  \mathcal{D}(\mu_T^{\hat{\boldw}}, \mu_S) &\leq \text{OPT}^{\text{continuous}} + \epsilon \\
  &= \min_{\boldw \in \left[0, \frac{1}{K}\right]^{2m} \cap \Delta_{2m}} \mathcal{D}\left(\mu_T^\boldw, \mu_S\right) + \epsilon \\
  &\stackrel{\text{(a)}}{\le} \min_{\mathcal{D}_T = \{(i, y_i)\}\colon |\mathcal{D}_T| = K} \mathcal{D}\left(\frac{1}{K} \sum_{(i, y_i) \in \mathcal{D}_T} \delta_{(\boldx_i, y_i)}, \mu_S\right) + \epsilon \\
  &= \text{OPT}^{\text{combinatorial}} + \epsilon,
\end{align} where (a) follows because the weight $(1/K, \ldots, 1/K)$ of the empirical meassure is in the feasible set. Therefore, the continuous solution is at least as good as the combinatorial solution. However, we need to carefully round the solution to obtain the final output to ensure that the rounding process does not degrade the quality of the solution much. This is the main challenge from a theoretical perspective.

\textbf{Random Selection}. We employ a randomized approach. We first point out that the distribution Bernoulli$(K \boldw_j)$ is well-defined as we set the optimization domain to $\boldw \in \left[0, \frac{1}{K}\right]^{2m} \cap \Delta_{2m}$ and $0 \le K \boldw_j \le 1$ holds. Let $\tilde{\boldw} \in \{0, 1/K\}^{2m}$ be the sample weights after the random selection, i.e., $\tilde{\boldw}_j = \frac{1}{K} I_j$. Then, \begin{align}
  \mathbb{E}[\tilde{\boldw}_j] = \frac{1}{K} \mathbb{E}[I_j] \stackrel{\text{(a)}}{=} \frac{1}{K} K \boldw_j = \boldw_j,
\end{align} where (a) follows from $I_j \sim \text{Bernoulli}(K \boldw_j)$. Therefore, the expected weight is the same as the continuous solution. This implies that $\mathcal{D}(\mu_T^{\tilde{\boldw}}, \mu_S)$ distributes around that of the continuous solution $\mathcal{D}(\mu_T^{\hat{\boldw}}, \mu_S) \approx \text{OPT}^{\text{continuous}}$. Next, we analyze the number of selected items, which is given by, in expectation, \begin{align}
  \mathbb{E}\left[\sum_{j = 1}^{2m} I_j\right] = \sum_{j = 1}^{2m} K \boldw_j \stackrel{\text{(a)}}{=} K, \label{eq: exp_num_items}
\end{align} where (a) follows from the fact that $\sum_{j = 1}^{2m} \boldw_j = 1$, i.e., $\boldw \in \Delta_{2m}$. In addition, \begin{align}
  \text{Var}\left[\sum_{j = 1}^{2m} I_j\right] &\stackrel{\text{(a)}}{=} \sum_{j = 1}^{2m} \text{Var}[I_j]
  \stackrel{\text{(b)}}{=} \sum_{j = 1}^{2m} K \boldw_j (1 - K \boldw_j)
  \stackrel{\text{(c)}}{\le} \sum_{j = 1}^{2m} K \boldw_j
  \stackrel{\text{(d)}}{=} K, \label{eq: var_num_items}
\end{align} where (a) follows from the independence of the Bernoulli random variables, (b) follows from the variance of the Bernoulli random variables, (c) follows from $\boldw_j \ge 0$, and (d) follows from $\sum_{j = 1}^{2m} \boldw_j = 1$. Therefore, the standard deviation is the order of $\sqrt{K}$, and the number of selected items is concentrated around $[K - O(\sqrt{K}), K + O(\sqrt{K})]$. This implies that the selection process does not blow up the error too much. We will elaborate this discussion in the following sections.

\textbf{Postprocessing}. Since the number of selected items may deviate from $K$, we may need to insert or remove items to make it exactly $K$. As the number of over- or under-selected items is on the order of $\sqrt{K}$, the error introduced by this step is also the order of $\frac{\sqrt{K}}{K} = K^{-1/2}$.

In the following sections, we will elaborate on the method and provide theoretical guarantees for MMD and the Wasserstein distance.








\subsection{Pretender for MMD} \label{sec: mmd}

We consider the case where we quantify the discrepancy between the distributions with MMD, defined as \begin{align}
  \text{MMD}(\mu, \nu) = \sup_{\|f\|_\mathcal{H} \le 1} \int f \, d\mu - \int f \, d\nu,
\end{align} where $\mathcal{H}$ is the RKHS with the kernel $k$. Equivalently, the MMD can be expressed as \begin{align}
  \text{MMD}(\mu, \nu)^2 &= \left\| \int \phi(x) \, d\mu(x) - \int \phi(x) \, d\nu(x) \right\|^2_\mathcal{H} \\
  &= \mathbb{E}_{x, x' \sim \mu} [k(x, x')] - 2 \mathbb{E}_{x \sim \mu, x' \sim \nu} [k(x, x')] + \mathbb{E}_{x, x' \sim \nu} [k(x, x')], \label{eq: mmd-def}
\end{align} where $\phi(x) = k(x, \cdot)$ is the feature map. Now, consider the empirical distributions \begin{align}
  \mu^\boldw = \sum_{j = 1}^{2m} \boldw_j \delta_{x_j}, \qquad
  \nu = \frac{1}{n} \sum_{j = 1}^n \delta_{x'_j}, \label{eq: empirical}
\end{align} where $x_j = (\boldx_{i_j}, y_j)$ is concatenation of the feature and label. Substituting these into Eq. (\ref{eq: mmd-def}), we obtain \begin{align}
  \text{MMD}(\mu^\boldw, \nu)^2 &= \sum_{j, j' = 1}^{2m} \boldw_j \boldw_{j'} k(x_j, x_{j'}) - 2 \sum_{j = 1}^{2m} \sum_{j' = 1}^n \boldw_j \frac{1}{n} k(x_j, x'_{j'}) + \frac{1}{n^2} \sum_{j, j' = 1}^n k(x'_j, x'_{j'}) \\
  &= \boldw^\top \boldK^{TT} \boldw - \frac{2}{n} \bold1^\top \boldK^{ST} \boldw + \frac{1}{n^2} \bold1^\top \boldK^{SS} \bold1 \\
  &= \boldw^\top \boldK^{TT} \boldw - \frac{2}{n} \bold1^\top \boldK^{ST} \boldw + \text{const.},
\end{align} where $\boldK^{TT} \in \mathbb{R}^{2m \times 2m}$, $\boldK^{ST} \in \mathbb{R}^{2m \times n}$, and $\boldK^{SS} \in \mathbb{R}^{n \times n}$ are the kernel matrices with $\boldK^{TT}_{jj'} = k(x_j, x_{j'})$, $\boldK^{ST}_{jj'} = k(x_j, x'_{j'})$, and $\boldK^{SS}_{jj'} = k(x'_j, x'_{j'})$. These matrices are positive semi-definite, and MMD$^2$ is convex in $\boldw$. Thus, the optimization problem to minimize MMD reduces to the following convex quadratic program: \begin{align}
  \begin{split}
  &\min_{\boldw \in \mathbb{R}^{2m}} \boldw^\top \boldK^{TT} \boldw - \frac{2}{n} \bold1^\top \boldK^{ST} \boldw, \\
  &\quad \text{s.t.} \quad  \sum_{j = 1}^{2m} \boldw_j = 1, \qquad 0 \le \boldw_j \le \frac{1}{K} ~(j = 1, 2, \ldots, 2m).
  \end{split}
\end{align} This problem can be solved with the Frank-Wolfe algorithm. We initialize $\boldw = \frac{\bold1}{2m}$, which is feasible. We then iteratively update $\boldw$ by the Frank-Wolfe algorithm with step size $\frac{2}{t + 2}$ for $t = 0, 1, \ldots, L - 1$. As the objective function is convex quadratic, we obtain the following guarantee.
\begin{proposition} \label{prop: mmd_optimization}
  When we run the Frank-Wolfe algorithm with the step size $\frac{2}{t + 2}$ for $t = 0, 1, \ldots, L - 1$, we obtain $\hat{\boldw}$ such that \begin{align}
    \text{MMD}(\mu_T^{\hat{\boldw}}, \mu_S) &\leq \text{OPT}^{\text{continuous}} + C L^{-1/2},
  \end{align} for some constant $C \in \mathbb{R}_+$.
\end{proposition}
\begin{proof}
  Since the objective function is quadratic, it is $\beta$-smooth. The Frank-Wolfe algorithm with a step size $\frac{2}{t + 2}$ achieves a convergence rate of $O(L^{-1})$ \cite{jaggi2013revisiting}. Given that the objective function is the squared MMD, the error in MMD is on the order of $O(L^{-1/2})$.
\end{proof}
As discussed in Section \ref{sec: optimization}, we then round the solution by selecting items with probability $K w_j$. To analyze this step, we make the following mild assumption.
\begin{assumption}[\textbf{Bounded Kernel}] \label{assumption: bounded_kernel}
  The kernel $k$ is bounded by $B$ in the sense that $k(x, x) = \langle \phi(x), \phi(x) \rangle = \|\phi(x)\|_\mathcal{H}^2 \le B$ for all $x \in \mathcal{X}$.
\end{assumption}
This assumption is mild and holds for many kernels such as the Gaussian, Laplacian, and Mat\'{e}rn kernels. We have the following guarantee for the random selection step.
\begin{proposition} \label{prop: mmd_random_selection}
  Let $\tilde{\boldw}_j = \frac{I_j}{K}$ with $I_j \sim \text{Bernoulli}(K \hat{\boldw}_j)$. Then, \begin{align}
    \left\|\sum_{j = 1}^{2m} \tilde{\boldw}_j \phi(x_j) - \sum_{j = 1}^{2m} \hat{\boldw}_j \phi(x_j)\right\|_\mathcal{H} &\leq \sqrt{\frac{B}{\delta K}}
  \end{align} with probability at least $1 - \delta$.
\end{proposition}
\begin{sketch}
  We evaluate $\mathbb{E}_I\left[\left\|\sum_j \tilde{\boldw}_j \phi(x_j) - \sum_j \hat{\boldw}_j \phi(x_j)\right\|_\mathcal{H}^2\right]$. This can be bounded by $\frac{B}{K}$ by a similar argument to Eq. \ref{eq: var_num_items}. Then we obtain the desired result by Markov's inequality. The full proof is provided in Appendix \ref{sec: proof_mmd_random_selection}.
\end{sketch}
We then analyze the postprocessing step. We have the following guarantee.
\begin{proposition} \label{prop: mmd_postprocessing}
  Let $\tilde{\boldw}'_j = \frac{1}{K}$ if the $j$-th item is in the final output $\mathcal{D}_T$ and $0$ otherwise. We have \begin{align}
    \left\|\sum_{j = 1}^{2m} \tilde{\boldw}'_j \phi(x_j) - \sum_{j = 1}^{2m} \tilde{\boldw}_j \phi(x_j)\right\|_\mathcal{H} \le \sqrt{\frac{B}{\delta K}}
  \end{align} with probability at least $1 - \delta$.
\end{proposition}
\begin{sketch}
  This also follows from a similar argument to Proposition \ref{prop: mmd_random_selection}. The proof is in Appendix \ref{sec: proof_mmd_postprocessing}.
\end{sketch}
By combining the above guarantees, we have the following guarantee for the final output.
\begin{theorem} \label{thm: mmd_sigle_shot}
  Under assumption \ref{assumption: bounded_kernel}, when we run the Frank-Wolfe algorithm with the step size $\frac{2}{t + 2}$ for $t = 0, 1, \ldots, L - 1$ and select items with the probability $K \boldw_j$, we obtain $\mathcal{D}_T$ such that \begin{align}
    \text{MMD}(\mu_T^{\mathcal{D}_T}, \mu_S) \le \text{OPT}^{\text{combinatorial}} + C L^{-1/2} + 2\sqrt{\frac{B}{\delta K}}
  \end{align} with probability at least $1 - 2\delta$.
\end{theorem}
\begin{sketch}
  This follows from the triangle inequality and union bound. The proof is in Appendix \ref{sec: proof_mmd_single_shot}.
\end{sketch}
\begin{corollary} \label{cor: mmd_final}
  Under assumption \ref{assumption: bounded_kernel}, for any $\epsilon > 0$ and $K \in \mathbb{Z}_D$, there exists $L$ and $R$ such that by running the Frank-Wolfe algorithm with $L$ iterations and repeating the rounding process $R$ times and choose the best $\mathcal{D}^*_T$ with smallest $\text{MMD}(\mu_T^{\mathcal{D}^*_T}, \mu_S)$, we have \begin{align}
    \text{MMD}(\mu_T^{\mathcal{D}^*_T}, \mu_S) &\le \text{OPT}^{\text{combinatorial}} + 2\sqrt{\frac{2B}{K}} + \epsilon \\ &= \text{OPT}^{\text{combinatorial}} + O(K^{-1/2})
  \end{align} with high probability.
\end{corollary}
\begin{proof}
  We set $L = 4 C^2 \varepsilon^{-2}$ and $\delta = \left(\frac{1}{\sqrt{2} + \frac{\varepsilon \sqrt{K}}{4\sqrt{B}}}\right)^2$, then we have \begin{align}
    \text{OPT}^{\text{combinatorial}} + C L^{-1/2} + 2\sqrt{\frac{B}{\delta K}} &\le \text{OPT}^{\text{combinatorial}} + 2\sqrt{\frac{2B}{K}} + \epsilon
  \end{align} with probability at least $1 - 2\delta > 0$ because $\delta < \frac{1}{2}$. Repeating the rounding process $R = \Theta((1 - 2\delta)^{-1})$ times, we can obtain the final output with the desired guarantee.
\end{proof}
Note that the rounding process can be de-randomized by dynamic programming and the conditional probability method.

We can also bound $\text{OPT}^{\text{combinatorial}}$ under the following assumption.
\begin{assumption}[\textbf{Bounded Density Ratio}] \label{assumption: bounded_density_ratio}
  The source items $\mathcal{D}_S$ and target candidate items $J_T$ are sampeled from distributions $P$ and $Q$, respectively, and the density ratio $r^* = \sup_x \frac{P(x)}{Q(x)}$ is bounded.
\end{assumption}
The items $\mathcal{D}_S$ and $J_T$ are not neccearily common (i.e., the first challenge), and the distributions are not necessarily identical. Instead, we assume that the target service has a chance to provide items similar to the source items with this assumption. Without this, it would be unlikely to find relevant items in the target service, making it impossible to transfer user preferences with a vanishing error. Under this assumption, we establish the following guarantee.
\begin{theorem} \label{thm: mmd_opt_combinatorial}
  Under Assumptions \ref{assumption: bounded_kernel} and \ref{assumption: bounded_density_ratio}, there exists $C_1, C_2 \in \mathbb{R}_+$ such that when $K \le C_1 \frac{|J_T|}{r^*}$, \begin{align}
    \text{MMD}(P, \mu^{\mathcal{D}_T}) \le C_2 \left(2 \sqrt{\frac{B}{|\mathcal{D}_S|}} + \sqrt{\frac{B}{K}}\right) \quad \text{and} \quad \text{OPT}^{\text{combinatorial}} \le C_2 \left(\sqrt{\frac{B}{|\mathcal{D}_S|}} + \sqrt{\frac{B}{K}}\right)
  \end{align} with high probability.
\end{theorem}
\begin{sketch}
  Under Assumption \ref{assumption: bounded_density_ratio}, $\frac{1}{r^*} P \le Q$ and $Q$ can be written as $\frac{1}{r^*} P + (1 - \frac{1}{r^*}) Q'$ for some distribution $P'$. We essentially have $\frac{|J_T|}{r^*}$ samples from $P$ in the target service. We bound the convergence rate of the empirical measure to the true measure by a standard argument of MMD. The proof is in Appendix \ref{sec: proof_mmd_opt_combinatorial}.
\end{sketch}
By combining the above guarantees, we have the following guarantee for the final output.
\begin{corollary}
  Under the assumptions of Corollary \ref{cor: mmd_final} and Theorem \ref{thm: mmd_opt_combinatorial}, we can obtain $\mathcal{D}_T$ such that \begin{align}
    \text{MMD}(\mu_T^{\mathcal{D}^*_T}, \mu_S) &\le O(|\mathcal{D}_S|^{-1/2} + K^{-1/2})
  \end{align} with high probability.
\end{corollary}
Therefore, if we prepare a sufficient number of source items, the target service provides a sufficiently rich set of items, and we click sufficiently many items, we can transfer the preference to the target service with a vanishing error in the order of $O(|\mathcal{D}_S|^{-1/2} + K^{-1/2})$.

With the same argument as in Section \ref{sec: formulation}, we can bound the error on the source items.
\begin{corollary} \label{cor: mmd_loss_bound}
  If the target service uses an unknown model $f_T(\cdot; \theta)$ and unknown loss function $\ell_T(\cdot, \cdot)$ to train the model, and the RKHS norm of loss $\ell_T(f_T(\boldx; \theta), y)$ is bounded by $R_\ell$, then the trained model incurs the error \begin{align}
    \mathbb{E}_{P}[\ell_T(f_T(\boldx; \theta), y)] &\le \frac{1}{|\mathcal{D}_T|} \sum_{(\boldx, y) \in \mathcal{D}_T} \ell_T(f_T(\boldx; \theta), y) + R_\ell \cdot \text{MMD}(P, \mathcal{D}_T).
  \end{align} Under the assumptions of Theorem \ref{thm: mmd_opt_combinatorial}, we have \begin{align}
    \mathbb{E}_{P}[\ell_T(f_T(\boldx; \theta), y)] &\le \frac{1}{|\mathcal{D}_T|} \sum_{(\boldx, y) \in \mathcal{D}_T} \ell_T(f_T(\boldx; \theta), y) + O(|\mathcal{D}_S|^{-1/2} + K^{-1/2}). \label{eq: mmd_final_cor}
  \end{align}
\end{corollary}
This corollary shows that if the target service trains the model sufficiently well so that the training error $\frac{1}{|\mathcal{D}_T|} \sum_{(\boldx, y) \in \mathcal{D}_T} \ell_T(f_T(\boldx; \theta), y)$ is small and if we prepare a sufficient number of source items and click sufficiently many items following \textsc{Predenter}, we can make the right hand side of Eq. \ref{eq: mmd_final_cor} arbitrarily small and we can ensure that the trained model incurs a small error on the source items, meaning that it effectively captures the userâ€™s preferences from the source service.







\subsection{Pretender for the Wasserstein Distance} \label{sec: wasserstein}

We now consider the case where we quantify the discrepancy between the distributions with the $1$-Wasserstein distance. Consider the empirical distributions $\mu_T^{\boldw}$ and $\mu_S$ defined in Eq. \ref{eq: empirical}. The $1$-Wasserstein distance is \begin{align}
  W_1(\mu^\boldw, \nu) &= \inf_{\boldgamma \in \Pi(\mu^\boldw, \nu)} \sum_{j, j'} \boldgamma_{jj'} \|x_j - x'_{j'}\| \label{eq: primal_wasserstein}
\end{align} where \begin{align}
  \Pi(\mu^\boldw, \nu) = \left\{\boldgamma \in \mathbb{R}^{2m \times n}_{\ge 0} \middle| \boldgamma \bold1 = \boldw, \boldgamma^\top \bold1 = \frac{\bold1}{n} \right\}
\end{align} is the set of coupling matrices. The $1$-Wasserstein distance also admits the following dual formulation: \begin{align}
  W_1(\mu^\boldw, \nu) &= \sup_{f \in \mathcal{F}_\text{Lip}} \sum_{j = 1}^{2m} \boldw_j f(x_j) - \frac{1}{n} \sum_{j = 1}^n f(x'_j),
\end{align} where $\mathcal{F}_\text{Lip}$ is the set of $1$-Lipschitz functions. We solve the following convex optimization problem: \begin{align}
  \begin{split}
  &\min_{\boldw \in \mathbb{R}^{2m}}W_1(\mu^\boldw, \nu), \\
  &\quad \text{s.t.} \quad \sum_{j = 1}^{2m} \boldw_j = 1, \qquad 0 \le \boldw_j \le \frac{1}{K} ~(j = 1, 2, \ldots, 2m).
  \end{split}
\end{align} By substituting the primal formulation (Eq. \ref{eq: primal_wasserstein}) and the definition of $\Pi(\mu^\boldw, \nu)$, we have \begin{align}
  \begin{split}
  &\min_{\boldw \in \mathbb{R}^{2m}, \boldgamma \in \mathbb{R}^{2m \times n}} \sum_{j, j'} \boldgamma_{jj'} \|x_j - x'_{j'}\|, \\
  &\quad \text{s.t.} \quad \boldw^\top \bold1 = 1, \qquad \boldgamma \bold1 = \boldw, \qquad \boldgamma^\top \bold1 = \frac{\bold1}{n} \\
  &\quad  \qquad 0 \le \boldw_j \le \frac{1}{K} ~(j = 1, 2, \ldots, 2m), \qquad \boldgamma_{jj'} \ge 0 ~(j = 1, 2, \ldots, 2m, j' = 1, 2, \ldots, n).
  \end{split}
\end{align} This problem is a linear program with optimization variables $\boldw$ and $\boldgamma$. We can solve this problem by a linear program solver in a polynomial time. Let $\hat{\boldw}$ be the solution of this problem. We then select items with the probability $K \hat{\boldw}_j$ and analyze the rounding process. To analyze this step, we make the following mild assumption.
\begin{assumption}[Compact Domain] \label{assumption: compact_domain}
  Each element $x_j$ lies is the unit cube $[0, 1]^d$ and the cost function $\|x - x'\|$ is the Euclidean distance.
\end{assumption}
If the data are not originally in the unit cube, they can be standardized to satisfy this assumption. To bound the Wasserstein distance, we need the following lemma, which is shown in Appendix \ref{sec: proof_covering}.
\begin{lemma} \label{lem: covering}
  Let $\mathcal{F}_\text{Lip}$ be the set of $1$-Lipschitz functions with $f(\frac{\bold1}{2}) = 0$. $\mathcal{F}_\text{Lip}$ can be $L_\infty$-covered by $\tilde{\mathcal{F}}_\varepsilon$,  i.e., for any $f \in \mathcal{F}_\text{Lip}$, there exists $\tilde{f} \in \tilde{\mathcal{F}}_\varepsilon$ such that $\|f - \tilde{f}\|_\infty \le \varepsilon$, with $|f(x)| \le \frac{\sqrt{d}}{2}, \forall f \in \tilde{\mathcal{F}}_\varepsilon$ and $|\tilde{\mathcal{F}}_\varepsilon| \le \exp\left(\log (3) \left(\frac{2 \sqrt{d}}{\varepsilon}\right)^d\right)$.
\end{lemma}
We first analyze the postprocessing step. We have the following guarantee.
\begin{proposition} \label{prop: wasserstein_postprocessing}
  Let $\tilde{\boldw}'_j = \frac{1}{K}$ if the $j$-th item is in the final output $\mathcal{D}_T$ and $0$ otherwise. For any $\varepsilon > 0$, \begin{align}
    \sup_{f \in \tilde{\mathcal{F}}_\varepsilon} \sum_{j = 1}^{2m} f(x_j) (\tilde{\boldw}'_j - \tilde{\boldw}_j) \le \sqrt{\frac{d}{2K} \log \frac{1}{\delta}} + \frac{\sqrt{d}}{3K} \log \frac{1}{\delta} \label{eq: wasserstein_postprocessing}
  \end{align} with probability at least $1 - \delta$.
\end{proposition}
\begin{sketch}
  The proof is similar to Proposition \ref{prop: mmd_postprocessing}, and we obtain the bound with Bernstein's inequality. The full proof is in Appendix \ref{sec: proof_wasserstein_postprocessing}.
\end{sketch}
We then analyze the random selection step. We first bound each test function.
\begin{lemma} \label{prop: wasserstein_random_selection}
  Let $\tilde{\boldw}_j = \frac{I_j}{K}$ with $I_j \sim \text{Bernoulli}(K \hat{\boldw}_j)$. Then, for any $f$ with $|f(x)| \le \frac{\sqrt{d}}{2}$, \begin{align}
    \sum_{j = 1}^{2m} f(x_j) (\tilde{\boldw}_j - \hat{\boldw}_j) \le \sqrt{\frac{d}{2K} \log \frac{1}{\delta}} + \frac{\sqrt{d}}{3K} \log \frac{1}{\delta}
  \end{align} with probability at least $1 - \delta$.
\end{lemma}
\begin{sketch}
  We bound the variance of independent random variables $f(x_j) (\tilde{\boldw}_j - \hat{\boldw}_j) ~ (j = 1, 2, \ldots, 2m)$ and apply Bernstein's inequality. The full proof is in Appendix \ref{sec: proof_wasserstein_random_selection}.
\end{sketch}
By combining Proposition \ref{prop: wasserstein_postprocessing}, Lemma \ref{prop: wasserstein_random_selection}, and Lemma \ref{lem: covering}, we have the following guarantee.
\begin{theorem} \label{thm: wasserstein1}
  For any $\delta > 0$, the $1$-Wasserstein distance between the final output $\mathcal{D}_T$ and the source distribution is \begin{align}
    W_1(\mu_T^{\mathcal{D}_T}, \mu_S) &\le \text{OPT}^{\text{combinatorial}} + 3 \sqrt{\frac{d}{K} \log \frac{1}{\delta}} + \frac{\sqrt{d}}{K} \log \frac{1}{\delta} + 8 \sqrt{d} K^{-\frac{1}{d+2}} + 11 \sqrt{d} K^{-\frac{2}{d+2}} + 6\sqrt{d \log \frac{1}{\delta}} K^{-\frac{1}{d+2}-\frac{1}{2}} \\
    & = \text{OPT}^{\text{combinatorial}} + O\left(K^{-\frac{1}{d+2}}\right)
  \end{align} with probability at least $1 - 2\delta$.
\end{theorem}
\begin{sketch}
  We set $\varepsilon = \sqrt{d} \left(2^{\frac{d-2}{d+2}} \log^{\frac{1}{d+2}}(3) K^{-\frac{1}{d+2}} + \frac{1}{2} \sqrt{\frac{1}{K} \log \frac{1}{\delta}}\right)$ in Lemma \ref{lem: covering}. We bound $|\tilde{\mathcal{F}}_\varepsilon|$, set $\delta \leftarrow \delta / |\tilde{\mathcal{F}}_\varepsilon|$ in Proposition \ref{prop: wasserstein_random_selection}, and apply the union bound. We then apply Proposition \ref{prop: wasserstein_postprocessing}, Lemma \ref{prop: wasserstein_random_selection}, Lemma \ref{lem: covering}, and the Kantrovich-Rubinstein duality to obtain the desired result. The full proof is in Appendix \ref{sec: proof_wasserstein1}.
\end{sketch}
We can also bound the error of the trained model on the source items by the same argument as in Theorem \ref{thm: mmd_opt_combinatorial} and Corollary \ref{cor: mmd_loss_bound}. The difference is that the sample complexity of the Wasserstein distance is $\Theta(\frac{1}{\varepsilon^d})$ \cite{dudley1969speed} instead of $\Theta(\frac{1}{\varepsilon^2})$ of the MMD. This is a fundamental property of the Wasserstein distance. Consequently, the optimal value $\text{OPT}^{\text{combinatorial}}$ and the loss bound scale as $\Theta(\frac{1}{\varepsilon^d})$ in the case of the Wasserstein distance, compared to $\Theta(\frac{1}{\varepsilon^2})$ for the MMD. This difference implies that the error bound can be significantly larger when the data dimension $d$ is high, making the Wasserstein distance less practical for high-dimensional settings.

\section{Discussions}

\subsection{Discussion on Variants}

We have presented the variants for MMD and the Wasserstein distance. Each has its own advantages and disadvantages. The primary advantage of MMD is its sample complexity. MMD is not affected by the number of dimensions of the feature space and sidesteps the curse of dimensionality while the Wasserstein distance suffers from it \cite{dudley1969speed}. The advantage of the Wasserstein distance is its generality. The assumption that the loss function $\ell_T(f_T(\boldx; \theta), y)$ is Lipschitz continuous is very mild and holds in many settings, such as logistic regression, matrix factorization, and neural networks. MMD requires the loss is in the RKHS, which may not hold in practice.

We also note that our analysis can be extended to other metrics as longs as the continuous optimization problem (Eq. \ref{eq: continuous_optimization}) is tractable and the convering number of the test function class $\mathcal{F}$ is bounded by the same argument of Section \ref{sec: wasserstein}. For example, our analysis can be applied to the discrepancy distance \cite{mansour2009domain}, yielding a sharp generalization bound when the model is linear and the loss is the squared loss. Practitioners can choose metrics that best suits their specific application.

\subsection{Optimum Value is Not Monotone} \label{sec: monotone}

It should be noted that the optimum value of the combinatorial problem is not monotonic in $K$.
\begin{proposition}
  $\text{OPT}^{\text{combinatorial}}$ is not monotonic in $K$.
\end{proposition}
\begin{proof}
  We prove the proposition by a counterexample. Let $I_S = \{1\}, I_T = \{1, 2\}, \boldx_1 = 0, \boldx_2 = 1$, and $\mathcal{D}_S = \{(1, 1)\}$. When $K = 1$, the optimum value is $0$ by selecting $\mathcal{D}_T = \{(1, 1)\}$. When $K = 2$, the optimum value is non-zero because we need to select items other than $(1, 1)$, and $\mathcal{D}_S \neq \mathcal{D}_T$.
\end{proof}
One might intuitively expect that increasing $K$ would lead to a better or at least no worse solution. However, the above proposition shows that this is not always the case. In many practical situations, the goal is not to click exactly $K$ items, but rather to minimize the number of interactions while effectively transferring preferences. To address this issue, we can parallelly run the algorithm with $K' = 1, 2, \ldots, K$ and select the best solution with smallest $\mathcal{D}(\mu_T, \mu_S)$ among them.

\subsection{Limitation: Inconsistent Features}

We have assumed that we have access to the features $\boldx_i$ of the items $i$ in the problem setting. In practice, rich feature representations may not always be available, or the target service may utilize a different feature space than what we expect for training its recommendation model. This issue is particularly relevant when the target service employs collaborative filtering, a widely used approach in recommender systems. In such cases, the service may not use explicit features such as text, tags, or visual information, but use collaborative features, which are not available to end users. In such a case, our approach and analysis cannot be directly applied. Nevertheless, we argue that our approach remains valuable even in such scenarios. First, explicit features often serve as good surrogates for implicit features. For example, movies with similar descriptions tend to attract similar audiences. Second, recent studies have demonstrated that even end users can estimate implicit features from recommendation networks \cite{sato2022towards}. These estimated features can then be fed into our approach. Extending our analysis to cases where feature spaces are inconsistent is an important direction for future work.



\section{Related Work}

\subsection{Cold Start Problem}

The cold start problem is a fundamental problem in recommender systems. It arises when a new user (or a new item) enters the system, and the system lacks sufficient information to provide accurate recommendations. Although many methods have been proposed to address the cold start problem \cite{park2006naive, gantner2010learning, lika2014facing, wang2019enhansing}, all of them require the service providers to implement the method. In contrast, our approach is unique in that it can be applied directly by an end user without requiring any modifications to the service itself. This characteristic broadens the applicability of our method, allowing it to be used in services that lack built-in functionalities to address the cold start problem.


\subsection{Quadrature}

Quadrature is a technique to approximate the integral of a function by summing the function values at a finite number of points \cite{rasmussen2002bayesian, huszar2012optimally, harvey2014near, equivalence2017bach, hayakawa2022positively}. This is essentially equivalent to finding a discrete measure that approximates the given measure. Quadrature is widely used in numerical analysis and machine learning. One of the common applications is coresets \cite{karnin2019discrepancy, sener2018active, campbell2019automated, mirzasoleiman2020coresets}, which is a small set of training points that approximates the loss of the model on the entire training set. Our approach can be seen as a quadrature of $\int \ell_T \, d\mu_S$ with points $\mathcal{D}_T$. The main difference is that standard quadrature methods use arbitrary points and/or weights and sidestep combinatorial problems \cite{back2012equivalence, briol2015frank}, while our approach uses only the items in the target set, which naturally lead to the combinatorial optimization problem. For example, methods based on the Frank-Wolfe algorithm \cite{welling2009herding, chen2010super, back2012equivalence} output sparse weights, but they may choose the same items repeatedly in general, and the resulting weights are not uniform. Such output cannot be applied to our setting because users cannot thumbs up the same item multiple times in most services. Some approaches \cite{karnin2019discrepancy, dwivedi2024kernel} such as Kernel thinning \cite{dwivedi2024kernel} output a subset of the input points with uniform weights, which is similar to our approach. However, these methods assume that the candidate points are sampled according to the distribution being approximated. If this assumption does not hold, as in our case, these methods cannot be directly applied. Our proposed method can be used even when the input points are arbitrary. Other methods \cite{wei2013using, wei2015submodularity} employ submodular optimization and greedy algorithms, achieving a $(1 - \frac{1}{e})$-approximation ratio. However, the gap of $(1 - \frac{1}{e})$ does not vanish as the number of items increases. By contrast, our approach can achieve the vanishing error as Corollary \ref{cor: mmd_loss_bound} shows thanks to the continuous optimization approach and the careful rouding process. To the best of our knowledge, our work is the first to provide a theoretical guarantee for such a general and combinatorial setting. We believe that this result is of independent interest.

\subsection{User-side Realization}

User-side realization refers to the concept in which end users implement desired functionalities on their own without requiring modifications to the service itself. Many users experience dissatisfaction with services. Even if they want some functinoalities and request them to the service provider, the provider may not implement them due to various reasons such as cost, complexity, and simple negligence. After all, service providers are not volunteers but businesses. In such cases, the only options users have are not satisfactory, keep using the service despite their dissatisfaction or leave the service. User-side realization provides a proactive alternative to this dilemma. This concept has been explored in various fields such as recommender systems \cite{sato2022private, sato2022towards, sato2024overhead}, search engines \cite{sato2022retreiving, sato2022clear, diligenti2000focused, radlinski2006improving, nakano2021webgpt}, and privacy \cite{sato2024making}. The main advantage of the user-side realization is that it can be used in services that do not have special functionalities to address the problem, and it broadens the scope of the applicability of the solution. For a more comprehensive discussion on user-side realization, we refer the reader to the Ph.D. thesis by \cite{sato2024user}. Our approach constitutes a novel application of user-side realization to the cold start problem in recommender systems.

\section{Experiments}

\subsection{Convergence Analysis} \label{sec: convergence_experiments}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{imgs/combined_plot.pdf}
  \caption{MMD as a function of the number of selected items $K$. The optimal value of the combinatorial optimization problem is intractable but is guaranteed to lie somewhere between the red and blue lines. We can see that the difference between the proposed method and the optimal value decreases as $K$ increases. As we discussed in Section \ref{sec: monotone}, the optimal value is not monotonic in $K$, and the sweet spots lie around $32$ to $64$ in both cases. We can also see that the optimal values get worse more quickly in (a) because the source data contain fewer items, the relevant items in the target set are exhausted more quickly, and we are forced to select less relevant items when we increase $K$.}
  \label{fig: convergence}
\end{figure}

We first confirm that the regret of \textsc{Pretender} converges to zero as the number $K$ of selected items increases.

\textbf{Experimental Setting.} We use the MovieLens 100K dataset \cite{harper2016movielens}. We consider rating $\ge 4$ as positive feedback (i.e., thumbs up) and rating $< 4$ as negative feedback (i.e., thumbs down). We create two virtual services $S$ and $T$. We include each movie in $S$ with probability $0.5$. We carry out the same process independently to create $T$. As a result, these services share about half of the movies. We use genres (e.g., Action, Adventure, Animation) and the release year as the features of the movies. They are encoded as a $90$-dimensional multi-hot vector. We concatenate $Cy \in \{0, C\}$ to the feature vector to define the discrete distribution, where $y \in \{0, 1\}$ indicates thumbs up or thumbs down, and $C = 10$ is the hyperparameter that controls the emphasis on the label when we measure the distance between two data points. We focus on user 308, who is the first user who has $10$ thumbs up and $10$ thumbs down, and user 21, who is the first user who has more than $100$ ratings. For each user $u$, we aim to transfer the preference $\mathcal{D}_u \cap \mathcal{D}_S$ by choosing $K$ items from $\mathcal{D}_T$. We use MMD with the Gaussian kernel and bandwidth $\sigma = 1$ as the distance measure. We use $L = 1000$ iterations of the Frank-Wolfe algorithm and $R = 100$ trials of the rounding process. We set $K = 1, 2, 4, 8, \ldots, 128$ to see the convergence behavior. 

The goal of the experiment is to confirm that the regret of the proposed algorithm, i.e., the MMD of the proposed method minus the MMD of the optimal solution, converges to zero as the number of selected items $K$ increases. However, computing the true optimal solution is intractable due to the combinatorial nature of the optimization problem. Instead, we use the optimal value of the continuous relaxation as a benchmark, which serves as a provable lower bound for the optimal value of the combinatorial optimization problem. 

\textbf{Results.} Figure \ref{fig: convergence} shows the regret of \textsc{Pretender} as a function of the number of selected items $K$. The optimal value of the combinatorial optimization problem is intractable but is guaranteed to lie somewhere between the red and blue lines. We observe that the difference between the proposed method and the optimal value decreases as $K$ increases. As we discussed in Section \ref{sec: monotone}, the optimal value is not monotonic in $K$, and the sweet spots lie around $32$ to $64$ in both cases. We can also see that the optimal values get worse more quickly in (a) because the source preferences contain fewer items, and the relevant items in the target set are exhausted more quickly. Consequently, when $K$ increases, the algorithm is forced to select less relevant items in (a).

\subsection{Quantitative Evaluation} \label{sec: quantitative_experiments}

\begin{figure*}[t]
  \captionsetup{labelformat=empty}
  \caption{Table 1. Performance Comparison. Each value represents the average MMD across all users. Lower is better. The standard deviation is computed across users. The proposed method is much better than the baseline methods and is close to the optimal continuous solution.}
  \vspace{-0.1in}
  \centering
  \includegraphics[width=0.9\linewidth]{imgs/table.pdf}
  \vspace{-0.2in}
  \label{tab: quantitative}
\end{figure*}
\addtocounter{table}{1}
\addtocounter{figure}{-1}

\textbf{Datasets.} We evaluate our method using datasets from three domains: MovieLens 100k (movies), Last.fm (music) \cite{cantador2011hetrec}, and Amazon-Home-and-Kitchen (e-commerce) \cite{he2016ups, mcauley2015image}. We use the same setting as the convergence analysis for the MovieLens 100K dataset except that we use all users. For the Last.fm dataset, we consider musicians with positive weights as thumbs up and we sample the same number of random musicians for thumbs down. We use the tags (e.g., metal, 80's, chillout) as the features of the artists. As the number of tags is large, we apply the principal component analysis (PCA) to reduce the dimension to $50$ and standardize the variance. For the Amazon-Home-and-Kitchen dataset, we consider ratings $\ge 4$ as thumbs up and ratings $< 4$ as thumbs down. We use the bag-of-words representation of the product reviews. We apply PCA to reduce the dimension to $50$ and standardize the variance. We consider two different settings for defining source and target services. The first setting, with intersection, is identical to the one used in the convergence analysis. The second setting, no intersection, is that we split the items into source and target services without any intersection. This setting is more challenging because the methods cannot copy any source items to the target service. For Last.fm and Amazon-Home-and-Kitchen datasets, we use the Gaussian kernel with the bandwidth $\sigma = 10$ to reflect the scale of the features. We set $K = 100$ for all datasets.

\textbf{Baselines.} Since there are no existing methods that solve the cold start problem by an end user, we use two simple baselines to validate that the proposed method is reasonablly effective. The first baseline is random selection, which randomly selects $K$ items from $\mathcal{D}_T$. The second baseline is the greedy selection, which selects $K$ items in $J_T$ that have the smallest distance $d(x, \mathcal{D}_S) = \min_{x' \in \mathcal{D}_S} \|x - x'\|_2$ to the source items $\mathcal{D}_S$. Additionally, we report the performance of the optimal solution of the continuous optimization problem.

\textbf{Results.} Table \ref{tab: quantitative} shows MMD values between the source preference and the output of the methods. Lower is better. \textsc{Pretender} is much better than the baseline methods and is close to the optimal continuous solution. Note that checking the closeness is useful in practice as well. The continuous solution is obtained as a byproduct of \textsc{Pretender}. After we obtain the final output, we can verify that the final solution is indeed a reasonable one just by comparing the two values. We observe that the greedy method is worse than the random method. We hypothesis that it is because the greedy method chooses items that are too similar to some source items and the diversity is lost. We also observe that \textsc{Pretender} is robust to the setting with no intersection. The performance is slightly worse than in the setting with intersection but is still reasonablly well.

\subsection{Case Study} \label{sec: case_study}

\begin{table}[tb]
  \caption{\textbf{Case Study.} The source items the user interacted and the $K = 20$ target items selected by \textsc{Pretender}. \textsc{Pretender} selects the very items that the user interacted with in the source service as shown in \textcolor[HTML]{ff4b00}{red}. The user interacted with movies in 1996 and 1997, and \textsc{Pretender} mostly selects movies in 1996 and 1997. Notable exceptions are Alice in Wonderland (1951) and Fantasia (1940) selected by \textsc{Pretender}, as shown in \textcolor[HTML]{005aff}{blue}. We hypothesis that this reflects the fact that the user thumbed down Cats Donâ€™t Dance (1997), which is an animated movie for children. Since Cats Don't Dance (1997) is not in the source service, \textsc{Pretender} selects similar movies from the target service and sends a signal that the user does not like this type of movies.}
  \label{tab: comparison}
  \centering
  \scalebox{0.8}{%
  \begin{tabular}{llll}
    \hline
    \textbf{Source Thumbs Up} & \textbf{Source Thumbs Down} & \textbf{Pretender Thumbs Up} & \textbf{Pretender Thumbs Down} \\
    \hline
    Kolya (1996) & Ulee's Gold (1997) & Shall We Dance? (1996) & Absolute Power (1997) \\
    \textcolor[HTML]{ff4b00}{The English Patient (1996)} & \textcolor[HTML]{ff4b00}{Fly Away Home (1996)} & \textcolor[HTML]{ff4b00}{The English Patient (1996)} & Breakdown (1997) \\
    \textcolor[HTML]{ff4b00}{G.I. Jane (1997)} & Mrs. Brown (1997) & \textcolor[HTML]{ff4b00}{G.I. Jane (1997)} & \textcolor[HTML]{ff4b00}{Fly Away Home (1996)} \\
    \textcolor[HTML]{ff4b00}{The Edge (1997)} & \textcolor[HTML]{ff4b00}{Lost Highway (1997)} & \textcolor[HTML]{ff4b00}{The Edge (1997)} & \textcolor[HTML]{ff4b00}{Lost Highway (1997)} \\
    A Smile Like Yours (1997) & The Game (1997) & Das Boot (1981) & \textcolor[HTML]{005aff}{Alice in Wonderland (1951)} \\
     & \textcolor[HTML]{ff4b00}{Seven Years in Tibet (1997)} & Addicted to Love (1997) & \textcolor[HTML]{005aff}{Fantasia (1940)} \\
     & \textcolor[HTML]{005aff}{Cats Don't Dance (1997)} & The Pest (1997) & The Jackal (1997) \\
     & Stag (1997) &  & \textcolor[HTML]{ff4b00}{Seven Years in Tibet (1997)} \\
     &  &  & Paradise Road (1997) \\
     &  &  & 'Til There Was You (1997) \\
     &  &  & Le Bonheur (1965) \\
     &  &  & All Over Me (1997) \\
     &  &  & Rough Magic (1995) \\
    \hline
    \end{tabular}    
  }
\end{table}

As a case study, we use  the MovieLens dataset and focus on user 308, who is the first user who has $10$ thumbs up and $10$ thumbs down. We investigate the target items selected by \textsc{Pretender} under the same experimental setup as Section \ref{sec: convergence_experiments} and with $K = 20$. Table \ref{tab: comparison} shows the source items the user interacted with and the target items selected by \textsc{Pretender}. Overall, \textsc{Pretender} suceeds in transferring the user's preferences. \textsc{Pretender} selects several items that the user previously interacted with in the source service, as highlighted in \textcolor[HTML]{ff4b00}{red}. Note that the virtual services $S$ and $T$ share some items but not all items, and thus we cannot copy all items. We can also see that the user interacted with movies in 1996 and 1997, and \textsc{Pretender} mostly selects movies from the same period. This leads to similar preferences between the source and target services. Notable exceptions are Alice in Wonderland (1951) and Fantasia (1940) selected by \textsc{Pretender}, as shown in \textcolor[HTML]{005aff}{blue}. We hypothesis that this reflects the fact that the user thumbed down Cats Don't Dance (1997), which is an animated movie for children. Since Cats Don't Dance (1997) is not in the target service, \textsc{Pretender} selects similar movies from the target service and sends a signal that the user does not like this type of movies to the terget service. This case study illustrates that \textsc{Pretender} effectively reflects the user's preferences.


\section{Conclusion}

In this paper, we made the following contributions.

\begin{itemize}
  \item We proposed a new problem setting, solving the cold start problem on the user's side (Section \ref{sec: problem_setting}).
  \item We proposed \textsc{Pretender} to solve the problem by formulating it as a metric minimization between distributions (Sections \ref{sec: formulation} -- \ref{sec: optimization}).
  \item We formally showed that the regret of \textsc{Predenter} converges to zero as the number of selected items increases (Sections \ref{sec: mmd} and \ref{sec: wasserstein}). \begin{itemize}
    \item We showed that the regret converges at a rate of $O(\frac{1}{\sqrt{K}})$ for MMD (Corollary \ref{cor: mmd_final}) and $O(K^{\frac{1}{d+2}})$ for the Wasserstein distance (Theorem \ref{thm: wasserstein1}).
    \item We also showed that the optimal value also converges to zero as the number of source items and the number of selected items increases (Corollary \ref{cor: mmd_loss_bound}).
  \end{itemize}
  \item We empirically validated the effectiveness of \textsc{Pretender} through experiments (Sections \ref{sec: convergence_experiments} -- \ref{sec: case_study}). \begin{itemize}
    \item We confirmed that the regret of the proposed algorithm converges to zero with a reasonable number of selected items (Section \ref{sec: convergence_experiments}).
    \item We showed that the proposed method is much better than baseline methods and is close to the optimal continuous solution with movie, music, and e-commerce datasets (Section \ref{sec: quantitative_experiments}).
    \item We presented a case study, illustrating that the items selected by \textsc{Pretender} effectively reflect the user's preferences (Section \ref{sec: case_study}).
  \end{itemize}
\end{itemize}

We believe that this work introduces a new research direction for addressing the cold start problem from the users' perspective. We hope that our work will inspire further research in this area.

\subsection*{Acknowledgements}

We thank Yuki Takezawa for helpful discussions. 

\bibliography{main}
\bibliographystyle{abbrvnat}

\appendix


\section{Proof of Proposition \ref{prop: mmd_random_selection}} \label{sec: proof_mmd_random_selection}


\begin{proposition}
  Let $\tilde{\boldw}_j = \frac{I_j}{K}$ with $I_j \sim \text{Bernoulli}(K \hat{\boldw}_j)$. Then, \begin{align}
    \left\|\sum_{j = 1}^{2m} \tilde{\boldw}_j \phi(x_j) - \sum_{j = 1}^{2m} \hat{\boldw}_j \phi(x_j)\right\|_\mathcal{H} &\leq \sqrt{\frac{B}{\delta K}}
  \end{align} with probability at least $1 - \delta$.
\end{proposition}

\begin{proof}
  \begin{align}
    &\mathbb{E}_I\left[\left\|\sum_j \tilde{\boldw}_j \phi(x_j) - \sum_j \hat{\boldw}_j \phi(x_j)\right\|_\mathcal{H}^2\right] \\ 
    &= \sum_{j, j'} \mathbb{E}_I[\tilde{\boldw}_j \tilde{\boldw}_{j'}] \langle \phi(x_j), \phi(x_{j'}) \rangle - 2 \sum_{j, j'} \mathbb{E}_I[\tilde{\boldw}_j \hat{\boldw}_{j'}] \langle \phi(x_j), \phi(x_{j'}) \rangle + \sum_{j, j'} \mathbb{E}_I[\hat{\boldw}_j \hat{\boldw}_{j'}] \langle \phi(x_j), \phi(x_{j'}) \rangle \\
    &\stackrel{\text{(a)}}{=} \sum_{j, j'} \mathbb{E}_I\left[\frac{I_j}{K} \frac{I_{j'}}{K}\right] \langle \phi(x_j), \phi(x_{j'}) \rangle - 2 \sum_{j, j'} \mathbb{E}_I\left[\frac{I_j}{K} \hat{\boldw}_{j'}\right] \langle \phi(x_j), \phi(x_{j'}) \rangle + \sum_{j, j'} \mathbb{E}_I[\hat{\boldw}_j \hat{\boldw}_{j'}] \langle \phi(x_j), \phi(x_{j'}) \rangle \\
    &\stackrel{\text{(b)}}{=}  \frac{1}{K^2} \sum_{j, j'} \mathbb{E}_I[I_j I_{j'}] \langle \phi(x_j), \phi(x_{j'}) \rangle - 2 \sum_{j, j'} \frac{1}{K} \mathbb{E}_I[I_j] \hat{\boldw}_{j'} \langle \phi(x_j), \phi(x_{j'}) \rangle + \sum_{j, j'} \hat{\boldw}_j \hat{\boldw}_{j'} \langle \phi(x_j), \phi(x_{j'}) \rangle \\
    &\stackrel{\text{(c)}}{=} \frac{1}{K^2} \sum_{j, j'} \mathbb{E}_I[I_j I_{j'}] \langle \phi(x_j), \phi(x_{j'}) \rangle - 2 \sum_{j, j'} \hat{\boldw}_j \hat{\boldw}_{j'} \langle \phi(x_j), \phi(x_{j'}) \rangle + \sum_{j, j'} \hat{\boldw}_j \hat{\boldw}_{j'} \langle \phi(x_j), \phi(x_{j'}) \rangle \\
    &= \frac{1}{K^2} \sum_{j, j'} \mathbb{E}_I[I_j I_{j'}] \langle \phi(x_j), \phi(x_{j'}) \rangle - \sum_{j, j'} \hat{\boldw}_j \hat{\boldw}_{j'} \langle \phi(x_j), \phi(x_{j'}) \rangle \\
    &= \frac{1}{K^2} \sum_j \mathbb{E}_I[I_j^2] \langle \phi(x_j), \phi(x_j) \rangle + \frac{1}{K^2} \sum_{j \neq j'} \mathbb{E}_I[I_j I_{j'}] \langle \phi(x_j), \phi(x_{j'}) \rangle - \sum_{j, j'} \hat{\boldw}_j \hat{\boldw}_{j'} \langle \phi(x_j), \phi(x_{j'}) \rangle \\
    &\stackrel{\text{(d)}}{=} \frac{1}{K^2} \sum_j \mathbb{E}_I[I_j] \langle \phi(x_j), \phi(x_j) \rangle + \frac{1}{K^2} \sum_{j \neq j'} \mathbb{E}_I[I_j] \mathbb{E}_I[I_{j'}] \langle \phi(x_j), \phi(x_{j'}) \rangle - \sum_{j, j'} \hat{\boldw}_j \hat{\boldw}_{j'} \langle \phi(x_j), \phi(x_{j'}) \rangle \\
    &\stackrel{\text{(e)}}{=} \frac{1}{K^2} \sum_j \mathbb{E}_I[I_j] \langle \phi(x_j), \phi(x_j) \rangle + \sum_{j \neq j'} \hat{\boldw}_j \hat{\boldw}_{j'} \langle \phi(x_j), \phi(x_{j'}) \rangle - \sum_{j, j'} \hat{\boldw}_j \hat{\boldw}_{j'} \langle \phi(x_j), \phi(x_{j'}) \rangle \\
    &= \frac{1}{K^2} \sum_j \mathbb{E}_I[I_j] \langle \phi(x_j), \phi(x_j) \rangle - \sum_j \hat{\boldw}_j^2 \langle \phi(x_j), \phi(x_j) \rangle \\
    &\stackrel{\text{(f)}}{=} \frac{1}{K} \sum_j \hat{\boldw}_j \langle \phi(x_j), \phi(x_j) \rangle - \sum_j \hat{\boldw}_j^2 \langle \phi(x_j), \phi(x_j) \rangle \\
    &= \frac{1}{K} \sum_j \hat{\boldw}_j \|\phi(x_j)\|_\mathcal{H}^2 - \sum_j \hat{\boldw}_j^2 \|\phi(x_j)\|_\mathcal{H}^2 \\
    &\stackrel{\text{(g)}}{\le} \frac{1}{K} \sum_j \hat{\boldw}_j \|\phi(x_j)\|_\mathcal{H}^2 \\
    &\stackrel{\text{(h)}}{\le} \frac{B}{K} \sum_j \hat{\boldw}_j \\
    &\stackrel{\text{(i)}}{=} \frac{B}{K},
  \end{align} where (a) follows from the definition of $\tilde{\boldw}$, (b) follows from the fact that $\frac{1}{K}$ and $\hat{\boldw}$ are deterministic, (c) follows from $\mathbb{E}_I[I_j] = K \hat{\boldw}_j$, (d) follows from the fact that $I^2_j = I_j$ and that $I_j$ and $I_{j'}$ are independent, (e) follows from $\mathbb{E}_I[I_j] = K \hat{\boldw}_j$, (f) follows from $\mathbb{E}_I[I_j] = K \hat{\boldw}_j$, (g) follows from $\hat{\boldw}_j^2 \|\phi(x_j)\|_\mathcal{H}^2 \ge 0$, (h) follows from Assumption \ref{assumption: bounded_kernel}. i.e., $\|\phi(x_j)\|_\mathcal{H}^2 \le B$, and (i) follows from $\sum_j \hat{\boldw}_j = 1$. From Markov's inequality, we have \begin{align}
    \mathbb{P}\left[\left\|\sum_{j = 1}^{2m} \tilde{\boldw}_j \phi(x_j) - \sum_{j = 1}^{2m} \hat{\boldw}_j \phi(x_j)\right\|_\mathcal{H} \ge \sqrt{\frac{B}{\delta K}}\right] \le \delta.
  \end{align}
\end{proof}


\section{Proof of Proposition \ref{prop: mmd_postprocessing}} \label{sec: proof_mmd_postprocessing}


\begin{proposition}
  Let $\tilde{\boldw}'_j = \frac{1}{K}$ if the $j$-th item is in the final output $\mathcal{D}_T$ and $0$ otherwise. We have \begin{align}
    \left\|\sum_{j = 1}^{2m} \tilde{\boldw}'_j \phi(x_j) - \sum_{j = 1}^{2m} \tilde{\boldw}_j \phi(x_j)\right\|_\mathcal{H} \le \sqrt{\frac{B}{\delta K}}
  \end{align} with probability at least $1 - \delta$.
\end{proposition}

\begin{proof}
  \begin{align}
    \left\|\sum_{j = 1}^{2m} \tilde{\boldw}'_j \phi(x_j) - \sum_{j = 1}^{2m} \tilde{\boldw}_j \phi(x_j)\right\|_\mathcal{H}
    &= \left\|\sum_{j = 1}^{2m} \left(\tilde{\boldw}'_j - \tilde{\boldw}_j\right) \phi(x_j)\right\|_\mathcal{H} \\
    &\stackrel{\text{(a)}}{\le} \sum_{j = 1}^{2m} \left\|\left(\tilde{\boldw}'_j - \tilde{\boldw}_j\right) \phi(x_j)\right\|_\mathcal{H} \\
    &= \sum_{j = 1}^{2m} |\tilde{\boldw}'_j - \tilde{\boldw}_j| \|\phi(x_j)\|_\mathcal{H} \\
    &\stackrel{\text{(b)}}{\le} \sqrt{B} \sum_{j = 1}^{2m} |\tilde{\boldw}'_j - \tilde{\boldw}_j| \\
    &= \sqrt{B} \sum_{j = 1}^{2m} \left|\frac{1}{K} 1[j \text{ is in the final output}] - \frac{1}{K} I_j\right| \\
    &= \frac{\sqrt{B}}{K} \sum_{j = 1}^{2m} |1[j \text{ is in the final output}] - I_j| \\
    &\stackrel{\text{(c)}}{=} \frac{\sqrt{B}}{K} \left|K - \sum_{j = 1}^{2m} I_j\right|, \label{eq: postprocessing}
  \end{align} where (a) follows from the triangle inequality, (b) follows from Assumption \ref{assumption: bounded_kernel}, and (c) follows from the fact that the non-zero elements of $\tilde{\boldw}'$ and $\tilde{\boldw}$ differ only when the corresponding items are included or removed in the postprocessing step, and the items are included or removed until the number of selected items is exactly $K$. From Eq. \ref{eq: exp_num_items}, $\mathbb{E}_I\left[\sum_{j = 1}^{2m} I_j\right] = K$, and from Eq. \ref{eq: var_num_items}, $\text{Var}_I\left[\sum_{j = 1}^{2m} I_j\right] \le K$. By Chebyshev's inequality to the right hand side of Eq. \ref{eq: postprocessing}, we have \begin{align}
    \mathbb{P}\left[\left\|\sum_{j = 1}^{2m} \tilde{\boldw}'_j \phi(x_j) - \sum_{j = 1}^{2m} \tilde{\boldw}_j \phi(x_j)\right\|_\mathcal{H} \ge \sqrt{\frac{B}{\delta K}}\right] \le \delta.
  \end{align}
\end{proof}


\section{Proof of Theorem \ref{thm: mmd_sigle_shot}} \label{sec: proof_mmd_single_shot}


\begin{theorem}
  Under assumption \ref{assumption: bounded_kernel}, when we run the Frank-Wolfe algorithm with the step size $\frac{2}{t + 2}$ for $t = 0, 1, \ldots, L - 1$ and select items with the probability $K \boldw_j$, we obtain $\mathcal{D}_T$ such that \begin{align}
    \text{MMD}(\mu_T^{\mathcal{D}_T}, \mu_S) \le \text{OPT}^{\text{combinatorial}} + C L^{-1/2} + 2\sqrt{\frac{B}{\delta K}}
  \end{align} with probability at least $1 - 2\delta$.
\end{theorem}

\begin{proof}
  \begin{align}
    &\text{MMD}(\mu_T^{\mathcal{D}_T}, \mu_S) \\
    &= \left\|\sum_{j = 1}^{2m} \tilde{\boldw}'_j \phi(x_j) - \int \phi(x) \, d\mu_S(x)\right\|_\mathcal{H} \\
    &\stackrel{\text{(a)}}{\le} \left\|\sum_{j = 1}^{2m} \tilde{\boldw}'_j \phi(x_j) - \sum_{j = 1}^{2m} \tilde{\boldw}_j \phi(x_j)\right\|_\mathcal{H} + \left\|\sum_{j = 1}^{2m} \tilde{\boldw}_j \phi(x_j) - \sum_{j = 1}^{2m} \hat{\boldw}_j \phi(x_j)\right\|_\mathcal{H} + \left\|\sum_{j = 1}^{2m} \hat{\boldw}_j \phi(x_j) - \int \phi(x) \, d\mu_S(x)\right\|_\mathcal{H} \\
    &= \left\|\sum_{j = 1}^{2m} \tilde{\boldw}'_j \phi(x_j) - \sum_{j = 1}^{2m} \tilde{\boldw}_j \phi(x_j)\right\|_\mathcal{H} + \left\|\sum_{j = 1}^{2m} \tilde{\boldw}_j \phi(x_j) - \sum_{j = 1}^{2m} \hat{\boldw}_j \phi(x_j)\right\|_\mathcal{H} + \text{MMD}(\mu_T^{\hat{\boldw}}, \mu_S) \\
    &\stackrel{\text{(b)}}{\le} \left\|\sum_{j = 1}^{2m} \tilde{\boldw}'_j \phi(x_j) - \sum_{j = 1}^{2m} \tilde{\boldw}_j \phi(x_j)\right\|_\mathcal{H} + \left\|\sum_{j = 1}^{2m} \tilde{\boldw}_j \phi(x_j) - \sum_{j = 1}^{2m} \hat{\boldw}_j \phi(x_j)\right\|_\mathcal{H} + \text{OPT}^{\text{combinatorial}} + C L^{-1/2} \\
    &\stackrel{\text{(c)}}{\le} \sqrt{\frac{B}{\delta K}} + \sqrt{\frac{B}{\delta K}} + \text{OPT}^{\text{combinatorial}} + C L^{-1/2} \\
    &= \text{OPT}^{\text{combinatorial}} + C L^{-1/2} + 2\sqrt{\frac{B}{\delta K}},
  \end{align} where (a) follows from the triangle inequality, (b) follows from Proposition \ref{prop: mmd_optimization}, (c) follows from Propositions \ref{prop: mmd_random_selection} and \ref{prop: mmd_postprocessing} and the union bound with probability at least $1 - 2\delta$.
\end{proof}


\section{Proof of Theorem \ref{thm: mmd_opt_combinatorial}} \label{sec: proof_mmd_opt_combinatorial}


\begin{theorem}
  Under Assumptions \ref{assumption: bounded_kernel} and \ref{assumption: bounded_density_ratio}, there exists $C_1, C_2 \in \mathbb{R}_+$ such that when $K \le C_1 \frac{|J_T|}{r^*}$, \begin{align}
    \text{MMD}(P, \mu^{\mathcal{D}_T}) \le C_2 \left(2 \sqrt{\frac{B}{|\mathcal{D}_S|}} + \sqrt{\frac{B}{K}}\right) \quad \text{and} \quad \text{OPT}^{\text{combinatorial}} \le C_2 \left(\sqrt{\frac{B}{|\mathcal{D}_S|}} + \sqrt{\frac{B}{K}}\right)
  \end{align} with high probability.
\end{theorem}

\begin{proof}
  Under Assumption \ref{assumption: bounded_density_ratio}, $\frac{1}{r^*} P \le Q$ and $Q$ can be written as $\frac{1}{r^*} P + (1 - \frac{1}{r^*}) Q'$ for some distribution $Q'$. Let $Z_i = 1$ with probability $\frac{1}{r^*}$ and $Z_i = 0$ with probability $1 - \frac{1}{r^*}$, and let sample $X_i$ from $P$ if $Z_i = 1$ and $Q'$ if $Z_i = 0$ for $i = 1, \ldots, |J_T|$. $X_i$ follows distribution $Q$. We identity $\{X_1, \ldots, X_{|J_T|}\}$ with $J_T$. Let $J_1 = \{X_i \mid Z_i = 1\}$. $\mathbb{E}\left[|J_1| = \frac{|J_T|}{r^*}\right]$ and $|J_1| \ge \frac{C_1 |J_T|}{r^*}$ for some constant $C_1$ with high probability by Markov's inequality. We assume this holds in the following. Suppose that $K \le C_1 \frac{|J_T|}{r^*}$, then $K \le |J_1|$. Let $J_2$ be the first $K$ samples from $J_1$. Elements in $J_2$ are independent and follow the distribution $P$. As the empirical distribution converges to the true distribution with rate $O(\sqrt{\frac{B}{K}})$ in MMD \cite{gretton2012kernel}, we have \begin{align}
    \text{MMD}(P, J_2) &\le C_2 \sqrt{\frac{B}{K}} \quad \text{and} \\
    \text{MMD}(P, \mathcal{D}_S) &\le C_2 \sqrt{\frac{B}{|\mathcal{D}_S|}},
  \end{align} for some constant $C_2$ with high probability. By the triangle inequality, we have \begin{align}
    \text{MMD}(J_2, \mathcal{D}_S) &\le \text{MMD}(P, J_2) + \text{MMD}(P, \mathcal{D}_S) \\
    &\le C_2 \left(\sqrt{\frac{B}{|\mathcal{D}_S|}} + \sqrt{\frac{B}{K}}\right).
  \end{align} As $J_2 \subseteq J_T$ and $|J_2| = K$, $J_2$ is a feasible solution of the combinatorial problem, and the optimal $\mathcal{D}_T \subseteq J_T$ has no worse MMD than $J_2$, and we have the desired guarantee.
\end{proof}


\section{Proof of Lemma \ref{lem: covering}} \label{sec: proof_covering}


\begin{lemma}
  Let $\mathcal{F}_\text{Lip}$ be the set of $1$-Lipschitz functions with $f(\frac{\bold1}{2}) = 0$. $\mathcal{F}_\text{Lip}$ can be $L_\infty$-covered by $\tilde{\mathcal{F}}_\varepsilon$,  i.e., for any $f \in \mathcal{F}_\text{Lip}$, there exists $\tilde{f} \in \tilde{\mathcal{F}}_\varepsilon$ such that $\|f - \tilde{f}\|_\infty \le \varepsilon$, with $|f(x)| \le \frac{\sqrt{d}}{2}, \forall f \in \tilde{\mathcal{F}}_\varepsilon$ and $|\tilde{\mathcal{F}}_\varepsilon| \le \exp\left(\log (3) \left(\frac{2 \sqrt{d}}{\varepsilon}\right)^d\right)$.
\end{lemma}

\begin{proof}
  We devide the domain $[0, 1]^d$ into $\lceil\frac{\sqrt{d}}{\varepsilon}\rceil^d$ hypercubes with diameter at most $\varepsilon$. For each hypercube $\mathcal{C}$ and $f \in \mathcal{F}_\text{Lip}$, let $f_\mathcal{C} = \frac{\max_{x \in \mathcal{C}} f(x) + \min_{x \in \mathcal{C}} f(x)}{2}$ and $\tilde{f}_\mathcal{C}$ be its nearest point in $\{k\varepsilon \mid k \in \mathbb{Z}\}$. We then define $\tilde{f}(x) = \tilde{f}_\mathcal{C}$ for $x \in \mathcal{C}$. We have $|f(x) - \tilde{f}(x)| \le \varepsilon$ since the diameter of $\mathcal{C}$ is at most $\varepsilon$ and $f$ is $1$-Lipschitz. We count the number of possible $\tilde{f}$. For the hypercube $\mathcal{C}_{\text{center}}$ with $\frac{\bold1}{2} \in \mathcal{C}_{\text{center}}$, $\tilde{f}_{\mathcal{C}_{\text{center}}} = 0$ since $f(\frac{\bold1}{2}) = 0$. Given the value of $\tilde{f}_\mathcal{C}$, possible values of an adjacent hypercube $\mathcal{C}'$ are $\tilde{f}_\mathcal{C} - \varepsilon, \tilde{f}_\mathcal{C}, \tilde{f}_\mathcal{C} + \varepsilon$. We can reach all the hypercubes by repeating this process. Therefore, the possible values of $\tilde{f}$ are at most $3^{s - 1}$, where $s = \lceil\frac{\sqrt{d}}{\varepsilon}\rceil^d$ is the number of hypercubes. When $\frac{\sqrt{d}}{\varepsilon} \ge 1$, we have $\lceil\frac{\sqrt{d}}{\varepsilon}\rceil \le \frac{\sqrt{d}}{\varepsilon} + 1 \le \frac{2\sqrt{d}}{\varepsilon}$, and $|\tilde{\mathcal{F}}| \le \exp\left(\log (3) \left(\frac{2 \sqrt{d}}{\varepsilon}\right)^d\right)$. When $\frac{\sqrt{d}}{\varepsilon} < 1$, we have $|\tilde{\mathcal{F}}| = 1$, in which case the lemma holds trivially.
\end{proof}



\section{Proof of Proposition \ref{prop: wasserstein_postprocessing}} \label{sec: proof_wasserstein_postprocessing}


\begin{proposition}
  Let $\tilde{\boldw}'_j = \frac{1}{K}$ if the $j$-th item is in the final output $\mathcal{D}_T$ and $0$ otherwise. For any $\varepsilon > 0$, \begin{align}
    \sup_{f \in \tilde{\mathcal{F}}_\varepsilon} \sum_{j = 1}^{2m} f(x_j) (\tilde{\boldw}'_j - \tilde{\boldw}_j) \le \sqrt{\frac{d}{2K} \log \frac{1}{\delta}} + \frac{\sqrt{d}}{3K} \log \frac{1}{\delta}
  \end{align} with probability at least $1 - \delta$.
\end{proposition}

\begin{proof}
  \begin{align}
    \sup_{f \in \tilde{\mathcal{F}}_\varepsilon} \sum_{j = 1}^{2m} (\tilde{\boldw}'_j - \tilde{\boldw}_j) f(x_j) &\le \|f\|_\infty \sum_{j = 1}^{2m} |\tilde{\boldw}'_j - \tilde{\boldw}_j| \\
    &\le \frac{\sqrt{d}}{2} \sum_{j = 1}^{2m} |\tilde{\boldw}'_j - \tilde{\boldw}_j| \\
    &\le \frac{\sqrt{d}}{2} \sum_{j = 1}^{2m} \left|\frac{1}{K} 1[j \text{ is in the final output}] - \frac{1}{K} I_j\right| \\
    &= \frac{\sqrt{d}}{2K} \sum_{j = 1}^{2m} |1[j \text{ is in the final output}] - I_j| \\
    &= \frac{\sqrt{d}}{2K} \left|K - \sum_{j = 1}^{2m} I_j\right|. \label{eq: wasserstein_postprocessing2}
  \end{align} $I_j$ are independent Bernoulli random variables with $|I_j| \le 1$, and from Eq. \ref{eq: exp_num_items}, $\mathbb{E}_I\left[\sum_{j = 1}^{2m} I_j\right] = K$, and from Eq. \ref{eq: var_num_items}, $\text{Var}_I\left[\sum_{j = 1}^{2m} I_j\right] \le K$. By Bernstein's inequality, we have \begin{align}
    \mathbb{P}\left[\left|K - \sum_{j = 1}^{2m} I_j\right| \ge \sqrt{2K \log \frac{1}{\delta}} + \frac{2}{3} \log \frac{1}{\delta}\right] \le \delta.
  \end{align} By combining it with Eq. \ref{eq: wasserstein_postprocessing2}, we have the desired guarantee.
\end{proof}



\section{Proof of Proposition \ref{prop: wasserstein_random_selection}} \label{sec: proof_wasserstein_random_selection}

\begin{lemma}
  Let $\tilde{\boldw}_j = \frac{I_j}{K}$ with $I_j \sim \text{Bernoulli}(K \hat{\boldw}_j)$. Then, for any $f$ with $|f(x)| \le \frac{\sqrt{d}}{2}$, \begin{align}
    \sum_{j = 1}^{2m} f(x_j) (\tilde{\boldw}_j - \hat{\boldw}_j) \le \sqrt{\frac{d}{2K} \log \frac{1}{\delta}} + \frac{\sqrt{d}}{3K} \log \frac{1}{\delta}
  \end{align} with probability at least $1 - \delta$.
\end{lemma}

\begin{proof}
   Since $0 \le \tilde{\boldw}_j, \hat{\boldw}_j \le \frac{1}{K}$, we have $|(\tilde{\boldw}_j - \hat{\boldw}_j) f(x_j)| \le \frac{\sqrt{d}}{2K}$. Here, $(\tilde{\boldw}_j - \hat{\boldw}_j) f(x_j)$ are independent random variables with mean $\mathbb{E}_I[(\tilde{\boldw}_j - \hat{\boldw}_j)] f(x_j) = 0$ and variance \begin{align}
    \text{Var}_I[(\tilde{\boldw}_j - \hat{\boldw}_j) f(x_j)] = f(x_j)^2 \text{Var}_I[\tilde{\boldw}_j] = f(x_j)^2 \text{Var}_I\left[\frac{I_j}{K}\right] = \frac{f(x_j)^2}{K^2} \text{Var}_I[I_j] = \frac{f(x_j)^2 K \hat{\boldw}_j (1 - K \hat{\boldw}_j)}{K^2}.
  \end{align} The sum of the variances is \begin{align}
    \sum_{j = 1}^{2m} \frac{f(x_j)^2 K \hat{\boldw}_j (1 - K \hat{\boldw}_j)}{K^2} &= \frac{1}{K} \sum_{j = 1}^{2m} f(x_j)^2 \hat{\boldw}_j (1 - K \hat{\boldw}_j) \\
    &\stackrel{\text{(a)}}{\le} \frac{d}{4K} \sum_{j = 1}^{2m} \hat{\boldw}_j (1 - K \hat{\boldw}_j) \\
    &\stackrel{\text{(b)}}{\le} \frac{d}{4K} \sum_{j = 1}^{2m} \hat{\boldw}_j \\
    &\stackrel{\text{(c)}}{=} \frac{d}{4K},
  \end{align} where (a) follows from $f(x_j) \le \frac{\sqrt{d}}{2}$, (b) follows from the fact that $1 \ge K \hat{\boldw}_j$, and (c) follows from $\sum_j \hat{\boldw}_j = 1$. By Bernstein's inequality, we have \begin{align}
    \mathbb{P}\left[\sum_{j = 1}^{2m} (\tilde{\boldw}_j - \hat{\boldw}_j) f(x_j) \ge \sqrt{\frac{d}{2K} \log \frac{1}{\delta}} + \frac{\sqrt{d}}{3K} \log \frac{1}{\delta}\right] \le \delta.
  \end{align}
\end{proof}





\section{Proof of Theorem \ref{thm: wasserstein1}} \label{sec: proof_wasserstein1}


\begin{theorem}
  For any $\delta > 0$, the $1$-Wasserstein distance between the final output $\mathcal{D}_T$ and the source distribution is \begin{align}
    W_1(\mu_T^{\mathcal{D}_T}, \mu_S) &\le \text{OPT}^{\text{combinatorial}} + 3 \sqrt{\frac{d}{K} \log \frac{1}{\delta}} + \frac{\sqrt{d}}{K} \log \frac{1}{\delta} + 8 \sqrt{d} K^{-\frac{1}{d+2}} + 11 \sqrt{d} K^{-\frac{2}{d+2}} + 6\sqrt{d \log \frac{1}{\delta}} K^{-\frac{1}{d+2}-\frac{1}{2}}
  \end{align} with probability at least $1 - 2\delta$.
\end{theorem}

\begin{proof}
  We use \begin{align}
    \varepsilon = \sqrt{d} \left(2^{\frac{d-2}{d+2}} \log^{\frac{1}{d+2}}(3) K^{-\frac{1}{d+2}} + \frac{1}{2} \sqrt{\frac{1}{K} \log \frac{1}{\delta}}\right)
  \end{align} in the following. Then, \begin{align}
    \frac{4K}{d} \varepsilon^{d+2} &\ge 4K d^{-1} d^{\frac{d + 2}{2}} \left(2^{\frac{d-2}{d+2}} \log^{\frac{1}{d+2}}(3) K^{-\frac{1}{d+2}}\right)^{d+2} \\
    &= d^{\frac{d}{2}} 2^d \log(3) \label{eq: wasserstein1-1}
  \end{align} and thus \begin{align}
    \left(\frac{2 \sqrt{d}}{\varepsilon}\right)^d \log (3) \le \frac{4K}{d} \varepsilon^{2}.
  \end{align} In addition, \begin{align}
    \frac{4K}{d} \varepsilon^2 &\ge 4K \left(\frac{1}{2} \sqrt{\frac{1}{K} \log \frac{1}{\delta}}\right)^2 \\
    &= \log \frac{1}{\delta}. \label{eq: wasserstein1-2}
  \end{align} By combining Eq. \ref{eq: wasserstein1-1} and Eq. \ref{eq: wasserstein1-2}, we have \begin{align}
    \left(\frac{2 \sqrt{d}}{\varepsilon}\right)^d \log (3) + \log \frac{1}{\delta} \le \frac{8K}{d} \varepsilon^2. \label{eq: wasserstein1-3}
  \end{align} By Lemma \ref{lem: covering}, $|\tilde{\mathcal{F}}_\varepsilon| \le \exp\left(\log (3) \left(\frac{2 \sqrt{d}}{\varepsilon}\right)^d\right)$. We have \begin{align}
    \frac{d}{2K} \log \frac{|\tilde{\mathcal{F}_\varepsilon}|}{\delta} &= \frac{d}{2K} (\log |\tilde{\mathcal{F}_\varepsilon}| + \log \frac{1}{\delta}) \\
    &\le \frac{d}{2K} \left(\log (3) \left(\frac{2 \sqrt{d}}{\varepsilon}\right)^d + \log \frac{1}{\delta}\right) \\
    &\stackrel{\text{(a)}}{\le} \frac{d}{2K} \left(\frac{8K}{d} \varepsilon^2\right) \\
    &= 4 \varepsilon^2, \label{eq: wasserstein1-4}
  \end{align} where (a) follows from Eq. \ref{eq: wasserstein1-3}. In addition, we have \begin{align}
    \frac{\sqrt{d}}{3K} \log \frac{|\tilde{\mathcal{F}_\varepsilon}|}{\delta} &= \frac{\sqrt{d}}{3K} (\log |\tilde{\mathcal{F}_\varepsilon}| + \log \frac{1}{\delta}) \\
    &\le \frac{\sqrt{d}}{3K} \left(\log (3) \left(\frac{2 \sqrt{d}}{\varepsilon}\right)^d + \log \frac{1}{\delta}\right) \\
    &\stackrel{\text{(a)}}{\le} \frac{\sqrt{d}}{3K} \left(\frac{8K}{d} \varepsilon^2\right) \\
    &= \frac{8}{3\sqrt{d}} \varepsilon^2, \label{eq: wasserstein1-5}
  \end{align} where (a) follows from Eq. \ref{eq: wasserstein1-3}. We set $\delta \leftarrow \delta / |\tilde{\mathcal{F}}|$ in Proposition \ref{prop: wasserstein_random_selection} and apply the union bound, then, we have \begin{align}
    \sum_{j = 1}^{2m} (\tilde{\boldw}_j - \hat{\boldw}_j) \tilde{f}(x_j) &\le \sqrt{\frac{d}{2K} \log \frac{|\tilde{\mathcal{F}|}}{\delta}} + \frac{\sqrt{d}}{3K} \log \frac{|\tilde{\mathcal{F}|}}{\delta} \\
    &\stackrel{\text{(a)}}{\le} 2 \varepsilon + \frac{8}{3\sqrt{d}} \varepsilon^2 \label{eq: wasserstein1-6}
  \end{align} for all $\tilde{f} \in \tilde{\mathcal{F}}$ with probability at least $1 - \delta$, where (a) follows from Eq. \ref{eq: wasserstein1-4} and Eq. \ref{eq: wasserstein1-5}. In the following, we assume that Eqs. \ref{eq: wasserstein_postprocessing} and \ref{eq: wasserstein1-6} hold, which is true with probability at least $1 - 2 \delta$. Then, we have \begin{align}
    \sum_{j = 1}^{2m} (\tilde{\boldw}'_j - \hat{\boldw}_j) \tilde{f}(x_j) &= \sum_{j = 1}^{2m} (\tilde{\boldw}'_j - \tilde{\boldw}_j) \tilde{f}(x_j) + \sum_{j = 1}^{2m} (\tilde{\boldw}_j - \hat{\boldw}_j) \tilde{f}(x_j) \\
    &\le \sqrt{\frac{d}{2K} \log \frac{1}{\delta}} + \frac{\sqrt{d}}{3K} \log \frac{1}{\delta} + 2 \varepsilon + \frac{8}{3\sqrt{d}} \varepsilon^2.
  \end{align} For any $f \in \mathcal{F}_\text{Lip}$, there exists $\tilde{f} \in \tilde{\mathcal{F}}$ such that $\|f - \tilde{f}\|_\infty \le \varepsilon$, and we have \begin{align}
    &\sum_{j = 1}^{2m} (\tilde{\boldw}'_j - \hat{\boldw}_j) f(x_j) \\
    &= \sum_{j = 1}^{2m} (\tilde{\boldw}'_j - \hat{\boldw}_j) \tilde{f}(x_j) + \sum_{j = 1}^{2m} (\tilde{\boldw}'_j - \hat{\boldw}_j) (f(x_j) - \tilde{f}(x_j)) \\
    &\stackrel{\text{(a)}}{\le} \sum_{j = 1}^{2m} (\tilde{\boldw}'_j - \hat{\boldw}_j) \tilde{f}(x_j) + \varepsilon \sum_{j = 1}^{2m} |\tilde{\boldw}'_j - \hat{\boldw}_j| \\
    &\stackrel{\text{(b)}}{\le} \sum_{j = 1}^{2m} (\tilde{\boldw}'_j - \hat{\boldw}_j) \tilde{f}(x_j) + 2\varepsilon \\
    &\le \sqrt{\frac{d}{2K} \log \frac{1}{\delta}} + \frac{\sqrt{d}}{3K} \log \frac{1}{\delta} + 2 \varepsilon + \frac{8}{3\sqrt{d}} \varepsilon^2 + 2\varepsilon \\
    &\le \sqrt{\frac{d}{2K} \log \frac{1}{\delta}} + \frac{\sqrt{d}}{3K} \log \frac{1}{\delta} + 4 \varepsilon + \frac{8}{3\sqrt{d}} \varepsilon^2 \\
    &\stackrel{\text{(c)}}{\le} \sqrt{\frac{d}{2K} \log \frac{1}{\delta}} + \frac{\sqrt{d}}{3K} \log \frac{1}{\delta} + 4 \sqrt{d} \left(2^{\frac{d-2}{d+2}} \log^{\frac{1}{d+2}}(3) K^{-\frac{1}{d+2}} + \frac{1}{2} \sqrt{\frac{1}{K} \log \frac{1}{\delta}}\right)  \\ &\qquad + \frac{8}{3} \sqrt{d} \left(2^{\frac{d-2}{d+2}} \log^{\frac{1}{d+2}}(3) K^{-\frac{1}{d+2}} + \frac{1}{2} \sqrt{\frac{1}{K} \log \frac{1}{\delta}}\right)^2 \\
    &= \sqrt{\frac{d}{2K} \log \frac{1}{\delta}} + \frac{\sqrt{d}}{3K} \log \frac{1}{\delta} + 4 \sqrt{d} \left(2 \left(\frac{\log 3}{2^4}\right)^{\frac{1}{d + 2}} K^{-\frac{1}{d+2}} + \frac{1}{2} \sqrt{\frac{1}{K} \log \frac{1}{\delta}}\right)  \\ &\qquad + \frac{8}{3} \sqrt{d} \left(2 \left(\frac{\log 3}{2^4}\right)^{\frac{1}{d + 2}} K^{-\frac{1}{d+2}} + \frac{1}{2} \sqrt{\frac{1}{K} \log \frac{1}{\delta}}\right)^2 \\
    &\le \sqrt{\frac{d}{2K} \log \frac{1}{\delta}} + \frac{\sqrt{d}}{3K} \log \frac{1}{\delta} + 4 \sqrt{d} \left(2 K^{-\frac{1}{d+2}} + \frac{1}{2} \sqrt{\frac{1}{K} \log \frac{1}{\delta}}\right)  \\ &\qquad + \frac{8}{3} \sqrt{d} \left(2  K^{-\frac{1}{d+2}} + \frac{1}{2} \sqrt{\frac{1}{K} \log \frac{1}{\delta}}\right)^2 \\
    &\le 3 \sqrt{\frac{d}{K} \log \frac{1}{\delta}} + \frac{\sqrt{d}}{K} \log \frac{1}{\delta} + 8 \sqrt{d} K^{-\frac{1}{d+2}} + 11 \sqrt{d} K^{-\frac{2}{d+2}} + 6\sqrt{d \log \frac{1}{\delta}} K^{-\frac{1}{d+2}-\frac{1}{2}}
  \end{align} where (a) follows from $\|f - \tilde{f}\|_\infty \le \varepsilon$ and (b) follows from $\tilde{\boldw}', \hat{\boldw} \in \Delta_{2m - 1}$ (c) follows from the definition of $\varepsilon$, and (d) follows from $d \ge 1$. From the Kantorovich-Rubinstein duality, we have \begin{align}
    W_1(\mu_T^{\mathcal{D}_T}, \mu_T^{\hat{\boldw}}) \le 3 \sqrt{\frac{d}{K} \log \frac{1}{\delta}} + \frac{\sqrt{d}}{K} \log \frac{1}{\delta} + 8 \sqrt{d} K^{-\frac{1}{d+2}} + 11 \sqrt{d} K^{-\frac{2}{d+2}} + 6\sqrt{d \log \frac{1}{\delta}} K^{-\frac{1}{d+2}-\frac{1}{2}}.
  \end{align} Note that we restrected test functions to $f(\frac{\bold1}{2}) = 0$ but this does not affect the bound by adding a bias. As $\hat{\boldw}$ is the optimal solution of the linear program, we have $W_1(\mu_T^{\hat{\boldw}}, \mu_S) = \text{OPT}^{\text{continuous}} \le \text{OPT}^{\text{combinatorial}}$. By the triangle inequality, we have the desired guarantee.
\end{proof}


\end{document}