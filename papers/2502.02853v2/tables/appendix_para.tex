\begin{table*}[ht]
\begin{minipage}[c]{0.48\textwidth}
\makeatletter\def\@captype{table}
\centering
\caption{Training hyperparameters of all (full $|$ partial) fine-tuning baselines in CortexBench.}
\begin{tabular}{@{}cc@{}}
    \toprule
    Hyperparameters & Training \\
    \midrule
    epoch & 50 $|$ 100  \\
    batch size &  256 $|$ 512  \\
    optimizer & AdamW  \\
    learning rate & 1e-4 $|$ 1e-3  \\
    weight decay & 1e-4 \\
    lr scheduler & Cosine  \\
    lr warm up & 0 \\
    clip grad & 100 \\
    augmentation & Resize, CenterCrop, Normalize \\
    history length & 3 \\
    \bottomrule
\end{tabular}
\label{tab:appendix_cortex_para}
\end{minipage}
\begin{minipage}[c]{0.48\textwidth}
\makeatletter\def\@captype{table}
\centering
  \caption{Training hyperparameters of all baselines in LIBERO.}
  \begin{tabular}{@{}cc@{}}
    \toprule
    Hyperparameters & Training  \\
    \midrule
    epoch & 50  \\
    batch size &  64  \\
    optimizer &  AdamW  \\
    learning rate & 1e-4  \\
    weight decay & 1e-4 \\
    lr scheduler & Cosine  \\
    lr warm up & 0 \\
    clip grad & 100 \\
    augmentation & Normalize, ColorJitter \\
    history length & 10 \\
    \bottomrule
  \end{tabular}
  \label{tab:appendix_libero_para}
\end{minipage}
\end{table*}




\begin{table*}[ht]
\centering
\caption{IB-related Hyperparameters of all baselines in both CortexBench and LIBERO.
}
\begin{tabular}{cc}
\toprule
IB-related Hyperparameters & Training \\
\toprule
\multicolumn{2}{c}{\textit{MINE model}} \\
\midrule
architecture & 4-layer MLP \\
hidden size & 512 \\
output size & 1 \\
optimizer & Adam \\
learning rate & 1e-5 \\
loss weight & 0.1 \\
\midrule
\multicolumn{2}{c}{\textit{IB loss}} \\
\midrule
Lagrange multiplier $\beta$ & [1e-4, 5e-3] \\
\bottomrule
\end{tabular}
\label{tab:appendix_ib_para}
\end{table*}