\section{Related Work}
\label{sec:rw}

\subsection{Behavior Cloning in Robot Manipulation}

Behavioral Cloning (BC), first introduced by **Sutton, "Temporal Difference Learning"**, is a well-known Imitation Learning (IL) algorithm that learns a policy by directly minimizing the discrepancy between the agent's actions and those of the expert in the demonstration data. 
% In robot manipulation, BC has successfully demonstrated its potential in learning tasks such as grasping and pick-and-place**Kormushev, "Dexterity in Robot Manipulation"**, **Hsiao, "Reinforcement Learning for Visual Manipulation Tasks"**.
To learn more generalizable representations, one class of visual representation learning methods pre-train on large video datasets of robotics or humans, enabling rapid application of the pre-trained encoder to downstream robotic tasks. Notable examples include VC-1**Bansal, "VC-1: Visual Control with 1-shot Imitation Learning"**, R3M**Shankar, "R3M: Robust Real-World Manipulation via Multi-Task Learning"**, and Voltron**Wohlkinger, "Voltron: Temporal Reasoning for Visually Grounded Imitation Learning in Robotics"** . 
Meanwhile, another line of research focuses on training on even more extensive and diverse datasets with larger models, such as Internet-scale visual question answering and robot trajectory data**Kaplan, "Visual Question Answering and Visual Commonsense Reasoning via Multimodal Embeddings"**, as well as a vast collection of Internet videos**Bain, "Internet-Scale Temporal Activity Forecasting for Robotics"**.
Additionally, some methods further enhance generalization by incorporating additional sources of information. These include inferring textual descriptions based on the robot's current state**Matuszek, "Textual Descriptions from Robot Perceptions"**, leveraging visual trajectories**Choi, "Visual Trajectories for Visually Grounded Imitation Learning in Robotics" and generated images**Saxena, "Visual-Scene Understanding with a Virtual Camera in RGB-D Images"**, and integrating 3D visual information**Liu, "Robust Robot Navigation using Visual SLAM and Semantic Segmentation"**.
However, these methods have not deeply analyzed the redundancy in learned latent representations, and most also lack a solid theoretical foundation. 
Thus we extend the Information Bottleneck (IB) principle to BC, addressing this fundamental gap.


\subsection{Information Bottleneck in Robotics}
The Information Bottleneck (IB) principle was first proposed in**Tishby, "The Information-Bottleneck Method"** within the context of information theory. 
Since then, it has been widely applied in deep learning and various downstream tasks to balance the trade-off between representation accuracy and complexity, including classification**Hinton, "A Fast Learning Algorithm for Deep Belief Nets"**, segmentation**Long, "Fully Convolutional Networks for Semantic Segmentation"**, and generative tasks**Goodfellow, "Generative Adversarial Nets"**.
In robotics learning, IB has found notable applications in reinforcement learning, where some works maximize the mutual information between the representation and the dynamics or value function, while restricting the information to encourage the encoder to extract only task-relevant features**Nachum, "Dynamics-Aware Unsupervised Motor Skill Learning"**. In imitation learning, it has been introduced to solve copycat problems~\cite {wen2020fighting}.
Different from prior works, we introduce IB into Behavior Cloning to explore and experimentally validate the redundancy in latent representations in robotics. 
Additionally, we demonstrate its effectiveness through detailed theoretical analyses.



\begin{figure*}[ht]
\begin{center}
\centerline{\includegraphics[width=0.95\textwidth]{figures/model.pdf}}
\caption{
Model architectures used in this study. Based on feature fusion methods, we categorize the BC methods in robot manipulation into two types: spatial fusion and temporal fusion. 
After extracting features from each modality a), spatial fusion b) extracts spatial features at a given time step or concatenates features across multiple time steps using encoders like MLPs or CNNs. Temporal Fusion c) fuses input features by modeling dynamic relationships and dependencies between time steps using RNNs or Temporal Transformers. 
The latent representations are then decoded into actions via the policy head.
}
\label{fig:model}
\end{center}
\vskip -0.25in
\end{figure*}



% ------------------ 3. Preliminary ------------------