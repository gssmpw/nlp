%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}
\pagestyle{plain}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% my
\usepackage{bm}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{multirow}
\DeclareUnicodeCharacter{2212}{\textminus}
\usepackage{wrapfig}



% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\begin{document}

\twocolumn[
\icmltitle{Rethinking Latent Representations in Behavior Cloning: An Information Bottleneck Approach for Robot Manipulation}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Shuanghao Bai}{1}
\icmlauthor{Wanqi Zhou}{1}
\icmlauthor{Pengxiang Ding}{2}
\icmlauthor{Wei Zhao}{2}
\icmlauthor{Donglin Wang}{2}
\icmlauthor{Badong Chen}{1}
\end{icmlauthorlist}

\icmlaffiliation{1}{Xi'an Jiaotong University}
\icmlaffiliation{2}{Westlake University}

\icmlcorrespondingauthor{Badong Chen}{chenbd@mail.xjtu.edu.cn}
\icmlcorrespondingauthor{Donglin Wang}{wangdonglin@westlake.edu.cn}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.



% ------------------ 0. Abstract ------------------
\begin{abstract}
Behavior Cloning (BC) is a widely adopted visual imitation learning method in robot manipulation.
Current BC approaches often enhance generalization by leveraging large datasets and incorporating additional visual and textual modalities to capture more diverse information.
However, these methods overlook whether the learned representations contain redundant information and lack a solid theoretical foundation to guide the learning process.
To address these limitations, we adopt an information-theoretic perspective and introduce mutual information to quantify and mitigate redundancy in latent representations.
Building on this, we incorporate the Information Bottleneck (IB) principle into BC, which extends the idea of reducing redundancy by providing a structured framework for compressing irrelevant information while preserving task-relevant features.
This work presents the first comprehensive study on redundancy in latent representations across various methods, backbones, and experimental settings, while extending the generalizability of the IB to BC.
Extensive experiments and analyses on the CortexBench and LIBERO benchmarks demonstrate significant performance improvements with IB, underscoring the importance of reducing input data redundancy and highlighting its practical value for more practical applications.
Project Page: \href{https://baishuanghao.github.io/BC-IB.github.io}{BC-IB Website}.
\end{abstract}



% ------------------ 1. Introduction ------------------
\section{Introduction}
\label{sec:intro}


\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figures/intro.pdf}}
\caption{
Policy architecture of BC. Current BC methods (black arrows) do not impose restrictions on the latent representations $Z$, potentially allowing redundant information from the input representations $X$.
}
\label{fig:intro}
\end{center}
\vskip -0.2in
\end{figure}

Behavior Cloning (BC), one of the simplest and most widely used methods in Imitation Learning (IL), learns a mapping from states to actions by training on state-action pairs from expert demonstrations. 
BC has been widely studied in autonomous driving~\cite{bain1995framework, torabi2018behavioral}, robotics control~\cite{argall2009survey} and game AI~\cite{pearce2022counter}. 
In robot manipulation, BC has become a foundational approach, enabling robots to replicate expert actions based on sensory inputs such as images or proprioception information like gripper states. 
To enhance the generalization of robots, most BC methods focus on incorporating large datasets of human or manipulation videos~\cite{jang2022bc, karamcheti2023language, brohan2023rt, cheang2024gr, saxena2025matters}, or integrating additional text and visual information~\cite{jia2024chain, wen2023any, hu2024video}.
While these methods have made significant progress in improving generalization by leveraging more diverse information, they often neglect a critical aspect: whether the learned representations contain significant redundant information.

\textit{\textbf{Why do we need to explore this?}}
Firstly, the inherent challenges of input data redundancy remain largely unexplored in BC for robot manipulation, despite their potential to significantly impact performance.
Secondly, most existing methods lack a solid theoretical foundation to guide the learning process.
Then, a key question arises: how can we quantify the redundancy in inputs or representations, and how can we effectively reduce it?

\textit{\textbf{How to explore this?}}
As illustrated in~\cref{fig:intro}, in BC, the inputs are typically encoded into individual representations and concatenated to form the input representation $X$. This is then processed through a feature fusion module to produce the latent representation $Z$, which is subsequently decoded to predict the action $A$. The policy is optimized by minimizing the discrepancy between the predicted actions and the expert-provided actions.
In information theory, mutual information between $X$ and $Z$, denoted as $I(X, Z)$, measures the amount of information gained about one random variable by knowing the other. In BC, if output $Y$ can be well predicted by $Z$, reducing $I(X, Z)$ means continuously eliminating redundant information from $X$.

Taking a step further, an information-theoretic approach that balances the trade-off between representation complexity and predictive power offers a \textit{natural framework} to address the problem of latent representation redundancy and the lack of a solid theoretical foundation, namely information bottleneck (IB) principle~\cite{tishby2000information}. 
IB regularizes the representation $Z$ by minimizing the mutual information $I(X, Z)$ between $X$ and $Z$, while maximizing the mutual information $I(Z, A)$ between $Y$ and $A$. 
The first term $I(X, Z)$ represents the compression of the representation, where a smaller mutual information indicates a greater degree of compression and redundancy reduction, while $I(Z, A)$ ensures predictive power is maintained.   

Motivated by this information-theoretic approach, we make the first attempt in this work to study the impact of latent representation redundancy in BC for robot manipulation and extend the IB method to this context, where redundancy in latent representations is quantified by $I(X, Z)$. 
We conduct extensive experiments in various settings and analyses to validate its effectiveness, highlighting the benefits of reducing redundancy to enhance generalization in robotic tasks.
Additionally, we provide detailed theoretical analyses, including generalization error bounds, to validate its effectiveness.

\textbf{\textit{How to apply IB to the BC architectures, and what are its potential applications?}}
To ensure the generality of our findings, we categorize BC architectures based on their feature fusion methods into two types: spatial fusion and temporal fusion. This allows us to identify the applicable scenarios for each fusion method, and by incorporating IB, we uncover a series of interesting findings. 
Furthermore, our experiments reveal that regardless of the pre-training stage, the final fine-tuning phase, or the size of the dataset, incorporating IB by reducing redundancy enables the model to learn more robust features and improve performance, suggesting its potential applicability in these scenarios.


% As shown in~\cref{fig:intro} b), since mutual information between variables is difficult to estimate directly, we approximate it using Kullback–Leibler divergence~\cite{wang2009divergence}. With IB, the KL divergence is reduced to one-quarter of its original value, and the success rate improves by 7.7\%.

Our contributions are three-fold. 
(1) We extend the IB to BC and provide a comprehensive study on the impact of latent representation redundancy in BC for robot manipulation.
(2) We empirically demonstrate that minimizing redundancy in latent representations helps existing BC algorithms significantly improve generalization performance on the Cortexbench and LIBERO benchmarks across various settings, indirectly highlighting the considerable redundancy present in current robot trajectory datasets.
(3) We provide a detailed theoretical analysis explaining why IB enhances the transferability of BC methods.



% ------------------ 2. Related Work ------------------
\section{Related Work}
\label{sec:rw}

\subsection{Behavior Cloning in Robot Manipulation}

Behavioral Cloning (BC), first introduced by~\cite{pomerleau1991efficient}, is a well-known Imitation Learning (IL) algorithm that learns a policy by directly minimizing the discrepancy between the agent's actions and those of the expert in the demonstration data. 
% In robot manipulation, BC has successfully demonstrated its potential in learning tasks such as grasping and pick-and-place~\cite{pastor2009learning, stepputtis2020language}.
To learn more generalizable representations, one class of visual representation learning methods pre-train on large video datasets of robotics or humans, enabling rapid application of the pre-trained encoder to downstream robotic tasks. Notable examples include VC-1~\cite{majumdar2023we}, R3M~\cite{nair2023r3m}, and Voltron~\cite{karamcheti2023language} . 
Meanwhile, another line of research focuses on training on even more extensive and diverse datasets with larger models, such as Internet-scale visual question answering and robot trajectory data~\cite{brohan2023rt}, as well as a vast collection of Internet videos~\cite{cheang2024gr}.
Additionally, some methods further enhance generalization by incorporating additional sources of information. These include inferring textual descriptions based on the robot's current state~\cite{zawalski2024robotic}, leveraging visual trajectories~\cite{wen2023any} and generated images~\cite{tian2025predictive}, and integrating 3D visual information~\cite{goyal2023rvt}.
However, these methods have not deeply analyzed the redundancy in learned latent representations, and most also lack a solid theoretical foundation. 
Thus we extend the Information Bottleneck (IB) principle to BC, addressing this fundamental gap.


\subsection{Information Bottleneck in Robotics}
The Information Bottleneck (IB) principle was first proposed in~\cite{tishby2000information} within the context of information theory. 
Since then, it has been widely applied in deep learning and various downstream tasks to balance the trade-off between representation accuracy and complexity, including classification~\cite{federici2019learning}, segmentation~\cite{bardera2009image, lee2021reducing}, and generative tasks~\cite{jeon2021ib}.
In robotics learning, IB has found notable applications in reinforcement learning, where some works maximize the mutual information between the representation and the dynamics or value function, while restricting the information to encourage the encoder to extract only task-relevant features~\cite{kim2019curiosity, bai2021dynamic, he2024bridging}. In imitation learning, it has been introduced to solve copycat problems~\cite {wen2020fighting}.
Different from prior works, we introduce IB into Behavior Cloning to explore and experimentally validate the redundancy in latent representations in robotics. 
Additionally, we demonstrate its effectiveness through detailed theoretical analyses.



\begin{figure*}[ht]
\begin{center}
\centerline{\includegraphics[width=0.95\textwidth]{figures/model.pdf}}
\caption{
Model architectures used in this study. Based on feature fusion methods, we categorize the BC methods in robot manipulation into two types: spatial fusion and temporal fusion. 
After extracting features from each modality a), spatial fusion b) extracts spatial features at a given time step or concatenates features across multiple time steps using encoders like MLPs or CNNs. Temporal Fusion c) fuses input features by modeling dynamic relationships and dependencies between time steps using RNNs or Temporal Transformers. 
The latent representations are then decoded into actions via the policy head.
}
\label{fig:model}
\end{center}
\vskip -0.25in
\end{figure*}



% ------------------ 3. Preliminary ------------------
\section{Preliminary}
\label{sec:pre}

\subsection{Problem Setting of Behavior Cloning}
\label{subsec:ps}

% In robot learning, the problem is often formulated as a Markov Decision Process (MDP)~\cite{torabi2018behavioral}, where the next state $s_{t+1}$ depends on the current state $s_t$ and action $a_t$.
% Behavior Cloning (BC) is one of the most fundamental approaches in robot learning. Unlike reinforcement learning, which explicitly defines a reward function, BC approaches the problem as supervised learning, where the agent learns to replicate expert actions by directly mapping states to actions.
BC can be formulated as the Markov Decision Process (MDP) framework~\cite{torabi2018behavioral}, which is often defined without an explicitly specified reward function, to model sequential action generation problems. The concept of rewards is replaced with supervised learning, and the agent learns by mimicking expert actions.
Formally, in robot manipulation, the state at each timestep consists of visual observations $o_t$, the robot’s proprioceptive state $s_t$, and optionally a language instruction $l$.
Let $x_t = (o_t, s_t, l)$ represent the overall state. 
The policy $\pi$ maps a sequence of states to an action: 
$\hat{a}_t = \pi(x_{t-\tau:t})$, where $\tau$ indicates the length of the state history. For simplicity, we set $\tau=1$.
The optimization process can be formulated as:
\begin{equation}\begin{aligned}\label{func:vanilla_bc}
\pi^* = \operatorname{argmin}_\pi \mathbb{E}_{(x_{t}, a_t) \sim \mathcal{D}_e}\left[\mathcal{L}\left(\pi\left(x_{t}\right), a_t\right)\right],
\end{aligned}\end{equation}
where $\mathcal{D}_e$ is expert trajectory dataset and $a_t$ is action labels. In vanilla BC, $\mathcal{L}$ typically represents the mean squared error (MSE) loss function for continuous action spaces, or cross-entropy (CE) loss for discrete action spaces. 
In this study, we adopt the continuous action spaces with MSE loss:
\begin{equation}\begin{aligned}\label{func:vanilla_bc_loss}
\mathcal{L}_{\mathrm{BC}}=\mathbb{E}_{\left(x_t, a_t\right) \sim \mathcal{D}_e}\left[\left\|\pi\left(x_t\right)-a_t\right\|^2\right].
\end{aligned}\end{equation}
Building on this vanilla BC loss, some methods also introduce alignment loss~\cite{jang2022bc, ma2024contrastive} and reconstruction loss~\cite{radosavovic2023real, karamcheti2023language}. However, in this study, to more clearly illustrate the relationship with representation redundancy, we focus solely on the vanilla BC loss.


\subsection{Mutual Information Neural Estimation}

Estimating mutual information between variables directly is challenging, thus we use Mutual Information Neural Estimation (MINE)~\cite{belghazi2018mutual} to estimate it. 
MINE is based on neural networks, which can efficiently handle high-dimensional, continuous, discrete, and hybrid data types without requiring assumptions about the underlying distributions.
MINE estimates mutual information by training a classifier to differentiate between samples from the joint distribution $P_{XZ}$ and the product of the marginal distributions $P_{X} \otimes P_{Z}$ of the random variables $X$ and $Z$.
MINE uses a lower bound for mutual information based on the Donsker-Varadhan representation~\cite{donsker1983asymptotic} of the Kullback-Leibler (KL) divergence:
\begin{equation}\begin{aligned}\label{func:mine}
\mathcal{I}(X&; Z):=\mathcal{D}_{KL}(P_{XZ} || P_{X} \otimes P_{Z}) \geq \widehat{\mathcal{I}}_\theta^{(DV)}(X; Z) \\
&:=\mathbb{E}_{P_{XZ}}\left[T_\theta(x, z)\right]-\log \mathbb{E}_{P_{X} \otimes P_{Z}}\left[e^{T_\theta(x, z)}\right],
\end{aligned}\end{equation}
where $T_\theta: \mathcal{X} \times \mathcal{Z} \to \mathcal{R}$ is a discriminator function modeled by a neural network with parameters $\theta$. 
We empirically sample from $P_{XZ}$, and for $P_{X} \otimes P_{Z}$, we shuffle the samples from the joint distribution along the batch axis.



% ------------------ 4. Method ------------------
\section{Pipeline of BC with IB}

\subsection{Model Architecture}
\label{subsec:ma}

Before introducing IB, we first need to define the input $X$ and latent representations $Z$. 
According to current methods in BC for robot manipulation, as discussed in~\cref{subsec:ps}, the input is typically multimodal, meaning it not only includes RGB images but may also incorporate the robot’s proprioceptive state, language instructions, and more. 
Previous work has shown that proprioceptive states can lead to overfitting~\cite{wang2024scaling}.
Additionally, for the sake of convenience in visualizing $I(X, Z)$, we do not treat the image alone as $X$, as done in previous studies. Instead, we use features extracted from all modalities through respective feature extractors as our input $X$, i.e., 
\begin{equation}\begin{aligned}\label{func:feature}
x_t=\text{concat}(\text{Enc}_\text{o}(o_t), \text{Enc}_\text{s}(s_t), \text{Enc}_\text{l}(l)),
\end{aligned}\end{equation}
where $\text{Enc}_{(\cdot)}$ denotes the feature extractor of each modality.
Then, regarding how to process the input $X$, or how to fuse information from multiple modalities into latent representations $Z$, we categorize the BC methods in robot manipulation based on feature fusion methods into two types: spatial fusion and temporal fusion.

As illustrated in Figure~\ref{fig:model} b), spatial fusion involves extracting spatial features from data at a given time step or concatenating features across multiple time steps along the feature dimensions. This approach does not explicitly differentiate between time steps but instead processes the aggregated features as a whole, emphasizing the modeling of inter-feature relationships. The spatial fusion module can be implemented using Multi-Layer Perceptrons (MLPs), Convolutional Neural Networks (CNNs), Spatial Transformers, or even simple concatenation operations.
% These methods are primarily applied to obtain a highly generalized encoder through visual representation learning and learning from large human video datasets, which is then fine-tuned for downstream robotics tasks~\cite{nair2023r3m, karamcheti2023language, radosavovic2023real, majumdar2023we, zeng2024learning}. 
On the other hand, as illustrated in Figure~\ref{fig:model} c), temporal fusion integrates input features by capturing dynamic relationships and dependencies across time steps, enabling the modeling of both long-term and short-term temporal dynamics in sequential data. Temporal fusion modules can be implemented using Recurrent Neural Networks (RNNs), Long Short-Term Memory networks (LSTMs), or Temporal Transformers.
% These methods are commonly integrated into most approaches that use Transformer-based backbones~\cite{wu2023unleashing, brohan2023rt, li2023vision, liu2024libero}.

The latent representation $Z$, which integrates both spatial and temporal information, is then passed through a policy head to generate actions. Existing policy heads primarily focus on using MLP, Gaussian Mixture Model~(GMM), and diffusion-based policy (DP) heads~\cite{chi2023diffusion, reuss2024multimodal}. For simplicity and clearer empirical demonstration, we use an MLP as the policy head.


\subsection{Behavior Cloning with Information Bottleneck}

The Information Bottleneck (IB) principle is an information-theoretic approach aimed at extracting the most relevant information from an input variable $X$ with respect to an output variable, \textit{i.e.}, action $A$.
The central idea is to find a compressed representation $Z$ of $X$ that retains the relevant information needed to predict $A$, while discarding irrelevant parts of $X$ that do not contribute to predicting $A$. 
The relevant information is quantified as the mutual information $I(X; A)$, and the optimal representation $Z$ is the minimal sufficient statistic of $X$ with respect to $A$. 
In practice, this can be achieved by minimizing a Lagrangian that balances the trade-off between retaining predictive information and compressing the input, which can be formulated as:
\begin{equation}\begin{aligned}\label{func:ib}
\mathcal{L} = \beta I(X; Z) - I(Z; A),
\end{aligned}\end{equation}
where $\beta$ is the Lagrange multiplier that balances the trade-off between the compression ability and the predictive power. Thus~\cref{func:vanilla_bc_loss} can be modified as:
\begin{equation}\begin{aligned}\label{func:bcib}
\mathcal{L}_{\mathrm{BC-IB}}=\mathbb{E}_{(x_t, a_t) \sim \mathcal{D}_e}\left[\beta I(x_t, z_t) + \|\pi(x_t)-a_t\|^2\right],
\end{aligned}\end{equation}
where $z_t=F(x_t)$ and $F(\cdot)$ denotes the fusion module. 
% MSE can serve as a proxy for maximizing the mutual information $I(Z; Y)$, but they are not strictly identical. The MSE is a loss function that indirectly reflects how well $Z$ captures the relevant information about $Y$, and optimizing this loss will likely lead to an increase in $I(Z; Y)$, especially in contexts like regression or prediction tasks.


\subsection{Theoretical Analysis}
We provide a theoretical analysis of our BC-IB objective in~\cref{func:ib}. 
\cref{theorem:1} and \cref{theorem:2} reveal that the generalization error admits an upper bound governed by the mutual information between the input and the latent representation.
By minimizing this mutual information, we effectively tighten the upper bound, thereby improving the model’s generalization performance.
\cref{theorem:ours} elucidates the optimization challenges associated with complex input states. 
When the input \(O\) contains a large amount of state information, as depicted in~\cref{fig:model}, directly minimizing the mutual information between \(O\) and the latent representation \(Z\) becomes computationally impractical. 
To address this, the input \(O\) is first compressed into an intermediate embedding \(X\) via a fusion network \(f\), and the mutual information between \(X\) and \(Z\) is minimized instead.
The theorem establishes that, under certain conditions, this intermediate approach can approximate the ideal optimization results, provided that the embedding \(X\) sufficiently preserves the essential information from the original input \(O\). 

\begin{theorem}
\label{theorem:1}
Generalization Bound Adapted from \cite{IB1}. Let \( S = \{(x_t, a_t)\}_{t=1}^n \) denote the training data sampled from the same distribution as the random variable pair \( (X, A) \). Given the policy \( \pi \) trained on \( S \), the generalization error is given by:
\begin{equation}
\Delta(S) = \mathbb{E}_{X, A}[\ell(\pi(X), A)] - \frac{1}{n} \sum_{t=1}^n \ell(\pi(x_t), a_t).
\end{equation}
Using the Probably Approximately Correct (PAC) bound framework and the Asymptotic Equipartition Property (AEP)~\cite{AEP}, with probability at least \( 1 - \delta \), the following upper bound on the generalization error holds:
\begin{equation}
\Delta(S) \leq \sqrt{\frac{2 I(X; Z) + \log \frac{2}{\delta}}{2n}},
\end{equation}
where \( I(X; Z) \) represents the mutual information between the input \( X \) and the intermediate representation \( Z \), and \( \delta \) is the confidence level.
Details of proof can be seen in Appendix A of \cite{IB1}.
\end{theorem}

\begin{theorem}
\label{theorem:2}
Generalization Bound Adapted from \cite{IB2}.
Let \( S = \{(x_t, a_t)\}_{t=1}^n \) denote the training data sampled from the same distribution as the random variable pair \( (X, A) \). The generalization error is approximately bounded by:
\begin{equation}\label{eq:theorem2}
\Delta(S) \propto \sqrt{\frac{I(X; Z \mid A) + I(\phi^S; S)}{n}}, 
\end{equation}
where \( \phi^S \) is the encoder mapping the input \( X \) to the intermediate representation \( Z \).
This bound indicates that the generalization error is:
\begin{itemize}
    \item Positively correlated with \( I(X; Z \mid A) \), which captures mutual information between the input \( X \) and the latent representation \( Z \), conditioned on the actions \( A \).
    This term reflects that the IB compresses \( X \) into \( Z \) while preserving the relevant information for predicting \( A \).
    \item Positively correlated with \( I(\phi^S; S) \), which reflects the information content of the representation \( \phi \) for the given dataset \( S \).
\end{itemize}
\end{theorem}

\begin{theorem}
\label{theorem:ours}
Optimization Gap under Different Input Compression.
Let \( o \to x \to z \) form a Markov chain, where \( o \) is transformed into \( x \) by a network \( f \), and \( x \) is further transformed into \( z \) by a network \( \phi \). 
Let \( \phi_o = f \circ \phi \). Define two optimization problems:
\begin{align}
    (\theta^\varepsilon, \phi_o^\varepsilon) & = \arg\min_{\theta, \phi_o} \mathbb{E}_{P_{\phi_o}(o,x,z)} \left[ \log \frac{P_{\phi}(z|x)}{P_{\phi}(z)} - \frac{1}{\beta} J(z; \theta) \right],  \\
    (\theta^\star, \phi_o^\star) & = \arg\min_{\theta, \phi_o} \mathbb{E}_{P_{\phi_o}(o,z)} \left[ \log \frac{P_{\phi_o}(z|o)}{P_{\phi_o}(z)} - \frac{1}{\beta} J(z; \theta) \right]. 
\end{align}
Let $J^\varepsilon = \mathbb{E}_{P_{f^\varepsilon, \phi^\varepsilon}(o, x, z)}[J(z; \theta^\varepsilon)], \quad 
J^\star = \mathbb{E}_{P_{\phi_o^\star}(o, z)}[J(z; \theta^\star)]$.

Assume the mutual information gap satisfies the following, for any $\delta$ have 

\begin{equation}
I(o, z; \phi_o^\varepsilon) - I(o,z;\phi_o^*) \leq \frac{\delta}{\beta}.
\end{equation}

Then, the gap between the two optimizations is bounded as:
\begin{equation}
|J^\star - J^\varepsilon| \leq \delta.
\end{equation}
The detailed proof can be found in Appendix \ref{proof:ours}.
\end{theorem}




% ------------------ 5. Experiments ------------------
\section{Experiments}

\subsection{Embodied Evaluation}
\textbf{Benchmarks.} 
We mainly evaluate BC with IB across two benchmarks, CortexBench~\cite{majumdar2023we} and LIBERO~\cite{liu2024libero}. 
CortexBench is a single-task benchmark. For validation, we selected four imitation learning-related simulators, encompassing a total of 14 tasks: Adroit (2 tasks)~\cite{rajeswaran2018learning}, Meta-World (5 tasks)~\cite{yu2020meta}, DMControl (5 tasks)~\cite{tassa2018deepmind}, and TriFinger (2 tasks)~\cite{wuthrich2021trifinger}.
During evaluation, the number of validation trajectories is set to 25, 10, 25, and 25, respectively.
LIBERO is a language-conditioned multi-task benchmark. For evaluation, we select four suites: LIBERO-Goal (10 tasks), LIBERO-Object (10 tasks), LIBERO-Spatial (10 tasks), and LIBERO-Long (10 tasks), each focusing on the controlled transfer of knowledge related to task goals, objects, spatial information, and long-horizon tasks, respectively. During evaluation, the number of validation trajectories is set to 20.

\textbf{Baselines.} 
In CortexBench, we evaluate four representation learning models: R3M~\cite{nair2023r3m}, Voltron~\cite{karamcheti2023language}, VC-1~\cite{majumdar2023we}, and MPI~\cite{zeng2024learning}. In line with the original papers, we use the pre-trained versions of these models to facilitate their application to downstream tasks, keeping the image encoders frozen. Additionally, we introduce two full fine-tuning baselines by replacing these encoders with part of uninitialized ResNet-18~\cite{he2016deep} and ViT-S~\cite{dosovitskiy2020image}, denoted as ResNet and ViT, respectively. 
All methods employ the two fusion techniques described in~\cref{subsec:ma}. For spatial fusion, we use an MLP, and for temporal fusion, we utilize a Temporal Transformer. 
In LIBERO, we implement four vision-language policy networks. One of them uses a spatial fusion approach, which employs ResNet as the image encoder and an MLP as the fusion module, referred to as BC-MLP. The other three use temporal fusion. Following the original paper, we rename them based on the combination of the image encoder and fusion module: BC-RNN, BC-Transformer, and BC-VILT~\cite{liu2024libero}.
The policy head for all methods is fixed as an MLP. 
% All demonstrations are used to train these policies.
Notably, all baselines with IB are referred to as BC+IB.

\textbf{Implementation.} 
In CortexBench, for four partial fine-tuning methods, we train for 100 epochs on each task using the Adam optimizer with a learning rate of 1e-3, a batch size of 512, and weight decay of 1e-4, with learning rate decay applied using a cosine annealing schedule. For the two full fine-tuning methods, we train for 50 epochs with a learning rate of 1e-4 and a batch size of 256. All other parameters remain unchanged.
In LIBERO, we train for 50 epochs using the AdamW optimizer with a learning rate of 1e-4 and a batch size of 64, decayed using a cosine annealing schedule.
For BC+IB methods, the model used in MINE consists of a two-layer MLP, with a learning rate of 1e-5. The Lagrange multiplier in~\cref{func:bcib} ranges from 1e-4 to 5e-3 in this work.

\textbf{Model Selection.}
For single-task benchmark CortexBench, we test the model every 5 or 10 epochs and select the model with the highest success rate.
For multi-task benchmark LIBERO, we select the model from the final epoch.

The appendix will provide detailed descriptions of each benchmark (\cref{sec:appendix_benchmarks}), all baselines (\cref{sec:appendix_baselines}), implementation details (\cref{sec:appendix_implementations}), and the rationale behind the model selection (\cref{sec:appendix_model_selection}).


\subsection{Performance on Cortexbench}

\textbf{The Selection of Fusion Method.} We first evaluate the effectiveness of the two fusion methods in the baselines on CortexBench, with the results shown in~\cref{fig:fusion}.

\textbf{\textit{Finding 1:}}
For simple single-task scenarios, spatial fusion is more efficient and effective than temporal fusion. As shown in~\cref{fig:fusion} b), the performance of methods with temporal fusion drops significantly. From~\cref{fig:fusion} a), this can be attributed to the slower loss reduction in methods using temporal fusion, which results in higher loss at the same training epoch. 
Therefore, we focus exclusively on presenting the results for methods employing spatial fusion.

\textbf{Results.} We next report the performance, \textit{i.e.,} success rate, of the baselines and baselines with IB on the single-task benchmark CortexBench in~\cref{tab:cortex} with a full-shot setting. Based on results, we derive the following findings.

\textbf{\textit{Finding 2:}} 
Whether using full fine-tuning or partial fine-tuning, all vanilla BC methods with different visual backbones incorporating IB outperform their vanilla counterparts across the board. 
In some benchmarks, the improvements are substantial. For example, ResNet with IB achieves a 10.01\% improvement on DMControl, and VC-1 with IB shows a 4.80\% improvement on Meta-World. In~\cref{sec:appendix_task-wise_exp}, we report the success rate for each task, where significant improvements can be observed in certain tasks.

\textbf{\textit{Finding 3:}} 
Finding 2 implicitly suggests that the latent representation $Z$ derived from input $X$ is redundant. Therefore, compressing information from input is essential, which can further enhance performance.

\textbf{\textit{Finding 4:}} 
For simple single-task downstream tasks, full fine-tuning of a simple, uninitialized model (ResNet) is sufficient and may even outperform a pre-trained larger model. However, the latter is more efficient for faster fine-tuning and deployment, and proves to be more effective for more complex tasks~\cite{burns2023makes}. 
% When fully fine-tuning a ViT-base model, slow data fitting leads to suboptimal performance. However, this issue is mitigated in pre-trained representation learning methods, highlighting the importance of pre-trained features for robotics.


\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figures/fusion_method.pdf}}
\caption{
a) BC loss variation for ResNet in spatial and temporal fusion methods on the bin-picking task of the Meta-World. b) Averaged success rates of ResNet and VC1 in spatial and temporal fusion methods across the Meta-World and DMControl.
}
\label{fig:fusion}
\end{center}
\vskip -0.2in
\end{figure}


\input{tables/cortexbench}
\input{tables/libero}


\begin{figure*}[ht]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{figures/mi.pdf}}
\caption{Comparison of vanilla BC and BC+IB on the LIBERO benchmark in terms of success rate (sr) and mutual information. BC-VILT is denoted as BC. BC+IB consistently achieves lower $I(X, Z)$ and higher success rates.
}
\label{fig:mi}
\end{center}
\vskip -0.2in
\end{figure*}


\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figures/beta.pdf}}
\vskip -0.05in
\caption{Effect of the Lagrange multiplier $\beta$ in BC-VILT+IB across three suites of LIBERO. When $\beta$=0, the method reduces to vanilla BC-VILT. 
% Within a certain range of $\beta$, performance improvements can be observed.
}
\label{fig:beta}
\end{center}
\vskip -0.2in
\end{figure}


\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=0.98\columnwidth]{figures/few-shot.pdf}}
\caption{Comparison of the success rates of BC-VILT+IB trained with 1, 5, 10, and 20 demonstrations against the vanilla BC-VILT in the LIBERO-Goal suite.
}
\label{fig:few-shot}
\end{center}
\vskip -0.2in
\end{figure}




\subsection{Performance on LIBERO}

\textbf{Results.} We report the performance on the multi-task benchmark LIBERO with a full-shot setting in~\cref{tab:libero}.

\textbf{\textit{Finding 5:}} 
For more complex language-conditioned multi-task scenarios, all baselines with different backbones incorporating IB consistently show performance improvements across all LIBERO benchmarks. For example, BC-VILT achieves large gains of 7.66\% and 9.00\% on LIBERO-Goal and LIBERO-Object, respectively, while BC-RNN shows a significant improvement of 10.83\% on LIBERO-Goal. 
IB proves to be more effective in more complex environments and settings. We attribute this to the difference in task complexity: in CortexBench, the history length is 3, while in LIBERO, it is 10, with LIBERO being a multi-task benchmark and CortexBench being single-task benchmark. The increased data complexity (task quantity and input information) suggests a higher level of data redundancy, making IB even more effective.

\textbf{\textit{Finding 6:}} 
We observe that in complex multi-task scenarios with more intricate inputs, such as a greater number of input modalities and extended historical information, using the Temporal Transformer in temporal fusion proves to be more effective than both spatial fusion and RNN-based temporal fusion. The evidence lies in the fact that the average success rates of BC-Transfomer and BC-VILT are over 30\% higher than those of BC-MLP and BC-RNN.
This is likely because Temporal Transformers excel in handling long-range interactions and capture dynamic dependencies across time steps, where RNNs and spatial fusion methods may struggle.
This finding, together with Finding 1, underscores the specific scenarios in which each fusion method is most applicable.

% This is likely because temporal fusion models the dynamic dependencies across time steps, making it better suited for tasks where the relationships between past and current inputs are crucial. While RNNs also capture temporal dependencies, the Temporal Transformer excels at handling long-range dependencies and complex interactions over time, particularly in environments with increased input complexity and longer histories, where RNNs may struggle to maintain the necessary context. In contrast, spatial fusion methods are less effective in capturing such temporal dynamics.


\textbf{\textit{Finding 7:}} 
IB is particularly effective for tasks requiring diverse feature extraction, such as distinguishing distinct task objectives or differentiating between various objects, as in LIBERO-Goal and LIBERO-Object. By filtering out irrelevant information, IB facilitates better generalization and more compact representations. However, its impact is less pronounced in spatial and long-horizon tasks, such as LIBERO-Spatial and LIBERO-Long, which heavily depend on structural and sequential dependencies that may be disrupted by excessive compression.


\subsection{More Analysis}

\textbf{Effect of the Lagrange multiplier $\beta$ of~\cref{func:bcib}.} 
This experiment evaluates how incorporating IB enhances performance. Since the MINE model’s parameters are fixed, the key difference between BC+IB and BC lies in the parameter $\beta$, which balances compression and predictive power. For LIBERO experiments, $\beta$ is explored within {1e-4, 1e-3, 5e-3, 1e-2}.
\textbf{\textit{Finding 8:}} As shown in~\cref{fig:beta}, IB improves performance within a specific $\beta$ range, with a peak observed at an undetermined value. However, across all experiments, $\beta$ around 1e-4 consistently yields stable improvements. The selected $\beta$ values for each experiment are detailed in~\ref{sec:appendix_task-wise_exp}.


% This experiment aims to validate how incorporating IB enhances the method's effectiveness. Since the parameters of the MINE model used to learn mutual information are fixed, the only fundamental difference between BC+IB and IB lies in the parameter $\beta$, which controls the balance between compression ability and predictive power. 
% For the experiments conducted on LIBERO, the range of $\beta$ with a focus on values at 1e-4, 1e-3, 5e-3, and 1e-2.
% \textbf{\textit{Finding 8:}} As shown in~\cref{fig:beta}, the results demonstrate that incorporating IB leads to performance improvements within a certain range of $\beta$, with a \textit{peak} observed at some point. Unfortunately, we were unable to determine the exact $\beta$ value corresponding to this peak. However, based on our all experiments, we find that a $\beta$ value around 1e-4 consistently yields stable performance improvements. We report all the $\beta$ values selected for these experiments in~\ref{sec:appendix_task-wise_exp}.


\textbf{Effect of the Number of Demonstrations.}
We evaluate IB's effectiveness in few-shot settings, as few-shot learning is crucial for fine-tuning on domain-specific tasks in real-world applications.
\textbf{\textit{Finding 9}}: As shown in~\cref{fig:few-shot}, IB significantly improves performance even with limited data, highlighting its effectiveness in real-world scenarios where data is scarce. This further underscores the potential of IB in improving model generalization in practical settings.

\textbf{Visualizations of $I(X, Z)$.}
As shown in~\cref{fig:mi}, BC+IB achieves a greater reduction in $I(X, Z)$ compared to vanilla BC, leading to significant performance improvements and further validating the effectiveness of IB. For instance, in LIBERO-Goal, incorporating IB reduces $I(X, Z)$ to one-quarter of its original value, resulting in a 7.7\% increase in success rate.

\textit{We provide real-world experiments in~\cref{sec:appendix_real_world_exp}}.


% ------------------ 6. Limitations ------------------
\section{Limitations and Discussion}
While our work provides extensive experimental validation of the effectiveness of IB and the necessity of input redundancy reduction in robotics representation learning, several limitations remain.
First, for scalability, we do not incorporate large models like vision-language-action models due to high computational and time costs. Consequently, architectures like RT-2~\cite{brohan2023rt} and OpenVLA~\cite{kim2024openvla}, which forgo a policy head and treat actions as text tokens, have not been explored. This is an avenue we aim to investigate in future work.
Second, we have not examined alternative policy heads, such as diffusion-based~\cite{chi2023diffusion} or transformer-based policy heads~\cite{team2024octo}, which may further enhance performance.
Third, while we evaluate our method on controlled benchmarks, its robustness to domain shifts, such as environmental or task variations, remains underexplored.
We hope our work will inspire future research and contribute to the ongoing development and refinement of these methods.


% ------------------ 7. Conclusions ------------------
\section{Conclusions}
In this study, we investigated the redundancy in representations for Behavior Cloning in robot manipulation and introduced the Information Bottleneck principle to mitigate this issue. By incorporating IB, we aimed to filter out redundant information in latent representations while preserving task-relevant features. Extensive experiments across various representation learning methods on CortexBench and LIBERO revealed insightful findings and demonstrated that IB significantly improves performance across diverse tasks and architectures. We hope our work will inspire further integration of information-theoretic principles into robotics and foster deeper theoretical analysis in this domain.


% ------------------ 8. Acknowledgements ------------------
\section*{Acknowledgements}
This work was supported by the National Natural Science Foundation of China with grant numbers (U21A20485 and 62436005), and the Fundamental Research Funds for Xi’an Jiaotong University under Grants xzy022024012.


% ------------------ 9. Impact Statement ------------------
% \section*{Impact Statement}
% This paper presents work whose goal is to advance the field of Machine Learning in Robotics. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{main}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn


\section{Proof of Theorem \ref{theorem:ours}} \label{proof:ours}

% \begin{theorem}[Optimization Gap under Different Input Compression]
% Let \( o \to x \to z \) form a Markov chain, where \( o \) is transformed into \( x \) by a network \( f \), and \( x \) is further transformed into \( z \) by a network \( \phi \). 
% Let \( \phi_o = f \circ \phi \). Define two optimization problems:
% \begin{align}
%     (\theta^\varepsilon, \phi^\varepsilon, f^\varepsilon) & = \arg\min_{\theta, \phi, f} \mathbb{E}_{P_{f, \phi}(o,x,z)} \left[ \log \frac{P_{\phi}(z|x)}{P_{\phi}(z)} - \frac{1}{\beta} J(z; \theta) \right],  \\
%     (\theta^\star, \phi_o^\star) & = \arg\min_{\theta, \phi_o} \mathbb{E}_{P_{\phi_o}(o,z)} \left[ \log \frac{P_{\phi_o}(z|o)}{P_{\phi_o}(z)} - \frac{1}{\beta} J(z; \theta) \right]. 
% \end{align}
% Let 
% \begin{equation}
% J^\varepsilon = \mathbb{E}_{P_{f^\varepsilon, \phi^\varepsilon}(o, x, z)}[J(z; \theta^\varepsilon)],
% \end{equation}
% and 
% \begin{equation}
% J^\star = \mathbb{E}_{P_{\phi_o^\star}(o, z)}[J(z; \theta^\star)].
% \end{equation}

% Assume the mutual information gap satisfies the following equation,
% \begin{equation}
% I(o, z; \phi_o^\varepsilon) - I(o,z;\phi_o^*) \leq \frac{\delta}{\beta}.
% \end{equation}

% Then, the gap between the two optimizations is bounded as:
% \begin{equation}
% |J^\star - J^\varepsilon| \leq \delta.
% \end{equation}
% \end{theorem}

\begin{proof}
The first optimization problem optimizes \( I(x, z) \), which imposes a looser constraint on \( I(o, z) \), as it does not directly regulate the information flow from \( o \) to \( z \). In contrast, the second optimization problem directly constrains \( I(o, z) \), which may result in a smaller \( I(o, z; \phi_o^\star) \). Therefore, we have:
\begin{equation}
    I(o, z; \phi_o^\varepsilon) \geq I(o, z; \phi_o^\star).
\end{equation}

From the optimization objectives of the two problems, it follows that:
\begin{equation}
    I(o, z; \phi_o^\varepsilon) - \frac{1}{\beta} J^\varepsilon \geq I(o, z; \phi_o^\star) - \frac{1}{\beta} J^\star.
\end{equation}

Rearranging this inequality gives:
\begin{equation}
    |J^\varepsilon - J^\star| \leq \beta \cdot \left( I(o, z; \phi_o^\varepsilon) - I(o, z; \phi_o^\star) \right).
\end{equation}

According to the assumption that the mutual information gap is bounded:
\begin{equation}
    I(o, z; \phi_o^\varepsilon) - I(o, z; \phi_o^\star) \leq \frac{\delta}{\beta},
\end{equation}
we substitute this bound into the inequality:
\begin{equation}
    |J^\varepsilon - J^\star| \leq \beta \cdot \frac{\delta}{\beta} = \delta.
\end{equation}

Thus, the performance gap is bounded as:
\begin{equation}
    |J^\star - J^\varepsilon| \leq \delta.
\end{equation}

This completes the proof.
\end{proof}






\section{Details of Experiment Setting}

\subsection{Details of Benchmarks}
\label{sec:appendix_benchmarks}

\subsubsection{CortexBench}

We provide a detailed overview of the four imitation learning benchmarks used in CortexBench~~\cite{majumdar2023we}. 
CortexBench is a single-task benchmark that includes 7 selected simulators, collectively offering 17 different embodied AI tasks spanning locomotion, navigation, and both dexterous and mobile manipulation. 
Three of the simulators are related to reinforcement learning, and thus are excluded from our analysis. 
The remaining four simulators, with a total of 14 tasks, are retained for validation: Adroit (2 tasks)~\cite{rajeswaran2018learning}, Meta-World (5 tasks)~\cite{yu2020meta}, DMControl (5 tasks)~\cite{tassa2018deepmind}, and TriFinger (2 tasks)~\cite{wuthrich2021trifinger}.

First, Adroit~\cite{rajeswaran2018learning} is a suite of dexterous manipulation tasks in which an agent controls a 28-DoF anthropomorphic hand. It includes two of the most challenging tasks: Relocate and Reorient-Pen. In these tasks, the agent must manipulate an object to achieve a specified goal position and orientation. Each task consists of 100 demonstrations.

Second, MetaWorld~\cite{yu2020meta} is a collection of tasks in which agents command a Sawyer robot arm to manipulate objects in a tabletop environment. CortexBench includes five tasks from MetaWorld: Assembly, Bin-Picking, Button-Press, Drawer-Open, and Hammer. Each task consists of 25 demonstrations.

Third, DeepMind Control (DMControl)~\cite{tassa2018deepmind} is a widely studied image-based continuous control benchmark, where agents perform locomotion and object manipulation tasks. CortexBench includes five DMC tasks: Finger-Spin, Reacher-Hard, Cheetah-Run, Walker-Stand, and Walker-Walk. Each task consists of 100 demonstrations.

Lastly, TriFinger (TF)~\cite{wuthrich2021trifinger} is a robot consisting of a three-finger hand with 3-DoF per finger. CortexBench includes two tasks from TriFinger: Push-Cube and Reach-Cube. Each task consists of 100 demonstrations.

Although only Meta-World is strictly a robot manipulation benchmark, we include all tasks to demonstrate the effectiveness of IB comprehensively. 
We provide visualizations for one task from each benchmark, as shown in~\cref{fig:cortex_case}.


\begin{figure*}[ht]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{figures/cortex_case.pdf}}
\caption{Visualizations for one task from each suite in CortexBench.
}
\label{fig:cortex_case}
\end{center}
\vskip -0.2in
\end{figure*}

\begin{figure*}[ht]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{figures/libero_case.pdf}}
\caption{Visualizations for one task from each suite in LIBERO.
}
\label{fig:libero_case}
\end{center}
\vskip -0.2in
\end{figure*}

\subsubsection{LIEBRO}
LIBERO is a language-conditioned multi-task benchmark comprising 130 tasks across five suites. LIBERO~\cite{liu2024libero} has four task suites: LIBERO-Goal (10 tasks), LIBERO-Object (10 tasks), LIBERO-Spatial (10 tasks), and LIBERO-100 (100 tasks).

LIBERO-Goal tasks share the same objects with fixed spatial relationships but differ in task goals, requiring the robot to continually acquire new knowledge about motions and behaviors. 
Examples include (1) opening the middle drawer of the cabinet, (2) opening the top drawer and placing the bowl inside, (3) pushing the plate to the front of the stove, (4) placing the bowl on the plate, (5) placing the bowl on the stove, (6) placing the bowl on top of the cabinet, (7) placing the cream cheese in the bowl, (8) placing the wine bottle on the rack, (9) placing the wine bottle on top of the cabinet, and (10) turning on the stove.

LIBERO-Object tasks involve the robot picking and placing unique objects, requiring it to continually learn and memorize new object types. 
Examples include (1) picking up the alphabet soup and placing it in the basket, (2) picking up the BBQ sauce and placing it in the basket, (3) picking up the butter and placing it in the basket, (4) picking up the chocolate pudding and placing it in the basket, (5) picking up the cream cheese and placing it in the basket, (6) picking up the ketchup and placing it in the basket, (7) picking up the milk and placing it in the basket, (8) picking up the orange juice and placing it in the basket, (9) picking up the salad dressing and placing it in the basket, and (10) picking up the tomato sauce and placing it in the basket.

LIBERO-Spatial requires the robot to place a bowl, selected from the same set of objects, onto a plate. The robot must continually learn and memorize new spatial relationships.
Examples include (1) picking up the black bowl between the plate and the ramekin and placing it on the plate, (2) picking up the black bowl from the table center and placing it on the plate, (3) picking up the black bowl in the top drawer of the wooden cabinet and placing it on the plate, (4) picking up the black bowl next to the cookie box and placing it on the plate, (5) picking up the black bowl next to the plate and placing it on the plate, (6) picking up the black bowl next to the ramekin and placing it on the plate, (7) picking up the black bowl on the cookie box and placing it on the plate, (8) picking up the black bowl on the ramekin and placing it on the plate, (9) picking up the black bowl on the stove and placing it on the plate, and (10) picking up the black bowl on the wooden cabinet and placing it on the plate.

LIBERO-100 consists of 100 tasks involving diverse object interactions and versatile motor skills. It can be divided into LIBERO-10 (10 tasks) and LIBERO-90 (90 tasks), where we use LIBERO-10, also referred to as LIBERO-Long, as our benchmark. 
LIBERO-Long requires the robot to learn long-horizon tasks, demanding it to plan and execute actions over extended periods to accomplish complex objectives.
Examples include (1) turning on the stove and placing the moka pot on it, (2) putting the black bowl in the bottom drawer of the cabinet and closing it, (3) putting the yellow and white mug in the microwave and closing it, (4) putting both moka pots on the stove, (5) putting both the alphabet soup and the cream cheese box in the basket, (6) putting both the alphabet soup and the tomato sauce in the basket, (7) putting the cream cheese box and the butter in the basket, (8) putting the white mug on the left plate and the yellow and white mug on the right plate, (9) putting the white mug on the plate and the chocolate pudding to the right of the plate, and (10) picking up the book and placing it in the back compartment of the caddy.

We provide visualizations for one task from each suite, as shown in~\cref{fig:libero_case}.


\subsection{Details of Baselines}
\label{sec:appendix_baselines}

\subsubsection{Baselines in CortexBench}

In CortexBench, the classification of baselines is primarily based on the visual encoder used.

For full fine-tuning baselines, ResNet~\cite{he2016deep} and ViT~\cite{dosovitskiy2020image} are baselines built from the original ResNet-18 and ViT-S models, using only a portion of their architecture and with uninitialized parameters.

For partial fine-tuning baselines, 
R3M~\cite{nair2023r3m} pre-trains a ResNet model on human videos~\cite{grauman2022ego4d} using time contrastive learning and video-language alignment. For direct comparison, we use the version reproduced with ViT.
VC-1~\cite{majumdar2023we} pre-trains a ViT using
Masked Auto-Encoding (MAE)~\cite{he2022masked} on a mix of human-object interaction videos, navigation, and the ImageNet~\cite{deng2009imagenet} datasets.
Voltron~\cite{karamcheti2023language}, a framework for language-driven representation learning from human videos and associated captions, pre-trains a ViT using MAE.
MPI~\cite{zeng2024learning}, a framework for interaction-oriented representation learning, directs the model to predict transition frames and detect manipulated objects using keyframes as input. It learns from human videos and associated captions.

If a proprioceptive state is available, it is first transformed into embeddings using a linear layer. Depending on the fusion method, these embeddings are then combined with the visual embeddings. For spatial fusion, an MLP is used, while for temporal fusion, a temporal transformer is employed. The fused features are ultimately processed through an MLP-based policy head to generate actions.


\subsubsection{Baselines in LEBERO}

Similar to previous work~\cite{zhu2024spa}, the baselines in LIBERO largely follow the three architectures outlined in the original paper~\cite{liu2024libero}, which we have renamed as BC-RNN, BC-Transformer, and BC-VILT. These three baselines are part of the temporal fusion methods.

BC-RNN uses a ResNet as the visual backbone to encode per-step visual observations, with an LSTM as the temporal backbone to process a sequence of encoded visual information. The language instruction is incorporated into the ResNet features using the FiLM method~\cite{perez2018film}, and is added to the LSTM inputs.

BC-Transformer employs a similar ResNet-based visual backbone but instead uses a transformer decoder~\cite{vaswani2017attention} as the temporal backbone to process outputs from ResNet, which are temporal sequences of visual tokens. The language embedding is treated as a separate token alongside the visual tokens in the input to the transformer.

BC-VILT utilizes a ViT as the visual backbone and a transformer decoder as the temporal backbone. The language embedding is treated as a separate token in the inputs of both the ViT and the transformer decoder. All temporal backbones output a latent vector at each decision-making step.

Additionally, we introduce a spatial fusion method, BC-MLP, which uses a similar ResNet-based visual backbone. The visual and language embeddings are directly concatenated and input into an MLP for fusion. After feature fusion, all methods use an MLP-based policy head to generate actions.


\input{tables/appendix_para}


\subsection{Details of Implementations}
\label{sec:appendix_implementations}

For all experiments, we use a single Nvidia V100 GPU (CUDA 11.3) with 12 CPUs for training and evaluation.

\subsubsection{CortexBench}
We largely adhere to the original parameter settings from the CortexBench paper~\cite{majumdar2023we}. 
For both full fine-tuning and partial fine-tuning methods, as shown in~\cref{tab:appendix_cortex_para}, training parameters are presented with full fine-tuning on the left and partial fine-tuning on the right.
For model architecture parameters, spatial fusion employs a 4-layer MLP, where the input dimension matches the output dimension of the image encoder. The features are first downsampled and then upsampled to maintain consistency with the input dimension.
For temporal fusion, each modality's feature dimension is first projected to 64, then processed through a four-layer, six-head Transformer.
For dataset configurations, we adopt a full-shot setting, training with 100 demonstrations for Adroit, 100 for DMControl, 25 for MetaWorld, and 100 for TriFinger. During evaluation, we assess performance using 25, 10, 25, and 25 test trajectories, respectively.


\begin{figure*}[ht]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{figures/model_selection.pdf}}
\caption{Comparison of success rate curves between single-task and multi-task training.
}
\label{fig:model_selection}
\end{center}
\vskip -0.2in
\end{figure*}


\subsubsection{LIBERO}
We largely follow the original parameter settings from the LIBERO paper~\cite{liu2024libero}. The training parameters are provided in~\cref{tab:appendix_libero_para}.
Regarding model architecture, Appendix A.1 of the original LIBERO paper~\cite{liu2024libero} describes the model parameters for BC-RNN, BC-Transformer, and BC-VILT. Here, we present the model parameters for BC-MLP, which shares the same architecture as BC-Transformer except for the fusion module. Specifically, BC-MLP employs a four-layer MLP with a hidden size of 256 as its fusion module.
For dataset configurations, in the full-shot setting, we use five demonstrations for evaluation, leaving the remaining 45 for training, which is considered the full-shot setting in our experiments. However, full-shot training typically refers to utilizing all 50 demonstrations without allocating any for evaluation, as robotic systems can operate without separate validation data.

\textit{For BC+IB, all training and model parameters remain identical to those of BC, except for the IB-specific parameters. The details of IB-related parameters are provided in~\cref{tab:appendix_ib_para}, while the specific values of the Lagrange multiplier are thoroughly discussed in~\cref{sec:appendix_task-wise_exp}.}


\subsection{Details of Model Selection}
\label{sec:appendix_model_selection}

\subsubsection{CortexBench}
In the single-task dataset CortexBench, we observed that the learning curves of certain tasks exhibit significant oscillations, such as the assemble task in MetaWorld, as shown in~\cref{fig:model_selection} a). Previous studies often record performance at intervals of many epochs or steps, selecting either the highest value~\cite{majumdar2023we} or the average of multiple peak values~\cite{ze20243d}. Following~\cite{majumdar2023we}, we directly use the highest value to explore the model's full potential on the given task.

\subsubsection{LIBERO}
For the multi-task dataset LIBERO, we observed that while the learning curve for individual tasks may still oscillate, a decrease in success rate for one task is often accompanied by an increase for another. This trade-off results in smoother overall learning curves across multiple tasks, as shown in~\cref{fig:model_selection} b). To make model selection more practical and representative, we directly select the model from the final epoch.



\input{tables/cortex_appendix}
\input{tables/libero_appendix}

\section{Additional Experiment Results}

\subsection{Details of Simulation Experiments}
\label{sec:appendix_task-wise_exp}

We provide the task-wise results and corresponding 
$\beta$ values for CortexBench. The results for Adroit and TriFinger in~\cref{tab:cortex_ad_tri_appendix}, DMControl in~\cref{tab:cortex_dmc_appendix}, and MetaWorld are shown in~\cref{tab:cortex_mw_appendix}.
Across almost all tasks in CortexBench, incorporating the IB consistently improves performance compared to vanilla BC methods. Notably, models such as ResNet+IB, VC-1+IB, and MPI+IB often achieve the highest success rates, demonstrating the benefits of redundancy reduction in latent representations. In most cases, properly tuning $\beta$ (e.g., selecting values in the range of 1e-4 to 5e-3) leads to noticeable improvements.

We also provide the corresponding 
$\beta$ values for LIBERO in~\cref{tab:libero_appendix}.
Across all suites in LIBERO, incorporating the IB consistently improves performance over vanilla BC methods. The chosen values of $\beta$ (e.g., 1e-4 to 5e-3) effectively balance compression and predictive power.


\begin{figure*}[ht]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{figures/real-world.pdf}}
\caption{Real-world robot experiments conducted on a table setup involving two tasks.
The left figure illustrates the experimental setup.
The top right figure presents an example of the predicted particle trajectories alongside the policy execution.
The bottom right figure provides the quantitative results,
}
\label{fig:real_world}
\end{center}
\vskip -0.2in
\end{figure*}


\subsection{Real-world Experiments}
\label{sec:appendix_real_world_exp}

As shown in~\cref{fig:real_world}, our real-world experiments involve a 6-DOF UR5 arm equipped with a Robotiq 2F-85 gripper and a RealSense L515 camera (base camera) for RGB image capture. 
We designed two simple tabletop manipulation tasks with manipulation skills:

\begin{itemize}
\item \textbf{Pick}. The robot grips the pink cup on the table and lifts it up in the sky.
\item \textbf{Pick and Place}. The robot grips the pink cup on the table and places it in the bowl.
\end{itemize}

The demonstrations used for training both BC and BC+IB policies are collected using a 3D mouse, with 25 demonstrations recorded for pick tasks and 50 for more challenging pick-and-place tasks, utilizing only the base camera.
We adopt VC-1~\cite{majumdar2023we} as the baseline BC method, where pre-trained representations remain frozen during policy training, maintaining consistency with the simulation setup. To ensure a fair comparison, all methods are evaluated under identical initial conditions for each task.

\begin{wrapfigure}{r}{0.5\textwidth}
  \begin{center}
    \includegraphics[width=0.5\textwidth]{figures/few-shot_appendix.pdf}
  \end{center}
  \caption{Comparison of the success rates of BC-VILT+IB trained with 10 demonstrations against the vanilla BC-VILT across four LIBERO suites.
  }
  \label{fig:appendix_few-shot}
  \vskip -0.2in
\end{wrapfigure}

The real-world robot experiments demonstrate that incorporating IB into BC significantly enhances task success rates. As shown in Table 10, BC+IB achieves a 9/10 success rate for picking the pink cup, compared to 5/10 for standard BC. Similarly, in the more complex task of placing the pink cup into the bowl, BC+IB outperforms BC with a success rate of 6/10 versus 3/10. This indicates that reducing redundancy in latent representations improves both grasping stability and overall task execution.


\subsection{Extention to Few-shot Setting}
We further evaluate the effectiveness of IB in a few-shot setting by conducting experiments with BC-VILT across multiple suites in LIBERO, as illustrated in~\cref{fig:appendix_few-shot}. The model is trained with only 10 demonstrations. The results consistently show that incorporating the IB improves success rates across all LIBERO suites, highlighting its efficacy in few-shot learning scenarios.


% \subsection{Grad-CAM Visualizations}


\subsection{Experiments on LIBERO-Object of LIBERO}
We observe that in the LIBERO-Object suite, the success rate does not consistently improve with an increasing number of demonstrations. Specifically, in the 10-shot setting, BC-VILT achieves a success rate of 56.17\%, but in the full-shot setting, its performance drops to 43.00\%. We hypothesize that this decline stems from inherent data distribution characteristics within the benchmark. In particular, we suspect that task-level imbalance in the dataset may contribute to overfitting.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
