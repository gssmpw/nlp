@ARTICLE{CP_Wildfire,
  author={Xu, Chen and Xie, Yao and Vazquez, Daniel A. Zuniga and Yao, Rui and Qiu, Feng},
  journal={IEEE Journal on Selected Areas in Information Theory}, 
  title={Spatio-Temporal Wildfire Prediction Using Multi-Modal Data}, 
  year={2023},
  volume={4},
  number={},
  pages={302-313},
  keywords={Predictive models;Sensors;Real-time systems;Spatiotemporal phenomena;Multisensor systems;Fire safety;Wildfires;Spatial-temporal point process;conformal prediction;multi-sensor network;fire safety},
  doi={10.1109/JSAIT.2023.3276054}}

@article{GENEVA2020109056,
title = {Modeling the dynamics of PDE systems with physics-constrained deep auto-regressive networks},
journal = {Journal of Computational Physics},
volume = {403},
pages = {109056},
year = {2020},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2019.109056},
url = {https://www.sciencedirect.com/science/article/pii/S0021999119307612},
author = {Nicholas Geneva and Nicholas Zabaras},
keywords = {Physics-informed machine learning, Auto-regressive model, Deep neural networks, Convolutional encoder-decoder, Uncertainty quantification, Dynamic partial differential equations},
abstract = {In recent years, deep learning has proven to be a viable methodology for surrogate modeling and uncertainty quantification for a vast number of physical systems. However, in their traditional form, such models can require a large amount of training data. This is of particular importance for various engineering and scientific applications where data may be extremely expensive to obtain. To overcome this shortcoming, physics-constrained deep learning provides a promising methodology as it only utilizes the governing equations. In this work, we propose a novel auto-regressive dense encoder-decoder convolutional neural network to solve and model non-linear dynamical systems without training data at a computational cost that is potentially magnitudes lower than standard numerical solvers. This model includes a Bayesian framework that allows for uncertainty quantification of the predicted quantities of interest at each time-step. We rigorously test this model on several non-linear transient partial differential equation systems including the turbulence of the Kuramoto-Sivashinsky equation, multi-shock formation and interaction with 1D Burgers' equation and 2D wave dynamics with coupled Burgers' equations. For each system, the predictive results and uncertainty are presented and discussed together with comparisons to the results obtained from traditional numerical analysis methods.}
}

@article{LiPino2024,
author = {Li, Zongyi and Zheng, Hongkai and Kovachki, Nikola and Jin, David and Chen, Haoxuan and Liu, Burigede and Azizzadenesheli, Kamyar and Anandkumar, Anima},
title = {Physics-Informed Neural Operator for Learning Partial Differential Equations},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
url = {https://doi.org/10.1145/3648506},
doi = {10.1145/3648506},
abstract = {In this article, we propose physics-informed neural operators (PINO) that combine training data and physics constraints to learn the solution operator of a given family of parametric Partial Differential Equations (PDE). PINO is the first hybrid approach incorporating data and PDE constraints at different resolutions to learn the operator. Specifically, in PINO, we combine coarse-resolution training data with PDE constraints imposed at a higher resolution. The resulting PINO model can accurately approximate the ground-truth solution operator for many popular PDE families and shows no degradation in accuracy even under zero-shot super-resolution, that is, being able to predict beyond the resolution of training data. PINO uses the Fourier neural operator (FNO) framework that is guaranteed to be a universal approximator for any continuous operator and discretization convergent in the limit of mesh refinement. By adding PDE constraints to FNO at a higher resolution, we obtain a high-fidelity reconstruction of the ground-truth operator. Moreover, PINO succeeds in settings where no training data is available and only PDE constraints are imposed, while previous approaches, such as the Physics-Informed Neural Network (PINN), fail due to optimization challenges, for example, in multi-scale dynamic systems such as Kolmogorov flows.PROBLEM STATEMENTMachine learning methods have recently shown promise in solving partial differential equations (PDEs) raised in science and engineering. They can be classified into two broad categories: approximating the solution function  and learning the solution operator. The Physics-Informed Neural Network (PINN) is an example of the former while the Fourier neural operator (FNO) is an example of the latter. Both these approaches have shortcomings. The optimization in PINN is challenging and prone to failure, especially on multi-scale dynamic systems. FNO does not suffer from this optimization issue since it carries out supervised learning on a given dataset, but obtaining such data may be too expensive or infeasible. In this paper, we consider a new learning paradigm, aiming to overcome the optimization challenge in PINN and relieve the data requirement in FNO.METHODSIn this paper, we propose physics-informed neural operators (PINO) that combine training data and physics constraints to learn the solution operator of a given family of parametric PDEs.In the operator-learning phase, PINO learns the solution operator over multiple instances of the parametric PDE family using training data and physics constraints. In the instance-wise fine-tuning phase, PINO optimizes the pre-trained operator ansatz for the querying instance of the PDE using the physics constraints only.Specifically, we combine coarse-resolution training data with PDE constraints imposed at a higher resolution. By adding PDE constraints to FNO at a higher resolution, we obtain a high-fidelity reconstruction of the ground-truth operator.RESULTSThe resulting PINO model can accurately approximate the ground-truth solution operator for many popular PDE families and shows no degradation in accuracy even under zero-shot super-resolution, i.e., being able to predict beyond the resolution of training data.Experiments show PINO outperforms previous ML methods on many popular PDE families while retaining the extraordinary speed-up of FNO compared to solvers. With the equation constraints, PINO requires few to no data to learn the Burgers, Darcy, and Navier-Stokes equation. In particular, PINO accurately solves long temporal transient flows and  Kolmogorov flows where other baseline methods fail to converge.SIGNIFICANCEPINO uses the neural operator framework that is guaranteed to be a universal approximator for any continuous operator and discretization convergent in the limit of mesh refinement. Moreover, PINO succeeds in settings where no training data is available and only PDE constraints are imposed. These advantages could lead to applications such as weather forecast, airfoil designs, and turbulence control.},
journal = {ACM / IMS J. Data Sci.},
month = {may},
articleno = {9},
numpages = {27},
keywords = {Neural operators, physics informed learning, partial differential equations}
}

@Article{PIML,
author={Karniadakis, George Em
and Kevrekidis, Ioannis G.
and Lu, Lu
and Perdikaris, Paris
and Wang, Sifan
and Yang, Liu},
title={Physics-informed machine learning},
journal={Nature Reviews Physics},
year={2021},
volume={3},
number={6},
pages={422--440},
abstract={Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems.},
issn={2522-5820},
doi={10.1038/s42254-021-00314-5},
url={https://doi.org/10.1038/s42254-021-00314-5}
}

@article{Psaros2023,
title = {Uncertainty quantification in scientific machine learning: Methods, metrics, and comparisons},
journal = {Journal of Computational Physics},
volume = {477},
pages = {111902},
year = {2023},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2022.111902},
url = {https://www.sciencedirect.com/science/article/pii/S0021999122009652},
author = {Apostolos F. Psaros and Xuhui Meng and Zongren Zou and Ling Guo and George Em Karniadakis},
keywords = {Scientific machine learning, Stochastic partial differential equations, Uncertainty quantification, Physics-informed neural networks, Neural operator learning, Bayesian framework},
abstract = {Neural networks (NNs) are currently changing the computational paradigm on how to combine data with mathematical laws in physics and engineering in a profound way, tackling challenging inverse and ill-posed problems not solvable with traditional methods. However, quantifying errors and uncertainties in NN-based inference is more complicated than in traditional methods. This is because in addition to aleatoric uncertainty associated with noisy data, there is also uncertainty due to limited data, but also due to NN hyperparameters, overparametrization, optimization and sampling errors as well as model misspecification. Although there are some recent works on uncertainty quantification (UQ) in NNs, there is no systematic investigation of suitable methods towards quantifying the total uncertainty effectively and efficiently even for function approximation, and there is even less work on solving partial differential equations and learning operator mappings between infinite-dimensional function spaces using NNs. In this work, we present a comprehensive framework that includes uncertainty modeling, new and existing solution methods, as well as evaluation metrics and post-hoc improvement approaches. To demonstrate the applicability and reliability of our framework, we present an extensive comparative study in which various methods are tested on prototype problems, including problems with mixed input-output data, and stochastic problems in high dimensions. In the Appendix, we include a comprehensive description of all the UQ methods employed. Further, to help facilitate the deployment of UQ in Scientific Machine Learning research and practice, we present and develop in [1] an open-source Python library (github.com/Crunch-UQ4MI/neuraluq), termed NeuralUQ, that is accompanied by an educational tutorial and additional computational experiments.}
}

@article{Raissi2019PINNs,
title = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
journal = {Journal of Computational Physics},
volume = {378},
pages = {686-707},
year = {2019},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2018.10.045},
url = {https://www.sciencedirect.com/science/article/pii/S0021999118307125},
author = {M. Raissi and P. Perdikaris and G.E. Karniadakis},
keywords = {Data-driven scientific computing, Machine learning, Predictive modeling, Runge–Kutta methods, Nonlinear dynamics},
abstract = {We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.}
}

@article{ZhuPCDLUQ2019,
title = {Physics-constrained deep learning for high-dimensional surrogate modeling and uncertainty quantification without labeled data},
journal = {Journal of Computational Physics},
volume = {394},
pages = {56-81},
year = {2019},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2019.05.024},
url = {https://www.sciencedirect.com/science/article/pii/S0021999119303559},
author = {Yinhao Zhu and Nicholas Zabaras and Phaedon-Stelios Koutsourelakis and Paris Perdikaris},
keywords = {Physics-constrained, Normalizing flow, Conditional generative model, Reverse KL divergence, Surrogate modeling, Uncertainty quantification},
abstract = {Surrogate modeling and uncertainty quantification tasks for PDE systems are most often considered as supervised learning problems where input and output data pairs are used for training. The construction of such emulators is by definition a small data problem which poses challenges to deep learning approaches that have been developed to operate in the big data regime. Even in cases where such models have been shown to have good predictive capability in high dimensions, they fail to address constraints in the data implied by the PDE model. This paper provides a methodology that incorporates the governing equations of the physical model in the loss/likelihood functions. The resulting physics-constrained, deep learning models are trained without any labeled data (e.g. employing only input data) and provide comparable predictive responses with data-driven models while obeying the constraints of the problem at hand. This work employs a convolutional encoder-decoder neural network approach as well as a conditional flow-based generative model for the solution of PDEs, surrogate model construction, and uncertainty quantification tasks. The methodology is posed as a minimization problem of the reverse Kullback-Leibler (KL) divergence between the model predictive density and the reference conditional density, where the later is defined as the Boltzmann-Gibbs distribution at a given inverse temperature with the underlying potential relating to the PDE system of interest. The generalization capability of these models to out-of-distribution input is considered. Quantification and interpretation of the predictive uncertainty is provided for a number of problems.}
}

@inproceedings{conformaltimeserires,
 author = {Stankeviciute, Kamile and M. Alaa, Ahmed and van der Schaar, Mihaela},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {6216--6228},
 publisher = {Curran Associates, Inc.},
 title = {Conformal Time-series Forecasting},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/312f1ba2a72318edaaa995a67835fad5-Paper.pdf},
 volume = {34},
 year = {2021}
}

@InProceedings{cp_dynamic_timeseries,
  title = 	 {Conformal prediction interval for dynamic time-series},
  author =       {Xu, Chen and Xie, Yao},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {11559--11569},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/xu21h/xu21h.pdf},
  url = 	 {https://proceedings.mlr.press/v139/xu21h.html},
  abstract = 	 {We develop a method to construct distribution-free prediction intervals for dynamic time-series, called \Verb|EnbPI| that wraps around any bootstrap ensemble estimator to construct sequential prediction intervals. \Verb|EnbPI| is closely related to the conformal prediction (CP) framework but does not require data exchangeability. Theoretically, these intervals attain finite-sample, \textit{approximately valid} marginal coverage for broad classes of regression functions and time-series with strongly mixing stochastic errors. Computationally, \Verb|EnbPI| avoids overfitting and requires neither data-splitting nor training multiple ensemble estimators; it efficiently aggregates bootstrap estimators that have been trained. In general, \Verb|EnbPI| is easy to implement, scalable to producing arbitrarily many prediction intervals sequentially, and well-suited to a wide range of regression functions. We perform extensive real-data analyses to demonstrate its effectiveness.}
}

@misc{gopakumar2024uncertaintyquantificationsurrogatemodels,
      title={Uncertainty Quantification of Surrogate Models using Conformal Prediction}, 
      author={Vignesh Gopakumar and Ander Gray and Joel Oskarsson and Lorenzo Zanisi and Stanislas Pamela and Daniel Giles and Matt Kusner and Marc Peter Deisenroth},
      year={2024},
      eprint={2408.09881},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2408.09881}, 
}

@misc{gray2025guaranteedconfidencebandenclosurespde,
      title={Guaranteed confidence-band enclosures for PDE surrogates}, 
      author={Ander Gray and Vignesh Gopakumar and Sylvain Rousseau and Sébastien Destercke},
      year={2025},
      eprint={2501.18426},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2501.18426}, 
}

@article{ma2024calibrated,
  title={Calibrated Uncertainty Quantification for Operator Learning via Conformal Prediction},
  author={Ma, Ziqi and Azizzadenesheli, Kamyar and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2402.01960},
  year={2024}
}

@misc{sun2022conformal,
      title={Conformal Methods for Quantifying Uncertainty in Spatiotemporal Data: A Survey}, 
      author={Sophia Sun},
      year={2022},
      eprint={2209.03580},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{zou2022neuraluq,
author = {Zou, Zongren and Meng, Xuhui and Psaros, Apostolos F. and Karniadakis, George E.},
title = {NeuralUQ: A Comprehensive Library for Uncertainty Quantification in Neural Differential Equations and Operators},
journal = {SIAM Review},
volume = {66},
number = {1},
pages = {161-190},
year = {2024},
doi = {10.1137/22M1518189},
URL = { 
        https://doi.org/10.1137/22M1518189},
eprint = {https://doi.org/10.1137/22M1518189},
    abstract = { Uncertainty quantification (UQ) in machine learning is currently drawing increasing research interest, driven by the rapid deployment of deep neural networks across different fields, such as computer vision and natural language processing, and by the need for reliable tools in risk-sensitive applications. Recently, various machine learning models have also been developed to tackle problems in the field of scientific computing with applications to computational science and engineering (CSE). Physics-informed neural networks and deep operator networks are two such models for solving partial differential equations (PDEs) and learning operator mappings, respectively. In this regard, a comprehensive study of UQ methods tailored specifically for scientific machine learning (SciML) models has been provided in [A. F. Psaros et al., J. Comput. Phys., 477 (2023), art. 111902]. Nevertheless, and despite their theoretical merit, implementations of these methods are not straightforward, especially in large-scale CSE applications, hindering their broad adoption in both research and industry settings. In this paper, we present an open-source Python library (ŭlhttps://github.com/Crunch-UQ4MI), termed NeuralUQ and accompanied by an educational tutorial, for employing UQ methods for SciML in a convenient and structured manner. The library, designed for both educational and research purposes, supports multiple modern UQ methods and SciML models. It is based on a succinct workflow and facilitates flexible employment and easy extensions by the users. We first present a tutorial of NeuralUQ and subsequently demonstrate its applicability and efficiency in four diverse examples, involving dynamical systems and high-dimensional parametric and time-dependent PDEs. }
}

