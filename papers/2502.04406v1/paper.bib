@inproceedings{conformalized_quantile_regression,
 author = {Romano, Yaniv and Patterson, Evan and Candes, Emmanuel},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Conformalized Quantile Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/5103c3584b063c431bd1268e9b5e76fb-Paper.pdf},
 volume = {32},
 year = {2019}
}



@article{gentle_introduction_CP,
author = {Angelopoulos, Anastasios N. and Bates, Stephen},
title = {Conformal Prediction: A Gentle Introduction},
year = {2023},
issue_date = {Mar 2023},
publisher = {Now Publishers Inc.},
address = {Hanover, MA, USA},
volume = {16},
number = {4},
issn = {1935-8237},
url = {https://doi.org/10.1561/2200000101},
doi = {10.1561/2200000101},
abstract = {Black-box machine learning models are now routinely used
        in high-risk settings, like medical diagnostics, which demand
        uncertainty quantification to avoid consequential model failures.
        Conformal prediction (a.k.a. conformal inference) is
        a user-friendly paradigm for creating statistically rigorous
        uncertainty sets/intervals for the predictions of such models.
        Critically, the sets are valid in a distribution-free sense: they
        possess explicit, non-asymptotic guarantees even without
        distributional assumptions or model assumptions. One can
        use conformal prediction with any pre-trained model, such
        as a neural network, to produce sets that are guaranteed
        to contain the ground truth with a user-specified probability,
        such as 90\%. It is easy-to-understand, easy-to-use,
        and general, applying naturally to problems arising in the
        fields of computer vision, natural language processing, deep
        reinforcement learning, and so on.This hands-on introduction is aimed to provide the reader a
        working understanding of conformal prediction and related
        distribution-free uncertainty quantification techniques with
        one self-contained document. We lead the reader through
        practical theory for and examples of conformal prediction
        and describe its extensions to complex machine learning
        tasks involving structured outputs, distribution shift, timeseries,
        outliers, models that abstain, and more. Throughout,
        there are many explanatory illustrations, examples, and
        code samples in Python. With each code sample comes a
        Jupyter notebook implementing the method on a real-data
        example; the notebooks can be accessed and easily run by
        following the code footnotes.},
journal = {Found. Trends Mach. Learn.},
month = {mar},
pages = {494–591},
numpages = {114}
}


@inproceedings{papadopoulos2007conformal,
  title={Conformal prediction with neural networks},
  author={Papadopoulos, Harris and Vovk, Volodya and Gammerman, Alex},
  booktitle={19th IEEE International Conference on Tools with Artificial Intelligence (ICTAI 2007)},
  volume={2},
  pages={388--395},
  year={2007},
  organization={IEEE}
}

@book{haykin1994neural,
  title={Neural Networks: A Comprehensive Foundation},
  author={Haykin, Simon},
  year={1994},
  publisher={Prentice Hall PTR}
}

@book{koenker_2005, place={Cambridge}, series={Econometric Society Monographs}, 
title={Quantile Regression}, DOI={10.1017/CBO9780511754098}, 
publisher={Cambridge University Press}, author={Koenker, Roger}, 
year={2005}, collection={Econometric Society Monographs}}





@article{error_residual,
author = {Jing Lei and Max G’Sell and Alessandro Rinaldo and Ryan J. Tibshirani and Larry Wasserman},
title = {Distribution-Free Predictive Inference for Regression},
journal = {Journal of the American Statistical Association},
volume = {113},
number = {523},
pages = {1094-1111},
year  = {2018},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.2017.1307116},
URL = {https://doi.org/10.1080/01621459.2017.1307116},
eprint = {https://doi.org/10.1080/01621459.2017.1307116}
}


@article{shafer2008tutorial,
  title={A Tutorial on Conformal Prediction.},
  author={Shafer, Glenn and Vovk, Vladimir},
  journal={Journal of Machine Learning Research},
  volume={9},
  number={3},
  year={2008}
}

@book{vovk2005algorithmic,
  title={Algorithmic Learning in a Random World},
  author={Vovk, Vladimir and Gammerman, Alexander and Shafer, Glenn},
  year={2005},
  publisher={Springer}
}

@incollection{papadopoulos2008inductive,
    author = {Harris Papadopoulos},
    title = {Inductive Conformal Prediction: Theory and Application to Neural Networks},
    booktitle = {Tools in Artificial Intelligence},
    publisher = {IntechOpen},
    address = {Rijeka},
    year = {2008},
    editor = {Paula Fritzsche},
    chapter = {18},
    doi = {10.5772/6078},
}

@article{tibshirani2019conformal,
  title={Conformal prediction under covariate shift},
  author={Tibshirani, Ryan J. and Foygel Barber, Rina and Candes, Emmanuel and Ramdas, Aaditya},
  journal={Advances in Neural Information Processing Systems},
  year={2019}
}

@InProceedings{exchangeable,
author="Aldous, David J.",
title="Exchangeability and related topics",
booktitle="{\'E}cole d'{\'E}t{\'e} de Probabilit{\'e}s de Saint-Flour XIII --- 1983",
year="1985",
pages="1--198",
isbn="978-3-540-39316-0"
}


@book{bellan2006fundamentals,
  title={Fundamentals of Plasma Physics},
  author={Bellan, Paul M.},
  isbn={9780511562105},
  url={https://books.google.co.uk/books?id=CoPtzQEACAAJ},
  year={2006},
  publisher={Cambridge University Press}
}



@article{Hoelzl2021jorek,
doi = {10.1088/1741-4326/abf99f},
url = {https://dx.doi.org/10.1088/1741-4326/abf99f},
year = {2021},
publisher = {IOP Publishing},
volume = {61},
number = {6},
pages = {065001},
author = {M. Hoelzl and G. T. A. Huijsmans and S. J. P. Pamela and M. Bécoulet and E. Nardon and F.J. Artola and B. Nkonga and C. V. Atanasiu and V. Bandaru and A. Bhole and D. Bonfiglio and A. Cathey and O. Czarny and A. Dvornova and T. Fehér and A. Fil and E. Franck and S. Futatani and M. Gruca and H. Guillard and J. W. Haverkort and I. Holod and D. Hu and S. K. Kim and S. Q. Korving and L. Kos and I. Krebs and L. Kripner and G. Latu and F. Liu and P. Merkel and D. Meshcheriakov and V. Mitterauer and S. Mochalskyy and J. A. Morales and R. Nies and N. Nikulsin and F. Orain and J. Pratt and R. Ramasamy and P. Ramet and C. Reux and K. Särkimäki and N. Schwarz and P. Singh Verma and S. F. Smith and C. Sommariva and E. Strumberger and D. C. van Vugt and M. Verbeek and E. Westerhof and F. Wieschollek and J. Zielinski},
title = {The JOREK non-linear extended MHD code and applications to large-scale instabilities and their control in magnetically confined fusion plasmas},
journal = {Nuclear Fusion},
abstract = {JOREK is a massively parallel fully implicit non-linear extended magneto-hydrodynamic (MHD) code for realistic tokamak X-point plasmas. It has become a widely used versatile simulation code for studying large-scale plasma instabilities and their control and is continuously developed in an international community with strong involvements in the European fusion research programme and ITER organization. This article gives a comprehensive overview of the physics models implemented, numerical methods applied for solving the equations and physics studies performed with the code. A dedicated section highlights some of the verification work done for the code. A hierarchy of different physics models is available including a free boundary and resistive wall extension and hybrid kinetic-fluid models. The code allows for flux-surface aligned iso-parametric finite element grids in single and double X-point plasmas which can be extended to the true physical walls and uses a robust fully implicit time stepping. Particular focus is laid on plasma edge and scrape-off layer (SOL) physics as well as disruption related phenomena. Among the key results obtained with JOREK regarding plasma edge and SOL, are deep insights into the dynamics of edge localized modes (ELMs), ELM cycles, and ELM control by resonant magnetic perturbations, pellet injection, as well as by vertical magnetic kicks. Also ELM free regimes, detachment physics, the generation and transport of impurities during an ELM, and electrostatic turbulence in the pedestal region are investigated. Regarding disruptions, the focus is on the dynamics of the thermal quench (TQ) and current quench triggered by massive gas injection and shattered pellet injection, runaway electron (RE) dynamics as well as the RE interaction with MHD modes, and vertical displacement events. Also the seeding and suppression of tearing modes (TMs), the dynamics of naturally occurring TQs triggered by locked modes, and radiative collapses are being studied.}
}

@Inbook{Hackbusch2017,
author="Hackbusch, Wolfgang",
title="The Poisson Equation",
bookTitle="Elliptic Differential Equations: Theory and Numerical Treatment",
year="2017",
publisher="Springer",
address="Berlin, Heidelberg",
pages="29--42",
abstract="In Section 3.1 the Poisson equation --$\Delta$u=f is introduced, and the uniqueness of the solution is proved. The Green function is defined in Section 3.2. It allows the representation (3.6) of the solution, provided it is existing. Concerning the existence, Theorem 3.13 contains a negative statement (cf. Section 3.3): The Poisson equation with a continuous right-hand side f may possess no classical solution. A sufficient condition for a classical solution is the H{\"o}lder continuity of f as stated in Theorem 3.18. Section 3.4 introduces Green's function for the ball. In the two-dimensional case, Riemann's mapping theorem allows the construction of the Green function for a large class of domains. In Section 3.5 we replace the Dirichlet boundary condition by the Neumann condition. The final Section 3.6 is a short introduction into the integral equation method. The solution of the boundary-value problem can indirectly be obtained by solving an integral equation.",
isbn="978-3-662-54961-2",
doi="10.1007/978-3-662-54961-2_3",
url="https://doi.org/10.1007/978-3-662-54961-2_3"
}


@article{py-pde,
    Author = {David Zwicker},
    Doi = {10.21105/joss.02158},
    Journal = {Journal of Open Source Software},
    Number = {48},
    Pages = {2158},
    Publisher = {The Open Journal},
    Title = {py-pde: A python package for solving partial differential equations},
    Url = {https://doi.org/10.21105/joss.02158},
    Volume = {5},
    Year = {2020}
}

@InProceedings{ronneberger2015unet,
author="Ronneberger, Olaf
and Fischer, Philipp
and Brox, Thomas",
editor="Navab, Nassir
and Hornegger, Joachim
and Wells, William M.
and Frangi, Alejandro F.",
title="U-Net: Convolutional Networks for Biomedical Image Segmentation",
booktitle="Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="234--241",
abstract="There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.",
isbn="978-3-319-24574-4"
}



@article{
gupta2022multispatiotemporalscale,
title={Towards Multi-spatiotemporal-scale Generalized {PDE} Modeling},
author={Jayesh K Gupta and Johannes Brandstetter},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=dPSTDbGtBY},
note={}
}


@article{GOPAKUMAR2023100464,
title = {Loss landscape engineering via data regulation on PINNs},
journal = {Machine Learning with Applications},
volume = {12},
pages = {100464},
year = {2023},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2023.100464},
url = {https://www.sciencedirect.com/science/article/pii/S2666827023000178},
author = {Vignesh Gopakumar and Stanislas Pamela and Debasmita Samaddar},
keywords = {Physics-Informed Neural Networks, Loss landscape, Sparse regularisation, Partial differential equations, Optimisation},
abstract = {Physics-Informed Neural Networks have shown unique utility in parameterising the solution of a well-defined partial differential equation using automatic differentiation and residual losses. Though they provide theoretical guarantees of convergence, in practice the required training regimes tend to be exacting and demanding. Through the course of this paper, we take a deep dive into understanding the loss landscapes associated with a PINN and how that offers some insight as to why PINNs are fundamentally hard to optimise for. We demonstrate how PINNs can be forced to converge better towards the solution, by way of feeding in sparse or coarse data as a regulator. The data regulates and morphs the topology of the loss landscape associated with the PINN to make it easily traversable for the minimiser. Data regulation of PINNs helps ease the optimisation required for convergence by invoking a hybrid unsupervised–supervised training approach, where the labelled data pushes the network towards the vicinity of the solution, and the unlabelled regime fine-tunes it to the solution.}
}


@inproceedings{adam,
  author       = {Diederik P. Kingma and
                  Jimmy Ba},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Adam: {A} Method for Stochastic Optimization},
  booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015,
                  San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year         = {2015},
  url          = {http://arxiv.org/abs/1412.6980},
  timestamp    = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
li2021fourier,
title={Fourier Neural Operator for Parametric Partial Differential Equations},
author={Zongyi Li and Nikola Borislavov Kovachki and Kamyar Azizzadenesheli and Burigede liu and Kaushik Bhattacharya and Andrew Stuart and Anima Anandkumar},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=c8P9NQVtmnO}
}



@article{balch2019satellite,
  title={Satellite conjunction analysis and the false confidence theorem},
  author={Balch, Michael S. and Martin, Ryan and Ferson, Scott},
  journal={Proceedings of the Royal Society A},
  volume={475},
  number={2227},
  pages={20180565},
  year={2019},
  publisher={The Royal Society Publishing}
}

@article{cella2022validity,
  title={Validity, consonant plausibility measures, and conformal prediction},
  author={Cella, Leonardo and Martin, Ryan},
  journal={International Journal of Approximate Reasoning},
  volume={141},
  pages={110--130},
  year={2022},
  publisher={Elsevier}
}

@article{balch2012mathematical,
  title={Mathematical foundations for a theory of confidence structures},
  author={Balch, Michael S.},
  journal={International Journal of Approximate Reasoning},
  volume={53},
  number={7},
  pages={1003--1019},
  year={2012},
  publisher={Elsevier}
}

@article{hose2021universal,
  title={A universal approach to imprecise probabilities in possibility theory},
  author={Hose, Dominik and Hanss, Michael},
  journal={International Journal of Approximate Reasoning},
  volume={133},
  pages={133--158},
  year={2021},
  publisher={Elsevier}
}

@article{LALONDE2021104696,
title = {Comparison of neural network types and architectures for generating a surrogate aerodynamic wind turbine blade model},
journal = {Journal of Wind Engineering and Industrial Aerodynamics},
volume = {216},
pages = {104696},
year = {2021},
issn = {0167-6105},
doi = {https://doi.org/10.1016/j.jweia.2021.104696},
url = {https://www.sciencedirect.com/science/article/pii/S0167610521001793},
author = {Eric Rowland Lalonde and Benjamin Vischschraper and Girma Bitsuamlak and Kaoshan Dai},
keywords = {Aerodynamic wind turbine blade model, Multilayer perceptron, Long short-term memory, Convolutional neural network, Surrogate model},
}
@article{Manek_2023,
doi = {10.1088/2632-2153/acb2b3},
url = {https://dx.doi.org/10.1088/2632-2153/acb2b3},
year = {2023},
publisher = {IOP Publishing},
volume = {4},
number = {1},
pages = {015008},
author = {Petr Mánek and Graham Van Goffrier and Vignesh Gopakumar and Nikos Nikolaou and Jonathan Shimwell and Ingo Waldmann},
title = {Fast regression of the tritium breeding ratio in fusion reactors},
journal = {Machine Learning: Science and Technology},
abstract = {The tritium breeding ratio (TBR) is an essential quantity for the design of modern and next-generation D-T fueled nuclear fusion reactors. Representing the ratio between tritium fuel generated in breeding blankets and fuel consumed during reactor runtime, the TBR depends on reactor geometry and material properties in a complex manner. In this work, we explored the training of surrogate models to produce a cheap but high-quality approximation for a Monte Carlo (MC) TBR model in use at the UK Atomic Energy Authority. We investigated possibilities for dimensional reduction of its feature space, reviewed 9 families of surrogate models for potential applicability, and performed hyperparameter optimization. Here we present the performance and scaling properties of these models, the fastest of which, an artificial neural network, demonstrated  and a mean prediction time of , representing a relative speedup of  with respect to the expensive MC model. We further present a novel adaptive sampling algorithm, Quality-Adaptive Surrogate Sampling, capable of interfacing with any of the individually studied surrogates. Our preliminary testing on a toy TBR theory has demonstrated the efficacy of this algorithm for accelerating the surrogate modelling process.}
}


@Article{Baldi2016,
author={Baldi, Pierre
and Cranmer, Kyle
and Faucett, Taylor
and Sadowski, Peter
and Whiteson, Daniel},
title={Parameterized neural networks for high-energy physics},
journal={The European Physical Journal C},
year={2016},
month={04},
day={27},
volume={76},
number={5},
pages={235},
abstract={We investigate a new structure for machine learning classifiers built with neural networks and applied to problems in high-energy physics by expanding the inputs to include not only measured features but also physics parameters. The physics parameters represent a smoothly varying learning task, and the resulting parameterized classifier can smoothly interpolate between them and replace sets of classifiers trained at individual values. This simplifies the training process and gives improved performance at intermediate values, even for complex problems requiring deep learning. Applications include tools parameterized in terms of theoretical model parameters, such as the mass of a particle, which allow for a single network to provide improved discrimination across a range of masses. This concept is simple to implement and allows for optimized interpolatable results.},
issn={1434-6052},
doi={10.1140/epjc/s10052-016-4099-4},
url={https://doi.org/10.1140/epjc/s10052-016-4099-4}
}

@article{autoencoders,
author = {Geoffrey E. Hinton  and Ruslan Salakhutdinov },
title = {Reducing the dimensionality of data with neural networks},
journal = {Science},
volume = {313},
number = {5786},
pages = {504--507},
year = {2006},
doi = {10.1126/science.1127647},
URL = {https://www.science.org/doi/abs/10.1126/science.1127647},
eprint = {https://www.science.org/doi/pdf/10.1126/science.1127647},
abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.}}





@INPROCEEDINGS{resnets,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={Deep residual learning for image recognition}, 
  year={2016},
  doi={10.1109/CVPR.2016.90}}


@article{Wen_2023,
	doi = {10.1039/d2ee04204e},
  
	url = {https://doi.org/10.10392Fd2ee04204e},
  
	year = 2023,
	publisher = {Royal Society of Chemistry ({RSC})},
  
	volume = {16},
  
	number = {4},
  
	pages = {1732--1741},
  
	author = {Gege Wen and Zongyi Li and Qirui Long and Kamyar Azizzadenesheli and Anima Anandkumar and Sally M. Benson},
  
	title = {Real-time high-resolution {CO}$_2$ geological storage prediction using nested Fourier neural operators},
  
	journal = {Energy \& Environmental Science}
}

@Article{PIML,
author={Karniadakis, George Em
and Kevrekidis, Ioannis G.
and Lu, Lu
and Perdikaris, Paris
and Wang, Sifan
and Yang, Liu},
title={Physics-informed machine learning},
journal={Nature Reviews Physics},
year={2021},
volume={3},
number={6},
pages={422--440},
abstract={Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems.},
issn={2522-5820},
doi={10.1038/s42254-021-00314-5},
url={https://doi.org/10.1038/s42254-021-00314-5}
}

@article{simintelligence,
  doi = {10.48550/ARXIV.2112.03235},
  
  url = {https://arxiv.org/abs/2112.03235},
  
  author = {Lavin, Alexander and Krakauer, David and Zenil, Hector and Gottschlich, Justin and Mattson, Tim and Brehmer, Johann and Anandkumar, Anima and Choudry, Sanjay and Rocki, Kamil and Baydin, Atılım Güneş and Prunkl, Carina and Paige, Brooks and Isayev, Olexandr and Peterson, Erik and McMahon, Peter L. and Macke, Jakob and Cranmer, Kyle and Zhang, Jiaxin and Wainwright, Haruko and Hanuka, Adi and Veloso, Manuela and Assefa, Samuel and Zheng, Stephan and Pfeffer, Avi},
  
  keywords = {Artificial Intelligence (cs.AI), Computational Engineering, Finance, and Science (cs.CE), Machine Learning (cs.LG), Mathematical Software (cs.MS), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Simulation intelligence: Towards a new generation of scientific methods},
  
  journal = {arXiv:2112.03235},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{lam2022graphcast,
    author = {Remi Lam  and Alvaro Sanchez-Gonzalez  and Matthew Willson  and Peter Wirnsberger  and Meire Fortunato  and Ferran Alet  and Suman Ravuri  and Timo Ewalds  and Zach Eaton-Rosen  and Weihua Hu  and Alexander Merose  and Stephan Hoyer  and George Holland  and Oriol Vinyals  and Jacklynn Stott  and Alexander Pritzel  and Shakir Mohamed  and Peter Battaglia },
    title = {Learning skillful medium-range global weather forecasting},
    journal = {Science},
    volume = {382},
    number = {6677},
    pages = {1416-1421},
    year = {2023},
    doi = {10.1126/science.adi2336},
    URL = {https://www.science.org/doi/abs/10.1126/science.adi2336},
    eprint = {https://www.science.org/doi/pdf/10.1126/science.adi2336}
}



@inproceedings{pathak2022fourcastnet,
author = {Kurth, Thorsten and Subramanian, Shashank and Harrington, Peter and Pathak, Jaideep and Mardani, Morteza and Hall, David and Miele, Andrea and Kashinath, Karthik and Anandkumar, Anima},
title = {FourCastNet: Accelerating Global High-Resolution Weather Forecasting Using Adaptive Fourier Neural Operators},
year = {2023},
isbn = {9798400701900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3592979.3593412},
doi = {10.1145/3592979.3593412},
abstract = {Extreme weather amplified by climate change is causing increasingly devastating impacts across the globe. The current use of physics-based numerical weather prediction (NWP) limits accuracy and resolution due to high computational cost and strict time-to-solution limits.We report that a data-driven deep learning Earth system emulator, FourCastNet, can predict global weather and generate medium-range forecasts five orders-of-magnitude faster than NWP while approaching state-of-the-art accuracy. FourCastNet is optimized and scales efficiently on three supercomputing systems: Selene, Perlmutter, and JUWELS Booster up to 3,808 NVIDIA A100 GPUs, attaining 140.8 petaFLOPS in mixed precision (11.9\% of peak at that scale). The time-to-solution for training FourCastNet measured on JUWELS Booster on 3,072 GPUs is 67.4 minutes, resulting in an 80,000 times faster time-to-solution relative to state-of-the-art NWP, in inference.FourCastNet produces accurate instantaneous weather predictions for a week in advance and enables enormous ensembles that could be used to improve predictions of rare weather extremes.},
booktitle = {Proceedings of the Platform for Advanced Scientific Computing Conference},
articleno = {13},
numpages = {11},
keywords = {deep learning, fourier neural operator, transformer, extreme weather, climate change},
location = {Davos, Switzerland},
series = {PASC '23}
}

@article{jiang2020meshfreeflownet,
      title={MeshfreeFlowNet: A physics-constrained deep continuous space-time super-resolution framework}, 
      author={Chiyu M. Jiang and Soheil Esmaeilzadeh and Kamyar Azizzadenesheli and Karthik Kashinath and Mustafa Mustafa and Hamdi A. Tchelepi and Philip Marcus and Prabhat and Anima Anandkumar},
      year={2020},
      journal={arXiv:2005.01463},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{
pfaff2021learning,
title={Learning Mesh-Based Simulation with Graph Networks},
author={Tobias Pfaff and Meire Fortunato and Alvaro Sanchez-Gonzalez and Peter Battaglia},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=roNqYL0_XP}
}

@article{van_de_Plassche_2020,
	doi = {10.1063/1.5134126},
  
	url = {https://doi.org/10.10632F1.5134126},
  
	year = 2020,
 
	publisher = {{AIP} Publishing},
  
	volume = {27},
  
	number = {2},
  
	pages = {022310},
  
	author = {K. L. van de Plassche and J. Citrin and C. Bourdelle and Y. Camenen and F. J. Casson and V. I. Dagnelie and F. Felici and A. Ho and S. Van Mulders and},
  
	title = {Fast modeling of turbulent transport in fusion plasmas using neural networks},
  
	journal = {Physics of Plasmas}
}

@article{Gopakumar_2020,
doi = {10.1088/2632-2153/ab5639},
url = {https://dx.doi.org/10.1088/2632-2153/ab5639},
year = {2020},
publisher = {IOP Publishing},
volume = {1},
number = {1},
pages = {015006},
author = {Vignesh Gopakumar and Debasmita Samaddar},
title = {Image mapping the temporal evolution of edge characteristics in Tokamaks using neural networks},
journal = {Machine Learning: Science and Technology},
abstract = {We propose a method for data-driven modelling of the temporal evolution of the plasma and neutral characteristics at the edge of a tokamak using neural networks. Our method proposes a novel fully convolutional network to serve as function approximators in modelling complex nonlinear phenomenon observed in the multi-physics representations of high energy physics. More specifically, we target the evolution of the temperatures, densities and parallel velocities of the electrons, ions and neutral particles at the edge. The central challenge in this context is in modelling together the different physics principles encapsulated in the evolution of plasma and the neutrals. We demonstrate that the inherent differences in nonlinear behaviour can be addressed by forking the network to process the plasma and neutral information individually before integrating as a holistic system. Our approach takes into account the spatial dependencies of the physics parameters across the grid while performing the temporal mappings, ensuring that the underlying physics is factored in and not lost to the black-box. Having used the conventional edge plasma-neutral solver code SOLPS to build the synthetic dataset, our method demonstrates a computational gain of over 5 orders of magnitude over it without a considerable compromise on accuracy.}
}

@article{GENEVA2020109056,
title = {Modeling the dynamics of PDE systems with physics-constrained deep auto-regressive networks},
journal = {Journal of Computational Physics},
volume = {403},
pages = {109056},
year = {2020},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2019.109056},
url = {https://www.sciencedirect.com/science/article/pii/S0021999119307612},
author = {Nicholas Geneva and Nicholas Zabaras},
keywords = {Physics-informed machine learning, Auto-regressive model, Deep neural networks, Convolutional encoder-decoder, Uncertainty quantification, Dynamic partial differential equations},
abstract = {In recent years, deep learning has proven to be a viable methodology for surrogate modeling and uncertainty quantification for a vast number of physical systems. However, in their traditional form, such models can require a large amount of training data. This is of particular importance for various engineering and scientific applications where data may be extremely expensive to obtain. To overcome this shortcoming, physics-constrained deep learning provides a promising methodology as it only utilizes the governing equations. In this work, we propose a novel auto-regressive dense encoder-decoder convolutional neural network to solve and model non-linear dynamical systems without training data at a computational cost that is potentially magnitudes lower than standard numerical solvers. This model includes a Bayesian framework that allows for uncertainty quantification of the predicted quantities of interest at each time-step. We rigorously test this model on several non-linear transient partial differential equation systems including the turbulence of the Kuramoto-Sivashinsky equation, multi-shock formation and interaction with 1D Burgers' equation and 2D wave dynamics with coupled Burgers' equations. For each system, the predictive results and uncertainty are presented and discussed together with comparisons to the results obtained from traditional numerical analysis methods.}
}

@article{ZHU2018415,
title = {Bayesian deep convolutional encoder–decoder networks for surrogate modeling and uncertainty quantification},
journal = {Journal of Computational Physics},
volume = {366},
pages = {415-447},
year = {2018},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2018.04.018},
url = {https://www.sciencedirect.com/science/article/pii/S0021999118302341},
author = {Yinhao Zhu and Nicholas Zabaras},
keywords = {Uncertainty quantification, Bayesian neural networks, Convolutional encoder–decoder networks, Deep learning, Porous media flows},
abstract = {We are interested in the development of surrogate models for uncertainty quantification and propagation in problems governed by stochastic PDEs using a deep convolutional encoder–decoder network in a similar fashion to approaches considered in deep learning for image-to-image regression tasks. Since normal neural networks are data-intensive and cannot provide predictive uncertainty, we propose a Bayesian approach to convolutional neural nets. A recently introduced variational gradient descent algorithm based on Stein's method is scaled to deep convolutional networks to perform approximate Bayesian inference on millions of uncertain network parameters. This approach achieves state of the art performance in terms of predictive accuracy and uncertainty quantification in comparison to other approaches in Bayesian neural networks as well as techniques that include Gaussian processes and ensemble methods even when the training data size is relatively small. To evaluate the performance of this approach, we consider standard uncertainty quantification tasks for flow in heterogeneous media using limited training data consisting of permeability realizations and the corresponding velocity and pressure fields. The performance of the surrogate model developed is very good even though there is no underlying structure shared between the input (permeability) and output (flow/pressure) fields as is often the case in the image-to-image regression models used in computer vision problems. Studies are performed with an underlying stochastic input dimensionality up to 4225 where most other uncertainty quantification methods fail. Uncertainty propagation tasks are considered and the predictive output Bayesian statistics are compared to those obtained with Monte Carlo estimates.}
}

@article{ALHAJERI202234,
title = {Physics-informed machine learning modeling for predictive control using noisy data},
journal = {Chemical Engineering Research and Design},
volume = {186},
pages = {34-49},
year = {2022},
issn = {0263-8762},
doi = {https://doi.org/10.1016/j.cherd.2022.07.035},
url = {https://www.sciencedirect.com/science/article/pii/S0263876222003847},
author = {Mohammed S. Alhajeri and Fahim Abdullah and Zhe Wu and Panagiotis D. Christofides},
keywords = {Process control, Model predictive control, Nonlinear processes, Machine Learning, Recurrent neural networks, Aspen Plus Dynamics},
abstract = {Due to the occurrence of over-fitting at the learning phase, the modeling of chemical processes via artificial neural networks (ANN) by using corrupted data (i.e., noisy data) is an ongoing challenge. Therefore, this work investigates the effect of both Gaussian and non-Gaussian noise on the performance of process-structure based recurrent neural networks (RNN) models, which take the form of partially-connected RNN models in this work, that are used to approximate a class of multi-input-multi-outputs nonlinear systems. Furthermore, two different techniques, specifically Monte Carlo dropout and co-teaching, are utilized in the development of partially-connected RNN models. These two techniques are employed to reduce the over-fitting in ANNs when noisy data is used in the training process and, hence, to improve the open-loop accuracy as well as the closed-loop performance under a Lyapunov-based model predictive controller (MPC). Aspen Plus Dynamics, a well-known high-fidelity process simulator, is used to simulate a large-scale chemical process application in order to demonstrate the anticipated improvements in both open-loop approximation and closed-loop controller performance in the presence of Gaussian and non-Gaussian noise in the data set using physics-informed RNNs.}
}

@inproceedings{gal2016dropout,
      title={Dropout as a Bayesian approximation: Representing model uncertainty in deep learning}, 
      author={Yarin Gal and Zoubin Ghahramani},
      year={2016},
      booktitle = {International Conference on Machine Learning}
}

@article{bnns,
    author = {MacKay, David J. C.},
    title = "{A Practical Bayesian Framework for Backpropagation Networks}",
    journal = {Neural Computation},
    volume = {4},
    number = {3},
    pages = {448-472},
    year = {1992},
    month = {05},
    abstract = "{A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks. The framework makes possible (1) objective comparisons between solutions using alternative network architectures, (2) objective stopping rules for network pruning or growing procedures, (3) objective choice of magnitude and type of weight decay terms or additive regularizers (for penalizing large weights, etc.), (4) a measure of the effective number of well-determined parameters in a model, (5) quantified estimates of the error bars on network parameters and on network output, and (6) objective comparisons with alternative learning and interpolation models such as splines and radial basis functions. The Bayesian "evidence" automatically embodies "Occam's razor," penalizing overflexible and overcomplex models. The Bayesian approach helps detect poor underlying assumptions in learning models. For learning models well matched to a problem, a good correlation between generalization ability and the Bayesian evidence is obtained.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1992.4.3.448},
    url = {https://doi.org/10.1162/neco.1992.4.3.448},
    eprint = {https://direct.mit.edu/neco/article-pdf/4/3/448/812348/neco.1992.4.3.448.pdf},
}



@misc{lakshminarayanan2017simple,
      title={Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles}, 
      author={Balaji Lakshminarayanan and Alexander Pritzel and Charles Blundell},
      year={2017},
      eprint={1612.01474},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@INPROCEEDINGS{uq_survey,
  author={Sudret, Bruno and Marelli, Stefano and Wiart, Joe},
  booktitle={2017 11th European Conference on Antennas and Propagation (EUCAP)}, 
  title={Surrogate models for uncertainty quantification: An overview}, 
  year={2017},
  volume={},
  number={},
  pages={793-797},
  doi={10.23919/EuCAP.2017.7928679}}

  @article{ABDAR2021243,
title = {A review of uncertainty quantification in deep learning: Techniques, applications and challenges},
journal = {Information Fusion},
volume = {76},
pages = {243--297},
year = {2021},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2021.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S1566253521001081},
author = {Moloud Abdar and Farhad Pourpanah and Sadiq Hussain and Dana Rezazadegan and Li Liu and Mohammad Ghavamzadeh and Paul Fieguth and Xiaochun Cao and Abbas Khosravi and U. Rajendra Acharya and Vladimir Makarenkov and Saeid Nahavandi},
keywords = {Artificial intelligence, Uncertainty quantification, Deep learning, Machine learning, Bayesian statistics, Ensemble learning},
abstract = {Uncertainty quantification (UQ) methods play a pivotal role in reducing the impact of uncertainties during both optimization and decision making processes. They have been applied to solve a variety of real-world problems in science and engineering. Bayesian approximation and ensemble learning techniques are two widely-used types of uncertainty quantification (UQ) methods. In this regard, researchers have proposed different UQ methods and examined their performance in a variety of applications such as computer vision (e.g., self-driving cars and object detection), image processing (e.g., image restoration), medical image analysis (e.g., medical image classification and segmentation), natural language processing (e.g., text classification, social media texts and recidivism risk-scoring), bioinformatics, etc. This study reviews recent advances in UQ methods used in deep learning, investigates the application of these methods in reinforcement learning, and highlights fundamental res4earch challenges and directions associated with UQ.}
}

@article{martin2019false,
  title={False confidence, non-additive beliefs, and valid statistical inference},
  author={Martin, Ryan},
  journal={International Journal of Approximate Reasoning},
  volume={113},
  pages={39--73},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{vovk2012conditional,
  title={Conditional validity of inductive conformal predictors},
  author={Vovk, Vladimir},
  booktitle={Asian Conference on Machine Learning},
  year={2012},
}


@article{zou2022neuraluq,
author = {Zou, Zongren and Meng, Xuhui and Psaros, Apostolos F. and Karniadakis, George E.},
title = {NeuralUQ: A Comprehensive Library for Uncertainty Quantification in Neural Differential Equations and Operators},
journal = {SIAM Review},
volume = {66},
number = {1},
pages = {161-190},
year = {2024},
doi = {10.1137/22M1518189},
URL = { 
        https://doi.org/10.1137/22M1518189},
eprint = {https://doi.org/10.1137/22M1518189},
    abstract = { Uncertainty quantification (UQ) in machine learning is currently drawing increasing research interest, driven by the rapid deployment of deep neural networks across different fields, such as computer vision and natural language processing, and by the need for reliable tools in risk-sensitive applications. Recently, various machine learning models have also been developed to tackle problems in the field of scientific computing with applications to computational science and engineering (CSE). Physics-informed neural networks and deep operator networks are two such models for solving partial differential equations (PDEs) and learning operator mappings, respectively. In this regard, a comprehensive study of UQ methods tailored specifically for scientific machine learning (SciML) models has been provided in [A. F. Psaros et al., J. Comput. Phys., 477 (2023), art. 111902]. Nevertheless, and despite their theoretical merit, implementations of these methods are not straightforward, especially in large-scale CSE applications, hindering their broad adoption in both research and industry settings. In this paper, we present an open-source Python library (ŭlhttps://github.com/Crunch-UQ4MI), termed NeuralUQ and accompanied by an educational tutorial, for employing UQ methods for SciML in a convenient and structured manner. The library, designed for both educational and research purposes, supports multiple modern UQ methods and SciML models. It is based on a succinct workflow and facilitates flexible employment and easy extensions by the users. We first present a tutorial of NeuralUQ and subsequently demonstrate its applicability and efficiency in four diverse examples, involving dynamical systems and high-dimensional parametric and time-dependent PDEs. }
}


@Article{Bertone2019,
  author       = {Gianfranco Bertone and Marc P. Deisenroth and Jong S. Kim and Sebastian Liem and Roberto {Ruiz de Austri} and Max Welling},
  journaltitle = {Physics of the Dark Universe},
  title        = {Accelerating the BSM interpretation of LHC data with machine learning},
  pages        = {100293},
  volume       = {24},
  journal      = {Physics of the Dark Universe},
  year         = {2019},
}

@article{Psaros2023,
title = {Uncertainty quantification in scientific machine learning: Methods, metrics, and comparisons},
journal = {Journal of Computational Physics},
volume = {477},
pages = {111902},
year = {2023},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2022.111902},
url = {https://www.sciencedirect.com/science/article/pii/S0021999122009652},
author = {Apostolos F. Psaros and Xuhui Meng and Zongren Zou and Ling Guo and George Em Karniadakis},
keywords = {Scientific machine learning, Stochastic partial differential equations, Uncertainty quantification, Physics-informed neural networks, Neural operator learning, Bayesian framework},
abstract = {Neural networks (NNs) are currently changing the computational paradigm on how to combine data with mathematical laws in physics and engineering in a profound way, tackling challenging inverse and ill-posed problems not solvable with traditional methods. However, quantifying errors and uncertainties in NN-based inference is more complicated than in traditional methods. This is because in addition to aleatoric uncertainty associated with noisy data, there is also uncertainty due to limited data, but also due to NN hyperparameters, overparametrization, optimization and sampling errors as well as model misspecification. Although there are some recent works on uncertainty quantification (UQ) in NNs, there is no systematic investigation of suitable methods towards quantifying the total uncertainty effectively and efficiently even for function approximation, and there is even less work on solving partial differential equations and learning operator mappings between infinite-dimensional function spaces using NNs. In this work, we present a comprehensive framework that includes uncertainty modeling, new and existing solution methods, as well as evaluation metrics and post-hoc improvement approaches. To demonstrate the applicability and reliability of our framework, we present an extensive comparative study in which various methods are tested on prototype problems, including problems with mixed input-output data, and stochastic problems in high dimensions. In the Appendix, we include a comprehensive description of all the UQ methods employed. Further, to help facilitate the deployment of UQ in Scientific Machine Learning research and practice, we present and develop in [1] an open-source Python library (github.com/Crunch-UQ4MI/neuraluq), termed NeuralUQ, that is accompanied by an educational tutorial and additional computational experiments.}
}

@Article{Courtois2023,
author={Courtois, Adrien
and Morel, Jean-Michel
and Arias, Pablo},
title={Can neural networks extrapolate? Discussion of a theorem by Pedro Domingos},
journal={Revista de la Real Academia de Ciencias Exactas, F{\'i}sicas y Naturales. Serie A. Matem{\'a}ticas},
year={2023},
month={Mar},
day={02},
volume={117},
number={2},
pages={79},
abstract={Neural networks trained on large datasets by minimizing a loss have become the state-of-the-art approach for resolving data science problems, particularly in computer vision, image processing and natural language processing. In spite of their striking results, our theoretical understanding about how neural networks operate is limited. In particular, what are the extrapolation capabilities of trained neural networks if any? In this paper we discuss a theorem of Domingos stating that ``every machine learned by continuous gradient descent is approximately a kernel machine''. According to Domingos, this fact leads to conclude that all machines trained on data are mere kernel machines. We first extend Domingo's result in the discrete case and to networks with vector-valued output. We then study its relevance and significance on simple examples. We find that in simple cases, the ``neural tangent kernel'' arising in Domingos' theorem does provide understanding of the networks' predictions. When the task given to the network grows in complexity, the interpolation capability of the network can be effectively explained by Domingos' theorem, and no extrapolation capability of the network beyond its learning domain is found, even when the network's structure would allow for it. We illustrate this fact on a classic perception theory problem: recovering a shape from its boundary.},
issn={1579-1505},
doi={10.1007/s13398-023-01411-z},
url={https://doi.org/10.1007/s13398-023-01411-z}
}

@book{atmospheric_modeling_book, 
    place={Cambridge}, 
    title={Atmospheric Modeling, Data Assimilation and Predictability}, 
    publisher={Cambridge University Press}, 
    author={Kalnay, Eugenia}, 
    year={2002}
}


@article{panguweather,
  title={Accurate medium-range global weather forecasting with 3D neural networks},
  author={Bi, Kaifeng and Xie, Lingxi and Zhang, Hengheng and Chen, Xin and Gu, Xiaotao and Tian, Qi},
  journal={Nature},
  pages={1--6},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{keisler,
	title = {Forecasting Global Weather with Graph Neural Networks},
	author = {Keisler, Ryan},
	year = {2022},
    journal={arXiv preprint arXiv:2202.07575}
}

@article{neural_lam,
  title={Probabilistic Weather Forecasting with Hierarchical Graph Neural Networks}, 
  author={Joel Oskarsson and Tomas Landelius and Marc Peter Deisenroth and Fredrik Lindsten},
  year={2024},
  journal={arXiv preprint arXiv:2406.04759}
}

@article{meps,
	title = {{AROME}-{MetCoOp}: A Nordic Convective-Scale Operational Weather Prediction Model},
	shorttitle = {{AROME}-{MetCoOp}},
	journaltitle = {Weather and Forecasting},
	journal = {Weather and Forecasting},
	author = {M\"{u}ller, Malte and Homleid, Mariken and Ivarsson, Karl-Ivar and K{\o}ltzow, Morten A. {\O} and Lindskog, Magnus and Midtb{\o}, Knut Helge and Andrae, Ulf and Aspelien, Trygve and Berggren, Lars and Bj{\o}rge, Dag and Dahlgren, Per and Kristiansen, J{\o}rn and Randriamampianina, Roger and Ridal, Martin and Vignes, Ole},
	date = {2017-04-01},
	year = {2017},
    publisher = {American Meteorological Society}
}

@article{fengwu,
	title = {{FengWu}: Pushing the Skillful Global Medium-range Weather Forecast beyond 10 Days Lead},
	author = {Chen, Kang and Han, Tao and Gong, Junchao and Bai, Lei and Ling, Fenghua and Luo, Jing-Jia and Chen, Xi and Ma, Leiming and Zhang, Tianning and Su, Rui and Ci, Yuanzheng and Li, Bin and Yang, Xiaokang and Ouyang, Wanli},
	year = {2023},
    journal={arXiv preprint arXiv:2304.02948}
}

@misc{weatherbench2,
      title={WeatherBench 2: A benchmark for the next generation of data-driven global weather models}, 
      author={Stephan Rasp and Stephan Hoyer and Alexander Merose and Ian Langmore and Peter Battaglia and Tyler Russel and Alvaro Sanchez-Gonzalez and Vivian Yang and Rob Carver and Shreya Agrawal and Matthew Chantry and Zied Ben Bouallegue and Peter Dueben and Carla Bromberg and Jared Sisk and Luke Barrington and Aaron Bell and Fei Sha},
      year={2024},
      eprint={2308.15560},
      archivePrefix={arXiv},
      primaryClass={physics.ao-ph}
}

@misc{fuxi,
      title={FuXi: A cascade machine learning forecasting system for 15-day global weather forecast}, 
      author={Lei Chen and Xiaohui Zhong and Feng Zhang and Yuan Cheng and Yinghui Xu and Yuan Qi and Hao Li},
      year={2023},
      eprint={2306.12873},
      archivePrefix={arXiv},
      primaryClass={physics.ao-ph}
}

@article{swin_vrnn,
author = {Hu, Yuan and Chen, Lei and Wang, Zhibin and Li, Hao},
title = {SwinVRNN: A Data-Driven Ensemble Forecasting Model via Learned Distribution Perturbation},
journal = {Journal of Advances in Modeling Earth Systems},
volume = {15},
number = {2},
pages = {e2022MS003211},
keywords = {medium-range weather forecasting, data-driven method, ensemble forecast, learned distribution perturbation},
doi = {https://doi.org/10.1029/2022MS003211},
url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2022MS003211},
eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2022MS003211},
note = {e2022MS003211 2022MS003211},
year = {2023}
}

@book{fundamentals_of_nwp,
  title={Fundamentals of numerical weather prediction},
  author={Coiffier, Jean},
  year={2011},
  publisher={Cambridge University Press}
}

@ARTICLE{GNNs2009,
  author={Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
  journal={IEEE Transactions on Neural Networks}, 
  title={The Graph Neural Network Model}, 
  year={2009},
  volume={20},
  number={1},
  pages={61-80},
  keywords={Neural networks;Biological system modeling;Data engineering;Computer vision;Chemistry;Biology;Pattern recognition;Data mining;Supervised learning;Parameter estimation;Graphical domains;graph neural networks (GNNs);graph processing;recursive neural networks},
  doi={10.1109/TNN.2008.2005605}}


@InProceedings{Yin_2022_CVPR,
    author    = {Yin, Hongxu and Vahdat, Arash and Alvarez, Jose M. and Mallya, Arun and Kautz, Jan and Molchanov, Pavlo},
    title     = {A-ViT: Adaptive Tokens for Efficient Vision Transformer},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {06},
    year      = {2022},
    pages     = {10809-10818}
}

@article{GENEVA2022272,
title = {Transformers for modeling physical systems},
journal = {Neural Networks},
volume = {146},
pages = {272-289},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.11.022},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004500},
author = {Nicholas Geneva and Nicholas Zabaras},
keywords = {Transformers, Deep learning, Self-attention, Physics, Koopman, Surrogate modeling},
abstract = {Transformers are widely used in natural language processing due to their ability to model longer-term dependencies in text. Although these models achieve state-of-the-art performance for many language related tasks, their applicability outside of the natural language processing field has been minimal. In this work, we propose the use of transformer models for the prediction of dynamical systems representative of physical phenomena. The use of Koopman based embeddings provides a unique and powerful method for projecting any dynamical system into a vector representation which can then be predicted by a transformer. The proposed model is able to accurately predict various dynamical systems and outperform classical methods that are commonly used in the scientific machine learning literature.11Code available at: https://github.com/zabaras/transformer-physx.}
}


@InProceedings{pmlr-v119-sanchez-gonzalez20a,
  title = 	 {Learning to Simulate Complex Physics with Graph Networks},
  author =       {Sanchez-Gonzalez, Alvaro and Godwin, Jonathan and Pfaff, Tobias and Ying, Rex and Leskovec, Jure and Battaglia, Peter},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {8459--8468},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {07},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/sanchez-gonzalez20a/sanchez-gonzalez20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/sanchez-gonzalez20a.html},
  abstract = 	 {Here we present a machine learning framework and model implementation that can learn to simulate a wide variety of challenging physical domains, involving fluids, rigid solids, and deformable materials interacting with one another. Our framework—which we term "Graph Network-based Simulators" (GNS)—represents the state of a physical system with particles, expressed as nodes in a graph, and computes dynamics via learned message-passing. Our results show that our model can generalize from single-timestep predictions with thousands of particles during training, to different initial conditions, thousands of timesteps, and at least an order of magnitude more particles at test time. Our model was robust to hyperparameter choices across various evaluation metrics: the main determinants of long-term performance were the number of message-passing steps, and mitigating the accumulation of error by corrupting the training data with noise. Our GNS framework advances the state-of-the-art in learned physical simulation, and holds promise for solving a wide range of complex forward and inverse problems.}
}

@inproceedings{
brandstetter2022message,
title={Message Passing Neural {PDE} Solvers},
author={Johannes Brandstetter and Daniel E. Worrall and Max Welling},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=vSix3HPYKSU}
}

@article{
li2023transformer,
title={Transformer for Partial Differential Equations{\textquoteright} Operator Learning},
author={Zijie Li and Kazem Meidani and Amir Barati Farimani},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=EPPqt3uERT},
note={}
}

@article{Chandrasekhar,
  title = {Stochastic Problems in Physics and Astronomy},
  author = {Chandrasekhar, S.},
  journal = {Rev. Mod. Phys.},
  volume = {15},
  issue = {1},
  pages = {1--89},
  numpages = {0},
  year = {1943},
  month = {1},
  publisher = {American Physical Society},
  doi = {10.1103/RevModPhys.15.1},
  url = {https://link.aps.org/doi/10.1103/RevModPhys.15.1}
}

@Book{thermodynamics_textbook,
author={Tipler, Paul Allen, 1933-},
title={Physics for scientists and engineers. Volume 1, Mechanics, oscillations and waves, thermodynamics},
year={2008},
publisher={Sixth edition. New York : W.H. Freeman, [2008] {\textcopyright}2008},
abstract={xxxi, 692 pages : illustrations ; 28 cm},
note={Contains chapters 1-20, R of complete sixth edition.;Includes bibliographical references and index.},
url={https://search.library.wisc.edu/catalog/9910077755502121}
}

@article{Gopakumar_2024,
doi = {10.1088/1741-4326/ad313a},
url = {https://dx.doi.org/10.1088/1741-4326/ad313a},
year = {2024},
month = {4},
publisher = {IOP Publishing},
volume = {64},
number = {5},
pages = {056025},
author = {Vignesh Gopakumar and Stanislas Pamela and Lorenzo Zanisi and Zongyi Li and Ander Gray and Daniel Brennand and Nitesh Bhatia and Gregory Stathopoulos and Matt Kusner and Marc Peter Deisenroth and Anima Anandkumar and the JOREK Team and MAST Team},
title = {Plasma surrogate modelling using Fourier neural operators},
journal = {Nuclear Fusion},
abstract = {Predicting plasma evolution within a Tokamak reactor is crucial to realizing the goal of sustainable fusion. Capabilities in forecasting the spatio-temporal evolution of plasma rapidly and accurately allow us to quickly iterate over design and control strategies on current Tokamak devices and future reactors. Modelling plasma evolution using numerical solvers is often expensive, consuming many hours on supercomputers, and hence, we need alternative inexpensive surrogate models. We demonstrate accurate predictions of plasma evolution both in simulation and experimental domains using deep learning-based surrogate modelling tools, viz., Fourier neural operators (FNO). We show that FNO has a speedup of six orders of magnitude over traditional solvers in predicting the plasma dynamics simulated from magnetohydrodynamic models, while maintaining a high accuracy (Mean Squared Error in the normalised domain ). Our modified version of the FNO is capable of solving multi-variable Partial Differential Equations, and can capture the dependence among the different variables in a single model. FNOs can also predict plasma evolution on real-world experimental data observed by the cameras positioned within the MAST Tokamak, i.e. cameras looking across the central solenoid and the divertor in the Tokamak. We show that FNOs are able to accurately forecast the evolution of plasma and have the potential to be deployed for real-time monitoring. We also illustrate their capability in forecasting the plasma shape, the locations of interactions of the plasma with the central solenoid and the divertor for the full (available) duration of the plasma shot within MAST. The FNO offers a viable alternative for surrogate modelling as it is quick to train and infer, and requires fewer data points, while being able to do zero-shot super-resolution and getting high-fidelity solutions.}
}

@Article{Azizzadenesheli2024,
author={Azizzadenesheli, Kamyar
and Kovachki, Nikola
and Li, Zongyi
and Liu-Schiaffini, Miguel
and Kossaifi, Jean
and Anandkumar, Anima},
title={Neural operators for accelerating scientific simulations and design},
journal={Nature Reviews Physics},
year={2024},
month={04},
day={08},
abstract={Scientific discovery and engineering design are currently limited by the time and cost of physical experiments. Numerical simulations are an alternative approach but are usually intractable for complex real-world problems. Artificial intelligence promises a solution through fast data-driven surrogate models. In particular, neural operators present a principled framework for learning mappings between functions defined on continuous domains, such as spatiotemporal processes and partial differential equations. Neural operators can extrapolate and predict solutions at new locations unseen during training. They can be integrated with physics and other domain constraints enforced at finer resolutions to obtain high-fidelity solutions and good generalization. Neural operators are differentiable, so they can directly optimize parameters for inverse design and other inverse problems. Neural operators can therefore augment, or even replace, existing numerical simulators in many applications, such as computational fluid dynamics, weather forecasting and material modelling, providing speedups of four to five orders of magnitude.},
issn={2522-5820},
doi={10.1038/s42254-024-00712-5},
url={https://doi.org/10.1038/s42254-024-00712-5}
}

@Article{Begoli2019,
author={Begoli, Edmon
and Bhattacharya, Tanmoy
and Kusnezov, Dimitri},
title={The need for uncertainty quantification in machine-assisted medical decision making},
journal={Nature Machine Intelligence},
year={2019},
month={1},
day={01},
volume={1},
number={1},
pages={20-23},
abstract={Medicine, even from the earliest days of artificial intelligence (AI) research, has been one of the most inspiring and promising domains for the application of AI-based approaches. Equally, it has been one of the more challenging areas to see an effective adoption. There are many reasons for this, primarily the reluctance to delegate decision making to machine intelligence in cases where patient safety is at stake. To address some of these challenges, medical AI, especially in its modern data-rich deep learning guise, needs to develop a principled and formal uncertainty quantification (UQ) discipline, just as we have seen in fields such as nuclear stockpile stewardship and risk management. The data-rich world of AI-based learning and the frequent absence of a well-understood underlying theory poses its own unique challenges to straightforward adoption of UQ. These challenges, while not trivial, also present significant new research opportunities for the development of new theoretical approaches, and for the practical applications of UQ in the area of machine-assisted medical decision making. Understanding prediction system structure and defensibly quantifying uncertainty is possible, and, if done, can significantly benefit both research and practical applications of AI in this critical domain.},
issn={2522-5839},
doi={10.1038/s42256-018-0004-1},
url={https://doi.org/10.1038/s42256-018-0004-1}
}

@book{dataset_shift_book,
author = {Quionero-Candela, Joaquin and Sugiyama, Masashi and Schwaighofer, Anton and Lawrence, Neil D.},
title = {Dataset Shift in Machine Learning},
year = {2009},
isbn = {0262170051},
publisher = {The MIT Press},
abstract = {Dataset shift is a common problem in predictive modeling that occurs when the joint distribution of inputs and outputs differs between training and test stages. Covariate shift, a particular case of dataset shift, occurs when only the input distribution changes. Dataset shift is present in most practical applications, for reasons ranging from the bias introduced by experimental design to the irreproducibility of the testing conditions at training time. (An example is -email spam filtering, which may fail to recognize spam that differs in form from the spam the automatic filter has been built on.) Despite this, and despite the attention given to the apparently similar problems of semi-supervised learning and active learning, dataset shift has received relatively little attention in the machine learning community until recently. This volume offers an overview of current efforts to deal with dataset and covariate shift. The chapters offer a mathematical and philosophical introduction to the problem, place dataset shift in relationship to transfer learning, transduction, local learning, active learning, and semi-supervised learning, provide theoretical views of dataset and covariate shift (including decision theoretic and Bayesian perspectives), and present algorithms for covariate shift. Contributors: Shai Ben-David, Steffen Bickel, Karsten Borgwardt, Michael Brckner, David Corfield, Amir Globerson, Arthur Gretton, Lars Kai Hansen, Matthias Hein, Jiayuan Huang, Takafumi Kanamori, Klaus-Robert Mller, Sam Roweis, Neil Rubens, Tobias Scheffer, Marcel Schmittfull, Bernhard Schlkopf, Hidetoshi Shimodaira, Alex Smola, Amos Storkey, Masashi Sugiyama, Choon Hui Teo Neural Information Processing series}
}


@inproceedings{attention2017,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}


@inproceedings{
mccabe2023multiple,
title={Multiple Physics Pretraining for Physical Surrogate Models},
author={Michael McCabe and Bruno R{\'e}galdo-Saint Blancard and Liam Parker and Ruben Ohana and Miles Cranmer and Alberto Bietti and Michael Eickenberg and Siavash Golkar and Geraud Krawezik and Francois Lanusse and Mariel Pettee and Tiberiu Tesileanu and Kyunghyun Cho and Shirley Ho},
booktitle={NeurIPS 2023 AI for Science Workshop},
year={2023},
url={https://openreview.net/forum?id=M12lmQKuxa}
}


@misc{alkin2024upt,
      title={Universal Physics Transformers: A Framework For Efficiently Scaling Neural Operators}, 
      author={Benedikt Alkin and Andreas Fürst and Simon Schmid and Lukas Gruber and Markus Holzleitner and Johannes Brandstetter},
      year={2024},
      eprint={2402.12365},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{hao2024dpot,
      title={DPOT: Auto-Regressive Denoising Operator Transformer for Large-Scale PDE Pre-Training}, 
      author={Zhongkai Hao and Chang Su and Songming Liu and Julius Berner and Chengyang Ying and Hang Su and Anima Anandkumar and Jian Song and Jun Zhu},
      year={2024},
      eprint={2403.03542},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{bommasani2022opportunities,
      title={On the Opportunities and Risks of Foundation Models}, 
      author={Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and Shyamal Buch and Dallas Card and Rodrigo Castellon and Niladri Chatterji and Annie Chen and Kathleen Creel and Jared Quincy Davis and Dora Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren Gillespie and Karan Goel and Noah Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and Omar Khattab and Pang Wei Koh and Mark Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Ben Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and Julian Nyarko and Giray Ogut and Laurel Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Rob Reich and Hongyu Ren and Frieda Rong and Yusuf Roohani and Camilo Ruiz and Jack Ryan and Christopher Ré and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishnan Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian Tramèr and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang},
      year={2022},
      eprint={2108.07258},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{rahman2024pretraining,
      title={Pretraining Codomain Attention Neural Operators for Solving Multiphysics PDEs}, 
      author={Md Ashiqur Rahman and Robert Joseph George and Mogab Elleithy and Daniel Leibovici and Zongyi Li and Boris Bonev and Colin White and Julius Berner and Raymond A. Yeh and Jean Kossaifi and Kamyar Azizzadenesheli and Anima Anandkumar},
      year={2024},
      eprint={2403.12553},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{yin2022avit,
    title={{A}-{V}i{T}: {A}daptive Tokens for Efficient Vision Transformer},
    author={Yin, Hongxu and Vahdat, Arash and Alvarez, Jose and Mallya, Arun and Kautz, Jan and Molchanov, Pavlo},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    year={2022}
}

@INPROCEEDINGS{safetycriticalsystems,
  author={Knight, J.C.},
  booktitle={Proceedings of the 24th International Conference on Software Engineering. ICSE 2002}, 
  title={Safety critical systems: challenges and directions}, 
  year={2002},
  volume={},
  number={},
  pages={547-550},
  keywords={Modems;Application software;Surgery;Aircraft;Aerospace control;Weapons;Software safety;Information security;Power system security;Pacemakers},
  doi={}}

@misc{sun2022conformal,
      title={Conformal Methods for Quantifying Uncertainty in Spatiotemporal Data: A Survey}, 
      author={Sophia Sun},
      year={2022},
      eprint={2209.03580},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@inproceedings{conformaltimeserires,
 author = {Stankeviciute, Kamile and M. Alaa, Ahmed and van der Schaar, Mihaela},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {6216--6228},
 publisher = {Curran Associates, Inc.},
 title = {Conformal Time-series Forecasting},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/312f1ba2a72318edaaa995a67835fad5-Paper.pdf},
 volume = {34},
 year = {2021}
}

@article{Poels_2023,
doi = {10.1088/1741-4326/acf70d},
url = {https://dx.doi.org/10.1088/1741-4326/acf70d},
year = {2023},
month = {sep},
publisher = {IOP Publishing},
volume = {63},
number = {12},
pages = {126012},
author = {Yoeri Poels and Gijs Derks and Egbert Westerhof and Koen Minartz and Sven Wiesen and Vlado Menkovski},
title = {Fast dynamic 1D simulation of divertor plasmas with neural PDE surrogates},
journal = {Nuclear Fusion},
abstract = {Managing divertor plasmas is crucial for operating reactor scale tokamak devices due to heat and particle flux constraints on the divertor target. Simulation is an important tool to understand and control these plasmas, however, for real-time applications or exhaustive parameter scans only simple approximations are currently fast enough. We address this lack of fast simulators using neural partial differential equation (PDE) surrogates, data-driven neural network-based surrogate models trained using solutions generated with a classical numerical method. The surrogate approximates a time-stepping operator that evolves the full spatial solution of a reference physics-based model over time. We use DIV1D, a 1D dynamic model of the divertor plasma, as reference model to generate data. DIV1D’s domain covers a 1D heat flux tube from the X-point (upstream) to the target. We simulate a realistic TCV divertor plasma with dynamics induced by upstream density ramps and provide an exploratory outlook towards fast transients. State-of-the-art neural PDE surrogates are evaluated in a common framework and extended for properties of the DIV1D data. We evaluate (1) the speed-accuracy trade-off; (2) recreating non-linear behavior; (3) data efficiency; and (4) parameter inter- and extrapolation. Once trained, neural PDE surrogates can faithfully approximate DIV1D’s divertor plasma dynamics at sub real-time computation speeds: In the proposed configuration,  ms of plasma dynamics can be computed in  ms of wall-clock time, several orders of magnitude faster than DIV1D.}
}



@InProceedings{cp_dynamic_timeseries,
  title = 	 {Conformal prediction interval for dynamic time-series},
  author =       {Xu, Chen and Xie, Yao},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {11559--11569},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/xu21h/xu21h.pdf},
  url = 	 {https://proceedings.mlr.press/v139/xu21h.html},
  abstract = 	 {We develop a method to construct distribution-free prediction intervals for dynamic time-series, called \Verb|EnbPI| that wraps around any bootstrap ensemble estimator to construct sequential prediction intervals. \Verb|EnbPI| is closely related to the conformal prediction (CP) framework but does not require data exchangeability. Theoretically, these intervals attain finite-sample, \textit{approximately valid} marginal coverage for broad classes of regression functions and time-series with strongly mixing stochastic errors. Computationally, \Verb|EnbPI| avoids overfitting and requires neither data-splitting nor training multiple ensemble estimators; it efficiently aggregates bootstrap estimators that have been trained. In general, \Verb|EnbPI| is easy to implement, scalable to producing arbitrarily many prediction intervals sequentially, and well-suited to a wide range of regression functions. We perform extensive real-data analyses to demonstrate its effectiveness.}
}


@ARTICLE{CP_Wildfire,
  author={Xu, Chen and Xie, Yao and Vazquez, Daniel A. Zuniga and Yao, Rui and Qiu, Feng},
  journal={IEEE Journal on Selected Areas in Information Theory}, 
  title={Spatio-Temporal Wildfire Prediction Using Multi-Modal Data}, 
  year={2023},
  volume={4},
  number={},
  pages={302-313},
  keywords={Predictive models;Sensors;Real-time systems;Spatiotemporal phenomena;Multisensor systems;Fire safety;Wildfires;Spatial-temporal point process;conformal prediction;multi-sensor network;fire safety},
  doi={10.1109/JSAIT.2023.3276054}}

@article{ma2024calibrated,
  title={Calibrated Uncertainty Quantification for Operator Learning via Conformal Prediction},
  author={Ma, Ziqi and Azizzadenesheli, Kamyar and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2402.01960},
  year={2024}
}

@article{ensembleforecasting_jcp,
title = {Ensemble forecasting},
journal = {Journal of Computational Physics},
volume = {227},
number = {7},
pages = {3515-3539},
year = {2008},
note = {Predicting weather, climate and extreme events},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2007.02.014},
url = {https://www.sciencedirect.com/science/article/pii/S0021999107000812},
author = {M. Leutbecher and T.N. Palmer},
keywords = {Uncertainty, Numerical weather prediction, Predictability},
abstract = {Numerical weather prediction models as well as the atmosphere itself can be viewed as nonlinear dynamical systems in which the evolution depends sensitively on the initial conditions. The fact that estimates of the current state are inaccurate and that numerical models have inadequacies, leads to forecast errors that grow with increasing forecast lead time. The growth of errors depends on the flow itself. Ensemble forecasting aims at quantifying this flow-dependent forecast uncertainty. The sources of uncertainty in weather forecasting are discussed. Then, an overview is given on evaluating probabilistic forecasts and their usefulness compared with single forecasts. Thereafter, the representation of uncertainties in ensemble forecasts is reviewed with an emphasis on the initial condition perturbations. The review is complemented by a detailed description of the methodology to generate initial condition perturbations of the Ensemble Prediction System (EPS) of the European Centre for Medium-Range Weather Forecasts (ECMWF). These perturbations are based on the leading part of the singular value decomposition of the operator describing the linearised dynamics over a finite time interval. The perturbations are flow-dependent as the linearisation is performed with respect to a solution of the nonlinear forecast model. The extent to which the current ECMWF ensemble prediction system is capable of predicting flow-dependent variations in uncertainty is assessed for the large-scale flow in mid-latitudes.}
}

@misc{Palmer2009,
  author = {T.N. Palmer and Roberto Buizza and F. Doblas-Reyes and T. Jung and Martin Leutbecher and G.J. Shutts and M. Steinheimer and Antje Weisheimer},
  title = {Stochastic parametrization and model uncertainty},
  abstract = {Stochastic parametrization provides a methodology for representing model uncertainty in ensemble forecasts, and also has the capability of reducing systematic error through the concept of nonlinear noise-induced rectification. The stochastically perturbed parametrization tendencies scheme and the stochastic backscatter scheme are described and their impact on medium-range forecast skill is discussed. The impact of these schemes on ensemble data assimilation and in seasonal forecasting is also considered. In all cases, the results are positive. Validation of the form of these stochastic parametrizations can be found by coarse-grain budgets of high resolution (e.g. cloud-resolving) models; some results are shown. Stochastic parametrization has been pioneered at ECMWF over the last decade, and now most operational centres use stochastic parametrization in their operational ensemble prediction systems - these are briefly discussed. The seamless prediction paradigm implies that serious consideration should now be given to the use of stochastic parametrization in next generation Earth System Models.},
  year = {2009},
  journal = {ECMWF Technical Memoranda},
  number = {598},
  pages = {42},
  month = {10/2009},
  publisher = {ECMWF},
  url = {https://www.ecmwf.int/node/11577},
  doi = {10.21957/ps8gbwbdv},
  language = {eng},
}

@article{Buizza2008,
author = {Buizza, Roberto and Leutbecher, Martin and Isaksen, Lars},
title = {Potential use of an ensemble of analyses in the ECMWF Ensemble Prediction System},
journal = {Quarterly Journal of the Royal Meteorological Society},
volume = {134},
number = {637},
pages = {2051-2066},
keywords = {ensemble prediction, ensemble data assimilation, predictability},
doi = {https://doi.org/10.1002/qj.346},
url = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.346},
eprint = {https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/qj.346},
abstract = {Abstract One of the crucial aspects of the design of an ensemble prediction system is the definition of the ensemble of initial states. This work investigates the use of singular vectors, an ensemble of analyses, and a combination of the two types of perturbations in the ECMWF operational ensemble prediction system. First, the similarity between perturbations generated using initial-time singular vectors (SVs) and analyses from the ensemble data assimilation (EDA) system is assessed. Results show that the EDA perturbations are less localized geographically and have a better coverage of the Tropics. EDA perturbations have also smaller scales than SV-based perturbations, and have a less evident upshear vertical tilt, which explains why they grow less with forecast time. Then, the use of EDA-based perturbations in the ECMWF ensemble prediction system is studied. Results indicate that if used alone, EDA-based perturbations lead to an under-dispersive and less skilful ensemble then the one based on initial-time SVs only. Combining the EDA and the initial-time SVs gives a system with a better agreement between ensemble spread and the error of the ensemble mean, a smaller ensemble-mean error and more skilful probabilistic forecasts than the current operational system based on initial-time and evolved SVs. Copyright © 2008 Royal Meteorological Society},
year = {2008}
}

@misc{bulte2024uncertainty,
      title={Uncertainty quantification for data-driven weather models}, 
      author={Christopher Bülte and Nina Horat and Julian Quinting and Sebastian Lerch},
      year={2024},
      eprint={2403.13458},
      archivePrefix={arXiv},
      primaryClass={physics.ao-ph}
}

@article{H_hlein_2024,
   title={Postprocessing of Ensemble Weather Forecasts Using Permutation-Invariant Neural Networks},
   volume={3},
   ISSN={2769-7525},
   url={http://dx.doi.org/10.1175/AIES-D-23-0070.1},
   DOI={10.1175/aies-d-23-0070.1},
   number={1},
   journal={Artificial Intelligence for the Earth Systems},
   publisher={American Meteorological Society},
   author={Höhlein, Kevin and Schulz, Benedikt and Westermann, Rüdiger and Lerch, Sebastian},
   year={2024},
   month=jan }


@Article{Bi2023,
author={Bi, Kaifeng
and Xie, Lingxi
and Zhang, Hengheng
and Chen, Xin
and Gu, Xiaotao
and Tian, Qi},
title={Accurate medium-range global weather forecasting with 3D neural networks},
journal={Nature},
year={2023},
month={Jul},
day={01},
volume={619},
number={7970},
pages={533-538},
abstract={Weather forecasting is important for science and society. At present, the most accurate forecast system is the numerical weather prediction (NWP) method, which represents atmospheric states as discretized grids and numerically solves partial differential equations that describe the transition between those states1. However, this procedure is computationally expensive. Recently, artificial-intelligence-based methods2 have shown potential in accelerating weather forecasting by orders of magnitude, but the forecast accuracy is still significantly lower than that of NWP methods. Here we introduce an artificial-intelligence-based method for accurate, medium-range global weather forecasting. We show that three-dimensional deep networks equipped with Earth-specific priors are effective at dealing with complex patterns in weather data, and that a hierarchical temporal aggregation strategy reduces accumulation errors in medium-range forecasting. Trained on 39{\thinspace}years of global data, our program, Pangu-Weather, obtains stronger deterministic forecast results on reanalysis data in all tested variables when compared with the world's best NWP system, the operational integrated forecasting system of the European Centre for Medium-Range Weather Forecasts (ECMWF)3. Our method also works well with extreme weather forecasts and ensemble forecasts. When initialized with reanalysis data, the accuracy of tracking tropical cyclones is also higher than that of ECMWF-HRES.},
issn={1476-4687},
doi={10.1038/s41586-023-06185-3},
url={https://doi.org/10.1038/s41586-023-06185-3}
}

@article{Mariana2019,
author = {Clare, Mariana C.A. and Jamil, Omar and Morcrette, Cyril J.},
title = {Combining distribution-based neural networks to predict weather forecast probabilities},
journal = {Quarterly Journal of the Royal Meteorological Society},
volume = {147},
number = {741},
pages = {4337-4357},
keywords = {data exploration, deep learning, ensemble dropout, probabilistic weather forecasting, probability density functions, ResNet, stacked neural network},
doi = {https://doi.org/10.1002/qj.4180},
url = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.4180},
eprint = {https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/qj.4180},
abstract = {Abstract The success of deep learning techniques over the last decades has opened up a new avenue of research for weather forecasting. Here, we take the novel approach of using a neural network to predict full probability density functions at each point in space and time rather than a single output value, thus producing a probabilistic weather forecast. This enables the calculation of both uncertainty and skill metrics for the neural network predictions, and overcomes the common difficulty of inferring uncertainty from these predictions. This approach is data-driven and the neural network is trained on the WeatherBench dataset (processed ERA5 data) to forecast geopotential and temperature 3 and 5 days ahead. Data exploration leads to the identification of the most important input variables. In order to increase computational efficiency, several neural networks are trained on small subsets of these variables. The outputs are then combined through a stacked neural network, the first time such a technique has been applied to weather data. Our approach is found to be more accurate than some coarse numerical weather prediction models and as accurate as more complex alternative neural networks, with the added benefit of providing key probabilistic information necessary for making informed weather forecasts.},
year = {2021}
}

@article{price2024gencast,
  title={Gencast: Diffusion-based ensemble forecasting for medium-range weather},
  author={Price, Ilan and Sanchez-Gonzalez, Alvaro and Alet, Ferran and Ewalds, Timo and El-Kadi, Andrew and Stott, Jacklynn and Mohamed, Shakir and Battaglia, Peter and Lam, Remi and Willson, Matthew},
  journal={arXiv preprint arXiv:2312.15796},
  year={2023}
}

@misc{keisler2022forecasting,
      title={Forecasting Global Weather with Graph Neural Networks}, 
      author={Ryan Keisler},
      year={2022},
      eprint={2202.07575},
      archivePrefix={arXiv},
      primaryClass={physics.ao-ph}
}


@Article{Chen2023,
author={Chen, Lei
and Zhong, Xiaohui
and Zhang, Feng
and Cheng, Yuan
and Xu, Yinghui
and Qi, Yuan
and Li, Hao},
title={FuXi: a cascade machine learning forecasting system for 15-day global weather forecast},
journal={npj Climate and Atmospheric Science},
year={2023},
month={Nov},
day={16},
volume={6},
number={1},
pages={190},
abstract={Over the past few years, the rapid development of machine learning (ML) models for weather forecasting has led to state-of-the-art ML models that have superior performance compared to the European Centre for Medium-Range Weather Forecasts (ECMWF)'s high-resolution forecast (HRES), which is widely considered as the world's best physics-based weather forecasting system. Specifically, ML models have outperformed HRES in 10-day forecasts with a spatial resolution of 0.25∘. However, the challenge remains in mitigating the accumulation of forecast errors for longer effective forecasts, such as achieving comparable performance to the ECMWF ensemble in 15-day forecasts. Despite various efforts to reduce accumulation errors, such as implementing autoregressive multi-time step loss, relying on a single model has been found to be insufficient for achieving optimal performance in both short and long lead times. Therefore, we present FuXi, a cascaded ML weather forecasting system that provides 15-day global forecasts at a temporal resolution of 6 hours and a spatial resolution of 0.25∘. FuXi is developed using 39 years of the ECMWF ERA5 reanalysis dataset. The performance evaluation demonstrates that FuXi has forecast performance comparable to ECMWF ensemble mean (EM) in 15-day forecasts. FuXi surpasses the skillful forecast lead time achieved by ECMWF HRES by extending the lead time for Z500 from 9.25 to 10.5 days and for T2M from 10 to 14.5 days. Moreover, the FuXi ensemble is created by perturbing initial conditions and model parameters, enabling it to provide forecast uncertainty and demonstrating promising results when compared to the ECMWF ensemble.},
issn={2397-3722},
doi={10.1038/s41612-023-00512-1},
url={https://doi.org/10.1038/s41612-023-00512-1}
}

@misc{nguyen2023climax,
      title={ClimaX: A foundation model for weather and climate}, 
      author={Tung Nguyen and Johannes Brandstetter and Ashish Kapoor and Jayesh K. Gupta and Aditya Grover},
      year={2023},
      eprint={2301.10343},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{lessig2023atmorep,
      title={AtmoRep: A stochastic model of atmosphere dynamics using large scale representation learning}, 
      author={Christian Lessig and Ilaria Luise and Bing Gong and Michael Langguth and Scarlet Stadtler and Martin Schultz},
      year={2023},
      eprint={2308.13280},
      archivePrefix={arXiv},
      primaryClass={physics.ao-ph}
}


@InProceedings{spherical_fno,
  title = 	 {Spherical {F}ourier Neural Operators: Learning Stable Dynamics on the Sphere},
  author =       {Bonev, Boris and Kurth, Thorsten and Hundt, Christian and Pathak, Jaideep and Baust, Maximilian and Kashinath, Karthik and Anandkumar, Anima},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {2806--2823},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/bonev23a/bonev23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/bonev23a.html},
  abstract = 	 {Fourier Neural Operators (FNOs) have proven to be an efficient and effective method for resolution-independent operator learning in a broad variety of application areas across scientific machine learning. A key reason for their success is their ability to accurately model long-range dependencies in spatio-temporal data by learning global convolutions in a computationally efficient manner. To this end, FNOs rely on the discrete Fourier transform (DFT), however, DFTs cause visual and spectral artifacts as well as pronounced dissipation when learning operators in spherical coordinates by incorrectly assuming flat geometry. To overcome this limitation, we generalize FNOs on the sphere, introducing Spherical FNOs (SFNOs) for learning operators on spherical geometries. We apply SFNOs to forecasting atmo- spheric dynamics, and demonstrate stable autoregressive rollouts for a year of simulated time (1,460 steps), while retaining physically plausible dynamics. The SFNO has important implications for machine learning-based simulation of climate dynamics that could eventually help accelerate our response to climate change.}
}

@misc{kochkov2024neural,
      title={Neural General Circulation Models for Weather and Climate}, 
      author={Dmitrii Kochkov and Janni Yuval and Ian Langmore and Peter Norgaard and Jamie Smith and Griffin Mooers and Milan Klöwer and James Lottes and Stephan Rasp and Peter Düben and Sam Hatfield and Peter Battaglia and Alvaro Sanchez-Gonzalez and Matthew Willson and Michael P. Brenner and Stephan Hoyer},
      year={2024},
      eprint={2311.07222},
      archivePrefix={arXiv},
      primaryClass={physics.ao-ph}
}

@inproceedings{
verma2024climode,
title={Clim{ODE}: Climate and Weather Forecasting with Physics-informed Neural {ODE}s},
author={Yogesh Verma and Markus Heinonen and Vikas Garg},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=xuY33XhEGR}
}

@inproceedings{calibration_of_large_neurwp,
  title={Calibration of Large Neural Weather Models},
  author={Graubner, Andre and Kamyar Azizzadenesheli, Kamyar and Pathak, Jaideep and Mardani, Morteza and Pritchard, Mike and Kashinath, Karthik and Anandkumar, Anima},
  booktitle={NeurIPS 2022 Workshop on Tackling Climate Change with Machine Learning},
  year={2022}
}

@article{neural_ensemble_svd,
	title = {Ensemble Methods for Neural Network-Based Weather Forecasts},
	journaltitle = {Journal of Advances in Modeling Earth Systems},
	journal = {Journal of Advances in Modeling Earth Systems},
	author = {Scher, Sebastian and Messori, Gabriele},
	urldate = {2023-09-12},
	year = {2021}
}

@techreport{bodnar2024aurora,
author = {Bodnar, Cristian and Bruinsma, Wessel and Lucic, Ana and Stanley, Megan and Brandstetter, Johannes and Garvan , Patrick and Riechert, Maik and Weyn, Jonathan and Dong, Haiyu and Vaughan, Anna and Gupta, Jayesh and Tambiratnam, Kit and Archibald, Alex and Heider, Elizabeth and Welling, Max and Turner, Richard and Perdikaris, Paris},
title = {Aurora: A Foundation Model of the Atmosphere},
institution = {Microsoft Research AI for Science},
year = {2024},
month = {May},
url = {https://www.microsoft.com/en-us/research/publication/aurora-a-foundation-model-of-the-atmosphere/},
number = {MSR-TR-2024-16},
}

@article{era5,
	title = {The {ERA}5 global reanalysis},
	journaltitle = {Quarterly Journal of the Royal Meteorological Society},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Hersbach, Hans and Bell, Bill and Berrisford, Paul and Hirahara, Shoji and Horányi, András and Muñoz-Sabater, Joaquín and Nicolas, Julien and Peubey, Carole and Radu, Raluca and Schepers, Dinand and Simmons, Adrian and Soci, Cornel and Abdalla, Saleh and Abellan, Xavier and Balsamo, Gianpaolo and Bechtold, Peter and Biavati, Gionata and Bidlot, Jean and Bonavita, Massimo and De Chiara, Giovanna and Dahlgren, Per and Dee, Dick and Diamantakis, Michail and Dragani, Rossana and Flemming, Johannes and Forbes, Richard and Fuentes, Manuel and Geer, Alan and Haimberger, Leo and Healy, Sean and Hogan, Robin J. and Hólm, Elías and Janisková, Marta and Keeley, Sarah and Laloyaux, Patrick and Lopez, Philippe and Lupu, Cristina and Radnoti, Gabor and de Rosnay, Patricia and Rozum, Iryna and Vamborg, Freja and Villaume, Sebastien and Thépaut, Jean-Noël},
	year = {2020},
}

@misc{diquigiovanni2021conformal,
      title={Conformal Prediction Bands for Multivariate Functional Data}, 
      author={Jacopo Diquigiovanni and Matteo Fontana and Simone Vantini},
      year={2021},
      eprint={2106.01792},
      archivePrefix={arXiv},
      primaryClass={id='stat.ME' full_name='Methodology' is_active=True alt_name=None in_archive='stat' is_general=False description='Design, Surveys, Model Selection, Multiple Testing, Multivariate Methods, Signal and Image Processing, Time Series, Smoothing, Spatial Statistics, Survival Analysis, Nonparametric and Semiparametric Methods'}
}

@article{messoudiCopula2021,
title = {Copula-based conformal prediction for multi-target regression},
journal = {Pattern Recognition},
volume = {120},
pages = {108101},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108101},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321002880},
author = {Soundouss Messoudi and Sébastien Destercke and Sylvain Rousseau},
keywords = {Inductive conformal prediction, Copula functions, Multi-target regression, Deep neural networks, Random forests},
abstract = {There are relatively few works dealing with conformal prediction for multi-task learning issues, and this is particularly true for multi-target regression. This paper focuses on the problem of providing valid (i.e., frequency calibrated) multi-variate predictions. To do so, we propose to use copula functions for inductive conformal prediction, and illustrate our proposal by applying it to deep neural networks and random forests. We show that the proposed method ensures efficiency and validity for multi-target regression problems on various data sets.}
}


@InProceedings{messoudiEllipsoidal2022,
  title = 	 {Ellipsoidal conformal inference for Multi-Target Regression},
  author =       {Messoudi, Soundouss and Destercke, S\'{e}bastien and Rousseau, Sylvain},
  booktitle = 	 {Proceedings of the Eleventh Symposium on Conformal and Probabilistic Prediction with Applications},
  pages = 	 {294--306},
  year = 	 {2022},
  editor = 	 {Johansson, Ulf and Boström, Henrik and An Nguyen, Khuong and Luo, Zhiyuan and Carlsson, Lars},
  volume = 	 {179},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {24--26 Aug},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v179/messoudi22a/messoudi22a.pdf},
  url = 	 {https://proceedings.mlr.press/v179/messoudi22a.html},
  abstract = 	 {Quantifying the uncertainty of a predictive model output is of essential importance in learning scenarios involving critical applications. As the learning task becomes more complex, so does uncertainty quantification. In this paper, we consider the task of multi-target regression and propose a method to output ellipsoidal confidence regions whose shapes are tailored to each instance to predict. We also guarantee that those confidence regions are well-calibrated, i.e., that they cover the ground truth with a specified probability. To achieve such a feat, we propose a conformal prediction method outputting ellipsoidal prediction regions. Experiments on both simulated and real-world data sets show that our methods outperform existing ones.  }
}

@INPROCEEDINGS{normalisedCP2021,
  author={Johansson, Ulf and Boström, Henrik and Löfström, Tuwe},
  booktitle={2021 IEEE Symposium Series on Computational Intelligence (SSCI)}, 
  title={Investigating Normalized Conformal Regressors}, 
  year={2021},
  volume={},
  number={},
  pages={01-08},
  keywords={Training;Computational modeling;Estimation;Forestry;Predictive models;Turning;Boosting;Conformal prediction;Predictive regression;Random forest;Gradient boosting},
  doi={10.1109/SSCI50451.2021.9659853}}



@article{Kirk2006,
doi = {10.1088/0741-3335/48/12B/S41},
url = {https://dx.doi.org/10.1088/0741-3335/48/12B/S41},
year = {2006},
month = {nov},
publisher = {},
volume = {48},
number = {12B},
pages = {B433},
author = {A Kirk and N Ben Ayed and G Counsell and B Dudson and T Eich and A Herrmann and B Koch and R Martin and A Meakins and S Saarelma and R Scannell and S Tallents and M Walsh and H R Wilson and the MAST team},
title = {Filament structures at the plasma edge on MAST},
journal = {Plasma Physics and Controlled Fusion},
abstract = {The boundary of the tokamak core plasma, or scrape-off layer, is normally characterized in terms of average parameters such as density, temperature and e-folding lengths suggesting diffusive losses. However, as is shown in this paper, localized filamentary structures play an important role in determining the radial efflux in both L mode and during edge localized modes (ELMs) on MAST. Understanding the size, poloidal and toroidal localization and the outward radial extent of these filaments is crucial in order to calculate their effect on power loading both on the first wall and the divertor target plates in future devices. The spatial and temporal evolution of filaments observed on MAST in L-mode and ELMs have been compared and contrasted in order to confront the predictions of various models that have been proposed to predict filament propagation and in particular ELM energy losses.}
}



@inproceedings{Ham2022,
	author = {Chris Ham and Andrew Kirk and Kevin Veruagh},
        title = {Insights on disruption physics in MAST using high speed visible camera data},
	booktitle = {IAEA Second Technical Meeting on Plasma Disruptions and their Mitigation},
	year = {2022},
	url = {https://conferences.iaea.org/event/281/contributions/24423/}
}



@Article{Walkden2022,
    author={Walkden, Nicholas
    and Riva, Fabio
    and Harrison, James
    and Militello, Fulvio
    and Farley, Thomas
    and Omotani, John
    and Lipschultz, Bruce},
    title={The physics of turbulence localised to the tokamak divertor volume},
    journal={Communications Physics},
    year={2022},
    month={Jun},
    day={01},
    volume={5},
    number={1},
    pages={139},
    abstract={Fusion power plant designs based on magnetic confinement, such as the tokamak design, offer a promising route to sustainable fusion power but require robust exhaust solutions capable of tolerating intense heat and particle fluxes from the plasma at the core of the device. Turbulent plasma transport in the region where the interface between the plasma and the materials of the device is handled - called the divertor volume - is poorly understood, yet impacts several key factors ultimately affecting device performance. In this article a comprehensive study of the underlying physics of turbulence in the divertor volume is conducted using data collected in the final experimental campaign of the Mega Ampere Spherical Tokamak device, compared to high fidelity nonlinear simulations. The physics of the turbulence is shown to be strongly dependant on the geometry of the divertor volume - a potentially important result as the community looks to advanced divertor designs with complex geometry for future fusion power plants. These results lay the foundations of a first-principles physics basis for turbulent transport in the tokamak divertor, providing a critical step towards a predictive understanding of tokamak divertor plasma solutions.},
    issn={2399-3650},
    doi={10.1038/s42005-022-00906-2},
    url={https://doi.org/10.1038/s42005-022-00906-2}
}

@article{giudicelli2024moose,
   title = {3.0 - {MOOSE}: Enabling massively parallel multiphysics simulations},
   author = {Guillaume Giudicelli and Alexander Lindsay and Logan Harbour and Casey Icenhour and
             Mengnan Li and Joshua E. Hansel and Peter German and Patrick Behne and Oana Marin and
             Roy H. Stogner and Jason M. Miller and Daniel Schwen and Yaqi Wang and Lynn Munday and
             Sebastian Schunert and Benjamin W. Spencer and Dewen Yushu and Antonio Recuero and
             Zachary M. Prince and Max Nezdyur and Tianchen Hu and Yinbin Miao and
             Yeon Sang Jung and Christopher Matthews and April Novak and Brandon Langley and
             Timothy Truster and Nuno Nobre and Brian Alger and David Andr{\v{s}} and
             Fande Kong and Robert Carlsen and Andrew E. Slaughter and John W. Peterson and
             Derek Gaston and Cody Permann},
    year = {2024},
 journal = {{SoftwareX}},
  volume = {26},
   pages = {101690},
    issn = {2352-7110},
     doi = {https://doi.org/10.1016/j.softx.2024.101690},
     url = {https://www.sciencedirect.com/science/article/pii/S235271102400061X},
keywords = {Framework, Finite-element, Finite-volume, Parallel, Multiphysics, Multiscale},
}

@ARTICLE{Hospital_Adam2015-bw,
  title    = "Molecular dynamics simulations: advances and applications",
  author   = "{Hospital, Adam} and Go{\~n}i, Josep Ramon and Orozco, Modesto
              and Gelp{\'\i}, Josep L",
  abstract = "Molecular dynamics simulations have evolved into a mature
              technique that can be used effectively to understand
              macromolecular structure-to-function relationships. Present
              simulation times are close to biologically relevant ones.
              Information gathered about the dynamic properties of
              macromolecules is rich enough to shift the usual paradigm of
              structural bioinformatics from studying single structures to
              analyze conformational ensembles. Here, we describe the
              foundations of molecular dynamics and the improvements made in
              the direction of getting such ensemble. Specific application of
              the technique to three main issues (allosteric regulation,
              docking, and structure refinement) is discussed.",
  journal  = "Adv Appl Bioinform Chem",
  volume   =  8,
  pages    = "37--47",
  month    =  nov,
  year     =  2015,
  address  = "New Zealand",
  keywords = "allostery; conformational ensembles; docking; molecular dynamics;
              refinement; structure prediction",
  language = "en"
}
@article{cesm2,
author = {Danabasoglu, G. and Lamarque, J.-F. and Bacmeister, J. and Bailey, D. A. and DuVivier, A. K.  and Edwards, J. and Emmons, L. K. and Fasullo, J. and Garcia, R. and Gettelman, A. and Hannay, C. and Holland, M. M. and Large, W. G. and Lauritzen, P. H. and Lawrence, D. M. and Lenaerts, J. T. M. and Lindsay, K. and Lipscomb, W. H. and Mills, M. J. and Neale, R. and Oleson, K. W. and Otto-Bliesner, B. and Phillips, A. S. and Sacks, W. and Tilmes, S. and van Kampenhout, L. and Vertenstein, M. and Bertini, A. and Dennis, J. and Deser, C. and Fischer, C. and Fox-Kemper, B. and Kay, J. E. and Kinnison, D. and Kushner, P. J. and Larson, V. E. and Long, M. C. and Mickelson, S. and Moore, J. K. and Nienhouse, E. and Polvani, L. and Rasch, P. J. and Strand, W. G.},
title = {The Community Earth System Model Version 2 (CESM2)},
journal = {Journal of Advances in Modeling Earth Systems},
volume = {12},
number = {2},
pages = {e2019MS001916},
keywords = {Community Earth System Model (CESM), global coupled Earth system modeling, preindustrial and historical simulations, coupled model development and evaluation},
doi = {https://doi.org/10.1029/2019MS001916},
url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2019MS001916},
eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2019MS001916},
note = {e2019MS001916 2019MS001916},
abstract = {Abstract An overview of the Community Earth System Model Version 2 (CESM2) is provided, including a discussion of the challenges encountered during its development and how they were addressed. In addition, an evaluation of a pair of CESM2 long preindustrial control and historical ensemble simulations is presented. These simulations were performed using the nominal 1° horizontal resolution configuration of the coupled model with both the “low-top” (40 km, with limited chemistry) and “high-top” (130 km, with comprehensive chemistry) versions of the atmospheric component. CESM2 contains many substantial science and infrastructure improvements and new capabilities since its previous major release, CESM1, resulting in improved historical simulations in comparison to CESM1 and available observations. These include major reductions in low-latitude precipitation and shortwave cloud forcing biases; better representation of the Madden-Julian Oscillation; better El Niño-Southern Oscillation-related teleconnections; and a global land carbon accumulation trend that agrees well with observationally based estimates. Most tropospheric and surface features of the low- and high-top simulations are very similar to each other, so these improvements are present in both configurations. CESM2 has an equilibrium climate sensitivity of 5.1–5.3 °C, larger than in CESM1, primarily due to a combination of relatively small changes to cloud microphysics and boundary layer parameters. In contrast, CESM2's transient climate response of 1.9–2.0 °C is comparable to that of CESM1. The model outputs from these and many other simulations are available to the research community, and they represent CESM2's contributions to the Coupled Model Intercomparison Project Phase 6.},
year = {2020}
}

@ARTICLE{cc_extremeweather,
  title    = "Extreme Weather and Climate Change: Population Health and Health
              System Implications",
  author   = "Ebi, Kristie L and Vanos, Jennifer and Baldwin, Jane W and Bell,
              Jesse E and Hondula, David M and Errett, Nicole A and Hayes,
              Katie and Reid, Colleen E and Saha, Shubhayu and Spector, June
              and Berry, Peter",
  abstract = "Extreme weather and climate events, such as heat waves, cyclones,
              and floods, are an expression of climate variability. These
              events and events influenced by climate change, such as
              wildfires, continue to cause significant human morbidity and
              mortality and adversely affect mental health and well-being.
              Although adverse health impacts from extreme events declined over
              the past few decades, climate change and more people moving into
              harm's way could alter this trend. Long-term changes to Earth's
              energy balance are increasing the frequency and intensity of many
              extreme events and the probability of compound events, with
              trends projected to accelerate under certain greenhouse gas
              emissions scenarios. While most of these events cannot be
              completely avoided, many of the health risks could be prevented
              through building climate-resilient health systems with improved
              risk reduction, preparation, response, and recovery. Conducting
              vulnerability and adaptation assessments and developing health
              system adaptation plans can identify priority actions to
              effectively reduce risks, such as disaster risk management and
              more resilient infrastructure. The risks are urgent, so action is
              needed now.",
  journal  = "Annu Rev Public Health",
  volume   =  42,
  pages    = "293--315",
  month    =  jan,
  year     =  2021,
  address  = "United States",
  keywords = "climate change; climate variability; extreme events; health
              systems; population health",
  language = "en"
}

@article{gcm_error_growth,
author = {Sheshadri, Aditi and Borrus, Marshall and Yoder, Mark and Robinson, Thomas},
title = {Midlatitude Error Growth in Atmospheric GCMs: The Role of Eddy Growth Rate},
journal = {Geophysical Research Letters},
volume = {48},
number = {23},
pages = {e2021GL096126},
doi = {https://doi.org/10.1029/2021GL096126},
url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2021GL096126},
eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2021GL096126},
note = {e2021GL096126 2021GL096126},
abstract = {Abstract Several studies have established that atmospheric flows have a finite range of predictability, which may be reasonably considered a consequence of the underlying dynamics. In the midlatitudes, error growth is predominantly associated with baroclinic disturbances. We consider midlatitude error growth in two models: an idealized dry dynamical core and a comprehensive atmospheric general circulation model (GCM). By systematically varying equator to pole temperature gradients in the dynamical core, we show that with increasing Eady growth rates, the time elapsed before errors saturate decreases, shortening the window in which weather predictions may be useful. We also consider the limits of midlatitude predictability in the comprehensive moist GCM in a range of climates. Our results show that the times to error saturation are shorter in warmer climates than colder climates, suggesting that warmer climates are inherently less predictable.},
year = {2021}
}


@article {ddw_risk,
      author = "Zied Ben Bouallègue and Mariana C. A. Clare and Linus Magnusson and Estibaliz Gascón and Michael Maier-Gerber and Martin Janoušek and Mark Rodwell and Florian Pinault and Jesper S. Dramsch and Simon T. K. Lang and Baudouin Raoult and Florence Rabier and Matthieu Chevallier and Irina Sandu and Peter Dueben and Matthew Chantry and Florian Pappenberger",
      title = "The Rise of Data-Driven Weather Forecasting: A First Statistical Assessment of Machine Learning–Based Weather Forecasts in an Operational-Like Context",
      journal = "Bulletin of the American Meteorological Society",
      year = "2024",
      publisher = "American Meteorological Society",
      address = "Boston MA, USA",
      volume = "105",
      number = "6",
      doi = "10.1175/BAMS-D-23-0162.1",
      pages=      "E864 - E883",
      url = "https://journals.ametsoc.org/view/journals/bams/105/6/BAMS-D-23-0162.1.xml"
}

@inproceedings{
li2023geometryinformed,
title={Geometry-Informed Neural Operator for Large-Scale 3D {PDE}s},
author={Zongyi Li and Nikola Borislavov Kovachki and Chris Choy and Boyi Li and Jean Kossaifi and Shourya Prakash Otta and Mohammad Amin Nabian and Maximilian Stadler and Christian Hundt and Kamyar Azizzadenesheli and Anima Anandkumar},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=86dXbqT5Ua}
}

@article{SHUKLAdnoairfoil,
title = {Deep neural operators as accurate surrogates for shape optimization},
journal = {Engineering Applications of Artificial Intelligence},
volume = {129},
pages = {107615},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.107615},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623017992},
author = {Khemraj Shukla and Vivek Oommen and Ahmad Peyvan and Michael Penwarden and Nicholas Plewacki and Luis Bravo and Anindya Ghoshal and Robert M. Kirby and George Em Karniadakis},
keywords = {Neural operators, DeepONet, Airfoil shape optimization, Navier–Stokes equations, Surrogate models},
abstract = {Deep neural operators, such as DeepONet, have changed the paradigm in high-dimensional nonlinear regression, paving the way for significant generalization and speed-up in computational engineering applications. Here, we investigate the use of DeepONet to infer flow fields around unseen airfoils with the aim of shape constrained optimization, an important design problem in aerodynamics that typically taxes computational resources heavily. We present results that display little to no degradation in prediction accuracy while reducing the online optimization cost by orders of magnitude. We consider NACA airfoils as a test case for our proposed approach, as the four-digit parameterization can easily define their shape. We successfully optimize the constrained NACA four-digit problem with respect to maximizing the lift-to-drag ratio and validate all results by comparing them to a high-order CFD solver. We find that DeepONets have a low generalization error, making them ideal for generating solutions of unseen shapes. Specifically, pressure, density, and velocity fields are accurately inferred at a fraction of a second, hence enabling the use of general objective functions beyond the maximization of the lift-to-drag ratio considered in the current work. Finally, we validate the ability of DeepONet to handle a complex 3D waverider geometry at hypersonic flight by inferring shear stress and heat flux distributions on its surface at unseen angles of attack. The main contribution of this paper is a modular integrated design framework that uses an over-parametrized neural operator as a surrogate model with good generalizability coupled seamlessly with multiple optimization solvers in a plug-and-play mode.}
}

@article{Yin_2023,
   title={Solving multiphysics-based inverse problems with learned surrogates and constraints},
   volume={10},
   ISSN={2213-7467},
   url={http://dx.doi.org/10.1186/s40323-023-00252-0},
   DOI={10.1186/s40323-023-00252-0},
   number={1},
   journal={Advanced Modeling and Simulation in Engineering Sciences},
   publisher={Springer Science and Business Media LLC},
   author={Yin, Ziyi and Orozco, Rafael and Louboutin, Mathias and Herrmann, Felix J.},
   year={2023},
   month=oct }


@article{LEREDE2023101144,
title = {Analysis of the possible contribution of different nuclear fusion technologies to the global energy transition},
journal = {Energy Strategy Reviews},
volume = {49},
pages = {101144},
year = {2023},
issn = {2211-467X},
doi = {https://doi.org/10.1016/j.esr.2023.101144},
url = {https://www.sciencedirect.com/science/article/pii/S2211467X23000949},
author = {D. Lerede and M. Nicoli and L. Savoldi and A. Trotta},
keywords = {Nuclear fusion, Energy system optimization, Energy scenarios, Electricity mix},
abstract = {Despite the huge uncertainties related to the possibility of a quick development of nuclear fusion technologies - being disputed that it may come too late to effectively contribute to emission mitigation - research is focusing on a wide set of options for fusion reactors. This paper presents a global scenario analysis using the energy system optimization model EUROfusion TIMES to analyze the possible future role of fusion according to three different technologies and using capacity curves based on historical trends for the electricity sector. The analyzed fusion options are based on ARC, EU-DEMO and Asian-DEMO reactor concepts, characterized in terms of techno-economic features according to publicly available literature and considering a set of educated growth rate for their penetration. Results concerning installed capacity trends and contribution to the electricity mix are presented up to 2100 in three socio-economic storylines and for different scenarios considering either the availability of competing technologies or delays in the development of fusion plants. Despite not contributing at all to the energy transition in Europe and the US, fusion may gain share in contexts characterized by highly growing electricity demand, contributing to satisfy stringent environmental constraints together with other low-carbon technologies in the second half of the century.}
}

@Article{Kates-Harbeck2019,
author={Kates-Harbeck, Julian
and Svyatkovskiy, Alexey
and Tang, William},
title={Predicting disruptive instabilities in controlled fusion plasmas through deep learning},
journal={Nature},
year={2019},
month={Apr},
day={01},
volume={568},
number={7753},
pages={526-531},
abstract={Nuclear fusion power delivered by magnetic-confinement tokamak reactors holds the promise of sustainable and clean energy1. The avoidance of large-scale plasma instabilities called disruptions within these reactors2,3 is one of the most pressing challenges4,5, because disruptions can halt power production and damage key components. Disruptions are particularly harmful for large burning-plasma systems such as the multibillion-dollar International Thermonuclear Experimental Reactor (ITER) project6 currently under construction, which aims to be the first reactor that produces more power from fusion than is injected to heat the plasma. Here we present a method based on deep learning for forecasting disruptions. Our method extends considerably the capabilities of previous strategies such as first-principles-based5 and classical machine-learning7--11 approaches. In particular, it delivers reliable predictions for machines other than the one on which it was trained---a crucial requirement for future large reactors that cannot afford training disruptions. Our approach takes advantage of high-dimensional training data to boost predictive performance while also engaging supercomputing resources at the largest scale to improve accuracy and speed. Trained on experimental data from the largest tokamaks in the United States (DIII-D12) and the world (Joint European Torus, JET13), our method can also be applied to specific tasks such as prediction with long warning times: this opens up the possibility of moving from passive disruption prediction to active reactor control and optimization. These initial results illustrate the potential for deep learning to accelerate progress in fusion-energy science and, more generally, in the understanding and prediction of complex physical systems.},
issn={1476-4687},
doi={10.1038/s41586-019-1116-4},
url={https://doi.org/10.1038/s41586-019-1116-4}
}

@article{PFC,
    author = {Linke, Jochen and Du, Juan and Loewenhoff, Thorsten and Pintsuk, Gerald and Spilker, Benjamin and Steudel, Isabel and Wirtz, Marius},
    title = "{Challenges for plasma-facing components in nuclear fusion}",
    journal = {Matter and Radiation at Extremes},
    volume = {4},
    number = {5},
    pages = {056201},
    year = {2019},
    month = {08},
    abstract = "{ The interaction processes between the burning plasma and the first wall in a fusion reactor are diverse: the first wall will be exposed to extreme thermal loads of up to several tens of megawatts per square meter during quasistationary operation, combined with repeated intense thermal shocks (with energy densities of up to several megajoules per square meter and pulse durations on a millisecond time scale). In addition to these thermal loads, the wall will be subjected to bombardment by plasma ions and neutral particles (D, T, and He) and by energetic neutrons with energies up to 14 MeV. Hopefully, ITER will not only demonstrate that thermonuclear fusion of deuterium and tritium is feasible in magnetic confinement regimes; it will also act as a first test device for plasma-facing materials (PFMs) and plasma-facing components (PFCs) under realistic synergistic loading scenarios that cover all the above-mentioned load types. In the absence of an integrated test device, material tests are being performed primarily in specialized facilities that concentrate only on the most essential material properties. New multipurpose test facilities are now available that can also focus on more complex loading scenarios and thus help to minimize the risk of an unexpected material or component failure. Thermonuclear fusion—both with magnetic and with inertial confinement—is making great progress, and the goal of scientific break-even will be reached soon. However, to achieve that end, significant technical problems, particularly in the field of high-temperature and radiation-resistant materials, must be solved. With ITER, the first nuclear reactor that burns a deuterium–tritium plasma with a fusion power gain Q ≥ 10 will start operation in the next decade. To guarantee safe operation of this rather sophisticated fusion device, new PFMs and PFCs that are qualified to withstand the harsh environments in such a tokamak reactor have been developed and are now entering the manufacturing stage. }",
    issn = {2468-2047},
    doi = {10.1063/1.5090100},
    url = {https://doi.org/10.1063/1.5090100},
    eprint = {https://pubs.aip.org/aip/mre/article-pdf/doi/10.1063/1.5090100/13886520/056201\_1\_online.pdf},
}



@Article{Degrave2022,
author={Degrave, Jonas
and Felici, Federico
and Buchli, Jonas
and Neunert, Michael
and Tracey, Brendan
and Carpanese, Francesco
and Ewalds, Timo
and Hafner, Roland
and Abdolmaleki, Abbas
and de las Casas, Diego
and Donner, Craig
and Fritz, Leslie
and Galperti, Cristian
and Huber, Andrea
and Keeling, James
and Tsimpoukelli, Maria
and Kay, Jackie
and Merle, Antoine
and Moret, Jean-Marc
and Noury, Seb
and Pesamosca, Federico
and Pfau, David
and Sauter, Olivier
and Sommariva, Cristian
and Coda, Stefano
and Duval, Basil
and Fasoli, Ambrogio
and Kohli, Pushmeet
and Kavukcuoglu, Koray
and Hassabis, Demis
and Riedmiller, Martin},
title={Magnetic control of tokamak plasmas through deep reinforcement learning},
journal={Nature},
year={2022},
month={Feb},
day={01},
volume={602},
number={7897},
pages={414-419},
abstract={Nuclear fusion using magnetic confinement, in particular in the tokamak configuration, is a promising path towards sustainable energy. A core challenge is to shape and maintain a high-temperature plasma within the tokamak vessel. This requires high-dimensional, high-frequency, closed-loop control using magnetic actuator coils, further complicated by the diverse requirements across a wide range of plasma configurations. In this work, we introduce a previously undescribed architecture for tokamak magnetic controller design that autonomously learns to command the full set of control coils. This architecture meets control objectives specified at a high level, at the same time satisfying physical and operational constraints. This approach has unprecedented flexibility and generality in problem specification and yields a notable reduction in design effort to produce new plasma configurations. We successfully produce and control a diverse set of plasma configurations on the Tokamak {\`a} Configuration Variable1,2, including elongated, conventional shapes, as well as advanced configurations, such as negative triangularity and `snowflake' configurations. Our approach achieves accurate tracking of the location, current and shape for these configurations. We also demonstrate sustained `droplets' on TCV, in which two separate plasmas are maintained simultaneously within the vessel. This represents a notable advance for tokamak feedback control, showing the potential of reinforcement learning to accelerate research in the fusion domain, and is one of the most challenging real-world systems to which reinforcement learning has been applied.},
issn={1476-4687},
doi={10.1038/s41586-021-04301-9},
url={https://doi.org/10.1038/s41586-021-04301-9}
}

@misc{carey2024dataefficiencylongterm,
      title={Data efficiency and long term prediction capabilities for neural operator surrogate models of core and edge plasma codes}, 
      author={N. Carey and L. Zanisi and S. Pamela and V. Gopakumar and J. Omotani and J. Buchanan and J. Brandstetter},
      year={2024},
      eprint={2402.08561},
      archivePrefix={arXiv},
      primaryClass={physics.plasm-ph},
      url={https://arxiv.org/abs/2402.08561}, 
}

@misc{pamela2024neuralpararealdynamicallytrainingneural,
      title={Neural-Parareal: Dynamically Training Neural Operators as Coarse Solvers for Time-Parallelisation of Fusion MHD Simulations}, 
      author={S. J. P. Pamela and N. Carey and J. Brandstetter and R. Akers and L. Zanisi and J. Buchanan and V. Gopakumar and M. Hoelzl and G. Huijsmans and K. Pentland and T. James and G. Antonucci and the JOREK Team},
      year={2024},
      eprint={2405.01355},
      archivePrefix={arXiv},
      primaryClass={physics.plasm-ph},
      url={https://arxiv.org/abs/2405.01355}, 
}



@misc{dehoop2022costaccuracytradeoffoperatorlearning,
      title={The Cost-Accuracy Trade-Off In Operator Learning With Neural Networks}, 
      author={Maarten V. de Hoop and Daniel Zhengyu Huang and Elizabeth Qian and Andrew M. Stuart},
      year={2022},
      eprint={2203.13181},
      archivePrefix={arXiv},
      primaryClass={math.NA},
      url={https://arxiv.org/abs/2203.13181}, 
}


@InProceedings{Kato_ncf_review_2024,
  title = 	 {A Review of Nonconformity Measures for Conformal
 Prediction in Regression},
  author =       {Kato, Yuko and Tax, David M.J. and Loog, Marco},
  booktitle = 	 {Proceedings of the Twelfth Symposium on Conformal
 and Probabilistic Prediction with Applications},
  pages = 	 {369--383},
  year = 	 {2023},
  editor = 	 {Papadopoulos, Harris and Nguyen, Khuong An and Boström, Henrik and Carlsson, Lars},
  volume = 	 {204},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--15 Sep},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v204/kato23a/kato23a.pdf},
  url = 	 {https://proceedings.mlr.press/v204/kato23a.html},
  abstract = 	 {Conformal prediction provides distribution-free
 uncertainty quantification under minimal
 assumptions. An important ingredient in conformal
 prediction is the so-called nonconformity measure,
 which quantifies how the test sample differs from
 the rest of the data. In this paper, existing
 nonconformity measures from the current literature
 are collected and their underlying ideas are
 analyzed. Furthermore, the influence of different
 factors on the performance of conformal prediction
 are pointed out by focusing on the relation between
 the influencing factors and the choice of
 nonconformity measures. Lastly, we provide
 suggestions for future work with regard to currently
 existing knowledge gaps and development of new
 nonconformity measures.}
}







@article{Youcef_GMRES_1986,
author = {Saad, Youcef and Schultz, Martin H.},
title = {GMRES: A Generalized Minimal Residual Algorithm for Solving Nonsymmetric Linear Systems},
journal = {SIAM Journal on Scientific and Statistical Computing},
volume = {7},
number = {3},
pages = {856-869},
year = {1986},
doi = {10.1137/0907058},

URL = { 
    
        https://doi.org/10.1137/0907058
    
    

},
eprint = { 
    
        https://doi.org/10.1137/0907058
    
    

}
,
    abstract = { We present an iterative method for solving linear systems, which has the property of minimizing at every step the norm of the residual vector over a Krylov subspace. The algorithm is derived from the Arnoldi process for constructing an \$l\_2 \$-orthogonal basis of Krylov subspaces. It can be considered as a generalization of Paige and Saunders’ MINRES algorithm and is theoretically equivalent to the Generalized Conjugate Residual (GCR) method and to ORTHODIR. The new algorithm presents several advantages over GCR and ORTHODIR. }
}




@book{iserles2009first,
  title={A first course in the numerical analysis of differential equations},
  author={Iserles, Arieh},
  year={2009},
  publisher={Cambridge university press}
}

@book{Pinder2018,
author={Pinder, George F.},
title={Numerical methods for solving partial differential equations : a comprehensive introduction for scientists and engineers},
year={2018},
publisher={John Wiley and Sons, Inc. : Wiley},
address={Hoboken, NJ},
}

@article{TOLSMA1998475,
title = {On computational differentiation},
journal = {Computers and Chemical Engineering},
volume = {22},
number = {4},
pages = {475-490},
year = {1998},
issn = {0098-1354},
doi = {https://doi.org/10.1016/S0098-1354(97)00264-0},
url = {https://www.sciencedirect.com/science/article/pii/S0098135497002640},
author = {John E. Tolsma and Paul I. Barton},
abstract = {Numerical derivatives play an important role in many computations. In many applications, the cost associated with the evaluation of numerical derivatives may be significant. Dramatic improvements in the speed of such calculations can be obtained through careful consideration of how these derivatives are computed. This paper reviews several ways in which numerical derivatives can be evaluated: hand-coding, finite difference approximations, reverse polish notation evaluation, symbolic differentiation, and automatic differentiation. It is concluded that automatic differentiation has significant advantages over all other approaches. Several ways of improving the efficiency of obtaining derivatives in an interpretive, symbolic environment are discussed. Example problems are compared to illustrate these improvements.}
}

@misc{gopakumar2024uncertaintyquantificationsurrogatemodels,
      title={Uncertainty Quantification of Surrogate Models using Conformal Prediction}, 
      author={Vignesh Gopakumar and Ander Gray and Joel Oskarsson and Lorenzo Zanisi and Stanislas Pamela and Daniel Giles and Matt Kusner and Marc Peter Deisenroth},
      year={2024},
      eprint={2408.09881},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2408.09881}, 
}

@article{DiquigiovanniCP_MV2022,
title = {Conformal prediction bands for multivariate functional data},
journal = {Journal of Multivariate Analysis},
volume = {189},
pages = {104879},
year = {2022},
issn = {0047-259X},
doi = {https://doi.org/10.1016/j.jmva.2021.104879},
url = {https://www.sciencedirect.com/science/article/pii/S0047259X21001573},
author = {Jacopo Diquigiovanni and Matteo Fontana and Simone Vantini},
keywords = {Conformal Prediction, Distribution-free prediction set, Exact prediction set, Finite-sample prediction set, Functional data, Prediction band},
abstract = {Motivated by the pressing request of methods able to create prediction sets in a general regression framework for a multivariate functional response, we propose a set of conformal predictors that produce finite-sample either valid or exact multivariate simultaneous prediction bands under the mild assumption of exchangeable regression pairs. The fact that the prediction bands can be built around any regression estimator and that can be easily found in closed form yields a very widely usable method, which is fairly straightforward to implement. In addition, we first introduce and then describe a specific conformal predictor that guarantees an asymptotic result in terms of efficiency and inducing prediction bands able to modulate their width based on the local behavior and magnitude of the functional data. The method is investigated and analyzed through a simulation study and a real-world application in the field of urban mobility.}
}

@article{Casella_acceptrejectsampling_2004,
 ISSN = {07492170},
 URL = {http://www.jstor.org/stable/4356322},
 abstract = {This paper extends the Accept--Reject algorithm to allow the proposal distribution to change at each iteration. We first establish a necessary and sufficient condition for this generalized Accept--Reject algorithm to be valid, and then show how the resulting estimator can be improved by Rao-Blackwellization. An application of these results is to the perfect sampling technique of Fill (1998), which is a generalized Accept--Reject algorithm.},
 author = {George Casella and Christian P. Robert and Martin T. Wells},
 journal = {Lecture Notes-Monograph Series},
 pages = {342--347},
 publisher = {Institute of Mathematical Statistics},
 title = {Generalized Accept-Reject Sampling Schemes},
 urldate = {2024-09-11},
 volume = {45},
 year = {2004}
}

@misc{diquigiovanni2021importancebandfinitesampleexact,
      title={The Importance of Being a Band: Finite-Sample Exact Distribution-Free Prediction Sets for Functional Data}, 
      author={Jacopo Diquigiovanni and Matteo Fontana and Simone Vantini},
      year={2021},
      eprint={2102.06746},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/2102.06746}, 
}

@INPROCEEDINGS{Actor2020-ng,
  title           = "Identification of kernels in a convolutional neural
                     network: connections between the level set equation and
                     deep learning for image segmentation",
  booktitle       = "Medical Imaging 2020: Image Processing",
  author          = "Actor, Jonas and Fuentes, David T and Riviere, Beatrice",
  editor          = "Landman, Bennett A and I{\v s}gum, Ivana",
  publisher       = "SPIE",
  month           =  mar,
  year            =  2020,
  conference      = "Image Processing",
  location        = "Houston, United States"
}

@misc{chen2024usingailibrariesincompressible,
      title={Using AI libraries for Incompressible Computational Fluid Dynamics}, 
      author={Boyang Chen and Claire E. Heaney and Christopher C. Pain},
      year={2024},
      eprint={2402.17913},
      archivePrefix={arXiv},
      primaryClass={physics.flu-dyn},
      url={https://arxiv.org/abs/2402.17913}, 
}

@article{CHEN2024116974,
title = {Solving the discretised multiphase flow equations with interface capturing on structured grids using machine learning libraries},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {426},
pages = {116974},
year = {2024},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2024.116974},
url = {https://www.sciencedirect.com/science/article/pii/S0045782524002305},
author = {Boyang Chen and Claire E. Heaney and Jefferson L.M.A. Gomes and Omar K. Matar and Christopher C. Pain},
keywords = {Artificial Intelligence, Partial differential equations, Convolutional neural networks, U-Net, Graphics Processing Units, Finite Element Method},
abstract = {This paper solves the discretised multiphase flow equations using tools and methods from machine-learning libraries. The idea comes from the observation that convolutional layers can be used to express a discretisation as a neural network whose weights are determined by the numerical method, rather than by training, and hence, we refer to this approach as Neural Networks for PDEs (NN4PDEs). To solve the discretised multiphase flow equations, a multigrid solver is implemented through a convolutional neural network with a U-Net architecture. Immiscible two-phase flow is modelled by the 3D incompressible Navier–Stokes equations with surface tension and advection of a volume fraction field, which describes the interface between the fluids. A new compressive algebraic volume-of-fluids method is introduced, based on a residual formulation using Petrov–Galerkin for accuracy and designed with NN4PDEs in mind. High-order finite-element based schemes are chosen to model a collapsing water column and a rising bubble. Results compare well with experimental data and other numerical results from the literature, demonstrating that, for the first time, finite element discretisations of multiphase flows can be solved using an approach based on (untrained) convolutional neural networks. A benefit of expressing numerical discretisations as neural networks is that the code can run, without modification, on CPUs, GPUs or the latest accelerators designed especially to run AI codes.}
}

@article{Gilbert_Topelitz_1986,
author = {Strang, Gilbert},
title = {A Proposal for Toeplitz Matrix Calculations},
journal = {Studies in Applied Mathematics},
volume = {74},
number = {2},
pages = {171-176},
doi = {https://doi.org/10.1002/sapm1986742171},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sapm1986742171},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sapm1986742171},
abstract = {In contrast to the usual (and successful) direct methods for Toeplitz systems Ax = b, we propose an algorithm based on the conjugate gradient method. The preconditioner is a circulant, so that all matrices have constant diagonals and all matrix-vector multiplications use the Fast Fourier Transform. We also suggest a technique for the eigenvalue problem, where current methods are less satisfactory. If the first indications are supported by further experiment, this new approach may have useful applications—including nearly Toeplitz systems, and parallel computations.},
year = {1986}
}

@Article{Fiorentino1991,
author={Fiorentino, G.
and Serra, S.},
title={Multigrid methods for toeplitz matrices},
journal={CALCOLO},
year={1991},
month={Sep},
day={01},
volume={28},
number={3},
pages={283-305},
abstract={We introduce a class of Multigrid methods for solving banded, symmetric Toeplitz systems Ax=b. We use a, special choice of the projection operator whose coefficients simply depend on some spectral properties of A. This choice leads to an iterative Multigrid method with convergence rate smaller than 1 independent of the condition number K2(A) and of the dimension of the matrix. In the second part the B0 class is introduced: this class, of Toeplitz matrices contains the linear space generated by the matrices arising from the finite differences discretization of the differential operators, m∈N+. To sum up we present an adaptive algorithm which has a input the coefficients of A and return an iterative Multigrid method with convergence speed independent of the mesh spacing h and with an asymptotical cost of O(n).},
issn={1126-5434},
doi={10.1007/BF02575816},
url={https://doi.org/10.1007/BF02575816}
}

@misc{paszke2019pytorchimperativestylehighperformance,
      title={PyTorch: An Imperative Style, High-Performance Deep Learning Library}, 
      author={Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas Köpf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
      year={2019},
      eprint={1912.01703},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1912.01703}, 
}

@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@article{Crank_Nicolson_1947, title={A practical method for numerical evaluation of solutions of partial differential equations of the heat-conduction type}, volume={43}, DOI={10.1017/S0305004100023197}, number={1}, journal={Mathematical Proceedings of the Cambridge Philosophical Society}, author={Crank, J. and Nicolson, P.}, year={1947}, pages={50–67}}

@book{canuto2007spectral,
  title={Spectral Methods: Evolution to Complex Geometries and Applications to Fluid Dynamics},
  author={Canuto, C. and Hussaini, M.Y. and Quarteroni, A. and Zang, T.A.},
  isbn={9783540307280},
  lccn={2007924823},
  series={Scientific Computation},
  url={https://books.google.co.uk/books?id=7COgEw5_EBQC},
  year={2007},
  publisher={Springer Berlin Heidelberg}
}

@Inbook{Gruber1985,
author="Gruber, Ralf
and Rappaz, Jacques",
title="The Ideal MHD Model",
bookTitle="Finite Element Methods in Linear Ideal Magnetohydrodynamics",
year="1985",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="34--41",
abstract="The plasma state is often called the fourth state following the solid, liquid and gaseous states. If a gas is heated above, say, 10 000 K, the gas is ionized due to collisions between particles. Such a mixture of ions and electrons is called a plasma (Chen 1974). If the temperature is raised to temperatures necessary in a thermonuclear reactor (T ≧ 10 keV ≈ 108 K) almost all electrons are free and the plasma becomes a very good conductor. In order to reach such high temperatures, the ionized hot gas has to be kept away from material walls, and the plasma has to be confined. This is done by applying a strong magnetic field, which acts on the charged particles in such a way that it provides a counterpressure to the gas pressure. A plasma in a magnetic bottle behaves like a mixture of fluids which can be described by the fluid equations coupled to the Maxwell equations. If one neglects the relative motion between ions and electrons and considers the plasma as only one averaged fluid, one uses the magnetohydrodynamic (MHD) equations. In the special case of an infinitely good conducting gas (with resistivity = 0), one uses what are called the ideal MHD equations. It is exactly this most simple model that we will treat numerically in the following chapters. It describes surprisingly well the equilibrium state of a magnetically confined plasma, and the rapid unstable global motions which can destroy the confinement on a microsecond timescale.",
isbn="978-3-642-86708-8",
doi="10.1007/978-3-642-86708-8_3",
url="https://doi.org/10.1007/978-3-642-86708-8_3"
}

@article{Orszag_Tang_1979, title={Small-scale structure of two-dimensional magnetohydrodynamic turbulence}, volume={90}, DOI={10.1017/S002211207900210X}, number={1}, journal={Journal of Fluid Mechanics}, author={Orszag, Steven A. and Tang, Cha-Mei}, year={1979}, pages={129–143}} 

@Article{ALFVÉN1942,
author={Alfv{\'e}n, H.},
title={Existence of Electromagnetic-Hydrodynamic Waves},
journal={Nature},
year={1942},
month={Oct},
day={01},
volume={150},
number={3805},
pages={405-406},
abstract={IF a conducting liquid is placed in a constant magnetic field, every motion of the liquid gives rise to an E. M. F. which produces electric currents. Owing to the magnetic field, these currents give mechanical forces which change the state of motion of the liquid. Thus a kind of combined electromagnetic-hydro-dynamic wave is produced which, so far as I know, has as yet attracted no attention.},
issn={1476-4687},
doi={10.1038/150405d0},
url={https://doi.org/10.1038/150405d0}
}

@article{Mocz_MHD_2014,
    author = {Mocz, Philip and Vogelsberger, Mark and Hernquist, Lars},
    title = "{A constrained transport scheme for MHD on unstructured static and moving meshes}",
    journal = {Monthly Notices of the Royal Astronomical Society},
    volume = {442},
    number = {1},
    pages = {43-55},
    year = {2014},
    month = {06},
    abstract = "{Magnetic fields play an important role in many astrophysical systems and a detailed understanding of their impact on the gas dynamics requires robust numerical simulations. Here we present a new method to evolve the ideal magnetohydrodynamic (MHD) equations on unstructured static and moving meshes that preserves the magnetic field divergence-free constraint to machine precision. The method overcomes the major problems of using a cleaning scheme on the magnetic fields instead, which is non-conservative, not fully Galilean invariant, does not eliminate divergence errors completely, and may produce incorrect jumps across shocks. Our new method is a generalization of the constrained transport (CT) algorithm used to enforce the ∇ · B = 0 condition on fixed Cartesian grids. Preserving ∇ · B = 0 at the discretized level is necessary to maintain the orthogonality between the Lorentz force and B. The possibility of performing CT on a moving mesh provides several advantages over static mesh methods due to the quasi-Lagrangian nature of the former (i.e. the mesh generating points move with the flow), such as making the simulation automatically adaptive and significantly reducing advection errors. Our method preserves magnetic fields and fluid quantities in pure advection exactly.}",
    issn = {0035-8711},
    doi = {10.1093/mnras/stu865},
    url = {https://doi.org/10.1093/mnras/stu865},
    eprint = {https://academic.oup.com/mnras/article-pdf/442/1/43/4072384/stu865.pdf},
}

@incollection{eymard2000finite,
  author = {Eymard, Robert and Gallouët, Thierry and Herbin, Raphaèle},
  title = {Finite Volume Methods},
  booktitle = {Solution of Equation in Rn (Part 3), Techniques of Scientific Computing (Part 3)},
  editor = {Lions, J. L. and Ciarlet, Philippe},
  series = {Handbook of Numerical Analysis},
  volume = {7},
  pages = {713--1020},
  year = {2000},
  publisher = {Elsevier},
  isbn = {9780444503503},
  doi = {10.1016/S1570-8659(00)070058},
  hal_id = {hal-02100732v2}
}



@article{Raissi2019PINNs,
title = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
journal = {Journal of Computational Physics},
volume = {378},
pages = {686-707},
year = {2019},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2018.10.045},
url = {https://www.sciencedirect.com/science/article/pii/S0021999118307125},
author = {M. Raissi and P. Perdikaris and G.E. Karniadakis},
keywords = {Data-driven scientific computing, Machine learning, Predictive modeling, Runge–Kutta methods, Nonlinear dynamics},
abstract = {We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.}
}

@article{LiPino2024,
author = {Li, Zongyi and Zheng, Hongkai and Kovachki, Nikola and Jin, David and Chen, Haoxuan and Liu, Burigede and Azizzadenesheli, Kamyar and Anandkumar, Anima},
title = {Physics-Informed Neural Operator for Learning Partial Differential Equations},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
url = {https://doi.org/10.1145/3648506},
doi = {10.1145/3648506},
abstract = {In this article, we propose physics-informed neural operators (PINO) that combine training data and physics constraints to learn the solution operator of a given family of parametric Partial Differential Equations (PDE). PINO is the first hybrid approach incorporating data and PDE constraints at different resolutions to learn the operator. Specifically, in PINO, we combine coarse-resolution training data with PDE constraints imposed at a higher resolution. The resulting PINO model can accurately approximate the ground-truth solution operator for many popular PDE families and shows no degradation in accuracy even under zero-shot super-resolution, that is, being able to predict beyond the resolution of training data. PINO uses the Fourier neural operator (FNO) framework that is guaranteed to be a universal approximator for any continuous operator and discretization convergent in the limit of mesh refinement. By adding PDE constraints to FNO at a higher resolution, we obtain a high-fidelity reconstruction of the ground-truth operator. Moreover, PINO succeeds in settings where no training data is available and only PDE constraints are imposed, while previous approaches, such as the Physics-Informed Neural Network (PINN), fail due to optimization challenges, for example, in multi-scale dynamic systems such as Kolmogorov flows.PROBLEM STATEMENTMachine learning methods have recently shown promise in solving partial differential equations (PDEs) raised in science and engineering. They can be classified into two broad categories: approximating the solution function  and learning the solution operator. The Physics-Informed Neural Network (PINN) is an example of the former while the Fourier neural operator (FNO) is an example of the latter. Both these approaches have shortcomings. The optimization in PINN is challenging and prone to failure, especially on multi-scale dynamic systems. FNO does not suffer from this optimization issue since it carries out supervised learning on a given dataset, but obtaining such data may be too expensive or infeasible. In this paper, we consider a new learning paradigm, aiming to overcome the optimization challenge in PINN and relieve the data requirement in FNO.METHODSIn this paper, we propose physics-informed neural operators (PINO) that combine training data and physics constraints to learn the solution operator of a given family of parametric PDEs.In the operator-learning phase, PINO learns the solution operator over multiple instances of the parametric PDE family using training data and physics constraints. In the instance-wise fine-tuning phase, PINO optimizes the pre-trained operator ansatz for the querying instance of the PDE using the physics constraints only.Specifically, we combine coarse-resolution training data with PDE constraints imposed at a higher resolution. By adding PDE constraints to FNO at a higher resolution, we obtain a high-fidelity reconstruction of the ground-truth operator.RESULTSThe resulting PINO model can accurately approximate the ground-truth solution operator for many popular PDE families and shows no degradation in accuracy even under zero-shot super-resolution, i.e., being able to predict beyond the resolution of training data.Experiments show PINO outperforms previous ML methods on many popular PDE families while retaining the extraordinary speed-up of FNO compared to solvers. With the equation constraints, PINO requires few to no data to learn the Burgers, Darcy, and Navier-Stokes equation. In particular, PINO accurately solves long temporal transient flows and  Kolmogorov flows where other baseline methods fail to converge.SIGNIFICANCEPINO uses the neural operator framework that is guaranteed to be a universal approximator for any continuous operator and discretization convergent in the limit of mesh refinement. Moreover, PINO succeeds in settings where no training data is available and only PDE constraints are imposed. These advantages could lead to applications such as weather forecast, airfoil designs, and turbulence control.},
journal = {ACM / IMS J. Data Sci.},
month = {may},
articleno = {9},
numpages = {27},
keywords = {Neural operators, physics informed learning, partial differential equations}
}

@inproceedings{
du2024neural,
title={Neural Spectral Methods: Self-supervised learning in the spectral domain},
author={Yiheng Du and Nithin Chalapathi and Aditi S. Krishnapriyan},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=2DbVeuoa6a}
}


@inproceedings{
chalapathi2024scaling,
title={Scaling physics-informed hard constraints with mixture-of-experts},
author={Nithin Chalapathi and Yiheng Du and Aditi S. Krishnapriyan},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=u3dX2CEIZb}
}

@article{ZhuPCDLUQ2019,
title = {Physics-constrained deep learning for high-dimensional surrogate modeling and uncertainty quantification without labeled data},
journal = {Journal of Computational Physics},
volume = {394},
pages = {56-81},
year = {2019},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2019.05.024},
url = {https://www.sciencedirect.com/science/article/pii/S0021999119303559},
author = {Yinhao Zhu and Nicholas Zabaras and Phaedon-Stelios Koutsourelakis and Paris Perdikaris},
keywords = {Physics-constrained, Normalizing flow, Conditional generative model, Reverse KL divergence, Surrogate modeling, Uncertainty quantification},
abstract = {Surrogate modeling and uncertainty quantification tasks for PDE systems are most often considered as supervised learning problems where input and output data pairs are used for training. The construction of such emulators is by definition a small data problem which poses challenges to deep learning approaches that have been developed to operate in the big data regime. Even in cases where such models have been shown to have good predictive capability in high dimensions, they fail to address constraints in the data implied by the PDE model. This paper provides a methodology that incorporates the governing equations of the physical model in the loss/likelihood functions. The resulting physics-constrained, deep learning models are trained without any labeled data (e.g. employing only input data) and provide comparable predictive responses with data-driven models while obeying the constraints of the problem at hand. This work employs a convolutional encoder-decoder neural network approach as well as a conditional flow-based generative model for the solution of PDEs, surrogate model construction, and uncertainty quantification tasks. The methodology is posed as a minimization problem of the reverse Kullback-Leibler (KL) divergence between the model predictive density and the reference conditional density, where the later is defined as the Boltzmann-Gibbs distribution at a given inverse temperature with the underlying potential relating to the PDE system of interest. The generalization capability of these models to out-of-distribution input is considered. Quantification and interpretation of the predictive uncertainty is provided for a number of problems.}
}

@misc{musekamp2024activelearningneuralpde,
      title={Active Learning for Neural PDE Solvers}, 
      author={Daniel Musekamp and Marimuthu Kalimuthu and David Holzmüller and Makoto Takamoto and Mathias Niepert},
      year={2024},
      eprint={2408.01536},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.01536}, 
}


@Article{AndrieuMCMC2003,
author={Andrieu, Christophe
and de Freitas, Nando
and Doucet, Arnaud
and Jordan, Michael I.},
title={An Introduction to MCMC for Machine Learning},
journal={Machine Learning},
year={2003},
month={Jan},
day={01},
volume={50},
number={1},
pages={5-43},
abstract={This purpose of this introductory paper is threefold. First, it introduces the Monte Carlo method with emphasis on probabilistic machine learning. Second, it reviews the main building blocks of modern Markov chain Monte Carlo simulation, thereby providing and introduction to the remaining papers of this special issue. Lastly, it discusses new interesting research horizons.},
issn={1573-0565},
doi={10.1023/A:1020281327116},
url={https://doi.org/10.1023/A:1020281327116}
}

@inproceedings{
teng2023predictive,
title={Predictive Inference with Feature Conformal Prediction},
author={Jiaye Teng and Chuan Wen and Dinghuai Zhang and Yoshua Bengio and Yang Gao and Yang Yuan},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=0uRm1YmFTu}
}

@misc{eliasof2020diffgcngraphconvolutionalnetworks,
      title={DiffGCN: Graph Convolutional Networks via Differential Operators and Algebraic Multigrid Pooling}, 
      author={Moshe Eliasof and Eran Treister},
      year={2020},
      eprint={2006.04115},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2006.04115}, 
}

@article{box1976science,
  author = {Box, George E. P.},
  title = {Science and Statistics},
  journal = {Journal of the American Statistical Association},
  volume = {71},
  number = {356},
  pages = {791--799},
  year = {1976},
  publisher = {Taylor \& Francis},
  doi = {10.1080/01621459.1976.10480949}
}


@PHDTHESIS{Math_Guarantees_PIML,
	copyright = {In Copyright - Non-Commercial Use Permitted},
	year = {2024},
	type = {Doctoral Thesis},
	author = {De Ryck, Tim},
	size = {185 p.},
	abstract = {Physics-informed machine learning is a popular framework that allows the numerical simulation of both forward and inverse problems for partial differential equations without needing any (potentially expensive) training data. It consists of the optimization of a parametric model based on a PDE residual-based loss function, with the most famous example being physics-informed neural networks (PINNs). This thesis addresses the relatively paucity in mathematical guarantees concerning the performance of these physics-informed models by proposing a unified framework in which the numerical analysis of the various components of the incurred error can be effectively carried out. We develop rigorous results on approximation, generalization and training errors and investigate their behavior with respect to the dimension of the underlying domain and the type of the PDE, with the Navier-Stokes equations, high-dimensional linear Kolmogorov equations and inviscid scalar conservation laws serving as primary case studies. In order to provide bounds on the approximation error, we prove that neural networks can approximate Sobolev regular functions arbitrarily well in higher-order Sobolev norms and that generic approximation results for neural networks, PINNs, and (physics-informed) operator learning can be obtained from each other after verifying a few assumptions. We demonstrate that approximation results can be obtained even when PDE solutions are discontinuous and the physics-informed loss function is based on a weak PDE residual, such as for weak PINNs for scalar conservation laws. Next, we prove for a number of PDEs that a small physics-informed loss indeed implies a small L 2 -error and investigate whether such stability holds as well for extended PINNs (XPINNs) and conservative PINNs (cPINNs). We follow up by providing upper bounds on the generalization gap, leading to guidelines on the necessary size of the training set for the model to generalize well to the whole (unseen) domain. Finally, we investigate the behavior of gradient descent algorithms in physics-informed machine learning and find that the difficulty in training these models is closely related to the conditioning of a differential operator associated to the Hermitian square of the differential operator of the underlying PDE, and to the chosen model. If this operator is ill-conditioned, it results in slow or infeasible training, which suggest that preconditioning this operator is crucial. We employ both rigorous mathematical analysis and empirical evaluations to investigate various strategies, explaining how they better condition this critical operator, and consequently improve training.},
	language = {en},
	address = {Zurich},
	publisher = {ETH Zurich},
	DOI = {10.3929/ethz-b-000674112},
	title = {Mathematical guarantees for physics-informed machine learning},
	school = {ETH Zurich}
}

@Book{Reddy2006FEM,
author={Reddy, J. N.},
title={Introduction to the Finite Element Method, Third Edition},
year={2006},
edition={3rd edition.},
publisher={McGraw-Hill Education},
address={New York},
abstract={J. N. Reddy's An Introduction to the Finite Element Method, Third Edition, is an update of one of the most popular FEM textbooks available. The book retains its strong conceptual approach, clearly examining the mathematical underpinnings of FEM and providing a general approach to engineering application areas. Known for its detailed, carefully selected example problems and extensive selection of homework problems, this book comprehensively covers a wide range of engineering areas, making it appropriate for all engineering majors, and underscores the wide range of use FEM has in the professional world. A supplementary text Web site located at http://www.mhhe.com/reddy3e contains password-protected solutions to end-of-chapter problems, general textbook information, chapters on the FEM1D and FEM2D computer programs, and more!},
isbn={9780072466850},
url={https://www.accessengineeringlibrary.com/content/book/9780072466850},
language={en}
}

@article{carbonfootprint_CFD,
    author = {Horwitz, J. A. K.},
    title = "{Estimating the carbon footprint of computational fluid dynamics}",
    journal = {Physics of Fluids},
    volume = {36},
    number = {4},
    pages = {045109},
    year = {2024},
    month = {04},
    abstract = "{Computational resources have grown exponentially in the past few decades. These machines make possible research and design in fields as diverse as medicine, astronomy, and engineering. Despite ever-increasing computational capabilities, direct simulation of complex systems has remained challenging owing to the degrees of freedom involved. At the cusp of exascale computing, high-resolution simulation of practical problems with minimal model assumptions may soon experience a renaissance. However, growing reliance on modern computers comes at the cost of a growing carbon footprint. To illustrate this, we examine historic computations in fluid dynamics where larger computers have afforded the opportunity to simulate flows at increasingly relevant Reynolds numbers. Under a variety of flow configurations, the carbon footprint of such simulations is found to scale roughly with the fourth power of Reynolds number. This is primarily explained by the computation cost in core-hours, which is also described by similar scaling, though regional differences in renewable energy use also play a role. Using the established correlation, we examine a large database of simulations to develop estimates for the carbon footprint of computational fluid dynamics in a given year. Collectively, the analysis provides an additional benchmark for new computations where, in addition to balancing considerations of model fidelity, carbon footprint should also be considered.}",
    issn = {1070-6631},
    doi = {10.1063/5.0199350},
    url = {https://doi.org/10.1063/5.0199350},
    eprint = {https://pubs.aip.org/aip/pof/article-pdf/doi/10.1063/5.0199350/19864736/045109\_1\_5.0199350.pdf},
}



@misc{gopakumar2023fourierneuraloperatorplasma,
      title={Fourier Neural Operator for Plasma Modelling}, 
      author={Vignesh Gopakumar and Stanislas Pamela and Lorenzo Zanisi and Zongyi Li and Anima Anandkumar and MAST Team},
      year={2023},
      eprint={2302.06542},
      archivePrefix={arXiv},
      primaryClass={physics.plasm-ph},
      url={https://arxiv.org/abs/2302.06542}, 
}

@inproceedings{2019MaddoxSWAG,
 author = {Maddox, Wesley J and Izmailov, Pavel and Garipov, Timur and Vetrov, Dmitry P and Wilson, Andrew Gordon},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {A Simple Baseline for Bayesian Uncertainty in Deep Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/118921efba23fc329e6560b27861f0c2-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{
bartolucci2023representation,
title={Representation Equivalent Neural Operators: a Framework for Alias-free Operator Learning},
author={Francesca Bartolucci and Emmanuel de Bezenac and Bogdan Raonic and Roberto Molinaro and Siddhartha Mishra and Rima Alaifari},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=7LSEkvEGCM}
}

@article{
mccabe2023towards,
title={Towards Stability of Autoregressive Neural Operators},
author={Michael McCabe and Peter Harrington and Shashank Subramanian and Jed Brown},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=RFfUUtKYOG},
note={}
}

@misc{nunn2023shapingmagneticfieldcoils,
      title={Shaping of Magnetic Field Coils in Fusion Reactors using Bayesian Optimisation}, 
      author={Timothy Nunn and Vignesh Gopakumar and Sebastien Kahn},
      year={2023},
      eprint={2310.01455},
      archivePrefix={arXiv},
      primaryClass={physics.plasm-ph},
      url={https://arxiv.org/abs/2310.01455}, 
}


@Inbook{Somov2012,
author="Somov, Boris V.",
title="Plasma Equilibrium in Magnetic Field",
bookTitle="Plasma Astrophysics, Part I: Fundamentals and Practice",
year="2012",
publisher="Springer New York",
address="New York, NY",
pages="403--427",
abstract="The concept of equilibrium is fundamental to any discussion of the energy contained in an astrophysical object or phenomenon. The MHD non-equilibrium is often related to the onset of dynamic phenomena in astrophysical plasma, for example, in the solar corona. In this chapter, we derive the virial theorem and the famous Shafranov theorem as well as we consider some general properties of equilibrium configurations.",
isbn="978-1-4614-4283-7",
doi="10.1007/978-1-4614-4283-7_19",
url="https://doi.org/10.1007/978-1-4614-4283-7_19"
}

@Article{Amorisco2024,
author={Amorisco, N. C.
and Agnello, A.
and Holt, G.
and Mars, M.
and Buchanan, J.
and Pamela, S.},
title={FreeGSNKE: A Python-based dynamic free-boundary toroidal plasma equilibrium solver},
journal={Physics of Plasmas},
year={2024},
month={Apr},
day={30},
volume={31},
number={4},
pages={042517},
abstract={We present a Python-based numerical solver for the two-dimensional dynamic plasma equilibrium problem. We model the time evolution of toroidally symmetric free-boundary tokamak plasma equilibria in the presence of the non-linear magnetohydrodynamic coupling with both currents in the ``active'' poloidal field coils, with assigned applied voltages, and eddy currents in the tokamak passive structures. FreeGSNKE (FreeGS Newton--Krylov Evolutive) builds and expands on the framework provided by the Python package FreeGS (Free boundary Grad--Shafranov). FreeGS solves the static free-boundary Grad--Shafranov (GS) problem, discretized in space using finite differences, by means of Picard iterations. FreeGSNKE introduces: (i) a solver for the static free-boundary GS problem based on the Newton--Krylov (NK) method, with improved stability and convergence properties; (ii) a solver for the linearized dynamic plasma equilibrium problem; and (iii) a solver for the non-linear dynamic problem, based on the NK method. We propose a novel ``staggered'' solution strategy for the non-linear problem, in which we make use of a set of equivalent formulations of the non-linear dynamic problem we derive. The alternation of NK solution steps in the currents and in the plasma flux lends this strategy an increased resilience to co-linearity and stagnation problems, resulting in favorable convergence properties. FreeGSNKE can be used for any user-defined tokamak geometry and coil configuration. FreeGSNKE's flexibility and ease of use make it a suitably robust control-oriented simulator of plasma magnetic equilibria. FreeGSNKE is entirely written in Python and easily interfaced with Python libraries, which facilitates machine learning based approaches to plasma control.},
issn={1070-664X},
doi={10.1063/5.0188467},
url={https://doi.org/10.1063/5.0188467}
}

@article{Lao_1985,
doi = {10.1088/0029-5515/25/11/007},
url = {https://dx.doi.org/10.1088/0029-5515/25/11/007},
year = {1985},
month = {nov},
publisher = {},
volume = {25},
number = {11},
pages = {1611},
author = {Lao, L.L. and St. John, H. and Stambaugh, R.D. and Kellman, A.G. and Pfeiffer, W.},
title = {Reconstruction of current profile parameters and plasma shapes in tokamaks},
journal = {Nuclear Fusion},
abstract = {An efficient method is given to reconstruct the current profile parameters, the plasma shape, and a current profile consistent with the magnetohydrodynamic equilibrium constraint from external magnetic measurements, based on a Picard iteration approach which approximately conserves the measurements. Computational efforts are reduced by parametrizing the current profile linearly in terms of a number of physical parameters. Results of detailed comparative calculations and a sensitivity study are described. Illustrative calculations to reconstruct the current profiles and plasma shapes in ohmically and auxiliarily heated Doublet III plasmas are given which show many interesting features of the current profiles.}
}

@Article{Joung2023,
author={Joung, Semin
and Ghim, Y.-C.
and Kim, Jaewook
and Kwak, Sehyun
and Kwon, Daeho
and Sung, C.
and Kim, D.
and Kim, Hyun-Seok
and Bak, J. G.
and Yoon, S. W.},
title={GS-DeepNet: mastering tokamak plasma equilibria with deep neural networks and the Grad--Shafranov equation},
journal={Scientific Reports},
year={2023},
month={Sep},
day={22},
volume={13},
number={1},
pages={15799},
abstract={The force-balanced state of magnetically confined plasmas heated up to 100 million degrees Celsius must be sustained long enough to achieve a burning-plasma state, such as in the case of ITER, a fusion reactor that promises a net energy gain. This force balance between the Lorentz force and the pressure gradient force, known as a plasma equilibrium, can be theoretically portrayed together with Maxwell's equations as plasmas are collections of charged particles. Nevertheless, identifying the plasma equilibrium in real time is challenging owing to its free-boundary and ill-posed conditions, which conventionally involves iterative numerical approach with a certain degree of subjective human decisions such as including or excluding certain magnetic measurements to achieve numerical convergence on the solution as well as to avoid unphysical solutions. Here, we introduce GS-DeepNet, which learns plasma equilibria through solely unsupervised learning, without using traditional numerical algorithms. GS-DeepNet includes two neural networks and teaches itself. One neural network generates a possible candidate of an equilibrium following Maxwell's equations and is taught by the other network satisfying the force balance under the equilibrium. Measurements constrain both networks. Our GS-DeepNet achieves reliable equilibria with uncertainties in contrast with existing methods, leading to possible better control of fusion-grade plasmas.},
issn={2045-2322},
doi={10.1038/s41598-023-42991-5},
url={https://doi.org/10.1038/s41598-023-42991-5}
}

@Article{Jang2024,
author={Jang, Byoungchan
and Kaptanoglu, Alan A.
and Gaur, Rahul
and Pan, Shaowu
and Landreman, Matt
and Dorland, William},
title={Grad--Shafranov equilibria via data-free physics informed neural networks},
journal={Physics of Plasmas},
year={2024},
month={Mar},
day={25},
volume={31},
number={3},
pages={032510},
abstract={A large number of magnetohydrodynamic (MHD) equilibrium calculations are often required for uncertainty quantification, optimization, and real-time diagnostic information, making MHD equilibrium codes vital to the field of plasma physics. In this paper, we explore a method for solving the Grad--Shafranov equation by using physics-informed neural networks (PINNs). For PINNs, we optimize neural networks by directly minimizing the residual of the partial differential equation as a loss function. We show that PINNs can accurately and effectively solve the Grad--Shafranov equation with several different boundary conditions, making it more flexible than traditional solvers. This method is flexible as it does not require any mesh and basis choice, thereby streamlining the computational process. We also explore the parameter space by varying the size of the model, the learning rate, and boundary conditions to map various tradeoffs such as between reconstruction error and computational speed. Additionally, we introduce a parameterized PINN framework, expanding the input space to include variables such as pressure, aspect ratio, elongation, and triangularity in order to handle a broader range of plasma scenarios within a single network. Parameterized PINNs could be used in future work to solve inverse problems such as shape optimization.},
issn={1070-664X},
doi={10.1063/5.0188634},
url={https://doi.org/10.1063/5.0188634}
}

@inproceedings{
takamoto2023pdebench,
title={{PDEBENCH}: {AN} {EXTENSIVE} {BENCHMARK} {FOR} {SCI}- {ENTIFIC} {MACHINE} {LEARNING}},
author={Makoto Takamoto and Timothy Praditia and Raphael Leiteritz and Dan MacKinlay and Francesco Alesiani and Dirk Pfl{\"u}ger and Mathias Niepert},
booktitle={ICLR 2023 Workshop on Physics for Machine Learning},
year={2023},
url={https://openreview.net/forum?id=b8SwOxZQ2kj}
}

@article{taylor_truncation,
author = {MacKinnon, Robert J. and Johnson, Richard W.},
title = {Differential-equation-based representation of truncation errors for accurate numerical simulation},
journal = {International Journal for Numerical Methods in Fluids},
volume = {13},
number = {6},
pages = {739-757},
keywords = {High-order finite difference method, Convection diffusion, Upwind differencing, Artificial diffusion, Navier-Stokes},
doi = {https://doi.org/10.1002/fld.1650130606},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/fld.1650130606},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/fld.1650130606},
abstract = {Abstract High-order compact finite difference schemes for two-dimensional convection-diffusion-type differential equations with constant and variable convection coefficients are derived. The governing equations are employed to represent leading truncation terms, including cross-derivatives, making the overall O(h4) schemes conform to a 3 × 3 stencil. We show that the two-dimensional constant coefficient scheme collapses to the optimal scheme for the one-dimensional case wherein the finite difference equation yields nodally exact results. The two-dimensional schemes are tested against standard model problems, including a Navier-Stokes application. Results show that the two schemes are generally more accurate, on comparable grids, than O(h2) centred differencing and commonly used O(h) and O(h3) upwinding schemes.},
year = {1991}
}

@article{optimal_tracking_tokamak,
author = {Li, Shunjie and Jiang, H. and Ren, Zhigang and Xu, C.},
year = {2014},
month = {06},
pages = {1-8},
title = {Optimal Tracking for a Divergent-Type Parabolic PDE System in Current Profile Control},
volume = {2014},
journal = {Abstract and Applied Analysis},
doi = {10.1155/2014/940965}
}


@article{Hendrick2024PlasmaBurn,
author = {Meyer, Hendrik and STEP Team},
title = {Plasma burn—mind the gap},
journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
volume = {382},
number = {2280},
pages = {20230406},
year = {2024},
doi = {10.1098/rsta.2023.0406},

URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rsta.2023.0406},
eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.2023.0406}
,
    abstract = { The programme to design plasma scenarios for the Spherical Tokamak for Energy Production (STEP), a reactor concept aiming at net electricity production, seeks to exploit the inherent advantages of the spherical tokamak (ST) while making conservative assumptions about plasma performance. This approach is motivated by the large gap between present-day STs and future burning plasmas based on this concept. It is concluded that plasma exhaust in such a device is most likely to be manageable in a double null (DN) configuration, and that high core performance is favoured by positive triangularity (PT) plasmas with an elevated central safety factor. Based on a full technical and physics assessment of external heating and current drive (CD) systems, it was decided that the external CD is provided most effectively by microwaves. Operation with active resistive wall mode (RWM) stabilization as well as high elongation is needed for the most compact solution. The gap between existing devices and STEP is most pronounced in the area of core transport, owing to high normalized plasma pressure in the latter which changes qualitatively the nature of the turbulence controlling transport. Plugging this gap will require dedicated experiments, particularly on high-performance STs, and the development of reduced models that faithfully represent turbulent transport at high normalized pressure. Plasma scenarios in STEP will also need to be such that edge localized modes (ELMs) either do not occur or are small enough to be compatible with material lifetime limits. The high current needed for a power plant-relevant plasma leads to the unavoidable generation of high runaway electron beam current during a disruption, where novel mitigation techniques may be needed. This article is part of the theme issue ‘Delivering Fusion Energy – The Spherical Tokamak for Energy Production (STEP)’. }
}

@misc{gray2025guaranteedconfidencebandenclosurespde,
      title={Guaranteed confidence-band enclosures for PDE surrogates}, 
      author={Ander Gray and Vignesh Gopakumar and Sylvain Rousseau and Sébastien Destercke},
      year={2025},
      eprint={2501.18426},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2501.18426}, 
}
