% \vspace{-3pt}
\section{NVR: Design Philosophy}
% \vspace{-3pt}

\begin{figure*}[t]
\vspace{-18pt}
\hspace{-15pt}
\includegraphics[width=1.04\textwidth]{fig3.pdf}
\vspace{-25pt}
    \caption{NVR micro-architecture and components. Purple blocks represent NVR additions to the system. Red blocks indicate shared components between NVR and NPU, assisting speculative execution during NPU sparse unit idle periods. \circled{b} SD: Stride Detector; \circled{c} LBD: Loop Bound Detector; \circled{d} SCD: Sparse Chain Detector; \circled{e} VMIG: Vectorisation Micro-Instruction Generator; \circled{f} NSB: Non-blocking Speculative Buffer.} %decode sparsity data structure.}
\vspace{-15pt}
\label{fig:fig3}
\end{figure*}

With these challenges in mind, we propose an adaptation of runahead tailored for NPU architectures, addressing three key aspects:



% \noindent \textbf{\underline{Decoupled Philosophy}}:
% 非侵入式原则
\noindent \textbf{\underline{Decoupled and Non-Invasive Philosophy}}:
% Throughput is the key performance metric for NPUs. To maximize it, prefetch should avoid introducing extra control logic that could block NPU execution and achieve the highest possible data parallelism\cite{}.
% For this consideration, our design is decoupling from the pipeline in NPU for future memory accesses. This approach enables speculative execution. 
% Our design achieves non-invasive integration by monitoring CPU and NPU states and instructions. 
% This decoupled architecture eliminates additional compatibility overhead and control logic burden, enabling efficient system operation.
Throughput is the primary metric for NPUs. To maximise it, prefetching mechanisms should avoid introducing additional control logic that stalls execution, ensuring maximum data parallelism\cite{hameed2010understanding}. 
% Our design addresses this by decoupling from the NPU pipeline for future memory access prediction, enabling speculative execution. We achieve non-invasive integration through monitoring of CPU and NPU states and instructions. Operating in parallel with the NPU, our system extracts critical information from load instructions and performs fuzzy prediction, initiating memory requests earlier than the NPU's native execution. Throughout this process, all operations except data transfer remain read-only, ensuring zero interference with NPU execution.
Our design decouples from NPU computation logic, allowing simultaneous speculative execution to proactively generate memory requests.
The system maintains non-invasive integration by passive monitoring the state and extracting load instruction information through read-only operations. 
This approach allows early initiation of memory requests without interfering with NPU execution or custom instruction requirements.
% This decoupled architecture eliminates additional compatibility overhead and control logic burden, enabling efficient system operation.

% our design takes a proactive approach to memory access exploitation

\noindent \textbf{\underline{Coverage-Oriented Philosophy}}:
% While CPU prefetching focuses primarily on latency reduction through cache optimization, NPU prefetching addresses bandwidth limitations to maintain continuous data flow for high-throughput computations. 
In NPU vector operations, computation can proceed only when all data in the batch are ready. 
Our experiments shown in Fig.~\ref{fig:fig8} (a) reveal that the overall cache miss rate decreases significantly faster than per-batch cache miss rates. 
While optimising per-batch cache misses is more challenging, it's crucial for substantial performance improvement.
% \mionote{need show?}\zhenote{Show numbers? Or detailed example?} \mionote{add numbers} 
Consequently, our design prioritises complete batch retrieval, accepting some prefetch redundancy as a reasonable trade-off. 
On the other hand, combining this with fuzzy range loading has the added benefit of reducing the overhead of control logic, like branch prediction. 
% \mionote{need how we tradeoff?}
% \zhenote{No, this is philosophy, discuss the trade-offs in the implementation.}
% \Nnote{Should discussions related to implementation be kept to a minimum in the philosophy section?}



\noindent \textbf{\underline{Micro-Instruction-Level Vectorisation Philosophy}}:
% We exploit NPUs' SIMD architecture through vectorized memory access strategies. 
% \zhenote{This sentence is redundant.}
NPUs' SIMD architecture operates with vectorised instructions, which typically could be decomposed into multiple micro-instructions spanning several cycles.
This fine-grained approach enables precise handling of cache misses while detecting stronger memory access patterns in this granularity.
Leveraging NPUs' native support for vector load instructions, we can bundle memory addresses without additional execution units, improving both prediction accuracy and MLP. 
%This architectural alignment ensures efficient bandwidth utilization and maintains continuous data flow for vector computations.


% NPU instructions tend to be coarse-grained and can be disassembled into a composition of multiple micro-instructions and take multiple cycles.
% Our Runahead implementation operates at the microinstruction level for two key advantages: 
% (1) precise handling of cache misses that occur at different points within a high-level instruction;
% (2) enhanced prefetch prediction accuracy due to stronger memory access patterns at the microinstruction granularity. 






% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{fig4.png}
%     \caption{Caption}
%     \label{fig:fig4}
% \end{figure}

In summary, vector runahead on NPU has several unique features. We can summarise our runahead design into three Q\&A and introduce detailed micro-architecture in the next session: 



\noindent \textbf{\texttt{Q\&A1. When}} to enter runahead mode: Enter runahead when a load instruction in the NPU's ROB executes, prefetching for the next load instruction in the reservation station. NVR executes in parallel with the NPU instruction stream, extracting information through snooping to generate vectorised memory requests ahead of NPU execution. By computing approximate boundaries in advance, it enables early issuance of vector load instructions.


\noindent \textbf{\texttt{Q\&A2. Where}} to execute NVR: 
The NVR is integrated between the CPU and NPU, prefetching speculatively. 
To achieve decoupled and non-invasive operation, the prefetching logic is architecturally separated from the NPU pipeline.
% NVR serves as a separate tiny patch positioned between CPU and NPU, prefetching speculatively.

\noindent \textbf{\texttt{Q\&A3. How}} to prefetch: 
NVR leverages idle computational resources in NPU's sparse processing units to perform approximate dependency chain calculations in parallel. Subsequently, NVR generates native NPU vectorised load instructions for the NPU pipeline, prefetching data into the L1/L2 cache hierarchy. This approach efficiently exploits NPU's vector instructions and architectural characteristics for effective prefetching and analysis.

