% \vspace{-3pt}
\section{Conclusion}
% \vspace{-3pt}

% In this paper, we present NVR, an innovative runahead prefetching mechanism specifically engineered to address the distinctive challenges posed by sparse DNN workloads on NPU architectures.
% NVR employs speculative execution to directly mitigate cache misses, yielding substantial improvements in both performance and efficiency. 
% Our empirical evaluations demonstrate that NVR achieves over 97\% reduction in L2 cache miss latency and delivers up to 4x acceleration in sparse workloads compared NPUs without prefetching capabilities. Furthermore, the integration of NSB amplifies NVR's effectiveness in cache miss reduction. 
% Through vectorised pipelining prefetch operations, NVR significantly reduces off-chip memory accesses, resulting in a 5x enhancement in bandwidth utilisation.



% In this paper, we present NVR, a runahead prefetching mechanism addressing the challenges of sparse DNN workloads on NPU architectures. 
% NVR leverages speculative execution to mitigate the impact of cache misses, improving both performance and efficiency. 
% Our evaluations show that NVR reduces L2 cache miss latency by over 97\% and achieves up to 4x speedup in sparse workloads compared to baseline NPUs. 
% The integration of NSB further enhances NVR's cache miss reduction capability. 
% Through vectorised prefetch operations, NVR reduces off-chip memory accesses, achieving 5x improvement in bandwidth utilisation.

In this paper, we present NVR, a runahead prefetching mechanism addressing the challenges of sparse DNN workloads on NPUs. 
This work demonstrates specialised prefetching techniques for DNN applications on custom architectures beyond general-purpose processors.

\parlabel{Lessons we learnt.}
This work adopted a holistic, workload-driven approach to accelerate the DNNs' executions. 
By workload profiling and architectural analysis, key bottlenecks were identified, guiding targeted system re-architecture. 
Instead of simply adding more hardware resources, this work addressed fundamental inefficiencies, achieving performance gains equivalent to multiple times the benefits of hardware scaling, but with significantly reduced area and power costs. 
The constructed NVR provides key insights that precise, interdisciplinary optimisations rooted in a deep understanding of workload-architecture interactions. 
We believe that such analysis offers valuable insights into bridging architectural and machine learning research, fostering more effective and sustainable design strategies.

% \noindent \textbf{Lessons learned.}
% Different from all previous work, we tackle the sparsity challenge from a different perspective. Instead of regularising irregular memory access patterns, our approach embraces the inherent characteristics of sparse computations. 



% Uncertainty
% Hopfield Network to Boltzman Machine
% Occam's Razor: Entities should not be multiplied unnecessarily

% \mionote{============Comments Lists===============}\\
% \mionote{1} \zhenote{Why? I think the stall time is the key, explicitly tell the truth.}
% \zhenote{Vague. Explicitly say why they do not work.}

% \mionote{2} \zhenote{Since we are different from all previous work. We need to explain why this is a good angle to solve the problem.}

% \zhenote{Settings are also important.}


% \mionote{3} \zhenote{All good, except the last two sentences, we need to explain why.}

% \mionote{4} 
% \zhenote{Answers should have the explanations.}






% \mionote{============CheckLists===============}\\
% \mionote{1. American English to British English}

% \mionote{2. check rob}

% \mionote{3. compact para. end}

% \mionote{4. suoxie}

% \mionote{5. cites}

% \mionote{6. no -ing -ed -en}



% \mionote{============Comments Lists===============}\\
% \zhenote{==== Outlines ==== \\
% (1. BACKGROUND) 
% Emerging machine learning workloads (e.g., Deep Neural Networks (DNNs) and Large Language Models (LLMs)), often involve hundreds of billions of parameters.
% This massive scale imposes a significant barrier to its edge deployment due to the extremely heavy computational and memory overheads~\cite{}.
% Luckily, many of these parameters are highly redundant, offering an opportunity to leverage sparsity to alleviate these resource constraints.
% That is, by identifying and setting low-weight blocks (structured sparsity) or individual elements (unstructured sparsity) to zero, the computational and storage pressure can be significantly reduced.
% As evidenced in [find a ref], up to 90\% of parameters in [xxx] can be pruned without significantly affecting overall performance.\\
% [A Fig.1 is required here to show 1) how the matrix from full parameters is sparsed, 2) how the memory addresses change, and how it causes the cache misses. ]\\
% (2. CHALLENGES) \\
% Although the intuitive effectiveness of sparsity in shrinking MK workload size has not been widely deployed, until today, it has not been widely deployed.
% This is because today's GPUs or neural engines can not deal with the frequent cache misses caused by the sparse. 
% Specifically, .....\\
% (3. EXISTING WORK) [Software did something]; [Hardware did something].\\
% (4. Contributions) Here, we consider this in a different way.}


% \mionote{=======================================}
% \Guan{What is a micro-instruction? Does it refer to splitting a large matrix computation into smaller blocks, for example, to perform the task in smaller parts? If it's something like that, then it's fine. However, if it's an instruction loop, meaning a function that contains multiple instructions within a loop, then its underlying operations should also be instructions, not micro-instructions.} \mionote{I mean the first one. For the instruction loop, we wait for them to be unrolled. If we need to mention this?} \Guan{No, i just confirm it. We don't need to invest so much content.} \mionote{getget!}

% \zhenote{Is the VR series belonged to the traditional runahead?} \mionote{This only refers to the first runahead}
% \zhenote{Be careful with the terminology, in our universe, all other runaheads are conventional runahead.} \mionote{hao}
% \zhenote{Memory-level Parallelism is not pre-defined. Please read through the whole paper, and check all abbreviations.}

% \zhenote{Is the CPU prefetching not ``preload''? I guess you wish to say `` the NPU preload the data using all the idle bandwidth''?} \Guan{It seems that NPU preload refers to the NPU loading data from memory (such as scratchpad) into the PEs ahead of time to minimise the time spent on subsequent TLB misses. This is achieved using a custom instruction. The distinction from prefetching lies it is not speculative and does not fail. It might not be entirely accurate to say `` the NPU preload the data using all the idle bandwidth."}
