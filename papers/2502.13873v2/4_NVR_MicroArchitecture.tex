% \vspace{-5pt}
\section{NVR Micro-Architecture}
% \vspace{-3pt}


% \zhenote{Here, add some descriptions to introduce what we have done, especially why we use Gemini.}

We use Gemmini~\cite{gemmini} as our baseline to demonstrate NVR's applicability to general NPUs, as it embodies typical NPU architecture. %\cite{}.
Its coarse-grained design inherently struggles with cache miss handling, making it ideal for demonstrating NVR's design and effectiveness.

% \vspace{-3pt}
% \vspace{-5pt}
\subsection{Overall Architecture}
\vspace{-1.5pt}
% \vspace{-3pt}
% \mionote{====== I will back, start from here =========}



Fig.~\ref{fig:fig3} illustrates NVR's micro-architectural modifications to the baseline Gemmini configuration, which comprises an in-order core and DNN accelerator sharing a unified L2 cache.
The original Gemmini does not support sparse computation, so we have incorporated a basic implementation of the sparse function to demonstrate our prefetcher's design.
% sparse unit的功能简单介绍
As illustrated in Fig.~\ref{fig:fig3}~\circled{b}, the sparse unit is primarily designed to handle alignment, skipping, and tiling of sparse data, three processing techniques discussed in Section \ref{sc:sc1}.


To support NVR, we augment the baseline with the following structures (purple blocks):
A snooper unit monitoring the status of the CPU and the NPU (Fig.~\ref{fig:fig3}~\circled{a}).
A stride detector (SD) that tracks stream memory access patterns (Fig.~\ref{fig:fig3}~\circled{b}).
A loop boundary detector (LBD) that performs predictive analysis of iteration bounds for both unrolled and nested loop structures (Fig.~\ref{fig:fig3}~\circled{c}).
A sparse chain detector (SCD) identifies indirect memory access dependency chains and computes their corresponding memory addresses (Fig.~\ref{fig:fig3}~\circled{d}).
A vectorisation micro-instruction generator (VMIG) that bundles prefetch sequences with related access patterns into vectorised prefetch operations, optimising bandwidth utilisation (Fig.~\ref{fig:fig3}~\circled{e}).
An optional non-blocking speculative buffer (NSB), which serves as a small in-NPU cache equipped with miss status holding registers (MSHRs) (Fig.~\ref{fig:fig3}~\circled{f}).

%介绍下总体执行流程
% The NVR system starts runahead execution when a new load instruction is detected in the NPU's ROB. 
% The snoopers evaluate CPU and NPU states, and when the NPU's sparse unit is idle, NVR promptly issues runahead requests for speculative computations on sparse data.
% These snoopers employ nonintrusive monitoring of critical signals. The stride detector simultaneously analyzes address patterns to enhance load prediction accuracy. 
% In parallel, the SCD utilizes historical data analysis to recognize indirect prefetching patterns, effectively projecting future access addresses. 
% Complementing this, the Loop Bound Detector systematically examines nested loop structures to optimize prefetch requests while ensuring boundary integrity. 
% This comprehensive analysis is then relayed to the VIG, which synthesizes information from all components to produce optimized prefetch instructions. 
% On the other hand，NSB is an optional approach in conjunction with our prefetching, experimentally proven to be effective in cache miss reduction. The following sections explain the details of each structure.


% The NVR starts runahead execution upon detecting new load instructions within the NPU's ROB. 
% The snoopers evaluate CPU and NPU states and let the controller trigger runahead requests for speculative computations on sparse data once the NPU sparse unit is idle. 
% These snoopers implement non-intrusive monitoring of critical signals.
% The stride detector captures the stride pattern at index $i$ and predicts the next $A[i]$.
% In order to predict $B[sparse\_align(A[i])]$ indirect access address generation for $sparse\_align$ needs to be implemented through the SCD.
% LBD prevents prefetching overruns by predicting the structure of loops and providing a basis for packing and prefetching access requests.
% Finally, the VIG synthesizes the information from multiple NVR components to generate vectorized prefetching instructions.
% Additionally, the NSB is an optional complementary mechanism to our prefetching strategy, and experimental validation has demonstrated its efficacy in reducing cache misses. 
% The subsequent sections provide detailed architectural analyses of each structural component.

The steps of the NVR startup and execution process are numbered with red circles in Fig.~\ref{fig:fig3}.
The NVR initiates runahead execution upon detecting new load instructions in execution within the NPU's ROB. 
The NVR's controller monitors CPU and NPU states via snoopers and triggers runahead request for speculative computations during NPU sparse unit idle periods.
% Using snoopers monitoring CPU and NPU states, the controller triggers runahead requests for speculative computations once the NPU sparse unit turns idle. 
%while maintaining non-intrusive monitoring of critical signals. 
% During execution, the stride detector captures patterns at index $i$ to predict subsequent $A[i]$ values, working in conjunction with the SCD which handles indirect access address generation for $B[sparse\_align(A[i])]$ predictions. 
During execution, the stride detector captures patterns at index $i$ to predict subsequent $\texttt{W[i]}$ values. 
The SCD supports this by generating indirect access address generation for $\texttt{IA[sparse\_func(W[i])]}$ predictions.
To optimise prefetch operations, the LBD analyses loop structures to prevent overruns and enables request packing. 
The VMIG reconstructs decomposed micro-instructions by synthesising information from these components and generates new vectorised prefetching instructions inserted into the NPU pipeline for prefetching operations.
As a complementary approach, the NSB offers an optional mechanism to enhance the prefetching strategy, demonstrating a reduction in cache misses experimentally. 
The following sections detail these components' design.

% \vspace{-5pt}
\vspace{-3pt}
\subsection{Stride Detector (SD)}
% \vspace{-2pt}
\vspace{-1.5pt}

The stride detector is a fundamental pattern recognition unit within the NVR design, tasked with predicting the next batch of addresses for $\texttt{W[i]}$. Its operation relies on identifying consistent striding memory access patterns to facilitate efficient stream prefetching.
The stride detector employs a reference prediction mechanism to track the progression of addresses, similar to reference prediction tables in traditional stride prefetchers. It identifies memory patterns by keeping track of the previous address, stride size, and other control parameters. By leveraging these records, the stride detector predicts subsequent addresses accurately, especially when handling repetitive patterns in workloads. 
In neural network workloads, such as travelling $\texttt{W[i]}$, the typical structure results in relatively fewer branch mispredictions, allowing for high prediction accuracy. 
This is further enhanced by continuously monitoring address differences and maintaining confidence metrics for stride patterns. By predicting the next data accesses effectively, the stride detector not only initiates prefetching requests promptly but also ensures minimal computational overhead, contributing to reduced latency in memory accesses.
% The stride detector is the fundamental pattern recognition unit in NVR design and primarily used to predict the next batch of addresses for $\texttt{W[i]}$.
% It continuously predicts the data memory access pattern for step patterns by stream prefetching. 
% For $\texttt{W[i]}$, neural networks tend to have relatively few branch predictions in their workloads, and a high prediction accuracy can be achieved by recording the address differences and the confidence levels.




% \vspace{-5pt}
% \vspace{-1.5pt}
\subsection{Snoopers and Controller}
\vspace{-2pt}
The snoopers are non-invasive probes used to precisely extract the architectural states of both the CPU and NPU. We monitor three critical signal types:
(1) branch instructions from the CPU, which provide LBD with nested loop context information,
(2) custom load-related instructions in the NPU, which are used to determine the optimal timing for runahead mode activation, and
(3) NPU sparse unit registers, which supply the metadata essential for the NVR prediction mechanisms.
The read-only, non-invasive design of the snoopers maintains architectural integrity by preventing any modifications to NPU computational logic and avoiding interference with the interactions between the CPU and NPU.
Upon entering runahead mode, the controller sends speculative execution requests to the sparse unit. 
When runahead mode is triggered, the controller sends speculative execution requests to the sparse unit, enabling it to proceed with predictive computations. The system then monitors the unit availability, using the snooper infrastructure to retrieve the necessary sparse unit data as soon as it becomes available.


\begin{figure}[t]
\vspace{-5pt}
\hspace{-20pt}
\includegraphics[width=0.55\textwidth]{fig4.pdf}
\vspace{-25pt}
\caption{Vectorisation micro-instruction generation pipeline. Micro-instruction 1-1 represents the first micro-instruction of instruction 1. %(corresponding to a single entry in VMIG). 
Each micro-instruction loads an indeterminate number of data.}
\vspace{-15pt}
\label{fig:fig4}
\end{figure}


% \vspace{-5pt}
% \vspace{-3pt}
\subsection{Sparse Chain Detector (SCD)}
\vspace{-2pt}
% \vspace{-0.5pt}

The SCD identifies and predicts patterns in sparse computations through two critical components: historical information preservation and indirect prefetching pattern learning.
An Indirect Pattern Table (IPT) is maintained to record indirect prefetch patterns, including details such as the Last Prefetch Indirect (LPI) and the sparse structure's start address.
% SCD主要由两部分组成 历史信息的保留 和 间接预取模式的学习
In DNN workloads, computation index patterns typically exhibit locality characteristics. Due to the large volume of data processing, indirect index patterns remain relatively stable over time intervals, often appearing as shallow indirect chains, as formulated below: %\ref{eq:eq1}.
%\mionote{aha?} 
\vspace{-5pt}
\begin{small}
% \begin{equation}
$$
\ia_{\texttt{address}} = \ia_{\texttt{ss\_start}} + (\w_{\texttt{LPI}} << \texttt{stride}) 
\vspace{-5pt}
$$
% \label{eq:eq1}
% \end{equation}
\end{small}
\noindent
, where $\ia_{\texttt{address}}$ represents the predicted indirect access address, $\ia_{\texttt{ss\_start}}$ denotes the base address at the start element of the loop iteration, and $\w_{\texttt{LPI}}$ indicates the value of last prefetched $\w$. Here, $\texttt{stride}$ defines the offset between consecutive memory accesses. 
% For array stride patterns, $\texttt{stride}$ represents the fixed increment of $\w_{\texttt{LPI}}$, typically a multiple of the data format size. 
The prediction for the address of $\ia$ depends on the value of $\w_{\texttt{LPI}}$.

By recording the structural information of ${\texttt{W}}$ and ${\texttt{IA}}$ from previous operations, SCD can effectively track sparse operation chains and predict subsequent indirect prefetching addresses via stride learning. 
This predictive mechanism is particularly beneficial in scenarios with repetitive sparse patterns.
The mechanism requires sparse processing boundaries and current processing indices, such as CSR $\texttt{col\_indices}$ which locates the non-zero elements in columns and $\texttt{rowptr}$ for tracking row start positions from NPU operations, information that is readily available in most sparse data formats.
Unlike traditional prefetchers, which lack access to NPU register-level information and often depend on statistical methods like EWMA\cite{crowder1992ewma}, leading to inefficiencies, our approach uses the NPU's sparse unit to run ahead. This allows us to preserve and leverage historical information, significantly improving the prediction of sparse vector accesses while reducing redundant memory requests.

% SCD maintains historical information through a Sparse Structure Table (SST), where each parallel port has dedicated entry to facilitate concurrent tracking of sparse chains.
% By recording the structural information of ${\texttt{W}}$ and ${\texttt{IA}}$ from previous operations, SCD tracks sparse operation chains and projects subsequent indirect prefetching addresses through stride learning.
% The mechanism requires sparse processing boundaries (e.g., CSR $\texttt{col\_indices}$) and current processing indices (e.g., CSR $\texttt{rowptr}$) from NPU operations, information readily available in most sparse data formats. 
% Traditional prefetchers lack access to NPU register information and depend on historical boundaries or statistical methods like EWMA\cite{crowder1992ewma}, leading to inefficient sparse vector prediction. 
% Our approach overcomes these limitations by running the sparse unit ahead and preserving the historical information. % for IA




% \vspace{-5pt}
\subsection{Loop Bound Detector (LBD)}
\vspace{-2pt}

\begin{figure*}[t]
\vspace{-7pt}
\hspace{-10pt}
% \centering
\includegraphics[width=1.01\textwidth]{fig5.pdf}
\vspace{-5pt}
\caption{Normalised wall-clock time latency for each sparse workload. Within each group, each bar from left to right denotes execution in density, in order execution, OoO execution, IMP, DVR, and NVR, respectively. The lower segment indicates the NPU base execution time, whilst the upper segment represents the stall time caused by cache misses.}
\vspace{-15pt}
\label{fig:fig5}
\end{figure*}

DNN workloads inherently consist of multiple nested and unrolled loops, primarily involving matrix and vector operations, where higher-dimensional operations naturally require deeper loop nesting structures. To address the challenges of memory access patterns in these structures, the LBD employs systematic loop behaviour and boundary tracking to optimise prefetch sizing while preventing boundary-crossing invalid prefetches. 

As illustrated in the listing of Fig.~\ref{fig:fig2}, nested loops typically occur during the traversal of rows and columns of a matrix, while unrolled loops are often used in parts of matrix multiplication where multiple indirect chains are executed in parallel. 
As depicted in Fig.~\ref{fig:fig3}~(b), LBD maintains historical information through a Sparse Structure Table (SST), where each parallel port has a dedicated entry to facilitate concurrent tracking of sparse chains. LBD implements a dual-mode boundary prediction, handling both sparse and normal boundaries. The mechanism distinguishes loop hierarchies from inner to outer levels through entry IDs, with each entry maintaining comprehensive loop instruction information, including PC value, boundary values, and operational mode. 
For standard loop boundaries (illustrated in line 1, Fig.~\ref{fig:fig2}~(right)), LBD captures historical boundary information by monitoring register values of jump instructions, as shown with RISC-V B-type branch instructions in Fig.~\ref{fig:fig3}~(b). 
For variable boundaries in sparse computations (shown in line 2, Fig.~\ref{fig:fig2}~(right)), boundary information is dynamically acquired through snoopers from sparse unit registers.
Within this framework, upon detecting loop instructions, the LBD not only uses historical data to predict the boundary but also learns loop boundaries by examining input register values from comparison instructions. 
At the same loop level, instructions are consolidated into memory access requests, while boundary values act as crucial constraints to prevent excessive prefetching. 

% Furthermore, when an address discontinuity is observed in stride detection, the LBD reads the values of registers that will soon be used by comparison instructions. 
% The difference between the observed current values, using the previously identified loop increment, assists in predicting the loop bounds. This prediction assumes that these values were initialized at the start of the loop and have not been modified or spilled before being used in subsequent comparison operations. 
% This proactive prediction during the first iteration of a loop helps ensure more accurate prefetching without waiting for subsequent iterations.
% Within this framework, instructions at the same loop level are efficiently consolidated into several memory access requests, while boundary values serve as critical constraints to prevent excessive prefetching operations.




% \vspace{-5pt}
% \subsection{Vectorization Micro-Instruction Generator (VMIG)}
% \vspace{-3pt}
% % 写细些+加张小图？
% % \mionote{this paragraphs may need more discuss}
% \begin{figure}[t]
% \vspace{-20pt}
% \includegraphics[width=0.475\textwidth]{fig5.png}
% \vspace{-10pt}
% \caption{Vectorization Micro-Instruction Generator}
% \vspace{-5pt}
% \label{fig:fig5}
% \end{figure}


\begin{table}[t]
\vspace{15pt}
\vspace{-5pt}
\Huge
\caption{Hardware Overhead of NVR.} 
\vspace{-5pt}
\centering
\setlength{\arrayrulewidth}{0.75mm} % 设置边框线宽
\renewcommand{\arraystretch}{1.25} % 调整行高
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{|clll|}
\hline
\multicolumn{1}{|c|}{\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}SD\\ (N=16)\end{tabular}}}   & $48$ bit PC                 & $48N$ bit prev. addr       &    $8N$ bit stride        \\
\multicolumn{1}{|c|}{}                                                                       & $Nlog_2N$ bit entry ID      & $48N$ bit last prefetch addr & $2N$ bit stride conf  \\ \hhline{|~|-|-|-|}
\multicolumn{1}{|c|}{}                                                                       & \multicolumn{3}{c|}{\cellcolor[rgb]{ .906,  .902,  .902}$48 + 16 \times 110$ = 1808 bits}  \\ \hline
\multicolumn{1}{|c|}{\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}SCD\\ (N=$2\times16$)\end{tabular}}} & $48$ bit PC                 & $48N$ bit ss start   &  $N$ bit valid \\
\multicolumn{1}{|c|}{}                                                                       & $Nlog_2N$ bit entry ID      & $10N$ bit ss offset  & \\ 
\multicolumn{1}{|c|}{}                                                                       & $10N$ bit LPI               & $4N$ bit vector size & \\ \hhline{|~|-|-|-|}
\multicolumn{1}{|c|}{}                                                                       & \multicolumn{3}{c|}{\cellcolor[rgb]{ .906,  .902,  .902} $48+ 32 \times 77 = 2464$ bits}    \\ \hline
\multicolumn{1}{|c|}{\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}LBD\\ (N=16)\end{tabular}}}  & $48N$ bit PC                & $16N$ bit iteration counter       & $N$ sparse mode   \\
\multicolumn{1}{|c|}{}                                                                       & $Nlog_2N$ bit entry ID     & $16N$ bit increment    & $2N$ level conf  \\
\multicolumn{1}{|c|}{}                                                                       & $16N$ bit loop boundary   & $4N$ bit boundary conf & \\ \hhline{|~|-|-|-|}
\multicolumn{1}{|c|}{}                                                                       & \multicolumn{3}{c|}{\cellcolor[rgb]{ .906,  .902,  .902} $32 \times 1027 = 3424$ bits}    \\ \hline
\multicolumn{1}{|c|}{\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}VMIG\\ (N=$2\times16$)\end{tabular}}}& $48N$ bit PC                 & $64N$ bit VRF    & $64N$ bit PIE        \\
\multicolumn{1}{|c|}{}                                                                       & $Nlog_2N$ bit entry ID       & $4N+4$ bit IRU   & $256$ bit VIGU         \\ \hhline{|~|-|-|-|}
\multicolumn{1}{|c|}{}                                                                       & \multicolumn{3}{c|}{\cellcolor[rgb]{ .906,  .902,  .902}$260 + 16 \times 184 = 3204$ bits}    \\ \hline
\multicolumn{1}{|c|}{\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Snooper\end{tabular}}}& $48$ bit CPU PC  & $64$ bit CPU register       &       \\
\multicolumn{1}{|c|}{}                                                                & $48$ bit NPU PC  & $(48+10+10)N$ sparse structure   &      \\ \hhline{|~|-|-|-|}
\multicolumn{1}{|c|}{}                                                                & \multicolumn{3}{c|}{\cellcolor[rgb]{ .906,  .902,  .902}$160 + 16 \times 68 = 1248 $ bits}   \\ \hline
% \multicolumn{1}{|c|}{\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}NSB\\ (optional)\end{tabular}}}& 48 bit PC               & 48 bit PC      & 48 bit LP            \\
% \multicolumn{1}{|c|}{}                                                                       & 48 bit PC                 &                & 16 bit LIL                \\ \hhline{|~|-|-|-|}
% \multicolumn{1}{|c|}{}                                                                       & \multicolumn{3}{l|}{\cellcolor[rgb]{ .906,  .902,  .902}xxKiB}                 \\ \hline
\multicolumn{1}{|c|}{Total}                                                                  & \multicolumn{3}{c|}{9.72 KiB + 16 KiB (optional NSB)}                               \\ \hline
\end{tabular}
}
\vspace{-10pt}
\end{table}

% \vspace{-5pt}
\subsection{Vectorisation Micro-Instruction Generator (VMIG)}
\vspace{-2pt}

% Sparse data processing in NPUs compromises SIMD pipeline efficiency, as traditional vector instructions underutilize memory bandwidth due to skipped or zero-valued elements. Employing the original load instructions to prefetch data would result in significant bandwidth wastage due to these sparse data characteristics.
% The VMIG addresses these limitations by reconstructing the micro-instructions within the SIMD load instructions.

In NPU-based sparse data processing, the efficiency of the SIMD pipeline is often compromised because traditional vector instructions fail to fully utilise memory bandwidth when dealing with skipped or zero-valued elements. 
Strictly following NPU runtime loading instructions for prefetching would lead to significant bandwidth wastage. 
Instead, prefetch operations can bypass the rigid loading logic of the NPU, and the VMIG addresses this by restructuring the NPU's native load instructions at the micro-instruction level. 
This approach optimises sparse data patterns, leveraging existing vector load units without additional hardware support. 
% Sparse data processing in NPUs compromises SIMD pipeline efficiency, as traditional vector instructions underutilise memory bandwidth due to skipped or zero-valued elements. Direct adherence to NPU runtime loading instructions for prefetching would result in significant bandwidth wastage. However, prefetch operations need not strictly follow the NPU's rigid \mionote{maybe rigid too fancy?} \Nnote{structured ? may better?} loading logic. Therefore, VMIG restructures the NPU's native load instructions at the micro-instruction level, optimising for sparse data patterns while leveraging existing vector load units without requiring additional hardware support.

% VMIG addresses these limitations through a three-stage pipeline: Micro-instruction Restructuring, Parallel Inference, and Vector Instruction Generation. In the initial restructuring phase, VMIG leverages the LBT while incorporating stride detection for speculative execution. The subsequent inference stage utilizes the IDT from SCD to execute $\texttt{sp\_func}$ predictions concurrently. 
% The hardware implementation comprises three corresponding units: the Instruction Reconstruction Unit (IRU), Parallel Inference Engine (PIE), and Vector Instruction Generation Unit (VIGU). The VIGU synthesizes analyses from both IRU and PIE, dynamically merging four restructured load instructions into a single vector operation using the NPU's native SIMD load capabilities, requiring no additional vector support.

VMIG implements a three-stage pipeline where each stage is executed by dedicated hardware units. 
Initially, the Instruction Reconstruction Unit (IRU) manages the micro-instruction restructuring, using the SST provided by the LBD and integrating stride detection for speculative execution. 
The following stage is handled by the Parallel Inference Engine (PIE), which uses the IPT from the SCD and the Vector Register File (VRF) to predict $\texttt{sp\_func}$ concurrently across multiple data streams. As shown in Fig.~\ref{fig:fig4}, it parallelly executes sixteen consecutive dependency chains that generate $\w$ from $\ia$.
Finally, the Vector Instruction Generation Unit (VIGU) synthesises these restructured load instructions into a single vector operation, dynamically optimising the memory access. 

% VMIG addresses these limitations through a three-stage pipeline architecture, with each stage implemented by dedicated hardware units. The Instruction Reconstruction Unit (IRU) manages the initial Micro-instruction Restructuring phase, leveraging the SST from LBD while incorporating stride detection for speculative execution. 
% The Parallel Inference Engine (PIE) handles the subsequent inference stage, utilising both the IPT from SCD and a vector register file (VRF) to execute $\texttt{sp\_func}$ predictions concurrently across multiple data streams. 
% The Vector Instruction Generation Unit (VIGU) performs the final synthesis by dynamically merging four restructured load instructions into a single vector operation. 
This optimisation strategy leverages the NPU's inherent SIMD load capabilities and vector registers, eliminating the need for additional vector hardware. 
As shown in Fig.~\ref{fig:fig4}, the process is fully pipelined, enhancing MLP through rapid consecutive memory request issuance.
Compared to executing single instructions, this pipelined approach significantly reduces memory access latency and improves bandwidth utilisation. 
The efficiency also depends on the MSHR, which prevents cache miss events from blocking subsequent prefetch operations.




% This optimisation leverages the NPU's native SIMD load capabilities and vector registers, requiring no additional vector hardware support beyond existing resources.
% %
% This process is fully pipelined, maximising MLP by enabling rapid issuance of consecutive memory requests. Compared to single-instruction execution, this pipelined approach significantly optimises both memory access latency and bandwidth utilisation. The efficiency relies on MSHR, which prevents cache miss events from blocking subsequent prefetch operations, as detailed below.



% 执行流程
% VMIG operates in three sequential stages: Micro-instruction Restructuring, Parallel Inference, and Vector Instruction Generation.
% In the initial restructuring phase, LBD analyzes load balance patterns while stride detection identifies memory access patterns, jointly optimizing micro-instruction organization. 
% The subsequent inference stage leverages SCD dependency chains to execute $\texttt{sp\_func}$ predictions concurrently, with loop bounds determining the speculative execution depth.
% The dedicated instruction generation unit then synthesizes these analyses, vectorizing both direct and indirect load micro-instructions into their corresponding vector formats. Finally, VMIG injects the optimized vectorized instructions into the NPU pipeline.



% % 硬件结构
% VMIG's hardware implementation consists of three primary functional units: the Instruction Reconstruction Unit (IRU), the Parallel Inference Engine (PIE), and the Vector Instruction Generation Unit (VIGU).
% The IRU leverages the LBT maintained by LBD to determine speculative execution depths, while incorporating its own stride detector to speculative execute forward. 
% The PIE interfaces with the IDT maintained by SCD to obtain sparse data dependency chains, conducting parallel inference of $\texttt{sp\_func}$. 
% The VIGU synthesizes analyses from both IRU and PIE, generating optimized vector instructions using the NPU's native SIMD load capabilities. 
% It includes a compression unit that dynamically merges four restructured load instructions into a single vector operation, maximizing pipeline efficiency while minimizing additional hardware overhead.
% leveraging the NPU's native vector load capabilities without requiring additional vector load support. 





% \vspace{-5pt}
\vspace{-3pt}
\subsection{Non-blocking Speculative Buffer (NSB)}
\vspace{-2pt}
% 加l1cache的理由
For discrete data structures, utilising scratchpad memory incurs substantial logic overhead for data transformation and retrieval operations. However, the characteristics of sparse data patterns present unique opportunities for cache-based optimisation. By strategically storing sparse discrete data in the cache while maintaining continuous data in scratchpad memory (e.g., dense vectors in one-side-sparsity), we can leverage implicit cache line reuse patterns. This cache-based approach naturally accommodates irregular reuse patterns without the costly pre-computation overhead required by scratchpad implementations.
To exploit these characteristics, we introduce NSB, a compact non-blocking cache architecture optimised for discrete element management. Sparse workloads exhibit irregular memory access patterns with extensive index spaces at high sparsity, direct-mapped or low-associativity configurations frequently encounter conflict misses. Thus, we implement a high-way set-associative mapping strategy.
While NSB cannot mitigate L2 cache misses during prefetch operations (as data inherently resides off-chip), it significantly reduces NPU-to-L2 latency and off-chip memory accesses during actual load instruction execution. Experimental results demonstrate that NSB further reduces bandwidth requirements by 5× compared to baseline NVR prefetching.
The design incorporates an MSHR file to manage concurrent memory operations, enabling tracking of outstanding load buffer requests and cache misses. This MSHR infrastructure coalesces multiple outstanding requests to the same cache line, eliminating redundant memory accesses and optimising bandwidth utilisation.


% The design purpose of the non-blocking speculative load buffer is based on the following insights, without the buffer, prefetching competes with computation units for the limited bandwidth of the shared cache, which adversely affects the coverage of prefetching. 
% In contrast, the prefetcher can issue requests to next-level memory through the buffer, while computation units simultaneously access data from the buffer, requests to the shared cache are increased in the event of misses during demand accesses, allowing the majority of memory traffic to be utilized for prefetching. The non-blocking design ensures that regular accesses and prefetching can occur concurrently.


% The Load Buffer is similar to a cache in the CPU but employs a fully associative mapping. Each entry consists of a valid bit, a tag, and data. 
% Miss-status holding registers (MSHRs) are utilized to manage access requests to the Load Buffer. The total size of the Load Buffer is 16 KB.

% \begin{figure}[t]
% \centering
% \includegraphics[width=0.45\textwidth]{fig6.png}
% \caption{VMAG}
% \label{fig:fig5}
% \end{figure}




\vspace{-5pt}
\subsection{Hardware Overhead}
\vspace{-3pt}
Table \ref{tab:tab1} reports NVR's hardware overhead, where $N$ determines the number of parallel entries, matching the vector processing width (default $N$=16). 
The storage overhead is merely 9.72 KiB, negligible relative to the NPU footprint. 
We implement it in RTL and synthesise our design using Synopsys Design Compiler on TSMC 28nm process technology at 2.0 GHz. 
The area overhead is 3\% and 4.6\% relative to the baseline Gemmini architecture, for configurations without and with NSB, respectively. 
These results demonstrate that NVR achieves its performance benefits with minimal hardware cost.

% As $N$, the dominant impactor of MLP and thus performance, grows to 64, the area grows linearly to give XXX KiB total overhead. 
% We evaluate on N = 16 by default but give other values in our evaluation to show the performance-area tradeoff.

% We implemente the NVR's design through RTL code and obtained an area overhead of $0.0305mm^2$, which caused only a 3\% increase in the area compared to Gemmini. 添加NSB的设计则增加4.7\% area. All designs are synthesized using the Synopsys Design Compiler at 2.0 GHz on 28nm TSMC process. 




