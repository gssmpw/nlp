\section{Evaluation}
% \vspace{-3pt}

\begin{table}[t]
    \centering
    % \vspace{5pt}
    \caption{Sparse Computation Workload.}
    \vspace{-5pt}
    \begin{tabular}{|c|c|c|}
    \hline  
        Workload & Short & Domain  \\
        \hline
        \multicolumn{3}{c}{\vspace{-7pt}} \\
        \hline
        Double Sparsity\cite{21doublesparsity}               & DS     & large language model            \\\hline
        Graph Attention Networks\cite{22GAT}      & GAT    & graph neural networks           \\\hline
        Graph Convolutional Networks\cite{23GCN}  & GCN    & graph neural networks           \\\hline
        Graph Sparse Attention \cite{24GSABT}            & GSABT  & sparse attention                \\\hline
        Heavy-Hitter Oracle\cite{25h2o}           & H2O    & large language model            \\\hline
        MinkowskiNet\cite{26MK}                & MK     & point cloud                     \\\hline
        SparseConvNet \cite{27SCN}                        & SCN    & point cloud                     \\\hline
        Switch Transformer\cite{28ST}             & ST     & mixture of experts              \\\hline
    \end{tabular}
    \vspace{-15pt}
    \label{tab:tab1}
\end{table}


% \begin{figure*}[htbp]
% \centering
% \vspace{-20pt}



% \vspace{-10pt}
% \caption{ Demo }
% \label{fig:fig7}
% \end{figure*}



% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.5\textwidth]{fig7_2.png}
% \caption{XXXXXXXX}
% \vspace{-20pt}
% \end{figure}




\vspace{-5pt}
\subsection{Experimental Setup}
\vspace{-3pt}
We implement and evaluate NVR by integrating it into a Gemmini-like NPU model constructed using the ScaleSim simulator\cite{scalesim}. 
Additionally, we leverage LLMCompass\cite{llmcompass}, a specialised simulator designed to evaluate hardware optimisations in LLM inference.% workloads.

% \textbf{\textit{Comparison}}
\noindent \textbf{Comparison}
We evaluate our approach against in-order Gemmini (serial execution of load and compute instructions), ideal out-of-order Gemmini (overlapping the load and computation time), and several prefetchers.
We evaluate the following prefetch techniques:
1) stream prefetcher\cite{stream}, the simplest prefetch mechanism based on stride;
2) IMP\cite{imp}, one of the SOTA indirect memory prefetchers, prefetching indirect memory access at the L1 cache;
3) DVR\cite{8DVR}, one of the SOTA runahead techniques, vectorising the same chain of indirect memory accesses across multiple invocations of an inner loop.
4) Our NPU Vector Runahead. 
For a fair comparison, we add the Gemmini custom ISA support for these prefetchers and expand them to the same number of parallels as the NVR. 

% \textbf{\textit{Workload}}
\noindent \textbf{Workload}
Sparsity is widely present across various workloads in DNNs. 
We select several key sources of sparsity, including attention mechanisms\cite{attention, sparseattention}, MoE structures\cite{MoE, glam}, 3D point cloud processing\cite{3Dpoint, dgcnn}, and so on. 
% We collect a variety of models based on their functionalities and extract their linear layers' memory access pattern to serve as representative workloads, as shown in Table \ref{tab:tab1}.
Table \ref{tab:tab1} presents representative workloads extracted from various models' linear layer memory access patterns based on their functionalities and domains.


% \begin{table}
%     \centering
%     \caption{Sparse matrice workload.}
%     \vspace{-5pt}
%     \begin{tabular}{|c|c|c|c|c|}
%     \hline  
%         Workload & Short & Domain & Linear Prop. & Density \\
%         \hline
%         \multicolumn{5}{c}{\vspace{-7pt}} \\
%         \hline
%         Double Sparsity\cite{}& SP &  LLM &         & \\\hline
%                               &     & LLM &         & \\\hline
%                               &     & MoE &         & \\\hline
%                               &     & MoE &         & \\\hline
%                               &     & Sparse Attn. &         & \\\hline
%                               &     & Point Cloud &         & \\\hline
%                               &     & Point Cloud &         & \\\hline
%          LightGaussian\cite{} & LG  & 3DGS &         & \\\hline
%                               &     & GNN  &         & \\\hline
%                               &     & GNN &  & \\\hline
%     \end{tabular}
%     \vspace{-15pt}
%     \label{tab:tab1}
% \end{table}

% \textbf{\textit{Parameter Selection}}

\begin{figure*}[t]
\centering
\vspace{-5pt}
\hspace{-10pt}
\subfigure{\includegraphics[height=2.7cm]{fig6_1.pdf}}
\hspace{-7pt}
\subfigure{\includegraphics[height=2.7cm]{fig6_2.pdf}}
\subfigure{\includegraphics[height=2.7cm]{fig6_3.pdf}}
\vspace{-8pt}
\caption{Prefetcher accuracy and coverage evaluation and the data movement optimisation results of NVR.}
\vspace{-18pt}
\label{fig:fig6}
\end{figure*}




\begin{figure}[t]
\centering
\vspace{-3pt}
\hspace{-10pt}
\subfigure{\includegraphics[height=2.5cm]{fig7_1.pdf}}
% \hspace{-5pt}
\hspace{-10pt}
\subfigure{\includegraphics[height=2.5cm]{fig7_2.pdf}}
\vspace{-8pt}
\caption{Normalised bandwidth allocation without/with NSB.}
\vspace{-5pt}
\label{fig:fig7}
\end{figure}


\begin{figure}[htbp]
\centering
\vspace{-10pt}
% \subfigure[XXXXX]{\includegraphics[height=2.5cm]{fig7_2.png}}
\hspace{-5pt}
\subfigure{\includegraphics[height=3.9cm]{fig8_1.pdf}}
\hspace{-17pt}
\subfigure{\includegraphics[height=3.9cm]{fig8_2.pdf}}
\hspace{-20pt}
\subfigure{\includegraphics[height=3.9cm]{fig8_3.pdf}}
% \vspace{-10pt}
% \caption{System-level Evaluation on LLMs \Nnote{height ?}}
% \vspace{-15pt}
% \includegraphics[width=0.48\textwidth]{Fig8.pdf}
% \includegraphics[width=0.48\textwidth,trim={20\% 0 20\% 0},clip]{Fig8.pdf}
\vspace{-10pt}
\caption{System-level Evaluation on LLMs. In (b) and (c), dashed and solid lines represent NVR and baseline performance respectively, where $l$ denotes input/output sequence length.}
\vspace{-15pt}
\label{fig:fig8}
\end{figure}




\vspace{-3pt}
\subsection{Performance Breakdown}
\vspace{-3pt}
Since DNNs commonly employ low bit-width computations, we evaluate different bit-width configurations (INT8, FP16, INT32) to analyse their impact on cache behaviour and system performance, as shown in Figs.~\ref{fig:fig5} (a), (b), and (c). 
Higher data bit-width inherently requires longer
data movement time and larger cache block capacities, increasing the probability of cache misses.


% Due to the widespread use of reduced bit-width computations in DNNs, we evaluate multiple bit-width configurations (INT8, FP16, and INT32) to analyse their impact on cache behaviour and overall system performance, as illustrated in Fig.\ref{fig:fig6} (a), (b), and (c). 
% Higher data bit-width requires longer data movement time and larger cache block capacities, increasing the probability of cache misses. 

Overall, cache miss stalls represent a substantial portion of wall-clock time across most sparse workloads. 
The Switch Transformer presents a notable exception, showing lower cache miss ratios due to its relatively fixed network architecture and block-like data distribution patterns.
For sparse workloads, the NPU's OoO (out-of-order) execution capabilities struggle to effectively overlap data movement with computation to mitigate cache miss latency, as several workloads are inherently IO-bound. 
While prefetching mechanisms generally reduce cache miss stall time effectively, stream prefetchers occasionally introduce performance penalties due to their lower accuracy. 
Notably, our NVR implementation achieve reductions in latency, averaging 98.3\%, 99.2\%, and 97.3\% for INT8, FP16, and INT32, respectively.

As shown in Fig.~\ref{fig:fig5}~(a),  we evaluate INT32 latency variations with NSB enabled. we evaluate latency variations with NSB using INT32 configurations. The red line represents the baseline latency without NSB. 
With NSB enabled, L2 cache miss latency decreased by about 15\%. 
When combined with NVR, NSB achieves a 40\% reduction in cache miss stalls, but performs poorly with conventional stream prefetchers. Thus, NSB activation depends on prefetcher accuracy.

% As illustrated in Figure 6 (d), we perform a detailed evaluation of latency variations with NSB implementation using INT32 configurations. The red line indicates the baseline latency without NSB implementation. Our results demonstrate that with NSB enabled, the L2 cache miss latency decreased by approximately 15\% in most cases for INT32 operations.
% Notably, we observe that the effectiveness of NSB is closely correlated with prefetcher accuracy. When paired with our NVR prefetcher, NSB achieve an average 40\% reduction in cache miss stall time. However, when combine with conventional stream prefetchers, performance is often degraded. Based on these findings, we implement NSB as an optional feature that is conditionally enabled based on prefetcher accuracy thresholds.





\vspace{-5pt}
% \vspace{-3pt}
\subsection{Prefetching Effectiveness and Off-Chip Bandwidth Reduction}
\vspace{-3pt}


This experiment evaluates prefetcher performance across sparse workloads using two key metrics: accuracy and coverage. NVR demonstrates consistently high performance, maintaining both accuracy and coverage rates above 90\% across most workloads. As shown in Fig.~\ref{fig:fig6}~(b), coverage, which is more crucial for NPU performance, presents a greater challenge than accuracy. Benefiting from our fuzzy prefetch strategy, NVR also achieves significantly higher performance in this metric compared to other prefetchers.

The combined effect of accuracy and coverage directly influences prefetch bandwidth and the volume of off-chip memory accesses caused by cache misses during NPU execution.
Figs.~\ref{fig:fig7} (a) and (b) demonstrate the total bandwidth, including prefetch bandwidth. In both cases, compared to the InO baseline, the off-chip memory bandwidth is reduced by around 75\%.
Since the prefetch bandwidth is often more easily overlapped, Fig.~\ref{fig:fig6}~(c) shows the NPU actual load execution time with the memory accesses bandwidth removed. NVR can effectively reduce the off-chip memory accesses by 30x, and with NSB, it can be further reduced by 5x.




% \noindent\textbf{Accuracy}
% Fig. \ref{fig:fig7} (a) illustrates the accuracy achieved by different prefetching strategies (streaming, IMP, DVR and NVR) across different workloads. The experimental results demonstrate a clear difference in accuracy among the various prefetching methods, particularly highlighting the superior performance of the NVR strategy.

% NVR consistently outperforms other strategies across the majority of workloads, achieving an average accuracy close to 1.0. The high accuracy of NVR can be attributed to its adaptive prefetching mechanism, which effectively predicts memory access patterns and reduces out-of-bound memory fetches. \zzpnote{This is my guess} For example, in the GSABT and SCN workloads, NVR achieves almost perfect accuracy, demonstrating its capability in scenarios where the memory access sequence is relatively predictable and aligned. This result emphasizes NVR's strength in managing complex access patterns and reducing unnecessary memory fetches, thereby maintaining high prediction accuracy.

% An interesting phenomenon is observed in the MK and GAT workloads: the accuracy of IMP, DVR, and even NVR shows a decline, while the accuracy of the stream strategy relatively increases. Despite this increase, NVR still remains the best-performing strategy. This can be explained by the more irregular memory access patterns present in MK and GAT workloads. IMP and DVR, relying on fixed prefetch depths, struggle to adapt to such complex memory access patterns, leading to frequent out-of-bound prefetches and incorrect predictions. Conversely, Stream, due to its simpler sequential prefetching approach, manages to perform slightly better under these specific conditions, but this advantage is insufficient to surpass the adaptive capabilities of NVR.

% Moreover, in the SCN workload, the performance of IMP and DVR is even worse than that of Stream. This is because the SCN memory access pattern is quite particular, and IMP and DVR are unable to effectively capture the loop bounds, resulting in either the prefetch not being triggered or frequent incorrect predictions when triggered. In contrast, the simple sequential nature of Stream allows it to achieve some level of correctness in this scenario. However, NVR, with its flexible adaptive mechanism, is better able to adjust its prefetching strategy, thereby maintaining a high level of accuracy in the SCN workload.

% The overall average accuracy (AVG) further confirms the superiority of NVR, which consistently maintains higher accuracy across diverse workloads. The results indicate that the adaptive capabilities of NVR make it well-suited for handling both regular and irregular memory access patterns, reducing the number of out-of-bound accesses and ensuring efficient data fetching.

% \noindent\textbf{Coverage}
% Fig. \ref{fig} compares the accuracy of different prefetching strategies (Stream, IMP, DVR, and NVR) across various workloads including DS, H2O, GCN, among others. 
% NVR demonstrates superior performance, achieving nearly 100\% accuracy across most workloads, particularly in GSABT and SCN cases.
% Notably, in MK and GAT workloads, which feature irregular memory access patterns, we observed a performance decline across all strategies. While Stream prefetching showed relative improvement in these cases due to its simpler sequential approach, NVR still maintained the highest overall accuracy. In the SCN workload, IMP and DVR performed worse than Stream due to their inability to effectively capture loop bounds, while NVR's adaptive mechanism maintained high accuracy.
% The average accuracy across all workloads confirms NVR's superior performance in handling both regular and irregular memory access patterns, demonstrating its effectiveness in reducing out-of-bound memory fetches.

% \noindent\textbf{Off-Chip Bandwidth Reduction}

% \vspace{-3pt}
\subsection{LLM Inference Evaluation}
\vspace{-3pt}

To validate our system-level optimisations, we evaluate the impact of NVR on the performance of LLMs. The experiments are conducted using the LLMCompass simulator.
As shown in Fig.~\ref{fig:fig8}~(a), the NVR implementation reduces overall latency by decreasing cache miss stall time in the LLMs.
Under NVR, both overall and per-batch cache miss rates decrease exponentially, with the latter showing a slower decay rate.
LLMs are typically composed of two main stages: prefill and decode. It has been proven that the prefill stage is compute-bound, while the decode stage is IO-bound.
Figs.~\ref{fig:fig8}~(b) and (c) demonstrate the benefits of applying NVR to the entire LLM network. 

It is evident that for the compute-bound prefill stage, NVR can expedite reaching maximum throughput, particularly in low-bandwidth scenarios.
For the IO-bound decode stage, the NVR architecture demonstrates an average 50\% throughput improvement through reduced off-chip memory accesses. This enhancement becomes increasingly pronounced with longer output sequences.
% The scatter points represent throughput improvements for different workload configurations before and after optimization. In the IO-bound decoding phase, we achieved XXXx speedup, with benefits becoming more pronounced as the context length increases.



% $$
% I_{\texttt{max}} =\frac{\pi}{\beta}
% $$




\begin{figure}[t]
\vspace{-2pt}
% \includegraphics[width=0.48\textwidth]{fig9.pdf}
\includegraphics[width=0.48\textwidth,height=0.2\textwidth]{fig9.pdf}
\vspace{-5pt}
\caption{NSB and L2 cache impact on performance. Performance metric: the inverse of latency and area (higher is better).}
\vspace{-12pt}
\label{fig:fig9}
\end{figure}

% \vspace{-5pt}
\vspace{-3pt}
\subsection{Sensitivity Analyses}
\vspace{-3pt}

To evaluate NSB's efficacy, we conduct comprehensive sensitivity analyses across varying NSB and L2 cache parameters. 
The performance metrics represent the inverse relationship between latency and area, calculated as the product of NSB and L2 Cache dimensions, ensuring equivalent area penalties for both scaling approaches.
As illustrated in Fig.~\ref{fig:fig9}, modest NSB buffer configurations yield substantially higher performance improvements than equivalent L2 cache scaling. Specifically, in a configuration with 256KB L2 cache and 4KB NSB, quadrupling the NSB capacity delivers 5x the performance benefit over scaling the L2 cache to 1024KB. 