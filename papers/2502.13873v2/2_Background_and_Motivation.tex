\vspace{-2pt}
\section{Background and Motivation}
\vspace{-2pt}
% \begin{figure}[t]
% \centering
% \includegraphics[height=0.1\textwidth]{fig2.png}
% \vspace{-8pt}
% \caption{Structured Sparsity and unstructured Sparsity}
% \label{fig:fig2}
% \end{figure}

% \begin{figure*}[t]
% \vspace{-20pt}
% \centering
% \includegraphics[width=1\textwidth]{fig3.png}
% \vspace{-20pt}
% \caption{Sparse Matrix Multiplication operations can be categorized into one-side-sparsity and two-sides-sparsity patterns, with higher sparsity offering greater speedup potential at the cost of more challenging access patterns. Here, $\mathrm{\mathbf{spatial\_for}}$ denotes parallel execution across the NPU, while $\mathrm{\mathbf{IA}}$ (input activation), $\mathrm{\mathbf{W}}$ (weight), and $\mathrm{\mathbf{OA}}$ (output activation) represent the input variables, weight parameters, and output results, respectively.}
% \vspace{-10pt}
% \label{fig:fig3}
% \end{figure*}

 

% \begin{figure}[t]
% \vspace{-20pt}
% \centering
% \includegraphics[width=0.47\textwidth]{fig3_1.png}
% \vspace{-10pt}
% \caption{Structured Sparsity and unstructured Sparsity}
% \label{fig:fig3}
% \vspace{-15pt}
% \end{figure}




% 第一节: sparse 有很多cache miss
% sparse Memory Accesses 为什么会大量cache miss
% 有大量的cache miss, 对于大多数NPU而言对cache miss的容忍性很差。造成的Bottleneck：1. 片外带宽的浪费 2，片内计算单元的空转

% 第二节：对于NPU，prefetch比 CPU更重要
%Another road: replace to explicitly preload and speculatively prefetch 现有方法的不足

% 第三节: prefetch on NPU的挑战（我们面对的挑战）
% 挑战一：workload的多样性，不规则访存的因素多样
% 挑战二：NPU架构的特殊性，
% \subsection{Performance Breakdown: }



Although sparse workloads offer high theoretical memory-level parallelism (MLP), hardware systems struggle to exploit this algorithmic advantage effectively.
As shown in Fig.~\ref{fig:fig1}~\circled{b}, we systematically profile the parameters of Double  Sparsity\cite{21doublesparsity} in LLMs workload and its performance on NPU with a 256KB L2 cache. 
We find that reducing parameters does not lead to proportional decreases in off-chip memory accesses, resulting in out-of-bounds accesses for explicit buffers (like scratchpad) or cache misses for L2 cache in the chip. As these two scenarios are essentially identical, we focus our analysis on cache misses in the following discussion.
% As shown in Fig. \ref{fig:fig2}, we systematically profile the characteristics of sparse workload and their performance on NPU. 
% We find that the challenge of accelerating sparse workloads comes from two challenges: 1) imbalance between compute and memory~2) huge cache miss caused by irregular memory access. 
% Due to reduced data reuse in sparse computations, the decrease in data movement overhead is often proportionally less than the reduction in computational workload, resulting in a shift toward I/O-bound operations.  In this context, the substantial number of cache misses further exacerbates the I/O pressure, becoming a critical performance bottleneck. \mionote{we discuss one challenge in the intro and two here... is this ok?} \mionote{delete the first one}
% 由于稀疏带来的数据复用的减少，所以稀疏带来数据搬运的消耗的缩减比例往往小于计算的缩减比例，造成workload向IO bound偏移。

% Therefore, the memory footprint of features in point cloud networks significantly surpasses CNNs. As shown in Figure \ref{} (right), the memory footprint of the features per point in point cloud networks can achieve up to 16 KB, which is 100× higher than CNNs. Thus the data movement alone can take up over 50\% of total runtime on CPUs and GPUs, as shown in Figure 6 (right).


% These applications typically feature extremely low computation-to-communication ratio and irregular memory accesses, meaning their performance is memory bound, and out-of-order cores provide significant performance advantages over their in-order counterparts.


% \vspace{-8pt}
\vspace{-2pt}
\subsection{Sparse Memory Accesses: Misses Are a Fact of Life}
\label{sc:sc1}
\vspace{-2pt}


In this section, we introduce the sparse computation and analyse the sparse irregular memory access patterns, which lead to a heavy cache miss and NPU stalls.
As an example, Fig.~\ref{fig:fig2}~(right) illustrates a typical sparse matrix multiplication (SpMM), which is common in DNNs, computation with compressed sparse row (CSR) format. The code snippet demonstrates the sparse weight matrix ($\texttt{W}$) selectively indexes input activation ($\texttt{IA}$) vectors for multiply-accumulate operations.
%  exhibits irregular and hard-to-predict memory access behaviours. 
% In this section, we explore the sources of these irregular accesses. % and our approach to effective prediction. 
This workload exhibits several critical factors contributing to cache misses: 



\noindent\textbf{Indirect Memory Accesses} Since only non-zero data is computed, the data must be aligned, which involves numerous index dependency chains.
While $\texttt{W[j]}$ accesses are often sequential and can be captured by streaming prefetch, $\texttt{IA[sparse\_func(W[i])]}$ accesses target non-sequential locations. Given that, $\texttt{IA}$ typically spans a large index space that exceeds L2 cache capacity, resulting in frequent cache misses. This alignment pattern is common in across sparsity applications, like hash-table indexing 
%in sparse attention 
and sampling operation in point cloud networks, where $\texttt{sparse\_func}$ is replaced by complex indexing schemes, often requiring dedicated processing units.


\noindent\textbf{Dynamic Loop Boundaries} %Tiling strategy introduces dynamic loop boundaries: 
The tiling strategy refers to reordering element arrangements for hardware computation units (e.g., systolic arrays, vector units). This manifests as irregular loop boundaries. In large dynamic routing architectures such as Mixture-of-Experts (MoE), the memory span between \texttt{rowptr[i]} and $\texttt{rowptr[i+1]}$ can be substantial, posing significant challenges for cache capacity. % For large matrices or high sparsity scenarios, 



% output reorder
\noindent\textbf{Data Shuffle} 
As shown in Fig.~\ref{fig:fig2}~(left), NPUs employ various skip strategies to accelerate computation by bypassing zero elements. 
NPUs with stronger reordering capabilities can group farther distant data elements for unified processing. 
This leads to densely packed, long-stride access patterns within short time intervals, causing catastrophic cache behaviour.
This issue is particularly evident in LLMs' KVCache~\cite{21doublesparsity}, where sparse attention employs TopK selection to retain only the k highest-scoring vectors for computation. For LLMs with extended contexts, KVCache indexing can span several gigabytes, significantly exceeding cache capacity.


% In addition to the aforementioned cache miss sources from computational mode, the data intrinsic sparsity
% Skipping Strategy introduces long-distance sparse skipping patterns: This pattern is prevalent across diverse applications, such as KVCache\cite{} parameter sparsity in LLMs and point cloud network dataset sparsity, where distant memory regions are frequently accessed. In the context of large language models with extended contexts, indexing within the entire KV Cache space potentially spans several gigabytes, far exceeding input buffer capacity. 


% skip 决定了跳过的维度的访存不连续程度，越越不连续
% align 决定了数据转换与Reorder的开销，使得预取要做出一定的妥协
% tile 决定了分界的跳跃程度，越越跳跃

% preload 的预取方式代价太大，所以选择prefetch的方式
% stride pattern: 可以通过
% sparse dependency pattern:
% loop pattern: 因为NPU的黑盒结构所以导致不好检测了



\vspace{-5pt}
% \vspace{-3pt}
% \subsection{More Pressing IO Miss Bound on NPU}
\subsection{Cache-miss Vulnerable NPUs}%Suffering % Sensitivity % Pressing
%Crystal Ball
\vspace{-3pt}

Facing numerous cache misses, NPUs suffer even more severe performance degradation than CPUs and GPUs.
While CPUs leverage Out-of-Order (OoO) execution and reorder buffers (ROB) to tolerate this through fine-grained instruction-level overlap, and GPUs utilise thread-level scheduling for cache misses hiding, NPUs remain limited in such capabilities.
Built primarily on Single Instruction Multiple Data (SIMD) architectures, NPUs possess only coarse-grained instruction parallelism, with compute and memory movement channels decoupled.
As demonstrated in Fig.~\ref{fig:fig5} in the experiment, even with ideal OoO execution on NPUs, it also shows suboptimal performance in handling cache misses.
Critically, the data-parallel nature of NPUs means a cache miss in any vector element stalls the entire processing pipeline, leading to performance degradation on sparse workloads.
% \zhenote{Any quantitive numbers?}

% \noindent\textbf{Explicit Preload and Speculative Prefetch}
NPU memory access overlap is constrained by explicit preload mechanisms and structured scratchpad usage. The preloading process, which requires executing complete load instructions, involves extensive computations and full dependency chains, making it difficult to effectively hide memory latency, particularly in IO-bound scenarios.
While Han et al.\cite{lin2021pointacc} proposed converting explicit memory into a cache, our experiments find this approach insufficient. 
In contrast, NVR’s speculative execution allows flexible prefetching without the need for precise dependency calculations, reducing latency.
% NPU memory access overlap is constrained to explicit preload mechanisms due to their highly structured architecture. They often utilize explicitly managed scratchpad memory for storage and perform row-based computations. 
% While Han et al.\cite{lin2021pointacc} proposes converting explicit memory into a cache to address sparse-induced out-of-memory issues, our experimental evaluation reveals this approach is not insufficient.\mionote{here} 

% Current NPU memory overlap relies on scheduling explicit preload, pre-executing load instructions to move data from main memory to pre-defined buffer positions. 
% However, this approach requires extensive and precise computations, along with complete dependency chains, often making it impractical to overlap memory latency through scheduling alone, particularly in IO-bound scenarios. 
% In contrast, NVR's speculative execution can prefetch without requiring the precise calculations needed to guarantee NPU logic, as long as the prefetch addresses are obtained, offering a more flexible solution.
% \Guan{Add some citations?} \mionote{for what?}
% \mionote{add conventional prefetcher}



% \vspace{-5pt}
\vspace{-5pt}
\subsection{Challenge for NPU Prefetch}
\vspace{-3pt}
% \zhenote{Combine them.}
% 传统预取器有针对Indirect Memory Access, Dynamic Loop Boundaries and Data Shuffle这些的预取，但是直接用到NPU存在多方面的问题.
There are many conventional prefetchers designed for handling irregular memory access patterns mentioned in section \ref{sc:sc1}.
For sparse workloads with abundant irregular memory accesses, simple pattern-based\cite{pattern-based, Feedback-Directed}
%Best-offset} 
or history-based\cite{history, SMS, VLDP} prefetchers often fail to effectively capture and predict these accesses, whereas runahead-based\cite{8runahead, 8CRE, 8PRE} prefetching has been widely adopted as the solution.
Runahead employs speculative execution to hide cache miss latency by prefetching future memory accesses.
VR\cite{8DVR} and DVR\cite{8VR} explore leveraging vector units to perform runahead in parallel, which is a well-suited approach for NPUs that inherently support vector operation instructions.
% However, applying these techniques directly to NPUs presents several challenges.
However, applying these techniques directly to NPUs presents several challenges.

Despite the success of these prefetching techniques in general-purpose processors, the unique characteristics of NPU architectures and their workloads necessitate a fundamentally different approach.
Diverse sparse workloads on NPUs often require customised hardware implementations, such as hash-table-based methods in point cloud processing.
%, exhibiting distinct patterns of irregular and difficult-to-predict memory accesses.
Meanwhile, sparse workloads commonly leverage specialised sparse data formats, like TACO's \cite{parker:2016:meng-thesis} multi-dimensional encoding and SMASH's \cite{kanellopoulos2019smash} hierarchical bitmap-based decompression, handled by dedicated processing units.
This diversity of dependency chains, coupled with the need for parallel execution, poses significant challenges for prefetchers in both pattern capture and overhead. 
A generalised prefetching approach decoupled from specific sparse computation patterns is necessary.
Moreover, NPUs operate with coarse-grained instructions that process entire vectors or matrices. The decoding of such coarse-grained instructions implies weaker locality awareness and significant computational overhead, making traditional instruction-level optimisations less applicable.
% \Nnote{Moreover, NPUs operate with coarse-grained instructions that process entire vectors or matrices, making traditional instruction-level optimizations less applicable. Consequently, it is imperative to design a prefetching approach that is generalized and independent of any particular sparse computation paradigm.}

% \noindent\textbf{Challenge I: Sparse Workloads' Diversity} %\emph{}

% Software.....Diverse sparse workloads exhibit distinct patterns of irregular and difficult-to-predict memory accesses driven by indexed lookups. 
% For different sparse representation formats, like TACO~\cite{parker:2016:meng-thesis} sparse encoding on multi-dimensions, while SMASH~\cite{kanellopoulos2019smash} needs hierarchical bitmap-based decompression for index reconstruction. 
% Additionally, alignment-oriented retrieval computations, such as hash-table-based methods, which are prevalent in point cloud processing workloads.
% The complexity and diversity of these dependency chains, coupled with the requirement for parallel execution, present significant challenges for prefetchers in both pattern recognition and overhead. This necessitates the design of a generalized prefetching mechanism that decouples from specific sparse computation patterns.



% \noindent\textbf{Challenge II: NPUs' Architectural Specificity} 
% 黑盒
% 与cpu的miss to latency不同，npu是为了减少片外访存，注重吞吐
% npu的指令是粗粒度的
% Hardware.....Previous research on runahead execution has primarily been applied to CPUs
% Although NPUs face similar challenges for irregular memory access, the objective of cache miss mitigation in NPUs is fundamentally different from CPUs. While CPUs primarily aim to reduce average memory access latency, NPUs focus on minimizing off-chip memory access requests due to their bandwidth-bound. Moreover, NPUs operate with coarse-grained instructions that process entire vectors or matrices, making traditional fine-grained instruction-level optimizations less applicable. 
% Additionally, NPU architectures often present as "black boxes" with limited visibility into their internal operations, further complicating the adaptation of CPU-oriented prefetching techniques. 

% Understanding Sources of Inefficiency in General-Purpose Chips








% To mitigate the aforementioned cache misses, two approaches can be considered. 
% One approach relies on NPU scheduling to implement "preload" operations. However, NPU preloading faces significant challenges: it requires extensive, precise computations and substantially related data, making it often impractical to mask memory latency through scheduling alone, particularly in IO-intensive scenarios. In contrast, NVR's speculative execution can initiate prefetching without requiring complete computational dependency chains, offering a more flexible solution. 
% Another alternative would be leveraging CPU hardware prefetchers. However, this approach is particularly challenging due to the opacity of NPU architectures and variations in execution methods across different NPUs.




% \Guan{So, what are our challenges? If I understand correctly, the text here mainly discusses the aspects we are focusing on (evaluation metrics), and does not clearly specify the challenges that runahead execution might introduce to our system.} \mionote{WIP, ==me}


% \circled{1} branch caused by sparse alignment: Sparse alignment means identifying non-zero elements or blocks that contain them, which cost the main overhead for sparse on NPU. 
% This process becomes particularly challenging in two-sided sparsity scenarios, where alignment must be performed bidirectionally, leading to increased cache misses due to irregular access patterns.
% \circled{1} Alignment Strategy caused indirect memory accesses in a sparse dependency chain: Accesses to $\mathrm{W[j]}$ are typically sequential and may be captured by streaming prefetchers. The $\mathrm{IA[sparse\_func(W[i])]}$ accesses, however, tend to touch non-sequential memory locations. In these applications, the size of $\mathrm{IA}$ is large and usually does not fit in the first-level cache; thus, the indirect accesses to A generate many cache misses. The alignment is commonly manifest in other sparsity applications, such as TopK indexing in sparse attention and sampling operations in point cloud networks. These operations essentially replace $\mathrm{sparse\_func}$ with more sophisticated indexing schemes, often requiring dedicated units for index processing.

% \mionote{Shall we remain the following sentences in this paragraph? or delete them.}
% \zhenote{I think we can delete them, not very important.}
% While streaming prefetchers can capture the sequential accesses to the matrix $IA$ and its indices, they fail to address the non-consecutive memory accesses inherent in sparse indirect addressing. 
% Although some techniques, prefetching by extracting dependency chains\cite{} or learning indirect address generation\cite{}, the NPU's black-box architecture obscures the complete address translation path from traditional prefetchers. 
% The limitation becomes particularly pronounced in two-sided sparsity computations, where the complex interaction between two sparse matrices creates access patterns that exceed the prediction capabilities of traditional prefetchers.

% Conventional prefetching mechanisms struggle to handle these access patterns effectively. While streaming prefetchers can capture the sequential accesses to the matrix $A$ and its indices, they fail to address the non-consecutive memory accesses inherent in sparse indirect addressing. 
% Although some techniques, prefetching by extracting dependency chains\cite{} or learning indirect address generation\cite{}, the NPU's black-box architecture obscures the complete address translation path from traditional prefetchers. 
% The limitation becomes particularly pronounced in two-sided sparsity computations, where the complex interaction between two sparse matrices creates access patterns that exceed the prediction capabilities of traditional prefetchers.


% 在这里不说我们的预取方案了就
% Our prefetching strategy leverages the observation that accesses to matrix $A$ constitute a relatively small portion of memory traffic compared to the dominant vectorized accesses to $B$. By over-prefetching matrix $A$, we effectively transform the two-side-sparsity problem into a one-side scenario. 
% For matrix $A$, the sequential access patterns can be efficiently handled by streaming prefetchers. 
% For matrix $B$, we combine streaming prefetchers with vectorized indirect addressing to tackle its non-consecutive access patterns. 
% Meanwhile, our loop boundary detector(LBD) predicts and mitigates cache misses at boundaries. 
% Through the runahead mechanism, we utilize NPU's sparse computation units to extract and complete the computation chains obscured by its black-box architecture.

%= ============== Prefetcher部分 ======================

% 可能还得再压一波。。。
% \subsection{Prefetch Technology}
% Among various approaches to reduce the slowdown caused by cache misses, prefetching stands as a fundamental optimization technique. In this section, we explore the feasibility of the main prefetching strategies in the NPU architecture.

% Software prefetching operates by inserting explicit prefetch instructions during compilation. While compiler techniques have been developed to automate this process\cite{}, their effectiveness in NPU contexts is limited due to high instruction overhead 
% and poor portability across different NPU programming models.
% % \cite Mowry/Ainsworth
% Hardware prefetching techniques can be categorized into pattern-based and execution-based approaches\cite{}. Pattern-based prefetchers, including stride\cite{}, temporal\cite{}, and content prefetchers\cite{}, demonstrate limitations when handling irregular memory patterns and indirect  access.
% Execution-based prefetching primarily encompasses helper threads and Runahead execution. Helper threads leverage multithreading for prefetching; however, the additional thread overhead proves costly within SIMD-oriented NPU architectures. Runahead execution exploits processor idle resources to speculatively execute instructions earlier, thereby hiding memory latency. 
% \mionote{please check this}
% % 这三个感觉说的不是很准确重新写下？每个半句话就行
% Recent advances include CRE\cite{} execute dependence chains in a loop, VR\cite{} executes multiple loop iterations as a single vector instruction, and DVR\cite{} detects loop bounds at runtime.

%= ====================================

% \par Fig. \ref{fig:fig3} illustrates a weight stationary (WS) computation pattern with one-side-sparsity. In this code, the accesses to the matrix $A$
% %($A\rightarrow values[j]$) 
% exhibit sequential patterns that can be effectively captured by streaming prefetchers. However, the indirect memory accesses to $B[A\rightarrow col\_indices[j]]$ demonstrate non-consecutive patterns. Given that each access to B is vectorized, these indirect memory access patterns inevitably result in substantial cache misses.
% Nevertheless, due to the compressed sparse storage, accesses to $A\rightarrow col\_indices[j]$ maintain sequential patterns that enable vectorized prefetching through learned sparse indirect addressing patterns. The purple blocks indicate memory access discontinuities at loop boundaries, which introduce significant cache misses that can be predicted by our loop boundary detector (LBD).

% \par In two-side-sparsity scenarios, where both matrices $A$ and $B$ are sparse, the memory access patterns become increasingly unpredictable, presenting additional challenges.
% Given that throughput is crucial for NPU performance, minimizing cache misses and inefficient control logic is essential. In the illustrated computation, vectorized accesses to $B$ dominate the memory traffic, while accesses to $A$ are relatively infrequent. Therefore, by over-prefetching $A$, we can effectively transform this two-sided sparsity problem into a one-sided sparsity scenario.

% 将双边稀疏转为单边稀疏
% 1. Stream Prefetch解决A
% 2. Stream Prefetch和向量化和间接访问解决B 
% 3. Boundary Detector



% \par Two-Step Memory Access Strategy for Unilateral Sparsity: In the case of unilateral sparsity, we introduce a two-step memory access strategy aimed at minimizing the inefficiencies caused by irregular memory access. The first step involves using a buffer array to manage the indices of matrix A. Each row of the buffer is linked to a row of matrix A, which enables locally consecutive access to A and reduces the likelihood of cache misses. After establishing this buffer, we use a row-tiling or column-tiling strategy to generate a memory access mask. This mask determines the target columns for the systolic array during each computation cycle, allowing us to preload values from A efficiently—filling in any missing data with zeros when necessary. Moreover, the mask directly informs the memory access requirements for matrix B, effectively aligning the access of both matrices and thereby enhancing computational efficiency.
% \par Vector-Like Approach for Bilateral Sparsity:  For bilateral sparsity, we adopt a vector-like computational approach to address the issues of reduced data reuse and irregular memory access. Given that the increased sparsity in both matrices A and B significantly reduces the potential for data reuse, systolic arrays are no longer suitable. Instead, we utilize a single column of multipliers, which allows for a simpler and more predictable access pattern. In this setup, both matrices A and B are accessed in a continuous manner—processing one row or one column at a time. After retrieval, the corresponding values are paired and fed into the computation units. This method greatly simplifies the memory access pattern, enhances predictability, and eliminates the complexities associated with indirect indexing.


% ===================== 完整版本 ===================
% \par In dense matrix computations, the memory access pattern is highly regular, enabling efficient prefetching and caching, which significantly improves computational efficiency. However, the situation is entirely different for sparse matrices. Sparse matrices offer considerable advantages in terms of storage efficiency by only storing non-zero elements. Nevertheless, this storage efficiency comes at the cost of irregular memory access patterns, which increases the complexity of memory operations and significantly impacts computational performance.
% \par \textbf{Increased Cache Misses:}  Sparse matrices, particularly those with unilateral sparsity, suffer from increased cache misses. For instance, in the case of matrix A with a sparsity level of 1/16, the corresponding memory access space for matrix B during a computation can expand up to 15 times. This expansion results in a substantial number of cache misses, as the memory access becomes scattered and inefficient. The increased number of cache misses limits the benefits of modern caching systems and leads to lower computational efficiency.
% \par \textbf{Irregular Memory Jumps:} Sparse matrices often require computations to be performed in smaller blocks to accommodate hardware constraints, such as systolic arrays. Although the data is stored in a format like Compressed Sparse Row (CSR) to maximize locality, accessing it in practice often involves irregular jumps between non-contiguous memory locations. This irregularity arises during blocked computations where matrix A is accessed indirectly via its index. These unpredictable memory jumps introduce significant latency and make it challenging to achieve efficient data throughput.
% \par \textbf{Challenges with Using Systolic Arrays:} The main advantage of sparse matrices is the reduced memory footprint and simplified calculations, which makes restoring them to dense formats undesirable. However, in unilateral sparsity scenarios, the sparsity of neighboring rows in matrix A can vary significantly, making systolic arrays inefficient. In bilateral sparsity, where both matrices A and B are sparse, the unpredictable nature of their memory access patterns creates further challenges, preventing efficient use of systolic arrays and complicating the computational process.
% \par To tackle these challenges associated with sparse matrix memory access, we propose distinct approaches tailored for unilateral and bilateral sparsity.
% \par \textbf{Two-Step Memory Access Strategy for Unilateral Sparsity:} In the case of unilateral sparsity, we introduce a two-step memory access strategy aimed at minimizing the inefficiencies caused by irregular memory access. The first step involves using a buffer array to manage the indices of matrix A. Each row of the buffer is linked to a row of matrix A, which enables locally consecutive access to A and reduces the likelihood of cache misses. After establishing this buffer, we use a row-tiling or column-tiling strategy to generate a memory access mask. This mask determines the target columns for the systolic array during each computation cycle, allowing us to preload values from A efficiently—filling in any missing data with zeros when necessary. Moreover, the mask directly informs the memory access requirements for matrix B, effectively aligning the access of both matrices and thereby enhancing computational efficiency.
% \par \textbf{Vector-Like Approach for Bilateral Sparsity:}  For bilateral sparsity, we adopt a vector-like computational approach to address the issues of reduced data reuse and irregular memory access. Given that the increased sparsity in both matrices A and B significantly reduces the potential for data reuse, systolic arrays are no longer suitable. Instead, we utilize a single column of multipliers, which allows for a simpler and more predictable access pattern. In this setup, both matrices A and B are accessed in a continuous manner—processing one row or one column at a time. After retrieval, the corresponding values are paired and fed into the computation units. This method greatly simplifies the memory access pattern, enhances predictability, and eliminates the complexities associated with indirect indexing.





% 可能还得再压一波。。。
% \subsection{Prefetch Technology}
% % There are many methods to reduce the slowdown caused by cache misses, including latency reduction, tolerance, and hiding techniques. 
% %which be classified into hardware and software solutions. 
% Among various approaches to reduce the slowdown caused by cache misses, prefetching stands as a fundamental optimization technique. In this section, we explore the feasibility of the main prefetching strategies in the NPU architecture.

% Software prefetching operates by inserting explicit prefetch instructions during compilation. While compiler techniques have been developed to automate this process\cite{}, their effectiveness in NPU contexts is limited due to high instruction overhead %(introducing the NPU's control flow redundant) 
% and poor portability across different NPU programming models.
% % \cite Mowry/Ainsworth
% Hardware prefetching techniques can be categorized into pattern-based and execution-based approaches\cite{}. Pattern-based prefetchers, including stride\cite{}, temporal\cite{}, and content prefetchers\cite{}, demonstrate limitations when handling irregular memory patterns and indirect  access.
% % However, these prefetchers are considered struggled to effectively handle complex irregular memory access patterns and indirect memory references.
% Execution-based prefetching primarily encompasses helper threads and Runahead execution. Helper threads leverage multithreading for prefetching; however, the additional thread overhead proves costly within SIMD-oriented NPU architectures. Runahead execution exploits processor idle resources to speculatively execute instructions earlier, thereby hiding memory latency. 
% \mionote{please check this}
% % 这三个感觉说的不是很准确重新写下？每个半句话就行
% Recent advances include CRE\cite{} execute dependence chains in a loop, VR\cite{} executes multiple loop iterations as a single vector instruction, and DVR\cite{} detects loop bounds at runtime.



% =============== 以下是完整版 ===================
% WJ：DOI: 10.1109/ISQED54688.2022.9806203 Figure 5: DRAM access delay with SPM 用缓存并不能有效提高 NPU 性能
% 有哪些隐藏内存的方式，怎么怎么不适合NPU
% There are many methods to reduce the slowdown of cache miss. The most relevant work is categories below. Prefetch technology can be classified into hardware prefetch and software prefetch. Hardware prefetch can be classified into three classes: stride prefetchers, temporal history prefetchers and content prefetchers.

% 有哪些Prefetch的方法，怎么怎么不适合NPU
% HW & SW
% Hardware prefetching is widely deployed to hide memory latency. 针对不同的内存访问模式和应用场景，需要选择合适的预取器类型和策略。
% 软件预取技术需要在程序代码中插入额外的指令，这可能会增加程序的复杂性和开发成本。
% Mowry 开发了在编译器中插入软件预取的算法。Ainsworth 和 Jones 开发了针对间接内存访问的编译器技术。

% Software prefetching achieves suboptimal performance partly due to its high instruction count. Mowry develops algorithms to insert software prefetches in the compiler. Ainsworth and Jones develop compiler techniques for indirect memory accesses. Software prefetching, however, execution requires additional instructions to be inserted into the program code, which can increase program complexity and development costs. 

% 有哪些HW prefetch 的方法，怎么怎么不适合NPU
% 基于历史/模式/内容的预取器  参考VR(zeotero已经做了标记) 
% 步长预取器针对于有步长（stride）规律的数据访问，但在面对复杂或不规则的数据访问模式时可能会遇到性能瓶颈。
% 时间历史预取器存储并重复观察到的模式，但不适合处理大数据工作负载。
% 内容导向的预取机制需要编译器的输入来调节预取任何看起来像是指针的数据，可能会浪费带宽并增加缓存污染。
% Hardware prefetching is widely deployed to hide memory latency. Stride prefetchers work well with stride access patterns but may face bottlenecks with complex or irregular ones. Temporal history prefetchers replicate patterns but struggle with large data workloads. Content prefetchers fetch memory based on knowledge of data structures, such as Event-Triggered Programmable and IMP, which can prefetch memory access patterns for specific workloads. Stride and temporal history prefetchers struggle with complex data access patterns, possibly missing NPU prefetching needs, while content-directed prefetching requires compiler input to adjust for pointer-like data, which can waste bandwidth and increase cache pollution.


% helper thread
% For a helper thread to be effective, it needs to execute ahead of the mainthread. Speculative Precomputation 许推测性的辅助线程生成它们自己的推测性辅助线程，以处理复杂的链式依赖。参考VR DVR(zeotero已经做了标记)  DeSC 将整个处理器分割成访问辅助线程和执行辅助线程 Ganusov 和 Burtscher  通过辅助线程模拟微架构预取器的方法  helper thread需要单独的线程或执行单元，并且可能需要程序员或编译器的支持
% Helper threads execute tasks proactively to enhance the performance of the main thread by completing work ahead of time. Speculative Precomputation enables helper threads to create additional threads for complex dependencies. Ganusov and Burtscher emulate hardware prefetchers on helper threads. DeSC decouples address computation and load-value usage by splitting the processor into access helper threads and execution helper threads. In contrast, XX requires no separate thread, execution units, or programmer/compiler support.

%其他 (IMP 标记)
%乱序执行 (OoO): 通过执行独立的指令来隐藏内存访问的延迟。然而，OoO 执行无法隐藏所有延迟，并且对稀疏数据结构的优化效果有限。
%同时多线程 (SMT): 允许多个线程共享硬件流水线，当一个线程因内存访问而停滞时，其他线程可以继续执行。然而，SMT 需要额外的硬件开销，并且需要软件使用额外的线程。
% Out-of-Order execution (OoO) hides latency by executing independent instructions while waiting for a memory access. OoO, however, execution cannot hide all latencies and has limited optimization effects on sparse data structures. Simultaneous multithreading (SMT) Allows multiple threads to share the hardware pipeline, so that when one thread stalls due to memory access, other threads can continue execution. SMT, however, requires additional hardware overhead and software to use additional threads.


% runahead: 利用处理器（NPU）的闲时资源
% NPU不能停，对传统runahead做了改进
% Runahead 执行是一种在遇到内存访问延迟时，预测并执行未来指令流以隐藏延迟的技术。
% Continuous Runahead 利用靠近内存控制器的连续执行依赖链来提升内存密集型工作负载性能。
% (Decoupled) Vector Runahead 不依赖于处理器前端中的线性猜测来找到未来的工作。相反，未来的循环迭代是独立地向量化，以隐藏延迟。
% SVR 利用现有指令流，生成多个独立的标量指令副本，SVR 通过重看内存访问来隐藏内存访问延迟。
% Runahead execution predicts and runs future instructions to mask memory access delays. Continuous Runahead improves memory-intensive workload performance by executing dependency chains near memory controllers. (Decoupled) Vector Runahead doesn't depend on linear predictions but vectorizes future loop iterations independently to hide delays. Scalar Vector Runahead predicts vector instruction execution using scalar results and accesses memory early. XX improved on traditional runahead by 




