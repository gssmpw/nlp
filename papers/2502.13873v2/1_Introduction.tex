\section{Introduction}
\vspace{-3pt}
Emerging Deep Neural Networks (DNNs), particularly Large Language Models (LLMs), often scale to hundreds of billions of parameters~\cite{1llms, yang2024efficient}, increasingly consuming more storage, memory bandwidth, and computational resources.
Fortunately, these workloads are typically over-parameterised~\cite{han2015learning}, where up to 90\% of parameters in prevalent models can be pruned while maintaining comparable performance~\cite{3sparsity}.
This redundancy presents an opportunity to leverage sparsity to reduce such intensive resource demands. 

Theoretically, more fine-grained sparsity patterns yield higher acceleration by skipping more zero-valued elements.
However, as shown in Fig.~\ref{fig:fig1}~\circled{a}, these patterns introduce unstructured characteristics when directly skipping zero values randomises vector access order, leading to irregular memory access patterns. These irregular patterns frequently result in cache misses, which not only waste main memory bandwidth but also degrade NPU performance.
% \zhenote{Explicitly give numbers regarding the impacts.} 
% As depicted in Fig.\ref{fig:fig1}(b), taking \textcolor{red}{XX} as an example, despite a \textcolor{red}{XX}x reduction in memory access volume, the frequent out-of-bounds SRAM accesses necessitate substantial main memory interactions. 
% This results in the actual memory access benefit being only \textcolor{red}{XX}x, significantly below theoretical expectations. Cache misses impose a barrier to ideal algorithmic performance.
Taking sparse KVCache in LLM workloads~\cite{21doublesparsity} as an example, Fig.~\ref{fig:fig1}~\circled{b} illustrates that despite a 16x parameter reduction, frequent cache misses lead to extensive main memory interactions. 
This results in only a 5x actual speedup, substantially below theoretical expectations. Cache misses thus emerge as a key barrier to sparse workloads.
% 预期多少，然后实际收益只有xxxx


% In light of this, recent work is increasingly exploring the methodology to reduce the side effects of fine-grained sparsity. 

% So recent work are increasingly exploring the implementation and deployment of more fine-grained sparsity, reducing the impact of irregular memory access.\zhenote{Vague}


% \deannote{I fell this version may better.}
% \Guan{hahaha, instead, i think the first one is clearer.}

% \Guan{Perhaps we can use subheadings to make the logic clearer? For example, adding 'Challenges' before the second paragraph and 'Existing works' in the following section.}\mionote{u are right, subheadings are needed.}


% Therefore, there remains significant potential in leveraging sparsity to accelerate execution, provided that the associated cache misses are addressed.
% Recent work is increasingly exploring the methodology to reduce the side effects of fine-grained sparsity. 
% Prior work on hardware-efficient sparse computation implementation has explored various optimization strategies. 
% Therefore, there remains significant potential in leveraging sparsity for acceleration.

\noindent \textbf{Existing Work.}
Recent research has focused on mitigating the side effects of fine-grained sparsity.
Farshchi et al.\cite{farshchi2019integrating} propose a bitmask sparse data format in NVDLA, while %Chen
Eyeriss \cite{chen2016eyeriss} design a run-length encoding for non-zero elements. 
% However, these solutions are often limited to specific hardware or algorithmic implementations. 
Albericio et al.\cite{albericio2016cnvlutin} introduce additional mapping algorithms, regularising memory access patterns through label cache mechanisms. 
Meanwhile, Liu\cite{liu202316} et al. explored sparse domain compute units, developing a butterfly-based architecture for efficient sparse data structure processing.
% \mionote{I think et al. is just for people?}
Whilst most of these approaches attempt to regularise memory access patterns to avoid cache misses, they often introduce substantial area and control logic overhead that outweighs their benefits; therefore, the practical applicability of these algorithm-specific are significantly restricted.

% \deannote{what is fragmented solution means?} \mionote{algorithms-bound? lake of generalizability? algorithms modification required?}
% \zhenote{Sounds like a recipe} 
% 加个double sparse量化


%SCNN\cite{6SCNN} and Gamma\cite{5Gamma}

% This is primarily because current hardware, such as GPUs and neural processing units (NPUs), struggles to handle the frequent cache misses caused by irregular memory access patterns inherent in both unstructured sparsity and structured sparsity. Unstructured sparsity operates at fine granularity without predefined patterns and offers higher accuracy and compression ratios but leads to irregular memory access patterns. While structured sparsity uses coarse-grained regrouping and is supported by commercial GPUs (e.g., Nvidia XXX's 4:2 sparsity\cite{4structured}), it still exhibits discontinuous access patterns (Fig. \ref{fig:fig1}(a)).

% 为什么我们做prefetch on NPU
% sparse算法很常见，但是并没得到广泛支持，原因是要结构化支持。现在我们通过承受大量cache miss实现支持。


% Fortunately, many of these parameters are highly redundant, providing an opportunity to leverage sparsity to alleviate these resource constraints\cite{}. 
% Specifically, by identifying and setting low-weight blocks (structured sparsity) or individual elements (unstructured sparsity) to zero, the computational and storage pressure can be greatly reduced. 
% This approach takes advantage of the fact that not all parameters contribute equally to the model's overall performance.

% As evidenced in previous studies\cite{}, up to 90\% of parameters in certain models can be pruned without significantly affecting their overall performance. This reveals a clear potential for reducing workload complexity and adapting these models for more efficient edge deployment.




% Fortunately, these workloads are typically over-parameterized\cite{}, as evidenced in \cite{}, up to 90\% of parameters in many models can be pruned while maintaining comparable performance\cite{3sparsity}.
% This redundancy presents an opportunity to leverage sparsity to reduce such intensive resource demands. 
% However, the implementation of sparsity-aware systems faces intricate challenges due to their hardware inefficiency. 

% As shown in Fig. 1, non-structured pruning is fine-grained and accurate but not hardware-friendly; structured pruning is coarse-grained and more efficient but with higher accuracy loss and structured overhead.


% Prior work on hardware-efficient sparse computations has explored various optimization strategies. 
% To enhance data locality, researchers have proposed several compression formats for non-zero elements, such as SCNN\cite{6SCNN} and Gamma\cite{5Gamma}. 
% By introducing structured pruning patterns, approaches like H2O\cite{} and XXX\cite{} reduce computational workload through preprocessing.
% Additionally, several architectures have incorporated specialized processing engines, as demonstrated in Sextans\cite{7sextans}.
% While these approaches aim to regularize memory access patterns, they often introduce substantial overheads and fragmented solutions, particularly for unstructured sparsity, limiting their practical applicability. 


% \par In this paper, we tackle the sparsity challenge from a different perspective. 
% Instead of regularizing irregular memory access patterns, our approach embraces this inherent characteristic of sparse computations. We leverage prefetching techniques directly to mitigate cache misses.
% We propose an NPU vector runahead (NVR) mechanism, drawing inspiration from the runahead execution model\cite{8runahead}.
% Through the integration of a lightweight prefetching unit between the CPU and NPU, NVR achieves an order-of-magnitude reduction in cache misses by speculatively executing idle NPU units. 
% NVR not only delivers substantial acceleration without any algorithmic support, but also remains orthogonal to prior optimization methods. 

% Our contributions can be summarized as follows:
% \vspace{-3pt}
% \begin{itemize}
% \item We conduct comprehensive modelling for sparse DNN workloads and propose NVR, to the best of our knowledge, which is the first system-level prefetching mechanism on NPUs.
% \item We design the architecture of NVR, achieving XXX\% speedup over the SOTA prefetch mechanism on the CPU. 
% \item We explore integrating a tiny cache in NPUs, adding a 16KB cache has the effect of expanding the original 256KB L2 cache by Xx in sparse scenarios. 
% \end{itemize}


% \par It's worth noting that NVR is not limited to sparse tasks. , 
% It can be applied to any operations exhibiting unstructured characteristics, resulting in irregular memory access.

% \mionote{==========================================}
% Deep Neural Networks (DNNs) have shown remarkable effectiveness in various domains. 
% While scaling up model size leads to improved performance, as evidenced by the emergence of Large Language Models (LLMs) with hundreds of billions of parameters\cite{1llms}, posing significant deployment challenges.
% DNNs are typically over-parameterized, containing a significant proportion of zero-valued elements, which is known as sparsity\cite{2dnn}.
% Several DNN models exhibit remarkable redundancy, retaining over 99\% accuracy even when up to 95\% of parameters are pruned\cite{3sparsity}.
% However, the implementation of sparsity-aware systems faces intricate challenges due to their hardware inefficiency. 
% \zhenote{Why do we go around and around in the first paragraph?}
% \mionote{move "DNNs are typically over-parameterized ……" to a separate paragraph?}



% By bypassing the storage, I/O, and computations associated with zero-valued elements, it can substantially reduce latency and energy. Furthermore, sparsity representation can significantly diminish the hardware area.
% 讲 DNN 稀疏 llms
% sparse DNN 
% 针对稀疏数据格式的预取


% \par DNNs are typically over-parameterized, containing a significant proportion of zero-valued elements, which is known as sparsity\cite{}. The traditional dense computations may result in load imbalances due to varying sparsity patterns. Consequently, compressing techniques have been proposed for efficient training and inference of DNNs. Sparsification, a compressing technique that prunes less important values to zeros in the weights or activations of DNNs, has been proposed to enable more efficient training and inference processes \cite{}.





% 1. 稀疏workload为什么难hardware inefficiency
% 2. 前人如何解决
% \par Sparse algorithms can be categorized into structured and unstructured methods based on granularity. Unstructured sparsity operates at fine granularity without predefined patterns, while commercial GPUs (e.g., Nvidia XXX's 4:2 sparsity\cite{4structured}) typically only support structured sparsity. Although unstructured sparsity offers higher accuracy and compression ratios, it leads to irregular memory access patterns. Even structured sparsity, despite using coarse-grained regrouping, exhibits jumpy access patterns (Fig. \ref{fig:fig1}(a)). These irregular accesses cause frequent cache misses, severely impacting both CPU and NPU latency. As shown in Fig. \ref{fig:fig1}(b), such cache misses and resulting NPU stalls can make sparse algorithms slower than their dense counterparts.




% \par Sparse algorithms can be categorized into structured and unstructured methods based on granularity. Unstructured sparsity operates at fine granularity without predefined patterns and offers higher accuracy and compression ratios but leads to irregular memory access patterns. While structured sparsity uses coarse-grained regrouping and is supported by commercial GPUs (e.g., Nvidia XXX's 4:2 sparsity\cite{4structured}),
% % Accelerating Transformer Pre-training with 2:4 Sparsity
% it still exhibits discontinuous access patterns (Fig. \ref{fig:fig1}(a)). These irregular accesses cause frequent cache misses, severely impacting both CPU and NPU latency. As shown in Fig. \ref{fig:fig1}(b), such cache misses and stalls NPU can make sparse algorithms slower than their dense counterparts.
%再具个double sparsity的例子

% The sparse algorithm can be broadly categorized into structured sparsity and unstructured sparsity, based on the granularity of sparsity. 
% Unstructured sparsity is straightforward, where sparsity occurs at fine granularity without any predefined pattern. Consequently, most commercial GPUs, such as Nvidia XXX’s 4:2 sparsity\cite{}, support only structured sparsity. 
% Unstructured sparsity offers greater flexibility in algorithm design with higher accuracy and compression ratios but leads to inefficient, irregular memory access. Although structural sparsity uses coarse-grained regroup methods to reduce irregularities, actual accesses are still jumpy, as seen in Fig. \ref{fig:fig1}(a). Irregular memory access leads to heavy and frequent cache misses, which is destructive for the latency of both CPU and NPU. 
% Performance degradation due to numerous cache misses is the core factor hindering sparse algorithms.
% Fig. \ref{fig:fig1}(b) illustrates how cache misses and resultant NPU stalls can cause sparse algorithms to become more time-consuming than the original.


% \par Extensive research has been conducted on optimizing sparse computations for enhanced hardware efficiency. \mionote{WIP}
% 自定义稀疏数据格式
% 修改算法
% 额外的硬件存储单元
% 高消耗的reorder调度
% The methodological approaches of these works can be summarized as focusing on making memory access patterns as regular as possible through algorithms, hardware, data formats, etc. 
% Unfortunately, these optimization strategies frequently incur substantial overheads and non-portable fragmentation, potentially negating the benefits of sparsity. Especially for unstructured sparsity, it is often necessary to customize the algorithm with a complete set of optimizations applicable only to this algorithm.


% Previous research has approached the problem of hardware-efficient sparse computations through both software and hardware optimization strategies.

% On the software side, various compression formats have been developed to enhance data locality and minimize memory overhead by focusing on the non-zero elements. Examples include SCNN\cite{6SCNN} and Gamma\cite{5Gamma}, which utilize specific data structures to compress sparse matrices, thereby improving data locality and reducing memory access costs. Other approaches, such as structured pruning techniques used in models like H2O\cite{} and similar methods, aim to reduce computational workload by removing less important parameters during preprocessing. 

% On the hardware side, specialized processing engines have been introduced to improve the efficiency of handling sparse computations. For instance, Sextans\cite{7sextans} features dedicated hardware components specifically designed for sparse workloads, aiming to reduce the latency caused by irregular memory accesses. Although these specialized hardware units can alleviate some of the issues inherent in sparsity, they come with increased design complexity, making it challenging to apply these solutions across different hardware platforms. Moreover, while these approaches strive to regularize memory access, they still face difficulties in achieving generalizability, particularly when dealing with unstructured sparsity.

% Overall, these efforts in both software and hardware have contributed to mitigating some of the challenges associated with sparse computations. However, they often come with trade-offs, such as increased complexity or specialized implementations, which limit their broader applicability. 



% \par Prior work on hardware-efficient sparse computations has explored various optimization strategies. To enhance data locality, researchers have proposed several compression formats for non-zero elements, such as SCNN\cite{6SCNN} and Gamma\cite{5Gamma}. By introducing structured pruning patterns, approaches like H2O\cite{} and XXX\cite{} reduce computational workload through preprocessing. Additionally, several architectures have incorporated specialized processing engines, as demonstrated in Sextans\cite{7sextans}, though at the cost of increased design complexity.
% While these approaches aim to regularize memory access patterns, they often introduce substantial overheads and fragmented solutions % result in specialized solutions, 
% \mionote{Algorithm-bound?}
% particularly for unstructured sparsity, limiting their practical applicability. 


%Extensive research has been conducted on optimizing sparse computations for enhanced hardware efficiency. These efforts generally focus on a few key methods:
% Custom data formats, such as those in SCNN and Gamma\cite{}, are used to compress non-zero elements and improve data locality, enhancing memory efficiency for specific hardware setups.
% Algorithm Modifications like structured pruning, seen in SCNN and Sextans\cite{}, eliminate redundant computations to reduce the workload but require significant preprocessing.
% Specialized hardware regroup units, like sextans'\cite{} dedicated processing engines, aim to reduce latency but increase design complexity and reduce generality.
% The methodological approaches of these works can be summarized as focusing on making memory access patterns as regular as possible through algorithms, hardware, data formats, etc. 
% Unfortunately, these optimization strategies frequently incur substantial overheads and result in non-portable designs, especially for unstructured sparsity, where customization is often necessary but limits the broad applicability of these solutions.


% 我们提出换一个思路解决
\noindent \textbf{Contributions:} 
Different from all previous works, we tackle the sparsity challenge from a different perspective. 
% We embrace irregular access patterns and leverage runahead prefetching technology directly to mitigate cache misses by pre-executing idle resources on the NPU ahead.
Instead of regularising irregular memory access patterns, our approach embraces the inherent characteristics of sparse computations. We leverage prefetching techniques directly to mitigate the impact of cache misses, rather than attempting to eliminate the irregularity itself, which incurs high overhead and lacks generality. 
% Our key insight is that while irregular memory access patterns are inherent to sparse computations, their performance impact can be minimized through effective prefetching techniques. 
We propose a vector runahead mechanism for NPU (NVR), drawing inspiration from the runahead execution model\cite{8runahead, 8VR}.
% An order-of-magnitude level of cache miss reduction in algorithmic senselessness can be achieved by simply inserting a little prefetcher between the CPU and the NPU.
NVR integrates a lightweight prefetcher decoupled with CPU and NPU, achieving significant cache miss reduction through speculative execution utilising idle NPU units.
%\mionote{if "decoupled" and "utilizing" conflict?}
NVR is a micro-architectural solution requiring no compiler or algorithmic support, remains orthogonal to prior optimisation methods. 
%of optimizing memory access patterns.
% By adopting NVR, we eliminate the need for complex data formats, algorithm modifications, or extensive hardware changes. 
% This approach enables sparse storage, I/O, and computation acceleration without requiring algorithm-specific optimizations.
%This paper presents an approach to directly tackle the fundamental issue of cache misses in sparse computing. 
%Drawing inspiration from the Runahead execution model, we introduce Neural Vector Runahead (NVR). 

\begin{figure}[t]
\vspace{-20pt}
% \centering
\hspace{-10pt}
% \subfigure{\includegraphics[width=0.22\textwidth]{fig1_2.pdf}}
% \subfigure{\includegraphics[width=0.22\textwidth]{fig1_2.pdf}}
\includegraphics[width=\linewidth]{fig1.pdf}
\vspace{-17pt}
\caption{Sparsity is a widely adopted approach for speedup and energy efficiency through skipping zero in processing, which introduces substantial irregular memory accesses.}
\label{fig:fig1}
\vspace{-17pt}
\end{figure}

% Our contributions can be summarized as follows:
% \vspace{-3pt}
% \begin{itemize}
% \item We conduct comprehensive modelling for sparse DNN workloads and propose NVR, to the best of our knowledge, which is the first system-level prefetching mechanism on NPUs.
% \item We design the architecture of NVR, achieving XXX\% speedup over the SOTA prefetch mechanism on the CPU. 
% \item We explore integrating a tiny cache in NPUs, adding a 16KB cache has the effect of expanding the original 256KB L2 cache by Xx in sparse scenarios. 
% \end{itemize}


To this end, we implement NVR, to the best of our knowledge, the first system-level prefetching mechanism specifically designed for NPUs. 
Our architecture reduces L2 cache misses by 90\% compared to SOTA prefetching techniques in general-purpose processors through the effective prediction of irregular access patterns, with under 5\% area overhead. %, while incurring less than 5\% area overhead.
Furthermore, NVR reduces off-chip memory accesses by 75\% during NPU execution. When combined with NSB (a small cache structure in NPUs), there is an additional 80\% off-chip memory access reduction benefit. 
Our system-level evaluation of LLM workloads further validates the effectiveness of NVR, demonstrating an average 50\% throughput improvement in IO-bound scenarios.


% 只是在decode阶段



% \begin{figure}[t]
% \centering
% \includegraphics[height=0.1\textwidth]{fig2.png}
% \vspace{-5pt}
% \caption{Structured Sparsity and unstructured Sparsity}
% \label{fig:fig2}
% \end{figure}

\begin{figure*}[t]
\vspace{-14pt}
\hspace{-13pt}
% \centering
\includegraphics[width=1.045\linewidth]{fig2.pdf}
\vspace{-23pt}
\caption{Sparse Matrix Multiplication can be categorised into one-side-sparsity and two-sides-sparsity patterns, with higher sparsity offering greater speedup potential at the cost of more challenging access patterns. Here, $\texttt{\textbf{spatial\_for}}$ denotes parallel operation on the NPU, while $\texttt{\textbf{IA}}$ (input activation), $\texttt{\textbf{W}}$ (weight), and $\texttt{\textbf{OA}}$ (output activation) represent the input variables, weight parameters, and output results, respectively.}
\vspace{-13pt}
\label{fig:fig2}
\end{figure*}

\par It's worth noting that NVR is not limited to sparse tasks. % While we focus on sparsity as a demonstration, 
% This technique 
It can be applied to any operations exhibiting unstructured characteristics, resulting in irregular memory accesses.
% 如动态的网络计算图



% requires overlapping to increase memory level parallelism (MLP) on NPU.
% This technique can be applied to any operation, effectively overlapping computation with high-latency memory accesses. 

