\section{Methodology}

In this section, we introduce our proposed \textbf{\textit{RetrievalPRM}} framework in detail.


\begin{figure*}[t]
  \centering
  \vspace{-30pt}
  \includegraphics[width=1.0\textwidth]{imgs/RetrievalPRM_Framework.pdf}
  \vspace{-20pt}
  \caption{The model structure of our proposed RetrievalPRM framework and its difference with traditional PRM. We design a Two-stage Retrieval Module to retrieve reference questions and steps in each stage. }
  \label{fig: Framework}
  \vspace{-10pt}
\end{figure*}


\subsection{Overview of RetrievalPRM}

The RetrievalPRM is developed to address the problem of out-of-distribution (OOD) scenarios in mathematical problem-solving, specifically focusing on both question OOD and step OOD. According to Figure~\ref{fig: Framework}, traditional PRM models are constrained by predefined solution steps and are unable to handle unseen questions or steps effectively, especially when the problem context shifts or the solution process deviates from previously seen examples. RetrievalPRM overcomes this challenge by incorporating a Two-stage Retrieval-enhanced Mechanism that dynamically fetches relevant questions and steps from a large pool of questions and their solutions. These retrieved questions and steps serve as a kind of warm-up for PRM, acting as example problems for reference. They not only help stimulate PRM's potential by warming up but also allow the system to handle more difficult problems by identifying similarities.

% By integrating retrieval mechanisms, the model can access a wide variety of solutions and apply them to both familiar and novel questions, effectively dealing with the distributional shift in problem types (question OOD) and solution steps (step OOD). This makes RetrievalPRM particularly effective in real-world scenarios where problem contexts are diverse and rapidly evolving.

\subsection{Two-stage Retrieval-enhanced Mechanism}
The core of RetrievalPRM is the Two-stage Retrieval-enhanced Mechanism, which consists of two key phases: Question-level Retrieval and Step-level Retrieval.

\subsubsection{Question-level Retrieval}
\label{question-level retrieval}
The first stage of retrieval tackles the question OOD issue. As is shown in Figure~\ref{fig: Framework}, the retrieval pool is the question database $\mathbb{D}_q = \{q_i\}_{i=1}^N$. During retrieval process, we treat:
\begin{itemize}[leftmargin=10pt]
    \item Query: the target question $q_t$.
    \item Key: all $q_i$ in the retrieval pool.
    \item Value: all the $(q_i,\mathbf{s}_i)$ pair in the retrieval pool.
\end{itemize}
We calculate their similarities $<q_i,q_t>$ to match the most similar n questions. Specifically, all questions will first pass through a Sentence-BERT model to encode questions and obtain their semantic representations.
\begin{equation}
    \{e_{q_i}\}_{i=1}^N = \text{SentenceBERT}(\{q_i\}_{i=1}^N)
\end{equation}
where $e_{q_i} \in \mathbb{R}^{D}$ is the embedding vector of the question $q_i$.

And then all the embeddings undergo Principle Component Analysis (PCA)~\cite{kurita2021principal} for dimensionality reduction to extract the most important dimensions. 
\begin{equation}
    \{e'_{q_i}\}_{i=1}^N = \text{PCA}( \{e_{q_i}\}_{i=1}^N )
\end{equation}
where $e'_{q_i} \in \mathbb{R}^d$ is the embedding after dimension reduction.

Finally, we compute the cosine similarity between the target question and the entire question pool, selecting the top-\textit{k} most similar questions and inputting them into the text.
\begin{equation}
    \begin{aligned}
    \langle q_i,q_t\rangle &= \frac{e'_{q_t} \cdot  e'_{q_i}}{|e'_{q_t}|\cdot|e'_{q_i}|}.\\
    \end{aligned}
\end{equation}

Now we sort the vector $\{\langle q_i,q_t\rangle\}_{i=1}^N$ of similarity and choose top-\textit{k} $(q_i,\mathbf{s}_i)$ pairs as reference questions $q_r$ and put them in RetrievalPRM's input together with the target question. Furthermore, we store all the solutions $\{\mathbf{s}_i\}_{i=1}^{m}$ of top-\textit{m} ($m>k$) questions in a new database to conduct a further step-level retrieval. 

\subsubsection{Step-level Retrieval}

We place step-level retrieval in the second stage of the two-stage retrieval process, rather than as a separate module, for two key reasons:

Firstly, for a solution to be meaningful, both the question and the steps must be similar. For example, two different types of questions might both use the letter "p" to represent an unknown variable, but in some problems, "p" represents a prime number, while in others, it represents probability. This results in steps that may appear similar but have entirely different meanings, rendering the retrieved steps potentially unhelpful.

Secondly, since there are many possible solutions to a question, this leads to a large number of steps. If the majority of these steps are irrelevant, the time spent calculating similarities becomes inefficient. By placing step-level retrieval in the second stage, we can save both time and computational resources.

Therefore, after retrieving the top-\textit{m} most similar questions, we inject all their solutions into a new steps database $\mathbb{D}_s$. Then, we use the target step as the query to retrieve reference steps from this new database. The similarity for retrieval is still calculated using Sentence-BERT, PCA, and cosine similarity, as mentioned in ~\ref{question-level retrieval}.


\subsection{Retrieval-based System Prompt}
In RetrievalPRM,  The system prompt serves as the instruction set for the model, framing the problem and directing it to evaluate each step of the solution.
Besides the traditional system prompt for PRM, the Retrieval-based System Prompt (RetSP) is extended with additional instructions, as shown in the red sentence in Figure~\ref{fig: Framework}, which encourages the model to leverage knowledge from reference questions. For example, we inform PRM that step labels "+" and "-" represent correct and incorrect steps, respectively. At the same time, to avoid noise, we specify that if the reference question or step contains no relevant or helpful information, it should not be considered. These retrieval-based system prompts give PRM a more flexible thinking process, enabling it to actively decide whether to use retrieval-based knowledge.

% For example, the prompt might ask the model to judge if the steps in the target solution are correct or not, based on the reference steps. If the model determines that a step is incorrect, the system prompts it to adjust or correct the solution based on the retrieved references. This retrieval-driven approach not only improves the modelâ€™s performance in judgment tasks but also allows the model to generalize better by learning from a diverse set of reference solutions, rather than being limited to pre-programmed rules or static examples.

% This methodology outlines the essential components and workflow of the RetrievalPRM model. The use of a retrieval-based system enhances its ability to solve mathematical problems by dynamically sourcing relevant references to assess solution correctness.
We define reference questions of $q_i$ as $\mathbf{q}_i^{r}$ and reference steps as $\mathbf{s}_i^r$. The whole input $\mathbf{x}_i^j$ of predicting the $j_{th}$ step of $q_i$ in RetrievalPRM can be formulated as:
\begin{equation}
\label{input template}
    \begin{aligned}
    \mathbf{x}^j_i = (RetSP,\mathbf{q}^r_i,&q_i,
    s^1_i,\ldots,s^{j-1}_i,\mathbf{s}^r_i,s^j_i,y^j_i),\\
    \hat{y}^j_i = & \text{PRM}(\mathbf{x}^j_i)
    \end{aligned}
\end{equation}
where $s^j_i$ is the $j_{th}$ step of solution $\mathbf{s}_i$.

According to the input template above, it is worth noting that when predicting step n, we assume that steps 1 through n-1 are correct ~\cite{luo2024improvemathematicalreasoninglanguage,zheng2024processbench}. At this point, the most important task for PRM is to predict step n, so PRM can only access the reference steps for step n and cannot see the reference steps for steps $1\sim n-1$.

% \subsection{Reference Step Attention Mask}
% According to the input template above, it is worth noting that when predicting step n, we assume that steps 1 through n-1 are correct ~\cite{luo2024improvemathematicalreasoninglanguage,zheng2024processbench}. At this point, the most important task for PRM is to predict step n, so PRM can only access the reference steps for step n and cannot see the reference steps for steps $1\sim n-1$, which we define as condition $c$.  Therefore, during training, since one solution has multiple steps, there are two ways in sample construction: 

% (1) one label in one sample: We divide one solution trajectory into multiple samples. A training example sample looks like Equation~\ref{input template}. This directly satisfies condition $c$. However, this construction method will cause sample number explode many times since when a solution $\mathbf{s}_i$ has $n_i$ step labels, it will construct $n_i$ training samples for this solution. 

% (2) Multiple label in one sample: We construct only one sample for one solution to avoid training time explosion on the approach above:
% \begin{equation}
%     \begin{aligned}
%         input = &(RetSP, ref\_q,q_t, ref\_s^1, s^1, y^1, \\ 
%         &ref\_s^2, s^2, y^2, \ldots, ref\_s^n, s^n, y^n)\\
%     \end{aligned}
% \end{equation}

% However, this doesn't satisfy condition $c$ since predicting $y^n$ will be affected by $ref\_s^1 \sim ref\_s^{n-1}$.

% So we manually construct an Reference Step Attention Mask to ensure that when predicting $y^n$, PRM can only access its own reference. Specifically, when predicting $y^j$, the attention mask for $ref\_s^k~(~\forall k<j~)$ will be dynamically set to 0 to make them invisible.


