\section{Related Works}
\subsection{Process Reward Models}
% Process reward models (PRMs) have been shown to have advancement over traditional outcome reward models (ORMs)\cite{gsm8k} in training modelsâ€™ process-level reasoning accuracy and improving their long-process reasoning abilities~\cite{lightman2023let,uesato2022solvingmathwordproblems}. More and more PRMs have been proposed for use in process-level RLHF~\cite{wang2024math,qin2024o1replicationjourneystrategic,xia2025evaluatingmathematicalreasoningaccuracy,skyworkopeno12024}. \cite{lightman2023let} released a large amount of labeled data at the human-annotated process level, providing great research opportunities for multi-step reasoning. \cite{wang2024math} introduces a self-supervised automatic data generation and PRM training pipeline that can automatically generate the process-level label. \cite{xia2025evaluatingmathematicalreasoningaccuracy} utilize PRM as an auto evaluator to assess the multistep reasoning accuracy of LMs. Due to the emergence of massive work on PRM training and data curating, many PRMs~\cite{skyworkopeno12024,xiong2024rlhflowmath,sun2024easytohardgeneralizationscalablealignment,gao2024llmcriticshelpcatch,wang2024openropensourceframework} have been proposed. In addition, some works focus on using natural language feedback from LLM as a reward~\cite{mcaleese2024llmcriticshelpcatch,zhang2024generativeverifiersrewardmodeling,gao2024llmcriticshelpcatch}, which are called critic models.
Process reward models (PRMs) have demonstrated significant advantages over traditional outcome reward models (ORMs)~\cite{gsm8k} in enhancing process-level reasoning accuracy and improving long-process reasoning abilities in model training~\cite{lightman2023let,uesato2022solvingmathwordproblems}. A growing number of PRMs have been proposed for application in process-level reinforcement learning with human feedback (RLHF)~\cite{wang2024math,qin2024o1replicationjourneystrategic,xia2025evaluatingmathematicalreasoningaccuracy,skyworkopeno12024}. For instance, \citet{lightman2023let} made a substantial contribution by releasing a large set of human-annotated data at the process level, opening up new research opportunities for multi-step reasoning. 

Additionally, \citet{wang2024math} introduces an automatic, self-supervised pipeline for generating process-level labels and training PRMs, enabling efficient data generation. \citet{xia2025evaluatingmathematicalreasoningaccuracy} employs PRMs as automatic evaluators to assess the accuracy of multi-step reasoning in language models (LMs). With the surge in PRM-focused research and data curation, numerous PRMs~\cite{skyworkopeno12024,xiong2024rlhflowmath,sun2024easytohardgeneralizationscalablealignment,gao2024llmcriticshelpcatch,wang2024openropensourceframework} have been proposed. Additionally, several studies focus on leveraging natural language feedback from large language models (LLMs) as rewards, which are termed critic models~\cite{mcaleese2024llmcriticshelpcatch,zhang2024generativeverifiersrewardmodeling,gao2024llmcriticshelpcatch}.

However, most existing PRMs trained on math datasets such as GSM8K and MATH inevitably encounter Out-of-distribution issues, which can be divided into two categories: \textbf{question OOD}, where PRMs trained on simpler or medium-difficulty datasets lack understanding of questions from more challenging datasets, and \textbf{step OOD}, where different base models and model sizes in LLMs lead to different step distributions for the same question. This is reflected in differences in chain length, problem-solving approaches, and methods. To address these issues, we propose the RetrievalPRM framework to tackle the OOD problems encountered in the current PRM field, achieving promising results.


\subsection{Retrieval-Augmented Generation}

Retrieval-augmented generation (RAG) enhances language models by dynamically integrating external knowledge, pioneered by~\cite{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp} through their joint retrieval-generation architecture. Subsequent advances refined this paradigm. \citet{guu2020realmretrievalaugmentedlanguagemodel} introduced REALM to co-train retrieval and generation modules via masked language modeling, while~\citet{izacard2021leveragingpassageretrievalgenerative} proposed Fusion-in-Decoder (FiD) to process multi-document contexts efficiently. Research further optimized retrieval precision through dense passage embeddings~\cite{karpukhin2020densepassageretrievalopendomain} and scaled retrieval to web-level corpora~\cite{borgeaud2022improvinglanguagemodelsretrieving}. 