\section{Introduction}

% In recent years, Large Language Models (LLMs) have made remarkable advances in mathematical reasoning ~\cite{openai2023gpt,dubey2024llama,zhu2024deepseek,shao2024deepseekmath,yang2024qwen2}, yet they can make mistakes, such as miscalculations or logical errors, leading to wrong conclusions. Moreover, even when achieving correct final answers, these powerful models can still regularly make up plausible reasoning steps, where the final answers build upon flawed calculations or derivations, which undermine the reliability and trustworthiness of LLMs’ reasoning processes. To address these challenges, Process Reward Models (PRMs)~\cite{lightman2023let,wang2024math}, have emerged as a critical component in the realm of automated reasoning. PRMs extend beyond traditional outcome-based evaluation by explicitly assessing the coherence and logical validity of intermediate reasoning steps~\cite{gsm8k}. This paradigm aligns with human pedagogical practices, where educators emphasize not only final answers but also the quality of problem-solving strategies.

While large language models (LLMs) have advanced mathematical reasoning~\cite{openai2023gpt,dubey2024llama,zhu2024deepseek,shao2024deepseekmath,yang2024qwen2}, they remain prone to critical flaws: explicit errors (e.g., miscalculations, logical inconsistencies) and implicit risks where correct answers mask flawed intermediate steps. Even when final results are accurate, LLMs often generate plausible-but-incorrect reasoning chains, eroding trust in their problem-solving processes \cite{lightman2023let}. To address this, Process Reward Models (PRMs) \cite{lightman2023let,wang2024math} have been developed to rigorously evaluate the logical validity of intermediate steps \cite{gsm8k}, mirroring human pedagogical practices that prioritize reasoning quality over answer correctness.


\begin{figure}[t]
  \centering
  \vspace{-10pt}
  \includegraphics[width=0.48\textwidth]{imgs/distribution_difference.pdf}
  \vspace{-20pt}
  \caption{The distribution differences across three datasets: GSM8K, MATH and Olympiad. We use sentence-bert to encode these questions and perform t-sne visualization.}
  \label{fig: question OOD}
  \vspace{-20pt}
\end{figure}


Existing works~\cite{wang2024openropensourceframework, skyworkopeno12024,zheng2024processbench} frame PRM as a binary classification problem. They train PRM on open-source base LLMs such as Qwen~\cite{yang2024qwen2} or Llama~\cite{dubey2024llama} using human-annotated dataset ~\cite{lightman2023let} or automated process supervision method ~\cite{wang2024math,luo2024improvemathematicalreasoninglanguage,qin2024o1replicationjourneystrategic}. Although these approaches show great performance and empirical success, they still face kinds of out-of-distribution challenges. 
We believe the out-of-distribution (OOD) problem can be viewed from the following perspectives:  

\begin{figure*}[t]
  \centering
  \vspace{-30pt}
  \includegraphics[width=\textwidth]{imgs/model_OOD.pdf}
  \vspace{-20pt}
  \caption{Processes and problem-solving ideas for the same question vary from different models with the perspectives of model types and model sizes. GPT tends to analyze and calculate, while Qwen-72B tends to solve equations. Qwen-1.5B is small and relatively weak. It can only enumerate, and its thinking chain is short, so its answers are also very wrong.}
  \label{fig: model OOD}
  \vspace{-10pt}
\end{figure*}

Firstly, \textbf{Step OOD} may occur because of different processes generated by different models. Due to the high cost of manual annotation, there are very few accurately labeled PRM expert datasets, such as PRM800K and ProcessBench, with processes generated by GPT~\cite{openai2023gpt} and Qwen~\cite{yang2024qwen2}, respectively. However, different model types (e.g., GPT, Qwen, Llama\cite{dubey2024llama}) approach problem-solving differently. As is shown in Figure~\ref{fig: model OOD}, when facing the same question, GPT-4o tends to analyze and calculate, while Qwen-72B tends to solve questions directly. They have different solution styles. Therefore, using process data generated by one model to train a PRM and then applying it to guide another model leads to an OOD issue. Moreover, models of different sizes also exhibit different reasoning processes. Larger models, like exceptional students, tend to have clearer and more accurate reasoning steps, while smaller models tend to have very short reasoning chains, as shown in Figure~\ref{fig: model OOD}.

Secondly, \textbf{Question OOD} emerges because of dataset shift. Current PRM datasets contain only a limited number of problems. For example, Math Shepherd and PRM800K cover problems from the GSM8K and MATH datasets, with GSM8K being at the elementary to middle school level and MATH at the high school to university level. However, real-world problems are far more diverse, such as those in the Olympic math competition dataset~\cite{he2024olympiadbenchchallengingbenchmarkpromoting}, leading to OOD issues in other datasets. As shown in the Figure~\ref{fig: question OOD}, we used Sentence-BERT~\cite{reimers2019sentence} to encode all the problems from the three datasets and visualized the distribution with t-SNE. It is evident that the distributions differ, and since both Olympic and MATH problems are typically from high school-level exams, they are semantically closer to each other than to GSM8K.

To address this issue, we propose a new framework, Retrieval Augmented Process Reward Model (\textbf{RetrievalPRM}), which leverages a Two-stage Retrieval-enhanced Mechanism to help PRMs solve the OOD problem. we retrieve relevant questions and steps in these two stages to address the issues of question OOD and step OOD, respectively. Specifically, when predicting a step for a given question, we select semantically similar questions based on their embeddings, placing them at the beginning of the entire prompt. Additionally, we select more fine-grained, similar steps and use them as references when predicting the correctness of the step. These retrieved questions and steps serve as a kind of warm-up for PRM, acting as example problems for reference. They not only help stimulate PRM’s potential by warming up but also allow the system to handle more difficult problems by identifying similarities, thus alleviating OOD issues. 
% Furthermore, we discuss the sample construction methods, putting multi-labels in one sample for efficiency and designing a Reference Step Attention Mask, where, when predicting the $n_{th}$ step's correctness, only the current node's references and the previous steps are visible, preventing the previous steps’ references from influencing the prediction.

\noindent Our main contributions are summarized as follows: 
\begin{itemize}[leftmargin=10pt]
    \item To the best of our knowledge, we are the first to highlight the key OOD problems in Process Reward Models (PRMs), particularly the question OOD and step OOD, which arise due to differences in reasoning patterns across model types (e.g., GPT, Qwen), model sizes (1.5B, 72B) and varying problem difficulties in real-world datasets.
    \item  We introduce the Retrieval-Augmented Process Reward Model (\textbf{RetrievalPRM}) framework, which utilizes a Two-stage Retrieval-enhanced Mechanism to address OOD issues by incorporating both Question-level Retrieval and Step-level Retrieval, thereby enhancing PRM's ability to generalize across diverse problem-solving scenarios.
    % \item We discuss the ways of sample construction methods and propose a novel Reference Step Attention Mask to prevent the influence of previous steps' references when predicting the $n_{th}$ step.
    \item We build a Retrieval-enhanced dataset for training PRM using RetrievalPRM framework. We have made our code publicly available.\footnote{https://anonymous.4open.science/r/RetrievalPRM-1C77} Our dataset\footnote{https://huggingface.co/datasets/gebro13/RetrievalPRM\_\\Dataset} and model\footnote{https://huggingface.co/gebro13/RetrievalPRM} are open-sourced.
    \item Extensive experiments on the ProcessBench~\cite{zheng2024processbench} on four public real-world datasets demonstrate that RetrievalPRM outperforms strong baselines and that the Out-of-distribution issue has been alleviated due to our retrieval approach. 
\end{itemize}