\section{Preliminary}
\label{sec:preliminary}

In this section, we formulate the whole problem and introduce PRM as a binary classification model.

\subsection{Problem Formulation}
We denote the Math dataset as $\mathcal{D}=\{(q_i,\mathbf{s}_i,\mathbf{y}_i)\}_{i=1}^N$, where $N$ is the number of data instances. 
The input $q_i$ is the $i^{th}$ Math question. $\mathbf{s}_i = \{ s^1_i,s^2_i,\ldots, s^{n_i}_i\}$ are the solution steps, where $n_i$ is the step number of solution $\mathbf{s}_i$. $\mathbf{y}_i = \{ y^1_i, y^2_i, \ldots, y^{n_i}_i \}$ and the label $y^j_i$ indicates the correctness from the $1^{st}$ step to the $j^{th}$ step.
\begin{equation}
	y^j_{i} = 
		\begin{cases}
		    1,~ (s^1_i,\ldots,s^j_i)~\text{is correct for} ~q_i; \\
			0,~ \text{otherwise.} 
		\end{cases}
\end{equation}
\subsection{ORM vs. PRM}
Outcome-supervised Reward Models are introduced (ORM) by~\cite{gsm8k}, where verifiers are trained for judging the final correctness of generated solutions. ORM only predicts the final label $\hat{y}^{n_i}_i$, which can be formulated as 
\begin{equation}
	\forall{i},\hat{y}^{n_i}_{i} = \text{ORM}(q_i,s^1_i,\ldots,s^{n_i}_i).
\end{equation}

Building on this, the concept of process reward models (PRM) is introduced as a more granular
 and transparent approach. Not only does PRM evaluate the final solutions but it also assesses intermediate processes, where $\hat{y}^{j}_{i}$ represents the predicted label for the $j^{th}$ step by PRM.
\begin{equation}
	\forall{i,j}, \hat{y}^{j}_{i} = \text{PRM} (q_i,s^1_i,\ldots,s^{j}_i).
\end{equation}

\subsection{Large Language Model for PRM scoring}
\label{llm as for prm scoing}
 
When directly adopting LLMs as the PRM for scoring, we need to convert the data $(q_i,\mathbf{s}_i,\mathbf{y}_i)$ with a hard prompt template. The whole template example is illustrated in Appendix~\ref{app:prompts}.
% \begin{figure}
%   \centering
%   % \vspace{-10pt}
%   \includegraphics[width=0.5\textwidth]{imgs/Prompt Illusttration.pdf}
%   \vspace{-10pt}
%   \caption{The illustration of PRM input template.}
%   \vspace{-10pt}
%   \label{fig:prompt template example}
% \end{figure}

The textual input consists of the question $q_i$ and steps $\mathbf{s}_i$, followed by a binary question about the correctness of these steps. 

To obtain the floating-point correctness estimation $\hat{y}_i^{j}\in[0,1]$ instead of discrete word tokens '+' or '-', we apply bidimensional softmax over the corresponding logits of the binary key answer tokens (ie., + \& -) from LLMs to accomplish the  correctness estimation during evaluation:
\begin{equation}
    \hat{y}_i^j=\frac{\exp(l_{i,\text{+}})}{\exp(l_{i,\text{+}})+\exp(l_{i,\text{-}})}\in(0,1).
\end{equation}
where $l_{i,\text{+}}$ and $l_{i,\text{-}}$ are the logits of token + and - in the  $i^{th}$ instance, respectively. 

It is important to note that the estimated PRM scoring $\hat{y}_i^j$ is used solely for evaluation on the testing set. If training is involved, we maintain the standard instruction tuning and causal language modeling paradigm for LLMs. In this way, we don't need to replace the language model head with binary classification head which is the last layer of LLM.

