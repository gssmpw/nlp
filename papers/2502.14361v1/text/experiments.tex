\section{Experiments}
\begin{table*}[h]
\centering    
\vspace{-10pt}
\caption{The performance of different models on ProcessBench. 
The best result is given in bold, and the second-best value is underlined. See Table~\ref{tab:All performance addition} in Appendix~\ref{app: supplementary results} for breakdown of evaluation results.}
% The symbol $\ast$ indicates a statistically significant improvement of RetrievalPRM over the best baseline with $p$-value < 0.01.}
\vspace{-8pt}

\label{tab:All performance}
\resizebox{1.0\textwidth}{!}{
\renewcommand\arraystretch{1.1}
\begin{tabular}{clccccccccc}
\hline
\multicolumn{2}{c}{\multirow{2}{*}{Model}} & \multicolumn{2}{c}{GSM8k} & \multicolumn{2}{c}{MATH} & \multicolumn{2}{c}{OlympiadBench} & \multicolumn{2}{c}{OmniMATH} & \multirow{2}{*}{Avg.F1}  \\ 
\cmidrule(r){3-4} \cmidrule(r){5-6} \cmidrule(r){7-8} \cmidrule(r){9-10}
\multicolumn{2}{c}{} & ArithACC & F1 & ArithACC & F1 & ArithACC & F1 & ArithACC & F1 &  \multicolumn{1}{c}{} \\ 
\hline 

\multicolumn{1}{c|}{\multirow{7}{*}{\makecell{Open-source \\ PRM}}}
& RetrievalPRM-7B(Ours) & \textbf{76.0} & \textbf{74.6} & \textbf{70.6}& \textbf{71.1} & \textbf{59.1} & \textbf{60.2} & \textbf{55.2} & \textbf{57.33} & \textbf{65.8} \\
\multicolumn{1}{c|}{\multirow{4}{*}{}} & Qwen2.5-Math-7B-PRM800K & \underline{73.5} & 68.2 &\underline{65.1} & \underline{62.6} & \underline{53.2} & \underline{50.7} & \underline{43.4} & \underline{44.3} &  \underline{56.5} \\
\multicolumn{1}{c|}{\multirow{4}{*}{}} & Skywork-PRM-7B & 71.6 & \underline{70.8} & 54.5 & 53.6 & 25.6 & 22.9 & 23.7 & 21.0 & 42.1 \\
\multicolumn{1}{c|}{\multirow{4}{*}{}} & Skywork-PRM-1.5B & 59.9 & 59.0 & 49.1 & 48.0 & 20.5 & 19.3 & 19.7 & 19.2 & 36.4 \\
\multicolumn{1}{c|}{\multirow{4}{*}{}} & Math-Shepherd-PRM-7B & 58.3 & 47.9 & 45.1 & 29.5 & 39.7 & 24.8 & 34.8 & 23.8 & 31.5 \\
\multicolumn{1}{c|}{\multirow{4}{*}{}} & RLHFlow-PRM-Mistral-8B & 62.3 & 50.4 & 42.1 & 33.4 & 22.3 & 13.8 & 19.1 & 15.8 & 28.4 \\
\multicolumn{1}{c|}{\multirow{4}{*}{}} & RLHFlow-PRM-Deepseek-8B & 56.9 & 38.8 & 45.1 & 33.8 & 26.5 & 16.9 & 23.2 & 16.9 & 26.6 \\


\hline

\multicolumn{1}{c|}{\multirow{17}{*}{\makecell{Language \\ Models \\ as Critic}}}
& QwQ-32B-Preview & \textbf{87.9} & \textbf{88.0} & \textbf{78.5} & \textbf{78.7} & \underline{59.2} & \textbf{57.8} &\textbf{61.1} & \textbf{61.3} & \textbf{71.5} \\
\multicolumn{1}{c|}{\multirow{4}{*}{}} & GPT-4o& 80.2 & 79.2& 63.4 &\underline{63.6} & 50.1&51.4& 50.1 & \underline{53.5} & \underline{61.9} \\
\multicolumn{1}{c|}{\multirow{4}{*}{}} & Qwen2.5-72B-Instruct & 77.9 & 76.2 & \underline{65.4} & 61.8 & \textbf{59.8} & \underline{54.6} & 55.1 & 52.2 &  61.2 \\

\multicolumn{1}{c|}{\multirow{4}{*}{}} & Llama-3.3-70B-Instruct & \underline{83.7} & \underline{82.9} & 63.7 & 59.4 & 54.3 & 46.7 & 51.0 & 43.0 & 58.0 \\
\multicolumn{1}{c|}{\multirow{4}{*}{}} & Qwen2.5-Coder-32B-Instruct & 72.0 & 68.9 & 64.5 & 60.1 & 57.0 & 48.9 & 52.5 & 46.3 & 56.1 \\
\multicolumn{1}{c|}{\multirow{4}{*}{}} & Llama-3.1-70B-Instruct & 75.3 & 74.9 & 52.6 & 48.2 & 50.0 & 46.7 & 43.2 & 41.0 & 52.7 \\
\multicolumn{1}{c|}{\multirow{4}{*}{}} & Qwen2.5-14B-Instruct & 72.3 & 69.3 & 59.2 & 53.3 & 50.2 & 45.0 & 43.5 & 41.3 & 52.2 \\
\multicolumn{1}{c|}{\multirow{4}{*}{}} & Qwen2-72B-Instruct & 67.8 & 67.6 & 52.3 & 49.2 & 43.3 & 42.1 & 39.3 & 40.2 & 49.8 \\
\multicolumn{1}{c|}{\multirow{4}{*}{}} & Qwen2.5-32B-Instruct & 70.6 & 65.6 & 61.9 & 53.1 & 53.5 & 40.0 & 47.7 & 38.3 & 49.3 \\
\multicolumn{1}{c|}{\multirow{4}{*}{}} & Qwen2.5-Math-72B-Instruct & 70.3 & 65.8 & 59.6 & 52.1 & 56.1 & 32.5 & 55.1 & 31.7 & 45.5 \\
\multicolumn{1}{c|}{\multirow{4}{*}{}} & Qwen2.5-Coder-14B-Instruct & 61.9 & 50.1 & 54.2 & 39.9 & 51.4 & 34.0 & \underline{55.6} & 27.3 & 37.8 \\
\multicolumn{1}{c|}{\multirow{4}{*}{}} & Qwen2.5-7B-Instruct & 37.8 & 36.5 & 36.9 & 36.6 & 29.9 & 29.7 & 27.3 & 27.4 & 32.6 \\
\multicolumn{1}{c|}{\multirow{4}{*}{}} & Meta-Llama-3-70B-Instruct & 62.4 & 52.2 & 48.3 & 22.8 & 46.2 & 21.2 & 44.8 & 20.0 & 29.1 \\
\multicolumn{1}{c|}{\multirow{4}{*}{}} & Qwen2.5-Math-7B-Instruct & 54.4 & 26.8 & 50.3 & 25.7 & 43.1 & 14.2 & 41.6 & 12.7 & 19.9 \\
\multicolumn{1}{c|}{\multirow{4}{*}{}} & Qwen2-7B-Instruct & 25.1 & 8.4 & 20.4 & 19.0 & 16.1 & 14.7 & 13.8 & 12.1 & 13.6 \\
\multicolumn{1}{c|}{\multirow{4}{*}{}} & Meta-Llama-3-8B-Instruct & 27.1 & 13.1 & 17.3 & 13.8 & 14.2 & 4.8 & 19.7 & 12.6 & 11.1 \\
\multicolumn{1}{c|}{\multirow{4}{*}{}} & Qwen2.5-Coder-7B-Instruct & 49.1 & 14.3 & 46.3 & 6.5 & 47.2 & 4.1 & 48.9 & 1.8 & 6.7 \\
\multicolumn{1}{c|}{\multirow{4}{*}{}} & Llama-3.1-8B-Instruct & 27.3 & 10.9 & 20.5 & 5.1 & 16.0 & 2.8 & 15.0 & 1.6 & 5.1 \\

\hline
\end{tabular}
\vspace{-5pt}
}
\end{table*}









\begin{table*}[h]
\centering    

\vspace{-5pt}
\caption{The performance of different variants of RetrievalPRM on ProcessBench.  We remove different components of RetrievalPRM to evaluate the contribution of each part to the model. The best result is given in bold, and the second-best value is underlined. See Table~\ref{tab:ablation performance addition} in Appendix~\ref{app: supplementary results} for breakdown of evaluation results.
}
\vspace{-8pt}

\label{tab:ablation performance}
\resizebox{1.0\textwidth}{!}{
\renewcommand\arraystretch{1.1}
\begin{tabular}{cccccccccccc}
% \toprule
\hline

\multicolumn{2}{c}{Retrieval Components} & \multicolumn{2}{c}{GSM8k} &\multicolumn{2}{c}{MATH} &\multicolumn{2}{c}{OlympiadBench}&\multicolumn{2}{c}{OmniMATH}&\multirow{2}{*}{Avg.F1}\\ 

 \cmidrule(r){3-4} \cmidrule(r){5-6}  \cmidrule(r){7-8}  \cmidrule(r){9-10}
Question-level &Step-level & ArithACC & F1 & ArithACC & F1 & ArithACC & F1 & ArithACC & F1 &  \multicolumn{1}{c}{}  \\ 
   \hline 
   



\checkmark &\checkmark& \underline{76.0}&\underline{74.6} &\underline{70.6}& \underline{71.1 }& \textbf{59.1}&\textbf{ 60.2}&\textbf{55.2}& \textbf{57.3}&\textbf{65.8}\\
\checkmark&$\times$ &\textbf{77.8} &\textbf{74.9} &\textbf{70.7}&\textbf{71.2}&\underline{58.4}&\underline{59.8} &50.5&54.4&\underline{65.0}\\
$\times$& \checkmark&73.8 &67.5 &69.5 & 69.2& 58.2& 58.9&\underline{52.2} & \underline{56.3} &63.0\\
$\times$&$\times$& 71.0&65.6 & 67.3 & 67.5&54.3 & 55.8 & 47.2&50.9 &59.9\\

\hline   
\end{tabular}
\vspace{-5pt}
}

\end{table*}


\begin{figure*}[t]
  \centering
  \vspace{-30pt}
  \includegraphics[width=1.0\textwidth]{imgs/F1_Scores_of_Models.pdf}
  \vspace{-25pt}
  \caption{We show the F1 scores of Retrieval-PRM on four datasets and their average, as the number of retrieval questions varies. Specifically, Top-0 means no retrieval questions. }
  \label{fig: Hyperparameter}
  \vspace{-10pt}
\end{figure*}

In this section, we present the experimental settings and results. Our implementation code of RetrievalPRM is publicly available.
% The following discussions are guided by three research questions.

% \noindent\textbf{RQ1} Does TR-PRM outperform existing baselines?

% \noindent\textbf{RQ2} What is the impact of different components in TR-PRM?

% \noindent\textbf{RQ3} What impact does the number of retrieval questions have on the performance of the model?
\subsection{Experiment Setup}
\subsubsection{Datasets}
Datasets are categorized into two kinds: Math reasoning datasets, and prm training datasets.

\noindent \textbf{Math Reasoning Datasets}

We conduct experiments on four public and widely used datasets in mathematical reasoning tasks: \textit{GSM8K}~\cite{gsm8k} which contains math problems from elementary to middle school, \textit{MATH}~\cite{hendrycks2021measuring} which contains math problems from basic to university level, \textit{OlympiadBench}~\cite{he2024olympiadbenchchallengingbenchmarkpromoting} which involves questions from the Mathematical Olympiad, \textit{Omni-MATH}~\cite{gao2024omnimathuniversalolympiadlevel} which covers multi-domain high-difficulty problems. Further details are provided in Appendix~\ref{app:datasets}.


Except for GSM8K, which focuses on grade school math problems, the other three datasets feature problems of competition or Olympiad-level difficulty.

\noindent \textbf{PRM training datasets}

We conduct experiments on two publicly available datasets for PRM:

\textit{PRM800K}~\cite{lightman2023let}: Based on the MATH dataset, it contains 800,000 manually annotated step-level correctness labels for training the Process Reward Model. It relies on expensive manual annotations.

\textit{Math-Shepherd}~\cite{wang2024math}: It generates 400,000 machine-annotated step-level labels (covering MATH and GSM8K datasets) by automatically building process supervision data, without manual annotation.

\subsubsection{Evaluation Metrics}
We evaluate our model in a public PRM benchmark ProcessBench~\cite{zheng2024processbench}. The aim is to judge whether PRM can find the first wrong step. It divides data into two parts: samples with incorrect and correct final answers and then conducts harmonic mean on the accuracy of these two parts to get the final F1-score. Moreover, we think since the sample number of each part isn't balanced, We add an additional metric: weighted arithmetic mean of these two parts, which is shown in Table~\ref{tab:All performance} as ArithACC.

\subsubsection{Baselines}
Following~\cite{zheng2024processbench}, we divide all baselines into two parts:

(1) \textit{Open-source PRM}, including Skywork~\cite{skyworkopeno12024}, Qwen2.5-PRM~\cite{zheng2024processbench}, Math-Shepherd~\cite{wang2024math} and RLHFlow~\cite{xiong2024rlhflowmath}. These models are binary classification PRMs.

(2) \textit{Language Models as Critic}, including Llama~\cite{dubey2024llama}, Qwen2~\cite{yang2024qwen2}, Qwen2.5~\cite{qwen2.5}, Qwen2.5-MATH~\cite{yang2024qwen25mathtechnicalreportmathematical}, Qwen2.5-Coder~\cite{hui2024qwen25codertechnicalreport}, GPT-4o~\cite{openai2024gpt4ocard}. These models are promoted to judge the steps with the help of majority voting.

Further details of these baselines are provided in Appendix~\ref{app:baselines} due to article length limitations. 

\subsubsection{Implementation Details}
Details like base models, hyperparameters, prompts, and training sizes are provided in Appendix~\ref{app:implementation details} due to the article length limitations.

\subsection{Overall Performance}
We evaluate RetrievalPRM against existing baselines on ProcessBench, and the results are presented in Table~\ref{tab:All performance}. The findings are as follows:

\begin{itemize}[leftmargin=10pt]
    \item RetrievalPRM-7B surpasses all open-source PRM baselines, achieving the highest performance. Notably, the most significant improvement is observed on OmniMATH, the most challenging dataset, with performance gains increasing as dataset difficulty rises. This phenomenon may stem from the fact that most baseline PRMs are trained on human- or machine-annotated datasets such as PRM800K or Math-Shepherd, which primarily focus on GSM8K or MATH and exhibit OOD issues when applied to more complex datasets. In contrast, our RetrievalPRM effectively mitigates the OOD problem through its retrieval-based approach, demonstrating the efficacy of our Two-stage Retrieval-enhanced Mechanism.
    \item When comparing models of different scales, RetrievalPRM outperforms all evaluated language models, including Qwen2.5-72B-Instruct and Llama3.3-70B-Instruct, with the sole exception of QwQ-32B-Preview. Remarkably, RetrievalPRM achieves this with a model size of just 7B. This highlights that PRMs, being both lightweight and task-specific, maintain strong competitiveness and potential compared to LLMs as critics.
\end{itemize}
\subsection{Ablation Study}
We analyze two main components in the Two-stage Retrieval-enhanced Mechanism: \emph{Question-level Retrieval} and \emph{Step-level Retrieval}—through the following ablations:

\noindent\textbf{RetrievalPRM (Ours)}: The complete version of our proposed method.

\noindent\textbf{RetrievalPRM (w/o Step-level Retrieval)}: This variant retains only the Question-level Retrieval, removing Step-level Retrieval during both training and inference.

\noindent\textbf{RetrievalPRM (w/o Question-level Retrieval)}: This variant retains only the Step-level Retrieval, removing Question-level Retrieval during both training and inference.

\noindent\textbf{RetrievalPRM (w/o Question-level and Step-level Retrieval)}: In this variant, both Question-level and Step-level Retrieval are removed during training and inference.

The performance of these variants is presented in Table~\ref{tab:ablation performance}, from which we can draw the following observations:
\begin{itemize}[leftmargin=10pt]
    \item The performance of RetrievalPRM (w/o Step-level Retrieval) remains almost identical to that of RetrievalPRM on GSM8K and MATH but exhibits a slight decline on OlympiadBench and OmniMATH. This can be attributed to the fact that Step-level Retrieval information is partially absorbed by Question-level Retrieval. As a result, Question-level Retrieval alone may be sufficiently effective for relatively easy datasets, as the reference steps it provides contain adequate knowledge for step prediction. However, for more challenging datasets, Step-level Retrieval becomes significantly more crucial, as it offers finer-grained guidance essential for handling complex problem-solving processes.
    \item RetrievalPRM (w/o Question-level Retrieval) shows lower performance, as it relies solely on Step-level Retrieval. The model lacks knowledge of reference questions, which is useful to alleviate question OOD, restricting its overall performance. 
    \item RetrievalPRM (w/o both Retrieval) performs the worst, which is expected, demonstrating the effectiveness of both question-level and Step-level Retrieval.
\end{itemize}

\subsection{Hyperparameter Study}


Figure~\ref{fig: Hyperparameter} illustrates the impact of the number of retrieval questions on the model's performance. The findings are as follows:

Compared to Top-0, where no retrieval questions are used, models that incorporate retrieval questions show improved performance, highlighting the importance of Question-level Retrieval. It inspires us that Reference questions are important for PRM to get warmup, no matter how many reference questions there are.

The performance of Top-3 exhibits a slight decline, potentially due to two factors: (1) An excessive number of reference questions may lead to an overly long input prompt, making it difficult for PRMs to comprehend or extract key information effectively. (2) A limited retrieval pool might result in later reference questions being less relevant than earlier ones, increasing the likelihood of misjudgments in the model’s predictions.








