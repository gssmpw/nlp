\section{Related Work and Background}
\subsection{Alignment Tuning}
Alignment tuning has become a central focus in enhancing LLMs to meet user expectations and ethical standards\cite{sage_rt}. Various preference-based learning techniques, particularly reinforcement learning methods such as Proximal Policy Optimization (PPO)\cite{ppo}, Direct Preference Optimization (DPO)\cite{dpo}, and Optimized Reinforcement Preference Optimization (ORPO)\cite{orpo}, have been developed to facilitate this tuning process. These methods rely on preference datasets, which typically contain pairs of responses generated by models based on given instructions, with each pair ranked according to human or model-based evaluations. Prominent alignment datasets like \textit{Ultrafeedback\cite{ultrafeedback}}, which gathers extensive human feedback, and \textit{HH-RLHF\cite{hh_rlhf}}, which uses human-annotated preferences, provide foundational resources for alignment. To minimize the reliance on labor-intensive data curation, recent automated approaches have been introduced to filter or regenerate preference data based on specific criteria, promoting data consistency and scalability. However, such approaches often lack nuanced control over data quality, as they fail to consider the role of the origin model’s capabilities in shaping alignment effectiveness \cite{safer_instruct}.

\subsection{Limitations in Existing Preference Data Approaches}
Effective preference data construction requires a clear, rigorous set of criteria to ensure alignment quality across generated response pairs. Common criteria, including \textit{Reasoning}, \textit{Truthfulness}, and \textit{Instruction-Following}, guide the selection of data that aligns with key ethical and functional standards\cite{ultrafeedback, hh_rlhf}. High-performing models, capable of producing responses that meet these standards, are often evaluated using benchmarks like \textit{ARC\cite{arc}}, \textit{MMLU\cite{mmlu}}, and the \textit{Instruction-Following eval\cite{ifeval}} suite, which assess a model’s factual accuracy, reasoning ability, and compliance with instructions. While these benchmarks provide a foundation for selecting responses that promote alignment goals, existing methods typically overlook the impact of variation in model capabilities on alignment consistency. 

The \textit{KARMA Framework} addresses this limitation by introducing a kinship-aware approach that emphasizes model compatibility in preference data selection. By curating preference pairs based on model kinship—aligning models with similar core competencies in knowledge, reasoning, and instruction-following—the KARMA Framework enhances alignment coherence and ensures stable, high-quality data. Further, its curriculum-based selection process sequences preference data from simpler to more complex distinctions, optimizing the alignment process for improved consistency and scalability.