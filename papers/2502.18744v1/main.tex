% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{kotex}
\usepackage{tabularx} % Include this in your preamble
\usepackage{adjustbox}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{booktabs}

% \usepackage[table]{xcolor}
% \usepackage{xcolor}
\usepackage{soul}
\definecolor{customblue}{rgb}{0.85, 0.847, 0.941}
\sethlcolor{customblue}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% \newcommand{\chanjun}[1]{\textcolor{blue}{[Chanjun: #1]}}
% \newcommand{\jeesu}[1]{\textcolor{olive}{[Jeesu: #1]}}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Like Father, Like Son: Kinship-Aware Preference Mapping (KARMA) \\for Automatic Alignment in Large Language Models}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
  Jeesu Jung$^{1}$, Chanjun Park$^{2}$, Sangkeun Jung$^{1}$\thanks{Corresponding author} \\
  $^{1}$Chungnam National University, $^{2}$Korea University \\
  \texttt{jisu.jung5@gmail.com}, \texttt{bcj1210@korea.ac.kr}, \texttt{hugmanskj@gmail.com}
}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Recent advancements in Large Language Model (LLM) alignment have sought to mitigate the cost of human annotations by leveraging pretrained models to generate preference data. However, existing methods often compare responses from models with substantially different capabilities, yielding superficial distinctions that fail to provide meaningful guidance on what constitutes a superior response. To address this limitation, we propose Kinship-Aware pReference MApping (KARMA), a novel framework that systematically pairs responses from models with comparable competencies. By constraining preference comparisons to outputs of similar complexity and quality, KARMA enhances the informativeness of preference data and improves the granularity of alignment signals. Empirical evaluations demonstrate that our kinship-aware approach leads to more consistent and interpretable alignment outcomes, ultimately facilitating a more principled and reliable pathway for aligning LLM behavior with human preferences.

%Recent efforts in Large Language Model (LLM) alignment have sought to reduce human annotation costs by automatically generating preference data from pretrained models. However, existing approaches often compare responses derived from models of substantially differing capabilities, producing only superficial contrasts that do little to clarify what constitutes a “better” response. This paper introduces the Kinship-Aware pReference MApping (KARMA) framework, which prioritizes pairing model outputs with comparable competencies, thereby focusing evaluative efforts on subtle and genuinely informative distinctions. By centering preference data on responses of similar complexity and quality, KARMA enhances the richness of alignment signals. Our empirical findings demonstrate that this kinship-based strategy leads to more coherent and meaningful alignment outcomes, offering a more reliable and insightful pathway for guiding LLMs toward responses aligned with human preferences.

\end{abstract}

\section{Introduction}
Aligning Large Language Models (LLMs) with human preferences is a fundamental challenge in artificial intelligence, where the effectiveness of alignment hinges on the quality and specificity of the preference data used to guide model outputs \cite{inconsistency}. Early alignment methodologies primarily relied on \textit{human-annotated} datasets with binary preference labels, wherein explicit human judgments directed models toward generating more desirable responses \cite{InstructGPT}. While these methods have played a critical role in improving LLM behavior, their dependence on extensive human labor imposes significant scalability constraints.


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/flow.pdf}
    \caption{Human Annotation\cite{rlhf}, AI labeler based RLAIF\cite{ultrafeedback}, and Model Kinship based Annotation(KARMA)}
    \label{fig:overall_flow}
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/overall_structure.pdf}
    \caption{Editing process of KARMA framework. $n$ is the number of models. $\{M_1, M_2, ..., M_n\}$ represents the total set of candidate models. $\{I_1, I_2, ..., I_n\}$ represents the instruction input of dataset. $\{R_1, R_2, ..., R_n\}$ represents the total responses from the candidate models. $R_x$ and $R_y$ are the chosen and rejected responses. $S_{pref}$ score summarizes the model's benchmarks abilit}
    \label{fig:overall_structure}
\end{figure*}

To address this limitation, recent research has explored automated preference data generation techniques, such as Reinforcement Learning from AI Feedback (RLAIF) \cite{RLAIF}. These methods leverage pretrained models to \textit{automatically} generate preference pairs, significantly reducing the need for human annotation. However, the quality of the resulting preference data remains a major concern. If responses within a preference pair exhibit substantial disparities in quality, complexity, or style, the preference signal may become trivial, capturing only superficial contrasts rather than informative distinctions that refine model behavior.

We define this issue as the \textit{Response Quality Gap}, which refers to the disparity in coherence, complexity, and reasoning quality between candidate responses. A large Response Quality Gap weakens preference signals, leading to misaligned or less informative training objectives. This problem is particularly pronounced in large-scale, automatically generated datasets, where manually curating response pairs to minimize the Response Quality Gap is infeasible due to the massive volume of data.

A promising solution lies in considering the \textit{origin} of the responses being compared—specifically, the LLM that generated them. Since a model’s reasoning ability, commonsense knowledge, and instruction-following capabilities are inherently shaped by its training data and architecture, the quality of its responses is largely determined by these characteristics. Consequently, comparing responses from models with vastly different capabilities introduces inconsistencies in preference data, making alignment objectives less reliable.

To formalize this idea, we introduce the notion of \textit{Kinship} among LLMs, which quantifies the similarity in reasoning ability, commonsense understanding, and instruction adherence among models. Measured using standard benchmarks, kinship provides a principled way to assess whether two models generate responses of comparable complexity and quality. Building on this insight, we propose the \textbf{Kinship-Aware pReference MApping (KARMA)} framework, which systematically pairs responses based on model kinship. Unlike conventional approaches that indiscriminately compare outputs from models of varying capabilities, KARMA ensures that preference pairs are drawn from models with similar competencies, thereby narrowing the Response Quality Gap. This results in preference data that captures more subtle and meaningful distinctions, leading to higher-quality alignment signals.

The effectiveness of KARMA is illustrated in Figure~\ref{fig:overall_flow}, which provides an overview of the framework, and Figure~\ref{fig:overall_structure}, which details its response evaluation process. By fostering more structured and granular comparisons, KARMA improves the coherence and interpretability of preference data, shifting alignment objectives from filtering out trivial differences to refining preference signals through \textit{comparability and consistency}. Our empirical results demonstrate that this kinship-aware approach significantly outperforms prior methods such as RLAIF, offering a more robust and scalable solution for aligning LLM behavior with human preferences.





% The alignment of Large Language Models (LLMs) with human preferences is contingent upon the quality and specificity of the data used to guide their outputs \cite{inconsistency}. Early alignment methodologies relied heavily on \textit{human-annotated} datasets with binary labels, where explicit judgments directed models toward generating more desirable responses \cite{InstructGPT}. While effective, these approaches are constrained by their reliance on extensive human labor, posing significant challenges to scalability. 

% To mitigate these limitations, recent efforts have turned to methods such as Reinforcement Learning from AI Feedback (RLAIF) \cite{RLAIF}, which utilize pretrained models to \textit{automatically} generate preference pairs. This approach reduces the need for direct human intervention but introduces new challenges in maintaining the informativeness and coherence of the preference data.

% The importance of response similarity in LLM alignment has recently gained attention. If candidate responses presented for evaluation vary greatly in quality, complexity, or style, preference data may end up reflecting only superficial contrasts. Analogous to human evaluation, asking a reviewer to choose between a simplistic and logically weak response versus a sophisticated and well-structured one provides little meaningful insight. Such evaluations become trivial and offer limited information about the subtle characteristics that constitute high-quality responses. 

% We term this phenomenon the \textit{Response Quality Gap}, referring to the disparity in quality, complexity, and coherence between two or more candidate responses being compared. A large Response Quality Gap leads to weak preference signals, which can hinder the development of more refined alignment objectives.

% In automatically generated instruction-response datasets, adjusting for this Response Quality Gap is challenging. Each data pair requires individual evaluation, incurring substantial costs and time. This instance-wise evaluation approach is particularly difficult to apply consistently across large-scale automatically generated datasets.

% To address this issue, we focus on the model that generated the response, referred to as the "\textit{Origin Model}." Since LLMs generate outputs only within the distribution of their trained data, the LLM’s performance and characteristics directly determine the quality of its responses. When models exhibit significant differences in reasoning ability, commonsense understanding, and instruction-following capabilities, comparing their responses becomes challenging. From this perspective, we define the similarity in reasoning, commonsense understanding, and instruction-following capabilities among origin models—measured using standard benchmarks—as \textit{Kinship}.

% We propose the \textit{Kinship-Aware pReference MApping (KARMA)} framework. KARMA centers on the principle of model kinship, wherein response pairs are formed from models with inherently similar capabilities. Unlike conventional approaches that compare outputs from models with divergent skill levels, KARMA emphasizes the selection of response pairs from models exhibiting comparable performance. This strategy narrows the Response Quality Gap, enabling preference data to capture subtler and more meaningful distinctions. 

% By fostering these nuanced comparisons, KARMA aims to provide alignment signals that more accurately reflect human-like judgments. This framework shifts the focus of alignment from filtering out trivial differences to highlighting informative contrasts, offering a parsimonious and effective pathway toward enhanced preference alignment. KARMA thereby advances the data-centric paradigm of LLM alignment by emphasizing the importance of \textit{coherence and comparability} in response generation. Figure~\ref{fig:overall_flow} provides an overview of the KARMA framework, while Figure~\ref{fig:overall_structure} illustrates its editing process. This approach offers significant improvements over prior methods such as RLAIF, paving the way for more consistent and reliable alignment criteria for LLMs.

\section{Kinship-Aware pReference MApping (KARMA) Framework}
Recent approaches to LLM alignment have reduced the reliance on human annotation through automated preference data generation. However, these methods often neglect the foundational influence of the \textit{Origin Model}. 
For example, a model trained using Llama’s methodology is going to produce outputs that fall within the distribution of Llama’s training data. Just as a son inherits traits from his father, a model’s quality directly influences the quality of its outputs. when preference pairs are derived from models with markedly disparate capabilities, the resulting comparisons yield only trivial distinctions.

To address this, the proposed \textit{Kinship-Aware pReference MApping (KARMA)} framework evaluates kinship based on the performance of origin models on selected benchmark metrics. With \textit{kinship} as a guiding principle for structuring binarized data, the framework systematically generates preference data for alignment tuning.


\subsection{Definition of Kinship}
In the context of binarized alignment data mapping, the similarity between the responses serves as a fundamental determinant of mapping efficacy. When data pairs are closely aligned, they provide more nuanced and meaningful guidance signals for alignment tuning. Building upon this principle of response similarity, KARMA introduces the concept of \textit{\textbf{kinship}} among models.

The kinship is measured by evaluating the capabilities of origin models. We measure the benchmarks as the origin model capability. We define a set of $n$ candidate origin models, $\{M_1, M_2, \ldots, M_n\}$, each evaluated according to multiple dimensions of competency. Let $\mathcal{B} = \{B_1, B_2, \ldots, B_m\}$ be a set of $m$ benchmark tasks. Each benchmark $B_k$ provides an evaluation metric $\mathcal{E}_{B_k}(M_i)$ that quantifies the performance of model $M_i$. This evaluation may include measures of factual correctness, reasoning, and adherence to instructions.

For each pair of models $(M_i, M_j)$, we define a kinship score $S_{ij}$, which reflects the similarity of their capabilities:
\begin{equation}
    S_{ij} = f\bigl(\{\mathcal{E}_{B_k}(M_i), \mathcal{E}_{B_k}(M_j)\}_{k=1}^m \bigr),
\end{equation}
where $f(\cdot)$ is a similarity function. Higher values of $S_{ij}$ indicate closer competency alignment between $M_i$ and $M_j$. The exact functional form of $f(\cdot)$ can vary depending on the chosen benchmarks and may incorporate multiple factors (denoted collectively as $IF$, $R$, and $K$) to minimize bias and ensure comprehensive evaluation.

\begin{table}[t]
    \centering
    \begin{adjustbox}{width=0.5\textwidth}
        \begin{tabular}{c|c|c}
   \toprule
   & \textbf{Descriptive} & \textbf{Multiple-Choice} \\
   \midrule
   \begin{tabular}{@{}c@{}}\textbf{Instruction} \\ \textbf{Following} \end{tabular} & IF eval\cite{ifeval} &  \\
   \midrule
   \textbf{Knowledge} &  & \begin{tabular}[c]{@{}c@{}}MMLU\cite{mmlu} \\ MMLU-pro\cite{mmlu_pro}\end{tabular} \\
   \midrule
   \textbf{Reasoning} &  & \begin{tabular}[c]{@{}c@{}}ARC-Easy\cite{arc} \\ ARC-Challenge\cite{arc} \\ Hellaswag\cite{hellaswag}\end{tabular} \\
   \bottomrule
        \end{tabular}
    \end{adjustbox}
    \caption{Selected benchmarks for evaluating the origin models knowledge, reasoning, and instruction-following capabilities}
    \label{tab:benchmark}
\end{table}

\subsection{Stage 1: Benchmark Evaluation for Model Competency}

To identify which models are suitable candidates for kinship-based pairing, we employ a series of benchmarks that meet three criteria: (1) they are widely validated in prior research, (2) they cover both general knowledge and domain-specific tasks, and (3) they provide well-defined quantitative metrics. A summary of selected benchmarks is provided in Table~\ref{tab:benchmark}.

Evaluating models on these benchmarks yields performance scores that serve as the basis for computing $S_{ij}$. The degree of kinship among models varies with the choice of benchmarks. To avoid relying solely on log-probabilities or overly simplistic metrics, we adopt prompt-based evaluations that require each model to produce explicit responses. 
Appendix \ref{app:benchmarks} presents the benchmark performance of all models used for kinship calculation, along with example calculations.

The prompt formats are adapted from templates in MMLU-pro \cite{mmlu_pro} and FLAN \cite{flan}, ensuring compatibility with each model’s input structure and recommended usage patterns. The prompt template can be found in Appendix \ref{app:prompt}.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/example_kinship.pdf}
    \caption{Example of calculation kinship score $K_{score}$, $K_{sim}$, and $K_{(score, sim)}$.}
    \label{fig:example_karma}
\end{figure}

\subsection{Stage 2: Kinship-Based Response Mapping}
Once we identify a subset of models whose pairwise kinship scores exceed a predetermined threshold $\tau$ (e.g., $S_{ij} \geq \tau$), we set $\tau$ at 0.1, as cosine similarity values below this threshold indicate a lack of meaningful similarity between models. we focus on collecting responses from these closely matched models. Kinship serves as an indicator of model's ability and similarity. While various criteria can be applied to measure this, we utilize the models' capabilities and performance similarities that can be estimated through benchmarks. We have defined kinship across three distinct cases:

\begin{itemize}
\item $K_{score}$: Mapping the two origin models with the highest average \textbf{benchmark scores}.
\item $K_{sim}$: Mapping the two origin models with the highest cosine \textbf{similarity} between their benchmark scores.
\item $K_{(score,sim)}$: Mapping the model with the \textbf{highest benchmark score to the most similar} model based on cosine similarity.
\end{itemize}

Figure \ref{fig:example_karma} shows the each kinship definition and calculation process. 

To generate preference data from these kinship relationships, we convert model responses into binary preference pairs using a consistent metric based on normalized benchmark performance.  This approach entirely eliminates the need for \textit{instance-wise annotations}, as it operates at the dataset level, transforming the entire instruction-response dataset into preference data without requiring instance-wise annotations from either models or humans. By leveraging aggregate performance metrics rather than per-instance comparisons, we maintain consistency while \textbf{removing computational overhead and human cost.}

% Once we identify a subset of models whose pairwise kinship scores exceed a predetermined threshold $\tau$ (e.g., $S_{ij} \geq \tau$), we focus on collecting responses from these closely matched models. Let $\mathcal{P}$ be a set of prompts to which each model responds. For each prompt $p \in \mathcal{P}$, let $R_i^p$ denote the response generated by model $M_i$.

% Because $M_i$ and $M_j$ are closely matched, $\mathrm{Pref}(M_i, M_j, p)$ does not merely indicate obvious superiority. Instead, it encodes subtle preferences that are more likely to guide fine-tuning toward meaningful and human-like alignment.

\subsection{Stage 3: Automatic Preference Integration with Alignment Processes}
To integrate kinship-based mapped data into existing alignment tuning, the data requires a preference-setting process. Accordingly, we construct our kinship-based preference estimation on the principle that higher-performing models tend to produce more reliable outputs.

To quantify this, we calculate a normalized benchmark score that aggregates multiple benchmark performances into a single value. For a model $M_i$, this score is computed as:

\begin{equation}
S_{pref}(M_i) = \frac{1}{m}\sum_{j=1}^{m} \frac{b_j - \min(\mathcal{B}_j)}{\max(\mathcal{B}_j) - \min(\mathcal{B}_j)}
\end{equation}

For a pair of kinship-aligned models $(M_i, M_j)$, we obtain a binary preference label:

\begin{equation}
\mathrm{Pref}(M_i, M_j) = \begin{cases}
1, & \text{if } S_{pref}(M_i) > S_{pref}(M_j), \\
0 & \text{otherwise.}
\end{cases}
\end{equation}

Our data maintains the Response Quality Gap through kinship-based model comparisons and can establish preferences via $\mathrm{Pref}(M_i, M_j)$ without additional annotation. This provides high-quality training signals, which can be leveraged through reinforcement learning or supervised fine-tuning.

In contrast to existing approaches (e.g., RLAIF \cite{RLAIF}), KARMA's focus on model kinship produces preference pairs that yield more nuanced insights. This shift in perspective—from relying on comparisons of models with vastly different capabilities to leveraging finely matched pairs—provides a more parsimonious and theoretically grounded method for constructing alignment-relevant preference data.


% \section{Introduction}
% The alignment of Large Language Models (LLMs) with human preferences fundamentally depends on the quality and coherence of preference data derived from LLM responses\cite{inconsistency}. Early alignment methods focused on manually curated, binary-labeled datasets, where human judgments directed models toward desirable outputs \cite{InstructGPT}. 

% However, due to the high cost and complexity of manual labeling, recent efforts have shifted towards generating preference data automatically, leveraging pretrained models to produce preference pairs that approximate human judgment\cite{RLAIF}. While this approach reduces the need for human labor, it raises questions about the influence of the models generating these responses on the resulting preferences \cite{RLAIF_eval}. 

% While this approach reduces the need for human labor, it introduces a critical challenge: the risk of bias from the models used to generate these preferences \cite{rl_limitation, limited}. The construction of preference data now involves generating responses from multiple models and determining preferences between them, which has become the most resource-intensive part of the process. We argue that the effectiveness of this method depends on selecting models with comparable capabilities.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=1\linewidth]{latex/figures/flow.png}
%     \caption{Human Annotation, AI labeler based RLAIF, and Model Kinship based Annotation(KARMA)}
%     \label{fig:overall_flow}
% \end{figure}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=1\linewidth]{latex/figures/overall_structure.png}
%     \caption{Editing process of our framework. $n$ is the number of models. $\{M_1, M_2, ..., M_n\}$ represents the total set of candidate models. $\{R_1, R_2, ..., R_n\}$ represents the total responses from the candidate models. $R_x$ and $R_y$ are the chosen and rejected responses.}
%     \label{fig:overall_structure}
% \end{figure}

% Variations in model capabilities—such as reasoning, general knowledge, and instruction-following abilities—can introduce inconsistencies within preference data, ultimately influencing the alignment outcome. We posit that alignment coherence is best achieved when data originates from models with comparable capabilities, as judged by similar performance on benchmark evaluations. By aligning outputs from models with closely matched reasoning, commonsense, and instruction-following capacities, we can create more informative and meaningful preference distinctions. Conversely, comparing responses from models with widely divergent abilities may produce trivial or less insightful contrasts, offering limited value for model fine-tuning.

% To address this challenge, we introduce the \textit{Kinship-Aware pReference MApping (KARMA) framework}, an origin-focused approach to preference alignment. KARMA goes beyond traditional data generation techniques by curating preference pairs through three core stages: (1) evaluating the similarity in capabilities among candidate models, as measured through benchmark scores, (2) assessing the complexity of preference pairs to filter out trivial distinctions, and (3) applying these insights within a curriculum learning-based data selection process that optimizes alignment coherence. By emphasizing model kinship in preference data selection, KARMA constructs alignment data that maintains consistency, ultimately enhancing alignment quality.

% Our empirical findings confirm that preference data rooted in model kinship significantly improves alignment coherence, underscoring the critical influence of the Origin Model’s capabilities. This study advances a data-centric approach to preference alignment, supporting more cohesive and effective tuning processes for LLMs.

% \section{Kinship-Aware pReference MApping (KARMA) Framework}
% In this paper, we aim to construct preference data based on the evaluated capabilities of multiple models to reduce additional costs associated with preferences and response generation. Despite advancements in data generation, a critical gap remains: the foundational impact of the “Origin Model” from which preference data originates. To achieve this, the KARMA Framework is designed to enhance alignment tuning in LLMs by strategically leveraging the foundational characteristics of the Origin Model. The rationale behind KARMA lies in the observation that alignment coherence is best maintained when preference data originates from models with closely aligned capabilities. By carefully curating preference data based on model similarity, response complexity, and a curriculum-based selection process, KARMA aims to construct alignment data that ensures consistent and meaningful model behavior.

% \subsection{Stage 1: Scoring Kinship among Origin Models}
% The first stage in the KARMA Framework focuses on identifying models with similar capabilities to serve as sources for preference data. Here, \textit{“\textbf{kinship}”} is defined through shared core competencies in knowledge, reasoning, and instruction-following, as measured by performance on standardized benchmarks. For each candidate model \( M_i \), a competency profile is established based on scores across benchmark datasets relevant to each competency. This evaluation forms a basis for selecting models that share similar strengths and limitations, ensuring that generated preference data aligns with consistent quality standards.

% We define the kinship score \( Score_{ij} \) between models score vector \( V_i \) and \( V_j \) as:

% \begin{equation}
% Score_{ij} = \frac{\mathbf{V_i} \cdot \mathbf{V_j}}{\|\mathbf{V_i}\| \|\mathbf{V_j}\|}
% \end{equation}

% \subsection{Stage 2: Pairing the Similar Models based on Models’ Kinship}
% For hierarchical clustering, clusters are formed based on a distance matrix computed between given data samples. Using cosine distance to calculate the pairwise distance between samples, clusters are progressively merged. The distance between each candidate model $M_i$ and $M_j$ is defined as follows:

% \[
% distance_{ij} = 1 - Score_{ij}
% \]

% Hierarchical clustering calculates the distance $d(C_i, C_j)$ between two clusters $C_i$ and $C_j$ using the single linkage method, which is defined as:

% \[
% d(C_i, C_j) = \min_{M_i \in C_i, M_j \in C_j} distance_{ij}
% \]



% The competency score vector for a model \( M_i \) is expressed as:

% \[
% V_i = \{ S_{i,c} \mid c \in \{ K, R, IF \} \}
% \]

% \( S_{i,c} \) denotes the score of model \( M_i \) on competency \( c \) (Knowledge \( K \), Reasoning \( R \), Instruction-Following \( IF \)). Lower values of \( Score_{ij} \) indicate stronger kinship between models, guiding the selection of compatible origin models for preference data generation.

% % where \( S_{i,c} \) and \( S_{j,c} \) denote the scores of models \( M_i \) and \( M_j \) on competency \( c \) (Knowledge \( K \), Reasoning \( R \), Instruction Following \( I \)). Lower values of \( K_{ij} \) signify stronger kinship, guiding the selection of compatible origin models for preference data generation.

% \subsection{Stage 3: Ranking Preference based on Model Abilities}
% The second stage of the KARMA framework evaluates the quality of the origin models and assigns priorities among the models matched based on kinship. The quality of a model is determined using benchmark scores. Specifically, an overall performance score is calculated based on comprehensive assessments of knowledge, reasoning, and instruction-following capabilities.
% Once kinship is established, the KARMA Framework progresses to filtering preference pairs based on their origin model scores, focusing on the models' abilities.

% Each data point \( p_j \) within a candidate preference pair is evaluated on factors such as response length, syntactic complexity, and contextual relevance, contributing to a complexity score \( C(p_j) \):

% \begin{equation}
% C(p_j) = w_l L(p_j) + w_s S(p_j) + w_c Cx(p_j)
% \end{equation}

% where \( L(p_j) \) is the response length, \( S(p_j) \) is syntactic complexity, and \( Cx(p_j) \) is contextual relevance, with weights \( w_l \), \( w_s \), and \( w_c \) calibrated to emphasize the most impactful elements. By focusing on non-trivial distinctions, this stage ensures that preference pairs contribute meaningful data for alignment, rather than reinforcing superficial differences.

% % \subsection{Stage 3: Alignment Tuning with Preference Data Using Model Kinship Strategy}
% % In the final stage, KARMA employs a curriculum learning approach to sequence preference data based on increasing complexity, progressively refining the model’s alignment. Each preference pair is assigned a learning priority \( P \), which reflects both the kinship score of the origin model and the complexity of the preference pair:

% % \begin{equation}
% % P(p_j) = \alpha K_{ij} + \beta C(p_j)
% % \end{equation}

% % where \( \alpha \) and \( \beta \) are weights indicating the relative importance of model kinship and response complexity. The selection process begins with simpler pairs from closely aligned models and advances to more complex data, allowing the model to establish stable alignment foundations before tackling intricate distinctions. This curriculum-based progression enables smoother and more effective preference-based tuning, optimizing alignment quality by leveraging model similarity and controlled data exposure.

% % By following these three stages, the KARMA Framework achieves a structured and origin-aware approach to preference data construction, resulting in alignment data that reinforces consistent and cohesive model behavior across diverse tasks and contexts.

% \subsection{Benchmark Evaluation for Model Competency}
% To determine which model qualifies as a "well-trained" model, we utilized benchmark evaluation for measuring \( K_{ij} \), as previously defined. The criteria for selecting benchmarks were as follows: 1) widely validated by existing research and models, 2) encompassing general knowledge and domain applicability, and 3) providing evaluation metrics. Details on the selected benchmarks can be found in Table \ref{tab:benchmark}.

% The proposed KARMA in this paper is significantly influenced by the criteria used to determine kinship between models. The extent of this kinship varies according to the characteristics of the benchmark, which serves as a competency standard. Figure \ref{fig:model_similarity} presents the results of this comparison. To avoid bias in these criteria, this paper comprehensively applies $IF$, $R$, and $K$ for assessment.

% In this study, while we utilize a multiple-choice benchmark format, we acknowledge the limitations of evaluation methods based solely on log-probability. Recognizing the importance of explaining and adopting appropriate formats, we conducted prompt-based evaluations across all benchmark datasets. 
% The prompt templates were designed based on templates provided by MMLU-pro\cite{mmlu_pro} and FLAN\cite{flan}, ensuring compatibility with the input structure. Additionally, we incorporated the recommended input formats specified in the respective papers of each model to refine the evaluation process.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{latex/figures/model_size.chosen.png}
%     \caption{Chosen and Rejected Frequency for UltraFeedback}
%     \label{fig:mother}
% \end{figure}

\begin{table}
    \centering
    \begin{adjustbox}{width=\linewidth}
    \begin{tabular}{l|c|ccc}
        \toprule
        & Baseline & \multicolumn{3}{c}{Ours} \\
        \cline{2-5}
        & RLAIF & $K_{score}$ & $K_{sim}$ & $K_{(score, sim)}$ \\
        \hline
        Total & \textbf{0.31} & \hl{\textbf{0.31}} & 0.29 
        % & 0.24 
        & 0.29 \\
        MMLU & \textbf{0.36} & 0.33 & 0.34 
        % & 0.28 
        & 0.30 \\
        MMLU-Pro & \textbf{0.15} & \hl{\textbf{0.15}} & \hl{\textbf{0.15} }
        % & 0.11 
        & 0.14 \\
        ARC-Easy & 0.40 & \hl{0.40} & 0.36 & 
        % \cellcolor{blue!15}0.31 & 
        \hl{\textbf{0.41}} \\
        ARC-Challenge & \textbf{0.40} &0.37 & 0.33 & 
        % \cellcolor{blue!15}0.30 & 
       0.39 \\
        IFeval & 0.28 & \hl{\textbf{0.30}} & \hl{0.29} & 
        % 0.22 &
        0.23 \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{Performance Comparison Between Instance-wise Binarized Data (Baseline) and Kinship-based Binarized Data (Ours)
The baseline corresponds to instance-wise scored RLAIF \cite{ultrafeedback}. The \hl{highlighted} cells indicate performance that is equal to or higher than the baseline. \textbf{Bold} text shows the best performance on the benchmarks.}
    \label{tab:performance_comparison}
\end{table}


\begin{figure}
    \includegraphics[width=1\linewidth]{figures/benchmark_kinship.pdf}
    \caption{Visualization of relationships among origin models using PCA\cite{PCA}. Similar models are positioned closer to each other.}
    \label{fig:Q1_kinship_vis}
\end{figure}

\section{Experiments}
In this paper, we propose the KARMA framework. To validate the functionality of this framework and the characteristics of Alignment Tuning, we conducted experiments to address the following Research Questions (RQs):  
\begin{itemize}  
   \item \textbf{RQ1}: Does Model Kinship represent the models' similarity?
   \item \textbf{RQ2}: Does Model Kinship affect the binarization process?  
   \item \textbf{RQ3}: Does Model Kinship have a greater influence than model quality, as verified through benchmark evaluations?
   \item \textbf{RQ4}: What benchmark represent the model capability: knowledge, instruction-following, and reasoning?
\end{itemize}  

\subsection{Alignment Dataset}
For preference binarization, we utilized the Ultrafeedback dataset \cite{ultrafeedback}, which comprises responses generated by a diverse set of language models. The dataset includes outputs from both commercial and open-source models, ensuring a broad representation of model capabilities. The commercial models incorporated in the dataset include GPT-4\cite{gpt4}, GPT-3.5 Turbo\cite{InstructGPT}, and Bard\cite{bard}. Additionally, several models from the Llama family were included, such as Llama-2 (7B, 13B, and 70B)-chat\cite{llama2}, UltraLM-13B\cite{ultralm}, WizardLM (7B, 13B, and 70B)\cite{wizardlm}, Vicuna-33B\cite{vicuna}, and Alpaca-7B\cite{alpaca}. Beyond the Llama-based architectures, the dataset also features responses from other notable models, including Falcon-40B-instruct\cite{falcon40b}, MPT-30B-chat\cite{MPT}, StarChat-Beta\cite{starchat}, and Pythia-12B\cite{pythia}. 

Although the Ultrafeedback dataset contains results from UltraLM-65B\cite{ultralm}, its performance could not be accurately assessed. To maintain the reliability of our evaluation, we excluded these results from the final dataset composition.

\subsection{Models}

To assess the effectiveness of the newly binarized dataset constructed using the KARMA framework, we fine-tuned and evaluated multiple instruction-following models. The selected models for evaluation include Llama-3.1-(3B, 8B)-Instruct\cite{llama3}, representing Llama-based architectures. Additionally, we included Qwen2.5-(3B, 8B)-Instruct\cite{qwen25}, enabling a comparative analysis across different model families. This evaluation setup ensures a comprehensive assessment of the impact of kinship-aware preference mapping on alignment performance.


% \subsection{Alignment Dataset}
% Binarization was conducted using Ultrafeedback\cite{ultrafeedback} dataset, which relies on the responses from various models.
% The list of models used to generate the dataset is provided below:

% \begin{itemize}
%     \item \textbf{Commercial Models:}
%     \begin{itemize}
%         \item GPT-4
%         \item GPT-3.5 Turbo
%         \item Bard
%     \end{itemize}
%     \item \textbf{Llama Family:}
%         \item Llama-2-(7B, 13B, 70B)-chat
%         \item UltraLM-13B
%         \item WizardLM-(7B, 13B, 70B)
%         \item Vicuna-33B
%         \item Alpaca-7B
%     \item \textbf{Non-Llama Series:}
%         \item Falcon-40B-instruct
%         \item MPT-30B-chat
%         \item StarChat-Beta
%         \item Pythia-12B
% \end{itemize}

% Although the Ultrafeedback dataset includes results from UltraLM-65B, its performance could not be accurately assessed. As a result, these results were excluded from the final dataset composition to ensure reliable evaluation.

% \subsection{Models} 
% To evaluate the effectiveness of the newly binarized dataset created using the KARMA Framework, we fine-tuned and assessed the following models:

% \begin{itemize}
%     \item meta-llama/Llama-3.1-3B-Instruct
%     \item meta-llama/Llama-3.1-8B-Instruct
%     \item qwen/Qwen2.5-3B-Instruct
%     \item qwen/Qwen2.5-7B-Instruct
% \end{itemize}

\subsection{Training Algorithm for Alignment Tuning}
We tested two different learning method for alignment tuning using the KARMA Framework:
\vspace{-0.7em}
\begin{itemize}
    \item Supervised Fine-Tuning (SFT)
    \item Direct Preference Optimization (DPO) \cite{dpo}
\end{itemize}
\vspace{-0.7em}
The implementation details are provided in Appendix \ref{app:implementation_detail}

\begin{figure*}
\centering
\begin{subfigure}{0.3\linewidth}
    \includegraphics[width=\linewidth]{figures/knowledge_performance.pdf}
    \caption{Knowledge}
    \label{fig:Knowledge}
\end{subfigure}
\begin{subfigure}{0.3\linewidth}
    \includegraphics[width=\linewidth]{figures/ifeval_performance.pdf}
    \caption{Instruction-following}
    \label{fig:Instruction_following}
\end{subfigure}
\begin{subfigure}{0.38\linewidth}
    \includegraphics[width=\linewidth, height=0.7\textwidth]{figures/reasoning_performance.pdf}\vspace{+15pt}
    \caption{Reasoning}
    \label{fig:Reasoning}
\end{subfigure}
\caption{Performance of Instance-wise and Kinship-based Binarization. The \textcolor{red}{red} bars represent the performance of instance-wise annotated binarization, serving as the baseline. The \textcolor{blue}{blue}-toned bars correspond to the Kinship-based cases generated through KARMA.}
\label{fig:performance}
\end{figure*}

% \subsubsection{Implementation Details}
% The model was trained for a single epoch using bfloat16 (bf16) quantization to optimize memory efficiency while maintaining numerical precision. The training process was configured with the following hyperparameters:

% \begin{itemize}
%     \item Number of epochs: 1
%     \item Per-device batch size: 6
%     \item Gradient accumulation steps: 4
%     \item Learning rate: 5e-5
%     \item Warm-up steps: 500
% \end{itemize}

% The training was conducted on L40 4 GPUs using an optimized deep learning framework. The training process did not include model evaluation during training, focusing solely on optimizing performance through checkpointing and logging.

\section{Experimental Results}

\begin{figure*}[t]
    \centering
\begin{subfigure}{0.23\linewidth}
    \includegraphics[width=\linewidth, height=4cm]{figures/Qwen_2.5_3B_Instruct.pdf}
    \caption{Qwen-2.5-3B}
\end{subfigure}
\begin{subfigure}{0.23\linewidth}
    \includegraphics[width=\linewidth, height=4cm]{figures/Llama_3.1_3B_Instruct.pdf}
    \caption{Llama-3.1-3B}
\end{subfigure}
\begin{subfigure}{0.23\linewidth}
    \includegraphics[width=\linewidth, height=4cm]{figures/Qwen_2.5_7B_Instruct.pdf}
    \caption{Qwen-2.5-7B}
\end{subfigure}
\begin{subfigure}{0.23\linewidth}
    \includegraphics[width=\linewidth, height=4cm]{figures/Llama_3.1_8B_Instruct.pdf}
    \caption{Llama-3.1-8B}
\end{subfigure}
    \caption{Visualization of performance across different models. This represents the performance when using the DPO training method. For smaller models, using $K_{score}$ results in better performance, whereas for larger models, $K_{sim}$ yields better performance}
    \label{fig:model_size_kinship}
\end{figure*}

\subsection{RQ1: Dataset Reconstruction using KARMA}
The proposed KARMA framework enables the application of a unified Total Ranking map across the entire dataset, facilitating a structured and consistent preference mapping process. Utilizing this reconstructed preference dataset, we conducted model training while ensuring that each critic’s binarized response was systematically incorporated. The effectiveness of this reconstructed dataset was assessed by evaluating the trained models on standardized benchmarks, including MMLU and IFeval. The detailed results of these evaluations are presented in Appendix \ref{app:benchmarks}.

For evaluation, we employed prediction-based assessment methodologies across all benchmark tasks. Specifically, for ARC, MMLU, and MMLU-Pro, we adapted the MMLU-Pro evaluation framework, modifying only the multiple-choice options to align with our dataset. For IFeval, we leveraged its native evaluation framework to ensure consistency in assessment.

To analyze the relationships between models, we computed kinship scores by normalizing evaluation results across six benchmark tasks. Figure \ref{fig:Q1_kinship_vis} presents a Principal Component Analysis (PCA)\cite{PCA} visualization of these relationships, illustrating distinct clustering patterns among models. Generally, smaller-scale models tend to cluster in the first quadrant, models with closer kinship relationships in the second quadrant, while Llama-based models and models exceeding 10B parameters are predominantly distributed in the third and fourth quadrants. This distribution underscores the critical role of model training data, training algorithms, and scale in determining kinship relationships among models.

These findings highlight the effectiveness of the KARMA framework in reconstructing preference datasets, providing a more structured and informative approach to model alignment.


% \subsection{RQ1: Dataset Reconstruction using KARMA}
% The proposed KARMA in this paper enables the application of a single Total Ranking map across the entire dataset. Using this reconstructed preference dataset, we proceeded with training. Each Critic’s binarized response for the model was reconstructed, and performance was evaluated on the trained model using MMLU and IFeval. The results are shown in Appendix \ref{app:benchmarks}.

% For evaluation, we employed prediction-based evaluation methods across all benchmarks. For ARC, MMLU, and MMLU-pro evaluations, we adapted the MMLU-pro evaluation code, modifying only the multiple-choice options. For IFeval, we utilized its provided evaluation framework.

% To aggregate results from the 6 evaluation benchmarks, we calculated kinship using normalized values. Figure \ref{fig:Q1_kinship_vis} illustrates the relationships between models through PCA visualization.
% Generally, smaller-scale models are distributed in the first quadrant, closed models in the second quadrant, and Llama-based models and models larger than 10B parameters are distributed in the third and fourth quadrants.
% Indeed, a model's training data, training algorithms, and scale are key factors in determining kinship between models.


\subsection{RQ2: Performance of KARMA Binarization}
To assess the effectiveness of KARMA binarization, we examined whether model performance can serve as an indicator of data quality. KARMA performs binarization in two sequential steps: first, it defines preference pairs based on kinship between models, and second, it ranks these pairs according to their relative performance. Through this process, we aimed to determine whether response similarity—estimated through model similarity—plays a more significant role in preference modeling than simply prioritizing higher-performing models by consistently positioning them as dominant within each pair.

The results of this comparison are presented in Figure \ref{fig:performance}. Across all benchmark tasks, the kinship-based scoring metric, $K_{score}$, demonstrates performance levels nearly equivalent to the existing instance-wise RLAIF method, with a minimal deviation of only 0.008. Notably, for MMLU-Pro and IFeval, KARMA-based binarization even surpasses the RLAIF baseline by approximately 0.01, indicating that structured preference mapping via kinship can yield competitive or superior alignment outcomes. The detailed results, including average scores for each methodology, are summarized in Appendix \ref{app:total_performance}.



% \subsection{RQ2: Performance of KARMA binarization}
% We verified that KARMA enables us to make inferences about data quality based on model performance. KARMA performs binarization in two steps: first, it defines pairs through kinship between models, and then it ranks these pairs according to their performance. In this process, we aimed to examine whether response similarity, as estimated through model similarity, plays a more crucial role than prioritizing higher-performing models by positioning them as dominant in the pair.

% Figure \ref{fig:performance} show the results of the comparison.

% When comparing average performance across all tasks, $K_{score}$ achieves nearly equivalent performance to the existing instance-wise RLAIF, with only a 0.008 deviation. Notably, for MMLU-Pro and IFeval, it even outperforms the baseline by approximately 0.01. Table \ref{tab:performance_comparison} shows the results of the average scores for each methodology..

\subsection{RQ3: Model Size and Kinship}
The impact of different kinship approaches varies significantly with model size, influencing how models learn from preference data. To investigate this relationship, we conducted experiments using models from the same family but with different parameter counts, specifically comparing small models (3B parameters) and larger models (7-8B parameters). Our analysis reveals a clear pattern in how model size determines the optimal kinship strategy.

Smaller models (3B) exhibit superior performance when trained using the $K_{score}$ algorithm, which prioritizes learning from response quality within the binarized dataset. In contrast, larger models (7-8B) achieve better results with the $K_{sim}$ algorithm, suggesting that response similarity becomes increasingly important as model size grows. 

This trend indicates that model size fundamentally influences how different models leverage preference data. While smaller models benefit more from explicit learning based on absolute quality differences, larger models demonstrate greater sensitivity to the nuanced relationships between similar responses in the training data. This relationship is visualized in Figure \ref{fig:model_size_kinship}, illustrating the distinct learning behaviors observed across different model scales.


% \subsection{RQ3: Model size and Kinship}
% The effectiveness of different Kinship approaches varies significantly with model size. To investigate this relationship, we conducted experiments using models from the same family but with different parameter counts: small models (3B parameters) and large models (7-8B parameters). Our analysis reveals a clear pattern in how model size influences the optimal Kinship strategy.

% Smaller models (3B) show superior performance when using the $K_{score}$ algorithm, which prioritizes learning from response quality in the binarized data. In contrast, larger models (7-8B) achieve better results with the $K_{sim}$ algorithm, indicating that the similarity between response pairs becomes more critical as model size increases.
% This pattern suggests that model size fundamentally affects how different models learn from preference data. While smaller models benefit most from learning based on absolute quality differences, larger models are more sensitive to the nuanced relationships between similar responses in the training data. This trend can be observed in Figure \ref{fig:model_size_kinship}.

\begin{figure}[]
\centering
\begin{subfigure}{0.48\textwidth}
    \includegraphics[width=\linewidth]{figures/total_dendrogram.pdf}
    \caption{Total}
    \label{fig:Knowledge}
\end{subfigure}

\begin{subfigure}{0.48\textwidth}
    \includegraphics[width=\linewidth]{figures/Knowledge_dendrogram.pdf}
    \caption{Knowledge}
    \label{fig:Knowledge}
\end{subfigure}

\begin{subfigure}{0.48\textwidth}
    \includegraphics[width=\linewidth]{figures/Reasoning_dendrogram.pdf}
    \caption{Reasoning}
    \label{fig:Reasoning}
\end{subfigure}

\begin{subfigure}{0.48\textwidth}
    \includegraphics[width=\linewidth]{figures/IF_dendrogram.pdf}
    \caption{Instruction-following}
    \label{fig:Instruction_following}
\end{subfigure}
\caption{Dendrogram for Evaluation Categories: Knowledge, Reasoning, and Instruction-Following. Clusters exceeding a specific distance threshold (0.4) are highlighted in different colors. Models deemed similar share the same color line. The \textcolor{red}{red} dotted line shows the major groups of models in the dendrogram.}
\label{fig:benchmark_similarity}
\end{figure}

\subsection{RQ4: Model Capability and Kinship Patterns}
Models exhibit distinct similarity patterns based on their capabilities, with these patterns varying across different evaluation metrics. Notably, model kinship is influenced by factors such as model scale and training methodology, leading to variations in clustering behavior across different capability dimensions.

\paragraph{Analysis Framework}
To systematically investigate these relationships, we analyzed model similarities across three key capability dimensions:

\begin{itemize}
    \item \textbf{Knowledge-Based Tasks}:     Unlike reasoning capability, clustering in instruction-following tasks is more strongly aligned with \textit{model families} rather than size. This indicates that training methodology and architectural choices exert a greater influence on instruction adherence.
    
    \item \textbf{Reasoning Capability}:  
    Models tend to cluster primarily based on parameter count, suggesting that \textit{model size} plays a dominant role in shaping reasoning performance.
    
    \item \textbf{Instruction-Following (IF) Tasks}: 
    A hierarchical influence pattern emerges, where:
    \begin{itemize}
        \item At the initial hierarchy, the \textit{model family} is the primary determinant.
        \item At the higher hierarchy, the \textit{model size} becomes a stronger predictor of performance.
    \end{itemize}
\end{itemize}

Figure~\ref{fig:benchmark_similarity} visualizes these relationships through dendrograms, illustrating the hierarchical clustering patterns that emerge across different capability dimensions.


%\subsection{RQ4: Model Capability and Kinship Patterns}
% Different models exhibit distinct similarity patterns based on their capabilities. These patterns vary according to different evaluation metrics of model capability, showing particular variation across model scales and training methodologies.
% \paragraph{Analysis Framework}
% To systematically analyze these relationships, we examined model similarities across three key capability dimensions:

% \begin{itemize}
% \item \textbf{Reasoning Capability}: Models show stronger clustering based on their parameter count, suggesting that model size is the dominant factor in determining reasoning capabilities.
% \item \textbf{Instruction Following (IF) Tasks}: The primary clustering pattern aligns with model families rather than size, indicating that training methodology and architectural choices have a more significant impact on instruction-following ability.

% \item \textbf{Knowledge-based Tasks}: We observe a hierarchical influence pattern where:
%     \begin{itemize}
%         \item Lower-tier performance is primarily determined by model size
%         \item Upper-tier performance shows stronger correlation with model family
%     \end{itemize}
% \end{itemize}

% Figure~\ref{fig:benchmark_similarity} presents these relationships through dendrograms, clearly visualizing the hierarchical clustering patterns across different capability dimensions.

% \section{Ablation Study}
% \subsection{Training Tendencies of Model Competency Criteria: Knowledge, Reasoning, and Instruction-Following}
% We conducted binarization on the three model competency categories proposed in this paper to evaluate the benchmark performance of the trained models. This approach allows us to verify whether the required competencies can be effectively adjusted. The results of this evaluation are shown in Figure \ref{fig:competency_evaluation}.

% \begin{table}[]
% \centering
% \begin{adjustbox}{width=0.5\textwidth}
% % Please add the following required packages to your document preamble:
% % \usepackage{multirow}
% \begin{tabular}{cc|c|c|c}
% \hline &       & Kinship(IF) & Kinship(R) & Kinship(K) \\ \hline
% \multicolumn{1}{c|}{\multirow{4}{}{Llama-3.1-3B-Instruct}} & SFT   & -       & -    & -  \\ \cline{2-5} 
% \multicolumn{1}{c|}{} & DPO   & -       & -    & -   \\ \hline
% \multicolumn{1}{c|}{\multirow{4}{}{Llama-2-7b-chat}}       & SFT   & -       & -    & -   \\ \cline{2-5} 
% \multicolumn{1}{c|}{} & DPO   & -       & -    & -   \\ \hline
% \multicolumn{1}{c|}{\multirow{4}{}{Qwen2.5-3B-Instruct}}   & SFT   & -       & -    & -    \\ \cline{2-5}  
% \multicolumn{1}{c|}{} & DPO   & -       & -    & -   \\ \hline
% \end{tabular}
% \end{adjustbox}
% \caption{\textcolor{red}{Ablation} Benchmark scores for the Trained Models.}
% \label{tab:Ablation}
% \end{table}

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{latex/tmp_results/competency_evaluation.png}
%     \caption{Relation between kinship competency element and the benchmark score.}
%     \label{fig:competency_evaluation}
% \end{figure}

\section{Conclusion}
This paper introduces KARMA, a kinship-based framework for generating high-quality preference data for language model alignment without requiring instance-wise annotation. KARMA achieves performance comparable to RLAIF while offering greater computational efficiency and surpassing baselines on MMLU-Pro and IFeval, demonstrating the effectiveness of kinship-aware preference mapping. Our findings show that model size significantly affects the optimal kinship strategy. Smaller models (3B) benefit from prioritizing response quality, while larger models (7-8B) perform better when leveraging response similarity. Additionally, kinship patterns vary across tasks, with reasoning clustering by model size, instruction-following by model family, and knowledge-based tasks exhibiting a hierarchical structure. These results highlight the need for size-adaptive preference learning strategies and demonstrate that high-quality preference data can be generated without costly manual annotation. Future work will extend KARMA to cross-family model comparisons and further explore the relationship between model kinship and human preference judgments.
  


% \section{Conclusion}
% % annotation을 하지 않고도 충분한 pairing 성능을 볼 수 있었으며, 특히 reasoning에 대해서 성능적 이득을 볼 수 있었음
% This paper introduces KARMA, a novel kinship-based approach for generating high-quality preference data for language model alignment. Through comprehensive experiments and analysis, we demonstrate several key findings:
% First, KARMA generates preference data without instance-wise annotation, achieving comparable performance to RLAIF while being more computationally efficient, and even outperforming baselines on MMLU-Pro and IFeval tasks.
% Second, model size significantly influences kinship effectiveness, with smaller models (3B) favoring $K_{score}$ and larger models (7-8B) performing better with $K_{sim}$, indicating the need for size-adapted preference learning strategies.
% Third, capability analysis reveals that reasoning tasks cluster by model size, instruction-following by model families, and knowledge tasks show a hierarchical pattern combining both factors.

% These findings suggest that effective alignment strategies should consider both model size and architecture when designing preference learning approaches. More importantly, KARMA demonstrates that high-quality preference data can be generated without relying on resource-intensive instance-wise annotations, providing a more scalable and efficient framework for model alignment.

% Future work could explore extending KARMA's zero-annotation approach to cross-family model comparisons and investigating how these patterns scale to even larger language models. Additionally, understanding the relationship between model kinship and human preferences remains an important area for further research.

\section{Related Work and Background}
\subsection{Alignment Tuning}
Alignment tuning has become a central focus in enhancing LLMs to meet user expectations and ethical standards\cite{sage_rt}. Various preference-based learning techniques, particularly reinforcement learning methods such as Proximal Policy Optimization (PPO)\cite{ppo}, Direct Preference Optimization (DPO)\cite{dpo}, and Optimized Reinforcement Preference Optimization (ORPO)\cite{orpo}, have been developed to facilitate this tuning process. These methods rely on preference datasets, which typically contain pairs of responses generated by models based on given instructions, with each pair ranked according to human or model-based evaluations. Prominent alignment datasets like \textit{Ultrafeedback\cite{ultrafeedback}}, which gathers extensive human feedback, and \textit{HH-RLHF\cite{hh_rlhf}}, which uses human-annotated preferences, provide foundational resources for alignment. To minimize the reliance on labor-intensive data curation, recent automated approaches have been introduced to filter or regenerate preference data based on specific criteria, promoting data consistency and scalability. However, such approaches often lack nuanced control over data quality, as they fail to consider the role of the origin model’s capabilities in shaping alignment effectiveness \cite{safer_instruct}.

\subsection{Limitations in Existing Preference Data Approaches}
Effective preference data construction requires a clear, rigorous set of criteria to ensure alignment quality across generated response pairs. Common criteria, including \textit{Reasoning}, \textit{Truthfulness}, and \textit{Instruction-Following}, guide the selection of data that aligns with key ethical and functional standards\cite{ultrafeedback, hh_rlhf}. High-performing models, capable of producing responses that meet these standards, are often evaluated using benchmarks like \textit{ARC\cite{arc}}, \textit{MMLU\cite{mmlu}}, and the \textit{Instruction-Following eval\cite{ifeval}} suite, which assess a model’s factual accuracy, reasoning ability, and compliance with instructions. While these benchmarks provide a foundation for selecting responses that promote alignment goals, existing methods typically overlook the impact of variation in model capabilities on alignment consistency. 

The \textit{KARMA Framework} addresses this limitation by introducing a kinship-aware approach that emphasizes model compatibility in preference data selection. By curating preference pairs based on model kinship—aligning models with similar core competencies in knowledge, reasoning, and instruction-following—the KARMA Framework enhances alignment coherence and ensures stable, high-quality data. Further, its curriculum-based selection process sequences preference data from simpler to more complex distinctions, optimizing the alignment process for improved consistency and scalability.


\section*{Limitation}
In this paper, we demonstrated that KARMA can achieve performance comparable to existing RLAIF without requiring annotations for SFT and DPO. However, due to resource and time constraints, we were unable to validate our approach across a broader range of alignment tuning techniques. Further evaluation is needed for methods.

Additionally, our evaluation primarily focused on fundamental abilities such as Reasoning, Knowledge, and Instruction-Following. However, we did not assess KARMA’s performance on other important values, including factuality and ethical standard. Future work should incorporate evaluations reflecting these aspects.

% \section{Acknowledgments}

% This document has been adapted by 

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\appendix
\section{Implementation Details for Model Training}\label{app:implementation_detail}
The model was trained for a single epoch using bfloat16 (bf16) quantization, which optimizes memory efficiency while preserving numerical precision. The training configuration incorporated the following hyperparameters: a per-device batch size of 6, gradient accumulation steps set to 4, and a learning rate of $5 \times 10^{-5}$, with 500 warm-up steps to facilitate stable convergence. 

Training was conducted on an L40 4-GPU setup, leveraging an optimized deep learning framework to enhance computational efficiency. The training pipeline focused on performance optimization through checkpointing and logging, without intermediate model evaluation during training.

\section{Prompt template for Evaluation Benchmarks}
\label{app:prompt}
To determine kinship relationships between models, this paper evaluates benchmark performance and compares the similarity of these numerical results. The benchmark performance of all 17 models considered in this study is presented in Table \ref{fig:prompt_template}. For the start and end tokens, the template recommended in the paper was used.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/evaluation_template.pdf}
    \caption{Evaluation template for multiple-choice benchmark evaluation. The descriptive benchmark (e.g., IFeval) was evaluated using the same template, excluding the option part.}
    \label{fig:prompt_template}
\end{figure}

\section{Evaluation Origin Model Capabilities for Kinship}
\label{app:benchmarks}
To determine kinship relationships between models, this paper evaluates benchmark performance and compares the similarity of these numerical results. The benchmark performance of all 17 models considered in this study is presented in Table \ref{tab:benchmark_performance}. The actual calculation example for \( K_{score} \) and \( K_{sim} \) can be found in Figure \ref{fig:example_sim}.

Due to the unavailability of Bard, its performance metrics have been substituted with those of Gemini-1.5-Flash.


\begin{table*}[t]
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{c|cccccc|c}
\toprule
     Model &  IFeval &  MMLU-STEM &  MMLU-pro &  Hellaswag &  ARC-easy &  ARC-Challenge &  Average Score \\
\midrule
     GPT-4 &        85.37 & 86.40 &      0.64 &      95.30 &     96.63 & 96.40 &  0.99 \\
      GPT-3.5-turbo &        72.54 & 70.00 &      0.38 &      85.00 &     92.80 & 83.02 &  0.77 \\
      bard &        89.33 & 71.80 &      0.59 &      84.70 &     84.43 & 77.13 &  0.86 \\
    Llama-2-7b-chat &        25.19 & 53.10 &      0.20 &      67.50 &     72.14 & 54.61 &  0.41 \\
   Llama-2-13b-chat &        24.82 & 57.80 &      0.25 &      71.20 &     72.05 & 58.02 &  0.45 \\
   Llama-2-70b-chat &        24.07 & 68.90 &      0.38 &      78.10 &     82.20 & 67.66 &  0.58 \\
        UltraLM-13b &        54.92 & 49.58 &      0.19 &      54.00 &     57.58 & 48.04 &  0.40 \\
        WizardLM-7b &        45.83 & 42.50 &      0.18 &      77.70 &     39.48 & 34.90 &  0.34 \\
       WizardLM-13b &        33.92 & 52.30 &      0.17 &      81.00 &     72.94 & 55.38 &  0.45 \\
       WizardLM-70b &        49.51 & 52.70 &      0.27 &      83.30 &     80.68 & 71.93 &  0.58 \\
         Vicuna-33b &        52.76 & 64.00 &      0.23 &      75.00 &     81.57 & 64.51 &  0.57 \\
 Alpaca-7b &        30.58 & 37.92 &      0.15 &      23.60 &     43.31 & 33.79 &  0.16 \\
Falcon-40b-instruct &        24.54 & 67.50 &      0.14 &      80.00 &     76.60 & 56.70 &  0.47 \\
       MPT-30b-chat &        30.70 & 50.40 &      0.20 &      24.53 &     87.12 & 70.73 &  0.38 \\
  Starchat &        28.30 & 40.12 &      0.12 &      25.40 &     16.96 &  9.07 &  0.05 \\
         Pythia-12b &        24.71 & 27.00 &      0.12 &      25.60 &     24.49 & 31.80 &  0.07 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Benchmark Scores for Trained Models. Multiple-choice benchmarks (MMLU-STEM, HellaSwag, ARC-Easy, and ARC-Challenge) are evaluated based on accuracy. IFeval and MMLU-Pro are assessed using its own metric. The average score is computed after min-max normalization.}
\label{tab:benchmark_performance}
\end{table*}

\begin{figure*}
    \includegraphics[width=1\linewidth]{figures/example_similarity.pdf}
    \caption{The process of calculating \( K_{score} \) and \( K_{sim} \). Benchmark performance is normalized using min-max normalization, and the overall kinship scoring is performed using cosine similarity.}
    \label{fig:example_sim}
\end{figure*}


\section{Total Performance}
\label{app:total_performance}

Table \ref{tab:total_performance} provides an aggregated view of the overall performance of each model across all benchmark datasets. The total performance scores were computed by averaging the normalized scores across the selected evaluation metrics, offering a holistic comparison of model capabilities.

% Table \ref{tab:total_performance} serves as a reference for identifying trends and relative strengths among different models, facilitating deeper insights into their comparative performance.

\begin{table*}[]
    \centering
    \begin{tabular}{c|c|c|c|c|c|c}
\hline
Benchmark  & \multicolumn{1}{c|}{Model}    & \multicolumn{1}{c|}{} & Baseline & $K_{score}$        & $K_{sim}$ & $K_{(score,sim)}$    \\ \hline
\multicolumn{1}{c|}{\multirow{8}{*}{MMLU-STEM}} & \multirow{2}{*}{Llama-3.1-3B} & SFT & 0.2331 & \textbf{0.2679} & 0.2434 & 0.2200 \\ \cline{3-7} 
\multicolumn{1}{c|}{}         &  & DPO & 0.3130 & \textbf{0.4868} & 0.2528 & 0.3940 \\ \cline{2-7} 
\multicolumn{1}{c|}{}         & \multirow{2}{*}{Llama-3.1-8B} & SFT & \textbf{0.2800} & 0.2460 & 0.2660 & 0.2240 \\ \cline{3-7} 
\multicolumn{1}{c|}{}         &  & DPO & \textbf{0.4902}   & 0.4349 & 0.4867 & 0.2880 \\ \cline{2-7} 
\multicolumn{1}{c|}{}         & \multirow{2}{*}{Qwen2.5-3B}   & SFT & \textbf{0.2760}   & 0.2200 & 0.2460 & 0.2500 \\ \cline{3-7} 
\multicolumn{1}{c|}{}         &  & DPO & \textbf{0.4706}   & 0.4212 & 0.4400 & 0.4580 \\ \cline{2-7} 
\multicolumn{1}{c|}{}         & \multirow{2}{*}{Qwen2.5-7B}   & SFT & \textbf{0.2800}   & 0.2280 & 0.2620 & 0.2780 \\ \cline{3-7} 
\multicolumn{1}{c|}{}         &  & DPO & \textbf{0.5120}   & 0.3620 & 0.4860 & 0.2800 \\ \hline

\multirow{8}{*}{MMLU-pro}      & \multirow{2}{*}{Llama-3.1-3B} & SFT & 0.1189   & 0.1230 & 0.1045 & \textbf{0.1332} \\ \cline{3-7} 
    &    & DPO & 0.1455   & \textbf{0.2193} & 0.1270 & 0.1516 \\ \cline{2-7} 
    & \multirow{2}{*}{Llama-3.1-8B} & SFT & 0.1004   & 0.1148 & 0.1045 & \textbf{0.1311} \\ \cline{3-7} 
    &    & DPO & \textbf{0.1168}   & 0.1025 & \textbf{0.1168} & 0.1107 \\ \cline{2-7} 
    & \multirow{2}{*}{Qwen2.5-3B}   & SFT & 0.1025   & 0.0840 & \textbf{0.1230} & 0.0820 \\ \cline{3-7} 
    &    & DPO & \textbf{0.2275}   & 0.2254 & 0.2029 & 0.2111 \\ \cline{2-7} 
    & \multirow{2}{*}{Qwen2.5-7B}   & SFT & 0.0861   & 0.1762 & \textbf{0.2275} & 0.1352 \\ \cline{3-7} 
    &    & DPO & \textbf{0.2377}   & 0.1721 & 0.2254 & 0.1700 \\ \hline
\multirow{8}{*}{IFeval}        & \multirow{2}{*}{Llama-3.1-3B} & SFT & \textbf{0.2494}   & 0.2410 & 0.2490 & 0.2206 \\ \cline{3-7} 
    &    & DPO & 0.3765   & \textbf{0.4940} & 0.4796 & 0.3033 \\ \cline{2-7} 
    
    & \multirow{2}{*}{Llama-3.1-8B} & SFT & 0.2494   & \textbf{0.4210} & 0.2470 & 0.2218 \\ \cline{3-7} 
    &    & DPO & 0.2421   & 0.1882 & 0.2292 & \textbf{0.2494} \\ \cline{2-7} 
    & \multirow{2}{*}{Qwen2.5-3B}   & SFT & \textbf{0.2190}   & 0.2134 & 0.2122 & 0.2050 \\ \cline{3-7} 
    &    & DPO & \textbf{0.3633}   & 0.3058 & 0.3094 & 0.1715 \\ \cline{2-7} 
    & \multirow{2}{*}{Qwen2.5-7B}   & SFT & \textbf{0.2290}   & 0.2134 & 0.2122 & 0.2083 \\ \cline{3-7} 
    &    & DPO & 0.3321   & 0.3177 & \textbf{0.3657} & 0.2407 \\ \hline    
\multirow{8}{*}{ARC-easy}      & \multirow{2}{*}{Llama-3.1-3B} & SFT & \textbf{0.2660}   & 0.2460 & 0.2000 & 0.2340 \\ \cline{3-7} 
    &    & DPO & 0.6111   & 0.5547 & 0.2618 & \textbf{0.6679} \\ \cline{2-7} 
    & \multirow{2}{*}{Llama-3.1-8B} & SFT & 0.2180   & 0.2330 & \textbf{0.2400} & 0.2360 \\ \cline{3-7} 
    &    & DPO & 0.2176   & 0.1540 & \textbf{0.4720} & 0.0680 \\ \cline{2-7} 
    & \multirow{2}{*}{Qwen2.5-3B}   & SFT & 0.2410   & \textbf{0.2560} & 0.2480 & \textbf{0.2560} \\ \cline{3-7} 
    &    & DPO & 0.8497   & \textbf{0.8754} & 0.5295 & 0.8157 \\ \cline{2-7}
    & \multirow{2}{*}{Qwen2.5-7B}   & SFT & 0.2550   & 0.2260 & 0.2280 & \textbf{0.2900} \\ \cline{3-7} 
    &    & DPO & 0.5700   & 0.6359 & 0.6738 & \textbf{0.7134} \\ \hline
    
\multirow{8}{*}{ARC-challenge} & \multirow{2}{*}{Llama-3.1-3B} & SFT & \textbf{0.2556}   & 0.2492 & 0.2266 & 0.2019 \\ \cline{3-7} 
    &    & DPO & 0.5122   & 0.4551 & 0.2466 & \textbf{0.5712} \\ \cline{2-7} 
    & \multirow{2}{*}{Llama-3.1-8B} & SFT & 0.2320   & 0.2297 & \textbf{0.2761} & 0.2268 \\ \cline{3-7} 
    &    & DPO & \textbf{0.5463}   & 0.1763 & 0.3596 & 0.2946 \\ \cline{2-7} 
    & \multirow{2}{*}{Qwen2.5-3B}   & SFT & \textbf{0.2645}   & 0.2483 & 0.2227 & 0.2343 \\ \cline{3-7} 
    &    & DPO & 0.7398   & \textbf{0.8241} & 0.4470 & 0.7078 \\ \cline{2-7} 
    & \multirow{2}{*}{Qwen2.5-7B}   & SFT & 0.2343   & \textbf{0.2552} & 0.2483 & 0.2390 \\ \cline{3-7} 
    &    & DPO & 0.4432   & 0.5358 & \textbf{0.5847} & 0.3870 \\ \hline

\end{tabular}
    \caption{Model Performance Comparisons on Knowledge, Instruction-Following, and Reasoning-Related Tasks. The baseline is Instance-wise RLAIF \cite{ultrafeedback}. "Llama 3.1" refers to the Llama-3.1-Instruct series, and "Qwen-2.5" refers to the Qwen2.5-Instruct series. The training methods include Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO). The \textbf{bold} text indicates the best performance for each model and training method.}
    \label{tab:total_performance}
\end{table*}



\end{document}
