[
  {
    "index": 0,
    "papers": [
      {
        "key": "sage_rt",
        "author": "Anurakt Kumar and Divyanshu Kumar and Jatan Loya and Nitin Aravind Birur and Tanay Baswa and Sahil Agarwal and Prashanth Harshangi",
        "title": "SAGE-RT: Synthetic Alignment data Generation for Safety Evaluation and Red Teaming"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "ppo",
        "author": "Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg",
        "title": "Proximal policy optimization algorithms"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "dpo",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "orpo",
        "author": "Hong, Jiwoo  and\nLee, Noah  and\nThorne, James",
        "title": "{ORPO}: Monolithic Preference Optimization without Reference Model"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "ultrafeedback",
        "author": "Cui, Ganqu and Yuan, Lifan and Ding, Ning and Yao, Guanming and Zhu, Wei and Ni, Yuan and Xie, Guotong and Liu, Zhiyuan and Sun, Maosong",
        "title": "Ultrafeedback: Boosting language models with high-quality feedback"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "hh_rlhf",
        "author": "Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others",
        "title": "Training a helpful and harmless assistant with reinforcement learning from human feedback"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "safer_instruct",
        "author": "Shi, Taiwei  and\nChen, Kai  and\nZhao, Jieyu",
        "title": "Safer-Instruct: Aligning Language Models with Automated Preference Data"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "ultrafeedback",
        "author": "Cui, Ganqu and Yuan, Lifan and Ding, Ning and Yao, Guanming and Zhu, Wei and Ni, Yuan and Xie, Guotong and Liu, Zhiyuan and Sun, Maosong",
        "title": "Ultrafeedback: Boosting language models with high-quality feedback"
      },
      {
        "key": "hh_rlhf",
        "author": "Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others",
        "title": "Training a helpful and harmless assistant with reinforcement learning from human feedback"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "arc",
        "author": "Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind",
        "title": "Think you have solved question answering? try arc, the ai2 reasoning challenge"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "mmlu",
        "author": "Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob",
        "title": "Measuring massive multitask language understanding"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "ifeval",
        "author": "Zhou, Jeffrey and Lu, Tianjian and Mishra, Swaroop and Brahma, Siddhartha and Basu, Sujoy and Luan, Yi and Zhou, Denny and Hou, Le",
        "title": "Instruction-following evaluation for large language models"
      }
    ]
  }
]