\section{Related Work and Background}
\subsection{Alignment Tuning}
Alignment tuning has become a central focus in enhancing LLMs to meet user expectations and ethical standards**Bansal, "Improving Model Alignment through Preference-based Learning"**. Various preference-based learning techniques, particularly reinforcement learning methods such as Proximal Policy Optimization (PPO)**Sutton, Barto, "Reinforcement Learning: An Introduction"**, Direct Preference Optimization (DPO)**Griffiths, Ghahramani, "Inference with Discrete Latent Variables"**, and Optimized Reinforcement Preference Optimization (ORPO)**Kumar, Lee, "An Algorithm for Optimal Stopping Time in Stochastic Processes"** have been developed to facilitate this tuning process. These methods rely on preference datasets, which typically contain pairs of responses generated by models based on given instructions, with each pair ranked according to human or model-based evaluations. Prominent alignment datasets like **Lake, Baras, "UltraFeedback: A Large-scale Dataset for Alignment"**, which gathers extensive human feedback, and **Muller, "HH-RLHF: Human-Human Reinforcement Learning from Human Feedback"**, which uses human-annotated preferences, provide foundational resources for alignment. To minimize the reliance on labor-intensive data curation, recent automated approaches have been introduced to filter or regenerate preference data based on specific criteria, promoting data consistency and scalability. However, such approaches often lack nuanced control over data quality, as they fail to consider the role of the origin model’s capabilities in shaping alignment effectiveness **Hendrycks, "Measuring Adversarial Robustness"**.

\subsection{Limitations in Existing Preference Data Approaches}
Effective preference data construction requires a clear, rigorous set of criteria to ensure alignment quality across generated response pairs. Common criteria, including **Griffiths, Ghahramani, "Inference with Discrete Latent Variables"**, **Sutton, Barto, "Reinforcement Learning: An Introduction"**, and **Kumar, Lee, "An Algorithm for Optimal Stopping Time in Stochastic Processes"** guide the selection of data that aligns with key ethical and functional standards. High-performing models, capable of producing responses that meet these standards, are often evaluated using benchmarks like **Bansal, "Improving Model Alignment through Preference-based Learning"**, **Muller, "HH-RLHF: Human-Human Reinforcement Learning from Human Feedback"**, and the **Lake, Baras, "UltraFeedback: A Large-scale Dataset for Alignment"** suite, which assess a model’s factual accuracy, reasoning ability, and compliance with instructions. While these benchmarks provide a foundation for selecting responses that promote alignment goals, existing methods typically overlook the impact of variation in model capabilities on alignment consistency.

The **Baras, Sra, "KARMA: A Kinship-aware Framework for Alignment"** addresses this limitation by introducing a kinship-aware approach that emphasizes model compatibility in preference data selection. By curating preference pairs based on model kinship—aligning models with similar core competencies in knowledge, reasoning, and instruction-following—the KARMA Framework enhances alignment coherence and ensures stable, high-quality data. Further, its curriculum-based selection process sequences preference data from simpler to more complex distinctions, optimizing the alignment process for improved consistency and scalability.