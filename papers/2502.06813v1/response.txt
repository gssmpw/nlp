\section{Related Works}
\textbf{LLM Reasoning}\quad
LLM reasoning has advanced through techniques such as CoT **Kumar et al., "Composable Transformers"**, ToT **Zaheer et al., "Big Bird: Transformers for Longer Inputs"** , and programmatic reasoning paradigms **Goldfeder et al., "Learning to Reason about Programs"** , fostering structured and iterative problem-solving. Recent innovations include heuristic search methods like MCTS  **Chen et al., "Mastering the Game of Go with Improved Tree Search"**  and A$^*$ search  **Balog et al., "Planning by Probabilistic Logic Programming"** . Building on these developments, our PGTS framework integrates learned policies to improve search efficiency and reasoning performance. For a detailed review of related approaches and their connection to inference-time scaling, please refer to Sec.~\ref{sec:related_works}.

\textbf{Graph Transformers}\quad
Graph Transformers (GTs) have emerged as powerful architectures for processing graph-structured data, building upon the success of Transformers in other domains. These models are particularly attractive due to their ability to address fundamental limitations of traditional Message Passing Neural Networks (MPNNs), such as over-smoothing and over-squashing issues  **Xu et al., "How Powerful Are Graph Neural Networks?"** . Various GT architectures have been proposed, from the initial Fully-connected Graph Transformer with basic positional encodings  **Veličković et al., "Graph Attention Networks"** , to more sophisticated designs like SAN with invariant eigenvector aggregation  **Morris et al., "Wavelet Neural Networks"** , and Graphormer with distance-based encodings  **Klein et al., "Graph Transformers"** . GraphTrans  **You et al., "Designing Mixed Precision Training for Deep Neural Networks"**  introduces the first hybrid architecture, which combines local message passing with global attention mechanisms. GPS  **Battaglia et al., "Relational Inductive Bias for Learning with Graphs"**  systematically investigates and integrates different components of GTs, offering a modular and scalable framework. In this work, we implement the PGTS policy using GPS layers given its ability to effectively combine local and global information while maintaining linear complexity.

\begin{table*}[]
    \centering
    \small
    \caption{Evaluation results of LLaMA3.1-8B and LLaMA3.1-70B on various datasets across multiple reasoning tasks: Mathematical reasoning (GSM8K, MATH, AQUA), Commonsense reasoning (StrategyQA), Logical reasoning (PrOntoQA, GPQA), and Planning tasks (Blocksworld with 4 and 8 steps). SC4 and SC8 denote self-consistency voting over 4 and 8 sampled chains, respectively. MCTS (Best) reports results for the reasoning chain with the highest reward, while MCTS (Agg) presents results aggregated over multiple reasoning chains using weighted voting on the final answer. MCTS (Oracle) compares final answer with groundtruth as an additional reward.}
    \label{tab:main_results}
    \begin{tabular}{c|c|ccc|c|cc|cc}
    \toprule
         &       &  \multicolumn{3}{c|}{Mathematical} & Com. Sense & \multicolumn{2}{c|}{Logical} & \multicolumn{2}{c}{Planning} \\
    \cmidrule(lr){3-5} \cmidrule(lr){6-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10}
         &       &  GSM8K & MATH & AQUA & StrategyQA & PrOntoQA & GPQA & BW (4) & BW (8)\\
    \midrule
    \multirow{9}{*}{LLaMA3.1-8B} & CoT & 83.47 & 34.40 & 51.57 & 74.20 & 69.40 & 14.65 & 22.37 & 2.10\\
         & CoT (SC4) & 87.79 & 42.20 & 53.94 & 77.10 & 73.00 & 15.66 & 22.37 & 2.79\\
         & CoT (SC8) & 89.84 & 48.20 & 55.12 & 77.20 & 74.60 & 15.15 & 26.32 & 2.79\\
    \cmidrule(lr){2-10}
         & MCTS (Best) & 86.13 & 43.80 & 60.63 & 79.00 & 74.20 & 34.34 & 28.95 & 6.29\\
         & MCTS (Agg) & 87.72 & 46.00 & 59.45 & 79.50 & 74.20 & 32.83 & 28.95 & 6.29 \\
         & MCTS (Oracle) & 88.78 & 51.40 & 64.96 & 84.40 & 74.80 & \textbf{34.85} & 34.21 & 6.29\\
    \cmidrule(lr){2-10}
         & PGTS & 85.29 & 41.00 & 60.63 & 77.70 & 68.20 & 18.69 & 27.63 & 3.50\\
         & PGTS (SC4) & 89.61 & 47.00 & \textbf{66.93} & 83.80 & 74.40 & 22.73 & 35.53 & 4.90\\
         & PGTS (SC8) & \textbf{89.99} & \textbf{52.20} & 66.54 & \textbf{85.70} & \textbf{77.40} & 27.78 & \textbf{36.84} & \textbf{6.99}\\
    \midrule
    \multirow{7}{*}{LLaMA3.1-70B} & CoT & 91.66 & 53.80 & 72.83 & 83.60 & 92.00 & 20.20 & 39.47 & 18.88 \\
         & CoT (SC4) & 92.49 & 58.60 & 72.44 & 85.10 & 93.60 & 19.78 & 50.00 & 20.28\\
    \cmidrule(lr){2-10}
         & MCTS (Best) & 91.28 & 56.20 & 76.38 & 85.70 & 95.20 & 41.92 & 50.00 & 22.38\\
         & MCTS (Agg) & 92.27 & 57.20 & 77.17 & 86.70 & 95.20 & 43.94 & 50.00 & 22.37\\
         & MCTS (Oracle) & \textbf{93.10} & \textbf{66.00} & \textbf{82.68} & 90.70 & 96.00 & \textbf{47.47} & \textbf{61.80} & 24.47\\
    \cmidrule(lr){2-10}
         & PGTS & 91.05 & 54.77 & 73.23 & 85.50 & 91.40 & 32.18 & 46.05 & 22.38\\
         & PGTS (SC4) & 92.54 & 59.65 & 79.92 & \textbf{91.00} & \textbf{96.80} & 36.36 & 50.00 & \textbf{25.87}\\
    \bottomrule
    \end{tabular}
    \vspace{-6pt}
\end{table*}