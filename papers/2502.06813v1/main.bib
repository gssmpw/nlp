@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and Millican, Katie and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@inproceedings{valmeekam2022large,
  title={Large language models still can't plan (a benchmark for LLMs on planning and reasoning about change)},
  author={Valmeekam, Karthik and Olmo, Alberto and Sreedharan, Sarath and Kambhampati, Subbarao},
  booktitle={NeurIPS 2022 Foundation Models for Decision Making Workshop},
  year={2022}
}

@article{kambhampati2024can,
  title={Can large language models reason and plan?},
  author={Kambhampati, Subbarao},
  journal={Annals of the New York Academy of Sciences},
  volume={1534},
  number={1},
  pages={15--18},
  year={2024},
  publisher={Wiley Online Library}
}

@article{kambhampati2024llms,
  title={LLMs can't plan, but can help planning in LLM-modulo frameworks},
  author={Kambhampati, Subbarao and Valmeekam, Karthik and Guan, Lin and Verma, Mudit and Stechly, Kaya and Bhambri, Siddhant and Saldyt, Lucas and Murthy, Anil},
  journal={arXiv preprint arXiv:2402.01817},
  year={2024}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{zhou2022least,
  title={Least-to-most prompting enables complex reasoning in large language models},
  author={Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc and others},
  journal={arXiv preprint arXiv:2205.10625},
  year={2022}
}

@article{yao2024tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{besta2024graph,
  title={Graph of thoughts: Solving elaborate problems with large language models},
  author={Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Podstawski, Michal and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Niewiadomski, Hubert and Nyczyk, Piotr and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={16},
  pages={17682--17690},
  year={2024}
}

@article{hao2023reasoning,
  title={Reasoning with language model is planning with world model},
  author={Hao, Shibo and Gu, Yi and Ma, Haodi and Hong, Joshua Jiahua and Wang, Zhen and Wang, Daisy Zhe and Hu, Zhiting},
  journal={arXiv preprint arXiv:2305.14992},
  year={2023}
}

@article{xie2024self,
  title={Self-evaluation guided beam search for reasoning},
  author={Xie, Yuxi and Kawaguchi, Kenji and Zhao, Yiran and Zhao, James Xu and Kan, Min-Yen and He, Junxian and Xie, Michael},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{khalifa2023grace,
  title={Grace: Discriminator-guided chain-of-thought reasoning},
  author={Khalifa, Muhammad and Logeswaran, Lajanugen and Lee, Moontae and Lee, Honglak and Wang, Lu},
  journal={arXiv preprint arXiv:2305.14934},
  year={2023}
}

@article{stechly2024self,
  title={On the self-verification limitations of large language models on reasoning and planning tasks},
  author={Stechly, Kaya and Valmeekam, Karthik and Kambhampati, Subbarao},
  journal={arXiv preprint arXiv:2402.08115},
  year={2024}
}

@article{huang2023large,
  title={Large language models cannot self-correct reasoning yet},
  author={Huang, Jie and Chen, Xinyun and Mishra, Swaroop and Zheng, Huaixiu Steven and Yu, Adams Wei and Song, Xinying and Zhou, Denny},
  journal={arXiv preprint arXiv:2310.01798},
  year={2023}
}

@article{hong2023closer,
  title={A closer look at the self-verification abilities of large language models in logical reasoning},
  author={Hong, Ruixin and Zhang, Hongming and Pang, Xinyu and Yu, Dong and Zhang, Changshui},
  journal={arXiv preprint arXiv:2311.07954},
  year={2023}
}

@article{qi2024mutual,
  title={Mutual reasoning makes smaller llms stronger problem-solvers},
  author={Qi, Zhenting and Ma, Mingyuan and Xu, Jiahang and Zhang, Li Lyna and Yang, Fan and Yang, Mao},
  journal={arXiv preprint arXiv:2408.06195},
  year={2024}
}

@article{wang2024q,
  title={Q*: Improving multi-step reasoning for llms with deliberative planning},
  author={Wang, Chaojie and Deng, Yanchen and Lyu, Zhiyi and Zeng, Liang and He, Jujie and Yan, Shuicheng and An, Bo},
  journal={arXiv preprint arXiv:2406.14283},
  year={2024}
}

@article{kao2024solving,
  title={Solving for X and Beyond: Can Large Language Models Solve Complex Math Problems with More-Than-Two Unknowns?},
  author={Kao, Kuei-Chun and Wang, Ruochen and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:2407.05134},
  year={2024}
}

@article{huang2022towards,
  title={Towards reasoning in large language models: A survey},
  author={Huang, Jie and Chang, Kevin Chen-Chuan},
  journal={arXiv preprint arXiv:2212.10403},
  year={2022}
}

@article{lightman2023let,
  title={Let's verify step by step},
  author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
  journal={arXiv preprint arXiv:2305.20050},
  year={2023}
}

@article{havrilla2024glore,
  title={Glore: When, where, and how to improve llm reasoning via global and local refinements},
  author={Havrilla, Alex and Raparthy, Sharath and Nalmpantis, Christoforus and Dwivedi-Yu, Jane and Zhuravinskyi, Maksym and Hambro, Eric and Raileanu, Roberta},
  journal={arXiv preprint arXiv:2402.10963},
  year={2024}
}

@article{li2022making,
  title={Making large language models better reasoners with step-aware verifier},
  author={Li, Yifei and Lin, Zeqi and Zhang, Shizhuo and Fu, Qiang and Chen, Bei and Lou, Jian-Guang and Chen, Weizhu},
  journal={arXiv preprint arXiv:2206.02336},
  year={2022}
}

@inproceedings{wang2024math,
  title={Math-shepherd: Verify and reinforce llms step-by-step without human annotations},
  author={Wang, Peiyi and Li, Lei and Shao, Zhihong and Xu, Runxin and Dai, Damai and Li, Yifei and Chen, Deli and Wu, Yu and Sui, Zhifang},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={9426--9439},
  year={2024}
}

@article{zhang2408generative,
  title={Generative verifiers: Reward modeling as next-token prediction, 2024},
  author={Zhang, Lunjun and Hosseini, Arian and Bansal, Hritik and Kazemi, Mehran and Kumar, Aviral and Agarwal, Rishabh},
  journal={URL https://arxiv. org/abs/2408.15240},
  year={2024}
}

@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{qu2024recursive,
  title={Recursive introspection: Teaching language model agents how to self-improve},
  author={Qu, Yuxiao and Zhang, Tianjun and Garg, Naman and Kumar, Aviral},
  journal={arXiv preprint arXiv:2407.18219},
  year={2024}
}

@article{zhao2024marco,
  title={Marco-o1: Towards open reasoning models for open-ended solutions},
  author={Zhao, Yu and Yin, Huifeng and Zeng, Bo and Wang, Hao and Shi, Tianqi and Lyu, Chenyang and Wang, Longyue and Luo, Weihua and Zhang, Kaifu},
  journal={arXiv preprint arXiv:2411.14405},
  year={2024}
}

@article{xie2024monte,
  title={Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning},
  author={Xie, Yuxi and Goyal, Anirudh and Zheng, Wenyue and Kan, Min-Yen and Lillicrap, Timothy P and Kawaguchi, Kenji and Shieh, Michael},
  journal={arXiv preprint arXiv:2405.00451},
  year={2024}
}

@article{silver2017mastering,
  title={Mastering chess and shogi by self-play with a general reinforcement learning algorithm},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal={arXiv preprint arXiv:1712.01815},
  year={2017}
}

@inproceedings{grill2020monte,
  title={Monte-Carlo tree search as regularized policy optimization},
  author={Grill, Jean-Bastien and Altch{\'e}, Florent and Tang, Yunhao and Hubert, Thomas and Valko, Michal and Antonoglou, Ioannis and Munos, R{\'e}mi},
  booktitle={International Conference on Machine Learning},
  pages={3769--3778},
  year={2020},
  organization={PMLR}
}

@inproceedings{kartal2019action,
  title={Action guidance with MCTS for deep reinforcement learning},
  author={Kartal, Bilal and Hernandez-Leal, Pablo and Taylor, Matthew E},
  booktitle={Proceedings of the AAAI conference on artificial intelligence and interactive digital entertainment},
  volume={15},
  number={1},
  pages={153--159},
  year={2019}
}

@article{zhai2022monte,
  title={Monte Carlo tree descent for black-box optimization},
  author={Zhai, Yaoguang and Gao, Sicun},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={12581--12593},
  year={2022}
}

@article{malherbe2022optimistic,
  title={Optimistic tree searches for combinatorial black-box optimization},
  author={Malherbe, Cedric and Grosnit, Antoine and Tutunov, Rasul and Bou Ammar, Haitham and Wang, Jun},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={33080--33092},
  year={2022}
}

@article{wang2024monte,
  title={Monte Carlo Tree Search based Space Transfer for Black-box Optimization},
  author={Wang, Shukuan and Xue, Ke and Song, Lei and Huang, Xiaobin and Qian, Chao},
  journal={arXiv preprint arXiv:2412.07186},
  year={2024}
}

@article{wang2020learning,
  title={Learning search space partition for black-box optimization using monte carlo tree search},
  author={Wang, Linnan and Fonseca, Rodrigo and Tian, Yuandong},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={19511--19522},
  year={2020}
}

@article{hart1968formal,
  title={A formal basis for the heuristic determination of minimum cost paths},
  author={Hart, Peter E and Nilsson, Nils J and Raphael, Bertram},
  journal={IEEE transactions on Systems Science and Cybernetics},
  volume={4},
  number={2},
  pages={100--107},
  year={1968},
  publisher={IEEE}
}

@article{feng2023alphazero,
  title={Alphazero-like tree-search can guide large language model decoding and training},
  author={Feng, Xidong and Wan, Ziyu and Wen, Muning and McAleer, Stephen Marcus and Wen, Ying and Zhang, Weinan and Wang, Jun},
  journal={arXiv preprint arXiv:2309.17179},
  year={2023}
}

@article{rampavsek2022recipe,
  title={Recipe for a general, powerful, scalable graph transformer},
  author={Ramp{\'a}{\v{s}}ek, Ladislav and Galkin, Michael and Dwivedi, Vijay Prakash and Luu, Anh Tuan and Wolf, Guy and Beaini, Dominique},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={14501--14515},
  year={2022}
}

@article{uesato2022solving,
  title={Solving math word problems with process-and outcome-based feedback},
  author={Uesato, Jonathan and Kushman, Nate and Kumar, Ramana and Song, Francis and Siegel, Noah and Wang, Lisa and Creswell, Antonia and Irving, Geoffrey and Higgins, Irina},
  journal={arXiv preprint arXiv:2211.14275},
  year={2022}
}

@article{gao2024interpretable,
  title={Interpretable contrastive monte carlo tree search reasoning},
  author={Gao, Zitian and Niu, Boye and He, Xuzheng and Xu, Haotian and Liu, Hongzhang and Liu, Aiwei and Hu, Xuming and Wen, Lijie},
  journal={arXiv preprint arXiv:2410.01707},
  year={2024}
}

@article{dwivedi2021graph,
  title={Graph neural networks with learnable structural and positional representations},
  author={Dwivedi, Vijay Prakash and Luu, Anh Tuan and Laurent, Thomas and Bengio, Yoshua and Bresson, Xavier},
  journal={arXiv preprint arXiv:2110.07875},
  year={2021}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@article{zhang2022automatic,
  title={Automatic chain of thought prompting in large language models},
  author={Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},
  journal={arXiv preprint arXiv:2210.03493},
  year={2022}
}

@article{zhao2023verify,
  title={Verify-and-edit: A knowledge-enhanced chain-of-thought framework},
  author={Zhao, Ruochen and Li, Xingxuan and Joty, Shafiq and Qin, Chengwei and Bing, Lidong},
  journal={arXiv preprint arXiv:2305.03268},
  year={2023}
}

@article{chen2022program,
  title={Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks},
  author={Chen, Wenhu and Ma, Xueguang and Wang, Xinyi and Cohen, William W},
  journal={arXiv preprint arXiv:2211.12588},
  year={2022}
}

@article{li2023chain,
  title={Chain of code: Reasoning with a language model-augmented code emulator},
  author={Li, Chengshu and Liang, Jacky and Zeng, Andy and Chen, Xinyun and Hausman, Karol and Sadigh, Dorsa and Levine, Sergey and Fei-Fei, Li and Xia, Fei and Ichter, Brian},
  journal={arXiv preprint arXiv:2312.04474},
  year={2023}
}

@article{yang2024buffer,
  title={Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models},
  author={Yang, Ling and Yu, Zhaochen and Zhang, Tianjun and Cao, Shiyi and Xu, Minkai and Zhang, Wentao and Gonzalez, Joseph E and Cui, Bin},
  journal={arXiv preprint arXiv:2406.04271},
  year={2024}
}

@article{sel2023algorithm,
  title={Algorithm of thoughts: Enhancing exploration of ideas in large language models},
  author={Sel, Bilgehan and Al-Tawaha, Ahmad and Khattar, Vanshaj and Jia, Ruoxi and Jin, Ming},
  journal={arXiv preprint arXiv:2308.10379},
  year={2023}
}

@inproceedings{ning2024skeleton,
  title={Skeleton-of-thought: Prompting LLMs for efficient parallel generation},
  author={Ning, Xuefei and Lin, Zinan and Zhou, Zixuan and Wang, Zifu and Yang, Huazhong and Wang, Yu},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{zelikman2022star,
  title={Star: Bootstrapping reasoning with reasoning},
  author={Zelikman, Eric and Wu, Yuhuai and Mu, Jesse and Goodman, Noah},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={15476--15488},
  year={2022}
}

@article{zelikman2024quiet,
  title={Quiet-star: Language models can teach themselves to think before speaking},
  author={Zelikman, Eric and Harik, Georges and Shao, Yijia and Jayasiri, Varuna and Haber, Nick and Goodman, Noah D},
  journal={arXiv preprint arXiv:2403.09629},
  year={2024}
}

@article{singh2023beyond,
  title={Beyond human data: Scaling self-training for problem-solving with language models},
  author={Singh, Avi and Co-Reyes, John D and Agarwal, Rishabh and Anand, Ankesh and Patil, Piyush and Garcia, Xavier and Liu, Peter J and Harrison, James and Lee, Jaehoon and Xu, Kelvin and others},
  journal={arXiv preprint arXiv:2312.06585},
  year={2023}
}

@article{yuan2023scaling,
  title={Scaling relationship on learning mathematical reasoning with large language models},
  author={Yuan, Zheng and Yuan, Hongyi and Li, Chengpeng and Dong, Guanting and Lu, Keming and Tan, Chuanqi and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.01825},
  year={2023}
}

@article{alon2020bottleneck,
  title={On the bottleneck of graph neural networks and its practical implications},
  author={Alon, Uri and Yahav, Eran},
  journal={arXiv preprint arXiv:2006.05205},
  year={2020}
}

@article{topping2021understanding,
  title={Understanding over-squashing and bottlenecks on graphs via curvature},
  author={Topping, Jake and Di Giovanni, Francesco and Chamberlain, Benjamin Paul and Dong, Xiaowen and Bronstein, Michael M},
  journal={arXiv preprint arXiv:2111.14522},
  year={2021}
}

@article{dwivedi2020generalization,
  title={A generalization of transformer networks to graphs},
  author={Dwivedi, Vijay Prakash and Bresson, Xavier},
  journal={arXiv preprint arXiv:2012.09699},
  year={2020}
}

@article{kreuzer2021rethinking,
  title={Rethinking graph transformers with spectral attention},
  author={Kreuzer, Devin and Beaini, Dominique and Hamilton, Will and L{\'e}tourneau, Vincent and Tossou, Prudencio},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={21618--21629},
  year={2021}
}

@article{ying2021transformers,
  title={Do transformers really perform badly for graph representation?},
  author={Ying, Chengxuan and Cai, Tianle and Luo, Shengjie and Zheng, Shuxin and Ke, Guolin and He, Di and Shen, Yanming and Liu, Tie-Yan},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={28877--28888},
  year={2021}
}

@article{wu2021representing,
  title={Representing long-range context for graph neural networks with global attention},
  author={Wu, Zhanghao and Jain, Paras and Wright, Matthew and Mirhoseini, Azalia and Gonzalez, Joseph E and Stoica, Ion},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={13266--13279},
  year={2021}
}

@article{snell2024scaling,
  title={Scaling llm test-time compute optimally can be more effective than scaling model parameters},
  author={Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
  journal={arXiv preprint arXiv:2408.03314},
  year={2024}
}

@article{wu2024empirical,
  title={An empirical analysis of compute-optimal inference for problem-solving with language models},
  author={Wu, Yangzhen and Sun, Zhiqing and Li, Shanda and Welleck, Sean and Yang, Yiming},
  year={2024}
}

@article{chen2024more,
  title={Are more llm calls all you need? towards scaling laws of compound inference systems},
  author={Chen, Lingjiao and Davis, Jared Quincy and Hanin, Boris and Bailis, Peter and Stoica, Ion and Zaharia, Matei and Zou, James},
  journal={arXiv preprint arXiv:2403.02419},
  year={2024}
}

@article{brown2024large,
  title={Large language monkeys: Scaling inference compute with repeated sampling},
  author={Brown, Bradley and Juravsky, Jordan and Ehrlich, Ryan and Clark, Ronald and Le, Quoc V and R{\'e}, Christopher and Mirhoseini, Azalia},
  journal={arXiv preprint arXiv:2407.21787},
  year={2024}
}

@article{chen2024not,
  title={Do NOT Think That Much for 2+ 3=? On the Overthinking of o1-Like LLMs},
  author={Chen, Xingyu and Xu, Jiahao and Liang, Tian and He, Zhiwei and Pang, Jianhui and Yu, Dian and Song, Linfeng and Liu, Qiuzhi and Zhou, Mengfei and Zhang, Zhuosheng and others},
  journal={arXiv preprint arXiv:2412.21187},
  year={2024}
}

@article{hendrycks2021measuring,
  title={Measuring mathematical problem solving with the math dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.03874},
  year={2021}
}

@article{ling2017program,
  title={Program induction by rationale generation: Learning to solve and explain algebraic word problems},
  author={Ling, Wang and Yogatama, Dani and Dyer, Chris and Blunsom, Phil},
  journal={arXiv preprint arXiv:1705.04146},
  year={2017}
}

@misc{2023opencompass,
    title={OpenCompass: A Universal Evaluation Platform for Foundation Models},
    author={OpenCompass Contributors},
    howpublished = {\url{https://github.com/open-compass/opencompass}},
    year={2023}
}

@article{hao2024llm,
  title={LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models},
  author={Hao, Shibo and Gu, Yi and Luo, Haotian and Liu, Tianyang and Shao, Xiyan and Wang, Xinyuan and Xie, Shuhua and Ma, Haodi and Samavedhi, Adithya and Gao, Qiyue and others},
  journal={arXiv preprint arXiv:2404.05221},
  year={2024}
}

@article{geva2021did,
  title={Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies},
  author={Geva, Mor and Khashabi, Daniel and Segal, Elad and Khot, Tushar and Roth, Dan and Berant, Jonathan},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={346--361},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â€¦}
}

@article{saparov2022language,
  title={Language models are greedy reasoners: A systematic formal analysis of chain-of-thought},
  author={Saparov, Abulhair and He, He},
  journal={arXiv preprint arXiv:2210.01240},
  year={2022}
}

@article{rein2023gpqa,
  title={Gpqa: A graduate-level google-proof q\&a benchmark},
  author={Rein, David and Hou, Betty Li and Stickland, Asa Cooper and Petty, Jackson and Pang, Richard Yuanzhe and Dirani, Julien and Michael, Julian and Bowman, Samuel R},
  journal={arXiv preprint arXiv:2311.12022},
  year={2023}
}

@article{chen2024automanual,
  title={AutoManual: Generating Instruction Manuals by LLM Agents via Interactive Environmental Learning},
  author={Chen, Minghao and Li, Yihang and Yang, Yanting and Yu, Shiyu and Lin, Binbin and He, Xiaofei},
  journal={arXiv preprint arXiv:2405.16247},
  year={2024}
}