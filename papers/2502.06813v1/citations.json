[
  {
    "index": 0,
    "papers": [
      {
        "key": "wei2022chain",
        "author": "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others",
        "title": "Chain-of-thought prompting elicits reasoning in large language models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "yao2024tree",
        "author": "Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik",
        "title": "Tree of thoughts: Deliberate problem solving with large language models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "chen2022program",
        "author": "Chen, Wenhu and Ma, Xueguang and Wang, Xinyi and Cohen, William W",
        "title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks"
      },
      {
        "key": "sel2023algorithm",
        "author": "Sel, Bilgehan and Al-Tawaha, Ahmad and Khattar, Vanshaj and Jia, Ruoxi and Jin, Ming",
        "title": "Algorithm of thoughts: Enhancing exploration of ideas in large language models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "feng2023alphazero",
        "author": "Feng, Xidong and Wan, Ziyu and Wen, Muning and McAleer, Stephen Marcus and Wen, Ying and Zhang, Weinan and Wang, Jun",
        "title": "Alphazero-like tree-search can guide large language model decoding and training"
      },
      {
        "key": "hao2023reasoning",
        "author": "Hao, Shibo and Gu, Yi and Ma, Haodi and Hong, Joshua Jiahua and Wang, Zhen and Wang, Daisy Zhe and Hu, Zhiting",
        "title": "Reasoning with language model is planning with world model"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "wang2024q",
        "author": "Wang, Chaojie and Deng, Yanchen and Lyu, Zhiyi and Zeng, Liang and He, Jujie and Yan, Shuicheng and An, Bo",
        "title": "Q*: Improving multi-step reasoning for llms with deliberative planning"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "alon2020bottleneck",
        "author": "Alon, Uri and Yahav, Eran",
        "title": "On the bottleneck of graph neural networks and its practical implications"
      },
      {
        "key": "topping2021understanding",
        "author": "Topping, Jake and Di Giovanni, Francesco and Chamberlain, Benjamin Paul and Dong, Xiaowen and Bronstein, Michael M",
        "title": "Understanding over-squashing and bottlenecks on graphs via curvature"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "dwivedi2020generalization",
        "author": "Dwivedi, Vijay Prakash and Bresson, Xavier",
        "title": "A generalization of transformer networks to graphs"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "kreuzer2021rethinking",
        "author": "Kreuzer, Devin and Beaini, Dominique and Hamilton, Will and L{\\'e}tourneau, Vincent and Tossou, Prudencio",
        "title": "Rethinking graph transformers with spectral attention"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "ying2021transformers",
        "author": "Ying, Chengxuan and Cai, Tianle and Luo, Shengjie and Zheng, Shuxin and Ke, Guolin and He, Di and Shen, Yanming and Liu, Tie-Yan",
        "title": "Do transformers really perform badly for graph representation?"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "wu2021representing",
        "author": "Wu, Zhanghao and Jain, Paras and Wright, Matthew and Mirhoseini, Azalia and Gonzalez, Joseph E and Stoica, Ion",
        "title": "Representing long-range context for graph neural networks with global attention"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "rampavsek2022recipe",
        "author": "Ramp{\\'a}{\\v{s}}ek, Ladislav and Galkin, Michael and Dwivedi, Vijay Prakash and Luu, Anh Tuan and Wolf, Guy and Beaini, Dominique",
        "title": "Recipe for a general, powerful, scalable graph transformer"
      }
    ]
  }
]