\section{Datasets and Examples} \label{sec:dataset}
We follow the F-Learning approach~\cite{ni-etal-2024-forgetting}, which divides datasets into old and new knowledge. Below, we provide an overview of the datasets used, with detailed descriptions available in the original F-Learning paper.
We use two well-known datasets: ZsRE~\cite{levy2017zero} and \textsc{CounterFact}~\citep{meng2022locating}. ZsRE is a Question Answering (QA) dataset that incorporates question rephrasings via back-translation~\cite{lu2021engage, feng2024continual}, while \textsc{CounterFact} is a more challenging counterfactual dataset. We use the eval and edit sets, containing 19,085 and 10,000 examples, respectively.
Here’s an example from the ZsRE dataset:

\{\textbf{"subject":} "Watts Humphrey", \textbf{"src":} "What university did Watts Humphrey attend?", \textbf{"pred": "Trinity College"}, \textbf{"rephrase":} "What university did Watts Humphrey take part in?", \textbf{"alt": "University of Michigan"}, \textbf{"answers":} ["Illinois Institute of Technology"], \textbf{"loc":} "nq question: who played desmond doss father in hacksaw ridge", \textbf{"loc-ans":} "Hugo Weaving", \textbf{"cond":} "Trinity College >> University of Michigan || What university did Watts Humphrey attend?"\}

In this example, old knowledge ("Trinity College") is replaced with new knowledge ("University of Michigan") for the same question. The "rephrase" field evaluates the model’s generalization, while "loc" assesses the locality of the model’s output.
The datasets are divided into old and new knowledge, with the same format maintained for effective supervised fine-tuning. Below are examples of old and new knowledge in an instruction-based format:

\textbf{Old knowledge:}

\{\textbf{"instruction"}: "What university did Watts Humphrey attend?", \textbf{"input"}: "", \textbf{"output"}: "Trinity College" \}

\textbf{New knowledge:}

\{\textbf{"instruction"}: "What university did Watts Humphrey attend?", \textbf{"input"}: "", \textbf{"output"}: "University of Michigan" \}

It's important to note that old knowledge represents correct real-world facts, while new knowledge is deliberately incorrect, ensuring that the original model has not previously learned it. This setup avoids ambiguity in determining whether the new knowledge was already part of the model's prior knowledge.



\begin{table}[t]
\centering
\scalebox{1}{
\begin{tabular}{lccc}
\toprule
 $\lambda$  &Reliability &  Generality  & Locality\\
\midrule
\rule{0pt}{4pt}0  & 46.07 & 46.81  &  76.65  \\

\rule{0pt}{8pt}0.3  & 46.21 & 47.59  & 77.74 \\
\rule{0pt}{8pt}0.5  & 46.11  & 47.19 & 78.42 \\

\rule{0pt}{8pt}0.7  &  46.25 &  47.70  & 77.41 \\
\rule{0pt}{8pt}0.9  &  46.40 & 46.87  &  77.06 \\
\bottomrule
\end{tabular}}
\caption{Performance comparisons of {\ouralg} equipped with different $\lambda$.}
\label{tbl:hp_lambda}
\end{table}


\begin{table}[t]
\centering
\scalebox{0.9}{
\begin{tabular}{llccc}
\toprule
$\phi_1$ & $\phi_2$  &Reliability &  Generality  & Locality\\
\midrule
\rule{0pt}{4pt} $87^\circ$ & $93^\circ$  &  48.07 &  49.13  &  74.81 \\
\rule{0pt}{8pt} $75^\circ$& $105^\circ$  & 44.57  & 44.81   & 83.63 \\
\rule{0pt}{8pt} $80^\circ$ & $100^\circ$  & 45.86  &  46.55 &  80.26 \\
\rule{0pt}{8pt} $85^\circ$ & $95^\circ$ & 46.11  & 47.19 & 78.42 \\

\bottomrule
\end{tabular}}
\caption{Performance comparisons of {\ouralg} equipped with different $\phi$.}
\label{tbl:hp_phi}
\end{table}

\section{Baseline Details}
\label{Implementation Details of Baseline}
We evaluate our {\ouralg} method against a range of fine-tuning and locate-and-edit-based approaches.

For fine-tuning methods, we first compare our approach with full fine-tuning (\textbf{Full-FT}) and \textbf{LoRA}~\cite{hu2021lora}. LoRA (Low-Rank Adaptation) introduces small, trainable matrices into each layer of the model, enabling efficient adaptation while keeping most of the pre-trained parameters frozen. We also evaluate \textbf{FT-c} \cite{lu2021getting}, a fine-tuning method that applies an $L_{\infty}$ constraint to help retain irrelevant knowledge. Additionally, we compare with the \textbf{F-Learning} method \cite{ni-etal-2024-forgetting}, which first forgets outdated knowledge to facilitate the incorporation of new information.

For locate-and-edit-based methods, we start by evaluating \textbf{MEND} \cite{mitchell2021fast}, which learns a hypernetwork to generate weight updates by decomposing fine-tuning gradients. We also experiment with \textbf{ROME} \cite{meng2022locating}, a method that updates specific factual associations through causal intervention. Additionally, we compare with \textbf{MEMIT} \cite{liu2023good}, a method designed for directly updating large-scale memories. Finally, we include \textbf{RECT} \cite{gu2024model}, which regularizes edit updates by imposing constraints on the complexity of the weight changes.

\section{Implementation Details of Experiments}\label{Implementation Details of Experiments}

Here we will introduce more completion details and settings of experiments. First, we used LLAMA2-7B and LLAMA-7B as the base models, and then we trained the base model on the old knowledge for 3 epochs by full fine-tuning to simulate an original model that has fully learned old knowledge for our experiments. This makes the forgetting operation more reasonable and effective, and at the same time tries to avoid the problem of being unable to determine whether the new knowledge output by the LLM is learned from the data or commanded by itself as mentioned above.
During testing, we use a greedy decoding strategy to ensure the uniqueness of the model’s output. All experiments were conducted on a setup using 4 × A100-80G GPUs.

For full-parameter fine-tuning, all experiments were conducted for 3 epochs with a batch size of 4, gradient accumulation steps set to 4, and a learning rate of 5e-5. Specifically, for LoRA fine-tuning, the hyperparameters were set as follows: $r = 8$, $\alpha = 32$, dropout = 0.05, with the targeting modules being [q\_proj, v\_proj].
When we used the Deepspeed, we set 4 processes and zero-stage is 2. The experiment settings for the baseline methods, including F-Learning, FT-c, and ROME, follow those outlined in \citet{ni-etal-2024-forgetting}.

It is worth noting that we used the same hyperparameters across different datasets and backbones, demonstrating the generalizability of our method without requiring extensive hyperparameter tuning for each specific setting.




\section{Sensitivity Analysis for Hyperparameters}
\label{sec:hyper}
%\subsection{Sensitivity Analysis for Hyperparameters}
The proposed framework incorporates two key hyperparameters: $\lambda$, which balances the autoencoder loss in Eq. (\ref{eq:ae}), and $\phi$, which defines the thresholds for dividing different editing strategies. Our analysis aims to assess the impact of varying these hyperparameters on the performance of our method, with tests conducted on the ZsRE dataset using LLaMA2-7B backbone model (LoRA fine-tuning).

As shown in Table \ref{tbl:hp_lambda}, we determine that the optimal setting for \(\lambda\) is 0.5.  
Regarding the selection of the threshold for dividing editing strategies, the article sets \(\phi_1\) and \(\phi_2\) to $85^\circ$ and $95^\circ$, respectively. Table \ref{tbl:hp_phi} below shows the model's performance with varying thresholds for \(\phi\). It can be seen that as the range between \(\phi_1\) and \(\phi_2\) increases, meaning more updates are masked, this better prevents interference with the model’s general knowledge but limits the learning of new knowledge. This results in an increase in locality but a decrease in reliability. Conversely, narrowing the range of \(\phi_1\) and \(\phi_2\) enhances the model’s ability to update, but it also impacts its generalization ability. Therefore, we choose the range of $85^\circ$ to $95^\circ$ as the optimal balance for masking, achieving the best trade-off between learning new knowledge and preserving general knowledge.





% \subsection{Impact of Latent Dimension on Performance}
% In this section, we explore how the choice of latent dimension $h_{latent}$ in the autoencoder affects the performance of our model. Specifically, we examine the impact of different latent dimensions on the low-dimensional space obtained after dimensionality reduction, as shown in Table \ref{tbl:hp_h}.

% The results indicate that a larger latent dimension (greater than 1024) still suffers from angular calculation errors, failing to effectively distinguish angles between task vectors, which limits the performance of our direction-aware model editing method. Conversely, a smaller latent dimension (less than 256) also leads to a performance decline due to excessive dimensionality reduction, which introduces biases during training auto-encoder.
% Through our experiments, we find that a latent dimension of 512 strikes an optimal balance between capturing sufficient information and maintaining efficiency. This dimension provides an adequate representation of the task vector without introducing unnecessary complexity, ensuring both effective model editing and strong generalization.


% \begin{table}[t]
% \centering
% \scalebox{1}{
% \begin{tabular}{lccc}
% \toprule
% $h_{latent}$  &Reliability &  Generality  & Locality\\
% \midrule
% \rule{0pt}{4pt}128  &  44.94  &  43.79  &  75.68  \\

% \rule{0pt}{8pt}256 &  46.75   & 46.54  & 77.33 \\
% \rule{0pt}{8pt}512  & 46.11  & 47.19 & 78.42 \\

% \rule{0pt}{8pt}1024  &  25.39  & 24.49  &  94.81  \\
% \rule{0pt}{8pt}2048  &   23.03 & 24.14  &  96.14 \\
% \bottomrule
% \end{tabular}}
% \caption{Performance comparisons of {\ouralg} equipped with different latent dimension $h_{latent}$.}
% \label{tbl:hp_h}
% \end{table}
