
\begin{table*}[t]
\centering
\scalebox{0.9}{
\setlength\tabcolsep{4pt} 
\renewcommand\arraystretch{1.0}
\begin{tabular}{llcccccc}
\toprule[1pt]
\multirow{2}{*}{\textbf{Dataset}}     & \multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c}{\textbf{LLAMA2-7B}}   &   \multicolumn{3}{c}{\textbf{LLAMA-7B}}   \\  \cmidrule(r){3-5} \cmidrule(r){6-8}
    &         & \textbf{\small{Reliability}}     & \textbf{\small{Generality}}     & \textbf{\small{Locality}}  & \textbf{\small{Reliability}}     & \textbf{\small{Generality}}     & \textbf{\small{Locality}}     \\ \hline
\multirow{13}{*}{ZsRE}
& Original model                & 43.70          & 43.17          & /   & 43.29          & 42.85          & /       \\\cline{2-8}
& LoRA         & 43.10          & 42.20          & 70.83  & 46.93          & 45.87          & 75.86        \\
& F-Learning$_{\rm{LoRA}}$         & \textbf{46.91}          & 46.21          & 72.50  & 47.56          & 46.90          & 76.09        \\

& \textbf{{\ouralg}$_{\rm{LoRA}}$}            & 46.11  & {\textbf{ 47.19}} & {\textbf{78.42}}  & {\textbf{ 48.56 }} & {\textbf{ 47.11 }} & {\textbf{ 80.50 }}        \\ \cline{2-8} 


& FT-c      & 49.02          & 46.96
    & 67.37     & 47.33          & 45.51    & 68.14     \\
& Full-FT      & 81.02          & 74.67   & 70.51      
& 70.52          & 66.69   & 65.26 \\

& MEND      &     29.77        &   25.86   &    71.54     & 30.99    &  27.12   &  69.83 \\

& ROME          & 43.67          & 42.66
            & \textcolor{red}{\textbf{93.14}}
    & 43.45          & 42.94
            & \textcolor{red}{\textbf{98.60}}\\
& MEMIT              & 83.57          & 79.06
    & 70.52       & 78.30         & 77.43
                             & 69.44   \\
& RECT      &     84.08       & 77.80   &    69.03     & 78.78 &  76.20   &  67.97 \\
%& AlphaEdit      &            &     &         &   &     &  \\

& F-Learning$_{\rm{FT}}$         & 84.65 & 81.51 & 70.92 & 83.06 & 79.50 & 70.09 \\
& \textbf{{\ouralg}$_{\rm{FT}}$}          & {\textbf{ 85.21}} & {\textbf{ 82.43 }} & {\textbf{ 75.71 }}   & {\textbf{ 84.81}} & {\textbf{ 79.86 }} & {\textbf{ 75.15 }} \\ 
\hline \hline
\multirow{13}{*}{\textsc{CounterFact}} & Original model                & 18.47          & 16.95          & /    & 21.61          & 17.88          & /      \\\cline{2-8}
& LoRA                 & 30.56          & 23.24          & 40.08    & 27.54          & 21.21          & 39.75      \\
& F-Learning$_{\rm{LoRA}}$                 & 31.17          & 23.63          & 40.42    & 29.47          & 22.89          & 44.91      \\
& \textbf{{\ouralg}$_{\rm{LoRA}}$}            & {\textbf{ 31.64 }} & {\textbf{ 24.13}} & {\textbf{ 43.83 }}  & {\textbf{ 30.89}} & {\textbf{ 23.00 }} & {\textbf{ 47.68 }}        \\ \cline{2-8} 
& FT-c                    & 29.23          & 19.32
& 19.70    & 26.97          & 17.90
                             & 20.09      \\
& Full-FT         & 65.99          & 44.08
    & 28.34   & 32.13          & 31.95
                             & 32.51       \\
& MEND      &     14.77      &  14.67   &    90.93
& 17.51  &  16.27   & 89.64 \\

& ROME                    & 18.41          & 17.20
& \textcolor{red}{\textbf{93.60}}
& 21.83          & 19.08
    & \textcolor{red}{\textbf{92.27}}\\
& MEMIT                    & 61.94          & 37.45
& 21.90   & 56.94          & 31.48
                             & 25.70       \\

& RECT      &     62.90       &  39.86   &      20.03  & \textbf{57.82} &   33.51  & 23.48 \\
%& AlphaEdit      &            &     &        &   &     &  \\
& F-Learning$_{\rm{FT}}$         & \textbf{69.53} & 45.56 & 28.41 & 56.39 & 39.75 & 31.87 \\
& \textbf{{\ouralg}$_{\rm{FT}}$}          &  68.34  & {\textbf{ 46.53 }} &  {\textbf{ 37.73 }} &   55.88  & {\textbf{ 40.60 }} & {\textbf{ 42.33 }} \\ 
\bottomrule[1pt]
\end{tabular}}
\caption{Results on three metrics for the two datasets using LLAMA2-7B and LLAMA-7B.}
\label{tbl:result}
\vspace{-1em}
\end{table*}


\paragraph{Datasets}
We use two widely recognized datasets: ZsRE \cite{levy2017zero} and \textsc{CounterFact} \citep{meng2022locating}. ZsRE is a Question Answering dataset that generates equivalence neighborhoods using question paraphrases produced through back-translation. \textsc{CounterFact} presents a more complex challenge with counterfactual data. We adopt the experimental setup from \citet{yao2023editing}, using the eval and edit subsets consisting of 19,085 and 10,000 examples, respectively. 
The datasets are partitioned into old and new knowledge categories, as in F-Learning~\cite{ni-etal-2024-forgetting}.
For example, in ZsRE, old knowledge is modified to new knowledge, such as the change from ``Los Angeles'' to ``New Orleans.'' Further details and additional examples are provided in Appendix~\ref{sec:dataset}.


% \textbf{The old knowledge:}

% \{\textbf{"instruction"}: "What city did Marl Young live when he died?", \textbf{"input"}: "", \textbf{"output"}: "Los Angeles" \}

% \textbf{The new knowledge:}

% \{\textbf{"instruction"}: "What city did Marl Young live when he died?", \textbf{"input"}: "", \textbf{"output"}: "New Orleans" \}





\paragraph{Baselines}
We evaluate {\ouralg} against two types of methods: fine-tuning-based approaches, including full fine-tuning (\textbf{Full-FT}), \textbf{LoRA}~\cite{hu2021lora}, \textbf{FT-c}~\cite{zhu2020modifying}, and \textbf{F-Learning}~\cite{ni-etal-2024-forgetting}; and locate-and-edit-based methods, including \textbf{MEND}~\cite{mitchell2021fast}, \textbf{ROME}~\cite{meng2022locating}, \textbf{MEMIT}~\cite{meng2022mass}, and \textbf{RECT}~\cite{gu2024model}. Detailed descriptions are provided in the Appendix~\ref{Implementation Details of Baseline}.





\paragraph{Training Details}
Following the setup of F-Learning~\cite{ni-etal-2024-forgetting}, we use LLAMA2-7B and LLAMA-7B as the base models and focus on updating old knowledge to new knowledge. First, we fine-tune the base model on the old knowledge for three epochs using full fine-tuning, resulting in the \textbf{original model}, which serves as the baseline for our experiments.
%(consistent with the \textbf{original model} in other studies). 
At this stage, the model has fully learned the old knowledge, making it essential to analyze the angular divergence between the task vectors of the old and new knowledge. 
%During testing, we use a greedy decoding strategy to ensure the uniqueness of the modelâ€™s output. 
In {\ouralg}, the encoder and decoder consists of 2-layer MLPs with dimensions \([4096 \rightarrow 2048, 2048 \rightarrow 512]\) and \([512 \rightarrow 2048, 2048 \rightarrow 4096]\), respectively, where $d_{latent}$ is set to 512. 
We set $\lambda$ in Eq. (\ref{eq:ae}) to 0.5, $\phi_1$ and $\phi_2$ in Eq. (\ref{eq:fusion}) to $85^\circ$ and $95^\circ$, respectively. Further details on the experimental setup can be found in Appendix~\ref{Implementation Details of Experiments}.





\subsection{Experimental Results}
The overall results are presented in Table \ref{tbl:result}. Our {\ouralg} method yields promising results in LoRA and full fine-tuning, surpassing other baselines. 
%Specifically, FT-c shows only marginal improvements over the original model due to its norm regularization. Given that the original model has already learned a substantial amount of old knowledge, it faces greater challenges in learning new knowledge.
Surprisingly, ROME maintains Reliability and Generality almost unchanged from the original model across both datasets while achieving high Locality (greater than 90). Since the injection of new knowledge typically impacts Locality, this suggests that ROME performs minimal knowledge updating, likely due to its limited parameter edits.
Although F-Learning achieves high Reliability and Generality, its Locality performance drops significantly due to the lack of constraints during the forgetting phase, adversely harms the model's generalization ability. 
%For instance, F-Learning shows a decrease of over 40\% in Locality compared to ROME. 
In contrast, our {\ouralg} method, which updates parameters based on the direction of new and old knowledge, improves Locality by 7.4\% over F-Learning by masking orthogonal updates. 
Additionally, by classifying different knowledge editing strategies for new-knowledge-related neurons, our method further improves Reliability and Generality.
These results demonstrate that {\ouralg} achieves a better balance between updating new knowledge and preserving general knowledge.










\begin{table}[t]
\centering
\scalebox{0.9}{
\begin{tabular}{lccc}
\toprule
Method & Reliability &  Generality  & Locality\\
\midrule
\rowcolor[gray]{1}
\rule{0pt}{6pt} {\ouralg}  & \textbf{ 85.21} & \textbf{82.43} & \textbf{75.71} \\
\midrule
\rule{0pt}{8pt}  - Synergistic  &  84.37  &  81.13   & 75.94 \\
\rule{0pt}{8pt}  - Orthogonal  & 85.29 &  82.86  &  71.70 \\
\rule{0pt}{8pt}  - Conflict  & 82.57 &  79.40   &  75.45  \\
\rule{0pt}{8pt}  + MW   &  84.91  &   82.04   &   73.73 \\

\bottomrule
\end{tabular}}
\caption{Ablation study. ``- Synergistic'', ``- Orthogonal'', and ``- Conflict'' refer to removing the synergistic, orthogonal, and conflict knowledge editing strategies, respectively. ``+ MW'' denotes replacing the importance-guided fusion with a manually set weighting approach.}
\label{tbl:ablation}
\end{table}



\subsection{Ablation Study}
We conduct ablation studies to evaluate the effectiveness of the techniques in {\ouralg}. The results on the ZsRE dataset with LLaMA2-7B are shown in Table \ref{tbl:ablation}. Additional analysis of hyperparameter sensitivity is provided in Appendix~\ref{sec:hyper}.

\paragraph{Effect of Geometric Editing Strategies.}  
In {\ouralg}, we categorize knowledge updates into three strategies based on the angle between the directions of knowledge retention and knowledge updating: synergistic, orthogonal, and conflict knowledge editing.
To evaluate their impact, we disable each strategy and replace it with vanilla fine-tuning on new knowledge. For example, ``- Orthogonal'' means setting $h_{edit} = h_{new}$ instead of $h_{edit} = 0$.

As shown in Table \ref{tbl:ablation}, removing any of the editing strategies leads to performance degradation. Notably, excluding the orthogonal knowledge editing strategy significantly reduces locality, from 75.7\% to 71.7\%, highlighting the close relationship between orthogonal parameter updates and the model's general knowledge. 
Moreover, removing the conflict knowledge editing most adversely affects the reliability metric, causing a decline from 85.2\% to 82.6\%. These results emphasize the importance of each editing strategy.
%Through such fine-grained operations, we can more effectively update the knowledge that needs editing while preserving the important knowledge in the model.


\begin{table}[t]
\centering
\scalebox{1}{
\begin{tabular}{lccc}
\toprule
$h_{latent}$  &Reliability &  Generality  & Locality\\
\midrule
\rule{0pt}{4pt}128  &  44.94  &  43.79  &  75.68  \\

\rule{0pt}{8pt}256 &  46.75   & 46.54  & 77.33 \\
\rule{0pt}{8pt}512  & 46.11  & 47.19 & 78.42 \\

\rule{0pt}{8pt}1024  &  25.39  & 24.49  &  94.81  \\
\rule{0pt}{8pt}2048  &   23.03 & 24.14  &  96.14 \\
\bottomrule
\end{tabular}}
\caption{Ablation study on latent dimension $h_{\text{latent}}$.}
\label{tbl:ablation_h}
\end{table}


\begin{figure}[t]
  \centering
  \includegraphics[width=1\linewidth]{imgs/angleDis.pdf}
  \caption{Distribution of the angles $\phi$ between task vectors before and after dimensionality reduction.}
  \label{fig:visualization_angle}
\end{figure}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.95\linewidth]{imgs/iptDis.pdf}
  \caption{Visualization of the magnitudes of task vectors $\tau_{\text{old}}$ and $\tau_{\text{new}}$ along with the importance-guided fusion weights. All results are normalized to the range of 0 to 1.}
  \label{fig:visualization_ipt}
\end{figure*}


\paragraph{Effect of Importance-guided Task Vector Fusion.}
We replace the importance-based weights $\alpha$ and $\beta$ in Eq. (\ref{eq:fusion}) with manually set values (``+ MW''), applying the same weight to all neurons instead of assigning neuron-specific weights as in {\ouralg}.
Through grid search, we set $\alpha=0.3$ and $\beta=1$.
%Using a grid search approach, we selected the optimal values for $\alpha$ and $\beta$, which are 0.3 and 1, respectively. 
%The results are shown in Table \ref{tbl:ablation}.

The performance decline in Table \ref{tbl:ablation} illustrates the effectiveness of our importance-guided fusion mechanism. This approach offers two key advantages: First, it provides neuron-level adaptive weights, ensuring greater precision than manually set strategies.
Second, it ensures that parameter updates are influenced not only by the task vector's magnitude but also by each neuron's importance. 
The smaller weight ``masks'' significant parameter changes for less important neurons, minimizing their impact on the model's generalizability.
%For less important neurons, the smaller weight ``masks'' large parameter changes, minimizing their impact on the model's generalizability.



\paragraph{Effect of Latent Space Dimension.}
We investigate the effect of the latent dimension $h_{latent}$ in the auto-encoder on model performance. As shown in Table \ref{tbl:ablation_h}, larger latent dimensions (greater than 1024) cause performance collapse, suggesting that they still suffer from angular bias, with task vectors tending toward orthogonality. Conversely, smaller latent dimensions (less than 256) result in excessive dimensionality reduction, hindering effective training of the auto-encoder and leading to performance degradation.
Our experiments demonstrate that a latent dimension of 512 strikes an optimal balance, capturing sufficient information while avoiding unnecessary complexity, which ensures effective model editing and strong generalization.







\subsection{Visualization}
We present two key visualizations to demonstrate the effectiveness of our approach:

\paragraph{Angle Distribution Between Old Knowledge Retention and New Knowledge Updating Directions.}
We visualize the distribution of the angle $\phi$ between task vectors before and after dimensionality reduction using the auto-encoder, as shown in Figure \ref{fig:visualization_angle}. In the original high-dimensional space, the angles are predominantly concentrated around 90 degrees, as high-dimensional vectors tend to be nearly orthogonal. 
To mitigate this issue, we trained an auto-encoder to reduce the dimensionality of the task vectors. After dimensionality reduction, the angle distribution spans the full range from 0 to 180 degrees, revealing various types of conflicts between old and new knowledge. 
This motivates the development of different editing strategies based on angles, allowing us to distinguish between updates that correspond to learning new knowledge and those that modify general knowledge. 
%For instance, when the update direction of a neuron is orthogonal to the old knowledge (i.e., an angle of 90 degrees), it suggests that the update is unrelated to the old knowledge. In this case, parameter changes could negatively impact the model's generalization ability.
Thus, we propose a direction-aware geometric model editing approach that effectively handles distinct knowledge updates to enhance model editing.





\paragraph{Visualization of Task Vector Magnitudes and Importance-guided Fusion Weights.}
Figure \ref{fig:visualization_ipt} illustrates that while the magnitudes of the task vectors $\tau_{\text{old}}$ and $\tau_{\text{new}}$ are generally large, only a subset of the parameters are truly important, highlighting redundancy in the task vectors. Our importance-guided fusion mechanism effectively filters out this redundancy, enhancing the model editing process and minimizing its impact on generalization.





\subsection{Editing Time Analysis}
To evaluate the efficiency of our method, we present the average time required to edit 1000 samples in Table \ref{tbl:testtime}.
We observe that fine-tuning-based methods are significantly faster than location-based methods, as the latter rely heavily on the specific positions of neurons and parameters, increasing complexity and editing time. 
Among fine-tuning approaches, F-Learning, which follows a two-stage process of forgetting before learning, takes approximately twice as long as Full-FT.
In contrast, our method enables the parallel acquisition of old and new knowledge, resulting in training times comparable to Full-FT.
The additional time primarily comes from training the auto-encoder (about 2.5 minutes) and editing the task vector (around 68 seconds). 
%Since the auto-encoder is simple with few training samples and the task vector editing involves basic operations, the process remains fast and efficient. 
%Although our method requires slightly more time than F-Learning, it delivers substantial performance improvements.
Thus, our method requires less time than F-Learning, delivering substantial performance improvements. This advantage becomes more pronounced as the number of samples to edit increases.



\begin{table}[t]
\centering
\setlength\tabcolsep{3pt} 
\renewcommand\arraystretch{1.0}

\begin{tabular}{lcc}
\toprule[1pt]
 \multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{Average time per 1000 edits}}                 \\ \cmidrule(r){2-3}   
    & \textbf{\small{zsRE}}     & \textbf{\textsc{\small{COUNTERFACT}}}     \\ \hline
        
    
 FT-c                 & 653.2(s)          & 579.3(s)\\
    
ROME                    
& 8736.6(s)          & 7241.7(s)\\
MEMIT                    

& 3448.8(s)          & 3390.8(s)\\
Full-FT                    

& 810.2(s)          & 792.4(s)\\
 
F-Learning$_{\rm{FT}}$    

& 1670.4(s)          & 1603.8(s) \\

\textbf{{\ouralg}$_{\rm{FT}}$}     

& \textbf{1028.0(s)}          & \textbf{ 1010.6(s)} \\

\bottomrule[1pt]
\end{tabular}
\caption{Editing time for two datasets on LLAMA2-7B.}
\label{tbl:testtime}
\vspace{-1em}
\end{table}

% \begin{table*}[t]
% \centering
% \setlength\tabcolsep{3pt} 
% \renewcommand\arraystretch{1.0}

% \begin{tabular}{lcccccc}
% \toprule[1pt]
%  \multirow{2}{*}{\textbf{Editor}} & \multicolumn{2}{c}{\textbf{1 edit}} & \multicolumn{2}{c}{\textbf{10 edits}} & \multicolumn{2}{c}{\textbf{100 edits}}                 \\ \cmidrule(r){2-3}   \cmidrule(r){4-5}   \cmidrule(r){6-7}
%     & \textbf{\small{zsRE}}     & \textbf{\textsc{\small{COUNTERFACT}}}  & \textbf{\small{zsRE}}     & \textbf{\textsc{\small{COUNTERFACT}}}  & \textbf{\small{zsRE}}     & \textbf{\textsc{\small{COUNTERFACT}}}        \\ \hline
        
    
%  FT-c                 
%  & 0.57(s)          & 0.54(s)      
%  & 6.58(s)          & 6.82(s)
%  & 21.77(s)          & 19.30(s)\\
    
% ROME                    
% & 20.47(s)          & 18.27(s)
% & 207.09(s)          & 179.30(s)
% & 2184.16(s)          & 1810.42(s)\\
% MEMIT                    
% & 28.32(s)          & 23.71(s)
% & 108.67(s)          & 96.71(s)
% & 862.20(s)          & 847.72(s)\\
% Full-FT                    
% & 0.76(s)          & 0.72(s)
% & 7.8(s)          & 7.3(s)
% & 25.36(s)          & 24.70(s)\\
 
% F-Learning$_{\rm{FT}}$    
% & 1.58(s)          & 1.47(s)
% & 15.32(s)          & 14.9(s)
% & 52.20(s)          & 50.12(s) \\

% \textbf{{\ouralg}$_{\rm{FT}}$}     
% & \textbf{158.99(s)}          & \textbf{ 159.96(s)}
% & \textbf{ 166.88(s)}          & \textbf{ 165.94(s)}
% & \textbf{183.03(s)}          & \textbf{ 182.26(s)} \\

% \bottomrule[1pt]
% \end{tabular}
% \caption{Editing time for 1 edit, 10 edits, 100 edits of the two dataset based on LLAMA2-7B.}
% \label{tbl:testtime}
% \vspace{-1em}
% \end{table*}

% ae train  150
% editing 8