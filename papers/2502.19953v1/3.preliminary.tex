

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{imgs/Fig2_new3.pdf}
\caption{\textbf{Overview of {\ouralg}.} \textbf{Step (a)}: Neuron-level task vectors $\tau_{old}$ and $\tau_{new}$ are extracted for both the old and new knowledge datasets using parametric arithmetic. \textbf{Step (b)}: An auto-encoder is trained to project a low-dimensional representation of the task vectors, eliminating the angular bias issue in high-dimensional space. \textbf{Step (c)}: The latent task vectors, $h_{new}$ and $h_{old}$, are used to classify neurons based on the angle between their corresponding task vectors, 
followed by the application of different editing strategies to obtain \( h_{edit} \). Finally, \( h_{edit} \) is passed through the decoder to produce \( \tau_{edit} \), which is added to the initial model to generate the edited model \(f_{\theta_{e}}\).}
  \label{fig:method}
\end{figure*}


\paragraph{Task Definition}
Model editing, also referred to as knowledge updating, involves modifying the behavior of an initial target model on specific edit examples without compromising its performance on unrelated examples.
More precisely, given an initial model \(f_{\theta}\) and a set of input-output knowledge pairs $D_{old} = \{ (x_1, y_1), (x_2, y_2), \dots, (x_k, y_k) \}$, the task is to update the model parameters to obtain a new model \(f_{\theta_{e}}\) and a corresponding set of new input-output pairs $D_{new} = \{ (x_1, y_1^{\text{new}}), (x_2, y_2^{\text{new}}), \dots, (x_k, y_k^{\text{new}}) \}$, where \( k \) denotes the number of knowledge pairs to be updated. 
%To illustrate this process, consider an edit example \((x_i, y_i^{\text{new}})\) with the edit label \(y_i^{\text{new}} \neq f_{\theta}(x_{i})\), the post-edit model \(f_{\theta_{e}}\) should generate the expected output such that \(f_{\theta_{e}}(x_{e}) = y_{e}\).
%We introduce the objectives of existing training-based model editing methods.
The objective of the post-edit model \(f_{\theta_{e}}\) is to meet three essential properties: reliability, generality, and locality~\cite{wang2024sss}.

%An ideal post-edit model \(f_{\theta_{e}}\) should satisfy three properties: reliability, generality, and locality~\cite{wang2024sss}.


\paragraph{Reliability} 
Reliability measures the accuracy of the updated model on the new knowledge.
Specifically, the output for ``Who is the President of the US?'' should be updated from ``Joe Biden'' to ``Donald Trump.'' This can be formalized as follows:
\begin{equation}
\mathbb{E}_{x_{e}, y_{e} \sim D_{new} } \mathbbm{1}\left\{\text{argmax}_{y} f_{\theta_{e}}(y | x_{e}) = y_{e}\right\}.
\end{equation}


\paragraph{Generality} Generality means that the new model \(f_{\theta_e}\) should also update rephrased in-scope examples \(I(x_{e}, y_{e})\). Such as the answer to ``Who holds the position of the President of the US?'' should also be changed from ``Joe Biden'' to ``Donald Trump''. This is evaluated by the average accuracy of \(f_{\theta^*}\) on examples from the equivalence neighborhood, as expressed by:
%As the model editing process can impact a wide range of examples that are closely associated with the edit example, known as the editing scope, a successful edit should also adjust the model behavior for in-scope examples \(I(x_{e}, y_{e})\), such as examples with similar expressions:
\begin{equation}
\mathbb{E}_{x^{\prime}_{e}, y^{\prime}_{e} \sim I(x_{e}, y_{e})} \mathbbm{1}\left\{\text{argmax}_{y} f_{\theta_{e}}(y | x^{\prime}_{e}) = y^{\prime}_{e}\right\}.
\end{equation}

    
\paragraph{Locality} 
A good edit should modify relevant knowledge without affecting other irrelevant out-of-scope examples \(O(x_{e}, y_{e})\). For example, the question, ``Who said: this is a battle for the soul of the nation?'' should remain unchanged as ``Joe Biden''.
Locality (or specificity) is defined as:
%This principle of locality (or specificity) can be defined as follows:
\begin{equation}
\mathbb{E}_{x^{\prime}_{e}, y^{\prime}_{e} \sim O(x_{e}, y_{e})} \mathbbm{1}\left\{\text{argmax}_{y} f_{\theta_{e}}(y | x^{\prime}_{e}) = f_{\theta}(y | x^{\prime}_{e})\right\}.
\end{equation}
