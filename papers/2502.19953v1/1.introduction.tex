

\begin{figure}[t]
  \centering
  \includegraphics[width=1\linewidth]{imgs/Fig1_new5.pdf}
  \caption{
  Conceptual illustration of F-Learning~\cite{ni-etal-2024-forgetting} and our proposed {\ouralg}. 
  }
  \label{fig:intro}
\end{figure}

Large language models (LLMs) have demonstrated the ability to store vast amounts of knowledge during pre-training and retrieve it during inference~\cite{yao2023editing, wang2024knowledge}. However, much of the knowledge in the real world is constantly evolving. For instance, the answer to the question ``Who is the President of the United States?'' was ``Joe Biden'' in 2024, but it is now ``Donald Trump''. As a result, some knowledge that was once correct in LLMs can become obsolete or inaccurate~\cite{li2024can, huang2024commonsense}.

To address this issue, model editing methods have been developed to update the target new knowledge while preserving unrelated general knowledge within the model~\cite{hong2024interpretability, ma2024robustness, wang2024editing}. Specifically, current model editing methods typically follow the ``locate-and-edit'' paradigm~\cite{wang2023cross, wang2024roselora, li2024consecutive}. The core idea is first to locate influential weights in LLMs and then edit them by introducing a perturbation.
%While effective, these approaches rely on the locality hypothesis of factual knowledge~\cite{hase2024does}, which requires sampling large amounts of data (such as from Wikipedia) to access the full extent of knowledge within the LLM. They aim to differentiate between general and new knowledge during editing~\cite{meng2022locating, meng2022mass, fang2024alphaedit}. However, these methods introduce additional costs and biases due to incomplete and random sampling.
Although effective, these approaches incur significant computational overhead to identify the important neurons and parameters~\cite{meng2022locating}. 
Some methods also require sampling additional data (e.g., from Wikipedia) to mitigate the impact on the general knowledge within LLMs during editing~\cite{meng2022mass, fang2024alphaedit}, introducing extra costs and potential biases.


In contrast, fine-tuning with updated knowledge, as demonstrated in recent studies~\cite{zhao2024ripplecot, wang2024lemoe, liu2024evedit}, offers a more straightforward solution through the use of parameter-efficient fine-tuning (PEFT) techniques. These methods employ various strategies to edit the model without the need to differentiate the importance of individual parameters~\cite{feng2025recurrentkif}.
%offers a more straightforward solution and is agnostic to model architectures~\cite{zhao2024ripplecot, wang2024lemoe, liu2024evedit}. 
%However, a major drawback of fine-tuning methods is their tendency to degrade the model's general knowledge.
Among these, the pioneering work F-Learning \cite{ni-etal-2024-forgetting} introduces a new learning framework called ``Forgetting before Learning,'' as illustrated in Figure~\ref{fig:intro}(a). This approach is based on the empirical observation that new knowledge can be difficult to learn when it conflicts with existing knowledge. By first forgetting outdated knowledge, the learning of new knowledge becomes easier.
%without conflict.
However, this approach has a critical limitation: it struggles to balance the integration of new knowledge with the preservation of existing general knowledge. Specifically, F-Learning assumes that all updates between old and new knowledge are inherently conflicting, which oversimplifies the complexity of knowledge integration.
%Furthermore, the unconstrained forgetting process can significantly harm generalization performance, leading to a more than 30\% decline in locality compared to the best methods, despite achieving state-of-the-art performance in reliability and generality. 
Furthermore, the uncontrained forgetting process can significantly impact the model's generalization ability to out-of-scope samples, considerably reducing the performance of the Locality metric (see Section~\ref{sec:Preliminary}).

% To address these flaws, we propose Geometric Knowledge Editing (GeoEdit), a novel framework that dynamically quantifies conflicts between old and new knowledge through angular divergence using task arithmetic~\cite{ilharco2022editing}, and then tailors editing strategies according to different conflict regimes. 
To address these limitations, we propose Geometric Knowledge Editing (\textbf{{\ouralg}}), a novel fine-tuning-based model editing framework that enhances editing precision while strongly preserving model generalization without the need for additional unrelated data. 
%The core insight of GeoEdit is to distinguish between new knowledge updates and general knowledge perturbations in the model parameters by analyzing the geometric relationships of parameter updates in high-dimensional space after fine-tuning.
The core insight of {\ouralg} is to distinguish between neurons associated with new knowledge updates and those linked to general knowledge perturbations by analyzing the geometric relationships of parameter updates caused by fine-tuning. By masking the updates of \textit{\textbf{general-knowledge-related neurons}}, we prevent negative impacts on the model's generalization ability. At the same time, we optimize the update strategy for \textit{\textbf{new-knowledge-related neurons}}, further enhancing the effectiveness of model editing.


% Specifically, GeoEdit first fine-tunes the initial model separately on old and new knowledge datasets, deriving neuron-level task vectors $\tau_{old}$, $\tau_{new}$ using task arithmetic~\cite{ilharco2022editing} to capture the directions of knowledge retention and knowledge updating, as shown in Figure~\ref{fig:intro}(b).

%In contrast to F-Learning, which follows a sequential process of forgetting before learning, {\ouralg} employs a parallel training pipeline. 
Specifically, we first fine-tune the current model separately on the old and new knowledge datasets. 
This allows us to derive neuron-level task vectors, $\tau_{old}$ and $\tau_{new}$, using task arithmetic~\cite{ilharco2022editing}, which capture the directions of knowledge retention and updating w.r.t. each neuron, as shown in Figure~\ref{fig:intro}(b).
%In detail, {\ouralg} first fine-tunes the initial model separately on old and new knowledge datasets, deriving neuron-level task vectors $\tau_{old}$, $\tau_{new}$ using task arithmetic~\cite{ilharco2022editing} to capture the directions of knowledge retention and knowledge updating, as shown in Figure~\ref{fig:intro}(b).
We then introduce a direction-aware knowledge identification method that computes the angle $\phi$ between these two directions to classify neurons, followed by customized editing strategies:
(i) \textit{\textbf{Orthogonal Knowledge Editing}} (for approximately orthogonal directions): Neurons with updates orthogonal to old knowledge are classified as general-knowledge-related neurons.
%These updates are deemed harmful to the model's generalization ability, so we do not update these neurons.  $\phi \approx 90^\circ$  $\phi < 90^\circ$
These updates are considered detrimental to the model's generalization ability, so we refrain from updating these neurons.
The remaining neurons are treated as new-knowledge-related neurons, which are updated with two different strategies:
(ii) \textit{\textbf{Synergistic Knowledge Editing}} (for aligned directions): 
%Minimal conflict between old and new knowledge allows us to leverage their similarity, enabling the simultaneous integration of both to enhance the editing process.
When there is slight conflict between old and new knowledge, we can leverage their similarities to simultaneously integrate both.
(iii) \textit{\textbf{Conflicting Knowledge Editing}} (for opposite directions): For updates with significant conflict, we apply the F-Learning strategy, where old knowledge is first forgotten before integrating new information.
%Conflicting updates prompt the forget-then-learn strategy, where old knowledge is discarded before new knowledge is integrated.

% (i) \textit{\textbf{Orthogonal Knowledge Editing}} ($\phi \approx 90^\circ$): Updates orthogonal to old knowledge are identified as harmful perturbations to general knowledge. To prevent degradation of model generalization, we mask $\tau_{new}$ to zero.
% (ii) \textit{\textbf{Synergistic Knowledge Editing}} ($\phi < 90^\circ$): Minimal conflict between old and new knowledge allows us to leverage their similarity, facilitating an enhanced knowledge update.
% (iii) \textit{\textbf{Conflict Knowledge Editing}} ($\phi > 90^\circ$): Contradictory updates prompt the forget-then-learn strategy, where old knowledge is first forgotten before new knowledge is learned.

% Next, we introduce a Direction-Aware Geometric Knowledge Editing mechanism, which classifies parameter updates into three categories based on the angle $\theta$ between the old and new knowledge task vectors:
% \begin{itemize}[leftmargin=*,itemsep=2pt,topsep=0pt,parsep=0pt]
% \item 
% Synergistic Knowledge Editing ($\theta < 90°$): This indicates minimal conflict between old and new knowledge, allowing us to leverage their similarity to enhance knowledge update.
% \item 
% Orthogonal Knowledge Editing ($\theta \approx 90°$): This indicates that the knowledge updates are orthogonal and unrelated. We mask the new knowledge update to preserve the original knowledge, ensuring minimal disruption to the model's generalization capabilities.
% \item 
% Conflict Knowledge Editing ($\theta > 90°$): It indicates a significant conflict between the old and new knowledge. We adopt the F-Learning strategy of forgetting old knowledge first before learning the new knowledge, thus improving the effectiveness of knowledge updating.
% \end{itemize}

%Additionally, to address the angular bias in high-dimensional space, GeoEdit maps both the old and new knowledge task vectors into lower-dimensional semantic latent spaces using an autoencoder. Angle calculations and related editing operations are performed in this low-dimensional space, after which the decoder reconstructs the original high-dimensional vectors. We also introduce an Importance-guided Task Vector Fusion technique, which guides the fusion of task vectors with weighted parameters according to the editing strategy.
Additionally, to mitigate angular bias in high-dimensional space, {\ouralg} projects the task vectors into a lower-dimensional semantic space using an auto-encoder. Angle calculations and editing operations are performed in this compressed space, after which the decoder reconstructs the final editing task vector $\tau_{edit}$ for parameter updates.
To optimize vector fusion, we introduce an importance-guided task vector fusion technique,  which applies fine-grained weights to the vectors and suppresses noise from redundant parameters, further enhancing the effectiveness of model editing.
%Extensive experiments demonstrate that GeoEdit not only enhances the effectiveness of new knowledge updates but also preserves general knowledge, achieving an average xx% improvement in the locality metric compared to F-Learning.

Our main contributions are summarized as:
\begin{itemize}[leftmargin=*,itemsep=2pt,topsep=0pt,parsep=0pt]
\item 

%We propose GeoEdit, a novel model editing \textbf{framework} that analyzes geometric relationships of parameter updates after fine-tuning to effectively identify and mitigate disturbances to general knowledge, preserving generalization.
We propose a novel geometric knowledge editing \textbf{framework} ({\ouralg}) for updating LLMs.


\item 
%We introduce an importance-guided task vector fusion \textbf{technique} that classifies model edits into synergistic, orthogonal, and conflict editing based on parameter update angles, while weighting by parameter importance to enhance editing effectiveness and filter out redundant parameters.
We develop new direction-aware knowledge identification and importance-guided task vector fusion \textbf{techniques}.


\item 
%Extensive \textbf{evaluation} on two datasets demonstrates that GeoEdit overcomes F-Learning's limitations, improving locality by xx\% while maintaining high reliability and generality.
Extensive \textbf{evaluation} on two widely-used datasets shows that {\ouralg} overcomes the limitations of F-Learning, improving the Locality metric by 7.4\% while maintaining the best performance in the Reliability and Generality metrics.

\end{itemize}