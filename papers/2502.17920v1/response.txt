\section{Related Work}
%\subsection{Continual Learning}

Continual Learning (CL) requires model to continuously learn and recognize new classes without forgetting what it has previously learned **Parisi, "Continual Lifelong Learning"**. Traditional CL methods can be grouped into four main approaches: replay-based, knowledge distillation, model architecture, and regularization-based methods. Replay-based methods **Shim**, "Incremental Learning Using Incremental Norm" involve store a subset of data from previous classes and replay them during training on new classes to preserve existing knowledge. Knowledge distillation methods **Li**, "Learning Transferable Features with Simultaneous Domain Adaptation and Partial Labeling" retain information from previously learned classes by using auxiliary loss functions that ensure the outputs of the old model is preserved while learning new classes. Model architecture methods **Serra**, "Overcoming Catastrophic Forgetting with Unsupervised Domain Adaptation" modify the network structure to integrate new classes by designing specialized parameter modules for each stages, ensuring that the learning of new classes doesn't interfere with previously acquired knowledge. Regularization-based methods **Kemker**, "A Tutorial on Continual Learning" impose constraints on the modelâ€™s parameters to prevent substantial changes that could lead to forgetting, thereby maintaining the integrity of important weights.


LoRA **Zhang**, "Linear Layer Adaptation for Efficient Model Fine-tuning" is an efficient model fine-tuning method. Instead of fine-tuning the entire pre-trained model, LoRA fine-tunes specific submodules of the pre-trained model by inserting low-rank matrices. Traditional LoRA performs well in static task environments, where the tasks and data remain fixed during training **Sun**, "Efficient Model Fine-Tuning with Low-Rank Matrices" . In continual learning settings, however, the traditional LoRA method faces challenges, particularly in handling the arrival of new tasks while retaining knowledge from previous ones to prevent catastrophic forgetting.
To address this challenge, a prevalent strategy involves combining multiple LoRA modules, each responsible for handling specific tasks within the input data **Kim**, "Efficient Continual Learning with Low-Rank Matrices" . The Mixture of Experts (MoE) **Mukhoti**, "Mixture of Experts for Efficient Model Fine-Tuning" framework integrates multiple expert networks and assigns tasks through a gating mechanism, thereby enhancing the model's performance across diverse tasks. Inspired by this, MoE-LoRA **Kim**, "Efficient Continual Learning with Mixture of Experts" introduces a mixture of experts model to allocate and select LoRA modules, reducing conflicts between new and existing tasks. Additionally, infLoRA **Gao**, "Independent LoRA for Efficient Continual Learning" uses independent LoRA submodules for each new task and employs regularization techniques to minimize interference between new and old tasks. EASE **Li**, "Efficient Adaptation with Shared Experts" addresses data distribution changes by storing prototypes of each task-specific subspace and adopting a semantic-guided prototype augmentation strategy. Online-LoRA **Wang**, "Online Weight Regularization for Efficient Continual Learning" implements an online weight regularization strategy that automatically identifies changes in data distribution to reduce forgetting.
However, these methods rely on multiple LoRA modules, leading to a linear increase in parameter and computational overhead, and the challenge of effectively integrating all LoRA modules during inference.