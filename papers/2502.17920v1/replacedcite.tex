\section{Related Work}
%\subsection{Continual Learning}

Continual Learning (CL) requires model to continuously learn and recognize new classes without forgetting what it has previously learned ____. Traditional CL methods can be grouped into four main approaches: replay-based, knowledge distillation, model architecture, and regularization-based methods. Replay-based methods ____ involve store a subset of data from previous classes and replay them during training on new classes to preserve existing knowledge. Knowledge distillation methods ____ retain information from previously learned classes by using auxiliary loss functions that ensure the outputs of the old model is preserved while learning new classes. Model architecture methods ____ modify the network structure to integrate new classes by designing specialized parameter modules for each stages, ensuring that the learning of new classes doesn't interfere with previously acquired knowledge. Regularization-based methods ____ impose constraints on the modelâ€™s parameters to prevent substantial changes that could lead to forgetting, thereby maintaining the integrity of important weights.


LoRA ____ is an efficient model fine-tuning method. Instead of fine-tuning the entire pre-trained model, LoRA fine-tunes specific submodules of the pre-trained model by inserting low-rank matrices. Traditional LoRA performs well in static task environments, where the tasks and data remain fixed during training ____. In continual learning settings, however, the traditional LoRA method faces challenges, particularly in handling the arrival of new tasks while retaining knowledge from previous ones to prevent catastrophic forgetting.
To address this challenge, a prevalent strategy involves combining multiple LoRA modules, each responsible for handling specific tasks within the input data ____. The Mixture of Experts (MoE) ____ framework integrates multiple expert networks and assigns tasks through a gating mechanism, thereby enhancing the model's performance across diverse tasks. Inspired by this, MoE-LoRA ____ introduces a mixture of experts model to allocate and select LoRA modules, reducing conflicts between new and existing tasks. Additionally, infLoRA ____ uses independent LoRA submodules for each new task and employs regularization techniques to minimize interference between new and old tasks. EASE ____ addresses data distribution changes by storing prototypes of each task-specific subspace and adopting a semantic-guided prototype augmentation strategy. Online-LoRA ____ implements an online weight regularization strategy that automatically identifies changes in data distribution to reduce forgetting.
However, these methods rely on multiple LoRA modules, leading to a linear increase in parameter and computational overhead, and the challenge of effectively integrating all LoRA modules during inference.

%% ----------  Methodology  ----------- %%