\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{balance}
\usepackage{booktabs}
\usepackage{float}
\usepackage{cite}


\begin{document}
\title{C-LoRA: Continual Low-Rank Adaptation for Pre-trained Models}
\author{Xin Zhang, Liang Bai, Xian Yang, Jiye Liang~\IEEEmembership{Fellow,~IEEE}
	
	\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem
		Xin Zhang, Liang Bai and Jiye Liang are with Institute of Intelligent Information Processing, Shanxi University, Taiyuan, 030006, China (Corresponding author: Liang Bai)\protect\\
		Email: 202322407065@email.sxu.edu.cn,  bailiang@sxu.edu.cn, ljy@sxu.edu.cn
		
		Xian Yang is  with Alliance Manchester Business School, The University of Manchester, Manchester, M13 9PL, UK\protect\\
		Email: xian.yang@manchester.ac.uk
	}
	\thanks{}
}

%\markboth{Journal of \LaTeX\ Class Files,~Vol.~18, No.~9, September~2020}%
%{How to Use the IEEEtran \LaTeX \ Templates}

\maketitle

\begin{abstract}
Low-Rank Adaptation (LoRA) is an efficient fine-tuning method that has been extensively applied in areas such as natural language processing and computer vision. Existing LoRA fine-tuning approaches excel in static environments but struggle in dynamic learning due to reliance on multiple adapter modules, increasing overhead and complicating inference.
We propose Continual Low-Rank Adaptation (C-LoRA), a novel extension of LoRA for continual learning. C-LoRA uses a learnable routing matrix to dynamically manage parameter updates across tasks, ensuring efficient reuse of learned subspaces while enforcing orthogonality to minimize interference and forgetting. Unlike existing approaches that require separate adapters for each task, C-LoRA enables a integrated approach for task adaptation, achieving both scalability and parameter efficiency in sequential learning scenarios.
C-LoRA achieves state-of-the-art accuracy and parameter efficiency on benchmarks while providing theoretical insights into its routing matrix’s role in retaining and transferring knowledge, establishing a scalable framework for continual learning.
\end{abstract}

\begin{IEEEkeywords}
Continual Learning, Catastrophic Forgetting, Low-Rank Adapter, Pre-trained Model.
\end{IEEEkeywords}


\section{Introduction}
\IEEEPARstart{L}{ow-Rank} Adaptation (LoRA) \cite{hu2021lora} is a parameter-efficient fine-tuning method that adapts large-scale pre-trained models to downstream tasks by introducing low-rank matrices to update a small subset of parameters while keeping the core pre-trained weights frozen. This approach preserves the general knowledge captured during pre-training, reduces computational and storage costs, and mitigates the risk of overfitting by limiting updates to task-specific components \cite{ayoub2023requirements}. LoRA has proven highly effective in fields such as natural language processing and computer vision, where large models are commonly deployed for diverse and evolving applications \cite{yu2024boosting,zhou2024expandable,aljundi2017expert,hu2023dense}.

Despite its advantages, existing LoRA frameworks are optimized for static environments with fixed datasets and tasks. In dynamic and evolving scenarios, such as continual learning, traditional LoRA methods face limitations. Continual learning requires a model to learn from a continuous stream of tasks or data, accumulating knowledge over time without forgetting previously acquired information \cite{mcclelland1995there}. The goal of continual learning is to overcome catastrophic forgetting, where a model loses its ability to recall and perform well on earlier tasks while adapting to new ones, resulting in a decrease to effectively transfer and accumulate knowledge and leading to a decline in performance \cite{french1999catastrophic,mccloskey1989catastrophic,robins1995catastrophic,parisi2019continual}. New adapter modules are typically introduced for each task to prevent interference, resulting in Linear growth in parameters and computational overhead \cite{yu2024boosting,zhou2024expandable,aljundi2017expert,hu2023dense}. This approach complicates inference, as selecting the appropriate adapter becomes increasingly challenging with the growing number of tasks.

To address these limitations, we propose Continual Low-Rank Adaptation (C-LoRA), an extension of LoRA designed for dynamic environments. C-LoRA introduces a learnable routing matrix $\boldsymbol{\mathcal{R}}$, which decomposes into components managing both previous and new task parameters. By enforcing orthogonality constraints, C-LoRA reduces interference between tasks, mitigating catastrophic forgetting and enhancing the model's ability to retain prior knowledge. This design allows for parameter-efficient continual learning by leveraging shared low-rank subspaces across tasks, reducing the need for extensive storage and computational resources. The contributions of this work are listed as follows:


% -------------------- %

\begin{itemize}
	\item C-LoRA addressed the question,  “Can One LoRA Replace Multiple LoRAs?” and shed light on the forgetting mechanisms between the routing matrix $\boldsymbol{\mathcal{R}}$ ande the LoRA subspace.
	
	\item C-LoRA subspace design and orthogonality constraint eliminate interference form new task on old tasks, mitigating catastrophic forgetting and enhancing the model's ability to retain prior knowledge.
	
	\item Experimental results show that C-LoRA outperforms existing state-of-the-art continual learning methods across multiple datasets.\\
\end{itemize}


\section{Related Work}

%\subsection{Continual Learning}

Continual Learning (CL) requires model to continuously learn and recognize new classes without forgetting what it has previously learned \cite{dong2022federated,dong2023federated,gao2022r,goswami2024fecam,wang2022beef,zhao2021mgsvf}. Traditional CL methods can be grouped into four main approaches: replay-based, knowledge distillation, model architecture, and regularization-based methods. Replay-based methods \cite{liu2021rmm,luo2023class,wu2019large,buzzega2020dark,wang2022memory,wang2021triple,rebuffi2017icarl} involve store a subset of data from previous classes and replay them during training on new classes to preserve existing knowledge. Knowledge distillation methods \cite{dhar2019learning,douillard2020podnet,li2017learning,simon2021learning,tao2020topology} retain information from previously learned classes by using auxiliary loss functions that ensure the outputs of the old model is preserved while learning new classes. Model architecture methods \cite{shi2022mimicking,zhao2020maintaining,hu2023dense,wang2023incorporating,yang2022continual} modify the network structure to integrate new classes by designing specialized parameter modules for each stages, ensuring that the learning of new classes doesn't interfere with previously acquired knowledge. Regularization-based methods \cite{zhou2023hierarchical,aljundi2018memory,zenke2017continual,xiang2022coarse,dhar2019learning,li2017learning} impose constraints on the model’s parameters to prevent substantial changes that could lead to forgetting, thereby maintaining the integrity of important weights.


LoRA \cite{hu2021lora} is an efficient model fine-tuning method. Instead of fine-tuning the entire pre-trained model, LoRA fine-tunes specific submodules of the pre-trained model by inserting low-rank matrices. Traditional LoRA performs well in static task environments, where the tasks and data remain fixed during training \cite{hayou2024lora+,li2023loftq,kopiczko2023vera}. In continual learning settings, however, the traditional LoRA method faces challenges, particularly in handling the arrival of new tasks while retaining knowledge from previous ones to prevent catastrophic forgetting.
To address this challenge, a prevalent strategy involves combining multiple LoRA modules, each responsible for handling specific tasks within the input data \cite{wei2024online,zhou2024expandable,liang2024inflora}. The Mixture of Experts (MoE) \cite{jacobs1991adaptive,shazeer2017outrageously} framework integrates multiple expert networks and assigns tasks through a gating mechanism, thereby enhancing the model's performance across diverse tasks. Inspired by this, MoE-LoRA \cite{yu2024boosting} introduces a mixture of experts model to allocate and select LoRA modules, reducing conflicts between new and existing tasks. Additionally, infLoRA \cite{liang2024inflora} uses independent LoRA submodules for each new task and employs regularization techniques to minimize interference between new and old tasks. EASE \cite{zhou2024expandable} addresses data distribution changes by storing prototypes of each task-specific subspace and adopting a semantic-guided prototype augmentation strategy. Online-LoRA \cite{wei2024online} implements an online weight regularization strategy that automatically identifies changes in data distribution to reduce forgetting.
However, these methods rely on multiple LoRA modules, leading to a linear increase in parameter and computational overhead, and the challenge of effectively integrating all LoRA modules during inference.

%% ----------  Methodology  ----------- %%

\section{Methodology}

In this chapter, we define the problem of Continual Learning and address the core question—“Can a single LoRA replace multiple LoRAs?” we propose a novel learnable matrix \( \mathbf{\boldsymbol{\mathcal{R}}} \) that dynamically controls how parameter subspaces are activated and updated, with its element magnitudes influencing the degree of forgetting. Excessive updates to shared subspaces risk overwriting prior knowledge. Based on this analysis, we propose the C-LoRA architecture and its training paradigm.

\subsection{Preliminaries}
LoRA \cite{hu2021lora} is a parameter-efficient fine-tuning method that adapts large-scale pre-trained models to specific tasks by introducing low-rank matrices. For a pre-trained linear layer $\mathbf{W_0} \in \mathbb{R}^{d \times k}$, LoRA applies a low-rank update $\Delta \mathbf{W} = \mathbf{AB}$, where $\mathbf{A} \in \mathbb{R}^{d \times r}$ and $\mathbf{B} \in \mathbb{R}^{r \times k}$ are trainable matrices with rank $r \ll \min(d, k)$. The updated weight for task $t$ is:
\begin{equation}
	\mathbf{W}_t = \mathbf{W}_0 + \Delta \mathbf{W}_t = \mathbf{W}_0 + \mathbf{A}_t \mathbf{B}_t,
\end{equation}
where $\mathbf{A}_t$ and $\mathbf{B}_t$ are the task-specific low-rank matrices. By keeping $\mathbf{W}_0$ frozen and only updating $\mathbf{A}_t$ and $\mathbf{B}_t$, LoRA achieves significant parameter efficiency compared to full fine-tuning.

In \textit{continual learning} scenarios, a model must learn a sequence of tasks $\{\mathcal{T}_1, \mathcal{T}_2, \dots, \mathcal{T}_T\}$ without forgetting prior knowledge. To prevent task interference, existing LoRA-based methods \cite{wei2024online,zhou2024expandable,liang2024inflora} introduce a separate low-rank update $\Delta \mathbf{W}_t = \mathbf{A}_t \mathbf{B}_t$ for each task $\mathcal{T}_t$. As a result, the model maintains a collection of task-specific parameters:
\begin{equation}
	\{(\mathbf{A}_1, \mathbf{B}_1), (\mathbf{A}_2, \mathbf{B}_2), \dots, (\mathbf{A}_T, \mathbf{B}_T)\}.
\end{equation}

However, this approach has two major limitations: 1) \textbf{Parameter Growth:} The number of LoRA modules grows linearly with the number of tasks, leading to significant storage and computational overhead. 2) \textbf{Inference Complexity:} During inference, selecting the correct adapter $(\mathbf{A}_T, \mathbf{B}_T)$ for a given task $\mathcal{T}_t$ becomes increasingly cumbersome as the number of tasks $T$ grows.
These challenges motivate the need for a approach that avoids parameter proliferation while retaining knowledge across tasks. Specifically, we pose the question:
\textbf{Can a single LoRA replace multiple LoRAs in continual learning?}
This question forms the foundation of our proposed approach, introduced in the next section.

\subsection{Continual Low-Rank Adaptation}

To address the limitations of existing LoRA-based methods in continual learning, we propose \textit{Continual Low-Rank Adaptation (C-LoRA)}. The core idea of C-LoRA is to replace multiple task-specific LoRA modules with a single adaptable LoRA mechanism, managed by a learnable routing matrix $\boldsymbol{\mathcal{R}}$.



The routing matrix $\boldsymbol{\mathcal{R}}$ dynamically controls how parameter subspaces are activated and updated for each task. Formally, for a pre-trained linear layer with weights $\mathbf{W}_0$, the task-specific adaptation in C-LoRA is expressed as:
\begin{equation}
	\mathbf{W}_t = \mathbf{W}_0 + \Delta \mathbf{W}_t, \quad \Delta \mathbf{W}_t = \mathbf{A} \boldsymbol{\mathcal{R}} \mathbf{B},
\end{equation}
where $\mathbf{A} \in \mathbb{R}^{d \times r}$ and $\mathbf{B} \in \mathbb{R}^{r \times k}$ are low-rank matrices shared across tasks, and $\boldsymbol{\mathcal{R}}\in \mathbb{R}^{r \times r}$ is the learnable routing matrix that determines the dynamic contribution of different subspaces.

As illustrated in Figure~\ref{fig:1}, the routing matrix $\boldsymbol{\mathcal{R}}$ generalizes the concept of a MoE \cite{yu2024boosting} by integrating contributions from multiple subspaces into a single low-rank adaptation framework. In MoE, each expert is associated with a weight $w_i$ and distinct low-rank projections $\mathbf{A}_i \in \mathbb{R}^{d \times \frac{r}{h}}$ and $\mathbf{B}_i \in \mathbb{R}^{\frac{r}{h} \times k}$, with the output aggregated as:
\begin{equation}
	\sum_{i=1}^h w_i \mathbf{A}_i \mathbf{B}_i.
\end{equation}
C-LoRA simplifies this structure by combining the subspaces into shared low-rank matrices $\mathbf{A}$ and $\mathbf{B}$ and using $\boldsymbol{\mathcal{R}}$ to represent the routing mechanism:
\begin{equation}
	\boldsymbol{\mathcal{R}} = \text{diag}(w_1 \mathbf{I}_\frac{r}{h}, w_2 \mathbf{I}_\frac{r}{h}, \dots, w_h \mathbf{I}_\frac{r}{h}),
\end{equation}
where $w_i \in \mathbb{R}$ are the weights associated with subspaces, $\mathbf{I}_\frac{r}{h}$ is an identity matrix of size $\frac{r}{h} \times \frac{r}{h}$, and $h$ represents the number of subspaces. This design allows efficient sharing of subspaces while isolating task-specific updates.

During training, $\boldsymbol{\mathcal{R}}$ is decomposed into $\boldsymbol{\mathcal{R}}_{\text{old}}$ and $
\boldsymbol{\mathcal{R}}_{\delta},$where $\boldsymbol{\mathcal{R}}_{\text{old}}$ captures shared knowledge from previous tasks, and $\boldsymbol{\mathcal{R}}_{\delta }$ introduces task-specific updates for the current task. Orthogonality constraints are applied to $\boldsymbol{\mathcal{R}}_{\delta }$, ensuring that updates for new tasks do not interfere with shared knowledge. As highlighted in Figure~\ref{fig:1}, this decomposition decouples the shared and task-specific components, with $\boldsymbol{\mathcal{R}}_{\text{old}}$ remaining frozen during gradient computation, ensuring efficient adaptation.

By leveraging the routing matrix $\boldsymbol{\mathcal{R}}$, C-LoRA achieves a mechanism for continual learning that consolidates multiple task-specific LoRA modules into a single adaptable framework. This design preserves previously learned information while enabling scalable and efficient adaptation to new tasks. In the following sections, we provide detailed theoretical insights into $\boldsymbol{\mathcal{R}}$ and describe the training paradigm of C-LoRA.

% ========
% ----- draw1 ----- % start

\begin{figure}[!t]
	\centering
	% 第一行
	\begin{minipage}{0.9\textwidth}
		\centering
		%\hspace{-0.1\textwidth} % 向左移动图片，调整空白
		\includegraphics[width=\textwidth]{draw1.pdf}
	\end{minipage}%
	\caption{ Comparison of MoE and proposed C-LoRA architecture. We extend the equivalent form of MoE and decompose the routing matrix $\boldsymbol{\mathcal{R}}$ into two parts: $\boldsymbol{\mathcal{R}}_\text{old}$ and $\boldsymbol{\mathcal{R}}_{\delta }$, where $\boldsymbol{\mathcal{R}}_\text{old}$ does not participate in gradient computation during training.}
	\label{fig:1} 
\end{figure}

% ----- draw1 ----- % end

\subsection{The Role of $\boldsymbol{\mathcal{R}}$ in Forgetting Control}

In this subsection, we further analyze the role of matrix \( \boldsymbol{\mathcal{R}} \) during the forward and backward propagation processes, establishing the theoretical foundation for our C-LoRA architecture. For clarity, we focus on a linear segment of the network, represented by \( \boldsymbol{y} = \boldsymbol{x}\mathbf{W} \), where the weight matrix \( \mathbf{W} \) is determined by \( \mathbf{W} = \mathbf{A \boldsymbol{\mathcal{R}} B} \). Since non-linear activation functions only perform non-linear transformations on the results of matrix multiplications and do not affect the low-rank decomposition of the weight matrices themselves, they are omitted here. 

\textbf{Representation of $\mathbf{A}$, $\mathbf{B}$, $\boldsymbol{\mathcal{R}}$:   }
Specifically, \( \mathbf{A} \in \mathbb{R}^{d \times r} \) is a matrix composed of \( r \) column vectors \( [\boldsymbol{a_1}, \boldsymbol{a_2}, \dots, \boldsymbol{a_r}] \), each \( \boldsymbol{a_i} \in \mathbb{R}^d \). \( \mathbf{B} \in \mathbb{R}^{r \times k} \) consists of \( r \) row vectors \( [\boldsymbol{b_1^\top}, \boldsymbol{b_2^\top}, \dots, \boldsymbol{b_r^\top}] \), each \( \boldsymbol{b_j^\top} \in \mathbb{R}^k \). Matrix \( \boldsymbol{\mathcal{R}} \in \mathbb{R}^{r \times r} \) contains elements \( r_{ij} \). We can express \( \mathbf{A} \), \( \boldsymbol{\mathcal{R}} \), and \( \mathbf{B} \) as follows:

\begin{equation}
	\label{e5}
	\mathbf{A} = \sum_{i=1}^{r} \boldsymbol{a_i e_i^\top}, \quad
	\boldsymbol{\mathcal{R}} = \sum_{i=1}^{r} \sum_{j=1}^{r} r_{ij} \boldsymbol{e_i e_j^\top}, \quad
	\mathbf{B} = \sum_{j=1}^{r} \boldsymbol{e_j b_j^\top},
\end{equation}

where \( \boldsymbol{e_i} \in \mathbb{R}^r \) is the \( i \)-th standard basis vector with a 1 in the $i$-th position, and all other entries are 0.

\textbf{Forward Propagation with $\boldsymbol{\mathcal{R}}$:  } During the forward propagation process, each element \( r_{ij} \) in matrix \( \boldsymbol{\mathcal{R}} \) determines the contribution of the \( i \)-th column of \( \mathbf{A} \) and the \( j \)-th row of \( \mathbf{B} \) to the feature transformation. Specifically,

\begin{small}
	\begin{equation}
		\label{e5}
		\begin{aligned}
			\mathbf{W} &= \left( \sum_{i=1}^{r} \boldsymbol{a_i e_i^\top} \right) \left( \sum_{i=1}^{r} \sum_{j=1}^{r} r_{ij} \boldsymbol{e_i e_j^\top} \right) \left( \sum_{j=1}^{r} \boldsymbol{e_j b_j^\top} \right) \\
			&= \sum_{i=1}^{r} \sum_{j=1}^{r} r_{ij} \boldsymbol{a_i b_j^\top}.
		\end{aligned}
	\end{equation}
\end{small}
Meaning that each non-zero element \( r_{ij} \) in \( \boldsymbol{\mathcal{R}} \) activates the \( i \)-th column vector \( \boldsymbol{a_i} \) of \( \mathbf{A} \) and the \( j \)-th row vector \( \boldsymbol{b_j^\top} \) of \( \mathbf{B} \) to varying degrees. Larger element values indicate a higher importance of the corresponding subspace in the current task.


\textbf{Backward Propagation and Gradient Updates:  }
During the backward propagation phase, matrix \( \boldsymbol{\mathcal{R}} \) plays a pivotal role in determining which parameters receive gradients and are subsequently updated. Specifically, Let \(\boldsymbol{G}\) represent the gradient with respect to \( \mathbf{W} \):
\begin{equation}
	\boldsymbol{G} = \frac{\partial \boldsymbol{y}}{\partial \mathbf{W}}.
\end{equation}
Consequently, the gradients of \( 	\mathbf{A} \) and \( 	\mathbf{B} \) are given by:

\begin{equation}
	\begin{aligned}
		\frac{\partial \boldsymbol{y}}{\partial \mathbf{A}} &= \boldsymbol{G} \mathbf{B^\top \boldsymbol{\mathcal{R}}^\top} = \boldsymbol{G} \left( \sum_{i=1}^{r} \sum_{j=1}^{r} r_{ij} \boldsymbol{b_j e_j^\top} \right), \\
		\frac{\partial \boldsymbol{y}}{\partial \mathbf{B}} &= \mathbf{\boldsymbol{\mathcal{R}}^\top A^\top} \boldsymbol{G} = \left( \sum_{i=1}^{r} \sum_{j=1}^{r} r_{ij} \boldsymbol{e_j a_i^\top} \right) \boldsymbol{G}.
	\end{aligned}
\end{equation}

Furthermore, the gradients of the \( i \)-th column of \( \mathbf{A} \) and the \( j \)-th row of \( \mathbf{B} \) can be expressed as:

\begin{equation}
	\left( \frac{\partial \boldsymbol{y}}{\partial \mathbf{A}} \right)_{:,i} = \boldsymbol{G} \sum_{j=1}^{r} r_{ij} \boldsymbol{b_j},   \quad
	\left( \frac{\partial \boldsymbol{y}}{\partial \mathbf{B}} \right)_{j,:} = \sum_{i=1}^{r} r_{ij} \boldsymbol{a_i^\top G}.
\end{equation}

These equations illustrate that each non-zero element \( r_{ij} \) in \( \mathbf{\boldsymbol{\mathcal{R}}} \) governs the gradient updates of the corresponding columns in \( \mathbf{A} \) and rows in \( \mathbf{B} \). The value of \( r_{ij} \) not only influences the representation capacity for the current task but also directly controls the extent of interference with parameters from previous tasks. A larger \( r_{ij} \) value indicates a higher importance of the corresponding subspace in the old task, making it more susceptible to overwriting or modifying previously acquired knowledge when learning new tasks. This, in turn, exacerbates the forgetting of knowledge from prior tasks.

\textbf{Decomposition of  $\boldsymbol{\mathcal{R}}$:   }To balance the conflict between newly acquired and previously learned knowledge, we propose decomposing matrix \( \mathbf{\boldsymbol{\mathcal{R}}} \) into two components:


\begin{equation}
	\mathbf{\boldsymbol{\mathcal{R}} = \boldsymbol{\mathcal{R}}_{\text{old}} + \boldsymbol{\mathcal{R}}_{\delta }}
\end{equation}


Here, \( \mathbf{\boldsymbol{\mathcal{R}}_{\text{old}}} \) accumulates the \( \mathbf{\boldsymbol{\mathcal{R}}_{\delta }} \) values learned from previous tasks, and preserves the importance of parameters in the subspace of old tasks. \( \mathbf{\boldsymbol{\mathcal{R}}_{\delta }} \) is dedicated to the incremental learning of new tasks. Specifically, \( \mathbf{\boldsymbol{\mathcal{R}}_{\delta }} \) is initialized as a near-zero matrix when a new task arrives, ensuring that learning new tasks minimally impacts the parameters of old tasks. This decomposition mechanism ensures that the critical parameters of old tasks are not significantly affected by the training of new tasks, while allowing the model to flexibly adjust and learn in new tasks. Specifically, during training, we isolate the parameter updates of the \( \mathbf{\boldsymbol{\mathcal{R}}_{\text{old}}} \) component, concentrating the majority of updates on the \( \mathbf{\boldsymbol{\mathcal{R}}}_{\delta } \) component. This approach protects the subspace of key parameters for old tasks from excessive interference by new tasks. After updating, our network can be expressed as:

\begin{equation}
	\boldsymbol{y'} = 	\boldsymbol{x}\mathbf{W'} \quad \text{where} \quad \mathbf{W'} = \phi(\mathbf{A \boldsymbol{\mathcal{R}}_{\text{old}} B}) + \mathbf{A \boldsymbol{\mathcal{R}}_{\delta } B},
\end{equation}

where $\phi(\cdot)$ denote that gradients are not computed for this operation. This ensures that the model leverages the knowledge from old tasks while incrementally learning new tasks, significantly mitigating the phenomenon of catastrophic forgetting.

\textbf{Theoretical Validation: } To further validate the effectiveness of this decomposition design, we present the following theorem:


% ----- theorem 3.1 ----- % start
\noindent \textbf{Theorem 3.1.} \emph{Let \( \mathbf{\boldsymbol{\mathcal{R}}} \) is devided into \( \boldsymbol{\mathcal{R}}_{\mathbf{\text{old}}} +  \mathbf{\boldsymbol{\mathcal{R}}}_{\delta} \) with \( \mathbf{\boldsymbol{\mathcal{R}}_{\text{old}}} \) fixed and \( \mathbf{\boldsymbol{\mathcal{R}}}_{\delta} \) trainable. If:}\\

1. \emph{\( \mathbf{\boldsymbol{\mathcal{R}}_{\text{old}}}^\top \mathbf{\boldsymbol{\mathcal{R}}}_{\delta} \) is positive definite.}

2. \emph{\( \mathbf{B G^\top} \neq 0 \) and \( \mathbf{G^\top A} \neq 0 \).}\\

\noindent \emph{Then, the upper bounds on the parameter changes of \( \mathbf{A} \) and \( \mathbf{B} \) using the decomposed routing \( \mathbf{\boldsymbol{\mathcal{R}}} \) are tighter compared to directly training \( \mathbf{\boldsymbol{\mathcal{R}}} \). Specifically:}
\begin{equation}
	\label{e5}
	\left\| \frac{\partial 	\boldsymbol{y}}{\partial \mathbf{A}} \right\|^2_F > \left\| \frac{\partial 	\boldsymbol{y'}}{\partial \mathbf{A}} \right\|^2_F, 
	\left\| \frac{\partial 	\boldsymbol{y}}{\partial \mathbf{B}} \right\|^2_F > \left\| \frac{\partial 	\boldsymbol{y'}}{\partial \mathbf{B}} \right\|^2_F.
\end{equation}

% ----- theorem 3.1 ----- % end

% Proof details are provided in Appendix A.
% ----- proof of theorem 3.1 ----- % start
\noindent \emph{Proof of }\textbf{Theorem 3.1.} Let $ \boldsymbol{G} = \frac{\partial \boldsymbol{y}}{\partial \mathbf{W}}.$ represent the gradient with respect to the weight matrix. The partial derivatives of \( y \) and \(y'\) with respect to \( A \) are given by:
\begin{equation}
	\label{e5}
	\begin{gathered}
		\frac{\partial \boldsymbol{y} }{\partial \mathbf{A}} = \mathbf{G B}^\top \boldsymbol{\mathcal{R}}^\top = \mathbf{G} (\boldsymbol{\mathcal{R}}_{\text{old}} + \boldsymbol{\mathcal{R}}_{\delta})^\top \mathbf{B}^\top,\\
		%\vspace{-5mm}
		\frac{\partial \boldsymbol{y'}}{\partial \mathbf{A}} = \mathbf{G} \boldsymbol{\mathcal{R}}_{\delta}^\top \mathbf{B}^\top.
	\end{gathered}
\end{equation}
\noindent Computing the squared Frobenius norm difference, we have:

\begin{small}
	\begin{equation}
		\begin{split}
			&\left\| \frac{\partial \boldsymbol{y}}{\partial \mathbf{A}} \right\|^2_F - \left\| \frac{\partial \boldsymbol{y'}}{\partial \mathbf{A}} \right\|^2_F
			= \left\| \mathbf{G B}^\top (\boldsymbol{\mathcal{R}}_{\text{old}} + \boldsymbol{\mathcal{R}}_{\delta})^\top \right\| - \left\| \mathbf{G B}^\top \boldsymbol{\mathcal{R}}_{\delta}^\top \right\| \\
			&\quad = \text{Tr}\left[ ( \mathbf{G B}^\top (\boldsymbol{\mathcal{R}}_{\text{old}} + \boldsymbol{\mathcal{R}}_{\delta})^\top)^\top ( \mathbf{G B}^\top (\boldsymbol{\mathcal{R}}_{\text{old}} + \boldsymbol{\mathcal{R}}_{\delta})^\top) \right] \\
			&\quad \quad - \text{Tr}\left[ (\mathbf{G B}^\top \boldsymbol{\mathcal{R}}_{\delta}^\top)^\top (\mathbf{G B}^\top \boldsymbol{\mathcal{R}}_{\delta}^\top) \right] \\
			&\quad = \text{Tr}\left[ (\boldsymbol{\mathcal{R}}_{\text{old}} + \boldsymbol{\mathcal{R}}_{\delta})\mathbf{B G}^\top \mathbf{G B}^\top (\boldsymbol{\mathcal{R}}_{\text{old}} + \boldsymbol{\mathcal{R}}_{\delta})^\top \right] \\
			&\quad\quad - \text{Tr}\left[ \boldsymbol{\mathcal{R}}_{\delta} \mathbf{B G}^\top \mathbf{G B}^\top \boldsymbol{\mathcal{R}}_{\delta}^\top \right] \\
			%&\quad = \text{Tr}\left[ \boldsymbol{\mathcal{R}}_{\text{old}} \mathbf{B G}^\top \mathbf{G B}^\top \boldsymbol{\mathcal{R}}_{\text{old}}^\top\right] + \text{Tr}\left[\delta\boldsymbol{\mathcal{R}} \mathbf{B G}^\top \mathbf{G B}^\top \boldsymbol{\mathcal{R}}_{\text{old}}^\top\right] \\
			%&\quad\quad + \text{Tr}\left[\boldsymbol{\mathcal{R}}_{\text{old}} \mathbf{B G}^\top \mathbf{G B}^\top \delta\boldsymbol{\mathcal{R}}^\top\right] + \text{Tr}\left[\delta\boldsymbol{\mathcal{R}} \mathbf{B G}^\top \mathbf{G B}^\top \delta\boldsymbol{\mathcal{R}}^\top \right] \\
			%&\quad\quad - \text{Tr}\left[  \delta\boldsymbol{\mathcal{R}} \mathbf{B G}^\top \mathbf{G B}^\top \delta\boldsymbol{\mathcal{R}}^\top \right] \\
			&\quad = \text{Tr}\left[ \boldsymbol{\mathcal{R}}_{\text{old}} \mathbf{B G}^\top \mathbf{G B}^\top \boldsymbol{\mathcal{R}}_{\text{old}}^\top\right] + 2\text{Tr}\left[\boldsymbol{\mathcal{R}}_{\delta} \mathbf{B G}^\top \mathbf{G B}^\top \boldsymbol{\mathcal{R}}_{\text{old}}^\top\right] \\ 
			&\quad = \text{Tr}\left[(\boldsymbol{\mathcal{R}}_{\text{old}}^\top \boldsymbol{\mathcal{R}}_{\text{old}} + 2\boldsymbol{\mathcal{R}}_{\text{old}}^\top \boldsymbol{\mathcal{R}}_{\delta}) ( \mathbf{B G}^\top \mathbf{G B}^\top) \right] \\ 
			&\quad > 0
		\end{split}
	\end{equation}
\end{small}

\noindent where the last inequality holds due to the positive dfiniteness of \( \boldsymbol{\mathcal{R}}_{\text{old}}^\top \boldsymbol{\mathcal{R}}_{\delta} \) and \( \mathbf{B G}^\top \neq 0 \).\\


\noindent Similarly, the partial derivatives of \( \boldsymbol{y} \) and \(\boldsymbol{y'}\) with respect to \( \mathbf{B} \) are given by:

\begin{equation}
	\label{e5}
	\begin{gathered}
		\frac{\partial \boldsymbol{y}}{\partial \mathbf{B}} = \boldsymbol{\mathcal{R}}^\top \mathbf{A^\top G} = (\boldsymbol{\mathcal{R}}_{\text{old}} + \boldsymbol{\mathcal{R}}_{\delta})^\top \mathbf{A^\top G},\\
		\frac{\partial \boldsymbol{y'}}{\partial \mathbf{B}} = \boldsymbol{\mathcal{R}}_{\delta}^\top \mathbf{A^\top G}.
	\end{gathered}
\end{equation}
\noindent Calculating the squared Frobenius norm difference, we have:
\begin{small}
	\begin{equation}
		\begin{split}
			&\left\| \frac{\partial \boldsymbol{y}}{\partial \mathbf{B}} \right\|^2_F - \left\| \frac{\partial \boldsymbol{y'}}{\partial \mathbf{B}} \right\|^2_F
			= \mathbf{ \left\|(\boldsymbol{\mathcal{R}}_{\text{old}}+\boldsymbol{\mathcal{R}}_{\delta})^\top A^\top G \right\| - \left\|\boldsymbol{\mathcal{R}}_{\delta}^\top A^\top G \right\|} \\
			&\quad = \mathbf{ \text{Tr}\left[ ((\boldsymbol{\mathcal{R}}_{\text{old}} + \boldsymbol{\mathcal{R}}_{\delta})^\top A^\top G)^\top ((\boldsymbol{\mathcal{R}}_{\text{old}} + \boldsymbol{\mathcal{R}}_{\delta})^\top A^\top G) \right] }\\
			&\quad\quad - \mathbf{ \text{Tr}\left[ (\boldsymbol{\mathcal{R}}_{\delta}^\top A^\top G)^\top (\boldsymbol{\mathcal{R}}_{\delta}^\top A^\top G) \right]} \\
			&\quad = \mathbf{ \text{Tr}\left[ G^\top A(\boldsymbol{\mathcal{R}}_{\text{old}} + \boldsymbol{\mathcal{R}}_{\delta})(\boldsymbol{\mathcal{R}}_{\text{old}} + \boldsymbol{\mathcal{R}}_{\delta})^\top A^\top G \right]} \\
			&\quad\quad - \mathbf{ \text{Tr}\left[ G^\top A\boldsymbol{\mathcal{R}}_{\delta}\boldsymbol{\mathcal{R}}_{\delta}^\top A^\top G \right] }\\
			%&\quad = \text{Tr}\left[  G^\top A \boldsymbol{\mathcal{R}}_{\text{old}}\boldsymbol{\mathcal{R}}_{\text{old}}^\top A^\top G\right] + \text{Tr}\left[G^\top A\delta\boldsymbol{\mathcal{R}} \boldsymbol{\mathcal{R}}_{\text{old}}^\top A^\top G\right] \\
			%&\quad\quad + \text{Tr}\left[G^\top A \boldsymbol{\mathcal{R}}_{\text{old}} \delta\boldsymbol{\mathcal{R}}^\top A^\top G\right] + \text{Tr}\left[G^\top A\delta\boldsymbol{\mathcal{R}} \delta\boldsymbol{\mathcal{R}}^\top A^\top G\right] \\
			%&\quad\quad - \text{Tr}\left[G^\top A \delta\boldsymbol{\mathcal{R}} \delta\boldsymbol{\mathcal{R}}^\top A^\top G\right] \\
			&\quad = \mathbf{ \text{Tr}\left[G^\top A \boldsymbol{\mathcal{R}}_{\text{old}}\boldsymbol{\mathcal{R}}_{\text{old}}^\top A^\top G\right] + 2\text{Tr}\left[G^\top A\boldsymbol{\mathcal{R}}_{\delta} \boldsymbol{\mathcal{R}}_{\text{old}}^\top A^\top G\right]} \\ 
			&\quad = \mathbf{ \text{Tr}\left[(\boldsymbol{\mathcal{R}}_{\text{old}} \boldsymbol{\mathcal{R}}_{\text{old}}^\top + 2\boldsymbol{\mathcal{R}}_{\delta} \boldsymbol{\mathcal{R}}_{\text{old}}^\top ) ( A^\top G G^\top A) \right]} \\ 
			&\quad > 0
		\end{split}
	\end{equation}
\end{small}
\noindent where the last inequality holds due to the positive dfiniteness of \( \boldsymbol{\mathcal{R}}_{\text{old}}^\top \boldsymbol{\mathcal{R}}_{\delta} \) and \( \mathbf{G^\top A} \neq 0 \).\hfill $\square$
\\
% ----- proof of theorem 3.1 ----- % start

This theorem demonstrates that by decomposing \( 	\mathbf{\boldsymbol{\mathcal{R}}} \) into \( 	\mathbf{\boldsymbol{\mathcal{R}}_{\text{old}} + \boldsymbol{\mathcal{R}}_{\delta }} \) and isolating the gradient updates of the \( 	\mathbf{\boldsymbol{\mathcal{R}}_{\text{old}}} \) component during training, the interference of new tasks with the critical parameters of old tasks is reduced. This supports the feasibility of our proposed method.

\subsection{Implementation Details: C-LoRA in ViT Application
	% C-LoRA: Adapter-tuning and Loss Function
}

Building on our C-LoRA approach, this section details its application to Vision Transformers (ViT) \cite{alexey2020image} for efficient continual learning. ViTs have emerged as a powerful architecture for image recognition tasks due to their ability to model long-range dependencies using self-attention mechanisms. Each layer of a ViT consists of two main components: the multi-head self-attention (MSA) block and the multi-layer perceptron (MLP) block. While the MSA block captures contextual relationships among input tokens, the MLP block processes feature embeddings and contributes significantly to the model’s parameter complexity. By integrating C-LoRA into the MLP block, we enable parameter-efficient updates that adapt to new tasks while preserving previously learned knowledge.

% ----- figure 2 ----- % start
\begin{figure*}[!t]
	\centering
	% 第一行
	\begin{minipage}{\textwidth}
		\centering
		%\hspace{-0.1\textwidth} % 向左移动图片，调整空白
		\includegraphics[width=\textwidth, trim=0cm 4cm 0cm 3cm, clip]{draw2.pdf}
	\end{minipage}%
	\caption{ Illustration of the Proposed Model. \textbf{Left:} Vision Transformer (ViT) integrated with the C-LoRA module, where the adapter and local classifier are incrementally trained in each session . \textbf{Right:} Proposed architecture mitigates catastrophic forgetting by decoupling \(\boldsymbol{\mathcal{R}}\) to constrain updates within the parameter space of previous tasks and enforcing orthogonality between \(\boldsymbol{\mathcal{R}}\) updates and the low-rank parameter space of past tasks.}
	\label{fig:2} 
\end{figure*}
% ----- figure 2 ----- % end

\textbf{Low-Rank Adaptation in MLP:  }
In a standard MLP block, the input features $\mathbf{x}_i \in \mathbb{R}^d$ are transformed as:
\begin{equation}
	\text{MLP}(\mathbf{x}_i) = \sigma(\mathbf{x}_i \mathbf{W}_1 + \mathbf{b}_1) \mathbf{W}_2 + \mathbf{b}_2,
\end{equation}
where $\mathbf{W}_1 \in \mathbb{R}^{d \times h}$ and $\mathbf{W}_2 \in \mathbb{R}^{h \times d}$ are the weight matrices, $\mathbf{b}_1$ and $\mathbf{b}_2$ are biases, and $\sigma(\cdot)$ is a non-linear activation function, such as ReLU. Directly updating these matrices for new tasks leads to significant parameter overhead and risks overwriting previously learned knowledge.

C-LoRA addresses these challenges by replacing full-rank updates with a low-rank adaptation mechanism. The modified MLP block operates by first projecting the input $\mathbf{x}_i$ into a low-rank subspace using a down-sampling matrix $\mathbf{A} \in \mathbb{R}^{d \times r}$, where $r \ll d$. The intermediate representation is computed as:
\begin{equation}
	\boldsymbol{z}_i = \sigma(\mathbf{x}_i \mathbf{A}),
\end{equation}
where $\mathbf{A}$ reduces the dimensionality of the input while preserving its essential features.

\textbf{Routing Matrix and Task-Specific Updates:  }
To facilitate continual learning, a routing matrix $\boldsymbol{\mathcal{R}} \in \mathbb{{R}}^{r \times r}$ is introduced. This matrix is decomposed into two components:
\begin{equation}
	\boldsymbol{\mathcal{R}} = \boldsymbol{\mathcal{R}}_{\text{old}} + \boldsymbol{\mathcal{R}}_{\delta },
\end{equation}
where $\boldsymbol{\mathcal{R}}_{\text{old}}$ encodes knowledge from previously learned tasks and remains frozen during new task training, while $\boldsymbol{\mathcal{R}}_{\delta }$ captures task-specific updates and is trainable for the current task.

The representation $z_i$ is modulated by both $\boldsymbol{\mathcal{R}}_{\text{old}}$ and $\boldsymbol{\mathcal{R}}_{\delta }$ before being up-sampled back to the original dimensionality using $\mathbf{B} \in \mathbb{{R}}^{r \times d}$. The combined output is expressed as:
\begin{equation}
	\label{eq:9}
	\text{C-LoRA}(\boldsymbol{x}_i)
	\;=\;
	\underbrace{\phi\bigl(\boldsymbol{z}_i\,\boldsymbol{\mathcal{R}}_{\text{old}}\,\mathbf{B}\bigr)}_{\text{stop-gradient for old}}
	\;+\;
	\underbrace{\bigl(\boldsymbol{z}_i\,\boldsymbol{\mathcal{R}}_{\delta}\,\mathbf{B}\bigr)}_{\text{update}}.
\end{equation}
where $\phi(\cdot)$ is a stop-gradient operator that prevents $\mathbf{R}_{\text{old}}$ from being updated during training. This mechanism ensures that shared knowledge is preserved while allowing flexibility for task-specific updates.

\textbf{Integration into ViT:  }
As shown in Figure~\ref{fig:2}, C-LoRA modifies the MLP block in ViT by combining its residual connection, original MLP computation, and the C-LoRA updates. The final output is computed as:
\begin{equation}
	\boldsymbol{out} 
	\;=\;
	\boldsymbol{x}_i + \text{MLP}(\boldsymbol{x}_i)
	\;+\;
	\text{C-LoRA}(\boldsymbol{x}_i).
\end{equation}
This formulation maintains the original representational power of the MLP block while incorporating task-specific adaptability through C-LoRA. Over successive tasks, the accumulated updates in $\boldsymbol{\mathcal{R}}_{\delta }$ enable the model to adapt to new tasks without compromising performance on previously learned ones.

\textbf{Loss Function Design:  }
To mitigate catastrophic forgetting and promote stable learning across tasks, the total loss function combines a classification loss with an orthogonality regularization term:
\begin{equation}
	\mathcal{L} = \mathcal{L}_{\text{ce}} + \lambda \mathcal{L}_{\text{orth}},
\end{equation}
where $\lambda$ balances the contributions of the two terms.

The classification loss for incremental learning is defined as:
\begin{equation}
	\mathcal{L}_{\text{ce}} = -\frac{1}{N_t} \sum_{j=1}^{N_t} \log \frac{\exp(z_{j,c^*})}{\sum_{\ell \in \mathcal{Y}(t)} \exp(z_{j,\ell})},
\end{equation}
where \(N_t\) is the total number of samples in the current task \(t\)'s dataset, and \(\mathcal{Y}(t)\) represents the set of class labels for the current task. The term 
\begin{equation}
	z_{j,\ell} =  \frac{s\small(\mathbf{w}_\ell^\top \mathbf{f}_j\small) }{\|\mathbf{w}_\ell\| \|\mathbf{f}_j\|}
\end{equation}
represents the scaled cosine similarity between the feature vector \(\mathbf{f}_j\) of sample \(j\) and the weight vector \(\mathbf{w}_\ell\) of class \(\ell\), with \(s\) being a scaling factor to stabilize optimization.
Here, \(\mathbf{f}_j \in \mathbb{R}^d\) denotes the feature vector of the \(j\)-th sample, and \(\mathbf{w}_\ell \in \mathbb{R}^d\) denotes the weight vector of class \(\ell\). Both vectors are normalized to ensure the similarity is computed based on their angular relationship. The numerator, \(\exp(z_{j,c^*})\), quantifies the alignment between the feature vector and the weight vector of the correct class \(c^*\), while the denominator aggregates the scaled similarities over all classes. This normalization ensures the logits are transformed into a probability distribution, allowing the model to focus on maximizing the likelihood of the correct class.



The orthogonality regularization ensures that task-specific updates in $\boldsymbol{\mathcal{R}}_{\delta } $ do not interfere with the old parameter subspace by measuring $\boldsymbol{\mathcal{R}}_{\delta }$'s projection onto the low-rank subspace $\mathbf{A}'$ of the old model:
% The orthogonality regularization ensures that task-specific updates in $\boldsymbol{\mathcal{R}}_{\delta }$ do not interfere with the subspace of $\boldsymbol{\mathcal{R}}_{\text{old}}$:
\begin{equation}
	\mathcal{L}_{\text{orth}} = \| (\mathbf{A}')^\top \boldsymbol{\mathcal{R}}_{\delta } \|_F^2,
\end{equation}
where $\mathbf{A}'$ epresents the current $\mathbf{A}$ during the training process when a new task sample arrives, and $\|\cdot\|_F^2$ denotes the Frobenius norm.

% \textbf{Feature Resampling for Incremental Learning:  } 
In class-incremental learning scenarios, access to data from previous tasks is typically restricted. To address this, we employ \textit{a feature resampling strategy} that generates synthetic features based on the statistical properties of each class. Specifically, we use the mean and covariance of features from each class to synthesize representative samples. This approach ensures that the model retains its discriminative power on old tasks without requiring direct access to their data.

\begin{table*}[ht]
	\centering
	\caption{Accuracy of different continual learning methods across four benchmark datasets, each divided into 5 incremental sessions. This setting reflects scenarios with fewer new classes introduced per session, offering insights into performance stability.}
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{lcccccccc}
			\toprule
			\textbf{Method} & \multicolumn{2}{c}{\textbf{Split CIFAR-100}} & \multicolumn{2}{c}{\textbf{Split ImageNet-A}} & \multicolumn{2}{c}{\textbf{Split CUB-200}} & \multicolumn{2}{c}{\textbf{Split CAR196}} \\
			\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
			& Last-Acc (\%) & Inc-Acc (\%) & Last-Acc (\%) & Inc-Acc (\%) & Last-Acc (\%) & Inc-Acc (\%) & Last-Acc (\%) & Inc-Acc (\%) \\
			\midrule
			Joint-Training & 93.22 & - & 79.61 & - & 90.78 & - & 68.66 & - \\
			EWC \cite{kirkpatrick2017overcoming} & 91.21 $\pm$ 0.53	& 93.93 $\pm$ 0.67	&	60.13 $\pm$ 2.28	& 68.74 $\pm$ 0.87	& 84.73 $\pm$ 0.74 &	91.07 $\pm$ 0.38 &	51.64 $\pm$ 1.36 &	61.47 $\pm$ 1.10  \\
			L2P  \cite{wang2022learning} & 84.60 $\pm$ 1.74	& 89.24 $\pm$ 1.27	& 46.94 $\pm$ 0.71	& 53.28 $\pm$ 1.13	& 73.07 $\pm$ 1.74	& 81.99 $\pm$ 1.34	& 56.03 $\pm$ 2.86	& 66.43 $\pm$ 2.40 \\
			DualPrompt \cite{wang2022dualprompt} & 82.66 $\pm$ 2.00	& 87.23 $\pm$ 0.88	& 50.54 $\pm$ 0.98	& 58.18 $\pm$ 1.30	& 74.28 $\pm$ 0.26	& 83.60 $\pm$ 0.44	& 48.90 $\pm$ 0.76	& 61.83 $\pm$ 0.44 \\
			LAE \cite{gao2023unified} & 82.45 $\pm$ 0.49	& 87.11 $\pm$ 1.11	& 47.99 $\pm$ 0.91	& 56.29 $\pm$ 2.32	& 75.40 $\pm$ 0.99	& 84.77 $\pm$ 0.56	& 50.97 $\pm$ 0.96	& 62.46 $\pm$ 0.90 \\
			SLCA \cite{zhang2023slca} & \textbf{92.19} $\pm$\textbf{ 0.56}	& \textbf{94.56} $\pm$\textbf{ 0.58}	& 63.14 $\pm$ 1.39	& 68.65 $\pm$ 0.72	& 86.54 $\pm$ 1.27	& 91.51 $\pm$ 0.96	& 75.73 $\pm$ 1.27	& 82.01 $\pm$ 0.72 \\
			%Adam+finetune (IJCV 2024) & 63.85 $\pm$ 1.47	& 75.07 $\pm$ 1.50	& 48.54 $\pm$ 0.79	& 56.81 $\pm$ 2.19	& 85.19 $\pm$ 0.44	& 90.05 $\pm$ 0.31	& 40.50 $\pm$ 0.25	& 49.52 $\pm$ 0.86 \\
			Adam+VPT-S \cite{zhou2023revisiting} &84.10 $\pm$ 0.88	& 87.89 $\pm$ 1.49	& 24.49 $\pm$ 2.35	& 31.16 $\pm$ 1.73	& 86.64 $\pm$ 0.17	& 91.25 $\pm$ 0.14	& 30.86 $\pm$ 11.39	& 39.34 $\pm$ 13.77 \\
			Adam+VPT-D \cite{zhou2023revisiting} &85.89 $\pm$ 4.11	& 89.41 $\pm$ 3.03	& 48.76 $\pm$ 4.90	& 56.59 $\pm$ 5.78	& 86.67 $\pm$ 0.13	& 91.23 $\pm$ 0.10	& 23.52 $\pm$ 13.70	& 30.80 $\pm$ 17.23 \\
			Adam+adapter \cite{zhou2023revisiting} & 88.63 $\pm$ 0.13	& 91.98 $\pm$ 0.85	& 49.79 $\pm$ 0.67	& 58.38 $\pm$ 1.33	& 87.25 $\pm$ 0.03	& 91.66 $\pm$ 0.23	& 40.75 $\pm$ 0.29	& 50.07 $\pm$ 1.81 \\
			Adam+ssf \cite{zhou2023revisiting} & 86.98 $\pm$ 0.26	& 90.91 $\pm$ 0.98	& 52.36 $\pm$ 1.53	& 61.08 $\pm$ 2.60	& 86.73 $\pm$ 0.05	& 91.18 $\pm$ 0.19	& 46.07 $\pm$ 0.66	& 55.58 $\pm$ 1.90 \\
			EASE \cite{zhou2024expandable} & 89.33 $\pm$ 0.16	& 92.53 $\pm$ 0.92	& 62.92 $\pm$ 0.83	& 70.19 $\pm$ 1.18	& 83.93 $\pm$ 1.48	& 89.55 $\pm$ 0.84	& 54.98 $\pm$ 0.80	& 67.09 $\pm$ 1.08 \\
			\midrule
			\textbf{Ours} & {92.17} $\pm${0.15}	& {94.48} $\pm$ {0.79}	& \textbf{63.84} $\pm$ \textbf{1.24}	& \textbf{71.50} $\pm$ \textbf{1.34}	&\textbf{ 90.55} $\pm$ \textbf{0.28}	& \textbf{93.60} $\pm$ \textbf{0.30}	& \textbf{78.70} $\pm$ \textbf{0.75}	& \textbf{84.48} $\pm$\textbf{ 0.94}\\
			\bottomrule
		\end{tabular}%
	}
	\label{tab:1}
	
\end{table*}


\begin{figure*}[!t]
	\centering
	\begin{minipage}{\textwidth} % CIFAR
		\centering
		%\hspace{-0.1\textwidth} % 向左移动图片，调整空白
		\includegraphics[,width=\textwidth, trim=0.5cm 10.5cm 1cm 1.8cm, clip]{draw3_5.pdf}
	\end{minipage}%
	\caption{ Accuracy performance in each of the 5 incremental sessions on CIFAR-100, ImageNet-A, CUB-200, and CAR196.}
	\label{fig:3} 
\end{figure*}
















% ### 表格1 ### % start
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table*}[ht]
	\centering
	\caption{Accuracy of different continual learning methods across four benchmark datasets, each divided into 10 incremental sessions. }
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{lcccccccc}
			\toprule
			\textbf{Method} & \multicolumn{2}{c}{\textbf{Split CIFAR-100}} & \multicolumn{2}{c}{\textbf{Split ImageNet-A}} & \multicolumn{2}{c}{\textbf{Split CUB-200}} & \multicolumn{2}{c}{\textbf{Split CAR196}} \\
			\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
			& Last-Acc (\%) & Inc-Acc (\%) & Last-Acc (\%) & Inc-Acc (\%) & Last-Acc (\%) & Inc-Acc (\%) & Last-Acc (\%) & Inc-Acc (\%) \\
			\midrule
			Joint-Training & 93.22 & - & 79.61 & - & 90.78 & - & 68.66 & - \\
			EWC \cite{kirkpatrick2017overcoming} & 90.59 $\pm$ 0.63	& 93.80 $\pm$ 0.97	& 56.24 $\pm$ 1.56	& 66.30 $\pm$ 1.71	& 85.10 $\pm$ 0.31	& 92.36 $\pm$ 0.51	& 45.58 $\pm$ 0.19	& 60.41 $\pm$ 0.94  \\
			L2P \cite{wang2022learning} & 84.17 $\pm$ 0.91	& 88.20 $\pm$ 1.30	& 43.47 $\pm$ 0.89	& 50.52 $\pm$ 2.29	& 67.13 $\pm$ 2.03	& 79.62 $\pm$ 1.56	& 45.81 $\pm$ 0.98	& 57.85 $\pm$ 1.92  \\
			DualPrompt \cite{wang2022dualprompt} & 81.70 $\pm$ 1.07	& 86.42 $\pm$ 0.98	& 46.41 $\pm$ 0.20	& 55.95 $\pm$ 1.34	& 68.68 $\pm$ 0.35	& 80.75 $\pm$ 0.87	& 39.36 $\pm$ 2.32	& 53.93 $\pm$ 2.40 \\
			LAE \cite{gao2023unified} & 87.53 $\pm$ 0.38	& 91.67 $\pm$ 1.04	& 60.74 $\pm$ 0.93	& 69.43 $\pm$ 1.31	& 81.55 $\pm$ 2.38	& 88.63 $\pm$ 1.84	& 44.07 $\pm$ 0.95	& 57.88 $\pm$ 1.12 \\
			SLCA \cite{zhang2023slca} & 91.07 $\pm$ 0.45	& 94.06 $\pm$ 1.07	& 58.96 $\pm$ 0.88	& 66.39 $\pm$ 2.16	& 84.55 $\pm$ 0.19	& 90.69 $\pm$ 0.65	& 66.49 $\pm$ 2.87	& 76.43 $\pm$ 0.61  \\
			%Adam+finetune (IJCV 2024) & 57.62 $\pm$ 2.57	& 68.03 $\pm$ 2.18	& 47.80 $\pm$ 1.20	& 58.03 $\pm$ 2.32	& 86.26 $\pm$ 0.30	& 91.62 $\pm$ 0.47	& 41.41 $\pm$ 0.68	& 52.41 $\pm$ 1.40 \\
			Adam+VPT-S \cite{zhou2023revisiting} &68.67 $\pm$ 12.40	& 75.22 $\pm$ 9.72	& 39.08 $\pm$ 9.51	& 49.08 $\pm$ 9.11	& 80.03 $\pm$ 11.42	& 86.60 $\pm$ 9.14	& 31.63 $\pm$ 10.15	& 41.78 $\pm$ 10.92 \\
			Adam+VPT-D \cite{zhou2023revisiting} &84.61 $\pm$ 3.01	& 88.83 $\pm$ 2.70	& 37.00 $\pm$ 13.81	& 46.29 $\pm$ 14.94	& 86.73 $\pm$ 0.33	& 91.91 $\pm$ 0.15	& 38.04 $\pm$ 12.80	& 48.00 $\pm$ 14.05 \\
			Adam+adapter \cite{zhou2023revisiting} & 87.32 $\pm$ 0.28	& 91.21 $\pm$ 1.35	& 48.94 $\pm$ 0.10	& 58.95 $\pm$ 1.38	& 87.04 $\pm$ 0.13	& 92.13 $\pm$ 0.27	& 37.65 $\pm$ 0.09	& 49.22 $\pm$ 1.66 \\
			Adam+ssf \cite{zhou2023revisiting} & 85.26 $\pm$ 0.40	& 89.92 $\pm$ 0.84	& 49.60 $\pm$ 0.80	& 58.90 $\pm$ 2.22	& 86.32 $\pm$ 0.43	& 91.80 $\pm$ 0.55	& 42.83 $\pm$ 0.72	& 53.97 $\pm$ 1.94 \\
			EASE  \cite{zhou2024expandable} &87.53 $\pm$ 0.38	& 91.67 $\pm$ 1.04	& 60.74 $\pm$ 0.93	& 69.43 $\pm$ 1.31	& 81.55 $\pm$ 2.38	& 88.63 $\pm$ 1.84	& 44.07 $\pm$ 0.95	& 57.88 $\pm$ 1.12 \\
			\midrule
			\textbf{Ours} & \textbf{91.67 $\pm$ 0.05}	& \textbf{94.29 $\pm$ 0.80}	& \textbf{63.51 $\pm$ 0.78}	& \textbf{71.88 $\pm$ 1.34}	& \textbf{90.16 $\pm$ 0.15}	& \textbf{93.87 $\pm$ 0.53}	& \textbf{72.07 $\pm$ 1.12}	&\textbf{ 80.42 $\pm$ 0.81}\\
			\bottomrule
		\end{tabular}%
	}
	\label{tab:2}
\end{table*}

\begin{figure*}[!t]
	\centering
	\begin{minipage}{\textwidth} % CIFAR
		\centering
		%\hspace{-0.1\textwidth} % 向左移动图片，调整空白
		\includegraphics[,width=\textwidth, trim=0.5cm 10.5cm 1cm 1.8cm, clip]{draw3_10.pdf}
	\end{minipage}%
	\caption{ Accuracy performance in each of the 10 incremental sessions on CIFAR-100, ImageNet-A, CUB-200, and CAR196.}
	\label{fig:4} 
\end{figure*}

% ----- ----- ----- experiment ----- ----- ----- %
\section{Experiments}

\subsection{Experimental Setups}

\textbf{Datasets.} Following previous studies \cite{mcdonnell2024ranpac,zhang2023slca,tan2024semantically}, we evaluated our method on four benchmark datasets. 
CIFAR-100 \cite{krizhevsky2009learning} consists of 100 classes with 60,000 images, approximately 50,000 for training (83\%) and 10,000 for testing (17\%). 
ImageNet-A \cite{hendrycks2021natural} includes 200 classes and roughly 7,500 test samples. 
CUB-200 \cite{wah2011caltech} contains 11,788 bird images across 200 subclasses, with about 6,000 for training (51\%) and 5,800 for testing (49\%). 
Car196 \cite{krause20133d} comprises 196 car models with 16,185 images, split into around 8,150 for training (50\%) and 8,035 for testing (50\%).


\noindent\textbf{Implementation Details.} Following prior works \cite{mcdonnell2024ranpac,zhang2023slca}, we employ the pre-trained VIT-B/16-IN21K as the backbone network. During each training session, we use the SGD optimizer with a learning rate of 0.01, coupled with a cosine annealing scheduler starting at an initial value of 0.0005. The batch size is fixed at 48. The routing matrix $\boldsymbol{\mathcal{R}}_{\delta}$ is initialized near zero at the start of each session, and we set the regularization parameter $\lambda = 0.01$ accordingly. 

To evaluate performance, we calculate two metrics: ``Last-Acc" and ``Inc-Acc." Last-Acc measures the average accuracy across all classes after the completion of the final task in the incremental learning process. Inc-Acc assesses the model's performance throughout the entire incremental learning process by calculating the average accuracy across all tasks at the conclusion of each incremental learning phase. Specifically, for each task added during incremental learning, we compute the accuracy on all learned classes and take the average across all phases to obtain Inc-Acc. This metric provides a holistic view of the model's ability to retain knowledge while learning new tasks, reflecting its robustness to catastrophic forgetting.
To ensure robust comparisons, all experiments are conducted using three fixed random seeds, and we report the mean and standard deviation of the results.



%#################################



\subsection{Evaluation Results Across Incremental Sessions}
%We evaluated the proposed method on four benchmark datasets under three incremental learning scenarios (5, 10, and 20 sessions). Table \ref{tab:2} summarizes the results for 10 incremental sessions, while additional results for 5 and 20 sessions are provided in  Table \ref{tab:1} and Table \ref{tab:3} from the Appendix.

We evaluated the proposed method on four benchmark datasets under three incremental learning scenarios (5, 10, and 20 sessions). Table \ref{tab:1}, \ref{tab:2}, and \ref{tab:3} respectively present the performance for 5, 10, and 20 incremental sessions. Figures~\ref{fig:3}, \ref{fig:4}, and \ref{fig:5} respectively illustrate the performance of our method on four datasets under 5, 10, and 20 incremental session settings. Each figure corresponds to a specific number of incremental sessions, with the datasets ordered from top to bottom as CIFAR-100, ImageNet-A, CUB-200, and CAR196. Our approach demonstrates strong adaptability and stability in handling diverse domain data and varying incremental settings, effectively mitigating catastrophic forgetting.

Our method consistently outperforms baseline approaches across datasets and session settings. For datasets with pre-training overlap, such as CIFAR-100 and CUB-200, our method achieves competitive Last-Acc and Inc-Acc scores with fewer parameter updates. For instance, on CIFAR-100, it achieves 91.67\% Last-Acc and 94.29\% Inc-Acc, matching or exceeding SLCA while requiring significantly less computation.
On datasets with significant domain gaps, such as ImageNet-A and CAR196, our method demonstrates superior robustness. For example, on ImageNet-A, our approach achieves a Last-Acc of 63.51\%, outperforming the second-best method by 4.14\%. Similarly, it achieves a Last-Acc of 72.07\% on CAR196, a 3.80\% improvement over alternatives.
Our method also exhibits remarkable stability across different incremental scenarios. While catastrophic forgetting increasingly impacts other methods in longer sequences, our approach maintains consistent Last-Acc and Inc-Acc scores. 








\subsection{Ablation Study}
To validate the effectiveness of each component in our proposed method, we conducted ablation experiments on the CUB-200 dataset, as shown in Table \ref{tab:4}. The basic LoRA configuration exhibited a significant performance decline in 10 incremental tasks, achieving an Last-Acc of 81.21\%, indicating that the module is susceptible to catastrophic forgetting without additional mechanisms.

We introduced the $\boldsymbol{\mathcal{R}}$ matrix to the basic LoRA configuration, denoted as LoRA+R, which resulted in improved Last-Acc of 82.49\%, enabling efficient reuse of parameter subspaces. Next, we decomposed the $\boldsymbol{\mathcal{R}}$ matrix into $\boldsymbol{\mathcal{R}}_{\text{old}}$ and $\boldsymbol{\mathcal{R}}_{\delta}$, referred to as LoRA+R+TD. This enhanced the module’s retention and transfer capabilities across sessions, achieving Last-Acc of 90.09\%. This modification facilitated the separation and independent adjustment of feature representations for old and new tasks.

Finally, we incorporated orthogonality constraints into the LoRA+R+TD configuration, resulting in C-LoRA, which further increased the Last-Acc to 90.30\%. These results highlight the importance of orthogonality in mitigating catastrophic forgetting and maintaining feature space stability.


% ### 表格1 ### % start
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table*}[ht]
	\centering
	\caption{Accuracy of different continual learning methods across four benchmark datasets, each divided into 20 incremental sessions. This setting emphasizes the challenge of longer task sequences, showcasing the resilience of our approach under extended incremental learning.}
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{lcccccccc}
			\toprule
			\textbf{Method} & \multicolumn{2}{c}{\textbf{Split CIFAR-100}} & \multicolumn{2}{c}{\textbf{Split ImageNet-A}} & \multicolumn{2}{c}{\textbf{Split CUB-200}} & \multicolumn{2}{c}{\textbf{Split CAR196}} \\
			\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
			& Last-Acc (\%) & Inc-Acc (\%) & Last-Acc (\%) & Inc-Acc (\%) & Last-Acc (\%) & Inc-Acc (\%) & Last-Acc (\%) & Inc-Acc (\%) \\
			\midrule
			Joint-Training & 93.22 & - & 79.61 & - & 90.78 & - & 68.66 & - \\
			EWC 2017)\cite{kirkpatrick2017overcoming} & 89.96 $\pm$ 0.15	& 93.31 $\pm$ 1.02	& 54.62 $\pm$ 1.39	& 63.55 $\pm$ 1.53	& 87.25 $\pm$ 1.00	& 93.45 $\pm$ 0.48	& 44.42 $\pm$ 0.56	& 61.75 $\pm$ 1.06  \\
			L2P \cite{wang2022learning} &79.32 $\pm$ 1.49	& 84.10 $\pm$ 1.11	& 40.07 $\pm$ 1.42	& 49.35 $\pm$ 1.37	& 60.30 $\pm$ 3.80	& 73.53 $\pm$ 3.34	& 29.83 $\pm$ 3.31	& 42.82 $\pm$ 1.14  \\
			DualPrompt \cite{wang2022dualprompt} & 76.57 $\pm$ 0.60	& 82.97 $\pm$ 1.78	& 41.48 $\pm$ 1.63	& 52.84 $\pm$ 1.57	& 65.22 $\pm$ 1.29	& 77.86 $\pm$ 1.31	& 25.11 $\pm$ 2.09	& 40.89 $\pm$ 0.77
			\\
			LAE \cite{gao2023unified} & 75.40 $\pm$ 1.01	& 80.66 $\pm$ 0.70	& 32.28 $\pm$ 1.80	& 42.24 $\pm$ 2.53	& 56.73 $\pm$ 1.89	& 72.67 $\pm$ 1.14	& 19.62 $\pm$ 1.82	& 35.66 $\pm$ 3.32
			\\
			SLCA \cite{zhang2023slca} & 90.37 $\pm$ 0.56	&\textbf{ 93.64 $\pm$ 0.99}	& 53.76 $\pm$ 5.54	& 61.82 $\pm$ 4.74	& 82.37 $\pm$ 0.42	& 90.12 $\pm$ 0.93	& 56.09 $\pm$ 3.89	& 68.50 $\pm$ 2.42  \\
			%Adam+finetune (IJCV 2024) & 60.95 $\pm$ 2.81	& 67.66 $\pm$ 2.65	& 47.82 $\pm$ 0.64	& 58.63 $\pm$ 1.52	& 86.43 $\pm$ 0.13	& 91.91 $\pm$ 0.45	& 42.04 $\pm$ 0.60	& 54.61 $\pm$ 0.66\\
			Adam+VPT-S \cite{zhou2023revisiting} &81.41 $\pm$ 1.00	& 86.48 $\pm$ 1.46	& 40.66 $\pm$ 5.37	& 51.32 $\pm$ 4.68	& 82.42 $\pm$ 5.75	& 89.46 $\pm$ 3.65	& 29.72 $\pm$ 8.31	& 41.08 $\pm$ 8.90
			\\
			Adam+VPT-D \cite{zhou2023revisiting} &73.60 $\pm$ 15.86	& 79.59 $\pm$ 13.27	& 37.07 $\pm$ 15.26	& 48.41 $\pm$ 15.17	& 86.67 $\pm$ 0.56	& 92.09 $\pm$ 0.87	& 36.76 $\pm$ 6.24	& 49.34 $\pm$ 7.03
			\\
			Adam+adapter \cite{zhou2023revisiting} & 84.86 $\pm$ 0.29	& 89.41 $\pm$ 1.32	& 48.72 $\pm$ 0.07	& 59.53 $\pm$ 1.02	& 86.80 $\pm$ 0.02	& 92.42 $\pm$ 0.31	& 37.69 $\pm$ 0.02	& 50.56 $\pm$ 1.40
			\\
			Adam+ssf \cite{zhou2023revisiting} & 82.96 $\pm$ 1.21	& 87.90 $\pm$ 0.05	& 47.86 $\pm$ 4.13	& 58.58 $\pm$ 4.27	& 86.37 $\pm$ 0.20	& 92.21 $\pm$ 0.63	& 41.98 $\pm$ 0.42	& 54.35 $\pm$ 1.29
			\\
			EASE \cite{zhou2024expandable} &84.88 $\pm$ 1.09	& 90.08 $\pm$ 1.01	& 55.50 $\pm$ 1.16	& 65.73 $\pm$ 1.77	& 78.26 $\pm$ 1.02	& 87.14 $\pm$ 1.19	& 31.83 $\pm$ 1.38	& 47.90 $\pm$ 1.89
			\\
			\midrule
			\textbf{Ours} &\textbf{ 90.69 $\pm$ 0.25}	& 93.57 $\pm$ 0.76	& \textbf{62.17 $\pm$ 0.86}	&\textbf{ 70.00 $\pm$ 1.62}	& \textbf{90.18 $\pm$ 0.11}	& \textbf{94.11 $\pm$ 0.48}	& \textbf{66.27 $\pm$ 0.68}	& \textbf{75.85 $\pm$ 0.99}
			\\
			\bottomrule
		\end{tabular}%
	}
	\label{tab:3}
	\centering
\end{table*}

\begin{figure*}[!t]
	\begin{minipage}{\textwidth} % CIFAR
		\centering
		%\hspace{-0.1\textwidth} % 向左移动图片，调整空白
		\includegraphics[,width=\textwidth, trim=0.5cm 10.5cm 1cm 1.8cm, clip]{draw3_20.pdf}
	\end{minipage}%
	\caption{ Accuracy performance in each of the 20 incremental sessions on CIFAR-100, ImageNet-A, CUB-200, and CAR196.}
	\label{fig:5} 
\end{figure*}


% ### 表格5 ### % start
% \usepackage{multirow}
\begin{table*}[!t]
	\centering
	\caption{Ablation results with different components on CUB-200.}
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{lccccccccccc}
			\toprule
			Method & Session1 & Session2 & Session3 & Session4 & Session5 & Session6 & Session7 & Session8 & Session9 & Session10 & Avg \\
			\midrule
			LoRA & 96.55 & 96.97 & 93.40 & 91.12 & 89.95 & 88.77 & 84.81 & 86.15 & 86.04 & 81.21 & 89.50 \\
			LoRA+R & 96.55 & 96.74 & 93.85 & 90.79 & 88.82 & 90.00 & 85.73 & 87.05 & 86.98 & 82.49 & 89.90 \\
			LoRA+R+TD & \textbf{ 97.04} & \textbf{97.44} & \textbf{96.40} & 93.50 & \textbf{92.46} & 92.90 & 92.34 & 92.38 & 91.51 & 90.09 & 93.61 \\
			C-LoRA & 96.55 & \textbf{97.44} & 95.95 & \textbf{93.60} & 92.37 & \textbf{92.97} & \textbf{92.77} & \textbf{92.75} & \textbf{91.89} & \textbf{90.30 }& \textbf{93.66} \\
			\hline
		\end{tabular}%
	}
	\label{tab:4}
\end{table*}
\begin{figure*}[!t]
	\centering
	% 第一行
	\begin{minipage}{\textwidth}
		\centering
		%\hspace{-0.1\textwidth} % 向左移动图片，调整空白
		\includegraphics[,width=\textwidth, trim=1.5cm 8.5cm 0cm 2.8cm, clip]{draw4.pdf}
	\end{minipage}%
	\caption{(Left) Accuracy performance at each incremental session; (Right) Accuracy performance across different class intervals after incremental training.}
	\label{fig:4} 
\end{figure*}

% ----- Figure 3 描述 ----- %

Figure \ref{fig:4} further report the effectiveness of the proposed components. The left shows accuracy changes over 10 incremental sessions. LoRA+R+TD outperforms both LoRA and LoRA+R, showing that our decomposition operation enhances continual learning. Although adding orthogonal loss results in a limited accuracy increase, it effectively reduces forgetting. The right  presents accuracy across different class intervals after incremental training. As more training tasks are added, performance on older classes declines compared to newer ones. In contrast, our method maintains high accuracy on older tasks, showing the approach's stability and performance improvements throughout the training process.


\section{Conclusion}

In summary, the proposed C-LoRA method effectively alleviates catastrophic forgetting and improves class-incremental learning performance across diverse datasets. Through multi-session scenarios and improved stability in our experiments, C-LoRA consistently delivers outstanding performance, surpassing existing approaches. Moreover, ablation studies underline the important role of the orthogonal low-rank adapter module, providing robust empirical support for our design in class-incremental learning.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\section{Detailed Results Across 5 and 20 Incremental Sessions}










%\section{Performance of Four Datasets Under Various Incremental Session Settings}
%
%Figure~\ref{fig:4} report the performance of our method on four datasets under 5, 10, and 20 incremental session settings. The sessions are arranged from left to right as 5, 10, and 20 incremental sessions, and the datasets are ordered from  top to bottom as CIFAR-100, ImageNet-A, CUB-200, and CAR196. Our approach exhibits strong adaptability and stability in handling diverse domain data and varying incremental settings, effectively mitigating catastrophic forgetting.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\bibliography{example_paper}
\bibliographystyle{IEEEtran}





















% --- % --- % --- % --- % --- % --- %




%\begin{IEEEbiographynophoto}{Jane Doe}
%Biography text here without a photo.
%\end{IEEEbiographynophoto}

%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig1.png}}]{IEEE Publications Technology Team}
%In this paragraph you can place your educational, professional background and research and other interests.\end{IEEEbiography}


\end{document}


