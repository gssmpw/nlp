\section{Unknown Word Detection Method}
\label{sec:method}

In this section, we explain why we chose gaze and PLM to detect unknown words. Then we describe how we collected and processed the data. Finally, we explain the architecture of our method in detail.


\subsection{Method Justification}
Even though whether a word is unknown to a user is subjective, linguistic priors related to word difficulty are helpful in unknown word prediction. Therefore, we assume a user's unknown word can be predicted based on (1) linguistic characteristics related to word difficulty, such as frequency, contextual distinctiveness, and COCA (Corpus of Contemporary American English) range, and (2) user-dependent factors which can be reflected by gaze trajectory. Therefore, we integrate text and gaze information to detect unknown words as shown in Fig.~\ref{fig:justification}:
\begin{enumerate}
    \item Linguistic characteristics of the target word's tokens and the textual context in the region of interest are captured by RoBERTa. RoBERTa is an effective and efficient language model for natural language understanding, ideally fitting our expectations, based on its proven performance in capturing complex linguistic features, such as contextual embeddings, syntactic structures, and semantic relationships. Prior research demonstrated that models like RoBERTa outperform traditional methods in tasks requiring a deep understanding of natural language~\cite{liu2019roberta}. Considering that the context around the word influences word difficulty~\cite{More_than_frequency}, we applied RoBERTa to all the text in the region of interest. Besides, as we aim at deploying \name{} to end-user devices like laptops, we do not consider using billion-level large language models for efficiency reasons. Meanwhile, generative models are also unsuitable in our use case, as our task is to identify users' unknown words in the given text rather than generating new content. Therefore, we choose RoBERTa instead of GPT. The model details related to this are explained in Section~\ref{sec:model_text}.
    \item Word-level knowledge of the target word such as term frequency, part of speech and named entity recognition are applied to enhance the performance. The tokenization is required to use RoBERTa. The word-level linguistic characteristics are partially lost during tokenization. Thus, we introduce word-level knowledge to compensate for the loss. The model details related to this are explained in Section~\ref{sec:model_knowledge}.
    \item Gaze pattern is automatically learned from gaze trajectory and text position by the model adapted from T5, one of the top encoder-decoder transformer-based architectures. Gaze patterns can reflect a userâ€™s cognitive process, indicating whether the user encounters an unknown word~\cite{just1980theory}. Considering the complexity of gaze patterns and the variability between users, we want to automatically learn these patterns using machine learning models based on gaze trajectory and text position. The encoder-decoder model can learn complex relationships between different types of sequential data. T5 has demonstrated cutting-edge performance on a wide range of tasks and is capable of capturing complex patterns and dependencies within input sequences~\cite{raffel2020exploring}. We take out the output of the last encoder layer as the positional encoding which represents the relationship between the gaze trajectory and text position. The model details related to this are explained in Section~\ref{sec:model_positional_data_encoding}.
\end{enumerate}

\begin{figure*}
  \centering
  % \includegraphics[scale = 0.43]{figures/justification.png}
   \includegraphics[width = 0.72\linewidth]{figures/justification.png}
  %\setlength{\abovecaptionskip}{0.1cm}
  \caption{Methodology for Detecting Unknown Words Using Integrated Text and Gaze Information. This integrated approach leverages both linguistic and user-dependent factors to effectively identify unknown words.}
  \label{fig:justification} 
\end{figure*}

Our method has three main advantages compared to previous works:
\begin{enumerate}
    \item \name{} has a higher tolerance for the noise of gaze data and has better performance by leveraging language models' ability to capture linguistic characteristics to reduce the reliance on gaze (results in Section~\ref{sec:main_result} and Section~\ref{sec:contribution_PLM}).
    \item \name{} eliminates tedious feature engineering by automatically learning gaze patterns based on gaze trajectory and text coordinates (detailed explanations in Section~\ref{sec:model_positional_data_encoding} and results in Section~\ref{sec:contribution_gaze}).
    \item \name{} enables real-time detection by only relying on the local information in the region of interest, which is benefit from the low reliance on gaze (results in Section~\ref{sec:user_evaluation}).
\end{enumerate}

\subsection{Unknown Word Detection Model}
\label{sec:method_model}
The goal of our model is to classify whether a word is an unknown word or not using both gaze information and text information. We use the encoder-decoder architecture to encode positional gaze and text information into the vector space. We also leverage RoBERTa~\cite{liu2019roberta} to integrate with text information. Moreover, we also add the prior knowledge to take the use of the word-level information. The overall architecture of our model is shown in Fig.~\ref{fig:model}.

\begin{figure*}[htbp]
  \centering
  % \includegraphics[scale = 0.32]{figures/model_new.png}
  \includegraphics[width = 0.9\linewidth]{figures/model_new.png}
  %\setlength{\abovecaptionskip}{0.1cm}
  \caption{Our model includes an encoder-decoder model to encode positional data, a pre-trained RoBERTa to encode text information, and learnable embeddings to encode the knowledge. The concatenation of these three matrices is input to a binary classifier.}
  \label{fig:model} 
\end{figure*}

\subsubsection{Positional Data Encoding}
\label{sec:model_positional_data_encoding}
To accurately capture the correlation between the user's gaze and the document, we adapt the model architecture from T5~\cite{raffel2020exploring}, the state-of-the-art encoder-decoder language model to process the positional data from those two modalities, where the encoder learns to capture the user's gaze pattern and the decoder is expected to predict whether a token belongs to an unknown word of the user, based on the encoder's outputs and the tokens' positional data. For the encoder part, we feed the raw and the moving averaged gaze trace to let the model capture both the fine-grained and general positional information from the user's behavior. As for the decoder part, instead of using the token-level positional data as the inputs only, we also calculate the averaged gaze-token distance $d(g, w)$ and the gaze duration $t(g, w)$ (the length of the time when the user's gaze lives in the bounding box of the token) for each token $w$
\begin{align}
    d(g, w) &= \sqrt{(\frac{\sum_{i=1}^{N_g} g_x^i}{{N_g}} - \frac{w_x^s + w_x^t}{2}) ^ 2 + (\frac{\sum_{i=1}^{N_g} g_y^i}{{N_g}} - \frac{w_y^s + w_y^t}{2}) ^ 2} \\
    t(g, w) &= |\{(g_x^i, g_y^i)| 1 \le i \le N_g \land w_x^s \le g_x^i \le w_x^t \land w_y^s \le g_y^i \le w_y^t\}|
\end{align}
where $N_g$ is the number of gaze samples within the sliding window, $w_x^s, w_x^t, w_y^s, w_y^t$ are the coordinates of the bounding box of the token $w$, and $g_x$ and $g_y$ are the user's gaze trace on x and y axis, respectively. Overall, this encoder-decoder model can be summarized as
\begin{align}
    H_g &= \text{Encoder}(g_x, g_y, g_x^{raw}, g_y^{raw}) \\
    P &= \text{Decoder}(H_g, w_x, w_y, d(g, w), t(g, w))
\end{align}
where $g_x, g_y$ are the moving-averaged gaze positional data, $g_x^{raw}, g_y^{raw}$ are the raw gaze data without filters applied, $w_x, w_y$ are the positions of each token, and $H_g$ is the final encoder outputs, which is used as the inputs (more specifically, keys and values) of the cross-attention modules in decoder layers. 


\subsubsection{Textual Information Capturing}
\label{sec:model_text}
We utilize a pre-trained RoBERTa, a widely used pre-trained language model based on the transformer architecture, to encode the text information. The layer consists of a self-attention module and a feed-forward layer. The structure can help the model better encode the text by using the surrounding text to establish the context. It encodes text data $s \in \mathbb{R}^{n_{txt}}$ to $Z \in \mathbb{R}^{n_{txt} \times n_r}$, in which $n_r$ is the hidden dimension of RoBERTa.

\begin{equation}
Z = \textrm{RoBERTa}(s)
\end{equation}

\subsubsection{Knowledge-grounded Enhancement}
\label{sec:model_knowledge}
The pre-trained language model takes tokens instead of words as input. Word-level information may be lost during tokenization. Therefore, we introduce word-level knowledge including the term frequencies, part of speech~\cite{bird2009natural}, and named entity recognition~\cite{honnibal2020spacy} to utilize word-level information. These features are encoded into a knowledge matrix $K \in \mathbb{R}^{n_{txt} \times n_k}$.

\subsubsection{Training}
We combine the encodings of the three modules' outputs as the inputs for the final classifier layer, a logistic regression module for binary classification. We fully fine-tune our model, including the positional encoder-decoder module, the RoBERTa pre-trained module, the knowledge embeddings, along with the final classifier on our dataset. It shall be noticed that the task of unknown word detection is significantly class imbalanced, where on average there are 15 times more negative tokens (known words) than positive ones (unknown words). To mitigate this issue, we use the focal binary entropy loss for our model's training which re-weights the loss term for more robust training:
\begin{align}
    H &= [P; Z; K] \\
    p &= \sigma(W_o \cdot H+ b_o) \\
    \mathcal{L}(p, \hat{y}) &= - \alpha \hat{y} (1-p)^\gamma \log(p) - (1-\alpha) (1-\hat{y}) p^\gamma \log(1-p) 
\end{align}
where $p$ is the model's prediction logits, $\hat{y}$ is the ground truth, while $\alpha$ and $\gamma$ are the hyper-parameters that control the weight between the two classes and the speed of the model's focus on difficult examples, respectively.

\subsection{Data Preparation}
\label{sec:method_data_preparation}
\subsubsection{Implementation}
\label{sec:method_implementation}
We built a system to collect gaze data from eye trackers and webcams at the same time. We used Tobii Pro Nano\footnote{https://www.tobii.com/products/eye-trackers/screen-based/tobii-pro-nano} eye tracker whose sampling rate is 60Hz and streamed its data to the computer through a Python script. The accuracy of the optimal conditions of Tobii Nano eye tracker is $0.3^\circ$. For webcam data, we used a SeeSo\footnote{https://seeso.io/}, a remote eye tracking platform, by integrating it into our PDF reader. To get text information including their contents and positions, we built a web-based PDF reader based on an open source Github repository\footnote{https://github.com/zotero/pdf-reader}. This platform can record gaze data using Seeso, retrieve text information when users finish reading, and record users' clicks while they are labeling their unknown words after the reading. 

The laptops we used were Macbook Pro (CPU: Apple M1 Pro, RAM: $16$ GB, Storage: $512$ GB, Screen Size: $14.2$ inches, Resolution: $1512 \times 982$)\footnote{https://support.apple.com/en-us/111902} and Huawei MateBook D14 2022 (CPU: i5-1155G7, RAM: $16$ GB, Storage: $512$ GB, Screen Size: $14$ inches, Resolution: $1920 \times 1080$)\footnote{https://consumer.huawei.com/en/laptops/matebook-d-14-2022/specs/}. The height of the line is $20.63$ pixels on Macbook and $20.74$ on Matebook, so the height of each line of text is approximately $4.1$ mm on the MacBook and about $3.3$ mm on the MateBook during the experiment. According to the Tobii calibration requirement\footnote{\url{https://connect.tobii.com/s/article/how-to-position-participants-and-the-eye-tracker?language=en_US}}, the distance between the participant's face and the screen during the experiment is about $50-60$ cm. Thus, the 10-point font size can be converted to $0.39^\circ - 0.47^\circ$ on a MacBook and $0.32^\circ - 0.39^\circ$ on a MateBook.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\columnwidth]{figures/implementation_new_v.png}
  %\setlength{\abovecaptionskip}{0.1cm}
  \caption{(A) During data collection, users are seated at a distance of approximately 50-60 cm from the screen, as instructed by the Tobii calibration interface. The eye tracker and webcam simultaneously collect data. (B) The data collection platform includes a Python script to read eye tracker data and a web-based PDF reader to read webcam data and record word labeling.}
  \label{fig:implementation} 
\end{figure}

\subsubsection{Participant and Material}
We recruited a total of 20 undergraduate and graduate students (5 females, 15 males) whose second language is English. Their Vocabulary Levels Test (VLT) scores ranged from 7 to 30 ($M = 20.24, SD = 7.15$). The VLT is widely used to measure the vocabulary level of English learners~\cite{nation1990teaching,schmitt2001developing}. We chose the 5000-word frequency level group because the TOEFL reading materials we used for the user study presume a vocabulary knowledge of approximately 4500 words~\cite{chujo2009many}. Their ages range from 21 to 26 years old (\textit{M} = 22.85, \textit{SD} = 1.65).
%, and the number of years they have studied English formally ranges from 9 to 21 years (\textit{M} = 18.70, \textit{SD} = 3.27). 
Among them, 16 people wore glasses during data collection and 4 did not.
The reading materials contain 120 articles of TOEFL and GRE reading with an average length of 363 words per article and 43534 words in total. We organized the text into a common paper format, which is single-spaced and has two columns. The font is Times New Roman and the font size is 10.

\subsubsection{Experiment Design and Procedure}
\label{sec:method_procedure}
To increase text diversity, we divided twenty participants into four groups, and participants in each group read the same 30 articles. These 30 articles were divided into 3 days to read, which took about an hour each day. There was a calibration session before data collection for each participant each day to calibrate both the eye tracker and webcam eye tracker. Participants can take a break whenever they feel fatigued. Typically, participants take a break after reading 3-5 articles. If they chose to take a break, there would be a re-calibration before the collection was restarted, considering that the person's sitting posture had a great impact on the accuracy of eye tracking. The average reading time per article is 2.88 minutes ($SD = 1.01$), which means the continuous reading time ranges from 8 to 14 minutes after one calibration.

Following the previous work~\cite{gaze-text_garain_2017}, we separated the collection of gaze data and the labeling of unknown words in order to avoid mouse clicks from affecting the user's normal eye movement behavior when reading. Participants read each article twice. During the first pass, eye movement data were collected while participants were reading. In the second pass, participants were asked to mark the unknown words they encountered in the first pass, and eye movement data were not collected for the second pass. Participants started the second pass right after the first pass to restore the feelings of the first time and help participants recall as much as possible the unknown words they encountered in the first pass. In addition, before participants read each article for the first time, the experimenter clicked a button to download all the words on the current page along with their corresponding coordinates. During the second pass, participants were required not to zoom or move the webpage to ensure that the positions of the text remained unchanged.

\subsubsection{Data Preprocessing}
To support real-time unknown word detection, we used a sliding time window to segment the data. The length of the window is 1 second, and the overlap between the two windows is zero. The data processing includes two steps. First, we located the text the user read within 1 second using the coordination of the gaze sequence. Then, we processed the raw gaze data and text data into 3 types of information which are gaze, text, and knowledge for each word in the sliding window.

\begin{figure*}[htbp]
  \centering
  % \includegraphics[scale = 0.4]{figures/preprocessing_new.png}
  \includegraphics[width = 0.9\linewidth]{figures/preprocessing_new.png}
  %\setlength{\abovecaptionskip}{0.1cm}
  \caption{A bounding box is derived from the gaze coordination within a 1-second sliding window. The gaze data, token-level text data, and word-level knowledge data are calculated for each candidate word in the bounding box.}
  \label{fig:preprocessing} 
\end{figure*}

As shown in the left part of Fig.~\ref{fig:preprocessing}, we firstly de-noised the gaze data to locate the content the user is reading by getting the bounding box of the gaze coordination. Blinking can cause sudden changes in gaze data in the y direction, which will cause the bounding box to be abnormally large. We removed these outliers to avoid the extra-large bounding boxes. We analyzed the distribution of the range of the y-coordinates of the gaze data within 1 second. When the distance between a small subset of the data in a window and the other part of the data exceeds the width of three lines, we removed the smaller portion of the data. If the y-coordinates of all data in the window fluctuated greatly, we ignored this window.

After getting the bounding box of 1s gaze (red dotted box in Fig.~\ref{fig:preprocessing}), all the words except the function word (such as articles and conjunctions) in the box were regarded as the word candidate. For gaze data, the 1-second window was extended to the 3-second window by adding one second before and after the window. The consideration for this extension is that the word identification span is about 7-8 characters~\cite{RAYNER19953} and the average reading speed of participants is 2.51 words per second. We applied the moving average filter and re-sampled the gaze data collected by webcam but not eye tracker, because webcam-based data is more noisy and the sampling rate of it varies from 24Hz to 27Hz. For text data, we tokenized the content within the 1-second window. We also added prior knowledge such as the term frequencies, part of speech, and named entity recognition to each candidate word. In the example shown in Fig.~\ref{fig:preprocessing}, finally we got 3 samples from this 1-second window which are 3 words with their gaze data, token-level text data, and word-level knowledge. Labels of these words are directly derived from the mouse click file.

Compared with the data collected with eye trackers, the data collected with the webcam is noisier, and the data quality is more affected by the user's sitting posture. Therefore, we aligned the data of the first article read by the user in each session with the coordinates of the article and applied the parameters of this alignment to other articles in this session.



