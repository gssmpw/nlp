\section{User Evaluation}
\label{sec:user_evaluation}
The user experience using the application built on \name{} depends on the real-time performance of \name{}. To evaluate the performance of our method in a real-time scenario, we built a reading assistance prototype that provided the translation and explanation of words while the user was reading. Then, we calculated the metrics such as F1-score and reading time of \name{} and also analyzed the subject feedback from users.

\subsection{User Study Design}
\label{sec:user_study_design}
The questions we want to know the answers are
\begin{enumerate}
    \item What is the real-time performance (F1-score, correctly triggered rate, and false alarm rate) of our unknown word detection method (Section~\ref{sec:eval_realtime_performance})?
    \item Can our method make reading more fluent (Section~\ref{sec:eval_reading_fluency})?
    \item How is user experience when comparing our system to the commonly used method (Section~\ref{sec:eval_user_experience})?
\end{enumerate}

To answer these questions, we implemented three methods for real-time unknown word detection and compared them regarding objective metrics and subject scale. The first method (\name{}) uses gaze to locate the region of interest and applies our model to detect unknown words. When a new word is detected, its translation and explanation will be automatically displayed in the sidebar, and users do not need to take any action. The second method (Click) imitates the typical process of looking up words by clicking. When a user selects or double-clicks a word, a "meaning" option pops up for translation and explanation of the word.  The third method (Ideal) simulates an ideal scenario where the model is highly accurate. Before users start reading, they are required to select unknown words from a list of candidate words. As users start reading, the words in the region of interest are compared with the selected unknown words. The system then displays the words in the unknown word list. To control variables other than prediction accuracy, all data processing and model inference procedures for Ideal are the same as those of \name{}. This approach aims to eliminate any degradation in user experience caused by model inaccuracies and to solely evaluate the potential of the unknown word detection method using gaze and text.

For the objective metrics, we calculated the accuracy, F1-score, correctly triggered rate, and false alarm rate for \name{}. We also recorded the reading time of users under three methods.

For user experience, we conducted the evaluation for the three reading assistance based on each method across the following five aspects using a 5-point Likert scale:
\begin{itemize}
    \item Preference: To which degree do you prefer the application? 1 for ``I prefer this one least''. 5 for ``I prefer this one most''.
    \item Willingness to use: Would you like to use it in the future? 1 for ``I won't use this application in the future''. 5 for ``I would definitely use the application if it were available.'' 
    \item Usefulness: How helpful do you think it is for your reading and vocabulary learning? 1 for ``Not helpful at all''. 5 for ``Very helpful''.
    \item Perceived reading fluency: How do you feel about your reading fluency when using this feature? 1 for ``My reading is still slow and unsmooth''. 5 for ``It speeds up my reading a lot and makes my reading very fluent''.
    \item Perceived latency: What do you think of the latency from when you need help to when the system pops up an explanation? 1 for ``The latency is very small and does not affect usage''. 5 for ``The latency is very large and makes the system not usable at all''.
\end{itemize}
Then we compared the rate for three methods using Wilcoxon signed-rank test at .05 significance level.
    
\subsection{Setup and Procedure}
\label{sec:eval_setup}
\subsubsection{Implementation of a Reading Assistance Proof-of-Concept}
The proof-of-concept is a web-based application as shown in Fig.~\ref{fig:PoC}. The back-end was built using Python and the front-end was built using TypeScript and React. The React-PDF-viewer\footnote{https://react-pdf-viewer.dev/} was used to show the pdf. For the \name{} and Ideal, the gaze was captured by Tobii Nano which was the same model as the one used in data collection for model training. Next, the data was transmitted to the Python back-end via TCP and input into the unknown word detection model for inference along with text data. The model we used here is the main model trained on all the data and without any ablation. The same data processing step as the training was applied to gaze and text data. Then, unknown words predicted by the model were sent to the front-end through a web socket. The OpenAI API was called in the front-end to get translations and contextual explanations of words. In the end, the translations and explanations of words were displayed on the web page's sidebar. For the Click, the unknown words were detected by a click listener and then called the OpenAI API to get the translations and explanations. The study was conducted on a MacBook Pro (CPU: Apple M1 Pro, RAM: 16G, screen: 14 inches).

\begin{figure}
  \centering
  \includegraphics[width=1.0\columnwidth]{figures/PoC_v.png}
  %\setlength{\abovecaptionskip}{0.1cm}
  \caption{The implementation of reading assistance prototype based on our unknown word detection method.}
  \label{fig:PoC} 
\end{figure}

\subsubsection{Participant and Material}
\label{sec:eval_participant}
We recruited 10 English as a second language learners aged between 22-28 ($M=23.80, SD=1.87$), including 4 males and 6 females. Participants in this evaluation are different from participants for data collection. Eight of them wore glasses and two of them didn't wear glasses. Their Vocabulary Levels Test (VLT) score on the 5000-word frequency level group ranged from 10 to 29 ($M=18.80, SD=5.07$).

The documents we used are from TOEFL reading material with similar lengths (390 words/24 lines, 353 words/23 lines, 371 words/22 lines). Another important reason for selecting these three documents is that, their total numbers of unknown words (49 words, 49 words, 52 words) marked in the previous data collection are close. During the user study, the text was displayed in two columns, consistent with the settings used for data collection. We employed Times New Roman font at size 10 with single line spacing.

\subsubsection{Procedure}
Firstly, the experimenter introduced the background of the study and randomly assigned three articles to three tasks (\name{}, Click, and Ideal). Afterward, the eye tracker was calibrated and the participants were required to try the \name{} and Click to get familiar with them. After the calibration, participants were informed to maintain the same posture as much as possible and read with their original reading habits. 

Participants used \name{} when reading the first document. The application automatically saved the unknown words detected by the model, the user's reading time, and gaze data. Participants marked unknown words they encountered for ground truth after the reading. Participants used Click and got word explanation by clicking the word when reading the second document. The reading time was recorded. For the Ideal, participants first deleted words they knew from the unknown word candidate list before the reading. The gaze function was turned to locate the region of interest and the explanations for words from the unknown word list were displayed when the participant encountered those words. Reading time and gaze data were recorded. After reading three documents, the participants were asked to rate \name{}, Click, and Ideal individually in the first aspects mentioned in Section.~\ref{sec:user_study_design}.

\subsection{Analysis and Results}
\label{sec:eval_result}
\subsubsection{Real-Time Performance}
\label{sec:eval_realtime_performance}
We calculated the F1-score, precision, and recall for each participant by comparing the unknown words marked by the participants and detected by the model in \name{} setting. The averaged F1-score, precision, and recall are respectively 56.54\%, 51.93\%, and 67.39\%, which is close to the cross-user result in Section~\ref{sec:cross-user} (59.6\%, 52.8\%, and 68.4\%). This result shows that our model performs the same when running offline and in real-time, which proves that our model can work in real-time. The average correctly triggered rate is equal to recall which is 67.39\% and the average false alert is 2.89\%. The false alert rate is similar to the previous work conducted using the more accurate head-mounted eye tracker~\cite{idict2006hyrskykari}. The low correctly triggered rate could be caused by the inaccuracy of our model and the inevitable change of posture when the user was reading.

\subsubsection{Reading Fluency}
\label{sec:eval_reading_fluency}
We evaluated the influence of our word detection method on improving users' reading fluency from two perspectives which are reading time and user perceived fluency. The reading time of \name{} ($p = 0.006$) and Ideal ($p = 0.048$) is significantly shorter than the reading time of Click. There is no significant difference between the reading time of \name{} and Ideal ($p = 0.193$).

For the perceived reading fluency, the rate of ideal is significantly higher than Click ($p = 0.030$), but there is no significant difference between \name{} and Click ($p = 0.0773$). Automatically detecting unknown words speeds up the reading by minimizing the number of operations. However, participants' feedback shows that the explanation of incorrect unknown words that popped up in \name{} distracted users and slowed down the reading when using \name{}. Ideal does not have this problem and will not interrupt reading due to mouse operation, so it makes the reading process more fluent.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.49\columnwidth]{figures/evaluation_reading_time_new.png}
    \includegraphics[width=0.49\columnwidth]{figures/evaluation_reading_fluency_new.png}
    \caption{\name{} enhances the fluency of reading for users compared with getting word explanation by clicking the unknown word.}
    \label{fig:eval_fluency}
\end{figure}

\subsubsection{User Experience}
\label{sec:eval_user_experience}
For user preference, the average scores for Click and Ideal are the same as shown in Fig.~\ref{fig:eval_subjective}. It indicates that detecting unknown words by gaze and PLMs has the potential to replace the conventional method of word lookup via mouse clicks. In terms of willingness to use, the score of Ideal (4.1) is higher than Click (3.5). Most participants stated they preferred real-time automatic word detection if the accuracy was high. For the usefulness, the score of Ideal (4.2) is slightly higher than Click (4.0). Participants indicated that our proposed method helps them read more fluently with less disruption. The perceived latency of Ideal (2.7) is lower than Click (3.1) demonstrating that the latency caused by the data processing and model inference is negligible because the Ideal also includes these components from \name{}. Moreover, the time saved by avoiding clicks can compensate for the delay introduced by GPT, thereby reducing overall latency.

The scores of \name{} are lower than Click, but Ideal can surpass traditional click methods in most aspects. Considering that the major difference between \name{} and Ideal is the accuracy of the unknown word detection model, \name{} could offer a more natural and efficient interaction mechanism by further improving the accuracy in the future.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.49\columnwidth]{figures/evaluating_subjective_new.png}
    \includegraphics[width=0.49\columnwidth]{figures/evaluation_latency_new.png}
    \caption{Subjective rating of user experience. Left: the ideal condition of our method improves the willingness to use and usefulness compared to the click method. Right: the latency that participants perceived of ideal condition is less than click.}
    \label{fig:eval_subjective}
\end{figure}