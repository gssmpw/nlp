\section{Experiment}
\label{sec:experiment}

\change{In this section, we systematically evaluated our model. First, we present the results of our model on eye tracker data and webcam data.
% , showing significant improvements compared to the gaze-based baseline. 
Second, we analyzed the contributions of gaze and the pre-trained language model (PLM) separately.
% The context information provided by the PLM played a major role, while gaze captured user differences and provided real-time regions of interest.
% The reason why gaze contributes marginally to the model compared to PLM is that the accuracy of the eye tracker ($0.3^\circ$) is insufficient to precisely detect target words under normal font size and line spacing (required $0.2-0.4^\circ$). Additionally, changes in the user's posture during prolonged reading further reduce the accuracy of the eye tracker. 
Finally, we discuss several important aspects for practical applications, including the model's generalizability, latency, and memory consumption.}

\subsection{Experiment Settings}
\subsubsection{Training and Evaluation of \name{}}
The statistics of our collected dataset are shown in Table~\ref{tab:stat}. In the default setting (main model), the data from all users and all articles are mixed and divided into training set, development set and testing set according to 8:1:1. We fully fine-tuned our model for 30 epochs with a batch size of 32 on our dataset. The learning rate of the positional encoder-decoder model is set as 1e-3, while the rest of the model's learning rate, including the RoBERTa backbone, knowledge embeddings, and the classifier are set as 2e-5. The sample rate of the user's gaze is set as 60 Hz, with a maximum length of 3 seconds, while the maximum number of context tokens is set as 64. 

\input{tables/stats}

We report word-level accuracy, precision, recall, and F1-score in our experiments. In detail, a word is recognized as unknown if there exists at least one token within it predicted as positive. We report the F1 score on the test dataset, after selecting the best model based on the F1 score on the dev set throughout training with early stopping, with the binary classification threshold searched between 0 to 1 with the step of 0.01. The result is shown in Table~\ref{tab:main-results}.

\input{tables/main_results}

\subsubsection{Baseline}
We compared our method to an SVM baseline implemented according to~\cite{gaze-text_garain_2017} and three simple baselines. For distance and fixation heuristics baseline, the distance and fixation time calculation follow those described in Section~\ref{sec:model_positional_data_encoding}. The only difference is that we use the bounding box of words instead of tokens in this case. The logistic regression model is trained with these two features above, plus word term frequency, part of speech, and named entity tags. Results are shown in Table~\ref{tab:main-results}.

When implementing the SVM baseline, we used four gaze features (number of forward gaze points, number of backward gaze points, duration of forward reading, duration of backward reading) plus two linguistic features (word length, word frequency) mentioned in~\cite{gaze-text_garain_2017}. Because the articles we chose are longer than those in previous work, there were lots of rereads, making it difficult to determine whether it is a new line or not based on sudden changes in the x value. In addition, due to the smaller line spacing, correcting the line number based on the y value is impossible. Therefore, we only assigned line numbers to the gaze based on which line was closest to the gaze coordination and used a median filter to remove outliers. When choosing the parameters for SVM, we did the grid search and used the best one for the evaluation. The result we reported was trained using SVM (RBF kernel, C=0.01, gamma=0.1) with a class weight ratio of 6:1 between positive and negative classes.

\subsubsection{Ablation Study}
As shown in Table~\ref{tab:ablation}, we conducted ablation studies on our method by removing the contextual encoding (the use of PLMs), gaze encoding (the positional encoder-decoder), and the knowledge embeddings, respectively. When ablating the text encoding component (the PLM), we remove the text information in the inputs and discard the text encoding in our model. The model should predict the users' unknown words based on their gaze trajectories' relationship to texts' positions plus prior knowledge. When the gaze encoding is ablated, corresponding positional data and modules in Fig.~\ref{fig:model} are removed. Similarly, knowledge-oriented inputs and embeddings will be removed when we do knowledge embedding ablation. As these ablations directly remove corresponding components in our model, each would reduce our model's parameter size. To further demonstrate our model succeeds in unknown word detection due to leveraging pre-trained RoBERTa weights rather than simply scaling the model's size up, we add an extra ablation (``w/o pretrained RoBERTa'') where the RoBERTa model parameters are added to the model but randomly initialized.


\subsubsection{N-Gram Baseline}
\label{sec:experiment_n-gram}
Due to the overlap of unknown words between the training set and test set, the model might exhibit a shortcut by memorizing the unknown words in the training set. To examine the impact of this shortcut on model performance, we implemented an n-gram-based baseline to simulate the scenario where the model "memorizes" the unknown words. During prediction, if the current word combined with its preceding $n - 1$ words forms an n-gram appearing in the training set while also with the current word being labeled as unknown, the current word is classified as an unknown word at inference time. The larger the $n$, the more contextual information the n-gram baseline utilized. Since the degree of overlap in unknown words between the training and test sets varies across the main, cross-user, and cross-document settings, we applied the n-gram baseline ($n=1,2,3$) to each of these settings individually (Table~\ref{tab:n-gram}).
% \input{tables/similarity}

\input{tables/ablation}

\subsection{Model Performance}
\label{sec:main_result}

% 想说整体效果好，比random classifier强，比 svm baseline强

The main results of our experiment are shown in Table~\ref{tab:main-results}, the F1-score, precision and recall of our model that was trained and tested on data collected by eye tracker are 71.1\%, 63.3\% and 79.0\%. Our model significantly outperforms the heuristic baseline (F1-score 22.9\%), logistic regression (F1-score 23.4\%) and the previous work based on SVM~\cite{gaze-text_garain_2017} (F1-score 29.9\%). The performance of SVM is much worse than that presented in their paper. This could be caused by the smaller line space, which made it harder to assign the gaze to the word correctly. Besides, the document we used was more than two times longer than theirs, which led to a longer reading time (2.88 minutes per document and 8-14 minutes per reading session) and more posture changes of participants. This could worsen the tracking accuracy of eye tracker. Compared to their method, our method relies less on the gaze modality and PLMs also provide rich linguistic characteristics, thus improving the performance.

We also tested our model on the relatively inaccurate gaze data collected using a webcam (Fig.~\ref{fig:gaze}). To quantitatively compare the quality of gaze data collected by the eye tracker and webcam, we calculated the median absolute error (MAE) between these two types of gaze data. Since the sampling rate of the webcam is lower than that of the eye tracker, we applied cubic spline interpolation to the gaze coordination collected by webcam. The MAE in the x-direction is 115.99 pixels ($SD = 46.32$). In the y-direction, the MAE is 67.25 pixels ($SD = 32.28$). Considering that the height of the text is 20.63 pixels per line, the data from the webcam is noisier compared to the data obtained from the eye tracker. The model trained and tested on the relatively inaccurate gaze data collected using a Webcam (Fig.~\ref{fig:gaze}) also achieves a high F1-score (65.1\%) compared to baselines, even though slightly worse compared to our method trained on eye tracker collected data. This is another evidence that our method has a higher tolerance for the noise of gaze data. This result also opens the potential for more accessible solutions than previously possible.

\begin{figure}
  \centering
  \includegraphics[width=1.0\columnwidth]{figures/gaze_data_new_v.png}
  \caption{(A) Gaze data collected by a Tobii eye tracker. (B) Gaze data collected by a webcam.}
  \label{fig:gaze} 
\end{figure}

\subsection{Contribution of Gaze}
\label{sec:contribution_gaze}
%gaze有用：
%1. similiary矩阵，说明用户间有差异，从原理上来说是gaze是必要的。
%2 ablation去掉gaze后precision没变recall降了说明fn多了。
%3. case study发现有用（这个图我还没画，明早画）。

Firstly, we analyzed the differences in unknown words among users to demonstrate that the user-dependent gaze feature is necessary for correctly identifying unknown words for different users. We computed the Jaccard similarity matrix for all users. Users were divided into four groups because different groups of users read different documents. The average cross-user Jaccard similarity score is 0.24 and 77.5\% of user-paired scores are below 0.3. The low Jaccard similarity score indicates that different users have different unknown words even reading the same document. Only relying on textual information cannot identify differences between users, so it is necessary to use the user-dependent gaze features.

\begin{figure}
  \centering
  \includegraphics[width=1.0\columnwidth]{figures/jaccard_similarity_v.png}
  \caption{Jaccard similarity of unknown words between users.}
  \label{fig:similarity} 
\end{figure}

Then, we removed the gaze encoding in the ablation study to determine the extent of the contribution of gaze to unknown word detection. After removing the gaze, the recall drops from 79.0\% to 74.4\% while the precision remains almost the same. This phenomenon implies that gaze is helpful for identifying an individual's unknown words based on the user's unique behaviors and patterns.

We also analyzed some cases to verify that gaze can help reflect differences between users and correctly predict unknown words in real time. We plotted the clip of gaze trajectory within 3 seconds (1-second gaze to locate the region of interest and additional 2-second gaze for features) and the corresponding text in the region of interest. In Case 1 shown in Fig.~\ref{fig:case_ignominious}, the word "ignominious" is an unknown word for user B but is not an unknown word for user A. There is a noticeable dwell on "ignominious" in B's case which can facilitate the model to detect it as an unknown word correctly. The model captured this information because it predicted correctly in both cases. 

\begin{figure}
  \centering
  \includegraphics[width=1.0\columnwidth]{figures/case1_new_v.png}
  \caption{Case 1: (A) The word "ignominious" is not an unknown word for user A. (B) The word "ignominious" is an unknown word for user B.}
  \label{fig:case_ignominious} 
\end{figure}

Another case is shown in Fig.~\ref{fig:case_2} in which the gaze in A and B are from the same user but in two consecutive seconds. The model correctly detects 'undergo' in the first second (A) and 'necrosis' in the second second (B) when the user's regions of interest in two consecutive seconds both contain 'undergo' and 'necrosis'. This result also indicates that our model learned gaze features and gaze plays an important role in real-time detection.

\begin{figure}
  \centering
  \includegraphics[width=1.0\columnwidth]{figures/case2_new_v.png}
  \caption{Case 2: (A) The word "undergo" was detected in the first second. (B) The word "necrosis" was detected in the second second.}
  \label{fig:case_2} 
\end{figure}

\subsection{Contribution of PLM}
\label{sec:contribution_PLM}
%plm的有用：
%1. ablation说明roberta有用。
%2. random init之后效果差了说明pretrain的有用。

To identify the contribution of the pre-trained language model, we separately remove the "pre-trained" weights ("w/o pre-trained RoBERTa" in Fig.~\ref{tab:ablation}) by randomly initializing RoBERTa weights and remove the whole language model ("w/o textual encoding" in Fig.~\ref{tab:ablation}) by removing the RoBERTa weights. When initializing the RoBERTa randomly ("w/o pre-trained RoBERTa"), the F1-score decreases from 71.1\% to 67.6\% which indicates that the linguistic characteristics of words learned by the PLM help \name{} to detect unknown words more accurately. After entirely removing the RoBERTa parameters ("w/o textual encoding"), the F1-score significantly dropped to 22.0\%, proving the effectiveness of using PLMs for better capturing the contextual information of the documents in the task of unknown word detection. The reason why the F1-score of "w/o pre-trained RoBERTa" is relatively high compared to "w/o textual encoding" is that the number of parameters remains constant in this case though the initialization is random, allowing RoBERTa to learn linguistic features incrementally during training.

Due to the overlap of unknown words between the training and test sets, we implemented an n-gram baseline as another naive baseline to simulate the scenario where the model 'memorizes' the unknown words to analyze the effect of this shortcut. As shown in Table~\ref{tab:n-gram}, the results are best when $n=2$, with an F1 score of 51.9\%. This result is lower than our model (71.1\%), indicating that merely memorizing words is insufficient. In contrast, our model leverages the PLM to provide a more comprehensive understanding of the linguistic information from context. Moreover, another interesting phenomenon is that the F1 score of the 2-gram baseline (51.9\%) is higher than the gaze-based SVM baseline (29.9\%). This suggests that textual information contributes more than gaze data when the line space is so small that the gaze is not accurate enough to locate individual words.

\input{tables/n-gram_baseline}

% \change{In Section.~\ref{sec:cross-user}, 
We also conducted the ablation study to the cross-document setting when the overlap of unknown words between the training and test sets is very small (Jaccard Similarity is 0.006). As shown in Table~\ref{tab:cross_doc_ablation}, removing gaze leads to a drop in performance, but it is less significant compared to the drop caused by the absence of pre-training.

Synthesizing the insights from these three experiments, we conclude that gaze is indispensable for locating regions of interest and providing real-time detection, and it also improves the model performance. However, the linguistic information introduced by the PLM plays the primary role in enhancing the model performance.

\subsection{Cross-User and Cross-Document Generalizability}
\label{sec:cross-user}
We evaluate our method with the cross-user generalizability with the leave-one-user-out setting, where the data collected from 18 randomly-selected users are set as the training dataset, while the rest of the two user's data are used as the dev and test sets. We find that in cross-user settings, the F1 score of our method drops to 59.6\%, with the accuracy, precision, and recall decreasing to 97.3\%, 52.8\%, and 68.4\%. Compared to the main setting, the performance decline of the model in the cross-user setting could be attributed to the loss of personalized information due to the absence of new user's gaze data. Another possible reason is the lack of labels for unknown words from new users, which affects the model's ability to assess word difficulty accurately for them. This suggests that it is necessary to collect some data from new users for fine-tuning before deployment to better adapt the model to different users. Additionally, relying heavily on the PLM might hinder the model's ability to achieve personalization.

\input{tables/cross_user_doc}
To demonstrate that \name{} can indeed detect users' unknown words instead of simply memorizing difficult words in the documents, we also evaluate our model with the leave-one-document-out setting. In this case, we equally split the 120 reading documents into groups of 10, and in each training run, we trained our model on 110 documents while testing the model on the remaining 10 documents. The average Jaccard similarity score between the training set and test set is 0.006, demonstrating that the unknown words in the test set are very different from those in the training set. Our model's F1 score drops to 51.1\%, with the precision, recall, and accuracy shifts to 43.3\%, 62.8\%, and 96.8\%, respectively. The result indicates that generalizing the capabilities of unknown word detection across documents is challenging because of the lack of text context. Since the difference in context between the training and test set in the cross-document setting is larger than that in the cross-user setting, the performance in the cross-document setting is worse than cross-user setting. However, our model still achieves non-trivial improvements compared to the random classifier (F1-score 11.1\%) and n-gram baseline (F1-score 7.9\%), indicating that \name{} learns essential user gaze patterns and linguistic knowledge through training beyond naively memorizing difficult words.

To further investigate how gaze and PLM impact model performance, we conducted an ablation study in this challenging cross-document setting. After removing gaze and pre-training, the F1-score decreased to 49.9\% and 37.4\%, respectively. This result indicates that gaze contributes to the model's performance, but its impact is smaller compared to the contribution of textual information. Combined with the fact that the n-gram baseline's F1-score is only 7.9\%, it can be concluded that the contribution of textual information lies not in merely memorizing words but in enhancing the model's understanding of context.

\input{tables/cross_doc_ablation}

\subsection{Latency and Memory Consumption}
To demonstrate the capabilities of our method being used in real-time applications, we evaluate our method's latency with both CPU and GPU usage during inference. We set the batch size as 1 during inference latency testing. In GPU usage testing, we test the model with an RTX 4090 graphic card, where the average inference latency of the model is 0.013 seconds. Meanwhile, in CPU usage testing, the model's latency is 0.036 seconds. Overall, these results indicate that our method can support real-time applications with latency within 1 second. Moreover, under both settings, the maximum memory consumption of our model is 488.41MB, further proving that our method can be easily adapted to different on-device settings, widening our method's practicality for various downstream application supports.
