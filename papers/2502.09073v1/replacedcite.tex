\section{Related Work}
\subsection{Active Learning}

Active learning (AL) is a widely adopted technique for optimizing the trade-off between annotation costs and model performance by selecting the most informative samples from large unlabeled datasets. Central to AL are three components: a labeling oracle, an unlabeled data pool, and a query strategy. Common strategies include uncertainty-based methods, which prioritize difficult samples based on prediction uncertainty ____, and diversity-based methods, which focus on selecting diverse samples to enrich datasets ____. 

Active learning (AL) has been effectively applied across various NLP tasks, including text classification ____, text summarization ____, and question answering ____, achieving significant cost reductions and performance improvements. These approaches have demonstrated strong potential in optimizing model training efficiency and enhancing overall system performance. Despite their successes, existing methods often neglect the influence of inherent sample properties on diversity. Addressing this gap, our work introduces a novel approach for evaluating sample diversity in the RAG context by comparing similarities across different data fields.

\subsection{Active Learning Meets LLMs}

As large language models (LLMs) continue to advance, their integration with AL has become a focal point for addressing high annotation costs ____ and challenges in effective knowledge utilization ____. Currently, the integration of AL with LLMs primarily involves three approaches: employing traditional active learning methods to select samples for the downstream processes of LLMs (e.g., fine-tuning, in-context learning, evaluation) ____, utilizing LLMs to assess sample quality (e.g., uncertainty estimation) ____, and leveraging LLMs to replace human annotators ____. For instance, Margatina et al. \yrcite{margatina2023active} demonstrated the effectiveness of similarity sampling for classification, framing in-context learningâ€™s example selection as a single-round AL task. Li et al. \yrcite{li2024active} proposed LDCAL for text summarization, while Rouzegar and Makrehchi \yrcite{rouzegar2024enhancing} balanced cost and accuracy in text classification. Other studies addressed noisy data filtering ____ and explored the use of LLMs as annotators ____, highlighting both strengths and limitations.

Our research integrates AL into the RAG framework, leveraging its capabilities to address the unique challenges of fine-tuning LLMs. Specifically, we focus on selecting high-impact samples that enhance model performance while considering diversity within the RAG setting. To the best of our knowledge, this is the first study to explore AL-driven optimization for LLMs in the RAG context.