% There is a longer version in \cref{app:additional_background}

% \subsection{What are Community Notes?} 
% \begin{figure*}[!ht]
%     \centering
%     \includegraphics[width=\textwidth, trim={0 0 0 1cm},clip]{Figures/link_types.png}
%     \caption{The categories of links used by Community notes' authors as a source.}
%     \label{fig:link_types}
% \end{figure*}


% 
% "Writing Impact reflects how often a contributor's notes have earned a status of Helpful when rated by others. Impact increases when a note earns the status of Helpful, and decreases when a note reaches the status of Not Helpful."
% "Note statuses aren’t reached by majority rule. A note will reach a status of Helpful or Not Helpful only when there's agreement between contributors who have sometimes disagreed in their past ratings. This helps prevent one-sided ratings."
% https://communitynotes.x.com/guide/en/contributing/getting-started
% https://communitynotes.x.com/guide/en/contributing/writing-and-rating-impact
% \cite{CommunityNoteFAQ2024}
% \begin{itemize}
%     \item explain what community notes are (see Figure \ref{fig:notes_sample})
%     \item explain rating system
%     \item algorithm requires that a 'helpful' note receives roughly equal helpfulness votes from users with diverse viewpoints -- How Twitter’s Birdwatch fact-checking project really works - The Washington Post. washingtonpost.com/technology/2022/11/09/twitter-birdwatch-factcheck-musk-misinfo/
%     Note ranking algorithm. https://communitynotes.x.com/guide/en/under-the-hood/ranking-notes
% \end{itemize}

\subsection{Community notes}

Community moderation has been proposed as a means of addressing the scalability \citep{martel2024crowdmisinfo} and cross-partisanship trust \citep{poynter2019republicans} challenges associated with fact-checking. 
Twitter/X's Community Notes programme (piloted in 2021 and publicly launched in October 2022 \citep{TwitterBirdwatch2021}) is a notable example of such a system.
Any platform user may volunteer as a Community Notes contributor, although they must achieve a particular `rating impact score' before they can write notes \citep{TwitterRatingWritingImpact}.
Notes that achieve a `helpful' rating appear underneath the post, explaining why the post is misleading (see \cref{fig:notes_sample}). To be rated `helpful', a note must receive similar levels of helpfulness rating from users with diverse viewpoints \citep{TwitterNoteRanking}.

\subsubsection{Characteristics of Community Notes}

A small but growing body of work has analysed Twitter/X's Community Notes dataset, focusing on the targets, sources, and limitations of notes. 

\noindent \textbf{Targets of notes.}
Community notes tend to focus on misleading posts from large accounts %more exclusively than snopes do 
\citep{pilarski_community_2024}, focusing on posts that lack important content or present unverified claims as facts \citep{prollochs2022community,drolsbach_diffusion_2023}. 
% More recent analyses, however, have also shown that the more influential the account, the lower the level of consensus among users \citep{prollochs2022community}. 

\noindent \textbf{Sources in notes.}
Analyses have indicated that notes were rated more helpful if they link to `trustworthy' sources \citep{prollochs2022community} and that the majority of sources cited by notes were `trustworthy'
% \footnote{rated to be of high factuality by \href{https://mediabiasfactcheck.com/}{Media Bias Fact Check}.}
left-leaning news outlets. %pointing to a potential bias in the platform's community fact-checking 
A recent study finds that 55\% of URLs used in notes were related to news websites, 18\% to research, 9\% to social media, 9\% to encyclopedic sources, but \textit{just 1.2\%} to fact-checking sources \citep{kangur_who_2024}.

\noindent \textbf{Limitations of notes.}
Only 11\% of submitted notes reach `helpful' status (i.e., shown to users) by achieving a cross-perspective \citep{renault_collaboratively_2024, wirtschafter2023future}, and the time frame for notes to reach the algorithm's required agreement level (15.5 hours on average) limits its capacity to halt misinformation spread \citep{renault_collaboratively_2024}.
Additional concerns about the notes' efficacy highlight their indifference to the expertise needed for certain claims and reliance on subjective helpfulness rather than objective facts, free labour and inadequate support and guardrails regarding explicit content \citep{Gilbert_2025}. 

Our work provides novel insights into the targets, sources and limitations of community notes by shedding light on the relationship between notes and professional fact-checking. Namely, we study the extent to which fact-checking sources form the basis of note-writers' efforts to counter misinformation and identify the strategies they employ. 

\subsubsection{Impact of Community Notes on misinformation spread}
% Mixed evidence has emerged about the effectiveness of community notes on misinformation spread.
Posts identified by community notes as misleading have been found to attain less virality (reposts, quote tweets and replies) than non-misleading posts \cite{drolsbach_diffusion_2023,renault_collaboratively_2024}.
Community notes have also been shown to increase the probability of tweet retractions and deletions and speed up the retraction process \cite{gao_can_2024,renault_collaboratively_2024}.
However, other studies have found less positive evidence; for example, that users' followers, likes and engagement increase after their post receives a community note \cite{wirtschafter2023future}.
Curiously, one study claims that showing community notes on posts reduced the spread of misleading posts by an average of 61\% \cite{chuai_community_2024}, while a more recent analysis by the same authors found no effect of community notes on engagement with misinformation \cite{chuai_did_2024}.

%% testing effects of community notes
People shown community notes alongside misleading social media posts were more accurate in identifying misleading posts, and the notes were judged to be more trustworthy than context-free misinformation flags (e.g., "Checked by fact-checkers" or "Checked by other social media users"), regardless of (US-centric) political beliefs \cite{drolsbach2024community}.
People shown either community notes or related news article suggestions were both less likely to to believe and report misleading information compared to a control group: community notes were more effective in reducing belief and sharing intention for positive rumours, while articles were more effective for negative rumours \cite{kankham_community_2024}.
% For positive rumours, people shown community notes were less likely to believe them and share them than people shown related articles, however for negative rumours, related articles were more effective in reducing self-reported belief and likelihood of sharing, although these findings were based on responses to a single health-related claim 
On the other hand, displaying community notes leads users to post more negative and angry replies to misleading posts \cite{chuai_community_2024-1}, while crowd workers are also prone to cognitive biases, such as overestimating a statement's truthfulness the more they liked its claimant, and general overconfidence in their ability to ascertain truthful statements \cite{draws_effects_2022}.
%this study not great, not sure if we want to include:
% . %% unclear to me if the negativity is targeted at the post itself or the community note


\subsection{Professional fact-checking and community note practices}
Although fact-checks and community notes share similarities in how they address misleading claims, they also differ in key elements of practice and techniques of persuasion and communication  \citep{kankham_community_2024}.
Fact-checking typically involves the analysis and verification of public claims (e.g., statements in news reports and social media). In addition to verifying claims, in recent years many fact-checking organisations have also assumed a wider role in combatting misinformation spread, conducting long-term investigative journalism projects and citizen media literacy programs \citep{juneja2022human}.
Professional fact-checkers in organisations signatory to the International Fact-Checking Network (IFCN) follow a rigorous set of principles and transparency commitments.\footnote{\url{https://www.ifcncodeofprinciples.poynter.org/the-commitments}}
In contrast, any platform user can contribute to community notes under anonymity, and the rating approach relies on the 'wisdom of crowds', with little oversight or transparency regarding the biases of the note-writers.
%rely on the 'wisdom of crowds'
Numerous studies have documented the structured workflow that fact-checkers follow: (i) claim selection; (ii) collecting evidence; (iii) deciding on a verdict; and (iv) writing the fact-checking article \citep{graves2017anatomy,micallef2022true,warren2025explainablefactchecking}.
Fact-checking articles, which are subject to multiple rounds of editorial scrutiny, are more formal and standardised in tone and style than community notes, which vary considerably.
Fact-checkers must rely on credible sources and evidence to convince the reader, while community note writers may employ a range of persuasion techniques, such as appeals to emotion or other logical fallacies.
Moreover, community notes typically serve as direct rebuttals to misleading posts, while fact-checking articles may address a more general claim than is expressed in a specific post.
Finally, fact-checking articles are a one-way exchange, while community notes represent a more horizontal and interactive dialogue between writer and recipient of the fact-check \citep{kankham_community_2024}.

Our work builds on current understanding of the relationship between professional fact-checking and amateur community moderation by examining the extent to which community note writers deploy the work of professional fact-checkers in their notes.






% articles about why community notes are insufficient from fact-checkers' point of view

% https://www.poynter.org/commentary/2025/meta-ditches-fact-checkers-podcast-poynter-report/
% https://www.poynter.org/fact-checking/2023/why-twitters-community-notes-feature-mostly-fails-to-combat-misinformation/

% https://www.poynter.org/commentary/2024/x-community-notes-role-2024-presidential-election/


%CNs used for misleading more often than non-misleading tweets, 
 % notes yield a lower level of consensus among users for influential users with many followers, and community-created fact checks are more likely to be seen as being incorrect and argumentative.
% Most common reasons for notes: "factual errors (32 \%), lack of important context (30 \%), or because they treat unverified claims as facts (26 \%), outdated information (5 \%), satire (3 \%), or manipulated media (2 \%) are relatively rare."
% misleading tweets had more negative sentiment but similar complexity (gunning-fog index). 
% explanations of misleading tweets were longer than non-misleading explanations.
% notes are estimated to be more helpful if they embed positive sentiment, and if they use more words and more complex language.
% targets of notes mostly US politicians and earthquake predictions
%tweets reported as being misleading tend to have a lower number of followers and a lower number of followees.
% 54 \% of all misleading tweets were posted by verified accounts.
% "users find that Birdwatch notes are particularly helpful if they are (i) informative (24 \%), (ii) clear (24 \%), and (iii) provide good sources (21 \%). To a lesser extent, users also value Birdwatch notes that provide unique/informative context (14 \%) and are empathetic (11 \%)."
% "users find Birdwatch notes unhelpful (i) if sources are missing or unreliable (19 \%), (ii) if there is opinion speculation or bias (19 \%), (iii) if key points are missing (18 \%), (iv) if it is argumentative or inflammatory (13 \%), or (v) if it is incorrect (10 \%). In some cases, users also perceive a Birdwatch note as unhelpful because it is offtopic (5 \%) or hard to understand (4 \%). Only few Birdwatch notes are perceived as unhelpful because of spam harassment (3 \%), outdated information (2 \%), irrelevant sources (2 \%),"


%% need to highlight key differences between kangur et al 2024 and ours -- they look at how bias/factuality impact helpfulness, not the type of source themselves. They identify far fewer fact-checking URLs than we do, and miss (i) news sources that do specific fact-checking e.g. ap news, USAToday, and (ii) how much the news sources themselves rely on fact-checking articles. -- need to point out the lack of visibility fact-checkers have, which we try to bring out.
% \cite{kangur_who_2024}: "majority of cited sources are news outlets that are left-leaning (left or centre left) and are of high factuality, pointing to a potential bias in the platform's community fact-checking"
% "notes with Right-wing sources are used relatively more when supporting tweets compared to notes that use Center or Left-wing sources"
% "Left biased and low factuality sources validate tweets more, while Center sources are used more often to refute tweet content."
% "source factuality significantly influences public agreement and helpfulness of the notes, highlighting the effectiveness of the Community Notes Ranking algorithm."
% tweets and Wikipedia are most cited
% 55% of links are news, 18% research, 9% social media, 9% encyclopedia
% found that only 1.2% of links were to fact-checking orgs? either there has been a drastic change or we are using v different lists
% "Left and Right sources are associated relatively more frequently with helpful than not helpful notes compared to Center sources. This might be due to people having a confirmation bias when looking at content confirming their views"
%"notes that hold low factuality sources are generally rated less helpful than those with medium or high factuality sources"
%"community notes citing more neutral or factually sound sources receive higher agreement levels"

% \cite{pilarski_community_2024}: community notes tend to fact-check posts from larger accounts and are less likely to focus on accurate/true claims than snopes (i.e., replying to tweets with links to fact-checking articles).
% Community notes are slower than snoping. -- but that's because the fact checking article must already exist for snoping to happen.
% Also found little overlap between targets of CNs and snoping, but high agreement when they do.
% \cite{bovermann_putting_2024}

% \cite{de_supernotes_2024}: using an LLM to generate many diverse Supernote candidates from existing proposed notes. compared to best existing note, participants rated the Supernotes as significantly more helpful, and when asked to choose between the two, preferred the Supernotes 75.2\% of the time, rated supernotes as better on quality, clarity, coverage, context, and argumentativeness. also rated the supernotes as better than LLM generated summaries

%Both internal pilot experiments and external analyses find that community notes have a significant impact on users’ engagement with and perceptions of misinformation [6, 7, 35, 42].

% \cite{yoon_collaborative_2025}: interviewed 8 ppl who do community moderation -- Reddit, Discord, and X (formerly Twitter). "common practices included sharing sources and citations, providing emotional support, giving others advice, and signaling positive feedback"
% % 5 interviewed from twitter

%basic analysis of dataset and tweets showing they focus on current events, not v interesting
% \cite{jones2022helpfulnotes}: 
% Although Birdwatch is fairly new, researchers have investigated dynamics on the platform such as partisanship [1], vulnerabilities of the system [3 ], and the efficacy of fact checking [ 15]. This work

% \cite{righes_community_2023}: demo to show how crowdsourced fact-checking works in practice when compared with human experts.
% both experts and community notes tend to focus on similar claims for selection
% evidence retrieval: most highly rated notes link to high quality sources, but some low rated notes also link to high qual sources - shows the partisan nature of community notes
% Community Notes participants use only 17 domains in common to those of the ClaimReview experts
% On average, a Community Notes provides a response 10X faster than an expert, if the tweet appears before the fact check -- but majority of the matched tweets follow the CTB pattern (fact check then tweet then note)

% \cite{phillips_emotional_2024}
% \cite{wang_empowered_2024}




%% new paper on political bias in community notes: https://osf.io/preprints/psyarxiv/vk5yj_v2

%% what happens if you get rid of fact-checking: https://factcheckhub.com/what-happens-if-you-get-rid-of-fact-checking/ 

%Epstein et al. [13] deployed a survey to 1000 Americans to study their perceived trust in popular news websites, finding that mainstream sources are usually more trusted than fact-checking websites or hyper-partisan sources.

% : community notes more trustworthy than all flags (duh). expert flag more trustworthy than community flag -- providing context is the important thing.
% %presented n = 1,810 Americans with 36 misleading and non-misleading social media posts and assessed their trust in different types of fact-checking intervention
% "across both sides of the political spectrum, community notes were perceived as significantly more trustworthy than simple misinformation flags. Our results further suggest that the higher trustworthiness primarily stemmed from the context provided in community notes (i.e. fact-checking explanations) rather than generally higher trust towards community fact-checkers. Community notes also improved the identification of misleading posts"

% Mixed evidence on effectiveness: %% papers by the same author claiming opposite results -- really weird and not sure what to make of it
% : "\cite{chuai_community_2024}
% \cite{chuai2024did}: no effect of community notes on engagement with misinformation
% The half-life of a tweet is short: about 95% of tweets have no relevant impressions after two days, and it only takes about 79.5 minutes before half of impressions are created for a tweet [ 51].

% data from Jan 2021 - Feb 2022 (20,000 notes)
% \cite{drolsbach_diffusion_2023}: "Community fact-checked misinformation is less viral. Specifically, misleading posts are estimated to receive 37\% fewer retweets than not misleading posts". community fact-checkers tend to factcheck posts from influential user accounts with many followers, while expert fact-checks tend to target posts that are shared by less influential users.
% differences between "community fact checking and expert fact-checking: "act-checks from Birdwatch contributors are more likely to endorse/emphasize the accuracy of not misleading tweets authored by influential users with a wide reach. Opposite to this, expert fact-checked tweets authored by influential accounts are more likely to convey false information."
% do not look at the content of the 'community fact-checking' notes.
%most community noted tweets are misleading
% most categorized as misleading because of factual errors (62.13 %), missing context (61.38 %), or because they treat unverified claims as fact (49.99 %). The other categories are relatively rare.
% significant differences in virality across different sub-types of misinformation
% when they talk about expert fact-checkers they compare to vosoughi et al 2018 --- weird test to perform considering diff time period 


% \cite{drolsbach2024community}: community notes more trustworthy than all flags (duh). expert flag more trustworthy than community flag -- providing context is the important thing.
% %presented n = 1,810 Americans with 36 misleading and non-misleading social media posts and assessed their trust in different types of fact-checking intervention
% "across both sides of the political spectrum, community notes were perceived as significantly more trustworthy than simple misinformation flags. Our results further suggest that the higher trustworthiness primarily stemmed from the context provided in community notes (i.e. fact-checking explanations) rather than generally higher trust towards community fact-checkers. Community notes also improved the identification of misleading posts"

% \cite{renault_collaboratively_2024}: community notes decrease number of retweets a tweet gets by almost half, also decrease number of quote tweets and replies.
% CNs also increase probability of a creator deleting their tweet (by 80\%).
% However, "overall impact on the spread of false or misleading tweets is much more modest (-16\% to -21\%) due to the publication delay of the Notes" -- mean delay between publication of misleading tweet and note is 15.5 hours -- quicker the intervention, the greater the effect.
% Only 11.3\% of notes reach CRH (currently rated helpful) status.

% \cite{wirtschafter2023future}: just 20\% of contributors have written helpful notes, number of  “helpful” notes attached to tweets as additional context has nearly tripled since the program's expansion, from 4\% to approximately 12\% of all written notes"
% % "among the tweets with additional context attached to them that were still online as of mid-March 2023, almost one out of every five (most common) contained doctored or mislead-ing images, videos, and quotes"
% Community Notes do not seem to impact the subsequent behaviour of Twitter users; tweets that received notes do not decline in engagement; instead, the users' average followers and engagement increase.
% %and on the day, a user receives a note on one of their tweets, average likes are higher than the days before and after... in addition to increased engagement, Twitter users who received a Community Note attached to one of their tweets also saw a boost in followers." % mostly for smaller accounts, no effect for users over 5 mil followers
% it is possible that Community Notes target viral tweets fromrelatively small accounts, putting the content moderation program at odds with Twitter’sown own recommender system. Although tweets that received a content warning from Twitterhave historically been deamplified, it is unclear what role Community Notes plays inmitigating tweet virality. Given that tweets that receive Community Notes are no lesslikely to be amplified, in some cases, they might even exacerbate this virality by drawingmore attention to a misleading tweet (Twitter2023a)

% \cite{kankham_community_2024}: compared community notes to related article suggestions.
% % has a good table comparing differences between community notes and news articles -- CNs are direct rebuttals, articles and indirect. Cns very in tone and style and may use various persuasion techniques, vs articles which have a journalistic style and rely only on credible articles and sources. CNs have direct engagement between users, articles are one-way communication.
% Both interventions reduced people's intentions to believe and repost vs a control.
% For wish (positive) rumours, people shown community notes were less likely to believe them and share them than people shown related articles. 
% But for dread (negative) rumours, related articles were more effective in reducing self-reported belief and likelihood of sharing.
% limitation: findings only based on one example health-related claim.

% %this study not great, not sure if we want to include:
% \cite{chuai_community_2024-1}: displaying community notes leads users to post more negative, angry, disgusted replies to misleading posts -- moral outrage %% unclear to me if the negativity is targeted at the post itself or the community note

% \cite{draws_effects_2022}: crowdworkers generally overestimate truthfulness, bias: the more workers like the claimant of a statement, the more they overestimate the statement’s truthfulness (and vice versa), overconfidence: the higher workers’ self-reported confidence in their ability to judge the truthfulness of statements, the less accurate their judgments generally were
% %Epstein et al. [13] deployed a survey to 1000 Americans to study their perceived trust in popular news websites, finding that mainstream sources are usually more trusted than fact-checking websites or hyper-partisan sources.