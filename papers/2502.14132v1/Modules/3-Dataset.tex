\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{Figures/link_types_tilt.png}
    \caption{The categories of links used by Community notes' authors as a source.}
    \label{fig:link_types}
\end{figure*}

We download files containing all community notes and their metadata from the official website,\footnote{\url{https://communitynotes.x.com/guide/en/under-the-hood/download-data}} which amounts to 1.5M notes authored between January 28th 2021 and January 6th 2025. Of these, a total of 135K are rated by the community as `Helpful', 51K are rated `Not helpful', and 1.3M are unpublished, i.e., did not receive enough community ratings to reach a verdict. See \cref{fig:notes_per_month} in \cref{app:additional_material} for statistics.

We filter the notes as follows. First, we remove 526K non-English notes, which we identify by applying the language detection library fast-langdetect.\footnote{\url{https://github.com/LlmKira/fast-langdetect}} Then, we further filter 268K ``unnecessary'' notes---notes attached to tweets that are classified by the community as ``not misleading''. Finally, to focus only on notes that are used to address misinformation, we filter out 44K notes that contain one of the words ``ad'', ``spam'', or ``phishing''. Following these filtration steps, we are left with a dataset containing 664K notes.

The next step involves categorising the sources that the note authors use to support their claims. First, we use regex to extract all the URLs found in the notes. See \cref{tab:top_domains} in \cref{app:additional_material} for a list of the top-100 most common domains. We classify each URL in our dataset of 664K notes into one of 13 categories (detailed in \cref{fig:link_types}) using the pipeline described below.

\begin{enumerate}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex] % I don't know if this is allowed
    \item Check whether the domain name of the URL is found in a manually curated list of domains of professional fact-checking organisations (See \cref{tab:fact_check_orgs} in \cref{app:additional_material} for the full list). If so, classify the URL as ``fact-checking''.
    \item Otherwise, search for paraphrases of the word ``fact-check'' in the URL,\footnote{These URLs mostly link to the fact-checking divisions of news outlets, e.g., \url{https://apnews.com/article/fact-checking-909101991741}} and classify it as ``fact-checking'' if a match was found.
    \item Otherwise, check whether the domain name is found in \cref{tab:top_domains}, which the authors of this paper manually annotated.
    \item Otherwise, use GPT-4\footnote{Version \texttt{gpt-4o-2024-08-06}.} to classify the domain name into one of the 13 categories. \Cref{lst:prompt_link} in \cref{app:reproducibility} details the prompt we used. 
    \item Finally, if  GPT-4 fails or outputs an unknown category, label the URL as ``unknown''.
\end{enumerate}

\noindent Using this pipeline, we successfully classify 95\% of the URLs to one of the 13 categories. 


Moreover, we further subsample the notes for performing the in-depth analysis required for answering RQ2 (\cref{sec:analysis_rq2}). From the notes rated as `Helpful', we sample 3.5K notes with a ``Fact-checking'' source and a random sample of 22K additional notes. We then used web crawling to scrape the text of the posts to which these notes were attached. We name this subset $\mathcal{S}_\text{text}$ for simplicity.




