\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{Figures/neural_hamiltonian_architecture.pdf}
    \label{fig:model-blueprint}
    \caption{Architecture of the Neural Hamiltonian (NH). First, the discrete CPM input undergoes a pixel-wise one-hot encoding. Then, $L$ iterations of NH layers are applied to extract a deep representation of the system that is equivariant to translations and permutations of cell indices.
    Finally, the extracted representation is pooled globally over the spatial and cell axes, yielding an invariant global representation of the system which is processed by a multi-layer perceptron to compute the Hamiltonian value.}
    \label{fig:neural-hamiltonian}
\end{figure*}


\section{Neural cellular Potts models}\label{sec:method}

\subsection{Neural Hamiltonian architecture}

Our goal is to learn a Neural Hamiltonian $H_\theta$ that parameterizes a CPM of which the stochastic dynamics align with the complex distribution over real cellular behavior. A challenge in modeling physical systems is that they often admit multiple equivalent representations, a property formalized through mathematical symmetries. Incorporating these symmetries in neural network architectures can greatly enhance model performance~\cite{bronstein2021geometric}. In the context of CPMs, symmetries arise in Hamiltonians that are \emph{invariant} to permutations of the set of cells $C$ and translations of the lattice $L$. As such, for any transformation $g$ that permutes $C$ and translates $L$, the Neural Hamiltonian should satisfy $H_\theta(x^t) = H_\theta(gx^t)$. 





Generally, GNNs~\cite{Gilmer2017mpgnn, kipf2017gnn}, Deep Set models~\cite{zaheer2017deepset}, or transformers~\cite{vaswani2017transformer} would be the designated building blocks for architectures that respect permutation symmetry. However, in the CPM context, such architectures do not apply, as they operate on node features represented by vectors, whereas in our case $x^t$ represents a regular lattice. Instead, we propose a Neural Hamiltonian architecture that is invariant to both translations and permutations, illustrated in Figure~\ref{fig:neural-hamiltonian}. The global structure of this architecture is as follows: first, $x_l^t$ is one-hot encoded, such that for each cell $c \in C$, we now have a separate grid $h^0_c$ representing that cell. $(h_{c}^0)_l$ equals 1 if $c$ occupies $l$, and 0 otherwise. Then, we iteratively process the embeddings $\left\{h^l_c\right\}_{c=0}^{|C|}$ with the $l$'th NH layer to produce a deep equivariant representation of the system. Each NH layer processes their input cell embeddings by passing each cell's embedding independently through a neural network $\phi^l$. $\phi^l$'s outputs $h'_c$ are then aggregated using a permutation-invariant function $\bigoplus$, in our case summation, to yield a global context lattice $A$ of the entire system. Finally, all $h^l_c$ are processed in tandem with $A$ by the cell-interaction CNN $\psi^l$, after which local spatial pooling, for example max-pooling, can be applied to compress the representation of the system. To respect the translation symmetry of the problem and promote localized pattern recognition, we parameterize $\phi^l$ and $\psi^l$ with convolutional neural networks (CNNs). After processing by the NH layers, we pool the embeddings to obtain an invariant representation before processing them with a shallow multi-layer perceptron to compute the Hamiltonian $H_\theta(x^t)$. 











\paragraph{Biology-informed Hamiltonians.} 
A key advantage of NeuralCPM is that it closely follows the cellular Potts modeling paradigm, which enables us to seamlessly integrate biological domain knowledge. Specifically, even though symbolic Hamiltonians are approximations, they may still account for partially known mechanisms underlying the observed dynamics. In this case, we wish to exploit this domain knowledge to expedite the model fitting task and to yield a more interpretable model. We achieve this by using the Neural Hamiltonian as a \emph{closure model} on top of an interpretable symbolic Hamiltonian:%
\begin{equation}\label{eq:closure-model}
    H_\theta(x) = w_S \cdot H_{\theta^S}(x) + w_{NN} \cdot H_{\theta^{NN}}(x),
\end{equation}
where $H_{\theta^S}(x)$ is the analytical component and $H_{\theta^{NN}}(x)$ is the Neural Hamiltonian component, and $w_S$ and $w_{NN}$ are learned weights to balance the contributions of both components. If the parameters $\theta^S$ of the symbolic component are not known, $H_\theta(x)$ can be trained end-to-end as long as $H_{\theta^S}(x)$ is differentiable with respect to $\theta^S$. Through its biology-informed structure, $H_{\theta^S}(x)$ expresses a strong prior on the overall Hamiltonian $H_\theta$, while $H_{\theta^{NN}}(x)$ is responsible for expressing higher-order terms that cannot be accounted for by $H_{\theta^S}(x)$. In this work, we consider the volume constraint and the interaction energies of Equation~\ref{eq:hamiltonian-glazier} as symbolic components; depending on the available knowledge of the biological mechanisms in the system at hand, more sophisticated expressions can be included.









\subsection{Training}
Our training strategy is to minimize the negative log-likelihood objective (Equation~\ref{eq:maxlikelihoodebm}) using gradient descent (Equation~\ref{eq:gradientebm}). 
Given a dataset $\mathcal{D}$ of observed cellular systems, we estimate the expectation over $p^*(x)$ in Equation~\ref{eq:gradientebm} with a batch of $B$ datapoints $\{x^+_b\}^B_{b=1}$, sampled uniformly from $\mathcal{D}$. The expectation over $p_\theta(x)$ in Equation~\ref{eq:gradientebm} is estimated with a batch of samples $\{x^-_b\}^B_{b=1}$ obtained using $B$ independent MCMC chains. Inspired by \cite{du2019implicit}, we also add a regularization term weighted by a small scalar $\lambda$ to the objective to regularize energy magnitudes of $H_\theta(x)$ and improve numerical stability. This gives the loss estimate:
\begin{multline}\label{eq:lossfn}
    \hat{\mathcal{L}}(\theta) = \frac{1}{B}\sum^B_{b=1} H_\theta(x^+_b) - H_\theta(x^-_b)\\ + \lambda \left(H_\theta(x^+_b)^2 +  H_\theta(x^-_b)^2\right) 
\end{multline}
The computational complexity of training is dominated by running the MCMC sampler to obtain samples from $p_\theta(x)$. Each MCMC step requires a forward pass of $H_\theta(x)$ and typically many thousands of MCMC steps are required before the chain has converged to an equilibrium sample of $p_\theta(x)$. Therefore, we introduce an approximate sampler that accelerates the original CPM sampler by performing multiple spin-flip attempts in parallel.
More details about the training algorithm, the approximate sampler and implementation can be found in Appendix~\ref{sec:appendiximplementation}.


