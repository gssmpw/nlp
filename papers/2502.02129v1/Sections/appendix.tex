\section{Data generation}\label{sec:app-data-gen}

\paragraph{Cell sorting}
Training data for fitting an analytical cell sorting Hamiltonian was generated by sampling states with the energy function given in equation (\ref{eq:hamiltonian-glazier}), where in this case $H_\text{case-specific} = 0$. Inspired by the original proposal of the CPM by \citet{Graner1992}, we consider cells of two distinct cell types that perform cell sorting due to differential adhesion. We distinguish the two scenarios a and b from \citet{edelstein2023simplecellsort}, characterized by different contact energies between cells, which are laid out in tables \ref{tbl:contact_energies_scenario_a} and \ref{tbl:contact_energies_scenario_b}. The Lagrange multiplier for scenario a was set to $\lambda_\mathrm{V} = 0.1$ while for scenario b $\lambda_\mathrm{V} = 0.5$ was chosen. Both sets of simulations were performed with a target volume of $V^* = 60$ and temperature $T=1$ on a $100 \times 100$ grid with 50 cells, 25 of each type, which were initialized as single pixels randomly scattered within a centered circle with a radius of 25 lattice sites. The resulting datasets comprised 128 independent full lattice snapshots each. 

\begin{table}[h]
\begin{minipage}{0.5\linewidth}
\centering
\caption{Contact energies for scenario a}
\vskip 0.15in
\label{tbl:contact_energies_scenario_a}

\begin{tabular}{|*{4}{c|}}
    \cline{1-2}
    Medium & 0.0 \\ \cline{1-3}
    Type 1 & 0.5 & 0.333333 \\ \hline
    Type 2 & 0.5 & 0.2 & 0.266667 \\ \hline
           & Medium & Type 1 & Type 2 \\ \hline
\end{tabular}
\end{minipage}
\begin{minipage}{.5\linewidth}
\centering
\caption{Contact energies for scenario b}
\vskip 0.15in
\label{tbl:contact_energies_scenario_b}

\begin{tabular}{|*{4}{c|}}
    \cline{1-2}
    Medium & 0.0 \\ \cline{1-3}
    Type 1 & 2.5 & 1.0 \\ \hline
    Type 2 & 1.0 & 4.5 & 1.0 \\ \hline
           & Medium & Type 1 & Type 2 \\ \hline
\end{tabular}
\end{minipage}
\end{table}

\paragraph{Cellular MNIST}

Ground truth data for the synthetic structural assembly experiment in section \ref{sec:exp1} was generated in a similar way to the cell sorting data described above. Notably, $H_\text{case-specific}(x)$ now took the form of an external potential
\begin{equation}\label{eq:app-H-external-potential}
    H_\text{case-specific}(x) = \sum_{i \in L} \mu(x_i) \phi_i
\end{equation}

where $\mu(x_i)$ can be considered the coupling strength to the potential $\phi_i$. The coupling $\mu(x_i)$ was then defined so that only cells of type 2 couple to $\phi_i$ with constant strength $\bar{\mu} = 10$, while $\phi_i$ was chosen such that cells of type 2 favor arrangements that mimic handwritten digits from the MNIST data set \cite{deng2012mnist}. To that end, the MNIST images were binarized by applying a threshold at half brightness. Subsequently, a Euclidean distance transform was performed. Finally, the image was scaled with cubic interpolation from the input resolution $28 \times 28$ to the chosen domain size of $100 \times 100$ pixels. The result was used as $\phi_i$ in the Hamiltonian. The distance transform yields a sloped potential which pushes the type 2 cells into the shape of the desired digit. Differential adhesion similar to the cell sorting case was imposed to better separate cells of different types; see table~\ref{tbl:contact_energies_mnist}. In addition to these parameters, we set $\lambda_{V} \approx 0.974$, $V^* = 100$, and $T=1$ and initialized the system with the same procedure as above. The final data set contained 1280 samples.

\begin{table}[h]
\centering
\caption{Contact energies for cellular MNIST}
\label{tbl:contact_energies_mnist}

\begin{tabular}{|*{4}{c|}}
    \cline{1-2}
    Medium & 0.0 \\ \cline{1-3}
    Type 1 & 6.0 & 3.0 \\ \hline
    Type 2 & 6.0 & 6.0 & 3.0 \\ \hline
           & Medium & Type 1 & Type 2 \\ \hline
\end{tabular}
\end{table}



\paragraph{Bi-polar axial organization}


The experimental cell aggregates of~\citet{Toda2018Science} consist of 200 to 240 cells in 3D which amounts to about 8 cells along a diameter and about 40 cells in the cross-section. To generate the synthetic training data, we therefore consider 40 interacting cells, 20 of each type. We use Morpheus~\cite{starruss2014morpheus} to randomly initialize a cluster of these 40 cells, after which we assign each cell of type two a preferred motion in the direction of one of the two poles. This results in artificially creating configurations where each cell of type 2 has clustered together in the pole it was assigned to move to. In addition, the standard cell sorting Hamiltonian applied, with $V^*(c) = 150$, $\lambda=1$, and contact energies as shown in Table~\ref{tbl:contact_energies_toda}. In addition, we set Morpheus' temperature parameter for this experiment to $T=2.0$. Using this procedure, we generated 1000 samples, which we randomly rotate for training.

\begin{table}[h]
\centering
\caption{Contact energies for synthetic bi-polar axial organization}
\label{tbl:contact_energies_toda}

\begin{tabular}{|*{4}{c|}}
    \cline{1-2}
    Medium & 0.0 \\ \cline{1-3}
    Type 1 & 16.0 & 6.0 \\ \hline
    Type 2 & 16.0 & 16.0 & 6.0 \\ \hline
           & Medium & Type 1 & Type 2 \\ \hline
\end{tabular}
\end{table}















\section{Implementation details}\label{sec:appendiximplementation}
Our implementation is built on JAX~\cite{jax2018github} and Equinox~\cite{kidger2021equinox}.

\subsection{Training and sampling}

\paragraph{Training loop}
A pseudocode description of the training loop is given in Algorithm \ref{alg:training}. We initialize the MCMC chains from datapoints, where we randomly permute the type of each cell. We use persistent chains instead of reinitializing in each iteration, following the Persistent Contrastive Divergence algorithm from~\cite{tieleman2008training}, but with the regularization term from~\cite{du2019implicit}. While using an approximation of the gradient of the max likelihood objective, this approach reduces the amount of compute per optimization step since less MCMC steps and thus less forward passes of $H_\theta(x)$ have to be performed. Note that the autodifferentiation step does not backpropagate through the sampling chain.
\begin{algorithm}[h]
   \caption{NeuralCPM training procedure}
   \label{alg:training}
\begin{algorithmic}
   \STATE {\bfseries Input:} dataset $\mathcal{D} = \{x_n\}^N_{n=1}\overset{i.i.d}{\sim}p^*(x)$, learning rate $\eta$, number of sampling steps $K$, number of parallel flips $P$, sampler reset probability $p$
   \hspace{4mm}\\
   \STATE Initialize $K$ sampling chains $\{x_b^-\}^B_{b=1} \sim \mathcal{D}$
   \STATE Initialize model $\theta$
    \WHILE{not converged}
    \STATE $\{x_b^+\}^B_{b=1} \sim \mathcal{D}$
    \STATE For each $x_b^-$, With $p$\% reinitialize $x_b^- \sim \mathcal{D}$
    \STATE $\{x_b^-\}^B_{b=1} \leftarrow \text{ApproxPCPM}(H_\theta, K, P, \{x_b^-\}^B_{b=1})$
    \STATE $\mathbf{g} \leftarrow \text{autodiff}(\hat{L}(\theta))$ (eq. \ref{eq:lossfn})
    \STATE $\theta \leftarrow$ Adam($\eta, \theta, \mathbf{g}$)
    \ENDWHILE
    \STATE return $\theta$
\end{algorithmic}
\end{algorithm}
Common hyperparameters are given in Table \ref{tbl:hyperparameters}. We used the Adam optimizer in all experiments with learning rate $\eta=1e-3$ and standard hyperparameters $\beta_1=0.9, \beta_2=0.999, \epsilon=1e-8$. The number of sampling steps per model update $K$ is determined in units of 'Monte Carlo Sweeps', i.e the total size of the lattice $|L|$ (aka the grid size), as is common in the CPM literature. Thus, $K = |L| * \text{Monte Carlo sweeps}$.
\begin{table}[h]
\centering
\caption{Training hyperparameters used in results of their respective section}
\vskip 0.15in
\label{tbl:hyperparameters}
\begin{tabular}{|l|l|l|l|} \hline
\textbf{Hyperparameter}                     & \ref{sec:exp0}     & \ref{sec:exp1}     & \ref{sec:exp2}     \\ \hline
Batch size $B$                     & 16      & 16      & 16      \\\hline
Num training steps                 & 1e4     & 1e4     & 1e4     \\\hline
Monte Carlo sweeps                 & 1.0     & 0.5     & 0.7     \\\hline
Lattice size                          & 200x200 & 100x100 & 125x125 \\\hline
EWA $\alpha$          & 0.0     & 0.99    & 0.99    \\\hline
Regularizer $\lambda$ & 0.0     & 0.0005  & 0.0005  \\\hline
Num parallel flips $P$                 & 100     & 50      & 50     \\ \hline
Sampler reset probability $p_{reset}$ & 100\% & 2.5\% & 2.5\% \\ \hline
\end{tabular}
\end{table}

\paragraph{Approximate sampler}
Estimating the loss function using an MCMC sampler that mixes quickly is imperative for scalable training, since each MCMC step requires a forward pass of $H_\theta(x)$ in the Metropolis-Hastings (MH) correction step. 
The original CPM sampler uses a proposal distribution that perturbs only one lattice site $l \in L$ at a time, resulting in a slow mixing rate and thus a very expensive training loop \cite{Graner1992}. We instead use approximate parallelized CPM dynamics with a proposal distribution that samples multiple sites and changes their cell state in parallel.

Pseudocode for the approximate CPM dynamics is given in Algorithm~\ref{alg:sampler}. Let $\mathcal{N}(l)$ define the set of all neighboring lattice sites of lattice site $l$. The sampler uses a proposal distribution where first, $P$ lattice sites are independently and uniformly sampled from the \textbf{boundary} of cells. This boundary subset $\mathcal{B}$ contains all $l \in L$ that have a neighboring lattice site that belongs to a different cell than the cell at the lattice site in question. Such structure in the proposal causes state updates where the system (and system energy) actually changes, increasing the convergence speed of the sampler towards more meaningful configurations. After sampling a set of lattices $\mathcal{S}$ from the boundary $\mathcal{B}$, we sample for each site $i \in \mathcal{S}$ a lattice site that is mapped to a different cell in $C$, a set we denote as $\mathcal{M}(i) = \{j \in \mathcal{N}(i)|x^t_i \neq x_j^t\}$. Then, we copy that neighbouring cell into the originally sampled lattice site and \textbf{perform an MH correction step for each site in $\mathcal{S}$ in parallel}. That is, we perform $P$ MH correction steps in parallel on states where only one lattice site has been permuted. Then, we keep all the permutations that were accepted and combine them together into the next system state. The reason for performing an MH correction step for each permutation individually is that 

The resulting transition probabilities do not satisfy detailed balance because we essentially use a faulty MH correction step, and thus we cannot guarantee that the system has a stationary distribution defined by $H_\theta(x)$. Nevertheless, we found in preliminary experiments that this custom approximate sampler achieved speedups over even state-of-the-art discrete MCMC samplers such as~\cite{grathwohl2021oops, zhang2022langevin, sun2023discrete}. We believe this is because the custom proposal is able to leverage structure (the boundary constraint) in its proposal that is specific to the cellular dynamics problem, and that the gradient approximations of these discrete systems as used in state-of-the-art samplers were rather poor, likely due to the very unsmooth nature of the neural Hamiltonian $H_\theta(x)$.
\begin{algorithm}[tb]
   \caption{ApproxPCPM}
   \label{alg:sampler}
\begin{algorithmic}
   \STATE {\bfseries Input:} Hamiltonian $H_\theta$, number of sampling steps $\tau$, number of parallel flips $P$, initial state $x^0$
    \FOR{$t$ in 1 to $\tau$}
    \STATE $x' \leftarrow x^{t-1}$
    \STATE $\mathcal{B}^t \leftarrow \{l \in L| \exists j \in \mathcal{N}(l): x^t_l \neq x^t_j \}$
    \STATE $\mathcal{S}^t = \{l_p\}^P_{p=1}\overset{i.i.d}{\sim}\mathcal{B}^t$
    \FOR{lattice site $i \in \mathcal{S}^t$ in parallel}
    \STATE $x'_i \leftarrow x^{t-1}_{j\sim \mathcal{M}(i)}$
    \STATE $\Delta \leftarrow H_\theta(x') - H_\theta(x^{t-1})$
    \STATE $p_i \leftarrow \min(1, e^{-\Delta})$
    \STATE $u_i \sim U(0, 1)$
    \STATE $x_i^t \leftarrow x'_i$ if $u_i \leq p_i$
    \ENDFOR
    \ENDFOR
    \STATE Return $x^\tau$
\end{algorithmic}
\end{algorithm}
\subsection{Model details and hyperparameters}\label{sec:app-all-model-details}
Here, we discuss details on the (hyper)parameters of the various Hamiltonian models used in our experiments.

\subsubsection{Analytical Hamiltonians}\label{sec:app-details-analytic-ham}
The cell sorting Hamiltonian is exactly as described in Equation~\ref{eq:hamiltonian-glazier}, where the parameters $J(c_1, c_2)$ and $\lambda$ are learnable; we set $V^*(c)$ to the average volume of all cells observed in the training data for all $c \in C$. The cell sorting model with external potential is the same as the cell sorting Hamiltonian, with the addition of an external potential as described in Equation~\ref{eq:app-H-external-potential}. However, rather than setting those parameters up-front as done in the data generation (Appendix~\ref{sec:app-data-gen}), we learn the parameters through stochastic gradient descent.


\subsubsection{Neural Hamiltonians}

\paragraph{Initial embedding layer.}
We first compress the sparse one-hot encoded representation to a dense representation by applying a learned downsampling through a single linear strided convolutional layer that operates on each cell independently. The stride and kernel size of this layer are the same and equal $3 \times 3$ and $5 \times 5$ for Cellular MNIST and bi-polar axial sorting respectively.

\paragraph{NH layers.}\label{sec:app-NH-design-details} For simplicity, we choose a fixed architecture for $\phi^l$ and $\psi^l$ throughout this work: both are two repetitions of $\{\text{Conv2D} \rightarrow \sigma\}$, where Conv2D is a convolution with a kernel size of 3 and $\sigma$ is the SiLU activation function~\cite{Elfwing2018silu}. We use summation as  permutation-invariant aggregation function $\bigoplus$. The hidden dimension (amount of channels) per cell for each NH layer increases progressively with the depth of the Neural Hamiltonian model, where the first Conv2D layer in both $\psi^l$ and $\phi^l$ maps the input to an output with \texttt{out channels} equal to this hidden dimension, and where subsequent Conv2D layers preserve the amount of channels within each NH layer:

\begin{align}
    \{h^l_c \in \mathbb{R}^{\texttt{in channels} \times h \times w}\} \rightarrow \phi^l &\rightarrow \{h'_c \in \mathbb{R}^{\texttt{out channels} \times h \times w|}\}\\
        \{h^l_c \in \mathbb{R}^{\texttt{in channels} \times h \times w}, A \in \mathbb{R}^{\texttt{out channels} \times h \times w|} \} \rightarrow \psi^l &\rightarrow \{o_c \in \mathbb{R}^{\texttt{out channels} \times h \times w|}\}
\end{align}

Additionally, we use a residual connection~\cite{He2016resnet} for each NH layer, connecting its input $h_c^l$ with its output $o_c$ before max-pooling.

The specific hidden dimensions and pooling design for the Neural Hamiltonians is then as follows:

\begin{itemize}
    \item Cellular MNIST: 4 NH layers
    \begin{itemize}
        \item Hidden dimensions: [8, 16, 32, 32]
        \item Max-pooling downsampling rates: [3, 2, 1, 1]
    \end{itemize}
    \item Bi-polar axial sorting: 6 NH layers
    \begin{itemize}
        \item Hidden dimensions: [16, 32, 32, 64, 64, 64]
        \item Max-pooling downsampling rates: [2, 1, 2, 1, 2, 1]
    \end{itemize}
\end{itemize}

We then apply a linear convolution with 32 output channels, before pooling over all cells and pixels using summation to get a vector representation of the system that is invariant to both permutations and translations. This is then processed by an MLP with two hidden layers with 32 SiLU nonlinearity and 32 neurons each before mapping to a scalar output with a single linear layer. As for the NH layers, the MLP layers model the residual with respect to the input.

For the 1-layer Neural Hamiltonian baseline, only the first of these NH layer is applied before pooling over the representation of all cells using pixel-wise summation.

\paragraph{NH-based baseline models.}

The Neural Hamiltonian+closure model uses the neural network to model an additive term on top of the cell sorting Hamiltonian (Equation~\ref{eq:hamiltonian-glazier}), where we take the same approach as in Section~\ref{sec:app-details-analytic-ham} for the analytical component.

The 1-layer NH + CNN model uses the first layer of the respective Neural Hamiltonians as described earlier in this section, before pooling over all cells but not over all pixels. This yields a grid representation of the system that is invariant to permutations, which is subsequently processed by a CNN architecture consisting of blocks of two $\{\text{Conv2D} \rightarrow \sigma\}$ repetitions, and a residual connection between the input and output of each block. The first layer of the convolution block maps the input to the specified hidden dimension, which remains the same for the second layer. We again apply max-pooling to the output of each block to get a compressed representation of the system. The specific desings are as follows:

\begin{itemize}
    \item Cellular MNIST: 3 convolution blocks
    \begin{itemize}
        \item Hidden dimensions: [32, 64, 128]
        \item Max-pooling downsampling rates: [1, 2, 2]
    \end{itemize}
    \item Bi-polar axial sorting: 6 convolution blocks
    \begin{itemize}
        \item Hidden dimensions: [32, 64, 64, 128]
        \item Max-pooling downsampling rates: [1, 2, 1, 2]
    \end{itemize}
\end{itemize}

The output is then processed in the exact same way as the output of the NH layers described in the paragraph above, with the exception that we already pooled over all cells before the CNN, and thus only aggregate over all pixels.


\section{Additional results}


\subsection{Fitting analytical Hamiltonians}\label{sec:app-exp-0-more-results}

The convergence plot of the parameters for type A cell sorting (analogous to Figure~\ref{fig:exp0-param-converge}) can be found in Figure~\ref{fig:app-typea-cellsort-converge}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\linewidth]{Figures/experiment_0/exp0_param_convergence_Sce_a.pdf}    
    \caption{Convergence of the parameters for Type A cell sorting. Dashed lines indicate the true values, solid lines indicate the learned values ($T=T^*$) over the course of training. The parameters converge rapidly to the true values.}
    \label{fig:app-typea-cellsort-converge}
\end{figure}


We also experimented with alternative discrete EBM sampling methods to try to fit the analytical Hamiltonian, namely Gibbs-with-Gradients (GWG)\cite{grathwohl2021oops} and standard Gibbs sampling (see e.g.~\cite{barbu2020monte}). However, as demonstrated by Table~\ref{tab:app-exp-0-additional}, these methods were not effective. The reason for this is that these methods are not constrained to perturb the system along the boundaries of the cells, which are the regions of the state that are the most informative to perturb for parameters relating to cell-cell interaction and cell volume. As such, they learn to radically increase the parameter values for the contact energies $J(c_i,c_j)$ to prevent fragmented cells. Still, even after discounting for the scale by fitting an optimal temperature $T=T^*$ (explained in Section~\ref{sec:exp0}), the learned parameters of these baseline methods are not close to the ground-truth parameters. 

\begin{table}[h]
\centering
\small
\setlength{\tabcolsep}{3.5pt}
\caption{$\log_{10}(\text{RMSE})$ of fitted coefficients for the cell sorting Hamiltonian for varying MCMC dynamics.}\label{tab:app-exp-0-additional}
\vskip 0.15in
\begin{tabular}{ccccc}
\toprule
 & \multicolumn{2}{c}{Type A} & \multicolumn{2}{c}{Type B} \\
 & $T=1$ & $T=T^*$ & $T=1$ & $T=T^*$ \\
\midrule
Gibbs sampling & 1.07 & 0.58 & 1.02 & 0.86 \\
Gibbs-with-Gradients & 0.82 & 0.54 & 0.64 & 0.64 \\
CPM sampler & -1.26 & -1.67 & -0.01 & -0.75 \\
\bottomrule
\end{tabular}
\end{table}


\subsection{Additional qualitative results}\label{sec:app-qualitative-all}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.49\linewidth]{Figures/experiment_1/appendix/traj_exp1_0.pdf}
    \includegraphics[width=0.49\linewidth]{Figures/experiment_1/appendix/traj_exp1_1.pdf}\\
        \includegraphics[width=0.49\linewidth]{Figures/experiment_1/appendix/traj_exp1_2.pdf}
    \includegraphics[width=0.49\linewidth]{Figures/experiment_1/appendix/traj_exp1_3.pdf}  
    \caption{Additional qualitative results for Cellular MNIST simulations.}
    \label{fig:exp-1-qualitative-appendix}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.49\linewidth]{Figures/experiment_2/appendix/traj_2_qual_0.pdf}
    \includegraphics[width=0.49\linewidth]{Figures/experiment_2/appendix/traj_2_qual_1.pdf}\\
        \includegraphics[width=0.49\linewidth]{Figures/experiment_2/appendix/traj_2_qual_2.pdf}
    \includegraphics[width=0.49\linewidth]{Figures/experiment_2/appendix/traj_2_qual_3.pdf}    
    \caption{Additional qualitative results for bi-polar axial sorting simulations.}
    \label{fig:exp-2-qualitative-appendix}
\end{figure}
