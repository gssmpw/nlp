\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
% \usepackage{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

% \usepackage{natbib}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{multirow}

\title{Mixture of Attention Yields Accurate Results for Tabular Data}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
    Xuechen Li \\
    Shanghai Warpdrive Technology Co.Ltd \\
    Floor 2, No.57 Boxia Road, Pudong, Shanghai \\
    \texttt{lixuechen@warpdriveai.com.cn} \\
    \And
    Yupeng Li \\
    Shanghai Warpdrive Technology Co.Ltd \\
    Floor 2, No.57 Boxia Road, Pudong, Shanghai \\
    \texttt{liyupeng@warpdriveai.com.cn} \\
    \And
    Jian Liu \thanks{Corresponding author} \\
    Shanghai Warpdrive Technology Co.Ltd \\
    Floor 2, No.57 Boxia Road, Pudong, Shanghai \\
    \texttt{liujian@warpdriveai.com.cn} \\
    \And
    Xiaolin Jin \\
    Shanghai Warpdrive Technology Co.Ltd \\
    Floor 2, No.57 Boxia Road, Pudong, Shanghai \\
    \texttt{jinxiaolin@warpdriveai.com.cn} \\
    \And
    Tian Yang \\
    Shanghai Warpdrive Technology Co.Ltd \\
    Floor 2, No.57 Boxia Road, Pudong, Shanghai \\
    \texttt{yangtian@warpdriveai.com.cn} \\
    \And
    Xin Hu \\
    Shanghai Warpdrive Technology Co.Ltd \\
    Floor 2, No.57 Boxia Road, Pudong, Shanghai \\
    \texttt{huxin@warpdriveai.com.cn} \\
  % David S.~Hippocampus\thanks{Use footnote for providing further information
  %   about author (webpage, alternative address)---\emph{not} for acknowledging
  %   funding agencies.} \\
  % Department of Computer Science\\
  % Cranberry-Lemon University\\
  % Pittsburgh, PA 15213 \\
  % \texttt{hippo@cs.cranberry-lemon.edu} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle


\begin{abstract}
  Tabular data inherently exhibits significant feature heterogeneity, but existing transformer-based methods lack specialized mechanisms to handle this property. To bridge the gap, we propose MAYA, an encoder-decoder transformer-based framework. In the encoder, we design a Mixture of Attention (MOA) that constructs multiple parallel attention branches and averages the features at each branch, effectively fusing heterogeneous features while limiting parameter growth. Additionally, we employ collaborative learning with a dynamic consistency weight constraint to produce more robust representations. In the decoder stage, cross-attention is utilized to seamlessly integrate tabular data with corresponding label features. This dual-attention mechanism effectively captures both intra-instance and inter-instance interactions. We evaluate the proposed method on a wide range of datasets and compare it with other state-of-the-art transformer-based methods. Extensive experiments demonstrate that our model achieves superior performance among transformer-based methods in both tabular classification and regression tasks.
\end{abstract}

\begin{figure}[t]
\centering
{\includegraphics[width=0.483\textwidth]{moa-cropped.pdf}}
\caption{The overview of MOA block. It employs multiple parallel MHA branches to enhance the richness of feature extraction. To maintain a constant size of hidden states, features are averaged at the branch level. During averaging, each branch's features are weighted by its corresponding branch weight, thereby applying a dynamic consistency constraint. This mechanism facilitates collaborative learning among the branches.}
\label{fig: moa}
\end{figure}

\section{Introduction}
As society rapidly advances, tabular data has become one of the most widely used data formats ~\cite{zhou24ftta}.  Although the information contained in tabular data is highly complex and manifold, due to its significant economic value, there has been increasing interest among researchers in analyzing and studying tabular data, leading to the proposal of various solutions~\cite{klambauer2017self,wang2021dcn,Popov2020Neural,bonet2024hyperfast,arik2021tabnet,ye2024modern}. 

Among these methods, inspired by the amazing success of the transformer architecture in NLP and speech, an increasing number of  transformer-like architectures  have been introduced to handle tabular data. AutoInt~\cite{song2019autoint} maps both the numerical and categorical features into the same low-dimensional space. Afterwards, a multi-head self-attention neural network with residual connections is proposed to explicitly model the feature interactions in the low-dimensional space. It can be efffciently
fit on large-scale raw data in an end-to-end fashion. To investigate the effectiveness of embeddings, Huang et al.~\cite{huang2020tabtransformer}
propose transforming the embeddings of categorical features into robust contextual representations, and the performance is comparable to that of tree-based methods~\cite{chen2016xgboost,prokhorenkova2018catboost} on certain datasets. FT-Transformer~\cite{gorishniy2021revisiting}, as a popular transformer-based tabular data model, performs well on multiple datasets and shows the potential for superior performance. Then, ExcelFormer~\cite{chen2024can} inroduces a semi-permeable attention module, which selectively permits more informative features to gather information from less informative ones.
To further improve performance, designing an effective feature interaction module, including intra-instance feature interaction and inter-instance feature interaction~\cite{xu2024bishop,chen2022danets}, has become a very popular way to enhance model competitiveness. Beyond simple feature interactions, arithmetic feature interactions are also commonly used to enrich feature information and improve feature representation~\cite{cheng2024arithmetic}.  

A significant limitation of current transformer architectures lies in their inability to effectively process heterogeneous data~\cite{chen2024can,cheng2024arithmetic,chen2023trompt}. While extracting diverse features represents a promising solution to this problem, simply increasing the number of attention heads in Multi-Head Attention mechanism proves inefficient. This approach leads to an expansion of feature dimensions due to the concatenation operation, consequently increasing the parameters quadratically of subsequent Feed-Forward Network (FFN) layers and compromising computational efficiency. Therefore, there exists a critical need to design a transformer architecture that can effectively extract diverse features from tabular data.

In this work, we propose MAYA (\textbf{M}ixture of \textbf{A}ttention \textbf{Y}ields \textbf{A}ccurate results for tabular data), a novel encoder-decoder transformer framework that achieves rich feature representations while maintaining parameter efficiency. Specifically, in the encoder, we propose an innovative Mixture of Attention (MOA) structure, as illustrated in Fig.\ref{fig: moa}, which implements parallelized independent attention branches within a single encoder block. This architecture enables effective feature fusion while significantly improving feature diversity for tabular data. Drawing inspiration from collaborative mechanisms~\cite{ke2020guided}, the framework employs collaborative policy to weight the outputs of multiple attention branches, ensuring robust and stable feature representation. To further enhance the model's capability, we cascade multiple MOA blocks, enabling the capture of high-level semantic information and significantly improving the encoder's feature representation capacity. The MOA's multi-branch design achieves two key objectives: it extracts diverse features while controlling dimensionality growth through averaging operations, thereby substantially reducing the parameter burden on downstream FFN layers. This design choice stands in contrast to traditional concatenation approaches, which would lead to quadratic parameter growth in subsequent network components. In the decoder, by integrating an inter-instance attention mechanism and label information, the model achieves more effective and dependable results.

We compared MAYA with a variety of SOTA transformer methods for tabular data, and the superior performance of MAYA is evaluated on several public datasets. Furthermore, our ablation studies also confirmed the effectiveness of the MOA structure of MAYA. Finally, we believe that MAYA can become a new transformer baseline method for tabular data. The main contributions of MAYA are:
\begin{itemize} 
    \item We empirically verify on several public datasets that the proposed MOA structure can effectively obtain diverse features to deal with heterogeneous data in tables.
    \item We show that the collaborative mechanism applied in branch weight computation of MOA can improve consistency training for tabular data. 
    \item A novel encoder-decoder transformer-based framework is proposed,  which employs distinct attention mechanisms at different stages to focus on intra-instance and inter-instance relationships, respectively.
\end{itemize}

\section{Related Work}
\subsection{Attention Mechanisms}
The attention mechanism was introduced to improve the performance of the encoder-decoder model for machine translation~\cite{vaswani2017attention}. It reveals the relative importance of each component in a sequence compared to the other components. Usually, self-attention and cross-attention are the most widely used in transformer architecture models. Self-attention captures relationships within a single input sequence, and cross-attention captures relationships between elements of different input sequences.
Somepalli et al.~\cite{somepalli2021saint} introduce self-attention and inter-sample attention mechanisms for the classification of tabular data. They utilize self-attention to focus on individual features within the same data sample and inter-sample attention to represent the relationships between different data samples of the table.  In our approach, we introduce two novel attention mechanisms: the Mixture of Attention (MOA) for self-attention and Inter-instance Attention Incorporating with Labels (IAIL) for cross-attention, to enhance data utilization and feature representation.

\subsection{Collaborative Learning}
Collaborative learning \cite{dillenbourg1999collaborative} refers to methods and environments in which learners work together on a shared task, with each individual relying on and being responsible to others. In pixel-wise tasks, collaborative policy has been used to train different subset networks and to ensemble the reliable pixels predicted by these networks.  As a result, this strategy covers a broad spectrum of pixel-wise tasks without requiring structural adaptation~\cite{ke2020guided}. Here, we use collaborative policy to construct branch weight in MOA block during training, with the weight proportional to the difference between the branch output and the true label. It turns out that this dynamic consistency constraint can provide substantial benefit to enrich feature representations and achieve impressive results for tabular tasks.

\section{Methods}
In this section, we will provide a detailed introduction to the proposed network MAYA. To present the information more clearly, we will adopt a top-down pattern for our description, starting from the holistic view of the network and gradually delving into its basic blocks. 

\subsection{Notations}
In this work, let's consider supervised learning problems on tabular data. A typical dataset with $N$ instances can be denoted as $\mathcal{D}=\{(x_i, y_i)\}_{i=1}^{N}$, where $x_i$ represents the features and $y_i$ the label of the $i$-th instance, respectively. Specifically, $x_i$ contains both numerical features $x_i^{num}$ and categorical features $x_i^{cat}$, that is to say, $x_i=(x_i^{num}, x_i^{cat})$. To be convenient and without loss of generality, we denote an instance with $k$ features as $x_i\in \mathbb{R}^k$. For the label $y_i$, three types of tasks are considered, which are binary classification $y_i\in\{0,1\}$, multi-class classification $y_i\in\{1,\dots,C\}$ and regression $y_i\in\mathbb{R}$.

\subsection{Tokenizer}
MAYA is built upon the transformer-like architecture. Consequently, prior to entering the network, instances need to be processed by a tokenizer, which maps the instances from the original feature space into an embedding space. For an instance $x_i \in \mathbb{R}^k$, we define the $d$-dimension embeddings obtained by tokenizer as $z_i$:
\begin{equation}
    z_i = {\rm Tokenizer}(x_i), z_i\in\mathbb{R}^{k \times d}
\end{equation}

The tokenizer can be implemented in various ways, while in this paper, we primarily utilize the tokenizer of \cite{gorishniy2021revisiting}, which embeds numerical features through element-wise multiplication and categorical features via a lookup table. 

Note that in contrast to \cite{gorishniy2021revisiting}, we observe that incorporating an activation function subsequent to the tokenizer enhances the predictive performance of the network. We attribute this to the manner in which the tokenizer's weights and biases are initialized: specifically, the use of Kaiming initialization facilitates performance improvement when combined with the ReLU activation function. To further investigate this detail, we will conduct ablation study in Section \ref{sec: abla_tokenzier}.

Before the extraction of features, a common practice is to append the embedding of a special token, i.e. [CLS] token, to $z_i$:
\begin{equation}
    z^{0}_i = {\rm Stack}[[CLS], z_i], z^{0}_i \in \mathbb{R}^{(k+1) \times d}
\end{equation}
where the subscript ``0" denotes the initial input to the network, i.e., the input to the first layer of the entire network.

\subsection{Encoder-Decoder Architecture}
Our network employs an encoder-decoder framework in its overall design, as demonstrated in Fig.\ref{fig: arch}. The initial input $z_i^{0}$ undergoes processing by the encoder, resulting in an output $\hat{z}_i$ (i.e. the processed [CLS] token, which we adopt as the representation of an instance). Subsequently, $\hat{z}_i$ is then passed through the decoder to yield $\tilde{z}_i$. Finally, a predictor is employed to generate the ultimate prediction result $p$. The whole procedure can be formulated as follows:
\begin{equation}
    \hat{z}_i = {\rm Encoder}(z_i^{0}), \hat{z}_i \in \mathbb{R}^{d}
    \label{Encoder}
\end{equation}
\begin{equation}
    \tilde{z}_i = {\rm Decoder}(\hat{z}_i), \tilde{z}_i \in \mathbb{R}^{d}
\end{equation}
\begin{equation}
    p = {\rm Predictor}(\tilde{z}_i)
    \label{Predictor}
\end{equation}

\begin{figure*}
\centering
{\includegraphics[width=0.8\textwidth]{arch-cropped.pdf}}
\caption{The overall structure of MAYA. It primarily consists of an encoder and a decoder, where the encoder is comprised of MOA blocks (arranged in the upper row) and the decoder is comprised of IAIL blocks (arranged in the lower row). Prior to entering the encoder, the input undergoes processing by the tokenizer. The output of the decoder is then passed through a Predictor to obtain the final prediction results.}
\label{fig: arch}
\end{figure*}

The predictor is an simple fully-connected layer, and encoder and decoder utilize distinct novel basic blocks, i.e. Mixture of Attention (MOA) block and Inter-instance Attention Incorporated with Labels (IAIL) block, which will be elaborated on in subsequent sections.

\subsection{Encoder}
In the field of tabular data, model performance often varies significantly across different datasets. To achieve consistently strong performance, we recommend two strategies: deep and wide integration. Deep integration involves the traditional cascading of basic blocks. For wide integration, we introduce a novel MOA block, where multiple individual attentions operate in parallel. Therefore, an encoder consisting of $L_e$ MOA blocks can be formulated as:
\begin{equation}
    z_i^{\ell} = {\rm MOA}^{\ell}(z_i^{\ell-1}), \ell \in [1, L_e]
    \label{MOA_io}
\end{equation}

\subsubsection{Mixture of Attention (MOA)}
An important characteristic of tabular data lies in the heterogeneity of its features, which often renders reliance on a single feature extractor insufficient. For the classical transformer, the Multi-Head Attention (MHA) mechanism enables the model to focus on information from different representation subspaces through various heads \cite{vaswani2017attention}. This implies that by increasing the number of heads, we can enhance the transformer's capacity to process diverse features. However, this approach introduces two primary issues: firstly, since the feature subspaces of each head in MHA are concatenated together, increasing the number of heads enlarges the size of attention output hidden states, which substantially increases the number of parameters in the subsequent FFN, leading to higher computational overhead. Secondly, the concatenated features undergo an output projection, which results in feature mixing, thereby diminishing the diversity among multiple head subspaces.

To address these issues, we propose the Mixture of Attention (MOA, as demonstrated in Fig.\ref{fig: moa}). It constructs multiple parallel attention branches based on MHA units and then averages the features at the branch level. This method ensures the diversity of feature extraction while maintaining a constant size of hidden states, thus eliminating the need to increase the parameter count in the FFN. Furthermore, averaging the features acts as a form of regularization, mitigating the risk of overfitting and enhancing robustness, as well as minimizing the impact of potential failures of individual MHAs.

Specifically, the input is replicated multiple times and fed into several completely identical attention branches. These outputs are then processed through a shared FFN (Feed-Forward Network), followed by individual layer normalization. Ultimately, these outputs are averaged and normalized by another layer normalization to serve as the output of a single MOA block. In a rigorous sense, the output $z_i^{\ell}$ of $\ell$-th MOA block, which comprises $n$ parallel attention branches, is derived as follows:
\begin{equation}
    z_i^{\ell} = {\rm LayerNorm}(\frac{1}{n} \sum_{j=1}^{n} {\rm Branch}_j(z_i^{\ell-1}) + z^{\ell-1}_{i})
    \label{Eq7}
\end{equation}
where ${\rm Branch}_j(\cdot) \coloneqq {\rm LayerNorm}_j({\rm FFN}({\rm Attention}_j(\cdot) + (\cdot)))$.

\subsubsection{Intra-block Balance: Branch Weight of MOA}
The output of a single MOA block is obtained by averaging the results from multiple attention branches, suggesting an equitable contribution of each attention branch's output to the final result. If we regard each attention branch as an individual entity, then this exhibits a paradigm of collaborative learning. On this basis, we can explicitly introduce a dynamic consistency constraint for each branch, leading us to propose a balancing strategy within the MOA block, namely, the utilization of branch weights (Fig.\ref{fig: branch weight}).
Specifically, during training, we attach a shared Predictor (same as in Eq.\ref{Predictor}) to each attention branch. If the prediction of a particular branch deviates significantly from the ground truth—quantified using Mean Squared Error (MSE) loss for regression tasks and Cross-Entropy (CE) loss for classification tasks—we assign a larger weight to this branch. These weights are computed by applying the softmax function across the losses of all branches. Consequently, branches exhibiting larger deviations receive greater penalty, thereby achieving a balance among the branches. To mitigate significant fluctuations in these branch weights, we apply Exponential Moving Average (EMA) for smoothing purposes. In conclusion, the formulation of branch weight $W_{B}$ can be articulated as follows:
\begin{equation}
    W_{B} = {\rm EMA}({\rm Softmax}(s))
\end{equation}
where $W_{B} = \{w_j\}_{j=1}^{n}$ and $s = \{s_{j}\}_{j=1}^{n}$. $s_{j}$ is the supervised loss of ${\rm Branch}_j$, that is 
\begin{equation}
    s_{j} = {\rm L}_{pred}({\rm Predictor}({\rm Branch}_j(z^{\ell-1})), y)
\end{equation}
where ${\rm L}_{pred}$ is MSE loss or CE loss.

By applying the branch weight, Eq.\ref{Eq7} can be reformulated as
\begin{equation}
    z_i^{\ell} = {\rm LayerNorm}(\sum_{j=1}^{n} w_j{\rm Branch}_j(z_i^{\ell-1}))
\end{equation}

\begin{figure}
\centering
{\includegraphics[width=0.45\textwidth]{branch_weight-cropped.pdf}}
\caption{Computation of branch weights. During training, each branch is appended with the shared Predictor and its loss is computed in relation to the labels. Branches with larger losses are assigned greater weights. This dynamic consistency constraint encourages collaborative learning among the branches.}
\label{fig: branch weight}
\end{figure}

\subsubsection{Inter-block Averaging}
In pursuit of not only intra-block balance but also inter-block efficacy in feature extraction, we proceed to average the outputs of the stacked $L_e$ MOA blocks and append a LayerNorm layer, thereby yielding the ultimate output of the encoder. Therefore, in summary, combining Eq.\ref{Encoder} and Eq.\ref{MOA_io}, the output $\hat{z}_i$ of the encoder is actually obtained through the following calculation:
\begin{equation}
    \hat{z}_i = {\rm LayerNorm} (\frac{1}{L_e} \sum_{l=1}^{L_e} {z_i^{l}})
\end{equation}

\subsection{Decoder}
The decoder consists of a novel inter-instance cross attention module. Given the encoder's proficiency in extracting representations for each individual instance, we innovatively conceptualize a set of instances $\hat{Z} = \{\hat{z}_i\}$ as a sequence, whether it comprises the entire training dataset or a specific batch (i.e. $|\hat{Z}| \leq N$), with each instance functioning as a token within this sequence. For example, in PyTorch, we can unsqueeze a tensor with a shape of $[batchsize, d]$ output by encoder into a tensor of shape $[1, batchsize, d]$. This conceptual shift facilitates the direct utilization of the encoder's output as the input for the inter-instance attention mechanism. On this basis, we introduce a novel foundational module within the decoder, which not only incorporates the inter-instance attention mechanism but also seamlessly integrates label information. 

\begin{figure}
\centering
{\includegraphics[width=0.45\textwidth]{iail-cropped.pdf}}
\caption{The overview of IAIL block in the decoder.}
\label{fig: iail}
\end{figure}

\subsubsection{Inter-instance Attention Incorporated with Labels}
We refer to the fundamental component of the decoder as IAIL (i.e. Inter-instance Attention Incorporated with Labels), which actually employs a cross-attention mechanism, hence it accommodates multiple inputs, as demonstrated in Fig.\ref{fig: iail}. For clarity, let us designate one input to the $\ell$-th IAIL (denoted as ${\rm IAIL}^{\ell}$) as $\hat{Z}^{\ell-1} = \{\hat{z}_i^{\ell-1}\}$, with its output being $\hat{Z}^{\ell} = \{\hat{z}_i^{\ell}\}$; specifically, the input to the first IAIL is defined as $\hat{Z}^{0} = \{\hat{z}_i^{0}\}$. To effectively integrate label information into the processing of IAIL, we introduce a trainable Label Embedding Layer (LEL), which projects labels into an embedding space, formulated as:
\begin{equation}
    \hat{y}_i = {\rm LEL}(y_i),\hat{y}_i \in \mathbb{R}^d
\end{equation}
where, for classification tasks, LEL functions as a lookup table, whereas for regression tasks, it serves as a linear mapper. To maintain consistency with the feature embedding representations, the embeddings of a set of labels are collectively denoted as $\hat{Y} = \{\hat{y}_i\}$.

During the phases of both training and inference, the cross-attention mechanism within IAIL is supplied with distinct inputs, as detailed below:
\begin{itemize}
    \item Training: for the $\ell$-th IAIL, the query is derived from the preceding IAIL's output, while the key is fixed as the raw output obtained from the encoder for instances within the same batch, and the value utilizes the label information corresponding to the instances in the same batch:
    \begin{equation}
        \hat{Z}_{batch}^{\ell-1} \rightarrow {\rm Query}^{\ell}
    \end{equation}
    \begin{equation}
        \hat{Z}_{batch}^{0} \rightarrow {\rm Key}^{\ell}
    \end{equation}
    \begin{equation}
        \hat{Y}_{batch} \rightarrow {\rm Value}^{\ell}
    \end{equation}
    where the subscript ``batch" denotes the collective group of instances within a particular batch.
    \item Inference: during the inference process, we utilize all instances from the training set as the input for both key and value:
    \begin{equation}
        \hat{z}_{i}^{\ell-1} \rightarrow {\rm Query}^{\ell}
    \end{equation}
    \begin{equation}
        \hat{Z}_{train}^{0} \rightarrow {\rm Key}^{\ell}
    \end{equation}
    \begin{equation}
        \hat{Y}_{train} \rightarrow {\rm Value}^{\ell}
    \end{equation}
    where the subscript ``i" signifies the specific $i$-th test instance, and the subscript ``train" indicates the training dataset from which these instances are sourced.
\end{itemize}

It is noteworthy that for both query and key, we apply identical weights for their respective projections, and assess their similarity through the utilization of the L2 distance instead of scaled dot-product. Consequently, owing to the adoption of the L2 distance, we omit the multi-head mechanism in this context.

\begin{table*}[t!]
    \centering
    \begin{tabular}{lccrrrrl}
    \toprule
Name                & Abbr & Task & Size  & \#Num & \#Cat & \#Class & \multicolumn{1}{c}{URL} \\
    \midrule
Adult               & AD   & cls  & 48842 & 6     & 8     & 2       & \href{ http://automl.chalearn.org/data}{adult} \\ 
%{\fontsize{6.35pt}{\baselineskip}\selectfont http://automl.chalearn.org/data}                        \\
Bank                & BA   & cls  & 45211 & 7     & 9     & 2       & \href{https://archive.ics.uci.edu/ml/datasets/Bank+Marketing}{bank}                        \\
Blastchar           & BL   & cls  & 7032  & 3     & 16    & 2       & \href{https://www.kaggle.com/blastchar/telco-customer-churn}{blastchar}                        \\
California Housing  & CA   & reg  & 20640 & 8     & 0     & -       & \href{https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html}{california$\_$housing}                        \\
Diamonds            & DI   & reg  & 53940 & 6     & 3     & -       & \href{https://www.kaggle.com/datasets/joebeachcapital/diamonds}{diamonds}                         \\
Helena              & HE   & cls  & 65196 & 27    & 0     & 100     & \href{https://www.openml.org/search?type=data&status=active&id=41169&sort=runs}{helena}                        \\
Income              & IN   & cls  & 48842 & 6     & 8     & 2       & \href{https://www.kaggle.com/lodetomasi1995/income-classification}{income}                        \\
Jannis              & JA   & cls  & 83733 & 54    & 0     & 4       & \href{https://www.openml.org/search?type=data&status=active&id=41168&sort=runs}{jannis}                        \\
Otto Group Products & OT   & cls  & 61878 & 93    & 0     & 9       & \href{https://www.kaggle.com/c/otto-group-product-classification-challenge/data}{otto$\_$group$\_$products}                        \\
QSAR Bio            & QS   & cls  & 1055  & 31    & 10    & 2       & \href{https://archive.ics.uci.edu/dataset/254/qsar+biodegradation}{qsar$\_$bio}                         \\
Seismic Bumps       & SE   & cls  & 2584  & 14    & 4     & 2       & \href{https://archive.ics.uci.edu/dataset/266/seismic+bumps}{seismic$\_$bumps}                        \\
Shrutime            & SH   & reg  & 10000 & 4     & 6     & -       & \href{https://www.openml.org/search?type=data&status=active&id=45062&sort=runs}{shrutime}                        \\
Spambase            & SP   & cls  & 4601  & 57    & 0     & 2       & \href{https://archive.ics.uci.edu/ml/datasets/spambase}{spambase}                        \\
Volume              & VO   & reg  & 50993 & 53    & 0     & -       & \href{https://archive.ics.uci.edu/dataset/363/facebook+comment+volume+dataset}{volume}      
            \\
    \bottomrule
    \end{tabular}
    \caption{The details of datasets. ``\#Num" and ``\#Cat" denote the number of numerical and categorical features, respectively.}
    \label{tab:info datasets}
\end{table*}

\section{Experiments}
In this section, we will compare the proposed MAYA to SOTA transformer-based methods on several real-world classification and regression datasets to demonstrate its superiority. Furthermore, we embark on a thorough investigation into the attributes of MAYA through ablation studies.

\subsection{Experimental Setting}
\paragraph{Datasets}
We have selected 14 datasets from previous works \cite{huang2020tabtransformer,somepalli2021saint,gorishniy2021revisiting}, with detailed information and statistics provided in Table \ref{tab:info datasets}. All datasets are partitioned into training, validation, and test sets in a ratio of 64\%/16\%/20\%, respectively, where validation set is utilized for hyper-parameter tuning and early stopping. The preprocessing of all datasets is conducted according to \cite{gorishniy2021revisiting}.

\paragraph{Evaluation} We primarily adhere to the settings of \cite{gorishniy2021revisiting} for our evaluation. Each dataset undergoes experimentation with 15 random seeds, and the average results on the test set are reported. For classification tasks, we employ accuracy as the evaluation metric; for regression tasks, we report the Root Mean Squared Error (RMSE). Besides, we also report the average rank (lower is better) of each method across all datasets to reflect the overall performance of a particular method.

\paragraph{Comparison Methods} The network proposed in this paper will be benchmarked against several SOTA transformer-based methods, including AutoInt (AT) \cite{song2019autoint}, TabTransformer (TT) \cite{huang2020tabtransformer}, SAINT (SNT) \cite{somepalli2021saint}, FT-Transformer (FTT) \cite{gorishniy2021revisiting}, ExcelFormer (EF) \cite{chen2024can} and AMFormer (AMF) \cite{cheng2024arithmetic}.
All methods are based on their officially released implementations.

\paragraph{Implementation Details}\label{sec: setting} Regarding the comparison methods, if results are reported in the respective papers, we directly cite them. Otherwise, hyper-parameter tuning is conducted within the corresponding space (see Appendix \ref{appendix: opt_space} for the detailed description). In cases where multiple results are available, we opt to cite the best one. For our method, we leverage Optuna \cite{akiba2019optuna} to execute 100 trials to ascertain the best-performed hyper-parameter configuration, which will be utilized across all 15 random seeds.
The hyper-parameter tuning for all methods are based on training and validation sets. 

\subsection{Main Results}
The comparison results of MAYA against other transformer-based methods are presented in Table \ref{tab: main results}. It is evident that MAYA significantly outperforms other methods in terms of the average rank, indicating that the overall performance of MAYA is currently the best among transformer-based models.

\begin{table*}[t!]
    \centering
    \begin{tabular}{ccccccc|c}
        \toprule
        Dataset & AI & TT  & SNT & FTT & EF & AMF &  MAYA (ours)\\
        \midrule
        AD $\uparrow$     & 0.8576     & 0.8509   & 0.8600     & 0.8588     & 0.8594    & 0.8594    & \textbf{0.8632} \\
        BA $\uparrow$     & 0.9077    & 0.8998   & 0.9075     & 0.9095     & 0.9092    & 0.9082    & \textbf{0.9100} \\
        BL $\uparrow$  & 0.7985    & 0.7775   & 0.8008     & 0.7995    & \textbf{0.8018}   & 0.7990    & 0.8003 \\
        CA $\downarrow$ & 0.5007    & 0.5936   & 0.4680     & 0.4564   & 0.4519    & 0.4626    & \textbf{0.4373} \\
        DI$_{\times 10^{3}}$ $\downarrow$ & 0.5372    & 0.7345   & 0.5466     & 0.5334    & 0.5331    & 0.5384    & \textbf{0.5324} \\
        HE $\uparrow$ & 0.3811    & 0.3589  & 0.3856   & \textbf{0.3890}    & 0.3802    & 0.3882    & 0.3831\\
        IN $\uparrow$ & 0.8605    & 0.8535   & 0.8661     & 0.8633    & 0.8665    & 0.8625    & \textbf{0.8681} \\
        JA $\uparrow$ & 0.7178    &  0.7084  & 0.7111   & \textbf{0.7306}    & 0.7262    & 0.7281    & 0.7207 \\
        OT $\uparrow$ & 0.8084    & 0.8019   & \textbf{0.8120}     & 0.8082    & 0.8035    & 0.8074    & 0.7984 \\
        QS $\uparrow$ & 0.8357    & 0.8461   & 0.8452    & 0.8330    & 0.8389    & \textbf{0.8537}    & 0.8515 \\
        SE $\uparrow$ & \textbf{0.9309}    & 0.9243   & 0.9259    & 0.9275    & 0.9282    & 0.9253     & 0.9299 \\
        SH $\downarrow$ & 0.3255    & 0.3464   & 0.3131    & 0.3222    & 0.3197    & 0.3149    & \textbf{0.3126} \\
        SP $\uparrow$ & 0.9362    & 0.9393   & 0.9294   & 0.9424    & 0.9346    & 0.9435    & \textbf{0.9425} \\
        VO$_{\times 10^{2}}$ $\downarrow$ & \textbf{0.3987}     & 0.4940   & 0.4171     & 0.4210   & 0.4253    & 0.4097    & 0.4037 \\
        \midrule
        rank    & 4.5714    & 6.4286   & 4.0000   & 3.5714    & 3.6429    & 3.5000    & \textbf{2.2857}\\
        \bottomrule
    \end{tabular}
    \caption{The results of all methods across 14 datasets. The upward arrow ($\uparrow$) indicates the accuracy for classification tasks (higher is better), while the downward arrow ($\downarrow$) represents the RMSE for regression tasks (lower is better). The average rank results also follow the principle that lower is better. The scientific notation next to the dataset names indicates the scale of the results. The best result for each dataset is bolded.}
    \label{tab: main results}
\end{table*}

\subsection{Ablation Studies}\label{sec: abla}
In this section, we undertake an exhaustive analysis of the design characteristics of the proposed network, utilizing six diverse datasets: BA/BL/QS/SE for classification and CA/SH for regression.

\paragraph{The Properties of MOA} First, let us explore the impact of some design details of the MOA block. We set the number of parallel branches in MOA block to 1, thereby degrading the attention mechanism within the MOA block to a single MHA. To ensure a fair comparison, we maintain the same number of heads for MHA as that for MOA in MAYA (i.e., num$\_$heads$_{\rm MHA}$ = num$\_$heads$_{per\_branch}$ $\times$ num$\_$branch). Based on this configuration, we test two settings: the first ensures that the size of hidden states (i.e., hidden$\_$size) in MHA is identical to that in MOA of MAYA, denoted as MHA$_{hidden\_size}$; the second guarantees that the subspace dimension per head (i.e., head$\_$dim) in MHA matches that in MOA of MAYA, denoted as MHA$_{head\_dim}$. Other parameters related to the model architecture, such as the upscaling factor for the intermediate layer of FFN (i.e., intermediate$\_$factor) and the number of blocks (i.e., num$\_$layers), remain consistent with MAYA, while the other parameters undergo hyper-parameter tuning as outlined in Section \ref{sec: setting}. Details are provided in Appendix \ref{appendix: abla}. Both settings enhance feature richness and diversity by increasing the number of heads. In the first setting, compared to MAYA, the subspace dimension per head of the single-branch MHA is reduced, resulting in equivalent overall feature richness but diminished expressiveness per subspace. In the second setting, compared to MAYA, the output hidden states of the single-branch MHA become larger, leading to an increase in the parameter size of the subsequent FFN, 
thereby significantly augmenting the model size and computational overhead. By compared to these two single-branch MHA configurations, we want to reveal the  advantage of MOA block in balancing feature complexity and parameter counts. Additionally, apart from setting the number of parallel branches to 1, we perform tuning on all other parameters, referred to as MHA$_{free}$. We also conduct ablation study on the branch weights within the MOA block. Specifically, we remove all branch weights and tune the hyper-parameters as described in Section \ref{sec: setting}. 

All the experimental results mentioned above are presented in Table \ref{tab: ablation of MOA}. It is evident that among all the ablation study settings related to MOA, MAYA achieves the best performance, followed by the configuration without branch weight, both of which outperform the other settings of the single-branch MHA. This indicates that, while enriching feature representations, MOA block's approach of averaging features at the branch level not only balances amount of parameters but also enhances the model's predictive capability in a manner akin to feature pooling. Additionally, the utilization of branch weight effectively encourages collaborative learning among branches, further boosting the model's predictive performance. Conversely, the worst results are observed for MHA$_{hidden\_size}$, suggesting that merely increasing the number of heads while reducing the feature dimension of each head's subspace does not improve the model's predictive capability.

\begin{table*}[t!]
    \centering
    \begin{tabular}{ccccc|c}
    \toprule
    Dataset & MHA$_{hidden\_size}$    & MHA$_{head\_dim}$   & MHA$_{free}$   & w/o $W_B$     & MAYA \\
    \midrule
    BA $\uparrow$     & 0.9084 (0.5$\times$)    & \textbf{0.9101} (7.3$\times$)    & 0.9092     & 0.9088       & 0.9100 \\
    BL $\uparrow$     & 0.8000 (0.4$\times$)    & 0.7995 (10.7$\times$)    & 0.7969    & 0.8001    & \textbf{0.8003} \\
    CA $\downarrow$     & 0.4479 (0.4$\times$)    & 0.4412 (12.0$\times$)    & 0.4405    & 0.4473    & \textbf{0.4373} \\
    QS $\uparrow$     & \textbf{0.8578} (0.2$\times$)     & 0.8559 (12.8$\times$)   & 0.8288    & 0.8385    & 0.8515 \\
    SE $\uparrow$     & 0.9275 (0.4$\times$)     & 0.9278 (6.4$\times$)    & 0.9287    & \textbf{0.9306}       & 0.9299 \\
    SH $\downarrow$     & 0.3161 (0.5$\times$)   & 0.3166 (4.2$\times$)    & 0.3143    & 0.3154    & \textbf{0.3126} \\
    \midrule
    rank    & 3.8333    & 3.1667     & 3.3333      & 3.0000    & \textbf{1.6667} \\
    \bottomrule
    \end{tabular}
    \caption{The results of ablation studies regarding the MOA block. MHA denotes the use of only one attention branch, where the subscript ``hidden$\_$size" indicates that the hidden$\_$size of this attention branch is consistent with that of MOA in MAYA. The subscript ``head$\_$dim" signifies that the head$\_$dim of this attention branch aligns with that of  MOA in MAYA. Other parameters related to the model structure (i.e. num$\_$heads, intermediate$\_$factor, and num$\_$layers) remain consistent with those in MAYA, while the remaining parameters undergo hyper-parameter tuning. The subscript ``free" indicates that all parameters are tuned. w/o $W_B$ represents the removal of the branch weight from MOA. The multiples $N\times$ following the results of MHA$_{hidden\_size}$ and MHA$_{head\_dim}$ indicate that the parameter count of the corresponding encoder is $N$ times that of MAYA. The computation details are provided in Appendix \ref{appendix: computation_of_para}. The best result for each dataset is in bold. $\uparrow$  $\thicksim$ accuracy (higher is better), $\downarrow$ $\thicksim$ RMSE (lower is better).}
    \label{tab: ablation of MOA}
\end{table*}

\paragraph{The Influence of Activation after Tokenizer}\label{sec: abla_tokenzier}We attempt to remove the activation function following the tokenizer, while maintaining all other configurations constant, and perform hyper-parameter tuning according to the protocol outlined in Section \ref{sec: setting}. This configuration is subsequently compared against MAYA that retains the activation function, to ascertain its impact. The results, presented in Table \ref{tab: ablation of tokenizer}, reveal that the incorporation of a ReLU activation function subsequent to tokenizer yields a marked enhancement in the model's predictive accuracy.

\begin{table}[h]
    \centering
    \begin{tabular}{ccc}
    \toprule
    Dataset & \textbf{w/o} activation in tokenizer  & MAYA \\
    \midrule
    BA $\uparrow$ & 0.9094  & \textbf{0.9100} \\
    BL $\uparrow$ & 0.7997  & \textbf{0.8003} \\
    CA $\downarrow$ & 0.4401   & \textbf{0.4373} \\
    QS $\uparrow$ & 0.8346  & \textbf{0.8515} \\
    SE $\uparrow$ & 0.9275  & \textbf{0.9299} \\
    SH $\downarrow$ & 0.3129   & \textbf{0.3126} \\
    \bottomrule
    \end{tabular}
    \caption{The results of ablation study regarding the activation following the tokenizer. The best result for each dataset is in bold. $\uparrow$ $\thicksim$ accuracy (higher is better), $\downarrow$ $\thicksim$ RMSE (lower is better).}
    \label{tab: ablation of tokenizer}
\end{table}

\section{Conclusion}
In this paper, we present MAYA, a novel transformer-based encoder-decoder architecture for tabular data. The design of dual-attention mechanism effectively promote the information fusion within and between instances. Additionally, to improve the ability of extracting diverse features, we introduce MOA to sufficiently utilize multiple attention branches. Experiments on several popular public datasets demonstrate that our method performs impressively in processing tabular data.  Furthermore, ablation studies investigate the effectiveness of the MOA blocks and activation operations after tokenizer in our model, suggesting that the multi-branch structure of the MOA blocks, along with activation functions subsequent to tokenizer, is essential for improving the performance of the proposed method for tabular tasks.

\bibliographystyle{named}
\bibliography{maya_arxiv}


\section{Appendix}

\subsection{Hyper-parameter Tuning Space}\label{appendix: opt_space}
Here we provide hyper-parameter tuning spaces used for Optuna tuning for all methods in Section 4.2.

\subsubsection{MAYA} \label{sec: hyper_para_space for maya}
The hyper-parameter tuning space for MAYA is presented in Table \ref{tab: opt_space_MAYA}.

\subsubsection{AutoInt}
The hyper-parameter tuning space for AutoInt is presented in Table \ref{tab: opt_space_autoint}.

\subsubsection{TabTransformer}
The hyper-parameter tuning space for TabTransformer is presented in Table \ref{tab: opt_space_tabtransformer}.

\subsubsection{SAINT}
The hyper-parameter tuning space for SAINT is presented in Table \ref{tab: opt_space_saint}.

\subsubsection{FT-Transformer}
The hyper-parameter tuning space for FT-Transformer is presented in Table \ref{tab: opt_space_ftt}.

\subsubsection{ExcelFormer}
The hyper-parameter tuning space for ExcelFormer is presented in Table \ref{tab: opt_space_excelformer}.

\subsubsection{AMFormer}
The hyper-parameter tuning space for AMFormer is presented in Table \ref{tab: opt_space_amformer}.

\begin{table*}[tbp]
\centering
\vspace{1em}
\begin{tabular}{l|ll}
    \toprule
    & Parameter & Distribution \\
    \midrule
    \multirow{9}{*}{encoder} 
                            & num$\_$layers & $\mathrm{UniformInt}[1,16]$ \\
                            & hidden$\_$size & $\mathrm{UniformInt}[64,256]$ \\
                            & num$\_$heads & $\mathrm{Categorical}[1,4,8,32]$ \\
                            & num$\_$branch & $\mathrm{UniformInt}[2,8]$ \\
                            & intermediate$\_$factor & $\mathrm{Categorical}[0.8,1,1.3,2]$ \\
                            & dropout & $\mathrm{Uniform}[0, 0.3]$ \\
                            & add$\_$act & $\mathrm{Categorical}[\mathrm{true, false}]$ \\
                            & if$\_$bias & $\mathrm{Categorical}[\mathrm{true, false}]$ \\
                            & act$\_$type & $\mathrm{Categorical}[\mathrm{relu, prelu}]$ \\
    \midrule
    \multirow{5}{*}{decoder} 
                            & num$\_$decoder$\_$layers & $\mathrm{UniformInt}[1,16]$ \\
                            & decoder$\_$intermediate$\_$factor & $\mathrm{Categorical}[0.8,1,1.3,2]$ \\
                            & decoder$\_$dropout & $\mathrm{Uniform}[0, 0.3]$ \\
                            & decoder$\_$if$\_$bias & $\mathrm{Categorical}[\mathrm{true, false}]$ \\
                            & decoder$\_$act$\_$type & $\mathrm{Categorical}[\mathrm{relu, prelu}]$ \\
    \midrule
    \multirow{3}{*}{others}
                            & batch$\_$size & $\mathrm{Categorical}[\mathrm{1024,2048,4096}]$ \\
                            & learning$\_$rate & $\mathrm{LogUniform}[1e\text{-}5, 5e\text{-}3]$ \\
                            & weight$\_$decay & $\mathrm{Uniform}[0,0.3]$ \\
    \bottomrule
\end{tabular}
\caption{Hyper-parameter tuning space for MAYA.}
\label{tab: opt_space_MAYA}
\end{table*}

\begin{table*}[htbp]
    \centering
    \begin{tabular}{ll}
    \toprule
    Parameter & Distribution \\
    \midrule
    n$\_$layers & $\mathrm{UniformInt}[1,6]$ \\
    %d$\_$token & $\mathrm{Categorical}[8,16,32,64,128]$ \\
    d$\_$token & $\mathrm{UniformInt}[8,64]$ \\
    residual$\_$dropout & $\{0, \mathrm{Uniform}[0, 0.2] \}$ \\
    attention$\_$dropout & $\{0, \mathrm{Uniform}[0, 0.5] \}$ \\
    learning$\_$rate & $\mathrm{LogUniform}[1e\text{-}5, 1e\text{-}3]$ \\
    weight$\_$decay & $\mathrm{LogUniform}[1e\text{-}6, 1e\text{-}3]$ \\
    \bottomrule
    \end{tabular}
    \caption{Hyper-parameter tuning space for AutoInt.}
    \label{tab: opt_space_autoint}
\end{table*}

\begin{table*}[htbp]
    \centering
    \begin{tabular}{ll}
    \toprule
    Parameter & Distribution \\
    \midrule
    dim & $\mathrm{Categorical}[32,64,128,256]$ \\
    depth & $\mathrm{Categorical}[1,2,3,6,12]$ \\
    heads & $\mathrm{Categorical}[2,4,8]$ \\
    attn$\_$dropout & $\mathrm{Uniform}[0, 0.5]$ \\
    ffn$\_$dropout & $\mathrm{Uniform}[0, 0.5]$ \\
    learning$\_$rate & $\mathrm{LogUniform}[1e\text{-}6, 1e\text{-}3]$ \\
    weight$\_$decay & $\mathrm{LogUniform}[1e\text{-}6, 1e\text{-}1]$ \\
    \bottomrule
    \end{tabular}
    \caption{Hyper-parameter tuning space for TabTransformer.}
    \label{tab: opt_space_tabtransformer}
\end{table*}

\begin{table*}[htbp]
    \centering
    \begin{tabular}{ll}
    \toprule
    Parameter & Distribution \\
    \midrule
    dim & $\mathrm{Categorical}[16,32,64]$ \\
    depth & $\mathrm{Categorical}[4,6]$ \\
    heads & $\mathrm{Categorical}[4,8]$ \\
    attn$\_$dropout & $\mathrm{Uniform}[0, 0.5]$ \\
    ffn$\_$dropout & $\mathrm{Uniform}[0, 0.5]$ \\
    attn$\_$type & $\{\mathrm{colrow}, \mathrm{Categorical}[\mathrm{colrow, row, col}] \}$ \\
    learning$\_$rate & $\mathrm{LogUniform}[3e\text{-}5, 1e\text{-}3]$ \\
    weight$\_$decay & $\{0, \mathrm{LogUniform}[1e\text{-}6, 1e\text{-}4] \}$ \\
    \bottomrule
    \end{tabular}
    \caption{Hyper-parameter tuning space for SAINT.}
    \label{tab: opt_space_saint}
\end{table*}

\begin{table*}[htbp]
    \centering
    \begin{tabular}{ll}
    \toprule
    Parameter & Distribution \\
    \midrule
    n$\_$layers & $\mathrm{UniformInt}[1,4]$ \\
    d$\_$token & $\mathrm{UniformInt}[64,512]$ \\
    residual$\_$dropout & $\{0, \mathrm{Uniform}[0, 0.2] \}$ \\
    attention$\_$dropout & $\mathrm{Uniform}[0, 0.5]$ \\
    ffn$\_$dropout & $\mathrm{Uniform}[0, 0.5]$ \\
    ffn$\_$factor & $\mathrm{Uniform}[\nicefrac{2}{3}, \nicefrac{8}{3}]$ \\
    learning$\_$rate & $\mathrm{LogUniform}[1e\text{-}5, 1e\text{-}3]$ \\
    weight$\_$decay & $\mathrm{LogUniform}[1e\text{-}6, 1e\text{-}3]$ \\
    \bottomrule
    \end{tabular}
    \caption{Hyper-parameter tuning space for FT-Transformer.}
    \label{tab: opt_space_ftt}
\end{table*}

\begin{table*}[htbp]
    \centering
    \begin{tabular}{ll}
    \toprule
    Parameter & Distribution \\
    \midrule
    n$\_$layers & $\mathrm{UniformInt}[2,5]$ \\
    d$\_$token & $\mathrm{Categorical}[64,128,256]$ \\
    n$\_$heads & $\mathrm{Categorical}[4,8,16,32]$ \\
    residual$\_$dropout & $\mathrm{Uniform}[0, 0.5]$ \\
    %attention$\_$dropout & $\mathrm{Uniform}[0, 0.5]$ \\
    %ffn$\_$dropout & $\mathrm{Uniform}[0, 0.5]$ \\
    mix$\_$type & $\mathrm{Categorical}[\mathrm{none}, \mathrm{feat}$\_$\mathrm{mix}, \mathrm{hidden}$\_$\mathrm{mix}]$ \\
    learning$\_$rate & $\mathrm{LogUniform}[3e\text{-}5, 1e\text{-}3]$ \\
    weight$\_$decay & $\{0, \mathrm{LogUniform}[1e\text{-}6, 1e\text{-}3] \}$ \\  
    \bottomrule
    \end{tabular}
    \caption{Hyper-parameter tuning space for ExcelFormer.}
    \label{tab: opt_space_excelformer}
\end{table*}

\begin{table*}[h!]
    \centering
    \begin{tabular}{ll}
    \toprule
    Parameter & Distribution \\
    \midrule
    dim & $\mathrm{Categorical}[8,16,32,64,128]$ \\
    depth & $\mathrm{Categorical}[1,4]$ \\
    attn$\_$dropout & $\mathrm{Uniform}[0, 0.5]$ \\
    ffn$\_$dropout & $\mathrm{Uniform}[0, 0.5]$ \\
    num$\_$special$\_$tokens & $\mathrm{UniformInt}[1,4]$ \\
    learning$\_$rate & $\mathrm{LogUniform}[1e\text{-}5, 1e\text{-}3]$ \\
    weight$\_$decay & $\mathrm{LogUniform}[1e\text{-}6, 1e\text{-}3]$ \\
    \bottomrule
    \end{tabular}
    \caption{Hyper-parameter tuning space for AMFormer.}
    \label{tab: opt_space_amformer}
\end{table*}

\section{Ablation Studies}
\subsection{Implementation Details}\label{appendix: abla}
In this section, we provide implementation details of the ablation studies on the properties of MOA block. The first two experimental settings (i.e., MHA$_{hidden\_size}$ and MHA$_{head\_dim}$) ensure that the number of heads in MHA matches that in MOA of MAYA, which is formulated as:
\begin{equation}
    {\rm{num\_head}}_{\rm MHA} = {\rm{num\_heads}}_{per\_branch} \times \rm{num\_branch}
\end{equation}

For the first setting, MHA$_{hidden\_size}$, we ensure that the hidden$\_$size is consistent with MOA. The hidden$\_$size for MHA should be:
\begin{equation}
    \rm{hidden\_size}_{MHA} = \rm{head\_dim}_{MHA} \times \rm{num\_head}_{MHA} 
    \label{eq: hidden_size_MHA}
\end{equation}
However, there may be cases where the hidden$\_$size cannot be evenly divided by the number of parallel attention branches (i.e., num$\_$branch), resulting in a non-integer value for head$\_$dim$_{\rm{MHA}}$ in Eq.\ref{eq: hidden_size_MHA}. In such cases, we perform a ceiling operation on head$\_$dim$_{\rm{MHA}}$, which may lead to hidden$\_$size$_{\rm{MHA}}$ being slightly larger than hidden$\_$size$_{\rm{MOA}}$. Please refer to the Table \ref{tab: MHA_hidden_size} for the specific hidden sizes corresponding to each dataset.

For the second setting, MHA$_{head\_dim}$, we ensure that the subspace dimension for each head, i.e. head$\_$dim$_{\rm{MHA}}$, remains identical with MOA, whereby the hidden$\_$size for MHA is still calculated using Eq.\ref{eq: hidden_size_MHA}. For the specific hidden sizes corresponding to each dataset, please refer to Table \ref{tab: MHA_head_dim}.

\begin{table*}[htbp]
\centering
\begin{tabular}{lccc|ccc}
\toprule
\multirow{2}{*}{} & \multicolumn{3}{c|}{MOA in MAYA}  & \multicolumn{3}{c}{MHA$_{hidden\_size}$} \\
& $\#_{branch}$ & $\#_{heads}$ & D$_{attn}$ & $\#_{heads}$   & D$_{head}$   & D$_{attn}$   \\
\midrule
BA                & 4       & 4      &  192      & $4\times4=16$         & $192\div16=12$        & $16\times12=192$                \\
BL                & 6       & 8      &  128      & $6\times8=48$         & $\lceil128\div48\rceil=3$         & $48\times3=144$                \\
CA                & 6       & 1      & 128       & $6\times1=6$         & $\lceil128\div6\rceil=22$        & $6\times22=132$                \\
QS                & 8       & 1      & 160       & $8\times1=8$         & $160\div8=20$        & $8\times20=160$                \\
SE                & 4       & 1      & 224       & $4\times1=4$    & $224\div4=56$         & $4\times56=224$                       \\
SH                & 3       & 4      & 224       & $3\times4=12$    & $\lceil224\div12\rceil=19$         & $12\times19=228$                    \\     
\bottomrule
\end{tabular}
\caption{Details for MHA$_{hidden\_size}$. Notations: $\#_{branch}$ $\thicksim$ num$\_$branch, $\#_{heads}$ $\thicksim$ num$\_$heads, D$_{attn}$ $\thicksim$ hidden$\_$size, D$_{head}$ $\thicksim$ head$\_$dim.}
\label{tab: MHA_hidden_size}
\end{table*}

\begin{table*}[htbp]
\centering
\begin{tabular}{lcccc|ccc}
\toprule
\multirow{2}{*}{} & \multicolumn{4}{c|}{MOA in MAYA}  & \multicolumn{3}{c}{MHA$_{head\_dim}$} \\
& $\#_{branch}$ & $\#_{heads}$ & D$_{attn}$ & D$_{head}$ & $\#_{heads}$   & D$_{head}$   & D$_{attn}$   \\
\midrule
BA                & 4       & 4      &  192     & $192\div4=48$ & $4\times4=16$         & 48        & $16\times48=768$                \\
BL                & 6       & 8      &  128      & $128\div8=16$ & $6\times8=48$         & 16        & $48\times16=768$                \\
CA                & 6       & 1      & 128      & $128\div1=128$ & $6\times1=6$         & 128        & $6\times128=768$                \\
QS                & 8       & 1      & 160      & $160\div1=160$ & $8\times1=8$         & 160        & $8\times160=1280$                \\
SE                & 4       & 1      & 224     & $224\div1=224$  & $4\times1=4$    & 224         & $4\times224=896$                       \\
SH                & 3       & 4      & 224     & $224\div4=56$  & $3\times4=12$    & 56         & $12\times56=672$                    \\     
\bottomrule
\end{tabular}
\caption{Details for MHA$_{head\_dim}$. Notations: $\#_{branch}$ $\thicksim$ num$\_$branch, $\#_{heads}$ $\thicksim$ num$\_$heads, D$_{attn}$ $\thicksim$ hidden$\_$size, D$_{head}$ $\thicksim$ head$\_$dim.}
\label{tab: MHA_head_dim}
\end{table*}

\subsection{Computation of Parameter Counts in Tabel 3}\label{appendix: computation_of_para}
In Table \ref{tab: ablation of MOA} of the main text, we present a comparison of the parameter counts in the encoder of MHA$_{hidden\_size}$ and MHA$_{head\_dim}$ with that of MAYA. Here, we provide the specific method for the calculation.

In the MOA block, each attention branch constitutes an MHA. The MHA is comprised of four linear layers, namely query, key, value, and output projection. The weights of each linear layer form a square matrix of dimensions hidden$\_$size $\times$ hidden$\_$size. Therefore, the number of parameters in the attention mechanism can be calculated as follows:
\begin{equation}
    P_{attn} = \rm{num\_branch} \times (4 \times \rm{hidden\_size}^2)
    \label{eq: para_attn}
\end{equation}

In the FFN, we employ ReGLU \cite{shazeer2020glu}, which differs from the original transformer architecture by incorporating three linear layers. Analogous to the naming convention used in LLaMA \cite{touvron2023llama}, we refer to these layers as up, gate, and down projections. Due to the presence of the intermediate$\_$factor (also known as the upscaling factor, see Section \ref{sec: hyper_para_space for maya}), the matrix weights of each linear layer are of dimension intermediate$\_$factor $\times$ hidden$\_$size $\times$ hidden$\_$size. Consequently, the number of parameters in the FFN can be calculated as follows:
\begin{equation}
    P_{ffn} = 3 \times \rm{intermediate\_factor} \times \rm{hidden\_size}^2
    \label{eq: para_ffn}
\end{equation}

Combining Eq.\ref{eq: para_attn} and Eq.\ref{eq: para_ffn}, the number of parameters in an encoder with num$\_$layers of blocks is calculated as follows:
\begin{align}
    \label{eq: total_para}
    P = & {\rm{num\_layers}} \times (P_{attn} + P_{ffn}) \\
    = & {\rm{num\_layers}} \times \rm{hidden\_size}^2 \\
    & \times (4\times\rm{num\_branch} + 3 \times \rm{intermediate\_factor})
\end{align}

According to Eq.\ref{eq: total_para}, when calculating the number of parameters for the encoders of MHA$_{hidden\_size}$, MHA$_{head\_dim}$, and MAYA, one simply substitutes the corresponding variables into the equation. Note that the num$\_$layers and intermediate$\_$factor for both MHA and MAYA are always kept consistent. For instance, for the BA dataset (intermediate$\_$factor$=2$, num$\_$layers$=15$), the number of parameters for the encoders of MHA$_{hidden\_size}$, MHA$_{head\_dim}$, and MAYA are as follows, respectively:
\begin{equation}
    P_{{\rm MHA}_{hidden\_size}} = 15 \times 192^2 \times (4 \times 1 + 3 \times 2)
\end{equation}
\begin{equation}
    P_{{\rm MHA}_{head\_dim}} = 15 \times 768^2 \times (4 \times 1 + 3 \times 2)
\end{equation}
\begin{equation}
    P_{{\rm MAYA}} = 15 \times 192^2 \times (4 \times 4 + 3 \times 2)
\end{equation}
Therefore, the multiples presented in Table 3 are
\begin{equation}
    P_{{\rm MHA}_{hidden\_size}}  / P_{{\rm MAYA}} \approx 0.5
\end{equation}
\begin{equation}
    P_{{\rm MHA}_{head\_dim}}  / P_{{\rm MAYA}} \approx 7.3
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}