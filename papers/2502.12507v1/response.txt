\section{Related Work}
\subsection{Attention Mechanisms}
The attention mechanism was introduced to improve the performance of the encoder-decoder model for machine translation **Vaswani et al., "Attention Is All You Need"**. It reveals the relative importance of each component in a sequence compared to the other components. Usually, self-attention and cross-attention are the most widely used in transformer architecture models. Self-attention captures relationships within a single input sequence, and cross-attention captures relationships between elements of different input sequences.
Somepalli et al., **"Inter-Sample Attention for Tabular Data Classification"** introduce self-attention and inter-sample attention mechanisms for the classification of tabular data. They utilize self-attention to focus on individual features within the same data sample and inter-sample attention to represent the relationships between different data samples of the table.  In our approach, we introduce two novel attention mechanisms: the Mixture of Attention (MOA) for self-attention and Inter-instance Attention Incorporating with Labels (IAIL) for cross-attention, to enhance data utilization and feature representation.

\subsection{Collaborative Learning}
Collaborative learning **"Learning from Multiple Experts: Knowledge Transfer through Joint Training"** refers to methods and environments in which learners work together on a shared task, with each individual relying on and being responsible to others. In pixel-wise tasks, collaborative policy has been used to train different subset networks and to ensemble the reliable pixels predicted by these networks.  As a result, this strategy covers a broad spectrum of pixel-wise tasks without requiring structural adaptation **"Pixel-Wise Learning via Collaborative Policy"**. Here, we use collaborative policy to construct branch weight in MOA block during training, with the weight proportional to the difference between the branch output and the true label. It turns out that this dynamic consistency constraint can provide substantial benefit to enrich feature representations and achieve impressive results for tabular tasks.