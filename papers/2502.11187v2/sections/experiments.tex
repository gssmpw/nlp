\section{Pretraining}
\label{sec:pretraining}

\subsection{Tokenizer Training}
We developed a custom tokenizer for Bangla text using Tiktoken\footnote{\url{https://github.com/openai/tiktoken}}, which employs Byte Pair Encoding (BPE)~\cite{sennrich2015neural} for tokenization. To train this tokenizer, we sampled 48 GB of data from our pretraining Bangla corpus. Additionally, we modified the original Tiktoken codebase to enhance its efficiency and better accommodate the morphological complexities of Bangla. After training multiple tokenizers on the same subset, we merged each newly trained tokenizer with the existing Llama-3.2 tokenizer. This merging process aimed to preserve the strengths of the original tokenizer while integrating domain-specific vocabulary and improving segmentation for Bangla. To evaluate the performance of each merged tokenizer, we computed the average tokens per word (TPW) on a separate 1 GB sample from the original corpus. Table~\ref{tab:tokens-per-word}, in Appendix, summarizes the TPW values for both the original Llama-3.2 tokenizer and the newly trained tokenizers. A lower TPW generally indicates more efficient segmentation, which reduces sequence lengths and may enhance downstream model performance.

We trained five tokenizers with different vocabulary sizes, as presented in Table~\ref{tab:tokens-per-word}. Each of these tokenizers was then merged with the Llama-3.2 tokenizer to create five new tokenizers. Notably, the Llama-3.2 tokenizer exhibits a very high TPW value, which affects its performance for Bangla. In contrast, the newly developed tokenizers demonstrate significantly lower TPW values.

The table also shows that increasing the vocabulary size of the new tokenizers generally results in a lower TPW count. However, the relationship between vocabulary size and TPW is not strictly linear. While TPW decreases with larger vocabularies, the reduction becomes less significant for tokenizers with very large vocabulary sizes.

% We have trained five tokenizers with five different vocabulary sizes as presented in Table~\ref{tab:tokens-per-word}, which were then merged with llama-3.2 tokenizer to prepare five new tokenizers. We can notice that the llama-3.2 tokenizer has a very high TPW value. As a result, the performance of llama-3.2 is not up to the mark for Bangla. Additionally, newly prepared tokenizers have a very low TPW. The table also indicates that if we increase the vocabulary size of new tokenizers, then the tokenizer's TPW count decreases. However, the token per word doesn't maintain a linear relation with vocabulary. Rather, the reduction of TPW is not significant for tokenizers with large vocabularies. 

% They are 32K, 48K, 64K, 80K, and 96K. 
% Then these tokenizers are merged with llama-3.2 tokenizer to prepare five new tokenizers: llama-3.2-plus-32K, llama-3.2-plus-48K, llama-3.2-plus-64K, llama-3.2-plus-80K, and llama-3.2-plus-96K.  



\subsection{Model Architecture}
We have modified llama-3.2-1b and llama-3.2-3b models according to the merged tokenizers. As too many new tokens will increase the model's size and the training complexity, we modified the models according to llama-3.2-plus-48K tokenizer. We added extra embedding vectors in the embedding layer and modified the lm-head according to the vocabulary size.

\subsection{Pretraining}
After modifying the models, we pre-trained them on our full dataset using LlamaFactory~\cite{zheng2024llamafactory}. Both models were trained with a context length of 4096, with packing enabled for maximum efficiency. Training for one epoch required 1750 H100 GPU hours. 
% Table~\ref{tab:titulm_tokens} presents the TituLM token counts from different sources.
% training repository for this process. 
% The training was conducted on our full Bangla corpus, which contains approximately 36.8 billion Llama-3.2-Plus-48K tokens. 



% \begin{table}[h]
%     \centering
%     \begin{tabular}{p{4cm} c}
%         \hline
%         \textbf{Data} & \textbf{TituLM tokens (B)} \\
%         \hline
%         Common Crawl, Books, and Sangraha Deduplicated Data & 18.80 \\ \midrule
%         Our Synthetic Data & \\
%         \quad Translated & 1.47 \\
%         \quad Romanized & 3.87 \\
%         \quad Conversation & 0.42 \\
%         \quad Audio Transcription & 1.30 \\ \midrule
%         Sangraha & \\
%         \quad Translated & 4.26 \\
%         \quad Romanized & 6.68 \\
%         \hline
%         \textbf{Total} & \textbf{36.80} \\
%         \hline
%     \end{tabular}
%     \caption{TituLM Tokens Count for Pretraining.}
%     \label{tab:titulm_tokens}
% \end{table}




% \label{sec:experiments}



% \subsection{Data Splits}
% #To DO Nahin 
% \subsection{Training data}

% \subsection{Evaluation data}
% \subsection{Models}
% #TODO Nahin 
% \subsection{Continual Pretraining}
% #TODO Nahin 

% \subsection{Evalution}
% Evaluation Hareness Framework 
% #sagor 