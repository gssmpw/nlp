\section{Experiments}
\label{sec:experiments}

\subsection{Data splits}
% Description 

\paragraph{Training set} 
For training the model, the dataset we selected comprises 17.64k hours of news channel content, 688.82 hours of talk shows, 0.02 hours of vlogs, and 4.08 hours of crime shows. Table \ref{tab:category_wise_training_data_dist} provides detailed information about each category and its corresponding duration in hours.

% Table:  Training set Category Hours  [Table]
\begin{table}[h]
\centering
\begin{tabular}{lr}
\hline
\textbf{Channels Category} & \textbf{Hours}\\
\hline
News & 17,640.00\\
Talkshow & 688.82\\
Vlog & 0.02 \\
Crime Show & 4.08 \\
\textbf{Total} & \textbf{18,332.92}\\ 
\hline
\end{tabular}
\caption{Training data distribution according to channel category and hours}
\label{tab:category_wise_training_data_dist}
\end{table}

\paragraph{Development set}
To investigate the robustness of the pseudo-labeling approach, we randomly selected 10 hours of speech to create a development set. 

\paragraph{Test set}
% We separate 1.8k hours of audio for evaluation and testing purposes. 
To evaluate the performance of the models, we used four test sets. Two of these were developed as part of the MegaBNSpeech corpus, while the remaining two (Fleurs and Common Voice) are commonly used test sets that are widely recognized by the speech community. 

% Among the five test sets, we prepared two test sets: 1) Hishab Youtube-Testset of 8.05 hours and 2) Telephony Testset of 1.9 hours. The other three test data are 1) Fleurs test set of 3.43 hours, 2) Common Voice test set of 16.5 hours, and 3) OpenSLR Random Set of 8 hours. The test datasets are quite diverse, with domain variants and well-suited for model performance analysis, benchmarking, and interpretation. 

\begin{itemize}
     \item \textbf{MegaBNSpeech-YT Test Set :}
    The test set has been prepared from a recent collection of YouTube videos, resulting in 8 hours of data. This set is manually transcribed for evaluation purposes. The domains of this set include News, Talkshow, Courses, Drama, Science, Waz (Islamic preaching), etc.
    \item \textbf{MegaBNSpeech-Tele Test Set:} To assess the model's generalization capabilities, we also included 1.9 hours of telephony conversations from our in-house dataset collection, which were subsequently manually transcribed. It involves telephone conversations covering various discussion topics, including online food orders, health services, online ticket bookings, and online banking. The calls were originally recorded using 8kHz sampling rate, which we then upsampled to 16kHz to match the ASR input.\footnote{The curated telephony dataset is open-ended conversations with pre-defined topics and includes consent from the interlocutors.}
    % such as \textcolor{red}{ any idea of the topics?}. 
    % and is carefully annotated. The reason for considering the Telephony test set is to introduce a different source of test set from our training data for better understanding and comparison among selected models. 
    
    \item \textbf{Fleurs:} Fleur's \cite{conneau2023fleurs} datasets are from FLoRes-101 datasets\footnote{\url{https://huggingface.co/datasets/gsarti/flores_101}} which contain 3001 Wikipedia sentences. The authors translated dev and train sentences from FLoRes-101 to 102 languages and annotated them to develop ASR. We have separated the Bangla test datasets which contain 920 audio files with 3.43 hours of data. Fleurs contains a total of 3,010 train, 920 test, and 402 validation audio files. We separated the test datasets and evaluated them with our selected models.
    \item \textbf{Common Voice:} Common voice \cite{ardila2019common} is a massively multilingual ASR dataset. The dataset currently consists of 17,689 validated hours in 108 languages, but more voices and languages are always added. Common Voice 13 contains a total of 20.7k train, 9.23k of test, and 9.23k of validation audio files. We separated the test datasets and evaluated them with our selected models.
\end{itemize}

\subsection{Contemporary ASR Models}

% The candidate models are: 1) Hishab Baseline, 2) Google ASR API, 3)MMS \cite{pratap2023scaling}, and 4) ODD-speech Conformer \cite{rakib2023ood}. MMS is a multi-lingual ASR developed by Meta and the ODD-speech ASR Conformer model is trained on the ODD speech\cite{rakib2023bangla} 11.8k dataset. 

\paragraph{Google:} Google speech-to-text\footnote{\url{https://cloud.google.com/speech-to-text}} is a cloud-based ASR service that provides transcription from input Audio for several languages. It provides different domain-specific models for task-specific ASR services. We used the default model and settings and set the language to Bangla.



\paragraph{MMS:} Massively Multilingual Speech (MMS \cite{pratap2023scaling}) is a fine-tuned model developed by Meta. This model is based on the Wav2Vec2 \cite{baevski2020wav2vec} architecture and makes use of adapter models to transcribe 1000+ languages. The model consists of 1 billion parameters and has been fine-tuned in 1,162 languages. The model checkpoint is published in the HuggingFace model hub.\footnote{\url{https://huggingface.co/facebook/mms-1b-all}}

\paragraph{OOD-speech ASR:}
OOD-speech ASR is a Conformer-CTC-based model trained on ODD speech datasets \cite{rakib2023bangla}. The model consists of 121 million parameters and is trained on 1,100+ hours of audio data which is crowdsourced from native Bangla speakers. The model was trained using NVIDIA NeMo\footnote{\url{https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/asr/examples/kinyarwanda_asr.html}} framework and published in Huggingface model hub.\footnote{\url{https://huggingface.co/bengaliAI/BanglaConformer}}

% \subsection{Data Preprocessing}

\subsection{MegaBNSpeech ASR}

% \paragraph{Settings:} 
% We have done an extensive baseline experiment with 
We trained the FastConformer model \cite{rekesh2023fast} using the full 18k MegaBNSpeech training sets. During the training phase, we employed a set of predefined parameters: a learning rate of 0.5, a weight decay of 0.001, a batch size of 32, AdamW optimizer, and a maximum audio duration of 15 seconds. We provide details of the hyperparameter settings in Table~\ref{tab:training_params}.

% for training.
% and 10 hours of audio for validation. The validation set is randomly selected from 1.8k testing sets. 

To optimize the performance of our model, we conducted experiments with various NVIDIA NeMo architectures and assessed their training accuracy. Specifically, we evaluated the Conformer-CTC, Conformer-Transducer, and Fast-Conformer models. Among these, the Conformer-CTC model exhibited the best performance, achieving a training loss of approximately 11.2\%.

To accelerate the training process, we deployed a total of 16 $A100-40G$ GPUs to handle the entire dataset. Despite leveraging significant computational resources, the training still took approximately $112$ hours to complete.


% \subsection{Training}
% \label{sss:training}
% \subsection{Training Parameters}
% We trained the model using the AdamW optimizer for a duration of 4 days, 18 hours, and 16 minutes. The model was trained on a GPU cluster consisting of 16 GPUs, enabling efficient parallel processing. 
% This configuration allowed for the exploration of large-scale datasets and contributed to the model's robustness and accuracy. Table~\ref{tab:training_params} describes the key parameters of our model evaluation.

%Need to insert a table%

% \subsection{Evaluation Measures}

% epoch
% 15
% global_step
% 90,911.296875
% learning_rate
% 0.00007328778156079
% train_backward_timing
% 0.16302824020385742
% train_loss
% 11.203718185424805
% train_step_timing
% 1.630632400512695
% global_step
% 90,910
% training_batch_wer
% 0.14923124015331268
% val_loss
% 27.589672088623047
% val_wer
% 0.20338508486747744
% validation_step_timing
% 0.08939909934997559


\begin{table}
\centering
\begin{tabular}{lc}
\hline
\textbf{Parameter} & \textbf{Value}\\
\hline
epoch & 15\\
global\_step & 90,911 \\
learning\_rate & 0.000073287 \\ 
train\_backward\_timing & 0.1630282 \\ 
train\_loss & 11.203718 \\
training\_batch\_wer & 0.149231  \\ 
val\_loss & 27.58967 \\ 
val\_wer & 0.203385 \\
validation\_step\_timing & 0.089399 \\\hline\end{tabular}
\caption{Details of the hyperparameter settings.
% Training parameters details for baseline experiment using Fast Conformer model on 18k ASR dataset
}
\label{tab:training_params}
\end{table}


% The output parameters offer valuable insights into the performance of the ASR-trained model. 
The model underwent training for 15 epochs, completing approximately 90,911 global steps. The chosen learning rate was relatively low, contributing to stable and incremental updates of the model's parameters. Although the training loss suggests potential for further improvement, it does indicate a narrowing gap between predicted and actual values during the training phase.

% As for the WER value, it can be said that our model performed with good accuracy. However, the validation loss is relatively a little high. These parameters provide valuable insights into the model's performance and can guide future optimization efforts to enhance its accuracy and reduce errors. 

As for the WER the value indicates that our model performed with commendable accuracy. However, the validation loss remains somewhat elevated. These metrics offer valuable insights into the model's performance and serve as a road map for future optimization efforts.
% aimed at enhancing accuracy and minimizing errors.

% Here are some graphs that were occupied from the training output: (?)

\subsection{Data Post-processing}
During the evaluation of the test sets, we apply a set of post-processing on predicted transcription and human annotation to reduce unexpected symbols, confused words, and misleading alignment. We find that there are some typing issues during manual labeling.
% such as \textbf{ookar} is written as \textbf{eekar} $+$ \textbf{aakar}, \textbf{oowkar} is written as \textbf{eekar} $+$ \textbf{ohikar} and so on. 
To resolve this, a typing error minimization function is applied. In addition, we added two common normalization rules including: (i) number-to-word conversation and (ii) punctuation removal. 
% There is a valid reason to use punctuation removal as we find that the word alignment is misleading for some transcriptions. 


\begin{figure} [!ht]
\centering
\includegraphics[width=0.2\textwidth]{figures/bn_glm_samples.png}
\caption{Sample of GLM entries.}
\label{fig:glm}
\end{figure}

\paragraph{Minimizing the confusion due to writing style}
An extensive analysis of transcriptions indicates many words have different forms of writing (as shown in Figure \ref{fig:glm}) based on different character combinations. 
% is slightly different for the paired confused words. 
In some cases, both words of confused pairs are acceptable as people annotated in different ways, especially for country names, along with borrowed or code-mixed words.
% pronounced words from other language. 
% The evidence we found on google prediction on our annotated telephony and YouTube testset. 

To minimize these differences, we created a simple Global Mapping File (GLM) that allows different variations of the word to be accepted during evaluation. The GLM file contains entries for different homophones, primarily those with spelling variations.
% ways of writing a same word or proper nouns. 
We employed the most frequently occurring confusion patterns for the task, although this approach may not cover all possible variations.
% from \footnote{https://github.com/usnistgov/SCTK} toolkit. The confused word pairs which have similar pronunciation and sometimes ins translated form,  are added to the GLM file and ignored during WER Calculation.  

\subsection{Evaluation Metrics}
To evaluate the performance of the models, we used widely accepted metrics such as Word Error Rate (WER) and Character Error Rate (CER). The reported WER values are presented using the GLM and postprocessing of the hypothesis and references. 

\begin{table*} [!ht]
\centering
\begin{tabular}{llllll}
\hline
\textbf{Category} & \textbf{Duration(hr)}  & \textbf{MegaBNSpeech}  & \textbf{Google}  & \textbf{MMS} & \textbf{OOD-speech}\\
\hline
MegaBNSpeech-YT & 8.1 & 6.4/3.39&	28.3/18.88&	51.1/23.49&	44.4/33.43 \\
MegaBNSpeech-Tel & 1.9 & $^*$40.7/24.38	& $^*$59/41.26&	$^*$76.8/39.36&	$^*$69.9/52.93 \\
Fleurs & 3.42 & $^*$36.1/8.43	&24.6/8.54&	$^*$39.4/11.58&	29.5/13.97 \\
Common Voice & 16.5  & $^*$42.3/11.44 &	23.6/ 8.31&	$^*$48/14.72 & 23.6/10.49 \\
\hline
\end{tabular}
\caption{Reported Word error rate (WER) /character error rate (CER) on four test sets using four ASR systems. $^*$ represent the training portion of the corresponding test set \textbf{was not} present in the ASR model. % and $^+$ means not sure about training data.
}
\label{overall_evaluation_all_model}
\end{table*}
