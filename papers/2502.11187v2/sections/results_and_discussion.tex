

% News	2.5/1.21	18.9/10.46	52.2/21.65	32.3/20.71
% Talkshow	6/3.29	28/18.71	48.8/21.5	45.8/34.59
% Courses	6.8/3.79	30.8/21.64	50.2/23.52	46/35.99
% Drama	10.3/7.47	37.3/27.43	64.3/32.74	53.6/45.14
% Science	5/1.92	20.6/11.4	45.3/19.93	33.4/23.11
% Vlog	11.3/6.69	33/22.9	57.9/27.18	49.3/37.22
% Recpie	7.5/3.29	26.4/16.6	53.3/26.89	41.2/29.39
% Waz	9.6/5.45	33.3/23.1	57.3/27.46	59.9/50.38
% Movie	8/4.64	35.2/23.88	64.4/34.96	50.9/42.13


% \begin{table*}[!ht]
% \centering
% \begin{tabular}{llllll}
% \hline
% \textbf{Category} & \textbf{Duration(hr)}  & \textbf{MegaBNSpeech ASR}  & \textbf{Google ASR}  & \textbf{MMS ASR} & \textbf{OOD-speech}\\
% \hline
% News & 1.21 & 2.5/1.21 & 18.9/10.46  & 52.2/21.65& 32.3/20.71 \\
% Talkshow & 1.39 & 6/3.29 &	28/18.71&	48.8/21.5&	45.8/34.59 \\
% Courses & 3.81 & 6.8/3.79&	30.8/21.64&	50.2/23.52&	46/35.99 \\
% Drama & 0.03  & 10.3/7.47 &	37.3/27.43 &	64.3/32.74	&53.6/45.14 \\
% Science & 0.26  & 5/1.92	&20.6/11.4	&45.3/19.93	&33.4/23.11 \\
% Vlog & 0.18 & 11.3/6.69	&33/22.9&	57.9/27.18&	49.3/37.22	 \\
% Recipie & 0.58 & 7.5/3.29&	26.4/16.6&	53.3/26.89&	41.2/29.39 \\
% Waz & 0.49  & 9.6/5.45&	33.3/23.1&	57.3/27.46&	59.9/50.38 \\
% Movie & 0.1  & 8/4.64	&35.2/23.88&	64.4/34.96&	50.9/42.13 \\
% \hline
% \end{tabular}
% \caption{ Reported Word error rate (WER) /character error rate (CER) on different categories present in MegaBNSpeech - YT test set for four different ASR systems.}
% \label{category_wise_test_set_evaluation_all_model}
% \end{table*}



\section{Results and Discussion}
\label{sec:results}

We evaluate each model in 0-shot and 5-shot settings to assess their few-shot adaptability. Table~\ref{tab:benchmark-results} presents the detailed results for all models,\footnote{GPT~\cite{openai2023gpt4}, Llama~\cite{touvron2023llama}, Gemma~\cite{team2024gemma}, Qwen~\cite{chu2024qwen2}, SmolLM2~\cite{allal2025smollm2}, BLOOM~\cite{le2023bloom}, BongLLaMA~\cite{zehady2024bongllama}.} including the \titu{} variants. Table \ref{tab:benchmark-results} shows the accuracy of various models with less than or equal to 3b parameters and the GPT-davinci-002 model.


\begin{table}[htb!]
\centering
\setlength{\tabcolsep}{2pt} 
\scalebox{0.73}{%
% \begin{tabular}{p{3.5cm} p{0.75cm} p{1.25cm} p{1.2cm} p{1.2cm} p{1.2cm} p{1.2cm}}
\begin{tabular}{@{}lcccccc@{}}
\toprule
\textbf{Model} & \textbf{S} & \textbf{BN MMLU} & \textbf{BoolQ} & \textbf{CSQA} & \textbf{OBQA} & \textbf{PIQA} \\
\midrule

\multirow{2}{*}{davinci} 
 & 0 & 0.30 & 0.53 & 0.22 & 0.30 & 0.52 \\
 & 5 & - & - & - & - & - \\

\hline

\multirow{2}{*}{Llama-3.2-1b} 
 & 0 & 0.28 & 0.53 & 0.23 & 0.32 & 0.53 \\
 & 5 & 0.28 & 0.58 & 0.23 & 0.32 & 0.54 \\
 
\multirow{2}{*}{Llama-3.2-3b} 
 & 0 & \textbf{0.33} & 0.53 & 0.26 & 0.32 & 0.57 \\
 & 5 & 0.34 & \textbf{0.69} & 0.29 & 0.32 & 0.57 \\

\hline

\multirow{2}{*}{Gemma-2-2b} 
 & 0 & 0.29 & 0.56 & 0.26 & \textbf{0.34} & 0.56 \\
 & 5 & 0.32 & 0.60 & 0.28 & 0.33 & 0.56 \\

 \hline

\multirow{2}{*}{Qwen-2.5-0.5b} 
 & 0 & 0.30 & 0.53 & 0.21 & 0.31 & 0.54 \\
 & 5 & 0.31 & 0.58 & 0.22 & 0.30 & 0.53 \\

\multirow{2}{*}{Qwen-2.5-1.5b} 
 & 0 & \textbf{0.33} & \textbf{0.62} & 0.23 & 0.29 & 0.53 \\
 & 5 & \textbf{0.35} & 0.68 & 0.23 & 0.30 & 0.52 \\

 \hline

\multirow{2}{*}{SmolLM2-135m} 
 & 0 & 0.23 & 0.53 & 0.22 & 0.31 & 0.52 \\
 & 5 & 0.23 & 0.51 & 0.21 & 0.30 & 0.52 \\

\multirow{2}{*}{SmolLM2-360m} 
 & 0 & 0.25 & 0.53 & 0.20 & 0.30 & 0.54 \\
 & 5 & 0.24 & 0.52 & 0.21 & 0.29 & 0.53 \\

\multirow{2}{*}{SmolLM2-1.7b} 
 & 0 & 0.29 & 0.53 & 0.22 & 0.31 & 0.53 \\
 & 5 & 0.30 & 0.55 & 0.21 & 0.30 & 0.53 \\

 \hline

\multirow{2}{*}{BLOOM-560m} 
 & 0 & 0.23 & 0.53 & 0.26 & 0.31 & 0.54 \\
 & 5 & 0.23 & 0.53 & 0.26 & 0.28 & 0.54 \\

\multirow{2}{*}{BLOOM-1b1} 
 & 0 & 0.26 & 0.56 & 0.27 & 0.31 & 0.54 \\
 & 5 & 0.23 & 0.58 & 0.27 & 0.31 & 0.55 \\

\multirow{2}{*}{BLOOM-1b7} 
 & 0 & 0.27 & 0.53 & 0.27 & 0.32 & 0.55 \\
 & 5 & 0.27 & 0.59 & 0.30 & 0.31 & 0.56 \\

\multirow{2}{*}{BLOOM-3b} 
 & 0 & 0.26 & 0.53 & 0.27 & 0.33 & \textbf{0.58} \\
 & 5 & 0.23 & 0.53 & 0.32 & 0.31 & 0.58 \\

 \hline

\multirow{2}{*}{BongLLaMA-3.2-1b} 
 & 0 & 0.25 & 0.53 & 0.22 & 0.33 & 0.52 \\
 & 5 & 0.26 & 0.53 & 0.24 & 0.31 & 0.53 \\

\multirow{2}{*}{BongLLaMA-3.2-3b} 
 & 0 & 0.30 & 0.53 & 0.21 & 0.27 & 0.51 \\
 & 5 & 0.33 & 0.54 & 0.20 & 0.29 & 0.50 \\

 \hline

\multirow{2}{*}{TituLLM-1b-v2.0} 
 & 0 & 0.25 & 0.53 & 0.26 & 0.32 & \textbf{0.58} \\
 & 5 & 0.25 & 0.51 & 0.28 & 0.33 & 0.57 \\

\multirow{2}{*}{TituLLM-3b-v2.0} 
 & 0 & 0.25 & 0.53 & \textbf{0.28} & 0.32 & \textbf{0.58} \\
 & 5 & 0.25 & 0.54 & \textbf{0.33} & \textbf{0.35} & \textbf{0.60} \\

\bottomrule
\end{tabular}
}
\vspace{-0.2cm}
\caption{Benchmark results (normalized accuracy) across models and datasets for 0-shot and 5-shot settings. S: Shots, BN MMLU: Bangla MMLU, davinci: GPT-davinci-002, }
\label{tab:benchmark-results}
\vspace{-0.2cm}
\end{table}

% \begin{table*}[htb!]
% \centering
% \setlength{\tabcolsep}{3pt} 
% \scalebox{0.72}{%
% \centering
% \begin{tabular}{@{}lrrrrrr|lrrrrrr@{}}
% \toprule
% \multicolumn{1}{c}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{Shots}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Bangla \\ MMLU\end{tabular}}} & \multicolumn{1}{c}{\textbf{BoolQ}} & \multicolumn{1}{c}{\textbf{CSQA}} & \multicolumn{1}{c}{\textbf{OBQA}} & \multicolumn{1}{c}{\textbf{PIQA}} & \multicolumn{1}{|c}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{Shots}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Bangla \\ MMLU\end{tabular}}} & \multicolumn{1}{c}{\textbf{BoolQ}} & \multicolumn{1}{c}{\textbf{CSQA}} & \multicolumn{1}{c}{\textbf{OBQA}} & \multicolumn{1}{c}{\textbf{PIQA}} \\ \midrule
% GPT-davinci-002 & 0 & 0.30 & 0.53 & 0.22 & 0.30 & 0.52 & BLOOM-560m & 0 & 0.23 & 0.53 & 0.26 & 0.31 & 0.54 \\
% GPT-davinci-002 & 5 & - & - & - & - & - & BLOOM-560m & 5 & 0.23 & 0.53 & 0.26 & 0.28 & 0.54 \\
% Llama-3.2-1b & 0 & 0.28 & 0.53 & 0.23 & 0.32 & 0.53 & BLOOM-1b1 & 0 & 0.26 & 0.56 & 0.27 & 0.31 & 0.54 \\
% Llama-3.2-1b & 5 & 0.28 & 0.58 & 0.23 & 0.32 & 0.54 & BLOOM-1b1 & 5 & 0.23 & 0.58 & 0.27 & 0.31 & 0.55 \\
% Llama-3.2-3b & 0 & \textbf{0.33} & 0.53 & 0.26 & 0.32 & 0.57 & BLOOM-1b7 & 0 & 0.27 & 0.53 & 0.27 & 0.32 & 0.55 \\
% Llama-3.2-3b & 5 & 0.34 & \textbf{0.69} & 0.29 & 0.32 & 0.57 & BLOOM-1b7 & 5 & 0.27 & 0.59 & 0.30 & 0.31 & 0.56 \\
% gemma-2-2b & 0 & 0.29 & 0.56 & 0.26 & \textbf{0.34} & 0.56 & BLOOM-3b & 0 & 0.26 & 0.53 & 0.27 & 0.33 & 0.58 \\
% gemma-2-2b & 5 & 0.32 & 0.60 & 0.28 & 0.33 & 0.56 & BLOOM-3b & 5 & 0.23 & 0.53 & 0.32 & 0.31 & 0.58 \\
% Qwen-2.5-0.5b & 0 & 0.30 & 0.53 & 0.21 & 0.31 & 0.54 & BanglaLLama-3.2-1b & 0 & 0.25 & 0.53 & 0.22 & 0.33 & 0.52 \\
% Qwen-2.5-0.5b & 5 & 0.31 & 0.58 & 0.22 & 0.3 & 0.53 & BanglaLLama-3.2-1b & 5 & 0.26 & 0.53 & 0.24 & 0.31 & 0.53 \\
% Qwen-2.5-1.5b & 0 & 0.33 & \textbf{0.62} & 0.23 & 0.29 & 0.53 & BanglaLLama-3.2-3b & 0 & 0.30 & 0.53 & 0.21 & 0.27 & 0.51 \\
% Qwen-2.5-1.5b & 5 & \textbf{0.35} & 0.68 & 0.23 & 0.3 & 0.52 & BanglaLLama-3.2-3b & 5 & 0.33 & 0.54 & 0.20 & 0.29 & 0.50 \\
% SmolLM2-135m & 0 & 0.23 & 0.53 & 0.22 & 0.31 & 0.52 & TituLLM-1b-v2.0 & 0 & 0.25 & 0.53 & 0.26 & 0.32 & \textbf{0.58} \\
% SmolLM2-135m & 5 & 0.23 & 0.51 & 0.21 & 0.30 & 0.52 & TituLLM-1b-v2.0 & 5 & 0.25 & 0.51 & 0.28 & 0.33 & 0.57 \\
% SmolLM2-360m & 0 & 0.25 & 0.53 & 0.2 & 0.30 & 0.54 & TituLLM-3b-v2.0 & 0 & 0.25 & 0.53 & \textbf{0.28} & 0.32 & \textbf{0.58} \\
% SmolLM2-360m & 5 & 0.24 & 0.52 & 0.21 & 0.29 & 0.53 & TituLLM-3b-v2.0 & 5 & 0.25 & 0.54 & \textbf{0.33} & \textbf{0.35} & \textbf{0.60} \\
% SmolLM2-1.7b & 0 & 0.29 & 0.53 & 0.22 & 0.31 & 0.53 &  &  &  &  &  &  &  \\
% SmolLM2-1.7b & 5 & 0.30 & 0.55 & 0.21 & 0.30 & 0.53 &  &  &  &  &  &  &  \\ \bottomrule
% \end{tabular}
% }
% \vspace{-0.3cm}
% \caption{Benchmark results (accuracy) across models and datasets for 0-shot and 5-shot settings.}
% \label{tab:benchmark-results}
% \vspace{-0.3cm}
% \end{table*}

\noindent  \textbf{Bangla MMLU:} In the 0-shot setting, both the TituLLM-1b and TituLLM-3b models score 0.25, placing them in the mid-range relative to other 1b--3b models in the Bangla MMLU benchmark. Neither model shows gains when moving to the 5-shot setting (both remain at 0.25), suggesting that additional examples do not substantially improve performance for this specialized knowledge benchmark. It is possible that the domain-specific knowledge required for MMLU-like tasks is not adequately captured by our model. The primary reason behind this can be the lack of extensive pertaining. We have trained our model with only $\sim37b$
% \textit{36.8b} 
tokens for one epoch. As a result, the model could not capture the full knowledge base \cite{hoffmann2022trainingcomputeoptimallargelanguage}. Another reason behind this can be diversity in datasets. For example, Llama and Qwen models are trained on a high volume of English datasets that have helped these models to have a better knowledge base. 

% Another reason behind this can be seen in the detailed performance analysis (Appendix: ) for the Bangla MMLU dataset. It can be noticed that Qwen2.5-1.5b has done exceptionally well in some topics like English and ICT. If we analyze the questions related to these topics, we will find that many questions contain a mixture of Bangla and English words. As Qwen2.5-1.5b is trained on a high volume of English datasets, the inherent knowledgebase of the model helps it to relate the answer to the question properly. 

\noindent  \textbf{BoolQ:} The BoolQ dataset measures the performance of the model for yes/no question-answering in Bangla. TituLLM-1b achieves 0.53 in the 0-shot setting but drops slightly to 0.51 in the 5-shot setting. In contrast, TituLLM-3B moves from 0.53 (0-shot) to 0.54 (5-shot). However, Llama-3b and Qwen-2.5-1.5b have done much better in this task. As the context length for all BoolQ data was as large as a News document, TituLLM's performance may drop for long contexts. It suggests further pertaining to the model should be done with long contexts \cite{chowdhery2022palmscalinglanguagemodeling,kaplan2020scalinglawsneurallanguage}.

\noindent  \textbf{CSQA, OBQA, and PIQA:} Commonsense reasoning tasks often challenge smaller-scale language models. The accuracy of the 3B variant of TituLLM, starts at 0.28 (0-shot) and exhibits a more pronounced jump to 0.33 (5-shot) which is the maximum among all models. TituLLM-1b has also shown decent performance on the CSQA dataset. 

OBQA requires both textbook knowledge and reasoning. Similar to CSQA, TituLLM-3b shows superior performance in this dataset 0.35. Both the dataset's results suggest that TituLLM's reasoning capability is better than other base models.

PIQA tests physical commonsense knowledge. TituLLM-3b model shows better performance in this task too with an accuracy of 0.60. By observing the results on the CSQA, OBQA, and PIQA datasets we can say that the model has captured Bangla Language specific reasoning well in spite of being trained with a smaller dataset than others but the results from MMLU and BoolQ shows the impact of limited training. 

\noindent \textbf{Performance of tokenizer: } The superior performance of our models in reasoning tasks is mainly an impact of our extended tokenizer. To justify this, we can observe the results of the BongLLaMa models. These models are continual pretrained models with existing open-source Bangla text corpus. If only the dataset could improve the performance then that would be reflected in BongLLaMA models. But we can see they are performing similarly to Llama models. To have an interpretation of our extended tokenizer's performance we can look into Figure-\ref{fig:tokens}. The figure shows Llama tokens and TituLLM tokens for a simple sentence in Bangla with two of the most common words. We can see that Llama tokenizer splits the text into character and byte levels. On the other hand, TituLLM tokenizes the sentence into word or subword levels. As a result, TituLLM can deliver more meaningful tokens than Llama for Bangla text. This is an important advantage of TituLLM that not only enables TituLLM to perform better with smaller datasets but also ensures low latency during inference.

\begin{figure}[t]
  \centering
\includegraphics[width=1.0\columnwidth]{figures/tokens.png}
  \caption{Example of tokenization of Llama and TituLLM tokenizers.}
  \vspace{-0.2cm}
  \label{fig:tokens}
  \vspace{-0.2cm}
\end{figure}

% \subsection{Comparison and Trends}

% % \begin{itemize}[noitemsep,topsep=0pt,labelsep=.5em]
% \noindent \textbf{Parameter Scaling Effects.} Across nearly all tasks, the 3B variant outperforms or equals the 1B model. The most pronounced gains appear in tasks requiring deeper commonsense reasoning and knowledge retrieval (e.g., CommonsenseQA BN and OpenBookQA BN).

% \noindent \textbf{Few-Shot Sensitivity.} Although the 1B model remains stable or decreases slightly on some tasks (e.g., BoolQ BN and PIQA BN), the 3B model typically extracts additional benefits from 5-shot prompts. This discrepancy likely stems from higher representational capacity in the larger model.

% \noindent \textbf{Task-Specific Performance.} Neither Titulm model displays strong improvement on Bangla MMLU when given 5-shot examples. For more specialized or domain-specific questions, a small number of examples may be insufficient. Future work could explore larger sets of exemplars or specialized fine-tuning to address domain expertise gaps.
% % \end{itemize}




