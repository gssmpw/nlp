\section{Pretraining Data}


 \subsection{Tokens in \titu{}}
Table \ref{tab:titulm_tokens} presents the token distribution used for pretraining \titu{}, which are collected from various sources. The largest portion, amounting to 18.80 billion tokens, originates from a deduplicated corpus combining Common Crawl, Books, and Sangraha data. Additionally, synthetic data generated for the model contributes 7.06 billion tokens, comprising translated (1.47b), romanized (3.87b), conversational (0.42b), and audio-transcribed (1.30b) text. The Sangraha dataset further adds 10.94 billion tokens, split between translated (4.26b) and romanized (6.68b) subsets. In total, the \titu{} pretraining corpus consists of $\sim37$ billion tokens, incorporating both natural and synthetic data sources to enhance linguistic diversity and representation.

\begin{table}[h]
    \centering
    \begin{tabular}{p{4cm} c}
        \hline
        \textbf{Data} & \textbf{ \# Tokens (B)} \\
        \hline
        Common Crawl, Books, and Sangraha & 18.80 \\ \midrule
        Synthetic & \\
        \quad Translated & 1.47 \\
        \quad Romanized & 3.87 \\
        \quad Conversation & 0.42 \\
        \quad Audio Transcription & 1.30 \\ \midrule
        Sangraha & \\
        \quad Translated & 4.26 \\
        \quad Romanized & 6.68 \\
        \hline
        \textbf{Total} & \textbf{36.80} \\
        \hline
    \end{tabular}
    \caption{Token distribution for \titu{} pretraining, including contributions from natural and synthetic data sources.}
    \label{tab:titulm_tokens}
\end{table}


\subsection{Rules for Data Filtering}
\label{ssec:rule_data_filtering}
% \todo[inline]{to add}

For pretraining data filtering, we applied a set of carefully designed hand-crafted rules, which are outlined below.

\noindent \textbf{Line Ending with Terminal Punctuation:}  
Determines whether a line concludes with a terminal punctuation mark, including \texttt{``.''}, \texttt{``!''}, \texttt{``?''}, and \texttt{``”''}. Helps assess sentence completeness and filter out incomplete or malformed content.

\noindent \textbf{Line Word Numbers:}  
Calculates the number of words in each line after normalization. Provides insights into whether text consists of single-word lines, short fragments, or full-length sentences.

\noindent \textbf{Line Start with Bullet Points:}  
Identifies lines starting with bullet points, including Unicode symbols like \texttt{\textbackslash u2022}, \texttt{\textbackslash u2023}, \texttt{\textbackslash u25B6}, and others. Useful for recognizing and handling structured lists.

\noindent \textbf{Line Numerical Character Fraction:}  
Measures the proportion of numerical characters in each line. Helps identify lines dominated by numbers, such as statistics, mathematical expressions, or financial reports.

\noindent \textbf{Is Adult URL:}  
Flags documents originating from adult content URLs for filtering inappropriate or explicit content.

\noindent \textbf{Document Languages Identification:} 
Uses FastText \cite{grave2018learning} for language identification, extract language percentages, and filter documents where Bangla meets the specified threshold.

\noindent \textbf{Document Sentence Count:}  
Counts the number of sentences using BNLP Tokenizer \cite{sarker2021bnlp}  for Bangla and NLTK Tokenizer \footnote{\url{}}for English , and it helps to assess document length and complexity.




\noindent \textbf{Document Word Count:}  
Computes the total number of words after normalization. Provides an overall measure of document length and verbosity.



\noindent \textbf{Document Mean Word Length:}  
Calculates the average length of words after normalization. Longer words often indicate more sophisticated vocabulary.

\noindent \textbf{Document Word to Symbol Ratio:}  
Determines the ratio of symbols (\texttt{``\#''}, \texttt{``...''}, \texttt{``…''}) to words. A high ratio may indicate unconventional formatting or non-standard text.

\noindent \textbf{Document Fraction End with Ellipsis:}  
Computes the fraction of lines ending with an ellipsis (\texttt{``...''}, \texttt{``…''}), which may suggest incomplete thoughts or trailing sentences.

\noindent \textbf{Document Unique Word Fraction:}  
Measures the fraction of unique words in a document, providing insight into vocabulary diversity and repetition.

\noindent \textbf{Document Unigram Entropy:}  
Calculates the entropy of the unigram distribution, measuring lexical variety using the formula:
\[
\sum \left(-\frac{x}{\text{total}} \log \frac{x}{\text{total}}\right)
\]
where \( x \) represents counts of unique words in the normalized content. Higher entropy suggests greater lexical diversity.

\noindent \textbf{Document Stop Word Fraction:}  
Determines the ratio of stop words (e.g., ``the,'' ``and,'' ``is'') to total words. A high ratio may indicate informal text, while a low ratio may suggest technical or keyword-dense content.

\noindent \textbf{Fraction of Characters in Top N-Gram:}  
Measures the proportion of characters within frequently occurring word n-grams. Helps assess text repetitiveness and structure.

% \noindent \textbf{Fraction of Characters in Duplicate Word N-Gram:}  
% Calculates the fraction of characters contained within repeated word n-grams, helping to identify redundant or excessively repetitive content.

% \noindent \textbf{Document Top Language Detection:}  
% Determines the primary language of a document, ensuring content is processed in the correct linguistic context and aiding in language-based filtering.

\noindent \textbf{Document Content Classification:}  
Categorizes document content based on profanity, vulgarity, or toxicity to filter inappropriate material.

\noindent \textbf{Document Bad Words Count:}  
Counts offensive or inappropriate words, serving as a stricter filter for explicit content.

% \noindent \textbf{Document Curly Bracket Ratio:}  
% Calculates the ratio of curly brackets (\texttt{``\{''}, \texttt{``\}''}) to total characters. A high ratio may indicate code, structured data, or formulaic content.

\noindent \textbf{Document Bracket Ratio:}  
Determines the ratio of all bracket types (e.g., \texttt{``()''}, \texttt{``[]''}, \texttt{``\{\}''}) to total characters. Useful for detecting technical or structured text such as programming code, mathematical expressions, or legal documents.


 By leveraging language-specific tokenizers and tools, we ensured that the evaluation framework effectively captured the characteristics and patterns unique to Bangla text, enabling robust filtering and alignment with intended use cases.

\subsection{Rules for Cleaning OCR-Extracted Text}
\label{ssec:app_rules_google_ocr}
% \todo[inline]{to add}
% We have collected text data from a diverse range of Bangla books, covering genres such as novels, essays, poetry, and academic materials. 
Figure~\ref{fig:data_collection_final} provides an overview of the book data collection process. 
% The following steps outline the procedure for obtaining the final version of the raw text data from these books.
We applied the following rules to filter the text from the OCR-extracted text. 

\begin{itemize}[noitemsep,topsep=0pt,labelsep=.5em] 
\item \textbf{Use of KenLM \cite{heafield2011kenlm}:} KenLM is an efficient statistical language modeling toolkit commonly used for constructing n-gram language models. We trained a language model using high-quality text data, which enabled us to calculate word and sentence scores for the OCR-extracted text. A histogram of these scores was plotted, and thresholds were set based on the distribution to identify poorly recognized sections. The threshold for filtering low-quality text was set at 95\%, meaning that only 95\% of the text was retained.

% The threshold for filtering low-quality text was set at 95\%, meaning that only text with a score above the 95th percentile of the histogram was retained. Mathematically, this can be expressed as:
% \[
% \text{Score Threshold} = \text{Percentile}(95)
% \]
% Where the \(\text{Percentile}(95)\) function returns the score value at the 95th percentile of the word and sentence scores.
    \item \textbf{Word and Sentence Count in Documents:} We calculated the average number of words and sentences per page in the collected books. A minimum threshold for these counts was established to help filter out books with low-quality text. 
    \item \textbf{Percentage of Correct Bangla Words:} We compiled a list of common Bangla words and computed the percentage of these words in each book. A threshold was determined based on the overall distribution of Bangla word occurrences across the corpus.
\end{itemize}

Once these thresholds were established, they were applied across the entire dataset, resulting in the filtering of approximately 50\% of the raw text data.

\section{Tokenizer Details}
Table \ref{tab:tokens-per-word} presents the Tokens per Word (TPW) values for different variants of the Llama-3.2 model. The base Llama-3.2 model has the highest TPW at 7.8397, while the extended Llama-3.2-plus models, with varying context lengths (32K to 96K), exhibit progressively lower TPW values. 

\begin{table}[h!]
\centering
\begin{tabular}{lc}
\hline
\textbf{Model} & \textbf{TPW} \\
\hline
\verb|Llama-3.2|       & 7.8397 \\
\verb|Llama-3.2-plus-32K|     & 2.1346 \\
\verb|Llama-3.2-plus-48K|     & 1.9029 \\
\verb|Llama-3.2-plus-64K|     & 1.7946 \\
\verb|Llama-3.2-plus-80K|     & 1.7370 \\
\verb|Llama-3.2-plus-96K|     & 1.7034 \\
\hline
\end{tabular}
\caption{Tokens per word (TPW) for different Llama models.}
\label{tab:tokens-per-word}
\end{table}

\section{Expressive Semantic Translation (EST)}
\label{sec:app_est}

\input{sections/EST_lite}

We evaluate EST method on a benchmarking test dataset by both well-known evaluation methods and LLM-based methods. 

\subsection{Evaluation}
\noindent  \textbf{Data Selection: } 
The dataset that has been utilized here for evaluation was taken from \cite{hasan2020not} where a customized sentence segmenter was used for Bangla and two novel methods, namely aligner ensembling and batch filtering, were used to develop a high-quality parallel corpus of Bangla and English with 2.75 million sentence pairings. The 1000 pairs that comprise up the test set of this data were created with extensive quality control and used in this assessment.   

\noindent \textbf{Evaluation metrics: } We employ the BLEU Score \cite{papineni2002bleu}, SacreBLEU Score \cite{post2018call}, BERT Score (F1) \cite{zhang2020bertscore}, and an LLM-Based Evaluation.  

\textbf{LLM-based evaluation: } This approach uses a large language model (GPT-4o) to assess translations qualitatively. The LLM is instructed by the given prompt to assess a Bangla translation against a reference text using the following criteria: accuracy (measures translation accuracy and semantic richness), fluency, readability, and faithfulness. A score between 1 and 10 is then assigned by the LLM, along with a rationale for each translation. Finally, an average score is computed for each translation method. LLM-based evaluation is more flexible and human-like than token-based methods since it may assess more semantic variations and fluency.

\noindent \textbf{Result: } Our comparison of the EST method against industry-standard models like Google Translation API and advanced systems like GPT-4o and Gemini highlights EST's superior performance. As shown in Table \ref{tab:translation_comparison}, EST leads with remarkable scores in BLEU (\textbf{0.57}), SacreBLEU (\textbf{49.50}), and BERTScore (F1) (\textbf{0.93}). These metrics underscore EST's unparalleled accuracy and contextual richness, with a notable \textbf{20-point} lead in SacreBLEU over the closest competitor. These results affirm the efficacy of EST's iterative refinement in elevating translation quality.


\begin{table}[h]
\centering
\setlength{\tabcolsep}{1pt} 
\scalebox{0.80}{%
\begin{tabular}{lrrrr}
\toprule
\textbf{Translator} & \textbf{BLEU} & \textbf{SBLEU} & \textbf{BS (F1)} & \textbf{LLM Score}  \\ 
Google                    & 0.33          & 29.00              & 0.91                    & \textbf{8.96} \\ 
GPT-4o                    & 0.26          & 22.05              & 0.90                    & 8.91          \\ 
IndicTrans2               & 0.27          & 23.74              & 0.90                    & 8.73          \\ 
Gemini-1.5-pro            & 0.31          & 27.08              & 0.89                    & 8.80          \\ 
 EST  & \textbf{0.57} & \textbf{49.50}     & \textbf{0.93}           & 8.95         \\ \bottomrule
\end{tabular}
}
\caption{Comparison of different translation models based on BLEU, SacreBLEU (SBLEU), BERTScore (BS) F1, and LLM scores.}
\label{tab:translation_comparison}
\end{table}


\section{Conversation Data Generation Prompt}
Our methodology for generating Bangla conversational texts involves two specialized roles: the Junior Content Writer and the Senior Content Writer as highlighted in Figure \ref{tab:agent_details}. The Junior initiates dialogues based on culturally significant topics. The Senior meticulously reviews these texts to ensure grammatical precision and enhance quality. This structured approach enables replicable, high-standard conversation generation for NLP research. 


\begin{table}[htb!]
\centering
\setlength{\tabcolsep}{3pt} 
\scalebox{0.8}{%
\begin{tabular}{p{2cm}|p{2.5cm}|p{4.5cm}}
\toprule
\textbf{Component} & \textbf{Description} & \textbf{Details} \\ \midrule
Input Text Snippet & Bangla text snippet provided to initiate the conversation. & The text is a prompt that provides a topic for conversation, such as ``Rabindranath Tagore's contributions to Bengali art'', sourced from educational and cultural databases like Wikipedia or Banglapedia. This serves as the basis for the conversation generation task. \\ \midrule
Junior Content Writer & Agent tasked with generating the initial conversation. & \textbf{Role:} Content Creator \newline \textbf{Goal:} To generate high-quality, coherent, and engaging conversational text in Bangla. \newline \textbf{Backstory:} As an expert in Bangla language conversation generation, your responsibility includes initiating and maintaining a dialogue that is both interesting and relevant to the given topic. \newline \textbf{Capabilities:} Produces verbose outputs to ensure detailed and extensive dialogue. No delegation is allowed. Employs a language learning model (LLM) without additional tools. \\ \midrule
Senior Content Writer & Agent tasked with reviewing and refining the conversation. & \textbf{Role:} Examiner \newline \textbf{Goal:} To edit and enhance the quality of Bangla conversational text, ensuring it meets high standards of grammar and coherence. \newline \textbf{Backstory:} You review the conversation generated by the Junior Content Writer, focusing on grammatical accuracy, linguistic quality, and content relevance, making necessary revisions to uphold quality standards. \newline \textbf{Capabilities:} Provides verbose feedback and detailed edits, uses a language model (LLM) for text correction and improvement, and operates independently without delegation. \\ \bottomrule
\end{tabular}
}
\caption{Detailed roles and tasks of agents for generating Bangla conversational text.}
\label{tab:agent_details}
\end{table}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/conversation.png}
    \vspace{-0.3cm}
    \caption{Conversation example between two agents on Rabindranath Tagore's contributions to Bangla art.
    }
    \label{fig:data_collection_final}
    \vspace{-0.3cm}
\end{figure*}


% \begin{longtable}{|p{2cm}|p{13cm}|}
%     \hline
%     \textbf{এজেন্ট} & \textbf{কথোপকথন} \\
%     \hline
%     \endfirsthead

%     \hline
%     \textbf{এজেন্ট} & \textbf{কথোপকথন} \\
%     \hline
%     \endhead

%     \hline
%     \endfoot

%     \hline
%     \endlastfoot

%     এজেন্ট-1 & রবীন্দ্রনাথ ঠাকুরের প্রতিভা সম্পর্কে আপনার কি ধারণা? \\
%     \hline
%     এজেন্ট-2 & রবীন্দ্রনাথ ঠাকুর ছিলেন একজন মহান কবি, সাহিত্যিক, দার্শনিক ও সঙ্গীতজ্ঞ। তিনি বাংলা সাহিত্যের অন্যতম শ্রেষ্ঠ কবি হিসেবে বিবেচিত হন। তার রচনার জন্য তিনি ১৯১৩ সালে সাহিত্যে নোবেল পুরস্কার পেয়েছিলেন। তিনি বাংলাদেশের জাতীয় সঙ্গীত "আমার সোনার বাংলা"র রচয়িতা। তার গানগুলি বাংলা সংস্কৃতির একটি অবিচ্ছেদ্য অঙ্গ। \\
%     \hline
%     এজেন্ট-1 & রবীন্দ্রনাথ ঠাকুরের কোন কোন রচনা আপনি পড়েছেন? \\
%     \hline
%     এজেন্ট-2 & আমি রবীন্দ্রনাথ ঠাকুরের বেশ কিছু রচনা পড়েছি। তার মধ্যে উল্লেখযোগ্য হল "গীতাঞ্জলি", "গল্পগুচ্ছ", "চিত্রা", "কাবুলিওয়ালা"। তার রচনার ভাষা, চরিত্রায়ন ও কাব্যিক সৌন্দর্য অতুলনীয়। \\
%     \hline
%     এজেন্ট-1 & রবীন্দ্রনাথ ঠাকুরের গানের সম্পর্কে আপনার মতামত কি? \\
%     \hline
%     এজেন্ট-2 & রবীন্দ্রনাথ ঠাকুরের গান বাংলা সঙ্গীতের অন্যতম উজ্জ্বল অধ্যায়। তার গানের সুর, কথা ও ভাব সবই অত্যন্ত মনোমুগ্ধকর। তার গানগুলি বাংলা সংস্কৃতির একটি অবিচ্ছেদ্য অঙ্গ এবং আজও লোকেরা তার গান শুনে আনন্দ পায়। \\
%     \hline
%     এজেন্ট-1 & রবীন্দ্রনাথ ঠাকুরের কোন কোন গান আপনার প্রিয়? \\
%     \hline
%     এজেন্ট-2 & রবীন্দ্রনাথ ঠাকুরের অনেক গানই আমার প্রিয়। তার মধ্যে উল্লেখযোগ্য হল "একলা চলো রে", "আমার সোনার বাংলা", "পথের পাঁচালী", "আজি বাঁশি বাজে"। তার গানগুলি আমার মনে এক অনন্য অনুভূতি জাগিয়ে তোলে। \\
%     \hline
%     এজেন্ট-1 & রবীন্দ্রনাথ ঠাকুরের কোন কোন চরিত্র আপনার পছন্দের? \\
%     \hline
%     এজেন্ট-2 & রবীন্দ্রনাথ ঠাকুরের গল্প ও উপন্যাসের চরিত্রগুলিও অত্যন্ত মনোমুগ্ধকর। তার চরিত্রগুলির মধ্যে উল্লেখযোগ্য হল "গোরা", "বিনোদিনী", "কাবুলিওয়ালা", "রাসমণি"। তার চরিত্রগুলির বাস্তবতা এবং জটিলতা পাঠকদের মনে দীর্ঘস্থায়ী ছাপ রেখে যায়। \\
%     \hline
%     এজেন্ট-1 & রবীন্দ্রনাথ ঠাকুরের কোন কোন দার্শনিক চিন্তা আপনাকে প্রভাবিত করেছে? \\
%     \hline
%     এজেন্ট-2 & রবীন্দ্রনাথ ঠাকুর একজন দার্শনিক হিসেবেও অত্যন্ত গুরুত্বপূর্ণ। তার দার্শনিক চিন্তাগুলির মধ্যে উল্লেখযোগ্য হল "বিশ্বভারতী", "শান্তিনিকেতন", "ব্রহ্মচর্য"। তার দার্শনিক চিন্তাগুলি মানবিকতা, সহিষ্ণুতা ও শান্তির উপর গুরুত্ব দেয়। \\
%     \hline
%     এজেন্ট-1 & রবীন্দ্রনাথ ঠাকুরের কোন কোন সামাজিক কাজ আপনাকে প্রভাবিত করেছে? \\
%     \hline
%     এজেন্ট-2 & রবীন্দ্রনাথ ঠাকুর একজন সামাজিক সংস্কারক হিসেবেও অত্যন্ত গুরুত্বপূর্ণ। তিনি নারী শিক্ষা, দলিতদের অধিকার, গ্রামীণ উন্নয়ন প্রভৃতি ক্ষেত্রে অবদান রেখেছেন। তার সামাজিক কাজগুলি সমাজে ইতিবাচক প্রভাব ফেলেছে। \\
%     \hline
%     এজেন্ট-1 & রবীন্দ্রনাথ ঠাকুরের প্রতি আপনার কি কোনো সমালোচনা আছে? \\
%     \hline
%     এজেন্ট-2 & রবীন্দ্রনাথ ঠাকুর একজন অসাধারণ ব্যক্তিত্ব ছিলেন, তবে তিনিও মানুষ ছিলেন এবং তিনিও ভুল করেছেন। তার কিছু সমালোচনা রয়েছে, যেমন তার সমসাময়িক রাজনৈতিক পরিস্থিতিতে তার ভূমিকা এবং তার ব্যক্তিগত জীবনের কিছু ঘটনা। \\
%     \hline
%     এজেন্ট-1 & রবীন্দ্রনাথ ঠাকুরের লেখা ও গানের সবচেয়ে গুরুত্বপূর্ণ বৈশিষ্ট্যগুলি কী কী বলে মনে করেন? \\
%     \hline
%     এজেন্ট-2 & রবীন্দ্রনাথ ঠাকুরের লেখা ও গানের সবচেয়ে গুরুত্বপূর্ণ বৈশিষ্ট্যগুলি হল:
%     \begin{itemize}
%         \item তাঁর রচনার ভাষাটি খুবই সহজ ও সাবলীল।
%         \item তাঁর রচনায় বাংলার লোকজ সংস্কৃতির প্রভাব স্পষ্ট।
%         \item তাঁর রচনায় প্রকৃতি, ভালোবাসা ও সৌন্দর্যের প্রতি গভীর অনুরাগ প্রকাশ পায়।
%         \item তাঁর গানের সুর খুবই সহজ ও মধুর।
%         \item তাঁর গানের কথায় জীবনের গভীর দর্শন ও অন্তর্দৃষ্টি প্রকাশ পায়।
%     \end{itemize} \\
%     \hline
% \end{longtable}


% \section{Conversation Data Generation Prompt}


% \begin{table}[ht]
% \centering
% \caption{Roles and Tasks of Agents in Bengali Conversation Generation}
% \label{tab:agent_roles_tasks}
% \begin{tabular}{|p{3cm}|p{5cm}|p{8cm}|}
% \hline
% \textbf{Component} & \textbf{Description} & \textbf{Details} \\ \hline
% Input Text Snippet & Bengali text snippet provided to initiate the conversation. & The text is a prompt on a specific topic, e.g., Rabindranath Tagore's impact on Bengali art, derived from resources like Wikipedia or Banglapedia. \\ \hline
% Junior Content Writer & Agent tasked with generating the initial conversation. & \textbf{Role:} Content Creator \newline \textbf{Goal:} Generating high-quality Bengali conversational text \newline \textbf{Backstory:} Expert in Bengali language conversation generation. Initiates and maintains conversation based on the input text. \newline \textbf{Capabilities:} Verbose output, no delegation allowed, uses a language model (LLM). \\ \hline
% Senior Content Writer & Agent tasked with reviewing and editing the conversation. & \textbf{Role:} Examiner \newline \textbf{Goal:} Editing and regenerating good quality Bengali text conversation \newline \textbf{Backstory:} Reviews the conversation generated by the Junior Content Writer for grammatical accuracy and overall quality, making necessary revisions. \newline \textbf{Capabilities:} Verbose output, no delegation allowed, uses a language model (LLM). \\ \hline
% \end{tabular}
% \end{table}

% The role of the two agents were junior content writer and senior content writer. Here, the user provides a query. The query is:

% \begin{quote}
% Using the provided Bengali text snippet, \texttt{\textbackslash n=====\textbackslash n'\{data[iteration]\}'\textbackslash n=====\textbackslash n}, create a lengthy and engaging conversation in the Bengali language with a minimum of 100 exchanges between two agents, named \textbf{এজেন্ট-১:} and \textbf{এজেন্ট-২:}. Ensure that the conversation flows naturally, maintains coherence and cohesion throughout, and each turn is relevant to the previous ones. Please ensure that the entire generated conversation is in the Bengali language.
% \end{quote}

% Here, \texttt{\{data[iteration]\}} contains a topic. The topic can be from Wikipedia, Banglapedia, or be synthetically generated. For example: 
% \begin{quote}
% Tagore modernised Bengali art by spurning rigid classical forms and resisting linguistic strictures. His novels, stories, songs, dance dramas, and essays spoke to topics political and personal. 
% \end{quote}

% The junior content writer will generate the conversation. The agent structure is:

% \begin{verbatim}
% junior_content_writer = Agent(
%       role='Content Creator',
%       goal='Generating good quality of Bangla conversational text',
%       backstory=''''"You are a content creator. You are expert in conversation generation in bangla language. Now your job is to create conversation between "এজেন্ট-১" and "এজেন্ট-২" in Bangla text.""",
%       verbose=True,
%       allow_delegation=False,
%       llm = llm,
%       tools=[]
%     )
% \end{verbatim}

% Output from the agent goes to the senior content writer agent, where its structure is:

% \begin{verbatim}
% senior_content_writer = Agent(
%       role='Examiner',
%       goal='Editing and Regenerating good quality of Bangla text conversation',
%       backstory="""You are senior content writer and examiner. You examine if the Bangla conversation that you got from the Content Creator is grammatically correct or not, also ensure its quality. Then regenerate the conversation fixing all errors.""",
%       verbose=True,
%       allow_delegation=False,
%       llm = llm,
%     )
% \end{verbatim}

\section{Benchmarking Datasets}
\label{sec:mmlu_dataset}

\begin{figure}[htb!]
\centering
\includegraphics[width=0.6\columnwidth]{figures/evaluation_data.png}
\vspace{-0.3cm}
\caption{Distribution of an benchmarking dataset totaling $\sim132k$ entries.
% distribution of an evaluation dataset totaling 131,928 entries across various subsets: Bangla MMLU with 87,869 entries, Piqa BN with 17,177 entries, Commonsenseqa BN with 10,962 entries, Openbookqa BN with 5,944 entries, and Boolq BN with 1,976 entries. 
% This highlights the significant representation of the Bangla MMLU subset within the evaluation data.
}
\label{fig:bangla_eval_data}
\vspace{-0.3cm}
\end{figure}

In Figure \ref{fig:bangla_eval_data}, we present the overall distribution of the benchmarking dataset, which consists of approximately 132k entries. 

% In Figure \ref{fig:bangla_eval_data}, we report overall distribution of the benchmarking dataset. In Table \ref{fig:bangla_mmlu_data}, we provide a detail distribution of the Bangla MMLU dataset. 

The dataset is composed of multiple subsets of the benchmarking set, including Bangla MMLU (87,869 entries), Piqa BN (17,177 entries), CommonsenseQA BN (10,962 entries), OpenBookQA BN (5,944 entries), and BoolQ BN (1,976 entries). This distribution highlights the significant dominance of the Bangla MMLU subset within the overall evaluation dataset.

Table \ref{fig:bangla_mmlu_data} provides a detailed breakdown of the Bangla MMLU dataset, which contains 116,503 questions spanning multiple educational categories. These include University Admission (62,906), Higher Secondary (32,512), Job Exams (12,864), Medical Admission (4,779), and Engineering Admission (3,442). The dataset reflects a diverse range of question types relevant to various levels of academic and professional assessments, making it a comprehensive benchmark for evaluating LLMs in Bangla educational contexts.

% \begin{figure}[htb!]
% \centering
% \includegraphics[width=0.6\columnwidth]{figures/evaluation_data.png}
% \caption{Distribution of an benchmarking dataset totaling $\sim132k$ entries.  distribution of an evaluation dataset totaling 131,928 entries across various subsets: Bangla MMLU with 87,869 entries, Piqa BN with 17,177 entries, Commonsenseqa BN with 10,962 entries, Openbookqa BN with 5,944 entries, and Boolq BN with 1,976 entries. This highlights the significant representation of the Bangla MMLU subset within the evaluation data.}
% \label{fig:bangla_eval_data}
% \end{figure}

\begin{figure}[htb!]
  \centering
  \includegraphics[width=0.75\columnwidth]{figures/mmlu_dataset.png}
  \caption{The Bangla MMLU dataset comprises a total of 116,503 questions distributed across various educational categories: University Admission (62,906), Higher Secondary (32,512), Job Exams (12,864), Medical Admission (4,779), and Engineering Admission (3,442).}
  \label{fig:bangla_mmlu_data}
  \vspace{-0.3cm}
\end{figure}


% \section{Additional Results}
% \label{sec:additional_results}

% In this section, we discuss the detailed report of Bangla MMLU benchmarking. Under each category mentioned in Figure-\ref{fig:bangla_mmlu_data}, we have some subcategories based on the topics. Table-\ref{tab:bangla_mmlu_detail} shows the accuracy in each of the subcategories for three different models. The first one is GPT-davinci-002 model. This is a base model from OpenAI and we report its score as a reference. Then we report the detailed score for titulm-llama-3b-v2.0 and Qwen2.5-1.5B model which is the best performer in this benchmark. We can notice that Qwen2.5-1.5B outperforms other models in almost every subcategory. In particular, it does exceptionally well in subcategories like English and ICT. OpenAI's davinci also does better in these subcategories. On the other hand, our model's accuracies are average in almost every category. The primary reason behind this is our models are trained on only the Bengali text with limited tokens. As a result, this model lacks knowledge from the English corpus that may have benefited the GPT-davinci-002 and Qwen2.5-1.5B models.

% \begin{table*}[htb!]
% \centering
% \small
% \setlength{\tabcolsep}{3pt} 
% \scalebox{0.75}{%
% \begin{tabular}{l|l|c|c|c|c}
% % {|l|l|c|c|c|c|}
% \toprule
% \textbf{Category} & \textbf{Task}          & \textbf{Shots} & \textbf{GPT-davinci-002} & \textbf{Qwen-2.5-1.5B} & \textbf{titulm-llama-3b-v2.0} \\ \hline
% \multirow{2}{*}{Higher Secondary} & \multirow{2}{*}{Biology\_Botany}        & 0              & 0.28          & \textbf{0.30}          & 0.21            \\
%                                   &                       & 5              & -             & \textbf{0.36}          & 0.21            \\ \cline{2-6}
%                                   & \multirow{2}{*}{Biology\_Zoology}       & 0              & 0.25          & \textbf{0.33}          & 0.28            \\
%                                   &                       & 5              & -             & \textbf{0.32}          & 0.27            \\ \cline{2-6}
%                                   & \multirow{2}{*}{Chemistry}              & 0              & \textbf{0.30}          & 0.29          & 0.27            \\
%                                   &                       & 5              & -             & \textbf{0.32}          & 0.26            \\ \cline{2-6}
%                                   & \multirow{2}{*}{Mathematics}            & 0              & 0.30          & \textbf{0.32}          & 0.26   \\        
%                                   &                       & 5              & -             & \textbf{0.39}          & 0.29            \\ \cline{2-6}
%                                   & \multirow{2}{*}{Physics}                & 0              & 0.28          & \textbf{0.33}          & 0.25            \\
%                                   &                       & 5              & -             & \textbf{0.36}          & 0.26            \\ \hline
% \multirow{2}{*}{Engineering Admission} & \multirow{2}{*}{Chemistry}          & 0              & \textbf{0.33}          & \textbf{0.33}          & 0.25            \\
%                                           &                       & 5              & -             & \textbf{0.36}          & 0.27            \\ \cline{2-6}
%                                           & \multirow{2}{*}{Mathematics}            & 0              & 0.31          & \textbf{0.35}          & 0.23            \\ 
%                                           &                       & 5              & -             & \textbf{0.35}          & 0.29            \\ \cline{2-6}
%                                           & \multirow{2}{*}{Physics}                & 0              & 0.30          & \textbf{0.34}          & 0.25            \\ 
%                                           &                       & 5              & -             & \textbf{0.34}          & 0.26            \\ \hline
% \multirow{2}{*}{Medical Admission}   & \multirow{2}{*}{Biology\_Botany}        & 0              & 0.29          & \textbf{0.31}          & 0.25            \\ 
%                                           &                       & 5              & -             & \textbf{0.33}          & 0.25            \\ \cline{2-6}
%                                           & \multirow{2}{*}{Biology\_Zoology}       & 0              & \textbf{0.28}          & 0.23          & 0.22            \\ 
%                                           &                       & 5              & -             & \textbf{0.29}          & 0.25            \\ \cline{2-6}
%                                           & \multirow{2}{*}{Chemistry}              & 0              & 0.30          & \textbf{0.31}          & 0.27            \\ 
%                                           &                       & 5              & -             & \textbf{0.35}          & 0.29            \\ \cline{2-6}
%                                           & \multirow{2}{*}{English}                & 0              & 0.50          & \textbf{0.52}          & 0.19            \\ 
%                                           &                       & 5              & -             & \textbf{0.58}          & 0.22            \\ \cline{2-6}
%                                           & \multirow{2}{*}{GK\_Bangladesh}         & 0              & 0.25          & \textbf{0.26}          & 0.24            \\ 
%                                           &                       & 5              & -             & \textbf{0.32}          & 0.21            \\ \cline{2-6}
%                                           & \multirow{2}{*}{GK\_International}      & 0              & 0.21          & \textbf{0.24}          & \textbf{0.24}            \\ 
%                                           &                       & 5              & -             & 0.25          & \textbf{0.32}            \\ \cline{2-6}
%                                           & \multirow{2}{*}{Physics}                & 0              & 0.30          & \textbf{0.31}          & 0.24            \\ 
%                                           &                       & 5              & -             & \textbf{0.32}          & 0.24            \\ \hline
% \multirow{2}{*}{University Admission}  & \multirow{2}{*}{Bengali\_Grammar}     & 0              & 0.31          & \textbf{0.33}          & 0.27            \\ 
%                                           &                       & 5              & -             & \textbf{0.36}          & 0.20            \\ \cline{2-6}
%                                           & \multirow{2}{*}{Bengali\_Literature}    & 0              & \textbf{0.27}          & 0.24          & 0.22            \\ 
%                                           &                       & 5              & -             & \textbf{0.30}          & 0.21            \\ \cline{2-6}
%                                           & \multirow{2}{*}{Biology\_Botany}        & 0              & 0.30          & \textbf{0.32}          & 0.25            \\ 
%                                           &                       & 5              & -             & \textbf{0.34}          & 0.23            \\ \cline{2-6}
%                                           & \multirow{2}{*}{Biology\_Zoology}       & 0              & \textbf{0.27}          & \textbf{0.27}          & 0.25            \\ 
%                                           &                       & 5              & -             & \textbf{0.31}          & 0.26            \\ \cline{2-6}
%                                           & \multirow{2}{*}{Chemistry}              & 0              & 0.30          & \textbf{0.35}          & 0.26            \\ 
%                                           &                       & 5              & -             & \textbf{0.38}          & 0.22            \\ \cline{2-6}
%                                           & \multirow{2}{*}{English}                & 0              & 0.50          & \textbf{0.59}          & 0.22            \\ 
%                                           &                       & 5              & -             & \textbf{0.62}          & 0.21            \\ \cline{2-6}
%                                           & \multirow{2}{*}{GK\_Bangladesh}         & 0              & 0.31          & \textbf{0.34}          & 0.28            \\ 
%                                           &                       & 5              & -             & \textbf{0.35}          & 0.23            \\ \cline{2-6}
%                                           & \multirow{2}{*}{GK\_International}      & 0              & 0.30          & \textbf{0.31}          & 0.25            \\ 
%                                           &                       & 5              & -             & \textbf{0.32}          & 0.26            \\ \cline{2-6}
%                                           & \multirow{2}{*}{ICT}                    & 0              & 0.33          & \textbf{0.40}          & 0.22            \\ 
%                                           &                       & 5              & -             & \textbf{0.44}          & 0.32            \\ \cline{2-6}
%                                           & \multirow{2}{*}{IQ}                     & 0              & 0.26          & \textbf{0.32}          & 0.22            \\ 
%                                           &                       & 5              & -             & \textbf{0.35}          & 0.26            \\ \cline{2-6}
%                                           & \multirow{2}{*}{Mathematics}            & 0              & 0.30          & \textbf{0.41}          & 0.26            \\ 
%                                           &                       & 5              & -             & \textbf{0.43}          & 0.24            \\ \cline{2-6}
%                                           & \multirow{2}{*}{Physics}                & 0              & 0.32          & \textbf{0.38}          & 0.26            \\ 
%                                           &                       & 5              & -             & \textbf{0.37}          & 0.28            \\ \hline
% \multirow{2}{*}{Job Exams}         & \multirow{2}{*}{Bengali\_Grammar}       & 0              & 0.26          & 0.27          & \textbf{0.28}            \\ 
%                                           &                       & 5              & -             & \textbf{0.29}          & 0.22            \\ \cline{2-6}
%                                           & \multirow{2}{*}{Bengali\_Literature}    & 0              & 0.28          & 0.30          & 0.26            \\
%                                           &                       & 5              & -             & \textbf{0.30}          & 0.21            \\ \cline{2-6}
%                                           & \multirow{2}{*}{English}                & 0              & 0.54          & \textbf{0.60}          & 0.24            \\ 
%                                           &                       & 5              & -             & \textbf{0.60}          & 0.27            \\ \cline{2-6}
%                                           & \multirow{2}{*}{GK\_Bangladesh}         & 0              & \textbf{0.28}          & 0.28          & 0.25            \\
%                                           &                       & 5              & -             & \textbf{0.30}          & 0.26            \\ \cline{2-6}
%                                           & \multirow{2}{*}{GK\_International}      & 0              & 0.31          & 0.34          & 0.26            \\ 
%                                           &                       & 5              & -             & \textbf{0.34}          & 0.24            \\ \cline{2-6}
%                                           & \multirow{2}{*}{ICT}                    & 0              & 0.38          & \textbf{0.60}          & 0.21            \\ 
%                                           &                       & 5              & -             & \textbf{0.62}          & 0.30            \\ \cline{2-6}
%                                           & \multirow{2}{*}{IQ}                     & 0              & 0.30          & 0.33          & 0.20            \\
%                                           &                       & 5              & -             & \textbf{0.40}          & 0.23            \\ \cline{2-6}
%                                           & \multirow{2}{*}{Mathematics}            & 0              & 0.32          & 0.36          & 0.24            \\ 
%                                           &                       & 5              & -             & \textbf{0.34}          & 0.21            \\ \bottomrule
% \end{tabular}
% }
% \caption{Detail accuracy table of various categories and tasks of Bangla MMLU dataset.}
% \label{tab:bangla_mmlu_detail}
% \end{table*}


