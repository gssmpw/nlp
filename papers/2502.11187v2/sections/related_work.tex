\section{Related Work}
\label{sec:related_work}
% Recent progress in Natural Language Processing (NLP) has highlighted the remarkable evolution of large language models (LLMs), which have transformed computational linguistics capabilities across a diverse array of languages. Nonetheless, the primary focus remains on high-resource languages, often overshadowing advancements in low-resource languages such as Bangla, which is spoken by almost 30 billion of people worldwide. This section explores the latest efforts aimed at improving Bangla language processing by leveraging the power of LLMs. 

% Recent advances in NLP through the development of large language models (LLMs) have shown considerable promise, especially for low-resource languages such as Bangla. This section reviews the literature surrounding the pretraining, instruction tuning, and evaluation of these models, particularly focusing on their application to the Bangla language.

% \subsection{Corpus}

\noindent
\paragraph{Pretraining:}
Pretraining LLMs on Bangla has involved the development of specialized models like BongLLaMA \cite{zehady2024bongllama}, which has been adapted from Llama to better understand and generate Bangla text. The pretraining phase typically leverages large-scale Bangla corpora to improve the model's foundational understanding of the language's syntax and semantics. For instance, \citet{zehady2024bongllama} focused on developing a robust model by pretraining on diverse Bangla data sources, significantly improving the model's performance on native text. Similar efforts have been made in previous research, such as BanglaBERT~\cite{bhattacharjee2022banglabert} and SahajBERT~\cite{diskin2021distributed}, where models underwent extensive pretraining on curated Bangla datasets to better capture linguistic nuances.

% https://arxiv.org/pdf/2406.14670

\noindent
\paragraph{Enhancing Tokenization:}
The evolution of token adaptation in NLP has progressed from linguistic cues and statistical methods~\cite{creutz2006morfessor, luong2013better, zhang2023ask} to phrase-level segmentation~\cite{koehn2007moses, koehn2003statistical}. The rise of deep learning shifted the focus to subword-level segmentation, enhancing the handling of rare words~\cite{sennrich2015neural, kudo2018subword, kudo-richardson-2018-sentencepiece}. More recent efforts emphasize integrating specialized vocabularies into pre-trained LLMs, prioritizing tokenization quality and cost-effectiveness~\cite{ahia2023all, zhang2023ask, zhang2024counting, tejaswi2024exploring}.
\citet{liu2023task} propose a model-agnostic approach for adapting extended vocabularies to LLMs by integrating task-specific vocabularies, prioritizing new tokens, and initializing their embeddings using averaged subword representations. \citet{cui2023efficient} extend Llama's existing vocabulary by incorporating an additional 20k Chinese tokens, enhancing its ability to understand and generate Chinese text. \citet{chiappeoptimizing} develop an adaptive tokenization algorithm that implements a dynamic tokenization dictionary within the Llama model, updating tokens in real-time based on frequency and contextual relevance.


\noindent
\paragraph{Cross-Lingual Model Adaptation:}
Cross-lingual transfer enables models trained in one language to adapt to others without retraining from scratch. Key adaptation techniques include embedding initialization, transliteration, and vocabulary extension.
\citet{jaavid2024romansetu} used transliteration to convert non-Latin languages into Latin scripts for better knowledge transfer. \citet{zhao2024llama} trained a model on millions of target-language tokens without vocabulary extension, achieving performance comparable to models trained on billions of tokens. However, tokenization mismatches reduced inference efficiency.
Studies by \citet{csaki2023efficiently, cui2023efficient, raffel2020exploring, lin2024mala} found that vocabulary extension improves performance while reducing computational inefficiencies. \citet{tejaswi2024exploring} further explored language-specific LLMs, highlighting trade-offs in adaptation for low-resource languages. Their findings emphasize that while vocabulary expansion enhances efficiency, selecting the right base model and vocabulary size is crucial.
% They also confirmed that simple mean embedding initialization is an effective alternative to complex methods.

% Token substitution \cite{csaki2023efficiently} for fertility reduction on target Hungarian and Thai languages. 

% One of the most crucial approaches in NLP is cross-lingual transfer, which allows models trained in one language to be adapted to use in other languages without training the model from scratch. Several studies have been conducted in recent years on successful cross-lingual adaptation, including embedding initialization,  transliteration, vocabulary extension, etc.

% One of the recent efforts was done by \cite{jaavid2024romansetu}, which involved transliteration. In this study, non-Latin languages were transliterated into Latin scripts to facilitate knowledge transfer from English-based language models. In a different investigation, \cite{zhao2024llama} trained the model using millions of target language tokens without any sort of vocabulary extension. According to this study, the performance is comparable to that of the state-of-the-art model that has been trained on billions of tokens. This strategy, however, reduces inference efficiency because the model finds it difficult to deal with tokenization mismatches.

% Some different approaches were also investigated, where \cite{csaki2023efficiently,cui2023efficient, raffel2020exploring,lin2024mala }  explored the effectiveness of cross-lingual generation with vocabulary extension. This study shows that adding additional tokens improves performance on target languages while lowering computational inefficiencies.

% One of the most recent studies conducted by {\cite{tejaswi2024exploring}} investigated potential designs for creating language-specific LLMs in further detail, emphasizing compute-efficient techniques, vocabulary expansion, and embedding initialization. Their research highlights trade-offs in model adaptation for low-resource languages and offers a comparative analysis of several methodologies. They discovered that although expanding one's vocabulary increases productivity, selecting the best base model and figuring out the ideal vocabulary size are essential when working with constrained computational resources. In line with results from earlier studies, they also verified that basic mean embedding initialization is a good substitute for more intricate initialization techniques.


\noindent
\paragraph{Benchmarking and Evaluation.}
Evaluating LLMs requires benchmarking datasets that assess a wide range of capabilities and tasks. For Bangla, most existing datasets focus on standard NLP tasks. The BanglaNLG benchmark dataset~\cite{bhattacharjee2023banglanlg} addresses this by integrating six distinct datasets designed to evaluate various aspects of natural language generation (NLG) in Bangla, including Machine Translation, Text Summarization, Question Answering, Multi-turn Dialogue, News Headline Generation, and Cross-lingual Summarization.
Beyond NLG, the Region-Specific Native-QA dataset~\cite{hasan2024nativqa} was developed to assess the question-answering capabilities of leading LLMs, such as GPT-4o, GPT-4, Gemini, Llama-3, and Mistral. By focusing on regionally relevant queries, this dataset ensures that models are tested in real-world Bangla language contexts.
For a broader evaluation of LLMs across multiple tasks, BenLLM~\cite{kabir2024benllm} provides the most comprehensive comparison of model performance. This study benchmarks LLMs against other pretrained models using datasets from diverse sources, offering insights into their strengths and limitations across various NLP tasks.

There is a significant lack of benchmarking datasets for evaluating LLMs. To address this gap, our study developed \textbf{\textit{five benchmarking datasets}}, each designed to assess different capabilities, including world knowledge and commonsense reasoning.

% The BanglaNLG benchmark dataset \cite{bhattacharjee2023banglanlg} integrates six diverse datasets, each designed to evaluate different facets of natural language generation in Bangla, including Machine Translation, Text Summarization, Question Answering, Multi-turn Dialogue, News Headline Generation, and Cross-lingual Summarization. These datasets challenge models on a variety of tasks from translating between Bangla and English to generating dialogue and summaries, using metrics such as SacreBLEU, ROUGE-2, and BLEU-1 to assess performance comprehensively. Region Specific Native-QA dataset \cite{hasan2024nativqa} is developed and evaluated with popular LLMs (GPT-4o, GPT-4, Gemini, LLama-3, and Mistral). Most comprehensive evalution of different tasks using LLMs are found in \cite{kabir2024benllm} where they demostrated the performance of LLMs with other pretrained models using different dataset from multiple sources. 

% \subsection{Downstreaming Tasks}

% Benchmarking on Bangla NLP tasks

% https://aclanthology.org/2024.lrec-main.201.pdf

% Bangla QA
% https://arxiv.org/pdf/2407.09823v1
