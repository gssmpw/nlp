\section{Introduction}
\label{sec:introduction}

%https://arxiv.org/pdf/2406.14670
%https://arxiv.org/pdf/2404.05829

The rapid advancements in large language models (LLMs) have reshaped the field of artificial intelligence, showcasing remarkable versatility across numerous tasks \cite{brown2020language,ouyang2022training,achiam2023gpt4,chowdhery2023palm}. These models demonstrate not only an ability to perform various NLP tasks but also an intriguing potential for self-assessment and continuous improvement \cite{liu2023gpteval,fu2023chain,chiang2023vicuna}. 
% As LLMs progress, their proficiency in self-reflection and correction has been the subject of considerable research \cite{bai2022constitutional,saunders2022self,madaan2023self,gou2024critic}.

Despite these advancements, LLM development—both open and closed—has predominantly focused on multilingual models, with a stronger emphasis on high-resource languages \cite{achiam2023gpt4,touvron2023llama}. While some models have extended coverage to medium- and low-resource languages \cite{le2023bloom,ustun2024aya,qwen2,gemma_2024}, their representation remains limited. Although some initiatives have aimed to train language-centric LLMs \cite{sengupta2023jais,fanar2024}, these efforts remain scarce due to the high costs associated with computational resources and data collection. Consequently, recent research has shifted towards adapting existing LLMs for new languages \cite{levine2024rakutenai,fanar2024}.

\begin{figure}[t]
  \centering
\includegraphics[width=0.85\columnwidth]{figures/tts_paper-5-shot.png}
  \caption{Performance scores per category TituLLM-3B and five other  models in 5-shot setting.}
  \vspace{-0.3cm}
  \label{fig:bangla_eval_data}
  \vspace{-0.3cm}
\end{figure}

Similarly, in the context of benchmarking LLMs, most efforts have primarily focused on high-resource languages~\cite{bang2023multitask, ahuja2023mega}, while low-resource languages, such as Bangla, have received limited attention~\cite{kabir2024benllm, zehady2024bongllama, bhattacharjee2023banglanlg}. \citet{zehady2024bongllama} developed LLMs for Bangla using Llama, leveraging only the Bangla subset of CulturaX~\cite{nguyen2024culturax}, which consists of 12.4 million diverse Bangla news articles. They further fine-tuned their model on 172k instruction samples from subsets of the Alpaca~\cite{taori2023stanford} and OpenOrca~\cite{lian2023openorca} datasets, which were translated from English. Their models were benchmarked on 120 queries across nine different generation tasks.

Addressing this gap is crucial, as linguistic and cultural diversity significantly impact language understanding and generation. Therefore, in this study, we focus on adapting existing LLMs (e.g., Llama) by expanding the model's vocabulary and performing continual pretraining. This process required extensive data collection from diverse sources. Given the relatively low availability of digital content in Bangla, we also developed synthesized datasets to supplement our training data.

Benchmarking LLM capabilities for Bangla remains challenging due to the lack of specialized datasets, particularly in areas such as world knowledge and commonsense reasoning. Although some efforts have been made to generate such datasets through translation \cite{lai-etal-2023-okapi}, they remain limited in scope. To address this gap, we have developed several native and translated datasets. \textbf{\textit{Compared to }}\citet{zehady2024bongllama}, our pretraining corpus is significantly larger ($\sim37b$ tokens) and is benchmarked on five different datasets covering world knowledge and commonsense reasoning, with a total dataset size of 132k entries.

% and evaluation have been largely focused on high-resource languages, 
% with comparatively less attention given to low-resource languages such as Bangla \cite{kabir2024benllm}. 

% To contribute to the growing body of LLM evaluation, we have developed a unique multiple-choice question (MCQ) and question-answer (QA) dataset specifically tailored for the low-resource Bangla language. This dataset is intended to benchmark various LLMs, evaluating their performance across essential tasks in Bangla and providing insights into their capacity for language comprehension, alignment, and safety in low-resource settings.


% \com{to do}
\begin{itemize}[noitemsep,topsep=0pt,labelsep=.5em] %leftmargin=*
\item We developed and released two models, \titu{}, adapted from Llama 3.2, which will enable future research. 
\item We provide a complete data collection recipe for pretraining LLMs including sources, approaches to synthetic data generation. 
\item We extended tokenizer to ingest language specific knowledge. 
% We prepare and release four types of datasets specifically designed for evaluating multilingual large language models (LLMs), including multiple-choice questions (MCQ) and question-answer (QA) pairs in Bangla;
\item We developed \textit{\textbf{five datasets}} to benchmark LLMs capabilities in terms of world knowledge, commonsense reasoning, and reading comprehension. Such datasets will serve as a first step to Benchmark LLMs for Bangla.
\item We proposed a novel translation techniques that helps to develop high quality benchmarking dataset. 
\item Using the benchmaked datasets we benchmark various LLMs including \titu{} comparing performance across models to assess understanding of Bangla language. 
% to evaluate their capabilities in handling Bangla, a low-resource language, comparing performance across models to assess understanding of Bangla language structures and task-specific accuracy;
% \item We develop domain-generalization test sets that cover a wide range of linguistic domains, ensuring robust evaluation of how well LLMs generalize across different contexts and tasks in Bangla;
% \item We discuss the current limitations in evaluating LLMs for Bangla, including challenges with dialectal variations and the need for more comprehensive datasets. We also explore future research directions to improve model performance and expand dataset diversity for Bangla. 
\end{itemize}

\noindent
Our study reveals several interesting findings:

\noindent
\textbf{Vocabulary Extension:} We explore the impact of vocabulary extensions on the base Llama Tokenizer by increasing the number of new tokens from 32K to 96K in increments of 16K. We found that average tokens per word (TPW) decreases as the number of tokens increases up to a certain point, after which it declines only minimally. Therefore, when adding new tokens, we must also consider the fertility rate to balance the trade-off between training and inference.

% \noindent
% \textbf{Performance of Adaptive Models:} Our analysis reveals that existing models exhibit stronger predictive capabilities when evaluated on code-mixed and knowledge-intensive data than original Bangla data. However, \titu{} outperforms other models on reasoning tasks in pure Bangla datasets as illustrated in Figure \ref{fig:bangla_eval_data}. This superior performance can be attributed to its tokenizer and pretraining corpus, which includes a substantial volume of cleaned Bangla text, enabling more effective language understanding.

\noindent
\textbf{Commonsense Capability:} \titu{} demonstrates strong commonsense knowledge but has limited capability in world knowledge (e.g., Bangla MMLU). Further training with instruction fine-tuning may enhance its performance in this area.

% Although we find that our model performs well on genaralized datasets (commonsenseqa, openbookqabn) but it doesn't perform well on factual data in MMLU testset. The possible reason might be the training epoch is 1. Further Extensive training should improve the performance of pretraining model and make ready for instruction finetuing. 



% Our contributions: 

% % 1. Diverse dataset: 

% % We collect dataset with around 25 billions comparsing high quality bangal text which is largest than any other publically avaialbe bangla data. 

% 2.  

% % We provide five different pretraining evaluation dataset prepared by human, multiagent translation, llm guideded synthetic data . Specially, MMLU dataset is totally human generated and good for benchmarking other multilingual llms. 

% 3. We proposed a nobel translation techniques that helps prepare high quality translated evaluation dataset. 


% As we perform only 1 epoch over 36.8 billions token, it may not perform well on factual datasets like MMLU. 
% How our work is different than prior work~\cite{zehady2024bongllama}, BanglaLLM\footnote{\url{https://huggingface.co/BanglaLLM}}? 

% BongoLLAMA vs TituLLMs 
% Pretraining dataset: 

% CultureX Bangla 
% 12.4 million
% diverse Bangla news articles. 

% large and diverse 
% Web data , 
% Books + traslitation+ translation+ synthetic conversation + audio transcriptiom 



% Instruction tuning: 

% Bangla-Alpaca-Orca =  Alpaca datase + OpenOrca dataset (172k instructions)

% N/A

% Tokenizer: 
% llama 27b 
% 8,000 Bangla tokens, increasing the total token
% count from 32,000 to 50,00

% llama3 , we add 42k+ resulting 128+42 =170k tokens  

% Evaluation Method: 

% Comparsion with meta 

% GPT-4o 


% GPT-40 

% Evaluation dataset: 
% 120 queires 9 different generation tasks 

% Bangla MMLU - textbooks + question bank   
% boolq bn - GPT Generated 
% commonsenseqa bn+ openbookqa+piqa using EST
% Model comparison:  
% Our work not only highlights the need for LLM evaluation in underrepresented languages but also lays the groundwork for future research aimed at improving model performance in Bengali, ensuring more equitable advancements in AI technology.


% %%%%%%%paper structure
% The rest of the paper is organized as follows: Section~\ref{sec:related_work} presents previous work, Section~\ref{sec:dataset} describes the dataset, Section~\ref{sec:experiments} formulates our experiments, Section~\ref{sec:results} discusses the evaluation results. Finally, Section~\ref{sec:conclusion} concludes and points to possible directions for future work.
