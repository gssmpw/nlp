\section{Pretraining Data}
\label{sec:datasets}
Pretraining data for Bangla is very limited compared to very high quality data available for English and other high resource languages \cite{penedo2023refinedweb,soldaini2024dolma}. Hence, we needed collect pretraining dataset for training \titu{}. We have compiled a substantial Bangla raw dataset from a diverse range of sources, encompassing both formal and informal linguistic styles. The dataset is primarily derived from three key sources: web documents, books, and synthetically generated text. The synthetically generated data includes translated data, transliterated data, audio transcripted data, etc. An outline of our data collection and preprocessing pipeline is show in Figure  \ref{fig:data_collection_final}. The final high-quality dataset amounts to approximately 268 GB, with 22 GB designated as a validation set, sampled proportionally to maintain the heterogeneity of the data. In total, the corpus contains $\sim37$ billion tokens, optimized for training and evaluation across various Bangla language processing applications. In Table~\ref{tab:titulm_tokens} (in Appendix) and in Figure \ref{fig:pretraining_data_distribution}, we report the distribution of tokens for different sources.


% , we  briefly describes our pretraining data collection pipeline. 
%64.19 


% \begin{table}[h]
%     \centering
%     \begin{tabular}{l c}
%         \hline
%         \textbf{Data} & \textbf{GPT-4o tokens (B)} \\
%         \hline
%         Web Documents & 46.84 \\
%         Books & 9.06 \\
%         Synthetic Data & \\
%         \quad Translated & 2.14 \\
%         \quad Romanized & 3.87 \\
%         \quad Conversation & 0.54 \\
%         \quad Audio Transcription & 1.74 \\
%         \hline
%         \textbf{Total} & \textbf{64.19} \\
%         \hline
%     \end{tabular}
%     \caption{GPT Tokens Count for Different Data Sources}
%     \label{tab:gpt_tokens}
% \end{table}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=\linewidth]{figures/pretraining-data-collection-pipeline.png}
%     \caption{Pipeline for pertaining data collection.}
%     \label{fig:Pipeline for pertaining data collection}
% \end{figure}


\begin{figure*}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/tts_paper-data-collection-icon-v3.drawio.png}
    \vspace{-0.2cm}
    \caption{Overview of the pretraining data collection and preprocessing pipeline -- A workflow illustrating the steps involved in gathering, filtering, and preparing data for LLM pretraining.
    }
    \label{fig:data_collection_final}
    \vspace{-0.3cm}
\end{figure*}


\begin{figure}[t]
  \centering
  \includegraphics[width=0.7\columnwidth]{figures/pretraining-data.png}
  \vspace{-0.3cm}
  \caption{The pretraining dataset consists of $\sim37b$ billion tokens distributed across various sources: Web (9.8b), Books (4b), Synthetic Data (7.06b), Sangraha Web (5b), and Sangraha Synthetic (10.94b). Synthetic Data includes Translated (1.47b), Romanized (3.87b), Conversation (0.42b), and Audio Transcription (1.30b), while Sangraha Synthetic comprises Translated (4.26b) and Romanized (6.68b).}
  \label{fig:pretraining_data_distribution}
  \vspace{-0.4cm}
\end{figure}

% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=\linewidth]{figures/tts_paper-edit.png}
%     \caption{Pipeline for pretraining data extraction.}
%     \label{fig:Pipeline for Bangla Data Collection New}
% \end{figure*}



% \subsection{Web Documents}

% % \begin{figure}[htb!]
% %     \centering
% %     \includegraphics[width=\linewidth]{figures/web-text-extraction-full-pipeline.png}
% %     \caption{Pipeline for text data extraction from web sources.}
% %     \label{fig:Pipeline for text data extraction from web sources}
% % \end{figure}

% \subsubsection{common crawl: } We have extracted text from web data collected from \textbf{common crawl}. The raw text data collected from websites have been processed through advanced content cleaning pipelines to curate a robust set of web-based Bangla texts, ensuring domain diversity and lexical richness. Figure - \ref{fig:Pipeline for text data extraction from web sources} shows the steps that are followed to prepare the final data from raw common crawl data. These are the steps - 
% \subsubsection{Separating Bengali HTML data and URLs by analyzing metadata}
% The Common Crawl dataset, comprising a vast collection of raw HTML and text data, was leveraged as the primary source for obtaining Bangla-specific web content. To extract relevant HTML data efficiently, Amazon Athena was employed as the querying tool. The filtering criteria for the queries included the following:  

% \begin{enumerate}
%     \item \textbf{Content Language:} Filtering by language metadata to isolate Bangla text.
%     \item \textbf{URL Patterns:} Identifying URLs that exhibited Bangla-specific patterns, such as \texttt{.gov.bd} or other domain indicators common in Bangladeshi websites.
%     \item \textbf{Host Information:} Filtering based on host-specific attributes to target Bangla-related domains.
% \end{enumerate}

% These queries are executed on Common Crawl datasets spanning the years 2017 to 2024. The resulting outputs are a collection of CSV files containing metadata, including crawl version identifiers and supplementary details. This metadata serves as a foundational layer for subsequent processing and analysis to refine and extract meaningful Bangla text content from the raw HTML data.
% \subsubsection{Extracting clean text from the HTML}
% Extracting meaningful text from Common Crawl's raw HTML data presented significant challenges due to the diversity and complexity of the HTML structures. There exist several tools for HTML text extraction, maintaining the integrity of text patterns and ensuring alignment with the original content was critical. Through manual evaluation, we identified \textit{Trafilatura} as an effective tool for this task. \textit{Trafilatura} has helped us to extract clean, structured text from raw HTML while preserving text patterns and minimizing noise. 

% Additionally, processing the queried HTML data required significant computational effort to ensure that the extracted text maintained high quality and alignment with the desired patterns. This approach provided a robust pipeline for converting raw HTML into usable textual data suitable for downstream analysis.

% \subsubsection{Generating quality signals based on defined rules}
% To ensure the usability and relevance of the extracted textual data, we generated a diverse set of quality signals specifically tailored for the Bangla language. These quality signals serve as metrics to evaluate and filter the data for downstream applications.  

% The quality signals were generated using Bangla-specific tokenizers and tools, ensuring precise handling of language nuances. In total, 20 distinct quality signals were generated for each document. The signals included:  
% % \begin{itemize}
% %     \item \textbf{Document Structure Metrics:} Word counts, character counts, and sentence counts to assess the overall length and density of Bangla content.  
% %     \item \textbf{Formatting Indicators:} Line endings with terminal punctuation to evaluate sentence completeness and coherence in Bangla text.  
% %     \item \textbf{Content Filtering:} Detection of adult content and other undesirable material, tailored to the linguistic and cultural context of Bangla.  
% % \end{itemize}

% \begin{itemize}
%     \item \textbf{Line Ending with Terminal Punctuation:}  
%     This quality signal determines whether a line concludes with a terminal punctuation mark. The recognized terminal punctuation marks include \texttt{"."}, \texttt{"!"}, \texttt{"?"}, and \texttt{"”"}. This signal helps assess the completeness of sentences within a document and can be useful for filtering out incomplete or malformed content.

%     \item \textbf{Line Word Numbers:}  
%     This metric calculates the number of words in each line after normalizing the text. It provides insights into the structural properties of the text, such as whether it consists of single-word lines, short fragments, or full-length sentences.

%     \item \textbf{Line Start with Bullet Points:}  
%     This quality signal identifies whether a line starts with a bullet point. The considered bullet point symbols include various Unicode characters such as \texttt{\textbackslash u2022}, \texttt{\textbackslash u2023}, \texttt{\textbackslash u25B6}, \texttt{\textbackslash u25C0}, \texttt{\textbackslash u25E6}, \texttt{\textbackslash u25A0}, \texttt{\textbackslash u25A1}, \texttt{\textbackslash u25AA}, \texttt{\textbackslash u25AB}, and \texttt{\textbackslash u2013}. This is useful for recognizing and handling structured lists within a document.

%     \item \textbf{Line Numerical Character Fraction:}  
%     This metric calculates the proportion of numerical characters relative to the total number of characters in each line. It helps identify lines that may be dominated by numerical data, such as statistics, mathematical expressions, or financial reports.

%     \item \textbf{Is Adult URL:}  
%     This pre-filtering signal assesses whether a document originates from an adult content URL. It is used to filter out inappropriate or explicit content before further text processing.

%     \item \textbf{Document Sentence Count:}  
%     This metric counts the number of sentences in a document. It is calculated using a customized sentence tokenizer for Bangla and the NLTK tokenizer for English. This helps determine the length and complexity of a document.

%     \item \textbf{Document Word Count:}  
%     This signal computes the total number of words in a document after normalization. It provides an overall measure of document length and verbosity.

%     \item \textbf{Document Mean Word Length:}  
%     This metric calculates the average length of words in a document after normalization. It can indicate the linguistic complexity of the content, with longer words often suggesting a more sophisticated vocabulary.

%     \item \textbf{Document Word to Symbol Ratio:}  
%     This metric determines the ratio of symbols to words in a document. Symbols are defined as \texttt{"\#"}, \texttt{"..."}, and \texttt{"…"} (ellipsis). A high symbol-to-word ratio may indicate unconventional formatting or non-standard text.

%     \item \textbf{Document Fraction End with Ellipsis:}  
%     This signal calculates the fraction of lines that end with an ellipsis (\texttt{"..."}, \texttt{"…"}). A high fraction may suggest incomplete thoughts, trailing sentences, or dramatic pauses within the text.

%     \item \textbf{Document Unique Word Fraction:}  
%     This metric measures the fraction of unique words within a document, also known as the degeneracy of a text sample. It provides insight into vocabulary diversity and repetition within the content.

%     \item \textbf{Document Unigram Entropy:}  
%     This quality signal calculates the entropy of the unigram distribution within the document. It measures content diversity using the formula:
%     \[
%     \sum \left(-\frac{x}{\text{total}} \log \frac{x}{\text{total}}\right)
%     \]
%     where \( x \) represents counts of unique words in the normalized content. Higher entropy suggests greater lexical variety.

%     \item \textbf{Document Stop Word Fraction:}  
%     This metric determines the ratio of stop words (common function words like "the," "and," "is") to total words in a document. A high stop word fraction may indicate informal or conversational text, whereas a low fraction may suggest highly technical or keyword-dense content.

%     \item \textbf{Fraction of Characters in Top N-Gram:}  
%     This signal measures the proportion of characters found within the most frequently occurring word n-grams. It helps assess the repetitiveness and structure of the text.

%     \item \textbf{Fraction of Characters in Duplicate Word N-Gram:}  
%     This metric calculates the fraction of characters contained within repeated word n-grams. It can help identify redundant or excessively repetitive content.

%     \item \textbf{Document Top Language Detection:}  
%     This post-filtering quality signal determines the primary language of a document. It ensures that content is processed within the correct linguistic context and can aid in filtering out unwanted languages.

%     \item \textbf{Document Content Classification:}  
%     This quality signal categorizes document content based on its level of profanity, vulgarity, or toxicity. It helps in filtering out inappropriate or offensive material.

%     \item \textbf{Document Bad Words Count:}  
%     This metric calculates the total number of offensive or inappropriate words in a document. It serves as a stricter filter for explicit content.

%     \item \textbf{Document Curly Bracket Ratio:}  
%     This metric calculates the ratio between the number of occurrences of curly brackets (\texttt{"\{"}, \texttt{"\}"}) and the total number of characters in the raw text. A high ratio might indicate the presence of code, structured data, or formulaic content.

%     \item \textbf{Document Bracket Ratio:}  
%     This signal determines the ratio of occurrences of all types of brackets (e.g., \texttt{"()"}, \texttt{"[]"}, \texttt{"\{\}"}) to the total number of characters in the document. It can be useful for detecting technical or structured text, such as programming code, mathematical expressions, or legal documents.

% \end{itemize}

%  By leveraging language-specific tokenizers and tools, we ensured that the evaluation framework effectively captured the characteristics and patterns unique to Bangla text, enabling robust filtering and alignment with intended use cases.
 
% \subsubsection{Applying Quality Signal Thresholds for Document Filtering: }
% In the final stage of processing, we applied predefined thresholds to each quality signal to filter and categorize documents. These thresholds were established following a \textit{Gopher Rule} approach, ensuring that the selected documents met the desired quality standards.  

% % \begin{figure}[htb!]
% %     \centering
% %     \includegraphics[width=\linewidth]{figures/cc_filtering_diagram.png}
% %     \caption{Pipeline for text document filtering.}
% %     \label{fig:Pipeline for text document filtering}
% % \end{figure}

% % The thresholds included:  
% % \begin{itemize}
% %     \item \textbf{Word Count:} Documents were required to have a word count between 50 and 10,000.  
% %     \item \textbf{Adult Content Filtering:} Only documents marked as \textit{is\_adult = false} were retained.  
% %     \item \textbf{Sentence Count:} Documents with more than 5 sentences were selected to ensure sufficient content density.  
% % \end{itemize}

% Using the quality signal thresholds, documents are categorized into two groups: \textit{pass} and \textit{fail}. This step has effectively filtered out low-quality or irrelevant documents, resulting in a refined dataset that meets the criteria for further analysis and applications.


\subsection{Web Documents}
We curated the \textit{Common Crawl}~\cite{raffel2020exploring} dataset and followed multiple steps to extract and clean the final dataset, as illustrated in Figure \ref{fig:data_collection_final}. Below, we briefly discuss each step. 

\noindent \textbf{SQL Query Engine:} Using Amazon Athena,\footnote{\url{https://aws.amazon.com/athena/}} we queried the vast Common Crawl dataset to isolate Bangla-specific HTML data and URLs. We applied filtering based on content language, URL patterns indicative of Bangla domains (e.g., \texttt{.gov.bd}), and host information, covering data from 2017 to 2024.  

\noindent \textbf{Text Extraction:} We used \textit{Trafilatura}~\cite{barbaresi-2021-trafilatura} tool for its effectiveness in extracting structured, clean text from HTML. This step preserved the structure of text patterns while minimizing extraneous noise, ensuring the retention of original content nuances.

\noindent \textbf{Filtering:} To assess the usability and relevance of the extracted text, we generated 18 distinct hand-crafted rules specifically designed for the Bangla language. These rules evaluated various aspects of text quality, including sentence completeness, document structure, and content appropriateness, leveraging Bangla-specific tokenizers for a proper filtering process. The details of these rules are discussed in Section \ref{ssec:rule_data_filtering}. After extraction, documents were assessed against predefined thresholds associated with each rule. Based on the threshold we filtered documents that did not pass the criteria. 
% %and tools for precise language handling.  
% using a \com{Gopher Rule} approach to ensure high standards. Based on these evaluations, documents were categorized into 'pass' and 'fail' groups, effectively segregating high-quality content for further analysis and applications.  

\noindent \textbf{Deduplication:} We applied the MinHash deduplication algorithm~\cite{kocetkov2022stack} to the filtered web dataset to eliminate redundant content.  

% This methodical approach not only streamlined the extraction of targeted content but also ensured the refinement of Bangla text, making it suitable for varied linguistic applications and analyses.



\subsection{Books}
We have compiled a diverse collection of open-source Bangla books, primarily in PDF format, spanning a broad temporal range from historical to contemporary works. Below we briefly discuss the steps taken to extract the text from the book collection.  


% We have collected text data from a variety of Bangla books, encompassing genres such as novels, essays, poetry, and academic materials. Figure - \ref{fig:Pipeline for Bangla Data Collection} briefly describes the processes of book data collection. The following steps outline the process of obtaining the final version of the raw text data from these books.
% % \begin{figure}[h]
% %     \centering
% %     \includegraphics[width=\linewidth]{figures/book-data-collection.png}
% %     \caption{Pipeline for book data extraction.}
% %     \label{fig:Pipeline for book data extraction}
% % \end{figure}

% \subsubsection{Books from Online Sources} Our collection includes about 75,000 books sourced from a range of online platforms, such as Library Genesis \cite{libgen}, Z-Library \cite{zlibrary}, the Internet Archive \cite{internetarchive}, Annas Archive \cite{annasarchive}, BDEbooks \cite{bdebooks}, and the National Curriculum and Textbook Board \cite{nctb}. These books represent a wide span of time, including both contemporary publications and older works. Figure-\ref{fig:book-data-extraction-pipeline} describes the full pipeline of text data extraction from books. 

% \noindent \paragraph{Raw Text} For digitally available texts, we directly extract the machine-readable content, which requires minimal processing due to the already high quality of these formats.

% \noindent \paragraph{PDF Images} 
% In our efforts to digitize a vast array of non-digital and older texts, we employ two prominent Optical Character Recognition (OCR) technologies: Google OCR\footnote{\url{https://cloud.google.com/use-cases/ocr}} and Tesseract\footnote{\url{https://github.com/tesseract-ocr/tesseract}}. These texts, often derived from sources that have deteriorated over time or were never intended for digital use, pose significant challenges due to their varied quality and legibility. To mitigate these issues and improve the accuracy of text extraction, we implement a meticulously designed process that involves several critical steps.

\noindent \paragraph{Raw Text:} For digitally available texts, we directly extract the machine-readable content, requiring minimal processing due to the already high quality of these formats.

\noindent \paragraph{PDF Images:} 
To digitize a vast collection of non-digital and older texts from books, we utilize two leading Optical Character Recognition (OCR) systems: Google OCR\footnote{\url{https://cloud.google.com/use-cases/ocr}} and Tesseract\footnote{\url{https://github.com/tesseract-ocr/tesseract}}. We used Tesseract to reduce the cost. These texts, often derived from sources that have deteriorated over time or originated in non-digital formats, pose significant challenges in terms of quality and legibility. To extract high-quality text, we implement carefully designed techniques comprising several steps.

\noindent \textbf{Text extraction using Google OCR:} For the majority of books, we utilized Google OCR to extract text. Although the OCR accuracy was generally high for more recent books, the quality varied significantly for older books. Identifying and filtering out poor-quality text from a large and diverse collection proved to be a challenging task. To address this issue, we applied several quality-control techniques: \textit{(i)} using KenLM \cite{heafield2011kenlm} to filter noisy text based on ranking, \textit{(ii)} evaluating the average number of words and sentences per page, and \textit{(iii)} calculating the percentage of correct Bangla words. Details are discussed in Section \ref{ssec:app_rules_google_ocr}.

\noindent \textbf{Document Segmentation and Tesseract OCR:} We performed document segmentation alongside OCR for a smaller portion of the collected books. Initially, we trained a YOLO segmentation model on a Bangla document segmentation dataset \cite{shihab2023badladlargemultidomainbengali} to identify and classify different components of the documents (e.g., text boxes, tables, paragraphs, and images). We removed complex sections such as tables and images, as they were not relevant to the text extraction process. Subsequently, we applied Tesseract OCR to the remaining document text and repeated the filtering process outlined earlier. In addition to the filtering steps, we introduced an additional measure: we calculated the number of words with more than 80\% confidence and set a threshold at the 95th percentile to filter out low-quality text. After applying all these processes, 50\% of the initial data was retained.    


% \noindent \subparagraph{A. Text extraction using Google OCR:} 
% For the majority of books, we utilized Google OCR to extract text. Although the OCR accuracy was generally high for the more recent books, the quality for older books varied significantly. Identifying and filtering out poor-quality text from a large and diverse collection proved to be a challenging task. To address this issue, we applied several quality-control techniques 

% \begin{itemize}
% \item \textbf{Use of KenLM \cite{heafield2011kenlm}:} KenLM is an efficient statistical language modeling toolkit commonly used for constructing n-gram language models. We trained a language model using high-quality text data, which enabled us to calculate word and sentence scores for the OCR-extracted text. A histogram of these scores was plotted, and thresholds were set based on the distribution to identify poorly recognized sections. The threshold for filtering low-quality text was set at 95\%, meaning that only text with a score above the 95th percentile of the histogram was retained. Mathematically, this can be expressed as:
% \[
% \text{Score Threshold} = \text{Percentile}(95)
% \]
% Where the \(\text{Percentile}(95)\) function returns the score value at the 95th percentile of the word and sentence scores.

%     \item \textbf{Word and Sentence Count in Documents:} We calculated the average number of words and sentences per page in the collected books. A minimum threshold (95th percentile) for these counts was established to help filter out books with low-quality text. 
%     \item \textbf{Percentage of Correct Bengali Words:} We compiled a list of common Bengali words and computed the percentage of these words in each book. A threshold (95th percentile) was determined based on the overall distribution of Bengali word occurrences across the corpus.
% \end{itemize}

% Once these thresholds were established, they were applied across the entire dataset, resulting in the filtering of approximately 40\% of the raw text data.

% \subparagraph{B. Document Segmentation and Tesseract OCR:} We have performed document segmentation in conjunction with OCR for a smaller portion of collected books because our Google OCR credits are limited. Initially, we have trained a YOLO segmentation model on a Bengali document segmentation dataset \cite{shihab2023badladlargemultidomainbengali} to identify and classify different components of the documents (e.g., textboxes, tables, paragraphs, and images). We removed complex sections such as tables and images, as they were not relevant to the text extraction process. Subsequently, we applied Tesseract OCR to the remaining document text and repeated the filtering process outlined earlier. Besides applying the filtering steps we have added another step here. We have calculated the number of words with more than 80\% confidence and set a threshold (95th percentile) to filter low-quality text. 50\% of the initial data is taken after applying all of these processes.

\subsection{Synthetic Data}
Due to the low representation of digital content in Bangla, we have developed a large-scale synthetic dataset for Bangla, which include transcription, translation and transliterated data. 

% \subsubsection{Transcribed Text}
\paragraph{Transcribed Text:}  
We collected conversational and spoken language data transcribed using the Bangla Automatic Speech Recognition (ASR) system~\cite{nandi-etal-2023-pseudo}. This system enables us to capture various colloquial and regional linguistic variations in Bangla. We collected approximately 56k hours of speech data from diverse online sources. All collected speech data were transcribed using the ASR system.

\paragraph{Translation Data:}
% \com{it is not clear why you also used The Llama-3.1-8B-Instruct for translation? Was not the trained MT model enough?} 

% \com{From Nahin: It wasn't enough because most of the open-source translated data was synthetically translated from English to Bengali. We wanted to ensure better quality and quantity. That's why we have curated clean Bengali text and translated Bengali text to English text. As the Bengali text is already curated from Wikipedia and news sources, they are good for training. Additionally, we found that llama3.1-8b can generate English text well rather than Bengali. As a result, we get a high-quality Bengali-English translation pair for training.}

To collect English-to-Bangla translated data, we trained an NLLB-based (600M-Distilled) model \cite{nllbteam2022languageleftbehindscaling} with the goal of developing a smaller, language-pair-specific (en-bn) model. We decided to train a translation model because our observations indicate that currently available multilingual models, such as Llama-3.1-8B-Instruct, have limited capability for Bangla-specific generation tasks. However, they have shown superior performance in English-specific generation.
% Additionally, our observations indicate that currently available multilingual models, such as Llama-3.1-8B-Instruct, are not sufficiently accurate for Bangla-specific generation tasks.

For training the en-bn machine translation (MT) model, we collected open-source translation data from various platforms, including BanglaNMT \cite{hasan-etal-2020-low} and Samanantar \cite{ramesh-etal-2022-samanantar}. Furthermore, we generated synthetic bn-en translation pairs using Bangla news sources and Wikipedia as source data, employing Llama-3.1-8B-Instruct ~\cite{touvron2023llama} for target data generation. We selected Llama for this task due to its superior English-language capabilities.

Using this approach, we created a dataset comprising approximately 60 million translation pairs, which we then used to train the NLLB model. On our in-house test dataset, the BLEU score of this model is 37.6. Once the model has been trained, we have used it to translate a corpus of English news articles \footnote{\url{https://www.kaggle.com/datasets/davidmckinley/all-the-news-dataset}} into Bangla.

% we have collected and trained our model with 

\paragraph{Transliteration Data:}
The use of romanized text is very common in everyday communication for Bangla~\cite{fahim2024banglatlit}. To address this, we have developed a Bangla-to-Romanized Bangla dataset by training an NLLB-based (600M-Distilled) model. For model training, we collected transliteration pairs from the Sangraha dataset and generated additional synthetic transliteration pairs using the GPT-4 model~\cite{achiam2023gpt4}. We then used this dataset to train the NLLB-based transliteration model. The BLEU score for this model is 65.1, as evaluated on an in-house test dataset. We then used this model to create the transliteration dataset by selecting a small subset of collected Bangla Wikipedia articles.

% We have also prepared a Bengali-to-Romanized Bengali transliteration dataset for training the NLLB-600M-Distilled model. \com{not clear -- The dataset includes both synthetic transliteration pairs sourced from the Sangraha Dataset and additional synthetic transliteration pairs generated using the GPT-4 model~\cite{achiam2023gpt4}.}

% \com{From Nahin: Synthetic data from Sangraha mostly contains Indian transliteration style, whereas we have generated some transliteration pair that follows Bangladeshi transliteration style. For example, 'যুব' is spelled as 'Yuva' in Indian style but in Bangladesh it is transliterated as 'Jubo'.}

% \cite{Nandi: vai, taking information from the team} 


% and used the trained model to produce a transliterated text corpus. This corpus was then incorporated into the final dataset for further processing and model training.

% \cite{openai2024gpt4technicalreport}. 

\paragraph{Conversational Data:}
To enhance the model with conversational capabilities, we enriched our dataset by incorporating conversational data. We have crawled topics (e.g.,  \textit{``Rabindranath Tagore's contributions
to Bengali art''}) from Wikipedia and Banglapedia on which we generated conversations between two agents. To achieve this we have developed an agentic system where two agents interacted each other on a given topic. In Table \ref{tab:agent_details}, we have provided examples of topic, roles and a detail of the prompt. An example conversation is provide in Figure \ref{fig:data_collection_final}. The average number of turns per conversation is 8. In total, we added approximately 1 million conversational data to the dataset. 
% The system was an automated system where agents were engaged in discussion with each other on various topics. 
% To maintain the quality of the dataset we have used a complex agentic system here which can work as a self-evaluator of the generated conversations. 


\subsection{Sangraha Dataset}
Additionally, we enriched our dataset by integrating the open-source Sangraha dataset~\cite{khan2024indicllmsuite}. Sangraha is the largest high-quality, cleaned Indic language corpus. We incorporated the Bangla portion of the Sangraha dataset into our training set. 
%, which contains approximately 30b tokens.  




% \begin{figure}[t]
%   \includegraphics[width=\columnwidth]{figures/tts_paper-1-shot.png}
%   \caption{ Radar charts to visualize the accuracies  of TituLLM-3B and Five other  models in 5-shot setting}
%   \label{fig:bangla_eval_data-0}
% \end{figure}

% \paragraph{Domain Specific Data}

% ++To ensure the diversity of the dataset, we incorporated domain-specific data. We collected over 10 thousand Bangladeshi web urls, focusing on high-indexed and information-rich pages from a variety of Bangladeshi websites. Next, we crawled raw text and all associated documents from these targeted websites. To achieve this, we used the open-source web scraping tool Trafilatura \cite{barbaresi-2021-trafilatura} alongside our custom-designed web crawler. This approach effectively handled the complex structures of websites and successfully extracted the majority of raw text from the pages. 

% \subsection{Data Processing and Quality Assurance}
% \subsection{Preprocessing Pipeline}



% A multi-stage data refinement pipeline was implemented to uphold the dataset's quality, involving:
% \begin{enumerate}
% \item \textbf{Deduplication:} Removal of redundant content to ensure uniqueness.
% \item \textbf{Noise Filtering:} Elimination of syntactically or semantically anomalous data.
% \item \textbf{Encoding Consistency:} Standardization of character encodings to maintain text integrity.
% \item \textbf{Linguistic Validation:} Manual and automated checks to validate contextual and grammatical correctness.
% \item \textbf{Content Filtering:} Exclusion of inappropriate or low-quality data to enhance overall usability.
% \end{enumerate}

% The rigorous preprocessing pipeline guarantees a linguistically diverse and high-fidelity dataset, positioning it as a robust foundation for advanced Bangla language modeling and natural language processing research.


% We have prepared two types of datasets. A detailed description is given here. 
% \subsection{Translated Data}
% \subsubsection{Data Collection}
% \subsubsection{Data Description}
% \# TOdo by Safee 

% \subsection{Curated Data}
% \subsubsection{Data Collection}
% \textbf{Scraping and manual checking:}
% \textbf{Heuristic and LLM: }
% \# TOdo by Nahin 

% \subsubsection{Data Description}


% \subsection{Preprocessing Pipeline}

