\begin{table*}[htb!]
    \centering
    \begin{tabular}{lccccccc}
        \toprule
        Model & Relevance & Clarity & Originality & Completeness & Specificity & Correctness & Consistency \\
        \midrule
        titulm-1b  & 3.370  & 2.928  & 2.345  & 2.478  & 2.287  & 2.858  & 2.590  \\
        titulm-3b  & 4.501  & 5.058  & 3.953  & 3.878  & 3.785  & 4.198  & 4.760  \\
        llama-1b   & 3.178  & 3.185  & 2.118  & 2.149  & 2.205  & 2.576  & 2.810  \\
        llama-3b   & 5.337  & 5.016  & 3.760  & 4.144  & 4.099  & 4.606  & 4.805  \\
        gemma-2b   & \textbf{5.851}  & \textbf{5.670}  & \textbf{5.056}  & \textbf{4.998}  & \textbf{4.963}  & \textbf{5.798}  & \textbf{5.974}  \\
        qwen-1.5b  & 2.707  & 2.509  & 1.827  & 1.801  & 1.840  & 2.198  & 2.265  \\
        \bottomrule
    \end{tabular}
    \caption{Performance (LLM as a judge score) comparison of various Instruction tuned LLMs on alpaca eval dataset}
    \label{tab:model_comparison_alpaca}
\end{table*}


\begin{table*}[htb!]
    \centering
    \begin{tabular}{lccccccc}
        \toprule
        Model & Relevance & Clarity & Originality & Completeness & Specificity & Correctness & Consistency \\
        \midrule
        titulm-1b  & 3.033  & 2.742  & 2.183  & 2.292  & 2.083  & 2.267  & 2.317  \\
        llama-1b   & 3.025  & 3.033  & 1.975  & 2.083  & 2.200  & 1.817  & 2.525  \\
        titulm-3b  & 4.583  & 4.725  & 3.967  & 3.783  & 3.925  & 3.742  & 4.425  \\
        llama-3b   & 6.033  & 5.600  & 4.358  & 4.850  & 4.975  & 5.133  & 5.442  \\
        gemma-2b   & \textbf{6.367}  & \textbf{5.692}  & \textbf{4.833}  & \textbf{4.975}  & \textbf{5.592}  & \textbf{5.892}  & \textbf{6.183}  \\
        qwen-1.5b  & 3.092  & 2.608  & 1.958  & 2.100  & 2.233  & 2.300  & 2.408  \\
        \bottomrule
    \end{tabular}
    \caption{Performance (LLM as a judge score) comparison of various Instruction tuned LLMs on bong eval dataset}
    \label{tab:model_comparison_bong}
\end{table*}


\begin{table*}[htb!]
    \centering
    % \begin{tabular}{lccccccccc}
    %     \toprule
    %     Model & Entertainment & Generation & Literature & Coding & Translation & Ethics & Factual QA & Reasoning & Open QA \\
    %     \midrule
    %     titulm-1b  & 3.444  & 3.371  & 3.260  & 1.541  & 1.800  & 1.089  & 2.776  & 1.773  & 2.731  \\
    %     titulm-3b  & 5.302  & 4.429  & 4.857  & 2.092  & 2.857  & 2.804  & 4.255  & 4.221  & 5.849  \\
    %     gemma-2b   & 3.825  & \textbf{5.676}  & 4.377  & \textbf{7.796}  & \textbf{6.543}  & 6.375  & \textbf{4.745}  & \textbf{6.662}  & 4.202  \\
    %     llama-3b   & \textbf{5.524}  & 4.276  & 4.208  & 5.857  & 5.157  & \textbf{7.589}  & 3.755  & 6.429  & 4.437  \\
    %     llama-1b   & 2.317  & 1.533  & 3.013  & 1.510  & 3.557  & 1.214  & 1.663  & 3.831  & 2.034  \\
    %     qwen-1.5     & 1.556  & 1.838  & 2.234  & 2.327  & 4.571  & 1.089  & 1.480  & 3.558  & 2.008  \\
    %     \bottomrule
    % \end{tabular}
    \begin{tabular}{lcccccc}
        \toprule
        Category & titulm-1b & titulm-3b & gemma2-2b & llama3.2-3b & llama3.2-1b & qwen2.5-1.5b \\
        \midrule
        Entertainment  & 3.444  & 5.302  & 3.825  & \textbf{5.524}  & 2.317  & 1.556  \\
        Generation     & 3.371  & 4.429  & \textbf{5.676}  & 4.276  & 1.533  & 1.838  \\
        Literature     & 3.260  & 4.857  & 4.377  & 4.208  & 3.013  & 2.234  \\
        Coding        & 1.541  & 2.092  & \textbf{7.796}  & 5.857  & 1.510  & 2.327  \\
        Translation    & 1.800  & 2.857  & \textbf{6.543}  & 5.157  & 3.557  & 4.571  \\
        Ethics        & 1.089  & 2.804  & 6.375  & \textbf{7.589}  & 1.214  & 1.089  \\
        Factual QA    & 2.776  & 4.255  & \textbf{4.745}  & 3.755  & 1.663  & 1.480  \\
        Reasoning     & 1.773  & 4.221  & \textbf{6.662}  & 6.429  & 3.831  & 3.558  \\
        Open QA       & 2.731  & \textbf{5.849}  & 4.202  & 4.437  & 2.034  & 2.008  \\
        \bottomrule
    \end{tabular}
    \caption{Performance (LLM as a judge score) comparison of various Instruction tuned LLMs on bong eval dataset for each categories}
    \label{tab:category_comparison_bong_category}
\end{table*}
