\section{Evaluation}

\subsection{Evaluation Setup}
For evaluation, we utilized the lm-evaluation-harness.\footnote{\url{https://github.com/EleutherAI/lm-evaluation-harness}} We used normalized accuracy as a metric. Our assessment focuses on key aspects such as knowledge and reasoning.

\subsection{Banchmarking Datasets}
We benchmarked \titu{} alongside other popular LLMs using five newly prepared evaluation datasets. 
Below, we describe the development process for each dataset. Table~\ref{tab:benchmarking_dataset} presents the distribution and splits of each dataset.

% \begin{figure}[htb!]
% \centering
% \includegraphics[width=0.6\columnwidth]{figures/evaluation_data.png}
% \vspace{-0.3cm}
% \caption{Distribution of an benchmarking dataset totaling $\sim132k$ entries.
% % distribution of an evaluation dataset totaling 131,928 entries across various subsets: Bangla MMLU with 87,869 entries, Piqa BN with 17,177 entries, Commonsenseqa BN with 10,962 entries, Openbookqa BN with 5,944 entries, and Boolq BN with 1,976 entries. 
% % This highlights the significant representation of the Bangla MMLU subset within the evaluation data.
% }
% \label{fig:bangla_eval_data}
% \vspace{-0.3cm}
% \end{figure}

% We evaluated our pre-trained models on both Bangla and English benchmark datasets. Despite being primarily trained on Bangla data, the modelâ€™s English capabilities were assessed to explore its cross-lingual potential. 
% In this section, we report the results for \textbf{titulm-gemma-2-2b-v1.0} models on standard automatic benchmarks. All evaluations were conducted using the \texttt{lm-evaluation-harness} library to ensure consistency and reproducibility.

% \begin{itemize}[noitemsep,topsep=0pt,labelsep=.5em]
\noindent \textbf{Bangla MMLU:} 
% A proprietary multiple-choice question dataset developed by Hishab, curated 
We curated multiple-choice questions from various opensource educational websites
% , such as Aapathshala\footnote{\url{https://aapathshala.com/}} 
and textbooks, inspired by the original MMLU dataset~\cite{hendrycks2020measuring}. The dataset includes multiple-choice questions from different Bangladeshi exams, such as job exams, the Bangladesh Civil Service Exam, and undergraduate admission exams. In Figure \ref{fig:bangla_mmlu_data}, we report category wise distributions. 

\noindent  \textbf{CommonsenseQA Bangla (CSQA):} We translated the CommonsenseQA dataset~\cite{talmor2018commonsenseqa} into Bangla using our custom translation-based approach, \textit{Expressive Semantic Translation (EST)}. This method generates multiple translations for a sentence and iteratively refines them to select the most suitable version. More details on this approach are discussed in Section~\ref{sec:app_est}.

% A Bangla translation of the CommonsenseQA dataset \cite{talmor2018commonsenseqa}, utilizing a novel \textit{Expressive Semantic Translation (EST)}(described in appendix) method. This method takes multiple translations for a sentence and iteratively refines to get most candidate version. 

\noindent \textbf{OpenBookQA Bangla (OBQA):} This dataset is a Bangla translation of the OpenBookQA dataset~\cite{mihaylov2018can}, which is designed to test a model's ability to apply elementary science knowledge to answer open-domain multiple-choice questions. We translated OpenBookQA into Bangla using our EST method.

\noindent  \textbf{PIQA Bangla (PIQA):} This dataset is a Bangla translation of the Physical Interaction: Question Answering (PIQA) dataset~\cite{bisk2020piqa}, which evaluates a model's understanding of everyday physical reasoning and common-sense interactions. PIQA consists of multiple-choice questions requiring knowledge about how objects interact in the real world, such as choosing the most practical way to perform a given task. For the translation, we used our EST method. 

\noindent  \textbf{BoolQ Bangla (BoolQ):} This dataset is inspired by BoolQ~\cite{clark2019boolq}, a reading comprehension benchmark that evaluates a model's ability to answer yes/no questions based on a given passage. The dataset consists of triplets in the form of (question, passage, answer). Passages were sourced from Bangla Wikipedia, Banglapedia, and news articles, ensuring a diverse range of topics and contexts. To generate high-quality questions and answers, we leveraged GPT-4.

\begin{table}[h]
\centering
\setlength{\tabcolsep}{3pt} 
\scalebox{0.75}{%
\begin{tabular}{@{}llrrrr@{}}
\toprule
\textbf{Dataset} & \textbf{Method} & \textbf{Train} & \textbf{Val.} & \textbf{Test} & \textbf{Dev} \\ 
\midrule
Bangla MMLU & Manual & - & 72,944 & \textbf{14750} & 175 \\ 
BoolQ & GPT-4 & 815 & 432 & \textbf{729} & - \\ 
CommonsenseQA & EST & 9,741 & \textbf{1,221} & - & - \\
OpenBookQA & EST & 4,947 & 500 & \textbf{497} & - \\
PIQA & EST & 15,339 & \textbf{1,838} & - & - \\ 
\bottomrule
\end{tabular}
}
\vspace{-0.2cm}
\caption{Data splits and distribution of the Benchmark dataset. Val.: Validation.}
\label{tab:benchmarking_dataset}
\vspace{-0.3cm}
\end{table}


% with 15,942 examples
% \end{itemize}


% \subsubsection{English Benchmark Datasets}

% \begin{itemize}
% \item \textbf{MMLU:} A massive multitask test consisting of multiple-choice questions across diverse domains of knowledge.
% \item \textbf{CommonsenseQA:} A dataset requiring commonsense reasoning to predict answers to multiple-choice questions.
% \item \textbf{OpenbookQA:} Designed to probe deeper understanding by requiring models to integrate open-book facts with contextual language.
% \item \textbf{PiQA:} Focused on physical commonsense reasoning, challenging AI to understand practical knowledge and unconventional solutions.
% \item \textbf{BoolQ:} A dataset for yes/no questions containing naturally occurring, unprompted examples in a text-pair classification format.
% \end{itemize}

% \subsection{Evaluation Results}

% \subsubsection{Evaluation on Bangla Benchmark Datasets}

% \begin{itemize}
% \item \textbf{gemma-2-2b} performs better in Bangla MMLU and BoolQ BN in the 0-shot setting.
% \item \textbf{titulm-gemma-2-2b-v1.0} outperforms in Commonsense QA BN, OpenBook QA BN, and PIQA BN across both 0-shot and 5-shot settings.
% \item In the 5-shot setting, \textbf{titulm-gemma-2-2b-v1.0} achieves the highest scores in BoolQ BN, Commonsense QA BN, and OpenBook QA BN.
% \item PIQA BN shows consistent performance across both models, with \textbf{titulm-gemma-2-2b-v1.0} leading in both settings.
% \end{itemize}

% \begin{table}[h]
% \centering
% \begin{tabular}{|l|c|c|c|c|c|}
% \hline
% \textbf{Model} & \textbf{Shots} & \textbf{Bangla MMLU} & \textbf{BoolQ BN} & \textbf{Commonsense QA BN} & \textbf{OpenBook QA BN} & \textbf{PIQA BN} \
% \hline
% \textbf{gemma-2-2b} & 0-shot & 0.32 & 0.63 & 0.26 & 0.34 & 0.56 \
% & 5-shot & 0.35 & 0.46 & 0.28 & 0.33 & 0.56 \
% \hline
% \textbf{titulm-gemma-2-2b-v1.0} & 0-shot & 0.31 & 0.59 & 0.31 & 0.36 & 0.63 \
% & 5-shot & 0.35 & 0.59 & 0.41 & 0.37 & 0.62 \
% \hline
% \end{tabular}
% \caption{Performance on Bangla Benchmark Datasets}
% \end{table}

% \subsubsection{Evaluation on English Benchmark Datasets}

% \begin{itemize}
% \item \textbf{gemma-2-2b} outperforms \textbf{titulm-gemma-2-2b-v1.0} across all tasks in both 0-shot and 5-shot settings, achieving the highest scores in MMLU, BoolQ, Commonsense QA, OpenBook QA, and PIQA, with a peak 5-shot score of 0.80 in PIQA.
% \item \textbf{titulm-gemma-2-2b-v1.0} shows competitive performance but lags behind \textbf{gemma-2-2b}, particularly in Commonsense QA and BoolQ, with the highest score being 0.77 in PIQA.
% \item This is expected, as the model was trained exclusively on Bangla text.
% \end{itemize}