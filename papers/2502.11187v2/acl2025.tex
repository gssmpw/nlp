% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath} 
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{inconsolata}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}
\usepackage{tabu}
\usepackage{fontawesome}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{xstring}
\usepackage{graphicx}
\usepackage{pbox}
\usepackage{subcaption}
\usepackage{epstopdf}
\usepackage{xstring}
\usepackage{multirow}
\newcommand{\sq}{\faCheckSquare}
\newcommand{\cq}{\faCheck}
\usepackage{soul}
\usepackage{color}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{comment}
\usepackage{todonotes}
\usepackage{tcolorbox}

% \usepackage{polyglossia}
\usepackage{listings} % Add this for using the listings package
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{verbatim}
\usepackage{stfloats}
\renewcommand{\UrlFont}{\ttfamily\small}

% \usepackage{xltxtra}  % Extra features for XeLaTeX
% \usepackage{polyglossia}
% \setmainlanguage{english}  % Set main language as English
% \setotherlanguage{bengali}  % Enable Bengali language
% \newfontfamily\bengalifont[Script=Bengali]{Noto Sans Bengali} % Set Bengali font


\lstset{
  basicstyle=\ttfamily, %\small,
  breaklines=true,
  aboveskip=4mm,
  belowskip=4mm,  
  captionpos=b, % Position the caption at the bottom
  %frame=single, % Adds a frame around the listing
  columns=fullflexible
}

\usepackage{xcolor} % For coloring text
\definecolor{lightblue}{rgb}{.50,.90,0.51}
\definecolor{tri}{rgb}{.25,.88,.82}
\definecolor{lilac}{rgb}{0.85,0.64,0.85}
\definecolor{atomictangerine}{rgb}{1.0, 0.6, 0.4}
% \newcommand\firoj[1]{\hlc[blue]{{\bf Firoj}: #1}}
\newcommand{\firoj}[1]{{\color{blue}\textbf{Firoj}: #1}}
\newcommand{\com}[1]{{\color{red}\textbf{Firoj}: #1}}
\newcommand{\titu}{\emph{TituLLMs}}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{TituLLMs: A Family of Bangla LLMs with Comprehensive Benchmarking}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}


\author{
\textbf{Shahriar Kabir Nahin}\textsuperscript{1},
\textbf{Rabindra Nath Nandi}\textsuperscript{1},
\textbf{Sagor Sarker}\textsuperscript{1}, \\
\textbf{Quazi Sarwar Muhtaseem}\textsuperscript{1}, 
\textbf{Md Kowsher}\textsuperscript{2},
\textbf{Apu Chandraw Shill}\textsuperscript{1},
\textbf{Md Ibrahim}\textsuperscript{1}, \\
\textbf{Mehadi Hasan Menon}\textsuperscript{1},
\textbf{Tareq Al Muntasir}\textsuperscript{1}, 
\textbf{Firoj Alam}\textsuperscript{3}, \\
\textsuperscript{1}Hishab Singapore Pte. Ltd, Singapore, 
\textsuperscript{2}University of Central Florida, USA \\
\textsuperscript{3}Qatar Computing Research Institute, Qatar
}




\begin{document}
\maketitle
\begin{abstract}
In this paper, we present \titu{}, the \textit{first} large pretrained Bangla LLMs, available in 1b and 3b parameter sizes. Due to computational constraints during both training and inference, we focused on smaller models. To train \titu{}, we collected a pretraining dataset of approximately $\sim37$ billion tokens. We extended the Llama-3.2 tokenizer to incorporate language- and culture-specific knowledge, which also enables faster training and inference.
There was a lack of benchmarking datasets to benchmark LLMs for Bangla. To address this gap, we developed \textit{five benchmarking datasets}. We benchmarked various LLMs, including \titu{}, and demonstrated that \titu{} outperforms its initial multilingual versions. However, this is not always the case, highlighting the complexities of language adaptation. Our work lays the groundwork for adapting existing multilingual open models to other low-resource languages. To facilitate broader adoption and further research, we have made the \titu{} models and benchmarking datasets publicly available.\footnote{\url{https://huggingface.co/collections/hishab/titulm-llama-family-6718d31fc1b83529276f490a}}


% Significant progress has been made in large language models (LLMs), leading to their widespread adoption across various disciplines and applications. However, a substantial gap remains in their capabilities for low-resource languages. One approach to addressing this issue is to adapt existing open LLMs for new languages. This, however, presents several challenges, including the need for vast amounts of training data, limitations in computational resources, and the difficulty of collecting benchmarking datasets.

% To benchmark the \titu{} including vario 
% explore the development of language-specific LLMs for Bangla and provide a comprehensive benchmarking analysis of several multilingual LLMs. 

% and develop \textit{five benchmarking datasets}. We train a family of models, \titu{}, based on Llama, by extending the tokenizer, which is the \textit{first} set of Bangla LLMs trained with a largest dataset. Our findings show that in several benchmarking datasets, the adapted LLMs outperform their initial multilingual versions. However, this is not always the case, highlighting the complexities of language adaptation.
% Our work lays the groundwork for adapting existing multilingual open models to other low-resource languages. To facilitate broader adoption and further research, we have made the \titu{} models and benchmarking datasets publicly available.\footnote{\url{anonymous.com}}  
% The widespread adoption of large language models (LLMs) for both research and industrial applications necessitates a comprehensive understanding of their performance across various languages, especially low-resource languages like Bangla. As LLMs such as ChatGPT, LLaMA, and others are increasingly employed in multilingual contexts, it is critical to evaluate their capabilities in these underrepresented languages. In this work, we focus on benchmarking popular multilingual LLMs on Bangla, a low-resource language, by creating diverse test sets comprising multiple-choice questions (MCQ), question-answer (QA) pairs, and true/false statements. Our aim is to assess the competency, strengths, and limitations of these models when handling Bangla-specific tasks. This benchmark evaluation will provide valuable insights into the performance gaps of current LLMs in Bangla and highlight potential areas for improvement, paving the way for more equitable advancements in natural language processing across different languages. The experimental resources will be publicly available.\footnote{\url{}}
\end{abstract}

\input{sections/introduction}
%\input{sections/dataset_details}
 \input{sections/dataset}

% \input{sections/EST}
\input{sections/experiments}
\input{sections/benchmarking}
%\input{sections/experiments}
\input{sections/results_and_discussion}
\input{sections/related_work}

% \section{Discussion}



% \section{Conclusion and Future Work}
% \label{sec:conclusion}

% The \textit{Titulm} series demonstrates competitive performance among models of comparable scale on Bengali tasks, with the 3B variant consistently outscoring its 1B counterpart. Moving from 1B to 3B parameters yields more robust improvements in tasks that involve commonsense or specialized reasoning. Additional Bengali tokens in the tokenizer have helped the model to understand the relation between words better. However, as a result of limited training the model performs less for long contexts. The model also shows minimal improvement in the Bangla MMLU dataset, highlighting potential directions for more specialized domain adaptation and training. Another drawback of our training is that the models are trained with only Bengali text data. To increase the training data and ensure more knowledge leveraging larger English-centric datasets can boost low-resource language performance. Such investigations could guide the development of more capable \textit{Titulm} models for under-resourced languages like Bengali, ultimately improving generalization and knowledge retrieval across a diverse range of tasks. Additionally, in future, these base models can be used for making generalized Bengali LLMs. However, with the lack of enough instruction tuning data available in Bengali, this task is beyond this research work for now.

\section{Conclusion}
\label{sec:conclusion}
% In this study, we present the \textit{first} pretrained Bangla LLMs (\textit{Titulm}) with $\sim37b$ tokens by adapting Llama-3.2 models. We have extended tokenizer to enrich language and culture specific knowledge, which also enables faster training and inference. Pretraining data collection is challenging for languages with low digital representation. We provide a complete recipe including raw web data collection, translation and synthetic data generation. Given that there was a lack of LLM based benchmakring dataset, therefore, we have developed five datasets consisting of $137k$ samples, covering knowledge and reasoning. The benchmakring dataset include both manual and our novel translation based approach. Using these dataset, we benchmarked different LLMs including \textit{Titulm}. We demonstrate that performs better in reasoning tasks without any instruction tuning. Future work include collecting large pretraining  datasets, and fine-tuning with instruction datasets. We have made our models publicly available for the community and for the reproduciblity, we aim to release training receipes and benchmarking datasets.
In this study, we present the \textit{first} pretrained Bangla LLMs, \textit{Titulm}, trained on $\sim37b$ tokens by adapting Llama-3.2 models. We extended the tokenizer to incorporate language- and culture-specific knowledge, which also enable faster training and inference. Pretraining data collection remains challenging for languages with low digital representation. To address this, we provide a comprehensive approach, including raw web data collection, translation, and synthetic data generation. Given the lack of LLM-based benchmarking datasets, we developed \textit{five datasets} comprising $137k$ samples, covering both knowledge and reasoning. The benchmarking dataset includes manually curated samples as well as a novel translation-based (EST) approach. Using these datasets, we benchmarked various LLMs, including \textit{Titulm}, demonstrating its superior performance in reasoning tasks without instruction tuning. Future work includes collecting larger pretraining datasets and fine-tuning with instruction-based datasets. We have made our models publicly available, and to support reproducibility, we plan to release training recipes and benchmarking datasets.


% \textit{Titulm} series, 
% This paper presents significant advancements in developing monolingual Bangla language models through the \textit{Titulm} series.  
% By extending the tokenizer and leveraging a large-scale Bangla MMLU dataset in our benchmarking, we demonstrate the effectiveness of these models in enhancing word relation understanding and improving performance on commonsense and specialized reasoning tasks.  
% This work lays a strong foundation for the development of more capable models for Bangla and other low-resource languages, offering valuable resources for further research and application in multilingual LLMs. Additionally, we introduce a novel English to Bangla translation technique to enhance the translation data quality, contributing to more accurate and context-aware translations.

% Additionally, it highlights the potential for domain adaptation and the importance of creating high-quality instruction tuning datasets to further improve performance in specialized tasks.
 
\section{Limitations}
\label{sec:limitations}
There are two limitations in this work. Firstly, despite the improvements observed in the 3b variant, the model's performance on long contexts remains suboptimal, suggesting the need for further enhancement in handling extended sequences. Secondly, while the current models are trained solely on Bangla text, their performance could benefit from incorporating larger, English-centric datasets. This could facilitate better knowledge leveraging and potentially improve low-resource language performance, indicating a direction for future research. Since there is a lack of instruction tuning data in Bangla, we do not explore the full potential of instruction tuning, which could have further improved the models' performance on specialized tasks and domain adaptation.

\section*{Ethical Consideration}
\label{sec:ethics}
We do not anticipate any ethical concerns in this study. All datasets used were collected from publicly available sources, ensuring compliance with ethical research standards. No personally identifiable information (PII) was gathered or utilized in the development of our models. While we do not foresee any potential risks arising from the outcomes of this study, we strongly encourage users of the released models to adhere to responsible AI usage guidelines. This includes avoiding the generation or dissemination of harmful, misleading, or biased content and ensuring that the models are employed in ethical and socially beneficial applications.


% \section*{Acknowledgments}

% Acknowledgments section here 


\bibliography{bibliography}
% \bibliographystyle{acl_natbib}

\appendix

\input{sections/appendix}

\label{sec:appendix}


\end{document}
