\section{Related Work}
\textbf{Self-supervised learning.} 
\looseness-1 
Self-supervised learning (SSL) leverages auxiliary tasks to learn from unlabeled data, often outperforming supervised methods on downstream tasks. SSL can be divided into two categories: discriminative and generative ____. Discriminative methods ____ focus on enforcing invariance or equivariance between data views in the representation space, while generative methods ____ rely on data reconstruction from, often, corrupted observations. Though generative SSL historically lagged in performance, recent work has bridged the gap by integrating strengths from both paradigms ____. Interestingly, recent discriminative methods employ cropping strategies to create distinct data views ____, which is reminiscent of image masking. ____ point out the misalignment between auxiliary and downstream tasks in reconstruction-based SSL and suggest novel masking strategies to help realign these objectives.

\textbf{Masked Image Modelling.} \looseness-1 MIM extends the successful masked language modeling paradigm to vision tasks. Early methods, such as Context Encoder ____, used a convolutional autoencoder to inpaint a central region of the image. The rise of Vision Transformers (ViTs) ____ has driven significant advancements in MIM. BEiT ____ combines a ViT encoder with image tokenizers ____ to predict discrete tokens for masked patches. SimMIM ____ simplifies the task by pairing a ViT encoder with a regression head to directly predict raw pixel values for the masked regions. MAE ____ introduces a more efficient encoder-decoder architecture, with a shallow decoder. MIMâ€™s domain-agnostic masking strategies have also proven effective in multi-modal tasks ____.

\textbf{Mask Design Strategies.} \looseness-1 A critical component of the masked image modeling paradigm is the design of effective masking strategies. Early MIM approaches have relied on random spatial masking techniques, such as masking out the central region of an image ____, image patches ____, and blocks of patches ____. Inspired by advances in language modeling, recent efforts have explored semantically guided mask design. ____ use self-attention maps to mask irrelevant regions, while ____ focus on masking semantically rich areas. ____ design masks through adversarial learning, where the resulting masks resemble semantic maps, a concept extended by ____ through progressive semantic region masking. Further advancing this direction, ____ and ____ introduce curriculum learning-inspired mask design methods. %
These methods often require additional training steps, components, or more complex objectives. More closely related to our work, ____ explore the use of pre-existing image representations for asked Image Modeling and image denoising. ____ introduce additive Gaussian noise to principal components as an alternative to the traditional Denoising Autoencoders. ____ utilize masked token modeling by leveraging the discrete latent space of a pre-trained VQVAE to develop an image generation model.