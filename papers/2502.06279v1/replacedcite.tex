\section{Related Works}
\label{related}
Reasoning in LLMs has been studied under multiple contexts such as logical reasoning, mathematical reasoning, theorem proving etc. In real life contexts, this extends to fields at the intersection of reasoning, decision-making, and communication, such as law, politics, and education. In this section we summarize the literature on natural language reasoning which is the closest related to our task.

% \noindent
\subsection{Natural Langauge Reasoning}
Natural Language datasets like \textbf{HellaSwag} ____,  \textbf{ARC} ____, and \textbf{MMLU} ____ test LLMs ability to understand natural language and ground their answers in reality. HellaSwag tests models' ability to complete sentence by choosing the most likely option from 4 sentences provided, while ARC and MMLU test models on either domain dependent information or common sense reasoning by asking questions on biology, law, economics, etc. The \textbf{TruthfulQA} ____ benchmark is especially designed to test models' grounding in reality by evaluating them on questions who's answer are prone to be misinformation or conspiratorial. 

Other benchmarks like \textbf{SuperGLUE} ____ and \textbf{WinoGrande} ____ test models' comprehension of natural languages by testing them on confusing or ambiguous passages.

More closely aligned to our work are argument evaluation benchmarks like \textbf{VivesDebate-Speech} ____, which is a dataset of 29 debates from the 2019 university debate tournament organized by the “Xarxa Vives d’universitats”. However, the debates in this benchmark are not originally English, and have been machine translated from Catalan. The credibility of machine translations in preserving complicated arguments is low. Moreover, DebateBench includes debates from renowned competitions hence the judges' scores are more credible and the debates are of a higher quality. Other "debating" datasets like \textbf{USElecDeb60To16} ____, and \textbf{ETHIC} ____ benchmark deal with political debates between U.S. Presidential candidates and in the British Parliament respectively. These debates are significantly different from competitive debates since the main focus is on rhetoric and not logical argumentation. These debates also don't have a quantifiable metric of evaluation. The \textbf{DebateSum} ____ deals with \textit{Policy Debates} wherein the topics are released as much a year ago and the competition focuses on the presentation of evidence and data instead of principled arguments. 

% \subsection{Argumentation and Complex Reasoning in LLMs}


% Traditionally, argumentation frameworks, such as those introduced by ____, have focused on abstract argumentation and formal representations. Recent efforts have shifted toward natural language reasoning, primarily utilizing pre-trained encoders and task-specific datasets ____. However, these approaches often lack generalizability, posing challenges in human alignment and dataset diversity. The emergence of LLMs has enabled the automation of debate evaluation tasks ____, yet critical gaps remain:

% \begin{enumerate}
% \item Long-context reasoning within human-aligned settings, such as debates, remains underexplored.
% \item Existing debate benchmarks are frequently derived from online sources, which may not reflect high-quality debates or expert-level argumentation.
% \end{enumerate}

% Debate quality and human alignment are particularly significant because debating transcends factual presentation, relying instead on principles such as argument weighing, quality assessment, and impact quantification. These aspects are central to adjudication criteria, as outlined in sources such as the WUDC judging manual.

\subsection{Long-Context Modeling Techniques}
Recent advancements in LLMs have integrated sophisticated long-context modeling techniques. For instance, LLaMa 2 employs Rotary Position Embedding (RoPE) ____, while Vicuna 1.5 ____ fine-tunes LLaMa 2 to extend context lengths to 16,000 tokens. Similarly, ChatGLM2-32k achieves a 32,000-token context window, demonstrating the scalability of these methods. State-of-the-art models like GPT-4-Turbo (128,000 tokens) and Claude-3.5-Sonnet (200,000 tokens) further push the boundaries of context length, enabling the processing of extensive information. Despite these advancements, there is a notable scarcity of human-aligned benchmarks designed to evaluate performance at such scale.