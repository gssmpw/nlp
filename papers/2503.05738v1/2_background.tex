\section{Background}
\label{sec:background}

\subsection{Flow Matching for protein structure generation}

\paragraph{Flow Matching}
In order to sample from a target distribution $p_1 : \mathcal{M} \rightarrow [0,1]$ on the data domain $\mathcal{M}$,
\citet{lipman2022flow} have proposed flow matching as generalization of diffusion models \cite{song2020score}.
A learned flow $\phi:\mathcal{M}\times[0,1]\rightarrow\mathcal{M}$ is used to transform samples $x_0\sim p_0$ from a simple prior distribution $p_0$ to samples $\phi(x_0,1)$ from the target distribution $p_1$.
The key idea is to learn a time-dependent flow vector field
\begin{equation}
v(x,t) : \mathcal{M}\times[0,1] \rightarrow \mathcal{T}_x\mathcal{M},
\quad
(x,t) \mapsto v(x,t),
\end{equation}
where $\mathcal{T}_x\mathcal{M}$ is the tangent space at point $x$.
%
The flow $\phi_t\equiv\phi(\cdot,t)$ is then defined by $v_t$ via integration of the flow ODE,
\begin{equation}
    \label{eqn:CNF_ODE}
    \frac{\mathrm{d}}{\mathrm{d}t} \phi_t(x) = v(\phi_t(x),t), \quad\phi_0(x)=x \, .
\end{equation}
The vector field $v_t$ can be learned by sampling $x_0\sim p_0$ and $x_1\sim p_1$, connecting them by a particle-wise flow $\psi(x_0,x_1,t)$ and regressing on the time derivative of $\psi$~\cite{lipman2022flow}.
On Riemannian manifolds, $\psi$ is usually chosen as geodesic~\cite{chen2023riemannian}.

\paragraph{Application to protein structure}
% \textcolor{red}{describe the representation, distributions and how to get derivatives of psi from xt and x1 only.}


A protein backbone can be represented as a sequence of Euclidean frames $x = (r, z) \in \mathrm{SE}(3)$ \cite{jumper2021highly}, each of which consists of a rotation $r \in \mathrm{SO}(3)$ and a translation $z \in \mathbb{R}^3$.
A flow matching process for protein structure can thus be formulated on the Riemannian manifold $\mathcal{M} \equiv \mathrm{SE}(3)^N$.
By choosing the metric on $\mathrm{SE}(3)^N$ as in \cite{yim2023frameflow}, the geodesic paths can be split into independent rotation and translation parts for each residue.
Typically, one parametrizes both the ground truth and predicted vector field by a current structure $x_t$ and a final structure $x_1$.
It can be shown \cite{yim2023frameflow,bose2023se} that the vector field components are then given by
\begin{equation}
\label{eq:vf_parametrization}
    v_{\mathrm{SO}(3)}(r_t,t|r_1)
    =
    \frac{\log_{r_t} \left(r_1\right)}{1-t}\,,
    \quad
    v_{\mathbb{R}^3}(z_t,t|z_1)
    =
    \frac{z_1 - z_t}{1-t}\,.
\end{equation}
A common choice for the prior distribution $p_0$ is independent Gaussians for the translations $z_0 \sim \mathcal{N}(0, \sigma^2)$ and uniform distributions for the rotations $r_0 \sim \mathcal{U}(\mathrm{SO}(3))$ \cite{yim2023frameflow}.


\subsection{Folding models for conformational ensembles}

\paragraph{Evolutionary sequence information}
In order to determine the structure of a protein, the challenging task of mapping from a one-dimensional sequence representation to a three-dimensional backbone geometry has to be solved.
\citet{jumper2021highly} propose the Evoformer architecture, which predicts a two-dimensional representation for encoding pairwise relationships of sequence elements such as spatial contact.
To achieve this, the Evoformer relies on Multiple Sequence Alignment (MSA) -- an algorithm that aligns the input sequence with related, naturally occurring protein sequences from a database during inference and training.
Since patterns in naturally occurring sequences are often due to proteins from different organisms being related via specific mutations over the course of evolution, such relationships are referred to as \textit{evolutionary information}.
Evolutionary information can encode properties of the structure a sequence is folded into.
For example, co-evolving pairs of residues indicate proximity of the respective residues in three-dimensional space \cite{morcos2011direct}.
While the calculation of an MSA during inference is computationally expensive, a more efficient strategy is to encode evolutionary information by extracting weights from a protein language model \cite{lin2023evolutionary,rives2021biological}.

\paragraph{FAPE loss}
For training the structure prediction model AlphaFold 2, \citet{jumper2021highly} introduce a distance measure $d$ for all-atom protein structures $x$ and $x'$,
\begin{equation}
d(x,x') = \text{FAPE}(x,x'),
\end{equation}
the Frame Aligned Prediction Error (FAPE).
For calculating FAPE for a protein of size $N$, the structure $x$ is rotated and translated $N$ times, such that each residue is aligned to the respective target residue exactly once.
FAPE is then defined as the mean of the $N$ Root Mean Square Deviations (RMSD) of the structure $x$ to the target structure $x'$.


\paragraph{AlphaFlow}
The folding model AlphaFold 2~\cite{jumper2021highly} predicts the structure of a protein from its sequence using evolutionary sequence information in the form of an MSA and a loss that relies on FAPE.
While its prediction is deterministic, \citep{jing2024alphafoldmeetsflowmatching} have shown that AlphaFold 2 can be fine-tuned as a flow matching model for conformational ensemble generation by training it to predict structures sampled in Molecular Dynamics (MD) simulation.
\citep{jing2024alphafoldmeetsflowmatching} propose to use a harmonic prior, which samples chain-like noisy states, and show that the flow matching vector field can be approximated by applying the squared FAPE loss of AlphaFold 2 to the predicted and target structures.
The resulting model, AlphaFlow, achieves outstanding performance at capturing ensemble properties like flexibility and principal components obtained from MD simulation.
While AlphaFlow is orders of magnitude faster than MD for sampling a set of representative conformations, its efficiency is limited since it relies on the expensive MSA and the large, pre-trained folding model AlphaFold 2.