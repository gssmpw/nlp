\begin{figure*}[!t]
    \centering
    \includegraphics
    [width=0.8\linewidth] {figure/network.pdf}
    \caption{Architecture Overview. The bearing vector (BV) and RGB information from the query image and 3D point cloud are first processed through an encoder to generate high-dimensional features. These features are then used to construct the graph nodes. In the self-attention layer, the angle-annular convolution is employed to extract discriminative geometric information from the neighboring points. After the GNNs, these enhanced features are used to establish initial correspondences via optimal transport. Outlier rejection is then applied to eliminate erroneous correspondences, resulting in a final set of accurate correspondences.}
    \label{fig:network}    
\end{figure*}

\section{Related work}
\label{sec:related work}

\noindent
\textbf{Structure-Based Localization.}
Structure-based visual localization has achieved high accuracy by leveraging a pre-built 3D map of the environment, which is crucial for establishing correspondences between 2D image pixels and 3D point clouds. Classical methods~\cite{sattler2011fast,sattler2012improving} relied on Structure-from-Motion (SfM) models to match features between query images and 3D points. In contrast, recent approaches have incorporated learning-based techniques, significantly enhancing image retrieval~\cite{arandjelovic2016netvlad,ge2020self,gordo2017end,revaud2019learning}, feature extraction~\cite{detone2018superpoint,revaud2019r2d2, dusmanu2019d2,tyszkiewicz2020disk}, and keypoint matching~\cite{arandjelovic2012three,sarlin2020superglue,lindenberger2023lightglue} in the structure-based localization pipeline. 

Image retrieval is a vital component of structure-based localization, as it identifies similar or overlapping images from a large database to narrow down the search space of the 3D points.
NetVLAD~\cite{arandjelovic2016netvlad} integrates a VLAD-like layer in CNNs and optimizes local descriptor extraction and aggregation, enhancing image retrieval accuracy.
Recent works~\cite{keetha2023anyloc,Izquierdo_CVPR_2024_SALAD} leverage powerful pretrained models to further improve retrieval accuracy.
Feature extraction methods~\cite{detone2018superpoint,revaud2019r2d2,dusmanu2019d2} 
extract robust visual representations from raw images, enabling more reliable keypoint detection and description. The integration of neural networks has further enhanced keypoint matching accuracy, particularly with advanced methods like SuperGlue~\cite{sarlin2020superglue}, which employs graph neural networks to refine the matching process. Recent advancements~\cite{shi2022clustergnn,lindenberger2023lightglue,jiang2024omniglue} continue to push the boundaries of accuracy and efficiency in this domain, making structure-based visual localization increasingly robust and scalable for real-world applications. 
In addition, detector-free methods~\cite{sun2021loftr,chen2022aspanformer,wang2024efficient} input images directly for matching and achieve significant improvements in accuracy but encounter challenges in heavy computation.


\noindent
\textbf{Descriptor-free Visual Localization.}
BPnPNet~\cite{campbell2020solving} makes significant progress in 2D pixel to 3D point cloud matching without relying on visual descriptors, introducing an end-to-end trainable matcher for cross-modal point matching. It also introduces the use of bearing vectors as a 2D keypoint representation, effectively bridging the 2D-3D gap and performing well in outlier-free scenarios. GoMatch~\cite{zhou2022geometry} further develops this concept, utilizing bearing vectors to represent both 2D and 3D keypoints for geometric-only matching. It employs SuperGlue-style~\cite{sarlin2020superglue} self- and cross-attention mechanisms to establish initial correspondences and integrates PointCN~\cite{yi2018learning} to remove outliers from the initial matches. DGC-GNN~\cite{wang2024dgc} further improves matching accuracy by leveraging additional RGB information, multiple geometric cues, and a global-to-local feature embedding strategy. It clusters points to create a global geometric graph, enhancing feature matching across clusters by encoding Euclidean and angular relations. However, this approach comes with a trade-off: its inference time is three times longer than that of GoMatch due to the iterative point clustering process.

\noindent
\textbf{Scene Compression.}
Scene compression for visual localization involves two primary approaches: map compression and descriptor compression. Map compression~\cite{camposeco2019hybrid,cao2014minimal,dymczyk2015gist,yang2022scenesqueezer,li2010location,laskar2024differentiable} reduces map size by pruning indistinguishable points,
% , thereby reducing the overall map size. 
while descriptor compression~\cite{jegou2010product,sattler2015hyperpoints,ke2004pca,dong2023learning,yang2022scenesqueezer,laskar2024differentiable} minimizes descriptor size while maintaining their essential properties for accurate matching.  
Scene coordinate regression~\cite{brachmann2021visual,brachmann2023accelerated, li2020hierarchical, wang2024hscnet++,wang2021continual, wang2024glace} uses neural networks to compress scene models and learns a compact representation.
It predicts scene coordinates directly from 2D images without visual descriptors, but faces challenges in generalizing to new scenes.
Hybrid methods~\cite{camposeco2019hybrid,yang2022scenesqueezer,laskar2024differentiable} combine both map and descriptor compression to achieve more compact representations. However, while scene compression techniques can save storage space, they do not fully address privacy concerns or the complexities associated with descriptor maintenance. Our method, which avoids the use of visual descriptors altogether, can also be integrated with scene compression techniques to further reduce the model size.

\noindent
\textbf{Privacy-preserving visual localization.}
As demonstrated in~\cite{pittaluga2019revealing}, image details can be recovered from descriptors. 
This raises significant data privacy concerns, as descriptors can be leaked during transmission across devices.
To address this concern, privacy-preserving visual localization methods~\cite{speciale2019privacy1,speciale2019privacy2,shibuya2020privacy,pan2023privacy,moon2024efficient} have been developed.
These methods represented 2D/3D points as lines or other geometric representations to obscure spatial details. However, they suffer from accuracy degradation, high storage requirements, and maintenance complexity.
% These methods fall into two categories: representing 2D/3D points as lines to obscure spatial details~\cite{speciale2019privacy1,speciale2019privacy2,shibuya2020privacy} and using map sparsification to reduce density and prevent reconstruction attacks~\cite{}. While they mitigate privacy concerns, the core issue lies in the usage of detailed descriptors. Integrating these techniques into our approach can further enhance privacy preservation.
% add citation and concerns 

Other privacy-preserving methods~\cite{do2022learning,pietrantoni2023segloc} take alternative strategies. SLD~\cite{do2022learning} identifies distinctive points as scene landmarks for camera localization.
SegLoc~\cite{pietrantoni2023segloc} utilizes semantic segmentation to create feature representations optimized for privacy and localization performance. Nevertheless, these methods rely on extensive priors or labels and limit generalizability in other scenarios. 
% Those privacy-preserving methods can be used in our method to further enhance privacy 

% comparison with our method