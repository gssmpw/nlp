
\maketitlesupplementary

\setcounter{page}{1}

\section{Additional Details and Results}
\label{sec: supplementaty exp}

\textbf{Data Preparation.}
Following the experimental settings in GoMatch~\cite{zhou2022geometry} and DGC-GNN~\cite{wang2024dgc}, we use the MegaDepth dataset~\cite{li2018megadepth} for training. MegaDepth is a large-scale outdoor dataset comprising 196 scenes from various landscapes around the world. We utilize 99 scenes for training, 16 scenes for validation, and 53 scenes for testing. The ground truth sparse 3D point clouds are reconstructed using COLMAP~\cite{schoenberger2016sfm}. During data preprocessing, a maximum of 500 query images are selected per scene. For each query, we gather its $k$ co-visible views, ensuring at least 35\% visual overlap. Queries lacking sufficient co-visible views are excluded from the training set. Visual overlap is computed as the ratio of co-observed 3D points to the total number of 3D points in the query image. The training set comprises 25,624 queries from 99 scenes, the validation set includes 3,146 queries from 16 scenes, and the test set consists of 12,399 samples from 49 scenes. For the training dataset, we control the number of keypoints per image to range from 100 to 1,024. During inference, this range is adjusted to 10 to 1,024 keypoints per image.

\noindent
\textbf{Representation Ablation Study.}
We present results with different 3D representations, as shown in Table~\ref{tab: representation learning}. The bearing vector as the representation in 3D side plays a crucial role in enhancing the results. The insight behind this improvement is that it integrates the pose of database images into feature learning, bringing one step further towards middle representation from 3D to 2D.





\noindent
\textbf{Generalizability.} 
Our model is trained on the MegaDepth dataset~\cite{li2018megadepth} using the SIFT~\cite{lowe2004distinctive} detector. To demonstrate the generalizability of our model, we conducted evaluations on the 7Scenes dataset using two keypoint detectors: SIFT and SuperPoint~\cite{detone2018superpoint}. The results are presented in Table~\ref{tab:detector}. The similar results in translation and rotation errors between the two detectors further demonstrate the robustness and generalizability of our model.

\noindent
\textbf{Hyperparameters Selection.}
Ablation studies on various hyperparameters in the self-attention layer are presented in Table~\ref{tab:hyparam_abla}. The outlier rejection threshold of $t=0.7$ yields the best results, achieving higher AUC and lower rotation and translation errors. We select $t=0.5$ in the main paper to make a fair comparison with other methods. The choice of the nearest neighbors parameter $k$ has minimal impact on performance. However, when fewer nearest neighbors are processed, it becomes more challenging to accurately capture the local geometric structures.


\begin{table}[t!]

\centering
\renewcommand\arraystretch{1.5}
\Large
\resizebox{0.45\textwidth}{!}{
  \begin{tabular}{cccc}
    \toprule
    \specialrule{0.5pt}{0.5pt}{0.5pt}
    \multirow{2}{*}{3D representation}   & Reproj. AUC (\%) &
  Rotation ($^\circ$)  &
  Translation (m)  \\ 
  & @1 / 5 / 10px ($\uparrow$) &
  \multicolumn{2}{c} {Quantile @25 / 50 / 75\% $(\downarrow)$} \\
    \midrule
    Coordinate & 7.69 / 27.96 / 32.82 & 0.28 / 12.6 /  59.64 & 0.02 / 1.32 / 5.34 \\
    Bearing vector & \textbf{12.72 / 41.84 / 48.02} &
  \textbf{0.12} / \phantom{1}\textbf{0.79} / \textbf{26.37} &
  \textbf{0.01 / 0.08 / 2.80} \\
    \bottomrule
    \specialrule{0.5pt}{0.5pt}{0.5pt}
\end{tabular}
}
\caption{Ablation results with different 3D representations on MegaDepth on top-1 image retrieval. 
} 
\label{tab: representation learning}
\end{table}

\begin{table}[!t]
    \centering
    \begin{tabular}{lcc}
        \specialrule{0.5pt}{0.5pt}{0.5pt}
        \hline
         
         7Scenes~\cite{shotton2013scene} & SIFT~\cite{lowe2004distinctive} & SuperPoint~\cite{detone2018superpoint} \\
         \specialrule{0.5pt}{0.5pt}{0.5pt}
         \hline
         Chess      & 3 /\phantom{1}1.37 & 3 /\phantom{1}1.41 \\
         Fire       & 5 /\phantom{1}1.78 & 6 /\phantom{1}1.99\\
         Heads      & 4 /\phantom{1}2.70 & 2 /\phantom{1}3.12\\
         Office     & 6 /\phantom{1}1.56 & 6 /\phantom{1}1.48\\
         Pumpkin    & 7 /\phantom{1}1.86 & 9 /\phantom{1}2.28\\
         Redkitchen & 7 /\phantom{1}2.00 & 8 /\phantom{1}2.08\\
         Stairs     & 72 /17.05 & 66 / 16.02\\
         \specialrule{0.5pt}{0.5pt}{0.5pt}
         \hline

    \end{tabular}
    \caption{Comparison on sift and superpoint as detector on 7Scenes dataset. Median translation and  rotation errors ($cm,^\circ$) are reported.}
    \label{tab:detector}
\end{table}

% \noindent
% \textbf{Visualization.}
% Figure~\ref{fig:viz} presents examples of 2D-3D matching results on the MegaDepth dataset. Compared to DGC-GNN~\cite{wang2024dgc}, our A2-GNN demonstrates superior performance by achieving a larger number of inliers. Consequently, our A2-GNN results in lower camera pose errors. These visualizations underscore the effectiveness of our model.

\noindent
\textbf{Timing and Model Size.}
The inference time per query image for A2-GNN is $\sim$34 ms, comprising four main components: the feature encoding ($\sim$2 ms), the attention layers ($\sim$14 ms), optimal transport ($\sim$9 ms), and the outlier rejection process ($\sim$6 ms). Our model contains 2.7 million parameters, with a total size of $\sim$10.6 MB. All experiments were conducted on a 32GB NVIDIA Tesla V100 GPU, using a maximum of 1,024 keypoints.

% \noindent
% \textbf{Acknowledgement.} This work was supported by the Academy of Finland (grants No). 
% We acknowledge the computational resources provided by the CSC-IT Center for Science, Finland.

\begin{table*}[h]
\begin{center}
\renewcommand\arraystretch{1.3}
\setlength{\tabcolsep}{3pt}
\resizebox{.99\textwidth}{!}
{
\begin{tabular}{lcccccc}
\hline
\specialrule{1.5pt}{0.5pt}{0.5pt}
\multirow{2}{*}{Methods}     & \multirow{2}{*}{Neighbors} & \multirow{2}{*}{Groups} & \multirow{2}{*}{OR Threshold} & Reproj. AUC (\%)  & Rotation ($^\circ$)  & Translation                 \\ & &  & & @1 / 5 / 10px  ($\uparrow$)    & \multicolumn{2}{c}{Quantile@25 / 50 / 75\% ($\downarrow$)} \\ \specialrule{1.5pt}{0.5pt}{0.5pt}
A2-GNN                      & 9      & 3                         & 0.5                           & 17.29 / 54.41 / 62.24          & \textbf{0.06} / 0.19 / 4.6 \phantom{1} & \textbf{0.01} / {0.02} / 0.48    \\ \hline
\multirow{5}{*}{HyperParam.} & 9 & 3 & 0.7                           & \textbf{18.59 / 58.84 / 66.48}        & \textbf{0.06 / 0.16 / 2.16}  & \textbf{0.01 / 0.01 / 0.22} \\    &
9 & 3  & 0.3  & 14.82 / 48.57 / 56.7 \phantom{1} &0.07 / 0.34 / 8.42& \textbf{0.01} / 0.03 / 0.9  \phantom{1}        \\ \cline{2-7} 
& 12  & 3       & 0.5        &   17.21 / 54.36 / 62.18            &  \textbf{0.06} / 0.2 \phantom{1} / 4.45  &  \textbf{0.01} / 0.02  / 0.46\\
  & 9    & no groups & 0.5     &  15.35 / 49.81 / 57.64  &   0.08 / 0.28 / 5.48    &   \textbf{0.01} / 0.03 / 0.58 \\
  & 6 & 3 & 0.5 & 16.41/ 51.84 / 59.52 & 0.06 / 0.22 / 7.49 & \textbf{0.01} / 0.02 / 0.81
        \\ \specialrule{1.5pt}{0.5pt}{0.5pt}
\end{tabular}
}
\vspace{10pt}
\caption{Ablations on Hyperparameters. 
The results are evaluated on MegaDepth with top-$10$ retrieval images. 
The best results are bold.
}
\label{tab:hyparam_abla}
\end{center}
\end{table*}

% \begin{figure*}[!t]
%   \centering
%   \includegraphics[width=0.8\linewidth]{figure/0064.pdf}
%   \includegraphics[width=0.99\linewidth]{figure/0090.pdf}
%   \caption{Matching Visualization. We visualize the number of inlier matches from A2-GNN and DGC-GNN on MegaDepth with top-$10$ retrieval images.
%   }
%   \label{fig:viz}
% \end{figure*}

