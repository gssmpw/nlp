
\section{Method}
\label{sec:method}

\subsection{Problem setting}


Let $p \in \reals^{2} $ denote a 2D point and $q \in \reals^{3} $ denote a 3D point.
Assume that a 2D image $P$ contains $M$ keypoints, and a 3D point cloud $Q$ contains $N$ keypoints.
We denote the sets of keypoints in 2D and 3D cases as $ \ccalP = \{p_i \mid i=1,\ldots,M\} $ and $ \ccalQ = \{ q_j \mid j = 1, \ldots, N\} $, respectively.
Our task is to predict a set of correspondences $\ccalM_{p,q} 
$ between 2D keypoints and 3D point clouds.







\noindent
\textbf{Keypoint Representation.}
Following GoMatch \cite{zhou2022geometry}, we utilize bearing vectors as keypoint representation, as it can bring 2D keypoints and 3D point clouds into the same modality.
For 2D keypoints, bearing vectors $\bbb_\bbp$ remove the effect of camera intrinsic $\bbK$
by the following equation:
\begin{equation}
    [\bbb_\bbp^\top, 1]^\top = \mathbf{K}^{-1}[u, v, 1]^\top ,
\end{equation} 
where $(u, v)$ represents 2D keypoint pixel coordinates and $\bbb_\bbp \in \reals^2$.  
The bearing vector brings pixel coordinates into the corresponding camera ray by connecting the camera center and pixel coordinates.
For 3D points, 
the bearing vector $\bbb_\bbq$ transforms its world coordinates $\bbp^\bbw$ to the camera ray by the following equation:
\begin{align}
    \bbp^\bbc & =  \mathbf{R}  \bbp^\bbw + \mathbf{t} \\
    [ \bbb_\bbq^\top, 1]^\top & =  \bbp^\bbc / {p^c_z},
\end{align}
where $\bbp^\bbc \in \reals^3 $ is its the camera coordinate, $p'_z$ is its $z$ coordinate and $\bbb^\bbq \in \reals^2$. 
The bearing vector first transforms 3D points from the world coordinates to the camera coordinates. 
Similar to the image plane, we take plane $z=1$ and connect the camera center to its camera coordinates.
The camera ray is obtained by connecting the camera center and the point where the line intersects the plane.


\subsection{Network Architecture}
We give an overview of the proposed Angle-Annular convolution Graph Neural Network, shortened as A2-GNN in Fig.~\ref{fig:network}. 
The architecture includes the following modules: feature encoder,  angle-annular geometric feature extraction, optimal transport, and outlier rejection.
The encoder transforms positional information and RGB color from low-dimensional inputs into high-dimensional features.
These features are then processed by A2-GNN to extract local geometric relations from neighboring points. Initial correspondences are established using optimal transport, and low-confidence matches are subsequently removed by the outlier rejection module.

\subsubsection{Feature Encoder}
\textbf{Feature Encoder.} We use a ResNet-style encoder~\cite{he2016deep,campbell2020solving,zhou2022geometry} to extract both position and color features directly from 2D kepoints or 3D point clouds, denoted as $\mathcal{F}_{b}$ and $\mathcal{F}_{c}$. The feature encoder encodes the bearing vector and RGB color separately, from low dimension vectors to high-dimensional (e.g., $d$=128) vectors. The point features $\bbf_p \in \reals^{N\times d}$ from query images and $\bbf_q \in \reals^{M\times d}$ from point clouds can be computed as:
\begin{subequations}
    \begin{align}
    \bbf_\bbp & = \mathcal{F}_{b}(\bbb_\bbp) + \mathcal{F}_{c} ( \bbc_\bbp) \\
    \bbf_\bbq & = \mathcal{F}_{b}(\bbb_\bbq) + \mathcal{F}_{c} (\bbc_\bbq),
\end{align}
\end{subequations}
where $\bbc_\bbp \in \reals ^ 3, \bbc_\bbq \in \reals ^ 3$ are RGB color of points from the query images and point clouds, respectively.

\subsubsection{A2-GNN} 

Graph Neural Networks enhance feature representation by aggregating information from neighboring nodes, which is utilized for both images and point clouds.
This section introduces the Angle-Annular Convolution Aggregation method, self-attention and cross-attention mechanisms.

\noindent
\textbf{Geometric Local Feature.}
After feature extraction, we construct local graphs for each keypoint based on its neighboring points. We consider the local graphs as a self-attention mechanism to extract local geometric features, thereby enhancing the representation and capturing the contextual relationships among the keypoints. Following previous works~\cite{zhou2022geometry,wang2024dgc}, edges $\mathcal{E}$ are constructed in Euclidean space by connecting each node to its $k$ nearest neighbors. The edge feature $\bbe_{ij} \in \reals ^{k \times 2d}$ between a node $\bbf_i$ and its neighbors $\bbf_{ij}$ is defined as:
\begin{equation}
    \bbe_{ij} = \text{cat}[ \bbf_i, \bbf_i - \bbf_{ij}], j = 1,2, \ldots k,
\end{equation}
where $\text{cat}[\cdot,\cdot]$ denotes concatenation. 
Then maxpooling operation is used to extract local information from neighbour nodes.
The feature $\bbf_{max} \in \reals^{1 \times d}$ can be updated using following equation:
\begin{equation}
    ^{(t+1)}\bbf_{max} = \max_{(i,j)\in \mathcal{E}} h_{\theta}( ^{(t)}\bbe_{ij}),
\end{equation}
where $h_{\theta}$ represents a linear layer followed by instance normalization~\cite{ulyanov2016instance} and LeakyReLU, and $\text{max}(\cdot)$ is element-/channel-wise maxpooling.


However, the maxpooling operation disregards the inherent structural relationships between nodes in the graph, as each graph node only retains the maximum value from its neighbors. We observe that if a pair of points are correct correspondences between the keypoints from the image and the point cloud, they should exhibit similar structures or geometric patterns within their respective neighborhoods. 
In order to dig more inherent structural geometry information between the node and its neighbors, we introduce \textbf{Angle-Annular convolution}.
First, we use annular convolution inspired by CLNet~\cite{zhao2021progressive} to capture the relationships of neighbors in a grouped manner. 
As shown in Fig.~\ref{fig:network} bottom, $k$ neighboring nodes are determined by ranking the Euclidean distances. Those $k$ nodes are then divided into $g$ groups where each group contains $\frac{k}{g}$ nodes.
We use convolution layers followed by one Batch Normalization layer~\cite{ioffe2015batch} with ReLU to process the grouped graphs. 
This convolution utilizes distance information indirectly by dividing nodes based on the distance between node and its neighbors.
The annular feature $\bbf_{ann} \in \reals^{1 \times d}$ can be formulated as:
\begin{equation}
    ^{(t+1)} \bbf_{ann}= g_2(g_1(^{(t)}\bbe_{ij})),
\end{equation}
where $g_1(\cdot)$ and $g_2(\cdot)$ denotes the convolution layers with $1 \times \frac{k}{g}$ kernels and $1\times g$ kernels.
 
\begin{figure}[h]
    \centering
    \includegraphics
    [width=0.8\linewidth] {figure/ang_cos.pdf}
    \caption{Illustration of angle embedding. The angle embedding between node and its neighbor is added to enhance feature representation.}
    \label{fig:angle}    
\end{figure}


Annular convolution efficiently captures structural information, but its effectiveness is limited by the variability in spatial distances between nodes and their neighbors, with some neighbors being adjacent and others more distant. Inspired by~\cite{qin2022geometric,wang2024dgc}, we incorporate geometric encoding that includes both distance and angle embeddings. The distance embedding is implicitly applied in constructing node neighborhoods. As illustrated in Fig \ref{fig:angle}, angle embedding between a node and its neighbors can also be incorporated. The cosine of the angle $\theta$, where $\theta \in (0, \pi)$, is used as the angle embedding.
The cosine value can be computed as:
\begin{equation}
    \text{cos} (\theta )= \frac{\overrightarrow{a} \cdot \overrightarrow{b}}{\| \overrightarrow{a} \| \| \overrightarrow{b} \|},
\end{equation}
where $\text{cos} (\cdot)$ is cosine function. After getting each angle embedding within its neighbors, we get angle edge $\mathcal{E}_{ang} \in \reals ^{k \times 1}$.
Here, we use annular convolution again to get angle feature $\bbf_{ang} \in \reals ^ {1 \times d}$:
\begin{equation}
    ^{(t+1)} \bbf_{ang} = g_4(g_3(\text{cos}(^{(t)} \mathcal{E}_{ang}))),
\end{equation}
where $g_3(\cdot)$ and $g_4(\cdot)$ are similar convolutions as $g_1(\cdot)$ and $g_2(\cdot)$ but with independent parameters.
After getting the angle feature, the annular-angle features are computed by merging the angle feature and angular feature as follows: $\bbf_{aa} = \bbf_{ann} + \bbf_{ang}$. The final feature $\bbf_{self}$ can be updated twice
and the final self-attention feature $\bbf_{self}$ are obtained as: 
\begin{align}
    \bbf_{self} 
    & = h_{\theta 1}(\text{cat}[^{(0)}\bbf,^{(1)}\bbf_{max},^{(2)}\bbf_{max}]) \notag \\
    & + h_{\theta 2}(\text{cat}[^{(0)} \bbf,^{(1)}\bbf_{aa},^{(2)}\bbf_{aa}],
\end{align}
where $h_{\theta 1}$ and $h_{\theta 2}$ denote a linear layer followed by instance normalization~\cite{ulyanov2016instance} and LeakyReLU.

\noindent
\textbf{Cross-attention layer.} 
Following~\cite{zhou2022geometry}, we apply cross-attention to enhance feature representation. Each graph node from the 2D is connected with every node from the point cloud. Specifically, given a node feature $\bbf_i$ from one modality and features $\bbg_j$ from the other modality, we form the query, keys and values according to 
$\boldsymbol{\mathsf{q}}_i = \bbW_\mathsf{q} \bbf_i, \bbk_j = \bbW_k \bbg_j$ and $\bbv_j = \bbW_v \bbg_j$, where $\bbW_s,\bbW_k,\bbW_v \in \reals^{d \times d}$ are learned parameters. We update feature $\bbf_i$ by following formulation:
\begin{align}
    \bbm_i & = \sum_j \alpha_{ij} \bbv_j \\
    ^{(t+1)} \bbf_{i} & = ^{(t)} \bbf_i + \text{MLP}(\text{cat}[\boldsymbol{\mathsf{q}}_i,\bbm_i]),
\end{align}
where attention weights $\alpha = \text{softmax}(\boldsymbol{\mathsf{q}}_j^\top \bbk_j / \sqrt{d})$.

\subsubsection{Optimal Transport}
After obtaining the 2D and 3D enhanced features $f^G_p$ and $f^G_q$ from the geometric embedding, optimal transport is employed to assign initial correspondences, denoted as $\ccalM_{init}$. We first compute the cost matrix $M \in \reals^{M \times N}$ using the $\text{L}2$ distance 
between those two feature sets. To handle unmatched points, we extend $M$ to $M' \in \reals^{(M+1) \times (N+1)}$ by adding an additional row and column as dustbins. The differentiable Sinkhorn algorithm~\cite{cuturi2013sinkhorn,sinkhorn1967concerning} is then applied to solve the optimal transport problem. Finally, the initial correspondences $\ccalM_{init}$ are obtained by removing the dustbins from $M'$ and performing a mutual nearest neighbor check.

\subsubsection{Outlier Rejection}
After the optimal transport layer, the initial correspondences $\ccalM_{init}$ still contain outliers. GoMatch~\cite{zhou2022geometry} and DGC-GNN~\cite{wang2024dgc} utilize an outlier classifier~\cite{yi2018learning} to predict scores indicating the probability of a correspondence being an inlier. The final correspondences $\ccalM_{final}$ are obtained if the probability is over the threshold $t$ (0.5 in experiments).
The method directly inputs enhanced geometric features into the classifier. However, it brings limitations as the enhanced features are primarily designed for the optimal transport layer and are mixed together with the RGB embedding, making it difficult to extract the epipolar geometry constraints necessary for accurate outlier rejection. Earlier learning-based outlier rejection methods~\cite{yi2018learning,zhang2019learning,zhao2021progressive} leveraged keypoint locations as input, which naturally encode epipolar geometry constraints. To address this issue, we propose using bearing vectors as input for the outlier rejection process, allowing the model to utilize geometric information better and improve the accuracy of inlier detection. 

\subsection{Training Loss}
Following GoMatch~\cite{zhou2022geometry} and DGC-GNN~\cite{wang2024dgc}, the loss function $\mathcal{L}$ consists of a matching loss $\mathcal{L}_{m}$ and an outlier rejection loss $\mathcal{L}_{or}$.
The matching loss $\mathcal{L}_{m}$ is designed to minimize the negative log-likelihood of the matching scores. 
The matching loss is presented as:
\begin{align}
    \mathcal{L}_{m} &= - \frac{1}{N_m} (\sum_{(i, j) \in \mathcal{M}_{gt}} \log \tilde{\text{P}}_{ij} +  \sum_{i \in \mathcal{U}_{q}} \log \tilde{\text{P}}_{i(N + 1)} \notag \\
    &+ \sum_{j \in \mathcal{U}_{d}} \log \tilde{\text{P}}_{(M + 1)j} ), 
\end{align}
where $\tilde{\text{P}}$ means matching score, 
$\mathcal{M}_{gt}$ denotes ground truth matches, $\mathcal{U}_{q}$ represents unmatched query keypoints and $\mathcal{U}_{d}$ is unmatched database 3D points. $N_m$ refers to the total keypoints number of ground truth, unmatched query, and unmatched database. 

The outlier rejection loss $\mathcal{L}_{or}$ serves to remove incorrect matches. 
It enhances the model's robustness against outliers and is defined as:
\begin{equation}
    \mathcal{L}_{or} = - \frac{1}{N_c}\sum_{i=1}^{N_c} w_i \left(y_i \log p_i + (1 - y_i) \log (1 - p_i) \right),
\end{equation}
where \(N_c\) means the total number of initial correspondences.
The classifier's probability output for each correspondence is indicated by $p_i$.
The target label for the correspondence is represented by $y_i$, and $w_i$ refers to the balance weight for negative and positive samples.


