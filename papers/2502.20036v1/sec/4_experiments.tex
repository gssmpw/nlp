\section{Experiments}
\label{sec:experiments}

\begin{table*}[!t]
\begin{center}
\renewcommand\arraystretch{1.4}
\resizebox{.99\textwidth}{!}
{
\tiny
\begin{tabular}{llcccccc}
\hline \specialrule{0.5pt}{0.5pt}{0.5pt}
\multicolumn{2}{l}{\multirow{2}{*}{Methods}} &
  Reproj. AUC (\%) &
  Rotation ($^\circ$)  &
  Translation &
  \multirow{2}{*}{Time (ms) $(\downarrow)$} \\ 

\multicolumn{2}{l}{} &
  @1 / 5 / 10px ($\uparrow$) &
  \multicolumn{2}{c}{Quantile @25 / 50 / 75\% $(\downarrow)$} & 
   \\ \hline \specialrule{0.5pt}{0.5pt}{0.5pt}
\multirow{4}{*}{k=1} &
  Oracle & 34.59 / 85.02 / 92.02 &\phantom{1} 0.04 /\phantom{1} 0.06 /\phantom{1} 0.12& 0.00 / 0.01 / 0.01 &- \\
&
  GoMatch~\cite{zhou2022geometry} &
  \phantom{1} 5.67 / 22.43 / 28.01 &
  \phantom{1} 0.60 / 10.08 / 34.63 & 
  0.06 / 1.06 / 3.73 & \textbf{24.4} \\  
 &
  DGC-GNN~\cite{wang2024dgc} &
  {10.20 / 37.64 / 44.04} &\phantom{1} {0.15} /\phantom{1} {1.53} / {27.93} &
  {\textbf{0.01} / 0.15 / 3.00} & 77.8 \\ \cline{3-6}
 &
  A2-GNN & \textbf{12.72 / 41.84 / 48.02} &
  \phantom{1} \textbf{0.12} /\phantom{1} \textbf{0.79} / \textbf{26.37} &
  \textbf{0.01 / 0.08 / 2.80} & {34.0} \\\cline{1-6}
\multirow{3}{*}{k=10} &
  GoMatch~\cite{zhou2022geometry} &
  \phantom{1}8.90 / 35.67 / 44.99 &
  \phantom{1}0.18 / \phantom{1}1.29 / 16.65 &
  0.02 / 0.12 / 1.92  & \textbf{263.1} \\ 
 &
 DGC-GNN~\cite{wang2024dgc} &
  {15.30 /  51.70 /  60.01} & \phantom{1}{0.07} /\phantom{1} {0.26} /\phantom{1} {5.41} &
   \textbf{0.01 / 0.02} / 0.57 & 701.9 \\ \cline{3-6} 
 &
  \textbf{A2-GNN} &
  \textbf{17.29 / 54.41 / 62.24} &\textbf{0.06} /\phantom{1} \textbf{0.19} /\phantom{1} \textbf{4.6} &
  \textbf{0.01 / 0.02 / 0.48} & 372.0 \\ \hline \specialrule{0.5pt}{0.5pt}{0.5pt}
\end{tabular}}
\caption{Matching results on MegaDepth~\cite{li2018megadepth}. The results include the reprojection AUC, rotation and translation errors, and inference time.
Parameter $k$ is the number of retrieved images. 
The best results are bold. 
} 
\label{tab: matching}
\end{center}
\end{table*}

\subsection{Implementation Details}

\textbf{Training.} 
We train our model on the MegaDepth dataset~\cite{li2018megadepth}. The number of nearest neighbors to build the local graph is set to $k = 9$. For annular convolutions, we use $g = 3$, meaning each group contains 3 nodes. 
The model is trained using the ADAM optimizer~\cite{kingma2014adam} with a learning rate $lr = 0.001$. Training is conducted on a single 32GB Tesla V100 GPU with a batch size $b = 16$. The entire training process takes around 22 hours to complete 50 epochs.

\noindent
\textbf{Datasets.} 
Following the setting in GoMatch~\cite{zhou2022geometry} and DGC-GNN~\cite{wang2024dgc}, we train and evaluate our model on the MegaDepth dataset and conduct visual localization evaluations on Cambridge Landmark \cite{kendall2015posenet} and 7Scenes~\cite{shotton2013scene} datasets. MegaDepth is a large-scale outdoor dataset with 196 scenes captured around the world.
We train our outdoor model on 99 scenes, validate on 16 scenes and test on 53 scenes.
The ground truth sparse 3D point clouds are reconstructed by the COLMAP~\cite{schoenberger2016sfm}. 
Cambridge Landmarks is a middle-scale outdoor dataset and
7Scenes is a small indoor dataset. 
The 3D reconstruction of Cambridge Landmarks is obtained by SfM, we evaluate the localization accuracy on four out of six scenes.
The 7Scenes dataset, captured with an RGB-D camera, provides sequences where the ground truth camera poses are determined using a SLAM system, we evaluate our model on all 7Scenes as in~\cite{zhou2022geometry, wang2024dgc}.

\noindent
\textbf{Ground Truth Correspondences.} 
During training, we obtain the ground truth correspondences by reprojecting the 3D point clouds from the top-$k$ retrieved images onto the query image plane. A correspondence is considered ground truth if the reprojection error is less than 0.001 in normalized image coordinates.

\noindent
\textbf{Evaluation Metrics.} 
Similar to~\cite{zhou2022geometry,wang2024dgc}, we report the AUC score for mean reprojection error at 1 / 5 / 10 pixel and translation and rotation errors quantiles at 25 / 50 / 75\% as evaluation metrics on MegaDepth dataset.
For Cambridge and 7Scenes, we report the commonly used median translation and rotation errors per scene.
The camera pose estimation is determined by using the PnP-RANSAC~\cite{fischler1981random,gao2003complete} for the final established correspondences.
Oracle results are obtained by utilizing ground truth matches as predictions.



\begin{table*}[t!]
\begin{center}
\renewcommand\arraystretch{1.6}
\setlength{\tabcolsep}{3pt}
\resizebox{0.99\textwidth}{!}
{
\Huge

\begin{tabular}{llccccccc|ccccccccc}
\hline \specialrule{2.5pt}{0.5pt}{0.5pt}
\multicolumn{2}{l}{\multirow{2}{*}{{Methods}}} &
  \multicolumn{1}{c}{\multirow{2}{*}{{\begin{tabular}[c]{@{}c@{}}No Desc. \\ Maint.\end{tabular}}}} &
  \multicolumn{1}{c}{\multirow{2}{*}{{Privacy}}} &
  \multicolumn{4}{c}{{Cambridge-Landmarks~\cite{kendall2015posenet} (cm, $^\circ$)}} &
  \multirow{2}{*}{{MB used}} &
  {} &
  \multicolumn{7}{c}{{7Scenes~\cite{shotton2013scene} (cm, $^\circ$)}} &
  \multirow{2}{*}{{MB used}} \\ \cline{5-8} \cline{11-17}
\multicolumn{2}{l}{} &
  \multicolumn{1}{c}{} &
  \multicolumn{1}{c}{} &
  {King’s} &
  {Hospital} &
  {Shop} &
  {St. Mary’s} &
   &
  {} &
  {Chess} &
  {Fire} &
  {Heads} &
  {Office} &
  {Pumpkin} &
  {Kitchen} &
  {Stairs} &
   \\ \hline \specialrule{2.5pt}{0.5pt}{0.5pt}
 \multirow{3}{*}{\rotatebox{90}{{E2E}}} &
  {MS-Trans.~\cite{shavit2021learning}} &  \Checkmark
   &   \Checkmark
   &
  83 / 1.47 &
  181 / 2.39 &
  86 / 3.07 &
  162 / 3.99 &
  \phantom{11}71 &
   &
  11 / 4.66 &
  24 / 9.60 &
  14 / 12.19 &
  17 / 5.66 &
  18 / 4.44 &
  17 / 5.94 &
  26 / 8.45 &
  \phantom{111}71 \\
 &
  {DSAC*~\cite{brachmann2021visual}} &  \Checkmark
   &   \Checkmark
   &
  \textbf{15 / 0.30} &
  \phantom{1}21 / 0.40 &
  \textbf{\phantom{1}5 / 0.30} &
  \phantom{1}13 / 0.40 &
  \phantom{1}112 &
   &
  \phantom{1}\textbf{2} / 1.10 &
  \phantom{1}\textbf{2} / 1.24 &
  \phantom{1}\textbf{1} / 1.82 &
  \phantom{1}\textbf{3} / 1.15 &
  \phantom{1}\textbf{4} / 1.34 &
  \phantom{1}\textbf{4} / 1.68 &
  \phantom{1}\textbf{3} / 1.16 &
  \phantom{11}196 \\
 &
  {HSCNet~\cite{li2020hierarchical}} & \Checkmark
   &   \Checkmark
   &
  18 / \textbf{0.30} &
  \phantom{1}\textbf{19 / 0.30} &
  \phantom{1}6 / \textbf{0.30} &
  \phantom{11}\textbf{9 / 0.30} &
  \phantom{1}592 &
   &
  \phantom{1}\textbf{2 / 0.70} &
  \phantom{1}\textbf{2 / 0.90} &
  \phantom{1}\textbf{1 / 0.90} &
  \phantom{1}\textbf{3 / 0.80} &
  \phantom{1}\textbf{4 / 1.00} &
  \phantom{1}\textbf{4 / 1.20} &
  \phantom{1}\textbf{3 / 0.80} &
  \phantom{1}1036 \\
 \hline
 \multirow{3}{*}{\rotatebox{90}{{DB}}} & 
 {HybridSC~\cite{camposeco2019hybrid}} &  \XSolidBrush
   & --
   &
  81 / 0.59 &
  \phantom{1}75 / 1.01 &
  19 / 0.54 &
  \phantom{1}50 / 0.49 &
  \phantom{111}3 &
  \multicolumn{1}{c}{} &
  \multicolumn{1}{c}{-} &
  \multicolumn{1}{c}{-} &
  \multicolumn{1}{c}{-} &
  \multicolumn{1}{c}{-} &
  \multicolumn{1}{c}{-} &
  \multicolumn{1}{c}{-} &
  \multicolumn{1}{c}{-} &
  - \\ &
  {AS~\cite{sattler2016efficient}}&  \XSolidBrush
  &  \XSolidBrush
   &
  13 / 0.22 &
  \phantom{1}20 / 0.36 &
  \phantom{1}\textbf{4} / 0.21 &
  \phantom{11}8 / 0.25 &
  \phantom{1}813 &
   &
  \phantom{1}3 / 0.87 &
  \phantom{1}\textbf{2} / 1.01 &
  \phantom{1}\textbf{1} / 0.82 &
  \phantom{1}4 / 1.15 &
  \phantom{1}7 / 1.69 &
  \phantom{1}5 / 1.72 &
  \textbf{4} / \textbf{1.01} &
  - \\
 &
  {SP~\cite{detone2018superpoint}+SG~\cite{sarlin2020superglue}} &  \XSolidBrush
    &  \XSolidBrush
   &
  \textbf{12} / 0.20 &
  \phantom{1}15 / 0.30 &
  \phantom{1}\textbf{4 / 0.20} &
  \phantom{11}\textbf{7 / 0.21} &
  3215 &
   &
  \phantom{1}\textbf{2} / 0.85 &
  \phantom{1}\textbf{2} / 0.94 &
  \phantom{1}\textbf{1 / 0.75} &
  \phantom{1}\textbf{3} / 0.92 &
  \phantom{1}5 / 1.30 &
  \phantom{1}\textbf{4 }/ 1.40 &
  5 / 1.47 &
  22977 \\ \hline
 \multirow{3}{*}{\rotatebox{90}{{DF}}} &
  {GoMatch~\cite{zhou2022geometry}} &  \Checkmark
   & \Checkmark
   &
  25 / 0.64 &
  283 / 8.14 &
  48 / 4.77 &
  335 / 9.94 &
  \phantom{11}48 &
   &
  \phantom{1}4 / 1.65 &
  13 / 3.86 &
  \phantom{1}9 / 5.17 &
  11 / 2.48 &
  16 / 3.32 &
  13 / 2.84 &
  \ 89 / 21.12 &
  \phantom{11}302 \\
 &
  {DGC-GNN~\cite{wang2024dgc}} & \Checkmark
   &  \Checkmark
   &
  {18 / 0.47} &
  {75 / 2.83} &
  {15 / 1.57} &
  {106 / 4.03} &
  \phantom{11}69 &
   &
  \phantom{1} {\textbf{3} / 1.41} &
  \phantom{1} {\textbf{5} / 1.81} &
  \phantom{1} {\textbf{4} / 3.13} &
  \phantom{1} {7 / 1.66} &
  \phantom{1} {8 / 2.03} &
  \phantom{1} {8 / 2.14} &
  \ {83 / 21.53} &
  \phantom{11}355 \\
& 
\textbf{A2-GNN} & \Checkmark
   &  \Checkmark
   &
  \textbf{15 / 0.39} &
  \textbf{59 / 1.74} &
  \textbf{12 / 1.16} &
  \textbf{\phantom{1}76 / 2.65} &
  \phantom{11}69 &
   &
  \phantom{1}\textbf{3 / 1.37} &
  \phantom{1}\textbf{5 / 1.78} &
  \phantom{1}\textbf{4 / 2.70} &
  \phantom{1}\textbf{6 / 1.56} &
  \phantom{1}\textbf{7 / 1.86} &
  \phantom{1}\textbf{7 / 2.00} &
 \ \textbf{72 / 17.05} &
  \phantom{11}355
 \\ \hline \specialrule{2.5pt}{0.5pt}{0.5pt}
\end{tabular}
}
\caption{The comparison to existing localization baselines. E2E, DB and DF indicate end-to-end methods, descriptor-based and descriptor-free methods, respectively. 
Median translation and rotation errors for each scene are reported, as well as storage demand. The best results in each group are bold.}
\label{tab: localization}
\end{center}
\end{table*}

\subsection{Results}

\textbf{Matching Results.}
Table~\ref{tab: matching} illustrates the matching results on MegaDepth dataset using top-$1$ and top-$10$ retrieved images.
The proposed A2-GNN outperforms both GoMatch and DGC-GNN. With $k = 1$ retrieved image, A2-GNN shows significant improvements in reprojection AUC, outperforming GoMatch by 7.05 / 19.41 / 20.01\% and DGC-GNN by 2.52 / 4.2 / 3.98\%. In terms of running time, A2-GNN is slightly slower than GoMatch due to the additional encoding of RGB and geometric information, but it is x2.2 times faster than DGC-GNN by avoiding multiple times point clustering operations. When using $k = 10$ retrieved images, the same conclusion holds: A2-GNN consistently outperforms the other methods, demonstrating its robust learning capabilities.

\begin{table*}[t!]
\begin{center}

\renewcommand\arraystretch{1.4}
\setlength{\tabcolsep}{3pt}
\resizebox{0.98\textwidth}{!}
{
\begin{tabular}{lcccccccc}
\hline
\specialrule{0.5pt}{0.5pt}{0.5pt}
\multirow{2}{*}{Methods} &

\multirow{2}{*}{OR Input} &
\multicolumn{3}{c}{Self-Attention} &
\multirow{2}{*}{Color} &

  Reproj. AUC (\%) &
  Rotation ($^\circ$) &
  Translation \\
&   & Maxpooling  &  Annular & Angle & & @1 / 5 / 10px  ($\uparrow$)           & \multicolumn{2}{c}{Quantile@25 / 50 / 75\% ($\downarrow$)}        \\ \hline 
\specialrule{0.5pt}{0.5pt}{0.5pt}
GoMatch~\cite{zhou2022geometry}       & Feat. &  \Checkmark & &  &    & \phantom{1}8.90 / 35.67 / 44.99 & 0.18 / 1.29 / 16.65 & 0.02 / 0.12 / 1.92 \\ \hline 

\multirow{5}{*}{Variants}  & BV &   \Checkmark & & &   & 10.57 / 39.22 / 47.98           & 0.13 / 0.96 / 17.77           & 0.01 / 0.09 / 1.96      \\
& BV & & \Checkmark &  &    &  12.02 / 42.42 / 50.67           &  0.10 / 0.67 / 15.99         &  0.01 / 0.06 / 1.76      \\
& BV & \Checkmark & \Checkmark &  &  &  
                                        
14.04 / 47.49 / 55.82 & 0.08 / 0.40 / 11.40       & 0.01 / 0.04 / 1.20 \\        
& BV & \Checkmark & \Checkmark & \Checkmark &  &  14.79 / 49.11 / 57.41           & 0.08 / 0.33 / \phantom{1}9.14         &  0.01 / 0.03 / 1.06      \\
& Feat. &  \Checkmark & \Checkmark & \Checkmark & \Checkmark &
15.97 / 52.01 / 59.88 & 0.06 / 0.23 / \phantom{1}7.34 & 0.01 / 0.02 / 0.79
\\

\hline
\textbf{A2-GNN}     & BV & \Checkmark & \Checkmark & \Checkmark & \Checkmark & \textbf{17.29 / 54.41 / 62.24} & \textbf{0.06 / 0.19 / \phantom{1}4.60} & \textbf{0.01 / 0.02 / 0.48} 

\\ \hline
\specialrule{0.5pt}{0.5pt}{0.5pt}
\end{tabular}
}
\caption{Architecture Ablations. The reprojection AUC, rotation and translation errors are reported. OR Input indicates the input types of outlier rejection. Feat. means enhanced feature from GNN as input, and BV refers to bearing vector. The best results are bold.
} 
\label{tab:abla}
\end{center}
\end{table*}

\noindent
\textbf{Visual Localization.} 
As presented in Table~\ref{tab: localization}, A2-GNN outperforms GoMatch and DGC-GNN, achieving state-of-the-art results in the visual descriptor-free group. Specifically, the average pose error for the Cambridge Landmarks dataset is 40.5 cm / 1.48$^\circ$ for A2-GNN, compared to 54 cm / 2.23$^\circ$ for DGC-GNN and 173 cm / 5.87$^\circ$ for GoMatch. For the 7Scenes dataset, the average pose error is 14 cm / 4.04$^\circ$ for A2-GNN, 16 cm / 4.81$^\circ$ for DGC-GNN, and 22 cm / 5.77$^\circ$ for GoMatch. Additionally, A2-GNN retains the advantages of DF methods, including low storage requirements, privacy preservation, and low maintenance costs.

\noindent
\textbf{Generalizability.}
Similar to previous work~\cite{zhou2022geometry,wang2024dgc}, we evaluate the generalizability of our model across different datasets and keypoint detectors for visual localization tasks. Our model is trained on the MegaDepth~\cite{li2018megadepth} using the SIFT~\cite{lowe2004distinctive} detector and evaluated on the indoor 7Scenes dataset~\cite{shotton2013scene} with the SIFT~\cite{lowe2004distinctive} keypoint detector and the outdoor Cambridge dataset~\cite{kendall2015posenet} using the SuperPoint~\cite{detone2018superpoint} detector. These experiments are summarized in Table~\ref{tab: localization}, providing a comprehensive overview of our A2-GNN performance under varying training and evaluation conditions. Remarkably, even when using the SuperPoint detector on the Cambridge dataset, A2-GNN demonstrates strong performance, highlighting its robust generalizability. 
\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{figure/outlier.png}
  \caption{Outlier Sensitivity.  Comparison of the AUC for GoMatch~\cite{zhou2022geometry}, DGC-GNN~\cite{wang2024dgc}, and the proposed A2-GNN under different outlier ratios at 1, 5, and 10 pixels thresholds.
  Oracle is the upper bound by using ground truth matches.
  }
  \label{fig: outliers}
\end{figure}


\noindent
\textbf{Ablation Studies.}
We validate our A2-GNN module design by conducting ablation studies on the MegaDepth dataset~\cite{li2018megadepth} with the top-10 retrieved images ($k=10$). The ablation results are presented in Table~\ref{tab:abla}. First, we analyze the impact of the outlier rejection inputs by changing the input from enhanced features to bearing vectors. This change improves the reprojection AUC by 1.67 / 3.55 / 2.99 \%, confirming our assumption that explicit positional information can enhance consensus in epipolar geometry, thereby improving outlier rejection accuracy. 
Next, we perform ablations on our geometric feature embedding. Incorporating both max-pooling and annular convolution significantly boosts reprojection AUC by 3.47 / 8.27 / 7.84 \%. 
We can conclude that max-pooling operation holds the permutation invariance, a property that is vital for robust feature learning.
Additionally, angle embedding further enhances geometric embedding and shows improvement by 0.75 / 1.62 / 1.59\%. 
Similar to~\cite{wang2024dgc}, we also observe notable gains when color information is added to the point feature.

The evaluation results without outlier rejection are shown in Table~\ref{tab: cl}. We use image retrieval with $k=1$ and set the outlier rejection threshold to 0. Our A2-GNN demonstrates the ability to generate high-quality 2D-3D correspondences without outlier rejection, outperforming both GoMatch and DGC-GNN by large margins.  
Unlike GoMatch and DGC-GNN, A2-GNN uses geometric features refined by GNN layers solely for matching, rather than simultaneously for outlier rejection. This highlights the goal disparities between matching and removing outliers.

\begin{table}[t!]

\centering
\renewcommand\arraystretch{1.5}
\Large
\resizebox{.48\textwidth}{!}{
  \begin{tabular}{lccc}
    \toprule
    \specialrule{0.5pt}{0.5pt}{0.5pt}
    \multirow{2}{*}{Method}   & Reproj. AUC (\%) &
  Rotation ($^\circ$)  &
  Translation (m)  \\ 
  & @1 / 5 / 10px ($\uparrow$) &
  \multicolumn{2}{c} {Quantile @25 / 50 / 75\% $(\downarrow)$} \\
    \midrule
    GoMatch (no OR) & \ 4.47 / 17.95 / 23.42 &1.29 / 11.85 / 33.60&0.11 / 1.18 / 3.58 \\
    DGC-GNN (no OR) & \ 8.56 / 30.79 / 37.02& 0.22 /  \ 4.85 / 30.07 & 0.02 / 0.47 / 3.10 \\
    \textbf{A2-GNN (no OR)} & \textbf{11.37 / 37.04 / 43.15 }&  \textbf{ 0.13 / \ 2.32 / 27.00} & \textbf{0.01 / 0.22 / 2.87 }\\
    \bottomrule
    \specialrule{0.5pt}{0.5pt}{0.5pt}
\end{tabular}
}
\caption{Ablation results without outlier rejection on MegaDepth on top-1 image retrieval. 
} 
\label{tab: cl}
\end{table}


\noindent
\textbf{Sensitivity to Keypoint Outliers.}
To evaluate our method's performance in handling keypoint outliers, we follow the procedure outlined in~\cite{zhou2022geometry, wang2024dgc} and vary the outlier ratios from 0 to 1 during inference. We present the AUC across various pixel thresholds and outlier ratios in Fig.~\ref{fig: outliers}. When the outlier ratio is 0, all input keypoints are derived from the ground truth, meaning no outliers are present. Conversely, at a ratio of 1, all keypoints are directly taken from the 2D query and the 3D points from the top-k retrieved images.

As shown in Fig.~\ref{fig: outliers}, our A2-GNN model consistently outperforms GoMatch and DGC-GNN across all outlier rate scenarios. When the outlier rate is below 50\%, the AUC of our method closely approaches the Oracle upper bound.
However, when the outlier rate exceeds 50\%, performance declines rapidly. This is due to the increased difficulty in identifying correct matches in high-outlier scenarios. Additionally, our model was trained under conditions with a 50\% outlier rate and thus does not perform as well when inputs are chosen randomly.