\section{Practical Algorithm: Stackelberg Self-Annotated Preference Optimization }
\label{sec:ssapo}

In this section, we bridge the theoretical Stackelberg framework (Section~\ref{sec:theory}) to a practical algorithm, \emph{Stackelberg Self-Annotated Preference Optimization} (\textbf{SSAPO}).  As described in Section~\ref{sec:theory}, the “leader” (policy) optimizes against a worst-case “follower” distribution of preferences in a Wasserstein ball around the empirical distribution.  However, several obstacles arise in practice:(A) \emph{Limited human-annotated data:}  We need to \emph{self-annotate} preferences from unlabeled data to enlarge the training distribution.
(B) \emph{Non-convex pairwise loss:} The term $-\!\log\,\sigma(\xi)$ is \emph{convex} in $\xi$, which complicates direct application of standard concave assumption in DRO solvers for the follower’s optimization.
(C) \emph{Scalability:} For large datasets, solving a full distributionally robust program over $N$ samples can be computationally expensive.

We address these via an iterative procedure:  
(1)) \emph{Self-Annotation} to handle data scarcity,  
(2) \emph{Tangent-line convexification} to ensure we can solve a convex follower problem, and 
(3) \emph{Grouping} to tackle scalability. 

The final output retains the main theoretical guarantees of SGPO (robustness, bounded regret) under mild approximations, as elaborated below.

\subsection{Worst-Case Distribution Computation via Wasserstein DRO}
\label{subsec:follower_esfahani}

Recall from Section~\ref{sec:theory} that SGPO is formulated as a 
two-player Stackelberg game, where the \emph{follower} selects 
\(
Q \;\;\in\;\; B_\epsilon(\hat{P}_N)\)
to minimize \(
\mathbb{E}_Q\Bigl[-\!\log\,\sigma\bigl(R_{\pi}(y_w) \;-\; R_{\pi}(y_l)\bigr)\Bigr],
\)
with $B_\epsilon(\hat{P}_N)$ the 1-Wasserstein ball of radius $\epsilon$ 
around the empirical distribution $\hat{P}_N$.  
To align with a standard distributionally robust optimization (DRO) viewpoint, 
one rewrites the problem equivalently as
\begin{equation}
\label{eq:negative_logloss}
\max_{Q \,\in\, B_\epsilon(\hat{P}_N)}
\quad
\mathbb{E}_Q\bigl[\ell(\xi)\bigr],
\end{equation}
where
\(\xi 
\;=\;
R_{\pi}(y_w)\;-\;R_{\pi}(y_l),
\;\;
\ell(\xi)
\;=\;
-\!\log\,\sigma(\xi).
\)

However, the powerful DRO result of \citet{Esfahani2018DataDrivenDR} 
requires that \(\ell(\cdot)\) be \emph{concave}; see 
Assumption~\ref{assumption:convexity_esfahani}.  
However, $-\!\log\,\sigma(\xi)$ is \emph{convex}, 
so we must deal with the violation of concavity of the loss function to invoke the theorem.  
Nevertheless, let us first restate (in a slightly specialized form) 
the \citet{Esfahani2018DataDrivenDR} result to clarify \emph{why} we aim to 
construct a finite convex program.

\begin{assumption}[Concavity Assumption \citep{Esfahani2018DataDrivenDR}]
\label{assumption:convexity_esfahani}
Let $\Xi \subset \mathbb{R}^m$ be convex and closed.  
Suppose there exist functions $\ell_k$, $k=1,\ldots,K$, such that
\[
\ell(\xi)
\;=\;
\max_{\,1\le k\le K}\,\ell_k(\xi),
\]
and each $\ell_k(\xi)$ is proper, concave, and lower semicontinuous; further, 
$\ell_k(\cdot)$ is not identically $-\infty$ on $\Xi$.  
Then $\ell(\cdot)$ is convex on $\Xi$.
\end{assumption}

\begin{theorem}[Worst-Case Extremal Distributions, \citealp{Esfahani2018DataDrivenDR}]
\label{thm:worst_case_esfahani}
Under Assumption~\ref{assumption:convexity_esfahani}, 
consider $N$ observations $\{\hat{\xi}_i\}_{i=1}^N$ forming the empirical 
distribution $\hat{P}_N = \tfrac{1}{N}\sum_{i=1}^N \delta_{\hat{\xi}_i}$, 
and let $B_\epsilon(\hat{P}_N)$ be the 1-Wasserstein ball of radius $\epsilon$.  
Then:
\begin{equation}
\label{eq:esfahani_obj}
\sup_{\,Q \,\in\,B_\epsilon(\hat{P}_N)}\;
\mathbb{E}_{Q}\bigl[\ell(\xi)\bigr]
\;=\;
\max_{\substack{\alpha_{ik},\,q_{ik}\\i=1,\ldots,N;\,k=1,\ldots,K}}
\;
\frac{1}{N}
\sum_{i=1}^N
\sum_{k=1}^K
\alpha_{ik}\,\ell_k\Bigl(\hat{\xi}_i - \frac{q_{ik}}{\alpha_{ik}}\Bigr),
\end{equation}
subject to:
\begin{align}
\label{eq:esfahani_cons1}
&\frac{1}{N}\sum_{i=1}^N\sum_{k=1}^K \|q_{ik}\|\;\le\;\epsilon,
\\
\label{eq:esfahani_cons2}
&\sum_{k=1}^K \alpha_{ik} \;=\;1,\quad \alpha_{ik}\ge 0,
\\
\label{eq:esfahani_cons3}
&\hat{\xi}_i - \tfrac{q_{ik}}{\alpha_{ik}} \;\in\;\Xi,
\end{align}
for all $i,k$.  Let 
$\{\alpha_{ik}^{(r)},\,q_{ik}^{(r)}\}_{r=1}^\infty$ 
be a sequence of feasible decisions in the right-hand side problem whose 
objectives converge to the supremum.  Then the discrete distributions
\begin{equation}
\label{eq:esfahani_extremal}
Q^{(r)}
\;=\;
\frac{1}{N}\sum_{i=1}^N \sum_{k=1}^K
\alpha_{ik}^{(r)} \,\delta_{\hat{\xi}_i - \tfrac{q_{ik}^{(r)}}{\alpha_{ik}^{(r)}}}
\;\;\;\in\;B_\epsilon(\hat{P}_N),
\quad
\sup_{\,Q \in B_\epsilon(\hat{P}_N)}\mathbb{E}_Q[\ell(\xi)]
\;=\;
\lim_{r\to\infty}\;\mathbb{E}_{\,Q^{(r)}}\bigl[\ell(\xi)\bigr].
\end{equation}
\end{theorem}

This theorem provides a \emph{finite-dimensional convex program} 
for the adversarial reweighting of samples 
(\(\hat{\xi}_i \mapsto \hat{\xi}_i - \tfrac{q_{ik}}{\alpha_{ik}}\)), 
enabling solutions via standard convex solvers.

\noindent
\textbf{Tangent-Line Approximation for $-\log\,\sigma(\cdot)$.}
To apply Theorem~\ref{thm:worst_case_esfahani} to the function 
\(\ell(\xi) = -\log\,\sigma(\xi)\) in \eqref{eq:negative_logloss}, 
we must reconcile the fact that $-\log\,\sigma(\xi)$ is \emph{concave} in $\xi$.  
A standard trick is to use a \emph{tangent-line} (affine) approximation: 
for any $\xi_0$, one has
\begin{equation}
\label{eq:concave_tangent}
-\log\,\sigma(\xi)
\;\;\le\;\;
-\log\,\sigma(\xi_0)
\;+\;
\nabla \bigl[-\!\log\,\sigma(\xi_0)\bigr]^\top\;(\,\xi - \xi_0),
\end{equation}
since $-\!\log\,\sigma(\cdot)$ is concave.  
Define the affine \emph{surrogate} function
\begin{equation}
\label{eq:ell_tangent}
\ell_{\text{tangent}}(\xi;\,\xi_0)
\;:=\;
-\!\log\,\sigma(\xi_0)
\;+\;
\nabla \bigl[-\!\log\,\sigma(\xi_0)\bigr]^\top\;\bigl(\xi - \xi_0\bigr),
\end{equation}
which is convex in $\xi$.  We then replace 
$\ell(\xi)$ by $\ell_{\text{tangent}}(\xi;\,\xi_0)$ inside 
\eqref{eq:esfahani_obj}--\eqref{eq:esfahani_extremal}, 
yielding a finite convex program.  In practice, we iterate:

\begin{itemize}
\item Compute or update $\{\xi_0^i\}_{i=1}^N$, typically from the current policy 
      parameters (i.e. $\xi_0^i = R_{\pi}(y_w^i)-R_{\pi}(y_l^i)$).
\item Construct the tangent surrogate $\ell_{\text{tangent}}(\cdot;\xi_0^i)$ 
      around each $\hat{\xi}_i = \xi_0^i$.
\item Solve the finite convex program to obtain a worst-case distribution 
      $Q^{*}$, then update the policy.
\end{itemize}

%Although $\ell_{\text{tangent}}(\cdot;\xi_0^i)$ is an \emph{upper bound} 
%on $-\log\,\sigma(\cdot)$, this iterative procedure (a “majorize-minimize” scheme) 
%improves robustness while remaining tractable.  


\paragraph{Applying Theorem~\ref{thm:worst_case_esfahani} in SSAPO.}
Once we replace $-\!\log\,\sigma(\xi)$ by its tangent surrogate $\ell_{\text{tangent}}(\xi;\xi_0^i)$ , it becomes \emph{affine} (hence convex), so Theorem~\ref{thm:worst_case_esfahani} applies directly.  We obtain a finite-dimensional convex program to find the adversarial $Q^*\in B_\epsilon(\hat{P}_N)$.  


\subsection{The SSAPO Algorithm}
\label{subsec:ssapo_algorithm}

Equipped with (1)~the \citet{Esfahani2018DataDrivenDR} finite-program result 
(Theorem~\ref{thm:worst_case_esfahani}), and 
(2)~the tangent-line approach for $-\log\,\sigma(\cdot)$ 
(Section~\ref{subsec:tangent_line}), 
we now describe SSAPO step by step.

\paragraph{Self-Annotated Preference Dataset.}
We start with a small \(\mathcal{D}_{\text{seed}}\) of human-labeled preference pairs.  
We also have a larger unlabeled set \(\mathcal{D}_{\text{unlabeled}}\).  At each iteration:

\begin{enumerate}
     \item[\textbf{(1)}]
      Self-annotation:
      We sample prompts from $\mathcal{D}_{\text{unlabeled}}$, let the current policy 
      $\pi_{\theta_t}$ generate candidate responses, and have $\pi_{\theta_t}$ 
      \emph{rank} them to form winner-loser pairs $(y_w,y_l)$.  We then add these 
      synthetic preferences to $\mathcal{D}$, effectively enlarging 
      our empirical distribution $\hat{P}_N$.
    
    \item[\textbf{(2)}] 
      Form $\hat{P}_N$:
      For each $(y_w^i,y_l^i)\in \mathcal{D}$, compute 
      $\hat{\xi}_i = R_{\theta_t}(y_w^i)-R_{\theta_t}(y_l^i)$.  
      Thus, $\hat{P}_N = \tfrac1N\sum_{i=1}^N \delta_{\hat{\xi}_i}$.

    \item[\textbf{(3)}] 
      Tangent-line surrogate:
      For each $\hat{\xi}_i$, define 
      $\ell_{\text{tangent}}(\xi;\,\hat{\xi}_i)$ 
      via \eqref{eq:concave_tangent}--\eqref{eq:ell_tangent}.

    \item[\textbf{(4)}] 
      Worst-case distribution:
      Solve the finite convex program in 
      \eqref{eq:esfahani_obj}--\eqref{eq:esfahani_extremal}, 
      but with $\ell_k(\cdot)$ replaced by 
      $\ell_{\text{tangent}}(\cdot;\,\hat{\xi}_i)$.  
      This yields a discrete $Q^*$ (or $Q^{(r)}$) in $B_\epsilon(\hat{P}_N)$ 
      that reweights or “shifts” each $\hat{\xi}_i$ adversarially within radius $\epsilon$.

    \item[\textbf{(5)}] 
      Policy update:
      We train $\pi_{\theta}$ via gradient-based methods on the objective
      $\min_{\theta}\,\mathbb{E}_{Q^*}\bigl[-\!\log\,\sigma(\,\xi_{\theta}\,)\bigr]$
      (or an equivalent DPO-style logistic loss), effectively performing 
      a robust best-response step against the adversarial distribution $Q^*$.
\end{enumerate}

Algorithm~\ref{algo:ssapo} sketches this procedure.  Over iterations, 
the policy $\pi_{\theta_t}$ and distribution $Q^*_t$ act as the leader-follower 
in the sense of \S\ref{sec:theory}, though we are approximating the exact 
SGPO solution with the linear surrogate.  

\begin{algorithm}[t!]
\caption{Stackelberg Self-Annotated Preference Optimization (SSAPO)}
\label{algo:ssapo}
\begin{algorithmic}[1]
\REQUIRE 
  Seed labeled set $\mathcal{D}_{\mathrm{seed}}$; 
  unlabeled data $\mathcal{D}_{\mathrm{unlabeled}}$; 
  Wasserstein radius $\epsilon$; 
  number of iterations $T$.

\STATE \textbf{(Initial DPO Training)}: 
   Train policy $\theta_0$ by applying DPO on $\mathcal{D}_{\mathrm{seed}}$ 
   to obtain a warm-start model.

\FOR{$t=0$ \textbf{to} $T-1$}
    \STATE \textbf{(Self-Annotation)}: 
      - Sample a batch of prompts from $\mathcal{D}_{\mathrm{unlabeled}}$. \\
      - Generate multiple candidate responses with $\pi_{\theta_t}$. \\
      - (Optionally) let $\pi_{\theta_t}$ rank them or use partial human checks to form winner-loser pairs $(y_w,y_l)$. \\
      - Add these \emph{new} pairs to $\mathcal{D}$.

    \STATE \textbf{(Form $\hat{P}_N$)}: 
      - Let $N=|\mathcal{D}|$.  
      - For each $(y_w^i,y_l^i)\in \mathcal{D}$, compute 
        $\hat{\xi}_i = R_{\theta_t}(y_w^i)-R_{\theta_t}(y_l^i)$. \\
      - Define $\hat{P}_N=\frac{1}{N}\sum_{i=1}^N \delta_{\hat{\xi}_i}$.

    \STATE \textbf{(Tangent Approx.)}:
      - For each $\hat{\xi}_i$, define $\ell_{\mathrm{tangent}}(\xi;\,\hat{\xi}_i)$ by linearizing $-\!\log\,\sigma(\xi)$ at $\xi=\hat{\xi}_i$.  
      - This yields a convex surrogate for each sample.

    \STATE \textbf{(Worst-Case Distribution)}:
      - Using Theorem~\ref{thm:worst_case_esfahani}, solve the finite convex program that replaces $\ell(\xi)$ with $\ell_{\mathrm{tangent}}(\xi;\,\hat{\xi}_i)$ inside \eqref{eq:esfahani_obj}, subject to $W(\cdot,\hat{P}_N)\le \epsilon$. \\
      - Obtain the adversarial distribution $Q^*_t \in B_\epsilon(\hat{P}_N)$, typically represented as a discrete re-weighting or shifted set of points.

    \STATE \textbf{(Policy Update)}:
      - Expand or re-weight the training set with $Q^*_t$.  
      - Retrain $\theta_{t+1}$ by minimizing 
        $\mathbb{E}_{Q^*_t}\!\bigl[-\!\log\,\sigma(R_{\theta}(y_w)-R_{\theta}(y_l))\bigr]$ 
        (or an equivalent logistic pairwise loss).  
      - In practice, use gradient-based methods (e.g.\ mini-batch SGD).

\ENDFOR

\STATE \textbf{Return} $\theta_T$ (final robust policy).
\end{algorithmic}
\end{algorithm}


\paragraph{Partition-Based Approximation.}
At iteration $t$, the step \emph{(Worst-Case Distribution)} in 
Algorithm~\ref{algo:ssapo} solves a convex program with $\mathcal{O}(N \times K)$ 
variables $(\alpha_{ik},\,q_{ik})$, where $K$ is the number of affine pieces used.  
For large $N$ (e.g.\ hundreds of thousands of preferences), 
this can be expensive.
%One may resort to primal-dual or specialized cutting-plane
%methods \citep{Esfahani2018DataDrivenDR}, or use approximate relaxations with 
%entropic regularization \citep{Cuturi2013Sinkhorn}.
To handle truly large $N$, SSAPO can \emph{partition} $\{\hat{\xi}_i\}$ 
into smaller groups (each of size $G \ll N$), solve the finite program 
independently in each group, and then \emph{average} the resulting $Q^*_m$.  
Formally, if we partition $N$ samples into $M = N/G$ subsets, let 
$Q^*_m$ be the worst-case distribution for subset $m$, then 
\begin{equation}
\label{eq:partition_average}
Q_{\mathrm{final}}^*
\;=\;
\frac{1}{M}
\sum_{m=1}^M
Q^*_m
\end{equation}
yields a practical approximate solution.  This approach, while not solving the 
global $N$-sample problem exactly, greatly reduces memory/compute cost, and is parallelizable.
%Alternatives include \emph{mini-batch} sampling \citep{Blanchet2019quantifying} 
%and \emph{clustering} \citep{Khezeli2019,Gao2022distributionally}.

\paragraph{Discussions on Algorithmic Gap.}
Although this procedure implements the SGPO idea, two important points arise:
\begin{enumerate}
\item \textbf{Tangent-Line Approximation Error:} Because $-\!\log\,\sigma(\xi)$ is concave, the surrogate $\ell_{\mathrm{tangent}}(\xi;\,\xi_0)$ is strictly an \emph{upper} approximation.  Hence the adversarial $Q^*_t$ is found for a slightly “inflated” or “conservative” approximation of the true worst-case distribution for $-\!\log\,\sigma(\xi)$.  So in principle the final solution is an \emph{approximate} Stackelberg equilibrium.  However, in practice, we can keep the mismatch small by choosing relevant $\xi_0$ points close to the model’s typical range of $(y_w,y_l)$ pairs. 
 
\item \textbf{Exact vs.\ Approximate DRO Solves:}  For large $N$, we seldom solve the follower’s problem exactly.  Instead, we do a partition-based approach.  Such approximations further create a small gap from the \emph{ideal} Stackelberg equilibrium.  Nonetheless, the theoretical analysis of \emph{distributionally robust} solutions ensures that even approximate $Q^*_t$ can significantly reduce worst-case regret, as shown in the bounding arguments of Section~\ref{sec:regret}. Beyond grouping, one might consider: Entropic Regularization \citep{Cuturi2013Sinkhorn} of the transport cost to
      accelerate primal-dual algorithms.Mini-batching or Online Approximations \citep{Blanchet2019quantifying} that randomly sample subsets of $\mathcal{D}$ at each iteration. Clustering or Coarse Binning \citep{Khezeli2019,Gao2022distributionally} to
      reduce $N$. We left the explorations of those advance techniques for future works.
\end{enumerate}

Despite these caveats, SSAPO preserves the key insights of SGPO---namely, the \emph{robustness} under $\epsilon$-bounded preference noise or shifts (cf.\ Theorem~\ref{thm:sgpo_regret_supp} in Section~\ref{sec:regret}).  It also offers a practical, end-to-end recipe for drastically reducing reliance on expensive human annotation: a small \textbf{seed} set can be iteratively expanded via \emph{self-annotation}, while the \emph{adversarial reweighting} step guards against errors or distributional differences in newly generated pairs.  Empirical results in Section~\ref{sec:experiments} confirm that SSAPO often matches state-of-the-art preference alignment performance with far fewer human labels.

