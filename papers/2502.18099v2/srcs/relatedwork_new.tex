\section{Related Work}
\label{sec:relatedwork}
\paragraph{LLM Alignment and Data-Efficient Methods}
Aligning large language models (LLMs) with human preferences is central to modern deployments
\citep{Ziegler2019Fine,Ouyang2022Training,Bai2022Training},.
While Reinforcement Learning with Human Feedback (RLHF) \citep{Christiano2017Deep} trains a reward model and then maximizes it under KL constraints, it typically requires massive human-annotated data. Recent alternatives focus on \emph{directly} fine-tuning LLMs from pairwise preference data without an explicit reward model. 
Notably, Direct Preference Optimization (DPO) \citep{Rafailov2023Direct}
derives a closed-form surrogate objective that recovers RLHF’s solution 
but avoids a separate reward modeling stage. Subsequent works simplify or extend this pipeline; for instance, \citet{Ethayarajh2024KTO} remove the need for pairwise labels by adopting a human utility model, 
while there are also works \citep{Meng2024Simpo,Hong2024ORPO,Azar2024General} introduce novel optimization objectives  to handle different preference formats. 
Despite progress, these approaches still rely on large-scale preference annotations, making label-efficiency a key challenge. To reduce the reliance on expensive human labels, several methods have explored letting the LLM or an auxiliary model 
generate and rank unlabeled responses, thereby creating synthetic preference data \citep{Jiang2023LLM,Yuan2024Self,Xiong2024Iterative,Kim2025Spread}. 
However, many of these approaches assume accessibility to a reliable well-aligned "judge", which could be prohibitive costly in realistic scenarios. To address the cost bottleneck, \citet{Kim2025Spread} propose a \emph{Spread Preference Annotation (SPA)} framework that starts from a small seed of human-annotated preferences and iteratively expands the dataset by self-annotation. Our work is closely related to SPA: we replicate its experimental setup by using the same small-scale seed preferences and iterating between new response generation and preference learning. 
However, our \emph{Stackelberg} perspective considers the inaccuracy of self-annotation, and explicitly defends against worst-case preference shifts. Empirically, we show that this game-theoretic \emph{distributional} approach yields stronger label efficiency.

\vspace{-0.15 in}
\paragraph{Game-Theoretic Alignment Methods}
An emerging body of work has begun to frame preference alignment of LLMs 
through the lens of \emph{games}. A conceptual similar work \citep{Makar2024Sta} propose \emph{Stackelberg Alignment RLHF}. However, their nested gradient-based heuristic does not guaranteed to converge to the equilibrium. While we prove our updates for the leader and follower converge to an equilibrium.
Meanwhile, \citet{Ye2024Evolving} present a framework that casts prompt-creator and solver asymmetric players in an evolving game, the differences between our work is we focus on evolving the distribution of the  responses , while they focus on evoling the distribution of the prompts. SPIN \citep{Chen2024SelfPlay} use self-play to iteratively refine 
a policy without additional human data, however they assume accessible to adequate supervised fine-tuning (SFT) data. 
Other works adopt \emph{Nash} or \emph{minimax} formulations:
\citet{Melnyk2024Distributional} study alignment via an optimal-transport objective to capture distributional preferences, 
\citet{Zhang2024Iterative} and \citet{Rosset2024Direct} formulate alignment as a two-player game aiming for a Nash policy, 
and \citet{Munos2024Nash} proposes “Nash learning from human feedback” 
by treating the policy and a competing policy as iterative players.
Likewise, \citet{Swamy2024Minimaximalist,Wu2024Self} introduce self-play preference optimization methods 
in which two policies repeatedly compete under a constant-sum setting. 
They demonstrate promising performance on synthetic and text-based benchmarks, but typically set both players as \emph{policy vs. policy}. By contrast, our \emph{SGPO} framework focuses on \emph{policy vs. distribution}:  the leader policy maximizes preference likelihood, while the follower adversarially reweights or shifts the empirical preference distribution. This setup offers a distinct distributional robust-control view, leading to tight theoretical guarantees (e.g., $\mathcal{O}(\epsilon)$-bounded regret) and a practical algorithm (SSAPO) that is readily integrated with self-annotation. Hence, our method complements the “policy vs.\ policy” family by delivering strong resistance to noisy or distribution-mismatched preferences at small annotation cost.