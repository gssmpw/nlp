\section{Experiments}
\label{sec:experiments}
In this section, we present an extensive empirical evaluation of our proposed \emph{Stackelberg Self-Annotated Preference Optimization} (SSAPO) algorithm.
\subsection{Experiment Setup}
We introduce the basic experiment setup in this subsection (Cf. Appendix~\ref{sec:experimental_details_more} for more details). The settings are mostly consistent to the recent literature \citet{Kim2025Spread}.
\textbf{Datasets}. We used the UltraFeedback dataset~\cite{Cui2023Ultrafeedback}, containing 60K samples. A seed of 2K human-labeled preferences (3.3\% of total data) was used for initial training. The rest (58K samples) were split into three subsets (8K, 20K, and 30K) for self-annotation in iterative stages.

\textbf{Models}.
We use the supervised fine-tuned Mistral-7B-0.1~\cite{jiang2023mistral} as the initial model $\pi_{\text{init}}$ and LLaMA-3-8B\footnote{\href{https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct}{meta-llama/Meta-Llama-3-8B-Instruct}} for compatibility checks. All models are fine-tuned on UltraChat~\cite{ding2023enhancing}.

\textbf{Evaluations}.
We use \textbf{AlpacaEval 2.0}~\cite{dubois2024alpacafarm} for instruction-following tasks and \textbf{MT-Bench}~\cite{zheng2023judging} to evaluate multi-turn performance across tasks like math, coding, and writing. Both benchmarks assess the alignment with human preferences and the model’s functional proficiency. We stress that AlpacaEval 2.0 is especially useful for measuring how well the model aligns with general user preferences (and controlling for length bias), whereas MT-Bench tests the model’s functional capabilities across a broader range of tasks.

\textbf{Implementation.}
We initialize training with DPO on 2K seed samples, followed by 3 iterative stages of self-annotation. In each stage, new preferences are generated via a policy that ranks response pairs. A distributionally robust optimization (DRO) is performed using sequential least squares programming (SLSQP) to adjust the model based on adversarial shifts within a Wasserstein ball. The group size $G$ for parallel computation is set to 100 unless otherwise specified.

\textbf{Baselines.}
We consider the following baselines for comparison: (1) DPO, which performs DPO training only on the seed data. (2) Iter DPO~\cite{Xiong2024Iterative}, which iteratively generates preference data using an external reward model (PairRM)~\cite{Jiang2023LLM} or LLM-as-judge~\cite{Li2024LLMs}. (3) SPA~\cite{Kim2025Spread}, which iteratively generates preference data using implicit reward model.

\begin{table*}[ht]
\centering
\caption{\textbf{Main results.} Evaluation results on AlpacaEval 2.0 and MT-Bench with different variants of Mistral-7B-v0.1 and LLaMA3-8B. All models use the same 3.3\% preference data with gold label as seed data.  The best and second-best results are highlighted in bold and underlined, respectively. Most of the baseline results are from \cite{Kim2025Spread}.}
\resizebox{\columnwidth}{!}{
\begin{tabular}{lccc}
\toprule
\textbf{Models} &  \multicolumn{2}{c}{\textbf{AlpacaEval 2.0}} & \textbf{MT-Bench} \\
\cmidrule(lr){2-4}
&  \textbf{Len-control. Win Rate (\%)} & \textbf{Win Rate vs. GPT-4 (\%)} & \textbf{Avg. Score (0-10)} \\

\midrule
Mistral-7B-DPO &  9.03 & 7.68 & 6.81 \\
Mistral-7B-Iter DPO (PairRM)  & 11.87 & 9.46 & 6.98 \\
Mistral-7B-Iter DPO (LLM-as-judge)  & 9.28 & 9.18 & 6.67 \\
Llama3-8B-DPO & 20.61 & 18.04 & - \\
Mistral-7B-SPA  & 15.39 & 21.13 & 6.94 \\
Llama3-8B-SPA  & 21.85 & 24.95 & 7.86 \\
\midrule
Mistral-7B-SSAPO (Ours)  & \underline{24.44} & \underline{35.82} & 6.68 \\
Llama3-8B-SSAPO (Ours)  & \bf{33.33} & \bf{40.12} & \bf{8.03} \\
\bottomrule
\end{tabular}}
\label{tab:main_results}
\end{table*}


\begin{table*}[ht]
\centering
\caption{\textbf{Comparison with different variants of Mistral.} Evaluation results on AlpacaEval 2.0 and MT-Bench with different variants of Mistral-7B-v0.1. The best scores are highlighted with bold. The baseline results are from \cite{Kim2025Spread} and \citep{dubois2024length}.}
\resizebox{\columnwidth}{!}{
\begin{tabular}{lcccc}
\toprule
\textbf{Models} & \textbf{Gold Label (\%)} & \multicolumn{2}{c}{\textbf{AlpacaEval 2.0}} & \textbf{MT-Bench} \\
\cmidrule(lr){3-5}
& & \textbf{Len-control. Win Rate (\%)} & \textbf{Win Rate vs. GPT-4 (\%)} & \textbf{Avg. Score (0-10)} \\


\midrule
Mistral-7B-v0.1 & - & 0.17 & 0.50 & 3.25 \\
Zephyr-7B-$\beta$ & 100 & 11.75 & 10.03 & 6.87 \\
Mistral-7B-SFT & - & 7.58 & 4.72 & 6.34 \\
Mistral-7B-DPO & 3.3 & 9.03 & 7.68 & 6.81 \\
Mistral-Large (123B) & - & 21.4 & 32.7 & - \\
Mistral-7B-SSAPO (Ours) & 3.3 & \bf{24.44} & \bf{35.82} & 6.68 \\
\bottomrule
\end{tabular}}
\label{tab:mistral_variants}
\end{table*}

\subsection{Main Results}
\label{subsec:main_results}

Table~\ref{tab:main_results} summarizes our primary comparison on \textbf{AlpacaEval 2.0} and \textbf{MT-Bench}. All models in this comparison use only 3.3\% of the UltraFeedback dataset as seed data (2K human-labeled preference pairs), with the remainder self-annotated. Our \emph{SSAPO} method consistently outperforms DPO and other iterative baselines (e.g., Iter-DPO, SPA) in both the length-controlled (LC) and raw win-rate metrics on AlpacaEval~2.0. For Mistral-7B, \textbf{SSAPO} achieves \underline{24.44\%} LC win rate and \underline{35.82\%} raw win rate, compared to only 9.03\% and 7.68\% with standard DPO. On the larger LLaMA-3-8B model, SSAPO reaches a \textbf{33.33\%} LC win rate and \textbf{40.12\%} raw win rate, surpassing its SPA counterpart by a wide margin. MT-Bench scores corroborate these improvements, indicating that SSAPO yields robust, high-quality responses across diverse tasks.  %Notably, all SSAPO results are obtained using only 2K human-annotated preferences as seed data. Despite this small fraction (3.3\%) of the total dataset, the model’s alignment performance exceeds or competes with baselines that rely on significantly more human-labeled samples. This confirms one of our key contributions: \emph{leveraging robust self-annotation and adversarial reweighting can drastically reduce the need for costly human labels} while maintaining high alignment quality. This result can be further confirmed by the following Table~\ref{tab:mistral_variants}.

To further illustrate SSAPO’s data-efficiency and robustness, Table~\ref{tab:mistral_variants} compares various Mistral models, including \emph{Mistral-7B-SFT}, \emph{Mistral-Large} (the number of parameters is 123B), and a fully-finetuned \emph{Zephyr-7B-$\beta$} variant with 100\% labeled data. Remarkably, \textbf{Mistral-7B-SSAPO} outperforms or closely approaches these stronger references in AlpacaEval~2.0, despite using only 3.3\% of the human-labeled training set. This demonstrates that \emph{a principled Stackelberg method can substantially mitigate the reliance on massive human annotations}. It also aligns with our theoretical findings (Section~\ref{sec:theory}) that SGPO-based approaches, when instantiated as SSAPO, achieve bounded regret under moderate preference shift.

\subsection{Ablation and Sensitivity Analysis}
\label{subsec:ablation_sensitivity}
\begin{table}[h]
\centering
\caption{\textbf{Effect of Wasserstein Radius $\epsilon$ on Performance.} Evaluation results on Mistral-7B, showing the impact of varying the Wasserstein radius on the Len-control. Win Rate and Win Rate vs. GPT-4.}
% \resizebox{1.0\columnwidth}{!}{
\begin{tabular}{lccccc}
\toprule
$\epsilon$ & 0 & 0.01 & 0.03 & 0.05 & 0.1 \\
\midrule
\textbf{Len-control. Win Rate (\%)} & 19.76 & \textbf{24.44} & 22.42 & 23.20 & 19.78 \\
\textbf{Win Rate vs. GPT-4 (\%)} & 26.58 & \textbf{35.82} & 32.30 & 32.92 & 25.84 \\
\bottomrule
\end{tabular}
% }
\label{tab:wasserstein-radius}
\end{table}

\begin{table}[h]
\centering
\caption{\textbf{Impact of Tangent Size on Model Performance.} Evaluation results on Mistral-7B, showing the effect of the number of tangents $K$ in the piecewise-linear approximation.}
\begin{tabular}{lccc}
\toprule
$K$ & 5 & 6 & 7  \\
\midrule
\textbf{Len-control. Win Rate (\%)} & 22.89 & \textbf{23.20} & 19.05 \\
\textbf{Win Rate vs. GPT-4 (\%)} & 29.19 & \textbf{32.92} & 25.84\\
\bottomrule
\end{tabular}
\label{tab:num-tangents}
\end{table}

\begin{table}[h]
\centering
\caption{\textbf{Effect of Group Size on Performance.} Evaluation results on Mistral-7B, showing how different group sizes $G$ impact the runtime and model performance.}
\begin{tabular}{lcccc}
\toprule
$G$ & 100 & 200 & 300 \\
\midrule
\textbf{CPU Runtime (min)} & 45 & 206 & 630 \\
\textbf{Len-control. Win Rate (\%)} & 13.70 & 14.81 & \textbf{16.95} \\
\textbf{Win Rate vs. GPT-4 (\%)} & 10.00 & 11.74 & \textbf{14.91} \\
\bottomrule
\end{tabular}
\label{tab:group-size}
\end{table}

\begin{table}[h]
\centering
\caption{\textbf{Impact of Seed Data on Model Performance.} Evaluation results on Mistral-7B, showing the effect of varying the seed data used in the iterative self-annotation process.}
\begin{tabular}{lccc}
\toprule
Seed Data & 1st & 2nd & 3rd \\
\midrule
\textbf{Len-control. Win Rate (\%)} & 22.43 & 23.20 & \textbf{23.75} \\
\textbf{Win Rate vs. GPT-4 (\%)} &  29.10 & \textbf{32.92} & 24.47 \\
\textbf{Average Length} & 2648 & 3416 & 2121 \\
\bottomrule
\end{tabular}
\label{tab:seed-data}
\end{table}

We conduct a series of ablation studies to understand the factors influencing the efficacy and robustness of our \emph{Stackelberg Self-Annotated Preference Optimization} (SSAPO). Specifically, we vary the Wasserstein radius $\epsilon$, the number of tangents $K$, the group size $G$, and the size/iteration of seed data. We conduct the experiments on the Mistral-7B model for budget consideration. These experiments confirm our method’s flexibility and validate the practical design choices guided by our theoretical framework.

\paragraph{Wasserstein Radius $\epsilon$.}
Table~\ref{tab:wasserstein-radius} demonstrates how model performance changes with different Wasserstein radius. When $\epsilon = 0$, our approach reduces to self-annotated DPO without robust reweighting, yielding weaker results (19.76\% LC win rate). 
As $\epsilon$ increases slightly (e.g., 0.01--0.05), both win-rates improve substantially, with the best outcomes at $\epsilon=0.01$. 
However, overly large $\epsilon$ (e.g., 0.1) can make the adversarial shift too pessimistic, degrading performance. 
These findings align with our theoretical analysis in Section~\ref{sec:theory}, where moderate $\epsilon$ provides a robust yet not overly conservative solution, thus striking the optimal balance between data fidelity and adversarial resilience.

\paragraph{Number of Tangents $K$.}
Since our piecewise-linear approximation of $-\!\log\,\sigma(\cdot)$ uses $K$ linear segments (cf.\ Section~\ref{sec:ssapo}), we examine how varying $K$ affects alignment (Table~\ref{tab:num-tangents}). 
At $K=5$, the model attains a 22.89\% LC win-rate, while increasing to $K=6$ yields a marginally better 23.20\%. 
Interestingly, moving to $K=7$ leads to performance drops (19.05\%). 
We hypothesize that while a larger $K$ refines the concave envelope, it may also overcomplicate optimization or amplify minor errors in the approximation. Thus, $K=6$ serves as a sweet spot in our setting, balancing expressiveness and computational stability.

\paragraph{Group Size $G$.}
Our distributionally robust optimization solver randomly partition data into groups of size $G$ for parallel subproblem solutions. Table~\ref{tab:group-size} illustrates the trade-off between computational cost and performance. 
A small group size ($G=100$) has faster runtime (45\,min) but yields a 13.70\% LC win-rate, whereas a larger $G=300$ reaches 16.95\% yet takes over 10 times longer (630\,min). 
This confirms that while bigger groups permit more fine-grained reweighting and hence improved alignment, the overhead grows significantly. 
In practice, we choose $G=100$ or $G=200$ for an acceptable performance--efficiency balance.

\subsubsection{Seed Data and Iterative Self-Annotation}
Finally, Table~\ref{tab:seed-data} examines the influence of different seed data on Mistral-7B’s final performance after three rounds of SSAPO. We can see that the results fluctuates across seed data. We hypothesis that the fluctuation is caused by the radius parameter $\epsilon$, which is not tuned with respect to new seed data. The radius describes the magnitude of our faith of distributional shift. A fixed radius may not truly reflect the dynamics of distribution shifts during iterations. For future work, one may consider develop an adaptive strategy for $\epsilon$. Though we have witnessed some fluctuations, our SAPPO consistently improve upon its rival DPO and SPA across different seed data. 

\paragraph{Iterative Performance Gains.}
\begin{figure}[t]
\centering
\includegraphics[width=.4\textwidth]{figs/win-rate-iterations.pdf}
\caption{\textbf{Improvement during iterations} Evaluation results on Alpcaca Eval 2.0 of initial DPO stage and each iterations, the results of the SFT model are from \cite{Kim2025Spread}}.
\label{fig:improv_dur_iters}
\end{figure}

Figure~\ref{fig:improv_dur_iters} provides a more direct illustration of iterative improvement over three rounds of SSAPO. 
Starting from a baseline DPO model, each round not only adds new self-annotated preferences but also reweights them adversarially within an $\epsilon$-Wasserstein ball. 
We observe a consistent upward trend in alignment metrics during the first two rounds, validating our claim that \emph{robust self-annotation can compensate for scarce human labels while preserving alignment quality.} 

Taken together, these ablations highlight the flexibility and effectiveness of SSAPO: Moderate $\epsilon$ balances robustness and data fidelity, confirming our theoretical finding that worst-case reweighting within a bounded radius can significantly enhance alignment without over-penalizing feasible distributions.Piecewise-linear approximations with small $K$ are sufficient to capture the shape of $-\!\log(\sigma(\cdot))$, maintaining computational tractability. Group size $G$ offers a controllable trade-off between runtime and fine-grained adversarial reweighting, making the approach scalable to different budget constraints.
 Iterative self-annotation with minimal seed data substantially boosts alignment, demonstrating that only 2K human-labeled preferences can suffice to achieve high performance. Overall, these experiments affirm our primary contributions: a \emph{data-efficient} and \emph{theoretically grounded} approach to preference alignment.
