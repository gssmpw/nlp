\section{Limitation, Future Work and Conclusion}
\label{sec:conclusion}
Aiming at a data-efficient alignment method, we have introduced SGPO alignment framework with $\mathcal{O}(\epsilon)$-bounded regret under moderate noise or distribution shifts. Our practical instantiation, \emph{SSAPO}, uses self-annotation and distributionally robust reweighting to achieve strong performance with far fewer human labels. The scalability bottleneck of SSAPO comes from the number of preferences $N$, we use a simple uniform group trick to balance between robustness and complexity. For further improvement, one may resort to primal-dual or specialized cutting-plane methods \citep{Esfahani2018Data}, or use approximate relaxations with 
entropic regularization \citep{Cuturi2013Sinkhorn}. We consider a preference data restricted scenario in this paper, however, as a self-annotation procedure, SSAPO can also be integrated with prompt-generation procedure such as EVA \citep{Ye2024Evolving}, which could be crucial to scaling large language model based intelligence, considering that high-quality human data is projected to run out in the next few years \citep{Villalobos2024Will}.
