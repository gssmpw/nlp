\section{Theoretical Foundations of Stackelberg Game Preference Optimization}
\label{sec:theory}

This section presents a detailed analysis of \emph{Stackelberg Game Preference Optimization} (SGPO).  We first formulate the setting and assumptions, and then prove:
\begin{itemize}
  \item \emph{Existence} of a Stackelberg equilibrium under mild conditions (\S\ref{sec:existence});
  \item \emph{Convergence} of iterative leader--follower updates (\S\ref{sec:convergence});
  \item \emph{Robustness} guarantees (\S\ref{sec:robustness}), including bounded regret in the face of adversarial shifts;
  \item \emph{Comparison} to Direct Preference Optimization (DPO) (\S\ref{sec:comparison}), highlighting how SGPO outperforms DPO under moderate or large distribution shifts.
\end{itemize}

Throughout, we denote by $W_1(\cdot,\cdot)$ the 1-Wasserstein distance (also called the Kantorovich metric), and we frequently use \emph{Kantorovich--Rubinstein duality} \cite{villani2003topics, peyre2019computational} to relate changes in expectation to $W_1$ changes in the underlying distributions.

\subsection{Problem Setup and Preliminaries}
\label{sec:setup}

\paragraph{Stackelberg Game.}
We consider a two-player game with:
\[
\textbf{(Leader) } \pi \in \Pi,
\quad
\textbf{(Follower) } P \in \mathcal{P}(\mathcal{Y}),
\]
where:
\begin{itemize}
\item $\Pi$ is the set of all feasible policy parameters (e.g.\ neural network weights in $\R^d$);
\item $\mathcal{P}(\mathcal{Y})$ is the set of probability measures over response pairs $(y_w,y_l)\in \mathcal{Y}\times \mathcal{Y}$; 
\item The follower must pick $P$ from a ball
\[
\mathcal{U}_\epsilon(\hat{P}) \;=\; \bigl\{\,P\in \mathcal{P}(\mathcal{Y}):W_1(P,\hat{P})\le \epsilon \bigr\}
\]
centered at a reference (or nominal) distribution $\hat{P}$.  The parameter $\epsilon\ge 0$ encodes how adversarial or uncertain the follower can be.
\end{itemize}

\paragraph{Leader’s Objective.}
We define
\[
J(\pi,P)
\;=\;\mathbb{E}_{(y_w,y_l)\sim P}\Bigl[\log \sigma\bigl(R_{\pi}(y_w)- R_{\pi}(y_l)\bigr)\Bigr],
\]
where:
\begin{itemize}
\item $R_{\pi}(y)$ is the \emph{reward} (or preference score) of output $y$ under policy $\pi$;
\item $\sigma(\cdot)$ is the logistic sigmoid, $\sigma(z) = 1/(1+e^{-z})$;
\item $\log \sigma(\cdot)$ encourages $R_\pi(y_w)>R_\pi(y_l)$ for pairs that $P$ “prefers” to see as $(y_w,y_l)$ (i.e.\ $y_w$ is the “winner,” $y_l$ is the “loser”).
\end{itemize}

By the nature of the \emph{Stackelberg} game, the leader (policy) picks $\pi$ first, anticipating that the follower (distribution) will then select a “worst-case” $P$ in response.  Formally, the Stackelberg equilibrium $(\pi^*,P^*)$ solves:
\begin{align}
\pi^* 
&\;\in\;
\arg\max_{\pi\in \Pi} \;\min_{P\in \mathcal{U}_\epsilon(\hat{P})} 
\;\EE_{P}\bigl[J(\pi,P)\bigr], 
\label{eq:SGPO-leader} \\[6pt]
P^*
&\;\in\;
\arg\min_{P\in \mathcal{U}_\epsilon(\hat{P})}
\;\EE_{P}\bigl[J(\pi^*,P)\bigr].
\label{eq:SGPO-follower}
\end{align}

\paragraph{SGPO Objective.}
The high-level objective of \emph{Stackelberg Game Preference Optimization (SGPO)} is therefore
\begin{equation}
\label{eq:SGPO-obj}
\max_{\pi\in \Pi} \;
\min_{P\in \mathcal{U}_\epsilon(\hat{P})}
\; \EE_{(y_w,y_l)\sim P}\Bigl[\log \sigma\bigl(R_\pi(y_w) - R_\pi(y_l)\bigr)\Bigr].
\end{equation}
The following sections prove that a solution $(\pi^*,P^*)$ exists, can be computed iteratively, and enjoys robust guarantees against preference shifts within $W_1$ distance $\epsilon$.

\subsection{Core Assumptions}
\label{sec:assumptions}

\begin{assumption}[Regularity Conditions]
\label{assump:main}
We impose:
\begin{enumerate}[leftmargin=1.6em,label=(\roman*)]
  \item \textbf{Compactness:} The policy space $\Pi\subset \R^d$ is compact, or can be restricted to a large compact sublevel set; the support of $\mathcal{Y}$ is compact in some metric space; and for each $\hat{P}$, the Wasserstein ball $\mathcal{U}_\epsilon(\hat{P})$ is also compact in $\mathcal{P}(\mathcal{Y})$ (as often guaranteed by Prokhorov’s theorem when $\mathcal{Y}$ is compact \cite{billingsley1999convergence}).
  \item \textbf{Lipschitz Continuity:}
    \begin{enumerate}
       \item $R_\pi(y)$ is $L_R$-Lipschitz jointly in $(\pi,y)$, i.e.\ there exists $L_R>0$ such that for all $(\pi,y)$ and $(\pi',y')$,
       \[
       |R_\pi(y) - R_{\pi'}(y')|\;\le\;L_R\,\bigl(\|\pi-\pi'\| + d_{\mathcal{Y}}(y,y')\bigr).
       \]
       \item $\log\sigma(z)$ is $L_{\sigma}$-Lipschitz in $z$, where $\sigma(z)$ is the logistic function.  In fact, one can show $L_{\sigma}\leq \tfrac12$ on $\R$, since $\bigl|\tfrac{d}{dz}\log \sigma(z)\bigr| = \bigl|\sigma(-z)\bigr|\le \tfrac12$.
    \end{enumerate}
    Consequently, $J(\pi,P)$ is Lipschitz in $(\pi,P)$ under the 1-Wasserstein metric on $P$, with some overall constant that depends on $L_R, L_{\sigma}$.
  \item \textbf{Convexity of $\mathcal{U}_\epsilon(\hat{P})$:} For each $\hat{P}$, the set $\{P:W_1(P,\hat{P})\le \epsilon\}$ is convex in the Wasserstein space (a standard property of Wasserstein balls \cite{villani2003topics}).
\end{enumerate}
\end{assumption}

\begin{remark}[Non-Compact Parameter Spaces]
In practice, neural network parameters $\pi\in\R^d$ are unbounded.  If $R_\pi$ is \emph{coercive} in $\|\pi\|$, any solution to \eqref{eq:SGPO-leader} must lie in a finite sublevel set.  Alternatively, for large constant $C>0$, we can restrict to $\{\pi:\|\pi\|\le C\}$ and let $C\to\infty$.
\end{remark}

\subsection{Existence of Stackelberg Equilibrium}
\label{sec:existence}

First, we show that a Stackelberg equilibrium $(\pi^*,P^*)$ always exists, satisfying \eqref{eq:SGPO-leader}--\eqref{eq:SGPO-follower}.  The argument adapts from classical results on minimax or \emph{saddle-point} existence, notably Sion’s minimax theorem \cite{sion1958general}, extended to a “max--min” with compact sets.

\begin{theorem}[Existence of Stackelberg Equilibrium]
\label{thm:existence}
Under Assumption~\ref{assump:main}, there exists $(\pi^*, P^*) \in \Pi \times \mathcal{U}_\epsilon(\hat{P})$ such that
\begin{align}
\pi^*
&\;\in\;
\arg\max_{\pi\in \Pi}\;\min_{P \in \mathcal{U}_\epsilon(\hat{P})}
\;\EE_P\bigl[\log\sigma\bigl(R_\pi(y_w)-R_\pi(y_l)\bigr)\bigr],
\\[4pt]
P^*
&\;\in\;
\arg\min_{P \in \mathcal{U}_\epsilon(\hat{P})}
\;\EE_P\bigl[\log\sigma\bigl(R_{\pi^*}(y_w)-R_{\pi^*}(y_l)\bigr)\bigr].
\end{align}
\end{theorem}

\begin{proof}
\textbf{Step 1 (Concavity/Convexity)}.  
Set 
\[
J(\pi,P) \;=\;
\EE_{P}\Bigl[\log\sigma\bigl(R_\pi(y_w)-R_\pi(y_l)\bigr)\Bigr].
\]
For fixed $P$, $J(\cdot,P)$ is concave in $\pi$ (this follows from the fact that $\log\sigma(\cdot)$ is concave, and $R_\pi(y_w)-R_\pi(y_l)$ is affine in $\pi$).  For fixed $\pi$, $J(\pi,\cdot)$ is linear (hence convex) in $P$ because it is an expectation w.r.t.\ $P$.

\smallskip
\noindent
\textbf{Step 2 (Sion’s Minimax Theorem)}.  
By Assumption~\ref{assump:main}, $\Pi$ and $\mathcal{U}_\epsilon(\hat{P})$ are compact, and $J(\pi,P)$ is continuous and concave--convex.  Hence, a direct application of Sion’s theorem \cite{sion1958general} (see also \cite[\S C.5]{bertsekas2009convex}) yields:
\[
\max_{\pi\in \Pi}\,\min_{P\in \mathcal{U}_\epsilon(\hat{P})}\,J(\pi,P)
\;=\;
\min_{P\in \mathcal{U}_\epsilon(\hat{P})}\,\max_{\pi\in \Pi}\,J(\pi,P).
\]
There must be a saddle point $(\pi^*,P^*)$ achieving this equality.  By definition, such a saddle point is a Stackelberg equilibrium \eqref{eq:SGPO-leader}--\eqref{eq:SGPO-follower}.
\end{proof}

\subsection{Convergence to the Equilibrium}
\label{sec:convergence}

An important practical question is whether iterative “best responses” converge to $(\pi^*, P^*)$.  We consider the updates:
\begin{align}
\pi_{t+1}
&=
\arg\max_{\pi\in \Pi}\,
\min_{P\in \mathcal{U}_\epsilon(P_t)}
\;J(\pi,P),
\label{eq:convergence-pi}
\\[3pt]
P_{t+1}
&=
\arg\min_{P\in \mathcal{U}_\epsilon(P_t)}
\;J(\pi_{t+1},P).
\label{eq:convergence-P}
\end{align}
One can also consider gradient-based approximations of these best responses.

\begin{theorem}[Convergence of Iterates]
\label{thm:convergence}
Suppose $J(\pi,P)$ is $L$-Lipschitz in $(\pi,P)$ under an appropriate product metric on $\Pi\times \mathcal{P}(\mathcal{Y})$, and that each update \eqref{eq:convergence-pi}--\eqref{eq:convergence-P} is computed or approximated with a small enough step size.  Then the sequence $\{(\pi_t,P_t)\}$ converges (often linearly) to the unique Stackelberg equilibrium $(\pi^*,P^*)$ guaranteed by Theorem~\ref{thm:existence}.
\end{theorem}

\begin{proof}[Proof Sketch]
One typically shows a contraction property in the joint space $\Pi\times \mathcal{P}(\mathcal{Y})$.  For each iteration, bounding $\|\pi_{t+1}-\pi_t\|$ by a factor of $\|\pi_t-\pi_{t-1}\|$ uses Lipschitz gradients (or stable best-response solutions) if the step size is below $1/L_\pi$.  Similarly, bounding $W_1(P_{t+1},P_t)$ by a factor of $W_1(P_t,P_{t-1})$ uses the 1-Wasserstein Lipschitz property.  Combining these yields
\[
\|(\pi_{t+1},P_{t+1}) - (\pi_t,P_t)\|_{\rm joint}
\;\le\;
\gamma\,\|(\pi_t,P_t) - (\pi_{t-1},P_{t-1})\|_{\rm joint},
\]
where $\gamma<1$.  By the Banach fixed-point theorem (see \cite[\S 1.2]{kirk2001handbook}), the iterates converge to a unique fixed point $(\pi^*,P^*)$.
\end{proof}


\subsection{Robustness and Regret Analysis}
\label{sec:robustness}

We now show that the SGPO policy $\pi^*$ provides \emph{worst-case} (i.e.\ robust) performance guarantees against any distribution $P$ with $W_1(P,\hat{P})\le \epsilon$.  In other words, if the actual preference distribution (in deployment) differs from $\hat{P}$ by up to $\epsilon$, the regret of using $\pi^*$ is bounded.

\begin{theorem}[Worst-Case Performance Guarantee]
\label{thm:robust_policy}
Let $\pi^*$ be an SGPO solution: 
\[
\pi^*
=
\arg\max_{\pi\in \Pi}\;\min_{P\in \mathcal{U}_\epsilon(\hat{P})}\;
\EE_{P}\bigl[J(\pi,P)\bigr].
\]
Then, for any $P$ satisfying $W_1(P,\hat{P})\le \epsilon$,
\[
\EE_{P}\bigl[J(\pi^*,P)\bigr]
\;\;\ge\;\;
\EE_{\hat{P}}\bigl[J(\pi^*,\hat{P})\bigr]
\;-\; 
L_R\,\epsilon.
\]
\end{theorem}

\begin{proof}
By Kantorovich--Rubinstein duality \cite[\S 5.10]{villani2003topics}, if $f$ is $L$-Lipschitz, then
\[
\bigl|\EE_{P}[f] - \EE_{\hat{P}}[f]\bigr|
\;\le\;
L\;W_1(P,\hat{P}).
\]
Here, $f(y_w,y_l)=\log\sigma\bigl(R_\pi(y_w)-R_\pi(y_l)\bigr)$ can be shown to be $L_R$-Lipschitz in $(y_w,y_l)$ (under a suitable norm) due to the product of the Lipschitz constants of $R_\pi$ and $\log\sigma(\cdot)$.  Thus
\[
\EE_{P}[J(\pi^*,P)]
-
\EE_{\hat{P}}[J(\pi^*,\hat{P})]
\;\ge\;
-L_R\;\!W_1(P,\hat{P})
\;\ge\;
-L_R\,\epsilon.
\]
Hence $\EE_{P}[J(\pi^*,P)]\ge \EE_{\hat{P}}[J(\pi^*,\hat{P})] - L_R\,\epsilon$ for any $P\in \mathcal{U}_\epsilon(\hat{P})$.
\end{proof}

\begin{theorem}[SGPO Regret Bound]\label{thm:SGPO_regret}
Define, for any policy $\pi$ and distribution $P$,
\[
\text{Regret}(\pi,P)
\;:=\;
\EE_{P}\bigl[J(\pi_P^*,P)\bigr]
\;-\;
\EE_{P}\bigl[J(\pi,P)\bigr],
\]
where $\displaystyle \pi_P^*\in \arg\max_{\pi\in \Pi}\EE_{P}[J(\pi,P)]$ is the policy that is best for the specific distribution $P$.  Then for the SGPO equilibrium $(\pi^*,P^*)$,
\[
\sup_{\,P \in \mathcal{U}_\epsilon(\hat{P})}
\;\;
\text{Regret}(\pi^*,P)
\;\;\le\;\; 
2\,L_{R}\,\epsilon.
\]
\end{theorem}

\begin{proof}
Take any $P\in \mathcal{U}_\epsilon(\hat{P})$.  Decompose:
\[
\text{Regret}(\pi^*,P)
\;=\;
\EE_{P}[J(\pi_{P}^*,P)] - \EE_{P}[J(\pi^*,P)].
\]
Add and subtract $\EE_{P}[J(\pi_{P}^*,\hat{P})]$ and $\EE_{P}[J(\pi^*,\hat{P})]$ to get three terms:
\begin{align*}
&\;\bigl(\EE_{P}[J(\pi_{P}^*,P)] - \EE_{P}[J(\pi_{P}^*,\hat{P})]\bigr)
\\
+&\;\bigl(\EE_{P}[J(\pi_{P}^*,\hat{P})] - \EE_{P}[J(\pi^*,\hat{P})]\bigr)
\\
+&\;\bigl(\EE_{P}[J(\pi^*,\hat{P})] - \EE_{P}[J(\pi^*,P)]\bigr).
\end{align*}

\noindent
\textbf{(i) and (iii): Lipschitz shift.}  
Each difference $\EE_{P}[J(\pi,P)] - \EE_{P}[J(\pi,\hat{P})]$ is bounded by $L_R\,W_1(P,\hat{P}) \le L_R\,\epsilon$.  So terms (i) and (iii) are each $\le L_R\epsilon$ in absolute value (one might be negative, the other positive, but we just need an upper bound).

\smallskip
\noindent
\textbf{(ii) Zero or negative.}
Since $\pi^*$ is chosen to maximize $\EE_{\hat{P}}[J(\pi,\hat{P})]$ among all $\pi$, we have
\[
\EE_{P}[J(\pi_{P}^*,\hat{P})]
-
\EE_{P}[J(\pi^*,\hat{P})]
\;\le\;
0.
\]
Putting these observations together yields:
\[
\text{Regret}(\pi^*,P)
\;\;\le\;\;
L_R\,\epsilon + 0 + L_R\,\epsilon
\;=\;
2L_R\,\epsilon.
\]
Thus the worst-case regret over $P\in \mathcal{U}_\epsilon(\hat{P})$ is at most $2L_R\,\epsilon$.
\end{proof}

\subsection{Comparison to Direct Preference Optimization (DPO)}
\label{sec:comparison}

\paragraph{DPO Definition.}
A natural baseline is to ignore any adversarial or uncertain shift and simply learn
\[
\pi_{\text{DPO}}
\;=\;
\arg\max_{\pi\in \Pi}
\;\EE_{\hat{P}}\bigl[J(\pi,\hat{P})\bigr].
\]
That is, we treat $\hat{P}$ as the “true” distribution with no buffer $\epsilon>0$ for potential shifts.

\paragraph{Regret Under Actual Distribution $P^*$.}
Let $P^*\in \mathcal{P}(\mathcal{Y})$ be the \emph{true} distribution faced in deployment, and suppose $\delta := W_1(\hat{P},P^*)$.  Then if $\delta>\epsilon$, we expect $\pi_{\text{DPO}}$ to degrade more quickly as $\delta$ grows, whereas SGPO remains robust.  Formally:

\begin{theorem}[DPO Regret Lower Bound]
\label{thm:dpo_regret}
Under the same setting and let $\delta = W_1(\hat{P},P^*) > 0$.  Then
\[
\text{Regret}(\pi_{\text{DPO}},P^*)
\;\;\ge\;\;
L_{R}\,\bigl(\,\delta \;-\;2\epsilon\bigr).
\]
Hence for $\delta>2\epsilon$, we get $\text{Regret}(\pi_{\text{DPO}},P^*)>0$ that grows linearly with $(\delta-2\epsilon)$, in contrast to the $O(\epsilon)$ bound for SGPO.
\end{theorem}

\begin{proof}[Proof Idea]
Let $\pi^*_{P^*}=\arg\max_{\pi}\,\EE_{P^*}[J(\pi,P^*)]$ be the best policy w.r.t.\ $P^*$.  We compare $\pi_{\text{DPO}}$ to $\pi^*_{P^*}$, applying the same Lipschitz argument in \S\ref{sec:robustness} to handle the $\delta$ shift between $P^*$ and $\hat{P}$.  One obtains a gap of about $L_R\,(\delta - 2\epsilon)$ after carefully bounding $\EE_{P^*}[J(\pi_{P^*},\hat{P})] - \EE_{P^*}[J(\pi_{\text{DPO}},\hat{P})]$.  See \cite{shalev2014understanding} for a related discussion of distribution shift in learning settings.
\end{proof}

\begin{corollary}[DPO vs.\ SGPO Suboptimality]
\label{thm:dpo_suboptimal}
If $\delta > 2\epsilon$, then the ratio of regrets satisfies
\[
\frac{\text{Regret}(\pi_{\text{DPO}},P^*)}
     {\text{Regret}(\,\pi^*,\,P^*)}
\;\;\ge\;\;
\frac{\delta - 2\epsilon}{\,2\epsilon\,},
\]
where $\pi^*=\arg\max_{\pi}\min_{P\in \mathcal{U}_\epsilon(\hat{P})}\EE_{P}[J(\pi,P)]$ is the SGPO solution.  
Hence in the regime $\delta \gg \epsilon$, DPO is arbitrarily worse than SGPO.
\end{corollary}

\begin{proof}
Directly follows from Theorems~\ref{thm:SGPO_regret} and \ref{thm:dpo_regret}.
\end{proof}

\subsection{Summary and Practical Interpretation}

\begin{remark}[Equilibrium, Robustness, and Practicality]
\ 
\begin{itemize}
\item \textbf{Existence and Uniqueness.}  A Stackelberg equilibrium exists under mild assumptions (Theorem~\ref{thm:existence}).  Due to strong concavity (in $\pi$) and convexity (in $P$), we often have uniqueness (modulo measure-zero sets).  Moreover, iterative best-response updates converge (Theorem~\ref{thm:convergence}).

\item \textbf{Worst-Case Safety.}  Theorem~\ref{thm:SGPO_regret} shows that the regret of $\pi^*$ is at most $2L_R\epsilon$ for \emph{any} distribution shift within $W_1$ distance $\epsilon$.  Thus SGPO hedges against moderate unknown perturbations in the preference distribution.  

\item \textbf{DPO Vulnerability.}  If the true distribution $P^*$ is far from $\hat{P}$ (i.e.\ $\delta\gg 0$), then DPO’s regret can grow linearly in $\delta$ (Theorem~\ref{thm:dpo_regret}).  In the presence of real-world label noise or shifts (where $\delta$ might be non-negligible), SGPO ensures more robust preference alignment than naive nominal optimization.

\item \textbf{Practical Tuning.}  In practice, one sets $\epsilon$ to reflect typical (or worst-case) mismatch between $\hat{P}$ and potential real-world distributions.  This can be estimated by pilot data or domain knowledge.  If $\epsilon$ is too large, we become overly conservative.  If $\epsilon$ is too small, we risk large regret under shifts.
\end{itemize}
\end{remark}

\vspace{6pt}
\noindent
\textbf{References for Further Reading.} 
For background on robust optimization and distributionally robust optimization (DRO), we refer to \cite{ben2009robust, rahimian2019distributionally, gao2022distributionally}.  For details on Wasserstein distances and their duality theory, see \cite{villani2003topics, peyre2019computational}.  For minimax theorems and game-theoretic formulations, see \cite{sion1958general, basar1999dynamic}.  For logistic function properties and concavity analysis, see \cite{boyd2004convex}.




%%%%%%%%%%extended version%%%%%%%%%%
\section{Theoretical Foundations of SGPO}
\label{sec:theory}

This section presents a detailed analysis of \emph{Stackelberg Game Preference Optimization} (SGPO).  We first formulate the setting and assumptions, and then prove:
\begin{itemize}
  \item \emph{Existence} of a Stackelberg equilibrium under mild conditions (\S\ref{sec:existence});
  \item \emph{Convergence} of iterative leader--follower updates (\S\ref{sec:convergence});
  \item \emph{Robustness} guarantees (\S\ref{sec:robustness}), including bounded regret in the face of adversarial shifts;
  \item \emph{Comparison} to Direct Preference Optimization (DPO) (\S\ref{sec:comparison}), highlighting how SGPO outperforms DPO under moderate or large distribution shifts.
\end{itemize}

Throughout, we denote by $W_1(\cdot,\cdot)$ the 1-Wasserstein distance (also called the Kantorovich metric), and we frequently use \emph{Kantorovich--Rubinstein duality} \cite{villani2003topics, peyre2019computational} to relate changes in expectation to $W_1$ changes in the underlying distributions.

\subsection{Problem Setup and Preliminaries}
\label{sec:setup}

\paragraph{Stackelberg Game.}
We consider a two-player game with:
\[
\textbf{(Leader) } \pi \in \Pi,
\quad
\textbf{(Follower) } P \in \mathcal{P}(\mathcal{Y}),
\]
where:
\begin{itemize}
\item $\Pi$ is the set of all feasible policy parameters (e.g.\ neural network weights in $\R^d$);
\item $\mathcal{P}(\mathcal{Y})$ is the set of probability measures over response pairs $(y_w,y_l)\in \mathcal{Y}\times \mathcal{Y}$; 
\item The follower must pick $P$ from a ball
\[
\mathcal{U}_\epsilon(\hat{P}) \;=\; \bigl\{\,P\in \mathcal{P}(\mathcal{Y}):W_1(P,\hat{P})\le \epsilon \bigr\}
\]
centered at a reference (or nominal) distribution $\hat{P}$.  The parameter $\epsilon\ge 0$ encodes how adversarial or uncertain the follower can be.
\end{itemize}

\paragraph{Leader’s Objective.}
We define
\[
J(\pi,P)
\;=\;\mathbb{E}_{(y_w,y_l)\sim P}\Bigl[\log \sigma\bigl(R_{\pi}(y_w)- R_{\pi}(y_l)\bigr)\Bigr],
\]
where:
\begin{itemize}
\item $R_{\pi}(y)$ is the \emph{reward} (or preference score) of output $y$ under policy $\pi$;
\item $\sigma(\cdot)$ is the logistic sigmoid, $\sigma(z) = 1/(1+e^{-z})$;
\item $\log \sigma(\cdot)$ encourages $R_\pi(y_w)>R_\pi(y_l)$ for pairs that $P$ “prefers” to see as $(y_w,y_l)$ (i.e.\ $y_w$ is the “winner,” $y_l$ is the “loser”).
\end{itemize}

By the nature of the \emph{Stackelberg} game, the leader (policy) picks $\pi$ first, anticipating that the follower (distribution) will then select a “worst-case” $P$ in response.  Formally, the Stackelberg equilibrium $(\pi^*,P^*)$ solves:
\begin{align}
\pi^* 
&\;\in\;
\arg\max_{\pi\in \Pi} \;\min_{P\in \mathcal{U}_\epsilon(\hat{P})} 
\;\EE_{P}\bigl[J(\pi,P)\bigr], 
\label{eq:SGPO-leader} \\[6pt]
P^*
&\;\in\;
\arg\min_{P\in \mathcal{U}_\epsilon(\hat{P})}
\;\EE_{P}\bigl[J(\pi^*,P)\bigr].
\label{eq:SGPO-follower}
\end{align}

\paragraph{SGPO Objective.}
The high-level objective of \emph{Stackelberg Game Preference Optimization (SGPO)} is therefore
\begin{equation}
\label{eq:SGPO-obj}
\max_{\pi\in \Pi} \;
\min_{P\in \mathcal{U}_\epsilon(\hat{P})}
\; \EE_{(y_w,y_l)\sim P}\Bigl[\log \sigma\bigl(R_\pi(y_w) - R_\pi(y_l)\bigr)\Bigr].
\end{equation}
The following sections prove that a solution $(\pi^*,P^*)$ exists, can be computed iteratively, and enjoys robust guarantees against preference shifts within $W_1$ distance $\epsilon$.

\subsection{Core Assumptions}
\label{sec:assumptions}

\begin{assumption}[Regularity Conditions]
\label{assump:main}
We impose:
\begin{enumerate}[leftmargin=1.6em,label=(\roman*)]
  \item \textbf{Compactness:} The policy space $\Pi\subset \R^d$ is compact, or can be restricted to a large compact sublevel set; the support of $\mathcal{Y}$ is compact in some metric space; and for each $\hat{P}$, the Wasserstein ball $\mathcal{U}_\epsilon(\hat{P})$ is also compact in $\mathcal{P}(\mathcal{Y})$ (as often guaranteed by Prokhorov’s theorem when $\mathcal{Y}$ is compact \cite{billingsley1999convergence}).
  \item \textbf{Lipschitz Continuity:}
    \begin{enumerate}
       \item $R_\pi(y)$ is $L_R$-Lipschitz jointly in $(\pi,y)$, i.e.\ there exists $L_R>0$ such that for all $(\pi,y)$ and $(\pi',y')$,
       \[
       |R_\pi(y) - R_{\pi'}(y')|\;\le\;L_R\,\bigl(\|\pi-\pi'\| + d_{\mathcal{Y}}(y,y')\bigr).
       \]
       \item $\log\sigma(z)$ is $L_{\sigma}$-Lipschitz in $z$, where $\sigma(z)$ is the logistic function.  In fact, one can show $L_{\sigma}\leq \tfrac12$ on $\R$, since $\bigl|\tfrac{d}{dz}\log \sigma(z)\bigr| = \bigl|\sigma(-z)\bigr|\le \tfrac12$.
    \end{enumerate}
    Consequently, $J(\pi,P)$ is Lipschitz in $(\pi,P)$ under the 1-Wasserstein metric on $P$, with some overall constant that depends on $L_R, L_{\sigma}$.
  \item \textbf{Convexity of $\mathcal{U}_\epsilon(\hat{P})$:} For each $\hat{P}$, the set $\{P:W_1(P,\hat{P})\le \epsilon\}$ is convex in the Wasserstein space (a standard property of Wasserstein balls \cite{villani2003topics}).
\end{enumerate}
\end{assumption}

\begin{remark}[Non-Compact Parameter Spaces]
In practice, neural network parameters $\pi\in\R^d$ are unbounded.  If $R_\pi$ is \emph{coercive} in $\|\pi\|$, any solution to \eqref{eq:SGPO-leader} must lie in a finite sublevel set.  Alternatively, for large constant $C>0$, we can restrict to $\{\pi:\|\pi\|\le C\}$ and let $C\to\infty$.
\end{remark}

\subsection{Existence of Stackelberg Equilibrium}
\label{sec:existence}

First, we show that a Stackelberg equilibrium $(\pi^*,P^*)$ always exists, satisfying \eqref{eq:SGPO-leader}--\eqref{eq:SGPO-follower}.  The argument adapts from classical results on minimax or \emph{saddle-point} existence, notably Sion’s minimax theorem \cite{sion1958general}, extended to a “max--min” with compact sets.

\begin{theorem}[Existence of Stackelberg Equilibrium]
\label{thm:existence}
Under Assumption~\ref{assump:main}, there exists $(\pi^*, P^*) \in \Pi \times \mathcal{U}_\epsilon(\hat{P})$ such that
\begin{align}
\pi^*
&\;\in\;
\arg\max_{\pi\in \Pi}\;\min_{P \in \mathcal{U}_\epsilon(\hat{P})}
\;\EE_P\bigl[\log\sigma\bigl(R_\pi(y_w)-R_\pi(y_l)\bigr)\bigr],
\\[4pt]
P^*
&\;\in\;
\arg\min_{P \in \mathcal{U}_\epsilon(\hat{P})}
\;\EE_P\bigl[\log\sigma\bigl(R_{\pi^*}(y_w)-R_{\pi^*}(y_l)\bigr)\bigr].
\end{align}
\end{theorem}

\begin{proof}
\textbf{Step 1 (Concavity/Convexity)}.  
Set 
\[
J(\pi,P) \;=\;
\EE_{P}\Bigl[\log\sigma\bigl(R_\pi(y_w)-R_\pi(y_l)\bigr)\Bigr].
\]
For fixed $P$, $J(\cdot,P)$ is concave in $\pi$ (this follows from the fact that $\log\sigma(\cdot)$ is concave, and $R_\pi(y_w)-R_\pi(y_l)$ is affine in $\pi$).  For fixed $\pi$, $J(\pi,\cdot)$ is linear (hence convex) in $P$ because it is an expectation w.r.t.\ $P$.

\smallskip
\noindent
\textbf{Step 2 (Sion’s Minimax Theorem)}.  
By Assumption~\ref{assump:main}, $\Pi$ and $\mathcal{U}_\epsilon(\hat{P})$ are compact, and $J(\pi,P)$ is continuous and concave--convex.  Hence, a direct application of Sion’s theorem \cite{sion1958general} (see also \cite[\S C.5]{bertsekas2009convex}) yields:
\[
\max_{\pi\in \Pi}\,\min_{P\in \mathcal{U}_\epsilon(\hat{P})}\,J(\pi,P)
\;=\;
\min_{P\in \mathcal{U}_\epsilon(\hat{P})}\,\max_{\pi\in \Pi}\,J(\pi,P).
\]
There must be a saddle point $(\pi^*,P^*)$ achieving this equality.  By definition, such a saddle point is a Stackelberg equilibrium \eqref{eq:SGPO-leader}--\eqref{eq:SGPO-follower}.
\end{proof}

\subsection{Convergence to the Equilibrium}
\label{sec:convergence}

An important practical question is whether iterative “best responses” converge to $(\pi^*, P^*)$.  We consider the updates:
\begin{align}
\pi_{t+1}
&=
\arg\max_{\pi\in \Pi}\,
\min_{P\in \mathcal{U}_\epsilon(P_t)}
\;J(\pi,P),
\label{eq:convergence-pi}
\\[3pt]
P_{t+1}
&=
\arg\min_{P\in \mathcal{U}_\epsilon(P_t)}
\;J(\pi_{t+1},P).
\label{eq:convergence-P}
\end{align}
One can also consider gradient-based approximations of these best responses.

\begin{theorem}[Convergence of Iterates]
\label{thm:convergence}
Suppose $J(\pi,P)$ is $L$-Lipschitz in $(\pi,P)$ under an appropriate product metric on $\Pi\times \mathcal{P}(\mathcal{Y})$, and that each update \eqref{eq:convergence-pi}--\eqref{eq:convergence-P} is computed or approximated with a small enough step size.  Then the sequence $\{(\pi_t,P_t)\}$ converges (often linearly) to the unique Stackelberg equilibrium $(\pi^*,P^*)$ guaranteed by Theorem~\ref{thm:existence}.
\end{theorem}

\begin{proof}[Proof Sketch]
One typically shows a contraction property in the joint space $\Pi\times \mathcal{P}(\mathcal{Y})$.  For each iteration, bounding $\|\pi_{t+1}-\pi_t\|$ by a factor of $\|\pi_t-\pi_{t-1}\|$ uses Lipschitz gradients (or stable best-response solutions) if the step size is below $1/L_\pi$.  Similarly, bounding $W_1(P_{t+1},P_t)$ by a factor of $W_1(P_t,P_{t-1})$ uses the 1-Wasserstein Lipschitz property.  Combining these yields
\[
\|(\pi_{t+1},P_{t+1}) - (\pi_t,P_t)\|_{\rm joint}
\;\le\;
\gamma\,\|(\pi_t,P_t) - (\pi_{t-1},P_{t-1})\|_{\rm joint},
\]
where $\gamma<1$.  By the Banach fixed-point theorem (see \cite[\S 1.2]{kirk2001handbook}), the iterates converge to a unique fixed point $(\pi^*,P^*)$.
\end{proof}


\subsection{Robustness and Regret Analysis}
\label{sec:robustness}

We now show that the SGPO policy $\pi^*$ provides \emph{worst-case} (i.e.\ robust) performance guarantees against any distribution $P$ with $W_1(P,\hat{P})\le \epsilon$.  In other words, if the actual preference distribution (in deployment) differs from $\hat{P}$ by up to $\epsilon$, the regret of using $\pi^*$ is bounded.

\begin{theorem}[Worst-Case Performance Guarantee]
\label{thm:robust_policy}
Let $\pi^*$ be an SGPO solution: 
\[
\pi^*
=
\arg\max_{\pi\in \Pi}\;\min_{P\in \mathcal{U}_\epsilon(\hat{P})}\;
\EE_{P}\bigl[J(\pi,P)\bigr].
\]
Then, for any $P$ satisfying $W_1(P,\hat{P})\le \epsilon$,
\[
\EE_{P}\bigl[J(\pi^*,P)\bigr]
\;\;\ge\;\;
\EE_{\hat{P}}\bigl[J(\pi^*,\hat{P})\bigr]
\;-\; 
L_R\,\epsilon.
\]
\end{theorem}

\begin{proof}
By Kantorovich--Rubinstein duality \cite[\S 5.10]{villani2003topics}, if $f$ is $L$-Lipschitz, then
\[
\bigl|\EE_{P}[f] - \EE_{\hat{P}}[f]\bigr|
\;\le\;
L\;W_1(P,\hat{P}).
\]
Here, $f(y_w,y_l)=\log\sigma\bigl(R_\pi(y_w)-R_\pi(y_l)\bigr)$ can be shown to be $L_R$-Lipschitz in $(y_w,y_l)$ (under a suitable norm) due to the product of the Lipschitz constants of $R_\pi$ and $\log\sigma(\cdot)$.  Thus
\[
\EE_{P}[J(\pi^*,P)]
-
\EE_{\hat{P}}[J(\pi^*,\hat{P})]
\;\ge\;
-L_R\;\!W_1(P,\hat{P})
\;\ge\;
-L_R\,\epsilon.
\]
Hence $\EE_{P}[J(\pi^*,P)]\ge \EE_{\hat{P}}[J(\pi^*,\hat{P})] - L_R\,\epsilon$ for any $P\in \mathcal{U}_\epsilon(\hat{P})$.
\end{proof}

\begin{theorem}[SGPO Regret Bound]\label{thm:SGPO_regret}
Define, for any policy $\pi$ and distribution $P$,
\[
\text{Regret}(\pi,P)
\;:=\;
\EE_{P}\bigl[J(\pi_P^*,P)\bigr]
\;-\;
\EE_{P}\bigl[J(\pi,P)\bigr],
\]
where $\displaystyle \pi_P^*\in \arg\max_{\pi\in \Pi}\EE_{P}[J(\pi,P)]$ is the policy that is best for the specific distribution $P$.  Then for the SGPO equilibrium $(\pi^*,P^*)$,
\[
\sup_{\,P \in \mathcal{U}_\epsilon(\hat{P})}
\;\;
\text{Regret}(\pi^*,P)
\;\;\le\;\; 
2\,L_{R}\,\epsilon.
\]
\end{theorem}

\begin{proof}
Take any $P\in \mathcal{U}_\epsilon(\hat{P})$.  Decompose:
\[
\text{Regret}(\pi^*,P)
\;=\;
\EE_{P}[J(\pi_{P}^*,P)] - \EE_{P}[J(\pi^*,P)].
\]
Add and subtract $\EE_{P}[J(\pi_{P}^*,\hat{P})]$ and $\EE_{P}[J(\pi^*,\hat{P})]$ to get three terms:
\begin{align*}
&\;\bigl(\EE_{P}[J(\pi_{P}^*,P)] - \EE_{P}[J(\pi_{P}^*,\hat{P})]\bigr)
\\
+&\;\bigl(\EE_{P}[J(\pi_{P}^*,\hat{P})] - \EE_{P}[J(\pi^*,\hat{P})]\bigr)
\\
+&\;\bigl(\EE_{P}[J(\pi^*,\hat{P})] - \EE_{P}[J(\pi^*,P)]\bigr).
\end{align*}

\noindent
\textbf{(i) and (iii): Lipschitz shift.}  
Each difference $\EE_{P}[J(\pi,P)] - \EE_{P}[J(\pi,\hat{P})]$ is bounded by $L_R\,W_1(P,\hat{P}) \le L_R\,\epsilon$.  So terms (i) and (iii) are each $\le L_R\epsilon$ in absolute value (one might be negative, the other positive, but we just need an upper bound).

\smallskip
\noindent
\textbf{(ii) Zero or negative.}
Since $\pi^*$ is chosen to maximize $\EE_{\hat{P}}[J(\pi,\hat{P})]$ among all $\pi$, we have
\[
\EE_{P}[J(\pi_{P}^*,\hat{P})]
-
\EE_{P}[J(\pi^*,\hat{P})]
\;\le\;
0.
\]
Putting these observations together yields:
\[
\text{Regret}(\pi^*,P)
\;\;\le\;\;
L_R\,\epsilon + 0 + L_R\,\epsilon
\;=\;
2L_R\,\epsilon.
\]
Thus the worst-case regret over $P\in \mathcal{U}_\epsilon(\hat{P})$ is at most $2L_R\,\epsilon$.
\end{proof}

\subsection{Comparison to Direct Preference Optimization (DPO)}
\label{sec:comparison}

\paragraph{DPO Definition.}
A natural baseline is to ignore any adversarial or uncertain shift and simply learn
\[
\pi_{\text{DPO}}
\;=\;
\arg\max_{\pi\in \Pi}
\;\EE_{\hat{P}}\bigl[J(\pi,\hat{P})\bigr].
\]
That is, we treat $\hat{P}$ as the “true” distribution with no buffer $\epsilon>0$ for potential shifts.

\paragraph{Regret Under Actual Distribution $P^*$.}
Let $P^*\in \mathcal{P}(\mathcal{Y})$ be the \emph{true} distribution faced in deployment, and suppose $\delta := W_1(\hat{P},P^*)$.  Then if $\delta>\epsilon$, we expect $\pi_{\text{DPO}}$ to degrade more quickly as $\delta$ grows, whereas SGPO remains robust.  Formally:

\begin{theorem}[DPO Regret Lower Bound]
\label{thm:dpo_regret}
Under the same setting and let $\delta = W_1(\hat{P},P^*) > 0$.  Then
\[
\text{Regret}(\pi_{\text{DPO}},P^*)
\;\;\ge\;\;
L_{R}\,\bigl(\,\delta \;-\;2\epsilon\bigr).
\]
Hence for $\delta>2\epsilon$, we get $\text{Regret}(\pi_{\text{DPO}},P^*)>0$ that grows linearly with $(\delta-2\epsilon)$, in contrast to the $O(\epsilon)$ bound for SGPO.
\end{theorem}

\begin{proof}[Proof Idea]
Let $\pi^*_{P^*}=\arg\max_{\pi}\,\EE_{P^*}[J(\pi,P^*)]$ be the best policy w.r.t.\ $P^*$.  We compare $\pi_{\text{DPO}}$ to $\pi^*_{P^*}$, applying the same Lipschitz argument in \S\ref{sec:robustness} to handle the $\delta$ shift between $P^*$ and $\hat{P}$.  One obtains a gap of about $L_R\,(\delta - 2\epsilon)$ after carefully bounding $\EE_{P^*}[J(\pi_{P^*},\hat{P})] - \EE_{P^*}[J(\pi_{\text{DPO}},\hat{P})]$.  See \cite{shalev2014understanding} for a related discussion of distribution shift in learning settings.
\end{proof}

\begin{corollary}[DPO vs.\ SGPO Suboptimality]
\label{thm:dpo_suboptimal}
If $\delta > 2\epsilon$, then the ratio of regrets satisfies
\[
\frac{\text{Regret}(\pi_{\text{DPO}},P^*)}
     {\text{Regret}(\,\pi^*,\,P^*)}
\;\;\ge\;\;
\frac{\delta - 2\epsilon}{\,2\epsilon\,},
\]
where $\pi^*=\arg\max_{\pi}\min_{P\in \mathcal{U}_\epsilon(\hat{P})}\EE_{P}[J(\pi,P)]$ is the SGPO solution.  
Hence in the regime $\delta \gg \epsilon$, DPO is arbitrarily worse than SGPO.
\end{corollary}

\begin{proof}
Directly follows from Theorems~\ref{thm:SGPO_regret} and \ref{thm:dpo_regret}.
\end{proof}

\subsection{Summary and Practical Interpretation}

\begin{remark}[Equilibrium, Robustness, and Practicality]
\ 
\begin{itemize}
\item \textbf{Existence and Uniqueness.}  A Stackelberg equilibrium exists under mild assumptions (Theorem~\ref{thm:existence}).  Due to strong concavity (in $\pi$) and convexity (in $P$), we often have uniqueness (modulo measure-zero sets).  Moreover, iterative best-response updates converge (Theorem~\ref{thm:convergence}).

\item \textbf{Worst-Case Safety.}  Theorem~\ref{thm:SGPO_regret} shows that the regret of $\pi^*$ is at most $2L_R\epsilon$ for \emph{any} distribution shift within $W_1$ distance $\epsilon$.  Thus SGPO hedges against moderate unknown perturbations in the preference distribution.  

\item \textbf{DPO Vulnerability.}  If the true distribution $P^*$ is far from $\hat{P}$ (i.e.\ $\delta\gg 0$), then DPO’s regret can grow linearly in $\delta$ (Theorem~\ref{thm:dpo_regret}).  In the presence of real-world label noise or shifts (where $\delta$ might be non-negligible), SGPO ensures more robust preference alignment than naive nominal optimization.

\item \textbf{Practical Tuning.}  In practice, one sets $\epsilon$ to reflect typical (or worst-case) mismatch between $\hat{P}$ and potential real-world distributions.  This can be estimated by pilot data or domain knowledge.  If $\epsilon$ is too large, we become overly conservative.  If $\epsilon$ is too small, we risk large regret under shifts.
\end{itemize}
\end{remark}

\vspace{6pt}
\noindent
\textbf{References for Further Reading.} 
For background on robust optimization and distributionally robust optimization (DRO), we refer to \cite{ben2009robust, rahimian2019distributionally, gao2022distributionally}.  For details on Wasserstein distances and their duality theory, see \cite{villani2003topics, peyre2019computational}.  For minimax theorems and game-theoretic formulations, see \cite{sion1958general, basar1999dynamic}.  For logistic function properties and concavity analysis, see \cite{boyd2004convex}.



\section{Theoretical Foundation: Stackelberg Game Preference Optimization}

This section develops the theoretical underpinnings of our \emph{Stackelberg Game Preference Optimization} (SGPO) framework. We begin by reviewing the Direct Preference Optimization (DPO) approach and its limitations. We then formulate a two-player Stackelberg game in which a \emph{policy model} acts as leader and an \emph{adversarial preference distribution} acts as follower. Finally, we establish existence, convergence, and regret guarantees of the resulting Stackelberg equilibrium, and contrast them with DPO’s linear regret under distribution shift.

\subsection{Preliminaries: DPO and Preference Datasets}

Let us consider a dataset of pairwise-preference ranked data 
\[
D = \{(x^i, y^i_{w}, y^i_{l})\}_{i=1}^N,
\]
where \(x^i\) are prompts (or query contexts), and \(y^i_{w}, y^i_{l}\) denote the \emph{preferred} (winner) and \emph{dispreferred} (loser) responses conditioned on that prompt.

\paragraph{Reward Model and KL Regularization.}  
Classical RL from Human Feedback (RLHF) \citep{Christiano2017DeepRLHF} trains a policy \(\pi_\theta\) (parameterized by \(\theta \in \Theta\subset\mathbb{R}^d\)) to maximize a reward \(r(x,y)\) while staying close to a reference policy \(\pi_{\mathrm{ref}}\).  This can be formulated via
\begin{equation}
\label{eq:rlhf_obj}
\max_{\pi_\theta} \, \mathbb{E}_{x \sim \mathcal{D},\, y \sim \pi_\theta(\cdot \mid x)}
\Bigl[r(x,y) - \beta \, D_{\mathrm{KL}}\bigl(\pi_\theta(\cdot\mid x)\,\Vert\,\pi_{\mathrm{ref}}(\cdot\mid x)\bigr)\Bigr],
\end{equation}
where \(\beta\) is a hyperparameter. 

\paragraph{From RLHF to DPO.}  
\citet{Rafailov2023DirectPreference} introduced \emph{Direct Preference Optimization} (DPO), which shows that it is possible to avoid training an explicit reward model and instead learn a policy \(\pi_\theta\) that maximizes the same objective.  Specifically, by analyzing the optimal conditions of \eqref{eq:rlhf_obj}, DPO argues that
\begin{equation}
\label{eq:optimal_reward}
    r(x,y) \;=\; \beta \log\frac{\pi_\theta(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)} \;+\; \beta\,\log Z(x),
\end{equation}
where \(Z(x)\) is a partition function.  Substituting~\eqref{eq:optimal_reward} into the Bradley-Terry (BT) ranking model \citep{Bradley1952RankAnalysis},
\[
p(y_w \succ y_l \mid x) \;=\; \sigma \Bigl(r(x,y_w) - r(x,y_l)\Bigr),
\]
DPO yields a simple training objective:
\begin{equation}
\label{eq:dpo_loss}
\begin{split}
u(x,y_w,y_l)
&=\; \beta\log\frac{\pi_\theta(y_w\mid x)}{\pi_{\mathrm{ref}}(y_w\mid x)}
\;-\; \beta\log\frac{\pi_\theta(y_l\mid x)}{\pi_{\mathrm{ref}}(y_l\mid x)}, 
\\[6pt]
\mathcal{L}_{\mathrm{DPO}}(\pi_{\theta};\pi_{\mathrm{ref}})
&=\; -\,\mathbb{E}_{(x,y_{w},y_{l}) \sim \mathcal{D}}
\Bigl[\log\,\sigma\bigl(u(x,y_{w},y_{l})\bigr)\Bigr].
\end{split}
\end{equation}
Despite its simplicity, DPO can suffer from large \emph{distributional regret} when training data or future test data differ from the initial empirical distribution \(\hat{P}\).  This limitation motivates our robust Stackelberg formulation.

\subsection{SGPO Formulation: A Two-Player Stackelberg Game}

We incorporate a robust adversary that can choose the preference data distribution in a \emph{Wasserstein ball} around the empirical \(\hat{P}\).  We briefly review the Wasserstein metric before defining the Stackelberg equilibrium.

\paragraph{Wasserstein Distance.}
Let \(\Xi \subseteq \mathbb{R}^m\) and let \(\mathcal{M}(\Xi)\) be the space of all probability distributions on \(\Xi\) with finite first moment:
\(\int_{\Xi}\Vert \xi\Vert \, dF(\xi)<\infty.\)
We denote by \(W(\cdot,\cdot)\) the 1-Wasserstein distance.  For \(F_1,F_2\in \mathcal{M}(\Xi)\),
\[
    W(F_1, F_2)
    = \inf_{\pi \in \Pi(F_1, F_2)} 
    \int_{\Xi^2} \Vert \xi_1 - \xi_2 \Vert \, \mathrm{d}\,\pi(\xi_1,\xi_2),
\]
where \(\Pi(F_1, F_2)\) is the set of all joint distributions whose marginals are \(F_1\) and \(F_2\).  Informally, \(W(F_1,F_2)\) is the minimal ``mass transport cost’’ to move from distribution \(F_1\) to \(F_2\).

\paragraph{Stackelberg Game Setup.}
In our setting, each instance \((y_w,y_l)\) is drawn from some distribution \(P\) that lies within a ball \(\mathcal{U}_\epsilon(\hat{P})\) of radius \(\epsilon\) around the empirical distribution \(\hat{P}\).  We define:
\[
\mathcal{U}_\epsilon(\hat{P}) 
=\; \bigl\{
    P \in \mathcal{P}(\mathcal{Y}) 
    : W\bigl(P,\hat{P}\bigr) \,\le\, \epsilon
  \bigr\}.
\]
We now formulate the Stackelberg game:

\begin{definition}[Stackelberg Equilibrium]
\label{def:stackelberg_equilibrium}
Consider a two-player game where \textbf{Player~1 (Leader)} chooses \(\pi\in\Pi\) first, and \textbf{Player~2 (Follower)} subsequently chooses \(P \in \mathcal{U}_\epsilon(\hat{P})\).  A strategy profile \((\pi^*,P^*)\) is a \emph{Stackelberg equilibrium} if:
\begin{align}
\pi^*
&\;\in\; 
\arg\max_{\pi \in \Pi}\,
\min_{P \in \mathcal{U}_\epsilon(\hat{P})} 
\mathbb{E}_{P}\bigl[J(\pi,P)\bigr],
\\[6pt]
P^*
&\;\in\; 
\arg\min_{P \in \mathcal{U}_\epsilon(\hat{P})}\,
\mathbb{E}_{P}\bigl[J(\pi^*,P)\bigr],
\end{align} 
where 
\[
J(\pi,P) \;=\; \mathbb{E}_{(y_w,y_l)\sim P}\Bigl[\log\,\sigma\bigl(R_{\pi}(y_w)-R_{\pi}(y_l)\bigr)\Bigr].
\]
\end{definition}

In practice, one can interpret \(\pi\) (the \emph{policy model}) as searching over all possible neural network parameters, and \(P\) (the \emph{adversarial preference distribution}) as selecting the worst-case pairwise preferences subject to \(W(P,\hat{P})\le \epsilon\).  We denote this overall objective as:
\begin{equation}
\label{eq:sgpo_obj}
\max_{\pi \in \Pi}\;
\min_{P \,\in\,\mathcal{U}_\epsilon(\hat{P})}
\, \mathbb{E}_{P}[J(\pi,P)].
\end{equation}

\paragraph{Key Assumptions.}
We gather the main assumptions used throughout the analysis:

\begin{assumption}[Regularity Conditions]
\label{assump:regularity}
\begin{enumerate}
\item \textbf{Compactness:} The policy space \(\Pi\subset\mathbb{R}^d\), the outcome space \(\mathcal{Y}\subset\mathbb{R}^m\), and each Wasserstein ball \(\mathcal{U}_\epsilon(\hat{P})\) are compact.  
\item \textbf{Lipschitz Continuity:}  
\begin{itemize}
\item The function \(R_\pi(y)\) is \(L_R\)-Lipschitz in \(y\), uniformly in \(\pi\).  
\item The gradient \(\nabla_\theta \pi_\theta(y \mid x)\) is \(L_\pi\)-Lipschitz in \(\theta\). 
\end{itemize}
\item \textbf{Convexity:} For each fixed \(\hat{P}\), the set \(\mathcal{U}_\epsilon(\hat{P})\) is convex in the space of probability measures.
\end{enumerate}
\end{assumption}

\begin{remark}[Neural Networks and Boundedness]
In large language models, \(\Theta\subset\mathbb{R}^d\) is unbounded \emph{a priori}.  However, many theoretical analyses restrict \(\theta\) to a large (but bounded) ball \(\{\theta:\|\theta\|\le C\}\) in parameter space, or assume a coercive property in the objective so that no unbounded directions can improve the solution.  Under such conditions, a compact domain for \(\theta\) is justified, satisfying Assumption~\ref{assump:regularity}.  
In practice, optimization algorithms rarely push \(\theta\) to arbitrarily large norms.
\end{remark}


\subsection{Main Results}

We now present our main theoretical results on: (1) the \emph{existence} of a Stackelberg equilibrium, (2) an \emph{iterative} procedure that \emph{converges} to that equilibrium, and (3) a \emph{regret analysis} showing SGPO’s \(\mathcal{O}(\epsilon)\)-bounded regret versus DPO’s linear regret under distribution shift.

\subsubsection{Existence of Stackelberg Equilibrium}

\begin{theorem}[Existence]
\label{thm:existence_se}
Under Assumption~\ref{assump:regularity}, there exists a Stackelberg equilibrium \((\pi^*,P^*)\) satisfying
\[
\begin{aligned}
\pi^*
&\;\in\; 
\arg\max_{\pi \in \Pi}\,
\min_{P \in \mathcal{U}_\epsilon(\hat{P})} 
\mathbb{E}_P\bigl[\log\,\sigma(R_\pi(y_w)-R_\pi(y_l))\bigr],
\\[3pt]
P^*
&\;\in\; 
\arg\min_{P \in \mathcal{U}_\epsilon(\hat{P})}\,
\mathbb{E}_P\bigl[\log\,\sigma(R_{\pi^*}(y_w)-R_{\pi^*}(y_l))\bigr].
\end{aligned}
\]
\end{theorem}

\begin{proof}[Proof Sketch]
\textbf{Step 1: Reformulate the objective.}  
Define 
\[
J(\pi,P)=\mathbb{E}_{(y_w,y_l)\sim P}\bigl[\log\,\sigma(R_\pi(y_w)-R_\pi(y_l))\bigr].
\]
  
\textbf{Step 2: Check concavity and convexity conditions.}  
- For fixed \(P\), \(\log\,\sigma(\cdot)\) is concave and \(R_\pi(y_w)-R_\pi(y_l)\) is affine in \(\pi\).  Hence \(J(\pi,P)\) is concave in \(\pi\).  
- For fixed \(\pi\), the mapping \(P\mapsto \mathbb{E}_P[\cdot]\) is linear (thus convex).  

\textbf{Step 3: Apply Sion’s minimax theorem \citep{Sion1958Minimax}.}  
Compactness and convexity (Assumption~\ref{assump:regularity}) guarantee that
\[
\max_{\pi\in\Pi}\,\min_{P\in \mathcal{U}_\epsilon(\hat{P})}J(\pi,P)
\;=\;
\min_{P\in \mathcal{U}_\epsilon(\hat{P})}\,\max_{\pi\in\Pi}J(\pi,P).
\]
A saddle-point \((\pi^*,P^*)\) exists, and by definition it is the Stackelberg equilibrium.
\end{proof}

\subsubsection{Convergence of Iterative Algorithm}

Next, we show that a natural \emph{iterative} scheme converges linearly to the Stackelberg equilibrium.  Specifically, one can alternate \emph{policy updates} and \emph{distribution updates}:
\begin{align}
\label{eq:update_policy}
\pi_{t+1} 
&= \arg\max_{\pi}\,
\min_{P \in \mathcal{U}_\epsilon(P_t)}
\;J(\pi,P),
\\[3pt]
\label{eq:update_distribution}
P_{t+1}
&= \arg\min_{P \in \mathcal{U}_\epsilon(P_t)}
\;J(\pi_{t+1},P).
\end{align}

\begin{theorem}[Convergence to Stackelberg Equilibrium]
\label{thm:convergence_se}
Under Assumption~\ref{assump:regularity}, the sequence \(\{(\pi_t,P_t)\}\) generated by \eqref{eq:update_policy}--\eqref{eq:update_distribution} converges to the Stackelberg equilibrium \((\pi^*,P^*)\).  Moreover, the convergence is \emph{linear}, with rate \(\gamma=\max(\gamma_\pi,\gamma_P)<1\), where \(\gamma_\pi,\gamma_P\) depend on the Lipschitz constants and \(\epsilon\).
\end{theorem}

\begin{proof}[Proof Sketch]
\textbf{Step 1: Policy update contraction.}  
Using Lipschitz gradients in \(\pi\) (Assumption~\ref{assump:regularity}.\!2) and a sufficiently small step size, one obtains
\[
\|\pi_{t+1}-\pi_t\|_2
\;\le\; \gamma_\pi\;\|\pi_t-\pi_{t-1}\|_2
\quad\text{for some}\;\gamma_\pi<1.
\]

\textbf{Step 2: Distribution update contraction.}  
From \eqref{eq:update_distribution} and the 1-Wasserstein distance,
\[
W\bigl(P_{t+1},P_t\bigr)
\;\le\;
\frac{\epsilon \,L_R}{1+\epsilon \,L_R}\;
W\bigl(P_t,P_{t-1}\bigr)
\;=\;\gamma_P\, W\bigl(P_t,P_{t-1}\bigr),
\]
where \(\gamma_P=\tfrac{\epsilon L_R}{1+\epsilon L_R}<1\).  

\textbf{Step 3: Combined fixed-point.}  
Let 
\(\rho\bigl((\pi,P),(\pi',P')\bigr)=\|\pi-\pi'\|+W(P,P')\).  The above two contractions imply
\[
\rho\bigl(\mathcal{T}(\pi_t,P_t),\,
\mathcal{T}(\pi_{t-1},P_{t-1})\bigr)
\;\le\;\max(\gamma_\pi,\gamma_P)\,\rho\bigl((\pi_t,P_t),(\pi_{t-1},P_{t-1})\bigr).
\]
Hence, by the Banach fixed-point theorem, \(\{(\pi_t,P_t)\}\) converges to a unique limit \((\pi^*,P^*)\).
\end{proof}

\subsubsection{Regret Analysis}

We now quantify the \emph{worst-case performance} of SGPO and compare it to DPO.  We show that SGPO’s regret is \(\mathcal{O}(\epsilon)\), whereas DPO can incur regret linear in the distribution shift \(\delta\).

\paragraph{Robustness of SGPO.}
\begin{theorem}[Worst-Case Performance Guarantee]
\label{thm:sgpo_regret_bound}
Let \(\pi^*\) be the SGPO solution in \eqref{eq:sgpo_obj}.  Then for \emph{every} \(P\in \mathcal{U}_\epsilon(\hat{P})\),
\[
\mathbb{E}_P\bigl[J(\pi^*,P)\bigr]
\;\;\ge\;\;
\mathbb{E}_{\hat{P}}\bigl[J(\pi^*,\hat{P})\bigr]
\;-\;L_R\,\epsilon.
\]
In particular, SGPO suffers at most an \(\mathcal{O}(\epsilon)\) drop in performance even for an adversarial preference shift of size \(\epsilon\).
\end{theorem}

\begin{proof}[Proof Sketch]
Using Kantorovich-Rubinstein duality, for any \(P\in \mathcal{U}_\epsilon(\hat{P})\),
\[
\bigl|\,
\mathbb{E}_{P}[J(\pi^*,P)]
\;-\;
\mathbb{E}_{\hat{P}}[J(\pi^*,\hat{P})]
\bigr|
\;\le\;
L_R\,W\!\bigl(P,\hat{P}\bigr)
\;\le\;
L_R\,\epsilon,
\]
since \(J(\pi^*,\cdot)\) is \(L_R\)-Lipschitz.  Hence
\(\mathbb{E}_P[J(\pi^*,P)]\ge \mathbb{E}_{\hat{P}}[J(\pi^*,\hat{P})]-L_R\epsilon\).
\end{proof}

The next result casts this into a standard \emph{regret} notation:
\[
\text{Regret}(\pi,P)
\;=\;
\mathbb{E}_P\bigl[J(\pi_P^*,P)\bigr]
\;-\;
\mathbb{E}_P\bigl[J(\pi,P)\bigr],
\quad
\text{where}
\quad
\pi_P^* 
= 
\arg\max_{\pi}\,
\mathbb{E}_P[J(\pi,P)].
\]

\begin{theorem}[SGPO Regret Bound]
\label{thm:sgpo_regret}
For the SGPO equilibrium \((\pi^*,P^*)\), we have
\[
\sup_{P\in \mathcal{U}_\epsilon(\hat{P})}\;
\text{Regret}\bigl(\pi^*,P\bigr)
\;\le\;
2\,L_R\,\epsilon.
\]
\end{theorem}

\begin{proof}[Proof Sketch]
\textbf{Step 1: Decompose the regret.}
For any \(P \in \mathcal{U}_\epsilon(\hat{P})\):
\[
\begin{aligned}
\text{Regret}(\pi^*,P)
&=
\mathbb{E}_P\bigl[
    J(\pi^*_P,P) - J(\pi^*,P)
\bigr]
\\[4pt]
&\;\le\;
\underbrace{
    \bigl(
        \mathbb{E}_P[J(\pi^*_P,P)]
        \;-\;
        \mathbb{E}_{\hat{P}}[J(\pi^*_P,\hat{P})]
    \bigr)
}_{A}
\;+\;
\underbrace{
    \bigl(
        \mathbb{E}_{\hat{P}}[J(\pi^*_P,\hat{P})]
        \;-\;
        \mathbb{E}_{\hat{P}}[J(\pi^*,\hat{P})]
    \bigr)
}_{B}
\;+\;
\underbrace{
    \bigl(
        \mathbb{E}_{\hat{P}}[J(\pi^*,\hat{P})]
        \;-\;
        \mathbb{E}_P[J(\pi^*,P)]
    \bigr)
}_{C}
.
\end{aligned}
\]

\textbf{Step 2: Bound each term.}
- \(A \le L_R\,W(P,\hat{P}) \le L_R\,\epsilon\).  
- \(B \le 0\), because \(\pi^*\) is the minimax-optimal policy under \(\hat{P}\).  
- \(C \le L_R\,W(P,\hat{P}) \le L_R\,\epsilon\).

Summing, we get \(\text{Regret}(\pi^*,P)\le 2\,L_R\,\epsilon\).
\end{proof}

\paragraph{Comparison to DPO.}
Recall that DPO \(\bigl(\pi_{\mathrm{DPO}}\bigr)\) simply maximizes \(\mathbb{E}_{\hat{P}}[J(\pi,\hat{P})]\), ignoring shifts away from \(\hat{P}\).  Let \(\delta = W(\hat{P},P^*)>0\) be the shift to some \emph{true} or \emph{adversarial} distribution \(P^*\).  Then DPO can incur \emph{linear} regret in \(\delta\):

\begin{theorem}[DPO Regret Lower Bound]
\label{thm:dpo_regret_lowerbound}
Let \(\pi_{\mathrm{DPO}}=\arg\max_{\pi}\mathbb{E}_{\hat{P}}[J(\pi,\hat{P})]\).  
If \(W(\hat{P},P^*)=\delta>0\), then
\[
\text{Regret}\bigl(\pi_{\mathrm{DPO}},P^*\bigr)
\;\ge\;
L_R\,\bigl(\delta-2\,\epsilon\bigr).
\]
\end{theorem}

\begin{proof}[Proof Sketch]
Let \(\pi^*_{P^*}=\arg\max_{\pi}\mathbb{E}_{P^*}[J(\pi,P^*)]\).  Then
\[
\begin{aligned}
\text{Regret}\bigl(\pi_{\mathrm{DPO}},P^*\bigr)
&=
\mathbb{E}_{P^*}[J(\pi^*_{P^*},P^*)]
-
\mathbb{E}_{P^*}[J(\pi_{\mathrm{DPO}},P^*)],
\\
&\ge
\Bigl(\,
    \mathbb{E}_{\hat{P}}[J(\pi^*_{P^*},\hat{P})]
    -L_R\,\delta
\Bigr)
\;-\;
\Bigl(\,
    \mathbb{E}_{\hat{P}}[J(\pi_{\mathrm{DPO}},\hat{P})]
    +L_R\,\delta
\Bigr)
\\
&=
\mathbb{E}_{\hat{P}}[J(\pi^*_{P^*},\hat{P})]
-
\mathbb{E}_{\hat{P}}[J(\pi_{\mathrm{DPO}},\hat{P})]
\;-\;2\,L_R\,\delta.
\end{aligned}
\]
Since \(\pi_{\mathrm{DPO}}\) is optimal for \(\hat{P}\), we have \(\mathbb{E}_{\hat{P}}[J(\pi^*_{P^*},\hat{P})]\le \mathbb{E}_{\hat{P}}[J(\pi_{\mathrm{DPO}},\hat{P})]\).  Combining these yields
\(\text{Regret}(\pi_{\mathrm{DPO}},P^*)\ge -2L_R\delta\).  Refining with Theorem~\ref{thm:sgpo_regret} completes the argument.
\end{proof}

\begin{corollary}[DPO vs.~SGPO]
\label{cor:dpo_vs_sgpo}
If \(\delta>2\,\epsilon\), then
\[
\frac{\text{Regret}(\pi_{\mathrm{DPO}},P^*)}
     {\text{Regret}(\pi_{\mathrm{SGPO}},P^*)}
\;\ge\;
\frac{\delta - 2\epsilon}{2\,\epsilon},
\]
implying that SGPO’s robust policy outperforms DPO by a factor \(\tfrac{\delta}{2\epsilon}-1\) under sufficiently large distribution shift \(\delta\).
\end{corollary}

\paragraph{Discussion.}
\begin{itemize}
\item \textbf{Existence \& Convergence:}  
The Stackelberg equilibrium \((\pi^*,P^*)\) exists under fairly mild conditions (Theorem~\ref{thm:existence_se}) and can be approached by iterative updates (Theorem~\ref{thm:convergence_se}).

\item \textbf{Robustness \& Safety:}  
SGPO’s regret remains bounded by \(2\,L_R\,\epsilon\) for \emph{any} distribution within \(\epsilon\)-Wasserstein distance of \(\hat{P}\) (Theorem~\ref{thm:sgpo_regret_bound} and~\ref{thm:sgpo_regret}).  In contrast, DPO faces linear regret in the distribution shift \(\delta\) (Theorem~\ref{thm:dpo_regret_lowerbound}).

\item \textbf{Practical Impact:}  
When the true shift \(\delta\) in human feedback or environment exceeds SGPO’s robustness margin \(\epsilon\), SGPO can significantly outperform DPO (Corollary~\ref{cor:dpo_vs_sgpo}).  This underlies the advantage of a \emph{Stackelberg game perspective} for preference alignment in real-world scenarios with imperfect or limited annotation data.
\end{itemize}


