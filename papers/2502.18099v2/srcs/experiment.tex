\section{Experiments}
\label{sec:experiments}
In this section, we present an extensive empirical evaluation of our proposed \emph{Stackelberg Self-Annotated Preference Optimization} (SSAPO) algorithm.
\subsection{Experiment Setup}
\paragraph{Datasets.}
For the preference learning dataset, we employed the UltraFeedback dataset~\cite{Cui2023Ultrafeedback}\footnote{\href{https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned}{argilla/ultrafeedback-binarized-preferences-cleaned}}, aligning with prior research~\cite{Rosset2024Direct, Kim2025Spread}. Specifically, we first extracted a seed dataset comprising 2K samples (3.3\% of the total 60K training samples), which included prompts, responses, and ground-truth preference labels. These ground-truth preference labels are referred to as gold labels in Table~\ref{tab:main_results}. The remaining training samples were then partitioned into three subsets of 8K, 20K, and 30K samples, retaining only the prompts. These subsets were utilized as the prompt sets for the 1st, 2nd, and 3rd iteration stages, respectively.

\paragraph{Models.}
Following previous work~\cite{Kim2025Spread}, we mainly conduct our experiments using the supervised fine-tuned Mistral-7B-0.1 model~\cite{jiang2023mistral} as the initial model $\pi_{init}$. Specifically, we use the open-sourced model\footnote{\href{https://huggingface.co/alignment-handbook/zephyr-7b-sft-full}{alignment-handbook/zephyr-7b-sft-full}} that follows the recipe of Zephyr~\cite{tunstall2023zephyr} and fine-tuned on the instructions of Ultrachat~\cite{ding2023enhancing}. In Table~\ref{tab:main_results}, we also use LLaMA-3-8B\footnote{\href{https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct}{meta-llama/Meta-Llama-3-8B-Instruct}} to validate the compatibility of our method with different models. We use the generally fine-tuned models as there are no models that have been fine-tuned on the UltraChat dataset.

\paragraph{Evaluations.}
Following standard practices for aligning LLMs, we employ two primary evaluation benchmarks to assess model performance. First, we utilize \textbf{AlpacaEval 2.0}~\cite{dubois2024alpacafarm, dubois2024length}, a benchmark designed to approximate human preferences in instruction-following tasks. This evaluation involves 805 diverse instructions sourced from multiple datasets, where responses from the model under test are compared against those generated by GPT-4~\cite{achiam2023gpt} to determine win rates. To address potential biases related to response length—a known factor influencing LLM preferences~\cite{zheng2023judging, wang2023far}, we report both the original win rate and a length-controlled (LC) win rate. The LC win rate is calculated using a regression model trained to neutralize the impact of response length, thereby focusing on the quality of the generated content~\cite{dubois2024length}.

Second, we employ \textbf{MT-Bench}~\cite{zheng2023judging} to evaluate the model’s capabilities across a broader range of tasks. MT-Bench assesses a chatbot’s performance in areas such as math, coding, role-playing, and writing through multi-turn interactions. Responses are scored by GPT-4, providing a comprehensive measure of the model’s proficiency in key LLM functionalities. Together, these benchmarks offer a robust evaluation of how well the model aligns with human preferences and its effectiveness in real-world applications.


\paragraph{Implementation details.}
% 参考SPA， 以及我们自己的超参数 \epsilon ,K (#of polynomial)  , M (# of group)

In initial alignment phase, we use DPO training the model on a seed dataset of 2K samples to obtain the base model $\pi_0$. Following up, we conduct $3$ iterative stages of data expansion. In the $i$-th iteration ($i = 1, 2, 3$), we generate preference data by independently sampling two responses for each prompt using a temperature of $0.7$ and labeling them as chosen or rejected through $R(x, y)$, resulting in a preference dataset $\{\xi_i\}_{i=1}^N$ ($N$ is the size of the $i$-th prompt set). To model the worst-case distribution program, we define a set of linear fucntion $\ell_k(x) = - \frac{K}{k}(x - \frac{k}{K}) - \log (\frac{k}{K})$ for $k = 1, \cdots, K$ (The family of tangents of the loss function at the $K$ equipartition of $[0,1]$). We solve the associated optimization program using the Sequential Least Squares Programming (SLSQP) method. The group size $G$ is set to $100$ unless otherwise specified for parallel computation of the convex program. Finally, we update the policy model by minimizing the reweighted loss to get $\pi_i$, ensuring improved alignment with the desired preferences.

\paragraph{Baselines.}
We consider the following baselines for comparison: (1) DPO, which performs DPO training only on the seed data. (2) Iter DPO~\cite{Xiong2024Iterative}, which iteratively generates preference data using an external reward model (PairRM)~\cite{Jiang2023LLM} or LLM-as-judge~\cite{Li2024LLMs}. (3) SPA~\cite{Kim2025Spread}, which iteratively generates preference data using implicit reward model.



\subsection{Main results}

% \begin{table*}[ht]
% \centering
% \caption{\textbf{Main results.} Evaluation results on AlpacaEval 2.0 and MT-Bench with different variants of Mistral-7B-v0.1 and LLaMA3-8B. The best scores for each model are highlighted with bold. The baseline results are from \cite{Kim2025Spread}.}
% \resizebox{2.0\columnwidth}{!}{
% \begin{tabular}{lcccc}
% \toprule
% \textbf{Models} & \textbf{Gold Label (\%)} & \multicolumn{2}{c}{\textbf{AlpacaEval 2.0}} & \textbf{MT-Bench} \\
% \cmidrule(lr){3-5}
% & & \textbf{Len-control.} & \textbf{Win Rate} & \textbf{Avg.} \\
% & & \textbf{Win Rate (\%)} & \textbf{vs. GPT-4 (\%)} & \textbf{Score (0-10)} \\

% \midrule
% Mistral-7B-v0.1 & - & 0.17 & 0.50 & 3.25 \\
% Zephyr-7b-$\beta$ & 100 & 11.75 & 10.03 & 6.87 \\
% Mistral-7B-SFT & - & 7.58 & 4.72 & 6.34 \\
% Mistral-7B-DPO & 3.3 & 9.03 & 7.68 & 6.81 \\
% Mistral-7B-Iter DPO (PairRM) & 3.3 & 11.87 & 9.46 & 6.98 \\
% Mistral-7B-Iter DPO (LLM-as-judge) & 3.3 & 9.28 & 9.18 & 6.67 \\
% Mistral-7B-SPA & 3.3 & 15.39 & 21.13 & 6.94 \\
% Mistral-7B-SSAPO (Ours) & 3.3 & \bf{24.44} & \bf{35.82} & 6.68 \\
% \bottomrule
% \midrule
% % Llama3-8B-Instruct & - & 22.90 & 22.60 & - \\
% Llama3-8B-SFT & - & 18.83 & 15.31 & - \\
% Llama3-8B-DPO & 100 & 20.61 & 18.04 & - \\
% Llama3-8B-SPA & 3.3 & 21.85 & 24.95 & - \\
% Llama3-8B-SSAPO (Ours) & 3.3 & \bf{33.33} & \bf{40.12} & 8.03 \\
% \bottomrule
% \end{tabular}}
% \label{tab:main_results}
% \end{table*}

\begin{table*}[ht]
\centering
\caption{\textbf{Main results.} Evaluation results on AlpacaEval 2.0 and MT-Bench with different variants of Mistral-7B-v0.1 and LLaMA3-8B. All models use the same 3.3\% preference data with gold label as seed data.  The best and second-best results are highlighted in bold and underlined, respectively. Most of the baseline results are from \cite{Kim2025Spread}.}
\resizebox{2.0\columnwidth}{!}{
\begin{tabular}{lccc}
\toprule
\textbf{Models} &  \multicolumn{2}{c}{\textbf{AlpacaEval 2.0}} & \textbf{MT-Bench} \\
\cmidrule(lr){2-4}
&  \textbf{Len-control. Win Rate (\%)} & \textbf{Win Rate vs. GPT-4 (\%)} & \textbf{Avg. Score (0-10)} \\

\midrule
Mistral-7B-DPO &  9.03 & 7.68 & 6.81 \\
Mistral-7B-Iter DPO (PairRM)  & 11.87 & 9.46 & 6.98 \\
Mistral-7B-Iter DPO (LLM-as-judge)  & 9.28 & 9.18 & 6.67 \\
Llama3-8B-DPO & 20.61 & 18.04 & - \\
Mistral-7B-SPA  & 15.39 & 21.13 & 6.94 \\
Llama3-8B-SPA  & 21.85 & 24.95 & 7.86 \\
\midrule
Mistral-7B-SSAPO (Ours)  & \underline{24.44} & \underline{35.82} & 6.68 \\
Llama3-8B-SSAPO (Ours)  & \bf{33.33} & \bf{40.12} & \bf{8.03} \\
\bottomrule
\end{tabular}}
\label{tab:main_results}
\end{table*}


\begin{table*}[ht]
\centering
\caption{\textbf{Comparison with different variants of Mistral.} Evaluation results on AlpacaEval 2.0 and MT-Bench with different variants of Mistral-7B-v0.1. The best scores are highlighted with bold. The baseline results are from \cite{Kim2025Spread} and \citep{dubois2024length}.}
\resizebox{2.0\columnwidth}{!}{
\begin{tabular}{lcccc}
\toprule
\textbf{Models} & \textbf{Gold Label (\%)} & \multicolumn{2}{c}{\textbf{AlpacaEval 2.0}} & \textbf{MT-Bench} \\
\cmidrule(lr){3-5}
& & \textbf{Len-control. Win Rate (\%)} & \textbf{Win Rate vs. GPT-4 (\%)} & \textbf{Avg. Score (0-10)} \\


\midrule
Mistral-7B-v0.1 & - & 0.17 & 0.50 & 3.25 \\
Zephyr-7b-$\beta$ & 100 & 11.75 & 10.03 & 6.87 \\
Mistral-7B-SFT & - & 7.58 & 4.72 & 6.34 \\
Mistral-7B-DPO & 3.3 & 9.03 & 7.68 & 6.81 \\
Mistral-Large (123B) & - & 21.4 & 32.7 & - \\
Mistral-7B-SSAPO (Ours) & 3.3 & \bf{24.44} & \bf{35.82} & 6.68 \\
\bottomrule
\end{tabular}}
\label{tab:mistral_variants}
\end{table*}


\subsection{Sensitivity Analysis}

\begin{table*}[h]
\centering
\caption{\textbf{Different Wasserstein Radius} }
\begin{tabular}{lccccc}
\toprule
$\epsilon$ & 0 & 0.01 & 0.03 & 0.05 & 0.1 \\
\midrule
\textbf{Len-control. Win Rate(\%)} & 19.76 & \textbf{24.44} & 22.42 & 23.20 & 19.78 \\
\textbf{Win Rate vs. GTP-4(\%)} & 26.58 & \textbf{35.82} & 32.30 & 32.92 & 25.84 \\
\bottomrule
\end{tabular}
\end{table*}

\begin{table}[h]
\centering
\caption{\textbf{Different size of tangents}}
\begin{tabular}{lccc}
\toprule
$K$ & 5 & 6 & 7  \\
\midrule
\textbf{Len-control. Win Rate(\%)} & 22.89 & \textbf{23.20} & 19.05 \\
\textbf{Win Rate vs. GTP-4(\%)} & 29.19 & \textbf{32.92} & 25.84\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{\textbf{Different group size}}
\begin{tabular}{lcccc}
\toprule
$G$ & 100 & 200 & 300 \\
\midrule
\textbf{CPU Runtime (min)} & 45 & 206 & 630 \\
\textbf{Len-control. Win Rate(\%)} & 13.70 & 14.81 & \textbf{16.95} \\
\textbf{Win Rate vs. GTP-4(\%)} & 10.00 & 11.74 & \textbf{14.91} \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}[h]
\centering
\caption{\textbf{Different seed data}}
\begin{tabular}{lccc}
\toprule
Seed Data & 1st & 2nd & 3rd \\
\midrule
\textbf{Len-control. Win Rate(\%)} & 22.43 & 23.20 & \textbf{23.75} \\
\textbf{Win Rate vs. GTP-4(\%)} &  29.10 & \textbf{32.92} & 24.47 \\
\textbf{Average Length} & 2648 & 3416 & 2121 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=.38\textwidth]{figs/win-rate-iterations.pdf}
\caption{\textbf{Improvement during iterations} Evaluation results on Alpcaca Eval 2.0 of initial DPO stage and each iterations, the results of the SFT model are from \cite{Kim2025Spread}}.
\label{fig:improv_dur_iters}
\end{figure}

Figure~\ref{fig:improv_dur_iters} illustrates the 3-stage self-annotation leads to considerable performance improvements during SSAPO. At the initial DPO stage, the model shows only minor improvements while by iteration $1$, substantial gains are observed, with LC win rate reaching 18.60\% and win rate at 18.54\%. And then by iteration $2$, LC win rate improves to 23.95\%, and win rate increases to 30.30\%. This trend continues into iteration $3$, achieving 24.44\% LC win rate and 35.82\% win rate.

% \begin{table}[h]
% \centering
% \caption{\textbf{Different percentage of Gold Labels}}
% \begin{tabular}{lcc}
% \toprule
% \textbf{Gold Label(\%)} & 3.3 & 10 \\ 
% \midrule
% \textbf{Len-control. Win Rate(\%)} & 24.44 & \\
% \textbf{Win Rate vs. GTP-4(\%)} & 35.82 &  \\
% \bottomrule
% \end{tabular}
% \end{table}