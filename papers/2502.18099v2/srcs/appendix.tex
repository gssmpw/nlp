\paragraph{Organization of the Appendix.}
\begin{itemize}
    \item \textbf{Section~\ref{sec:related_supp}} recap LLM alignment and data-efficient methods, as well as the Game-theoretic alignment methods. And dicuss the connection and distinction between SGPO/SSAPO with them.
    \item \textbf{Section~\ref{sec:Wasserstein_Prelim}} revisits the core definitions and properties of the 1-Wasserstein metric, including a statement of the Kantorovich--Rubinstein duality.
    \item \textbf{Section~\ref{sec:regularity_conditions}} restates and discusses the regularity conditions needed for our theoretical guarantees, such as compactness and Lipschitz continuity.
    \item \textbf{Section~\ref{sec:theory_proofs}} provides detailed proofs for the existence and convergence of the Stackelberg equilibrium, as well as the regret bounds for SGPO and comparisons with DPO.
    \item \textbf{Section~\ref{sec:SSAPO_algorithm+analysis}} presents the SSAPO algorithm in pseudocode and includes an analysis of its computational complexity.
    \item \textbf{Section~\ref{sec:experimental_details_more}} gives supplementary information on experimental setups, hyperparameter choices, grouping strategies for DRO, and other implementation details.
    \item \textbf{Section~\ref{sec:qualitative_examples}} illustrates additional qualitative comparisons of model outputs, highlighting the differences between DPO, SPA, and SSAPO in practice.
\end{itemize}

\section{More Detailed Related Work}
\label{sec:related_supp}
\paragraph{LLM Alignment and Data-Efficient Methods}
Aligning large language models (LLMs) with human preferences is central to modern deployments
\citep{Ziegler2019Fine,Ouyang2022Training,Bai2022Training},.
While Reinforcement Learning with Human Feedback (RLHF) \citep{Christiano2017Deep} trains a reward model and then maximizes it under KL constraints, it typically requires massive human-annotated data. Recent alternatives focus on \emph{directly} fine-tuning LLMs from pairwise preference data without an explicit reward model. 
Notably, Direct Preference Optimization (DPO) \citep{Rafailov2023Direct}
derives a closed-form surrogate objective that recovers RLHF’s solution 
but avoids a separate reward modeling stage. Subsequent works simplify or extend this pipeline; for instance, \citet{Ethayarajh2024KTO} remove the need for pairwise labels by adopting a human utility model, 
while there are also works \citep{Meng2024Simpo,Hong2024ORPO,Azar2024General} introduce novel optimization objectives  to handle different preference formats. 
Despite progress, these approaches still rely on large-scale preference annotations, making label-efficiency a key challenge. To reduce the reliance on expensive human labels, several methods have explored letting the LLM or an auxiliary model 
generate and rank unlabeled responses, thereby creating synthetic preference data \citep{Jiang2023LLM,Yuan2024Self,Xiong2024Iterative,Kim2025Spread}. 
However, many of these approaches assume accessibility to a reliable well-aligned "judge", which could be prohibitive costly in realistic scenarios. To address the cost bottleneck, \citet{Kim2025Spread} propose a \emph{Spread Preference Annotation (SPA)} framework that starts from a small seed of human-annotated preferences and iteratively expands the dataset by self-annotation. Our work is closely related to SPA: we replicate its experimental setup by using the same small-scale seed preferences and iterating between new response generation and preference learning. 
However, our \emph{Stackelberg} perspective considers the inaccuracy of self-annotation, and explicitly defends against worst-case preference shifts. Empirically, we show that this game-theoretic \emph{distributional} approach yields stronger label efficiency.

\vspace{-0.15 in}
\paragraph{Game-Theoretic Alignment Methods}
An emerging body of work has begun to frame preference alignment of LLMs 
through the lens of \emph{games}. A conceptual similar work \citep{Makar2024Sta} propose \emph{Stackelberg Alignment RLHF}. However, their nested gradient-based heuristic does not guaranteed to converge to the equilibrium. While we prove our updates for the leader and follower converge to an equilibrium.
Meanwhile, \citet{Ye2024Evolving} present a framework that casts prompt-creator and solver asymmetric players in an evolving game, the differences between our work is we focus on evolving the distribution of the  responses , while they focus on evoling the distribution of the prompts. SPIN \citep{Chen2024SelfPlay} use self-play to iteratively refine 
a policy without additional human data, however they assume accessible to adequate supervised fine-tuning (SFT) data. 
Other works adopt \emph{Nash} or \emph{minimax} formulations:
\citet{Melnyk2024Distributional} study alignment via an optimal-transport objective to capture distributional preferences, 
\citet{Zhang2024Iterative} and \citet{Rosset2024Direct} formulate alignment as a two-player game aiming for a Nash policy, 
and \citet{Munos2024Nash} proposes “Nash learning from human feedback” 
by treating the policy and a competing policy as iterative players.
Likewise, \citet{Swamy2024Minimaximalist,Wu2024Self} introduce self-play preference optimization methods 
in which two policies repeatedly compete under a constant-sum setting. 
They demonstrate promising performance on synthetic and text-based benchmarks, but typically set both players as \emph{policy vs. policy}. By contrast, our \emph{SGPO} framework focuses on \emph{policy vs. distribution}:  the leader policy maximizes preference likelihood, while the follower adversarially reweights or shifts the empirical preference distribution. This setup offers a distinct distributional robust-control view, leading to tight theoretical guarantees (e.g., $\mathcal{O}(\epsilon)$-bounded regret) and a practical algorithm (SSAPO) that is readily integrated with self-annotation. Hence, our method complements the “policy vs.\ policy” family by delivering strong resistance to noisy or distribution-mismatched preferences at small annotation cost.
\section{Preliminaries on the Wasserstein Metric Space}
\label{sec:Wasserstein_Prelim}

Wasserstein (or Earth Mover’s) distances are widely used in robust optimization and optimal transport to measure how far two probability distributions are from one another \citep{Villani2009Optimal}. Below, we give a concise overview of the 1-Wasserstein metric on a subset $\Xi \subseteq \mathbb{R}^m$. We also recap the Kantorovich--Rubinstein duality (Lemma~\ref{lemma:kantorovich_rubinstein_duality}), which is central to several of our regret and robustness proofs in the main text.

\subsection{Definition of the 1-Wasserstein Metric}
Let $\mathcal{M}(\Xi)$ be the space of all probability distributions supported on $\Xi$ such that 
$$
\mathbb{E}_{\xi \sim F} \bigl[\|\xi\|\bigr] \;=\; \int_{\Xi} \|\xi\|\;\mathrm{d}F(\xi) \;<\;\infty.
$$
In our setting, $\|\cdot\|$ can be any norm on $\mathbb{R}^m$, typically the Euclidean norm (although other choices are possible).

\begin{definition}[1-Wasserstein Metric]
\label{def:1_wasserstein}
For two probability distributions $F_1, F_2 \in \mathcal{M}(\Xi)$, the \emph{1-Wasserstein} distance (often just called “the Wasserstein distance”) is defined as
\begin{equation}
\label{eq:wasserstein_def}
d(F_1, F_2) \;:=\; \inf_{\pi \in \Pi(F_1, F_2)}
\;\left\{
    \int_{\Xi \times \Xi} \bigl\|\xi_1 \;-\; \xi_2\bigr\|\;\mathrm{d}\pi(\xi_1, \xi_2)
\right\},
\end{equation}
where $\Pi(F_1, F_2)$ is the set of all joint distributions on $\Xi \times \Xi$ whose marginals are $F_1$ and $F_2$, respectively. Intuitively, $\pi$ specifies how “mass” is transported from points in the support of $F_1$ to points in the support of $F_2$, and $\|\xi_1 - \xi_2\|$ is the cost of moving a unit of mass from $\xi_1$ to $\xi_2$.
\end{definition}

Equivalently, one can interpret the Wasserstein distance as the minimal cost of transforming the distribution $F_1$ into $F_2$ when the cost of moving a unit mass from $\xi_1$ to $\xi_2$ is $\|\xi_1 - \xi_2\|$. This framework underpins many distributionally robust methods, including the SGPO formulation in our paper.

\subsection{Kantorovich--Rubinstein Duality}
\label{subsec:KR-duality}

A crucial result for the 1-Wasserstein distance is the Kantorovich--Rubinstein duality (Theorem 5.9 in \citet{Villani2009Optimal}), which states that the infimum over transport plans (as in Definition~\ref{def:1_wasserstein}) is equivalent to a supremum over 1-Lipschitz test functions. We use this lemma extensively to derive Lipschitz-based bounds in the main proofs (e.g., Theorems~\ref{thm:sgpo_regret_bound_detailed}--\ref{thm:dpo_regret_lowerbound_supp}).

\begin{lemma}[Kantorovich--Rubinstein Duality]
\label{lemma:kantorovich_rubinstein_duality}
Let $F_1, F_2 \in \mathcal{M}(\Xi)$ with finite first moments. Then the 1-Wasserstein distance \eqref{eq:wasserstein_def} admits the following dual representation:
\begin{equation}
\label{eq:KR-duality}
d(F_1, F_2)
\;=\;
\sup_{\|f\|_{\mathrm{Lip}}\le 1}
\;\bigl(\mathbb{E}_{\xi \sim F_1}[f(\xi)] \;-\; \mathbb{E}_{\xi \sim F_2}[f(\xi)]\bigr),
\end{equation}
where the supremum is taken over all 1-Lipschitz functions $f : \Xi \to \mathbb{R}$, i.e.\ functions satisfying
$$
\bigl|f(\xi) - f(\xi')\bigr|
\;\le\;
\|\xi - \xi'\|
\quad \forall\,\xi,\xi' \in \Xi.
$$
\end{lemma}
\noindent
Lemma~\ref{lemma:kantorovich_rubinstein_duality} underpins many of our theoretical arguments, particularly in bounding the gap between performance under the empirical distribution $\hat{P}$ and any perturbed distribution $P$ in a Wasserstein ball $\mathcal{U}_\epsilon(\hat{P})$. As shown in Section~\ref{sec:theory_proofs} of our paper, it simplifies comparing $\mathbb{E}_{P}[f]$ and $\mathbb{E}_{\hat{P}}[f]$ when $f$ is Lipschitz in model parameters or responses.


\section{Regularity Conditions for Stackelberg Game Preference Optimization}
\label{sec:regularity_conditions}
Throughout the analysis, we require standard continuity, convexity, and compactness conditions:

\begin{assumption}[Regularity Conditions]
\label{assump:regularity_extended}
\begin{itemize}
    \item \textbf{Compactness.}
$\Pi \subseteq \mathbb{R}^d$ is compact (or effectively constrained), $\mathcal{Y}\subseteq \mathbb{R}^m$ is bounded, and the Wasserstein ball $\mathcal{U}_{\epsilon}(\hat{P})$ is compact in $\mathcal{P}(\mathcal{Y}\times \mathcal{Y})$.
    \item \textbf{Lipschitz Continuity.}
We assume : The function $R_\pi(y)$ is $L_R$-Lipschitz in $y$, uniformly for $\pi\in\Pi$.  That is, $\bigl|R_\pi(y)-R_\pi(y')\bigr|\le L_R\,\|y-y'\|$. The policy mapping $\theta \mapsto \pi_\theta$ is sufficiently smooth, e.g.\ its gradient is $L_\pi$-Lipschitz, though only a bounded-gradient property may also suffice.
    \item \textbf{Convexity.} The set $\mathcal{U}_{\epsilon}(\hat{P})$ is convex in the space of probability measures with respect to the Wasserstein metric, consistent with standard definitions of Wasserstein balls \citep{Villani2009Optimal, Esfahani2018Data}.
\end{itemize}
\end{assumption}

\begin{remark}[Bounded Domain for Neural Networks]
Although neural network parameters $\theta \in \mathbb{R}^d$ are technically unbounded, many theoretical analyses restrict $\theta$ to a large but bounded ball (via a norm constraint) or rely on a coercive objective to prevent unbounded parameter growth.  Hence, requiring $\Pi$ to be compact is common in theoretical treatments.  In practice, gradient-based optimization does not typically push $\|\theta\|$ to infinity.
\end{remark}


\section{Theoretical Results}
\label{sec:theory_proofs}
\subsection{Existence and Convergence to a Stackelberg Equilibrium}
\begin{theorem}[Existence of Stackelberg Equilibrium]
\label{thm:existence_se_detailed}
Under Assumption~\ref{assump:regularity_extended}, the two-player game defined by
$$
\max_{\pi \in \Pi}
\;
\min_{P \,\in\, \mathcal{U}_\epsilon(\hat{P})}
\;\;
J(\pi,P),
$$
$$
\text{where} \quad
J(\pi,P)
=
\mathbb{E}_{P}\Bigl[\log \sigma\bigl(R_{\pi}(y_w) - R_{\pi}(y_l)\bigr)\Bigr]
$$
admits at least one Stackelberg equilibrium $\bigl(\pi^*,P^*\bigr)$.
\end{theorem}

\begin{proof}
We proof the existence by quoting the famous Sion's Minimax Theorem \citep{Sion1958Minimax} as the key lemma.
\begin{lemma}[Sion's Minimax Theorem \citet{Sion1958Minimax}]
\label{lemma:sion_minimax}
Let $X$ and $Y$ be convex and compact subsets of topological vector spaces. Suppose $\phi: X \times Y \to \mathbb{R}$ is a function satisfying the following conditions:
\begin{itemize}
    \item For each fixed $y \in Y$, the map $x \mapsto \phi(x,y)$ is \emph{concave} and upper semi-continuous in $x$.
    \item For each fixed $x \in X$, the map $y \mapsto \phi(x,y)$ is \emph{convex} and lower semi-continuous in $y$.
\end{itemize}
Then
$$
\max_{x \in X} \;\min_{y \in Y} \;\phi(x,y)
\;\;=\;\;
\min_{y \in Y} \;\max_{x \in X} \;\phi(x,y).
$$
\end{lemma}

\textbf{Step 1 (Concavity in $\pi$):}  
Fix $P\in \mathcal{P}(\mathcal{Y}\times\mathcal{Y})$.  The function 
$$
\pi \;\mapsto\; \log\,\sigma\bigl(R_\pi(y_w)-R_\pi(y_l)\bigr)
$$
is concave in $\pi$ provided $R_\pi(y)$ is affine or at least concave in $\pi$.  More precisely, if we treat $R_\pi(y)$ as a linear functional (e.g.\ the logit induced by $\pi_\theta$), then 
$\log \sigma(\cdot)$ is concave in its argument.  Under mild conditions (e.g.\ standard neural network approximation with linear final layer), the resulting composition remains concave or quasi-concave in $\theta$.  

\textbf{Step 2 (Convexity in $P$):} 
For a fixed $\pi$, 
$$
P \;\mapsto\;
\mathbb{E}_P\bigl[\log\,\sigma\bigl(R_\pi(y_w)-R_\pi(y_l)\bigr)\bigr]
$$
is linear (hence convex) in $P$.  Indeed, $\mathbb{E}_{P}[\cdot]$ is always an affine operator in probability measures.

\textbf{Step 3 (Compactness \& Sion’s Theorem):}
By Assumption~\ref{assump:regularity_extended}, $\Pi$ is compact and $\mathcal{U}_\epsilon(\hat{P})$ is convex and compact in the Wasserstein sense.  Hence, Sion’s minimax theorem \citep{Sion1958Minimax} applies and guarantees a saddle-point solution $\bigl(\pi^*,\,P^*\bigr)$.  By definition, that saddle point coincides with a Stackelberg equilibrium in this setting:
$$
\max_{\pi \in \Pi}\;\min_{P \in \mathcal{U}_\epsilon(\hat{P})}\; J(\pi,P)
\;=\;
\min_{P \in \mathcal{U}_\epsilon(\hat{P})}\;\max_{\pi \in \Pi}\; J(\pi,P).
$$
Thus, an equilibrium pair exists.
\end{proof}

\begin{theorem}[Linear Convergence to Stackelberg Equilibrium]
\label{thm:convergence_iterative_detailed_supp}
Under Assumption~\ref{assump:regularity_extended}, the sequence $\{(\pi_t,P_t)\}_{t \ge 0}$ generated by by the following procedure converges to the unique Stackelberg equilibrium $(\pi^*,P^*)$.
\begin{equation}
\label{eq:iterative_update_policy_supp}
\pi_{t+1}
\;=\;
\arg\max_{\pi \in \Pi}
\;
\min_{P \,\in\, \mathcal{U}_\epsilon(P_t)}
\;
J(\pi,P),
\end{equation}
\begin{equation}
\label{eq:iterative_update_distribution_supp}
P_{t+1}
\;=\;
\arg\min_{P \,\in\, \mathcal{U}_\epsilon(P_t)}
\;
J(\pi_{t+1},P),
\end{equation}
starting from an initial pair $(\pi_0, P_0)$.  Here, we shift the center of the Wasserstein ball in each iteration to $P_t$.  

Moreover, the convergence is \emph{linear}, i.e.\ there exists $\gamma < 1$ such that 
$$
\rho\bigl((\pi_{t+1},P_{t+1}),(\pi^*,P^*)\bigr)
\;\;\le\;\;
\gamma
\;\rho\bigl((\pi_t,P_t),(\pi^*,P^*)\bigr),
$$
where $\rho$ is a suitable metric (e.g.\ $\rho((\pi,P),(\pi',P'))=\|\pi-\pi'\|+ W(P,P')$).
\end{theorem}

\begin{proof}
\textbf{Step 1 (Policy update $\pi_{t}\to\pi_{t+1}$):}
By definition,
$$
\pi_{t+1}
\;=\;
\arg\max_{\pi}
\min_{P \,\in\,\mathcal{U}_\epsilon(P_t)}
\,
J(\pi,P).
$$
If we assume (or approximate) that $\max_{\pi}\min_{P}$ is well-approximated by a (sub)gradient-based ascent with step size $\eta$, then Lipschitz continuity in $\pi$ (Assumption~\ref{assump:regularity_extended}) ensures a contraction of the form:
$$
\|\pi_{t+1} - \pi_t\|
\;\le\;
\gamma_\pi
\;\|\pi_t - \pi_{t-1}\|,
$$
with $\gamma_\pi < 1$ provided $\eta$ is sufficiently small.  
(Alternatively, one can rely on standard monotone operator theory to show a unique fixed point.)

\textbf{Step 2 (Distribution update $P_{t}\to P_{t+1}$):}
From \eqref{eq:iterative_update_distribution_supp},
$$
P_{t+1}
\;=\;
\arg\min_{P \,\in\, \mathcal{U}_\epsilon(P_t)}
\;
J(\pi_{t+1},P).
$$
Since $J(\pi_{t+1},P)$ is $L_R$-Lipschitz in the support of $P$, the Wasserstein-bounded set $\mathcal{U}_\epsilon(P_t)$ implies
$$
W\bigl(P_{t+1},\,P_t\bigr)
\;\;\le\;
\frac{\epsilon \,L_R}{1 + \epsilon \,L_R}
\;W\bigl(P_t,\,P_{t-1}\bigr),
$$
yielding a contraction factor $\gamma_P = \tfrac{\epsilon \,L_R}{1 + \epsilon \,L_R} < 1$.

\textbf{Step 3 (Combined contraction):}
Define 
$\rho\bigl((\pi,P),(\pi',P')\bigr) = \|\pi - \pi'\| + W\bigl(P,P'\bigr).$
From Steps 1--2, one can show 
$$
\rho\bigl((\pi_{t+1},P_{t+1}),(\pi_t,P_t)\bigr)
\;\;\le\;\;
\max(\gamma_\pi,\gamma_P)
\;\rho\bigl((\pi_t,P_t),(\pi_{t-1},P_{t-1})\bigr).
$$
Hence, by the Banach fixed-point theorem, the sequence converges linearly to a unique fixed point.  That fixed point necessarily satisfies the Stackelberg equilibrium conditions (Definition~\ref{def:se}).
\end{proof}

\subsection{Regret Analysis and Comparison between SGPO and DPO}
\subsubsection{SGPO's Regret}
\begin{theorem}[Worst-Case Performance Guarantee for SGPO]
\label{thm:sgpo_regret_bound_detailed}
Under Assumption~\ref{assump:regularity_extended}, let $\pi^*$ be the SGPO solution.  Then for \emph{every} $P\in \mathcal{U}_\epsilon(\hat{P})$,
\begin{equation}
\mathcal{P}\bigl(\pi^*,P\bigr)
\;\;\ge\;\;
\mathcal{P}\bigl(\pi^*,\hat{P}\bigr)
\;-\;
L_R\,\epsilon.
\end{equation}
In other words, the performance drop from $\hat{P}$ to any $P\in \mathcal{U}_\epsilon(\hat{P})$ is at most $L_R\epsilon$.
\end{theorem}

\begin{proof}
By the Kantorovich-Rubinstein duality in Lemma~\ref{lemma:kantorovich_rubinstein_duality}, for any $1$-Lipschitz function $f$ and distributions $P_1,P_2$,
$$
\bigl|\,\mathbb{E}_{P_1}[f] - \mathbb{E}_{P_2}[f]\,\bigr|
\;\le\;
W\!\bigl(P_1,\,P_2\bigr).
$$
Take $f\bigl(y_w,y_l\bigr) = \log \sigma\bigl(R_{\pi^*}(y_w)-R_{\pi^*}(y_l)\bigr)$.  By assumption, $R_{\pi^*}(\cdot)$ is $L_R$-Lipschitz, so we can bound the overall Lipschitz constant of $f$ by $L_R$.  Hence,
$$
\Bigl|\,
\mathcal{P}\bigl(\pi^*, P\bigr)
\;-\;
\mathcal{P}\bigl(\pi^*, \hat{P}\bigr)
\Bigr|
\;\le\;
L_R\,W\!\bigl(P,\hat{P}\bigr)
\;\le\;
L_R\,\epsilon.
$$
Thus,
$\mathcal{P}\bigl(\pi^*, P\bigr)\ge \mathcal{P}\bigl(\pi^*, \hat{P}\bigr)-L_R\epsilon.$
\end{proof}

\paragraph{Regret Notation.}
We define the regret of a policy $\pi$ on a distribution $P$ as
\begin{equation}
\text{Regret}\bigl(\pi,P\bigr)
\;=\;
\max_{\tilde{\pi}}\,\mathcal{P}\!\bigl(\tilde{\pi},P\bigr)
\;-\;
\mathcal{P}\!\bigl(\pi,P\bigr).
\end{equation}
If $\pi_P^*=\arg\max_{\tilde{\pi}}\mathcal{P}(\tilde{\pi},P)$, then
$\text{Regret}\bigl(\pi,P\bigr)=\mathcal{P}\bigl(\pi_P^*,P\bigr)-\mathcal{P}\bigl(\pi,P\bigr).$

\begin{theorem}[SGPO Regret Bound]
\label{thm:sgpo_regret_supp}
For the SGPO policy $\pi^*$, we have
\begin{equation}
\sup_{P \,\in\,\mathcal{U}_\epsilon(\hat{P})}
\;\;
\text{Regret}\bigl(\pi^*,P\bigr)
\;\;\le\;\;
2\,L_R\,\epsilon.
\end{equation}
Thus, SGPO is robust: under any shift of at most $\epsilon$, its regret is bounded by a constant factor of $\epsilon$.
\end{theorem}

\begin{proof}
Let $\pi^*_P\in \arg\max_{\pi}\,\mathcal{P}(\pi,P)$.  Then
$$
\text{Regret}\bigl(\pi^*,P\bigr)
=
\mathcal{P}\bigl(\pi^*_P,P\bigr)
-\,
\mathcal{P}\bigl(\pi^*,P\bigr).
$$
By Theorem~\ref{thm:sgpo_regret_bound_detailed} and the triangle inequality,
$$
\begin{aligned}
\text{Regret}\bigl(\pi^*,P\bigr)
&\;\le\;
\Bigl[\,
  \mathcal{P}\bigl(\pi^*_P,\hat{P}\bigr)
  \;+\;L_R\,\epsilon
\Bigr]
\;-\;
\Bigl[\,
  \mathcal{P}\bigl(\pi^*,\hat{P}\bigr)
  \;-\;L_R\,\epsilon
\Bigr]
\\[4pt]
&\;=\;
\Bigl[\,
  \mathcal{P}\bigl(\pi^*_P,\hat{P}\bigr)
  \;-\;
  \mathcal{P}\bigl(\pi^*,\hat{P}\bigr)
\Bigr]
\;+\;
2\,L_R\,\epsilon.
\end{aligned}
$$
Since $\pi^*$ is the minimax solution under $\hat{P}$, we have $\mathcal{P}\bigl(\pi^*_P,\hat{P}\bigr)\le \mathcal{P}\bigl(\pi^*,\hat{P}\bigr)$.  Hence the bracketed term is $\le 0$.  Therefore,
$$
\text{Regret}\bigl(\pi^*,P\bigr)
\;\le\;
2\,L_R\,\epsilon.
$$
Taking the supremum over $P\in \mathcal{U}_\epsilon(\hat{P})$ proves the claim.
\end{proof}

\subsubsection{Comparing DPO's Regret}
\begin{theorem}[DPO Regret Lower Bound]
\label{thm:dpo_regret_lowerbound_supp}
Let $\pi_{\mathrm{DPO}}=\arg\max_{\pi}\mathcal{P}(\pi,\hat{P})$, and let $P^*$ be a distribution satisfying $W(\hat{P},P^*)=\delta$.  Then
\begin{equation}
\text{Regret}\bigl(\pi_{\mathrm{DPO}},\,P^*\bigr)
\;\ge\;
L_R\;\bigl(\delta - 2\,\epsilon\bigr).
\end{equation}
In particular, if $\delta \gg \epsilon$, DPO’s regret grows \emph{linearly} in $\delta$.
\end{theorem}
\begin{proof}
Let $\pi^*_{P^*}\in\arg\max_{\pi}\;\mathcal{P}(\pi,P^*)$.  Then
\begin{equation}
\begin{aligned}
\text{Regret}\bigl(\pi_{\mathrm{DPO}},P^*\bigr)
&=
\mathcal{P}\bigl(\pi^*_{P^*},P^*\bigr)
-\,
\mathcal{P}\bigl(\pi_{\mathrm{DPO}},P^*\bigr)
\\
&>
\Bigl[
  \mathcal{P}\bigl(\pi^*_{P^*},\hat{P}\bigr)
  -\,L_R\,\delta
\Bigr]
-
\Bigl[
  \mathcal{P}\bigl(\pi_{\mathrm{DPO}},\hat{P}\bigr)
  +\,L_R\,\delta
\Bigr]\\
&=
\Bigl[
  \mathcal{P}\bigl(\pi^*_{P^*},\hat{P}\bigr)
  -\,\mathcal{P}\bigl(\pi_{\mathrm{DPO}},\hat{P}\bigr)
\Bigr]
-\,2\,L_R\,\delta.
\end{aligned}
\end{equation}
Since $\pi_{\mathrm{DPO}}$ is optimal on $\hat{P}$, the bracketed term is $\le 0$.  Thus
$$
\text{Regret}\bigl(\pi_{\mathrm{DPO}},P^*\bigr)
\;\ge\;
-2\,L_R\,\delta.
$$
However, once $\delta>2\epsilon$, Theorem~\ref{thm:sgpo_regret_supp} implies 
$\text{Regret}\bigl(\pi^*,P^*\bigr)\le 2\,L_R\,\epsilon < L_R\,\delta.$
Hence
$$
\text{Regret}\bigl(\pi_{\mathrm{DPO}},P^*\bigr)
\;-\;
\text{Regret}\bigl(\pi^*,P^*\bigr)
\;\ge\;
L_R\,(\delta - 2\epsilon).
$$
Equivalently,
$\text{Regret}\bigl(\pi_{\mathrm{DPO}},P^*\bigr)\,\ge\,L_R\,(\delta - 2\epsilon)$.
\end{proof}

\begin{corollary}[SGPO Advantage Over DPO]
\label{cor:sgpo_advantage}
If $W\!\bigl(\hat{P},P^*\bigr)=\delta > 2\,\epsilon$, then
\begin{equation}
\frac{\text{Regret}\bigl(\pi_{\mathrm{DPO}},P^*\bigr)}%
     {\text{Regret}\bigl(\pi^{*},P^*\bigr)}
\;\ge\;
\frac{\delta - 2\epsilon}{2\,\epsilon}.
\end{equation}
Thus, SGPO’s robust policy can outperform DPO by a factor of $\tfrac{\delta}{2\epsilon} - 1$ under sufficiently large distribution shift $\delta$.
\end{corollary}
\begin{proof}
    The proof is straightforward following the SGPO's regret bound and the DPO's regret lower bound.
\end{proof}

\section{SSAPO algorithm and Analysis on Computational Complexity}
\label{sec:SSAPO_algorithm+analysis}
\subsection{The SSAPO algorithm}
\label{sec:SSAPO_algorithm}
\begin{algorithm}[t!]
\caption{Stackelberg Self-Annotated Preference Optimization (SSAPO)}
\label{algo:ssapo}
\begin{algorithmic}[1]
\REQUIRE 
  \emph{Seed labeled set} $\mathcal{D}_{\mathrm{seed}}$; 
  \emph{unlabeled data} $\mathcal{D}_{\mathrm{unlabeled}}$; 
  \emph{Wasserstein radius} $\epsilon$; 
  \emph{number of linear pieces} $K$; 
  \emph{max iterations} $T$.

\STATE Initialize policy $\theta_0$, set $\mathcal{D} \!\leftarrow \!\mathcal{D}_{\mathrm{seed}}$.

\FOR{$t=0$ \textbf{to} $T-1$}
    \STATE \textbf{(Self-Annotation)}: 
      From $\mathcal{D}_{\mathrm{unlabeled}}$, sample prompts, generate \& rank responses under
      $\pi_{\theta_t}$, add new preference pairs $(y_w,y_l)$ to $\mathcal{D}$.

    \STATE \textbf{(Form $\hat{P}_N$)}:
      For each $(y_w^i,y_l^i)\in \mathcal{D}$, define $\hat{\xi}_i = R_{\theta_t}(y_w^i)-R_{\theta_t}(y_l^i)$,
      and let $\hat{P}_N=\tfrac1N\sum_{i=1}^N \delta_{\hat{\xi}_i}$.

    \STATE \textbf{(Concave Pieces)}:
      Choose $K$ linear functions $\ell_k(\cdot)$ such that
      $\widetilde{\ell}(\xi)=\max_{1\le k\le K}\ell_k(\xi)\le -\!\log\,\sigma(\xi)$.
      E.g., pick $K$ equidistant $\{\xi^{(k)}\}$ in $[0,1]$ (or a suitable domain) 
      and define $\ell_k(\cdot)$ to be a chord or tangent from below.

    \STATE \textbf{(Worst-Case Distribution)}:
      Solve the DRO finite convex program 
      $$
      P_t^* 
      \;\in\;
      \arg\max_{P\in B_\epsilon(\hat{P}_N)}
      \;\;\mathbb{E}_P\bigl[\widetilde{\ell}(\xi)\bigr].
      $$
      By Theorem~\ref{thm:worst_case_concave}, $P_t^*$ is discrete with atoms 
      $\bigl\{\hat{\xi}_i - \tfrac{q_{ik}^*}{\alpha_{ik}^*}\bigr\}$ 
      and weights $\alpha_{ik}^*/N$.

    \STATE \textbf{(Policy Update)}:
      Update $\theta_{t+1}$ by minimizing
      $\mathbb{E}_{P_t^*}\!\bigl[-\!\log\,\sigma(R_{\theta}(y_w)-R_{\theta}(y_l))\bigr]$
      (or a logistic/contrastive variant), via standard gradient methods.

\ENDFOR

\STATE \textbf{return} $\theta_T$ (final policy).
\end{algorithmic}
\end{algorithm}


\subsection{Computational Complexity of SSAPO}
\label{appendix:complexity}

In this subsection, we analyze the computational costs incurred by each step of the Stackelberg Self-Annotated Preference Optimization (SSAPO) algorithm (Algorithm~\ref{algo:ssapo}). We denote:
\begin{itemize}
    \item $N$: the total number of preference pairs in the dataset $\mathcal{D}$ at a given iteration,
    \item $K$: the number of linear pieces used in the concave piecewise approximation of $-\!\log \sigma(\xi)$,
    \item $T$: the total number of outer iterations for SSAPO.
\end{itemize}
We assume each \emph{iteration} refers to Steps~1--5 of Algorithm~\ref{algo:ssapo}.

\paragraph{Step 1 (Self-Annotation)}
The cost of self-annotation depends on the number of prompts and the policy's inference procedure. Let $M_t$ denote the number of new prompts labeled at iteration $t$. Generating and ranking responses under $\pi_{\theta_t}$ typically dominates this step. If:
\begin{itemize}
    \item $G_t$ is the number of candidate responses generated per prompt,
    \item $C_{\mathrm{inference}}$ is the average cost of a single forward pass (token generation) under $\pi_{\theta_t}$,
\end{itemize}
then the time complexity for Step~1 is approximately
$$
\mathcal{O}\bigl(M_t \cdot G_t \cdot C_{\mathrm{inference}}\bigr),
$$
plus any overhead for storing new winner--loser pairs in $\mathcal{D}$. Since the number of newly added preferences grows over iterations, $N$ itself typically increases from iteration to iteration.

\paragraph{Step 2 (Forming \texorpdfstring{$\hat{P}_N$}{the Empirical Distribution})}
\label{appendix:step2}
Once $\mathcal{D}$ is updated, we compute $\hat{\xi}_i = R_{\theta_t}(y_w^i) - R_{\theta_t}(y_l^i)$ for each pair. The cost here depends on:
\begin{itemize}
    \item $N$, the current size of $\mathcal{D}$,
    \item $C_{\mathrm{reward}}$, the average cost to compute $R_{\theta_t}(y) = \beta \,\log\frac{\pi_{\theta_t}(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)}$ for a given response $y$.
\end{itemize}
Because each preference pair requires evaluating $R_{\theta_t}$ on $(y^i_w, y^i_l)$, this step has complexity
$$
\mathcal{O}\bigl(N \cdot C_{\mathrm{reward}}\bigr).
$$
In practical implementations, $R_{\theta_t}(y)$ often just reads off the log-probabilities from $\pi_{\theta_t}$ and $\pi_{\mathrm{ref}}$ at the final tokens, making $C_{\mathrm{reward}}$ similar to a single forward-pass cost per response.

\paragraph{Step 3 (Concave Piecewise Approximation)}
We construct $K$ linear functions $\ell_k(\xi)$ such that $\widetilde{\ell}(\xi) \!= \max_{1\le k\le K} \ell_k(\xi) \le -\log\,\sigma(\xi)$. In principle, one can precompute these $K$ pieces over a small interval (e.g., $[0,1]$) once and reuse them in every iteration. Hence, the complexity for updating or verifying the piecewise function at iteration $t$ is typically:
$$
\mathcal{O}(K),
$$
assuming $\{\xi^{(k)}\}_{k=1}^K$ are fixed or can be quickly adapted based on the range of $\{\hat{\xi}_i\}$. This step is therefore relatively cheap compared to distributionally robust optimization.

\paragraph{Step4 (Worst-Case Distribution)}
Step~4 solves the \emph{distributionally robust optimization} (DRO) finite convex program 
$$
P_t^* \;=\;\arg\max_{P\in B_\epsilon(\hat{P}_N)}\;\mathbb{E}_{P}\bigl[\widetilde{\ell}(\xi)\bigr].
$$
The \emph{naive} formulation (per \citep{Esfahani2018Data}) becomes high-dimensional if $N$ is large, because each sample point $\hat{\xi}_i$ and each piecewise component $\ell_k$ introduces auxiliary variables (such as $\alpha_{ik}, q_{ik}$). Concretely, the number of decision variables can scale like $\mathcal{O}(N \cdot K)$, and the resulting linear or convex program might require $\mathcal{O}((N K)^\gamma)$ time in the worst case for some exponent $\gamma > 1$ (depending on the chosen solver and constraints).

However, several factors can reduce this cost:
\begin{itemize}
    \item \textbf{Approximate Solvers.} In practice, specialized cutting-plane or primal-dual methods solve these DRO problems more efficiently than the worst-case theoretical bound.
    \item \textbf{Grouping Heuristics.} If one partitions the $N$ samples into smaller groups (each of size $G < N$), the complexity per group is $\mathcal{O}((G K)^\gamma)$. Then one aggregates $M = \frac{N}{G}$ group-level solutions. This lowers the complexity significantly if $G \ll N$.
\end{itemize}
Hence, the worst-case step here is often $\mathcal{O}(N \cdot K)$ to $\mathcal{O}((N K)^\gamma)$, but can be much more tractable in practice with grouping or approximate methods. Regardless, Step~4 typically dominates the iteration complexity for large $N$.

\paragraph{Step 5 (Policy Update)}
Finally, we minimize 
$$
\mathbb{E}_{P_t^*}\bigl[-\!\log\,\sigma(R_{\theta}(y_w) - R_{\theta}(y_l))\bigr]
$$
via standard gradient methods. Each gradient step requires sampling from (or reweighting by) $P_t^*$ and evaluating $\sigma(\cdot)$, plus $R_{\theta}(y)$ for $y\in\{y_w, y_l\}$. If $B$ is the mini-batch size for gradient descent, then each policy update epoch scales approximately as 
$$
\mathcal{O}\bigl(N \cdot C_{\mathrm{reward}}\bigr),
$$
assuming each of the $N$ preference pairs in $P_t^*$ can be sampled over multiple epochs. In many implementations, $N$ can be large, so the training complexity depends heavily on how many gradient epochs or passes one uses at iteration $t$. 

\paragraph{Overall Complexity per Iteration}
Putting the above pieces together, let us summarize the dominating terms:

\begin{enumerate}
    \item \emph{Self-Annotation (Step~1)}: $\mathcal{O}(M_t \cdot G_t \cdot C_{\mathrm{inference}})$,
    \item \emph{Forming $\hat{P}_N$ (Step~2)}: $\mathcal{O}(N \cdot C_{\mathrm{reward}})$,
    \item \emph{Concave Piecewise Approx. (Step~3)}: $\mathcal{O}(K)$,
    \item \emph{Worst-Case Distribution (Step~4)}: $\mathcal{O}((N K)^\gamma)$ in the naive case, often reduced by grouping,
    \item \emph{Policy Update (Step~5)}: $\mathcal{O}(N \cdot C_{\mathrm{reward}} \cdot \mathrm{(\text{number of epochs})})$.
\end{enumerate}

If we denote the cost of solving the DRO subproblem by $C_{\mathrm{DRO}}(N,K)$ (which could itself be significantly reduced by grouping into subproblems of size $G$), then each iteration of SSAPO costs approximately:
$$
\mathcal{O}\Bigl(M_t \cdot G_t \cdot C_{\mathrm{inference}} + N \cdot C_{\mathrm{reward}} + C_{\mathrm{DRO}}(N,K) + \ldots \Bigr).
$$
In most scenarios, \emph{either} the distributionally robust optimization (Step~4) \emph{or} the gradient-based policy update (Step~5) will be the main bottleneck, depending on solver implementation and whether grouping is employed.

\paragraph{Total Complexity over $T$ Iterations}
Over $T$ total iterations, we multiply the above per-iteration cost by $T$. Additionally, note that $N$ can increase each iteration if new self-annotated preferences are continuously appended to $\mathcal{D}$. Denoting $N_t$ as the dataset size at iteration $t$, the total complexity from Steps 2--5 is roughly $\sum_{t=0}^{T-1} \Bigl[\mathcal{O}(N_t \cdot C_{\mathrm{reward}}) + C_{\mathrm{DRO}}(N_t,K)\Bigr]$, plus the self-annotation cost from Step~1. If $N$ grows in a controlled manner (for example, linearly in $t$), the cumulative cost can be bounded accordingly.

\paragraph{Practical Guidelines.}
\begin{itemize}
    \item \textbf{Grouping for DRO.} To handle large $N$, we recommend partitioning the data into multiple groups $G \ll N$. The overall complexity then becomes $\mathcal{O}\bigl(M\cdot C_{\mathrm{DRO}}(G,K)\bigr)$, where $M = N/G$, which can be significantly faster in practice.
    \item \textbf{Caching Log-Probabilities.} The reward $R_{\theta_t}(y)$ can be computed from log-probabilities of $\pi_{\theta_t}$ and $\pi_{\mathrm{ref}}$. Caching or reusing these values may reduce $C_{\mathrm{reward}}$.
    \item \textbf{Adjusting $K$.} Increasing $K$ refines the concave approximation but grows the size of the DRO problem. Hence, $K$ is a hyperparameter balancing approximation quality and computational overhead.
\end{itemize}

Overall, the time complexity of SSAPO grows with $N$, $K$, and the iteration count $T$. By employing grouping and efficient solvers, We can typically achieve robustness benefits without incurring excessive computational cost.


\section{More Details of Experimental Setups}
\label{sec:experimental_details_more}

\subsection{Detailed Experimental Setups}
We introduce more detailed experimental setups in Section~\ref{sec:experiments} as follows.

\paragraph{Datasets.}
For preference learning, we employed the UltraFeedback dataset~\cite{Cui2023Ultrafeedback}\footnote{\href{https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned}{argilla/ultrafeedback-binarized-preferences-cleaned}}, aligning with prior research~\cite{Rosset2024Direct, Kim2025Spread}. Specifically, we extracted a seed dataset comprising 2K samples (3.3\% of the total 60K training samples), which included prompts, responses, and ground-truth preference labels. These ground-truth preference labels are referred to as gold labels in Table~\ref{tab:main_results}. The remaining training samples were then partitioned into three subsets of 8K, 20K, and 30K samples, retaining only the prompts. These subsets were utilized as the prompt sets for the 1st, 2nd, and 3rd iteration stages, respectively.

\paragraph{Models.}
Following previous work~\cite{Kim2025Spread}, we primarily conducted our experiments using the supervised fine-tuned Mistral-7B-0.1 model~\cite{jiang2023mistral} as the initial model $\pi_{\text{init}}$. Specifically, we used the open-sourced model\footnote{\href{https://huggingface.co/alignment-handbook/zephyr-7b-sft-full}{alignment-handbook/zephyr-7b-sft-full}} that follows the recipe of Zephyr~\cite{tunstall2023zephyr} and is fine-tuned on the instructions of UltraChat~\cite{ding2023enhancing}. In Table~\ref{tab:main_results}, we also used LLaMA-3-8B\footnote{\href{https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct}{meta-llama/Meta-Llama-3-8B-Instruct}} to validate the compatibility of our method across different models. We used the generally fine-tuned models as there are no models that have been fine-tuned on the UltraChat dataset.

\paragraph{Evaluations.}
Following standard practices for aligning LLMs, we employed two primary evaluation benchmarks to assess model performance. First, we used \textbf{AlpacaEval 2.0}~\cite{dubois2024alpacafarm, dubois2024length}, a benchmark designed to approximate human preferences in instruction-following tasks. This evaluation involves 805 diverse instructions sourced from multiple datasets, where responses from the model under test are compared against those generated by GPT-4~\cite{achiam2023gpt} to determine win rates. To address potential biases related to response length—a known factor influencing LLM preferences~\cite{zheng2023judging, wang2023far}, we report both the original win rate and a length-controlled (LC) win rate. The LC win rate is calculated using a regression model trained to neutralize the impact of response length, thereby focusing on the quality of the generated content~\cite{dubois2024length}.

Second, we employed \textbf{MT-Bench}~\cite{zheng2023judging} to evaluate the model’s capabilities across a broader range of tasks. MT-Bench assesses a chatbot’s performance in areas such as math, coding, role-playing, and writing through multi-turn interactions. Responses are scored by GPT-4, providing a comprehensive measure of the model’s proficiency in key LLM functionalities. Together, these benchmarks offer a robust evaluation of how well the model aligns with human preferences and its effectiveness in real-world applications.

\paragraph{Implementation Details.}
In the initial alignment phase, we train the model using Direct Preference Optimization (DPO) on a seed dataset of 2K samples to obtain the base model $\pi_0$. Following this, we conduct 3 iterative stages of data expansion. In the $i$-th iteration ($i = 1, 2, 3$), we generate preference data by independently sampling two responses for each prompt using a temperature of 0.7 and labeling them as chosen or rejected through $R(x, y)$, resulting in a preference dataset $\{\xi_i\}_{i=1}^N$ ($N$ is the size of the $i$-th prompt set). Following SPA~\citep{Kim2025Spread}, we restricted the maximum token length for self-generated responses to 300 tokens. This limit corresponds to approximately 900 characters. To model the worst-case distribution program, we define a set of linear functions $\ell_k(x) = - \frac{K}{k}(x - \frac{k}{K}) - \log (\frac{k}{K})$ for $k = 1, \dots, K$ (the family of tangents of the loss function at the $K$-equipartition of $[0,1]$). We solve the associated optimization program using the Sequential Least Squares Programming (SLSQP) method. The group size $G$ is set to 100 unless otherwise specified for parallel computation of the convex program. Finally, we update the policy model by minimizing the reweighted loss to get $\pi_i$, ensuring improved alignment with the desired preferences.

\paragraph{Hyper-parameters for Different LLMs. }
For \textbf{Mistral-7B-0.1}, We set $ \text{learning rate} = 5 \times  10^{-7}$ and DPO hyper-parameter  $\beta = 0.1$ throughout the entire preference learning process. We conduct 3 epoch for the initial DPO training and 3 iteration for SSAPO game play (leader-follower updates).

For \textbf{LLaMA-3}, We set $ \text{learning rate} = 1 \times  10^{-6}$ and DPO hyper-parameter $\beta = 0.05$ throughout the entire preference learning process. We conduct 1 epoch for the initial DPO training and 2 iteration for SSAPO game play (leader-follower updates).

\subsection{Construction of Seed Data}

Seed data (e.g. the initial labeled training data) has an impact on the performance of self-training algorithms. In our experiments, we explore two different methods of selecting seed data from the original training set, namely UltraFeedback dataset~\cite{Cui2023Ultrafeedback} . Ground truth labels (e.g. the preferred and dispreferred response) of the selected seed data are retained, while the labels of the remaining data are discarded. As a result, our proposed method use only a small proportion of ground truth labels.

\textbf{Random sampling.} We select seed data from the entire training dataset randomly. 

\textbf{Farthest point sampling.} Intuitively, the distribution of the seed data should "cover" as much part of the entire training dataset as possible, such that subsequent self-training iterations won't suffer from distribution shift. With this motivation, we first leverage the SFT model to obtain an embedding vector for each prompt in the dataset, and then use farthest point sampling algorithm to select a subset of the embeddings (and corresponding preference pairs) as seed data. Specifically, we feed each prompt into the SFT model, and extract the hidden state of the last token of the last layer as the embedding. We use cosine similarity as the distance metric for embedding vectors.

In our implementation, we find that Mistral-7B model is not sensitive to either seed data sampling method. And LLama-8B model enjoy a slightly better performance with respect to the farther point sampling.


\subsection{Infrastructure}

For all experiments, we utilized 4 A800 GPUs. Under this computational resource, generating responses for 10K prompts takes approximately 15 minutes, and preference judging for generated responses takes about 30 minutes. Solving worst-case distribution with Eq. \ref{eq:widetilde_dro} takes about 40 minutes. Policy update takes about 1 hour per 10K training data. 



\section{Qualitative Examples}
\label{sec:qualitative_examples}
In this section, we present qualitative examples of the responses from the models trained under different methods: DPO (only train on initial seed data using DPO), SPA\citep{Kim2025Spread}, and SSAPO (our proposed method).  The results are presented below. These examples show that the model trained under SSAPO provides more detailed and precise responses to various questions. 

\noindent\rule{\linewidth}{0.4mm}
\textbf{Prompt:}

The given text must be classified as offensive or not offensive.

You acted really crazy last night.

\noindent\rule{\linewidth}{0.2mm}
\textbf{DPO response:}

I do not have any information about the specific context or situation in which the statement \"you acted really crazy last night\" was made. Therefore, I am unable to determine whether it is classified as offensive or not. Please provide more context or details about the situation so that I can provide a more accurate response.

\noindent\rule{\linewidth}{0.2mm}
\textbf{SPA response:}

Based on the given text, it can be classified as "offensive" because it uses a colloquial and informal expression ("acted really crazy") to describe someone's behavior, which may convey a negative or derogatory connotation towards the person's actions. In formal or professional contexts, using such language may be considered inappropriate or unprofessional, and it could potentially offend the person involved or others who may be present. Using precise and objective language to describe behavior or actions is generally recommended in such situations to avoid any potential offense or misunderstanding. Therefore, using a more neutral or appropriate expression, such as "displayed unconventional behavior last night," may be a better option in this case.

\noindent\rule{\linewidth}{0.2mm}

\textbf{SSAPO response:}

Based on the given text "You acted really crazy last night," it is difficult to determine whether it is classified as offensive or not offensive. The text does not contain any explicit or direct insults, profanities, or derogatory language directed towards the person addressed. The phrase "acted really crazy" could be interpreted as a subjective assessment of the person's behavior or actions during the previous night, and may not necessarily be intended as an insult or criticism.

However, the tone or context in which the text is used could influence its perceived offensiveness. If the phrase "acted really crazy" is used sarcastically or with a condescending tone, it may be interpreted as an indirect insult or criticism aimed at the person's behavior or character. Alternatively, if the phrase is used casually or with a neutral tone, it may be interpreted as a straightforward observation or comment on the person's actions without any malicious intent.

In summary, the given text "You acted really crazy last night" may or may not be classified as offensive, depending on the tone, context, and intended meaning behind the phrase. Without additional information or context, it cannot be definitively labeled as either offensive or not offensive.

\noindent\rule{\linewidth}{0.4mm}

\noindent\rule{\linewidth}{0.4mm}
\textbf{Prompt:}

Verify the correctness of the given statement.

"For all integers j and k, if j and k are odd, then jk is odd."

\noindent\rule{\linewidth}{0.2mm}
\textbf{DPO response:}

The statement "For all integers j and k, if j and k are odd, then jk is odd" is correct.

Proof:

Let j and k be any two odd integers. 

Since j and k are both odd, there exist positive integers a and b such that:

j = 2a + 1 and k = 2b + 1 

where a and b are integers.

Multiplying j and k, we get:

jk = (2a + 1)(2b + 1)

= 2(2ab + a + b) + 1

= 2(2a*2b + a + b) + 1

= 2(2a*2b + 1) + (2*a + 2*b + 1)

Since 2a and 2b are even integers, the product 2(2a*2b + 1) is an odd integer. Also, (2*a + 2*b + 1) is an odd integer since it is of the form 2*c + 1 where c is the sum of two even integers (2a and 2b). Therefore, jk is an odd integer.

Hence, for all integers j and k, if j and k are odd, then jk is odd. 

Q.E.D.

\noindent\rule{\linewidth}{0.2mm}

\textbf{SPA response:}

The statement "For all integers j and k, if j and k are odd, then jk is odd" is correct.

To prove this statement, we can use mathematical induction on the sum of the absolute values of j and k, denoted by $\mid$j$\mid$ + $\mid$k$\mid$.

First, consider the base case where $\mid$j$\mid$ + $\mid$k$\mid$ = 1. In this case, either j = 1 and k = 0 or j = 0 and k = 1. In both cases, jk = 1, which is odd.

Next, assume the statement holds for all pairs of integers j and k such that $\mid$j$\mid$ + $\mid$k$\mid$ = n, where n is a positive integer. Let j and k be any two integers such that $\mid$j$\mid$ + $\mid$k$\mid$ = n + 2. Then, either $\mid$j$\mid$ = n + 1 and $\mid$k$\mid$ = 1 or vice versa. Without loss of generality, let $\mid$j$\mid$ = n + 1 and $\mid$k$\mid$ = 1. Then, j = n + 1 and k = 1 or j = -(n + 1) and k = -1. In both cases, jk = (n + 1) * 1 = n + 1, which is odd by the induction hypothesis.

By mathematical induction, the statement holds for all integers j and k if j and k are odd, and jk is odd. Thus, the statement is correct.

\noindent\rule{\linewidth}{0.2mm}

\textbf{SSAPO response:}

The given statement "For all integers j and k, if j and k are odd, then jk is odd" is correct.

To prove this statement, we need to show that for all integers j and k, if both j and k are odd, then their product jk is also odd.

Let j and k be two odd integers, i.e., j = 2n + 1 and k = 2m + 1, where n and m are any integers.

Then, their product jk can be written as:

jk = (2n + 1) * (2m + 1) = 4nm + 2n + 2m + 1
= 2(2nm + n + m) + 1

Since n, m are integers, the product 2nm + n + m is also an integer. Therefore, the above expression jk is equal to the sum of an even number (2(2nm + n + m)) and an odd number (1).

As the sum of two odd numbers is always even, the sum 2(2nm + n + m) + 1 is an odd number.

Hence, the product jk = (2n + 1) * (2m + 1) = 2(2nm + n + m) + 1 is an odd integer, which proves the given statement.

Therefore, the given statement "For all integers j and k, if j and k are odd, then jk is odd" is correct.

\noindent\rule{\linewidth}{0.4mm}


\noindent\rule{\linewidth}{0.4mm}
\textbf{Prompt:}

Find the answer that best describes the underlined SAT word. Select the correct option and explain the meaning of the underlined word.

Despite the \_cacophony, the student tried to study. 

A. Loud sounds

B. Difficult subject

C. Late hour

D. Low lighting

\noindent\rule{\linewidth}{0.2mm}
\textbf{DPO response:}

The correct option is A. Loud sounds. The word cacophony means "a loud and confusing mixture of sounds." In this sentence, it describes the noise level in the environment where the student is trying to study. Therefore, the underlined word cacophony refers to the loud sounds that are making it difficult for the student to concentrate.

\noindent\rule{\linewidth}{0.2mm}

\textbf{SPA response:}

A) Loud sounds (cacophony)

Explanation:

The word "cacophony" refers to a loud and unpleasant combination of sounds, typically characterized by a discordant or confusing mixture of noise. In the context of the sentence, "despite the \_cacophony" suggests that the student is trying to study despite the overwhelming and distracting noise around them. The underlined word "cacophony" modifies the phrase "loud sounds" and provides a descriptive adjective.

Option B) Difficult subject, C) Late hour, and D) Low lighting are not relevant to the given context as they do not refer to loud or unpleasant sounds. Option A) "loud sounds" is a synonym for "cacophony" and accurately describes the situation mentioned in the sentence.

Therefore, the correct option for the underlined word is A) Loud sounds (cacophony).

\noindent\rule{\linewidth}{0.2mm}

\textbf{SSAPO response:}

Option A: "Despite the cacophony, the student tried to study"

The underlined word "cacophony" means a loud or chaotic combination of sounds. It describes a situation where there is an overwhelming or unpleasant mixture of noise or sounds.

Cacophony is derived from the Greek words "kakos" meaning "bad" or "ugly" and "phone" meaning "sound". It is commonly used to describe a variety of loud or unpleasant sounds, such as traffic noise, barking dogs, screaming children, or discordant music. In the given sentence, the student is trying to study despite the distracting and disruptive noise or sounds, which creates a cacophony. The word "despite" implies that the student is facing an obstacle or hindrance, which is the excessive noise or chaos, but they still persist in their efforts to concentrate and study.

Therefore, the meaning of the underlined word "cacophony" is a loud or chaotic combination of sounds, which creates an unpleasant or distracting environment for the student's studying. Option A is the correct answer. Option B refers to a difficult subject, Option C refers to a late hour, and Option D refers to low lighting, none of which are relevant to the given context.

\noindent\rule{\linewidth}{0.4mm}