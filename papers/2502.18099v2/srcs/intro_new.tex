%\bibliographystyle{plainnat}
%\begin{thebibliography}{9}

%\bibitem[Christiano et~al.(2017)]{Christiano2017DeepRLHF}
%Paul Christiano, Jan Leike, Tom Brown, et~al.
%\newblock Deep reinforcement learning from human preferences.
%\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2017.

%\bibitem[Rafailov et~al.(2023)]{Rafailov2023DirectPreference}
%R~Rafailov, Mitchell Wortsman, Ludwig Schmidt, et~al.
%\newblock Direct preference optimization: Your language model is secretly a reward model.
%\newblock \emph{arXiv preprint arXiv:2305.17888}, 2023.

%\bibitem[Von Stackelberg(1934)]{VonStackelberg1934Marktform}
%Heinrich Von Stackelberg.
%\newblock \emph{Marktform und Gleichgewicht}.
%\newblock Julius Springer, 1934.

%\bibitem[Basar and Olsder(1999)]{Basar1999DynamicNG}
%Tamer Basar and Geert~Jan Olsder.
%\newblock \emph{Dynamic Noncooperative Game Theory}.
%\newblock SIAM, 1999.

%\bibitem[Villani(2008)]{Villani2008OptimalTW}
%Cédric Villani.
%\newblock \emph{Optimal transport: old and new}.
%\newblock Springer, 2008.

%\end{thebibliography}

\section{Introduction}
\label{sec:intro}
Recent breakthroughs in large language models (LLMs) have made it increasingly crucial to \emph{align} generated text with human preferences for both usability and safety \citep{Ouyang2022Training,Bai2022Training}. Traditional approaches such as Reinforcement Learning from Human Feedback (RLHF) \citep{Christiano2017Deep} and Direct Preference Optimization (DPO) \citep{Rafailov2023Direct} often require \emph{massive} amounts of meticulously curated preference data. Not only is gathering such a dataset expensive and time-consuming, but any mislabeling can propagate through iterative alignment stages \citep{Casper2023Open}, leading to suboptimal or even unsafe model behaviors. This raises a critical challenge: \emph{How can we achieve preference data-efficient alignment of language models while maintaining robustness to annotation noise?}

From the perspective of data efficiency and robustness, existing alignment approaches often suffer from two main issues:\emph{Self-annotation gaps} and \emph{Lack of equilibrium guarantees under noise}. Recent work explores self-annotation \citep{Lee2024Rlaif,Yuan2024Self,Kim2025Spread}, where an LLM generates labels for new prompt--response pairs instead of relying on humans. While this indeed lowers annotation cost, such methods often treat policy updates and preference annotation as disconnected processes. Consequently, once noisy synthetic preferences are generated, there is limited recourse if the LLM’s self-labels embed systematic biases or errors that can corrupt future training \citep{Chowdhury2024Provably}. Some adversarial training approaches \citep{Cheng2023Adversarial,Wu2024Towards} attempt to counter distributional shifts in preference data, but they often lack formal \emph{equilibrium} guarantees and can lead to unstable optimization cycles in practice. Furthermore, these adversarial approaches are not specifically tailored for data-scarce alignment regimes, thus limiting their applicability when human labels are extremely expensive. \emph{We delay a more thorough \textbf{related work} section in the Appendix~\ref{sec:related_supp}}.

To address these issues, we propose \textbf{Stackelberg Game Preference Optimization (SGPO)}, a framework that models alignment as a \emph{two-player} Stackelberg game between: a \emph{policy} (the leader), which aims to satisfy real human preferences, and an \emph{adversarial preference distribution} (the follower), which explores worst-case shifts within a defined Wasserstein ball of radius $\epsilon$. Drawing inspiration from Stackelberg dynamics \citep{Bacsar1998Dynamic} and optimal transport \citep{Villani2009Optimal}, SGPO ensures that the policy optimizes against the worst plausible shifts in preference data. Specifically, under $\epsilon$-bounded shifts (or annotation noise), we prove that the resulting policy’s regret is at most $\mathcal{O}(\epsilon)$ (see Section~\ref{sec:theory}), whereas standard DPO can incur \emph{linear} regret growth with respect to the magnitude of distribution mismatch. Although our analysis uses $\epsilon$-Wasserstein balls as a tractable model of moderate noise or mismatch, it remains relevant for practical alignment scenarios where annotation errors are not unbounded but still matter.

On top of SGPO, we develop the \textbf{Stackelberg Self-Annotated Preference Optimization (SSAPO)} (Section~\ref{sec:ssapo}) algorithm, a procedure aimed at drastically lowering human annotation needs. SSAPO starts with a small human-labeled seed (about $1/30$ of the usual scale in our experiments) and then: \emph{(1) Self-Annotates} newly sampled prompts by generating responses and extracting winner--loser pairs from the current policy's own comparisons. \emph{(2) Adversarially Reweights} these pairs within a Wasserstein ball of radius $\epsilon$, by solving a distributionally robust optimization (DRO) \citep{Esfahani2018Data} program,ensuring that potentially corrupted or unrepresentative synthetic preferences do not overwhelm the policy update.
By iterating these two steps, SSAPO instantiates the SGPO framework, preserving theoretical bounded-regret guarantees while yielding significant data-efficiency gains. In practice, we find that with only 1/30 of the usual human annotations (from the UltraFeedback dataset \citep{Cui2023Ultrafeedback}), SSAPO attains 35.82~\% GPT4 win-rate (24.44\% LC win-rate) on Mistral-7B, 40.12\% win-rate (33.33\% LC win-rate) on Llama3-8B-instruct. Which matches Mixtral Large (21.4\% win-rate and 32.7\% LC win-rate) and Llama3-70B-instruct (33.2\% winrate and 34.4\% LC win-rate) according to the AlphacaEval~2.0 leaderboard~\citep{dubois2024length}.% Concretely, within 3 self-annotation rounds, our model matches methods that use \textbf{30 times} more human-labeled data.

In summary, We formulate DPO-like alignment as a Stackelberg game, demonstrate \emph{existence} of an equilibrium, and establish that \emph{SGPO} achieves an $\mathcal{O}(\epsilon)$ regret bound under $\epsilon$-bounded noise, in contrast to the linear regret behavior of DPO when facing similarly scaled shifts.(Section~\ref{sec:theory}). We implement SGPO via \emph{SSAPO}, which combines self-annotation with distributionaly robust optimization.(Section~\ref{sec:ssapo}) %We tackle the concavity requirement of Standard DRO with piecewise-linear \emph{concave} envelope. We handle large-scale preference data via a \emph{uniform grouping} strategy for parallel subproblem solutions.  
 Extensive experiments show that SSAPO attains strong alignment performance with substantially fewer human labels, thereby showing its real-world viability for cost-effective preference alignment of LLMs (Section~\ref{sec:experiments}).

%Overall, our main contributions are: (1) \textbf{Theoretical Foundation} (Section~\ref{sec:theory}): We formulate alignment under limited labels as a Stackelberg game, demonstrate \emph{existence} of an equilibrium, and establish that \emph{SGPO} achieves an $\mathcal{O}(\epsilon)$ regret bound under $\epsilon$-bounded noise. This stands in contrast to the linear regret behavior of DPO when facing similarly scaled shifts.(Theorems~\ref{thm:existence_se}--\ref{thm:sgpo_regret}). (2) \textbf{Scalable Algorithm} (Section~\ref{sec:ssapo}):  We implement SGPO via \emph{SSAPO}, which combines self-annotation with distributionaly robust optimization (DRO). We tackle the concavity requirement of Standard DRO with piecewise-linear \emph{concave} envelope. We handle large-scale preference data via a \emph{uniform grouping} strategy for parallel subproblem solutions. (3) \textbf{Empirical Validation} (Section~\ref{sec:experiments}): Extensive experiments show that SSAPO attains strong alignment performance with substantially fewer human labels, thereby showing its real-world viability for cost-effective preference alignment of LLMs.
%By furnishing both formal guarantees and an effective, scalable method, we hope to advance the practice of LLM alignment under realistic labeling budgets.