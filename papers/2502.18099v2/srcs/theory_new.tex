\section{Theoretical Foundation: Stackelberg Game Preference Optimization Framework}
\label{sec:theory}
In this section, we introduce our \emph{Stackelberg Game Preference Optimization} (SGPO) framework. First, we recap the standard DPO approach (Section~\ref{sec:dpo_prelims}). We then formulate SGPO as a two-player Stackelberg game (Section~\ref{sec:sgpo_formulation}), prove the existence of an equilibrium and characterize its convergence under iterative updates (Sections~\ref{sec:existence and convergence}).  Lastly, we provide a regret analysis (Section~\ref{sec:regret}), showing that SGPO suffers at most $\mathcal{O}(\epsilon)$ regret under $\epsilon$-bounded shifts, while DPO’s regret can grow linearly in the distribution mismatch. Proofs in this section is delayed to Appendix~\ref{sec:theory_proofs}.

\subsection{Preliminaries: Preference Datasets and DPO}
\label{sec:dpo_prelims}

\paragraph{Preference-Ranked Dataset.}
We consider a dataset
$D \;=\; \{(x^i, y^i_w, y^i_l)\}_{i=1}^N,$where $x^i$ is a prompt (or context), and $(y^i_w, y^i_l)$ denote the \emph{winner} and \emph{loser} responses. This dataset is generally obtained by human judgments or, in some cases, by partial self-annotation.

\vspace{-0.1 in}
\paragraph{RLHF and KL Regularization.}
Classical RL from Human Feedback (RLHF; \citep{Christiano2017Deep}) trains a parameterized policy $\pi_\theta$ by maximizing a reward $R(x,y)$ subject to staying close to a reference policy $\pi_{\text{ref}}$.  A common formulation is:
\begin{small}
   \begin{equation}
\label{eq:rlhf_obj_appendix}
\max_{\theta \in \Theta}
\mathbb{E}_{x \sim \mathcal{D},\, y \sim \pi_\theta(\cdot \mid x)}
\Bigl[
  R(x,y)
  -
  \beta D_{\mathrm{KL}}\!\bigl(\pi_\theta(\cdot\mid x)\,\Vert\,\pi_{\mathrm{ref}}(\cdot\mid x)\bigr)
\Bigr],
\end{equation} 
\end{small}
\noindent 
where $\beta$ controls the strength of the KL penalty.  The distribution $\mathcal{D}$ is typically the distribution of prompts seen during training or evaluation.

\paragraph{Direct Preference Optimization (DPO).}
\citet{Rafailov2023Direct} introduced an alternative that bypasses explicit reward modeling (i.e., estimating $R(x,y)$ seperately) by leveraging the Bradley-Terry (BT) pairwise preference model:
$$
p(y_w \succ y_l \mid x)
\;=\;
\sigma\Bigl(R(x, y_w) - R(x, y_l)\Bigr),
$$
where $\sigma(z) = \frac{1}{1 + e^{-z}}$. Under first-order optimality of a related KL-regularized objective, the optimal reward for a given policy $\pi_\theta$ must take the form
\begin{equation}
\label{eq:optimal_reward_supp}
R(x,y)
=
\beta \,\log \frac{\pi_\theta(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)} \;+\;\beta \,\log Z(x),
\end{equation}
with $Z(\cdot)$ a partition function. Substituting this form into the BT model yields a \emph{direct} method to optimize $\theta$ by maximum likelihood optimization with preference pairs,
\begin{small}
\begin{equation}
\label{eq:dpo_loss_supp}
\mathcal{L}_{\mathrm{DPO}}(\theta)
=\max_{\theta \in \Theta}
\mathbb{E}_{-log(x,y_w,y_l) \sim D}
\Bigl[
  \log\,
  \sigma\bigl(R(x,y_w)-R(x,y_l)\bigr)
\Bigr].
\end{equation}
\end{small}
\noindent
Despite its appealing simplicity, DPO lacks a built-in mechanism for handling shifts away from the empirical distribution of preferences in $D$.  As a result, if future or adversarial data differ substantially from the training set, DPO can incur large performance drops \citep{Chowdhury2024Provably}. This motivates a more robust approach.


\subsection{SGPO: A Two-Player Stackelberg Game}
\label{sec:sgpo_formulation}

We propose to defend against distributional uncertainty by treating the learning process as a two-player Stackelberg game \citep{Bacsar1998Dynamic}.  Concretely:

\begin{compactitem}
\item \textbf{Policy (the leader):} A policy model $\pi_\theta$, parameterized by $\theta \in \Theta \subset \mathbb{R}^d$.  This player chooses a parameter $\theta$ to \emph{maximize} its worst-case expected performance (likelihood) against an adversarial preference distribution.
\item \textbf{Adversarial Preference Distribution (the follower):} A distribution $P$ over pairwise outcomes $(y_w,y_l)$.  This player chooses, \emph{after} seeing $\theta$, a preference distribution within an $\epsilon$-ball of the empirical distribution $\hat{P}$\footnote{Throughout the paper, let $\hat{\xi}_i = R_\theta(x,y^i_w) - R_\theta(x,y^i_l)$,$i=1,\dots,N$, $N$ denotes the preference sample size. Define the empirical measure $\hat{P}_N = \tfrac{1}{N}\sum_i \delta_{\hat{\xi}_i}$. Thus the $\epsilon$-ball can be viewed as the neighbourhood of the observed preference distribution.}.  The follower’s goal is to \emph{minimize} the policy’s performance.
\end{compactitem}

To formalize “$\epsilon$-ball,” we adopt the 1-Wasserstein distance (Cf. Appendix~\ref{sec:Wasserstein_Prelim} for more preliminaries on the Wasserstein metric space)\citep{Villani2009Optimal} and define:
$$
\mathcal{U}_{\epsilon}(\hat{P})
\;=\;
\Bigl\{
   P \,\in\, \mathcal{P}(\mathcal{Y}\times \mathcal{Y})
   \;\Bigl|\;
   W\bigl(P,\hat{P}\bigr) \,\le\, \epsilon
\Bigr\}.
$$
Hence, the leader’s robust objective is:
\begin{equation}
\label{eq:sgpo_robust_obj}
\max_{\pi \in \Pi}
\;
\min_{P \,\in\, \mathcal{U}_\epsilon(\hat{P})}
\;
\mathbb{E}_{(y_w,y_l)\sim P}
\Bigl[
  \log \,\sigma\bigl(R_{\pi}(y_w) - R_{\pi}(y_l)\bigr)
\Bigr],
\end{equation}
where $R_{\pi}(y)$\footnote{We drop the term $x$ in $R(x,y)$ for simplicity hereafter.} is the policy-induced \emph{logit} (akin to the reward term Eq.~\eqref{eq:optimal_reward_supp}) or more generally a function measuring how favorable $y$ is under $\pi$.  This induces the Stackelberg equilibrium:

\begin{definition}[Stackelberg Equilibrium]
\label{def:se}
A pair $\bigl(\pi^*,\,P^*\bigr)$ is a \emph{Stackelberg equilibrium} if
\begin{numcases}{}
            \pi^*
\;\in\;
\arg\max_{\pi \in \Pi}
\;
\min_{P \,\in\, \mathcal{U}_\epsilon(\hat{P})}
\;
\mathbb{E}_{P}\!\bigl[\,J(\pi,\,P)\bigr], \ \{\textit{leader}\}\\
P^*
\;\in\;
\arg\min_{P \,\in\, \mathcal{U}_\epsilon(\hat{P})}
\;
\mathbb{E}_{P}\!\bigl[\,J(\pi^*,\,P)\bigr], \  \{\textit{follower}\}
\end{numcases}
where
\begin{equation}
J(\pi,P)
\;:=\;
\mathbb{E}_{(y_w,y_l)\sim P}
\Bigl[
  \log \,\sigma\bigl(R_{\pi}(y_w) \;-\; R_{\pi}(y_l)\bigr)
\Bigr].
\end{equation}
\end{definition}

Under real-world annotation noise or the noisy self-annotation scenario considered in this paper, the “true” preference distribution can \emph{deviate} from the empirical training data.  SGPO prepares for the worst-case shift within radius $\epsilon$.  By adopting a Stackelberg perspective, we derive a policy that is simultaneously (i) high-performing on the empirical data and (ii) robust to preference shifts.  



\subsection{Existence and Convergence of a Stackelberg Equilibrium}
\label{sec:existence and convergence}
Under standard regularity conditions (continuity, convexity, and compactness \citet{Villani2009Optimal, Esfahani2018Data}, confer Assumption~\ref{assump:regularity_extended} for details ) for distributionally robust optimization, we can prove that an Stackelberg equilibrium exist, and a natural alternating procedure converges to the Stackelberg equilibrium. 

\begin{theorem}[Existence of Stackelberg Equilibrium]
\label{thm:existence_se}
Under the regularity assumptions (Assumption~\ref{assump:regularity_extended}), the two-player game defined by
$$
\max_{\pi \in \Pi}
\;
\min_{P \,\in\, \mathcal{U}_\epsilon(\hat{P})}
\;\;
J(\pi,P),
$$
$$
\text{where} \quad
J(\pi,P)
=
\mathbb{E}_{P}\Bigl[\log \sigma\bigl(R_{\pi}(y_w) - R_{\pi}(y_l)\bigr)\Bigr]
$$
admits at least one Stackelberg equilibrium $\bigl(\pi^*,P^*\bigr)$.
\end{theorem}

A natural alternating procedure---iteratively updating the policy to best respond against the adversary, and then updating the adversary’s distribution within the $\epsilon$-ball---converges to the Stackelberg equilibrium.  One such procedure is:
\vspace{-0.05 in}
\begin{numcases}{}
\pi_{t+1}
\;=\;
\arg\max_{\pi \in \Pi}
\;
\min_{P \,\in\, \mathcal{U}_\epsilon(P_t)}
\;
J(\pi,P), \ \{\textit{leader}\}\label{eq:iterative_update_policy}\\
P_{t+1}
\;=\;
\arg\min_{P \,\in\, \mathcal{U}_\epsilon(P_t)}
\;
J(\pi_{t+1},P), \  \{\textit{follower}\}\label{eq:iterative_update_distribution}
\end{numcases}

starting from an initial pair $(\pi_0, P_0)$.  Here, we shift the center of the Wasserstein ball in each iteration to $P_t$.  

\begin{theorem}[Linear Convergence to Stackelberg Equilibrium]
\label{thm:convergence_iterative_detailed}
Under the regularity assumptions (Assumption~\ref{assump:regularity_extended}), the sequence $\{(\pi_t,P_t)\}_{t \ge 0}$ generated by \eqref{eq:iterative_update_policy}--\eqref{eq:iterative_update_distribution} converges to the Stackelberg equilibrium $(\pi^*,P^*)$.  Moreover, the convergence is \emph{linear}, i.e.\ there exists $\gamma < 1$ such that 
$$
\rho\bigl((\pi_{t+1},P_{t+1}),(\pi^*,P^*)\bigr)
\;\;\le\;\;
\gamma
\;\rho\bigl((\pi_t,P_t),(\pi^*,P^*)\bigr),
$$
where $\rho$ is a suitable metric (e.g., \ $\rho((\pi,P),(\pi',P'))=\|\pi-\pi'\|+ W(P,P')$).
\end{theorem}

In practice, one may not directly implement \eqref{eq:iterative_update_policy}--\eqref{eq:iterative_update_distribution}, but the Theorem~\ref{thm:convergence_iterative_detailed} shows that any procedure that approximates these alternating best-response updates can converge to the robust equilibrium.  This provides a theoretical grounding for the SSAPO algorithm (to be introduced in the section~\ref{sec:ssapo}), which combines standard gradient-based optimization with distributionally robust optimization.

\subsection{Regret Analysis and Comparison with DPO}
\label{sec:regret}
%A central benefit of SGPO is its ability to limit regret to $\mathcal{O}(\epsilon)$ when the true preference distribution resides within an $\epsilon$-Wasserstein ball of $\hat{P}_N$. By contrast, standard DPO can can incur regret $\propto \delta$, where $\delta$ is the magnitude of distribution shift. While real-world labeling noise may be more complex than a simple $\epsilon$-bounded assumption, the following analysis provides a theoretical foundation explaining why SGPO is more preferred than DPO under moderate but nontrivial distribution mismatch.
We now quantify SGPO’s performance under worst-case preference shifts.  We show that SGPO enjoys an $\mathcal{O}(\epsilon)$ bound on its regret, whereas DPO can incur regret $\propto \delta$, where $\delta$ is the magnitude of distribution shift.

\subsubsection{SGPO’s Bounded Regret}
Let $\pi^*$ be the SGPO policy obtained from the robust problem~\eqref{eq:sgpo_robust_obj}.  For any distribution $P\in\mathcal{U}_\epsilon(\hat{P})$, we define the (absolute) performance as
\begin{equation}
\mathcal{P}\!\bigl(\pi, P\bigr)
\;=\;
\mathbb{E}_{(y_w,y_l)\sim P}
\Bigl[
  \log\,\sigma\bigl(R_{\pi}(y_w)-R_{\pi}(y_l)\bigr)
\Bigr].
\end{equation}
We prove that $\pi^*$ maintains high performance on \emph{all} distributions $P$ within $\epsilon$-Wasserstein distance of $\hat{P}$.  In particular, the drop from $\hat{P}$ to any $P$ is at most $\mathcal{O}(\epsilon)$.

\begin{theorem}[Worst-Case Performance Guarantee for SGPO]
\label{thm:sgpo_regret_bound}
Under Assumption~\ref{assump:regularity_extended}, let $\pi^*$ be the SGPO solution.  Then for \emph{every} $P\in \mathcal{U}_\epsilon(\hat{P})$,
\begin{equation}
\mathcal{P}\bigl(\pi^*,P\bigr)
\;\;\ge\;\;
\mathcal{P}\bigl(\pi^*,\hat{P}\bigr)
\;-\;
L_R\,\epsilon.
\end{equation}
In other words, the performance drop from $\hat{P}$ to any $P\in \mathcal{U}_\epsilon(\hat{P})$ is at most $L_R\epsilon$.
\end{theorem}

\paragraph{Regret Notation.}
We define the regret of a policy $\pi$ on a distribution $P$ as
\begin{equation}
\text{Regret}\bigl(\pi,P\bigr)
\;=\;
\max_{\tilde{\pi}}\,\mathcal{P}\!\bigl(\tilde{\pi},P\bigr)
\;-\;
\mathcal{P}\!\bigl(\pi,P\bigr).
\end{equation}
If $\pi_P^*=\arg\max_{\tilde{\pi}}\mathcal{P}(\tilde{\pi},P)$, then
$\text{Regret}\bigl(\pi,P\bigr)=\mathcal{P}\bigl(\pi_P^*,P\bigr)-\mathcal{P}\bigl(\pi,P\bigr).$

\begin{theorem}[SGPO Regret Bound]
\label{thm:sgpo_regret}
For the SGPO policy $\pi^*$, we have
\begin{equation}
\sup_{P \,\in\,\mathcal{U}_\epsilon(\hat{P})}
\;\;
\text{Regret}\bigl(\pi^*,P\bigr)
\;\;\le\;\;
2\,L_R\,\epsilon.
\end{equation}
Thus, SGPO is robust: under any shift of at most $\epsilon$, its regret is bounded by a constant factor of $\epsilon$.
\end{theorem}

\subsubsection{Comparison: DPO’s Linear Regret}
\label{sec:compare_dpo}

Recall that DPO $\pi_{\mathrm{DPO}}$ \citep{Rafailov2023Direct} maximizes $\mathcal{P}(\pi,\hat{P})$ (Eq.~\eqref{eq:dpo_loss_supp} with no regard for shifts away from $\hat{P}$.  Let $\delta = W(\hat{P},P^*)$.  We show DPO can be arbitrarily suboptimal under large $\delta$, scaling linearly with $\delta$.

\begin{theorem}[DPO Regret Lower Bound]
\label{thm:dpo_regret_lowerbound}
Let $\pi_{\mathrm{DPO}}=\arg\max_{\pi}\mathcal{P}(\pi,\hat{P})$, and let $P^*$ be a distribution satisfying $W(\hat{P},P^*)=\delta$.  Then
\begin{equation}
\text{Regret}\bigl(\pi_{\mathrm{DPO}},\,P^*\bigr)
\;\ge\;
L_R\;\bigl(\delta - 2\,\epsilon\bigr).
\end{equation}
In particular, if $\delta \gg \epsilon$, DPO’s regret grows \emph{linearly} in $\delta$.
\end{theorem}

\begin{corollary}[SGPO Advantage Over DPO]
\label{cor:sgpo_advantage}
If $W\!\bigl(\hat{P},P^*\bigr)=\delta > 2\,\epsilon$, then
\begin{equation}
\frac{\text{Regret}\bigl(\pi_{\mathrm{DPO}},P^*\bigr)}%
     {\text{Regret}\bigl(\pi^{*},P^*\bigr)}
\;\ge\;
\frac{\delta - 2\epsilon}{2\,\epsilon}.
\end{equation}
Thus, SGPO’s robust policy can outperform DPO by a factor of $\tfrac{\delta}{2\epsilon} - 1$ under sufficiently large distribution shift $\delta$.
\end{corollary}

\paragraph{Discussion of Theoretical Results.}
Collectively, these results clarify \emph{why} SGPO is well-suited for preference alignment: if the “true” preference distribution lies within an $\epsilon$-Wasserstein ball of the empirical distribution, SGPO guarantees a mere $\mathcal{O}(\epsilon)$ penalty in regret, thereby remaining robust under moderate annotation noise or distribution shift.  Moreover, whereas DPO’s regret can grow linearly with the magnitude of the shift, SGPO constrains worst-case losses even in adversarial scenarios.  Finally, by treating preference alignment as a robust $\min_{P}$ optimization, SGPO can mitigate \emph{mismatches between labeled and unlabeled sets}, making it particularly appealing when human annotations are scarce or must be supplemented by self-annotation.  Altogether, these properties form the theoretical foundation for the practical algorithm described next (Section~\ref{sec:ssapo}). 

%\paragraph{Discussion of Theoretical Results.}
%Taken together, our theorems underscore \emph{why} SGPO is well-suited for preference alignment in practical settings: (1) Robustness to Noise and Shift: If the “true” preference distribution lies within an $\epsilon$-Wasserstein ball of $\hat{P}$, Theorem~\ref{thm:sgpo_regret} ensures SGPO’s regret increases by only $\mathcal{O}(\epsilon)$.  Thus, it gracefully handles real-world annotation noise or moderate distributional shifts. (2) Enhanced Safety in Adversarial Scenarios: Unlike DPO, whose regret can grow linearly with the distribution mismatch, SGPO constrains worst-case losses.  It remains stable under maliciously altered or systematically shifted preference data. (3) Control Under Limited Annotations:  When labeling every instance is impractical, SGPO’s robust $\min_{P}$ formulation mitigates the mismatch between labeled and unlabeled data.  As we detail in Section~\ref{sec:ssapo}, one can instantiate SGPO by generating a \emph{synthetic adversarial preference distribution} to approximate that worst-case inner optimization, thereby retaining theoretical guarantees with substantially fewer human labels.
%\begin{compactitem}
%    \item \textbf{Robustness to Noise and Shift.}  If the “true” preference distribution lies within an $\epsilon$-Wasserstein ball of $\hat{P}$, Theorem~\ref{thm:sgpo_regret} ensures SGPO’s regret increases by only $\mathcal{O}(\epsilon)$.  Thus, it gracefully handles real-world annotation noise or moderate distributional shifts.
%    \item \textbf{Enhanced Safety in Adversarial Scenarios.}  Unlike DPO, whose regret can grow linearly with the distribution mismatch, SGPO constrains worst-case losses.  It remains stable under maliciously altered or systematically shifted preference data.
%    \item \textbf{Control Under Limited Annotations.}  When labeling every instance is impractical, SGPO’s robust $\min_{P}$ formulation mitigates the mismatch between labeled and unlabeled data.  As we detail in Section~\ref{sec:ssapo}, one can instantiate SGPO by generating a \emph{synthetic adversarial preference distribution} to approximate that worst-case inner optimization, thereby retaining theoretical guarantees with substantially fewer human labels.
%\end{compactitem}

%\noindent
%In summary, the $\mathcal{O}(\epsilon)$-bounded worst-case regret and accompanying convergence properties make SGPO particularly compelling for aligning LLMs under noisy or partial human feedback. These results lay the groundwork for our practical self-annotation approach in the next section.
