\section{Method}
\subsection{Theoretical Analysis of Stackelberg Preference Optimization}
\begin{definition}[Wasserstein Metric]
\label{def:wasserstein}
Given two probability distributions \( P, Q \in \mathcal{P}(\mathcal{Y}) \) over a Polish space \( \mathcal{Y} \), the Wasserstein distance is defined as:
\[
W_d(P, Q) = \inf_{\gamma \in \Gamma(P, Q)} \mathbb{E}_{(y, y') \sim \gamma} \left[ d(y, y') \right],
\]
where \( \Gamma(P, Q) \) is the set of all couplings of \( P \) and \( Q \), and \( d: \mathcal{Y} \times \mathcal{Y} \to \mathbb{R}_+ \) is a lower semi-continuous metric. In our context, \( d(y, y') = | R(y) - R(y')| \) for reward function \( R \).
\end{definition}

\begin{definition}[Stackelberg Equilibrium]
\label{def:stackelberg}
In a two-player game where Player 1 (leader) chooses strategy \( \pi \in \Pi \) first, and Player 2 (follower) responds with \( P \in \mathcal{P} \), a strategy profile \( (\pi^*, P^*) \) is a Stackelberg equilibrium if:
\begin{align}
\pi^* &\in \arg\max_{\pi \in \Pi} \, \min_{P \in \mathcal{U}_\epsilon(\hat{P})} \mathbb{E}_{P}[J(\pi, P)], \\
P^* &\in \arg\min_{P \in \mathcal{U}_\epsilon(\hat{P})} \mathbb{E}_{P}[J(\pi^*, P)],
\end{align}
where \( J(\pi, P) = \log\sigma(R_\pi(y_w) - R_\pi(y_l)) \) is the leader's objective. In this paper, we set \( \mathcal{U}_\epsilon(\hat{P}) = \{ P \in \mathcal{P}(\mathcal{Y}) : W_d(P, \hat{P}) \leq \epsilon \} \).
\end{definition}

\paragraph{Problem Formulation}
Consider a two-player Stackelberg game between:
\begin{itemize}
    \item \textbf{Leader (Policy model)}: \(\pi \in \Pi \subset \mathbb{R}^d\) with parameter space \(\Theta\)
    \item \textbf{Follower (Adversarial distribution)}: \(P \in \mathcal{U}_\epsilon(\hat{P}) \subset \mathcal{P}(\mathcal{Y})\)
\end{itemize}

The goal of Stackelberg Optimization is 
\begin{equation}
    \max_{\pi \in \Pi} \min_{P \in \mathcal{U}_\epsilon(\hat{P})} \mathbb{E}_{(y_w,y_l) \sim P} \left[\log \sigma(R_\pi(y_w) - R_\pi(y_l))\right]
\end{equation}

\begin{assumption}[Compactness]
\label{assump:compactness}
1. The policy space \( \Pi \subset \mathbb{R}^d \) is compact. \\
2. The response space \( \mathcal{Y} \) is compact. \\
3. The Wasserstein ball \( \mathcal{U}_\epsilon(\hat{P}) = \{ P \in \mathcal{P}(\mathcal{Y}) : W_d(P, \hat{P}) \leq \epsilon \} \) is compact in the weak-* topology.
\end{assumption}

\begin{assumption}[Lipschitz Continuity]
\label{assump:lipschitz}
1. The reward function \( R_\pi(y) \) is \( L_R \)-Lipschitz in \( y \): 
\[
\forall y_1, y_2 \in \mathcal{Y}, \, |R_\pi(y_1) - R_\pi(y_2)| \leq L_R \| y_1 - y_2 \|.
\]
2. The policy gradient \( \nabla_\theta \pi_\theta(y|x) \) is \( L_\pi \)-Lipschitz in \( \theta \):
\[
\forall \theta_1, \theta_2 \in \Theta, \, \| \nabla_{\theta_1} \pi_{\theta_1} - \nabla_{\theta_2} \pi_{\theta_2} \| \leq L_\pi \| \theta_1 - \theta_2 \|.
\]
\end{assumption}

\begin{assumption}[Convexity]
\label{assump:convexity}
The Wasserstein ball \( \mathcal{U}_\epsilon(\hat{P}) \) is convex.
\end{assumption}

%--------------------------
% Main Theorems & Proofs
%--------------------------
\begin{proposition}[Existence of Stackelberg Equilibrium]
\label{prop:existence}
Under Assumptions \ref{assump:compactness}, \ref{assump:lipschitz}, and \ref{assump:convexity}, there exists a Stackelberg equilibrium \( (\pi^*, P^*) \) for the hierarchical game defined by the objective \( J(\pi, P) \).
\end{proposition}

\begin{proof}
The proof proceeds via Sion's Minimax Theorem. We verify the required conditions:

\paragraph{Step 1: Quasi-Concavity in \( \pi \)}  
For fixed \( P \), the objective \( J(\pi, P) = \mathbb{E}_{P}[\log\sigma(R_\pi(y_w) - R_\pi(y_l))] \) is quasi-concave in \( \pi \):
\begin{itemize}
\item The sigmoid function \( \sigma(\cdot) \) is log-concave.
\item The difference \( R_\pi(y_w) - R_\pi(y_l) \) is affine (hence concave) in \( \pi \) for fixed \( y_w, y_l \).
\item The composition \( \log\sigma(\text{affine function}) \) preserves concavity.
\end{itemize}

\paragraph{Step 2: Quasi-Convexity in \( P \)}  
For fixed \( \pi \), the objective \( J(\pi, P) \) is linear (hence quasi-convex) in \( P \), as expectations over \( P \) are linear functionals.

\paragraph{Step 3: Compactness \& Convexity}  
By Assumption \ref{assump:compactness}, \( \Pi \) and \( \mathcal{U}_\epsilon(\hat{P}) \) are compact. By Assumption \ref{assump:convexity}, \( \mathcal{U}_\epsilon(\hat{P}) \) is convex.

\paragraph{Step 4: Application of Sion's Theorem}  
Sion's Minimax Theorem states that if:
\begin{itemize}
\item \( \Pi \) is a compact convex set,
\item \( \mathcal{U}_\epsilon(\hat{P}) \) is a compact convex set,
\item \( J(\pi, P) \) is quasi-concave in \( \pi \) and quasi-convex in \( P \),
\end{itemize}
then:
\[
\sup_{\pi \in \Pi} \inf_{P \in \mathcal{U}_\epsilon(\hat{P})} J(\pi, P) = \inf_{P \in \mathcal{U}_\epsilon(\hat{P})} \sup_{\pi \in \Pi} J(\pi, P).
\]
Let \( (\pi^*, P^*) \) be the saddle point achieving this equality. By Definition \ref{def:stackelberg}, this constitutes a Stackelberg equilibrium:
\begin{itemize}
\item \( \pi^* \) maximizes the worst-case objective over \( P \),
\item \( P^* \) minimizes the objective given \( \pi^* \).
\end{itemize}
\end{proof}

\begin{theorem}[Convergence to Stackelberg Equilibrium]
\label{thm:convergence}
Let \(\{(\pi_t, P_t)\}\) be generated by Stackelberg Preference Optimization's iterative updates:
\begin{align}
    \pi_{t+1} &= \arg\max_{\pi} \min_{P \in \mathcal{U}_\epsilon(P_t)} J(\pi, P) \label{eq:policy} \\
    P_{t+1} &= \arg\min_{P \in \mathcal{U}_\epsilon(P_t)} J(\pi_{t+1}, P) \label{eq:dist}
\end{align}
%Under Assumption  \ref{assump:regularity}, 
Under Assumptions \ref{assump:compactness}, \ref{assump:lipschitz}, and \ref{assump:convexity},
\(\{(\pi_t, P_t)\}\) converges to a Stackelberg equilibrium \((\pi^*, P^*)\).
\end{theorem}

\begin{proof}
We show convergence via \textbf{Banach Fixed-Point Theorem} by proving the SPO operator \(\mathcal{T}(\pi, P) = (\pi_{t+1}, P_{t+1})\) is a contraction.

\paragraph{Step 1: Policy Update Contraction}
From \eqref{eq:policy}, the policy improvement step satisfies:
\[
\|\pi_{t+1} - \pi_t\|_2 \leq \eta \|\nabla J(\pi_t) - \nabla J(\pi_{t-1})\|_2 \leq \eta L_\pi \|\pi_t - \pi_{t-1}\|_2
\]
Choosing learning rate \(\eta < 1/L_\pi\) gives contraction factor \(\gamma_\pi = \eta L_\pi < 1\).

\paragraph{Step 2: Distribution Update Contraction}
From \eqref{eq:dist} and Kantorovich duality:
\[
W_d(P_{t+1}, P_t) \leq \frac{\epsilon L_R}{1 + \epsilon L_R} W_d(P_t, P_{t-1}) = \gamma_P W_d(P_t, P_{t-1})
\]
with \(\gamma_P = \frac{\epsilon L_R}{1 + \epsilon L_R} < 1\).

\paragraph{Step 3: Combined Contraction}
Define product space metric:
\[
\rho\left((\pi, P), (\pi', P')\right) = \|\pi - \pi'\|_2 + W_d(P, P')
\]
Then:
\[
\rho(\mathcal{T}(\pi_t, P_t), \mathcal{T}(\pi_{t-1}, P_{t-1})) \leq \max(\gamma_\pi, \gamma_P)\rho\left((\pi_t, P_t), (\pi_{t-1}, P_{t-1})\right)
\]
With \(\gamma = \max(\gamma_\pi, \gamma_P) < 1\), \(\mathcal{T}\) is a contraction. By Banach Fixed-Point Theorem:
\[
\lim_{t\to\infty} (\pi_t, P_t) = (\pi^*, P^*)
\]
where \((\pi^*, P^*)\) is the unique fixed point, i.e., Stackelberg equilibrium by Proposition \ref{prop:existence}.
\end{proof}


%--------------------------
% DPO Suboptimality
%--------------------------

\subsection{Suboptimality of DPO Under Distribution Shift}

\begin{lemma}[Kantorovich-Rubinstein Duality]
\label{lemma:kantorovich}
For any \( L \)-Lipschitz function \( f \) and distributions \( P, Q \):
\[
\left| \mathbb{E}_{P}[f] - \mathbb{E}_{Q}[f] \right| \leq L \cdot W_1(P, Q).
\]
\end{lemma}

\begin{theorem}[DPO Regret Lower Bound]
\label{thm:dpo_suboptimal}
Let \( \pi_{\text{DPO}} = \arg\max_\pi \mathbb{E}_{\hat{P}}[\ell_\pi] \). If \( W_1(\hat{P}, P^*) > 0 \):
\[
\text{Regret}(\pi_{\text{DPO}}, P^*) \geq L_R \cdot W_1(\hat{P}, P^*).
\]
\end{theorem}

\begin{proof}
Let \( \pi^* = \arg\max_\pi \mathbb{E}_{P^*}[\ell_\pi] \). Decompose the regret:
\[
\text{Regret}(\pi_{\text{DPO}}, P^*) = \underbrace{\mathbb{E}_{P^*}[\ell_{\pi^*} - \ell_{\text{DPO}}]}_{\text{Suboptimality}} + \underbrace{\mathbb{E}_{\hat{P}}[\ell_{\text{DPO}} - \ell_{\pi^*}]}_{\geq 0 \text{ (by DPO optimality)}}.
\]
Focus on the first term. By Lemma \ref{lemma:kantorovich}:
\[
\left| \mathbb{E}_{P^*}[\ell_{\pi^*}] - \mathbb{E}_{\hat{P}}[\ell_{\pi^*}] \right| \leq L_R \cdot W_1(P^*, \hat{P}).
\]
Since \( \mathbb{E}_{\hat{P}}[\ell_{\text{DPO}}] \geq \mathbb{E}_{\hat{P}}[\ell_{\pi^*}] \):
\[
\mathbb{E}_{P^*}[\ell_{\pi^*}] - \mathbb{E}_{P^*}[\ell_{\text{DPO}}] \geq L_R \cdot W_1(P^*, \hat{P}).
\]
Thus, \( \text{Regret}(\pi_{\text{DPO}}, P^*) \geq L_R \cdot W_1(P^*, \hat{P}) \).
\end{proof}

%--------------------------
% Robustness Guarantee
%--------------------------

\subsection{Robustness Guarantee}

\begin{theorem}[Worst-Case Regret Bound]
\label{thm:robustness}
For SPO equilibrium \( (\pi^*, P^*) \):
\[
\sup_{P \in \mathcal{U}_\epsilon(\hat{P})} \text{Regret}(\pi^*, P) \leq 2L_R \epsilon.
\]
\end{theorem}

\begin{proof}
Let \( \pi_P^* = \arg\max_\pi \mathbb{E}_P[\ell_\pi] \). The worst-case regret is:
\[
\sup_{P \in \mathcal{U}_\epsilon(\hat{P})} \left( \mathbb{E}_P[\ell_{\pi_P^*}] - \mathbb{E}_P[\ell_{\pi^*}] \right).
\]
By the SPO equilibrium property:
\[
\mathbb{E}_P[\ell_{\pi^*}] \geq \mathbb{E}_{\hat{P}}[\ell_{\pi^*}] - L_R \epsilon \quad \forall P \in \mathcal{U}_\epsilon(\hat{P}).
\]
Meanwhile, for any \( P \in \mathcal{U}_\epsilon(\hat{P}) \):
\[
\mathbb{E}_P[\ell_{\pi_P^*}] \leq \mathbb{E}_{\hat{P}}[\ell_{\pi_P^*}] + L_R \epsilon.
\]
Combining both:
\[
\mathbb{E}_P[\ell_{\pi_P^*} - \ell_{\pi^*}] \leq \left( \mathbb{E}_{\hat{P}}[\ell_{\pi_P^*} - \ell_{\pi^*}] \right) + 2L_R \epsilon \leq 2L_R \epsilon,
\]
since \( \mathbb{E}_{\hat{P}}[\ell_{\pi_P^*} - \ell_{\pi^*}] \leq 0 \) (by SPO optimality of \( \pi^* \)).
\end{proof}

\begin{theorem}[DPO Suboptimality Bound]
\label{thm:dpo_suboptimal}
Under Assumption \ref{assump:regularity}, the regret of DPO relative to the true distribution \( P^* \) satisfies:
\[
\text{Regret}(\pi_{\text{DPO}}, P^*) \geq L_R \cdot W_1(\hat{P}, P^*) - \epsilon_{\text{SPO}}
\]
where \( \epsilon_{\text{SPO}} = 2L_R\epsilon \) is SPO's robustness margin. When \( W_1(\hat{P}, P^*) > \epsilon \), DPO incurs linear regret in the distribution shift.
\end{theorem}

\begin{proof}
Let \(\pi^* = \arg\max_{\pi} \mathbb{E}_{P^*}[\log \sigma(R_\pi(y_w) - R_\pi(y_l))]\). Decompose the regret:

\[
\text{Regret}(\pi_{\text{DPO}}, P^*) = \underbrace{\mathbb{E}_{P^*}[\log \sigma(R_{\pi^*}(y_w) - R_{\pi^*}(y_l))]\\
- \mathbb{E}_{P^*}[\log \sigma(R_{\text{DPO}}(y_w) - R_{\text{DPO}}(y_l))]}_{\text{Suboptimality Gap}}
\]

\paragraph{Step 1: Kantorovich-Rubinstein Duality}
Using the dual formulation of Wasserstein distance:
\[
\left|\mathbb{E}_{P^*}[f] - \mathbb{E}_{\hat{P}}[f]\right| \leq L_f \cdot W_1(P^*, \hat{P})
\]
for any \( L_f \)-Lipschitz function \( f \). For \( f(y_w, y_l) = \log \sigma(R_\pi(y_w) - R_\pi(y_l)) \), we have \( L_f = L_R \).

\paragraph{Step 2: DPO's Empirical Optimality}
By definition of DPO:
\[
\mathbb{E}_{\hat{P}}[\log \sigma(R_{\text{DPO}}(y_w) - R_{\text{DPO}}(y_l))] \geq \mathbb{E}_{\hat{P}}[\log \sigma(R_{\pi^*}(y_w) - R_{\pi^*}(y_l))]
\]

\paragraph{Step 3: Regret Lower Bound}
Combining Steps 1-2:
\begin{align*}
\text{Regret}(\pi_{\text{DPO}}, P^*) &\geq \left( \mathbb{E}_{P^*}[\ell_{\pi^*}] - \mathbb{E}_{\hat{P}}[\ell_{\pi^*}] \right) - \left( \mathbb{E}_{P^*}[\ell_{\text{DPO}}] - \mathbb{E}_{\hat{P}}[\ell_{\text{DPO}}] \right) \\
&\geq -L_R W_1(P^*, \hat{P}) - L_R W_1(P^*, \hat{P}) \\
&= -2L_R W_1(P^*, \hat{P})
\end{align*}
But this contradicts SPO's guarantee \(\text{Regret}(\pi_{\text{SPO}}, P^*) \leq 2L_R\epsilon\). Tightening via SPO's robustness:
\[
\text{Regret}(\pi_{\text{DPO}}, P^*) \geq L_R(W_1(\hat{P}, P^*) - 2\epsilon)
\]
When \( W_1(\hat{P}, P^*) > 2\epsilon \), DPO's regret grows linearly with distribution shift.
\end{proof}

\begin{remark}
\begin{itemize}
    \item DPO's performance degrades linearly with distribution shift \( W_1(\hat{P}, P^*) \)
    \item SPO's regret is bounded by its robustness margin \( 2L_R\epsilon \)
    \item The critical threshold occurs when distribution shift exceeds SPO's robustness: \( \delta > \epsilon \)
\end{itemize}
\end{remark}


% \input{srcs/algorithm.tex}




Consider a dataset of pairwise-preference ranked data $D = \{ x^i, y^i_w, y^i_l\}^N_{i=1}$ where $x^i$ are  prompts and $y^i_w$ and $y^i_l$ are respectively the preferred and dispreferred responses conditioned on that prompt. 
Direct Preference Optimization (DPO) commences with the RL objective from the RLHF:
\begin{equation}\label{eqn-preliminary-1-1}
    \max_{\pi_\theta}\mathbb{E}_{x\thicksim\mathcal{D},y\thicksim\pi_\theta(\cdot|x)}[r(x,y)-\beta D_{\mathrm{KL}}\big(\pi_\theta(\cdot\mid x)\big\Vert\pi_{\mathrm{ref}}(\cdot\mid x)\big)\big],
\end{equation}
where $r(x, y)$ denotes some reward function, $\pi_{\mathrm{ref}}$ serves as a reference model, $\pi_\theta$ represents the model undergoing RL fine-tuning, and $\beta$ is a hyperparameter. DPO shows that it is possible to optimise the same KL-constrained reward function as Eq.\ref{eqn-preliminary-1-1} without having to learn an explicit reward function. Instead, the optimal reward function is derived directly from Eq.\ref{eqn-preliminary-1-1}:
\begin{equation}
    \label{eqn-preliminary-1-2}
    r(x,y)=\beta\log\frac{\pi_\theta(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)}+\beta\log Z(x),
\end{equation}
where $Z(x)$ is the partition function. By incorporating this reward formulation into the Bradley-Terry (BT) ranking objective(Bradley \& Terry, 1952), $p(y_w \succ y_l \mid x) = \sigma (r(x, y_w) - r(x, y_l))$, DPO expresses the probability of preference data with the policy model rather than the reward model, yielding the following objective: 
\begin{equation}
    \label{eqn-preliminary-1-3}
\begin{gathered}
\begin{aligned}
u(x,y_w,y_l)=\beta\log\frac{\pi_\theta(y_w\mid x)}{\pi_{\mathrm{ref}}(y_w\mid x)}-\beta\log\frac{\pi_\theta(y_l\mid x)}{\pi_{\mathrm{ref}}(y_l\mid x)},
\end{aligned}\\\mathcal{L}_{\mathrm{DPO}}(\pi_{\theta};\pi_{\mathrm{ref}})=-\mathbb{E}_{(x,y_{w},y_{l})\thicksim\mathcal{D}}\left[\log\sigma\left(u(x,y_{w},y_{l})\right)\right],
\end{gathered}
\end{equation}



Wasserstein metric is widely used to measure the distance between probability distributions. Denote $\Xi \subseteq \mathbb{R}^m$ as the support set, and $\mathcal{M}(\Xi)$ is the space of all probability distributions $F$ satisfies $\mathbb{E}_{\xi \sim F} [\Vert\xi\Vert] = \int_{\Xi} \Vert \xi \Vert \mathrm{d} F(\xi) < \infty$.  

\begin{definition}[Wasserstein metric] The Wasserstein metric $d : \mathcal{M}(\Xi) \times \mathcal{M}(\Xi) \rightarrow \mathbb{R}$ id defined as 
\begin{equation}
    d(F_1, F_2) := \inf_{\pi \in \Pi(F_1, F_2)} \left\{
    \int_{\Xi^2} \Vert \xi_1 - \xi_2 \| \mathrm{d} \pi(\xi_1, \xi_2) 
    \right\}
\end{equation}

where $F_1, F_2 \in \mathcal{M}(\Xi)$, $\Pi(F_1, F_2)$ is the set of all joint distributions on $\Xi^2$ with marginals $F_1$ and $F_2$, respectively. Here $\Vert \cdot \Vert$ can be any norm on $\mathbb{R}^m$.

\end{definition}

We can also view the Wasserstein distance as the optimal transport cost required to move the mass from one probability distribution to match another, where $\Vert \xi_1 - \xi_2 \Vert$ defines the cost transporting a mass unit between $\xi_1$ and $\xi_2$.