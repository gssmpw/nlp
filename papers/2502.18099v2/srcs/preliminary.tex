\section{Preliminaries}

\subsection{Direct Preference Optimization}
%麻烦天宇来帮忙写下 direct preference optimization 的preliminary，参考spread preference或者别的文章，讲清楚dpo，避免跟spread preference 重复
%For language generation, a language model (LM) is prompted with prompt (question) x to generate a response (answer) y, where both x and y consist of a sequence of tokens. 
Consider a dataset of pairwise-preference ranked data $D = \{ x^i, y^i_w, y^i_l\}^N_{i=1}$ where $x^i$ are  prompts and $y^i_w$ and $y^i_l$ are respectively the preferred and dispreferred responses conditioned on that prompt. 
%One of the most popular techniques for learning from preference data $D$ is direct preference optimisation (DPO) [Rafailov et al., 2023].  
Direct Preference Optimization (DPO) commences with the RL objective from the RLHF:
\begin{equation}\label{eqn-preliminary-1-1}
    \max_{\pi_\theta}\mathbb{E}_{x\thicksim\mathcal{D},y\thicksim\pi_\theta(\cdot|x)}[r(x,y)-\beta D_{\mathrm{KL}}\big(\pi_\theta(\cdot\mid x)\big\Vert\pi_{\mathrm{ref}}(\cdot\mid x)\big)\big],
\end{equation}
where $r(x, y)$ denotes some reward function, $\pi_{\mathrm{ref}}$ serves as a reference model, $\pi_\theta$ represents the model undergoing RL fine-tuning, and $\beta$ is a hyperparameter. DPO shows that it is possible to optimise the same KL-constrained reward function as Eq.\ref{eqn-preliminary-1-1} without having to learn an explicit reward function. Instead, the optimal reward function is derived directly from Eq.\ref{eqn-preliminary-1-1}:
\begin{equation}
    \label{eqn-preliminary-1-2}
    r(x,y)=\beta\log\frac{\pi_\theta(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)}+\beta\log Z(x),
\end{equation}
where $Z(x)$ is the partition function. By incorporating this reward formulation into the Bradley-Terry (BT) ranking objective(Bradley \& Terry, 1952), $p(y_w \succ y_l \mid x) = \sigma (r(x, y_w) - r(x, y_l))$, DPO expresses the probability of preference data with the policy model rather than the reward model, yielding the following objective: 
\begin{equation}
    \label{eqn-preliminary-1-3}
\begin{gathered}
\begin{aligned}
u(x,y_w,y_l)=\beta\log\frac{\pi_\theta(y_w\mid x)}{\pi_{\mathrm{ref}}(y_w\mid x)}-\beta\log\frac{\pi_\theta(y_l\mid x)}{\pi_{\mathrm{ref}}(y_l\mid x)},
\end{aligned}\\\mathcal{L}_{\mathrm{DPO}}(\pi_{\theta};\pi_{\mathrm{ref}})=-\mathbb{E}_{(x,y_{w},y_{l})\thicksim\mathcal{D}}\left[\log\sigma\left(u(x,y_{w},y_{l})\right)\right],
\end{gathered}
\end{equation}


\subsection{Wasserstein Metric Space}
%志心 写一下，参考Data-Driven Distributionally Robust Optimization Using the Wasserstein Metric

Wasserstein metric is widely used to measure the distance between probability distributions. Denote $\Xi \subseteq \mathbb{R}^m$ as the support set, and $\mathcal{M}(\Xi)$ is the space of all probability distributions $F$ satisfies $\mathbb{E}_{\xi \sim F} [\Vert\xi\Vert] = \int_{\Xi} \Vert \xi \Vert \mathrm{d} F(\xi) < \infty$.  

\begin{definition}[Wasserstein metric] The Wasserstein metric $d : \mathcal{M}(\Xi) \times \mathcal{M}(\Xi) \rightarrow \mathbb{R}$ id defined as 
\begin{equation}
    d(F_1, F_2) := \inf_{\pi \in \Pi(F_1, F_2)} \left\{
    \int_{\Xi^2} \Vert \xi_1 - \xi_2 \| \mathrm{d} \pi(\xi_1, \xi_2) 
    \right\}
\end{equation}

where $F_1, F_2 \in \mathcal{M}(\Xi)$, $\Pi(F_1, F_2)$ is the set of all joint distributions on $\Xi^2$ with marginals $F_1$ and $F_2$, respectively. Here $\Vert \cdot \Vert$ can be any norm on $\mathbb{R}^m$.

\end{definition}

We can also view the Wasserstein distance as the optimal transport cost required to move the mass from one probability distribution to match another, where $\Vert \xi_1 - \xi_2 \Vert$ defines the cost transporting a mass unit between $\xi_1$ and $\xi_2$.




\subsection{Problem Formulation}
%志心根据天宇的符号写problem setting, 现在先写self play吧，先不写noise
