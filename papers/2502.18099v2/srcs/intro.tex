\section{Introduction}
\paragraph{Problem \& Motivation:}
Aligning LLMs with human preferences is critical for safety and usability, but existing methods like Direct Preference Optimization (DPO) rely heavily on large-scale, high-quality human feedback—a resource-intensive and often impractical requirement.

Key Challenges:

Prohibitive Cost: Curating preference datasets (e.g., UltraFeedback) demands significant human labor, limiting accessibility for smaller organizations or low-resource domains.

Noisy Annotations: Human annotators make mistakes, and synthetic labels (e.g., from model self-annotation) inherit errors, degrading alignment performance.

Brittle Optimization: Current methods like DPO naively maximize the likelihood of observed preferences, overfitting to noise and leading to suboptimal policies.

Lack of Theoretical Guarantees: Few prior work formally links alignment robustness to annotation noise levels.

Limitations of Prior Work:
While recent work explores reducing human annotation via synthetic data or reward modeling, these approaches:

Lack robustness to annotation noise, as errors propagate through self-annotation loops.

Ignore strategic interactions between policy learning and data generation, treating them as separate stages.

(Self-annotation reduces costs but amplifies noise without theoretical safeguards)

Offer no theoretical guarantees for performance under noisy or imperfect supervision.

Adversarial training lacks equilibrium guarantees, leading to unstable optimization.

Our solution
Introduce SGPO and SSAPO as a two-stage framework:

Theoretical Foundation (SGPO):
Formalizes alignment as a Stackelberg game with leader-follower dynamics.

Proves convergence to equilibrium with bounded regret under noise.

Practical Algorithm (SSAPO):

Implements SGPO via self-annotation with synthetic noise injection.


Reduces reliance on human labels to 3\% via iterative self-annotation, minimizing annotation costs.


\paragraph{Theoretical Contribution}
Stackelberg Equilibrium Analysis: First formal proof of convergence for preference alignment under annotation noise.

Regret Bounds: 
$\mathcal{O}(\epsilon)$-bounded regret for SGPO vs. DPO’s linear degradation.

\paragraph{Methodological Contribution}
SASPO Framework: A Stackelberg game where:

Leader (policy): Uses SGD to optimize against worst-case rewards derived from adversarially perturbed preferences.

Follower (adversary): Constructs worst-case preference distributions via convex optimization over a Wasserstein uncertainty set (DRO-inspired).

Self-Annotating Mechanism: Bootstraps alignment with only 1/30 of seed human labels (UltraFeedback), then iteratively generates synthetic preferences via the policy itself.
(Matches SOTA performance on Alpaca Eval 2.0 with 1/30 labels.)


\paragraph{Empirical Contribution}

Achieves strong alignment performance on Alpaca Eval 2.0 with just 3 rounds of iterative self-annotation.

Reduces reliance on human labels by 30 times. %while outperforming DPO in noisy-label settings.



\section{Introduction}
\label{sec:intro}

Recent advances in large language models (LLMs) have underscored the importance of \emph{aligning} generated text with human preferences for both usability and safety. Yet, achieving high-quality alignment often entails expensive, large-scale data collection, as in Reinforcement Learning from Human Feedback \cite{Christiano2017DeepRLHF} and Direct Preference Optimization (DPO) \cite{Rafailov2023DirectPreference}. Reducing this dependence on massive annotated datasets is thus a pressing challenge for practical deployment of LLMs.

A central obstacle is that most alignment algorithms require vast amounts of carefully labeled prompts and responses, which may be infeasible or costly to obtain at scale. In addition, even minor annotation errors can degrade performance if the model is insufficiently guarded against imperfect supervision. This raises the question: \emph{How can we align powerful language models using only a fraction of the usual human-labeled data, while still maintaining high fidelity to human preferences?}

We address this question with a new approach, \textbf{Stackelberg Game Preference Optimization} (SGPO). At its core, SGPO models the alignment process as a two-player Stackelberg game \cite{VonStackelberg1934Marktform,Basar1999DynamicNG}. Concretely:
(A) A \textbf{leader} (the policy) chooses parameters to maximize performance on preference data.
(B) A \textbf{follower} (a preference distribution) selects possible shifts in the empirical preferences, restricted by a Wasserstein-ball of radius $\epsilon$.

By simultaneously accounting for policy updates and potential variations in preference data, SGPO yields a robust theoretical framework. Although the primary motivation of our work is data efficiency rather than adversarial robustness, SGPO still ensures, on paper, that under a bounded shift $\epsilon$ in preference distributions, the learned policy’s regret remains $\mathcal{O}(\epsilon)$ (see Section~\ref{sec:theory}). This theoretical lens underscores why a careful leader--follower setup can handle imperfectly annotated data more reliably than purely maximum likelyhood optimization.

Building on the SGPO framework, we propose \textbf{Stackelberg Self-Annotated Preference Optimization} (SSAPO) to address the high cost of data collection. SSAPO starts with a small \emph{seed} set of human labels and \emph{self-annotates} additional unlabeled samples using the current model. It then applies a distributionally robust step to weight or shift preferences within the allowable Wasserstein radius $\epsilon$. This procedure translates the theoretical idea of SGPO into a tractable iterative process, as described in Section~\ref{sec:ssapo}.

While the SGPO formulation can also be interpreted as robust to noise, our experiments primarily focus on \emph{data efficiency}. Concretely, we show that with only $\tfrac{1}{30}$ of the usual human annotations (from the UltraFeedback dataset), SSAPO can achieve competitive alignment on Llama3-8B-instruct. Within three self-annotation iterations, our method reaches a 40.12\% GPT-4 Turbo win rate (33.33\% length-controlled win rate) compared to baselines including those using substantially more annotated data. These results support the claim that a Stackelberg-based procedure can perform preference alignment in a label-efficient manner without sacrificing performance.

Overall, our contributions are:
\begin{itemize}
\item \textbf{A New Game-Theoretic View of Alignment.} We introduce SGPO, formulating alignment under limited human labels through a leader--follower framework (Section~\ref{sec:theory}). We prove an equilibrium exists (Theorem~\ref{thm:existence_se_detailed}) and provide convergence and regret analyses (Theorems~\ref{thm:convergence_iterative_detailed}--\ref{thm:sgpo_regret_supp}).
\item \textbf{Data-Efficient Algorithm (SSAPO).} We instantiate SGPO in practice with SSAPO, which combines self-annotation of unlabeled data with a distributionally robust reweighting step (Section~\ref{sec:ssapo}). 
\item \textbf{Empirical Validation.} Despite focusing on data efficiency rather than explicit noise injection, SSAPO achieves strong performance on Llama3-8B-instruct with only a fraction of standard labeling costs (Section~\ref{sec:experiments}).
\end{itemize}



\section{Introduction}
\label{sec:intro}

Aligning the outputs of large language models (LLMs) with human preferences is essential for both usability and safety. However, most existing alignment approaches---such as Reinforcement Learning from Human Feedback (RLHF; \cite{Christiano2017DeepRLHF}) and Direct Preference Optimization (DPO; \cite{Rafailov2023DirectPreference})---rely on large-scale, high-quality preference annotations. Collecting these annotations is notoriously costly, posing a significant obstacle to widespread deployment of aligned LLMs.

A central challenge lies in the need for vast amounts of carefully labeled prompt--response pairs. Such data are expensive to acquire and prone to noise or partial inconsistency. Even small annotation errors can destabilize alignment if the training procedure lacks robustness mechanisms. This raises a key question:
\textit{How can we align powerful language models using only a fraction of the usual human-labeled data, while still maintaining high fidelity to human preferences?}

We address this question with a new approach, \textbf{Stackelberg Game Preference Optimization} (SGPO). At its core, SGPO models the alignment process as a two-player Stackelberg game \cite{VonStackelberg1934Marktform,Basar1999DynamicNG}. Concretely:
(A) A \textbf{leader} (the policy) chooses parameters to maximize performance on preference data.
(B) A \textbf{follower} (a preference distribution) selects possible shifts in the empirical preferences, restricted by a Wasserstein-ball of radius $\epsilon$.


By unifying policy updates with adversarial shifts in preference data, SGPO provides rigorous distributional robustness. In particular, under $\epsilon$-bounded shifts (or annotation noise), we prove that the policy’s regret remains $\mathcal{O}(\epsilon)$ (see Section~\ref{sec:theory}), significantly improving on the linear regret growth of standard DPO under such shifts. While our theoretical development admits adversarial data distortions, our primary motivation is \emph{data efficiency}: we aim to tolerate and even leverage imperfectly or sparsely labeled data in a principled manner.

Building on SGPO, we develop a practical algorithm called \emph{Stackelberg Self-Annotated Preference Optimization} (SSAPO) (Section~\ref{sec:ssapo}). SSAPO starts with a small “seed” set of human-labeled preferences and then: (1) \textbf{Self-Annotates} unlabeled prompts by letting the current model generate responses and rank them to produce synthetic winner--loser pairs, and (2) \textbf{Reweights} these pairs adversarially within a Wasserstein ball of radius $\epsilon$, ensuring robustness to potential noise or imperfect self-annotations.

Through iterative self-annotation and robust preference reweighting, SSAPO concretely instantiates SGPO’s theoretical guarantees while drastically reducing human labeling needs. Concretely, we show that with only $\tfrac{1}{30}$ of the usual human annotations (from the UltraFeedback dataset), SSAPO can achieve competitive alignment on Llama3-8B-instruct. Within three self-annotation iterations, our method reaches a 40.12\% GPT-4 Turbo win rate (33.33\% length-controlled win rate) compared to baselines including those using substantially more annotated data. These results support the claim that a Stackelberg-based procedure can perform preference alignment in a label-efficient manner without sacrificing performance.

Overall, our main contributions are:
\begin{itemize}
    \item \textbf{A Game-Theoretic Perspective on Preference Alignment.}   We introduce SGPO, formulating alignment under limited human labels through a Stakelberg game framework (Section~\ref{sec:theory}). We prove the existence of an equilibrium and establish $\mathcal{O}(\epsilon)$-bounded regret under $\epsilon$-bounded shifts (Theorems~\ref{thm:existence_se_detailed}--\ref{thm:sgpo_regret_supp}).
    \item \textbf{A Data-Efficient Algorithmic Instantiation (SSAPO).}  
          We instantiate SGPO via self-annotation and distributionally robust reweighting, achieving computational tractability through a concave under-approximation and grouping scheme (Section~\ref{sec:ssapo}).
    \item \textbf{Empirical Validation.}  
          Extensive experiments show that SSAPO attains strong alignment performance with substantially reduced human labeling, thereby demonstrating its practicality for real-world LLM alignment (Section~\ref{sec:experiments}).
\end{itemize}

By providing both theoretical robustness guarantees and a scalable implementation, we hope SGPO and SSAPO open the door to more label-efficient, reliable alignment of large language models under realistic data constraints.

\bibliographystyle{plainnat}
\begin{thebibliography}{9}

\bibitem[Christiano et~al.(2017)]{Christiano2017DeepRLHF}
Paul Christiano, Jan Leike, Tom Brown, et~al.
\newblock Deep reinforcement learning from human preferences.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2017.

\bibitem[Rafailov et~al.(2023)]{Rafailov2023DirectPreference}
R~Rafailov, Mitchell Wortsman, Ludwig Schmidt, et~al.
\newblock Direct preference optimization: Your language model is secretly a reward model.
\newblock \emph{arXiv preprint arXiv:2305.17888}, 2023.

\bibitem[Von Stackelberg(1934)]{VonStackelberg1934Marktform}
Heinrich Von Stackelberg.
\newblock \emph{Marktform und Gleichgewicht}.
\newblock Julius Springer, 1934.

\bibitem[Basar and Olsder(1999)]{Basar1999DynamicNG}
Tamer Basar and Geert~Jan Olsder.
\newblock \emph{Dynamic Noncooperative Game Theory}.
\newblock SIAM, 1999.

\bibitem[Villani(2008)]{Villani2008OptimalTW}
Cédric Villani.
\newblock \emph{Optimal transport: old and new}.
\newblock Springer, 2008.

\end{thebibliography}


\section{Introduction}
\label{sec:intro}
Recent advances in large language models (LLMs) have underscored the importance of \emph{aligning} generated text with human preferences for usability and safety \citep{Ouyang2022Training, Bai2022Training}. Traditional alignment methods like Reinforcement Learning from Human Feedback (RLHF) \citep{Christiano2017Deep} and Direct Preference Optimization (DPO) \citep{Rafailov2024Direct} rely heavily on large-scale, high-quality preference datasets. However, collecting such data is prohibitively expensive, and even minor annotation errors can propagate through alignment stages \citep{Casper2023Open}. This raises a critical challenge: \emph{How can we achieve preference data-efficient alignment of language models while maintaining robustness to annotation noise?}

From the data-efficient and robust perspective, existing alignment methods suffer from two key limitations. First, aiming at data-efficiency, self-annotation techniques \citep{Lee2024Rlaif,Yuan2024Self,Kim2025Spread} label generated preferences with LLMs instead of human. However, they ignore the strategic interplay between policy updates and preference annotation, treating them as separate stages. This could lead to error accumulation, where noisy self-labels could corrupt subsequent training rounds \citep{Chowdhury2024Provably} . %Second, methods \citep{Rafailov2024Direct,Rosset2024Direct} bypassing the reward modeling procedure lack robustness to annotation noise of the preference data \citep{Chowdhury2024Provably}. 
Second, though there are some adversarial training methods \citep{Cheng2023Adversarial,Wu2024Towards} models noise in preference data. They lack equilibrium guarantees, which could result in unstable optimization cycles. Besides, the adversarial methods are not designed specially for data-efficient alignment These issues highlight the need for a principled approach that unifies data efficiency with theoretical safeguards against errors from self-annotation.

We address these challenges with \textbf{Stackelberg Game Preference Optimization} (SGPO), a framework that models alignment as a two-player game between a policy (leader) and an adversarial preference distribution (follower). Inspired by distributionally robust optimization \citep{Esfahani2018Data} and Stackelberg dynamics \citep{Bacsar1998Dynamic}, SGPO ensures the policy optimizes against worst-case preference perturbations within a Wasserstein-ball of radius $\epsilon$.  In particular, under $\epsilon$-bounded shifts (or annotation noise), we prove that the policy’s regret remains $\mathcal{O}(\epsilon)$ (see Section~\ref{sec:theory}), significantly improving on the linear regret growth of standard DPO under such shifts. While our theoretical development admits adversarial data distortions, our primary motivation is \emph{data efficiency}: we aim to tolerate and even leverage imperfectly or sparsely labeled data in a principled manner.

Building on SGPO, we develop a practical algorithm called \textbf{Stackelberg Self-Annotated Preference Optimization} (SSAPO) (Section~\ref{sec:ssapo}). SSAPO starts with a small “seed” set of human-labeled preferences and then: (1) \emph{Self-Annotates} unlabeled prompts by letting the current model generate responses and rank them to produce synthetic winner--loser pairs, and (2) \emph{Reweights} these pairs adversarially within a Wasserstein ball of radius $\epsilon$, ensuring robustness to potential noise or imperfect self-annotations.

Through iterative self-annotation and robust preference reweighting, SSAPO concretely instantiates SGPO’s theoretical guarantees while drastically reducing human labeling needs. Concretely, we show that with only $\tfrac{1}{30}$ of the usual human annotations (from the UltraFeedback dataset \citet{Cui2023Ultrafeedback}), SSAPO can achieve competitive alignment on Llama3-8B-instruct. Within three self-annotation iterations, our method reaches a 40.12\% GPT-4 Turbo win rate (33.33\% length-controlled win rate) compared to baselines including those using substantially more annotated data. These results indicate that a Stackelberg-based procedure can perform preference alignment in a human-label-efficient manner without sacrificing performance.

Overall, our main contributions are: 
\begin{itemize}
    \item Theoretically,  We introduce SGPO, formulating alignment under limited human labels through a Stakelberg game framework (Section~\ref{sec:theory}). We prove the existence of an equilibrium and establish $\mathcal{O}(\epsilon)$-bounded regret under $\epsilon$-bounded shifts (Theorems~\ref{thm:existence_se_detailed}--\ref{thm:sgpo_regret_supp}).
    \item Methodologically, we instantiate SGPO with the SSAPO algorithm via self-annotation and distributionally robust reweighting, achieving computational tractability through a concave under-approximation and grouping scheme (Section~\ref{sec:ssapo}).
    \item Empirically, extensive experiments show that SSAPO attains strong alignment performance with substantially reduced human labeling, demonstrating its practicality for real-world LLM alignment (Section~\ref{sec:experiments}).
\end{itemize}

By providing both theoretical robustness guarantees and a scalable implementation, we hope SGPO and SSAPO open the door to more label-efficient, reliable alignment of large language models under realistic data constraints.

