\section{Theoretical Foundation: Stackelberg Game Preference Optimization Framework}
\label{sec:theory}
In this section, we introduce our \emph{Stackelberg Game Preference Optimization} (SGPO) framework. First, we recap the standard DPO approach (Section~\ref{sec:dpo_prelims}). We then formulate SGPO as a two-player Stackelberg game (Section~\ref{sec:sgpo_formulation}), prove the existence of an equilibrium and characterize its convergence under iterative updates (Sections~\ref{sec:existence and convergence}).  Lastly, we provide a regret analysis (Section~\ref{sec:regret}), showing that SGPO suffers at most \(\mathcal{O}(\epsilon)\) regret under \(\epsilon\)-bounded shifts, while DPO’s regret can grow linearly in the distribution mismatch. Proofs in this section is delayed to Appendix~\ref{sec:theory_proofs}.

\subsection{Preliminaries: Preference Datasets and DPO}
omitted

\subsection{SGPO: A Two-Player Stackelberg Game}
\label{sec:sgpo_formulation}

We propose to defend against distributional uncertainty by treating the learning process as a two-player Stackelberg game \citep{Bacsar1998Dynamic}.  Concretely:

\begin{compactitem}
\item \textbf{Policy (the leader):} A policy model \(\pi_\theta\), parameterized by \(\theta \in \Theta \subset \mathbb{R}^d\).  This player chooses a parameter \(\theta\) to \emph{maximize} its worst-case expected performance (likelihood) against an adversarial preference distribution.
\item \textbf{Adversarial Preference Distribution (the follower):} A distribution \(P\) over pairwise outcomes \((y_w,y_l)\).  This player chooses, \emph{after} seeing \(\theta\), a preference distribution within an \(\epsilon\)-ball of the empirical distribution \(\hat{P}\)\footnote{Throughout the paper, let $\hat{\xi}_i = R_\theta(x,y^i_w) - R_\theta(x,y^i_l)$,$i=1,\dots,N$, $N$ denotes the preference sample size. Define the empirical measure $\hat{P}_N = \tfrac{1}{N}\sum_i \delta_{\hat{\xi}_i}$. Thus the \(\epsilon\)-ball can be viewed as the neighbourhood of the observed preference distribution.}.  The follower’s goal is to \emph{minimize} the policy’s performance.
\end{compactitem}

To formalize “\(\epsilon\)-ball,” we adopt the 1-Wasserstein distance (Cf. Appendix~\ref{sec:Wasserstein_Prelim} for more preliminaries on the Wasserstein metric space)\citep{Villani2009Optimal} and define:
$$
\mathcal{U}_{\epsilon}(\hat{P})
\;=\;
\Bigl\{
   P \,\in\, \mathcal{P}(\mathcal{Y}\times \mathcal{Y})
   \;\Bigl|\;
   W\bigl(P,\hat{P}\bigr) \,\le\, \epsilon
\Bigr\}.
$$
Hence, the leader’s robust objective is:
\begin{equation}
\label{eq:sgpo_robust_obj}
\max_{\pi \in \Pi}
\;
\min_{P \,\in\, \mathcal{U}_\epsilon(\hat{P})}
\;
\mathbb{E}_{(y_w,y_l)\sim P}
\Bigl[
  \log \,\sigma\bigl(R_{\pi}(y_w) - R_{\pi}(y_l)\bigr)
\Bigr],
\end{equation}
where \(R_{\pi}(y)\)\footnote{We drop the term $x$ in $R(x,y)$ for simplicity hereafter.} is the policy-induced \emph{logit} (akin to the reward term Eq.~\eqref{eq:optimal_reward_supp}) or more generally a function measuring how favorable \(y\) is under \(\pi\).  This induces the Stackelberg equilibrium:

\begin{definition}[Stackelberg Equilibrium]
\label{def:se}
A pair \(\bigl(\pi^*,\,P^*\bigr)\) is a \emph{Stackelberg equilibrium} if
\begin{numcases}{}
            \pi^*
\;\in\;
\arg\max_{\pi \in \Pi}
\;
\min_{P \,\in\, \mathcal{U}_\epsilon(\hat{P})}
\;
\mathbb{E}_{P}\!\bigl[\,J(\pi,\,P)\bigr], \ \{\textit{leader}\}\\
P^*
\;\in\;
\arg\min_{P \,\in\, \mathcal{U}_\epsilon(\hat{P})}
\;
\mathbb{E}_{P}\!\bigl[\,J(\pi^*,\,P)\bigr], \  \{\textit{follower}\}
\end{numcases}
where
\begin{equation}
J(\pi,P)
\;:=\;
\mathbb{E}_{(y_w,y_l)\sim P}
\Bigl[
  \log \,\sigma\bigl(R_{\pi}(y_w) \;-\; R_{\pi}(y_l)\bigr)
\Bigr].
\end{equation}
\end{definition}

Under real-world annotation noise or the noisy self-annotation scenario considered in this paper, the “true” preference distribution can \emph{deviate} from the empirical training data.  SGPO prepares for the worst-case shift within radius \(\epsilon\).  By adopting a Stackelberg perspective, we derive a policy that is simultaneously (i) high-performing on the empirical data and (ii) robust to preference shifts.  



\subsection{Existence and Convergence of a Stackelberg Equilibrium}
\label{sec:existence and convergence}
Under standard regularity conditions (continuity, convexity, and compactness \citet{Villani2009Optimal, Esfahani2018Data}, confer Assumption~\ref{assump:regularity_extended} for details ) for distributionally robust optimization, we can prove that an Stackelberg equilibrium exist, and a natural alternating procedure converges to the Stackelberg equilibrium. 

\begin{theorem}[Existence of Stackelberg Equilibrium]
\label{thm:existence_se}
Under the regularity assumptions (Assumption~\ref{assump:regularity_extended}), the two-player game defined by
$$
\max_{\pi \in \Pi}
\;
\min_{P \,\in\, \mathcal{U}_\epsilon(\hat{P})}
\;\;
J(\pi,P),
$$
$$
\text{where} \quad
J(\pi,P)
=
\mathbb{E}_{P}\Bigl[\log \sigma\bigl(R_{\pi}(y_w) - R_{\pi}(y_l)\bigr)\Bigr]
$$
admits at least one Stackelberg equilibrium \(\bigl(\pi^*,P^*\bigr)\).
\end{theorem}

A natural alternating procedure---iteratively updating the policy to best respond against the adversary, and then updating the adversary’s distribution within the \(\epsilon\)-ball---converges to the Stackelberg equilibrium.  One such procedure is:
\vspace{-0.05 in}
\begin{numcases}{}
\pi_{t+1}
\;=\;
\arg\max_{\pi \in \Pi}
\;
\min_{P \,\in\, \mathcal{U}_\epsilon(P_t)}
\;
J(\pi,P), \ \{\textit{leader}\}\label{eq:iterative_update_policy}\\
P_{t+1}
\;=\;
\arg\min_{P \,\in\, \mathcal{U}_\epsilon(P_t)}
\;
J(\pi_{t+1},P), \  \{\textit{follower}\}\label{eq:iterative_update_distribution}
\end{numcases}

starting from an initial pair \((\pi_0, P_0)\).  Here, we shift the center of the Wasserstein ball in each iteration to \(P_t\).  

\begin{theorem}[Linear Convergence to Stackelberg Equilibrium]
\label{thm:convergence_iterative_detailed}
Under the regularity assumptions (Assumption~\ref{assump:regularity_extended}), the sequence \(\{(\pi_t,P_t)\}_{t \ge 0}\) generated by \eqref{eq:iterative_update_policy}--\eqref{eq:iterative_update_distribution} converges to the Stackelberg equilibrium \((\pi^*,P^*)\).  Moreover, the convergence is \emph{linear}, i.e.\ there exists \(\gamma < 1\) such that 
$$
\rho\bigl((\pi_{t+1},P_{t+1}),(\pi^*,P^*)\bigr)
\;\;\le\;\;
\gamma
\;\rho\bigl((\pi_t,P_t),(\pi^*,P^*)\bigr),
$$
where $\rho$ is a suitable metric (e.g., \ $\rho((\pi,P),(\pi',P'))=\|\pi-\pi'\|+ W(P,P')$).
\end{theorem}

In practice, one may not directly implement \eqref{eq:iterative_update_policy}--\eqref{eq:iterative_update_distribution}, but the Theorem~\ref{thm:convergence_iterative_detailed} shows that any procedure that approximates these alternating best-response updates can converge to the robust equilibrium.  This provides a theoretical grounding for the SSAPO algorithm (to be introduced in the section~\ref{sec:ssapo}), which combines standard gradient-based optimization with distributionally robust optimization.

\subsection{Regret Analysis and Comparison with DPO}
\label{sec:regret}

We now quantify SGPO’s performance under worst-case preference shifts.  We show that SGPO enjoys an $\mathcal{O}(\epsilon)$ bound on its regret, whereas DPO can incur regret $\propto \delta$, where $\delta$ is the magnitude of distribution shift.

\subsubsection{SGPO’s Bounded Regret}
Let \(\pi^*\) be the SGPO policy obtained from the robust problem~\eqref{eq:sgpo_robust_obj}.  For any distribution \(P\in\mathcal{U}_\epsilon(\hat{P})\), we define the (absolute) performance as
\begin{equation}
\mathcal{P}\!\bigl(\pi, P\bigr)
\;=\;
\mathbb{E}_{(y_w,y_l)\sim P}
\Bigl[
  \log\,\sigma\bigl(R_{\pi}(y_w)-R_{\pi}(y_l)\bigr)
\Bigr].
\end{equation}
We prove that \(\pi^*\) maintains high performance on \emph{all} distributions \(P\) within \(\epsilon\)-Wasserstein distance of \(\hat{P}\).  In particular, the drop from \(\hat{P}\) to any \(P\) is at most \(\mathcal{O}(\epsilon)\).

\begin{theorem}[Worst-Case Performance Guarantee for SGPO]
\label{thm:sgpo_regret_bound}
Under Assumption~\ref{assump:regularity_extended}, let \(\pi^*\) be the SGPO solution.  Then for \emph{every} $P\in \mathcal{U}_\epsilon(\hat{P})$,
\begin{equation}
\mathcal{P}\bigl(\pi^*,P\bigr)
\;\;\ge\;\;
\mathcal{P}\bigl(\pi^*,\hat{P}\bigr)
\;-\;
L_R\,\epsilon.
\end{equation}
In other words, the performance drop from \(\hat{P}\) to any \(P\in \mathcal{U}_\epsilon(\hat{P})\) is at most \(L_R\epsilon\).
\end{theorem}

\paragraph{Regret Notation.}
We define the regret of a policy \(\pi\) on a distribution \(P\) as
\begin{equation}
\text{Regret}\bigl(\pi,P\bigr)
\;=\;
\max_{\tilde{\pi}}\,\mathcal{P}\!\bigl(\tilde{\pi},P\bigr)
\;-\;
\mathcal{P}\!\bigl(\pi,P\bigr).
\end{equation}
If $\pi_P^*=\arg\max_{\tilde{\pi}}\mathcal{P}(\tilde{\pi},P)$, then
$\text{Regret}\bigl(\pi,P\bigr)=\mathcal{P}\bigl(\pi_P^*,P\bigr)-\mathcal{P}\bigl(\pi,P\bigr).$

\begin{theorem}[SGPO Regret Bound]
\label{thm:sgpo_regret}
For the SGPO policy \(\pi^*\), we have
\begin{equation}
\sup_{P \,\in\,\mathcal{U}_\epsilon(\hat{P})}
\;\;
\text{Regret}\bigl(\pi^*,P\bigr)
\;\;\le\;\;
2\,L_R\,\epsilon.
\end{equation}
Thus, SGPO is robust: under any shift of at most \(\epsilon\), its regret is bounded by a constant factor of \(\epsilon\).
\end{theorem}

\subsubsection{Comparison: DPO’s Linear Regret}
\label{sec:compare_dpo}

Recall that DPO $\pi_{\mathrm{DPO}}$ \citep{Rafailov2023Direct} maximizes $\mathcal{P}(\pi,\hat{P})$ (Eq.~\eqref{eq:dpo_loss_supp} with no regard for shifts away from $\hat{P}$.  Let $\delta = W(\hat{P},P^*)$.  We show DPO can be arbitrarily suboptimal under large $\delta$, scaling linearly with $\delta$.

\begin{theorem}[DPO Regret Lower Bound]
\label{thm:dpo_regret_lowerbound}
Let $\pi_{\mathrm{DPO}}=\arg\max_{\pi}\mathcal{P}(\pi,\hat{P})$, and let $P^*$ be a distribution satisfying $W(\hat{P},P^*)=\delta$.  Then
\begin{equation}
\text{Regret}\bigl(\pi_{\mathrm{DPO}},\,P^*\bigr)
\;\ge\;
L_R\;\bigl(\delta - 2\,\epsilon\bigr).
\end{equation}
In particular, if $\delta \gg \epsilon$, DPO’s regret grows \emph{linearly} in $\delta$.
\end{theorem}

\begin{corollary}[SGPO Advantage Over DPO]
\label{cor:sgpo_advantage}
If $W\!\bigl(\hat{P},P^*\bigr)=\delta > 2\,\epsilon$, then
\begin{equation}
\frac{\text{Regret}\bigl(\pi_{\mathrm{DPO}},P^*\bigr)}%
     {\text{Regret}\bigl(\pi^{*},P^*\bigr)}
\;\ge\;
\frac{\delta - 2\epsilon}{2\,\epsilon}.
\end{equation}
Thus, SGPO’s robust policy can outperform DPO by a factor of $\tfrac{\delta}{2\epsilon} - 1$ under sufficiently large distribution shift $\delta$.
\end{corollary}

\paragraph{Discussion of Theoretical Results.}
Collectively, these results clarify \emph{why} SGPO is well-suited for preference alignment: if the “true” preference distribution lies within an $\epsilon$-Wasserstein ball of the empirical distribution, SGPO guarantees a mere $\mathcal{O}(\epsilon)$ penalty in regret, thereby remaining robust under moderate annotation noise or distribution shift.  Moreover, whereas DPO’s regret can grow linearly with the magnitude of the shift, SGPO constrains worst-case losses even in adversarial scenarios.  Finally, by treating preference alignment as a robust $\min_{P}$ optimization, SGPO can mitigate \emph{mismatches between labeled and unlabeled sets}, making it particularly appealing when human annotations are scarce or must be supplemented by self-annotation.  Altogether, these properties form the theoretical foundation for the practical algorithm described next (Section~\ref{sec:ssapo}). 

\section{Practical Instantiation: SSAPO Algorithm}
\label{sec:ssapo}
We now present a \emph{practical} and \emph{computationally tractable} realization of SGPO, 
called \emph{Stackelberg Self-Annotated Preference Optimization (SSAPO)}. 
This method approximates the iterative leader--follower updates 
of Theorem~\ref{thm:convergence_iterative_detailed} 
and Eqs.~\eqref{eq:iterative_update_policy}--\eqref{eq:iterative_update_distribution}, 
overcoming three major challenges in realistic preference alignment:

\begin{compactenum}
    \item \emph{Minimal Human Labels via Self-Annotation.}  We begin with a small “seed” of human-labeled preferences and augment the dataset 
    by letting the \emph{current policy} rank its own responses on unlabeled prompts.
    \item \emph{Convexity/Concavity Mismatch.} When the function \(\ell(\cdot) \) is \emph{concave}, the DRO literature \citet{Esfahani2018Data} provides a finite-dimensional \(\max\)-form program that solves the follower update. However \(\ell(\xi) = -\!\log\,\sigma(\xi)\) is convex in $\xi$. We therefore approximate \(\ell(\xi)\) by a piecewise-linear \emph{concave} function. 
    \item \emph{Scalability via Uniform Grouping.} 
    For large-scale datasets (potentially hundreds of thousands of preferences), we split the data into subsets and solve each subset’s robust problem in parallel, then merge solutions to form an adversarial $P^*$ for the entire set.
\end{compactenum}

Below, we restate the relevant DRO theorem and describe how to approximate 
$-\!\log\,\sigma(\xi)$ by a concave, piecewise-linear function 
for solving the follower update (Section~\ref{subsec:follower_update}). 
We then detail how SSAPO’s leader--follower loop integrates \emph{self-annotation} and \emph{uniform grouping} (Section~\ref{subsec:ssapo_algorithm}), and finally discuss the impact of these approximations on SGPO’s theoretical guarantees.

\subsection{Solving the Follower's Update with DRO}
\label{subsec:follower_update}
\subsubsection{Construct Worst-Case Distribution}
\label{subsec:concavity_dro_theorem}

Consider the distributionally robust problem
\begin{equation*}
\label{eq:DRO_concave}
\sup_{P \,\in\, B_\epsilon(\hat{P}_N)}\,\;
\mathbb{E}_P\bigl[\ell(\xi)\bigr], 
\end{equation*}
\(\hat{P}_N = \tfrac1N \sum_{i=1}^N \delta_{\hat{\xi}_i},
B_\epsilon(\hat{P}_N)=\Bigl\{
  P : W(P,\hat{P}_N)\le \epsilon
\Bigr\}.\)

\citet{Esfahani2018Data} show that if \(\ell(\xi)\) is 
\emph{concave}, then 
\(\sup_{P} \mathbb{E}_P[\ell(\xi)]\)
admits a \emph{finite convex program} in the variables 
\(\{\alpha_{ik}, q_{ik}\}\) whose solution yields 
a worst-case \emph{extremal distribution} $P^*\in B_\epsilon(\hat{P}_N)$.  
Conceptually, each original sample \(\hat{\xi}_i\) can “shift” by $q_{ik}/\alpha_{ik}$ 
subject to $\ell(\cdot)$ being evaluated at $\hat{\xi}_i - (q_{ik}/\alpha_{ik})$.  
In the \(\max\)-form, that shift tries to \emph{increase} 
$\ell(\xi)$ in a worst-case sense. Formally.

\begin{theorem}[Worst-Case Extremal Distributions, specialized from Theorem 4.4 in \citealp{Esfahani2018Data}]
\label{thm:worst_case_concave}
If $\ell(\cdot)$ is proper, concave, and lower semicontinuous on $\Xi \subset \mathbb{R}^m$, 
then
\begin{tiny}
    \begin{align*}
    \sup_{P\in B_\epsilon(\hat{P}_N)}
\mathbb{E}_P\bigl[\ell(\xi)\bigr]=\max_{\substack{\alpha_{ik},\,q_{ik}\\i=1,\dots,N;\,k=1,\dots,K}}
\frac{1}{N} \sum_{i=1}^N \sum_{k=1}^K
\alpha_{ik}
\,\ell\Bigl(\hat{\xi}_i-\tfrac{q_{ik}}{\alpha_{ik}}\Bigr)
\end{align*}
\end{tiny}
subject to the usual Wasserstein and feasibility constraints
\(\tfrac{1}{N}\sum_{i,k}\|q_{ik}\|\le \epsilon\), 
\(\sum_{k}\alpha_{ik}=1, \alpha_{ik}\ge 0\), and 
\(\hat{\xi}_i-\tfrac{q_{ik}}{\alpha_{ik}}\in \Xi\).  
A discrete distribution 
\(\tfrac{1}{N}\sum_{i,k} \alpha_{ik} \delta_{\hat{\xi}_i - \tfrac{q_{ik}}{\alpha_{ik}}}\)
achieves the supremum, thus providing $P^*\in B_\epsilon(\hat{P}_N)$.
\end{theorem}

Since $\ell(\xi)=-\!\log\,\sigma(\xi)$ is actually \emph{convex} rather than concave, 
we cannot directly apply Theorem~\ref{thm:worst_case_concave}. 
Hence, Section~\ref{subsec:concave_approx} explains how to construct 
a concave \emph{piecewise-linear} under-approximation 
$\widetilde{\ell}(\cdot)\le -\!\log\,\sigma(\cdot)$, 
enabling us to adopt the same finite convex program framework.

\subsubsection{Concave Piecewise-Linear Approximation}
\label{subsec:concave_approx}

To embed our \(\ell(\xi)=-\!\log\,\sigma(\xi)\) into 
Theorem~\ref{thm:worst_case_concave}, we construct a \emph{concave under-approximation} 
\(\widetilde{\ell}(\xi)\) represented by $K$ linear pieces:
\begin{equation}
\label{eq:piecewise_concave_ell}
\widetilde{\ell}(\xi)
\;=\;
\max_{1\le k\le K}\;\ell_k(\xi),
\end{equation}

where each $\ell_k(\xi)$is an affine function, and
\(\forall\,\xi:\;
\widetilde{\ell}(\xi)
\;\;\le\;\;
-\!\log\,\sigma(\xi)\).
Because the $\ell_k(\cdot)$ are linear and we take a \(\max\), 
\(\widetilde{\ell}(\cdot)\) is indeed a \emph{concave}, piecewise-linear function.  

One practical construction is to partition an interval of interest into $K$ “knots” 
$\{\xi^{(k)}\}$ and define $\ell_k(\xi)$ as the tangent line \emph{from below} 
(or a chord) such that $\ell_k(\xi^{(k)}) = -\!\log\,\sigma(\xi^{(k)})$ 
but $\ell_k(\xi)\le -\!\log\,\sigma(\xi)$ for all $\xi$ in the domain.  
Equidistant points can be taken in the interval $[0,1]$, considering the image of sigmoid activations $\sigma(\cdot)$ is $[0,1]$.

\paragraph{Follower’s DRO Problem with $\widetilde{\ell}(\cdot)$.}
Replacing \(\ell(\xi)\) in Theorem~\ref{thm:worst_case_concave} with 
$\widetilde{\ell}(\xi)$ gives a \emph{finite convex program} that yields 
$P^*\in B_\epsilon(\hat{P}_N)$.  Concretely, 
\begin{small}
  \begin{equation}
\label{eq:widetilde_dro}
\max_{P \in B_\epsilon(\hat{P}_N)}
\mathbb{E}_P[\widetilde{\ell}(\xi)]
=
\max_{\,\{\alpha_{ik},\,q_{ik}\}}
\;\;
\frac{1}{N}
\sum_{i=1}^N
\sum_{k=1}^K
\alpha_{ik}
\;\ell_k\!\Bigl(\hat{\xi}_i \;-\; \tfrac{q_{ik}}{\alpha_{ik}}\Bigr),
\end{equation}  
\end{small}
subject to standard Wasserstein constraints.  
Since $\widetilde{\ell}(\xi)\le -\!\log\,\sigma(\xi)$, 
this \emph{under-approximation} yields a $P^*$ that is valid---but 
\emph{less adversarial}---for the original $\ell(\xi)$.

\subsection{SSAPO: Algorithmic Steps}
\label{subsec:ssapo_algorithm}
Figure~\ref{fig:ssapo_framework} summarizes the SSAPO workflow. 
Starting with a small seed of human-labeled preferences plus a large unlabeled pool, 
we proceed in the following loop:

\begin{compactenum}
\item \emph{Self-Annotation.}  
  At each iteration, we sample new unlabeled prompts from 
  \(\mathcal{D}_{\mathrm{unlabeled}}\) and let the current policy 
  \(\pi_{\theta_t}\) generate candidate responses.  The policy \emph{ranks} them 
  to form winner-loser pairs $(y_w,y_l)$, which expand the preference dataset 
  \(\mathcal{D}\).  From these, we build \(\hat{\xi}_i = R_{\theta_t}(y_w^i) - R_{\theta_t}(y_l^i)\).

\item \emph{Concave Piecewise-Linear Approx.}  
  We fix $K$ and define $\{\ell_k\}_{k=1}^K$ such that 
  $\widetilde{\ell}(\xi) = \max_k \ell_k(\xi)\le -\!\log\,\sigma(\xi)$ 
  (as in Eq.~\eqref{eq:piecewise_concave_ell}).  

\item \emph{Worst-Case Distribution.}  
  We form $\hat{P}_N=\tfrac1N\sum_{i=1}^N \delta_{\hat{\xi}_i}$, 
  then solve Eq.~\eqref{eq:widetilde_dro} with $\widetilde{\ell}$.  
  By Theorem~\ref{thm:worst_case_concave}, the solution is a discrete distribution 
  $P^*_t \in B_\epsilon(\hat{P}_N)$ that \emph{shifts} each $\hat{\xi}_i$ 
  by up to $q_{ik}^{*}/\alpha_{ik}^{*}$, then applies the affine $\ell_k(\cdot)$ 
  and weights $\alpha_{ik}^{*}$.  

\item \emph{Policy Update.}  
  We train $\pi_{\theta}$ on $P^*_t$ by minimizing 
  $\mathbb{E}_{P^*_t}\bigl[-\!\log\,\sigma(R_\theta(y_w)-R_\theta(y_l))\bigr]$.  
  Noting that $P^*_t$ identifies how often each shifted $\xi_i$ is “activated,” 
  we can equivalently reweight the original samples for gradient-based updates.
\end{compactenum}

Repeating for $T$ total iterations yields the final aligned policy $\pi_{\theta_T}$. A more explicit version of SSAPO is provided in Algorithm~\ref{algo:ssapo} (Appendix~\ref{sec:SSAPO_algorithm+analysis}), along with its computational complexity analysis.

\subsubsection{Scalability via Uniform Grouping and Approximation}
\label{subsec:discussion_approx}

\paragraph{Grouping Large Datasets.}
When $N$ is large (e.g.\ $\!10^5\!$ or more preferences), solving the convex program 
in Step~(Worst-Case Distribution) can be expensive.  A popular heuristic partitions 
$\{\hat{\xi}_1,\dots,\hat{\xi}_N\}$ into $M$ groups (each of size $G=N/M$), 
and solves the finite program \eqref{eq:widetilde_dro} separately within each group.  

The resulting distributions $P^*_m$ are then averaged (or merged proportionally):
\[
P_{\mathrm{final}}^*
\;=\;
\frac{1}{M}
\sum_{m=1}^M
P^*_m.
\]
While not an \emph{exact} solution to the global $N$-sample problem, 
this still confers substantial robustness while reducing complexity from 
$N$ to $G \ll N$ in each subproblem. In summary, this grouping approach greatly reduces memory/compute cost, and is parallelizable.

\begin{remark}[Approximation Effects on SGPO Guarantees]
    Two approximations separate SSAPO from the ideal solution of 
Section~\ref{sec:theory}: (1) \emph{Concave Under-Approximation:}  
 Replacing $-\!\log\,\sigma(\cdot)$ with its piecewise-linear approximation $\widetilde{\ell}(\cdot)$ (i.e., $\widetilde{\ell}(\xi)\le -\!\log\,\sigma(\xi)$) ensures feasibility in Theorem~\ref{thm:worst_case_concave}, but slightly weakens the adversarial effect. Consequently, the policy may be \emph{less robust} than in the fully ideal solution $\max_{P}\!\mathbb{E}_P[-\!\log\,\sigma(\cdot)]$. Increasing $K$ (i.e., refining the linear approximation) narrows this gap. (2)\emph{Partitioned Solver:}   Rather than solving one global $\arg\max_{P \in B_\epsilon(\hat{P}_N)}$, SSAPO partitions the dataset into $M$ groups and separately solves $M$ subproblems. Merging their solutions may deviate from a unified optimum, but it still explores adversarial reweighting in each subgroup, preserving much of SGPO’s robustness against distributional shifts.
\end{remark}

\vspace{-0.1 in}
Despite these approximations, SSAPO preserves the \emph{Stackelberg} essence: the policy is trained against worst-case reweightings within an $\epsilon$-Wasserstein distance. As a result, it retains the key benefit of \emph{bounded regret} for moderate shifts (Theorem~\ref{thm:sgpo_regret}), while remaining computationally tractable. Indeed, if $K \!\to\! \infty$ and $M\!=\!1$, SSAPO recovers the exact follower solution (subject to sampling). Empirically (Section~\ref{sec:experiments}), these approximations still yield substantial label-efficiency and robustness gains over standard DPO.