\section{Related Work}
\label{sec:related_work }

Our work bridges preference optimization, distributionally robust training, and game-theoretic alignment. We situate our contributions within these areas, emphasizing theoretical robustness and label efficiency.

\paragraph{Preference Optimization and Human Feedback}
Reinforcement Learning from Human Feedback (RLHF) \citep{Christiano2017DeepRLHF} pioneered aligning language models with human preferences via reward modeling and KL-constrained policy updates. Direct Preference Optimization (DPO) \citep{Rafailov2023DirectPreference} bypassed reward modeling by reparameterizing the RLHF objective, achieving comparable performance with simpler training. However, both methods assume access to large, high-quality preference datasets and lack robustness to annotation noise \citep{Zhang2023DPOPitfalls}. Recent analyses reveal that DPO’s performance degrades linearly with distributional mismatch \citep{Liu2023StatisticalUG}, a vulnerability our Stackelberg framework directly addresses via $\mathcal{O}(\epsilon)$-bounded regret guarantees.

\paragraph{Data-Efficient Alignment Methods}
To reduce reliance on human labels, self-training methods like Self-Rewarding Language Models \citep{Yuan2024Self-RewardingLM} and SPIN \citep{Chen2024SelfPlay} iteratively generate synthetic preferences using the current policy. However, these approaches treat policy updates and data generation as decoupled stages, leading to error accumulation from noisy self-labels \citep{Pal2024BeyondSynthetic}. In contrast, SSAPO unifies policy optimization and adversarial data generation within a Stackelberg game, dynamically reweighting synthetic preferences to mitigate error propagation. Concurrent work on synthetic data generation \citep{Guo2024SyntheticDP} also reduces annotation costs but lacks theoretical guarantees against distribution shifts.

\paragraph{Adversarial and Distributionally Robust Training}
Adversarial training frameworks \citep{Zhu2023FightingFI} aim to improve model robustness by perturbing inputs or rewards, but they lack equilibrium guarantees and often exhibit unstable optimization cycles. Distributionally Robust Optimization (DRO) \citep{Esfahani2018DataDrivenDR} formalizes robustness via Wasserstein-ball constraints, yet existing DRO applications to language models \citep{Pang2024TextGrad} focus on input perturbations rather than preference shifts. SGPO advances this line by framing alignment as a minimax game over preference distributions, with provable convergence to a Stackelberg equilibrium—a novel contribution in the context of language model alignment.

\paragraph{Game-Theoretic Approaches in Machine Learning}
Stackelberg games have been applied to adversarial robustness \citep{Letcher2023StablePS} and multi-agent RL \citep{Balduzzi2018RepeatedSG}, but their use in preference optimization is unexplored. Our work is the first to model preference alignment as a two-player Stackelberg game, where the policy optimizes against worst-case preference distributions generated by an adversarial follower. This contrasts with prior game-theoretic alignment approaches \citep{Tien2023GameTheoreticAM}, which focus on Nash equilibria in multi-agent settings rather than robustness to distribution shifts. Theoretically, we extend existing Stackelberg convergence results \citep{Basar1999DynamicNG} to the Wasserstein-constrained preference setting, proving linear convergence rates under practical regularity conditions.

\paragraph{Self-Annotation and Synthetic Preference Generation}
Recent methods like RAFT \citep{Dong2024Raft} and AlpacaFarm \citep{Dubois2024AlpacaFarm} reduce annotation costs by generating synthetic preferences via model self-evaluation. However, these approaches naively trust the model’s self-annotations, ignoring strategic interactions between policy updates and data quality \citep{Casper2023OpenProblems}. SSAPO introduces controlled noise injection during self-annotation, simulating real-world imperfections while adversarially reweighting samples within a Wasserstein ball. This mechanism ensures robustness to synthetic label noise—a key advantage over methods that treat self-annotations as ground truth \citep{Chen2024SelfPlay}.

%\paragraph{Positioning of Our Work}
%SGPO and SSAPO uniquely integrate three pillars: (1) Stackelberg game theory for equilibrium-guaranteed robustness, (2) distributionally robust optimization against preference shifts, and (3) self-annotation with noise-aware reweighting. This synthesis enables label-efficient alignment with provable$\mathcal{O}(\epsilon)$ regret bounds, addressing the limitations of DPO’s linear regret growth and self-annotation’s error accumulation. Empirically, SSAPO achieves state-of-the-art performance with 30X fewer human labels, advancing the frontier of data-efficient alignment.

\section{Related Work}
\label{sec:related_work }

Our work bridges game-theoretic optimization, distributionally robust training, and label-efficient alignment. We situate our contributions within these areas, emphasizing theoretical robustness and data efficiency.

\paragraph{Game-Theoretic Approaches to LLM Alignment}
Recent advances formalize alignment as strategic interactions between policies or reward models. Self-play preference optimization (SPPO, \citealp{Chen2024SelfPlay}) and direct Nash optimization (DNO, \citealp{Rosset2024DirectNO}) model alignment as symmetric games where policies iteratively improve via pairwise comparisons, inspired by adversarial training in RL \citep{Balduzzi2018RepeatedSG}. Minimaximalist RLHF \citep{Zhu2024MinimaximalistRF} and Nash learning from human feedback \citep{Tien2024NashLF} extend this to general preference models, optimizing policies under Nash equilibrium constraints. While these methods improve over static RLHF objectives, they focus on symmetric equilibria and assume access to high-quality preference data. In contrast, SGPO adopts a \emph{leader--follower} Stackelberg structure, where the policy (leader) proactively optimizes against worst-case preference perturbations—a critical distinction that enables robustness to distribution shifts with provable $\mathcal{O}(\epsilon)$ regret bounds. Our framework also generalizes iterative Nash policy optimization \citep{Feng2024IterativeNP}, which lacks explicit distributional robustness guarantees, by unifying policy updates with adversarial preference generation under Wasserstein constraints.

\paragraph{Data-Efficient Alignment via Synthetic Preferences}
Reducing reliance on human labels has spurred methods like self-rewarding models \citep{Yuan2024Self-RewardingLM} and SPIN \citep{spa}, which iteratively generate synthetic preferences using the current policy. However, these approaches treat policy training and synthetic data generation as decoupled stages, leading to error accumulation from noisy self-labels \citep{Pal2024BeyondSynthetic}. RAFT \citep{Dong2024Raft} and AlpacaFarm \citep{Dubois2024AlpacaFarm} mitigate this via rejection sampling but naively trust self-annotations as ground truth. SSAPO advances this paradigm by modeling the \emph{strategic interplay} between policy optimization and preference generation: our adversarial follower dynamically reweights synthetic samples within a Wasserstein ball, simulating real-world annotation noise while preventing overfitting to imperfect self-labels. This contrasts with concurrent work on synthetic data pruning \citep{Guo2024SyntheticDP}, which lacks theoretical safeguards against distribution shifts.

\paragraph{Distributional Robustness and Adversarial Training}
Distributionally robust optimization (DRO) \citep{Esfahani2018DataDrivenDR} formalizes robustness via Wasserstein-ball constraints, but applications to language models \citep{Pang2024TextGrad} focus on input perturbations rather than preference shifts. Adversarial preference training \citep{Zhu2023FightingFI} perturbs rewards to improve robustness but lacks equilibrium guarantees. SGPO bridges this gap by framing alignment as a minimax game over preference distributions, with provable convergence to a Stackelberg equilibrium—a novel contribution in language model alignment. Our theory extends DRO to sequential leader--follower dynamics, ensuring the policy anticipates worst-case preference perturbations during training.