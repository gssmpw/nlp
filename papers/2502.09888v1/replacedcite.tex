\section{RELATED WORK}
Wukong ____ explored parameter scaling in retrieval models but relied on strong assumptions about feature engineering. HSTU ____ reformulated recommendations as sequential transduction tasks, achieving trillion-parameter models with linear computational scaling via hierarchical attention and stochastic sequence sampling. However, HSTU’s focus on generative modeling left gaps in bridging traditional feature-based DLRMs. Concurrently, MARM ____ proposed caching intermediate attention results to reduce inference complexity from ${O(n^2d)}$ to $O(nd)$, empirically validating cache size as a new scaling dimension. While effective, MARM’s caching strategy assumes static user patterns, overlooking real-time behavior shifts.

Techniques to mitigate computational costs have been widely adopted. In NLP, KV caching ____ avoids redundant attention computations during autoregressive inference. MARM adapted this idea to recommendations by storing historical attention outputs, enabling multi-layer target-attention with minimal FLOPs overhead. Similarly, HSTU introduced Stochastic Length to sparsify long sequences algorithmically, reducing training costs by 80\% without quality degradation. For advertisement retrieval, Wang et al. ____ designed $R/R^*$, an eCPM-aware offline metric, to estimate online revenue scaling laws at low experimental costs. These works collectively highlight the importance of tailoring efficiency strategies to recommendation-specific constraints, such as high-cardinality features and millisecond-level latency requirements.