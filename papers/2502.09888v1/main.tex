%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf]{acmart}
\usepackage{enumitem}
\setlist[itemize]{leftmargin=*, itemsep=0pt, topsep=0pt}
\usepackage{graphicx}
\graphicspath{ {./graph/} }
\usepackage{svg}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{subfigure}
\usepackage{booktabs} % 用于更美观的表格线条
\usepackage{tabularx} % 用于设置表格宽度
\usepackage{xcolor} % 用于文本颜色
\usepackage{makecell} % 用于多行表头
% \usepackage{soul} % 加载 soul 宏包
% \usepackage{ulem} % 加载 ulem 宏包
\usepackage{hyperref}
\usepackage{microtype}

% \usepackage{geometry}
% \geometry{a4paper, margin=1in, columnsep=10pt}
% \usepackage{setspace}
% \setlength{\parskip}{0.5em}
% \usepackage{microtype}
\usepackage{balance}

\newcommand{\MSP}{MSP }
\newcommand{\ATL}{ATL }
\newcommand{\BFM}{BFM }


%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June 03--05,
  218}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{An Efficient Large Recommendation Model: Towards a Resource-Optimal Scaling Law}
\renewcommand{\thefootnote}{\fnsymbol{footnote}} % 设置脚注符号为非数字形式
\author{Songpei Xu, Shijia Wang\footnotemark, Da Guo, Xianwen Guo, Qiang Xiao\footnotemark, Fangjian Li, Chuanjiang Luo}
\email{{xusongpei,wangshijia1,guoda,guoxianwen,hzxiaoqiang,hzlifangjian,luochuanjiang03}@corp.netease.com}
\affiliation{
  \institution{NetEase Cloud Music}
  \city{Hangzhou}
  \country{China}
}
% \author{Songpei Xu}
% \email{xusongpei@corp.netease.com}
% \affiliation{%
%   \institution{NetEase Cloud Music}
%   \city{Hangzhou}
%   \country{China}
% }

% \author{Shijia Wang}
% % \authornote{Both authors contributed equally to this research.}
% \authornote{Corresponding author.}
% \email{wangshijia1@corp.netease.com}
% % \authornotemark[2]
% \affiliation{%
%   \institution{NetEase Cloud Music}
%   \city{Hangzhou}
%   \country{China}
% }

% \author{Da Guo}
% % \authornote{Both authors contributed equally to this research.}
% \email{guoda@corp.netease.com}
% % \authornotemark[2]
% \affiliation{%
%   \institution{NetEase Cloud Music}
%   \city{Hangzhou}
%   \country{China}
% }

% \author{Xianwen Guo}
% % \authornote{Both authors contributed equally to this research.}
% \email{guoxianwen@corp.netease.com}
% % \authornotemark[2]
% \affiliation{%
%   \institution{NetEase Cloud Music}
%   \city{Hangzhou}
%   \country{China}
% }

% \author{Qiang Xiao}
% \authornote{Corresponding author.}
% \email{hzxiaoqiang@corp.netease.com}
% % \authornotemark[2]
% \affiliation{%
%   \institution{NetEase Cloud Music}
%   \city{Hangzhou}
%   \country{China}
% }

% \author{Fangjian Li}
% \email{hzlifangjian@corp.netease.com}
% % \authornotemark[2]
% \affiliation{%
%   \institution{NetEase Cloud Music}
%   \city{Hangzhou}
%   \country{China}
% }

% \author{Chuanjiang Luo}
% \email{luochuanjiang03@corp.netease.com}
% \affiliation{%
%   \institution{NetEase Cloud Music}
%   \city{Hangzhou}
%   \country{China}
% }
%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

% \author{Ben Trovato}
% \authornote{Both authors contributed equally to this research.}
% \email{trovato@corporation.com}
% \orcid{1234-5678-9012}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
% \affiliation{%
%   \institution{Institute for Clarity in Documentation}
%   \city{Dublin}
%   \state{Ohio}
%   \country{USA}
% }

% \author{Lars Th{\o}rv{\"a}ld}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \city{Hekla}
%   \country{Iceland}}
% \email{larst@affiliation.org}

% \author{Valerie B\'eranger}
% \affiliation{%
%   \institution{Inria Paris-Rocquencourt}
%   \city{Rocquencourt}
%   \country{France}
% }

% \author{Aparna Patel}
% \affiliation{%
%  \institution{Rajiv Gandhi University}
%  \city{Doimukh}
%  \state{Arunachal Pradesh}
%  \country{India}}

% \author{Huifen Chan}
% \affiliation{%
%   \institution{Tsinghua University}
%   \city{Haidian Qu}
%   \state{Beijing Shi}
%   \country{China}}

% \author{Charles Palmer}
% \affiliation{%
%   \institution{Palmer Research Laboratories}
%   \city{San Antonio}
%   \state{Texas}
%   \country{USA}}
% \email{cpalmer@prl.com}

% \author{John Smith}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \city{Hekla}
%   \country{Iceland}}
% \email{jsmith@affiliation.org}

% \author{Julius P. Kumquat}
% \affiliation{%
%   \institution{The Kumquat Consortium}
%   \city{New York}
%   \country{USA}}
% \email{jpkumquat@consortium.net}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
The pursuit of scaling up recommendation models confronts intrinsic tensions between expanding model capacity and preserving computational tractability. While prior studies have explored scaling laws for recommendation systems, their resource-intensive paradigms---often requiring tens of thousands of A100 GPU hours---remain impractical for most industrial applications. This work addresses a critical gap: achieving sustainable model scaling under strict computational budgets. We propose Climber, a resource-efficient recommendation framework comprising two synergistic components: the ASTRO model architecture for algorithmic innovation and the TURBO acceleration framework for engineering optimization. ASTRO (Adaptive Scalable Transformer for RecOmmendation) adopts two core innovations: (1) multi-scale sequence partitioning that reduces attention complexity from $O(n^2d)$ to $O(n^2d/N_b)$ via hierarchical blocks, enabling more efficient scaling with sequence length; (2) dynamic temperature modulation that adaptively adjusts attention scores for multimodal distributions arising from inherent multi-scenario and multi-behavior interactions. Complemented by TURBO (Two-stage Unified Ranking with Batched Output), a co-designed acceleration framework integrating gradient-aware feature compression and memory-efficient Key-Value caching, Climber achieves 5.15$\times$ throughput gains without performance degradation.

Comprehensive offline experiments on multiple datasets validate that Climber exhibits a more ideal scaling curve. To our knowledge, this is the first publicly documented framework where controlled model scaling drives continuous online metric growth (12.19\% overall lift) without prohibitive resource costs. Climber has been successfully deployed on Netease Cloud Music, one of China's largest music streaming platforms, serving tens of millions of users daily. These advancements establish a new paradigm for industrial recommendation systems, demonstrating that coordinated algorithmic-engineering innovation---not just brute-force scaling---unlocks sustainable performance growth under real-world computational constraints.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\ccsdesc[500]{Information systems~Retrieval models and ranking}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Recommendation system, Transformer, Scaling law}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
% \begin{teaserfigure}
%   \includegraphics[width=\textwidth]{sampleteaser}
%   \caption{Seattle Mariners at Spring Training, 2010.}
%   \Description{Enjoying the baseball game from the third-base
%   seats. Ichiro Suzuki preparing to bat.}
%   \label{fig:teaser}
% \end{teaserfigure}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\footnotetext[1]{Corresponding Author}
\footnotetext[2]{Corresponding Author}
% \footnotetext[3]{The baseline model that has been implemented for online deployment.}


\section{Introduction}
Scaling laws, initially explored in language models \cite{kaplan2020scaling,hoffmann2022training}, establish predictable relationships between model performance and key factors such as model size, training data volume, and computational budget. For instance, Kaplan et al. \cite{kaplan2020scaling} demonstrated that transformer-based language models follow power-law improvements in perplexity as model parameters and token counts increase. Similar trends have been observed in vision \cite{cherti2023reproducible} and multimodal models \cite{bai2023qwen}, where scaling model dimensions and data diversity directly correlate with the performance of downstream tasks. 

Recent research has shown that scaling laws also apply to recommendation systems, providing valuable insights into model design and resource allocation\cite{ardalani2022understanding,guo2024scaling}. The HSTU model \cite{zhai2024actions} employs hierarchical self-attention mechanisms to model long-term user behavior sequences, achieving better performance than traditional Transformers. Similarly, the MARM model \cite{lv2024marm} introduces memory augmentation techniques to reduce computational complexity, enabling multi-layer sequence modeling with minimal inference costs.However, these approaches demand prohibitive computational resources (e.g., thousands of GPU hours or terabytes of memory), rendering them impractical for real-world deployment. Furthermore, the interplay between key scaling factors---sequence length, model depth, and heterogeneous user behaviors---remains underexplored, leading to suboptimal resource allocation and diminished returns on scaling efforts.


% Recently, scalable recommendation models have been developed and have demonstrated promising online performance. However, these models still exhibit certain limitations. In the WuKong model, scalability is primarily determined by feature engineering and interaction, which is not generalizable to other scenarios. In the HSTU research, the model is based on Transformer sequence modeling. When comparing DLRM with HSTU in ranking set, DLRM shows superior performance with fewer computational complexity (FLOPs). This phenomenon is also observed in our system, as illustrated in Figure \ref{fig:intro}(a). Furthermore, both HSTU and MARM have achieved significant online results. However, they also consume substantial resources, such as GPU and storage resources.

% Inspired by the above work, we effectively implement the Transformer model within Netease’s large-scale music recommendation system to replace our traditional DLRM. However, there are still several specific challenges in recommendation systems:

\renewcommand{\thefootnote}{\fnsymbol{footnote}} % 设置脚注符号为非数字形式
% Inspired by DeepSeek series work \cite{liu2024deepseek1,liu2024deepseek2,bi2024deepseek3}, which greatly improve the efficiency of buliding LLMs and reduce the cost of computional resource, we aim to answer the question: how to scaling up recommendation model with significant lower cost?  
Inspired by the DeepSeek series \cite{liu2024deepseek1,liu2024deepseek2,bi2024deepseek3}, which has significantly enhanced the efficiency of large language model (LLM) development and reduced computational resource costs, we aim to address the following question: \textbf{\textit{how can we scale up the recommendation model at a substantially lower cost?}} To gain insights, we conducted industrial-scale analysis on two mainstream models, the Deep Learning Recommendation Model (DLRM)\cite{mudigere2022software} and the Transformer model.
In Figure \ref{fig:intro}(a), we present the scaling curves for both DLRM\footnotemark and Transformer models, and an oracle curve is included to represent an ideal scaling curve, characterized by a higher starting point and a larger slope. In Figure \ref{fig:intro}(b), we present an AUC curves derived from simulations of various combinations of sequence lengths and layer number, and introduce the concept of "performance interval", which represents the range of AUC variation for the model under the equivalent FLOPs.
\footnotetext{The baseline model that has been implemented for online deployment.}
% In Figure \ref{fig:intro}(b), it is shown that the Transformer model exhibits different AUC values under the same FLOPs, highlighting the performance and computational resource losses caused by unreasonable resource allocation in the model.
However, our findings reveal that some issues still remain in recommendation systems: 
\begin{itemize}
\item \textbf{Transformer's degraded performance under FLOPS constraints}: As shown in Figure \ref{fig:intro}(a), the FLOPs corresponding to the crossover point are $10^{8.2}$. Using this FLOPs value as the boundary, the performance comparison between DLRM and Transformer shows different trends. Transformer model outperforms traditional architectures such as DLRM when FLOPS exceed $10^{8.2}$ magnitude. However, when FLOPS less than $10^{8.2}$, Transformer model shows worse performance compared to DLRM. This highlights the pursuit for models with the oracle curve shown in Figure \ref{fig:intro}(a), which enables the model to achieve better performance even when the FLOPs are limited.
\item \textbf{The impact of factor combinations on model performance under equivalent FLOPs}: In recommender systems, factors such as sequence length and layer number significantly influence FLOPs, and different combinations of  these factors can lead to varying model performance. For instance, there is a obvious performance interval(nearly 1\%) under different combinations at $10^9$ FLOPs, as shown in Figure \ref{fig:intro}(b). Current researches lack comprehensive analysis of how factor combinations impact recommendation model performance, which hinders efficient scaling for models. 
% Similarly, FLOPs values of $10^{7.4}$ and $10^{8.1}$ achieve the same AUC, yet their computational complexity or inference efficiency varies by a factor of $10^{0.7}$.
\item \textbf{Huge resource consumption}: As depicted in Figure \ref{fig:intro}(a), to outperform the DLRM, Transformers require FLOPs on the order of $10^{9}$ to $10^{10}$. However, the FLOPs of the currently deployed DLRM models are only at the order of $10^7$, indicating a 100$\times$ increase of FLOPS to achieve comparable performance, which requires tens of thousands of GPU hours in practice. These resource demands are already beyond the affordability of most industrial applications. Consequently, achieving resource-optimal scaling remains a significant challenge for industrial recommender systems.
\end{itemize}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{intro_v4.pdf}
  \caption{(a) Scalability: DLRM vs. Transformer. The left part of crossover point indicates that Transformer underperforms DLRM when FLOPs are limited. The right part of crossover point highlights the increasing computational demands as Transformer performance improves. 
  (b) Toy Example: Simulated AUC curve with performance interval. This figure depicts an AUC curve derived from simulations of various combinations of sequence length and layer number.
  % For instance, at $10^9$
  % FLOPs, the lower bound of this interval corresponds to 4X sequence length and Y layers, yielding an AUC of 0.827. Conversely, the upper bound corresponds to X sequence length and 4Y layers, yielding an AUC of 0.834. The lower and upper bounds of the performance interval represent the worst and best model performance under given FLOPs, respectively.
  }
  \label{fig:intro}
  \Description{}
\end{figure}

% Inspired by the above work, we effectively implement the Transformer model within Netease’s large-scale music recommendation system to replace our traditional DLRM. However, there are still several major challenges: 1) low efficiency: We find that the performance of the model improves as the number of computations (flops) increases. These factors that affect the flops of the recommender system typically include the sequence length and the number of layers, etc. Besides, different combinations of length and number of layers affect the model's performance under the same flops. Currently, there is a lack of research on the effect of different combinations of parameters on the model, which results in a lot of work that cannot be scaled up quickly. 2) high cost: Increasing the sequence length and the number of layers will improve the model performance in recommender systems. However, in industrial systems, the model cannot be infinitely enlarged due to the requirement for fast and efficient online requests. It is struggling to balance model performance and training and inference efficiency in the case of limited resources. 3) low extensibility: The existing scalable models in recommendation systems are commonly designed for specific scenarios. If a specific model is designed for each scenario, the cost will also increase many times over. Therefore, a scalable model that can be used in multiple scenarios is needed. 

% In this paper, we propose a new sequence recommendation model, Adaptive Self-attention based TRansformer blOck (ASTRO). ASTRO adjusts the distribution of attention across different sequences and scenarios by adaptively learning the temperature coefficient in softmax. It then uses multiple transformers to perform parallel operations on multiple sequences. This approach mitigates problems such as high computational complexity due to sequence length and varying task distributions within recommendation systems, thereby facilitating rapid scale-up under all traffic conditions. Besides, we implement a two-stage Unified Ranking with Batched Output (TURBO) framework. As an unified sorting framework, TURBO compresses traditional (1 user, 1 item) samples into a sample format that is closer to the actual online requests (1 user, N item), and accelerates forward propagation during training and inference through Encoder level KV Cache. TURBO achieves significant efficiency improvements in training and inference with the same computational resources. Finally, we explore the scaling laws of multiple factor combinations in the block perspective, which can be used to allocate resources more rationally and guide us to identify the key factors to quickly scale the model.

Driven by the above insights, we introduce Climber, a novel framework that rethinks the scaling paradigm for recommendation systems. At its core, Climber integrates two complementary innovations: Adaptive Scalable Transformer for RecOmmendation (ASTRO) model architecture and Two-stage Unified Ranking with Batched Output (TURBO) acceleration framework. ASTRO redefines how recommendation systems handle heterogeneous user behaviors by introducing multi-scale sequence partitioning, which decomposes user behavior sequences into smaller, fine-grained subsequences. This approach not only reduces computational complexity, but also enables more precise modeling of user interests across diverse scenarios. In addition, ASTRO incorporates dynamic temperature modulation, a mechanism that adaptively adjusts attention scores to account for the varying importance of different behaviors and scenarios. On the engineering side, TURBO introduces a unified ranking framework that optimizes both training and inference efficiency. As a unified ranking framework, TURBO transforms traditional "Single User, Single Item" samples into a format that aligns with actual online requests, namely "Single User, Multiple Items". TURBO accelerates forward propagation during training and inference with encoder-level KV Cache, achieving significant efficiency improvements.
Finally, we investigate the scalability of our proposed model and the impact of factor combinations on AUC under equivalent FLOPs. These insights enable more rational resource allocation and guide the identification of key factors for rapid model scaling.

Our contributions are mainly categorized as follows:
\begin{itemize}
% \item We give details of both offline and online scaling laws from a new perspective, and are the first to explore the parameter combinations with the highest ROI under the same FLOPs. This allows us to quickly scale up the model performance at the lowest cost. The online metric is increased by 11.45\%, achieving the largest improvement in recent one year in our music recommender system.
\item We present the first industrial-scale study of scaling laws in recommendation systems from the computational resources-constrained aspects, explicitly quantifying the impact of factor combinations under equivalent FLOPs. This analysis reveals that balanced scaling—alternating sequence and depth expansions—yields both offline and online metric gains.
\item We propose a novel Transformer variant, ASTRO, which resolves the scaling dilemmas in recommendation systems through multi-scale partitioning and adaptive temperature modulation. To our knowledge, the proposed method pioneers sustainable scaling—delivering +12.19\% online metric growth, which is the highest annual improvement in our production system.
\item A unified framework bridging training-inference gaps via dynamic feature compression and block-parallel KV caching. Deployed on Netease Cloud Music, TURBO sustains 5.15$\times$ throughput gains, enabling 100$\times$ model scaling with moderate increase in computational resources.
\end{itemize}

\section{RELATED WORK}
Wukong \cite{zhang2024wukong} explored parameter scaling in retrieval models but relied on strong assumptions about feature engineering. HSTU \cite{zhai2024actions} reformulated recommendations as sequential transduction tasks, achieving trillion-parameter models with linear computational scaling via hierarchical attention and stochastic sequence sampling. However, HSTU’s focus on generative modeling left gaps in bridging traditional feature-based DLRMs. Concurrently, MARM \cite{lv2024marm} proposed caching intermediate attention results to reduce inference complexity from ${O(n^2d)}$ to $O(nd)$, empirically validating cache size as a new scaling dimension. While effective, MARM’s caching strategy assumes static user patterns, overlooking real-time behavior shifts.

Techniques to mitigate computational costs have been widely adopted. In NLP, KV caching \cite{liu2024scissorhands,dong2024get} avoids redundant attention computations during autoregressive inference. MARM adapted this idea to recommendations by storing historical attention outputs, enabling multi-layer target-attention with minimal FLOPs overhead. Similarly, HSTU introduced Stochastic Length to sparsify long sequences algorithmically, reducing training costs by 80\% without quality degradation. For advertisement retrieval, Wang et al. \cite{wang2024scaling} designed $R/R^*$, an eCPM-aware offline metric, to estimate online revenue scaling laws at low experimental costs. These works collectively highlight the importance of tailoring efficiency strategies to recommendation-specific constraints, such as high-cardinality features and millisecond-level latency requirements.

\section{Method}

% In this section, we focus on introducing the Climber framework. We discuss the challenges associated with model scaling. These challenges include the demand for computational resources and the mismatch between heterogeneous user behaviors and Transformer architecture. To address these issues, we introduce ASTRO model architecture and a co-designed TURBO acceleration framework.  
To achieve efficient scaling, we first propose the ASTRO model, which reduces computational complexity through multi-scale sequence partitioning and adapts Transformer architecture to recommender systems by integrating multi-scenario and multi-behavior characteristics. Besides, we introduce a co-design acceleration framework, TURBO, which incorporates dynamic compression and encoder-level KV cache to enhance the efficiency of training and inference without performance degradation.

% However, there still exists issues encountered when scaling up recommendation model:

% Complexity: it is known that the time complexity $O(n^2 * d)$ of self attention mechanism is much higher than $O(n * d)$ of target attention mechanism. Especially in the case of longer sequence, self attention mechanism can cause a large increasing in FLOPs, resulting in offline/online evident computation time increment. It means that when the sequence length increases by $m$ times, the time complexity will increase by $m^2$ times, which cause the degradation in training and inference efficiency.

% Extensibility: As an important feature, user history behavior is a continuous list of behaviors generated by users in the past period of time. Many recommendation systems typically choose a fixed time interval for behavior selection, such as 30, 90, 180, 360 days. However, there are many items in the sequence that users are almost unaware of (i.e. browsing time less than 5 seconds in video/music scene), and these items provide very little effective information. As a result, the benefits of extending the sequence in this way are in fact very small.

% Conflict: The user behavior type of each platform is diverse, and on our music platform, the important behaviors include clicking, playing-end, collecting and etc. Some important behaviors are actually relatively sparse, which leads to  conflicts in attention allocation between a few important behaviors and the majority of unimportant behaviors. Taking collection behavior as an example, according to the above way of organizing one sequence in a limited time window, the collection behavior accounts for less than 5\% in the whole sequence, so its attention score is expected to be $E(1/20)$. Important behavior will be diluted in a single sequential organization, which is not conducive to important tasks prediction.

% Under these specialized issues in the recommendation domain, it is difficult to simply elongate the sequence and increase the number of model layers to enable the model to have the ability to scale up quickly even with a transformer structure. The framework described in the following section is able to overcome these problems to achieve the purpose of our fast scale-up.

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=\linewidth]{kv_cache.pdf}
%   \caption{Turbo}
%   \Description{}
% \end{figure}



\subsection{Scaling Dilemma in Recommender Systems}
% \subsubsection{Computational Complexity}
In Recommendation Systems(RS), model scaling mainly involves two approaches: feature scaling and model capacity scaling. Feature scaling, particularly the extension of user behavior sequences, has been a focal point, with extensive research demonstrating its effectiveness in enhancing model performance \cite{pi2020search,chen2021end,chang2023twin,si2024twin}. Meanwhile, the integration of Transformer architectures has markedly enhanced the modeling capabilities of recommendation systems\cite{zivic2024scaling,liu2024kuaiformer,sun2019bert4rec,chen2019behavior}. However, combining these two approaches for synchronous scaling results in a quadratic increase in computational cost. This increase not only raises the resource requirements for training and inference but also puts more stress on real-time efficiency. Therefore, achieving a balance between performance improvement and computational efficiency has become an urgent challenge in recommendation systems.

In NLP, words form continuous sequences with explicit syntactic and semantic relationships. In contrast, user behavior sequences in RS are inherently fragmented, spanning multiple scenarios (e.g., browsing, purchasing) and heterogeneous behaviors (e.g., clicks, likes)\cite{zhang2024scaling,hou2022towards,li2023text,hou2023learning}. When fed into Transformer models, these fragmented sequences often lead to disordered attention distributions, as the model struggles to prioritize relevant behaviors within sparse and irregular patterns. This inefficiency limits the Transformer's scalability, particularly under computational constraints, where specialized models like DLRMs often outperform due to their tailored handling of sparse data.
% }

\begin{figure}[htbp]
  \centering
  % \includegraphics[width=6in]{model_v2.pdf}
  \includegraphics[width=\linewidth]{model_v6.pdf}
  \caption{
  ASTRO Model Architecture.
  % 1) Multi-Scale Sequence Partitioning is adopted to generate multi-scale sequences from user lifecycle sequence. 2) Adaptive Transformer Layer learns temperature coefficient in Softmax activation to adjust attention score distribution. 3) Bit-wise Gating Fusion module is designed to fuse the representations of different subsequences extracted by different blocks.
  }
  \Description{}
  \label{fig:model_arch}
\end{figure}

\subsection{ASTRO Model Architecture}
\subsubsection{Overall}  
% To explore the fast and effective scalability of recommendation systems, we propose the ASTRO model, which integrates the multi-behavior and multi-scenario characteristics of recommendation systems into the Transformer architecture. Unlike NLP tasks that typically handle single continuous sequences, recommendation systems involve non-continuous and diverse sequences. Our design philosophy follows the principle of "1 to N, N to 1," and our model comprises three modules: positive behavior extraction, adaptive Transformer layer, and block fusion module. Specifically, positive behavior extraction generates multiple subsequences from user lifecycle behaviors (1 to N). The adaptive Transformer layer then discriminatively learns the similarity between candidate items and these multiple subsequences. Finally, block fusion module aggregates the representations from subsequences to produce a unified output (N to 1).

% A more detailed workflow of our model architecture is presented in Figure \ref{fig:comp_sketch}. We transform sequence to different blocks, and these subsequences can represent different types of behavior, as well as long-term and short-term sequences of a certain behavior. We use a corresponding transformer block for each subsequence to extract users' interests. Besides, we treat some important behaviors as a separate block, and extent its time period to the whole lifetime. Then an adaptive temperature in activation is used to adjust the attention distribution of different kinds of subsequences in different scenarios. For example, if some behaviors are related to the user's short-term interest, a lower temperature coefficient may be adopted, while some behaviors are related to the user's long-term interest, a higher temperature coefficient may be adopted. Different scenarios will also have different temperature coefficients in the training process. With user interest representation extracted from user subsequences, a fusion module is proposed to realize cross attention between different subsequences by self attention and bit-wise gating mechanism. 

% Figure \ref{fig:comp_sketch} illustrates the detailed workflow of our model architecture. In positive behavior extraction, we divide the user behavior sequences into multiple subsequences, where each subsequence represents different types of sequence (multi-scenario/multi-behavior/short and long-term). For each subsequence, we apply a corresponding Transformer block to extract user interests. Additionally, we extend important subsequences' time span to the user's entire lifecycle. In adaptive transformer layer, we use an adaptive temperature coefficient in the activation function to adjust the attention distribution across different subsequences in various scenarios. For example, behaviors related to short-term interests learn a lower temperature coefficient, while those related to long-term interests learn a higher temperature coefficient. Different scenarios are assigned different temperature coefficients during training. In block fusion module, we integrate the extracted user interest representations from these subsequences. This module enables interest fusion between different subsequences through self-attention and a bit-wise gating mechanism.

To address the dual challenges of computational complexity and fragmented sequence structures in recommendation systems, we propose ASTRO model. This model integrates the multi-behavior and multi-scenario characteristics of recommendation systems into the Transformer architecture, while considering the resource demands associated with scaling. 
% Unlike NLP tasks that typically handle single continuous sequences, recommendation systems involve non-continuous and diverse sequences. 
% Our design philosophy follows the principle of "one to many, many to one,"
Our model comprises three modules: Multi-scale Sequence Partitioning (MSP), Adaptive Transformer Layer (ATL), and Bit-wise Gating Fusion (BGF).
Specifically, \MSP generates multi-scale sequences from user lifecycle sequence. These multi-scale sequences represent different types of subsequences (multi-scenario/multi-behavior/short and long-term behaviors). For each subsequence, we apply a corresponding transformer block composed of stacked ATLs to extract user interest. Additionally, we extend the time span of important subsequences to cover the user's entire lifecycle.
\ATL uses an adaptive temperature coefficient in the activation function to adjust the attention distribution across different subsequences in various scenarios. 
% For example, behaviors related to short-term interests learn a lower temperature coefficient, while those related to long-term interests learn a higher temperature coefficient. Different scenarios are also assigned different temperature coefficients during training.
Finally, BGF aggregates the representations from adaptive Transformer blocks corresponding to each subsequence to produce a unified output, which enables interest fusion between multi-scale sequences through ATL and a bit-wise gating mechanism.
% This design not only enhances the model's ability to handle diverse user behaviors across multiple scenarios but also balances performance improvement with computational efficiency.
Figure \ref{fig:model_arch} illustrates the detailed workflow of ASTRO model.


% Through the above programs, various problems in preliminaries are basically solved:

% Complextity: When the sequence length is increased by $m$ times, the time complexity can be reduced from the $m^2$ increase to the $m$ increase. Significant acceleration can be obtained during training and inference stage.

% Extensibility: It has been demonstrated that the temporal limitations imposed on critical behaviors can be circumvented. For behaviors that are of paramount importance and infrequent, the full lifecycle behavior is employed, yielding a more pristine sequence than the equivalent sequence of length $10^3$. This approach is compatible with the block framework, thereby enhancing the efficacy of training and inference processes.

% Conflict: This issue is addressed by extending the critical sparse behaviors to the user's entire lifecycle, which contributes significantly to the available information augment. Additionally, the block approach alleviates the attention conflicts between different subsequences.

\subsubsection{\textbf{Multi-Scale Sequence Partitioning}}
The approach of \MSP involves reorganizing user sequences based on different strategies, which can be represented by the following formula:
\begin{gather}
S = \{x_1,x_2,...,x_{n_{s}}\} \label{eqn:S} \\
S_k = \text{MSP}(S, a_k) = \{x^{a_k}_{1},x^{a_k}_{2},...,x^{a_k}_{n_k}\} \label{eqn:Sk}
\end{gather}
where $S$ represents the user lifecycle sequence,  $x_i$ denotes $i$-th item ID from the entire item set $\textbf{X}$. $S_k$ represents the $k$-th subsequence extracted from the user lifecycle sequence $S$ based on the extraction strategy $a_k$, and $x^{a_k}_{j}$ indicates the $j$-th item in the subsequence $S_k$. The $n_s$ and $n_k$ denote the length of $S$ and $S_k$, respectively. We assume that there are a total of $N_b$ extraction strategies and $\sum_{k=1}^{N_b} n_k=n$. In our practical application, $n \ll n_s$. This is because the extraction strategy typically retains only the user's positive behaviors. Consequently, the computational complexity under a single Transformer can be reduced from $O(n_s^2d)$ to $O(n^2d)$. We further improve the training process by employing the corresponding Transformer block for each subsequence $S_k$, thereby achieving a time complexity of $O(n_k^2d)$. We design each extraction strategy to extract subsequences of equal length, such that $n_k=n/N_b$. In the case of fully serial operations, this results in a time complexity of $O(n^2d/N_b)$. Therefore, even when $N_b=2$, we can still achieve substantial training acceleration.
In summary, \MSP reduces the computational complexity from $O(n^2d)$ to $O(n^2d/N_b)$, and transform user lifecycle sequence into multi-scale sequences. These enhancements improve the efficiency and scalability of the recommendation system.

% This section introduces positive behavior extraction using a multi-behavior segmentation approach, motivated by two key challenges. First, the time complexity $O(n^2d)$ of Transformer significantly increases computational burdens for training and inference as sequence length increases. Second, integrating multiple behaviors into a single sequence can dilute the impact of critical yet sparse behaviors. To address these challenges, we organize sequences based on distinct behaviors. Specifically, for frequent behaviors (e.g., playend), we use data from the past 3 months; for extremely sparse behaviors (e.g., share and comment), we use data from the past 1 year; and for user asset behaviors (e.g., collect), which are sparse but crucial for capturing user interests, we consider the user entire lifecycle. It should be noted that the sequence length is constrained. For entire lifecycle behavior sequences, the collect sequence length is set to $10^3$, covering over 95\% of users' all collected items in our application scenario. Importantly, subsequence extraction is flexible: if a frequent behavior sequence is excessively long, it is divided temporally to balance model performance and efficiency. Thus, positive behavior extraction can be defined as follows:
% \begin{equation}
% \scriptstyle (i_1,i_2,...,i_{n_i}),(j_1,j_2,...,j_{n_j}),(k_1,k_2,...k_{n_k}) = PSE(x_{1}, x_{2},..., x_{n})
% \end{equation}
% we transform Equation 4 to a new multi-subsequence function as:
% \begin{equation}
% \scriptstyle f((a_{i}^{'},i_1,i_2,...,i_{n_i}),(a_{j}^{'},j_1,j_2,...,j_{n_j}),(a_{k}^{'},k_1,k_2,...k_{n_k}),x_{c})
% \end{equation}
% where we transform sequence to different blocks, and $i, j, k$ represent different subsequences divided according to extraction strategy $a^{'}$, respectively.

% In this approach, subsequence division is predominantly predicated on the type of behavior. There are two main factors that inspired this part. Firstly, as the sequence lengthens, the Transformer's $n^2$ time complexity imposes a substantial burden on the training and inference processes due to its high computational demands. Secondly, when a multi-behavior sequence is integrated into a single sequence, the impact of other behaviors on critical behaviors is exacerbated due to their inherent sparsity. To address these challenges, we propose a modification to the sequence organization, wherein multiple sequences are arranged based on distinct behaviors, facilitating more efficient scalability. The definition of organizational behaviors is as follows: for relatively frequent behaviors, such as play-end behaviors, we select $N$ months of behavioral data; for extremely sparse behaviors, such as user sharing and commenting, the time is stretched to 1 year; for user asset sequences, such as user collection, which is sparse but expresses the user's interest in the core, the time is expanded to the whole life cycle. It should be noted that the number of sequences is limited. For the full life cycle behavior sequences, the order of magnitude in the model training is set to $10^3$, which can cover more than 95\% of the users' collection sequences in the application scenario under study. It is noteworthy that there is no rigid prerequisite for subsequence splitting. In instances where a frequent behavior sequence is exceedingly extensive, it will be divided according to time, thereby achieving a balance between model performance and efficiency. Thus, the sequence organization called positive signal extraction can be defined as:

\subsubsection{\textbf{Adaptive Transformer Layer}}
The Softmax activation function, which normalizes the attention scores, plays a pivotal role in the Transformer architecture. Specifically, the attention mechanism is normalized by $\frac{QK^T}{\sqrt{d_{k}}}$ and subsequently multiplied by $V$. The division by $\sqrt{d_{k}}$ ensures that the distribution of the attention matrix aligns with that of $Q$ and $K$ \cite{hinton2015distilling}. 
However, we generate multi-scale sequences based on different extraction strategies from user lifecycle sequence, and apply corresponding Transformer block to each subsequence. A single scaling factor $\sqrt{d_{k}}$ is insufficient to accommodate the diverse requirements of all Transformer blocks corresponding to multi-scale sequences \cite{he2018determining}.
To further refine the attention distribution within each Transformer block, we introduce an adaptive temperature coefficient for each layer of each block.
We refer to this change as Adaptive Transformer Layer (ATL) , which can be mathematically expressed as follows:
% \begin{equation}
% \begin{gathered}
% Q,K,V =f_{QKV}(S_k) \\
% A(S_k) = \text{Softmax}(QK^{T}/f_T(a_k, r)) \\
% Y(S_k) = f_{FFN}(A(S_k)V)
% \end{gathered}
% \end{equation}
\begin{equation}
\begin{gathered}
Q,K,V =f_{QKV}(X(S_k)), \\
R(S_k) = QK^{T} + f_{b}^{p,t}(a_k, r),  \\ 
A(S_k) = \text{Softmax}(R(S_k)/f_{tc}(a_k, r)), \\
Y(S_k) = f_{FFN}(A(S_k)V).
\end{gathered}
\label{eqn:ATL}
\end{equation}
Compared to conventional Transformer layer, we introduce an adaptive temperature coefficient and adjust relative attention bias from a recommendation perspective. Here, $X(S_k)\in \mathbb{R}^{s \times d}$, $R(S_k)\in \mathbb{R}^{h \times s \times s} $, $A(S_k) \in \mathbb{R}^{h \times s \times s}$ and $Y(S_k) \in \mathbb{R}^{s \times d}$ represent layer input, raw attention matrix, normalized attention matrix, and layer output, respectively. $s$, $h$, $d$ represent sequence length, head number and feature dimension, respectively.
$f_{QKV}(X(S_k))$ is used to derive query, key, and value matrices from input $X(S_k)$.
$f_{b}^{p,t}(a_k, r)$ denotes relative attention bias \cite{zhai2024actions, raffel2020exploring} that incorporates positional ($p$) and temporal ($t$) information.
$f_{tc}(a_k, r)$ denotes a function that derives the temperature coefficient.
It is important to note that both extraction strategy $a_k$ and recommendation scenario $r$ influence the relative attention bias and the temperature coefficient.
$f_{FFN}$ processes the attention-weighted value matrix through a feedforward neural network (FFN) to produce the final output of the layer. This approach is inspired by the multi-scenario and multi-behavior characteristics of recommendation systems. Specifically, the adaptive temperature coefficient allows for more flexible attention weighting, addressing the limitations of the fixed scaling factor $\sqrt{d_{k}}$ in capturing the inherence of diverse behaviors and scenarios.
  
% The detailed distribution of attention scores will be elaborated in the experimental section.
% From multi-scenario perspective, our model is applied to all application scenarios, which are divided into two main categories: list recommendation and stream recommendation. List recommendation scenarios generate a static list of items for users, while stream recommendation scenarios continuously deliver items based on user real-time interactions. Therefore, stream recommendation benefit from lower temperature coefficients to emphasize recent behaviors, and list recommendation is more suitable for higher temperature coefficients. From multi-behavior perspective, playend sequence is more influenced by recent items and thus is suitable for lower temperature coefficient, while collect sequence reflecting long-term preferences, is suitable for higher temperature coefficient.

% The Softmax activation function used to normalize the attention score plays a pivotal role in the Transformer's architecture. The Softmax is normalized by $\frac{QK}{\sqrt{d_{k}}}$ and subsequently multiplied by V, where dividing by $d_{k}$ is to ensure that the distribution of the attention matrix is consistent with $Q$ and $K$. An adaptive temperature coefficient has been incorporated to model the personalization within each block. This approach draws inspiration from the multi-scene and multi-task problem inherent to recommender systems. First, the model is applied to all the recommendation scenarios of cloud music, which includes two core scenarios, "dailysong" and "userfm". The recommendation form of "dailysong" is list recommendation, while "userfm" is a real-time stream recommendation. Therefore, "userfm" may be more suitable for lower temperature coefficients to strengthen the expression of recent behaviors, while "dailysong" is suitable for smoother temperature coefficients. For different tasks, behaviors such as sharing and commenting are more susceptible to the influence of recent behavior, while collecting behavior is more susceptible to the influence of long-term behavior. Consequently, these two tasks are suitable for lower and higher temperature coefficients, respectively. This point will be discussed in detail in the experimental section. We define it as an adaptive transformer, which can be expressed using this formula:

\begin{figure*}[htbp]
  \centering
  \subfigure[]{\includegraphics[scale=0.4]{CompSketch2.pdf}}
  \hspace{5mm}
  \subfigure[]{\includegraphics[scale=0.3]{kv_cache_v1.pdf}}
  \caption{Two key features of TURBO: (a) Sketch of compression algorithm on training dataset; (b) Sketch of encoder-level KV cache with block-wise parallelism}
  \Description{}
  \label{fig:comp_sketch}
\end{figure*}

\begin{figure}[htbp]
  \centering
  % \includegraphics[width=\linewidth]{CompRatioAndFLOPs.pdf}
  \includegraphics[width=\linewidth]{Compression2.pdf}
  \caption{Compression Ratio and Training FLOPs based on our dataset}
  \Description{}
  \label{fig:comp_ratio_flops}
\end{figure}

\subsubsection{\textbf{Bit-wise Gating Fusion}}
However, when user lifecycle sequence is separated into multi-scale sequences, there is a lack of interaction between them \cite{xiao2020deep,li2019multi}. Therefore, we propose a bit-wise gating fusion module that integrates information across the $N_b$ subsequences corresponding to our $N_b$ blocks. Specifically, each block generates an output vector $E(S_k)$ through each adaptive Transformer block, and these $N_b$ output vectors are concatenated as $E(S) \in \mathbb{R}^{N_b \times d}$. The concatenated vectors are then processed through a new ATL, followed by a sigmoid activation function to achieve bit-level gating. Finally, the vectors pass through a subsequent network to produce the final output score. The bit-wise gating fusion module can be formally expressed as:
\begin{equation}
\begin{gathered}
    E(S) = \{E(S_1),E(S_2),...,E(S_{N_b})\}, \\
    G(S) = \text{ATL}(E(S)), \\
    Y(S) = G(S) \odot \sigma\big(f_{gate}(G(S))\big).
\end{gathered}
\end{equation}
where $\sigma$ is sigmoid activation and $f_{gate}$ represents a squeeze-and-excitation module \cite{hu2018squeeze}, which ensure the identity between the input and output dimensions to dynamically adjusts the contribution of each element in $G(S) \in \mathbb{R}^{N_b \times d}$.
$f_{gate}(G(S)) \in \mathbb{R}^{N_b \times d}$ and $Y(S) \in \mathbb{R}^{N_b \times d}$ represent the bit-wise attention matrix and the output of fusion module, respectively.
$\text{ATL}$ (Adaptive Transformer Layer in Equation \ref{eqn:ATL}) is used to calculate the similarity between different blocks to interact the different interest from subsequences. In contrast, the ATL in fusion function doesn't incorporate relative attention bias, and the temperature coefficient is solely determined by the recommendation scenario.
The attention operation of ATL on $N_b$ blocks can be regarded as field-wise interactions \cite{hu2018squeeze,huang2019fibinet}, which facilitate information exchange at the feature level. Our method enhances interest fusion by adding bit-wise interactions. This allows the model to capture precise relationships among sequences. As a result, the model's ability to understand multi-scale sequences is significantly improved.
It is worth mentioning that although this attention mechanism has a complexity of $O(N_b^2d)$, the number of extraction strategy $N_b$ is much smaller than the dimension $d$ of the feature vector in the fusion stage. Thus the computational complexity of bit-wise gating fusion module is relatively low.

% The proposed recommendation model exhibits remarkable merits that significantly enhance its performance and applicability. 1) the model complexity achieves a substantial reduction from $O(n^{2}d)$ to $O(n^{2}d/m)$, where $m$ denotes the number of subsequence block. This enhancement enables a palpable acceleration in both training and inference processes even in the case of $m=2$, thereby rendering the model more efficient and scalable for handling large-scale datasets.
% 2) our model breaks through temporal constraints for certain crucial behaviors. For highly sparse yet significant behaviors, we utilize users' entire life-cycle behaviors. Compared to sequences of the same length $10^{3}$, our approach yields purer sequences. Moreover, the compatibility with the block framework not only ensures seamless integration but also further boosts the efficiency of training and inference, making the model more adaptable to diverse recommendation tasks and datasets.
% 3) By extending important sparse behaviors to the entire life-cycle behaviors, the model gains a significant enrichment of information. Additionally, the block-wise approach alleviates unfair allocation of attention scores between different subsequences, which will be discussed in the next section. Overall, these advantages collectively contribute to the robustness and superiority of our recommendation model, setting it apart as a highly promising solution in the realm of recommendation systems.

\subsection{TURBO Acceleration Framework}
A typical ranking model for recommendation systems involves elaborate features from one user and one item as model inputs. Probabilities or scores for specific tasks such as  is-click or not are calculated as model outputs through a well-designed neural network. Such an arrangement for model inputs and outputs can be summarized as "Single User, Single Item (SUSI)". It's instinctive to expand the first dimension of input tensor to batch size during training stage to achieve "Multiple Users, Multiple Items (MUMI)".

When it comes to the online serving system, "Single User, Multiple Items (SUMI)" is required to get scores for multiple candidates in a single request from one single user. Simply stacking user features to the number of candidates as "MUMI" seems like a common approach. However, "MUMI" might not be affordable for online serving due to the extra time costs caused by repeated computation of user feature. This redundant computation is almost unacceptable especially for transformer-dominated architectures with user features composed by long sequences. Zhai, et al.\cite{zhai2024actions} proposed the M-FALCON algorithm to achieve "SUMI"-style batched inference. Online throughput performance can be boosted with the aid of kv-cache \cite{pope2023efficiently} and micro-batch parallelism. 

Further, we propose TURBO, formulating these prefill-encode two stages into one forward pass. With re-arrangement of training dataset via dynamic compression algorithm explained below, TURBO serves as a unified framework compatible for both offline training and online inference.

\subsubsection{Dynamic Compression on Training Data}
To accommodate the training phase to the "SUMI" setting in TURBO, a fixed number of labels and features from the item side related to the same user need to be aggregated to the form of label list and feature list. The list can be padded to a constant size $N_c$ across all users, and we call $N_c$ as the degree of aggregation. 
It should be noted that user behavior might slowly accumulate over a period of time. If user behavior sequence grows up within an aggregated batch, the longest as well as the latest sequence should be kept to avoid loss of information. What's more, an extra dependence list need to be stored for each row of training sample as shown in Figure \ref{fig:comp_sketch}. With the help of this auxiliary list, we can build customized attention mask matrix for each row to prevent violation of causality. 

The value of $N_c$ should be determined carefully for a given training dataset. Either too small or too large value of $N_c$ would be harmful to data compression and computation efficiency. Sample distribution $g(n)$ of a given dataset can be defined as the number of unique user who have the total number of record $n$. For instance, $g(2)=1000$ means that there are 1000 unique users each has 2 row of records in a given dataset. Let $N$ denote total number of record of training dataset, $N_p$ denote total number of rows to be padded, $D_{user}$ and $D_{item}$ denote the average number of bytes in each row to store user features (including user behavior sequence) and item features respectively. The compression ratio $\alpha$ can be estimated by the following formulation \ref{eqn:comp_ratio}. The term "$\operatorname{sizeof}(int)$" shows the part of storage for dependence list composed by integer element.

\begin{gather}
p(n) = \left\lceil \frac{n}{N_c} \right\rceil \cdot N_c - n \\
N_p = \sum_n g(n)\cdot p(n) \\
\alpha = \frac{(N + N_p)D_{user}/N_c + (N + N_p)(D_{item} + \operatorname{sizeof}(int))}{N(D_{user} + D_{item})} \label{eqn:comp_ratio}
\end{gather}


\subsubsection{Encoder-level KV-cache with Block-wise Parallelism}
Since each block under ASTRO architecture is independent, calculations between blocks can be parallelized. We propose to fuse all transformer blocks with the same number of layers into a whole module. The first dimension in input tensors, weight and bias parameter is extended, which represents the block number in multi-head attention module and FFN module. Block-wise parallelism is actually achieved through scheduling of larger matrix operations at CUDA level. 

To achieve lower computation costs, we followed the idea in M-FALCON to conduct a two-stage forward pass. The first stage is to get key/value cache vector for items in user behavior sequence. For the last layer in transformer module, results of kv-cache can be returned immediately without subsequent calculating like attention scores and FFN module. We named this trick "last layer short-circuit" to save a certain amount of computation.
The second stage is to get scores for target items in a batched way with kv-cache obtained in the first stage. Computation cost in the second stage is proportional to the size of micro-batch, which is $N_c$ during model training phase. The total training FLOPs $F_{total}$ for one epoch can be estimated as summation of the two stages:
\begin{align}
F_{total} = \frac{N + N_p}{N_c} \cdot \left( F_{stage1} + F_{stage2}(N_c) \right) \label{eqn:train_flops}
\end{align}

We plot the total training FLOPs for one epoch of our dataset against $N_c$ in Figure \ref{fig:comp_ratio_flops}, along with the compression ratio estimated through Equation \ref{eqn:comp_ratio}. It can be shown that the optimal $N_c$ to achieve maximum storage savings and the one to achieve minimum training cost are very close. Thus, we determined to use $N_c=5$ as the base setting for the rest of the discussion.

\section{Experiments}
% In this chapter, we will first compare the performance of our model structure with other state-of-the-art methods on different datasets; Then, the attention score will be used as an intermediary to discuss how the model structure modification points are related to specific problems in the recommendation system; Finally, we presented the scaling laws curves for sequence length, and model layers, as well as how we determined the iterative path for online scaling up based on the curves. Finally, we obtained online metrics for scaling up in multiple scenarios and corresponding performance improvements.
In this section, we detail the offline and online experiments conducted on real industrial data to evaluate our proposed method, addressing the following four research questions:
\begin{itemize}
\item RQ1: How does ASTRO model perform in offline evaluation compared to state-of-the-art (SOTA) models?
\item RQ2: How does ASTRO model demonstrate superior scalability compared to DLRM and Transformer?
\item RQ3: How can we allocate resources to scale up our model by considering the impact of different factor combinations on AUC under equivalent FLOPs? 
\item RQ4: How does Climber framework perform in industrial recommendation systems?
\end{itemize}

% \begin{table}[t]
% \centering
% \caption{Dataset Statics}
% \label{tab:Dataset}
% \begin{tabular}{@{}lcccccc@{}}
% \toprule
% Dataset & \#User & \#Item & \#Interaction \\
% \midrule
% Industrial &  &  &  \\
% Spotify &  &  &  \\
% 30Music &  &  & \\
% Amazon-Book & &  & \\
% \bottomrule
% \end{tabular}
% \end{table}

% \subsection{The results of model structure on different datasets}
\subsection{Experimental Setting}
\subsubsection{Dataset}
To validate the effectiveness of our method in recommendation systems, we construct a dataset using real user behavior sequence as the primary feature. The dataset is used to predicts different user actions on the candidate items based on historical interactions. Important user behaviors include full-play, like, share and comment. 
In order to protect data privacy, we only present statistical data for our recommendation scenarios.
% Our dataset comprises \textcolor{red}{tens of} millions daily active users, \textcolor{red}{several} million tracks available for recommendation each day, and \textcolor{red}{>100} million samples collected daily. 
% The average user behaviors per day are approximately \textcolor{red}{>60}. Due to the time required for users to consume one track, the number of generated interactions is smaller compared to other short-video platforms. The average sequence length of user full-play and like interactions over a month is \textcolor{red}{>30} and \textcolor{red}{>2}, respectively. 
Additionally, we evaluated our model on three recommendation datasets, \textbf{Spotify}\cite{brost2019music}, \textbf{30Music}\cite{turrin201530music} and \textbf{Amazon-Book}\cite{mcauley2015image}. 
A more detailed analysis of these datasets after preprocessing is presented in Appendix \hyperref[appendix:B.1]{B.1}.
% In our dataset, we exclude statistical features but retain only some categorical features of users and items. 

% \renewcommand{\thefootnote}{\fnsymbol{footnote}} % 设置脚注符号为非数字形式
\begin{table*}[htbp]
\centering
\caption{Evaluation of methods on public/industrial datasets}
\begin{tabular}{c|cc|cc|cc|cc}
% \begin{tabularx}{\columnwidth}{@{}ccccccccc@{}}
\hline
\multicolumn{1}{c|}{} & \multicolumn{2}{c|}{Spotify} & \multicolumn{2}{c|}{Amazon-Book} & \multicolumn{2}{c|}{30Music} & \multicolumn{2}{c}{Industrial} \\
 & AUC & LogLoss & AUC & LogLoss & AUC & LogLoss & AUC & LogLoss \\ \hline
DLRM(Baseline) & 0.7606 & 0.5761 & 0.7842 & 0.5541 & 0.8927 & 0.2012 & 0.8216 & 0.7067 \\ \hline
DIN & 0.7557 & 0.5803 & 0.7796 & 0.5580 & 0.8861 & 0.2044 & 0.8158 & 0.7109 \\
TWIN & 0.7589 & 0.5772 & 0.7831 & 0.5563 & 0.8903 & 0.2019 & 0.8203 & 0.7078 \\ 
Transformer & 0.7621 & 0.5735 & 0.7836 & 0.5557 & 0.8930 & 0.2007 & 0.8214 & 0.7074 \\ 
HSTU & 0.7626 & 0.5722 & 0.7869 & 0.5520 & 0.8938 & 0.1982 & 0.8217 & 0.7053 \\ \hline
ASTRO (-ATL,-BGF) & 0.7635 & 0.5710 & 0.7873 & 0.5518 & 0.8944 & 0.1978 & 0.8221 & 0.7045 \\
ASTRO (-BGF) & 0.7655 & 0.5694 & 0.7881 & 0.5510 & 0.8950 & 0.1972 & 0.8225 & 0.7034 \\ 
\underline{ASTRO} & \underline{0.7663} & \underline{0.5687} & \underline{0.7887} & \underline{0.5501} & \underline{0.8986} & \underline{0.1957} & \underline{0.8230} & \underline{0.7029}
\\
\textbf{ASTRO-large} & \textbf{0.7702} & \textbf{0.5666} & \textbf{0.7914} & \textbf{0.5472} & \textbf{0.9035} & \textbf{0.1916} & \textbf{0.8398} & \textbf{0.6911} \\ \hline
\textbf{\%Improve} & \textbf{+1.26\%} & \textbf{-1.64\%} & \textbf{+0.91\%} & \textbf{-1.24\%} & \textbf{+1.20\%} & \textbf{-4.77\%} & \textbf{+2.21\%} & \textbf{-2.20\%} \\ \hline
\end{tabular}
\label{tab:ASTRO}
\end{table*}
% \footnotetext{The baseline model that has been implemented for online deployment.}

\subsubsection{Compared methods}
% In the selection of comparison methods, in addition to DLRM as the comparison model on our front line, we also introduced two transformer based models, BST and HSTU, as the comparison models. In order to compare the model structure, we introduced block, activation, and fusion for experimentation. In order to ensure the fairness of the experiment, a unified setting was used for both the structure and features

% In our comparative study, we select DLRM as the primary benchmark model. In addition, we incorporate two Transformer-based models: Transformer and HSTU. For a systematic evaluation of the model architectures, we focused on three key aspects: postive behavior extraction, adaptive transformer layer, and block fusion module. Throughout the experiments, we ensured fairness and consistency by adopting a unified configuration for both the structural components and feature representations across all models.

In our comparative study, we select DLRM as benchmark model. DLRM is a model that leverages lifelong sequences and complex feature interactions, which has been deployed in our online systems. In addition to DLRM, we incorporate several SOTA models to provide a comprehensive evaluation. These models include: DIN\cite{zhou2018deep}, TWIN\cite{chang2023twin}, Transformer\cite{vaswani2017attention}, and HSTU\cite{zhai2024actions}. To systematically evaluate the model architectures, we focus on three key components: multi-scale sequence partitioning, adaptive Transformer layer and bit-wise gating fusion. We utilize the ASTRO-large model (6$\times$ layer number, and 4$\times$ sequence length in Industrial setting) to validate the model's scalability. 
Throughout the experiments, we ensure fairness and consistency by adopting a unified configuration for both training hyper-parameters and feature representations across all models. More details about compared methods and experimental setting are presented in Appendix \hyperref[appendix:B.2]{B.2}.
% \footnotetext[1]{http://jmcauley.ucsd.edu/data/amazon/}

\subsection{Overall Performance (RQ1)}
% \subsubsection{Results on Public Datasets}
\subsubsection{Performance Comparison}
% In this section, we will demonstrate the performance improvement of the model after reshaping, with relevant indicators shown in Table 1. From the table, it can be seen that no additional structural changes are required, and significant performance improvements can be achieved by simply disassembling the sequence; Our dataset contains data for all recommended scenarios, and it can be seen that with the introduction of adaptive temperature coefficients for each scenario, offline AUC also shows an increase; The fusion module facilitates the fusion of sequence information between different types, which is helpful for multi task prediction

As shown in Table \ref{tab:ASTRO}, the ASTRO model achieve the best performance in four recommendation datasets. Notably, the application scenarios of \textbf{Spotify} and \textbf{30Music} belong to the same domain as our industrial dataset, which is music recommendation systems. In contrast, \textbf{Amazon-Book} differs significantly from our scenario. However, our model still achieves favorable results on this dataset, indicating its potential adaptability to diverse applications.
Next, we focus on comparing the AUC improvements of ASTRO relative to other methods. 1) as our primary online model, DLRM outperforms DIN and TWIN because DLRM includes a wide range of feature interaction structures beyond lifelong sequence and attention mechanisms. 
2) The Transformer achieves +0.134\% AUC improvement over TWIN. This is because Transformer computes the similarity between all historical items and target item in a single stage.
% 3) The Transformer exhibits an AUC decrease of 0.0002(-0.024\%) compared to DLRM. This phenomenon is also observed in other datasets, suggesting a general trend across different recommendation scenarios. The result is primarily attributed to DLRM's focus on feature interaction, which allows it to achieve better performance before the overall model complexity reaches a level that would be required for the Transformer to match or exceed its performance.
3) HSTU implements several enhancements to the Transformer architecture, achieving an AUC improvement of +0.036\% on our dataset compared to Transformer. However, these enhancements also result in increased computational complexity.
4) ASTRO reduces computational complexity by sequence partitioning and adjusts attention distribution across multi-scenario and multi-behavior through adaptive temperature coefficients. This results in a +0.170\% improvement over DLRM. Furthermore, ASTRO-large achieves a +2.21\% AUC improvement by scaling up the model, achieving the largest offline gain in the past year.

\subsubsection{Abalation Study}
% First, we compare the Transformer and DLRM. The Transformer exhibits an AUC decrease of 0.0025 relative to DLRM. This phenomenon is also observed in other datasets, suggesting a general trend across different recommendation scenarios. The result is primarily attributed to DLRM's focus on feature interaction, which allows it to achieve better performance before the overall model complexity reaches a level that would be required for the Transformer to match or exceed its performance.
% Second, HSTU implements several enhancements to the Transformer architecture, achieving an AUC improvement of 0.0033 on our dataset compared Transformer. However, these enhancements also result in increased computational complexity.

% The subsequent analysis focuses on the impact of our model on the Industrial dataset. 
% Compared to Transformer, ASTRO (-ATL, -Fusion) achieves a positive AUC gain of 0.0040 by transformer users' lifecycle sequence into multiple subsequence blocks through positive behavior extraction. Besides, ASTRO (-Fusion) further enhances the Transformer by incorporating an adaptive temperature coefficient, resulting in an additional AUC improvement of 0.0009. Both enhancements directly impact the attention scores, which will be analyzed in detail in the following section.
% Following the incorporation of the fusion module, an improvement of 0.0005 in AUC is observed. This module is designed to integrate user interests represented by different subsequences, thereby emphasizing the importance of interest fusion in recommendation systems.
% In summary, our model demonstrates strong performance and adaptability based on the offline evaluations across various datasets.

To systematically evaluate the contributions of each component in our ASTRO model, we conduct a comprehensive set of experiments across multiple datasets. For illustrative purposes, we focus on the detailed ablation study conducted on the Industrial dataset and select Transformer for comparison with ASTRO series. By incorporating MSP, ASTRO (-ATL, -BGF) transforms user lifecycle sequence into multi-scale subsequence blocks. This enhancement results in a positive AUC gain of +0.085\% compared to the Transformer model. ASTRO (-BGF) further improves the model by introducing an adaptive temperature coefficient. This component adjusts the attention distribution dynamically, leading to an AUC improvement of +0.134\%. Finally, the incorporation of BGF brings about an AUC improvement of +0.195\%. This module integrates user interests represented by different subsequences, emphasizing the importance of interest fusion in recommendation systems. In summary, our model demonstrates strong performance and adaptability based on the offline evaluations across various datasets.

% \subsection{Discussion on the Influence of Model Structure}
% \subsection{Attention Distribution Analysis (RQ2)}
% Sequence modeling in recommendation systems is essential for identifying items in a user's historical sequence that are similar to the target item, with this similarity quantified through attention scores. Before the adoption of attention mechanisms, sequence processing relied on average pooling, which assigned uniform attention scores to all items. The attention mechanism in DIN has been shown to enhance recommendation performance by assigning distinct attention scores to different items. The Transformer-based model emphasizes attention mechanisms. Thus this section explores the impact of positive behavior extraction and adaptive temperature coefficients on the attention distribution. The attention score will be introduced for two distinct behaviors: playend and collect.

% \subsubsection{The impact of positive behavior extraction}
% Positive behavior extraction not only reduces computational complexity but also directly impacts attention distribution. Specifically, the proportion of "playend" behaviors is higher than that of "collect" behaviors. To address this imbalance, we distinguish between these two behaviors and extend the time window for "collect" behaviors, thereby making the number of "collect" and "playend" behaviors approximately equal.
% We conduct comparative experiments between the Transformer and ASTRO. In the Transformer experiment, sequences contain equal proportions of "collect" and "play" behaviors for computation. In contrast, the ASTRO model separates these behaviors and trains them in a block-wise manner, with attention distributions illustrated in Figure \ref{fig:deploy}.
% In the Transformer structure, even with a similar number of "collect" and "playend" behaviors, the attention ratio for "playend" behaviors reaches 71\%. This is primarily because "playend" samples have a higher proportion and naturally occupy more gradients. Consequently, "playend" behaviors receive higher attention scores due to the nature of the task, resulting in less attention to "collect" behaviors and poorer performance on "collect" tasks.
% In the ASTRO model, each behavior is assigned to a separate block with its own attention distribution. Through positive behavior extraction, we can not only increase the number of sparse yet important behaviors but also improve their attention distribution, thereby enhancing model effectiveness. Performance comparisons between the Transformer and ASTRO (-ATL, -FUSION) across different datasets are shown in Table 1, which indicates that significant improvement can be achieved solely through positive behavior extraction.

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=\linewidth]{attention.pdf}
%   \caption{Attention Distribution of Different Models on Different Behaviors}
%   \Description{}
% \end{figure}

% \subsubsection{The influence of adaptive temperature on attention distribution}
% The motivation for this point is that after breaking down the block, a point can be found that the high scores of the end behavior are concentrated in the recent part in Figure \ref{fig:comp_sketch}, and the polarization phenomenon of scoring will be more severe; However, there is no obvious concentration phenomenon in the collect behavior, and the scoring is relatively more uniform. Therefore, considering the setting of adaptive temperature coefficients for different scenarios and tasks, The temperature coefficients corresponding to (fm,end), (fm,red), (daily, end), and (daily, red) are 1, 2, 3, and 4, respectively. It can be seen that in the same scenario, the end play action has a lower coefficient than the collect action, because the end play action tends to be more inclined towards the user's short-term interests, while the red heart, as a representative of the user's long-term interests, has a higher temperature coefficient. In the same task, real-time recommended scenes will also have lower temperature coefficients in different scenarios. In temperature adaptation, there are only two independent variables: scene and task. In order to have a more direct impact, the most direct influence method is selected.

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=\linewidth]{scale_v1.pdf}
  \caption{Scaling Curve. (a) Scalability: DLRM vs Transformer vs ASTRO in Industrial Dataset. (b) Model performance of scaling layer number with constant sequence length. (c) Model performance of scaling sequence length with constant layer number.}
  \Description{}
  \label{fig:scale}
\end{figure*}

% \subsection{Efficient scaling law for recommendation systems}
% \subsubsection{Exploring online guidance from scaling curves}
\subsection{Scalability (RQ2)}
% Before discussing the scaling curve, we first define FLOPs as $C \propto s * l$, where $s$ represents the length of the sequence and $l$ represents the number of layers in the model, respectively. As described in Figure \ref{fig:comp_sketch}, with the increase of FLOPs, the AUC in the test dataset follows a certain power-law improvement. As the comparison method, the Transformer model is equivalent to ASTRO model without fusion module and $T=1$ and $blockNum=1$. Under the same FLOPs, the Astro model has better improvement than the Transformer model. It is indicated that splitting blocks, adaptive temperature and fusion module can help the model achieve efficient scale up almost without increasing complexity. Besides, We further analyzed the relationship between the Astro’s performance with the sequence length and the layer number. Although we found that increasing the sequence length and layer number can improve the model performance, simply controlling one factor can't improve the performance without limitations. The next step is to discuss how to reasonably control model variables to scale up our model.

Before discussing model scalability, we formally define FLOPs:
\begin{equation}
C \propto s * l 
\end{equation}
where $s$ represents the sequence length and $l$ represents the layer number in the model. 
% Our analysis reveals that when $s$ is small, $s$ dominates the relationship between $C$ and $s$ compared to $s^2$, hence only $s$ is retained.
In large-scale scenarios, the quadratic computational complexity of attention mechanisms accounts for only a minor proportion of the model's overall FLOPs even with longer sequence\cite{casson2023transformerflops,kaplan2020scaling,hoffmann2022training}. Thus in our computational analysis, we focus solely on the linear part of sequence length $s$ within  $C \propto s * l$ relationship.
The scaling curves of DLRM, Transformer, and ASTRO are shown in Figure \ref{fig:scale}(a). Although Transformer can achieve better performance than DLRM when FLOPs exceed $10^9$, its efficiency is notably lower than DLRM between $10^7$ and $10^8$. Compared to the Transformer, ASTRO exhibits a more ideal scaling curve due to its higher starting point and larger slope. When FLOPs are below $10^{7.5}$, ASTRO's performance remains weaker than DLRM, but the crossover point shifts to the left, enabling the ASTRO model to achieve performance transformation more efficiently than Transformer.
In this experiment, the two main factors affecting FLOPs are layer number and sequence length. For the ASTRO, we illustrate the relationship between model performance and the layer number in Figure \ref{fig:scale}(b) and sequence length in Figure \ref{fig:scale}(c), respectively. When the sequence length is fixed, model performance improves in a manner similar to a power-law with the increase in the layer number; when the layer number is fixed, there is a similar improvement in model performance as the sequence length increases.
Thus, our proposed ASTRO model exhibits scaling curves in terms of FLOPs, sequence length, and layer number, and has a more efficient scaling curve compared to Transformer.

\subsection{Efficient Allocation (RQ3)}
% Results are shown in Table 3. We observe that under the same FLOPs, the combination of layer number and sequence length can lead to significant changes on offline test AUC. Firstly, due to the linear relationship between flops and the layer number and the sequence length, the product of both variables is also basically the same with the same FLOPs. When the flops are on the order of $10^9$, the model with 8 layers and a length of 400 achieves the best performance; When flops are on the order of $10^8$, the model with 4 layers and 400 lengths achieves the best performance. We can find that the expansion of a single factor may limit the development of the model, so when it comes to expanding the model, it's best to consider multiple factors simultaneously. An interesting phenomenon is that, based on the model with a length of 400 and 4 layers, doubling the number of layers yields greater results, which leads to the observation that stacking layers can achieve greater benefits before the model flops reaches a higher level. Another reason is that block design itself can support important sparse behaviors with longer lifetimes. Therefore, in our scenario, blocks can amplify the sequence benefits, and our engineering side can ensure that both the sequence length and layers can be effectively increased.

From Figure \ref{fig:scale}(b, c), it is evident that increasing the sequence length and the layer number can improve the model's AUC. However, the priority of these two parameters has not been extensively discussed. Table \ref{tab:performance_comparison} presents the model's AUC for the same FLOPs with different layer number and sequence length. It is clear that under the same FLOPs, the combination of layer number and sequence length can lead to significant changes in the offline testing AUC.
According to $C \propto s*l$, the product of layer number and sequence length remains constant for the same FLOPs. When the FLOPs is $4.11 \times 10^8$, the model achieves the best performance of 0.8301 AUC on a ($400s\times4l$) model; When the FLOPs is $1.01 \times 10^9$, the model achieves the best performance of 0.8335 AUC on a ($400s\times8l$) model.
We observe that expanding a single factor may limit the model's development. Therefore, it is best to consider both layer number and sequence length equally when scaling up the model. For example, with a ($400s\times4l$) model, if we need to increase FLOPs by 4 times, we can choose between ($1600s\times4l$), ($800s\times8l$), and ($400s\times16l$). From Table \ref{tab:performance_comparison}, it can be found that the best choice is ($800s\times8l$), which jointly expands both factors to increase the model's AUC from 0.8301 to 0.8382.
This conclusion also guides how to allocate resources online. In our practical recommendation systems, usually only one factor is chosen for each iteration. Therefore, when scaling up online, we alternate between increasing sequence length and layer number.

\begin{table}[htbp]
\centering
\caption{
% Performance Comparison of Sequence Length and Layer Number under Equivalent FLOPs
Performance Comparison under Equivalent FLOPs
}
\label{tab:performance_comparison}
\begin{tabular}{@{}lcccccc@{}}
\toprule
FLOPs & Sequence Length & Layer Number & AUC \\
\midrule
\multirow{4}{*}{$4.11 \times 10^{8}$} & 1600 & 1 & 0.8212   \\
& 800 & 2 & 0.8280   \\
& \textbf{400} & \textbf{4} & \textbf{0.8301}  \\
& 200 & 8 & 0.8297   \\
\midrule
\multirow{4}{*}{$1.01 \times 10^{9}$} & 1600 & 2 & 0.8286  \\
 & 800 & 4 & 0.8323   \\
 & \textbf{400} & \textbf{8} & \textbf{0.8335}  \\
 & 200 & 16 & 0.8321 \\
 \midrule
\multirow{3}{*}{$2.55 \times 10^{9}$} & 1600 & 4 & 0.8365  \\
 & \textbf{800} & \textbf{8} & \textbf{0.8382}   \\
 & 400 & 16 & 0.8367  \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp]
  \caption{Online Metric Improvement Compared with DLRM}
  \label{tab:online_metric}
  \begin{tabularx}{\columnwidth}{@{}ccccc@{}}
    \toprule
    Method & FLOPs & \makecell{Sequence \\ Length} & \makecell{Layer \\ Number} & \makecell{Online Metric} \\ 
    \midrule
    DLRM & $3.45\times 10^7 (6\times)$ & - & - & $+0\%$\\
    \midrule
    \multirow{8}{*}{Climber} & $5.82\times 10^6 (1\times)$ & $100$ & $1$ & $-4.95\%$\\
     & $1.84\times 10^7 (3\times)$ & $400$ & $1$ & $-1.31\%$\\
     & $3.46\times 10^7 (6\times)$ & $800$ & $1$ & $-1.22\%$\\
     & $4.11\times 10^8 (71\times)$ & $400$ & $4$ & $+3.65\%$\\
     & $1.01\times 10^9 (174\times)$ & $800$ & $4$ & $+4.29\%$\\
     & $2.79\times 10^9 (479\times)$ & $1600$ & $4$ & $+7.78\%$\\
     & $2.31\times 10^9 (397\times)$ & $800$ & $8$ & $+10.68\%$\\
     & $3.61\times 10^9 (620\times)$ & $800$ & $12$ & \textbf{$+12.19\%$}\\
  \bottomrule
  \end{tabularx}
\end{table}


% \subsection{Scaling up online recommendation models}
\subsection{Online A/B Test (RQ4)}
% As shown in Table 3, the results of deploying ASTRO model online using TURBO framework are presented. Here, Metric indicates the relative improvement of core online metirc, and Efficiency reflects the relative improvement of actual online inference efficiency. Consistent with the conclusion that sequence length and layer number are equally important, our model demonstrates online scaling curves for Metric and FLOPs by only adjusting sequence length and layer number.
% Firstly, ASTRO model with $5.82\times10^7$ FLOPs exhibits a negative metric improvement but achieves a 400\% improvement in efficiency due to TURBO framework. When the FLOPs value of the ASTRO model match that of DLRM, there is only a slight negative metric, indicating that ASTRO model's efficiency is lower with fewer FLOPs compared with DLRM, but there is still a significant improvement in inference efficiency. Furthermore, when the FLOPs value of ASTRO model is $2.79\times10^9$, the online Metric improvement reaches 7.78\%, accompanied by a +16\% increase in inference efficiency. Finally, when the FLOPs value is $3.61\times10^9$, an online metric improvement of +12.19\% is achieved. Despite a 100x increase in FLOPs compared to DLRM, the negative inference efficiency improvement is considered acceptable in practical online applications. At present, this model has been applied to serve all user traffic in our actual music recommendation system.
% It is noteworthy that there has been no increase in the CPU/GPU/Memory resources throughout this process.
% To the best of our knowledge, ASTRO is the first recommendation model to display both offline and online scaling curves while maintaining resource balance. Moreover, it achieves +12.19\% metric improvement, representing the largest improvement in the past year.

As shown in Table \ref{tab:online_metric}, the results of deploying Climber framework are presented. 
% Here, \textcolor{blue}{online metric} indicates the relative improvement of core online metirc. 
Consistent with the conclusion that sequence length and layer number are equally important, our model demonstrates online scaling curves for metric and FLOPs by only adjusting sequence length and layer number.
Firstly, ASTRO model with $5.82\times10^6$ FLOPs exhibits a negative metric improvement. When the FLOPs value of the ASTRO (6$\times$) model match that of DLRM (6$\times$), there is only a slight negative metric, indicating that ASTRO model's efficiency is lower with fewer FLOPs compared with DLRM. Furthermore, when the FLOPs value of ASTRO (479$\times$) model is $2.79\times10^9$, the online metric improvement reaches +7.78\%. Finally, when the FLOPs value of ASTRO (620$\times$) model is $3.61\times10^9$, an online metric improvement of +12.19\% is achieved. Despite a 100$\times$ increase in FLOPs compared to DLRM, the change in online inference efficiency is regarded as acceptable in practical online applications due to TURBO acceleration framework. Our TURBO framework achieves a 5.15$\times$ acceleration during training and a 2.92$\times$ acceleration in online inference with the same FLOPs. This significantly boosts the efficiency of both training and inference stage. Currently, this model has been applied to serve all user traffic in our actual music recommendation system.
It is noteworthy that there has been only a moderate increase in the CPU/GPU/Memory resources throughout this process.
To the best of our knowledge, ASTRO is the first recommendation model to display both offline and online scaling curves while maintaining resource balance. Moreover, it achieves +12.19\% metric improvement, representing the largest improvement in the past year.

\section{Conclusion}
The Climber method is designed to scale up recommendation models under resource constraints, which consists of two main components: the ASTRO model architecture and the TURBO acceleration framework. 
The ASTRO model integrates the multi-scenario and multi-behavior characteristics of recommendation systems into the Transformer architecture through multi-scale sequence partitioning, adaptive Transformer layer, and bit-wise gating fusion, while reducing the model's time complexity. 
This integration enables the model to exhibit superior scalability in offline evaluation compared to both DLRM and Transformer models.
Furthermore, we introduce the TURBO acceleration framework, which employs dynamic compression and an encoder-level KV cache. This framework allows the deployment of models that are 100$\times$ more complex without increasing prohibitive computing resources. 
The Climber method demonstrates a scaling curve online and achieves a 12.19\% improvement in online metric.
Additionally, experimental findings have elucidated the impact of sequence length and layer number on model performance under equivalent FLOPs, emphasizing the significance of both factors in scaling up model. This insight contributes to the rational allocation of resources.
Our findings demonstrate the superiority of the Climber method in efficiently implementing scaling laws while maintaining resource balance. This capability empowers recommendation systems to overcome resource constraints and explore larger models.

% We propose the ASTRO model, which integrates the unique multi-scenario and multi-behavior characteristics of recommendation systems into the Transformer architecture through multi-scale sequence partitioning, adaptive Transformer layer, and bit-wise gating fusion. This integration enables the model to exhibit superior scalability in offline settings compared to both DLRM and Transformer models.
% Furthermore, we introduce the TURBO framework, which employs dynamic compression and an encoder-level KV cache. This framework allows the deployment of models that are 100x more complex without increasing computing resources. The ASTRO model demonstrates a scaling curve online and achieves a 12.19\% improvement in online metric.
% Additionally, our experimental results elucidate the impact of sequence length and layer number on model performance under equivalent FLOPs, underscoring the significance of both factors in scaling the model. This insight facilitates the rational allocation of resources.
% Our findings demonstrate the superiority of the ASTRO model in efficiently implementing scaling laws while maintaining constant resources. This capability empowers recommendation systems to overcome resource constraints and explore larger models.








%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
% 指定参考文献样式
\bibliographystyle{plain}

% 引入.bbl文件
\input{main.bbl}

\appendix
\section{More Details about Method and Deployment}

\subsection{Rank Task Paradigm Statement}
In recommendation systems, Click-Through Rate (CTR) prediction is a key task that estimates the probability of a user clicking on a recommended item. This task is crucial for enhancing user engagement and optimizing system performance. CTR prediction can be formulated as a supervised binary classification problem. Given a user $u$ and a candidate item $c$, the goal is to predict the probability that the user will click on the item. This can be mathematically represented as:
\begin{equation}
\hat{y}_{u,c} = \sigma(f(X_u, X_c))
\end{equation}
where $y_{u,c}$ is a binary label indicating whether the user clicked on the item (1) or not (0), $\hat{y}_{u,c}$ is the prediction value of $y_{u,c}$. $X_u$ and $X_c$ are the feature vectors representing the user and item, respectively. $f(\cdot)$ is a model that estimates the probability of a click and $\sigma(\cdot)$ is the sigmoid function. The model is trained by minimizing the negative log-likelihood:
\begin{equation}
\mathcal{L} = -\frac{1}{N_s} \sum_{u,c} \left[ y_{u,c} \log(\hat{y}_{u,c}) + (1 - y_{u,c}) \log(1 - \hat{y}_{u,c}) \right]
\end{equation}
where $N_s$ is the total number of samples.

Traditional DLRM predicts users' next actions on candidate items based on their historical characteristics and elaborately engineered item features. This approach is typically represented as: 
\begin{equation}
f(X_u, X_c) = f(u_{fea},c_{fea})
\end{equation}
where $u_{fea}$ and $c_{fea}$ respectively denote the user features and item features generated through feature engineering. However, due to limitations in scalability and the complexity of feature engineering, we propose replacing handcrafted features with user behavior sequences. Specifically, we adopt a Transformer-based sequence model to scale up our recommendation system.

% With the sequence modeling, our task is to recommend the next song that the user is interested in based on the user's historical behavior sequence. It is commonly known that we utilize the user's historical $n$ songs to predict the user's $(n+1)_{th}$ song. It can be formulated as $p(a_{n+1}|(x_{1}, a_{1}), (x_{2}, a_{2}), ..., (x_{n}, a_{n}), x_{n+1})$, in which $x_{i}$  denotes a music ID from entire music set X, and $x_1$-$x_n$ denotes items on which the user has already taken action, while $x_{n+1}$ is from the result of recall and prerank, indicating the items on which the user may take action next. $a_{i}$ represents the behavior corresponding to the item $x_{i}$. But the method proposed in this paper will divide the user sequence into sub sequences with different periods or behaviors, and the specific prediction paradigm will be mentioned later.

\begin{figure*}[t]
  \centering
  % \includegraphics[width=6in]{model_v2.pdf}
  % \includegraphics[width=\linewidth]{overview_v1.pdf}
  % \includegraphics[scale=0.35]{overview_v1.pdf}
  \includegraphics[width=\linewidth]{overview_v1.pdf}
  \caption{An Overview of our efficient large recommendation model deployment}
  \Description{}
  \label{fig:deploy}
\end{figure*}

With sequence modeling, our task is to predict the user's behaviors on candidate items based on their historical behavior sequences. Specifically, we utilize the user's historical $n$ items and their corresponding behaviors to predict the potential behavior on the candidate item $c$, which can be formulated as:
\begin{equation}
f(X_u, X_c) = f(\{x_{1}, x_{2}, ..., x_{n}\}, x_{c})
\end{equation}
where $x_i$ denotes a item ID from the entire music set X. Here, $x_1$ to $x_n$ represent items on which the user has already taken action, while $x_c$ is a candidate item obtained through recall and prerank stage.

ASTRO model employs multi-scale sequence partitioning to extract multi-scale subsequences with extraction strategy from user lifecycle behaviors. By integrating Equation \ref{eqn:S} and \ref{eqn:Sk}, the task paradigm associated with this sequence structuring under the ASTRO model is characterized as follows:
\begin{equation}
f(X_u, X_c) = f(\{a_k,x^{a_k}_{1},x^{a_k}_{2},...,x^{a_k}_{n_k}\}_{k=1}^{N_b},x_{c})
\end{equation}
where $N_b$ represents the number of extraction strategy $a_k$, and $n_k$ represents the length of subsequence extracted by $a_k$.

To ensure unified training and inference stage and align training data more closely with real online requests, TURBO organizes samples according to "SUMI" style. Therefore, we predict multiple candidate items for one user simultaneously in the training stage, which can be expressed as:
\begin{equation}
f(X_u, \{X_{c,j}\}_{j=1}^{N_c}) = f(\{a_k,x^{a_k}_{1},x^{a_k}_{2},...,x^{a_k}_{n_k}\}_{k=1}^{N_b},\{x_{c,j}\}_{j=1}^{N_c})
\end{equation}
where $x_{c,j}$ represents the $j$-th candidate item, and $N_c$ represents the number of total candidate items.

% However, the method proposed in this paper divides the user sequence into subsequences based on different periods or behaviors. The specific prediction paradigm will be detailed later.


The proposed recommendation model offers several notable advantages that significantly enhance its performance and applicability:
(1) The model achieves a substantial reduction in computational complexity from $O(n^{2}d)$ to $O(n^{2}d/N_b)$, where $N_b$ denotes the number of subsequence blocks. This improvement enables a marked acceleration in both training and inference processes, even when $N_b=2$. (2) Our model overcomes temporal limitations for certain crucial behaviors. For highly sparse yet significant behaviors, we leverage users' entire lifecycle behaviors. Compared to the previous method that relies solely on data within a single time window, our method enables the incorporation of a broader range of items that more effectively represent user interests. (3) By extending important sparse behaviors to the entire lifecycle, the model gains a significant enrichment of information. Additionally, the block-wise approach alleviates the unfair allocation of attention scores between different subsequences, which will be further discussed below.
Overall, these advantages contribute to the robustness and superiority of our recommendation model, thereby reinforcing its potential as a valuable approach in recommendation systems.

\subsection{Deployment}
As depicted in Figure \ref{fig:deploy}, the implementation of the comprehensive recommendation system integrates online services, offline training, and model inference procedures. The system's performance is augmented by the TURBO acceleration framework and the ASTRO model.
At the online service stage, the system initially conducts recall and prerank operations to identify the candidate item set. Subsequently, it utilizes the ASTRO model in conjunction with the TURBO framework to process these candidates, eventually generating a prediction list.
In the offline training phase, user behavior logs are recorded and stored in a database. These logs are then dynamically compressed and transferred to an offline storage system. The log data are used to create a list of project labels, offering crucial data support for model training. During the training process, the system computes the loss and optimizes the model parameters via the backpropagation mechanism.
The actual sorting process is divided into two stages. In the first stage, the model analyzes the user behavior sequence to generate a KV-cache vector, thus swiftly capturing user characteristics. In the second stage, the model capitalizes on these extracted features to score the candidate items and generate the final recommendation list. The collaborative design of the TURBO framework and the ASTRO model boosts the efficiency of the recommendation system and guarantees the accuracy of the recommendation results, thereby furnishing users with more personalized and satisfactory recommendation services.

\begin{table}[t]
\centering
\caption{Dataset Statistics}
\label{tab:Dataset}
\begin{tabular}{@{}lcccccc@{}}
\toprule
Dataset & \#User & \#Item & \#Interaction \\
\midrule
% Industrial & 7.0M  & 6.0M  & 21.8M  \\
% Industrial & 44.9M  & 6.0M  & 930M  \\
Industrial & >40M  & >6M  & >1B  \\
Spotify & 0.16M & 3.7M & 1.2M  \\
30Music & 0.02M & 4.5M & 16M \\
Amazon-Book & 0.54M & 0.37M  & 1.09M \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{attention_v1.pdf}
  \caption{Attention Distribution of Different Models on Different Behaviors}
  \Description{}
  \label{fig:attn}
\end{figure}

\section{More Details about Experimental Setting}
\subsection{Dataset}
\label{appendix:B.1}
Table \ref{tab:Dataset} presents the number of users, items, and interactions for the four processed datasets. To protect data privacy, we have applied special processing to the Industrial dataset. As a result, the data volume shown in the table is lower than the actual amount. However, it is clear that the scale of our industrial dataset still significantly exceeds that of the other datasets. This substantial data volume provides a robust foundation for conducting scaling experiments.

\subsection{Compared Methods}
\label{appendix:B.2}
This study assesses the performance of various models, including DLRM, DIN, TWIN, Transformer, HSTU, and ASTRO variants. The subsequent section will introduce the detailed experimental setting in Industrial Dataset associated with each of these models.
\begin{itemize}
\item DLRM: The DLRM is selected as the baseline model after extensive online iterations, which has demonstrated remarkable effectiveness in both offline and online metrics. Its feature architecture incorporates statistical features and user behavior sequences. In terms of modeling methodology, we primarily employ a two-stage TWIN approach to extract user sequence representations. These representations are subsequently combined with statistical features and processed through MLP layers for feature interaction. The sequence length is set to 2000.
\item DIN: DIN is a widely-used model that captures user interests through a target attention mechanism, focusing on the interaction between user historical behaviors and the target item. In terms of the experimental setup, we do not employ statistical features and only utilize behavior sequences with 1000 length.
\item TWIN: A model that aligns the computation methods of GSU and ESU to enhance consistency and accuracy in modeling long-term user behavior. In the experimental setup, we also do not employ statistical features. The input sequence length for GSU is set to 2000, while for ESU, it is set to 1000.
\item Transformer: A foundational model leveraging the Transformer architecture for sequence modeling. We employ a one-stage Transformer encoder to directly process behavior sequences with a length of 2000. 
\item HSTU: The feature-level sequence undergoes temporal reorganization by chronological ordering of item-action pairs. This restructured sequence is then processed through the HSTU model at the structural level to predict target item-specific user action. The sequence length is set to 2000.
\item ASTRO(-ATL,-BGF): In the previous method, the sequence length of 2000 is confined to a fixed time window and user behavior is not filtered. After introducing multi-scale sequence partitioning, the time period of the behavior sequence is extended to the entire user lifecycle (> 10 years), and extraction strategies are established based on the business logic. Ultimately, this process retains a sequence length of 200. 
\item ASTRO(-BGF): Based on multi-scale sequence partitioning, all traditional Transformer layers are replaced with Adaptive Transformer Layers. The attention distribution of each block is adaptively adjusted using the recommendation scenario and extraction strategy. 
\item ASTRO: This model integrates multi-scale sequence partitioning, adaptive transformer layer, and bit-wise gating fusion. Meanwhile, the layer number in the model is set to 2, which is consistent with the layer setting in the previous method.
\item ASTRO-large: Compared to the ASTRO model, we have increased 4x sequence length and 6x layer number. As a result, the final sequence length is 800, and the layer number is 12. 
\end{itemize}

\section{Attention Distribution Analysis}
Sequence modeling in recommendation systems is essential for identifying items in a user's historical sequence that are similar to the target item, with this similarity quantified through attention scores. Before the adoption of attention mechanisms, sequence processing relied on average pooling, which assigned uniform attention scores to all items. The attention mechanism in DIN has been shown to enhance recommendation performance by assigning distinct attention scores to different items. The Transformer-based model emphasizes attention mechanisms. Thus this section explores the impact of multi-scale sequence partitioning on the attention distribution. The attention score will be introduced for two distinct behaviors: full-play and like.

% \subsection{Impact of Multi-Scale Sequence Partitioning}
Multi-scale sequence partitioning not only reduces computational complexity but also directly impacts attention distribution. Specifically, the proportion of "full-play" behaviors is higher than that of "like" behaviors. To address this imbalance, we distinguish between these two behaviors and extend the time window for "like" behaviors, thereby making the number of "like" and "full-play" behaviors approximately equal.
We conduct comparative experiments between the Transformer and ASTRO. In the Transformer experiment, sequences contain equal proportions of "like" and "full-play" behaviors for computation. In contrast, the ASTRO model separates these behaviors and trains them in a block-wise manner, with attention distributions illustrated in Figure \ref{fig:attn}.
In the Transformer structure, even with a similar number of "like" and "full-play" behaviors, the attention ratio for "full-play" behaviors reaches 71\%. This is primarily because "full-play" samples have a higher proportion and naturally occupy more gradients. Consequently, "full-play" behaviors receive higher attention scores due to the nature of the task, resulting in less attention to "like" behaviors and poorer performance on "like" tasks.
In the ASTRO model, each behavior is assigned to a separate block with its own attention distribution. Through multi-scale sequence partitioning, we can not only increase the number of sparse yet important behaviors but also improve their attention distribution, thereby enhancing model effectiveness. Performance comparisons between the Transformer and ASTRO (-ATL, -BGF) across different datasets are shown in Table \ref{tab:ASTRO}, which indicates that significant improvement can be achieved solely through multi-scale partitioning.

% Figure \ref{fig:attn} shows the attention score distribution when the red heart and completion are in the same sequence. It can be seen that the overall proportion of completion is higher than that of the red heart, because the ratio of completion to red heart in the task is approximately 71: 29, The completion task naturally occupies more gradients due to its high proportion, which leads to a tendency to have higher attention scores for the completion behavior during attention operations due to the task itself, resulting in less attention paid to the red heart behavior and poor performance in the red heart task. And when blocks are introduced, there will be their own distributions. From a specific case perspective, within a single sequence, the item corresponding to the max score of the finished broadcast is 50\%, and the item corresponding to the max score of the red heart is 50\%; When split into multiple sequences, the item corresponding to the max score of the finished broadcast is 50\%, and the item corresponding to the max score of the red heart is xx; After disassembling the sequence, more attention will be paid to the sequence corresponding to a sparser task, thus achieving better prediction results on that task
% \subsection{The influence of adaptive temperature on attention distribution}
% The motivation for this point is that after breaking down the block, a point can be found that the high scores of the end behavior are concentrated in the recent part in Figure \ref{fig:comp_sketch}, and the polarization phenomenon of scoring will be more severe; However, there is no obvious concentration phenomenon in the collect behavior, and the scoring is relatively more uniform. Therefore, considering the setting of adaptive temperature coefficients for different scenarios and tasks, The temperature coefficients corresponding to (fm,end), (fm,red), (daily, end), and (daily, red) are 1, 2, 3, and 4, respectively. It can be seen that in the same scenario, the end play action has a lower coefficient than the collect action, because the end play action tends to be more inclined towards the user's short-term interests, while the red heart, as a representative of the user's long-term interests, has a higher temperature coefficient. In the same task, real-time recommended scenes will also have lower temperature coefficients in different scenarios. In temperature adaptation, there are only two independent variables: scene and task. In order to have a more direct impact, the most direct influence method is selected.


%%
%% If your work has an appendix, this is the place to put it.

\end{document}
% \endinput
%%
%% End of file `sample-sigconf.tex'.
