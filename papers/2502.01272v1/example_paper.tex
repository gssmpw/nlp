%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\usepackage{multirow} 
\usepackage{colortbl}
% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Boosting Graph Robustness Against Backdoor Attacks: An Over-Similarity Perspective}

\makeatletter
\renewcommand{\Notice@String}{}  % 设为空字符串，隐藏版权信息
\makeatother


\begin{document}

\twocolumn[
\icmltitle{Boosting Graph Robustness Against Backdoor Attacks: An Over-Similarity Perspective}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Chang Liu}{bupt}
\icmlauthor{Hai Huang}{bupt}
\icmlauthor{Yujie Xing}{bupt}
\icmlauthor{Xingquan Zuo}{bupt}
\end{icmlauthorlist}

\icmlaffiliation{bupt}{Beijing University of Posts and Telecommunications}

\icmlcorrespondingauthor{Hai Huang}{hhuang@bupt.edu.cn}



% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{} % otherwise use the standard text.


\begin{abstract}


Graph Neural Networks (GNNs) have achieved notable success in tasks such as social and transportation networks. However, recent studies have highlighted the vulnerability of GNNs to backdoor attacks, raising significant concerns about their reliability in real-world applications. Despite initial efforts to defend against specific graph backdoor attacks, existing defense methods face two main challenges: either the inability to establish a clear distinction between triggers and clean nodes, resulting in the removal of many clean nodes, or the failure to eliminate the impact of triggers, making it challenging to restore the target nodes to their pre-attack state. Through empirical analysis of various existing graph backdoor attacks, we observe that the triggers generated by these methods exhibit over-similarity in both features and structure. Based on this observation, we propose a novel graph backdoor defense method SimGuard. We first utilizes a similarity-based metric to detect triggers and then employs contrastive learning to train a backdoor detector that generates embeddings capable of separating triggers from clean nodes, thereby improving detection efficiency. Extensive experiments conducted on real-world datasets demonstrate that our proposed method effectively defends against various graph backdoor attacks while preserving performance on clean nodes. The code will be released upon acceptance.
\end{abstract}

\section{Introduction}
\label{introduction}
Graph-structured data, which serves as a fundamental and ubiquitous representation in the real world, plays a pivotal role in modeling complex interactions among entities, such as those observed in social networks, transportation systems, and protein-protein interaction networks \cite{fan2019graph, rahmani2023graph}. Graph Neural Networks (GNNs) \cite{kipf2016semi, velickovic2017graph, hamilton2017inductive}, widely recognized as representative methodologies in graph-based machine learning, are capable of deriving high-quality representations from graph data.  However, despite the remarkable performance of GNNs across various tasks, recent studies \cite{xi2021graph, zhang2021backdoor, dai2023unnoticeable, zhang2024rethinking} have revealed that they are vulnerable to backdoor attacks. Backdoor attacks on GNNs typically involve generating and attaching backdoor triggers to a selected set of target nodes, which are subsequently assigned to a specific target class. These triggers, often represented as nodes or subgraphs, can be either predefined or dynamically created using a trigger generator. During training on a dataset contaminated with these triggers, due to the graph message-passing paradigm, the GNN model learns to associate the presence of the trigger with the specific target class. Consequently, during inference, the backdoored model misclassifies test nodes containing the trigger into the target class while maintaining high predictive accuracy for clean nodes without triggers. Backdoor attacks on graphs pose a significant challenge to the reliable deployment of GNNs in real-world applications.

The study of graph backdoor attacks and defenses has garnered increasing attention, with several foundational efforts emerging in this area \cite{zhang2021backdoor, dai2023unnoticeable}. As the pioneering work on graph backdoor attacks, SBA \cite{zhang2021backdoor} employs randomly generated graphs as triggers to launch attacks. GTA \cite{xi2021graph} first employs a backdoor trigger generator that creates more effective triggers. Building on this foundation, UGBA \cite{dai2023unnoticeable} and DPGBA \cite{zhang2024rethinking} constrain the trigger generator from the perspectives of homogeneity and feature distribution, making the generated triggers more difficult to detect. As graph backdoor attacks have advanced, efforts have also been made to develop graph backdoor defenses. Existing graph backdoor defense methods can generally be categorized into two strategies: deletion-based strategies and robust training-based strategies. Deletion-based defense methods detect and remove suspicious backdoor connections using trigger detection techniques. When accurate, they effectively eliminate trigger influence on target nodes. However, these methods often rely on specific graph properties. Prune \cite{dai2023unnoticeable} utilizes graph homogeneity, while OD \cite{zhang2024rethinking} relies on node distribution to counter backdoor threats. This reliance can lead to misclassification of clean nodes due to the use of fixed rather than adaptive thresholds. Moreover, generated triggers may resemble clean nodes, further complicating detection. Robust training-based strategies focus on accurately detecting a small subset of triggers, such as RIGBD \cite{zhang2024robustness}, which applies random edge dropping for trigger identification. The model is then fine-tuned to enhance its robustness against backdoor attacks. However, since the true class labels of target nodes remain unknown during fine-tuning, these methods cannot fully eliminate trigger influence.



% The study of graph backdoor attacks and defenses has garnered increasing attention, with several foundational efforts emerging in this area \cite{zhang2021backdoor, dai2023unnoticeable}. As the pioneering work on graph backdoor attacks, SBA \cite{zhang2021backdoor} employs randomly generated graphs as triggers to launch attacks. Building on this foundation, GTA \cite{xi2021graph} introduces a backdoor trigger generator that creates more effective, sample-specific triggers, significantly improving the attack success rate(ASR). Observing that backdoor triggers and target nodes often exhibit low feature similarity, Prune \cite{dai2023unnoticeable} was developed as an effective defense mechanism. This method selectively removes edges between nodes with dissimilar features by computing the cosine similarity between node pairs and eliminating connections below a specified threshold, thereby significantly reducing the ASR of existing backdoor attacks. To increase the unnoticeable of backdoor attacks, UGBA \cite{dai2023unnoticeable} introduces an unnoticeable attack method that increases the cosine similarity between backdoor triggers and target nodes. By incorporating a similarity constraint to ensure high feature similarity, this approach effectively undermines the efficacy of Prune in detecting graph backdoor attacks. Furthermore, recent study \cite{zhang2024rethinking} reveal that backdoor triggers often behave as outliers, making them vulnerable to graph outlier detection (OD). By removing these outlier nodes during training, OD effectively reduces the ASR of previous attack strategies. Building on this, DPGBA \cite{zhang2024rethinking} generates in-distribution triggers that evade detection by OD methods. Despite significant advancements in backdoor defense, existing trigger detection methods often rely on inherent graph properties to identify triggers, making it difficult to establish adaptive boundaries between triggers and clean nodes, which in turn leads to relatively low trigger detection accuracy. Although RIGBD \cite{zhang2024robustness} attempts to alleviate the reliance on inherent graph properties by using random edge dropping for trigger identification, it still fails to effectively eliminate the impact of backdoor trigger effects. We provide a detailed discussion of this issue in \cref{revisit:backdoor_attacks}. 

To overcome the limitations of graph backdoor defenses, we revisit existing graph backdoor attacks. Although these methods claim to generate sample-specific triggers \cite{zhang2021backdoor, dai2023unnoticeable, zhang2024rethinking}, the process of generating such triggers remains inherently complex, especially in the graph domain. This raises an important yet previously unexplored question:

\vspace{-0.05in}
\begin{center}
    \textit{Are triggers in existing graph backdoor attacks truly sample-specific, or do they exhibit inherent similarity?}
\end{center}
\vspace{-0.05in}

Upon revisiting existing graph backdoor attack methods, we discovered an important phenomenon: triggers generated by these attacks exhibit over-similarity, displaying high feature and structural similarity. This over-similarity makes them easily distinguishable from clean nodes, likely because the trigger generation process lacks sufficient constraints.

Building on this insight, we propose SimGuard, a novel graph backdoor defense framework designed to precisely identify triggers and effectively eliminate their impact. SimGuard begins by integrating overall anomaly detection with density-based clustering methods, such as DBSCAN \cite{ester1996density}, to identify potential triggers. It establishes flexible boundaries to distinguish triggers from clean nodes, thereby enhancing detection accuracy. Since trigger detection methods are not model-based, relying solely on detection techniques to identify triggers is not practical for large datasets, particularly in an inductive setting, where diverse changes in graph data during the inference phase make the process time-consuming. To address this challenge, we propose a contrastive learning-based trigger detection module that improves detection efficiency while reducing computational cost. By using a carefully designed contrastive loss function, the embeddings of triggers are effectively separated from those of clean nodes, enabling accurate identification. Notably, our method remains effective and can be reliably applied during both the training and inference stages. Our contributions are as follows:
\begin{itemize}
\item We reveal a critical phenomenon: existing graph backdoor attacks often generate triggers that exhibit over-similarity. Empirical evidence demonstrates that this issue significantly distinguishes these triggers from clean nodes.
\item We introduce SimGuard, an effective defense mechanism against graph backdoor attacks. It employs over-similarity analysis to enable efficient and accurate trigger detection. Moreover, we develop a contrastive learning-based trigger detector that seamlessly integrates into training and inference while improving computational efficiency and detection speed.

\item We propose a defense metric, Defense Recovery Rate, to provide a more comprehensive evaluation of graph backdoor defense methods. Comprehensive experimental results demonstrate the efficacy of SimGuard in mitigating backdoor attacks while preserving the accuracy on clean data.
\end{itemize}

\section{Preliminaries}

\subsection{Backdoor and Problem Definition}

We define an attributed graph as \( G = (V, E, X) \), where \( V = \{v_1, \dots, v_N\} \) represents the set of \( N \) nodes, \( E \subseteq V \times V \) denotes the set of edges, and \( X = \{x_1, \dots, x_N\} \) corresponds to the attributes associated with \( V \). In this work, we focus on the inductive setting. During training, we have access to a graph \( G_T = (V_T, E_T, X_T) \). The clean node set and the backdoored node set in \( G_T \) are denoted as \( V_C \subseteq V_T \) and \( V_B \subseteq V_T \), respectively. A clean node \( v_i \in V_C \) is labeled with its correct label \( y_i \), whereas a backdoored node \( v_j \in V_B \) is assigned a target label \( y_t \). Nodes not included in \( V_C \cup V_B \) are unlabeled. The edges linking a backdoored node \( v_i \in V_B \) to its trigger \( g_i \) form the edge set \( E_B \subseteq E_T \). During inference, an unseen graph \( G_U = (V_U, E_U, X_U) \) is presented, where \( V_U = V_U^C \cup V_U^B \). Nodes in \( V_U^C \) are clean, while nodes in \( V_U^B \) are backdoored. The training graph \( G_T \) and the unseen graph \( G_U \) are disjoint, such that \( V_U \cap V_T = \emptyset \). Similarly, the edges connecting backdoored nodes \( v_j \in V_U^B \) to their respective triggers \( g_j \) are denoted as \( E_U^B \subseteq E_U \). The neighbors of a node \( v_i \) are represented as \( \mathcal{N}(i) \).

\textbf{Threat Model.} The attacker's objective is to implant backdoor triggers, which can be nodes or subgraphs, into a subset of target nodes \( V_B \) in the training graph and assign them the target class \( y_t \). The goal is to manipulate a GNN trained on the poisoned graph to achieve two outcomes: (i) classify nodes with attached triggers as \( y_t \), and (ii) maintain normal classification behavior for clean nodes without triggers.


\textbf{Defender's Knowledge and Capability.} The defender trains a node classification model on the backdoored training graph \( G_T \) without knowing the identities of the backdoored nodes \( V_B \) or the target class \( y_t \). During inference, the defender is tasked with classifying nodes in an unseen backdoored graph \( G_U \), which contains both clean and backdoored nodes.


\textbf{Graph Backdoor Defense.} The defense problem is to train a GNN model \( f \) on the backdoored graph \( G_T \) such that it is resistant to backdoor triggers when applied to an unseen backdoored graph \( G_U \), while retaining high classification accuracy on clean data. The problem can be formulated as:

\begin{equation}
\min_f \sum_{v_i \in V_C} l(f(v_i), y_i) + \sum_{v_j \in V_B} l(f(v_j), y_j),
\end{equation}

where \( l \) denotes the classification loss. It is important to note that we adopt a more challenging defense setting, requiring target nodes to revert to their pre-attack states. In contrast, previous graph backdoor defenses primarily focus on deviating from the attack target class. Under this setting, robust training-based strategies perform well as they do not require target nodes to recover their pre-attack states. However, they fail in our setting because they do not know the true labels of target nodes, which we formally state in the following proposition.

\begin{proposition}
\label{prop:1}
For clean nodes, each node in the target set is assumed to belong to the target class with a probability of 1, and their labels are modified to the target class prior to training. To defend against graph backdoor attacks, robust training minimizes the objective \( \min_{f} L_f = \sum_{v_i \in V_D} \log f(v_i)_{y_t} + \sum_{v_j \in V_T \setminus V_D} L(f(v_j), y_j) \). Using \( g(v_i) \) to represent the predicted class of \( v_i \), fine-tuning can ensure that \( g(v_t) \neq y_t \), but it cannot guarantee that \( g(v_t) = y_o \).

\end{proposition}

where \( f \) represents the model prediction, \( L \) is the cross-entropy loss, \( y_t \) is the target class, and \( y_o \) is the classification result before the attack. \( V_D \) denotes the set of detected triggers, and \( V_T \) refers to the set of nodes used for training. For a more detailed proof, please refer to \cref{proof:pro1}.






\subsection{Contrastive Learning Preliminaries}

Contrastive learning is a self-supervised learning approach that derives representations by contrasting positive and negative pairs \cite{oord2018representation, chen2020simple, grill2020bootstrap}. Positive pairs, generated through augmentations of the same data point, encourage similar embeddings, while negative pairs, derived from different data points, promote dissimilarity. The learning objective is typically formulated using the InfoNCE loss \cite{oord2018representation}, which aims to minimize the embedding distance of positive pairs while maximizing that of negative pairs. The InfoNCE loss is defined as follows:

\begin{equation}
\mathcal{L}_{\text{contrastive}} = -\frac{1}{N} \sum_{i=1}^{N} \log \frac{e^{\text{sim}(z_i, z_j) / \tau}}{\sum_{k=1}^{N} e^{\text{sim}(z_i, z_k) / \tau}},
\end{equation}


where \( z_i \) and \( z_j \) are embeddings of a positive pair, \( z_i \) and \( z_k \) are embeddings of a negative pair, \( \text{sim}(\cdot, \cdot) \) is the similarity function (e.g., cosine similarity), and \( \tau \) is the temperature parameter.





\section{Over-Similarity Phenomenon}
\label{Over-Similarity Problem}

In this section, we examine the over-similarity of triggers generated by existing graph backdoor attacks from two perspectives: features and structures.


\subsection{Over-Similarity in Features}
\label{Over-Similarity in features}
We first examine the over-similarity phenomenon in trigger features. Specifically, we identify trigger nodes directly connected to the target, which we define as \textbf{Trigger-Bridge Nodes (TBN)}, since they propagate the influence of the trigger to the target through message passing. \textbf{All subsequent discussions on trigger detection and removal refer to TBN.} Next, we compute the pairwise cosine similarity among these nodes and average the results for statistical analysis. The resulting score, denoted as \( C_k \), is computed as follows::


\begin{equation}
C_k = \frac{1}{n-1} \sum_{\substack{j=1 \\ j \neq i}}^{n} \frac{\mathbf{v}_i \cdot \mathbf{v}_j}{\|\mathbf{v}_i\| \|\mathbf{v}_j\|}
\label{C_k}
\end{equation}


where \( \mathbf{v}_i \) and \( \mathbf{v}_j \) are the feature vectors of nodes \( i \) and \( j \), and \( n \) is the total number of such nodes.

\begin{figure*}[htb]

\begin{center}
\subfigure[GTA]{
\label{fig:CkReal}
\includegraphics[width=0.65\columnwidth]{figures/similarity_analysis/GTA/Cora_poison_data_cosine_similarity.pdf}
}
\subfigure[UGBA]{
\label{fig:attn_k_vt}
\includegraphics[width=0.65\columnwidth]{figures/similarity_analysis/UGBA/Cora_poison_data_cosine_similarity.pdf}
}
\subfigure[DPGBA]{
\label{fig:attn_k_node}
\includegraphics[width=0.65\columnwidth]{figures/similarity_analysis/DPGBA/Cora_poison_data_cosine_similarity.pdf}
}
\end{center}
\vskip -0.1in
\caption{Visualization of the similarity among trigger nodes generated by various graph backdoor attacks on the Cora dataset.}
\label{fig:attn_k}
\end{figure*}



We evaluate this metric across three mainstream graph backdoor attack methods: GTA, UGBA, and DPGBA, all of which rely on trigger generators to produce triggers. All attack configurations strictly follow the settings described in the original papers. It is worth noting that, typically, existing graph backdoor attacks establish an edge between a trigger and a target node \cite{dai2023unnoticeable, zhang2024rethinking}. In this section, we focus our analysis on this specific scenario. For a more detailed discussion of the over-similarity phenomenon in different settings, please refer to \cref{appendix_fullob}.

\begin{figure}[htb]
\begin{center}
\centerline{\includegraphics[width=0.8\columnwidth]{figures/similarity_analysis/UGBA_normal/Cora_poison_data_cosine_similarity.pdf}}
\caption{Visualization of the similarity between trigger nodes and clean nodes generated by UGBA on the Cora dataset.}
\label{fig:UGBA_among_clean}
\end{center}
\vskip -0.3in % 可根据需要调整垂直间距
\end{figure}

Under the above settings, we studied \(C_k\) on Cora. In \cref{fig:attn_k}, we present the statistics of \(C_k\), where orange represents clean nodes and blue represents triggers. The statistics are calculated using \cref{C_k}, with the horizontal axis representing \(C_k\) and the vertical axis representing density (which is equivalent to frequency). From \cref{fig:attn_k}, we observe that: (i) the \(C_k\) values between triggers are significantly higher than those between clean nodes; and (ii) in GTA and DPGBA, almost all triggers collapse into a single feature. Although the triggers generated by UGBA do not collapse into a single feature as observed in GTA and DPGBA, they still exhibit excessive similarity compared to clean nodes. This raises a further question: is UGBA closer to the true distribution of clean nodes? To investigate, we computed \(C_k\) values between triggers and clean nodes. The results in \cref{fig:UGBA_among_clean} show that while the generated triggers preserve local homophily, the trigger generator fails to capture global information. Specifically, this homophily extends beyond local neighbors to distant nodes, differing from the characteristics of clean nodes. This suggests that although triggers generated by existing trigger generators achieve a high Attack Success Rate (ASR), they fail to capture feature relationships among clean nodes, resulting in an abnormal feature distribution.



\subsection{Over-Similarity in Structure}
\label{Over-Similarity in structure}
To further understand the over-similarity phenomenon of triggers, we examine the over-similarity in trigger structures. Following the experimental setup in \cref{Over-Similarity in features}, we analyze the degree of each trigger by calculating the average and variance of their degrees to measure the structural properties of the triggers.

\begin{table}[htb]

\centering
\caption{Comparison of structural similarities in triggers generated by different graph backdoor attacks}
\label{tab:attack_comparison}
\vspace{0.2in} % 调整表注与表格之间的间距
\begin{tabular}{lcc|cc|cc}
\toprule
& \multicolumn{2}{c|}{Cora} & \multicolumn{2}{c|}{CiteSeer} & \multicolumn{2}{c}{PubMed} \\ 
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
& Mean & Var & Mean & Var & Mean & Var \\ 
\midrule
GTA   & 1.0 & 0.0  & 1.0 & 0.0  & 1.0 & 0.0 \\ 
UGBA  & 2.7 & 0.45 & 3.0 & 0.0  & 1.0 & 0.0 \\ 
DPGBA & 3.0 & 0.0  & 3.0 & 0.0  & 3.0 & 0.0 \\ 
\bottomrule
\end{tabular}
\label{struct_analysis}

\end{table}



The \cref{struct_analysis} illustrates the structural distributions of triggers connected to the target nodes generated by GTA, UGBA, and DPGBA across Cora, CiteSeer, and PubMed. From the table, several notable observations can be made: (i) The triggers generated by GTA and DPGBA exhibit extremely high structural similarity, with the degrees of all triggers displaying identical distributions within each dataset. (ii) UGBA also demonstrates completely identical trigger structural distributions in CiteSeer and PubMed, while showing slightly alleviated similarity in Cora, where the mean degree is 2.7, and the variance is 0.45, indicating relatively high structural similarity. From the above analysis, we can conclude that triggers generated by existing graph backdoor attack methods generally exhibit significant structural similarity. For the structural analysis of clean nodes, see \cref{appendix_struct_clean}.




\section{Defense Method}

\begin{figure*}[htb]
\vskip -0.05in
\begin{center}
\centerline{\includegraphics[width=\textwidth]{figures/framework.pdf}}
\caption{Framework of SimGuard. SimGuard begins by detecting triggers through a combined approach utilizing DBSCAN and global anomaly score. Subsequently, it trains a trigger detector that can be effectively deployed during both the training and inference phases.}
\label{fig:framework}
\end{center}
\vskip -0.2in % 可根据需要调整垂直间距
\end{figure*}


Unlike the image domain, where triggers easily blend with normal pixels \cite{chen2017targeted, gu2019badnets, li2022backdoor}, graph backdoor attacks mainly rely on node injection. Before message passing, triggers and clean nodes remain distinct in both feature and structural spaces. Once identified, triggers can be directly removed to restore the representations of target nodes. Based on this characteristic and the observed over-similarity phenomenon, we propose a detection framework to identify triggers and train a trigger detector for use during both training and inference. The SimGuard framework is presented in \cref{fig:framework}, and its execution procedure is detailed in \cref{appendix_algorithm}.

\subsection{Identify Triggers}

We propose a novel detection approach to identify triggers during training while minimizing clean node misclassification. It is applied prior to message passing to avoid the mixing of clean node features with trigger features. As discussed in \cref{Over-Similarity Problem}, triggers generated by GTA and DPGBA exhibit strong over-similarity, nearly causing feature collapse and creating high-density clusters in the feature space. Based on this observation, we apply a density-based detection method, such as DBSCAN, to raw node features to identify highly similar subsets in the training set that likely correspond to anomalous nodes. However, this method may misclassify normal nodes in datasets with high homogeneity, such as OGB-arxiv, where some clean nodes also exhibit high feature similarity. To enhance trigger identification, we integrate structural analysis into the DBSCAN detection process by calculating the variance of node degrees within each cluster. If the variance gets close to zero, the cluster is identified as containing triggers. The clustering process is formulated as follows:

\begin{equation}
\begin{gathered}
\mathcal{C} = \text{DBSCAN}(\mathcal{F}, \epsilon, \text{minPts}) \\
\mu(C_i) = \frac{1}{|C_i|} \sum_{u \in C_i} d(u), \\
\sigma^2(C_i) = \frac{1}{|C_i|} \sum_{u \in C_i} \left(d(u) - \mu(C_i)\right)^2, \\[0.5em]
\{C_i \mid \sigma^2(C_i) < \delta\}
\label{eq_cluster}
\end{gathered}
\end{equation}



where \( \mathcal{C} = \{C_1, C_2, \dots, C_K\} \) represents the \( K \) clusters obtained through DBSCAN, \( \sigma^2(C_i) \) and \( \mu(C_i) \) denote the variance and mean degree of nodes within cluster \( C_i \), respectively. Clusters with a variance below a predefined threshold \( \delta \), set to a very small value (e.g., 0.001), are identified as trigger clusters.



While the aforementioned methods effectively detect attack strategies exhibiting feature collapse, not all attacks display this behavior. For instance, UGBA utilizes local similarity (e.g., homogeneity) to constrain trigger generators, causing trigger features to closely resemble target nodes. Applying DBSCAN in such cases presents challenges, particularly in setting clustering thresholds appropriately. Although local homogeneity constraints can partially alleviate feature collapse, they inevitably lead to globally anomalous over-similarity, as analyzed in \cref{Over-Similarity in features}. To improve trigger detection, we propose an anomaly detection approach for global over-similarity using the Canberra distance. The Canberra distance increases when a feature exhibits global over-similarity, which typically occurs when nodes have large values across multiple dimensions. Compared to cosine similarity, Canberra distance is more effective in capturing such patterns. The global anomaly score is calculated as follows: 

\begin{equation}
 \text{G}(\mathbf{x}) = \frac{1}{m} \sum_{i=1}^m \frac{\|\mathbf{x} - \mathbf{y}_i\|_1}{\|\mathbf{x}\|_1 + \|\mathbf{y}_i\|_1}
\end{equation}

\begin{equation}
 \begin{gathered}
k = \arg\max_{j} \left(  \text{G}_{\text{sorted}}(j) -  \text{G}_{\text{sorted}}(j+1) \right), \\ 
\quad \mathcal{S} = \{ \pi(i) \mid  0 \leq i \leq k \}
\label{eq:global_score}
 \end{gathered}
\end{equation}


where \( G(\mathbf{x}) \) is the global anomaly score of node \( x \), \( y_i \) denotes the selected clean nodes, and \( G_{\text{sorted}} \) represents the values of \( G \) sorted in descending order. The index \( k \) corresponds to the maximum difference in \( G_{\text{sorted}} \), and \( \pi \) maps the sorted indices to the original indices. The set \( \mathcal{S} \) is identified as the triggers.






\subsection{Training a Trigger Detector Based on Contrastive Learning}

Although our detection method effectively identifies most triggers during the training phase, merely removing them is insufficient to defend against strong graph backdoor attacks during the inference phase \cite{zhang2024robustness}.

\begin{proposition}
If the trigger generation capability is sufficiently strong, there exists a mapping \(M\) for edge addition such that, by inserting triggers, the classification result of the target node \( u \) satisfies $f_u(G') = y_t.$
\label{prop:2}
\end{proposition}

We provide a theoretical proof for \cref{prop:2} in \cref{proof:pro2}. Furthermore, processing the entire graph structure during each inference phase incurs significant computational costs. Since triggers often exhibit excessive similarity, a straightforward solution is to train a trigger detector that can effectively identify triggers from clean nodes. During inference on an unseen graph \( G_U \), this detector can be used to detect and remove potential triggers. However, two key challenges arise: (i) how to reliably identify a subset of clean nodes with high confidence, and (ii) how to design a more advanced method to train a effective trigger detector.
  

\subsubsection{Subset of Normal Nodes}
Trigger detection and the training of trigger detectors both rely on the availability of clean nodes. While most triggers can be identified during training, ensuring that the remaining data is entirely clean remains challenging. Traditional defense methods primarily aim to detect all anomalous nodes \cite{dai2023unnoticeable, zhang2024rethinking}; in contrast, we adopt an alternative strategy that focuses on identifying a small subset of high-confidence clean nodes, as clean nodes typically constitute the majority of the training data. Specifically, inspired by a previous study \cite{zhang2024rethinking} that utilized an autoencoder for trigger detection based on feature reconstruction, we employ an autoencoder to reconstruct node features and analyze reconstruction losses to identify clean nodes \cite{ding2019deep}. However, triggers may manipulate the data distribution by repeating specific features (e.g., multiple identical triggers), forcing the model to learn a distribution that minimizes reconstruction loss for triggers. To address this issue, nodes exhibiting such trigger patterns are explicitly excluded during the selection process, ensuring a more reliable identification of clean nodes.

The reconstruction loss of a node \(v_i\) is computed as:

\begin{equation}
\begin{gathered}
L_{\text{recon}}(v_i) = \| \mathbf{X}_i - f_{\text{decoder}}(f_{\text{encoder}}(\mathbf{X}_i)) \|_1, \\[1em]
O = \{ v_i \mid L_{\text{recon}}(v_i) < \delta \}.
\end{gathered}
\end{equation}

where \( \mathbf{X}_i \) represents the feature vector of node \( v_i \), and \( f_{\text{encoder}} \) and \( f_{\text{decoder}} \) denote the encoder and decoder functions of the autoencoder, respectively. The parameter \( \delta \) corresponds to a percentile of \( L_{\text{recon}} \), and \( O \) denotes the set of selected clean nodes.


\subsubsection{Train a Trigger Detection}

After identifying a subset of clean nodes and triggers, we attempted to train a trigger detector. Directly training a simple multi-layer perceptron (MLP) often results in overfitting and fails to effectively distinguish triggers from clean nodes. To overcome this challenge, we propose a contrastive learning-based method for training the trigger detector. By constructing contrastive pairs, this method achieves the goal of bringing the embeddings of triggers closer to each other and clean nodes closer to each other, while simultaneously increasing the separation between triggers and clean nodes.

Since triggers typically constitute only a small proportion during the training phase, we employ a random sampling strategy for clean nodes to reduce computational cost and enhance the efficiency of learning contrastive features. Specifically, during each training epoch, we sample \(m\) clean nodes, where \(m\) represents the number of detected triggers. We define the positive and negative samples as follows.


\begin{equation}
\label{eq:similarities}
\left\{
\begin{aligned}
   u_i &= \frac{1}{m - 1} \sum_{j=1, j \neq i}^{m} e^{\mathbf{z}_i^+ \cdot \mathbf{z}_j^+ / \tau}, \\
   v_i &= \frac{1}{m - 1} \sum_{j=1, j \neq i}^{m} e^{\mathbf{z}_i^- \cdot \mathbf{z}_j^- / \tau}, \\
   q_i &= \frac{1}{m} \sum_{j=1}^{m} e^{\mathbf{z}_i^+ \cdot \mathbf{z}_j^- / \tau}.
\end{aligned}
\right.
\end{equation}


Where $\mathbf{z}_i^+$ and $\mathbf{z}_i^-$ represent the normalized feature vectors of clean and trigger nodes. \( u_i \) aims to bring the embeddings of clean nodes closer together, \( v_i \) is designed to bring the embeddings of trigger nodes closer together, and \( q_i \) promotes a clear separation between the representations of clean and trigger nodes in the embedding space.


The overall contrastive learning loss is formulated as:

\begin{equation}
L = - \frac{1}{m} \left( \sum_{i=1}^{m} \log \frac{u_i}{q_i} + \sum_{i=1}^{m} \log \frac{v_i}{q_i} \right),
\label{eq:cl_loss}
\end{equation}

\cref{eq:cl_loss} guides the model to learn embeddings by bringing clean nodes closer to each other, trigger nodes closer to each other, and ensuring sufficient separation between them in the latent space. After obtaining the node embeddings, an MLP is used for the final binary classification task.

\section{Experimental}
In this section, we conduct experiments to answer the following research questions: 
(Q1) How effective is SimGuard in defending against graph backdoor attacks?
(Q2) How effective is SimGuard in detecting triggers?
(Q3) How effective is SimGuard under different hyperparameter settings?

\begin{table*}[t]
\centering
\caption{Results of backdoor defense across different datasets and defense methods. ASR: Attack Success Rate (lower is better), ACC: Accuracy (higher is better), DRR: Recall (higher is better).}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|l|ccc|ccc|ccc|ccc|ccc|ccc}
\toprule
\multirow{2}{*}{Attacks} & \multirow{2}{*}{Defense} & \multicolumn{3}{c|}{Cora} & \multicolumn{3}{c|}{CiteSeer} & \multicolumn{3}{c|}{PubMed} & \multicolumn{3}{c|}{Physics} & \multicolumn{3}{c|}{Flickr} & \multicolumn{3}{c}{OGB-arxiv} \\ 
\cline{3-20} 
& & ASR(\%) $\downarrow$ & ACC(\%) $\uparrow$ & DRR(\%) $\uparrow$ & ASR(\%) $\downarrow$ & ACC(\%) $\uparrow$ & DRR(\%) $\uparrow$ & ASR(\%) $\downarrow$ & ACC(\%) $\uparrow$ & DRR(\%) $\uparrow$ & ASR(\%) $\downarrow$ & ACC(\%) $\uparrow$ & DRR(\%) $\uparrow$ & ASR(\%) $\downarrow$ & ACC(\%) $\uparrow$ & DRR(\%) $\uparrow$ & ASR(\%) $\downarrow$ & ACC(\%) $\uparrow$ & DRR(\%) $\uparrow$ \\
\midrule
\multirow{8}{*}{SBA} 
& GCN & 52.03 & 84.07 & - & 12.31 & 74.09 & - & 42.70 & 84.93 & - & 19.19 & 96.08 & - & 0.00 & 45.75 & - & 15.17 & 67.05 & - \\
& GNNGuard & 7.11 & 78.88 & 90.4 & 1.00 & 63.55 & 93.69 & 5.66 & 81.68 & 94.57 & 0.67 & 96.46 & 99.04 & 98.24 & 50.28 & 1.62 & 54.37 & 68.11 & 39.44 \\
& RobustGCN & 68.88 & 83.70 & 40.95 & 61.40 & 71.38 & 38.43 & 28.44 & 85.33 & 76.52 & 6.49 & 96.52 & 94.34 & 0.00 & 41.34 & 96.30 & 66.45 & 62.12 & 31.86 \\
& Prune & 19.11 & 82.22 & 74.53 & 0.33 & 68.07 & 80.78 & 3.79 & 85.43 & 92.74 & 0.77 & 96.08 & 91.25 & 0.00 & 42.57 & 90.96 & 0.01 & 63.75 & 83.31 \\
& OD & 55.55 & 84.07 & 53.87 & 5.03 & 73.79 & 90.69 & 29.47 & 85.28 & 76.06 & 2.20 & 96.14 & 97.15 & 0.00 & 42.77 & 95.20 & 10.72 & 66.36 & 79.66 \\
& RIGBD & 5.33 & 84.07 & 76.75 & 1.33 & 73.09 & 80.33 & 0.71 & 79.50 & 70.63 & 1.33 & 94.57 & 96.05 & 0.00 & 3.50 & 0.00 & 0.00 & 66.49 & 87.65 \\
& RIGBD-Perfect & 0.00 & 84.44 & 53.13 & 0.00 & 72.89 & 75.97 & 0.32 & 84.88 & 66.37 & 0.00 & 95.88 & 87.35 & 0.00 & 45.38 & 70.14 & 0.00 & 66.45 & 86.20 \\
& \cellcolor{gray!20}SimGuard & \cellcolor{gray!20}5.78 & \cellcolor{gray!20}84.44 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}0.00 & \cellcolor{gray!20}74.40 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}4.05 & \cellcolor{gray!20}85.54 & \cellcolor{gray!20}98.88 & \cellcolor{gray!20}0.70 & \cellcolor{gray!20}96.40 & \cellcolor{gray!20}99.73 & \cellcolor{gray!20}0.00 & \cellcolor{gray!20}45.74 & \cellcolor{gray!20}99.59 & \cellcolor{gray!20}0.03 & \cellcolor{gray!20}66.94 & \cellcolor{gray!20}97.94 \\
\midrule

\multirow{8}{*}{GTA} 
& GCN & 100.00 & 75.19 & - & 99.69 & 66.86 & - & 98.73 & 81.08 & - & 96.05 & 94.96 & - & 100.00 & 42.39 & - & 97.52 & 60.55 & - \\
& GNNGuard & 12.00 & 78.14 & 89.29 & 1.67 & 63.55 & 93.39 & 11.64 & 79.80 & 90.46 & 55.84 & 96.23 & 53.58 & 0.00 & 50.35 & 94.27 & 1.00 & 68.09 & 95.93 \\
& RobustGCN & 99.55 & 82.59 & 15.49 & 100.00 & 72.89 & 3.00 & 97.97 & 85.38 & 22.46 & 21.13 & 96.54 & 82.02 & 100.00 & 41.19 & 0.00 & 99.74 & 62.08 & 0.30 \\
& Prune & 5.33 & 82.22 & 85.60 & 3.02 & 65.36 & 77.77 & 5.59 & 85.59 & 91.17 & 0.77 & 95.96 & 98.26 & 0.00 & 42.46 & 90.64 & 0.00 & 63.87 & 86.12 \\
& OD & 100.00 & 72.22 & 16.97 & 81.87 & 70.48 & 10.81 & 97.16 & 81.32 & 22.41 & 96.52 & 95.76 & 20.49 & 0.00 & 41.01 & 90.24 & 0.35 & 44.43 & 0.39 \\
& RIGBD & 99.55 & 75.92 & 16.97 & 0.00 & 19.57 & 19.51 & 3.02 & 5.27 & 38.33 & 0.00 & 95.73 & 64.71 & 0.00 & 3.50 & 0.00 & 0.00 & 2.35 & 0.02 \\
& RIGBD-Perfect & 0.00 & 76.29 & 15.86 & 0.00 & 66.26 & 20.72 & 0.06 & 80.77 & 40.66 & 0.00 & 95.85 & 15.71 & 0.00 & 45.07 & 78.33 & 0.00 & 64.67 & 36.49 \\
& \cellcolor{gray!20}SimGuard & \cellcolor{gray!20}5.78 & \cellcolor{gray!20}84.44 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}0.00 & \cellcolor{gray!20}74.40 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}3.80 & \cellcolor{gray!20}85.13 & \cellcolor{gray!20}99.89 & \cellcolor{gray!20}0.63 & \cellcolor{gray!20}95.45 & \cellcolor{gray!20}99.50 & \cellcolor{gray!20}0.00 & \cellcolor{gray!20}45.80 & \cellcolor{gray!20}99.83 & \cellcolor{gray!20}0.02 & \cellcolor{gray!20}66.87 & \cellcolor{gray!20}97.16 \\

\bottomrule

\multirow{8}{*}{UGBA} 
& GCN & 100.00 & 82.59 & - & 100.00 & 62.95 & - & 93.47 & 82.29 & - & 100.00 & - & - & 91.75 & 44.47 & - & 97.52 & 60.55 & - \\
& GNNGuard & 100.00 & 76.29 & 15.86 & 90.93 & 59.03 & 11.11 & 4.76 & 80.66 & 87.17 & 100.00 & 47.89 & 17.25 & 94.58 & 45.49 & 2.47 & 60.18 & 65.42 & 30.81 \\
& RobustGCN & 80.88 & 83.33 & 28.78 & 100.00 & 6.62 & 3 & 97.23 & 85.43 & 22.66 & 100.00 & 14.98 & 17.54 & 95.34 & 40.95 & 3.65 & 100.00 & 0.36 & 0.05 \\
& Prune & 99.55 & 78.14 & 17.34 & 75.83 & 66.86 & 77.17 & 92.34 & 84.37 & 26.57 & 95.85 & 47.46 & 20.81 & 99.8 & 42.76 & 0.11 & 94.40 & 62.48 & 3.63 \\
& OD & 100.00 & 81.11 & 16.97 & 0.00 & 63.85 & 21.32 & 15.54 & 84.98 & 77.68 & 0.00 & 80.47 & 51.08 & 0.00 & 42.90 & 85.78 & 15.51 & 65.01 & 6.69 \\
& RIGBD & 3.11 & 78.14 & 32.1 & 0.00 & 65.06 & 19.51 & 97.03 & 76.71 & 23.07 & 100.00 & 46.18 & 17.54 & 0.00 & 44.99 & 11.55 & 0.01 & 64.60 & 0.62 \\
& RIGBD-Perfect & 0.00 & 81.48 & 15.86 & 0.00 & 62.34 & 19.51 & 1.35 & 84.62 & 79.46 & 0.00 & 46.36 & 51.02 & 0.00 & 43.97 & 77.43 & 0.00 & 64.93 & 2.22 \\
& \cellcolor{gray!20}SimGuard & \cellcolor{gray!20}5.78 & \cellcolor{gray!20}84.44 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}0.00 & \cellcolor{gray!20}74.40 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}3.80 & \cellcolor{gray!20}85.54 & \cellcolor{gray!20}98.93 & \cellcolor{gray!20}0.63 & \cellcolor{gray!20}96.26 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}0.00 & \cellcolor{gray!20}45.64 & \cellcolor{gray!20}99.84 & \cellcolor{gray!20}0.02 & \cellcolor{gray!20}66.87 & \cellcolor{gray!20}98.98 \\

\bottomrule

\multirow{8}{*}{DPGBA} 
& GCN & 96.31 & 80.00 & - & 99.69 & 67.46 & - & 94.98 & 84.07 & - & 95.47 & 94.49 & - & 87.11 & 45.61 & 92.57 & 65.09 & -\\
& GNNGuard & 5.21 & 79.25 & 89.66 & 11.02 & 63.85 & 95.79 & 32.05 & 81.43 & 83.57 & 94.04 & 96.37 & 53.00 & 71.89 & 49.72 & 83.77 & 92.57 & 65.09 & 8.29 \\
& RobustGCN & 98.22 & 82.59 & 16.97 & 100.00 & 73.19 & 21.32 & 95.68 & 85.59 & 41.37 & 96.03 & 96.63 & 52.85 & 98.14 & 41.08 & 93.92 & 87.89 & 61.51 & 11.26 \\
& Prune & 20.00 & 79.25 & 82.28 & 10.29 & 67.16 & 78.97 & 47.36 & 85.54 & 91.88 & 4.14 & 95.88 & 98.05 & 94.23 & 42.42 & 92.29 & 10.67 & 63.50 & 85.70 \\
& OD & 95.65 & 80.74 & 17.71 & 98.52 & 67.77 & 24.02 & 92.23 & 84.11 & 44.47 & 90.95 & 93.91 & 55.52 & 92.95 & 42.78 & 95.18 & 92.03 & 65.01 & 7.25 \\
& RIGBD & 0.00 & 1.85 & 1.84 & 0.00 & 6.62 & 1.20 & 0.00 & 1.85 & 1.84 & 0.81 & 94.69 & 42.07 & 3.08 & 6.90 & 18.35
& 4.02 & 64.32 & 7.25\\
& RIGBD-Perfect & 1.73 & 80.37 & 24.72 & 0.00 & 69.57 & 21.32 & 0.00 & 82.29 & 32.55 & 0.00 & 90.05 & 10.35 & 0.08 & 43.82 & 9.34
& 0.00 & 65.7 & 48.64 \\
& \cellcolor{gray!20}SimGuard & \cellcolor{gray!20}0.87 & \cellcolor{gray!20}84.44 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}9.56 & \cellcolor{gray!20}74.40 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}7.76 & \cellcolor{gray!20}85.44 & \cellcolor{gray!20}98.34 & \cellcolor{gray!20}3.79 & \cellcolor{gray!20}96.35 & \cellcolor{gray!20}99.68 & \cellcolor{gray!20}86.08 & \cellcolor{gray!20}45.93 & \cellcolor{gray!20}99.78 & \cellcolor{gray!20}0.20 & \cellcolor{gray!20}66.30 & \cellcolor{gray!20}95.11 \\
\bottomrule

\end{tabular}
}
\label{table_1}
\end{table*}


\subsection{Experimental Setup}
\label{Experimental_Setup}
\textbf{Datasets}. We use six commonly used node classification benchmark datasets to evaluate the effectiveness of SimGuard in defending against different graph backdoor attack methods. These datasets include the classic citation networks Cora, Citeseer, and Pubmed \cite{sen2008collective}, the Physics collaboration network \cite{sinha2015overview}, the Flickr social network \cite{zeng2019graphsaint}, and the OGB-arxiv citation dataset \cite{hu2020open}. Detailed descriptions of these datasets are provided in \cref{appendix_dataset}.

\textbf{Attack Methods}. To evaluate the defense capabilities of SimGuard, we conducted experiments against four graph backdoor attack methods: SBA \cite{zhang2021backdoor}, GTA \cite{xi2021graph}, UGBA \cite{dai2023unnoticeable}, and DPGBA \cite{zhang2024rethinking}. SBA, as the most classical and foundational method, represents the earliest exploration of graph backdoor attacks, while GTA, UGBA, and DPGBA are state-of-the-art methods employing adaptive trigger generation techniques. Detailed descriptions of these attack methods are provided in \cref{appendix_attack_detail}.

\textbf{Compared Methods}. To ensure a comprehensive comparison, we evaluate defense methods in three categories: purification strategies for graph backdoor attacks (e.g., Prune \cite{dai2023unnoticeable} and OD \cite{zhang2024rethinking}), robust training strategies for graph backdoor attacks (e.g., RIGBD \cite{zhang2024rethinking}), and robust GNN variants (e.g., RobustGCN \cite{zhu2019robust} and GNNGuard \cite{zhang2020gnnguard}). Additionally, we introduce RIGBD-Perfect, an enhanced variant of RIGBD that achieves perfect detection performance. Detailed implementation specifics are provided in \cref{appendix_defense_detail}.

\textbf{Evaluation Protocol}. Following existing representative graph backdoor attacks \cite{dai2023unnoticeable, zhang2024rethinking}, the graph is divided into two disjoint subgraphs, \(G_T\) and \(G_U\), in an 80:20 ratio. \(G_T\) is used to train the attacker, which selects target nodes \(V_B\) and attaches triggers, forming the backdoored graph \(G_T\). Model parameters for different attack methods follow those specified in their respective papers rather than a unified configuration, further highlighting the effectiveness of our defense. The defender trains a model on the poisoned graph \(G_T\). During evaluation, half of the nodes in \(G_U\) are randomly selected as poisoned nodes with backdoor triggers to assess the Attack Success Rate (ASR), while the remaining clean nodes measure Clean Accuracy (ACC). To comprehensively evaluate the defense, we introduce Defense Recovery Rate (DRR) to assess whether attacked nodes can revert to their pre-attack state. Additionally, we report recall and precision in trigger identification. Recall measures the proportion of correctly identified trigger nodes among actual triggers connected to target nodes, while precision measures the proportion of correctly identified trigger nodes among all detected candidates. Notably, in our setup, subgraph triggers not connected to target nodes are considered neither triggers nor clean nodes.


\subsection{Performance of the Defense}

To answer Q1, we evaluated SimGuard against baseline defenses on six datasets. Specific parameter configurations of SimGuard are provided in \cref{appendix_params}. \cref{table_1} presents ASR, ACC, and DRR for a comprehensive comparison. Key observations are as follows:

(i) SimGuard achieves the highest DRR across all datasets and attack methods, typically nearing 100\%. Although SimGuard does not always achieve the lowest ASR, this can be attributed to the inherent limitations of model classification accuracy, where a small number of nodes may naturally be classified into the target class in clean models. Prune demonstrates stronger backdoor removal capabilities compared to OD, as Prune can be applied during the inference phase, whereas OD is limited to the training phase. Moreover, out-of-distribution detectors are less stable than homogeneity-based detection methods. While RIGBD achieves remarkably low ASR values, its DRR is alarmingly low because it fails to completely remove backdoor effects, merely causing the predictions of the target nodes to deviate from the target class. These findings highlight the effectiveness of SimGuard in defending against various types of backdoor triggers and attacks.
(ii) SimGuard maintains accuracy on clean data comparable to the vanilla GCN, whereas other baselines generally experience a decline in clean accuracy. This is because they often disrupt normal node representations, for example, by removing essential connections. In contrast, SimGuard precisely detects backdoor triggers, effectively eliminating them while preserving classification accuracy on clean data.








\subsection{Ability to Detect Triggers}




To address Q2, we present the recall and precision of SimGuard in identifying triggers. Since most methods focus only on the training phase, we follow the experimental setup in \cref{Experimental_Setup} and report detection results during training. The results cover the Cora, Citeseer, and Pubmed datasets, with additional details provided in \cref{trigger_detect_full}. From \cref{figure_2}, we observe that (i) SimGuard consistently achieves high precision and recall, both exceeding 95\%, in identifying triggers. This demonstrates that our trigger detection method accurately distinguishes triggers from clean nodes while reducing misclassification of clean nodes. (ii) While some baseline methods achieve relatively high recall, their precision is significantly lower. This indicates an inability to clearly define the boundary between triggers and clean nodes, leading to a substantial number of clean nodes being misclassified as triggers. Although this misclassification may appear minor relative to the number of clean nodes, its proportion is considerably high compared to the number of triggers.



\begin{table}[htb]
\centering
\caption{Results of backdoor defense across selected datasets and methods (higher recall and F1-score indicate better performance).}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l|l|cc|cc|cc}
\toprule
\multirow{2}{*}{Attacks} & \multirow{2}{*}{Defense} & \multicolumn{2}{c|}{Cora} & \multicolumn{2}{c|}{CiteSeer} & \multicolumn{2}{c}{PubMed} \\
\cline{3-8} 
& & Recall $\uparrow$ & Precision $\uparrow$ & Recall $\uparrow$ & Precision $\uparrow$ & Recall $\uparrow$ & Precision $\uparrow$ \\
\midrule
\multirow{4}{*}{SBA} 
& Prune & 100.00 & 0.86 & 100.00 & 0.41 & 100.00 & 0.45 \\
& OD & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
& RIGBD & 50.00 & 71.43 & 20.00 & 100.00 & 45.00 & 29.03 \\
& \cellcolor{gray!20}SimGuard & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}98.77 \\
\midrule
\multirow{4}{*}{GTA} 
& Prune & 100.00 & 0.86 & 100.00 & 0.41 & 90.00 & 0.41 \\
& OD & 100.00 & 8.2 & 100.00 & 9.01 & 100.00 & 6.71 \\
& RIGBD & 0.00 & 0.00 & 100.00 & 1.48 & 100.00 & 1.00 \\
& \cellcolor{gray!20}SimGuard & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 \\
\midrule
\multirow{4}{*}{UGBA} 
& Prune & 0.00 & 0.00 & 20.00 & 0.20 & 0.00 & 0.00 \\
& OD & 100.00 & 15.15 & 100.00. & 12.35 & 100.00 & 6.71 \\
& RIGBD & 10.00 & 33.33 & 90.00 & 100.00 & 0.00 & 0.00 \\
& \cellcolor{gray!20}SimGuard & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}10.00 \\
\midrule
\multirow{4}{*}{DPGBA} 
& Prune & 90.00 & 0.07 & 100.00 & 0.419 & 0.925 & 0.004 \\
& OD & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
& RIGBD & 100.00 & 1.82 & 100.00 & 1.48 & 100.00 & 1.00 \\
& \cellcolor{gray!20}SimGuard & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}95.24 \\
\bottomrule
\end{tabular}
}

\label{figure_2}
\end{table}


\subsection{Hyperparameter Analysis}
\label{exp_Hyperparameter_Analysis}
To answer Q3, we conduct experiments to evaluate the impact of different DBSCAN parameters— the neighborhood radius in clustering, eps, and the minimum cluster size, min\_samples—on the performance of SimGuard. Specifically, we vary eps within the range \{0.01, 0.02, 0.04, 0.10, 0.20\} and min\_samples within \{2, 4, 6, 10, 15\}. The attack method used is DPGBA, and all other settings follow those described in \cref{Experimental_Setup}. The results on the Cora are presented in \cref{hyp_exp}. From the figure, we observe that as min\_samples increases, the detection accuracy improves, because a small portion of clean nodes exhibit high similarity. On the other hand, varying eps within a small range does not significantly affect the accuracy. Moreover, under different settings, the recall consistently reaches 100\%. For more detailed analysis, please refer to \cref{appendix_hyp}.



\begin{figure}[htb]

\begin{center}
\subfigure[Heatmap of Recall]{

\includegraphics[width=0.47\columnwidth]{figures/hyp_analysis/recall_heatmap.pdf}
}
\subfigure[Heatmap of Precision]{
\includegraphics[width=0.47\columnwidth]{figures/hyp_analysis/precision_heatmap.pdf}
}
\end{center}
\vskip -0.1in
\caption{Hyperparameter sensitivity analysis.}
\label{hyp_exp}
\end{figure}



\subsection{Performance with Different Trigger Settings.}
The performance and analysis of SimGuard against different numbers of triggers are provided in \cref{appendix_diff_number}, and against different numbers of edges connecting triggers to poisoned nodes in \cref{appendix_diff_edges}.



\section{Conclusion}
In this paper, we identify the over-similarity phenomenon in triggers generated by existing graph backdoor attack methods. Inspired by this observation, we propose SimGuard, a novel graph backdoor defense framework that incorporates an innovative trigger detection method and a contrastive learning-based trigger detector. SimGuard efficiently and accurately identifies triggers and eliminates their impact. Extensive experiments validate the effectiveness of SimGuard.


\section*{Impact Statement}
This paper contributes to the advancement of research on graph defenses and will promote the application of graph machine learning in real-world scenarios, such as financial networks and social network recommendation systems. Although our work may have societal implications, we do not identify any that warrant specific emphasis.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite

\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Details of Related Works}
\subsection{Graph Backdoor Attacks}
Graph backdoor attacks have increasingly garnered attention from researchers, particularly in the context of backdoor attacks targeting GNNs \cite{xi2021graph, dai2023unnoticeable, zhang2024rethinking}. Unlike traditional poisoning and evasion attacks, graph backdoor attacks typically embed malicious subgraphs as triggers into the training data. When these triggers appear in test samples, the model produces incorrect predictions. This approach involves subtle manipulation during the training phase, ensuring that the model performs as expected under normal conditions but fails when trigger-embedded inputs are encountered. Early studies in this area introduced methods such as SBA \cite{zhang2021backdoor}, which proposed a trigger injection technique based on randomly generated subgraphs. However, the attack success rate of this method was relatively low. To enhance the effectiveness of graph backdoor attacks, GTA \cite{xi2021graph} developed a trigger generator training algorithm that customizes perturbations for individual samples, significantly improving attack performance. Building on GTA, UGBA \cite{dai2023unnoticeable} introduced an optimized algorithm for selecting poisoned nodes, improving the efficiency of attack budget utilization. By incorporating cosine similarity constraints, UGBA generated triggers that better align with graph homophily, thereby achieving notable improvements in both stealth and effectiveness. DPGBA \cite{zhang2024rethinking} further highlighted the limitations of existing graph backdoor attack methods, particularly their low success rates and susceptibility to outliers. To address these issues, it proposed an adversarial learning strategy to generate in-distribution triggers and introduced a novel loss function to significantly enhance the attack success rate. This paper focuses on defending against backdoor attacks that involve attaching triggers to target nodes, rather than those that directly modify the attributes of original data. Such defense mechanisms are particularly relevant in real-world scenarios, such as social media networks \cite{fan2019graph}, where adversaries are more likely to create malicious accounts and connect them to target nodes rather than alter the attributes of existing nodes. Given the potential threats posed by such attacks, it is imperative to develop robust defense frameworks capable of effectively mitigating their impact \cite{zhang2024robustness}.
\subsection{Graph Backdoor Defense}

To alleviate the threat of backdoor attacks, researchers have proposed various defense methods. While backdoor attacks in the image domain have been extensively studied \cite{wang2019neural, kolouri2020universal, li2021anti, weber2023rab}, defenses focused on graph backdoor attacks remain relatively limited. Dai et al. \cite{dai2023unnoticeable} highlighted that in the GTA attack method, the attributes of triggers significantly differ from those of the connected poisoned nodes, violating the homophily property commonly observed in real-world graphs. To address this issue, they proposed a defense method called Prune \cite{dai2023unnoticeable}
, which removes edges connecting nodes with low cosine similarity, significantly reducing the attack success rate. Zhang et al. \cite{zhang2024rethinking} further noted that triggers in previous methods often exhibit outlier characteristics. To address this limitation, they introduced a defense strategy called OD \cite{zhang2024rethinking}, which trains a graph auto-encoder to identify and remove nodes with high reconstruction loss, effectively mitigating the impact of outlier triggers. To reduce dependency on inherent graph properties, a defense strategy called RIGBD \cite{zhang2024robustness} has been proposed, which utilizes robust training and randomized edge dropping. It enhances resistance to backdoor attacks by identifying a small number of triggers during the training phase and fine-tuning the model accordingly. However, existing defense methods for graph backdoor attacks face notable challenges: they often mistakenly remove a substantial number of clean nodes during the process or fail to entirely neutralize the effects of backdoor triggers, leaving the model unable to fully recover to its pre-attack state. These limitations highlight the urgent need for designing more robust and effective defense strategies.

\section{Time Complexity Analysis}

The time complexity of the proposed SimGuard method involves several components. In the DBSCAN \cite{ester1996density} clustering phase, pairwise distances between node features are computed, leading to a worst-case complexity of \( O(|\mathcal{V}_T|^2) \). However, when a high clustering threshold is applied, the process can be approximated on a sparse graph, reducing the complexity to \( O(k |\mathcal{V}_T|) \), where \( k \) is the average number of neighbors. The structural analysis phase, which calculates the mean and variance of node degrees for each cluster, has a complexity of \( O(|\mathcal{V}_c|) \), where \( \mathcal{V}_c \) represents the number of nodes within the clusters. Training an autoencoder for selecting clean nodes requires \( O(T d |\mathcal{V}_T|) \), where \( T \) is the number of training epochs and \( d \) is the feature dimension. The global over-similarity detection phase involves computing the Canberra distance for each node relative to \( |\mathcal{C}| \) reference nodes, where \( |\mathcal{C}| \) represents the number of clean nodes, resulting in \( O(d |\mathcal{C}| |\mathcal{V}_T|) \). Finally, the contrastive learning phase, which computes embeddings and optimizes the contrastive loss, incurs a complexity of \( O(T'|\mathcal{S}|^2 d + T''|\mathcal{S}||\mathcal{C}|) \), where \( |\mathcal{S}| \) denotes the number of triggers, and \( T' \) and \( T'' \) represent the training epochs for contrastive learning. 


\section{Detailed Proofs}

\subsection{Proof of \cref{prop:1}}
\label{proof:pro1}
To analyze the instability in classifying nodes in \( V_T \) during robust training, we examine the gradient dynamics of the objective function:

\begin{equation}
\min_{f} L_f = \sum_{v_i \in V_D} \log f(v_i)_{y_t} + \sum_{v_j \in V_D \setminus V_T} L(f(v_j), y_j),
\end{equation}

where \( f(v_i)_{y_t} = \frac{e^{z_{y_t}}}{\sum_{k} e^{z_k}} \) represents the probability of classifying \( v_i \) into the target class \( y_t \).

For target nodes \( v_i \in V_T \), we derive the gradient of \( \log f(v_i)_{y_t} \) with respect to the logits \( z_k \), where \( z_k \) represents the output logits of the model.


\begin{equation}
\frac{\partial \log f(v_i)_{y_t}}{\partial z_k} =
\begin{cases}
1 - f(v_i)_{y_t}, & \text{if } k = y_t, \\[6pt]
-f(v_i)_k, & \text{if } k \neq y_t.
\end{cases}
\end{equation}

After \( t \) training steps, the logits for each class can be expressed as:

\begin{equation}
z_k^{(t)} =
\begin{cases}
z_{y_t}^{(0)} - \eta \sum_{n=1}^t \big( 1 - f(v_i)_{y_t}^{(n)} \big), & \text{if } k = y_t, \\[6pt]
z_k^{(0)} + \eta \sum_{n=1}^t f(v_i)_k^{(n)}, & \text{if } k \neq y_t.
\end{cases}
\end{equation}

For the model to correctly classify target nodes after robust training, the following inequality must hold:

\begin{equation}
z_{y_{\text{true}}}^{(0)} + \eta \sum_{n=1}^{t} f\left( v_i \right)_{y_{\text{true}}}^{(n)} > 
z_{k}^{(0)} + \eta \sum_{n=1}^{t} f\left( v_i \right)_{k}^{(n)}
\end{equation}


This condition implies that the second highest probability class during pre-defense training should correspond to the true class of the target node. However, since the nodes are poisoned during pre-defense training with incorrect target labels, there is no mechanism to ensure or control the second highest probability class. This limitation fundamentally restricts the model's ability to recover the correct classifications during robust training.




\subsection{Proof of \cref{prop:2}}
\label{proof:pro2}
We prove \cref{prop:2} by induction on the number of linearized layers. First of all, we will show prove \cref{prop:2}
holds for 1-layer and 2-layer linearized GNN as a motivating example. The model is as $f_{\theta} = \hat{A}^2 X \Theta$
with $H = \hat{A}X\Theta \quad \text{and} \quad Z = f_{\theta}.$

Here, we define the mapping \(M\) for edge addition. For each edge perturbation pair \((u, v)\) generated by graph backdoor attacks (GBA), we can insert a new node \(w\) to connect \(u\) and \(v\). The influence of adversaries can be identified as follows.

In layer (1):

\begin{itemize}
    \item \textbf{Clean nodes:}
    \begin{equation}
    H_i = \sum_{t \in N(i) \cup \{i\}} \frac{1}{\sqrt{d_i d_t}} \, X_t 
    \tag{16}
    \end{equation}

    \item \textbf{GBA:}
    \begin{equation}
    H'_i 
    = 
    \begin{cases}
    \displaystyle
    \sum_{t \in N(i)\cup\{i\}} \frac{1}{\sqrt{\,d_t\,\bigl(d_i + 1\bigr)}} \, X_t 
     \;+\; 
     \frac{1}{\sqrt{\,\bigl(d_v \bigl)\,\bigl(d_i + 1\bigr)}} \, X_v, 
     & i \in \{u\}, 
    \\[8pt]
    \displaystyle
    \sum_{t \in N(i)\cup\{i\}} \frac{1}{\sqrt{\,d_t\,\bigl(d_i + 1\bigr)}} \, X_t 
     \;+\; 
     \frac{1}{\sqrt{\,\bigl(d_u \bigl)\,\bigl(d_i + 1\bigr)}} \, X_u, 
     & i \in \{v\}, 
    \\[8pt]
    H_i, 
     & i \notin \{u, v\}.
    \end{cases}
    \tag{17}
    \end{equation}

\end{itemize}


where \(d_i\) refers to the degree of node \(i\) with self-loops added for simplicity. Thus, in layer \((1)\), First, we assume the existence of a clean node \(i\), which is correctly classified into the target class \(t\). For a target node \(u\), we consider injecting a new node \(v\) into the graph. We then prove that there exists a feature vector for node \(v\) such that the updated representation of node \(u\) becomes identical to the representation of node \(i\)


\begin{equation}
\sum_{t \in N(i)\cup\{i\}}
\frac{1}{\sqrt{\,d_i d_t\,}} \, X_t 
\;=\; 
\sum_{t \in N(j)\cup\{j\}}
\frac{1}{\sqrt{\,d_j d_t\,}} \, X_t 
\;+\;
\frac{1}{\sqrt{(d_v + 1)(d_j + 1)}} \, X_v.
\end{equation}



\begin{equation}
\mathrm{LHS} 
= 
\sum_{t \in N(i)\cup\{i\}} \frac{1}{\sqrt{d_i d_t}}\, X_t
= 
\frac{1}{\sqrt{d_i}} \sum_{t \in N(i)\cup\{i\}} \frac{1}{\sqrt{d_t}}\, X_t
=:
\frac{1}{\sqrt{d_i}} \, A,
\end{equation}

\begin{equation}
\begin{aligned}
\mathrm{RHS} 
&= \sum_{t \in N(j)\cup\{j\}} \frac{1}{\sqrt{d_j d_t}}\, X_t
 + \frac{1}{\sqrt{(d_v + 1)(d_j + 1)}} X_v. \\
&= \frac{1}{\sqrt{d_j}} \sum_{t \in N(j)\cup\{j\}} \frac{1}{\sqrt{d_t}}\, X_t
 + \frac{1}{\sqrt{(d_v + 1)(d_j + 1)}} X_v. \\
&= \frac{1}{\sqrt{d_j}} \, B
+ \frac{1}{\sqrt{(d_v + 1)(d_j + 1)}} X_v.
\end{aligned}
\end{equation}


\begin{equation}
\begin{alignedat}{3}
    \frac{1}{\sqrt{d_i}} \, A
    &= \frac{1}{\sqrt{d_j}} \, B
    + \frac{1}{\sqrt{(d_v + 1)(d_j + 1)}} \, X_v, \\[10pt]
    \frac{1}{\sqrt{(d_v + 1)(d_j + 1)}} \, X_v
    &= \frac{1}{\sqrt{d_i}} \, A
    - \frac{1}{\sqrt{d_j}} \, B, \\[10pt]
    X_v
    &= \left(\frac{1}{\sqrt{d_i}} \, A - \frac{1}{\sqrt{d_j}} \, B\right)
    \sqrt{(d_v + 1)(d_j + 1)}.
\end{alignedat}
\end{equation}


Then, we go deeper to layer (2).

\begin{itemize}
    \item \textbf{Clean nodes:}
    \[
    Z_i 
    = 
    \sum_{t \in N(i)\cup\{i\}} 
    \frac{1}{\sqrt{\,d_i d_t}}\, H_t 
    \tag{21}
    \]
\end{itemize}

\begin{itemize}
    \item The graph representation under GBA is defined as:
    \[
    Z'_i 
    = 
    \begin{cases}
    \displaystyle
    \sum_{t \in N(i)} \frac{H_t}{\sqrt{\,d_t\,\bigl(d_i + 1\bigr)}} 
     + \frac{H'_i}{\,d_i + 1\,}
     + \frac{H'_v}{\sqrt{\,(d_v + 1)\,(d_i + 1\,)}} ,
     & i \in \{u\},
    \\[8pt]
    \displaystyle
    \sum_{t \in N(i)} \frac{H_t}{\sqrt{\,d_t\,\bigl(d_i + 1\bigr)}} 
     + \frac{H'_i}{\,d_i + 1\,}
     + \frac{H'_u}{\sqrt{\,(d_u + 1)\,(d_i + 1\,)}} ,
     & i \in \{v\},
    \\[8pt]
    \displaystyle
    \sum_{t \in N(i)} \frac{H'_t}{\sqrt{\,d_t\,\bigl(d_i + 1\bigr)}},
     & i \in N(u)\cup N(v),
    \\[8pt]
    Z_u, 
     & \text{otherwise}.
    \end{cases}
    \tag{22}
    \]
\end{itemize}




Similarly, to make \(Z'_u = Z''_u\), we have to satisfy the following constraint:

\begin{equation}
\sum_{t \in N(u)} \frac{H_t}{\sqrt{\,d_t\,\bigl(d_i + 1\bigr)}} 
     + \frac{H'_i}{\,d_i + 1\,}
     + \frac{H'_v}{\sqrt{\,(d_v + 1)\,(d_i + 1\,)}}
     =
\sum_{t \in N(j)\cup\{j\}} 
    \frac{1}{\sqrt{\,d_j d_t}}\, H_t 
\end{equation}


\begin{equation}
\begin{aligned}
&
\frac{1}{(d_u + 1)\sqrt{d_v}} \, X_v
+
\frac{1}{\sqrt{(d_v+1)(d_u+1)}}
\sum_{\tau \in N(v)\cup\{v\}}
\frac{1}{\sqrt{d_\tau (d_v+1)}} \, X_\tau
\\
&=
\sum_{t \in N(j)\cup\{j\}}
\frac{1}{\sqrt{d_j d_t}}
\sum_{\tau \in N(t)\cup\{t\}}
\frac{1}{\sqrt{d_t d_\tau}} \, X_\tau
- 
\sum_{t \in N(u)} 
\frac{1}{\sqrt{d_t (d_u + 1)}}
\sum_{\tau \in N(t)\cup\{t\}}
\frac{1}{\sqrt{d_t d_\tau}} \, X_\tau
\\
&- 
\frac{1}{d_u + 1}
\sum_{t \in N(u)\cup\{u\}}
\frac{1}{(d_u + 1)\sqrt{d_t}} \, X_t
-
\frac{1}{(d_v+1)\sqrt{(d_u+1) d_u}} \, X_u
\end{aligned}
\label{eq:proof2_pro2}
\end{equation}


For a 2-layer linear GNN, the backdoor attack can be successfully achieved by satisfying \cref{eq:proof2_pro2}. Similarly, our proof can be generalized to an \(n\)-layer GNN. Therefore, if the trigger generation capability is sufficiently strong, there exists a mapping \(M\) for edge addition that can directly influence the target classification results during the inference phase by inserting triggers. It is worth emphasizing that, as the trigger typically exists in the form of a subgraph, the solution provided in \cref{eq:proof2_pro2} is not an exact solution but rather a set of solutions.



\section{Further Observations on the Over-Similarity Phenomenon}
\label{appendix_fullob}




\begin{figure}[!htbp]
\vskip -0.15in
\begin{center}
\subfigure[]{
\includegraphics[width=0.3\textwidth]{figures/similarity_analysis/GTA/Cora_poison_data_cosine_similarity.pdf}
\label{fig:similarity_GTA_Cora}
}
\subfigure[]{
\includegraphics[width=0.3\textwidth]{figures/similarity_analysis/GTA/Citeseer_poison_data_cosine_similarity.pdf}
\label{fig:similarity_GTA_Citeseer}
}
\subfigure[]{
\includegraphics[width=0.3\textwidth]{figures/similarity_analysis/GTA/Pubmed_poison_data_cosine_similarity.pdf}
\label{fig:similarity_GTA_Pubmed}
}
\subfigure[]{
\includegraphics[width=0.3\textwidth]{figures/similarity_analysis/GTA/Physics_poison_data_cosine_similarity.pdf}
\label{fig:similarity_GTA_Physics}
}
\subfigure[]{
\includegraphics[width=0.3\textwidth]{figures/similarity_analysis/GTA/Flickr_poison_data_cosine_similarity.pdf}
\label{fig:similarity_GTA_Flickr}
}
\subfigure[]{
\includegraphics[width=0.3\textwidth]{figures/similarity_analysis/GTA/Arxiv_poison_data_cosine_similarity.pdf}
\label{fig:similarity_GTA_Arxiv}
}
\vskip -0.1in
\caption{Visualization of over-similarity among triggers generated by GTA.}
\label{fig:similarity_GTA_analysis}
\end{center}
\end{figure}


\begin{figure}[!htbp]
\begin{center}
\subfigure[]{
\label{fig:similarity_UGBA_Cora}
\includegraphics[width=0.3\textwidth]{figures/similarity_analysis/UGBA/Cora_poison_data_cosine_similarity.pdf}
}
\subfigure[]{
\label{fig:similarity_UGBA_Citeseer}
\includegraphics[width=0.3\textwidth]{figures/similarity_analysis/UGBA/Citeseer_poison_data_cosine_similarity.pdf}
}
\subfigure[]{
\label{fig:similarity_UGBA_Pubmed}
\includegraphics[width=0.3\textwidth]{figures/similarity_analysis/UGBA/Pubmed_poison_data_cosine_similarity.pdf}
}
\subfigure[]{
\label{fig:similarity_UGBA_Physics}
\includegraphics[width=0.3\textwidth]{figures/similarity_analysis/UGBA/Physics_poison_data_cosine_similarity.pdf}
}
\subfigure[]{
\label{fig:similarity_UGBA_Flickr}
\includegraphics[width=0.3\textwidth]{figures/similarity_analysis/UGBA/Flickr_poison_data_cosine_similarity.pdf}
}
\subfigure[]{
\label{fig:similarity_UGBA_Arxiv}
\includegraphics[width=0.3\textwidth]{figures/similarity_analysis/UGBA/Arxiv_poison_data_cosine_similarity.pdf}
}
\vskip -0.1in
\label{fig:similarity_UGBA_analysis}
\caption{Visualization of over-similarity among triggers generated by UGBA.}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[!htbp]
\begin{center}
\subfigure[]{
\label{fig:similarity_DPGBA_Cora}
\includegraphics[width=0.3\textwidth]{figures/similarity_analysis/DPGBA/Cora_poison_data_cosine_similarity.pdf}
}
\subfigure[]{
\label{fig:similarity_DPGBA_Citeseer}
\includegraphics[width=0.3\textwidth]{figures/similarity_analysis/DPGBA/Citeseer_poison_data_cosine_similarity.pdf}
}
\subfigure[]{
\label{fig:similarity_DPGBA_Pubmed}
\includegraphics[width=0.3\textwidth]{figures/similarity_analysis/DPGBA/Pubmed_poison_data_cosine_similarity.pdf}
}
\subfigure[]{
\label{fig:similarity_DPGBA_Physics}
\includegraphics[width=0.3\textwidth]{figures/similarity_analysis/DPGBA/Physics_poison_data_cosine_similarity.pdf}
}
\subfigure[]{
\label{fig:similarity_DPGBA_Flickr}
\includegraphics[width=0.3\textwidth]{figures/similarity_analysis/DPGBA/Flickr_poison_data_cosine_similarity.pdf}
}
\vskip -0.1in
\caption{Visualization of over-similarity among triggers generated by DPGBA.}
\label{fig:similarity_DPGBA_analysis}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[!htbp]
\begin{center}
\subfigure[]{
\label{fig:similarity_UGBA_normal_Cora}
\includegraphics[width=0.3\textwidth]{figures/similarity_analysis/UGBA_normal/Cora_poison_data_cosine_similarity.pdf}
}
\subfigure[]{
\label{fig:similarity_UGBA_normal_Citeseer}
\includegraphics[width=0.3\textwidth]{figures/similarity_analysis/UGBA_normal/Citeseer_poison_data_cosine_similarity.pdf}
}
\subfigure[]{
\label{fig:similarity_UGBA_normal_Pubmed}
\includegraphics[width=0.3\textwidth]{figures/similarity_analysis/UGBA_normal/Pubmed_poison_data_cosine_similarity.pdf}
}
\subfigure[]{
\label{fig:similarity_UGBA_normal_Physics}
\includegraphics[width=0.3\textwidth]{figures/similarity_analysis/UGBA_normal/Physics_poison_data_cosine_similarity.pdf}
}
\subfigure[]{
\label{fig:similarity_UGBA_normal_Flickr}
\includegraphics[width=0.3\textwidth]{figures/similarity_analysis/UGBA_normal/Flickr_poison_data_cosine_similarity.pdf}
}
\subfigure[]{
\label{fig:similarity_UGBA_normal_Arxiv}
\includegraphics[width=0.3\textwidth]{figures/similarity_analysis/UGBA_normal/Arxiv_poison_data_cosine_similarity.pdf}
}
\vskip -0.1in
\caption{Visualization of global anomaly scores for triggers generated by UGBA.}
\label{fig:similarity_UGBA_normal_analysis}
\end{center}
\vskip -0.2in
\end{figure}

In this section, we conduct a comprehensive analysis of the over-similarity problem exhibited by triggers generated by various graph backdoor attack methods under the \cref{Experimental_Setup} setting across different datasets. Specifically, Figures a-f correspond to subgraphs from the datasets Cora, Citeseer, PubMed, Physics, Flickr, and Arxiv, respectively. The range from \cref{fig:similarity_GTA_analysis} to \cref{fig:similarity_DPGBA_analysis} represents the similarity analysis between triggers generated by different graph backdoor attack methods, while \cref{fig:similarity_UGBA_normal_analysis} indicates the similarity analysis between triggers generated by UGBA and clean nodes. From \cref{fig:similarity_GTA_analysis} to \cref{fig:similarity_UGBA_normal_analysis}, we observe that the triggers produced by state-of-the-art graph backdoor attack methods exhibit significant over-similarity across subgraphs from all six datasets. This finding highlights that, despite their optimization objectives, these methods fail to sufficiently diversify the generated triggers, leading to high similarity.

\section{Detailed Experimental Results on Trigger Detection}
\label{trigger_detect_full}
In this section, we further demonstrate the capability of SimGuard to detect triggers across different datasets and various graph backdoor attacks. We present the recall and precision of SimGuard in identifying triggers. The experimental setup follows the description in \cref{Experimental_Setup}, and the results are provided in \cref{detect_train} and \cref{detect_infer}.. Additionally, we showcase the detection performance of corresponding methods during the inference phase. However, since most methods are simply applied during inference, we mark certain methods with a "-" to highlight this limitation, as shown in \cref{detect_train} and \cref{detect_infer}. From the tables, we observe the following: i) SimGuard achieves detection performance during the training phase with recall rates consistently above 95\%, while rarely misclassifying clean nodes. ii) Even if a small number of clean nodes are misclassified as triggers, this does not affect clean nodes further during the inference phase. This demonstrates the robustness of our method—despite minimal misclassification, the model effectively differentiates the embeddings of clean nodes and triggers.





\begin{table}[!htbp]
\centering
\caption{Trigger detection by different graph backdoor defense methods during the training phase.}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|l|cc|cc|cc|cc|cc|cc}
\toprule
\multirow{2}{*}{Attacks} & \multirow{2}{*}{Defense} & \multicolumn{2}{c|}{Cora} & \multicolumn{2}{c|}{CiteSeer} & \multicolumn{2}{c|}{PubMed} & \multicolumn{2}{c|}{Physics} & \multicolumn{2}{c|}{Flickr} & \multicolumn{2}{c}{Arxiv} \\
\cline{3-14} 
& & Recall $\uparrow$ & Precision $\uparrow$ & Recall $\uparrow$ & Precision $\uparrow$ & Recall $\uparrow$ & Precision $\uparrow$ & Recall $\uparrow$ & Precision $\uparrow$ & Recall $\uparrow$ & Precision $\uparrow$ & Recall $\uparrow$ & Precision $\uparrow$ \\
\midrule
\multirow{4}{*}{SBA} 
& Prune & 100.00 & 0.86 & 100.00 & 0.418 & 100.00 & 0.45 & 100.00 & 0.268 & 100.00 & 0.11 & 100.00 & 0.19 \\
& OD & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
& RIGBD & 50 & 71.43 & 20 & 100.00 & 45 & 29.03 & 12.5 & 10.64 & 100.00 & 0.45 & 28.21 & 97.83 \\
& \cellcolor{gray!20}SimGuard & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}98.77 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}93.02 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}99.38 \\
\midrule
\multirow{4}{*}{GTA} 
& Prune & 100.00 & 0.86 & 100.00 & 0.419 & 90 & 0.41 & 100.00 & 0.268 & 100.00 & 0.11 & 100.00 & 0.195 \\
& OD & 100.00 & 8.2 & 100.00 & 9.01 & 100.00 & 6.71 & 0.00 & 0.00 & 100.00 & 2.98 & 100.00 & 3.35 \\
& RIGBD & 0.00 & 0.00 & 100.00 & 1.48 & 100.00 & 1.0 & 82.5 & 75 & 100.00 & 0.45 & 100.00 & 0.47 \\
& \cellcolor{gray!20}SimGuard & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}95.24 & \cellcolor{gray!20}1 & \cellcolor{gray!20}99.38 \\
\midrule
\multirow{4}{*}{UGBA} 
& Prune & 0.00 & 0.00 & 20 & 0.20 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 1.87 & 0.00 \\
& OD & 100.00 & 15.15 & 100.00 & 12.35 & 100.00 & 6.71 & 0.00 & 0.00 & 100.00 & 3.71 & 100.00 & 3.35 \\
& RIGBD & 10 & 33.33 & 90 & 100.00 & 0.00 & 0.00 & 12.5 & 9.62 & 98.75 & 91.86 & 98.12 & 99.37 \\
& \cellcolor{gray!20}SimGuard & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}95.24 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}99.38 \\
\midrule
\multirow{4}{*}{DPGBA} 
& Prune & 0.9 & 0.007 & 100.00 & 0.419 & 0.925 & 0.004 & 99.37 & 1.052 & 0.062 & 0.00 & 0.00 & 0.00 \\
& OD & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
& RIGBD & 100.00 & 1.82 & 100.00 & 1.48 & 100.00 & 1.0 & 4.37 & 77.78 & 0.00 & 0.00 & 0.00 & 0.00 \\
& \cellcolor{gray!20}SimGuard & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}95.24 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}97.56 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}98.77 \\
\bottomrule
\end{tabular}
}
\label{detect_train}
\end{table}

\begin{table}[!htbp]
\centering
\caption{Trigger detection by different graph backdoor defense methods during the inference phase.}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|l|cc|cc|cc|cc|cc|cc}
\toprule
\multirow{2}{*}{Attacks} & \multirow{2}{*}{Defense} & \multicolumn{2}{c|}{Cora} & \multicolumn{2}{c|}{CiteSeer} & \multicolumn{2}{c|}{PubMed} & \multicolumn{2}{c|}{Physics} & \multicolumn{2}{c|}{Flickr} & \multicolumn{2}{c}{Arxiv} \\
\cline{3-14} 
& & Recall $\uparrow$ & Precision $\uparrow$ & Recall $\uparrow$ & Precision $\uparrow$ & Recall $\uparrow$ & Precision $\uparrow$ & Recall $\uparrow$ & Precision $\uparrow$ & Recall $\uparrow$ & Precision $\uparrow$ & Recall $\uparrow$ & Precision $\uparrow$ \\
\midrule
\multirow{4}{*}{SBA} 
& Prune & 100.00 & 13.99 & 100.00 & 9.21 & 100.00 & 26.68 & 100.00 & 13.75 & 100.00 & 9.1 & 100.00 & 12.85 \\
& OD & - & - & - & - & - & - & - & - & - & - & - & - \\
& RIGBD & - & - & - & - & - & - & - & - & - & - & - & - \\
& \cellcolor{gray!20}SimGuard & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}99.9 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}99.9 \\
\midrule
\multirow{4}{*}{GTA} 
& Prune & 98.89 & 13.65 & 100.00 & 9.21 & 92.29 & 12.31 & 100.00 & 13.8 & 100.00 & 9.1 & 100.00 & 12.67 \\
& OD & - & - & - & - & - & - & - & - & - & - & - & - \\
& RIGBD & - & - & - & - & - & - & - & - & - & - & - & - \\
& \cellcolor{gray!20}SimGuard & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}99.96 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}99.99 \\
\midrule
\multirow{4}{*}{UGBA} 
& Prune & 0.36 & 0.06 & 29.72 & 6.3 & 0.00 & 0.00 & 18.41 & 7.7 & 0.056 & 0.005 & 5.35 & 0.81 \\
& OD & - & - & - & - & - & - & - & - & - & - & - & - \\
& RIGBD & - & - & - & - & - & - & - & - & - & - & - & - \\
& \cellcolor{gray!20}SimGuard & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}99.99 \\
\midrule
\multirow{4}{*}{DPGBA} 
& Prune & 0.885 & 0.124 & 100.00 & 9.21 & 0.858 & 0.115 & 99.88 & 13.7 & 0.88 & 0.08 & 100.00 & 2.18 \\
& OD & - & - & - & - & - & - & - & - & - & - & - & - \\
& RIGBD & - & - & - & - & - & - & - & - & - & - & - & - \\
& \cellcolor{gray!20}SimGuard & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}99.60 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}99.79 & \cellcolor{gray!20}100.00 & \cellcolor{gray!20}100.00 \\
\bottomrule
\end{tabular}
}
\label{detect_infer}
\end{table}


\section{Training Alogirithm}
\label{appendix_algorithm}
The SimGuard method aims to identify triggers in a backdoored graph and train a robust trigger detector. The process begins by performing clustering on the node set \(\mathcal{V}_T\) of the backdoored graph \(\mathcal{G}_T\) using the DBSCAN algorithm with parameters \(\epsilon\) and \(\text{minPts}\) (line 1). For each cluster \(\mathcal{C}_i\), the variance of node degrees within the cluster, \(\text{Var}(\mathcal{C}_i)\), is computed (lines 2--3). Clusters with zero variance are identified as anomalous, and all nodes within such clusters are added to the anomaly set \(S_1\) (lines 4--6). Subsequently, an autoencoder is employed to reconstruct the feature matrix \(\mathbf{X}_T - S_1\), excluding nodes in \(S_1\), and to compute the reconstruction error for each node (line 8). Based on a predefined criterion, a small set of high-confidence clean nodes, denoted as \(C\), is selected (line 8). The \textbf{SimGuard} then computes global anomaly scores for the backdoored graph \(\mathcal{G}_T\) using the clean node set \(C\) during the training phase (line 9). Additional anomalous nodes are identified using a threshold, forming the anomaly set \(S_2\) (line 9). The final anomaly set \(S\) is obtained as the union of \(S_1\) and \(S_2\) (line 10). Finally, the trigger detector \(\mathcal{R}\) is trained using the clean node set \(C\) and the anomaly set \(S\) through a contrastive learning approach (line 11). The trained trigger detector is returned as the output of the \textbf{SimGuard} (line 12).
\begin{algorithm}
\caption{Algorithm for SimGuard}
\begin{algorithmic}[1]
\REQUIRE Backdoored graph $\mathcal{G}_T=(\mathcal{V}_T,\mathcal{E}_T, \mathbf{X}_T)$, clustering parameters $\epsilon$ and $\text{minPts}$
\ENSURE Trigger Detector $\mathcal{R}$

\STATE Perform clustering on the node set $\mathcal{V}_T$ of the backdoored graph $\mathcal{G}_T$ using the DBSCAN algorithm with parameters $\epsilon$ and $\text{minPts}$;
\FOR{each cluster $\mathcal{C}_i$}
    \STATE Compute the variance of node degrees within the cluster, $\text{Var}(\mathcal{C}_i)$;
    \IF{$\text{Var}(\mathcal{C}_i) = 0$}
        \STATE Label all nodes in $\mathcal{C}_i$ as anomalous and add them to the triggers set $S_1$;
    \ENDIF
\ENDFOR

\STATE Use an autoencoder to reconstruct the feature matrix $\mathbf{X}_T - S_1$ (excluding nodes in $S_1$) and calculate the reconstruction error for each node. Select a high-confidence clean node set $C$ based on the criterion defined in Eq. \eqref{eq_cluster};

\STATE Compute global anomaly scores on the backdoored graph \( \mathcal{G}_T \) using the clean node set \( C \), and identify additional triggers \( S_2 \) according to Eq. \eqref{eq:global_score};


\STATE Combine $S_1$ and $S_2$ to form the final anomaly set $S = S_1 \cup S_2$;

\STATE Train the trigger detector $\mathcal{R}$ using the clean node set $C$ and the triggers set $S$ via contrastive learning as defined in Eq. \eqref{eq:cl_loss};

\STATE Return Trigger detector $\mathcal{R}$;

\end{algorithmic}
\label{algo_trigger}
\end{algorithm}






\section{Additional Details of Experiment Settings}

In this section, we provide a detailed explanation of the experimental setup and the compared methods utilized in our study.

\subsection{Dataset Statistics}
\label{appendix_dataset}
\textbf{Cora, CiteSeer, and PubMed:} Cora, CiteSeer, and PubMed \cite{sen2008collective} have been extensively utilized in academic research. Specifically, Cora consists of 2,708 nodes and 5,429 edges, where each node represents a scientific publication characterized by a 1,433-dimensional bag-of-words feature vector, and papers are classified into seven categories. CiteSeer encompasses 3,312 nodes and 4,732 edges in the computer and information science domain, with each node similarly featuring a 1,433-dimensional bag-of-words vector. PubMed comprises 19,717 nodes and 44,338 edges from the biomedical literature domain, where nodes possess 500-dimensional feature vectors, and papers are categorized into three classes.

\textbf{Physics:} The Coauthor Physics \cite{sinha2015overview} represents a collaboration network in the physics domain. It consists of 34,493 nodes and 495,924 edges, where nodes correspond to authors and edges denote co-authorship relationships. Each node is described by an 8,415-dimensional feature vector, offering rich author-level information. This dataset is often used for studying academic collaboration patterns and community detection.

\textbf{Flickr:} Flickr \cite{zeng2019graphsaint} as a social image network, contains 89,250 nodes and 899,756 edges. Nodes represent users, and edges denote following relationships. Each node is associated with a 500-dimensional feature vector extracted from image attributes, making it particularly suitable for tasks related to social network analysis and recommendation systems.

\textbf{OGB-arxiv:} The OGB-arxiv \cref{appendix_dataset}, part of the Open Graph Benchmark (OGB), is constructed from the arXiv citation network. It comprises 169,343 nodes and 1,166,243 edges, where nodes represent papers and edges indicate citation relationships. Each node is described by a 128-dimensional feature vector derived from paper titles and abstracts, and nodes are categorized into 40 classes. A key characteristic of this dataset is its time-based train/validation/test split, which better reflects real-world scenarios. Due to its large scale and complexity, OGB-arxiv serves as a valuable benchmark for evaluating the performance of graph neural networks on large-scale graphs.

The datasets used in this study were all obtained through the PyTorch Geometric (PyG) library \cite{fey2019fast}.

\begin{table}[h]
\centering
\caption{Summary of dataset statistics.}
\vspace{5pt} % 增加表注与表格之间的垂直间距
\label{tab:dataset_statistics}
\begin{tabular}{lrrrr}
\hline
\textbf{Dataset}  & \textbf{Nodes} & \textbf{Edges} & \textbf{Features} & \textbf{Classes} \\ \hline
Cora              & 2,708          & 5,429          & 1,433             & 7                \\ 
CiteSeer          & 3,327          & 4,552          & 3,703             & 3                \\ 
PubMed            & 19,717         & 44,338         & 500               & 3                \\ 
Coauthor Physics  & 34,493         & 495,924        & 8,415             & 5                \\ 
Flickr            & 89,250         & 899,756        & 500               & 7                \\
OGB-arXiv         & 169,343        & 1,166,243      & 128               & 40               \\ \hline
\end{tabular}
\end{table}


\subsection{Attack Methods}
\label{appendix_attack_detail}
\begin{itemize}
    \item \textbf{SBA}: SBA \cite{zhang2021backdoor} represents the first study focusing on graph backdoor attacks. It employs a random graph generation method (Erdős-Rényi, ER) to construct the topology of triggers and assigns random features to the trigger nodes. However, due to the randomness inherent in the trigger generation process, this graph backdoor attack method exhibits relatively low attack success rates and poor unnoticeable.
    \item \textbf{GTA}: GTA \cite{xi2021graph} is the first approach to leverage a trigger generator for creating sample-specific, customized subgraph triggers. The optimization of the trigger generator solely aims at maximizing the backdoor attack success rate, often overlooking unnoticeable, making the generated backdoors more detectable.
    \item \textbf{UGBA}: UGBA \cite{dai2023unnoticeable} selects representative and diverse nodes as poisoned nodes to efficiently utilize the attack budget. Based on the trigger generator proposed by GTA, UGBA incorporates a homophily constraint into the loss function to ensure that the features of the generated triggers are similar to those of the target nodes, thereby improving the unnoticeable and effectiveness of the attack.
    \item \textbf{DPGBA}: DPGBA \cite{zhang2024rethinking} introduces an adversarial learning strategy to generate in-distribution triggers. A novel loss function is proposed to guide the adaptive trigger generator in producing highly efficient in-distribution triggers, significantly improving the attack success rate while maintaining unnoticeable.
\end{itemize}

\subsection{Defense Methods}
\label{appendix_defense_detail}
\textbf{Deletion-Based Defense Methods}

The deletion-based defense methods designed to counter backdoor attacks include:

\begin{itemize}
    \item \textbf{Prune:} Prune \cite{dai2023unnoticeable} identifies that the triggers generated by previous methods significantly violate the homophily property commonly observed in real-world graphs. This method removes edges connecting nodes with low similarity, thereby mitigating the impact of triggers. Prune can be applied during both the training and inference phases.
    
    \item \textbf{OD:} OD \cite{zhang2024rethinking} incorporates a graph auto-encoder to filter out nodes with high reconstruction loss, effectively removing anomalous triggers compared to clean nodes. This method is primarily applied during the training phase.
\end{itemize}

\textbf{Robust Training-Based Defense Methods}

The robust training-based defense methods designed to counter backdoor attacks include:

\begin{itemize}
    \item \textbf{RIGBD:} RIGBD \cite{zhang2024robustness} identifies a small subset of triggers during the training phase through a randomized edge-dropping strategy. It then fine-tunes the model using adversarial training to resist graph backdoor attacks. This method is mainly applied during the training phase.
\end{itemize}

\textbf{Robust GNN Methods}

Since backdoor attacks are a specific case of poisoning attacks, we also evaluate two representative robust GNN methods:

\begin{itemize}
    \item \textbf{GNNGuard:} GNNGuard \cite{zhang2020gnnguard} leverages node similarity to filter out adversarial edges, thus protecting against adversarial attacks. It employs a multi-stage defense strategy, dynamically adjusting edge weights during training to enhance resilience to structural perturbations.
    
    \item \textbf{RobustGCN:} RobustGCN \cite{zhu2019robust} improves the robustness of GCNs against adversarial attacks by modeling node representations as Gaussian distributions, which absorb adversarial changes. Additionally, it introduces a variance-based attention mechanism to assign different weights to neighboring nodes based on their variances, reducing the propagation of adversarial effects through the graph.
\end{itemize}


\subsection{Implementation Details}

We implemented our methods using PyTorch Geometric. All experiments were conducted on Linux servers equipped with 112-core Intel(R) Xeon(R) Gold 6330 CPUs @ 2.00GHz, 256 GB of memory, and running Ubuntu 20.04.6 LTS. The server was also equipped with four NVIDIA GeForce RTX 4090 GPUs, each with CUDA 12.4 installed and a total of 98.1 GB of GPU memory.

\subsection{SimGuard Parameter Settings}
\label{appendix_params}

This subsection will be released upon acceptance.
% The key parameters of SimGuard include DBSCAN clustering parameters, the clean node selection ratio, and the temperature coefficient for contrastive learning. Specifically, the DBSCAN parameters are set to \texttt{eps} = 0.01 and \texttt{min\_samples} = 10. The clean node selection ratio is configured to be 10\% of the total number of nodes, while the temperature coefficient for contrastive learning is set to 0.1.


\subsection{Reproducibility}
The code will be released upon acceptance.

\section{Detailed Analysis of Hyperparameter Settings}
\label{appendix_hyp}


This section provides a detailed analysis of SimGuard performance under different hyperparameter settings. The experimental setup follows the description in \cref{exp_Hyperparameter_Analysis}. We primarily analyze the impact of different DBSCAN parameters, with the results presented in \cref{appendix_full_hyp_exp}. Additionally, we examine the effect of varying clean node selection ratios and contrastive learning temperature settings, with the corresponding results shown in \cref{appendix_full_cl_hyp_exp}. From the table, we can observe the following: (i) SimGuard demonstrates strong robustness across different parameter combinations, particularly with varying contrastive learning temperatures and clean node ratios. This robustness may stem from the significant differences between triggers and clean nodes, enabling the model to effectively capture the distinctions between them. (ii) For different DBSCAN parameter settings, the recall consistently remains at 100\%, and even in the worst-case scenario, the accuracy exceeds 25\%.

\begin{figure}[htb]
\begin{center}
\subfigure[Heatmap of Recall]{

\includegraphics[width=0.3\columnwidth]{figures/hyp_analysis/recall_heatmap.pdf}
}
\subfigure[Heatmap of Precision]{
\includegraphics[width=0.3\columnwidth]{figures/hyp_analysis/precision_heatmap.pdf}
}
\subfigure[Heatmap of DRR]{
\includegraphics[width=0.3\columnwidth]{figures/hyp_analysis/drr_heatmap.pdf}
}
\end{center}
\vskip -0.1in
\caption{Hyperparameter sensitivity analysis.}
\label{appendix_full_hyp_exp}
\end{figure}

\begin{figure}[htb]
\begin{center}
\subfigure[Heatmap of Recall]{

\includegraphics[width=0.3\columnwidth]{figures/hyp_analysis/cl_recall_heatmap.pdf}
}
\subfigure[Heatmap of Precision]{
\includegraphics[width=0.3\columnwidth]{figures/hyp_analysis/cl_precision_heatmap.pdf}
}
\subfigure[Heatmap of DRR]{
\includegraphics[width=0.3\columnwidth]{figures/hyp_analysis/cl_drr_heatmap.pdf}
}
\end{center}
\vskip -0.1in
\caption{Hyperparameter sensitivity analysis.}
\label{appendix_full_cl_hyp_exp}
\end{figure}

\section{Different Numbers of Triggers}
\label{appendix_diff_number}
In this section, we present experiments to evaluate how a different number of triggers affects the performance of SimGuard in terms of backdoor defense and poisoned node detection. Specifically, for the Cora dataset, we set the number of triggers to {10, 20, 40, 80, 160}, while for the PubMed dataset, the number of triggers is set to {20, 80, 160, 240, 320}. The attack methods considered are DPGBA and UGBA. The remaining experimental settings follow those described in \cref{Experimental_Setup}. The results are summarized in \cref{trigger_diff_num_train} and \cref{trigger_diff_num_infer}. From the results, we observe the following: (i) Across different numbers of triggers, our method consistently achieves near-perfect detection performance, with recall and precision values approaching 100\%. (ii) Our approach also demonstrates robust defensive capabilities, maintaining an almost 100\% recovery rate for target nodes regardless of the number of triggers. These findings highlight the effectiveness of our method in accurately detecting and mitigating the influence of backdoor triggers while rarely misclassifying clean nodes as triggers.







\begin{table}[!htbp]
\centering
\caption{Results of backdoor defense across different datasets and methods with varying numbers of triggers.}
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|c|ccccc|ccccc}
\toprule
\multirow{2}{*}{Datasets} & \multirow{2}{*}{Triggers} & \multicolumn{5}{c}{UGBA} & \multicolumn{5}{c}{DPGBA} \\ 
\cmidrule(lr){3-7} \cmidrule(lr){8-12} 
& & ASR(\%) $\uparrow$ & ACC(\%) $\uparrow$ & DRR(\%) $\uparrow$ & ASR(\%) $\downarrow$ & ACC(\%) $\uparrow$ & ASR(\%) $\uparrow$ & ACC(\%) $\uparrow$ & DRR(\%) $\uparrow$ & ASR(\%) $\downarrow$ & ACC(\%) $\uparrow$ \\
\midrule
\multirow{5}{*}{Cora} 
& 10 & 100.00 & 81.11 & 100.00 & 5.78 & 84.44 & 95.21 & 80.37 & 100.00 & 0.87 & 84.44 \\
& 20 & 100.00 & 83.33 & 100.00 & 5.78 & 84.44 & 86.52 & 82.96 & 100.00 & 0.87 & 84.44 \\
& 40 & 99.31 & 81.33 & 100.00 & 5.78 & 84.44 & 92.85 & 80.61 & 100.00 & 0.87 & 84.44 \\
& 80 & 98.32 & 74.07 & 100.00 & 5.78 & 84.44 & 98.26 & 80.37 & 100.00 & 0.87 & 84.44 \\
& 160 & 99.11 & 72.22 & 100.00 & 5.78 & 84.44 & 99.56 & 77.77 & 98.15 & 0.87 & 84.81 \\
\midrule

\multirow{5}{*}{PubMed}
& 20 & 92.40 & 83.71 & 99.89 & 8.35 & 85.39 & 100.00 & 83.46 & 99.29 & 3.8 & 85.19 \\
& 80 & 92.15 & 84.27 & 99.23 & 8.35 & 85.54 & 92.85 & 80.61 & 99.79 & 3.73 & 84.93 \\
& 160 & 94.82 & 83.25 & 99.23 & 7.76 & 85.49 & 92.98 & 81.38 & 99.13 & 3.73 & 85.13 \\
& 240 & 95.74 & 83.00 & 99.08 & 8.68 & 85.19 & 94.20 & 82.09 & 99.03 & 3.73 & 85.19 \\
& 320 & 94.82 & 83.15 & 98.88 & 8.68 & 85.19 & 93.05 & 82.90 & 99.34 & 3.86 & 85.24 \\
\bottomrule
\end{tabular}%
}
\label{trigger_diff_num_train}
\end{table}

\begin{table*}[!htbp]
\centering
\caption{Results for defense and trigger detection with different numbers of triggers. For each backdoor attack method, the first two columns represent detection during the training phase, and the last two columns represent detection during the inference phase.}
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|c|cccc|cccc}
\toprule
\multirow{2}{*}{Datasets} & \multirow{2}{*}{Triggers} & \multicolumn{4}{c|}{UGBA} & \multicolumn{4}{c}{DPGBA} \\ 
\cmidrule(lr){3-6} \cmidrule(lr){7-10} 
& & Precision (\%) $\uparrow$ & Recall (\%) $\uparrow$ & Precision (\%) $\uparrow$ & Recall (\%) $\uparrow$ & Precision (\%) $\uparrow$ & Recall (\%) $\uparrow$ & Precision (\%) $\uparrow$ & Recall (\%) $\uparrow$  \\
\midrule
\multirow{5}{*}{Cora} 
& 10 & 100.00 & 100.00 & 100.00 & 100.00 & 90.91 & 100.00  & 100.00 & 99.63 \\
& 20 & 100.00 & 100.00 & 100.00 & 100.00 & 95.24 & 100.00 & 100.00  & 99.63  \\
& 40 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00  & 100.00 \\
& 80 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00  & 100.00 \\
& 160 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00  & 100.00 \\
\midrule

\multirow{5}{*}{PubMed}
& 20 & 100.00 & 100.00 & 100.00 & 100.00 & 83.33 & 100.00 & 100.00  & 99.80 \\
& 80 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00  & 100.00  \\
& 160 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00  & 100.00 \\
& 240 & 100.00 & 100.00 & 100.00 & 100.00 & 97.56 & 100.00 & 100.00  & 99.70 \\
& 320 & 100.00 & 100.00 & 100.00 & 100.00 & 99.38 & 100.00 & 100.00  & 99.90 \\
\bottomrule
\end{tabular}%
}
\label{trigger_diff_num_infer}
\end{table*}

\section{Different Numbers of Trigger Connection Edges}
\label{appendix_diff_edges}
In this section, we evaluate through experiments how the number of edges connecting triggers and target nodes affects the performance of SimGuard in terms of backdoor defense and poisoned node detection. Specifically, for the Cora and PubMed datasets, we set the number of connecting edges to {1, 2, 3}, as the size of most triggers typically ranges around 3. The attack methods used are DPGBA and UGBA. The remaining experimental settings follow those described in \cref{Experimental_Setup}. The results are summarized in \cref{trigger_edges_num_train} and \cref{trigger_edges_num_infer}. From the results, we observe the following: (i) Despite variations in the number of edges connecting the trigger and the target node, our method consistently exhibits near-perfect detection performance, with recall and precision approaching 100\%. (ii) Our method also demonstrates robust defensive capabilities, maintaining a recovery rate of nearly 100\% for target nodes, regardless of the number of edges. These findings highlight the effectiveness of our method in accurately detecting and mitigating the influence of backdoor triggers, while also indirectly suggesting that, even with an increase in the number of connecting edges, the triggers generated by existing graph backdoor attacks still exhibit significant over-similarity.

\begin{table*}[!htbp]
\centering
\caption{Results of backdoor defense across different datasets and methods with varying numbers of trigger connection edges.}
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|c|ccccc|ccccc}
\toprule
\multirow{2}{*}{Datasets} & \multirow{2}{*}{Triggers} & \multicolumn{5}{c}{UGBA} & \multicolumn{5}{c}{DPGBA} \\ 
\cmidrule(lr){3-7} \cmidrule(lr){8-12} 
& & ASR(\%) $\uparrow$ & ACC(\%) $\uparrow$ & DRR(\%) $\uparrow$ & ASR(\%) $\downarrow$ & ACC(\%) $\uparrow$ & ASR(\%) $\uparrow$ & ACC(\%) $\uparrow$ & DRR(\%) $\uparrow$ & ASR(\%) $\downarrow$ & ACC(\%) $\uparrow$ \\
\midrule
\multirow{3}{*}{Cora} 
& 1 & 100.00 & 82.22 & 100.00 & 5.78 & 84.44 & 97.82 & 79.25 & 100.00 & 0.87 & 84.44 \\
& 2 & 99.55 & 82.59 & 100.00 & 5.78 & 84.44 &  83.91 & 81.11 & 100.00 & 0.87 & 84.44 \\
& 3 & 97.33 & 74.44 & 100.00 & 5.78 & 84.44 &  86.95 & 82.59 & 100.00 & 0.87 & 84.44 \\

\midrule

\multirow{3}{*}{PubMed}
& 1 & 94.78 & 81.43 & 98.98 & 3.67 & 85.29 & 93.65 &  83.30 & 100.00 & 8.6 & 85.29 \\
& 2 & 96.71 & 83.71 & 99.29 & 3.80 & 85.19 & 91.48 &  84.42 & 98.58 & 8.01 & 85.19 \\
& 3 & 97.36 & 83.51 & 99.08 & 3.73 & 85.19 & 70.11 &  85.08 & 99.23 & 8.35 & 85.54 \\

\bottomrule
\end{tabular}%
}
\label{trigger_edges_num_train}
\end{table*}




\begin{table*}[!htbp]
\centering
\caption{Results for defense and trigger detection with different numbers of trigger connection edges. For each backdoor attack method, the first two columns show detection during the training phase, while the last two columns show detection during the inference phase.}
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|c|cccc|cccc}
\toprule
\multirow{2}{*}{Datasets} & \multirow{2}{*}{Attach\_edges} & \multicolumn{4}{c|}{UGBA} & \multicolumn{4}{c}{DPGBA} \\ 
\cmidrule(lr){3-6} \cmidrule(lr){7-10} 
& & Recall (\%) $\uparrow$ & Precision (\%) $\uparrow$ & Recall (\%) $\uparrow$ & Precision (\%) $\uparrow$ & Recall (\%) $\uparrow$ & Precision (\%) $\uparrow$ & Recall (\%) $\uparrow$ & Precision (\%) $\uparrow$ \\
\midrule
\multirow{3}{*}{Cora} 
& 1 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 \\
& 2 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 \\
& 3 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 \\
\midrule
\multirow{3}{*}{PubMed}
& 1 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 \\
& 2 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 90.91 & 99.80 \\
& 3 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 \\
\bottomrule
\end{tabular}%
}
\label{trigger_edges_num_infer}
\end{table*}


\section{Defending Against Mixed Attacks}

In this section, we examine the performance of SimGuard in defending against mixed graph backdoor attacks. To this end, we integrate two graph backdoor attack methods, DPGBA and UGBA, to generate triggers that simultaneously maintain in-distribution characteristics and local homophily. We evaluate the defense performance on three defense methods: Prune, OD, and SimGuard, with the configurations of the defense methods following those in prior work \cite{zhang2024rethinking}. 

The experimental results are shown in \cref{appendix_mid_attack_defense}--\cref{recall2_precision2_results}. From \cref{appendix_mid_attack_defense}, we observe that mixed attacks can effectively evade the defense mechanisms of OD and Prune, maintaining an attack success rate of over 90\% across four different datasets. Furthermore, on datasets such as Cora, the defense recovery rate (DRR) does not exceed 50\%. In the Flickr dataset, due to the inherently low classification accuracy of the model, the target nodes are classified into the target class even under normal conditions, resulting in a relatively higher recovery rate. In contrast, SimGuard effectively mitigates mixed attacks, achieving a DRR of nearly 99\% across all datasets. From \cref{recall1_precision1_results} and \cref{recall2_precision2_results}, it is evident that OD and Prune remain less effective than SimGuard in trigger detection. OD and Prune fail to achieve high recall and precision, whereas SimGuard consistently demonstrates superior performance, exceeding 90\% on most datasets and reaching nearly 99\% in several cases. These results indicate that our proposed method can accurately and effectively detect triggers.


\begin{table*}[!htbp]
\centering
\caption{Performance of different defense methods against mixed backdoor attacks.}
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|ccc|ccc|ccc|ccc}
\toprule
\multirow{2}{*}{Methods} & \multicolumn{3}{c|}{Cora} & \multicolumn{3}{c|}{Citeseer} & \multicolumn{3}{c|}{PubMed} & \multicolumn{3}{c}{Flickr} \\ 
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13} 
& ASR (\%) $\uparrow$ & ACC (\%) $\uparrow$ & DRR (\%) $\uparrow$ & ASR (\%) $\uparrow$ & ACC (\%) $\uparrow$ & DRR (\%) $\uparrow$ & ASR (\%) $\uparrow$ & ACC (\%) $\uparrow$ & DRR (\%) $\uparrow$ & ASR (\%) $\uparrow$ & ACC (\%) $\uparrow$ & DRR (\%) $\uparrow$ \\
\midrule
GCN & 96.31 & 80 & - & 98.79 & 69.57 & - & 93.8 & 84.7 & - & 86.34 & 45.55 & - \\
Prune & 95.65 & 80 & 17.71 & 98.63 & 67.46 & 23.81 & 92.32 & 83.86 & 44.52 & 81.81 & 45.66 & 96.08 \\
OD & 95.65 & 80.74 & 17.71 & 98.52 & 67.77 & 24.02 & 92.32 & 84.11 & 43.25 & 93.32 & 42.76 & 94.89 \\
SimGuard & 0.87 & 84.44 & 100.00 & 9.56 & 74.4 & 100.00 & 7.43 & 85.19 & 98.73 & 85.42 & 45.97 & 99.85 \\
\bottomrule
\end{tabular}%
}
\label{appendix_mid_attack_defense}
\end{table*}


\begin{table*}[!htbp]
\centering
\caption{Detection performance of defense methods against mixed backdoor attacks during the training phase.}
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|cc|cc|cc|cc}
\toprule
\multirow{2}{*}{Methods} & \multicolumn{2}{c|}{Cora} & \multicolumn{2}{c|}{Citeseer} & \multicolumn{2}{c|}{PubMed} & \multicolumn{2}{c}{Flickr} \\ 
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} 
& Recall 1 (\%) $\uparrow$ & Precision 1 (\%) $\uparrow$ & Recall 1 (\%) $\uparrow$ & Precision 1 (\%) $\uparrow$ & Recall 1 (\%) $\uparrow$ & Precision 1 (\%) $\uparrow$ & Recall 1 (\%) $\uparrow$ & Precision 1 (\%) $\uparrow$ \\
\midrule
Prune & 40.00 & 0.83 & 60.00 & 1.36 & 60.00 & 0.71 & 0.00 & 0.00 \\
OD & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
SimGuard & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 92.49 \\
\bottomrule
\end{tabular}%
}
\label{recall1_precision1_results}
\end{table*}

\begin{table*}[!htbp]
\centering
\caption{Detection performance of defense methods against mixed backdoor attacks during the inference phase.}
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|cc|cc|cc|cc}
\toprule
\multirow{2}{*}{Methods} & \multicolumn{2}{c|}{Cora} & \multicolumn{2}{c|}{Citeseer} & \multicolumn{2}{c|}{PubMed} & \multicolumn{2}{c}{Flickr} \\ 
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} 
& Recall 2 (\%) $\uparrow$ & Precision 2 (\%) $\uparrow$ & Recall 2 (\%) $\uparrow$ & Precision 2 (\%) $\uparrow$ & Recall 2 (\%) $\uparrow$ & Precision 2 (\%) $\uparrow$ & Recall 2 (\%) $\uparrow$ & Precision 2 (\%) $\uparrow$ \\
\midrule
Prune & 39.48 & 12.39 & 69.06 & 28.08 & 53.14 & 0.71 & 0.2 & 0.04 \\
OD & - & - & - & - & - & - & - & - \\
SimGuard & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 99.85 \\
\bottomrule
\end{tabular}%
}
\label{recall2_precision2_results}
\end{table*}



\section{VS MLP}

In this section, to demonstrate the differences in effectiveness between trigger detectors trained using contrastive learning and those trained with MLP, we generate triggers using the UGBA method across six different datasets. The experimental setup follows the description in \cref{Experimental_Setup}, and the results are presented in \cref{fig:vs_mlp}. From the figure, we observe that while using an MLP as a substitute for our trigger detection module maintains a relatively high trigger detection precision, its recall on the Cora and PubMed datasets is lower than that of the contrastive learning-based trigger detection module. This indicates that the generalization ability of the MLP is inferior to that of the contrastive learning-based detector. The reason is that contrastive learning, compared to a standard MLP, can better capture the underlying relationships among features.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/analyse_pdf_mlp_compare.pdf} % 图像文件名及相对路径
    \caption{Contrastive learning-based trigger detector vs. MLP-based trigger detector.} % 图注
    \label{fig:vs_mlp} % 图的标签，用于引用
\end{figure}


\section{Structural Analysis of Clean Nodes}
\label{appendix_struct_clean}

In this section, we analyze the structural distribution of clean nodes to understand the expected degree distribution of normal nodes. We use the OGB-arxiv dataset for this analysis because its high homophily makes feature aggregation more likely. Feature aggregation means that node features become very similar, which is less common in low-homophily datasets like Cora. We first use DBSCAN to cluster the clean node features, setting eps to 0.05 and min\_samples to 20. Then, we calculate the degree distribution of nodes in each cluster. The results are shown in \cref{appendix_clean_node_struct_analysis}. From the table, we observe that although some nodes in the high-homophily OGB-arxiv have very similar features, their structural distributions are quite different. In contrast, triggers generated by existing backdoor attack methods show an overly high level of similarity.


\begin{table}[htb]
\centering
\caption{Cluster structure analysis of clean nodes in OGB-Arxiv.}
\label{appendix_clean_node_struct_analysis}
\vspace{0.2in} % 调整表注与表格之间的间距
\begin{tabular}{lccc}
\toprule
Cluster\_id & Total Nodes & Mean & Variance \\ 
\midrule
0  & 68   & 2.13  & 5.55  \\ 
1  & 17061 & 12.25  & 10277.16  \\ 
2  & 23   & 14.13  & 739.59  \\ 
3  & 21   & 9.42  & 99.00  \\ 
4  & 33   & 7.60  & 124.84  \\ 
5  & 33   & 3.90  & 33.17  \\ 
6  & 19   & 3.81  & 22.34  \\ 
7  & 38   & 6.19  & 93.50  \\ 
8  & 54   & 5.55  & 54.06  \\ 
9  & 18   & 32.33  & 563.22  \\ 
10 & 34   & 3.41  & 10.88  \\ 
11 & 14   & 17.64  & 332.65  \\ 
12 & 20   & 8.00  & 183.30  \\ 
13 & 17   & 6.88  & 75.04  \\ 
\bottomrule
\end{tabular}
\end{table}



\section{Limitations and Future Work}

Our proposed framework is designed to defend against generative graph backdoor attacks \cite{zhang2021backdoor, dai2023unnoticeable, zhang2024rethinking}, which are the main focus of current research on graph backdoor attacks. It can be seamlessly applied to both the training and inference phases, effectively reducing the risks posed by existing graph backdoor attacks. It is important to note that this work does not propose improvements to existing graph backdoor trigger generators, as designing diverse generators remains a challenging task, such as diffusion models \cite{gasteiger2019diffusion, chamberlain2021grand, vignac2022digress}. This paper primarily reveals the significant similarities among mainstream graph backdoor attack methods. We hope this finding will encourage further advancements in graph backdoor attack techniques and contribute to a better understanding of graph neural network (GNN) backdoor attacks.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
