% ****** Start of file apssamp.tex ******
%
%   This file is part of the APS files in the REVTeX 4.2 distribution.
%   Version 4.2a of REVTeX, December 2014
%
%   Copyright (c) 2014 The American Physical Society.
%
%   See the REVTeX 4 README file for restrictions and more information.
%
% TeX'ing this file requires that you have AMS-LaTeX 2.0 installed
% as well as the rest of the prerequisites for REVTeX 4.2
%
% See the REVTeX 4 README file
% It also requires running BibTeX. The commands are as follows:
%
%  1)  latex apssamp.tex
%  2)  bibtex apssamp
%  3)  latex apssamp.tex
%  4)  latex apssamp.tex
%
\documentclass[%
 %preprint, 
 %linenumbers,
%superscriptaddress,
%groupedaddress,
%unsortedaddress,
%runinaddress,
%frontmatterverbose, 
%preprint,
%preprintnumbers,
%nofootinbib,
%nobibnotes,
%bibnotes,
 amsmath,amssymb,
 aps, physrev,
%pra,
%prb,
%rmp,
%prstab,
%prstper,
%floatfix,
twocolumn,superscriptaddress,showpacs,nofootinbibfloatfix,amsmath,amsfonts,amssymb,longbibliography,prl
]{revtex4-2}
\usepackage{amsmath,amssymb}
\usepackage{hyperref} 
\usepackage{braket}
\usepackage{xcolor}
\usepackage{tikz}
%\usepackage[format=plain]{caption} % Add this in your preamble
\usetikzlibrary{arrows.meta}
\usetikzlibrary{shapes, arrows, calc, positioning}
\usepackage[normalem]{ulem}
\usepackage{ulem}
\usepackage{csquotes}
\usepackage{cancel}
\usepackage{graphicx}% Include figure files
%\usepackage{subcaption}
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage{svg}

%\usepackage{hyperref}% add hypertext capabilities
%\usepackage[mathlines]{lineno}% Enable numbering of text and display math
%\linenumbers\relax % Commence numbering lines

%\usepackage[showframe,%Uncomment any one of the following lines to test 
%%scale=0.7, marginratio={1:1, 2:3}, ignoreall,% default settings
%%text={7in,10in},centering,
%%margin=1.5in,
%%total={6.5in,8.75in}, top=1.2in, left=0.9in, includefoot,
%%height=10in,a5paper,hmargin={3cm,0.8in},
%]{geometry}
\newcommand{\boxsizeA}{0.5cm}  % Adjust this value to control box size
\newcommand{\boxsizeB}{0.5cm} 
\begin{document}

%\preprint{APS}

\title{Effective Field Neural Network %\sout{Inspired by  Field Theory}
}% 


\author{Xi Liu}
\affiliation{%
 Department of Physics, Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong SAR, China
}%
 %\email{xi.liu@u.nus.edu}
\author{Yujun Zhao}%
\affiliation{%
 Department of Physics, Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong SAR, China
}%




\author{Chun Yu Wan}
 \affiliation{%
 Department of Physics, Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong SAR, China
}%
\author{Yang Zhang}
 %\affiliation{%
 %Department of Physics, Massachusetts Institute of Technology, Cambridge, Massachusetts 02139, USA
%}%
\affiliation{Department of Physics and Astronomy, University of Tennessee, Knoxville, TN 37996, USA}
\affiliation{Min H. Kao Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxville, Tennessee 37996, USA}


\author{Junwei Liu}
\email{liuj@ust.hk}
 \affiliation{%
 Department of Physics, Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong SAR, China
}%
 



\date{\today}

\begin{abstract}
In recent years, with the rapid development of machine learning, physicists have been exploring its new applications in solving or alleviating the curse of dimensionality in many-body problems. In order to accurately reflect the underlying physics of the problem, domain knowledge must be encoded into the machine learning algorithms. In this work, inspired by field theory, we propose a new set of machine learning models called effective field neural networks (EFNNs) that can automatically and efficiently capture important many-body interactions through multiple self-refining processes. Taking the classical $3$-spin infinite-range model and the quantum double exchange model as case studies, we explicitly demonstrate that EFNNs significantly outperform fully-connected deep neural networks (DNNs) and the effective model. Furthermore, with the help of convolution operations, the EFNNs learned in a small system can be seamlessly used in a larger system without additional training and the relative errors even decrease, which further demonstrates the efficacy of EFNNs in representing core physical behaviors.
\end{abstract}


\maketitle

%\tableofcontents

The collective behaviors of many interacting particles---such as spins, molecules, and atoms---give rise to some of the most intriguing phenomena in condensed matter physics. However, theoretical and computational studies of many-body problems often confront the curse of dimensionality and one must devise strategies to circumvent it. Machine learning has emerged as a powerful tool for extracting effective features from high-dimensional datasets, garnering significant attention for its ability to tackle long-standing problems in condensed matter physics. Numerous machine learning techniques have been applied to a wide range of classical and quantum many-body systems, including classifying phases of matter with supervised~\cite{carrasquilla2017machine,ch2017machine,iakovlev2018supervised} and unsupervised machine learning~\cite{wang2016discovering,van2017learning,ch2018unsupervised,rodriguez2019identifying,arnold2021interpretable}, representing many-body quantum states~\cite{carleo2017solving,nomura2017restricted,choo2018symmetries,yoshioka2019constructing,carrasquilla2019reconstructing}, accelerating Monte-Carlo simulations~\cite{liu2017self,liu2017self2,huang2017accelerated,kanwar2020equivariant}, fitting high-dimensional potential energy surfaces~\cite{behler2007generalized, rupp2012fast,bartok2013representing,behler2015constructing, marchand2020machine} and searching for spin glass ground states~\cite{fan2023searching}. 

 

Standard deep neural networks (DNNs) have been shown to be ineffective in solving many-body problems unless augmented with physical knowledge.  For example, mapping the spin configuration to energy in an $8\times 8$ classical two-dimensional Ising model requires highly complex DNN structures~\cite{mills2018deep}. Consequently, extensive research has focused on encoding physics into neural network architectures to enhance their effectiveness in physical applications, such as incorporating physical laws~\cite{wu2018physics,raissi2019physics,brunton2020machine,lu2021deepxde} and physical constraints~\cite{mattheakis2019physical, bogatskiy2020lorentz}, mimicking physical processes~\cite{yang2020physics},  accounting for symmetry~\cite{mills2019extensive} and integrating graph structure in atomic bonding~\cite{Batzner2022E3,XIE2024lm,10.1063/5.0142281sj,Zhong2023_general_xhj,GAO2021466Enhancing,10.1093/nsr/nwad128MAGUS,PhysRevB.109.144426Spin-dependent_graph}, employing renormalization group procedures~\cite{li2018neural}. These physics-encoded neural networks have found applications in computing effective Hamiltonians~\cite{Zhong2024xhj,zhang2024advancingnonadiabaticmoleculardynamics_xhj,PhysRevB.105.174422spin_ann}, property prediction~\cite{Zhong2024xhj,Zhong2023_general_xhj,PhysRevB.105.174422spin_ann,PhysRevB.108.235159charge,10.1063/5.0106617GPUMD,Xie2024LASP,Yang2024Attention,barrosoluque2024openmaterials2024omat24,10.1063/5.0142281reconstructions,PhysRevB.109.144426Spin-dependent_graph}, structure search~\cite{GAO2021466Enhancing,10.1093/nsr/nwad128MAGUS}, etc. These methods, although often outperforming  standard DNNs, tend to rely on intuitive and simplified imitations of physical principles, and still remain  elusive in revealing the underlying the many-body interactions.  



In this work, inspired by  field theory, we propose a new class of machine learning models, called effective field neural networks (EFNNs). The core motivation is to decompose many-body interactions into single quasi-particle representations governed by an emergent effective field. Different from standard DNNs, where an adjacent layer relies only on the previous layer, EFNNs incorporate the initial feature values at every layer to form the self-similarity structure with parameters trains via a recursive self-refining process, in the spirit of renormalization group theory, and hence can effectively capture the underlying physics of many-body interactions. 
Unlike approaches such as FermiNet~\cite{
PhysRevResearch.2.033429FermiNet}, which directly parameterizes high-dimensional wave-functions without explicit separation of quasi-particles and fields, our method does not presuppose a fixed ansatz for the wavefunction. Instead, the quasi-particle and effective field representations are learned through an recursive self-refining process, mirroring the renormalization group (RG) framework in field theory, while leveraging the expressive power of deep neural networks. This recursive approach progressively refines the accuracy of both the quasi-particle and effective field descriptions.
%The key modification in EFNNs compared to conventional DNNs lies in its recursive computational structure. 


\begin{figure}[tbp]
    \centering\includegraphics[width=0.9\linewidth]{fig_all/fig1.pdf}
    % First figure (Top figure) with scaling
       %\makebox[\textwidth][l]{\includegraphics[width=1\linewidth]{fig1/hadamard_fig1.pdf}
     %}
    \caption{Energy evaluation of a 2D Ising model reformulated as a neural network. (a) Calculation of the effective field by summing the interacting spins. Each interacting spin is multiplied by its corresponding effective field to obtain independent spin values, and the total energy is determined by summing up these independent spins. (b) Neural network representation of the energy evaluation process. $\odot$ represents the element-wise multiplication. }
    \label{fig:figure1}
\end{figure}

We first illustrate the EFNN architecture by reformulating the classical 2D Ising model within the EFNN framework. The Hamiltonian of a classical 2D Ising model is defined as $H(S)=-J \sum_{\langle i j\rangle} s_{i} s_{j}$, where $\langle i j\rangle$ denotes a summation over pairs of nearest-neighbor sites, $s_{i}=\pm 1$ represents the spin at site $i$, $J$ is the interaction strength between two spins, and $S$ is the collection of all spins. First, we define the effective field on spin $s_{i}$ as $\phi_{i}(S)$, which is equal to the sum of the nearest neighbor spins of $s_{i}$ multiplied by $-\frac{J}{2}$. Then, we define the independent spin (quasi-particle) as $s_{i}\phi_{i}(S)$, hence the total energy reads $H(S)=\sum_{i}s_{i}\phi_{i}(S)$. As illustrated schematically in Fig.~\ref{fig:figure1}(a), the interacting spins are first mapped to an effective field, then the effective field combining with the interacting spins is mapped to independent spins. Finally, a summation is performed over the independent spins, and the total energy is obtained. We reformulate the computational procedure within a DNN-like structure, depicted in Fig.~\ref{fig:figure1}(b). Here, $S_{0}=S$ is the input layer, representing the interacting spins. The effective field layer $F_{1}$ and the quasi-particle layer $S_{1}$ constitute a single field-particle (FP) layer. The evaluation of $S_{1}$ relies on both the effective layer $F_{1}$ and the interacting spin layer $S_{0}$, therefore a connection is required from $S_{0}$ to $S_{1}$, as well as from $F_{1}$ to $S_{1}$, distinguishing this architecture from a standard DNN. Finally, a summation is performed over $S_{1}$ to obtain the energy $E$.
\begin{figure}[hbtp!]
    \centering\includegraphics[width=0.9\linewidth]{fig_all/fig2.pdf}
     %\makebox[\textwidth][l]{\includegraphics[width=1\linewidth,trim=7 7 7 7,  % Adjust these values as needed
    %clip]{V20Figs/fig_efnn_vs_dnn/hadamard_spin3.pdf}
    %}
    \caption{Performance of EFNNs on a classical 3-spin infinite range model. (a) Architecture of the EFNN. (b) Performance of EFNNs and DNNs on the test set. (c) Performance of EFNNs as the neuron number increases.}
    \label{fig_spin3}
\end{figure}


%In the classical 2D Ising model with simple nearest-neighbor interactions, 
In the classical 2D Ising model, both the effective field and quasi-particles can be exactly calculated, which, however, becomes infeasible for more complex many-body interactions. Following the spirit of renormalization from field theory, we can recursively refine the evaluations of the effective field and the quasi-particles. As shown in Fig.~\ref{fig_spin3}(a), we can extend the FP layer number in Fig.~\ref{fig:figure1}(b) to obtain the deep effective field neural networks. The  evaluation procedure is detailed as
\begin{align}
 F_{i}&=f_{i-1}(S_{i-1}),\, S_{i}=g_i(S_0)\odot F_{i}, \, E=q(S_n), \nonumber%i=1,\ldots,n.\label{eq1}
\end{align}
where the function $f_{i-1}$ maps the previous quasi-particle layer $S_{i-1}$ (for $i\geq 2$) or the initial layer $S_{0}$ (for $i=1$) to the effective field layer $F_{i}$. The input layer $S_{0}$ is first processed by $g_{i}$, then multiplied in elements by $F_{i}$ to produce the quasi-particle layer $S_{i}$. Each FP layer consists of $F_{i}$ and $S_{i}$. The final quasi-particle layer $S_{n}$ generates the output energy $E$ through the function $q$, which sums all elements of the last layer.  Typically, $f_{i}$ and $g_{j}$ are nonlinear to enhance expressiveness, though linear functions may suffice for simpler models like the classical 2D Ising model with nearest-neighbor interactions.

We present two case studies to demonstrate the capability of EFNNs to capture many-body interactions. The first case study focuses on the classical 3-spin infinite range model in 1D, with Hamiltonian:
\begin{align}
 H(S)&= -\sum_{i<j<k}J_{ijk}s_{i}s_{j}s_{k}
    \label{eqn_H_stochastic},
\end{align}
where the spins take binary values $0,1$, $J_{ijk}$ are constants, and the system comprises 15 lattice sites. We train EFNNs to evaluate $H(S)$ in Eq.~(\ref{eqn_H_stochastic}), and compare its performance with that of  DNNs.   

\begin{figure*}
    \centering
     \makebox[\textwidth][c]{
    \hspace{-0.5cm}
    \includegraphics[width=0.9\linewidth]{fig_all/fig3.pdf}
    }
    \caption{Computational workflow of  EFNNs with symmetrization. (a) 
    Symmetrization: Each of the three channels of $S_{0}$ ($S_{0,x}$, $S_{0,y}$, $S_{0,z}$) undergoes convolution and element-wise multiplication within the same channel. The resulting products are summed to form the symmetrization layer $T_{j}(S_{0})$. Subsequently, $T_{j}(S_{0})$ is passed through a sequence of layers---convolution, batch normalization, $\tanh$ activation, and another convolution---to produce $F_{1}$ for $j=0$ or $g_{j}(T_{j}(S_{0}))$ for $j=1,\ldots,n$.
    (b) Generation of quasi-particle layers: The transformed symmetrization layer $g_{j}(T_{j}(S_{0}))$ is element-wise multiplied with the effective field layer $F_{j}$, resulting in the quasi-particle layer $S_{j}$, $j=1,\ldots,n$. (c) Transformation to effective field layers: Each quasi-particle layer $S_{k-1}$ is processed through convolution, batch normalization, $\tanh$ activation, and another convolution to generate the effective field layer $F_{k}$, for $k=2,\ldots,n$. (d) Energy evaluation: The final quasi-particle layer $S_{n}$ is transformed into a single-channel matrix. An element-wise summation of this matrix yields the scalar energy prediction $E$. These images are generated after training the model with $C=10$ channels and $n=2$ layers. Only the first five channels are displayed for clarity, using the 576th sample from the test set as input $S_{0}$. }
    \label{fig_conv_details}
\end{figure*}

We generate 16000 training and 4000 test samples using Eq.~(\ref{eqn_H_stochastic}). Utilizing the EFNN architecture in Fig.~\ref{fig_spin3}(a), the nonlinear mappings $f_{i}$ and $g_{j}$ are constructed by sequential linear layers with $\tanh$ activations. The network is trained with the Adam optimizer and a decaying learning rate. Performance is evaluated by the relative error, defined as the square root of the MSE on the test set divided by the absolute mean of $H(S)$. We compare EFNNs to standard DNNs, using 18 neurons per layer and varying the number of FP layers ($n=1,2,3$). With one FP layer, the DNN achieves approximately a $7\times 10^{-2}$ relative error, while the EFNN attains around $4\times 10^{-2}$. As the number of FP layers increases, the DNN's error rapidly worsens, whereas the EFNN's error decreases to about $1\times 10^{-2}$ with three FP layers (Fig.~\ref{fig_spin3}(b)). Additionally, Fig.~\ref{fig_spin3}(c) shows that increasing the number of neurons in the EFNN from 15 to 150 leads to an algebraic decrease in relative error, reaching a minimum around 150 neurons for all tested FP layers, and optimal performance is achieved with three FP layers. These results demonstrate that EFNNs effectively capture many-body interactions through multiple feed-forward paths, improving accuracy as layers and neurons increase. In contrast, DNNs lose their ability to fit the energy accurately with additional layers. Both the number of FP layers and the internal neuron count are crucial for the EFNNs' high performance.


We proceed to our second, more challenging case study: evaluating the Monte-Carlo energy of the quantum double exchange model~\cite{zener1951interaction,anderson1955considerations,de1960effects} at finite temperature. The Hamiltonian on an $N\times N$ lattice reads

\begin{equation}
    H(S)=-t \sum_{\langle i, j\rangle, \alpha}\left(\hat{c}_{i \alpha}^{\dagger} \hat{c}_{j \alpha}+\text {h.c.}\right)-\frac{J}{2} \sum_{i, \alpha, \beta} \vec{s}_{i} \cdot \hat{c}_{i \alpha}^{\dagger} \vec{\sigma}_{\alpha \beta} \hat{c}_{i \beta}, \label{eqn_H_quantum}
\end{equation}
where ${\langle i j\rangle}$ denotes nearest neighbors, $\hat{c}_{i \alpha}$ is the fermion annihilation operator with spin ${\alpha}$ at site $i$, $\vec{\sigma}$ are the Pauli matrices, and $\vec{s}_{i}\in\mathbb{S}^{2}$ is a 3D classical unit vector representing the local spin at site $i$, the classical spin interacts with the fermions through an on-site coupling, and $S$ is the collection of all classical spins $\vec{s}_{i}$. We set $N=10$, $J=16t$, $\mu=-8.3t$, $T=0.1t$, $t=1$.



At finite temperature $T$, the Monte-Carlo energy is defined as $E_{\operatorname{MC}}(S) =-T \sum_n\log (1+e^{-\frac{1}{T} E_{n}(S)})$, where $\{E_{n}(S)\}$ are the eigen-energies of $H(S)$ in Eq.~(\ref{eqn_H_quantum}) for a given spin configuration $S$. This quantity proves to be useful in Monte-Carlo simulations of the double exchange model, as it can replace exact diagonalization in Determinant Quantum Monte Carlo (DQMC) computations~\cite{liu2017femion}. Diagonalizing a $2N^{2}\times 2N^{2}$ Hermitian matrix for Eq.~(\ref{eqn_H_quantum}) has a computational complexity of $O(N^{6})$ for each spin update. Both effective models and neural networks can significantly reduce this complexity. We compare the performance of an   effective model incorporating 4-body RKKY-type interactions~\cite{ruderman1954indirect,kasuya1956theory,yosida1957magnetic} with that of EFNNs adopting symmetrization. The effective model is expressed as
\begin{align}
    E_{\operatorname{eff}}(S)&\sim J_{0}+\sum_{ij}g_{ij}\vec{s}_{i}\cdot \vec{s}_{j}+\sum_{ijkl}g_{ijkl}(\vec{s}_{i}\cdot \vec{s}_{j})(\vec{s}_{k}\cdot \vec{s}_{l}).\label{eqn_quantum_eff}
\end{align} 
Here, the dot products include all pairs of spins on the lattice, including self-interactions. Clearly, the number of parameters factorially increases with $N$ and the effective model cannot be solved exactly for a large $N$. To  simplify, we divide the $10\times 10$ lattice into  four $5\times 5$ tiles, restricting two- and four-body interactions within each tile. %For $N=10$,  this approach reduces the number of parameters from $1+\binom{N^{2}+1}{2}+\binom{\binom{N^{2}+1}{2}+1}{2}=12758826$ to $1+4\bigg[\binom{\frac{N^{2}}{4}+1}{2}+\binom{\binom{\frac{N^{2}}{4}+1}{2}+1}{2} \bigg]=213201$ in the effective model. 

The spin interactions in Eq.~(\ref{eqn_quantum_eff}) are exclusively expressed as dot products to satisfy the $\operatorname{O}(3)$ symmetry of Eq.~(\ref{eqn_H_quantum}). Inspired by this symmetry, we incorporate symmetrization layers into the EFNN architecture, depicted in Fig.~\ref{fig_conv_details}(a). The symmetrization is achieved by summing dot products of each channel of $S_{0}$ after convolution:
\begin{align}
    T_{j}(S_{0})&=\sum_{k=x,y,z}\operatorname{conv}(S_{0,k})\odot \operatorname{conv}(S_{0,k}),\,\,j=0,1,\ldots,n.\label{eqn_efnn_sym_Tj}\nonumber
\end{align}
Here, each $S_{0,k}\in\mathbb{R}^{N\times N}$ is convolved into $\mathbb{R}^{C\times N \times N}$.  Supplementary Material~\cite{supplementary_material} details the convolution process and proves that $T_{j}$ remains invariant under $\operatorname{O}(3)$ transformations. As shown in Fig.~\ref{fig_efnn_vs_eff}(a), in the first FP layer, $F_{1}$ and $S_{1}$ are initialized as
\begin{align}
    F_{1}&=f_{0}(T_{0}(S_{0})),\,\, S_{1}=g_{1}(T_{1}(S_{0}))\odot F_{1}. \nonumber
\end{align}
For subsequent layers $j=2,\ldots,n$, the effective field and quasi-particle layers are
\begin{align}
    F_{j}&=f_{j-1}(S_{j-1}), \,\, S_{j}=g_{j}(T_{j}(S_{0}))\odot F_{j}. \nonumber
\end{align}
The mappings $f_{j}$ and $g_{k}$ consist of a convolutional layer, batch normalization, a $\tanh$ activation, and another convolutional layer. Fig.~\ref{fig_conv_details} illustrates the computational workflow: (a) symmetrization, (b) generation of quasi-particle layers, (c) transformation to effective field layers for subsequent FP layers, and (d) evaluation of energy. For the energy function $q$, after passing through the aforementioned layers, an extra summation layer generates a scalar value as the predicted energy. Specifically, the final quasi-particle layer $S_{n}$ is transformed into a single-channel matrix, and an element-wise summation is performed to produce the scalar energy prediction $E$, as shown in Fig.~\ref{fig_conv_details}(d). The layer dimensions are as follows: input layer $S_{0}\in\mathbb{R}^{3\times N\times N}$, intermediate layers $F_{j}$, $S_{j}\in\mathbb{R}^{C\times N\times N}$, for $j=1,\ldots,n$, and output $E=q(S_{n})\in\mathbb{R}$.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{fig_all/fig4.pdf}
     %\makebox[\textwidth][l]{\includegraphics[width=0.9\linewidth, %trim=15 10 10 10,  % Adjust these values as needed
    %clip]{V20Figs/fig_qt_efnn/hadamard_efnn_vs_eff.pdf}
    %}
    \caption{Relative error of EFNNs. (a) Architecture of EFNN incorporating symmetrization layers $T_{j}$, $j=0,1,\ldots,n$. (b) Comparison of EFNNs' and the effective model's performance on a test set with lattice size $N=10$, for channel numbers $C=10,15,20,25$. (c) Performance of the EFNN model, trained on a $10\times 10$ lattice with $C=20$ channels, when applied to larger lattice systems. }
    \label{fig_efnn_vs_eff}
\end{figure}


We generate a total of $2\times 10^{6}$ data points and split them into 80\% for training and 20\% for testing. These data are obtained by exact diagonalization of the Hamiltonian in Eq.~(\ref{eqn_H_quantum}). For the effective model in Eq.~(\ref{eqn_quantum_eff}), we perform a linear regression to obtain the coefficients. The EFNNs with symmetrization are trained using networks with $C=10,15,20,25$ channels and $n=1,2,3$ FP layers. As shown in Fig.~\ref{fig_efnn_vs_eff}(b), the effective model achieves a relative error of approximately $2\times 10^{-2}$. In contrast, the EFNNs with symmetrization consistently  reduce the relative error below $3\times 10^{-3}$ as $C$ increases from 10 to 25, for $n=1,2,3$. Notably, the EFNNs outperform the effective model by a factor of six while utilizing significantly fewer parameters, underscoring their  capability to capture high-order interactions in quantum many-body systems. Their parameter count scales as $O(nC^{2})$, resulting in 26250, 49560, and 72870 parameters for $n=1,2,3$ at $C=10$. In stark contrast, the perturbative expansion in Eq.~(\ref{eqn_quantum_eff}) scales factorially as $O\bigg(\binom{\frac{1}{2}N^{4}+\frac{1}{2}N^{2}+\frac{1}{2}k-1}{\frac{1}{2}k}\bigg)$, with $k$ denoting the interaction order---a scaling that can lead to divergence at higher orders. The full effective model in Eq.~(\ref{eqn_quantum_eff}) contains 12758826 parameters, which reduces to 213201 when interactions are restricted within each tile. Furthermore, EFNNs effectively select the most significant interactions automatically, an essential aspect of renormalization. By renormalizing the interactions and prioritizing key terms, EFNNs avoid both divergence and explosive growth in the number of parameters.


We further assess the extrapolation performance of EFNNs with symmetrization with the model trained on a $10 \times 10$ lattice and tested on systems of $N = 15, 20, 25, 30, 35$, and 40. Using the relative error of energy as the performance metric,  Fig.~\ref{fig_efnn_vs_eff}(c) illustrates that for $N = 10$, all EFNNs accurately predict the energy, consistent with Fig.~\ref{fig_efnn_vs_eff}(b). As the lattice size increases, the relative error slightly decreases for all FP layers with $n=1,2,3$. This occurs because the absolute error increases only marginally with $N$, while the absolute value of the average energy grows significantly, thereby reducing the relative error. Moreover,  EFNNs with $n=2$ and $n=3$ FP layers outperform those with a single FP layer, demonstrating enhanced renormalization capability with additional layers. Notably, even for larger systems, all models maintain relative errors below  $3\times 10^{-3}$, with the $n=2$ configuration achieving a minimum relative error of $1\times 10^{-3}$. These results underscore the EFNN's efficacy: the convolution
operations with padding under boundary condition PBC effectively
capture interactions between localized and neighboring spins, and the renormalization through multiple FP layers ensures robust energy predictions across varying lattice sizes. 










 
\begin{figure}[tb!]
    \centering \includegraphics[width=1\linewidth]{fig_all/fig5.pdf}
       %\makebox[\textwidth][l]{\includegraphics[width=1\linewidth]{V20Figs/fig_pade/hadamard_pade.pdf}
     %}
\caption{Continued function representation of an EFNN with 3 FP layers. (a) In an EFNN, the initial layer $S_{0}$ is recursively integrated into every quasi-particle layer. After a mapping, $S_{0}$ is multiplied with subsequent layers, forming a continued function representation (see right-hand side equation). (b) A typical architecture of ResNet, only one skip connection starts from $S_{0}$ and summation is used, which limits its renormalization capability. (c) Removing the $S_{0}$ connections transforms an EFNN into a standard DNN, where  $S_{0}$ only appears at the beginning of the iterations, resulting in significantly diminished expressive power. }
\label{fig_continued_fraction}
    \end{figure}

We give some discussions and conclude this paper. In both case studies, we have demonstrated that increasing the number of layers and neurons (or channels in convolutional layers) enhances the neural network's accuracy, aligning with well-established findings in fields like computer vision. EFNN's self-similar structure mirrors the renormalization process---a non-perturbative method for evaluating complicated functions, such as the Yang-Mills $\beta$ function~\cite{SchwenkPolonyi2012} or Becke's correction to the LDA functional in DFT~\cite{PhysRevA.38.3098Becke}. This approximation framework stems from  Pad\'{e} approximants~\cite{Baker_Graves-Morris_1996} using rational functions, which can also be expressed as continued fractions---an important and specific example of continued functions~\cite{BenderOrszag1999}, known for superior convergence with much fewer parameters~\cite{Martin_2004unreasonable, Abhignan2021SimpleTools, 10.1063/1.531619exponentials, POLAND1998394}. Actually, EFNNs possess exactly the structure of continued functions, as shown in Fig.~\ref{fig_continued_fraction}(a).  In an EFNN, the initial feature layer $S_{0}$ connects recursively to every quasi-particle layer via mappings $g_{j}$; each nonlinear mapping $f_{j-1}$ is multiplied with $g_{j}(S_{0})$. In addition, EFNNs employ the $\tanh$ activation function within their renormalization-like structure, making them compatible with standard  deep learning frameworks and enabling training via stochastic gradient descent with back-propagation. This contrasts with Pad\'{e} approximants, which typically use the inverse function and hence usually determine parameters through perturbative series matching due to potential singularity in the gradient descent.
As shown in Fig.~\ref{fig_continued_fraction}(b), ResNets ~\cite{7780459Resnet,8100117_aggregate,zagoruyko2017wideresidualnetworks,8099726_dense} adopt a similar but fundamentally different skip-like connections from EFNNs. In a ResNet, $S_{0}$ is skip-connected to later layers only once, with later skip connections starting in the middle, and its summation-based aggregation fails to meet the continued function formulation---resulting in suboptimal performance to characterize the many-body interactions. Standard DNNs (Fig.~\ref{fig_continued_fraction}(c)) lack skip connections entirely, and $S_{0}$ only appears in the beginning of iterations, resulting in a very limited expressive capability. Moreover, by partitioning intermediate layers into effective field and quasi-particle layers, EFNNs provide a clear physical interpretation: quasi-particles emerge from the dot product of interacting particles (via a mapping) and effective fields, which is absent in ResNets and standard DNNs.
%In contrast, the summation in ResNet skip connections lacks physical meaning and fails to capture the underlying many-body interactions. 

%We give some discussions and conclude this paper. In both case studies, we have demonstrated that increasing the number of layers and neurons (or channels in convolutional layers) enhances the neural network's accuracy, aligning with well-established findings in fields like computer vision. The connections from earlier to later layers in EFNNs resemble \enquote{skip connections} found in residual neural  networks~\cite{7780459Resnet,8100117_aggregate,zagoruyko2017wideresidualnetworks}. Notably, DenseNet~\cite{8099726_dense} connects each layer to every subsequent layer, similar to EFNN's structure. Our approach offers two significant advantages. First, by partitioning intermediate layers into effective field and quasi-particle layers, EFNNs provide a clear physical interpretation. Specifically, quasi-particles arise from the dot products of interacting particles (with a mapping) and effective fields, allowing EFNNs to greatly outperform ResNets.  In contrast, ResNets rely on simple summations in their skip connections, which fail to capture many-body interactions, similar to conventional DNNs. Second, EFNN's self-similar structure mirrors the renormalization process---a non-perturbative method for evaluating complicated functions, such as the Yang-Mills $\beta$ function~\cite{SchwenkPolonyi2012} or Becke's correction to the LDA functional in DFT~\cite{PhysRevA.38.3098Becke}. This approximation framework stems from  Pad\'{e} approximants~\cite{Baker_Graves-Morris_1996} using rational functions, which can also be expressed as continued fractions---an important and specific example of continued functions~\cite{BenderOrszag1999}, known for their remarkable convergence properties~\cite{Martin_2004unreasonable, Abhignan2021SimpleTools, 10.1063/1.531619exponentials, POLAND1998394}. EFNNs possess, in fact, the structure of continued functions, as shown in Fig.~\ref{fig_continued_fraction}(a).  In an EFNN, the initial feature layer $S_{0}$ connects to subsequent quasi-particle layers through mappings $g_{j}$, this makes $S_{0}$ appear recursively in the evaluation of each quasi-particle layer, as shown in the equation in Fig.~\ref{fig_continued_fraction}(a), where $f_{0}$, $f_{1}$ and $f_{2}$ are the  nonlinear mappings required in a continued function. 
%Conversely, standard DNNs lack these connections, and $S_{0}$ only appears as the initial value of the iterations, as shown in Fig.~\ref{fig_continued_fraction}(b) . Such a purely iterative structure  does not have much expressive power.  EFNNs employ the $\tanh$ activation function within their renormalization-like structure for the nonlinear function in the continued function evaluation process, compatible with deep learning frameworks and trained via stochastic gradient descent with back-propagation. This contrasts with Pad\'{e} approximants, which typically use inverse as the nonlinear function and  determine parameters through perturbative series matching.




In conclusion, inspired by field theory, we have proposed a novel deep neural network architecture called EFNN, which effectively captures high-order interactions in both classical and quantum models. EFNN's high accuracy and efficiency arise from the recursive refinement of effective fields and the construction of quasi-particles that encapsulate complex many-body effects.  As discussed earlier, the connections from the true particle layer $S_{0}$  to the quasi-particle layers (or the preceding symmetrization layers)  establish a  structure of continued functions, and  have renormalization power analogous  to Pad\'{e} approximants. Once the effective fields and quasi-particles are properly renormalized, the trained model can accurately predict the energy for a system with complicated many-body interactions. Furthermore, EFNNs maintain high accuracy even when applied to significantly larger systems. In future work, we aim to integrate Pad\'{e} approximation and continued function theories more rigorously into the EFNN framework and derive error estimates for the neural network's predictions.

\textit{Acknowledgments}---This work is supported by the Hong Kong Research Grants Council (16306220) and National Key R$\&$D Program of China (2021YFA1401500). Y.Z. is supported by the Max Planck Partner lab on quantum materials from Max Planck Institute Chemical Physics of Solids. 


%\textit{Acknowledgments}---This work is supported by the Hong Kong Research Grants Council (16306220), National Key R$\&$D Program of China (2021YFA1401500), and National Natural Science Foundation of China (12022416).








%\nocite{*}
%TC:ignore
%\printbibliography
\bibliography{EFNN}
%TC:endignore

\end{document}
%
% ****** End of file apssamp.tex ******
