
## Related Work: Low-rank preconditioner projection
@inproceedings{shazeer2018adafactor,
  title={Adafactor: Adaptive learning rates with sublinear memory cost},
  author={Shazeer, Noam and Stern, Mitchell},
  booktitle={International Conference on Machine Learning},
  pages={4596--4604},
  year={2018},
  organization={PMLR}
}

@article{sharir2021image,
  title={An image is worth 16x16 words, what is a video worth?},
  author={Sharir, Gilad and Noy, Asaf and Zelnik-Manor, Lihi},
  journal={arXiv preprint arXiv:2103.13915},
  year={2021}
}

@article{zhao2024galore,
  title={GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection},
  author={Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang and Anandkumar, Anima and Tian, Yuandong},
  journal={arXiv preprint arXiv:2403.03507},
  year={2024}
}


@article{smith2018cocoa,
  title={CoCoA: A general framework for communication-efficient distributed optimization},
  author={Smith, Virginia and Forte, Simone and Ma, Chenxin and Tak{\'a}{\v{c}}, Martin and Jordan, Michael I and Jaggi, Martin},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={230},
  pages={1--49},
  year={2018}
}


@article{douillard2023diloco,
  title={Diloco: Distributed low-communication training of language models},
  author={Douillard, Arthur and Feng, Qixuan and Rusu, Andrei A and Chhaparia, Rachita and Donchev, Yani and Kuncoro, Adhiguna and Ranzato, Marc'Aurelio and Szlam, Arthur and Shen, Jiajun},
  journal={arXiv preprint arXiv:2311.08105},
  year={2023}
}

@article{stich2018local,
  title={Local SGD converges fast and communicates little},
  author={Stich, Sebastian U},
  journal={arXiv preprint arXiv:1805.09767},
  year={2018}
}



@article{wu2023rethinking,
  title={Rethinking memory and communication cost for efficient large language model training},
  author={Wu, Chan and Zhang, Hanxiao and Ju, Lin and Huang, Jinjing and Xiao, Youshao and Huan, Zhaoxin and Li, Siyuan and Meng, Fanzhuang and Liang, Lei and Zhang, Xiaolu and others},
  journal={arXiv preprint arXiv:2310.06003},
  year={2023}
}



@article{chen2019extreme,
  title={Extreme tensoring for low-memory preconditioning},
  author={Chen, Xinyi and Agarwal, Naman and Hazan, Elad and Zhang, Cyril and Zhang, Yi},
  journal={arXiv preprint arXiv:1902.04620},
  year={2019}
}

@article{anil2019memory,
  title={Memory efficient adaptive optimization},
  author={Anil, Rohan and Gupta, Vineet and Koren, Tomer and Singer, Yoram},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{gupta2018shampoo,
  title={Shampoo: Preconditioned stochastic tensor optimization},
  author={Gupta, Vineet and Koren, Tomer and Singer, Yoram},
  booktitle={International Conference on Machine Learning},
  pages={1842--1850},
  year={2018},
  organization={PMLR}
}

@article{AdamNoConverge,
    author =       "Sashank Reddi and Satyen Kale and Sanjiv Kumar",
    title =        "On the Convergence of Adam and Beyond",
    journal =      "International Conference on Learning Representations",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2018",
    DOI =          ""
}

@article{ADAM,
    author =       "Diederik P. Kingma and Jimmy Ba",
    title =        "Adam: A Method for Stochastic Optimization",
    journal =      "International Conference for Learning Representations",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2015",
    DOI =          ""
}

@article{AdaptiveFederatedOptimization,
    author =       "Sashank Reddi and Zachary Charles and Manzil Zaheer and Zachary Garrett and Keith Rush and Jakub Konecný and Sanjiv Kumar and Brendan McMahan",
    title =        "Adaptive Federated Optimization",
    journal =      "International Conference on Learning Representations",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2021",
    DOI =          ""
}


@article{HeavyTailedNoisePaper,
    author =       "Jingzhao Zhang and Sai Karimireddy and Andreas Veit and Seungyeon Kim and Sashank Reddi and Sanjiv Kumar and Suvrit Sra",
    title =        "Why are Adaptive Methods Good for Attention Models?",
    journal =      "Advances in  Neural Information Processing Systems",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2020",
}

@article{heavytail1,
    author =       "Thanh Huy Nguyen and Umut Simsekli and Mert Gurbuzbalaban and Gael Richard",
    title =        "First Exit Time Analysis of Stochastic Gradient Descent Under Heavy-Tailed Gradient Noise",
    journal =      "Advances in Neural Information Processing Systems",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2019",
}


@article{heavytail2,
    author =       "Umut Simsekli and Levent Sagun and Mert Gurbuzbalaban",
    title =        "A Tail-Index Analysis of Stochastic Gradient Noise in Deep Neural Networks",
    journal =      "International Conference on Machine Learning",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2019",
}


@article{heavytail3,
    author =       "Umut Simsekli and Lingjiong Zhu and Yee Whye Teh and Mert Gurbuzbalaban",
    title =        "Fractional Underdamped Langevin Dynamics: Retargeting SGD with Momentum under Heavy-Tailed Gradient Noise",
    journal =      "International Conference on Machine Learning",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2020",
}


@article{SGD,
    author =       "Herbert Robbins and Sutton Monro",
    title =        "A Stochastic Approximation Method",
    journal =      "The Annals of Mathematical Statistics",
    volume =       "22",
    number =       "3",
    pages =        "400--407",
    year =         "1951",
}

@article{BERT,
    author =       "Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova",
    title =        "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    journal =      "arXiv",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2018",
}

@article{GPT3,
    author =       "Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario",
    title =        "Language Models are Few-Shot Learners",
    journal =      "Advances in Neural Information Processing Systems",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2020",
}

@article{ViT,
    author =       "Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby",
    title =        "An image is worth 16X16 words: Transformers for Image recognition at scale",
    journal =      "International Conference on Learning Representations",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2021",
}


@article{TianReviewPaper,
    author =       "Tian Li and Anit Kumar Sahu and Ameet Talwalkar and Virginia Smith",
    title =        "Federated Learning: Challenges, Methods, and Future Directions",
    journal =      "IEEE Signal Processing Magazine",
    volume =       "37",
    number =       "3",
    pages =        "50--60",
    year =         "2020",
}


@article{LMLargeMemory,
    author =       "Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu",
    title =        "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
    journal =      "Journal of Machine Learning Research",
    volume =       "21",
    number =       "",
    pages =        "1--67",
    year =         "2020",
}

@article{AdaGrad,
    author =       "John Duchi and Elad Hazan and Yoram Singer ",
    title =        "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization",
    journal =      "Journal of Machine Learning Research",
    volume =       "12",
    number =       "",
    pages =        "2121--2159",
    year =         "2011",
}


@article{FedAMS,
    author =       "Yujia Wang and Lu Lin and Jinghui Chen",
    title =        "Communication-Efficient Adaptive Federated Learning",
    journal =      "International Conference on Machine Learning",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2022",
    DOI = ""
}


@article{FedAvgConvOnNonIID,
    author =       "Xiang Li and Kaixuan Huang and Wenhao Yang and Shusen Wang and Zhihua Zhang",
    title =        "On the Convergence of FedAVG on Non-IID Data",
    journal =      "International Conference on Learning Representations",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2020",
    DOI = ""
}

@article{AdaAlter,
    author =       "Cong Xie and Oluwasanmi Koyejo and Indranil Gupta and Haibin Lin ",
    title =        "Local AdaAlter: Communication-Efficient Stochastic Gradient Descent with Adaptive Learning Rates",
    journal =      "OPT2020: 12th Annual Workshop on Optimization for Machine Learning",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2020",
    DOI = ""
}


@article{FedNOVA,
    author =       "Jianyu Wang and Qinghua Liu and Hao Liang and Gauri Joshi and H. Vincent Poor",
    title =        "Tackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization",
    journal =      "Advances in Neural Information Processing Systems",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2020",
    DOI = ""
}


@article{MIMEpaper,
  title={Breaking the centralized barrier for cross-device federated learning},
  author={Sai Praneeth Karimireddy and Martin Jaggi and Satyen Kale and Mehryar Mohri and Sashank Reddi and Sebastian U. Stich and Ananda Theertha Suresh},
  journal={Advances in Neural Information Processing Systems},
  volume={},
  pages={},
  year={2021}
}

@article{delayedpreconditioner,
    author =       "Tian Li and Manzil Zaheer and Ziyu Liu and Sashank Reddi and Brendan McMahan and Virginia Smith",
    title =        "Differentially Private Adaptive Optimization with Delayed Preconditioners",
    journal =      "International Conference on Learning Representations",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2023",
    DOI = ""
}

@article{linearattentionisallyouneed,
    author =       "Kwangjun Ahn and Xiang Cheng and Minhak Song and Chulhee Yun and Ali Jadbabaie and Suvrit Sra",
    title =        "Linear attention is (maybe) all you need (to understand transformer optimization)",
    journal =      "International Conference on Learning Representations",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2024",
    DOI = ""
}

@article{adadps,
    author =       "Tian Li and Manzil Zaheer and Sashank J. Reddi and Virginia Smith",
    title =        "Private Adaptive Optimization with Side information",
    journal =      "International Conference on Machine Learning",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2022",
    DOI = ""
}


@article{heavytailedclassimbalance,
    author =       "Frederik Kunstner and Robin Yadav and Alan Milligan and Mark Schmidt and Alberto Bietti",
    title =        "Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models",
    journal =      "ArXiv",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2024",
    DOI = ""
}

@article{CLIP,
    author =       "Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever ",
    title =        "Learning Transferable Visual Models From Natural Language Supervision",
    journal =      "International Conference on Machine Learning",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2021",
    DOI = ""
}

@article{HyperspherePaper,
    author =       "Tongzhou Wang and Phillip Isola",
    title =        "Understanding Contrastive Representation Learning through
Alignment and Uniformity on the Hypersphere",
    journal =      "International Conference on Machine Learning",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2020",
    DOI = ""
}

@article{EfficientAdaptiveFederatedOptimization,
    author =       "Su Hyeong Lee and Sidharth Sharma and Manzil Zaheer and Tian Li",
    title =        "Efficient Adaptive Federated Optimization",
    journal =      "ICML Workshop on Advancing Neural Network Training",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2024",
    DOI = ""
}


@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  year={2017}
}


@article{wang2021field,
  title={A field guide to federated optimization},
  author={Wang, Jianyu and Charles, Zachary and Xu, Zheng and Joshi, Gauri and McMahan, H Brendan and Al-Shedivat, Maruan and Andrew, Galen and Avestimehr, Salman and Daly, Katharine and Data, Deepesh and others},
  journal={arXiv preprint arXiv:2107.06917},
  year={2021}
}

@article{kairouz2021advances,
  title={Advances and open problems in federated learning},
  author={Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and others},
  journal={Foundations and trends{\textregistered} in machine learning},
  volume={14},
  number={1--2},
  pages={1--210},
  year={2021},
  publisher={Now Publishers, Inc.}
}


@article{tong2020effective,
  title={Effective federated adaptive gradient methods with non-iid decentralized data},
  author={Tong, Qianqian and Liang, Guannan and Bi, Jinbo},
  journal={arXiv preprint arXiv:2009.06557},
  year={2020}
}

@inproceedings{wang2022communication,
  title={Communication-efficient adaptive federated learning},
  author={Wang, Yujia and Lin, Lu and Chen, Jinghui},
  booktitle={International Conference on Machine Learning},
  pages={22802--22838},
  year={2022},
  organization={PMLR}
}

@article{sun2023fedlalr,
  title={FedLALR: Client-Specific Adaptive Learning Rates Achieve Linear Speedup for Non-IID Data},
  author={Sun, Hao and Shen, Li and Chen, Shixiang and Sun, Jingwei and Li, Jing and Sun, Guangzhong and Tao, Dacheng},
  journal={arXiv preprint arXiv:2309.09719},
  year={2023}
}


@article{xie2019local,
  title={Local adaalter: Communication-efficient stochastic gradient descent with adaptive learning rates},
  author={Xie, Cong and Koyejo, Oluwasanmi and Gupta, Indranil and Lin, Haibin},
  journal={arXiv preprint arXiv:1911.09030},
  year={2019}
}

@inproceedings{chen2020toward,
  title={Toward communication efficient adaptive gradient method},
  author={Chen, Xiangyi and Li, Xiaoyun and Li, Ping},
  booktitle={Proceedings of the 2020 ACM-IMS on Foundations of Data Science Conference},
  pages={119--128},
  year={2020}
}

@article{wang2021local,
  title={Local adaptivity in federated learning: Convergence and consistency},
  author={Wang, Jianyu and Xu, Zheng and Garrett, Zachary and Charles, Zachary and Liu, Luyang and Joshi, Gauri},
  journal={arXiv preprint arXiv:2106.02305},
  year={2021}
}

@inproceedings{Krizhevsky2009LearningML,
  title={Learning Multiple Layers of Features from Tiny Images},
  author={Alex Krizhevsky},
  year={2009},
  url={https://api.semanticscholar.org/CorpusID:18268744}
}

@article{lecun1998mnist,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={IEEE}
}

@article{blei2003latent,
  title={Latent Dirichlet Allocation},
  author={Blei, David M and Ng, Andrew Y and Jordan, Michael I},
  journal={Journal of Machine Learning Research},
  volume={3},
  pages={993--1022},
  year={2003}
}

@inproceedings{49052,title	= {Google Landmarks Dataset v2 - A Large-Scale Benchmark for Instance-Level Recognition and Retrieval},author	= {Tobias Weyand and André Araujo and Bingyi Cao and Jack Sim},year	= {2020},URL	= {https://arxiv.org/abs/2004.01804},booktitle	= {CVPR}}


@article{ImageNet21K,
  title={ImageNet-21K Pretraining for the Masses},
  author={Ridnik, Tal and Ben-Baruch, Emanuel and Noy, Asaf and Zelnik, Lihi},
  journal={Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks},
  volume={},
  pages={},
  year={2021}
}

@article{modelsoup,
    author =       "Mitchell Wortsman and Gabriel Ilharco and Samir Yitzhak Gadre and Rebecca Roelofs and Raphael Gontijo-Lopes and Ari S. Morcos and Hongseok Namkoong and Ali Farhadi and Yair Carmon and Simon Kornblith and Ludwig Schmidt",
    title =        "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
    journal =      "International Conference on Machine Learning",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2022",
}


@article{DNNnonconvex,
    author = "Anna Choromanska and Mikael Henaff and Michael Mathieu and G\'{e}rard Ben Arous and Yann LeCun",
    title = "The Loss Surfaces of Multilayer Networks",
    journal = "Proceedings of the 18th International Conference on Artificial Intelligence and Statistics",
    volume = "",
    number = "",
    pages = "",
    year = "2015",
}

@article{DNNapproximatelyconvexoverSGD,
    author = "Ian J Goodfellow and Oriol Vinyals and Andrew M Saxe",
    title = "Qualitatively characterizing neural network optimization problems",
    journal = "International Conference on Learning Representations",
    volume = "",
    number = "",
    pages = "",
    year = "2015",
}

@article{SWA,
    author = "Pavel Izmailov and Dmitrii Podoprikhin and Timur Garipov and Dmitry Vetrov and Andrew Gordon Wilson",
    title = "Averaging Weights Leads to Wider Optima and Better Generalization",
    journal = "Conference on Uncertainty in Artificial Intelligence",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2018",
}

@article{GradClipCentralGeomPerspective,
    author = "Xiangyi Chen and Zhiwei Steven Wu and Mingyi Hong",
    title = "Understanding Gradient Clipping in Private SGD: A Geometric Perspective",
    journal = "Advances in Neural Information Processing Systems",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2020",
}


@article{DiLoCo,
  title={DiLoCo: Distributed Low-Communication Training of Language Models},
  author={Arthur Douillard and Qixuan Feng and Andrei A. Rusu and Rachita Chhaparia and Yani Donchev and Adhiguna Kuncoro and Marc'Aurelio Ranzato and Arthur Szlam and Jiajun Shen},
  journal={ICML Workshop on Advancing Neural Network Training},
  volume={},
  pages={},
  year={2024}
}

@article{DiLoCoAsynchronous,
  title={Asynchronous Local-SGD Training for Language Modeling},
  author={Bo Liu and Rachita Chhaparia and Arthur Douillard and Satyen Kale and Andrei A. Rusu and Jiajun Shen and Arthur Szlam and Marc'Aurelio Ranzato},
  journal={ICML Workshop on Advancing Neural Network Training},
  volume={},
  pages={},
  year={2024}
}

@article{OpenDiLoCo,
  title={OpenDiLoCo: An Open-Source Framework for Globally Distributed Low-Communication Training},
  author={Sami Jaghouar and Jack Min Ong and Johannes Hagemann},
  journal={ArXiv},
  volume={},
  pages={},
  year={2024}
}

@article{healthcare1,
  title={Patient clustering improves efficiency of federated machine learning to predict mortality and hospital stay time using distributed electronic medical records},
  author={Li Huang and Andrew Shea and Huining Qian and Aditya Masurkar and Hao Deng and Dianbo Liu},
  journal={Journal of Biomedical Informatics},
  volume={99},
  pages={},
  year={2019}
}

@article{healthcare2,
  title={Federated Learning in Distributed Medical Databases: Meta-Analysis of Large-Scale Subcortical Brain Data},
  author={Silva, Santiago and Gutman, Boris A. and Romero, Eduardo and Thompson, Paul M. and Altmann, Andre and Lorenzi, Marco},
  journal={2019 IEEE 16th International Symposium on Biomedical Imaging},
  volume={},
  pages={},
  year={2019}
}

@article{institution1,
  title={Federated Learning for Emoji Prediction in a Mobile Keyboard},
  author={Swaroop Ramaswamy and Rajiv Mathews and Kanishka Rao and Françoise Beaufays},
  journal={ArXiv},
  volume={},
  pages={},
  year={2019}
}


@article{finance1,
  title={Privacy protection federated learning system based on blockchain and edge computing in mobile crowdsourcing},
  author={Weilong Wang and Yingjie Wang and Yan Huang and Chunxiao Mu and Zice Sun and Xiangrong Tong and Zhipeng Cai},
  journal={Computer Networks},
  volume={215},
  pages={},
  year={2022}
}

@article{finance2,
  title={Efficient and Secure Federated Learning for Financial Applications},
  author={Tao Liu and Zhi Wang and Hui He and Wei Shi and Liangliang Lin and Wei Shi and Ran An and Chenhao Li},
  journal={ArXiv},
  volume={},
  pages={},
  year={2023}
}

@article{institution2,
  title={FedNCF: Federated Neural Collaborative Filtering for Privacy-preserving Recommender System},
  author={Jiang, Xueyong and Liu, Baisong and Qin, Jiangchen and Zhang, Yunchong and Qian, Jiangbo},
  journal={International Joint Conference on Neural Networks},
  volume={},
  pages={},
  year={2022}
}



@article{empirical5,
  title={Statistical Language Models based on Neural Networks},
  author={Tomas Mikolov},
  journal={Ph.D. thesis, Brno University of Technology},
  volume={},
  pages={},
  year={2012}
}


@article{empirical1,
  title={Convolutional sequence to sequence learning},
  author={Jonas Gehring and Michael Auli and David Grangier and Denis Yarats and Yann N Dauphin},
  journal={International Conference on Machine Learning},
  volume={},
  pages={},
  year={2017}
}

@article{theoretical1,
  title={Can gradient clipping mitigate label noise?},
  author={Aditya Krishna Menon and Ankit Singh Rawat and Sashank J Reddi and Sanjiv Kumar},
  journal={International Conference on Learning Representations},
  volume={},
  pages={},
  year={2020}
}

@article{empirical2,
  title={Regularizing and Optimizing LSTM Language Models},
  author={Stephen Merity and Nitish Shirish Keskar and Richard Socher},
  journal={International Conference on Learning Representations},
  volume={},
  pages={},
  year={2018}
}

@article{empirical3,
  title={Deep contextualized word representations},
  author={Matthew E Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},
  journal={International Conference on Learning Representations},
  volume={},
  pages={},
  year={2018}
}

@article{theoretical2,
  title={Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity},
  author={Jingzhao Zhang and Tianxing He and Suvrit Sra and Ali Jadbabaie},
  journal={International Conference on Learning Representations},
  volume={},
  pages={},
  year={2020}
}

@article{theoretical4,
  title={Improved Analysis of Clipping Algorithms for Non-convex Optimization},
  author={Bohang Zhang and Jikai Jin and Cong Fang and Liwei Wang},
  journal={Advances in Neural Information Processing Systems},
  volume={},
  pages={},
  year={2020}
}


@article{inprobsupreme,
  title={Stochastic Optimization with Heavy-Tailed Noise via Accelerated Gradient Clipping},
  author={Eduard Gorbunov and Marina Danilova and Alexander Gasnikov},
  journal={Advances in Neural Information Processing Systems},
  volume={},
  pages={},
  year={2020}
}

@article{RevisitGradClipModern,
  title={Revisiting gradient clipping: Stochastic bias and tight convergence guarantees},
  author={Anastasia Koloskova and Hadrien Hendrikx and Sebastian U Stich},
  journal={International Conference on Learning Representations},
  volume={ArXiv},
  pages={},
  year={2023}
}

@article{Debias1,
  title={Clip21: Error Feedback for Gradient Clipping},
  author={Sarit Khirirat and Eduard Gorbunov and Samuel Horváth and Rustem Islamov and Fakhri Karray and Peter Richtarik},
  journal={ArXiv},
  volume={},
  pages={},
  year={2023}
}

@article{Debias2,
  title={Differentially private sgd without clipping bias: An error-feedback approach},
  author={Xinwei Zhang and Zhiqi Bu and Zhiwei Steven Wu and and Mingyi Hong},
  journal={International Conference on Learning Representations},
  volume={},
  pages={},
  year={2024}
}

@article{angulardependencerestrictive,
  title={Understanding Gradient Clipping In Incremental Gradient Methods},
  author={Jiang Qian and Yuren Wu and Bojin Zhuang and Shaojun Wang and Jing Xiao},
  journal={Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  volume={},
  pages={},
  year={2021}
}

@article{symmetricnoise_distributed,
  title={A Communication-Efficient Distributed Gradient Clipping Algorithm for Training Deep Neural Networks},
  author={Mingrui Liu and Zhenxun Zhuang and Yunwei Lei and Chunyang Liao},
  journal={Advances in Neural Information Processing Systems},
  volume={},
  pages={},
  year={2022}
}

@article{hospitalmajor,
  title={Shifting machine learning for healthcare from development to deployment and from models to data},
  author={Angela Zhang and Lei Xing and James Zou and Joseph C. Wu},
  journal={Nature Biomedical Engineering},
  volume={6},
  pages={1330--1345},
  year={2022}
}

@article{highprobabilityunboundedvariance,
  title={High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance},
  author={Abdurakhmon Sadiev and Marina Danilova and Eduard Gorbunov and Samuel Horvath and Gauthier Gidel and Pavel Dvurechensky and Alexander Gasnikov and Peter Richtarik},
  journal={International Conference on Machine Learning},
  volume={},
  pages={},
  year={2023}
}

@article{highprobabilitydistributed,
  title={High-Probability Convergence for Composite and Distributed Stochastic Minimization and Variational Inequalities with Heavy-Tailed Noise},
  author={Eduard Gorbunov and Abdurakhmon Sadiev and Marina Danilova and Samuel Horvath and Gauthier Gidel and Pavel Dvurechensky and Alexander Gasnikov and Peter Richtarik},
  journal={International Conference on Machine Learning},
  volume={},
  pages={},
  year={2024}
}

@article{highprob1,
  title={Algorithms of Robust Stochastic Optimization Based on Mirror Descent Method},
  author={Anatoli Juditsky and Alexander Nazin and Arkadi Nemirovsky and Alexandre Tsybakov},
  journal={Automation and Remote Control},
  volume={},
  pages={},
  year={2019}
}

@article{highprob2,
  title={From Low Probability to High Confidence in Stochastic Convex Optimization},
  author={Damek Davis and Dmitriy Drusvyatskiy and Lin Xiao and Junyu Zhang},
  journal={Journal of Machine Learning Research},
  volume={22},
  pages={1--38},
  year={2021}
}


@article{highprob3,
  title={Clipped Stochastic Methods for Variational Inequalities with Heavy-Tailed Noise},
  author={Eduard Gorbunov and Marina Danilova and David Dobre and Pavel Dvurechensky and Alexander Gasnikov and Gauthier Gidel},
  journal={Advances in Neural Information Processing Systems},
  volume={},
  pages={},
  year={2022}
}


@article{highprobClip,
  title={High-probability bounds for Non-Convex Stochastic Optimization with Heavy Tails},
  author={Ashok Cutkosky and Harsh Mehta},
  journal={Advances in Neural Information Processing Systems},
  volume={},
  pages={},
  year={2021}
}


@article{CentralClip,
  title={Algorithms of Robust Stochastic Optimization Based on Mirror Descent Method},
  author={Anatoli Juditsky and Alexander Nazin and Arkadi Nemirovsky and Alexandre Tsybakov},
  journal={Automation and Remote Control},
  volume={80},
  pages={1607--1627},
  year={2019}
}


@article{CentralBddVarClip,
  title={High Probability Bounds for Stochastic Subgradient Schemes with Heavy Tailed Noise},
  author={Daniela A. Parletta and Andrea Paudice and Massimiliano Pontil and Saverio Salzo},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={6},
  pages={953-977},
  year={2024}
}

@article{heavytail4,
  title={On the Heavy-Tailed Theory of Stochastic Gradient Descent for Deep Neural Networks},
  author={Umut Simsekli and Mert Gurbuzbalaban and Thanh Huy Nguyen and Gael Richard and Levent Sagun},
  journal={Arxiv},
  volume={},
  pages={},
  year={2019}
}

@article{CentralBddVarClip1,
  title={High Probability Guarantees for Nonconvex Stochastic Gradient Descent with Heavy Tails},
  author={Shaojie Li and Yong Liu},
  journal={International Conference on Machine Learning},
  volume={},
  pages={},
  year={2022}
}


@article{CentralBddVarClip2,
  title={Stochastic optimization with heavytailed noise via accelerated gradient clipping},
  author={Eduard Gorbunov and Marina Danilova and Alexander Gasnikov},
  journal={Advances in Neural Information Processing Systems},
  volume={},
  pages={},
  year={2020}
}

@article{CentralClip1,
  title={High Probability Convergence of Clipped-SGD Under Heavy-tailed Noise},
  author={Ta Duy Nguyen and Thien Hang Nguyen and Alina Ene and Huy Le Nguyen},
  journal={Arxiv},
  volume={},
  pages={},
  year={2023}
}

@article{CentralClip1plus,
  title={Improved Convergence in High Probability of Clipped Gradient Methods with Heavy Tailed Noise},
  author={Ta Duy Nguyen and Thien Hang Nguyen and Alina Ene and Huy Le Nguyen},
  journal={Advances in Neural Information Processing Systems},
  volume={},
  pages={},
  year={2023}
}

@article{CentralClip2,
  title={Breaking the Lower Bound with (Little) Structure: Acceleration in Non-Convex Stochastic Optimization with Heavy-Tailed Noise},
  author={Zijian Liu and Jiawei Zhang and Zhengyuan Zhou},
  journal={Proceedings of Thirty Sixth Conference on Learning Theory},
  volume={195},
  pages={2266--2290},
  year={2023}
}

@article{CentralClip3,
  title={Parameter-free Regret in High Probability with Heavy Tails},
  author={Jiujia Zhang and Ashok Cutkosky},
  journal={Advances in Neural Information Processing Systems},
  volume={},
  pages={},
  year={2022}
}

@article{CentralClip4,
  title={Breaking the Heavy-Tailed Noise Barrier in Stochastic Optimization Problems},
  author={Nikita Puchkin and Eduard Gorbunov and Nikolay Kutuzov and Alexander Gasnikov},
  journal={AISTATS},
  volume={},
  pages={},
  year={2024}
}

@article{CentralClip5,
  title={High-Probability Complexity Bounds for Non-smooth Stochastic Convex Optimization with Heavy-Tailed Noise},
  author={Eduard Gorbunov and Marina Danilova and Innokentiy Shibaev and Pavel Dvurechensky and Alexander Gasnikov},
  journal={Journal of Optimization Theory and Applications},
  volume={},
  pages={},
  year={2024}
}

@article{CentralClipAdaptive,
  title={Gradient Clipping Improves AdaGrad when the Noise Is Heavy-Tailed},
  author={Savelii Chezhegov and Yaroslav Klyukin and Andrei Semenov and Aleksandr Beznosikov and Alexander Gasnikov and Samuel Horváth and Martin Takac and Eduard Gorbunov},
  journal={Arxiv},
  volume={},
  pages={},
  year={2024}
}

@article{sun,
  title={Distributed Stochastic Strongly Convex Optimization under Heavy-Tailed Noises},
  author={Chao Sun and Bo Chen},
  journal={2024 IEEE International Conference on Cybernetics and Intelligent Systems (CIS) and IEEE International Conference on Robotics, Automation and Mechatronics (RAM)},
  volume={},
  pages={},
  year={2024}
}

@article{errorfeedback,
  title={Smoothed Gradient Clipping and Error Feedback for Decentralized Optimization under Symmetric Heavy-Tailed Noise},
  author={Shuhua Yu and Dusan Jakovetic and Soummya Kar},
  journal={Arxiv},
  volume={},
  pages={},
  year={2024}
}

@article{fatclip,
  title={Taming Fat-Tailed (Heavier-Tailed with Potentially Infinite Variance) Noise in Federated Learning},
  author={Haibo Yang and Peiwen Qiu and Jia Liu},
  journal={Advances in Neural Information Processing Systems},
  volume={},
  pages={},
  year={2022}
}

@article{HighProbAdaGConv,
    author =       "Savelii Chezhegov and Yaroslav Klyukin and ndrei Semenov and Aleksandr Beznosikov and Alexander Gasnikov and Skoltech Samuel Horvath and Martin Takac and Eduard Gorbunov",
    title =        "Gradient Clipping Improves AdaGrad when the Noise Is Heavy-Tailed",
    journal =      "ArXiv",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2024",
}

@article{billion1,
    author =       "Guilherme Moraes Rosa and Luiz Bonifacio and Vitor Jeronymo and Hugo Abonizio and Roberto Lotufo and Rodrigo Nogueira",
    title =        "Billions of Parameters Are Worth More Than In-domain Training Data: A case study in the Legal Case Entailment Task",
    journal =      "ArXiv",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2022",
}

@article{billion2,
    author =       "Zechun Liu and Changsheng Zhao and Forrest Iandola and Chen Lai and Yuandong Tian and Igor Fedorov and Yunyang Xiong and Ernie Chang and Yangyang Shi and Raghuraman Krishnamoorthi and Liangzhen Lai and Vikas Chandra",
    title =        "MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases",
    journal =      "International Conference on Machine Learning",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2024",
}

@article{billion3,
    author =       "Anuroop Sriram and Abhishek Das and Brandon M. Wood and Siddharth Goyal and Lawrence Zitnick",
    title =        "Towards Training Billion Parameter Graph Neural networks for Atomic Simulations",
    journal =      "International Conference on Learning Representations",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2022",
}

@article{billion4,
    author =       "Mostafa Dehghani and Josip Djolonga and Basil Mustafa and Piotr Padlewski and Jonathan Heek and Justin Gilmer and Andreas Peter Steiner and Mathilde Caron and Robert Geirhos and Ibrahim Alabdulmohsin and Rodolphe Jenatton and Lucas Beyer and Michael Tschannen and Anurag Arnab and Xiao Wang and Carlos Riquelme Ruiz and Matthias Minderer and Joan Puigcerver and Utku Evci and Manoj Kumar and Sjoerd Van Steenkiste and Gamaleldin Fathy Elsayed and Aravindh Mahendran and Fisher Yu and Avital Oliver and Fantine Huot and Jasmijn Bastings and Mark Collier and Alexey A. Gritsenko and Vighnesh Birodkar and Cristina Nader Vasconcelos and Yi Tay and Thomas Mensink and Alexander Kolesnikov and Filip Pavetic and Dustin Tran and Thomas Kipf and Mario Lucic and Xiaohua Zhai and Daniel Keysers and Jeremiah J. Harmsen and Neil Houlsby",
    title =        "Scaling Vision Transformers to 22 Billion Parameters",
    journal =      "International Conference on Machine Learning",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2023",
}

@article{scaleability1,
    author =       "Youssef Allouah and Anastasia Koloskova and Aymane El Firdoussi and Martin Jaggi and Rachid Guerraoui",
    title =        "The Privacy Power of Correlated Noise in Decentralized Learning",
    journal =      "International Conference on Machine Learning",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2024",
}


@article{scalability2,
    author =       "Ali Forootani and Raffaele Iervolino",
    title =        "Asynchronous Federated Learning: A Scalable Approach for Decentralized Machine Learning",
    journal =      "ArXiv",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2024",
}

@article{warmup1,
    author =       "Atli Kosson and Bettina Messmer and Martin Jaggi",
    title =        "Analyzing and Reducing the Need for Learning Rate Warmup in GPT Training",
    journal =      "Advances in Neural Information Processing Systems",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2024",
}

@article{warmup2,
    author =       "Dayal Singh Kalra and Maissam Barkeshli",
    title =        "Why Warmup the Learning Rate? Underlying Mechanisms and Improvements",
    journal =      "Advances in Neural Information Processing Systems",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2024",
}


@article{warmup3,
    author =       "Jerry Ma and Denis Yarats",
    title =        "On the Adequacy of Untuned Warmup for Adaptive Optimization",
    journal =      "Association for the Advancement of Artificial Intelligence",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2021",
}

@article{t5,
    author =       "Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu",
    title =        "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
    journal =      "Journal of Machine Learning Research",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2020",
}

@article{IWSLT2017,
    author =       "Mauro Cettolo and Marcello Federico and Luisa Bentivogli and Jan Niehues and Sebastian Stuker and Katsuhito Sudoh and Koichiro Yoshino and Christian Federmann",
    title =        "Overview of the IWSLT 2017 Evaluation Campaign",
    journal =      "Proceedings of the 14th International Conference on Spoken Language Translation",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2017",
}

@article{wmt,
    author =       "Wikimedia Foundation",
    title =        "Shared Task: Machine Translation of News",
    journal =      "Association for Computational Linguistics Conference on Machine Translation",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2019",
}


@article{GLUE,
    author =       "Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman",
    title =        "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    journal =      "International Conference for Learning Representations",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2019",
    DOI =          ""
}

@article{FedMOM,
    author =       "Zhouyuan Huo and Qian Yang and Bin Gu and Lawrence Carin and Heng Huang",
    title =        "Faster On-Device Training Using New Federated Momentum Algorithm",
    journal =      "Association for Computing Machinery",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2020",
    DOI =          ""
}






@article{LEAF,
author = {Sebastian Caldas and Sai Meher Karthik Duddu and Peter Wu and Tian Li and Jakub Konecny and H. Brendan McMahan and Virginia Smith and Ameet Talwalkar},
journal = {Arxiv},
pages = {},
title = {LEAF: A Benchmark for Federated Settings},
volume = {},
year = {2018}
}

@article{RoBERTa,
    author =       "Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov",
    title =        "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
    journal =      "Arxiv",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2019",
    DOI =          ""
}


@article{clippedsgd_online_estimate,
    author =       "Aniket Das and Dheeraj Mysore Nagaraj and Soumyabrata Pal and Arun Suggala and Prateek Varshney",
    title =        "Near-Optimal Streaming Heavy-Tailed Statistical Estimation with Clipped SGD",
    journal =      "Advances in Neural Information Processing Systems",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2024",
}

@article{DeepSeek,
    author =       "DeepSeek-AI",
    title =        "DeepSeek-V3 Technical Report",
    journal =      "ArXiv",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2024",
}

@article{Adagrad_McMahan,
    author =       "Matthew Streeter and Brendan McMahan",
    title =        "Less Regret via Online Conditioning",
    journal =      "ArXiv",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2010",
}


@article{SoTA1,
    author =       "Jiaxiang Li and Xuxing Chen and Shiqian Ma and Mingyi Hong",
    title =        "Problem-Parameter-Free Decentralized Nonconvex Stochastic Optimization",
    journal =      "ArXiv",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2024",
}

@article{SoTA2,
    author =       "Yossi Arjevani and Yair Carmon and John C. Duchi and Dylan J. Foster and Nathan Srebro and Blake Woodworth",
    title =        "Lower bounds for non-convex stochastic optimization",
    journal =      "Mathematical Programming",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2023",
}

@article{SoTA3,
    author =       "Krishna Pillutla and Yassine Laguel and Jerome Malick and Zaid Harchaoui",
    title =        "Federated learning with superquantile aggregation for heterogeneous data",
    journal =      "Machine Learning",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2024",
}


