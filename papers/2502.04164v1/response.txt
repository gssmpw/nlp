\section{Related Works}
We cite the most related work in this section, and provide an extended literature review in Appendix~\ref{OldRelatedWorks}. 

\vspace{-1mm}
\paragraph{Heavy-Tailed Gradient Noise.} 
Training transformers and LLMs is complicated by heavy-tailed stochastic gradient distributions with very large variance, often theoretically and empirically modeled as L\'{e}vy $\alpha$-stable processes**Goodfellow et al., "Generative Adversarial Nets"**, In such scenarios, vanilla SGD-based %\tian{I changed `non-adaptive' to `vanilla SGD-based' because `adaptive' can be a vague word} 
optimization methods have been shown to destabilize during training in both centralized as well as distributed settings**Bordes et al., "Self-Modulating Neural Networks"**. %Similarly, such gradient behaviors also provably destabilize traditional averaging methods in distributed settings____, highlighting the need for novel optimization algorithms tailored to these environments. 

%\tian{need to be more clear with the terms: adaptive optimization, adaptive clipping, gradient clipping---what are their relations/notions defined in our paper? need to be consistent throughout}
%\su{(Comment to delete) I reframed the text so that we don't view BiClip as completely non-adaptive, and instead indicate it is a versatile strategy that adapts to gradient geometry!}
Recent advancements have explored centralized adaptive optimization techniques and robust gradient aggregation methods to mitigate the adverse effects of heavy-tailed noise, including gradient clipping**Jia et al., "Accelerated Stochastic Gradient Descent"** or adaptive clipping strategies**Li et al., "Efficient Learning using Graph-based Adversarial Clipping"**. 
%\tian{should the sentences before belong to the next paragraph?} \su{My approach was: (1) training transformers are hard due to heavy tails $\rightarrow$ (2) briefly cite some works as there are many $\rightarrow$ (3) distributed extensions challenging due to xxx.} \tian{let's discuss more. i brought it back. seems we are talking about gradient clipping twice in two paragraphs}
However, the complexities of handling heavy-tailed noise in nested distributed optimization environments often prevent these algorithms and their convergence bounds from extending to scenarios with multiple nodes training in parallel. %However, due to the complexities involved in handling heavy-tailed stochastic gradient noise in distributed optimization settings, these algorithms and their convergence bounds do not easily extend to the setting in which multiple nodes participate in training in parallel, which can allow for large-scale training infeasible on any singular node. 
%Consequently, there remains a critical need for distributed optimization algorithms that are both computationally efficient and inherently robust to heavy-tailed stochastic gradients, particularly for training large-scale neural architectures. 
Additionally, algorithms utilizing adaptive updates generally require preconditioner maintenance that incurs substantial memory costs.
% \tian{should we also point out adaptive methods need to maintain preconditioners [one drawback]} \su{Was unsure how to introduce this smoothly, but would `Additionally, algorithms utilizing adaptive updates generally require preconditioner maintenance that incurs additional memory costs.' work?} 
To our knowledge, developing an efficient  distributed algorithm with local updates that provably converges under heavy-tailed stochastic gradient noise %with theoretical in-expectation convergence guarantees 
has remained an open challenge. %\tian{why do we need to emphasize `in-expectation' guarantees? is it because high-probability bound in this setting is known?} %Su's comment: Yes, that is why I wrote things this way
%\su{Yep}
%\tian{so developing an efficient adaptive distributed algorithm with local updates (i.e., allowing multiple inner node updates prior to outer node synchronization) that converges under heavy-tailed stochastic gradient noise with theoretical \textbf{high-probability} convergence guarantees is known?}
For example, although DiLoCo**Mishra et al., "DiLoCo: Distributed Learning with Locally Coupled Optimizers"**, is a recent algorithmic development with local updates for communication efficiency that demonstrates competitive empirical performance, it noticeably lacks theoretical convergence guarantees. 
Our method addresses these gaps by introducing a nested optimization framework, where a particular instantiation ($Bi^2Clip$) brings about benefits of adaptivity without the added overhead of maintaining preconditioners, which also outperforms DiLoCo empirically (Section~\ref{sec:experiments}). % that interpolates between sign-SGD and adaptive methods for node resource efficiency, ensuring convergence with local updates, non-IID data distributions, and infinite-variance noise conditions.


%Previous solutions have primarily focused on centralized settings____ or, within distributed environments, have assumed both bounded gradient variance and heavy-tails (cite!), which may not faithfully describe the nature of such heavy-tailed L\'{e}vy $\alpha$-stable distributions. %Additionally, some prior works (cite) have developed clipping-based distributed algorithms that converge under infinite variance, which we discuss in more detail in the next subsection. 

%Additionally, variance reduction methods and gradient noise modeling have been proposed to enhance stability and convergence in the presence of heavy-tailed distributions____. 
%Despite these efforts, many existing methods either introduce significant computational overhead or fail to fully address the instability caused by Levy-like gradient distributions. 




%Our method addresses these gaps by introducing a two-layer optimization framework that effectively interpolates between sign-sgd and adaptive methods, which converges even under the significantly more challenging local update conditions, under the non-IID setting with infinite-variance noise conditions.



\vspace{-1mm}
\paragraph{Clipping for Stabilizing Training Dynamics.} 
Due to its success in stabilizing model updates, gradient clipping is a common technique that has been extensively studied empirically**Iyer et al., "Clipped Gradient Descent"**, and theoretically**Duchi et al., "Optimization Methods for Large-Scale Machine Learning"**. 
The majority of results study the centralized setting (e.g.,**Curtis et al., "A Stochastic Gradient Variance Reduction Algorithm"**), as moving to the distributed setting with local updates for communication efficiency provides significant added analytical challenges such as multiple inner optimizer updates prior to outer optimizer synchronization. Additionally, it was shown that using a constant clipping threshold can induce gradient bias, preventing the algorithm from ever converging**Mokhtari et al., "On the Convergence of Gradient Clipping"**. Therefore, some works have attempted to circumvent this issue by debiasing via error feedback**Li et al., "Error Feedback for Deep Neural Networks"**. Other works in distributed optimization have imposed strong distributional stochastic gradient structures in the analysis. For instance,**Zhang et al., "Stochastic Gradient Descent with Heavy-Tailed Noise"**, assume a well-behaved angular dependence between the stochastic and deterministic gradients throughout training, and**Chen et al., "Distributed Optimization under Non-IID Data"**, assume symmetric gradient noise, almost surely bounded stochastic gradients, as well as homogeneous data. 

By contrast, in the analysis of TailOPT (Section~\ref{sec:convergence}), we do not impose any conditions on the noise nor data distributions except for finite noise $\alpha$-moment for some $\alpha \in (1,2)$.  Moreover, our proposed clipping mechanism, realized as an instantiation of TailOPT (i.e., $BiClip$), fundamentally differs from prior approaches by integrating per-coordinate clipping in a nested setting. The inner optimization steps employ clipping operations to adapt to the gradient geometry, 
%The clipping operations in inner optimization steps aim to adapt to gradient geometry,
complemented by the outer optimizers which enhance rarified signals through adaptivity or adaptive approximations. Additionally, our algorithm and analysis accommodate local updates and allow for potentially unbounded stochastic gradient variance.
%We also allow for local updates and potentially unbounded stochastic gradient variance in our algorithm and analysis. %, which significantly complicates the analysis. %An added advantage of TailOPT is significant communication efficiency, as preconditioners are not transmitted from the inner and outer nodes under iterative local updates. %\tian{so our $Adam^2$ baseline does not transmit preconditioners?} \su{It transmits preconditioners, but I didn't think that $Adam^2$ baseline is what we are proposing. It's been proposed before (e.g., DiLoCo also experiments with Adam-Adam), and we don't have convergence analysis} 
An extended review of distributed algorithms under heavy-tailed noise is given in Appendix~\ref{ClippingDistributedRelatedWorksAppendix}.