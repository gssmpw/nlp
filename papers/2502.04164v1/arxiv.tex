\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
%\usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%\usepackage[final]{neurips_arxiv}
%\usepackage{biblatex}
%\addbibresource{references.bib}
%\usepackage{natbib}

\usepackage{geometry}
\setlength\parindent{0pt}

\geometry{
a4paper,
total={180mm,270mm},
%total={180mm,250mm}
%left=20mm,
%top=20mm,
}

\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}   
\usepackage{fullpage}
% \usepackage[numbers]{natbib}

\date{}

\setlength{\parskip}{\baselineskip}%
\setlength{\parindent}{0pt}%

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{xspace}         % Might have been added manually?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{etoc}
\usepackage{amssymb}        % Su added these manually
\usepackage{graphicx}
\usepackage[normalem]{ulem}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath}
%\bibliographystyle{unsrt}
\DeclareMathOperator*{\argmin}{arg\,min}
% \usepackage{natbib}
% \PassOptionsToPackage{numbers, compress}{natbib}
\PassOptionsToPackage{numbers,sort,compress}{natbib}
\usepackage{natbib}
% \usepackage[numbers,sort,compress]{natbib}

%\bibliographystyle{plainnat}

\usepackage{tcolorbox}

\usepackage{algorithm} % For the algorithm environment
\usepackage{algorithmic} % For pseudocode
\usepackage{xcolor} % for colored text
\usepackage{amsthm} % For theorem environments below
\usepackage{booktabs} % For formal tables
\usepackage{multirow}
\usepackage{float}

\newtheorem{definition}{Definition}
\newtheorem{problem}{Problem}
\newtheorem{assumption}{Assumption}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem*{theorem*}{Theorem}
\newtheorem{remark}{Remark}
\newtheorem{lemma}{Lemma}
\newtheorem*{lemma*}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem*{corollary*}{Corollary}

% Define custom colors {0.0, 0.0, 0.75}
\definecolor{DarkBlue}{rgb}{0.17, 0.34, 0.80}  % Dark blue 
\definecolor{DarkerBlue}{rgb}{0.5, 0.0, 0.75}  % Darker blue 
\definecolor{MediumBlue}{rgb}{0.0, 0.99, 0.75} % Medium blue
\definecolor{LightBlue}{rgb}{0.68, 0.85, 0.9} % Lighter blue
\definecolor{LighterBlue}{cmyk}{0.4, 0.2, 0, 0} % Custom lighter blue


\newcommand{\name}{{\texttt{FedAda$^2$}}\xspace}

\newcommand\tian[1]{{\color{cyan}\textbf{TL: }{#1} }}

\usepackage[toc,page,header]{appendix}
\usepackage{minitoc}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Efficient Distributed Optimization under Heavy-Tailed Noise}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
   Su Hyeong Lee$^{1}$ 
   \and
   Manzil Zaheer$^{2}$ 
  \and
  Tian Li$^{1}$ 
}

% % Make the "Part I" text invisible
% \renewcommand \thepart{}
% \renewcommand \partname{}

% \doparttoc % Tell to minitoc to generate a toc for the parts
% \faketableofcontents % Run a fake tableofcontents command for the partocs
% \part{} % Start the document part
% %\parttoc % Insert the document TOC

\usepackage{etoc}
\etocdepthtag.toc{mtchapter}
\etocsettagdepth{mtchapter}{subsection}
\etocsettagdepth{mtappendix}{none}

\begin{document}

\maketitle


\renewcommand\thefootnote{$^1$}\footnotetext{University of Chicago, \texttt{sulee@uchicago.edu}}
\renewcommand\thefootnote{$^2$}\footnotetext{Google DeepMind,  \texttt{manzilzaheer@google.com}}
\renewcommand\thefootnote{$^1$}\footnotetext{University of Chicago,  \texttt{litian@uchicago.edu}}


\begin{abstract}
Distributed optimization has become the default training paradigm in modern machine learning due to the growing scale of models and datasets. To mitigate communication overhead, local updates are often applied before global aggregation, resulting in a nested optimization approach with inner and outer steps. However, heavy-tailed stochastic gradient noise remains a significant challenge, particularly in attention-based models, hindering effective training. In this work, we propose TailOPT, an efficient framework designed to address heavy-tailed noise by leveraging adaptive optimization or clipping techniques. We establish convergence guarantees for the TailOPT framework under heavy-tailed noise with potentially unbounded gradient variance and local updates. 
Among its variants, we highlight a memory and communication efficient instantiation which we call $Bi^2Clip$,  which performs coordinate-wise clipping at both the inner and outer optimizers, achieving adaptive-like performance (e.g., Adam) without the cost of maintaining or transmitting additional gradient statistics. Empirically, TailOPT, including $Bi^2Clip$, demonstrates superior performance on several language tasks and models, outperforming state-of-the-art methods.
\end{abstract}


\section{Introduction}\label{introduction}

The training of deep learning models including large language models (LLMs) has become increasingly resource-intensive, driven by expansive datasets and models with billions of parameters~\cite{billion1,billion2,billion3,billion4}. As the computational demands escalate, %the traditional centralized training paradigms confront significant scalability and memory challenges, rendering them prohibitively expensive for many organizations~\cite{scaleability1,scalability2}. To navigate these constraints, 
distributed learning has emerged as the default approach, enabling the parallel activation of training processes across multiple compute nodes such as GPUs or datacenters. However, this paradigm introduces a new bottleneck of communication overhead, especially as the progress in compute power has outpaced that of network infrastructure~\cite{wu2023rethinking,DeepSeek}. %More specifically, the rapid advancement of accelerators like GPUs and TPUs has outpaced the development of network infrastructures, leading to scenarios where the interconnect bandwidth cannot keep pace with advancing compute capabilities~\cite{wu2023rethinking}. 
%For instance, while the latest generation of TPUs\footnote{\url{https://cloud.google.com/blog/products/compute/introducing-trillium-6th-gen-tpus}} has seen a 4.7x increase in compute performance, interconnect bandwidth has only grown by less than 2x, underscoring the widening gap between computation and communication speeds. Consequently, the efficiency of large-scale training is increasingly hampered by the latency and bandwidth limitations inherent in data center environments. %\tian{can we shorten this paragraph}

To mitigate these communication challenges, one promising strategy is the utilization of local updates. By allowing each compute node to perform multiple gradient updates locally before aggregation, the frequency and volume of inter-node communication can be significantly reduced~\cite{smith2018cocoa,stich2018local,mcmahan2017communication,EfficientAdaptiveFederatedOptimization,DiLoCoAsynchronous,OpenDiLoCo}. For instance, the recent DiLoCo algorithm for training LLMs in datacenter environments can apply around $500$ local gradient updates prior to aggregation to relieve communication costs~\cite{DiLoCo}. This approach naturally formulates a \textbf{nested} optimization problem, where \textit{inner} optimization occurs within each compute node, and \textit{outer} optimization is orchestrated by the coordinating node(s). %Previous studies have demonstrated the potential of such frameworks in enhancing training efficiency and scalability~\cite{EfficientAdaptiveFederatedOptimization,DiLoCoAsynchronous,OpenDiLoCo}.


However, training attention-based models like LLMs introduce an additional challenge due to the properties of their stochastic gradient distributions. Empirical and theoretical investigations have consistently demonstrated that the gradient noise in these models follows a heavy-tailed distribution%(e.g., modeled as L\'{e}vy $\alpha$-stable processes)
~\cite{linearattentionisallyouneed,heavytail1,heavytail2,heavytail3,heavytailedclassimbalance,inprobsupreme}. This heavy-tailed behavior, characterized by high or infinite variance and potentially very large deviations, poses significant challenges to the stability and convergence of existing optimization algorithms~\cite{EfficientAdaptiveFederatedOptimization,HeavyTailedNoisePaper}. Addressing these challenges necessitates the development of novel optimization strategies and a more principled understanding of their theoretical underpinnings.
%, necessitating the development as well as more principled understandings of novel optimization strategies. %Additionally,  heavy-tailed algorithmic convergence guarantees in the centralized setting (e.g.,~\cite{CentralClip,CentralClip1}) do not readily generalize to distributed setups due to the added complexities of multiple participating nodes and local updates. \tian{has BiClip been proposed and analyzed in centralized training?} \tian{this sentence sounds like our contribution is to analyze an existing algorithm in the distributed setting}
%For instance,~\cite{EfficientAdaptiveFederatedOptimization} shows that SGD under the presence of heavy-tailed noise attains infinite regret in expectation and greatly destabilizes training dynamics, which also forms the motivation behind adaptive frameworks such as DiLoCo~\cite{DiLoCo}.
% \tian{add a sentence or two saying why previous works on heavy-tailed noise cannot solve the issue yet} \su{I tried to write about this in the `Recent Advancements' paragraph in page 2. But to add a sentence here, I think this might work: `Additionally,  heavy-tailed algorithmic convergence guarantees in the centralized setting (e.g.,~\cite{CentralClip,CentralClip1}) do not readily generalize to distributed setups due to the added complexities of multiple participating nodes and local updates.'}

In this work, we propose TailOPT, an efficient and theoretically principled nested training framework, designed to address the challenges posed by heavy-tailed gradient noise in distributed training with local updates. TailOPT introduces several key strategies, including clipping mechanisms (such as coordinate-wise or $L_2$-clipping) and adaptivity, applied at both inner and outer optimizers, to mitigate the adverse effects of heavy-tailed noise. We note that the preconditioning step in adaptive optimizers (e.g.,~\cite{Adagrad_McMahan}) may be viewed as a  form of soft clipping.
%that the step of preconditioning in many adaptive optimizers can conceptually be viewed as soft clipping (e.g.,~\cite{AdaGrad}).
We analyze the convergence of TailOPT while incorporating such adaptive methods, while allowing for heavy-tailed noise with unbounded variance. Among the various instantiations of the TailOPT framework, we highlight $Bi^2Clip$, a particularly scalable method that applies coordinate-wise clipping to gradients during inner iterations, and to model parameter updates at outer communication rounds, enforcing thresholding from both above and below on a per-coordinate basis.
%Among various instantiations of the TailOPT framework, we highlight a particularly scalable method, $Bi^2Clip$, that performs coordinate-wise gradient clipping at each inner each inner iteration as well as model updates at each outer communication round, from the above and below, on a per-coordinate basis.   
Our empirical and theoretical results demonstrate that TailOPT is strongly effective in mollifying heavy-tailed noise, enhancing the stability and convergence of the training dynamics across several language benchmarks as well as synthetic data. 

Our contributions may be summarized as follows.
% \setlength\itemsep{0em}
\begin{itemize}
    \item We introduce TailOPT, a general  distributed training framework for large-scale models under communication-efficient local updates and heavy-tailed gradient distributions. Among its instantiations, we highlight $Bi^2Clip$, which adjusts to gradient geometry similar to adaptive optimizers  (e.g., Adam~\cite{ADAM})  while avoiding additional memory and communication overhead for maintaining or transmitting preconditioners. 
    
    \item We provide convergence guarantees for a class of TailOPT algorithms that leverage adaptive optimizers and various clipping strategies, effectively addressing heavy-tailed noise with potentially infinite variance. This is achieved using a nested optimization framework, where the inner optimizer employs clipping operations to mitigate heavy-tailed gradient noise, while the outer optimizer utilizes either fully adaptive or %computationally 
    efficient approximations of adaptive updates to guide the optimization process.

    \item We validate the practicality and effectiveness of TailOPT through extensive experiments on synthetic and real-world datasets in large-scale settings. Our experiments demonstrate that TailOPT produces several algorithmic instantiations that consistently outperform state-of-the-art baselines while being more efficient. 

\end{itemize}

% \vspace{-10pt}



\section{Related Works}

We cite the most related work in this section, and provide an extended literature review in Appendix~\ref{OldRelatedWorks}. 

\vspace{-1mm}
\paragraph{Heavy-Tailed Gradient Noise.} 
Training transformers and LLMs is complicated by heavy-tailed stochastic gradient distributions with very large variance, often theoretically and empirically modeled as L\'{e}vy $\alpha$-stable processes~\cite{linearattentionisallyouneed,heavytail1,heavytail2,heavytail3,inprobsupreme,heavytailedclassimbalance,HighProbAdaGConv}. In such scenarios, vanilla SGD-based %\tian{I changed `non-adaptive' to `vanilla SGD-based' because `adaptive' can be a vague word} 
optimization methods have been shown to destabilize during training in both centralized as well as distributed settings~\cite{EfficientAdaptiveFederatedOptimization,inprobsupreme,HeavyTailedNoisePaper}. %Similarly, such gradient behaviors also provably destabilize traditional averaging methods in distributed settings~\cite{EfficientAdaptiveFederatedOptimization}, highlighting the need for novel optimization algorithms tailored to these environments. 

%\tian{need to be more clear with the terms: adaptive optimization, adaptive clipping, gradient clipping---what are their relations/notions defined in our paper? need to be consistent throughout}
%\su{(Comment to delete) I reframed the text so that we don't view BiClip as completely non-adaptive, and instead indicate it is a versatile strategy that adapts to gradient geometry!}
Recent advancements have explored centralized adaptive optimization techniques and robust gradient aggregation methods to mitigate the adverse effects of heavy-tailed noise, including gradient clipping~\cite{heavytail2, CentralClip, CentralClip5, highprobabilityunboundedvariance, highprobClip, CentralClip1} or adaptive clipping strategies~\cite{HighProbAdaGConv}. 
%\tian{should the sentences before belong to the next paragraph?} \su{My approach was: (1) training transformers are hard due to heavy tails $\rightarrow$ (2) briefly cite some works as there are many $\rightarrow$ (3) distributed extensions challenging due to xxx.} \tian{let's discuss more. i brought it back. seems we are talking about gradient clipping twice in two paragraphs}
However, the complexities of handling heavy-tailed noise in nested distributed optimization environments often prevent these algorithms and their convergence bounds from extending to scenarios with multiple nodes training in parallel. %However, due to the complexities involved in handling heavy-tailed stochastic gradient noise in distributed optimization settings, these algorithms and their convergence bounds do not easily extend to the setting in which multiple nodes participate in training in parallel, which can allow for large-scale training infeasible on any singular node. 
%Consequently, there remains a critical need for distributed optimization algorithms that are both computationally efficient and inherently robust to heavy-tailed stochastic gradients, particularly for training large-scale neural architectures. 
Additionally, algorithms utilizing adaptive updates generally require preconditioner maintenance that incurs substantial memory costs.
% \tian{should we also point out adaptive methods need to maintain preconditioners [one drawback]} \su{Was unsure how to introduce this smoothly, but would `Additionally, algorithms utilizing adaptive updates generally require preconditioner maintenance that incurs additional memory costs.' work?} 
To our knowledge, developing an efficient  distributed algorithm with local updates that provably converges under heavy-tailed stochastic gradient noise %with theoretical in-expectation convergence guarantees 
has remained an open challenge. %\tian{why do we need to emphasize `in-expectation' guarantees? is it because high-probability bound in this setting is known?} %Su's comment: Yes, that is why I wrote things this way
%\su{Yep}
%\tian{so developing an efficient adaptive distributed algorithm with local updates (i.e., allowing multiple inner node updates prior to outer node synchronization) that converges under heavy-tailed stochastic gradient noise with theoretical \textbf{high-probability} convergence guarantees is known?}
For example, although DiLoCo~\cite{DiLoCoAsynchronous,OpenDiLoCo,DiLoCo} is a recent algorithmic development with local updates for communication efficiency that demonstrates competitive empirical performance, it noticeably lacks theoretical convergence guarantees. 
Our method addresses these gaps by introducing a nested optimization framework, where a particular instantiation ($Bi^2Clip$) brings about benefits of adaptivity without the added overhead of maintaining preconditioners, which also outperforms DiLoCo empirically (Section~\ref{sec:experiments}). % that interpolates between sign-SGD and adaptive methods for node resource efficiency, ensuring convergence with local updates, non-IID data distributions, and infinite-variance noise conditions.


%Previous solutions have primarily focused on centralized settings~\cite{heavytail2, CentralClip, CentralClip5, highprobabilityunboundedvariance, highprobClip, CentralClip1} or, within distributed environments, have assumed both bounded gradient variance and heavy-tails (cite!), which may not faithfully describe the nature of such heavy-tailed L\'{e}vy $\alpha$-stable distributions. %Additionally, some prior works (cite) have developed clipping-based distributed algorithms that converge under infinite variance, which we discuss in more detail in the next subsection. 

%Additionally, variance reduction methods and gradient noise modeling have been proposed to enhance stability and convergence in the presence of heavy-tailed distributions~\cite{HeavyTailedNoiseModel, VarianceReductionHeavyTail}. 
%Despite these efforts, many existing methods either introduce significant computational overhead or fail to fully address the instability caused by Levy-like gradient distributions. 




%Our method addresses these gaps by introducing a two-layer optimization framework that effectively interpolates between sign-sgd and adaptive methods, which converges even under the significantly more challenging local update conditions, under the non-IID setting with infinite-variance noise conditions.



\vspace{-1mm}
\paragraph{Clipping for Stabilizing Training Dynamics.} 
Due to its success in stabilizing model updates, gradient clipping is a common technique that has been extensively studied empirically~\cite{empirical1,empirical2,empirical3,empirical5} and theoretically~\cite{inprobsupreme,HeavyTailedNoisePaper,HighProbAdaGConv,highprobClip,theoretical1,theoretical4,GradClipCentralGeomPerspective,RevisitGradClipModern,clippedsgd_online_estimate}. 
The majority of results study the centralized setting (e.g.,~\cite{CentralClip5,CentralClip4,CentralClip3,CentralClip2,CentralClip1plus,CentralBddVarClip, CentralBddVarClip1}), as moving to the distributed setting with local updates for communication efficiency provides significant added analytical challenges such as multiple inner optimizer updates prior to outer optimizer synchronization. Additionally, it was shown that using a constant clipping threshold can induce gradient bias, preventing the algorithm from ever converging~\cite{GradClipCentralGeomPerspective,RevisitGradClipModern}. Therefore, some works have attempted to circumvent this issue by debiasing via error feedback~\cite{Debias1,Debias2}. Other works in distributed optimization have imposed strong distributional stochastic gradient structures in the analysis. For instance,~\cite{angulardependencerestrictive}
assume a well-behaved angular dependence between the stochastic and deterministic gradients throughout training, and~\cite{symmetricnoise_distributed} assume symmetric gradient noise, almost surely bounded stochastic gradients, as well as homogeneous data. 

By contrast, in the analysis of TailOPT (Section~\ref{sec:convergence}), we do not impose any conditions on the noise nor data distributions except for finite noise $\alpha$-moment for some $\alpha \in (1,2)$.  Moreover, our proposed clipping mechanism, realized as an instantiation of TailOPT (i.e., $BiClip$), fundamentally differs from prior approaches by integrating per-coordinate clipping in a nested setting. The inner optimization steps employ clipping operations to adapt to the gradient geometry, 
%The clipping operations in inner optimization steps aim to adapt to gradient geometry,
complemented by the outer optimizers which enhance rarified signals through adaptivity or adaptive approximations. Additionally, our algorithm and analysis accommodate local updates and allow for potentially unbounded stochastic gradient variance.
%We also allow for local updates and potentially unbounded stochastic gradient variance in our algorithm and analysis. %, which significantly complicates the analysis. %An added advantage of TailOPT is significant communication efficiency, as preconditioners are not transmitted from the inner and outer nodes under iterative local updates. %\tian{so our $Adam^2$ baseline does not transmit preconditioners?} \su{It transmits preconditioners, but I didn't think that $Adam^2$ baseline is what we are proposing. It's been proposed before (e.g., DiLoCo also experiments with Adam-Adam), and we don't have convergence analysis} 
An extended review of distributed algorithms under heavy-tailed noise is given in Appendix~\ref{ClippingDistributedRelatedWorksAppendix}.

\section{Problem Formulation}\label{problemformulation}
In distributed optimization, the global objective is constructed by taking a weighted average over the local node objectives $F_i(x)$ for model parameters $x \in \mathbb{R}^d$ and node $i$. In scenarios where data sizes at each node are unbalanced or sampling probabilities vary, the objective becomes:
\vspace{-2mm}
\begin{equation}\label{unbalancedcase}
\vspace{-2mm}
    F(x) = \sum_{i=0}^{N-1} p_i F_i(x),
\end{equation}
where $p_i$ is proportional to the local data size of node $i$. Here, $F_i(x)$ is defined as $\mathbb{E}_{\xi \sim \mathcal{D}_{i}}\left[F_i(x, \xi)\right]$, where $F_i(x, \xi) = F_i(x) + \langle \xi, x \rangle$ 
represents the stochastic local objective, and $\mathcal{D}_{i}$ is the noise distribution of node $i$. This term comes from integrating the gradient noise model % $\nabla F_i(x, \xi) = \nabla F_i(x) + \xi$. 
%We decouple $F_i(x, \xi)$ into $F_i(x) + \langle \xi, x \rangle$ for notational convenience \tian{is `for notational convenience' correct? because i am trying to explain why we have this term which does not look common at first sight} so that we can represent the stochastic gradient as full gradient plus a random noise variable $\xi$, i.e., 
$\nabla F_i(x^{t}_{i},\xi^{t}_{i}) = \nabla F(x^{t}_{i}) + \xi^{t}_{i}$, where $x^{t}_{i}$, $\xi^{t}_{i}$ are the parameter weights and gradient noise of node $i$ at timestep $t$. In our formulation and theoretical analysis (Section~\ref{sec:convergence}), we allow for both independent and identically distributed (IID) data across $N$ nodes, as commonly observed in datacenter environments, as well as more challenging non-IID data distributions. % and  model contemporary, large-scale neural network training across multiple powerful datacenters or GPU clusters, in which data can typically be preprocessed IID to optimize for training. However, for fullest generality, we conduct our theoretical analysis in the more challenging, non-IID setting, while employing an IID split for our empirical evaluation to realistically model datacenter environments. 
We now present the assumptions used in the convergence analysis. 
%
\begin{assumption}[$L$-smoothness] \label{assum:smoothness}
    For all $x, y \in \mathcal{X}$ and $i \in [N]$, the local objectives $F_i(x)$ satisfy $F_i(x) \le F_i(y) + \langle x-y, \nabla F_i(y) \rangle + L_i \|x-y\|^2/2$.
\end{assumption}
%
\begin{assumption}[Bounded $\alpha$-moment] \label{assum:bounded_moment}
    For all nodes $i \in [N]$ with noise distribution $\mathcal{D}_i$, there exists $\alpha_i \in (1,2)$, $B_i>0$ such that $\mathbb{E}[\|\xi_i\|^{\alpha_i}] < B_i^{\alpha_i}$. %\tian{I changed `client' to node. Need to double check other places.} \su{I have completed a double sweep for this in the main text}
    \vspace{-2mm}
\end{assumption}
%
Assumption 2 expresses that the noise distribution can be heavy-tailed. In particular, we note that the variance of the noise can be infinite ($\alpha_i = 2$), a setting in which distributed SGD was shown to fail to converge, both empirically and theoretically~\cite{EfficientAdaptiveFederatedOptimization,fatclip} %\tian{maybe add another citation here (to hide our identities :)}. \su{I added another citation that explores this empirically to make us more anonymous! Although there are several works that aim to optimize over heavy tails, I haven't been able to locate one that actually shows that this is a problem theoretically for distributed settings. They mostly cite works for the central setting as motivation. }
This condition on the $\alpha_i$ 
%\tian{should be $\alpha_i$?} 
%\su{Yep}
is `optimally weakest', in that sending $\alpha_i \to 1^+$ recovers the integrability condition of the noise, the minimal assumption necessary to form expectations. Furthermore, we note that $\mathbb{E}\|\xi\|^\alpha < \infty \implies \mathbb{E}\|\xi\|^\beta < \infty$ for $\forall \beta < \alpha$, $\alpha \in \mathbb{R}$. Therefore, we let $\alpha:=\min_{i \in [N]} \alpha_i \in (1,2)$ in the proceeding analysis for notational convenience.

We note that this is strictly weaker than a conventional heavy-tailed assumption on the stochastic gradients, which is commonly given (e.g.,~\cite{HeavyTailedNoisePaper}) as 
\begin{equation*}
\mathbb{E}[\|\nabla F_i\left(x^t_i, \xi^t_i\right)\|^{\alpha_i}] < B_i^{\alpha_i},
\end{equation*}
which implies that $\nabla F_i\left(x^t_i\right)$ is bounded. By contrast, this cannot be implied by Assumption~\ref{assum:bounded_moment}. %, which is the setting in which our convergence proofs for $L_2Clip$-SGD (Appendix~\ref{AppendixSectionforConvexConvergenceCrossSilo}) are presented. %\tian{this method has not been introduced before}  
%For the convergence proofs for algorithmic frameworks involving $BiClip$ however, we assume that model weights are realized in a bounded domain $\mathcal{X}$ during training, under which the above assumptions are equivalent with standard finite heavy-tailed stochastic gradient $\alpha$-moments. \tian{this is too technically detailed, and i deleted it. if reviewer ask, we can explain}
%
%Various prior works indicate that the training of attention models can follow an $\alpha$-stable L\'{e}vy distribution~\citep[e.g.,][]{HeavyTailedNoisePaper,HighProbAdaGConv,heavytail2}, which has infinite variance. 
We also note that some works in the literature also define heavy-tailed distributions with \textit{bounded} variance when establishing algorithm convergence bounds (e.g.,~\cite{inprobsupreme,clippedsgd_online_estimate,CentralBddVarClip, CentralBddVarClip1}), which differs from our definition. We carry out our convergence proofs which subsumes the more general infinite variance setting, which naturally implies convergence under bounded stochastic gradients or variance. 


%And finally, Assumption 4 is naturally motivated by neural networks being a finite composition of affine functions and piece-wise differentiable activations, thus we formally include it as an assumption. 

%Is bounded gradients. 
%non-degenerate, i.e., the gradient updates are not perfectly annihilated by infinite noise distributions. In addition, we also assume that the noise distribution is centered at $0$, $\mathbb{E}[\xi]=0$, a standard assumption. To our knowledge, these gradient noise conditions are weaker than all previous assumptions such as bounded gradients, or even bounded-moment heavy-tails (e.g., $\mathbb{E}\|\xi\|^\alpha \le \sigma^\alpha$ for $\alpha \in [1,2)$) utilized in prior convergence proofs in the literature. 


\section{TailOPT: An Efficient Heavy-Tailed Optimization Framework}\label{BiClipSecton}
In this section, we begin by motivating the Heavy-Tailed Optimization Framework (TailOPT), a scalable training setup for heavy-tailed learning. %We aim to devise an algorithm which is robust to significantly weaker non-IID distributional assumptions on the stochastic noise, including heavy-tails with infinite variance or with distribution shifts. 
SGD is a strong candidate given its simplicity and efficiency, but it has been shown to diverge under heavy-tailed noise in both centralized~\cite{HeavyTailedNoisePaper} and distributed settings~\cite{EfficientAdaptiveFederatedOptimization}. Therefore, modifications are necessary to stabilize the noised updates.

Gradient clipping is a widely adopted technique to modulate model updates by mitigating the impact of large gradients~\cite{theoretical1,theoretical4,GradClipCentralGeomPerspective,RevisitGradClipModern,fatclip}. Typically, the $Clip(\cdot)$ operator rescales the gradient uniformly to ensure it remains below a predefined threshold. This procedure is mathematically equivalent to applying a dynamically adjusted, lower learning rate when large stochastic gradients are encountered. Therefore, we first include the usage of $L_2$ clipping ($L_2Clip$) in TailOPT to stabilize noised updates from heavy-tailed stochastic noise. More specifically, $L_2$ clipping  is deployed on the gradients prior to standard gradient descent updates on each node, while a global model weight projection strategy is utilized on the outer optimizer after synchronizing all the collected updates. For additional clarity, the precise pseudocode is given as Algorithm~\ref{L2ClipSGD} in Appendix~\ref{AppendixSectionforConvexConvergenceCrossSilo}. 

%In Appendix ~\ref{AppendixSectionforConvexConvergenceCrossSilo}, Theorem~\ref{ConvexConvergenceThmAppendixVer}, we formally show as an intermediary result that this projection-$L_2$ clipping strategy is able to converge in expectation under heavy-tailed noise.

% Theorem~\ref{ConvexConvergenceThmAppendixVer} (Appendix~\ref{AppendixSectionforConvexConvergenceCrossSilo}) demonstrates  its convergence in expectation under heavy-tailed noise.%, despite potential clipping-induced bias. 

\vspace{-2mm}
\paragraph{Interpolating Adaptivity: $BiClip$.} However, previous works on  $L_2$ clipping of gradients or model updates (e.g.,~\cite{fatclip}) do not adapt to gradient geometry, as they proportionally and uniformly downscale each gradient coordinate. Therefore, smaller signals become even more difficult to detect and propagate. Adaptive optimizers have consistently demonstrated superior performance for training modern architectures~\cite{EfficientAdaptiveFederatedOptimization,HeavyTailedNoisePaper,AdaptiveFederatedOptimization}. Key among adaptive methods such as Adam~\cite{ADAM} and Adagrad~\cite{Adagrad_McMahan,AdaGrad} is the use of preconditioning, where preconditioners that are derived from historical gradients can help to procure an effective per-coordinate learning rate. % with exponentially decaying memory--ensures stability. 
This process dynamically modulates model updates: rare gradient coordinates are amplified, while uninformative gradients are scaled down, speeding up the convergence. The trade-off, however, lies in the increased systems requirements to maintain preconditioners. For instance, deploying Adam can instantly triple the memory demand to host model parameters during minibatch backpropagation, due to the inclusion of first and second moment exponentially decaying moving average statistics compared to vanilla SGD. %gradients are preconditioned by their statistics such as exponentially decaying second moment moving average. 

To take advantage of adaptivity without incurring additional memory or communication overhead, we propose a new clipping mechanism, $BiClip$, that performs coordinate-wise clipping from both above and below. $BiClip$ is motivated by an interpolation between clipped-SGD and adaptive methods, employing a stabilizing absolute-value clipping mechanism that modulates model updates while eliminating the overhead of preconditioner maintenance.
% To take advantage of adaptivity without increasing memory or communication overhead, we propose a new clipping mechanism, $BiClip$, that performs coordinate-wise clipping from both above and below. $BiClip$ is motivated by an interpolation between clipped-SGD and adaptive methods through a stabilizing absolute-value clipping mechanism that acts from both above and below, while removing the overhead of maintaining preconditioners. 
%The convergence results for $BiClip$-SGD builds upon said proofs, and offers another layer of generalization. The motivation behind the $BiClip$ operation is to further interpolate between standard SGD or sign-SGD, and adaptive methods via a stabilizing clipping operation. In particular, adaptive methods have been argued to achieve superior performance in training modern architectures in numerous works within the literature (cite). One significant feature of Adam~\cite{ADAM} or AdamW (cite) is the preconditioning operation, where the preconditioner is collected from historical gradient information with exponentially decaying memory. Intuitively, the preconditioners ensure that coordinates which have communicated stagnant, small-gradient updates upscale their updates to a larger value, and downscales coordinates with consistently larger updates, offering stability. This is done by diving gradients by a moving average of the second moment gradient, emphasizing as well as stabilizing mobility at the cost of essentially doubling the optimizer memory requirements to train the model. 
%$BiClip$-SGD builds upon this intuition by uniformly increasing the temperature on low-value gradient coordinates, and cooling down high-value gradient coordinates. Coupled with a outer node adaptive optimizer, which is able to leverage sensitive gradient updates on the client-side, aim to attain the benefits of adaptivity. Letting $0/0 :=0$ for notational convenience, the $L_2$-$BiClip$ operation is formally defined as:  
The formal definition of the $BiClip(\cdot)$ operation is as follows:\footnote{For clarity in notation, we define $0/0 := 0$.\vspace{-5mm}} %\tian{I deleted the definition of $L2 bi^2Clip$ because that may be distracting. can mention it in the appendix}
% \begin{equation*}
% \begin{aligned}
% &BiClip(u_t,d_t,x) = x \cdot \frac{d_t}{\|x\|} \ \chi\left(\|x\|\le d_t\right) \\
% &\quad + x \cdot \frac{u_t}{\|x\|} \ \chi\left(\|x\|\ge u_t\right) + x \cdot \chi\left(d_t<\|x\|< u_t\right),
% \end{aligned}
% \end{equation*}
For a model parameter $x \in \mathbb{R}^m$, parameter coordinate $j \in [m]$, lower clipping threshold $d$, and upper clipping threshold $u$ ($0\leq d \leq u$), we formally define $BiClip$ as
\begin{equation}
\begin{aligned} 
&BiClip(u,d,x)_j := \operatorname{sign}(x_j) \ \left[ d \ \chi\left(|x_j|\le d\right)\right]  \label{eq:biclip} \\ 
&+ \operatorname{sign}(x_j)  \left[u \ \chi\left(|x_j|\ge u \right) + |x_j|  \chi\left(d<|x_j|< u\right) \right], 
\end{aligned}
\end{equation}
% \begin{equation*}
% \begin{aligned}
% &BiClip(u_t,d_t,x)_j = \operatorname{sign}(x_j)  \left[ d_t \ \chi\left(|x_j|\le d_t\right)\right] \\
% &+ \operatorname{sign}(x_j)  \left[u_t \ \chi\left(|x_j|\ge u_t\right) + |x_j| \ \chi\left(d_t<|x_j|< u_t\right) \right],
% \end{aligned}
% \end{equation*}
% $$BiClip(u_t,d_t,x)_j = \operatorname{sign}(x_j) \ \left[ d_t \ \chi\left(|x_j|\le d_t\right) + u_t \ \chi\left(|x_j|\ge u_t\right) + |x_j| \ \chi\left(d_t<|x_j|< u_t\right) \right], $$ 
%for coordinate indices $\forall j \in [d]$, where $\operatorname{sign}(a) = -\chi (a<0) + \chi(a>0).$ Here, the absolute value clipping thresholds satisfy $u_t > d_t \ge 0$.  \tian{should we define $\chi, t, u_t, d_t \in \mathbb{R}$ somewhere}
where $\chi$ is the indicator function.


%\tian{add a sentence clarifying how do we set $u_t, d_t$ (a) theoretically, and (b) empirically? (at least empirically because that is a natural question)}
$BiClip$ draws on the intuition of adaptive methods by selectively amplifying smaller gradient values while tempering larger gradients. When combined with an outer (potentially adaptive) optimizer, this approach leverages sensitive, amplified gradient updates from the participating compute nodes, thus emulating the advantages of adaptive optimization. %This setup does not exist in the centralized optimization setting, \tian{it is fair to say only biclip in centralized setting is a contribution?} where a single optimizer strategy is typically employed concurrently in contrast with distributed optimization. 
%We note that $BiClip$ collapses into sign-SGD in the limit $d_t \to u_t^-$. 
Additionally, in contrast to adaptive optimizers, $BiClip$ does not require preconditioner maintenance, with significantly reduced optimizer requirements identical to SGD. %\tian{what if the reviewers argue it needs to maintain $d$ clipping thresholds?} \su{The $d$ clipping threshold is a scalar, so I would argue that it is essentially nothing at all (just an added scalar). In the Appendix, I write a bit about extensions using bespoke auto-tuning for $d$.}
While our focus is on the distributed setting, which aligns with practical applications, we note that $BiClip$ can also be effectively applied in centralized settings. 
This serves as the main building blocks of Algorithm~\ref{Adaptive_BiClip_SGD}. 

\vspace{-1mm}
\paragraph{TailOPT.} In the TailOPT framework (Algorithm~\ref{Adaptive_BiClip_SGD}), {the inner optimization strategy, denoted $TailClip$, refers to either $BiClip$ or $L_2Clip$.} In Line 10, the outer optimization strategy can be either adaptive or non-adaptive methods that incorporate clipping, adaptivity, or momentum on top of $\Delta_t$ by treating them as \textit{pseudogradients}. 
We present multiple instantiations of TailOPT along with their convergence bounds under heavy-tailed noise in Section~\ref{sec:convergence}, as well as in Appendix~\ref{AdaptiveBiClipSGD}. 

% \hspace{-10pt}

% {
% % \captionsetup[algorithm]{skip=-10mm}
% \setlength{\textfloatsep}{0mm}
% \setlength{\floatsep}{0mm}
% \setlength{\intextsep}{0mm} 

\begin{algorithm}[H]
\caption{Heavy-Tailed Optimization (TailOPT)}\label{Adaptive_BiClip_SGD}
\begin{algorithmic}[1]
\REQUIRE Initial model $x_1$, 
learning rate schedule $\eta_t$ \\Clipping schedules $u_t \ge d_t \ge 0$, \\ Synchronization timestep $z \in \mathbb{Z}_{>0}$
\FOR{$t = 1, \dots, T$} 
    \FOR{each node $i \in [N]$ in parallel}
    \STATE $x_{i,0}^t \leftarrow x_t$
    \FOR{each local step $k \in [z]$}
        \STATE Draw gradient $g^t_{i,k} = \nabla F_i(x^t_{i,k}, \xi^t_{i,k})$
        \STATE $x^{t+1}_{i,k} \leftarrow  x^t_{i,k} - \eta_t \cdot TailClip(u_t,d_t,g^t_{i,k})$ 
        %\STATE $\texttt{//}~TailClip:$ Either $L_2Clip$ or $BiClip$ 
        \ENDFOR
    \ENDFOR
    \STATE $\Delta_t = \frac{1}{N} \sum_{i \in [N]} \left(x_{i,z}^{t} - x_{t-1}\right)$
    %\STATE $\widetilde{m}_t = \widetilde{\beta}_1 \widetilde{m}_{t-1} + (1 - \widetilde{\beta}_1) \Delta_t$
    %\STATE $\widetilde{v}_t = \widetilde{v}_{t-1} + \Delta_t^2$ 
    \STATE $x_{t} = Outer\_Optimizer \ (x_{t-1},\Delta_t)$
\ENDFOR
\end{algorithmic}
\end{algorithm}%\aftergroup\noindent}
% \vspace{-10mm}}
% [Inner optimizer, outer optimizer, for 6, 10. In Algorithm 1, and then, add in some sentences about different instantiations and point to pseudocode as well,]

\vspace{-1mm}
Among those, we propose and highlight one efficient method that achieves superior empirical performance which utilizes the $BiClip (\cdot)$ operator (Eq.~\eqref{eq:biclip}) %\textit{
in both the inner and outer optimizers, called $Bi^2Clip$. The exact pseudocode is presented in Algorithm~\ref{BiSquare_Clip_SGD} (Appendix~\ref{BiSquareAppendix}).
Intuitively, $Bi^2Clip$ mitigates the effects of heavy-tailed noise across all inner as well as outer optimizers, while mimicking adaptive updates to amplify rare gradient signals. %(e.g., Appendix~\ref{IntuitionsonHeavyTailedNoise}). 
In Section~\ref{sec:experiments}, we empirically demonstrate that $Bi^2Clip$ outperforms state-of-the-art baselines without transferring or maintaining preconditioners in the distributed setting. 

For clarity, throughout the paper, we  list the outer optimizer followed by the inner optimizer when referencing algorithms. For example, `Adam-$BiClip$' instantiates Adam as the outer optimizer and $BiClip$ as the inner optimizer. Similarly, `RMSProp-$TailClip$' refers to RMSProp as outer optimizer, and $TailClip$ (either $L_2Clip$ or $BiClip$) as the inner optimizer. Finally, `$Bi^2Clip$' refers to the algorithm with $BiClip$ as both inner and outer optimizers.
% We refer to the compute node that deploys the outer optimizer as the outer node. 
% $BiClip$ by default refers to the coordinate-wise clipping proposed in Eq.~\eqref{eq:biclip}, unless explicitly specified otherwise. %\tian{set up the notation here: methodA-methodB means inner-outer optimizer throughout the paper. For instance, Avg-SGD, etc etc.}
%Our empirical results demonstrate training and testing performance which is on par with adaptive methods such as Adam (Table~\ref{tab:glue-results}). 
%$Bi^2Clip$ can be shown to converge under heavy-tailed noise (Theorem~\ref{BiSquare_Clip_SGD_Conv_Thm}); all convergence proofs are provided in full in the Appendix.

\section{Convergence of the TailOPT Framework} \label{sec:convergence}
Due to space constraints, we present convergence results for only a subset of TailOPT algorithms in the main text. For a comprehensive analysis, Appendices~\ref{AppendixSectionforConvexConvergenceCrossSilo},~\ref{AppendixSectionforConvexConvergenceCrossDevice} provide detailed convergence bounds for Avg-$L_2Clip$, and Appendices~\ref{BiSquareAppendix}-\ref{AdamBiClipAppendixSubsection} include additional convergence analyses and precise pseudocodes for various (adaptive) instantiations of the TailOPT framework incorporating Adagrad, RMSProp, or Adam. Additionally, we note that the formulation of $Bi^2Clip$ subsumes algorithms such as Avg-$BiClip$.
% \tian{can we add a sentence or two saying we only present results for a subset of the instances, and see appendix xxx for xxx exact algorithm}

While clipping offers the benefit of stabilization, it introduces a non-zero bias on the stochastic gradients, rendering them to be no longer unbiased estimators of the true gradient. %Prior work, such as~\cite{GradClipCentralGeomPerspective}, presents illustrative examples where using a fixed clipping threshold can bias the gradient dynamics to the extent that the optimum is no longer a steady state, preventing SGD from ever converging. 
Theorems~\ref{RMS_Clip_SGD_Conv_Thm} and~\ref{BiSquare_Clip_SGD_Conv_Thm} demonstrate that with appropriately chosen (increasing) upper clipping $u_t$ and (decreasing) learning rate $\eta_t$ and lower clipping $d_t$ schedules, convergence of Algorithm~\ref{Adaptive_BiClip_SGD} is nevertheless attainable. 
Up to $\mathcal{O}(d)$, the presented convergence bounds hold for both gradient-wise clipping as well as coordinate-wise clipping. Generalization to layer-wise clipping with varying thresholds specific to each layer or model weight tensor slice is straightforward.

% We first describe the convergence results of the RMSProp-$TaiClip$ instantiation (Algorithm~\ref{RMS_BiClip_SGD}). Recall that in Section~\ref{BiClipSecton}, we define the inner optimizer $TailClip$ to include both $L_2$ clipping or BiClip (Eq.~\eqref{eq:biclip}).

We carry out our analysis where the model weights $x_t \in \mathcal{X}$ are contained within a sufficiently large, compact set $\mathcal{X} \subset \mathbb{R}^d$. In such settings, finding the global minimum is known to be NP-Hard, and the standard convergence metric is the stabilization of the minimum gradient~\cite{symmetricnoise_distributed}. We then obtain the following theorems.%, where the pseudocode for each optimizer instantiation is detailed in Appendix~\ref{AdaptiveBiClipSGD}.


\begin{theorem}~\label{RMS_Clip_SGD_Conv_Thm} Let assumptions 1-2 hold. Instantiate the outer optimizer in Algorithm~\ref{Adaptive_BiClip_SGD} with RMSProp, giving Algorithm~\ref{RMS_BiClip_SGD} (RMSProp-$TailClip$). Let the clipping and learning rate thresholds satisfy $\eta_t = \Theta(t^\omega)$, $\eta_\ell^t = \Theta(t^\nu)$, $d_t = \Theta(t^\gamma)$, and $u_t = \Theta(t^\zeta)$ for the conditions %\tian{.. was trying to make equations small}
\setlength{\abovedisplayskip}{5pt}
\setlength{\belowdisplayskip}{2pt}
\begin{equation*}
\begin{aligned}
\vspace{-0.5in}
& \nu < \min\left\{-\frac{1}{6} - \frac{4}{3}\zeta, -\frac{1}{4} - \frac{3}{2}\zeta - \frac{1}{2}\omega, -\frac{1}{2} + (\alpha - 2) \zeta \right\},\\
&0<\zeta< \min\left\{\frac{1}{4},\omega + \frac{1}{2}\right\}, \quad -\frac{1}{2}<\omega \le 0,\\
& \gamma < \min \left\{0,-\nu -\zeta - \frac{1}{2}\right\}.
\vspace{-0.2in}
\end{aligned}
\end{equation*}
Then, we have that
\setlength{\abovedisplayskip}{5pt}
\setlength{\belowdisplayskip}{2pt}
\vspace{-0.1in}
\begin{equation*}
\min_{t \in [T]}\mathbb{E}\left\| \nabla F(x_t)\right\|^2 \le \sum_{i=1}^6 \Psi_i, 
% \vspace{-0.2in}
\end{equation*}
where the $\Psi_i$ are upper bounded by
\begin{equation*}
\begin{aligned}
& \Psi_1 \le \mathcal{O}(T^{-\omega + \zeta - \frac{1}{2}}), \quad \Psi_2 \le \mathcal{O}(T^{\omega + 2\nu + 3\zeta + \frac{1}{2}}),\\
& \Psi_3 \le \mathcal{O}(T^{4\zeta + 3\nu + \frac{1}{2}}), \Psi_4 \le \mathcal{O}(T^{2\nu + 2\zeta + \frac{1}{2}}),\\
& \Psi_5 \le \mathcal{O}(T^{\nu + \gamma + \zeta + \frac{1}{2}}), \quad \Psi_6 \le \mathcal{O}(T^{\nu + (2-\alpha)\zeta + \frac{1}{2}}),
\end{aligned}
\end{equation*}
which guarantees convergence via an inversely proportional power law decay with respect to $T$. Here, the exponential moving average parameter of the second pseudogradient moment is fixed within the range $\widetilde{\beta}_2 \in [0,1)$.
\end{theorem}

In particular, the proof of this result immediately implies the following summarizing corollary.
\begin{corollary}\label{RMSBiClipCorollary}
 Algorithm~\ref{RMS_BiClip_SGD} (RMSProp-$TailClip$) convergences under heavy-tailed stochastic gradient noise. The maximal convergence rate can be attained in the limit $\zeta \to 0^+$ for an asymptotically near-constant upper clip threshold $u_t = \Theta(t^\zeta)$ as $\mathcal{O}(1/\sqrt{T})$. 
\end{corollary}
%
 The full proofs of all results in this section are given in Appendix~\ref{AdaptiveBiClipSGD}, which holds for both convex and non-convex functions. % \su{I included this line, but I'm still a bit worried... My personal preference would not be to say this, but if emphasizing something along these lines make the paper better, we should definitely keep it in}
This achieves the state-of-the-art convergence rate of $\mathcal{O}(1/\sqrt{T})$~\cite{SoTA1,SoTA2,SoTA3} even in the presence of heavy-tailed noise with local updates. We also obtain a $\mathcal{O}(1/\sqrt{T})$ rate for an alternate instantiation (Adagrad-$TailClip$) and provide the exact algorithm in Algorithm~\ref{Adagrad_BiClip_SGD} and convergence result in Theorem~\ref{ServerAdagradConvergenceTheroem} of the appendix. 

When deploying distributed optimization, adaptive optimizers such as Adam can considerably increase the memory requirements on each compute node due to preconditioner storage, which matches the model parameter tensor size. For instance, $Adam^2$~\cite{wang2021local}, which applies Adam across all compute nodes, increases overhead by transmitting preconditioners from outer to inner optimizers to maximize performance, posing significant communication and memory challenges. Algorithm~\ref{RMS_BiClip_SGD} (RMSProp-$TailClip$) eliminates this bottleneck by removing both preconditioner transmission and maintenance on all inner optimizers, while imitating adaptivity through $BiClip$. % and yields a highly efficient alternative by approximating away the predonditioners. 
%Given that preconditioners typically double memory utilization, this drastically reduces resource demands across the entire compute cluster. 
This naturally intuits the question of whether TailOPT can incorporate further efficient adaptive approximations on the outer optimizer, while ensuring convergence under heavy-tailed noise. This motivates $Bi^2Clip$, which leverages $BiClip$ at both inner and outer optimizers, retaining the benefits of adaptivity with minimal overhead. Convergence results are given below.


% Next, we present results of the $Bi^2Clip$ method for potentially non-convex functions as well.


%In contrast to the convergence proofs presented in Appendix ( ) and ( ) for $L_2$ clipping, we now move to the general, non-convex setting and drop Assumption 3, $\mu$-strong convexity (Appendix ..). In addition, 

%We carry out our analysis where the model weights $x_t \in \mathcal{X}$ are contained within a sufficiently large, compact set $\mathcal{X} \subset \mathbb{R}^d$. In such settings, finding the global minimum is known to be NP-Hard, and the standard convergence metric is the stabilization of the minimum gradient~\cite{symmetricnoise_distributed}. We then obtain the following theorems, where the pseudocode for each optimizer instantiation is detailed in Appendix~\ref{AdaptiveBiClipSGD}.

\begin{theorem}\label{BiSquare_Clip_SGD_Conv_Thm} Let the learning rate and clipping schedules satisfy $\eta_t = \Theta(t^\omega)$, $\eta_\ell^t = \Theta(t^\nu)$, $d_t = \Theta(t^\gamma)$, $u_t = \Theta(t^\zeta)$, $\widetilde{d}_t = \Theta(t^{\widetilde{\gamma}})$, and $u_t = \Theta(t^{\widetilde{\zeta}})$. For $Bi^2Clip$ (Algorithm~\ref{BiSquare_Clip_SGD}), %\tian{let assumptions 1-2 hold?} 
we have that the minimum gradient satisfies %\tian{can we emphasize it holds for both convex and nonconvex cases?} \su{I am not quite sure if it holds for the convex case; unless there is a result indicating that convex functions are $L$-smooth} \tian{but we already assume smoothness, right?} \su{Yes, we assume smoothness in assumption 1, but my understanding was that if we say that something holds in both convex and non-convex L-smooth cases, I thought that meant that it held in convex and not L-smooth case as well}%\tian{Prateek's comment is that the theorem statements are too long, can we put some assumptions outside the theorems. [note: it's okay not to address, just put it here in case]} \su{I think their comment was made before we had the `let assumptions 1-2 hold.' statement, and I understood it as them asking `Can you write that in the statement'. They're reviewing an older version of the paper, and many different changes were made to the paper since submitting}
\vspace{-1mm}
\setlength{\abovedisplayskip}{5pt}
\setlength{\belowdisplayskip}{2pt}
\begin{equation*}
\begin{aligned}
\min_{t \in [T]} \mathbb{E}[\|\nabla F(x_{t-1})\|^2] \lesssim \sum_{i=1}^7 \Psi_i ,
\end{aligned}
\end{equation*}
where the $\Psi_i$ are given
\begin{equation*}
% \resizebox{0.5\textwidth}{!}{%
% \setlength{\abovedisplayskip}{5pt}
% \setlength{\belowdisplayskip}{2pt}
% $
\begin{aligned}
&\Psi_1 = \mathcal{O}\left( T^{-\omega - \nu - 1}\right), \quad \Psi_2 = \mathcal{O}\left(T^{\omega + 2\widetilde{\zeta} - \nu} \right), \quad\Psi_3 = \mathcal{O}\left(T^\gamma\right),\\
&\Psi_4 = \mathcal{O}\left(T^{\widetilde{\gamma} - \nu}  \right), \quad \Psi_5 = \mathcal{O}\left( T^{(\alpha-1)\nu + (1-\alpha)\widetilde{\zeta}} \right),\\
&\Psi_6 = \mathcal{O}\left(T^{(1-\alpha)\zeta}  \right),\quad \Psi_7 = \mathcal{O}\left( T^{\nu + \zeta} \right).
\end{aligned}%$%
%}
\end{equation*}
To attain convergence, we impose  $\zeta,\widetilde{\zeta} > 0 > \gamma, \widetilde{\gamma}$, for $\omega, \nu \le 0$, as well as the following conditions
\begin{equation*}
-1< \omega + \nu, \quad \nu + \zeta < 0, \quad \max\{\omega + 2\widetilde{\zeta},\widetilde{\gamma}\} < \nu. 
\end{equation*} 
Then, $Bi^2Clip$ converges with maximal rate at least $\mathcal{O}(T^{-r})$, where for $\widetilde{\varepsilon} \in (0,1/8)$ and $\alpha > 1$,
\begin{equation*}
%\resizebox{0.5\textwidth}{!}{%
%$
r := \min \left\{\frac{(\alpha-1)\alpha}{4}, \ \widetilde{\varepsilon}, \ \frac{\alpha-1}{4}-(1-\alpha)(\frac{1}{8}-\widetilde{\varepsilon}) \right\}.
%$%
%}
\end{equation*}
\end{theorem}

This gives the following corollary.
\begin{corollary}\label{Bi2ClipCorollary}
 Algorithm~\ref{BiSquare_Clip_SGD} ($Bi^2Clip$) converges with respect to heavy-tailed stochastic gradient noise ($\alpha > 1$). For instance, if the moment is further constrained by $\alpha > 1.5$, the algorithm converges with a maximal rate of at least $\mathcal{O}(T^{-r})$ for $r = 1/8$. 
\end{corollary}

Similar as RMSProp-$TailClip$, the results here hold for both convex and non-convex functions as long as the assumptions are satisfied. The convergence rate given in Corollary~\ref{Bi2ClipCorollary} represents a lower bound on the maximal achievable rate, obtained by a fixed selection of hyperparameters. Interestingly, our empirical results demonstrate that $Bi^2Clip$ outperforms other methods, suggesting that the current convergence bounds could be further refined.

\vspace{-1.5mm}
\paragraph{Discussions.} To ensure convergence and mitigate bias in the derived bound, it is necessary for the upper clipping threshold $u_t \to \infty$ and the lower clipping threshold $d_t \to 0$ as $t \to \infty$, consistent with established counterexamples that occur due to unmitigated clipping bias~\cite{GradClipCentralGeomPerspective,RevisitGradClipModern}. In cases where stochastic gradients are sampled from large-variance distributions, this necessitates a continual warm-up phase that is continuously relaxed, akin to learning rate \textit{warm-up} schemes that conclude after a finite period~\cite{warmup1}.

The clipping schedules prescribed by Theorems~\ref{RMS_Clip_SGD_Conv_Thm},~\ref{BiSquare_Clip_SGD_Conv_Thm} grow polynomially with respect to $t$, which depict the realization of model weights throughout training. This effectively deactivates gradient clipping after an initial warm-up phase that is shaped by the noise distribution's tail behavior and the clipping thresholds. This may help to explain why learning rate warm-ups are observed to significantly improve training~\cite{warmup2,warmup3} in the presence of heavy-tailed stochastic gradients. %Additionally, centralized batched SGD can be viewed as a special case of distributed or nested SGD, where each batch element functions as a small client sampling from the population distribution. 
Finally, as the maximal bounded moment condition $\alpha$ approaches the integrability threshold ($\alpha = 1$), or as $\gamma$ nears $0^-$, the convergence bound is mollified. Despite this, in our experiments, we set $\nu = \zeta = \gamma = 0$, which yielded strong empirical performance. Intuitively, this setup corresponds to a continual amplification of informative coordinates and attenuation of uninformative covariates.

\vspace{-1.5mm}
\paragraph{Other Instantiations and Extensions.} As noted previously, we extend our analysis to support an Adagrad-based outer optimizer (Algorithm~\ref{Adagrad_BiClip_SGD}) and provide a convergence guarantee under heavy-tailed noise, detailed in Theorem~\ref{ServerAdagradConvergenceTheroem}. 
In Appendix~\ref{AdamBiClipAppendixSubsection}, we further generalize our framework by incorporating momentum into the first-order stochastic pseudogradient statistics, resulting in an outer optimizer Adam instantiation. While we establish that the expected minimum gradient is asymptotically bounded even under restarting (Theorem~\ref{AdamBiClipBoundedTheorem}), proving formal convergence to $0$ remains an open challenge. The difficulty arises from the moving average applied to the first moment, which retains all historical gradient information and complicates the analytical proof structure. We also extend convergence results for certain instantiations to allow for \textit{node drop or failures} at each round (Appendix~\ref{AppendixSectionforConvexConvergenceCrossDevice}). Our bound further highlights that the dominating terms are influenced by the upper clipping threshold $u_r$, which we tune empirically in Section~\ref{sec:experiments} by sweeping over a hyperparameter grid defined in Appendix~\ref{hyperparametergridappendix}. For this result, we extremize the noise tails such that there $\nexists \alpha$ such that the $\alpha$-moment is finite for $\forall \alpha > 1$, under which $u_t$ stabilizes the gradient dynamics.  

% Additionally, we  observe that proof directly generalizes to scenarios where clipping and learning rates are scheduled to increase or decay at each outer node synchronization steprather than at each inner node minibatch backpropagation step. %\tian{add one sentence to explain?} 
% A similar extension applies to layer-wise $BiClip$, as opposed to full-gradient $BiClip$. 
%The difficulty arises from the moving average applied to the first moment, which retains all historical gradient information and complicates the analytical proof structure. Exploring the conditions necessary for convergence in this setting is a promising direction for future research. 
%Our bound further highlights that the dominating terms are influenced by the upper clipping threshold $u_r$, suggesting that the algorithms convergence behavior is sensitive to the choice of this threshold and should be carefully tuned in practice. For this result, we extremize the noise tails such that there $\nexists \alpha$ such that the $\alpha$-moment is finite for $\forall \alpha > 1$, under which $u_t$ stabilizes the gradient dynamics. 




%\begin{theorem}
%Let the exponentially decaying moving average parameters satisfy $\widetilde{\beta}_1 \in (0,1)$, $\widetilde{\beta}_2 \in [0,1)$ for the outer node first and second order pseudogradient moments, respectively. Extremize the unbiased stochastic noise such that $\nexists \alpha_i \in (1,2)$ for which $\mathbb{E}[\|\xi_i\|^{\alpha_i}] < B_i^{\alpha_i}$ for integrable $\xi_i$. Then, Algorithm~\ref{Adam_BiClip_SGD} gives under constant upper clipping threshold invariant to global timestep $t$ ($\zeta = 0$) that
%\begin{equation*}
%\min_{t \in [T]}\mathbb{E}[\|\nabla F(x_t)\|^2] \lesssim \mathcal{O}(1),
%\end{equation*}
%where for $\eta_t = \Theta(t^\omega)$, $\eta_\ell^t = \Theta(t^\nu)$, and $d_t = \Theta(t^\gamma)$, we impose
%\begin{equation}%\label{AdamConditions}
%\begin{aligned}
%\nu \in (-1,0), \quad  -\nu - 1 < \omega \le 0, \quad -(1+\nu + \omega) < \gamma < 0. 
%\end{aligned}
%\end{equation}
%\end{theorem}




%Empirical and theoretical evidence consistently demonstrates the effectiveness of warm-up phases in enhancing optimizer performance, particularly for transformer-based models where stochastic gradients exhibit heavy-tailed behavior~\cite{warmup1,warmup2,warmup3}. This observation aligns with the destabilization tendencies of SGD and distributed SGD under heavy-tailed or infinite-variance distributions~\cite{HeavyTailedNoisePaper,EfficientAdaptiveFederatedOptimization}. 
 %Intuitively, this setup corresponds to a continual amplification of rare coordinates and attenuation of common features, effectively smoothing the optimization landscape.


\section{Experiments} \label{sec:experiments}

% \tian{can we add a paragraph or subsection brief talking about experiment setups (datasets, hyperparameter tuning, etc---can point to appendix)}

We assess the performance of various TailOPT instantiations across a range of empirical tasks, benchmarking them against state-of-the-art algorithms from the literature. Extended details of the experimental setup, dataset descriptions, and extensive hyperparameter tuning procedures are provided in Appendix~\ref{ExperimentSetupAppendix}. Our experiments include synthetic tests with heavy-tailed noise injection, as well as evaluations of language models on real-world benchmarks, including GLUE~\cite{GLUE} for natural language understanding and WMT~\cite{wmt} for machine translation. A brief summary of each experimental setup is presented in the following subsections.

\subsection{Convex Models}


\begin{figure}[!b]
% \vspace{-5mm}
\centering
%\hfill
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Synthetic_Pure.pdf}
        \subcaption{No Noise}
    \end{subfigure}
    %\hfill
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Gaussian_Scale_1_legend_f.pdf}
        \subcaption{Light-Tailed (scale 1)}
    \end{subfigure}
        \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Gaussian_Scale_3_nolegend.pdf}
        \subcaption{Light-Tailed (scale 3) }
    \end{subfigure}
    %\hfill
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{updated_sweep.pdf}
        % tdist_scale_1_nolegend_new
        \subcaption{Heavy-Tailed}
    \end{subfigure}%\\
    \hfill
    \caption{The impact of heavy-tailed noise on a synthetic dataset. When injected gradient noise is absent, Avg-SGD achieves the best performance (c.f., (a)). However, as the noise tails grow heavier, the performance of Avg-SGD deteriorates considerably. By contrast, both clipping mechanisms and adaptive updates demonstrate considerable performance in locating the ground truth $w_*$, and effectively mitigates the adverse effects of heavy-tailed noise (d). Noise with lighter tails (b-c) may not significantly destabilize the dynamics of non-adaptive Avg-SGD.
    %The effects of heavy-tailed noise on a synthetic dataset. As the tails of the gradient noise become heavier, the Avg + SGD algorithm becomes significantly worse. By contrast, both clipping as well as adaptive updates demonstrate considerable performance in locating the ground truth $w_*$. When injected gradient noise is absent, we note that Avg + SGD does the best (c.f., (a)). However as tails become heavier, non-adaptive Avg + SGD becomes worse, but clipping or adaptivity can help to mollify the effects of the heavy noise as shown in (d). Additionally, we observe that non-heavy-tailed noise (b-c) may not be sufficient to destabilize non-adaptive Avg + SGD dynamics. 
    }
    \vspace{-10pt}
\label{InjectedNoise}
\end{figure} 



\begin{table*}[!b]
\centering
\caption{Evaluation results on GLUE Benchmark datasets during test time. Metric descriptions are given in Appendix~\ref{GLUEAppendix}, and the full table  is given as Table~\ref{tab:glue-results-appendix}. Entries marked with $0.0$ indicate failure to learn. Top \textcolor{DarkerBlue}{\textbf{first}}, \textcolor{blue}{\textbf{second}}, and \textcolor{cyan}{\textbf{third}} best-performing algorithms are highlighted. 
For $Adam^2$, preconditioners are transmitted between the inner and outer optimizers, whereas DiLoCo requires maintaining preconditioners on the inner optimizers, both of which incur significant communication or memory overhead than $Bi^2Clip$. Our experiments show that $Bi^2Clip$ achieves the best performance with the smallest overhead.
} 
\label{tab:glue-results}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}lccccccccc@{}}
\toprule
\textbf{Algorithm} & \textbf{MNLI} & \textbf{QNLI} & \textbf{QQP (Acc/F1)} & \textbf{RTE} & \textbf{SST-2} & \textbf{MRPC (Acc/F1)} & \textbf{CoLA} & \textbf{STS-B (S/P)} & \textbf{Average} \\
\midrule

\textbf{Avg-SGD}~\cite{mcmahan2017communication} 
& 81.13 
& 83.21 
& 78.71/78.69 
& 57.40 
& 90.94 
& 67.30/80.52 
& 0.0 
& 26.76/28.20 
& 61.17 \\

\textbf{Avg-$L_2Clip$}~\cite{fatclip} 
& 81.82 
& 85.68 
& 80.00/79.82 
& 54.51 
& 91.97 
& 68.38/81.22 
& 0.0 
& 41.27/40.96 
& 64.15 \\

\textbf{Avg-Adagrad} 
& 84.70 
& 88.79 
& 87.09/83.34 
& 64.26 
& 93.34 
& 71.56/82.63 
& 27.72 
& 81.93/81.26 
& 76.97 \\

\textbf{Avg-Adam} 
& 84.97 
& 89.47 
& 87.66/84.09 
& 64.62 
& \textcolor{blue}{\textbf{93.80}} 
& 81.86/87.74 
& 41.41 
& \textcolor{cyan}{\textbf{86.21}}/\textcolor{cyan}{\textbf{86.55}} 
& 80.76 \\

\textbf{Avg-$BiClip$} 
& 85.08 
& 89.45 
& 87.83/84.12 
& 66.06 
& \textcolor{DarkerBlue}{\textbf{94.03}} 
& 71.32/82.45 
& 41.40 
& 84.08/84.48 
& 79.12 \\

\midrule

\textbf{Adagrad-SGD}~\cite{AdaptiveFederatedOptimization} 
& 82.40 
& 86.61 
& 82.51/77.68 
& 71.48 
& 92.08 
& 85.53/89.52 
& 47.80 
& 40.37/42.24 
& 72.69 \\

\textbf{Adagrad-$BiClip$} 
& \textcolor{cyan}{\textbf{85.54}} 
& \textcolor{DarkerBlue}{\textbf{90.02}} 
& 88.60/\textcolor{cyan}{\textbf{85.05}} 
& \textcolor{blue}{\textbf{73.36}} 
& 93.23 
& 85.78/89.86 
& 48.87 
& 84.03/85.90 
& 82.75 \\

\textbf{RMSProp-SGD}~\cite{AdaptiveFederatedOptimization}
& 84.20 
& 88.46 
& 87.12/83.30 
& \textcolor{cyan}{\textbf{72.56}} 
& 91.85 
& 85.50/89.17 
& 52.39 
& 45.72/41.80 
& 74.73 \\

\textbf{RMSProp-$BiClip$} 
& \textcolor{blue}{\textbf{85.56}} 
& \textcolor{cyan}{\textbf{89.82}} 
& 88.50/84.44 
& 70.75 
& \textcolor{cyan}{\textbf{93.69}} 
& 84.80/88.92 
& 50.99 
& \textcolor{DarkerBlue}{\textbf{87.65}}/\textcolor{DarkerBlue}{\textbf{87.79}} 
& \textcolor{cyan}{\textbf{82.99}} \\

\midrule

\textbf{Adam-SGD}~\cite{AdaptiveFederatedOptimization}
& 82.93 
& 86.98 
& 85.99/80.87 
& 66.78 
& 90.71 
& 87.01/90.09 
& 49.93 
& 44.48/41.26 
& 73.37 \\

\textbf{Adam-$L_2Clip$} 
& 82.54 
& 86.69 
& 85.88/80.72 
& 59.92 
& 89.67 
& 85.29/89.90 
& 48.54 
& 69.19/67.16 
& 76.86 \\

\textbf{Adam-$BiClip$} 
& 84.26 
& 89.20 
& \textcolor{cyan}{\textbf{88.64}}/84.74 
& 69.67 
& 92.43 
& 86.52/90.09 
& \textcolor{blue}{\textbf{56.12}} 
& 82.83/79.71 
& 82.20 \\

\textbf{$Adam^2$}~\cite{wang2021local} 
& 85.11 
& 88.87 
& \textcolor{DarkerBlue}{\textbf{89.04}}/\textcolor{DarkerBlue}{\textbf{85.51}} 
& 71.48 
& 92.66 
& \textcolor{cyan}{\textbf{87.50}}/\textcolor{cyan}{\textbf{91.03}} 
& 52.70 
& 84.47/83.82 
& 82.93 \\

\textbf{DiLoCo}~\cite{DiLoCo} 
& \textcolor{DarkerBlue}{\textbf{85.68}} 
& \textcolor{blue}{\textbf{89.87}} 
& \textcolor{blue}{\textbf{88.78}}/\textcolor{blue}{\textbf{85.19}} 
& 67.87 
& 91.89 
& \textcolor{blue}{\textbf{87.99}}/\textcolor{blue}{\textbf{91.20}} 
& \textcolor{cyan}{\textbf{54.77}} 
& 85.93/84.76 
& \textcolor{blue}{\textbf{83.08}} \\

\textbf{$Bi^2Clip$} 
& 85.06 
& 89.73 
& 84.93/83.97 
& \textcolor{DarkerBlue}{\textbf{76.53}} 
& \textcolor{blue}{\textbf{93.80}} 
& \textcolor{DarkerBlue}{\textbf{89.21}}/\textcolor{DarkerBlue}{\textbf{92.44}} 
& \textcolor{DarkerBlue}{\textbf{60.08}} 
& \textcolor{blue}{\textbf{87.07}}/\textcolor{blue}{\textbf{86.89}} 
& \textcolor{DarkerBlue}{\textbf{84.52}} \\

\bottomrule
\end{tabular}
}
\vspace{-3mm}
\end{table*}

We designed our convex, synthetic dataset setup to explicitly control and inject heavy-tailed noise, enabling a focused study of its effects. In language tasks, the frequencies of words or tokens typically follows a heavy-tailed distribution, where a small subset of tokens occurs with high frequency, while the majority appear infrequently yet carry significant contextual information. To mirror this phenomenon, emulating a similar setup in~\cite{adadps}, we partitioned the input feature space into common and rare features. Specifically, we set the first $p = 10\%$ features (or tokens) from data $X$ as common features, with each feature activated according to a Bernoulli distribution $\text{Bern}(0.9)$. The remaining $90\%$ of the features are configured as rare, each sampled from $\text{Bern}(0.1)$. The weight vector $w_*$ is drawn from a standard multivariate normal distribution, $w_* \sim \mathcal{N}(0, I_m)$, and the labels are generated as $\hat{y} = Xw_* + \xi_{noise}$. A neural network with model weight $\hat{w}$ is then trained to learn the ground truth $w_*$. %\su{Yep! That is also in fact how I implemented things in Pytorch} \tian{neural network?} 
A comprehensive explanation of the dataset construction and experimental setup is provided in Appendix~\ref{AllSyntheticExperimentAppendix}. We inject noise $\xi_{noise}$ sampled from a heavy-tailed distribution, which implies that the induced stochastic gradients must be heavy-tailed under MSE loss. In Figure~\ref{InjectedNoise}, we sample from the Gaussian and Student $t$ distributions for the non-heavy-tailed and heavy-tailed $\xi_{noise}$, respectively. By default, we multiply the noise by scale $1$ unless otherwise specified (Figure~\ref{InjectedNoise} (c)).



We observe that while SGD demonstrates strong performance in non-noisy settings, its effectiveness diminishes as noise tails become heaviera scenario where adaptive methods and $BiClip$ excel. Similarly, $L_2Clip$ shows some ability to mitigate heavy-tailed noise but exhibits a comparable decline in performance under heavy-tailed conditions. %\tian{add a sentence to brief summarize results in Figure 1}





\subsection{Transformer Encoders}


To evaluate the effectiveness of our approach, we fine-tuned RoBERTa~\cite{RoBERTa} on the General Language Understanding Evaluation (GLUE) benchmark~\cite{GLUE}, a widely-used suite of natural language understanding tasks. The GLUE benchmark includes diverse tasks such as sentiment analysis, sentence similarity, textual entailment, and natural language inference, providing a comprehensive evaluation of model performance across multiple linguistic phenomena. We followed standard fine-tuning protocols for RoBERTa, leveraging pre-trained weights and optimizing task-specific objectives for each dataset in GLUE. Model performance was evaluated using the benchmark's metrics, such as accuracy, Matthews Correlation Coefficient (MCC), and F1 score, depending on the task. Our results demonstrate that $BiClip$ attains competitive performance similar to Adam, despite being entirely non-adaptive and memory efficient. Detailed discussions for each task are provided in Appendix~\ref{GLUEAppendix}.% Due to uniformly poor performance across all algorithms, WNLI was not included in the evaluation. We note that it is standard to exclude the evaluation of WNLI from the validation performance when reporting GLUE average, due to the intrinsically adversarial nature of the dataset (i.e., validation data are constructed as subtle perturbations applied to the training data with opposite labels)~\cite{t5}.
Table~\ref{tab:glue-results} presents the performance of the algorithms of interest on the GLUE benchmark. Our results show that $L_2Clip$ enhances performance on real-world data. Adaptive methods further improve upon these results, consistently outperforming $L_2Clip$ (e.g., convergence curves in Figure~\ref{visualization}). Notably, the newly proposed clipping method in TailOPT, $BiClip$, demonstrates superior performance compared to $L_2Clip$ and, in some cases, even surpasses Adam during test time (c.f., comparing $Bi^2Clip$ and $Adam^2$), highlighting its potential as an efficient and effective optimizer in real-world applications. Additionally, instantiations of TailOPT achieving $\ge$ 80\% average accuracy generally employ adaptive or adaptive-approximating optimizers across all nodes. In particular, adaptivity on the inner optimizer appears crucial for performance, as SGD-based methods perform considerably worse ($\le 75\%$). By contrast, both $BiClip$ or Adam reach $\sim 80\%$  even when combined with a simple averaging outer optimizer strategy.

% \begin{figure}[h]
% \vfill
\begin{figure*}[!t]
\vspace{-1mm}
\centering
\hfill
    \captionsetup[subfigure]{aboveskip=2pt,belowskip=-2pt}
    \begin{subfigure}[b]{0.28\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Q1_redo.pdf} %Q1_f
        \subcaption{Effects of $L_2$ clipping}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.28\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Q2_redo.pdf} %Q2_f
        \subcaption{Effects of Adaptivity}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.28\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Q3_redo1.pdf} %Q3_f_v3
        \subcaption{Effects of $BiClip$}
    \end{subfigure}
    \hspace{5mm}
    \vspace{-1mm}
    \caption{Convergence curves on the QNLI dataset. In (a), we see that $L_2Clip$ (one option of $TailClip$) can help to improve performance under different outer optimizers. (b) demonstrates that adaptivity also helps to mitigate the negative effects of heavy-tailed noise. In all three plots (a)-(c), $L_2Clip$ performs worse than adaptive methods, but the coordinate-wise $BiClip$ optimizer performs comparably or even better than adaptive optimization frameworks, manifesting Adam-like performance. We note that the $Adam^2$ baseline, which applies Adam both in inner and outer optimization, requires transmitting preconditioners of the same size as the model weights to inner optimizers, resulting in substantial communication and memory overhead to deploy. By contrast, $Bi^2Clip$ removes the necessity of preconditioner maintenance, sidestepping this bottleneck entirely.}
    \label{visualization}
    \vspace{-3.2mm}
\end{figure*}

\subsection{Generative Models}
We also evaluate TailOPT on machine translation tasks utilizing the WMT datasets, a widely used benchmark for translation research~\cite{wmt}. Specifically, we fine-tune the T5~\cite{t5} generative model on the TED Talks and News Commentary parallel training datasets. The TED Talks dataset, originally sourced from IWSLT 2017~\cite{IWSLT2017}, comprises multilingual translations of TED Talk transcripts, while the News Commentary dataset includes parallel text from news articles across various languages. We report both Bleu and Meteor scores across several variants of source and target language translations in Table~\ref{tab:mt-results-average}. 



\begin{table*}[h!]
\centering
\caption{Evaluation results on machine translation benchmarks. Metrics reported are BLEU and METEOR scores for various language pairs across the TED Talks and News Commentary datasets. The final column represents the average score across all metrics for each algorithm.}
\label{tab:mt-results-average}
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccccc}
\toprule
\textbf{Algorithm} & \multicolumn{2}{c}{\textbf{TED Talks (en-de)}} & \multicolumn{2}{c}{\textbf{TED Talks (en-fr)}} & \multicolumn{2}{c}{\textbf{News Commentary (en-fr)}} & \textbf{Average} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
 & BLEU & METEOR & BLEU & METEOR & BLEU & METEOR & \\
\midrule
\textbf{Avg-SGD} 
 & 28.02 & 58.52 
 & 27.48 & 54.67
 & 30.07 & 54.13 
 % & 00.0 & 00.0 
 & 42.15 \\
 
\textbf{Avg-$L_2Clip$} 
 & 28.99 & 58.94 
 & 29.66 & 57.40 
 & 31.02 & 56.73 
 % & 00.0 & 00.0 
 & 43.79 \\
 
\textbf{$Bi^2Clip$} 
 & \textbf{29.41} & \textbf{59.18} 
 & 30.70 & \textbf{58.13} 
 & \textbf{31.79} & \textbf{57.69} 
 % & 00.0 & 00.0 
 & \textbf{44.48} \\
 
\textbf{$Adam^2$} 
 & 28.06 & 58.05 
 & \textbf{30.94} & 57.48 
 & 30.97 & 55.85 
 % & 00.0 & 00.0 
 & 43.56 \\
 
\bottomrule
\end{tabular}
}
\end{table*}



\vspace{-2.5mm}
\paragraph{Discussion.} For language reasoning benchmarks, the performance differences across algorithmic instantiations are particularly pronounced. While $L_2$ clipping is a common stabilization strategy, it exhibits limited effectiveness. In contrast, coordinate-wise $BiClip$ demonstrates significantly better stability and performance. Moreover, frameworks aiming to utilize or mimic adaptivity in both the inner and outer optimizers generally achieve superior results, surpassing $80\%$ average performance across all benchmarks. Notably, performance is highly sensitive to the choice of inner optimizers, with SGD and $L_2$ clipping yielding the lowest results. For machine translation fine-tuning tasks however, the performance variance across different optimizer strategies is relatively small when optimal hyperparameters are selected. An expanded table with a more extensive evaluation is provided in Appendix~\ref{Additional_Experiments_Appendix} as Table~\ref{tab:glue-results-appendix}.

In resource-constrained settings, $BiClip$ emerges as a strong candidate, where $Bi^2Clip$ outperforms even $Adam^2$ in our experiments. While its design aims to emulate adaptivity under heavy-tailed noise, $BiClip$ exhibits characteristics that can interpolate between non-adaptive and adaptive methods, capturing benefits from both without necessarily fully belonging to either paradigm (Figure~\ref{GradientDistributionComparison}, Appendix~\ref{Additional_Experiments_Appendix}). $Bi^2Clip$ retains the same memory requirements as standard \textit{vanilla} SGD, which cements a  highly resource-efficient adaptive approximation while strictly adhering to resource constraints. %So far, we note that $Bi^2Clip$ has deliberately not incorporated momentum for maximal resource efficiency, %where momentum is  despite its impressive performance.

\vspace{-0.5mm}
\section{Conclusion}% and Discussions}
\vspace{-0.5mm}

In this work, we have introduced TailOPT,
 %an efficient heavy-tailed optimization framework, TailClip Optimization (TailOPT), 
a framework for efficient heavy-tailed optimization. We  have proposed
the  $BiClip$ optimizer based on coordinate-wise clipping from above and below, which utilizes nearly identical
memory and compute resources to vanilla SGD yet manifests Adam-like performance. We establish convergence guarantees for our TailOPT under potentially unbounded variance and provide a thorough empirical evaluation with real-world as well as  synthetic datasets. Our experiments indicate that $BiClip$ stabilizes training under heavy-tailed noise and achieves the benefits of efficient adaptive optimization, exceeding the state-of-the-art performance. 

Future work could explore the autonomous selection of $u_t$ and $d_t$ based on initial statistics or bespoke estimators, which could provide practical solutions. Alternatively, allowing the clipping thresholds to vary depending on coordinate partition subsets (e.g., across tensor slices), similar to compressed preconditioners such as SM3~\cite{anil2019memory}, may further enhance performance. 
%Another avenue for research would be applying $BiClip$ before passing the gradients to an adaptive optimizer, both within a single compute node, which could improve stability while reducing reliance on the adaptivity parameter. 
An extended conclusion with possible future directions is provided in Appendix~\ref{Extended_Conclusion_Appendix}.
%\vspace{-5mm}






% DarkBlue, cyan, LighterBlue

% \textcolor{DarkBlue}{This is dark blue.}

% \textcolor{MediumBlue}{This is medium blue.}

% \textcolor{cyan}{This is cyan blue.}

% \textcolor{LightBlue}{This is light blue.}

% \textcolor{LighterBlue}{This is lighter blue.}

% \clearpage


\bibliographystyle{unsrt}
{\small \bibliography{references.bib}}

\newpage
\appendix
%For convenience, we provide a table of contents for the Appendix. 

% \addcontentsline{toc}{section}{Appendix} % Add the appendix text to the document TOC
% \part{Appendix} % Start the appendix part
% \parttoc % Insert the appendix TOC


\etocdepthtag.toc{mtappendix}
\etocsettagdepth{mtchapter}{none}
\etocsettagdepth{mtappendix}{subsection}
{
\parskip=0em
\tableofcontents
}


\clearpage




\section{Additional Related Works}\label{OldRelatedWorks}

% \paragraph{Challenges of Training Transformers \& LLMs.}
% Training transformers and LLMs is complicated by heavy-tailed stochastic gradient distributions with very large variance, often theoretically and empirically modeled as L\'{e}vy $\alpha$-stable processes~\cite{linearattentionisallyouneed,heavytail1,heavytail2,heavytail3,inprobsupreme,heavytailedclassimbalance,HighProbAdaGConv}. In such settings, non-adaptive optimization methods have been shown to destabilize during training due to the heavy-tailed nature of stochastic gradients inherent in large-scale models~\cite{inprobsupreme,HeavyTailedNoisePaper}. Similarly, such gradient behaviors also provably destabilize traditional averaging methods in distributed settings~\cite{EfficientAdaptiveFederatedOptimization}, highlighting the need for novel optimization algorithms tailored to these environments. 

% Recent advancements have explored centralized adaptive optimization techniques and robust gradient aggregation methods to mitigate the adverse effects of heavy-tailed noise, including gradient clipping~\cite{heavytail2, CentralClip, CentralClip5, highprobabilityunboundedvariance, highprobClip, CentralClip1} or adaptive clipping strategies~\cite{HighProbAdaGConv}. However, the complexities of handling heavy-tailed noise in distributed optimization settings often prevent these algorithms and their convergence bounds from extending to scenarios with multiple nodes training in parallel, which are essential for scalable training beyond the capacity of individual nodes. %However, due to the complexities involved in handling heavy-tailed stochastic gradient noise in distributed optimization settings, these algorithms and their convergence bounds do not easily extend to the setting in which multiple nodes participate in training in parallel, which can allow for large-scale training infeasible on any singular node. 
% Consequently, there remains a critical need for distributed optimization algorithms that are both computationally efficient and inherently robust to heavy-tailed stochastic gradients, particularly for training large-scale neural architectures. To our knowledge, developing an adaptive distributed algorithm with local updates (i.e., allowing multiple inner optimizer updates prior to outer optimizer synchronization) that converges under heavy-tailed stochastic gradient noise with convergence guarantees has remained an open challenge. %Our method addresses these gaps by introducing a two-layer optimization framework that interpolates between sign-SGD and adaptive methods for node resource efficiency, ensuring convergence with local updates, non-IID data distributions, and infinite-variance noise conditions.


%Previous solutions have primarily focused on centralized settings~\cite{heavytail2, CentralClip, CentralClip5, highprobabilityunboundedvariance, highprobClip, CentralClip1} or, within distributed environments, have assumed both bounded gradient variance and heavy-tails (cite!), which may not faithfully describe the nature of such heavy-tailed L\'{e}vy $\alpha$-stable distributions. %Additionally, some prior works (cite) have developed clipping-based distributed algorithms that converge under infinite variance, which we discuss in more detail in the next subsection. 

%Additionally, variance reduction methods and gradient noise modeling have been proposed to enhance stability and convergence in the presence of heavy-tailed distributions~\cite{HeavyTailedNoiseModel, VarianceReductionHeavyTail}. 
%Despite these efforts, many existing methods either introduce significant computational overhead or fail to fully address the instability caused by Levy-like gradient distributions. 




%Our method addresses these gaps by introducing a two-layer optimization framework that effectively interpolates between sign-sgd and adaptive methods, which converges even under the significantly more challenging local update conditions, under the non-IID setting with infinite-variance noise conditions.




\paragraph{Clipping for Stabilizing Training Dynamics.}\label{ClippingDistributedRelatedWorksAppendix} 
Due to its success in stabilizing model updates, gradient clipping has been extensively studied empirically~\cite{empirical1,empirical2,empirical3,empirical5} and theoretically~\cite{HighProbAdaGConv,HeavyTailedNoisePaper,theoretical1,theoretical4,GradClipCentralGeomPerspective,RevisitGradClipModern,inprobsupreme,highprobClip}. The majority of results study the centralized setting (e.g.,~\cite{CentralClip2, CentralClip3,CentralBddVarClip, CentralBddVarClip1,CentralClip4,CentralClip1plus,CentralClip5}), as moving to the distributed setting provides significant challenges such as multiple inner optimizer updates prior to outer optimizer synchronization. Additionally, it was shown that using a constant clip threshold can induce gradient bias, preventing the algorithm from ever converging~\cite{RevisitGradClipModern,GradClipCentralGeomPerspective}. Therefore, some works have attempted to circumvent this issue by debiasing via error feedback~\cite{Debias1,Debias2}. Other works in distributed optimization have imposed strong distributional stochastic gradient structures in the analysis. For instance,~\cite{angulardependencerestrictive}
assumes a well-behaved angular dependence between the stochastic and deterministic gradients throughout training, and~\cite{symmetricnoise_distributed} assumes symmetric gradient noise, almost surely bounded stochastic gradients, as well as homogeneous data. In contrast with these works, we do not impose any conditions on the noise nor data distributions except for bounded noise $\alpha$-moments for $\alpha \in (1,2)$. This also sharpens the sensitivity of our bounds to gradient distributions, as $\alpha$ may be selected as the minimal (or close to infimum) $\alpha$-moment value such that the moment is bounded.

There are some recent results studying the dynamics of heavy-tailed clipped-SGD in the distributed setting. The works~\cite{sun,highprobabilitydistributed,errorfeedback} study distributed optimization with no local updates, where global synchronization is done after every update which has connections with batched centralized training. In particular,~\cite{sun} studies the convergence of distributed clipped-sgd for strongly-convex objectives in the absence of a central server, where smaller nodes communicate with their neighbors according to a strongly connected graph. %Under clipping of the stochastic gradients given an $L_2$-norm constraint, projection, and averaging weights from nearby nodes, convergence is shown for strongly convex objectives. 
By contrast,~\cite{errorfeedback} proposes `smooth-clipping' the difference between a local gradient estimator and the local stochastic gradient (using a custom smoothed $L_2$ clipping function), which is shown to converge under only the integrability condition (finite first moment) for strongly convex objectives when assuming symmetric noise distributions. Finally, the work by~\cite{fatclip} studies the case with local updates, and is the closest in comparison to our algorithm. There, a so-called `FAT-Clipping' algorithmic framework is proven to attain convergence under $L_2$ clipping for heavy-tailed stochastic gradients. Two variants are studied, clipping per every local iteration as well as clipping once prior to global synchronization. It is shown that per-iteration clipping achieves faster speedup and better performance (compared against in our paper as the `Avg-$L_2Clip$' baseline in Table~\ref{tab:glue-results}). 

Our proposed clipping mechanism, $BiClip$, differs from these approaches by incorporating coordinate-wise clipping in conjunction with adaptivity in a nested setting. The clipping operations on the inner optimizers in TailOPT temper large gradient updates while amplifying smaller ones, complemented by the outer optimizer which enhances rarified signals through adaptive mimicry or adaptivity. An added advantage of TailOPT is significant communication efficiency, as we do not transmit preconditioners from the inner and outer optimizers under iterative local updates. Our analysis covers both convex and non-convex functions without additional assumptions on the noise distribution except for heavy-tailedness with potentially unbounded variance. It also holds for a variety of adaptive optimizers and different clipping methods.
%The $BiClip$ optimizer reported in our work may also be thought of as a generalization of $L_2$ clipping, which further `warms' the optimizer landscape to achieve more stable and resource-efficient training dynamics. Additionally, we complement this approach by implementing more rare signal-sensitive adaptive optimization strategies at the outer node layer.


\paragraph{Federated Learning.}\label{FLReviewAppendix}
Federated learning (FL) is a distributed learning paradigm designed to train machine learning models across multiple clients without requiring the transmission of raw data~\cite{mcmahan2017communication,TianReviewPaper,wang2021field}. This decentralized approach is particularly relevant in privacy-sensitive domains, such as healthcare and finance~\cite{finance1,finance2,healthcare1}, where data-sharing restrictions make centralized data aggregation impractical. In its basic form, FL involves a central server that coordinates the training process by distributing a global model to a subset of clients, which can range from a dozen in cross-silo settings (e.g., hospitals~\cite{healthcare2}, research institutions~\cite{institution1,institution2}, or datacenters~\cite{DiLoCo,DiLoCoAsynchronous,OpenDiLoCo}) to millions in cross-device scenarios (e.g., mobile phones~\cite{TianReviewPaper}). Each client performs local updates using stochastic gradient descent (SGD) on its own data and, after several local training steps, sends the aggregated models back to the server. The server then averages these updates to refine the global model. This training paradigm, commonly referred to as \textit{FedAvg}, has become the foundation for many federated learning algorithms~\cite{mcmahan2017communication,AdaptiveFederatedOptimization,FedNOVA}. Despite its effectiveness, FedAvg faces significant challenges, especially in heterogeneous environments where client data is non-IID~\cite{wang2021field}. Cross-device settings, for example, often exhibit highly diverse data distributions and stochastic gradients, as each client has access to only a small, biased subset of the overall data. These issues have motivated a rich body of research aimed at analyzing the behavior of learning algorithms under federated settings (e.g.,~\cite{AdaptiveFederatedOptimization}) to determine whether they can handle the complexities of real-world federated training, particularly in the presence of data heterogeneity and heavy-tails~\cite{sun,highprobabilitydistributed,errorfeedback,fatclip}. In particular, TailOPT may also have applications in federated learning (see Appendix~\ref{Appendix_nonIID_experiments} for experiments), especially when the local data shards induce heavy-tailed stochastic gradients. %\tian{Add reference to the experiments (in Appendix), as well} % \tian{do we have non-IID data experiments? need to point out why do we mention federated learning here, and explicitly connect the discussions in this paragraph with the contributions of our work}

%In contrast, cross-silo settings typically involve more structured data but face other hurdles, such as communication bottlenecks and privacy constraints. Moreover, FedAvg assumes equal participation from all clients, yet in practice, client availability and network conditions can vary widely, particularly in cross-device federated learning. As a result, the global model may converge slowly or diverge altogether, particularly under conditions of unbalanced data or client participation. These issues have motivated a rich body of research aimed at analyzing the behavior of learning algorithms under federated settings to determine whether they can handle the complexities of real-world federated training,  particularly in the presence of data heterogeneity. developing more adaptive and robust variants of FedAvg that can handle the complexities of real-world federated learning systems, particularly in the presence of data heterogeneity and communication constraints.

\paragraph{Convergence Bounds under Heavy-Tailed Gradient Noise.}

In general, there are two primary types of convergence bounds: in-probability bounds~\cite{highprob1,highprob2,highprob3,inprobsupreme,highprobabilityunboundedvariance,highprobClip,CentralClip5,highprobabilitydistributed} and in-expectation bounds~\cite{FedAMS, FedAvgConvOnNonIID, AdaptiveFederatedOptimization, AdaAlter, FedNOVA, MIMEpaper, delayedpreconditioner, adadps}. Each type has distinct characteristics that complement the other. In-probability bounds provide an upper limit on the number of timesteps required to achieve model parameters $x$ such that $\mathbb{P}\{\mathcal{M}(x) \leq \varepsilon\} \geq 1 - \delta$ for a given evaluation metric $\mathcal{M}(x)$ (e.g., $\min_{t \in {1, \dots, T}} |\nabla F(x_t)|$). Here, $\delta$ represents the failure probability, or confidence level, of the bound. As $\delta \to 0^+$, the required communication complexity or number of timesteps diverges, as expected. The key challenge is to mitigate this divergence as effectively as possible through novel algorithm designs or refined mathematical analysis, such as by deriving a polylogarithmic dependence on $\delta$ rather than a more severe inverse power-law dependence.

By contrast, in-expectation bounds complement in-probability bounds by ensuring that convergence to an optimal point is guaranteed under expectations, without a confidence level that determines the success or failure of the algorithm. However, the majority of such analyses assume a bounded noise variance, typically denoted by an upper bound $G$ or $\sigma$, which appears as constants in the upper bound of the communication complexity required for convergence~\cite{inprobsupreme,CentralBddVarClip, CentralBddVarClip1}. 
Due to this dependence, some works (e.g., those studying high-probability results~\cite{highprob2, inprobsupreme, CentralClip5}) argue that in-expectation bounds are insensitive to the underlying distributional structures of the stochastic gradients, due to being compressed or approximated away by $G$.
Relaxing this assumption is particularly challenging because unbounded noise adds significant uncertainty to controlling model updates.  Furthermore, works such as~\cite{EfficientAdaptiveFederatedOptimization} have demonstrated that under stochastic gradient descent, unbounded noise is instantaneously transmitted to the model parameters in both centralized and distributed settings, leading to severe instability and ensuring divergence in expectation. Such results elucidate the additional difficulties induced by efforts to remove the bounded gradient condition. %additional challenges

%Is it possible to remove this dependence on $G$? However, as demonstrated by works such as~\cite{EfficientAdaptiveFederatedOptimization}, under stochastic gradient descent, unbounded noise is instantaneously transmitted to the model parameters in both centralized and distributed settings, leading to severe instability and ensuring divergence in expectation.

A recent work by~\cite{highprobabilityunboundedvariance} provides the first high-probability results under unbounded variance for clipped-SGD applied to star-convex or quasi-convex objectives in a distributed setting without local updates. Their analysis reveals an inverse logarithmic dependence on the confidence level. % prompting the question of what happens as $\delta$ approaches zero: will the method stabilize or diverge? This naturally motivates a broader inquirycan we derive in-expectation results without the bounded variance assumption, \textit{with} local updates, providing a complementary counterpart to the high-probability bounds? %To date, no papers have addressed this question, and this work also fills that gap. 
In this paper, we develop a more efficient and general TailOPT framework, and study the dynamics of TailOPT under heavy-tailed stochastic gradient distributions. Specifically, we provide the in-expectation convergence guarantees under infinite variance and local updates for potentially non-convex functions, offering new bounds that are more sensitive to distributional structures of mini-batch noise. %Unlike traditional in-expectation bounds, which rely on the bounded noise variance $G$, our bounds are based on $\alpha$-moment conditions for $\alpha \in (1,2)$. Our analysis is carried out in the distributed setting with local updates, ensuring communication efficiency.

\section{Future Directions and Possible Extensions}\label{Extended_Conclusion_Appendix}

% \clearpage
% \paragraph{Future Directions.} 
Efficient estimation of the clipping thresholds $d_t$ and $u_t$ in $BiClip$ remains an open avenue for research. One potential approach is to segment the thresholds into coordinate subsets (e.g., row-wise or column-wise), similar to the memory-efficient partitioning strategies employed in approximate optimizers such as SM3~\cite{anil2019memory}. Alternatively, autonomous selection of $u_t$ and $d_t$ based on initial statistics or bespoke estimators could provide practical solutions. Our experiments indicate that coordinate-wise $BiClip$, rather than standard $L_2$ clipping, achieves the benefits of adaptive optimization without incurring any additional memory overhead compared to SGD. Notably, methods like Adam at least double memory usage, whereas $BiClip$ maintains parity with non-adaptive methods. This suggests that uniformly amplifying small updates can contribute to optimization efficiency. Furthermore, layer-wise $BiClip$ can be readily generalized, with proofs extending straightforwardly.

% The challenges posed by heavy-tailed noise are further exacerbated in settings with non-IID data partitions, which introduce additional variability in gradient distributions. %Appendix~\ref{IntuitionsonHeavyTailedNoise} provides brief heuristics on the interplay between tokenization and heavy-tailed noise.
% These challenges have catalyzed a growing body of research aimed at developing algorithms that provably converge under heavy-tailed conditions. Establishing a robust theoretical framework for such algorithms is critical for aligning with the optimization dynamics observed in modern architectures, such as transformers, where heavy-tailedness is a prominent characteristic. %Our framework is also closely related to, and has applications in, federated learning. We provide a self-contained literature review in Appendix~\ref{OldRelatedWorks}. 
% \tian{this paragraph is not future directions? or if we want to frame it as future directions, can we say more on what is undone there [my understanding is that we already considered non-iid data in the convergence?]} 

Another intriguing direction for future research is the integration of Adam on top of $BiClip$ to enhance optimization stability in centralized training or distributed training for either inner or outer optimizers. Notably, when employing the Adam optimizer, some studies apply $L_2$ clipping to gradients prior to synthesizing Adam updates to improve stability of the optimization dynamics~\cite{CentralClipAdaptive}. A natural extension of this approach is to substitute $BiClip$ for $L_2$ clipping before passing updates to the adaptive optimizer. This modification could not only enhance stability but also potentially reduce dependence on the adaptivity parameter, offering a more robust optimization framework.



\section{Convergence of TailOPT}\label{AdaptiveBiClipSGD}

In this section, we rigorously analyze the convergence of TailOPT under heavy-tailed noise, beginning with the simpler case of Avg-$L_2Clip$ to enhance readability before progressively advancing to more sophisticated TailOPT variants incorporating $BiClip$ and other adaptive outer optimizers. We first establish the foundational convergence proof for Avg-$L_2Clip$ in Appendix~\ref{AppendixSectionforConvexConvergenceCrossSilo}, which serves as the basis for subsequent analyses. The proof for Avg-$L_2Clip$ studies a virtual history of model weights synthesized by inner optimizers, which is inaccessible in real-world settings except when the model updates are communicated to the outer optimizer. However, by analyzing the virtual history, we are able to attain convergence of a moving average of accessible model weights to the optimum, which can be materialized in practice. In Appendix~\ref{AppendixSectionforConvexConvergenceCrossDevice}, we extend this proof to settings with partial participation and failing compute nodes, examining the resulting dynamics under heavy-tailed noise. 

In Appendix~\ref{BiSquareAppendix}, we further generalize the analysis to the $Bi^2Clip$ instantiation, where $BiClip$ is applied to both the inner and outer optimizers. Notably, $Bi^2Clip$ encompasses Avg-$BiClip$ as a special case under specific hyperparameter choices, which in turn subsumes Avg-$L_2Clip$. Finally, in Appendices~\ref{AdaGrad_BiClip_Appendix},~\ref{RMSProp_BiClip_Appendix}, and~\ref{AdamBiClipAppendixSubsection}, we investigate the convergence properties of TailOPT when the outer optimizer is instantiated with Adagrad, RMSProp, and Adam, respectively.


\subsection{Convergence of Avg-$L_2Clip$}\label{AppendixSectionforConvexConvergenceCrossSilo}
% \tian{can we put it under the same section of Convergence of TailOPT because it is one instantiation}
% \tian{change the name}

We aim to model contemporary, large-scale neural network training across multiple powerful compute nodes (datacenters or GPU clusters), in which data is typically preprocessed IID to optimize for training. However, for fullest generality, we conduct our theoretical analysis in the more challenging, non-IID setting. Our setup is identical to Section~\ref{problemformulation}, with some added notation. 
% In distributed optimization paradigms, the global objective is constructed by taking a weighted average over the compute node objectives $F_i(x)$. In scenarios where node data sizes are unbalanced or sampling probabilities vary, the objective becomes:
% \begin{equation}\label{unbalancedcaseappendix}
%     F(x) = \sum_{k=1}^N p_i F_i(x),
% \end{equation}
% where $p_i$ is proportional to the local data size or sampling probability of node $k$. Here, $F_i(x)$ is defined as $\mathbb{E}_{\xi \sim \mathcal{D}_{k}}\left[F_i(x, \xi)\right]$, where $F_i(x, \xi) = F_i(x) + \langle \xi, x \rangle$ 
% represents the stochastically realized local objective. $\mathcal{D}_{k}$ is the noise distribution of node $k$, and we have that $\nabla F_i(x^t_{i,0},\xi^t_{i,0}) = \nabla F(x^t_{i,0}) + \xi^t_{i,0}$. Here, $x^t_{i,0}$, $\xi^t_{i,0}$ are the model weights and gradient noise of node $k$ at timestep $t$. 
We denote $x^*$ to represent the global optimum of $F(x)$ with a minimum value $F^* = F(x^*)$, and additionally, we let $x_i^*$ be the global optimum of $F_i(x) = \mathbb{E}_{\xi} [F_i(x,\xi)]$, with a minimum value $F_i^* = F(x_i^*)$. %\tian{need to clean up this part, re-use the setup in the main text}

% \tian{We use the same setup in the main text, state} Also change the notation

For model weight or stochastic gradient averages, we use the following notation 
$$
\begin{aligned}
    &\overline{x}_t=\sum_{i=1}^N p_i x^t_{i,0}, \quad g_t=\sum_{i=1}^N p_i \cdot Clip(c_t,\nabla F_i\left(x^t_{i,0}, \xi^t_{i,0}\right)), \quad Clip(c,y) := \min \left\{1,\frac{c}{\|y\|}\right\}y.
\end{aligned}
$$
The use of the notation $x_{i,0}^t$ instead of $x_{i}^t$ carefully reflects the flow of the proof, which studies a `virtual synchronization' of the model weights synthesized by the inner optimizer at each time $t \in [T]$ (see Algorithm~\ref{L2ClipSGD}). In other words, we first analyze the virtual average $\overline{x}_t$ which is not materially realized except at outer optimizer synchronization steps, before modifying the proof to procure a moving average of weights which is solely dependent on those communicated to the outer optimizer, which can now be obtained. 

We now present some assumptions used in the convergence analysis for this section. We take the model weight projection domain to be $\mathcal{X} = \mathcal{B}(0,B) \subset \mathbb{R}^d$, where $\mathcal{B}(0,B)$ is the closed ball centered at the origin with radius $B$. Clearly, $B>0$ needs to be large enough to contain $x^{*}, x_{i}^* \in \mathcal{X}$ for convergence. However, we note that the convergence analysis holds for $\mathcal{X}$ any large enough compact, convex set. % The proof of convergence under heavy-tailed noise is carried out via analyzing a history of virtual averages of participating compute node models, which is only periodically available at each outer optimizer synchronization step. 

\begin{paragraph}{Assumption 3 ($\mu$-strong convexity).} 
For all $x, y \in \mathcal{X}$ and $i \in [N]$, $F_i(x)$ satisfies $F_i(x) \ge F_i(y) + \langle x-y, \nabla F_i(y) \rangle + \mu_i \|x-y\|^2/2$.
\end{paragraph}

One motivation behind Assumption 3 is that while the optimization of DNNs is a non-convex problem~\cite{DNNnonconvex},~\cite{DNNapproximatelyconvexoverSGD} observe that loss surfaces are often approximately convex in practice, over a single optimization trajectory. Additionally, modern training paradigms, such as the fine-tuning of foundation models, have been empirically reported to belong to a shared convex loss basin~\cite{modelsoup,SWA}. We note that Proposition~\ref{preservesmoothing} shows that gradient perturbations do not affect dominance of nor over second order approximations, which preserves the values of $L, \mu$. 

% For completeness, we also formally include the definition of heavy-tailedness 
% defined in prior works studying the training dynamics of attention-based models such as transformers~\cite{EfficientAdaptiveFederatedOptimization}:
% \begin{definition}\label{heavytaileddefinition}
% A random variable $\xi \sim \mathcal{D}$ follows a \textbf{heavy-tailed} distribution if there exists $\alpha \in (1,2)$ such that the $\alpha$-moment is finite. That is, the stochastic gradient noise $G(x)-\nabla F(x)$  is Heavy-Tailed if  $\exists \alpha \in (1,2)$ such that $\mathbb{E}\left[\|G(x)-\nabla F(x)\|^{\alpha}\right] \le G^\alpha$ for $G > 0$. Here, $G(x)$ is the stochastic gradient under model weights $x$, and $\nabla F(x)$ the full deterministic gradient. 
% \end{definition}
% \tian{do we use this definition or the definition is the main text? the def in the main text is weaker?}

Gradient clipping is a widely adopted technique to stabilize model updates by mitigating the impact of large gradients~\cite{theoretical1,theoretical4,GradClipCentralGeomPerspective,RevisitGradClipModern}. The $Clip(\cdot)$ operator rescales the gradient uniformly to ensure it remains below a predefined threshold. This procedure is mathematically equivalent to applying a dynamically adjusted, lower learning rate when large stochastic gradients are encountered. Another related technique is projection, which operates in the model weight space rather than the gradient space, effectively stabilizing the model parameters themselves instead of acting on the updates. These observations motivate Algorithm~\ref{L2ClipSGD}, which may be interpreted as dynamically modulating the learning rates as well as backtracking toward the model origin $\overline{0}$ when heavy-tailed stochastic gradient updates are realized.

\begin{algorithm}
\caption{Avg-$L_2Clip$}\label{L2ClipSGD} % \tian{should it be $Avg-L_2Clip$?}
\begin{algorithmic}[1]
\REQUIRE Initial model $x_1$, 
learning rate schedule $\eta_t$, clipping schedule $c_t$ \\ Synchronization timestep $z \in \mathbb{Z}_{>0}$, projection domain $\mathcal{X}$
\FOR{$t = 1, \dots, T$} 
    \FOR{each node $i \in [N]$}
        \STATE Draw minibatch gradient $g^t_{i,0} = \nabla F_i(x^t_{i,0}, \xi^t_{i,0})$
        \STATE $x^{t+1}_{i,0} \leftarrow  x^t_{i,0} - \eta_t \cdot Clip(c_t,g^t_{i,0})$ 
    \ENDFOR
    \STATE \textbf{if }$t-1 \in z \cdot \mathbb{Z}_{\ge 0}:$ \ 
    \STATE \ \ \ \ $ x^{t+1}_{i,0} \leftarrow \operatorname{Proj}_{\mathcal{X}}\left(\sum_{i \in [N]} p_i x^{t+1}_{i,0}\right) $, for $\forall i \in [N]$
\ENDFOR
\end{algorithmic}
\end{algorithm}

%\begin{algorithm}
%\caption{Projected $L_2$-$Clip$ SGD}\label{L2ClipSGD_NonProofNotation}
%\begin{algorithmic}[1]
%\REQUIRE Initial model $x_1$, 
%learning rate schedule $\eta_t$, clipping schedule $c_t$ \\ Synchronization timestep $z \in \mathbb{Z}_{>0}$, projection domain $\mathcal{X}$
%\FOR{$t = 1, \dots, T$} 
%    \FOR{each client $i \in [N]$ in parallel}
%    \STATE $x_{i,0}^t \leftarrow x_t$
%    \FOR{each local step $k \in [z]$}
%        \STATE Draw minibatch gradient $g^t_{i,k} = \nabla F_i(x^t_{i,k}, \xi^t_{i,k})$
%        \STATE $x^{t+1}_{i,k} \leftarrow  x^t_{i,k} - \eta_t \cdot Clip(c_t,g^t_{i,k})$ 
%        \ENDFOR
%    \ENDFOR
%    \STATE $ x_{t+1} \leftarrow \operatorname{Proj}_{\mathcal{X}}\left(\sum_{k \in [N]} p_i x^{t}_{i,z}\right) $
%\ENDFOR
%\end{algorithmic}
%\end{algorithm}

%We present convergence bounds for this algorithm in Appendix~\ref{AppendixSectionforConvexConvergenceCrossSilo}, alongside further detailed analysis. 
Theorem~\ref{ConvexConvergenceThmAppendixVer} demonstrates that distributed Avg-$L_2Clip$ converges in expectation under heavy-tailed noise, despite potential clipping-induced bias. We also offer the first proof demonstrating convergence under an extension of these results to accommodate failing nodes (e.g., partial datacenter participation) for additional utility in Appendix~\ref{AppendixSectionforConvexConvergenceCrossDevice}. To proceed with the analysis, we first provide a simple proposition:

%\begin{theorem}[Simplified]\label{ConvexConvergenceThm}
%Let the clipping threshold in $L_2$-$Clip$ SGD (Algorithm~\ref{L2ClipSGD}) satisfy $c_t = c\eta_t^{\gamma}$ for $c > 0$ and $1/2 > \gamma > 0$. Decay the learning rate with schedule $\eta_t = r/(t+1)$ for $r>2/\mu$, where $\mu = \min_{k \in [N]} \mu_k$ and $L = \max_{k \in [N]} L_k$. Then, we have for $\alpha = \min_{k \in [N]} \alpha_k$ that
%\begin{equation*}
%\|F(\tilde{x}_T) - F(x^*)\| \le \mathcal{O}\left(t^{-\omega}\right), \quad \omega = \min\{1-2\gamma, 1 - (2-2\alpha)\gamma, 1-(2-\alpha)\gamma,2- 2\gamma\}
%\end{equation*}
%where $\alpha = \min_{k \in [N]}$ and $\tilde{x}_T:= \sum_{t=1}^T t\mathbb{E}[\overline{x}_t]/T(T+1)$ is a decaying moving average of the $\overline{x}_t$.
%\end{theorem}

%Gradient clipping is a widely adopted technique to stabilize model updates by mitigating the impact of large gradients. The $Clip(\cdot)$ operator rescales the gradient uniformly to ensure it remains below a predefined threshold. This procedure is mathematically equivalent to applying a dynamically adjusted, lower learning rate when large stochastic gradients are encountered. Another related technique is projection, which operates in the model weight space rather than the gradient space, effectively stabilizing the model parameters themselves instead of acting on the updates. These observations motivate Algorithm~\ref{L2ClipSGD} as follows:

%The pseudocode is written to be amenable with this flow of the proof for readability.

\begin{proposition}\label{preservesmoothing}
If $F_i(x)$ is $\mu$-strongly convex (or $L$-smooth), then so is $F_i(x,\xi)$ for the identical $\mu$ (or $L$). 
\end{proposition}
\begin{proof}
The proof is simple. By $\mu$-strong convexity or $L$-smoothness, we have
\begin{equation*}
\begin{aligned}
F_i(x) &\ge F_i(y) + \langle x-y,\nabla F_i(y) \rangle + \frac{\mu}{2}\|x-y \|^2, \\
F_i(x) &\le F_i(y) + \langle x-y,\nabla F_i(y) \rangle + \frac{L}{2}\|x-y \|^2.
\end{aligned}
\end{equation*}
Then, note the following equations for $\langle \xi, x \rangle$:
\begin{equation*}
\begin{aligned}
\langle \xi, x \rangle & \ge \langle \xi, y \rangle + \langle x-y,\xi \rangle, \\
\langle \xi, x \rangle & \le \langle \xi, y \rangle + \langle x-y,\xi \rangle.
\end{aligned}
\end{equation*}
Collecting these inequalities give the result.
\end{proof}

While clipping offers the benefit of stabilization, it introduces complexities that significantly complicate the convergence analysis. In particular, clipping induces a non-zero bias on the stochastic gradients, rendering them to be no longer unbiased estimators of the true gradient. Prior work, such as~\cite{GradClipCentralGeomPerspective}, presents illustrative examples where using a fixed clipping threshold can bias the gradient dynamics to the extent that the optimum is no longer a steady state, preventing SGD from ever converging. Furthermore, unlike in previous analyses, our work also considers scenarios involving distributions with infinite variance, where the clipping bias is exacerbated by the presence of heavy tails. Despite these challenges, Theorem~\ref{ConvexConvergenceThmAppendixVer} demonstrates that with appropriately chosen (increasing) clipping and (decreasing) learning rate schedules, convergence of Algorithm~\ref{L2ClipSGD} is nevertheless attainable in expectation. 
\begin{theorem}\label{ConvexConvergenceThmAppendixVer}
Let Assumptions 1-3 hold, and the clipping threshold in Avg-$L_2Clip$ (Algorithm~\ref{L2ClipSGD}) satisfy $c_t = c\eta_t^{\gamma}$ for $c > 0$ and $1/2 > \gamma > 0$. Decay the learning rate with schedule $\eta_t = r/(t+1)$ for $r>2/\mu$, where $\mu = \min_{k \in [N]} \mu_k$ and $L = \max_{k \in [N]} L_k$. Then, we have for $\tilde{x}_T:= \sum_{t=1}^T t\mathbb{E}[\overline{x}_t]/T(T+1)$ that %\tian{need to say under what assumptions. do we need the two assumptions in the main text?}
\begin{equation*}
F(\tilde{x}_T) -F(x^*) \le \Psi_1 + \Psi_2 + \Psi_3 + \Psi_4,
\end{equation*}
where
\begin{align*}
    &\Psi_1 = \frac{rc^2 T^{2\gamma + 1}}{(4 \gamma+2) T(T+1)},\\
    &\Psi_2 = \frac{(M^\alpha + B^\alpha)^2 c^{2-2\alpha} (T^{(2-2\alpha)\gamma + 1} + 1)}{2(\mu-2/r) ((2-2\alpha)\gamma + 1) T(T+1)}, \\
    &\Psi_3 = \frac{c^{2-\alpha}rzu (M^\alpha + B^\alpha) L T^{(2-\alpha)\gamma + 1} }{(\mu-2/r)((2-\alpha)\gamma + 1)T(T+1)},\\
    &\Psi_4 = \frac{r^2c^2z^2u^2L^2 (T^{2\gamma}+1)}{4\gamma(\mu-2/r)T(T+1)}.
\end{align*}
Here, we have used the notation
\begin{equation*}
M= \sqrt{\max_{k \in [N], x \in \mathcal{\tilde{X}}}\frac{2L^2}{\mu}(F_i(x) - F_i(x_{i}^*))}, \quad \alpha = \min_{k \in [N]} \alpha_k,\quad B = \max_{k \in [N]} B_k, \quad u= \frac{z+1}{2},
\end{equation*}
where $\tilde{\mathcal{X}}$ is a compact domain constructed by a uniformly closed extension of $\mathcal{X}$ with $L_2$ distance $\sum_{t=1}^z rct^{\gamma-1}$. 
\end{theorem}

\begin{proof}
Let us bound the distance between the averaged model weights $\overline{x}_t$ and the global optimum $x^*$. Assume that $t \in z \cdot \mathbb{Z}$. We consider the following function 
\begin{equation*}
f(t) = \|x^*-\operatorname{Proj}_{\mathcal{X}}(\overline{x}_t - \eta_t g_t) + t(- \overline{x}_t + \eta_t g_t + \operatorname{Proj}_{\mathcal{X}}(\overline{x}_t - \eta_t g_t)) \|^2,
\end{equation*}
for which 
\begin{equation*}
f^\prime (0) = 2 \langle x^*-\operatorname{Proj}_{\mathcal{X}}(\overline{x}_t - \eta_t g_t), - \overline{x}_t + \eta_t g_t + \operatorname{Proj}_{X}(\overline{x}_t - \eta_t g_t) \rangle.
\end{equation*}
Now, consider the function
\begin{equation*}
g(t) = \| (1-t)\operatorname{Proj}_{\mathcal{X}} (\overline{x}_t - \eta_t g_t) + t\operatorname{Proj}_\mathcal{X}(x^*) - \overline{x}_t + \eta_t g_t\|
\end{equation*}
By the projective property, 
\begin{equation*}
g(t) \ge \| \operatorname{Proj}_{X}(\overline{x}_t - \eta_t g_t) - (\overline{x}_t - \eta_t g_t) \|.
\end{equation*}
holds for $t \in [0,1]$ via convexity of $\mathcal{X}$. Additionally, $g(t)^2$ meets its minimum at $t=0$. Therefore, we have that $\mathrm{d} g(t)^2 / \mathrm{d}t_{t=0} \ge 0$ due to $g(t)^2$ being quadratic with respect to $t$. Noting that $f^\prime(0) = \mathrm{d} g(t)^2 / \mathrm{d}t|_{t=0}$, we have that $f(t)$ is monotonically increasing for $t \ge 0$, again due to properties of a quadratic. Then, $f(1) \ge f(0)$ gives that 
\begin{equation*}
\left\|\operatorname{Proj}_\mathcal{X}\left(\overline{x}_t-\eta_t g_t\right)-x^{*}\right\|^2 \le \left\|\overline{x}_t-\eta_t g_t-x^{*}\right\|^2.
\end{equation*}
Therefore, we may conclude
\begin{equation*}%\label{firstequation}
\begin{aligned}
\left\|\overline{x}_{t+1}-x^{*}\right\|^2  &=\left\|\sum_{i=1}^N p_i\operatorname{Proj}_\mathcal{X}\left(\overline{x}_t-\eta_t g_t\right)-x^{*}\right\|^2=\left\|\operatorname{Proj}_\mathcal{X}\left(\overline{x}_t-\eta_t g_t\right)-x^{*}\right\|^2 \\
&\le \left\|\overline{x}_t-\eta_t g_t-x^{*}\right\|^2=\left\|\overline{x}_t-x^{*}\right\|^2 -2 \eta_t\left\langle\overline{x}_t-x^{*}, g_t\right\rangle + \eta_t^2\left\|g_t\right\|^2\\
& = \left\|\overline{x}_t-x^{*}\right\|^2 \underbrace{-2 \eta_t\left\langle\overline{x}_t-x^{*}, g_t-\nabla F(\overline{x}_t)\right\rangle}_{A_1} \underbrace{-2 \eta_t\left\langle\overline{x}_t-x^{*}, \nabla F(\overline{x}_t)\right\rangle}_{A_2} + \underbrace{\eta_t^2\left\|g_t\right\|^2}_{A_3}.
\end{aligned}
\end{equation*}
Note that the final inequality LHS $\le$ RHS also holds for $t \notin z \cdot \mathbb{Z}$. In bounding $A_2$, we aim to derive a term that decays $\left\|\overline{x}_t-x^{*}\right\|^2$ by inducing a coefficient $(1-\tilde{c}\eta_t)\left\|\overline{x}_t-x^{*}\right\|^2$ for some $\tilde{c} > 0$ to be determined. By $\mu$-strong convexity of $F(x)$,  
\begin{equation*}
\begin{aligned}
& F(x^*) \ge F(\overline{x}_t) - \langle \overline{x}_t - x^*,\nabla F_i(\overline{x}_t)\rangle + \frac{\mu}{2} \|x^* - \overline{x}_t\|^2 \\
& \implies -\left(F(\overline{x}_t) - F(x^*) \right) - \frac{\mu}{2} \|\overline{x}_t - x^*\|^2 \ge - \langle \overline{x}_t - x^*,\nabla F(\overline{x}_t)\rangle.
\end{aligned}
\end{equation*}
To bound $A_1$, we consider conditional expectations
\begin{equation*}
-2 \eta_t\left\langle\overline{x}_t-x^{*}, \mathbb{E}_{t}[g_t]-\nabla F(\overline{x}_t)\right\rangle\le 2 \eta_t \|\overline{x}_t-x^{*} \| \| \mathbb{E}_{t}[g_t]-\nabla F(\overline{x}_t)\|,
\end{equation*}
where $\mathbb{E}_t[\cdot]$ conditions on all realizations up to time $t$. Unraveling definitions gives
\begin{equation}\label{preparatorybiasbound}
\begin{aligned}
\|\mathbb{E}_{t}[g_t]-\nabla F(\overline{x}_t)\| &= \|\sum_{i \in [N]}p_i (\mathbb{E}_t[Clip(c_t, \nabla F_i(x^t_{i,0},\xi^t_{i,0}))] - \nabla F_i(x^t_{i,0}) + \nabla F_i(x^t_{i,0}) - \nabla F_i(\overline{x}_t))\|\\
&\le \sum_{i \in [N]}p_i \| \mathbb{E}_t[Clip(c_t, \nabla F_i(x^t_{i,0},\xi^t_{i,0})) - \nabla F_i(x^t_{i,0},\xi^t_{i,0}))]\| + \sum_{i \in [N]}p_i \|\nabla F_i(x^t_{i,0}) - \nabla F_i(\overline{x}_t)\| \\
&\le \sum_{i \in [N]}p_i  \underbrace{\mathbb{E}_t[\|Clip(c_t, \nabla F_i(x^t_{i,0},\xi^t_{i,0})) - \nabla F_i(x^t_{i,0},\xi^t_{i,0}))\|]}_{A_4} + \sum_{i \in [N]}p_i L \|x^t_{i,0} - \overline{x}_t\|,
\end{aligned}
\end{equation}
where the second line used Jensen and triangle inequality, and the third line used $L$-smoothness as well as Jensen. Now, we note that clipping biases the expectation in $A_4$, and we seek to ease out a measure of the clipping bias. For this purpose, we quantify the $\alpha$-moment of the stochastic gradient:
\begin{equation*}
2^\alpha \mathbb{E}_{t}\left\|\frac{\nabla F_i(x) + \xi^t_{i,0}}{2} \right\|^\alpha \le 2^{\alpha - 1} \left(\mathbb{E}_{t}\left\| \nabla F_i(x)\right\|^\alpha + \mathbb{E}_{t}\left\|\xi^t_{i,0} \right\|^\alpha\right) \le 2^{\alpha - 1} \left(\left\| \nabla F_i(x)\right\|^\alpha + B_i^\alpha\right).
\end{equation*}
Here, we have used the notation $B_i < \infty$ for readability, but strictly speaking this is not identical to the $B_i$ given in Assumption 2 as $\alpha := \min_{i\in [N]}\alpha_i$. Finally, the projection in each outer optimizer synchronization step ensures that the $x^t_{i,0}$ remain in a compact set $\tilde{\mathcal{X}}$. Therefore, to bound gradients, we use $L$-smoothness and $\mu$-strong convexity of $F_i(x)$ as follows: 
\begin{equation*}
\left\| \nabla F_i(x) \right\|^2 \le L^2 \left\| x - x_{i}^*  \right\|^2, 
\end{equation*}
where $x_{i}^*$ is the optimum of $F_i(x)$. Then, convexity gives that 
\begin{equation*}
F_i(x) \ge F_i(x_{i}^*) + \frac{\mu}{2} \| x - x_{i}^*\|^2,
\end{equation*}
from which we conclude
\begin{equation}\label{boundedgrad}
\left\| \nabla F_i(x) \right\|^2 \le \frac{2L^2}{\mu}(F_i(x) - F_i(x_{i}^*)) \le M^2:= \max_{k \in [N], x \in \mathcal{\tilde{X}}}\frac{2L^2}{\mu}(F_i(x) - F_i(x_{i}^*)).
\end{equation}
Piecewise continuity of $F_i(x)$ is clear due to the existence of $\nabla F_i(x)$. Therefore, 
%Due to piecewise continuity of $\nabla F_i(x)$, we may thus conclude
\begin{equation*}
\mathbb{E}_{t}\left\|\nabla F_i(x^t_{i,0}) + \xi^t_{i,0} \right\|^\alpha \le \frac{(M^\alpha + B^\alpha)}{2}.% := \frac{(\max_{k \in [N], x \in \mathcal{\tilde{X}}} F_i(x))^\alpha + (\max_{k \in [N]}B_k)^\alpha}{2}.
\end{equation*}

Now, note that if $\|\nabla F_i(x^t_{i,0},\xi^t_{i,0}) \| \le c_t$, clipping has no effect in $A_4$. Thus, we focus on the case $\|\nabla F_i(x^t_{i,0},\xi^t_{i,0}) \| > c_t$. Additionally, clipping only downscales each stochastic gradient by a scalar, which preserves direction. Therefore, 
\begin{equation}\label{clipbiasboundwithalpha}
\begin{aligned}
A_4 & = \mathbb{E}_t\left[\|Clip(c_t, \nabla F_i(x^t_{i,0},\xi^t_{i,0})) - \nabla F_i(x^t_{i,0},\xi^t_{i,0}))\| \cdot \chi \left( \|\nabla F_i(x^t_{i,0},\xi^t_{i,0}) \| > c_t\right)\right]\\
& \le \mathbb{E}_t\left[\|\nabla F_i(x^t_{i,0},\xi^t_{i,0})\| \cdot \chi \left( \|\nabla F_i(x^t_{i,0},\xi^t_{i,0}) \| > c_t\right)\right] \\
& \le \mathbb{E}_t\left[\|\nabla F_i(x^t_{i,0},\xi^t_{i,0})\|^{\alpha} \cdot \|\nabla F_i(x^t_{i,0},\xi^t_{i,0}))\|^{1-\alpha} \cdot \chi \left( \|\nabla F_i(x^t_{i,0},\xi^t_{i,0}) \| > c_t\right)\right] \le (M^\alpha + B^\alpha) c_t^{1-\alpha}. 
\end{aligned}
\end{equation}
Putting these inequalities together, we obtain as an intermediary step for $a > 0$:
\begin{equation*}
\begin{aligned}
A_1 &\le 2 \eta_t \|\overline{x}_t-x^{*} \| ((M^\alpha + B^\alpha) c_t^{1-\alpha} + \sum_{i \in [N]}p_i L \|x^t_{i,0} - \overline{x}_t\|)\\
&\le \mu a \eta_t \|\overline{x}_t-x^{*} \|^2 + \frac{\eta_t}{\mu a} ((M^\alpha + B^\alpha) c_t^{1-\alpha} + L \sum_{i \in [N]}p_i \|x^t_{i,0} - \overline{x}_t\|)^2.
\end{aligned}
\end{equation*}
Thus, our next step is to ease out $\|x^t_{i,0}-\overline{x}_t \| = \mathcal{O}(\eta_t)$. For this purpose, our intuition is that the drift in model weights from local updates are bounded by the update size, as well as by taking a maximum of $z$ local steps after global  synchronization. Therefore, we naturally consider the timestep $t_s(t)$ of the latest synchronization round up to $t$, and observe that if the random variable $X:= x^t_{i,0}-\overline{x}_{t_s}$, then $\mathbb{E}_{k}[X] = \overline{x}_{t}-\overline{x}_{t_s}$. Noting that the variance of $X$ is no greater than its second moment, we proceed as follows via telescoping:
\begin{equation}\label{boundedclientdrift}
\begin{aligned}
\mathbb{E}_{k}[\|x^t_{i,0}-\overline{x}_t \|^2] &= \sum_{i=1}^N p_i \|x^t_{i,0}-\overline{x}_t \|^2 =  \mathbb{E}_{k}[\|X-\mathbb{E}_{k}[X]\|^2] \\
&\le \mathbb{E}_{k}[\|X\|^2] = \sum_{i=1}^N p_i \|x^t_{i,0}-\overline{x}_{t_s} \|^2 \\
&= \sum_{i=1}^N p_i \left\|x^t_{i,0} + \sum_{\tilde{t}=t_s+1}^{t-1} (-x_{\tilde{t}}^{k} + x_{\tilde{t}}^{k}) - \overline{x}_{t_s} \right\|^2 \\
&\le \sum_{i=1}^N p_i (t-t_s-1)^2 \max_{t^{\prime} \in [t_s,t]} \eta_{t^{\prime}}^2 \|Clip(c_t^{\prime}, \nabla F_i(x^t_{i,0},\xi^t_{i,0})) \|^2 \\
&\le \sum_{i=1}^N p_i z^2 \eta_{t_s}^2 c_t^2 = z^2 \eta_{t_s}^2 c_t^2 \le z^2 u^2 \eta_{t}^2 c_t^2.
\end{aligned}
\end{equation}
The final inequality was obtained by noting that $\eta_t \to 0^+$ monotonically from above and that $c_t \ge c_{t-1}$. The above holds for all $t \in \mathbb{Z}_{\ge 0}$, as if $t$ is a synchronization step, $\mathbb{E}_{k}\|x^t_{i,0}-\overline{x}_{t}\|^2 = 0$. The final inequality used that the monotonic near-harmonic decay of $\eta_t$ allows $\eta_{t_s} \le u \eta_t$ for $u = (z+1)/2$.  Finally, by Cauchy-Schwartz, 
\begin{equation*}
\left(\sum_{i=1}^N p_i\|\overline{x}_t-x^t_{i,0} \|\right)^2 \le \left(\sum_{i=1}^N p_i\right)\left(\sum_{i=1}^N p_i\|\overline{x}_t-x^t_{i,0} \|^2\right),
\end{equation*}
from which we conclude
\begin{equation}\label{BoundingA1Term}
A_1 \le \mu a \eta_t \|\overline{x}_t-x^{*} \|^2 + \frac{\eta_t}{\mu a} ((M^\alpha + B^\alpha) c_t^{1-\alpha} + \eta_tc_tzuL)^2
\end{equation}

It now remains to bound $A_3$, which can be done straightforwardly via Jensen:
\begin{equation*}
A_3=\eta_t^2\left\|g_t\right\|^2 \leq \eta_t^2 \sum_{i=1}^N p_i\left\|  Clip(c_t,\nabla F_i\left(x^t_{i,0}, \xi^t_{i,0}\right))\right\|^2 \le \eta_t^2 c_t^2.
\end{equation*}
Collecting all inequalities gathered thus far gives the simple form 
\begin{equation*}
\mathbb{E}_{t}[\left\|\overline{x}_{t+1}-x^{*}\right\|^2] \le (1 - (1-a)\mu\eta_t) \left\|\overline{x}_t-x^{*}\right\|^2 -2\eta_t\left(F(\overline{x}_t) - F(x^*) \right) + \eta_t^2 c_t^2 + \frac{\eta_t}{\mu a} ((M^\alpha + B^\alpha) c_t^{1-\alpha} + \eta_tc_tzuL)^2, 
\end{equation*}
which under tower law of expectations is amenable to telescoping. Intuitively, we want to control the learning rate and form a quadratically decaying average on the LHS, which by Jensen and convexity will give a desired near-optimal point. The rest is a matter of carefully easing out a rate schedule that enables averaging, which also converges. Rearranging gives 
\begin{equation}\label{mainequation}
\begin{aligned}
\mathbb{E}[F(\overline{x}_t)] - F(x^*) &\le \frac{(\eta_t^{-1} - (1-a)\mu)}{2} \mathbb{E}[\left\|\overline{x}_t-x^{*}\right\|^2] - \frac{1}{2\eta_t}\mathbb{E}[\left\|\overline{x}_{t+1}-x^{*}\right\|^2] + \frac{\eta_t c_t^2}{2} \\
&+ \frac{1}{2\mu a} ((M^\alpha + B^\alpha)^2 c_t^{2-2\alpha} + 2(M^\alpha + B^\alpha) c_t^{2-\alpha}\eta_t zuL + \eta_t^2c_t^2z^2u^2L^2).
\end{aligned}
\end{equation}
Letting $\eta_t = r/(t+1)$, $a = 1-2/(r\mu)$ for $r > 2/\mu$, we have 
\begin{equation}\label{impliedbymainequation}
\begin{aligned}
t \mathbb{E}[F(\overline{x}_t)] - t F(x^*) &\le \frac{t(t-1)}{2} \mathbb{E}[\left\|\overline{x}_t-x^{*}\right\|^2] - \frac{(t+1)t}{2}\mathbb{E}[\left\|\overline{x}_{t+1}-x^{*}\right\|^2] + \frac{t\eta_t c_t^2}{2} \\
&+ \frac{t}{2\mu a} ((M^\alpha + B^\alpha)^2 c_t^{2-2\alpha} + 2(M^\alpha + B^\alpha) c_t^{2-\alpha}\eta_t zuL + \eta_t^2c_t^2z^2u^2L^2)
\end{aligned}
\end{equation}
Setting $c_t = c t^{\gamma}$ for $1/2 > \gamma>0$, $c>0$ gives after telescoping
\begin{equation*}
\begin{aligned}
\frac{\sum_{t = 1}^T t\mathbb{E}[F(\overline{x}_t)]}{T(T+1)} - F(x^*) &\le \frac{rc^2 \sum_{t = 1}^T t^{2\gamma}}{2 T(T+1)} + \frac{(M^\alpha + B^\alpha)^2 c^{2-2\alpha} \sum_{t = 1}^T t^{(2-2\alpha)\gamma}}{2(\mu-2/r) T(T+1)} \\
&+ \frac{c^{2-\alpha}rzu (M^\alpha + B^\alpha) L \sum_{t = 1}^T t^{(2-\alpha)\gamma} }{(\mu-2/r)T(T+1)} + \frac{r^2c^2z^2u^2L^2 \sum_{t = 1}^T t^{2\gamma-1}}{2(\mu-2/r)T(T+1)}.
\end{aligned}
\end{equation*}
Standard integral bounds give
\begin{equation*}
\begin{aligned}
\frac{\sum_{t = 1}^T t\mathbb{E}[F(\overline{x}_t)]}{T(T+1)} - F(x^*) &\le \frac{rc^2 T^{2\gamma + 1}}{(4 \gamma+2) T(T+1)} + \frac{(M^\alpha + B^\alpha)^2 c^{2-2\alpha} (T^{(2-2\alpha)\gamma + 1} + 1)}{2(\mu-2/r) ((2-2\alpha)\gamma + 1) T(T+1)} \\
&+ \frac{c^{2-\alpha}rzu (M^\alpha + B^\alpha) L T^{(2-\alpha)\gamma + 1} }{(\mu-2/r)((2-\alpha)\gamma + 1)T(T+1)} + \frac{r^2c^2z^2u^2L^2 (T^{2\gamma}+1)}{4\gamma(\mu-2/r)T(T+1)}.
\end{aligned}
\end{equation*}
Finally, note that by Jensen and convexity, the left hand side is lower bounded by 
\begin{equation*}
0 \le F(\tilde{x}_T) -F(x^*) \le \frac{\sum_{t = 1}^T t\mathbb{E}[F(\overline{x}_t)]}{T(T+1)} - F(x^*)    
\end{equation*}
where $\tilde{x}_T:= \sum_{t=1}^T t\mathbb{E}[\overline{x}_t]/T(T+1)$ is a quadratically decaying average. This concludes the proof. It is straightforward to extend to the case in which the learning rate is scheduled to decay in each outer optimizer synchronization step instead of at each local step, by letting $\eta_t = r/(\lceil t/z\rceil +1)$ in equation~\eqref{mainequation}. 
\end{proof}

The value of the tail-index parameter $\alpha$ has a significant impact on the convergence behavior. When $\alpha$ is close to 1, the convergence becomes substantially slower due to the heavy-tailed nature of the induced stochastic gradients and the increased variance they introduce. Conversely, when $\alpha$ approaches 2, the variance is more controlled, leading to faster convergence rates. Importantly, our results demonstrate that even in the presence of infinite variance (i.e., $\alpha < 2$), convergence can still be achieved, showcasing the robustness of the clipping approach under extreme heavy-tailed conditions.


The averages $\overline{x}_t$ are virtual constructs used for theoretical analysis of Algorithm~\ref{L2ClipSGD}, which are not accumulated during the execution phase. That is, these quantities are only available at the outer optimizer synchronization steps, $t \in z \cdot \mathbb{Z}_{\ge 0}$, and are not collected otherwise (as models are not saved for every local timestep prior to synchronization). As a result, the application of Avg-$L_2Clip$ creates a virtual history on the compute node models, where the aggregation of ephemeral model weights can theoretically induce convergence. However, in practice, this conflicts with the use of local epochs for communication efficiency, necessitating adjustments to the convergence theorem. This leads to the development of Corollary~\ref{PracticalConvexConvergenceAppendix}.

\begin{corollary}\label{PracticalConvexConvergenceAppendix}
Let the conditions of Theorem~\ref{ConvexConvergenceThmAppendixVer} hold. Then, we have that
\begin{equation*}
\mathbb{E}\left[F\left(\frac{\sum_{t \in Z} (t-1)\overline{x}_t}{\sum_{t \in Z} (t-1)}\right)\right] - F(x^*) \le \frac{(T+1)z}{(T-z)}\left(\psi_1 + \psi_2 + \psi_3 + \psi_4\right),
\end{equation*}
where the $\psi_i$ are defined as in the statement of Theorem~\ref{ConvexConvergenceThmAppendixVer} and $Z$ is the set of all outer optimizer synchronization steps. 
\end{corollary}
\begin{proof}
We may start with equation~\eqref{impliedbymainequation}, where we use the same notation as the proof of Theorem~\ref{ConvexConvergenceThmAppendixVer}. Recall that $0 \le F(x)-F(x^*)$ for all $x$. Therefore, we have for $Z = \{1,z+1, \dots, z\lfloor T/z \rfloor + 1 \} $ for $T \notin z \cdot \mathbb{Z}$ and $Z = \{1,z+1, \dots, z(\lfloor T/z \rfloor-1) + 1 \} $ otherwise,
\begin{equation*}
\begin{aligned}
&\sum_{t \in Z} t\left(\mathbb{E}[F(\overline{x}_t)] - F(x^*)\right) \le \sum_{t \in [T]} \left(\frac{t(t-1)}{2} \mathbb{E}[\left\|\overline{x}_t-x^{*}\right\|^2] - \frac{(t+1)t}{2}\mathbb{E}[\left\|\overline{x}_{t+1}-x^{*}\right\|^2] \right)\\
&+ \sum_{t \in [T]} \frac{t\eta_t c_t^2}{2} + \sum_{t \in [T]} \frac{t}{2\mu a}\left( (M^\alpha + B^\alpha)^2 c_t^{2-2\alpha} + 2(M^\alpha + B^\alpha) c_t^{2-\alpha}\eta_t zuL + \eta_t^2c_t^2z^2u^2L^2\right).
\end{aligned}
\end{equation*}
Noting that 
\begin{equation*}
\sum_{t \in Z} (t-1)\left(\mathbb{E}[F(\overline{x}_t)] - F(x^*)\right) \le \sum_{t \in Z} t\left(\mathbb{E}[F(\overline{x}_t)] - F(x^*)\right), 
\end{equation*}
\begin{equation*}
\frac{(T - z) T}{2z} \le \frac{z(\lceil T/z \rceil - 1)\lceil T/z \rceil}{2} \le  \frac{z(\lfloor T/z \rfloor+1)\lfloor T/z \rfloor}{2} ,
\end{equation*}
we obtain
\begin{equation*}
\mathbb{E}\left[F\left(\frac{\sum_{t \in Z} (t-1)\overline{x}_t}{\sum_{t \in Z} (t-1)}\right)\right] - F(x^*) \le \frac{(T+1)z}{(T-z)}\left(\psi_1 + \psi_2 + \psi_3 + \psi_4\right).
\end{equation*}
As before, extension to the case where the learning rate decays at each outer optimizer synchronization step is straightforward. Therefore, the asymptotic convergence rate is identical that give in Theorem~\ref{ConvexConvergenceThmAppendixVer}.
\end{proof}
% \tian{can we have the actual convergence rate in a new corollary}
In particular, we immediately deduce the following corollary.
\begin{corollary}
Let the conditions of Theorem~\ref{ConvexConvergenceThmAppendixVer} hold. Then, Avg-$L_2Clip$ converges under heavy-tailed noise with rate $\mathcal{O}(T^{-1/2})$. That is, the algorithm recovers a point $\widetilde{x}_T$ which is materialized during training such that 
\begin{equation*}
 \mathbb{E}[F(\widetilde{x}_T)] - F(x^*) \lesssim \mathcal{O}(T^{-1/2}).
\end{equation*}
\end{corollary}
\begin{proof}
The maximal rate of convergence is immediately attained in the limit $\gamma \to 0^+$, where the dominating terms are $\Psi_i$ for $i = 1,2,3$.
\end{proof}

\subsection{Dynamics of Avg-$L_2Clip$ under Failing Compute Nodes}\label{AppendixSectionforConvexConvergenceCrossDevice} 
% \tian{maybe a subsection somewhere?}
% Su: I will leave "Datacenter" instead of changing to compute node, somehow that feels nicer here
Distributed optimization operates in two primary modes: full participation or partial participation (known in some fields such as federated learning as cross-silo or cross-device). Full participation distributed optimization is relevant for scenarios such as training language models in datacenters or healthcare models across hospitals~\cite{DiLoCoAsynchronous,DiLoCo,healthcare1,healthcare2}, where bypassing legislative geolocation restrictions enables access to larger datasets and promotes fairer, balanced model training~\cite{hospitalmajor}. In contrast, partial participation involves training small-scale, personalized models on restricted compute nodes such as mobile devices~\cite{TianReviewPaper}. In such settings, local data shards are often highly heterogeneous and non-IID, leading to diverse gradient distributions induced by the distributed outer global model weights synthesized by the outer optimizer. Consequently, it is crucial to conduct a theoretical performance analysis of Avg-$L_2Clip$ within environments to accommodate the presence of failing compute nodes or partial participation.

In this setting, line 2 of Avg-$L_2Clip$ is modified to sample a subset of participating nodes, $\mathcal{S} \subset [N]$, rather than selecting $S = [N]$. Additionally, normalized averaging is performed across only the participating compute nodes in line 7. Typically, extending the analysis from full to partial participation introduces additional complexities due to the randomness of node subsampling and the fact that most compute nodes remain idle. However, we can leverage elements of our previous analysis by considering a highly resource-inefficient algorithm that mimics full participation Avg-$L_2Clip$, in which all compute nodes remain active. We refer to this algorithm as $SludgeClip$ to emphasize its impracticality, despite being functionally equivalent to Avg-$L_2Clip$. By analyzing $SludgeClip$, we are able to establish convergence of Avg-$L_2Clip$ in when several datacenters or compute nodes fail to partake in training.

% \tian{also change the name here}
\begin{algorithm}
\caption{$SludgeClip$}\label{SludgeClip-SGD}
\begin{algorithmic}[1]
\REQUIRE Initial model $x_1$, 
learning rate schedule $\eta_t$, clipping schedule $c_t$ \\ Synchronization timestep $z \in \mathbb{Z}_{>0}$, projection domain $\mathcal{X}$
\FOR{$t = 1, \dots, T$} 
    \STATE Sample participating compute nodes $S \subset [N]$ according to $p_i$
    \FOR{each node $i \in [N]$}
        \STATE Draw minibatch gradient $g^{t}_{i,0} = \nabla F_i(x^{t}_{i,0}, \xi^{t}_{i,0})$
        \STATE $x^{k}_{t+1} \leftarrow  x^{k}_t - \eta_t \cdot L_2Clip(c_t,g^{t}_{i,0})$ 
    \ENDFOR
%    \FOR{each client $k \notin [N]$}
%        \STATE $x^{k}_{t+1} \leftarrow (1-(\sum_{i^{\prime} \in S} p_{i^{\prime}})^{-1}) \sum_{i^{\prime} \in S} p_{i^{\prime}} x^{i^{\prime}}_{t+1}$ 
%    \ENDFOR
    \STATE \textbf{if }$t-1 \in z \cdot \mathbb{Z}_{\ge 0}:$ \ 
    \STATE \ \ \ \ $ x_{t+1}^k \leftarrow \operatorname{Proj}_{\mathcal{X}}\left((\sum_{i^{\prime} \in S} p_{i^{\prime}})^{-1} \sum_{i^{\prime} \in S} p_{i^{\prime}} x^{i^{\prime}}_{t+1} \right) $, for $\forall k \in [N]$
\ENDFOR
\end{algorithmic}
\end{algorithm}
\begin{theorem}\label{ConvexConvergenceThmCrossDeviceAppendixVer} Let the clipping threshold in $SludgeClip$ (Algorithm~\ref{SludgeClip-SGD}) satisfy $c_t = c\eta_t^{\gamma}$ for $c > 0$ and $1/2 > \gamma > 0$. Decay the learning rate with schedule $\eta_t = r/(t+1)$ for $r>2/\mu$. If the sampling scheme preserves the global objective\footnote{For example, $p_i = 1/N$ satisfies this condition. That is, given any selection of $p_i$ and $F_i(x)$, we may rescale the local objectives $F_i(x)$ such that $p_i = 1/N$ by controlling the influence of each local gradient update.}, that is, $$\mathbb{E}_{S} \left[\sum_{i \in [S]} p_i F_i(x)\right] = \sum_{i \in [N]} p_i F_i(x) = F(x),$$
then we have for $Z$ the set of synchronization steps up to $T$ that 
\begin{equation*}
\mathbb{E}\left[F\left(\tilde{x}_T^\prime\right)\right] - F(x^*) := \mathbb{E}\left[F\left(\frac{\sum_{t \in Z} (t-1)\overline{x}_t}{\sum_{t \in Z} (t-1)}\right)\right] - F(x^*) \le  z \cdot \mathcal{O}\left(t^{-\omega}\right),
\end{equation*}
where now $\omega$ satisfies
\begin{equation*}
\omega = \min\{1-2\gamma, 1 - (2-2\alpha)\gamma, 1-(2-\alpha)\gamma,2- 2\gamma, 2\gamma(\alpha-1)\}.
\end{equation*}
If the subsampling scheme fails to preserve the global objective (e.g., by sampling only a strict subset of avaliable nodes repeatedly), then Algorithm~\ref{SludgeClip-SGD} asymptotes toward biased minimizer points within an increasing region determined by the clipping threshold $\mathbb{E}\left[F\left(\tilde{x}_T^\prime\right)\right] - F(x^*) \lesssim \mathcal{O}(t^{2\gamma})$.
\end{theorem}
We note that convergence is not clearly guaranteed when subsampling procedures violate the global objective in expectation. Specifically, we evaluate the algorithm's output relative to $x^*$, the global optimum of the true objective $F(x)$. However, when subsampling alters the objective, the algorithm no longer optimizes for $F(x)$, thereby clearly undermining convergence toward $x^*$. We then measure the propensity of the algorithm output to $x^*$, the global optimum of the true objective $F(x)$ which is no longer the objective of the subsampled algorithm.
\begin{proof}
We first analyze the case in which the subsampling strategy preserves the correct global objective, which allows for convergence to $x^*$. Recall that $SludgeClip$-SGD was constructed to allow the analysis for non-synchronization steps to be analogous to full-participation Avg-$L_2Clip$. Therefore, we focus on outer optimizer synchronization steps while incorporating the elements of the previous analysis for Theorem~\ref{ConvexConvergenceThmAppendixVer}. We now use the following notation for subsampled averages of participating compute node devices:
\begin{equation*}
\tilde{x}_t=\frac{\sum_{i\in S}p_i x^t_{i,0}}{\sum_{i\in S}p_i}, \quad \tilde{g}_t=\frac{\sum_{i\in S} p_i \cdot Clip(c_t,\nabla F_i\left(x^t_{i,0}, \xi^t_{i,0}\right))}{\sum_{i\in S}p_i}.
\end{equation*}
For added clarity, we denote $g_t$ as $\overline{g}_t$ to indicate that normalized averages are taken over all inner compute nodes, and not solely participating nodes as in $\tilde{g}_t$. Then for $t+1$ a synchronization step, we have that 
\begin{equation*}%\label{firstequation}
\begin{aligned}
\left\|\tilde{x}_{t+1}-x^{*}\right\|^2  &\le \left\|\tilde{x}_{t}-x^{*} - \eta_t \tilde{g}_t\right\|^2 = \left\| \overline{x}_{t} + (\tilde{x}_{t} -\overline{x}_{t}) -x^{*} - \eta_t \tilde{g}_t + (\eta_t \overline{g}_t - \eta_t \overline{g}_t)\right\|^2 \\
&= \left\|\overline{x}_t-x^{*}\right\|^2 + 2 \langle \overline{x}_t-x^{*}, \underbrace{\tilde{x}_{t} - \overline{x}_{t} - \eta_t \tilde{g}_t + (\eta_t \overline{g}_t - \eta_t \overline{g}_t)}_{B_1} \rangle + B_1^2
\\
& \le \left\|\overline{x}_t-x^{*}\right\|^2 \underbrace{-2 \eta_t\left\langle\overline{x}_t-x^{*}, \overline{g}_t-\nabla F(\overline{x}_t)\right\rangle}_{A_1} \underbrace{-2 \eta_t\left\langle\overline{x}_t-x^{*}, \nabla F(\overline{x}_t)\right\rangle}_{A_2}\\
&\underbrace{+2 \langle \overline{x}_t-x^{*}, \tilde{x}_{t} - \overline{x}_{t} \rangle }_{B_2}
\underbrace{+ 2\eta_t \langle \overline{x}_t-x^{*}, \overline{g}_t-\tilde{g}_t \rangle }_{B_3}
\underbrace{+\left\| \tilde{x}_{t} - \overline{x}_{t} - \eta_t \tilde{g}_t \right\|^2}_{B_4}.
\end{aligned}
\end{equation*}
In this form, the $A_i$ terms are therefore shared with the previous analysis, and $A_2$ may be bounded by $\mu$-strong convexity as before. This gives that 
\begin{equation*}
A_2 \le - \mu\eta_t \|\overline{x}_t - x^*\|^2 -2\eta_t\left(F(\overline{x}_t) - F(x^*) \right) .
\end{equation*}
$A_1$ is once again bounded under conditional expectations $\mathbb{E}_{t}[\cdot]$ by equation~\eqref{BoundingA1Term}, though with a different value of $a^\prime>0$ than in the previous proof, 
\begin{equation}\tag{\ref{BoundingA1Term}}
A_1 \le \mu a^\prime \eta_t \|\overline{x}_t-x^{*} \|^2 + \frac{\eta_t}{\mu a^\prime} ((M^\alpha + B^\alpha) c_t^{1-\alpha} + \eta_tc_tzuL)^2.
\end{equation}
Now, as $B_2$ is eliminated under expectations under subsampling, we focus on the remaining terms. It is clear that we must bound and $\|  \overline{g}_t-\tilde{g}_t \|$ to proceed. Intuitively, this is controlled by normalized averages and model drift across participating nodes. Therefore, we consider the nearest or most recent synchronization timestep $t_s(t)$ as before and rearrange to incorporate elements of our previous analysis. Assuming interchangeability between the integrals $\mathbb{E}_{S}$ (integrating over the randomness of node subsampling) and $\mathbb{E}_{t}$ (integrating over randomness of $\xi^t_{i,0}$), 
\begin{equation*}
\begin{aligned}
&\|\mathbb{E}_{t}\left[\mathbb{E}_{S}[\tilde{g}_t] - \overline{g}_t \right]\|  = \left\|\mathbb{E}_{t}\left[\mathbb{E}_{S}\left[\sum_{i\in S} \frac{p_i}{\sum_{i^{\prime}\in S}p_{i^\prime}} (Clip(c_t,\nabla F_i\left(x^t_{i,0}, \xi^t_{i,0}\right))-\nabla F_i(\overline{x}_t))\right] - (\overline{g}_t-\nabla F(\overline{x}_t))\right] \right\|\\
&= \left\|\mathbb{E}_{S}\left[\mathbb{E}_{t}\left[\sum_{i\in S} \frac{p_i}{\sum_{i^{\prime}\in S}p_{i^\prime}} (Clip(c_t,\nabla F_i\left(x^t_{i,0}, \xi^t_{i,0}\right))-\nabla F_i(\overline{x}_t, \xi^t_{i,0}))\right]\right] - \mathbb{E}_{t}\left[ \overline{g}_t-\nabla F(\overline{x}_t)\right] \right\|\\
&\le\mathbb{E}_{S} \left[\sum_{i\in S} \frac{p_i}{\sum_{i^{\prime}\in S}p_{i^\prime}} \mathbb{E}_t[\left\| Clip(c_t,\nabla F_i\left(x^t_{i,0}, \xi^t_{i,0}\right))-\nabla F_i(\overline{x}_t, \xi^t_{i,0})\right\|]\right] + \mathbb{E}_{t}[\| \overline{g}_t-\nabla F(\overline{x}_t)\|] \le 2(M^\alpha + B^\alpha) c_t^{1-\alpha}
\end{aligned}
\end{equation*}
where to obtain the final line we used Jensen and an analogous reasoning as in equation~\eqref{clipbiasboundwithalpha}.

Therefore, we have for $b >0$ that 
\begin{equation*}
\begin{aligned}
%B_2 &\le b_1 \eta_t\|\overline{x}_t - x^*\|^2 + \frac{1}{b_1 \eta_t} \|\tilde{x}_t - \overline{x}_t \|^2 \le b_1 \eta_t\|\overline{x}_t - x^*\|^2 + \frac{2z^2u^2c_t^2\eta_t}{b_1}, \\
B_3 &\le b \eta_t \|\overline{x}_t - x^*\|^2 + 4\eta_t(M^\alpha + B^\alpha)^2 c_t^{2(1-\alpha)}.
\end{aligned}
\end{equation*}
It now remains to bound $B_4$, which can be done straightforwardly:
\begin{equation*}
B_4\le 2\left\| \tilde{x}_{t} - \overline{x}_{t}\right\|^2 + 2 \eta_t^2 \left\| \tilde{g}_t \right\|^2 \leq 4z^2u^2\eta_t^2c_t^2 + 2 \eta_t^2 c_t^2.
\end{equation*}
Collecting all inequalities gathered under the tower law of expectation, we have 
\begin{equation*}%\label{firstequation}
\begin{aligned}
\mathbb{E}[\left\|\tilde{x}_{t+1}-x^{*}\right\|^2]  &\le (1 - ((1-a)\mu +b)\eta_t) \mathbb{E}[\left\|\overline{x}_t-x^{*}\right\|^2] -2\eta_t\mathbb{E}\left[F(\overline{x}_t) - F(x^*) \right] 
\\
& + \frac{\eta_t}{\mu a} ((M^\alpha + B^\alpha) c_t^{1-\alpha} + \eta_tc_tzuL)^2 + 4z^2u^2\eta_t^2c_t^2 + 2 \eta_t^2 c_t^2 + 4\eta_t(M^\alpha + B^\alpha)^2 c_t^{2(1-\alpha)}.
\end{aligned}
\end{equation*}
Recall the learning rate schedule $\eta_t = r/(t+1)$, while setting $a^\prime,b$ such that $r((1-a^\prime)\mu + b) = 2$. Then, we have for $Z$ the set of all synchronization steps,
\begin{equation*}
\begin{aligned}
&\sum_{t+1 \in Z} t(\mathbb{E}[F(\overline{x}_t)] - F(x^*)) \le \sum_{t+1 \in Z}\left[ \frac{t(t-1)}{2} \mathbb{E}[\left\|\overline{x}_t-x^{*}\right\|^2] - \frac{(t+1)t}{2}\mathbb{E}[\left\|\tilde{x}_{t+1}-x^{*}\right\|^2] \right]  \\
& \underbrace{+ \sum_{t+1 \in Z} 2(M^\alpha + B^\alpha)^2 t c_t^{2(1-\alpha)}}_{B_5} \underbrace{+ \sum_{t+1 \in Z}  \frac{1}{2\mu a} ((M^\alpha + B^\alpha) c_t^{1-\alpha} + \eta_tc_tzuL)^2}_{\sim \Psi_{2} + \Psi_{3} + \Psi_{4}} + \underbrace{\sum_{t+1 \in Z} t\eta_t c_t^2(2z^2u^2+1)}_{\sim \Psi_1} .
\end{aligned}
\end{equation*}
For $t +1 \notin Z$, we use the standard telescoping sum in equation~\eqref{impliedbymainequation} while noting that $\tilde{x}_{t+1} = \overline{x}_{t+1}$ due to the synchronization step. We do not repeat mechanical calculation steps here to not obscure the intuitions behind the proof, and instead indicate asympototically equivalent terms to $\Psi_i$ under $1/(T^2+T)$ averaging on the right hand side. It remains to bound the residual term $B_5$ under the averaging step, which gives
\begin{equation*}
\frac{B_5}{T(T+1)} \lesssim \mathcal{O}(t^{2\gamma(1-\alpha)}),
\end{equation*}
which concludes the proof for the first case. 

In the setting in which the subsampling procedure fails to preserve the global objective, we bound $\|\tilde{x}_t-\overline{x}_t\|$ as follows:
\begin{equation*}
\begin{aligned}
\|\tilde{x}_t - \overline{x}_t \|  &= \left\|\sum_{i \in [S]} \left(\frac{\sum_{\tilde{k} \notin [S]} p_{\tilde{k}}}{\sum_{i^\prime \in [S]} p_{i^\prime}}\right)p_i x^t_{i,0}  - \sum_{i \notin [S]} p_i x^t_{i,0} \right\| \\
&\le \sum_{i \in [S]} \left(\frac{\sum_{\tilde{k} \notin [S]}p_{\tilde{k}}}{\sum_{i^\prime \in [S]} p_{i^\prime}}\right)p_i \| x^t_{i,0} -\overline{x}_{t_s}\| + \sum_{i \notin [S]} p_i \| x^t_{i,0} - \overline{x}_{t_s} \| \le 2zu\eta_t c_t,
\end{aligned}
\end{equation*}
due to triangle inequality and Jensen. That is, by the synchronization step, we have $x_{t_s}^k = \overline{x}_{t_s}$, $\forall k \in [N]$ via to full available node activation in $SludgeClip$. This gives
\begin{equation*}
\| x^t_{i,0} -\overline{x}_{t_s}\| = \left\| x^t_{i,0} + \sum_{t^{\prime}=t_s+1}^{t-1} (-x_{t^{\prime}}^{k} + x_{t^{\prime}}^{k}) -\overline{x}_{t_s} \right\| \le\sum_{t^{\prime}=t_s+1}^{t-1} \|x_{t^{\prime}}^k - x_{t^{\prime}-1}^k \| \le zu\eta_tc_t
\end{equation*}
as in equation~\eqref{boundedclientdrift}. 
Similarly, we have by Jensen and convexity of the norm that
\begin{equation*}
\|\tilde{g}_t - \overline{g}_t \| \le 2 c_t.
\end{equation*}
Therefore, we obtain for $b_1, b_2 >0$  
\begin{equation*}
\begin{aligned}
B_2 &\le b_1 \eta_t\|\overline{x}_t - x^*\|^2 + \frac{1}{b_1 \eta_t} \|\tilde{x}_t - \overline{x}_t \|^2 \le b_1 \eta_t\|\overline{x}_t - x^*\|^2 + \frac{2z^2u^2c_t^2\eta_t}{b_1}, \\
B_3 &\le b_2 \eta_t \|\overline{x}_t - x^*\|^2 + 4\eta_t c_t^2.
\end{aligned}
\end{equation*}
Following analogous calculations as in the case where the subsampling does not violate the global objective, we arrive at a new residual term
\begin{equation*}
\frac{B_6}{T(T+1)} \lesssim \mathcal{O}(t^{2\gamma}),
\end{equation*}
which controls the expansion of the bias due to the incorrect sampling strategy.
\end{proof}

\subsection{Convergence of $Bi^2Clip$}\label{BiSquareAppendix}

In this section, we analyze the convergence of $Bi^2Clip$ under heavy-tailed noise. By employing $BiClip$ at both the inner and outer optimizers, $Bi^2Clip$ can represent a highly competitive algorithm realized by TailOPT that utilizes adaptive mimicry, aiming to adjust to gradient distributional statistics while strictly maintaining resource efficiency. Unlike $Adam^2$, which applies Adam at both the inner and outer optimizers, $Bi^2Clip$ achieves comparable empirical performance while requiring no additional memory or computational overhead beyond standard SGD (Table~\ref{tab:glue-results}). This highlights its efficiency and practicality, particularly in resource-constrained settings. We begin with the pseudocode for $Bi^2Clip$, Algorithm~\ref{BiSquare_Clip_SGD}.

\begin{algorithm}
\caption{$Bi^2Clip$}\label{BiSquare_Clip_SGD}
\begin{algorithmic}[1]
\REQUIRE Initial model $x_1$, 
learning rate schedule $\eta_t$, clipping schedules $u_t,$ $d_t$, $\widetilde{u}_t,$ $\widetilde{d}_t$ \\ Synchronization timestep $z \in \mathbb{Z}_{>0}$
\FOR{$t = 1, \dots, T$} 
    \FOR{each node $i \in [N]$ in parallel}
    \STATE $x_{i,0}^t \leftarrow x_t$
    \FOR{each local step $k \in [z]$}
        \STATE Draw minibatch gradient $g^t_{i,k} = \nabla F_i(x^t_{i,k}, \xi^t_{i,k})$
        \STATE $x^{t+1}_{i,k} \leftarrow  x^t_{i,k} - \eta_t \cdot BiClip(u_t,d_t,g^t_{i,k})$ 
        \ENDFOR
    \ENDFOR
    \STATE $\Delta_t = \frac{1}{N} \sum_{i \in [N]} \left(x_{i,z}^{t} - x_{t-1}\right)$, \quad $\widetilde{m}_t \leftarrow \Delta_t$
    %\STATE $\widetilde{m}_t \leftarrow \Delta_t$%= \widetilde{\beta}_1 \widetilde{m}_{t-1} + (1 - \widetilde{\beta}_1) \Delta_t$
    %\STATE $\widetilde{v}_t = \widetilde{v}_{t-1} + \Delta_t^2$ 
    \STATE $x_{t} = x_{t-1} + \eta BiClip(\widetilde{u}_t,\widetilde{d}_t,\widetilde{m}_t)$
\ENDFOR
\end{algorithmic}
\end{algorithm}


\paragraph{Bounded domain.} We carry out the analysis over a sufficiently large, compact domain $\mathcal{X}$. Let $\nabla F(x)$ be the deterministic gradient, obtained by integrating over $\nabla F(x,\xi)$, the stochastic gradient with a heavy-tailed distribution. The existence of $\nabla F(x)$ implies $F(x)$ is continuous, which gives boundedness via the extremal value theorem. Therefore, from now onward, we formally assume $\nabla F(x)$ is coordinatewise bounded by $G$ in absolute value. We have the following theorem. 

\begin{theorem}
Let assumptions 1-2 hold, and the learning rate and clipping schedules satisfy $\eta_t = \Theta(t^\omega)$, $\eta_\ell^t = \Theta(t^\nu)$, $d_t = \Theta(t^\gamma)$, $u_t = \Theta(t^\zeta)$, $\widetilde{d}_t = \Theta(t^{\widetilde{\gamma}})$, and $u_t = \Theta(t^{\widetilde{\zeta}})$. Imposing $\zeta,\widetilde{\zeta} > 0 > \gamma, \widetilde{\gamma}$, for $\omega, \nu \le 0$, as well as the following conditions
\begin{equation*}
-1< \omega + \nu, \quad \nu + \zeta < 0, \quad \max\{\omega + 2\widetilde{\zeta},\widetilde{\gamma}\} < \nu, 
\end{equation*}
for $Bi^2Clip$ (Algorithm~\ref{BiSquare_Clip_SGD}), we have that 
\begin{equation*}
\begin{aligned}
\min_{t \in [T]} \mathbb{E}[\|\nabla F(x_{t-1})\|^2] \lesssim \Psi_1 + \Psi_2 + \Psi_3 + \Psi_4 + \Psi_5 + \Psi_6 + \Psi_7,
\end{aligned}
\end{equation*}
where the $\Psi_i$ are given
\begin{equation*}
\begin{aligned}
&\Psi_1 = \mathcal{O}\left( T^{-\omega - \nu - 1}\right), \quad \Psi_2 = \mathcal{O}\left(T^{\omega + 2\widetilde{\zeta} - \nu} \right),\quad \Psi_3 = \mathcal{O}\left(T^{\widetilde{\gamma} - \nu}  \right),\quad \Psi_4 = \mathcal{O}\left(T^\gamma\right),\\
&\quad \Psi_5 = \mathcal{O}\left( T^{(\alpha-1)\nu + (1-\alpha)\widetilde{\zeta}} \right),\quad \Psi_6 = \mathcal{O}\left(T^{(1-\alpha)\zeta}  \right),\quad \Psi_7 = \mathcal{O}\left( T^{\nu + \zeta} \right).
\end{aligned}
\end{equation*}
\end{theorem}

\begin{proof}
We provide the proof for $L_2$-wise $BiClip(\cdot)$ for illustrative purposes and notational convenience. The extension to coordinate-wise $BiClip(\cdot)$ is straightforward as described in the comments following the proof of Theorem~\ref{ServerAdagradConvergenceTheroem}, Remark~\ref{remark}. For completeness and readability, we formally provide the definition of $L_2$-wise $BiClip(\cdot)$ as
\begin{equation*}
\begin{aligned}
&BiClip(u_t,d_t,x) = x \cdot \frac{d_t}{\|x\|} \ \chi\left(\|x\|\le d_t\right) \\
&\quad + x \cdot \frac{u_t}{\|x\|} \ \chi\left(\|x\|\ge u_t\right) + x \cdot \chi\left(d_t<\|x\|< u_t\right).
\end{aligned}
\end{equation*}
Here, $\chi$ is the indicator function, and $u_t \ge d_t \ge 0$ are the clipping thresholds. By default, we take $a/0 := 0$ for $\forall a \in \mathbb{R}$. Now, we begin by noting that due to $L$-smoothness, we have where $\mathbb{E}_{t}[ \ \cdot \ ]$ takes expectation up to $x_{t-1}$ that
\begin{equation*}
\begin{aligned}
\mathbb{E}_{t}[F(x_t)] - F(x_{t-1}) &\le  \left\langle \nabla F(x_{t-1}), \mathbb{E}_{t}[x_{t} - x_{t-1}] \right\rangle + \frac{L}{2} \mathbb{E}_{t}[\left\| x_{t} - x_{t-1} \right\|^2] \\
&\le \eta_t \underbrace{\left\langle \nabla F(x_{t-1}), -\mathbb{E}_{t}[BiClip(\widetilde{u}_t,\widetilde{d}_t,-\Delta_t)] \right\rangle}_{A_1} + \frac{L\eta_t^2}{2} \mathbb{E}_{t}\left[\left\| BiClip(\widetilde{u}_t,\widetilde{d}_t,\Delta_t) \right\|^2\right].
\end{aligned}
\end{equation*}
Now, we expand to obtain the following form 
\begin{equation*}
\begin{aligned}
A_1 &= -\left\langle \nabla F(x_{t-1}), \mathbb{E}_{t}[BiClip(\widetilde{u}_t,\widetilde{d}_t,-\Delta_t) \pm \Delta_t ]\mp \eta_\ell^t \sum_{i \in [N]}\sum_{\nu \in [K]-1} p_i \mathbb{E}_{t}[\nabla F_{i}(x_{i,\nu}^t)] \mp K\eta_\ell^t \nabla F(x_{t-1}) \right\rangle\\
&=\underbrace{-\left\langle \nabla F(x_{t-1}), \mathbb{E}_{t}[BiClip(\widetilde{u}_t,\widetilde{d}_t,-\Delta_t) + \Delta_t]  \right\rangle}_{B_1}  \underbrace{-\left\langle \nabla F(x_{t-1}),-\eta_\ell^t \sum_{i \in [N]}\sum_{\nu \in [K]-1} p_i \mathbb{E}_{t}[\nabla F_{i}(x_{i,\nu}^t)] - \mathbb{E}_{t}[\Delta_t] \right\rangle}_{B_2} \\
&\underbrace{-\left\langle \nabla F(x_{t-1}), \eta_\ell^t \sum_{i \in [N]}\sum_{\nu \in [K]-1} p_i \mathbb{E}_{t}[\nabla F_{i}(x_{i,\nu}^t)] - K\eta_\ell^t \nabla F(x_{t-1}) \right\rangle}_{B_3} - K\eta_\ell^t \|\nabla F(x_{t-1})\|^2.
\end{aligned}
\end{equation*}
Using the convexity of compositions (via $\alpha \ge 1$) and Jensen, we deduce
\begin{equation*}
\begin{aligned}
\mathbb{E}_{t}[\|\Delta_t\|^\alpha] &=  \mathbb{E}_{t}[\|\eta_\ell^t \sum_{i \in [N]}\sum_{\nu \in [K]-1} p_i \cdot BiClip(u_t,d_t,\nabla F_{i}(x_{i,\nu}^t,\xi_{i,\nu}^t))\|^\alpha] \\
&\le (\eta_\ell^t)^\alpha K^\alpha \mathbb{E}_{t}\left[\left\|\frac{1}{K}\cdot \sum_{i \in [N]}\sum_{\nu \in [K]-1} p_i \cdot BiClip(u_t,d_t,\nabla F_{i}(x_{i,\nu}^t,\xi_{i,\nu}^t))\right\|^\alpha\right] \\
&\le (\eta_\ell^t)^\alpha K^{\alpha-1} \sum_{i \in [N]}\sum_{\nu \in [K]-1} p_i \mathbb{E}_{t}[\|BiClip(u_t,d_t,\nabla F_{i}(x_{i,\nu}^t,\xi_{i,\nu}^t))\|^\alpha] \\
&\le (\eta_\ell^t)^\alpha K^{\alpha-1} \sum_{i \in [N]}\sum_{\nu \in [K]-1} p_i (d_t^\alpha + \mathbb{E}_{t}[\|\nabla F_{i}(x_{i,\nu}^t,\xi_{i,\nu}^t)\|^\alpha])
\\
&\le (\eta_\ell^t)^\alpha K^{\alpha-1} \sum_{i \in [N]}\sum_{\nu \in [K]-1} p_i d_t^\alpha + \underbrace{(\eta_\ell^t)^\alpha K^{\alpha-1} \sum_{i \in [N]}\sum_{\nu \in [K]-1} p_i \mathbb{E}_{t}[\|\nabla F_{i}(x_{i,\nu}^t,\xi_{i,\nu}^t)\|^\alpha]}_{C}.
\end{aligned}
\end{equation*}
Note that the term $C$ can be bounded as
\begin{equation*}
\begin{aligned}
    C &\le (\eta_\ell^t)^\alpha K^{\alpha-1} \sum_{i \in [N]}\sum_{\nu \in [K]-1} p_i 2^\alpha \mathbb{E}_{t}\left[\frac{\left\|\nabla F_i(x_{i,v}^t)\right\|^\alpha}{2} + \frac{\left\|\xi_{i,v}^t\right\|^\alpha}{2}\right]\\
    &\le (\eta_\ell^t)^\alpha K^{\alpha-1} \sum_{i \in [N]}\sum_{\nu \in [K]-1} p_i 2^{\alpha - 1} (M^\alpha + B^\alpha) = (\eta_\ell^t)^\alpha K^{\alpha-1} \sum_{\nu \in [K]-1} 2^{\alpha - 1} (M^\alpha + B^\alpha),
\end{aligned}
\end{equation*}
where $M:= \max_{x\in\mathcal{X}, i \in [N]} \|\nabla F_i (x)\|$ and $B^{\alpha}:= \max_{i\in[N], \ \nu\in[K]-1}\mathbb{E}_{t}[\|\xi_{i,v}^t\|^\alpha] \le \sup_{i \in [N]} (B_{i})^{\alpha_i}$. We note that this results holds also under distribution shift for the stochastic noise $\xi_i^t$, where $t \in [T]$ and $i \in [N]$, as long as the $\alpha$-moment remains universally bounded. Therefore, we conclude
\begin{equation*}
\mathbb{E}_{t}[\|\Delta_t\|^\alpha] \le (\eta_\ell^t)^\alpha K^{\alpha-1} \sum_{\nu \in [K]-1} d_t^\alpha + (\eta_\ell^t)^\alpha K^{\alpha-1}2^{\alpha - 1} \sum_{\nu \in [K]-1}  (M^\alpha + B^\alpha) =: (\eta_\ell^t)^\alpha \widetilde{M}.
\end{equation*}
This gives by the Cauchy-Schwartz inequality that 
\begin{equation*}
\begin{aligned}
B_1 &\le \| \nabla F(x_{t-1})\| \|\mathbb{E}_{t}[BiClip(\widetilde{u}_t,\widetilde{d}_t,-\Delta_t)] + \Delta_t \|\\
&\le G \cdot \mathbb{E}_{t}[\chi (\|\Delta_t\| \le \widetilde{d}_t ) \ \widetilde{d}_t + \chi \left(\widetilde{u}_t\le \|\Delta_t\| \right) \|\Delta_t\|^{\alpha} \|\Delta_t\|^{1-\alpha} ]\\
&\le G \left[\mathbb{P} (\|\Delta_t\| \le \widetilde{d}_t ) \ \widetilde{d}_t + \mathbb{P} \left(\widetilde{u}_t\le \|\Delta_t\| \right) (\eta_\ell^t)^{\alpha} \widetilde{u}_t^{1-\alpha} \widetilde{M} \right].
\end{aligned}
\end{equation*}
Now, $B_2$ may be bounded as follows:
\begin{equation*}
\begin{aligned}
B_2 &\le G \left\| \eta_\ell^t \sum_{i \in [N]}\sum_{\nu \in [K]-1} p_i \mathbb{E}_{t}[\nabla F_{i}(x_{i,\nu}^t)] + \mathbb{E}_{t}[\Delta_t] \right\| \\
&= G \left\| \mathbb{E}_{t}[\eta_\ell^t\sum_{i \in [N]}\sum_{\nu \in [K]-1} p_i \nabla F_{i}(x_{i,\nu}^t, \xi_{i,\nu}^t) + \Delta_t] \right\|\\ 
&\le G \mathbb{E}_{t} \left[\left\| \eta_\ell^t \sum_{i \in [N]}\sum_{\nu \in [K]-1} p_i \nabla F_{i}(x_{i,\nu}^t, \xi_{i,\nu}^t) + \Delta_t \right\| \right], 
\end{aligned}
\end{equation*}
where we used convexity, Jensen, and that the stochastic gradient noise is unbiased. Unraveling the definition of the pseudogradient $\Delta_t$ gives
\begin{equation*}
\begin{aligned}
    B_2 &\le G \eta_\ell^t \mathbb{E}_{t} \left[\left\| \sum_{i \in [N]}\sum_{\nu \in [K]-1} p_i \nabla F_{i}(x_{i,\nu}^t, \xi_{i,\nu}^t) - \sum_{i \in [N]}\sum_{\nu \in [K]-1} p_i BiClip(u_t,d_t,\nabla F_{i}(x_{i,\nu}^t, \xi_{i,\nu}^t)) \right\| \right] \\
    &\le G \eta_\ell^t \sum_{i \in [N]}\sum_{\nu \in [K]-1} p_i \mathbb{E}_{t} \left[\left\|  \nabla F_{i}(x_{i,\nu}^t, \xi_{i,\nu}^t) - BiClip(u_t,d_t,\nabla F_{i}(x_{i,\nu}^t, \xi_{i,\nu}^t)) \right\| \right] \\
    &\le G \eta_\ell^t \sum_{i \in [N]}\sum_{\nu \in [K]-1} p_i \left[d_t \mathbb{P}(\| \nabla F_{i}(x_{i,\nu}^t, \xi_{i,\nu}^t)\| \le d_t) + \mathbb{P} (\|\nabla F_{i}(x_{i,\nu}^t, \xi_{i,\nu}^t) \| \ge u_t) u_t^{1-\alpha} 2^{\alpha - 1} (M^\alpha + B^\alpha) \right] \\
    &\le G \eta_\ell^t \sum_{\nu \in [K]-1} \left[d_t +  u_t^{1-\alpha} 2^{\alpha - 1} (M^\alpha + B^\alpha) \right].
\end{aligned}
\end{equation*}
Additionally, $B_3$ may be bounded via $L$-smoothness and telescoping:
\begin{equation*}
\begin{aligned}
B_3 & \le \eta_\ell^t G \left\| \sum_{i \in [N]}\sum_{\nu \in [K]-1} p_i \nabla F_{i}(x_{i,\nu}^t) - K \nabla F(x_{t-1}) \right\|\\
& \le \eta_\ell^t G \left\| \sum_{i \in [N]}\sum_{\nu \in [K]-1} p_i \nabla F_{i}(x_{i,\nu}^t) - \sum_{i \in [N]} \sum_{\nu \in [K]-1} p_i \nabla F_i(x_{i,0}^{t}) \right\| \\
&\le \eta_\ell^t G \sum_{i \in [N]}\sum_{\nu \in [K]-1} p_i L \|x_{i,\nu}^t -x_{i,0}^t\| \\
&\le \eta_\ell^t G \sum_{i \in [N]}\sum_{\nu \in [K]-1} p_i L \left\|x_{i,\nu}^t + \sum_{r=1}^{v-1} (x_{i,r}^t-x_{i,r}^t) -x_{i,0}^t\right\| \\
&\le \eta_\ell^t G L \sum_{i \in [N]}p_i\cdot \left(\sum_{\nu \in [K]-1}  \sum_{r=1}^{v-1}  \|x_{i,r}^t -x_{i,r-1}^t\|\right) \le \frac{(\eta_\ell^t)^2 G L K^2 u_t}{2}.
\end{aligned}
\end{equation*}
Collecting all inequalities gathered thus far, we have
\begin{equation*}
\begin{aligned}
\mathbb{E}_{t}[F(x_t)] - F(x_{t-1}) &\le \frac{L\eta_t^2 \widetilde{u}_t^2}{2} - K\eta_\ell^t\eta_t \|\nabla F(x_{t-1})\|^2 + G\eta_t \widetilde{d}_t + G \eta_t(\eta_\ell^t)^{\alpha} \widetilde{u}_t^{1-\alpha} \widetilde{M}\\
&+ G \eta_\ell^t \eta_t \sum_{\nu \in [K]-1} \left[d_t +  u_t^{1-\alpha} 2^{\alpha - 1} (M^\alpha + B^\alpha) \right] + \frac{\eta_t(\eta_\ell^t)^2 G L K^2 u_t}{2}.\\
\end{aligned}
\end{equation*}
Telescoping under the law of iterated expectations gives
\begin{equation*}
\begin{aligned}
\sum_{t \in [T]} K\eta_\ell^t\eta_t \mathbb{E}[\|\nabla F(x_{t-1})\|^2]  &\le F(x_0) - \mathbb{E}[F(x_{T})] + \sum_{t \in [T]}\left(\frac{L\eta_t^2 \widetilde{u}_t^2}{2}  + G\eta_t \widetilde{d}_t + G \eta_t(\eta_\ell^t)^{\alpha} \widetilde{u}_t^{1-\alpha} \widetilde{M}\right)\\
&+ G \sum_{t \in [T]}\eta_\ell^t \eta_t \sum_{\nu \in [K]-1} \left[d_t +  u_t^{1-\alpha} 2^{\alpha - 1} (M^\alpha + B^\alpha) \right] + \sum_{t \in [T]}\frac{\eta_t(\eta_\ell^t)^2 G L K^2 u_t}{2}.\\
\end{aligned}
\end{equation*}
Now, we move to the asymptotic regime. Let $\eta_t = \Theta(t^\omega)$, $\eta_\ell^t = \Theta(t^\nu)$, $d_t = \Theta(t^\gamma)$, $u_t = \Theta(t^\zeta)$, $\widetilde{d}_t = \Theta(t^{\widetilde{\gamma}})$, and $u_t = \Theta(t^{\widetilde{\zeta}})$. This gives after routine calculations that
\begin{equation*}
\begin{aligned}
\min_{t \in [T]} \mathbb{E}[\|\nabla F(x_{t-1})\|^2] \lesssim \mathcal{O}\left(T^{-\omega - \nu - 1} + T^{\omega + 2\widetilde{\zeta} - \nu} +T^{\widetilde{\gamma} - \nu} + T^{(\alpha-1)\nu + (1-\alpha)\widetilde{\zeta}} + T^\gamma + T^{(1-\alpha)\zeta} + T^{\nu + \zeta} \right).
\end{aligned}
\end{equation*}
To attain convergence of the RHS, it is clear that we must impose $\zeta,\widetilde{\zeta} > 0 > \gamma, \widetilde{\gamma}$, for $\omega, \nu \le 0$. Additionally, we have further constrained
\begin{equation*}
-1< \omega + \nu, \quad \nu + \zeta < 0, \quad \max\{\omega + 2\widetilde{\zeta},\widetilde{\gamma}\} < \nu, 
\end{equation*}
which ensures that the LHS diverges at a scale faster than logarithmic, validating the asymptotic regime and concluding the proof. To obtain the rate of convergence, we may let for $\widetilde{\varepsilon} \in (0,1/8)$,
\begin{equation*}
\omega = -\frac{1}{2}, \quad \nu = -\frac{1}{4}, \quad \widetilde{\zeta} = \frac{1}{8} - \widetilde{\varepsilon}, \quad \widetilde{\gamma} = - \frac{1}{8} - \widetilde{\varepsilon}, \quad \zeta = \frac{\alpha(1-\alpha)}{4}.
\end{equation*}
This gives that $Bi^2Clip$ converges with maximal rate at least $\mathcal{O}(T^{-r})$, where for $\widetilde{\varepsilon} \in (0,1/8)$ and $\alpha > 1$,
\begin{equation*}
%\resizebox{0.5\textwidth}{!}{%
%$
r := \min \left\{\frac{(\alpha-1)\alpha}{4}, \ \widetilde{\varepsilon}, \ \frac{\alpha-1}{4}-(1-\alpha)(\frac{1}{8}-\widetilde{\varepsilon}) \right\}.
%$%
%}
\end{equation*}
\end{proof}


\begin{remark}
We note that setting $\widetilde{d}_t = 0$, $\widetilde{u}_t = \infty$, and $\eta_t = 1$ recovers the simple averaging operation that can be done at the outer optimizer as a special case of $Bi^2Clip$, procuring Avg-$BiClip$. Therefore, one perspective of viewing $Bi^2Clip$ may be the addition of computation and memory efficient adaptive mimicry into traditional SGD-Averaging distributed training frameworks, that aims to dynamically adjust to the gradient distributional geometry. Similarly, for specific hyperparameter choices, $Bi^2Clip$ collapses into $BiClip$-SGD, with upper and lower thresholding applied by the outer optimizers only to accumulated model updates from the inner compute nodes.
\end{remark}

% \begin{remark}
% Additionally, $Bi^2Clip$ collapses into Avg-$BiClip$ or $BiClip$-SGD for select hyperparameters. 
% \end{remark}

% \paragraph{TailOPT with adaptive outer optimizers.}

Now, in the following subsections, we further analyze the convergence behavior of TailOPT under additional varying adaptive optimizer instantiations. The Adagrad instantiation (Algorithm~\ref{Adagrad_BiClip_SGD}) collects pseudogradients and sums their squares, effectively implementing a form of implicit clipping. However, it aggressively decays coordinate-wise learning rates, which can limit performance. To address this, we introduce RMSProp-$TailClip$ (Algorithm~\ref{RMS_BiClip_SGD}), which relaxes the preconditioning by employing an exponentially decaying moving average of the second moment. In both cases, we prove that the minimum expected gradient converges to $0$. Additionally, by incorporating a moving average of the first pseudogradient moment as a form of momentum, we derive Algorithm~\ref{Adam_BiClip_SGD}. For this variant, we show that the expected minimal gradient does not diverge even under restarting of the algorithm, which in practice translates to the update of any singular step not diverging in expectation. As in the main paper, $TailClip$ refers to either $BiClip$ or $L_2Clip$, and we provide our proofs for $BiClip$ for added generality over $L_2Clip$.


\subsection{Convergence of Adagrad-$TailClip$}\label{AdaGrad_BiClip_Appendix}


We begin by providing the pseudocode of Adagrad-$TailClip$ (Algorithm~\ref{Adagrad_BiClip_SGD}). Then, we have the following result.

\begin{algorithm}
\caption{Adagrad-$TailClip$}\label{Adagrad_BiClip_SGD}
\begin{algorithmic}[1]
\REQUIRE Initial model $x_1$, 
learning rate schedule $\eta_t$, clipping schedules $u_t,$ $d_t$ \\ Synchronization timestep $z \in \mathbb{Z}_{>0}$, adaptivity parameter $\tau > 0$
\FOR{$t = 1, \dots, T$} 
    \FOR{each node $i \in [N]$ in parallel}
    \STATE $x_{i,0}^t \leftarrow x_t$
    \FOR{each local step $k \in [z]$}
        \STATE Draw minibatch gradient $g^t_{i,k} = \nabla F_i(x^t_{i,k}, \xi^t_{i,k})$
        \STATE $x^{t+1}_{i,k} \leftarrow  x^t_{i,k} - \eta_t \cdot TailClip(u_t,d_t,g^t_{i,k})$ 
        \ENDFOR
    \ENDFOR
    \STATE $\Delta_t = \frac{1}{N} \sum_{i \in [N]} \left(x_{i,z}^{t} - x_{t-1}\right)$, \quad $\widetilde{m}_t \leftarrow \Delta_t$
    %\STATE $\widetilde{m}_t \leftarrow \Delta_t$%= \widetilde{\beta}_1 \widetilde{m}_{t-1} + (1 - \widetilde{\beta}_1) \Delta_t$
    \STATE $\widetilde{v}_t = \widetilde{v}_{t-1} + \Delta_t^2$ 
    \STATE $x_{t} = x_{t-1} + \eta \frac{\widetilde{m}_t}{\sqrt{\widetilde{v}_t} + \tau}$
\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{theorem}~\label{ServerAdagradConvergenceTheroem}
Let the clipping and learning rate thresholds satisfy $\eta_t = \Theta(t^\omega)$, $\eta_\ell^t = \Theta(t^\nu)$, $d_t = \Theta(t^\gamma)$, and $u_t = \Theta(t^\zeta)$ for the conditions
\begin{equation*}
\begin{aligned}
&0<\zeta< \min\left\{\frac{1}{4},\omega + \frac{1}{2}\right\}, \quad -\frac{1}{2}<\omega \le 0, \quad \gamma < \min \left\{0,-\nu -\zeta - \frac{1}{2}\right\},\\
&\quad \quad \nu < \min\left\{-\frac{1}{6} - \frac{4}{3}\zeta, -\frac{1}{4} - \frac{3}{2}\zeta - \frac{1}{2}\omega, -\frac{1}{2} + (\alpha - 2) \zeta \right\} .
\end{aligned}
\end{equation*}
Then, we have that
\begin{equation*}
\min_{t \in [T]}\mathbb{E}\left\| \nabla F(x_t)\right\|^2 \le \Psi_1 + \Psi_2 + \Psi_3 + \Psi_4 + \Psi_5 + \Psi_6, 
\end{equation*}
where the $\Psi_i$ are upper bounded by
\begin{equation*}
\begin{aligned}
& \Psi_1 \le \mathcal{O}(T^{-\omega + \zeta - \frac{1}{2}}), \quad \Psi_2 \le \mathcal{O}(T^{\omega + 2\nu + 3\zeta + \frac{1}{2}}), \quad \Psi_3 \le \mathcal{O}(T^{4\zeta + 3\nu + \frac{1}{2}}), \\
& \Psi_4 \le \mathcal{O}(T^{2\nu + 2\zeta + \frac{1}{2}}), \quad \Psi_5 \le \mathcal{O}(T^{\nu + \gamma + \zeta + \frac{1}{2}}), \quad \Psi_6 \le \mathcal{O}(T^{\nu + (2-\alpha)\zeta + \frac{1}{2}}),
\end{aligned}
\end{equation*}
which guarantees convergence via an inversely proportional power law decay with respect to $T$. The maximal convergence rate is given by $\mathcal{O}(1/\sqrt{T})$.
\end{theorem}

\begin{proof}
We analyze the convergence of the global objective, where model weights are updated in a distributed fashion via local $BiClip$ under heavy-tailed noise. By $L$-smoothness, we have 
\begin{align}
F(x_{t}) &\leq F(x_{t-1}) + \left\langle \nabla F(x_{t-1}), x_{t} - x_{t-1} \right\rangle + \frac{L}{2} \left\| x_{t} - x_{t-1} \right\|^2 \nonumber \\
&= F(x_{t-1}) + \underbrace{\eta_{t} \left\langle \nabla F(x_{t-1}), \frac{\Delta_t}{\sqrt{\widetilde{v}_t} + \tau} \right\rangle}_{A_1} + \frac{\eta_{t}^2 L}{2} \left\| \frac{\Delta_t}{\sqrt{\widetilde{v}_t} + \tau} \right\|^2, \nonumber
\end{align}
which we further decompose via noting that 
\begin{equation*}
\begin{aligned}
A_1 &=  \eta_{t} \left\langle \nabla F(x_{t-1}), \frac{\Delta_t(\sqrt{\widetilde{v}_{t-1}} - \sqrt{\widetilde{v}_{t}})}{(\sqrt{\widetilde{v}_t} + \tau)(\sqrt{\widetilde{v}_{t-1}} + \tau)} \right\rangle + \eta_{t} \left\langle \nabla F(x_{t-1}), \frac{\Delta_t}{\sqrt{\widetilde{v}_{t-1}} + \tau} \right\rangle \\
&= \eta_{t} \left\langle \nabla F(x_{t-1}), \frac{-\Delta_t^3}{(\sqrt{\widetilde{v}_t} + \tau)(\sqrt{\widetilde{v}_{t-1}} + \tau)(\sqrt{\widetilde{v}_{t-1}} + \sqrt{\widetilde{v}_{t}})} \right\rangle + \eta_{t} \left\langle \nabla F(x_{t-1}), \frac{\Delta_t}{\sqrt{\widetilde{v}_{t-1}} + \tau} \right\rangle \\
&\le \eta_{t} \left\langle \left| \nabla F(x_{t-1})\right|, \frac{|\Delta_t|^3}{(\sqrt{\widetilde{v}_t} + \tau)(\sqrt{\widetilde{v}_{t-1}} + \tau)(\sqrt{\widetilde{v}_{t-1}} + \sqrt{\widetilde{v}_{t}})} \right\rangle + \underbrace{\eta_{t} \left\langle \nabla F(x_{t-1}), \frac{\Delta_t}{\sqrt{\widetilde{v}_{t-1}} + \tau} \right\rangle}_{B_1}.
\end{aligned}
\end{equation*}
To bound $B_1$, we extract a negative gradient norm
\begin{equation*}
\begin{aligned}
&B_1 = \underbrace{\eta_{t} \left\langle \nabla F(x_{t-1}), \frac{\Delta_t}{\sqrt{\widetilde{v}_{t-1}} + \tau} + \frac{K \eta_\ell^t \nabla F(x_{t-1})}{\sqrt{\widetilde{v}_{t-1}} + \tau} \right\rangle}_{B_2} - K \eta_{t} \eta_\ell^{t} \left\| \frac{\nabla F(x_{t-1})}{\sqrt{\sqrt{\widetilde{v}_{t-1}} + \tau}} \right\|^2 ,
\end{aligned}
\end{equation*}
where $B_2$ decomposes further into
\begin{equation*}
\begin{aligned}
B_2 &= \eta_{t} \left\langle \nabla F(x_{t-1}), \frac{\Delta_t}{\sqrt{\widetilde{v}_{t-1}} + \tau} + \frac{\sum_{i \in [N]} \sum_{v \in [K]-1} p_i \eta_\ell^t(\nabla F_i(x_{i,v}^t) - \nabla F_i(x_{i,v}^t))}{\sqrt{\widetilde{v}_{t-1}} + \tau}  + \frac{K \eta_\ell^t \nabla F(x_{t-1})}{\sqrt{\widetilde{v}_{t-1}} + \tau} \right\rangle
\end{aligned}
\end{equation*}
Here, we use the convention $[K]-1 = \{0,\dots, K-1 \}$, and that summation over null indices are zero (e.g. $\sum_{j = K}^{K-1} [\ \cdot \ ] = 0$). Now, recall  
\begin{equation*}
\begin{aligned}
\Delta_t &:= \sum_{i \in [N]} p_i \Delta_i^t = \sum_{i \in [N]} p_i (x_{i,K}^t - x_{i,0}^t) = -\sum_{i \in [N]} \sum_{v \in [K]-1} p_i \eta_\ell^t \cdot  \hat{g}_{i,v}^t\\
&= -\sum_{i \in [N]} \sum_{v \in [K]-1} p_i \eta_\ell^t \cdot BiClip(u_t,d_t,\nabla F_i(x_{i,v}^t) + \xi_{i,v}^t), 
\end{aligned}
\end{equation*}
which implies $B_2 = C_1 + C_2$ for 
\begin{equation*}
\begin{aligned}
    C_1 &= \eta_{t} \left\langle \nabla F(x_{t-1}), \frac{\sum_{i \in [N]} \sum_{v \in [K]-1} p_i \eta_\ell^t (\nabla F_i(x_{i,v}^t) - BiClip(u_t,d_t,\nabla F_i(x_{i,v}^t) + \xi_{i,v}^t)) }{\sqrt{\widetilde{v}_{t-1}} + \tau} \right\rangle\\
    C_2 &= \eta_{t} \left\langle \nabla F(x_{t-1}), \frac{\sum_{i \in [N]} \sum_{v \in [K]-1} p_i \eta_\ell^t (\nabla F_i(x_{i,0}^t) - \nabla F_i(x_{i,v}^t)) }{\sqrt{\widetilde{v}_{t-1}} + \tau} \right\rangle.
\end{aligned}
\end{equation*}
Letting $\mathbb{E}_{t}[\ \cdot \ ]$ condition over all stochasticity up to global step $t$, we have that $\mathbb{E}_{t}[C_1]$ is equal to 
\begin{equation*}
\begin{aligned}
\eta_{t} \left\langle \nabla F(x_{t-1}), \frac{\sum_{i \in [N]} \sum_{v \in [K]-1} p_i \eta_\ell^t (\mathbb{E}_{t}[\nabla F_i(x_{i,v}^t) + \xi_{i,v}^t - BiClip(u_t,d_t,\nabla F_i(x_{i,v}^t) + \xi_{i,v}^t)]) }{\sqrt{\widetilde{v}_{t-1}} + \tau} \right\rangle.\\
\end{aligned}
\end{equation*}
For $D_1:= \mathbb{E}_{t}[\nabla F_i(x_{i,v}^t) + \xi_{i,v}^t - BiClip(u_t,d_t,\nabla F_i(x_{i,v}^t) + \xi_{i,v}^t)]$, we have by convexity and Jensen that
\begin{equation*}
\begin{aligned}
\|D_1\| &\le \mathbb{E}_{t}[\|\nabla F_i(x_{i,v}^t) + \xi_{i,v}^t - BiClip(u_t,d_t,\nabla F_i(x_{i,v}^t) + \xi_{i,v}^t)\|]\\
&\le d_t \mathbb{P}(\|\nabla F_i(x_{i,v}^t) + \xi_{i,v}^t)\|\le d_t) \\
&+ \underbrace{\mathbb{E}_{t}[\|\nabla F_i(x_{i,v}^t) + \xi_{i,v}^t - BiClip(u_t,d_t,\nabla F_i(x_{i,v}^t) + \xi_{i,v}^t)\|\chi \left( \|\nabla F_i(x_{i,v}^t) + \xi_{i,v}^t\|\ge u_t\right)]}_{D_2}.
\end{aligned}
\end{equation*}
Piecewise continuity of $F_i(x)$ is clear via the existence of $\nabla F_i(x)$. This gives that 
\begin{equation*}
\begin{aligned}
&\mathbb{E}_{t}[\|\nabla F_i(x_{i,v}^t) + \xi_{i,v}^t \|^\alpha \chi \left( \|\nabla F_i(x_{i,v}^t) + \xi_{i,v}^t))\|\ge u_t\right)] \le \mathbb{E}_{t}[\|\nabla F_i(x_{i,v}^t) + \xi_{i,v}^t \|^\alpha]\\
&\le 2^\alpha \mathbb{E}_{t}\left[\left\|\frac{\nabla F_i(x_{i,v}^t) + \xi_{i,v}^t}{2} \right\|^\alpha\right] \le 
2^\alpha \mathbb{E}_{t}\left[\frac{\left\|\nabla F_i(x_{i,v}^t)\right\|^\alpha}{2} + \frac{\left\|\xi_{i,v}^t\right\|^\alpha}{2}\right]%\\&
=2^{\alpha - 1} (M^\alpha + B^\alpha),
\end{aligned}
\end{equation*}
where now, $M:= \max_{x\in\mathcal{X}, i \in [N]} \|\nabla F_i (x)\|$. Thus, we may bound $D_2$ via reduction to the $\alpha$-moment:
\begin{equation*}
\begin{aligned}
D_2 &\le 2^{\alpha-1} (M^\alpha + B^\alpha) \mathbb{E}_{t}[\|\nabla F_i(x_{i,v}^t) + \xi_{i,v}^t\|^{1-\alpha} \chi \left( \|\nabla F_i(x_{i,v}^t) + \xi_{i,v}^t))\|\ge u_t\right)]\\
&\le 2^{\alpha-1} (M^\alpha + B^\alpha) u_t^{1-\alpha} \mathbb{P}\left( \|\nabla F_i(x_{i,v}^t;\xi_{i,v}^t))\|\ge u_t\right).
\end{aligned}
\end{equation*}
Collecting inequalities gives 
\begin{equation*}
\|D_1\| \le d_t \mathbb{P}(\|\nabla F_i(x_{i,v}^t; \xi_{i,v}^t))\|\le d_t) + 2^{\alpha-1}(M^\alpha + B^\alpha) u_t^{1-\alpha}\mathbb{P}\left( \|\nabla F_i(x_{i,v}^t;\xi_{i,v}^t))\|\ge u_t\right).
\end{equation*}
Therefore, 
\begin{equation*}
\begin{aligned}
\mathbb{E}_t[C_1] &\le \frac{\eta_{t} G d}{\tau} \sum_{i \in [N]} \sum_{v \in [K]-1} p_i \eta_\ell^t d_t \mathbb{P}(\|\nabla F_i(x_{i,v}^t; \xi_{i,v}^t))\|\le d_t) \\
&+ \frac{2^{\alpha-1} \ \eta_{t} G d}{\tau} \sum_{i \in [N]} \sum_{v \in [K]-1} p_i \eta_\ell^t (M^\alpha + B^\alpha) u_t^{1-\alpha}\mathbb{P}\left( \|\nabla F_i(x_{i,v}^t;\xi_{i,v}^t))\|\ge u_t\right).
\end{aligned}
\end{equation*}
To bound $C_2$, we note that via $L$-smoothness, we have
\begin{equation*}
\begin{aligned}
C_2 &\le \frac{\eta_{t} GLd}{\tau} \sum_{i \in [N]} \sum_{v \in [K]-1} p_i \eta_\ell^t \|x_{i,0}^t - x_{i,v}^t\| \\
&\le \frac{\eta_{t} GLd}{\tau} \sum_{i \in [N]} \sum_{v \in [K]-1} p_i \eta_\ell^t \|x_{i,0}^t + \sum_{r=1}^{v-1} (x_{i,r}^t-x_{i,r}^t) - x_{i,v}^t\|\\
&\le \frac{\eta_{t} GLd}{\tau} \sum_{i \in [N]} \sum_{v \in [K]-1} \sum_{r \in [v]} p_i \eta_\ell^t \|x_{i,r}^t -x_{i,r-1}^t\|\\
&\le \frac{\eta_{t} GLK^2d}{2\tau} (\eta_\ell^t)^2 u_t.
\end{aligned}
\end{equation*}
Noting that $\|\Delta_t\| \le \eta_\ell^t u_t K$, we thus obtain
\begin{align}
\mathbb{E}_{t}[F(x_{t})] &\leq F(x_{t-1}) + \frac{\eta_{t}^2(\eta_\ell^t)^2 u_t^2 K^2L}{2\tau^2} + \frac{\eta_{t} G d K^3 u_t^3 (\eta_\ell^t)^3}{\tau^3} - K \eta_{t} \eta_\ell^t \left\| \frac{\nabla F(x_{t-1})}{\sqrt{\sqrt{\widetilde{v}_{t-1}} + \tau}} \right\|^2 \nonumber \\
&+ \frac{\eta_{t} GLK^2d}{2\tau} (\eta_\ell^t)^2 u_t + \frac{\eta_{t} G d}{\tau} \sum_{i \in [N]} \sum_{v \in [K]-1} p_i \eta_\ell^t d_t \mathbb{P}(\|\nabla F_i(x_{i,v}^t; \xi_{i,v}^t))\|\le d_t) \nonumber \\
&+ \frac{2^{\alpha-1} \ \eta_{t} G d}{\tau} \sum_{i \in [N]} \sum_{v \in [K]-1} p_i \eta_\ell^t(M^\alpha + B^\alpha) u_t^{1-\alpha}\mathbb{P}\left( \|\nabla F_i(x_{i,v}^t;\xi_{i,v}^t))\|\ge u_t\right). \nonumber
%\label{whereitstartedAdaSquareSM3}
\end{align}
Taking expectations on both sides and telescoping gives via the tower law of expectation,
\begin{equation*}
\begin{aligned}
&\underbrace{\sum_{t \in [T]} K \eta_{t} \eta_\ell^t \mathbb{E}\left[\left\| \frac{\nabla F(x_{t-1})}{\sqrt{\sqrt{\widetilde{v}_{t-1}} + \tau}} \right\|^2\right]}_{E_1}  \le  \underbrace{\mathbb{E}[F(x_{T})-F(x_{0})]}_{E_2} \underbrace{+ \sum_{t \in [T]}  \frac{\eta_{t}^2(\eta_\ell^t)^2 u_t^2 K^2L}{2\tau^2}}_{E_3} \underbrace{+ \sum_{t \in [T]} \frac{\eta_{t} G d K^3 u_t^3 (\eta_\ell^t)^3}{\tau^3}}_{E_4}\\
&\underbrace{+\sum_{t \in [T]} \frac{\eta_{t} GLK^2d}{2\tau} (\eta_\ell^t)^2 u_t}_{E_5} \underbrace{+ \sum_{t \in [T]}\frac{\eta_{t} G d}{\tau} \sum_{i \in [N]} \sum_{v \in [K]-1} p_i \eta_\ell^t d_t \mathbb{P}(\|\nabla F_i(x_{i,v}^t; \xi_{i,v}^t))\|\le d_t)}_{E_6} \\
&\underbrace{+ \sum_{t \in [T]}\frac{2^{\alpha -1} \ \eta_{t} G d}{\tau} \sum_{i \in [N]} \sum_{v \in [K]-1} p_i \eta_\ell^t(M^\alpha + B^\alpha) u_t^{1-\alpha}\mathbb{P}\left( \|\nabla F_i(x_{i,v}^t;\xi_{i,v}^t))\|\ge u_t\right)}_{E_7},
\end{aligned}
\end{equation*}
where we have enumerated each term from $E_1$ to $E_7$ for clarity. To simplify notation, we now move to the asymptotic regime. Letting $\eta_t = \Theta(t^\omega)$, $\eta_\ell^t = \Theta(t^\nu)$, $d_t = \Theta(t^\gamma)$, and $u_t = \Theta(t^\zeta)$, we have via standard integral bounds that  
\begin{equation*}
\begin{aligned}
& E_1 \ge \Omega \left( T^{\omega + \nu + 1} \cdot T^{-\zeta - \nu - \frac{1}{2}} \cdot \min_{t \in [T]} \mathbb{E}[\|\nabla F(x_{t})\|^2] \right) = \Omega \left( T^{\omega -\zeta + \frac{1}{2}} \cdot \min_{t \in [T]} \mathbb{E}[\|\nabla F(x_{t})\|] \right), \\
& E_2 \le \max_{x \in \mathcal{X}} F(x) - \min_{y \in \mathcal{X}} F(y) = \mathcal{O}(1), \quad E_3 \le \mathcal{O}(T^{2 \omega + 2 \nu + 2 \zeta + 1}), \quad E_4 \le \mathcal{O}(T^{\omega + 3\zeta + 3\nu + 1}), \\
& E_5 \le \mathcal{O}(T^{\omega + 2\nu + \zeta + 1}), \quad E_6 \le \mathcal{O}(T^{\omega + \nu + \gamma + 1}),\quad E_7 \le \mathcal{O}(T^{\omega + \nu + (1-\alpha) \zeta + 1})
\end{aligned}
\end{equation*}
where any $E_i$ residues of $\mathcal{O}(1)$ for $i \ge 2$ have been incorporated into the upper bound for $E_2$. We note that the bound may be sharpened as the probabilistic terms must necessarily decay if $d_t \to 0$, $u_t \to \infty$, which further diminishes $E_6$, $E_7$. Now, to attain convergence of the minimal gradient, we impose the conditions
\begin{equation*}
\begin{aligned}
& \Lambda_1 : \zeta > 0 \quad \text{and} \quad \gamma < 0,  \quad \Lambda_2: \omega - \zeta + \frac{1}{2} > 0, \quad \Lambda_3:  \omega +2\nu + 3\zeta + \frac{1}{2} < 0,   \\
&\Lambda_4 : 4\zeta + 3\nu + \frac{1}{2} < 0, \quad \Lambda_5 : 2\nu + 2\zeta + \frac{1}{2} < 0, \quad \Lambda_6: \nu + \gamma + \zeta + \frac{1}{2} < 0, \\
& \Lambda_7: \nu + (2-\alpha)\zeta + \frac{1}{2} < 0.
\end{aligned}
\end{equation*}
We note that each condition $\Lambda_{i\ge2}$ comes from $E_{i}/E_{1} \to 0$, $T \to \infty$, as any residual terms are subsumed by $\mathcal{O}(1)$, which decays via $\Lambda_2$. Setting $0< \zeta < 1/4$, we have
\begin{equation*}
\begin{aligned}
& \nu < \min\{-\frac{1}{6} - \frac{4}{3}\zeta, -\frac{1}{4} - \frac{3}{2}\zeta - \frac{1}{2}\omega, -\frac{1}{2} + (\alpha - 2) \zeta \} \\
& \gamma < -\nu -\zeta - \frac{1}{2}, \quad \omega + \frac{1}{2} > \zeta, \quad -\frac{1}{2}<\omega \le 0.
\end{aligned}
\end{equation*}
Therefore, any such selection stabilizes the minimum gradient, which guarantees convergence. It is straightforward to see that $\Lambda_2$ is the dominating condition, for which $\omega\le 0$ and $\zeta \in (0,1/4)$ gives the convergence rate $\mathcal{O}(1/\sqrt{T})$ as $\omega = 0$ and $\zeta \to 0^+$.
\end{proof}

\begin{remark}\label{remark}
In the case of coordinate-wise clipping, all major adjustments up to a scaling factor of $\sqrt{d}$ are made in the terms bounding $\mathbb{E}[C_1]$. In this case, the proof proceeds as follows. 
\end{remark}

Defining $| \cdot |$ to act coordinatewise, $\mathbb{E}_{t}[C_1]$ is now less than or equal to 
\begin{equation*}
\begin{aligned}
\eta_{t} \left\langle \left|\nabla F(x_{t-1})\right|, \frac{\sum_{i \in [N]} \sum_{v \in [K]-1} p_i \eta_\ell^t |\mathbb{E}_{t}[\nabla F_i(x_{i,v}^t) + \xi_{i,v}^t - BiClip(u_t,d_t,\nabla F_i(x_{i,v}^t) + \xi_{i,v}^t)]| }{\sqrt{\widetilde{v}_{t-1}} + \tau} \right\rangle.\\
\end{aligned}
\end{equation*}
Therefore by Jensen, 
\begin{equation*}
\begin{aligned}
\mathbb{E}_{t}[C_1]&\le \frac{\eta_t \eta_\ell^t G}{\tau} \sum_{i \in [N]} \sum_{v \in [K]-1} \sum_{j \in [d]} p_i \mathbb{E}_{t}[\underbrace{|\nabla F_i(x_{i,v}^t) + \xi_{i,v}^t - BiClip(u_t,d_t,\nabla F_i(x_{i,v}^t) + \xi_{i,v}^t)|_j}_{D_{1,j}}].\\
\end{aligned}
\end{equation*}
We note that $\mathbb{E}_{t} [D_{1,j}]$ can be upper bounded by $D_{2,j} + D_{3,j}$ where
\begin{equation*}
\begin{aligned}
D_{2,j} &= \mathbb{E}_t \left[ D_{1,j} \cdot \chi \left( |\nabla F_i (x_{i,v}^t;\xi_{i,v}^t)|_j \le d_t)\right)\right] \le d_t \mathbb{P}\left( |\nabla F_i (x_{i,v}^t;\xi_{i,v}^t)|_j \le d_t)\right) \\
D_{3,j} &= \mathbb{E}_t \left[ |\nabla F_i (x_{i,v}^t;\xi_{i,v}^t)|_j \chi \left( |\nabla F_i (x_{i,v}^t;\xi_{i,v}^t)|_j \ge u_t)\right) \right] .
\end{aligned}
\end{equation*}
It follows that 
\begin{equation*}
\begin{aligned}
D_{3,j} & \le \mathbb{E}_t \left[ |\nabla F_i (x_{i,v}^t;\xi_{i,v}^t)|_j^\alpha |\nabla F_i (x_{i,v}^t;\xi_{i,v}^t)|_j^{1-\alpha} \chi \left( |\nabla F_i (x_{i,v}^t;\xi_{i,v}^t)|_j \ge u_t\right) \right]\\
&\le 2^{\alpha -1} (M^{\alpha} + B^{\alpha}) u_t^{1-\alpha} \mathbb{P} \left( |\nabla F_i (x_{i,v}^t;\xi_{i,v}^t)|_j \ge u_t\right).
\end{aligned}
\end{equation*}
Note that we used coordinate-wise bounded alpha moments for some $\alpha \in (1,2)$, $\mathbb{E}[|\xi_i|_j^\alpha] \le B_{i,j}^\alpha$. We therefore define the $M$ and $B$ to be
\begin{equation*}
M:= \max_{x \in \mathcal{X}, i \in [N], j \in [d]} |\nabla F_{i}(x)|_j \quad \text{and} \quad B = \max_{i \in [N], j\in [d]} B_{i,j}.
\end{equation*}
Comparing terms gives the identical asymptotic order of convergence to $L_2$ clipping in Theorem~\ref{ServerAdagradConvergenceTheroem}.

\subsection{Convergence of RMSProp-$TailClip$}\label{RMSProp_BiClip_Appendix}

\begin{algorithm}
\caption{RMS-$TailClip$}\label{RMS_BiClip_SGD}
\begin{algorithmic}[1]
\REQUIRE Initial model $x_1$, 
learning rate schedule $\eta_t$, clipping schedules $u_t,$ $d_t$ \\ Synchronization timestep $z \in \mathbb{Z}_{>0}$, adaptivity/EMA parameters $\tau > 0$, $\widetilde{\beta}_2 \in [0,1)$
\FOR{$t = 1, \dots, T$} 
    \FOR{each node $i \in [N]$ in parallel}
    \STATE $x_{i,0}^t \leftarrow x_t$
    \FOR{each local step $k \in [z]$}
        \STATE Draw minibatch gradient $g^t_{i,k} = \nabla F_i(x^t_{i,k}, \xi^t_{i,k})$
        \STATE $x^{t+1}_{i,k} \leftarrow  x^t_{i,k} - \eta_t \cdot TailClip(u_t,d_t,g^t_{i,k})$ 
        \ENDFOR
    \ENDFOR
    \STATE $\Delta_t = \frac{1}{N} \sum_{i \in [N]} \left(x_{i,z}^{t} - x_{t-1}\right)$, \quad $\widetilde{m}_t \leftarrow \Delta_t$
    %\STATE $\widetilde{m}_t \leftarrow \Delta_t$%= \widetilde{\beta}_1 \widetilde{m}_{t-1} + (1 - \widetilde{\beta}_1) \Delta_t$
    \STATE $\widetilde{v}_t = \widetilde{\beta}_2 \widetilde{v}_{t-1} + (1 - \widetilde{\beta}_2)\Delta_t^2$ 
    \STATE $x_{t} = x_{t-1} + \eta \frac{\widetilde{m}_t}{\sqrt{\widetilde{v}_t} + \tau}$
\ENDFOR
\end{algorithmic}
\end{algorithm}

For Algorithm~\ref{RMS_BiClip_SGD}, we have the following convergence bound.

\begin{theorem}\label{ServerRMSPropConvergenceTheroem}
For clipping and learning rate thresholds satisfying $\eta_t = \Theta(t^\omega)$, $\eta_\ell^t = \Theta(t^\nu)$, $d_t = \Theta(t^\gamma)$, and $u_t = \Theta(t^\zeta)$, let the conditions listed in Theorem~\ref{ServerAdagradConvergenceTheroem} hold. Then, local $BiClip$ with outer optimizer RMSProp stabilizes the expected minimum gradient $\min_{t \in [T]}\mathbb{E}[\|\nabla F(x_t)\|^2] \to 0^+$ with maximal rate $\mathcal{O}(1/\sqrt{T})$. Here, the exponential moving average parameter of the second pseudogradient moment is fixed within the range $\widetilde{\beta}_2 \in [0,1)$.
\end{theorem}
\begin{proof}
The proof for outer optimizer RMSProp builds on the prior proof for $BiClip$ with outer optimizer Adagrad. We skip repeated details for clarity of exposition, and concisely present only the main steps and ideas central to the proof for readability. $L$-smoothness gives as before  
\begin{align}
F(x_{t}) &\leq F(x_{t-1}) + \left\langle \nabla F(x_{t-1}), x_{t} - x_{t-1} \right\rangle + \frac{L}{2} \left\| x_{t} - x_{t-1} \right\|^2 \nonumber \\
&= F(x_{t-1}) + \eta_{t} \left\langle \nabla F(x_{t-1}), \frac{\Delta_t}{\sqrt{\widetilde{v}_t} + \tau} \right\rangle +  \frac{\eta_{t}^2 L}{2} \left\| \frac{\Delta_t}{\sqrt{\widetilde{v}_t} + \tau} \right\|^2. \label{globalRMSPropInequality}
\end{align}
We note the decomposition 
\begin{equation*}
\begin{aligned}
&\left\langle \nabla F (x_{t-1}), \frac{\Delta_t}{\sqrt{\widetilde{v}_t} + \tau} \right\rangle = \underbrace{\left\langle \nabla F (x_{t-1}), \frac{\Delta_t}{\sqrt{\widetilde{v}_{t}} + \tau} - \frac{\Delta_t}{\sqrt{\widetilde{\beta}_2\widetilde{v}_{t-1}} + \tau} \right\rangle}_{B_1}  +  \underbrace{\left\langle \nabla F (x_{t-1}), \frac{\Delta_t}{\sqrt{\widetilde{\beta}_2\widetilde{v}_{t-1}} + \tau} \right\rangle}_{B_2} .
\end{aligned}
\end{equation*}
To form an upper bound, we use that
\begin{equation*}
\begin{aligned}
& B_2 = \underbrace{\left\langle \nabla F (x_{t-1}), \frac{\Delta_t}{\sqrt{\widetilde{\beta}_2\widetilde{v}_{t-1}} + \tau} + \frac{K\eta_\ell^t \nabla F (x_{t-1})}{\sqrt{\widetilde{\beta}_2\widetilde{v}_{t-1}} + \tau} \right\rangle}_{C_{0}} - K\eta_\ell^t\left\| \frac{ \nabla F (x_{t-1})}{\sqrt{\sqrt{\widetilde{\beta}_2\widetilde{v}_{t-1}} + \tau}} \right\|^2 \\
\end{aligned}
\end{equation*}
where $C_{0} = C_{1} + C_{2}$ for
\begin{equation*}
\begin{aligned}
    C_{1} &= \left\langle \nabla F(x_{t-1}), \frac{\sum_{i \in [N]} \sum_{v \in [K]-1} p_i \eta_\ell^t (\nabla F_i(x_{i,v}^t) - BiClip(u_t,d_t,\nabla F_i(x_{i,v}^t) + \xi_{i,v}^t)) }{\sqrt{\widetilde{\beta}_2\widetilde{v}_{t-1}} + \tau} \right\rangle\\
    C_{2} &= \left\langle \nabla F(x_{t-1}), \frac{\sum_{i \in [N]} \sum_{v \in [K]-1} p_i \eta_\ell^t (\nabla F_i(x_{i,0}^t) - \nabla F_i(x_{i,v}^t)) }{\sqrt{\widetilde{\beta}_2\widetilde{v}_{t-1}} + \tau} \right\rangle.
\end{aligned}
\end{equation*}
By the tower law and conditioning on stochastic realizations up to $t-1$, we have as before
\begin{equation*}
\begin{aligned}
\mathbb{E} [C_{0}] &\le \frac{G d}{\tau} \sum_{i \in [N]} \sum_{v \in [K]-1} p_i \eta_\ell^t d_t \mathbb{P}(\|\nabla F_i(x_{i,v}^t; \xi_{i,v}^t))\|\le d_t) + \frac{GLK^2d}{2\tau} (\eta_\ell^t)^2 u_t\\
&+ \frac{2^{\alpha-1} G d}{\tau} \sum_{i \in [N]} \sum_{v \in [K]-1} p_i \eta_\ell^t (M^\alpha + B^\alpha) u_t^{1-\alpha}\mathbb{P}\left( \|\nabla F_i(x_{i,v}^t;\xi_{i,v}^t))\|\ge u_t\right)\\
&\le \frac{G d}{\tau} K\eta_\ell^t d_t + \frac{GLK^2d}{2\tau} (\eta_\ell^t)^2 u_t+ \frac{2^{\alpha-1} G d}{\tau} K \eta_\ell^t (M^\alpha + B^\alpha) u_t^{1-\alpha}.
\end{aligned}
\end{equation*}
To bound $B_1$, we have
\begin{equation*}
\begin{aligned}
B_1 &= \left\langle \nabla F (x_{t-1}), \frac{\Delta_t}{\sqrt{\widetilde{v}_{t} + \tau}} - \frac{\Delta_t}{\sqrt{\widetilde{\beta}_2\widetilde{v}_{t-1}} + \tau} \right\rangle \\
&= \left\langle \nabla F (x_{t-1}), \frac{(\widetilde{\beta}_2-1)\Delta_t^3}{\left(\sqrt{\widetilde{v}_{t}} + \tau \right) \left(\sqrt{\widetilde{\beta}_2\widetilde{v}_{t-1}} + \tau\right)\left(\sqrt{\widetilde{v}_{t}}+\sqrt{\widetilde{\beta}_2\widetilde{v}_{t-1}}\right)}  \right\rangle 
\end{aligned}
\end{equation*}
We prepare the global inequality~\eqref{globalRMSPropInequality} for telescoping. It is straightforward to see that collecting inequalities gives
\begin{equation*}
\begin{aligned}
&\mathbb{E}[F(x_t)] \le \mathbb{E}[F(x_{t-1})] + \frac{\eta_{t}^2 L K^2 u_t^2 (\eta_\ell^t)^2}{2\tau^2} - K\eta_t\eta_\ell^t\left\| \frac{ \nabla F (x_{t-1})}{\sqrt{\sqrt{\widetilde{\beta}_2\widetilde{v}_{t-1}} + \tau}} \right\|^2 \\
&\frac{G d}{\tau} K\eta_t\eta_\ell^t d_t + \frac{GLK^2d}{2\tau} \eta_t(\eta_\ell^t)^2 u_t+ \frac{2^{\alpha-1} G d}{\tau} K \eta_t\eta_\ell^t (M^\alpha + B^\alpha) u_t^{1-\alpha}  + \frac{dG(1-\widetilde{\beta}_2)(u_t\eta_\ell^t)^3}{\tau^3}
\end{aligned}
\end{equation*}
Rearranging and telescoping gives
\begin{equation*}
\begin{aligned}
& \sum_{t=1}^T K\eta_t\eta_\ell^t\mathbb{E} \left[\left\| \frac{ \nabla F (x_{t-1})}{\sqrt{\sqrt{\widetilde{\beta}_2\widetilde{v}_{t-1}} + \tau}} \right\|^2\right] \le \mathbb{E}[F(x_{0})] -\mathbb{E}[F(x_T)] + \sum_{t=1}^T \frac{\eta_{t}^2 L K^2 u_t^2 (\eta_\ell^t)^2}{2\tau^2} \\
&+\sum_{t=1}^T\left(\frac{G d}{\tau} K\eta_t\eta_\ell^t d_t + \frac{GLK^2d}{2\tau} \eta_t(\eta_\ell^t)^2 u_t+ \frac{2^{\alpha-1} G d}{\tau} K \eta_t\eta_\ell^t (M^\alpha + B^\alpha) u_t^{1-\alpha}  + \frac{dG(1-\widetilde{\beta}_2)(u_t\eta_\ell^t)^3}{\tau^3}\right)
\end{aligned}
\end{equation*}
By non-negativity of squared pseudogradients, we immediately obtain $\widetilde{\beta}_2\widetilde{v}_{t-1} \le \widetilde{v}_{t-1}$. Therefore up to constants, the convergence bound collapses to asymptotically equivalent bounds than that of Theorem~\ref{ServerAdagradConvergenceTheroem}, up to constant multiples from the exponentially decaying moving average of the second moment pseudogradient. The modification to coordinate-wise clipping instead of $L_2$ clipping follows analogous steps.
\end{proof}

Incorporating momentum into the first pseudogradient moment further complicates the analysis, and yields the results presented in Section~\ref{AdamBiClipAppendixSubsection}.


\subsection{Convergence of Adam-$TailClip$}\label{AdamBiClipAppendixSubsection}
\begin{algorithm}
\caption{Adam-$TailClip$}\label{Adam_BiClip_SGD}
\begin{algorithmic}[1]
\REQUIRE Initial model $x_1$, 
learning rate schedule $\eta_t$, clipping schedules $u_t,$ $d_t$ \\ Synchronization timestep $z \in \mathbb{Z}_{>0}$, adaptivity/EMA parameters $\tau > 0$, $\widetilde{\beta}_1, \widetilde{\beta}_2 \in [0,1)$
\FOR{$t = 1, \dots, T$} 
    \FOR{each node $i \in [N]$ in parallel}
    \STATE $x_{i,0}^t \leftarrow x_t$
    \FOR{each local step $k \in [z]$}
        \STATE Draw minibatch gradient $g^t_{i,k} = \nabla F_i(x^t_{i,k}, \xi^t_{i,k})$
        \STATE $x^{t+1}_{i,k} \leftarrow  x^t_{i,k} - \eta_t \cdot TailClip(u_t,d_t,g^t_{i,k})$ 
        \ENDFOR
    \ENDFOR
    \STATE $\Delta_t = \frac{1}{N} \sum_{i \in [N]} \left(x_{i,z}^{t} - x_{t-1}\right)$
    \STATE $\widetilde{m}_t = \widetilde{\beta}_1 \widetilde{m}_{t-1} + (1 - \widetilde{\beta}_1) \Delta_t$
    \STATE $\widetilde{v}_t = \widetilde{\beta}_2 \widetilde{v}_{t-1} + (1 - \widetilde{\beta}_2)\Delta_t^2$  
    \STATE $x_{t} = x_{t-1} + \eta \frac{\widetilde{m}_t}{\sqrt{\widetilde{v}_t} + \tau}$
\ENDFOR
\end{algorithmic}
\end{algorithm}

By incorporating a moving average of the first pseudogradient moment as a form of momentum, we derive Algorithm~\ref{Adam_BiClip_SGD}. For this variant, we demonstrate that the expected minimal gradient does not diverge, even when the algorithm undergoes restarts. Practically, this ensures that the located gradient value update of any single step remains bounded in expectation. The key challenge in proving convergence to $0$ arises from the moving average applied to the first moment, which effectively retains historical gradient information, significantly complicating the proof structure. Investigating the conditions required to guarantee convergence under this framework presents a promising avenue for future research. Our bound highlights that the dominating terms are influenced by the upper clipping threshold $u_r$, suggesting that the algorithms convergence behavior may be closely related the choice of this threshold and can be tuned in practice.

\begin{theorem}\label{AdamBiClipBoundedTheorem}
Let the exponentially decaying moving average parameters satisfy $\widetilde{\beta}_1 \in (0,1)$, $\widetilde{\beta}_2 \in [0,1)$ for the outer optimizer first and second order pseudogradient moments, respectively. Extremize the unbiased stochastic noise such that $\nexists \alpha_k \in (1,2)$ for which $\mathbb{E}[\|\xi_k\|^{\alpha_k}] < B_k^{\alpha_k}$ for integrable $\xi_k$. Then, Algorithm~\ref{Adam_BiClip_SGD} gives under constant upper clipping threshold invariant to global timestep $t$ ($\zeta = 0$) that
\begin{equation*}
\min_{t \in [T]}\mathbb{E}[\|\nabla F(x_t)\|^2] \lesssim \mathcal{O}(1),
\end{equation*}
where for $\eta_t = \Theta(t^\omega)$, $\eta_\ell^t = \Theta(t^\nu)$, and $d_t = \Theta(t^\gamma)$, we impose
\begin{equation}\label{AdamConditions}
\begin{aligned}
\nu \in (-1,0), \quad  -\nu - 1 < \omega \le 0, \quad -(1+\nu + \omega) < \gamma < 0. 
\end{aligned}
\end{equation}
\end{theorem}


\begin{proof}
As in the case of outer optimizer Adagrad, we analyze the convergence of the global objective. By $L$-smoothness, we have  
\begin{align}
F(x_{t}) &\leq F(x_{t-1}) + \left\langle \nabla F(x_{t-1}), x_{t} - x_{t-1} \right\rangle + \frac{L}{2} \left\| x_{t} - x_{t-1} \right\|^2 \nonumber \\
&= F(x_{t-1}) + \eta_{t} \left\langle \nabla F(x_{t-1}), \underbrace{\frac{\widetilde{\beta}_1^t \widetilde{m}_0 + (1-\widetilde{\beta}_1)\sum_{r=1}^t \widetilde{\beta}_1^{t-r} \Delta_r}{\sqrt{\widetilde{v}_t} + \tau}}_{A_1} \right\rangle +  \frac{\eta_{t}^2 L}{2} \left\|A_1\right\|^2. 
\end{align}
To proceed with the proof, we note that
\begin{equation*}
\left\langle \nabla F(x_{t-1}),A_1 \right\rangle = \left\langle \nabla F(x_{t-1}),\frac{\widetilde{\beta}_1^t \widetilde{m}_0 }{\sqrt{\widetilde{v}_t} + \tau} \right\rangle + (1-\widetilde{\beta}_1)\sum_{r=1}^t \widetilde{\beta}_1^{t-r} \left\langle \nabla F(x_{t-1}),\frac{\Delta_r}{\sqrt{\widetilde{v}_t} + \tau} \right\rangle,
\end{equation*}
which we further decompose by using
\begin{equation*}
\begin{aligned}
&\left\langle \nabla F (x_{t-1}), \frac{\Delta_r}{\sqrt{\widetilde{v}_t} + \tau} \right\rangle = \underbrace{\sum_{q=0}^{t-r} \left\langle \nabla F (x_{t-1}), \frac{\Delta_r}{\sqrt{\widetilde{\beta}_2^{q}\widetilde{v}_{t-q}} + \tau} - \frac{\Delta_r}{\sqrt{\widetilde{\beta}_2^{q+1}\widetilde{v}_{t-q-1}} + \tau} \right\rangle}_{A_{1,q}} \\
& \underbrace{+  \left\langle \nabla F (x_{t-1}) - \nabla F (x_{r-1}), \frac{\Delta_r}{\sqrt{\widetilde{\beta}_2^{t-r+1}\widetilde{v}_{r-1}} + \tau} \right\rangle}_{B_1} \underbrace{+  \left\langle \nabla F (x_{r-1}), \frac{\Delta_r}{\sqrt{\widetilde{\beta}_2^{t-r+1}\widetilde{v}_{r-1}} + \tau} \right\rangle}_{B_2}.
\end{aligned}
\end{equation*}
We have that
\begin{equation*}
\begin{aligned}
&A_{1,q} =  \sum_{q=0}^{t-r} \left\langle \nabla F (x_{t-1}), \frac{\Delta_r\left(\sqrt{\widetilde{\beta}_2^{q+1}\widetilde{v}_{t-q-1}} - \sqrt{\widetilde{\beta}_2^{q}\widetilde{v}_{t-q}} \right)}{\left(\sqrt{\widetilde{\beta}_2^{q}\widetilde{v}_{t-q}} + \tau\right)\left(\sqrt{\widetilde{\beta}_2^{q+1}\widetilde{v}_{t-q-1}} + \tau\right)} \right\rangle =  \sum_{q=0}^{t-r} B_{1,q} \\
&:=  \sum_{q=0}^{t-r} \left\langle \nabla F (x_{t-1}), \frac{-(1-\widetilde{\beta}_2) \widetilde{\beta}_2^q \Delta_{t-q}^2\Delta_r}{\left(\sqrt{\widetilde{\beta}_2^{q}\widetilde{v}_{t-q}} + \tau\right)\left(\sqrt{\widetilde{\beta}_2^{q+1}\widetilde{v}_{t-q-1}} + \tau\right) \left(\sqrt{\widetilde{\beta}_2^{q+1}\widetilde{v}_{t-q-1}} + \sqrt{\widetilde{\beta}_2^{q}\widetilde{v}_{t-q}} \right)} \right\rangle .
\end{aligned}
\end{equation*}
To upper bound $B_2$, we observe
\begin{equation*}
\begin{aligned}
& B_2 = \underbrace{\left\langle \nabla F (x_{r-1}), \frac{\Delta_r}{\sqrt{\widetilde{\beta}_2^{t-r+1}\widetilde{v}_{r-1}} + \tau} + \frac{K\eta_\ell^r \nabla F (x_{r-1})}{\sqrt{\widetilde{\beta}_2^{t-r+1}\widetilde{v}_{r-1}} + \tau} \right\rangle}_{C_{0,r}} - K\eta_\ell^r\left\| \frac{ \nabla F (x_{r-1})}{\sqrt{\sqrt{\widetilde{\beta}_2^{t-r+1}\widetilde{v}_{r-1}} + \tau}} \right\|^2 \\
\end{aligned}
\end{equation*}
where $C_{0,r} = C_{1,r} + C_{2,r}$ for
\begin{equation*}
\begin{aligned}
    C_{1,r} &= \left\langle \nabla F(x_{r-1}), \frac{\sum_{i \in [N]} \sum_{v \in [K]-1} p_i \eta_\ell^r (\nabla F_i(x_{i,v}^r) - BiClip(u_r,d_r,\nabla F_i(x_{i,v}^r) + \xi_{i,v}^r)) }{\sqrt{\widetilde{\beta}_2^{t-r+1}\widetilde{v}_{r-1}} + \tau} \right\rangle\\
    C_{2,r} &= \left\langle \nabla F(x_{r-1}), \frac{\sum_{i \in [N]} \sum_{v \in [K]-1} p_i \eta_\ell^r (\nabla F_i(x_{i,0}^r) - \nabla F_i(x_{i,v}^r)) }{\sqrt{\widetilde{\beta}_2^{t-r+1}\widetilde{v}_{r-1}} + \tau} \right\rangle.
\end{aligned}
\end{equation*}
Noting that $\mathbb{E} [\ \cdot \ ] = \mathbb{E} [\mathbb{E}_r [\ \cdot \ ]]$ by the tower law, we have as before
\begin{equation*}
\begin{aligned}
\mathbb{E} [C_{0,r}] &\le \frac{G d}{\tau} \sum_{i \in [N]} \sum_{v \in [K]-1} p_i \eta_\ell^r d_r \mathbb{P}(\|\nabla F_i(x_{i,v}^r; \xi_{i,v}^r))\|\le d_r) + \frac{GLK^2d}{2\tau} (\eta_\ell^r)^2 u_r\\
&+ \frac{2^{\alpha-1} G d}{\tau} \sum_{i \in [N]} \sum_{v \in [K]-1} p_i \eta_\ell^r (M^\alpha + B^\alpha) u_r^{1-\alpha}\mathbb{P}\left( \|\nabla F_i(x_{i,v}^r;\xi_{i,v}^r))\|\ge u_r\right)\\
&\le \frac{G d}{\tau} K\eta_\ell^r d_r + \frac{GLK^2d}{2\tau} (\eta_\ell^r)^2 u_r+ \frac{2^{\alpha-1} G d}{\tau} K \eta_\ell^r (M^\alpha + B^\alpha) u_r^{1-\alpha}.
\end{aligned}
\end{equation*}
We retain the $\alpha$ for clarity and to draw comparision to previous proofs, however we note that $\alpha = 1$ as higher moments do not exist. Now, to bound $B_1$, we use $L$-smoothness:
\begin{equation*}
\begin{aligned}
\|B_1\| &\le \frac{L\eta_\ell^r u_r K}{\tau} \|x_{t-1}-x_{r-1} \| \le \frac{L\eta_\ell^r u_r K\operatorname{diam}(\mathcal{X})}{\tau} .
\end{aligned}
\end{equation*}
Collecting all inequalities gathered thus far gives 
\begin{equation*}
\begin{aligned}
&\mathbb{E}[F(x_t)] \le \mathbb{E}[F(x_{t-1})] + \frac{\eta_{t}^2 L}{2} \mathbb{E}[\left\|A_1\right\|^2] +  \widetilde{\beta}_1^t \eta_t \mathbb{E}\left[\left\langle \nabla F(x_{t-1}),\frac{ \widetilde{m}_0 }{\sqrt{\widetilde{v}_t} + \tau} \right\rangle\right]\\
&+(1-\widetilde{\beta}_1)\eta_t\sum_{r=1}^t \widetilde{\beta}_1^{t-r} \left(\sum_{q=0}^{t-r} \mathbb{E}[B_{1,q}] - K\eta_\ell^r\mathbb{E}\left[\left\| \frac{ \nabla F (x_{r-1})}{\sqrt{\sqrt{\widetilde{\beta}_2^{t-r+1}\widetilde{v}_{r-1}} + \tau}} \right\|^2\right] + \frac{L\eta_\ell^r u_r K\operatorname{diam}(\mathcal{X})}{\tau}  \right)\\
&+(1-\widetilde{\beta}_1)\eta_t\sum_{r=1}^t \widetilde{\beta}_1^{t-r} \left(\frac{G d}{\tau} K\eta_\ell^r d_r + \frac{GLK^2d}{2\tau} (\eta_\ell^r)^2 u_r+ \frac{2^{\alpha-1}G d}{\tau} K \eta_\ell^r (M^\alpha + B^\alpha) u_r^{1-\alpha} \right).
\end{aligned}
\end{equation*}
We note the use of Jensen and convexity to ensure $\|\mathbb{E}[B_1]\| \le \mathbb{E}[\|B_1\|]$. We now rearrange and telescope $t \in [1,T]$:
\begin{equation*}
\begin{aligned}
&\underbrace{(1-\widetilde{\beta}_1)\sum_{t=1}^T \eta_t \sum_{r=1}^t \widetilde{\beta}_1^{t-r} \left(K\eta_\ell^r\mathbb{E}\left[\left\| \frac{ \nabla F (x_{r-1})}{\sqrt{\sqrt{\widetilde{\beta}_2^{t-r+1}\widetilde{v}_{r-1}} + \tau}} \right\|^2\right]\right)}_{F_1} \le \underbrace{\mathbb{E}[F(x_{0})] - \mathbb{E}[F(x_T)]}_{F_2} + \underbrace{\sum_{t=1}^T \frac{\eta_{t}^2 L}{2} \mathbb{E}[\left\|A_1\right\|^2]}_{F_3} \\
&+  \underbrace{\sum_{t=1}^T\eta_t\widetilde{\beta}_1^t\mathbb{E}\left[\left\langle \nabla F(x_{t-1}),\frac{ \widetilde{m}_0 }{\sqrt{\widetilde{v}_t} + \tau} \right\rangle\right]}_{F_4} +\underbrace{(1-\widetilde{\beta}_1)\sum_{t=1}^T\eta_t\sum_{r=1}^t \widetilde{\beta}_1^{t-r}}_{F_5} \left(\underbrace{\sum_{q=0}^{t-r} \mathbb{E}[B_{1,q}]}_{F_6} + \underbrace{\frac{L\eta_\ell^r u_r K\operatorname{diam}(\mathcal{X})}{\tau}}_{F_7}  \right)\\
&+\underbrace{(1-\widetilde{\beta}_1)\sum_{t=1}^T\eta_t\sum_{r=1}^t \widetilde{\beta}_1^{t-r}}_{F_5} \left(\underbrace{\frac{G d}{\tau} K\eta_\ell^r d_r}_{F_8} + \underbrace{\frac{GLK^2d}{2\tau} (\eta_\ell^r)^2 u_r}_{F_9}+ \underbrace{\frac{2^{\alpha-1} G d}{\tau} K \eta_\ell^r (M^\alpha + B^\alpha) u_r^{1-\alpha}}_{F_{10}} \right).
\end{aligned}
\end{equation*}
We now aim to bound each term in the left hand side from below, and right hand side from above. Letting $\eta_t = \Theta(t^\omega)$, $\eta_\ell^t = \Theta(t^\nu)$, $d_t = \Theta(t^\gamma)$, and $u_t = \Theta(t^\zeta)$, we move to the asymptotic regime to simplify notation and suppress auxiliary constants for readability. We have that
\begin{equation}\label{rightmosttermthing}
(1-\widetilde{\beta}_1)\sum_{t=1}^T \sum_{r=1}^t \eta_t \widetilde{\beta}_1^{t-r} \eta_\ell^r = (1-\widetilde{\beta}_1)\sum_{t=1}^T \eta_t \widetilde{\beta}_1^{t} \left(\sum_{r=1}^t \widetilde{\beta}_1^{-r} \eta_\ell^r \right) \gtrsim (1-\widetilde{\beta}_1)\sum_{t=1}^T \eta_t \widetilde{\beta}_1^{t} \int_{1}^t \widetilde{\beta}_1^{-r} r^\nu \ \mathrm{d}r.
\end{equation}
Then, L'H\^opital's rule allows us to derive an asymptotically sharp bound as follows:
\begin{equation}\label{asymptoticallysharp}
\int_{1}^t \widetilde{\beta}_1^{-r} r^\nu \ \mathrm{d}r =  \left[\frac{\widetilde{\beta}_1^{-r} r^\nu}{-\log_{e}(\widetilde{\beta}_1)} \right]_{r=1}^{t} -  \int_{1}^t \frac{\nu \widetilde{\beta}_1^{-r} r^{\nu-1}}{-\log_{e}(\widetilde{\beta}_1)} \ \mathrm{d}r \gtrsim  \frac{\widetilde{\beta}_1^{-t} t^\nu}{|\log_{e}(\widetilde{\beta}_1)|}
\end{equation}
Here, we used that $\nu \le 0$ and $0<\widetilde{\beta}_1 < e$. Asymptotic equivalence is verified via
\begin{equation*}
\lim_{t \to \infty} \frac{|\log_{e}(\widetilde{\beta}_1)|(\int_{1}^t \widetilde{\beta}_1^{-r} r^\nu \ \mathrm{d}r)}{\widetilde{\beta}_1^{-t} t^\nu} =  \lim_{t \to \infty} \frac{|\log_{e}(\widetilde{\beta}_1)| \widetilde{\beta}_1^{-t} t^\nu}{-\log_e(\widetilde{\beta}_1)\widetilde{\beta}_1^{-t} t^\nu + \nu \widetilde{\beta}_1^{-t} t^{\nu-1}} = 1.
\end{equation*}
Therefore, the rightmost side of~\eqref{asymptoticallysharp} is an asymptotically sharp approximation, relieving the condition $\nu \le 0$ for validity of the approximation. Within $\widetilde{\beta}_1 \in (0,1)$, the approximation diverges as expected, validating the asymptotic analysis. Recall that $|\Delta_r| \le K \eta_\ell^r u_r$, which now gives via \eqref{asymptoticallysharp}
\begin{equation}\label{LHSfirstpart}
\widetilde{\beta}_2^{t-r+1} \widetilde{v}_{r-1} \lesssim  \sum_{z=1}^{r-1} \widetilde{\beta}_2^{r-1-z} \Delta_z^2 \lesssim \widetilde{\beta}_2^{r-1} \sum_{z=1}^{r-1} \widetilde{\beta}_2^{-z} (\eta_\ell^z)^2 u_z^2 \lesssim \max \left\{\mathcal{O}(1), T^{2(\nu+\zeta)}\right\}.
\end{equation}
Here, we used $\widetilde{\beta}_2 \le 1$ and $r \le T$. We thus obtain
\begin{equation*}
(1-\widetilde{\beta}_1)\sum_{t=1}^T \sum_{r=1}^t \eta_t \widetilde{\beta}_1^{t-r} \eta_\ell^r \gtrsim (1-\widetilde{\beta}_1)\sum_{t=1}^T \eta_t \frac{t^{\nu}}{|\log_e(\widetilde{\beta}_1)|} \gtrsim (1-\widetilde{\beta}_1) \int_1^T \frac{t^{\omega + \nu}}{\log_e(\widetilde{\beta}_1)} \mathrm{d}t \approx  \frac{(1-\widetilde{\beta}_1)T^{\omega + \nu + 1}}{(\omega + \nu + 1)|\log_e(\widetilde{\beta}_1)|}.
\end{equation*}
Therefore as $\nu + \zeta < 0$, we conclude that
\begin{equation*}
F_1 \gtrsim \Omega \left(\frac{(1-\widetilde{\beta}_1)}{(\omega + \nu + 1)\log_e(\widetilde{\beta}_1)} \cdot T^{\omega + \nu + 1} \cdot \min_{t \in [T]} \mathbb{E}[\|\nabla F(x_{t})\|^2] \right).
\end{equation*}
Clearly, $F_2 \lesssim \mathcal{O}(1)$. To bound $F_3$, we have 
\begin{equation*}
\begin{aligned}
F_3 &= \sum_{t=1}^T \frac{\eta_t^2L}{2}\left\|\frac{\widetilde{\beta}_1^t \widetilde{m}_0 + (1-\widetilde{\beta}_1)\sum_{r=1}^t \widetilde{\beta}_1^{t-r} \Delta_r}{\sqrt{\widetilde{v}_t} + \tau}\right\|^2 \lesssim \sum_{t=1}^T \frac{t^{2\omega}}{\tau^2}\left(\widetilde{\beta}_1^{2t} \|\widetilde{m}_0\|^2 + (1-\widetilde{\beta}_1)^2\left\|\sum_{r=1}^t \widetilde{\beta}_1^{t-r} \Delta_r\right\|^2 \right)\\
&\lesssim \frac{\mathcal{O}(1)}{\tau^2} + \frac{(1-\widetilde{\beta}_1)^2 \sum_{t=1}^T t^{2\nu + 2\zeta + 2\omega} }{\tau^2 (\log_e(\widetilde{\beta}_1))^2} \lesssim \frac{\mathcal{O}(1)}{\tau^2} + \frac{(1-\widetilde{\beta}_1)^2 \ T^{2(\nu + \zeta + \omega) + 1} }{\tau^2(\log_e(\widetilde{\beta}_1))^2}.
\end{aligned}
\end{equation*}
$F_4$ is bounded similarly after using Jensen, 
\begin{equation*}
|F_4| \le \sum_{t=1}^T \eta_t \widetilde{\beta}_1^t\mathbb{E}\left[\left\langle |\nabla F(x_{t-1})|,\frac{ |\widetilde{m}_0| }{\sqrt{\widetilde{v}_t} + \tau} \right\rangle\right] \le \sum_{t=1}^T \eta_t \widetilde{\beta}_1^t dG \cdot \max_{j \in [d]} \frac{ |\widetilde{m}_0|_j }{\sqrt{[\widetilde{v}_t]_j} + \tau} \lesssim \mathcal{O}(1).
\end{equation*}
Bounding $F_5$ and $F_6$ is more complex. We begin by noting that
\begin{equation*}
\begin{aligned}
|\mathbb{E}[B_{1,q}]| &\le \sum_{j=1}^d \frac{G(1-\widetilde{\beta}_2) \widetilde{\beta}_2^{\frac{q}{2}}}{\tau^2} \cdot \mathbb{E}\left[\frac{[\Delta_{t-q}^2|\Delta_r|]_j}{\sqrt{[\widetilde{v}_{t-q}]_{j}}}\right]\\
&\le \sum_{j=1}^d \frac{G(1-\widetilde{\beta}_2) \widetilde{\beta}_2^{\frac{q}{2}}}{\tau^2 } \cdot \mathbb{E}\left[\frac{[\Delta_{t-q}^2|\Delta_r|]_j}{\sqrt{\max \{[\widetilde{\beta}_2^{t-q} \widetilde{v}_0 + (1-\widetilde{\beta}_2)\sum_{o=1}^{t-q} \widetilde{\beta}_2^{t-q-o} \Delta_o^2]_{j} ,\tau^2\}}}\right] \\
& \lesssim \sum_{j=1}^d \frac{(1-\widetilde{\beta}_2) \widetilde{\beta}_2^{\frac{q}{2}}}{\tau^3} \cdot \mathbb{E}\left[[\Delta_{t-q}^2|\Delta_r|]_j\right].
\end{aligned}
\end{equation*}
Therefore,
\begin{equation*}
\begin{aligned}
F_5 F_6 &\lesssim (1-\widetilde{\beta}_1)\sum_{t=1}^T\eta_t \sum_{r=1}^t \widetilde{\beta}_1^{t-r}  (1-\widetilde{\beta}_2) \sum_{q=0}^{t-r}\widetilde{\beta}_2^{\frac{q}{2}} \cdot \mathbb{E}\left[\Delta_{t-q}^2|\Delta_r|\right]   \\
& \le (1-\widetilde{\beta}_1)\sum_{t=1}^T \eta_t\sum_{r=1}^t \widetilde{\beta}_1^{t-r}  (1-\widetilde{\beta}_2) \eta_\ell^{r} u_{r} \sum_{q=0}^{t-r} \widetilde{\beta}_2^{\frac{q}{2}} (\eta_\ell^{t-q} u_{t-q})^2.
\end{aligned}
\end{equation*}
Under the substitution $q \leftarrow t- \widetilde{q}$, we have that
\begin{equation*}
\begin{aligned}
F_5 F_6 & \lesssim (1-\widetilde{\beta}_1)\sum_{t=1}^T\eta_t\sum_{r=1}^t \widetilde{\beta}_1^{t-r}  (1-\widetilde{\beta}_2) \eta_\ell^{r} u_{r} \widetilde{\beta}_2^{\frac{t}{2}} \sum_{\widetilde{q}=r}^{t} \widetilde{\beta}_2^{\frac{-\widetilde{q}}{2}} (\eta_\ell^{\widetilde{q}} u_{\widetilde{q}})^2 \\
&\lesssim (1-\widetilde{\beta}_1)\sum_{t=1}^T\eta_t\sum_{r=1}^t \widetilde{\beta}_1^{t-r}  (1-\widetilde{\beta}_2) \eta_\ell^{r} u_{r} \cdot 2^{\nu+\zeta} \frac{t^{2(\nu+\zeta)}}{|\log_e(\widetilde{\beta}_2)|}\\
&\lesssim (1-\widetilde{\beta}_1)\sum_{t=1}^T \frac{t^{\omega+2(\nu+\zeta)}}{|\log_e(\widetilde{\beta}_2)|}\widetilde{\beta}_1^{t}  (1-\widetilde{\beta}_2) \sum_{r=1}^t \widetilde{\beta}_1^{-r} r^{\nu+\zeta} %t^{2(\nu+\zeta)} 
\\
&\lesssim (1-\widetilde{\beta}_1)\sum_{t=1}^T (1-\widetilde{\beta}_2) \frac{t^{\omega+3(\nu+\zeta)}}{|\log_e(\widetilde{\beta}_1)||\log_e(\widetilde{\beta}_2)|} \approx \frac{(1-\widetilde{\beta}_1)(1-\widetilde{\beta}_2)}{|\log_e(\widetilde{\beta}_1)||\log_e(\widetilde{\beta}_2)|} \cdot \max\left\{\mathcal{O}(1),T^{\omega +3(\nu+\zeta) +1}\right\}.
\end{aligned}
\end{equation*}
As $\mathcal{O}(1)$ terms are subsumed by $F_4$, $F_5F_7$ is bounded via
\begin{equation*}
\begin{aligned}
(1-\widetilde{\beta}_1)\sum_{t=1}^T \eta_t \sum_{r=1}^t \widetilde{\beta}_1^{t-r} \frac{L\eta_\ell^r u_r K\operatorname{diam}(\mathcal{X})}{\tau} &\lesssim (1-\widetilde{\beta}_1)\sum_{t=1}^T \eta_t \sum_{r=1}^t \widetilde{\beta}_1^{t}\frac{\eta_\ell^r u_r \widetilde{\beta}_1^{-r}}{\tau} \\
&\lesssim (1-\widetilde{\beta}_1)\sum_{t=1}^T  \frac{t^{\nu + \zeta + \omega}}{\tau|\log_e(\widetilde{\beta}_1)|} \lesssim \frac{(1-\widetilde{\beta}_1) T^{\omega  + \nu + \zeta+ 1}}{\tau|\log_e(\widetilde{\beta}_1)|}.
\end{aligned}
\end{equation*}
The remaining terms may also be bounded as follows:  
\begin{equation*}
\begin{aligned}
F_5F_8&\lesssim \frac{(1-\widetilde{\beta}_1)}{\tau}\sum_{t=1}^T\eta_t\sum_{r=1}^t \widetilde{\beta}_1^{t-r} \eta_\ell^r d_r \lesssim \frac{(1-\widetilde{\beta}_1)}{\tau}\sum_{t=1}^T\eta_t\sum_{r=1}^t \widetilde{\beta}_1^{t}\widetilde{\beta}_1^{-r} r^{\nu + \gamma} \\
&\lesssim \frac{(1-\widetilde{\beta}_1)}{|\log_e(\widetilde{\beta}_1)|} \sum_{t=1}^T t^{\omega} t^{\nu + \gamma} \lesssim \frac{(1-\widetilde{\beta}_1)}{|\log_e(\widetilde{\beta}_1)|}\max\{T^{\omega + \nu + \gamma + 1},\mathcal{O}(1)\}
\end{aligned}
\end{equation*}
where $F_9$ and $F_{10}$ can be bounded via
\begin{equation*}
\begin{aligned}
F_5F_9 & \lesssim (1-\widetilde{\beta}_1)\sum_{t=1}^T\sum_{r=1}^t  \frac{\eta_t \widetilde{\beta}_1^{t-r}(\eta_\ell^r)^2 u_r }{\tau} \lesssim \frac{(1-\widetilde{\beta}_1)}{|\log_e(\widetilde{\beta}_1)|} \sum_{t=1}^T\sum_{r=1}^t  \frac{\eta_t \widetilde{\beta}_1^{t-r}r^{2\nu + \zeta} }{\tau} \lesssim \frac{T^{2\nu + \zeta + 1 + \omega}}{\tau},
\end{aligned}
\end{equation*}
\begin{equation*}
\begin{aligned}
F_5F_{10} &\lesssim (1-\widetilde{\beta}_1)\sum_{t=1}^T \sum_{r=1}^t \eta_t \widetilde{\beta}_1^{t-r}  \frac{\eta_\ell^r u_r^{1-\alpha}}{\tau} \lesssim (1-\widetilde{\beta}_1)\sum_{t=1}^T \sum_{r=1}^t t^{\omega} \widetilde{\beta}_1^{t}  \frac{\widetilde{\beta}_1^{-r} r^{\nu + \zeta(1-\alpha)} }{\tau} \\
&\lesssim \sum_{t=1}^T t^\omega \frac{(1-\widetilde{\beta}_1)}{|\log_e(\widetilde{\beta}_1)|}t^{\nu + \zeta(1-\alpha)} \lesssim \frac{(1-\widetilde{\beta}_1)}{|\log_e 
 (\widetilde{\beta}_1)|} T^{\omega + \nu + \zeta(1-\alpha) +1} .
\end{aligned}
\end{equation*}
Standard calculations imply that under the conditions~\eqref{AdamConditions}, the dominating terms are $F_7$, $F_{10}$ with order $\mathcal{O}(1)$. Within the derived upper bound, $\zeta > 0$ destabilizes $F_7$ and decays $F_{10}$ to $0$, while $\zeta<0$ gives the analogous properties with $F_7$ and $F_{10}$ swapped. 
\end{proof}

\section{Experiment Setup \& Full Results}\label{ExperimentSetupAppendix}


In this section, we present the experimental setups and results across two primary domains: synthetic data and natural language processing tasks. More precisely, we evaluate the performance of TailOPT instantiations with state-of-the-art benchmarks on convex models (with synthetic data), transformer encoders, as well as generative models. For convex, synthetic experiments, we construct datasets to emulate heavy-tailed stochastic gradients, focusing on linear regression models trained under contaminated label noise. The design includes generating feature matrices and labels while injecting noise from heavy-tailed distributions to study convergence behaviors. Additionally, we introduce the \textbf{SynToken} dataset, which models the heavy-tailed distribution of token frequencies observed in natural language processing. For brevity, we only include the results of the SynToken dataset, denoted `Synthetic data', in the main text (Figure~\ref{InjectedNoise}). This allows us to evaluate learning algorithms in controlled settings, easing out and exploring the effects of both common and rare features.

For assessing the optimization of transformer encoders on natural language processing tasks, we evaluate RoBERTa~\cite{RoBERTa} on the General Language Understanding Evaluation (GLUE) benchmark~\cite{GLUE}, which encompasses a diverse range of tasks such as sentiment analysis, paraphrase detection, and natural language inference. By fine-tuning RoBERTa on GLUE, we assess its generalization capabilities and robustness. The benchmark's inclusion of multiple datasets ensures a comprehensive evaluation of model performance across various linguistic phenomena. Additionally, we also evaluate the capabilities of the T5~\cite{t5} generative model on WMT machine translation tasks~\cite{wmt}. These experiments provide insights into the behavior of optimization algorithms and pretrained models under realistic and challenging conditions. For RoBERTa, we optimize over GLUE across 10 simulated compute nodes, whereas for T5, we model 3 compute node fine-tuning on WMT benchmark datasets. 

\textbf{Compute Resources.}
We conducted our experiments on a compute cluster equipped with dozens of GPUs, with dynamic availability fluctuating based on overall cluster usage by other users. The cluster featured a set of GPU models, including H100, L40S, and A40 machines. %The comprehensive evaluation presented in this work required substantial computational resources and was completed over the course of at least four months.

\subsection{Convex Models (Synthetic Experiments)}\label{AllSyntheticExperimentAppendix}
\subsubsection{Data Generation Process}
To simulate heavy-tailed stochastic gradients in a simple yet controlled linear regression setting, we generated a synthetic dataset as follows. The feature matrix \( X \in \mathbb{R}^{M \times m} \) was constructed with entries drawn independently from a standard normal distribution, \( X_{ij} \sim \mathcal{N}(0, 1) \). The true weight vector \( w_{\text{true}} \in \mathbb{R}^m \) was sampled from \( \mathcal{N}(0, I_m) \), where \( I_m \) is the \( m \times m \) identity matrix.

The true labels were computed using:
\begin{equation*}
y_{\text{true}} = X w_{\text{true}}.
\end{equation*}
To induce heavy-tailed stochastic gradients, we injected noise into the label vector by adding a noise term \( \xi \), resulting in contaminated labels:
\begin{equation*}
\hat{y} = y_{\text{true}} + \xi,
\end{equation*}
where \( \xi \in \mathbb{R}^M \) is a noise vector with entries drawn independently from a heavy-tailed distribution \( \mathcal{D} \). For simplicity, we assume coordinate-wise independence of the noise components.

After generating the dataset, we distributed the data across \( n = 10 \) data centers in an IID fashion. Notably, the heavy-tailed noise was injected once prior to distribution, and no additional data were generated afterward. This approach ensured that the same contaminated training data are used locally throughout the training process. % We note that pedantically speaking, using a fixed, finite dataset with the noise injection calibrated only once will induce large variance, but not strictly infinite-variance. %, which closely models real-world distributed environments. %While this setup induces large variance due to the injection of heavy-tailed noise, pedantically speaking, it also implies the stochastic gradients are not strictly heavy-tailed over time, given the finite number of samples and the absence of additional noise injection in subsequent training rounds. We note that pedantically speaking, using a fixed, finite dataset with the noise injection calibrated once will induce large variance, but not strictly infinite-variance,  

\subsubsection{Linear Regression Model}

We consider a single-layer neural network without biases, parameterized by \( w \in \mathbb{R}^m \), which is equivalent to linear regression. Training is performed using the contaminated labels \( (X, \hat{y}) \) with the mean-squared error (MSE) loss function:
\begin{equation*}
\mathcal{L}(w) = \frac{1}{2} \| \hat{y} - X w \|^2.
\end{equation*}
The gradient of the loss with respect to \( w \) is given by:
\begin{equation*}
\nabla_{w} \mathcal{L}(w) = -X^\top (\hat{y} - X w).
\end{equation*}
Substituting \( \hat{y} = y_{\text{true}} + \xi = X w_{\text{true}} + \xi \), we have:
\begin{equation*}
\nabla_{w} \mathcal{L}(w) = -X^\top (X w_{\text{true}} + \xi - X w) = -X^\top X (w_{\text{true}} - w) - X^\top \xi.
\end{equation*}
Simplifying, we obtain:
\begin{equation*}
\nabla_{w} \mathcal{L}(w) = X^\top X (w - w_{\text{true}}) - X^\top \xi.
\end{equation*}
The term \( -X^\top \xi \) reflects the influence of the heavy-tailed noise on the gradient. Given that \( X \) has Gaussian entries and \( \xi \) follows a heavy-tailed distribution, the stochastic gradients \( \nabla_{w} \mathcal{L}(w) \) are also heavy-tailed.

\subsubsection{The SynToken Dataset}

To model the heavy-tailed nature of token frequencies observed in natural language processing, we created the synthetic \textbf{SynToken} dataset. In natural language, word or token usage often follows a heavy-tailed distribution. That is, a small number of tokens appear very frequently, while a large number of tokens appear infrequently but carry significant contextual information.

In our dataset, we partitioned the feature space into common and rare features to reflect this phenomenon. Specifically, we designated the first \( p = 10\% \) of the columns of \( X \) as common features and the remaining \( 90\% \) as rare features. The common features were generated by sampling from a Bernoulli distribution with a high probability of success:
\begin{equation*}
X_{\text{common}} \sim \text{Bernoulli}(0.9),
\end{equation*}
resulting in features that are frequently active. The rare features were sampled from a Bernoulli distribution with a low probability of success:
\begin{equation*}
X_{\text{rare}} \sim \text{Bernoulli}(0.1),
\end{equation*}
introducing sparsity and emulating infrequently occurring tokens.

The complete feature matrix \( X \) was formed by concatenating \( X_{\text{common}} \) and \( X_{\text{rare}} \):
\begin{equation*}
X = \left[ X_{\text{common}},\ X_{\text{rare}} \right].
\end{equation*}
The weight vector \( w \) was sampled from a standard multivariate normal distribution, \( w \sim \mathcal{N}(0, I_m) \), consistent with the previous setup. Noise injection was analogously applied to the labels as before. This approach was taken to mimic the key characteristics of tokenization and word embeddings in natural language processing, via a minimal yet effective model. One benefit of synthetic datasets is that by simulating the distribution of common and rare tokens, the \textbf{SynToken} dataset allows us to study the effects of heavy-tailed data distributions on learning algorithms in a controlled setting. Additionally, we note that the problem being studied is $\mu$-strongly convex with probability $1$, as the setting is linear regression under Gaussian features.

\begin{figure}[h]
\centering
%\hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Synthetic_Pure_Non_Tokenized_f.pdf}
        \subcaption{Pure non-tokenized task}
    \end{subfigure}
        \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Synthetic_t_distribution_Non_Tokenized_f.pdf}
        \subcaption{Noised non-tokenized task}
    \end{subfigure}
        \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Synthetic_Pure_Tokenized_f.pdf}
        \subcaption{Pure tokenized task}
    \end{subfigure}
        \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Synthetic_t_distribution_Tokenized_f.pdf}
        \subcaption{Noised tokenized task}
    \end{subfigure}
    \hfill
    \caption{(Top) The results on the non-tokenized synthetic dataset are presented. In the absence of noise injection, Avg-Adam, Avg-SGD, and RMSProp-$BiClip$ demonstrate the most competitive performance. However, under heavy-tailed noise injection, RMSProp-$BiClip$ and Adam-$BiClip$ achieve the highest performance, while Avg-SGD exhibits among the poorest outcomes. Notably, oscillations observed in Adam-$BiClip$ may reflect the impact of amplified update step sizes in the outer optimizer, potentially enabling finer-grained exploration of the optimization landscape. (Bottom) Tokenization drastically alters algorithmic performance. Without noise, Avg-SGD decays the fastest, while Avg-Adam converges to a superior optimum. However, when synthetic, unbiased heavy-tailed noise is introduced, Avg-SGD becomes highly unstable, whereas Adam-$BiClip$ and RMSProp-$BiClip$ consistently deliver the best results.%\tian{change the method names in the caption. e.g., Avg+Adam should be Avg-Adam}
    }
    %\vspace{-0.2in}
\label{SynToken}
\end{figure}

\subsection{Synthetic Experiments Discussion}

\paragraph{Does the heavy-tailed distribution of covariates matter?} 
% \tian{not clear what is heavy-tailed tokenization}
Figure~\ref{SynToken} (a) and (c) illustrate that a heavy-tailed distribution of token frequencies has significant impacts on the performance of optimization strategies. In (a), RMSProp-$BiClip$ performs competitively under standard tokenization. However, in (c), heavy-tailed tokenization applied to the feature matrix destabilizes RMSProp-$BiClip$. Interestingly, under tokenized conditions without noise, RMSProp exhibits oscillatory behavior, whereas Adam maintains relative stability. This is consistent with the interpretation of Adam as incorporating an exponentially decaying moving average of the gradient's first moment, which augments optimization stability. Upon noise injection, best performing hyperparameters for RMSProp-$BiClip$ does not show oscillatory behavior, but is larger in terms of distance $\|w_* - \hat{w}\|$ than the case without noise.

\paragraph{Does noise matter?} When noise is injected into the labels, the performance dynamics shift considerably. outer optimizer adaptive or non-adaptive methods \textit{combined} with inner optimizer SGD perform poorly, which may indicate that inner optimizers should take a focal role in addressing the challenges posed by heavy-tailed noise. While the choice of the outer optimizer may appear to a limited impact on the binary question of learnability for this specific synthetic data (i.e., ``Can the algorithm decrease distance to the true $w_*$ or not?''), under tokenized conditions with heavy-tailed noise (Figure~\ref{SynToken}(d)), outer optimizer Adam demonstrates the best performance. Figure~\ref{SynToken} reveals that heavy-tailed noise generally destabilizes all algorithms, including adaptive methods, clipped approaches, and pure SGD (c.f., minimum values in (a) and (c) to (b) and (d)). Notably, coordinate-wise $BiClip$ consistently outperforms $L_2$ clipping, aligning with the results in Table~\ref{tab:glue-results}.

\paragraph{How far should these results generalize?} A word of caution is warranted against overgeneralization. These results are derived from a simplified regression model, limiting the ability to generalize the observed trends. Nevertheless, the experiments underscore the pronounced effects of heavy-tailed noise in a controlled synthetic environment and highlight the noise-mitigating capabilities of optimizers such as Adam, RMSProp, and $BiClip$. Additionally, it is important to note that real-world transformer models often comprise tens of millions to billions of parameters. %Thus in controlled, synthetic scenarios, algorithms specifically designed to address heavy-tailed noise may perform even more effectively than in real-world settings. %, as suggested by the controlled setting experiments.

\subsection{Transformer Encoders (RoBERTa \& GLUE Benchmarks)}\label{GLUEAppendix}
%[Edit more later] 

% 'mrpc'(30), 'qnli'(10), 'rte'(30), 'sst2'(5), 'wnli', 'stsb'(10), 'mnli'(7), 'qqp'(5)/3enuf, 'cola'(15)

The General Language Understanding Evaluation (GLUE) benchmark~\cite{GLUE} serves as a comprehensive framework for evaluating natural language understanding (NLU) models across a diverse range of tasks. By incorporating datasets that span various linguistic challenges, GLUE provides a rigorous testbed for assessing the generalization capabilities of NLP models. Below, we summarize the datasets and tasks included in GLUE:

CoLA (Corpus of Linguistic Acceptability): A binary classification task that evaluates a model's ability to determine whether a given sentence is grammatically acceptable. Sentences are drawn from linguistic theory literature, with performance measured by the Matthews Correlation Coefficient (MCC). We fine-tune for 15 epochs (15 outer optimizer steps, where each inner optimizer performs 1 epoch on their allocated data).

SST-2 (Stanford Sentiment Treebank): This binary sentiment analysis task involves classifying movie reviews as expressing positive or negative sentiment. Accuracy is the primary evaluation metric. We fine-tune for 5 epochs.

MRPC (Microsoft Research Paraphrase Corpus): A paraphrase detection task where the goal is to identify whether two sentences, often drawn from news sources, have equivalent meanings. Performance is evaluated using both accuracy and F1 score. We fine-tune for 30 epochs.

STS-B (Semantic Textual Similarity Benchmark): A regression task that assesses the semantic similarity between two sentences on a continuous scale from 0 (unrelated) to 5 (identical in meaning). The dataset combines multiple sources, with evaluation based on Pearson and Spearman correlations. We fine-tune for 10 epochs.

QQP (Quora Question Pairs): Another paraphrase detection task, QQP focuses on identifying whether pairs of questions from the Quora platform are semantically equivalent. Metrics include accuracy and F1 score. We fine-tune for 5 epochs.

MNLI (Multi-Genre Natural Language Inference): A three-class classification task (entailment, neutral, contradiction) that evaluates a model's ability to perform natural language inference across multiple genres, including fiction, government reports, and spoken dialogue. We fine-tune for 7 epochs.

QNLI (Question Natural Language Inference): Adapted from the Stanford Question Answering Dataset (SQuAD), this binary classification task assesses whether a given sentence provides a valid answer to a question. We fine-tune for 10 epochs.

RTE (Recognizing Textual Entailment): Similar to MNLI but on a smaller scale, this binary classification task involves determining whether a hypothesis logically follows from a given premise. Data sources include news articles and Wikipedia. We fine-tune for 30 epochs.

WNLI (Winograd Natural Language Inference): A specialized task focusing on pronoun resolution in sentences. The dataset is based on the Winograd Schema Challenge, where resolving pronouns requires understanding contextual nuances. %Due to uniformly poor performance across all algorithms, WNLI was not included in the evaluation. 
We note that it is standard to exclude the evaluation of WNLI when reporting GLUE results, due to the intrinsically adversarial nature of the dataset (i.e., validation data are constructed as subtle perturbations applied to the training data with opposite labels)~\cite{t5}.

%In addition to these datasets, GLUE includes a diagnostic dataset designed to analyze model performance on specific linguistic phenomena, such as negation, coreference, and quantifiers. By covering a wide array of linguistic tasks, GLUE has driven significant advancements in NLU, serving as the benchmark for developing and evaluating influential models such as BERT, RoBERTa, and GPT.

\paragraph{RobERTa.} 
RoBERTa is a state-of-the-art transformer-based model designed to enhance the performance of the original BERT architecture through improved pretraining strategies. Proposed by~\cite{RoBERTa}, RoBERTa optimizes BERT by refining its training setup, enabling more robust natural language understanding (NLU) across diverse tasks. Key innovations introduced by RoBERTa include the removal of the next sentence prediction (NSP) objective, an increase in batch sizes and training data, and the use of longer training schedules. Additionally, RoBERTa employs dynamic masking during training, which prevents models from overfitting to static token masks.

Trained on significantly larger datasets (e.g., the BooksCorpus, CC-News, and OpenWebText), RoBERTa achieves superior performance on several benchmarks, including GLUE, SuperGLUE, and SQuAD. Its flexibility and robustness make it particularly effective for fine-tuning on a wide range of downstream tasks, from sentiment analysis to question answering. By refining BERT's pretraining process, RoBERTa underscores the importance of hyperparameter tuning and data utilization in achieving state-of-the-art results.

\subsection{Generative Models (T5 \& WMT Dataset Benchmarks)}
We additionally evaluate our method using T5~\cite{t5}, a state-of-the-art text-to-text transformer model developed by Google Research. T5 unifies natural language processing tasks under a text-to-text framework, where both inputs and outputs are text strings, making it highly versatile across tasks such as summarization, translation, and classification. The model was pretrained on the Colossal Clean Crawled Corpus (C4) using a span corruption objective and is available in multiple sizes, ranging from T5-Small (60M parameters) to T5-XXL (11B parameters). This unified framework and scalability allow T5 to excel in a wide range of tasks, making it a strong baseline for evaluating our proposed method.

To evaluate machine translation tasks, we utilize the WMT datasets, a widely recognized benchmark for translation research~\cite{wmt}. Specifically, we fine-tune T5 on the TED Talks and News Commentary datasets. The TED Talks dataset, originally sourced from IWSLT 2017~\cite{IWSLT2017}, provides multilingual translations of TED Talk transcripts, offering diverse linguistic and domain-specific challenges. In contrast, the News Commentary dataset contains parallel text derived from news articles in various languages, presenting a more formal and structured domain. These datasets represent distinct styles and linguistic features, providing a rigorous evaluation of algorithm agility in optimizing across various domains or tasks.


\subsection{Hyperparameter Sweep Grid}\label{hyperparametergridappendix}
The sweep grids in Tables~\ref{tab:hparam_sweeps_clipping},~\ref{tab:hparam_sweeps_lr_eps} were determined by first performing a coarser sweep using an approximate grid, then localizing near the discovered well-performing hyperparameters.
\begin{table*}[ht]
\centering
\caption{Hyperparameter Sweeps: Gradient Clipping Parameters. 
\texttt{i\_u}, \texttt{i\_d} = inner optimizer $u$, $d$, 
\texttt{o\_u}, \texttt{o\_d} = outer optimizer $u$, $d$.}
\label{tab:hparam_sweeps_clipping}
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccc}
\toprule
\textbf{Algorithm} & \textbf{\texttt{i\_u}} & \textbf{\texttt{i\_d}} & \textbf{\texttt{o\_u}} & \textbf{\texttt{o\_d}} \\
\midrule
\textbf{Avg-SGD} 
    & - 
    & - 
    & - 
    & - 
    \\
\midrule
\textbf{Avg-$L_2$Clip SGD}
    & \(\text{np.linspace}(10^{-4},\,1.5,\,12)\) 
    & 0.0
    & - 
    & - 
    \\
\midrule
\textbf{Avg-$BiClip$}
    & \(\text{np.linspace}(10^{-4},\,1.5,\,4)\) 
    & \(\text{np.linspace}(10^{-7},\,\texttt{i\_u},\,4)\) 
    & - 
    & - 
    \\
\midrule
\textbf{Avg-$BiClip$ ($L_2$)}
    & \(\text{np.linspace}(10^{-4},\,1.5,\,4)\) 
    & \(\text{np.linspace}(10^{-7},\,\texttt{i\_u},\,4)\) 
    & - 
    & - 
    \\
\midrule
\textbf{Avg-Adagrad} 
    & - 
    & - 
    & - 
    & - 
    \\
\midrule
\textbf{Avg-Adam} 
    & - 
    & - 
    & - 
    & - 
    \\
\midrule
\textbf{Adagrad-SGD}
    & - 
    & - 
    & - 
    & - 
    \\
\midrule
\textbf{RMSProp-SGD}
    & - 
    & - 
    & - 
    & - 
    \\
\midrule
\textbf{Adam-SGD}
    & - 
    & - 
    & - 
    & - 
    \\
\midrule
\textbf{Adagrad-$BiClip$}
    & \(\text{np.linspace}(10^{-4},\,1.5,\,3)\) 
    & \(\text{np.linspace}(10^{-7},\,\texttt{i\_u},\,3)\) 
    & - 
    & - 
    \\
\midrule
\textbf{RMSProp-$BiClip$}
    & \(\text{np.linspace}(10^{-4},\,1.5,\,3)\) 
    & \(\text{np.linspace}(10^{-7},\,\texttt{i\_u},\,3)\) 
    & - 
    & - 
    \\
\midrule
\textbf{Adam-$L_2$Clip} 
    & \(\text{np.linspace}(10^{-4},\,1.5,\,12)\) 
    & 0.0 
    & - 
    & -
    \\
\midrule
\textbf{Adam-$BiClip$}
    & \(\text{np.logspace}(-2,\,1,\,5)\) 
    & \(\text{np.linspace}(10^{-7},\,\texttt{i\_u},\,3)\) 
    & - 
    & - 
    \\
\midrule
\textbf{Adam-$BiClip$ ($L_2$)}
    & \(\text{np.linspace}(10^{-4},\,1.5,\,3)\) 
    & \(\text{np.linspace}(10^{-7},\,\texttt{i\_u},\,3)\) 
    & - 
    & - 
    \\
\midrule
\textbf{$Adam^2$}
    & - 
    & - 
    & - 
    & - 
    \\
\midrule
\textbf{$Bi^2Clip$ (Coordinate-wise)}
    & \(\text{np.linspace}(10^{-4},\,1.5,\,3)\) 
    & \(\text{np.linspace}(10^{-7},\,\texttt{i\_u},\,3)\)  
    & \(\text{np.linspace}(10^{-4},\,1.5,\,3)\) 
    & \(\text{np.linspace}(10^{-7},\,\texttt{o\_u},\,3)\) 
    \\
\midrule
\textbf{$Bi^2Clip$ ($L_2$)}
    & \(\text{np.logspace}(-1,\,0.5,\,3)\)  
    & \(\text{np.linspace}(10^{-7},\,\texttt{i\_u},\,3)\)
    & \(\text{np.logspace}(-1,\,0.5,\,3)\) 
    & \(\text{np.linspace}(10^{-7},\,\texttt{o\_u},\,3)\) 
    \\
\midrule
\textbf{DiLoCo}
    & - 
    & - 
    & - 
    & -
    \\
\bottomrule
\end{tabular}
}
\end{table*}

\begin{table*}[ht]
\centering
\caption{Hyperparameter Sweeps: Learning Rates and Adaptivity Parameters. 
\texttt{ilr} = inner optimizer learning rate, 
\texttt{olr} = outer optimizer learning rate, 
\texttt{ieps} = inner optimizer $\varepsilon$, 
\texttt{oeps} = outer optimizer $\varepsilon$. Additionally, DiLoCo swept over the nesterov learning rates $(0.9, 0.95)$, and inner optimizer weight decay parameters $(10^{-1}, 10^{-4})$, reported in prior works such as~\cite{DiLoCo,FedMOM}.}
\label{tab:hparam_sweeps_lr_eps}
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccc}
\toprule
\textbf{Algorithm} & \textbf{\texttt{ilr}} & \textbf{\texttt{olr}} & \textbf{\texttt{ieps}} & \textbf{\texttt{oeps}} \\
\midrule
\textbf{Avg-SGD} 
    & \(\text{np.logspace}(-9,\,1,\,100)\) 
    & - 
    & - 
    & - 
    \\
\midrule
\textbf{Avg-$L_2$Clip SGD}
    & \(\text{np.linspace}(10^{-9},\,1,\,10)\) 
    & - 
    & - 
    & - 
    \\
\midrule
\textbf{Avg-$BiClip$}
    & \(\text{np.linspace}(10^{-9},\,1,\,10)\) 
    & -
    & - 
    & - 
    \\
\midrule
\textbf{Avg-$BiClip$ ($L_2$)}
    & \(\text{np.linspace}(10^{-9},\,1,\,10)\) 
    & -
    & - 
    & - 
    \\
\midrule
\textbf{Avg-Adagrad} 
    & \(\text{np.linspace}(10^{-9}, 1, 30)\) 
    & - 
    & \(\{10^{-8},10^{-6},10^{-4},10^{-3}\}\) %\((1e\text{-}8, 1e\text{-}6, 1e\text{-}4, 1e\text{-}3)\) 
    & - 
    \\
\midrule
\textbf{Avg-Adam} 
    & \(\text{np.linspace}(10^{-9}, 1, 30)\) 
    & - 
    & \(\{10^{-8},10^{-6},10^{-4},10^{-3}\}\) %\((1e\text{-}8, 1e\text{-}6, 1e\text{-}4, 1e\text{-}3)\)
    & - 
    \\
\midrule
\textbf{Adagrad-SGD}
    & \(\text{np.linspace}(10^{-5},\,0.1,\,7)\) 
    & \(\text{np.logspace}(-5,\,-1,\,7)\) 
    & - 
    & \(\{10^{-7},\,10^{-5},\,10^{-3}\}\) 
    \\
\midrule
\textbf{RMSProp-SGD}
    & \(\text{np.linspace}(10^{-5},\,0.1,\,7)\) 
    & \(\text{np.linspace}(10^{-5},\,0.1,\,7)\) 
    & - 
    & \(\{10^{-7},\,10^{-5},\,10^{-3}\}\)  
    \\
\midrule
\textbf{Adam-SGD}
    & \(\text{np.linspace}(10^{-5},\,0.1,\,7)\) 
    & \(\text{np.logspace}(-5,\,-1,\,7)\) 
    & - 
    & \(\{10^{-7},\,10^{-5},\,10^{-3}\}\) 
    \\
\midrule
\textbf{Adagrad-$BiClip$}
    & \(\text{np.linspace}(10^{-5},\,0.1,\,4)\) 
    & \(\text{np.logspace}(-5,\,-1,\,4)\) 
    & - 
    & \(\{10^{-7},\,10^{-5},\,10^{-3}\}\) 
    \\
\midrule
\textbf{RMSProp-$BiClip$}
    & \(\text{np.linspace}(10^{-5},\,0.1,\,4)\) 
    & \(\text{np.logspace}(-5,\,-1,\,4)\) 
    & - 
    & \(\{10^{-7},\,10^{-5},\,10^{-3}\}\) 
    \\
\midrule
\textbf{Adam-$L_2$Clip}
    & \(\text{np.linspace}(10^{-5},\,0.1,\,4)\) 
    & \(\text{np.linspace}(10^{-5},\,0.1,\,4)\) 
    & - 
    & \(\{10^{-7},\,10^{-5},\,10^{-3}\}\) 
    \\
\midrule
\textbf{Adam-$BiClip$}
    & \(\text{np.logspace}(-6,\,-1,\,5)\) 
    & \(\text{np.logspace}(-6,\,-1,\,5)\) 
    & - 
    & \(\{10^{-7},\,10^{-5},\,10^{-3}\}\) 
    \\
\midrule
\textbf{Adam-$BiClip$ ($L_2$)}
    & \(\text{np.linspace}(10^{-5},\,0.1,\,4)\) 
    & \(\text{np.linspace}(10^{-5},\,0.1,\,4)\) 
    & - 
    & \(\{10^{-7},\,10^{-5},\,10^{-3}\}\) 
    \\
\midrule
\textbf{$Adam^2$}
    & \(\text{np.logspace}(-6,\,-1,\,5)\) 
    & \(\text{np.logspace}(-6,\,-1,\,5)\) 
    & \(\{10^{-7},\,10^{-5},\,10^{-3}\}\) 
    & \(\{10^{-7},\,10^{-5},\,10^{-3}\}\) 
    \\
\midrule
\textbf{$Bi^2Clip$ (Coordinate-wise)}
    & \(\text{np.linspace}(10^{-9},\,1,\,3)\) 
    & \(\text{np.linspace}(10^{-9},\,1,\,3)\)
    & - 
    & - 
    \\
\midrule
\textbf{$Bi^2Clip$ ($L_2$)}
    & \(\text{np.logspace}(-1,\,0.5,\,3)\)
    & \(\text{np.logspace}(-1,\,0.5,\,3)\) 
    & - 
    & - 
    \\
\midrule
\textbf{DiLoCo}
    & \(\text{np.logspace}(-5,\,-1,\,5)\)
    & \(\{1, 0.7, 0.5, 10^{-1}, 10^{-2} \}\) 
    & - 
    & \(\{10^{-7},\,10^{-5},\,10^{-3}\}\) 
    \\
\bottomrule
\end{tabular}
}
\end{table*}


\subsection{Optimal Hyperparameters}
In this subsection, we display the optimal hyperparameters located during our extensive sweep. For readability, we report the results as Tables~\ref{tab:hyperparam-grid}-\ref{tab:hyperparam-grid_WMT}. 

\begin{table*}[ht]
\centering
\caption{Best hyperparameter selection over a sweep of various parameter grids. 
`ilr' = inner optimizer learning rate, `olr' = outer optimizer learning rate, 
`ieps' = inner optimizer $\varepsilon$, `oeps' = outer optimizer $\varepsilon$, 
`o\_u', `o\_d' = outer optimizer $u$, $d$, 
`i\_u', `i\_d' = inner optimizer $u$, $d$. Here, $\varepsilon$ is the adaptivity or $\varepsilon$-smoothing parameter employed in the denominator of adaptive optimizers to enhance stability of learning dynamics. }
\label{tab:hyperparam-grid1}
%\resizebox{\textwidth}{!}{
\begin{tabular}{llcccccccc}
\toprule
Algorithm & Dataset & ilr & olr & ieps & oeps & o\_u & o\_d & i\_u & i\_d \\
\midrule
\textbf{Avg-SGD} & STS-B & 0.019 & - & - & - & - & - & - & - \\
 & RTE & 0.095 & - & - & - & - & - & - & - \\
 & QNLI & 0.0059 & - & - & - & - & - & - & - \\
 & QQP & 0.0074 & - & - & - & - & - & - & - \\
 & CoLA & 0.019 & - & - & - & - & - & - & - \\
 & SST-2 & 0.0074 & - & - & - & - & - & - & - \\
 & MRPC & 0.038 & - & - & - & - & - & - & - \\
 & MNLI & 0.0059 & - & - & - & - & - & - & - \\
\midrule
\textbf{Avg-$L_2$Clip} & STS-B & 0.56 & - & - & - & - & - & 1.5 & 0.0 \\
 & RTE & 1 & - & - & - & - & - & 0.14 & 0.0 \\
 & QNLI & 0.33 & - & - & - & - & - & 0.14 & 0.0 \\
 & QQP & 0.44 & - & - & - & - & - & 0.14 & 0.0 \\
 & CoLA & 0.33 & - & - & - & - & - & 0.14 & 0.0 \\
 & SST-2 & 0.11 & - & - & - & - & - & 0.27 & 0.0 \\
 & MRPC & 0.22 & - & - & - & - & - & 0.41 & 0.0 \\
 & MNLI & 0.11 & - & - & - & - & - & 0.41 & 0.0\\
\midrule
\textbf{Avg-$BiClip$} & STS-B & 0.44 & - & - & - & - & - & 0.0001 & 0.0001 \\
 & RTE & 1 & - & - & - & - & - & 0.0001 & 6.7e-5 \\
 & QNLI & 0.44 & - & - & - & - & - & 0.0001 & 6.7e-5 \\
 & QQP & 0.56 & - & - & - & - & - & 0.0001 & 3.3e-5 \\
 & CoLA & 0.89 & - & - & - & - & - & 0.0001 & 0.0001 \\
 & SST-2 & 0.56 & - & - & - & - & - & 0.0001 & 6.7e-5 \\
 & MRPC & 0.89 & - & - & - & - & - & 0.0001 & 6.7e-5 \\
 & MNLI & 0.56 & - & - & - & - & - & 0.0001 & 3.3e-5 \\
\midrule
\textbf{Avg-$BiClip$} ($L_2$) & STS-B & 0.067 & - & - & - & - & - & 0.75 & 0.75 \\
 & RTE & 1 & - & - & - & - & - & 0.0001 & 6.7e-5 \\
 & QNLI & 0.067 & - & - & - & - & - & 0.75 & 0.75 \\
 & QQP & 0.11 & - & - & - & - & - & 0.5 & 0.33 \\
 & CoLA & 0.067 & - & - & - & - & - & 0.75 & 0.75 \\
 & SST-2 & 0.1 & - & - & - & - & - & 0.75 & 0.38 \\
 & MRPC & 0.11 & - & - & - & - & - & 1 & 1 \\
 & MNLI & 0.033 & - & - & - & - & - & 1.5 & 1.5 \\
\midrule
$Bi^2Clip$ & STS-B & 0.5 & 0.5 & - & - & 0.0001 & 0.0001 & 0.0001 & 1e-7 \\
 & RTE & 1 & 1 & - & - & 0.0001 & 0.0001 & 0.001 & 5e-5 \\
 & QNLI & 0.5 & 1 & - & - & 0.0001 & 0.0001 & 0.0001 & 5e-5 \\
 & QQP & 0.5 & 1 & - & - & 1.5 & 1e-7 & 0.0001 & 5e-5 \\
 & CoLA & 0.5 & 1 & - & - & 0.0001 & 0.0001 & 0.0001 & 0.0001 \\
 & SST-2 & 0.5 & 1 & - & - & 0.75 & 1e-7 & 0.0001 & 1e-7 \\
 & MRPC & 1 & 1 & - & - & 0.0001 & 0.0001 & 0.0001 & 1e-7 \\
 & MNLI & 0.5 & 1 & - & - & 0.75 & 1e-7 & 0.0001 & 1e-7 \\
\midrule
$Bi^2Clip$ ($L_2$) & STS-B & 0.56 & 3.2 & - & - & 0.1 & 0.05 & 0.1 & 0.05 \\
 & RTE & 0.1 & 0.56 & - & - & 0.1 & 0.1 & 0.56 & 0.56 \\
 & QNLI & 0.1 & 0.1 & - & - & 3.2 & 3.2 & 0.56 & 1e-7 \\
 & QQP & 0.1 & 3.2 & - & - & 0.56 & 1e-7 & 0.56 & 0.56 \\
 & CoLA & 0.1 & 3.2 & - & - & 0.1 & 0.05 & 0.56 & 1e-7 \\
 & SST-2 & 0.56 & 0.1 & - & - & 3.2 & 3.2 & 0.1 & 1e-7 \\
 & MRPC & 0.56 & 0.1 & - & - & 0.56 & 0.56 & 0.1 & 0.1 \\
 & MNLI & 0.1 & 0.56 & - & - & 3.2 & 1.6 & 0.56 & 1e-7 \\
\bottomrule
\end{tabular}
%}
\end{table*}


\begin{table*}[ht]
\centering
\caption{Best hyperparameter selection over a sweep of various parameter grids. 
`ilr' = inner optimizer learning rate, `olr' = outer optimizer learning rate, 
`ieps' = inner optimizer $\varepsilon$, `oeps' = outer optimizer $\varepsilon$, 
`o\_u', `o\_d' = outer optimizer $u$, $d$, 
`i\_u', `i\_d' = inner optimizer $u$, $d$. Here, $\varepsilon$ is the adaptivity or $\varepsilon$-smoothing parameter employed in the denominator of adaptive optimizers to enhance stability of learning dynamics. }
\label{tab:hyperparam-grid}
%\resizebox{\textwidth}{!}{
\begin{tabular}{llcccccccc}
\toprule
Algorithm & Dataset & ilr & olr & ieps & oeps & o\_u & o\_d & i\_u & i\_d \\
\midrule
\textbf{Adam-SGD} & STS-B & 0.017 & 4.6e-5 & - & 1e-7 & - & - & - & - \\
 & RTE & 0.033 & 4.6e-5 & - & 1e-7 & - & - & - & - \\
 & QNLI & 0.017 & 2.2e-4 & - & 1e-7 & - & - & - & - \\
 & QQP & 0.017 & 2.2e-4 & - & 1e-7 & - & - & - & - \\
 & CoLA & 0.033 & 0.001 & - & 1e-5 & - & - & - & - \\
 & SST-2 & 0.017 & 4.6e-5 & - & 1e-7 & - & - & - & - \\
 & MRPC & 0.017 & 4.6e-5 & - & 1e-7 & - & - & - & - \\
 & MNLI & 0.017 & 2.2e-4 & - & 1e-7 & - & - & - & - \\
\midrule
\textbf{Adam-$L_2Clip$} & STS-B & 0.067 & 0.033 & - & 0.001 & - & - & 0.75 & 0.0 \\
 & RTE & 0.033 & 1e-5 & - & 1e-7 & - & - & 1.5 & 0.0 \\
 & QNLI & 0.067 & 0.067 & - & 0.001 & - & - & 0.75 & 0.0 \\
 & QQP & 0.067 & 0.033 & - & 0.001 & - & - & 1.5 & 0.0 \\
 & CoLA & 0.1 & 0.033 & - & 0.001 & - & - & 0.75 & 0.0 \\
 & SST-2 & 0.1 & 0.033 & - & 0.001 & - & - & 1.5 & 0.0 \\
 & MRPC & 0.033 & 0.033 & - & 0.001 & - & - & 0.75 & 0.0 \\
 & MNLI & 0.067 & 0.033 & - & 0.001 & - & - & 0.75 & 0.0 \\
\midrule
\textbf{Adam-$BiClip$} & STS-B & 0.0056 & 3.2e-4 & - & 1e-5 & - & - & 0.01 & 0.0067 \\
 & RTE & 3.2e-4 & 1.8e-5 & - & 1e-7 & - & - & 0.01 & 0.0067 \\
 & QNLI & 0.0056 & 3.2e-4 & - & 1e-7 & - & - & 0.01 & 0.0067 \\
 & QQP & 0.0056 & 0.00032 & - & 1e-7 & - & - & 0.01 & 0.0033 \\
 & CoLA & 0.0056 & 1.8e-5 & - & 1e-7 & - & - & 0.01 & 0.01 \\
 & SST-2 & 0.0056 & 1.8e-5 & - & 1e-7 & - & - & 0.01 & 0.0067 \\
 & MRPC & 0.0056 & 0.0056 & - & 0.001 & - & - & 0.056 & 0.019 \\
 & MNLI & 0.0056 & 3.2e-4 & - & 1e-5 & - & - & 0.01 & 0.0033 \\
\midrule
\textbf{Adam-$BiClip$} ($L_2$) & STS-B & 0.033 & 0.033 & - & 0.001 & - & - & 1.5 & 0.75 \\
 & RTE & 0.033 & 0.067 & - & 0.001 & - & - & 0.75 & 0.38 \\
 & QNLI & 0.033 & 0.067 & - & 0.001 & - & - & 1.5 & 0.75 \\
 & QQP & 0.067 & 0.033 & - & 0.0001 & - & - & 0.75 & 0.38 \\
 & CoLA & 0.033 & 0.033 & - & 0.001 & - & - & 1.5 & 0.75 \\
 & SST-2 & 0.067 & 0.033 & - & 0.001 & - & - & 1.5 & 1e-7 \\
 & MRPC & 0.033 & 0.033 & - & 0.001 & - & - & 1.5 & 1e-7 \\
 & MNLI & 0.067 & 0.033 & - & 0.001 & - & - & 1.5 & 0.75 \\
\midrule
$Adam^2$ & STS-B & 1.8e-5 & 1.8e-5 & 1e-5 & 1e-7 & - & - & - & - \\
 & RTE & 1.8e-5 & 1.8e-5 & 1e-5 & 1e-7 & - & - & - & - \\
 & QNLI & 1.8e-5 & 3.2e-4 & 1e-5 & 1e-5 & - & - & - & - \\
 & QQP & 1.8e-5 & 3.2e-4 & 1e-5 & 1e-7 & - & - & - & - \\
 & CoLA & 1.8e-5 & 0.0056 & 1e-5 & 0.001 & - & - & - & - \\ 
 & SST-2 & 1.8e-5 & 1.8e-5 & 0.001 & 1e-7 & - & - & - & - \\
 & MRPC & 1.8e-5 & 1.8e-5 & 1e-5 & 1e-7 & - & - & - & - \\
 & MNLI & 1.8e-5 & 3.2e-4 & 1e-5 & 1e-7 & - & - & - & - \\
\bottomrule
\end{tabular}
%}
\end{table*}
\clearpage
\begin{table*}[ht]
\centering
\caption{The notational setup is analogous to Table~\ref{tab:hyperparam-grid}. For DiLoCo$^*$, we provide the Nesterov learning rate and weight decay parameter in the i\_u, i\_d entries, respectively.}
\label{tab:hyperparam-grid2}
%\resizebox{\textwidth}{!}{
\begin{tabular}{llcccccccc}
\toprule
Algorithm & Dataset & ilr & olr & ieps & oeps & o\_u & o\_d & i\_u & i\_d \\
\midrule
\textbf{Adagrad-SGD} & STS-B & 0.017 & 0.0046 & - & 0.001 & - & - & - & - \\
 & RTE & 0.033 & 0.001 & - & 1e-5 & - & - & - & - \\
 & QNLI & 0.017 & 0.001 & - & 1e-5 & - & - & - & - \\
 & QQP & 0.017 & 0.0001 & - & 1e-5 & - & - & - & - \\
 & CoLA & 0.017 & 2.2e-4 & - & 1e-7 & - & - & - & - \\
 & SST-2 & 0.017 & 2.2e-4 & - & 1e-5 & - & - & - & - \\
 & MRPC & 0.017 & 2.2e-4 & - & 1e-7 & - & - & - & - \\
 & MNLI & 0.017 & 0.0001 & - & 1e-7 & - & - & - & - \\
\midrule
\textbf{RMSProp-SGD} & STS-B & 0.017 & 1e-5 & - & 1e-7 & - & - & - & - \\
 & RTE & 0.017 & 1e-5 & - & 1e-7 & - & - & - & - \\
 & QNLI & 0.033 & 0.001 & - & 1e-5 & - & - & - & - \\
 & QQP & 0.017 & 1e-5 & - & 1e-7 & - & - & - & - \\
 & CoLA & 0.017 & 1e-5 & - & 1e-7 & - & - & - & - \\
 & SST-2 & 0.017 & 1e-5 & - & 1e-7 & - & - & - & - \\
 & MRPC & 0.033 & 1e-5 & - & 1e-7 & - & - & - & - \\
 & MNLI & 0.017 & 1e-5 & - & 1e-7 & - & - & - & - \\
\midrule
\textbf{Adagrad-$BiClip$} & STS-B & 1e-5 & 2.2e-4 & - & 1e-7 & - & - & 1.5 & 1.5 \\
 & RTE & 0.033 & 2.2e-4 & - & 1e-7 & - & - & 1.5 & 1e-7 \\
 & QNLI & 1e-5 & 0.0046 & - & 0.001 & - & - & 1.5 & 1.5 \\
 & QQP & 1e-5 & 0.0046 & - & 0.0001 & - & - & 1.5 & 1.5 \\
 & CoLA & 0.1 & 2.2e-4 & - & 1e-7 & - & - & 0.0001 & 5e-5 \\
 & SST-2 & 1e-5 & 0.0046 & - & 0.001 & - & - & 1.5 & 1.5 \\
 & MRPC & 1e-5 & 2.2e-4 & - & 1e-7 & - & - & 1.5 & 0.75 \\
 & MNLI & 1e-5 & 0.0046 & - & 0.001 & - & - & 1.5 & 1.5 \\
\midrule
\textbf{RMSProp-$BiClip$} & STS-B & 1e-5 & 1e-5 & - & 1e-7 & - & - & 1.5 & 1.5 \\
 & RTE & 0.067 & 1e-5 & - & 1e-7 & - & - & 0.0001 & 5e-5 \\
 & QNLI & 0.1 & 1e-5 & - & 1e-7 & - & - & 0.0001 & 0.0001 \\
 & QQP & 0.1 & 0.0046 & - & 1e-7 & - & - & 0.0001 & 5e-5 \\
 & CoLA & 0.1 & 0.0046 & - & 0.001 & - & - & 0.0001 & 1e-7 \\
 & SST-2 & 0.1 & 1e-5 & - & 1e-7 & - & - & 0.0001 & 0.0001 \\
 & MRPC & 1e-5 & 0.0046 & - & 0.001 & - & - & 0.75 & 0.75 \\
 & MNLI & 0.1 & 0.0046 & - & 0.001 & - & - & 0.0001 & 0.0001 \\
\midrule
\textbf{DiLoCo}$^*$ & STS-B & 1.8e-5 & 0.7 & 1e-5 & - & - & - & 0.9 & 0.1 \\
 & RTE & 1.8e-5 & 1 & 1e-5 & - & - & - & 0.95 & 0.0001 \\
 & QNLI & 1.8e-5 & 1 & 1e-5 & - & - & - & 0.9 & 0.0001 \\
 & QQP & 1.8e-5 & 1 & 1e-5 & - & - & - & 0.95 & 0.0001 \\
 & CoLA & 1.8e-5 & 1 & 1e-5 & - & - & - & 0.95 & 0.1 \\
 & SST-2 & 1.8e-5 & 0.1 & 1e-5 & - & - & - & 0.9 & 0.0001 \\
 & MRPC & 1.8e-5 & 0.7 & 1e-5 & - & - & - & 0.9 & 0.1 \\
 & MNLI & 1.8e-5 & 1 & 1e-5 & - & - & - & 0.9 & 0.1 \\
\bottomrule
\end{tabular}
%}
\end{table*}

\begin{table*}[ht]
\centering
\caption{Best hyperparameter selection over a sweep of various parameter grids for GLUE tasks. The notation is analogous to Table~\ref{tab:hyperparam-grid}. }
\label{tab:hyperparam-grid3}
%\resizebox{\textwidth}{!}{
\begin{tabular}{llcccccccc}
\toprule
Algorithm & Dataset & ilr & olr & ieps & oeps & o\_u & o\_d & i\_u & i\_d \\
\midrule
\textbf{Avg-Adagrad} & STS-B & 3e-5 & - & 1e-8 & - & - & - & - & - \\
 & RTE & 1.5e-4 & - & 1e-6 & - & - & - & - & - \\
 & QNLI & 3.3e-4 & - & 0.001 & - & - & - & - & - \\
 & QQP & 3.3e-4 & - & 0.001 & - & - & - & - & - \\
 & CoLA & 6.7e-5 & - & 1e-6 & - & - & - & - & - \\
 & SST-2 & 3.3e-4 & - & 0.001 & - & - & - & - & - \\
 & MRPC & 1.5e-4 & - & 1e-6 & - & - & - & - & - \\
 & MNLI & 3.3e-4 & - & 0.001 & - & - & - & - & - \\
\midrule
\textbf{Avg-Adam} & STS-B & 1.4e-5 & - & 1e-6 & - & - & - & - & - \\
 & RTE & 3e-5 & - & 1e-8 & - & - & - & - & - \\
 & QNLI & 6.2e-6 & - & 1e-8 & - & - & - & - & - \\
 & QQP & 1.4e-5 & - & 1e-8 & - & - & - & - & - \\
 & CoLA & 6.2e-6 & - & 1e-8 & - & - & - & - & - \\
 & SST-2 & 6.2e-6 & - & 1e-8 & - & - & - & - & - \\
 & MRPC & 3e-5 & - & 1e-8 & - & - & - & - & - \\
 & MNLI & 3e-5 & - & 0.0001 & - & - & - & - & - \\

\bottomrule
\end{tabular}
%}
\end{table*}


\begin{table*}[h!]
\centering
\caption{Best hyperparameter selection over a sweep of various parameter grids for WMT. The conventions are identical with Tables~\ref{tab:hyperparam-grid}-\ref{tab:hyperparam-grid3}.}
\label{tab:hyperparam-grid_WMT}
%\resizebox{\textwidth}{!}{
\begin{tabular}{llcccccccc}
\toprule
Algorithm & Dataset & ilr & olr & ieps & oeps & o\_u & o\_d & i\_u & i\_d \\
\midrule
\textbf{Avg-SGD} & TED-T (en-de) & 0.03 & - & - & - & - & - & - & - \\
 & TED-T (en-fr)  & 0.015 & - & - & - & - & - & - & - \\
 % & NewsComm (en-it) & 0.0059 & - & - & - & - & - & - & - \\
 & NewsComm (en-fr) & 0.015 & - & - & - & - & - & - & - \\
\midrule
\textbf{Avg-$L_2Clip$} & TED-T (en-de) & 0.89 & - & - & - & - & - & 1.4 & 0.0 \\
 & TED-T (en-fr) & 0.89 & - & - & - & - & - & 0.55 & 0.0 \\
 % & NewsComm (en-it) & 0.33 & - & - & - & - & - & 0.14 & 0.0 \\
 & NewsComm (en-fr) & 0.78 & - & - & - & - & - & 0.41 & 0.0 \\
\midrule
$Bi^2Clip$ & TED-T (en-de) & 1 & 1 & - & - & 0.0001 & 0.0001 & 0.75 & 1e-7 \\
 & TED-T (en-fr) & 1 & 1 & - & - & 0.0001 & 0.0001 & 0.75 & 1e-7 \\
 % & NewsComm (en-it) & 0.5 & 1 & - & - & 0.0001 & 0.0001 & 0.0001 & 5e-5 \\
 & NewsComm (en-fr) & 0.5 & 1 & - & - & 1.5 & 1e-7 & 0.0001 & 5e-5 \\
\midrule
$Adam^2$ & TED-T (en-de) & 3.2e-4 & 0.0056 & 1e-7 & 0.001 & - & - & - & - \\
 & TED-T (en-fr) & 1.8e-5 & 1.8e-5 & 1e-5 & 1e-7 & - & - & - & - \\
 % & NewsComm (en-it) & 1.8e-5 & 3.2e-4 & 1e-5 & 1e-5 & - & - & - & - \\
 & NewsComm (en-fr) & 3.2e-4 & 0.0056 & 1e-5 & 0.001 & - & - & - & - \\
\bottomrule
\end{tabular}
%}
\end{table*}

\clearpage



\section{Additional Experiments}\label{Additional_Experiments_Appendix}

$BiClip$ is inspired by the principles of adaptivity, particularly the selection of coordinate-wise learning rates based on historical gradient statistics in adaptive optimizers. It leverages this intuition by efficiently amplifying smaller gradient values while tempering larger gradients. This selective adjustment enables $BiClip$ to maintain computational efficiency while achieving highly competitive performance, as demonstrated in Tables~\ref{tab:glue-results} and~\ref{tab:mt-results-average}, where it rivals more resource-intensive optimizers such as Adam.

However, Figure~\ref{GradientDistributionComparison} highlights how gradient distributions can be distinctly altered by adaptive or clipping operations, which is reflected in their respective optimal learning rates. We note that $L_2$ clipping primarily affects gradients at the extremesthose whose $L_2$-norms exceed a predefined thresholdwhile leaving the broader gradient distribution largely unchanged during the optimization process. This limited modification contrasts with the more nuanced adjustments achieved by $BiClip$ or Adam.

\begin{figure*}[h]
\centering
%\hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{F0.pdf}
        % \subcaption{Pure Stochastic Gradients}
    \end{subfigure}
    %\hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{F1.pdf}
        % \subcaption{Pure Stochastic Gradients}
    \end{subfigure}
    %\hfill
        \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{F2.pdf}
        % \subcaption{Pure Stochastic Gradients}
    \end{subfigure}
    %\hfill
        \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{F3.pdf}
        % \subcaption{Pure Stochastic Gradients}
    \end{subfigure}
    %\hfill
        \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{F4.pdf}
        % \subcaption{Pure Stochastic Gradients}
    \end{subfigure}
    %\hfill
        \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{BP0.pdf}
        % \subcaption{Pure Stochastic Gradients}
    \end{subfigure}
    %\hfill
        \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{BP1.pdf}
        % \subcaption{Pure Stochastic Gradients}
    \end{subfigure}
    %\hfill
        \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{BP2.pdf}
        % \subcaption{Pure Stochastic Gradients}
    \end{subfigure}
    %\hfill
        \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{BP3.pdf}
        % \subcaption{Pure Stochastic Gradients}
    \end{subfigure}
    %\hfill
        \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{BP4.pdf}
        % \subcaption{Pure Stochastic Gradients}
    \end{subfigure}
            \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{BU0.pdf}
        % \subcaption{Pure Stochastic Gradients}
    \end{subfigure}
    %\hfill
        \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{BU1.pdf}
        % \subcaption{Pure Stochastic Gradients}
    \end{subfigure}
    %\hfill
        \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{BU2.pdf}
        % \subcaption{Pure Stochastic Gradients}
    \end{subfigure}
    %\hfill
        \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{BU3.pdf}
        % \subcaption{Pure Stochastic Gradients}
    \end{subfigure}
    %\hfill
        \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{BU4.pdf}
        % \subcaption{Pure Stochastic Gradients}
    \end{subfigure}
            \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{AP0.pdf}
        % \subcaption{Pure Stochastic Gradients}
    \end{subfigure}
    %\hfill
        \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{AP1.pdf}
        % \subcaption{Pure Stochastic Gradients}
    \end{subfigure}
    %\hfill
        \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{AP2.pdf}
        % \subcaption{Pure Stochastic Gradients}
    \end{subfigure}
    %\hfill
        \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{AP3.pdf}
        % \subcaption{Pure Stochastic Gradients}
    \end{subfigure}
    %\hfill
        \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{AP4.pdf}
        % \subcaption{Pure Stochastic Gradients}
    \end{subfigure}
    %\hfill
        \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{AU0.pdf}
        % \subcaption{Pure Stochastic Gradients}
    \end{subfigure}
    %\hfill
        \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{AU1.pdf}
        % \subcaption{Pure Stochastic Gradients}
    \end{subfigure}
    %\hfill
        \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{AU2.pdf}
        % \subcaption{Pure Stochastic Gradients}
    \end{subfigure}
    %\hfill
        \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{AU3.pdf}
        % \subcaption{Pure Stochastic Gradients}
    \end{subfigure}
    %\hfill
        \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{AU4.pdf}
        % \subcaption{Pure Stochastic Gradients}
    \end{subfigure}
    %\hfill
    % \begin{subfigure}[b]{0.28\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{example-image}
    %     \subcaption{$Bi^2Clip$-SGD Gradients}
    % \end{subfigure}
    % %\hfill
    % \begin{subfigure}[b]{0.28\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{example-image}
    %     \subcaption{$Adam^2$ Gradients}
    % \end{subfigure}%\\
    % \hfill
    \caption{Gradient statistics for MNLI in the GLUE Benchmark across different algorithms for the first 5 communication rounds, where rounds increase from left to right. (Top) We visualize local minibatch stochastic gradient (used as model updates in Avg-SGD) distributions, where the outliers can dominate model updates upon outer pseudogradient aggregation. The $BiClip$ and Adam optimizers mitigate this phenomenon in different ways. (Middle) Row 2 displays the local gradients accumulated from all inner optimizers during $Bi^2Clip$ prior to clipping, which uncovers the presence of outliers akin to those visible in Avg-SGD. In Row 3, the identical gradients are plotted after the coordinate-wise $BiClip$ operation is applied. It is observed that $BiClip$ stabilizes updates by thresholding large and small gradient coordinates, constraining model update lengths within a defined range. The distribution of gradient lengths have changed significantly, with outliers autonomously being mollified. (Bottom) Similar to above, row 4 shows the accumulated gradient lengths across all inner optimizers while training via $Adam^2$. In row 5, it is observed that Adam amplifies gradients across a larger scale, with optimal hyperparameters accordingly downscaling model updates by utilizing smaller learning rates at both inner and outer optimizers. Optimal inner optimizer learning rates are $0.0059$, $0.5$, and $1.8e$-$5$ for Avg-SGD, $Bi^2Clip$, and $Adam^2$, respectively, with corresponding outer optimizer learning rates of $1$ and $3.2e$-$4$ for the latter two algorithms. Test-time results show that $Bi^2Clip$ outperforms $Adam^2$, which in turn outperforms Avg-SGD (Table~\ref{tab:glue-results}). Finally, we note that upon centering, the aggregate update gradient histograms in red depict the stochastic gradient noise distributions upon application of the optimizer strategy. $BiClip$ attenuates the pure gradient noise (in blue) by projecting the noise distribution to an almost bell-shaped curve (in red), while Adam implicitly samples gradient noise from a left-leaning, skewed distribution.
    }
    %\vspace{-0.2in}
\label{GradientDistributionComparison}
\vspace{-10pt}
\end{figure*}

\subsection{Expanded Algorithm Performance Evaluation (GLUE)}

\begin{table*}[h!]
\centering
\caption{Evaluation results on GLUE Benchmark datasets during test time. Metrics: 
CoLA (Matthews Correlation Coefficient, MCC), 
SST-2 (Accuracy), 
MRPC (Accuracy/F1), 
STS-B (Spearman/Pearson), 
QQP (Accuracy/F1), 
MNLI (Accuracy), 
QNLI (Accuracy), 
RTE (Accuracy). Entries marked with $0.0$ indicate the actual metric value (averaged across the granularity of each datapoint in the baseline dataset), which implies random guessing or failure to learn. Top \textcolor{DarkerBlue}{\textbf{first}}, \textcolor{blue}{\textbf{second}}, and \textcolor{cyan}{\textbf{third}} best-performing algorithms are highlighted. We note that nested optimization algorithms utilizing adaptivity or coordinate-wise $BiClip$ on both inner and outer optimizers generally achieve greater than $80\%$ averaged performance (out of $100\%$). For $Adam^2$, preconditioners are transmitted between the inner and outer optimizers, whereas DiLoCo requires maintaining preconditioners on the inner optimizers, both of which incur significant communication or memory overhead. 
} 
\label{tab:glue-results-appendix}
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccccc}
\toprule
\textbf{Algorithm} & \textbf{MNLI} & \textbf{QNLI} & \textbf{QQP (Acc/F1)} & \textbf{RTE} & \textbf{SST-2} & \textbf{MRPC (Acc/F1)} & \textbf{CoLA} & \textbf{STS-B (S/P)} & \textbf{Average} \\
\midrule

\textbf{Avg-SGD}~\cite{mcmahan2017communication} 
& 81.13 
& 83.21 
& 78.71/78.69 
& 57.40 
& 90.94 
& 67.30/80.52 
& 0.0 
& 26.76/28.20 
& 61.17 \\

\textbf{Avg-$L_2Clip$}~\cite{fatclip} 
& 81.82 
& 85.68 
& 80.00/79.82 
& 54.51 
& 91.97 
& 68.38/81.22 
& 0.0 
& 41.27/40.96 
& 64.15 \\

\textbf{Avg-$BiClip$ ($L_2$)} 
& 81.95 
& 86.16 
& 84.62/79.89 
& 55.59 
& 92.31 
& 68.38/81.23 
& 0.0 
& 36.93/37.22 
& 64.03 \\

\textbf{Avg-Adagrad} 
& 84.70 
& 88.79 
& 87.09/83.34 
& 64.26 
& 93.34 
& 71.56/82.63 
& 27.72 
& 81.93/81.26 
& 76.97 \\

\textbf{Avg-Adam} 
& 84.97 
& 89.47 
& 87.66/84.09 
& 64.62 
& \textcolor{blue}{\textbf{93.80}} 
& 81.86/87.74 
& 41.41 
& \textcolor{cyan}{\textbf{86.21}}/\textcolor{cyan}{\textbf{86.55}} 
& 80.76 \\

\textbf{Avg-$BiClip$} 
& 85.08 
& 89.45 
& 87.83/84.12 
& 66.06 
& \textcolor{DarkerBlue}{\textbf{94.03}} 
& 71.32/82.45 
& 41.40 
& 84.08/84.48 
& 79.12 \\

\textbf{$Bi^2Clip$ ($L_2$)} 
& 84.31 
& 89.20 
& 86.36/82.60 
& 72.20 
& 93.34 
& 86.52/90.23 
& \textcolor{blue}{\textbf{60.02}} 
& 82.41/83.00
& 82.74 \\

\textbf{Adagrad-SGD}~\cite{AdaptiveFederatedOptimization} 
& 82.40 
& 86.61 
& 82.51/77.68 
& 71.48 
& 92.08 
& 85.53/89.52 
& 47.80 
& 40.37/42.24 
& 72.69 \\

\textbf{RMSProp-SGD}~\cite{AdaptiveFederatedOptimization}
& 84.20 
& 88.46 
& 87.12/83.30 
& \textcolor{cyan}{\textbf{72.56}} 
& 91.85 
& 85.50/89.17 
& 52.39 
& 45.72/41.80 
& 74.73 \\

\textbf{Adam-SGD}~\cite{AdaptiveFederatedOptimization}
& 82.93 
& 86.98 
& 85.99/80.87 
& 66.78 
& 90.71 
& 87.01/90.09 
& 49.93 
& 44.48/41.26 
& 73.37 \\

\textbf{Adam-$L_2Clip$} 
& 82.54 
& 86.69 
& 85.88/80.72 
& 59.92 
& 89.67 
& 85.29/89.90 
& 48.54 
& 69.19/67.16 
& 76.86 \\

\textbf{Adagrad-$BiClip$} 
& \textcolor{cyan}{\textbf{85.54}} 
& \textcolor{DarkerBlue}{\textbf{90.02}} 
& 88.60/\textcolor{cyan}{\textbf{85.05}} 
& \textcolor{blue}{\textbf{73.36}} 
& 93.23 
& 85.78/89.86 
& 48.87 
& 84.03/85.90 
& 82.75 \\

\textbf{RMSProp-$BiClip$} 
& \textcolor{blue}{\textbf{85.56}} 
& \textcolor{cyan}{\textbf{89.82}} 
& 88.50/84.44 
& 70.75 
& \textcolor{cyan}{\textbf{93.69}} 
& 84.80/88.92 
& 50.99 
& \textcolor{DarkerBlue}{\textbf{87.65}}/\textcolor{DarkerBlue}{\textbf{87.79}} 
& \textcolor{cyan}{\textbf{82.99}} \\

\textbf{Adam-$BiClip$} 
& 84.26 
& 89.20 
& \textcolor{cyan}{\textbf{88.64}}/84.74 
& 69.67 
& 92.43 
& 86.52/90.09 
& \textcolor{cyan}{\textbf{56.12}} 
& 82.83/79.71 
& 82.20 \\

\textbf{Adam-$BiClip$ ($L_2$)} 
& 83.18 
& 86.47 
& 85.63/80.27 
& 67.50 
& 89.56 
& 86.02/89.65 
& 53.17 
& 74.73/73.48 
& 79.06 \\

\textbf{$Adam^2$}~\cite{wang2021local} 
& 85.11 
& 88.87 
& \textcolor{DarkerBlue}{\textbf{89.04}}/\textcolor{DarkerBlue}{\textbf{85.51}} 
& 71.48 
& 92.66 
& \textcolor{cyan}{\textbf{87.50}}/\textcolor{cyan}{\textbf{91.03}} 
& 52.70 
& 84.47/83.82 
& 82.93 \\

\textbf{DiLoCo}~\cite{DiLoCo} 
& \textcolor{DarkerBlue}{\textbf{85.68}} 
& \textcolor{blue}{\textbf{89.87}} 
& \textcolor{blue}{\textbf{88.78}}/\textcolor{blue}{\textbf{85.19}} 
& 67.87 
& 91.89 
& \textcolor{blue}{\textbf{87.99}}/\textcolor{blue}{\textbf{91.20}} 
& 54.77 
& 85.93/84.76 
& \textcolor{blue}{\textbf{83.08}} \\

\textbf{$Bi^2Clip$} 
& 85.06 
& 89.73 
& 84.93/83.97 
& \textcolor{DarkerBlue}{\textbf{76.53}} 
& \textcolor{blue}{\textbf{93.80}} 
& \textcolor{DarkerBlue}{\textbf{89.21}}/\textcolor{DarkerBlue}{\textbf{92.44}} 
& \textcolor{DarkerBlue}{\textbf{60.08}} 
& \textcolor{blue}{\textbf{87.07}}/\textcolor{blue}{\textbf{86.89}} 
& \textcolor{DarkerBlue}{\textbf{84.52}} \\

\bottomrule
\end{tabular}
}

% %%%%%%%%%%%%%%%%%%%%%%%% Old Table
% \resizebox{\textwidth}{!}{
% \begin{tabular}{lccccccccc}
% \toprule
% \textbf{Algorithm} & \textbf{MNLI} & \textbf{QNLI} & \textbf{QQP (Acc/F1)} & \textbf{RTE} & \textbf{SST-2} & \textbf{MRPC (Acc/F1)} & \textbf{CoLA} & \textbf{STS-B (S/P)} & \textbf{Average} \\
% \midrule

% \textbf{Avg-SGD}~\cite{mcmahan2017communication} 
% & 81.13 
% & 83.21 
% & 78.71/78.69 
% & 57.40 
% & 90.94 
% & 67.30/80.52 
% & 0.0 
% & 26.76/28.20 
% & 61.17 \\

% \textbf{Avg-$L_2Clip$}~\cite{fatclip} 
% & 81.82 
% & 85.68 
% & 80.00/79.82 
% & 54.51 
% & 91.97 
% & 68.38/81.22 
% & 0.0 
% & 41.27/40.96 
% & 64.15 \\

% \textbf{Avg-$BiClip$ ($L_2$)} 
% & 81.95 
% & 86.16 
% & 84.62/79.89 
% & 55.59 
% & 92.31 
% & 68.38/81.23 
% & 0.0 
% & 20.55/21.70 
% & 61.13 \\

% \textbf{Avg-Adagrad} 
% & 84.70 
% & 88.79 
% & 87.09/\textcolor{cyan}{\textbf{83.34}} 
% & 64.26 
% & \textcolor{cyan}{\textbf{93.34}} 
% & 71.56/82.63 
% & 27.72 
% & 81.93/81.26 
% & 76.97 \\

% \textbf{Avg-Adam} 
% & 84.97 
% & 89.47 
% & 87.66/\textcolor{blue}{\textbf{84.09}} 
% & 64.62 
% & \textcolor{blue}{\textbf{93.80}} 
% & 81.86/87.74 
% & 41.41 
% & \textcolor{blue}{\textbf{86.21}}/\textcolor{DarkerBlue}{\textbf{86.55}} 
% & 80.76 \\


% \textbf{Avg-$BiClip$} 
% & 85.08 
% & 89.45 
% & 87.83/\textcolor{blue}{\textbf{84.12}} 
% & 66.06 
% & \textcolor{DarkerBlue}{\textbf{94.03}} 
% & 71.32/82.45 
% & 41.40 
% & 84.08/\textcolor{cyan}{\textbf{84.48}} 
% & 79.12 \\

% \textbf{$Bi^2Clip$ ($L_2$)} 
% & 84.31 
% & 89.20 
% & 86.36/82.60 
% & 72.20 
% & \textcolor{cyan}{\textbf{93.34}} 
% & 86.52/\textcolor{cyan}{\textbf{90.23}} 
% & \textcolor{DarkerBlue}{\textbf{60.02}} 
% & 82.41/83.00
% & 82.74 \\

% \textbf{Adagrad-SGD}~\cite{AdaptiveFederatedOptimization} 
% & 82.40 
% & 86.61 
% & 82.51/77.68 
% & 71.48 
% & 92.08 
% & 85.53/89.52 
% & 47.80 
% & 40.37/42.24 
% & 72.69 \\

% \textbf{RMSProp-SGD}~\cite{AdaptiveFederatedOptimization}
% & 84.20 
% & 88.46 
% & 87.12/\textcolor{cyan}{\textbf{83.30}} 
% & \textcolor{cyan}{\textbf{72.56}} 
% & 91.85 
% & 85.50/89.17 
% & 52.39 
% & 45.72/41.80 
% & 74.73 \\

% \textbf{Adam-SGD}~\cite{AdaptiveFederatedOptimization}
% & 82.93 
% & 86.98 
% & 85.99/80.87 
% & 66.78 
% & 90.71 
% & 87.01/90.09 
% & 49.93 
% & 44.48/41.26 
% & 73.37 \\

% \textbf{Adam-$L_2Clip$} 
% & 82.54 
% & 86.69 
% & 85.88/80.72 
% & 59.92 
% & 89.67 
% & 85.29/89.90 
% & 48.54 
% & 69.19/67.16 
% & 76.86 \\

% \textbf{Adagrad-$BiClip$} 
% & \textcolor{blue}{\textbf{85.54}} 
% & \textcolor{blue}{\textbf{90.02}} 
% & \textcolor{cyan}{\textbf{88.60}}/\textcolor{DarkerBlue}{\textbf{85.05}} 
% & \textcolor{blue}{\textbf{73.36}} 
% & 93.23 
% & 85.78/89.86 
% & 48.87 
% & 84.03/\textcolor{blue}{\textbf{85.90}} 
% & 82.75 \\

% \textbf{RMSProp-$BiClip$} 
% & 85.56 
% & \textcolor{DarkerBlue}{\textbf{89.82}} 
% & 88.50/\textcolor{cyan}{\textbf{84.44}} 
% & 70.75 
% & 93.69 
% & 84.80/88.92 
% & 50.99 
% & \textcolor{blue}{\textbf{87.65}}/\textcolor{blue}{\textbf{87.79}} 
% & 82.99 \\

% \textbf{Adam-$BiClip$} 
% & 84.26 
% & 89.20 
% & 88.64/\textcolor{cyan}{\textbf{84.74}} 
% & 69.67 
% & 92.43 
% & 86.52/90.09 
% & 56.12 
% & 82.83/79.71 
% & 82.20 \\

% \textbf{Adam-$BiClip$ ($L_2$)} 
% & 83.18 
% & 86.47 
% & 85.63/80.27 
% & 67.50 
% & 89.56 
% & 86.02/89.65 
% & \textcolor{cyan}{\textbf{53.17}} 
% & 74.73/73.48 
% & 79.06 \\

% \textbf{$Adam^2$}~\cite{wang2021local} 
% & \textcolor{cyan}{\textbf{85.11}} 
% & 88.87 
% & \textcolor{DarkerBlue}{\textbf{89.04}}/\textcolor{DarkerBlue}{\textbf{85.51}} 
% & 71.48 
% & 92.66 
% & \textcolor{cyan}{\textbf{87.50}}/\textcolor{blue}{\textbf{91.03}} 
% & 52.70 
% & 84.47/83.82 
% & \textcolor{cyan}{\textbf{82.93}} \\

% \textbf{DiLoCo}~\cite{DiLoCo} 
% & \textcolor{DarkerBlue}{\textbf{85.68}} 
% & \textcolor{cyan}{\textbf{89.87}} 
% & \textcolor{blue}{\textbf{88.78}}/\textcolor{DarkerBlue}{\textbf{85.19}} 
% & 67.87 
% & 91.89 
% & \textcolor{blue}{\textbf{87.99}}/\textcolor{blue}{\textbf{91.20}} 
% & \textcolor{blue}{\textbf{54.77}} 
% & \textcolor{blue}{\textbf{85.93}}/\textcolor{cyan}{\textbf{84.76}} 
% & \textcolor{blue}{\textbf{83.08}} \\


% \textbf{$Bi^2Clip$} 
% & 85.06 
% & 89.73 
% & 84.93/\textcolor{cyan}{\textbf{83.97}} 
% & \textcolor{DarkerBlue}{\textbf{76.53}} 
% & \textcolor{blue}{\textbf{93.80}} 
% & \textcolor{DarkerBlue}{\textbf{89.21}}/\textcolor{DarkerBlue}{\textbf{92.44}} 
% & \textcolor{DarkerBlue}{\textbf{60.08}} 
% & \textcolor{DarkerBlue}{\textbf{87.07}}/\textcolor{cyan}{\textbf{86.89}} 
% & \textcolor{DarkerBlue}{\textbf{84.52}} \\

% \bottomrule
% \end{tabular}
% }
\end{table*}



\subsection{Performance under Non-IID Data }\label{Appendix_nonIID_experiments}
% I will leave "datacenter here" instead of compute node
\subsubsection{Custom Shakespeare Dataset}
Though not the main focus of this work, in this section, we aim to briefly evaluate the performance of TailOPT and baselines under non-datacenter, distributed environments. We utilized the LEAF repository~\cite{LEAF}, originally a benchmark suite for federated learning, which provides datasets, tools, and baselines to evaluate algorithms under real-world conditions. LEAF emphasizes non-IID data distributions, enabling the study of federated systems where data is naturally heterogeneous across smaller compute nodes. Among the datasets in LEAF, we modified the Shakespeare dataset, originally designed for next-character prediction, where each user now represented a character from Shakespeares works. After preprocessing, the dataset contained 1144 inner compute nodes, each corresponding to a characters dialogue, with substantial variations in sample sizes, vocabulary, and syntax across compute nodes. This structure mirrors the imbalanced, domain-specific data distributions often encountered in federated learning.

To better align with common NLP tasks, we further modified the Shakespeare dataset by redefining the prediction task from (LSTM) next-character prediction to (transformer) next-token prediction. More specifically, the text was tokenized into sequences of words rather than characters, making the task more semantically meaningful while retaining the dataset's inherent non-IID nature. %Each datacenters data remained limited to its corresponding characters dialogue, preserving the diversity in linguistic context and data size. 

% \begin{table*}[h!]
% \centering
% \caption{Evaluation results on the Federated Shakespeare Next Word Prediction Task, for 0.1\% participation percentage. Metrics represent perplexity scores.}
% \label{tab:shakespeare-results}
% % \resizebox{\textwidth}{!}{
% \begin{tabular}{lcccccccc}
% \toprule
% \textbf{Algorithm} & \textbf{0.5\%} & \textbf{1\%} & \textbf{1.5\%} & \textbf{2\%} & \textbf{4\%} & \textbf{6\%} & \textbf{8\%} & \textbf{Average} \\
% \midrule

% \textbf{Avg + SGD} 
%  & 00.00  
%  & 00.00 
%  & 00.00 
%  & 00.00 
%  & 00.00 
%  & 00.00 
%  & 00.00 
%  & 00.00 \\

% \textbf{Avg + $L_2$Clip} 
%  & 00.00 
%  & 00.00 
%  & 00.00 
%  & 00.00 
%  & 00.00 
%  & 00.00 
%  & 00.00 
%  & 00.00 \\

% \textbf{$Bi^2$Clip} 
%  & 00.00 
%  & 00.00 
%  & 00.00 
%  & 00.00 
%  & 00.00 
%  & 00.00 
%  & 00.00 
%  & 00.00 \\

% \textbf{$Adam^2$} 
%  & 00.00 
%  & 00.00 
%  & 00.00 
%  & 00.00 
%  & 00.00 
%  & 00.00 
%  & 00.00 
%  & 00.00 \\

% \bottomrule
% \end{tabular}
% % }
% \end{table*}

\begin{table*}[h!]
\centering
\caption{Perplexity scores on the Federated Shakespeare Next Word Prediction Task at a 0.1\% participation rate, for distillGPT-2 architecture fine-tuning after $3$ communication rounds. %\tian{again, change the method name}
}
\label{tab:shakespeare-results}
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccc}
\toprule
\textbf{Algorithm} & \textbf{Avg-SGD} & \textbf{Avg-$L_2Clip$} & \textbf{Avg-$BiClip$} & \textbf{RMSProp-$BiClip$}  & \textbf{$Bi^2Clip$} & \textbf{$Adam^2$} \\
\midrule
Perplexity Score & 1.9813 & 2.0126 & \textbf{1.7827} & 2.0054 & 1.9112 & 1.9445 \\
\bottomrule
\end{tabular}
}
\end{table*}


\subsubsection{Custom Philosopher Dataset}

To mitigate potential data leakage, we constructed a custom dataset, termed the Philosopher Dataset, to evaluate the non-IID setting and facilitate training from scratch. The Philosopher Dataset was synthesized by allocating each literary work to one of eight compute nodes, followed by an 80-20 train-test split. These texts were open sourced from Project Gutenberg\footnote{\url{https://www.gutenberg.org/}}, an extensive online repository offering over 75,000 classic or traditional books while strictly adhering to copyright protections.

\begin{table}[h!]
    \centering
    \caption{Composition of the Philosopher Dataset.}
    \label{tab:philosopher-dataset}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lll}
        \toprule
        \textbf{Title} & \textbf{Author} & \textbf{Translator} \\
        \midrule
        The Critique of Pure Reason & Immanuel Kant & J. Meiklejohn \\
        The Collected Works of William Hazlitt, Volume One & William Hazlitt & - \\
        The Works of Jane Austen & Jane Austen & - \\
        The Republic & Plato & Benjamin Jowett \\
        War and Peace & Leo Tolstoy & - \\
        The Federalist Papers & Alexander Hamilton, John Jay, James Madison & - \\
        The Count of Monte Cristo & Alexandre Dumas & - \\
        The Brothers Karamazov & Fyodor Dostoevsky & Constance Garnett \\
        \bottomrule
    \end{tabular}}
\end{table}

We instantiated a shallower GPT-2 architecture comprising 2 layers, 256 embedding dimensions, and 4 attention heads. This model was trained from scratch on the Philosopher Dataset. The training results are summarized in Table~\ref{tab:gpt2-results}.

\begin{table}[h!]
    \centering
    \caption{Perplexity scores on the Philosopher Next Word Prediction Task at a 100\% participation rate for the compressed GPT-2 architecture after 3 communication rounds. %\tian{change the method name}
    }
    \label{tab:gpt2-results}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lcccccc}
    \toprule
    \textbf{Algorithm} & \textbf{Avg-SGD} & \textbf{Avg-$L_2Clip$} & \textbf{Avg-$BiClip$} & \textbf{RMSProp-$BiClip$}  & \textbf{$Bi^2Clip$} & \textbf{$Adam^2$} \\
    \midrule
    Perplexity Score & 2.6361 & 2.1183 & \textbf{1.6266} & 1.7983 & 2.3488 & 2.5861 \\
    \bottomrule
    \end{tabular}
    }
\end{table}

\paragraph{Discussion.} In the synthesized non-IID setting, we observe that algorithmic instantiations employing joint adaptivity or adaptive approximations--i.e., incorporating adaptivity or its efficient approximations at both the inner and outer optimizers--tend to underperform slightly. This aligns with the theoretical intuition that highly sensitive, rapidly adapting optimizers are more susceptible to unmitigated client drift, effectively overfitting to the biases of local data shards at the inner optimizers. However, Avg-$BiClip$, which integrates a clipping mechanism to regulate noise variance and stabilize optimization dynamics, exhibits notably robust performance. In particular, Avg-$BiClip$ achieves the strongest results in settings with high data heterogeneity across compute nodes, suggesting that $BiClip$ mitigates not only noise variance but also client drift. We further compare these findings to results on the synthetic dataset (Appendix~\ref{AllSyntheticExperimentAppendix}) where noise-injected data were distributed IID across nodes, contrasting with the Shakespeare and Philosopher datasets, which are explicitly designed to be non-IID. 


We note that the perplexities obtained are lower compared to those achieved on larger text datasets, such as WikiText-103 or large-scale Common Crawl subsets (e.g., distillGPT reportedly achieves a perplexity of around 16 on the WikiText-103 benchmark, a long-term dependency language modeling dataset)\footnote{\url{https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation}}. This arises from the smaller size of the Shakespeare and Philosopher datasets in comparison to larger benchmarks. Finally, we provide the optimal hyperparameters for the non-IID experiments in Table~\ref{tab:hyperparam-grid-noniid}.

\begin{table*}[h!]
\centering
\caption{Best hyperparameter selection over a sweep of various parameter grids. The conventions are identical with Tables~\ref{tab:hyperparam-grid}-\ref{tab:hyperparam-grid_WMT}. %\tian{make the format consistent with the table in the main text, i.e., bold whatever method names we should bold in the first column}
}
\label{tab:hyperparam-grid-noniid}
%\resizebox{\textwidth}{!}{
\begin{tabular}{llcccccccc}
\toprule
Algorithm & Dataset & ilr & olr & ieps & oeps & o\_u & o\_d & i\_u & i\_d \\
\midrule
\textbf{Avg-SGD} & Shakespeare & 0.012 & - & - & - & - & - & - & - \\
         & Philosopher & 0.15 & - & - & - & - & - & - & - \\
\midrule
\textbf{Avg-$L_2Clip$} & Shakespeare & 0.56 & - & - & - & - & - & 0.55 & 0 \\
                & Philosopher & 1 & - & - & - & - & - & 0.41 & 0 \\
\midrule
\textbf{Avg-$BiClip$} & Shakespeare & 1 & - & - & - & - & - & 0.0001 & 3.3e-5 \\
           & Philosopher & 1 & - & - & - & - & - & 0.0001 & 3.3e-5\\
\midrule
\textbf{RMSProp-$BiClip$} & Shakespeare & 0.067 & 2.2e-4 & - & 1e-5 & - & - & 0.75 & 1e-7 \\
               & Philosopher & 0.067 & 0.0046 & - & 0.001 & - & - & 0.75 & 1e-7 \\
\midrule
$Bi^2Clip$ & Shakespeare & 1 & 1 & - & - & 1.5 & 1e-7 & 0.0001 & 0.0001 \\
            & Philosopher & 1 & 1 & - & - & 1.5 & 1e-7 & 0.0001 & 5e-5 \\
\midrule
$Adam^2$ & Shakespeare & 1.8e-5 & 0.0056 & 1e-7 & 0.001 & - & - & - & - \\
           & Philosopher & 1.8e-5 & 0.0056 & 1e-5 & 1e-5 & - & - & - & - \\
\bottomrule
\end{tabular}
%}
\end{table*}

\end{document}
