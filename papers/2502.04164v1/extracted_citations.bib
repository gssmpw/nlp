@article{CentralBddVarClip,
  title={High Probability Bounds for Stochastic Subgradient Schemes with Heavy Tailed Noise},
  author={Daniela A. Parletta and Andrea Paudice and Massimiliano Pontil and Saverio Salzo},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={6},
  pages={953-977},
  year={2024}
}

@article{CentralBddVarClip1,
  title={High Probability Guarantees for Nonconvex Stochastic Gradient Descent with Heavy Tails},
  author={Shaojie Li and Yong Liu},
  journal={International Conference on Machine Learning},
  volume={},
  pages={},
  year={2022}
}

@article{CentralClip,
  title={Algorithms of Robust Stochastic Optimization Based on Mirror Descent Method},
  author={Anatoli Juditsky and Alexander Nazin and Arkadi Nemirovsky and Alexandre Tsybakov},
  journal={Automation and Remote Control},
  volume={80},
  pages={1607--1627},
  year={2019}
}

@article{CentralClip1,
  title={High Probability Convergence of Clipped-SGD Under Heavy-tailed Noise},
  author={Ta Duy Nguyen and Thien Hang Nguyen and Alina Ene and Huy Le Nguyen},
  journal={Arxiv},
  volume={},
  pages={},
  year={2023}
}

@article{CentralClip1plus,
  title={Improved Convergence in High Probability of Clipped Gradient Methods with Heavy Tailed Noise},
  author={Ta Duy Nguyen and Thien Hang Nguyen and Alina Ene and Huy Le Nguyen},
  journal={Advances in Neural Information Processing Systems},
  volume={},
  pages={},
  year={2023}
}

@article{CentralClip2,
  title={Breaking the Lower Bound with (Little) Structure: Acceleration in Non-Convex Stochastic Optimization with Heavy-Tailed Noise},
  author={Zijian Liu and Jiawei Zhang and Zhengyuan Zhou},
  journal={Proceedings of Thirty Sixth Conference on Learning Theory},
  volume={195},
  pages={2266--2290},
  year={2023}
}

@article{CentralClip3,
  title={Parameter-free Regret in High Probability with Heavy Tails},
  author={Jiujia Zhang and Ashok Cutkosky},
  journal={Advances in Neural Information Processing Systems},
  volume={},
  pages={},
  year={2022}
}

@article{CentralClip4,
  title={Breaking the Heavy-Tailed Noise Barrier in Stochastic Optimization Problems},
  author={Nikita Puchkin and Eduard Gorbunov and Nikolay Kutuzov and Alexander Gasnikov},
  journal={AISTATS},
  volume={},
  pages={},
  year={2024}
}

@article{CentralClip5,
  title={High-Probability Complexity Bounds for Non-smooth Stochastic Convex Optimization with Heavy-Tailed Noise},
  author={Eduard Gorbunov and Marina Danilova and Innokentiy Shibaev and Pavel Dvurechensky and Alexander Gasnikov},
  journal={Journal of Optimization Theory and Applications},
  volume={},
  pages={},
  year={2024}
}

@article{Debias1,
  title={Clip21: Error Feedback for Gradient Clipping},
  author={Sarit Khirirat and Eduard Gorbunov and Samuel Horv√°th and Rustem Islamov and Fakhri Karray and Peter Richtarik},
  journal={ArXiv},
  volume={},
  pages={},
  year={2023}
}

@article{Debias2,
  title={Differentially private sgd without clipping bias: An error-feedback approach},
  author={Xinwei Zhang and Zhiqi Bu and Zhiwei Steven Wu and and Mingyi Hong},
  journal={International Conference on Learning Representations},
  volume={},
  pages={},
  year={2024}
}

@article{DiLoCo,
  title={DiLoCo: Distributed Low-Communication Training of Language Models},
  author={Arthur Douillard and Qixuan Feng and Andrei A. Rusu and Rachita Chhaparia and Yani Donchev and Adhiguna Kuncoro and Marc'Aurelio Ranzato and Arthur Szlam and Jiajun Shen},
  journal={ICML Workshop on Advancing Neural Network Training},
  volume={},
  pages={},
  year={2024}
}

@article{DiLoCoAsynchronous,
  title={Asynchronous Local-SGD Training for Language Modeling},
  author={Bo Liu and Rachita Chhaparia and Arthur Douillard and Satyen Kale and Andrei A. Rusu and Jiajun Shen and Arthur Szlam and Marc'Aurelio Ranzato},
  journal={ICML Workshop on Advancing Neural Network Training},
  volume={},
  pages={},
  year={2024}
}

@article{EfficientAdaptiveFederatedOptimization,
    author =       "Su Hyeong Lee and Sidharth Sharma and Manzil Zaheer and Tian Li",
    title =        "Efficient Adaptive Federated Optimization",
    journal =      "ICML Workshop on Advancing Neural Network Training",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2024",
    DOI = ""
}

@article{GradClipCentralGeomPerspective,
    author = "Xiangyi Chen and Zhiwei Steven Wu and Mingyi Hong",
    title = "Understanding Gradient Clipping in Private SGD: A Geometric Perspective",
    journal = "Advances in Neural Information Processing Systems",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2020",
}

@article{HeavyTailedNoisePaper,
    author =       "Jingzhao Zhang and Sai Karimireddy and Andreas Veit and Seungyeon Kim and Sashank Reddi and Sanjiv Kumar and Suvrit Sra",
    title =        "Why are Adaptive Methods Good for Attention Models?",
    journal =      "Advances in  Neural Information Processing Systems",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2020",
}

@article{HighProbAdaGConv,
    author =       "Savelii Chezhegov and Yaroslav Klyukin and ndrei Semenov and Aleksandr Beznosikov and Alexander Gasnikov and Skoltech Samuel Horvath and Martin Takac and Eduard Gorbunov",
    title =        "Gradient Clipping Improves AdaGrad when the Noise Is Heavy-Tailed",
    journal =      "ArXiv",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2024",
}

@article{OpenDiLoCo,
  title={OpenDiLoCo: An Open-Source Framework for Globally Distributed Low-Communication Training},
  author={Sami Jaghouar and Jack Min Ong and Johannes Hagemann},
  journal={ArXiv},
  volume={},
  pages={},
  year={2024}
}

@article{RevisitGradClipModern,
  title={Revisiting gradient clipping: Stochastic bias and tight convergence guarantees},
  author={Anastasia Koloskova and Hadrien Hendrikx and Sebastian U Stich},
  journal={International Conference on Learning Representations},
  volume={ArXiv},
  pages={},
  year={2023}
}

@article{angulardependencerestrictive,
  title={Understanding Gradient Clipping In Incremental Gradient Methods},
  author={Jiang Qian and Yuren Wu and Bojin Zhuang and Shaojun Wang and Jing Xiao},
  journal={Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  volume={},
  pages={},
  year={2021}
}

@article{clippedsgd_online_estimate,
    author =       "Aniket Das and Dheeraj Mysore Nagaraj and Soumyabrata Pal and Arun Suggala and Prateek Varshney",
    title =        "Near-Optimal Streaming Heavy-Tailed Statistical Estimation with Clipped SGD",
    journal =      "Advances in Neural Information Processing Systems",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2024",
}

@article{empirical1,
  title={Convolutional sequence to sequence learning},
  author={Jonas Gehring and Michael Auli and David Grangier and Denis Yarats and Yann N Dauphin},
  journal={International Conference on Machine Learning},
  volume={},
  pages={},
  year={2017}
}

@article{empirical2,
  title={Regularizing and Optimizing LSTM Language Models},
  author={Stephen Merity and Nitish Shirish Keskar and Richard Socher},
  journal={International Conference on Learning Representations},
  volume={},
  pages={},
  year={2018}
}

@article{empirical3,
  title={Deep contextualized word representations},
  author={Matthew E Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},
  journal={International Conference on Learning Representations},
  volume={},
  pages={},
  year={2018}
}

@article{empirical5,
  title={Statistical Language Models based on Neural Networks},
  author={Tomas Mikolov},
  journal={Ph.D. thesis, Brno University of Technology},
  volume={},
  pages={},
  year={2012}
}

@article{heavytail1,
    author =       "Thanh Huy Nguyen and Umut Simsekli and Mert Gurbuzbalaban and Gael Richard",
    title =        "First Exit Time Analysis of Stochastic Gradient Descent Under Heavy-Tailed Gradient Noise",
    journal =      "Advances in Neural Information Processing Systems",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2019",
}

@article{heavytail2,
    author =       "Umut Simsekli and Levent Sagun and Mert Gurbuzbalaban",
    title =        "A Tail-Index Analysis of Stochastic Gradient Noise in Deep Neural Networks",
    journal =      "International Conference on Machine Learning",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2019",
}

@article{heavytail3,
    author =       "Umut Simsekli and Lingjiong Zhu and Yee Whye Teh and Mert Gurbuzbalaban",
    title =        "Fractional Underdamped Langevin Dynamics: Retargeting SGD with Momentum under Heavy-Tailed Gradient Noise",
    journal =      "International Conference on Machine Learning",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2020",
}

@article{heavytailedclassimbalance,
    author =       "Frederik Kunstner and Robin Yadav and Alan Milligan and Mark Schmidt and Alberto Bietti",
    title =        "Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models",
    journal =      "ArXiv",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2024",
    DOI = ""
}

@article{highprobClip,
  title={High-probability bounds for Non-Convex Stochastic Optimization with Heavy Tails},
  author={Ashok Cutkosky and Harsh Mehta},
  journal={Advances in Neural Information Processing Systems},
  volume={},
  pages={},
  year={2021}
}

@article{highprobabilityunboundedvariance,
  title={High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance},
  author={Abdurakhmon Sadiev and Marina Danilova and Eduard Gorbunov and Samuel Horvath and Gauthier Gidel and Pavel Dvurechensky and Alexander Gasnikov and Peter Richtarik},
  journal={International Conference on Machine Learning},
  volume={},
  pages={},
  year={2023}
}

@article{inprobsupreme,
  title={Stochastic Optimization with Heavy-Tailed Noise via Accelerated Gradient Clipping},
  author={Eduard Gorbunov and Marina Danilova and Alexander Gasnikov},
  journal={Advances in Neural Information Processing Systems},
  volume={},
  pages={},
  year={2020}
}

@article{linearattentionisallyouneed,
    author =       "Kwangjun Ahn and Xiang Cheng and Minhak Song and Chulhee Yun and Ali Jadbabaie and Suvrit Sra",
    title =        "Linear attention is (maybe) all you need (to understand transformer optimization)",
    journal =      "International Conference on Learning Representations",
    volume =       "",
    number =       "",
    pages =        "",
    year =         "2024",
    DOI = ""
}

@article{symmetricnoise_distributed,
  title={A Communication-Efficient Distributed Gradient Clipping Algorithm for Training Deep Neural Networks},
  author={Mingrui Liu and Zhenxun Zhuang and Yunwei Lei and Chunyang Liao},
  journal={Advances in Neural Information Processing Systems},
  volume={},
  pages={},
  year={2022}
}

@article{theoretical1,
  title={Can gradient clipping mitigate label noise?},
  author={Aditya Krishna Menon and Ankit Singh Rawat and Sashank J Reddi and Sanjiv Kumar},
  journal={International Conference on Learning Representations},
  volume={},
  pages={},
  year={2020}
}

@article{theoretical4,
  title={Improved Analysis of Clipping Algorithms for Non-convex Optimization},
  author={Bohang Zhang and Jikai Jin and Cong Fang and Liwei Wang},
  journal={Advances in Neural Information Processing Systems},
  volume={},
  pages={},
  year={2020}
}

