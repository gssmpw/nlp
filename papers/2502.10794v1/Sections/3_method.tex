\section{Methodology}
\label{sec:method}



\input{Figures/figure_framework}


Beyond the standard LLMs, MLLMs also encode visual content into token sequences, saying that the intrinsic interactions among visual elements can affect the model’s ability to defend against harmful prompts. However, most of the existing approaches~\citep{li2024images, liu2023query, liu2025mm} only take a single, complete image as input, resulting in notable similarities both (1) within image patches, and (2) between the image and the targeted jailbreak content. Such similarities may make the model likely to detect harmful inputs, potentially leading to jailbreak failure. To address these issues, we propose a novel jailbreak framework, called CS-DJ, as illustrated in Figure~\ref{fig:framework}. CS-DJ leverages a multi-subimage structure to bypass the models' intrinsic safety detection mechanism, with introducing multi-level distraction. Specifically, CS-DJ consists of two main components: \textbf{structured distraction}, achieved through query decomposition, and \textbf{visual-enhanced distraction}, realized through the construction of contrasting subimages. The resulting composite image input, combined with a carefully designed instruction, is fed into MLLMs to achieve jailbreak. Further details are described in the following sections.

\subsection{Structured Distraction via Query Decomposition}
\label{subsec:structuredistraction}

MLLMs have strong defense ability against harmful textual inputs, reinforced by specific optimizations aligned with human preference data~\citep{katz2024gpt}. To circumvent these defenses, we introduce a structured distraction strategy that disrupts and fragments information alignment through query decomposition, thereby reducing the model’s overall sensitivity to harmful content. As illustrated on the left of Figure~\ref{fig:framework}, we begin by decomposing a original harmful query $Q$ into multiple sub-queries, denoted as $\{Q_s^{(i)}\}_{i=1}^{m}$. Each sub-query addresses part of the original query from different aspects or intermediate steps.
\begin{equation}
    \{Q_s^i\}_{i=1}^{m} = \mathcal{G}(Q),
\end{equation}
where $\mathcal{G}$ is the auxiliary decomposition model and $m$ denotes the number of sub-queries. On one hand, this fragmented structure distracts the model’s focus, creating a distributional shift that obscures the original intent and disrupts its ability to detect harmful content, thereby bypassing its safety mechanisms. On the other hand, during the jailbreak execution phase, we carefully design prompts that guide the MLLM to address all sub-queries simultaneously. This strategy causes the model to divide its attention among multiple tasks rather than focusing on a single query. Further details are discussed in Section~\ref{sec:jail_exec}. In practice, we utilized \texttt{Qwen2.5-3B-Instruct}~\cite{qwen2.5} for query decomposition. 
 
The details of the query decomposition prompt are provided in the Appendix. 


The next step involves converting the sub-queries into images:
\begin{equation}
    {\{I_s^i\}}_{i=1}^{m} = \mathcal{T}(\{{Q_s^i\}}_{i=1}^{m}),
\end{equation}
where $\mathcal{T}$ denotes the transformation function that converts each sub-query $Q_s^i$ into a corresponding image  $I_s^i$. This transformation introduces a new form of distraction by altering the input modality, complicating the model’s ability to link and identify harmful content patterns. Presenting textual prompts as visual elements obscures their original intent, further disrupting the model’s detection mechanisms.


\subsection{Visual-Enhanced Distraction via Multi-subimage Construction}
\label{subsec:visualdistraction}

In contrast to prior work that emphasizes the harmfulness of a single image for MLLM jailbreaks, we focus on the impact of distraction. To enhance visual input distraction, we construct multiple contrasting visual elements, each treated as a subimage that collectively forms the input image. Since MLLMs represent visual content as token sequences similar to text, the distraction must include two aspects: (1) the distraction between the text query and the visual subimage, and (2) the distraction among the visual subimages themselves. Directly constructing an image with sufficient distraction relative to a text query is challenging. Therefore, we simplify this process by transforming it into an image retrieval problem, aimed at retrieving $k$ images from a dataset. These $k$ images are selected to have the lowest similarity to the query and to each other. Given the complexity of this optimization problem, we further approximate it for practical implementation.
The procedure is illustrated in the middle of Figure~\ref{fig:framework}.
Specifically, we first encode the query $Q$ as a dense vector using the CLIP model:
\begin{equation}
    v(Q)= \text{CLIP}(Q).
\end{equation}
In practice, CLIP-ViT-L/14~\cite{radford2021learning} is utilized as the extractor. We then retrieve an image from a dataset that most contrasts with $v(Q)$, effectively by minimizing the cosine similarity as:
\begin{equation}
I_c^1 = \arg \min_{I \in \mathcal{D}} \textbf{cos}(v(Q), v(I)),
\end{equation}
where $\mathcal{D}$ denotes the image dataset, and $\textbf{cos($\cdot,\cdot$)}$ represents the cosine similarity. Next, we proceed to retrieve the subsequent subimages:
\begin{equation}
I_{c}^{j} = \arg \min_{I \in \mathcal{D}} \left( \textbf{cos}(v(Q), v(I)) + \sum_{i=1}^{j-1} \textbf{cos}\left( v(I_c^i), v(I)\right) \right),
\end{equation}
where $j$ denotes the index of the current subimage being selected. This approach systematically ensures that each subimage has minimal similarity to both the query and the previously selected subimages, thereby maximizing contrast and enhancing the overall distraction effect for the MLLM jailbreak process.



\subsection{Jailbreaking Execution}
\label{sec:jail_exec}


With the constructed set of subimages, we form a composite input that maximizes distraction for MLLMs. Specifically, the final composite image  $I_{\text{comp}}$  is created by combining the subimages  $\{I_c^j\}_{j=1}^k$  along with the transformed sub-queries  $\{I_s^i\}_{i=1}^{m}$ :

\begin{equation}
I_{\text{comp}} = \textbf{Combine}\left({\{I_c^j\}}_{j=1}^k, {\{I_s^i\}}_{i=1}^{m}\right),
\end{equation}
where the function $\textbf{Combine}(\cdot,\cdot)$ represents the structured arrangement of the subimages and sub-query transformations.

This composite input $I_{\text{comp}}$, along with a harmless instruction $P$, is fed into a MLLM for processing:

\begin{equation}
Y = \text{MLLM}(I_{\text{comp}}, P),
\end{equation}
where $Y$ denotes the textual output of the MLLM.


The use of $I_{\text{comp}}$ with contrasting visual elements maximizes distraction, effectively disrupting the model’s internal alignment and coherence mechanisms. To complement $I_{\text{comp}}$, we carefully design $P$ to further enhance the distraction effect. As illustrated in Figure~\ref{fig:Figure_banner}, $P$ comprises three main components: (1) a role-guiding section, (2) a task-guiding section, and (3) a visual-guiding section. The role-guiding strategy is commonly employed across various LLM tasks. The second component of $P$ directs the model to process multiple sub-queries simultaneously, increasing task complexity and dispersing attention across multiple tasks instead of focusing on a single one. Finally, $P$ includes a misleading instruction, informing the model that the visual subimages may contain useful information, further diverting its attention. By presenting these inputs together, the approach reduces the model’s ability to detect and mitigate harmful content, effectively bypassing its intrinsic safety mechanisms through multi-level distraction.


\subsection{Distraction Distance Measurement}
To quantify the effectiveness of our multi-subimage input for distraction, we introduce a metric called Distraction Distance. This metric evaluates interactions among the query $Q$ and contrasting visual images ${\{I_c^j\}}_{j=1}^k$, treating all as equally important nodes within a unified structure.

Each node is represented by its CLIP-encoded vector. For a given node, we calculate the $\text{L}_2$ distance between its vector and the vectors of all other nodes. The sum of these $\text{L}_2$ distances for each node is calculated across all other nodes to obtain the Distraction Distance:
\begin{equation}
DL = \sum_{i=1}^{N} \sum_{j \neq i} ||v_i - v_j||_2,
\end{equation}
where $N$ denotes the total number of nodes, and  $v_i$  represents the CLIP-encoded vector of node $i$. This approach accounts for both the number of nodes and their pairwise distances, providing a comprehensive measure of the dispersion and contrast within the multi-subimage structure. By maximizing this Distraction Distance, we amplify the overall distracting effect on MLLMs, thereby disrupting their ability to accurately detect harmful content.



