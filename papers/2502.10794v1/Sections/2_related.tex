\section{Related Work}
\label{sec:related}

\subsection{Multimodal Large Language Models}



In recent years, Multimodal Large Language Models (MLLMs)~\cite{wang2024comprehensive} have emerged as the leading solution for handling complex multimodal data, such as images\cite{zhang2024vision} and audio~\cite{fathullah2024prompting}. 
Inspired by instruction-tuning algorithms in LLMs, many MLLMs are fine-tuned from pre-trained LLMs. Notable examples include LLaVA~\cite{liu2024visual} and MiniGPT-4~\cite{zhu2023minigpt}, which utilize similar architectures by connecting pre-trained vision backbones to large language models via a linear layer. These models are then fine-tuned using ChatGPT-generated data, enabling deep integration between vision and language. To enhance performance and usability, LLaVA-1.5~\cite{liu2024improved} replaces the linear layer with MLPs and incorporates prompt-based response formats optimized for academic VQA datasets. 

MM1~\cite{mckinzie2024mm1} and MoE-LLaVA~\cite{lin2024moe} further introduce MoE~\cite{shazeer2017outrageously} structure to achieve better performance, without increasing computational cost.
However, the integration of emerging modalities further intensifies the challenges associated with the safety alignment. 

To achieve this, MLLMs are typically fine-tuned using data annotated with human preferences to ensure that their outputs are helpful and harmless. Despite these efforts, this approach remains vulnerable to attacks using OOD inputs, which will be thoroughly explored in this work. 




\subsection{Jailbreak Attacks against MLLMs}
Jailbreak attacks against MLLMs can be conducted through both visual and textual inputs, leading to complex and diverse attack patterns~\cite{jin2024jailbreakzoo,niu2024jailbreaking}.
Current jailbreak attack techniques can be broadly categorized into two types: image perturbation injection and prompt-to-image infection jailbreaks. 
Among them, image perturbation injection techniques are particularly effective for white-box jailbreak scenarios.
 
In particular, to bypass the modelâ€™s security detection mechanisms, Qi \textit{et al.}~\cite{qi2024visual} conceal harmful textual information within images by injecting invisible noise~\cite{zhou2023advclip} through gradient optimization. 
Furthermore, JIP~\cite{shayegani2023jailbreak} conceals harmful image information within benign images through latent space semantic alignment.
To enable jailbreaks on black-box MLLMs, Hades~\cite{li2024images} investigates the influence of image harmfulness on model jailbreaks and proposes a prompt-to-image infection jailbreak attack method. It generates harmful images using diffusion models~\cite{song2020denoising,yang2023diffusion,ma2024visual} and amplifies their effect through optimization. The generated image is then combined with the query text to form a jailbreak prompt~\cite{liu2023query,liu2024arondight,gong2023figstep}, successfully achieving a jailbreak. 
In a word, existing MLLM jailbreak attack methods generally focus on crafting out-of-distribution (OOD) inputs. However, it remains unclear how to effectively leverage the visual space to construct OOD inputs for successful black-box MLLM jailbreaks. To address this, we propose CS-DJ, which employs distraction to guide the construction of OOD inputs.




