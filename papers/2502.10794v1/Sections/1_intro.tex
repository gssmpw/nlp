\section{Introduction}
\label{sec:intro}


Since the early 2020s, Multimodal Large Language Models (MLLMs) have emerged as a powerful tool for bridging natural language processing and computer vision \cite{cui2024survey}, owing to their ability to integrate and process multimodal information efficiently~\cite{tan2024harnessing}.

Despite their impressive performance, one notable limitation of these methods is that the training materials may contain toxic content, potentially leading to inappropriate or harmful content \cite{zhang2024jailbreak, deshpande2023toxicity}, the leakage of sensitive information \cite{yu2024llm}, and even serious vulnerabilities in the model security and privacy protection. To this end, significant efforts have been devoted to using reinforcement learning from human feedback (RLHF) to align LLMs outputs, ensuring readiness for deployment in high-stakes domains~\citep{ouyang2022training, wang2023self}. However, beyond Large Language Models (LLMs), the integration of visual inputs in MLLMs introduces a new challenge: \textit{securing models against vulnerabilities arising from newly integrated visual modalities}, which remains a significant issue and lacks a comprehensive understanding~\cite{yu2024hallucidoctor, lu2023set, liu2025mm}


In this study, rather than following the human feedback alignment research at the defense side, we focus on exploring an alternative approach, namely jailbreak attacks~\cite{wei2024jailbroken,yu2024llm,ma2024visual}. This line of research also seeks to provide insights into how safety concerns emerge, while, considering the attacker's perspective. Specifically, it aims to actively uncover the defense vulnerabilities in current model by deliberately crafting inputs to manipulate MLLMs, leading to unintended and potentially harmful outputs~\citep{liu2024making}. Motivated by the fact that most current defense mechanisms rely on RLHF and are trained on human preference data~\citep{ouyang2022training}, a natural hypothesis regarding the defense vulnerabilities of MLLMs is that the model may be susceptible to out-of-distribution (OOD) inputs.


Given that harmful textual content is already effectively detected in the field of LLMs, current jailbreak attack research on MLLMs primarily focuses on constructing \textbf{OOD visual inputs}, with two main approaches: image perturbation injection and prompt-to-image infection. The image perturbation injection involves subtly modifying images and combining them with query text to compose jailbreak prompts~\cite{wu2023jailbreaking}.
This attack leverages adversarial approaches \cite{wang2024transferable} to introduce noisy OOD inputs that exploit the MLLM’s visual-text decision boundary \cite{zhao2024evaluating}. Unfortunately, these methods typically require access to the gradient of the victim or surrogate model \cite{wang2024white}, which limits their applicability in closed-source models \cite{tu2023many}.
In contrast, prompt-to-image infection generates images with similar semantics by using malicious text. These generated images are then combined with query text to craft jailbreak prompts that compel the MLLM to produce harmful responses \cite{gong2023figstep,li2024images}. Since these generated images differ in distribution from those used for MLLM safety alignment training, they may bypass security measures and achieve jailbreak \cite{liu2023query,tao2024imgtrojan,chen2024bathe}.
Despite this, the extensive training of large MLLMs on vast datasets makes it challenging for image generation models to produce truly OOD images. A failure case is depicted in the upper portion of Figure~\ref{fig:Figure_banner}. 


\textbf{Findings.} Despite progress, it remains unclear how to fully leverage the visual space to construct OOD inputs for effective black-box MLLM jailbreaks. Building on the observation that most harmless alignment processes primarily involve simple images~\citep{gong2023figstep, li2024images, qi2024visual}, we propose that increasing image complexity, potentially through the use of multiple subimages, could generate more effective OOD inputs for jailbreak attack. To verify this, we conduct several ablation analyses ($\S$\ref{subsec:diversitymatters}), including varying the number of subimages and examining the relationship between image content and task. Our findings reveal that it is the complexity of the subimages, rather than their conceptual content, that drives the jailbreak success. Based on this insight, we propose the Distraction Hypothesis to explain the visual effects on jailbreak attacks against MLLMs.

\begin{empheqboxed}
    \looseness=-1 \textbf{Distraction Hypothesis.}
    \textit{Encoding complex images in the input prompt increases token complexity/diversity, which raises the processing burden on MLLMs. This overload can weaken the model's defenses, making it more prone to induce unintended outputs and improving jailbreak attack effectiveness.}
\end{empheqboxed}
\textbf{Structure and contributions.} Building on the Distraction Hypothesis, we further introduce a novel framework, namely \textit{\textbf{C}ontrasting \textbf{S}ubimage \textbf{D}istraction \textbf{J}ailbreaking} (CS-DJ) ($\S$\ref{sec:method}), which leverages this insight to strategically design OOD inputs and enhance the effectiveness of jailbreak attacks. Specifically, CS-DJ explores both textual and visual components of the input space, introducing two key methods: structured distraction via query decomposition ($\S$\ref{subsec:structuredistraction}) and visual-enhanced distraction through contrasting subimages ($\S$\ref{subsec:visualdistraction}). (i) The structured distraction component decomposes the original harmful query into multiple sub-queries, each representing different aspects or intermediate steps of the original query. This decomposition induces a distributional shift that disperses the model’s focus and weakens its ability to detect harmful content. (ii) The visual-enhanced distraction component manipulates the visual input by constructing multiple contrasting subimages, effectively disrupting interactions among visual elements during processing. Then, to guide the model’s response and enhance its susceptibility to the multi-level distractions, CS-DJ incorporates a carefully designed harmless prompt alongside the composite visual input, as illustrated in Figure~\ref{fig:Figure_banner}. This integrated approach disrupts the model’s internal coherence and safety mechanisms, effectively bypassing its defenses.

To evaluate the effectiveness of CS-DJ, we conduct extensive experiments across five representative scenarios and four widely used closed-source MLLMs ($\S$\ref{sec:experiment}). Our results demonstrate that CS-DJ outperforms state-of-the-art jailbreak attacks, achieving average success rates of \textbf{52.40\%} for attack success rate and \textbf{74.10\%} for ensemble attack success rate.





