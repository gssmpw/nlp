\begin{remark}
%
Following Definition \ref{def-transformer-model}, multi-head attention consists of parallelizing the self-attention operation across $H$ different heads with an embedding space $d_h < d$, 
%
\begin{equation}
    \hat{\bm{X}}(\bm{X}) = \bm{X} + \text{concat}\big(\bm{A}_1\bm{V}_1, \bm{A}_2\bm{V}_2, \dots, \bm{A}_h\bm{V}_h\big)\bm{W}_o
\end{equation}
%
where $\bm{A}_h = \sigma(d^{-1/2}\,\,\bm{X} \,\bm{W}_{q,h}\,\bm{W}^\top_{k,h}\,\bm{X}^T)$ is the self-attention of the $h$-th head, $\bm{W}_{q,h}\in \mathbb{R}^{d,d_h}$, $\bm{W}_{k,h}\in \mathbb{R}^{d,d_h}$ and $\bm{W}_{v,h}\in \mathbb{R}^{d,d_h}$ are the query, key, and value matrices of the $h$-th attention head, respectively, and $\bm{W}_o \in \mathbb{R}^{d,d}$ is a linear transformation \citep{vaswaniAttentionAllYou2017}.
%
Operationally, the self-attention computation is performed in parallel by factorizing the $\bm{W}_q$ and $\bm{W}_k$ matrices into $H$ rectangular blocks, as follows,
%
\begin{equation}
\begin{split}
    & \bm{W}_q = \big[\bm{W}_{q,1} \big| \bm{W}_{q,2} \big| \dots \big| \bm{W}_{q,H}\big] \\
    & \bm{W}_k = \big[\bm{W}_{k,1} \big| \bm{W}_{k,2} \big| \dots \big| \bm{W}_{k,H}\big] \,,
\end{split}
\end{equation}
%
and performing the matrix multiplication $\bm{W}_{q,h}\bm{W}_{k,h}^\top$ per every $h$-th head independently in one step.
%
It follows that the full $\bm{W}_{qk}$ matrix is given by the sum of the bilinear forms $\bm{W}_{qk,h}$ of every head, as follows,
%
\begin{equation}
    \bm{W}_{qk} = \bm{W}_q\bm{W}_k^\top = \sum_h \bm{W}_{q,h}\bm{W}^\top_{k,h} = \sum_h \bm{W}_{qk,h}
\end{equation}
%
where each $\bm{W}_{qk,h} \in \mathbb{R}^{d,d}$ is a square matrix with $\text{rank}(\bm{W}_{qk,h}) \leq d_h$.
%
Therefore, each head perform independent projections onto $\text{Conv}(\bm{X})$ that are then summed together, as follows,
%
\begin{equation}
    \sum_j\hat{\alpha}_{ij}\, \bm{x}_j = \sum_j \bm{x}_i^\top \bm{W}_{qk} \bm{x}_j\,\bm{x}_j = 
    \sum_j \bm{x}_i^\top \Big(\sum_h\bm{W}_{qk,h}\Big) \bm{x}_j\,\bm{x}_j = \sum_j \sum_h \langle \bm{x}_i, \bm{W}_{qk,h}\bm{x}_j \rangle \,\bm{x}_j \,,
\end{equation}
%
thus performing the same operations as in Equation \eqref{eq-proof-self-attention-bilinear-form}.
%
\end{remark}
%