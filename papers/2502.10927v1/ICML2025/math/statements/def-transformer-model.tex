%
\begin{definition}
\label{def-transformer-model}
%
(\textbf{\emph{Transformer architecture}}).
%
%
Let $\bm{U} \in \mathbb{R}^{N,V}$ be a matrix representing the sequence of $N$ one-hot encoded tokens of dimension $V$.
%
A Transformer architecture consists of $L$ stacked attention blocks, each one composed of a self-attention layer and a feedforward layer, as follows,
%
\begin{equation}
\label{eq:def:transformer-model}
\begin{cases}
%
\bm{X}_0(\bm{U}) = \bm{U} \bm{W_e} + \bm{W}_p \\[5pt]
\vspace{5pt}
\begin{aligned}
\Bigg\{
\begin{aligned}
&\hat{\bm{X}}_l = \bm{X}_{l-1} + a_l(\bm{X}_{l-1}; \bm{W}_q^l, \bm{W}_k^l, \bm{W}_v^l) \\[2pt]
&\bm{X}_l = \hat{\bm{X}}_l + m_l(\hat{\bm{X}}_l ; \bm{W}_1^l, \bm{W}_2^l)
\end{aligned} & \,\,\, \forall l \in [1, L] 
\end{aligned} \\
\sigma\big(\bm{Z}\big) = \sigma\big(\bm{X}_L \bm{W_u}\big) \,,
%
\end{cases}
\end{equation}
%
where $\bm{W_e}\in \mathbb{R}^{V,d}$ represents the linear transformation from the vocabulary space to the embedding space of dimension $d$, $\bm{W_p}\in \mathbb{R}^{V,d}$ represents the positional encoding,  $\bm{X}_0\in \mathbb{R}^{N,d}$ is the initial embedding of the sequence, 
$a_l(\cdot)$ is a self-attention function given by 
%
\begin{equation}
    a(\bm{X}_{l-1}) = \bm{A}^l(\bm{X}_{l-1})\bm{V}^l(\bm{X}_{l-1})
\end{equation}
%
where the matrix of attention scores $\bm{A}^l(\bm{X}_{l-1})$ is given by 
%
\begin{equation}
\label{eq:results:self-attention-transformers}
\begin{cases}
%
\bm{Q}^l(\bm{X}_{l-1}) = \bm{X}_{l-1}\bm{W}^l_q \\
\bm{K}^l(\bm{X}_{l-1}) = \bm{X}_{l-1}\bm{W}^l_k \\
\bm{A}^l(\bm{X}_{l-1}) = \sigma\left(\frac{1}{\sqrt{d}}\,\bm{Q}^l {\bm{K}^l}^T\right) \,,\\
%
\end{cases}
\end{equation}
%
where $1/\sqrt{d}$ is a constant normalization factor, and $\bm{W}_q^l\in \mathbb{R}^{d,d}$ and $\bm{W}^l_k\in \mathbb{R}^{d,d}$ represent linear transformations within the embedding space,
$\bm{V}^l(\bm{X}_{l-1}) = \bm{X}_{l-1}\bm{W}^l_v$ represents a linear transformation within the embedding space, $m_l(\cdot)$ is a position-wise feedforward layer with hidden dimension $d_f$ and learnable matrices $\bm{W}^l_1 \in \mathbf{R}^{d,d_f}$ and $\bm{W}^l_2\in \mathbf{R}^{d_f,d}$, $\bm{W_u}\in \mathbb{R}^{d, V}$ represents the linear transformation from the embedding space back to the vocabulary space, $\sigma(\cdot)$ is the row-wise softmax function,  and $\sigma\big(\bm{Z}\big)\in \mathbb{R}^{N,V}$ is the estimated probability distribution over the vocabulary.
%
We omit layer normalization and biases for simplicity (see also \citep{elhageMathematicalFrameworkTransformer2021}).
%
\end{definition}
%