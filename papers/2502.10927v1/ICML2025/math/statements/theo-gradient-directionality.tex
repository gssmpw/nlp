\begin{theorem}
\label{theo-gradient-directionality}
%
(\textbf{Autoregressive training induces directionality})
%
Let $\mathcal{V} = [t_0, \dots, t_V]$ be a set of tokens.
%
Let $U$ be an ordered sequence of $N$ tokens $U = [t_1, \dots, t_N]$ where $\Pr[t_j = t^*]$ is the probability that the token at index $j$ in $U$ is given by $t^* \in \mathcal{V}$.
%
Let $\{\bm{x}_1 \dots, \bm{x}_n\}$ be the embedding associated with $\mathcal{V}$ such that each $\bm{x}_i \sim \mathcal{D}$ is drawn i.i.d. from a probability distribution $\mathcal{D}$ with zero mean and semipositive covariance $\text{Cov}(\bm{x}_i)  \neq \sigma_x^2\mathbb{I}$.
%
Let $\Delta \bm{W}_{qk}$ be the weight update of $\bm{W}_{qk}$ from Proposition \ref{prop-gradients-self-attention}.
%
Let $\Delta \bm{W}_{qk}$ be derived from an autoregressive objective function as in Definition \ref{def-objective-functions}.
%
Then, there exists a value $\gamma \in \mathbb{R}$ such that,
%
\begin{equation}
    \text{Pr}[||\bm{w}_{\cdot, k}|| > w] > \text{Pr}[|| \bm{w}_{m, \cdot}|| > w] \,\,\ \forall \,w > \gamma
\end{equation}
%
% Then, for autoregressive training, the squared norm of the $m$-th row and $k$-th column satifies
% %
% \begin{equation}
% \frac{\mathbb{E}\left[ ||\Delta \bm{w}_{\cdot, k}||^2 \right]}{\mathbb{E}\left[ ||\Delta \bm{w}_{m, \cdot}||^2 \right]} > 1 \quad \forall k,m \in \{1,\dots,d\}\,,
% \end{equation}
% %
% and the number of columns with high-norm is higher than the number of rows with high-norm.
%
\end{theorem}