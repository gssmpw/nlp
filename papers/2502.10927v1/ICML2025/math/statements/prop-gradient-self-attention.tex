\begin{proposition}
\label{prop-gradients-self-attention}
%
(\textbf{The implicit weight update as sum of rank-1 matrices}).
% %
% Let $\mathcal{L}(\bm{U})$ be the negative log-likelihood of a given sequence of one-hot encoded tokens $\bm{U} = [\bm{t}_0, \bm{t}_1, \dots, \bm{t}_N]$, and let
% $\bm{z}_i$ be the prediction of the $i$-th token $\bm{t}_i$  from the last layer of a Transformer model as in Definition~\ref{def-transformer-model}, such that
% %
% \begin{equation}
%  \mathcal{L}(\bm{U}) = \sum_{i=1}^N \mathcal{L}(\bm{t}_i) = \sum_{i=1}^N \bm{t}_i\log(\sigma(\bm{z}_i))\,.
% \end{equation}
% %
Let $U = \{t_1, \dots, t_N\}$ be a  sequence of $N$ tokens, and let $\mathcal{L}(U)$ be the negative log-likelihood 
%
\begin{equation}
    \mathcal{L}(U;\mathcal{W}) = \sum_i \mathcal{L}(t_i)=  - \sum_i \log p(t_i \,|\, \{t_j : j \in  \mathcal{C}_i\}; \mathcal{W}) \,,
\end{equation}
%
with the conditional probability modeled with a Transformer model with learnable parameters $\mathcal{W}$.
%
Let the self-attention function be defined with a bilinear form as in Equation~\eqref{eq:results:self-attention-bilinear-form}.
%
Following the gradient of $\mathcal{L}(U)$, the implicit weight update of $\bm{W}_{qk}^l$ at the $l$-th layer is proportional to the sum of the contributions of each $t_j \in \mathcal{C}_i$ in predicting $t_i$, as follows,  
%
\begin{equation}
\label{eq:weight-update-bilinear-form}
\Delta \bm{W}_{qk}^l\propto  \sum_i \sum_{j\in C_i}  \Delta \bm{W}_{qk}^l\Big|_{t_i \leftarrow t_j} = \sum_i \sum_{j\in C_i} \beta^l_{ij} \bm{K}^{l-1}_{ij} 
\end{equation}
%
or equivalently to the sum of the contribution of each $t_i \in P_j$ when predicted by $t_j$,
%
\begin{equation}
\Delta \bm{W}_{qk}^l\propto  \sum_{i \in P_j} \sum_{j} \Delta \bm{W}_{qk}^l\Big|_{t_i \leftarrow t_j} =\sum_{i \in P_j} \sum_{j} \beta^l_{ij} \bm{K}^{l-1}_{ij}\,,
\end{equation}
%
where $\beta^l_{ij}$ is a scalar given by the contribution of the token embedding $\bm{x}_j$ to the error in predicting the token embedding $\bm{x}_i$ at the $l$-th self-attention layer,
and where $\bm{K}^{l-1}_{ij} \in \mathbb{M}_d$ is a rank-1 matrix given by the outer product of the token embeddings at the previous layers $l-1$, 
%
\begin{equation}
    \bm{K}^{l-1}_{ij} = \bm{x}^{l-1}_i{\bm{x}^{l-1}_j}^\top\,.
\end{equation}
%
\end{proposition}