
\begin{proposition}
\label{prop-self-attention-bilinear-form}
%
(\textbf{Self-attention scores preserve the structure of geometrical projections}).
%
Let $\bm{X} \in \mathbb{R}^{N,d}$ be the embedding of a sequence of $N$ tokens such that $\bm{X} = [\bm{x}_0^\top, \bm{x}_1^\top, \dots, \bm{x}_N^\top]$ and $\bm{x}_i \in \mathbb{R}^d \,\,\forall \,i$.  
%
Let $\bm{A}$ be the matrix of attention scores, and let $\alpha_{ij} = [\bm{A}]_{ij}$ be the $(i,j)$ element of the attention matrix.
%
We define $\bm{W}_{qk} = \bm{W}_q\bm{W}_k^\top$.
%
The self-attention transformation maps the $i$-th token $\bm{x}_i$ into $\hat{\bm{x}}_i$ as follows,
%
\begin{equation}
    \hat{\bm{x}}^\top_i = \bm{x}^\top_i + \sum_{j= 1}^N  \alpha_{ij}\,\bm{x}_j\bm{W}_v \,,
\label{eq-prop-self-attention-bilinear-form}
\end{equation}
%
where the coefficients $\{\alpha_{ij}\}$ represent a convex combination of $\bm{x}_j$ within the subspace spanned by $\bm{X} = \{\bm{x}_0^\top, \bm{x}_1^\top, \dots, \bm{x}_N^\top\}$.
%
These coefficients preserve the ordering of the geometrical projection of $\bm{x}_i$ onto the subspace $\bm{X}$ given by the bilinear form $\bm{W}_{qk}$,
%
\begin{equation}
    \hat{\alpha}_{ij}(\bm{W}_{qk}) = \langle \bm{x}_i, \bm{W}_{qk} \bm{x}_j \rangle = \langle \bm{x}_i, \bm{x}_j \rangle_{\bm{W}_{qk}} \,,
\end{equation}
%
such that,
%
\begin{equation}
     \alpha_{ij} < \alpha_{ij'} \,  \Leftrightarrow \,\hat{\alpha}_{ij} < \hat{\alpha}_{ij'}\quad \forall i,j, j' \,.
\end{equation}
%
\end{proposition}
%