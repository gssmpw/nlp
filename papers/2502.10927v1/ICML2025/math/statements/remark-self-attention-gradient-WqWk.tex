\begin{remark}
%
Let $\mathcal{L}(\bm{U})$ be the negative log-likelihood of a given sequence of one-hot encoded tokens $\bm{U}$.
%
Let $\bm{W}^l_q$ and $\bm{W}^l_k$ be the query and key transformation matrices of the $l$-th layer of Transformer models, see Definition \ref{def-transformer-model}.
%
Let $\bm{W}^l_q$ and $\bm{W}^l_k$ be updated via gradient descent, that is, $\bm{W}^l_q \rightarrow \bm{W}^l_q +  \eta\nabla_{\bm{W}_q^l} \mathcal{L}(\bm{U})$ and $\bm{W}^l_k \rightarrow \bm{W}^l_k +  \eta\nabla_{\bm{W}_k^l} \mathcal{L}(\bm{U})$, where $\eta$ is the learning rate.
%
It follows that the matrix $\bm{W}_{qk}^l = \bm{W}^l_q{\bm{W}^l_k}^\top$ is implicitly updated following,
%
% \begin{equation}
%     \bm{W}_{qk}^l = \bm{W}^l_q{\bm{W}^l_k}^\top 
%     \rightarrow
%     \,\big(\, \bm{W}^l_q + \eta\nabla_{\bm{W}_q^l} \mathcal{L}(U) \,\big)\big(\,\bm{W}^l_k + \eta\nabla_{\bm{W}_k^l} \mathcal{L}(U) \,\big)^\top
% \end{equation}

% which we rewrite explicitly as,
%
\begin{equation}
\begin{split}
    \bm{W}_{qk}^l \rightarrow & 
    \,\big(\, \bm{W}^l_q + \eta\nabla_{\bm{W}_q^l} \mathcal{L}(U) \,\big)\big(\,\bm{W}^l_k + \eta\nabla_{\bm{W}_k^l} \mathcal{L}(U) \,\big)^\top \\
    & = \bm{W}^l_q{\bm{W}^l_k}^\top + \eta\, \big(\, \bm{W}^l_q\,{\nabla_{\bm{W}_k^l} \mathcal{L}}^\top (U) + \nabla_{\bm{W}_q^l} \mathcal{L}(U){\bm{W}^l_k}^\top \,\big) + o(\eta^2) \\
    & = \bm{W}_{qk}^l + \eta \sum_i \sum_{j\in C_i} \beta_{ij}^l\big(\bm{W}_q^l{\bm{W}_q^l}^\top\, \bm{K}_{ij}^{l-1} + \bm{K}_{ij}^{l-1}\,\bm{W}_k^l{\bm{W}_k^l}^\top \big) 
    + o(\eta^2) \\
    & = \bm{W}_{qk}^l + \Delta \bm{W}_{qk}^l + o(\eta^2) \simeq \bm{W}_{qk}^l + \Delta \bm{W}_{qk}^l \,,
\end{split}
\end{equation}
%
assuming that the learning rate $\eta$ is small. 
%
Therefore, the implicit weight update of $\bm{W}_{qk}^l$ following gradient descent is given by
%
\begin{equation}
    \Delta \bm{W}_{qk}^l    = \eta \sum_i \sum_{j\in C_i} \beta_{ij}^l
    \left[\big(\bm{W}_q^l{\bm{W}_q^l}^\top\, \bm{K}_{ij}^{l-1} \big)
    + 
    \big(\bm{K}_{ij}^{l-1}\,\bm{W}_k^l{\bm{W}_k^l}^\top \big)\right] 
    \propto \sum_i \sum_{j\in C_i} \beta_{ij}^l\, \bm{K}_{ij}^{l-1} \,,
\end{equation}
%
where both $\bm{W}_q^l{\bm{W}_q^l}^\top\, \bm{K}_{ij}^{l-1}$ and $\bm{K}_{ij}^{l-1}\,\bm{W}_k^l{\bm{W}_k^l}^\top$ are rank-1 matrices, 
%
\begin{equation}
\begin{split}
    & \bm{W}_q^l{\bm{W}_q^l}^\top\, \bm{K}_{ij}^{l-1} = \Big( \bm{W}_q^l{\bm{W}_q^l} \,\bm{x}^{l-1}_i\Big){\bm{x}^{l-1}_j}^\top = \bm{\bar{x}}^{l-1}_i {\bm{x}^{l-1}_j}^\top  \\
    & \bm{K}_{ij}^{l-1}\,\bm{W}_k^l{\bm{W}_k^l}^\top = \bm{x}^{l-1}_i\Big({\bm{x}^{l-1}_j}^\top\,\bm{W}_k^l{\bm{W}_k^l}^\top \Big) = \bm{x}^{l-1}_i \bm{\bar{x}}_j^{{l-1}^\top} \,.
\end{split}
\end{equation}
%
%
\end{remark}