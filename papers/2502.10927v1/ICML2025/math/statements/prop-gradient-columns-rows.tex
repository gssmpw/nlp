\begin{proposition}
\label{prop-gradient-columns-rows}
%
(\textbf{Different implicit updates for context and prediction}).
%
%
Let $U = [t_0, \dots, t_N]$ be a sequence of tokens and let $\bm{x}_i \in \mathbb{R}^d$ be the embedding of the $i$-th token. 
%
Let $\Delta \bm{W}_{qk}$ be the weight update from Proposition \ref{prop-gradients-self-attention}.
%
Let $t^*$ be a given token in $U$.
%
When using $t^*$ as context, the $k$-th column of $\Delta \bm{W}_{qk}$ is given by
%
\begin{equation}
    \Delta \bm{w}_{\cdot, k} \bigg|_{t_j = t^*}= [\bm{x}_{t^*}]_k \left(\sum_{i \in P_{t^*}}\beta_{ij}\bm{x}_i\right)\,,
\end{equation}
%
while the $m$-th row of $\Delta \bm{W}_{qk}$ is given by
%
\begin{equation}
    \Delta \bm{w}_{m, \cdot} \bigg|_{t_j = t^*}= \left(\sum_{i \in P_{t^*}}\beta_{ij}[\bm{x}_i]_m\right)\bm{x}_{t^*} \,,
\end{equation}
%
where $P_{t^*}$ is the set of tokens predicted by $t^*$.
%
When predicting $t^*$, the $k$-th column of $\Delta \bm{W}_{qk}$ is given by
%
\begin{equation}
    \Delta \bm{w}_{k, \cdot} \bigg|_{t_i = t^*} = \left(\sum_{j \in C_{t^*}}\beta_{t^*j}[\bm{x}_j]_k\right)\bm{x}_{t^*} \,,
\end{equation}
%
while the $m$-th row of $\Delta \bm{W}_{qk}$ is given by
%
\begin{equation}
    \Delta \bm{w}_{m, \cdot} \bigg|_{t_i = t^*} = [\bm{x}_{t^*}]_m \left(\sum_{j \in C_{t^*}}\beta_{t^*j}\bm{x}_j^\top\right)\,.
\end{equation}
%
where $C_{t^*}$ is the set of context tokens for $t^*$.
%
\end{proposition}
%
% First we consider the gradient of a column for a given predictor-predicted pair $ij$, and derive the gradient associated to it
% \begin{equation}
%     \Delta \bm{w}_{\cdot, k}(i,j) = \Delta \bm{w}_{\cdot, k}\Big|_{\bm{t}_i \leftarrow \bm{t}_j}
%     = 
%      (\beta_{ij}[\bm{x}_j]_k)\,\bm{x}_i \,,
% \end{equation}
% where clearly the direction is given by the $\bm{x}_i$
% \begin{equation}
%     \Delta \bm{w}_{\cdot, k}(i,j) \mathbin{\|}  \bm{x}_i.
% \end{equation}
% Second, we note that the norm is proportional to the error ($\bm{t}_i - \sigma(\bm{z}_i)$ and is also proportional to the occurrence of $j$ before $i$. Assuming that most words have low predictive power, we would have that for each $i$ there are few $j$ with high gradients. 

% Now we look at the norm of the gradient for one column for the rest (for one of the high-gradient $j$)
% \begin{equation}
%     \dfrac{\|\Delta \bm{w}_{\cdot, k}(i,j)\|}{\sum_k \|\Delta \bm{w}_{\cdot, c}(i,j)\|} 
% \end{equation}
% and since $\beta_{ij}$, is the same scalar for all columns and the $\bm{x}_i$ is the same vector for all columns,
% \begin{equation}
%     \dfrac{\|\Delta \bm{w}_{\cdot, k}(i,j)\|}{\sum_c \|\Delta \bm{w}_{\cdot, k}(i,j)\|} 
%     = \dfrac{|[\bm{x}_j]_k|}{\sum_c |[\bm{x}_j]_k|}.
% \end{equation}
% Thus, for each pair $i,j$ the gradient would grow proportionally to $\bm{x}_j$ in the direction of $\bm{x}_i$.

% %Stop point 1: We assume that there is a low-rank structure in the data: few j that are predicting i. Then 
% Now we study how does the gradient change the attention, which is quantified