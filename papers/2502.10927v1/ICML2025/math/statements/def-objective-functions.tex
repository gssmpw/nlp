\begin{definition}
\label{def-objective-functions}
%
(\textbf{Autoregressive and bidirectional objectives})
%
Let $U = \{t_1, \dots, t_N\}$ a sequence of tokens.
%
The joint probability of $U$ is factorized autoregressively as follows,
%
\begin{equation}
    \Pr[U] = \Pr[t_1,\dots,t_N] = \Pi_{i=1}^N \Pr[t_i|t_1,\dots,t_{i-1}]\,.
\end{equation}
%
During autoregressive training, a model with set of parameters $\mathcal{W}$ is optimized to minimize the following negative log-likelihood
%
\begin{equation}
\mathcal{L}(U; \mathcal{W}) = - \sum_{i=1}^{N} \log p(t_i \,|\, \{t_j\} : j < i\,; \mathcal{W})\,,
\end{equation}
%
where the conditional probabilities are modeled with learnable parameters $\mathcal{W}$.
%
The join probability of $U$ can also be factorized bidirectionally as follows,
%
\begin{equation}
    \Pr[U] = \Pr[t_1,\dots,t_N] = \Pi_{i=1}^N \Pr[t_i|t_1,\dots,t_{i-1}, t_{i+1}, \dots, t_N]\,.
\end{equation}
%
During bidirectional training, a model with set of parameters $\mathcal{W}$ is optimized to minimize the following negative log-likelihood
%
\begin{equation}
\mathcal{L}(U; \mathcal{W}) = - \sum_{i=1}^{N} \log p(t_i \,|\, \{t_j\} : j \neq i\,; \mathcal{W})\,.
\end{equation}
%
In practice, only a subset $M\in [1,\dots,N]$ of the tokens are predicted as in  Masked Language Modelling (MLM) \cite[see ][]{devlinBERTPretrainingDeep2019,warnerSmarterBetterFaster2024}, leading to the following negative log-likelihood
%
\begin{equation}
\mathcal{L}(U; \mathcal{W}) = - \sum_{i\in M} \log p(t_i \,|\, \{t_j\} : j \notin M\,; \mathcal{W})\,.
\end{equation}
%
\end{definition}