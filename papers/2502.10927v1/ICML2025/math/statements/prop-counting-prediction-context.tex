\begin{proposition}
\label{prop-counting-prediction-context}
%
Let $\mathcal{V} = [t_0, \dots, t_V]$ be a set of tokens.
%
Let $\mathcal{U}$ be the sample space of all possible sequences of $N$ tokens, and let $U\in\mathcal{U}$ be a sequence $U = [t_1, \dots, t_N]$.
%
Let $\Pr[t_j = t^*]$ be the probability that the token at index $j$ in is given by $t^* \in \mathcal{V}$.
%
Let $\mathbb{E}[\mu_c(t^*)]$ be the expected number of tokens that are predicted by a given token $t^*$, and $\mathbb{E}[\mu_p(t^*)]$ the expected number of tokens that predict a given token $t^*$.
%
For autoregressive training, the ratio
%
\begin{equation}
    \frac{\mathbb{E}[\mu_c(t^*)]}{\mathbb{E}[\mu_p(t^*)]} = \frac{\sum_{k=1}^N (N-k)\Pr[t_k = t^*]}{\sum_{k=1}^N(k-1)\Pr[t_k = t^*]} \,,
\end{equation}
%
depends on $\Pr[t_j = t^*]$, while for bidirectional training the same ratio is given by 
%
\begin{equation}
     \frac{\mathbb{E}[\mu_c(t^*)]}{\mathbb{E}[\mu_p(t^*)]} = 1 \,.
\end{equation}
%    
\end{proposition}