\begin{proposition}
%
%
% (1) it follows from the rank-1 update that columns and rows have different updates, specifically the way context and predicted tokens are added to the different spaces --> derive the weight update for columns and rows separately - highlight the two cases: (1) if \bar{t} is context what happens to rows and columns vs (2) if \bar{t} is predicted what happens to rows and columns.
%
% (2) example with extreme case with 1 context token and many predicted tokens in sequences of 2 tokens or 1 predicted tokens and many context tokens in a sequence of 2 tokens --> covariance matrix argument works because every vector is randomly drawn from that distribution with higher variance, so they tend to have correlation between directions in embedding space.
% 
% (2.1) if one case happens more often than the other, than columns or rows have higher norm
%
% (3) show that the (i,j) are also random variable, defining the probability of occurrence of a given token \bar{t} as context or prediction. 
%
% (3) connect this to the counting proposition: S_j has more element than C_i, so the probability structures in the corpus affects the weight matrices.
%
% IMPORTANT - the number of times a token is predicted is p(t)N, but every time there are (j-1) tokens that predict it, so the argument only works for the non iid case, where 
%
% (4.2) generalize this argument by saying that, for a given token $\bar{t}$, this case is way more common than the opposite case where $\bar{t}$ is predicted by other tokens -> counting argument on how many tokens are predicted bu a given token vs how many tokens predict that given token. 


    
\end{proposition}