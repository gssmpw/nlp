\begin{corollary}
%
(\textbf{Bidirectional training induces rows and columns with equal norm})
%
Let $\mathcal{V} = [t_0, \dots, t_V]$ be a set of tokens.
%
Let $U$ be an ordered sequence of $N$ tokens $U = [t_1, \dots, t_N]$ where $\Pr[t_j = t^*]$ is the probability that the token at index $j$ in $U$ is given by $t^* \in \mathcal{V}$.
%
Let $\{\bm{x}_1 \dots, \bm{x}_n\}$ be the random embedding of the tokens as in Theorem \ref{theo-gradient-directionality}.
%
Let $\Delta \bm{W}_{qk}$ be the weight update of $\bm{W}_{qk}$ from Proposition \ref{prop-gradients-self-attention}.
%
Let $\Delta \bm{W}_{qk}$ be derived from a bidirectional objective function as in Definition \ref{def-objective-functions}.
%
Then, the variance of the norm of the rows and the columns at the end of training is equal,
%
%
\begin{equation}
    \text{Var}\left(|\bm{w}_{m, \cdot}|\right) =\text{Var}\left(|\bm{w}_{ \cdot, k}|\right) \,\, \forall k,m \,.
\end{equation}
%
\end{corollary}