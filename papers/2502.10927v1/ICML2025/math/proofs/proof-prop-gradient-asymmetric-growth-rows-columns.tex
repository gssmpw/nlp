\begin{proof}
%
%
Let $U = [t_0, \dots, t_N]$ a sequence of tokens, and let $t^*$ be the context of every token $t_i \in U$. 
%
Let $\{\bm{x}_i\}$ be the set of token embeddings associated with $U$ and let $\bm{y}$ be the token embedding of $t^*$.
%
It follows from Proposition \ref{prop-gradient-columns-rows} that the squared norm of the weight update of the $k$-th column is given by 
%
%
\begin{equation}
\Delta \bm{w}_{\cdot, k} = \sum_{j \in \{t^*\}}\sum_{i\in P_j} \beta_{ij}\bm{x}_i[\bm{x_j}]_k  = \sum_{i=1}^N \beta_{it^*}\bm{x}_i[\bm{y}]_k 
%
\,\,\Rightarrow \,\,
%
||\Delta \bm{w}_{\cdot, k}||^2 = [\bm{y}]^2_k || \sum_{i=1}^N \beta_{it^*}\bm{x}_i||^2 \,,
\end{equation}
%
while the squared norm of the weight update of the $m$-th row by
%
%
\begin{equation}
\Delta \bm{w}_{m, \cdot} = \sum_{j \in \{t^*\}}\sum_{i\in P_j} \beta_{it^*}[\bm{x}_i]_m\bm{x}_j= \sum_{i=1}^N \beta_{it^*}[\bm{x}_i]_m\bm{y} 
%
\,\,\Rightarrow \,\,
%
||\Delta \bm{w}_{m, \cdot}||^2 = \big(\sum_{i=1}^N\beta_{it^*}[\bm{x}_i]_m\big)^2 ||  \bm{y}||^2 \,.
\end{equation}
%
Let $\{\bm{x}_i\} \sim \mathcal{D}$ be a set of i.i.d. random vectors from a probability distribution $\mathcal{D}$ such that $\mathbb{E}[\bm{x}_i] = 0$ and $\text{Cov}(\bm{x}_i) = \Sigma \in \mathbb{R}^{d,d}$.
%
Therefore, each $k$-th coordinate $[\bm{x}_i]_k$ is such that  $\mathbb{E}[[\bm{x}_i]_k] = 0$ and $\text{Var}([\bm{x}_i]_k) = \Sigma_{k,k}$. 
%
We also assume that $\bm{y}$ is statistically independent from $\bm{x}_i \, \forall i$ with $\mathbb{E}[\bm{y}] = 0$ and covariance $\text{Cov}(\bm{y})  = \sigma_y^2\mathbb{I}$.
%
It follows that the expected value of $||\Delta \bm{w}_{\cdot, k}||^2$ over  $\mathcal{D}$ is given by
%
\begin{equation}
\mathbb{E}\left[ ||\Delta \bm{w}_{\cdot, k}||^2 \right] 
= 
\mathbb{E}\left[ [\bm{y}]^2_k ||\sum_{i=1}^N \beta_{it^*}\bm{x}_i||^2 \right]
= 
\mathbb{E}\left[ [\bm{y}]^2_k \right] \mathbb{E}\left[ || \sum_{i=1}^N \beta_{it^*}\bm{x}_i||^2 \right] \,.
\end{equation}
%
Given the statistical independence between the entries of $\bm{x}_i$, the second term is equal to
%
\begin{equation}
\begin{split}
\mathbb{E}\left[ || \sum_{i=1}^N \beta_{it^*}\bm{x}_i||^2 \right]
& = \sum_{i,i'}\mathbb{E}\left[ \beta_{it^*}\beta_{i't^*}\bm{x}^\top_{i}\bm{x}_{i'}\right]
\\
&= \sum_i\beta^2_{it^*}\mathbb{E}\left[\bm{x}^\top_{i}\bm{x}_{i}\right] +
\sum_{i'\neq i}\beta_{it^*}\beta_{i't^*}\mathbb{E}\left[\bm{x}^\top_{i}\bm{x}_{i'}\right]
\\
& =
\sum_i\beta^2_{it^*}\text{Tr}\big(\mathbb{E}\left[\bm{x}_{i}\bm{x}_{i}^\top\right]\big) +
\sum_{i'\neq i}\beta_{it^*}\beta_{i't^*}\mathbb{E}\left[\bm{x}^\top_{i}\bm{x}_{i'}\right] 
\\
& = \text{Tr}(\Sigma)\sum_i\beta^2_{it^*} \,,
\end{split}
\end{equation}
%
and therefore
%
\begin{equation}
    \mathbb{E}\left[ ||\Delta \bm{w}_{\cdot, k}||^2 \right] = \Gamma_{k,k}\text{Tr} (\Sigma)\sum_i\beta^2_{it^*} \,.
\end{equation}
%
Similarly, the expected value of $||\Delta \bm{w}_{m, \cdot}||^2$ is given by
%
\begin{equation}
\mathbb{E}\left[ ||\Delta \bm{w}_{m, \cdot}||^2 \right] 
= 
\mathbb{E}\left[\big(\sum_{i=1}^N\beta_{it^*}[\bm{x}_i]_m\big)^2 ||  \bm{y}||^2
\right]
= 
\mathbb{E}\left[ \big(\sum_{i=1}^N\beta_{it^*}[\bm{x}_i]_m\big)^2 \right] \mathbb{E}\left[ ||  \bm{y}||^2\right]\,.
\end{equation}
%
The first term can be decomposed as
%
\begin{equation}
\mathbb{E}\left[ \big(\sum_{i=1}^n\beta_{it^*}[\bm{x}_i]_m\big)^2 \right]  =
\sum_{i=1}^n \beta^2_{it^*}\mathbb{E}\left[[\bm{x}_i]_m^2 \right]  + \sum_{i' \neq i}\beta_{it^*}\beta_{i't^*}\mathbb{E}\left[ [\bm{x}_{i'}]_m [\bm{x}_{i}]_m \right] =  \Sigma_{m,m}\sum_{i=1}^n\beta^2_{it^*} \,,
\end{equation}
%
and therefore
%
\begin{equation}
\mathbb{E}\left[ ||\Delta \bm{w}_{m, \cdot}||^2 \right] = \Sigma_{m,m}\text{Tr}(\Gamma)\sum_i\beta^2_{it^*}\,.
\end{equation}
%
The ratio of the expected value of these squared norms is
%
\begin{equation}
\frac{\mathbb{E}\left[ ||\Delta \bm{w}_{\cdot, k}||^2 \right]}{\mathbb{E}\left[ ||\Delta \bm{w}_{m, \cdot}||^2 \right]} = \frac{ \Gamma_{k,k}\text{Tr}(\Sigma)}{ \Sigma_{m,m}\text{Tr}(\Gamma)} = \frac{\Gamma_{k,k}}{\text{Tr}(\Gamma)}\frac{\text{Tr}(\Sigma)}{\Sigma_{m,m}} = \frac{1}{d}\frac{\text{Tr}(\Sigma)}{\Sigma_{m,m}} \,.
\end{equation}
%
We assume a non-isotropic covariance structure in $\Sigma$, that is, the average variance per dimension is lower than the total variance across all dimensions.
%
This implies $\text{Tr}(\Sigma) > d\,\Sigma_{m,m}$ for some $m$.
%
It follows that,
%
\begin{equation}
\frac{\mathbb{E}\left[ ||\Delta \bm{w}_{\cdot, k}||^2 \right]}{\mathbb{E}\left[ ||\Delta \bm{w}_{m, \cdot}||^2 \right]} = \frac{1}{d}\frac{\text{Tr}(\Sigma)}{\Sigma_{m,m}}  > \frac{1}{d}\frac{d \,\Sigma_{m,m}}{\Sigma_{m,m}} = 1 \quad \forall m \,\,\text{s.t.}\,\,\Sigma_{m,m} < \frac{\text{Tr}(\Sigma)}{d}\,,
\end{equation}
%
thus concluding the proof.
%
\end{proof}