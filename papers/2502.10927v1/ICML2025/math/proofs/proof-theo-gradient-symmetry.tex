\begin{proof}
%
%
Let $U$ be an ordered sequence of $N$ tokens $U = [t_1, \dots, t_N]$.
%
It follows from Proposition \ref{prop-gradients-self-attention} that the weight update for $\bm{W}_{qk}^l$ following the gradient of $\mathcal{L}(\bm{U})$ w.r.t. $\bm{W}_{qk}^l$ is given by
%
\begin{equation}
\label{eq:gradient-self-attention-summations}
    \Delta \bm{W}_{qk}^l = \sum_{i =1}^N \sum_{j=1}^N \beta^l_{ij} \bm{K}^{l-1}_{ij} \,,
\end{equation}
%
where we neglect any constant of proportionality - such as a learning rate - for simplicity.
%
The double summation in Equation \eqref{eq:gradient-self-attention-summations} contains $N^2$ elements.
%
We can rewrite the double summation as follows,
%
\begin{equation}
\sum_{i = 1}^N \sum_{j =1}^N \beta^l_{ij} \bm{K}^{l-1}_{ij} =
\sum_{i =1}^N \beta^l_{ii} \bm{K}^{l-1}_{ii} +
\sum_{\substack{i,j = 1 \\ i < j}}^N(\beta^l_{ij} \bm{K}^{l-1}_{ij} + \beta^l_{ji} \bm{K}^{l-1}_{ji})
\end{equation}
%
where the first term includes the diagonal terms, and the second includes the contributions of every pair $(i,j)$ with $i,j \in [0, \dots, N]$.
%
The second term can be written as,
%
\begin{equation}
 \sum_{\substack{i,j = 1 \\ i < j}}^N (\beta^l_{ij} \bm{K}^{l-1}_{ij} + \beta^l_{ji} \bm{K}^{l-1}_{ji}) = \sum_{\substack{i,j = 1 \\ i < j}}^N(\beta^l_{ij} \bm{K}^{l-1}_{ij} + \beta^l_{ji} {\bm{K}^{l-1}_{ij}}^\top) \,, 
\end{equation}
%
and by decomposing $\bm{K}^{l-1}_{ij}$ in its symmetric and skew-symmetric parts, such that $\bm{K}^{l-1}_{ij} = \bm{S}^{l-1}_{ij} + \bm{N}^{l-1}_{ij}$, we obtain,
%
\begin{equation}
\label{eq:gradients-symmetric-terms}
\begin{split}
    \sum_{\substack{i,j = 1 \\ i < j}}^N (\beta^l_{ij} \bm{K}^{l-1}_{ij} + \beta^l_{ji} {\bm{K}^{l-1}_{ij}}^\top)
    & = \sum_{\substack{i,j = 1 \\ i < j}}^N \big[ \beta_{ij}^l(\bm{S}^{l-1}_{ij} + \bm{N}^{l-1}_{ij}) + \beta_{ji}^l(\bm{S}^{l-1}_{ij} + \bm{N}^{l-1}_{ij})^T \big] \\
    & = \sum_{\substack{i,j = 1 \\ i < j}}^N \big[(\beta^l_{ij} + \beta^l_{ji})\bm{S}^{l-1}_{ij} + (\beta^l_{ij} - \beta^l_{ji})\bm{N}^{l-1}_{ij}\big]\,.
\end{split}
\end{equation}
%
Let $\Delta \bm{W}_{qk}^l\big|_{\bm{t}_i \leftrightarrow \bm{t}_j} = \beta_{ij}^l\bm{K}^{l-1}_{ij} + \beta_{ji}^l{\bm{K}^{l-1}_{ij}}^T$,
and let $\beta^l_{ij}$ and $\beta^l_{ji}$ be such that $\text{sign}(\beta^l_{ij}) = \text{sign}(\beta^l_{ji})$ and $|\beta^l_{ij}| \approx $$|\beta^l_{ji}|$.
%
It follows that
%
\begin{equation}
 \Delta \bm{W}_{qk}^l\big|_{\bm{t}_i \leftrightarrow \bm{t}_j} \approx \sum_{\substack{i,j = 1 \\ i < j}}^N \beta^l_{ij}\bm{S}^{l-1}_{ij} = \sum_{\substack{i,j = 1 \\ i < j}}^N \beta^l_{ij}{\bm{S}^{l-1}}^\top_{ij} = \Delta {\bm{W}_{qk}^l}^\top\big|_{\bm{t}_i \leftrightarrow \bm{t}_j} \,
\end{equation}
%
thus concluding the proof.
%
%
%
%
% Let $\bm{U} = [\bm{t}_0, \bm{t}_1, \dots, \bm{t}_N]$ be a sequence of $N$ one-hot encoded tokens $\bm{t}_i \in \mathbb{R}^V$ where $V$ is the dimension of the vocabulary.
% %
% Following Equation \eqref{eq:results:self-supervised-pretraining}, let $\mathcal{L}(\bm{t}_i)$ be the cross-entropy of the one-hot encoded token $\bm{t}_i$  and let  $\sigma(\bm{z}_i) \in \mathbb{R}^V$ be the estimated probability distribution, as follows,
% %
% \begin{equation}
%  \mathcal{L}(\bm{U}) = \sum_{i=1}^N \mathcal{L}(\bm{t}_i) = \sum_{i=1}^N \bm{t}_i\log(\sigma(\bm{z}_i))\,.
% \end{equation}
% %
% Following Definition \ref{def-transformer-model}, let $\sigma(\bm{z}_i)$ the prediction of the $i$-th from the last layer of a Transformer models with $L$ layers,
% %
% \begin{equation}
% \begin{cases}
% %
% {\bm{x}^0_i}^\top = \bm{t_i}^\top \bm{W_e} + \bm{W}_p \\[5pt]
% \vspace{5pt}
% {\bm{x}_i^l}^\top = \mathcal{F}_l(\bm{x}_i^{l-1})\quad \forall l \in [1, L] \\
% \sigma\big(\bm{z}_i\big)  = \sigma\big({\bm{x}_i^L}^\top \bm{W_u}\big)\,,
% %
% \end{cases}
% \end{equation}
% %
% where ${\bm{x}^l_i}^\top = \mathcal{F}_l(\bm{x}_i^{l-1})$ is a short notation for the self-attention and multi-layered perception transformation of the $l$-th layer,
% %
% \begin{equation}
% \mathcal{F}_l(\bm{x}_i^{l-1}) =
% \left\{
% \begin{aligned}
% &{\hat{\bm{x}}^l_i}^\top = {\bm{x}^{l-1}_i}^\top + a_l(\bm{x}^{l-1}_i) \\
% &{\bm{x}^l_i}^\top = {\hat{\bm{x}}^l_i}^\top + m_l(\hat{\bm{x}}^l_i)
% \end{aligned}
% \right.\,.
% \end{equation}
% %
%
%
%
%
% We first obtain a relation between $\beta_{ij}$ and $\beta_{ji}$ in the case of $L = 1$ (1-layer self-attention).
% %
% It follows from Proposition \ref{prop-gradients-self-attention} that $\beta_{ij}$ is given by,
% %
% \begin{equation}
% \begin{split}
%     \beta_{ij} 
%     & = {\bm{\delta}_i}^\top \bm{x}_j 
%     \\
%     & = {\bm{\delta}_i}^\top \Big(\bm{x}_j^0 + a(\bm{x}_j^0) + m(\bm{\hat{x}}_j) \Big)
%     \\
%     & = {\bm{\delta}_i}^\top \Big(\bm{W_e}^\top\bm{t}_j + \bm{W}^\top_p +a(\bm{x}_j^0) + m(\bm{\hat{x}}_j) \Big)
%     \\
%     & = {\bm{\delta}_i}^\top\,\bm{W_e}^\top\bm{t}_j + \bm{\delta}_i^\top \Big(\bm{W}_p^\top + a(\bm{x}_j^0) + m(\bm{\hat{x}}_j) \Big)
%     \\
%     & = (\bm{t}_i - \sigma(\bm{z}_i))^\top \bm{W}_u^\top {\bm{W}_v}^\top\,\bm{W_e}^\top\bm{t}_j+ \bm{\delta}_i^\top \Big(\bm{W}_p^\top + a(\bm{x}_j^0) + m(\bm{\hat{x}}_j) \Big)
%     \\
%     & = \bm{t}_i^\top \bm{W}_u^\top {\bm{W}_v}^\top\,\bm{W}_e^\top \bm{t}_j- \sigma(\bm{z}_i)^\top \bm{W}_u^\top {\bm{W}_v}^\top\,\bm{W}_e^\top \bm{t}_j + \bm{\delta}_i^\top \Big(\bm{W}_p^\top + a(\bm{x}_j^0) + m(\bm{\hat{x}}_j) \Big) \,,
% \end{split}
% \end{equation}
% %
% while $\beta_{ji}$ is given by,
% %
% \begin{equation}
%     \beta_{ji} = \bm{t}_j^\top \bm{W}_u^\top {\bm{W}_v}^\top\,\bm{W}_e^\top \bm{t}_i- \sigma(\bm{z}_j)^\top \bm{W}_u^\top {\bm{W}_v}^\top\,\bm{W}_e^\top \bm{t}_i + \bm{\delta}_j^\top \Big(\bm{W}_p^\top + a(\bm{x}_i^0) + m(\bm{\hat{x}}_i) \Big) \,.
% \end{equation}
% %
% Therefore, the first terms in $\beta_{ij}$ and $\beta_{ji}$ are correlated, while the other terms are not.
% %
% Next, we generalize the relation between $\beta_{ij}$ and $\beta_{ji}$ for the case $L>1$.
% %
% In this case, $\beta^l_{ij}$ ath the $l$-th layer is given by,
% %
% \begin{equation}
% \begin{split}
%     \beta_{ij}^l 
%     & = {\bm{\delta}^l_i}^\top \bm{x}^{l-1}_j  
%     \\
%     & = {\bm{\delta}_i}^\top \Big(\bm{x}_j^0 + \sum_{l' =1}^l \mathcal{F}_l(\bm{x}^l_j)\Big) 
%     \\
%     & = {\bm{\delta}_i}^\top \Big(\bm{W_e}^\top\bm{t}_j + \bm{W}^\top_p + \sum_{l' =1}^l \mathcal{F}_l(\bm{x}^l_j)\Big) 
%     \\
%     & = {\bm{\delta}_i}^\top\,\bm{W_e}^\top\bm{t}_j + \bm{\delta}_i^\top \Big(\bm{W}_p^\top + \sum_{l' =1}^l \mathcal{F}_l(\bm{x}^l_j) \Big)
%     \\
%     & = (\bm{t}_i - \sigma(\bm{z}_i))^\top \bm{W}_u^\top
%     \left[
%     \left(1 + \sum_{l' = l}^{L-1}\mathcal{F}_l'(\bm{x}_i^{l'})\right) \left(1 +  m_l'(\hat{\bm{x}}^l_i)\right)
%     \right]
%     {\bm{W}_v}^\top\,\bm{W_e}^\top\bm{t}_j
%     + 
%     \bm{\delta}_i^\top \Big(\bm{W}_p^\top + \sum_{l' =1}^l \mathcal{F}_l(\bm{x}^l_j) \Big) \,,
%     \\
% \end{split}
% \end{equation}
% %
% which can be written as,
% %
% \begin{equation}
% \begin{split}
%     \beta_{ij}^l = 
%     & 
%     \,\, \bm{t}_i^\top \bm{W}_u^\top {\bm{W}_v}^\top\,\bm{W}_e^\top \bm{t}_j + \bm{t}_i^\top \bm{W}_u^\top \left(
%     \sum_{l' = l}^{L-1}\mathcal{F}_l'(\bm{x}_i^{l'}) +  m_l'(\hat{\bm{x}}^l_i) + m_l'(\hat{\bm{x}}^l_i)\,\sum_{l' = l}^{L-1}\mathcal{F}_l'(\bm{x}_i^{l'})
%     \right){\bm{W}_v}^\top\,\bm{W}_e^\top \bm{t}_j
%     \\
%     &
%     + \bm{\delta}_i^\top \Big(\bm{W}_p^\top + \sum_{l' =1}^l \mathcal{F}_l(\bm{x}^l_j) \Big)\,.
% \end{split}
% \end{equation}
% %
% Similarly, $\beta^l_{ji}$ is given by,
% %
% \begin{equation}
% \begin{split}
%     \beta_{ji}^l = 
%     & 
%     \,\, \bm{t}_j^\top \bm{W}_u^\top {\bm{W}_v}^\top\,\bm{W}_e^\top \bm{t}_i + \bm{t}_j^\top \bm{W}_u^\top \left(
%     \sum_{l' = l}^{L-1}\mathcal{F}_l'(\bm{x}_j^{l'}) +  m_l'(\hat{\bm{x}}^l_j) + m_l'(\hat{\bm{x}}^l_j)\,\sum_{l' = l}^{L-1}\mathcal{F}_l'(\bm{x}_j^{l'}) 
%     \right){\bm{W}_v}^\top\,\bm{W}_e^\top \bm{t}_i
%     \\
%     &
%     + \bm{\delta}_j^\top \Big(\bm{W}_p^\top + \sum_{l' =1}^l \mathcal{F}_l(\bm{x}^l_i) \Big)\,.
% \end{split}
% \end{equation}
% %
% The first terms in $\beta^l_{ij}$ and $\beta^l_{ji}$ are the same as for the case $L=1$, therefore the same correlation argument holds for the case $L>1$.
% %
% %
\end{proof}