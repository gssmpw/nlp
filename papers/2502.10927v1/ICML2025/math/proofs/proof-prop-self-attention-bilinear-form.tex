\begin{proof}
%
Let $\bm{A}$ be the matrix of attention scores in a self-attention layer
as in Definition \ref{def-transformer-model}.
%
It follows that,
%
\begin{equation}
    \hat{\bm{X}}(\bm{X}) = \bm{X} + \bm{A}(\bm{X})\bm{V}(\bm{X}) = \bm{X} + \sigma\left(\frac{1}{\sqrt{d}}\,\bm{X} \bm{W}_{qk}\bm{X}^T\right)\bm{X}\bm{W}_v \,,
\end{equation}
%
where $\bm{W}_{qk} = \bm{W}_q \bm{W}_k^\top$, and where we refer to a general layer in a Transformer model, omitting the subscript $l$.
%
It follows that the entry $\alpha_{ij} = [\bm{A}]_{ij}$ of the attention score matrix is given by, 
%
\begin{equation}
    \alpha_{ij} = \frac{\exp{(\bm{x}_i^\top \bm{W}_{qk} \bm{x}_j)}}{\sum_j \exp{(\bm{x}_i^\top \bm{W}_{qk} \bm{x}_j)}} = \frac{\exp{(\hat{\alpha}_{ij})}}{\mathcal{N}_i} \,,
\label{eq-proof-self-attention-coefficients-convex-hull}
\end{equation}
%
where we neglect the scaling factor $1 / \sqrt{d}$ for simplicity, every $\hat{\alpha}_{ij}$ is a coefficient given by $\hat{\alpha}_{ij} = \bm{x}_i^\top \bm{W}_{qk} \bm{x}_j$, and $\mathcal{N}_i$ is a normalization factor defined by the row-wise softmax function $\sigma(\cdot)$ and shared across all entries in the $i$-th row of $\bm{A}$,
%
\begin{equation}
    \mathcal{N}_i(\bm{X}; \bm{W}_{qk}) = \sum_j \exp{(\bm{x}_i^\top \bm{W}_{qk} \bm{x}_j)} = \sum_j \exp{(\hat{\alpha}_{ij})} \,.
\end{equation}
%
The coefficients $\{\hat{\alpha}_{ij}\}$ in the second term represent the projection of $\bm{x}_i$ onto $\text{span}\{\bm{X}\}$ (the subspace spanned by $\bm{X} = \{\bm{x}_0^\top, \bm{x}_1^\top, \dots, \bm{x}_N^\top\})$ in the transformed embedding space defined by $\bm{W}_{qk}$,
%
\begin{equation}
    \sum_j\hat{\alpha}_{ij}\, \bm{x}_j = \sum_j \bm{x}_i^\top \bm{W}_{qk} \bm{x}_j\,\bm{x}_j = \sum_j \langle \bm{x}_i, \bm{W}_{qk}\bm{x}_j \rangle \,\bm{x}_j \,.
\label{eq-proof-self-attention-bilinear-form}
\end{equation}
%
It follows from the monotonically increasing property of the soft-max function that the coefficients $\{\alpha_{ij}\}$ defined in Equation \eqref{eq-proof-self-attention-coefficients-convex-hull} preserve the order of the coefficients $\{\hat{\alpha}_{ij}\}$ while exponentially suppressing negative and near-zero values,
%
\begin{equation}
     \alpha_{ij} < \alpha_{ij'} \,  \Leftrightarrow \,\hat{\alpha}_{ij} < \hat{\alpha}_{ij'}\quad \forall i,j, j' \,.
\end{equation}
%
Therefore, the coefficients $\{\alpha_{ij}\}$ specify a convex combination that restricts the resulting vector in the convex hull $\text{Conv}(\bm{X}) \subset \text{span}\{\bm{X}\}$, where the $i$-th row of $\bm{X}$ corresponding to the $i$-th token in the sequence is transformed into the row vector $\hat{\bm{x}}_i$ as follows,
%
\begin{equation}
\hat{\bm{x}}^\top_i = \bm{x}^\top_i + \sum_j\alpha_{ij}\,\bm{x}_j\bm{W}_v\,,
\end{equation}
%
thus concluding the proof.
% 
\end{proof}