\begin{proof}
%
Let $U = \{t_1, \dots, t_N\}$ be a  sequence of $N$ tokens from a vocabulary of dimension $V$.
%
Let $\mathcal{L}(U)$ be the negative log-likelihood of each token $t_i$, expressed as
%
\begin{equation}
\label{eq:results:self-supervised-pretraining}
    \mathcal{L}(U) = \sum_i \mathcal{L}(t_i)=  - \sum_i \log p(t_i \,|\, \{t_j : j \in  \mathcal{C}_i\}) \,,
\end{equation}
%
where $\mathcal{C}_i \subset [0,1,\dots, N]$ is the set of indices defining the set of tokens $\{t_j\}$ of the conditional probability distribution.
%
Let $\bm{U} = [\bm{t}_0, \bm{t}_1, \dots, \bm{t}_N]$ be the sequence of $N$ one-hot encoded tokens $\bm{t}_i \in \mathbb{R}^V$ associated with $U$, where $V$ is the dimension of the vocabulary.
%
Let $\mathcal{L}(\bm{t}_i)$ be the cross-entropy of the one-hot encoded token $\bm{t}_i$  and the estimated probability distribution $\sigma(\bm{z}_i) \in \mathbb{R}^V$, as follows,
%
\begin{equation}
 \mathcal{L}(\bm{U}) = \sum_{i=1}^N \mathcal{L}(\bm{t}_i) = \sum_{i=1}^N \bm{t}_i\log(\sigma(\bm{z}_i))\,,
\end{equation}
%
where we let $\bm{z}_i$ be the prediction of the $i$-th token $\bm{t}_i$ from the representations in the last layer of a Transformer model, following Definition \ref{def-transformer-model},
%
%Let the self-attention function in Definition~\ref{def-transformer-model} be linear, that is,  
%
% \begin{equation}
%     \bm{A}^l(\bm{X}_{l-1}) = \,\bm{Q}^l {\bm{K}^l}^T
% \end{equation}
%
% where we neglect the scaling factor $\sqrt{d}$ for simplicity.
% %
% The self-attention function thus maps the token embeddings $\{\bm{x}_i^{l-1}\}$ from layer $l-1$ to the embeddings $\{\hat{\bm{x}}^l_i\}$ as follows,
% %
% \begin{equation}
% {\bm{\hat{x}}^{l-1}_i}^\top = {\bm{x}^{l-1}_i}^\top + \sum_{j\in C_i} 
% \alpha^l_{ij}\,
% {\bm{x}^{l-1}_j}^\top \bm{W}^l_v \,,
% \end{equation}
% %
% where $C_i \subset \bm{U}$ is the subset of tokens that defines the conditional probability distribution.
%
\begin{equation}
\begin{cases}
%
{\bm{x}^0_i}^\top = \bm{t_i}^\top \bm{W_e} + \bm{W}_p \\[5pt]
\vspace{5pt}
{\bm{x}_i^l}^\top = \mathcal{F}_l(\bm{x}_i^{l-1})\quad \forall l \in [1, L] \\
\sigma\big(\bm{z}_i\big)  = \sigma\big({\bm{x}_i^L}^\top \bm{W_u}\big)\,,
%
\end{cases}
\label{eq-proof-gradient-bilinear-form-transformer-model}
\end{equation}
%
where ${\bm{x}^l_i}^\top = \mathcal{F}_l(\bm{x}_i^{l-1})$ is a short notation for the self-attention and multi-layered perception transformation of the $l$-th layer,
%
\begin{equation}
\mathcal{F}_l(\bm{x}_i^{l-1}) =
\left\{
\begin{aligned}
& \hat{\bm{x}}_i^{l^{\top}} = \hat{\bm{x}}_i^{{l-1}^{\top}} + a_l(\bm{x}_i^{l-1}) \\
& \bm{x}_i^{l^{\top}} = \hat{\bm{x}}_i^{l^{\top}} + m_l(\hat{\bm{x}}^l_i)
\end{aligned}
\right.\,,
\end{equation}
%
where the self-attention function is given by
%
\begin{equation}
    a_l(\bm{x}_i^{l-1}) = \sum_{j\in C_i} 
\alpha^l_{ij}(w^l)\,
{\bm{x}^{l-1}_j}^\top \bm{W}^l_v \,.
\end{equation}
%
Let attention coefficients $\alpha^l_{ij} \equiv \alpha^l_{ij} (w^l)$ of the $l$-th layer be parameterized with a general parameter $w^l$.
%
% We divide the derivation of the gradient of $\mathcal{L}(\bm{t}_i)$ w.r.t. the self-attention function into two steps.
%
Let the gradient of $\mathcal{L}(\bm{t}_i)$ w.r.t. $w_l$ (the parameterization of the attention scores) be factorized as follows,
%
\begin{equation}
    \nabla_{w^l} \mathcal{L}(\bm{t}_i) = \frac{\partial \mathcal{L}(\bm{t}_i)}{\partial \alpha^l_{ij}} \frac{\partial \alpha^l_{ij}}{\partial w^l}\,.
\end{equation}
%
It follows that,
%
\begin{equation}
\begin{split}
    \frac{\partial \mathcal{L}(\bm{t}_i)}{\partial \alpha^l_{ij}} 
    & = \frac{\partial \mathcal{L}(\bm{t}_i)}{\partial \bm{z}_i} \, \frac{\partial \bm{z}_i}{\partial \bm{x}^L_i} \, \frac{\partial \bm{x}^L_i}{\partial \hat{\bm{x}}^l_i}\,\frac{\partial \hat{\bm{x}}^l_i} {\partial \alpha^l_{ij}} 
    \\
    & = (\bm{t}_i - \sigma(\bm{z}_i))^\top \bm{W}_u^\top \, \frac{\partial \bm{x}^L_i}{\partial \hat{\bm{x}}^l_i}\,{\bm{W}^l_v}^\top \sum_{j\in C_i} \bm{x}^{l-1}_j \,,
\end{split}
\end{equation}
%
where the term $\partial \bm{x}_i^L / \partial \hat{\bm{x}}_i^l$ includes the set of partial derivatives that define the gradient of the representation $\bm{x}_i^L $ at the last layer w.r.t. the self-attention representation $\hat{\bm{x}}_i^l$ at the $l$-layer, as follows,
%
\begin{equation}
    \frac{\partial \bm{x}_i^L}{\partial \hat{\bm{x}}_i^l} = \left(1 + \sum_{m = l}^{L-1}\mathcal{F}_l'(\bm{x}_i^m)\right) \left(1 + m_l'(\hat{\bm{x}}^l_i)\right)\,,
\end{equation}
%
where 
%
\begin{equation}
    \mathcal{F}_l'(\bm{x}_i^m) = \frac{\partial}{\partial \bm{x}_i^l}\mathcal{F}_l(\bm{x}_i^m) \quad ; \quad m_l'(\hat{\bm{x}}^l_i) = \frac{\partial}{\partial \bm{\hat{x}}_i^l} m_l(\hat{\bm{x}}^l_i) \,.
\end{equation}
%
Let $\bm{\delta}^l_i$ be the error at the last layer propagated to the self-attention function at the $l$-th layer, as follows, 
%
\begin{equation}
    {\bm{\delta}^l_i}^\top = (\bm{t}_i - \sigma(\bm{z}_i))^\top \bm{W}_u^\top \, \frac{\partial \bm{x}^L_i}{\partial \hat{\bm{x}}^l_i} \,
     {\bm{W}^l_v}^\top \,,
\end{equation}
%
thus obtaining the following equation for the gradient,
%
\begin{equation}
     \nabla_{w^l} \mathcal{L}(\bm{t}_i) = {\bm{\delta}^l_i}^\top  \sum_{j\in C_i} \bm{x}^{l-1}_j  \frac{\partial \alpha^l_{ij}}{\partial w^l} \,.
\end{equation}
%
Let the attention scores be computed without the row-wise softmax operation and explicitly with the bilinear form $\bm{W}_{qk}$, such that the following expression gives the score between the $i$-th and $j$th token,
%
\begin{equation}
    \alpha^l_{ij} \equiv \alpha^l_{ij}(\bm{W}_{qk}^l) = {\bm{x}^{l-1}_i}^\top \bm{W}_{qk}^l \bm{x}^{l-1}_j \,,
\end{equation}
%
from which we obtain
%
\begin{equation}
    \frac{\partial \alpha^l_{ij}}{\partial \bm{W}_{qk}^l} = \bm{x}^{l-1}_i{\bm{x}^{l-1}_j}^\top = \bm{K}^{l-1}_{ij}\, ,
\end{equation}
%
where $\bm{K}^{l-1}_{ij} \in \mathbb{M}_n$ is a square rank-1 matrix given by the outer product between the $i$-th and $j$-th token from the $l-1$-th layer.
%
It follows that the total gradient of $\mathcal{L}(\bm{t}_i)$ is given by
%
\begin{equation}
    \nabla_{\bm{W}_{qk}^l} \mathcal{L}(\bm{t}_i) = {\bm{\delta}^l_i}^\top \sum_{j\in C_i} \bm{x}^{l-1}_j \bm{K}^{l-1}_{ij} \,,
\end{equation}
%
where we notice that, for every $j$, the term ${\bm{\delta}^l_i}^\top \bm{x}^{l-1}_j$ is a scalar quantity that we define as $\beta^l_{ij}$, thus obtaining,
%
\begin{equation}
\label{eq:math:gradient-linear-combination-rank-1-matrices}
    \nabla_{\bm{W}_{qk}^l} \mathcal{L}(\bm{t}_i) = \sum_{j\in C_i} \beta^l_{ij} \bm{K}^{l-1}_{ij} \,,
\end{equation}
%
and therefore,
%
\begin{equation}
    \nabla_{\bm{W}_{qk}^l} \mathcal{L}(U) = \sum_i \sum_{j\in C_i} \beta^l_{ij} \bm{K}^{l-1}_{ij} \quad \forall\, \bm{W}_{qk}^l, \,l \in [1,L]\,.
\end{equation}
%
Equivalently, we can rewrite the double summation as follows,
%
\begin{equation}
    \nabla_{\bm{W}_{qk}^l} \mathcal{L}(U) = \sum_{i \in P_j} \sum_{j} \beta^l_{ij} \bm{K}^{l-1}_{ij} \quad \forall\, \bm{W}_{qk}^l, \,l \in [1,L]\,,
\end{equation}
%
where $P_j \subset [0,1,\dots, N]$ is the set of indices defining the set of tokens $\{t_i\}$ that are predicted by $t_j$, thus concluding the proof.
%
\end{proof}
