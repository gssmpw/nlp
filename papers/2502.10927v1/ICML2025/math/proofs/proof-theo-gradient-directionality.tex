\begin{proof}
%


Let $\mathbb{E}[\mu_c(t^*)]$ be the expected number of tokens that are predicted by a given token $t^*$, and $\mathbb{E}[\mu_p(t^*)]$ the expected number of tokens that predict a given token $t^*$.
%
When assuming autoregressive training, it follows from Proposition \ref{prop-counting-prediction-context} that
%
\begin{equation}
    \frac{\mathbb{E}[\mu_c(t^*)]}{\mathbb{E}[\mu_p(t^*)]} = \frac{\sum_{j=1}^N (N-j)\Pr[t_j = t^*]}{\sum_{j=1}^N(j-1)\Pr[t_j = t^*]} \,.
\end{equation}
%
Let the statistical correlation between tokens in $U$,
%
\begin{equation}
\frac{\mathbb{E}[\mu_c(t^*)]}{\mathbb{E}[\mu_p(t^*)]} > 1\,, 
\end{equation}
%
for most $t^* \in \mathcal{V}$.
%
It follows that the average number of tokens that are predicted by $t^*$ is smaller than the average number of tokens that predict it.
%
Therefore, the number of times $t^*$ contributes to the weight update of $\bm{W}_{qk}$ as predicted token is on average smaller than the number of times it contributes as context token.
%
It follows from Proposition \ref{prop-gradient-asymmetric-growth-rows-columns} that the number of times the following is true,
%
\begin{equation}
\frac{\mathbb{E}\left[ ||\Delta \bm{w}_{\cdot, k}||^2 \right]}{\mathbb{E}\left[ ||\Delta \bm{w}_{m, \cdot}||^2 \right]} > 1 \,,
\end{equation}
%
is on average higher than the number of times the following is true,
%
\begin{equation}
\label{eq:varianceGrowthColsVsRows}
\frac{\mathbb{E}\left[ ||\Delta \bm{w}_{\cdot, k}||^2 \right]}{\mathbb{E}\left[ ||\Delta \bm{w}_{m, \cdot}||^2 \right]} < 1 \,.
\end{equation}
%
Therefore, the net increase of the norm of the columns is bigger than the net increase of the norm of the rows. 
%
We now prove that the number of columns with a high norm exceeds the number of rows with a high norm.
%
At initialization, let each $(k,m)$ entry of $\bm{W}_{qk}$ be an i.i.d. random variable drawn from a probability distribution with mean $\mu$ and variance $\sigma^2$.
%
It follows that the mean norm of rows and columns are equal at initialization, as follows,
%
\begin{equation}
\mathbb{E} \left[|\bm{w}_{m, \cdot}|\right]\bigg|_{\text{init}} = \mathbb{E} \left[|\bm{w}_{\cdot, k}|\right]\bigg|_{\text{init}} = \mu \,\, \forall k,m\,.
\end{equation}
%
Furthermore, the variance of the norms of the rows $\sigma_\text{rows}^2$ and of the columns $\sigma_\text{cols}^2$ are equal at initialization, 
%
\begin{equation}
    \text{Var}\left(|\bm{w}_{m, \cdot}|\right)\bigg|_{\text{init}} = \text{Var}\left(|\bm{w}_{\cdot, k, \cdot}|\right)\bigg|_{\text{init}} =\sigma^2 \,\, \forall k,m\,.
\end{equation}
%
Therefore, the following inequality holds at the end of training,
%
\begin{equation}
    \text{Var}\left(|\bm{w}_{m, \cdot}|\right) > \text{Var}\left(|\bm{w}_{ \cdot, k}|\right) \,\, \forall k,m \,.
\end{equation}
%
Finally, we obtain from Lemma~\ref{lemma-same_mean_different_variance_bound} that the following inequality holds,
%
\begin{equation}
    \text{Pr}[||\bm{w}_{\cdot, k}|| > w] > \text{Pr}[|| \bm{w}_{m, \cdot}|| > w] \,\,\ \forall \,w > \gamma
\end{equation}
%
where $\gamma = \sqrt{\text{Var}\left(|\bm{w}_{m, \cdot}|\right)\text{Var}\left(|\bm{w}_{\cdot, k}|\right)}-\mu $, thus concluding the proof.
%
%
% It follows that the mean absolute value of both rows $\mu_\text{rows}$ and columns $\mu_\text{cols}$ are equal, that is, 
% \begin{equation}
%     \mu_{\text{cols}} = \mathbb{E}_r\left[\mathbb{E}_c\left[ |\bm{w}_{c,r}| \right]\right]
%     =
%     \mathbb{E}_c\left[\mathbb{E}_r\left[ |\bm{w}_{c,r}| \right]\right] = \mu_{\text{rows}}\,,
% \end{equation}
% where $r,c$ indicate rows and columns, respectively.
%
\end{proof}