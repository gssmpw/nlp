\begin{proof}
%
Let $\mathcal{V} = [t_0, \dots, t_V]$ be a set of tokens.
%
Let $\mathcal{U}$ be the sample space of all possible sequences of $N$ tokens, and let $U$ be a sequence $U = [t_1, \dots, t_N] \in\mathcal{U}$.
%
Let $E = \{U \in \mathcal{U}: t_k = t^*\}$ the event of all sequences where the $k$-th token is $t^*$. 
%
The indicator function $\mathds{1}\{E\}(U)$ is a random variable defined as
%
\begin{equation}
\mathds{1}\{E\}(U) =
\begin{cases}
    1 \,\,\text{if} \,\,t_k = t^*\\
    0 \,\, \text{otherwise} \,,
\end{cases}
\end{equation}
%
and as such its expected value over $\mathcal{U}$ is
%
\begin{equation}
\mathbb{E}\left[\mathds{1}\{t_k = t^*\}(U)\right] = \sum_{U \in \,\mathcal{U}} \mathds{1}\{t_k = t^*\}(U)\Pr[U] = \sum_{U \in \mathcal{U} \,:\, w_j = t^*} \Pr[U] = \Pr[t_k = t^*] \,.
\end{equation}
%  
Let $\mu_c(t^*)$ be the random variable quantifying the number of tokens predicted by $t^*$, while $\mu_p(t^*)$ is the random variable quantifying the number of tokens predicted by $t^*$.
%
We analyzed the case of autoregressive and bidirectional training separately.
%

During autoregressive training, each time the token $t^*$ appears at position $k$ it is used as context to predict $N-k$ tokens, and it is predicted by $k-1$ tokens.
%
It follows that $\mu_c(t^*)$ is given by 
%
\begin{equation}
\mu_c(t^*) = \sum_{l=2}^N\sum_{k=1}^{l-1} \mathds{1}\{t_k = t^*\} = \sum_{k=1}^N (N-k)\mathds{1}\{t_k = t^*\}\,,
\end{equation}
%
while $\mu_p(t^*)$ is given by 
%
\begin{equation}
\mu_p(t^*) = \sum_{l=1}^{N-1}\sum_{k=1}^{l-1} \mathds{1}\{t_l = t^*\} = \sum_{k=1}^N (k-1)\mathds{1}\{t_k = t^*\}\,.
\end{equation}
%
Therefore, the expected value of $\mu_c(t^*)$ over all possible sequences is
%
\begin{equation}
\mathbb{E}[\mu_c(t^*)] = \mathbb{E}\left[\sum_{k=0}^N (N-k)\mathds{1}\{t_k = t^*\}\right] =   \sum_{k=0}^N(N-k)\Pr[t_k = t^*]\,,
\end{equation}
%
the expected value of $\mu_p(t^*)$ is
%
\begin{equation}
\mathbb{E}[\mu_p(t^*)] = \mathbb{E}\left[\sum_{k=0}^N (k-1)\mathds{1}\{t_k = t^*\}\right] =   \sum_{k=0}^N(k-1)\Pr[t_k = t^*]\,,
\end{equation}
%
and their ratio is given by
%
\begin{equation}
    \frac{\mathbb{E}[\mu_c(t^*)]}{\mathbb{E}[\mu_p(t^*)]} = \frac{\sum_{k=1}^N (N-k)\Pr[t_k = t^*]}{\sum_{k=1}^N(k-1)\Pr[t_k = t^*]} \,.
\end{equation}
%

During bidirectional training, each time the token $t^*$ is masked at position $k$, it is predicted by $N-1$ tokens.
%
We assume that the token $t^*$ significantly contributes to the context of a masked token only if it is itself not masked.
%
During bidirectional training, whenever a token $t^*$ at position 
$k$ is masked, it is predicted using the other $N-1$ tokens in the sequence.
%
We assume that $t^*$ significantly contributes to the context for predicting the masked token only if
$t^*$ itself is not masked.
%
Additionally, we assume that the probability $\rho$ of masking any given position $k$ is the same for all positions and does not depend on the specific token $t_k$ at that position.
%
It follows that $\mu_c(t^*)$ is given by 
%
\begin{equation}
\mu_c(t^*) = \sum_{k=1}^N\mathds{1}\{t_k = t^*\} \mathds{1}\{\text{mask}(k) = 0\}\sum_{k' \neq k} \mathds{1}\{\text{mask}(k') = 1\}  \,,
\end{equation}
%
while $\mu_p(t^*)$ is given by 
%
\begin{equation}
\mu_p(t^*) = \sum_{k=1}^N\mathds{1}\{t_k = t^*\} \mathds{1}\{\text{mask}(k) = 1\}\sum_{k' \neq k} \mathds{1}\{\text{mask}(k') = 0\}\,.
\end{equation}
%
Therefore, their expected values are given by 
%
\begin{equation}
\begin{split}
\mathbb{E}[\mu_c(t^*)] 
& =  \mathbb{E} \left[\sum_{k=1}^N\mathds{1}\{t_k = t^*\} \mathds{1}\{\text{mask}(k) = 0\}\sum_{k' \neq k} \mathds{1}\{\text{mask}(k') = 1\} \right] \\
& = \sum_{k=1}^N \mathbb{E} \left[\mathds{1}\{t_k = t^*\} \mathds{1}\{\text{mask}(k) = 0\}\sum_{k' \neq k} \mathds{1}\{\text{mask}(k') = 1\} \right]\\
& = \sum_{k=1}^N \mathbb{E} \left[\mathds{1}\{t_k = t^*\} \mathds{1}\{\text{mask}(k) = 0\}\right]\sum_{k' \neq k} \left[\mathds{1}\{\text{mask}(k') = 1\} \right]\\
& = \sum_{k=1}^N \text{Pr}(t_k = t^*) \text{Pr}(\text{mask}(k) = 0)\sum_{k' \neq k} \text{Pr}(\text{mask}(k') = 1) \\
& = N\rho(N-1)(1 - \rho) \sum_{k=1}^N \text{Pr}(t_k = t^*)\,,
\end{split}
\end{equation}
%
and
%
\begin{equation}
\begin{split}
\mathbb{E}[\mu_p(t^*)] 
& =  \mathbb{E} \left[\sum_{k=1}^N\mathds{1}\{t_k = t^*\} \mathds{1}\{\text{mask}(k) = 1\}\sum_{k' \neq k} \mathds{1}\{\text{mask}(k') = 0\} \right] \\
& = N\rho(N-1)(1 - \rho)\sum_{k=1}^N \text{Pr}(t_k = t^*)  \,,
\end{split}
\end{equation}
%
and their ratio is
\begin{equation}
     \frac{\mathbb{E}[\mu_c(t^*)]}{\mathbb{E}[\mu_p(t^*)]} = 1 \,,
\end{equation}
%
thus concluding the proof.
%











% Assuming that $\text{Pr}[t_j = t^*] = p(t^*)$.
% %
% For autoregressive training, 
% %
% \begin{equation}
% \mathbb{E}[\mu_c(t^*)] = \mathbb{E}\left[\sum_j (N-j)\mathds{1}\{t_j = t^*\}\right] =   \sum_{j=0}^N(N-j)\Pr[t_j = t^*] =  \left(\sum_{j=0}^N(N-j)\right)p(t^*) = \frac{N(N-1)}{2}p(t^*)\,.
% \end{equation}
% %
% \begin{equation}
% \mathbb{E}[\mu_p(t^*)] = \mathbb{E}\left[\sum_j (j-1)\mathds{1}\{t_j = t^*\}\right] = \sum_{j=0}^N(j-1)\Pr[t_j = t^*] =  \left(\sum_{j=0}^N(j-1)\right)p(t^*)= \frac{N(N-1)}{2}p(t^*)\,.
% \end{equation}
% %
% \begin{equation}
%      \frac{\mathbb{E}[\mu_c(t^*)]}{\mathbb{E}[\mu_p(t^*)]} = 1 \,.
% \end{equation}
% %    
% %
% For bidirectional training, 
% %
% \begin{equation}
% \begin{split}
% \mathbb{E}[\mu_c(t^*)] 
% & =  \mathbb{E} \left[\sum_{j=1}^N\mathds{1}\{t_j = t^*\} \mathds{1}\{\text{mask}(j) = 0\}\sum_{j' \neq j} \mathds{1}\{\text{mask}(j') = 1\} \right] \\
% & = \sum_{j=1}^N \mathbb{E} \left[\mathds{1}\{t_j = t^*\} \mathds{1}\{\text{mask}(j) = 0\}\sum_{j' \neq j} \mathds{1}\{\text{mask}(j') = 1\} \right]\\
% & = \sum_{j=1}^N \mathbb{E} \left[\mathds{1}\{t_j = t^*\} \mathds{1}\{\text{mask}(j) = 0\}\right]\sum_{j' \neq j} \left[\mathds{1}\{\text{mask}(j') = 1\} \right]\\
% & = \sum_{j=1}^N \text{Pr}(t_j = t^*) \text{Pr}(\text{mask}(j) = 0)\sum_{j' \neq j} \text{Pr}(\text{mask}(j') = 1) \\
% & = p(t^*)N\rho(N-1)(1 - \rho)
% \end{split}
% \end{equation}
% %
% \begin{equation}
% \begin{split}
% \mathbb{E}[\mu_c(t^*)] 
% & =  \mathbb{E} \left[\sum_{j=1}^N\mathds{1}\{t_j = t^*\} \mathds{1}\{\text{mask}(j) = 1\}\sum_{j' \neq j} \mathds{1}\{\text{mask}(j') = 0\} \right] \\
% & = p(t^*)N\rho(N-1)(1 - \rho)
% \end{split}
% \end{equation}
% %
% \begin{equation}
%      \frac{\mathbb{E}[\mu_c(t^*)]}{\mathbb{E}[\mu_p(t^*)]} = 1 \,.
% \end{equation}
% %

% Relaxing the assumption on $\text{Pr}[t_j = t^*] = p(t^*)$.
% %
% For autoregressive training,
% %
% \begin{equation}
%     \frac{\mathbb{E}[\mu_c(t^*)]}{\mathbb{E}[\mu_p(t^*)]} = \frac{\sum_{j=1}^N (N-j)\Pr[t_j = t^*]}{\sum_{j=1}^N(j-1)\Pr[t_j = t^*]} \,,
% \end{equation}
% %
% For bidirectional training
% %
% \begin{equation}
%     \frac{\mathbb{E}[\mu_c(t^*)]}{\mathbb{E}[\mu_p(t^*)]} =  \frac{\rho(N-1)(1 - \rho)\sum_{j=1}^N\Pr[t_j = t^*]}{\rho(N-1)(1 - \rho)\sum_{j=1}^N\Pr[t_j = t^*]} = 1 \,.
% \end{equation}
% %
% thus concluding the proof.



% %
% % Let $\lambda_p(t^*)$ and $\lambda_c(t^*)$ be the number of times $t^*$ is predicted and used as context in a given sequence, respectively.
% % %
% % First, we derive the expected value of $\lambda_p(t^*)$ and $\lambda_c(t^*)$ for the case in which there is no correlation structure between tokens, $\Pr[t_j = t^*] = p(t^*) \,\, \forall t^* \in V$. 
% % %
% % %and as such $\Pr[t_1 = t^*_1, \dots, t_N = t^*_N] = \Pi_i p(t^*_i)$.
% % %
% % During autoregressive training, a token $t^*$ is predicted every time it appears in a given sequence, and it is used as context to predict every subsequent token.
% % %
% % The expected value of $\lambda_p(t^*)$ is thus given by,
% % %
% % \begin{equation}
% %     \langle \lambda_p(t^*) \rangle = \mathbb{E} \left[\sum_{j=0}^N \mathds{1}\{t_j = t^*\} \right] = \sum_{j=0}^N \Pr[t_j = t^*] = N p(t^*)\,,
% % \end{equation}
% % %
% % where $\mathds{1}\{t_j = t^*\}$ be the indicator function for the event $t_j = t^*$.
% % %
% % The expected value of $\lambda_c(t^*)$ is,
% % %
% % \begin{equation}
% %     \langle \lambda_c(t^*) \rangle = \mathbb{E} \left[\sum_{j'=1}^N \sum_{j=0}^{j'}\mathds{1}\{t_j = t^*\} \right] = \sum_{j=0}^N(N-j)p(t^*) = \frac{N(N-1)}{2}p(t^*)\,.
% % \end{equation}
% % %
% % The ratio of $\langle \lambda_c(t^*) \rangle$ and $\langle \lambda_p(t^*) \rangle$ is thus given by
% % %
% % \begin{equation}
% %     \frac{\langle \lambda_c(t^*) \rangle}{\langle \lambda_p(t^*) \rangle} = \frac{N-1}{2} \,,
% % \end{equation}
% % %
% % which does not depend on $p(t^*)$.
% % %
% % During bidirectional training, a token $t^*$ is predicted every time it appears in a position masked with probability $\rho$, and it is used as context to predict every masked token in the sequence.
% % %
% % The expected $\langle \lambda_p(t^*) \rangle$ is thus given by,
% % %
% % \begin{equation}
% %     \langle \lambda_p(t^*) \rangle = 
% %     \mathbb{E} \left[\sum_{j=1}^N\mathds{1}\{t_j = t^*\} \mathds{1}\{\text{mask}(j) = 1\} \right] = \sum_{j=1}^N\Pr[t_j = t^*]\Pr[\text{mask}(j) = 1] = \rho N p(t^*)\,
% % \end{equation}
% % %
% % while $\langle \lambda_c(t^*) \rangle$ is given by
% % %
% % \begin{equation}
% % \begin{split}
% %     \langle \lambda_c(t^*) \rangle 
% %     & = 
% %     \mathbb{E} \left[\sum_{j=1}^N\mathds{1}\{t_j = t^*\} \mathds{1}\{\text{mask}(j) = 0\}\sum_{j' \neq j} \mathds{1}\{\text{mask}(j') = 1\} \right] 
% %     \\
% %     & = (1-\rho)Np(t^*)(N-1)\rho \,,
% % \end{split}
% % \end{equation}
% % %
% % where we assume independence between random masking (the presence or absence of a mask at $j$ is  independent of masks at the other positions $\{1,\dots,N\} \ j$), thus obtaining
% % %
% % \begin{equation}
% %     \frac{\langle \lambda_c(t^*) \rangle}{\langle \lambda_p(t^*) \rangle} = (N-1)(1 - \rho)\,,
% % \end{equation}
% % %
% % which also does not depend on $p(t^*)$.
% % %
% % We now generalize this to the case where there are correlation structures between tokens, $\Pr[t_j = t^*] \neq p(t^*)$.
% % %
% % This is the case when (1) there are correlations between tokens such as bigrams, trigrams, etc (these correlations can cause certain tokens to appear in contexts that precede more tokens or fewer tokens), (2) if data is chunked into sequences of different lengths, or if some tokens appear more often in the shorter and longer segment, and (3) there are correlations in higher-level structures such as paragraphs, sections, and documents.
% % %
% % For example, 
% % %
% % For unidirectional training, $\langle \lambda_p(t^*) \rangle$ is given by
% % %
% % \begin{equation}
% %     \langle \lambda_p(t^*) \rangle = \sum_{j=0}^N \Pr[t_j = t^*]\,,
% % \end{equation}
% % %
% % while $\langle \lambda_c(t^*) \rangle$ is given by 
% % %
% % \begin{equation}
% %     \langle \lambda_c(t^*) \rangle = \sum_{j=0}^N (N-j)\Pr[t_j = t^*]\,.
% % \end{equation}
% % %
% % The ratio is thus given by 
% % %
% % \begin{equation}
% %     \frac{\langle \lambda_c(t^*) \rangle}{\langle \lambda_p(t^*) \rangle} = \frac{\sum_{j=0}^N (N-j)\Pr[t_j = t^*]}{\sum_{j=0}^N \Pr[t_j = t^*]} \,.
% % \end{equation}
% % %
% % On the other hand, bidirectional training leads to
% % %
% % \begin{equation}
% %     \langle \lambda_p(t^*) \rangle = \rho \sum_{j=0}^N\Pr[t_j = t^*]\,,
% % \end{equation}
% % %
% % and
% % %
% % \begin{equation}
% %     \langle \lambda_c(t^*) \rangle = (N-1)(1 - \rho)\rho\sum_{j=0}^N \Pr[t_j = t^*]\,,
% % \end{equation}
% % %
% % where the ratio is given by
% % %
% % \begin{equation}
% %     \frac{\langle \lambda_c(t^*) \rangle}{\langle \lambda_p(t^*) \rangle} = \frac{(N-1)(1 - \rho)\rho\sum_{j=0}^N \Pr[t_j = t^*]}{\rho \sum_{j=0}^N\Pr[t_j = t^*]} = (1 - \rho)(N-1)\,,
% % \end{equation}
% % %
% % which is independent of $\Pr[t_j = t^*]$, thus concluding the proof.
% % %
\end{proof}