\begin{proof}
%
Let $U = [t_0, \dots, t_N]$ a sequence of tokens, and let $t^*$ be predicted by every token $t_i \in U$. 
%
Let $\{\bm{x}_i\}$ be the set of token embeddings associated with $U$ and let $\bm{y}$ be the token embedding of $t^*$.
%
It follows from Proposition \ref{prop-gradient-columns-rows} that the squared norm of the weight update of the $k$-th column is given by 
%
\begin{equation}
\Delta \bm{w}_{\cdot, k} = \sum_{i = \{t^*\}}\sum_{j\in C_i} [\bm{x}_j]_k\bm{x}_i= \sum_{j=1}^N [\bm{x}_j]_k\bm{y} 
%
\,\,\Rightarrow \,\,
%
||\Delta \bm{w}_{\cdot, k}||^2 = \big(\sum_{i=1}^N[\bm{x}_j]_k\big)^2 ||  \bm{y}||^2 \,,
\end{equation}
%
while the weight update of the $m$-th row follows
%
\begin{equation}
\Delta \bm{w}_{m, \cdot} = \sum_{i = \{t^*\}}\sum_{j\in C_i} \bm{x}_j[\bm{x_i}]_m  = \sum_{j=1} ^N\bm{x}_j[\bm{y}]_m 
\,\,\Rightarrow \,\,
||\Delta \bm{w}_{m, \cdot}||^2 = [\bm{y}]^2_m || \sum_{i=1}^N \bm{x}_j||^2 \,.
\end{equation}
%
Therefore, following the same arguments as in Proposition \ref{supp-math-prop-gradient-asymmetric-growth-rows-columns}, the ratio of the expected value of the square norms is given by
%
\begin{equation}
\frac{\mathbb{E}\left[ ||\Delta \bm{w}_{\cdot, k}||^2 \right]}{\mathbb{E}\left[ ||\Delta \bm{w}_{m, \cdot}||^2 \right]} = \frac{ \Sigma_{k,k}\text{Tr}(\Gamma)}{ \Gamma_{m,m}\text{Tr}(\Sigma)} = d\frac{\Sigma_{k,k}}{\text{Tr}(\Sigma)}  < \frac{d\,\Sigma_{k,k}}{d\,\Sigma_{k,k}} = 1 \quad \forall k \,\,\text{s.t.}\,\,\Sigma_{k,k} < \frac{\text{Tr}(\Sigma)}{d} \,,
\end{equation}
%
thus concluding the proof.
%
%
\end{proof}