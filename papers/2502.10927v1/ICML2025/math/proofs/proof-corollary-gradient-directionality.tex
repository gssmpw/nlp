\begin{proof}
%
It follows from Proposition \ref{prop-counting-prediction-context} that the number of times the following is true,
%
\begin{equation}
\frac{\mathbb{E}\left[ ||\Delta \bm{w}_{\cdot, k}||^2 \right]}{\mathbb{E}\left[ ||\Delta \bm{w}_{m, \cdot}||^2 \right]} > 1 \,,
\end{equation}
%
is on average equal to the number of times the following is true,
%
\begin{equation}
\frac{\mathbb{E}\left[ ||\Delta \bm{w}_{\cdot, k}||^2 \right]}{\mathbb{E}\left[ ||\Delta \bm{w}_{m, \cdot}||^2 \right]} < 1 \,.
\end{equation}
%
Therefore, the net increase of the norm of the columns is equal to the net increase of the norm of the rows.
%
Following the same initialization assumptions as in Theorem \ref{theo-gradient-directionality}, it follows that at the end of training,
%
\begin{equation}
    \text{Var}\left(|\bm{w}_{m, \cdot}|\right) =\text{Var}\left(|\bm{w}_{ \cdot, k}|\right) \,\, \forall k,m \,.
\end{equation}
%
thus concluding the proof.
%
\end{proof}