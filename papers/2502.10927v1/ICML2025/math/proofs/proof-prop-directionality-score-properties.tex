\begin{proof}
%
To prove that the directionality score is bounded in the interval $[-1,1]$, note that $\bar{c}_{\mathbf{M}}, \bar{r}_{\mathbf{M}}>0$ simply because they are sums of norms. As both are positive,
\begin{equation}
    |\bar{c}_{\mathbf{M}}- \bar{r}_{\mathbf{M}}|
    < \bar{c}_{\mathbf{M}}
    < \bar{c}_{\mathbf{M}}+ \bar{r}_{\mathbf{M}}
\end{equation}
and thus 
\begin{equation}
    \dfrac{\bar{c}_{\mathbf{M}}- \bar{r}_{\mathbf{M}}}{\bar{c}_{\mathbf{M}}+ \bar{r}_{\mathbf{M}}} <1
\end{equation}
and taking the negative sign for the absolute value,
\begin{equation}
   - \dfrac{\bar{r}_{\mathbf{M}}- \bar{c}_{\mathbf{M}}}{\bar{c}_{\mathbf{M}}+ \bar{r}_{\mathbf{M}}} > -1
\Rightarrow   \dfrac{\bar{c}_{\mathbf{M}}- \bar{r}_{\mathbf{M}}}{\bar{c}_{\mathbf{M}}+ \bar{r}_{\mathbf{M}}} > -1.
\end{equation}

In the extremes $d=\pm1$, the numerator must be equal to the denominator in absolute value, implying that either $\bar{r}_{\mathbf{M}}$ or $\bar{c}_{\mathbf{M}}$ are zero and the other is positive. For completeness, we define the score as zero if both are zero.

Finally, we study the case of a random matrix. We start by noting that the values of $\bar{r}_{\mathbf{M}},\ \bar{c}_{\mathbf{M}}$ are interchanged when we take the transpose, hence
\begin{equation}\label{eq:rowColTranspose_direction}
    \bar{r}_{\mathbf{M}} =  \bar{c}_{\mathbf{M}^\top}
\end{equation}
Regardless of the scaling of the matrix and the value $\gamma$, the key property of a random matrix is that all entries are drawn from the same distribution. Hence,
 \begin{equation}
     \text{Pr}\left[\bm{M}_{ij} = x\right] = 
     \text{Pr}\left[\bm{M}_{ji} = x\right]
     \Rightarrow
      \text{Pr}\left[\bm{M} = \bm{X}\right] 
      = \text{Pr}\left[\bm{M} = \bm{X}^\top\right] 
 \end{equation}
 for $x$ and $\bm{X}$ being any arbitrary value or matrix. As a consequence,
 \begin{equation}
     \text{Pr}\left[\bar{c}_{\mathbf{M}} = x\right]
     = \text{Pr}\left[\bar{c}_{\mathbf{M}^\top} = x\right] 
     = \text{Pr}\left[\bar{r}_{\mathbf{M}} = x\right]
 \end{equation}
where the last equality comes from Eq.~\ref{eq:rowColTranspose_direction}. The main point here is that the probability distribution of both rows and columns is the same. Pushing this forward, the expected value of $\bar{r}_{\mathbf{M}}- \bar{c}_{\mathbf{M}}$ is
\begin{align}
    \text{E}\left[\bar{r}_{\mathbf{M}}- \bar{c}_{\mathbf{M}}\right]
   & = \text{E}\left[\bar{r}_{\mathbf{M}}\right]-\text{E}\left[\bar{c}_{\mathbf{M}}\right]
    = \int \bar{c}_{\mathbf{M}} \text{Pr}\left[\bm{M}\right] d\bm{M}
    - \int \bar{r}_{\mathbf{M}} \text{Pr}\left[\bm{M}\right] d\bm{M}\\
     &= \int \bar{r}_{\mathbf{M}} \text{Pr}\left[\bm{M}^\top\right] d\bm{M}^\top
    - \int \bar{r}_{\mathbf{M}} \text{Pr}\left[\bm{M}\right] d\bm{M}
    = 0
\end{align}
Furthermore, the expected value of $\bar{r}_{\mathbf{M}}+ \bar{c}_{\mathbf{M}}$ is strictly positive, since both values are positive. Thus, their ratio, the directionality score of a random matrix, is zero. 

Notice that to be thorough we must show that their variance is bounded scales down. Since weight initialization has been extensively studied, we will just make a general reference to it here. In machine learning, all weights are initialized with zero mean and variances that scale as $O(n^{-1})$. As $\bm{M}$ is a product of two matrices with such scaling, each entry would consist of the sum of $n$ random variables, where each one has a scaling of $O(n^{-2})$ since it is the product of two random variables with an $O(n^{-1})$ scaling. Thus, the entries of  $\bm{M}$ also have a scaling of  $O(n^{-1})$. Applying the mean value theorem gives us the desired result.


\end{proof}