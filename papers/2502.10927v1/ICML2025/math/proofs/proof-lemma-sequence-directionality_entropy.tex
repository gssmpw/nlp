\begin{proof}
%
First the covariance. If there is a non-zero mutual information between two symbols,
\begin{equation}
    H(\bm{s}_j, \bm{s}_i) 
    = H(\bm{s}_j)  + H(\bm{s}_j)  - I(\bm{s}_j, \bm{s}_i) 
    <  H(\bm{s}_j)  + H(\bm{s}_j).
\end{equation}
If there is a covariance between pairs of symbols, their joint entropy is lower than the sum of their entropies, and hence $I(\bm{s}_j, \bm{s}_i) >0$.

Now the predictability. For a given pair of indexes $i,j$ with $i<j$, if $H(\bm{s}_j)  = H(\bm{s}_j)$
\begin{equation}
    H(\bm{s}_j| \bm{s}_i)  =  H(\bm{s}_j) - I(\bm{s}_j, \bm{s}_i) <   H(\bm{s}_j) =  H(\bm{s}_i)  
\end{equation}
Clearly, this only holds true because $\bm{s}_j$ is conditioned on $\bm{s_i}$. This is the case for the autoregressive setting, where $i$ comes before $j$.

When we write this in terms of specific symbols, it means that the probability of any specific 

If the entropy of each index is the same in isolation, but there is mutual information between the entries in the indexes, then there is some shared information, and 
% 
\end{proof}