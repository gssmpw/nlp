In this section, we validate our theoretical findings by quantifying empirically the degree of symmetry and directionality in different families of open-source Transformer models.
%
To do so, we define two scores for symmetry and directionality in square matrices.
%
First, we define the symmetry score $s \in \mathbb{R}$ as follows,
%
\input{ICML2025/math/statements/def-symmetry-score}
%
Here, positive and negative symmetry scores indicate the presence of symmetric and skew-symmetric structures, respectively (see Appendix \ref{supp-math-symmetry-score}).
%
Second, we define the directionality score $d \in \mathbb{R}$ as follows,
%
\input{ICML2025/math/statements/def-directionality-score}
%
Here, positive and negative directionality scores indicate the dominance of high norm rows or columns, respectively (see Appendix \ref{supp-math-directionality-score}).
%
Finally, we compute the matrix $\bm{W}_{qk}$ for every layer, calculate the median symmetry and directionality score across layers, and analyze the differences between encoder- and decoder-only variants.

We find that encoder-only models remarkably show a higher degree of symmetry than decoder-only (Figure \ref{fig-pretrained-models}a).
%
This difference is consistent across multiple families of models and input modalities, such as BERT \citep{devlinBERTPretrainingDeep2019}, GPT \citep{radfordImprovingLanguageUnderstanding2018, radfordLanguageModelsAre2019}, LLAMA3 \citep{touvronLLaMAOpenEfficient2023}, Phi \citep{hughesPhi2SurprisingPower2023,abdinPhi3TechnicalReport2024}, MISTRAL \citep{jiangMistral7B2023}, ModernBERT \citep{warnerSmarterBetterFaster2024}, and many others (see Figure \ref{suppfig-pretrained-models} for vision and audio models).
%
Strikingly, we observe that decoder-only models have higher degrees of directionality than encoder-only models (Figure \ref{fig-pretrained-models}b).
%
Again, this difference is consistent across all the models and input modalities we consider. 
%
We show in Figure \ref{suppfig-encoder-decoder-models} that a similar pattern is observed when including full encoder-decoder Transformers (e.g. the language T5 models \citep{xueMT5MassivelyMultilingual2021}), despite these models having an overall lower degree of directionality.