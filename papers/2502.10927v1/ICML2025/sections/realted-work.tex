% Related work -> new version for main text instead of appendix
%
\paragraph{Mechanistic Interpretability (MI)}
%
In contrast to interpretability approaches that focus on explaining specific data instances by analyzing features \citep{Wu_Chen_2020, Lundstrom_Huang_2022}, attention scores \citep{Hoover_Strobelt_2020, Barkan_2021, Yeh_Chen_2023}, or output variations \citep{Jin_Jin_2020, Wang_Xu_2022}, mechanistic interpretability (MI) seeks to provide a more general understanding of Transformer models.
%
MI is based on the study of ``circuits,'' analyzing the between activations across different components of a Transformer  \citep{olahZoomIntroductionCircuits2020}.
%
Following the categorization by \citep{Rai_Zhou_Feng_Saparov_Yao_2024}, MI techniques include:
(i)
The \emph{logit lens} \citep{nostalgebraist_2020, Geva_Schuster_2021} projects layer activations or weights into the vocabulary space $\mathcal{V}$, allowing the derivation of logits and revealing the influence of individual components on the prediction.
This technique can also be applied to query-key matrices $\bm{W}_{qk}$ to study how attention heads transform source into target tokens \citep{darAnalyzingTransformersEmbedding2023};
%
(ii)
\emph{Probing techniques} allow to identify correlations between layer activations and features by training a linear classifier or shallow neural network (the probe) to predict the presence of a feature in layer activations \citep{Dalvi_Durrani_2019, gurnee2023finding};
%
(iii)
\emph{Sparse autoencoders} map activations into a higher-dimensional yet sparse representation, facilitating the identification of independent (monosemantic) features \citep{Huben_Cunningham_Smith_Ewart_Sharkey_2024, bricken2023monosemanticity};
%
(iv)
\emph{Visualization techniques} facilitate the analysis of attention scores \citep{olssonIncontextLearningInduction2022, Lieberum_Rahtz_Kram√°r_Nanda_Irving_Shah_Mikulik_2023} and neuronal activity patterns \citep{Elhage_Softmax_2023} by rendering them in a graphical format. However, their utility often depends on human comprehension of the resulting visualizations;
%
(v)
\emph{Ablation studies} assess the importance of model components by systematically removing or modifying them and observing the resulting behavioral changes \citep{olssonIncontextLearningInduction2022, wangInterpretabilityWildCircuit2022};
%
(vi)
\emph{Causal mediation analysis (CMA)} analyzes the importance of components \citep{Vig_Gehrmann_Belinkov_Qian_Nevo_Singer_Shieber_2020, Meng_Bau_Andonian_Belinkov_2022} or connections \citep{wangInterpretabilityWildCircuit2022, Goldowsky-Dill_MacLeod_Sato_Arora_2023} between them by introducing perturbations (e.g., noise) and selectively patching them to measure the recovery of lost capabilities.
%
% What we do differently
Our work adds a new perspective on MI by providing a scalable and generalizable approach to the mechanistic understanding of self-attention.
In contrast to existing work, it is not limited to analyzing fully trained models but investigates the influence of learning and can analyze models of different sizes across all modalities.


\paragraph{MI for model enhancement}

Insights from MI have been instrumental in various applications, including improving model safety \citep{Belrose_Furman_Smith_Halawi_Ostrovsky_McKinney_Biderman_Steinhardt_2023}, updating the model's learned knowledge \citep{Meng_Bau_Andonian_Belinkov_2022}, and guiding the generation process of generative models \citep{Geva_Caciularu_2022}.
%
One of the most exciting applications is leveraging MI techniques to improve model performance.
For instance, \citep{Marks_Rager_Michaud_Belinkov_Bau_Mueller_2024} identify and eliminate spurious features, leading to improved generalization in classification tasks.
%
Similarly, \citep{trockman_mimetic_2023} observe that query-key matrices ($\bm{W}_{qk}$) frequently exhibit a pronounced negative diagonal, prompting them to initialize it with approximately the identity matrix, leading to enhanced accuracy in image classification tasks.
%
% What we do
Similar to \citep{trockman_mimetic_2023}, we demonstrate that findings about the structure of $\bm{W}_{qk}$  can inform initialization strategies that improve Transformer performance. However, since the identity matrix is one instance of a symmetric matrix, we consider the work by \citep{trockman_mimetic_2023} as a special instance of our broader approach, confirming our findings in the image domain.