%% (1) introduction
%
In this section, we introduce a novel framework that links the self-attention matrices $\bm{W}_{qk}$ to bilinear forms, enabling us to analyze how an objective function influences their structure.
%
This approach reveals fundamental patterns in $\bm{W}_{qk}$  that are not apparent when examining $\bm{W}_q$ and $\bm{W}_k$ separately.  
%
For example, we prove that autoregressive and bidirectional training induce directional and symmetric structures in the $\bm{W}_{qk}$ matrices.
%
In the following sections, we define and formalize these concepts.
%










%% (3) Interpreting self-attention with bilinear forms
\subsection{Interpreting self-attention with bilinear forms}
%
%The self-attention function defined in Equation \eqref{eq:results:self-attention-transformers} is 
%
Self-attention \citep{vaswaniAttentionAllYou2017,radfordLanguageModelsAre2019} is 
a type of score function $A: \mathbf{R}^{N,d} \times \mathbf{R}^{N,d} \rightarrow \mathbf{R}^{N,N}$ that maps a sequence of $N$ token embeddings with dimension $d$ into a matrix of attention scores.
%
Except for the row-wise softmax function $\sigma(\cdot)$, self-attention is a linear transformation of the embedded tokens.
%
In particular, 
%
\begin{equation}
\label{eq:results:self-attention-bilinear-form}
\begin{split}
    \bm{A}(\bm{X}) 
    & = \sigma\left(\frac{1}{\sqrt{d}}\,\hat{\bm{A}}(\bm{X})\right) \\
    & = \sigma\left(\frac{1}{\sqrt{d}}\,\bm{Q} \bm{K}^T\right) \\
    & = \sigma\left(\frac{1}{\sqrt{d}}\,\bm{X} \bm{W}_{qk}\bm{X}^T\right) \,,
\end{split}
\end{equation}
%s
where $\hat{\bm{A}}(\bm{X})$ is the linear part of self-attention (raw unscaled attention scores), $\bm{X} = [\bm{x}^\top_1, \dots, \bm{x}^\top_N] \in \mathbb{R}^{N,d} $ is the sequence of $N$ token embeddings $\bm{x}_i \in \mathbb{R}^d$,  and $\bm{W}_{qk} = \bm{W}_q\bm{W}_k^\top \in \mathbb{R}^{d,d}$.
%
This equation shows that the linear transformation $\bm{W}_q$ and $\bm{W}_k$ are always combined to compute attention scores with one single matrix $\bm{W}_{qk}$.
%
While the matrices $\bm{W}_q$ and $\bm{W}_k$ are defined separately for computational efficiency, this formulation remains mathematically equivalent \citep[see also][]{elhageMathematicalFrameworkTransformer2021, olssonIncontextLearningInduction2022,darAnalyzingTransformersEmbedding2023}.
%

We observe that $\bm{X} \bm{W}_{qk}\bm{X}^\top$ corresponds to a bilinear form (see Definition \ref{def-bilinear-form}). 
%
Specifically, the entry $\hat{\alpha}_{ij} = [\hat{\bm{A}}]_{ij}$ can be formulated in two equivalent ways: (1) as the canonical dot product between a query $\bm{q}_i$ and a key $\bm{k}_j$  (like in standard Transformer models), or (2) as the dot product between tokens $\bm{x}_i$ and $\bm{x}_j$ under the bilinear form $\bm{W}_{qk}$,
%
\begin{equation}
    \hat{\alpha}_{ij} = \langle \bm{q}_i, \,\bm{k}_j \rangle = \langle \bm{x}_i, \bm{W}_{qk} \bm{x}_j \rangle = \langle \bm{x}_i, \bm{x}_j \rangle_{\bm{W}_{qk}} \,.
\end{equation}
%
Intuitively, this indicates that $\bm{W}_q$ and $\bm{W}_k$ define an alternative metric $\bm{W}_{qk}$ in the embedding space, which quantifies the score of $\bm{x}_i$ and $\bm{x}_j$ without requiring explicit query and key vectors.
%
We formalize this intuition in the following,
%
\input{ICML2025/math/statements/prop-self-attention-bilinear-form}
%
A proof of this proposition and its generalization for multi-head attention is in Appendix \ref{supp-math-self-attention-bilinear-form}.
%
The order of the attention scores is thereby preserved from the raw attention scores to the final attention scores (after soft-max).

It follows that $\bm{W}_{qk}$ defines the geometric structure that encodes the relevance of the token embedding  $\bm{x}_j$ in predicting $\bm{x}_i$ through projections within the subspace (see Equation \eqref{eq-prop-self-attention-bilinear-form}). 
%
Therefore, token prediction depends on learning an optimal bilinear form $\bm{W}_{qk}$ to project tokens in the embedding space effectively.
%
We note that $\bm{W}_v$ is a linear transformation that is applied independently to each projection in the sum and thus does not influence our derivation.
%
In the following sections, we demonstrate how this equivalent formulation of self-attention provides a useful framework for analyzing the training of Transformer models.
%
Specifically, we show that the choice of the objective function such as autoregressive prediction \citep{radfordLanguageModelsAre2019} or bidirectional training \citep{devlinBERTPretrainingDeep2019} produces distinct structural patterns in $\bm{W}_{qk}$.
%





%% (4) the implicit weight updates of the bilinear form
\subsection{Deriving the gradients of self-attention with bilinear forms}
%
\input{ICML2025/figures/figure-illustration-one-column}
%
%
To show the connection between the objective function and the structures of self-attention matrices, we derive a convenient formulation for the weight update of $\bm{W}_{qk}$.
%
We first formulate a sequence modeling problem with self-supervised training as follows. 
%
Let $U = \{t_1, \dots, t_N\}$ be a  sequence of $N$ tokens from a vocabulary of dimension $V$.
%
Let $\mathcal{L}(U)$ be the negative log-likelihood of each token $t_i$, expressed as
%
\begin{equation}
    \mathcal{L}(U;\mathcal{W}) = \sum_i \mathcal{L}(t_i)=  - \sum_i \log p(t_i \,|\, \{t_j : j \in  \mathcal{C}_i\}; \mathcal{W}) \,,
\end{equation}
%
where $\mathcal{W}$ is the set of trainable parameters, and $\mathcal{C}_i \subset [0,1,\dots, N]$ is the set of indices defining the set of tokens $\{t_j\}$ of the conditional probability distribution.
%
This allows us to isolate the contribution of each token $t_j$ to the prediction of the target token $t_i$.
%
Here, we demonstrate that the updates to the matrix $\bm{W}_{qk}$ follow a structured pattern: the contribution of the token $t_j $ in the prediction of the embedding $t_i$ results in adding a rank-1 matrix $\bm{K}_{ij}$ to the matrix $\bm{W}_{qk}$.
%
As a result, the total weight update to $\bm{W}_{qk}$ is expressed as a linear combination of these rank-1 matrices.
%
We formalize this observation in the following proposition.
%
\input{ICML2025/math/statements/prop-gradient-self-attention}
%

We provide proof for this proposition with related remarks in Appendix \ref{supp-math-gradients-self-attention}, and an illustrative description of the forward and backward pass in Figure \ref{fig-illustration}.
%




\subsection{The relation between objective functions and structures in self-attention matrices}
%
Finally, we show how the formulation of $\Delta \bm{W}_{qk}$ enables the analysis of the contribution of any given token $t^*$ to the weight updates, how this affects the properties of $\bm{W}_{qk}$, and how to relate these properties to the specific objective function.
%
Indeed, Proposition~\ref{prop-gradients-self-attention} indicates that a token $t^*$ impacts the updates of $\bm{W}_{qk}$ differently when serving as context for predicting other tokens or being itself predicted.
%
When a token $t^*$ serves as context ($t_j = t^*$), the embeddings of all predicted tokens contribute to the column space of $\bm{W}_{qk}$, where the update of the $k$-th column $\bm{w}_{\cdot, k}$ is given by
%
\begin{equation}
    \Delta \bm{w}_{\cdot, k} \bigg|_{t_j = t^*}= [\bm{x}_{t^*}]_k \left(\sum_{i \in P_{t}}\beta_{ij}\bm{x}_i\right)\,.
\end{equation}
%
Only the embedding of $t^*$ is instead added to the row space, where the update of the $m$-th row $\Delta \bm{w}_{m, \cdot}$ is given by
%
\begin{equation}
    \Delta \bm{w}_{m, \cdot} \bigg|_{t_i = t^*} = \left(\sum_{i \in P_{t}}\beta_{ij}[\bm{x}_{i}]_k\right)\bm{x}_{t^*}\,.
\end{equation}
%
Intuitively, using $t^*$ as context increases the dimensionality of the column space proportionally to the embeddings of the predicted tokens, while reducing the row space along the direction of the embedding of $t^*$.
%
Conversely, when $t^*$ is being predicted ($t_i = t^*$), all token embeddings from the context are added to the row space of $\bm{W}_{qk}$, while only the embedding of $t^*$ is added to the column space.
%
Importantly, any token $t^*$ is used differently as context or as prediction depending on the training objective. 
%
Autoregressive training implicitly introduces directionality by predicting each token based solely on its preceding tokens. 
%
In contrast, bidirectional training uses tokens as context and as predictions symmetrically.
%
This fundamental difference between the two objective functions affects the weight updates of $\bm{W}_{qk}$, and in turn, the structures encoded in its columns and rows.
%
We describe this in the following two informal Theorems.
%
\input{ICML2025/math/statements/theo-informal-directionality}
%
\input{ICML2025/math/statements/theo-informal-symmetry}
%
We provide a formal definition and proof of these two Theorems, and the related Propositions and Lemmas in Appendix \ref{supp-math-directionality-symmetry}.
%
%
%
% %
% A first important implication of Proposition~\ref{prop-gradients-self-attention} is that any pair ($\bm{x}_i$, $\bm{x}_j$) has a distinct contribution in shaping the column space of $\bm{W}_{qk}$.
% %
% During the forward pass, $\bm{x}_j$ contributes to the prediction of $\bm{x}_i$ proportionally to how it aligns to it following the geometry of $\bm{W}_{qk}$ (Figure \ref{fig-illustration}).
% %
% %This is a soft-gating mechanism represented by $\alpha_{ij}$ .
% %
% In particular, we can separate the contribution of the $k$-th column of $\bm{W}_{qk}$ (represented by the vector $\bm{w}_{\cdot, k}$) to the attention score, 
% %
% \begin{equation}
%     \alpha_{ij} = \sum_k (\bm{x}_i^\top\bm{w}_{\cdot, k} )[\bm{x}_j]_k \,.
% \label{eq:column-contribution-attention-score}
% \end{equation}
% %
% Intuitively, when $\bm{x}_i$ aligns with the $k$-th column $\bm{w}_{\cdot, k}$ and this corresponds to $\bm{x}_j$ having a strong component in that direction, then the $k$-th column contributes strongly to the attention score (Figure \ref{suppfig-illustration}a).
% %
% During the backward pass, the pair ($\bm{x}_i$, $\bm{x}_j$) contributes to the update of $\bm{W}_{qk}$ proportionally to the alignment between the error $\bm{\delta}_i$ and $\bm{x}_j$ (Figure \ref{fig-illustration}).
% %
% By separating the weight update to the $k$-th column $\bm{w}_{\cdot, k}$ we obtain,
% %
% \begin{equation}
%     \Delta \bm{w}_{\cdot, k} = \sum_i\sum_{j\in C_i} \Delta \bm{w}_{\cdot, k}\Big|_{\bm{t}_i \leftarrow \bm{t}_j}
%     = \sum_i\sum_{j\in C_i}
%      (\beta_{ij}[\bm{x}_j]_k)\,\bm{x}_i \,,
% \label{eq:column-weight-update}
% \end{equation}
% %
% where the column update given by the pair ($\bm{x}_i$, $\bm{x}_j$) consists of adding $\bm{x}_i$ to the $k$-th column, proportionally to the $k$-th element of $\bm{x}_j$ and to $\beta_{ij}$ (Figure \ref{suppfig-illustration}b).
% %
% Therefore, columns that align with $\bm{x}_i$ can either be positively reinforced to align more with $\bm{x}_i$ (when $\beta_{ij}$ and $[\bm{x}_j]_k$ have the same sign) or moved in the opposite direction of $\bm{x}_i$ (when $\beta_{ij}$ and $[\bm{x}_j]_k$ have opposite signs).
% %
% As such, incrementally updating through the gradient of $\mathcal{L}(\bm{U})$ encodes information in the column space of $\bm{W}_{qk}$.
% %

% A second implication of Proposition~\ref{prop-gradients-self-attention} is that the choice of $\mathcal{C}_i$ not only determines which $\bm{t}_j$ contributes to the prediction of $\bm{t}_i$ but also the updates of the matrix $\bm{W}_{qk}^l$ during gradient descent. 
% %
% Importantly, autoregressive and bidirectional training define different sets $\mathcal{C}_i$ and as such, result in different combinations of $\bm{K}_{ij}$ during weight updates.
% %
% \subsection{The relation between objective functions and structures in self-attention matrices}
% %
% % \input{ICML2025/figures/figure-illustration-matrices-one-column}
% %
% In the following, we show how this fundamental difference between the two objective functions affects the weight updates of $\bm{W}_{qk}$, and in turn, the structures encoded in its columns.
% %

% \textbf{Autoregressive training leads to directional weight updates}. 
% %
% First, we prove that autoregressive training induces $\bm{W}_{qk}$ matrices with few high-norm columns.
% %
% In autoregressive training, models are optimized to predict the next token in the sequence, thereby $\{\bm{t}_j \, : j < i\}$ \citep{radfordImprovingLanguageUnderstanding2018,radfordLanguageModelsAre2019}.
% % 
% As such, there is .
% %
% \ms{TO DO: add the Theorem for directional weight updates, proof in the appendix, and related text}
% %

% \input{ICML2025/figures/figure-pretrained-models-language}
% \textbf{Bidirectional training leads to symmetric weight updates.}
% %
% Next, we prove that bidirectional training induces symmetrical weight updates.
% %
% In this setting, models are optimized to predict a random subset of tokens $M \subset [0,1,\dots, N]$ given the entire sequence, such that $\mathcal{C}_i = [0,1,\dots, N]$ \citep{devlinBERTPretrainingDeep2019, warnerSmarterBetterFaster2024}.
% %
% To predict the $i$-th token from the subset $M$, we consider every token in the sequence, including another token, the $j$-th, which is also part of the subset $M$ to be predicted. 
% %
% Likewise, the prediction of the $j$-th token also considers the $i$-th token.
% %
% Therefore, for any pair of tokens $i$ and $j$ in $M$, the final weight update includes two complementary terms,
% %
% \begin{equation}
% \label{eq:results:complementary-terms}
% \begin{split}
% & \Delta \bm{W}_{qk}^l\big|_{\bm{t}_i \leftarrow \bm{t}_j} \propto \beta^l_{ij}\bm{K}^{l-1}_{ij} \\
% &  \Delta \bm{W}_{qk}^l\big|_{\bm{t}_j \leftarrow \bm{t}_i} \propto \beta^l_{ji}{\bm{K}^{l-1}_{ij}}^\top \,,
% \end{split}
% \end{equation}
% %
% that can be combined as one term $\Delta \bm{W}_{qk}^l\big|_{\bm{t}_i \leftrightarrow \bm{t}_j}$.
% %
% This is a key difference with the weight updates of decoder-only models, indicating that $\bm{x}^{l-1}_i$ and $\bm{x}^{l-1}_j$ are both stored in the column and row spaces of $\bm{W}_{qk}^l$ during a weight update.
% %
% We formulate this in the following Theorem.
% %
% \input{ICML2025/math/statements/theo-gradient-symmetry}
% %
% We provide a proof for this Theorem and its generalization for $i$ running over a random subset of tokens $M$ in Appendix \ref{supp-math-theo-symmetry-gradients}.
% %
% The symmetrical nature of bidirectional training implies that bidirectional training encodes similar information in the column and row spaces of $\bm{W}_{qk}$.%
% %
% The incremental addition of symmetric matrices to the randomly initialized $\bm{W}^l_{qk}$ leads to the emergence of symmetric structures.
% % 
