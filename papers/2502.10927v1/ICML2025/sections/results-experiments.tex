% % 
% In this section, we train Transformer models from scratch and perform a series of experiments to analyze how symmetric and directional structures develop during training across layers.
% %
% Next, we provide a first endeavor in leveraging these symmetric structures to achieve more efficient training of Transformer models on standard datasets.
% %
In this final section, we test if using structural priors based on our previous results can improve the pretraining of Transformer models. 
%
To do so, we train Transformer models from scratch and perform a series of experiments to analyze how symmetric and directional structures develop during training across layers.
%





\subsection{Evolution of symmetric and directional structures during learning}
\label{sec-evolution-structures}
%
\input{ICML2025/figures/figure-training-layers}
%
To test the applicability of our result, we first train 12-layer transformer models in both encoder and decoder modes and quantify the median symmetry and directionality scores across epochs.
%
At initialization, the symmetry and directionality score of the matrix $\bm{W}_{qk}$ at any layer is zero (see  Definition \ref{def-symmetry-score} and Definition \ref{def-directionality-score} and related Appendix  \ref{supp-math-symmetry-score} and \ref{supp-math-directionality-score}).
%
The incremental update of $\bm{W}_{qk}$ we described in the previous sections predicts that decoder-only models develop high-norm columns incrementally during training (see Theorem \ref{theo-informal-directionality}).
%
Likewise, as symmetric weight updates are added to $\bm{W}_{qk}$ in encoder-only models, Theorem \ref{theo-informal-symmetry} predicts that symmetric structures emerge incrementally during training.
%

% It follows from Definition \ref{def-symmetry-score} and Definition \ref{def-directionality-score} that the bilinear form at any $l$-th layer $\bm{W}^l_{qk}$  has both a symmetry score and a directionality score of zero at initialization (see Appendix \ref{supp-math-symmetry-score} and \ref{supp-math-directionality-score}).
%
% Theorem \ref{theo-unidirectional-structures}
% predicts that decoder-only models develop high-norm columns incrementally during training.
% %
% Likewise, as symmetric weight updates are added to $\bm{W}_{qk}$ in encoder-only models, Theorem \ref{theo-symmetric-structures} predicts that symmetric structures emerge incrementally during training. 
% %
% To test these predictions, we train 12-layer transformer models in both encoder and decoder modes and quantify the median symmetry and directionality scores across epochs (Figure \ref{fig-training-layers}a-b).
% %

Consistent with our results on pre-trained models, encoder-only models show a higher degree of symmetry than decoder-only models (Figure \ref{fig-training-layers}a). 
%
In contrast, decoder-only models have a higher directionality score (Figure \ref{fig-training-layers}b).
%
We observe this difference on all datasets we tested (Jigsaw \citep{jigsaw_challenge}, Wikipedia \citep{wikidump}, Red Pajama \citep{together2023redpajama}, see Figure \ref{suppfig-training-custom-models}).
%
Furthermore, late layers of encoder-only models are more symmetric and converge faster than early layers when training bidirectionally. At the same time, decoder-only models learn almost non-symmetric matrices with strong skew-symmetric matrices in the middle layers (Figure \ref{fig-training-layers}c).
% 
When training unidirectionally, both encoder and decoder models show a higher degree of directionality for late layers, which is remarkably stronger for decoder-only models (Figure \ref{fig-training-layers}d).
%
We observe similar differences across layers with all the datasets we tested (Figure \ref{suppfig-training-custom-models-layers}), despite these models having
less significant differences in directionality scores.
%
See Appendix~\ref{sec-experimental-details} for a detailed description of the experiments.
% 







\subsection{Enforcing symmetry at initialization improves the training of encoder-only models}
\label{sec-symmetric-initialization}
%
\input{ICML2025/tables/table-results-symmetric-initialization}
%
The previous section showed that symmetric structures incrementally emerge during training in the $\bm{W}_{qk}$ matrices of encoder-only models.
%
Here, we first provide evidence that these findings can be exploited to speed up training using symmetry as an inductive bias.
%
Specifically, we explore how symmetric initialization influences the training dynamics of the model and whether it enhances learning efficiency and overall performance.
%

We train 4-layer and 12-layer encoder-only models, comparing two initialization strategies: initialize the self-attention matrices independently versus initializing the $\bm{W}_q$ and $\bm{W}_k$ in each self-attention layer to ensure that $\bm{W}_{qk}$ is symmetric (see Appendix \ref{sec-experimental-details}).
%
We report the results of our experiments in Table \ref{table:symmetric-initialization}.
%
We observe that enforcing symmetry at initialization leads to lower loss values at the end of training for most of the models.
%
Importantly, symmetric initialization significantly accelerates convergence, reaching the final loss value faster than those with random initialization (up to $~75\%$ faster for 4-layer models and $~35\%$ faster for 12-layer models, see also Figure \ref{suppfig-symmetric-initialization}a).
%
Moreover, we observe that self-attention matrices initialized symmetrically lose symmetry during training but converge to higher symmetry levels than random initialization (Figure \ref{suppfig-symmetric-initialization}b)
%
This symmetric initialization decreases the gap in symmetric scores between layers compared to random initialization (Figure \ref{suppfig-symmetric-initialization}c).
%
% Interestingly, we also observe that enforcing symmetry at initialization improves accuracy on small, linear self-attention models (Figure \ref{suppfig:linear-attention-configs-symmetric-data}). 
%
These results highlight that embedding symmetry as an inductive bias across all Transformer layers can enhance training efficiency and model performance.
%