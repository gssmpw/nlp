%% New introduction
Transformer models now achieve state-of-the-art performance across a wide range of tasks and domains \citep{radfordLanguageModelsAre2019, dosovitskiyImageWorth16x162021, radford_robust_2023}.
%
Despite their success, the internal mechanisms governing their decision-making processes remain poorly understood, raising concerns regarding model alignment, reliability, and safety \citep{wang_aligning_2023, yao_survey_2024}.
%
A key challenge in understanding these models is unraveling the structures of self-attention, which is essential to Transformer architectures.
%
Current literature largely overlooks the nature of the self-attention weight matrices during autoregressive training, where the model predicts the next token in a sequence given previous ones \citep{radfordLanguageModelsAre2019,blackGPTNeoLargeScale2021,touvronLLaMAOpenEfficient2023} and bidirectional training, where the model predicts a missing token given the full sequence \citep{devlinBERTPretrainingDeep2019, baoBEiTBERTPreTraining2022,warnerSmarterBetterFaster2024}.
%
Understanding self-attention requires answering two fundamental questions: 
%
%
%% general introduction to self-attention
%
%Transformer networks now deliver state-of-the-art performance across various domains \citep{}. 
%
%Pretraining with self-supervised objectives is crucial for their success, typically optimizing models to either (a) predict the next token in a sequence given the previous ones (autoregressive training \citep{radfordLanguageModelsAre2019,blackGPTNeoLargeScale2021,touvronLLaMAOpenEfficient2023}), or (b) predict a missing token given the full sequence (bidirectional training \citep{devlinBERTPretrainingDeep2019, baoBEiTBERTPreTraining2022,warnerSmarterBetterFaster2024}).
%
%The intrinsic differences in these training schemes suggest that Transformers might learn different self-attention matrices in the two cases.
%
%% statement of the problem/knowledge gap
%However, the nature of the self-attention weight matrices that Transformers learn remains largely unexplored.  
%
How can we interpret the structures learned in the self-attention matrices? 
%
What is the impact of different objective functions on these matrices?
%
%Identifying such structures is fundamental not only for improving the performance of Transformer models, but also for their safety, alignment, and interpretability \citep{olahMechanisticInterpretabilityVariables2022}.
%

%% limitations of other mechanistic interpretability work
%
Previous work used sparse auto-encoders to identify interpretable features \citep{Huben_Cunningham_Smith_Ewart_Sharkey_2024, bricken2023monosemanticity}, circuit analysis to interpret Transformer components \citep{olahZoomIntroductionCircuits2020,elhageMathematicalFrameworkTransformer2021,olahMechanisticInterpretabilityVariables2022}, and techniques like the logit lens to analyze self-attention mechanisms \citep{Geva_Schuster_2021, darAnalyzingTransformersEmbedding2023} (for a detailed discussion, see Section \ref{sec-related-work}).
%
%
%
%Previous work demonstrates that sparse auto-encoders can identify units that consistently activate for specific input features, suggesting that Transformers can be described as compositions of interpretable feature detectors \citep{Huben_Cunningham_Smith_Ewart_Sharkey_2024, bricken2023monosemanticity}.
%
%While effective, this approach is data-dependent and does not reveal the structure of weight matrices.
%
%Other approaches focus on circuit analysis, mapping Transformer components onto interpretable computational graphs to study their functions \citep{olahZoomIntroductionCircuits2020,elhageMathematicalFrameworkTransformer2021,olahMechanisticInterpretabilityVariables2022}.
%
%Additionally, methods like the \emph{logit lens} project the query $\bm{W}_q$ and key $\bm{W}_k$ matrices into the vocabulary space to study how self-attention mixes and transfers information \citep{Geva_Schuster_2021, darAnalyzingTransformersEmbedding2023}.
%
However, these methods do not reveal the structural patterns in self-attention matrices or the transformations they encode.
% 
Crucially, how autoregressive and bidirectional training shape specific weight structures remains unclear.


%% results/contributions
%
To address this gap, we introduce a novel framework for analyzing self-attention matrices and understanding how different objective functions define their weight updates.
%
We then use this framework to derive understandable mathematical structures that should emerge from such updates. 
%
Finally, we verify these interpretable structures numerically on many pre-trained and custom models across different modalities, supporting the universality of our results.
%
Identifying these universal structures is fundamental not only for improving the performance of Transformer models, but also for their safety, alignment, and interpretability \citep{olahMechanisticInterpretabilityVariables2022}.

%We then use this framework to connect the matrix $\bm{W}_{qk} = \bm{W}_q\bm{W}_k^\top$ of self-attention with bilinear forms, offering novel insights compared to studying query and key matrices alone.
%
Specifically, we connect the matrix $\bm{W}_{qk} = \bm{W}_q\bm{W}_k^\top$ of self-attention with bilinear forms, offering novel insights compared to studying query and key matrices alone.
%
We reveal structured patterns in the implicit weight updates of $\bm{W}_{qk}$, uncovering key differences between encoder-only and decoder-only models:
%
\begin{enumerate}[noitemsep,nolistsep]
%
    \item \emph{Decoder-only models}: Training with an autoregressive objective produces a few columns with disproportionately high norms, introducing directionality in $\bm{W}_{qk}$.
    \item \emph{Encoder-only models}: Bidirectional optimization induces symmetric structures in $\bm{W}_{qk}$, reflecting the balanced nature of the training objective.
    \item We validate these theoretical findings across diverse Transformer architectures and input modalities, showing that they generalize across models and tasks.
    \item Empirically, we find that symmetric structures in $\bm{W}_{qk}$ enhance training efficiency for encoder-only models, leading to higher accuracy and faster convergence in language tasks.
    %
\end{enumerate}
%
