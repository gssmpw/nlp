
%
% Transformer models learn powerful representations by integrating contextual information through self-attention. 
%
% Yet, the distinct impacts of autoregressive (decoder-only) and bidirectional (encoder-only) training on self-attention matrices are not well understood.
%
%
% In this work, we propose a novel approach: instead of examining individual attention matrices, we formalize the product of query and key (the composite matrix) as a bilinear form and study its learning dynamics. 
%
% Additionally, we show that these structural properties emerge dynamically during training, with distinct patterns across different layers.
%
% Interpreting self-attention matrices as bilinear forms reveals a striking difference between encoder-only and decoder-only models and the structures learned during training.
%
Self-attention is essential to Transformer architectures, yet how information is embedded in the self-attention matrices and how different objective functions impact this process remains unclear.
%
We present a mathematical framework to analyze self-attention matrices by deriving the structures governing their weight updates.
%
Using this framework, we demonstrate that bidirectional training induces symmetry in the weight matrices, while autoregressive training results in directionality and column dominance.
%
Our theoretical findings are validated across multiple Transformer models — including ModernBERT, GPT, LLaMA3, and Mistral — and input modalities like text, vision, and audio.
%
Finally, we apply these insights by showing that symmetric initialization improves the performance of encoder-only models on language tasks.
%
This mathematical analysis offers a novel theoretical perspective on how information is embedded through self-attention, thereby improving the interpretability of Transformer models.
%
% This mathematical analysis of self-attention strengthens our understanding of Transformer models and provides a new theoretical perspective on self-attention mechanisms.

%Transformer models are usually trained with autoregressive or bidirectional objectives, yet the distinct impacts of these two training schemes on self-attention matrices are poorly understood.
%
%This work presents a new approach to studying self-attention matrices, introducing a novel perspective on these matrices and deriving the mathematical structures that govern their weight updates.
%
%Through this lens, we prove the connections between self-attention matrices and bilinear forms, showing that bidirectional training induces symmetry while autoregressive training enforces directionality and column dominance.
%
%We validate these theoretical findings across multiple Transformer models — including ModernBERT, GPT, LLaMA3, and Mistral — and input modalities such as text, vision, and audio.
%
%Additionally, we show that symmetry and directionality emerge dynamically during training, with distinct patterns across different layers.
%
%Finally, we demonstrate a practical application of these insights by showing that symmetric initialization enhances the performance of encoder-only models on language tasks.
%