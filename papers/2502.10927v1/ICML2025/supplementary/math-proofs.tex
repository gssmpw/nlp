%% PRELIMINARIES

%% (2) background and preliminaries
\subsection{Preliminaries}
%
Following the notation in \citep{vaswaniAttentionAllYou2017, radfordImprovingLanguageUnderstanding2018}, we define a Transformer architecture as,
%
\input{ICML2025/math/statements/def-transformer-model}
%
Furthermore, we use the following definition of a bilinear form,
\input{ICML2025/math/statements/def-bilinear-form}
%
Finally, we provide the following definition of autoregressive and bidirectional training objectives,
%
\input{ICML2025/math/statements/def-objective-functions}





%% PROOF PROPOSITION SELF-ATTENTION AND BILINEAR FORM
\subsection{Proof of Proposition \ref{prop-self-attention-bilinear-form} and related remarks}
\label{supp-math-self-attention-bilinear-form}
%
\input{ICML2025/math/proofs/proof-prop-self-attention-bilinear-form}
\input{ICML2025/math/statements/remark-self-attention-bilinear-form-projection}
\input{ICML2025/math/statements/remark-self-attention-bilinear-form-multihead}



%% PROOF PROPOSITION GRADIENTS AND RANK-1 MATRIX
\subsection{Proof of Proposition \ref{prop-gradients-self-attention} and related remarks}
\label{supp-math-gradients-self-attention}
%
\input{ICML2025/math/proofs/proof-prop-gradient-self-attention}
%
In standard Transformer models, the bilinear form $\bm{W}_{qk}$ is not directly computed, and as such, it is not explicitly updated through gradient descent.
%
Nonetheless, $\bm{W}_{qk}$ is implicitly updated with a combination of the weight updates of $\bm{W}_q$ and $\bm{W}_k$ having the same form as in Proposition \ref{prop-gradients-self-attention}, see the following.
%
\input{ICML2025/math/statements/remark-self-attention-gradient-WqWk}



\subsection{Formal proofs for the connection between objective functions, directionality, and symmetry}
\label{supp-math-directionality-symmetry}
%
In this section, we provide a formal mathematical derivation of the informal Theorems \ref{theo-informal-directionality} and \ref{theo-informal-symmetry} introduced in the main text.
%
To do so, we provide a formal description and proof of the Theorems for the directionality and symmetric structures during autoregressive and bidirectional training, respectively with Theorem \ref{theo-gradient-directionality} and Theorem \ref{theo-gradients-symmetry}.
%
We also introduce a series of related Propositions and Lemmas.
%
%

%
%% (1) PROPOSITION COLUMN AND ROWS
%
\subsubsection{Different implicit update of rows and columns}
\label{supp-math-prop-gradient-column-rows}
%
First, we show that a token $t^*$ contributes differently to the updates of $\bm{W}_{qk}$ depending on whether it serves as context for predicting other tokens or is itself predicted.
%
When $t^*$ is used to predict a set of tokens:
%
\begin{itemize}[noitemsep,nolistsep]
%
    \item All predicted tokens contribute to the column space of $\bm{W}_{qk}$ with the update of the $k$-th column scaled by the associated entry $[\bm{x}_{t^*}]_k$.
    \item Only the token $t^*$ contributes to the row space, with the update to the $m$-th row scaled a linear combination of  $[\bm{x}_i]_m$ for all predicted tokens.
%
\end{itemize}
%
On the other hand, when $t^*$ is predicted by a set of tokens:
%
\begin{itemize}[noitemsep,nolistsep]
%
    \item Only the token $t^*$ contributes to the column space, with the update to the $k$-th row scaled a linear combination of  $[\bm{x}_i]_k$ for all predicted tokens.
    \item All context tokens contribute to the row space with the update of the $m$-th column scaled by the associated entry $[\bm{x}_{t^*}]_m$.
%
\end{itemize}
%
We formalize this in the following Proposition.
%
\input{ICML2025/math/statements/prop-gradient-columns-rows}
\input{ICML2025/math/proofs/proof-prop-gradient-columns-rows}
%

%





%%
%% (2) 
\subsubsection{Asymmetric growth of rows and columns during weight update}
\label{supp-math-prop-gradient-asymmetric-growth-rows-columns}
%
Next, we demonstrate that under reasonable assumptions about the statistical distribution of token embeddings, the expected norm of column updates exceeds that of row updates when using the token $t^*$ to predict other tokens. 
%
Conversely, when $t^*$ is being predicted by other tokens, the row updates become dominant.
%
We begin by assuming that the token embeddings $\bm{x}_i$ are independent and identically distributed (i.i.d.) random vectors drawn from a probability distribution $\mathcal{D}$ with zero mean, $\mathbb{E}[\bm{x}_i] = \mathbf{0}$, and covariance matrix $\text{Cov}(\bm{x}_i) = \Sigma$.
%
This assumption holds true at initialization for any Transformer model with learnable embeddings. 
%
Additionally, we assume that the covariance matrix satisfies $\Sigma \neq \sigma^2 \mathbb{I}$,.
%
More specifically, we posit that there is partial alignment between the embeddings $\bm{x}_i$ due to the semantic and predictive relationships between tokens, which typically emerge in the vector embeddings during training.
%
Similar to Proposition \ref{prop-gradient-columns-rows}, the scenarios where $t^*$ is used to predict other tokens and where $t^*$ is being predicted by other tokens are complementary. 
%
In the following Proposition, we focus solely on the case where $t^*$ serves as context to predict a set of tokens. 
%
A formal derivation for the opposite case where $t^*$ is predicted by other tokens is provided in a subsequent Corollary.

\input{ICML2025/math/statements/prop-gradient-asymmetric-growth-rows-columns}
\input{ICML2025/math/proofs/proof-prop-gradient-asymmetric-growth-rows-columns}
%

%
Again, a complementary argument on asymmetric weight update can be made when a set of tokens $U = \{t_1, \dots, t_N\}$ is used to predict a given token $t^*$, that is, $C_i = \{t^*\} \,\,\forall t_i \in U$. We formalize this in the following Corollary.
%
\input{ICML2025/math/statements/corollary-gradient-asymmetric-growth-rows-columns}
\input{ICML2025/math/proofs/proof-corollary-gradient-asymmetry-growth-rows-columns}






%
%% (3) PROPOSITION COUNTING + PROOF
\subsubsection{Average contribution of a given token for context and prediction}
\label{supp-math-prop-counting-prediction-context}
%%
%%
%% PROVIDE ASSUMPTIONS ON MASKING AND BIDIRECTIONAL TRAINING
%
Additionally, we demonstrate that during autoregressive training, the expected number of tokens predicted by a specific token $t^*$ can differ from the expected number of tokens that predict $t^*$.
%
More specifically, in autoregressive training, the ratio between these two expected numbers is influenced by the statistical correlations between tokens in the dataset.
%
In contrast, for bidirectional training, the expected number of tokens predicted by $t^*$ and the number of tokens that predict $t^*$ are always equal. 
%
This equality holds true regardless of how tokens are correlated within the corpus, resulting in a ratio of 1.
%
We formalize this in the following Proposition.
%
\input{ICML2025/math/statements/prop-counting-prediction-context}
\input{ICML2025/math/proofs/proof-prop-counting-prediction-context}
%

Let us assume that there is no statistical correlation between the tokens in $U$, we can factorize the joint probability $\Pr[U]$ as follows
%
\begin{equation}
    \Pr[U] = \Pr[t_1,\dots,t_N] = \Pi_{i=1}^N\Pr[t_i]\,.
\end{equation}
%
Additionally, let us assume that each token is identically distributed independently of the position.
%
Therefore, during autoregressive training, the expected number  $\mathbb{E}[\mu_c(t^*)]$ can be simplified as
%
\begin{equation}
\mathbb{E}[\mu_c(t^*)] = \sum_{k=0}^N(N-k)\Pr[t_k = t^*] = \sum_{k=0}^N(N-k)\Pr[t^*] = \Pr[t^*]\frac{N(N-1)}{2} \,,
\end{equation}
%
and the same is true for $\mathbb{E}[\mu_p(t^*)]$
%
\begin{equation}
\mathbb{E}[\mu_p(t^*)] = \sum_{k=0}^N(k-1)\Pr[t_k = t^*] = \sum_{k=0}^N(k-1)\Pr[t^*] = \Pr[t^*]\frac{N(N-1)}{2} \,.
\end{equation}
%
It follows that, in the case of statistical independence between tokens, the expected number of tokens predicted by $t^*$ and the expected number of tokens predicting $t^*$.
%
Under these assumptions, there is no difference between autoregressive and bidirectional training.

On the other hand, let us assume a general joint probability between tokens, $\Pr[U] = \Pr[t_1,\dots,t_N]$.
%
When conditioning the probabilities over the future tokens, we obtain a factorization of the joint probability as follows,
%
%
\begin{equation}
    \Pr[U] = \prod_{i=1}^{N} \Pr[t_i \mid t_{i+1}, \dots, t_N]\,.
\end{equation}
%
The probability of the $k$-th token being $t^*$ across the possible sequences is thus given by
%
\begin{equation}
\begin{split}
\Pr[t_k = t^*] & = \sum_{t_1, \dots,t_N} \mathds{1}\{t_k = t^*\}\Pr[t_1,\dots,t_n] \\
&= \sum_{\substack{t_1, \dots, t_{k-1}, t_{k+1}, \dots, t_N}} \left( \prod_{\substack{l=1 \\ l \neq k}}^{T} \Pr[t_l | t_{l+1}, \dots, t_N] \right) \cdot \Pr[t_k = t | t_{k+1}, \dots, t_N] \,,
\end{split}
\end{equation}
%
leading to the ratio
%
\begin{equation}
    \frac{\mathbb{E}[\mu_c(t^*)]}{\mathbb{E}[\mu_p(t^*)]} = \frac{\sum_{k=1}^N (N-k)\sum_{\substack{t_1, \dots, t_{k-1}, t_{k+1}, \dots, t_N}} \left( \prod_{\substack{l=1 \\ l \neq k}}^{T} \Pr[t_l | t_{l+1}, \dots, t_N] \right) \cdot \Pr[t_k = t | t_{k+1}, \dots, t_N]}{\sum_{k=1}^N(k-1)\sum_{\substack{t_1, \dots, t_{k-1}, t_{k+1}, \dots, t_N}} \left( \prod_{\substack{l=1 \\ l \neq k}}^{T} \Pr[t_l | t_{l+1}, \dots, t_N] \right) \cdot \Pr[t_k = t | t_{k+1}, \dots, t_N]} \,.
\end{equation}
%
Therefore, when $t^*$ has a high probability of occurring before a given set of tokens, or when $t^*$ is likely to occur at the beginning of sentences and specific pieces of text, this ratio is likely to be higher than 1.






%% (5) THEOREM DIRECTIONALITY
\subsubsection{Theorem on the emergence of directionality}
\label{supp-math-theo-gradient-directionality}
%
Here, we show that, under the same assumptions as in Proposition \ref{prop-gradient-asymmetric-growth-rows-columns}, the weight updates of $\bm{W}_{qk}$ induce column dominance during autoregressive training. 
%
Specifically, there are on average more columns with high norms than rows with high norms.
%
We assume that, at initialization, the entries of $\bm{W}_{qk}$ are drawn from a probability distribution $\mathcal{P}$ with finite mean $\mu$ and variance $\sigma^2$. 
%
This assumption holds for any standard machine learning initialization scheme.
%
This implies that each $k$-th column $\bm{w}_{\cdot,k}$ and $m$-th row $\bm{w}_{m, \cdot}$ have the same mean $\mu$ and variance $\sigma^2/\sqrt{n}$.
%
Furthermore, the squared norm of both rows and columns have equal mean $n(\sigma^2 + \mu^2)$ and variance $n(2\sigma^4 + 4\mu^2)$.
%


%% LEMMA OUTLIERS
%
To demonstrate the column dominance, we first show that if one probability distribution has a higher variance than another, it is more likely to produce samples with higher values.
%
We formalize this in the following Lemma.
%
\input{ICML2025/math/statements/lemma-probability_same_mean_different_variance}
\input{ICML2025/math/proofs/proof_lemma-probability_same_mean_different_variance}
%

We now formalize the connection between this result and the asymmetric growth of columns in the following Theorem.
%
\input{ICML2025/math/statements/theo-gradient-directionality}
\input{ICML2025/math/proofs/proof-theo-gradient-directionality}
%







%% THEOREM SYMMETRY
\subsubsection{Theorem on the emergence of symmetry}
\label{supp-math-theo-symmetry-gradients}
%
Finally, we prove that the weight updates of $\bm{W}_{qk}$ induce symmetry during bidirectional training.
%
Importantly, the column dominance is present only during autoregressive training.
%
Indeed, it follows from Proposition \ref{prop-counting-prediction-context} that, during bidirectional training, the net increase of the norm of the columns is equal to the net increase of the norm of the rows.
%
We formalize this in the following Corollary.
%
\input{ICML2025/math/statements/corollary-gradient-directionality}
\input{ICML2025/math/proofs/proof-corollary-gradient-directionality}
%
%

Now, we show that the bidirectional nature of the training objective is such that every pair of tokens $(i,j)$ in a given sequence contributes to a term that is approximately symmetric.
%
Here, we assume that predicting the $i$-th token from the $j$-th token is correlated to predicting the $j$-th token from the $i$-th token.
%
In other words, predicting one token from the other gives similar predictions, that is, the term $\beta_{ij}$ and $\beta_{ji}$ from Proposition \ref{prop-gradients-self-attention} are correlated.
%
We formalize this in the following Theorem.
%
\input{ICML2025/math/statements/theo-gradient-symmetry}
\input{ICML2025/math/proofs/proof-theo-gradient-symmetry}
%

Encoder-only models are typically not trained to predict every token in a sequence, but rather a random subset of tokens, and the model can attend to tokens bidirectionally.
%
This is usually called Masked Language Modeling (MLM) \citep{devlinBERTPretrainingDeep2019, liuRoBERTaRobustlyOptimized2019, lanALBERTLiteBERT2020, warnerSmarterBetterFaster2024}.
%
Therefore, only a subset of terms in the double summation of Equation \eqref{eq:gradients-symmetric-terms} has the symmetric properties described above. 
%
We generalize the proof to this case in the following.
%
\input{ICML2025/math/statements/remark-gradient-symmetry-MLM}
%
Let $|M| = pN$ with $0<p<1$ being the percentage of tokens to be predicted during bidirectional training.
%
The total number of pairs in the second term of Equation \eqref{eq:factorization-summation} is given by a binomial coefficient, thus the total number of elements in the summation is $pN(pN - 1)$.
%
The total number of elements in the third term is instead the product $pN(N - pN)$.
%
Therefore, the percentage of symmetric weight updates from the second term over the total number of updates in the third term is given by
%
\begin{equation}
    \frac{pN(pN - 1)}{pN(N - pN)} \approx \, \frac{pN}{(N - pN)}\,,
\end{equation}
%
in the limit of large $N$.
%
In practice, $p$ is set to be around 15\%-30\% \citep{devlinBERTPretrainingDeep2019, liuRoBERTaRobustlyOptimized2019,lanALBERTLiteBERT2020,warnerSmarterBetterFaster2024}, leading to $ \approx 25\%$ of symmetric weight updates on average.
%





%% PROOF PROPERTIES OF SYMMETRY SCORE
\subsection{Properties of the symmetry score in Definition \ref{def-symmetry-score} and related proofs}
\label{supp-math-symmetry-score}
%
The score $s$ we introduce in Section~\ref{sec-results-pretrained-models} indicates the \emph{degree} of symmetry of a matrix $\bm{M}$ by quantifying the contribution to the Frobenious norm of its symmetric and skew-symmetric parts. 
%
In particular, $s$ equals 1 and -1 for a fully symmetric and skew-symmetric matrix.
%
Accordingly, positive (negative) values of $s$ indicate the presence of symmetric (skew-symmetric) structures.
%
Here, we provide a proof for these properties. 
%
First, we show that the Frobenious norm of any square matrix $\bm{M}$ can be decomposed in the sum of the Frobenious norm of its symmetric and skew-symmetric components, as in the following Lemma,
%
\input{ICML2025/math/statements/lemma-decomposition-norm-symmetric-skew-symmetric}
\input{ICML2025/math/proofs/proof-lemma-decomposition-norm-symmetric-skew-symmetric}
%
Next, we formulate the properties of the symmetry score as follows,
%
\input{ICML2025/math/statements/prop-score-symmetry-properties}
\input{ICML2025/math/proofs/proof-prop-symmetry-score-properties}
%



%% PROOF PROPERTIES OF DIRECTIONALITY SCORE
\subsection{Properties of the directionality score in Definition \ref{def-directionality-score} and related proofs}
\label{supp-math-directionality-score}
%
The score $d$ we introduce in Section~\ref{sec-results-pretrained-models} quantifies the directional bias of a square matrix $\bm{M}$ by comparing the total norm of the "outliers" rows and columns, that is, that are higher than $\gamma$ times the standard deviations of the norms.
%
A directionality score $d$ of 1 indicates the presence of rows with high ``outlier'' norms and the absence of outliers in the distribution of the column norms.  
%
The opposite is true for a directionality score $d$ of -1.
%
Accordingly, positive (negative) values of $d$ indicate the presence of row (column) dominance in the matrix.
%
Here, we provide a proof for these properties.
%
\input{ICML2025/math/statements/prop-score-directionality-properties}
\input{ICML2025/math/proofs/proof-prop-directionality-score-properties}