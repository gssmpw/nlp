% MECHANISTIC INTERPRETABILITY
% (1) looking at features 
%
% how? feeding input to the model and analyzing resulting activations
% dictionary learning with sparse autoencoders: https://transformer-circuits.pub/2023/monosemantic-features, https://transformer-circuits.pub/2024/scaling-monosemanticity/
% -> monosemanticty 
% -> superposition (polysemanticity)
%
% (2) circuit analysis
%
% While the study of features helps us to understand what information is encoded in a model’s activations, it does not inform us of how these features are extracted from the input and then processed to implement specific LM behaviors (e.g., reasoning).
%
%  explaining a model algorithm by document- ing a low-level computation graph, mapping from the graph to the model nodes that implement the computation, and performing experimentation ver- ification.
%
% --> induction heads
%
% (3) universality -> very important: which of these results turns out to be universal, shared across multiple LLMs? Some induction heads and stuff are found everywhere, but evidence is mixed.
%
% Studies on Complex Tasks and LLMs Current MI studies are mostly performed on simpler tasks, often criticized as “streetlight interpretability” (Casper, 2023; Wang, 2022). For instance, Wang (2022) intentionally selected the IOI task (Wang et al., 2022a) because it is a simple algorithmic task. Similarly, although a few studies were done on “production-level” LMs (Lieberum et al., 2023; Templeton et al., 2024), most still used small LMs, which may have limited generalizability, given the mixed results on universality.
%
%
%
% (4) logit lens on the column of weight matrices
%
% Our approach is different: we formalize a specific lens of looking at weight matrices, we derive understandable mathematical structures that should emerge from the learning dynamics, we verify this interpretable structure numerically on pretrained and custom models, and we can check how to use them in practice
%
%
\textbf{Transformer models} Transformer models \cite{vaswaniAttentionAllYou2017} are a class of deep learning architectures that have achieved state-of-the-art results across several domains, including natural language processing (NLP) \cite{devlinBERTPretrainingDeep2019, Perez_sentiment_2021, Raffel_T5_2020}, computer vision (CV) \cite{dosovitskiyImageWorth16x162021}, speech processing \cite{hsuHuBERTSelfSupervisedSpeech2021}, and multi-modal applications \cite{Li_Li_Xiong_Hoi_2022}. Originally introduced for sequence-to-sequence (seq2seq) tasks \cite{Sutskever_Vinyals_Le_2014}, Transformers comprise two main components: an encoder and a decoder, each built from a stack of $L$ identical layers. An encoder block consists of a multi-head self-attention mechanism and a position-wise feed-forward network. Residual connections \cite{He_Zhang_Ren_Sun_2016} are employed around these two sub-modules, followed by layer normalization \cite{Ba_Kiros_Hinton_2016}.
Decoder layers incorporate cross-attention mechanisms in addition to self-attention, allowing the model to attend to the encoder outputs. Furthermore, the decoder applies masking to the self-attention layer to prevent it from attending to future positions, ensuring proper autoregressive behavior. The complete Transformer architecture, comprising both encoder and decoder, is mainly used for seq2seq tasks such as machine translation \cite{Raffel_T5_2020}, image captioning \cite{Li_Li_Xiong_Hoi_2022}, and text summarization \cite{Raffel_T5_2020}.
However, many tasks can be effectively solved using only parts of the architecture.
For instance, the encoder can be utilized to map input data to latent representations, which are useful for classification tasks such as sentiment analysis \cite{Perez_sentiment_2021}, named entity recognition \cite{Luoma_Exploring_2020}, image classification \cite{dosovitskiyImageWorth16x162021}, and spoken language classification \cite{Baevski_wav2vec2_2020}.
When removing the cross-attention module, the decoder can be used on its own for sequence generation. Typical tasks include text generation and language modeling \cite{radfordImprovingLanguageUnderstanding2018}. Encoder models are typically trained by corrupting the input data (for instance, by randomly masking words of a text sequence), and the model's objective is to reconstruct the original input sequence \cite{devlinBERTPretrainingDeep2019}.
Since the attention layers can access all the tokens in the initial sequence, these models leverage ``bi-directional'' attention, allowing the model to predict masked tokens by considering both preceding and subsequent tokens in the sequence.
In contrast, decoder models are trained in an autoregressive manner, where the task is to predict the next token in a sequence based on the previously observed tokens \cite{radfordImprovingLanguageUnderstanding2018}. Consequently, these models rely on ``uni-directional'' attention, where predictions are based solely on past tokens. Encoder-decoder models combine these approaches, with the encoder attending to all tokens in the input sequence and the decoder attending only to previous target tokens.
Although these models can be trained using objectives similar to those of purely encoder-based or decoder-based models, their pre-training strategies are often tailored to the specific architecture and task. For example, in T5 \cite{Raffel_T5_2020}, a sequence of several words is replaced with a single masking token, and the objective is to predict the entire sequence of masked words. Numerous variations of the standard Transformer architecture, often referred to as X-formers, have been proposed to improve efficiency and performance.
These variations typically adapt the attention mechanism \cite{Guo_Qiu_Xue_Zhang_2019, Wang_Li_Khabsa_Fang_Ma_2020, Beltagy_Peters_Cohan_2020}, the feed-forward layer \cite{Yang_Wang_Shi_Tadepalli_Lee_Tu_2020, Fedus_Zoph_Shazeer_2022}, the positional encoding \cite{devlinBERTPretrainingDeep2019, Raffel_T5_2020}, or the layer normalization \cite{Nguyen_Salazar_2019, Shen_Yao_Gholami_Mahoney_Keutzer_2020}.
In this work, we focus on the original Transformer architecture in both encoder and decoder configurations and slight variations such as absolute positional encoding \cite{devlinBERTPretrainingDeep2019}. However, we believe that our findings are broadly applicable to most X-former variants, given their similar underlying structure.

\textbf{Transformer Explainability} A vast amount of literature focuses on the explainability of Transformer models. These approaches can be distinguished into two main categories: Methods analyzing the prompting and methods focusing on the network activations and weight.
Since our work does not consider prompting, we focus this discussion exclusively on the latter. Techniques that investigate activations and weights generally fall into two categories: those that provide explanations at the model level, seeking generally valid explanations, and those that focus on individual data instances.
Methods that offer general explanations at the model level include probing techniques \cite{Peters_Neumann_2018, Tenney_Xia_2019, Maudslay_Cotterell_2021}, which aim to elucidate the model's learned properties (such as linguistic features in large language models), relate neuronal activations to these properties \cite{Dalvi_Durrani_2019, Torroba_Hennigen_2020, Antverg_Belinkov_2022}, map input data to predefined concepts to assess their importance \cite{Kim_Wattenberg_2018, Mu_Andreas_2020}, or employ mechanistic interpretability \cite{bricken2023monosemanticity, Geva_Schuster_2021, Meng_Bau_2022, Geva_Caciularu_2022, Lieberum_Rahtz_2023} to explore the functions of neurons and their interconnections. In contrast, methods that concentrate on specific data instances typically assess the importance of individual input features \cite{Wu_Chen_2020, Lundstrom_Huang_2022}, analyze attention scores \cite{Hoover_Strobelt_2020, Barkan_2021, Yeh_Chen_2023}, investigate changes in output in response to variations in input \cite{Jin_Jin_2020, Wang_Xu_2022}, or, in the context of large language models, generate natural language explanations \cite{Rajani_McCann_2019}. The presented work advances the mechanistic understanding of Transformers. Specifically, we investigate certain properties of the self-attention mechanism, a fundamental component of the Transformer circuit.
While multiple approaches investigate the role of fully connected layers as part of a larger circuit \cite{Geva_Schuster_2021, Meng_Bau_2022, Geva_Caciularu_2022}, there is considerably less work focusing on the role of attention within the circuit. One related work investigating self-attention mechanistically is from Elhage et al. \cite{elhageMathematicalFrameworkTransformer2021}:
% TODO MATTEO
%
Having multiple heads means dividing both matrices into blocks $\bm{W}^h_q$ and $\bm{W}^h_q$ of dimensions In multi-head self-attention, the product $\bm{W}^h_q\bm{W}^h_k$ for the $h$-th head is a low-rank matrix with maximum rank defined by the head size.
%
While it is computationally efficient to define the matrices $\bm{W}_q$ and $\bm{W}_k$ separately and compute multiple attention heads in one matrix multiplication, 





Previous research has characterized and cataloged attention heads based on the structures in the attention matrices generated from sequences of tokens \citep{clarkWhatDoesBERT2019, kovalevaRevealingDarkSecrets2019, coenenVisualizingMeasuringGeometry2019, abnarQuantifyingAttentionFlow2020}. %O
Other works also investigated how the tokens' alignment differs across layers of encoder-only and decoder-only models \citep{ethayarajhHowContextualAre2019}.
%
However, interpreting these scores is challenging due to their input dependency \citep{}, inconsistency across tasks and models \citep{}, and general complexity \citep{}.
%
Other approaches have focused on breaking down the operations of Transformers to identify interpretable components \citep{}, but these methods often struggle with scalability, lack generalizability to different architectures, and tend to overemphasize specific mechanisms \citep{}.

% NEW BLOCK -> TO CHECK @MATTEO
Previous research analyzing the influence of attention scores on token sequences often encountered scalability limitations and have been restricted to specific token sequences under investigation. Similarly, work on mechanistic interpretability, particularly treating Transformer networks as computational circuits, has made significant progress in understanding fully connected layers but lacks a comprehensive understanding of the attention mechanism.
Our work addressed these gaps by advancing a scalable and generalizable approach to the mechanistic understanding of self-attention.

Beyond its implications for explainability, these findings have two practical consequences for Transformer training. 
%
First, 
%after reducing the self-attention mechanism to a minimalistic linear form, 
%
our findings suggest that bidirectional training outperforms next-token prediction when optimizing linear self-attention on symmetric datasets.
%
% implying that the structure of the training data may dictate the optimal training strategy. 
%
Second, we showed that bidirectional training benefits from initializing the query $\bm{W}_q$ and key $\bm{W}_k$ matrices such that their product $\bm{W}_{qk} = \bm{W}_q\bm{W}^\top_k$ is symmetric, improving model convergence speed.
%
Previous work has shown that initializing query and key matrices as the identity matrix accelerates training \citep{trockman_mimetic_2023}.
%
Importantly, since the identity matrix is inherently symmetric, these findings support our results.
%
%showing that the product of the query and key matrices become approximately the identity and that such initializations accelerate training. In fact, our work generalizes these previous results, as diagonal matrices are a specific case of symmetric matrices.
%
