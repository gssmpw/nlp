%
Given a self-attention layer as in \cref{def:transformer-model} and considering Equation \cref{eq:results:self-attention-bilinear-form}, we obtain the following expression for $\hat{\bm{X}}$,
%
\begin{equation}
    \hat{\bm{X}}(\bm{X}) = \bm{X} + \bm{A}(\bm{X})\bm{V}(\bm{X}) = \bm{X} + \sigma\left(\frac{1}{\sqrt{d}}\,\bm{X} \bm{M}\bm{X}^T\right)\bm{X}\bm{W}_v \,.
\end{equation}
%
For clarity, we refer to a general layer in a Transformer model, omitting the subscript $l$.
%
It follows that the element $[\bm{A}]_{ij}$ of the attention score matrix is given by, 
%
\begin{equation}
    [\bm{A}]_{ij} =  \hat{\alpha}_{ij}(\bm{M}) = \frac{\exp{(\bm{x}_i^\top \bm{M} \bm{x}_j)}}{\sum_j \exp{(\bm{x}_i^\top \bm{M} \bm{x}_j)}} = \frac{1}{\mathcal{N}_i}\exp{(\bm{x}_i^\top \bm{M} \bm{x}_j)} = \frac{\alpha_{ij}(\bm{M})}{\mathcal{N}_i} \,,
\end{equation}
%
where we neglect the scaling factor $1 / \sqrt{d}$, and $\mathcal{N}_i$ is a normalization factor defined by the row-wise softmax function $\sigma(\cdot)$ and shared across all entries in the $i$-th row of $\bm{A}$,
%
\begin{equation}
    \mathcal{N}_i(\bm{X}; \bm{M}) = \sum_j \exp{(\bm{x}_i^\top \bm{M} \bm{x}_j)} = \sum_j \alpha_{ij}(\bm{M}) \,.
\end{equation}
%
Therefore, the $i$-th row of $\bm{X}$ corresponding to the $i$-th token in the sequence is transformed into the column vector $\hat{\bm{x}}_i$ as follows,
%
\begin{equation}
\hat{\bm{x}}^\top_i = \bm{x}^\top_i + \frac{1}{\mathcal{N}_i}\sum_j\alpha_{ij}(\bm{M})\,\bm{x}_j\bm{W}_v\,.
\end{equation}
%
We analyze the elements of $\bm{A}$ in two steps.

%% (1) the matrix M defines a bilinear product in the embedding space
\textbf{Self-attention defines a bilinear product in the embedding space}. First, we observe that the coefficients $\{\alpha_{ij}\}$ represent the projection of $\bm{x}_i$ onto the subspace spanned by $\bm{X} = \{\bm{x}_0^\top, \bm{x}_1^\top, \dots, \bm{x}_N^\top\}$ in the transformed embedding space defined by $\bm{M}$:
%
\begin{equation}
\label{eq:math:linear-decomposition-subspace-tokens}
     \sum_j \alpha_{ij}(\bm{M})\, \bm{x}_j = \sum_j \bm{x}_i^\top \bm{M} \bm{x}_j\,\bm{x}_j = \sum_j \langle \bm{x}_i, \bm{M}\bm{x}_j \rangle \,\bm{x}_j = \sum_j P_{\bm{M}}(\bm{x}_i ,\bm{x}_j)\,,
\end{equation}
%
where $P_{\bm{M}}(\,\cdot\,, \bm{x}_j)$ are operators over the subset $\bm{X}$.
%
It is important to note that, in general, the set $\bm{X}$ is linearly dependent, and the number of tokens $N$ in the sequence can be smaller, bigger, or equal to the embedding space dimension $d$.
%
We also note that the bilinear form $\bm{M}$ is generally indefinite and possibly non-invertible, and thus does not formally define an inner product in the embedding space. 
%
Furthermore, the operators $P_{\bm{M}}(\cdot, \bm{x}_j)$ are not formal projection operators since they are not nilpotent ($P_{\bm{M}}(\,\cdot\, \bm{x}_j) \circ P_{\bm{M}}(\,\cdot\, \bm{x}_j) \neq P_{\bm{M}}(\,\cdot\, \bm{x}_j)$).
%
Nevertheless, the sum in Equation \ref{eq:math:linear-decomposition-subspace-tokens} represents a decomposition of $\bm{x}_i$ on the subspace $\text{span}\{\bm{X}\}$ where each coefficient $\alpha_{ij}(\bm{M})$ quantifies the aligment of $\bm{x}_i$ with the corresponding element of $\bm{X}$ following the bilinear form $\bm{M}$. 
%

%% (2) adding softmax moves the linear combination in the convex hull, but the relative ranking is the same.
\textbf{Softmax translates the linear combination in the convex hull, but preserves relative ranking}. Next, the coefficients $\{\alpha_{ij}(\bm{M})\}$ are used to calculate a new set of coefficients,
%
\begin{equation}
    \hat{\alpha}_{ij}(\bm{M}) = \frac{1}{\mathcal{N}_i}\, \exp(\alpha_{ij}(\bm{M}))\,.
\end{equation}
%
This set of coefficients $\{\hat{\alpha}_{ij}\}$ defines a specific convex combination that restricts the resulting vector in the convex hull $\text{Conv}(\bm{X}) \subset \text{span}\{\bm{X}\}$.
%
Since the softmax function is monotonically increasing, $\{\hat{\alpha}_{ij}(\bm{M})\}$ preserves the relative ranking between the coefficients while exponentially suppressing negative and near-zero values.
%
Therefore, the coefficients $\{\hat{\alpha}_{ij}\}$ represent a decomposition of $\bm{x}_i$ onto the convex-hull $\text{Conv}(\bm{X})$ that preserves the structure of the decomposition in Equation \ref{eq:math:linear-decomposition-subspace-tokens}, as follows,
%
\begin{equation}
     \hat{\alpha}_{ij}\,\bm{x}_j < \hat{\alpha}_{ij'}\,\bm{x}_{j'} \quad \Leftrightarrow \quad \alpha_{ij}\,\bm{x}_j < \alpha_{ij'}\,\bm{x}_{j'} \quad \forall i,j \,.
\end{equation}
%
This demonstrates that self-attention computes a generalized decomposition of $\bm{x}_i$ on $\text{Conv}(\bm{X})$ in the transformed embedding space defined by $\bm{M}$.
%
The use of a convex combination ensures that the resulting vector remains within the region enclosed by the basis vectors $\bm{X}$.
%

Thus, we can rewrite the final column vector $\hat{\bm{x}}^\top_i$ as shown in Equation \ref{eq:results:self-attention-projection-basis},
%
\begin{equation}
    \hat{\bm{x}}^\top_i = \bm{x}^\top_i + \sum_{j=1}^N  \hat{\alpha}_{ij}(\bm{M})\,\bm{x}_j\bm{W}_v 
\end{equation}
%
where $\bm{W}_v$ represents a linear transformation that is applied independently after the linear decomposition. 
%
