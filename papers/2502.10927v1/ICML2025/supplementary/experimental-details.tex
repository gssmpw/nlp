We trained two BERT models \citep{devlinBERTPretrainingDeep2019} to examine the evolution of the symmetry and directionality scores throughout the training process. Detailed information regarding the training procedure is provided below.

\subsection{Models}\label{sec:exp_bertmodels_models}
%
\input{ICML2025/tables/supp-table-bert-configs}

We train the standard BERT model (referred to as \emph{BERT}) and a smaller version (referred to as \emph{BERT-Mini}), following the implementation by \citep{devlinBERTPretrainingDeep2019}. 
Table \ref{tab:bert_configs} provides an overview of the model parameters. 
The standard BERT model has $12$ layers, $12$ attention heads, and embedding dimensions of $768$ for the hidden layers and $3072$ for the intermediate layers. In contrast, the smaller BERT-Mini model uses $4$ layers, $4$ attention heads, and embedding dimensions of $256$ and $1024$, respectively.

\paragraph{Initialization}
We optionally initialize the models with either symmetric or skew-symmetric attention weights by initializing the key and query weight matrices of all self-attention blocks following a specific procedure.
For symmetric initialization, the query weight matrix $\bm{W_q}$ is initialized randomly, and then the key weight matrix is set to $\bm{W_k} = \bm{W_q}$.
For skew-symmetric initialization, $\bm{W_q}$ is initialized randomly, and a skew-symmetric matrix $\bm{S}$ is generated by first initializing a random matrix, $\bm{A}$ and then calculating \(\bm{S} = \bm{A} - \bm{A}^\top\). The key matrix is then defined as $\bm{W_k} = \bm{W_q} \bm{S}^\top$, ensuring that the matrix multiplication $\bm{W_q} \bm{W_k}^\top$ is skew-symmetric.

\paragraph{Loss Constraints}
The loss function $\mathcal{L}(\bm{U})$ described in Equation \eqref{eq:results:self-supervised-pretraining} is optionally extended with a constraint to either encourage symmetric or skew-symmetric attention weights.
%
\begin{equation}
    \mathcal{L}(\bm{U}) := \mathcal{L}(\bm{U}) + \gamma \mathcal{L}_{c}(\mathcal{W}).
\end{equation}
%
Here, $\gamma$ is a coefficient defining the importance of the $\mathcal{L}_{c}$ constraint.
To encourage symmetry, we minimize the condition
%
\begin{equation}
    \mathcal{L}_c(\mathcal{W}) = \frac{\bm{M}^2}{\bm{M}_s^2}.
\end{equation}
%
This constraint reaches its minimum if all attention weights have a symmetry score $s=1$.
To encourage skew-symmetry, we minimize the condition
%
\begin{equation}
    \mathcal{L}_c(\mathcal{W}) = \frac{\bm{M}^2}{\bm{M}_n^2},
\end{equation}
%
leading to a symmetry score of $s=0$.


\subsection{Datasets}\label{sec:exp_bertmodels_datasets}
The models are trained on three datasets.
First, we use the ``20220301.en'' snapshot from the Wikipedia dataset, which consists of 6.46 million samples crawled from Wikipedia. Second, we utilize the Jigsaw dataset with $159$K samples, originally collected for a toxic comment classification challenge.
Finally, we train on the English ``2023-14'' snapshot of the RedPajama-V2 dataset, which contains approximately $5.12$ billion samples.

\subsection{Training Settings}\label{sec:exp_bertmodels_training}
The models are trained for $200,000$ update steps with a batch size of $32$ and $8$ gradient accumulation steps, effectively increasing the batch size to $256$ before each parameter update. The optimization is done using the AdamW optimizer \citep{Loshchilov_Hutter_2019}, and the training schedule includes $200$ warmup steps to stabilize early training, followed by a linear decay learning rate schedule, starting at an initial learning rate of $5 \times 10^{-5}$ and weight decay of $0.01$.
Mixed precision (fp16) training was utilized to maximize training efficiency, which reduces memory consumption and speeds up computation without significant loss of precision. The training data was processed with a masked language modeling (MLM) probability of $15$\%, ensuring that $15$\% of tokens were masked during training.
The models are trained in the encoder and decoder mode, i.e., to predict masked tokens and subsequent tokens respectively.

% \subsection{Linear Attention}\label{sec:exp_linearattention}
% We train a minimalistic attention block to investigate the origins and properties of the symmetry of the attention weights.
% Details about this model, model configuration, and datasets are provided in the following.

% \subsubsection{Models}\label{sec:exp_linearattention_models}
% The minimal model consists of an embedding layer that maps the token to embeddings, a positional encoding layer to add positional information to the embeddings, and one or multiple self-attention blocks \citep{vaswaniAttentionAllYou2017}.
% We apply skip-connections around the attention blocks but do not use subsequent linear layers or non-linear activation functions (hence the name ``linear attention'').
% After the last attention block, we use an output layer to map the embeddings to the dimensionality of the tokens' space.
% We use the same weights for the output and token embedding layers, i.e., implement weight tying \citep{press-wolf-2017-using}.
% Similar to the BERT models (cf. Section \ref{sec:exp_bertmodels_models}), we optionally apply symmetric or skew-symmetric initialization and a symmetric or skew-symmetric training constraint.

% \subsubsection{Datasets}\label{sec:exp_linearattention_datasets}
% We generate two artificial datasets for our experiments. The first dataset is constructed using a vocabulary of $10$ tokens. Each sample in this dataset consists of token sequences. To create a sequence, we randomly select a token from the vocabulary and append it between $0$ and $3$ times to the sequence. This process is repeated for all tokens, forming a sub-sequence until a total length of $100$ tokens is reached.

% The second artificial dataset consists of symmetric sequences mirrored in the middle. The vocabulary comprises $48$ tokens. A token is randomly selected from the vocabulary to construct a sequence, repeated between $1$ and $3$ times, and appended to the sequence. This process is repeated until a sequence of $48$ tokens is obtained. The resulting sequence is then duplicated, mirrored, and concatenated to the original, producing a final sequence of $96$ tokens.
% To make the prediction task more challenging, we add jitter directly to the training data by randomly replacing some tokens with a certain probability.

% \subsubsection{Training Settings}\label{sec:exp_linearattention_training}
% We train the models for at least $2,000$ epochs to capture changes in symmetry even though the models' performance (in terms of loss and prediction accuracy) already levels off after a few hundred epochs.
% We use a batch size of $128$ and a constant learning rate of $1 \cdot 10^{-3}$ using either the AdamW \citep{Loshchilov_Hutter_2019} optimizer or SGD.