 \begin{table}
\centering
\caption{Directionality score for open-source pretrained language models. All models are available on Hugging Face \citep{wolfHuggingFaceTransformersStateoftheart2020}.}
\label{table:directionality-score-models}
\vspace{5pt}
\begin{tabular}{lcc|lcc}
    \toprule
    \textbf{Model} & \textbf{Median} & \textbf{Interquartile range} & \textbf{Model} & \textbf{Median} & \textbf{Interquartile range} \\ 
    \midrule
    BERT-tiny     & -0.79 & $\pm$ [0.11, 0.11] & GPT-Neo1.3B    & -0.49 & $\pm$ [0.19, 0.13] \\ 
    BERT-mini     & -0.33 & $\pm$ [0.03, 0.04] & GPT-Neo2.7B    & -0.57 & $\pm$ [0.15, 0.16] \\ 
    BERT-small    & -0.22 & $\pm$ [0.04, 0.03] & GPT-J6B        & -0.28 & $\pm$ [0.09, 0.08] \\ 
    BERT-medium   & -0.23 & $\pm$ [0.06, 0.10] & OpenAI-GPT     & -0.18 & $\pm$ [0.08, 0.07] \\ 
    BERT-base     & -0.08 & $\pm$ [0.02, 0.03] & GPT2-XL        & -0.23 & $\pm$ [0.11, 0.10] \\ 
    BERT-large    & -0.03 & $\pm$ [0.02, 0.06] & DistilGPT2     & -0.51 & $\pm$ [0.03, 0.07] \\ 
    DistilBERT    & -0.13 & $\pm$ [0.00, 0.06] & GPT-Neo125M    & -0.56 & $\pm$ [0.21, 0.08] \\ 
    BERT-2L-128   & -0.79 & $\pm$ [0.11, 0.11] & GPT-Neo1.3B    & -0.49 & $\pm$ [0.19, 0.13] \\ 
    BERT-4L-256   & -0.33 & $\pm$ [0.03, 0.04] & GPT-Neo2.7B    & -0.49 & $\pm$ [0.15, 0.21] \\ 
    BERT-4L-512   & -0.22 & $\pm$ [0.04, 0.03] & GPT-J6B        & -0.28 & $\pm$ [0.09, 0.08] \\ 
    BERT-8L-512   & -0.23 & $\pm$ [0.06, 0.10] & LLaMA2-7B      & -0.26 & $\pm$ [0.09, 0.13] \\ 
    BERT-base     & -0.08 & $\pm$ [0.02, 0.03] & LLaMA2-13B     & -0.15 & $\pm$ [0.11, 0.03] \\ 
    BERT-large    & -0.03 & $\pm$ [0.02, 0.06] & LLaMA3-8B      & -0.65 & $\pm$ [0.13, 0.20] \\ 
    DistilBERT    & -0.13 & $\pm$ [0.00, 0.06] & LLaMA3.1-8B    & -0.64 & $\pm$ [0.17, 0.19] \\ 
    BEiT-base     & -0.10 & $\pm$ [0.06, 0.15] & LLaMA3.2-8B    & -0.59 & $\pm$ [0.18, 0.22] \\ 
    BEiT-large    & -0.15 & $\pm$ [0.08, 0.07] & LLaMA3.2-1B    & -0.59 & $\pm$ [0.18, 0.22] \\ 
    BEiT-base     & -0.14 & $\pm$ [0.15, 0.21] & LLaMA3.2-3B    & -0.77 & $\pm$ [0.08, 0.19] \\ 
    BEiT-large    & -0.14 & $\pm$ [0.04, 0.14] & LLaMA2-7B-chat & -0.29 & $\pm$ [0.07, 0.14] \\ 
    BEiT-base     & -0.14 & $\pm$ [0.15, 0.21] & LLaMA2-70B     & -0.24 & $\pm$ [0.10, 0.06] \\ 
    BEiT-large    & -0.14 & $\pm$ [0.04, 0.14] & LLaMA2-7B-chat & -0.29 & $\pm$ [0.07, 0.14] \\ 
    BEiT-large    & -0.15 & $\pm$ [0.03, 0.14] & LLaMA2-13B-chat & -0.19 & $\pm$ [0.12, 0.04] \\ 
    ALBERT-base   & -0.07 & $\pm$ [0.00, 0.00] & LLaMA3-8B      &  0.01 & $\pm$ [0.05, 0.05] \\ 
    ALBERT-large  & -0.17 & $\pm$ [0.00, 0.00] & LLaMA3-70B     & -0.37 & $\pm$ [0.09, 0.12] \\ 
    ALBERT-xlarge & -0.24 & $\pm$ [0.00, 0.00] & LLaMA3.1-8B    & -0.57 & $\pm$ [0.16, 0.13] \\ 
    ALBERT-xxlarge & -0.15 & $\pm$ [0.00, 0.00] & LLaMA3.1-70B   & -0.37 & $\pm$ [0.08, 0.11] \\ 
    RoBERTa-base  & -0.12 & $\pm$ [0.11, 0.03] & LLaMA3.1-405B  & -0.17 & $\pm$ [0.07, 0.07] \\ 
    RoBERTa-large & -0.06 & $\pm$ [0.03, 0.03] & LLaMA3.2-1B    & -0.02 & $\pm$ [0.13, 0.08] \\ 
    XLM-R-base    & -0.02 & $\pm$ [0.02, 0.02] & LLaMA3.2-3B    & -0.70 & $\pm$ [0.07, 0.22] \\ 
    XLM-R-large   & -0.02 & $\pm$ [0.03, 0.02] & Mistral-7B     & -0.58 & $\pm$ [0.15, 0.13] \\ 
    RoBERTa-mnli  & -0.06 & $\pm$ [0.03, 0.03] & Mixtral-8x22B  & -0.66 & $\pm$ [0.09, 0.16] \\ 
    DistilRoBERTa & -0.14 & $\pm$ [0.09, 0.07] & MobileLLM125M  & -0.13 & $\pm$ [0.15, 0.10] \\ 
    ModernBERT-base & -0.04 & $\pm$ [0.05, 0.04] & MobileLLM350M  & -0.34 & $\pm$ [0.13, 0.23] \\ 
    GPT1          & -0.18 & $\pm$ [0.08, 0.07] & Phi-1.5        & -0.28 & $\pm$ [0.22, 0.19] \\ 
    GPT2          & -0.58 & $\pm$ [0.06, 0.14] & Phi-1          & -0.40 & $\pm$ [0.03, 0.04] \\ 
    \bottomrule
\end{tabular}
\end{table}
