\begin{table}
\centering
\caption{Symmetry score for open-source pretrained language models. All models are available on Hugging Face \citep{wolfHuggingFaceTransformersStateoftheart2020}.}
\label{table:symmetry-score-models}
\vspace{5pt}
\begin{tabular}{lcc|lcc}
    \toprule
    \textbf{Model} & \textbf{Median} & \textbf{Interquartile range} & \textbf{Model} & \textbf{Median} & \textbf{Interquartile range} \\ 
    \midrule
    BERT-tiny     & 0.77 & $\pm$ [0.07, 0.07] & GPT-Neo1.3B    & 0.14 & $\pm$ [0.03, 0.03] \\ 
    BERT-mini     & 0.62 & $\pm$ [0.03, 0.05] & GPT-Neo2.7B    & 0.13 & $\pm$ [0.02, 0.04] \\ 
    BERT-small    & 0.69 & $\pm$ [0.10, 0.08] & GPT-J6B        & 0.11 & $\pm$ [0.02, 0.03] \\ 
    BERT-medium   & 0.60 & $\pm$ [0.01, 0.02] & OpenAI-GPT     & 0.07 & $\pm$ [0.04, 0.03] \\ 
    BERT-base     & 0.51 & $\pm$ [0.09, 0.07] & GPT2-XL        & 0.12 & $\pm$ [0.03, 0.05] \\ 
    BERT-large    & 0.44 & $\pm$ [0.03, 0.08] & DistilGPT2     & 0.19 & $\pm$ [0.05, 0.05] \\ 
    DistilBERT    & 0.43 & $\pm$ [0.10, 0.13] & GPT-Neo125M    & 0.14 & $\pm$ [0.09, 0.14] \\ 
    BERT-2L-128   & 0.77 & $\pm$ [0.07, 0.07] & GPT-Neo1.3B    & 0.14 & $\pm$ [0.03, 0.03] \\ 
    BERT-4L-256   & 0.62 & $\pm$ [0.03, 0.05] & GPT-Neo2.7B    & 0.14 & $\pm$ [0.03, 0.02] \\ 
    BERT-4L-512   & 0.69 & $\pm$ [0.10, 0.08] & GPT-J6B        & 0.11 & $\pm$ [0.02, 0.03] \\ 
    BERT-8L-512   & 0.60 & $\pm$ [0.01, 0.02] & LLaMA2-7B      & 0.12 & $\pm$ [0.02, 0.03] \\ 
    BERT-base     & 0.51 & $\pm$ [0.09, 0.07] & LLaMA2-13B     & 0.17 & $\pm$ [0.02, 0.02] \\ 
    BERT-large    & 0.44 & $\pm$ [0.03, 0.08] & LLaMA3-8B      & 0.00 & $\pm$ [0.00, 0.01] \\ 
    DistilBERT    & 0.43 & $\pm$ [0.10, 0.13] & LLaMA3.1-8B    & 0.00 & $\pm$ [0.00, 0.01] \\ 
    BEiT-base     & 0.40 & $\pm$ [0.08, 0.02] & LLaMA3.2-8B    & 0.01 & $\pm$ [0.01, 0.01] \\ 
    BEiT-large    & 0.33 & $\pm$ [0.05, 0.07] & LLaMA3.2-1B    & 0.01 & $\pm$ [0.01, 0.01] \\ 
    BEiT-base     & 0.39 & $\pm$ [0.23, 0.07] & LLaMA3.2-3B    & 0.01 & $\pm$ [0.01, 0.01] \\ 
    BEiT-large    & 0.26 & $\pm$ [0.17, 0.13] & LLaMA2-7B-chat & 0.12 & $\pm$ [0.02, 0.03] \\ 
    BEiT-base     & 0.39 & $\pm$ [0.23, 0.07] & LLaMA2-70B     & 0.02 & $\pm$ [0.01, 0.02] \\ 
    BEiT-large    & 0.26 & $\pm$ [0.17, 0.13] & LLaMA2-7B-chat & 0.12 & $\pm$ [0.02, 0.03] \\ 
    BEiT-large    & 0.26 & $\pm$ [0.17, 0.13] & LLaMA2-13B-chat & 0.17 & $\pm$ [0.02, 0.02] \\ 
    ALBERT-base   & 0.72 & $\pm$ [0.00, 0.00] & LLaMA3-8B      & 0.00 & $\pm$ [0.00, 0.00] \\ 
    ALBERT-large  & 0.70 & $\pm$ [0.00, 0.00] & LLaMA3-70B     & 0.02 & $\pm$ [0.01, 0.01] \\ 
    ALBERT-xlarge & 0.59 & $\pm$ [0.00, 0.00] & LLaMA3.1-8B    & 0.00 & $\pm$ [0.00, 0.01] \\ 
    ALBERT-xxlarge & 0.46 & $\pm$ [0.00, 0.00] & LLaMA3.1-70B   & 0.01 & $\pm$ [0.00, 0.01] \\ 
    RoBERTa-base  & 0.49 & $\pm$ [0.03, 0.06] & LLaMA3.1-405B  & 0.03 & $\pm$ [0.01, 0.03] \\ 
    RoBERTa-large & 0.47 & $\pm$ [0.06, 0.06] & LLaMA3.2-1B    & 0.00 & $\pm$ [0.00, 0.00] \\ 
    XLM-R-base    & 0.51 & $\pm$ [0.05, 0.03] & LLaMA3.2-3B    & 0.01 & $\pm$ [0.01, 0.01] \\ 
    XLM-R-large   & 0.49 & $\pm$ [0.16, 0.12] & Mistral-7B     & 0.00 & $\pm$ [0.00, 0.01] \\ 
    RoBERTa-mnli  & 0.47 & $\pm$ [0.06, 0.06] & Mixtral-8x22B  & 0.00 & $\pm$ [0.00, 0.00] \\ 
    DistilRoBERTa & 0.53 & $\pm$ [0.02, 0.06] & MobileLLM125M  & 0.03 & $\pm$ [0.02, 0.03] \\ 
    ModernBERT-base & 0.18 & $\pm$ [0.06, 0.18] & MobileLLM350M & 0.01 & $\pm$ [0.01, 0.01] \\ 
    GPT1          & 0.07 & $\pm$ [0.04, 0.03] & Phi-1.5        & 0.09 & $\pm$ [0.03, 0.03] \\ 
    GPT2          & 0.15 & $\pm$ [0.02, 0.03] & Phi-1          & 0.14 & $\pm$ [0.02, 0.01] \\ 
    GPT2-medium   & 0.17 & $\pm$ [0.03, 0.05] & Phi-2          & 0.07 & $\pm$ [0.03, 0.06] \\ 
    \bottomrule
\end{tabular}
\end{table}
