\section{Experiments}
\subsection{Experimental setup}

%The evaluation uses the first 3-second vocal clip of the ground truth as the reference voice for all our models.
\begin{table*}[t]
\caption{Objective and Subjective evaluation of Text-to-Song generation. * denotes that we finetune Parlet-tts using our training data. The overall first and second results are marked with \textbf{bold} and \underline{underline}, respectively. The top subjective results in both of our generation modes are highlighted in \textcolor{yellow}{yellow}.}
\label{table:t2s}
\begin{center}
\resizebox{1.95\columnwidth}{!}{
\begin{tabular}{l|lccccc|ccccc}
\toprule
\multicolumn{2}{l}{Model} & FAD $\downarrow$ & KL $\downarrow$ & CLAP $\uparrow$    & PER $\downarrow$ & SECS $\uparrow$  & OVL.$\uparrow$ & REL.$\uparrow$ &VQ.$\uparrow$ & HAM. $\uparrow$  &SS. $\uparrow$\\ 
\midrule
\multicolumn{2}{l}{Ground Truth} &- & - & -  &- & - &\textbf{4.57} 	&\textbf{4.49} 	&\textbf{4.49} 	&\textbf{4.47} 	&\textbf{4.58}  \\
\multicolumn{2}{l}{Suno} &- &- &- &- &- &\underline{4.28} 	&3.31 	&\underline{4.22} 	&\underline{4.33}  &-\\

\midrule
\multicolumn{2}{l}{Stable Audio Open~\cite{evans2024stable}} &4.87 &1.15 &0.28  &- &- &3.01 &2.87 &1.29 &- &-\\
\multicolumn{2}{l}{MusicGen~\cite{copet2024musicgen}} &5.17 &0.89 &0.09 &- &- &3.15 &2.44 &- &- &-\\
\multicolumn{2}{l}{Parler-tts*~\cite{lyth2024parler}}  &4.13 &1.00 &0.19 &58.61 &64.37 &2.58 &2.13 &2.28 &2.35 &-\\
\midrule
% \multicolumn{11}{c}{\textit{Ours}} \\
\midrule

\multirow{2}{*}{\rotatebox{90}{Mixed}}

&Mixed &1.74 &0.71  &\textbf{0.35}   &51.84  &73.69 &3.58 	&3.70 	&3.55 	&3.39 	&3.92  \\

% \rowcolor{gray!15}
&\textbf{Mixed pro (ours)}  &\textbf{1.71} &\textbf{0.69} &\textbf{0.35} &\underline{40.58} &\textbf{73.78} &\cellcolor[HTML]{FFFF99}3.96 	&3.86 	&4.07 	&\cellcolor[HTML]{FFFF99}4.01 	&\cellcolor[HTML]{FFFF99}\underline{4.04} \\
\midrule
\multirow{5}{*}{\rotatebox{90}{Dual-track}}
&Parallel (standard)  &2.45 &0.75 &0.33  &48.40 &72.27 &3.19 &3.27 & 3.36 &2.98 &3.44 \\
&Parallel (V-A) &2.54 &0.73 &0.33  &46.30 &72.43 &3.36 &3.32 &3.48 &3.08 &3.47\\
&Parallel (A-V)  &2.31 &0.72 &0.34  &47.00 &72.50 &3.40 	&3.33 	&3.51 	&3.21 	&3.51 \\
 \cmidrule{2-12}
&Interleaving (V-A)  &1.96 &0.71 &0.34  &41.82&73.12 &3.77 &3.69 &3.98 &3.65 &3.88\\
% Interleaving (A-V)  &1.78 &0.92 &1.11 &1.15 &2.33 &0.73 &0.33 &0.23 &66.59 &70.61\\
% \rowcolor{gray!15}
&\textbf{Interleaving (A-V) (ours)}   &\underline{1.87} &\textbf{0.69} &\textbf{0.35}  &\textbf{39.46} &\underline{73.16} &3.95 	&\cellcolor[HTML]{FFFF99}\underline{3.87} 	&\cellcolor[HTML]{FFFF99}4.15 	&3.82 	&3.93 
\\

\bottomrule
\end{tabular}
}
\vspace{-15pt}
\end{center}
\end{table*}


 





\noindent\textbf{Baselines.}
To the best of our knowledge, no open-source text-to-song model is currently available. We use two state-of-the-art text-to-music models as baselines: Stable Audio Open \cite{evans2024stable} and MusicGen~\cite{copet2024musicgen}, both of which generate instrumental music from text.
Additionally, we fine-tune Parler-tts~\cite{lyth2024parler}, a text-to-speech model that generates speech from both transcript and description texts, using our own training data.
We also compare our model with Suno, a commercial product, using subjective evaluations.
% Samples from these models are also provided on our demo page.
%~\footnote{https://suno.com/}

\noindent\textbf{Evaluation dataset and metrics.}
For the evaluation dataset, we filter the English-voiced song samples from MusicCaps benchmark \cite{agostinelli2023musiclm}, yielding a test set of 326 samples, with the lyrics annotated by our preprocessing pipeline. 
We conduct both objective and subjective evaluations.
% Five objective metrics are used to assess the generated audios: 
For \textbf{objective evaluations}, Frechet Audio Distance (FAD) measures the generation fidelity; Kullback-Leibler Divergence (KL) evaluates conceptual similarity with the target audio; CLAP Score measures the alignment between the audio and the text description; Speaker Embedding Cosine Similarity (SECS) assesses the similarity of speaker identity; Phoneme Error Rate (PER) gauges adherence to the provided lyrics. Note that due to limitations in the ASR model, the PER values are higher than the actual error rate, but the relative differences remain meaningful.
% It is important to note that due to limitations in the ASR model, the PER values are higher than the actual error rate, but the relative differences remain meaningful. 
For each method, we generate the audio five times with different random seeds and report the average metric.
%将wer改成phoneme, 参考SongEditor: Adapting Zero-Shot Song Generation Language Model as a Multi-Task Editor
For \textbf{subjective evaluations}, we employ Mean Opinion Score (MOS) tests, assessing five key aspects: overall quality (OVL.), focusing on musicality and naturalness; relevance to the text description (REL.); vocal quality, with an emphasis on clarity and intelligibility (VQ.); harmony between vocals and accompaniment (HAM.); and similarity to the original singer (SS.).
The appendix \ref{sec: eval_detail} shows details of the evaluations.
% Moreover, ABX preference tests are also conducted.

\subsection{Results of Text-to-song Generation}
As shown in Table \ref{table:t2s}, we compare our models, including mixed mode and dual-track mode, with several baselines. For all our models in this table, we use the first 3-second vocal clip of the ground truth as the reference voice.

\noindent\textbf{Comparison with Baselines.}
SongGen significantly outperforms Stable Audio Open, MusicGen, and Parler-tts across both subjective and objective metrics. 
% Notably, MusicGen receives very low CLAP scores, which can be attributed to the 
The test set contains many voice-related descriptions, such as “A group of female vocalists sings this energetic swing song.” However, MusicGen generates pure music, lacking any vocal signals, which results in exceptionally low CLAP scores. Stable Audio Open generates some vocal signals based on the input text, but these signals do not form recognizable words.
Although Parler-tts has achieved remarkable success in controllable text-to-speech tasks, fine-tuning it for the text-to-song task proves to be ineffective. This highlights the greater complexity of the text-to-song task compared to text-to-speech.
%https://songgen66.github.io/demos/

% \noindent\textbf{Comparison with Ground Truth and Suno.}
Although SongGen shows some gaps when compared to Ground Truth and Suno, it is important to note that we use only 2k hours of labeled data, sourced from publicly available datasets. Despite the limited data, SongGen achieves competitive performance on an out-of-domain test dataset. Figure \ref{fig:vibrato} shows a mel-spectrogram of our generated songs, demonstrating that SongGen produces songs with various singing techniques like vibrato. Compared to Suno, a commercial product, SongGen outperforms in terms of text relevance and vocal control. Suno struggles to adhere to the highly detailed textual descriptions in MusicCaps (as shown by the REL. metric) and lacks voice cloning support, which gives our model a clear advantage in these aspects.

\begin{figure}[tb!]
	\centering
	\includegraphics[width=0.95\columnwidth]{Figure/vibrato.png}
    \vspace{-15pt}
	\caption{Mel-spectrogram visualization of our generated song featuring various singing techniques. }
	\label{fig:vibrato}
    \vspace{-18pt}
\end{figure}




\textbf{Mixed Mode and Dual-Track Mode.}
We further analyze the performance of the mixed mode and dual-track mode of our framework. 
In mixed mode generation, the "Mixed Pro" approach outperforms the basic "Mixed" model across all metrics, particularly in vocal quality (as indicated by the PER and VQ.).
It indicates that by incorporating an auxiliary vocal token prediction target, the learning biases in mixed mode are effectively mitigated.
% It confirms that the information imbalance between vocals and accompaniment in the mixed mode causes learning biases. By incorporating an auxiliary vocal token prediction target, these biases are effectively mitigated. 

 In dual-track mode, the ``Interleaving (A-V)” pattern obtains the best performance.
 Although the parallel pattern is more computationally efficient, its performance lags behind the interleaving pattern. This is likely because, in parallel mode, each hidden state mixes vocals and accompaniment, making separation difficult with only two linear heads. 
 % This is likely because, in parallel mode, each hidden state contains a mixture of vocals and accompaniment information, making it difficult to separate them using only two linear heads. % for the final hidden states.
 Interestingly, regardless of the pattern (parallel or interleaving), placing the accompaniment before the vocals leads to better results than the reverse order.






  
  % This is likely because in the parallel mode, each hidden state contains a mix of vocals and accompaniment, and separating them based only on the two language model heads for the final hidden states proves to be difficult. In contrast, the interleaving model alternately outputs vocal and accompaniment tokens, allowing the model to internally decouple these two components while accounting for their mutual influence.
 %原因？
%This suggests that processing the accompaniment prior to the vocal allows the model to focus more on generating the vocal, improving overall audio quality and musicality. This also highlights the importance of the order in which the accompaniment is handled in the generation process, as it directly impacts the harmony and coherence of the final output.

% ``Interleaving (A-V)” achieves the best performance in dual-track mode.
Compared to ``Mixed pro",``Interleaving (A-V)" shows competitive performance, with only slightly worse result in FAD. Further comparison reveals that ``Interleaving (A-V)" achieves better vocal quality (VQ.), but its harmony (HAM.) is slightly inferior to that of the ``Mixed pro". This highlights the distinct advantages and challenges of each generation mode. 
We further visualize the attention scores in the decoder to explore the internal mechanisms of the transformer in both modes. Figures \ref{fig:attan} (a),(b), and (c) show self-attention over 500 steps in layer 18 for the ``mixed pro", and over 50 steps in layers 8 and 21. Figures \ref{fig:attan} (a),(b), and (c) present the same for the ``interleaving (A-V)" pattern. From (a) and (d), we observe evenly spaced parallel lines along the diagonal. 
% In the ``interleaving (A-V)", the spacing between these lines is roughly twice as wide as in the ``mixed pro", which corresponds to the fact that the sequence length in interleaving pattern is double that of the mixed mode. 
Since songs typically have repetitive structures, this attention pattern suggests that our model has effectively learned the underlying structure of music. Interestingly, in (f), the attention follows a checkerboard pattern, where attention scores for odd steps are strong with other odd steps and similarly, for even steps. This indicates that in the ``interleaving (A-V)" mode, higher layers focus more on learning intra-track relationships, while lower layers (shown in (c)) capture inter-track interactions.



% shows slightly worse results in terms of FAD and KL, likely due to inherent biases in the input data introduced by the source separation tool. Furthermore, most dual-track models typically add the waveform of the vocal and accompaniment together as the final audio result. However, in real-world music production, mixing is a complex and specialized process. This simplistic addition introduces distortion.
% Nevertheless, in subjective metrics, Interleaving (A-V) performs comparably to the mixed mode in terms of overall quality (OVL). 


\begin{figure}[tb!]
	\centering
	\includegraphics[width=0.98\columnwidth]{Figure/attan.png}
    \vspace{-10pt}
	\caption{Visualization of decoder attention.}
    \vspace{-15pt}
	\label{fig:attan}
\end{figure}

\begin{table}[t]
\caption{Text-to-Song results without voice input.}

\label{table:wo_voice}
\begin{center}
\resizebox{0.95\columnwidth}{!}{
\begin{tabular}{lccccc}
\toprule
Model & FAD $\downarrow$    & OVL.$\uparrow$ & REL.$\uparrow$ &VQ.$\uparrow$ & HAM. $\uparrow$   \\
\midrule
mixed  pro &1.96 &3.72 	&3.48 	&3.88 	&3.87\\
inter.(A-V) &2.21 &3.70 &3.47 &3.91 &3.83 \\

\bottomrule
\end{tabular}
}
\vspace{-20pt}
\end{center}
\end{table}



\begin{table}[t]
\caption{Ablation results on training scheme. HQFT is short for High-Quality Finetuning and CL stands for curriculum learning.}
\vspace{-3pt}
\label{table:loss}
\begin{center}
\resizebox{0.92\columnwidth}{!}{
\begin{tabular}{lccccc}
\toprule
Model & FAD $\downarrow$ & KL $\downarrow$ & CLAP $\uparrow$   & PER $\downarrow$ & SECS $\uparrow$  \\
\midrule
w/o HQFT &2.01 &0.72 &0.32 &43.68 &72.83 \\
w/o CL &2.35 &0.73 &0.33 &55.71  &72.81 \\
\midrule
\textbf{ours}  &\textbf{1.71} &\textbf{0.69} &\textbf{0.35} & \textbf{40.58} &\textbf{73.78}\\ 
\bottomrule
\end{tabular}
}
\vspace{-18pt}
\end{center}
\end{table}



\begin{table}[t]
\caption{Ablation results on different lyric integration methods.}
\vspace{-12pt}
\label{table:lyrics}
\begin{center}
\resizebox{\columnwidth}{!}{
\begin{tabular}{ccccccccc}
\toprule
Tokenizer & \makecell[c]{w/ Lyrics \\Encoder} & \makecell[c]{prepend \\/ cross} & FAD $\downarrow$ & PER $\downarrow$ & SECS $\uparrow$ \\
  
% Tokenizer & LyricsEnocder & prepend / cross & FAD$\downarrow$  & PER $\downarrow$ & SECS $\uparrow$  \\
\midrule
VoiceBPE & \xmark &prepend   &3.41   &62.38 &69.09\\
VoiceBPE & \cmark &prepend &3.56   &56.21 &70.70\\ 
VoiceBPE &\xmark &cross &1.95  &61.81 &72.59\\
T5  &\cmark  &cross &1.88  &55.27 &73.67\\
\midrule
VoiceBPE &\cmark &cross &\textbf{1.73}  &\textbf{43.34} &\textbf{73.59}\\
\bottomrule
\end{tabular}
}
\vspace{-20pt}
\end{center}
\end{table}


\noindent\textbf{Without a reference voice.}
We explore the song generation capability of SongGen without a reference voice. Table \ref{table:wo_voice} shows that performance declines slightly. However, the listening test results demonstrate that the model continues to produce enjoyable songs with coherent vocals. %NOTE 不解释rel了
% Although the relevance to the text description slightly diminishes—likely due to the highly detailed descriptions in the MusicCaps test set—the overall output remains impressive.









\subsection{Ablation Studies}
In this section, we conduct extensive ablation studies. Since both mode are based on a unified framework, we present results from the mixed mode setting due to space limitations.
% , as the two modes have minimal impact on the results.

\noindent\textbf{Effect of training strategy.}
In Table \ref{table:loss}, we evaluate the effectiveness of our High-Quality Finetuning (HQFT) and curriculum learning (CL) strategy for codebook loss weights. HQFT improves all metrics, confirming the effectiveness of our quality filtering criteria. 
% However, overly strict filtering can reduce training data and limit musical style diversity. For example, rock music often has less clear vocals, leading to higher edit distances. If the threshold is too strict, such songs might be excluded. Therefore, using a more relaxed quality filter in Stage 1 and applying HQFT in Stage 3 helps the model generalize while improving song quality.
Compared to the ``w/o CL" variant, where each codebook's loss weight is fixed and equal, our CL strategy improves performance. This demonstrates that prioritizing the most important tasks first and then progressively refining the details is effective.






\noindent\textbf{Effect of Lyrics Module Design.}
% We further investigate the impact of different lyric integration methods, considering factors including the choice between a phoneme-like tokenizer (VoiceBPE) and a word-level tokenizer (T5), the inclusion of a lyrics encoder, and the integration approach (pre-pending or cross-attention).
We further investigate the impact of different lyric integration methods, including the choice of tokenizer (VoiceBPE vs. T5), the use of a lyrics encoder, and the integration approach (pre-pending vs. cross-attention). Figure \ref{table:lyrics} shows the results after Step 1 training for each variant. Our design (VoiceBPE, w/ lyrics encoder, cross-attention) achieves the best results across all metrics, validating the effectiveness. Unlike most TTS works, which prepend transcripts before audio tokens, we find that the cross-attention approach is more effective and stable. This may be because cross-attention allows the decoder to focus solely on generating the audio modality. Additionally, phoneme-like tokenizer (VoiceBPE) is more suitable than the word-level tokenizer (T5) for song lyric tokenization. Under this mechanism, the lyrics encoder can capture the relationships between lyric tokens, learning pronunciation patterns from different token combinations, and thus alleviating the burden of modality alignment on the decoder. 













