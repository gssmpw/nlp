%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\redtext}[1]{\textcolor{red}{#1}} 
% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

%Lzh add
\usepackage{amssymb}
\usepackage{color}
\usepackage{colortbl}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{bbding}
\usepackage{wrapfig}
\usepackage{amsfonts}
\usepackage{pifont} %http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}% %
\newcommand{\xmark}{\ding{55}}%


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{SongGen: A Single Stage Auto-regressive Transformer for Text-to-Song Generation}

\begin{document}

\twocolumn[
\icmltitle{SongGen: A Single Stage Auto-regressive Transformer \\for Text-to-Song Generation}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.


\begin{icmlauthorlist}
\icmlauthor{Zihan Liu}{buaa,shlab}
\icmlauthor{Shuangrui Ding}{cuhk}
\icmlauthor{Zhixiong Zhang}{shlab}
\icmlauthor{Xiaoyi Dong}{shlab}
\icmlauthor{Pan Zhang}{shlab}
\icmlauthor{Yuhang Zang}{shlab}
\icmlauthor{Yuhang Cao}{shlab}
\icmlauthor{Dahua Lin}{cuhk,shlab}
\icmlauthor{Jiaqi Wang}{shlab}
\end{icmlauthorlist}

\icmlaffiliation{buaa}{Beihang University, Beijing, China}
\icmlaffiliation{shlab}{Shanghai AI Laboratory, Shanghai, China}
\icmlaffiliation{cuhk}{The Chinese University of Hong Kong, Hong Kong, China}

\icmlcorrespondingauthor{Jiaqi Wang}{wangjiaqi@pjlab.org.cn}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}

% Text-to-song generation, the task of creating harmonized vocals and accompaniment from textual descriptions, is challenging due to domain complexity and data scarcity. Previous methods using multi-stage generation processes lead to cumbersome training and inference pipelines. In this paper, we introduce SongGen, a fully open-sourced single-stage text-to-song model. Our model allows natural language control over various musical aspects, including lyrics and text descriptions, such as instruments, genre, mood, and timbre, with an optional 3-second reference clip for voice cloning. We propose a general framework supporting two-generation modes: mixed-mode and dual-track mode. Innovatively, we introduce an auxiliary vocal loss in mixed-mode, significantly improving vocal quality. Additionally, we design efficient track combination patterns to ensure harmony between vocals and accompaniment in dual-track mode. We also develop an automated data preprocessing pipeline with effective quality filters. We will release our model code, weights, preprocessing pipeline, and annotated data to provide a simple and effective baseline, fostering community development. Audio samples are available at [link].

% Text-to-song generation, the creation of harmonized vocals and accompaniment from textual descriptions, is a challenging task due to the complexity of vocals, the need for high-quality paired datasets, and the harmony between vocals and accompaniment. In this paper, we introduce SongGen, a simple and controllable model that generates high-quality songs from lyrics and descriptive text, with the ability to control various aspects such as instruments, genre, mood, timbre, and more. Moreover, SongGen supports zero-shot voice cloning with just a 3-second reference vocal clip. The model utilizes a single-stage autoregressive language model and an off-the-shelf neural semantic audio codec, enabling a unified framework and fast inference. We also address the challenge of generating clear and musically coherent vocals by proposing a novel dual-track modeling strategy that generates separate vocal and accompaniment tracks simultaneously within a single decoder. We make our model's code, weights, annotated dataset, and data processing pipeline publicly available to promote reproducibility and encourage further research in the field of text-to-song generation.

Text-to-song generation, the task of creating vocals and accompaniment from textual inputs, poses significant challenges due to domain complexity and data scarcity. Existing approaches often employ multi-stage generation procedures, resulting in cumbersome training and inference pipelines. In this paper, we propose \textbf{SongGen}, a fully open-source, single-stage auto-regressive transformer designed for controllable song generation. The proposed model facilitates fine-grained control over diverse musical attributes, including lyrics and textual descriptions of instrumentation, genre, mood, and timbre, while also offering an optional three-second reference clip for voice cloning. Within a unified auto-regressive framework, SongGen supports two output modes: \textbf{mixed mode}, which generates a mixture of vocals and accompaniment directly, and \textbf{dual-track mode}, which synthesizes them separately for greater flexibility in downstream applications. We explore diverse token pattern strategies for each mode, leading to notable improvements and valuable insights. Furthermore, we design an automated data preprocessing pipeline with effective quality control. To foster community engagement and future research, we will release our model weights, training code, annotated data, and preprocessing pipeline.
The generated samples are showcased on our project page at \url{https://liuzh-19.github.io/SongGen/}, and the code will be available at \url{https://github.com/LiuZH-19/SongGen}.
% Audio samples are available at \url{https://songgen66.github.io/demos/}.

%, facilitating streamlined model training
\end{abstract}

\input{1-Introduction}
\input{2-Related_work}
\input{3-Methodology}
\input{4-Experiments}
\input{5-Conclusion}



\clearpage
\section*{Impact Statement}
The proposed work, SongGen, a controllable text-to-song generation model, has the potential to impact various aspects of society. On the positive side, SongGen enables both content creators and novices to effortlessly express their creativity with a low entry barrier, while also streamlining the workflow for experienced music producers.

However, since SongGen autonomously generates songs and supports voice cloning, there are risks of copyright infringement, intellectual property misuse, and the creation of deepfake audio. Proper constraints are needed to ensure the model is not misused in illegal or unethical ways.

In conclusion, while SongGen presents exciting possibilities for the music industry and creative expression, its development should be accompanied by careful consideration of its ethical and societal implications

% We have conducted a preliminary memorization analysis shown in Figure~\ref{fig:mem}. However, proper measures still need to be in place to ensure that the generated songs adhere to copyright laws and protect the rights of original composers and authors.



% \nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\input{Appendix}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
