\section{Introduction}
% Song, combining vocals and accompaniment, is an essential component of music. Unlike instrumental music, songs uniquely express human emotions through emotive lyrics and diverse melodies. However, creating a song is a complex, multi-stage process involving composition, instrumental arrangement, and vocal performance, among other tasks. This process requires substantial time and expertise, making it a challenging endeavor for most individuals.
% In recent years, the rise of Artificial Intelligence Generated Content (AIGC) has revolutionized creative domains, extending from text, image, and speech generation to more intricate forms of art, such as music.
% Inspired by the success of AIGC in other areas, text-to-song generative models aim to produce full song audio from natural language descriptions, lowering barriers for novices and enhancing efficiency of creative processes.

Songs, blending vocals with instrumental accompaniment, are a cornerstone of musical expression. Unlike purely instrumental music, songs uniquely capture human emotions through emotive lyrics and diverse melodies. However, creating a song is a complex, multi-stage process involving composition, instrumental arrangement, vocal performance, and more. This process requires substantial time and expertise, making it challenging for most individuals.
With the rise of AI Generated Content (AIGC), creative fields have been revolutionized, extending from text and image generation~\cite{rombach2022high, zhang2023adding,achiam2023gpt} to sophisticated artistic domains like music~\cite{huang2018music, dhariwal2020jukebox,ji2020comprehensive}. Building on these advancements, text-to-song generative models aim to transform natural language descriptions into full-song audio, making music creation more accessible and efficient.




\begin{figure}[tb!]
	\centering
	\includegraphics[width=0.98\columnwidth]{Figure/motivation.png}
    \vspace{-10pt}
	\caption{
    % Previous approaches usually adopt multi-stage processes, resulting in inflexible and cumbersome pipelines. SongGen simplifies this with a single-stage autoregressive transformer, supporting output of both mixed mode and dual-track mode.
    Traditional methods often rely on multi-stage processes, making pipelines inflexible and complex. SongGen simplifies this with a single-stage auto-regressive transformer that supports both mixed mode and dual-track mode song generation.
    }
	\label{fig:motivation}
    \vspace{-10pt}
\end{figure}

% Despite being an emerging field, text-to-song generation remains in its early stages, with 
% Due to the greater domain complexity compared to speech or instrumental music, along with the scarcity of open-source data, research in this area remains limited.
% Existing methods \cite{hong2024Melodist, li2024melodyLM}, decouple the vocals and accompaniment into separate tracks and employ multi-stage generation processes. As illustrated in Figure~\ref{fig:motivation} (top-left), these models first generate the vocal track based on the lyrics, and subsequently create the accompaniment using natural language prompts alongside the previously generated vocal track.
% However, multi-stage processes result in cumbersome training and inference pipelines and fail to simultaneously control both vocals and accompaniment.
%SongCreator 严格意义上来说不算 text-to-Song 他不支持文本控制，
% Another model, Song Creator, utilizes a dual-sequence language model to separately model vocals and accompaniment information, and employs a dynamic bidirectional cross-attention module to capture the influences between these two sequences. But it lacks text-based control and produces vocals with limited clarity. 
% To enable faster inference and more flexible control, a natural question arises: Could a simple single-stage model achieve text-to-song generation?
% Could a simple single-stage language model achieve similar performance in text-to-song generation? 

Song generation presents greater complexity than speech or instrumental music generation~\cite{lyth2024parler, chen2024valle2valle2,liu2024audioldm, copet2024musicgen}, and the scarcity of open-source data further limits research in this area.
Current approaches~\cite{hong2024Melodist, li2024melodyLM} tackle this challenge by separating vocals and accompaniment into distinct tracks, relying on multi-stage generation processes. As illustrated in Figure~\ref{fig:motivation}, these models first generate the vocal track from lyrics, then produce the accompaniment using natural language prompts alongside the generated vocals.
However, multi-stage generation results in cumbersome training and inference pipelines while lacking unified control over both vocals and accompaniment. To improve pipeline simplicity and control flexibility, an important question arises: Is it possible for a single-stage model to achieve effective text-to-song generation?


% Our framework is simple but effective, consisting of an off-the-shelf neural semantic audio codec and a single autoregressive language model, which introduces multi-modal user control via cross-attention mechanisms. 
% In this paper, we introduce SongGen, a fully open-source, one-stage text-to-song generation model, for the first time.
% SongGen generates songs with harmonious vocals and accompaniment from lyrics and descriptive text, offering control over instruments, genre, mood, timbre, and more. With a 3-second reference vocal clip, the model can perform zero-shot voice cloning. The generated songs also feature rich vocal techniques, like vibrato, for a more authentic and expressive performance.
% We propose a general framework that supports two generation modes: mixed-mode and dual-track mode.

In this paper, we introduce SongGen, a fully open-source, single-stage text-to-song generation model based on an auto-regressive transformer architecture. SongGen transforms lyrics and descriptive text into songs with harmonized vocals and accompaniment, allowing fine-grained control over instruments, genre, mood, timbre, and other musical elements. With a three-second reference vocal clip, it also supports zero-shot voice cloning. These user-defined controls are incorporated through modal-specific encoders, learnable projectors, and cross-attention mechanisms.  
SongGen offers two flexible generation modes: \textbf{mixed mode}, which blends vocals and accompaniment into a single output, and \textbf{dual-track mode}, which synthesizes them separately to facilitate professional post-production editing.

However, due to the sophisticated relationship between vocals and accompaniment in a song, jointly predicting them with natural expressiveness is a non-trivial task. To this end, we perform extensive explorations into output token patterns, yielding valuable insights. Specifically, (1) in \textbf{mixed mode}, while the model generates high-quality accompaniment, it struggles with natural-sounding vocals. Accompaniment, with higher energy and stable spectral distribution, is easier to produce, whereas vocals, with higher semantic density and a lower signal-to-noise ratio due to overlap, pose a greater challenge. This learning bias makes it difficult to generate vocals with clear lyrics, a problem typically addressed by decoupling and multi-stage methods. To mitigate this issue, we introduce an auxiliary vocal token prediction target, enhancing the model’s focus on vocal features and significantly improving vocal clarity in mixed-token outputs. (2) In \textbf{dual-track mode}, vocals and accompaniment are treated as distinct yet interconnected sequences, generated in sync by a single transformer decoder. We explore various track combination patterns to maintain precise frame-level alignment. Experimental results indicate that the optimal pattern yields well-coordinated vocals and accompaniment, achieving quality on par with mixed-mode generation.


% In mixed-mode, vocals and accompaniment are treated as a single entity, and the model generates a combined track directly. In practice, we observe that while the model is capable of producing high-quality accompaniment, it struggles to generate natural-sounding vocals.
% Accompaniment typically exhibits a higher energy range and a more stable spectral distribution, making it relatively easier to generate. In contrast, vocals tend to have higher semantic density and suffer from a lower signal-to-noise ratio due to overlap with the accompaniment, which makes it challenging for the model to accurately capture vocal features and produce intelligible lyrics. This ``learning bias" presents a key challenge in mixed-mode song generation, a problem typically addressed by the decoupling and multi-stage approaches mentioned earlier. 
% Building on this observation and analysis, we introduce an auxiliary vocal loss to enhance the model's focus on and learning of the vocal component. Experimental results show that the introduction of this loss improves vocal quality by a large margin.

% Furthermore, to facilitate post-production editing by professionals working with the generated vocal and accompaniment tracks, our model also supports dual-track mode. In dual-track mode, vocals and accompaniment are treated as separate yet interrelated sequences, with both tracks generated simultaneously within a single transformer decoder.  
% We propose several track combination patterns to ensure frame-level alignment between the two tracks. Extensive experiments validate that the most effective pattern generates harmoniously coordinated vocals and accompaniment, with overall quality competitive with that of the mixed-mode generation.

Moreover, the text-to-song generation community has long been constrained by data scarcity. To the best of our knowledge, no publicly available dataset currently includes paired audio, lyrics, and captions. To bridge this gap, we develop an automated pipeline for data cleaning, processing, and quality filtering, resulting in a high-quality dataset of 540K song clips spanning over 2,000 hours of audio. 

To evaluate the effectiveness of the proposed SongGen framework, we conduct extensive experiments on the MusicCaps~\cite{agostinelli2023musiclm} test set, using both objective and subjective evaluations. 
The results demonstrate that SongGen generates songs with excellent musicality and vocal-instrument harmony, achieving performance that is competitive with the ground truth. 
Surprisingly, the generated songs feature expressive vocal techniques, such as vibrato, enhancing naturalness and authenticity.
Our contributions can be summarized as follows:
\vspace{-8pt}
\begin{itemize}
\setlength{\itemsep}{0pt}
% \setlength{\leftmargini}{-5pt}
 \item We introduce SongGen, a single-stage auto-regressive transformer for text-to-song generation, offering versatile control via lyrics, descriptive text, and an optional reference voice.
 \item SongGen supports both mixed and dual-track mode to accommodate diverse requirements. Our experiments provide valuable insights for optimizing both modes.
 \item By releasing the model weights, code, annotated data, and preprocessing pipeline, we aim to establish a simple yet effective baseline for future song generation research.
 % \item  We make the model weights, code, annotated data, and processing pipeline publicly available, promoting reproducibility and encouraging further research in the song
 % generation field.
\end{itemize}






%%%%版本 2.0
% However, compared to other audio generation tasks like text-to-music and text-to-speech, text-to-song generation presents even more profound challenges.
% The complexity of vocals in singing far exceeds that of speech. Beyond merely matching the given lyrics, sung vocals encompass a wide pitch range, varying phoneme durations, and diverse expressive techniques. Moreover, as two essential components of a song, vocals and accompaniment must not only retain their individual musicality but also harmonize with each other.
% Despite being an emerging field, song generation remains in its early stages, with only limited research addressing these challenges. Current approaches for song generation can be broadly categorized based on how they handle the relationship between vocals and accompaniment: one treats the vocals and accompaniment as a unified entity (mixed-mode), while the other separates them into distinct tracks (dual-track mode).

% The mixed-mode approach, exemplified by Jukebox, has demonstrated impressive performance in generating songs by treating vocals and accompaniment as a single entity. However, its controllability is currently limited to specific artist and genre tags. Although promising, this approach relies on a massive dataset of 1.2 million songs and a multi-scale VQ-VAE architecture, resulting in substantial computational costs and long inference times. Additionally, in mixed-mode generation, the vocal component typically suffers from a low signal-to-noise ratio due to its overlap with the accompaniment, making it challenging for models to capture vocal features accurately and produce intelligible lyrics. Since human listeners are particularly sensitive to the clarity and naturalness of the vocal track, this aspect of song generation is critical to producing high-quality outputs. While this issue could potentially be addressed by increasing the dataset size and training time, a more fundamental problem remains:
% there are currently no publicly available datasets with paired data for text-to-song generation, and the annotation process is both difficult and time-consuming. This lack of high-quality annotated data is one of the most significant barriers to advancing research in this field.
 
%  To address the issue of limited data, an alternative approach has emerged, which decouples vocals and accompaniment into separate tracks.
%  Models like Melodist and MelodyLM adopt multi-stage generation processes to mitigate some of the challenges posed by mixed-mode approaches. They initially generating the vocal track from lyrics, followed by the prediction of accompanying music. However, these multi-stage models result in a cumbersome training and inference process and lack the ability to simultaneously control the generation of both vocals and accompaniment.

% Furthermore, these dual-track mode approaches rely on source separation tools, which often introduce errors due to imperfect separation quality. Additionally, these models struggle to ensure fine-grained vocal-accompaniment harmony—a crucial aspect of song generation.

% Currently, the code and datasets for these three models are not publicly available.  
% Although industry tools like Suno and Udio have recently emerged for song
% generation, neither has disclosed their methodologies nor expanded into broader music-related generation tasks with flexible controls.

% In this paper, we introduce SongGen, a fully open, one-stage text-to-song generation model.
% Our framework is simple but effective, consisting of an off-the-shelf neural semantic audio codec and a single autoregressive language model, which introduces user control via cross-attention mechanisms. In this unified framework,we propose two key solutions to enhance vocal quality (clarity and intelligibility) and harmonize vocals with the accompaniment: a mixed-mode approach that improves vocal learning by adding an additional loss term specifically for the vocal track, alongside the mixed audio token generation; and a dual-track modeling mode that generates separate vocal and accompaniment tracks simultaneously at the frame level within a single transformer decoder.
% SongGen is capable of generating songs with harmonious vocals and accompaniment based on lyrics and descriptive text, allowing control over various aspects such as instruments, genre, mood, timbre, and more. Furthermore, with just a 3-second reference vocal clip, our model can perform zero-shot voice cloning. Additionally, the generated songs exhibit rich vocal techniques, such as vibrato, contributing to a more authentic and expressive performance.
% % Due to the lack of publicly available text-to-song datasets, we carefully annotated a large volume of song audios with paired lyrics and captions.  
% Furthermore, we develop an automated pipeline for data cleaning, processing, and quality filtering, which resulted in a high-quality dataset comprising 540K song clips, each paired with lyrics and captions, totaling over 2,000 hours of audio. By making the model weights, code, annotated data, and processing pipeline publicly available, we aim to provide a simple yet effective baseline for future research in the field of text-to-song generation.






%%%%版本 1.0
% Text-to-music (T2M) and text-to-speech (TTS) are closely related tasks. T2M uses descriptive text to guide the creation of instrumental music but struggles to produce realistic vocals. TTS synthesizes speech from transcriptions, though typically with a quieter background. Both tasks have made significant progress in academia and industry in recent years. 

% \begin{figure}[tb!]
% 	\centering
% 	\includegraphics[width=\columnwidth]{Figure/spec3.png}
% 	\caption{}
% 	\label{fig:spec}
% \end{figure}

% However, compared to other audio generation tasks, such as text-to-music and text-to-speech, the development of text-to-song models presents even more significant challenges.
% First,  the complexity of vocals in singing far exceeds that of speech. Beyond merely matching the given lyrics, sung vocals encompass a wide pitch range, varying phoneme durations, and diverse expressive techniques. %（本身很难）
% Moreover, the vocal component in a mixed song typically exhibits a low signal-to-noise ratio due to its overlap with the accompaniment, making it difficult for models to accurately capture and reproduce. Despite this, human listeners are particularly sensitive to the clarity and naturalness of the vocal track, which makes this aspect of song generation crucial for successful output.
% Second, as vital components of a song, vocals and accompaniment must not only retain their individual musicality, but also harmonize with each other.
% Third, training a text-to-song generative model requires paired data, consisting of lyrics, captions and audios. Unfortunately, there are no publicly available datasets with such paired data, and the process of annotation is both difficult and time-consuming. This lack of high-quality annotated data is one of the most intractable barriers to advancing research in this field.
% Given the limited annotated data, ensuring both vocal quality while maintaining harmony between the vocals and accompaniment presents a tremendous challenge. As an emerging field, song generation remains in its early stages, with only limited research addressing this complex issue.


% Jukebox, the first and only publicly available model capable of generating both vocals and accompaniment simultaneously, using a language model in combination with multi-scale VQ-VAE, but with limited control (gener tag and artist name) and long inference time.
%（multi-satge）

% In this paper, we introduce SongGen, a simple and controllable text-to-song generation model. 
% SongGen generates high-quality songs with harmonized vocals and accompaniment, based on lyrics and descriptive text that control aspects such as instruments, genre, mood, timbre, and more.
% Our model consists of two main components: an off-the-shelf neural semantic audio codec and a single-stage autoregressive language model. The audio tokens processed by the codec serve as intermediate representations for sequence modeling, and the language model generates these tokens with various conditioning via cross-attention.
% %one stage的好处是什么？ inference 快
% A major challenge in our approach is ensuring that the one-stage model could generate clear and musically coherent vocals. During training, when generating mixed audio directly, the model focuses on learning the accompaniment but struggles to capture the vocals. This issue arises because the vocal information becomes sparse and less prominent when mixed with the accompaniment, making it difficult for the model to effectively learn the vocals.
% To address this challenge, we proposes two main solutions within our unified framework:
%  The first solution enhances the learning of vocals by adding an additional loss term specifically for the vocal track, alongside the mixed audio token generation. The second solution leverages the fact that vocals and accompaniment are separate but interact with each other. We design a dual-track modeling pattern, where the model generates separate vocal and accompaniment tracks simultaneously in a single transformer decoder. We implemented five variants of this approach and analyzed their performance.
% Due to the lack of publicly available paired song datasets, we carefully annotated a large volume of song audios with paired lyrics and captions.  We developed an automated pipeline for data cleaning, processing, and filtering, resulting in a high-quality dataset of 540K paired clips, totaling over 2,000 hours. 
% Our contributions can be summarized as follows:
% \begin{itemize}

%  \item We propose the first single-stage song generation model that enables flexible control through lyrics, text descriptions, and just 3-second vocal clips for zero-shot voice cloning. %effective zero-shot voice cloning
%  \item We introduce a novel dual-track modeling strategy, which generates separate vocal and accompaniment tracks in harmony using a single Transformer-based decoder.
%  \item  We make the model weights, code, annotated data, and processing pipeline publicly available, promoting reproducibility and encouraging further research in the  text-to-song field.

% \end{itemize}

