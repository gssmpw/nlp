% \section{You \emph{can} have an appendix here.}
% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.


\section{Limitations and Future Work}
We acknowledge the limitations of our proposed SongGen model. Due to the scarcity of open-source song data, the current model can only generate songs up to 30 seconds in length, which is insufficient for producing songs with complete structures. Additionally, the current audio codec, X-Codec, operates at a sampling rate of 16kHz. To improve fidelity, our future work will involve training a renderer to upsample the audio for higher quality output.

\section{Implementation and Training Details}
In SongGen, the lyrics encoder is a 6-layer transformer with a hidden size of 1024. The transformer decoder, consisting of 24 layers with 1024 hidden size, includes both causal self-attention and cross-attention blocks in each layer. In ``Mixed Pro" Mode, the vocal loss weight $\lambda$ is set to 0.1. The model is trained for approximately 400K steps using 16 Nvidia A100 (80GB) GPUs, with a batch size of 16 per GPU. For optimization, we employ the AdamW optimizer~\cite{loshchilov2018decoupled} with $\beta_1 = 0.9$, $\beta_2 = 0.99$, and a weight decay of $10^{-4}$. During training step 1, the learning rate is set to $10^{-4}$, while for the subsequent fine-tuning steps, the learning rate is reduced to $5 \times10^{-5}$. A cosine learning rate schedule is applied for all traning steps. To facilitate reproducibility, we will make our training configurations publicly available.





\section{Details in Evaluations}
For evaluation, we select 326 samples from the MusicCaps \cite{agostinelli2023musiclm} benchmark, with no overlap with the training set. MusicCaps test set contains 2.8K samples, with captions written by expert musicians. However, many of the samples are instrumental music, sound effects, or speech. We filter the English-voiced song samples from MusicCaps test set using our automated data preprocessing pipeline, resulting a final set of 326 samples. Note that the evaluation set was selected impartially, with no intention to influence fairness.

% Note that the evaluation set was selected impartially, with no intention to influence fairness.

\label{sec: eval_detail}
For objective metrics, all samples are normalized at-14dB LUFS for fairness. We use the following five metrics:
\begin{itemize}
    \item Frechet Audio Distance (FAD)~\cite{kilgour2019fad}: Evaluates the fidelity of generated songs by calculating the distribution distance between features of the target and generated audio, extracted from the VGGish~\cite{hershey2017vggish} model.
    \item Kullback-Leibler Divergence (KL): Measures the similarity between the generated and target audio with the label calculated by the audio tagging model. A lower KL suggests that the generated music shares similar concepts with the reference.
     \item CLAP Score: Evaluates the alignment between generated audio and the given text prompt using the official CLAP model~\cite{laionclap2023}. 
    \item Phoneme Error Rate (PER): Assesses the adherence of the generated audio to the provided lyrics by transcribing the audio using Distill Whisper\cite{gandhi2023distilwhisper} and computing the phoneme error rate against the reference lyrics. However, PER is not an ideal measure of vocal quality, as current ASR models struggle with sung vocals.
     \item Speaker Embedding Cosine Similarity (SECS): Assesses the similarity of speaker identity using the Resemblyzer\footnote{Implemented based on: \url{https://github.com/resemble-ai/Resemblyzer}} speaker encoder to compute the SECS between reference 3-second vocal clips and generated audio.
   
    % We use the same CLAP checkpoint as that used in Stable Audio Open~\cite{evans2024stable}.
\end{itemize}
For the subjective evaluations, we randomly select 36 audio samples generated by our models, and each sample is evaluated by 20 listeners. We conduct the commonly used MOS (Mean Opinion Score) tests across five aspects. The rating scale ranges from 1 to 5, with higher scores indicating better performance.
For the Overall Quality (OVL.) evaluation, we instruct the raters to focus on musicality and naturalness, while ignoring style differences.
For the Relevance to Text Description (REL.) evaluation, we ask the raters to score based on the proportion of key points from the text description that are reflected in the generated song.
For the Vocal Quality (VQ.) evaluation, we emphasize the importance of clarity, lyric accuracy, and the naturalness and coherence of the vocals in the ratings.
For Harmony (HAM.), we ask the raters to pay particular attention to the temporal correspondence between the accompaniment and the vocals.
For Speaker Similarity, we ask the raters to focus on the similarity of the speaker's identity (timbre) to the reference, ignoring differences in content.
A small subset of the samples used in the test is available on our anonymous project page \url{https://liuzh-19.github.io/SongGen/}.

% For subjective eveluations, we randomly selected 36 audio samples generated from our models and each sample
%  evaluated by 20 listening subjects. we conduct the MOS (mean opinion score) tests on five aspects. 
%  The rating scale is 1 to 5, where higher scores denote superior performance.


 
% For Overall Quality evaluation, we explicitly instruct the raters to focus on musicality and naturalness, ignore the differences of style. 



%  For Relevance to Text Description, 要求生成的歌曲符合文本描述中的关键点的比例打分。

%  For vocal quality evaluation, 我们强调rater对人声的清晰度、歌词准确性以及人声是否自然连贯进行给分.
%  for harmony ，we ask the rates to focus more
% on temporal correspondence between accompaniment and vocal,
% For speaker similarity evaluation, we ask the
% raters to focus on the similarity of the speaker
% identity (timbre) to the reference and ignore the
% differences in content.

% A small subset of samples used in the test is available at our anonymous project page 
% \begin{figure}[tb!]
% 	\centering
% 	\includegraphics[width=\columnwidth]{Figure/pipeline.png}
% 	\caption{}
% 	\label{fig:pipeline}
% \end{figure}

% \section{Song Caption}
% % 在图7中，我们可是化了song caption在我们训练数据集中




\section{The Impact of Different Audio Codecs.}
We compare the performance of three different codecs: XCodec, Encodec (24kHz)~\cite{defossez2022encodec}, and DAC (44.1kHz)~\cite{kumar2024DAC}. X-Codec, employed in our SongGen, considers both acoustic and semantic information. In contrast, Encodec and DAC have been widely used in previous audio generation systems, particularly in speech~\cite{lyth2024parler} and pure music domains~\cite{copet2024musicgen}, but both primarily focus on acoustic reconstruction.
Table \ref{table:codec} shows the results after training Step 1. X-Codec greatly surpasses both Encodec and DAC on all metrics. Additionally, the loss curves in Figure \ref{fig:codec} demonstrate that X-Codec exhibits more stable training and faster convergence. This indicates that the integration of semantic information into the audio token is highly effective and crucial for the song generation task.

% \begin{table}[t]
% \caption{Ablation results of different neural audio codecs.}
% \label{table:codec}
% \begin{center}
% \resizebox{0.5\columnwidth}{!}{
% \begin{tabular}{lccccc}
% \toprule
% Model & FAD $\downarrow$ & KL $\downarrow$ & CLAP $\uparrow$   & PER $\downarrow$ & SECS $\uparrow$  \\
% \midrule
% Encodec &10.84 &0.99 &0.19 &60.67 &71.36\\ %WER68.86
% DAC &4.36 &0.86 &0.24 &68.64 &71.66\\ %&4.60 &0.83 &0.26  &85.01  &68.94 \\
% \midrule
% X-Codec (ours) &\textbf{1.73} &\textbf{0.70} &\textbf{0.33}  &\textbf{43.34} &\textbf{73.59} \\
% \bottomrule
% \end{tabular}
% }
% \end{center}
% \end{table}

% \begin{figure}[tb!]
% 	\centering
% 	\includegraphics[width=0.45\columnwidth]{Figure/codec.png}
% 	\caption{Training loss curves of different audio codecs}
% 	\label{fig:codec}
% \end{figure}


\begin{figure*}[tb!]
	\centering
	\begin{minipage}{0.49\textwidth}
		\centering
		\begin{table}[H]
			\caption{Ablation results of different neural audio codecs.}
			\label{table:codec}
			\begin{center}
				\resizebox{1\columnwidth}{!}{
					\begin{tabular}{lccccc}
						\toprule
						Model & FAD $\downarrow$ & KL $\downarrow$ & CLAP $\uparrow$ & PER $\downarrow$ & SECS $\uparrow$ \\
						\midrule
						Encodec &10.84 &0.99 &0.19 &60.67 &71.36\\
						DAC &4.36 &0.86 &0.24 &68.64 &71.66\\
						\midrule
						X-Codec (ours) &\textbf{1.73} &\textbf{0.70} &\textbf{0.33} &\textbf{43.34} &\textbf{73.59} \\
						\bottomrule
					\end{tabular}
				}
			\end{center}
		\end{table}
	\end{minipage}
	\hfill
	\begin{minipage}{0.48\textwidth}
		\centering
		\includegraphics[width=0.8\textwidth]{Figure/codec.png}
            \vspace{-6pt}
		\caption{Training loss curves of different audio codecs}
		\label{fig:codec}
	\end{minipage}
\end{figure*}



% \begin{figure}[tb!]
% 	\centering
% 	\includegraphics[width=\columnwidth]{Figure/caption_dist.jpg}
% 	\caption{. }
% 	\label{fig:caption}
% \end{figure}
