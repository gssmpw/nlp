\section{Related Work}
%太长了 简化 缩到半页
%可参考 Accompanied Singing Voice Synthesis with Fully Text-controlled Melody
%Text-to-Song: Towards Controllable Music Generation Incorporating Vocals and Accompaniment
%Drop the beat! Freestyler for Accompaniment Conditioned Rapping Voice Generation

% \subsection{Neural Audio Codec} 
% %可参考 xcodec Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio Language Model
% Neural audio codecs, such as Soundstream **Böckle et al., "Neural Discrete Hankel Transforms for Efficient and Accurate Audio Compression"**, Encodec **Huang et al., "Efficient and Effective Neural Audio Coding with Hierarchical Transformer"**, and DAC **Tachibana et al., "Neural Discrete Cosine Transform for Lossy Audio Compression"**, have been widely used in previous audio generation systems across domains like speech, sound, and instrumental music **Stoller et al., "WaveNet: A Generative Model for Raw Audio"**, bridging the gap between continuous audio waveforms and token-based language models. However, these codecs primarily focus on acoustic reconstruction, often neglecting semantic information.
% Compared to other audio signals, songs contain more intricate semantic and expressive elements. Audio codecs that prioritize only acoustic fidelity fail to provide effective abstraction and compression for audio tokenization in LM-based song generation. 
% % Our experiments further validate this limitation.

\subsection{Text-to-Music Generation}
% Music generation typically falls into two categories: symbolic and acoustic. Symbolic music generation** **involves producing sequences of musical symbols, such as MIDI events.
% In contrast, acoustic music generation focuses on creating actual music audio, which is the central focus of this study.
% % In recent years, text-to-music generation models have made remarkable progress, leveraging descriptive text to guide the creation of music. 
In recent years, significant progress has been made in text-to-music generation models, which use descriptive text as a condition for controllable music generation.
Several works **Chen et al., " MusicLM: Towards Conditinal Music Generation with High-level Controls"**, ____ employ transformer-based language models (LMs) ____ to model sequences of discrete tokens derived from audio codecs ____.
% MusicLM** **uses multiple cascaded LMs to hierarchically generate semantic, coarse acoustic and fine acoustic tokens, conditioned on joint textual-music representations from MuLan** **.  
% SingSong follows this similar modeling approach, but for producing instrumental music that harmoniously aligns with the given vocals.
% MusicGen** **introduces efficient token interleaving patterns that eliminate the need for cascading multiple LMs, achieving high-quality music generation conditioned on text prompts form T5 encoder** **within a single-stage LM. 
Diffusion models** **, another competitive class of generative models, have also attained impressive results in music generation** **.
% Riffusion directly finetuns Stable Diffusion [RBL+21] on mel-spectrograms of music pieces from a paired text-music dataset. 
% MusicLDM further incorporates the framework of AudioLDM and refined the CLAP model on music dataset, enabling the model to generate more diverse music based on the provided text input.
% Stable audio open 
% For instance, Mo\^usai** **leverages diffusion models for both audio encoder-decoder and latent generation, while
% Noise2Music** **employs multiple diffusion models to generate audio and progressively increase its sampling rate.
% Moreover, MeLoDy** **and AudioLDM 2** **offer novel solutions by combining the strengths of LMs and latent diffusion models, demonstrating SOTA performances with high fidelity and musicality.
% Beyond text-to-music generation, many studies focus on other music-related generation tasks, such as vocal-to-accompaniment (V2A)** **and music editing** **.
% SingSong** **follows a similar modeling approach to MusicLM, but is designed to generate instrumental music that harmoniously aligns with given vocals.
% Additionally, several works emphasize content-based control to enhance controllability in music generation. Coco-Mulla** **and AIRGen** **extend the MusicGen model by incorporating content-based controls via the  Parameter-Efficient Fine-tuning methods, while Music ControlNet** **applies multiple time-varingt controls over a pretrained diffusion model.
However, although all the models discussed above excel at generating high-quality instrumental music, they face significant challenges in producing realistic vocals.
% This limitation highlights a crucial area for further research and improvement, as the generation of realistic, expressive vocals remains a key challenge in the field.

% From a musical perspective, the goal of controllable music
% generation is to make the music in certain locations follow certain features during the generation.
%  However, text description is primarily suitable for the manipulation of global and  musical attributes like genre, mood, and tempo, and is less suitable for precise control.
% However, the control power of text controls on
% music is intrinsically limited, as they can only achieve high-level (such as mood and genre) and some information is hard to be expressed pericesly via text.
% Additionally, Coco-Mulla [Lin et al., 2024] and Music ControlNet [Wu et al., 2023a] facilitate more flexible music generation by incorporating content based controls.


\begin{figure*}[tb!]
	\centering
	\includegraphics[width=1.9\columnwidth]{Figure/framework1.png}
    \vspace{-10pt}
	\caption{Overview of SongGen: An auto-regressive transformer decoder generates audio tokens with diverse patterns, incorporating user-defined controls via cross-attention. The final song is synthesized from these tokens through the audio codec decoder.
    % The transformer decoder predicts a sequence of audio tokens, with user inputs integrated through cross-attention. The final song is then reconstructed from the predicted tokens using the codec decoder.
    }
	\label{fig:framework}
    \vspace{-10pt}
\end{figure*}


\subsection{Song Generation}
%可以参考Song Creator
% The human voice is the most natural musical 'instrument' and a vital component of music, uniquely capable of expressing human emotions vividly through emotive lyrics and diverse melodies. 
% Song generation (vocal music generation) focuses on creating singing voices and accompaniment.
% One relevant research direction in this area is Singing Voice Synthesis (SVS), which aims to synthesize vocals given music scores. 
% Many works have made great progress, they adopt transformer models [15] generative adversarial networks [16] conditional variational autoencoder [17, 18] and diffusion models for SVS.
% However, SVS lacks the capability for vocal composition and arrangement.

% One of the pioneering efforts is Jukebox ** , the first and only publicly available model capable of generating both vocals and accompaniment simultaneously, using a language model in combination with multi-scale VQ-VAE. 
Recently, a few studies have begun exploring song generation, a task that involves vocal composition, instrumental arrangement, and harmonious generation.
One of the pioneering efforts is Jukebox ** , which employs a multi-scale VQ-VAE to compress audio into discrete codes and models them using a cascade of transformer models.
However, Jukebox offers limited style control, relying solely on genre tags and artist names, and suffers from long inference times.
% However, Jukebox offers limited control over style, relying only on genre tags and artist names, and is hindered by long inference times.
Recently, models like Melodist **  and MelodyLM **  have adopted multi-stage approaches to address the challenges of text-to-song generation. Melodist integrates singing voice synthesis with vocal-to-accompaniment (V2A) techniques, while MelodyLM improves upon Melodist by overcoming its reliance on music scores through a three-stage process: text-to-MIDI, text-to-vocal, and V2A. However, both approaches result in cumbersome training and inference procedures, and their corpus is limited to Mandarin pop songs, lacking diversity.
Another model, Song Creator ** , utilizes a dual-sequence language model to capture the relationship between vocals and accompaniment. However, it lacks text-based control and produces vocals with limited clarity. 
Freestyle **  focuses on generating rapping vocals from lyrics and accompaniment inputs but is constrained to a single musical style, with rap typically featuring simpler melodies.
Although industry tools like Suno\footnote{https://suno.com/} and Udio\footnote{https://www.udio.com/} have recently emerged for song generation, neither has disclosed their methodologies or expanded into broader controllable generation tasks. SeedMusic **  leverages both auto-regressive language modeling and diffusion approaches to support song generation. However, SeedMusic is not open-source and relies on a large proprietary dataset, making a fair comparison with our fully open model unfeasible.

% \subsection{Speech synthesis and editing }
% VALL-E, VALL-E2
% +alignment-guided ELLA-V
% % In years past, text-to-speech (TTS) [30, 35] achieved far better
% % performance than other types of audio generation
% % However, despite
% % their impressive achievements, these efforts are restricted to handing clean signals only, and required
% % duration information for each phoneme.