\section{Methodology}

% 参考musicLM 和 parler写 
% 都是基于以往模型进行改进


\subsection{Overview}
%认为定义, 给定条件， 生成音乐




% The transformer decoder predicts a sequence of audio tokens, enabling control through various user inputs via cross-attention. The final song is reconstructed from the predicted tokens using the codec decoder. 
The objective of this paper is to guide the generation of a song using a text description, lyrics, and an optional reference voice.
As illustrated in Figure \ref{fig:framework}, SongGen is composed of an auto-regressive transformer decoder with an off-the-shelf neural audio codec. The transformer decoder predicts a sequence of audio tokens, allowing control through user inputs via cross-attention. The final song is synthesized from these tokens using the codec decoder.
In the subsequent section, we will elaborate on the details of SongGen. Section~\ref{sec:mode} will introduce the two generation modes supported by our unified framework: mixed mode and dual-track mode. Section~\ref{sec:cond_model} will discuss the lyric, voice, and text conditions. Section~\ref{sec:pipeline} will outline our data processing pipeline and quality filtering metrics. Section~\ref{sec:train} will present our training scheme for progressively enhancing model performance.


% Next, because the model has a tendency to learn the accompaniment in a shortcut manner due to its repetitive and distinct pattern when generating the two components in one stage, it is difficult to do so. Previous works~\cite{} have used a two-stage approach to achieve this. 
% The auto-regressive language model struggles to learn the vocal in a single stage because of the accompaniment's repetitive pattern, prompting previous works to use a two-stage approach.
% To overcome this, we design two modes to model the mixed song within a single Transformer-based decoder, as presented in section~\ref{sec:mode}.}

% In addition to directly generating mixed song, we introduce a novel dual-track modeling strategy that generates separate vocal and accompaniment tracks harmoniously within a single Transformer-based decoder. 













% \begin{figure}[tb!]
% 	\centering
% 	\includegraphics[width=\columnwidth]{Figure/spec2.png}
% 	\caption{}
% 	\label{fig:spec}
% \end{figure}

% \begin{figure}[tb!]
% 	\centering
% 	\includegraphics[width=\columnwidth]{Figure/spec3.png}
% 	\caption{}
% 	\label{fig:spec}
% \end{figure}

% \begin{figure*}[tb!]
% 	\centering
% 	\includegraphics[width=2\columnwidth]{Figure/spec.png}
% 	\caption{}
% 	\label{fig:spec}
% \end{figure*}


\begin{figure*}[tb!]
	\centering
	\includegraphics[width=2\columnwidth]{Figure/pattern2.png}
    \vspace{-10pt}
	\caption{Illustration of token patterns for different generation modes. The codebook-delay pattern (from MusicGen) is applied to every audio token. (a) \textbf{Mixed Pro}: Directly decoding mixed tokens, with an auxiliary vocal token prediction target to enhance vocal learning.  \textbf{Dual-track mode:} (b) Parallel: Vocal and accompaniment tokens are concatenated along the codebook dimension, with three track order variants. (c) Interleaving: Tokens from both tracks are interleaved along the temporal dimension, with two track order variants.}
	\label{fig:pattern}
    \vspace{-10pt}
\end{figure*}


\subsection{Auto-regressive Codec Language Modeling}
\label{sec:mode}
\subsubsection{Audio tokenization}
The effectiveness of the audio tokenizer is critical to the success of transformer-based song generation.
Our framework is compatible with mainstream Codec designs.
In experiments, we employ X-Codec \cite{ye2024xcodec}, an audio codec based on Residual Vector Quantizer (RVQ)~\cite{zeghidour2021soundstream}, to produce discrete audio tokens.
% X-Codec enriches semantic information while considering acoustic features for each token within a Residual Vector Quantizer (RVQ)~\cite{zeghidour2021soundstream} structure, thereby reducing the model's burden in interpreting tokens and accelerating model convergence.
% The X-Codec compress $f_s = 16$ kHz audio signals into tokens with frame rate $f_r= 50$ Hz . 
It utilizes $N_{q}=8$ codebooks, each with a codebook size of $K=1024$. Given an audio signal $X \in \mathbb{R}^{d \cdot f_s}$, where $d$ is the audio duration and $f_s = 16$ kHz is the sampling rate, X-Codec encodes and quantizes $X$ into a sequence of token vectors $\mathbf{S} = [\mathbf{s}^1, \mathbf{s}^2, \dots, \mathbf{s}^T] \in \mathbb{R}^{N_q \times T}$, where $T= d\cdot f_r$ and $f_r=50$ HZ is the frame rate. Each vector $\mathbf{s}^t = [s^{1,t}, s^{2,t}, \dots, s^{N_q, t}]$ consists of $N_q$ codes, with $s^{k,t}$ taking integer values from 0 to $K-1$ for $k \in [1, N_q]$. 
%The X-Codec decoder is then used for audio reconstruction.
We apply the codebook-delay pattern~\cite{copet2024musicgen} to handle the multiple code sequences within a single transformer decoder architecture. Figure \ref{fig:pattern} at the top-right corner illustrates this process for the case of $N_q = 3$, where a one-step delay is maintained between adjacent sequences from different codebooks. After applying the delay pattern, the resulting code sequences are denoted as $\hat{S}  \in \mathbb{R}^{N_q \times T'}$. 


\subsubsection{Mixed Mode}
%and appending the BOS and EOS special tokens
In mixed mode generation, we directly use the mixed audio tokens $\hat{\mathbf{S}}_{\text{mixed}}$, which are encoded by X-Codec from mixed audio (i.e. raw audio), as the output target.
For each step, the vector of audio tokens 
% $\hat{\mathbf{s}}_{\text{mixed}}^t$
% $\hat{\mathbf{s}}_{\text{mixed}}^t = [\hat{s}_{\text{mixed}}^{1,t}, \hat{s}_{\text{mixed}}^{2,t}, 
% \dots, \hat{s}_{\text{mixed}}^{N_q, t}]$ 
from $N_q$ codebooks are embedded using a group of $N_q$ learnable embedding matrices, and then summed up to form the decoder input. Additionally, a sinusoidal positional embedding is added at each step.
% The input is then fed into the transformer decoder. Each layer consists of a causal self-attention block and a cross-attention block that incorporates the conditioning embedding $E_{\text{cond}}$. 
The last hidden state of decoder is passed to a group of $N_q$ linear heads, with each head predicting the logits corresponding to its respective codebook.

During training, we employ the teacher-forcing scheme. 
Since each quantizer in the RVQ encodes the quantization error from the previous quantizer, earlier codebooks are more critical. Therefore, we compute a weighted sum of the losses from different codebooks, assigning higher importance to the losses from earlier codebooks:
\vspace{-3pt}
\begin{equation}
\begin{aligned} 
\label{eq:mix_loss}
\mathcal{L}_{\text{mixed}} &= \sum_{k=1}^{N_q} w_k \cdot \mathcal{L}^k_{\text{mixed}}, 
% \\
% \mathcal{L}^k_{\text{mixed}} &= - \frac{1}{T'}\sum_{t=1}^{T'} \log q(\hat{s}_{\text{mixed}}^{k,t}), 
\end{aligned} 
\vspace{-3pt}
\end{equation}
where $k$ denotes the codebook index, and $w_k$ represents the weight, satisfying $w_k \leq w_j$ for $ k < j $ and $ \sum_{k=1}^{N_q} w_k=1$. $\mathcal{L}^k_{\text{mixed}}$ is the cross-entropy loss for the $k$-th codebook.
% , and $ q(\hat{s}_{\text{mix}}^{k,t})$ denotes the posterior probability of $\hat{s}_{\text{mix}}^{k,t}$.

%NOTE 
% As illustrated in Figure \ref{fig:spec}, we present the mixed track alongside its accompaniment and vocal tracks, visualized in both waveform and spectrogram form. The accompaniment typically exhibits a higher energy range and more stable spectral distribution. In contrast, the vocal component is sparser, more dynamically variable, and subject to greater instantaneous fre=equency fluctuations, making it inherently more complex and challenging to predict. When mixed audio is used as the training target, the model often prioritizes the more predictable accompaniment, while allocating insufficient attention to the vocal features, which are characterized by a lower signal-to-noise ratio.
% Nevertheless, human perception is particularly sensitive to the naturalness and clarity of vocal tracks—especially for listeners who are not professional musicians—making these aspects of the audio critically important.
However, this basic approach, referred to as ``Mixed", presents challenges in producing coherent and clear vocals. 
In mixed audio, vocals suffer from a low signal-to-noise ratio because of overlap with the accompaniment. While the accompaniment typically exhibits higher energy and a more stable spectral distribution, the vocals tend to be sparser, more irregular, and prone to greater instantaneous frequency fluctuations. For example, vocals often feature rapid pitch changes to perform various singing techniques. Moreover, vocals carry more semantic meaning from the lyrics. When mixed audio is used as the training target, the model tends to prioritize the more predictable accompaniment, often neglecting the vocal features. Nevertheless, human perception is sensitive to the naturalness and clarity of vocals, making these aspects critically important in song generation.
% Addtionaly, the vocal coantin more senmatic information of lyrics .  When mixed audio is used as the training target, the model often prioritizes the more predictable accompaniment, while allocating insufficient attention to the vocal features.Nevertheless, human perception is particularly sensitive to the naturalness and clarity of vocal tracks, making these aspects of the audio critically important.
% The accompaniment typically exhibits a higher energy range and more stable spectral distribution. In contrast, the vocal component is sparser, more dynamically variable, and subject to greater instantaneous frequency fluctuations, making it inherently more complex and challenging to predict. When mixed audio is used as the training target, the model often prioritizes the more predictable accompaniment, while allocating insufficient attention to the vocal features, which are characterized by a lower signal-to-noise ratio. Nevertheless, human perception is particularly sensitive to the naturalness and clarity of vocal tracks, making these aspects of the audio critically important.
% 

Building on this, we propose a method called ``Mixed Pro" that emphasizes vocal learning by introducing an auxiliary vocal token prediction target. 
As depicted in Figure \ref{fig:pattern} (a), we incorporate a dedicated group of linear heads to predict logits for vocal tokens. These tokens, encoded by X-Codec from the vocal track, are aligned frame-by-frame with the mixed tokens. 
The overall loss function is formulated as:
\begin{equation}
\begin{aligned} 
\mathcal{L}_{\text{mixed-pro}} &=  \mathcal{L}_{\text{mixed}} + \lambda \mathcal{L}_{\text{vocal}},
\end{aligned}    
\end{equation}
where $\lambda$  controls the contribution of the vocal loss to the total loss.
It is important to note that these newly introduced vocal heads are used only during training to compute the auxiliary loss and do not affect inference.
% It is important to note that this newly introduced group of vocal heads is used solely during training to compute the auxiliary loss and does not contribute to the  inference. %next-token input during


 
 


\subsubsection{Dual-track Mode}

In dual-track generation mode, the two key components of a song—the vocal and the accompaniment—are separated, and SongGen synchronously generates both tracks within this unified framework.
Considering the importance of harmony between vocals and accompaniment, we introduce two combination patterns, namely Parallel and Interleaving, to ensure frame-level alignment across the two tracks.
% Another alternative approach is to model the vocal and accompaniment components separately, rather than as a single entity. Unlike previous methods that employ multi-stage processes or multiple decoders, we propose a dual-track modeling strategy that enables the simultaneous generation of both tracks within a single transformer decoder.
% $\hat{\mathbf{S}}_{\text{vocal}}$ and accompaniment tokens $\hat{\mathbf{S}}_{\text{acc}}$ during generation: Parallel and Interleaving.

\textbf{Parallel:}
Inspired by the stereo channel modeling of MusicGen~\cite{copet2024musicgen}, which simultaneously outputs audio tokens for two channels, we design a parallel pattern. As shown in Figure \ref{fig:pattern} (b), the accompaniment and vocal audio tokens are concatenated along the codebook dimension, with each step containing $N_q$ vocal tokens and $N_q$ accompaniment tokens. On the temporal dimension, we introduce three variants. In the ``Standard" variant, the audio tokens for both tracks are strictly aligned frame by frame. The ``Parallel (A-V)" variant delays the vocal tokens by one step relative to the accompaniment tokens. Thus, the vocal token prediction at each frame considers both the previous vocal token and the accompaniment token at the current frame. 
Conversely, in the ``Parallel (V-A)" variant, the accompaniment tokens are delayed by one step relative to the vocal tokens. 
Two groups of code embeddings are used to separately embed the audio tokens for the two tracks. All embeddings are then averaged to form a combined input. Two groups of linear heads are employed to predict the audio tokens for each track. The training loss is defined as:
\vspace{-3pt}
\begin{equation}
\vspace{-8pt}
\label{eq:dual_loss}
\begin{aligned}
\mathcal{L}_{\text{parallel}} &= \frac{1}{2} (\mathcal{L}_{\text{vocal}} +  \mathcal{L}_{\text{acc}}),
\end{aligned} 
\vspace{-3pt}
\end{equation}

where  $\mathcal{L}_{\text{vocal}} $ and $ \mathcal{L}_{\text{acc}} $ represent the individual losses for the vocal and accompaniment tracks, respectively. The calculation method is the same as in Equation \ref{eq:mix_loss}.

\textbf{Interleaving:}
In this pattern, the audio tokens of the two tracks are interleaved along the temporal dimension, as illustrated in Figure \ref{fig:pattern} (c). There are two variants: ``Interleaving (A-V)", where the accompaniment tokens precede the vocal tokens at each frame; and ``Interleaving (V-A)", where the vocal tokens precede the accompaniment tokens. 
In the "Interleaving (A-V)" variant, each vocal token prediction at a given frame considers both the previous vocal token and the accompaniment token from the same frame, with the reverse for the "Interleaving (V-A)" variant.
In this pattern, only a single group of code embeddings and one group of heads are used. The training loss is calculated in the same way as in Equation \ref{eq:dual_loss}.

Although the interleaving pattern requires longer sequence lengths than the parallel pattern, it provides a more effective approach to modeling the relationship between vocals and accompaniment. In the lower layers of the transformer, the interleaving pattern facilitates learning the interactions between the vocal and accompaniment tracks, while the higher layers focus on refining the distinct characteristics of each track. The attention visualizations in Figure \ref{fig:attan} provide additional evidence for this. In contrast, the parallel pattern is unable to decouple the vocal and accompaniment information before reaching the heads.












 % Another solution 也很自然, 即分开建模vocal和伴奏。 然而我们并没有像之前多阶段的，获得多decoder的方法，  We design a dual-track modeling pattern, where the model generates separate vocal and accompaniment tracks simultaneously in a single transformer decoder. 
 % The second solution leverages the fact that vocals and accompaniment are separate but interact with each other. We design a dual-track modeling pattern, where the model generates separate vocal and accompaniment tracks simultaneously in a single transformer decoder. We implemented five variants of this approach and analyzed their performance.


\subsection{Model Conditioning}
\label{sec:cond_model}
\textbf{Lyrics Conditioning.}
To address the challenge of data scarcity, we apply a 6681-token voice Byte-Pair Encoding (VoiceBPE) tokenizer \cite{casanova2024xtts} to convert the lyrics $C_{\text{lyrics}}$ into a sequence of phoneme-like tokens. Word-level tokenizers, like the T5\cite{raffel2020T5} tokenizer, lead to sparse training samples for each token embedding. In contrast, VoiceBPE not only enhances the model's ability to generalize to unseen words but also adapts more effectively to the variations in phoneme duration and pitch range inherent in sung vocals. Subsequently, the lyrics embedding $E_{\text{lyrics}} \in \mathbb{R}^{T_l \times F_l}$ is obtained by passing the lyric tokens through a small transformer-based encoder (i.e., Lyrics Encoder) to extract critical pronunciation-related information. Here, $T_l$ denotes the length of the lyric tokens, and $F_l$ represents the dimensionality of the embedding.
% The embedding obtained from this process, $E_{\text{lyrics}} \in \mathbb{R}^{T_l \times F_l}$, is defined as:
% \begin{align}
%  E_{\text{lyrics}} = \text{LyricsEncoder}(\text{VoiceBPE}(C_{\text{lyrics}})),   
% \end{align}



\textbf{Voice Conditioning. }
% To enable control over vocal timbre and singing techniques, we introduce voice conditioning into our model. 
As demonstrated by the Marble~\cite{yuan2023marble} benchmark, MERT~\cite{li2024mert}, a music representation model, consistently achieves state-of-the-art performance in vocal technique detection and singer identification tasks. Consequently, we employ a frozen MERT encoder to generate robust voice feature embeddings, enabling control over vocal timbre and singing techniques.
Specifically, we randomly select 3-second clips from vocal segments to serve as the voice condition input, denoted as $C_{\text{voice}}$. 
The outputs from MERT's 24 hidden layers and 1 output layer are aggregated via a 1D convolutional layer, yielding the voice embedding $E_{\text{voice}} \in \mathbb{R}^{T_v \times F_v}$, where $T_v$ denotes the temporal length and $F_v$ represents the feature dimensionality of the embedding.


\textbf{Text Conditioning. }
Our text descriptions cover a wide range of musical attributes, including but not limited to the instruments used, musical emotion, tempo, genre, and the singer’s gender, offering more depth than simple tags or short phrases.
Given a description $C_{\text{text}}$  matching the song, we apply a frozen FLAN-T5~\cite{chung2022flanT5} encoder to obtain the text embedding, denoted as $E_{\text{text}} \in \mathbb{R}^{T_t \times F_t}$.

The above three condition embeddings—$E_{\text{lyrics}}$, $ E_{\text{voice}} $, and $ E_{\text{text}} $—are each passed through their respective projection layers to obtain transformed embeddings,  $\hat{E}_{\text{lyrics}} $, $ \hat{E}_{\text{voice}} $, and $\hat{E}_{\text{text}} $. These embeddings are then concatenated along the temporal dimension:
\begin{align}
 E_{\text{cond}} = \hat{E}_{\text{voice}} \oplus  \hat{E}_{\text{text}}  \oplus \hat{E}_{\text{lyrics}} \in \mathbb{R}^{(T_v+T_l+T_t) \times D},   
\end{align}
where $D$ denotes the dimension of the decoder hidden states.
This concatenated embedding \( E_{\text{cond}} \) is used to control song generation via cross attention.


\subsection{Automated Data Preprocessing Pipeline}
\label{sec:pipeline}
To the best of our knowledge, there is currently no publicly available dataset for text-to-song generation that includes paired audio, lyrics, and captions. To address this gap, we develop an automated data annotation pipeline that incorporates several filtering strategies to ensure high-quality data. 
\textbf{(1) Data Source:} We collect 8,000 hours of audio from Million Song Dataset (MSD) \cite{Bertin2011msd}, Free Music Archive (FMA) \cite{fma_challenge} and MTG-Jamendo Dataset \cite{bogdanov2019mtg}.
\textbf{(2) Source Separation:} We utilize Demucs \citep{rouard2022demucs} to separate vocals and accompaniment from the original audio. %, a powerful music source separation model, 
\textbf{(3) Segmentation:} We employ a voice activity detection (VAD) tool~\cite{gao2023funasr} to detect voiced segments in the separate vocal tracks. 
% Adjacent segments are sequentially merged if the unvoiced gap is less than 1.5 seconds. 
Vocal, accompaniment, and mixed tracks are then sliced according to the VAD results, with an average clip duration of 15 seconds. 
Additionally, the energy of each clip is calculated as the sum of the squared amplitude over time, providing a measure of loudness. Clips with low energy in either the accompaniment or vocals are discarded.
\textbf{(4) Lyric Recognition:} Lyric recognition accuracy is crucial for song generation, but it is challenging. Existing Automatic Speech Recognition (ASR) models, trained on speech data, struggle with the complexity and variability of sung vocals. 
Errors arise from two main factors: ASR limitations (misrecognitions and hallucinations); and inherently unclear vocal data, such as noise or genre-specific characteristics like those in rock music.
% Recognition errors stem from two main sources. First, some vocal data is inherently unclear or features pronunciations that lack semantic meaning. Second, ASR model performance is limited by issues such as misrecognitions and hallucinations. 
To tackle this issue, we apply two ASR models, Whisper-large-v2 and Whisper-larger-v3 ~\citep{radford2022whisper}, to automatically transcribe the vocals and generate two lyric transcriptions. We compute the edit distance between them to assess quality, excluding clips with an edit distance greater than 20\%, and retaining only those with relatively clearer vocals and higher recognition confidence.
% We then compute the edit distance between these transcriptions to assess their quality. Clips with an edit distance greater than 20\% are excluded, retaining only those with relatively clearer vocals and higher recognition confidence.
\textbf{(5) Captioning:} 
We use LP-MusicCaps-MSD~\cite{doh2023lp} for MSD captions. For song clips without captions, we generate pseudo-captions using a music captioning model~\citep{doh2023lp}. The accuracy of the captions is evaluated by CLAP Score, which measures the alignment between audio and text with the official CLAP\cite{laionclap2023} model. Samples with low CLAP scores are discarded, and any available original tags are added as a supplement.
After preprocessing, the training dataset contains about 540K English-voiced clips, totaling around 2K hours of audio.
% We use the captions from LP-MusicCaps-MSD~\cite{doh2023lp} as the MSD captions. For datasets lacking captions, we generate pseudo-captions from the segmented mixed audio using a music captioning model~\citep{doh2023lp}. We then calculate the CLAP score as the metric to evaluate the accuracy of the generated captions. Samples with low CLAP scores are discarded. And if original tag information is available, it is added as a supplement to the caption. 






\subsection{Training Scheme}
\label{sec:train}
\textbf{Mixed Mode Training. }
Our mixed mode training consists of three key steps, aimed at progressively boost model performance. \textit{Step 1: Modality Alignment.} We train the entire model using total paired data to align the modalities between the various conditioning inputs and the audio output. 
\textit{Step 2 : Voice-Free Support.} 
To enable the model to function without a reference voice, we apply a 50\% random drop to the reference voice input. To maintain the model's original capabilities, we freeze all modules related to user inputs and fine-tune only the transformer decoder. Once the decoder adapts, we unfreeze the entire model and fine-tune all parameters to optimize performance.
\textit{Step 3: High-Quality Fine-tuning.} The final stage refines the model using a carefully selected subset of data filtered by these quality metrics: $\text{edit\_distance} \leq 5\%$, $\text{CLAP}_{src} \geq 25\%$, $\text{energy} > 1000$. This yields 100K high-quality pairs for fine-tuning, enabling the model to enhance the quality of audio by learning from cleaner, more relevant data. %generation while maintaining its generalization ability 

% We filter the data based on several quality metrics, including an edit distance of less than 5\%, a clap score of at least 25\%, and an energy metric greater than 1000. This results in 100K high-quality data samples for further model fine-tuning. This stage helps the model learn from cleaner, more relevant data, thereby enhancing its ability to generate high-quality audio while maintaining generalization capabilities.


\textbf{Dual-track Mode Training. }
Our experiments revealed that training the dual-track mode from scratch is challenging. To address this, we initialize the dual-track model with the pre-trained mixed mode model after Step 1. \textit{Step 1.5: Dual-Track Mode Adaptation.} After initialization, we freeze user input modules and fine-tune only the transformer decoder to adapt it to the new token pattern. Once the adaptation is complete, we unfreeze all model weights and proceed to fine-tune the entire model. The subsequent training steps mirror those of Steps 2 and 3 in the mixed mode.
% In the parallel pattern, vocal code embeddings and language model heads are initialized from the mixed mode model.
% In our experiments, we found that training the dual-track mode from scratch is challenging. To overcome this, we initialize the dual-track model using the pre-trained model from Stage 1 of the mixed mode. In the parallel pattern, the additional vocal code embeddings and language model heads are initialized using the corresponding codec embeddings and language model heads from the mixed mode model.
% \textbf{Stage 1.5: Dual-Track Mode Adaptation.} After initialization, we freeze all user input modules and fine-tune only the transformer decoder to adapt it to the new token pattern. Once this adaptation is complete, we unfreeze all model weights and proceed with fine-tuning the entire model. The training steps after Stage 1.5 for the dual-track mode are similar to those in Stages 2 and 3 of the mixed mode training.

\textbf{Curriculum Learning for Codebook Loss Weight Adjustment. }
We propose a curriculum learning strategy to adjust the weights of codebook losses during training. Initially, the first three codebooks have weights of 0.25, while the rest are set to 0.05. 
 This encourages the model to focus on the most important components first. As training progresses, the weights are gradually balanced, enabling the model to capture finer audio details step by step.

% We propose a curriculum learning strategy for adjusting the weights of different codebook losses throughout the training process. Initially, the weights for the first three codebooks are set to 0.25, while the weights for the remaining codebooks are set to 0.05. This approach encourages the model to prioritize learning the most essential components. As training progresses, the weights are gradually balanced to allow the model to learn finer details of the audio, step by step.


% \subsubsection{Multi-Stage Training}

% %可参照mini-Omni
% (1)Modality Alignment
% (2)Adaption Training
% (3)Finetuning


% The predominant zero-shot
% approaches for TTS or SVS usually involve two main methods: using voice prompts to leverage the language model’s
% in-context learning capabilities, or using a speaker embedding as a global timbre condition.
% \subsubsection{In-Context Learning via Prompting}
%可参考 vall-E









