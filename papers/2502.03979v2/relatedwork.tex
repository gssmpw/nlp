\section{Related Work}
\label{sec:related}

Below we will discuss some of the existing literature in MER research. For a more comprehensive overview of the literature, the reader is referred to \cite{kang2024we, shelke2024exploring}.

\subsection{Single-Task MER Models}
Most MER models mainly concentrate on single-task learning, which involves training models on individual datasets which use particular emotion labeling schemes, like categorical labels (e.g., happy, sad) or dimensional labels (e.g., valence-arousal). Convolutional neural network (CNN)-based methods have demonstrated strong performance in MER across various datasets. For instance, Liu et al. \cite{liu2017cnn} proposed a spectrogram-based CNN model which captures temporal and spectral features. This model has a macro F1-score of 0.472 and a micro F1-score of 0.534 on CAL500 dataset \cite{turnbull2007towards}, and a macro F1-score of 0.596 and a micro F1-score of 0.709 on CAL500exp dataset \cite{wang2014towards}, outperforming traditional approaches. Bour et al. \cite{bour2021frequency} introduced the frequency-dependent convolutions in a CNN model, achieving the highest performance at MediaEval 2021 with a PR-AUC-macro of 0.1509 and a ROC-AUC-macro of 0.7748 on MTG-Jamendo dataset \cite{bogdanov2019mtg}. Recently, Jia \cite{jia2022music} introduced a CNN-based model which combines MFCCs with residual phase features, achieving a recognition accuracy of 92.06\% on a dataset comprising 2,906 songs categorized into four emotion classes: anger, happiness, relaxation, and sadness.

Given the temporal nature of music, many researchers use recurrent neural network (RNN)-based architectures, such as Long-Short Term Memory networks (LSTMs) and Gated Recurrent Units (GRUs). For instance, Rajesh et al. \cite{rajesh2020musical} used LSTMs with MFCC features to predict emotion on the DEAM dataset \cite{aljanaki2017developing}, achieving an accuracy of 89.3\%, which is better than SVM’s 85.7\%. Chaki et al. \cite{chaki2020attentive} added attention mechanisms to LSTMs to concentrate on emotionally significant segments, resulting in \(R^2\) scores of 0.53 for valence and 0.75 for arousal on the EmoMusic dataset \cite{soleymani20131000}, surpassing the performance of the LSTM variant that lacked attention mechanisms.

% In recent times, Transformer-based methods have become increasingly popular. For instance, Suresh et al. \cite{suresh2023transformer} introduced a Transformer-based multimodal framework for classifying music moods, utilizing both textual and acoustic features. Their multimodal model, tested on a subset of the MoodyLyrics dataset \cite{ccano2017moodylyrics}, which comprises audio and lyrics for 680 songs, achieved an accuracy of 77.94\%, outperforming unimodal methods.

% Due to the lack of official train/test splits in many datasets like EmoMusic, direct comparisons between models are challenging, except for the MTG-Jamendo dataset, which has official train/test splits, allowing us to compare our model with those from MediaEval 2021 in the experiments. Despite these achievements, single-task models frequently face challenges in generalizing across various datasets, underscoring the need for research that integrates multiple datasets.

Recently, Transformer-based methods have seen a rise in popularity. For instance, Suresh et al. \cite{suresh2023transformer} proposed the multimodal model with Transformer-based architecture for classifying the mood of music by leveraging the features from audio and lyrics. Their proposed model was evaluated on a subset of the MoodyLyrics dataset \cite{ccano2017moodylyrics} which contains audio and lyrics of 680 songs. Their proposed model outperforms the unimodal model with an accuracy of 77.94\%.

The absence of official train/test splits in many datasets, such as the PMEmo dataset \cite{zhang2018pmemo}, makes it difficult to compare models directly, except for the MTG-Jamendo dataset \cite{bogdanov2019mtg}, which provides official splits. This enables us to compare our model with those from MediaEval 2021 \cite{bour2021frequency,pham2021selab,tan2021semi,mayerl2021recognizing} as well as more recent methods \cite{hasumi2025music,greer2023creating,li2023mert} in our experiments. Despite these achievements, single-task models frequently face challenges in generalizing across various datasets, underscoring the need for research that integrates multiple datasets.


\subsection{MER Models with Multi-Dataset Integration}
Integrating multiple datasets is essential for improving generalization but is challenging due to inconsistencies in labeling schemes, especially between categorical and dimensional labels. Liu et al. \cite{liu2024leveraging} addressed this issue by leveraging a large language model (LLM) to align categorical labels from various datasets into a common semantic space. They used three disjoint datasets with categorical labels, which include MTG-Jamendo~\cite{bogdanov2019mtg}, CAL500~\cite{turnbull2007towards}, and Emotify \cite{aljanaki2016studying}. They showed the effectiveness of their approach by performing zero-shot inference on a new dataset.

Mazzetta et al. \cite{multi-source-mer2024} introduced a multi-source learning framework that integrates features and labels from heterogeneous datasets, including 4Q \cite{panda2018musical}, PMEmo \cite{zhang2018pmemo}, EmoMusic \cite{soleymani20131000}, and the Bi-Modal Emotion Dataset \cite{malheiro2016bi} to enhance the model’s robustness. These datasets use Russell’s Circumplex Model of Affect, and focus on the valence and arousal dimensions. While this framework effectively utilizes dimensional labels to improve robustness, it does not incorporate categorical labels, thus limiting its ability to fully leverage the diversity of available datasets \cite{kang2024we}. Developing frameworks that integrate both label types remains an open challenge, which we address in this work.

\begin{figure*}[!ht]
\centering
\includegraphics[width=6.1in]{overview_new.png}
\caption{Overall architecture of our proposed framework.}
\label{fig:framework}
\end{figure*}

\subsection{Multitask Learning in MER}
Multitask learning (MTL) \cite{caruana1997multitask} aims to enhance the performance of multiple related tasks by leveraging shared knowledge among them. Qiu et al. \cite{qiu2022novel} introduced an MTL framework for Symbolic Music Emotion Recognition (SMER). They combines emotion recognition task with auxiliary tasks such as key and velocity classification. The idea is that by forcing the model to learn key and velocity information, it will also better understand the resulting emotion. Evaluated on the EMOPIA \cite{hung2021emopia} and VGMIDI \cite{ferreira2021learning} datasets, their approach achieved accuracy improvements of 4.17\% and 1.97\%, respectively, compared to single-task models. Ji et al. \cite{huang2022adff} introduced an attention-based deep feature fusion (ADFF) method for valence-arousal prediction within a multitask learning framework. They integrate spatial feature extraction using an adapted VGGNet with a squeeze-and-excitation (SE) attention-based temporal module to enhance affect-salient features. Their model achieved \(R^2\) scores of 0.4575 for valence and 0.6394 for arousal on the PMEmo dataset \cite{zhang2018pmemo}.

Existing MTL frameworks in MER are typically still trained on datasets with one type of emotion label. In this research, we explore how we can leverage an MTL architecture to enable predicting across heterogeneous emotion datasets.

% >> Shorten it coz one of the reviwer said to make this section shorter
\subsection{Knowledge Distillation for MER}
Knowledge distillation (KD) is a technique where a smaller student model learns to replicate the behavior of a larger teacher model by aligning its soft predictions with those of the teacher, rather than solely relying on the actual labels \cite{hinton2015distilling}. In contrast to transfer learning, which usually involves adjusting a pre-trained model for a new task or dataset, KD emphasizes the use of the teacher model's outputs (soft labels) to guide the training of the student model.
Tong \cite{tong2022multimodal} proposed a KD-based multimodal MER framework that uses a teacher-student model, where a pre-trained genre classification model transfers knowledge to an emotion recognition model. Knowledge transfer is guided by Exponential Moving Average (EMA) analysis that refines the student model’s parameters without backpropagation. The training minimizes a combined KL divergence and cross-entropy loss, which improves emotion recognition in terms of both labeled and unlabeled data while maintaining efficiency. 
Jeong et al. \cite{jeong2022multitask} demonstrated the potential of KD in a multitask setting for valence-arousal prediction, facial expression recognition, and action unit prediction. Despite its promise, the use of KD in multitask frameworks for integrating heterogeneous labels remains underexplored.

% \subsection{Knowledge Distillation for MER}
% Knowledge distillation (KD) is a technique where a smaller student model learns to replicate the behavior of a larger teacher model by aligning its soft predictions with those of the teacher, rather than solely relying on the actual labels \cite{hinton2015distilling}. While both KD and fine-tuning fall under the broader umbrella of transfer learning, they differ in their approaches. Fine-tuning typically involves adapting a pre-trained model to a new task or dataset by updating its weights, whereas KD focuses on transferring the "knowledge" captured in the teacher's output distribution to the student model, often improving generalization and performance, especially when labeled data is scarce. Tong \cite{tong2022multimodal} proposed a KD-based multimodal MER framework that uses a teacher-student model, where a pre-trained genre classification model transfers knowledge to an emotion recognition model. Knowledge transfer is guided by Exponential Moving Average (EMA) analysis that refines the student model’s parameters without backpropagation. The training minimizes a combined KL divergence and cross-entropy loss, which improves emotion recognition in terms of both labeled and unlabeled data while maintaining efficiency. 
% Jeong et al. \cite{jeong2022multitask} demonstrated the potential of KD in a multitask setting for valence-arousal prediction, facial expression recognition, and action unit prediction. Despite its promise, the use of KD in multitask frameworks for integrating heterogeneous labels remains underexplored.

\subsection{Feature Engineering for MER}

Feature engineering is a key component in Music Emotion Recognition (MER). Earlier works relied heavily on hand-crafted features such as MFCCs, chroma features, and rhythmic descriptors \cite{hizlisoy2021music}, which capture low-level signal properties but fall short in representing higher-level musical semantics \cite{panda2018novel}. Other approaches leverage embeddings from audio models trained on large-scale datasets. For example, VGGish \cite{hershey2017cnn}, a convolutional model trained in a supervised fashion for audio classification, and CLAP \cite{elizalde2023clap}, which is trained with a contrastive audio-text alignment objective, have been used to extract richer representations of audio content.

Pre-trained encoders have also gained traction in MER due to their ability to learn from unlabeled audio data. Among these, MERT \cite{yizhi2023mert} has demonstrated strong performance relative to other pre-trained encoders. For instance, MERT-95M achieves a PR-AUC of 0.134 and an ROC-AUC of 0.764 on the MTG-Jamendo dataset, while the larger MERT-330M model improves these scores slightly to a PR-AUC of 0.14 and an ROC-AUC of 0.765. These results highlight the potential of pre-trained encoders for emotion prediction tasks.

Beyond embeddings, high-level musical features such as key signatures and chord progressions have also been shown to play a significant role in emotion prediction \cite{cho2016music}. In this study, we aim to enhance MER by integrating large-scale audio embeddings with these symbolic, high-level musical descriptors.

% Feature engineering is an important facet of Music Emotion Recognition (MER). The earlier works relied on hand-crafted features like MFCCs, chroma features, and rhythmic descriptors \cite{hizlisoy2021music} which could capture low-level details of the audio, but were not good at depicting higher-level musical semantics \cite{panda2018novel}. 

% Lately, models like VGGish \cite{hershey2017cnn}, CLAP \cite{elizalde2023clap}, and MERT \cite{li2023mert}, which are trained with self-supervised learning on large-scale datasets, are used to create more elaborate musical embeddings. Among these, MERT has shown remarkable performance in MER. For instance, MERT-95M achieves a PR-AUC of 0.134 and an ROC-AUC of 0.764 on MTG-Jamendo, while the larger MERT-330M model improves these scores to a PR-AUC of 0.14 and an ROC-AUC of 0.765. In addition to embeddings, the high-level musical elements such as key signatures and chord progressions are highly significant in emotion prediction \cite{cho2016music}. In this study, the goal is to enhance the MER techniques by combining these large scale embeddings with the high-level musical components.