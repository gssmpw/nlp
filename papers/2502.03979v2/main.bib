@article{park2019bi,
  title={A bi-directional transformer for musical chord recognition},
  author={Park, Jonggwon and Choi, Kyoyun and Jeon, Sungwook and Kim, Dokyun and Park, Jonghun},
  journal={arXiv:1907.02698},
  year={2019}
}

@inproceedings{jonggwon2019bi,
  title={A bi-directional transformer for musical chord recognition},
  author={Jonggwon, Park and Kyoyun, C and Sungwook, J and Dokyun, K and Jonghun, P},
  booktitle={20th International Society for Music Information Retrieval Conference (ISMIR), Delft, The Netherlands},
  year={2019}
}

@article{lim2021temporal,
  title={Temporal fusion transformers for interpretable multi-horizon time series forecasting},
  author={Lim, Bryan and Ar{\i}k, Sercan {\"O} and Loeff, Nicolas and Pfister, Tomas},
  journal={International Journal of Forecasting},
  volume={37},
  number={4},
  pages={1748--1764},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{bogdanov2019mtg,
  title={The mtg-jamendo dataset for automatic music tagging},
  author={Bogdanov, Dmitry and Won, Minz and Tovstogan, Philip and Porter, Alastair and Serra, Xavier},
  year={2019},
  organization={ICML}
}

@article{ong2023constructing,
  title={Constructing time-series momentum portfolios with deep multi-task learning},
  author={Ong, Joel and Herremans, Dorien},
  journal={Expert Systems with Applications},
  volume={230},
  pages={120587},
  year={2023},
  publisher={Elsevier}
}

@phdthesis{meyer1954emotion,
  title={Emotion and meaning in music},
  author={Meyer, Leonard B},
  year={1954},
  school={The University of Chicago}
}

@inproceedings{soleymani20131000,
  title={1000 songs for emotional analysis of music},
  author={Soleymani, Mohammad and Caro, Micheal N and Schmidt, Erik M and Sha, Cheng-Ya and Yang, Yi-Hsuan},
  booktitle={Proc. of the 2nd ACM Int. workshop on Crowdsourcing for multimedia},
  pages={1--6},
  year={2013}
}

@inproceedings{zhang2018pmemo,
  title={The PMEmo dataset for music emotion recognition},
  author={Zhang, Kejun and Zhang, Hui and Li, Simeng and Yang, Changyuan and Sun, Lingyun},
  booktitle={Proc. of the 2018 acm on Int.  Conf. on multimedia retrieval},
  pages={135--142},
  year={2018}
}

@article{aljanaki2017developing,
  title={Developing a benchmark for emotional analysis of music},
  author={Aljanaki, Anna and Yang, Yi-Hsuan and Soleymani, Mohammad},
  journal={PloS one},
  volume={12},
  number={3},
  pages={e0173392},
  year={2017},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{li2023mert,
  title={Mert: Acoustic music understanding model with large-scale self-supervised training},
  author={Li, Yizhi and Yuan, Ruibin and Zhang, Ge and Ma, Yinghao and Chen, Xingran and Yin, Hanzhi and Xiao, Chenghao and Lin, Chenghua and Ragni, Anton and Benetos, Emmanouil and others},
  journal={arXiv:2306.00107},
  year={2023}
}

@inproceedings{yizhi2023mert,
  title={MERT: Acoustic music understanding model with large-scale self-supervised training},
  author={Yizhi, LI and Yuan, Ruibin and Zhang, Ge and Ma, Yinghao and Chen, Xingran and Yin, Hanzhi and Xiao, Chenghao and Lin, Chenghua and Ragni, Anton and Benetos, Emmanouil and others},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}


@article{jia2022music,
  title={A music emotion classification model based on the improved convolutional neural network},
  author={Jia, Xiaosong},
  journal={Computational Intelligence and Neuroscience},
  volume={2022},
  number={1},
  pages={6749622},
  year={2022},
  publisher={Wiley Online Library}
}

@article{liu2017cnn,
  title={CNN based music emotion classification},
  author={Liu, Xin and Chen, Qingcai and Wu, Xiangping and Liu, Yan and Liu, Yang},
  journal={arXiv:1704.05665},
  year={2017}
}

@inproceedings{wang2014towards,
  title={Towards time-varying music auto-tagging based on cal500 expansion},
  author={Wang, Shuo-Yang and Wang, Ju-Chiang and Yang, Yi-Hsuan and Wang, Hsin-Min},
  booktitle={2014 IEEE Int.  Conf. on Multimedia and Expo (ICME)},
  pages={1--6},
  year={2014},
  organization={IEEE}
}

@inproceedings{turnbull2007towards,
  title={Towards musical query-by-semantic-description using the cal500 data set},
  author={Turnbull, Douglas and Barrington, Luke and Torres, David and Lanckriet, Gert},
  booktitle={Proc. of the 30th annual Int.  ACM SIGIR Conf. on Research and development in information retrieval},
  pages={439--446},
  year={2007}
}

@article{liu2024leveraging,
  title={Leveraging LLM Embeddings for Cross Dataset Label Alignment and Zero Shot Music Emotion Prediction},
  author={Liu, Renhang and Roy, Abhinaba and Herremans, Dorien},
  journal={arXiv:2410.11522},
  year={2024}
}

@article{kang2024we,
  title={Are we there yet? A brief survey of Music Emotion Prediction Datasets, Models and Outstanding Challenges},
  author={Kang, Jaeyong and Herremans, Dorien},
  journal={arXiv:2406.08809},
  year={2024}
}



@article{herremans2017morpheus,
  title={MorpheuS: generating structured music with constrained patterns and tension},
  author={Herremans, Dorien and Chew, Elaine},
  journal={IEEE Transactions on Affective Computing},
  volume={10},
  number={4},
  pages={510--523},
  year={2017},
  publisher={IEEE}
}



@article{koh2022merp,
  title={MERP: a music dataset with emotion ratings and ratersâ€™ profile information},
  author={Koh, En Yan and Cheuk, Kin Wai and Heung, Kwan Yee and Agres, Kat R and Herremans, Dorien},
  journal={Sensors},
  volume={23},
  number={1},
  pages={382},
  year={2022},
  publisher={MDPI}
}

@article{chua2022predicting,
  title={Predicting emotion from music videos: exploring the relative contribution of visual and auditory information to affective responses},
  author={Chua, Phoebe and Makris, Dimos and Herremans, Dorien and Roig, Gemma and Agres, Kat},
  journal={arXiv:2202.10453},
  year={2022}
}



@article{thao2023emomv,
  title={EmoMV: Affective music-video correspondence learning datasets for classification and retrieval},
  author={Thao, Ha Thi Phuong and Roig, Gemma and Herremans, Dorien},
  journal={Information Fusion},
  volume={91},
  pages={64--79},
  year={2023},
  publisher={Elsevier}
}


@inproceedings{cheuk2020regression,
  title={Regression-based music emotion prediction using triplet neural networks},
  author={Cheuk, Kin Wai and Luo, Yin-Jyun and Balamurali, BT and Roig, Gemma and Herremans, Dorien},
  booktitle={2020 Int.  joint Conf. on neural networks (ijcnn)},
  pages={1--7},
  year={2020},
  organization={IEEE}
}

@article{agres2021music,
  title={Music, computing, and health: a roadmap for the current and future roles of music technology for health care and well-being},
  author={Agres, Kat R and Schaefer, Rebecca S and Volk, Anja and van Hooren, Susan and Holzapfel, Andre and Dalla Bella, Simone and M{\"u}ller, Meinard and De Witte, Martina and Herremans, Dorien and Ramirez Melendez, Rafael and others},
  journal={Music \& Science},
  volume={4},
  pages={2059204321997709},
  year={2021},
  publisher={SAGE Publications Sage UK: London, England}
}



@inproceedings{thao2021attendaffectnet,
  title={Attendaffectnet: Self-attention based networks for predicting affective responses from movies},
  author={Thao, Ha Thi Phuong and Balamurali, BT and Herremans, Dorien and Roig, Gemma},
  booktitle={2020 25th Int.  Conf. on Pattern Recognition (ICPR)},
  pages={8719--8726},
  year={2021},
  organization={IEEE}
}



@inproceedings{makris2021generating,
  title={Generating lead sheets with affect: A novel conditional seq2seq framework},
  author={Makris, Dimos and Agres, Kat R and Herremans, Dorien},
  booktitle={2021 Int.  Joint Conf. on Neural Networks (IJCNN)},
  pages={1--8},
  year={2021},
  organization={IEEE}
}
@article{tan2020music,
  title={Music fadernets: Controllable music generation based on high-level features via low-level feature modelling},
  author={Tan, Hao Hao and Herremans, Dorien},
  journal={Proc. of ISMIR},
  year={2020}
}

@inproceedings{multi-source-mer2024,
  author    = {Giovanni Mazzetta and
               Antonio Greco and
               Marco Tagliasacchi and
               Antonio Pescap{\`{e}}},
  title     = {A Multi-Source Deep Learning Model for Music Emotion Recognition},
  booktitle = {Proc. of the AIxHMI Workshop at CEUR-WS},
  year      = {2024},
  volume    = {3903},
  pages     = {1--8},
  url       = {https://ceur-ws.org/Vol-3903/AIxHMI2024_paper4.pdf},
  publisher = {CEUR Workshop Proc.}
}

@article{aljanaki2016studying,
  title={Studying emotion induced by music through a crowdsourcing game},
  author={Aljanaki, Anna and Wiering, Frans and Veltkamp, Remco C},
  journal={Information Processing \& Management},
  volume={52},
  number={1},
  pages={115--128},
  year={2016},
  publisher={Elsevier}
}

@inproceedings{mayerl2021recognizing,
  title={Recognizing Song Mood and Theme: Clustering-based Ensembles.},
  author={Mayerl, Maximilian and V{\"o}tter, Michael and Peintner, Andreas and Specht, G{\"u}nther and Zangerle, Eva},
  booktitle={MediaEval},
  year={2021}
}

@article{tan2021semi,
  title={Semi-supervised music emotion recognition using noisy student training and harmonic pitch class profiles},
  author={Tan, Hao Hao},
  journal={arXiv:2112.00702},
  year={2021}
}

@inproceedings{bour2021frequency,
  title={Frequency Dependent Convolutions for Music Tagging.},
  author={Bour, Vincent},
  booktitle={MediaEval},
  year={2021}
}

@inproceedings{pham2021selab,
  title={SELAB-HCMUS at MediaEval 2021: Music Theme and Emotion Classification with Co-teaching Training Strategy.},
  author={Pham, Phu-Thinh and Huynh, Minh-Hieu and Nguyen, Hai-Dang and Tran, Minh-Triet},
  booktitle={MediaEval},
  year={2021}
}


@inproceedings{raffel2014mir_eval,
  title={MIR\_EVAL: A Transparent Implementation of Common MIR Metrics.},
  author={Raffel, Colin and McFee, Brian and Humphrey, Eric J and Salamon, Justin and Nieto, Oriol and Liang, Dawen and Ellis, Daniel PW and Raffel, C Colin},
  booktitle={ISMIR},
  volume={10},
  pages={2014},
  year={2014}
}


@article{cuthbert2010music21,
  title={music21: A toolkit for computer-aided musicology and symbolic music data},
  author={Cuthbert, Michael Scott and Ariza, Christopher},
  year={2010},
  publisher={Int. Society for Music Information Retrieval}
}

@article{huang2022adff,
  title={ADFF: Attention based deep feature fusion approach for music emotion recognition},
  author={Huang, Zi and Ji, Shulei and Hu, Zhilan and Cai, Chuangjian and Luo, Jing and Yang, Xinyu},
  journal={arXiv:2204.05649},
  year={2022}
}

@article{qiu2022novel,
  title={A novel multi-task learning method for symbolic music emotion recognition},
  author={Qiu, Jibao and Chen, CL and Zhang, Tong},
  journal={arXiv:2201.05782},
  year={2022}
}

@article{tong2022multimodal,
  title={Multimodal music emotion recognition method based on the combination of knowledge distillation and transfer learning},
  author={Tong, Guiying},
  journal={Scientific Programming},
  volume={2022},
  number={1},
  pages={2802573},
  year={2022},
  publisher={Wiley Online Library}
}

@article{jeong2022multitask,
  title={Multitask emotion recognition model with knowledge distillation and task discriminator},
  author={Jeong, Euiseok and Oh, Geesung and Lim, Sejoon},
  journal={arXiv:2203.13072},
  year={2022}
}

@article{chen2020multimodal,
  title={A multimodal music emotion classification method based on multifeature combined network classifier},
  author={Chen, Changfeng and Li, Qiang},
  journal={Mathematical Problems in Engineering},
  volume={2020},
  number={1},
  pages={4606027},
  year={2020},
  publisher={Wiley Online Library}
}

@article{pandeya2021deep,
  title={Deep learning-based late fusion of multimodal information for emotion classification of music video},
  author={Pandeya, Yagya Raj and Lee, Joonwhoan},
  journal={Multimedia Tools and Applications},
  volume={80},
  number={2},
  pages={2887--2905},
  year={2021},
  publisher={Springer}
}


@article{hizlisoy2021music,
  title={Music emotion recognition using convolutional long short term memory deep neural networks},
  author={Hizlisoy, Serhat and Yildirim, Serdar and Tufekci, Zekeriya},
  journal={Eng. Sci. Technol. Int J.},
  volume={24},
  number={3},
  pages={760--767},
  year={2021},
  publisher={Elsevier}
}

@article{rajesh2020musical,
  title={Musical instrument emotion recognition using deep recurrent neural network},
  author={Rajesh, Sangeetha and Nalini, NJ},
  journal={Procedia Comput. Sci.},
  volume={167},
  pages={16--25},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{chaki2020attentive,
  title={Attentive RNNs for Continuous-time Emotion Prediction in Music Clips.},
  author={Chaki, Sanga and Doshi, Pranjal and Patnaik, Priyadarshi and Bhattacharya, Sourangshu},
  booktitle={AffCon@ AAAI},
  pages={36--46},
  year={2020}
}

@inproceedings{agrawal2021transformer,
  title={Transformer-based approach towards music emotion recognition from lyrics},
  author={Agrawal, Yudhik and Shanker, Ramaguru Guru Ravi and Alluri, Vinoo},
  booktitle={European Conf. on information retrieval},
  pages={167--175},
  year={2021},
  organization={Springer}
}

@article{suresh2023transformer,
  title={Transformer-based automatic music mood classification using multi-modal framework},
  author={Suresh Kumar, Sujeesha Ajithakumari and Rajan, Rajeev},
  journal={Journal of Computer Science \& Technology},
  volume={23},
  year={2023}
}

@inproceedings{tovstogan2021mediaeval,
  title={MediaEval 2021: Emotion and Theme Recognition in Music Using Jamendo.},
  author={Tovstogan, Philip and Bogdanov, Dmitry and Porter, Alastair},
  booktitle={MediaEval},
  year={2021}
}

@article{zhang2022music,
  title={Music Emotion Representation Learning Based on Multisource Data Fusion and Its Application},
  author={Zhang, Haibo},
  journal={Mobile Information Systems},
  volume={2022},
  number={1},
  pages={3983201},
  year={2022},
  publisher={Wiley Online Library}
}

@article{russell1980circumplex,
  title={A circumplex model of affect.},
  author={Russell, James A},
  journal={Journal of personality and social psychology},
  volume={39},
  number={6},
  pages={1161},
  year={1980},
  publisher={American Psychological Association}
}

@inproceedings{cai2021speech,
  title={Speech emotion recognition with multi-task learning.},
  author={Cai, Xingyu and Yuan, Jiahong and Zheng, Renjie and Huang, Liang and Church, Kenneth},
  booktitle={Interspeech},
  volume={2021},
  pages={4508--4512},
  year={2021},
  organization={Brno}
}

@article{ghosh2022mmer,
  title={Mmer: Multimodal multi-task learning for speech emotion recognition},
  author={Ghosh, Sreyan and Tyagi, Utkarsh and Ramaneswaran, S and Srivastava, Harshvardhan and Manocha, Dinesh},
  journal={arXiv:2203.16794},
  year={2022}
}

@article{greer2023creating,
  title={Creating musical features using multi-faceted, multi-task encoders based on transformers},
  author={Greer, Timothy and Shi, Xuan and Ma, Benjamin and Narayanan, Shrikanth},
  journal={Scientific Reports},
  volume={13},
  number={1},
  pages={10713},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{hinton2015distilling,
  title={Distilling the Knowledge in a Neural Network},
  author={Hinton, Geoffrey},
  journal={arXiv:1503.02531},
  year={2015}
}
@article{hasumi2025music,
  title={Music Tagging with Classifier Group Chains},
  author={Hasumi, Takuya and Komatsu, Tatsuya and Fujita, Yusuke},
  journal={arXiv:2501.05050},
  year={2025}
}

@inproceedings{lyberatos2024perceptual,
  title={Perceptual musical features for interpretable audio tagging},
  author={Lyberatos, Vassilis and Kantarelis, Spyridon and Dervakos, Edmund and Stamou, Giorgos},
  booktitle={2024 IEEE Int. Conf. on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)},
  pages={878--882},
  year={2024},
  organization={IEEE}
}


@inproceedings{tran2023emotion,
  title={Emotion-aware music recommendation},
  author={Tran, Hieu and Le, Tuan and Do, Anh and Vu, Tram and Bogaerts, Steven and Howard, Brian},
  booktitle={Proc. of the AAAI Conf. on Artificial Intelligence},
  volume={37},
  number={13},
  pages={16087--16095},
  year={2023}
}

@inproceedings{shelke2024exploring,
  title={Exploring Machine Learning Techniques for Music Emotion Classification: A Comprehensive Review},
  author={Shelke, Sheetal and Patil, Mangal},
  booktitle={2024 11th Int. Conf. on Computing for Sustainable Global Development (INDIACom)},
  pages={1188--1195},
  year={2024},
  organization={IEEE}
}


@inproceedings{panda2018musical,
  title={Musical texture and expressivity features for music emotion recognition},
  author={Panda, Renato and Malheiro, Ricardo and Paiva, Rui Pedro},
  booktitle={19th Int. Society for Music Information Retrieval Conf. (ISMIR 2018)},
  pages={383--391},
  year={2018}
}

@inproceedings{malheiro2016bi,
  title={Bi-modal music emotion recognition: Novel lyrical features and dataset},
  author={Malheiro, Ricardo and Panda, Renato and Gomes, Paulo JS and Paiva, Rui Pedro},
  booktitle={9th Int. Workshop on Music and Machine Learning--MML},
  year={2016}
}

@article{caruana1997multitask,
  title={Multitask learning},
  author={Caruana, Rich},
  journal={Machine learning},
  volume={28},
  pages={41--75},
  year={1997},
  publisher={Springer}
}

@article{fawcett2006introduction,
  title={An introduction to ROC analysis},
  author={Fawcett, Tom},
  journal={Pattern recognition letters},
  volume={27},
  number={8},
  pages={861--874},
  year={2006},
  publisher={Elsevier}
}

@inproceedings{davis2006relationship,
  title={The relationship between Precision-Recall and ROC curves},
  author={Davis, Jesse and Goadrich, Mark},
  booktitle={Proc. of the 23rd Int. conference on Machine learning},
  pages={233--240},
  year={2006}
}

@book{draper1998applied,
  title={Applied regression analysis},
  author={Draper, NR},
  year={1998},
  publisher={McGraw-Hill. Inc}
}

@inproceedings{hershey2017cnn,
  title={CNN architectures for large-scale audio classification},
  author={Hershey, Shawn and Chaudhuri, Sourish and Ellis, Daniel PW and Gemmeke, Jort F and Jansen, Aren and Moore, R Channing and Plakal, Manoj and Platt, Devin and Saurous, Rif A and Seybold, Bryan and others},
  booktitle={2017 ieee Int. conference on acoustics, speech and signal processing (icassp)},
  pages={131--135},
  year={2017},
  organization={IEEE}
}

@inproceedings{cramer2019look,
  title={Look, listen, and learn more: Design choices for deep audio embeddings},
  author={Cramer, Aurora Linh and Wu, Ho-Hsiang and Salamon, Justin and Bello, Juan Pablo},
  booktitle={ICASSP 2019-2019 IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={3852--3856},
  year={2019},
  organization={IEEE}
}

@inproceedings{elizalde2023clap,
  title={Clap learning audio concepts from natural language supervision},
  author={Elizalde, Benjamin and Deshmukh, Soham and Al Ismail, Mahmoud and Wang, Huaming},
  booktitle={ICASSP 2023-2023 IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}

@inproceedings{cho2016music,
  title={Music emotion recognition using chord progressions},
  author={Cho, Yong-Hun and Lim, Hyunki and Kim, Dae-Won and Lee, In-Kwon},
  booktitle={2016 IEEE Int. Conf. on Systems, Man, and Cybernetics (SMC)},
  pages={002588--002593},
  year={2016},
  organization={IEEE}
}

@article{panda2018novel,
  title={Novel audio features for music emotion recognition},
  author={Panda, Renato and Malheiro, Ricardo and Paiva, Rui Pedro},
  journal={IEEE Transactions on Affective Computing},
  volume={11},
  number={4},
  pages={614--626},
  year={2018},
  publisher={IEEE}
}


@article{hung2021emopia,
  title={EMOPIA: A multi-modal pop piano dataset for emotion recognition and emotion-based music generation},
  author={Hung, Hsiao-Tzu and Ching, Joann and Doh, Seungheon and Kim, Nabin and Nam, Juhan and Yang, Yi-Hsuan},
  journal={arXiv:2108.01374},
  year={2021}
}

@article{ferreira2021learning,
  title={Learning to generate music with sentiment},
  author={Ferreira, Lucas N and Whitehead, Jim},
  journal={arXiv:2103.06125},
  year={2021}
}

@inproceedings{ccano2017moodylyrics,
  title={Moodylyrics: A sentiment annotated lyrics dataset},
  author={{\c{C}}ano, Erion and Morisio, Maurizio},
  booktitle={Proceedings of the 2017 international conference on intelligent systems, metaheuristics \& swarm intelligence},
  pages={118--124},
  year={2017}
}



@article{mccallum2022supervised,
  title={Supervised and unsupervised learning of audio representations for music understanding},
  author={McCallum, Matthew C and Korzeniowski, Filip and Oramas, Sergio and Gouyon, Fabien and Ehmann, Andreas F},
  journal={arXiv preprint arXiv:2210.03799},
  year={2022}
}

@article{alonso2023efficient,
  title={Efficient supervised training of audio transformers for music representation learning},
  author={Alonso-Jim{\'e}nez, Pablo and Serra, Xavier and Bogdanov, Dmitry},
  journal={arXiv preprint arXiv:2309.16418},
  year={2023}
}


@inproceedings{di2013automatic,
  title={Automatic chord recognition based on the probabilistic modeling of diatonic modal harmony},
  author={Di Giorgi, Bruno and Zanoni, Massimiliano and Sarti, Augusto and Tubaro, Stefano},
  booktitle={nDS'13; Proceedings of the 8th International Workshop on Multidimensional Systems},
  pages={1--6},
  year={2013},
  organization={VDE}
}