\section{Conclusion}

This paper systematically addresses the critical challenge of loss spikes in large model pre-training through gradient dynamics analysis. Our key insight reveals two limitations in conventional approaches: temporal gradient norm decay requires adaptive thresholds, while parameter-specific gradient heterogeneity demands localized control. These findings motivate \textbf{AdaGC}, which integrates exponential moving average-based threshold adaptation with per-parameter clipping while maintaining Adam-comparable $\mathcal{O}(1/\sqrt{T})$ convergence rate - proving local clipping preserves theoretical convergence properties. Comprehensive experiments demonstrate AdaGC's dual benefits: complete loss spike elimination across LLMs (3.5\% WikiText perplexity reduction on Llama-2 7B) and LVLMs (25\% faster CLIP convergence than StableAdamW), coupled with enhanced training stability and final model quality. The consistent success across architectures and optimizers validates that jointly addressing temporal adaptation and spatial parameter characteristics provides an effective stabilization paradigm for large-scale training.
