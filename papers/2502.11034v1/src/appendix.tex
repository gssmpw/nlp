\section{Convergence Proof}
\label{appendix_converge}



In this section, we provide the necessary assumptions and lemmas for the proofs of Theorem \ref{theorem:1}.

\paragraph{Notations}  The $k$-th component of a vector $v_t$ is denoted as ${v}_{t, k}$.  Other than that, all computations that involve vectors shall be understood in the component-wise way. We say a vector $v_t \geq 0$ if every component of $v_t$ is non-negative, and $v_t \geq w_t$ if $v_{t,k} \geq w_{t,k}$ for all $k=1, 2, \ldots, d$. The $\ell_1$ norm of a vector $v_t$ is defined as $\|v_t\|_1 = \sum_{k=1}^d |{v}_{t, k}|$. The $\ell_2$ norm is defined as $\|v_t\|^2 =\langle v_t, v_t \rangle = \sum_{k=1}^d |{v}_{t,k}|^2$. Given a positive vector $\hat{\eta}_t$, it will be helpful to define the following weighted norm: $\|v_t\|^2_{\eta_t} = \langle v_t, \hat{\eta}_t v_t \rangle = \sum_{k=1}^d \hat{\eta}_{t, k}|{v}_{t, k}|^2$.

\begin{assumption}
\label{assumption_1}
The function f is lower bounded by $\underline{f}$ with L-Lipschitz gradient.
\end{assumption}

\begin{assumption}
\label{assumption_2}
The gradient estimator $g$ is unbiased with bounded norm, e.g,
\[
\begin{split}
    &\mathbb{E}[g|x_t] =\nabla f(x_t), \ \|g_t\| \leq G. 
\end{split}
\]
\end{assumption}


\begin{assumption}
\label{assumption_3}
The coefficient of clipping $h_{t,i}$ is lower bounded by some $h_0>0$.
\end{assumption}



\begin{assumption}
\label{assumption_4}
$\|g_t - \nabla f(x_t)\| \leq p \|\nabla f(x_t)\|$ holds for some $p<1$ and for all t.
\end{assumption}


\begin{remark}
    Assumption \ref{assumption_1} and Assumption \ref{assumption_2} are widely used in the proof of optimization algorithm with adaptive learning rates \cite{reddi2018convergence}. Assumption \ref{assumption_3} is because the gradient norm changes slowly when training the neural network, and the last assumption holds when the batch size is large enough.
\end{remark}


\begin{lemma}\label{lem1-001}
Let $\zeta := \beta_1^2/{\beta_2}$. We have the following estimate
\begin{equation}
m_t^2 \leq \frac{1}{(1-\zeta)(1-\beta_2)}v_t,~\forall t.
\end{equation}
\end{lemma}

\begin{proof}
By the iteration formula $m_t = \beta_1 m_{t-1} + (1-\beta_1)\hat{g}_t$ and $m_0 = 0$, we have
\begin{equation*}
m = \sum_{i=1}^t \beta_1^{t-i} (1-\beta_1)\hat{g}_i.
\end{equation*}

Similarly, by $v_t = \beta_2 v_{t-1} + (1-\beta_2) \hat{g}_t^2$ and $v_0 = 0$, we have
\begin{equation*}
v_t = \sum_{i=1}^t \beta_2^{t-i}(1-\beta_2) \hat{g}_i^2
\end{equation*}
It follows by arithmetic inequality that
\begin{equation*}
\begin{split}
m_t^2 &= \left( \sum_{i=1}^t \frac{(1-\beta_1)\beta_1^{t-i}}{\sqrt{(1-\beta_2)\beta_2^{t-i}}} \sqrt{(1-\beta_2)\beta_2^{t-i}} \hat{g}_i\right)^2 \\
&\leq \left(\sum_{i=1}^t \frac{(1 - \beta_1)^2\beta_1^{2(t-i)}}{(1-\beta_2)\beta_2^{t-i}}\right)
\left(\sum_{i=1}^t (1-\beta_2)\beta_2^{t-i}\hat{g}_i^2\right) 
= \left(\sum_{i=1}^t \frac{(1 - \beta_1)^2\beta_1^{2(t-i)}}{(1-\beta_2)\beta_2^{t-i}}\right)
v_t.
\end{split}
\end{equation*}
Further, we have
\begin{equation*}\label{1-005} 
\sum_{i=1}^t \frac{(1 - \beta_1)^2\beta_1^{2(t-i)}}{(1-\beta_2)\beta_2^{t-i}}
\leq \frac{1}{1-\beta_2}\sum_{i=1}^t \left(\frac{\beta_1^2}{\beta_2}\right)^{t-i} = \frac{1}{1-\beta_2}\sum_{k=0}^{t-1} \zeta^k 
\leq \frac{1}{(1-\zeta)(1-\beta_2)}. 
\end{equation*}
The proof is completed.
\end{proof}



\medskip

\begin{lemma}\label{lem1-004}
The following estimate holds
\[
\sum_{t=1}^T\|\Delta_t\|^2 \leq \frac{\alpha^2 G^2}{\epsilon}
\]
\end{lemma}

\begin{proof}
By using the definition of $m_{t}$, it holds $\|m_t\|^2\leq G^2$. 

Then,  $\|\Delta_t\|^2 = \|\frac{\alpha_tm_t}{\sqrt{v_t}+\epsilon}\|^2\leq \frac{G^2}{\epsilon}\alpha_t^2$ by using the definition of $ \Delta_{t}$. 

Therefore, $\small \sum_{t=1}^T\|\Delta_t\|^2 \leq \frac{G^2}{\epsilon}\sum_{t=1}^T\frac{\alpha^2}{T} = \frac{G^2\alpha^2}{\epsilon}$.

\end{proof}

\medskip


\begin{lemma}\label{lem1-005}
With the Assumption \ref{assumption_3} and \ref{assumption_4}, it holds that
\[
\mathbb{E} \left\langle \nabla f\left(\theta_t\right), \hat{\eta}_t\hat{g}_t \right\rangle \geq h_0\mathbb{E}  \left\|\nabla f\left(\theta_t\right)\right\|_{\hat{\eta}_t}^2. 
\]
\end{lemma}
\begin{proof}
    According to Assumption \ref{assumption_4}, it holds that
    \[
    \begin{split}
    \left\langle \nabla_i f\left(\theta_t\right), g_{t,i}\right\rangle &= -\frac{1}{2} \left(\left\|\nabla_i f\left(\theta_t\right) - g_{t,i}\right\|^2 - \left\|\nabla_i f\left(\theta_t\right)\right\|^2 - \left\|g_{t,i}\right\|^2\right)\\
    &\geq (1-p^2) \left\|\nabla_i f\left(\theta_t\right)\right\|^2 \geq 0.    
    \end{split}
    \]

    Thus, it holds that
    \[
    \begin{split}
    \mathbb{E} \left[\left\langle \nabla f(x_t), \hat{\eta}_t \hat{g}_t \right\rangle \right]&= \mathbb{E} \left[\sum_i \left\langle \nabla_i f(\theta_t), h_{t,i} \hat{\eta}_{t,i}g_{t,i} \right\rangle\right]\\
    &\geq  h_0 \mathbb{E}\left[ \sum_i \left\langle \nabla_i f(x_t), h_{t,i} \hat{\eta}_{t,i}g_{t,i} \right\rangle\right]\\
    &= h_0 \mathbb{E} \left\langle \nabla f(\theta_t), \hat{\eta}_t g_t \right\rangle = h_0 \mathbb{E} \|\nabla f(\theta_t)\|^2_{\hat{\eta}_t}.
    \end{split}
    \]
\end{proof}

\medskip


Let $\Delta_t := \theta_{t+1} - \theta_t = -\alpha_t m_t/(\sqrt{v_t}+\epsilon)$. Let $\hat{v}_t = \beta_2 v_{t-1} + (1-\beta_2) \delta_t^2$, where $\delta_t^2 = \mathbb{E}_t \left[\hat{g}_t^2\right]$ and let $\hat{\eta}_t = \alpha_t/\sqrt{\hat{v}_t + \epsilon}$. 


\begin{lemma}\label{lem1-006}
Let $M_t = \mathbb{E} \left[\langle \nabla f(\theta_{t}), \Delta_{t}\rangle + L\|\Delta_{t}\|^2\right]$. Let $\alpha_t = \alpha/\sqrt T$ and $\beta_2 = 1-\beta/T$. Then, for $T\geq 1$ we have
\begin{equation}\label{1-037}
\begin{split}
\sum_{t=1}^T M_t 
\leq \frac{C_2}{1-\sqrt{\zeta}} + \frac{LG^2\alpha^2}{(1-\sqrt{\zeta})\epsilon} - \frac{(1-\beta_1)h_0}{2}\sum_{t=1}^T\mathbb{E} \|\nabla f(\theta_t)\|_{\hat{\eta}_t}^2,
\end{split}
\end{equation}
where $C_2 = \frac{5}{2(1-\beta_1)h_0 } \left((1-\beta_1)^2 \frac{4\alpha \beta G^4}{\epsilon^3} + \beta_1^2 \alpha\beta \left(\frac{G^4}{\beta_2 \epsilon^3} + \frac{(1+\epsilon) G^2}{(1-\zeta) \epsilon \beta_2} + \frac{G^4}{\beta_2}\right)\right).$
\end{lemma}

\begin{proof}
To split $M_t$, firstly we introduce the following two equalities. Using the definitions of $v_{t}$ and $\hat{v}_{t}$, we obtain
\[
   \begin{split}
       \frac{\left(1-\beta_1\right)\alpha_t \hat{g}_t}{\sqrt{v_t}+\epsilon} &= \frac{\left(1-\beta_1\right)\alpha_t \hat{g}_t}{\sqrt{\hat{v}_t}+\epsilon} + \left(1-\beta_1\right)\alpha_t\hat{g}_t\left(\frac{1}{\sqrt{v_t}+\epsilon} - \frac{1}{\sqrt{\hat{v}_t}+\epsilon}\right)\\
       & = \left(1-\beta_1\right)\hat{\eta}_t\hat{g}_t + \left(1-\beta_1\right)\alpha_t\hat{g}_t\frac{\left(1-\beta_2\right)\left(\sigma_t^2-\hat{g}_t^2\right)}{\left(\sqrt{v_t}+\epsilon\right)\left(\sqrt{\hat{v}_t}+\epsilon\right)\left(\sqrt{v_t} + \sqrt{\hat{v}_t}\right)}\\
       & = \left(1-\beta_1\right)\hat{\eta}_t\hat{g}_t + \left(1-\beta_1\right)\hat{\eta}_t\hat{g}_t\frac{\left(1-\beta_2\right)\left(\sigma_t^2-\hat{g}_t^2\right)}{\left(\sqrt{v_t}+\epsilon\right)\left(\sqrt{v_t} + \sqrt{\hat{v}_t}\right)}
   \end{split}
\]

In addition, we can obtain:
\[ 
\begin{split}
& \beta_1\alpha_tm_{t-1}\left(\frac{1}{\sqrt{\beta_2 v_{t-1}}+\sqrt{\beta_2} \epsilon} - \frac{1}{\sqrt{v_t}+\epsilon}\right) \\
&= \beta_1\alpha_tm_{t-1}\frac{\left(1-\beta_2\right)\hat{g}_t^2}{\left(\sqrt{v_t}+\epsilon\right)\left(\sqrt{\beta_2 v_{t-1}} + \sqrt{\beta_2}\epsilon\right)\left(\sqrt{v_t} + \sqrt{\beta_2 v_{t-1}}\right)}
+ \beta_1 \alpha_t m_{t-1} \frac{\left(1-\sqrt{\beta_2}\right)\epsilon}{\left(\sqrt{v_t} + \epsilon\right)\left(\sqrt{\beta_2v_{t-1}} + \sqrt{\beta_2}\epsilon\right)}\\
& = \beta_1\alpha_tm_{t-1}\frac{\left(1-\beta_2\right)\hat{g}_t^2}{\left(\sqrt{\hat{v}_t}+\epsilon\right)\left(\sqrt{\beta_2 v_{t-1}} + \sqrt{\beta_2}\epsilon\right)\left(\sqrt{v_t} + \sqrt{\beta_2 v_{t-1}}\right)} 
\\
&\qquad + \beta_1\alpha_tm_{t-1}\frac{\left(1-\beta_2\right)\hat{g}_t^2}{\left(\sqrt{\beta_2 v_{t-1}} + \sqrt{\beta_2}\epsilon\right)\left(\sqrt{v_t} + \sqrt{\beta_2 v_{t-1}}\right)}\left(\frac{1}{\sqrt{\hat{v}_t} + \epsilon}- \frac{1}{\sqrt{v_t} + \epsilon}\right)\\
&\qquad + \beta_1 \alpha_t m_{t-1} \frac{\left(1-\sqrt{\beta_2}\right)\epsilon}{\left(\sqrt{\hat{v}_t} + \epsilon\right)\left(\sqrt{\beta_2v_{t-1}} + \sqrt{\beta_2}\epsilon\right)} + \beta_1 \alpha_t m_{t-1} \frac{\left(1-\sqrt{\beta_2}\right)\epsilon}{\sqrt{\beta_2v_{t-1}} + \sqrt{\beta_2}\epsilon}\left(\frac{1}{\sqrt{\hat{v}_t} + \epsilon}- \frac{1}{\sqrt{v_t} + \epsilon}\right)\\
& = \beta_1m_{t-1}\hat{\eta}_t\frac{\left(1-\beta_2\right)\hat{g}_t^2}{\left(\sqrt{\beta_2 v_{t-1}} + \sqrt{\beta_2}\epsilon\right)\left(\sqrt{v_t} + \sqrt{\beta_2 v_{t-1}}\right)}\\
& \qquad + \beta_1\hat{\eta}_tm_{t-1}\frac{(1-\beta_2)^2\hat{g}_t^2(\sigma_t^2 -\hat{g}_t^2)}{(\sqrt{v_t}+\epsilon)(\sqrt{v_t} + \sqrt{\hat{v}_t})(\sqrt{\beta_2v_{t-1}} +\sqrt{\beta_2}\epsilon)(\sqrt{v_t} + \sqrt{\beta_2v_{t-1}})}
\\&\qquad  + \beta_1 \hat{\eta}_t  m_{t-1}\frac{(1-\sqrt{\beta_2})\epsilon}{\sqrt{\beta_2 v_{t-1}} + \sqrt{\beta_2}\epsilon} + \beta_1\hat{\eta}_tm_{t-1}\frac{(1-\sqrt{\beta_2})(1-\beta_2)\epsilon(\sigma_t^2 - \hat{g}_t^2)}{(\sqrt{v_t} + \epsilon)(\sqrt{v_t} + \sqrt{\hat{v}_t})(\sqrt{\beta_2v_{t-1}} + \sqrt{\beta_2}\epsilon)} .
\end{split}
\]

For simplicity, we denote 
\[
\begin{split}
    &A_t^1 = \left(1-\beta_1\right)\sqrt{\hat{\eta}_t}\hat{g}_t\frac{\left(1-\beta_2\right)\left(\sigma_t^2-\hat{g}_t^2\right)}{\left(\sqrt{v_t}+\epsilon\right)\left(\sqrt{v_t} + \sqrt{\hat{v}_t}\right)}\\
    &A_t^2 = \beta_1m_{t-1}\sqrt{\hat{\eta}_t}\frac{\left(1-\beta_2\right)\hat{g}_t^2}{\left(\sqrt{\beta_2 v_{t-1}} + \sqrt{\beta_2}\epsilon\right)\left(\sqrt{v_t} + \sqrt{\beta_2 v_{t-1}}\right)}\\
    &A_t^3 = \beta_1\sqrt{\hat{\eta}_t}m_{t-1}\frac{(1-\beta_2)^2\hat{g}_t^2(\sigma_t^2 -\hat{g}_t^2)}{(\sqrt{v_t}+\epsilon)(\sqrt{v_t} + \sqrt{\hat{v}_t})(\sqrt{\beta_2v_{t-1}} +\sqrt{\beta_2}\epsilon)(\sqrt{v_t} + \sqrt{\beta_2v_{t-1}})}\\
    &A_t^4 = \beta_1 \sqrt{\hat{\eta}_t} m_{t-1}\frac{(1-\sqrt{\beta_2})\epsilon}{\sqrt{\beta_2 v_{t-1}} + \sqrt{\beta_2}\epsilon}\\
    &A_t^5 = \beta_1\sqrt{\hat{\eta}_t}m_{t-1}\frac{(1-\sqrt{\beta_2})(1-\beta_2)\epsilon(\sigma_t^2 - \hat{g}_t^2)}{(\sqrt{v_t} + \epsilon)(\sqrt{v_t} + \sqrt{\hat{v}_t})(\sqrt{\beta_2v_{t-1}} + \sqrt{\beta_2}\epsilon)}\\
\end{split}
\]


Then, we obtain
\[
\begin{split}
    \Delta_t - \frac{\beta_1 \alpha_t}{\sqrt{\beta_2}\alpha_{t-1}}\Delta_{t-1} &= -\frac{\alpha_tm_t}{\sqrt{v_t} +\epsilon} + \frac{\beta_1\alpha_t m_{t-1}}{\sqrt{\beta_2 v_{t-1} + \sqrt{\beta_2}\epsilon}}\\
     &= -\frac{(1-\beta_1) \alpha_t \hat{g}_t}{\sqrt{v_t} + \epsilon} + \beta_1 \alpha_t m_{t-1} \left(\frac{1}{\sqrt{\beta_2v_{t-1}}+\sqrt{\beta_2}\epsilon} - \frac{1}{\sqrt{v_t} + \epsilon}\right)\\
     & = -(1-\beta_1)\hat{\eta}_{t} \hat{g}_t -\sqrt{\hat{\eta}_t}A_{t}^1 +\sqrt{\hat{\eta}_t} A_t^2+\sqrt{\hat{\eta}_t} A_t^3+ \sqrt{\hat{\eta}_t}A_t^4+\sqrt{\hat{\eta}_t} A_t^5
\end{split}
\]

Thus, it holds that
\begin{equation}
\label{lemma6-eq1}
\begin{split}
    \mathbb{E} \left\langle \nabla f(\theta_t),\Delta_t\right\rangle & = \frac{\beta_1\alpha_t}{\sqrt{\beta_2}\alpha_{t-1}} \left\langle \nabla f(\theta_t),\Delta_{t-1}\right\rangle + \mathbb{E} \left\langle \nabla f(\theta_t), \Delta_t - \frac{\beta_1\alpha_t}{\sqrt{\beta_2}\alpha_{t-1}}\Delta_{t-1}\right\rangle\\
    &=\frac{\beta_1\alpha_t}{\sqrt{\beta_2}\alpha_{t-1}}\left(\mathbb{E}\langle \nabla f(\theta_t), \Delta_{t-1}\rangle + \mathbb{E}\langle \nabla f(\theta_{t}) - \nabla f(\theta_{t-1}),\Delta_{t-1}\rangle \right)\\
    &\qquad  + \mathbb{E} \langle \nabla f(\theta_t), -(1-\beta_1) \hat{\eta}_t \hat{g}_t\rangle + \mathbb{E} \langle \nabla f(\theta_t), -\sqrt{\hat{\eta}_t}A_t^1\rangle+ \mathbb{E} \langle \nabla f(\theta_t), \sqrt{\hat{\eta}_t}A_t^2\rangle\\
    &\qquad + \mathbb{E} \langle \nabla f(\theta_t), \sqrt{\hat{\eta}_t}A_t^3\rangle + \mathbb{E} \langle \nabla f(\theta_t), \sqrt{\hat{\eta}_t}A_t^4\rangle+ \mathbb{E} \langle \nabla f(\theta_t), \sqrt{\hat{\eta}_t}A_t^5\rangle 
\end{split}
\end{equation}

For the first term of \eqref{lemma6-eq1}, it holds that
\[
\begin{split}
    &\frac{\beta_1\alpha_t}{\sqrt{\beta_2}\alpha_{t-1}}\left(\mathbb{E}\langle \nabla f(\theta_t), \Delta_{t-1}\rangle + \mathbb{E}\langle \nabla f(\theta_{t}) - \nabla f(\theta_{t-1}),\Delta_{t-1}\rangle \right)\\
    &\leq \frac{\beta_1\alpha_t}{\sqrt{\beta_2}\alpha_{t-1}}\left(\mathbb{E}\langle \nabla f(\theta_t), \Delta_{t-1}\rangle + \mathbb{E}\|\nabla f(\theta_{t}) - \nabla f(\theta_{t-1})\|\|\Delta_{t-1}\| \right)\\
    &\leq \frac{\beta_1\alpha_t}{\sqrt{\beta_2}\alpha_{t-1}}\left(\mathbb{E}\langle \nabla f(\theta_t), \Delta_{t-1}\rangle + L\mathbb{E} \|\Delta_{t-1}\|^2 \right)\\
    & = \frac{\beta_1\alpha_t}{\sqrt{\beta_2}\alpha_{t-1}}M_{t-1}
\end{split}
\]

For the second term of \eqref{lemma6-eq1}, it holds that
\[
\mathbb{E} \langle \nabla f(\theta_t), -(1-\beta_1) \hat{\eta}_t \hat{g}_t\rangle \leq -(1-\beta_1) h_0 \mathbb{E} \|\nabla f(\theta_t)\|_{\hat{\eta}_t}^2.
\]


For the rest of the terms, it holds that
\[
\begin{split}
    \mathbb{E} \langle \nabla f(\theta_t), -\sqrt{\hat{\eta}_t} A_t^1 \rangle \leq \frac{h_0(1-\beta_1)}{10}\mathbb{E} \|\nabla f(\theta_t)\|_{\hat{\eta}_t}^2 + \frac{5}{2(1-\beta_1)h_0}\left\|A_t^1\right\|^2\\
    \mathbb{E} \langle \nabla f(\theta_t), +\sqrt{\hat{\eta}_t} A_t^2 \rangle \leq \frac{h_0(1-\beta_1)}{10}\mathbb{E} \|\nabla f(\theta_t)\|_{\hat{\eta}_t}^2 + \frac{5}{2(1-\beta_1)h_0}\left\|A_t^2\right\|^2\\
    \mathbb{E} \langle \nabla f(\theta_t), +\sqrt{\hat{\eta}_t} A_t^3 \rangle \leq \frac{h_0(1-\beta_1)}{10}\mathbb{E} \|\nabla f(\theta_t)\|_{\hat{\eta}_t}^2 + \frac{5}{2(1-\beta_1)h_0}\left\|A_t^3\right\|^2\\
    \mathbb{E} \langle \nabla f(\theta_t), +\sqrt{\hat{\eta}_t} A_t^4 \rangle \leq \frac{h_0(1-\beta_1)}{10}\mathbb{E} \|\nabla f(\theta_t)\|_{\hat{\eta}_t}^2 + \frac{5}{2(1-\beta_1)h_0}\left\|A_t^4\right\|^2\\
    \mathbb{E} \langle \nabla f(\theta_t), +\sqrt{\hat{\eta}_t} A_t^5 \rangle \leq \frac{h_0(1-\beta_1)}{10}\mathbb{E} \|\nabla f(\theta_t)\|_{\hat{\eta}_t}^2 + \frac{5}{2(1-\beta_1)h_0}\left\|A_t^5\right\|^2\\
\end{split}
    \]

On the other hand, it holds that
\[
\begin{split}
&\left\|A_t^1\right\|^2 \leq (1-\beta_1)^2 \frac{4\alpha\beta G^4}{T \epsilon^3}, 
\left\|A_t^2\right\|^2 \leq \beta_1^2 \frac{\alpha \beta G^4}{T\beta_2 \epsilon^3}, 
\left\|A_t^3\right\|^2 \leq \beta_1^2 \frac{\alpha \beta G^2}{(1-\zeta) \epsilon T \beta_2},\\
&\left\|A_t^4\right\|^2 \leq \beta_1^2 \frac{\alpha \beta G^4}{T \beta_2}, 
\left\|A_t^5\right\|^2 \leq \beta_1^2 \frac{\alpha \beta G^2}{(1-\zeta) \beta_2  T}
\end{split}
\]
\end{proof}

Define $N_t = \frac{C_2}{T}+ L\mathbb{E}\|\Delta_t\|^2$, where $C_2 = \frac{5}{2(1-\beta_1)h_0 } \left((1-\beta_1)^2 \frac{4\alpha \beta G^4}{\epsilon^3} + \beta_1^2 \alpha\beta \left(\frac{G^4}{\beta_2 \epsilon^3} + \frac{(1+\epsilon) G^2}{(1-\zeta) \epsilon \beta_2} + \frac{G^4}{\beta_2}\right)\right)$.
It holds that
\[
    M_t \leq \frac{\beta_1\alpha_t}{\sqrt{\beta_2}\alpha_{t-1}}M_{t-1} + N_t - \frac{1-\beta_1}{2} \hat{\eta}_t \mathbb{E} \|\nabla f(\theta_t)\|^2_{\hat{\eta}_t}
    \leq \sum_{i=1}^t \sqrt{\zeta}^{t-i} N_i - \frac{1-\beta_1}{2} h_0 \mathbb{E} \|\nabla f(\theta_t)\|_{\hat{\eta}_t}^2 
\]

Thus, by summing $t$ from 1 to $T$, it holds that
\[
\begin{split}
\sum_{t=1}^T M_t &\leq \sum_{t=1}^T \sum_{i=1}^t \sqrt{\zeta}^{t-i} N_i - \frac{(1-\beta_1)h_0}{2}  \mathbb{E} \|\nabla f(\theta_t)\|_{\hat{\eta}_t}^2\\
& \leq \frac{1}{1-\sqrt{\zeta}}\sum_{t=1}^T N_t - \frac{(1-\beta_1)h_0}{2}\mathbb{E} \|\nabla f(\theta_t)\|_{\hat{\eta}_t}^2\\
&\leq \frac{C_2}{1-\sqrt{\zeta}} + \frac{LG^2\alpha^2}{(1-\sqrt{\zeta})\epsilon} - \frac{(1-\beta_1)h_0}{2}\sum_{t=1}^T\mathbb{E} \|\nabla f(\theta_t)\|_{\hat{\eta}_t}^2.
\end{split}
\]
\medskip

\begin{lemma}\label{lem1-007}
  Let $\tau$ be randomly chosen from  $\{1,2,\cdots,T\}$ with equal probabilities $p_\tau = \frac{1}{T}$. We have the following estimate:
  \[ 
  \mathbb{E}[\|\nabla f\left(\theta_\tau\right)\|^2]\leq \frac{\sqrt{G^2+\epsilon d}}{\alpha\sqrt{T}}\mathbb{E}\left[\sum_{t=1}^T\|\nabla f\left(\theta_t\right)\|_{\hat{\eta_t}}^2\right].
  \]
\end{lemma}
\begin{proof}
Note that $\|\hat{v}_t\|_1 = \beta_2 \|v_{t-1}\|_1 +\left(1-\beta_2\right) \|\sigma_t\|^2$ and $\|\hat{g}_t\|\leq G$. It is straightforward to prove $\|v_t\|_1 \leq G^2$. Hence, we have $\|\hat{v}_t + \epsilon\|_1 \leq G^2 + \epsilon d$. 

Utilizing this inequality, we have
\[
\begin{split} 
     \|\nabla f\left(\theta_t\right)\|^2 &= \frac{\|\nabla f\left(\theta_t\right)\|^2}{\sqrt{\|\hat{v}_t+\epsilon\|_1}}\sqrt{\|\hat{v}_t+\epsilon\|_1} = \sqrt{\|\hat{v}_t +\epsilon \|_1}\sum_{k=1}^d\frac{|\nabla_k f\left(\theta_t\right)|^2}{\sqrt{\sum_{l=1}^d \hat{v}_{t,l}+\epsilon}}\\
     &\leq \sqrt{\|\hat{v}_t+\epsilon\|_1}\alpha_t^{-1}\sum_{k=1}^d \frac{\alpha_t}{\sqrt{\hat{v}_{t,k}+\epsilon}}|\nabla_k f\left(\theta_t\right)|^2 
     = \sqrt{\|\hat{v}_t+\epsilon\|_1}\alpha_t^{-1}\|\nabla f\left(\theta_t\right)\|_{\hat{\eta}_t}^2 \\ 
     &\leq \sqrt{G^2+\epsilon d}\alpha_t^{-1}\|\nabla f\left(\theta_t\right)\|_{\hat{\eta}_t}^2 
       \leq \frac{\sqrt{G^2 + \epsilon d}}{\alpha_T}\|\nabla f\left(\theta_t\right)\|^2_{\hat{\eta}_t}.
\end{split}
\]
Then, by using the definition of $\theta_{\tau}$, we obtain
\[
     \mathbb{E}\left[\|\nabla f\left(\theta_\tau\right)\|^2\right] = \frac{1}{T}\sum_{t=1}^T\mathbb{E}\left[\|\nabla f\left(\theta_t\right)\|^2\right] \leq \frac{\sqrt{G^2 + \epsilon d}}{\alpha\sqrt{T}} \mathbb{E}\left[\sum_{t=1}^T \|\nabla f\left(\theta_t\right)\|^2_{\hat{\eta}_t}\right].
\]
Thus, the desired result is obtained. 
\end{proof}


\begin{theorem}
    Let $\{\theta_t\}$ be a sequence generated by AdaGC for initial values $\theta_1$ and $m_0 = v_0 = 0$. Assumptions \ref{assumption_1} to \ref{assumption_4} hold. With the hyperparameters $\alpha_t = \alpha/\sqrt{T}$,$\beta_2 = 1-\beta/T$ and $\zeta = \beta_1^2/\beta_2 < 1$. Let $\tau$ be randomly chosen from $\{1,2,\cdots, T\}$ with equal probabilities. We have
    \[
    \mathbb{E} \|\nabla f(\theta_\tau)\|^2 \leq \frac{C}{\sqrt{T}}
    \]
    where $C = \frac{\sqrt{G^2+\epsilon d}}{\alpha}\left(f(\theta_1) - \underline{f} +  \frac{C_2}{1-\sqrt{\zeta}} + \frac{LG^2\alpha^2}{(1-\sqrt{\zeta})\epsilon}\right)$ and $C_2 = \frac{5}{2(1-\beta_1)h_0 } \left((1-\beta_1)^2 \frac{4\alpha \beta G^4}{\epsilon^3} + \beta_1^2 \alpha\beta \left(\frac{G^4}{\beta_2 \epsilon^3} + \frac{(1+\epsilon) G^2}{(1-\zeta) \epsilon \beta_2} + \frac{G^4}{\beta_2}\right)\right)$
\end{theorem}
\begin{proof}
    With the Lipschitz continuity condition of $f$, it holds that
    \[
    \mathbb{E} f(\theta_{t+1}) \leq \mathbb{E} \left[f(\theta_t) + \langle \nabla f(\theta_t), \Delta_t\rangle + \frac{L}{2} \|\Delta_t\|^2\right] \leq \mathbb{E} f(\theta_t) + M_t.
    \]

    By summing $t$ from $1$ to $T$, it holds that
    \[
    \mathbb{E} f(\theta_{T+1}) \leq f(\theta_1) + \sum_{t=1}^T M_t \leq f(\theta_1) + \frac{C_2}{1-\sqrt{\zeta}} + \frac{LG^2\alpha^2}{(1-\sqrt{\zeta})\epsilon} - \frac{(1-\beta_1)h_0}{2}\sum_{t=1}^T\mathbb{E} \|\nabla f(\theta_t)\|_{\hat{\eta}_t}^2
    \]

    Thus, it holds that
    \[
    \begin{split}
    \mathbb{E} \left[\|\nabla f(\theta_\tau\|^2\right] &\leq \frac{\sqrt{G^2+\epsilon d}}{\alpha\sqrt{T}}\mathbb{E} \left[\sum_{t=1}^T \|\nabla f(\theta_t)\|_{\hat{\eta}_t}^2\right]\\
    &\leq \frac{\sqrt{G^2+\epsilon d}}{\alpha\sqrt{T}}\left(f(\theta_1) - \mathbb{E} [f(\theta_{T+1})] +  \frac{C_2}{1-\sqrt{\zeta}} + \frac{LG^2\alpha^2}{(1-\sqrt{\zeta})\epsilon}\right) \\\
    & \leq \frac{\sqrt{G^2+\epsilon d}}{\alpha\sqrt{T}}\left(f(\theta_1) - \underline{f} +  \frac{C_2}{1-\sqrt{\zeta}} + \frac{LG^2\alpha^2}{(1-\sqrt{\zeta})\epsilon}\right)
    \end{split}
    \]
\end{proof}



\section{LLMs Hpyer-Parameters}\label{appendix:hyper_parameters}

Table~\ref{tab:llm_hyper_param} shows the hyper-parameters used in our experiments on LLMs. Note that Llama-2 Tiny is a smaller version of the Llama-2 model, designed based on the network configuration of the GPT-2 model with 345 million parameters. We named it Llama-2 Tiny to facilitate our initial experimental exploration.
\begin{table*}[!h]
    \centering
    \caption{Hyper-parameters used in our LLMs experiments. $\lambda_{abs}$ represents the absolute global clipping threshold of GlobalGC. $\lambda_{rel}$ and $\beta$ represent the relative clipping threshold and the momentum of our AdaGC, respectively.}
    \label{tab:llm_hyper_param}
    \vskip 0.1in
    \resizebox{0.6\linewidth}{!}{\input{table/hyperparameter_llm}}
    \vskip -0.1in
\end{table*}






\section{Experimental Details for CLIP}


In addition to pre-training tasks for Large Language Models (LLMs), we have conducted pre-training for large-scale Vision-Language Models (LVLMs), with a specific focus on the widely acknowledged CLIP model~\cite{radford2021learning}. We train the CLIP models on the LAION-400M~\cite{schuhmann2021laion} dataset and conduct zero-shot evaluations on the ImageNet~\cite{russakovsky2015imagenet} dataset. We opt for the ViT-Base configuration, which contains 151 million parameters. The ViT-Base model is trained over 20K steps, observing 320M samples. In alignment with~\cite{wortsman2023stable}, we implement patch-dropout 0.5~\cite{li2023scaling}, learning rate 0.002 and weight decay 0.2, with the initial 5K steps dedicated to linear warmup~\cite{goyal2017accurate} and the subsequent steps follow a cosine decay pattern~\cite{loshchilov2016sgdr}.


\section{Results of Batch Size Scalability Experiments}
\label{appendix:llama_tiny_large_bs}


\begin{figure*}[!ht]
    \vskip 0.1in
    \centering
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figs/gpt2_batch_size_512.pdf}
        \caption{batch size = $512$, lr = $3.0 \times 10^{-4}$}
        \label{fig:gpt2_bs_512}
    \end{subfigure}\hfill
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figs/gpt2_batch_size_2048.pdf}
        \caption{batch size = $2048$, lr = $6.0 \times 10^{-4}$}
        \label{fig:gpt2_bs_2048}
    \end{subfigure}\vfill
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figs/gpt2_batch_size_4096.pdf}
        \caption{batch size = $4096$, lr = $8.5 \times 10^{-4}$}
        \label{fig:gpt2_bs_4096}
    \end{subfigure}\hfill
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figs/gpt2_batch_size_8192.pdf}
        \caption{batch size = $8192$, lr = $1.2 \times 10^{-3}$}
        \label{fig:gpt2_bs_8192}
    \end{subfigure}
    \caption{AdaGC's large-batch scalability on GPT-2 345M: Maintains 0 loss spikes (vs GlobalGC's 100\% rate) with batch sizes up to 8192 (16$\times$ baseline) and proportional learning rate scaling, achieving 1.28\%-19.55\% perplexity reduction.}
    \label{fig:gpt2_large_bs_and_lr}
    \vskip -0.1in
\end{figure*}

\begin{figure*}[!t]
    \vskip 0.1in
    \centering
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figs/all_llama_tiny_batch_size_512_new.pdf}
        \caption{batch size = $512$, lr = $3.0 \times 10^{-4}$}
        \label{fig:llama_tiny_bs_512}
    \end{subfigure}\hfill
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figs/all_llama_tiny_batch_size_2048_new.pdf}
        \caption{batch size = $2048$, lr = $6.0 \times 10^{-4}$}
        \label{fig:llama_tiny_bs_2048}
    \end{subfigure}\vfill
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figs/all_llama_tiny_batch_size_4096_new.pdf}
        \caption{batch size = $4096$, lr = $8.5 \times 10^{-4}$}
    \end{subfigure}\hfill
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figs/all_llama_tiny_batch_size_8192_new.pdf}
        \caption{batch size = $8192$, lr = $1.2 \times 10^{-3}$}
    \end{subfigure}
    \caption{AdaGC's large-batch scalability on Llama-2 Tiny: Cross-architecture validation shows 6.19\% lower perplexity at 8192 batch size.}
    \label{fig:llama_tiny_large_bs_and_lr}
    \vskip -0.1in
\end{figure*}
