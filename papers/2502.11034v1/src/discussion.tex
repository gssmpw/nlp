\section{Discussion}
\subsection{Socio-Economic Value}
Pre-training large models entails substantial costs and is vulnerable to significant economic losses due to instabilities in the training process. Loss spikes, typically addressed by resuming training from the latest checkpoint as illustrated by the PaLM~\cite{chowdhery2023palm} approach, can have severe economic impacts. According to the MegaScale~\cite{jiang2024megascale} report, the average recovery time from an interruption is about 15 minutes. Considering the Llama-3~\footnote{\url{https://ai.meta.com/blog/meta-llama-3/}} model, which utilizes 16,000 NVIDIA H100 GPUs priced between \$2.29 to \$2.49 per hour~\footnote{\url{https://lambdalabs.com/}}, each interruption due to a Loss Spike can lead to direct economic losses ranging from approximately \$9,160 to \$9,960. Our proposed adaptive gradient clipping method effectively reduces the frequency of Loss Spikes, thereby enhancing training stability and minimizing economic waste.

\subsection{Limitations}
Resource constraints prevented this study from conducting long-term training experiments on the Llama-2 open-source model. However, our internal business validations indicate that the method remains effective throughout the mid to late stages of training. Future plans, resources permitting, include conducting comprehensive experiments on the Llama-2 model and sharing the findings with the academic community.
