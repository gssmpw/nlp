\section{Methodology: AdaGC}
\label{sec:adagc}

\subsection{Preliminaries}
\textbf{Notations.} Let $x_t \in \mathbb{R}^d$ denote a parameter vector where $x_t^j$ represents its $j$-th coordinate for $j \in [d]$. We write $\nabla_x f(x)$ for the gradient of any differentiable function $f: \mathbb{R}^d \rightarrow \mathbb{R}$, and use $u^2$ and $u/v$ to denote element-wise square and division operations for vectors $u,v \in \mathbb{R}^d$. The $\ell_2$-norm and $\ell_\infty$-norm are denoted by $\|\cdot\|$ and $\|\cdot\|_\infty$, respectively. For asymptotic comparisons, we write $f = \mathcal{O}(g)$ if $\exists c > 0$ such that $f(x) \leq cg(x)$ for all $x$ in the domain.

\textbf{Gradient Clipping Fundamentals.} Consider a stochastic optimization problem with parameters $\theta \in \mathbb{R}^d$ and loss function $f(\theta; X_t)$ evaluated on mini-batch $X_t$ at step $t$. Standard gradient descent updates follow:
\begin{equation}
    \theta_t = \theta_{t-1} - \alpha \nabla_\theta f(\theta_{t-1}, X_t)
\end{equation}
To prevent unstable updates from gradient explosions, gradient clipping~\cite{pascanu2013difficulty} modifies the update rule as:
\begin{equation}
    \begin{array}{c}
    \theta_t = \theta_{t-1} - \alpha h_t \nabla_{\theta}f(\theta_{t-1}, X_t) \\[0.4em]
    \text{ where } h_t := \min \left\{\frac{\lambda_{abs}}{\|\nabla_{\theta}f(\theta_{t-1}; X_{t})\|}, 1.0 \right\} 
    \end{array}
\end{equation}

Here $\lambda_{\text{abs}}$ is an absolute clipping threshold requiring careful tuning. Our work focuses on \textit{norm-based} clipping (scaling entire gradients exceeding $\lambda_{\text{abs}}$) rather than \textit{value-based} clipping (element-wise truncation).

\subsection{Adaptive Gradient Clipping based on Local Gradient Norm}
This section introduces a novel gradient clipping strategy termed Adaptive Gradient Clipping (AdaGC), which distinguishes itself by not relying on a global gradient norm. Instead, AdaGC focuses on the local gradient norm of each parameter and utilizes a dynamic adaptive mechanism for gradient clipping. The proposed method employs an Exponential Moving Average (EMA) mechanism to maintain smoothed estimates of historical gradient norms per parameter, thus enhancing the accuracy of anomalous gradient detection and enabling independent clipping adjustments tailored to each parameter's specific conditions. EMA is widely used in deep learning, and within AdaGC, it facilitates the balancing of historical and current gradient norms. The formulation is as follows:
\begin{equation}
    \label{eq:AdaptiveNormClip}
    \begin{array}{c}
    \boldsymbol{g}_{t, i} \leftarrow h_{t, i} \cdot \boldsymbol{g}_{t, i} , \text{where } h_{t, i} = \min \left\{\lambda_{rel} \frac{\gamma_{t-1, i}}{\|\boldsymbol{g}_{t, i}\|}, 1.0\right\}, \\[1em]
    \gamma_{t, i} = \beta \gamma_{t-1, i} + (1 - \beta) \|\boldsymbol{g}_{t, i}\|.
    \end{array}
\end{equation}
Here, $\lambda_{rel}$ is a predefined relative clipping threshold, $\boldsymbol{g}_{t,i}$ represents the gradient of the $i$-th parameter at time step $t$, and $h_{t,i}$ is a clipping function activated when $\|\boldsymbol{g}_{t, i}\| > \lambda_{rel} \cdot \gamma_{t-1,i}$, thereby scaling the gradient norm to $\lambda_{rel} \cdot \gamma_{t-1,i}$. Additionally, $\beta$ is the smoothing coefficient for EMA. We consistently incorporate the clipped gradient norm into the historical observations rather than the pre-clipped values.

Despite its simplicity, AdaGC adaptively adjusts based on the magnitude of each parameter's gradient norm. Whenever the gradient norm at a current timestep exceeds a predefined range of average norms within a historical window, it effectively suppresses these outlier gradients. 

However, during the initial stages of model training (e.g., the first 100 steps), the gradient norms are typically large and fluctuate significantly, indicating a substantial decreasing trend. Direct application of AdaGC during this period could lead to two issues: first, erroneously accumulating the early large gradient norms into the historical values, resulting in compounded errors; second, compared to traditional global-norm-based methods, AdaGC might delay clipping, thus potentially slowing down the loss reduction. To address these issues, we introduce a hyperparameter $T_{start}$(default set to 100), representing a warm-up period during which traditional global gradient norm clipping is applied.

Additionally, the AdaGC strategy can be seamlessly integrated with various optimizers, such as AdamW~\cite{loshchilov2017decoupled}, enhancing its practicality and flexibility. Algorithm~\ref{algorithm1} demonstrates its implementation with the AdamW optimizer.

Overall, AdaGC offers a more precise and effective gradient management method for large-scale model training, contributing to improved training stability and performance.

\begin{algorithm}[!ht]
    \small
    \caption{AdamW with AdaGC}
    \label{algorithm1}
    \setlength{\baselineskip}{1.25em}
\begin{algorithmic}[1]
    \STATE {\bfseries given:} $\{\alpha_t\}_{t=1}^{T}, \lambda_{w}, \epsilon_1, \beta_1, \beta_2, \textcolor{blue}{\beta} \in [0, 1), \textcolor{blue}{\lambda_{abs}}, \textcolor{blue}{T_{start}}$
    \STATE {\bfseries initialize:} $\boldsymbol{\theta}_{0}, m_0 \leftarrow 0, v_0 \leftarrow 0, t \leftarrow 0$
    \REPEAT
    \STATE \textbf{compute} $\boldsymbol{g}_{t}$ = $\nabla_{\boldsymbol{\theta}} f_t(\boldsymbol{\theta}_{t-1}, X_t)$
    \IF{\textcolor{blue}{$t < T_{start}$}}
    \STATE $\textcolor{blue}{h_{t} = \min \left\{\frac{\lambda_{abs}}{\|\boldsymbol{g}_{t}\|}, 1.0\right\}}$
    \STATE $\textcolor{blue}{\widehat{\boldsymbol{g}_t} = h_{t} \cdot \boldsymbol{g}_t}$
    \FOR{\textcolor{blue}{$i \in |\boldsymbol{\theta}|$}}
        \STATE $\textcolor{blue}{\gamma_{t, i} = \min \left\{\gamma_{t-1, i}, \|\widehat{\boldsymbol{g}_{t, i}}\|\right\}, \gamma_{0, i} = \|\widehat{\boldsymbol{g}_{1, i}}\|}$
    \ENDFOR
    \ELSE
    \FOR{\textcolor{blue}{$i \in |\boldsymbol{\theta}|$}}
        \STATE $\textcolor{blue}{h_{t, i} = \min \left\{\lambda_{rel} \frac{\gamma_{t-1, i}}{\|\boldsymbol{g}_{t, i}\|}, 1.0\right\}}$
        \STATE $\textcolor{blue}{\widehat{\boldsymbol{g}_{t, i}} = h_{t, i} \cdot \boldsymbol{g}_{t, i}}$
        \STATE $\textcolor{blue}{\gamma_{t, i} = \beta \gamma_{t-1, i} + (1 - \beta) \|\widehat{\boldsymbol{g}_{t, i}}\|}$
    \ENDFOR
    \ENDIF
    \STATE $\boldsymbol{m}_{t} = \beta_1 \boldsymbol{m}_{t-1} + (1-\beta_1) \textcolor{blue}{\widehat{\boldsymbol{g}_{t}}}$
    \STATE $\boldsymbol{v}_{t} = \beta_2 \boldsymbol{v}_{t-1} + (1-\beta_2) \textcolor{blue}{\widehat{\boldsymbol{g}_{t}}}^2 $
    \STATE $\widehat{\boldsymbol{m}_{t}} = \boldsymbol{m}_{t}/(1 - \beta_1^t), \quad \widehat{\boldsymbol{v}_{t}} = \boldsymbol{v}_{t}/(1 - \beta_2^t) $
    \STATE $\boldsymbol{\theta}_t = \boldsymbol{\theta}_{t-1} - \alpha_t \lambda_{w} \boldsymbol{\theta}_{t-1} - \alpha_t \widehat{\boldsymbol{m}_{t}}/(\sqrt{\widehat{\boldsymbol{v}_{t}}} + \epsilon_1)$
    \UNTIL{$\boldsymbol{\theta}_{t}$ not converge}
    \vskip -0.2in
\end{algorithmic}
\end{algorithm}


\subsection{Convergence Analysis}
In this section, we give the convergence guarantee of Adam with AdaGC stated as follows:
\begin{theorem}
    \label{theorem:1}
    Under mild assumptions, by selecting $\alpha_t = \mathcal{O}(1/\sqrt{T})$, $\beta_2 = 1- \mathcal{O}(1/T)$ and $\beta_1 < \sqrt{\beta_2}$, 
    when $\tau$ is randomly chosen from $\{1,2,\cdots,T\}$ with equal probabilities, it holds that
    \[
    \mathbb{E} \|\nabla f(\theta_\tau)\|^2  = \mathcal{O}\left(\frac{1}{\sqrt{T}}\right).
    \]
\end{theorem}
Theorem \ref{theorem:1} shows that even with local clipped gradient, Adam with AdaGC can converge in the same rate as vanilla Adam. Due to the limited space the formal assumptions, theorem statement with detailed proof can be found in Appendix \ref{appendix_converge}.





