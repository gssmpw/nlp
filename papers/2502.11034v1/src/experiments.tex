\section{Experiments}

This paper primarily focuses on the pre-training of large language models, using Llama-2~\cite{touvron2023llama} as our primary experimental subject. Experiments were conducted using NVIDIA GPU A100 40G. Moreover, our proposed method demonstrates strong generalization across various VLM models, achieving robust results.

\begin{table*}[h!]
  \centering
  \caption{Zero-shot performance of Llama-2 7B on WikiText and LAMBADA datasets.}
  \label{tab:llama_7b_zero_shot}
  \vskip 0.1in
  \resizebox{1.0\linewidth}{!}{\input{table/llama7b_zero_shot}}
  \vskip -0.2in
\end{table*}

\subsection{Experimental Setup}
\textbf{Models and Datasets:} We evaluated the efficacy of AdaGC on various Llama-2 models including Tiny, 7B and 13B. The C4-en~\cite{raffel2020exploring} dataset, a clean corpus of English text extracted from Common Crawl, served as our pre-training dataset. We assessed model performance by computing perplexity on the WikiText~\cite{merity2016pointer} and LAMBADA~\cite{paperno2016lambada} datasets. We also conducted experiments on GPT-2~\cite{radford2019language} and CLIP~\cite{radford2021learning} models to further validate AdaGC's generalization capabilities.

\textbf{Comparison Methods:} We selected several techniques including Global Gradient Clipping (GlobalGC for short)~\cite{pascanu2013difficulty}, Adaptive Gradient Clipping~\cite{brock2021high}, StableAdamW~\cite{wortsman2023stable}, and Scaled Embed~\cite{takase2023spike} from different approaches such as gradient clipping, update clipping, and parameter initialization for comparison.

\textbf{Training Details:} Pre-training large scale models, particularly Llama-2 7B and 13B, is typically resource-intensive. Like StableAdamW~\cite{wortsman2023stable} and Scaled Embed~\cite{takase2023spike}, our primary focus was to explore training instability rather than achieve ultimate accuracy. For ease of multiple experiments, we conducted 9,000 training steps on 36 billion tokens for both Llama-2 7B and 13B models, and 36,000 steps on 36 billion tokens for the Llama-2 Tiny model. For additional details on the hyperparameters, please refer to the Appendix~\ref{appendix:hyper_parameters}.

\textbf{Critical Hyperparameter Selection}. We conducted a systematic study to evaluate the two critical hyperparameters in AdaGC: the EMA coefficient $\beta$ and the relative clipping threshold $\lambda_{rel}$. Our analysis utilized coordinate descent~\cite{bertsekas1997nonlinear} for hyperparameter optimization on the Llama-2 Tiny model. Initially, we examined the relative clipping threshold $\lambda_{rel}$ with a fixed EMA coefficient $\beta = 0.98$, testing the values $\{1.01, 1.05, 1.10\}$. As depicted in Figure~\ref{fig:llama_tiny_clip_ratio}, $\lambda_{rel} = 1.05$ achieved optimal performance by effectively balancing gradient regulation effectiveness (compromised at $\lambda_{rel} \ge 1.10$) and update stability (degraded at $\lambda_{rel} \le 1.01$). With this optimal threshold established, we proceeded to investigate the EMA smoothing factor $\beta \in \{0.95, 0.98, 0.99\}$. Figure~\ref{fig:llama_tiny_beta} demonstrates that $\beta = 0.98$ provides superior adaptation speed while maintaining historical consistency, yielding the best performance. Consequently, we adopted the parameter settings $\lambda_{rel} = 1.05$ and $\beta = 0.98$ for all subsequent experiments reported in this paper.
\begin{figure*}[!ht]
    \vskip 0.1in
    \centering
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figs/all_llama_tiny_clip_ratio_new.pdf}
        \caption{Relative clipping threshold $\lambda_{\text{rel}}$.}
        \label{fig:llama_tiny_clip_ratio}
    \end{subfigure}\hfill
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figs/all_llama_tiny_beta_new.pdf}
        \caption{EMA coefficient $\beta$.}
        \label{fig:llama_tiny_beta}
    \end{subfigure}
    \caption{Study of AdaGC's hyperparameters on Llama-2 Tiny. (a) Relative clipping threshold analysis demonstrates $\lambda_{\text{rel}}=1.05$ achieves optimal gradient regulation. (b) EMA coefficient analysis reveals $\beta=0.98$ best balances historical consistency with rapid adaptation.}
    \label{fig:llama_tiny_ablation_study}
    \vskip -0.2in
\end{figure*}

\subsection{Experimental Results}
\begin{figure*}[!ht]
    \vskip 0.1in
    \centering
    \begin{subfigure}{1.0\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figs/all_llama_7b_new.pdf}
        \caption{Llama-2 7B training dynamics: AdaGC (green) eliminates loss spikes observed in baseline methods (GlobalGC-blue, ScaledEmbed-orange, AdaptiveGradientClip-purple, StableAdamW-cyan), achieving final training loss 0.02 lower than GlobalGC. Validation perplexity (right) shows correlated improvement with stable gradient norms (middle).}
        \label{fig:llama_7b}
    \end{subfigure}
    \vfill
    \begin{subfigure}{1.0\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figs/all_llama_13b_new.pdf}
        \caption{Llama-2 13B scalability analysis: AdaGC maintains superior stability at scale, reducing training loss and validation perplexity by 0.64\% and 1.47\% respectively versus GlobalGC, demonstrating method scalability.}
        \label{fig:llama_13b}
    \end{subfigure}
    \caption{Large language model training analysis: (a) 7B model comparison shows AdaGC's loss spike elimination and performance gains, (b) 13B results demonstrate method scalability.}
    \label{fig:llama_7b_and_13b_all}
    \vskip -0.2in
\end{figure*}



Our comprehensive evaluation demonstrates AdaGC's effectiveness across model scales and architectures. Figure~\ref{fig:llama_7b_and_13b_all} presents comparative results on Llama-2 7B and 13B models, analyzing training dynamics through loss trajectories, gradient norms, and validation perplexities. For the 7B model, baseline methods (GlobalGC, Scaled Embed, Adaptive Gradient Clipping, and StableAdamW) exhibit frequent loss spikes during training, while AdaGC completely eliminates these instability events. This stability translates to measurable performance gains, with AdaGC achieving a 0.02 lower final training loss (2.28 vs 2.30) and 2.1\% reduced validation perplexity compared to the strongest baseline (GlobalGC).

The approach demonstrates strong scalability on the Llama-2 13B model, where AdaGC eliminates loss spikes entirely compared to GlobalGC. Qualitative analysis reveals AdaGC's superior outlier gradient management, maintaining smoother loss trajectories throughout training. This enhanced stability yields concrete performance improvements: 0.65\% lower training loss (absolute difference of 0.0146) and 1.47\% reduced validation perplexity relative to GlobalGC.

Downstream zero-shot evaluation on WikiText and LAMBADA datasets (Table~\ref{tab:llama_7b_zero_shot}) confirms the practical benefits of stable training. AdaGC achieves state-of-the-art performance across all benchmarks, reducing WikiText perplexity by 3.5\% (20.434 vs 21.173) while improving LAMBADA accuracy by 0.14 percentage points (49.49\% vs 49.35\%) compared to GlobalGC. These results establish a direct correlation between training stability and final model quality.

To validate architectural generality, we extend evaluation to vision-language pretraining with CLIP ViT-Base. As shown in Figure~\ref{fig:openclip_vit_b_32_a}, AdaGC completely eliminates loss spikes observed in vision-specific stabilization methods while achieving 25\% faster convergence than StableAdamW (reaching target loss in 15k vs 20k steps). The improved training dynamics translate to superior zero-shot recognition performance (Figure~\ref{fig:openclip_vit_b_32_b}), with AdaGC demonstrating consistent accuracy gains across modalities - including a 0.27 percentage point improvement on ImageNet-1K (39.84\% vs 39.57\%) over GlobalGC.


\begin{figure}[!t]
    \vskip 0.1in
    \centering
    \begin{subfigure}{0.495\columnwidth}
        \includegraphics[width=1\columnwidth]{figs/vit_b_32_loss_new.pdf}
        \caption{}
        \label{fig:openclip_vit_b_32_a}
    \end{subfigure} \hfill
    \begin{subfigure}{0.495\columnwidth}
        \includegraphics[width=1\columnwidth]{figs/vit_b_32_zero_shot_acc_new.pdf}
        \caption{}
        \label{fig:openclip_vit_b_32_b}
    \end{subfigure}
    \caption{CLIP ViT-Base results: (a) Training loss trajectory comparison. AdaGC achieves 25\% faster convergence (15k vs 20k steps) with complete loss spike elimination compared to StableAdamW. (b) Zero-shot recognition performance. Final accuracy improves 0.27pp (39.84\% vs 39.57\%) on ImageNet-1K, demonstrating cross-modal generalization.}
    \label{fig:openclip_vit_b_32}
    \vskip -0.2in
\end{figure}

\subsection{Ablation Study}
\label{sec:ablation}

We conduct systematic ablation studies across three critical dimensions of AdaGC: (1) EMA gradient norm initialization strategies, (2) adaptivity efficacy, and (3) locality granularity. The adaptivity property has been empirically validated through experimental analysis (Section~\ref{sec:motivations}) and comprehensive experimental results across all benchmarks.

\textbf{EMA Initialization Strategy}. 
 
The initialization of EMA gradient norms requires careful design due to large initial gradient fluctuations during early training phases (first 100 steps). We evaluate five initialization variants on GPT-2 345M: The default AdaGC strategy employs GlobalGC during warm-up while tracking minimum per-parameter norms ($\gamma_{t,i} = \min(\|\boldsymbol{g}_{t,i}\|, \gamma_{t-1,i})$). Comparative approaches include: (1) norm initialization without GlobalGC warm-up (directly using $\gamma_{t,i} = \min(\|\boldsymbol{g}_{t,i}\|, \gamma_{t-1,i})$ from step 0), (2) constant initialization ($\gamma_{0,i} \in \{0.5, 1.0\}$), and (3) thresholded initialization ($\gamma_{t,i} = \min(\|\boldsymbol{g}_{t,i}\|, 0.1)$). Figure~\ref{fig:init_method} demonstrates that while all variants eliminate loss spikes, convergence quality varies within 0.36\%. The default strategy achieves optimal final loss (2.9708 vs 2.9725 for next-best), showing that GlobalGC-guided warm-up better preserves parameter update consistency than direct initialization. This establishes the importance of phased initialization for gradient norm adaptation.

\textbf{Locality Granularity}. 
We analyze clipping granularity across three implementation levels. Global-level clipping applies a single threshold for the entire model, while parameter-wise adaptation operates per individual parameter. On GPT-2 345M (Figure~\ref{fig:gpt345m_global_vs_tensorwise_adagc}), global clipping reduces but fails to eliminate spikes (1 event vs 0 for parameter-wise), with 0.25\% (2.9639 vs 2.9712) higher final loss. 

As model scale increases to 7B parameters, TensorParallel distributed training becomes necessary for efficient optimization. We therefore evaluate shard-wise adaptation on Llama-2 7B, where parameters are partitioned across devices and clipping operates per-shard. Figure~\ref{fig:llama7b_tensorwise_vs_shard_tensorwise_adagc} reveals that while shard-wise clipping eliminates spikes, it introduces parameter update inconsistencies across devices, increasing loss variance by 4.3\% (2.3836 vs 2.28) compared to parameter-wise granularity. This degradation stems from independent clipping of parameter shards disrupting holistic parameter updates. 

These results establish parameter-wise granularity as optimal, achieving precise adaptation while maintaining parameter update coherence across distributed systems.




\begin{figure}[!t]
    \vskip 0.1in
    \centering
    \begin{subfigure}{0.495\columnwidth}
        \includegraphics[width=1\columnwidth]{figs/adagc_global_local_ablation_study.pdf}
        \caption{}
        \label{fig:gpt345m_global_vs_tensorwise_adagc}
    \end{subfigure} \hfill
    \begin{subfigure}{0.495\columnwidth}
        \includegraphics[width=1\columnwidth]{figs/adagc_shard_unshard_ablation_study.pdf}
        \caption{}
        \label{fig:llama7b_tensorwise_vs_shard_tensorwise_adagc}
    \end{subfigure}
    \caption{Locality granularity analysis: (a) Global vs parameter-wise clipping on GPT-2 345M model demonstrates spike elimination capability, (b) Distributed shard-wise clipping on 7B model reveals parameter integrity requirements.}
    \label{fig:enter-label}
    \vskip -0.2in
\end{figure}

\begin{figure*}[!t]
    \vskip 0.1in
    \centering
    \begin{subfigure}{0.745\textwidth}
        \includegraphics[width=1\linewidth]{figs/large_lr_ablation_study.pdf}
        \caption{}
        \label{fig:large_lr}
    \end{subfigure} \hfill
    \begin{subfigure}{0.25\textwidth}
        \includegraphics[width=1\linewidth]{figs/adagc_init_ablation_study.pdf}
        \caption{}
        \label{fig:init_method}
    \end{subfigure}
    \vspace{-7mm}
    \caption{(a) Large rate tolerance on GPT-2 345M: AdaGC enables stable training at 10$\times$ learning rate (left), outperforms baselines (mid), and accelerates convergence through rate scaling (right). (b) EMA initialization strategies.}
    \label{}
    \vskip -0.2in
\end{figure*}




\subsection{Exploring AdaGC's Capabilities}
\label{sec:capabilities}

We demonstrate AdaGC's potential advantages over global gradient clipping through three critical dimensions: large learning rate tolerance, batch size scalability, and optimizer compatibility.

\paragraph{Large Learning Rate Tolerance} 
Figure~\ref{fig:large_lr} analyzes training stability under aggressive learning rate regimes using GPT-2 345M. At \(3 \times 10^{-3}\) learning rate (10$\times$ baseline), AdaGC maintains stable training (0 loss spikes) while achieving 2.52\% lower final loss than GlobalGC (left panel). Comparative analysis with stabilization baselines (middle panel) reveals AdaGC's unique capability: 1.52\% lower training loss than GlobalGC+ScaledEmbed (best competitor) without spike mitigation overhead. The scaling law analysis (right panel) suggests a noticeable convergence acceleration with increasing learning rates (from \(3 \times 10^{-4}\) to \(3 \times 10^{-3}\)). This indicates that learning rate scaling may contribute to a faster training process, albeit not with a perfect linear relationship.

\paragraph{Batch Size Scalability}
Figure~\ref{fig:gpt2_large_bs_and_lr_only_loss} validates AdaGC's effectiveness under scaled batch training paradigms. With batch sizes up to 8192 (16$\times$ baseline) and proportional learning rate scaling, AdaGC maintains perfect stability (0 spikes vs 100\% spike rate for GlobalGC) while reducing final perplexity by 1.28\% - 19.55\% across configurations. The Llama-2 Tiny experiments confirm cross-architecture effectiveness, showing 6.19\% lower perplexity than GlobalGC at a batch size of 8192. This demonstrates AdaGC's capability to enable efficient large-batch distributed training without stability compromises. Additional details can be found in Figure~\ref{fig:gpt2_large_bs_and_lr} and Figure~\ref{fig:llama_tiny_large_bs_and_lr} in Appendix~\ref{appendix:llama_tiny_large_bs}.


\begin{figure*}[!ht]
    \vskip 0.1in
    \centering
    \begin{subfigure}{0.246\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figs/gpt2_batch_size_loss_512.pdf}
        \caption{bs = $512$, lr = $3.0 \times 10^{-4}$}
        \label{fig:gpt_bs_512}
    \end{subfigure} \hfill
    \begin{subfigure}{0.246\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figs/gpt2_batch_size_loss_2048.pdf}
        \caption{bs = $2048$, lr = $6.0 \times 10^{-4}$}
        \label{fig:gpt_bs_2048}
    \end{subfigure} \hfill
    \begin{subfigure}{0.246\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figs/gpt2_batch_size_loss_4096.pdf}
        \caption{bs = $4096$, lr = $8.5 \times 10^{-4}$}
        \label{fig:gpt_bs_4096}
    \end{subfigure} \hfill
    \begin{subfigure}{0.246\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figs/gpt2_batch_size_loss_8192.pdf}
        \caption{bs = $8192$, lr = $1.2 \times 10^{-3}$}
        \label{fig:gpt_bs_8192}
    \end{subfigure}
    \vspace{-7mm}
    \caption{AdaGC's large-batch scalability on GPT-2 345M: Maintains 0 loss spikes (vs GlobalGC's 100\% rate) with batch sizes up to 8192 (16$\times$ baseline).}
    \label{fig:gpt2_large_bs_and_lr_only_loss}
    \vskip -0.2in
\end{figure*}


\paragraph{Optimizer Compatibility}
Figure~\ref{fig:openclip_vit_b_32_combine} demonstrates AdaGC's generalization across optimization frameworks. Integrated with Lion, AdaGC achieves comparable training loss to GlobalGC (1.9912 vs 2.0165) while improving ImageNet-1K top-1 accuracy by 0.16 percentage points (40.81\% vs 40.65\%). When combined with StableAdamW (which explicitly avoids gradient clipping), AdaGC provides additional 19.23\% training loss reduction (2.0523 vs 2.5412) and 0.96 percentage point accuracy gain (39.71\% vs 38.75\%), establishing complementary benefits. These results position AdaGC as a universal stabilization component for modern optimization paradigms.

\begin{figure}[!t]
    \centering
    \captionsetup{skip=2pt} %
    \begin{subfigure}{0.495\columnwidth}
        \includegraphics[width=1\columnwidth]{figs/vit_b_32_loss_mix_new.pdf}
        \vspace{-5mm} %
        \label{}
    \end{subfigure} \hfill
    \begin{subfigure}{0.495\columnwidth}
        \includegraphics[width=1\columnwidth]{figs/vit_b_32_zero_shot_acc_mix_new.pdf}
        \vspace{-5mm} %
        \label{}
    \end{subfigure}
    \caption{AdaGC's optimizer compatibility on CLIP ViT-Base: With Lion, achieves 0.16pp higher accuracy (40.81\% vs 40.65\%) at comparable loss; Combined with StableAdamW, reduces loss by 19.23\% (2.0523 vs 2.5412) with 0.96pp accuracy gain. Demonstrates universal stabilization across optimization frameworks.}
    \label{fig:openclip_vit_b_32_combine}
    \vspace{-2mm}
\end{figure}
