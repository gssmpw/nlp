\section{Motivations}
\label{sec:motivations}
\begin{figure}[!t]
    \vskip 0.1in
    \centering
    \begin{subfigure}{\columnwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figs/motivation_adaptivity_gpt2.pdf}
        \vspace{-6mm}
        \caption{Temporal threshold adaptation necessity: Fixed thresholds (0.4) prevent early spikes but fail later (10k steps), requiring dynamic adjustment.}
        \label{fig:motivation_adaptivity}
    \end{subfigure}
    \begin{subfigure}{\columnwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figs/abnormal_tensor_norm.pdf}
        \vspace{-6mm}
        \caption{Spikes in gradient norms occur at different training steps across parameters, indicating heterogeneous gradient behaviors.}
        \label{fig:abnormal_tensor_norm}
    \end{subfigure}
    \begin{subfigure}{\columnwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figs/motivation_locality_gpt2.pdf}
        \vspace{-6mm}
        \caption{Per-parameter gradient clipping mitigates loss spikes that global clipping cannot, leading to faster convergence.}
        \label{fig:motivation_locality}
    \end{subfigure}
    \caption{Empirical motivation for AdaGC: (a) Temporal threshold decay necessitates adaptive clipping, (b) Parameter-specific gradient spikes demand localized control, (c) Fine-grained clipping outperforms global approaches.}
    \label{fig:motivation}
    \vskip -0.2in
\end{figure}

In this section, we will introduce some phenomena of gradient clipping that motivate the design of the algorithm. 


{\bf Adaptivity}: The magnitudes of gradients in neural network training exhibit dynamic variations throughout the learning process. This renders fixed gradient clipping thresholds suboptimal for mitigating loss spikes, as demonstrated in Figure~\ref{fig:motivation_adaptivity}. While a threshold of 0.4 successfully mitigates loss spike during the initial training phase of the GPT-2 345M model, it becomes ineffective against the severe loss spike occurring around 10,000 iterations, where a substantially reduced threshold is required for stabilization. This phenomenon correlates with the observation that optimal clipping thresholds for convergence speed are strongly influenced by recent gradient norms (see corresponding subfigures). Motivated by these findings, we propose an adaptive clipping mechanism using the exponential moving average of gradient magnitudes as the per-step clipping threshold.
 
{\bf Locality:} Figure~\ref{fig:abnormal_tensor_norm} reveals that different parameters exhibit divergent gradient behaviors, with spikes occurring at distinct training phases. Using a global clipping threshold can degrade the convergence speed of certain parameters. Therefore, we propose replacing global gradient norm clipping with per-parameter norm clipping, where clipping is applied independently to each parameter. Furthermore, as illustrated in Figure~\ref{fig:motivation_locality}, we evaluate the effectiveness of per-parameter clipping thresholds categorized by specific ratios on GPT2-345M. The results indicate that, under the same global gradient norm conditions, per-parameter clipping can address spike phenomena that global clipping cannot resolve, and achieve faster convergence speeds.

These observations collectively motivate our two core design principles: \textit{adaptive thresholding} to address temporal gradient norm decay, and \textit{localized control} to handle parameter-wise gradient heterogeneity. The dynamic nature of optimal clipping thresholds (Figure~\ref{fig:motivation_adaptivity}) necessitates our exponential moving average mechanism for temporal adaptation, while asynchronous parameter gradient spikes (Figure~\ref{fig:abnormal_tensor_norm}) justify per-parameter clipping granularity. Together, these principles form the foundation of AdaGC - adaptively adjusting localized thresholds that respect both the temporal evolution and spatial distribution of gradients.






