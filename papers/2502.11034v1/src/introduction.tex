\section{Introduction}
Large Language Models (LLMs)~\cite{brown2020language, chowdhery2023palm, touvron2023llama} have become a cornerstone of modern Natural Language Processing (NLP) research, demonstrating outstanding performance in a wide range of language tasks. These models, characterized by their extensive parameter counts and vast training datasets, are designed to capture the subtleties of human language. However, as these models grow in size and complexity, they often encounter a significant training obstacle: the loss spikeâ€”a sharp, unexpected increase in training loss that can seriously undermine the training process's efficiency and effectiveness~\cite{chowdhery2023palm, scao2022language, takase2023spike, zeng2022glm}.



Among the various methods for addressing spike issues, gradient clipping is an effective solution. Existing gradient clipping approaches \cite{pascanu2013difficulty} typically apply a predefined threshold to the gradient norm of the entire neural network. However, we have identified two significant shortcomings with this global threshold method. First, as the gradient magnitudes decrease during training, an excessively small threshold throughout the process can slow down the training speed, while a consistently large threshold fails to effectively mitigate spike issues in the later stages of training. Second, due to the inherent differences between structures in neural networks~\cite{zhang2024transformers}, particularly in transformers, applying a uniform clipping coefficient across all layers does not adequately resolve the problems caused by spikes, as illustrated in the Figure \ref{fig:motivation}. To address these two issues with global gradient clipping, we propose using local gradient clipping along with an adaptive update of the clipping threshold for each parameter.


Specifically, we introduce a novel method called Adaptive Gradient Clipping based on Local Gradient Norm (AdaGC). AdaGC is specifically designed to mitigate loss spike by selectively suppressing abnormal gradients, thereby maintaining training stability and enhancing model performance. At its core, AdaGC employs the Exponential Moving Average (EMA) to accurately track historical gradient norm variations at the tensor level. This enables the precise identification and adaptive clipping of anomalous local gradients, effectively containing the loss spike.




Convergence analysis shows AdaGC maintains the $\mathcal{O}(1/\sqrt{T})$ rate comparable to Adam in non-convex settings, confirming that localized clipping preserves convergence properties. Comprehensive experiments on Llama-2 7B/13B~\cite{touvron2023llama} and CLIP ViT-Base~\cite{radford2021learning} demonstrate complete loss spike elimination while improving model convergence (3.5\% perplexity reduction, 25\% faster convergence than StableAdamW~\cite{wortsman2023stable}), with successful integration into modern optimizers like AdamW~\cite{loshchilov2017decoupled} and Lion~\cite{chen2024symbolic}.

In summary, our main contributions are three-fold:
\begin{itemize}

    \item Identification of two critical limitations in global gradient clipping methods: inadequate handling of decaying gradient norms and parameter-wise heterogeneity, motivating the need for localized adaptive clipping.
    \item Proposal of Adaptive Gradient Clipping (AdaGC), featuring parameter-wise threshold adaptation through exponential moving averages, effectively addressing both temporal gradient decay and spatial parameter variations.
    \item Comprehensive empirical validation across architectures (Llama-2 7B/13B) and modalities (CLIP ViT-Base), demonstrating complete loss spike elimination with measurable performance gains, plus convergence analysis confirming comparable rates to standard optimizers.
\end{itemize}
