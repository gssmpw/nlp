\section{Related Work}
Loss spikes present a significant challenge when scaling models, potentially impeding learning or destabilizing the training process. Various strategies have been proposed to enhance the stability of pre-training in large language models.

\textbf{Training Strategies:} Effective training strategies are pivotal for achieving stability. Adopting the bfloat16 format~\cite{wang2019bfloat16} is a cornerstone for stable training. Using smaller learning rates~\cite{wortsman2023small, zhang2022opt} also promotes stable training dynamics. To mitigate loss spikes, PaLM~\cite{chowdhery2023palm} resumes training from a checkpoint approximately 100 steps before the spike and bypasses 200-500 data batches to avoid those preceding and during the spike. Varying the sequence length~\cite{li2022stability} is also effective for stabilizing the pre-training of large language models.

\textbf{Model Architecture:} Advanced architectures are crucial for stability in training large-scale models. Research shows that Pre-LN Transformers offer enhanced stability over Post-LN Transformers~\cite{xiong2020layer, vaswani2017attention, liu2020understanding}, both theoretically and practically. Thus, contemporary studies predominantly employ Pre-LN Transformers for building large-scale models. Techniques such as NormHead~\cite{yang2023baichuan}, which normalizes output embeddings, and RMSNorm~\cite{zhang2019root}, as reported in Llama~\cite{touvron2023llama1}, contribute to stable training. Applying layer normalization to the embedding layer~\cite{scao2022language}, normalizing the input vector before the standard softmax function (NormSoftmax)~\cite{jiang2023normsoftmax}, and the shrink embedding gradient technique~\cite{zeng2022glm} are also beneficial.

\textbf{Model Initialization:} Proper model initialization significantly enhances training stability. Scaling the embedding layer with an appropriate constant~\cite{takase2023spike} reinforces LLM pre-training stability. Initializing Transformers with small parameter values~\cite{nguyen2019transformers} also improves pre-training stability. Some researchers propose that removing layer normalizations in Transformers is feasible with specific initialization methods~\cite{zhang2019fixup, huang2020improving}.

\textbf{Auxiliary Loss Function:} Incorporating an auxiliary loss function alongside conventional language modeling loss enhances training stability. Notable examples include Max-z loss, which benefits the pre-training process~\cite{chowdhery2023palm, wortsman2023small, yang2023baichuan}.

\textbf{Gradient/Update Clipping:} Gradient and update clipping achieve stability by limiting the magnitude of parameter updates, preventing excessively large weight updates. Global gradient clipping~\cite{pascanu2013difficulty} is prevalent, with innovative approaches like adaptive gradient clipping~\cite{brock2021high} and Clippy~\cite{tang2023improving}, which use model weights to adjust the clipping threshold.  Alternatives like Adafactor~\cite{shazeer2018adafactor}, StableAdamW~\cite{wortsman2023stable}, and LAMB~\cite{you2019large} offer update clipping techniques better suited for stability training of large-scale models. Nonetheless, a significant number of loss spikes still occur during the training of large language models, even with the application of these methodologies.
