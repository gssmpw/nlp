\begin{tabular}{ c | c c c c}
    \toprule
    Model                & LLaMA-tiny      & LLaMA-7B     & LLaMA-13B   & GPT-2 \\
    \specialrule{0em}{2pt}{1pt}
    \midrule
    \specialrule{0em}{2pt}{1pt}
    Precision                     & bfloat16                 & bfloat16              & bfloat16             & bfloat16  \\
    \specialrule{0em}{0.5pt}{1pt}
    Layer num                     & 24                       & 32                    & 40                   & 24       \\
    \specialrule{0em}{0.5pt}{1pt}
    Hidden dim size               & 1024                     & 4096                  & 5120                 & 1024     \\
    \specialrule{0em}{0.5pt}{1pt}
    FFN dim size                  & 2371                     & 11008                 & 13824                & 4096     \\
    \specialrule{0em}{0.5pt}{1pt}
    Attention heads               & 16                       & 32                    & 40                   & 16        \\
    \specialrule{0em}{0.5pt}{1pt}
    Sequence length               & 2048                     & 2048                  & 2048                 & 2048      \\
    \specialrule{0em}{0.5pt}{1pt}
    Batch size                    & 512                      & 2048                  & 2048                 & 512       \\
    \specialrule{0em}{0.5pt}{1pt}
    iterations                    & 36000                    & 9000                  & 9000                 & 30000     \\
    \specialrule{0em}{0.5pt}{1pt}
    Learning rate                 & $3.0 \times 10^{-4}$     & $3.0 \times 10^{-4}$  & $3.0 \times 10^{-4}$ & $3.0 \times 10^{-4}$ \\
    \specialrule{0em}{0.5pt}{1pt}
    Lr decay style                & cosine                   & cosine                & cosine               & cosine      \\
    \specialrule{0em}{0.5pt}{1pt}
    Warmup iterations             & 2000                     & 2000                  & 2000                 & 300          \\
    \specialrule{0em}{0.5pt}{1pt}
    Weight decay                  & 0.1                      & 0.1                   & 0.1                  & 0.1          \\
    \specialrule{0em}{0.5pt}{1pt}
    $\lambda_{abs}$               & 1.0                      & 1.0                   & 1.0                  & 1.0           \\
    \specialrule{0em}{0.5pt}{1pt}
    $\lambda_{rel}$               & 1.05                     & 1.05                  & 1.05                 & 1.05           \\
    \specialrule{0em}{0.5pt}{1pt}
    $\beta$                       & 0.98                     & 0.98                  & 0.98                 & 0.98            \\
    \specialrule{0em}{2pt}{1pt}
    \bottomrule
\end{tabular} 
