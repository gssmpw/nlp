\section{Related Work}
State Space Models____ have been recently introduced as an alternative recurrent architecture aimed at rivaling the well established transformer backbone____ widely used across a range of domains, from language modeling to vision. The model was originally introduced as a discretization of a time-continuous linear dynamical system____. Recent works tried to re-frame the architecture from a linear recurrent perspective. For example____ tried to understand what components of the architecture are necessary, highlighting the importance of linearity, as well as of the particular parametrization that allows control over the stability of the model.

However, there are still many gaps in understanding this family of models____, such as questions around expressivity____. This is particularly important given the proliferation of Mamba-inspired architectures that has emerged since the introduction of the model. 
%Since its introduction, several architectures have emerged to enhance the Mamba model's capabilities. 
For example, Mamba-Spike ____ integrates a spiking front-end with the Mamba backbone, leveraging event-driven processing for efficient temporal data handling. The Mixture of Mamba approach combines Mamba's selective state spaces with the Transformer's Mixture of Experts (MoE) layer ____, potentially offering faster convergence and improved performance scaling. Vision Mamba (Vim) ____ adapts the architecture for visual tasks, employing bidirectional Mamba blocks to process high-resolution images efficiently. Besides specific adaptation of the model into new architectures, the core Mamba block also evolved, for example, by the introduction of gating in Mamba-2____ and similar SSM architectures____ which for example invalidates to a certain extent the convolutional view of the architecture. 

Our work squarely focuses on understanding the representation power of Mamba, and ICL, which remains an unanswered question. We believe such insights can help navigate the many flavors of SSM architectures. In this space, recent studies have shown that SSMs can perform gradient-based learning for in-context adaptation, similar to transformers ____, where they show that a single SSM layer, augmented with local self-attention, is able to reproduce outputs of an implicit linear model after one step of gradient descent. While some work suggests that Mamba's ICL abilities rival state-of-the-art transformer models ____, other research indicates that the pretrained Mamba models perform worse in in-context learning than comparable transformer models ____. SSMs have also demonstrated promising results in in-context reinforcement learning tasks, offering fast and performant solutions ____. ____ has introduced a novel weight construction for SSMs, enabling them to predict the next state of any dynamical system in-context. These findings highlight the potential of SSMs as efficient alternatives to transformers for ICL tasks, particularly those involving long input sequences.