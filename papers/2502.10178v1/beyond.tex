% !TEX root = main.tex

\section{Beyond Markov}
\label{sec:beyond}
% Here we focus on Mamba with non-Markovian data.

\subsection{Switching Markov model}
\label{sec:switch}
A key component of \ref{eq:mamba_block} enabling selectivity is the state-transition factor $a_t$, that controls the flow of information from the past state $H_{t-1}$ to the current $H_t$: if $a_t=1$, the past information is fully utilized in computing the current state, and hence the output, whereas $a_t=0$ completely ignores the past. In the Markovian setting considered so far, the role of $a_t$ has largely been dormant: $a_t \approx 1$ for all $t\geq 1$, as the optimal Laplacian predictor requires counts of all transitions, demanding the use of full past (\prettyref{sec:main_thm}). To better highlight this selectivity mechanism, we consider a non-Markovian process, where the role of $a_t$ becomes fundamental. Specifically, we focus on the \emph{switching Markov process}, where we add a \emph{switch} token to the binary alphabet, \ie we consider $\calX = \{0,1,S\}$. The key difference here compared to the random Markov generation in \prettyref{sec:markov_background} is that until we hit switch token, the sequence follows the same binary Markov chain, but once the switch state is reached, the next symbols follow a newly sampled Markov chain, independent of the previous one. The switch tokens are sampled according to a parallel \iid Bernoulli process with probability $p_{\rm switch}$ ($0.01$ in our experiments). More formally, the process consists of the following steps:
\begin{enumerate}
    \item Initialize $t=1$.
    \vspace{-0.5em}
    \item Draw a binary Markov kernel $P$ with each conditional distribution $P_{i_1^k}$ sampled \iid from $\dir{\beta}$.
    \vspace{-0.5em}
    \item Let $x_t = S$ with probability $p_{\rm switch}$, or sample $x_{t} \sim P_{x_{t-k+1}^t}$ with probability $1 - p_{\rm switch}$ (the first $k$ samples after each switch token are sampled from $\unif{\binary}$).
    \vspace{-0.5em}
    \item If $x_t = S$, set $t = t+1$ and go to step 2; if $x_t \neq S$, set $t = t+1$ and go to step 3.
\end{enumerate}

\begin{figure*}[t]
\captionsetup[sub]{}
\centering
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/switch-estimator.pdf} 
    \put(-125,-7){\fontsize{9}{3}\selectfont $t: x_t = 0$}
      \put(-242,86.5){\rotatebox[origin=t]{90}{\fontsize{8}{3}\selectfont Predicted probability $\mathbb{P}_{\btheta}\pth{{x_{t+1}=1 \mid x_1^t}}$}}
      \put(-76,157){\fontsize{7.5}{3}\selectfont $1$-layer Mamba}
      \put(-76,146){\fontsize{7.5}{3}\selectfont Optimal estimator}
\caption{Predicted probabilities}
\label{fig:switch-estimator}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/switch-at.pdf} 
    \put(-132,-7){\fontsize{9}{3}\selectfont Position $t$}
      \put(-238,88){\rotatebox[origin=t]{90}{\fontsize{10}{3}\selectfont $a_t$}}
\caption{Value of $a_t$ across positions}
\label{fig:switch-at}
\end{subfigure}
\caption{Single-layer Mamba on data generated from the switching Markov process with $p_{\rm switch} = 0.01$. The red vertical lines mark the positions of switch tokens. Figure (a) shows that the model's prediction follows very precisely that of the optimal estimator also in this more complex scenario. Figure (b) highlights the selectivity process of the model: every time a switch token appears, the model erases all information about the past by setting $a_t=0$.}
\label{fig:mamba-switch}
\end{figure*}

\noindent {\bf Mamba learns the optimal estimator.} With this data model, the optimal prediction strategy is to use the add-$\beta$ estimator in between two switch tokens, and reset the transition counts every time a switch occurs. Indeed, \prettyref{fig:mamba-switch} illustrates that Mamba implements precisely this strategy, closely tracking the switching events via the transition factor $a_t$: it sets $a_t$ to be zero whenever $x_t = S$ and to one otherwise. This enables the model to zero out the transition counts at every switch event, so that it can estimate the statistics of the new chain from scratch.


\subsection{Natural language modeling}
\label{sec:english}
To test the generality of our finding that convolution plays key role on Markovian data (\prettyref{fig:conv}), we conduct experiments on the language modeling task using the WikiText-103 dataset with a sequence length of $256$. We use a standard 2-layer Transformer consisting of attention and MLP modules with a model dimension of $256$. To ensure a comparable parameter count, we stack the Mamba-$2$ cell across four layers, following the design in \cite{dao2024transformers}. By adding or removing convolution in both these models, we obtain the results shown in \autoref{tab:lm_result}. The results illustrate that while convolution significantly enhances the performance of Mamba-2, reducing perplexity from $30.68$ to $27.55$, the improvement for the Transformer is marginal. This highlights the fundamental role of convolution even on the complex language modeling tasks. 
% These results on language modeling further demonstrate the effectiveness of convolution in Mamba models.

\begin{table}[h]
\centering
\caption{Perplexity results on the WikiText-$103$ dataset for both models. (w/o conv) denotes the absence of convolution, while (w/ conv) indicates its use.}
\begin{tabular}{lll}
\toprule
\bf Model & \bf \# Params. (M) &\bf Perplexity ($\downarrow$)\\
\midrule
Mamba-$2$ (w/o conv)& $ 14.53 $ & $ 30.68 $ \\ 
Mamba-$2$ (w/conv)& $14.54$ &  $\bf{27.55}$ \\
\midrule
Transformer (w/o conv) & $14.46$ & $29.28$ \\
Transformer (w/ conv) & $14.46$ &  $\bf{28.67}$  \\
\bottomrule
\end{tabular}
\label{tab:lm_result}
\end{table}

