% !TEX root = main.tex

\section{Problem setup}
\label{sec:background}
We formally define the problem setting and provide necessary background. We use the following notation: scalars are denoted by such italic lower case letters as $x,y$, Euclidean vectors by bold $\bx, \by$, and matrices by upper case $X, Y$, etc. $\| \cdot \|_p $ denotes the $\ell_p$-norm for $p \in [1, \infty]$. $\bm{1}$ refers to the all-one vector. For $T \in \naturals$, $[T] \define \{1, \ldots, T \}$, and for a sequence $(x_t)_{t \geq 1}$, define $x_k^t \triangleq (x_k, \ldots, x_t)$ if $k \geq 1$ and $(x_1, \ldots, x_t)$ otherwise. For $z \in \reals$,  $\sigmoid(z) \triangleq 1/(1+e^{-z}), \relu(z) \triangleq \max(0,z)$, and $\softplus(z) \define \log(1+e^z)$. $\mathrm{Unif} (S)$ denotes the uniform distribution over a set $S$ and $\mathrm{Dir}(\bbeta)$ denotes the Dirichlet distribution with parameter $\bbeta >0  $. $\KL{P}{Q}$ denotes the KL-divergence between distributions $P$ and $Q$. \looseness=-1
\subsection{Input data: Random Markov chains}
\label{sec:markov_background}
To investigate the ICL capabilities of Mamba, we let the input tokens to be stochastic and drawn from a random Markov chain of order $k$ \cite{edelman2024evolution}. That is, the token sequence $x = (x_t)_{t=1}^T \in \calX^T$ on the state space (vocabulary) $\calX$ follows the transition dynamics: 
\begin{align}
\prob{x_{t+1} = \cdot \mid x_1^{t}} = \prob{x_{t+1} = \cdot \mid x_{t-k+1}^{t}}, 
\end{align}
almost surely for all $t \in [T]$, and the \kth Markov kernels, $ \prob{x_{t+1} = \cdot \mid x_{t-k+1}^{t}=i_{t-k+1}^{t}}$, are sampled independently for each tuple $(i_{t-k+1},\cdots,i_t)$ from the Dirichlet prior $\dir{\beta}$, with $\beta >0$. When $\beta=1$, this corresponds to the uniform distribution on the $S$-dimensional simplex $\Delta_1^S$, where size $S=|\calX|$. 

% (\bP \pth{\cdot \mid i_1^k} )
The transition matrix $P =(P_{i_1^k})_{i_1^k \in \calX^k}, P_{i_1^k} \in [0,1]^S$, encapsulates the set of all $S^k$ conditional probabilities of the chain, each row corresponding to one of them. While this transition matrix governs the generation of each token $x_t$ for $t >k$, the first $k$-tokens $x_1,\ldots,x_k$ are drawn \iid from $\unif{\calX}$. This constitutes the joint law of the random variables $(P,x)$, termed random Markov distribution henceforth. More succinctly, 


\noindent {\bf Data generation} (Random Markov sequences).
\vspace{-0.5em}
    \begin{enumerate}
        \item Draw $P$ with each row sampled \iid from $\dir{\beta}$. \vspace{-0.5em}
        \item For $t=1,\ldots,k$, sample $x_t \sim \unif{\vocab}$.
        \vspace{-0.5em}
        \item For $t=k, \ldots, T$, sample $x_{t+1} \sim P_{x_{t-k+1}^t} $.
        \vspace{-0.5em}
        \item Return the input $x=(x_t)_{t=1}^T$.
              \vspace{-0.5em}
        \item Repeat the above steps to generate a batch $\{x^{(b)}\}_{b \in [B]}$.
    \end{enumerate}

\noindent {\bf Relation to ICL.} As a consequence of the generation process, no two sequences $x$ share the same transition matrix $P$ and hence every sequence follows a different Markov distribution. Owing to this fact, even when a model is trained on this random Markovian data, at inference, for every test sequence it has to estimate the next token in-context. Hence this data class serves as a good sandbox to gauge the ICL capabilities of Mamba, which was also used in a similar context for transformers \cite{nic2024trans}. In this paper, we let the state space $\vocab = \binary$ for the ease of exposition and the order $k$ to be any $k \geq 1$. Our results and insights can be readily extended to any finite vocabulary, as demonstrated for the natural language in \prettyref{sec:english}.


% Since a \kth Markov chain on $\vocab$ is equivalent to a $(k \cdot \log |\vocab|)^{\text{th}}$-order process on the binary state space $\binary$, in this paper, without loss of generality, we study binary Markov chains of order $k \geq 1$. \nb{Michael}.

% \begin{figure}[t]
%     \centering
%     \input{figures/data_gen}
%     \caption{Generation of random Markov chains.}
%     \label{fig:markov_gen}
% \end{figure}

\subsection{Mamba architecture}
\label{sec:mamba_background}
Selective SSMs such as Mamba and Mamba-2 are a class of sequence-to-sequence models that are closely related to RNNs and classical state space models \cite{mamba2023gu}. A key feature underpinning these models is the \emph{selectivity mechanism}, enabling them to selectively choose inputs at every timestep, as opposed to linear time-invariant (LTI) systems. While we believe our work captures the behavior of all selective SSMs, we will specifically focus on the state-of-the-art Mamba-2 model to simplify exposition. By slight abuse of terminology, henceforth we will also refer to this model simply as Mamba. Mathematically speaking, Mamba implements the sequence-to-sequence mapping $\mamba: \reals^{d \times T} \mapsto \reals^{d \times T} $, where given a sequence of input embeddings $\bx = (\bx_t)_{t=1}^T \inr{d\times T}$ of dimension $d$, it outputs the corresponding output embeddings $\bo = (\bo_t)_{t=1}^T \inr{d\times T}$ of the same dimension with $\bo = \mamba(\bx) $. More precisely, fix $t \in \set T$. Then the output $\bo_t$ at time $t$ is computed as $ \bo_t = \mamba(\bx_1^t)$ using the following recurrence equations \cite{dao2024transformers}:
\begin{equation}
\begin{split}
H_t &= a_t \, H_{t-1} + \tilde{\bx}_t \, \bb_t^\top \inr{ed \times N}, \\
\by_t &= H_t\, \bc_t \inr{ed}, \\
\bz_t &= \by_t \odot \relu(W_z \, \bx_t) \inr{ed}, \\
\bo_t &= W_o\, \bz_t \inr{d},
\end{split}
\label{eq:mamba_block}
\tag{Mamba}     
\end{equation}
where the initial state $H_0 = 0$, $W_z \inr{ed \times d}$ and $W_o \inr{d\times ed}$ are learnable parameters, and the input-selective
\begin{equation}
\begin{split}
a_t &\define \exp(-a \cdot \Delta_t) \in (0,1), \, \text{with} \\
\Delta_t &\define \softplus(\inner{\bw_{\Delta}}{\bx_t}+ \delta) \inr{}, \\
\tilde{\bx}_t  &\define \relu(\conv_X(W_X \, \bx_{t-w+1}^t)) \cdot \Delta_t \inr{ed}, \\
\bb_t &\define \relu(\conv_B(W_B \, \bx_{t-w+1}^t)) \inr{N}, \\
\bc_t &\define \relu(\conv_C(W_C \, \bx_{t-w+1}^t)) \inr{N},
\end{split}
\tag{Input selectivity} 
\label{eq:mamba-params}
\end{equation}
where $a \geq  0, \bw_{\Delta}\inr{d}, \delta\inr{}, W_X\inr{ed \times d}, W_B\inr{N \times d}$ and $W_C\inr{N\times d}$ are all learnable parameters and $\conv(\bz_{t-w+1}^t)$ is a (learnable) time-wise convolution of 
window $w \in \naturals$ with distinct kernels per dimension. Here $e\in\mathbb{N}$ denotes the expansion factor for the features, typically $2$. Let $\thetamamba$ denote the set of all these parameters. \looseness=-1

\noindent {\bf Intuition behind Mamba.} While the update equations in \ref{eq:mamba_block} might seem complicated at a first glance, the underlying intuition is simple: given a sequence of input embeddings $(\bx_t)$, we first capture their local temporal information using three separate convolutions to compute $\tilde{\bx}_t, \bb_t$, and $\bc_t$ respectively (\ref{eq:mamba-params}). Equipped with this local memory, we perform the classical linear state update to compute the current state $H_t$ from the past $H_{t-1}$, weighed by an input-dependent decay factor $a_t \in (0,1)$, and $(\tilde{\bx}_t, \bb_t)$. Subsequently, we compute the state projection $\by_t$, modulate it with an input-selective ReLU term to yield $\bz_t$, and finally project it down to get the $d$-dimensional output embedding $\bo_t$. Thus the output $\bo_t$ at time $t$ is a function of the entire input sequence till then, $\bx_1^t$, yielding $ \bo_t = \mamba(\bx_1^t)$. \looseness=-1

\begin{figure}[t]
\centering
   \scalebox{0.8}{\input{figures/mamba}} 
    \caption{Mamba-based language model with binary input data: for each $t\in [T]$, the next-token prediction probability is $\predprob= \mathbb{P}_{\btheta}\pth{x_{t+1}=\cdot \mid x_1^t}$.}
    \label{fig:mamba}
\end{figure}

\noindent  {\bf Mamba-based language model.}  \ref{eq:mamba_block} block is then incorporated into a full-fledged language model as follows: let $ x= (x_1, x_2, \cdots, x_T) \in \vocab^T $ be an input token-sequence over the alphabet $\calX$; here $\calX= \binary$ as explained in \prettyref{sec:markov_background}. Then, at every $t\in [T]$, the output of the language model $\btheta$ is given by the following sequence of equations \cite{dao2024transformers}:
\begin{align}
\bx_t  &= \be
_{x_t} = x_t \, \be_1 + (1-x_t) \, \be_0 \inrd, \tag{{Embedding}}     \label{eq:embedding}  \\
\bu_t &= \bx_t + \mamba(\bx_1^t) \inrd, \tag{{Mamba}}\label{eq:mamba} \\
\bv_t &= \bu_t + W_2[ \relu(W_1 \bu_t) + \odot W_3 \bu_t ] \inrd, \tag{{\small MLP}} \label{eq:mlp} \\
\logit_t &= W_{\ell}\, \bv_t \inr{2}, \tag{{Linear}} \label{eq:logit} \\
f_{\btheta}(x_1^t) & \define \mathbb{P}_{\btheta}\pth{x_{t+1}= \cdot \mid x_1^t} = \softmax(\logit_t) \in [0,1]^2, \tag{{Prediction}} \label{eq:prediction}
\end{align}
where the parameters $\be_0, \be_1 \inrd, W_1 \inr{4d\times d}, W_2 \inr{d\times 4d}$ and $W_{\ell} \inr{2\times d}$ are learnable, and $\predprob$ is the probability law for the next symbol $x_{t+1}$ conditioned on the past $x_1^t$. We omit the layer norm here for simplicity. We compactly denote the set of all model parameters as $\btheta$, \ie $\btheta= (\be_0, \be_1, \thetamamba, W_{1,2,3},  W_\ell) \inr{D} $. \looseness=-1


\subsection{Learning task: next-token prediction}
With the objective of auto-regressively estimating the next token,  we train the model parameters $\btheta$ to minimize the cross-entropy loss between the next-token predicted probability $\predprob$ and the corresponding ground-truth symbol $x_{t+1}$ across all the positions $t \in \set T$:
\begin{equation}
L(\btheta) \define -\frac{1}{T} \sum_{t \in \set T} \Expect_P \Expect_{x_1^{t+1} \sim P}\big[x_{t+1} \cdot \log f_{\btheta}^{(1)} (x_1^t) + (1- x_{t+1}) \cdot \log f_{\btheta}^{(0)}(x_1^t) \big],
\label{eq:loss}
\end{equation}
where $f_{\btheta}^{(j)} (x_1^t) \define \mathbb{P}_{\btheta}\pth{x_{t+1}=j \mid x_1^t}$ for $j\in\binary$, and the expectation is both over the transition kernels $P$ and the Markov sequences $x= (x_t)_{t=1}^T$ sampled from $P$. In practice, it is replaced
by empirical average across a finite set of batches, sampled according to the random Markov distribution in \prettyref{sec:markov_background}. For our experiments we use the AdamW optimizer \cite{Kingma2017}. 
% At inference, the trained model is evaluated on a fresh Markov sequence of length $T$, fixed beforehand.

\subsection{Optimal estimator: Laplacian smoothing}
\label{sec:laplace}
Given the Bayesian prediction loss in \prettyref{eq:loss}, it is natural to ask: \emph{what's the optimal $\btheta$ minimizing it?} It follows from a classical result in statistics (\cite{rissanen1984}, \S~\ref{app:laplace}) that this minimum is achieved when the corresponding model prediction matches the (averaged) ground-truth predictive distribution, \ie $\mathbb{P}_{\btheta}\pth{x_{t+1}= 1 \mid x_1^t} = \Expect_{P|x_1^t} \qth{ \prob{x_{t+1} =1 \mid x_1^t}} $, for all $t$. Given the joint distribution of the pair $(P, x_1^{t+1})$ in \prettyref{sec:markov_background}, where the kernel $P \sim \dir{\beta}$, it can be shown (\S~\ref{app:laplace}) that the conditional expectation above simplifies to the well-known \emph{Laplacian smoothing}, also known as the \emph{add-$\beta$ estimator} (see e.g. \cite{merhav1998}):
\begin{align}
\mathbb{P}_\beta^{(k)}\pth{x_{t+1} = 1 \mid x_1^{t}}   \define \Expect_{P|x_1^t} \qth{ \prob{x_{t+1} =1 \mid x_1^t}}  =  \frac{n_1 + \beta}{n + 2 \beta},   
\tag{Laplacian smoothing}
\label{eq:laplace_smooth}
\end{align}
where $n_1$ is the number of times token $1$ follows the current \kth context $x_{t-k+1}^t$ in the sequence $x_1^t$, \ie $n_1 = | \{ i: (x_{i-k}^{i-1}, x_i) = (x_{t-k+1}^t, 1) \}  |  $ and $n$ is the frequency of this context, \ie $ n = | \{ i: x_{i-k}^{i-1} = x_{t-k+1}^t \}  | $. Adjusting these counts by $\beta$ plays the role of additive smoothing, which avoids assigning zero probabilities to unseen events, an idea dating back to Laplace \cite{laplace1814essaiprob}. It is also known that the add-$\beta$ estimator is asymptotically minimax optimal, as $T\to\infty$ \cite{xie-barron-1997, orlitsky2018mc}. If Mamba realizes this smoothing estimator, \ie $\mathbb{P}_{\btheta} = \mathbb{P}_{\beta}^{(k)}$, it automatically implies its ICL abilities: given a fresh test sequence at inference, in order to optimally predict the next token, it processes the input tokens in-context to compute the relevant counts, as in the \ref{eq:laplace_smooth}. \emph{But does it realize in practice this optimal smoothing-based counting estimator?}