% !TEX root = main.tex

\section{Does Mamba learn in-context estimators?}
\label{sec:empirical}

To investigate the ICL capabilities of Mamba, we consider the problem setup described above and train a single-layer Mamba using AdamW on the next-token prediction loss in \prettyref{eq:loss} on random Markov chains. We repeat the same procedure for both single and two layer transformers and report the best performance for all these models (we refer to \S~\ref{app:architecture} for more experimental details). These experiments reveal interesting and rather surprising insights about Mamba:

\begin{enumerate}
    \item Mamba learns the optimal Laplacian smoothing estimator on the Markov prediction task, even with a single layer (\prettyref{fig:estimator-prob}). 
    \item Convolution mechanism plays a fundamental role in Mamba, aiding in its learning abilities (\prettyref{fig:conv-no-conv})
\end{enumerate}

In the sequel, we expand upon these observations in detail. 

\noindent {\bf 1) Mamba learns the Laplacian smoothing.} After training, we evaluate the Mamba and transformer models on the same test sequence fixed beforehand and compare their performance to that of the optimal \ref{eq:laplace_smooth} estimator. Specifically, we compare their next-token prediction probabilities with those of the \adbeta estimator. \prettyref{fig:estimators} illustrates these results, which uncovers a surprising phenomenon: \emph{even a single-layer Mamba sharply matches the optimal estimator on the whole sequence}. In fact, this approximation error is small even for various Markov orders. On the other hand, for transformers we observe that a two-layer model also matches the predictor, albeit less sharply and more variance, whereas a single layer transformers fails to solve the task. This is not fully surprising, given a recent theoretical result \cite{sanford2024onelayer} that while an induction head (realizing the counting estimator) can be efficiently represented by a two-layer transformer, its single-layer variant needs to be exponentially large.

\noindent {\bf 2) Convolution is the key.} To decipher the key architectural component behind Mamba's success in Markov prediction task, we do an ablation study on its three main features: (i) convolution in \ref{eq:mamba-params}, (ii) ReLU non-linearity in \ref{eq:mamba-params}, and (iii) the gating mechanism in \ref{eq:mamba_block} and \ref{eq:mlp}. Amongst them, surprisingly, \emph{convolution} plays a fundamental role in the model's performance, as illustrated in \prettyref{fig:conv-no-conv}. Here we compare the full Mamba architecture from \prettyref{sec:mamba_background}, Mamba with just the convolution in \ref{eq:mamba-params} removed, and a simplified Mamba architecture with only convolution ($\simplemamba$ in \prettyref{sec:mambazero}). As a metric of comparison, we use the closeness of each of these models $\btheta$ to the optimal add-$\beta$ estimator, \ie \ie $|L(\btheta) - L(\beta)|$, where $L(\beta)$ is the optimal loss incurred by the add-$\beta$ estimator. Closer this metric is to zero, the better the model's performance is. Remarkably, the simplified Mamba with just the convolution succeeds on the Markov prediction task, while the full model without convolution fails, highlighting its fundamental importance. This raises a natural question: \emph{how does convolution help Mamba to implement the optimal Laplacian estimator?} 

In the next section, we investigate this theoretically. 

\begin{figure*}
\captionsetup[sub]{}
\centering
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/conv-no-conv.pdf} 
    \put(-136,-7){\fontsize{9}{3}\selectfont Iteration ($\times 200$)}
      \put(-238,86){\rotatebox[origin=t]{90}{\fontsize{9}{3}\selectfont Test loss}}
      \put(-113,153.5){\fontsize{8}{3}\selectfont $\mamba$}
      \put(-113,143){\fontsize{8}{3}\selectfont $\simplemamba$}
      \put(-113,132){\fontsize{8}{3}\selectfont $\mamba$ without convolution}
\caption{Importance of convolution}
\label{fig:conv-no-conv}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/conv-order.pdf} 
      \put(-170,-7){\fontsize{9}{3}\selectfont Convolution window of Mamba}
      \put(-238,85){\rotatebox[origin=t]{90}{\fontsize{9}{3}\selectfont Test loss}}
      \put(-39,154){\fontsize{8}{3}\selectfont Order $1$}
      \put(-39,143){\fontsize{8}{3}\selectfont Order $2$}
      \put(-39,131.5){\fontsize{8}{3}\selectfont Order $3$}
      \put(-39,120){\fontsize{8}{3}\selectfont Order $4$}
\caption{Relation between window size and Markov order}
\label{fig:conv-order}
\end{subfigure}
\caption{(a) illustrates the fundamental role of convolution, without which the model fails to learn the task. In contrast, a simplified variant with just the convolution ($\simplemamba$) matches the performance of that of the full model. (b) highlights the relation between the Markov order $k$ and the window size $w$ of $\mamba$. It is required that $w \geq k+1$ for the model to learn the order-$k$ prediction task.}
\label{fig:conv}
\end{figure*}