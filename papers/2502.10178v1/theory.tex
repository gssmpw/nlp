% !TEX root = main.tex

\section{Theoretical results}
\label{sec:theory}
Motivated by Mamba's success in learning the optimal smoothing estimator, and convolution's pivotal role in it, here we study how it can represent Laplacian smoothing. \looseness=-1

% The aim of this section is to understand how Mamba is able to implement the optimal add-constant estimator. To reach this objective, we start from empirical observations about what structures the model has actually learned during training, and from it, we build our theoretical understanding of the role of each building block of Mamba in realizing the optimal estimator. 
% Following the empirical insights of the previous section, that highlight the fundamental role of the convolution for Mamba to be able to in-context learn the Markov prediction task, we study a simplified architecture where only the essential elements are present: an embedding layer, the Mamba block with convolution, and a final linear layer to output the logits.

\subsection{MambaZero: Simplified model}
\label{sec:mambazero}
Building upon the insight that Mamba with just the convolution achieves the same performance as that of the full model (\prettyref{fig:conv-no-conv}), we consider its simplified version: $\simplemamba$.  $\simplemamba$ retains only the essential elements of the full model in \prettyref{sec:mamba_background}: the \ref{eq:embedding} layer, the convolution inside the Mamba block in \ref{eq:mamba-params}, and the \ref{eq:logit} layer. More formally, it's given by:
\begin{align}
\bx_t &= x_t \, \be_1 + (1-x_t) \, \be_0 \inrd, \tag{Embedding}  \label{eq:embedding-simple}  \\
\bu_t &= \bx_t + \simplemamba(\bx_1^t) \inrd, \tag{MambaZero}  \label{eq:mamba-simple} \\
\logit_t &= W_{\ell} \, {\bu_t} \inr{2}, \tag{Linear} \label{eq:logit-simple} \\
f_{\btheta}(x_1^t) & \define \mathbb{P}_{\btheta}\pth{x_{t+1} = \cdot \mid x_1^t} = \pth{ \logit_t/\|\logit_t\|_1} \in [0,1]^2, \tag{Prediction}    \label{eq:prediction-simple}
\end{align}
where the $\simplemamba$ block is
\begin{equation}
\begin{split}
H_t &= a_t H_{t-1} + \tilde{\bx}_t \, \bb_t^\top \inr{e d \times N}, \\
\by_t &= H_t\, \bc_t  \inr{ed}, \\
\bo_t &= W_o\, \by_t \inr{d},
\end{split}
\tag{MambaZero}
\label{eq:mamba-eqs-simple}
\end{equation}
and the input-selective terms  $a_t, \tilde{\bx}_t, \bb_t$ and $\bc_t$ are computed as in \ref{eq:mamba-params} without ReLU and just the convolution. Here we use the $L_1$ normalization instead of the $\softmax$ in the \ref{eq:prediction-simple} layer ease of theoretical analysis, similar to \cite{nic2024trans,rajaraman2024transformersmarkovdataconstant}. Let $\btheta= (\be_0, \be_1, \btheta_{\simplemamba} ,  W_\ell) \inr{D} $ denote the full set of parameters for appropriate $D \geq 1$. 

\subsection{Main theorem for first-order Markov}
\label{sec:main_thm}

Towards establishing how $\simplemamba$ can represent Laplacian smoothing, we start with the random first-order Markov sequences. Our main result is the following.

\begin{theorem}[$\simplemamba$ represents order-$1$ Laplacian smoothing]
\label{thm:order1}
For the canonical $\simplemamba$ model with dimensions $d = N = 2$, $e=1$, and convolution window $w=2$, there is a choice of parameters such that the model prediction is arbitrarily close to the Laplacian estimator for random first-order Markov chains. More formally, for any $\beta > 0$ and $\epsilon \in (0,1)$, there exists a set of parameters $\bm{\theta}$ such that, for all sequences $(x_t)_{t\geq 1}$ and all $t\geq 1$,
\begin{align*}
\KL{\mathbb{P}_\beta^{(1)}(\cdot \mid x_1^t)}{\thetaprob{\cdot \mid x_1^t}} \leq \epsilon.    
\end{align*}
% Consequently, for random first-order Markov chains,  $\simplemamba$ realizes the optimal \ref{eq:laplace_smooth} estimator arbitrarily close. 
\end{theorem}
{\bf Remark.} The KL divergence above is precisely the penalty paid in the cross-entropy loss at time $t$ (\prettyref{eq:loss}) when using the predictor $\mathbb{P}_{\btheta}$ instead of the optimal $\mathbb{P}_\beta^{(1)}$. In other words, the result implies that the loss of $\simplemamba$ can be made arbitrarily close to that of the optimal.

We defer the full proof to \S~\ref{app:proof1} and outline the sketch below. \looseness=-1

\subsubsection{Proof sketch}
\label{sec:proof_sketch}
{\bf Main idea.} To build our intuition towards how $\simplemamba$ can realize the \adbeta counting estimator for first-order Markov sequences, let's focus on the core \ref{eq:mamba-eqs-simple} block. The key observation here is the following: if the state $H_{t-1}$ can capture all the transition counts $i \to j$ till $x_{1}^{t-1}$, the new state $H_t$ can be updated to account for the current transition $x_{t-1} \to x_t$ on top of the existing counts, by a suitable choice of $a_t, \tilde{\bx}_t$, and $\bb_t$. Then the relevant count information corresponding to the current prefix $x_t$ could be read off from the state projection $\by_t = H_t \bc_t$, and be modified to account for $\beta$-smoothing via the \ref{eq:logit-simple} and \ref{eq:prediction-simple} layers. Buttressing this idea are two key empirical facts, which in fact hold for any $k \geq 1$, underpinning our construction:

{\bf (i) State-to-state transition factor $a_t \approx 1$ for all $t \geq 1$.} We empirically observe that when $\simplemamba$ model is trained on random first-order Markov data, at convergence we have $a_t \approx 1$ for all $t\geq1$ (\prettyref{fig:at}). Since $a_t$ modulates how much past information flows into the present, $a_t=1$ serves as a natural choice when the states $H_t$ carry count information till current tokens, which we would like to update without any diminishing factor. Note that this can be easily achieved by setting either $a$ or $\Delta_t$ to be zero in \ref{eq:mamba-params}, which we empirically observe as well.

% This observation is not surprising: through Eq. (8a), $a_t$ is the parameter that is responsible for retaining information about the past. Implementing the optimal add-constant estimator requires counting all transitions' occurrences from the beginning of the sequence. If $a_t$ were small, old tokens would be forgotten by the model, and so would be their transitions. Due to this fundamental property, we will assume from now on that $a_t = 1$. (Note that this can easily be achieved by setting either $a$ or $\Delta_t$ to be zero.) Experiments show that fixing $a_t = 1$ without learning does not invalidate the ability of the model to learn the optimal predictor. 

{\bf (ii) Convolution window $w \geq k+1$.} Recalling that $k$ is the Markov order, we empirically observe that the window that $w=k+1$ is sufficient for the full Mamba to learn the Laplacian smoothing on \kth Markov chains. To understand why, note that in the $\simplemamba$ architecture above, apart from the \ref{eq:mamba-eqs-simple} block, all remaining equations operate on the current token at time $t$. In the \ref{eq:mamba-eqs-simple} block, same as the \ref{eq:mamba_block} block except ReLU, the dependency of the output $\by_t$ on the previous tokens is due to that of the state $H_t$ on $(\tilde{\bx}_t, \bb_t)$ in the update equation, and of $\bc_t$ in the state projection. Since $(\tilde{\bx}_t, \bb_t, \bc_t)$ depend on the past through the convolutions, a window of size $k+1$ enables them to keep track of the current token as well as its length-$k$ prefix, which is necessary to compute the counts needed in \ref{eq:laplace_smooth}. On the other hand, if $w_X, w_B \leq k$, then one can find \emph{confusable} sequences, \ie sequences that share the same number of occurrences of all length-$k$ prefixes, but whose counts of the tokens following each prefix is different, resulting in the model's estimate to deviate from that of the optimal add-$\beta$. We refer to \S~\ref{sec:warmup} for more details. While having all the window sizes $w_X, w_B, w_c \geq k+1$ is sufficient, it can be further strengthened to $w_c = k$ (\S~\ref{sec:warmup}).

% For such sequences, the state $H_t$ is the same, and so are the predicted probabilities by the Mamba model; however, the optimal estimator, depending on the transition counts, would give very different probability estimates, allowing Mamba's prediction loss to deviate from that of the optimal. For example, consider $k=1$. If $w=1$, then $(\tilde{\bx}_t, \bb_t, \bc_t)$ depend only on the current token $x_t$. Then, consider the two sequences $x = (0,1,0,1,0,1)$ and $\tilde{x} = (0,0,0,1,1,1)$. At time $t=6$, these two sequences would give the same state $H_t$ and the same output $\by_t$, since they share the same number of tokens $0$ and $1$. Therefore, the estimated probability given by the model would be the same in both cases. However, the optimal add-constant estimator (with $\beta=1$) would estimate the probability of $x_{t+1}=1$ to be $1/4$ for $\bx$, and $3/4$ for $\tilde{\bx}$.

We now detail our construction for the order-$1$ case, capitalizing on these insights. 

\noindent {\bf Construction.} Let us fix $w=k+1=2$. Then, $\tilde{\bx}_t$ and $\bb_t$ only depend on the current token $x_t$ and the previous one $x_{t-1}$, while $\bc_t$ only depends on $x_t$. Thus, $\tilde{\bx}_t$ and $\bb_t$ can only take four possible values depending on the last transition in the sequence, whereas $\bc_t$ only two. To ease the notation, we will denote these values by $\tilde{\bx}^{(ij)}$, $\bb^{(ij)}$, and $\bC^{(i)}$ respectively, for $i,j \in\binary$. Additionally, at $t=1$, these terms depend only on the current symbol, taking two additional values each, denoted by $\tilde{\bx}^{(i)}, \bb^{(i)}$. Let $n_{ij}$ denote the number of transitions $i \to j$ in the input sequence $x_1^t$. Then, unfolding the state update recursion in \ref{eq:mamba-simple},
\begin{equation*}
\label{eq:ht1}
H_t = \tilde{\bx}_0 \bb_0^\top + \sum_{ij} n_{ij} \, \tilde{\bx}^{(ij)} \bb^{(ij)\top} .
\end{equation*}
Thus the output of the \ref{eq:mamba-simple} block is
\begin{equation}
\label{eq:out1}
\bo_t = W_o \, \tilde{\bx}_0 \bb_0^\top \bc_t + \sum_{ij}n_{ij}\, W_o \, \tilde{\bx}^{(ij)} \bb^{(ij)\top} \bc_t .
\end{equation}
While the output in \prettyref{eq:out1} depends on all the transition counts, in view of \ref{eq:laplace_smooth}, we ideally want only those counts pertaining to relevant transitions, \ie if $x_t = 0$, the counts $n_{00}$ and $n_{01}$, and vice versa. To this end, we empirically observe that at convergence, the model's parameters are such that
\begin{align*}
\bb^{(11)\top} \bc^{(0)} \approx 0 \, \, \text{and} \, \, \bb^{(00)\top} \bc^{(1)} \approx 0.
\end{align*}
Due to this property, the counts $n_{00}$ and $n_{11}$ do not contribute to the output $\bo_t$, respectively when $x_t = 1$ and $x_t=0$. On the other hand, another key theoretical insight is that the transition counts in binary sequences are strongly correlated. Specifically, $n_{01}$ and $n_{10}$ are at most one apart: since, every time a transition $0\to 1$ occurs, either the sequence is followed by only $1$'s until the end, or a subsequent transition $1\to 0$ also occurs, and vice versa. In fact, the precise relationship depends on the initial token $x_1$, and on the current token $x_t$. If $x_1 = x_t$, then $n_{01} = n_{10}$; if $x_1=0$ and $x_t = 1$, then $n_{01} = n_{10}+1$; while if $x_1 = 1$ and $x_t=0$, then $n_{10} = n_{01}+1$. Therefore, the dependency of the output on $n_{01}$ is in fact a dependency on $n_{10}$, and vice versa. As we will see, we can leverage this property to help $\simplemamba$ realize Laplacian smoothing with just two-dimensional embeddings, \ie $d=2$. Stitching these facts, the final logits in the \ref{eq:logit-simple} layer depend on the first and current token via
\begin{equation*}
\begin{split}
    \logit_t = W_{\ell} \, \bx_t + W_{\ell}W_o \, \tilde{\bx}^{(x_1)}\bb^{(x_1)^\top} \bc^{(x_t)} 
	+ \mathbbm{1}_{\{x_1 \neq x_t\}} W_{\ell}W_o \, \tilde{\bx}^{(x_1 x_t)} \bb^{(x_1 x_t)\top} \bc^{(x_t)} \\
	+ \sum_{j} n_{x_t j} W_{\ell} W_o \, \tilde{\bx}^{(x_t j)} \bb^{(x_t j)\top} \bc_t.
\end{split}
\label{eq:logit-final}
\end{equation*}
The final step is to then show that for properly chosen parameters, one can make the two vectors associated with the counts, \ie $W_{\ell} W_o \, \tilde{\bx}^{(x_t j)} \bb^{(x_t j)\top} \bc_t$, for $ j \in\binary$, to be orthogonal, and the other vectors, independent of the counts, to sum up to a vector close to $(\beta, \beta)^\top$. Subsequently, the $L_1$ normalization in \ref{eq:prediction-simple} layer will give a next-token probability estimate, matching that of the add-$\beta$ estimator. We refer to \S~\ref{sec:actual_proof1} for full details.  

\subsection{Higher order processes ($k>1$)}
While empirical evidence suggests that \prettyref{thm:order1} would also hold for $k>1$ (Figs. \ref{fig:estimator-norm}, \ref{fig:conv-order}), theoretically the proof becomes intractable due to the difficulty in tracking the correlations between the transition counts as $k$ increases. Nonetheless, we believe that it is possible to extend the main proof ideas from the first-order case to here, though with a more involved construction, leading to the conjecture:
\begin{conjecture}
\label{conj:orderk}
The canonical $\simplemamba$ model can implement a predictor that is arbitrarily close to the add-$\beta$ estimator, for the binary order-$k$ Markov setting, with dimensions $d = N = 2^{k+1}$, $e=1$, and window $w=k+1$.
\end{conjecture}


We further strengthen this claim by the following matching lower bound, which shows that, with finite bit precision, any recurrent architecture such as $\mamba$ cannot implement the Laplacian estimator with arbitrarily small error if the hidden dimension does not scale as $\Omega(2^k)$.

\begin{theorem}
\label{thm:lower-bound}
Consider a recurrent model of the form 
\begin{align*}
H_t &= h (H_{t-1},x_t), \\
y_t &= \thetaprob{\cdot \mid x_1^t} = g(H_t),
\end{align*} 
with transformations $(h,g)$, where $H_t \in \mathbb{R}^d$ and the model has a bit precision of $\mathtt{p}$. Suppose that the Markov kernel $P$ is sampled from the Dirichlet prior with $\beta=1$, $P \sim \dir{1}$. Suppose also that the recurrent architecture satisfies the following point-wise guarantee: for any sufficiently large $t$, almost surely over $P$ and $x_1^t \sim P$,
\begin{equation} \label{eq:41}
    \left\| \thetaprob{\cdot \mid x_1^t} - \mathbb{P}_{1}^{(k)}(\cdot \mid x_1^t) \right\|_\infty \le \varepsilon.
\end{equation}
Then, the recurrent architecture must satisfy
\begin{equation*}
    d \cdot \mathtt{p} \ge 2^k (1 - 3 \varepsilon) \log (1/\varepsilon).
\end{equation*}
\end{theorem}

{\bf Remark.} While we used time-invariant functions $(h,g)$ in \prettyref{thm:lower-bound} for notational simplicity, the proof does not rely on this fact and holds for any general $H_t = h_t (H_{t-1},x_t)$ and $y_t = g_t (H_t)$, which subsumes Mamba.

We provide the intuition behind the proof here. We defer the full proof and additional details to \prettyref{app:lower-bound}.

{\bf Intuition.} The intuition behind this result is in the manner in which the recurrent architecture carries out computation: by proceeding sequentially and compressing the information from the sequence it has seen thus far at some time $t$ into a small hidden vector, the model does not know what the next $k$ tokens will be: the knowledge of this is vital to be able to compute the add-$\beta$ estimator at time $t+k+1$ with a small memory footprint. Indeed, when the identity of the next $k$ tokens changes, the output of the model at time $t+k+1$ must look drastically different (as the add-$\beta$ estimator corresponds to approximately evaluating $\mathbb{P}(\cdot|i_1^k)$, which are unrelated distributions under different choices of $i_1^k$). There are $\sim 2^{2^k}$ possible values the set $P = \{ \mathbb{P}(\cdot|i_1^k) : i_1^k \in \{0,1\}^k \}$ can take. But when $d$ and $\mathtt{p}$ are small, the output of the model just cannot take so many values: it can realize at most $2^{d\mathtt{p}}$ possible sets. In other words, in order to succeed, the recurrent architecture is essentially forced to keep track of the number of occurrences of each $i_1^k \in \{ 0,1 \}^k$ in the sequence at each time $t$, which costs an exponential dependence on $k$ in the hidden dimension/precision.