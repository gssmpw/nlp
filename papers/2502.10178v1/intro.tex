% !TEX root = main.tex

\section{Introduction}
\label{sec:intro}
Transformers have been at the forefront of recent breakthroughs in language modeling, driving the AI revolution \cite{vaswani2017attention, Radford2018ImprovingLU, devlin18}. Despite their empirical success, transformers suffer from high computational complexity, such as quadratic scaling in sequence length during training and linear cache size at inference \cite{gu2023mamba}. To address these limitations, there is a growing interest in designing alternative efficient architectures among which structured state space models (SSMs) are the most prominent. In particular, Selective SSMs such as Mamba and Mamba-$2$, have achieved state-of-the-art results in various language modeling tasks, while greatly improving the inference throughput \cite{cirone2025theoreticalfoundationsdeepselective}.

Motivated by this success, there is tremendous interest in understanding the sequential modeling abilities of SSMs, especially that of Mamba. In particular, mirroring a theme that has been successful in unraveling fundamental mechanisms (\eg induction heads) behind transformers \cite{makkuva2024attention, makkuva2024local, rajaraman2024transformersmarkovdataconstant, nic2024trans, edelman2024evolution}, a growing body of research explores Mamba through its in-context learning (ICL) capabilities \cite{grazzi2024mamba,halloran2024mamba,akyurek2024context}. While these works reveal interesting insights about Mamba's ICL abilities vis-a-vis transformers, they are largely empirical in nature, and we currently lack a fundamental understanding of Mamba and its underlying learning mechanisms. We are thus motivated to ask:
\begin{quote}
\centering
{\bf Can we systematically characterize the ICL capabilities of Mamba?}
\end{quote}

To address this, in this paper we introduce a principled framework for theoretical and empirical analysis of Mamba's ICL capabilities via random Markov chains. Leveraging this framework, we uncover a surprising phenomenon: unlike transformers, even a single-layer Mamba efficiently learns the in-context Laplacian smoothing estimator, which is both
Bayes and minimax optimal, for all Markov orders (\prettyref{fig:estimators}). Towards explaining this, we theoretically characterize the representation capacity of Mamba and demonstrate that the convolution mechanism plays a fundamental role in realizing the Laplacian smoothing. Further we showcase that these theoretical insights align strongly with empirical results, both on Markovian and complex natural language data. To the best of our knowledge, this is the first result of its kind connecting Mamba and optimal statistical estimators.


\begin{figure*}[t]
\captionsetup[sub]{}
\centering
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/estimator.pdf} 
    \put(-132,-7){\fontsize{9}{3}\selectfont $t: x_t = 0$}
      \put(-240,86){\rotatebox[origin=t]{90}{\fontsize{8}{3}\selectfont Predicted probability $\mathbb{P}_{\btheta}\pth{{x_{t+1}=1 \mid x_1^t}}$}}
      \put(-83,156){\fontsize{8}{3}\selectfont $1$-layer Mamba}
      \put(-83,145){\fontsize{8}{3}\selectfont $1$-layer Transformer}
      \put(-83,134){\fontsize{8}{3}\selectfont $2$-layer Transformer}
      \put(-83,123){\fontsize{8}{3}\selectfont Optimal estimator}
\caption{Next-token probability estimation}
\label{fig:estimator-prob}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/estimator-norm.pdf} 
    \put(-138,-7){\fontsize{10}{3}\selectfont Markov order}
      \put(-238,86){\rotatebox[origin=t]{90}{\fontsize{9}{3}\selectfont $L_1$ distance}}
      \put(-82.5,153.5){\fontsize{8}{3}\selectfont $1$-layer Mamba}
      \put(-82.5,142.5){\fontsize{8}{3}\selectfont $1$-layer Transformer}
      \put(-82.5,131){\fontsize{8}{3}\selectfont $2$-layer Transformer}
\caption{$L_1$ distance of model estimation from optimal}
\label{fig:estimator-norm}
\end{subfigure}
\caption{Single-layer Mamba learns the optimal Laplacian smoothing when trained on random Markov chains, exhibiting in-context learning. A two-layer transformer also learns the same, albeit less precisely. In contrast, a single-layer transformer fails to solve this task. We observe the same phenomenon for various Markov orders.}
\label{fig:estimators}
\end{figure*}

In summary, we make the following contributions:

\begin{itemize}
    \item We provide a novel framework for a precise theoretical and empirical study of Mamba's ICL via random Markov chains (\prettyref{sec:markov_background}). \looseness=-1
   \item Capitalizing our framework, we uncover the surprising fact that the convolution mechanism plays a pivotal role in Mamba's learning abilities (\prettyref{sec:empirical}).\looseness=-1
    \item Building upon these insights, we characterize the representational capacity of single-layer Mamba and demonstrate that it can represent the optimal Laplacian smoothing estimator for first-order processes (\prettyref{sec:theory}).\looseness=-1
    \item We demonstrate the generality of our findings on non-Markovian data and illustrate the fundamental role of convolution even on complex language-modeling tasks (\prettyref{sec:beyond}).     
    % empirically investigate how our findings translate to higher order Markov chains and deeper architectures, and outline interesting open problems in this area (\prettyref{sec:higherorder}). \looseness=-1
\end{itemize}

\subsection{Related Work}

State Space Models~\cite{gu2020hippo, gu2021combining} have been recently introduced as an alternative recurrent architecture aimed at rivaling the well established transformer backbone~\cite{vaswani2017attention} widely used across a range of domains, from language modeling to vision. The model was originally introduced as a discretization of a time-continuous linear dynamical system~\cite{gu2021combining}. Recent works tried to re-frame the architecture from a linear recurrent perspective. For example~\cite{orvieto2023resurrecting} tried to understand what components of the architecture are necessary, highlighting the importance of linearity, as well as of the particular parametrization that allows control over the stability of the model.

However, there are still many gaps in understanding this family of models~\cite{team2024jamba}, such as questions around expressivity~\cite{orvieto2023universality}. This is particularly important given the proliferation of Mamba-inspired architectures that has emerged since the introduction of the model. 
%Since its introduction, several architectures have emerged to enhance the Mamba model's capabilities. 
For example, Mamba-Spike \cite{qin2024mamba} integrates a spiking front-end with the Mamba backbone, leveraging event-driven processing for efficient temporal data handling. The Mixture of Mamba approach combines Mamba's selective state spaces with the Transformer's Mixture of Experts (MoE) layer \cite{csordas2024moeut}, potentially offering faster convergence and improved performance scaling. Vision Mamba (Vim) \cite{zhu2401vision} adapts the architecture for visual tasks, employing bidirectional Mamba blocks to process high-resolution images efficiently. Besides specific adaptation of the model into new architectures, the core Mamba block also evolved, for example, by the introduction of gating in Mamba-2~\cite{mamba2023gu} and similar SSM architectures~\cite{de2024griffin,Beck2024xLSTM} which for example invalidates to a certain extent the convolutional view of the architecture. 

Our work squarely focuses on understanding the representation power of Mamba, and ICL, which remains an unanswered question. We believe such insights can help navigate the many flavors of SSM architectures. In this space, recent studies have shown that SSMs can perform gradient-based learning for in-context adaptation, similar to transformers \cite{sushma2024state}, where they show that a single SSM layer, augmented with local self-attention, is able to reproduce outputs of an implicit linear model after one step of gradient descent. While some work suggests that Mamba's ICL abilities rival state-of-the-art transformer models \cite{grazzi2024mamba}, other research indicates that the pretrained Mamba models perform worse in in-context learning than comparable transformer models \cite{halloran2024mamba,akyurek2024context}. SSMs have also demonstrated promising results in in-context reinforcement learning tasks, offering fast and performant solutions \cite{lu2024structured}. \cite{joseph2024hippo} has introduced a novel weight construction for SSMs, enabling them to predict the next state of any dynamical system in-context. These findings highlight the potential of SSMs as efficient alternatives to transformers for ICL tasks, particularly those involving long input sequences.
