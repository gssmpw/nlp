% !TEX root = main.tex

\section{Conclusion}
Structured state space sequence models (SSMs) and Selective SSMs such as Mamba have shown remarkable inference speed-ups over transformers while achieving comparable or superior performance on complex language modeling tasks. In this paper, we studied in-context learning (ICL) capabilities of Mamba on Markov chains and show that, unlike transformers, even a single-layer Mamba efficiently learns the in-context Laplacian smoothing estimator. To explain this, we theoretically characterized the representation capacity of Mamba, which revealed the fundamental role of convolution in enabling it. We provided empirical results that align with our theoretical insights. Extending our results to deeper Mamba models and understanding the role of depth are some interesting future directions.