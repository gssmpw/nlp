\section{Related Work}
State Space Models~\cite{gu2020hippo, gu2021combining} have been recently introduced as an alternative recurrent architecture aimed at rivaling the well established transformer backbone~\cite{vaswani2017attention} widely used across a range of domains, from language modeling to vision. The model was originally introduced as a discretization of a time-continuous linear dynamical system~\cite{gu2021combining}. Recent works tried to re-frame the architecture from a linear recurrent perspective. For example~\cite{orvieto2023resurrecting} tried to understand what components of the architecture are necessary, highlighting the importance of linearity, as well as of the particular parametrization that allows control over the stability of the model.

However, there are still many gaps in understanding this family of models~\cite{team2024jamba}, such as questions around expressivity~\cite{orvieto2023universality}. This is particularly important given the proliferation of Mamba-inspired architectures that has emerged since the introduction of the model. 
%Since its introduction, several architectures have emerged to enhance the Mamba model's capabilities. 
For example, Mamba-Spike \cite{qin2024mamba} integrates a spiking front-end with the Mamba backbone, leveraging event-driven processing for efficient temporal data handling. The Mixture of Mamba approach combines Mamba's selective state spaces with the Transformer's Mixture of Experts (MoE) layer \cite{csordas2024moeut}, potentially offering faster convergence and improved performance scaling. Vision Mamba (Vim) \cite{zhu2401vision} adapts the architecture for visual tasks, employing bidirectional Mamba blocks to process high-resolution images efficiently. Besides specific adaptation of the model into new architectures, the core Mamba block also evolved, for example, by the introduction of gating in Mamba-2~\cite{mamba2023gu} and similar SSM architectures~\cite{de2024griffin,Beck2024xLSTM} which for example invalidates to a certain extent the convolutional view of the architecture. 

Our work squarely focuses on understanding the representation power of Mamba, and ICL, which remains an unanswered question. We believe such insights can help navigate the many flavors of SSM architectures. In this space, recent studies have shown that SSMs can perform gradient-based learning for in-context adaptation, similar to transformers \cite{sushma2024state}, where they show that a single SSM layer, augmented with local self-attention, is able to reproduce outputs of an implicit linear model after one step of gradient descent. While some work suggests that Mamba's ICL abilities rival state-of-the-art transformer models \cite{grazzi2024mamba}, other research indicates that the pretrained Mamba models perform worse in in-context learning than comparable transformer models \cite{halloran2024mamba,akyurek2024context}. SSMs have also demonstrated promising results in in-context reinforcement learning tasks, offering fast and performant solutions \cite{lu2024structured}. \cite{joseph2024hippo} has introduced a novel weight construction for SSMs, enabling them to predict the next state of any dynamical system in-context. These findings highlight the potential of SSMs as efficient alternatives to transformers for ICL tasks, particularly those involving long input sequences.