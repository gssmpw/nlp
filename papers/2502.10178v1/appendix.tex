\newpage
\appendix
\onecolumn

% \tableofcontents
\section{Preliminaries on Laplacian smoothing}
\label{app:laplace}

Laplacian smoothing is a mature and well understood topic. An account can be found, e.g., in~\cite{merhav1998,CesaBL:2006}, with some recent updates in~\cite{BondaschiG:24isit,bondaschi1}.
For the sake of completeness, we provide a brief outline of how it applies to our context. For $k$-th order Markov data, at every time instant $t$, the Laplacian add-$\beta$ estimator applied to the subsequence of tokens with the same context $i_1^k \in \calX^k$ as the current one is the predictor that minimizes the Bayesian cross-entropy loss in \prettyref{eq:loss}, when the Markov kernel is sampled according to the product Dirichlet distribution $\dir{\beta}$. We first give an intuition of why this is the case, and we provide a full proof at the end of the section. We consider the binary case $\calX = \binary$, but the results can be extended to arbitrary finite alphabets.

Consider a given sequence $(x_t)_{t=1}^T$. For every length-$k$ context $i_1^k \in \calX^k$, let $(x_t)|_{i_1^k}$ be the subsequence of tokens preceded by $i_1^k$. Note that, since each sequence $(x_t)$ is generated by a $k$-th order Markov chain, all the tokens in the sequence with the same length-$k$ prefix share the same conditional probability distribution. Furthermore, since each of the conditional distributions of the chain is randomly chosen independently from the others, the subsequence $(x_t)|_{i_1^k}$ is a sufficient statistic to estimate the probability distribution of all the tokens with the same prefix $i_1^k$. Therefore, the optimal prediction for a sequence $(x_t)_{t=1}^T$ is given by employing the optimal predictor for each i.i.d. subsequence $(x_t)|_{i_1^k}$, for every $i_1^k \in \calX^k$. Since each conditional distribution is sampled from a Dirichlet distribution with parameter $\beta$, it is well known that the optimal predictor for such subsequences is the add-constant estimator, with constant equal to $\beta$. More specifically, if $x_{t-k}^{t-1} = i_1^k$, then the optimal estimation for $x_t$ is
\begin{equation}
\label{eq:add-beta}
\betaprob{\beta}{k}{x_{t+1} = j \mid x_1^t} = \frac{n_j + \beta}{n + 2\beta}\ ,
\end{equation}
where $n_j$ is the number of times token $j$ appears in the subsequence $x_1^t|_{i_1^k} = (x_{\ell} \in x_1^t : x_{\ell-k}^{\ell-1} = i_1^k)$, and $n$ is the length of the subsequence. 

We now provide a formal proof of this fact.
\begin{theorem}
Consider the class of all $k$-th order Markov kernels $P = (P_{i_1^k})_{i_1^k \in \calX^k}$, where each $P_{i_1^k} = \mathbb{P}(\cdot \mid i_1^k)$ is a probability distribution on $\calX = \binary$. Let each $P_{i_1^k}$ be sampled i.i.d. from $\dir{\beta}$, and let $x_1^k \sim \unif{\calX^k}$ and $x_{t+1} | x_1^t \sim P_{x_{t-k+1}^t}$. Then, the predictor $f^{(j)}(x_1^t) = \hat{\mathbb{P}}(x_{t+1} = j \mid x_1^t)$, for $j\in\binary$, that minimizes the loss
\begin{equation}
L \define -\frac{1}{T} \sum_{t \in \set T} \Expect_P \Expect_{x_1^{t+1} \sim P}\big[x_{t+1} \cdot \log f^{(1)} (x_1^t) + (1- x_{t+1}) \cdot \log f^{(0)}(x_1^t) \big]
\end{equation}
is the add-$\beta$ estimator in \prettyref{eq:add-beta}, \ie the minimizer $f_*^{(j)}(x_1^t) = \betaprob{\beta}{k}{x_{t+1} = j \mid x_1^t}$, for all $t\geq k$.
\end{theorem}
\begin{proof}
First note that
\begin{align*}
L &= -\frac{1}{T} \sum_t \Expect_P \Expect_{x_1^{t+1} \sim P}\big[x_{t+1} \cdot \log f^{(1)} (x_1^t) + (1- x_{t+1}) \cdot \log f^{(0)}(x_1^t) \big] \\
&= -\frac{1}{T} \sum_t \Expect_{x_1^t} \Expect_{x_{t+1}|x_1^t} \big[x_{t+1} \cdot \log f^{(1)} (x_1^t) + (1- x_{t+1}) \cdot \log f^{(0)}(x_1^t) \big] \\
&= -\frac{1}{T} \sum_t \Expect_{x_1^t} \big[\Expect_{x_{t+1}|x_1^t}[x_{t+1}] \cdot \log f^{(1)} (x_1^t) + (1- \Expect_{x_{t+1}|x_1^t}[x_{t+1}]) \cdot \log f^{(0)}(x_1^t) \big].
\end{align*}
Let us define the distribution $f_*^{(1)}(x_1^t) \define \Expect_{x_{t+1}|x_1^t}[x_{t+1}]$ and $f_*^{(0)}(x_1^t) \define 1 - f_*^{(1)}(x_1^t)$. Then, we can rewrite the loss as
\begin{equation*}
L = \frac{1}{T} \sum_t \Expect_{x_1^t} \big[-f_*^{(1)}(x_1^t) \cdot \log f^{(1)} (x_1^t) - f_*^{(0)}(x_1^t) \cdot \log f^{(0)}(x_1^t) \big].
\end{equation*}
For every $t \in [T]$ and every $x_1^t \in \calX^t$, the term inside the expectation is minimized by picking $f^{(1)}(x_1^t) = f^{(1)}_*(x_1^t)$. In fact, note that it can be rewritten as
\begin{align*}
-f_*^{(1)}&(x_1^t) \cdot \log f^{(1)} (x_1^t) - f_*^{(0)}(x_1^t) \cdot \log f^{(0)}(x_1^t) \notag\\
&= f_*^{(1)}(x_1^t) \cdot \log \frac{f_*^{(1)}(x_1^t)}{f^{(1)} (x_1^t)} + f_*^{(0)}(x_1^t) \cdot \log \frac{f_*^{(0)}(x_1^t)}{f^{(0)}(x_1^t)} - f_*^{(1)}(x_1^t)\log f_*^{(1)}(x_1^t) - f_*^{(0)}(x_1^t) \log f_*^{(0)}(x_1^t) \\
&= \KL{f_*(x_1^t)}{f(x_1^t)} + H(f_*(x_1^t)),
\end{align*}
which is minimized when $\KL{f_*(x_1^t)}{f(x_1^t)} = 0$, i.e., when $f(x_1^t) = f_*(x_1^t)$. We will now show that $f_*(x_1^t)$ is precisely the add-$\beta$ estimator. Consider any context $i_1^k$ and any sequence $x_1^t$ such that $x_{t-k+1}^t = i_1^k$. Let also $p \define P_{i_1^k}(1) = \mathbb{P}(1\mid i_1^k)$. Then, 
\begin{align*}
f_*^{(1)}(x_1^t) &\define \Expect_{x_{t+1}|x_1^t}[x_{t+1}] \\
&= \Expect_{P_{i_1^k} | x_1^t} \Expect_{x_{t+1}|x_1^t, P_{i_1^k}}[x_{t+1}] \\
&= \Expect_{P_{i_1^k} | x_1^t} [P_{i_1^k}(1)] \\
&= \Expect_{P_{i_1^k} | x_1^t|_{i_1^k}} [P_{i_1^k}(1)],
\end{align*}
where in the last equation we used the fact that, when $x_1^k \sim \unif{\calX^k}$, the subsequence $x_1^t|_{i_1^k}$ is a sufficient statistic for $P_{i_1^k}$. Hence,
\begin{align*}
f_*^{(1)}(x_1^t) &= \Expect_{P_{i_1^k} | x_1^t|_{i_1^k}} [P_{i_1^k}(1)] \\
&= \int_0^1 \frac{p^{\beta-1}(1-p)^{\beta-1} p^{n_1} (1-p)^{n_0}}{\int_0^1 q^{\beta-1}(1-q)^{\beta-1} q^{n_1} (1-q)^{n_0} \, dq} \cdot p\, dp \\
&= \frac{\int_0^1 p^{n_1+\beta}(1-p)^{n_0+\beta-1} \, dp}{\int_0^1 q^{n_1+\beta-1}(1-q)^{n_0+\beta-1} \, dq} \\
&= \frac{\Gamma(n_1+\beta+1)\Gamma(n_0+\beta)}{\Gamma(n+2\beta+1)} \cdot \frac{\Gamma(n+2\beta)}{\Gamma(n_1+\beta)\Gamma(n_0+\beta)} \\
&= \frac{n_1 + \beta}{n+2\beta},
\end{align*}
where we used the fact that $P_{i_1^k} \sim \dir{\beta}$, that $\int_0^1 q^{z_1-1} (1-q)^{z_0-1} = \Gamma(z_1) \Gamma(z_0) / \Gamma(z_1 + z_0)$, and that $\Gamma(z+1) = z \Gamma(z)$.
\end{proof}

{\bf Remark.} The proof above is for $x_1^k \sim \unif{\calX^k}$. However, note that the same proof would also work for $x_1^k$ distributed according to any distribution that is independent of the Markov kernel $P$. If instead the distribution depends on $P$ (e.g., the stationary distribution of the Markov chain), then the proof would fail in the step where $x_1^t|_{i_1^k}$ is a sufficient statistic for $P_{i_1^k}$.

{\bf Remark.} It is important to note that, to be able to implement such a predictor requires in-context capabilities: at inference, in order to optimally predict the next token, the model must be able to look into the previous tokens of the test sequence, and count the tokens with the correct prefix. 


\section{Preliminaries and Proof of \prettyref{thm:order1}}
\label{app:proof1}
\subsection{Empirical insights}
\label{sec:warmup}
Here we expand upon our empirical observations in \ref{sec:proof_sketch}, which form the basis of our proof.

{\bf State-to-state transition factor $a_t \approx 1$ for all $t \geq 1$.} We empirical evidence supporting this observation in \prettyref{fig:at}.

\begin{figure*}[!h]
\centering
\includegraphics[width=\textwidth]{figures/at_final.pdf} 
    \put(-356,-7){\fontsize{10}{3}\selectfont Position $t$}
      \put(-480,93){\rotatebox[origin=t]{90}{\fontsize{11}{3}\selectfont $a_t$}}
\caption{Value of $a_t$ across positions at convergence.}
\label{fig:at}
\end{figure*}

{\bf Convolution window $w \geq k+1$.} Recalling that $k$ is the Markov order, we empirically observe that the window that $w=k+1$ is sufficient for the full Mamba to learn the Laplacian smoothing on \kth Markov chains. To understand why, note that in the $\simplemamba$ architecture above, apart from the \ref{eq:mamba-eqs-simple} block, all remaining equations operate on the current token at time $t$. In the \ref{eq:mamba-eqs-simple} block, same as the \ref{eq:mamba_block} block except ReLU, the dependency of the output $\by_t$ on the previous tokens is due to that of the state $H_t$ on $(\tilde{\bx}_t, \bb_t)$ in the update equation, and of $\bc_t$ in the state projection. Since $(\tilde{\bx}_t, \bb_t, \bc_t)$ depend on the past through the convolutions, a window of size $k+1$ enables them to keep track of the current token as well as its length-$k$ prefix, which is necessary to compute the counts needed in \ref{eq:laplace_smooth}. On the other hand, if $w \leq k$, then one can find \emph{confusable} sequences, \ie sequences that share the same number of occurrences of all length-$k$ prefixes, but whose counts of the tokens following each prefix is different. 

For such sequences, the state $H_t$ is the same, and so are the predicted probabilities by the Mamba model; however, the optimal estimator, depending on the transition counts, would give very different probability estimates, allowing Mamba's prediction loss to deviate from that of the optimal. For example, consider $k=1$. If $w=1$, then $(\tilde{\bx}_t, \bb_t, \bc_t)$ depend only on the current token $x_t$. Then, consider the two sequences $x = (0,1,0,1,0,1)$ and $\tilde{x} = (0,0,0,1,1,1)$. At time $t=6$, these two sequences would give the same state $H_t$ and the same output $\by_t$, since they share the same number of tokens $0$ and $1$. Therefore, the estimated probability given by the model would be the same in both cases. However, the optimal add-constant estimator (with $\beta=1$) would estimate the probability of $x_{t+1}=1$ to be $1/4$ for $\bx$, and $3/4$ for $\tilde{\bx}$.

Further, \emph{it is sufficient that the convolution for $\bc_t$ has window $w_C = k$.} That is, the convolution $\conv_C$ involved in the computation of $\bc_t$ can have a window size equal to the Markov order $k$ (i.e., one less than $\conv_X$ and $\conv_B$) without affecting the model's capability of learning the task (or, equivalently, the left-most kernel coefficients of $\conv_C$ can be taken to be zero). Intuitively, this is because the role of $\bc_t$ in the state projection is to select the correct transition counts for the computation of the estimator, distilled into $\by_t$. In order to do so, it is sufficient to know the length-$k$ context of the current symbol $x_t$, which can be encoded by a convolution with window size $k$.


\subsection{Proof of \prettyref{thm:order1}}
\label{sec:actual_proof1}
Fix $\epsilon > 0$ and let $\beta > 0$ be the constant of the considered add-constant estimator. Let us fix $a=0$ and $\Delta_t = 1$, so that $a_t = 1$, for all $t\geq 1$. This can be done by picking, e.g., $\bw_{\Delta} = \boldsymbol{0}$ and $\delta$ such that $\softplus(\delta) = 1$. Let us compactly denote the convolution kernels as
\begin{equation}
\conv_X = \begin{pmatrix}
    \alpha_{00} &\alpha_{01} \\
    \alpha_{10} &\alpha_{11}
\end{pmatrix}, \qquad
\conv_B = \begin{pmatrix}
    \gamma_{00} &\gamma_{01} \\
    \gamma_{10} &\gamma_{11}
\end{pmatrix}
\end{equation}
where each row corresponds to the kernel weights applied time-wise to each coordinate of the input sequence $(\bx_t)_{t\geq 1}$. Since the window for $\conv_C$ is $w_C = 1$, we can simply assume w.l.o.g. that $C_t = W_C \bx_t$. 

Let us denote the embedding vectors to be $\be_0 = (e_{00}, e_{01})^\top$ and $\be_1 = (e_{10},e_{11})^\top$, and assume that the vectors are not collinear. Take also $W_X = W_B$ such that
\begin{equation}
W_X \,\be_0 = \begin{pmatrix}
    1 \\
    0
\end{pmatrix}, \qquad
W_X \,\be_1 = \begin{pmatrix}
    0 \\
    1
\end{pmatrix}
\end{equation}
and take $W_C$ such that
\begin{equation}
W_C \,\be_0 = \begin{pmatrix}
    c_0 \\
    0
\end{pmatrix}, \qquad
W_C \,\be_1 = \begin{pmatrix}
    0 \\
    c_1
\end{pmatrix}.
\end{equation}
Let us also take the kernels of $\conv_X$ and $\conv_B$ to be the same across coordinates, i.e., 
\begin{equation}
\conv_X = \begin{pmatrix}
    \alpha_{0} &\alpha_{1} \\
    \alpha_{0} &\alpha_{1}
\end{pmatrix}, \qquad 
\conv_B = \begin{pmatrix}
    \gamma_{0} &\gamma_{1} \\
    \gamma_{0} &\gamma_{1}
\end{pmatrix}
\end{equation}
such that the following conditions are satisfied:
\begin{equation}
\begin{cases}
\alpha_0\gamma_0 + \alpha_1\gamma_1 = 0 \\
\alpha_0\gamma_1 + \alpha_1\gamma_0 > 0 \\
\alpha_0 \neq \alpha_1 \\
\frac{\alpha_0 \gamma_1}{\alpha_0 \gamma_1 + \alpha_1 \gamma_0} = -\beta \epsilon
\end{cases}
\end{equation}
Note that, with such a choice of parameters, we have
\begin{equation}
X^{(0)} = \begin{pmatrix}
\alpha_1 \\
0
\end{pmatrix}, \qquad
X^{(1)} = \begin{pmatrix}
0 \\
\alpha_1
\end{pmatrix}, \qquad
B^{(0)} = \begin{pmatrix}
\gamma_1 \\
0
\end{pmatrix}, \qquad
B^{(1)} = \begin{pmatrix}
0 \\
\gamma_1
\end{pmatrix}
\end{equation}
\begin{equation}
C^{(0)} = \begin{pmatrix}
c_0 \\
0
\end{pmatrix}, \qquad
C^{(1)} = \begin{pmatrix}
0 \\
c_1
\end{pmatrix}
\end{equation}
\begin{equation}
X^{(00)} = \begin{pmatrix}
\alpha_0 + \alpha_1 \\
0
\end{pmatrix}, \qquad
X^{(01)} = \begin{pmatrix}
\alpha_0 \\
\alpha_1
\end{pmatrix}, \qquad
X^{(10)} = \begin{pmatrix}
\alpha_1 \\
\alpha_0
\end{pmatrix}, \qquad
X^{(11)} = \begin{pmatrix}
0 \\
\alpha_0 + \alpha_1
\end{pmatrix}
\end{equation}
\begin{equation}
B^{(00)} = \begin{pmatrix}
\gamma_0 + \gamma_1 \\
0
\end{pmatrix}, \qquad
B^{(01)} = \begin{pmatrix}
\gamma_0 \\
\gamma_1
\end{pmatrix}, \qquad
B^{(10)} = \begin{pmatrix}
\gamma_1 \\
\gamma_0
\end{pmatrix}, \qquad
B^{(11)} = \begin{pmatrix}
0 \\
\gamma_0 + \gamma_1
\end{pmatrix}.
\end{equation}
(We replaced the vector notation of \prettyref{sec:theory} with matrix notation, so that $X^{(0)}$ has to be intended as $\tilde{\bx}^{(0)}$, and so on.)
Take also $W_o = W_{\ell} = I$. With this choice of parameters, the final logit vector becomes
\begin{align}
\logit_t &= W_{\ell} \bx_t + W_{\ell}W_o X^{(x_1)}B^{(x_1)^\top} C^{(x_t)} + \mathbbm{1}_{\{x_1 \neq x_t\}} W_{\ell}W_o X^{(x_1 x_t)} B^{(x_1 x_t)\top} C^{(x_t)} \notag\\
    &\hspace{25em}+ \sum_{j} n_{x_t j} W_{\ell} W_o X^{(x_t j)} B^{(x_t j)\top} C_t \\
    &= \begin{pmatrix}
    e_{00} + c_0 \alpha_1 \gamma_1 \\
    e_{01}
    \end{pmatrix} +
    \mathbbm{1}_{\{x_1 = 1\}} \cdot \begin{pmatrix}
    0 \\
    c_0 \alpha_0 \gamma_1
    \end{pmatrix} +
    n_{00} \cdot \begin{pmatrix}
    (\alpha_0 + \alpha_1)(\gamma_0 + \gamma_1) c_0 \\
    0
    \end{pmatrix} \notag\\
    &\hspace{26em}+n_{01} \cdot \begin{pmatrix}
    (\alpha_0 \gamma_0 + \alpha_1 \gamma_1) c_0 \\
    (\alpha_0 \gamma_1 + \alpha_1 \gamma_0) c_0
    \end{pmatrix} \\
    &= \begin{pmatrix}
    e_{00} + c_0 \alpha_1 \gamma_1 \\
    e_{01}
    \end{pmatrix} +
    \mathbbm{1}_{\{x_1 = 1\}} \cdot \begin{pmatrix}
    0 \\
    c_0 \alpha_0 \gamma_1
    \end{pmatrix} +
    n_{00} \cdot \begin{pmatrix}
    (\alpha_0 \gamma_1 + \alpha_1 \gamma_0) c_0 \\
    0
    \end{pmatrix} \notag\\
    &\hspace{26em}+ n_{01} \cdot \begin{pmatrix}
    0 \\
    (\alpha_0 \gamma_1 + \alpha_1 \gamma_0) c_0
    \end{pmatrix}
\end{align}
if $x_t = 0$, and
\begin{align}
\logit_t &= \begin{pmatrix}
    e_{10} \\
    e_{11} + c_1 \alpha_1 \gamma_1
    \end{pmatrix} +
    \mathbbm{1}_{\{x_1 = 0\}} \cdot \begin{pmatrix}
    c_1 \alpha_0 \gamma_1 \\
    0
    \end{pmatrix} +
    n_{10} \cdot \begin{pmatrix}
    (\alpha_0 \gamma_1 + \alpha_1 \gamma_0) c_1 \\
    (\alpha_0 \gamma_0 + \alpha_1 \gamma_1) c_1
    \end{pmatrix} \notag\\
    &\hspace{24em}+ n_{11} \cdot \begin{pmatrix}
    0 \\
    (\alpha_0 + \alpha_1)(\gamma_0 + \gamma_1) c_1
    \end{pmatrix} \\
    &= \begin{pmatrix}
    e_{10} \\
    e_{11} + c_1 \alpha_1 \gamma_1
    \end{pmatrix} +
    \mathbbm{1}_{\{x_1 = 0\}} \cdot \begin{pmatrix}
    c_1 \alpha_0 \gamma_1 \\
    0
    \end{pmatrix} +
    n_{10} \cdot \begin{pmatrix}
    (\alpha_0 \gamma_1 + \alpha_1 \gamma_0) c_1 \\
    0
    \end{pmatrix} \notag\\
    &\hspace{24em}+ n_{11} \cdot \begin{pmatrix}
    0 \\
    (\alpha_0 \gamma_1 + \alpha_1 \gamma_0) c_1
    \end{pmatrix}
\end{align}
if $x_t = 1$.
Take now
\begin{align}
e_{00} &= e_{11} = (\alpha_0 \gamma_1 + \alpha_1 \gamma_0) \beta c_0 - \alpha_1\gamma_1 c_0 \\
e_{01} &= e_{10} = (\alpha_0 \gamma_1 + \alpha_1 \gamma_0) \beta c_0 - \alpha_0\gamma_1 c_0
\end{align}
With this choice of parameters, after the layer normalization, the final output probability vector is
\begin{equation}
f_{\btheta}(x_1^t) = \left(\frac{n_{00} + \beta}{n_{00} + n_{01} + 2\beta + \mathbbm{1}_{\{x_1 = 0\}}\cdot\beta\epsilon} \ , \ \frac{n_{01} + \beta + \mathbbm{1}_{\{x_1 = 0\}}\cdot\beta\epsilon}{n_{00} + n_{01} + 2\beta + \mathbbm{1}_{\{x_1 = 0\}}\cdot\beta\epsilon}\right)^\top
\end{equation}
if $x_t = 0$, and
\begin{equation}
f_{\btheta}(x_1^t) = \left(\frac{n_{10} + \beta + \mathbbm{1}_{\{x_1 = 1\}}\cdot\beta\epsilon}{n_{10} + n_{11} + 2\beta + \mathbbm{1}_{\{x_1 = 1\}}\cdot\beta\epsilon} \ , \ \frac{n_{11} + \beta}{n_{10} + n_{11} + 2\beta + \mathbbm{1}_{\{x_1 = 1\}}\cdot\beta\epsilon}\right)^\top
\end{equation}
if $x_t = 1$.
Note that the resulting predicted probabilities exactly match the add-$\beta$ estimator when $x_1 \neq x_t$, but they are slightly different when $x_1 = x_t$ due to the additional $\beta\epsilon$ factor. We now show that, when the additional factor is present, the two predictors nevertheless differ by at most $\epsilon$ in KL distance. We show it for the case $x_1 = x_t = 0$, the other case follows in the same way. In fact, note that
\begin{equation}
\frac{n_{01} + \beta + \beta\epsilon}{n_{00} + n_{01} + 2\beta + \beta\epsilon} = \frac{n_{01} + \beta}{n_{00} + n_{01} + 2\beta} \cdot\frac{1 + \frac{\beta\epsilon}{n_{01}+\beta}}{1 + \frac{\beta\epsilon}{n_{00}+n_{01}+2\beta}}.
\end{equation}
Now, since
\begin{equation}
1 \leq 1 + \frac{\beta\epsilon}{n_{01}+\beta} \leq 1+\epsilon
\end{equation}
and
\begin{equation}
1 \leq 1 + \frac{\beta\epsilon}{n_{00}+n_{01}+2\beta} \leq 1+\epsilon
\end{equation}
we have that
\begin{equation}
\frac{n_{01} + \beta}{n_{00} + n_{01} + 2\beta} \leq \frac{n_{01} + \beta + \beta\epsilon}{n_{00} + n_{01} + 2\beta + \beta\epsilon} \cdot \left(1+\epsilon\right)
\end{equation}
but we also have
\begin{equation}
\frac{n_{00} + n_{01} + 2\beta + \beta\epsilon}{n_{00} + n_{01} + 2\beta} \leq 1 + \frac{\beta\epsilon}{n_{00} + n_{01} + 2\beta} \leq 1 + \epsilon,
\end{equation}
so that
\begin{align}
D_{\mathrm{KL}}\left(\betaprob{\beta}{1}{\cdot \mid x_1^t} \| \thetaprob{\cdot \mid x_1^t}\right) &= \betaprob{\beta}{1}{x_{t+1}=0 \mid x_1^t} \log \frac{\betaprob{\beta}{1}{x_{t+1}=0 \mid x_1^t}}{\thetaprob{x_{t+1}=0 \mid x_1^t}} \notag\\
    &\hspace{7em}+ \betaprob{\beta}{1}{x_{t+1}=1 \mid x_1^t} \log \frac{\betaprob{\beta}{1}{x_{t+1}=1 \mid x_1^t}}{\thetaprob{x_{t+1}=1 \mid x_1^t}} \\
    &= \frac{n_{00} + \beta}{n_{00} + n_{01} + 2\beta} \log \frac{\frac{n_{00} + \beta}{n_{00} + n_{01} + 2\beta}}{\frac{n_{00} + \beta}{n_{00} + n_{01} + 2\beta + \beta\epsilon}} \notag\\
    &\hspace{11em}+ \frac{n_{01} + \beta}{n_{00} + n_{01} + 2\beta} \log\frac{\frac{n_{01} + \beta}{n_{00} + n_{01} + 2\beta}}{\frac{n_{01} + \beta + \beta\epsilon}{n_{00} + n_{01} + 2\beta + \beta\epsilon}} \\
    &\leq \frac{n_{00} + \beta}{n_{00} + n_{01} + 2\beta} \log(1+\epsilon) + \frac{n_{01} + \beta}{n_{00} + n_{01} + 2\beta} \log(1+\epsilon) \\
    &\leq \log(1+\epsilon) \\
    &\leq \epsilon
\end{align}
concluding the proof. \qed




\section{Proof of \prettyref{thm:lower-bound}}
\label{app:lower-bound}
Consider a recurrent model of the form $H_t = h (H_{t-1},x_t)$ and $y_{t} = g(H_t)$ for each $t \ge 1$ where $H_t \in \mathbb{R}^d$ and the model has a bit precision of $p$. In this proof, we will assume that the state space of the underlying Markov chain is $\{0,1\}$. By the recurrent architecture, the predicted distribution over the next token $x_{t+k+1}$ is of the form,
\begin{equation}
y_{t+k} = \thetaprob{x_{t+k+1} = z \mid x_1^{t+k}} = g(z, H_{t+k}).
\end{equation}
Recall that the add-$1$ estimator is defined as,
\begin{align}
    \frac{n(z,x_{t+1}^{t+k}) + 1}{n(x_{t+1}^{t+k}) + 2},
\end{align}
where $n(z_1^k) = \sum_{i=1}^{t+1} \mathbb{I} (x_{i}^{i+k-1} = z_1^k)$ indicates the number of times $z_1^k$ appears in the sequence. This is the optimal estimator for sequences drawn from the product-Dirichlet prior: for every $i_1^k$, $P(\cdot \mid i_1^k) \sim \dir{1}$, which is the distribution we will assume for this proof. Fixing $x_1^t$, we can write the add-$1$ estimator more explicitly as a function of $x_{t+1}^{t+k}$ as,
\begin{align}
    \betaprob{1}{k}{x_{t+k+1} = z \mid x_1^t, x_{t+1}^{t+k} = z_1^k} = \frac{n(z_1^k,z) + 1}{n(z_1^k) + 2}.
\end{align}
Now, fixing $x_1^t$, correctness of the recurrent model means that, almost surely over $P$ drawn from the prior, and $x_1^t \sim P$ and $z_1^k \sim P (\cdot | x_1^t)$,
\begin{align} \label{eq:44}
    g ( x_{t+k+1} = 0, H_{t+k} ) \in \betaprob{1}{k}{ x_{t+k+1} = 0 \mid x_1^t, x_{t+1}^{t+k} = z_1^k} + [-\varepsilon,\varepsilon].
\end{align}
where $H_{t+k}$ is a function of $x_1^t$ and $z_1^k$.
As $t \to \infty$, under the randomness of the draw of $x_1^t \sim P$, by the strong law of large numbers RHS converges almost surely to the conditional distribution under $P$, almost surely over the choice of $P$ from the product-Dirichlet prior. Here we use the fact that for $P$ drawn from the product-Dirichlet prior, $P(z|z_1^k) > 0$ almost surely, and so the resulting distributions are exponentially mixing and ergodic. Namely, for each $z_1^k \in \{0,1\}^k$, almost surely over $P$ drawn from the product-Dirichlet prior,
\begin{align}
    \Pr \left( \limsup_{t \to \infty} \left|\betaprob{1}{k}{ x_{t+k+1} = 0 \mid x_1^t, x_{t+1}^{t+k} = z_1^k} - P(0 | z_1^k ) \right| > \gamma ) \right) = 0
\end{align}
for any $\gamma > 0$. Therefore, a necessary condition to satisfy \cref{eq:44} is, for each $z_1^k \in \mathcal{X}^k$,
\begin{align}
    g ( x_{t+k+1} = 0, H_{t+k} ) \in P(0|z_1^k) + [-\varepsilon-\eta_P (t),\varepsilon+\eta_P (t)].
\end{align}
for some $\eta_P (t)$, which is a function of $P$ satisfying $\limsup_{t \to \infty} \eta_P (t) = 0$ almost surely over $P$ drawn from the prior; note that $H_{t+k}$ is implicitly a function of $x_1^t$ and $z_1^k$. Divide the interval $[0,1]$ into $1/\varepsilon$ disjoint intervals of size $\varepsilon$ each.  Recall that $P (\cdot|z_1^k) \sim \rho = \dir{1}$, which implies that the random variable $P (0|z_1^k)$ for each fixed $z_1^k$ (randomness is over $P$) is distributed as,
\begin{align}
    \Pr_\rho \left[ P (0|z_1^k) = \cdot \middle| z_1^k \right] = \unif{[0,1]}.
\end{align}
Consider the buckets $B_{\varepsilon} = \{ [0,\varepsilon), [\varepsilon,2\varepsilon), \cdots, [1-\varepsilon,1]\}$. Define the function $\round (p) : [0,1] \to \{ 0,\cdots,|B_{\varepsilon}|-1 \}$ to return the index of the bucket in $B_\varepsilon$ such that $p$ falls in that bucket.

\begin{lemma} \label{lemma:1}
Consider any function $f (z_1^k) : \mathcal{X}^k \to \{ 0,\cdots, |B_\varepsilon| - 1 \}$ such that, pointwise,
\begin{align}
    |\round ( P(0|z_1^k)) - f(z_1^k)| \le r.
\end{align}
Then, when $P(0|z_1^k) \overset{\text{i.i.d.}}{\sim} \dir{1}$,
\begin{align}
    H_{\text{Shannon}} ( \{ f(z_1^k) : z_1^k \in \{0,1\}^k \} ) \ge 2^k \left( (1 - 3 \varepsilon) \log (1/\varepsilon) - \log (2r+1)\right)
\end{align}
where the randomness is over the draw of $P$ and $H_{\text{Shannon}}$ is the discrete Shannon entropy.
\end{lemma}
\begin{proof}
Recall that $P(0|z_1^k) \overset{\text{i.i.d.}}{\sim} \text{Unif} ([0,1])$ across $z_1^k \in \mathcal{X}^k$. Then,
\begin{align}
    \Pr (\round (P(0|z_1^k)) = j) &= \Pr (P(0|z_1^k) \in [\varepsilon(j-1/2),\varepsilon(j+1/2))) \\
    &= \begin{cases}
        \varepsilon \quad &\text{if } 1 \le j \le |B_\varepsilon|-2, \\
        3\varepsilon/2 &\text{if } j = 0 \text{ or } j = |B_\varepsilon|-1.
    \end{cases}
\end{align}
This implies that, by independence of the $P (0|z_1^k)$'s across $z_1^k \in \mathcal{X}^k$,
\begin{align}
    H_{\text{Shannon}} \left( \left\{ P(0|z_1^k) : z_1^k \in \mathcal{X}^k \right\} \right) \ge |\mathcal{X}|^k  (1 - 3 \varepsilon) \log (1/\varepsilon).
\end{align}
Let $e(z_1^k)$ be the random variable $P(0|z_1^k) - f(z_1^k)$. $P(0|z_1^k)$ is a measurable function of $f(z_1^k)$ and $e (z_1^k)$, and therefore,
\begin{align}
    H_{\text{Shannon}} \left( \left\{ f(z_1^k) : z_1^k \in \mathcal{X}^k \right\} \cup \left\{ e(z_1^k) : z_1^k \in \mathcal{X}^k \right\} \right) \ge H_{\text{Shannon}} \left( \left\{ P(0|z_1^k) : z_1^k \in \mathcal{X}^k \right\} \right)
\end{align}
Note that $e (z_1^k)$ is bounded in the rate $\{ -r,\cdots,r\}$ and can take at most $2r+1$ values. Therefore, $H \left( \left\{ e(z_1^k) : z_1^k \in \mathcal{X}^k \right\} \right) \le |\mathcal{X}|^k \log (2r+1)$. Since $H(A,B) \le H(A) + H(B)$, we have that,
\begin{align}
    H_{\text{Shannon}} \left( \left\{ f(z_1^k) : z_1^k \in \mathcal{X}^k \right\} \right) &\ge H_{\text{Shannon}} \left( \left\{ P(0|z_1^k) : z_1^k \in \mathcal{X}^k \right\} \right) - H \left( \left\{ e(z_1^k) : z_1^k \in \mathcal{X}^k \right\} \right) \\
    &\ge |\mathcal{X}|^k \left( (1 - 3 \varepsilon) \log(1/\varepsilon) - \log (2r+1) \right).
\end{align}
\end{proof}

Recall that we are guaranteed that $g(x_{t+k+1} = 0, H_{t+k}) \in P(0|z_1^k) + [-\varepsilon-\eta_P (t),\varepsilon+\eta_P (t)]$. This implies that the recurrent model is able to recover $\round(p)$ for $p = P(0|z_1^k)$ up to an error of $r=\lceil \eta(t)/\varepsilon \rceil$ for each $z_1^k \in \mathcal{X}^k$ by computing $\round (\hat{p})$ where $\hat{p} = g(x_{t+k+1} = 0, H_{t+k})$. Informally, this just means that $\hat{p}$ is likely to fall in a bucket close to $p$. In combination with \Cref{lemma:1}, for $f(z_1^k) = g(x_{t+k+1} = 0, H_{t+k})$ we have that,
\begin{align}
    H_{\text{Shannon}} ( \{ g(x_{t+k+1} = 0, H_{t+k}) : z_1^k \in \{0,1\}^k \} ) \ge 2^k \left( (1 - 3 \varepsilon) \log (1/\varepsilon) - \log (2 \lceil \eta_P (t)/\varepsilon \rceil +1)\right)
\end{align}
Note however, that $g(x_{t+k+1} = 0, H_{t+k})$ is a function of $z_1^k$ implicitly, through $H_{t+k}$ (which is also a function of $x_1^t$). Since the dimensionality of $H_{t+k}$ is $d$ and the model is implemented to $\mathtt{p}$ bits of precision,
\begin{align}
    H_{\text{Shannon}} ( \{ g(x_{t+k+1} = 0, H_{t+k}) : z_1^k \in \{0,1\}^k \} )  \le H_{\text{Shannon}} ( H_{t+k} ) \le d \mathtt{p}
\end{align}
where all randomness here is induced by the random draw of the \kth Markov kernel $P$. Therefore, for the correctness guarantee \Cref{eq:44} to hold, we need,
\begin{align}
    d \mathtt{p} \ge 2^k \left( (1 - 3 \varepsilon) \log (1/\varepsilon) - \log (2 \lceil \eta_P (t)/\varepsilon \rceil +1)\right)
\end{align}
in the limit $t \to \infty$, and noting that $\limsup_{t \to \infty} \eta_P (t) = 0$ almost surely over $P$ drawn from the prior, it is necessary that,
\begin{align}
    d \mathtt{p} \ge 2^k (1 - 3 \varepsilon) \log (1/\varepsilon).
\end{align}
\qed

{\bf Remark.} The proof above assumes that the \kth Markov chain is on a binary state space. However, the result can easily be extended to give the lower bound $d \cdot \mathtt{p} \ge \Omega(|\mathcal{X}|^k)$ for larger state spaces, as well as similar scaling results for priors $\dir{\beta}$ for any $\beta > 0$. Furthermore, we believe it should be possible to replace the $L_\infty$ error guarantee in \cref{eq:41} by the KL-divergence between the two distributions without significantly changing the conclusion ($d \cdot \mathtt{p} = 2^{\Omega(k)}$).

\newpage
\section{Model architectures and hyper-parameters}
\label{app:architecture}

\begin{table}[h]
\caption{Parameters in the Mamba architecture with their shape.}
\label{tab:mamba-parameters}
\vspace{1mm}
\small%
\newcolumntype{R}{>{\raggedleft\arraybackslash}X}
\begin{tabularx}{\linewidth}{Xllr}
\toprule
Parameter
& Matrix shape \\
\cmidrule(lr){1-2}
embedding
& $2 \times d$ \\
mamba.A
& $1$ \\
mamba.dt
& $1$ \\
mamba.in\_proj
& $(2ed + 2N + 1) \times d$ \\
mamba.conv1d
& $(ed + 2N) \times w$ \\
mamba.out\_proj
& $d \times (2ed + 2N + 1)$ \\
mlp.fc1 
& $4d \times d$ \\
mlp.fc2
& $d \times 4d$ \\
lm\_head
& $d \times 2$ \\
\bottomrule
\end{tabularx}
\end{table}

\begin{table}[h]
\caption{Settings and parameters for the Mamba model used in the experiments.}
\label{tab:mamba-setup}
\vspace{1mm}
\small%
\newcolumntype{R}{>{\raggedleft\arraybackslash}X}
\begin{tabularx}{\linewidth}{lX}
    \toprule
    % Parameter & Default value \\
    % \midrule
    Dataset & $k$-th order binary Markov source \\
    Architecture & Based on the Mamba-2 architecture as implemented in \cite{dao2024transformers} \\
    \midrule
    Batch size & Grid-searched in $\{16, 32, 64, 128, 256\}$ \\
    Accumulation steps & 1 \\
    \midrule
    Optimizer & AdamW ($\beta_1 = 0.9, \beta_2 = 0.95$) \\
    Learning rate & $0.001$ \\
    Scheduler & Cosine \\
    \# Iterations & $10000$ \\
    Weight decay & $\num{1e-3}$\\
    \midrule
    Dropout & $0$ \\
    Sequence length & Grid-searched in $\{128, 256, 512\}$ \\
    Embedding dimension & Grid-searched in $\{2,4,8,16,32\}$ \\
    Mamba layers & $1$ \\
    Heads & $1$ \\
    Convolution window & Between $2$ and $6$ \\
    \midrule
    Repetitions & $5$\\
    \bottomrule
\end{tabularx}
\end{table}

\begin{table}[h]
\caption{Parameters in the transformer architecture with their shape.}
\label{tab:transformer-parameters}
\vspace{1mm}
\small%
\newcolumntype{R}{>{\raggedleft\arraybackslash}X}
\begin{tabularx}{\linewidth}{Xllr}
\toprule
Parameter
& Matrix shape \\
\cmidrule(lr){1-2}
transformer.wte 
& $2 \times d$ \\
transformer.wpe 
& $N \times d$ \\
transformer.h.ln\_1 $(\times \ell)$
& $d \times 1$ \\
transformer.h.attn.c\_attn $(\times \ell)$
& $3d \times d$ \\
transformer.h.attn.c\_proj $(\times \ell)$
& $d \times d$ \\
transformer.h.ln\_2 $(\times \ell)$
& $d \times 1$ \\
transformer.h.mlp.c\_fc $(\times \ell)$
& $4d \times d$ \\
transformer.h.mlp.c\_proj $(\times \ell)$
& $d \times 4d$ \\
transformer.ln\_f
& $d \times 1$ \\
\bottomrule
\end{tabularx}
\end{table}

\begin{table}[h]
\caption{Settings and parameters for the transformer model used in the experiments.}
\label{tab:transformer-setup}
\vspace{1mm}
\small%
\newcolumntype{R}{>{\raggedleft\arraybackslash}X}
\begin{tabularx}{\linewidth}{lX}
    \toprule
    % Parameter & Default value \\
    % \midrule
    Dataset & $k$-th order binary Markov source \\
    Architecture & Based on the \gptwo architecture as implemented in \cite{pagliardini-llm} \\
    \midrule
    Batch size & Grid-searched in $\{16, 32, 64, 128, 256\}$ \\
    Accumulation steps & 1 \\
    \midrule
    Optimizer & AdamW ($\beta_1 = 0.9, \beta_2 = 0.95$) \\
    Learning rate & $0.001$ \\
    Scheduler & Cosine \\
    \# Iterations & $10000$ \\
    Weight decay & $\num{1e-3}$\\
    \midrule
    Dropout & $0$ \\
    Sequence length & Grid-searched in $\{128, 256, 512, 1024\}$ \\
    Embedding dimension & Grid-searched in $\{4,8,16,32\}$ \\
    Transformer layers & Between $1$ and $2$ depending on the experiment \\
    Attention heads & $1$ \\
    \midrule
    Repetitions & $5$\\
    \bottomrule
\end{tabularx}
\end{table}

