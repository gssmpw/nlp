\section{Related Works}
\subsection{Existing Approaches in MARL}
Multi-agent systems are categorized as cooperative, competitive, or mixed based on agent interactions____. Cooperative MARL, with its potential in applications like path planning and autonomous driving, has garnered significant research interest in recent years____.

Decentralized execution is essential for fully cooperative multi-agent systems in real-world scenarios, making the CTDE paradigm crucial. Some of the typical methods are as follows. VDN____ represents a foundational value decomposition, addressing the ``lazy agent" problem by decomposing the joint action-value function $Q_{tot}$ into a sum of individual $Q$-values $Q_i$, conditioned on local observations. QMIX____ builds on VDN by introducing a hyper-network to enforce monotonicity between $Q_{tot}$ and local value functions, incorporating global state information. QTRAN____ relaxes the additivity and monotonicity constraints, deriving the IGM principle. Qatten____ employs a multi-head attention mechanism for value decomposition. Weighted QMIX____ reduces suboptimal joint action weights to avoid local optima. QPLEX____ extends IGM by using advantage-function consistency constraints and an attention mechanism. RA3____ accelerates training by framing $Q_{tot}$ updates as a fixed-point iterative task. QDAP____ improves cooperation by weighting historical trajectories of agents, addressing the impact of dead agents. QEN____ enhances collaboration through a graph neural network based on Pearson correlation coefficients of agentsâ€™ trajectory similarities.

CTDE-based methods often face challenges from cumulative inherent error, limiting agents' ability to achieve optimal policies. Additionally, inadequate exploration strategies during training can hinder learning, compromising policy robustness and adaptability. For example, Nguyen____ addresses the undervaluation of individual policies and communication complexity by employing a greed-driven approach and an incentive-based communication module to foster cooperation. GDIR____ incorporates intrinsic rewards and organizes learning into ``Go" and ``Explore" phases for continuous learning and faster adaptation. Despite these advances, achieving low-cost, reliable communication remains a significant challenge in MARL, and poorly designed rewards can lead to interference in agent learning.

To address these challenges, we propose a double distillation learning method that not only mitigates inherent error through distillation models but also leverages global state information to generate state-dependent intrinsic rewards. This approach optimizes the decision-making process of agents and enhances the overall performance of multi-agent systems.

\subsection{Knowledge Distillation in MARL}
The key idea in Knowledge distillation____ is to transfer knowledge from a large teacher model to a lightweight student model, facilitating deployment across various tasks____, which meets the need to eliminate inherent error in MARL training. Policy distillation____ first applied this concept to train smaller, more efficient networks for agent policies. While CTDS____ distills policies for decentralized execution by approximating teacher estimates (based on global observations) with partial observations. PTDE____ uses a two-phase approach to distill personalized global information into local agent policies, enabling decentralized execution. IGM-DA____ trains a global expert and decomposes its policies into local observation-based ones through imitation learning. However, these methods are aimed at indirectly incorporating state information, and the training efficiency methods still need to be improved.

Our proposed DDN incorporates two modules: an Internal Distillation Module balancing exploration and exploitation, and an external module focusing on knowledge transfer between decisions and intermediate feature learning, which can use global state information directly and efficiently to improve training efficiency.