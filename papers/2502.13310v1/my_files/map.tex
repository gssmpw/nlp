

Research questions
\begin{itemize}
\item Do we need turn level annotations No
\item Do llms generalize to unseen domain in TOD context Yes
\item Is prompting enough No
\item Do LLMs complete task No
\item with augmentation Yes
\item Is there any benefits of fine-tuning for generalization Yes

\end{itemize}

Introduction
\begin{itemize}
    \item Introduce TOD
    \item Manual Annotation
    \item Limitations of annotations
    \item API calls
    \item SOTA avoid API calls
    \item Pretrained LLMs do generalize
    \item Good in generic domains(alarm, services, weather, media), struggle in complex domains (hotels, ridesharing, trains, events, restaurants)
    \item Our proposals
    \item no annotations required, allows to use existing data
    \item make api calls
    \item Generalizes and handles complex domains
    \item Eval on 2 dataset, LLMs of different sizes, ablation study for augmentation
    \item Results better than SOTA models trained on annotated data and AUTO TOD.
\end{itemize}

Results
Focus more on API calls
Very little discussion on Response Generation
Show domain wise results of nltod with auto tod to show that auto tod struggles on certain domains and nltod is consistent across domains
Ablation study of augmentation, show example of party size and num of people 


Drop?
\begin{itemize}
    \item Mixed
    \item Detailed performance across NL-TOD model with different sizes
    \item Replace with abalation study of manual annotation
    \item Multi domain
\end{itemize}