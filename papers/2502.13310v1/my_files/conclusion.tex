\vspace{-6pt}
\section{Conclusion}
\vspace{-4pt}
% This work introduces a novel zero-shot generalizable task-oriented dialog (ToD) system, {\oursys}, which reduces reliance on manually annotated turn-level data. Such annotations are often costly, error-prone, and inconsistent. Our approach enables researchers and practitioners to utilize the vast amounts of available conversational data to train TOD systems, eliminating the need for extensive manual labeling.
% We consider API calls a core task, as it is crucial for a TOD system to know when and how to interact with external data sources.
% To demonstrate the possibility of training TOD systems effectively without turn-level annotations, we employed multi-task instruction fine-tuning and trained three LLMs of varying sizes.
% To promote zero-shot generalization, we use domain schema as an additional conditioning variable.
% In fine-tuning these models, we experimented with both full-model training and low-rank adaptation techniques. 
% We conducted extensive experiments using three diverse TOD datasets, analyzing overall response and API generation capabilities, as well as performance on sub-tasks.
% Our results show that {\oursys} outperforms OpenAI's GPT models and existing state-of-the-art (SOTA) approaches that rely on turn-level annotated dialogs.


% A ToD system that can engage with users through natural language, reliably complete tasks through accurate API calls, and generalize effectively to out-of-domain scenarios would address critical limitations of current approaches and represent a major step forward in real-world applicability.

%\section{Acknowledgements}

%This work used Delta-GPU at Delta through allocation CIS230196 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services \& Support (ACCESS) program~\cite{boerner2023access}, which is supported by National Science Foundation grants \#2138259, \#2138286, \#2138307, \#2137603, and \#2138296.



% In this work, we explored the feasibility of building TOD systems without annotated data, enhancing out-of-domain generalization, and understanding the role of tine-tuning in TOD models. Our approach formulates TOD as a multi-task instruction learning problem, allowing models to generate responses without requiring turn-level annotations. Experimental results show that this approach not only eliminates the need for manual annotations, but also surpasses traditional systems in performance. To address the challenge of out-of-domain generalization, we introduce a schema augmentation mechanism, that significantly enhances performance in unseen domains by enabling the model to adapt to novel schemas more effectively. Furthermore, our findings highlight the necessity of fine-tuning, as it plays a key role in helping the model decide when to make {\apicall}s and ensures stable performance across both simple and complex domains. Our study demonstrates that TOD models can be designed to operate without annotation dependencies while maintaining strong generalization and robustness, paving the way for more scalable and adaptable dialog systems. 



% In this work, we investigated three fundamental questions in TOD systems. RQ1: Can TOD systems function without annotated data? 
% To answer this, we formulated TOD as a multi-task instruction learning problem, enabling models to generate system responses without requiring turn-level annotations. 
% Our experimental results demonstrate that this approach not only removes the dependency on manual annotations but also outperforms traditional systems. 
% RQ2: Can we improve out-of-domain generalization? 
% To address this, we introduced a schema augmentation mechanism that enhances generalization by allowing models to adapt more effectively to unseen domains through exposure to diverse schema variations. 
% RQ3: Is fine-tuning necessary for TOD models? Our findings highlight the necessity of fine-tuning, as it helps the model determine when to make {\apicall}s and ensures strong performance across both simple and complex domains. 
% Overall, our study demonstrates that TOD models can be designed to function without annotation dependencies while maintaining strong generalization and robustness, paving the way for more scalable and adaptable dialog systems.

\input{my_files/tables/autotod}



This work demonstrates that LLMs fine-tuned solely on natural language dialogs can effectively generalize to unseen domains by framing ToD as a multi-task instruction fine-tuning problem.
To further enhance their {\ood} task completion performance, we introduce schema augmentation, which improves model adaptability to unseen domains and strengthens task completion performance. To ensure robust evaluation of task completion, we explicitly incorporate {\apicall}s as a core task and assess performance using both automatic metrics and human evaluations.
Furthermore, we show that fine-tuned ToD systems generalize better to unseen domains than fine-tuning-free approaches that rely on large-scale proprietary LLMs.
These results highlight the feasibility of developing cost-effective, scalable, and zero-shot generalizable ToD systems that achieve strong {\ood} generalization without requiring turn-level annotations, paving the way for their practical adoption in real-world applications.

%Our results show that fine-tuning not only improves task completion accuracy but also enables smaller, domain-adapted models that outperform larger proprietary LLMs, offering a more effective and cost-efficient alternative for ToD applications.




% In this work, we explored how ToD systems can function without annotated data by framing ToD as a multi-task instruction learning problem. Through schema augmentation, we enhanced the {\ood} generalization of ToD systems, improving their adaptability to unseen domains.
% To ensure effective task completion, we incorporate {\apicall}s as a core task and evaluate performance using automatic metrics. We also compared against fine-tuning-free approaches, and demonstrated that fine-tuning significantly enhances performance in complex domains.
% Our approach presents a scalable and adaptable alternative to traditional ToD models, reducing dependency on manual annotations while maintaining strong generalization and robustness.



% \fix{BLEU scores in appendix, sample dialogs in appendix, full prompt in appendix}