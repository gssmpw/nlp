% old abstract
%Task-oriented dialogue (TOD) systems enable users to achieve their goals through natural language interactions. 
%Traditionally, these systems have relied on turn-level manually annotated metadata, such as dialogue states and policy annotations, which are expensive, time-consuming, and often inconsistent or error-prone. This dependence limits the potential to leverage vast amounts of readily available conversational data for training TOD systems.
%Additionally, a critical challenge in TOD system design is \emph{determining when and how to access and integrate information from external sources}. 
%Current approaches typically expect this information to be provided alongside the dialogue context, rather than learning to identify and retrieve it autonomously.
%While pre-trained large language models (LLMs) have been used to develop TOD systems, their potential to train such systems \emph{without laborious annotations remains largely unexplored}. 
%This work employs multi-task instruction fine-tuning to create more efficient and scalable TOD systems that can effectively leverage natural language conversational data without manual annotations, while autonomously managing external information retrieval.
%Our extensive experimental evaluations, using two TOD datasets and three LLMs of varying sizes, demonstrate that our approach can generalize to new, unseen domains. 
%Notably, our approach outperforms both state-of-the-art models trained on annotated data and billion-scale parameter off-the-shelf ChatGPT models.
% end old abstract

% The capabilities of LLMs in single-turn tasks (e.g., classification, prompt completion) are well-studied. 
% LLMs' capabilities are not fully evaluated in the context of ToD systems, where multiple intelligent sequential decisions are needed for success. 
% While traditionally, ToD systems are trained using turn-level annotated dialog data, this work demonstrates that customizing LLMs without turn-level annotations is feasible by appropriately reframing the task. Specifically, we investigate how well LLMs fine-tuned using raw natural language dialogs without turn-level annotations generalize to new unseen domains as compared to baseline approaches that leverage turn-level annotated data.
% Based on our extensive experimental evaluations using three well-known open-source LLMs of varying sizes and two diverse ToD datasets, we discovered that while such LLMs can generate plausible natural language responses in ToD contexts, they cannot still complete the desired tasks by making accurate API calls, particularly in unseen domains. To address this, we introduce a schema augmentation mechanism, which significantly enhances LLMs' performance in making accurate API calls in unseen domains.
% Finally, we investigate whether fine-tuning-free approaches (e.g., prompting off-the-shelf LLMs) are sufficient in ToD systems, or whether our proposed approach can enable LLMs to better generalize to unseen domains in comparison with expensive large-scale proprietary LLMs.
%better than fine-tuning-free approaches (RQ3)? 

% We did not mention NLTOD in the abstract.
% just by reformulating the problem, LLMs can work without turn-level annotations and generalize to unseen domains.



%Large language models (LLMs) have demonstrated remarkable capabilities in single-turn tasks such as classification and prompt completion. However, their potential in task-oriented dialog (ToD) systems -- where success requires making multiple intelligent, sequential decisions -- remains underexplored. 
% Traditional task-oriented dialog (ToD) systems rely heavily on labor-intensive, turn-level annotated dialog data, such as dialogue states and policy annotations, for training. This work demonstrates that large language models (LLMs) can be customized for ToD tasks without such annotations by appropriately framing the problem. Specifically, we evaluate the generalization capabilities of LLMs fine-tuned on natural language dialogs (without turn-level annotations) to new, unseen domains and compare them with baseline approaches trained on turn-level annotated data.
% Through extensive experiments with three widely used open-source LLMs of varying sizes and two diverse ToD datasets, we observe that while LLMs fine-tuned without turn-level annotations can generate coherent and contextually appropriate turn-level responses, they struggle to complete user tasks requiring {\apicall}s, especially in unseen domains.
% To overcome this limitation, we introduce {\oursys}, a framework that incorporates a schema augmentation mechanism that significantly improves {\apicall} accuracy and enhances task completion performance, particularly in out-of-domain scenarios. 
% Furthermore, we evaluate {\oursys} against fine-tuning-free alternatives, such as prompting off-the-shelf LLMs. 
% Our empirical results show that {\oursys} enables smaller, fine-tuned models to achieve superior generalization to unseen domains and surpass large-scale, expensive proprietary LLMs in task completion rates. 
% We also conduct human studies, which verify empirical results.
% These findings suggest the feasibility of developing cost-effective, scalable, and zero-shot generalizable task-oriented dialog systems for real-world applications.
%These findings highlight the generalizability, scalability, and cost-effectiveness of {\oursys} for real-world task-oriented dialog systems.


Traditional task-oriented dialog (ToD) systems rely heavily on labor-intensive turn-level annotations, such as dialogue states and policy labels, for training. 
This work explores whether large language models (LLMs) can be fine-tuned solely on natural language dialogs to perform ToD tasks, without requiring such annotations. We evaluate their ability to generalize to unseen domains and compare their performance with models trained on fully annotated data.
Through extensive experiments with three open-source LLMs of varying sizes and two diverse ToD datasets, we find that models fine-tuned without turn-level annotations generate coherent and contextually appropriate responses.
However, their task completion performance -- measured by accurate execution of {\apicall}s -- remains suboptimal, with the best models achieving only around 53\% success in unseen domains.
To improve task completion, we propose {\oursys}, a framework that incorporates a schema augmentation mechanism to enhance {\apicall} accuracy and overall task completion rates, particularly in out-of-domain settings. 
We also compare {\oursys} with fine-tuning-free alternatives, such as prompting off-the-shelf LLMs, and find that our framework enables smaller, fine-tuned models that outperform large-scale proprietary LLMs in task completion. 
Additionally, a human study evaluating informativeness, fluency, and task completion confirms our empirical findings. 
These findings suggest the feasibility of developing cost-effective, scalable, and zero-shot generalizable ToD systems for real-world applications.
