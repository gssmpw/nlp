\vspace{-9pt}
\section{Results}
\vspace{-3pt}


% \fix{I do not see proper captions of tables/tables.}

\vspace{-5pt}
\noindent
% \textbf{Performance without turn level annotations.}
% \begin{tcolorbox}[colframe=gray!20, colback=gray!10, coltitle=black, boxrule=0.4mm, width=\columnwidth, left=1mm, right=1mm, top=1mm, bottom=1mm]
% RQ1: Can TOD systems operate without annotated data? 
% \end{tcolorbox}
\noindent
%Table~\ref{tab:turn_annotations} presents the findings related to RQ1: Can TOD systems operate without annotated data? 
Table~\ref{tab:turn_annotations} presents the findings for \textit{RQ1: Can pre-trained LLMs be adapted into effective ToD systems without turn-level annotated data ?}
Our results show that {\oursys} models, which do not rely on turn-level annotations, outperform models trained with annotated data in response generation.
A key reason for this improvement is that {\oursys} models focus solely on generating system responses, whereas annotation-based models must produce structured outputs that include dialog state, system actions, and responsesâ€”requiring the model to optimize for multiple complex tasks simultaneously. 
Furthermore, the substantial performance gap between the baseline approaches built with {\gpt} and the {\gpt} variant of {\oursys} suggests that learning to generate responses directly is a more effective approach for ToD systems.

For task completion, all models trained without turn-level annotations consistently outperform the annotated models. This finding highlights the sufficiency of dialogue history as a standalone source of context for completing complex tasks. 
Table~\ref{tab:turn_annotations} reveals more insights about the different {\oursys} models.  {\flan} and {\llamai} being the larger models, significantly outperform the smaller {\gpt} model for task completion. 
However, even though {\llamai} is a larger model than {\flan}, it does not have better task completion performance. 

This discrepancy may stem from differences in the training methodologies. Specifically, {\llamai} was trained using 8-bit quantization and LoRA adapters, whereas {\flan} underwent full fine-tuning. The use of LoRA significantly reduces the number of trainable parameters and the 8-bit quantization introduces precision loss due to the reduced bit width. These factors likely contributed to {\llamai}'s lower performance despite its larger model size.

% \fixed{: Added text about lora and 8bit. }\fix{above passage:
% Size of llama? is it lora? also 8-bit?}

\input{my_files/tables/schema_augmentation}



\noindent
\textbf{Detailed Response Generation Performance.}
To get a better understanding of the response generation task, we break it down into two sub-tasks---Inform and Request---and present the results in Table~\ref{tab:additional_response}. The inform sub-task focuses on providing responses to user requests, while the request sub-task involves prompting users for additional information. 
Similar to Table~\ref{tab:turn_annotations}, the results here show a consistent trend, with {\oursys} models outperforming those trained on annotated data.
Additionally, we observe that the Request sub-task is significantly more challenging than Inform. This is expected, as there are multiple plausible pieces of information a system could request, but if they do not align with the gold standard, the model receives a lower score. In contrast, the Inform sub-task is more straightforward since the user explicitly requests specific information, making it easier for the system to generate the correct response.

\noindent
\textbf{Schema Augmentation Performance.}
Table~\ref{tab:augmentation} presents the results for \textit{RQ2: How can we improve the {\ood} generalization of ToD systems for task completion??} 
% It compares the performance of {\oursys} models with the schema augmentation mechanism. 
Across all the models, we can see that the response generation performance is similar, but there are improvements in task completion performance, specially a big increment in the unseen domain. For seen domains, there is a small improvement, which is expected as the augmentation mainly teaches the models how to use the schema to generalize to {\ood} data, however for unseen domains, this learning is very useful and the models have shown considerable improvements.
Between {\llamai} and {\flan}, we can see that for seen domains {\llamai} has a slightly better performance however for unseen domains {\llamai} has much lower performance. One reason for this could be the size of the two models, {\llamai} being the larger model may have a higher capacity to memorize the training data, which could explain its stronger performance on seen domains. However, this can also make it more prone to over-fitting and may not generalize well to new, unseen domains.


\input{my_files/tables/api_metrics}

\noindent
\textbf{Detailed Task Completion Performance.}
To complete a task, a model has to make a correct {\apicall}. An {\apicall} has many aspects in it, and we present detailed results in Table~\ref{tab:api_metrics}. We can see that {\oursys} models considerably outperform baseline approaches across all metrics. Upon inspecting the {\apicall} Invoke Accuracy, we see that baseline approaches have much lower scores, indicating that they struggle in identifying when to make {\apicall}s. The {\apicall} Method Accuracy evaluates whether a model generates the correct method name in the \apicall. A common pattern that we see across all models is that there is a drop in parameter names accuracy when compared to the previous metrics. Generating the correct list of parameters for the {\apicall} is inherently a harder problem than deciding when to make an {\apicall} and what method to use, so the performance degradation is understandable. 

A key observation from Table~\ref{tab:api_metrics} is the significant impact of the schema augmentation on the {\apicall} parameter names metric. Our results indicate that schema augmentation yields the largest improvement for this metric. {\apicall} parameters are directly derived from the schema, and schema augmentation enables the models to better recognize and utilize these patterns, thus improving the model's ability to generate the correct list of parameters, leading to a notable increase in parameter names accuracy. Furthermore, the {\apicall} parameter values accuracy also improved as a result, since a model is only rewarded for generating the correct value if it is assigned to the appropriate parameter name. 

For instance, consider the task of finding a bus using the \texttt{FindBus} method. We compare two schema variations, \texttt{Buses\_1} and \texttt{Buses\_11}, which define different slot names for the same concepts. In \texttt{Buses\_1}, the slot names are \texttt{from\_station} and \texttt{to\_station}, and for \texttt{Buses\_11}, the slot names are \texttt{origin} and \texttt{destination}.

A model trained without schema augmentation tends to overfit to specific slot names seen during training. If the model was trained on \texttt{Buses\_1}, it might always generate \texttt{from\_station} and \texttt{to\_station}, even when interacting with \texttt{Buses\_11}, leading to incorrect {\apicall}s.
For example, given the user utterance: \textit{``I want to find a bus from LA to SFO''}, the model without augmentation might generate: 
\(
\begin{aligned}
    &\quad\texttt{\apicall(method=FindBus, parameters=} \\
    &\quad \quad \texttt{from\_station=LA, to\_station=SFO \}).}
\end{aligned}
\)
% $$
% \texttt{\apicall(method=FindBus, parameters=\{from\_station=LA, to\_station=SFO \}).} 
% $$
In the \texttt{Buses\_11} schema, the slot names \texttt{from\_station} and \texttt{to\_station} do not exist, thus making the {\apicall} invalid.

On the other hand, a model trained with schema augmentation learns to generalize across schema variations by recognizing slot name patterns from multiple schemas, and might generate:
\(
\begin{aligned}
    &\quad\texttt{\apicall(method=FindBus, parameters=} \\
    &\quad\quad\texttt{origin=LA, destination=SFO\}).} 
\end{aligned}
\)
% $$
% \texttt{\apicall(method=FindBus, parameters=\{departure\_station=LA, arrival\_station=SFO\}).}
% $$

The model can dynamically align its output with the schema it is conditioned on. By learning to use the slot names from the provided schema rather than relying on the memorized slot names, a model trained with schema augmentation demonstrates improved robustness and generalization.

% \fixed{: Added example with schema variations. }\fix{The above needs a better explanation about generalization through schema. Also, we should not bring Washington, D.C. and DC (a simple look-up can fix it.)}

\noindent
\textbf{Fine-tuning Performance.}
% Table~\ref{tab:autotod_results} presents the results on unseen domains for {\autotod} and {\oursys} models with schema augmentation. 
% Table~\ref{tab:autotod_results} presents the results for \textit{RQ3: How does the {\ood} generalization of fine-tuned ToD systems compare to that of large-scale, proprietary LLMs?} by comparing the performance of {\oursys} models against {\autotod}, which was built using GPT-4o.
Table~\ref{tab:autotod_results} presents the results on unseen domains for {\oursys} models, and {\autotod}, which was built using GPT-4o. Using the results in Table~\ref{tab:autotod_results}, we can answer \textit{RQ3: How does the {\ood} generalization of fine-tuned ToD systems compare to that of large-scale, proprietary LLMs?}
For the Complete API Accuracy metric, except for the {\gpt} model, all other {\oursys} models outperform {\autotod}. For all the other metrics, {\autotod} has much lower scores than the {\oursys} models. 
A key metric to note here is the API Invoke Accuracy, which measures whether a model is making an API call on the right turn, and {\autotod} has a very low score on this metric when compared to {\oursys} models.
Due to this issue, {\autotod} also has a much lower score for the Overall Response metric, as it makes \apicall s on turns where a general interaction is expected. Based on these results, we can state that fine-tuning is an important step to identify the timing of making an {\apicall} in ToD systems.


\input{my_files/tables/domain_wise}

\noindent
\textbf{Domain Specific Results.}
To get a deeper understanding of the performance of {\autotod} and {\oursys} models, we present some domain specific results for the API Invoke Accuracy and Complete API Accuracy metrics in Table~\ref{tab:unseen_domain_results}. For the Api Invoke Accuracy, we see the same pattern as before, with {\autotod} having much lower scores than {\oursys} models. 
From these results, we can make another interesting observation, {\autotod} has higher Complete Api Accuracy for simple domains like \texttt{Alarm} and \texttt{Movies}, however it has poor performance for complex domains like \texttt{Restaurants}, \texttt{Buses}, and \texttt{Music}. 
Since {\oursys} models have been fine-tuned, the models have a better understanding of the structure of complex domains. The models do not have a big drop in performance across domains, showing the robustness achieved through fine-tuning. {\flan} and {\llamai} being the larger models, show more stability in performance over the smaller {\gpt} model. 


\vspace{-5pt}
\begin{figure}
   \centering
   \includegraphics[width=0.95\linewidth]{assets/overall_scores.pdf}
   
   \caption{
       Human Evaluation Study on SGD and KETOD. Evaluators were asked to rate the dialog samples between a range of 1-5 on 3 categories.
   }
   \vspace{-15pt}
   \label{fig:human_evaluation}
\end{figure}
\vspace{-2pt}

\vspace{3pt}
\noindent
\textbf{Human Evaluation.} 
To supplement the automatic metrics and get a qualitative analysis, we conducted a human evaluation using Amazon Mechanical Turk to assess the performance of various models. Two baseline models ({\soloist} and {\autotod}) and three {\oursys} models ({\gpt}, {\llamai}, and {\flan}) were taken into account. We sampled 100 dialogs from each dataset, with 50 coming from single-domain tasks and the remaining 50 from multi-domain tasks, all from the test dataset. Human evaluators were asked to rate the models on a scale from 1 to 5 on three questions: the accuracy of information presented in the responses (Informativeness), how fluent and natural the conversation is (Fluency), and whether the models can make accurate {\apicall} (Task Completion).

The results, shown in Figure~\ref{fig:human_evaluation}, align with the automatic metrics, where {\oursys} models outperform the existing SOTA approaches. This demonstrates a strong alignment between quantitative and qualitative assessments. Notably, for task completion and fluency, {\llamai} and {\flan} demonstrate superior performance compared to all other models, which is consistent with our previous findings. Another important observation is that {\llamai} and {\flan} have less variance in performance across all tasks when compared to all other models, which further solidifies the robustness of our approach. 

% \fix{Should we not have more to say in human study? we do not even discuss each metric individually.
% which dataset these results belong to?}

