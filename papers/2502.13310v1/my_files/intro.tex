\section{Introduction}

% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=0.97\linewidth]{assets/NL_ToD.pdf}
%     \vspace{-6pt}
%     \caption{
% This work leverages natural language conversational data alone, eliminating the need for extensive manual annotations required by existing works.
% It achieves generalization to new, unseen domains through the use of domain schemas.}
%     \label{fig:approach}
%     \vspace{-14pt}
% \end{figure}

Task-oriented dialog (ToD) systems~\cite{zhang2020task} enable users to accomplish diverse tasks through natural language interactions. These systems power virtual assistants, customer service chatbots, and various other applications such as making reservations or scheduling appointments~\cite{Williams2016TheDS, Zhang2019TaskOrientedDS}. 
To be effective, ToD systems must not only engage in user interactions to collect and provide task-specific information but also interface with external systems to accurately complete user tasks.
%To be effective, ToD systems must engage with users, understand task-specific intents, and take appropriate actions to fulfill user requests.


Traditionally, the development of ToD systems has relied heavily on turn-level manually annotated conversational data, where natural language turns are labeled with dialog states and policy actions~\cite{Zhang2020RecentAA}. However, this \emph{reliance on turn-level annotated data limits the scalability of ToD systems}, as it prevents them from fully leveraging the vast amounts of readily available unannotated task-oriented conversational data. Furthermore, the annotation process is labor-intensive, expensive, and prone to inconsistencies and errors~\cite{eric2020multiwoz,zang2020multiwoz,han2021multiwoz,budzianowski2019challenges}.


Recent advancements in natural language processing, particularly the emergence of pre-trained large language models (LLMs)~\cite{vaswani2017attention,devlin2019bert,radford2019language}, offer new opportunities to address these scalability challenges. LLMs have demonstrated remarkable capabilities in diverse language tasks, from understanding context to generating coherent responses. 
While pre-trained models (e.g., GPT-2) have been employed to develop ToD systems~\cite{hosseini2020simple,Yang2020UBARTF,Mosharrof2023ZeroShotGE,budzianowski2018towards}, their potential to train ToD systems without turn-level annotations remains largely unexplored, as does their \emph{ability to generalize effectively to unseen domains}. %Evaluating and improving {\ood} generalization of ToD systems is critical for their real-world deployments.
%, as predefined domains and intents cannot be guaranteed in real-world settings.


%In addition to engaging with users through natural language interactions -- such as requesting task-specific information or providing updates -- a critical aspect of ToD systems is their ability to interact with external systems (e.g., databases) to ensure successful task completion. 
Beyond natural language interactions -- such as requesting task-specific information or providing updates -- ToD systems must also interact with external systems (e.g., databases) to ensure successful task completion.
This often requires retrieving information or executing actions, such as making a reservation via \emph{an \apicall}. 
While ToD systems described in the literature could, in theory, be trained to make such {\apicall}s, this capability \emph{is rarely evaluated in practice}. The lack of rigorous evaluation in this area leaves a significant gap in understanding the readiness of current ToD systems for real-world deployments.


% v1 intro last para

% To overcome the aforementioned challenges, in this paper we propose a TOD approach that enables task completion without requiring additional turn-level annotations, reducing the need for manual data labeling while maintaining strong performance. We employ a multi-task instruction fine-tuning, where the model jointly learns to generate system responses and {\apicall}s, enabling models to learn task completion in a more unified manner. To address the key challenge of poor generalization to unseen domains, we introduce a schema augmentation mechanism by enriching the training data with diverse schema variations, that enhances the ability of models to adapt to new domains. Additionally, we explore whether fine-tuning is necessary for TOD systems by comparing fine-tuned models against fine-tuning-free approaches.


% v2 intro last para

% In this paper, we investigate three fundamental questions in TOD systems: 
%Motivated by the need to evaluate and improve {\ood} generalization of ToD systems, in this work, we investigate three fundamental research questions:

Motivated by the need to evaluate and enhance the {\ood} generalization of ToD systems, this work investigates three research questions:

\noindent
\textbf{RQ1:} Can pre-trained LLMs be adapted into effective ToD systems without turn-level annotated data (e.g., annotated dialog states)?
%ToD systems function effectively ?
%Can ToD systems function effectively without turn-level annotated data?

\noindent
\textbf{RQ2:} How can we improve the {\ood} generalization of ToD systems for task completion?

\noindent
\textbf{RQ3:} How does the {\ood} generalization of fine-tuned ToD systems compare to that of large-scale, proprietary LLMs?
%Is fine-tuning still necessary for ToD systems?


To address RQ1, we frame ToD as a multi-task instruction fine-tuning problem, where the model learns to generate both natural language responses and {\apicall}s by conditioning on the dialog history and domain schema. 
To enhance task completion performance, we introduce a schema augmentation mechanism that enriches training data with diverse schema variations, significantly improving robustness in unseen domains (RQ2). Finally, to investigate RQ3, we compare fine-tuned ToD systems against fine-tuning-free approaches that rely on large-scale, proprietary LLMs, which are often costly and less controllable.


We conduct extensive experiments on two benchmark ToD datasets -- SGD~\cite{Rastogi2019TowardsSM} and KETOD~\cite{Chen2022KETODKT} -- using three open-source models: {\gpt}\cite{radford2019language}, {\llamai}, and {\flan}~\cite{Chung2022ScalingIL}. To provide a comprehensive evaluation, we introduce multiple metrics to assess {\apicall} generation, including method name accuracy, parameter correctness, and complete {\apicall} accuracy. For response generation, we use BERTScore~\cite{Zhang2019BERTScoreET} to better capture the semantic similarity between system outputs and ground truth responses. Additionally, we conduct human studies and qualitative analyses on a subset of both datasets to complement automatic evaluations.

% We conduct extensive experiments on two benchmark ToD datasets -- SGD~\cite{Rastogi2019TowardsSM} and KETOD~\cite{Chen2022KETODKT} -- using three open-source model architectures: {\gpt}~\cite{radford2019language}, {\llamai}, and {\flan}~\cite{Chung2022ScalingIL}.
% To provide a comprehensive evaluation of ToD systems, we introduce multiple metrics to assess different aspects of {\apicall} generation, including method name accuracy, parameter correctness, and overall {\apicall} accuracy.
% For response generation, we incorporate BERTScore~\cite{Zhang2019BERTScoreET} to better capture the semantic similarity between system outputs and ground truth responses. 
% We also perform human studies and conduct a qualitative analysis of the outputs of all models in {\oursys} and various baseline models on a subset from both datasets.
%To get a qualitative analysis of the model outputs, we perform a human evaluation by selecting a random subset of dialogs from both datasets.

% Our results demonstrate that the proposed approach enables TOD systems to operate without manually annotated data, making them more scalable and cost-efficient. 
% The schema augmentation mechanism improves the out of domain generalization ability of models, and enhances the robustness and adaptability across diverse tasks. 
% Furthermore, our experiments show the importance of fine-tuning, as it teaches the model when to make {\apicall}s and also helps to maintain strong performance on complex task scenarios.

% \fix{we need number in the following passage.}

% Our results show that .... 
% Specifically, .... numbers

%Our results provide clear answers to the research questions posed in this study. 

Our empirical results provide clear answers to the research questions posed in this study. For RQ1, we compare our approach against state-of-the-art (SOTA) methods that rely on annotated data and find that ToD systems can function effectively without manual annotations by leveraging multi-task instruction fine-tuning. 
On the complete API accuracy metric, our best model improves by an average of 62.9\% across both datasets compared to the strongest baseline SOTA model trained with turn-level annotated data.
For RQ2, we evaluate the impact of schema augmentation by comparing models trained with and without this mechanism. Our results show that augmentation significantly enhances {\ood} generalization, improving complete API accuracy on unseen domains by 17.05\% for {\flan} and 35.6\% for {\llamai} compared to their non-augmented counterparts.


For RQ3, we compare {\oursys} against fine-tuning-free alternatives in unseen domains and confirm that fine-tuning is advantageous for learning when to make {\apicall}s and maintaining strong {\ood} performance in complex, multi-turn task completion scenarios. 
On complete API accuracy for unseen domains, {\flan} achieves an average improvement of 30.45\% over the best SOTA approach built with the large-scale GPT-4o model.
Furthermore, human study results evaluating informativeness, fluency, and task completion closely align with automatic metrics, confirming our empirical findings. 
%These results highlight the feasibility of developing cost-effective, scalable, and zero-shot generalizable ToD systems that achieve strong OOD generalization without requiring turn-level annotations, paving the way for their practical adoption in real-world applications.



% % For RQ1, we compare our approach against SOTA approaches that work with annotated data, and our experiments reveal a \textbf{62.9\%} average improvement across both datasets for our best model against the best SOTA approach in terms of API accuracy \fix{complete api call, unseen}. This demonstrates that TOD systems can function effectively without manually annotated data by leveraging multi-task instruction learning, making them more scalable and cost-efficient.
% % For RQ1, we compare our approach against SOTA approaches that work with annotated data. Our experiments reveal that ToD systems can function effectively without manually annotated data by leveraging multi-task instruction learning. For the Complete API Accuracy metric, we see an average improvement of \textbf{62.9\%} across both datasets for our best model against the best SOTA approach.
% % In response to RQ2, our schema augmentation mechanism significantly improves {\ood} generalization, enhancing the model's robustness and adaptability across diverse taks, particularly for the API accuracy on unseen domains, we observed an improvement of \textbf{17.05\%} with {\flan} and \textbf{35.6\%} with {\llamai} \fix{metric, dataset, competing model?}.
% For RQ2, we compare models with and without schema augmentation and see that augmentation significantly improves {\ood} generalization, enhancing the model's robustness and adaptability across diverse tasks.
% For the Complete API Accuracy metric on unseen domains, we observed an average improvement of \textbf{17.05\%} with {\flan} and \textbf{35.6\%} with {\llamai} on both datasets.
% % Finally, for RQ3, we compare our approach against SOTA approaches that are fine-tuning free, and our experiments confirm the necessity of fine-tuning, as it enables the model to learn when to make {\apicall}s and helps to maintain strong performance in complex task scenarios. In terms of API accuracy, {\flan} outperformed the best SOTA approach \fix{large-scale LLM, chatgpt} by an average of \textbf{30.45\%} across both datasets.
% For RQ3, we compare {\oursys}, which is fine-tuned against approaches that are fine-tuning free. Our experiments confirm the necessity of fine-tuning, as it enables models to learn when to make {\apicall}s and helps to maintain strong performance in complex task scenarios. For the Complete API Accuracy metric on unseen domains, we observed that across both datasets {\flan} had an average improvement of \textbf{30.45\%} over the best SOTA approach built with large-scale LLMs like GPT-4o.
% The the human study results closely align with the automatic metrics, reinforcing the reliability of the quantitative measures. These results demonstrate the potential for building scalable, cost-efficient, and annotation-free ToD systems with improved {\ood} generalization. 

% % \fix{2-3 lines about main findings from user study}


% % \fixed{}\fix{no mention of baselines in rq1; rq3 does not discuss prompting-based method.}