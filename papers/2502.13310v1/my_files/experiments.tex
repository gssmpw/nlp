\vspace{-3pt}
\section{Experimental Setup}
\vspace{-4pt}

% \noindent
% \textbf{Datasets.}
\subsection{Datasets}
We use two ToD datasets: Schema-Guided Dialog (SGD) dataset, and Knowledge-Enhanced Task-Oriented Dialog (KETOD) dataset. 
Table~\ref{tab:data_statistics} shows detailed statistics about the datasets.
These datasets are publicly available, large, and represent a wide range of domains that span different tasks.
We have selected these datasets as they describe the domain using schema and have the necessary information to simulate communication with external resources through {\apicall}s.

\input{my_files/tables/data_stats}
% \noindent
% \textbf{Evaluation.}
\vspace{-3pt}
\subsection{Evaluation}

We evaluate the system across four domain categories: \textit{All Domains} (dialogs from all domains), \textit{Seen Domains} (dialogs from training domains), \textit{Unseen Domains} (dialogs from domains not included in the training data), and \textit{Mixed Domains} (dialogs with both seen and unseen domains).
% To better understand the generalization ability of {\oursys}, we evaluate the system on dialogs grouped by domain categories. This evaluation is structured into for categories: All, Seen, Unseen, and Mixed.
% \textit{All Domains} includes dialogs from all domains, providing a comprehensive evaluation of the system's overall performance.
% \textit{Seen Domains} contains dialogs from domains that were present in the training data. This setting demonstrates the supervised learning performance.
% \textit{Unseen Domains} comprises of dialogs from domains that were not included in the training data. This setting demonstrates the out-of-domain generalization performance.
% \textit{Mixed Domains} consists of dialogs that have multiple domains, where some domains are in seen and some in unseen. This setting serves as an intermediary between supervised and out-of-domain setting. It tests the system's ability to manage dialogs where known and unknown domains are intermixed, reflecting more complex real-world scenarios.
We analyze the performance of overall responses as well as its sub-tasks---Request and Inform. For task completion, we introduce custom metrics to assess the performance of individual components.
% We report scores across four domain categories: \textit{All}, \textit{Seen}, \textit{Unseen}, and \textit{Mixed}. \textit{All Domains} includes dialogs from all domains, offering a comprehensive performance evaluation. \textit{Seen Domains} consists of dialogs from training domains, reflecting supervised learning performance, while \textit{Unseen Domains} contains dialogs from domains not included in the training data, testing out-of-domain generalization. Mixed Domains features dialogs with both seen and unseen domains, testing the system's ability to manage mixed real-world scenarios.
% We evaluate {\oursys} on its overall responses as well as its performance on the sub-tasks—Request and Inform—using custom metrics to assess task completion and component performance. 




% \fixed{: I have moved it to the correct position. }\fix{I do not see metrics here.}
\input{my_files/tables/turn_annotations}

\vspace{4pt}
\noindent
\textbf{Response Generation.}
To evaluate the quality of the response generation of models, we report BERTScore.
We used \texttt{microsoft/mpnet-base} as the model type for calculating the BERTScore. We report BLEU-4~\cite{Papineni2002BleuAM} scores in Appendix~\ref{sec:appendix_bleu}


% \fixed{: Reorganized some text. }\fix{I think, the next items are old, please revise.}
\noindent
\textbf{{\apicall}s.} 
The format for an {\apicall} is:
\(
\texttt{APICall}( \texttt{method=method\_name, parameters} = \{ (s_i,v_i)_{i=1}^n \} ).
\)
The parameters attribute is a list of slot name and slot value pairs, where $s_i$ represents the slot name and $v_i$ represents the value of that slot. 

We use regular expressions to extract different parts of the {\apicall}, and apply custom metrics to access different parts of an {\apicall}.

\textit{Invoke Accuracy} measures whether the system can understand when to make an \apicall.
%metric ensures that the correct type of API query was performed by verifying the query type. This metric also measures whether the system can understand when to make an API call.    
 \textit{Method Accuracy} checks whether the appropriate method name was used in the \apicall.      
 \textit{Param Name Accuracy} assesses whether all the parameter names used to construct the {\apicall} are accurately.
 \textit{Param Value Accuracy} evaluates whether each parameter value corresponding to a parameter name is correct. It is important to note that this metric will only be considered if the corresponding parameter name is correct. 
 %If the parameter name is incorrect, the parameter value accuracy is scored as 0, even if the value itself was correct. Instead of performing an exact string match, fuzzy string matching is performed.
 \textit{Complete {\apicall} Accuracy} metric checks whether the complete {\apicall} (i.e., all components) was generated correctly.

\input{my_files/tables/response}

% \noindent
% \textbf{Baselines.}
\vspace{-5pt}
\subsection{Baselines}
\vspace{-3pt}
% To measure the effectiveness of our approach, we compare {\oursys} against popular SOTA approaches. 
% It is important to note that, apart from {\autotod}, all other approaches use annotated data. Existing approaches do not report {\apicall} metrics, so we implemented them to the best of our ability to report the metrics. {\soloist}, {\simpletod} and {\zstod} were all implemented using a GPT-2 Medium model. For these approaches, during inference we extract the system response from the generation and disregard additional information like the dialog state and system actions.
\noindent
\textit{\soloist}~\cite{Peng2021SoloistBT} introduced an E2E ToD system that employs a transformer-based autoregressive model that generates dialog responses grounded in user goals and real-world knowledge for task completion.

\noindent
\textit{SimpleTOD}~\cite{Chen2022KETODKT} introduced a ToD model as an end-to-end sequence generation problem that utilizes the dialog history, dialog states and system actions to generate system responses.

\noindent
\textit{ZS-TOD}~\cite{Mosharrof2023ZeroShotGE} introduced a zero-shot generalizable E2E ToD model that incorporates domain schema and dialog annotations to generate dialog responses.

\noindent
\textit{AutoTOD}~\cite{Xu2024RethinkingTD} introduced a zero shot autonomous ToD agent, that works without manual annotations and also has the ability to communicate with external resources.

{\soloist}, {\simpletod} and {\zstod} were implemented using GPT-2 Medium. During inference, we extract the system response and disregard the additional information like dialog state and system actions.