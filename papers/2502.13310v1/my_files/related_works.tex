\vspace{-8pt}
\section{Related Work}
\vspace{-7pt}

\textbf{Pipeline Approaches.}
ToD systems have traditionally been designed as pipeline systems, where separate components for Natural Language Understanding (NLU), Dialog State Tracking (DST), Dialog Policy, and Natural Language Generation (NLG) are used to handle specific parts of the dialog processing~\cite{ren2018towards, lee2013structured,peng2018deep,le2021predictable,wen2015semantically,peng2020few,chen2019semantically,budzianowski2018towards,Mosharrof2023TowardOS}. However, this approach has drawbacks like error propagation, where errors made in early stages adversely effect modules later on in the pipeline. 

\noindent
\textbf{End-to-End Approaches.}
Recent works have shifted towards E2E learning methods, where the ToD task is formulated as a conditional generation, where the model generates responses based on the entire dialog history and other relevant annotations (e.g., DST)~\cite{hosseini2020simple,Lin2021LeveragingSD,Bang2023TaskOptimizedAF,Zhang2023EnhancingPO,Ham2020EndtoEndNP,Chung2023InstructTODSLL,Yang2020UBARTF,Sun2022BORTBA,Imrattanatrai2023EndtoEndTD,Sun2022MarsSC,Zhao2022AnyTODAP,Peng2021SoloistBT,Mosharrof2023ZeroShotGE,Siddique2022PersonalizingTD}. 
% For example, T5DST \cite{Lin2021LeveragingSD} was introduced a slot description enhanced generative approach for zero-shot ToD. 
% \citet{Zhang2023EnhancingPO} developed FiD-ToD that employs a caching mechanism for the dialog annotations, which can be extracted using a retrieval module.
A major drawback of these approaches is the dependency on manually annotated data, thus limiting the usage of the wealth of available data. Additionally, most of these approaches assume that {\apicall} results are included in the annotated data, thus limiting their ability to evaluate task completion.

\noindent
\textbf{Prompting Approaches.}
Another recent research direction in ToD systems is in-context learning, where pre-trained LLMs are adapted to specific domains based on contextual examples without requiring fine-tuning~\cite{Labruna2023UnravelingCA,Hudevcek2023AreLL,Dingliwal2021FewSD, Madotto2020LanguageMA, Li2022ControllableDS, Madotto2021FewShotBP,Xu2024RethinkingTD}.
Even though these approaches show promise on generic domains, they fail on complex domains, and have specialized structure or requirements. 

% Most works in all the categories require turn-level annotated data to train TOD models, which can be a significant limitation to scale in real world where such data may be scarce or costly.
% Moreover, most systems assume that knowledge from external sources will be provided in the dialog context.
% \emph{Our work, in contrast, focuses on training TOD models using only readily available natural language interactions, while leveraging domain schema for {\ood} generalization.}

% \noindent
% \textbf{Graph Based Approaches.}
% Additionally, some works used graph-based methods to model the flow of dialog, e.g., by representing DST and policy decisions as graphs~\cite{Sohn2023TODFlowMT, Tuan2022TowardsLI, He2023KRPDSAK}. 
% For instance, \citet{He2023KRPDSAK} introduced KRP-DS, a system that incorporates a knowledge graph as external information through a dedicated knowledge module, enabling context-aware path reasoning.
% This graph assists in knowledge prediction, which is used to enhance response generation. 
% \cite{Sohn2023TODFlowMT} introduced the TOD-Flow graph, where a graph is created from the dialog data and annotations, which uncovers the underlying task structure. 
% Unfortunately, these approaches introduce the overhead of building additional modules which makes the process more complex and hard to generalize.