\section{Related work}
\subsubsection{Dynamic graph algorithms}

Dynamic graph algorithms constitute a deep and well-studied research topic, 
as was recently presented in the thorough survey of Hanauer, Henzinger and Schulz~\cite{HanauerHS22}.
The most recent dynamic algorithm for connectivity is by
Huang, Huang, Kopelowitz, Pettie and Thorup~\cite{HuangHKPT23}, and has $\tu=O(\log n \log^2\log n)$ amortized update time and $\tq=O(\log n/\log\log\log n)$ time per query.
Small subgraph counting was recently studied by Hanauer, Henzinger and Hua~\cite{HanauerHH22}, 
who showed, e.g., that counting \paths st3 and $s$-triangles (triangles through a given node $s$) can be done with $\tu=O(n)$ and constant query time 
(see also \cref{sec:poly lbs} below).
Kara, Nikolic, Ngo, Olteanu and Zhang~\cite{KaraNNOZ20} studied databases that support enumeration of triangles in trinary relations,
and their work implies triangle counting in dynamic graphs with $\tp=O(m_0^{3/2}), \tu=O(m^{1/2})$, and~$\tq=O(1)$.

P\u{a}tra\c{s}cu and Demaine~\cite{PatrascuD06} 
proved the first unconditional lower bound for dynamic graph algorithms, focusing on connectivity problems in undirected graphs.
One of their results asserts that there is no dynamic algorithm for deciding whether the graph is connected with a constant query time
and $\tu=o(\log n)$	amortized time
(see~\cref{lem:lb for connectivity-original}),
a results that remains the best known to date.
Follow-up works on their work~\cite{PatrascuT11a,CliffordGL15,LarsenY23},
considered, e.g., the case of parameterized queries (querying whether two given nodes are in the same connected component) and directed graphs.


Following a line of work on conditional lower bounds in dynamic graphs~\cite{AbboudW14,Patrascu10,DorHZ00,RodittyZ11,KopelowitzPP16}, which used several different conjectures,
Henzinger, Krinninger, Nanongkai and Saranurak~\cite{HenzingerKNS15} introduced the \omv conjecture as a unified conjecture.
They showed, e.g., that 
$s$-triangle detection,
bipartite matchings
and
bipartite vertex cover cannot be done with
polynomial preprocessing time, 
	$\tu=O(n^{1-\epsilon})$
and
$\tq=O(n^{2-\epsilon})$,
for any $\epsilon>0$ and even amortized and randomized,
unless the \omv conjecture is false.	
The main focus of the works until this point was on worst-case analysis,
and average-case analysis was rarely applied, as described next.


\subsubsection{Beyond worst-case analysis of dynamic graph algorithms}

The first to study average-case complexity of dynamic graph algorithms were 
Nikoletseas, Reif, Spirakis and Yung~\cite{NikoletseasRSY95}.
They consider an initial empty graph, a phase of addition of random edges, and then a phase where random edges are added or remove with equal probabilities. 
In this setting, they present an algorithm for $(u,v)$-connectivity, i.e., where a query includes a pair $(u,v)$ of nodes and has to determine if they are in the same connected component.

Following this, Alberts and Henzinger~\cite{AlbertsH98} 
defined the ``restricted randomness'' model.
To this end, they note that each change consists of a type (add or remove) and a parameter (which edge to add or to remove).
In their model, an adversary chooses a sequence of types (add or remove) for the changes,
and then the parameter (edge) of each change is chosen uniformly at random from the relevant set (existing edges for a remove operation, node-pairs without edges for an add operation).
They show that several problems have better amortized time complexities in the restricted randomness model compared to the best worst-case bounds known at the time (almost no lower bounds where known at the time).
The problems considered include connectivity and its variants (edge and node connectivity), 
maximum cardinality matching, 
minimum spanning forest, and bipartiteness. 

Indeed, one could see this model as an average-case model, but we prefer to use this term only for uniformly random inputs (as in~\cite{NikoletseasRSY95,HLS22}). 
It can also be seen as a smoothed analysis model, though without the ability to parameterize the amount of randomness.

As this model consists in an adversary and a random process, it should not come as a surprise that both an oblivious adversary and an adaptive one can be considered in this setting.
Here, an adversary that chooses the sequence of types in advance is called an oblivious one, and an adversary that can choose each type according to the outcome of the random process so far is an adaptive one.
These variants are not equivalent:
for example, an adaptive adversary can make sure that all the $n$-edge graphs in the sequence are connected (by adding an $n$-th edge to an $(n-1)$-edge graph only if it is connected), while with an oblivious adversary every $n$-edge graph has equal probability, and most of them are not connected.
The distinction between adversaries is not made in~\cite{AlbertsH98}, but the proofs use the fact that each graph has equal probability (conditioned on the number of edges), thus the model there is oblivious.
 
 
The average-case complexity of dynamic graph algorithms was disregarded for decades, until recently Henzinger, Lincoln and Saha~\cite{HLS22}
approached it again with new tools.
They studied subgraph counting problems, 
presented some simple algorithms inspired by~\cite{AlbertsH98},
and focused mainly on proving conditional lower bounds.
Their proofs use ideas from fine-grained complexity~\cite{HenzingerKNS15}, and specifically lower bounds for average-case complexity in this setting~\cite{DalirrooyfardLW20,Boix-AdseraBB19}.
For some problems, such as counting \paths st5, they show that the average-case complexity cannot be better than the worst-case one (up to an $n^\epsilon$ factor), while for others, such as 
\paths st3, they prove a big gap exists.
As smoothed analysis lie between the worst case and the average one, we focus the current work only on problems of the last type, where a gap exists.
Some of our lower bound proofs use techniques from~\cite{HLS22};
for example, while they do not give a lower bound for \paths st3 (since it is easy in the average case) we give a lower bound for this problem in the smoothed case
% ; interestingly, we do so 
by extending the technique they used for proving a lower bound for \paths st5.

\subsubsection{Smoothed analysis in different models}
Smoothed analysis was introduced by Spielman and Teng~\cite{SpielmanT04} in relation to using pivoting rules in the simplex algorithm. 
Since then, it have received much attention in sequential algorithm design.
In particular interest to us are smoothed analysis results for static graphs~\cite{krivelevich2006smoothed, friedrich2011smoothed,krivelevich2015smoothed} 
and dynamic networks~\cite{DFGN18,MPS20,GMPS21,DFGN22},
which we use as inspiration for our model of noise.

In recent years, smoothed analysis was also applied in settings of a repeated game, where inputs are chosen successively.
These include dynamic networks, population protocols~\cite{SS21} and online learning~\cite{HRS20,HRS21}.
The effect of the adversary's adaptivity was first studied with regard to information spreading in dynamic networks~\cite{MPS20},
which is shown to be polynomially slower when the adversary is adaptive compared to an oblivious adversary.
The adaptivity question was also studied in the context of online learning~\cite{HRS21}, and a handful of problems were shown to have the same smoothed complexity whether the adversary is adaptive or oblivious.
From these, the dynamic networks works are the most relevant to ours, as they defined similar smoothing models. 
Their results, however, focus on round complexity and not computation time, and thus are incomparable with ours.