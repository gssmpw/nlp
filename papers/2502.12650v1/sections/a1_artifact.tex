% LaTeX template for Artifact Evaluation V20240722
%
% Prepared by Grigori Fursin with contributions from Bruce Childers,
%   Michael Heroux, Michela Taufer and other colleagues.
%
% See examples of this Artifact Appendix in
%  * ASPLOS'24 "PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation": 
%      https://dl.acm.org/doi/10.1145/3620665.3640366
%  * SC'17 paper: https://dl.acm.org/citation.cfm?id=3126948
%  * CGO'17 paper: https://www.cl.cam.ac.uk/~sa614/papers/Software-Prefetching-CGO2017.pdf
%  * ACM ReQuEST-ASPLOS'18 paper: https://dl.acm.org/citation.cfm?doid=3229762.3229763
%
% (C)opyright 2014-2024 cTuning.org
%
% CC BY 4.0 license
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% When adding this appendix to your paper, 
% please remove above part
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Artifact Appendix}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Abstract}

Our artifact contains the data, source code, and scripts needed to reproduce our results.
We provide: 1) the source code of our simulation infrastructure based on Ramulator2 and 2) all evaluated memory access traces and all major evaluation results.
We provide Bash and Python scripts to analyze and plot the results automatically.

\subsection{Artifact Check-list (meta-information)}

% \urlstyle{sf}
\begin{table}[H]
  \centering
  \scriptsize
  \setlength{\tabcolsep}{0.7\tabcolsep}
  \captionsetup{justification=centering, singlelinecheck=false, labelsep=colon}
    \begin{tabular}{ll}
        {{\bf Parameter}} & \textbf{Value} \\
        \hline
                        &  C++ program \\
        Program         &  Python3 scripts \\
                        &  Shell scripts \\
        \hline
        Compilation     &  C++ compiler with c++20 features \\
        \hline
                             &  Ubuntu 20.04 (or similar) Linux \\
                             &  C++20 build toolchain (tested with GCC 10) \\
        Run-time environment &  Python 3.10+ \\
                             &  Podman 4.5+ \\
                             &  Git \\
        \hline
        Metrics  &  \makecell[l]{Weighted speedup\\DRAM energy} \\
        \hline
        Experiment workflow & \makecell[l]{Perform simulations, aggregate results, and\\run analysis scripts on the result} \\
        \hline
        Experiment customization & Possible. See \secref{sec:expcustom} \\
        \hline
        Disk space requirement & $\approx$ 30GiB \\
        \hline
        Workflow preparation time & $\approx$ 30 minutes \\
        \hline
        Experiment completion time & $\approx$ 1 day (on a compute cluster with 250 cores) \\
        \hline
                                          &  Benchmarks (\url{https://zenodo.org/record/14281771})  \\
        \makecell[l]{Publicly available?} &  Zenodo (\url{https://zenodo.org/record/14741186}) \\
                                          &  GitHub (\url{https://github.com/CMU-SAFARI/Chronus})  \\
        \hline
    \end{tabular}
    \label{tab:artifact_table}
\end{table}
% \urlstyle{tt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Description}
 
\noindent\emph{We highly recommend using Slurm with a cluster that can run experiments in bulk.}

\head{How To Access}
Source code and scripts are available at \url{https://github.com/CMU-SAFARI/Chronus}.

\head{Hardware Dependencies}
We recommend using a PC with 32 GiB of main memory.
Approximately 30 GiB of disk space is needed to store intermediate and final evaluation results.

\head{Software Dependencies}
\begin{itemize}
    \item GNU Make, CMake 3.20+
    \item C++20 build toolchain (tested with GCC 10)
    \item Python 3.9+
    \item pip packages: matplotlib, pandas, seaborn, pyyaml, wget, and scipy
    \item Ubuntu 22.04
    \item (Optional) Slurm 20+
    \item (Optional) Podman 4.5+
\end{itemize}

\head{Benchmarks}
We use workload memory traces collected from SPEC2006, SPEC2017, TPC, MediaBench, and YCSB benchmark suites.
These traces are available at \url{https://zenodo.org/records/14281771}.
Install scripts will download and extract the traces.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Installation}

\lstset{
    backgroundcolor=\color{gray!20}, % Set the background color
    basicstyle=\ttfamily\bfseries\footnotesize,
    columns=fullflexible,
    frame=single,
    breaklines=true,
    postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
    showstringspaces=false,
    numbersep=5pt,
    xleftmargin=6pt,
    xrightmargin=4pt,
    numbers=none,
    keywordstyle=\color{black},  % Set keyword color to black
    identifierstyle=\color{black},  % Set identifier color to black
    commentstyle=\color{black},  % Set comment color to black
    stringstyle=\color{black}  % Set string color to black
}

The following steps will download and prepare the repository for the main experiments:
\begin{enumerate}
    \item Clone the git repository.
    \begin{lstlisting}[language=bash]
$ git clone \
git@github.com:CMU-SAFARI/Chronus.git
    \end{lstlisting}
    \item (Optional) Build the Podman container to run the scripts.
    \begin{lstlisting}[language=bash]
$ podman build . -t chronus_artifact
    \end{lstlisting}
    The following command runs a script using the container:
    \begin{lstlisting}[language=bash]
$ podman run --rm -v $PWD:/app \
    chronus_artifact <script>
    \end{lstlisting}
    \item Install Python dependencies, compile Ramulator2, download workload traces, and run a small sanity check.
    \begin{lstlisting}[language=bash]
$ ./run_simple_test.sh
    \end{lstlisting}
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Evaluation and Expected Results}

\head{Claim 1 (C1)}
Latest industry solutions to read disturbance induce prohibitively large system performance overhead for both modern (i.e., \gls{nrh}$ = $1K) and future (i.e., \gls{nrh}$\leq$1K) DRAM chips.
This property is proven by evaluating the effect of state-of-the-art read disturbance mitigation mechanisms on system performance (E1) as described in \secref{sec:sensitivity} whose results are illustrated in \figref{fig:sensitivity_performance}.

\head{Claim 2 (C2)}
\X{} outperforms both 1) state-of-the-art industry solutions to read disturbance (i.e., \gls{prac}~\cite{jedec2024jesd795c} and \gls{prfm}~\cite{jedec2020jesd795}) and 2) state-of-the-art academic solutions to RowHammer (i.e., Graphene~\cite{park2020graphene}, Hydra~\cite{qureshi2022hydra}, and PARA~\cite{kim2014flipping}) in terms of system performance (\figsref{fig:benign_singlecore},~\ref{fig:benign_scaling}, and~\ref{fig:benign_workloads}), DRAM energy (\figref{fig:benign_energy}), and storage (\figref{fig:benign_storage}).
This property is proven by evaluating and comparing the impact of \X{} and prior state-of-the-art read disturbance mitigation mechanisms on system performance and DRAM energy on single-core and multi-core workloads (E2) as described in \secref{sec:methodology}.

\head{Experiments (E1 and E2)}
[Ramulator2 simulations]
[10 human-minutes + 20 compute-hours (assuming $\sim$500 Ramulator2 simulations run in parallel) + 30GiB disk]

We prove our claims in two steps:
1) Execute Ramulator2 simulations to generate data supporting C1 and C2;
2) Plot all figures that prove C1 and C2.

\begin{enumerate}
    \item Launch all Ramulator2 simulation jobs.\footnote{Slurm job partition and the maximum number of jobs are configurable via the \textit{AE\_SLURM\_PART\_NAME} variable in \texttt{./run\_with\_slurm.sh} and \textit{MAX\_SLURM\_JOBS} variable in \texttt{scripts/run\_config.py}, respectively.}
    \begin{lstlisting}[language=bash]
$ ./run_with_personalcomputer.sh
(or ./run_with_slurm.sh if Slurm is available)
    \end{lstlisting}
    \item Wait for the simulations to end. The following displays the status and generates scripts to restart failed runs:
    \begin{lstlisting}[language=bash]
$ ./check_run_status.sh
    \end{lstlisting}
    \item Parse simulation results and collect statistics.
    \begin{lstlisting}[language=bash]
$ ./parse_results.sh
    \end{lstlisting}
    \item Generate all figures that support C1 and C2.
    \begin{lstlisting}[language=bash]
$ ./plot_all_figures.sh
    \end{lstlisting}
\end{enumerate}

\subsection{Experiment Customization}
\label{sec:expcustom}
Our scripts provide easy configuration of the 1) evaluated read disturbance mitigation mechanisms, 2) tested read disturbance thresholds, 3) simulation duration, and 4) simulated workload combinations.
The run parameters are configurable in \texttt{scripts/run\_config.py} with 1) \textit{mitigation\_list}, 2) \textit{tRH\_list}, and 3) \textit{NUM\_EXPECTED\_INSTS} or \textit{NUM\_MAX\_CYCLES}, respectively.
Simulated single-core and multi-core workload combinations can be updated in \texttt{mixes/hpcasingle.mix} and \texttt{mixes/hpcabenign.mix}, respectively.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Methodology}

Submission, reviewing and badging methodology:

\begin{itemize}
  \item \url{https://www.acm.org/publications/policies/artifact-review-and-badging-current}
  \item \url{https://cTuning.org/ae}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% When adding this appendix to your paper, 
% please remove below part
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%