\section{Overhead Analysis \om{4}{of \gls{prac}}}
\label{sec:sensitivity}

We evaluate \gls{prac}'s performance overheads for existing and future DRAM chips, by sweeping \gls{nrh} from \param{1K} down to \param{20} (lowest secure \gls{nrh} value for \gls{prac}).
To evaluate performance, we conduct cycle-level simulations using Ramulator 2.0~\cite{luo2023ramulator2, ramulator2github} \om{4}{(which builds on the original Ramulator~\cite{kim2016ramulator, ramulatorgithub})}.
We extend Ramulator 2.0 with the implementations of \gls{prac}, \gls{rfm}, and the back-off signal.
We evaluate system performance using the weighted speedup metric~\cite{eyerman2008systemlevel, snavely2000symbiotic}.

\tabref{table:system_configuration} shows our system configuration.
We assume a realistic quad-core system, connected to a dual-rank memory with \param{eight} bank groups, each containing \param{four} banks (\param{64} banks total).
The memory controller employs the FR-FCFS memory scheduler\cite{frfcfs, zuravleff1997controller} with a Cap on Column-Over-Row Reordering (FR-FCFS+Cap)~\cite{mutlu2007stall} of \param{four}.
We extend the memory controller to delay requests that \emph{cannot} be served within \gls{taboact}.

\newcolumntype{C}[1]{>{\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\begin{table}[ht]
\scriptsize
\centering
\caption{{Simulated} System Configuration}
\begin{tabular}{l|C{5.8cm}}
 \hline
 \textbf{Processor} & {\SI{4.2}{\giga\hertz}, 4-core, 4-wide issue, {128-entry} instr. window} \\ \hline
 \textbf{Last-Level Cache} & {64-byte} cache line, 8-way {set-associative, \SI{8}{\mega\byte}} \\ \hline
 \textbf{Memory Controller} & {64-entry read/write request queues; Scheduling policy: FR-FCFS+Cap of \param{4}~\cite{mutlu2007stall}}; Address mapping: MOP~\cite{kaseridis2011minimalistic} \\ \hline
 \textbf{Main Memory} & DDR5 DRAM~\cite{ramulator2github}, 1 channel, 2 ranks, 8 bank groups, 4 banks/bank group, {64K} rows/bank\\ \hline
 \end{tabular}
 \label{table:system_configuration}
\end{table}

\head{Workloads}
We evaluate applications from five benchmark suites: SPEC CPU2006~\cite{spec2006}, SPEC CPU2017~\cite{spec2017}, TPC~\cite{tpc}, MediaBench~\cite{fritts2009media}, and YCSB~\cite{ycsb}. We group all applications into three memory-intensity groups based on their row buffer misses per kilo instructions (RBMPKIs), similar to prior works~\cite{olgun2024abacus, bostanci2024comet}. These groups are High (H), Medium (M), and Low (L) for the lowest MPKI values of \param{10, 2, and 0}, respectively. Then, we create \param{60} workload mixes with \param{10} of each HHHH, MMMM, LLLL, HHMM, MMLL, and LLHH combination types. We simulate each workload until all cores execute \param{100}M instructions \om{4}{each}.      

\subsection{Performance Evaluation}

\figref{fig:sensitivity_performance} presents the performance overheads of the evaluated read disturbance mitigation mechanisms as \gls{nrh} decreases.
Axes respectively show the \gls{nrh} values (x axis) and system performance (y axis) in terms of weighted speedup~\cite{eyerman2008systemlevel, snavely2000symbiotic} normalized to a baseline with \emph{no} read disturbance mitigation (higher y value is better).
Different bars identify different read disturbance mitigation mechanisms and the red edge color indicates \om{5}{unsafe (i.e., read disturbance vulnerable)} configurations.
We make \param{six} observations from \figref{fig:sensitivity_performance}.

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{figures/_fourcore_sensitivity_scaling.pdf}
\caption{Performance impact of evaluated \gls{prac} and \gls{rfm} configurations on 60 benign four-core workloads}
\label{fig:sensitivity_performance}
\end{figure}

\head{1. Reducing $\bm{\nrh{}}$ increases performance overheads}
As \gls{nrh} decreases, performance overheads of all studied mitigation configurations increase, as expected, due to the more frequent mitigating actions (i.e., preventive refreshes) performed.

\head{2. \gls{prac} \agy{0}{has non-negligible performance overheads}}
At \ous{6}{$\nrh{} =$~1K}, \gls{prac}-4 incurs an average (maximum) system performance overhead of \param{9.7}\% (\param{13.4}\%) across all workloads.
We attribute \gls{prac}'s non-negligible overhead at relatively high \gls{nrh} values to \gls{prac}'s increased DRAM timing parameters~\cite{jedec2024jesd795c} (\secref{sec:briefsummary}) as \gls{prac}-4 sends only 6 back-offs (1.9$\times$10$^{-8}$ back-offs per million cycles) on average across all workloads.

\head{3. \gls{prac} overheads slightly increase until $\bm{\nrh{} =}$~64}
When \gls{nrh} decreases from \param{1K} to \param{64}, \gls{prac}-4's system performance overheads slightly increase (i.e., $\approx$\param{2\%}).
We attribute the steady system performance as \gls{nrh} decreases to \gls{prac} accurately tracking aggressor row activations and refreshing rows by \emph{borrowing} time from periodic refreshes.

\head{4. \gls{prac} becomes prohibitively expensive at $\bm{\nrh{} \leq}$~64}
Between \gls{nrh} values of \param{64} and \param{20}, \gls{prac}-4's average (maximum) system performance overheads across all workloads significantly increase from \param{11.7}\% (\param{16.0}\%) to \param{81.2}\% (\param{91.9}\%).
We attribute the significant increase in system performance overhead to \gls{prac} performing more frequent preventive refreshes.
For example, with \gls{prac}-4 at \gls{nrh} values of \param{64} and \param{20}, the four-core benign workload of \emph{523.xalancbmk}, \emph{435.gromacs}, \emph{459.GemsFDTD}, and \emph{434.zeusmp} trigger 11.7 and 124.6 \ous{4}{back-offs} per million cycles, resulting in \param{18.6}\% and \param{88.0}\% system performance overhead, respectively.

\head{5. \gls{prfm} performs poorly}
When \gls{nrh} decreases from \param{1K} to \param{20}, the average (maximum) system performance overhead of \gls{prfm} increases from \param{2.1}\% (\param{3.7}\%) to \param{51.8}\% (\param{68.7}\%).
We attribute this significant overhead increase to \gls{prfm}'s configuration against the wave attack drastically increasing the frequency of preventive refreshes as \gls{nrh} decreases, similar to \gls{prac}.
%for \gls{nrh} between \param{64} and \param{20}

\head{6. \gls{prac}+\gls{prfm} performs poorly}\omcomment{4}{Maybe we did not pair them well? Is there a better way that does not increase performance?}\ouscomment{4}{We paired them as JEDEC describes (just throw them both in). I cannot think of a better way right now, but I added this question to our GDOC and will think about it.}
Pairing \gls{prac}-4 with \gls{prfm} increases \gls{prac}'s system performance overhead by an average (maximum) of \param{13.6}\% (\param{40.2}\%) across all $\nrh{}$ values.
This is because 1)~\gls{prac}'s secure configurations (\secref{sec:configurationandsecurity}) already preventively refresh all rows before they reach a critical level and
2)~pairing \gls{prac} with \gls{prfm} causes performance degradation due to unnecessary preventive refreshes.

We conclude that
1)~\gls{prac}'s increased DRAM timing parameters incur significant overheads even under infrequent preventive refreshes for modern DRAM chips (i.e., \gls{nrh} $=$ 1K),
2)~\gls{prac}'s system performance overheads slightly increase as \gls{nrh} decreases until \param{32}, where \gls{prac} starts performing significantly worse,
3)~\gls{prfm} incurs significant system performance loss with decreasing \gls{nrh}, and
4)~pairing \gls{prac} with \gls{prfm} provides no system performance advantage.

\subsection{PRAC's Two Major Outstanding Problems}

We identify \param{two} major outstanding problems \om{4}{with \gls{prac}}.
First, \gls{prac} counters are incremented \om{4}{while} a DRAM row is \om{4}{closed}.
Doing so increases \om{4}{some} timing parameters and degrades system performance.
Second, \gls{prac} allows a \ous{2}{fixed} number of preventive refreshes with each back-off and forces the memory controller to send the same number of activations as refreshed rows before new preventive refreshes can be requested.
The possible number of row activations exceeds the number of preventive refreshes performed with \gls{prac}'s preventive actions.\ouscomment{4}{removed i.e. part}
Thus, \gls{prac} is vulnerable to the wave attack~\cite{yaglikci2021security, devaux2021method}, which significantly reduces the necessary thresholds to mitigate read disturbance and makes mounting memory performance attacks~\cite{mutlu2007stall} easier (\secref{sec:evaluation_dos}).
