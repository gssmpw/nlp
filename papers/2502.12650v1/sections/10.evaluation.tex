\section{Experimental Evaluation}
\label{sec:evaluation}

\head{\ous{2}{Single-core Performance}}
\figref{fig:benign_singlecore} presents the performance overheads of the evaluated read disturbance mitigation mechanisms for \om{4}{different} single-core applications at \gls{nrh} values of \param{1K} (top) and \param{32} (bottom).
Axes respectively show the single-core applications (x axis) and system performance (y axis) in terms of weighted speedup normalized to a baseline with \emph{no} read disturbance mitigation.
Different bars identify different read disturbance mitigation mechanisms.
\om{5}{\emph{geomean}} depicts the geometric mean of each mechanism across \param{57} single-core applications \ous{5}{and error bars show the standard error of the mean}~\cite{altman2005standard}.

We make \param{two} observations from \figref{fig:benign_singlecore}.
First, at \ous{6}{$\nrh{} =$~1K}, \X{} incurs the \om{5}{lowest} system performance overhead ($<$\param{0.1}\% on average) across all evaluated mitigation mechanisms.
We attribute the low system performance overhead of \X{} at relatively high \gls{nrh} values (i.e., $\nrh{} \geq$ 1K) to \ous{5}{the CCU mechanism of \X{} because \X{}-PB \om{6}{(which implements CCU but \emph{not} Chronus Back-Off)} performs similarly well at these thresholds}.
Second, \X{} outperforms all mitigation mechanisms even at very low \gls{nrh} values.
At \ous{6}{$\nrh{} =$~32}, \X{} induces \om{4}{\emph{only}} \param{6.8}\% average system performance overhead.
In \om{4}{contrast}, Graphene, Hydra, and \gls{prac}-4 induce \param{28.1}\%, \param{30.6}\%, and \param{46.1}\% average system performance overhead, respectively.
We attribute the low system performance overhead of \X{} at relatively low \gls{nrh} values (i.e., $\nrh{} \leq$ 32) to \ous{4}{securely preventing a wave attack and thereby having a less aggressive back-off threshold because \X{}-PB and \gls{prac}, \ous{5}{mechanisms that are vulnerable to a wave attack due to using \gls{prac} Back-Off,} both induce high system performance \om{6}{overheads} (respectively \param{30.6}\% and \param{46.1}\%)} for these same thresholds.

We conclude that
1) \X{} incurs \ous{5}{the lowest} system performance overhead to single-core applications at \gls{nrh} values \om{4}{similar to current DRAM chips} (e.g., $\nrh{} =$ \param{1K}) and
2) \X{} \ous{5}{is the best performing mechanism with $<$\param{6.8}\% system performance overhead at} future \gls{nrh} values (e.g., $\nrh{} <$ \param{64}).

\head{\ous{2}{Multi-core System Performance}}
\figref{fig:benign_scaling} presents the performance overheads of the evaluated read disturbance mitigation mechanisms across \param{60} benign \param{four}-core workloads \om{4}{for \gls{nrh} values from \param{1K} to \param{20}}.
x and y axes respectively show the \gls{nrh} values and system performance in terms of weighted speedup normalized to a baseline with \emph{no} read disturbance mitigation (higher y value is better).
\ous{6}{Each colored bar depicts the mean system performance of a mechanism across 60 four-core workloads and error bars show the standard error of the mean across 60 four-core workloads}.\omcomment{6}{Where is the mean?}\ouscomment{6}{Each bar is a mean across 60 four-core workloads}
% \ieycomment{6}{say something like: Each colored bar represents the mean...across.. . Or define this when you introduce the y-axis, which is the mean normalized weighted speedup (not sure about my wording.)}\ouscomment{6}{revised}

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{figures/_fourcore_benign_scaling.pdf}
\caption{Performance impact of evaluated read disturbance mitigation mechanisms on 60 benign four-core workloads}
\label{fig:benign_scaling}
\end{figure}

% \ieycomment{6}{If you have space, reporting results would be much better. In some observations, you are reporting results. To make things consistent, reporting a result after an observation is always good. Only concern I have is Onur did not say anything about that, which is weird.}\ouscomment{6}{We don't report all comparisons for mitigation papers I think (?). I believe Comet and Abacus also skips them in most places (unless there is a non-trivial one to one comparison)}
% \ieycomment{6}{tested?}\ouscomment{6}{added evaluated for consistency}
We make \ous{5}{\param{four}} observations from \figref{fig:benign_scaling}.
\ous{5}{First}, across all evaluated \gls{nrh} values, \X{} outperforms all evaluated read disturbance mitigation mechanisms.
\ous{5}{Second, across all \ous{6}{evaluated} mechanisms, \X{}'s system performance scales the best as the \iey{6}{concurrent\ous{6}{ly} running} application count in a workload increases.
For example, as the \ous{6}{concurrently running} application count increases from \param{one} (57 single-core workloads total \om{6}{in \figref{fig:benign_singlecore}}) to \param{four} (60 four-core workloads total \om{6}{in \figref{fig:benign_scaling}}) at \ous{6}{$\nrh{} =$~32},
\X{}'s average system performance improves respectively over \gls{prac}-4 by \param{23.5}\%, Graphene by \param{14.8}\%, Hydra by \param{5.0}\%, and PARA by \param{64.4}\%.}
\ous{5}{Third}, as \gls{nrh} decreases, the system performance overhead of all \ous{6}{evaluated} mitigation mechanisms increases.
\ous{5}{Fourth}, \gls{prac}-4 underperforms against all \ous{6}{evaluated} mitigation mechanisms \om{6}{at $\nrh{} =$~20}.
\ous{5}{This is because \gls{prac}-4 inefficiently performs 4 preventive refreshes with each back-off and requires conservative configuration} against a potential wave attack.
In contrast, \X{}'s performance does \emph{not} get drastically \ous{6}{(i.e., $\geq$10\% overhead)} exacerbated \ous{6}{at $\nrh{} =$~20 for two reasons: \X{}}
\ous{6}{1)} dynamically adjusts the number of preventive refreshes as necessary and
\ous{6}{2)} can be less aggressively configured due to \om{7}{its mechanism} securely preventing a potential wave attack.
% \ieycomment{6}{here Onur said "at nrh=20". Lets change all instances like this.}\ouscomment{6}{Revised. I think I changed them everywhere}
% \ieycomment{6}{by how much?}\ouscomment{6}{revised}
% \ieycomment{6}{Two questions/concerns here: 1) PRAC-4 does not dynamically adjust things right as far as I understand. Is Chronus the only mechanism that does this? and 2) less aggressive configuration due to security is very hard to understand or a bit implicit and the thing that you want to say is not coming out (maybe it is explained well in previous sections). What do you think?}
% \ieycomment{6}{a bit convoluted as you use "because" and "as" in one sentence. Try to break it into multiple sentences. Also we need a conclusion after every figure, no?}\ouscomment{6}{Revised}
% \ieycomment{6}{here this is i.e., but later it becomes e.g.,. apparently you use current rh threshold as 1k everywhere so I am assuming you explain that in earlier sections so you can ignore my later comment about this}\ouscomment{6}{I think background most clearly mentions this. Earlier sections (PRAC analysis and evaluation) also use these numbers. I made the e.g., part consistent though.} 

\ous{6}{We conclude that
1) \X{} incurs low system performance overhead on evaluated multi-core workloads at \gls{nrh} values similar to current DRAM chips (\ous{6}{e.g.}, $\nrh{} =$~\param{1K}) and
2) \X{} scales the best to future \gls{nrh} values (e.g., $\nrh{} <$~\param{64}) with $<$\param{8.3}\% system performance overhead}.
% \ieycomment{6}{I am not a fan of combining the conclusion of two experiments. Usually for each experiment/result/plot, we make observations and after that we conclude.}\ouscomment{6}{Revised}

\head{\ous{2}{Sensitivity to Workload Memory Intensity}}
We further study the system performance overhead \ous{4}{of each mechanism at different memory intensity levels}.
\figref{fig:benign_workloads} presents the performance overheads of the evaluated read disturbance mitigation mechanisms for \ous{0}{\param{four}-core} workload types of varying intensities at \ous{6}{$\nrh{} =$~32}.
Axes respectively show the workload types (x axis) and system performance (y axis) in terms of weighted speedup normalized to a baseline with \emph{no} read disturbance mitigation (higher y value is better).
Each letter in the workload type identifies the memory intensity of an application as High (H), Medium (M), or Low (L).
\ous{6}{In each workload type, colored bars show the mean system performance of a mechanism across 10 workloads and error bars show the standard error of the mean across 10 workloads}.
\om{5}{\emph{geomean}} depicts the geometric mean of each mechanism across \param{60} four-core workloads.

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{figures/_fourcore_benign_workloads.pdf}
\caption{Performance impact of evaluated read disturbance mitigation mechanisms on \om{5}{\param{six}} workload types of varying \om{5}{memory} intensities}
\label{fig:benign_workloads}
\end{figure}

We make \param{two} observations from \figref{fig:benign_workloads}.
First, \X{} outperforms all evaluated mitigation mechanisms across all workload intensity types.
Second, the system performance overhead of evaluated mitigation mechanisms increases with the memory intensity of the workloads.
\ous{6}{We conclude that \X{}'s system performance stays the best with varying memory intensity}.

\head{\ous{2}{DRAM Energy}}
We study the DRAM energy overheads of read disturbance mechanisms.
\figref{fig:benign_energy} presents the energy consumption of the evaluated read disturbance mitigation mechanisms (y axis) \ous{4}{normalized to a baseline with \emph{no} read disturbance mitigation mechanism} as \gls{nrh} (x axis) decreases.
\ous{6}{Each colored bar depicts the mean energy consumption of a mechanism across 60 four-core workloads and error bars show the standard error of the mean across 60 four-core workloads}.\omcomment{6}{Where is the mean?}\ouscomment{6}{Each bar is a mean across 60 four-core workloads}
% \ieycomment{6}{integrate this into the text as well for all figure definitions}\ouscomment{6}{Revised everywhere}
% \ieycomment{6}{dash or no dash?}\ouscomment{6}{i think we used no dashes across the whole paper, I also prefer no dashes}

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{figures/_fourcore_benign_energy.pdf}
\caption{Energy impact of evaluated read disturbance mitigation mechanisms on \param{60} benign four-core workloads}
\label{fig:benign_energy}
\end{figure}

We make \ous{4}{\param{seven}} observations from \figref{fig:benign_energy}.
First, as \gls{nrh} decreases, the energy consumption \om{7}{overheads} of all \ous{6}{evaluated} mitigation mechanisms increases.
Second, at \ous{6}{$\nrh{} =$~1K}, \X{} increases the average (maximum) energy \ous{6}{consumption} by \param{10.3}\% (\param{10.7}\%) \ous{4}{over a baseline with \emph{no} read disturbance mechanism}.
At this threshold, \X{} significantly outperforms \gls{prac}-4's \ous{6}{energy overhead} by \param{44.4}\%.
This is because \X{}'s \ous{5}{CCU} \om{5}{mechanism} \ous{4}{introduces less energy overhead than} \gls{prac}'s energy overheads, \om{5}{which are high due to higher} DRAM timing parameters and \om{5}{consequently slower} \ous{4}{execution}.
Third, when \gls{nrh} decreases from \param{1K} to \param{20}, \X{}'s average (maximum) \ous{6}{energy overhead} increases from \param{10.3}\% (\param{10.7}\%) to \param{17.9}\% (\param{20.7}\%).
\ous{4}{Fourth}, \X{} \om{7}{leads to lower energy consumption than} all \ous{6}{evaluated} mitigation mechanisms \ous{6}{at $\nrh{} \leq$~64}.
\ous{4}{Fifth}, when \gls{nrh} decreases from \param{1K} to \param{20}, \gls{prfm}'s average energy \om{7}{overhead} increases from \ous{6}{$<$1.1x to 4.8x}.
% \ieycomment{6}{wait. from percentage to times?}\ouscomment{6}{I usually switch to times if anything goes above 200\%. I can change them if this is a no-no zone.}\ieycomment{6}{this is very confusing for me. But ask this to Nisa as well.}
\ous{4}{Sixth}, when \gls{nrh} decreases from \param{1K} to \param{20}, \gls{prac}-4's average energy \om{7}{overhead} significantly increases from 1.2x to 7.9x.
We attribute the high \om{7}{increases} in energy consumption \om{7}{overheads} of \gls{prfm} and \gls{prac}-4 as \gls{nrh} decreases to
1) their conservative preventive refresh thresholds against \ous{6}{a potential} wave attack and
2) benign applications triggering many preventive refreshes.
\ous{4}{Seventh}, as \gls{nrh} decreases from \param{1K} to \param{20}, average energy overheads of Graphene and Hydra increase \om{5}{greatly}, from $<$\param{0.1}\% and \param{0.3}\% to \param{33.0}\% and \param{62.0}\%, respectively.

We conclude that
1) \X{} consumes \om{5}{lower} DRAM energy compared to \gls{prac} at \gls{nrh} values \ous{4}{similar to current DRAM chips} (e.g., $\nrh{} =$~\param{1K}) and
2) \X{} \om{5}{is the lowest energy-overhead solution at} \ous{5}{future \gls{nrh} values (e.g., $\nrh{} <$~64)}.
% \ieycomment{6}{cite? rowpress?}\ouscomment{6}{I think a citation here would look ugly but we can maybe cite it in the methodology section? We have a sentence saying ``for existing and future DRAM
% chips, by sweeping NRH from 1K down to 20''.}\ieycomment{6}{You are making a statement that current DRAM chips have a 1K rh threshold. This should be backed up by something. if Onur did not say anything then I am fine with it also.}

\head{\ous{2}{Storage}}
\label{subsec:storageeval}
\figref{fig:benign_storage} shows the storage overhead \ous{0}{in MiB (y axis)} of the evaluated read disturbance mitigation mechanisms as \gls{nrh} decreases \ous{0}{(x axis)}.
\ous{0}{We evaluate the storage usage of \X{} (DRAM), Graphene (CAM), Hydra (DRAM+SRAM), PRAC (DRAM), and PRFM (SRAM) as a function of \gls{nrh} for a DRAM module with \param{64} banks and \param{128K} rows per bank.}\footnote{\ous{4}{PARA~\cite{kim2014flipping} is \emph{stateless} (i.e., does \emph{not} track or store row activation history) and only requires a random number generator. Therefore, we do \emph{not} include PARA in our storage overhead evaluation.}}\ouscomment{5}{This footnote causes the line (and the previous paragraphs) to get placed weirdly and I can't find a way to quickly fix now. I will hunt it if it remains this way in the ``beautification'' process.}

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{figures/_fourcore_benign_storage.pdf}
\caption{Storage used by \ous{0}{\X{} (DRAM), Graphene (CAM), Hydra (DRAM+SRAM), PRAC (DRAM), and PRFM (SRAM)} as a function of RowHammer threshold for a DRAM module with \param{64} banks and \param{128K} rows per bank}
\label{fig:benign_storage}
\end{figure}

% \ieycomment{6}{we make XX observations from Fig.XXX.}\ouscomment{6}{Revised}
\ous{6}{We make \param{four} observations from \figref{fig:benign_storage}}.
First, as \gls{nrh} decreases from \param{1K} to \param{20}, \X{}'s storage overhead in DRAM \ous{0}{is the same as \gls{prac}'s as they both employ a counter per DRAM row}.
Second, as \gls{nrh} decreases from \param{1K} to \param{20}, Graphene's storage overhead in CPU increases significantly (by \param{50.3}x) due to the need to track many more rows.
Third, as \gls{nrh} decreases from \param{1K} to \param{20}, \ous{0}{\X{}} and Hydra's storage overheads in DRAM reduce by \param{45.5}\% and \param{50.0}\%, respectively.
We note that while Hydra's cache structure in CPU does \emph{not} change, the overall cache size reduces with \gls{nrh} (by \param{43.9}\% from \param{1K} to \param{20}) as smaller cache entries are sufficient to track activations.
Fourth, \gls{prfm} incurs the least storage overhead in the CPU among the evaluated mitigation techniques as it requires \om{4}{only} one counter per bank.

We conclude that \X{}, \gls{prac}, \gls{prfm}, and Hydra incur low storage overheads and scale well with decreasing \gls{nrh} values as they either $i$) keep counters in DRAM where a large amount of storage is available at high density or $ii$) require only a small set of counters.

\section{Performance Degradation Attack}
\label{sec:evaluation_dos}

An attacker can take advantage of \ous{0}{\X{} and} \gls{prac} to mount memory performance (or denial of memory service) attacks~\cite{mutlu2007memory} by triggering many \ous{0}{back-offs}.
This section presents
1) the \ous{5}{\emph{worst-case} access pattern that causes the maximum theoretical DRAM bandwidth consumption in a} \X{} and \gls{prac}-protected system and
2) simulation results.\footnote{\ous{9}{We prove the worst-case property of the adversarial pattern in Appendix~\ref{apx:adversarialproof}}.}

\head{\gls{prac} Theoretical Analysis}
We calculate the maximum possible fraction of \om{4}{execution} time that preventive actions take in a \gls{prac}-protected system.
\ous{0}{Triggering a \gls{prac}} back-off signal takes $\aboth\times\trc$, which causes $\bonrefs$ RFM commands, blocking the bank for a time window of $\bonrefs\times\trfm$.\footnote{Attacking rows \om{5}{concurrently in different banks} does \emph{not} increase attack efficiency because the memory controller issues \om{4}{all-bank RFM commands (\emph{RFMab})} after a back-off, thereby refreshing the potential victim \om{7}{rows} of all concurrent aggressor \om{7}{rows} \emph{without} additional overhead.}
Therefore, an attacker can block a DRAM bank for $(\bonrefs\times\trfm)/(\bonrefs\times\trfm+\aboth\times\trc)$ of time.
We configure $\aboth$, $\bonrefs$, $\trfm$, and $\trc$ as \param{1}, \param{4}, \param{\SI{350}{\nano\second}}, and \param{\SI{52}{\nano\second}} \om{7}{for} \ous{6}{$\nrh{} =$~20}, based on the \param{DDR5-3200AN} \ous{0}{speed bin} in the JEDEC standard~\cite{jedec2024jesd795c}.
% 16 GB 3200AN (all ns)
% tRC = 52 (increased due to PRAC)
% tREFW = 32000000
% tREFI = 3900
% tRFC = 295
% tRFM = 350 (increased due to PRAC)
% t_avail = 32000000 - 295*(32000000/3900) = 29579487.1795
% T_attack = 228 * 52 + 4 * 350 = 4144
% t_prevent = 4 * 350 * 29579487.1795 / 4144 = 9993069.99
% t_prevent / t_avail = 9993069.99 / 29579487.1795 = 0.337837838
% By plugging the \param{DDR5-3200AN}~\cite{jedec2024jesd795c} timing preset, 
% we obtain the $t_{available}$, $T_{attack}$, and $t_{prevent}$ as \param{$\approx$29.58ms}, \param{3859ns}, and \param{$\approx$9.04ms}, respectively.
% Based on these values, 
We observe that an attacker can \emph{theoretically} consume \param{94\%} of DRAM throughput by triggering \gls{prac} back-offs.

\head{\X{} Theoretical Analysis}
\ous{0}{We calculate the maximum possible fraction of \ous{4}{execution} time that preventive actions take in a \X{}-protected system.
Triggering a \X{} back-off signal takes $\aboth\times\trc$, which causes \param{one}\footnote{An attacker should \emph{not} trigger more than one \gls{rfm} command per back-off as it takes $N_{BO}\times\trc$ per additional \gls{rfm} command (time to increase another rows activation count), significantly decreasing attack efficiency.} RFM command, blocking the bank for a time window of $\trfm$.
Therefore, an attacker can block a DRAM bank for $(\trfm)/(\trfm+\aboth\times\trc)$ of time.
We configure $\aboth$, $\trfm$, and $\trc$ as \param{16}, \param{\SI{350}{\nano\second}}, and \param{\SI{47}{\nano\second}} \om{7}{for} \ous{6}{$\nrh{} =$~20}, based on the \param{DDR5-3200AN} \ous{0}{speed bin} in the JEDEC standard~\cite{jedec2024jesd795c}.
We observe that an attacker can \emph{theoretically} consume \agy{0}{up to} \param{32\%} of DRAM throughput by triggering \X{} back-offs, which is significantly smaller \om{4}{than} \gls{prac}'s \emph{theoretical} overheads (\param{94}\%).}

\head{Simulation}
To understand the system performance degradation an attacker could cause by hogging the available DRAM throughput with preventive refreshes, we simulate \param{60} \param{four}-core workload mixes of varying memory intensities where one core maliciously hammers \param{8} rows in each of \param{4} banks.\footnote{We experimentally found these values to yield the highest performance overhead for \X{} and \gls{prac} in our system configuration.}

\ous{0}{Our system performance (weighted speedup~\cite{eyerman2008systemlevel, snavely2000symbiotic}) and maximum slowdown \om{4}{of} a single application~\cite{kim2010thread} results for \gls{nrh} values of 128 and 20 show that
1) \gls{prac}-4 reduces system performance on average (maximum) by \param{18.4}\% (\param{29.0}\%) and \param{86.8}\% (\param{94.6}\%) with a maximum slowdown of \param{64.5}\% and \param{97.7}\%, respectively and
2) \X{} reduces system performance on average (maximum) by \param{13.3}\% (\param{24.9}\%) and \param{25.9}\% (\param{39.3}\%) with a maximum slowdown of \param{62.3}\% and \param{69.9}\%, respectively.}
\ous{0}{We conclude that \X{} significantly reduces the memory performance attack vulnerability compared to \gls{prac}.}
