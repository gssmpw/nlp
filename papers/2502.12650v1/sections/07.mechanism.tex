\addtocounter{figure}{1}
\begin{figure*}[hb!]
\centering
\includegraphics[width=\linewidth]{figures/waveattackcheck.pdf}
\caption{Wave attack visualization for four different classes of read disturbance mitigation mechanisms}
\label{fig:waveattackcheck}
\end{figure*}
\addtocounter{figure}{-2}

\section{\X{}: Alleviating Counter Access Latencies and Back-Off Delays}
\label{sec:mechanism}

We propose \X{}, a new mechanism that addresses \gls{prac}'s \param{two} \ous{2}{major} weaknesses.
\ous{5}{\X{} implements two key components to address these weaknesses: \emph{Concurrent Counter Update} (CCU) and \emph{Chronus Back-Off}}.
First, \ous{5}{CCU} updates row activation counters concurrently \ous{5}{while} serving accesses.
\X{} achieves this by \om{4}{physically} separating counters from the data stored in the DRAM array.
As such, \X{} prevents the increase in DRAM timing parameters \om{4}{due to \gls{prac}}.
Second, \ous{5}{Chronus Back-Off} extends \gls{prac}'s back-off policy such that \X{} performs as many preventive refreshes as needed when a back-off is \ous{2}{triggered} as opposed to \gls{prac}'s policy of \emph{only} allowing a \ous{2}{fixed} number of preventive refreshes and enforcing a delay between consecutive back-offs to serve memory accesses.
By doing so, \X{} prevents an attacker from maliciously using the back-off delay for a wave attack.

\subsection{\ous{5}{CCU: Concurrent Counter Update}}
The latest JEDEC DDR5 DRAM specification \ous{2}{(as of April 2024~\cite{jedec2024jesd795c})} \om{4}{allows} manufacturers to extend DRAM rows with counter bits.
These row activation counters are incremented while the row is being closed, which increases critical \ous{2}{DRAM} timing parameters (\secref{sec:briefsummary}) and degrades system performance even at relatively high \ous{4}{read disturbance} thresholds, e.g., $\nrh{} =$ 1K, as we show in \secref{sec:sensitivity}.
These performance overheads can be eliminated by updating counters \om{4}{\emph{concurrently \ous{5}{while} serving accesses}}, \ous{5}{i.e., Concurrent Counter Update (CCU)}.\ouscomment{5}{CCU defined here}
\om{4}{To achieve this}, we propose \om{4}{physically} separating counters \om{4}{from data} by leveraging subarray-level parallelism~\cite{kim2012acase, chang2014improving, yaglikci2022hira, yuksel2024functionally}.\omcomment{6}{Revise to clearly say what we add and what methodology we use to evaluate their overhead. Point me to where it is said.}

\head{High Level Explanation}
\figref{fig:chronushighlevel} depicts a high-level overview of \om{4}{\X{}'} separate counters.
We propose leveraging subarray-level parallelism by storing row activation counters of a DRAM bank in a sufficiently small (e.g., \om{4}{\param{64}-row}) additional subarray, which we call \om{4}{the} \emph{counter subarray}~\circled{1}, within each bank.
The counter subarray 1) \om{4}{stores} the row activation counts \om{4}{of all data rows in the bank,} \ous{6}{2) implements custom circuitry to update~\circled{2}} (read -- increment by 1 -- write back) an activation count when a row is activated, \ous{6}{and 3) employs a mechanism to prevent bitflips within the counter subarray~\circled{3}}.\ouscomment{6}{Revised here to \emph{clearly} say what we add at a high level. Each detailed section provides overheads and the methodology on how the overhead is calculated (I also attached comments to where their overheads are discussed)}
As the counter subarray can be accessed concurrently with \om{5}{\emph{regular subarrays}} that store data~\cite{kim2012acase, chang2014improving, yaglikci2022hira, yuksel2024functionally, yuksel2024simultaneous}, the latency of reading, updating, and writing a counter can be hidden by the latency of opening, accessing, and closing the corresponding row.\footnote{We propose using subarray-level parallelism~\cite{kim2012acase} only between the counter subarray and regular subarrays. \X{} can also be combined with \om{6}{exploiting} subarray-level parallelism between regular subarrays~\cite{kim2012acase}. We leave the exploration of the benefits and challenges of such design to future work.}

\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{figures/Chronus.pdf}
\caption{\om{4}{Overview of \X{}' \om{6}{Concurrent Counter Update technique}}}
\label{fig:chronushighlevel}
\end{figure}
\addtocounter{figure}{1}

\head{Storing the Counters \circled{1}}
\X{} leverages the high density of the DRAM array by storing the activation counters in a set of DRAM rows, located in the counter subarray.
Assuming a realistic implementation, where\ouscomment{6}{Counter subarray storage overhead calculation}
1)~each row has an \param{8-bit} row activation count,\footnote{\X{}'s performance and energy overheads are reasonably low ($<$\param{0.1\%}) for an \gls{nrh} of \param{256} (\secref{sec:evaluation}). Therefore, we assume a counter width of \param{8} bits.}
2)~a DRAM bank contains \param{128K} DRAM rows~\cite{jedec2024jesd795c}, and
3)~each DRAM row contains \param{16Kbit} in a DRAM chip~\cite{jedec2024jesd795c},
\X{} maintains \param{128KB} (128K rows $\times$ 8-bits) of activation count metadata for a DRAM bank of \param{256MB} (128K rows $\times$ 16Kbit), which fits into \param{64} DRAM rows (128KB $/$ 16Kbit).
As such, the counter subarray incurs \om{4}{only} a \param{0.05\%} of capacity overhead.

\head{Updating the Counters \circled{2}}
When a DRAM row is activated, \X{} updates the corresponding activation counter in \param{five} steps.
First, \X{} activates the row that contains the corresponding activation count in the counter subarray. 
Second, \X{} reads the column that contains the corresponding activation count. 
Third, \X{} reads the activation count by parsing the corresponding bits in the read column.
Fourth, \X{} updates the activation count.
Fifth, \X{} writes the updated counter back.
To read/write the counter, \X{} parses three sets of bits from the externally provided row address and identifies the row, column, and byte addresses used in the counter subarray.
To achieve low hardware complexity, \X{} uses custom circuitry to decrement the counter for a row by one \ous{5}{(we call this circuit \emph{the decrementer circuit})} when the row is activated and triggers a \ous{6}{back-off} when the counter reaches zero (e.g., every \param{256} activations).
For \gls{nrh} values smaller than \param{256}, \X{} compares the counter against \param{$256 - \nrh{}$} and triggers \ous{6}{a back-off} accordingly. 

\ous{4}{We implement the decrementer circuit using NAND, NOR, NOT, and MUX gates (gates that are already implemented in the local sense amplifiers).
Our design consists of \param{21} gates and can be implemented with \param{96} transistors.
We evaluate the critical path delay of the decrementer circuit to be \SI{0.627}{\nano\second}\ouscomment{6}{decrementer hardware complexity},\footnote{We use \om{4}{the} Synopsys Design Compiler~\cite{synopsys} with a Global Foundaries 22nm technology~\cite{carter201622nm}, configured to take into account the performance degradation of implementing logic using the DRAM process.
We set the output load of the decrementer circuit to be 1000fF and \om{4}{apply} a latency penalty factor of 22.91\% \om{5}{used in prior work}~\cite{chen1996Assessing}.}
which is significantly smaller than the row cycle latency (\gls{trc}) of \SI{47}{\nano\second} \om{4}{and hence can be completed \om{5}{concurrently} with an access to another subarray}.
Appendix~\ref{apx:decrementer} provides a detailed implementation of the decrementer circuit}.

\head{Avoiding Bitflips in the Counter Subarray~\circled{3}}\ouscomment{6}{Moved ``bitflip mitigation'' up (such that headings follow 1, 2, and 3 in the figure)}
\ous{2}{The counter subarray consists of DRAM cells and thus it is also vulnerable to read disturbance bitflips}.
To avoid bitflips in the counter subarray, we recommend implementing one of three effective countermeasures:
1)~tracking the activation counts of rows in the counter subarray in a separate SRAM array and refreshing them when necessary, similar to the Silver Bullet technique~\cite{devaux2021method, yaglikci2021security},
2)~refreshing potential victim rows in the counter subarray frequently in parallel to accessing other rows by latching the fetched row in a redundant array of sense amplifiers, similar to REGA~\cite{marazzi2023rega}, and
3)~allocating guard rows in between consecutive rows in the counter subarray, similar to GuardION~\cite{van2018guardion} and ZebRAM~\cite{razavi2016flip}.
Although these can be costly solutions for protecting all rows, their costs can be reasonable when limited to the counter subarray \ous{2}{(equivalent to protecting only \param{0.05\%} of a DRAM bank)}.\ouscomment{6}{Bitflip mitigation hardware complexity}
Our CACTI~\cite{cacti} and mathematical evaluations show that \ous{4}{applying any of these solutions} incur near-zero ($<$0.1\%) area overhead per DRAM bank.\omcomment{4}{What about REGA?}\ouscomment{4}{Using the methodology in the REGA paper, the overhead is 0.0157\% (again extremely low because we only protect the counter subarray)}

\head{Resetting the Counters}
\ous{4}{As DRAM cells are periodically refreshed to retain their data, DRAM read disturbance mechanisms periodically reset activation counters to reduce the number of preventive actions taken~\cite{park2020graphene, olgun2024abacus, lee2019twice, bostanci2024comet, qureshi2022hydra}.
Doing so also prevents an adversarial access pattern that can hog a significant amount of DRAM bandwidth availability due to preventive actions.
This adversarial pattern\nbcomment{7}{I am not aware of any works actually describing this potential attack.}\omcomment{7}{Not even in submission? If so, ok.}\ouscomment{7}{Not even in submission. (verified by Nisa)}
1) \om{5}{hammers} many rows close to the preventive action threshold and
2) \om{5}{triggers} preventive actions on each row quickly.\footnote{\ous{4}{An attacker can also hog a significant amount of DRAM bandwidth availability with different adversarial access patterns at future \gls{nrh} values, as shown by prior work~\cite{canpolat2024breakhammer}. We perform a similar study by analyzing \X{} and \gls{prac}'s vulnerability to memory performance attacks~\cite{mutlu2007stall} in \secref{sec:evaluation_dos}.}}
To reduce the number of \ous{5}{preventive actions (i.e., back-off)} and to mitigate the adversarial access pattern, \X{} borrows time from periodic refreshes (similarly to \gls{prac}, as we describe in \secref{sec:configurationandsecurity}).
As such, \X{} periodically \ous{5}{refreshes the victims of \emph{a recently accessed row with a relatively high activation count}}, thereby 1) resetting the counter of the row before it is activated enough times to trigger a back-off and 2) preventing the build-up of an adversarial access pattern that could hog DRAM bandwidth availability}.

\head{DRAM Energy Overhead of the Counter Subarray}
\om{5}{A} row activation in the counter subarray \om{4}{(which is concurrent with an activation \om{5}{of} a row in a regular subarray)} does \emph{not} significantly increase DRAM power consumption, as shown by two independent prior studies~\cite{yuksel2024simultaneous,mathew2017using}.
First, real DRAM chip measurements with multiple row activations~\cite{yuksel2024simultaneous} show that additional row activations introduce marginal \om{4}{additional} power consumption, e.g., simultaneously activating 32 rows consumes 21.19\% less power than a periodic refresh operation.
Second, a separate prior work~\cite{mathew2017using} \ous{4}{shows} that power consumption largely comes from driving the peripheral circuitry instead of asserting wordlines.
For our evaluations, we conduct SPICE~\cite{nagel1973spice} simulations using prior state-of-the-art open-source DRAM subarray models~\cite{hassan2019crow} to estimate Chronus' power and energy overheads.
Our SPICE simulations show that the row activation \om{5}{in the counter subarray and the} counter update increase the DRAM energy consumption of a DRAM row access (i.e., opening and closing a row) by \param{19.07}\%.

\subsection{\ous{5}{\X{} Back-Off}}
The wave attack is only possible if an attacker can perform row activations \om{4}{more quickly} than the \ous{2}{read disturbance} mitigation mechanism refreshes \ous{2}{victims}.
\figref{fig:waveattackcheck} visualizes the wave attack buildup for \param{four} classes of mitigation mechanisms: (a) Memory Controller Based~\mcBasedRowHammerMitigations{}, (b) Refresh Management~\cite{jedec2024jesd795c}, (c) \gls{prac} Back-Off~\cite{jedec2024jesd795c}, and (d) \X{} Back-Off (this work).

The memory controller based mechanisms control the flow of traffic to DRAM.
Therefore, \ous{0}{these mechanisms can} stop \ous{0}{serving} DRAM \ous{0}{accesses} and \ous{0}{freely} refresh all potential victims of aggressor rows that reach \ous{2}{a critical activation count} (e.g., \param{32} in \figref{fig:waveattackcheck}).
Refresh Management, \agy{0}{as described in} the JEDEC standard~\cite{jedec2024jesd795c}, requires \gls{rfm} commands to be issued periodically based on the number of activations \ous{0}{to a bank}.
\agy{0}{Therefore, the} activations between periodic preventive refreshes \ous{2}{enable a wave attack}.
\gls{prac} Back-Off suffers from \param{three} limitations that both reduce performance and \ous{2}{enable a wave attack}:
$L1$) The memory controller \emph{can} issue activations (with a maximum \ous{0}{number} of \SI{180}{\nano\second}/$\trc{}$) \ous{0}{after a back-off is triggered (during the window of normal traffic)},
$L2$) the memory controller inefficiently sends a \ous{2}{fixed} number of preventive refresh commands ($\bonrefs{}$) with each back-off \ous{2}{(regardless of the number of preventive refreshes needed to refresh all potential victims)}, and
$L3$) the DRAM module \emph{cannot} trigger another back-off \ous{0}{(delay period)} until the DRAM module receives a certain number of activations ($\bonacts{}$).
\om{4}{In contrast}, \X{} Back-Off \om{4}{securely} \emph{prevents} \ous{2}{a wave attack} (and performance loss due to unnecessary preventive refreshes) by solving \param{two} key weaknesses ($L2$ and $L3$)\footnote{We do \emph{not} propose \ous{5}{reducing or removing} the window of normal traffic \ous{5}{(L1) because such a change} requires 1) the \texttt{alert\_n} to be a fast signal and 2) the memory controller to react to a back-off more quickly, thereby potentially \ous{5}{increasing memory scheduling complexity}. We leave a rigorous analysis of \om{5}{using} a more intelligent DRAM interface and protocol (such as~\cite{hassan2024self}) \om{5}{together with \X{}} to future work.} of \gls{prac} Back-Off.
\ous{4}{First}, \X{} keeps the back-off signal asserted until the DRAM module \ous{4}{refreshes the last aggressor row with an activation count that exceeds \gls{aboth}.
While the back-off signal is asserted, the memory controller continues to send preventive refreshes.
Second}, \X{} does \emph{not} enforce a delay period.
Doing so prevents unnecessary refreshes by allowing \X{} to dynamically adjust the number of preventive refreshes.

\head{Challenges of Enabling Chronus Back-Off}
We discuss \param{two} design \ous{2}{considerations} \ous{6}{to enable} Chronus Back-Off:
1) latency of the back-off signal and
2) re-asserting the back-off signal without a delay period.
First, the latency of the back-off signal is low enough to provide dynamic control over the number of RFM commands because an RFM command has a relatively high duration (e.g., \SI{350}{\nano\second}~\cite{jedec2024jesd795c}).
This high duration is enough for the memory controller to acknowledge the de-asserted back-off signal and stop sending RFM commands;
The JEDEC DDR5 standard~\cite{jedec2024jesd795c} already utilizes the pin used by back-off signals (i.e., \texttt{alert\_n} pin) at a lower latency in two places:
1) \emph{Write CRC Error Handling}~\cite{jedec2024jesd795c} requires \texttt{alert\_n} to be asserted for 12 to 20 command clock cycles with a potential delay of \SI{3}{\nano\second} to \SI{13}{\nano\second} and
2) \emph{PRAC Back-Off}~\cite{jedec2024jesd795c} requires the memory controller to acknowledge \texttt{alert\_n} with a latency of \SI{180}{\nano\second} (i.e., duration of normal traffic). 
Second, \emph{PRAC Back-Off} already \emph{allows} the \texttt{alert\_n} to be asserted during the delay period \om{4}{due to} other causes (e.g., Write DQ CRC errors~\cite{jedec2024jesd795c}).
We conclude that there are \emph{no} fundamental limitations in the JEDEC DDR5 standard~\cite{jedec2024jesd795c} against enabling Chronus Back-Offs.
