\section{Comparison to ABACuS}
\label{apx:abacuscomparison}

We compare \X{} to ABACuS~\cite{olgun2024abacus}, a storage-optimized deterministic mechanism that uses the Misra-Gries frequent item counting algorithm~\cite{misra1982finding} and maintains frequently accessed row counters completely within the memory controller.
ABACuS makes a key observation that \ous{9}{many workloads (both benign workloads and RowHammer attacks) tend to access DRAM rows with the same row address in multiple DRAM banks at \emph{around the same time} because i) modern memory address mapping schemes interleave consecutive cache blocks across different banks and ii) workloads tend to access cache blocks in close proximity around the same time due to the spatial locality in their memory accesses~\cite{olgun2024abacus}}.\omcomment{9}{Odd writing. Is this correct?}\ouscomment{9}{It is correct. But changed writing for better clarity and less ambiguity.}
ABACuS exploits this observation by maintaining a single counter across \om{9}{\emph{all}} banks instead of maintaining a counter \om{9}{\emph{per}} bank (e.g., ~\cite{park2020graphene,bostanci2024comet}).
By doing so, ABACuS accurately tracks activation counts at significantly reduced area overhead~\cite{olgun2024abacus}.
We implement ABACuS in Ramulator 2.0~\cite{ramulator2github, chronusgithub} and follow the same methodology in \secref{sec:methodology}.
We use the address mapping described in the ABACuS paper \om{9}{(see §9 of~\cite{olgun2024abacus})} instead of RoBaRaCoCh used in \secref{sec:evaluation}.

\head{System Performance}
\figref{fig:abacusperf} presents the performance overheads of \X{} and ABACuS across \param{60} benign \param{four}-core workloads for \gls{nrh} values from \param{1K} to \param{20}.
x and y axes respectively show the \gls{nrh} values and system performance in terms of weighted speedup normalized to a baseline with \emph{no} read disturbance mitigation (higher y value is better).
Each colored bar depicts the mean system performance of a mechanism across 60 four-core workloads and error bars show the standard error of the mean across 60 four-core workloads.

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{figures/abacusperf.pdf}
\caption{Performance impact of \X{} and ABACuS on 60 benign four-core workloads}
\label{fig:abacusperf}
\end{figure}

We make \param{three} observations from \figref{fig:abacusperf}.
First, \X{} outperforms ABACuS across all evaluated \gls{nrh} values.
Second, as \gls{nrh} decreases from 1K to 20, ABACuS's performance overhead increases from \param{1.7}\% to \param{26.4}\%.
In contrast, \X{}'s performance overhead increases from \param{<0.1}\% to \om{9}{only} \param{3.2}\%.
Third, \X{}'s system performance overhead decreases with ABACuS's address mapping.
For example, at $\nrh{} =$ 20, \X{}'s performance overhead decreases from \param{17.9}\% (\ous{9}{shown in \figref{fig:benign_scaling}}) to \param{3.2}\% when the address mapping changes from RoBaRaCoCh to ABACuS's mapping.
This is likely due to ABACuS's mapping \om{9}{leading to a lower row conflict rate} by better interleaving rows across banks.\footnote{\ous{9}{Reduced row conflict rate in ABACuS's mapping increases baseline system performance. When a read disturbance mitigation mechanism is present, the benefits can be further improved because DRAM rows are activated fewer times on average, and thus the mitigation mechanism can perform fewer preventive refreshes. We do \emph{not} use ABACuS's mapping in our main evaluation (\secref{sec:evaluation}) because each mapping differently affects different read disturbance mechanisms. For example, Hydra's~\cite{qureshi2022hydra} system performance overhead significantly (\param{>10}\%) increases with ABACuS's mapping compared to RoBaRaCoCh (not shown). We leave a rigorous evaluation of the memory address mapping's effect on different read disturbance mitigation mechanisms to future work.}}

\head{Storage}
\figref{fig:abacusstorage} shows the storage overhead in MiB (y axis) of the evaluated read disturbance mitigation mechanisms as \gls{nrh} decreases (x axis).
We evaluate the storage usage of \ous{9}{\X{} (DRAM) and ABACuS (CAM+SRAM in CPU)} as a function of \gls{nrh} for a DRAM module with \param{64} banks and \param{128K} rows per bank.

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{figures/abacusstorage.pdf}
\caption{Storage used by \X{} (DRAM) and \ous{9}{ABACuS (CAM+SRAM in CPU)} as a function of RowHammer threshold for a DRAM module with \param{64} banks and \param{128K} rows per bank}
\label{fig:abacusstorage}
\end{figure}

\ous{9}{We make \param{two} observations from \figref{fig:abacusstorage}.
First, ABACuS's storage overhead in CPU is significantly lower compared to \X{}'s storage overhead in DRAM.
However, \X{} overhead is completely within the DRAM chip where the per-bit storage hardware complexity is relatively low.
Second, as \gls{nrh} decreases from \param{1K} to \param{20}, ABACuS’s storage overhead in CPU increases from 8KB to 324KB.
This is because as \gls{nrh} decreases, ABACuS needs to track many more rows and thereby requires more counters (implemented as CAM and SRAM)}.