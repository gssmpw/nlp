\section{Extended Experiment Details and Ablation Studies} \label{section:ablation_studies}
In this section, we present extended experiment details regarding the results presented in the main body of the paper. To isolate the individual effects of regret and novelty, we conduct an ablation study in which only novelty is used to prioritize levels in the level buffer. We denote the CENIE-augmented versions of the PLR$^\perp$ and ACCEL, which use only novelty for level prioritization (set $\alpha=1$ in Equation \ref{eq:level_replay_weightage}), as PLR-CENIE$^\dag$ and ACCEL-CENIE$^\dag$, respectively.

Interestingly, we observed that in several instances, the ablation models, PLR-CENIE$^\dag$ and ACCEL-CENIE$^\dag$, demonstrated comparable or even superior zero-shot transfer performance compared to their regret metric counterparts, PLR$^\perp$ and ACCEL. This finding suggests that, in specific scenarios, prioritizing training levels based on novelty alone can effectively shape curricula. This is especially notable because our GMM-based novelty metric, unlike regret, does not rely on predefined domain-specific reward structures; rather, it is derived solely from the agent’s trajectory data across different levels. 

However, it is important to note that these ablation results do not imply that regret should be entirely replaced by novelty-based level selection. Novelty alone may encounter limitations in extremely large state-action spaces where a balance with regret is essential for effective exploration. By combining novelty and regret in CENIE to shape the training curriculum, we significantly enhance the agent’s generalization capabilities beyond those of previous algorithms, as shown in our main experiments. This finding highlights the powerful synergy between CENIE’s novelty metric and traditional regret-based approaches, resulting in a more robust and effective training paradigm.


\subsection{Minigrid Domain}
After training all the student agents for 30k PPO updates ($\sim$250M steps), we evaluate their transfer capabilities on eight held-out testing environments (see the first row in Figure \ref{fig:mg_ablation_results}). We summarize all the results in Figure \ref{fig:mg_ablation_results}. In addition to the zero-shot transfer evaluation, we summarize the students' aggregate zero-shot transfer performance, i.e., IQM and Optimality Gap, in Figure \ref{fig:mg_ablation_iqm}.

In Figure \ref{fig:mg_ablation_results}, PLR-CENIE$^\dag$ outperforms PLR$^\perp$ in most of the testing environments (6 out of 8), indicating that the novelty metric plays a more significant role compared to regret in the Minigrid domain for the PLR$^\perp$ algorithm. In contrast, ACCEL shows a marginal performance advantage over ACCEL-CENIE$^\dag$ in the testing environments, with two wins, four losses, and two ties. Importantly, for both cases -- PLR-CENIE and ACCEL-CENIE -- the combination of both regret and novelty yields the strongest performance, outperforming their individual metric counterparts. This finding supports the assertion that the CENIE framework effectively complements the regret metric, helping UED algorithms achieve better performance.

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/cenie/ablation/mg_ablation_results.png}
  \caption{Zero-shot transfer performances in Minigrid. The plots are based on the median and interquartile range of solved rates across 5 independent runs. All student models are evaluated after 30k student PPO updates.}
  \label{fig:mg_ablation_results}
\end{figure}

The aggregate IQM and Optimality Gap results shown in Figure \ref{fig:mg_ablation_iqm} further validates the above conclusion. ACCEL-CENIE and PLR-CENIE outperform their counterparts -- (ACCEL, ACCEL-CENIE$^\dag$) and (PLR$^\perp$, PLR-CENIE$^\dag$) -- in terms of both IQM and Optimality Gap. In particular, within the PLR$^\perp$ framework, the novelty-driven level selection strategy (PLR-CENIE$^\dag$) significantly surpasses the regret-based approach (PLR$^\perp$) in performance.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\linewidth]{figures/cenie/ablation/mg_ablation_iqm.png}
  \caption{IQM and Optimality Gap ablations in Minigrid domain. Results are measured across 5 independent runs.}
  \label{fig:mg_ablation_iqm}
\end{figure}

We also provide a qualitative analysis of the effect of the novelty metric on the level replay buffer of PLR-CENIE in Minigrid for the experiments detailed under Section~\ref{subsection:minigrid} in the main body. Specifically, we highlight levels that feature the lowest regret (bottom 10) yet exhibit the highest novelty (top 10); these are showcased in the first row of Figure ~\ref{fig:low_regret_high_novelty}. Conversely, levels that score within the lowest 10 for both regret and novelty are displayed in the second row of the same figure. Visually, we observe that levels with high novelty and low regret present complex and diverse scenarios that challenge the student. In contrast, the levels displayed in the second row, characterized by low regret and low novelty, often resemble simple, empty mazes that offer limited learning opportunities. While it is not feasible to present every example level here, the contrast between the two groups is stark. Levels selected based on low regret but high novelty are significantly more varied and intricate than those chosen for their low novelty, despite both groups having low regret scores. This demonstrates that incorporating novelty alongside regret in the selection process enhances the ability to identify levels that present more interesting trajectories (experiences) for the student to learn from.
\begin{figure} [H]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/cenie/low_regret_high_novelty.png}
    \caption{Levels in the level replay buffer of PLR-CENIE. X-axis: number of student PPO updates.}
    \label{fig:low_regret_high_novelty}
\end{figure}


\subsection{BipedalWalker Domain} \label{section:bipedal-walker-extended}
We closely tracked the evolution of state-action space coverage during the training to reveal how the incorporation of a novelty objective affected the curriculum generation. State-action pairs encountered by the agent during training are collected for PLR$^\perp$, ACCEL, PLR-CENIE, and ACCEL-CENIE. Given the high-dimensionality of the state-action pairs in the BipedalWalker domain, we employed t-distributed Stochastic Neighbor Embedding (t-SNE;~\cite{van2008visualizing}), a nonlinear dimensionality reduction technique, to project the state-action pairs onto a more manageable two-dimensional manifold. t-SNE captures much of the local structure of the high-dimensional data, while also revealing global structures, such as the presence of clusters at several scales~\cite{van2008visualizing,wattenberg2016use}. The resulting embedded state-action pairs are mapped onto a 2-D scatterplot, allowing us to visualize the exploration of the state-action space by each algorithm as the number of policy updates increases. The evolution is illustrated in Figure \ref{fig:cov_accel_evolution}. 

Furthermore, we quantified the occupancy of the 2-D scatterplot by each method. To achieve this, we discretized the scatterplot into cells and computed the percentage of total cells occupied by data points generated by each method. Table \ref{tab:state_action_coverage_percentage} in the main paper presents the state-action space coverage percentages for each method. Notably, both PLR-CENIE and ACCEL-CENIE exhibit significantly broader coverage of the state-action space compared to their predecessors. This evidence supports the assertion that the outperformance of CENIE-augmented algorithms is associated with the broader coverage of the state-action space. Note that although the PLR-based algorithms exhibit higher state-action space coverage, they show poorer transfer performance compared to ACCEL-based algorithms. This discrepancy is likely because ACCEL initiates the curriculum with ``easy'' levels, and gradually introducing complexity via minor mutations, whereas PLR relies on DR, which lacks the fine-grained control over difficulty progression that ACCEL's mutation-based method offers. As a result, while CENIE enhances state-action space coverage for both ACCEL and PLR, it is likely that ACCEL's gradual complexity introduction mechanism capitalizes on this enhancement more effectively.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/cenie/bw_cov_evolution.png}
  \caption{Evolution of the state-action space coverage of ACCEL-CENIE, ACCEL, PLR-CENIE, and PLR for a seed. The checkpoints are 1k, 10k, 20k, and 30k policy updates during the training.}
  \label{fig:cov_accel_evolution}
\end{figure}

To plot the level difficulty composition of the replayed levels by ACCEL and ACCEL-CENIE in Figure~\ref{fig:level_composition} of the main paper, we adapted the difficulty thresholds originally defined in ~\citet{wang2019poet}. This is because their thresholds were designed for a smaller 5-D encoding BipedalWalker environment, whereas our setting uses an 8-D encoding, which allows for higher complexity of levels to be generated. Specifically, we introduced an additional threshold for maximum stairs height, as shown in Table~\ref{tab:difficulty_thresholds_bw}. A level is classified as Easy if it meets none of the thresholds, and as Moderate, Challenging, Very Challenging, or Extremely Challenging if it meets one, two, three, or four thresholds, respectively.

\begin{table}[h]
\caption{Environment encoding thresholds for 8D BipedalWalker.}
\label{tab:difficulty_thresholds_bw}
\centering
\begin{tabular}{cccc}
\toprule
\textbf{Stump Height (High)} & \textbf{Pit Gap (High)} & \textbf{Ground Roughness} & \textbf{Stairs Height (High)} \\ 
\midrule
$\geq 2.4$ & $\geq 6$ & $\geq 4.5$ & $\geq 5$ \\ 
\bottomrule
\end{tabular}
% \vspace{-1em}
\end{table}

Note that our Figure~\ref{fig:level_composition} differs from Figure 12 in \citet{parker2022evolving} which shows the difficulty distribution of the levels \textbf{generated and added into the buffer}, but not the actual levels selected by the teacher for the student to \textbf{replay/train on}. Also, their figure is defined for the 5D encoding setting. On that note, this also demonstrates that CENIE remedies an inefficiency in the original ACCEL algorithm, where mutation-based generation is capable of producing high complexity levels but are not selected for student training due to solely depending on regret for level prioritization. 

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/cenie/ablation/bw_ablation_results.png}
  \caption{Zero-shot transfer return ablations in BipedalWalker domain. The plot is based on mean and standard error over 5 independent runs.}
  \label{fig:bw_ablation_results}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\linewidth]{figures/cenie/ablation/bw_ablation_iqm.png}
  \caption{IQM and Optimality Gap ablations in BipedalWalker domain. Results are measured across 5 independent runs.}
  \label{fig:bw_ablation_iqm}
\end{figure}

In addition to the state-action space coverage, we also conduct the ablation study in the BipedalWalker domain. We repeat the same experiment settings as in the Minigrid domain, where both ACCEL-CENIE$^\dag$ and PLR-CENIE$^\dag$ utilize only novelty to prioritize replay levels, and ACCEL-CENIE and PLR-CENIE integrate both novelty and regret for prioritization. We assess the algorithm performance with the same evaluations as in the main paper, providing both the transfer performance during training (Figure \ref{fig:bw_ablation_results}) and IQM and Optimality Gap (Figure \ref{fig:bw_ablation_iqm}). 

Summarizing the observations from both Figure \ref{fig:bw_ablation_results} and Figure \ref{fig:bw_ablation_iqm}, we observe that novelty-driven level replay selection exhibits a similar effect as regret on PLR$^\perp$ but is not as effective as regret on ACCEL. PLR-CENIE$^\dag$ performs on par with the regret metric counterpart (i.e., PLR$^\perp$) while ACCEL-CENIE$^\dag$ is outperformed by ACCEL and ACCEL-CENIE in this domain. The observations differ from the ablation studies conducted in the Minigrid domain. This discrepancy is possibly due to the greater importance of exploration in Minigrid, which features a sparse reward setting, compared to the dense reward, continuous control domain of BipedalWalker.

\subsection{CarRacing Domain}
To monitor the evolution of the students' transfer performance, we evaluate the students every 100 PPO updates on four racing tracks throughout the training period and plot the results in Figure \ref{fig:car_racing_plot_results}. PLR-CENIE outperforms both its predecessor, PLR$^\perp$, and the state-of-the-art algorithm, DIPLR, in the CarRacing domain. 

Since both DIPLR and PLR-CENIE achieve near-optimal performance on the four testing tracks, we conduct a more extensive and rigorous evaluation by measuring the students' transfer performance on 20 human-designed F1 racing tracks from \citet{jiang2021replay}. We also include the ablation model, PLR-CENIE$^\dag$ which uses novelty alone to prioritize replay levels, in our evaluation. The detailed results of each algorithm are listed in Table \ref{tab:carracing_all_results}. For better visualization and straightforward comparison, we plotted the IQM and Optimality Gap performances in Figure \ref{fig:car_racing_abaltion_iqm}.

\begin{table}[ht]
\caption{Test returns of each method on all the CarRacing F1 benchmarks. Results are measured
across 5 runs at 2.75k PPO updates and 50 trials per track. Bold indicates being
within one standard error of the best mean. Observe that PLR-CENIE consistently outperforms
the other algorithms or matches the best-performing algorithm. PLR-CENIE$^\dag$ is the ablation model.}
\label{tab:carracing_all_results}
\centering
\begin{tabular}{llllllll}
\toprule
      Track &       DR &  Minimax &   PAIRED &    DIPLR &      PLR$^\perp$  & \begin{tabular}[c]{@{}l@{}}PLR-\\CENIE\end{tabular} & \begin{tabular}[c]{@{}l@{}}PLR-\\CENIE$^\dag$\end{tabular} \\
\midrule
  Australia &  304$\pm$133 &  107$\pm$97 & 224$\pm$173 &  \textbf{715$\pm$50} &  574$\pm$69 &   \textbf{745$\pm$32} & 616$\pm$45 \\
    Austria &  299$\pm$118 & 152$\pm$106 & 159$\pm$160 &  \textbf{587$\pm$49} &  458$\pm$44 &   \textbf{566$\pm$38} & 496$\pm$46 \\
    Bahrain &  208$\pm$136 &  44$\pm$101 & 118$\pm$159 &  \textbf{514$\pm$48} &  377$\pm$75 &   \textbf{537$\pm$58} & 453$\pm$38 \\
    Belgium &  225$\pm$104 &  131$\pm$87 & 110$\pm$100 &  440$\pm$31 &  362$\pm$36 &   \textbf{500$\pm$41} & 436$\pm$35 \\
     Brazil &  192$\pm$106 &   57$\pm$61 & 147$\pm$124 &  \textbf{451$\pm$39} &  368$\pm$42 &   \textbf{485$\pm$27} & 312$\pm$40 \\
      China &  -35$\pm$57 &  -29$\pm$80 &  -71$\pm$63 &  93$\pm$102 &  -23$\pm$28 &  \textbf{278$\pm$100} & \textbf{281$\pm$52} \\
     France &  124$\pm$111 &  48$\pm$129 &   8$\pm$126 &  487$\pm$75 &  311$\pm$98 &   \textbf{564$\pm$65} & 435$\pm$97 \\
    Germany &  172$\pm$105 &  94$\pm$100 &    2$\pm$97 &  \textbf{477$\pm$59} &  358$\pm$35 &   \textbf{512$\pm$80} & \textbf{500$\pm$82} \\
    Hungary &  319$\pm$155 & 133$\pm$113 & 139$\pm$161 &  \textbf{686$\pm$50} &  597$\pm$72 &   \textbf{678$\pm$40} & 604$\pm$70 \\
      Italy &  267$\pm$114 &  204$\pm$89 & 198$\pm$135 &  676$\pm$30 &  559$\pm$63 &   \textbf{708$\pm$26} & 588$\pm$34 \\
   Malaysia &  142$\pm$107 &   39$\pm$94 &  51$\pm$104 &  404$\pm$30 &  265$\pm$44 &   \textbf{469$\pm$79} & 338$\pm$22 \\
     Mexico &  331$\pm$199 & 193$\pm$123 & 102$\pm$169 &  \textbf{675$\pm$24} &  570$\pm$76 &   \textbf{674$\pm$51} & 602$\pm$57 \\
     Monaco &  80$\pm$78 &  100$\pm$94 &  34$\pm$111 & 369$\pm$122 & 139$\pm$112 &   \textbf{641$\pm$46} & 476$\pm$96 \\
Netherlands &  143$\pm$109 &  104$\pm$95 &   42$\pm$77 &  \textbf{540$\pm$34} &  400$\pm$61 &   \textbf{558$\pm$59} & 403$\pm$84 \\
   Portugal &  174$\pm$118 &   39$\pm$94 &  88$\pm$153 &  412$\pm$22 &  353$\pm$27 &   \textbf{495$\pm$66} & 394$\pm$43 \\
     Russia &  343$\pm$151 & 118$\pm$105 & 204$\pm$163 &  \textbf{609$\pm$60} &  \textbf{644$\pm$31} &   \textbf{594$\pm$58} & 550$\pm$60 \\
  Singapore &  209$\pm$108 &   75$\pm$93 &  88$\pm$153 &  \textbf{479$\pm$78} &  423$\pm$51 &   \textbf{530$\pm$48} & 454$\pm$55 \\
      Spain &  296$\pm$133 & 181$\pm$110 & 249$\pm$157 &  \textbf{619$\pm$39} &  517$\pm$41 &   \textbf{588$\pm$43} & 499$\pm$39 \\
         UK &  303$\pm$127 & 187$\pm$101 & 194$\pm$156 &  \textbf{558$\pm$49} &  443$\pm$45 &   \textbf{562$\pm$26} & 506$\pm$36 \\
        USA & 173$\pm$95 &   -2$\pm$84 &   2$\pm$161 & 191$\pm$110 &  155$\pm$90 &  \textbf{416$\pm$143} & 363$\pm$61 \\ 
\midrule
       Mean &  214$\pm$115 &   99$\pm$92 & 105$\pm$132 &  499$\pm$20 &  392$\pm$28 &   \textbf{553$\pm$32} & 465$\pm$42 \\
\bottomrule
\end{tabular}
\end{table}

From both Table \ref{tab:carracing_all_results} and Figure \ref{fig:car_racing_abaltion_iqm}, we observe that the ablation model, PLR-CENIE$^\dag$, outperforms PLR$^\perp$ by a significant margin, indicating that novelty is more important for level replay prioritization than the regret metric in PLR$^\perp$ for the CarRacing domain. Moreover, PLR-CENIE surpasses DIPLR and achieves state-of-the-art transfer performance in the CarRacing domain by effectively combining the strength of both novelty and regret.  

\begin{figure}[ht]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/cenie/ablation/cr_ablation_results.png}
  \caption{Complete training process of each algorithm on four CarRacing test environments. Plots show mean and standard error over 5 independent runs, with an evaluation interval of 100 PPO updates.}
  \label{fig:car_racing_plot_results}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.7\linewidth]{figures/cenie/ablation/cr_ablation_iqm.png}
  \caption{IQM and Optimality Gap ablations on the full CarRacing benchmark (20 F1 tracks). Results are measured across 5 independent runs after 2.75k PPO updates.}
  \label{fig:car_racing_abaltion_iqm}
\end{figure}

\section{Extended Related Work} \label{section:extended_related_work}
\paragraph{Curiosity-driven Approaches in RL} CENIE and curiosity-driven RL~\cite{schmidhuber1991,singh2010} share a conceptual similarity in leveraging novelty or unfamiliarity to guide learning. However, they differ significantly in their application and theoretical foundations. Curiosity-driven learning seeks to quantify ``curiosity" as an intrinsic reward for the agent such that it learns to prioritize the exploration of interesting experiences within a static environment~\cite{pathak2017curiosity}, or across a set of predefined tasks~\cite{burda2018large}. In contrast, CENIE is an autocurricula approach that focuses on curating environments interesting or useful for the agent's learning, shaping the learning curriculum itself rather than the exploration reward signal. This distinction is analogous to the difference between Prioritized Experience Replay~\cite{schaul2015prioritized} in traditional RL and Prioritized Level Replay~\cite{jiang2021prioritized} in UED. The former is an ``inner-loop'' method prioritizing past experiences for training, while the latter is an ``outer-loop'' method using past experiences to inform the collection/generation of future experiences. Similarly, curiosity-driven learning prioritizes novel experiences for policy updates, whereas CENIE focuses on generating and curating levels that induce these novel experiences. This fundamental difference in purposes makes theoretical and empirical comparisons between curiosity-driven approaches and CENIE less direct. However, many of the previous works in curiosity-driven RL provide inspiration for the CENIE framework. Specifically, curiosity-driven RL methods often seek to represent the ``visitation counts" of state-action to shape the intrinsic reward. The use of GMMs to model state-action space coverage in CENIE is motivated by successes in curiosity-driven RL approaches which have tackled the counting problem using density models. Notably, density models have been flexibly used to model state-action visitations for both large discrete state-action spaces (via pseudo-counts~\cite{bellemare2016,ostrovski2017count}) and continuous state-action spaces~\cite{zhao2019maximum,zhao2020curiositydriven}.

\paragraph{Automatic Curriculum Learning} UED is related to {\em Automated Curriculum Learning} (ACL;\cite{portelas2021acl}), which emcompasses a family of mechanisms that automatically adapt the distribution of training data by selecting learning situations tailored to the capabilities of DRL agents. Many ACL methods prioritize sampling of environment instances where the agent achieves high {\em learning progress} (LP). A particular relevant method in this space is {\em ALP-GMM}, introduced by \citet{portelas2019alpgmm}. ALP-GMM operates by periodically fitting a Gaussian Mixture Model to a dataset of previously sampled environment parameters, each associated with an Absolute Learning Progress (ALP) score. The approach employs an EXP4~\cite{auer2002exp4} bandit algorithm to select Gaussians as arms, with each Gaussian’s utility defined by its ALP score. ALP-GMM's approach to fitting multiple GMMs using different number of Gaussian components and keeping the best one inspired our GMM approach. However, they evaluate the GMM's quality using the Akaike's Information Criterion~\cite{bozdoganModelselection1987} (AIC). AIC introduces a penalty for the number of parameters in the model (which increases with the number of Gaussian components and dimensions of the data). This penalizes GMMs with more components, which may not be ideal for accurately modeling well-separated clusters in the state-action space which is crucial for identifying sparse regions and estimating novelty. To address this, our work uses the silhouette score~\cite{rousseeuw1987silhouettes} instead, which better evaluates clustering quality by considering both intra-cluster cohesion and inter-cluster separation, making it better suited for modeling novelty in state-action spaces. Additionally, ALP-GMM uses GMMs to sample environment parameters that are likely to yield high ALP scores, aligning task difficulty with the agent’s progress. This approach contrasts with how GMMs are used in the CENIE framework, where the goal is to model the novelty of an environment based on state-action coverage, independent of specific environment parameters. ACL methods like ALP-GMM generally assume a predefined target task distribution. This differs from the UED framework which only requires only an underspecified task space, i.e. $\theta$ in the UPOMDP formalism. UED seeks to directly maximize the student’s robustness over any possible environments, even those that are out-of-distribution from training. Similarly, CENIE provides a general approach for quantifying novelty using state-action coverage, without relying on any predefined task distribution. Due to this generality, we believe the CENIE framework holds significant potential for crossover applications in the ACL domain, providing a robust method for assessing and prioritizing environment novelty to enhance curriculum learning.

\paragraph{Open-endedness and Novelty Search in Evolutionary Computation} Long-running UED processes in expansive UPDOMPs closely resemble continual learning in open-ended domains. As such, UED is fundamentally connected to the fields of {\em open-endedness}~\cite{stanley2015} and evolutionary computation. When the task space allows for unbounded complexity, autocurricula methods such as UED offer promising pathways to open-endedness by co-evolving an adaptive, infinite set of tasks for the agent. Traditionally, learning without extrinsic rewards or fitness functions has been studied in evolutionary computation where it is referred to as ‘novelty search’~\cite{lehman2008,lehman2011}. In novelty search, the novelty of an agent’s behavior is typically quantified by measuring the distance between a user-defined feature or behavioral descriptor and its nearest neighbor in the population. Consistent with our findings, the open-ended learning literature has long recognized that high-performing solutions often emerge not through fitness optimization alone but through novelty-driven exploration. Despite these parallels, novelty search in environment design remains underdeveloped. Early work such as POET~\cite{wang2019poet} and its successor~\cite{wang2020enhanced} in open-ended RL have started drawing connections, linking environment design with principles of open-ended exploration. However, these approaches rely on a population of agents and distance-based novelty measures that lack curriculum-awareness; they do not adapt to the specific experiences induced by the curriculum nor improve the agent’s sample efficiency in reducing uncertainty across the state-action space. More recent work by \citet{zhang2024omni} proposed to leverage foundation models to quantify human notions of ``interestingness" (e.g. tasks that are both novel and worthwhile) in order to narrow the environment search space. It is unclear how to combine the insights from ~\citet{zhang2024omni} and this paper. Integrating these insights with our work presents an intriguing challenge. On one hand, CENIE provides a principled, general approach to quantifying novelty through state-action coverage, circumventing the need for subjective evaluations of ``interestingness" using foundation models. On the other hand, \citet{zhang2024omni} points out critical pitfalls in novelty search, such as the potential for agents to exploit novelty measures, generating superficial variations that fail to yield genuinely meaningful insights. This highlights numerous exciting research directions for aligning novelty search with the concept of ``interestingness," potentially combining the strengths of principled coverage-based novelty measures with more nuanced assessments of task value.


\section{Future Work and Limitations} \label{section:future_work_limitations}
In this paper, we demonstrated the application of GMMs to quantify the novelty of environments generated under the UED paradigm. We then validated the effectiveness of this novelty metric in prioritizing levels. Nevertheless, our work has some limitations. First, while we demonstrated the utility of the CENIE framework for novelty quantification and level prioritization, we did not explore its potential for directly generating novel environments. We anticipate that with creative manipulations, the GMM likelihood scores could directly inform level generation, either through a principled level generator (as in PAIRED) or by guiding mutations (as in ACCEL). This approach may lead to a more sample-efficient generation process, reducing the variance inherent in random generation.

Second, we did not experiment with alternative weightings between regret and CENIE’s novelty in level replay prioritization, as our experiments used a fixed 0.5-0.5 weighting (as in Eq.\ref{eq:level_replay_weightage}). We hypothesize that tuning these weights based on domain characteristics, such as the required level of exploration or reward sparsity, could improve performance. Additionally, employing dynamic weighting schemes, such as linearly decaying weight adjustments or adaptive strategies based on the agent’s learning progress, may further enhance curriculum optimization.

Third, GMM-based clustering may encounter challenges due to the curse of dimensionality in high-dimensional state-action spaces. While our current CENIE-augmented algorithms demonstrated significant improvements, future work could explore dimensionality reduction techniques, such as Principal Component Analysis (PCA;~\cite{pearson1901pca}) or t-distributed Stochastic Neighbor Embedding (t-SNE;~\cite{van2008visualizing}), to improve coverage representation in higher-dimensional settings. However, even with dimensionality reduction, such representations may still struggle in environments where the observations contain exogenous information irrelevant to the agent’s control. Specifically, although our experiments showed strong empirical gains in simplified environments, the current approach is vulnerable to the noisy TV problem~\cite{burda2018explorationrandom}, where novelty-driven level prioritization may focus on unpredictable noise elements of the environment, rather than beneficial learning experiences. This limitation highlights the importance of balancing level prioritization between novelty and regret to ensure the agent focuses on genuinely novel environments rich in learning potential. 

Furthermore, effective state representation is crucial for the CENIE framework. The CENIE framework is not restricted to raw state inputs; it can operate on indirect encodings, such as latent-space representations obtained from a generative model of the environment. This approach would allow CENIE to capture action-relevant information and necessary temporal dependencies between states, providing a more focused basis for novelty estimation. We expect that density-based novelty estimation could improve further by using latent representations from more expressive generative models, such as Variational Autoencoders (VAEs)~\cite{kingma2022autoencodingvariationalbayes}, which can capture richer, more informative structures in the state space.

Finally, while we used GMMs for environment novelty quantification, the CENIE framework is not limited to this model, as mentioned in the main body of this paper. GMMs may face limitations in capturing more complex distributions in real-world settings, and our choice of GMMs was primarily intended to illustrate the empirical benefits of quantifying novelty using state-action coverage in simpler environment settings. It is important to point out that fitting multiple GMM on the updated state-action coverage distribution and selecting the best one every rollout can incur additional computational costs. For future work aiming to replicate our approach, exploring periodic refitting (similar to the strategy used in ALP-GMM) could be worthwhile, as it may achieve comparable effectiveness while significantly reducing computational demands. Future work could also investigate more advanced density models, such as Variational Gaussian Mixture Models~\cite{blei2006variational}, Deep Gaussian Mixture Models~\cite{viroli2017deepgaussianmixturemodels}, or Normalizing Flow Models~\cite{rezende2016variational}. Additionally, there may be alternative approaches beyond density models for representing state-action coverage that could further enhance CENIE’s effectiveness. We believe there are many promising directions for the CENIE framework, and we leave these potential extensions to future work.


\section{Implementation Details} \label{section:implementation_details}
In this section, we provide the details about the experiments and implementations, including domain properties and additional information about CENIE and the baseline algorithms. All of our experiments are run with a single V100 GPU or GeForce 3090 GPU, using 10 Intel Xeon E5-2698 v4 CPUs. The baseline algorithms and evaluation environments are implemented using the \texttt{DCD} codebase provided by~\citet{jiang2021replay,parker2022evolving}. The CENIE framework and our current evaluations build upon and significantly extend a preliminary version of our work~\cite{jayden2024unifying}, where the framework was initially named ``GENIE." We have since enhanced the framework and opted to rename it to CENIE, following the release of a similarly-named, related work by \citet{bruce2024genie}, which appeared around the same time. This change was made to distinguish our contributions clearly and avoid confusion within the research community.


\subsection{Fitting Gaussian Mixture Models}\label{section:fit_gaussian_mixtures}
In this section, we provide more details about the GMM fitting process that was absent from the main body. Given an initial buffer containing past state-action pairs, $\Gamma$, and a selected number of Gaussians, $K$, we first use the {\em k-means++} algorithm to perform a fast and efficient initialization of the GMM parameters~\cite{blomer2013simple, arthur2007k}, $\lambda_\Gamma=\left\{(\alpha_1, \mu_1, \Sigma_1), ..., (\alpha_K, \mu_K, \Sigma_K)\right\}$. We then optimize $\lambda_{\Gamma}$ using the Expectation Maximization (EM) algorithm~\cite{dempster1977maximum,redner1984mixture}. The EM algorithm uses the initial values $\lambda_{\Gamma}$ to estimate a new $\lambda_{\Gamma}'$ such that $P(X|\lambda_{\Gamma}') > P(X|\lambda_{\Gamma})$. This process is repeated iteratively until some convergence threshold is fulfilled. Each iteration of the EM algorithm can be separated into the E-step and M-step. In E-step, the posterior probability for each component $i$ generating the sample point $x_t$ is denoted by $w_{t,i}$,
\begin{align}
w_{t,i} = P(i|x_t) = \frac{\alpha_i \mathcal{N}(x_t|\mu_i,\sigma_i)}{\sum^{K}_{i}\alpha_i \mathcal{N}(x_t|\mu_i,\sigma_i)} \nonumber
\end{align}
where $t=1,2,...,N$ and $i=1,2,...,K$. M-step computes the maximum likelihood estimation (MLE) using $w_{t,i}$ following re-estimation formulas which are derived from the partial derivatives of the log-likelihood functions and guarantee a monotonic increase in the model’s likelihood value. 
\begin{align}
\alpha_i &= \frac{1}{N}\sum^{N}_{i=1} w_{t,i}, \quad 
\mu_i = \frac{\sum^{N}_{i=1} w_{t,i}}{\sum^{N}_{i=1} w_{t,i}} x_t \nonumber \\
\sigma_i &= \frac{\sum^{N}_{i=1} w_{t,i}}{\sum^{N}_{i=1} w_{t,i}}(x_t-\mu_i)(x_t-\mu_i)^T \nonumber 
\end{align}
We iteratively apply the E-step and M-step until the parameters converge, i.e., $||\lambda_{\Gamma}'-\lambda_{\Gamma}|| < \epsilon$, where $\epsilon$ is a small threshold.

As mentioned in the main paper, we deliberately employ a finite window for $\Gamma$ to account for the effects of catastrophic forgetting. This allows levels with state-action pairs encountered in the past but subsequently forgotten by the agent's policy to regain novelty and be included back in the agent’s training curriculum. Furthermore, to ensure effectiveness in clustering the state-action space, we utilized a semi-online GMM model that is able to adapt its number of Gaussians, i.e. $K$, to that of the highest silhouette score. 

We use the \texttt{PyCave}~\cite{pycave2022} Python library to fit the GMM using GPU acceleration, which also provides an efficient abstraction for the Expectation-Maximization (EM) algorithm. We use the \texttt{PyTorch Adapt}~\cite{Musgrave2022PyTorchA} Python library to calculate the silhouette scores. The hyperparameters for fitting the GMM for all domains are shown in Table \ref{tab:hyperparams_all}.

\subsection{CENIE-Augmented Algorithms}
Besides the algorithm for ACCEL-CENIE shown in the main paper under Algorithm~\ref{alg:accel_cenie}, we also provide the algorithm for PLR-CENIE here under Algorithm \ref{alg:plr_cenie}.

\begin{algorithm}[t]
    \caption{PLR-CENIE}
    \label{alg:plr_cenie}
    \textbf{Input}: Level buffer size $N$, \textcolor{blue}{Component range $[K_{\text{min}}$}, \textcolor{blue}{$K_{\text{max}}]$, FIFO window size $\mathcal{W}$}, random level generator $\mathcal{G}$ \\
    \textbf{Initialize}: Student policy $\pi_\eta$, level buffer $\mathcal{B}$, \textcolor{blue}{state-action buffer $\Gamma$}, \textcolor{blue}{GMM parameters $\lambda_{\Gamma}$}
    
    \begin{algorithmic}[1]
    \STATE Generate $N$ initial levels by $\mathcal{G}$ to populate $\mathcal{B}$ 
    \STATE Collect $\pi_\eta$'s trajectories on each level in $\mathcal{B}$ and fill up $\Gamma$ 
    
    \WHILE{not converged}
    \STATE Sample replay decision, $\epsilon \sim U[0, 1]$
    \IF {$\epsilon \geq 0.5$}
    \STATE Generate a new level $l_{\theta}$ by $\mathcal{G}$
    \STATE Collect trajectories $\tau$ on $l_{\theta}$, with stop-gradient $\eta_{\perp}$
    \begingroup
    \color{blue}
    \STATE {Compute novelty score for $l_{\theta}$ using $\lambda_{\Gamma}$} (Eq.\ref{eq:log_novelty} and Eq.\ref{eq:replay_prob})
    \endgroup
    \STATE Compute regret score for $l_{\theta}$ (Eq.\ref{eq:gae} and Eq.\ref{eq:replay_prob})
    \STATE Update $\mathcal{B}$ with $l_{\theta}$ if $P_{replay}(l_{\theta})$ is greater than that of any levels in $\mathcal{B}$ (Eq.\ref{eq:level_replay_weightage})
    \ELSE
    \STATE Sample a replay level $l_{\theta} \sim \mathcal{B}$ according to $P_{replay}$
    \STATE Collect trajectories $\tau$ on $l_{\theta}$
    \STATE Update $\pi_\eta$ with rewards $R(\tau)$
    \begingroup
    \color{blue}
    \STATE {Compute novelty score for $l_{\theta}$ using $\lambda_{\Gamma}$} (Eq.\ref{eq:log_novelty} and Eq.\ref{eq:replay_prob})
    \endgroup 
    \STATE Compute regret score for $l_{\theta}$ (Eq.\ref{eq:gae} and Eq.\ref{eq:replay_prob})
    \STATE Update $P_{replay}$ with novelty and regret scores
    \begingroup
    \color{blue}
    \STATE {Update $\Gamma$ with $\tau$ and resize to $\mathcal{W}$}
    \FOR {$k$ in range $K_{\text{min}}$ to $K_{\text{max}}$}
    \STATE Fit a GMM$_k$ with $k$ components on $\Gamma$ and compute its silhouette score
    \ENDFOR
    \STATE Select GMM parameters with the highest silhouette score to replace $\lambda_{\Gamma}$
    \endgroup
    \STATE Collect trajectories $\tau$ on $l_{\theta}$, with stop-gradient $\eta_{\perp}$ 
    \STATE Update $\mathcal{B}$ with $l_{\theta}$ if $P_{replay}(l_{\theta})$ is greater than that of any levels in $\mathcal{B}$ (Eq.\ref{eq:level_replay_weightage})
    \ENDIF
    \ENDWHILE
    \end{algorithmic}
\end{algorithm}




\subsection{Minigrid Domain}
% In Minigrid, the teacher agent creates maze instances consisting of a 15x15 grid, where each empty tile can be occupied by the agent (blue triangle), the goal (green block), an obstacle (gray block), or an empty space that the agent can navigate through. The student's state space includes its orientation and a 5*5 view in front of it. The student has three discrete actions, move forward, turn left, and turn right, and will stay in place if it hits an obstacle. The student agent needs to navigate to the goal grid within 256 steps and it only receives reward=(\textit{num-steps}/256) upon success (i.e., reaching the goal).
In the Minigrid domain, the teacher creates maze instances consisting of a $15 \times 15$ grid, where each empty tile can be occupied by the agent, the goal, an obstacle (i.e. block), or an empty space that can navigate through. The student is aware of its orientation and is limited by partial observability, i.e. it only has a $5 \times 5$ view in front of it. The student agent can only move forward and turn left/right, and will stay in place if it hits an obstacle. The student agent is implemented based on PPO \cite{schulman2017proximal} with an LSTM-based recurrent network structure to deal with partial observability. We use the LSTM hidden states as representations within our GMM, allowing the density model to capture temporal dependencies between states. The student agent receives a reward upon reaching the goal, where $H$ is the episode length and $H_{max}$ is the maximum length (set to 250 at training) for an episode. The agent receives a reward of $r=1-(num_{step}/H_{max})$ when it reaches the goal position and 0 if it fails to reach the goal. The collection of states in this domain depicts the scenarios the agent needs to navigate through. 

\subsection{BipedalWalker Domain}
In BipedalWalker, the teacher agent generates new levels by specifying the values of the eight environment parameters (e.g., ground roughness, number of stair steps, pit gap width, etc). As for the student agent, it needs to determine the torques applied on its joints and is constrained by partial observability where it only knows its horizontal speed, vertical speed, angular speed, positions of joints, etc. The student agent receives positive rewards as it walks towards the goal position and will receive a large negative penalty if it falls down. 
The BipedalWalker domain is modified on top of the BipedalWalkerHardcore environment from OpenAI Gym, introduced by \cite{wang2019poet} and improved by \cite{portelas2020teacher,parker2022evolving}. The student agent receives a 24-dimensional proprioceptive state corresponding to inputs from its lidar sensors, angles, and contacts, which also form the state representation for our GMM. The partial observability here means the agent does not have access to its positional coordinates. The environment parameters and their corresponding ranges are shown in Table \ref{tab:env_params_bw}. Note, there will be a singular value to specify Ground Roughness and the Number of Stair Steps, and a min and a max value to define the PitGap Width, Stump Height, and Stair Height, and thus we will have eight environment parameters in total.

\begin{table}[h]
    \caption{Environment parameters and their ranges in the BipedalWalker domain. To define PitGap, StumpHeight, and StairHeight, we need a min and a max value. Hence, there are a total of eight parameters.}
    \label{tab:env_params_bw}
    \centering
    \begin{tabular}{lrrrrr}
        \toprule
        \textbf{Parameter} & Roughness & Num of Stair Steps & PitGap Width & Stump Height & Stair Height \\
        \midrule
        Range        & [0,10] & [1,9] & [0,10] & [0,5] & [0,5]         \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.7\linewidth]{figures/bw_domain.png}
  \caption{Examples of testing levels in BipedalWalker domain. (a) BipedalWalker, (b) Hardcore, (c) Stair, (d) PitGap, (e) Stump, and (f)Roughness.}
  \label{fig:bw_domain}
\end{figure}

\subsection{CarRacing Domain}
The CarRacing domain was introduced and customized by \cite{jiang2021replay}. In CarRacing, the teacher creates tracks by using Bézier curves by 12 control points within a fixed radius of the center of the playfield. A track consists of a sequence of $L$ polygons and $L$ is fixed on the training tracks and varies on different testing tracks. While driving on the tracks, the student receives a reward equal to $1000/L$. The student additionally receives a reward of $-0.1$ at each time step. The student observes an RBG image of size (96x96x3), where (96x96) is the (height x width) of the observed image and 3 is the number of RGB channels. Consistent with previous UED literature, we employ a CNN model to preprocess the raw image observations. The CNN extracts high-level features from the raw images, reducing their dimensionality and capturing important spatial patterns that are crucial for understanding the environment's dynamics. By feeding these feature representations into the GMM model instead of the raw image input, we ensure that the density estimation focuses on meaningful aspects of the state space rather than being overwhelmed by the complexity and noise of raw pixel data. 

The 20 testing F1 tracks by \cite{jiang2021replay} have various track lengths and different maximum episode steps. As such, different min-max normalization ranges are used for each track to produce the IQM and Optimality Gap plots. We list the 20 test tracks and their corresponding min-max normalization ranges in Table \ref{tab:car_racing_min_max}.

\begin{table}[ht]
    \caption{Min-max ranges for different CarRacing F1 tracks that are used for IQM and Optimality Gap plotting.}
    \label{tab:car_racing_min_max}
    \centering
    \begin{tabular}{lrr}
        \toprule
        Track       & Episode Steps          & Min-Max Reward Range                     \\
        \midrule
        Australia   & \multirow{5}{*}{1500}  & \multirow{5}{*}{{[}-150, 850{]}}  \\
        Austria     &                        &                                   \\
        Belgium     &                        &                                   \\
        Italy       &                        &                                   \\
        Monaco      &                        &                                   \\
        \midrule
        Brazil      & \multirow{11}{*}{2000} & \multirow{11}{*}{{[}-200, 800{]}} \\
        China       &                        &                                   \\
        France      &                        &                                   \\
        Germany     &                        &                                   \\
        Hungary     &                        &                                   \\
        Netherlands &                        &                                   \\
        Russia      &                        &                                   \\
        Singapore   &                        &                                   \\
        Spain       &                        &                                   \\
        UK          &                        &                                   \\
        USA         &                        &                                   \\
        \midrule
        Bahrain     & \multirow{3}{*}{2500}  & \multirow{3}{*}{{[}-250, 750{]}}  \\
        Malaysia    &                        &                                   \\
        Portugal    &                        &                                   \\
        \midrule
        Mexico      & 3000                   & {[}-300, 700{]} \\
        \bottomrule
    \end{tabular}
\end{table}


\section{More Details On Baseline Algorithms} \label{app:baseline_algos}
In this section, we provide more technical details on some of the baseline algorithms used in our experiments, specifically Domain Randomization (DR), Minimax, PAIRED, PLR$^\perp$, DIPLR, and ACCEL. We summarize the key differences between the baseline algorithms in Table \ref{tab:comparison_of_UED_algorithms_1}.

\begin{table}[h]
\caption{Overview of the fundamental UED algorithms and CENIE-augmented algorithms.}
\label{tab:comparison_of_UED_algorithms_1}
\centering
\begin{tabular}{lllll}
\toprule
\textbf{Algorithm} & \begin{tabular}[c]{@{}l@{}}\textbf{Generation}\\ \textbf{Strategy}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{Generator}\\ \textbf{Obj}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{Curation}\\ \textbf{Obj}\end{tabular}  & \textbf{Setting} \\ 
\midrule
POET      & Mutation    & Minimax  & MCC   & Population           \\
PAIRED    & RL & Minimax Regret & None  & Single-agent   \\
PLR$^\perp$      & Random     & None    & Minimax Regret  & Single-agent         \\
DIPLR     & Random     & None    & \begin{tabular}[c]{@{}l@{}}Minimax Regret +\\Diversity\end{tabular} & Single-agent   \\
ACCEL     & \begin{tabular}[c]{@{}l@{}}Random +\\Mutation\end{tabular} & Minimax Regret  & Minimax Regret  & Single-agent  \\ 
\midrule
PLR-CENIE     & Random     & None & \begin{tabular}[c]{@{}l@{}}Minimax Regret +\\Novelty\end{tabular} & Single-agent   \\
ACCEL-CENIE    & \begin{tabular}[c]{@{}l@{}}Random +\\Mutation\end{tabular}     & \begin{tabular}[c]{@{}l@{}}Minimax Regret +\\Novelty\end{tabular}  & \begin{tabular}[c]{@{}l@{}}Minimax Regret +\\Novelty\end{tabular}  & Single-agent \\ 
\bottomrule
\end{tabular}
\end{table}

The Domain Randomization (DR) teacher uniformly randomizes each dimension in the environment parameter space to generate various environments.

The PAIRED teacher estimates regret by leverage two agents: an antagonist agent and a protagonist agent (student). In practice, PAIRED derives regret by taking the antagonist's ($A$) maximum performance and the protagonist's ($P$) average performance over several trajectories, allowing for more accurate approximations. Let $U^\pi(\tau)$ denote the total reward obtained by a trajectory $\tau$ produced by policy $\pi$ on the level $\theta$. Regret is measured in PAIRED via:
\begin{align*}
\normalfont\textsc{Regret}^\theta(\pi_A, \pi_P, \theta) =  \max_{\tau^A} U^\theta(\tau^A) - \mathbb{E}_{\tau^P}[U^\theta(\tau^P)]
\end{align*}
where $\pi^A$ and $\pi^P$ are the antagonist's policy and the protagonist's policy, respectively. PAIRED teacher constantly creates levels that are slightly beyond the ability range of the protagonist and within the ability range of the antagonist such that the regret is maximized. The pseudocode of the PAIRED algorithm is given in Algorithm \ref{alg:paired}.

\begin{algorithm}[H]
\caption{PAIRED}
\label{alg:paired}
\textbf{Input}:Randomly initialize Protagonist $\pi^P$, Antagonist $\pi^A$, and teacher $\Lambda$ \\
\textbf{Initialize}: replay buffers $\mathcal{B}$ 
\begin{algorithmic}[1]
\WHILE{not converge}
\STATE Use teacher to generate environment parameters: $\theta \sim \Lambda$. Use $\theta$ to create environments, $l_{\theta}$ \\
\STATE Collect Protagonist trajectory $\tau^P$ in $l_{\theta}$. Compute Protagonist's average return: $\mathbb{E}^{\theta}[V(\pi^P)]$ \\
\STATE Collect Antagonist trajectory $\tau^A$ in $l_{\theta}$. Compute Antagonist's average return: $\mathbb{E}^{\theta}[V(\pi^A)]$ \\
\STATE Compute regret: REGRET = $\mathbb{E}^{\theta}[V(\pi^A)]$ - $\mathbb{E}^{\theta}[V(\pi^P)]$ \\

\STATE Train Protagonist policy $\pi^P$ with RL update and reward = -REGRET \\
\STATE Train Antagonist policy $\pi^A$ with RL update and reward = REGRET  \\
\STATE Train teacher policy  with RL update and reward = REGRET 
\ENDWHILE
\end{algorithmic}
\end{algorithm}

However, the PAIRED algorithm faces several drawbacks~\cite{mediratta2023stabilizing}. Both the antagonist and protagonist policies are constantly updating, making the problem nonstationary. Furthermore, PAIRED suffers from a long-horizon credit assignment problem since the teacher must fully specify an environment before receiving a sparse reward in the form of feedback from the antagonist and protagonist agents. PLR seeks to circumvent this issue through the use of regret for prioritized selection of levels for replay rather than active generation. PLR uses {\em Positive Value Loss} (PVL), an approximation of regret based on Generalized Advantage Estimation (GAE;~\cite{schulman2015high}):
\begin{align*}
\normalfont\textsc{PVL}^\theta(\pi) &= \frac{1}{T} \sum_{t=0}^{T} \max \left( \sum_{k=t}^{T} (\gamma \lambda)^{k-t} \delta^{\theta}_k, 0 \right),
\end{align*}
where $\gamma$, $\lambda$ and $T$ are the MDP discount factor, GAE discount factor and MDP horizon, respectively. $\delta^{\theta}_k$ is the TD-error at time step $k$ for $\theta$. However, the use of PVL may introduce bias due to the bootstrapped value target. An alternate heuristic score function is {\em Maximum Monte Carlo} (MaxMC), which replaces the bootstrapped value target with the highest return observed on the level during training. By using this maximal return, the regret estimates become independent of the agent’s current policy:
\begin{align*}
\normalfont\textsc{MaxMC}^\theta(\pi) &= \frac{1}{T} \sum_{t=0}^{T} \left( R^{\theta}_{\text{max}} - U(\tau^\pi) \right),
\end{align*}
where $R^{\theta}_{\text{max}}$ is the maximal return of $\pi$ on $\theta$. We primarily focus on PVL because the original implementations of ACCEL and PLR$^\perp$ in \citet{jiang2021replay, parker2022evolving} found better success with the PVL scoring function for the experiments domains, i.e. Minigrid, BipedalWalker, and CarRacing, used in this paper. Future research could explore the potential of using the MaxMC scoring function to see if it yields different outcomes when combining ACCEL and PLR$^\perp$ with CENIE. The Diversity Induced Prioritized Level Replay (DIPLR~\cite{li2023effective}) algorithm extends PLR$^\perp$ by prioritizing level replay based on both regret and diversity. Here, diversity is quantified using the Wasserstein distance between the agent's trajectories across levels in the replay buffer. The limitations of DIPLR are highlighted in the main body (see Section \ref{section:related_works}). The pseudocode of DIPLR is provided in Algorithm \ref{alg:diplr}. 

\begin{algorithm}[H]
    \caption{DIPLR}
    \label{alg:diplr}
    \textbf{Input}: Level buffer size $N$, level generator $\mathcal{G}$ \\
    \textbf{Initialize}: student policy $\pi_\eta$, level buffer $L$, trajectory buffer $\Gamma$
    \begin{algorithmic}[1]
    \STATE Generate $N$ initial levels by $\mathcal{G}$ to populate $L$
    \STATE Collect trajectories on each replay level in $L$ and fill up $\Gamma$
        \WHILE{not converged}
        \STATE Sample replay-decision, $\epsilon \sim U[0,1]$
        \IF {$\epsilon \geq 0.5$}
        \STATE Generate a new level $l_{\theta_i}$ by $\mathcal{G}$
        \STATE Collect trajectories $\tau_i$ on $l_{\theta_i}$, with stop-gradient $\eta_{\perp}$
        \STATE Compute the regret, staleness and distance for $l_{\theta_i}$
        \ELSE
        \STATE Sample a replay level $l_{\theta_j}\in L$ according to $P_{replay}$
        \STATE Collect trajectories $\tau_j$ on $l_{\theta_j}$ and update $\pi_\eta$ with rewards $R(\tau_j)$
        \STATE Compute the regret, staleness and distance for $l_j$
        \ENDIF
        \STATE Flush $\Gamma$ and collect trajectories on all replay levels to fill up $\Gamma$
        \STATE Update regret, staleness, and distance for $l_{\theta_i}$ or $l_{\theta_j}$
        \STATE Update $L$ with new level $l_{\theta_i}$ if its replay probability is greater than any levels in $L$
        \STATE Update replay probability $P_{replay}$ 
        \ENDWHILE
    \end{algorithmic}
\end{algorithm} 

Finally, the state-of-the-art UED algorithm, ACCEL, improves PLR$^\perp$ by replacing its random level generation with an editor that mutates previously curated levels to gradually introduce complexity into the curriculum. ACCEL makes the key assumption that regret varies smoothly with the environment parameters $\theta$, such that the regret of a level is close to the regret of others within a small edit distance. If this is the case, then small edits to a single high-regret level should lead to the discovery of entire batches of high-regret levels -- which could be an otherwise challenging task in high-dimensional design spaces. An intriguing area for future exploration is the interaction between ACCEL's editing mechanism and the novelty-driven level prioritization introduced through CENIE. Specifically, it is worth investigating whether the editing mechanism does synergize with CENIE to produce levels that simultaneously maximize \textbf{both novelty and regret}, further enhancing the diversity and effectiveness of the generated curriculum.

\section{Hyperparameters}
In this section, we provide the hyperparameters we used for both CENIE-augmented and baseline algorithms in our experiments. We employ the same set of CENIE parameters for both ACCEL-CENIE and PLR-CENIE. We provide all the parameters for our implementations in Table \ref{tab:hyperparams_all}. 


\begin{table}[h]
    \caption{Hyperparameters used for training PLR-CENIE and ACCEL-CENIE in Minigrid, BipedalWalker and CarRacing domains. Note that the we inherit the original PLR$^\perp$ and ACCEL hyperparameters and only adjust the CENIE hyperparameters.}
    \label{tab:hyperparams_all}
    \centering
    \begin{tabular}{lrrr}
        \toprule
        Parameter         & Minigrid     & BipedalWalker & CarRacing   \\
        \midrule
        {\em \textbf{PPO}}& \text{ }    & \text{ }   & \text{ }      \\
        $\gamma$          & 0.995          & 0.99    & 0.99        \\
        $\lambda_{GAE}$   & 0.95          & 0.9      & 0.9      \\
        PPO rollout length    & 256          & 2048   & 125         \\
        PPO epochs        & 5          & 5      & 8      \\
        PPO minibatches per epoch & 1  & 32    & 4        \\
        PPO clip range    & 0.2          & 0.2  & 0.2          \\
        PPO number of workers  & 32     & 16     &  16       \\
        Adam learning rate   & 1e-4       & 3e-4  &    3e-4         \\
        Adam $\epsilon$       & 1e-5      & 1e-5     &    1e-5     \\
        PPO max gradient norm  & 0.5     & 0.5   &  0.5      \\
        PPO value clipping        & yes  & no     & no     \\
        return normalization      & no  & yes    &  yes     \\
        value loss coefficient     & 0.5 & 0.5    & 0.5       \\
        student entropy coefficient & 0.0 & 1e-3   & 0.0       \\
        \\
        {\em \textbf{PLR}$^\perp$} & \text{ }     & \text{ }  &  \text{ }    \\
        Scoring function   &   positive value loss   & positive value loss  &  positive value loss  \\
        Replay rate, \em{p}    &   0.5    &     0.5   & 0.5     \\
        Buffer size, \em{K}       &  4000   &   1000  &  8000      \\
        \\
        {\em \textbf{ACCEL}}  & \text{ }     & \text{ }   & \text{ }   \\
        Edit rate, \em{q}  &    1.0      &    1.0     & N/A      \\
        Replay rate, \em{p}    &   0.8    &     0.9    & N/A    \\
        Buffer size, \em{K}       &  4000   &   1000    & N/A   \\
        Scoring function   &   positive value loss   & positive value loss & N/A  \\
        Edit method    &    random   & random &  N/A \\
        Number of edits  &  5   &   3 &  N/A \\
        Levels edited     &    batch    &    batch  & N/A  \\
        Prioritization, $\beta$    &    0.3    &    0.1   & N/A  \\
        Staleness coefficient, $\rho$  &     0.5   &     0.5 & N/A \\
        \\
        {\em \textbf{CENIE}}  & \text{ }     & \text{ }    & \text{ }     \\
        Initialization strategy & k-means++ & k-means++ & k-means++ \\
        Convergence threshold, $\epsilon$ &  0.001 & 0.001 & 0.001 \\
        GMM components       & [6,15]            & [6,15]     & [6,15]        \\
        Covariance regularization & 1e-2 & 1e-6 & 1e-1 \\
        Window size (no. of levels)       &  32  & 32  & 32     \\
        Novelty coefficient &  0.5       &  0.5    &  0.5       \\
        \bottomrule
    \end{tabular}
\end{table}
