\section{Related Work}
Traditional MOBO methods such as $q$-EHVI \cite{daulton2020differentiable} assume that all Pareto-optimal solutions are equally desirable to the user, which might not be the case in practice. Instead, preference-based MOBO models user preferences with utility functions. \citet{lin2022preference,astudillo20a} propose the EUBO and qEIUU acquisition functions respectively, which take advantage of user-preference when querying new points.

Various works have proposed different methods to incorporate gradients as additional information in single-objective BO to enhance global search. \citet{wu2017gradient} construct a joint GP to correlate zeroth and first order information, demonstrating that gradients aid the surrogate in approximating the posterior and change which points to query. Similarly, \citet{makrygiorgos2023no} leverage gradients to establish stationarity conditions within a Karush-Kuhn-Tucker (KKT) formulation.

Other works instead use gradients for first-order optimization, also in the single-objective BO context. Bayesian and local optimisation sample-wise switching optimisation method (BLOSSOM) switches from global search to local search with BFGS when the posterior objective is close to the objective \cite{mcleod2018optimization}. On the other hand, \cite{muller2021local, nguyen2022local} abandon global search and construct local GPs for local optimization. 
%
To our knowledge, involving gradients for preference-based MOBO has not been studied and PUB-MOBO is the first algorithm to do this.