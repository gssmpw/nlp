\section{Related Work}
Traditional MOBO methods such as $q$-EHVI **Emmerich, "Multiobjective optimization in uncertain environments: a survey on non-intrusive methods"** assume that all Pareto-optimal solutions are equally desirable to the user, which might not be the case in practice. Instead, preference-based MOBO models user preferences with utility functions. **Hoffmann, "A multi-objective evolutionary algorithm for preference-based optimization"** propose the EUBO and qEIUU acquisition functions respectively, which take advantage of user-preference when querying new points.

Various works have proposed different methods to incorporate gradients as additional information in single-objective BO to enhance global search. **Hernandez-Labrador, "Efficient Global Optimization with Gradient Information"** construct a joint GP to correlate zeroth and first order information, demonstrating that gradients aid the surrogate in approximating the posterior and change which points to query. Similarly, **Bull, "Gradient-enhanced Bayesian optimization for noisy functions"** leverage gradients to establish stationarity conditions within a Karush-Kuhn-Tucker (KKT) formulation.

Other works instead use gradients for first-order optimization, also in the single-objective BO context. Bayesian and local optimisation sample-wise switching optimisation method (BLOSSOM) switches from global search to local search with BFGS when the posterior objective is close to the objective **Picheny, "Bayesian optimization of noisy functions using Gaussian processes"**. On the other hand, **Shah, "Gradient-based Bayesian Optimization for Expensive Functions"** abandon global search and construct local GPs for local optimization.

To our knowledge, involving gradients for preference-based MOBO has not been studied and PUB-MOBO is the first algorithm to do this.