\section{Experiments}
\label{sec: experiments}

\textbf{Setup}. We follow the settings of \cite{nachkov2024autonomous}. We use the Waymax simulator, whose initial simulator states are built over the WOMD \cite{ettinger2021large}. We measure the quality of a simulated trajectory using the average displacement error (ADE) compared to the expert (log) one. Since the policy is stochastic, we realize multiple trajectories and report the ADE of the best one, leading to minADE. We also report the minimum overlap rate, which represents collisions, and the minimum offroad rate, which represents agents navigating beyond the drivable regions. These rates represent the proportion of scenarios, in which at least one collision/offroad event occurs. All the following results are on the WOMD validation set. 

\begin{figure*}[t]
    \centering
    % \vspace{-11pt}
    \includegraphics[width=\textwidth]{figures/relative_odometry.png}
    \captionsetup{belowskip=-0.35cm, aboveskip=0.2cm}
    \caption{\textbf{Predictions from the next state predictor.} We condition the agent to turn or accelerate. The imagined trajectories, shown as scattered colored circles, represent the imagined odometry of the ego-vehicle in the next 1 second, at different points in time. They align with the actual realized trajectory, which implies that the agent can imagine its motion accurately.}
    \label{fig: relative_odometry_trajectories}
\end{figure*}

\subsection{Evaluating the analytic predictors}
Here we present independent, isolated evaluations for each of the four tasks described in Sec. \ref{sec: method}.

\textbf{Optimal control.} Optimal control in a differentiable environment can be solved using APG. Hence, we evaluate the reactive performance of our policies similarly to how it was evaluated in the baseline APG method \cite{nachkov2024autonomous}. Table \ref{table: reactive_apg_performance} shows the details. Naturally, the trajectories obtained from rolling out a reactive policy trained with APG are accurate. Increasing the number of rollouts improves performance in terms of min ADE, min overlap, and min offroad by up to 12\%.

% full - entropy5
\begin{table}[h]
    \small
    \centering
    \begin{tabular}[width=\textwidth]{ p{0.21\columnwidth} | p{0.11\columnwidth} | p{0.11\columnwidth} p{0.14\columnwidth} p{0.16\columnwidth} } \toprule[1.5pt]
         % \rowcolor[gray]{0.9}
         \textbf{APG setup} & Num. rollouts & min ADE $\downarrow$ & min \mbox{overlap $\downarrow$} &  min offroad $\downarrow$  \\
         \midrule[1pt]
         APG \cite{nachkov2024autonomous} & 32 & 2.0083 &  0.0800 & 0.0282 \\
         APG (ours) & 1 & 1.8121 & 0.0669 & 0.0263 \\
         APG (ours) & 2 & 1.7923 & 0.0663 & 0.0257 \\
         APG (ours) & 4 & 1.7765 & 0.0658 & 0.0256 \\
         APG (ours) & 8 & 1.7629 & 0.0653 & 0.0252 \\
         APG (ours) & 16 & 1.7516 & 0.0651 & 0.0251 \\
         APG (ours) & 32 & \textbf{1.7416} & \textbf{0.0648} & \textbf{0.0248} \\
         \bottomrule[1.5pt]
    \end{tabular}
    \captionsetup{aboveskip=0.3cm, belowskip=-0.3cm}
    \caption{\textbf{APG reactive performance.} Compared to the baseline, we rely on a more efficient training procedure which improves performance by 9\% while having 32 times less compute. With equal compute, our method is 12\% better in terms of min ADE. Inference time per rollout is similar to the baseline.}
    \label{table: reactive_apg_performance}
\end{table}

Compared to the baseline \cite{nachkov2024autonomous}, we use a more efficient training strategy that trains for a smaller number of epochs to limit possible overfitting. Additionally, we aim to increase the diversity of the policy as follows. As previously state, the policy parametrizes a Gaussian mixture with 6 components. We add a regularization term that maximizes the categorical entropy of the mixture distribution (\emph{not} the Gaussians themselves). To prevent the Gaussians from degenerating into single values, we clip the predicted variances.


\textbf{Relative odometry.} To evaluate the next state prediction we first produce qualitative results that demonstrate the controllability of the learned odometry predictor. Specifically, we condition the agent, for example, to intentionally commit to a turn over a long time frame. Concurrently, the odometry, along with the latent state predictors are used to imagine the next second of the planned motion, conditional on the actions. We judge the imagined trajectory to be accurate if the imagination precisely aligns with the realized trajectory. Fig. \ref{fig: relative_odometry_trajectories} shows an example. Overall, we observe accurate controllability -- if we condition the agent to turn left/right, accelerate/decelerate, the imagined trajectories also represent similar motion.

Manually conditioning the predicted odometry on a desired action sequence could easily lead to out-of-distribution state-action sequences. For example, driving offroad, making sudden sharp U-turns, or maximally accelerating can be considered rare events within the expert distribution. The accurate alignment between the imagined trajectory and the executed one shows that the network learns to generalize effectively. Nonetheless, the complexity of the scene and the autoregressive prediction length do limit the accuracy of the imagined trajectories. Finally, for in-distribution sequences and for shorter future horizons the odometry is very accurate, as shown in Table \ref{table: odometry_accuracy}.

% full
\begin{table}[h]
    \small
    \centering
    \begin{tabular}[width=\textwidth]{ p{0.4\columnwidth} | p{0.4\columnwidth} } \toprule[1.5pt]
         % \rowcolor[gray]{0.9}
         \textbf{Future horizon (steps)} & Avg. displacement error $\downarrow$  \\
         \midrule[1pt]
         5 (0.5 sec) & 0.1698 \\
         10 (1 sec) & 0.3475 \\   
         15 (1.5 sec) & 0.5496 \\       
         \bottomrule[1.5pt]
    \end{tabular}
    \captionsetup{aboveskip=0.3cm, belowskip=-0.3cm}
    \caption{\textbf{Odometry accuracy vs time horizon.} We measure the average distance between the imagined trajectory (of variable length) and the simulated trajectory. }
    \label{table: odometry_accuracy}
\end{table}

% NOTE: Could have a table showing how the performance deteriorates as the prediction horizon increases.

\begin{figure*}[t]
    \centering
    % \vspace{-11pt}
    \includegraphics[width=\textwidth]{figures/planner_preds3.png}
    \captionsetup{belowskip=-0.35cm, aboveskip=0.2cm}
    \caption{\textbf{Trajectories executed using the optimal planner.} They are realistic and resemble those from the policy. Training such planners is possible due to the analytically available dynamics and inverse kinematics.}
    \label{fig: planner_trajectories}
\end{figure*}

% full
\begin{table}[h]
    \small
    \centering
    \begin{tabular}[width=\textwidth]{ p{0.28\columnwidth} | p{0.11\columnwidth} p{0.11\columnwidth} p{0.14\columnwidth} p{0.16\columnwidth} } \toprule[1.5pt]
         % \rowcolor[gray]{0.9}
         \textbf{Setting} & min ADE $\downarrow$ & min \mbox{overlap $\downarrow$} &  min offroad $\downarrow$  \\
         \midrule[1pt]
         APG \cite{nachkov2024autonomous} & 2.0083 &  0.0800 & 0.0282 \\
         Planner, Sec. \ref{subsection: optimal_planners} & \textbf{1.8734} &  \textbf{0.0719} & \textbf{0.0254} \\   
         % Planner, plan mode & \textbf{1.8734} &  \textbf{0.0722} & \textbf{0.0253} \\     
         \bottomrule[1.5pt]
    \end{tabular}
    \captionsetup{aboveskip=0.3cm, belowskip=-0.3cm}
    \caption{\textbf{Reactive evaluation of the planner.} A differentiable simulator allows us to train a planner with strong performance. Here the planner is deterministic.}
    \label{table: planner_reactive}
\end{table}


\textbf{Optimal planners.} Here we evaluate with a combination of quantitative and qualitative metrics. Table \ref{table: planner_reactive} shows that obtaining an optimal planner using a differentiable environment is possible and results in strong performance, improving over the baseline APG method \cite{nachkov2024autonomous} on all metrics. For simplicity, the planner is deterministic. During the training, we found it useful to disable the action clipping that is used by default in the inverse dynamics, as it prevents gradients from 
flowing backward and updating the planner. This is needed especially early on in the training when the planner network is not accurate and the resulting actions are too large in magnitude. At test time, we evaluate with the default dynamics where action clipping is enabled.

Fig. \ref{fig: planner_trajectories} shows how the trajectories look qualitatively. Overall, they are smooth and realistic. Errors occur mostly from over-accelerating. Some turns are sharper but still reasonable.


\textbf{Inverse optimal state prediction.} Finding a state in which a given action is optimal is an inverse task. To motivate the setup for its evaluation, we reason as follows. If we start from a log state $\hat{\mathbf{s}}_t$ and the selected action is optimal, $\hat{\mathbf{a}}_t$, then the state we are looking for has a displacement of exactly $\mathbf{0}$ from the given state $\hat{\mathbf{s}}_t$. Thus, if we assume that the given actions are similar to the expert ones, then the predicted displacement should indicate how far the ego-vehicle is from the current log state $\hat{\mathbf{s}}_t$. This allows us, for example, to take the negative norm of the predicted displacement, $-\lVert f_\theta(\mathbf{s}_t, \mathbf{a}_t) \rVert$, and use it as a confidence-based reward when planning. Table \ref{table: inverse_opt_state_as_rewards} shows the performance of this setup. A trained policy is assumed to be near-optimal, hence the predicted inverse state displacements can serve as a reward/uncertainty measure for how close the agent is to where it should be.

% full
\begin{table}[h]
    \small
    \centering
    \begin{tabular}[width=\textwidth]{ p{0.38\columnwidth} | p{0.11\columnwidth} p{0.14\columnwidth} p{0.16\columnwidth} } \toprule[1.5pt]
         % \rowcolor[gray]{0.9}
         \textbf{Rewards}  & min ADE $\downarrow$ & min \mbox{overlap$^\ast$ $\downarrow$} &  min offroad$^\ast$ $\downarrow$  \\
         \midrule[1pt]
         Neg. dist. to next log state  & \textbf{1.8136} &  \textbf{0.0645} & 0.0226 \\
         Pos. dist. to next log state  & 1.8247 &  0.0649 & 0.0229 \\
         Neg. norm. of inv. state & 1.8138 &  0.0647 & \textbf{0.0218} \\
         \bottomrule[1.5pt]
    \end{tabular}
    \captionsetup{aboveskip=0.3cm, belowskip=-0.3cm}
    \caption{\textbf{Using the inverse state predictions when planning.} Using the negative norm of the inverse state displacement as reward provides meaningful and strong results. It is only marginally less accurate than using the standard rewards, defined as the negative distance to the next log state, due to the actions not being perfectly optimal. The case of selecting the worst reward (pos. dist to next log state) is also included for reference.}
    \label{table: inverse_opt_state_as_rewards}
\end{table}

We also provide qualitative evaluation in Fig. \ref{fig: inverse_state_preds}. The results are generally meaningful -- as the agent drifts off from the log trajectory, for example by lagging behind it, the predicted inverse optimal state has larger displacement relative to the current state.

\begin{figure}[h]
    \centering
    % \vspace{-11pt}
    \includegraphics[width=\columnwidth]{figures/imagined_inv_state_38.png}
    \captionsetup{belowskip=-0.35cm, aboveskip=0.2cm}
    \caption{\textbf{Executed trajectory colored according to the norm of the predicted inverse state displacements.} Here, since the ego-vehicle drives slower than the log expert, the norm of the optimal inverse state predictions gradually increases.}
    \label{fig: inverse_state_preds}
\end{figure}



\subsection{Planning}
Next, we assess the performance of the planning agent, which uses the analytic predictors. Evaluating with the best of multiple trajectory realizations is common practice in the literature \cite{ettinger2021large, montali2024waymo, caesar2020nuscenes}, yet is unrealistic because in the real world the agent can execute only one trajectory. Planning at test time is a considerable improvement to this setup because it allows the agent to simulate multiple virtual trajectories while only executing a single real one.

Table \ref{table: planning} shows results when planning. We increase the number of imagined trajectories while keeping the length of each one constant and equal to one step in the future. This is because the path with minimum ADE on the whole trajectory contains the path of minimum ADE on any part of the trajectory, allowing one to be greedy and to choose actions only based on their immediate effects. Increasing the number of imagined trajectories from which to aggregate the selected action improves performance. The effect is small but visible. It is small because the variance of the action distribution is small and the agent is confident. Hence, the different considered actions are also similar. Yet, the reward signal is precise enough to allow for the selection of the best ones.

% full
\begin{table}[h]
    \small
    \centering
    \begin{tabular}[width=\textwidth]{ p{0.25\columnwidth} | p{0.11\columnwidth} | p{0.11\columnwidth} p{0.14\columnwidth} p{0.16\columnwidth} } \toprule[1.5pt]
         % \rowcolor[gray]{0.9}
         \textbf{Setting} & Num. rollouts & min ADE $\downarrow$ & min \mbox{overlap$^\ast$ $\downarrow$} &  min offroad$^\ast$ $\downarrow$  \\
         \midrule[1pt]
          Plan + heading & 1 & 1.8147 & 0.0645 & 0.0222 \\     
          Plan + heading & 4 & 1.8121 & \textbf{0.0642} & \textbf{0.0220} \\
          Plan + heading & 8 & \textbf{1.8113} & 0.0643 & 0.0224 \\ 
          Plan + waypoint & 1 & 0.6031 & 0.0110 & 0.0098 \\     
          Plan + waypoint & 6 & 0.5988 & 0.0107 & 0.0092 \\
          Plan + waypoint & 12 & \textbf{0.5980} & \textbf{0.0106} & \textbf{0.0092} \\ 
         \bottomrule[1.5pt]
    \end{tabular}
    \captionsetup{aboveskip=0.3cm, belowskip=-0.3cm}
    \caption{\textbf{Planning at test time using MPC.} As the number of imagined trajectories increases, performance improves. We condition on either the heading or the last expert waypoint. Effects are limited by the policy diversity, yet are noticeable.}
    \label{table: planning}
\end{table}


\subsection{Additional experiments}

\textbf{Route conditioning}. To obtain good performance in terms of ADE the agent needs to know both where to go, and how to get there. Route conditioning can be used to provide more information to the agent, effectively narrowing down the possible directions that could be taken. We explore two forms of route conditioning -- heading, where the agent receives the heading angle to keep in order to reach a future expert state, and waypoint, where the agent observes the $(x, y)$ location of the last expert state. Both are used in the ego reference frame.

Table \ref{table: different_levels_of_conditioning} shows that adding route conditioning helps noticeably. Using the waypoint is the most useful because to reach the expert location, the agent needs to adapt both its steering and acceleration. The heading conditioning only provides information about the steering, not the acceleration. In general, as more conditioning is added, the task becomes closer and closer to trajectory following. The heading conditioning is the default setting we use in the other experiments.

\begin{table}[h]
    \small
    \centering
    \begin{tabular}[width=\textwidth]{ p{0.21\columnwidth} | p{0.11\columnwidth} | p{0.11\columnwidth} p{0.14\columnwidth} p{0.16\columnwidth} } \toprule[1.5pt]
         % \rowcolor[gray]{0.9}
         \textbf{Conditioning} & Num. rollouts & min ADE $\downarrow$ & min \mbox{overlap $\downarrow$} &  min offroad $\downarrow$  \\
         \midrule[1pt]
         \multirow{2}{*}{None} & 1 & 2.3244 &  0.1150 & 0.0774 \\
         & 8 & 2.2349 &  0.1103 & 0.0738 \\
         \multirow{2}{*}{Heading} & 1 & 1.7463 &  0.0609 & 0.0234 \\
         & 8 & 1.6718 &  0.0579 & 0.0222 \\
         \multirow{2}{*}{Last waypoint} & 1 &  \textbf{0.6029} &  \textbf{0.0110} & \textbf{0.0098} \\
         & 8 &  \textbf{0.5736} &  \textbf{0.0108} & \textbf{0.0094} \\    \hline
         Wayformer & 1 & 2.3800 & 0.1068 & 0.0789 \\
         \bottomrule[1.5pt]
    \end{tabular}
    \captionsetup{aboveskip=0.3cm, belowskip=-0.3cm}
    \caption{\textbf{Performance of the reactive policy with different levels of conditioning.} Adding more path conditioning improves performance because the policy does not need to decide where to go, only how to get there. For simplicity, we evaluate only the reactive policies. The Wayformer \cite{nayakanti2023wayformer} from \cite{gulino2024waymax} uses Delta dynamics (not bicycle) and is unconditioned. It is added for reference.}
    \label{table: different_levels_of_conditioning}
\end{table}

\textbf{Differentiable overlap.} We also provide experiments related to the differentiable overlap approximation used by our method. Table \ref{table: differentiable_overlap} shows that the differentiable overlap has a small effect -- it hurts the ADE, but improves the overlap metric. The differentiable overlap loss pushes the agent in the direction of maximally decreasing overlap when there is a collision. This direction may be different than the one towards the expert state (see Fig. \ref{fig: optimizing diff_overlap}). Thus, we recognize a trade-off. To limit the effect of conflicting gradients, we set the weight of the differentiable loss term to a small number.

\begin{figure}[h]
    \centering
    % \vspace{-11pt}
    \includegraphics[width=0.98\columnwidth]{figures/diff_overlap.png}
    \captionsetup{belowskip=-0.35cm, aboveskip=-0.0cm}
    \caption{\textbf{Optimizing differentiable overlap.} Here we use two losses -- an overlap loss to push the gray box away from the red box and a trajectory loss to pull the gray box toward the blue one. The gradients from these losses point in different directions at every step, leading to a curved trajectory.}
    \label{fig: optimizing diff_overlap}
\end{figure}

% !~!
\begin{table}[h]
    \small
    \centering
    \begin{tabular}[width=\textwidth]{ p{0.21\columnwidth} | p{0.11\columnwidth} | p{0.11\columnwidth} p{0.14\columnwidth} p{0.16\columnwidth} } \toprule[1.5pt]
         % \rowcolor[gray]{0.9}
         \textbf{Differentiable collision loss} & Num. rollouts & min ADE $\downarrow$ & min \mbox{overlap$^\ast$ $\downarrow$} &  min offroad$^\ast$ $\downarrow$  \\
         \midrule[1pt]
         \multirow{2}{*}{\xmark} & 1 & \textbf{1.7992} &  0.0146 & \textbf{0.0056} \\
         & 8 & \textbf{1.7261} &  0.0141 & 0.0056 \\
         \multirow{2}{*}{\cmark} & 1 & 1.8186 &  \textbf{0.0139} & 0.0057 \\
         & 8 & 1.7456 &  \textbf{0.0131} & \textbf{0.0055} \\         
         \bottomrule[1.5pt]
    \end{tabular}
    \captionsetup{aboveskip=0.3cm, belowskip=-0.4cm}
    \caption{\textbf{Adding differentiable overlap.} Adding a small differentiable overlap loss term hurts the ADE metric, but optimizes the overlap. The effect is small because the collision signal is sparse and its loss weight is small. Here, overlap$^\ast$ and offroad$^\ast$ denote that the metric has been calculated as a percentage of all individual \emph{steps}, not \emph{trajectories}, to better show small changes.}
    \label{table: differentiable_overlap}
\end{table}


\subsection{Implementation details}
\textbf{Training details.} We use Adam for the optimizer and a cosine schedule for the learning rate. Training lasts 40 epochs and is done on 8 A100 GPUs, with a total batch size of 256 samples. Each sample here refers to a full WOMD scenario of length 9 seconds. The agent is lightweight with 6M parameters altogether, similar to other models working over intermediate-level traffic scenario representations \cite{montali2024waymo} (roadgraph, 2D boxes). Inference speed is almost 23K timesteps processed per second on a single GPU. Further details, including how the prediction tasks are formulated for the specific dynamics in Waymax \cite{gulino2024waymax}, are available in the supplementary materials.