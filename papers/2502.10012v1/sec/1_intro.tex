\section{Introduction}
\label{sec: intro}

Differentiable simulation has emerged as a powerful tool to train controllers and predictors across different domains like physics \cite{holl2020learning, um2020solver, list2022learned}, graphics \cite{laine2020modular, jakob2022dr, zhao2020physics}, and robotics \cite{hu2019difftaichi, degrave2019differentiable, chen2022diffsrl, si2024difftactile}. Within the field of autonomous vehicles (AVs), it was recently shown that differentiable motion dynamics can serve as a useful stepping stone for training robust and realistic vehicle policies \cite{nachkov2024autonomous}. The framework is straightforward and bears similarity to backpropagation through time -- it involves rolling out a trajectory and supervising it with a ground-truth (GT) expert one. The process is sample-efficient because the gradients of the dynamics automatically guide the policy toward optimality and there is no search involved, unlike when the environment is treated as black box.

% Introduce planning and why it's important
Yet, this has only been explored for single policies, which are \emph{reactive} \cite{sutton1991dyna} in their nature -- at test-time they simply associate an action with each observation without providing any guarantee for their expected performance. Unlike them, model-based methods use planning at test time \cite{polydoros2017survey}, which \emph{guarantees} to maximize the estimated reward. They are considered more interpretable compared to model-free methods, due to the simulated world dynamics, and more amenable to conditioning, which makes them potentially safer \cite{ostafew2016robust}. They are also sample-efficient due to the self-supervised training \cite{chua2018deep}. Consequently, the ability to plan at test time is a compelling requirement towards accurate and safe autonomous driving.


\begin{figure}[t]
    \centering
    % \vspace{-11pt}
    \includegraphics[width=\columnwidth]{figures/teaser_v2.pdf}
    \captionsetup{belowskip=-0.55cm}
    \caption{\textbf{Differentiable simulation allows for a variety of learning tasks.}
    Previously, differentiable simulators have been used to train controllers using analytic policy gradients (bottom). Here, we propose to use them for learning relative odometry, state planning, and inverse state estimation (top).}
    \label{fig: teaser}
\end{figure}

An open question is \emph{whether model-based methods can be trained and utilized in a differentiable environment}, and what would be the benefits of doing so. We tackle this question here. Naturally, planning requires learning a world model, but the concept of a world model is rather nuanced, as there are different ways to understand the effect of one's own actions. Fig. \ref{fig: teaser} shows our approach, which uses the differentiability of the simulator to formulate three novel tasks related to world modeling. First, the effect of an agent's action could be understood as the difference between the agent's next state and its current state. If a vehicle's state consists of its position, yaw, and velocity, then this setup has an odometric interpretation. Second, an agent could predict not an action, but a \emph{desired} next state to visit, which is a form of state planning. Third, we can ask \emph{"Given an action in a particular state, what should the state be so that this action is optimal?"}, which is another form of world modeling but also an inverse problem.

Thus, we are motivated to understand the kinds of tasks solvable in a differentiable simulator for vehicle motion. Policy learning with differentiable simulation is called Analytic Policy Gradients (APG). Similarly, we call the proposed approach Analytic World Models (AWMs). It is intuitive, yet distinct from APG because APG requires the gradients of the next state with respect to the current actions, while AWM requires the gradients of the next state with respect to the current state.

The benefit of using a differentiable environment for solving these tasks is twofold. First, by not assuming black box dynamics, one can avoid \emph{searching} for the solution, which improves sample efficiency. Second, when the simulator is put into an end-to-end training loop, the differentiable dynamics serve to better condition the predictions, leading to more physically-consistent representations, as evidenced from other works. This happens because the gradients of the dynamics get mixed together with the gradients of the predictors. More generally, similar to how differentiable rendering \cite{laine2020modular, jakob2022dr} allows us to solve inverse graphics tasks, we believe that differentiable simulation allows us to solve new inverse dynamics tasks.

Having established the world modeling tasks, we use them for planning at test time. Specifically, we formulate a method based on model-predictive control (MPC), which allows the agent to autoregressively imagine future trajectories and apply the proposed world modeling predictors on their imagined states. Thus, the agent can efficiently compose these predictors in time. Note that this requires another world modeling predictor -- one that predicts the next state latent sensory features from the current ones. This is the de facto standard world model \cite{ha2018world}. Its necessity results only from the architectural design, to drive the autoregressive generation. Contrary to it, the predictions from our three proposed tasks are interpretable and meaningful, and represent a step towards the goal of having robust accurate planning for driving.

We use Waymax \cite{gulino2024waymax} as our simulator of choice, due to it being fully differentiable and data-driven. The scenarios are instantiated from the large-scale Waymo Open Motion Dataset (WOMD) \cite{ettinger2021large} and are realistic in terms of roadgraph layouts and traffic actors. Apart from establishing new ways of using the differentiable simulator, we offer an improved training recipe over the previous APG work, resulting in up to 12\% improvement in the average displacement error on WOMD while using an equal or smaller amount of compute. This improvement results from shorter training to limit any possible overfitting to the training trajectories and a regularization term that encourages the entropy of the policy. Additionally, we substitute the non-differentiable computation for collision detection in Waymax with a differentiable approximation based on Gaussian overlap. This allows us to differentiate not only through the dynamics, but also through the agent poses themselves. The learning signal from this overlap is sparse, yet conceptually useful to make the training setup complete. 

\textbf{Contributions.} Our contributions are the following:
\begin{itemize}
    \item In Sec. \ref{subsection: dynamics} to \ref{subsection: inverse_state_estimation} we present three world modeling tasks, solvable within a differentiable simulator -- relative odometry, state planning, and inverse state estimation.
    \item In Sec. \ref{subsection: architecture} and \ref{subsection: planning} we implement predictors for these tasks and present a planning agent that uses them.
    \item We train and evaluate our proposed setups in the Waymax simulator \cite{gulino2024waymax}, and further study them in multiple settings and conditions. In the process, we introduce technical modifications, such as differentiable overlap, for improved training relative to the baseline APG method.
\end{itemize}