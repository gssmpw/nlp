% NOTE: FOR RSS ENABLE THE FOLLOWING, FOR ARXIV, DISABLE IT
% \clearpage
% \setcounter{page}{1}
% ENDNOTE


\begin{appendices}
% Here we provide additional reasoning and comprehensive details supporting our method.

\section{Implementation details}
The task formulations presented in Section \ref{sec: method} are conceptual. We now discuss their concrete formulation in the Waymax simulator \cite{gulino2024waymax} for the bicycle dynamics, where the agent predicts acceleration $a$ and steering $\kappa$.

\textbf{Bicycle model}. In the bicycle model the agent state is given by $(x, y, \theta, v_x, v_y)$, containing the $(x, y)$ location in global coordinates, the yaw angle $\theta$, and the velocities $(v_x, v_y)$ over the two global axes. The actions $\mathbf{a} = (a, \kappa)$ consist of acceleration and steering. Given an action, the agent state evolves to its new values $(x', y', \theta', v_x', v_y')$ as follows:
\begin{equation}
    \begin{aligned}
    & x' = x + v_x\Delta t + \frac{1}{2} a \cos \theta \Delta t^2 \\
    & y' = y + v_y\Delta t + \frac{1}{2} a \sin \theta \Delta t^2 \\
    & \theta' = \theta + \kappa \left[ \sqrt{v_x^2 + v_y^2} \Delta t + \frac{1}{2} a \Delta t^2\right] \\
    & v' = \sqrt{v_x^2 + v_y^2} + a \Delta t \\
    & v_x' = v' \cos \theta' \\
    & v_y' = v' \sin \theta'.\\
    \end{aligned}
\end{equation}

We study the dynamics by fixing a single transition from a trajectory and exploring the behavior of a predictor being trained with gradient descent to overfit on this transition. 

\textbf{Relative odometry.} For the relative odometry task in \ref{subsection: dynamics}, one can notice that, while $\mathbf{s}_{t+1} - \mathbf{s}_t$ is a guaranteed optimal, where the loss function attains a value of zero, it is not the only optimal point. Indeed, our results suggest that if the network $f_\phi$ predicts a full agent state $\tilde{\mathbf{s}}_t = (x, y, \theta, v_x, v_y)$, then there are many states $\tilde{\mathbf{s}}_t$ from which taking an action $\mathbf{a}_t$ will bring us to $\mathbf{s}_{t+1} = (x', y', \theta', v_x', v_y')$. Intuitively, if $(x, y)$ is predicted closer to $(x', y')$, then $(v_x, v_y)$ can also be predicted smaller. Under the fixed action, we do not need a large initial velocity to reach a point that is relatively close. However, what can happen is that $(x, y)$ is predicted to be far from $(x', y')$, and $(v_x, v_y)$ are also predicted to be large. From a more distant point the agent needs to have a larger initial velocity to reach the target point, under a fixed action $\mathbf{a}_t = (a, \kappa)$.

Empirically, this means that even though gradient descent perfectly minimizes the loss, it may not learn to predict $\mathbf{s}_{t+1} - \mathbf{s}_t$. Since it is useful to have precisely this difference learned by the network, and since at test time it is mostly the $(x, y, \theta)$ variables that matter for our planning, we limit our dynamics predictor $f_\phi$ to predict only $(\Delta x, \Delta y, \Delta \theta)$, and take the velocities to be fixed at their current values in $\mathbf{s}_t$. This guarantees the prediction convergence to $\mathbf{s}_{t+1} - \mathbf{s}_t$. Yet, we can supervise all five variables $(x, y, \theta, v_x, v_y)$ of the simulated state, because the relative odometry task uses only transitions collected from the behavioral policy.

In brief, the concrete relative odometry task we solve is: given action $(a, \kappa)$, predict $(\Delta x, \Delta y, \Delta \theta)$ such that the state $(x_t + \Delta x, y_t + \Delta y, \theta_t + \Delta \theta, v_{x,t}, v_{y,t})$ evolves to $(x_{t+1}, y_{t+1}, \theta_{t+1}, v_{x,t+1}, v_{y, t+1})$. Formulated in this way, there is a unique global minimizer.

\textbf{Optimal inverse state.} The setting is similar for the inverse optimal state estimation, presented in Section \ref{subsection: inverse_state_estimation}. One can either predict the full state $(x, y, \theta, v_x, v_y)$, obtaining one of possibly many solutions, or one can predict only the elements $(x, y, \theta)$, in which case whatever prediction gradient descent converges to will be the unique prediction. We opt for the latter. Our final prediction problem is: given action $(a, \kappa)$, predict $(\Delta x, \Delta y, \Delta \theta)$ such that the state $(x_t + \Delta x, y_t + \Delta y, \theta_t + \Delta \theta, v_{x,t}, v_{y,t})$ evolves to $(\hat{x}_{t+1}, \hat{y}_{t+1}, \hat{\theta}_{t+1}, \hat{v}_{x,t+1}, \hat{v}_{y, t+1})$, where the target is, this time, the expert state. We apply the loss only over $(x, y)$ elements because the target combination of $(\hat{x}_{t+1}, \hat{y}_{t+1}, \hat{v}_{x,t+1}, \hat{v}_{y,t+1})$ may be, in general, unreachable from the current state.

\textbf{Optimal planning}. For the optimal planner task in \ref{subsection: optimal_planners}, we study the inverse kinematics equations:
\begin{equation}
    \begin{aligned}
    & a = \left[\sqrt{v_x'^2 + v_y'^2} - \sqrt{v_x^2 + v_y^2} \ \right]/\Delta t \\
    & \kappa = \big(\arctan \frac{v_x'}{v_y'} - \theta \big) / \left [ \sqrt{v_x^2 + v_y^2}\Delta t + \frac{1}{2}a \Delta t^2 \right ] \\
    \end{aligned}
\end{equation}

Here, it is enough to predict only $(v_x', v_y')$ in order to determine the action that brings the agent to the next state. In this setting, we have also found it useful to remove the typical action clipping applied, because in the cases where $(v_x', v_y')$ are too large, the action obtained from the inverse kinematics will be clipped, which prevents gradients from flowing back and updating the model weights. To make this task have a unique minimizer for each transition, we predict $(v_x', v_y')$ and supervise only the $(x, y, \theta)$ elements.

In summary, these tasks require inverting the bicycle dynamics, which depending on the formulation, may not always be one-to-one. We need to condition the prediction on additional variables, so that the minimizers are unique. This requirement stems from the underlying dynamics model itself.

\textbf{Coordinate systems.} Even though we phrase the world modeling tasks using actual simulator states, e.g. $\mathbf{s}_t, \mathbf{a} \mapsto \mathbf{s}_{t+1}$, the precise inputs to our world modeling predictors $f_\theta$ are always in the local time-dependent ego-vehicle reference frame. Additionally, one needs to take care to make sure that the state additions and subtractions in Eqns. \ref{eq:world_model_without_inverse}, \ref{eq:optimal_planner}, and \ref{eq:inverse_state_estimation} are all done in the same coordinate frame.

\end{appendices}