\section{Limitations}

While our setup yields strong results, it has limitations. 

\textbf{Camera tokens.} Raw camera images are unavailable in WOMD \cite{ettinger2021large}. Recently however, camera token features have been released \cite{mu2024most}. They are obtained from a custom pretrained VIT-VQGAN \cite{yu2021vector}. While they are not ideal, we recognize their potential utility in providing high-quality semantic information. Processing them is usually done with a separate transformer \cite{dosovitskiy2020image, vaswani2017attention}, increasing computational cost and memory usage, which we decided to avoid here. 

\textbf{Policy gradient improvements.} Despite using differentiable simulation, our APG approach follows the vanilla REINFORCE algorithm in style. It does not use importance-sampling for off-policy correction or rollout buffers for storing previous trajectories. Each batch of collected trajectories is used to update the policy only once, after which it is discarded. Techniques similar to PPO \cite{schulman2017proximal} and TRPO \cite{schulman2015trust} could be added to improve stability and data reuse.

\textbf{Multi-modal policy.} In most experiments where we do not use categorical entropy regularization, the Gaussian mixture usually degenerates to a single Gaussian that is selected with 99\% probability as training progresses. We view this as expected, given that there is only one expert trajectory per scenario, yet we recognize that more work is needed to maintain a multimodal stochastic policy.


\section{Conclusion}
\label{sec: conclusion}

In this work, we presented Analytic World Models (AWMs) -- different forms of learning world dynamics within a differentiable simulator. We formulated tasks for realtive odometry, optimal planning, and inverse optimal state estimation, all relying on the white box nature of the environment dynamics. We implemented predictors for these tasks within an autonomous vehicle navigation setting. When combined with MPC planning, the world modeling predictors can be used to better gauge the knowledge of the agent and inspect its decisions in a more understandable manner. Additionally, through a better training recipe, our model significantly outperforms the previous APG baseline, obtaining up to 12\% lower average displacement error with no increase in compute cost when used in a reactive manner.