\section{Related Work}
\label{sec: related_work}

\textbf{Differentiable simulation.} The most relevant work is Analytic Policy Gradients (APG) applied for vehicle motion \cite{nachkov2024autonomous}. It trains near-optimal policies in a supervised manner, relying on the differentiability of the Waymax simulator \cite{gulino2024waymax}. The authors present a recurrent model that selects actions autoregressively from the observed agent locations, nearest roadgraph points, traffic lights, and goal heading. At training time the model learns to select those actions that would bring the simulated trajectory as close as possible to the log-trajectory. By adopting a GRU architecture, the derivatives of the dynamics from each timestep mix with those of the RNN hidden state and propagate backwards until the start of the trajectory. Compared to this APG model, we aim to introduce planning in the differentiable environment.

More generally, differentiable simulators have grown in popularity because they allow one to solve ill-posed inverse problems related to the dynamics. As examples, an object's physical parameters like mass, friction, and elasticity could be estimated directly from videos and real-world experiments \cite{de2018end, murthy2020gradsim, geilinger2020add}, or simulations of soft material cutting could enable precise calibration and policy learning \cite{heiden2021disect}. Simulations can be parallelized across accelerators to enable efficient scaling of problem and experiment sizes \cite{xu2022accelerated, warp2022, freeman2021brax}.

Within the field of robotics, differentiable simulation is used extensively, especially for training robotic policies in physically-realistic settings \cite{newbury2024review, lutter2021differentiable, toussaint2018differentiable, qiao2020scalable, holl2020learning}. The focus has often been on object manipulation \cite{li2023dexdeform, xu2021end, xu2023efficient, lin2022diffskill} which requires having differentiable contact models for detecting collisions -- something lacking in the Waymax dynamics. Analytic policy gradients (APG) has been used to train policies for trajectory tracking and navigation in quadrotors and fixed-wing drones \cite{wiedemann2023training}, quadruped locomotion \cite{song2024learning}, and for quadrotor control from visual features \cite{heeg2024learning}. 

Driving simulators like CARLA \cite{dosovitskiy2017carla, martinez2017beyond} have been used in autonomous driving \cite{codevilla2018end, zhang2019vr}, with less focus on differentiable ones \cite{lavington2024torchdriveenv}. Some are differentiable but lack expert actions \cite{lavington2024torchdriveenv}, others are lacking acceleration support, which is crucial for large scale training \cite{sun2022intersim, li2022metadrive, vinitsky2022nocturne}. Waymax \cite{gulino2024waymax} was introduced recently as a data-driven simulator for vehicle motion. It represents vehicles as 2D boxes and supports acceleration. 
% It is now used extensively.

\textbf{Planning.} Planning using world models \cite{schrittwieser2020mastering, ha2018world, moerland2023model} is a classic topic with two main approaches: model-predictive control (MPC) \cite{bertsekas2012dynamic} and Dyna-style imagination \cite{sutton1991dyna}. With MPC \cite{arroyo2022reinforced, romero2024actor, karnchanachari2020practical}, one starts with a random policy from which actions are sampled and evaluated. Then, the policy is repeatedly refit on only the best trajectories, from which new trajectories are sampled. Eventually, an aggregated action from the best trajectories is selected and executed. Being closed-loop, this strategy is repeated at every timestep. Dyna-style planning relies on the agent simulating entire trajectories and using them to update its policy \cite{hafner2019dream, hafner2020mastering, hafner2023mastering}. Compared to MPC which is computationally heavy at test-time, Dyna-like algorithms are lightweights and rely on the reactive policy being trained on both real and imagined transitions.