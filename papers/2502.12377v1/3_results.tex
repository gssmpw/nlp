\section{Results}\label{sec:results}
In this work, we hypothesize that there is a relationship between model robustness and alignment, due to the inherent similarity of the goals in each of these spaces. Here, we focus on answering the question \textit{are more aligned machine learning models more robust to adversarial examples?}

\begin{figure}[t!]
    \centering
    \subfloat[Neural Alignment]{\includegraphics[width=0.33\textwidth]{figures/scatterplots/neural_vision_vs_robust.pdf}}
    \hfill 
    \subfloat[Behavioral Alignment]{\includegraphics[width=0.33\textwidth]{figures/scatterplots/behavior_vision_vs_robust.pdf}}
    \hfill 
    \subfloat[Engineering Task Performance]{\includegraphics[width=0.33\textwidth]{figures/scatterplots/engineering_vision_vs_robust.pdf}}
    \caption{Average vision alignment score vs robust accuracy on neural, behavioral, and engineering benchmarks.}
    \label{fig:avg_vision_robustness}
\end{figure}

% \subsection{Experimental Details}
To facilitate our experiments, we use the BrainScore library v2.2.4 to measure alignment~\cite{schrimpf_brain-score_2018} and load models. Details on models evaluated can be found in \autoref{sec:methods}. Once these models have been loaded and their alignment has been measured across the 106 alignment benchmarks, we evaluate their robustness using AutoAttack~\cite{croce_reliable_2020} from the TorchAttacks~\cite{kim_torchattacks_2021} library v3.5.1. The ImageNet~\cite{russakovsky_imagenet_2015} validation set is used for clean inputs to the model and serves as the starting point to generate adversarial examples. All experiments are run across 12 A100 GPUs with 40 GB of VRAM and CUDA version 11.1 or greater.  



\subsection{Average Alignment}
We first investigate how well different classes of alignment predict the robustness of a model. Here, we study neural alignment, behavioral alignment, and engineering task performance. For each of these classes, we take the average score across all the benchmarks, giving us a single score for each model in the class. While many works have typically studied average vision alignment overall (i.e., the average of all the benchmarks across all classes), it has been shown that this can overemphasize behavioral alignment at the cost of neural alignment~\cite{ahlert_how_2024}. For each model, we then compute its robust accuracy against AutoAttack at 3 different values of epsilon $\epsilon = \{0.001, 0.00196, 0.00392\}$, which corresponds to $\{\frac{0.25}{255}, \frac{0.5}{255},$ and $ \frac{1}{255}\}$, respectively. 


In \autoref{fig:avg_vision_robustness}, we analyze the average score for neural alignment, behavioral alignment, and engineering task performance on the x-axis and the robust accuracy on the y-axis. Each dot represents a model, and the 3 colors correspond to the model's robust accuracy at 3 different epsilon values. We compute the line-fit of the data at each epsilon value and report the statistical significance. 

We find statistically significant correlations at: all $\epsilon$ values for neural alignment (explaining up to 13\% of variance), $\epsilon=0.001$ for behavioral alignment (7.1\% of variance), and at the two lowest $\epsilon$ values for engineering task performance (up to 27\% of variance). Overall, the relatively low $R^2$ values, coupled with the difficulty of getting statistically significant correlations at higher epsilon values, suggests that average alignment scores are, at best, a weak indicator of robust accuracy. 

\subsection{Individual Benchmarks}
Motivated by the previous experiment where we find that average alignment is weakly correlated with robust accuracy, we hypothesize this counter-intuitive result occurs because averaging scores across different benchmarks may obscure that some individual benchmarks are stronger predictors of robust accuracy than others.
% we hypothesize that this counter-intuitive result may be due to the fact that since the scores are averaged across many benchmarks, it could suggest that some benchmarks serve as a much better indicator of robust accuracy than others. 
To further explore this hypothesis, we collect all models' scores on individual benchmarks for the three classes (neural alignment, behavioral alignment, and engineering task performance) and compute the correlation between each of these scores and robust accuracy at our three different $\epsilon$ values. \autoref{fig:robust_only_correlation} shows a heatmap of the 106 different benchmarks on the x-axis and robust accuracy at three different $\epsilon$ values on the y-axis. In each cell, we report the Pearson correlation coefficient between the selected benchmark score and robust accuracy across models. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/heatmaps/robust_only_correlation_alt.pdf}
    \caption{Heatmap of each of the BrainScore benchmarks, ordered and separated (black bars) by area of alignment (neural, behavioral, engineering) vs the robust accuracy. Each cell represents the correlation between a benchmark across models and the robust accuracy for those models.}
    \label{fig:robust_only_correlation}
\end{figure}

From this figure, we find multiple interesting trends. First, we see a wide range of correlations between different benchmarks, confirming our hypothesis that not every current alignment metric is a good indicator of robust accuracy. Additionally, we sometimes see significant changes to the correlation of robust accuracy and a benchmark as the $\epsilon$ value increases (and thus becomes a stronger attack). These changes appear to cluster by class of alignment. Roughly speaking, the neural alignment benchmarks (shown from the first to second black bar) tend to have more stable (and more positive) correlations as $\epsilon$ increases. The behavioral benchmarks (shown from the second to third black bars) tend to be, surprisingly, often negatively correlated with robust accuracy at mid and high $\epsilon$ values, and the correlation mostly decreases as $\epsilon$ increases. Finally, engineering task performance (shown from the third black bar to the end of the figure) seems highly dependent on the task, with benchmarks in this category having correlations at both ends of the spectrum. 

Interestingly, we find some trends in the benchmarks that strongly correlate with robust accuracy. Most notably, many of the benchmarks that exhibit strong positive correlations with robust accuracy even at high values of epsilon tend to measure a models bias toward texture to some degree. In the neural category, we found strong postive correlations in \texttt{FreemanZiemba2013.V1-pls} and \texttt{FreemanZiemba2013.V2-pls} from~\cite{freeman_functional_2013}, which measures neural responses in V1 and V2 to naturalistic texture stimuli. In the engineering category, two sets of benchmarks stood out as having strong correlations with robust accuracy. First is the \texttt{Geirhos2021cueconflict-top1} benchmark from~\cite{geirhos_imagenet-trained_2019}, which measures the probability of a model classifying an object using shape information rather than texture via  texture-shape conflicted images. The other is the set of benchmarks from~\cite{hermann_origins_2020}: \texttt{Hermann2020cueconflict-shape\_bias} and \texttt{Hermann2020cueconflict-shape\_match}, which similarly measures the probability of a model classifying an object using shape information and the percentage of the times the model classifies according to the shape class, rather than texture or other classes.



