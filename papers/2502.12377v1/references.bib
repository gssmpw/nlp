
@misc{ahlert_how_2024,
	title = {How {Aligned} are {Different} {Alignment} {Metrics}?},
	url = {http://arxiv.org/abs/2407.07530},
	doi = {10.48550/arXiv.2407.07530},
	abstract = {In recent years, various methods and benchmarks have been proposed to empirically evaluate the alignment of artificial neural networks to human neural and behavioral data. But how aligned are different alignment metrics? To answer this question, we analyze visual data from Brain-Score (Schrimpf et al., 2018), including metrics from the model-vs-human toolbox (Geirhos et al., 2021), together with human feature alignment (Linsley et al., 2018; Fel et al., 2022) and human similarity judgements (Muttenthaler et al., 2022). We find that pairwise correlations between neural scores and behavioral scores are quite low and sometimes even negative. For instance, the average correlation between those 80 models on Brain-Score that were fully evaluated on all 69 alignment metrics we considered is only 0.198. Assuming that all of the employed metrics are sound, this implies that alignment with human perception may best be thought of as a multidimensional concept, with different methods measuring fundamentally different aspects. Our results underline the importance of integrative benchmarking, but also raise questions about how to correctly combine and aggregate individual metrics. Aggregating by taking the arithmetic average, as done in Brain-Score, leads to the overall performance currently being dominated by behavior (95.25\% explained variance) while the neural predictivity plays a less important role (only 33.33\% explained variance). As a first step towards making sure that different alignment metrics all contribute fairly towards an integrative benchmark score, we therefore conclude by comparing three different aggregation options.},
	urldate = {2025-02-05},
	publisher = {arXiv},
	author = {Ahlert, Jannis and Klein, Thomas and Wichmann, Felix and Geirhos, Robert},
	month = jul,
	year = {2024},
	note = {arXiv:2407.07530 [q-bio]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition},
}

@inproceedings{russakovsky_imagenet_2015,
	title = {{ImageNet} {Large} {Scale} {Visual} {Recognition} {Challenge}},
	url = {http://arxiv.org/abs/1409.0575},
	abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classiﬁcation and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than ﬁfty institutions.},
	language = {en},
	urldate = {2023-12-05},
	booktitle = {{IJCV} 2015},
	publisher = {arXiv},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	month = jan,
	year = {2015},
	note = {arXiv:1409.0575 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, I.4.8, I.5.2, notion},
}

@misc{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	doi = {10.48550/arXiv.2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2025-02-04},
	publisher = {arXiv},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv:2010.11929 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{cheng_rtify_2024,
	title = {{RTify}: {Aligning} {Deep} {Neural} {Networks} with {Human} {Behavioral} {Decisions}},
	shorttitle = {{RTify}},
	url = {http://arxiv.org/abs/2411.03630},
	doi = {10.48550/arXiv.2411.03630},
	abstract = {Current neural network models of primate vision focus on replicating overall levels of behavioral accuracy, often neglecting perceptual decisions' rich, dynamic nature. Here, we introduce a novel computational framework to model the dynamics of human behavioral choices by learning to align the temporal dynamics of a recurrent neural network (RNN) to human reaction times (RTs). We describe an approximation that allows us to constrain the number of time steps an RNN takes to solve a task with human RTs. The approach is extensively evaluated against various psychophysics experiments. We also show that the approximation can be used to optimize an "ideal-observer" RNN model to achieve an optimal tradeoff between speed and accuracy without human data. The resulting model is found to account well for human RT data. Finally, we use the approximation to train a deep learning implementation of the popular Wong-Wang decision-making model. The model is integrated with a convolutional neural network (CNN) model of visual processing and evaluated using both artificial and natural image stimuli. Overall, we present a novel framework that helps align current vision models with human behavior, bringing us closer to an integrated model of human vision.},
	urldate = {2025-02-04},
	publisher = {arXiv},
	author = {Cheng, Yu-Ang and Rodriguez, Ivan Felipe and Chen, Sixuan and Kar, Kohitij and Watanabe, Takeo and Serre, Thomas},
	month = dec,
	year = {2024},
	note = {arXiv:2411.03630 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Quantitative Biology - Neurons and Cognition},
}

@misc{simonyan_deep_2014,
	title = {Deep {Inside} {Convolutional} {Networks}: {Visualising} {Image} {Classification} {Models} and {Saliency} {Maps}},
	shorttitle = {Deep {Inside} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1312.6034},
	doi = {10.48550/arXiv.1312.6034},
	abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
	urldate = {2025-02-04},
	publisher = {arXiv},
	author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
	month = apr,
	year = {2014},
	note = {arXiv:1312.6034 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{selvaraju_grad-cam_2020,
	title = {Grad-{CAM}: {Visual} {Explanations} from {Deep} {Networks} via {Gradient}-based {Localization}},
	volume = {128},
	issn = {0920-5691, 1573-1405},
	shorttitle = {Grad-{CAM}},
	url = {http://arxiv.org/abs/1610.02391},
	doi = {10.1007/s11263-019-01228-7},
	abstract = {We propose a technique for producing ‘visual explanations’ for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent and explainable.},
	language = {en},
	number = {2},
	urldate = {2025-02-04},
	journal = {International Journal of Computer Vision},
	author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	month = feb,
	year = {2020},
	note = {arXiv:1610.02391 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	pages = {336--359},
}

@article{kriegeskorte_representational_2008,
	title = {Representational similarity analysis - connecting the branches of systems neuroscience},
	volume = {2},
	issn = {1662-5137},
	url = {https://www.frontiersin.org/journals/systems-neuroscience/articles/10.3389/neuro.06.004.2008/full},
	doi = {10.3389/neuro.06.004.2008},
	abstract = {{\textless}p{\textgreater}A fundamental challenge for systems neuroscience is to quantitatively relate its three major branches of research: brain-activity measurement, behavioral measurement, and computational modeling. Using measured brain-activity patterns to evaluate computational network models is complicated by the need to define the correspondency between the units of the model and the channels of the brain-activity data, e.g., single-cell recordings or voxels from functional magnetic resonance imaging (fMRI). Similar correspondency problems complicate relating activity patterns between different modalities of brain-activity measurement (e.g., fMRI and invasive or scalp electrophysiology), and between subjects and species. In order to bridge these divides, we suggest abstracting from the activity patterns themselves and computing representational dissimilarity matrices (RDMs), which characterize the information carried by a given representation in a brain or model. Building on a rich psychological and mathematical literature on similarity analysis, we propose a new experimental and data-analytical framework called representational similarity analysis (RSA), in which multi-channel measures of neural activity are quantitatively related to each other and to computational theory and behavior by comparing RDMs. We demonstrate RSA by relating representations of visual objects as measured with fMRI in early visual cortex and the fusiform face area to computational models spanning a wide range of complexities. The RDMs are simultaneously related via second-level application of multidimensional scaling and tested using randomization and bootstrap techniques. We discuss the broad potential of RSA, including novel approaches to experimental design, and argue that these ideas, which have deep roots in psychology and neuroscience, will allow the integrated quantitative analysis of data from all three branches, thus contributing to a more unified systems neuroscience.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2025-02-04},
	journal = {Frontiers in Systems Neuroscience},
	author = {Kriegeskorte, Nikolaus and Mur, Marieke and Bandettini, Peter A.},
	month = nov,
	year = {2008},
	note = {Publisher: Frontiers},
	keywords = {Electrophysiology, Similarity, computational modeling, fMRI, population code, representation},
}

@inproceedings{sheatsley_space_2023,
	title = {The {Space} of {Adversarial} {Strategies}},
	isbn = {978-1-939133-37-3},
	url = {https://www.usenix.org/conference/usenixsecurity23/presentation/sheatsley},
	language = {en},
	urldate = {2025-01-31},
	booktitle = {32nd {USENIX} {Security} {Symposium} ({USENIX} {Security} 23)},
	author = {Sheatsley, Ryan and Hoak, Blaine and Pauley, Eric and McDaniel, Patrick},
	year = {2023},
	pages = {3745--3761},
}

@article{felleman_distributed_1991,
	title = {Distributed hierarchical processing in the primate cerebral cortex},
	volume = {1},
	issn = {1047-3211},
	url = {https://academic.oup.com/cercor/article-pdf/1/1/1/1139605/1-1-1.pdf},
	doi = {10.1093/cercor/1.1.1-a},
	abstract = {In recent years, many new cortical areas have been identified in the macaque monkey. The number of identified connections between areas has increased even more dramatically. We report here on (1) a summary of the layout of cortical areas associated with vision and with other modalities, (2) a computerized database for storing and representing large amounts of information on connectivity patterns, and (3) the application of these data to the analysis of hierarchical organization of the cerebral cortex. Our analysis concentrates on the visual system, which includes 25 neocortical areas that are predominantly or exclusively visual in function, plus an additional 7 areas that we regard as visual-association areas on the basis of their extensive visual inputs. A total of 305 connections among these 32 visual and visual-association areas have been reported. This represents 31\% of the possible number of pathways if each area were connected with all others. The actual degree of connectivity is likely to be closer to 40\%. The great majority of pathways involve reciprocal connections between areas. There are also extensive connections with cortical areas outside the visual system proper, including the somatosensory cortex, as well as neocortical, transitional, and archicortical regions in the temporal and frontal lobes. In the somatosensory/motor system, there are 62 identified pathways linking 13 cortical areas, suggesting an overall connectivity of about 40\%. Based on the laminar patterns of connections between areas, we propose a hierarchy of visual areas and of somatosensory/motor areas that is more comprehensive than those suggested in other recent studies. The current version of the visual hierarchy includes 10 levels of cortical processing. Altogether, it contains 14 levels if one includes the retina and lateral geniculate nucleus at the bottom as well as the entorhinal cortex and hippocampus at the top. Within this hierarchy, there are multiple, intertwined processing streams, which, at a low level, are related to the compartmental organization of areas V1 and V2 and, at a high level, are related to the distinction between processing centers in the temporal and parietal lobes. However, there are some pathways and relationships (about 10\% of the total) whose descriptions do not fit cleanly into this hierarchical scheme for one reason or another. In most instances, though, it is unclear whether these represent genuine exceptions to a strict hierarchy rather than inaccuracies or uncertainities in the reported assignment.},
	language = {eng},
	number = {1},
	journal = {Cerebral Cortex (New York, N.Y.: 1991)},
	author = {Felleman, D. J. and Van Essen, D. C.},
	year = {1991},
	pmid = {1822724},
	keywords = {Animals, Brain Mapping, Cerebral Cortex, Macaca, Mental Processes},
	pages = {1--47},
}

@inproceedings{hermann_origins_2020,
	title = {The {Origins} and {Prevalence} of {Texture} {Bias} in {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1911.09071},
	abstract = {Recent work has indicated that, unlike humans, ImageNet-trained CNNs tend to classify images by texture rather than by shape. How pervasive is this bias, and where does it come from? We ﬁnd that, when trained on datasets of images with conﬂicting shape and texture, CNNs learn to classify by shape at least as easily as by texture. What factors, then, produce the texture bias in CNNs trained on ImageNet? Different unsupervised training objectives and different architectures have small but signiﬁcant and largely independent effects on the level of texture bias. However, all objectives and architectures still lead to models that make texturebased classiﬁcation decisions a majority of the time, even if shape information is decodable from their hidden representations. The effect of data augmentation is much larger. By taking less aggressive random crops at training time and applying simple, naturalistic augmentation (color distortion, noise, and blur), we train models that classify ambiguous images by shape a majority of the time, and outperform baselines on out-of-distribution test sets. Our results indicate that apparent differences in the way humans and ImageNet-trained CNNs process images may arise not primarily from differences in their internal workings, but from differences in the data that they see.},
	language = {en},
	urldate = {2022-12-08},
	booktitle = {{NeurIPS} 2020},
	publisher = {arXiv},
	author = {Hermann, Katherine L. and Chen, Ting and Kornblith, Simon},
	month = nov,
	year = {2020},
	note = {arXiv:1911.09071 [cs, q-bio]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition, notion},
}

@misc{hoak_err_2024,
	title = {Err on the {Side} of {Texture}: {Texture} {Bias} on {Real} {Data}},
	shorttitle = {Err on the {Side} of {Texture}},
	url = {http://arxiv.org/abs/2412.10597},
	doi = {10.48550/arXiv.2412.10597},
	abstract = {Bias significantly undermines both the accuracy and trustworthiness of machine learning models. To date, one of the strongest biases observed in image classification models is texture bias-where models overly rely on texture information rather than shape information. Yet, existing approaches for measuring and mitigating texture bias have not been able to capture how textures impact model robustness in real-world settings. In this work, we introduce the Texture Association Value (TAV), a novel metric that quantifies how strongly models rely on the presence of specific textures when classifying objects. Leveraging TAV, we demonstrate that model accuracy and robustness are heavily influenced by texture. Our results show that texture bias explains the existence of natural adversarial examples, where over 90\% of these samples contain textures that are misaligned with the learned texture of their true label, resulting in confident mispredictions.},
	urldate = {2025-01-31},
	publisher = {arXiv},
	author = {Hoak, Blaine and Sheatsley, Ryan and McDaniel, Patrick},
	month = dec,
	year = {2024},
	note = {arXiv:2412.10597 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security},
}

@inproceedings{geirhos_imagenet-trained_2019,
	title = {{ImageNet}-trained {CNNs} are biased towards texture; increasing shape bias improves accuracy and robustness},
	url = {http://arxiv.org/abs/1811.12231},
	abstract = {Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conﬂicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conﬂict. We show that ImageNettrained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classiﬁcation strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on ‘StylizedImageNet’, a stylized version of ImageNet. This provides a much better ﬁt for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent beneﬁts such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation.},
	language = {en},
	urldate = {2022-10-24},
	booktitle = {{ICLR}},
	author = {Geirhos, Robert and Rubisch, Patricia and Michaelis, Claudio and Bethge, Matthias and Wichmann, Felix A. and Brendel, Wieland},
	month = jan,
	year = {2019},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning, notion},
}

@inproceedings{geirhos_generalisation_2020,
	title = {Generalisation in humans and deep neural networks},
	url = {http://arxiv.org/abs/1808.08750},
	doi = {10.48550/arXiv.1808.08750},
	abstract = {We compare the robustness of humans and current convolutional deep neural networks (DNNs) on object recognition under twelve different types of image degradations. First, using three well known DNNs (ResNet-152, VGG-19, GoogLeNet) we find the human visual system to be more robust to nearly all of the tested image manipulations, and we observe progressively diverging classification error-patterns between humans and DNNs when the signal gets weaker. Secondly, we show that DNNs trained directly on distorted images consistently surpass human performance on the exact distortion types they were trained on, yet they display extremely poor generalisation abilities when tested on other distortion types. For example, training on salt-and-pepper noise does not imply robustness on uniform white noise and vice versa. Thus, changes in the noise distribution between training and testing constitutes a crucial challenge to deep learning vision systems that can be systematically addressed in a lifelong machine learning approach. Our new dataset consisting of 83K carefully measured human psychophysical trials provide a useful reference for lifelong robustness against image degradations set by the human visual system.},
	urldate = {2024-09-24},
	booktitle = {{NeurIPS} 2018},
	publisher = {arXiv},
	author = {Geirhos, Robert and Temme, Carlos R. Medina and Rauber, Jonas and Schütt, Heiko H. and Bethge, Matthias and Wichmann, Felix A.},
	month = oct,
	year = {2020},
	note = {arXiv:1808.08750 [cs, q-bio, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning, notion},
}

@inproceedings{brendel_approximating_2019,
	title = {Approximating {CNNs} with {Bag}-of-local-{Features} models works surprisingly well on {ImageNet}},
	url = {http://arxiv.org/abs/1904.00760},
	doi = {10.48550/arXiv.1904.00760},
	abstract = {Deep Neural Networks (DNNs) excel on many complex perceptual tasks but it has proven notoriously difficult to understand how they reach their decisions. We here introduce a high-performance DNN architecture on ImageNet whose decisions are considerably easier to explain. Our model, a simple variant of the ResNet-50 architecture called BagNet, classifies an image based on the occurrences of small local image features without taking into account their spatial ordering. This strategy is closely related to the bag-of-feature (BoF) models popular before the onset of deep learning and reaches a surprisingly high accuracy on ImageNet (87.6\% top-5 for 33 x 33 px features and Alexnet performance for 17 x 17 px features). The constraint on local features makes it straight-forward to analyse how exactly each part of the image influences the classification. Furthermore, the BagNets behave similar to state-of-the art deep neural networks such as VGG-16, ResNet-152 or DenseNet-169 in terms of feature sensitivity, error distribution and interactions between image parts. This suggests that the improvements of DNNs over previous bag-of-feature classifiers in the last few years is mostly achieved by better fine-tuning rather than by qualitatively different decision strategies.},
	urldate = {2024-09-17},
	booktitle = {{ICLR} 2019},
	publisher = {arXiv},
	author = {Brendel, Wieland and Bethge, Matthias},
	month = mar,
	year = {2019},
	note = {arXiv:1904.00760 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}

@inproceedings{hoak_explorations_2024,
	title = {Explorations in {Texture} {Learning}},
	url = {http://arxiv.org/abs/2403.09543},
	doi = {10.48550/arXiv.2403.09543},
	abstract = {In this work, we investigate {\textbackslash}textit\{texture learning\}: the identification of textures learned by object classification models, and the extent to which they rely on these textures. We build texture-object associations that uncover new insights about the relationships between texture and object classes in CNNs and find three classes of results: associations that are strong and expected, strong and not expected, and expected but not present. Our analysis demonstrates that investigations in texture learning enable new methods for interpretability and have the potential to uncover unexpected biases.},
	urldate = {2024-03-15},
	booktitle = {{ICLR} 2024, {Tiny} {Papers} {Track}},
	publisher = {arXiv},
	author = {Hoak, Blaine and McDaniel, Patrick},
	month = mar,
	year = {2024},
	note = {arXiv:2403.09543 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, notion},
}

@misc{zhang_practical_2022,
	title = {Practical {No}-box {Adversarial} {Attacks} with {Training}-free {Hybrid} {Image} {Transformation}},
	url = {http://arxiv.org/abs/2203.04607},
	doi = {10.48550/arXiv.2203.04607},
	abstract = {Recently, the adversarial vulnerability of deep neural networks (DNNs) has raised increasing attention. Among all the threat models, no-box attacks are the most practical but extremely challenging since they neither rely on any knowledge of the target model or similar substitute model, nor access the dataset for training a new substitute model. Although a recent method has attempted such an attack in a loose sense, its performance is not good enough and computational overhead of training is expensive. In this paper, we move a step forward and show the existence of a training-free adversarial perturbation under the no-box threat model, which can be successfully used to attack different DNNs in real-time. Motivated by our observation that high-frequency component (HFC) domains in low-level features and plays a crucial role in classiﬁcation, we attack an image mainly by suppression of the original HFC and adding of noisy HFC. We empirically and experimentally analyze the requirements of effective noisy HFC and show that it should be regionally homogeneous, repeating and dense. Remarkably, on ImageNet dataset, our method attacks ten well-known models with a success rate of 98.13\% on average, which outperforms state-of-the-art no-box attacks by 29.39\%. Furthermore, our method is even competitive to mainstream transfer-based black-box attacks. Our code is available at our Supplementary.},
	language = {en},
	urldate = {2025-01-31},
	publisher = {arXiv},
	author = {Zhang, Qilong and Zhang, Chaoning and Li, Chaoqun and Song, Jingkuan and Gao, Lianli},
	month = nov,
	year = {2022},
	note = {arXiv:2203.04607 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{athalye_obfuscated_2018,
	title = {Obfuscated {Gradients} {Give} a {False} {Sense} of {Security}: {Circumventing} {Defenses} to {Adversarial} {Examples}},
	shorttitle = {Obfuscated {Gradients} {Give} a {False} {Sense} of {Security}},
	url = {https://proceedings.mlr.press/v80/athalye18a.html},
	abstract = {We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat iterative optimization-based attacks, we find defenses relying on this effect can be circumvented. We describe characteristic behaviors of defenses exhibiting the effect, and for each of the three types of obfuscated gradients we discover, we develop attack techniques to overcome it. In a case study, examining non-certified white-box-secure defenses at ICLR 2018, we find obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 completely, and 1 partially, in the original threat model each paper considers.},
	language = {en},
	urldate = {2025-01-31},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Athalye, Anish and Carlini, Nicholas and Wagner, David},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {274--283},
}

@misc{carlini_evaluating_2019,
	title = {On {Evaluating} {Adversarial} {Robustness}},
	url = {http://arxiv.org/abs/1902.06705},
	doi = {10.48550/arXiv.1902.06705},
	abstract = {Correctly evaluating defenses against adversarial examples has proven to be extremely difficult. Despite the significant amount of recent work attempting to design defenses that withstand adaptive attacks, few have succeeded; most papers that propose defenses are quickly shown to be incorrect. We believe a large contributing factor is the difficulty of performing security evaluations. In this paper, we discuss the methodological foundations, review commonly accepted best practices, and suggest new methods for evaluating defenses to adversarial examples. We hope that both researchers developing defenses as well as readers and reviewers who wish to understand the completeness of an evaluation consider our advice in order to avoid common pitfalls.},
	urldate = {2025-01-31},
	publisher = {arXiv},
	author = {Carlini, Nicholas and Athalye, Anish and Papernot, Nicolas and Brendel, Wieland and Rauber, Jonas and Tsipras, Dimitris and Goodfellow, Ian and Madry, Aleksander and Kurakin, Alexey},
	month = feb,
	year = {2019},
	note = {arXiv:1902.06705 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{szegedy_intriguing_2014,
	title = {Intriguing properties of neural networks},
	url = {http://arxiv.org/abs/1312.6199},
	doi = {10.48550/arXiv.1312.6199},
	abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.},
	language = {en},
	urldate = {2025-01-31},
	publisher = {arXiv},
	author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
	month = feb,
	year = {2014},
	note = {arXiv:1312.6199 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{yamins_hierarchical_2013,
	title = {Hierarchical {Modular} {Optimization} of {Convolutional} {Networks} {Achieves} {Representations} {Similar} to {Macaque} {IT} and {Human} {Ventral} {Stream}},
	volume = {26},
	url = {https://papers.nips.cc/paper_files/paper/2013/hash/9a1756fd0c741126d7bbd4b692ccbd91-Abstract.html},
	abstract = {Humans recognize visually-presented objects rapidly and accurately. To understand this ability, we seek to construct models of the ventral stream, the series of cortical areas thought to subserve object recognition. One tool to assess the quality of a model of the ventral stream is the Representation Dissimilarity Matrix (RDM), which uses a set of visual stimuli and measures the distances produced in either the brain (i.e. fMRI voxel responses, neural firing rates) or in models (features). Previous work has shown that all known models of the ventral stream fail to capture the RDM pattern observed in either IT cortex, the highest ventral area, or in the human ventral stream. In this work, we construct models of the ventral stream using a novel optimization procedure for category-level object recognition problems, and produce RDMs resembling both macaque IT and human ventral stream. The model, while novel in the optimization procedure, further develops a long-standing functional hypothesis that the ventral visual stream is a hierarchically arranged series of processing stages optimized for visual object recognition.},
	urldate = {2025-01-31},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Yamins, Daniel L and Hong, Ha and Cadieu, Charles and DiCarlo, James J},
	year = {2013},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	copyright = {2015 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	language = {en},
	number = {7553},
	urldate = {2025-01-31},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Mathematics and computing},
	pages = {436--444},
}

@misc{sucholutsky_getting_2024,
	title = {Getting aligned on representational alignment},
	url = {http://arxiv.org/abs/2310.13018},
	doi = {10.48550/arXiv.2310.13018},
	abstract = {Biological and artificial information processing systems form representations of the world that they can use to categorize, reason, plan, navigate, and make decisions. How can we measure the similarity between the representations formed by these diverse systems? Do similarities in representations then translate into similar behavior? If so, then how can a system’s representations be modified to better match those of another system? These questions pertaining to the study of representational alignment are at the heart of some of the most promising research areas in contemporary cognitive science, neuroscience, and machine learning. In this Perspective, we survey the exciting recent developments in representational alignment research in the fields of cognitive science, neuroscience, and machine learning. Despite their overlapping interests, there is limited knowledge transfer between these fields, so work in one field ends up duplicated in another, and useful innovations are not shared effectively. To improve communication, we propose a unifying framework that can serve as a common language for research on representational alignment, and map several streams of existing work across fields within our framework. We also lay out open problems in representational alignment where progress can benefit all three of these fields. We hope that this paper will catalyze cross-disciplinary collaboration and accelerate progress for all communities studying and developing information processing systems.},
	language = {en},
	urldate = {2025-01-31},
	publisher = {arXiv},
	author = {Sucholutsky, Ilia and Muttenthaler, Lukas and Weller, Adrian and Peng, Andi and Bobu, Andreea and Kim, Been and Love, Bradley C. and Cueva, Christopher J. and Grant, Erin and Groen, Iris and Achterberg, Jascha and Tenenbaum, Joshua B. and Collins, Katherine M. and Hermann, Katherine L. and Oktar, Kerem and Greff, Klaus and Hebart, Martin N. and Cloos, Nathan and Kriegeskorte, Nikolaus and Jacoby, Nori and Zhang, Qiuyi and Marjieh, Raja and Geirhos, Robert and Chen, Sherol and Kornblith, Simon and Rane, Sunayana and Konkle, Talia and O'Connell, Thomas P. and Unterthiner, Thomas and Lampinen, Andrew K. and Müller, Klaus-Robert and Toneva, Mariya and Griffiths, Thomas L.},
	month = nov,
	year = {2024},
	note = {arXiv:2310.13018 [q-bio]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition},
}

@inproceedings{li_learning_2019,
	title = {Learning from brains how to regularize machines},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/70117ee3c0b15a2950f1e82a215e812b-Abstract.html},
	abstract = {Despite impressive performance on numerous visual tasks, Convolutional Neural Networks (CNNs) --- unlike brains --- are often highly sensitive to small perturbations of their input, e.g. adversarial noise leading to erroneous decisions. We propose to regularize CNNs using large-scale neuroscience data to learn more robust neural features in terms of representational similarity. We presented natural images to mice and measured the responses of thousands of neurons from cortical visual areas. Next, we denoised the notoriously variable neural activity using strong predictive models trained on this large corpus of responses from the mouse visual system, and calculated the representational similarity for millions of pairs of images from the model's predictions. We then used the neural representation similarity to regularize CNNs trained on image classification by penalizing intermediate representations that deviated from neural ones. This preserved performance of baseline models when classifying images under standard benchmarks, while maintaining substantially higher performance compared to baseline or control models when classifying noisy images. Moreover, the models regularized with cortical representations also improved model robustness in terms of adversarial attacks. This demonstrates that regularizing with neural data can be an effective tool to create an inductive bias towards more robust inference.},
	urldate = {2025-01-31},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Li, Zhe and Brendel, Wieland and Walker, Edgar and Cobos, Erick and Muhammad, Taliah and Reimer, Jacob and Bethge, Matthias and Sinz, Fabian and Pitkow, Zachary and Tolias, Andreas},
	year = {2019},
	keywords = {notion},
}

@article{freeman_functional_2013,
	title = {A functional and perceptual signature of the second visual area in primates},
	volume = {16},
	copyright = {2013 Springer Nature America, Inc.},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/nn.3402},
	doi = {10.1038/nn.3402},
	abstract = {The authors examined neuronal responses in V1 and V2 to synthetic texture stimuli that replicate higher-order statistical dependencies found in natural images. V2, but not V1, responded differentially to these textures, in both macaque (single neurons) and human (fMRI). Human detection of naturalistic structure in the same images was predicted by V2 responses, suggesting a role for V2 in representing natural image structure.},
	language = {en},
	number = {7},
	urldate = {2025-01-30},
	journal = {Nature Neuroscience},
	author = {Freeman, Jeremy and Ziemba, Corey M. and Heeger, David J. and Simoncelli, Eero P. and Movshon, J. Anthony},
	month = jul,
	year = {2013},
	note = {Publisher: Nature Publishing Group},
	keywords = {Extrastriate cortex, Neural encoding},
	pages = {974--981},
}

@article{yamins_performance-optimized_2014,
	title = {Performance-optimized hierarchical models predict neural responses in higher visual cortex},
	volume = {111},
	url = {https://www.pnas.org/doi/10.1073/pnas.1403112111},
	doi = {10.1073/pnas.1403112111},
	abstract = {The ventral visual stream underlies key human visual object recognition abilities. However, neural encoding in the higher areas of the ventral stream remains poorly understood. Here, we describe a modeling approach that yields a quantitatively accurate model of inferior temporal (IT) cortex, the highest ventral cortical area. Using high-throughput computational techniques, we discovered that, within a class of biologically plausible hierarchical neural network models, there is a strong correlation between a model’s categorization performance and its ability to predict individual IT neural unit response data. To pursue this idea, we then identified a high-performing neural network that matches human performance on a range of recognition tasks. Critically, even though we did not constrain this model to match neural data, its top output layer turns out to be highly predictive of IT spiking responses to complex naturalistic images at both the single site and population levels. Moreover, the model’s intermediate layers are highly predictive of neural responses in the V4 cortex, a midlevel visual area that provides the dominant cortical input to IT. These results show that performance optimization—applied in a biologically appropriate model class—can be used to build quantitative predictive models of neural processing.},
	number = {23},
	urldate = {2025-01-30},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Yamins, Daniel L. K. and Hong, Ha and Cadieu, Charles F. and Solomon, Ethan A. and Seibert, Darren and DiCarlo, James J.},
	month = jun,
	year = {2014},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {8619--8624},
}

@article{singh_revisiting_2023,
	title = {Revisiting {Adversarial} {Training} for {ImageNet}: {Architectures}, {Training} and {Generalization} across {Threat} {Models}},
	volume = {36},
	shorttitle = {Revisiting {Adversarial} {Training} for {ImageNet}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/2d3b007613940def7a5ec9d6d635937b-Abstract-Conference.html},
	language = {en},
	urldate = {2025-01-30},
	journal = {Advances in Neural Information Processing Systems},
	author = {Singh, Naman Deep and Croce, Francesco and Hein, Matthias},
	month = dec,
	year = {2023},
	pages = {13931--13955},
}

@misc{schrimpf_brain-score_2018,
	title = {Brain-{Score}: {Which} {Artificial} {Neural} {Network} for {Object} {Recognition} is most {Brain}-{Like}?},
	shorttitle = {Brain-{Score}},
	url = {http://biorxiv.org/lookup/doi/10.1101/407007},
	doi = {10.1101/407007},
	abstract = {The internal representations of early deep artificial neural networks (ANNs) were found to be remarkably similar to the internal neural representations measured experimentally in the primate brain. Here we ask, as deep ANNs have continued to evolve, are they becoming more or less brain-like? ANNs that are most functionally similar to the brain will contain mechanisms that are most like those used by the brain. We therefore developed
            Brain-Score
            – a composite of multiple neural and behavioral benchmarks that score any ANN on how similar it is to the brain’s mechanisms for core object recognition – and we deployed it to evaluate a wide range of state-of-the-art deep ANNs. Using this scoring system, we here report that: (1) DenseNet-169, CORnet-S and ResNet-101 are the most brain-like ANNs. (2) There remains considerable variability in neural and behavioral responses that is not predicted by any ANN, suggesting that no ANN model has yet captured all the relevant mechanisms. (3) Extending prior work, we found that gains in ANN ImageNet performance led to gains on Brain-Score. However, correlation weakened at ≥ 70\% top-1 ImageNet performance, suggesting that additional guidance from neuroscience is needed to make further advances in capturing brain mechanisms. (4) We uncovered smaller (i.e. less complex) ANNs that are more brain-like than many of the best-performing ImageNet models, which suggests the opportunity to simplify ANNs to better understand the ventral stream. The scoring system used here is far from complete. However, we propose that evaluating and tracking model-benchmark correspondences through a Brain-Score that is regularly updated with new brain data is an exciting opportunity: experimental benchmarks can be used to guide machine network evolution, and machine networks are mechanistic hypotheses of the brain’s network and thus drive next experiments. To facilitate both of these, we release
            Brain-Score.org
            : a platform that hosts the neural and behavioral benchmarks, where ANNs for visual processing can be submitted to receive a Brain-Score and their rank relative to other models, and where new experimental data can be naturally incorporated.},
	language = {en},
	urldate = {2025-01-29},
	publisher = {Neuroscience},
	author = {Schrimpf, Martin and Kubilius, Jonas and Hong, Ha and Majaj, Najib J. and Rajalingham, Rishi and Issa, Elias B. and Kar, Kohitij and Bashivan, Pouya and Prescott-Roy, Jonathan and Geiger, Franziska and Schmidt, Kailyn and Yamins, Daniel L. K. and DiCarlo, James J.},
	month = sep,
	year = {2018},
}

@inproceedings{subramanian_spatial-frequency_2023,
	title = {Spatial-frequency channels, shape bias, and adversarial robustness},
	abstract = {What spatial frequency information do humans and neural networks use to recognize objects? In neuroscience, critical band masking is an established tool that can reveal the frequency-selective filters used for object recognition. Critical band masking measures the sensitivity of recognition performance to noise added at each spatial frequency. Existing critical band masking studies show that humans recognize periodic patterns (gratings) and letters by means of a spatial-frequency filter (or “channel”) that has a frequency bandwidth of one octave (doubling of frequency). Here, we introduce critical band masking as a task for network-human comparison and test 14 humans and 76 neural networks on 16-way ImageNet categorization in the presence of narrowband noise. We find that humans recognize objects in natural images using the same one-octave-wide channel that they use for letters and gratings, making it a canonical feature of human object recognition. Unlike humans, the neural network channel is very broad, 2-4 times wider than the human channel. This means that the network channel extends to frequencies higher and lower than those that humans are sensitive to. Thus, noise at those frequencies will impair network performance and spare human performance. Adversarial and augmented-image training are commonly used to increase network robustness and shape bias. Does this training align network and human object recognition channels? Three network channel properties (bandwidth, center frequency, peak noise sensitivity) correlate strongly with shape bias (51\% variance explained) and robustness of adversarially-trained networks (66\% variance explained). Adversarial training increases robustness but expands the channel bandwidth even further beyond the human bandwidth. Thus, critical band masking reveals that the network channel is more than twice as wide as the human channel, and that adversarial training only makes it worse. Networks with narrower channels might be more robust.},
	language = {en},
	booktitle = {Conference on {Neural} {Information} {Processing} {Systems}},
	publisher = {NeurIPS},
	author = {Subramanian, Ajay and Sizikova, Elena and Majaj, Najib J and Pelli, Denis G},
	year = {2023},
}

@inproceedings{geirhos_partial_2021,
	title = {Partial success in closing the gap between human and machine vision},
	url = {http://arxiv.org/abs/2106.07411},
	doi = {10.48550/arXiv.2106.07411},
	abstract = {A few years ago, the ﬁrst CNN surpassed human performance on ImageNet. However, it soon became clear that machines lack robustness on more challenging test cases, a major obstacle towards deploying machines “in the wild” and towards obtaining better computational models of human visual perception. Here we ask: Are we making progress in closing the gap between human and machine vision? To answer this question, we tested human observers on a broad range of out-ofdistribution (OOD) datasets, recording 85,120 psychophysical trials across 90 participants. We then investigated a range of promising machine learning developments that crucially deviate from standard supervised CNNs along three axes: objective function (self-supervised, adversarially trained, CLIP language-image training), architecture (e.g. vision transformers), and dataset size (ranging from 1M to 1B).},
	language = {en},
	urldate = {2025-01-28},
	booktitle = {35th {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {NeurIPS},
	author = {Geirhos, Robert and Narayanappa, Kantharaju and Mitzkus, Benjamin and Thieringer, Tizian and Bethge, Matthias and Wichmann, Felix A. and Brendel, Wieland},
	month = oct,
	year = {2021},
	note = {arXiv:2106.07411 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition},
}

@misc{kim_torchattacks_2021,
	title = {Torchattacks: {A} {PyTorch} {Repository} for {Adversarial} {Attacks}},
	shorttitle = {Torchattacks},
	url = {http://arxiv.org/abs/2010.01950},
	doi = {10.48550/arXiv.2010.01950},
	abstract = {Torchattacks is a PyTorch library that contains adversarial attacks to generate adversarial examples and to verify the robustness of deep learning models. The code can be found at https://github.com/Harry24k/adversarial-attacks-pytorch.},
	urldate = {2025-01-28},
	publisher = {arXiv},
	author = {Kim, Hoki},
	month = feb,
	year = {2021},
	note = {arXiv:2010.01950 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{schrimpf_brain-score_2020,
	title = {Brain-{Score}: {Which} {Artificial} {Neural} {Network} for {Object} {Recognition} is most {Brain}-{Like}?},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	shorttitle = {Brain-{Score}},
	url = {https://www.biorxiv.org/content/10.1101/407007v2},
	doi = {10.1101/407007},
	abstract = {The internal representations of early deep artificial neural networks (ANNs) were found to be remarkably similar to the internal neural representations measured experimentally in the primate brain. Here we ask, as deep ANNs have continued to evolve, are they becoming more or less brain-like? ANNs that are most functionally similar to the brain will contain mechanisms that are most like those used by the brain. We therefore developed Brain-Score – a composite of multiple neural and behavioral benchmarks that score any ANN on how similar it is to the brain’s mechanisms for core object recognition – and we deployed it to evaluate a wide range of state-of-the-art deep ANNs. Using this scoring system, we here report that: (1) DenseNet-169, CORnet-S and ResNet-101 are the most brain-like ANNs. (2) There remains considerable variability in neural and behavioral responses that is not predicted by any ANN, suggesting that no ANN model has yet captured all the relevant mechanisms. (3) Extending prior work, we found that gains in ANN ImageNet performance led to gains on Brain-Score. However, correlation weakened at ≥ 70\% top-1 ImageNet performance, suggesting that additional guidance from neuroscience is needed to make further advances in capturing brain mechanisms. (4) We uncovered smaller (i.e. less complex) ANNs that are more brain-like than many of the best-performing ImageNet models, which suggests the opportunity to simplify ANNs to better understand the ventral stream. The scoring system used here is far from complete. However, we propose that evaluating and tracking model-benchmark correspondences through a Brain-Score that is regularly updated with new brain data is an exciting opportunity: experimental benchmarks can be used to guide machine network evolution, and machine networks are mechanistic hypotheses of the brain’s network and thus drive next experiments. To facilitate both of these, we release Brain-Score.org: a platform that hosts the neural and behavioral benchmarks, where ANNs for visual processing can be submitted to receive a Brain-Score and their rank relative to other models, and where new experimental data can be naturally incorporated.},
	language = {en},
	urldate = {2025-01-28},
	publisher = {bioRxiv},
	author = {Schrimpf, Martin and Kubilius, Jonas and Hong, Ha and Majaj, Najib J. and Rajalingham, Rishi and Issa, Elias B. and Kar, Kohitij and Bashivan, Pouya and Prescott-Roy, Jonathan and Geiger, Franziska and Schmidt, Kailyn and Yamins, Daniel L. K. and DiCarlo, James J.},
	month = jan,
	year = {2020},
	note = {Pages: 407007
Section: New Results},
}

@inproceedings{croce_reliable_2020,
	title = {Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks},
	url = {https://proceedings.mlr.press/v119/croce20b.html},
	abstract = {The field of defense strategies against adversarial attacks has significantly grown over the last years, but progress is hampered as the evaluation of adversarial defenses is often insufficient and thus gives a wrong impression of robustness. Many promising defenses could be broken later on, making it difficult to identify the state-of-the-art. Frequent pitfalls in the evaluation are improper tuning of hyperparameters of the attacks, gradient obfuscation or masking. In this paper we first propose two extensions of the PGD-attack overcoming failures due to suboptimal step size and problems of the objective function. We then combine our novel attacks with two complementary existing ones to form a parameter-free, computationally affordable and user-independent ensemble of attacks to test adversarial robustness. We apply our ensemble to over 50 models from papers published at recent top machine learning and computer vision venues. In all except one of the cases we achieve lower robust test accuracy than reported in these papers, often by more than 101010\%, identifying several broken defenses.},
	language = {en},
	urldate = {2025-01-23},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Croce, Francesco and Hein, Matthias},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {2206--2216},
}

@inproceedings{croce_robustbench_2021,
	title = {{RobustBench}: a standardized adversarial robustness benchmark},
	shorttitle = {{RobustBench}},
	url = {http://arxiv.org/abs/2010.09670},
	doi = {10.48550/arXiv.2010.09670},
	abstract = {As a research community, we are still lacking a systematic understanding of the progress on adversarial robustness which often makes it hard to identify the most promising ideas in training robust models. A key challenge in benchmarking robustness is that its evaluation is often error-prone leading to robustness overestimation. Our goal is to establish a standardized benchmark of adversarial robustness, which as accurately as possible reflects the robustness of the considered models within a reasonable computational budget. To this end, we start by considering the image classification task and introduce restrictions (possibly loosened in the future) on the allowed models. We evaluate adversarial robustness with AutoAttack, an ensemble of white- and black-box attacks, which was recently shown in a large-scale study to improve almost all robustness evaluations compared to the original publications. To prevent overadaptation of new defenses to AutoAttack, we welcome external evaluations based on adaptive attacks, especially where AutoAttack flags a potential overestimation of robustness. Our leaderboard, hosted at https://robustbench.github.io/, contains evaluations of 120+ models and aims at reflecting the current state of the art in image classification on a set of well-defined tasks in \${\textbackslash}ell\_{\textbackslash}infty\$- and \${\textbackslash}ell\_2\$-threat models and on common corruptions, with possible extensions in the future. Additionally, we open-source the library https://github.com/RobustBench/robustbench that provides unified access to 80+ robust models to facilitate their downstream applications. Finally, based on the collected models, we analyze the impact of robustness on the performance on distribution shifts, calibration, out-of-distribution detection, fairness, privacy leakage, smoothness, and transferability.},
	urldate = {2025-01-23},
	booktitle = {{NeurIPS}},
	publisher = {arXiv},
	author = {Croce, Francesco and Andriushchenko, Maksym and Sehwag, Vikash and Debenedetti, Edoardo and Flammarion, Nicolas and Chiang, Mung and Mittal, Prateek and Hein, Matthias},
	month = oct,
	year = {2021},
	note = {arXiv:2010.09670 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{liu_convnet_2022,
	title = {A {ConvNet} for the 2020s},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Liu_A_ConvNet_for_the_2020s_CVPR_2022_paper.html},
	language = {en},
	urldate = {2025-01-23},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
	year = {2022},
	pages = {11976--11986},
}

@misc{carlini_towards_2017,
	title = {Towards {Evaluating} the {Robustness} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/1608.04644},
	doi = {10.48550/arXiv.1608.04644},
	abstract = {Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input x and any target classiﬁcation t, it is possible to ﬁnd a new input x′ that is similar to x but classiﬁed as t. This makes it difﬁcult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks’ ability to ﬁnd adversarial examples from 95\% to 0.5\%.},
	language = {en},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Carlini, Nicholas and Wagner, David},
	month = mar,
	year = {2017},
	note = {arXiv:1608.04644 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security},
}

@misc{madry_towards_2019,
	title = {Towards {Deep} {Learning} {Models} {Resistant} to {Adversarial} {Attacks}},
	url = {http://arxiv.org/abs/1706.06083},
	doi = {10.48550/arXiv.1706.06083},
	abstract = {Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist\_challenge and https://github.com/MadryLab/cifar10\_challenge.},
	language = {en},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
	month = sep,
	year = {2019},
	note = {arXiv:1706.06083 [stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{goodfellow_explaining_2015,
	title = {Explaining and {Harnessing} {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1412.6572},
	doi = {10.48550/arXiv.1412.6572},
	abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples—inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high conﬁdence. Early attempts at explaining this phenomenon focused on nonlinearity and overﬁtting. We argue instead that the primary cause of neural networks’ vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the ﬁrst explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
	language = {en},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	month = mar,
	year = {2015},
	note = {arXiv:1412.6572 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{dosovitskiy_image_2021-1,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	doi = {10.48550/arXiv.2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	language = {en},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv:2010.11929 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{he_deep_2016,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {https://ieeexplore.ieee.org/document/7780459/?arnumber=7780459},
	doi = {10.1109/CVPR.2016.90},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2025-01-23},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = jun,
	year = {2016},
	note = {ISSN: 1063-6919},
	keywords = {Complexity theory, Degradation, Image recognition, Image segmentation, Neural networks, Training, Visualization},
	pages = {770--778},
}

@article{krizhevsky_imagenet_2017,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {60},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	language = {en},
	number = {6},
	urldate = {2025-01-23},
	journal = {Communications of the ACM},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	month = may,
	year = {2017},
	pages = {84--90},
}

@misc{bartoldson_adversarial_2024,
	title = {Adversarial {Robustness} {Limits} via {Scaling}-{Law} and {Human}-{Alignment} {Studies}},
	url = {http://arxiv.org/abs/2404.09349},
	doi = {10.48550/arXiv.2404.09349},
	abstract = {This paper revisits the simple, long-studied, yet still unsolved problem of making image classifiers robust to imperceptible perturbations. Taking CIFAR10 as an example, SOTA clean accuracy is about \$100\$\%, but SOTA robustness to \${\textbackslash}ell\_\{{\textbackslash}infty\}\$-norm bounded perturbations barely exceeds \$70\$\%. To understand this gap, we analyze how model size, dataset size, and synthetic data quality affect robustness by developing the first scaling laws for adversarial training. Our scaling laws reveal inefficiencies in prior art and provide actionable feedback to advance the field. For instance, we discovered that SOTA methods diverge notably from compute-optimal setups, using excess compute for their level of robustness. Leveraging a compute-efficient setup, we surpass the prior SOTA with \$20\$\% (\$70\$\%) fewer training (inference) FLOPs. We trained various compute-efficient models, with our best achieving \$74\$\% AutoAttack accuracy (\$+3\$\% gain). However, our scaling laws also predict robustness slowly grows then plateaus at \$90\$\%: dwarfing our new SOTA by scaling is impractical, and perfect robustness is impossible. To better understand this predicted limit, we carry out a small-scale human evaluation on the AutoAttack data that fools our top-performing model. Concerningly, we estimate that human performance also plateaus near \$90\$\%, which we show to be attributable to \${\textbackslash}ell\_\{{\textbackslash}infty\}\$-constrained attacks' generation of invalid images not consistent with their original labels. Having characterized limiting roadblocks, we outline promising paths for future research.},
	urldate = {2024-12-10},
	publisher = {arXiv},
	author = {Bartoldson, Brian R. and Diffenderfer, James and Parasyris, Konstantinos and Kailkhura, Bhavya},
	month = jul,
	year = {2024},
	note = {arXiv:2404.09349 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, notion},
}

@inproceedings{dapello_simulating_2020,
	title = {Simulating a {Primary} {Visual} {Cortex} at the {Front} of {CNNs} {Improves} {Robustness} to {Image} {Perturbations}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/98b17f068d5d9b7668e19fb8ae470841-Abstract.html},
	abstract = {Current state-of-the-art object recognition models are largely based on convolutional neural network (CNN) architectures, which are loosely inspired by the primate visual system. However, these CNNs can be fooled by imperceptibly small, explicitly crafted perturbations, and struggle to recognize objects in corrupted images that are easily recognized by humans. Here, by making comparisons with primate neural data, we first observed that CNN models with a neural hidden layer that better matches primate primary visual cortex (V1) are also more robust to adversarial attacks. Inspired by this observation, we developed VOneNets, a new class of hybrid CNN vision models. Each VOneNet contains a fixed weight neural network front-end that simulates primate V1, called the VOneBlock, followed by a neural network back-end adapted from current CNN vision models. The VOneBlock is based on a classical neuroscientific model of V1: the linear-nonlinear-Poisson model, consisting of a biologically-constrained Gabor filter bank, simple and complex cell nonlinearities, and a V1 neuronal stochasticity generator. After training, VOneNets retain high ImageNet performance, but each is substantially more robust, outperforming the base CNNs and state-of-the-art methods by 18\% and 3\%, respectively, on a conglomerate benchmark of perturbations comprised of white box adversarial attacks and common image corruptions. Finally, we show that all components of the VOneBlock work in synergy to improve robustness. While current CNN architectures are arguably brain-inspired, the results presented here demonstrate that more precisely mimicking just one stage of the primate visual system leads to new gains in ImageNet-level computer vision applications.},
	urldate = {2025-01-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Dapello, Joel and Marques, Tiago and Schrimpf, Martin and Geiger, Franziska and Cox, David and DiCarlo, James J},
	year = {2020},
	pages = {13073--13087},
}

@inproceedings{nanda_measuring_2022,
	title = {Measuring {Representational} {Robustness} of {Neural} {Networks} {Through} {Shared} {Invariances}},
	url = {https://proceedings.mlr.press/v162/nanda22a.html},
	abstract = {A major challenge in studying robustness in deep learning is defining the set of “meaningless” perturbations to which a given Neural Network (NN) should be invariant. Most work on robustness implicitly uses a human as the reference model to define such perturbations. Our work offers a new view on robustness by using another reference NN to define the set of perturbations a given NN should be invariant to, thus generalizing the reliance on a reference “human NN” to any NN. This makes measuring robustness equivalent to measuring the extent to which two NNs share invariances. We propose a measure called {\textbackslash}stir, which faithfully captures the extent to which two NNs share invariances. {\textbackslash}stir re-purposes existing representation similarity measures to make them suitable for measuring shared invariances. Using our measure, we are able to gain insights about how shared invariances vary with changes in weight initialization, architecture, loss functions, and training dataset. Our implementation is available at: {\textbackslash}url\{https://github.com/nvedant07/STIR\}.},
	language = {en},
	urldate = {2025-01-17},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Nanda, Vedant and Speicher, Till and Kolling, Camila and Dickerson, John P. and Gummadi, Krishna and Weller, Adrian},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {16368--16382},
}

@misc{hebart_revealing_2020,
	title = {Revealing the multidimensional mental representations of natural objects underlying human similarity judgments},
	url = {https://osf.io/7wrgh},
	doi = {10.31234/osf.io/7wrgh},
	abstract = {Objects can be characterized according to a vast number of possible criteria (e.g. animacy, shape, color, function), but some dimensions are more useful than others for making sense of the objects around us. To identify these "core dimensions" of object representations, we developed a data-driven computational model of similarity judgments for real-world images of 1,854 objects. The model captured most explainable variance in similarity judgments and produced 49 highly reproducible and meaningful object dimensions that reflect various conceptual and perceptual properties of those objects. These dimensions predicted external categorization behavior and reflected typicality judgments of those categories. Further, humans can accurately rate objects along these dimensions, highlighting their interpretability and opening up a way to generate similarity estimates from object dimensions alone. Collectively, these results demonstrate that human similarity judgments can be captured by a fairly low-dimensional, interpretable embedding that generalizes to external behavior.},
	language = {en-us},
	urldate = {2025-01-17},
	publisher = {OSF},
	author = {Hebart, Martin N. and Zheng, Charles and Pereira, Francisco and Baker, Chris},
	month = mar,
	year = {2020},
	keywords = {big data, computational modeling, large-scale, mental representation, object recognition, similarity},
}

@misc{muttenthaler_improving_2023,
	title = {Improving neural network representations using human similarity judgments},
	url = {http://arxiv.org/abs/2306.04507},
	doi = {10.48550/arXiv.2306.04507},
	abstract = {Deep neural networks have reached human-level performance on many computer vision tasks. However, the objectives used to train these networks enforce only that similar images are embedded at similar locations in the representation space, and do not directly constrain the global structure of the resulting space. Here, we explore the impact of supervising this global structure by linearly aligning it with human similarity judgments. We find that a naive approach leads to large changes in local representational structure that harm downstream performance. Thus, we propose a novel method that aligns the global structure of representations while preserving their local structure. This global-local transform considerably improves accuracy across a variety of few-shot learning and anomaly detection tasks. Our results indicate that human visual representations are globally organized in a way that facilitates learning from few examples, and incorporating this global structure into neural network representations improves performance on downstream tasks.},
	urldate = {2025-01-17},
	publisher = {arXiv},
	author = {Muttenthaler, Lukas and Linhardt, Lorenz and Dippel, Jonas and Vandermeulen, Robert A. and Hermann, Katherine and Lampinen, Andrew K. and Kornblith, Simon},
	month = sep,
	year = {2023},
	note = {arXiv:2306.04507 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{park_adversarial_2024,
	address = {New York, NY, USA},
	series = {{AISec} '24},
	title = {Adversarial {Feature} {Alignment}: {Balancing} {Robustness} and {Accuracy} in {Deep} {Learning} via {Adversarial} {Training}},
	isbn = {9798400712289},
	shorttitle = {Adversarial {Feature} {Alignment}},
	url = {https://dl.acm.org/doi/10.1145/3689932.3694765},
	doi = {10.1145/3689932.3694765},
	abstract = {Deep learning models are continually improving in accuracy, but they remain vulnerable to adversarial attacks, often resulting in the misclassification of adversarial examples. Adversarial training can mitigate this problem by enhancing the model's robust accuracy on adversarial examples. However, such training typically compromises the model's standard accuracy on clean samples. The necessity for deep learning models to balance both robustness and accuracy for security is evident, but achieving this balance remains challenging, and the underlying reasons are yet to be clarified. This paper proposes an innovative pre-training method called Adversarial Feature Alignment (AFA) to address these problems. Our approach involves identifying the trade-off in the model's feature space and fine-tuning it to achieve accuracy on both standard and adversarial examples concurrently. Our research unveils an intriguing insight: misalignment within the feature space often leads to misclassification, regardless of whether the samples are benign or adversarial. AFA mitigates this risk by employing a novel optimization algorithm based on contrastive learning to alleviate potential feature misalignment. Through our evaluations, we demonstrate the superior performance of AFA. Our method delivers state-of-the-art robust accuracy while minimizing the drop in clean accuracy to 1.86\% and 8.91\% on CIFAR10 and CIFAR100, respectively, compared to cross-entropy. We also show that joint optimization of AFA and TRADES, accompanied by data augmentation using a recent diffusion model, achieves state-of-the-art accuracy and robustness. Through AFA, we expect to enhance security while preserving accuracy in deep learning models through adversarial training.},
	urldate = {2025-01-17},
	booktitle = {Proceedings of the 2024 {Workshop} on {Artificial} {Intelligence} and {Security}},
	publisher = {Association for Computing Machinery},
	author = {Park, Leo Hyun and Kim, Jaeuk and Oh, Myung Gyo and Park, Jaewoo and Kwon, Taekyoung},
	month = nov,
	year = {2024},
	pages = {101--112},
}

@misc{kornblith_similarity_2019,
	title = {Similarity of {Neural} {Network} {Representations} {Revisited}},
	url = {http://arxiv.org/abs/1905.00414},
	doi = {10.48550/arXiv.1905.00414},
	abstract = {Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify correspondences between representations in networks trained from different initializations.},
	urldate = {2025-01-17},
	publisher = {arXiv},
	author = {Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
	month = jul,
	year = {2019},
	note = {arXiv:1905.00414 [cs]},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning, notion},
}

@misc{sucholutsky_getting_2024-1,
	title = {Getting aligned on representational alignment},
	url = {http://arxiv.org/abs/2310.13018},
	doi = {10.48550/arXiv.2310.13018},
	abstract = {Biological and artificial information processing systems form representations of the world that they can use to categorize, reason, plan, navigate, and make decisions. How can we measure the similarity between the representations formed by these diverse systems? Do similarities in representations then translate into similar behavior? If so, then how can a system's representations be modified to better match those of another system? These questions pertaining to the study of representational alignment are at the heart of some of the most promising research areas in contemporary cognitive science, neuroscience, and machine learning. In this Perspective, we survey the exciting recent developments in representational alignment research in the fields of cognitive science, neuroscience, and machine learning. Despite their overlapping interests, there is limited knowledge transfer between these fields, so work in one field ends up duplicated in another, and useful innovations are not shared effectively. To improve communication, we propose a unifying framework that can serve as a common language for research on representational alignment, and map several streams of existing work across fields within our framework. We also lay out open problems in representational alignment where progress can benefit all three of these fields. We hope that this paper will catalyze cross-disciplinary collaboration and accelerate progress for all communities studying and developing information processing systems.},
	urldate = {2025-01-17},
	publisher = {arXiv},
	author = {Sucholutsky, Ilia and Muttenthaler, Lukas and Weller, Adrian and Peng, Andi and Bobu, Andreea and Kim, Been and Love, Bradley C. and Cueva, Christopher J. and Grant, Erin and Groen, Iris and Achterberg, Jascha and Tenenbaum, Joshua B. and Collins, Katherine M. and Hermann, Katherine L. and Oktar, Kerem and Greff, Klaus and Hebart, Martin N. and Cloos, Nathan and Kriegeskorte, Nikolaus and Jacoby, Nori and Zhang, Qiuyi and Marjieh, Raja and Geirhos, Robert and Chen, Sherol and Kornblith, Simon and Rane, Sunayana and Konkle, Talia and O'Connell, Thomas P. and Unterthiner, Thomas and Lampinen, Andrew K. and Müller, Klaus-Robert and Toneva, Mariya and Griffiths, Thomas L.},
	month = nov,
	year = {2024},
	note = {arXiv:2310.13018 [q-bio]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition},
}

@inproceedings{sucholutsky_alignment_2023,
	title = {Alignment with human representations supports robust few-shot learning},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/e8ddc03b001d4c4b44b29bc1167e7fdd-Abstract-Conference.html},
	abstract = {Should we care whether AI systems have representations of the world that are similar to those of humans? We provide an information-theoretic analysis that suggests that there should be a U-shaped relationship between the degree of representational alignment with humans and performance on few-shot learning tasks. We confirm this prediction empirically, finding such a relationship in an analysis of the performance of 491 computer vision models. We also show that highly-aligned models are more robust to both natural adversarial attacks and domain shifts. Our results suggest that human-alignment is often a sufficient, but not necessary, condition for models to make effective use of limited data, be robust, and generalize well.},
	urldate = {2025-01-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Sucholutsky, Ilia and Griffiths, Thomas L.},
	month = oct,
	year = {2023},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Statistics - Machine Learning, notion},
	pages = {73464--73479},
}
