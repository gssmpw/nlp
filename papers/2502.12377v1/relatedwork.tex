\section{Related Work}
\label{sec:related}
There has been substantial progress on bridging the representational differences between humans and machine learning models over the last few years. \cite{geirhos_partial_2021} shows many of the high-performance models match or in many cases exceed human feedforward performance on most of the OOD datasets studied. New models have also been introduced to promote both alignment and robustness. For example, \cite{dapello_simulating_2020} designs a new block for CNNs called the VOne block, which simulates V1 area processing. This work found that incorporating the VOne block into ResNet models increased robustness to both white box adversarial examples and common corruptions without sacrificing clean performance on ImageNet. \cite{li_learning_2019} introduced a technique for regularizing machine learning models based on human neural readings and found that the resultant regularized models were more robust and human-aligned.


\cite{subramanian_spatial-frequency_2023} shows that the property difference of the spatial frequency channel between humans and neural networks explains both shape bias and adversarial robustness of networks. 
Models with higher levels of human alignment have also been shown to be more robust to distribution shifts and ImageNet-A data~\cite{sucholutsky_alignment_2023}. Additionally, it has been shown that models tend to prioritize texture information over shape information~\cite{geirhos_imagenet-trained_2019, hermann_origins_2020} and that this bias extends to real-data decisions and is one of the major causes for vulnerability to natural adversarial samples~\cite{hoak_err_2024, hoak_explorations_2024}.