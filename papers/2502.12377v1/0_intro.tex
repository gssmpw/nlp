\section{Introduction}\label{sec:intro}
A longstanding goal in computer vision is to develop models that process images in a way that aligns with human perception. Representational alignment---how closely a model resembles biological vision---has been studied extensively with the goal of measuring, bridging, or increasing alignment in machine learning models~\cite{sucholutsky_getting_2024}. 
Recent observations suggest that alignment may have implications beyond neuroscience: models that are more aligned with human perception have also exhibited increased robustness to adversarial examples---inputs with near-imperceptible perturbations that induce model misclassification---~\cite{dapello_simulating_2020,li_learning_2019}, hinting at a deeper connection between alignment and security.

However, the relationship between representational alignment and adversarial robustness remains poorly understood. While the former seeks to align models with human cognition, adversarial examples in security highlight a fundamental misalignment: imperceptible perturbations can drastically degrade model accuracy while leaving human perception unaffected. Prior robustness techniques, such as adversarial training~\cite{madry_towards_2019}, are computationally expensive and potentially vulnerable to new attack strategies. Meanwhile, alignment research has not systematically examined whether more human-aligned models are inherently more robust to adversarial attacks. A fundamental question remains: do these objectives complement each other, leading to better-aligned and more robust models, or do they introduce conflicting trade-offs? 


% Solution
In this work, we investigate the relationship between human alignment and robustness to adversarial examples in vision models through a diverse, large-scale empirical analysis. In our analysis, we study \nummodels{} models across different architectures and training schemes, measure their alignment across 106 different benchmarks on neural, behavioral, and engineering tasks via the BrainScore library~\cite{schrimpf_brain-score_2018}. We then evaluate the adversarial robustness of these models using AutoAttack~\cite{croce_reliable_2020}, a state-of-the-art ensemble attack. 

In analyzing the correlations between model robustness and alignment, our findings reveal that while robustness is weakly correlated with vision alignment on average, certain alignment benchmarks serve as strong indicators of model robustness. Specifically, we find that the top six benchmarks that were most positively correlated with robust accuracy, even with strong perturbations, all measured a model's selectivity towards texture. These results suggest that different forms of alignment play distinct roles in model robustness, motivating further investigation into how alignment-driven approaches can be leveraged to build  more secure and perceptually-grounded vision models.

