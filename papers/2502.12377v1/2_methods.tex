\section{Methods}\label{sec:methods}

\shortsection{Alignment.} To measure alignment and download candidate models, we leverage the BrainScore~\cite{schrimpf_brain-score_2018} library. BrainScore provides a standardized framework for evaluating model similarity to biological vision through a set of neural, behavioral, and engineering benchmarks, supplying 106 benchmarks in total. These benchmarks quantify how closely a modelâ€™s internal representations and outputs correspond to neurophysiological recordings, human psychophysical behavior, and performance on engineered vision tasks. Neural alignment is measured by comparing activations from DNNs to neural recordings from primate visual cortex regions (e.g., V1, V2, V4, and IT), using similarity metrics like Representational Similarity Analysis (RSA) \cite{kriegeskorte_representational_2008}. Behavioral alignment assesses whether models replicate human psychophysical responses in object recognition and perturbation tests, while engineering alignment evaluates model robustness to controlled distortions, such as contrast reductions, or performance on out of distribution data. 


In total, the BrainScore library has documented benchmark scores for 434 models. Out of those, there are 197 models available in their registry (the remaining 237 models were either submitted privately or have been deprecated). From the 197 models in the registry, we removed an additional 72 models because either loading the model produced a ClientError due to a moved or removed model hosting location or the model was incompatible with ImageNet (either does not output 1000 classes or expects video streams). After this, we had to discard an additional 7 models, which represented all the VOne class models~\cite{dapello_simulating_2020} because they were not able to run on AutoAttack due to gradient alteration or masking, suggesting that previous results finding that VOne models are more robust to adversarial examples could have been due to overestimated robustness and highlighting the importance of evaluating robustness under comprehensive attack strategies. After this filtering process, we were left with \nummodels{} models (see \autoref{sec:appendix}) for our evaluation.

\shortsection{Robustness.} To evaluate the robustness of our models, we use AutoAttack~\cite{croce_reliable_2020, croce_robustbench_2021}, which serves as the standard for evaluating the robustness of neural networks due to its strong attack performance and fully automated parameter-free design. AutoAttack contains 4 attacks: APGD-CE, APGD-DLR, FAB, and Square Attack. By evaluating on AutoAttack, we are not only evaluating on the most performant attacks, but also integrating in both white-box attacks and black-box attacks which has been recommended in previous works to combat reporting overestimated robustness due to gradient masking or obfuscation~\cite{carlini_towards_2017}.

To better understand how the relationship between adversarial robustness and alignment changes as attacks change, we evaluate the $\ell_\infty$ robustness of our models at three different epsilon levels: $\epsilon = \{\frac{0.25}{255}, \frac{0.5}{255}, \frac{1}{255}\}$ to represent adversaries at different capability levels and small, medium, and large image distortion levels. While these values are typically lower than what would be benchmarked on platforms such as RobustBench~\cite{croce_robustbench_2021}, we choose these values with the goal of having a wide distribution of robust accuracies to identify separability between models, rather than the goal of bringing the model down to 0\% accuracy as what is typically done. 