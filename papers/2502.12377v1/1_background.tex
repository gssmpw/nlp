\section{Background}\label{sec:background}
\shortsection{Representational Alignment.}
Representational alignment studies the extent to which internal representations of machine learning models correspond to human cognitive processes. 
Early studies found that deep neural networks (DNNs) trained on large-scale image datasets develop hierarchical feature representations similar to those observed in the primate ventral stream, particularly in high-level visual areas like the inferior temporal (IT) cortex \cite{yamins_hierarchical_2013, schrimpf_brain-score_2018}. This led to efforts to quantify the alignment between artificial and biological vision, using techniques such as Representational Similarity Analysis (RSA) \cite{kriegeskorte_representational_2008} and Centered Kernel Alignment (CKA) \cite{kornblith_similarity_2019}. Current research in the area primarily focuses on measuring, bridging, and increasing both neural and behavioral alignment. 



To improve alignment, researchers have proposed strategies that incorporate cognitive constraints or psychological priors into model architectures~\cite{dapello_simulating_2020}. Supervised fine-tuning with human-annotated datasets~\cite{dosovitskiy_image_2021} ensures that learned representations align more closely with human-understandable features. Furthermore, novel techniques~\cite{muttenthaler_improving_2023,li_learning_2019,cheng_rtify_2024} have been developed to encourage similarity between model activations and human neural responses as recorded through fMRI and EEG experiments. In this study, we use a comprehensive set of neural, behavioral, and engineering alignment metrics to quantify representational alignment. 
  


\shortsection{Adversarial Examples.}
% Why do people care about adversarial examples? 
Although machine learning models have shown strong capabilities in achieving high accuracy across various tasks~\cite{liu_convnet_2022, dosovitskiy_image_2021, krizhevsky_imagenet_2017, he_deep_2016}, they remain vulnerable to adversarial examples~\cite{croce_reliable_2020,madry_towards_2019, carlini_towards_2017,goodfellow_explaining_2015, sheatsley_space_2023}. Adversarial examples are specially crafted inputs that contain perturbations which are imperceptible to humans, yet significantly decrease model accuracy. In computer vision systems, there have been many studies on developing attack algorithms, such as FGSM~\cite{goodfellow_explaining_2015}, PGD~\cite{madry_towards_2019}, and AutoAttack~\cite{croce_reliable_2020}. These methods  aim to maximize model's loss subject to constraints of perturbations defined by certain $\ell_p$-norms as follows:


\begin{center}
    $x_{adv} = \argmax_{\left \| \delta \right \|_{p}\leq\epsilon} L(x + \delta, y)$
\end{center}

where $x$ and $y$ represent the original image and its predicted label, respectively, $\delta$ is the perturbation to solve for, and $L$ is the model's loss function. The perturbation constraint $\epsilon$ is measured through an $\ell_p$-norm---most commonly $\ell_\infty$. While many works have historically evaluated the robustness of their model through the PGD attack~\cite{madry_towards_2019}, it has been shown that ``robust'' models can often suffer from gradient masking, causing gradient-based attacks like PGD to fail~\cite{athalye_obfuscated_2018}, and leading to a sense of overestimated robustness. To overcome this, multiple attacks, including both white- and black-box attacks should be used~\cite{carlini_evaluating_2019}. Thus, the  AutoAttack ensemble~\cite{croce_reliable_2020} has become the de-facto standard for evaluating robustness.



