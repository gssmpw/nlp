@inproceedings{dapello_simulating_2020,
	title = {Simulating a {Primary} {Visual} {Cortex} at the {Front} of {CNNs} {Improves} {Robustness} to {Image} {Perturbations}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/98b17f068d5d9b7668e19fb8ae470841-Abstract.html},
	abstract = {Current state-of-the-art object recognition models are largely based on convolutional neural network (CNN) architectures, which are loosely inspired by the primate visual system. However, these CNNs can be fooled by imperceptibly small, explicitly crafted perturbations, and struggle to recognize objects in corrupted images that are easily recognized by humans. Here, by making comparisons with primate neural data, we first observed that CNN models with a neural hidden layer that better matches primate primary visual cortex (V1) are also more robust to adversarial attacks. Inspired by this observation, we developed VOneNets, a new class of hybrid CNN vision models. Each VOneNet contains a fixed weight neural network front-end that simulates primate V1, called the VOneBlock, followed by a neural network back-end adapted from current CNN vision models. The VOneBlock is based on a classical neuroscientific model of V1: the linear-nonlinear-Poisson model, consisting of a biologically-constrained Gabor filter bank, simple and complex cell nonlinearities, and a V1 neuronal stochasticity generator. After training, VOneNets retain high ImageNet performance, but each is substantially more robust, outperforming the base CNNs and state-of-the-art methods by 18\% and 3\%, respectively, on a conglomerate benchmark of perturbations comprised of white box adversarial attacks and common image corruptions. Finally, we show that all components of the VOneBlock work in synergy to improve robustness. While current CNN architectures are arguably brain-inspired, the results presented here demonstrate that more precisely mimicking just one stage of the primate visual system leads to new gains in ImageNet-level computer vision applications.},
	urldate = {2025-01-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Dapello, Joel and Marques, Tiago and Schrimpf, Martin and Geiger, Franziska and Cox, David and DiCarlo, James J},
	year = {2020},
	pages = {13073--13087},
}

@inproceedings{geirhos_imagenet-trained_2019,
	title = {{ImageNet}-trained {CNNs} are biased towards texture; increasing shape bias improves accuracy and robustness},
	url = {http://arxiv.org/abs/1811.12231},
	abstract = {Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conﬂicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conﬂict. We show that ImageNettrained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classiﬁcation strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on ‘StylizedImageNet’, a stylized version of ImageNet. This provides a much better ﬁt for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent beneﬁts such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation.},
	language = {en},
	urldate = {2022-10-24},
	booktitle = {{ICLR}},
	author = {Geirhos, Robert and Rubisch, Patricia and Michaelis, Claudio and Bethge, Matthias and Wichmann, Felix A. and Brendel, Wieland},
	month = jan,
	year = {2019},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning, notion},
}

@inproceedings{geirhos_partial_2021,
	title = {Partial success in closing the gap between human and machine vision},
	url = {http://arxiv.org/abs/2106.07411},
	doi = {10.48550/arXiv.2106.07411},
	abstract = {A few years ago, the ﬁrst CNN surpassed human performance on ImageNet. However, it soon became clear that machines lack robustness on more challenging test cases, a major obstacle towards deploying machines “in the wild” and towards obtaining better computational models of human visual perception. Here we ask: Are we making progress in closing the gap between human and machine vision? To answer this question, we tested human observers on a broad range of out-ofdistribution (OOD) datasets, recording 85,120 psychophysical trials across 90 participants. We then investigated a range of promising machine learning developments that crucially deviate from standard supervised CNNs along three axes: objective function (self-supervised, adversarially trained, CLIP language-image training), architecture (e.g. vision transformers), and dataset size (ranging from 1M to 1B).},
	language = {en},
	urldate = {2025-01-28},
	booktitle = {35th {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {NeurIPS},
	author = {Geirhos, Robert and Narayanappa, Kantharaju and Mitzkus, Benjamin and Thieringer, Tizian and Bethge, Matthias and Wichmann, Felix A. and Brendel, Wieland},
	month = oct,
	year = {2021},
	note = {arXiv:2106.07411 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition},
}

@inproceedings{hermann_origins_2020,
	title = {The {Origins} and {Prevalence} of {Texture} {Bias} in {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1911.09071},
	abstract = {Recent work has indicated that, unlike humans, ImageNet-trained CNNs tend to classify images by texture rather than by shape. How pervasive is this bias, and where does it come from? We ﬁnd that, when trained on datasets of images with conﬂicting shape and texture, CNNs learn to classify by shape at least as easily as by texture. What factors, then, produce the texture bias in CNNs trained on ImageNet? Different unsupervised training objectives and different architectures have small but signiﬁcant and largely independent effects on the level of texture bias. However, all objectives and architectures still lead to models that make texturebased classiﬁcation decisions a majority of the time, even if shape information is decodable from their hidden representations. The effect of data augmentation is much larger. By taking less aggressive random crops at training time and applying simple, naturalistic augmentation (color distortion, noise, and blur), we train models that classify ambiguous images by shape a majority of the time, and outperform baselines on out-of-distribution test sets. Our results indicate that apparent differences in the way humans and ImageNet-trained CNNs process images may arise not primarily from differences in their internal workings, but from differences in the data that they see.},
	language = {en},
	urldate = {2022-12-08},
	booktitle = {{NeurIPS} 2020},
	publisher = {arXiv},
	author = {Hermann, Katherine L. and Chen, Ting and Kornblith, Simon},
	month = nov,
	year = {2020},
	note = {arXiv:1911.09071 [cs, q-bio]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition, notion},
}

@misc{hoak_err_2024,
	title = {Err on the {Side} of {Texture}: {Texture} {Bias} on {Real} {Data}},
	shorttitle = {Err on the {Side} of {Texture}},
	url = {http://arxiv.org/abs/2412.10597},
	doi = {10.48550/arXiv.2412.10597},
	abstract = {Bias significantly undermines both the accuracy and trustworthiness of machine learning models. To date, one of the strongest biases observed in image classification models is texture bias-where models overly rely on texture information rather than shape information. Yet, existing approaches for measuring and mitigating texture bias have not been able to capture how textures impact model robustness in real-world settings. In this work, we introduce the Texture Association Value (TAV), a novel metric that quantifies how strongly models rely on the presence of specific textures when classifying objects. Leveraging TAV, we demonstrate that model accuracy and robustness are heavily influenced by texture. Our results show that texture bias explains the existence of natural adversarial examples, where over 90\% of these samples contain textures that are misaligned with the learned texture of their true label, resulting in confident mispredictions.},
	urldate = {2025-01-31},
	publisher = {arXiv},
	author = {Hoak, Blaine and Sheatsley, Ryan and McDaniel, Patrick},
	month = dec,
	year = {2024},
	note = {arXiv:2412.10597 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security},
}

@inproceedings{hoak_explorations_2024,
	title = {Explorations in {Texture} {Learning}},
	url = {http://arxiv.org/abs/2403.09543},
	doi = {10.48550/arXiv.2403.09543},
	abstract = {In this work, we investigate {\textbackslash}textit\{texture learning\}: the identification of textures learned by object classification models, and the extent to which they rely on these textures. We build texture-object associations that uncover new insights about the relationships between texture and object classes in CNNs and find three classes of results: associations that are strong and expected, strong and not expected, and expected but not present. Our analysis demonstrates that investigations in texture learning enable new methods for interpretability and have the potential to uncover unexpected biases.},
	urldate = {2024-03-15},
	booktitle = {{ICLR} 2024, {Tiny} {Papers} {Track}},
	publisher = {arXiv},
	author = {Hoak, Blaine and McDaniel, Patrick},
	month = mar,
	year = {2024},
	note = {arXiv:2403.09543 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, notion},
}

@inproceedings{li_learning_2019,
	title = {Learning from brains how to regularize machines},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/70117ee3c0b15a2950f1e82a215e812b-Abstract.html},
	abstract = {Despite impressive performance on numerous visual tasks, Convolutional Neural Networks (CNNs) --- unlike brains --- are often highly sensitive to small perturbations of their input, e.g. adversarial noise leading to erroneous decisions. We propose to regularize CNNs using large-scale neuroscience data to learn more robust neural features in terms of representational similarity. We presented natural images to mice and measured the responses of thousands of neurons from cortical visual areas. Next, we denoised the notoriously variable neural activity using strong predictive models trained on this large corpus of responses from the mouse visual system, and calculated the representational similarity for millions of pairs of images from the model's predictions. We then used the neural representation similarity to regularize CNNs trained on image classification by penalizing intermediate representations that deviated from neural ones. This preserved performance of baseline models when classifying images under standard benchmarks, while maintaining substantially higher performance compared to baseline or control models when classifying noisy images. Moreover, the models regularized with cortical representations also improved model robustness in terms of adversarial attacks. This demonstrates that regularizing with neural data can be an effective tool to create an inductive bias towards more robust inference.},
	urldate = {2025-01-31},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Li, Zhe and Brendel, Wieland and Walker, Edgar and Cobos, Erick and Muhammad, Taliah and Reimer, Jacob and Bethge, Matthias and Sinz, Fabian and Pitkow, Zachary and Tolias, Andreas},
	year = {2019},
	keywords = {notion},
}

@inproceedings{subramanian_spatial-frequency_2023,
	title = {Spatial-frequency channels, shape bias, and adversarial robustness},
	abstract = {What spatial frequency information do humans and neural networks use to recognize objects? In neuroscience, critical band masking is an established tool that can reveal the frequency-selective filters used for object recognition. Critical band masking measures the sensitivity of recognition performance to noise added at each spatial frequency. Existing critical band masking studies show that humans recognize periodic patterns (gratings) and letters by means of a spatial-frequency filter (or “channel”) that has a frequency bandwidth of one octave (doubling of frequency). Here, we introduce critical band masking as a task for network-human comparison and test 14 humans and 76 neural networks on 16-way ImageNet categorization in the presence of narrowband noise. We find that humans recognize objects in natural images using the same one-octave-wide channel that they use for letters and gratings, making it a canonical feature of human object recognition. Unlike humans, the neural network channel is very broad, 2-4 times wider than the human channel. This means that the network channel extends to frequencies higher and lower than those that humans are sensitive to. Thus, noise at those frequencies will impair network performance and spare human performance. Adversarial and augmented-image training are commonly used to increase network robustness and shape bias. Does this training align network and human object recognition channels? Three network channel properties (bandwidth, center frequency, peak noise sensitivity) correlate strongly with shape bias (51\% variance explained) and robustness of adversarially-trained networks (66\% variance explained). Adversarial training increases robustness but expands the channel bandwidth even further beyond the human bandwidth. Thus, critical band masking reveals that the network channel is more than twice as wide as the human channel, and that adversarial training only makes it worse. Networks with narrower channels might be more robust.},
	language = {en},
	booktitle = {Conference on {Neural} {Information} {Processing} {Systems}},
	publisher = {NeurIPS},
	author = {Subramanian, Ajay and Sizikova, Elena and Majaj, Najib J and Pelli, Denis G},
	year = {2023},
}

@inproceedings{sucholutsky_alignment_2023,
	title = {Alignment with human representations supports robust few-shot learning},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/e8ddc03b001d4c4b44b29bc1167e7fdd-Abstract-Conference.html},
	abstract = {Should we care whether AI systems have representations of the world that are similar to those of humans? We provide an information-theoretic analysis that suggests that there should be a U-shaped relationship between the degree of representational alignment with humans and performance on few-shot learning tasks. We confirm this prediction empirically, finding such a relationship in an analysis of the performance of 491 computer vision models. We also show that highly-aligned models are more robust to both natural adversarial attacks and domain shifts. Our results suggest that human-alignment is often a sufficient, but not necessary, condition for models to make effective use of limited data, be robust, and generalize well.},
	urldate = {2025-01-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Sucholutsky, Ilia and Griffiths, Thomas L.},
	month = oct,
	year = {2023},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Statistics - Machine Learning, notion},
	pages = {73464--73479},
}

