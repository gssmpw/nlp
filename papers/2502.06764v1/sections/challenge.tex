\section{Challenges when Guiding with History}
\label{sec:history_guidance_challenges}

\input{figures/method/sampling} 


Video diffusion models are conditional diffusion models  $p(\bx|\bc)$, where $\bx$ denotes frames to be generated, and $\bc$ represents the conditioning (e.g. text prompt, or a few observed prior frames). For simplicity, we refer to the latter as \emph{history}, even when the observed images could be e.g. a subset of keyframes that are spaced across time. Our discussion of $\bc$ will focus exclusively on history conditioning and exclude text or other forms of conditioning in notation. Formally, let $\bx_{\cT}$ denote a $T$-frame video clips with indices $\cT = \{1, 2, \ldots, T\}$. Define $\cH \subset \cT$ as the indices of history frames used for conditioning, and $\cG = \cT \setminus \cH$ as the indices of the frames to be generated. Our objective is to model the conditional distribution $p(\xG | \xH)$ with a diffusion model. 

We aim to extend classifier-free guidance (CFG) to this setting. Since the history $\xH$ serves as conditioning, sampling can be performed by estimating the following score: 
\begin{equation} 
\label{eq:history_guidance}
\score p_k(\xGk)
+ \omega \big[\score p_k(\xGk|\xH)  - \score p_k(\xGk)\big].
\end{equation}
This approach differs from conventional CFG in two ways: 1) The generation $\xG$ and conditioning history $\xH$ belong to the same signal $\bx_{\cT}$, differing only in their indices $\cG, \cH\subset \cT$; thus, the generated $\xG$ can be reused as conditioning $\xH$ for generating subsequent frames. 2) The history $\xH$ can be any subset of $\cT$, allowing its length to vary. Guiding with history, therefore, requires a model that can estimate both conditional and unconditional scores given arbitrary subsets of video frames. Below, we analyze how these differences present challenges for implementation within the current paradigm of video diffusion models (VDMs).

\textbf{Architectures with fixed-length conditioning.}
As shown in \cref{fig:architecture-conventional}, DiT~\cite{peebles2023scalable} or U-Net-based diffusion models~\cite{bao2023all,rombach2022high} typically inject conditioning using AdaLN~\cite{peebles2023scalable, perez2018film} layers or by concatenating the conditioning with noisy input frames along the channel dimension.
This design constrains conditioning to a fixed-size vector. While some models adopt sequence encoders for variable-length conditioning (e.g., for text inputs), these encoders are often pre-trained~\cite{yang2024cogvideox} and cannot share parameters with the diffusion model to encode history frames. %
Consequently, guidance has been limited to fixed-length and generally short history~\cite{blattmann2023stable, xing2023dynamicrafter, yang2024cogvideox, watson2024controlling}.

\textbf{Framewise Binary Dropout performs poorly.} 
Classifier-free guidance is typically implemented using a single network that jointly represents the conditional and unconditional models. These are trained via \emph{binary dropout}, where the conditioning variable $\bc$ is randomly masked during training  with a certain probability. 
History guidance can, in principle, be achieved by randomly dropping out subsets of history frames during training.
However, our ablations (Sec.~\ref{sec:exp_ablation}) reveal that this approach performs poorly. We hypothesize that this is due to inefficient token utilization: although the model processes all $|\cT|$ frames via attention, only a random subset of $|\cG|$ frames contribute to the loss. This becomes more pronounced as videos grow longer, making framewise binary dropout a suboptimal choice.



