\section{The Diffusion Forcing Transfomer}
\label{sec:dft}


In this section, we introduce the \method (\mtd), a simple yet powerful video diffusion framework designed to model score functions associated with \emph{different portions of history}. This includes variable-length histories, arbitrary subsets of frames, and even history processed at different image-domain frequencies. \mtd improves video generation performance as a base model even without guidance. By addressing the challenges outlined in \cref{sec:history_guidance_challenges}, \mtd further enables guidance with flexible history and a more advanced family of history guidance methods described in \cref{sec:our_history_guidance}.

 \textbf{Noise as Masking.}
The forward diffusion process turns the $t$-th frame $\bx_t$ of a video sequence into a noisy frame $\xtk$ at noise levels $k_t \in [0, 1]$.
 One can interpret this as progressively masking $\bx_t$ with noise~\cite{chen2024diffusion} - $\bx_t^0$ is clean and hence unmasked, $\bx_t^1$ is \emph{fully masked} and contains no information about the original $\bx_t$. Intermediate noise levels $(0 < k_t < 1)$ yield a \emph{partially masked} frame $\xtk$,  retaining a noisy snapshot of the original frame's information.

\textbf{History as noise-free frames.} Denoising generated frames $\xGk$ conditioned on history $\xH$ can be unified under the noise-as-masking framework. Specifically, this involves denoising the entire sequence of frames $\xH \cup \xGk$ with noise levels $k_{\cT} = \left[k_1, k_2, \cdots, k_T\right]$ defined as:
\vspace{-0.09in}
\begin{equation}
    k_t = 
    \begin{cases} 
    0 & \text{if } t \in \cH \\
    k & \text{if } t \in \cG. \\
    \end{cases}
    \vspace{-5pt}
\label{eqn:history_as_clean}
\end{equation}
This formulation treats history and generated frames as parts of the same input to the transformer, rather than separating history as a distinct ``conditioning'' input (see \cref{fig:architecture} and \cref{sec:history_guidance_challenges}). This unification allows any full-sequence transformer to be fine-tuned into a history-conditional model with variable-length history, simply by varying the noise levels within each sequence.

\textbf{Training: Per-frame Independent Noise Levels.} As illustrated in \cref{fig:architecture-ours}, instead of setting noise levels to zero for all history frames, we adopt \emph{per-frame independent noise levels} introduced in Diffusion Forcing~\cite{chen2024diffusion}. Each frame $\bx_t \in \bx_\cT$ is assigned an independent noise level $k_t \in [0, 1]$, resulting in random sequences of noise levels $k_{\cT}$ in contrast with Equation~\ref{eqn:history_as_clean}.
{}
The \mtd model is then trained to minimize the following noise prediction loss, where $\beps_\cT$ denotes noise added to all frames: 
\vspace{-5pt}
\begin{equation}
\label{eq:train}
\mathop{\mathlarger{\mathbb{E}}}_{\substack{k_{\cT}, \bx_\cT, \beps_\cT}}
\Big[\| \beps_\cT - \beps_\theta(\bx_\cT^{k_{\cT}},k_{\cT}) \|^2\Big],
\vspace{-5pt}
\end{equation}
 Crucially, noise levels are selected independently for all frames without distinguishing the past and the future. This enables parallel training while also allowing \emph{non-causal} conditioning on partially masked future frames. In ~\cref{app:method_details_objective_causal}, we further discuss a simplified objective when $\max(|\cH|)\ll T$ and a causal adaptation of our model. 
{}
In Appendix~\ref{appendix:theory_elbo}, we justify this training objective as optimizing a (reweighted) valid Evidence Lower Bound (ELBO) on the expected log-likelihoods:
\begin{theorem}[Informal] 
    The \mtd training objective (\Cref{eq:train}) optimizes a reweighting of an Evidence Lower Bound (ELBO) on the expected log-likelihoods. 
    \label{theorem:informal}
\end{theorem}
Compared to conventional video diffusion methods, where a single noise level $k \in [0,1]$  is uniformly applied to all generation frames $\xG$, our approach provides two key benefits: (1) token utilization is improved by computing a loss conditioned on all frames $\bx_{\cT}$ instead of a smaller subset; second, (2) this objective places variable history lengths ``in-distribution'' of the training objective, leading to more flexible use of history lengths as detailed below.  

\textbf{Sampling: Conditioning on Arbitrary History.} Unlike standard VDMs that require fixed-length history during sampling, \mtd allows conditioning on arbitrary history. To generate $\xG$ conditioned on $\xH$ at each sampling step with noise level $k$, we estimate the conditional score $\score p_k(\xGk|\xH)$ by feeding the model noisy $\xGk$ and clean history frames $\bx^0_{\cH}$. Sampling is then performed using standard score-based sampling schemes such as DDPM~\cite{ho2020denoising} or DDIM~\cite{ddim}. This flexibility in conditioning enables history guidance and its more advanced variants, as described in the next section.

