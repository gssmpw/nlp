\subsection{Sampling with \mtd and History Guidance}
\newcommand{\algcomment}[1]{\small{\hfill \(\triangleright\) #1}}
\begin{algorithm}[t]
    \caption{\textbf{Flexible Sampling with \mtd and (optionally) History Guidance}}
    \label{alg:sampling}
    \begin{algorithmic}
    \STATE {\bfseries Task:} specified by indices $\cH$, $\cG = \cT \setminus \cH$, and history frames $\xH$.
    \STATE {\bfseries Input:} diffusion process defined by $\alpha_k, \sigma_k$, diffusion sampler $\mathcal{S}$ with sampling steps $N$,\\
    \textbf{\mtd} model $\rvs_\vtheta(\cdot, \cdot)$, and
    \textbf{History Guidance} scheme specified by $\{(\cH_i, k_{\cH_i}, \omega_i)\}_{i=1}^I$.
    \STATE Sample $\rvx_\cG ~\sim \mathcal{N}(0, I)$, then $\rvx_{\cT} \gets \rvx_{\cH} \oplus \rvx_{\cG}$ \algcomment{Sample random noise for generation frames}
    \FOR{$n=N, N-1, \ldots, 1$}
        \STATE $k_{\cT} \gets (k_t)_{t=1}^T$ where {\small$\begin{cases} k_t = \frac{n}{N} & \text{if } t \in \cG \\ k_t = 1 & \text{if } t \in \cH \end{cases}$}
        \STATE $\hat{\rvx}_{\cT} \gets \rvx_{\cT}$, then \emph{replace} $\hat{\rvx}_{\cH} \gets \beps$ where $\beps \sim \mathcal{N}(0, I)$ \algcomment{Fully mask history}
        \STATE $\hat{\rvs}^{\varnothing} \gets \rvs_\vtheta(\hat{\rvx}_{\cT}, k_{\cT})$ \algcomment{Estimate unconditional score}
        \FOR{$i=1, \ldots, I$}
            \STATE $k_{\cT} \gets (k_t)_{t=1}^T$ where {\small$\begin{cases} k_t = \frac{n}{N} & \text{if } t \in \cG \\ k_t = k_{\cH_i} & \text{if } t \in \cH_i \\ k_t = 1 & \text{if } t \in \cH \setminus \cH_i \end{cases}$}
            \STATE $\hat{\rvx}_{\cT} \gets \rvx_{\cT}$, then \emph{replace} $\begin{cases} \hat{\rvx}_{\cH_i} \gets \alpha_{k_{\cH_i}} \hat{\rvx}_{\cH_i} + \sigma_{k_{\cH_i}} \beps \text{ where } \beps \sim \mathcal{N}(0, I) \\ \hat{\rvx}_{\cH \setminus \cH_i} \gets \beps \text{ where } \beps \sim \mathcal{N}(0, I) \end{cases}$ \algcomment{Mask history based on $\cH_i$ and $k_{\cH_i}$}
            \STATE $\hat{\rvs}^i \gets \rvs_\vtheta(\hat{\rvx}_{\cT}, k_{\cT})$ \algcomment{Estimate $i$-th conditional score}
        \ENDFOR
        \STATE $\hat{\rvs} \gets \hat{\rvs}^{\varnothing} + \sum_{i=1}^I \omega_i \cdot (\hat{\rvs}^i - \hat{\rvs}^{\varnothing})$ \algcomment{Compose scores}
        \STATE $\rvx_{\cG} \gets \mathcal{S}(\rvx_{\cG}, \hat{\rvs}_{\cG}; \frac{n}{N}, \frac{n-1}{N})$ \algcomment{Denoise $k = \frac{n}{N} \rightarrow \frac{n-1}{N}$}
        
    \ENDFOR
    \STATE {\bfseries Output:} $\rvx_\cG$
    \end{algorithmic}
\end{algorithm}

\mtd is capable of flexible sampling conditioning on \emph{arbitrary history}, and is further capable of performing \emph{history guidance}, a family of guidance methods we propose. In \cref{alg:sampling}, we provide a detailed sampling procedure for \mtd and history guidance, where any score-based sampler such as DDPM~\cite{ho2020denoising} or DDIM~\cite{ddim} can be used for $\mathcal{S}$. Importantly, when estimating a score conditioned on a masked history, it is crucial to pass the corresponding noise levels $k_\cT$ and to \emph{replace} the clean history frames with noisy frames, which are created by diffusing the clean history to the noise levels. This ensures that the model input is consistent with what it encounters during training time. Note that \cref{alg:sampling} can be applied given arbitrary history frames. For instance, to \emph{extrapolate} the history of length $\tau$ to $T$ frames, set $\cH = \{1, \ldots, \tau\}$ and $\cG = \{\tau+1, \ldots, T\}$; to \emph{interpolate} between two frames, set $\cH = \{1, T\}$ and $\cG = \{2, \ldots, T-1\}$. Below we provide several representative examples of how the algorithm is applied:
\begin{itemize}[topsep=0pt, itemsep=0pt]
    \item \textbf{Conditional Sampling without History Guidance}: $\{(\cH_i, k_{\cH_i}, \omega_i)\}_{i=1}^I = \{(\cH, 0, 1)\}$
    \item \textbf{Vanilla History Guidance} with a guidance scale $\omega > 1$: $\{(\cH_i, k_{\cH_i}, \omega_i)\}_{i=1}^I = \{(\cH, 0, \omega)\}$
    \item \textbf{Temporal History Guidance} with $I$ subsequences $\{\cH_i\}_{i=1}^I$ and guidance scales $\{\omega_i\}_{i=1}^I$: $\{(\cH_i, k_{\cH_i}, \omega_i)\}_{i=1}^I = \{(\cH_i, 0, \omega_i)\}_{i=1}^I$
    \item \textbf{Fractional History Guidance} with a guidance scale $\omega$ and fractional masking level $k_\cH$: $\{(\cH_i, k_{\cH_i}, \omega_i)\}_{i=1}^I = \{(\cH, 0, 1), (\cH, k_\cH, \omega - 1)\}$
\end{itemize}

\subsection{Simplifying Training Objective}
\label{app:method_details_objective_causal}
Diffusion Forcing~\cite{chen2024diffusion} proposes to train the entire sequence with independent noise per frame. A natural question to ask is whether this mixed objective includes too many tasks compared to what one actually needs. Here we provide some insights from our experiments throughout the project: When the number of frames is small e.g. $10$ latent frames, there is no noticeable decrease in training efficiency - Diffusion Forcing seems to converge as fast as standard diffusion from both training and validation curves. However, when we grow the number of latent frames to $50$, we start to witness decreased performance at sampling time. While we firmly believe that binary dropout is not the ideal way to achieve objective reduction from our experiments, we believe that one can easily reduce our training objective by only applying independent noise up to the maximum training length one wants to support. In particular, if one wants to generate the next $10$ frames from previous $1-10$ frames, it doesn't seem necessary for frame $11$ to be independently masked as noise from time to time, since we will never need to mask it out for flexible conditioning. In addition, one may want to consider treating the number of history frames as a random variable at training time, sampling a length first and then applying uniform levels of masking to the history, though independent from the noise level of the generation target. We didn't investigate these simplifications in detail because we simply find Diffusion Forcing's training objective very versatile for many of the tasks we want to do, e.g. interpolation, and varying noise level sampling. However, we do believe that these schemes could worth more exploration if one is to scale up our method to a much bigger number of context frames.

\subsection{Causal Variant}
In principle, one can implement \mtd and History Guidance with a causal transformer as well. For example, CausVid~\cite{yin2024slow} has proved the effectiveness of Diffusion Forcing on fast causal video synthesis and doesn't conflict with History Guidance. However, we'd like to highlight that one can also use our non-causal \mtd to achieve causal sampling. Different from traditional transformer-based models, \mtd doesn't need to enforce an attention mask to achieve causality. Instead, at generation time, one can mask out the future with white noise to prevent any information from the future from leaking into the neural network. In fact, there might be use cases when one may want some low-frequency information from the future, and then one can fractionally mask out the future via noise as masking to achieve so. On the other hand, the motivation behind causal video diffusion models is often speed and real-time generation using KV caching. In that case, one either needs to train a causal \mtd directly or consult advanced techniques like attention sink~\cite{xiao2023efficient} to perform windowed attention effectively.

\subsection{Incorporating Other Conditioning}
Throughout our discussions in the main paper, conditioning is history exclusively. What if one wants to integrate the \method into a text-conditioned diffusion model? One claim of the \mtd is that it doesn't require architectural changes so one can fine-tune an existing model into a \mtd model. This is still the case here: if one already has a text-conditioned video diffusion model, presumably built to accept such conditioning via an adaptive layer norm, one simply take \mtd as an add on to their existing architecture to obtain a \mtd model that accepts both text and history as conditioning. \mtd's Figure~\ref{fig:architecture} does not assert that one cannot use an external AdaLN layer with \mtd, but is rather saying no architectural changes is needed.

\subsection{Extended Temporal History Guidance}
\label{app:method_details_temporal}

Temporal history guidance addresses the challenge of out-of-distribution (OOD) history by composing scores conditioned on different, shorter history subsequences, which are closer to being in-distribution. However, since the model receives the entire video sequence as input during sampling—including both the history and the noisy frames being generated—the OOD problem can arise throughout the entire video sequence, not just in the history portion. To mitigate this, we propose further decomposing the generation $\cG$ into generation subsequences $\cG_1, \cG_2, \ldots, \cG_{J} \subset \cG$. In line with the original temporal history guidance, the history $\cH$ is already decomposed into history subsequences $\cH_1, \cH_2, \ldots, \cH_{I} \subset \cH$. This allows us to compose scores conditioned on even shorter, and thus more in-distribution, subsequences in $\{\cH_i\}_{i=1}^{I} \times \{\cG_j\}_{j=1}^{J}$. Specifically, the composed score is given by:
\begin{equation}
    \scalebox{1.0}{$
    \bigoplus_{j=1}^{J} \sum_{i=1}^{I} \score p_k(\rvx_{\cG_j}^k | \rvx_{\cH_i})$}
\end{equation}
where $\bigoplus$ denotes a frame-wise averaging operation. We refer to this method as \emph{Extended Temporal History Guidance}, as it extends the concept of temporal history guidance by composing both history and generation subsequences. Empirically, we find this method to be more effective than the original temporal history guidance when the video sequence is clearly OOD (e.g., RealEstate10K OOD history experiment), and thus requires shorter subsequences to be in-distribution.
