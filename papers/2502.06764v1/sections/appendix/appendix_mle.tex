\subsection{A Maximum Likelihood Interpretation for Score Addition.}
\label{appendix:add_score}
The \method achieves history guidance across time and frequency by sampling with linearly weighted diffusion scores conditioned on different history lengths. Though this appears to be purely heuristic, as in classifier-free guidance, we provide a meaningful probabilistic interpretation of the algorithm. 

\textbf{Intuition for guidance via Gaussian MLE.} We begin by justifying linearly combining scores in simple Gaussian models. For now, let us assume that the goal is to sample $\bx \sim q^\star(\bx)$, and the aim is to estimate the score $s^\star(\bx) = \nabla_{\bx} \ln q(\bx)$. 

\newcommand{\veceps}{\vec{\bm{\epsilon}}}
We make a strong assumption that we have $N$ estimators for the score functions, $(\hat s_i(\bx))_{1 \le i \le n}$, and that errors are Gaussian. %
\begin{assumption}[Gaussian Errors] We assume that, conditioned on $\bx$, the errors $\veceps := (\hat s_1(\bx) - s^\star(\bx), \hat s_2(\bx) - s^\star(\bx), \dots, \hat s_n(\bx) - s^\star(\bx))$ form a Gaussian vector with mean zero and covariance $\bSigma(\bx) \in \R^{dn \times dn}$.    
\end{assumption}
Though the assumption is clearly not true in practice, it helps build intuition for the idea. Moreover, given that the reverse process of an SDE essentially involves Gaussian predictions, it is plausible to expect that the individual steps of the denoising process model Gaussian distributions, and consequently, errors are ``Gaussian-like'' \cite{huang2023diffusion} .


\newcommand{\smle}{\hat s^{\textsc{mle}}}
\newcommand{\argmax}{\mathrm{argmax}}
Let us now consider the maximum likelihood score estimator in this model. We introduce the notation 

\begin{align}
    \mathbb{I}^\top= [\mathbf{I}_{d\times d}^\top,\mathbf{I}^\top_{d\times d} \dots \mathbf{I}^\top_{d\times d}]^\top.
\end{align}
In this case, we have
\begin{align}
\hat{\mathbf{s}}_{1:n}(\bx) = (\hat s_1(\bx), \hat s_2(\bx),\dots, s_n(\bx)) \mid \bx \sim \cN(\mathbb I s^\star(\bx), \bm \Sigma(\bx)).
\end{align}
Let us now characterize the maximum likelihood estimator, $\smle$. This solves
\begin{align*}
    \smle(\bx) &= \argmax_{s(\bx)} p(\hat{\bm{s}}_{1:n}(\bx); s(\bx))\\
    &=\arg\max_{s(\bx)} \ \frac{1}{\sqrt{(2\pi)^k |\boldsymbol{\Sigma}|}} \exp\left(-\frac{1}{2} \veceps^\top \bSigma(\bx)^{-1} \veceps(\bx)\right) \tag{$\veceps = \hat{\mathbf{s}}_{1:n} - \mathbb I s(\bx)$}\\
    &=\min_{s(\bx)}   \frac{1}{2} \veceps^\top \bSigma(\bx)^{-1} \veceps(\bx)  \tag{$\veceps = \hat{\mathbf{s}}_{1:n} - \mathbb I s^\star(\bx)$}\\
&= \arg\min_{s(\bx)}  (\hat{\mathbf s}_N(\bx)-\mathbb{I} s^\star(\bx))^\top \bm\Sigma(\bx)^{-1} (\hat{\mathbf s}_N(\bx)-\mathbb{I} s^\star(\bx)).
\end{align*}
\newcommand{\shatonen}{\hat{\mathbf{s}}_{1:n}}

An exercise in Calculus reveals that 
\begin{align}
    \smle(\bx) = \bm \left(\mathbb I^\top \Sigma(\bx)^{-1} \mathbb{I}\right)^{-1}\left(\mathbb I^\top \Sigma(\bx)^{-1}\right) \hat{\mathbf s}_{1:n}(\bx).
\end{align}
In other words, $\smle$ is some ($\bx$-dependent) linear function of $\shatonen$. 

We now describe a couple of special cases:

\paragraph{Case 1: $d =1$ ($\bx$ is scalar) scores are independent.} In this case, $\bSigma(\bx)$ has a diagonal inverse, and by positive definiteness, its entries are strictly positive. Thus, letting $\alpha_i$ denote the diagonal entries of $\bSigma(\bx)^{-1}$, we have $\mathbb{I}^\top\bSigma(\bx)^{-1}$ is a vector with strictly positive entries  $(\alpha_1(\bx),\dots,\alpha_n(\bx))$, and $\mathbb{I}^\top\bSigma(\bx)^{-1} \mathbb{I} = \sum_{i=1}^n \alpha_i(\bx)$ is their sum. 

In this case,     \begin{align}
    \smle(\bx) = \sum_{i=1}^n \frac{\alpha_i}{(\sum_{j}\alpha_j(\bx))}\hat{{s}}_i(\bx)
\end{align}
is a convex combination of the various scores. 

\paragraph{Case 2: general $d$ ($\bx$ is scalar) scores are independent, and the errors $\hat{s}_i - s^\star$ have scaled identity covariance.} In this case, $\bSigma(\bx)$ is block diagonal with scaled-indenity blocks, so we can also show 
  \begin{align}
    \smle(\bx) = \sum_{i=1}^n \frac{\alpha_i(\bx)}{(\sum_{j}\alpha_j(\bx))}\hat{{s}}_i(\bx),
\end{align}
where $\alpha_i^{-1}$ are the scalings of the identity blocks. 














Now we can examine the specific case of history guidance. Let the $n$ pieces of evidences be the $n$ different history segments of different lengths that our model condition on. \method is essentially trying to combine these evidences with Maximum A Posteriori (MAP) to get an overall estimation of the score of future tokens.


\textbf{Why  MLE / Averaging Works in General?}
Though the averages derived above hold for Gaussian case, there is a very general theory for combining multiple estimators into one called \emph{Optimal Aggregation of Estimators} (see, e.g. \cite{rigollet2007linear}). In this case, even beyond Gaussian settings, there are known benefits to optimizing over the convex hull of a family of estimators rather than choosing the best single one (see, e.g. \cite{bellec2017optimal}).
Another rational for combining estimators is that an average of $n$ estimators can do better than the best single estimator. 
\newcommand{\cX}{\mathcal{X}}%
Indeed, suppose that you have $n$ maps $\hat{s}_i: \mathbf{x}  \in \cX \to [0,1]$, and assume that the optimal value (for simplicity) is $s^\star_i(\bx) = 0$ (also, scalar for simplicity). Suppose you partition the $\mathbf x$ space into $n$ components $\cX_1,\dots,\cX_n$ such that
\begin{align}
    \Pr[\mathbf{x} \in \cX_i] = \frac{1}{n}, \quad \hat{s}_i(\bx) = \begin{cases} 1 & \mathbf x \in \cX_i\\
    0 & \text{otherwise}
    \end{cases}
\end{align}
For any estimator, the expected square error is then 
\begin{align}
    \Exp[(\hat{s}_i)^2] = \mathbb{P}[\mathbf x \in \cX_i] = \frac{1}{n}.
\end{align}
However, for any $\bx$, $\frac{1}{n}\sum_{i=1}^n\hat{s}_i(\bx) = \frac{1}{n}\sum_{i}^n \mathbb{I}(x \in \cX_i) = \frac{1}{n}$. Thus, 
\begin{align}
    \Exp\left[\left(\frac{1}{n}\sum_{i=1}^n\hat{s}_i\right)^2\right] = \mathbb{P}[\mathbf x \in \cX_i] = \frac{1}{n^2}.
\end{align}
Because estimators make errors on complementary regions of state space, they work in concert to cancel out errors to reduce overall error. 

We suspect history guidance functions in a similar fashion: though attending to different history contexts may result in errors for different realizations of past frames, but by averaging all these effects out, we ameliorate total error. 








