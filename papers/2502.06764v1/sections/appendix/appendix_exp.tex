\section{Experimental Details}
\label{app:exp_details}

Below, we provide additional details on datasets, architectures, training, evaluation metrics, and protocols for our experiments.

\subsection{Datasets}
\label{app:exp_details_datasets}

\textbf{Kinetics-600~\cite{kay2017kinetics}} is a widely used benchmark dataset for video generation, featuring 600 classes of approximately 400K action videos. In addition to its role as a standard benchmark, the task is history-conditioned video generation, making it ideal for evaluating our methods. Following prior works, we use a resolution of $128 \times 128$ pixels. Despite the large volume of videos and their low resolution, generating high-quality samples from the Kinetics-600 dataset is challenging even with large models due to the diversity and complexity of the content, and thus qualifies as our primary benchmark.


\textbf{RealEstate10K~\cite{zhou2018stereo}} is a dataset of home walkthrough videos, accompanied by camera pose annotations. While the dataset is predominantly used in novel view synthesis tasks, we utilize it for several reasons: 1) The camera poses allow for a more controlled evaluation of video models; for instance, we can easily switch between highly stochastic and deterministic tasks by altering the camera poses, 2) The dataset's nature enables the examination of the consistency of generated videos at a 3D level, and 3) The dataset's relatively smaller size compared to other text-conditioned video datasets makes it more computationally feasible to train our models, while still providing high-resolution videos. We use a resolution of $256 \times 256$ pixels.


\textbf{Minecraft~\cite{yan2023temporally}} is a dataset of Minecraft gameplay videos, where the player randomly navigates using 3 actions: forward, left, and right. The dataset consists of 200K videos, each with a length of 300 frames, each frame has a corresponding action label. The dataset is designed in a way that good FVD can only be achieved with a long context under action conditioned setting. Specifically, the dataset contains many trajectories where the player turns around and visits areas that it had visited before.  While the original dataset is $128 \times 128$ pixels, we train and evaluate on an upsampled version of $256 \times 256$ pixels, to generate higher-quality samples. 

\textbf{Fruit Swapping} is an imitation learning dataset associated with a fruit rearrangement task adopted from Diffusion Forcing~\cite{chen2024diffusion}. The task involves a tabletop setup where an apple and an orange are randomly put in two of the three empty clots. A single-arm robot is tasked with swapping the two fruits' slots using the third, empty slot as shown in Figure~\ref{fig:robot_generated}. The task requires long-horizon memory since one must remember the initial configuration of the slots to determine the final, target configuration. While the three slots provide a discrete state, each slot has a diameter of 15 centimeters and the fruit can be anywhere in the slot as soon as half of its column resides inside the slot. The task is made even harder when an adversarial human deliberately perturbs the fruit within its slot during the task execution - if there are $10$ possible locations within each slot, there would already be $10^3$ combinations of waypoints. This requires a robot policy to be reactive to the fruit locations rather than memorizing all possible combinations. The dataset contains $300$ expert demonstrations of the entire swapping task collected by a model-based planner, during which no disturbance happens. The robot may move an apple from slot 1 to the center of slot 2, move the orange from slot 3 to the center of slot 1, and then move the apple from slot 2 to slot 3. Notably, it had never seen a situation where the apple changed its location from center to edge during the middle of the manipulation due to adversarial humans. In addition, the dataset features $300$ additional demonstrations of re-grasping, which is a very short recovery behavior when it narrowly misses the fruit. In these re-grasping demonstrations, the robot arm only repositions to grab the missed object without moving it to another slot. Therefore, the dataset contains $300$ demonstrations that involve moving fruits but no regrasping, and $300$ demonstrations of regrasping but no moving fruit. The former has an average length of 540 frames and the later has an average length of around 50 frames.

\subsection{Implementation Details}

\input{tables/training_details}

We provide a summary of our implementation details in \cref{tab:training_details} and discuss them below.

\textbf{Pixel vs. Latent Diffusion.} In this work, we validate \mtd and \HG using both pixel and latent diffusion models. For Kinetics-600 and Minecraft, we train a latent diffusion model to enhance computational efficiency. Specifically, for Minecraft, we train an ImageVAE \cite{kingma2013auto} from scratch, which compresses $256 \times 256$ images into $32 \times 32$ latents, following the approach of Stable Diffusion \cite{rombach2022high}. For Kinetics-600, we train a chunk-wise VideoVAE that compresses $\{1, 4\} \times 128 \times 128$ video chunks into $16 \times 16$ latents, to more aggressively reduce computational costs. This approach resembles CausalVideoVAE, commonly used in prior works \cite{yu2023language,gupta2023photorealistic}, which compresses an entire $17 \times 128 \times 128$ video into $5 \times 16 \times 16$ latents via causal convolutions. However, we choose to compress every 4 frames separately to preserve \mtd's flexibility. Moreover, this ensures that consistency is influenced solely by the performance of the diffusion model, not the VAE. We implement the VideoVAE and training procedure following Open-Sora-Plan~\cite{lin2024open}. Lastly, for RealEstate10K, we train directly in pixel space, based on the observation that latent diffusion models struggle to correctly follow camera pose conditioning, leading to poor performance on this dataset. Architectures and training details differ significantly between pixel and latent diffusion models, as we discuss in the following sections.

\textbf{Architecture.} We employ the DiT~\cite{peebles2023scalable} and U-ViT~\cite{hoogeboom2023simple, hoogeboom2024simpler} backbones for the latent and pixel diffusion models, respectively. Both are transformer-based architectures; however, the key difference is that DiT's transformer blocks operate at a single resolution, whereas U-ViT incorporates multiple resolutions, with transformer blocks residing at each resolution. Due to this difference, we observe that the U-ViT backbone scales better in the pixel space. For improved scalability and temporal consistency, instead of using factorized attention \cite{ho2022video}, where attention is applied separately to spatial and temporal dimensions, we employ 3D attention that operates on all tokens simultaneously. In addition to this, we incorporate 3D RoPE~\cite{su2023roformer,gervet2023act3d} as relative positional encodings for the $T, H, W$ dimensions.

All conditioning inputs, including noise levels, actions, and camera poses, are injected into the model using an AdaLN layer, following \cite{peebles2023scalable}. For noise levels, since each frame retains independent noise levels in \mtd, an AdaLN layer is applied separately to each token, using the noise level of the corresponding frame. Minecraft actions are converted into one-hot vectors, which are then transformed into embeddings through an MLP layer and added to the noise level embeddings. For camera pose conditioning in RealEstate10K, we compute the relative camera pose with respect to the first frame. Following the methodologies of 3DiM \cite{watson2022novel} and 4DiM \cite{watson2024controlling}, this relative pose is then converted into ray origins and directions, which are then transformed into 180-dimensional positional embeddings, similar to Nerf~\cite{mildenhall2021nerf}. Across the resolutions of U-ViT, the camera pose embeddings are spatially downsampled to match the resolution before being injected into the model.

\textbf{Diffusion.} We use a cosine noise schedule~\cite{nichol2021improved} for all of our diffusion models. For the RealEstate10K and Minecraft models, we shift the noise schedule to be significantly noisier~\cite{hoogeboom2023simple} by a factor of 0.125, which we find markedly enhances sample quality, especially for RealEstate10K. This finding aligns with prior works~\cite{chen2023importance, hoogeboom2023simple} that highlight the importance of adding sufficient noise during training, especially when dealing with highly redundant images, such as those with high resolution. Another important design choice is the parameterization of diffusion models. We employ the $\rvv$-parameterization~\cite{vparameterization} for all models, which has been widely adopted in image and video diffusion models~\cite{ho2022imagen,lin2024common} due to its superior sample quality and quicker convergence, except for the robot model, where we use the $\rvx_0$-parameterization. Lastly, to expedite training, we use min-SNR loss reweighting~\cite{min_snr} for Kinetics and robot learning, and sigmoid loss reweighting~\cite{kingma2023understanding,hoogeboom2024simpler} for RealEstate10K and Minecraft.

\textbf{Training.} We train models \emph{for each dataset} and \emph{for each model class (e.g., \mtd, SD, etc.)}, using the same pipeline within each dataset. We apply a \emph{frame skip}, where training video clips are subsampled by a specific stride: a value of 1 for Kinetics-600, 2 for Minecraft, and 1 for Imitation Learning. For RealEstate10K, we use an increasing frame skip, starting from 10 and extending to the maximum frame skip possible within each video, to help the model learn various camera poses. Throughout all training, We employ the AdamW~\cite{loshchilov2017decoupled} optimizer, with linear warmup and a constant learning rate. Additionally, we utilize fp16 precision for computational efficiency and clip gradients to a maximum norm of 1.0 to stabilize training. For robot imitation learning, we follow the setup in Diffusion Forcing~\cite{chen2024diffusion} where we concatenate actions and the next observation together for diffusion, with the exception that we stack the next 15 actions together for every video frame.



\textbf{Sampling.} For all experiments, we use the deterministic DDIM~\cite{ddim} sampler with 50 steps. Sampling with history guidance, which requires multiple scores at every sampling step, is implemented by stacking the corresponding inputs across the batch dimension to compute the scores in parallel. These scores are then composed to obtain the final score for the DDIM update.


\textbf{Compute Resources.} We utilize 12 H100 GPUs for training all of our video diffusion models, with each model requiring approximately 5 days to train under our chosen batch size. One exception is the Robot model, which is trained on 4 RTX4090 GPUs for 4 hours. We note that most of the video models converge in validation metrics with a fraction of our reported total training steps. However, we chose to train them longer because the industry baselines on these datasets~\cite{yu2023magvit,ruhe2024rolling} are trained for a great number of epochs that are even unmatched by our final training steps. There was no noticeable overfitting throughout the process.

\subsection{Evaluation Metrics.}

\textbf{Fr\'echet Video Distance (FVD, \citet{unterthiner2018towards}).} We employ FVD as the primary evaluation metric for video generation performance. Similar to FID~\cite{heusel2017gans}, FVD computes the Fr\'echet distance between the feature distributions of generated and real videos, with features extracted from a pre-trained I3D network~\cite{carreira2017quo}. Lower FVD scores indicate better video generation performance. Unlike image-wise metrics such as FID, FVD evaluates entire video sequences, capturing temporal consistency and dynamics in addition to quality and diversity, making it the most suitable metric for our video generation tasks. Moreover, FVD is computed for the entire video, including both history and generated frames, to assess the consistency between them.

\textbf{VBench~\cite{huang2024vbench}.} We use VBench, an evaluation suite designed to assess video generation models in a comprehensive manner, when separate evaluation for different aspects of video generation is needed. Among 16 sub-metrics, we focus on 5 metrics to assess three aspects: 1) \emph{Frame-wise Quality}, calculated as the average of \emph{Aesthetic Quality} and \emph{Imaging Quality}, assesses the visual quality of individual frames; 2) \emph{(Temporal) Consistency}, derived as the average of \emph{Subject Consistency} and \emph{Background Consistency}, evaluates the short- and long-term consistency of generated videos; and 3) \emph{Dynamic Degree} assesses the degree of dynamics, i.e., the amount of motion in the generated videos. All metrics are better when higher, evaluate the generated videos independently without comparison to the ground truth, and are computed by averaging over all generated videos.

\textbf{Learned Perceptual Image Patch Similarity (LPIPS, \citet{zhang2018unreasonable}).} We use LPIPS as an alternative metric for highly deterministic tasks, where video-wise metrics may not be as sensitive and accurate. LPIPS computes the perceptual similarity between the generated and corresponding ground truth frames, with lower scores indicating higher similarity. We compute LPIPS only for the generated frames, excluding the history frames, to evaluate whether the generated frames are visually similar to the ground truth frames.

\subsection{Details on Video Generation Benchmark (\cref{sec:exp_ablation})}
\label{app:exp_details_benchmarks}

\textbf{Kinetics-600 Benchmark.} We closely follow the experimental setup of prior works~\cite{ho2022video, yu2023magvit, yu2023language, ruhe2024rolling}. On the test split of the dataset, we evaluate the models on a video prediction task, where the model is conditioned on the first 5 history frames and asked to predict the next 11 frames. Since our models, utilizing VideoVAE, generate 3 future tokens corresponding to 12 frames, we drop the last frame to align with the prediction task. We report the FVD score computed on 50K generated 16-frame videos, using three different random seeds.

\textbf{Resource Comparison Against Industry-Level Literature Baselines.} In \cref{tab:comparison_quantitative}, we show that \mtd not only outperforms generic diffusion baselines trained with the same pipeline but also holds its ground against strong literature baselines, including Video Diffusion~\cite{ho2022video}, MAGVIT~\cite{yu2023magvit}, MAGVIT-v2~\cite{yu2023language}, W.A.L.T~\cite{gupta2023photorealistic}, and Rolling Diffusion~\cite{ruhe2024rolling}. We have selected only the highest-performing baselines from the literature for comparison, omitting others for brevity.

A critical aspect of our evaluation is the comparison of computational resources. Our \mtd is trained with fewer resources compared to these industry-level baselines. Specifically, two primary factors affect the performance of diffusion models: network complexity and training batch size. Our \mtd model is a 673M parameter model with a DiT backbone, trained with a batch size of 196.

\emph{(i) Network Complexity.} As Video Diffusion and Rolling Diffusion have different backbones from ours, we compare the number of parameters; they are billion-parameter models, each with 1.1B and 1.2B, significantly larger than our model.  For MAGVIT, MAGVIT-v2, and W.A.L.T, which are pure transformer models with similar backbones, we use Gflops as a measure of computational complexity, as suggested by \cite{peebles2023scalable}. Our model is of DiT/XL size, whereas the baselines are DiT/L size, making them slightly smaller. In terms of Gflops, our model has $\approx 1.5$ times more Gflops compared to these baselines.

\emph{(ii) Batch Size.} Video Diffusion, MAGVIT, and MAGVIT-v2 are trained with a batch size of 256, while W.A.L.T and Rolling Diffusion are trained with a batch size of 512, which is significantly larger than ours.

When considering both network complexity and training batch size, MAGVIT and MAGVIT-v2 use comparable resources to our model, whereas Video Diffusion, W.A.L.T, and Rolling Diffusion require significantly more resources. Despite this resource disadvantage, \mtd proves to be highly competitive with these strong baselines. It is only slightly behind W.A.L.T, comparable to MAGVIT-v2, and outperforms the rest. This highlights the superior performance of \mtd as a base video diffusion model.


\subsection{Details on History Guidance Experiment (\cref{sec:exp_history_guidance})}
For the Kinetics-600 rollout experiment, the models generate the next 59 frames using sliding windows, given the first 5 history frames. The sliding windows are applied such that the model is always conditioned on the last 2 latent tokens and generates the next 3 latent tokens. As with the Kinetics-600 benchmark, we drop the last frame to align with the task. We assess the FVD and VBench scores on 1,024 generated 64-frame videos.

\textbf{\emph{History Guidance Scheme.}} To investigate the effect of \HGv and \HGf, we vary guidance scales using an equally spaced set of $\omega \in \{1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0\}$ for both methods. For \HGf, we use a fixed fractional masking degree of $k_\cH = 0.8$, which we find to generate videos with sufficient dynamics.

\subsection{Details on OOD History Experiment (\cref{sec:exp_temporal_guidance}, \textbf{Task 1})} In \textbf{Task 1} of \cref{sec:exp_temporal_guidance}, we have shown that video diffusion models easily fail to generalize when the conditioning history is OOD, and temporal history guidance resolves this challenge, through a systematic study on RealEstate10K. Below, we detail the experiment.

\textbf{What makes a history OOD?} As shown in the training data distribution of \cref{fig:ood_history}, we find that the rotation angle of the camera poses within a single training scene is typically small, rarely exceeding 100°. Hence, a history with a wider rotation angle, such as 150°, is considered OOD. Based on this observation, we assign the following tasks to the models: \emph{``Given a 4-frame history, with varying rotation angles, generate 4 frames that interpolates between these frames.''}

\textbf{Evaluation Based on Rotation Angles.} We categorize all scenes based on their rotation angles, into the bins of $[0^\circ, 10^\circ], [10^\circ, 20^\circ], \ldots, [170^\circ, 180^\circ]$. Based on the statistics of the training scenes, we conceptually classify the bins of $[0^\circ, 10^\circ], \ldots, [90^\circ, 100^\circ]$ as \emph{in-distribution}, $[100^\circ, 110^\circ], \ldots, [130^\circ, 140^\circ]$ as \emph{slightly OOD} ($< 500$ training scenes), and $[140^\circ, 150^\circ], \ldots$ as \emph{OOD} ($< 100$ training scenes). We then randomly select 32 test scenes (or less if the bin contains fewer scenes) from each bin. For each scene, we select 4 equally spaced frames from the beginning and end of it as the history, and designate the target frames as those in between. We evaluate by computing the LPIPS between the generated and target frames, and report the average LPIPS score for each bin, as shown in \cref{fig:ood_history}.

\textbf{\emph{History Guidance Scheme.}} From a full history $\cH = \{0, 1, 2, 3\}$, we compose scores conditioned on the following two history subsequences: $\cH_1 = \{0, 1, 2\}$ and $\cH_2 = \{1, 2, 3\}$, each with a guidance scale of $\omega_1 = \omega_2 = 2$. Additionally, we implement an extended version of temporal history guidance discussed in \cref{app:method_details_temporal}, by also composing generation subsequences: $\cG_1 = \{4, 5, 6\}$ and $\cG_2 = \{5, 6, 7\}$ chosen from the full generation $\cG = \{4, 5, 6, 7\}$. For the baseline using vanilla history guidance, we apply a guidance scale of $\omega = 2$ to the full history $\cH$.


\subsection{Details on Long Context Generation (\cref{sec:exp_temporal_guidance}, \textbf{Task 2}).}
We train a $50$-frame \mtd model that can condition on history up to a length of $25$ following the simplified objective Appendix ~\ref{app:method_details_objective_causal}. Note that this is equivalent to $100$ frames under the original video with a frameskip of $2$, or one-third of the maximum video length. We sample an initial context of $25$ from the dataset and use our trained model to auto-regressively diffuse the next $25$ frames conditioned on the previous $25$. We roll out $5$ times, or 125 frames in total, converging the maximum video length in the dataset. %

\textbf{\emph{History Guidance Scheme.}}
During sampling, we compose the scores from one long-context model and one short-context model, with context lengths of $25$ and $4$ respectively. Subtracting the unconditioned score doesn't play a significant role on this dataset so we proceed to compose the above two scores only, with a simple weighting of $50\%$ each. 

\subsection{Details on Robot Imitation Learning (\cref{sec:exp_temporal_guidance}, \textbf{Task 3}).} 

\textbf{Baselines.}
We compare against other diffusion-based imitation learning methods using our same architecture and implementation. First, we compare against a typical Markovian model, which diffuses the next few actions only based on current observation. Then, we use a variant of this Markovian model, which can see the previous two frames as a short history but still no long-term memory. Notice that these two short history lengths represent the current mainstream approaches ~\cite{chi2023diffusion}. In addition, we have a third baseline trained to condition on the entire history so far, representing a family of decision-making as sequence generation methods. For the convenience of notation, we will refer to these baselines as Markov model, 2-frame model, and full-history model. All baselines are trained to diffuse actions and next observations jointly.

\textbf{The Need to Compose Subtrajectories.}
As we mentioned in the dataset description, robot imitation learning is a sequence task that requires both long-term memory and local reactive behavior. While both are important to the final task's success, a short-context model will trivially fail most of the time since it won't remember which final state to proceed to. Therefore we focus on our experiment design on exploiting the failure mode of long-context models. One predominant failure mode is overfitting - since the imitation learning dataset is extremely small, a long-context model can attribute an action to any coincidental features. For example, all swapping trajectories in the dataset feature the behavior of putting the first fruit in the very center of the initially empty slot and coming back later to move it away from that center location. How should the model determine where it should pick up this fruit? There is little guarantee for it to determine correctly that it shall proceed to move its gripper right above that fruit versus just blindly going to the center. Whenever a human perturbs this fruit from the very center of the slot to the edge of the slot, an overfitted model will still move to the very center and proceed to grasp air, ignoring the actual location of that fruit. Therefore, theoretically, a full-history model would never be able to react to such perturbation, since it had never seen a trajectory with such perturbation and a successful trajectory would be out-of-distribution. Instead, it needs to mix in some behavior from a local reactive policy to perform the task, leveraging the fact that whenever a long history is out-of-distribution, you can always fall back to a shorter context model and imitate relevant sub-trajectories. Therefore, the only way to solve this task under the adversarial human is to stitch sub-trajectories together while keeping a long-term memory. 

\textbf{\emph{History Guidance Scheme.}}
To achieve the aforementioned stitched behavior, we compose three diffusion models with a context of $1$ frames, $4$ frames, and full history. We assign the full-history model with a small weight of $0.2$, the $1$ frame model, and the $4$ frame model with a weight of $0.45$ each. Like Minecraft, we didn't find subtracting unconditioned score super important in this task so we omitted it. The frames here refer to the bundle of the next $15$ actions and the single future video frame after that as we mentioned earlier in implementation details. 

\subsection{Details on Ultra Long Video Generation (\cref{sec:exp_long_navigation}).}
\label{appendix:long_rollout_details}
We provide additional details on generating long navigation videos on RealEstate10K, incorporating all advanced techniques associated with \mtd and history guidance. The generation of long navigation videos is divided into two phases: (i) a rollout phase, where the model generates a long video using a sliding window approach, and (ii) an interpolation phase, where the generated frames are further interpolated to create a smooth video. The process is detailed below.

\textbf{(i) Rollout Phase.} During the rollout phase, starting with a \emph{single image} randomly selected from the dataset, the model generates a long video using a sliding window, where it is conditioned on the last 4 frames to generate the next 4 frames. The first iteration is an exception, where the model is conditioned on the single image and generates the next 7 frames. Importantly, navigation cannot rely on the ground truth camera poses for two reasons: 1) videos in the dataset are relatively short (less than 300 frames), so we quickly exhaust available camera poses, and 2) the navigation task is highly stochastic, meaning the ground truth camera poses may not align with the generated frames (e.g., moving straight into a wall). To address this, we have developed a simple navigation UI, allowing a \emph{user to navigate freely in the scene by providing inputs} after each sliding window iteration. Specifically, the user can specify the horizontal and vertical angles, relative to the current frame, for the desired navigation direction, as well as the movement distance. This input is converted into a sequence of camera poses, which are then used as conditioning input for the model to sample the next set of frames. This process is repeated until the desired video length is achieved. 

\textbf{(ii) Interpolation Phase.} Next, in the interpolation phase, leveraging \mtd's flexibility which supports interpolation, we interpolate between the generated frames by a factor of 7. Specifically, using every pair of consecutive generated frames as history, we interpolate 6 frames between them. Camera poses for the interpolated frames, which should be given as input to the model, are computed by linearly interpolating the camera poses of the frames at both ends. More specifically, rotation matrices are interpolated using SLERP~\cite{shoemake1985animating}, and translation vectors are linearly interpolated.

\textbf{\emph{History Guidance Scheme.}} Finally, we discuss how history guidance is utilized throughout the navigation task. During the sliding window rollout, the default \HG scheme is \HGf, which we find to be extremely stable during long rollouts. Specifically, we apply \HGf with a guidance scale of $\omega = 4$ with a fractional masking degree of $k_\cH = 0.4$, chosen to ensure optimal stability. Additionally, we switch to \HGv with a guidance scale of $\omega = 4$ for more challenging situations, such as when the model needs to ``extrapolate'' to new areas. This is because \HGv performs better in such challenging scenarios, although it is less stable than \HGf, and thus is used sparingly. This switch is triggered when the model is asked to change the direction by more than 30°, or when the model is asked to move further than a certain distance. During the interpolation phase, we apply \HGv with a small guidance scale of $\omega = 1.5$, to ensure the interpolated video is smooth and consistent.

\textbf{Stabilization.} As an additional techinique, we also employ the stabilization technique proposed in Diffusion Forcing~\cite{chen2024diffusion}, where the previously generated frames are marked to be slightly noisy at a level of $k=0.02$, to prevent error accumulation, thereby further stabilizing the long rollout.



\section{Additional Experimental Results}
\label{app:exp_results}

In this section, we present additional experimental results to (i) answer potential questions that may provide further insights into our proposed \mtd and \HG, and (ii) further elaborate and provide additional samples for \cref{sec:experiments}.

\subsection{Additional Results on Fine-tuning to \mtd}
\label{app:exp_finetune}

\input{figures/fine_tune/training_curve}

Below we provide detailed results on fine-tuning a pre-trained full-sequence (FS) model to \mtd, both from training and sampling perspectives.

\textbf{Training Dynamics.} We show the training loss curves of for two variants of \mtd, one trained from \emph{scratch} for 640k iterations, and the other \emph{fine-tuned} from the pre-trained FS model for 80k iterations, in \cref{fig:training_curve}. We observe that the pre-trained model already provides a good initialization for \mtd, as the model starts with a low training loss and converges rapidly in the early iterations, in \cref{fig:training_curve_full}. Surprisingly, the fine-tuned model achieves a lower training loss than the model trained from scratch after only 80k iterations, as shown in \cref{fig:training_curve_zoomed}. Moreover, after 40k iterations, the fine-tuned model exhibits a training loss comparable to the model trained from scratch for 405k iterations, which is $\sim$10x speedup. This highlights the superior efficiency and ease of training \mtd by fine-tuning from a pre-trained model. While this opens up the possibility of fine-tuning large foundational video diffusion models to \mtd with small computational cost, we leave this as future work.

\textbf{FVD Metric Evolution.} In contrast to the training loss, \cref{fig:training_curve_zoomed} (or \cref{tab:comparison_quantitative}) shows that the fine-tuned model achieves a slightly higher FVD score than the model trained from scratch, although being highly competitive even after 40k iterations. We attribute this discrepancy to the use of EMA, which is commonly employed in diffusion models to enhance sample quality~\cite{ddpm,dhariwal2021diffusion}. By default, we use an EMA decay of 0.9999, and thus the model weights used for sampling are affected by the last tens of thousands of training iterations. Therefore, the fine-tuned model's superior training loss does not immediately translate to a lower FVD score, but we expect it to outperform the model trained from scratch after an additional short training period. While one may consider simply fine-tuning the model without EMA to speed up, EMA is crucial for sample quality; for example, at 80k iterations, FVD without EMA is 7.3, significantly higher than the 4.7 with EMA. This suggests that choosing a smaller EMA decay that still guarantees sample quality, through sophisticated strategies such as post hoc EMA tuning~\cite{karras2024analyzing}, may be a promising direction for future work.


\subsection{Ablation Study on Binary-Dropout Diffusion with Vanilla History Guidance}

\input{figures/binary_guidance/quantitative_qualitative}

While we have shown that Binary-Dropout Diffusion (BD) performs poorly as a base model (\textbf{Q2} of \cref{sec:exp_ablation}), BD still can implement vanilla history guidance due to its binary dropout training. As such, a natural question is: \emph{How does BD perform with \HGv, compared to \mtd?} To answer this question, we repeat the Kinetics-600 rollout experiment in \cref{sec:exp_history_guidance} using BD with \HGv, comparing against \mtd with \HG. See \cref{fig:binary_guidance} for the results. We observe that \mtd consistently outperforms BD across all guidance scales except for $\omega = 2.5$, as shown in \cref{fig:binary_guidance_quantitative}. Under their optimal guidance scales of $\omega = 1.5$, \mtd achieves a lower FVD score of 181.6 compared to BD's 196.0, and qualitatively, generates more consistent, high-quality samples, as shown in \cref{fig:binary_guidance_qualitative}. When using \HGf, which is only applicable to \mtd, \mtd further outperforms BD, achieving an FVD score of 170.4. These results highlight that \mtd is a better base model for implementing history guidance, both in performance and in a variety of guidance methods that can be applied.

\subsection{Detailed Results on Long Context Generation (\cref{sec:exp_temporal_guidance}, \textbf{Task 2})}
\label{app:exp_results_minecraft}
We calculate the FVD on $1024$ samples across all $125$ generated frames. A simple conditional diffusion model with context full context achieves an FVD of 97.625 while our temporal guidance achieves an FVD of 79.19 (lower is better). We note that while traditionally FVD is a bad metric for videos with high intrinsic variance, it's well-suited for our benchmark since both action-conditioning and the dataset design constrain the possible variance. We visually observe that \method's prediction aligns well with the ground truth semantically over the majority of the frames in a video, showing the variance is well-warranted. We visualize one randomly picked sample in Figure~\ref{fig:minecraft_vis}, showing that temporal guidance can maintain high-quality details far into the future even without CFG. In the meanwhile, the long-context model without temporal guidance can suffer from the high dimensional context, which makes it much more likely to see out-of-distribution frames in its history.

\subsection{Detailed Results on Long-horizon yet Reactive Imitation Learning (\cref{sec:exp_temporal_guidance}, \textbf{Task 3})}
\label{app:exp_results_robot}
\input{figures/imitation_learning/robot_generated}

We examine the success rate of robot imitation learning quantitatively by randomizing the environment 100 times before testing the temporal guidance model as well as its baselines. We found that the Markov baseline fails to perform the task completely as expected since it has trouble sticking to a specific plan - it would move away from fruit and then move back halfway since it has no memory. The $4$-frame model suffers from the same issue and cannot finish the task. It does react well to perturbations on the object and picks up the fruit from time to time, showing short context indeed prevents overfitting from temporal locality. We found that the full-history model, with the maximum possible memory, performs well whenever there is no human perturbation. However, as soon as the adversarial human perturbs the fruit during the task execution, this policy often blindly goes to the very center of the third slot while the object is already moved to the edge of the slot. The policy will then proceed to close its gripper, holding nothing, and then move to the next slot, thinking it has something in its hand. There are occasional cases when this doesn't happen and the model actually reacts to the adversarial perturbation, although infrequently and only happens to the case then perturbation from the slot center isn't too big. Overall this shows that using a full-context model naively can make the model suffer from overfitting and one may want to manually emphasize the temporal locality prior. Finally, we tested \mtd composed guidance and found it to achieve a much higher success rate of $83\%$, showing that it's actually stitching the subtrajectories to make decisions, or at least simultaneously borrowing the memory from the full-context model while staying locally reactive using the short-context model. In addition, we attempted a few stronger perturbations such that the adversarial human will deliberately knock off the fruit from the robot's gripper when it's closing. We found that temporal guidance can even react to this by regrasping and eventually finishing the whole swapping task. However, even temporal guidance achieves only $28\%$ to this strong perturbation since it's way too out-of-distribution and may require more data. Qualitatively, we visualize a generated robot trajectory with an unseen configuration in Figure~\ref{fig:robot_generated}. 

\subsection{Additional Qualitative Results}
\label{app:exp_results_additional}

We present additional qualitative results to supplement our main findings in \cref{sec:experiments}. Please refer to \cref{fig:comparison_qualitative_additional,fig:flexibility,fig:vanilla_re10k,fig:ood_history_qualitative_full,fig:navigation_comparison,fig:navigation} for detailed visual comparisons, which are discussed below.

\textbf{\mtd vs. Baselines (\cref{sec:exp_ablation}, Q1).} We present additional qualitative comparisons of \mtd against baselines in \cref{fig:comparison_qualitative_additional}, as an extension to the qualitative results shown in \cref{fig:comparison_qualitative}. Consistent with the quantitative findings in \cref{tab:comparison_quantitative}, \mtd produces more consistent and higher-quality samples compared to all baselines.

\textbf{Empirical Flexibility of \mtd (\cref{sec:exp_ablation}, Q3).}
As evidence of the empirical flexibility of \mtd, we present additional qualitative results on RealEstate10K in \cref{fig:flexibility}. Our \mtd model successfully generates consistent samples, given histories that vary both in length and timestamps. This highlights the effectiveness of our new training objective, which transforms \mtd into a flexible multi-task model, uniformly achieving high performance across diverse tasks.

\textbf{Improving Video Generation via History Guidance (\cref{sec:exp_history_guidance}).} In addition to the results shown in \cref{fig:vanilla_guidance} for Kinetics-600, we present further qualitative results on RealEstate10K in \cref{fig:vanilla_re10k}, highlighting the effectiveness of vanilla history guidance in improving video generation. With increasing guidance scales, the generated samples exhibit significantly higher frame quality and consistency, likewise to the results on Kinetics-600. This behavior is consistent across different tasks—extrapolation and showcasing the broad applicability of history guidance in any history-conditioned video generation task.


\textbf{Robustness to Out-of-Distribution (OOD) History (\cref{sec:exp_temporal_guidance}, Task 1).} We provide additional qualitative results for \textbf{Task 1} from \cref{sec:exp_temporal_guidance}, as illustrated in \cref{fig:ood_history_qualitative_full}. These results demonstrate that \HGt enables \mtd to \emph{uniquely} remain robust to OOD history. Failure cases clearly observed in baselines show that typically, video diffusion models only perform well when the history is in-distribution. By composing in-distribution short history windows, \HGt can effectively approximate strictly OOD histories that were unseen during training.

\subsection{Detailed results on Ultra Long Video Generation (\cref{sec:exp_long_navigation}).}
\label{app:exp_results_navigation}

We present extended results from \cref{sec:exp_long_navigation} below.

\textbf{DFoT vs. SD on Long Rollout.} To begin with, we highlight the significant challenges of generating long navigation videos using the RealEstate10K dataset. Specifically, we investigate the performance of SD, the most conventional and competitive baseline. To mitigate the stochastic nature of navigation that complicates comparisons, we evaluate \mtd with \HG and SD on a simple navigation task of moving straight, which is almost deterministic. We avoid using interpolation—applicable only to \mtd—to ensure a fair comparison. The results, shown in \cref{fig:navigation_comparison}, indicate that SD struggles to maintain consistency with the history frame, failing around frame $\sim$30. We attribute this to SD's inferior quality and consistency, along with its inability to recover from small errors during generation. In contrast, \mtd with \HG succeeds to stably roll out beyond frame 72. Alongside the qualitative comparison, we note that 4DiM~\cite{watson2024controlling}, an SD model that, to our knowledge, produces the longest and highest-quality videos on RealEstate10K among the methods in the literature, generates videos with a maximum length of 32 frames, which is significantly shorter than our long navigation videos.

\textbf{More Samples.} We present four samples of long navigation videos generated by \mtd with \HG in \cref{fig:navigation_1,fig:navigation_2,fig:navigation_3,fig:navigation_4}. These samples demonstrate the capability of \mtd with \HG to stably generate extremely long videos. The generated videos are notably longer than those in the training dataset, which primarily cover a single room or small area, rather than multiple connected rooms or areas.
