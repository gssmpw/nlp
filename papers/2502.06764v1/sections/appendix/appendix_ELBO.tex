
\section{Proofs, Explanations, and Extensions}
\subsection{Derivation of an ELBO}
\label{appendix:theory_elbo}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\floor}[1]{\lfloor#1\rfloor}

\newcommand{\veck}{\mathbf{k}}
\newcommand{\bbK}{\mathbb{K}}
\newcommand{\ptheta}{p_{\bm{\theta}}}
\newcommand{\bk}{\mathbf{k}}

This section includes a derivation of an ELBO corresponding to the \mtd{} training objective. By taking a sequence modeling perspective, the derivation below streamlines that of the Diffusion Forcing ELBO in \cite{chen2024diffusion}.

Let $\cT$ denote the index set associated with a  sequence $\bx$, so that $\bx_{\cT} = (\bx_t)_{t \in \cT}$ is the whole sequence. We use the notation $\bk =(k_t)_{t \in \cT}$ for the sequence of noise levels. A \emph{path} $\rho$ is a sequence of noising steps that transition from an unnoised sequence to a noised one. Specifically,
\begin{definition}[Path] We define a \textbf{path} $\rho$ as a sequence $(\bk^j)_{0   \le j \le N}$ that begins at zero noise $\bk^0 = (0,0,\dots,0)$, and terminates at full noise $\bk^N = (K, K,\dots, K)$.
\end{definition}
Given a path $\rho$, we let $\bx^{\rho} = \bx^{\bk^{0:N}}$ denote the sequence with $(\bx_t^{\bk_t})_{t \in \cT}$. Note that there is nothing intrinsically causal or temporal about the indices $t$; indeed, we can define noising paths on other objects like trees or graphs. Examples of paths include:
\begin{itemize}
    \item Autoregressive diffusion, where $k^{j}_t$ is equal to $K$ if $t \le \floor{j/K}$, equal to $0$ if $t > \floor{j/K} + 1$, and equal to $j - K\floor{j/K}$ otherwise. This path looks like $(0,\dots,0)$, $(1,0,\dots,0)$,$\dots$, $(K,0,\dots,0)$, $(K,1,0,\dots,0)$, increasing lexicographically.
    \item Full-sequence diffusion, where $k^j_t = j$ and $N = K$; i.e. all points are denoised together. 
    \item We can accomodate skips in noiseless, e.g. DDIM, or paths with linearly increasing noise, such as those considered in \cite{chen2024diffusion}. 
\end{itemize}
Typically, we assume that $k^j_t$ is non-decreasing in $j$ (the noise level is monotonic up to $\bk^N = (K,\dots,K)$).

The essential property that we require is that our learned model and forward process factor nicely along such paths. It is straightforward to check that this is indeed the case for \method with these monotonic paths:


\begin{definition}[Factoring Property]
\label{defn:factoring} We say that a model $\ptheta$ and forward process $q$ factor along a path $\rho$ if for any path, $\rho = (\bk^1,\dots,\bk^N)$ be a path,  $q(\bx^{\bk^{1:N}}
 \mid \bx^{\bk^0})$ factors as $q(\bx^{\bk^{1:N}}
 \mid \bx^{\bk^0}) = \prod_{j=1}^n q(\bx^{\bk^j}\mid \bx^{\bk^{j-1}})$, and  $\ptheta $ factors as $\ptheta(\bx^{\bk^{0:N}}) = \prod_{j=1}^N \ptheta(\bx^{\bk^j}\mid \bx^{\bk^{j-1}})\ptheta(\bx^{\bk^N})$, with $\ptheta(\bx^{\bk^N})$ not depending on $\theta$. 
\end{definition}
When the model factors along paths, a general ELBO holds. We first state the general form, then specialize to Diffusion via Gaussian forward processes, and conclude with the proof of the general result.
\begin{theorem}\label{thm:ELBO} Suppose that $(\ptheta,q)$ factor along a path $\rho = (\bk^1,\dots,\bk^N)$. Then, for some constant $C$ not depending on $\theta$, we have
 \begin{align}
     \ln p(\bx^{\bk^0}) \ge C + \Exp_{\bx^{\bk^{1:N}} \sim q(\bx^{\bk^{1:N}}\mid \bx^{\bk_0})}\left[\ln {\ptheta(\bx^{\bk^0} \mid \bx^{\bk^1})} + \sum_{j=1}^{N-1} \Dkl({\ptheta(\bx^{\bk^{j}} \mid \bx^{\bk^{j+1}})}\parallel {q(\bx^{\bk^{j}} \mid \bx^{\bk^{j+1}},\bx^{\bk^0})})\right].
 \end{align}
 Consequently, if $\Exp_{\bk^{1:N} \sim \cD_{p}}$ denotes an expectation over paths $\rho = (\bk^1,\dots,\bk^K)$ along which $(\ptheta,q)$ factor, then 
    \begin{align*}
     \ln p(\bx^{\bk^0}) \ge C + \Exp_{\bk^{1:N} \sim \cD_{p}}\Exp_{\bx^{\bk^{1:N}} \sim q(\bx^{\bk^{1:N}}\mid \bx^{\bk_0})}\left[\ln {\ptheta(\bx^{\bk^0} \mid \bx^{\bk^1})} + \sum_{j=1}^{N-1} \Dkl({\ptheta(\bx^{\bk^{j}} \mid \bx^{\bk^{j+1}})}\parallel {q(\bx^{\bk^{j}} \mid \bx^{\bk^{j+1}},\bx^{\bk^0})})\right].
 \end{align*}
\end{theorem}


We now specialize \Cref{thm:ELBO} to Gaussian diffusion. For now, we focus on the ``$\bx$-prediction'' formulation of diffusion. The ``$\beps$-prediction'', used throughout the main body of the text and the ``$\rvv$-prediction formalism, which is the one used in our implementation, can be derived similarly (see Section 2 of \cite{chan2024tutorial} for a clean exposition). The following theorem is derived directly by applying standard likelihood and KL-divergence computations for the DDPM \cite{ho2020denoising,chan2024tutorial} to \Cref{thm:ELBO}.  
\newcommand{\xthet}{\hat{\bx}_{\bm\theta}}
\newcommand{\mutheta}{\mu_{\bm{\theta}}}


For simplicity, we focus on paths with a single increment (e.g. DDPM), but extending to jumps (e.g. DDIM) is straightforward (albeit more notationally burdensome). 
\begin{corollary}\label{cor:elbo} Consider only paths $\rho$ for which $\bk^j \ge \bk^{j-1}$ entrywise, and for any $t$ and $j$ for which $ k^j_t > k^{j-1}_t$, $k^j_t = k^{j-1}_t + 1$ increments by one. 
\begin{align}
q(\bx^{\bk^{j+1}} \mid \bx_t^{\bk^j}) = \prod_{t:k^j_t < k^{j+1}_{t}}\mathcal{N}(\bx_t^{k^j_t}; \sqrt{1-\beta_{k^j_t}}\bx^{k^{j-1}_t}, \beta_{k^j_t} \mathbf{I}), \label{eq:Q_form}
\end{align}
and define $\alpha_k = (1-\beta_k)$, $\bar{\alpha}_k = \prod_{j=1}^k \alpha_j$.  Suppose that we parameterize $\ptheta(\bx^{\bk^j}; \bx^{\bk^{j+1}},\bk^j) = \cN(\mutheta(\bx^{\bk^j}; \bx^{\bk^{j+1}},\bk^j),\sigma_j^2)$, where further, 
\begin{align*}
\mutheta(\bx^{\bk^j}; \bx^{\bk^{j+1}},\bk^j) = \frac{(1 - \bar{\alpha}_{j-1})\sqrt{{\alpha}_j}}{1-\bar{\alpha}_j} \bx^{\bk_j} +  \frac{(1 - {\alpha}_{j})\sqrt{\bar{\alpha}_{j-1}}}{1-\bar{\alpha}_j}\xthet(\bx^{\bk^j}; \bx^{\bk^{j+1}},\bk^j), \quad \sigma_j^2 := \frac{(1 - \alpha_{j})(1-\sqrt{\bar{\alpha}_{j-1}})}{1-\bar{\alpha}_j}.
\end{align*}
Further, let $\hat{\bx}^0_{\bm{\theta}}(\bx_t^{k^j}; \bx^{\bk^{j+1}},\bk^j) = \hat{\bx}^0_{\bm{\theta}}(\bx^{\bk^j}; \bx^{\bk^{j+1}},\bk^j)_t$ denote the $t$-block component of $\hat{\bx}^0_{\bm{\theta}}(\bx^{\bk^j}; \bx^{\bk^{j+1}},\bk^j)$, and suppose that if $k_t^j = k_t^{j+1}$, then $\hat{\bx}^0_{\bm{\theta}}(\bx_t^{k^j}; \bx^{\bk^{j+1}},\bk^j) = \bx^{\bk^{j+1}}$ (i.e., if no denoising occurs, we do not re-predict the denoising). Then, for some distribution $\cD_{\rho}$ over paths $\rho$ along which $(\ptheta,q)$ satisfy the requisite factoring property, and for some constant $C$ independent of $\ptheta$,  
\begin{align*}
\ln  \ptheta((\bx^{\bk^0})]  &\ge C+ \Exp_{\rho = \bk^{0:N} \sim \cD_{\rho}}\Epz \left[\sum_{j=1}^{N}\sum_{t\in \cT: k_t^j < k_t^{j+1}} c_{k_t^j}\| \hat{\bx}^0_{\bm{\theta}}(\bx_t^{k^j}; \bx^{\bk^{j+1}},\bk^j) - \bx_t^{k^0_t}\|^2 \right],
\end{align*}
where above, we define $c_i = \frac{(1 - \alpha_{j})^2\bar{\alpha}_{i-1}}{2\sigma^2(1 - \bar\alpha_{i})^2}$. 
\end{corollary}
\begin{proof}[Proof of \Cref{cor:elbo}] The first inequality follows from the standard computations for the ``$\bx$-prediction'' formulation of Diffusion (see Section 2.7 of  \cite{chan2024tutorial} and references therein). 
\end{proof}
\begin{remark}[Factoring]
Observe that forward  process in \Cref{eq:Q_form} naturally factorizes across all the paths $\rho$ considered in \Cref{cor:elbo}. While $\ptheta$ (by definition) factors across any single path $\rho$, these factorizations may be inconsistent across paths. Enforcing some explicit consistency remains open for future work.
\end{remark}


\begin{proof}[Proof of \Cref{thm:ELBO}] The first step is the standard ELBO trick:
\begin{align*}
\ln p(\bx^{\bk^0}) &= \ln \int_{\bx^{\bk^{1:N}}}p(\bx^{\bk^{0:N}})\rmd \bx^{\bk^{1:N}}\\
&= \ln \Exp_{\bx^{\bk^{1:N}} \sim q(\bx^{\bk^{1:N}} \mid \bx^{\bk^0})}\frac{p(\bx^{\bk^{0:N}})}{q(\bx^{\bk^{1:N}} \mid \bx^{\bk^0})}\\
&\ge \Exp_{\bx^{\bk^{1:N}} \sim q(\bx^{\bk^{1:N}}   \mid \bx^{\bk^0})} \ln \frac{p(\bx^{\bk^{0:N}})}{q(\bx^{\bk^{1:N}} \mid \bx^{\bk^0})}.
\end{align*}
where the last step follows from Jensen's inequality.


We now expand
\begin{align*}
&\ln \frac{ \ptheta(\bx^{\bk^{0:N}} )}{q(\bx^{\bk^{1:N}} \mid \bx^{\bk^0})} \\
&= \ln  \ptheta (\bx^{\bk^N}) + \ln \frac{\ptheta(\bx^{\bk^0} \mid \bx^{\bk^1})}{q(\bx^{\bk^1} \mid \bx^{\bk^0})} + \sum_{j=1}^{N-1}\ln \frac{\ptheta(\bx^{\bk^{j}} \mid \bx^{\bk^{j+1}})}{q(\bx^{\bk^{j+1}} \mid \bx^{\bk^{j}},\bx^{\bk^0})} \tag{Factoring, \Cref{defn:factoring}}\\
&= \ln  \ptheta (\bx^{\bk^N}) + \ln \frac{\ptheta(\bx^{\bk^0} \mid \bx^{\bk^1})}{q(\bx^{\bk^1} \mid \bx^{\bk^0})} + \sum_{j=1}^{N-1}\ln \frac{\ptheta(\bx^{\bk^{j}} \mid \bx^{\bk^{j+1}})}{q(\bx^{\bk^{j}} \mid \bx^{\bk^{j+1}},\bx^{\bk^0})} + \ln \frac{q(\bx^{\bk^{j}} \mid \bx^{\bk^0})}{q(\bx^{\bk^{j+1}} \mid \bx^{\bk^0})} \tag{Bayes' Rule on $q$}\\
&= \ln  \ptheta (\bx^{\bk^N}) + \ln \frac{\ptheta(\bx^{\bk^0} \mid \bx^{\bk^1})}{q(\bx^{\bk^1} \mid \bx^{\bk^0})} +  \ln \frac{q(\bx^{\bk^{1}} \mid \bx^{\bk^0})}{q(\bx^{\bk^{N}} \mid \bx^{\bk^0})} +  \sum_{j=1}^{N-1}\ln \frac{\ptheta(\bx^{\bk^{j}} \mid \bx^{\bk^{j+1}})}{q(\bx^{\bk^{j}} \mid \bx^{\bk^{j+1}},\bx^{\bk^0})} \tag{Telescoping}\\
&=\ln \frac{ \ptheta (\bx^{\bk^N})}{q(\bx^{\bk^{N}} \mid \bx^{\bk^0})} + \ln \ptheta(\bx^{\bk^0} \mid \bx^{\bk^1}) +  \sum_{j=1}^{N-1}\ln \frac{\ptheta(\bx^{\bk^{j}} \mid \bx^{\bk^{j+1}})}{q(\bx^{\bk^{j}} \mid \bx^{\bk^{j+1}},\bx^{\bk^0})} \tag{Canceling}.
\end{align*}
We observe that $\ln  \ptheta (\bx^{\bk^N})$ and $  \ln \frac{1}{q(\bx^{\bk^{N}} \mid \bx^{\bk^0})}$ do not depend on $\bm{\theta}$ (recall $\ptheta (\bx^{\bk^N})$ is the distribution over noise), so taking an expectation over the $q(\cdot)$, we can regard these as a constant $C$. This yields
\begin{align*}
\ln p(\bx^{\bk^0}) &\ge C + \Exp_{\bx^{\bk^{1:N}} \sim q(\bx^{\bk^{1:N}}   \mid \bx^{\bk^0})}\left[  \ln {\ptheta(\bx^{\bk^0} \mid \bx^{\bk^1})}  +  \sum_{j=1}^{N-1} \ln \frac{\ptheta(\bx^{\bk^{j}} \mid \bx^{\bk^{j+1}})}{q(\bx^{\bk^{j}} \mid \bx^{\bk^{j+1}},\bx^{\bk^0})} \right]\\
&= C + \Exp_{\bx^{\bk^{1:N}} \sim q(\bx^{\bk^{1:N}}\mid \bx^{\bk_0})}\left[\ln {\ptheta(\bx^{\bk^0} \mid \bx^{\bk^1})} + \sum_{j=1}^{N-1} \Dkl({\ptheta(\bx^{\bk^{j}} \mid \bx^{\bk^{j+1}})}\parallel {q(\bx^{\bk^{j}} \mid \bx^{\bk^{j+1}},\bx^{\bk^0})})\right].
\end{align*}

\end{proof}








