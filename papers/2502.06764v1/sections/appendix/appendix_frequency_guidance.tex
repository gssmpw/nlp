\subsection{Understanding Frequency Guidance}
\label{app:frequency_guidance}

\newcommand{\bSigma}{\bm{\Sigma}}
\newcommand{\eye}{\mathbf{I}}
\newcommand{\bhatx}{\hat{\bx}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bS}{\mathbf{S}}

\newcommand{\Four}[1][d]{\mathcal{F}_{#1}}
\newcommand{\R}{\mathbb{R}}
For simplicity, we focus on $1$-dimensional discrete signals with even dimension $d$, but extending to $2$-dimensions is straightforward. We provide a simple mathematical explanation that ``noising'' a feature corresponds to a form of low-pass filtering.

Specifically, we consider a regression setting with features $\bx \in \R^d$ and targets $\by \in \R^m$. We now study the conditional distribution of $\by \mid \bx_{\sigma}$, where $\bx_{\sigma} = \bx + \sigma \bz$ is a noisy measurement of $\bx$. To understand effects in the frequency domain, we study the conditional distribution of the Fourier transform of $\by$  given a measurement of $\bx_{\sigma}$. We assume that the entries of $\bx$ can be interpreted as entries in a sequence and we interpret this conditional distribution as a function of the Fourier transformation, $\Four(\bx)$, of $\bx$. Similarly, we define $\Four[m](\by)$. For simplicity, we focus on a $1$-d Fourier transform, but analogous statements hold for $2$-d features $\bx$ (e.g. $2$-d frames in a video).


We begin by recalling the Fourier transform of a vector. %
\begin{definition} Let $\Four: \R^d \to \R^d$ denote the (real) discrete Fourier transform, specified by 
\begin{align}
    \Four(\bx)(k) = 
\begin{cases}\sum_{i=1}^d \bx[i]\sin(ik/2\pi) & 1 \le k \le d/2\\
    \sum_{i=1}^d \bx[i]\cos(ik/2\pi) & d/2 < k \le 1\\
    \end{cases}
\end{align}
We note that, by Parseval's theorem, $\Four$ is an isometry:
\begin{align}
    \frac{1}{d}\|\Four(\bx)\|_{\ell_2}^2 = \|\bx\|_{\ell_2}^2
\end{align}
Because $\Four$ is a bijective linear mapping, we identify it with an invertible matrix in $\R^{d \times d}$. 
\end{definition}
\newcommand{\Nfreq}{\cN_{\mathrm{freq}}}

We now characterize the conditional of $\Four[m](\by) \mid \Four(\bx)$.

\begin{proposition}\label{lem:fourier_eiv} Let $\bx \sim \cN(0,\bSigma_x^2)$,  and $\by \mid \bx \sim \cN(\bA \bx, \bSigma_y^2)$. Define $\bx_{\sigma} = \bx + \sigma \bz$, where $\bz \sim \cN(0,\eye)$ is independent of $\bx,\by$.   Define $\hat \bA := \Four[m] \bA \Four^{-1}$, $\hat \bSigma_x := \Four \bSigma_x \Four^\top$ and $\hat \bS(\sigma) := \hat \bSigma_x ( \hat \bSigma_x + d\sigma^2  \eye)^{-1}$, and $\hat \bSigma_y := \Four[m] \bSigma_y \Four[m]^\top$. 
Then, 
\begin{itemize}
    \item $\Four(\bx) \sim \cN(0, \hat \bSigma_x)$
    \item $\Four[m](\by) \mid \Four(\bx) \sim \cN(\hat \bA \Four(\bx), \hat \bSigma_y)$
    \item The distribution of $\Four[m](\by) \mid \bx_{\sigma} $ (or $\Four[m](\by) \mid \Four(\bx_{\sigma}) $)  is
    \begin{align}
\cN( \hat \bA \hat \bS(\sigma)  \Four(\bx_{\sigma}), \hat \bSigma_y + d \sigma^2 \hat \bA \hat \bS(\sigma)\hat\bA^\top ) \label{eq:conditional_fourier}
\end{align}
\end{itemize}

    
\end{proposition}
\begin{proof} Set $\hat \bx = \Four(\bx)$ and $\hat \by = \Four(\by)$. As $\Four,\Four[m]$ are linear, we see that $\hat \bx \sim \cN(0,\hat \Sigma_x^2)$ and $\hat \by \sim \cN(\Four[m]\bA(\bx), \Four[m]\bSigma_y \Four[m]^\top) = \cN(\hat \bA \Four(\bx), \hat \bSigma_y)$. 

For the last statement, we have that $\Four(\bx_\sigma) = \hat \bx + \sigma \Four(\bz)$. As $\frac{1}{\sqrt{d}}\Four$ is an isometry (i.e orthogonal), we have $\frac{1}{d}\Exp[\Four(\bz)\Four(\bz)^\top] = \eye$. Thus, $\sigma \Four(\bz) = \sigma \sqrt{d} \hat \bz$, where $\hat \bz \sim \cN(0,\eye_d)$ is independent of $\hat \bx, \hat \by$. We may now invoke \Cref{eq:lem_gaussian_eiv} to show that  \Cref{eq:conditional_fourier} describes the distribution of $\Four[m](\by) \mid \Four(\bx_{\sigma})$. As $\Four$ is a bijection, conditioning on $\Four(\bx_{\sigma})$ and $\bx_{\sigma}$ is equivalent.
\end{proof}

\paragraph{Interpretation in Terms for Frequency Attenuation:} It is common that natural signals exhibit power-law decay in the frequency domain. As an illustration, consider  $\hat \bSigma_x = C \mathrm{Diag}(\{i^{-\alpha})\}_{1 \le i \le d})$; that is, in the Fourier domain, $\bx$ is independent across frequencies and exhibits a power-law decay with exponent $\alpha$. Then, $\hat \bS(\sigma)$ is diagonal, and 
\begin{align*}
\hat \bS(\sigma)_{ii} = \frac{1}{1 + d\sigma^2 i^{\alpha}/C} \sim \begin{cases} 1 & i \le (\frac{C}{d\sigma^2})^{1/\alpha}  \text{ or } \sigma^2 \le Ci^{\alpha}/d\\
 i^{-\alpha}  & i \ge (\frac{C}{d\sigma^2})^{1/\alpha} \text{ or } \sigma^2 \ge Ci^{\alpha}/d\\
\end{cases}
\end{align*}
also exhibits power law decay. Hence, when conditioning on $\bx_{\sigma}$, the shrinkage operator $\hat \bSigma(\sigma)$ attenuates the contribution of the $i$-th frequency of $\bx_{\sigma}$ in proportion to $i^{-\alpha}$ for $i$-large. Moreover, as $\sigma$ becomes larger, more frequencies are attenuated. In other words, conditioning on noisier examples leads to more aggressive attenuation.

Importantly, \textbf{there is no intrinsic bias of Gaussian noising towards preferring lower frequencies. Rather, noising serves to regularize away weaker frequencies. For natural images, this corresponds to high frequencies, but may not in other application domains.}


\begin{lemma}[Gaussian Conditional Computation]\label{eq:lem_gaussian_eiv} Let $\bx \sim \cN(0,\bSigma_x^2)$,  and $\by \mid \bx\sim \cN(\bA \bx, \bSigma_y^2)$. Define $\bx_{\sigma} = \bx + \sigma \bz$, where $\bz \sim \cN(0,\eye)$ is independent of $\bx,\by$.  Set $\bS(\sigma) := \bSigma_x(\bSigma_x + \sigma^2 \eye)^{-1}$.  Then, the distribution of $\by \mid \bx_{\sigma} $ is         $\cN(\bA \bS(\sigma) \bx_{\sigma}, \bSigma_y + \sigma^2 \bA \bS(\sigma)\bA^\top )$.
\end{lemma}
\begin{proof} First, we observe that $(\bx_{\sigma},\by)$ are jointly Gaussian random variables with mean zero. We set $\bSigma_{22} = \Exp[\bx_{\sigma}^2] = \sigma^2 \eye + \bSigma_x$, and $\bSigma_{11} = \Exp[\by^2] = \bSigma_y + \bA \bSigma_x \bA^\top$. Moreover, $\bSigma_{12} := \Exp[\by \bx_{\sigma}^\top] = \bA \bSigma_x$. Hence, from the standard formula for Gaussian conditional distributions, we have 
\begin{align*}
    \by \mid \bx_{\sigma} 
    &\sim \cN\left(\bSigma_{12}\bSigma_{22}^{-1}\bx_{\sigma}, \bSigma_{11} - \bSigma_{12}\bSigma_{22}^{-1}\bSigma_{12}\right)\\
    &= \cN\left( \bA \bSigma_x(\bSigma_x + \sigma^2 \eye)^{-1} \bx_{\sigma}, \bSigma_y + \bA \bSigma_x \bA^\top - \bA \bSigma_x(\bSigma_x + \sigma^2 \eye)^{-1} \bSigma_x \bA^\top \right).
\end{align*}
    We may then simplify $\bA \bSigma_x \bA^\top - \bA \bSigma_x(\bSigma_x + \sigma \eye)^{-1} \bSigma_x \bA^\top = \bA (\bSigma_x - \bSigma_x(\bSigma_x + \sigma \eye)^{-1} \bSigma_x)  \bA^\top$. Note that $(\bSigma_x - \bSigma_x(\bSigma_x + \sigma^2 \eye)^{-1} \bSigma_x)  = (\bSigma_x - \bSigma_x(\bSigma_x + \sigma \eye)^{-1} (\bSigma_x + \sigma^2 \eye) - \bSigma_x(\bSigma_x + \sigma^2 \eye)^{-1} \sigma^2 \eye ) = \sigma^2 \bSigma_x (\bSigma_x + \sigma^2 \eye)^{-1}$. Define $\bS(\sigma) := \bSigma_x(\bSigma_x + \sigma^2 \eye)^{-1}$. We conclude that
    \begin{align}
        \by \mid \bx_{\sigma}   \sim \cN(\bA \bS(\sigma) \bx_{\sigma}, \bSigma_y + \sigma^2 \bA \bS(\sigma)\bA^\top ), 
    \end{align}
\end{proof}

