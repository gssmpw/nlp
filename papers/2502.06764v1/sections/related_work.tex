\section{Preliminaries and Related Work}

\input{figures/method/architecture}
 
\textbf{Diffusion Models.}
Diffusion models~\cite{sohl2015deep, ddpm, song2021scorebased} define a forward process that transforms a data distribution into white noise via a stochastic process over increasing \emph{noise levels} $k \in [0, 1]$: $\bx^k = \alpha_k \bx^0 + \sigma_k \beps$, where $\beps \tightsim \mathcal{N}(0, I)$. The goal of the model is to reverse this process by learning to estimate the \emph{score function} $s_\vtheta(\bx^k, k) \approx \score p_k(\bx^k)$~\cite{vincent2011connection}, which enables iterative denoising of a data point, gradually transforming it from white noise back to a sample from the original distribution. In practice, the score function is often parameterized as an affine function of alternative objectives such as the noise prediction $\beps_\theta(\bx^k, k)\approx 
 \beps$. 

\textbf{Video Diffusion Models (VDMs).} VDMs have enabled the generation of realistic, high-resolution videos~\cite{videoworldsimulators2024,yang2024cogvideox,zheng2024open,kong2024hunyuanvideo}. Their success is largely attributed to advancements such as transferring successful image diffusion models~\cite{singer2022make, guo2023animatediff}, scaling data and model~\cite{blattmann2023stable}, improving transformer-based architectures~\cite{peebles2023scalable, gupta2023photorealistic,jin2024pyramidal}, and enhancing computational efficiency through multi-stage approaches like latent VDMs~\cite{he2022latent, blattmann2023align, ma2024latte,yin2024slow}. 
Many of these models~\cite{blattmann2023stable, yang2024cogvideox} focus on generating videos from a single first image. In contrast, our model is designed to condition on arbitrary length histories, a crucial capability for autoregressively extending newly generated videos.

\textbf{Conditional Diffusion Sampling with Guidance.} \emph{Classifier-free guidance} (CFG)~\cite{ho2022classifierfree} is a crucial technique for improving sample quality in diffusion models. CFG jointly trains conditional and unconditional models $s_\vtheta(\bx, \rvc, k) \approx \score p_k(\bx^k | \rvc)$ and $s_\vtheta(\bx, \varnothing, k) \approx \score p_k(\bx^k)$ by randomly dropping out the conditioning $\rvc$. During sampling, the true conditional score $\score p_k(\bx^k | \rvc)$ is replaced with the weighted score
\vspace{-4pt}
\begin{equation}
\score p_k(\bx^k) + \omega \big[ \score p_k(\bx^k | \rvc) - \score p_k(\bx^k) \big],
\vspace{-4pt}
\end{equation}
where $\omega \geq 1$ is the \emph{guidance scale} that pushes the sample towards the conditioning. In VDMs, CFG is predominantly used for text guidance ~\cite{ho2022video,wang2023modelscope}. For frame conditioning, ``first frame'' guidance is commonplace in image-to-video models~\cite{blattmann2023stable,yang2024cogvideox}, or ``fixed set of few frames''~\cite{blattmann2023align,gupta2023photorealistic,watson2024controlling}, likewise in multi-view diffusion models~\cite{gao2024cat3d}. 

Our work generalizes CFG by enabling guidance with a variable number of conditioning frames and later extends beyond the conventional approach of subtracting an unconditioned score - similar to prior works in compositional generative models~\cite{du2024compositional, liu2022compositional, du2023reduce}, we compose score from multiple conditioning to combine their behaviors. Additionally, we eliminate the reliance on binary-dropout training, the default mechanism for enabling CFG, which we empirically show performs sub-optimally when extended to history guidance.


\textbf{Diffusion Forcing.}  Traditionally, diffusion models are trained using uniform noise levels across all tokens. Diffusion Forcing (DF)~\cite{chen2024diffusion} proposes training sequence diffusion models with independently varied noise levels per frame. Although DF provides theoretical and empirical support for this approach, their work focuses on causal, state-space models. CausVid~\cite{yin2024slow} builds on DF by scaling it to a causal transformer, creating an autoregressive video foundation model. Our work extends the flexibility of DF by developing both the theory and architecture for non-causal, state-free models, enabling new, unexplored capabilities in video generation.

