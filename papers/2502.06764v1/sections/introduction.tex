\section{Introduction}
Diffusion models are effective generative models in  domains such as image, sound, and video. Critical to their success is classifier-free guidance (CFG) \cite{ho2022classifierfree}, which trades off between sample quality and diversity by jointly training a conditional and an unconditional diffusion model and combining their score estimates when sampling.

In the realm of video generative models, CFG commonly relies on either text or image prompts as conditioning variables. Yet, another conditioning variable, namely the entire collection of previous video frames, or \emph{history},  deserves further exploration. In this paper, we investigate the following question: \ul{Can we use different portions of history - variable lengths, subsets of frames, and even different image-domain frequencies - as a form of guidance for video generation?}
{}
Importantly, CFG with flexible history is incompatible with existing diffusion model architectures and the most obvious fix significantly degrades sample quality   (see \Cref{sec:history_guidance_challenges}). %

To address these limitations, we propose the \method (\mtd), a video diffusion framework that enables flexible conditioning on any portion of the input history. Extending the ``noising-as-masking" paradigm in Diffusion Forcing~\cite{chen2024diffusion} to non-causal transformers, \mtd trains video diffusion models by applying independent noise levels to each frame. During sampling, portions of the history can be selectively masked with noise, enabling flexible conditioning and guidance. For instance, in CFG, the unconditional score corresponds to our model with the entire history masked out. Notably, \mtd is compatible with existing architectures such as DiT~\cite{peebles2023scalable} and U-ViT~\cite{hoogeboom2023simple, hoogeboom2024simpler} and can be efficiently implemented through fine-tuning of pre-trained video diffusion models.

At sampling time, the \mtd facilitates a family of history-conditioned guidance methods, collectively referred to as \emph{History Guidance} (HG). The simplest of these, \emph{Vanilla History Guidance} (\HGv), uses an arbitrary length of history as the conditioning variable for CFG. Notably, even this simple method significantly enhances video quality. We further introduce two advanced methods enabled by the \mtd: \emph{Temporal History Guidance} (\HGt) and \emph{Fractional History Guidance} (\HGf)
. These extend history guidance beyond a special case of CFG. Temporal History Guidance combines scores from different history windows. Fractional History Guidance conditions on history windows corrupted by varying levels of noise, effectively acting as a ``low-pass filter'' on historical frames. With minor modifications, it can also target specific \emph{frequency bandwidths} to enhance the dynamic degree of generated videos (hence the frequency-based terminology). Together, we compose \HGt and \HGf to create a comprehensive history guidance paradigm, which we term \emph{history guidance across time and frequency} (\HGtf).



The \method and associated History Guidance methods dramatically improve the quality and consistency of video generation, enabling the creation of exceptionally long videos through autoregressive extension, outperforming the de facto standard DiT diffusion and performing on par with industry models trained with an order of magnitude more compute. In Fig.~\ref{fig:teaser}, we showcase our method by using history guidance across time and frequency with \mtd{} to generate an 862-frame navigation video from a single imageâ€”many times longer than prior results and the maximum video length in the training set.


Our contributions can be summarized as follows: 
\textbf{1.}~We propose the \emph{\method} (\mtd), a competitive video diffusion framework that enables sampling-time conditioning using \emph{any portion} of history, a capability that is difficult to achieve with existing models.
\textbf{2.}~We introduce \emph{History Guidance} (\HG), a family of history-conditioned guidance methods enabled by \mtd that significantly enhance sample consistency, motion dynamics, and visual quality in video diffusion.
\textbf{3.}~We empirically demonstrate the state-of-the-art performance and new capabilities enabled by our method, especially in long video generation. Additionally, we provide a theoretical justification of the training objective through a variational lower bound.

