\begin{figure*}[t]
\centering
\begin{subfigure}[b]{0\textwidth}
\end{subfigure}
\begin{subfigure}[b]{0.95\textwidth}
    \includegraphics[width=\textwidth]{figures/method/fig_architecture.pdf}
    \phantomcaption
    \vspace{-10pt}
    \label{fig:architecture-conventional}
\end{subfigure}
\begin{subfigure}[b]{0\textwidth}
    \phantomcaption
    \label{fig:architecture-ours}
\end{subfigure}
\vspace{-10pt}
\caption{ \small \textbf{Comparison of the conventional conditional video diffusion models and Diffusion Forcing Transformer.}  At training time, conventional (a) approaches treat history as part of the conditioning input, first encoded by an \emph{separate} encoder and then injected to the DiT via Adaptive Layer Norm and scaling. The Diffusion Forcing Transformer (b) instead does not distinguish between history and generation target frames. It trains a DiT to denoise \emph{all} frames of a sequence, where frames have independently varying noise levels. }
\label{fig:architecture}
\vspace{-10pt}
\end{figure*}

