\section{Proposed Approach}
\label{sec:proposed}

\begin{figure*}[tb]
 \centering
\includegraphics[width=0.95\linewidth]{figures/kGraph_pipeline.pdf}
\vspace{-0.2cm}
 \caption{$k$-Graph pipeline.}
 \label{fig:pipeline}
\vspace{-0.3cm}
\end{figure*}

In this section, we describe $k$-Graph, an approach for time series clustering, as a solution to tackle the problem described in the previous section. 
For a given dataset $\mathcal{D}$, the overall $k$-Graph process is divided into three main steps as follows:

\begin{enumerate}[noitemsep, topsep=1pt, parsep=1pt, partopsep=1pt, leftmargin=0.5cm]
\item {\bf Graph Embedding} (Section~\ref{sol:graphembedding}): for $M$ different subsequence lengths, we compute $M$ graphs.
For a given subsequence length $\ell$, The set of nodes represent groups of similar subsequences of length $\ell$ within the dataset $\mathcal{D}$. The edges have weights corresponding to the number of times one subsequence of a given node has been followed by a subsequence of the other node.

\item {\bf Graph Clustering} (Section~\ref{sol:graphclustering}): For each graph, we extract a set of features for all time series of the dataset $\mathcal{D}$. These features correspond to the nodes \paul{and edges}
that the time series crossed. Then, we use these features to cluster the time series using $k$-Means \paul{for scalability reasons}.

\item {\bf Consensus Clustering} (Section~\ref{sol:consensusclustering}): At this point we have $M$ clustering partitions (i.e., one per graph). \paul{We build a consensus matrix $M_C$. We then cluster this matrix using spectral clustering in the objective of grouping time series that are highly connected (i.e., grouped in the same cluster in most of the $M$ partitions)}. 
The output of this clustering step is the labels provided by $k$-Graph.

\item {\bf Interpretability Computation} (Section~\ref{sol:interpretability}): after obtaining the clustering partition, we select the most relevant graph (among the $M$ graphs), and we compute the \paul{interpretable} graphoids.
\end{enumerate}

\IncMargin{0.5em}
\begin{algorithm}[tb]
{\small
    \caption{\textbf{$k$-Graph}}\label{alg:kgraph}
    \SetKwInOut{Input}{input}
    \SetKwInOut{Output}{output}
    \Input{Dataset $\mathcal{D}$, Number of clusters $k$, Number of length $M$ (default is 30), Maximum length rate $rml$ (default is 0.4), Sample rate $smpl$ (default is 10)}
    \Output{Clustering labels $\mathcal{L}$}
    \BlankLine
    $all$-$\mathcal{L}$ $\leftarrow$ $[]$ \;
    $R$ $\leftarrow$ $M$ random lengths in $[5,(min_{T \in \mathcal{D}} |T|)*rml]$ \;
    \ForEach{$\ell \in R$ }
    {
        $\mathcal{G}_\ell \leftarrow Graph(\mathcal{D},\ell,smpl)$ \tcp{Section~\ref{sol:graphembedding}}
        $F_{\mathcal{G},\ell} \leftarrow extractFeature(\mathcal{D},\mathcal{G}_\ell)$ \tcp{Section~\ref{sol:graphclustering}}
        $\mathcal{L}_{\ell} \leftarrow k$-$Means(F_{\mathcal{G},\ell},k)$ \tcp{Section~\ref{sol:graphclustering}}
        add $\mathcal{L}_{\ell}$ in $all$-$\mathcal{L}$\;
    }
    $M_C$ $\leftarrow$ $Consensus(all$-$\mathcal{L})$ \tcp{Section~\ref{sol:consensusclustering}}
    $\mathcal{L}$ $\leftarrow$ $SpectralClustering(M_c,k)$ \tcp{Section~\ref{sol:consensusclustering}}

    return $\mathcal{L}$

    
}
\end{algorithm}
\DecMargin{0.5em}

Figure~\ref{fig:pipeline} illustrates the different steps of $k$-Graph, detailed in Algorithm~\ref{alg:kgraph}.
Below, we describe each step in detail.

\subsection{Graph Embedding}
\label{sol:graphembedding}

As described in Section~\ref{sec:Probdef}, the general objective is to build one graph representing the time series dataset on which we can perform clustering. 
\paul{For this step, a solution could be to use the Series2Graph algorithm~\cite{Series2GraphPaper}. 
Nevertheless, Series2Graph presents three limitations. 
First, it accepts a continuous time series as input, rather than a dataset, that may contain several series of different lengths. 
Second, the user must specify a subsequence length $\ell$. 
This becomes problematic in interpretable clustering, as determining an optimal length in advance is challenging. 
Third, the embedding employed by Series2Graph does not scale to large dataset sizes, which would be a significant drawback in our case. 
In $k$-Graph, we propose the following graph embedding procedure. 

For a dataset $\mathcal{D}$, we build $M$ different graphs $\mathcal{G}_\ell=(\mathcal{N}_\ell,\mathcal{E}_\ell)$ for $M$ different subsequence length $\ell$. Algorithm~\ref{alg:GraphEmbedind} outlines the procedure for constructing a single graph for a given dataset and subsequence length.
The procedure is the following:

\begin{enumerate}[noitemsep, topsep=1pt, parsep=1pt, partopsep=1pt, leftmargin=0.5cm]
    \item {\bf Subsequence Embedding}: For each time series $T \in \mathcal{D}$, we collect all the subsequences of a given length $\ell$ into an array called $Proj(T,\lambda)$. We then concatenate all the computed $Proj(T,\lambda)$ into $Proj$ for all the time series in the dataset (Line~\ref{concatenation}).
    We then sample $Proj$ (user-defined parameter $smpl$) and keep only a limited number of subsequences stored in $Proj_{smpl}$. We use the latter to train a Principal Component Analysis (PCA) (Line~\ref{pcatrain}). We then use the trained PCA and a rotation step to project all the subsequences into a two-dimensional space that only preserves the shapes of the subsequences. The result is denoted as $SProj$. We denote the PCA and rotation steps $Reduce(Proj,pca)$, where $pca$ is the trained PCA.
    
    \item {\bf Node Creation}: Create a node for each of the densest parts of the above two-dimensional space~\cite{Series2GraphPaper}. 
    In practice, we perform a radial scan of $SProj_{smpl}$.
    For each radius, we collect the intersection with the trajectories of $SProj_{smpl}$, and we apply kernel density estimation on the intersected points: each local maximum of the density estimation curve is assigned to a node.
    These nodes can be seen as a summarization of all the major length patterns $\ell$ that occurred in $\mathcal{D}$. For this step, we only consider the sampled collection of subsequences $SProj_{smpl}$. We denote this step $NodeCr(SProj_{smpl})$ (Line~\ref{nodec}).

    \item {\bf Edge Creation}: Retrieve all transitions between pairs of subsequences represented by two different nodes: each transition corresponds to a pair of subsequences, where one occurs immediately after the other in a time series $T$ of the dataset $\mathcal{D}$. We represent transitions with an edge between the corresponding nodes. We note this step $EdgeCr(SProj,\mathcal{N}_\ell)$ (Line~\ref{nodec}). $\mathcal{N}_\ell$ is the node set extracted in the above step.
\end{enumerate}
}
\IncMargin{0.5em}
\begin{algorithm}[tb]
{\small
    \caption{\textbf{Graph Embedding $Graph(\mathcal{D},\ell)$}}\label{alg:GraphEmbedind}
    \SetKwInOut{Input}{input}
    \SetKwInOut{Output}{output}
    \Input{Dataset $\mathcal{D}$, input length $\ell$, sample $smpl$}
    \Output{graph $\mathcal{G}_\ell$}
    \BlankLine
    \tcp{Concatenante subsequences}
    $Proj \leftarrow []$\;
    \ForEach{$T \in \mathcal{D}$ }
    {
        add $Proj(T,\ell)$ in $Proj$\;\label{concatenation}
    }
    \tcp{Reduce to a two-dimensional space}
    $Proj_{smpl}$ $\leftarrow$ randomly select $\lfloor\frac{|Proj|}{smpl}\rfloor$ elements in $Proj$\;
    $pca$ $\leftarrow$ $PCA.fit(Proj_{smpl})$\;\label{pcatrain}
    $SProj$ $\leftarrow$ $Reduce(Proj,pca)$\;

    \tcp{Extract the nodes and edges}
    $\mathcal{N}_\ell, \mathcal{E}_\ell \leftarrow NodeCr(SProj_{smpl}), EdgeCr(SProj,\mathcal{N}_\ell)$\;\label{nodec}

    return $\mathcal{G}_\ell = (\mathcal{N}_\ell,\mathcal{E}_\ell)$

    
} % font size
\end{algorithm}
\DecMargin{0.5em}

\rev{Despite the linear aspect of the PCA (in the subsequence embedding step), we observed in practice that using the first three components was sufficient to embed enough shape-based information~\cite{Series2GraphPaper}. Furthermore, PCA is renowned for its ability to identify and emphasize principal patterns while diminishing the impact of noise~\cite{Soltysik2015ImprovingTU,Lei2014SparsistencyAA}. The latter guarantees a graph-based representation less affected by the presence of noise in the time series subsequences.}


As mentioned above, the graph embedding requires a parameter $smpl$ that controls the number of subsequences used to train the PCA and to create the nodes. 
We set by default the parameter $smpl=10$ and evaluate its influence in Section~\ref{sec:exp}. 
Moreover, we generate the graph embedding using Algorithm~\ref{alg:GraphEmbedind} for $M$ different lengths randomly selected from a predefined interval. 
We denote the set of randomly selected lengths as $R$, where the user can specify the number of lengths by setting $M = |R|$. 
Our observations show that accuracy and execution time tend to improve with an increase in $M$. However, the accuracy reaches a plateau after $M=30$, as demonstrated in Section~\ref{sec:exp}.

Since the subsequence embedding step necessitates subsequences of at least 5 points, the length interval can be constrained to $[5, \min_{T \in \mathcal{D}} |T|]$. 
However, empirical observations indicate that this interval can be further optimized. We define this interval as $[5, ( \min_{T \in \mathcal{D}} |T|) \times rml]$, where $rml$ (rate maximum length) is a user-defined parameter with a default value of 0.4 
(evaluated in Section~\ref{sec:exp}).

Figure \ref{fig:pipeline}, we show the graphs for $3$ different lengths ($\ell$ equal to 10, 30, and 50) for the TwoLeadECG dataset of the UCR-Archive~\cite{Dau2018TheUT}. 
The variation in the graph's topology emphasizes the significance of subsequence length. 
Building multiple graphs ensures that $k$-Graph does not rely on a predetermined subsequence length, thus avoiding situations where an erroneous or unsuitable choice of the single subsequence length leads to a suboptimal graph, potentially failing to capture critical data patterns.


\subsection{Graph Clustering}
\label{sol:graphclustering}

At this stage, we have $M$ distinct graphs corresponding to $M$ different subsequence lengths. 
To perform a clustering partition of the dataset $\mathcal{D}$ using these graphs, we opt for a feature-based approach for execution time and interpretability. 
Our goal is to maximize information extraction from the graph. For this purpose, we extract \paulJournal{three} types of features for each time series: (i) node-based, (ii) edge-based features, \paulJournal{and (iii) degree-based features.} Specifically, \paulJournal{for two first types of features}, we quantify how many times a given time series intersects each node and edge in the graph. \paulJournal{For the third type, we measure the degree for each node in the subgraph corresponding to each time series. For example, the same node can have a high degree for one time series (corresponding to a specific subgraph in $\mathcal{G}_\ell$) and a low degree for another time series (corresponding to another subgraph).}
Formally, for a given dataset $\mathcal{D}$ and its graph embedding $\mathcal{G}_\ell=(\mathcal{N}_\ell,\mathcal{E}_\ell)$ computed using a subsequence length $\ell$, we define for a time series $T \in \mathcal{D}$, $\mathcal{M}_\ell(T) = \langle N^{(1)},N^{(2)},...,N^{(m)} \rangle$ the path of $T$ in $\mathcal{G}_\ell$, \paulJournal{ and $\mathcal{G}_\ell(T)$ the corresponding subgraph.} 
Thus, we define the feature matrix $F_{\mathcal{G},\ell} \in \mathbb{R}^{(|\mathcal{D}|,|\mathcal{N}_\ell|+|\mathcal{E}_\ell|)}$ as follows:

\paulJournal{
{\small
\begin{align*}
\begin{split}
\forall T_i \in \mathcal{D}, F_{\mathcal{G},\ell}[i] = [f_{T_i,N_1},...,f_{T_i,N_{|\mathcal{N}_\ell|}}, &f_{T_i,E_1},...,f_{T_i,E_{|\mathcal{E}_\ell|}}, \\
&f^d_{T_i,N_1},...,f^d_{T_i,N_{|\mathcal{N}_\ell|}}] \\
\end{split}
\end{align*}
}

With $\forall N,E \in \mathcal{N}_\ell,\mathcal{E}_\ell$:
{\small
%{\small
\begin{align*}
\begin{split}
    f_{T_i,N} &= \sum_{N^{(j)} \in \mathcal{M}_\ell(T_i)} 1_{[N=N^{(j)}]} \\
    f_{T_i,E} &= \sum_{(N^{(j)},N^{(j+1)}) \in \mathcal{M}_\ell(T_i)} 1_{[E=(N^{(j)},N^{(j+1)})]}\\
    f^{d}_{T_i,N} &=  deg_{\mathcal{G}_\ell(T)}(N)*1_{[N=(N^{(j)},N^{(j+1)})]}
\end{split}
\end{align*}
%}
}
}
\paulJournal{In the equation above, $deg_{\mathcal{G}_\ell(T)}(N)$ denotes the degree of the node $N$ within the subgraph $\mathcal{G}_\ell(T)$.}
As described above, $F_{\mathcal{G},\ell}$ is a sparse matrix with values greater than zero only if a time series crosses a node or an edge. 
However, as time series can be of different lengths, the number of nodes and edges crossed can vary significantly from one dataset instance to another. 
As a consequence, we normalize each row $F_{\mathcal{G},\ell}[i]$ by subtracting the its mean $\mu(F_{\mathcal{G},\ell}[i])$ and dividing by its standard deviation $\sigma(F_{\mathcal{G},\ell})$. 
We can then apply any clustering algorithm on $F_{\mathcal{G},\ell}$. 

\rev{We use $k$-Means with Euclidean distance to produce a partition $\mathcal{L}_\ell$ for a given graph $\mathcal{G}_\ell$. Normalization is applied only during the clustering step, while the unnormalized feature matrix is retained for computing graphoids. The choice of $k$-Means is motivated by its scalability for large datasets and its centroid-based approach, which aids interpretability. The centroids from graph clustering are crucial for computing graphoids, which are key to interpreting the clustering results of k-Graph (see Section~\ref{sol:interpretability}).}


\subsection{Consensus Clustering}
\label{sol:consensusclustering}

At this point, we have one clustering partition $\mathcal{L}_\ell$ per graph built in the graph embedding step ($M$ in total, for $\ell$ in the set of randomly selected lengths $R$). 
The objective is to compute a consensus from all these partitions. 
The problem of consensus clustering, or ensemble clustering, is a well-studied problem with several methods proposed in the literature~\cite{Strehl2003ClusterE,Fred2005CombiningMC}.

Following established practice~\cite{Strehl2003ClusterE}, we build a consensus matrix, which we employ to measure how many times two time series have been grouped in the same cluster for two graphs built with two different lengths. 
Formally, we define $M_C \in \mathbb{R}^{(|\mathcal{D}|,|\mathcal{D}|)}$ a matrix computed as follows:
\begin{equation}
\small{
    \forall T_i,T_j \in \mathcal{D}, M_C[i,j] = \frac{1}{|R|}\sum_{\ell \in R} 1_{[\mathcal{L}_{\ell,i} = \mathcal{L}_{\ell,j}]}
}
\end{equation}

$M_C$ can be seen as a similarity matrix about the clustering results obtained on each graph. More specifically, for two time series $T_i$ and $T_j$, if $M_C[i,j]$ is high, they have been associated in the same cluster for several subsequence lengths and can be grouped in the same cluster. On the contrary, if $M_C[i,j]$ is low, the two time series were usually grouped in different clusters regardless of the subsequence length. \paul{Therefore, the $M_C$ matrix can be seen as the adjacency matrix of a graph. 
In this graph, nodes are the time series of the dataset, and an edge exists if two time series have been clustered together in the same cluster (the weights of these edges are the number of times these two time series have been clustered together). 
As the objective is to find communities of highly connected nodes (i.e., time series that were grouped multiple times in the same cluster), we use spectral clustering (with $M_C$ used as a pre-computed similarity matrix)}. 
%\ang{because of low complexity?}
The output of the spectral clustering is the final labels $\mathcal{L}$ of $k$-Graph.

\subsection{Interpretability and Explainability}
\label{sol:interpretability}

\begin{figure}[tb]
 \centering
\includegraphics[width=\linewidth]{figures/length_selection.pdf}

 \caption{$W_c$ and $W_e$ for $k$-Graph (with $k=4$ and $M=10$) applied on the Trace dataset of the UCR-Archive.}
\vspace{-0.3cm}
 \label{fig:lengthselection}
\end{figure}

The previous section elucidated $k$-Graph's approach to the time series clustering problem. 
Now, we delve into the steps necessary for solving the interpretable clustering problem. 
As outlined in Section~\ref{sec:Probdef}, we formalized the interpretability problem through the computation of graphoids. 
The challenge lies in the assumption that there is a unique graph. 
However, $k$-Graph operates with M graphs to produce one dataset partition. 
We are presented with two options: (i) compute the graphoids for each graph, or (ii) select one graph that is most representative of the final labeling $\mathcal{L}$.

Considering option (i), we assume equal contributions from each graph to the labels $\mathcal{L}$. 
However, this assumption falters due to the consensus clustering step. 
For instance, among the M graphs, one-third might generate one type of partition, while the remaining two-thirds yield another. 
Consequently, the partition from the latter two-thirds would be selected as the final labels $\mathcal{L}$ of $k$-Graph. 
As a result, one-third of the graphs might be irrelevant for the final labels, yielding graphoids that do not correctly interpret the clustering.

Hence, we opt for option (ii), choosing one of the M graphs: the one that is most relevant to the clustering labels $\mathcal{L}$ produced by $k$-Graph. 
To make this selection, we employ two criteria: the consistency of $\mathcal{L}_\ell$ with $\mathcal{L}$, and the interpretability factor of $\mathcal{G}_\ell$ associated with $\mathcal{L}$.

\noindent{\bf [Consistency]} To gauge the consistency between the final labels $\mathcal{L}$ and those generated by each graph $\mathcal{G}_\ell$ (for $\ell \in R$), we utilize the Adjusted Rand Index (ARI). \rev{The latter is particularly suitable for unbalance datasets as it accounts for expected similarity due
to chance (See Section~\ref{sec:exp}). } Formally, the consistency is expressed as $W_c(\ell) = ARI(\mathcal{L},\mathcal{L}_\ell)$, measuring the agreement between the clustering labels produced by $k$-Graph and those corresponding to a specific subsequence length $\ell$. 
This index provides a quantitative similarity measure, with higher values indicating greater consistency.

For a practical illustration, Figure~\ref{fig:lengthselection}(a) displays the values of $W_c$ across different subsequence lengths ($\ell$) in the range $R=[10,28,27,36,45,54,63,72,81,90]$. 
In this example, $k$-Graph, configured with $k=4$ and $M=10$, is applied to the Trace dataset from the UCR-Archive. 
The graph generated with subsequence lengths between $27$ and $54$ demonstrates high consistency (ARI values above $0.9$) with the final labels $\mathcal{L}$, indicating that these lengths are more suitable for interpretation. 
Conversely, other lengths result in lower consistency, suggesting that labels produced under these conditions are less aligned with the final clustering labels of $k$-Graph. 
Therefore, for optimal interpretability, it is advisable to consider subsequence lengths within the range of $27$ to $54$.

\noindent{\bf [Interpretability Factor]} The second criterion focuses on selecting the most interpretable graphs in tandem with a given clustering partition. 
As elucidated in Section~\ref{sec:Probdef}, interpretability is achieved by ensuring that each cluster has precisely one node, characterized by $|N|_{C_i}=Pr_{C_i}(N) = 1$. 
To quantify interpretability, we calculate the interpretability factor as the maximum $Pr_{C_i}(N)$ across all nodes $N$ in the graph $\mathcal{G}_\ell$.
Formally, let $\mathcal{D}$ be a dataset, and $\mathcal{G}_\ell=(\mathcal{N}_\ell,\mathcal{E}_\ell)$ be its graph embedding computed with a subsequence length $\ell$. 
We denote $C={C_1,...,C_k}$ as the clustering partition associated with labels $\mathcal{L}$ obtained using $k$-Graph. The interpretability factor $W_e(\ell)$ of $\mathcal{G}_\ell$ is defined as:

\begin{equation}
\small{
    W_e(\ell) = \frac{1}{k} \sum_{C_i \in C} \max_{N \in \mathcal{N}_\ell} Pr_{C_i}(N)
}
\end{equation}

In practical terms, the $\mathcal{M}$ function in Definition~\ref{defnodeEx} is instantiated as follows: for a time series $T \in \mathcal{D}$, $\mathcal{M}_\ell(T) = \langle N^{(1)},N^{(2)},...,N^{(m)} \rangle$, representing the path of $T$ in $\mathcal{G}_\ell$. 
Figure~\ref{fig:lengthselection}(b) illustrates $W_e$ for the Trace dataset of the UCR-Archive, revealing that graphs computed with shorter lengths exhibit a higher interpretability factor.
To select the optimal length $\bar{\ell}$ and, consequently, the graph $\mathcal{G}_\ell$ that maximizes both $W_c$ and $W_e$, we employ the following criterion:

\begin{equation}
\small{
    \bar{\ell} = \underset{\ell \in R}{\operatorname{argmax}} \big[W_e(\ell).W_c(\ell)\big]
    }
\end{equation}

In Figure~\ref{fig:lengthselection}, the sole length that maximizes the product $W_e(\ell) \cdot W_c(\ell)$ is $\bar{\ell}=36$. Subsequently, the graph associated with this length is utilized to compute the graphoids.

To compute the graphoids, we use the feature matrix $F_{G,\bar{\ell}}$ (described in Section~\ref{sol:graphclustering}). In practice, we compute the representativity and the exclusivity of a node $N$ in $\mathcal{G}_\ell$. We can compute the two previous measures for all nodes in $\mathcal{G}_\ell$, and then select only those that satisfy the property of $\lambda$-Graphoid and $\gamma$-Graphoid for a desired value of $\lambda$ and $\gamma$. As one highly representative and exclusive node per cluster is enough, we automatically return one node $\bar{N}_{C_i}$ per cluster such that:

\begin{equation}
\small{
    \bar{N}_{C_i} = \underset{N \in \mathcal{N}_\ell}{\operatorname{argmax}} \big[|N|_{C_i}.Pr_{C_i}(N)\big]
}
\end{equation}

The node $\bar{N}_{C_i}$ is considered the most interpretable node for a given cluster $C_i$. Finally, for each node, we store the timestamps of the subsequences in $\mathcal{D}$ associated with it. Therefore, we compute the centroid of all the subsequences that compose $\bar{N}_{C_i}$. Figure~\ref{fig:interpretability} and Figure~\ref{fig:interpretability_ex} show the most interpretable nodes (and the corresponding centroids) identified by $k$-Graph for several example datasets.

\rev{In our approach, interpretability (i.e., graphoid for each cluster) is integrated during the training phase rather than applied post-hoc. By embedding interpretability into the clustering process, we ensure that the selected graph consistently represents the clustering logic during both training and subsequent predictions. This ensures reliable interpretability, even when incoming time series exhibit variability or contain multiple relevant subsequences.}


\subsection{Complexity Analysis}
\label{sol:complexity}

This section analyzes the computational complexity of different steps in $k$-Graph. 
For the Graph Embedding step, the sequence embedding complexity is bounded by the PCA (using a randomized SVD solver) with complexity $O(|\mathcal{D}|^2\cdot|T|^2)$. 
The node creation and edge creation steps are both $O(|\mathcal{D}|\cdot|T|)$ on average.
Following the graph embedding step, the complexity for feature matrix creation and $k$-Means clustering is $O(|\mathcal{D}|\cdot(|\mathcal{N}|+|\mathcal{E}|))$ and $O(k.|\mathcal{D}|\cdot(|\mathcal{N}|+|\mathcal{E}|))$ respectively. In practice, the number of nodes and edges is significantly smaller than the number of subsequences in the dataset, making this step negligible compared to the graph embedding step. 
Finally, building the consensus matrix requires $O(M\cdot|\mathcal{D}|^2)$. 
As the consensus matrix is already built, spectral clustering requires, in the worst case, $O(|\mathcal{D}|^2)$. 
Overall, since the first two steps are executed $M$ times (for $M$ different subsequence lengths), the $k$-Graph complexity is $O(M\cdot|\mathcal{D}|^2\cdot|T|^2)$. 
However, as the computations of the $M$ graphs are independent, they can be executed in parallel, significantly reducing execution time. 
Moreover, the $smpl$ parameter significantly reduces $|\mathcal{D}|$. 
Section~\ref{sec:exp} evaluates the influence of $M$ and $smpl$ on execution time.