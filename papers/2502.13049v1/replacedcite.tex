\section{Background and Related Work}
\label{sec:background}

We first introduce
notations useful for the rest of the paper (Section~\ref{sec:notation}). Then, we review
existing time-series clustering methods and discuss their limitations related to interpretability and user interaction (Section~\ref{sec:clus_methods}). We then motivate and introduce the usage of Graph embedding for time series clustering (Section~\ref{sec:graphforts}). We finally properly define the problem tackled in this paper (Section~\ref{sec:Probdef}).

\subsection{Time Series and Graph Notation}
\label{sec:notation}

\textbf{Time Series: }A \rev{univariate } time series $T \in \mathbb{R}^n $ is a sequence of real-valued numbers $T_i\in\mathbb{R}$ $[T_1,T_2,...,T_n]$, where $n=|T|$ is the length of $T$, and $T_i$ is the $i^{th}$ point of $T$. \rev{In the rest of this paper , we refer to univariate time series as {\it time series}. } We are typically interested in local regions of the time series, known as subsequences. A subsequence $T_{i,\ell} \in \mathbb{R}^\ell$ of a time series $T$ is a continuous subset of the values of $T$ of length $\ell$ starting at position $i$. Formally, $T_{i,\ell} = [T_i, T_{i+1},...,T_{i+\ell-1}]$.
A dataset $\mathcal{D}$ is a set of time series (possibly of different lengths). 
We define the size of $\mathcal{D}$ as $|\mathcal{D}|$.

\noindent \textbf{Graph: }
We introduce some basic definitions for graphs, which we will use in this paper.
We define a Node Set $\mathcal{N}$ as a set of unique integers.
Given a Node Set $\mathcal{N}$, an Edge Set $\mathcal{E}$ is then a set composed of tuples $(x_i,x_j)$, where $x_i,x_j \in \mathcal{N}$. 
Given a Node Set $\mathcal{N}$, an Edge Set $\mathcal{E}$ (pairs of nodes in $\mathcal{N}$), a Graph $\mathcal{G}$ is an ordered pair $\mathcal{G}=(\mathcal{N},\mathcal{E})$.
A directed graph $\mathcal{G}$ is an ordered pair $\mathcal{G}=(\mathcal{N},\mathcal{E})$ where $\mathcal{N}$ is a Node Set, and $\mathcal{E}$ is an ordered Edge Set.
In the rest of this paper, we will only use directed graphs, denoted as $\mathcal{G}$.

\subsection{Time Series Clustering}
\label{sec:clus_methods}
Time series clustering plays a pivotal role in uncovering meaningful patterns within temporal datasets, where the primary goal is to group similar time series for insightful analysis. The specific challenge addressed by partitional time series clustering involves partitioning a dataset of $n$ time series, denoted as $D$, into $k$ clusters $(C = \{C_{1}, C_{2}, ..., C_{k}\})$, where the number of clusters, $k$, is predetermined. Although the choice of $k$ can be determined using wrapper methods, it is assumed to be fixed in advance in many experiments. 

\subsubsection{Raw-Based Approaches}

Clustering algorithms for time series can either operate directly on raw data or use transformations to derive features before clustering. Algorithms specifically designed for time series often prefer using raw data____ for several reasons. First, \textbf{Preservation of Information}: raw time series preserve intricate details of temporal patterns, ensuring information fidelity. Second, \textbf{Temporal Dependencies}: they capture dynamic patterns and temporal interdependencies that might be lost during feature extraction. Finally, \textbf{Data Exploration}: direct analysis of raw time series fosters the discovery of unexpected patterns and trends, supporting an exploratory, discovery-driven approach.

In the raw-based approach, the $k$-Means algorithm is commonly used to identify patterns and similarities within temporal data. Each time series is treated as a multidimensional vector, with data points representing observations over time. The goal is to partition the time series into $k$ clusters, optimizing the assignment of series to cluster centers.

$k$-Shape____ is a well-known algorithm for time series clustering in this approach. It starts by randomly selecting initial cluster centers and uses a specialized distance metric to measure dissimilarity based on shapes, addressing temporal dynamics. The algorithm iteratively assigns each time series to the cluster with the smallest shape-based distance and updates cluster centers based on the mean shape. This process continues until convergence, resulting in clusters that highlight shape similarities and provide insights into temporal dynamics.

\rev{A recent study____ has compared $k$-Shape with a large amount of clustering baselines and has demonstrated that none of the recent baselines outperform significantly $k$-Shape. }
\rev{Finally, several raw-basd appraoch are using the Dynamic Time Warping (DTW), a well-known algorithm for measuring the similarity between two time series by aligning them through time axis warping____. This process minimizes the distance between sequences, accommodating shifts and stretches in the time dimension. Although DTW is highly accurate, it is computationally intensive, often necessitating optimizations for improved efficiency. SOMTimeS____ is an algorithm that addresses this challenge. It is a self-organizing map (SOM) designed for clustering time series data, utilizing DTW as a distance measure to effectively align and compare time series. SOMTimeS enhances computational efficiency by implementing a pruning strategy that significantly reduces the number of DTW calculations required during the training.} 

\dt{However, it is important to note that raw-based approaches may face challenges when dealing with noise in raw time series, potentially obscuring meaningful patterns. Additionally, the direct clustering of raw time series might result in clusters lacking clear distinctions or meaningful insights, making extracting valuable information from the data challenging.}

\subsubsection{Feature-Based Approaches}
Adopting a feature-based ____ approach offers several advantages that address the challenges associated with raw-based methods: 
\textbf{Dimensionality Reduction}, feature extraction often reduces dimensionality. Indeed, lower-dimensional feature representations enhance computational efficiency and reduce the risk of the curse of dimensionality; \textbf{Enhanced Discrimination}, feature selection can be tailored to emphasize specific characteristics crucial for discrimination, enhancing the ability of clustering algorithms to distinguish subtle differences between time series, leading to more accurate clustering; and \textbf{Interpretability}, clusters derived from features often yield more interpretable results than those directly from raw time series. Interpretability is crucial for extracting insights, and features-based approaches provide clearer explanations for grouping.

\rev{TS3C____ is an example of feature-based clustering approach. The latter segments the time series and extracts a set of statistical features to represent them. The features are then used to cluster time series. }
FeatTS____ emerges as a feature-based algorithm tailored for univariate time series clustering. It leverages the TSFresh library and employs Principal Features Analysis to extract salient features from time series data. 
Building upon this foundation, the authors introduce Time2Feat (T2F)____, an algorithm designed explicitly for multivariate time series clustering. T2F distinguishes itself by adopting two distinct feature extraction approaches named intra and inter-signal feature extraction. 
Furthermore, the authors enhance the feature selection process by introducing a grid search method for selecting optimal features for clustering multivariate time series.
FeatTS and Time2Feat are two approaches that accommodate unsupervised and semi-supervised clustering scenarios. Notably, Time2Feat stands out as the current state-of-the-art solution for multivariate time series clustering, showcasing the advancements in feature extraction and selection methodologies.

\dt{Nevertheless, feature-based approaches may suffer from the information loss induced by the transformation of the original time series (i.e., sequential patterns) into features. 
Handling high-dimensional feature spaces poses a challenge, especially when dealing with numerous derived features. This complexity can compromise the interpretability of the algorithm, making it difficult for users to examine all the extracted features.
Additionally, the sensitivity of feature engineering poses a challenge, where selecting inappropriate features or applying unsuitable transformations may affect clustering quality.}


\subsubsection{Deep Learning Approaches}

In recent years, deep learning strategies____ have emerged to address the challenges of time series clustering, demonstrating strong performance. These approaches leverage deep learning's ability to directly interface with time series data, marking a significant shift from traditional methods. A key advantage is the \textbf{Hierarchy of Abstraction}, where deep learning architectures capture complex relationships within temporal data, revealing intricate patterns essential for effective clustering. Additionally, these methods exhibit \textbf{High Efficacy}, surpassing traditional techniques with superior accuracy and efficiency.

The Deep Auto-Encoder (DAE)____ is a popular solution, serving as an unsupervised model for representation learning. It transforms raw input data into new space representations, extracting valuable features through encoding. The DAE architecture, characterized by seven fully connected layers, effectively harnesses learned features through an internal layer. These features are subsequently input into a clustering loss function, minimizing the distance between data points and their respective assigned cluster centers.

Another approach is Deep Temporal Clustering (DTC)____, which uses the DAE for feature representation and clustering. The DTC's clustering layer optimizes the Kullback-Leibler (KL) divergence objective, aligning with a self-training target distribution. The encoding process influences clustering performance, with learned representations fed into the $k$-Means algorithm for final clustering.

\dt{In conclusion, despite the high efficacy and the ability to uncover complex patterns, deep learning approaches encounter fundamental challenges related to the interpretability of their decision-making processes. The inherent complexity of these machine learning models often results in a lack of transparency in understanding obtained results. Furthermore, these approaches may struggle to integrate domain-specific knowledge seamlessly, presenting obstacles to guiding the clustering process based on expert insights.}


\subsection{Graph Embedding of Time Series}
\label{sec:graphforts}


We argue that to address the challenges of maintaining information preservation while maximizing interpretability, a viable solution is to represent time series as a (suitably constructed) graph. 
Constructing such a graph involves processing subsequences of the time series dataset. 
These subsequences represent various types of patterns and their temporal succession. This approach furnishes clustering methods with substantial information, contributing to the sustenance of high accuracy. Additionally, this approach facilitates user-friendly navigation through the time series.

Various methods have been proposed to convert time series into graphs for specific analytical tasks to overcome the challenges mentioned earlier. 
Series2Graph embeds univariate time series into a directed graph____, primarily employed for anomaly detection. Similarly, approaches like Time2Graph____ utilize graph representations of time series to address time series classification.
The advantages of such time series graph representations are threefold. 
First, these graph representations are easily interpretable by any user. Second, constructed directly from subsequences of the time series, they preserve essential information. Lastly, a unified embedding can significantly reduce execution time, as evidenced in anomaly detection scenarios____.

However, prior works on graph embedding for time series were often proposed either under supervised settings____, simplifying the graph construction task, or in an unsupervised manner but for continuous time series____.
For the specific task of time series clustering, a graph embedding of time series necessitates the unsupervised construction of a single, comprehensive graph for an entire dataset, encompassing multiple continuous time series. While a straightforward solution would be building one graph per time series, this approach diminishes the interpretability advantage of having a single, concise graph.


\subsection{Problem Formulation}
\label{sec:Probdef}

\begin{figure}[tb]
 \centering
\includegraphics[width=\linewidth]{figures/graphoid.pdf}

 \caption{$\lambda$-Graphoids and $\gamma$-Graphoids for different $\lambda$ and $\gamma$.}
 \label{fig:def}
\vspace{-0.3cm}
\end{figure}

We propose a novel 
approach to time series clustering by employing a graph embedding. The essence of this methodology lies in transforming a time series dataset into a sequence of abstract states corresponding to different subsequences within the dataset. These states are represented as nodes, denoted by $\mathcal{N}$, in a directed graph, $\mathcal{G}=(\mathcal{N},\mathcal{E})$. The edges, $\mathcal{E}$, encode the frequency with which one state occurs after another____. We define this graph as follows:

\begin{definition}[Graph Embedding]   
Let a time series dataset be defined as $\mathcal{D} = \{T_1,T_2,...,T_n\}$. Let $\mathcal{G}=(\mathcal{N},\mathcal{E})$ be a directed graph. 
$\mathcal{G}$ is the graph embedding of $\mathcal{D}$ if there exists a function $\mathcal{M}$ such that for any $T \in \mathcal{D}$, $\mathcal{M}(T) =  \langle N^{(1)},N^{(2)},...,N^{(m)} \rangle $, and  $\forall i \in [1,m], N^{(i)} \in \mathcal{N}$, and $\forall i \in [1,m-1], (N^{(i)},N^{(i+1)}) \in \mathcal{E}$. 
\label{defGraph}  
\end{definition}

The rest of the section considers $\mathcal{M}(T)$ as a subgraph.
For the sake of interpretability, it is imperative that different sections of the graph distinctly capture the similarity between time series. Nodes, representing similar patterns from various time series, and edges, denoting possible transitions between these patterns, play a pivotal role in this context. Consequently, a comparable time series is found to be placed within the same region of the graph.
As a result, in a given dataset $\mathcal{D} = \{T_0, T_1, ..., T_n\}$, a designated cluster of time series, denoted as $C_i \subset \mathcal{D}$, corresponds to a discernible subgraph $\mathcal{G}_{C_i} \subset \mathcal{G}$. This subgraph is formally called a ``\textit{Graphoid}."


\begin{definition}[$Graphoid$]   
Let $\mathcal{D}$ be a time series dataset and $C_i \subset \mathcal{D}$ a given cluster such as $C_i = \{T_1,T_2,...,T_{k'}\}$. Let $\mathcal{G}$ be the graph embedding of $\mathcal{D}$ resulting from a function $\mathcal{M}$. We define the {\it Graphoid} of $C_i$ as:
{\small
\[
\mathcal{G}_{C_i} = \bigcup_{T \in C_i} \mathcal{M}(T)
\]
}
\label{defGraphoid}  
\end{definition}

In Definition~\ref{defGraphoid}, the {\it Graphoid} of a given cluster contains all the nodes and edges that at least one time series of that cluster crossed. Therefore, a node of $\mathcal{G}$ may belong to multiple graphoids. As a consequence, no distinction is made between nodes that contain subsequences of all time series of one cluster (i.e., {\bf representativity}) or time series subsequences of one cluster only (i.e., {\bf exclusivity}) and nodes that are contained in all clusters. We thus introduce the two following definitions:

\begin{definition}[Node representativity]   
Let $\mathcal{D}$ a time series dataset and $C = \{C_1,C_2,...,C_k\}$ a clustering partition. Let $\mathcal{G}=(\mathcal{N},\mathcal{E})$ be the graph embedding of $\mathcal{D}$ resulting from a function $\mathcal{M}$. We define the {\it Representativity} of node $N \in \mathcal{N}$ as $Re(N) = (|N|_{C_1}, ..., |N|_{C_k}$ with $|N|_{C_i}$ defined as:
{\small
\[
|N|_{C_i} = \frac{1}{{|C_i|}}\sum_{T \in C_i} 1_{[N \in \mathcal{M}(T)]}
\]
}
\label{defnodeRep}  
\end{definition}

\begin{definition}[Node Exclusivity]   
Let $\mathcal{D}$ a time series dataset and $C = \{C_1,C_2,...,C_k\}$ a clustering partition. Let $\mathcal{G}=(\mathcal{N},\mathcal{E})$ be the graph embedding of $\mathcal{D}$ resulting from a function $\mathcal{M}$. We define the {\it Exclusivity} of node $N \in \mathcal{N}$ as $Ex(N) = (Pr_{C_1}(N), ..., Pr_{C_k}(N))$ with $Pr_{C_i}(N)$ defined as:
{\small
\[
Pr_{C_i}(N) = \frac{|C_i||N|_{C_i}}{\sum_{T \in \mathcal{D}} 1_{[N \in \mathcal{M}(T)]}}
\]
}
\label{defnodeEx}  
\end{definition}

In other words, the {\bf representativity} of a node is the number of time series of a given cluster that crossed the node divided by the total number of time series within that cluster. 
The {\bf exclusivity} of a node is the number of time series of a given cluster that crossed the node divided by the total number of time series that crossed that same node. The same definitions can be used for edges. 
Based on the above definitions, we can restrict the definition of a {\it Graphoid} based on exclusivity and representativity. 
We thus introduce $\lambda$-$Graphoid$ and $\gamma$-$Graphoid$ defined as follows:

\begin{definition}[$\lambda$-$Graphoid$]   
For a given dataset $\mathcal{D}$, $\mathcal{G}$ the graph embedding of $\mathcal{D}$, and a given cluster $C_i$. The $\lambda$-$Graphoid$ of $C_i$ is defined as $\mathcal{G}^{\lambda}_{C_i} = (\mathcal{N}^{\lambda}_{C_i},\mathcal{E}^{\lambda}_{C_i})$ such as $\forall N \in \mathcal{N}^{\lambda}_{C_i}, \forall E \in \mathcal{E}^{\lambda}_{C_i}, Pr_{C_i}(N) \geq \lambda$ and $Pr_{C_i}(E) \geq \lambda$.
\label{deflambdaGraph}  
\end{definition}

\begin{definition}[$\gamma$-$Graphoid$]   
For a given dataset $\mathcal{D}$, $\mathcal{G}$ the graph embedding of $\mathcal{D}$, and a given cluster $C_i$. The $\gamma$-$Graphoid$ of $C_i$ is defined as $\mathcal{G}^{\gamma}_{C_i} = (\mathcal{N}^{\gamma}_{C_i},\mathcal{E}^{\gamma}_{C_i})$ such as $\forall N \in \mathcal{N}^{\gamma}_{C_i}, \forall E \in \mathcal{E}^{\gamma}_{C_i}, |N|_{C_i} \geq \gamma$ and $|E|_{C_i} \geq \gamma$.
\label{defgammaGraph}  
\end{definition}

The concepts introduced above can be better illustrated using Figure~\ref{fig:def}. The $\lambda$-$Graphoid$ and $\gamma$-$Graphoid$ are influenced by the chosen values of $\lambda$ and $\gamma$. As demonstrated in Figure~\ref{fig:def}, higher values of $\lambda$ and $\gamma$ lead to more restrictive graphoids. In the illustration, nodes $N^{(5)},N^{(6)},N^{(7)}$ are exclusively crossed by time series from cluster $C_1$, highlighting unique patterns specific to this cluster. On the other hand, node $N^{(3)}$ is traversed by all time series of $C_1$ and is considered the most representative pattern for this cluster. However, it also represents a common pattern for $C_0$.

This scenario emphasizes the trade-off between representativity and exclusivity. While $N^{(3)}$ provides a comprehensive representation of $C_1$, it lacks exclusivity to this cluster. In contrast, $N^{(5)}, N^{(6)}, N^{(7)}$ offer exclusive patterns for $C_1$ but might not be present in all time series of this cluster, limiting the interpretability. Thus, finding an optimal balance between higher values of $\lambda$ and $\gamma$ is crucial for maximizing the interpretability of a clustering partition through graph embedding.
Based on the above definitions, we can state the following:

\begin{lemma}
    For a given clustering partition $C = \{C_1,C_2,...,C_k\}$, if $\lambda \leq k$, then $\bigcup_{C_i \in C} \mathcal{G}^{\lambda}_{C_i} = \mathcal{G}$.
    if $\lambda > 0.5$, then $\bigcap_{C_i \in C} \mathcal{G}^{\lambda}_{C_i} = \emptyset$.
    \label{Lemma1} 
\end{lemma} 

\begin{lemma}
    For a given clustering partition $C = \{C_1,C_2,...,C_k\}$, if $\forall C_i \in C, \mathcal{G}^{\lambda=1}_{C_i} = \mathcal{G}^{\gamma=\frac{1}{|C_i|}}_{C_i}$, then $\bigcap_{C_i \in C} \mathcal{G}_{C_i} = \emptyset$
    \label{Lemma2} 
\end{lemma} 

Lemma~\ref{Lemma2} corresponds to the perfect partition, in which each graph region exclusively represents one cluster. 
However, we do not need to have all the nodes of a {\it Graphoid} exclusively representing only one cluster. 
It is sufficient to have one node for each cluster with $|N|_{C_i} = Pr_{C_i}(N) = 1$.  
Therefore, the problem we want to solve is the following.

\begin{problem}[Time Series Graph Clustering]
	Given a dataset $\mathcal{D}$, automatically construct the graph $\mathcal{G}(\mathcal{N},\mathcal{E})$, and compute a clustering partition $C = \{C_1,...,C_k\}$ of $\mathcal{D}$, such that:  
{\small
 \[
 \forall C_i \in C, |\mathcal{G}^{\lambda=1}_{C_i}  \cap \mathcal{G}^{\gamma=1}_{C_i} |> 0
 \]
 }
 \label{problem}
\end{problem}

As this problem is impossible to solve in some use cases, the objective is to find the largest possible values of $\lambda$ and $\gamma$, such that the condition in Problem~\ref{problem} holds and the values of $\lambda$ and $\gamma$ indicate the quality of the clustering interpretability. Table~\ref{SymbolTable} summarizes the symbols used in this paper.

\begin{table}[tb]
\centering
\begin{tabular}{c|c}
\hline
{\bf Symbol} & {\bf Description} \\
\hline
$T$									& a time series (of length $|T|$) \\
$\ell$ 								& subsequence length\\
$\mathcal{D}$ 						& a dataset of time series\\
$C_i$							    & a cluster of a clustering partition $C$\\
$\mathcal{L}$						& labels generated by clustering\\
$k$							        & number of clusters\\
$\mathcal{N},\mathcal{E}$			& set of nodes and edges\\
$d$			& degrees of a set of nodes\\
$\mathcal{G}$			            & directed graph\\
$\mathcal{M}$ 						& function that transform $T$ into $\mathcal{G}$ \\
$\mathcal{G}_{\ell}$ 				& graph embedding built with length $\ell$ \\
$F_{\mathcal{G},\ell}$ 						& feature matrix for graph $\mathcal{G}_\ell$ \\
$M_C$ 								& consensus matrix \\
$\mathcal{G}_{C_i}$,$\mathcal{G}^{\lambda}_{C_i},\mathcal{G}^{\gamma}_{C_i}$ 		& graphoid, $\lambda$-graphoid and $\gamma$-graphoid of $C_i$ \\
$|N|_{C_i}$, $Pr_{C_i}(N)$						& representativity, exclusivity of node $N$ for $C_i$\\
\hline
\end{tabular}
\caption{Table of symbols}
\label{SymbolTable}
\end{table}