\section{Related Work}
\subsection{Art Identification}

Art identification, particularly for identifying artistic authors, has a long history, initially relying on traditional image processing techniques. Early methods focused on handcrafted features, including color histograms, texture descriptors such as Gabor filters, and shape features like Histogram of Oriented Gradients (HOG) and Scale-Invariant Feature Transform (SIFT). These approaches were foundational in the field but have limitations when applied to complex artworks, where subtle variations and intricate stylistic elements are common.

For example, SIFT (Lowe, 2004) excels at identifying distinctive keypoints in images and is highly effective at capturing local texture features**Lowe, "Distinctive Image Features from Scale-Invariant Keypoints"**. However, SIFT struggles with handling images that contain complex or dynamic backgrounds, often producing suboptimal results in the presence of noise or clutter, which are typical in artistic works. Similarly, other traditional methods like HOG (Dalal \& Triggs, 2005) are focused on detecting object shapes and edges but fail to capture the nuance of artistic details, such as the subtle interplay of color gradients and fine brushstrokes.

The advent of deep learning techniques, particularly Convolutional Neural Networks (CNNs), has significantly advanced the field of art identification**Dosovitskiy et al., "ImageNet Large Scale Visual Recognition Challenge"**, **Simonyan \& Zisserman, "Very Deep Convolutional Networks for Large-Scale Image Recognition"**. CNNs, unlike traditional methods, can automatically learn hierarchical features from images, enabling them to capture complex patterns, textures, and relationships in a way that manual feature extraction cannot. In art identification, CNNs have shown significant promise. For instance, Dosovitskiy et al. (2016) proposed a method for classifying art styles using CNNs, achieving excellent results in style identification tasks. CNNs, such as VGG (Simonyan \& Zisserman, 2014)**He et al., "Deep Residual Learning for Image Recognition"**,**Dosovitskiy et al., "ImageNet Large Scale Visual Recognition Challenge"**, have demonstrated their ability to automatically learn features that are highly useful for distinguishing between art authors. However, these methods typically extract local features and lack the ability to model global information, which is essential for understanding complex artworks that contain multiple interconnected elements, colors, and textures.

Despite the effectiveness of CNNs in extracting local features, there is an increasing realization that global context is crucial for fully understanding artistic works. This limitation has led to a growing interest in incorporating additional models capable of capturing long-range dependencies and the global structure of images.

\subsection{Application of Transformer in Visual Tasks}

The Transformer model**Vaswani et al., "Attention Is All You Need"**, first introduced by Vaswani et al. (2017) for natural language processing (NLP), has since revolutionized the field of NLP and has begun to make its mark on computer vision. The key innovation of the Transformer model lies in its self-attention mechanism, which allows the model to capture long-range dependencies within the data, rather than relying on sequential processing as seen in traditional Recurrent Neural Networks (RNNs). This capability enables Transformers to model the global context of a sequence or image effectively.

Building on this success, the Vision Transformer (ViT) **Dosovitskiy et al., "An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale"**, was introduced as a model specifically designed for image processing tasks. ViT divides an image into fixed-size patches, which are then treated as tokens in a similar way to words in NLP tasks. These tokens are passed through multiple layers of self-attention to model the dependencies between them. This approach allows the Transformer to capture global relationships within the image, making it particularly effective for tasks like image classification and object detection, where understanding the context and relationships between distant regions in an image is crucial. ViT has shown that, when trained on large datasets, it outperforms CNN-based models, especially in tasks that require the understanding of broader patterns and the overall structure of the image.

Despite the success of Transformers in many image classification tasks, they have notable challenges. ViT models, for instance, typically require large datasets for effective training, as they do not perform well on small datasets compared to CNNs. This is primarily due to the large number of parameters in Transformers, which require extensive training data to generalize effectively. Additionally, Transformers, while excellent at capturing global information, often struggle with local feature extraction, which is crucial for tasks such as fine-grained art style identification.

\subsection{Related Research on CNN-Transformer Fusion}

Given the limitations of both CNNs and Transformers, researchers have begun to explore hybrid models that combine the strengths of both architectures. These fusion approaches aim to leverage CNNs for extracting local features and Transformers for modeling global dependencies, creating models that can handle both fine-grained details and large-scale contextual information.

One prominent approach is proposed by Xie et al., who introduced a hybrid model **Xie et al., "Spatial Fusion for Efficient Image Segmentation"** that first uses CNNs to extract low-level features from images and then passes these features to a Transformer for global modeling. This model has demonstrated good performance in various image classification tasks, including facial identification and general object classification. By combining CNNs' local feature extraction with Transformers' ability to capture long-range dependencies, the model achieves enhanced performance over using either architecture in isolation.

Liu et al. (2021) also explored the fusion of CNNs and Transformers, focusing on a multi-scale feature fusion approach. In this method, CNNs extract features at different scales, and these features are subsequently fused with the global features captured by the Transformer. This multi-scale approach is particularly useful when dealing with small datasets, as it allows the model to capture both local and global features from images at different resolutions. This fusion technique has shown significant improvements in small-data environments, making it highly relevant for art identification tasks, where high-quality labeled datasets may be scarce.

While the fusion of CNNs and Transformers has shown promising results in various applications, the integration of these models for the specific task of art author classification remains an area in need of further exploration. Existing research on hybrid models primarily focuses on generic image classification tasks, and there has been limited investigation into optimizing CNN-Transformer fusion architectures for the unique challenges posed by artwork identification, such as capturing fine brushstrokes, identifying complex color gradients, and distinguishing between different artistic authors. Therefore, further work is needed to refine the fusion strategies and adapt them to the specific nuances of art identification tasks.