% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\usepackage{subcaption}
\usepackage{mwe}

\usepackage{multirow}
\usepackage{microtype}
\usepackage{booktabs}

\usepackage{amsfonts}

\usepackage{float} % Add this in your preamble
\usepackage{amsmath}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{rotating}

\title{The Lookahead Limitation:\\ Why Multi-Operand Addition is Hard for LLMs}

\newcommand{\affilsup}[1]{\rlap{\textsuperscript{\normalfont#1}}}

\author{
    Tanja Baeumel\affilsup{1,2}
    \qquad
    Josef van Genabith\affilsup{1, 3}
    \qquad
    Simon Ostermann\affilsup{1,2}
    \\
    $^1$German Research Center for Artificial Intelligence (DFKI) \\
    $^2$Centre for European Research in Trusted AI (CERTAIN) \\
    $^3$Department of Language Science and Technology, Saarland University \\
    Saarland Informatics Campus, Saarbrücken, Germany\\
    \texttt{\{firstname.lastname\}@dfki.de} \\
}


\begin{document}
\maketitle
\begin{abstract}
Autoregressive large language models (LLMs) exhibit impressive performance across various tasks but struggle with simple arithmetic, such as additions of two or more operands. We show that this struggle arises from LLMs’ use of a \textbf{simple one-digit lookahead heuristic}, which works fairly well (but not perfect) for two-operand addition but fails in multi-operand cases, where the carry-over logic is more complex. Our probing experiments and digit-wise accuracy evaluation show that LLMs fail precisely where a one-digit lookahead is insufficient to account for cascading carries. We analyze the impact of tokenization strategies on arithmetic performance and show that all investigated models, regardless of tokenization, are inherently limited in the addition of multiple operands due to their reliance on a one-digit lookahead heuristic. Our findings reveal fundamental limitations that prevent LLMs from generalizing to more complex numerical reasoning.
\end{abstract}
\input{chapters/01_Introduction}
\input{chapters/02_Accuracy_multi-op}
\input{chapters/03_Probing_Digits}
\input{chapters/04_Carry_Heuristic}
\input{chapters/05_H1_Two_Operand}
\input{chapters/06_H1_Multi_Operand}
\input{chapters/07_H1_Llama}
\input{chapters/08_RelatedWork}
\input{chapters/09_Conclusion}

\section*{Limitations}
Our work highlights limited lookahead as a key challenge for LLMs when adding multiple numbers. However, it remains unclear whether this limitation extends to other arithmetic operations, such as subtraction. Additionally, we cannot determine whether the limited lookahead is a heuristic explicitly learned for arithmetic tasks, or if it could also affect general language generation tasks as thus hinder performance of other tasks that require long-range dependencies. Future work should explore the depth of lookahead in tasks beyond arithmetic.

While the lookahead heuristic offers a straightforward explanation for the upper performance limit of LLMs on addition, it does not fully account for why LLMs still somewhat underperform relative to the heuristic in addition tasks with many operands (e.g., adding 8–11 numbers). We suspect this discrepancy may be related to limited training exposure to these many-operand addition tasks, but further investigation is needed to confirm this.

Our work also does not address whether larger models within the same family (e.g., 70B parameter models) exhibit a deeper lookahead. Future studies should examine whether scaling model size leads to improved performance by enabling a deeper lookahead.

Finally, we do not tackle methods to overcome the shallow lookahead. Future work should investigate whether targeted training on tasks requiring deeper lookahead can encourage models to deepen their lookahead.

%\section*{Ethics Statement}

\section*{Acknowledgements}
We thank Patrick Schramowski for his helpful feedback on the paper draft. This work has been supported by the German Ministry of Education and Research (BMBF) as part of the project TRAILS (01IW24005).


% Entries for the entire Anthology, followed by custom entries
\bibliography{custom, custom1}
\bibliographystyle{acl_natbib}

\appendix
\input{Appendices/Appendix_A}
\input{Appendices/Appendix_B}
\input{Appendices/Appendix_C}
\input{Appendices/Appendix_E}
\input{Appendices/Appendix_D}
\input{Appendices/Appendix_F}
\input{Appendices/Appendix_G}

\end{document}
