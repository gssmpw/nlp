\section{Introduction}

Large language models (LLMs) demonstrate remarkable performance across a wide range of tasks \cite{bai2023qwen,gemmateam2024gemmaopenmodelsbased,guo2025deepseek}, yet consistently struggle with simple arithmetic tasks, such as the addition of multiple or large numbers \cite{mcleish2024transformers, shen2023positionaldescriptionmatterstransformers, zhou2023algorithmstransformerslearnstudy, zhou2024transformersachievelengthgeneralization}.

Figure \ref{fig:figure1} shows an example of an addition with 2 operands, $147$ and $255$, each with three digits ($0$ to $9$). The \textit{length} of an operand is the 
number of digits it contains. Figure \ref{fig:figure1} provides an example where the LLM 
fails (even in a two-operand case) to provide a correct output due to its insensitivity to a carry emerging from later computations.

The difficulty LLMs face in such tasks stems from the mismatch between the left-to-right nature of autoregressive language modeling and the right-to-left structure of standard arithmetic algorithms. Conventional addition methods process numbers digit by digit from right to left, propagating carries, while LLMs generate numbers sequentially from left to right without explicit intermediate calculations. This raises the question: What strategy do LLMs use to handle this misalignment in addition?

\begin{figure}[t]
    \centering
    \includegraphics[width=0.3\textwidth]{Images/figure1.png}
    \caption{An addition of two three-digit operands.
    LLMs rely on a one-digit lookahead when performing addition. If a relevant carry emerges at a later stage in prediction, they fail to account for it, leading to errors in earlier generated result digits.}
    \label{fig:figure1}
    \vspace{-0.5cm}
\end{figure}

In this work, we show that in fact LLMs rely on a simple heuristic that enables high (though not perfect) accuracy in adding two operands
(e.g., $147 + 291 = 438$, henceforth \textit{two-operand addition}). This heuristic attempts to bridge the gap between the left-to-right generation
and the resulting need to 'look ahead' to account for propagating carries from less significant digits. Rather than performing an exhaustive lookahead to fully anticipate carry propagation, \textbf{LLMs rely on a simple heuristic that involves a lookahead of only a single digit to anticipate the value of carries in addition}. 
We show that while this strategy works fairy well
for two-operand addition, due to relevant digit combinatorics, it deteriorates substantially with multiple operands
(e.g., in four-operand addition such as $147 + 245+ 312 + 104 = 808$, henceforth generalized as \textit{multi-operand addition} for any number of operands $>2$), where anticipating carries becomes less predictable. The reliance on the heuristic explains the lack of robustness in LLMs' arithmetic performance. 

Figure \ref{fig:figure1} illustrates this shortcoming of the heuristic: A one-digit lookahead anticipates no carry (because for the sum of the second, i.e. middle, digits in the operands $4+5=9$), leading to the inaccurate prediction of the first result digit as $3$, unable to accurately anticipate the cascading carry originating from the unit position. 

To gather evidence that the heuristic accurately describes the strategy used by LLMs to solve addition from left to right, we present results from three state-of-the-art LLMs with different tokenization strategies (single digit and multiple digit) for numerical outputs. 
By evaluating prediction accuracy on carefully curated datasets and employing probing techniques, 
we provide multiple lines of evidence that LLMs struggle specifically with addition tasks where a one-digit lookahead is insufficient to account for cascading carries.
For instance, in two-operand addition, we show that this issue occurs when the sum of the digits at the lookahead position is $9$, leading to failure in correctly predicting the numerical value at the current position.
For example, in $147 + 255 =$, no carry is predicted for the middle digits, even though a cascading carry from the $10^0$ position affects the sum of the $10^1$ digits, and thus the $10^2$ position.

Our findings show that all investigated LLMs are inherently limited in their performance on multi-operand addition tasks due to this heuristic, regardless of their tokenization strategy.

Our contributions are as follows: 
\begin{itemize} 
\item \textbf{Evaluation of Addition Capabilities}: We show that LLMs fail on multi-operand addition (Section \ref{sec:acc_data}) and then systematically evaluate the capabilities of LLMs on two-operand addition tasks via probing (Section \ref{sec:probing}).
\item \textbf{Heuristic Discovery}: Inspired by results of the evaluation, we
formalize left-to-right addition in LLMs for multi-operand addition with a simple heuristic that uses a shallow lookahead of one to attempt left-to-right addition (\textbf{H1}, Section \ref{subsec:digit10}).
\item \textbf{Empirical Validation}: We demonstrate that \textbf{H1} is fragile in multi-operand addition and explain the performance decline as a function of the increasing number of operands in large comprehensive addition experiments. We find that model performance aligns \textit{precisely} with the predicted limitations of \textbf{H1} (Sections \ref{sec:h1_2op} and \ref{sec:multi_fail}). We find that \textbf{H1} holds independently of tokenization strategies (Section \ref{sec:llama}).
\end{itemize}
