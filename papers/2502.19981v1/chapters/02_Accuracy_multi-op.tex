\section{LLMs Struggle with Multi-Operand Addition}
\label{sec:acc_data}

In this section, we define the data and models used in this work and demonstrate that LLMs fail on multi-operand additions by looking at prediction accuracy.

\subsection{Models and Data}
\label{sec:models_data}
\paragraph{Models.}
We compare Mistral-7B \cite{jiang_mistral_2023}, Gemma-7B \cite{gemmateam2024gemmaopenmodelsbased} and Meta-Llama-3-8B \cite{grattafiori2024llama3herdmodels, llama3modelcard} as they employ different tokenization strategies for numerical outputs: 
While Mistral and Gemma exclusively employ a single-digit tokenization strategy for their numeric input and generated output (e.g., input = ['1', '4', '7', '+', '2', '5', '5', '='], output = ['4', '0', '2']), Llama-3 employs a multi-digit numeric tokenization strategy (e.g., input = [' 147', ' +', ' 255', ' ='], output = [' 402']), typically favoring numeric tokens of length 3. 
\paragraph{Data.}
For all experiments in this paper, we compile a range of datasets containing simple arithmetic task prompts of the form \textit{147 + 255 = }. We create a dataset for each addition task ranging from 2-operand to 11-operand addition, where each operand is a triple-digit number between 100 and 899. Each of the 10 datasets contains 5,000 unique arithmetic problems, both in a zero-shot and one-shot setting. In the zero-shot setting, an example for a 2-operand addition prompt is ``147 + 255 = ''. An example for a 4-operand addition prompt is ``251 + 613 + 392 + 137 = ''.  Our one-shot prompt template follows the scheme \textit{q1 r1; q2 }, e.g.~``359 + 276 = 635; 147 + 255 = '', where \textit{q1} is a sample query from the same dataset and \textit{r1} is the correct result of the addition task in \textit{q1}. \textit{q2} is the query containing the addition task to be solved.

In the remainder of the paper, we use $s_n$ (with $n\geq 0$) to denote the result digit generated at digit position \(10^n\).
For example, in ``147 + 255 ='', with expected output 402, $s_2 = 4$, $s_1 = 0$, and $s_0 = 2$. 

\subsection{LLM Accuracy on Addition Tasks}
Figure \ref{fig:multi_op_accuracy_overall} illustrates the significant decline in performance of Mistral-7B \cite{jiang_mistral_2023}, Gemma-7B \cite{gemmateam2024gemmaopenmodelsbased} and Meta-Llama-3-8B \cite{llama3modelcard} in multi-operand addition as the number of operands increases. This drastic decrease highlights the inability of these models to generalize effectively to addition tasks involving a higher number of operands, despite their strong overall capabilities. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.42\textwidth]{Images/figure2.png}
    \caption{Accuracy of Mistral, Gemma and Llama-3 on multi-operand addition of triple-digit numbers, in a zero- and one-shot setting.}
    \label{fig:multi_op_accuracy_overall}
\end{figure}