\section{Related Work}
\label{sec:related-work}
Recent work has benchmarked the arithmetic capabilities of LLMs using text-based evaluations and handcrafted tests \cite{yuan2023well,lightman2023lets,NEURIPS2023_58168e8a,zhuang2023efficiently}. Numerous studies consistently show that LLMs struggle with arithmetic tasks \cite{nogueira2021investigatinglimitationstransformerssimple, qian2022limitationslanguagemodelsarithmetic, dziri2023faithfatelimitstransformers, yu2024metamathbootstrapmathematicalquestions}. 

\citet{zhou2023algorithmstransformerslearnstudy} and \citet{zhou2024transformersachievelengthgeneralization} examine transformers' ability to learn algorithmic procedures and find challenges in length generalization \cite{anil2022exploringlengthgeneralizationlarge}. Similarly, \citet{xiao2024theorylengthgeneralizationlearning} propose a theoretical explanation for LLMs' difficulties with length generalization in arithmetic. \citet{gambardella2024language} find that LLMs can reliably predict the first digit in multiplication but struggle with subsequent digits.

The focus of research has recently shifted from mere benchmarking of LLMs to trying to understand \textit{why} LLMs struggle with arithmetic reasoning. Using circuit analysis, \citet{stolfo_mechanistic_2023} and \citet{hanna2023doesgpt2computegreaterthan} explore internal processing in arithmetic tasks, while \citet{nikankin2024arithmetic} reveal that LLMs use a variety of heuristics managed by identifiable circuits and neurons. In contrast, \citet{deng2024language} argue that LLMs rely on symbolic pattern recognition rather than true numerical computation. Recently, \citet{kantamneni2025languagemodelsusetrigonometry} showed that LLMs represent numbers as generalized helixes and perform addition using a “Clock” algorithm \cite{nanda_progress_2023}.

Related work has also examined how LLMs encode numbers. \citet{levy2024language} demonstrate that numbers are represented digit-by-digit, extending \citet{gould2023successor}, who find that LLMs encode numeric values modulo 10. \citet{zhu-etal-2025-language} suggest that numbers are encoded linearly, while \citet{marjieh2025number} indicate that number representations can blend string-like and numerical forms.

Another line of research explores how tokenization influences arithmetic capabilities. \citet{lee2024digitstodecisions} show that single-digit tokenization outperforms other methods in simple arithmetic tasks. \citet{singh2024tokenization} highlight that right-to-left (R2L) tokenization—where tokens are right-aligned—improves arithmetic performance. 
Additionally, the role of embeddings and positional encodings is emphasized by \citet{mcleish2024transformers}, who demonstrate that suitable embeddings enable transformers to learn arithmetic, and by \citet{shen2023positionaldescriptionmatterstransformers}, who show that positional encoding improves arithmetic performance.
