\section{Multi-Digit Tokenization Models Employ the Same Heuristic}
\label{sec:llama}
While \citet{levy2024language} demonstrate that all LLMs, regardless of the tokenization strategy, internally represent numbers as individual digits, it remained unclear whether models with multi-digit tokenization also rely on a one-digit lookahead when generating addition results. In this section, we show that perhaps surprisingly multi-digit tokenization models, such as Llama-3, also employ a lookahead of one \textbf{digit} when predicting carry bits. 
To show this, we design 3 controlled datasets that force the multi-digit tokenization model Llama-3 to generate results across multiple tokens. 

\paragraph{Experimental Setup.}
To examine whether Llama-3 employs a one-digit lookahead, we use six-digit numbers in two-operand addition (e.g., ``231234 + 124514 = ''), where each operand is tokenized into two three-digit tokens by the model's tokenizer, such as: [`` 231'',`` 234'', `` +'', `` 124'', `` 514'', `` =''] and the result is generated as two triple-digit tokens as well, in this example [`` 355'', `` 748'']. The first generated triple-digit token $s_5 s_4 s_3$ corresponds to digit base positions $10^5$, $10^4$, and $10^3$. If Llama-3 did employ \textbf{H1} it would look ahead to digit position $10^2$, but ignore digit positions $10^1$ and $10^0$, as they fall outside the lookahead window.

\paragraph{Carry Scenarios.}
We evaluate model behavior in three datasets with six-digit operands (ranging from 100,000 to 899,999) and results between 200,000 and 999,999. We use a zero-shot prompt template. Each dataset consist of 100 samples:
\begin{itemize}
    \setlength{\itemsep}{0.1pt}
    \setlength{\parskip}{0pt}
    \setlength{\parsep}{0.5pt}
    \item \textbf{DS6: No carry.} The addition does not produce any carry and no digits sum to 9.  (e.g., \(111,234 + 111,514 = 222,748\)).
    \item \textbf{DS7: Direct carry in position \(10^2\).} A carry is generated at \(10^2\) and directly affects \(10^3\) (e.g., \(111,721 + 111,435 = 223,156\)). 
    \item \textbf{DS8: Cascading carry from \(10^1\) to \(10^3\).} A carry originates at \(10^1\), cascades to \(10^2\) and then affects \(10^3\) (e.g., \(111,382 + 111,634 = 223,016\)).
\end{itemize}

\paragraph{Expected Outcomes.}
If Llama-3 employs \textbf{H1}, we expect that DS6 should be easy, as no carry propagation is required. DS7 should also be easy, since the carry affecting \(10^3\) is within the one-digit lookahead window. DS8 in contrast should be challenging, as the carry originates from \(10^1\), from beyond the modelâ€™s lookahead range. We expect a lower accuracy in generating \(10^3\), the result digit that is affected by the potentially inaccurate carry.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.4\textwidth]{Images/figure8.png} 
    \caption{Per-digit generation accuracy of Llama on datasets DS6-DS8. Each dataset represents a different carry scenario.} 
    \label{fig:llama_carry_scenarios}
\end{figure}

\paragraph{Results.}
Figure \ref{fig:llama_carry_scenarios} shows that Llama-3 exhibits the expected pattern predicted by \textbf{H1}. The sharp drop in accuracy in dataset DS8 on digit \(10^3\) provides evidence that Llama-3, regardless of its multi-digit tokenization strategy, relies on the same one-digit lookahead for solving addition left to right. 