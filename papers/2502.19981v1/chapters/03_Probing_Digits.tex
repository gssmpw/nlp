\section{Probing LLMs on Digits in Two-Operand Addition Tasks}
\label{sec:probing}

Solving arithmetic tasks presents a fundamental challenge for LLMs, as they generate text from left to right, while addition requires a right-to-left process due to carry propagation from the least significant to the most significant digit.
For instance, predicting the first result digit $s_2 = 4$ in ``147 + 255 = '' requires the model to anticipate that a carry originating from $s_0$ cascades through $s_1$ to $s_2$. Robust left-to-right addition thus requires a lookahead spanning all result digits, raising the question: Do LLMs internally represent future result digits when predicting $s_2$ - and if so, how far can they ``look into the future''?

To answer this question, we probe whether models accurately encode future result digits $s_1$ or $s_0$ while generating $s_2$. Building on \citet{levy2024language}, who show that, irrespective of a model's numeric tokenization strategy, LLMs internally represent numbers digit-by-digit in base 10, we analyze digit-wise probing accuracy on the two-operand addition dataset described in Section \ref{sec:models_data}.

\subsection{Methodology and Experiments} 
\paragraph{Data.}
We split the two-operand addition dataset (see Section \ref{sec:models_data}) into train (n=4500) and test (n=500) for the probing experiments. The two-operand addition dataset is designed such that correct results for the addition tasks are triple-digit numbers between 200 and 999. We use the zero-shot prompt setting for the probing experiment.

\paragraph{Probing Setup.}
Our goal is to determine which result digits are available at the prediction step of $s_2$. We thus train probes to predict the result digits $s_2$, $s_1$, and $s_0$ from hidden states of the model during the prediction step of $s_2$. 

Specifically, we train one-layer linear probes to predict individual digit values of the results from the hidden state of the last token at each model layer.  Probes are trained on the train split of the two-operand addition dataset and evaluated on the test split. 
We train separate probes to predict individual result digits $s_2$, $s_1$, and $s_0$, for all models at all layers.\footnote{We choose a low temperature of 0.1 during model inference to ensure deterministic and consistent outputs, reducing randomness in token generation and improving the reliability of numerical calculations.}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{Images/figure3.png}
    \caption{Probing accuracy of individual result digits as predicted by the hidden states of Mistral, Gemma and Llama-3. For two-operand, zero-shot addition prompts.}
    \label{fig:probing_multi_op_accuracy_overall}
\end{figure}

\subsection{Results}
\label{subsec:results}
The probing accuracy of individual result digits is shown in Figure \ref{fig:probing_multi_op_accuracy_overall}. Gemma and Mistral with their digit-wise tokenization internally represent only $s_2$ with high accuracy. In contrast, there is a high probing accuracy across \textit{all} result digits in Llama-3. This is due to the fact that Llama-3 tokenizes numbers into 3-digit numeric tokens: It is forced by its tokenization to generate all result digits ($s_2$, $s_1$, and $s_0$) in one step as a single token.

The single-digit tokenization models Mistral and Gemma exhibit a low probing accuracy on $s_0$ ($< 0.24$) in all layers.
Recall that $s_0$ is probed from the models' hidden states while they autoregressively generate $s_2$. 
We interpret the lack of internal representation of $s_0$ as evidence that these models disregard the potential influence of $s_0$ (including any cascading carry) when generating $s_2$.

In line with this, Gemma and Mistral show notably higher probing accuracy on $s_1$ compared to $s_0$, when probing from the models' hidden states as they generate $s_2$. 
We thus conjecture that the single-digit-token models seem to recognize the potential influence of the carry resulting from the sum of the \(10^1\) operand digits. Simply put, generating the digit at $10^2$ might employ a lookahead of one digit to the \(10^1\) intermediate result. 
Based on this observation, we formulate a hypothesis for a heuristic used by LLMs:
\begin{center}
    \textbf{H1: \indent LLMs employ a look ahead of one digit to generate the current digit of an addition task.}
\end{center}

\textbf{H1} would explain why LLMs cannot effectively represent each necessary digit of the result during generation, making it difficult to anticipate later carry values correctly. We first formalize \textbf{H1}, which explains the patterns observed in Figure \ref{fig:probing_multi_op_accuracy_overall}, in the next Section, and then verify the fit of \textbf{H1} with empirical addition outcomes generated by the models in Sections \ref{sec:h1_2op}, \ref{sec:multi_fail}, and \ref{sec:llama}.
