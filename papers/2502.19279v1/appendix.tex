\section{Hyperparameters}
\label{sec:hyperparameter}

\subsection{Hyperparameters for CritiQ Flow}

We have manually tried different sets of hyperparameters and the chosen
hyperparameters for the final experiments are shown in Table~\ref{tab:hyperparameter}.

\begin{table}[htbp]
    \centering
    \begin{tabular}{lccc}
        \toprule            & \textbf{Code} & \textbf{Math} & \textbf{Logic} \\
        \midrule \#Criteria & 20            & 20            & 20             \\
        \#Iterations        & 3             & 5             & 3              \\
        Retrieval Threshold & 0.5           & 0.5           & 0.5            \\
        High Threshold      & 0.9           & 0.8           & 0.8            \\
        Low Threshold       & 0.8           & 0.7           & 0.7            \\
        Final Threshold     & 0.9           & 0.7           & 0.8            \\
        \bottomrule
    \end{tabular}
    \caption{Hyperparameters for CritiQ Flow.}
    \label{tab:hyperparameter}
\end{table}

\subsection{Hyperparameters for CritiQ Scorer}

We use the \texttt{trl}~\citep{vonwerra2022trl} library to train CritiQ Scorers. On the 3 domains, we train each CritiQ Scorer using AdamW~\citep{loshchilov_decoupled_2019}
optimizer with learning rate $2 \times 10^{-5}$ and weight decay $0.01$ for $4$
epochs. The learning rate warmups in the first $20\%$ training steps and cosine decay
in the rest steps. We truncate the text longer than $32,768$ tokens. The global training
batch size is $128$. We randomly select $5\%$ from the CritiQ Scorer training
set $D_{\text{agent}}$ as the validation set, and use the rest to train the scoring
model. We save the model every $50$ training steps and select the checkpoint
with the best validation accuracy as the final CritiQ Scorer.

\subsection{Hyperparameters for Continual Pretraining}

We use AdamW~\citep{loshchilov_decoupled_2019} optimizer with the maximum
learning rate $1\times 10^{-4}$, the minimal learning rate $1\times 10^{-5}$,
and weight decay $0.01$ for $4$ epochs. The learning rate increases in the first
$5\%$ training steps, and cosine decays in the rest steps. The training sequence
length is $8192$ and global batch size is $4M$ tokens. Each model is trained on
$32$ NVIDIA H800 GPUs.

\section{Annotation}
\label{sec:annotation}

\subsection{Annotators}

Our annotation team consists of three annotators for each domain (code, math, and
logic). The annotators are paper authors who meet the following qualifications:

\begin{itemize}
    \item Hold bachelor's or master's degrees

    \item Have multiple years of professional programming experience

    \item Possess foundational mathematical knowledge

    \item Demonstrate competency in logical reasoning
\end{itemize}

The annotators volunteered their time without additional compensation. As authors
of the paper, they had a vested interest in producing high-quality annotations, since
the annotation results directly impacted the experimental outcomes and overall research
quality.

\subsection{Annotation Guidelines}

\subsubsection{Annotation Guidelines for Code}

Please compare the two Python Code files and choose the one of higher quality.

Low-quality code often has the following characteristics:

\begin{itemize}
    \item The code is badly formatted or has syntax errors.

    \item The code consists solely of comments or package imports, which is non-informative.

    \item The code only consists of simple class or function definitions, which
        is hard to understand without other files.

    \item The code just defines meaningless variables while do not perform any operations.

    \item The code is too simple.

    \item The code contains too much hard-coded data or is a configuration or an
        entrypoint file to a larger project, which is not helpful in learning
        programming.
\end{itemize}

High-quality code often has the following characteristics:

\begin{itemize}
    \item The code is educational for code starters, which shows basic
        programming principles, design patterns, or data structures.

    \item The code is a solution to an algorithm problem, which is beneficial
        for learning algorithm.

    \item The code is well-structured with proper code comments, which leads to
        high readability and maintainability.

    \item The code shows clear purpose and can accurately solve certain kind of problems,
        while keeps extensible and flexible.

    \item The code has self-contained classes or functions that can be understood
        without other files, which shows high simplicity and reusability.
\end{itemize}

Choose the better one of A and B according to the above guidelines and your preferences
for code quality. If the two files are of similar level, answer C.

\subsubsection{Annotation Guidelines for Math}

Please compare the two text data related to math and choose the one of higher
quality.

High-quality math data show significant mathematical intelligence and is
educational for math learners. Mathematical quality can be evaluated based on several
key aspects:

(1) Logical Structure: Content should demonstrate clear reasoning with properly
structured arguments, proofs and deductions, avoiding inconsistencies or
unjustified assumptions;

(2) Mathematical Rigor: Expressions should use precise and consistent notation,
terminology and symbols throughout, with all necessary steps clearly stated;

(3) Pedagogical Value: The content should be build systematically from
fundamentals to advanced ideas, including instructive examples that reinforce understanding;

(4) Conceptual Depth: Material should go beyond elementary arithmetic to explore
deeper mathematical concepts and problem-solving techniques, showing connections
between different ideas;

(5) Technical Accuracy: Content should be free of mathematical errors, misconceptions,
ambiguous notation, or incorrect terminology that could impede understanding.

High-quality mathematical content will excel in these areas while maintaining accessibility,
whereas lower-quality content may be lacking in one or more of these essential
aspects.

Choose the better one of A and B according to the above guidelines and your
preferences for mathematical quality. If the two texts are of similar level,
answer C.

\subsubsection{Annotation Guidelines for Logic}

Compare the following two texts, determine which one better requires and
promotes logical thinking by evaluating these three essential criteria:

1. Does understanding later content require careful reasoning from previous
information?

- Positive: Text that builds logical arguments progressively.

- Negative: Text that can be understood superficially without deeper thinking
Contextual Integration.

2. Does comprehension require connecting multiple pieces of evidence or ideas?

- Positive: Text with interconnected logical elements.

- Negative: Simple chronological narratives or disconnected descriptions
Structured Interpretation.

3. Can the content be understood through clear rational analysis?

- Positive: Text with well-defined logical relationships.

- Negative: Ambiguous literary expressions with multiple subjective
interpretations.

Choose the better one of A and B according to the above guidelines and your
preferences for logical quality. If the two texts are of similar level, answer C.

\section{Prompts Generated by TextGrad}
\label{sec:textgrad}

We show the prompts generated by TextGrad for the three domains in Section~\ref{sec:appendix_prompts}.
The quality criteria are in bold.

\begin{tcolorbox}
    [title = {Code}, breakable] \footnotesize \#\# Task Instruction$\backslash$nYou
    are tasked with performing a comprehensive comparison of the quality and structure
    of two Python code files. Evaluate them based on \textbf{readability, efficiency,
    adherence to Python coding standards (PEP 8), and maintainability}.
    Highlight strengths and weaknesses for each file and suggest specific
    improvements where necessary. $\backslash$n$\backslash$n\#\# Code File A$\backslash$n\{A\}$\backslash$n$\backslash$n\#\#
    Code File B$\backslash$n\{B\}
\end{tcolorbox}

\begin{tcolorbox}
    [title = {Math}, breakable] \footnotesize \#\# Compare the Mathematical Quality
    of Two Solutions$\backslash$nPlease evaluate the mathematical quality of the
    two provided solutions. Consider factors such as \textbf{correctness, clarity,
    logical reasoning, and mathematical rigor} in your assessment. Once you have
    thoroughly reviewed both solutions, choose "A" or "B" to identify the solution
    that exhibits superior mathematical quality.$\backslash$n$\backslash$n[A]$\backslash$n\{A\}$\backslash$n$\backslash$n[B]$\backslash$n\{B\$\}
\end{tcolorbox}

\begin{tcolorbox}
    [title = {Logic}, breakable] \footnotesize Assess the logical consistency
    between the two text pieces provided below. Identify which text is more \textbf{logically
    consistent}$\backslash$n of A, B, or if they are equally consistent. Clearly
    explain your reasoning behind the evaluation.$\backslash$n$\backslash$n[A]$\backslash$n\{A\}$\backslash$n[/A]$\backslash$n$\backslash$n[B]$\backslash$n\{B\}$\backslash$n[/B]
\end{tcolorbox}

\section{Responsible NLP Research Statements}

We used generative AI to assist in this work. We used GitHub Copilot for short-form
input assistance when writing the code. We used ChatGPT and Claude for
paraphrasing and polishing the original content in the paper.

The datasets used in this work are publicly accessible. The usage of the Stack v2
is under Terms of Use for The Stack v2~\footnote{\url{https://huggingface.co/datasets/bigcode/the-stack-v2}}.
The usage of OpenWebMath is under ODC-By 1.0 license~\footnote{\url{https://opendatacommons.org/licenses/by/1-0/}}
and the CommonCrawl ToU~\footnote{\url{https://commoncrawl.org/terms-of-use/}}. The
usage of Zyda-2 is under the terms of Open Data Commons License~\footnote{\url{https://opendatacommons.org/licenses/by/1-0/}}.

We used \texttt{gpt-4o} for the experiments, which is under OpenAI's Terms of Use~\footnote{\url{https://openai.com/policies/terms-of-use/}}.
We used \texttt{Qwen2.5-72B-Instruct}, whose weight is distributed under Qwen
LICENSE AGREEMENT~\footnote{\url{https://huggingface.co/Qwen/Qwen2.5-72B-Instruct/blob/main/LICENSE}}.
We trained \texttt{Llama-3.1} respect to LLAMA 3.1 COMMUNITY LICENSE AGREEMENT~\footnote{\url{https://www.llama.com/llama3_1/license/}}.

\section{Prompts}
\label{sec:appendix_prompts}

\subsection{Prompts for Knowledge Base}

\begin{tcolorbox}
    [title = {Judge if a paper releases a dataset.}, breakable] \footnotesize
    There is a research paper about artificial intelligence.$\backslash$n$\backslash$nTitle:
    <TITLE>$\backslash$nAbstract: <ABSTRACT>$\backslash$n$\backslash$nInstruction:
    Does this paper propose a dataset? Return your answer in the following format:$\backslash$n$\backslash$n```
    json \{ "analysis": "Your analysis. For example, the main contribution of the
    paper.", "dataset": "The name of the dataset if it is proposed. Otherwise,
    answer 'N/A'.", "answer": "Yes/No/Unsure" \} ```
\end{tcolorbox}

\begin{tcolorbox}
    [title = {Extract quality criteria from papers.}, breakable] \footnotesize
    There is a research paper about artificial intelligence which proposed a new
    dataset named <DATASET\_NAME>.$\backslash$n$\backslash$n[BEGIN\_OF\_PAPER]$\backslash$n
    <PAPER\_CONTENT>$\backslash$n[END\_OF\_PAPER]$\backslash$n$\backslash$nI want
    to learn how to distinguish between data of high and low quality from the process
    of constructing the <DATASET\_NAME> dataset. Please conclude the criteria
    for determining data quality from the paper.$\backslash$n$\backslash$n- The
    criteria should be able to used to filter the data for the dataset.$\backslash$n
    - The criteria should be general enough to be applied to other datasets.$\backslash$n
    - If the paper proposed a data processing method, you should describe the criteria
    for the processed data which may be of higher quality.$\backslash$n - You
    should not just copy the criteria from the paper, but summarize them in your
    own words.$\backslash$n$\backslash$n```json \{ "name\_of\_the\_criterion": "description\_of\_the\_criterion",
    "name\_of\_another\_criterion": "description\_of\_another\_criterion", ...
    \} $\backslash$n$\backslash$nThe names of criteria should be a descriptive word.
    The descriptions should show what the criteria is about and how it can be
    used to determine if a data record should be included in the dataset. ```
\end{tcolorbox}

\begin{tcolorbox}
    [title = {Retrieve Code Criteria},breakable] \footnotesize \# Instruction$\backslash$nIs
    this criterion applicable for evaluating the quality of Python code?$\backslash$n$\backslash$n\#
    Criterion$\backslash$n<CRITERION>: <DESCRIPTION>$\backslash$n$\backslash$nYou
    should simply reply 'yes' or 'no'.
\end{tcolorbox}

\begin{tcolorbox}
    [title = {Retrieve Math Criteria},breakable] \footnotesize Is the following criterion
    applicable to measure the mathematical quality of text data?$\backslash$n$\backslash$n\#\#\#
    Criterion$\backslash$n*<CRITERION>*: <DESCRIPTION>$\backslash$n$\backslash$nYou
    should simply reply 'yes' or 'no'.
\end{tcolorbox}

\begin{tcolorbox}
    [title = {Retrieve Logic Criteria},breakable] \footnotesize \# Instruction$\backslash$nIs
    the following criterion applicable to evaluate the logical quality of text data?$\backslash$n$\backslash$n\#
    Criterion$\backslash$n<CRITERION>: <DESCRIPTION>$\backslash$n$\backslash$nYou
    should simply reply 'yes' or 'no'.
\end{tcolorbox}

\subsection{Domain Specific Prompts for Worker Agents}

\begin{tcolorbox}
    [title = {Pairwise Judgment for Code},breakable] \footnotesize \#\#
    Instruction$\backslash$nGiven criterion **{criterion}**, compare two Python code
    files and determine which one human annotators will consider to be of higher
    quality.$\backslash$n$\backslash$n\#\# A$\backslash$n\{A\}$\backslash$n$\backslash$n\#\#
    B$\backslash$n\{B\}$\backslash$n$\backslash$n\# Criterion$\backslash$n**\{criterion\}**:
    \{description\}
\end{tcolorbox}

\begin{tcolorbox}
    [title = {Pairwise Judgment for Math},breakable] \footnotesize \#\#
    Instruction$\backslash$nGiven criterion **\{criterion\}**, evaluate and determine
    which of the two text data is of higher quality in mathematics.$\backslash$n$\backslash$n[DATA\_A]$\backslash$n \{A\}$\backslash$n [/DATA\_A]$\backslash$n$\backslash$n[DATA\_B] $\backslash$n \{B\}$\backslash$n [/DATA\_B]$\backslash$n$\backslash$n\#
    Criterion$\backslash$n**\{criterion\}**: \{description\}
\end{tcolorbox}

\begin{tcolorbox}
    [title = {Pairwise Judgment for Logic},breakable] \footnotesize Which text piece
    of A and B is more logical based on **\{criterion\}**?$\backslash$n$\backslash$n\{criterion\}:
    \{description\}$\backslash$n$\backslash$n[A]$\backslash$n\{A\}$\backslash$n[/A]$\backslash$n$\backslash$n[B]$\backslash$n\{B\}$\backslash$n[/B]
\end{tcolorbox}

\subsection{Domain Specific Prompts for Manager Agents}

\begin{tcolorbox}
    [title = {Generate Initial Code Criteria},breakable] \footnotesize List and describe
    20 criteria on how human compare the overall quality of two Python code files.
\end{tcolorbox}

\begin{tcolorbox}
    [title = {Generate Initial Math Criteria},breakable] \footnotesize List and describe
    20 criteria on evaluating whether a text data is high quality math data.
\end{tcolorbox}

\begin{tcolorbox}
    [title = {Generate Initial Logic Criteria},breakable] \footnotesize List and
    describe 20 criteria to tell which is more logical of two text pieces.
\end{tcolorbox}

\subsection{General Prompts for CritiQ Flow}

The full prompts of CritiQ Flow are complex. We simply list the source code here.
Details can be checked in our released CritiQ software.

\lstinputlisting{prompts.py}

\section{Examples for Criteria Refinement}
\label{sec:appendix_ex_refinement}
\subsection{Generated Criterion}
\begin{tcolorbox}
    [title = {Criterion \textbf{algorithm\_efficiency}},breakable] Before refinement
    \tcblower \footnotesize This criterion assesses the efficiency of the algorithm
    implemented in the code. It measures time complexity, space complexity, and
    overall performance under different input sizes. By comparing how well the code
    scales and performs, evaluators can determine which implementation is
    superior in terms of resource optimization. It is particularly applicable to
    competitive programming or performance-critical applications.
\end{tcolorbox}
\begin{tcolorbox}
    [title = {Criterion \textbf{algorithm\_efficiency}},breakable] After refinement
    \tcblower \footnotesize The 'algorithm\_efficiency' criterion evaluates the computational
    performance of algorithms implemented within the code, focusing on
    measurable metrics such as time complexity, space complexity, and overall
    scalability. It assesses how well the code optimizes operations under varying
    input sizes and work complexity. This criterion is specifically applicable
    when the code implements clear and testable algorithms designed to solve
    computational or algorithmic tasks (e.g., sorting, searching, graph traversal).
    It is not concerned with the broader functionality, structure, or aesthetics
    of the code unless they directly impact efficiency. \par How to evaluate: Workers
    should focus on examining the implementation of algorithms for efficiency improvements
    based on established computational principles. Consider aspects like: 1. Does
    the algorithm exhibit better performance by reducing computational steps (time
    complexity)? 2. Does it use fewer resources, such as memory, to achieve its
    output (space complexity)? 3. How does the algorithm perform under different
    input sizes (scalability)? 4. Are there opportunities to optimize the
    algorithm further (e.g., using better data structures or reducing redundant operations)?\par
    When it is applicable: This criterion applies when both code files contain algorithms
    or computational content that can be directly analyzed for efficiency in terms
    of performance under realistic conditions. For example, files implementing tasks
    like sorting, searching, mathematical computations, or data processing
    should be evaluated under this criterion. \par When it is inapplicable: This
    criterion should be marked 'Not Applicable' when: 1. Neither file contains algorithmic
    or computational content (e.g., configuration files, import statements, or
    static setup code). 2. The files are of vastly different purposes or
    functionalities, making a direct comparison of algorithmic efficiency irrelevant.
    3. The code involves orchestration logic (e.g., API calls) rather than
    computational tasks. 4. The algorithms are incomplete, trivial, or lack sufficient
    context for meaningful evaluation (e.g., placeholders, stubs, or boilerplate
    code).\par Additional information: Workers should avoid judging efficiency
    based on perceived complexity or verbosity. Simpler code that efficiently achieves
    the desired result should be favored over unnecessarily complex solutions.
    Ensure all evaluations consider the problem the algorithm is designed to
    solve in its context. To assist with clarity, workers should consider
    explicitly marking the criterion inapplicable when one or both files fail to
    meet the stated requirements for algorithm efficiency evaluation. Clear examples:
    - Applicable: Comparing two sorting algorithms for time and space complexity.
    - Inapplicable: Comparing a configuration file to a data processing algorithm.
    - Inapplicable: Comparing an I/O-bound script with CPU-intensive code.\par This
    improved description ensures that workers consider the scope and context of 'algorithm\_efficiency'
    before making decisions, reducing ambiguity and inaccuracies.
\end{tcolorbox}

\subsection{Retrieved Criterion}
\begin{tcolorbox}
    [title = {Criterion \textbf{error\_analysis}},breakable] Before refinement \tcblower
    \footnotesize A thorough analysis of the reasoning flaws must be present.
    This includes understanding the causes of errors and what correct reasoning
    should have been applied at that point.
\end{tcolorbox}
\begin{tcolorbox}
    [title = {Criterion \textbf{error\_analysis}},breakable] After refinement \tcblower
    \footnotesize Error analysis evaluates the quality and depth of reasoning related
    to identifying, diagnosing, handling, and mitigating potential or actual
    errors within the code. This includes examining error-handling mechanisms such
    as exception blocks, validation checks, logging, or any other explicit
    strategies to anticipate and address errors. Additionally, it considers the
    code's explanation or reasoning about errors, focusing on detail and thoroughness
    in addressing potential edge cases or failure points. To evaluate error
    analysis, workers should consider the following steps: (1) Identify the
    presence of error-handling logic or mechanisms in the code (e.g., try-except
    blocks, assertions, logging); (2) Assess whether the provided error-handling
    logic is appropriate for the scope and context of the code; (3) Pay
    attention to any accompanying comments or documentation explaining the
    approach to mitigating errors; and (4) Evaluate whether patterns of reasoning
    about errors are logical and well-structured, including how edge cases are
    anticipated.\par This criterion is applicable only to code that contains
    logical processes, algorithms, or decision-making components where errors
    are likely to occur and need to be reasoned about or handled. It should be marked
    inapplicable for code that lacks relevant error-handling context, such as
    configuration files, boilerplate code, or import-only scripts. In cases where
    both pieces of code lack any mention or handling of errors, the criterion
    should also be deemed inapplicable, and no preference should be made.\par Key
    aspects to avoid include judging the code based on its overall complexity,
    functionality, or modularity unless they directly affect error analysis.
    Highlighting superficial error handling or assuming error-free code does not
    inherently satisfy this criterion. Workers should focus on explicit reasoning
    about errors and how the code mitigates or avoids potential failures.
    Concrete examples of good error analysis include thorough exception handling
    with explanations, detailed error logging, validations targeting specific
    failure scenarios, and robust test cases explicitly aimed at uncovering edge
    cases or logical flaws.
\end{tcolorbox}

\subsection{Refined Criterion}
\begin{tcolorbox}
    [title = {Criterion \textbf{commented\_context}},breakable] Before refinement
    \tcblower \footnotesize The 'commented\_context' criterion evaluates the presence,
    relevance, and quality of comments or documentation within a code file,
    ensuring they enhance understanding of the code's purpose, functionality,
    and any non-obvious logic. Comments should provide meaningful insights about
    the code's intent, clarify complex or non-intuitive sections, and offer context,
    such as explaining critical operations or unusual design decisions. This criterion
    does not favor the mere presence of comments or their verbosity but instead focuses
    on their necessity and utility in aiding comprehension. \par Approach for
    evaluation: Workers should assess whether comments are directly relevant to specific
    parts of the code and whether they provide significant contextual value to understanding
    its intent and usage. For instance, comments explaining business logic,
    algorithmic choices, or intricate areas of code are highly valuable.
    Irrelevant, redundant, or excessively verbose comments that do not add
    clarity should not be positively weighted. Self-documenting code, where the
    use of clear variable/function names and logical structure makes it
    inherently understandable, should not be penalized for a lack of comments.
    \par Applicability: This criterion is most relevant when comparing code that
    requires additional explanation due to complexity or specialized logic. It
    is less applicable or should be marked inapplicable when both files contain
    minimal or no comments, but their code is simple and self-explanatory.
    Examples include boilerplate files, import-only files, or scripts so
    straightforward that no additional context is needed. \par Additional considerations:
    Workers should not rely on style or verbosity as sole indicators of quality.
    Comments that are overly generic (e.g., 'This is a for loop') or unrelated (e.g.,
    boilerplate licensing information) should not factor into the evaluation.
    When both files feature sufficient documentation for their respective levels
    of complexity, preference should be given to concise, context-rich comments
    over verbose or unnecessary ones. If both files lack meaningful comments and
    are equally understandable without additional documentation, this criterion
    may not provide a basis for comparison.
\end{tcolorbox}
\begin{tcolorbox}
    [title = {Criterion \textbf{commented\_context}},breakable] After refinement
    \tcblower \footnotesize The 'commented\_context' criterion evaluates the presence,
    relevance, and necessity of comments or documentation within a code file.
    Comments should meaningfully enhance understanding by providing critical
    context, explaining complex logic, or clarifying non-obvious design
    decisions. The value of comments should be judged by their ability to aid comprehension,
    rather than their quantity or verbosity. High-quality comments are concise,
    appropriately placed, and directly related to the code's purpose and functionality.
    For example, comments explaining intricate algorithms, decision-making
    processes, or domain-specific details are valuable, whereas redundant, trivial,
    or boilerplate comments (e.g., licensing headers, generic statements like 'this
    is a loop') are not.\par Evaluation Steps: 1. Assess whether the file
    contains comments, and if present, determine whether they address essential
    aspects of the code's logic, design, or purpose. 2. Focus on relevance:
    Identify whether the comments clarify concepts that are not immediately
    understandable from the code structure itself. 3. Consider necessity:
    Evaluate if the complexity of the code requires additional explanation, or if
    the code is inherently self-explanatory (e.g., simple utility scripts or
    well-named variables/functions). 4. Judge quality: Favor concise, meaningful
    comments over verbose, generic, or redundant ones. 5. Evaluate whether comments
    contribute to maintainability by providing future developers with clear insights
    into the code's intent or potential edge cases.\par Applicability: - This
    criterion is applicable when the code includes non-obvious logic, intricate design,
    or contextual details that are essential for understanding. For example, it
    applies to files with algorithms, configuration settings, or any code where
    additional clarification adds significant value. - It is not applicable for files
    containing minimal or self-explanatory code, such as import statements, trivial
    scripts, or boilerplate content, where comments are unnecessary. - When
    comparing two files, if both lack comments but are sufficiently self-documenting,
    this criterion should be marked as inapplicable rather than favoring one
    file over the other based on the absence of comments.\par Additional Notes: -
    Avoid penalizing files that are simple and naturally clear without requiring
    comments. Instead, prioritize whether the comments add actual value relative
    to the code’s complexity. - Clear examples should be provided to illustrate
    appropriate use, such as comments that explain unexpected behavior or unconventional
    approaches, versus meaningless or excessive commentary that does not enhance
    comprehension. - Do not elevate files with verbose or irrelevant comments over
    those with concise, targeted, and effective comments. Focus on substance, not
    volume. - Metadata comments, like licensing information, may be required for
    compliance but should not be counted as contributing to 'commented context'
    unless they add value to the understanding of the code.\par In summary, this
    criterion focuses on whether comments are necessary, relevant, and useful in
    providing additional context or understanding. It recognizes that not all
    code requires extensive commenting and explicitly allows for marking the
    criterion as 'Not Applicable' in cases of minimalistic, self-explanatory, or
    trivial files.
\end{tcolorbox}