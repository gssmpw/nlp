\subsection{Evolution of Criteria Distribution}

\begin{figure*}[htbp]
    \centering
    \begin{subfigure}
        [b]{0.48\textwidth}
        \includegraphics[width=\linewidth]{figures/evolution.pdf}
        \caption{Distribution of accuracy.}
        \label{fig:evolution_acc}
    \end{subfigure}
    \hfill
    \begin{subfigure}
        [b]{0.48\textwidth}
        \includegraphics[width=\linewidth]{figures/refuse_rate.pdf}
        \caption{Distribution of refuse rate.}
        \label{fig:evolution_refuse_rate}
    \end{subfigure}
    \caption{Evolution of distributions of the top-$k$ Python code quality
    criteria through evolution iterations, where $k$ is the number of the final criteria.}
    \label{fig:criteria_evolution}
\end{figure*}

In this section, we analyze how the distribution of quality criteria evolves during
the evolution process. Using the code domain as a representative example, Figure~\ref{fig:evolution_acc}
shows the distribution of training accuracies for all criteria across optimization
iterations. The plot reveals a clear upward trend, with the distribution progressively
shifting and concentrating towards higher values as the optimization proceeds.
This trend demonstrates the effectiveness of our iterative optimization process.

Notably, several criteria achieve 100\% accuracy. As explained in Section~\ref{sec:voting},
we exclude the cases where the worker agent explicitly declines to provide a
judgment. Through the optimization process, the manager agent refines the
criteria descriptions to be more precise about their applicability. These highly
accurate criteria are particularly valuable as they effectively characterize code
quality and guide the worker agent to make accurate assessments when applicable,
even if they may not cover all possible scenarios.

In addition, we analyze the distribution of the refuse rate of the criteria. As
shown in Figure~\ref{fig:evolution_refuse_rate}, the refuse rate falls predominantly
in lower ranges, indicating that most criteria are widely applicable, while there
are still a few criteria with refuse rates higher than 60\% that are retained
due to their high accuracy when applicable.

\subsection{Criterion Refinement}
\label{sec:refinement}

The improvement in accuracy of CritiQ Flow is driven by two key processes:
deprecating low-quality criteria and refining the mid-quality criteria by
revising the descriptions. Deprecating the low-quality ones is something like reject
sampling, which is straightforward in improving performance. In this section, we
analyze how mid-quality criteria are refined by the manager agent.

We categorize the criteria refinement into 2 types: (1) refining the criteria retrieved
from the knowledge base or generated by the manager agent, and (2) continually
refining the already refined criteria. We show examples of criteria before and after
refinement in Appendix~\ref{sec:appendix_ex_refinement}.

\paragraph{Refinement for Retrieved or Generated Criteria.}
The knowledge base is built on previous dataset research, so the criteria retrieved
from the knowledge base are often too general. When the knowledge base can not
provide enough criteria or some criteria are deprecated due to low accuracy, the
manager agent proposes new criteria. In this case, the initial descriptions of these
criteria are usually too vague, because they have not been evaluated by the
worker agent, thus the manager agent does not have enough information to generate
precise descriptions. As a result, the manager agent can refine those criteria by
rewriting them to fit the current domain, adding detailed guidelines for the
worker agent, and specifying the applicability.

\paragraph{Refinement for Refined Criteria.}
For previously refined criteria, the manager agent can further improve them by adding
more detailed descriptions or examples. However, we also observe that despite the
iterative optimization process, refinements do not always yield higher accuracy,
especially for already well-refined criteria. Excessive refinement by the
manager agent can lead to over-fitting, particularly with small training sets.
To address this, we encourage the manager agent to keep the criteria simple and concise.

\subsection{Majority Voting}

We have demonstrated the majority voting mechanism in Section~\ref{sec:voting}. In
this section, we investigate the impact of the voting mechanism by evaluating
the accuracy of combining all criteria into a single prompt. We use the same quality
criteria derived by CritiQ Flow and query the worker agent for judgments. The
accuracies are shown in Table~\ref{tab:majority_voting}. In all domains, the
accuracy decreases without the majority voting mechanism, indicating that the majority
voting mechanism is essential for the performance of CritiQ Flow.

\begin{table}[htbp]
    \centering
    \begin{tabular}{lcccc}
        \toprule      & \textbf{Code} & \textbf{Math} & \textbf{Logic} & \textbf{Avg.} \\
        \midrule Ours & \textbf{89.33}         & \textbf{84.57}         & \textbf{88.06}          & \textbf{87.32}         \\
        w/o voting    & 84.16         & 81.14         & 85.22          & 83.51         \\
        \bottomrule
    \end{tabular}
    \caption{Accuracies with / without Majority Voting on the human-annotated
    $D_{\text{test}}$ across 3 domains. The higher values are in bold.}
    \label{tab:majority_voting}
\end{table}