We verify the effectiveness of CritiQ Flow in improving the accuracies on human-annotated
test sets. Hyperparameters for CritiQ Flow are
shown in Appendix~\ref{sec:hyperparameter}. We continually pretrain a \texttt{Llama-3.1-3B}
model to show the improved quality of our selected subset compared to the
original dataset.

\subsection{Setup}

\begin{table}[htbp]
    \centering
    \begin{tabular}{lccc}
        \toprule \textbf{Domain} & \#$D_{\text{human}}$ & \#$D_{\text{agent}}$ & \#$D_{\text{test}}$ \\
        \midrule \textbf{Code}   & 25                   & 25000                & 193                 \\
        \textbf{Math}            & 30                   & 25000                & 70                  \\
        \textbf{Logic}           & 30                   & 25000                & 134                 \\
        \bottomrule
    \end{tabular}
    \caption{Number of pairs in each split.}
    \label{tab:number_dataset_pairs}
\end{table}

\paragraph{Dataset.}

We focus on three domains: code, math and logic. We use the Python subset of the
Stack v2~\citep{lozhkov_starcoder_2024}, the non-code subset of OpenWebMath~\citep{paster_openwebmath_2023}
and Zyda-2~\citep{tokpanov_zyda-2_2024} datasets as the source dataset $D$. The
numbers of pairs of $D_{\text{human}}$ and $D_{\text{agent}}$ are shown in Table~\ref{tab:number_dataset_pairs}.

\paragraph{Models.}
We employ \texttt{GPT-4o}~\footnote{The specific version is \texttt{gpt-4o-2024-11-20}.}
as the manager agent which is good at reflection but is costly, and \texttt{Qwen2.5-72B-Instruct}
as the worker agent which can perform simple pairwise comparison while is
relatively cheap. We initialize CritiQ Scorer by \texttt{Qwen2.5-1.5B} for
efficiency considerations. Hyperparameters for CritiQ Scorer are shown in Appendix~\ref{sec:hyperparameter}.

\paragraph{Baselines.}
Directly prompting the worker LLM for data quality comparison serves a vanilla
baseline. We use the same prompt as ours without specifying a criterion for vanilla
baseline experiments. We compare the optimization algorithm in our workflow with
TextGrad~\citep{yuksekgonul_textgrad_2024}. The initial prompt for TextGrad is
the same as the vanilla baseline. We run TextGrad optimizations on the same training
set $D_{\text{agent}}$ as ours. We compare our criteria with those proposed by
QuRating~\citep{wettig_qurating_2024}. The prompts for QuRating are from their
original work.

\paragraph{Evaluation.}
We evaluate CritiQ Flow by the accuracy on the human-annotated test set $D_{\text{test}}$.
High accuracy indicates effectiveness in capturing human preferences for data
quality. For each pair, three annotators will determine which data point
exhibits higher quality independently under the same annotation guidelines with
$D_{\text{human}}$. We only keep the pairs for which all three annotators give the
same judgment. The final number of pairs in $D_{\text{test}}$ is shown in Table~\ref{tab:number_dataset_pairs}.
We emphasize that although we take human effort to annotate more pairs for validation
purpose, and the workflow itself just need a tiny annotated dataset to work. We will
show how well CritiQ Flow mines data quality criteria by only $\sim$30 human
annotated pairs and gets high accuracies on $D_{\text{test}}$.

\subsection{Results}

We report the accuracies of the baselines and CritiQ on the test set of all 3 domains
in Table~\ref{tab:main_results}. In addition, we report the ablation results for
the knowledge base and the criteria evolution process.

\paragraph{Vanilla method can be improved by TextGrad and CritiQ Flow.}
Although the vanilla method is not low in the agreement rate with human
annotators, it can be further improved by TextGrad~\citep{yuksekgonul_textgrad_2024}
and CritiQ Flow. Detailed descriptions and instructions help the worker agent to
perform better judgments.

\paragraph{CritiQ Flow outperforms TextGrad.}
Compared with TextGrad, CritiQ Flow achieves higher accuracies in all domains,
indicating higher effectiveness in capturing human preferences for data quality.
Interestingly, we find that TextGrad is also trying to find quality criteria, but
it is not as effective as CritiQ Flow. This suggests that the optimization
algorithm in our workflow is more effective in the scenarios of mining quality
criteria from human preferences. We show the prompts generated by TextGrad in Appendix~\ref{sec:textgrad}.

\paragraph{CritiQ Flow surpasses single criteria.}
Any single criterion proposed by QuRating~\citep{wettig_qurating_2024} fails to achieve
a high accuracy. Although, as highlighted in many related studies~\citep{zhang_autonomous_2024,gunasekar_textbooks_2023,wei_arctic-snowcoder_2024},
the Educational Value criterion shows relatively higher consistency with human judgment,
it can not comprehensively describe data quality. This suggests that compared to
single criterion, and CritiQ Flow which uses multiple criteria is better.

\paragraph{Evolution and knowledge base help CritiQ Flow improve the
performance.}
Ablation shows that both the iterative evolution process and knowledge base in our
workflow help improve the accuracies. This indicates that the criteria extracted
from previous work are effective in judging data quality, while still have the potential
to be optimized according to the specific domain and dataset; and that the
optimization process is effective in improving the criteria with only $\sim$30 human annotations.

\paragraph{CritiQ Scorer shows increased accuracy.}
Notably, CritiQ Scorer achieves higher accuracies than the direct multi-criteria
voting by worker agents across all domains, despite being trained on data
annotated by them. This suggests that our method effectively extracts human's inner
quality evaluation criteria, and these criteria demonstrate strong generalization
capability.

\subsection{Continual Pretraining}

We choose \texttt{Llama-3.1-3B} as the base model for the continual pretraining experiments.
We sample 10B tokens from the Stack v2 and Zyda-2, and 3B from OpenWebMath. We
perform uniform sampling and sampling using CritiQ Scorer with temperature
$\tau=1$ for the code and math datasets and $\tau=0.5$ for the logic dataset.
We continually train the models on the six datasets separately. Hyperparameters
are shown in Appendix~\ref{sec:hyperparameter}.

We evaluate the continually trained models on corresponding downstream tasks, including
4 code-writing tasks: HumanEval~\citep{chen_evaluating_2021}, MBPP~\citep{austin_program_2021},
HumanEval+, and MBPP+~\citep{liu_is_2023}; 3 math problem solving tasks: GSM8k~\citep{cobbe_training_2021},
SAT-Math~\citep{zhong_agieval_2023}, and MATH~\citep{hendrycks_measuring_2021};
and 2 logic reasoning tasks ARC-Challenge~\citep{clark_think_2018} and LogiQA~\citep{zhong_agieval_2023}.
Coding tasks are evaluated using EvalPlus~\citep{liu_is_2023}, while others are evaluated
by OpenCompass~\citep{2023opencompass}. The results are shown in Table~\ref{tab:continual_pretraining}.
The models trained on our selected high-quality subsets show improved
performance on downstream tasks compared to the models trained on the uniformly
sampled subsets.

\begin{table}[t]
    \centering
    \footnotesize
    \begin{tabularx}
        {0.97\linewidth} {l@{\hskip 9pt}c@{\hskip 9pt}c@{\hskip 9pt}c} \toprule \textbf{Code}
        & \textbf{HumanEval / +} & \textbf{MBPP / +} & \textbf{Avg. / +} \\ \midrule
        Raw & 28.66 / 25.61 & 48.94 / 39.15 & 38.80 / 32.38 \\ Stack & 31.71 / 27.44
        & 56.61 / 46.30 & 44.16 / 36.87 \\ Ours & \textbf{39.02} / \textbf{33.54}
        & \textbf{68.73} / \textbf{48.41} & \textbf{53.88} / \textbf{40.98} \\
    \end{tabularx}
    \begin{tabularx}
        {0.97\linewidth} {l@{\hskip 14pt}c@{\hskip 14pt}c@{\hskip 14pt}c@{\hskip 14pt}c}
        \midrule \textbf{Math} & \textbf{GSM8k} & \textbf{SAT-Math} & \textbf{MATH}
        & \textbf{Avg.} \\ \midrule Raw & 27.60 & 35.00 & 5.50 & 22.70 \\ OWM &
        28.51 & 32.27 & 5.80 & 22.19 \\ Ours & \textbf{32.22} & \textbf{39.55} &
        \textbf{6.34} & \textbf{26.04} \\
    \end{tabularx}
    \begin{tabularx}
        {0.97\linewidth} {l@{\hskip 28pt}c@{\hskip 28pt}c@{\hskip 28pt}c} \midrule
        \textbf{Logic} & \textbf{ARC-C} & \textbf{LogiQA} & \textbf{Avg.} \\ \midrule
        Raw & 37.97 & 27.34 & 32.66 \\ Zyda-2 & 36.61 & 23.50 & 30.06 \\ Ours & \textbf{38.31}
        & \textbf{30.41} & \textbf{34.36} \\ \bottomrule
    \end{tabularx}
    \caption{Evaluation results on downstream tasks of the continually trained
    model. ``Raw'' is the original \texttt{Llama-3.1-3B} model without any
    continual pretraining. ``+'' for HumanEval+ or MBPP+~\citep{liu_is_2023}. ``Stack''
    for the Python subset of the Stack v2~\citep{lozhkov_starcoder_2024}. OWM for
    the non-code subset of OpenWebMath~\citep{paster_openwebmath_2023}.}
    \label{tab:continual_pretraining}
\end{table}
