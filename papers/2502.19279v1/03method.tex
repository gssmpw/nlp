\subsection{Overview}

In CritiQ, we first use an agent workflow, CritiQ Flow, to automatically extract
quality criteria from human preferences for data quality with limited human
annotation, and then use these criteria to train a scoring model, CritiQ Scorer,
to efficiently perform large-scale data selection.

For a specific text dataset $D$, we sample $\sim$30 pairs of data points. Compared to
works that the authors carefully design prompts~\citep{dubey_llama_2024, sachdeva_how_2024,zhang_autonomous_2024,gunasekar_textbooks_2023, wei_arctic-snowcoder_2024},
a small amount of data annotation requires less human effort. We employ human expert
annotators to determine which data point in each pair is of higher quality,
forming the training set $D_{\text{human}}$ for CritiQ Flow. Details for annotation
are shown in Appendix~\ref{sec:annotation}. Figure~\ref{fig:method} shows how
CritiQ Flow mines quality criteria from $D_{\text{human}}$. Prompts we used are shown
in Appendix~\ref{sec:appendix_prompts}.

To perform large-scale data selection, we train CritiQ Scorer, a lightweight scoring
model. Following~\citet{korbak_pretraining_2023,wettig_qurating_2024}, we use a
Bradley-Terry model~\citep{bradley_rank_1952} to convert the pairwise comparison
into a numerical score. We randomly sample a larger number of text pairs, forming
the training dataset $D_{\text{agent}}$ for CritiQ Scorer. The quality
preference labels will be annotated by the worker agents through the pairwise
judgment process under the obtained quality criteria. Finally, we use CritiQ
Scorer to score all text data in $D$ and select the high-quality subset
according to the quality scores.

\begin{table*}
    [t]
    \centering
    %\footnotesize
    \begin{tabular}{llcccccccc}
        \toprule \multicolumn{2}{l}{\textbf{Method}} & \textbf{Code}     & $\Delta$                                  & \textbf{Math}                    & $\Delta$                               & \textbf{Logic}                   & $\Delta$                               & \textbf{Avg.}                 & $\Delta$                               \\
        \midrule \multicolumn{2}{l}{Vanilla}         & 82.02             & -                                         & 72.86                            & -                                      & 72.99                            & -                                      & 75.96                         & -                                      \\
        \midrule \multicolumn{2}{l}{TextGrad}        & 72.70             & \textcolor{DarkRed}{\ \ \ -9.32}          & 78.57                            & \textcolor{DarkGreen}{\ \ +5.71}       & 75.22                            & \textcolor{DarkGreen}{\ \ +2.23}       & 75.50                         & \textcolor{DarkRed}{\ \ \ -0.46}       \\
        \multirow{4}{*}{\rotatebox{90}{QuRating}}    & Writing Style     & 73.03                                     & \textcolor{DarkRed}{\ \ \ -8.99} & 52.86                                  & \textcolor{DarkRed}{\ -20.00}    & 59.70                                  & \textcolor{DarkRed}{\ -13.29} & 61.86                                 & \textcolor{DarkRed}{\ -14.09}    \\
                                                     & Facts \& Trivia   & 76.40                                     & \textcolor{DarkRed}{\ \ \ -5.62} & 44.29                                  & \textcolor{DarkRed}{\ -28.57}    & 84.33                                  & \textcolor{DarkGreen}{+11.34} & 68.34                                 & \textcolor{DarkRed}{\ \ \ -7.62} \\
                                                     & Educational Value & 85.39                                     & \textcolor{DarkGreen}{\ \ +3.37} & 68.57                                  & \textcolor{DarkRed}{\ \ \ -4.29} & 84.33                                  & \textcolor{DarkGreen}{+11.34} & 79.43                                 & \textcolor{DarkGreen}{\ \ +3.47} \\
                                                     & Require Expertise & 79.21                                     & \textcolor{DarkRed}{\ \ \ -2.81} & 52.86                                  & \textcolor{DarkRed}{\ -20.00}    & 84.33                                  & \textcolor{DarkGreen}{+11.34} & 72.13                                 & \textcolor{DarkRed}{\ \ \ -3.82} \\
        \midrule \multicolumn{2}{l}{CritiQ Flow}     & \textbf{89.33}    & \textcolor{DarkGreen}{\textbf{\ \ +7.31}} & \textbf{84.57}                   & \textcolor{DarkGreen}{\textbf{+11.71}} & \textbf{88.06}                   & \textcolor{DarkGreen}{\textbf{+15.07}} & \textbf{87.32}                & \textcolor{DarkGreen}{\textbf{+11.36}} \\
        \multicolumn{2}{l}{\quad w/o evo.}           & 86.40             & \textcolor{DarkGreen}{\ \ +4.38}          & 78.00                            & \textcolor{DarkGreen}{\ \ +5.14}       & 85.97                            & \textcolor{DarkGreen}{+12.98}          & 83.46                         & \textcolor{DarkGreen}{\ \ +7.50}       \\
        \multicolumn{2}{l}{\quad w/o k.b.}           & 87.19             & \textcolor{DarkGreen}{\ \ +5.17}          & 82.57                            & \textcolor{DarkGreen}{\ \ +9.71}       & 81.64                            & \textcolor{DarkGreen}{\ \ +8.65}       & 83.80                         & \textcolor{DarkGreen}{\ \ +7.84}       \\
        \multicolumn{2}{l}{\quad w/o evo. \& k.b.}   & 83.03             & \textcolor{DarkGreen}{\ \ +1.01}          & 76.29                            & \textcolor{DarkGreen}{\ \ +3.43}       & 68.36                            & \textcolor{DarkRed}{\ \ \ -4.63}       & 75.89                         & \textcolor{DarkRed}{\ \ \ -0.06}       \\
        \multicolumn{2}{l}{CritiQ Scorer}            & \textbf{89.89}    & \textcolor{DarkGreen}{\textbf{\ \ +7.87}} & \textbf{90.00}                   & \textcolor{DarkGreen}{\textbf{+17.14}} & \textbf{90.22}                   & \textcolor{DarkGreen}{\textbf{+17.23}} & \textbf{90.04}                & \textcolor{DarkGreen}{\textbf{+14.08}} \\
        \bottomrule
    \end{tabular}
    \caption{Accuracies on the human-annotated $D_{\text{test}}$. The best results
    and the best results without training a model are in bold. ``$\Delta$'' is
    the delta value with the vanilla results. ``evo.'' for iterative criteria
    evolution. ``k.b.'' for retrieving initial critieria from the knowledge base
    instead of generating all initial critieria by the manager agent. The results
    are the average over 5 experiments with different random seeds.}
    \label{tab:main_results}
\end{table*}

\subsection{Knowledge Base}
\label{sec:knowledge_base}

As an iterative agent workflow, the quality of the initial criteria is crucial for CritiQ Flow. Many research papers have shared valuable insights on quality standards and have succeeded in data selection. Therefore, we can leverage findings from these data selection studies to establish a criteria knowledge base. Drawing from well-validated methodologies, the knowledge base can enhance the initialization of CritiQ Flow, ensuring a strong foundation for subsequent refinements.

To construct the knowledge base, we first crawl the cited papers of the datasets published on the Hugging Face
Hub~\footnote{\href{https://huggingface.co/datasets}{https://huggingface.co/datasets}
data collected before July 2024}. We
only use the arXiv papers available in HTML format, avoiding potential issues with PDF
parsing. We employ \texttt{GPT-4o-mini} to identify papers that introduce datasets
from the titles and abstracts. Subsequently, we use \texttt{GPT-4o-mini} to
systematically extract quality criteria from these papers. After de-duplication,
we establish a knowledge base $C_{\text{knowledge}}$ comprising 342 distinct
quality criteria.

\begin{algorithm}
    [htbp]
    \caption{Retrieve Criteria from $C_{\text{domain}}$}
    \label{alg:kb}
    \begin{algorithmic}
        [1]
        \renewcommand{\algorithmicrequire}{\textbf{Input:}}
        \renewcommand{\algorithmicensure}{\textbf{Output:}}
        \Require $C_{\text{domain}}$, $D_{\text{human}}$, $n$ \Ensure $C_{\text{retrieved}}$
        \State Initialize $C_{\text{retrieved}}[\ ], \text{Acc}[\ ]$\For{$c_{i}\in C_{\text{domain}}$}
        \State $\text{Acc}_{i}\gets$acc over\ $ D_{\text{human}}$ \EndFor \State
        Sort $C_{\text{domain}}$ by $\text{Acc}$ \Comment{Descending order} \For{$c_{i}\in C_{\text{domain}}$}
        \If{$\Call{Length}{C_{\text{retrieved}}\geq n}$} \State break \EndIf \If{$\text{Acc}_{i}> 0.5$}
        \State \Call{Append}{$C_{\text{retrieved}}, c_{i}$} \EndIf \EndFor
        \State \Return $C_{\text{retrieved}}$
    \end{algorithmic}
\end{algorithm}

We use this $C_{\text{knowledge}}$ to provide initial criteria for CritiQ Flow. We query
a model with the domain description of the dataset to retrieve potentially
useful criteria from $C_{\text{knowledge}}$, forming $C_{\text{domain}}$. As shown
in Algorithm~\ref{alg:kb}, we then retrieve $n$ criteria from $C_{\text{domain}}$.
If the criteria are not enough, we query the manager agent to propose new criteria.

\subsection{Multi-Criteria Pairwise Judgment}
\label{sec:voting}

Given a set of quality criteria $C$ and a pair of data points $p=(\text{text}_{\text{A}}
, \text{text}_{\text{B}}) \in D_{h}$, the pairwise judgment process gives a
quality preference by a worker agent. Each criterion has a corresponding description
to guide the comparison. In consideration of cost and efficiency, we do not use an
expensive model as the worker agent. Instead, we use a model that can perform simple
comparisons under a single criterion, which is not difficult for many open-source
LLMs.

For each criterion $c_{i}\in C$, we query a distinct worker agent to determine which
data point exhibits higher quality. The worker agent analyzes both data points
with respect to $c_{i}$ before making a judgment. If $c_{i}$ is not applicable or
if both text A and B of $p$ demonstrate comparable quality, the worker agent can
refuse to provide an answer, i.e., answer ``null''. The final judgment across all
criteria is made through majority voting, i.e.,
\[
    \text{judge}(p, C) = \text{majority}_{c_{i} \in C}(\{\text{worker}_{i}(p, c_{i}
    )\}),
\]
where $\text{worker}_{i}(p, c_{i}) \in \{\text{A}, \text{B}, \text{null}\}$ is
the worker agent's judgment of $p$ under $c_{i}$.

Because we only focus on whether the final judgment is consistent with the human
annotation and do not require all criteria to be applicable to a certain pair, we
do not take these situations into consideration when calculating the accuracy for
this criterion. The criterion accuracy for $c_{i}$ on dataset $D_{h}$ is
calculated as

\begin{footnotesize}
    \[
        \text{acc}(c_{i}|D_{h}) = \frac{|\{p \in D_{h}|\text{w}_{i}(p,c_{i})=\text{h}(p)\}|}{|D_{h}|-|\{p
        \in D_{h}|\text{w}_{i}(p,c_{i})=\text{null}\}|},
    \]
\end{footnotesize}

where $h(p) \in \{A, B\}$ is the human-annotated higher-quality one in $p$, and $\text{w}_{i}
(p,c_{i})$ is the worker agent's judgment of $p$ according to $c_{i}$.

\subsection{Criteria Evolution}

After retrieving the initial criteria from the knowledge base, we perform an iterative
criteria evolution to improve the accuracy on $D_{\text{human}}$. For each
iteration, we first make pairwise judgments on $D_{\text{human}}$. Based on the
accuracy $acc_{i}$ of each criterion $c_{i}$, we then divide them into three groups
by a high threshold $t_{\text{high}}$ and a low threshold $t_{\text{low}}$. For
$c_{i}$ with $acc_{i}\geq t_{\text{high}}$, we keep them directly. For $c_{i}$
with $acc_{i}\leq t_{\text{low}}$, we remove them and query the manager agent to generate
new criteria. Simultaneously, they will be recorded to avoid
being generated again by the manager agent in subsequent iterations. For $c_{i}$
with $t_{\text{low}}< acc_{i}< t_{\text{high}}$, we ask the manager agent to do reflection.
For each incorrect judgment of
$p\in \{p|worker(p, c_{i})\notin \{h(p ), null\} \}$, we provide the manager with
the right answer $h(p)$ and worker agent's thought. The manager agent should
analyze why the worker agent makes mistakes and provide a suggestion to itself
on how to improve the criteria. Given all suggestions from the wrong cases, the
manager agent should refine the description of $c_{i}$ as $c'_{i}$. $acc'_{i}$ will
be calculated in the next iteration.

Unlike the gradient descent algorithm, text-based optimization does not
guarantee that the loss will decrease within a neighborhood of the current state.
Therefore, we need to introduce external constraints to ensure this. In CritiQ Flow,
we save all criteria $c_{i}$ throughout the evolution process with their
accuracies $acc_{i}$. After getting the new accuracy $acc'_{i}$ of a revised criterion
$c'_{i}$, we will only update the description of it when
$acc'_{i}\geq acc_{i}$. This constraint ensures that the description revision will
not make the criterion worse. The final criteria are those with the highest
accuracy of all criteria across iterations.

\subsection{Train the Scoring Model}

After obtaining the quality criteria, we can use them to annotate a larger
number of pairs from the dataset $D$ to train CritiQ Scorer. To form the pairs,
we randomly sample several data points and group them by the length of the text
to remove the potential influences of length biases of the worker
agent. We then use the pairwise judgment process to annotate the pairs according to the
quality criteria mined by CritiQ Flow, forming $D_{\text{agent}}$. Only worker agents are employed in
this process, which get rid of the high cost API calls to the manager agent.

Training the CritiQ Scorer $s_{\theta}$ is straightforward by minimizing the loss
function,
\[
    \mathcal{L}(\theta) = - \frac{1}{N}\sum_{p \in D_{\text{agent}}}\log\sigma (s
    _{\theta}(d_{\text{high}}) - s_{\theta}(d_{\text{low}})),
\]
where $\sigma$ is the sigmoid function, $d_{\text{high}}$ and $d_{\text{low}}$ are
the relatively high- and low- quality data point in the pair $p$.

\subsection{Selecting Data}

In consideration of cost and efficiency, we use a lightweight base model as the scoring
model, which increases the speed of scoring the entire dataset $D$. After getting
a score $s_{\theta}(d_{i})$ for each data point $d_{i}$ in $D$, we normalize the
scores to obtain the final quality score $s_{i}$. As QuRating~\citep{wettig_qurating_2024}
suggests, sampling is better than naive top-$k$ selection. We select each data
point $d_{i}$ with the probability $p_{i}\propto exp(\frac{s_{i}}{\tau})$, where $\tau$
is the temperature. This process is implicitly equivalent to reward-weighted
regression~\citep{wettig_qurating_2024,korbak_pretraining_2023,peters_reinforcement_2007}.
We use the Gumble top-$k$ trick~\citep{wettig_qurating_2024,kool_stochastic_2019}
to perform efficient sampling without replacement.
