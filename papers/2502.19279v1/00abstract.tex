Language model heavily depends on high-quality data for
optimal performance. Existing approaches rely on manually designed heuristics, the perplexity of existing models, training classifiers, or
careful prompt engineering, which require significant expert experience and human annotation effort while
introduce biases. We introduce \textbf{CritiQ}~\footnote{\textbf{Crit}er\textbf{i}a
of Data \textbf{Q}uality, pronounced as ``critic''. Code is available at \url{https://github.com/KYLN24/CritiQ}}, a novel data selection method that
automatically mines criteria from human preferences for data quality with only $\sim$30 human-annotated
pairs and performs efficient data selection. The main component, CritiQ Flow, employs a manager agent to evolve
quality criteria and worker agents to make pairwise judgments. We build a knowledge
base that extracts quality criteria from previous work to boost CritiQ Flow. Compared to perplexity- and classifier- based methods, verbal criteria are more interpretable and possess reusable value. After deriving the criteria, we train the CritiQ Scorer to give quality scores and
perform efficient data selection. We demonstrate the effectiveness of our method
in the code, math, and logic domains, achieving high accuracy on human-annotated test
sets. To validate the quality of the selected data, we continually train
\texttt{Llama 3.1} models and observe improved performance on downstream tasks compared
to uniform sampling. Ablation studies validate the benefits of the knowledge
base and the reflection process. We analyze how criteria evolve and the effectiveness
of majority voting.
