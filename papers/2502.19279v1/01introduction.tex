Large language models (LLMs) show significant performance in various downstream
tasks~\citep{brown_language_2020,openai_gpt-4_2024,dubey_llama_2024}. Studies
have found that training on high quality corpus improves the ability of LLMs
to solve different problems such as writing code, doing math exercises, and
answering logic questions~\citep{cai_internlm2_2024,deepseek-ai_deepseek-v3_2024,qwen_qwen25_2024}.
Therefore, effectively selecting high-quality text data is an important subject for
training LLM.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/head.pdf}
    \caption{The overview of CritiQ. We (1) employ human annotators to annotate $\sim$30
    pairwise quality comparisons, (2) use CritiQ Flow to mine quality criteria, (3)
    use the derived criteria to annotate 25k pairs, and (4) train the CritiQ Scorer to
    perform efficient data selection.}
    \label{fig:overview}
\end{figure}

To select high-quality data from a large corpus, researchers manually design heuristics~\citep{dubey_llama_2024,rae_scaling_2022},
calculate perplexity using existing LLMs~\citep{marion2023moreinvestigatingdatapruning,wenzek2019ccnetextractinghighquality},
train classifiers~\citep{brown_language_2020,dubey_llama_2024,xie_data_2023} and
query LLMs for text quality through careful prompt engineering~\citep{gunasekar_textbooks_2023,wettig_qurating_2024,sachdeva_how_2024}.
Large-scale human annotation and prompt engineering require a lot of human
effort. Giving a comprehensive description of what high-quality data is like is also
challenging. As a result, manually designing heuristics lacks robustness and introduces
biases to the data processing pipeline, potentially harming model performance
and generalization. In addition, quality standards vary across different
domains. These methods can not be directly applied to other domains without significant
modifications.

To address these problems, we introduce CritiQ, a novel method to automatically
and effectively capture human preferences for data quality and perform efficient data
selection. Figure~\ref{fig:overview} gives an overview of CritiQ, comprising an agent
workflow, CritiQ Flow, and a scoring model, CritiQ Scorer. Instead of manually describing
how high quality is defined, we employ LLM-based agents to summarize quality
criteria from only $\sim$30 human-annotated pairs.

CritiQ Flow starts from a knowledge base of data quality criteria. The worker
agents are responsible to perform pairwise judgment under a given
criterion. The manager agent generates new criteria and refines them through reflection
on worker agents' performance. The final judgment is made by majority voting among
all worker agents, which gives a multi-perspective view of data quality.

To perform efficient data selection, we employ the worker agents to annotate a randomly
selected pairwise subset, which is ~1000x larger than the human-annotated one.
Following \citet{korbak_pretraining_2023,wettig_qurating_2024}, we train CritiQ
Scorer, a lightweight Bradley-Terry model~\citep{bradley_rank_1952} to convert
pairwise preferences into numerical scores for each text. We use CritiQ Scorer to
score the entire corpus and sample the high-quality subset.

For our experiments, we established human-annotated test sets to quantitatively
evaluate the agreement rate with human annotators on data quality preferences. We implemented the manager agent by \texttt{GPT-4o} and the worker
agent by \texttt{Qwen2.5-72B-Insruct}. We conducted experiments on different
domains including code, math, and logic, in which CritiQ Flow shows a consistent
improvement in the accuracies on the test sets, demonstrating the effectiveness
of our method in capturing human preferences for data quality. To validate the quality
of the selected dataset, we continually train \texttt{Llama 3.1}~\citep{dubey_llama_2024}
models and find that the models achieve better performance on downstream tasks
compared to models trained on the uniformly sampled subsets.

We highlight our contributions as follows. We will release the code to facilitate
future research.

\begin{itemize}
    \item We introduce CritiQ, a method that captures human preferences for data
        quality and performs efficient data selection at little cost of human
        annotation effort.

    \item Continual pretraining experiments show improved model performance in code,
        math, and logic tasks trained on our selected high-quality subset compared to the raw dataset.

    \item Ablation studies demonstrate the effectiveness of the knowledge base and
        the the reflection process.
\end{itemize}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/method.pdf}
    \caption{CritiQ Flow comprises two major components: multi-criteria pairwise
    judgment and the criteria evolution process. The multi-criteria pairwise
    judgment process employs a series of worker agents to make quality
    comparisons under a certain criterion. The criteria evolution process aims to
    obtain data quality criteria that highly align with human judgment through
    an iterative evolution. The initial criteria are retrieved from the
    knowledge base. After evolution, we select the final criteria to annotate
    the dataset for training CritiQ Scorer.}
    \label{fig:method}
\end{figure*}
