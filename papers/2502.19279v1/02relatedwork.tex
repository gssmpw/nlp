\paragraph{Heuristics for Data Selection.}

Using manually designed heuristics to identify data with specific
characteristics is a basic approach for data selection. Common rules include keyword
or stopword matching, length-based filtering, data source filtering, in-document
duplication~\citep{dubey_llama_2024,cai_internlm2_2024}, and training
classifiers~\citep{noauthor_improving_2024,xie_data_2023, dubey_llama_2024, wei_arctic-snowcoder_2024,korbak_pretraining_2023,lv_longwanjuan_2024}.
Designing these rules requires much experience and human effort.

Researchers also design specific rules to select high-quality domain data~\citep{wang_generative_2023,lozhkov_starcoder_2024,huang_opencoder_2024},
which requires much expert experience and lacks scalability and generalization.

\paragraph{Quality Signals from LLMs.}

The use of LLMs to assess data quality has become a prevalent approach. Researchers
employ manual-designed prompts to query LLMs for quality assessment~\citep{dubey_llama_2024, sachdeva_how_2024,zhang_autonomous_2024},
often using educational value as a proxy for data quality~\citep{gunasekar_textbooks_2023, wei_arctic-snowcoder_2024}.
However, their focus remains limited to fixed aspects of data quality. Although these
methods reduce the need for human annotation, they introduce inherent biases
through predefined rules and standards.

Previous works like QuRating~\citep{wettig_qurating_2024} evaluate data quality using
multiple manually defined criteria including writing style, factual accuracy, level
of expertise, and educational value. These predefined criteria show varying effectiveness across
different domains, suggesting that manually summarized criteria lack generalization
and can not accurately describe data quality. In contrast, CritiQ Flow automatically
discovers quality criteria by effectively capturing human preferences about
data quality assessment from a few number of human-annotated pairs.

\paragraph{Thought and Reflection of LLMs.}

Prompting
LLMs to reason before giving the final answer improves the model's
performance on various tasks~\citep{kojima_large_2023,yao_react_2023}. In our work, we also require the agents to think
and analyze before making the quality comparison.

Reflection is a common technique to improve the performance of LLMs through
iterative critiquing and refinement~\citep{shinn_reflexion_2023,madaan_self-refine_2023,saunders_self-critiquing_2022,xi2024enhancingllmreasoningcritique}.
Existing frameworks have integrated the reflection mechanism to build LLM-based
agents and do prompt engineering~\citep{yuksekgonul_textgrad_2024,asai_self-rag_2023,wu_autogen_2023}.
In CritiQ Flow, we also prompt the agent to
examine the wrong predictions and refine the quality criteria accordingly.
