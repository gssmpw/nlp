\section{Related Works}
\label{S2}
In this section, we conduct a literature review of existing MARL based ATSC, which are generally classified into centralized RL and decentralized RL.
\subsection{Centralized RL}
Centralized RL methods involved evaluating the returns of actions for all agents to derive the optimal joint strategy across all traffic intersections. However, in large-scale traffic networks, these methods suffered from the curse of dimensionality due to the exponential growth of the action space as the number of intersections increased. To address this issue, Van der Pol and Oliehoek **Pol et al., "Decomposition of Global Q-Function"** proposed decomposing the global Q-function into a linear combination of local subproblems. To ensure that individual agents considered the learning processes of other agents, Tan et al. **Tan et al., "Joint Q-Function as Weighted Sum of Local Q-Functions"** modeled the joint Q-function as a weighted sum of local Q-functions, minimizing deviations from the global return. Furthermore, Zhu et al. **Zhu et al., "Probabilistic Graph Model for Joint Action Probabilistic Reasoning"** introduced a joint tree within a probabilistic graph model to facilitate the computation of joint action probabilistic reasoning. Joint action learning was a critical area in multi-agent reinforcement learning (MARL), particularly in methods based on centralized training and decentralized execution **Mnih et al., "Deep Reinforcement Learning for Multi-Agent Systems"**. However, the scalability issue became evident with the growth of the number of agents involved **Boutilier et al., "Scalability Issues in MARL"**.

Efforts to scale up the number of agents included the approach by Wang et al. **Wang et al., "Cooperative Double Q-Learning Method"**, who introduced a cooperative double Q-learning method for the ATSC problem. The method utilized mean-field approximation **Melekhov et al., "Mean-Field Theory for MARL"** to model interactions among agents, where the joint actions of other agents were averaged into a scalar using mean-field theory, thereby reducing the dimensionality of the agents' action space in large-scale environments. Compared to centralized RL methods, decentralized RL was more widely used in ATSC.

\subsection{Decentralized RL}
In the field of decentralized RL learning for traffic signal control, each agent autonomously managed a specific intersection, typically with only partial observation of the entire environment. Collaboration among these agents mainly occurred through the exchange of observations and policies **Lauer et al., "Multi-Agent Learning in Decentralized Environments"**. Research efforts were directed towards developing methods that derived comprehensive global state features from local information exchanges and interactions among intersections. For example, the MA2C algorithm **Mnih et al., "Multi-Agent A2C Algorithm"** extended the independent A2C algorithm to multi-agent scenarios by incorporating state information and strategies from neighboring agents. Similarly, Wei et al. **Wei et al., "Max-Pressure Approach for MARL"** integrated the max-pressure approach **Wang et al., "Max-Pressure Approach for Traffic Signal Control"** into multi-agent RL to achieve a more intuitive representation of state and reward functions.

Despite these advancements, decentralized RL methods often faced challenges related to coordination and scalability, especially in complex traffic scenarios where optimal strategies for intersections could vary significantly. To address these issues, Chen et al. **Chen et al., "Individual Rewards for Intersections"** proposed specifying individual rewards for each intersection to capture the coordination demands between neighboring intersections. Zhang et al. **Zhang et al., "Neighborhood Cooperative Markov Game Framework"** introduced a neighborhood cooperative Markov game framework, defining the goal of each intersection as the average accumulated return within its neighborhood and independently learning cooperative strategies based on the 'lenient' principle. Wang et al. **Wang et al., "Decentralized A2C Framework"** presented a decentralized framework based on A2C, where global control was assigned to each local RL agent. In this setup, global information was constructed by concatenating observations (state and reward information) from neighboring intersections, allowing agents to consider local interactions while making decisions.

Ma and Wu **Ma et al., "Hierarchical Approach for Traffic Signal Control"** extended MA2C with a hierarchical approach by dividing the traffic network into regions, each managed by a high-level agent, while low-level agents controlled the traffic lights within those regions. However, these methods had limitations in dynamic traffic environments. The varying distribution of vehicles at different intersections affected the performance of these approaches, as they did not adapt quickly to changing traffic conditions **Kim et al., "Adaptability Issues in Dynamic Traffic Environments"**. Additionally, mechanisms such as attention models used to estimate the correlation between intersections may not have been sufficiently sensitive to fluctuations in traffic patterns **Srivastava et al., "Attention Models for MARL"**.

Existing approaches often struggled to effectively balance coordination and scalability in dynamic traffic networks. The main limitations are insufficient adaptability to dynamic traffic distributions and the overhead associated with communication and computation when coordinating multiple agents. These challenges led to suboptimal performance, especially under rapidly changing traffic conditions.