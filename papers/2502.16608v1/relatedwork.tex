\section{Related Works}
\label{S2}
In this section, we conduct a literature review of existing MARL based ATSC, which are generally classified into centralized RL and decentralized RL.
\subsection{Centralized RL}
Centralized RL methods involved evaluating the returns of actions for all agents to derive the optimal joint strategy across all traffic intersections. However, in large-scale traffic networks, these methods suffered from the curse of dimensionality due to the exponential growth of the action space as the number of intersections increased. To address this issue, Van der Pol and Oliehoek \cite{22} proposed decomposing the global Q-function into a linear combination of local subproblems. To ensure that individual agents considered the learning processes of other agents, Tan et al. \cite{23} modeled the joint Q-function as a weighted sum of local Q-functions, minimizing deviations from the global return. Furthermore, Zhu et al. \cite{24} introduced a joint tree within a probabilistic graph model to facilitate the computation of joint action probabilistic reasoning. Joint action learning was a critical area in multi-agent reinforcement learning (MARL), particularly in methods based on centralized training and decentralized execution \cite{25}\cite{26}. However, the scalability issue became evident with the growth of the number of agents involved \cite{new7}.

Efforts to scale up the number of agents included the approach by Wang et al. \cite{27}, who introduced a cooperative double Q-learning method for the ATSC problem. The method utilized mean-field approximation \cite{28} to model interactions among agents, where the joint actions of other agents were averaged into a scalar using mean-field theory, thereby reducing the dimensionality of the agents' action space in large-scale environments. Compared to centralized RL methods, decentralized RL was more widely used in ATSC.

\subsection{Decentralized RL}
In the field of decentralized RL learning for traffic signal control, each agent autonomously managed a specific intersection, typically with only partial observation of the entire environment. Collaboration among these agents mainly occurred through the exchange of observations and policies \cite{29}. Research efforts were directed towards developing methods that derived comprehensive global state features from local information exchanges and interactions among intersections. For example, the MA2C algorithm \cite{30} extended the independent A2C algorithm to multi-agent scenarios by incorporating state information and strategies from neighboring agents. Similarly, Wei et al. \cite{31} integrated the max-pressure approach \cite{32} into multi-agent RL to achieve a more intuitive representation of state and reward functions.

Despite these advancements, decentralized RL methods often faced challenges related to coordination and scalability, especially in complex traffic scenarios where optimal strategies for intersections could vary significantly. To address these issues, Chen et al. \cite{33} proposed specifying individual rewards for each intersection to capture the coordination demands between neighboring intersections. Zhang et al. \cite{34} introduced a neighborhood cooperative Markov game framework, defining the goal of each intersection as the average accumulated return within its neighborhood and independently learning cooperative strategies based on the 'lenient' principle. Wang et al. \cite{35} presented a decentralized framework based on A2C, where global control was assigned to each local RL agent. In this setup, global information was constructed by concatenating observations (state and reward information) from neighboring intersections, allowing agents to consider local interactions while making decisions.

Ma and Wu \cite{36} extended MA2C with a hierarchical approach by dividing the traffic network into regions, each managed by a high-level agent, while low-level agents controlled the traffic lights within those regions. However, these methods had limitations in dynamic traffic environments. The varying distribution of vehicles at different intersections affected the performance of these approaches, as they did not adapt quickly to changing traffic conditions \cite{new8}. Additionally, mechanisms such as attention models used to estimate the correlation between intersections may not have been sufficiently sensitive to fluctuations in traffic patterns \cite{new9}.

Existing approaches often struggled to effectively balance coordination and scalability in dynamic traffic networks. The main limitations are insufficient adaptability to dynamic traffic distributions and the overhead associated with communication and computation when coordinating multiple agents. These challenges led to suboptimal performance, especially under rapidly changing traffic conditions.