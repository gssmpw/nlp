%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10pt, conference]{template/ieeeconf}  % Comment this line out if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper
\input{template/math_commands.tex}
\usepackage[T1]{fontenc}
\usepackage{wrapfig}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{accents}
\usepackage{todonotes}
\usepackage{colortbl}
\usepackage{caption}
\usepackage{diagbox}



\presetkeys%
{todonotes}%
{inline,backgroundcolor=yellow}{}
\usepackage{chngcntr}
\usepackage{siunitx}
\usepackage{tikz}
\usepackage{adjustbox}
\usepackage{url}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage[ruled,linesnumbered]{algorithm2e}


\presetkeys%
    {todonotes}%
    {inline,backgroundcolor=yellow}{}
\usepackage{chngcntr}
\usepackage{siunitx}
\usepackage{tikz}
\usepackage{adjustbox}
\usepackage{url}
\usepackage{subfigure}

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.
%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document


\newcommand{\Red}        {\textcolor{red}}

\title{\LARGE \bf
Diffusion Trajectory-guided Policy for Long-horizon \\ Robot Manipulation
% Diffusion Trajectory-guided Policy: A Novel Framework for Long-horizon Robot Manipulation
}


\author{Shichao Fan$^{1}$, Quantao Yang$^{4}$, Yajie Liu$^{2}$, Kun Wu$^{3}$, Zhengping Che$^{3}$, Qingjie Liu$^{2*}$, Min Wan$^{1}$ % <-this % stops a space
\thanks{$^{1}$ Shichao Fan and Min Wan are with School of Mechanical Engineering and Automation, BeiHang University, China.}%
\thanks{$^{2}$ Yajie Liu and Qingjie Liu are with School of Computer Science and Engineering, BeiHang University, China.
        {\tt\footnotesize *Corresponding Author: qingjie.liu@buaa.edu.cn}}%
\thanks{$^{3}$ Kun Wu and Zhengping Che are with Beijing Innovation Center of Humanoid Robotics, China.}%
\thanks{$^{4}$ Quantao Yang is with Division of Robotics, Perception and Learning (RPL), KTH Royal Institute of Technology, Sweden.}%
}


\usepackage[colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue]{hyperref}  

\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
% \todo{Write one statement about the general problem}
% \todo{Tell what this paper is about}
% \todo{Describe how it solves the problem}
% \todo{Emphasize what is new or better}
% \todo{Mention the evidence indicating the advantages of the proposed approach}
Recently, Vision-Language-Action models (VLA) have advanced robot imitation learning, but high data collection costs and limited demonstrations hinder generalization and current imitation learning methods struggle in out-of-distribution scenarios, especially for long-horizon tasks. A key challenge is how to mitigate compounding errors in imitation learning, which lead to cascading failures over extended trajectories. To address these challenges, we propose the Diffusion Trajectory-guided Policy (DTP) framework, which generates 2D trajectories through a diffusion model to guide policy learning for long-horizon tasks. By leveraging task-relevant trajectories, DTP provides trajectory-level guidance to reduce error accumulation. Our two-stage approach first trains a generative vision-language model to create diffusion-based trajectories, then refines the imitation policy using them.
Experiments on the CALVIN benchmark show that DTP outperforms state-of-the-art baselines by $25\%$ in success rate, starting from scratch without external pretraining. Moreover, DTP significantly improves real-world robot performance. All source code and experiment videos will be released upon acceptance.

% 写一两句为什么可以减少compuning error，比如生成的是pixel图像上的轨迹，可以从视觉上引导。
% 而且现在直接叫trajectory感觉有点歧义

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:introduction}

% \todo{Start with a motivation}
% \todo{Tell what this paper is about}
% \todo{Explain what makes this work relevant}
% \todo{(optional) Add a section about the structure of the paper.}
Imitation Learning (IL) demonstrates significant potential in addressing manipulation tasks within real robotic systems, this is evidenced by its ability to acquire diverse behaviors such as preparing coffee~\cite{zhu2023viola} and flipping mugs~\cite{chi2023diffusionpolicy} through learning from expert demonstrations. 
% However, these demonstrations often fail to encompass every potential robot pose and environment variation, from start to finish of tasks in long-horizon manipulation (Fig.~\ref{fig:overview}(a)). 
However, these demonstrations are often limited in coverage, failing to encompass every possible robot pose and environmental variation throughout long-horizon manipulation tasks (Fig.~\ref{fig:overview}(a)). This limitation leads to a key challenge in IL—compounding errors over extended trajectories, where small deviations from the expert trajectory accumulate, ultimately causing task failures.
% Moreover, unlike tasks in natural language processing (NLP) and computer vision (CV) \cite{he2022masked,achiam2023gpt,li2022blip}, the IL faces significant challenges due to the disparate semantic features between vision, language, and action spaces. 
Additionally, robot data is often scarce compared to computer vision tasks because it requires costly and time-consuming human demonstrations. Therefore, improving the generalization capabilities of imitation learning methods using extremely limited and scarce data, given the constraints and high costs of expert demonstrations, becomes a significant challenge. 

Recent research has proposed Vision-Language Action (VLA) models \cite{brohan2022rt,brohan2023rt,ma2024survey} to map multi-modality inputs to robot actions by using transformer structures \cite{vaswani2017attention}. To guide learning imitation policy, several approaches propose to integrate vision and language to generate a goal image, as seen in methods like Susie \cite {black2023zero} or future videos \cite{du2023video,du2024learning}, which are pretrained on large-scale video dataset from the Internet. The RT-trajectory \cite{gu2023rt} uses coarse trajectory sketches as modality instead of language, while the RT-H \cite{belkhale2024rt} involves breaking down complex language instructions into simpler, hierarchical commands. For example, instruction as ``Close the pistachio jar" can be decomposed step by step into actions like ``rotate arm right", ``move the arm forward", etc., thereby facilitating robot action generation. 
% These methods share a common goal of reducing the feature disparity between the language and action spaces. This includes approaches such as transferring complex language to a goal image, which then generates the action, replacing language instructions with coarse trajectory sketches that are more intuitive for the action space, or simplifying language instructions into directional commands that are easier to map to actions, thereby facilitating more effective task execution.
These approaches transform complex language instructions into goal images for action generation, replace language commands with coarse trajectory sketches that better align with the action space, or simplify instructions into directional commands that are easier to map to actions. By doing so, they help mitigate compounding errors in imitation policies, particularly in long-horizon tasks. While effective, these methods often rely on manually provided trajectories or goal images, limiting their flexibility, especially in diverse or unstructured environments.
% For model output, the Diffusion Policy \cite{chi2023diffusionpolicy} offers a unique perspective by defining action outputs as generative tasks, similar to image generation \cite{ho2022cascaded}. This novel insight presents a promising method to address the generalization challenges in imitation learning policies. 

\begin{figure*}
\begin{center}
\includegraphics[width=\linewidth]{fig/overview.png} 
\end{center}
\caption{\textbf{System overview.} (a) and (b) present a task instruction with the initial task observation, allowing our Diffusion Trajectory Model to predict the complete future 2D-particle trajectories; (c) illustrates the Diffusion Trajectory-guided pipeline, showcasing how these predicted trajectories guide the manipulation policy.}
\label{fig:overview} 
\end{figure*}


% our method
In this paper, we introduce a novel diffusion-based paradigm designed to reduce the feature disparity between the vision-language input and action spaces. By using vision-language input to generate task-relevant 2D trajectories, which are then mapped to the action space, our approach enhances performance in long-horizon robotic manipulation tasks. Unlike robots, which often rely on precise instructions, humans use high-level visualization, such as imagined task-relevant trajectories, to intuitively guide their actions. This visualization aids in adapting to changing conditions and refining our movements in real-time. Similarly, when instructing a robot using language, it should be feasible to envision a task-relevant trajectory to guide the robot's future actions based on current observations. To facilitate this process, We introduce the \textbf{D}iffusion \textbf{T}rajectory-guided \textbf{P}olicy (DTP), which consists of two stages: the \textbf{D}iffusion \textbf{T}rajectory \textbf{M}odel (DTM) learning stage and the vision language action policy learning stage. The first stage involves generating a task-relevant trajectory based on a diffusion model. In the second stage, this diffusion trajectory serves as guidance for learning the robot's manipulation policy, enabling the robot to perform tasks with better data efficiency and improved generalization.  We validated our method through extensive experiments on the CALVIN simulation benchmark \cite{mees2022calvin}, where it outperformed state-of-the-art baselines by an average success rate of 25\% across various settings. Additionally, Our approach is computationally cost-effective requiring only consumer-grade GPUs.
%contribution
The main contributions of the paper include:
\begin{enumerate}
    \item We propose the DTP, a novel imitation learning framework that utilizes a diffusion trajectory model to guide policy learning for long-horizon robot manipulation tasks.

    \item We leverage robot video data to pretrain a generative vision-language diffusion model, which enhances imitation policy training efficiency by fully utilizing available robot data. Furthermore, our method can be combined with large-scale pretraining methods, serving as a simple and effective plugin to enhance performance.

    \item We conducted extensive experiments in both simulated and real-world environments to evaluate the performance of DTP across diverse settings.
\end{enumerate}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\label{sec:related_work}
% \todo{Put your paper into the scientific context.}
% \todo{Don't forget to provide citations~\cite{samplePaper}}
% \todo{What is the work previously done by others?}
% \todo{Describe for every other paper, how your work differs.}
% \todo{Summarize in which way your paper goes beyond the state of the art.}

\textbf{Language-conditioned Visual Manipulation Policy Control.} Language-conditioned visual manipulation has made significant progress due to advancements in large language models (LLMs) and vision-language models (VLMs).
By using task planners like GPT-4 \cite{achiam2023gpt} or Palm-E \cite{driess2023palm}, it is possible to break down complex embodied tasks into simpler, naturally articulated instructions. Recently, several innovative methods have been developed in this domain. RT-1 \cite{brohan2022rt} pioneered the end-to-end generation of actions for robotic tasks. RT-2 \cite{brohan2023rt} explores the capabilities of LLMs for Vision-Language-Action (VLA) tasks by leveraging large-scale internet data. RoboFlamingo \cite{li2024visionlanguage} follows a similar motivation as RT-2, focusing on the utilization of extensive datasets. RT-X prioritizes the accumulation of additional robotic demonstration data to refine training and establish scaling laws in robotic tasks. The Diffusion Policy \cite{chi2023diffusionpolicy} addresses the prediction of robot actions using a denoising model. Lastly, Octo \cite{octo_2023} serves as a framework for integrating the aforementioned contributions into a unified system, further advancing the filed of language-conditioned visual manipulation.

\textbf{Policy Conditioning Representations.}
Due to the high-dimensional semantic information contained in language, using video prediction as a pre-training method \cite{du2024learning,escontrela2024video} yields reasonable results. In these approaches, a video prediction model generates future subgoals, which the policy then learns to achieve. Similarly, the goal image generation method \cite{black2023zero} utilizes images of subgoals instead of predicting entire video sequences for policy learning. However, both video prediction and goal image generation models often produce hallucinations and unrealistic physical movements. Additionally, these pre-training models demand significant computational resources, posing challenges particularly during inference. 
RT-trajectory \cite{gu2023rt} and ATM \cite{wen2023any} offer innovative perspectives on generating coarse or particle trajectories, which have proven effective and intuitive. Inspired by these approaches, our method introduces unique adaptations. 
Unlike RT-trajectory, which generates relatively coarse trajectories through image generation or sketch,and is accompanied by noise with relatively large errors, our method does not completely replace language instructions with coarse trajectories. Instead, we produce high-quality trajectories that can be directly used for end-to-end model inference. Additionally, we use particle trajectories rather than linear trajectories, allowing for more precise and flexible task execution.
In contrast to ATM, we model the entire task process using a single key point representing the end-effector's position in RGB. This key point groundtruth can be readily acquired through the utilization of the camera's intrinsic and extrinsic parameters. To unify the concept of 2D points or waypoints in the RGB domain, we refer to the sequences of key points from the start to the end of a task as 2D-particle trajectories (Fig.~\ref{fig:overview}(b)). Our method functions similarly to video prediction, serving as a plugin to enhance policy learning.

\textbf{Diffusion Model for Generation.}
Diffusion models in robotics are primarily utilized in two areas. Firstly, as previously discussed, they are used for generating future imagery in both video and goal image generation tasks. Secondly, diffusion models are applied to visuomotor policy development, as detailed in recent studies \cite{chi2023diffusionpolicy, reuss2024multimodal, octo_2023}. These applications highlight the versatility of diffusion models in enhancing robotic functionalities. 
Unlike other methods, our approach does not use diffusion models to directly generate the final policy. Given the high-dimensional semantic richness of language, we propose utilizing diffusion models to create a 2D-particle trajectory. This trajectory represents future end-gripper movements planing in the RGB domain.

\begin{figure*}
\begin{center}
%\vspace{-0.8cm}
\includegraphics[width=\linewidth]{fig/framework.png} 
\end{center}
\caption{\textbf{System architecture} for learning language-conditioned policies. a) shows the input modalities, including vision, language, and proprioception. b) describes the Diffusion Trajectory Model, detailing how vision and language inputs generate diffusion particle trajectories. c) explains how these trajectories guide the training of robot policies, focusing on the learning of the Diffusion Trajectory Policy. Masked learnable tokens represent the particle trajectory prediction token, action token, and video prediction token, respectively. 
% These masked tokens serve as the output of the policy.
}
\label{fig:framework} 
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{\uppercase{Method}}
% for example: Encoding motion behaviors as Gaussian Process models
\label{sec:method}

%\todo{Describe the work you have done in a way that other( student)s are able to re-implement it.}
% \todo{Describe the foundations, if necessary.\cite{kwon2024safe},\cite{wu2023unleashing}}
% \todo{Give sufficient technical details.}
% \todo{Include the underlying equations.}
% \todo{Add figures to make your description more easily understandable.}
% \todo{Mention the advantages of the approach.}
% \todo{Describe the complexity.}

Our goal is to create a policy that enables robots to handle long-horizon manipulation tasks by interpreting vision and language inputs. We simplify the VLA task using two distinct phases (Fig.~\ref{fig:framework}(b)(c)): a DTM learning phase and a DTP learning phase. First, we generate diffusion-based 2D particle trajectories for the task. Subsequently, in the second stage, these trajectories are used to guide the learning of the manipulation policy.

\subsection{Problem Formulation}
\textbf{Multi-Task Visual Robot Manipulation.} We consider the problem of learning a language-conditioned policy $\pi_\theta$ that take advantage of language instruction $l$, observation $\vo_t$, robot states $\vs_t$ and diffusion trajectory $\vp_{t:T}$ to generate a robot action $\va_t$:
\begin{equation}
\label{base_policy}	\pi_\theta(l,\vo_t,\vs_t,\vp_{t:T})\rightarrow\\\va_t\
\end{equation}
The robot receives language instructions detailing its objectives, such as "turn on the light bulb". The observation sequence, $\vo_{t-h:t}$, captures the environment's data from the previous $h$ time steps. The state sequence, $\vs_{t-h:t}$, records the robot's configurations, including the pose of the end-effector and the status of the gripper. The diffusion trajectory, $\vp_{t:T}$, predicts the future movement of the end-gripper from time $t$ to the task's completion at time $T$. Our dataset, $\sD$, comprises $n$ expert trajectories across $m$ different tasks, denoted as $\sD_m=\{\tau_i\}_{i=1}^n$. Each expert trajectory $\tau$ includes a language instruction along with a sequence of observation images, robot states, and actions: $\tau = \{\{l,\vo_1,\vs_1,\va_1\}\,...,\{l,\vo_T,\vs_T,\va_T\}\}$.
\subsection{Framework}
\label{Framework}
We introduce the Diffusion Trajectory-guided Policy, as illustrated in Fig.~\ref{fig:framework}. DTP operates within a two-stage framework. In the first stage, our primary focus is on generating the diffusion trajectory $\vp_{t:T}$ which outlines the motion trends essential for completing the task, as observed from a static perspective camera (Fig.~\ref{fig:framework}(b)). This 2D-particle trajectory serves as the guidance for subsequent policy learning.
We take a causal transformer as the backbone network which is designed to handle diverse modalities, processing inputs to predict future images and robotic actions with learnable observation and action query tokens respectively. It integrates CLIP \cite{radford2021learning} as the language encoder for processing language instructions $l$ and employs a MAE \cite{he2022masked} as the vision encoder for \(\vo_{t-h:t}\), both of which are with frozen parameters. The vision tokens are then processed with a perceiver resampler \cite{jaegle2021perceiver} to reduce their number. Additionally, it incorporates the robot's state \(\vs_{t-h:t}\) in world coordinates, as part of its input. All input modalities are shown in Fig.~\ref{fig:framework}(a).
% The reason for incorporating this baseline into our framework is detailed in Section \ref{Ablation Studies}. 
Our approach is divided into two main sections. Initially, we detail the process of learning a diffusion trajectory model from the dataset $\sD$ in Section \ref{Diffusion Trajectory Model}. Subsequently, in Section \ref{Diffusion Trajectory-guided Policy}, we illustrate how diffusion trajectories can be used to guide policy learning for long-horizon robot tasks. 
\subsection{Diffusion Trajectory Model}
\label{Diffusion Trajectory Model}
In the first stage (Fig.~\ref{fig:framework}(b)), we focus on generating diffusion trajectory that maps out the motion trends required for task completion, as viewed from a static perspective camera. To achieve this, we employ a model $\mM_d$ to transform language instructions $l$ and initial visual observations $\vo_t$ into a sequence of diffusion 2D-particle trajectories \(\vp_{t:T}\). These points indicate the anticipated movements for the remainder of the task:
\begin{equation}
	\mM_d(l,{\vo_t})\rightarrow\\\vp_{t:T}\
\label{eq:diff}
\end{equation}
\subsubsection{Data Preparation}
According to Eq. \ref{eq:diff}, our input consists of observations $\vo_t$ and language instruction $l$. For outputs, our aim is to determine the future 2D-particle trajectory $\vp_{t:T}$ of the end effector gripper for finishing the task. Recent advancements in video tracking work make it easy to monitor the end effector gripper \cite{yang2023track}. For enhanced convenience and precision, we achieve this by mapping the world coordinates $(x_w,y_w,z_w)$ to pixel-level positions $(x_c,y_c)$ according to camera’s intrinsic and extrinsic parameters in the static camera frame, as shown in (Fig.~\ref{fig:framework}(b)) right part. In the first stage, our data format is structured as \(\sD_{\text{trajectory}} = \{l, \vo_t, \vp_{t:T}\}\), facilitating straightforward acquisition of the sequence \(\vp_{t:T}\), thereby simplifying the process of training our model to accurately predict end effector positions.
\subsubsection{Training Objective}
Denoising Diffusion Probabilistic Models (DDPMs) \cite{ho2020denoising} constitute a class of generative models that predict and subsequently remove noise during the generation process. In our approach, we utilize a causal diffusion decoding structure \cite{chi2023diffusionpolicy} to generate diffusion 2D-particle trajectories $\vp_{t:T}$. Specifically, we initiate the generation process by sampling a Gaussian noise vector $x^K \sim \mathcal{N}(0, I)$ and proceed through $K$ denoising steps using a learned denoising network \( \epsilon_\theta(x^k, k) \) where $x^k$ represents the diffusion trajectory noised over $K$ steps. This network iteratively predicts and removes noise $K$ times, ultimately resulting in the output $x^0$, which denotes the complete removal of noise. The process is described in the equation below, where $\alpha$, $\gamma$, and $\sigma$ are parameters that define the denoising schedule:
\begin{equation}
x^{k-1} = \alpha (x^k - \gamma \epsilon_\theta (x^k, k)) + \mathcal{N}(0, \sigma^2 I)
\label{eq:base_diff}
\end{equation}
Eq. \ref{eq:base_diff}, illustrates the functioning of the basic diffusion model. For our application, we adapt this model to generate diffusion trajectories $\vp_{t:T}$ based on the observation $\vo_t$ and language instruction $l$:
\begin{equation}
\vp_{t:T}^{k-1} = \alpha (\vp_{t:T}^k - \gamma \epsilon_\theta (\vo_t,l,\vp_{t:T}^k, k)) + \mathcal{N}(0, \sigma^2 I)
\label{eq:2d_diff}
\end{equation}
During the training process, the loss is calculated as Mean Square Error (MSE), where $\epsilon_k$ represents Gaussian noise sampled randomly for step $k$:
\begin{equation}
\mathcal{L}_{DTM} = \text{MSE}(\epsilon_k, \epsilon_\theta(\vo_t, l, \vp_{t:T} + \epsilon_k, k))
\end{equation}

This transformation integrates our specific inputs into the diffusion process, enabling the tailored generation of diffusion trajectory in alignment with both the observed data and the provided language instruction.
This training loss ensures that diffusion 2D-particle trajectories are accurately generated by systematically reducing noise, thereby enhancing the precision of the final trajectory predictions.

\subsection{Diffusion Trajectory-guided Policy}
\label{Diffusion Trajectory-guided Policy}
In the second stage, we focus on illustrating how the diffusion trajectory guides the robot manipulation policy (Fig.~\ref{fig:framework}(c)). As previously outlined in our problem formulation, we define our task as a language-conditioned visual robot manipulation task. We base our Diffusion Trajectory-guided Policy on the GR-1 \cite{wu2023unleashing} model and incorporate our diffusion trajectory $\vp_{t:T}$ as an additional input, as specified in Eq. \ref{base_policy}.	

\textbf{Policy Input.} This consists of language and image inputs, as detailed in the Sec.~\ref{Framework} and shown in the left side of 
Fig.~\ref{fig:framework}(c). To clearly demonstrate our method's performance, we maintain the same configuration as GR-1.
Importantly, for the diffusion trajectory, we do not rely on the inference results from the first training stage. Instead, we use the labeled data from this stage as the diffusion trajectory. This approach enhances precision in training and conserves computational resources by using the labels directly. The simplest training approach is to inject the diffusion particle trajectory directly into the causal baseline. However, our fixed set of 2D particle trajectories $\vp_{t:T}$ can lead to computational intensity during training due to the high number of tokens. Inspired by the perceiver resampler \cite{jaegle2021perceiver}, we designed a diffusion trajectory resampler module to reduce the number of trajectory tokens, as shown in Fig.~\ref{fig:framework}(b) and (c). 

\textbf{Diffusion Trajectory as Policy Training.} During the policy learning phase (Fig.~\ref{fig:framework}(c)), we generate future particle trajectories 
to supervise the diffusion trajectory resampler module with \(\mathcal{L}_{\text{trajectory}}\). Our policy framework employs a causal transformer architecture, where future particle trajectory tokens are generated prior to action tokens with \(\mathcal{L}_{\text{action}}\).  This ensures that the particle trajectory tokens effectively guide the formation of action tokens, optimizing the action prediction process in a contextually relevant manner. Additionally, we retain the output of video prediction with \(\mathcal{L}_{\text{video}}\), maintaining the same setting as GR-1. This consistency in output makes it easier to conduct ablation studies, as we can directly compare our approach to the original GR-1 model. The optimal DTP objective can be expressed as the following equation:

\begin{equation}
\mathcal{L}_{DTP} = \mathcal{L}_{trajectory} + \mathcal{L}_{action} + \mathcal{L}_{video}
\end{equation}

Furthermore, to demonstrate the effectiveness and superiority of our method in the ablation study, we split the GR-1 baseline into two versions: one that is fully pretrained on the video dataset and another that only uses the GR-1 structure without any pretraining. We will discuss these two baseline configurations in Sec.~\ref{sec:experiment}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\uppercase{Experiment}}
\label{sec:experiment}

% \todo{Explain common data sets and setups used in the experiments}
% \todo{For each contribution listed in the introduction:\\
% - Give a detailed explanation of the individual experiments\\
% - Explain why you make the individual experiment. What contribution does it support?\\
% - Use graphs and tables to summarize your results.\\
% - Compare your approach to alternative ones\\
% - Perform statistical tests indicating that your approach is “significantly better than alternative techniques”.
% Does the data support our claim?\\
% - What is the intuition behind these results?\\
% - What are the implications?
% }


In this section, we evaluate the performance of Diffusion Trajectory Policy on the CALVIN benchmark~\cite{mees2022calvin} and real robot. 
\subsection{CALVIN Benchmark And Baseline}

CALVIN \cite{mees2022calvin} is a comprehensive simulated benchmark designed for evaluating language-conditioned policies in long-horizon robot manipulation tasks. It comprises four distinct yet similar environments (A,B,C, and D) which vary  in desk shades and item layouts, as shown in Fig.~\ref{fig:env}. This benchmark includes 34 manipulation tasks with unconstrained language instructions. Each environment features a Franka Emika Panda robot equipped with a parallel-jaw gripper, and a desk that includes a sliding door, a drawable drawer, color-varied blocks, an LED, and a light bulb, all of which can be interacted with or manipulated.


\textbf{Experiment Setup.} we train DTP to predict relative action in $xyz$ positions and Euler angles for arm movements, alongside binary actions for the gripper. The training dataset comprises over 20,000 expert trajectories from four scenes, each paired with language instruction labels. Our DTP method is assessed using the long-horizon benchmark, featuring 1,000 unique sequences of instruction chains articulated in natural language. Each sequence requires the robot to sequentially complete five tasks. 
% \begin{wrapfigure}{r}{0.4\textwidth}
%     \vspace{-0.3cm}
%     \centering
%     \includegraphics[width=\linewidth]{fig/env.png}
%     \caption{The top four environments correspond to the CALVIN ABCD settings, differing mainly in the positions of the sliding door, LED, bulb, light switch, button, and desk shades. The bottom section shows a sequence of five long-horizon tasks, each guided by a specific instruction.}
%     \label{fig:env}
% \end{wrapfigure}


\textbf{Baselines.} We compare our proposed policy against the following state-of-the-art language-conditioned multi-task policies on CALVIN: 
\textbf{MT-ACT} \cite{bharadhwaj2024roboagent} is a multitask transformer-based policy which predicts action chunk instead of single actions.
\textbf{HULC} \cite{mees2022matters} is a hierarchical approach which predicts latent features of subgoals based on language instructions and observation. 
\textbf{RT-1} \cite{brohan2022rt} represents the first approach that utilizes convolutional layers and transformers to generate actions in an end-to-end manner, integrating both language and observational inputs.
% It demonstrates the feasibility of an end-to-end vision-language-action framework in a structured method approach. 
\textbf{RoboFlamingo} \cite{li2023vision} is a fine-tuned Vision-Language Foundation model with 3 billion parameters. It has an additional recurrent policy head specifically designed for action prediction. 
\textbf{GR-1} \cite{wu2023unleashing} leverages pretraining on the Ego4D dataset, which contains massive-scale human-object interactions captured through web videos.
\textbf{3D Diffuser Actor} \cite{ke20243d} integrates 3D scene representations with diffusion objectives to learn robot policies from demonstrations. 
% This approach facilitates a comprehensive understanding and execution of complex manipulative tasks.
\begin{table}[t]
\centering
% \vspace{-0.4cm}
\caption{Summary of Experiments}
\label{tab:experiment_summary} % 给表格添加标签
\resizebox{0.5\textwidth}{!}{ % 调整表格宽度适应页面宽度
\begin{tabular}{c|c|ccccc|c}
\toprule

\textbf{Method} & \textbf{Experiment} & \multicolumn{5}{c}{\textbf{Tasks completed in a row}} & \textbf{Avg. Len.} \\
\cmidrule(lr){3-7}
& & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \\
\midrule
HULC     & D$\rightarrow$D & 0.827 & 0.649 & 0.504 & 0.385 & 0.283 & 2.64 \\
GR-1     & D$\rightarrow$D & 0.822 & 0.653 & 0.491 & 0.386 & 0.294 & 2.65 \\
MT-ACT   & D$\rightarrow$D & 0.884 & 0.722 & 0.572 & 0.449 & 0.353 & 3.03 \\
HULC++    & D$\rightarrow$D & 0.930 & 0.790 & 0.640 & 0.520 & 0.400 & 3.30 \\
\rowcolor{gray!30} DTP(Ours) & D$\rightarrow$D & 0.924 & 0.819 & 0.702 & 0.603 & 0.509 & 3.55 \\

\midrule
HULC             & ABC$\rightarrow$D & 0.418 & 0.165 & 0.057 & 0.019 & 0.011 & 0.67 \\
RT-1             & ABC$\rightarrow$D & 0.533 & 0.222 & 0.094 & 0.038 & 0.013 & 0.90 \\
RoboFlamingo     & ABC$\rightarrow$D & 0.824 & 0.619 & 0.466 & 0.380 & 0.260 & 2.69 \\
GR-1             & ABC$\rightarrow$D & 0.854 & 0.712 & 0.596 & 0.497 & 0.401 & 3.06 \\
3D Diffuser Actor & ABC$\rightarrow$D & 0.922 & 0.787 & 0.639 & 0.512 & 0.412 & 3.27 \\
\rowcolor{gray!30} DTP(Ours) & ABC$\rightarrow$D & 0.890 & 0.773 & 0.679 & 0.592 & 0.497 & 3.43 \\
\midrule
RT-1             & 10\%ABCD$\rightarrow$D &0.249&0.069&0.015&0.006&0.000&0.34 \\ 
HULC             & 10\%ABCD$\rightarrow$D &0.668&0.295&0.103&0.032&0.013&1.11 \\  
GR-1             & 10\%ABCD$\rightarrow$D &0.778&0.533&0.332&0.218&0.139&2.00 \\ 
\rowcolor{gray!30} DTP(Ours)     & 10\%ABCD$\rightarrow$D & 0.813 & 0.623 & 0.477 & 0.364 & 0.275 & 2.55 \\ 

\bottomrule
\end{tabular}
}
\caption*{\footnotesize This table details the performance of all baseline methods in sequentially completing 1, 2, 3, 4, and 5 tasks in a row. The average length, shown in the last column and calculated by averaging the number of completed tasks in a series of 5 across all evaluated sequences, illustrates the models' long-horizon capabilities. 10\%ABCD$\rightarrow$D indicates that only 10\% of the training data is used.}
\end{table}

\subsection{Comparisons with State-of-the-Art Methods}
\textbf{Known Scene Results.} This experiment is conducted in the D→D setting, utilizing about 5,000 expert demonstrations for training. The training process takes approximately 1.5 days on 8 NVIDIA 24G RTX 3090 GPUs. As shown in Tab.~\ref{tab:experiment_summary}, DTP significantly outperforms all baseline methods across all metrics in the context of long-horizon tasks. Specifically, DTP increases the success rate for Task 5 from 0.400 to 0.509 and raises the average successful sequence length from 3.30 to \textbf{3.55}. Notably, compared to GR-1, our baseline model, DTP enhances performance across all metrics, with the average sequence length increasing by 33.9\%. These results indicate that DTP demonstrates superior performance in long-horizon tasks, particularly as the task length increases. 

\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{fig/env2.png} 
\end{center}
\caption{The left four environments correspond to the CALVIN ABCD settings, differing mainly in the positions of the asserts. The right section shows a sequence of five long-horizon tasks, each guided by a specific instruction.}
\label{fig:env} 
\end{figure}

\textbf{Unseen Scene Results.} This experiment is conducted in the ABC→D setting, which is particularly challenging: models are trained using data from environments A, B, and C and then tested in environment D, an unseen setting during the training phase. The training process takes approximately 5 days on 8 NVIDIA 24GB RTX 3090 GPUs. We validated DTP's generalization capability in a new environment. The results are presented in Tab.~\ref{tab:experiment_summary}. When compared with the baseline GR-1, there is an increase in the average task completion length from 3.06 to \textbf{3.43}. Additionally, the success rate for completing Task 5 increased to 0.497, the highest recorded value. Notably, even though our method does not use depth modality for training, it outperformed the 3D Diffuser Actor in these tests. This underscores a critical insight: DTP can effectively guide policy learning for long-horizon robot tasks in unseen settings.

\textbf{Data Efficiency.} Robot data is more costly and scarce compared to vision-language data. To evaluate data efficiency, we trained using only 10\% of the full dataset in the ABCD→D setting, randomly selecting around 2,000 expert demonstrations from over 20,000 episodes. Training took approximately 1 day on 8 NVIDIA 24GB RTX 3090 GPUs. The results are shown in Tab.~\ref{tab:experiment_summary}. While performance declines for all methods compared to training on the full dataset, the best baseline method, GR-1, achieves a success rate of 0.778 with an average length of 2.00. DTP shows clear benefits for long-horizon tasks; as task numbers increase, the success rate rises, and the average length reaches \textbf{2.55}, outperforming other methods. This highlights DTP's data efficiency. Imitation learning helps the model learn positional preferences, which are essential in long-horizon tasks. When the robot starts from an unseen position, task failures are more likely. However, DTP guides the robot arm with a diffusion trajectory, providing the correct path. Thus, even with fewer demonstrations, DTP quickly acquires the skills.

\subsection{Ablation Studies}
\label{Ablation Studies}
In this section, we conduct ablation studies to evaluate how the diffusion trajectory improves policy learning in visual robot manipulation tasks. The diffusion trajectory, our key contribution, significantly boosts the efficiency of imitation policy training by fully utilizing available robot data. Furthermore, when integrated with large-scale pretraining baseline methods, our approach serves as a straightforward and effective enhancement to performance. To measure the effectiveness of our method, we contrast it with two fundamental baselines. The first baseline employs the GR-1 framework (Sec.~\ref{Framework}) without video pretraining, while the second utilizes large-scale video pretraining with the Ego4D dataset \cite{grauman2022ego4d}, also based on GR-1 framework. Two baselines are established to verify the efficacy and compatibility of our method with other approaches. 


\begin{table}[t]
\centering
\caption{Ablation Studies}
\label{tab:Ablation Studies} % 给表格添加标签
\resizebox{0.5\textwidth}{!}{ % 调整表格宽度适应页面宽度
\begin{tabular}{cc|c|ccccc|c}
\toprule
\textbf{Pre-Training}  & \textbf{DTP (Ours)} & \textbf{Data} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{Avg. Len.} \\
\midrule

$\times$  & $\times$  & ABC$\rightarrow$D & 0.815 & 0.651 & 0.498 & 0.392 & 0.297 & 2.65 \\
\rowcolor{gray!30}$\times$  & \checkmark  & ABC$\rightarrow$D & 0.869 & 0.751 & 0.636 & 0.549 & 0.465 & 3.27 \\
$\times$& $\times$ &10\%ABCD$\rightarrow$D &0.698&0.415&0.223&0.133&0.052&1.52\\
\rowcolor{gray!30}$\times$& \checkmark &10\%ABCD$\rightarrow$D &0.742&0.511&0.372&0.269&0.188&2.08\\

\midrule

\checkmark  & $\times$ & ABC$\rightarrow$D & 0.854 & 0.712 & 0.596 & 0.497 & 0.401 & 3.06 \\
\rowcolor{gray!30}\checkmark  & \checkmark & ABC$\rightarrow$D & 0.890 & 0.773 & 0.679 & 0.592 & 0.497 & 3.43 \\
\checkmark& $\times$ &10\%ABCD$\rightarrow$D &0.778&0.533&0.332&0.218&0.139&2.00\\  
\rowcolor{gray!30}\checkmark & \checkmark &10\%ABCD$\rightarrow$D&0.813 & 0.623 & 0.477 & 0.364 & 0.275 & 2.55 \\ 
\rowcolor{gray!30}\checkmark & 100\%\checkmark &10\%ABCD$\rightarrow$D& 0.822 & 0.643 & 0.526 & 0.416 & 0.302 & 2.71 \\ 

\bottomrule
\end{tabular}
}

\caption*{\footnotesize Pre-Training indicates whether we use only the baseline model structure or the baseline pre-trained on the Ego4D dataset. In our ablation studies, we established these two baselines to evaluate the effectiveness and compatibility of our DTM method with other approaches. 10\%ABCD$\rightarrow$D indicates that only 10\% of the training data is used. 100\%\checkmark indicates DTM trained on full ABCD$\rightarrow$D.}
\end{table}



\textbf{Diffusion Trajectory Policy Scratch.} 
First, we evaluate our method in the ABC→D and 10\% ABCD→D settings, as shown in the upper part of Tab.~\ref{tab:Ablation Studies}.
The results demonstrate that our diffusion trajectory method significantly enhances performance even without any pretraining. Specifically, our method not only excels in sequentially completed tasks but also shows notable gains in the average task completion length for long-horizon tasks increase of 23.4\%. Notably, the success rate for the task 5, which is indicative of the overall long-horizon success, has risen by 56.6\%. When compared with the 3D Diffuser Actor, as shown in Tab.~\ref{tab:experiment_summary}, despite not utilizing depth modality. This highlights our method's efficiency and capability in handling complex robot manipulation tasks without the need for depth data.

\textbf{Diffusion Trajectory Policy with Video Pretrain.} 
As illustrated in the bottom part of Tab.~\ref{tab:Ablation Studies}, 
the variants utilizing our diffusion trajectory effectively serve as a plugin, boosting baseline model performance to state-of-the-art levels. We evaluated our method under both the ABC→D and 10\% ABCD→D settings, and the results consistently show improvements over the traditional scratch training method. This clearly indicates that our approach complements and significantly enhances baseline performance across various benchmarks. Additionally, the success rates for each subsequent task show notable increases, with the growth rate rising from 4.2\% in the first task to 23.9\% in the fifth task. These outcomes further validate that DTP can substantially improve performance in long-horizon manipulation tasks.

\textbf{Diffusion Trajectory Model Scaling Law.} 
The last row highlights the initial training stage of our Diffusion Trajectory Model. Increasing the training data allows the model to generate more accurate points, enhancing the DTP. The bottom row demonstrates that even with limited demonstration data for imitation learning, scaling up the training for the diffusion trajectory can significantly improve both the success rate and average task completion length. This experimental setup points to a potential direction: although robot demonstration data is costly to obtain, the data for the DTM is relatively easy to annotate. Individuals only need to sketch a coarse trajectory on an RGB image and associate it with relevant language instructions. 

\begin{figure}
\begin{center}
\includegraphics[width=\linewidth]{fig/Franka_env.png} 
\end{center}
\caption{Real-world robotic setup. A Franka Emika Panda robot equipped with three Intel RealSense D435i cameras (left, right, and top views) and a Robotiq gripper.}
\label{fig:franka_env} 
\end{figure}

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=\linewidth]{fig/real_robot_tasks.png} 
\end{center}
\caption{Five distinct manipulation tasks, \textit{PickBread}, \textit{PickStrawberry}, \textit{OpenTrash},  \textit{CloseSideDrawer},  \textit{OpensideDrawer}, including object transportation and articulated object manipulation.}
\label{fig:real_robot_tasks} 
\end{figure}


%Real robot
\subsection{Real Robot Experiment}

% To evaluate the real-world performance of DTP, we conducted five practical task experiments. Alongside the DTP, we tested three baseline methods for comparison. %This setup enables a thorough assessment of our method’s effectiveness relative to existing approaches, offering clear insights into its practical utility.



\textbf{Experiment Setup.} Our real-world robotic system, depicted in Fig.~\ref{fig:franka_env}, consists of a Franka Emika Panda robot equipped with three Intel RealSense D435i cameras (left, right, and top views) and a Robotiq gripper.  First, We collected 1086 demonstrations for all tasks by teleoperation system~\cite{wu2024gello}. Specifically, we collected 290, 258, 100, 184, and 254 demonstrations for five tasks—\textit{PickBread}, \textit{PickStrawberry}, \textit{OpenTrash}, \textit{CloseSideDrawer}, and \textit{OpenSideDrawer} respectively, encompassing object transportation and articulated object manipulation, as illustrated in Fig.~\ref{fig:real_robot_tasks}. Training required approximately 1 day on 4 NVIDIA 24GB RTX3090 GPUs with 20 epochs.



\textbf{Results.} The performance of DTP and baseline methods is summarized in Tab.~\ref{tab:real_robot_results}. Each task was evaluated over 10 trials, with success rates calculated for comparison. Overall, DTP achieved the highest aggregate success rate across tasks. However, in the \textit{PickStrawberry} task, DTP underperformed compared to ACT. We attribute this to the small size of the target object, as DTP uses an image input resolution of 224x224, while ACT operates at a higher resolution of 480x640, which likely impacts performance. In long-horizon tasks, the robot arm’s initial pose is determined by the completion of the previous task, resulting in random starting configurations. To evaluate DTP’s robustness in such scenarios, we tested it on the \textit{OpenSideDrawer} task with randomized initial arm poses. DTP achieved a success rate twice as high as the second-best method. Additionally, in the \textit{OpenTrash} task, which requires precise alignment to a specific area to open the trash bin, DTP demonstrated superior guidance capabilities. While other baseline methods positioned the arm near the target, they often failed to locate the precise opening mechanism, leading to task failure.

\textbf{Visualization of Diffusion Trajectory Model.} As shown in Fig.~\ref{fig:visualization}, we present the overall visualization of the diffusion trajectory generation phase, tested in both the Calvin environment and real-world scenarios. The visualizations demonstrate that the trajectories generated by our diffusion trajectory prediction closely match the ground truth. Even when minor deviations occur, the generated trajectories still align with the robotic arm paths dictated by the language instructions.





\begin{table}
	\caption{Summary of Real Robot Experiments}
    \resizebox{0.5\textwidth}{!}{ % 调整表格宽度适应页面宽度
	\begin{tabular}{l|ccccc}
		\toprule
		\diagbox [width=5em,trim=l] {Method}{Tasks} & PickBread & PickStrawberry & OpenTrash   & CloseSideDrawer  & OpenSideDrawer*  \\
		\hline
		ACT       & 0.7 & 0.9 & 0.3 & 0.3 & 0.4  \\
		BAKU\cite{haldar2024baku}      & 0.0 & 0.5 & 0.2 & 0.2 & 0.3 \\
		GR1       & 0.7 & 0.7 & 0.2 & 0.4 & 0.4  \\
        \rowcolor{gray!30} DTP(Ours) & {\bf 0.8} & 0.8 & {\bf 0.9} & {\bf 0.9} & {\bf 0.8}  \\
		\bottomrule
	\end{tabular}\vspace{0cm}
    }
\caption*{\footnotesize OpenSideDrawer* means robot initial pose is random}
\label{tab:real_robot_results}
\end{table}
%\vspace{-5pt} 

\begin{figure}
\begin{center}

\includegraphics[width=\linewidth]{fig/2d_trajectory_visualization.png} 
\end{center}

\caption{\textbf{Diffusion Trajectory Visualization.} The upper section illustrates diffusion trajectory generation in the CALVIN environment, while the lower section depicts trajectory generation in a real-world robotic scenario.}
\label{fig:visualization} 
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\uppercase{Conclusion}}
\label{sec:conclusion}
% \todo{Again describe the approach presented in this paper}
% \todo{Again mention the advantages and what is novel compared to previous approaches}
% \todo{Mention the implementation and the successful outcome of the experiments}
% \todo{Potentially discuss options for future work\\
% - Don’t be too critical on your own work\\
% - Don’t be too enthusiastic about what else could and maybe should have been done.}
The limited availability of robot data poses significant challenges in generalizing long-horizon tasks to unseen robot poses and environments. This paper introduces a diffusion trajectory-guided framework that utilizes diffusion trajectories, generated in the RGB domain, to enhance policy learning in long-horizon robot manipulation tasks. This method facilitates the creation of additional training data through data augmentation or manually crafted labels, thereby generating more accuracy diffusion trajectories. Our approach involves two main stages: first, training a diffusion trajectory model to generate task-relevant trajectories; second, using these trajectories to guide the robot's manipulation policy. We validated our method through extensive experiments on the CALVIN simulation benchmark, where it outperformed state-of-the-art baselines by an average success rate of 25\% across various settings. Our results confirm that our method not only substantially improves performance using only robot data but also effectively complements and enhances baseline performance across various settings in the CALVIN benchmarks. Moreover, our method brings about a remarkable enhancement in the performance of real-world robots.


In future work, we plan to extend our method to other state-of-the-art policies, as we believe that incorporating diffusion trajectories will further enhance their effectiveness. Another potential direction is to obtain the diffusion trajectory label using the camera’s intrinsic and extrinsic parameters, which are not fully available from open-source datasets \cite{padalkar2023open}. 
Recently, Track-Anything \cite{yang2023track} demonstrated strong capabilities in tracking arbitrary objects. We could adopt this method to generate diffusion trajectory labels. Furthermore, with similar tracking methods, we can pretrain on large-scale video datasets to train our diffusion trajectory tasks, similar to video prediction tasks. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{template/IEEEtran}
% \bibliography{template/IEEEabrv,DTP}
\bibliography{DTP}
\end{document}

