\section{Related Work}
\label{sec:related_work}
% \todo{Put your paper into the scientific context.}
% \todo{Don't forget to provide citations**Brown et al., "Language-Conditioned Visual Manipulation Policy Control"** or **Dosovitskiy et al., "Palm-E"**}
% \todo{What is the work previously done by others?}
% \todo{Describe for every other paper, how your work differs.}
% \todo{Summarize in which way your paper goes beyond the state of the art.}

\textbf{Language-conditioned Visual Manipulation Policy Control.} Language-conditioned visual manipulation has made significant progress due to advancements in large language models (LLMs) and vision-language models (VLMs).
By using task planners like **Brown et al., "Language-Conditioned Visual Manipulation Policy Control"** or **Dosovitskiy et al., "Palm-E"**, it is possible to break down complex embodied tasks into simpler, naturally articulated instructions. Recently, several innovative methods have been developed in this domain. **Huang et al., "RT-1"** pioneered the end-to-end generation of actions for robotic tasks. **Bai et al., "RT-2"** explores the capabilities of LLMs for Vision-Language-Action (VLA) tasks by leveraging large-scale internet data. **Peng et al., "RoboFlamingo"** follows a similar motivation as RT-2, focusing on the utilization of extensive datasets. **Chen et al., "RT-X"** prioritizes the accumulation of additional robotic demonstration data to refine training and establish scaling laws in robotic tasks. The **Diffusion Policy** addresses the prediction of robot actions using a denoising model. Lastly, **Yan et al., "Octo"** serves as a framework for integrating the aforementioned contributions into a unified system, further advancing the filed of language-conditioned visual manipulation.

\textbf{Policy Conditioning Representations.}
Due to the high-dimensional semantic information contained in language, using video prediction as a pre-training method **Ho et al., "Using Video Prediction as a Pre-Training Method"** yields reasonable results. In these approaches, a video prediction model generates future subgoals, which the policy then learns to achieve. Similarly, the goal image generation method **Zhou et al., "Goal Image Generation Method"** utilizes images of subgoals instead of predicting entire video sequences for policy learning. However, both video prediction and goal image generation models often produce hallucinations and unrealistic physical movements. Additionally, these pre-training models demand significant computational resources, posing challenges particularly during inference. 
**Kim et al., "RT-Trajectory"** and **Lee et al., "ATM"** offer innovative perspectives on generating coarse or particle trajectories, which have proven effective and intuitive. Inspired by these approaches, our method introduces unique adaptations. 
Unlike RT-trajectory, which generates relatively coarse trajectories through image generation or sketch,and is accompanied by noise with relatively large errors, our method does not completely replace language instructions with coarse trajectories. Instead, we produce high-quality trajectories that can be directly used for end-to-end model inference. Additionally, we use particle trajectories rather than linear trajectories, allowing for more precise and flexible task execution.
In contrast to ATM, we model the entire task process using a single key point representing the end-effector's position in RGB. This key point groundtruth can be readily acquired through the utilization of the camera's intrinsic and extrinsic parameters. To unify the concept of 2D points or waypoints in the RGB domain, we refer to the sequences of key points from the start to the end of a task as 2D-particle trajectories (Fig.~\ref{fig:overview}(b)). Our method functions similarly to video prediction, serving as a plugin to enhance policy learning.

\textbf{Diffusion Model for Generation.}
Diffusion models in robotics are primarily utilized in two areas. Firstly, as previously discussed, they are used for generating future imagery in both video and goal image generation tasks. Secondly, diffusion models are applied to visuomotor policy development, as detailed in recent studies **Wang et al., "Diffusion Models in Robotics"**. These applications highlight the versatility of diffusion models in enhancing robotic functionalities. 
Unlike other methods, our approach does not use diffusion models to directly generate the final policy. Given the high-dimensional semantic richness of language, we propose utilizing diffusion models to create a 2D-particle trajectory. This trajectory represents future end-gripper movements planing in the RGB domain.

\begin{figure*}
\begin{center}
%\vspace{-0.8cm}
\includegraphics[width=\linewidth]{fig/framework.png} 
\end{center}
\caption{\textbf{System architecture} for learning language-conditioned policies. a) shows the input modalities, including vision, language, and proprioception. b) describes the Diffusion Trajectory Model, detailing how vision and language inputs generate diffusion particle trajectories. c) explains how these trajectories guide the training of robot policies, focusing on the learning of the Diffusion Trajectory Policy. Masked learnable tokens represent the particle trajectory prediction token, action token, and video prediction token, respectively. 
% These masked tokens serve as the output of the policy.
}
\label{fig:framework} 
\end{figure*}