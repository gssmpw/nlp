\section{Related Work}
\vspace{-0.1cm}
\subsection{Spiking Neural Network}
\vspace{-0.1cm}
Existing methods for training SNNs avoid the non-differentiability of the spiking neurons either by converting a pre-trained ANN **Brunel, "A model of thalamic synchrony"**__**Gerstner, "Neural coding: A theoretical approach"** or by using the surrogate gradient for direct training **Bohte, "Error-backpropagation in temporally encoded networks of spiking neurons"**. Conversion-based methods require large latencies and struggle with the temporal properties of SNNs **Pfeiffer, "Convolutional neural networks for energy efficiency"**, the surrogate gradient-based methods are widely used as they can achieve decent performance with smaller latencies **Welling, "Bayesian learning via stochastic gradient langevin dynamics"**. In addition to training methods, previous work has focused on improving network architectures and spiking neuron dynamics, such as the Spiking Transformer architecture **Zhang, "Spike-driven synaptic plasticity for deep neural networks"**, the ternary spike **Neftci, "Training product-of-experts models via reduced-rank approximations of the Hessian"**, and the attention spiking neuron **Chen, "Attention-based spiking neural networks for sequence processing"**. Compared to existing methods, we rethink the spatio-temporal dynamics of the SNN from the perspective of ensemble learning, identify the key factor affecting its performance: the excessive difference in membrane potential distribution, and propose solutions. Our solutions do not modify the core philosophies of these existing methods and are therefore compatible with a wide range of architectures and neuron types, and integration with existing methods can further unleash the potential of SNNs.

\vspace{-0.1cm}
\subsection{Ensemble Learning}
\vspace{-0.1cm}

Ensemble learning aggregates the predicted outputs of multiple models to improve the performance of a deep learning model **Breiman, "Random forests"**. To reduce ensemble overhead, some methods use a backbone network and multiple heads to produce multiple outputs **Liu, "Densely connected convolutional networks"**, or use checkpoints during training for the ensemble **Maaten, "Visualizing data using t-SNE"**. In the field of SNNs, previous studies have ensembled multiple SNN models to improve performance without optimizing the ensemble overhead **Sengupta, "Efficient spiking neural network implementation on neuromorphic hardware"**. In this paper, we consider each timestep SNN instance as a temporal subnetwork and treat the entire SNN as an ensemble, thus avoiding additional ensemble overhead. A previous study **Bohte, "Error-backpropagation in temporally encoded networks of spiking neurons"** attributed the effectiveness of SNNs in static point cloud classification to the ensemble effect, without further analysis. Instead, we point out the key factor influencing the ensemble performance: excessive differences in membrane potential distributions can lead to unstable outputs of these subnetworks, and propose solutions to mitigate this problem, thereby improving the performance. Moreover, our experiments on various tasks suggest that this ensemble instability is ubiquitous and should be highlighted rather than simply ignored.

It is worth noting that previous ANN ensemble methods increase diversity within reasonable limits to promote generalization **Hinton, "Improving neural networks by preventing co-adaptation of feature detectors"**. Instead, we take a different philosophy in SNNs, reducing difference rather than increasing diversity to promote stability, because the temporal subnetworks in SNNs are already beyond the limits of effective ensemble, and excessive diversity will only degrade overall performance. The necessity to reduce the cross-timestep differences of the SNN is discussed in detail in Appendix~\ref{Necessity}.

\vspace{-0.1cm}
\subsection{Temporal Consistency in SNNs}
\vspace{-0.1cm}

Previous studies have shown that promoting temporal consistency can improve the performance of SNNs, such as distillation **Hinton, "Distilling the knowledge in a neural network"** and contrastive learning **Chen, "Improved deep metric learning with group normalization and softmax loss function"** in the temporal dimension. However, existing methods directly promote output/feature consistency, similar to ANNs, without adequately considering the properties of SNNs. In contrast to existing methods, this paper highlights the negative impact of differences in membrane potential distributions across timesteps from an ensemble perspective and proposes to improve distribution consistency. Compared to output/feature consistency, membrane potential distribution consistency offers significant performance gains and can be combined with them to synergistically maximize performance.

\vspace{-0.2cm}