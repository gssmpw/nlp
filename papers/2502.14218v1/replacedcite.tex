\section{Related Work}
\vspace{-0.1cm}
\subsection{Spiking Neural Network}
\vspace{-0.1cm}
Existing methods for training SNNs avoid the non-differentiability of the spiking neurons either by converting a pre-trained ANN____ or by using the surrogate gradient for direct training____. Conversion-based methods require large latencies and struggle with the temporal properties of SNNs____, the surrogate gradient-based methods are widely used as they can achieve decent performance with smaller latencies____. In addition to training methods, previous work has focused on improving network architectures and spiking neuron dynamics, such as the Spiking Transformer architecture____, the ternary spike____, and the attention spiking neuron____. Compared to existing methods, we rethink the spatio-temporal dynamics of the SNN from the perspective of ensemble learning, identify the key factor affecting its performance: the excessive difference in membrane potential distribution, and propose solutions. Our solutions do not modify the core philosophies of these existing methods and are therefore compatible with a wide range of architectures and neuron types, and integration with existing methods can further unleash the potential of SNNs.

\vspace{-0.1cm}
\subsection{Ensemble Learning}
\vspace{-0.1cm}

Ensemble learning aggregates the predicted outputs of multiple models to improve the performance of a deep learning model____. To reduce ensemble overhead, some methods use a backbone network and multiple heads to produce multiple outputs____, or use checkpoints during training for the ensemble____. In the field of SNNs, previous studies have ensembled multiple SNN models to improve performance without optimizing the ensemble overhead____. In this paper, we consider each timestep SNN instance as a temporal subnetwork and treat the entire SNN as an ensemble, thus avoiding additional ensemble overhead. A previous study____ attributed the effectiveness of SNNs in static point cloud classification to the ensemble effect, without further analysis. Instead, we point out the key factor influencing the ensemble performance: excessive differences in membrane potential distributions can lead to unstable outputs of these subnetworks, and propose solutions to mitigate this problem, thereby improving the performance. Moreover, our experiments on various tasks suggest that this ensemble instability is ubiquitous and should be highlighted rather than simply ignored.

It is worth noting that previous ANN ensemble methods increase diversity within reasonable limits to promote generalization____. Instead, we take a different philosophy in SNNs, reducing difference rather than increasing diversity to promote stability, because the temporal subnetworks in SNNs are already beyond the limits of effective ensemble, and excessive diversity will only degrade overall performance. The necessity to reduce the cross-timestep differences of the SNN is discussed in detail in Appendix~\ref{Necessity}.

\vspace{-0.1cm}
\subsection{Temporal Consistency in SNNs}
\vspace{-0.1cm}

Previous studies have shown that promoting temporal consistency can improve the performance of SNNs, such as distillation____ and contrastive learning____ in the temporal dimension. However, existing methods directly promote output/feature consistency, similar to ANNs, without adequately considering the properties of SNNs. In contrast to existing methods, this paper highlights the negative impact of differences in membrane potential distributions across timesteps from an ensemble perspective and proposes to improve distribution consistency. Compared to output/feature consistency, membrane potential distribution consistency offers significant performance gains and can be combined with them to synergistically maximize performance.

\vspace{-0.2cm}