
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{threeparttable}
\usepackage{wrapfig}
\usepackage{colortbl}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{newfloat}

\usepackage{listings}
\usepackage[caption=false,font=footnotesize]{subfig}
\newcommand{\dprime}{{\prime\prime}}
\usepackage{amssymb}
\usepackage{xcolor}
\definecolor{newgreen}{RGB}{0,176,80}
\definecolor{newblue}{RGB}{0,176,240}
\definecolor{ourcolor}{HTML}{f58220}
\colorlet{ourcolor}{ourcolor!10}

% \usepackage{algorithmic}

\title{Rethinking Spiking Neural Networks from an Ensemble Learning Perspective}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
\iclrfinalcopy
\author{Yongqi Ding, Lin Zuo\thanks{Corresponding author: linzuo@uestc.edu.cn. Emails: Yongqi Ding (yqding@std.uestc.edu.cn), Mengmeng Jing (jingmeng1992@gmail.com), Pei He (hepei@std.uestc.edu.cn), Hanpu Deng (hpdeng@std.uestc.edu.cn).} , Mengmeng Jing, Pei He, Hanpu Deng  \\
School of Information and Software Engineering\\
University of Electronic Science and Technology of China\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\definecolor{ccr}{RGB}{10,110,150}
\hypersetup{hypertex=true,
    colorlinks=true,
    citecolor=ccr}


%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Spiking neural networks (SNNs) exhibit superior energy efficiency but suffer from limited performance. In this paper, we consider SNNs as ensembles of temporal subnetworks that share architectures and weights, and highlight a crucial issue that affects their performance: excessive differences in initial states (neuronal membrane potentials) across timesteps lead to unstable subnetwork outputs, resulting in degraded performance. To mitigate this, we promote the consistency of the initial membrane potential distribution and output through membrane potential smoothing and temporally adjacent subnetwork guidance, respectively, to improve overall stability and performance. Moreover, membrane potential smoothing facilitates forward propagation of information and backward propagation of gradients, mitigating the notorious temporal gradient vanishing problem. Our method requires only minimal modification of the spiking neurons without adapting the network structure, making our method generalizable and showing consistent performance gains in 1D speech, 2D object, and 3D point cloud recognition tasks. In particular, on the challenging CIFAR10-DVS dataset, we achieved 83.20\% accuracy with only four timesteps. This provides valuable insights into unleashing the potential of SNNs.
\end{abstract}

\section{Introduction}
As the third generation of neural networks, spiking neural networks (SNNs) transmit discrete spikes between neurons and operate over multiple timesteps~\citep{maass1997networks}. Benefiting from the low power consumption and spatio-temporal feature extraction capability, SNNs have achieved widespread applications in spatio-temporal tasks~\citep{wang2024autaptic,chakraborty2024sparse}. In particular, ultra-low latency and low-power inference can be achieved when SNNs are integrated with neuromorphic sensors and neuromorphic chips~\citep{yao2023spikedriven,ding2024shrinking}.

To advance the performance of SNNs, previous work has improved the training method~\citep{STBP,bu2022optimal,TRT}, network architecture~\citep{yao2023spikedriven,Shi_2024_CVPR}, and neuron dynamics~\citep{taylor2023addressing,PLIF,ding2023improved} to significantly reduce the performance gap between SNNs and artificial neural networks (ANNs). Typically, these methods treat the spiking neurons in an SNN as an activation function that evolves over timesteps $T$, with the membrane potential expressing a continuous neuronal state. In this paper, we seek to rethink the spatio-temporal dynamics of SNNs from an alternative perspective: ensemble learning, and explore the key factor influencing the ensemble to optimize its performance.

For an SNN $f(\theta)$, its instance $f(\theta)_t$ produces an output $O_t$ at each timestep $t$, we consider this instance to be a temporal subnetwork. In this way, we can obtain a collection of $T$ temporal subnetworks with the same architecture $f(\cdot)$ and parameters $\theta$: $\{f(\theta)_1,f(\theta)_2,\cdots,f(\theta)_T\}$. In general, the final output of the SNN is the average of the outputs over $T$ timesteps: $O=\frac{1}{T}\sum_{t=1}^T{O_t}$. We view this averaging operation as an ensemble strategy: averaging the different outputs of multiple models improves overall performance~\citep{rokach2010ensemble,allen2020towards}. From this perspective, we can consider an SNN as an ensemble of $T$ temporal subnetworks. 

Temporal subnetworks share architecture and parameters, and their output variance arises from their different neuronal states, i.e., different membrane potentials $U(t)$ at each timestep trigger different output spikes $S(t)$. For example, for a leaky integrate-and-fire (LIF) neuron~\citep{STBP} (see Section~\ref{neuronmodel}) with an initial membrane potential of 0 and a time constant and firing threshold of 2.0 and 1.0, respectively, the neuron will produce spikes $\{0,1,1,0,1\}$ even if inputs of intensity 0.8 are repeated over 5 timesteps. In light of this, we have identified a factor that is usually overlooked, but has a major impact on the performance of this temporal subnetwork ensemble learning: \textit{if the difference in membrane potentials across timesteps is too large, it will lead to unstable outputs and thus affect the ensemble performance.} In particular, the initial membrane potential is usually set to 0~\citep{STBP,ding2024shrinking}, which leads to a drastic discrepancy in the membrane potential for the first two timesteps, thus degrading the performance of the SNN. To illustrate this phenomenon, we have visualized the membrane potential distribution in Fig~\ref{fig:distribution}(Top), where the distribution differences across timesteps can be clearly seen. In Table~\ref{tab:Timestep}, we further explore the performance of the trained SNN for inference with 1 to 5 timesteps. The results show that the output of the vanilla SNN is poorly informative at the first timestep and only achieves decent performance after integrating subsequent temporal subnetworks with smaller differences in the membrane potential distribution. Additionally, the output is visualized in Fig.~\ref{fig:tsne}, again showing that the first two timestep outputs are confusing and thus affect the overall output. Therefore, to improve overall performance, the problem of excessive difference in membrane potential distribution and the resulting output should be mitigated rather than simply ignored.


\begin{figure}[t]
  \centering
  \includegraphics[width=0.2\linewidth]{vanilla_T0.pdf}\hspace{-1.34mm}
  \includegraphics[width=0.2\linewidth]{vanilla_T1.pdf}\hspace{-1.34mm}
  \includegraphics[width=0.2\linewidth]{vanilla_T2.pdf}\hspace{-1.34mm}
  \includegraphics[width=0.2\linewidth]{vanilla_T3.pdf}\hspace{-1.34mm}
  \includegraphics[width=0.2\linewidth]{vanilla_T4.pdf}
  \\
  \includegraphics[width=0.2\linewidth]{our_T0.pdf}\hspace{-1.34mm}
  \includegraphics[width=0.2\linewidth]{our_T1.pdf}\hspace{-1.34mm}
  \includegraphics[width=0.2\linewidth]{our_T2.pdf}\hspace{-1.34mm}
  \includegraphics[width=0.2\linewidth]{our_T3.pdf}\hspace{-1.34mm}
  \includegraphics[width=0.2\linewidth]{our_T4.pdf}\\
  \caption{Membrane potential distribution on CIFAR10-DVS, where $\mu$ and $\sigma$ denotes the mean and standard deviation, respectively. \textbf{Top}: The membrane potential distribution of the vanilla SNN varies greatly across timesteps, which affects performance. \textbf{Bottom}: Our method allows for a more stable distribution with smaller differences across timesteps. See Appendix~\ref{addvis} for more visualizations.}
  \label{fig:distribution}
\vskip -0.2in
\end{figure}

\begin{wraptable}{r}{6cm}
\vspace{-0.75cm}
\tabcolsep=0.008\columnwidth
  \caption{Comparison of SNNs with 1 to 5 inference timesteps.}
   \scalebox{0.84}{
  \begin{tabular}{cccccc}
    \toprule
     & T=1 & T=2 & T=3 & T=4 & T=5\\
    \midrule
    Vanilla SNN & 10.00 & 60.10 & 69.50 & 73.30 & 74.10\\
    Random MP & 11.90 & 61.50 & 71.40 & 72.30 & 74.90\\
    \hline
    \rowcolor{pink!20}Ours & 66.60 & 74.30 & 75.50 & 75.70 & 76.60\\
    \bottomrule
  \end{tabular}
  }
\label{tab:Timestep}
\vspace{-0.3cm}
\end{wraptable}

To this end, we propose membrane potential smoothing to reduce the difference in membrane potential distribution and thus improve the overall ensemble performance. At each timestep, we adaptively smooth the membrane potentials using that of the previous timestep, pushing the initial states of these temporal subnetworks more consistent and preventing them from producing outputs with excessive variances. We visualize the smoothed membrane potential in Fig.~\ref{fig:distribution}(Bottom), where the obtained membrane potential is shown to be smoother than the vanilla SNN. Meanwhile, membrane potential smoothing creates new pathways for forward propagation of information and backward propagation of gradients (see Fig.~\ref{fig:MPsmoothing}), alleviating temporal gradient vanishing~\citep{Meng_2023_ICCV,huang2024clif} and boosting the performance from another side. In addition, we propose to guide temporally adjacent subnetworks through distillation, encouraging them to produce more consistent outputs, further enhancing ensemble stability and performance. Notably, membrane potential smoothing employs only one additional smoothing coefficient, and the distillation guidance only modifies the training loss without affecting the architecture, making our method superiorly generalizable. Extensive experiments in 1D, 2D, and 3D scenarios demonstrate the effectiveness, generalizability, and performance advantages of our method. Our contribution can be summarized as follows:
\begin{itemize}
    \item We consider the SNN as an ensemble of multiple temporal subnetworks and point out that excessive differences in membrane potential distributions across timesteps, and hence output instability, is the key factor affecting performance.
    \item We propose membrane potential smoothing and temporally adjacent subnetwork guidance to adaptively reduce membrane potential differences and enhance output consistency across timesteps, respectively, while facilitating the propagation of forward information and backward gradients to improve ensemble stability and overall performance.
    \item Extensive experiments on neuromorphic 1D speech/2D object recognition, and 3D point cloud classification tasks confirm the effectiveness, versatility, and performance advantages of our method. With only 4 timesteps, we achieved 83.20\% accuracy on the challenging CIFAR10-DVS dataset.
\end{itemize}

\vspace{-0.2cm}
\section{Related Work}

\vspace{-0.1cm}
\subsection{Spiking Neural Network}
\vspace{-0.1cm}
Existing methods for training SNNs avoid the non-differentiability of the spiking neurons either by converting a pre-trained ANN~\citep{rueckauer2017conversion} or by using the surrogate gradient for direct training~\citep{STBP}. Conversion-based methods require large latencies and struggle with the temporal properties of SNNs~\citep{deng2021optimal,bu2022optimal,you2024spikeziptf}, the surrogate gradient-based methods are widely used as they can achieve decent performance with smaller latencies~\citep{taylor2023addressing,TSSD,hu2024highperformance}. In addition to training methods, previous work has focused on improving network architectures and spiking neuron dynamics, such as the Spiking Transformer architecture~\citep{yao2023spikedriven,Shi_2024_CVPR}, the ternary spike~\citep{guo2024ternary}, and the attention spiking neuron~\citep{ding2023improved}. Compared to existing methods, we rethink the spatio-temporal dynamics of the SNN from the perspective of ensemble learning, identify the key factor affecting its performance: the excessive difference in membrane potential distribution, and propose solutions. Our solutions do not modify the core philosophies of these existing methods and are therefore compatible with a wide range of architectures and neuron types, and integration with existing methods can further unleash the potential of SNNs.

\vspace{-0.1cm}
\subsection{Ensemble Learning}
\vspace{-0.1cm}

Ensemble learning aggregates the predicted outputs of multiple models to improve the performance of a deep learning model~\citep{rokach2010ensemble,allen2020towards}. To reduce ensemble overhead, some methods use a backbone network and multiple heads to produce multiple outputs~\citep{tran2020hydra,RuanSMAIFD23}, or use checkpoints during training for the ensemble~\citep{furlanello2018born,lee2022learning}. In the field of SNNs, previous studies have ensembled multiple SNN models to improve performance without optimizing the ensemble overhead~\citep{neculae2021ensemblesspikingneuralnetworks,9308568,7966177,10160749}. In this paper, we consider each timestep SNN instance as a temporal subnetwork and treat the entire SNN as an ensemble, thus avoiding additional ensemble overhead. A previous study~\citep{ren2023spiking} attributed the effectiveness of SNNs in static point cloud classification to the ensemble effect, without further analysis. Instead, we point out the key factor influencing the ensemble performance: excessive differences in membrane potential distributions can lead to unstable outputs of these subnetworks, and propose solutions to mitigate this problem, thereby improving the performance. Moreover, our experiments on various tasks suggest that this ensemble instability is ubiquitous and should be highlighted rather than simply ignored.

It is worth noting that previous ANN ensemble methods increase diversity within reasonable limits to promote generalization~\citep{9677845,NEURIPS2020_b86e8d03}. Instead, we take a different philosophy in SNNs, reducing difference rather than increasing diversity to promote stability, because the temporal subnetworks in SNNs are already beyond the limits of effective ensemble, and excessive diversity will only degrade overall performance. The necessity to reduce the cross-timestep differences of the SNN is discussed in detail in Appendix~\ref{Necessity}.

\vspace{-0.1cm}
\subsection{Temporal Consistency in SNNs}
\vspace{-0.1cm}

Previous studies have shown that promoting temporal consistency can improve the performance of SNNs, such as distillation~\citep{TSSD,TKS} and contrastive learning~\citep{temporalcontrastivelearningspiking} in the temporal dimension. However, existing methods directly promote output/feature consistency, similar to ANNs, without adequately considering the properties of SNNs. In contrast to existing methods, this paper highlights the negative impact of differences in membrane potential distributions across timesteps from an ensemble perspective and proposes to improve distribution consistency. Compared to output/feature consistency, membrane potential distribution consistency offers significant performance gains and can be combined with them to synergistically maximize performance.

\vspace{-0.2cm}
\section{Method}
\vspace{-0.2cm}

In this section, we describe the temporal dynamics of the SNN from the basic LIF neuron model and interpret it as the ensemble of multiple temporal subnetworks. We then show that excessive differences in membrane potentials across timesteps affect ensemble performance, and we mitigate this problem by adaptively smoothing membrane potentials. In addition, we encourage temporally adjacent subnetworks to produce stable and consistent outputs through distillation, further facilitating ensemble stability and performance.


\subsection{SNN as an Ensemble of Temporal Subnetworks}
\label{neuronmodel}
SNNs transmit information by generating binary spikes from spiking neurons. In this paper, we use the most commonly used LIF neuron model~\citep{STBP}. LIF neurons continuously receive inputs from presynaptic neurons accumulating membrane potential $H$, and a spike $S$ is fired and resets the membrane potential when it reaches the firing threshold $\vartheta$. The LIF neuron dynamics can be expressed as:
\begin{equation}
H_{i}^{l}(t)=U_{i}^{l}(t)+I_{i}^{l}(t)=\left(1-\frac{1}{\tau}\right) H_{i}^{l}(t-1)+I_{i}^{l}(t), \textcolor[RGB]{161, 163, 166}{\text{charge}}
\label{eq1}
\end{equation}
\begin{equation}
S_{i}^{l}(t) = \left\{
\begin{array}{cl}
1,\quad H_{i}^{l}(t) \ge \vartheta \\
0,\quad H_{i}^{l}(t) < \vartheta \\
\end{array},\textcolor[RGB]{161, 163, 166}{\text{fire spike}}
\right.
\label{eq2}
\end{equation}
\begin{equation}
H_{i}^{l}(t) = H_{i}^{l}(t)-S_{i}^{l}(t)\vartheta, \textcolor[RGB]{161, 163, 166}{\text{reset}}
\label{eq3}
\end{equation}
where $l$, $i$, and $t$ denote the layer, neuron, and timestep indexes, respectively, and $I_{i}^{l}(t) = \sum_{j}^{}W_{i,j}^{l}S_{j}^{l-1}(t)$ is the cumulative current of the previous layer's neuron outputs and weights. $\tau$ is the time constant that controls the decay of the membrane potential with time. $U^l_i(t)$ is the initial membrane potential at timestep $t$.

From Eq. \ref{eq1}, we can see that the output $S_{i}^{l}(t)$ of the neuron at timestep $t$ depends on its initial membrane potential $U_{i}^{l}(t)$. The membrane potential evolves continuously, so the SNN output varies from timestep to timestep. Established methods recklessly compute the average over $T$ timesteps outputs $O=\frac{1}{T}\sum_{t=1}^T{O_t}$ without considering the rationale behind it, leading to suboptimal performance, whereas we explore this from an ensemble perspective to improve the performance of SNNs.

We consider each timestep of the SNN as a temporal subnetwork sharing the architecture $f(\cdot)$ and the parameter $\theta$, and different subnetworks produce different outputs $O_t$ arising from distinct neuron membrane potentials $U(t)$. When the membrane potential difference of these subnetworks is small, their outputs vary slightly, and the ensemble can promote the generalizability of the SNN. However, excessive differences in membrane potentials can lead to drastically different outputs from these subnetworks and degrade the SNN. Unfortunately, vanilla SNNs seem to suffer from this degradation, especially since the initial membrane potential is usually set to 0~\citep{}, the membrane potentials of the first two timesteps show drastic differences, see Fig.~\ref{fig:distribution}. This membrane potential discrepancy leads to highly unstable outputs across timesteps, which increases the optimization difficulty and degrades the ensemble performance. In addition, we show the performance of the trained SNN with different inference timesteps in Table~\ref{tab:Timestep}, and the results show that the output of the vanilla SNN at the first timestep is not discriminative at all.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.99\linewidth]{MPS.pdf}
  \caption{Illustration of (a) the vanilla LIF neuron and (b) the membrane potential smoothing. We smooth the membrane potential at timestep $t$ using the layer-shared coefficient $\alpha^l$ and the smoothed membrane potential $\tilde{H}^l_i(t-1)$ at timestep $t-1$ to reduce membrane potential differences and create additional information/gradient propagation pathways.}
  \label{fig:MPsmoothing}
  \vspace{-0.4cm}
\end{figure}

\subsection{Membrane Potential Smoothing}
To mitigate the above degradation problem, a natural solution is to randomly initialize the membrane potential to reduce the membrane potential difference for the first two timesteps, similar to~\citep{ren2023spiking}. However, the experiments we show in Table~\ref{tab:Timestep} indicate that this practice (Random MP) only slightly alleviates the problem and still struggles to achieve satisfactory performance. To this end, we propose membrane potential smoothing, which mitigates degradation by adaptively reducing the membrane potential difference between adjacent timesteps using a learnable smoothing coefficient.

At each timestep, we consider the initial membrane potential state $U^l_i(t)$ of the spiking neuron as the initial state of the corresponding subnetwork. We argue that by allowing these subnetworks to have similar initial states, the output spikes generated after receiving the input current will also be similar, resulting in stable SNN outputs (The input currents typically follow the same distribution due to the normalization layer). Therefore, we weight the current initial state by the smoothed state of the previous timestep with a layer-shared smoothing coefficient $\alpha^l$ to reduce the difference between the two. The smoothed membrane potential $\tilde{H}^l_i(t)$ receives the input current from the previous layer, which then generates spikes and resets the membrane potential, iterating to the next timestep. The membrane potential smoothing and the charge dynamics of a spiking neuron can be expressed as:
\begin{equation}
\tilde{H}_{i}^{l}(t)=\alpha^l\tilde{H}_{i}^{l}(t-1)+(1-\alpha^l)U_{i}^{l}(t), \textcolor[RGB]{161, 163, 166}{\text{smoothing}}
\label{eq4}
\end{equation}
\begin{equation}
H_{i}^{l}(t)=\tilde{H}_{i}^{l}(t)+I_{i}^{l}(t). \textcolor[RGB]{161, 163, 166}{\text{charge}}
\label{eq5}
\end{equation}

The smoothing coefficient $\alpha^l$ and the parameter $\theta$ of the SNN are co-optimized during training to achieve the optimal smoothing effect. To ensure that $\alpha^l \in (0,1)$, in the practical implementation we train the parameter $\beta^l$ and let $\alpha^l = sigmoid(\beta^l)$. By default, $\beta^l$ is initialized to 0, i.e. the initial value of $\alpha^l$ is 0.5. In Section~\ref{sca}, we will analyze the influence of the initial value of $\alpha^l$ on the performance and convergence. Since the spike activity (Eq. \ref{eq2}) is not differentiable, we use the rectangular function~\citep{STBP} to calculate the spike derivative:
\begin{equation}
\frac{\partial S_{i}^{l}(t)}{\partial H_{i}^{l}(t)} \approx \frac{\partial h(H_{i}^{l}(t), \vartheta)}{\partial H_{i}^{l}(t)} = \frac{1}{a} \text{sign} (|H_{i}^{l}(t)-\vartheta|<\frac{a}{2}),
\label{eq6}
\end{equation}
where $a$ is the hyperparameter that controls the shape of the rectangular function and is set to 1.0. Accordingly, the derivative of a spike with respect to $\alpha^l$ can be calculated as:
\begin{equation}
\frac{\partial S_{i}^{l}(t)}{\partial \alpha^{l}}=\frac{\partial S_{i}^{l}(t)}{\partial H_{i}^{l}(t)} \frac{\partial H_{i}^{l}(t)}{\partial \tilde{H}_{i}^{l}(t)} \frac{\partial \tilde{H}_{i}^{l}(t)}{\partial \alpha^l}
\approx \frac{1}{a} \text{sign} (|H_{i}^{l}(t)-\vartheta|<\frac{a}{2})(\tilde{H}_{i}^{l}(t-1)-U_{i}^{l}(t)).
\label{eq7}
\end{equation}
The derivative of the loss function $\mathcal{L}$ with respect to $\alpha^l$ can be calculated as:
\begin{equation}
\frac{\partial \mathcal{L}}{\partial \alpha^{l}}=\sum_t^T\sum_{i}^{n_l}\frac{\partial \mathcal{L}}{\partial S_{i}^{l}(t)}\frac{\partial S_{i}^{l}(t)}{\partial \alpha^{l}},
\label{eq8}
\end{equation}
where $n_l$ is the number of neurons in layer $l$.

It is worth noting that in addition to mitigating the difference in membrane potential distribution, membrane potential smoothing can also facilitate the propagation of the gradient in the temporal dimension. Previous studies have shown that in SNNs, the temporal gradient is a small percentage~\citep{Meng_2023_ICCV} and is highly susceptible to gradient vanishing, leading to performance degradation~\citep{huang2024clif}. As shown in Fig. \ref{fig:MPsmoothing}, our method establishes a forward information transfer pathway from $\tilde{H}^l_i(t-1)$ to $\tilde{H}^l_i(t)$, and also propagates the error gradient in the backward direction, thus mitigating the influence of the temporal gradient vanishing (Detailed theoretical analysis is provided in Appendix~\ref{MPS_vanishing}). From another perspective, the additional pathways can be viewed as residual connections in the temporal dimension, facilitating the propagation of information and gradients.

Notably, membrane potential smoothing is integrated with LIF neurons in this paper (see Appendix~\ref{MPSLIF} for complete neuron dynamics), but this smoothing method is not limited to specific neuron types and can be integrated with other pre-existing neurons to further improve performance (See Table~\ref{tab:gen_neron}).

\subsection{Temporally Adjacent Subnetwork Guidance}
Membrane potential smoothing aims to pull together the initial states of the subnetworks. In addition, we propose the temporally adjacent subnetwork guidance to further promote the output stability of these subnetworks and improve the ensemble performance.


Inspired by knowledge distillation~\citep{hinton2015distilling}, we guide the output by identifying the ``teacher" and ``student" from two temporally adjacent subnetworks. Since spiking neurons need to accumulate membrane potentials before they can produce stable spiking outputs, we treat the early timestep subnetwork as a weak ``student" that is guided by the stable ``teacher" with a later timestep. Taking the $t$-th and $t+1$-th subnetworks as an example, the output probabilities are first calculated based on the logits $O_t$ and $O_{t+1}$ of the two subnetworks, respectively:
\begin{equation}
p(t)=\frac{e^{O_{t,j}/T_{KL}}}{\sum^C_{c=1}{e^{O_{t,c}/T_{KL}}}},p(t+1)=\frac{e^{O_{t+1,j}/T_{KL}}}{\sum^C_{c=1}{e^{O_{t+1,c}/T_{KL}}}},
\end{equation}
where $C$ denotes the $C$-way classification task, the subscript $j$ indicates the $j$-th class, and $T_{KL}$ is the temperature hyperparameter set to 2. We then use KL divergence to encourage the output probability of subnetwork $t$ to be as similar as possible to the output probability of subnetwork $t+1$ (For the regression task of predicting continuous values, we compute the output MSE loss directly, as in the object detection task in Appendix~\ref{detection}):
\begin{equation}
\mathcal{L}_t={T_{KL}}^2KL(p(t+1)||p(t))={T_{KL}}^2\sum^{C}_{j=1}{p(t+1)_jlog(\frac{p(t+1)_j}{p(t)_j})}.
\label{eq:guidance_loss}
\end{equation}
\vspace{-0.4cm}

By performing guidance between each pair of temporally adjacent subnetworks, we obtain a total of $T-1$ guidance losses: $\{\mathcal{L}_1, \mathcal{L}_2,\cdots, \mathcal{L}_{T-1}\}$. Instead of accumulating all these losses directly, we keep the largest one and drop the others with a probability $P$, as in~\citep{UNIC}. This promotes consistency while preserving diversity among subnetworks, rather than eliminating differences altogether, thus ensuring generalization of the ensemble. In this paper, $P$ is set to 0.5.

We define the function that selects the largest loss and randomly discards the others as $drop(\cdot)$ (see Appendix~\ref{appendix_code} for pseudocode). During training, $drop(\{\mathcal{L}_1, \mathcal{L}_2,\cdots, \mathcal{L}_{T-1}\})$ and cross-entropy loss $\mathcal{L}_{CE}$ synergistically train the SNN:
\begin{equation}
\mathcal{L}_{total} = \gamma \mathcal{L}_{guidance} + \mathcal{L}_{CE} = \gamma drop(\{\mathcal{L}_1, \mathcal{L}_2,\cdots, \mathcal{L}_{T-1}\}) + \mathcal{L}_{CE},
\label{eq:total_loss}
\end{equation}
where $\gamma$ is the coefficient for controlling the guidance loss, which is set to 1.0 by default.

In this way, we synergistically increase the stability of the ensemble at both the level of the initial state (membrane potential) and the output of the subnetwork, greatly improving the overall performance of the SNN. We show the pseudocode for the training process of the temporally adjacent subnetwork guidance in Algorithm~\ref{alg:guidance}.

\begin{algorithm}[tb]
\caption{Temporally adjacent subnetwork guidance for SNNs}
\label{alg:guidance}
\textbf{Input}: input data $x$, label $Y$.\\
\textbf{Parameter}: timestep $T$, Guidance loss coefficient $\gamma$.\\
\textbf{Output}: Trained SNN.
\begin{algorithmic}[1]
\STATE Initialize SNN $f(\cdot)$ with parameters $\theta$
\STATE // Optimization over multiple iterations\\
\FOR{$i=1,2,\cdots,i_{train}$ iterations}
\FOR{$t=1,2,\cdots,T$}
\STATE $O_t = f(\theta;x(t))$ ;       // Calculate the output at timestep $t$\\
\IF{$t > 1$}
\STATE $\mathcal{L}_{t-1}= \text{KL}(O_{t-1};O_{t})$ ;       // Calculate the guidance loss by Eq.~\ref{eq:guidance_loss}\\
\ENDIF
\ENDFOR
\STATE $O = \frac{1}{T}\sum_{t=1}^TO_t$ ;       // Calculate the ensemble average output\\
\STATE $\mathcal{L}_{CE} = \text{Cross-entropy}(O,Y)$ ;       // Calculate the cross-entropy loss\\

\STATE $\mathcal{L}_{guidance}= drop(\{\mathcal{L}_1, \mathcal{L}_2,\cdots, \mathcal{L}_{T-1}\})$ ;       // Random drop guidance loss\\
\STATE $\mathcal{L}_{total} = \gamma \mathcal{L}_{guidance} + \mathcal{L}_{CE}$ ; // Calculate total loss by Eq.~\ref{eq:total_loss}\\
\STATE Backpropagation and optimize model parameters $\theta$;\\
\ENDFOR
\STATE \textbf{return} Trained SNN.
\end{algorithmic}
\end{algorithm}

\section{Experiments}

We have conducted extensive experiments with various architectures (RNN, VGG, ResNet, Transformer, PointNet++) on neuromorphic speech (1D), object (2D) recognition, and 3D point cloud classification tasks. Please see the Appendix~\ref{appendix_detail} for detailed experimental setup.

\vspace{-0.3cm}
\subsection{Ablation Study}
\vspace{-0.2cm}

\begin{figure}[t]
\vspace{-0.3cm}
  \centering
    \includegraphics[width=0.2\linewidth]{our_before_T0.pdf}\hspace{-1.34mm}
  \includegraphics[width=0.2\linewidth]{our_before_T1.pdf}\hspace{-1.34mm}
  \includegraphics[width=0.2\linewidth]{our_before_T2.pdf}\hspace{-1.34mm}
  \includegraphics[width=0.2\linewidth]{our_before_T3.pdf}\hspace{-1.34mm}
  \includegraphics[width=0.2\linewidth]{our_before_T4.pdf}
  \\
  \includegraphics[width=0.2\linewidth]{our_after_T0.pdf}\hspace{-1.34mm}
  \includegraphics[width=0.2\linewidth]{our_after_T1.pdf}\hspace{-1.34mm}
  \includegraphics[width=0.2\linewidth]{our_after_T2.pdf}\hspace{-1.34mm}
  \includegraphics[width=0.2\linewidth]{our_after_T3.pdf}\hspace{-1.34mm}
  \includegraphics[width=0.2\linewidth]{our_after_T4.pdf}
  \\
  \vspace{-0.2cm}
  \caption{Visualization of the membrane potential distribution before (top) and after (bottom) smoothing. Smoothing reduces distribution differences, especially for $T1 \to T2 \to T3$.}
  \label{fig:ablation_distribution}
\vspace{-0.6cm}
\end{figure}

\begin{wraptable}{r}{8.8cm}
\vspace{-1cm}
\tabcolsep=0.008\columnwidth
  \caption{Ablation study results of the proposed method (\%).}
  \label{tab:ablation}
  \scalebox{0.84}{
  \begin{tabular}{ccccc}
  \toprule
  Dataset & Method & VGG-9 & ResNet-18 & SpikingResformer\\
  \midrule
  \multirow{4}{*}{CIFAR10-DVS} & Baseline & 73.97 & 66.73 & 77.60 \\ & +Smooth & $74.80_{+0.83}$ & $68.07_{+1.34}$ & $78.45_{+0.85}$ \\ & +Guidance & $76.23_{+2.26}$ & $69.33_{+2.60}$ & $79.23_{+1.63}$\\ & \cellcolor{ourcolor}+Both & \cellcolor{ourcolor}$\textbf{76.77}_{+2.80}$ & \cellcolor{ourcolor}$\textbf{70.03}_{+3.30}$ & \cellcolor{ourcolor}$\textbf{80.60}_{+3.00}$ \\ 
  \hline
  \multirow{4}{*}{DVS-Gesture} & Baseline & 87.85 & 80.56 & 90.63 \\ & +Smooth & $89.93_{+2.08}$ & $82.99_{+2.43}$ & $91.32_{+0.69}$\\ & +Guidance & $91.32_{+3.47}$ & $84.84_{+4.28}$ & $93.06_{+2.43}$\\
  & \cellcolor{ourcolor}+Both & \cellcolor{ourcolor}$\textbf{93.23}_{+5.38}$ & \cellcolor{ourcolor}$\textbf{85.30}_{+4.74}$ & \cellcolor{ourcolor}$\textbf{94.44}_{+3.81}$\\
  \bottomrule
 \end{tabular}
  }
  \vspace{-0.4cm}
\end{wraptable}

We perform ablation studies using the VGG-9, ResNet-18, and SpikingResformer~\citep{Shi_2024_CVPR} architectures on the neuromorphic object recognition benchmark datasets CIFAR10-DVS~\citep{CIFAR10-DVS} and DVS-Gesture~\citep{DVS-Gesture}. The results in Table~\ref{tab:ablation} show that excessive differences in membrane potentials across timesteps are prevalent across different architectures and datasets, and that our solution consistently mitigates this problem. The visualization of the membrane potential distribution before and after smoothing is shown in Fig.~\ref{fig:ablation_distribution}, and it can be seen that smoothing does indeed reduce the distribution difference. 

\begin{wraptable}{r}{7.5cm}
\tabcolsep=0.008\columnwidth
  \caption{Generalizability experiments (\%) of membrane potential smoothing on different neurons.}
  \label{tab:gen_neron}
  \scalebox{0.84}{
  \begin{tabular}{ccc|cc}
  \toprule
   & PLIF & CLIF & RNN-LIF & DH-LIF\\
  \midrule
  Original & 74.83 & 74.97 & 81.87 & 89.86\\
  \rowcolor{ourcolor}+Smooth & $75.10_{+0.27}$ & $75.97_{+1.00}$ & $83.04_{+1.17}$ & $90.33_{+0.47}$\\
  \bottomrule
 \end{tabular}
  }
  \vspace{-0.4cm}
\end{wraptable}

In addition, the generalizability experiments with membrane potential smoothing for PLIF~\citep{PLIF} and CLIF~\citep{huang2024clif} (on CIFAR10-DVS) as well as for RNN-LIF and DH-LIF~\citep{DHLIF} (on the neuromorphic speech dataset SHD~\citep{SHD}) are shown in Table~\ref{tab:gen_neron}. In particular, we reduce rather than eliminate the differences between subnetworks, preserving the temporal properties of SNNs and thus providing performance gains even in time-dependent speech recognition tasks. Additional ablation experimental results and analysis are presented in Appendix~\ref{shd_ablation}.

\vspace{-0.3cm}
\subsection{Output Visualization}
\vspace{-0.2cm}

We have visualized the output of the SNN in Fig.~\ref{fig:tsne} with 2D t-distributed stochastic neighbor embedding (t-SNE) to show the improvement in output stability and distinguishability with our method. In Fig.~\ref{fig:tsne}(Top), the output of the vanilla SNN at each individual timestep varies widely, and the output of the first two timesteps in particular is confusing. This results in a poor distinguishability of its ensemble average output, which limits the recognition performance. Our SNN on the other hand, has more stable outputs across timesteps, and in particular the outputs generated at the first two timesteps are also well distinguished, as shown in Fig.~\ref{fig:tsne}(Bottom). Benefiting from the stability across timesteps, our final output is more spread across different classes of clusters and more compactly distributed within the clusters, yielding better performance. Please see Appendix~\ref{addvis} for more visualization comparisons.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.161\linewidth]{TSNE-vanilla-T0.pdf}
  \includegraphics[width=0.161\linewidth]{TSNE-vanilla-T1.pdf}
  \includegraphics[width=0.161\linewidth]{TSNE-vanilla-T2.pdf}
  \includegraphics[width=0.161\linewidth]{TSNE-vanilla-T3.pdf}
  \includegraphics[width=0.161\linewidth]{TSNE-vanilla-T4.pdf}
  \includegraphics[width=0.161\linewidth]{TSNE-vanilla-total.pdf}\\
  \includegraphics[width=0.161\linewidth]{TSNE-ours-T0.pdf}
  \includegraphics[width=0.161\linewidth]{TSNE-ours-T1.pdf}
  \includegraphics[width=0.161\linewidth]{TSNE-ours-T2.pdf}
  \includegraphics[width=0.161\linewidth]{TSNE-ours-T3.pdf}
  \includegraphics[width=0.161\linewidth]{TSNE-ours-T4.pdf}
  \includegraphics[width=0.161\linewidth]{TSNE-ours-total.pdf}\\
  \vspace{-0.15cm}
  \caption{Two-dimensional t-SNE visualization on the CIFAR10-DVS dataset. \textbf{Top}: The output of the vanilla SNN varies greatly across timesteps, and the overall output is confusing, making it difficult to distinguish between classes. \textbf{Bottom}: The output of our SNN is more stable across timesteps and more distinguishable across classes, especially for the first two timesteps.}
  \label{fig:tsne}
  \vskip -0.15in
\end{figure}

\vspace{-0.3cm}
\subsection{Smoothing Coefficient Analysis}
\label{sca}
\vspace{-0.1cm}

\begin{figure}[!t]
\centering
	\includegraphics[width=1.35in]{cifar10dvs_alpha_layer0.pdf}
	\includegraphics[width=1.35in]{cifar10dvs_alpha_layer1.pdf}
	\includegraphics[width=1.35in]{cifar10dvs_alpha_layer2.pdf}
	\includegraphics[width=1.35in]{cifar10dvs_alpha_layer3.pdf}\\
	\includegraphics[width=1.35in]{cifar10dvs_alpha_layer4.pdf}
	\includegraphics[width=1.35in]{cifar10dvs_alpha_layer5.pdf}
	\includegraphics[width=1.35in]{cifar10dvs_alpha_layer6.pdf}
	\includegraphics[width=1.35in]{cifar10dvs_alpha_layer7.pdf}	\\
\vspace{-0.3cm}
\caption{The optimization trend of $\alpha$ during training. $\alpha$ with different initial values gradually converge with training iterations, indicating that our method is insensitive to the initial value of $\alpha$.}
\label{fig:trend}
\vskip -0.22in
\end{figure}


By default, $\alpha$ is initialized to 0.5. To analyze the influence of the initialization value on $\alpha$, we visualize the optimization trend of $\alpha$ in each layer of VGG-9 trained on CIFAR10-DVS in Fig.~\ref{fig:trend}, where the initial values of $\alpha$ are taken from \{0.2,0.35,0.5,0.65,0.8\}. As can be seen in Fig.~\ref{fig:trend}, $\alpha$ gradually converges at each layer during the training iterations. In particular, $\alpha$ in the last four layers gradually converges to almost the same optimal value, indicating that our method is robust to initial values. The first few layers of $\alpha$ do not converge to the same optimal value, which we attribute to the accumulation of errors in the surrogate gradient. The SNN uses the surrogate gradient instead of the derivative of the spike activity, which introduces a gradient error in backpropagation, and this error accumulates the further ahead the layer is. Eventually, these gradient errors cause the parameters of the earlier layers to be underoptimized, so that $\alpha$ does not converge to the same optimal value. However, compared to the 73.97\% accuracy of the vanilla SNN, these different $\alpha$ initialization values achieved average accuracies of 76.20\%, 76.80\%, 76.77\%, 75.75\%, and 76.85\%, respectively, both of which provide significant performance gains.

\begin{figure}[t]
\centering
\subfloat[Timestep]
	{
	\includegraphics[width=1.32in]{timestep.pdf}
	}
\subfloat[Drop probability]
	{
	\includegraphics[width=1.32in]{drop_probability.pdf}
	}
\subfloat[Coefficient $\gamma$]
	{
	\includegraphics[width=1.32in]{gamma.pdf}
	}
\subfloat[Temperature]
	{
	\includegraphics[width=1.32in]{temperature.pdf}
	}
\vspace{-0.35cm}
\caption{Influence of hyperparameters on performance. (a) Our method consistently outperforms vanilla SNN across different timesteps. (b) (c) Our method is insensitive to the drop probability of the guidance loss and the coefficient $\gamma$. (d) Our method is insensitive to the temperature hyperparameter within a reasonable range ($T_{KL}\ge1$).}
\label{fig:hyper}
\vspace{-0.6cm}
\end{figure}

\begin{table}[!t]
\vspace{-0.1cm}
 \centering
 \caption{Comparative results on neuromorphic datasets. $^{*}$: self-implementation results with open-source code. $^{\dag}$: knowledge transfer from static data. $^{+}$: Data augmentation.}
 \label{tab:com_neuro}
 \begin{threeparttable}
 \scalebox{0.84}{
 \begin{tabular}{ccccc}
  \toprule
  Dataset & Method & Architecture & T & Accuracy (\%)\\
  \midrule
  \multirow{12}{*}{CIFAR10-DVS} 
  & Ternary Spike~\citep{guo2024ternary} & ResNet-20 & 10 & 78.70\\ 
  & SLTT~\citep{Meng_2023_ICCV} & VGG-11 & 10 & 77.17\\ 
  &NDOT~\citep{NDOT} & VGG-11 & 10 & 77.50\\
  & SSNN~\citep{ding2024shrinking} & VGG-9 & 5 & 73.63\\ 
  & SLT~\citep{SLT} & VGG-9 & 5 & 74.23\tnote{*}\\ 
  & CLIF~\citep{huang2024clif} & VGG-9 & 5 & 74.97\tnote{*}\\
  & SpikingResformer~\citep{Shi_2024_CVPR} & SpikingResformer-Ti & 5 & 77.60\tnote{*}\\
  & SDT~\citep{yao2023spikedriven} & Spiking Transformer-2-256 & 5 & 72.53\tnote{*}\\
  & TRT~\citep{TRT} & Spiking Transformer-2-256 & 5 & 75.55\\
  \cline{2-5}
  & \multirow{2}{*}{\textbf{Ours}} & \cellcolor{ourcolor}VGG-9 & \cellcolor{ourcolor}5 & \cellcolor{ourcolor}76.77\\ 
  & & \cellcolor{ourcolor}SpikingResformer-Ti & \cellcolor{ourcolor}5 & \cellcolor{ourcolor}\textbf{80.60}\\
  & & \cellcolor{ourcolor}VGGSNN & \cellcolor{ourcolor}4 & \cellcolor{ourcolor}\textbf{83.20}\tnote{+}\\
  \hline
  \multirow{7}{*}{DVS-Gesture} & SSNN~\citep{ding2024shrinking} & VGG-9 & 5 & 90.74\\ 
  & TRT~\citep{TRT} & VGG-9 & 5 & 91.67\\
    & SLT~\citep{SLT} & VGG-9 & 5 & 89.35\tnote{*}\\ 
  & SpikingResformer~\citep{Shi_2024_CVPR} & SpikingResformer-Ti & 5 & 90.63\tnote{*}\\
  & SDT~\citep{yao2023spikedriven} & Spiking Transformer-2-256 & 5 & 92.24\tnote{*}\\
  \cline{2-5}
  & \multirow{2}{*}{\textbf{Ours}} & \cellcolor{ourcolor}VGG-9 & \cellcolor{ourcolor}5 & \cellcolor{ourcolor}93.23\\
  & & \cellcolor{ourcolor}SpikingResformer-Ti & \cellcolor{ourcolor}5 & \cellcolor{ourcolor}\textbf{94.44}\\
  
  \hline
  \multirow{9}{*}{N-Caltech101} 
  &TCJA-TET-SNN~\citep{TCJA} & CombinedSNN & 14 & 82.50\\
  & EventMix~\citep{eventmix} & ResNet-18 & 10 & 79.47\\ 
  &TIM~\citep{TIM} & Spikformer & 10 & 79.00\\
  & NDA~\citep{NDA} & VGG-11 & 10 & 78.20\\ 
  & Knowledge-Transfer~\citep{he2024efficient}& VGGSNN & 10 & 91.72\tnote{*$\dag$}\\ 
  & SSNN~\citep{ding2024shrinking} & VGG-9 & 5 & 77.97 \\ 
  & TEBN~\citep{TEBN} & VGG-9 & 5 & 81.24\tnote{*} \\ 
  \cline{2-5}
  & \multirow{3}{*}{\textbf{Ours}} & \cellcolor{ourcolor}VGG-9 & \cellcolor{ourcolor}5 & \cellcolor{ourcolor}82.71\\
  & & \cellcolor{ourcolor}VGGSNN & \cellcolor{ourcolor}10 & \cellcolor{ourcolor}\textbf{93.68}\tnote{$\dag$}\\
  \bottomrule
 \end{tabular}
 }
 \end{threeparttable}
\vskip -0.25in
\end{table}

\vspace{-0.35cm}
\subsection{Influence of Hyperparameters}
\vspace{-0.25cm}
To explore the influence of hyperparameters on the performance of the proposed method, we show in Fig.~\ref{fig:hyper} the performance of VGG-9 on CIFAR10-DVS with different hyperparameter settings. We increased the timestep from 3 to 8 and found that the overall performance of the model also gradually increased and then saturated, and the average accuracy reached a maximum of 77.4\% when the timestep was 7, as shown in Fig.~\ref{fig:hyper}(a). Compared to the vanilla SNN, our method consistently shows better performance. 

The influence of the drop probability of the guidance loss and the coefficient $\gamma$ on the performance is shown in Fig.~\ref{fig:hyper}(b) and Fig.~\ref{fig:hyper}(c), and the results show that our method is not sensitive to these hyperparameters and consistently outperforms the vanilla SNN. This indicates that our method can significantly improve model performance without intentionally adjusting the hyperparameters.

In Fig.~\ref{fig:hyper}(d), we investigate the influence of the temperature hyperparameter $T_{KL}$ in the guidance on the performance, taking values set to $\{0.5,1,2,3,4\}$. The results show that the performance of our method fluctuates only slightly when $T_{KL}>1$, and degrades when $T_{KL}=0.5$ (still outperforming the vanilla SNN). We argue that this is due to the fact that too small a $T_{KL}$ causes the softened subnetwork logit to be too sharp, making it difficult to pull together subnetwork outputs that are already too dissimilar. These experiments show that our method is not sensitive to specific values as long as the hyperparameters are within reasonable ranges, and thus offers great robustness.

\begin{wraptable}{r}{8.2cm}
\vspace{-0.65cm}
\tabcolsep=0.008\columnwidth
  \caption{Comparative results on the 1D SHD dataset.}
  \label{tab:com_shd}
  \scalebox{0.84}{
  \begin{threeparttable}
  \begin{tabular}{ccccccc}
  \toprule
  Method & Architecture & Param (M)& Acc. (\%)\\
  \midrule
  Origin~\citep{SHD} & LSTM & 0.43 & 89.20\\
  TIM~\citep{TIM} & Spikformer & - & 86.30\\
  TC-LIF~\citep{zhang2024tc} & Recurrent & 0.39 & 88.91\tnote{*}\\
  DH-LIF~\citep{DHLIF} & Recurrent & 0.39 & 89.86\\
  \hline
  \textbf{Ours} & Recurrent & 0.39 & \textbf{91.19}\\
  \bottomrule
 \end{tabular}
 \end{threeparttable}
  }
  \vspace{-0.3cm}
\end{wraptable}

\vspace{-0.35cm}
\subsection{Comparison with Existing Methods}
\vspace{-0.2cm}

\textbf{1D neuromorphic speech recognition.} Time-dependent speech recognition tasks require models with complex temporal processing capabilities, so stability across timesteps is extremely important. The comparative results with other SNNs on the SHD dataset are shown in Table~\ref{tab:com_shd}. Our method achieves a recognition accuracy of 91.19\% with a lightweight RNN architecture, outperforming other methods. This confirms the effectiveness of our method on long time sequences.

\textbf{2D neuromorphic object recognition.} As shown in Table~\ref{tab:com_neuro}, our VGG-9 achieves an accuracy of 76.77\% and 93.23\% on CIFAR10-DVS and DVS-Gesture, respectively, with only 5 timesteps. Using the Transformer architecture, we achieved accuracies of 80.60\% and 94.44\%, respectively, significantly exceeding other methods. Specifically, with data augmentation, we were able to achieve 83.20\% accuracy on the CIFAR10-DVS with 4 timesteps. In addition, we conducted experiments on N-Caltech101~\citep{N-Caltech101} and achieved 82.71\% accuracy using VGG-9. Note that we only used vanilla LIF neurons and the general training method, which can further improve the performance when combined with other methods. For example, employing the knowledge transfer strategy~\citep{he2024efficient}, we were able to increase our accuracy to 93.68\%.

\begin{wraptable}{r}{9cm}
\vspace{-0.65cm}
\tabcolsep=0.008\columnwidth
  \caption{Comparative results (\%) on point cloud classification.}
  \label{tab:com_point}
  \scalebox{0.84}{
  \begin{tabular}{cccccc}
  \toprule
  Method & Type & T  & ModelNet10 & ModelNet40\\
  \midrule
  PointNet++~\citep{NIPS2017_d8bf84be} & ANN & - & 95.50 & 92.16\\
  Converted SNN~\citep{Lan_2023_ICCV} & SNN & 16 & 92.75 & 89.45\\
  Spiking PointNet~\citep{ren2023spiking} & SNN & 2 & 92.98 &  88.46\\
  P2SResLNet~\citep{wu2024point} & SNN & 1& - & 89.20\\
  \hline
  \multirow{2}{*}{\textbf{Ours}} & \multirow{2}{*}{SNN} & \cellcolor{ourcolor}2& \cellcolor{ourcolor}\textbf{94.54} & \cellcolor{ourcolor}\textbf{91.13} \\
  && \cellcolor{ourcolor}1  & \cellcolor{ourcolor}94.39 &  \cellcolor{ourcolor}89.82\\
  \bottomrule
 \end{tabular}
  }
  \vspace{-0.3cm}
\end{wraptable}

\textbf{3D point cloud classification.} For the challenging point cloud classification, we performed experiments on the ModelNet10/40 datasets~\citep{Wu_2015_CVPR} using the lightweight PointNet++ architecture~\citep{NIPS2017_d8bf84be}, and the comparative results are shown in Table~\ref{tab:com_point}. With $T=2$, our method achieves 94.54\% and 91.13\% accuracy, respectively, outperforming other SNN models. To evaluate the one-timestep inference performance, we directly infer the trained two-timestep model with one timestep. This preserves the benefits of our method while avoiding additional training overhead. The results show that even with only one timestep, we can achieve accuracies of 94.39\% and 89.82\%, which still outperform other SNNs. 


\vspace{-0.4cm}
\section{Conclusion}
\vspace{-0.3cm}
In this paper, we highlight a key factor that is generally overlooked in SNNs: excessive differences in membrane potential distributions can lead to unstable outputs across timesteps, thereby affecting performance. To mitigate this, we propose membrane potential smoothing and temporally adjacent subnetwork guidance to facilitate consistency of initial membrane potentials and outputs, respectively, thereby improving stability and performance. Meanwhile, membrane potential smoothing also facilitates the propagation of forward information and backward gradients, mitigating the temporal gradient vanishing and providing dual gains. Extensive experiments on neuromorphic speech (1D)/object (2D) recognition and 3D point cloud classification tasks confirmed the effectiveness and versatility of our method. We expect that our work will inspire the community to further analyze the spatio-temporal properties of SNNs.


\subsubsection*{Acknowledgments}
This work was supported by the National Natural Science Foundation of China under Grant No. 62276054 and 62406060.
\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage
\appendix
\section{Appendix}
The Appendix section provides detailed theoretical analysis, neural dynamics, and additional experiments described below:
\begin{itemize}
    \item Appendix~\ref{Necessity} details and experimentally confirms the necessity to reduce the variance across timesteps in SNNs.
    \item Appendix~\ref{MPSLIF} shows the dynamics of LIF neurons with integrated membrane potential smoothing.
    \item Appendix~\ref{MPS_vanishing} analyzes the effectiveness of membrane potential smoothing to mitigate the temporal gradient vanishing.
    \item Appendix~\ref{appendix_code} provides pseudo-code for randomly dropping guidance losses and temporally adjacent subnetwork guidance.
    \item Appendix~\ref{appendix_detail} provides experimental details such as the dataset and training setup.
    \item Appendix~\ref{shd_ablation} shows the additional ablation studies and analysis of our method.
    \item Appendix~\ref{detection} demonstrates the application of the temporally adjacent subnetwork guidance to the object detection task.
    \item Appendix~\ref{spiking_Transformer_power} presents the analysis of the effectiveness and power consumption of our method for the spiking Transformer model with different timesteps.
    \item Appendix~\ref{static_exp} provides additional experiments on the static image datasets.
    \item Appendix~\ref{addvis} shows additional visualizations to demonstrate the effectiveness of our method in promoting consistency in the membrane potential distribution.
\end{itemize}


\newpage
\subsection{The Necessity to Reduce Cross-Timestep Differences in SNNs}
\label{Necessity}
Previous research on ensemble learning has shown that increasing the diversity of members in an ensemble contributes to overall generalization under certain constraints~\citep{NIPS1994_b8c37e33,pmlr-v119-frankle20a,NEURIPS2020_b86e8d03}. In contrast, we point out that the differences in the output of the SNN across timesteps should be reduced, which at first glance seems to contradict the idea of ensemble learning. In this section, we will elucidate the necessity to reduce the differences in SNNs across timesteps by analyzing the constraints of ensemble learning and experimental results.

Previous studies have shown that the solutions learned by Stochastic Gradient Descent (SGD) lie on a nonlinear manifold~\citep{pmlr-v80-draxler18a,NEURIPS2018_be3087e7,pmlr-v97-nguyen19a} and that converged models with a common initial optimization path are linearly connected with a low-loss barrier~\citep{pmlr-v119-frankle20a}. Multiple optimal solutions that are linearly connected belong to a common \textit{basin} that is separated from other regions of the loss landscape~\citep{Jain_2023_CVPR}. The linearly connected multiple models in a common basin can thus be ensembled for additional gains. From this we derive a constraint that is important for ensemble, but typically overlooked: \textbf{The members of the ensemble should be converged optimal solutions, or at the very least optimized feasible solutions.} However, it is possible that a particular temporal submodel in the SNN is not a feasible solution. For example, the results in Table~\ref{tab:Timestep} show that the SNN submodel is only 10\% accurate at the first timestep on the ten-class CIFAR10-DVS dataset, which is equivalent to a randomly initialized network that is far from an optimal or feasible solution. At this point, it has stepped outside the constraints of the ensemble member, so it cannot continue to increase the diversity of its members as it would in normal practice. Instead, we can make the infeasible solution gradually converge to the feasible solution by decreasing the difference between this overly outlier submodel (or infeasible solution) and the other submodels (feasible solutions). Finally, these feasible solutions can be used as an effective ensemble to improve overall stability and performance.


From another point of view, it is difficult to have both performance and diversity of tasks in an ensemble. To balance performance and diversity, existing methods use multiple losses during training to guide the optimization of the neural network ensemble~\citep{9677845,NEURIPS2020_b86e8d03}. Following~\citep{NEURIPS2020_b86e8d03}, we divide the training loss of an ensemble into three parts (Take the classification task as an example): i) the cross-entropy loss of ensemble members and labels; ii) the diversity of ensemble members; and iii) the aggregated loss of the ensemble.

\begin{itemize}
    \item The cross-entropy loss of ensemble members and labels can be calculated as
    \begin{equation}
    \mathcal{L}_{s}=\mathcal{H}(q(\mathcal{T}_i(\mathbf{X})), \mathbf{Y}),
    \end{equation}
    where $\mathbf{X}$ is the input, $\mathbf{Y}$ is the label, $\mathcal{T}_i$ is the $i$-th member model, $q(\cdot)$ is the normalization function, and $\mathcal{H}$ is the cross entropy.
    \item The diversity of ensemble members can be calculated as
    \begin{equation}
    \mathcal{L}_{d}=1-\frac{1}{N} \sum_{1 \leq i \neq j \leq N} q\left(\mathcal{T}_i(\mathbf{X})\right) q\left(\mathcal{T}_j(\mathbf{X})\right),
    \end{equation}
    where $N$ is the number of members in the ensemble.
    \item The aggregated loss of the ensemble can be calculated as
    \begin{equation}
    \mathcal{L}_{a}=\mathcal{H}(\sum_i^N{\gamma_i \cdot \mathcal{T}_i(\mathbf{X})},\mathbf{Y}),
    \end{equation}
    where $\gamma_i$ is the integration weight, bounded by $\sum_i^N{\gamma_i}=1$.
\end{itemize}

The total loss during training is
\begin{equation}
\mathcal{L}_{ensemble}=\mathcal{L}_{s} + \mathcal{L}_{a} - \alpha \mathcal{L}_{d},
\label{eq:ensemble}
\end{equation}
where $\alpha$ is the coefficient controlling the increase in diversity that can be adaptively updated in~\citep{NEURIPS2020_b86e8d03} with training iterations.

From Eq.~\ref{eq:ensemble}, we can see that \textbf{ensemble learning cannot blindly increase the diversity of its members, but must strike a balance between overall performance and diversity}. Our method, on the other hand, takes this into account by pointing out that excessive variance in the SNN (over-diversity) can degrade overall performance, thus requiring an appropriate reduction in diversity to achieve a better balance between performance and diversity.

To further confirm the effectiveness of our method in promoting a balance between overall performance and diversity, a comparison of the ensemble metrics on the CIFAR10-DVS dataset is presented in Table~\ref{tab:ensemble}. The results show that although our method slightly reduces the diversity of the individual timestep submodels of the SNN, it achieves better performance in all other three metrics. In particular, our diversity is degraded by only a negligible 3.05\% compared to the vanilla SNN, but the final integration loss is reduced by 40.05\%, significantly improving the overall performance.

\begin{table}[t]
\caption{Comparative results ($\times 10^{-3}$) of ensemble metrics on the CIFAR10-DVS dataset.}
\label{tab:ensemble}
\begin{center}
\begin{small}
  \begin{tabular}{ccccc}
    \toprule
     & $\mathcal{L}_{s} \downarrow $ & $\mathcal{L}_{d} \uparrow $ & $\mathcal{L}_{a} \downarrow $ & $\mathcal{L}_{ensemble} \downarrow $\\
    \midrule
    Vanilla SNN & 25.1526 & \textbf{60.0711} & 13.6560 & -21.2624 \\
    \rowcolor{ourcolor}Ours & \textbf{16.2025} ($ \downarrow 35.58\%)$ & 58.2364 ($ \downarrow 3.05\%$) & \textbf{12.2561} ($ \downarrow 10.25\%$) & \textbf{-29.7778} ($ \downarrow 40.05\%$)\\
    \bottomrule
  \end{tabular}
\end{small}
\end{center}
\end{table}

\subsection{Integrating Membrane Potential Smoothing in LIF Neurons}
\label{MPSLIF}

In this paper, membrane potential smoothing is integrated into commonly used LIF neurons. The neuron dynamics after integration can be expressed as:

\begin{equation}
U_{i}^{l}(t)=\left(1-\frac{1}{\tau}\right) H_{i}^{l}(t-1), \textcolor[RGB]{161, 163, 166}{\text{leakage}}
\label{smoothingLIF1}
\end{equation}

\begin{equation}
\tilde{H}_{i}^{l}(t)=\alpha^l\tilde{H}_{i}^{l}(t-1)+(1-\alpha^l)U_{i}^{l}(t), \textcolor[RGB]{161, 163, 166}{\text{smoothing}}
\label{smoothingLIF2}
\end{equation}
\begin{equation}
H_{i}^{l}(t)=\tilde{H}_{i}^{l}(t)+I_{i}^{l}(t), \textcolor[RGB]{161, 163, 166}{\text{charge}}
\label{smoothingLIF3}
\end{equation}
\begin{equation}
S_{i}^{l}(t) = \left\{
\begin{array}{cl}
1,\quad H_{i}^{l}(t) \ge \vartheta \\
0,\quad H_{i}^{l}(t) < \vartheta \\
\end{array},\textcolor[RGB]{161, 163, 166}{\text{fire spike}}
\right.
\label{smoothingLIF4}
\end{equation}
\begin{equation}
H_{i}^{l}(t) = H_{i}^{l}(t)-S_{i}^{l}(t)\vartheta, \textcolor[RGB]{161, 163, 166}{\text{reset}}
\label{smoothingLIF5}
\end{equation}
where $\tilde{H}$ is the smoothed membrane potential. Note that $\tilde{H}(t-1)$ does not exist when $t = 0$, at which point the dynamics of the neuron are the same as the original LIF dynamics. When $t = 1$, $\tilde{H}(t-1)=\tilde{H}(0)$ is set to the initialized value of membrane potential $H$, which is 0 in this paper.

\newpage
\subsection{Membrane Potential Smoothing Mitigates Temporal Gradient Vanishing}
\label{MPS_vanishing}

When training SNNs directly with the surrogate gradient, the gradient propagates backward in both spatial and temporal domains. Taking the vanilla LIF neuron as an example, we convert Eq.~\ref{eq1} to Eq.~\ref{eq3} into the following form to illustrate the temporal gradient vanishing problem:
\begin{equation}
\mathbf{H}^{l}(t)=(1-\frac{1}{\tau})(\mathbf{H}^{l}(t-1) - \vartheta \mathbf{S}^{l}(t-1))+ \mathbf{W}^{l} \mathbf{S}^{l-1}(t),
\end{equation}
The gradient of the loss $\mathcal{L}$ with respect to the weights $\mathbf{W}^l$ over $T$ timesteps is:
\begin{equation}
\nabla_{\mathbf{W}^{l}}\mathcal{L}
=\sum_{t=0}^{T-1}
\frac{\partial \mathcal{L}}{\partial \mathbf{H}^{l}(t)} ^\top
\mathbf{S}^{l-1}[t]^\top, \ l = L, L-1,\cdots,1,
\end{equation}
where $L$ is the total number of layers in the SNN.

The derivative of the loss $\mathcal{L}$ with respect to the membrane potential $\mathbf{H}^l(t)$ is:
\begin{equation}
\label{eq14}
\frac{\partial \mathcal{L}}{\partial \mathbf{H}^l(t)}
=\textcolor{newblue}{
\frac{\partial \mathcal{L}}{\partial \mathbf{S}^{l}(t)} \frac{\partial \mathbf{S}^{l}(t)}{\partial \mathbf{H}^l(t)}
}
+ \textcolor{newgreen}{
\sum_{t^\prime=t+1}^{T-1} \frac{\partial \mathcal{L}}{\partial \mathbf{S}^{l}(t^\prime)} \frac{\partial \mathbf{S}^{l}(t^\prime)}{\partial \mathbf{H}^l(t^\prime)}
\prod_{t^{\dprime}=1}^{t^\prime - t} \mathbf{\epsilon}^{L}(t^\prime-t^\dprime)
}, \ \textcolor[RGB]{161, 163, 166}{l = L},
\end{equation}
\begin{equation}
\label{eq15}
\frac{\partial \mathcal{L}}{\partial \mathbf{H}^l(t)}
= \textcolor{newblue}{
\frac{\partial \mathcal{L}}{\partial \mathbf{H}^{l+1}(t)} \frac{\partial \mathbf{H}^{l+1}(t)}{\partial \mathbf{S}^{l}(t)} \frac{\partial \mathbf{S}^{l}(t)}{\partial \mathbf{H}^l(t)}
}
+ \textcolor{newgreen}{
\sum_{t^\prime=t+1}^{T-1} \frac{\partial \mathcal{L}}{\partial \mathbf{H}^{l+1}(t^\prime)} \frac{\partial \mathbf{H}^{l+1}(t^\prime)}{\partial \mathbf{S}^{l}(t^\prime)} \frac{\partial \mathbf{S}^{l}(t^\prime)}{\partial \mathbf{H}^l(t^\prime)}
\prod_{t^{\dprime}=1}^{t^\prime - t} \mathbf{\epsilon}^{L}(t^\prime-t^\dprime)
}, \ \textcolor[RGB]{161, 163, 166}{l < L},
\end{equation}
where the blue portion on the left indicates the \textcolor{newblue}{spatial gradient} and the green portion on the right indicates the \textcolor{newgreen}{temporal gradient}. $\mathbf{\epsilon}^{L}(t)$ is defined as the sensitivity of the membrane potential $\mathbf{H}^l(t+1)$ to $\mathbf{H}^l(t)$ in adjacent timesteps~\citep{Meng_2023_ICCV,huang2024clif}:
\begin{equation}
\label{mem_sensitivity}
\mathbf{\epsilon}^{l}(t) \triangleq
\frac{\partial \mathbf{H}^{l}(t+1)}{\partial \mathbf{H}^{l}(t)}
+\frac{\partial \mathbf{H}^{l}(t+1)}{\partial \mathbf{S}^{l}(t)}
\frac{\partial \mathbf{S}^{l}(t)}{\partial \mathbf{H}^{l}(t)}.
\end{equation}
As can be seen in Eq.~\ref{mem_sensitivity}, the sensitivity $\mathbf{\epsilon}^{L}(t)$ controls the percentage of the temporal gradient in the total gradient. However, for typical surrogate gradient settings, the diagonal matrix $\prod_{t^{\dprime}=1}^{t^\prime - t} \textcolor{black}{\mathbf{\epsilon}^{l}(t^\prime-t^\dprime)}$ has only a small spectral norm~\citep{Meng_2023_ICCV}. To illustrate, for the rectangular function used in Eq.~\ref{eq6}, when $a=\vartheta$, the diagonal elements of $\epsilon^{l}(t)$ are
\begin{equation}
\mathbf{\epsilon}^{l}(t)_{jj} = \left\{
\begin{array}{l}0, \quad \frac{1}{2}\vartheta<H_j^{l}(t)<\frac{3}{2}\vartheta, \\
1-\frac{1}{\tau}, \quad \text{otherwise}.
\end{array}\right.
\end{equation}
Typically, $1-\frac{1}{\tau}$ is set to values less than 1, such as 0.5 and 0.25~\citep{ding2024shrinking,RecDis}, which causes the diagonal values of the matrix $\prod_{t^{\dprime}=1}^{t^\prime - t} \textcolor{black}{\mathbf{\epsilon}^{l}(t^\prime-t^\dprime)}$ to become smaller as $t^\prime - t$ increases, eventually causing the \textbf{temporal gradient vanishing} and degrading performance.

When membrane potential smoothing is used, the neuron at each timestep is smoothed by the smoothed membrane potential of the previous timestep, and the modified LIF neuron dynamics are shown in Appendix~\ref{MPSLIF}. When $t > 0$, the sensitivity of the membrane potential at timestep $t+1$ with respect to the membrane potential at timestep $t$ can be calculated as:
\begin{equation}
\mathbf{\tilde{\epsilon}}^{l}(\Delta t=1)
= \frac{\partial \mathbf{H}^l(t+1)}{\partial \mathbf{\tilde{H}}^l(t+1)} \frac{\partial \mathbf{\tilde{H}}^l(t+1)}{\partial \mathbf{H}^l(t)}
= (1-\alpha) \mathbf{\epsilon}^{l}(t).
\end{equation}
When the timestep interval is 2, the sensitivity of the temporal gradient is:
\begin{equation}
\begin{aligned}
\mathbf{\tilde{\epsilon}}^{l}(\Delta t=2)
&= \frac{\partial \mathbf{H}^l(t+2)}{\partial \mathbf{\tilde{H}}^l(t+2)} (\frac{\partial \mathbf{\tilde{H}}^l(t+2)}{\partial \mathbf{\tilde{H}}^l(t+1)} \frac{\partial \mathbf{\tilde{H}}^l(t+1)}{\partial \mathbf{H}^l(t)}
+ \frac{\partial \mathbf{\tilde{H}}^l(t+2)}{\partial \mathbf{H}^l(t+1)} \mathbf{\tilde{\epsilon}}^{l}(\Delta t=1))
\\
&= \alpha(1-\alpha)\mathbf{\epsilon}^{l}(t) + (1-\alpha)\mathbf{\epsilon}^{l}(t) \mathbf{\tilde{\epsilon}}^{l}(\Delta t=1)
\\
&= (\alpha + (1-\alpha)\mathbf{\epsilon}^{l}(t)) \mathbf{\tilde{\epsilon}}^{l}(\Delta t=1).
\end{aligned}
\end{equation}

By iterating over timestep according to the chain rule, the sensitivity of the temporal gradient can be obtained as
\begin{equation}
\mathbf{\tilde{\epsilon}}^{l}(\Delta t)
= (1-\alpha) \mathbf{\epsilon}^{l}(t) (\alpha + (1-\alpha)\mathbf{\epsilon}^{l}(t))^{\Delta t-1}
\end{equation}

Thus, the derivative of the loss $\mathcal{L}$ with respect to the membrane potential $\mathbf{H}^l(t)$ from Eq.~\ref{eq14} and Eq.~\ref{eq15} becomes
\begin{equation}
\label{eq21}
\frac{\partial \mathcal{L}}{\partial \mathbf{H}^l(t)}
=\textcolor{newblue}{
\frac{\partial \mathcal{L}}{\partial \mathbf{S}^{l}(t)} \frac{\partial \mathbf{S}^{l}(t)}{\partial \mathbf{H}^l(t)}
}
+ \textcolor{newgreen}{
\sum_{t^\prime=t+1}^{T-1} \frac{\partial \mathcal{L}}{\partial \mathbf{S}^{l}(t^\prime)} \frac{\partial \mathbf{S}^{l}(t^\prime)}{\partial \mathbf{H}^l(t^\prime)}}
\textcolor{orange}{\mathbf{\tilde{\epsilon}}^{l}(\Delta t=t^\prime - t)}, \ \textcolor[RGB]{161, 163, 166}{l = L},
\end{equation}
\begin{equation}
\label{eq22}
\frac{\partial \mathcal{L}}{\partial \mathbf{H}^l(t)}
= \textcolor{newblue}{
\frac{\partial \mathcal{L}}{\partial \mathbf{H}^{l+1}(t)} \frac{\partial \mathbf{H}^{l+1}(t)}{\partial \mathbf{S}^{l}(t)} \frac{\partial \mathbf{S}^{l}(t)}{\partial \mathbf{H}^l(t)}
}
+ \textcolor{newgreen}{
\sum_{t^\prime=t+1}^{T-1} \frac{\partial \mathcal{L}}{\partial \mathbf{H}^{l+1}(t^\prime)} \frac{\partial \mathbf{H}^{l+1}(t^\prime)}{\partial \mathbf{S}^{l}(t^\prime)} \frac{\partial \mathbf{S}^{l}(t^\prime)}{\partial \mathbf{H}^l(t^\prime)}}
\textcolor{orange}{\mathbf{\tilde{\epsilon}}^{l}(\Delta t=t^\prime - t)}
, \ \textcolor[RGB]{161, 163, 166}{l < L},
\end{equation}

It can be seen that the sensitivity of the temporal gradient in the neuron after smoothing the membrane potential becomes $\mathbf{\tilde{\epsilon}}^{l}(\Delta t=t^\prime - t)$ instead of $\prod_{t^{\dprime}=1}^{t^\prime - t} \textcolor{black}{\mathbf{\epsilon}^{l}(t^\prime-t^\dprime)}$. If the time step interval is small, such as 1, then $\mathbf{\tilde{\epsilon}}^{l}(\Delta t=t^\prime - t)$ is slightly lower than $\prod_{t^{\dprime}=1}^{t^\prime - t} \textcolor{black}{\mathbf{\epsilon}^{l}(t^\prime-t^\dprime)}$, but this does not matter because the temporal gradient does not vanish at this point. If the time step interval is large, then the diagonal values in $\prod_{t^{\dprime}=1}^{t^\prime - t} \textcolor{black}{\mathbf{\epsilon}^{l}(t^\prime-t^\dprime)}$ are greatly reduced, causing performance degradation. In contrast, $\mathbf{\tilde{\epsilon}}^{l}(\Delta t=t^\prime - t)$ can produce larger values at this point to mitigate the temporal gradient vanishing. For example, if $1-\frac{1}{\tau}=0.5$, $\alpha=0.25$, and the timestep interval $\Delta t$ is 3, the valid value of the diagonal element in $\prod_{t^{\dprime}=1}^{t^\prime - t} \textcolor{black}{\mathbf{\epsilon}^{l}(t^\prime-t^\dprime)}$ is 0.125, while the valid value in $\mathbf{\tilde{\epsilon}}^{l}(\Delta t=t^\prime - t)$ is 0.146. When the timestep interval is increased to 5, the difference between the two becomes 0.03125 versus 0.05722, at which point the sensitivity of the temporal gradient increases by 83\%. This demonstrates the effectiveness of membrane potential smoothing in mitigating the temporal gradient vanishing.

\subsection{PyTorch-Style Code}
\label{appendix_code}

For reproducibility, we provide Pytorch-style code for the drop function $drop(\cdot)$, which randomly drops guidance losses in the Algorithm~\ref{alg: dropcode} inspired by~\citep{UNIC}.

\begin{algorithm}[h]\small
    \caption{PyTorch-style code for randomly dropping guidance losses}
    \label{alg: dropcode}
    \definecolor{codeblue}{rgb}{0.25,0.5,0.5}
    \definecolor{codepink}{rgb}{1,0.5,0.5}
    \definecolor{codedark}{rgb}{1,0.7,0.8}
    \lstset{
        backgroundcolor=\color{white},
        basicstyle=\fontsize{7.2pt}{7.2pt}\ttfamily\selectfont,
        columns=fullflexible,
        breaklines=true,
        captionpos=b,
        commentstyle=\fontsize{8pt}{8pt}\color{codeblue},
        keywordstyle=\fontsize{8.0pt}{8.0pt}\color{codepink},
        emph={DemoNet, Blk}, %
        emphstyle=\color{purple}, %
    }
    {\small
    \begin{lstlisting}[language=python]
# losses: Guidance loss between T-1 temporally adjacent sub-networks.
# losses.shape: [T-1] 
# p: random discard probability.
def drop(losses,p):
    T,B,C,H,W = x.shape
    w = torch.ones((losses.shape[0]))
    index = torch.argmax(losses)
    for i in range(losses.shape[0]):
        if i == index:
            continue
        else:
            p = random.random()
            if p < prob:
                w[i] = 0
    w.div_(w.sum())
    return w * kd_loss
    \end{lstlisting}
    }
\end{algorithm}

\newpage
\subsection{Experimental Details}
\label{appendix_detail}

\subsubsection{Datasets}

In this paper, we perform experiments on the neuromorphic datasets CIFAR10-DVS, DVS-Gesture, and N-Caltech101, as well as the static image datasets CIFAR10, CIFAR100, and the 3D point cloud datasets ModelNet10 and ModelNet40.


CIFAR10-DVS~\citep{CIFAR10-DVS} is a benchmark dataset for neuromorphic object recognition, which contains 10,000 event samples of size $128\times128$. The dimension of each event sample $x$ is $[t,p,x,y]$, where $t$ is the time stamp, $p$ is the polarity of the event, indicating the increase or decrease of the pixel value, and $[x,y]$ are the spatial coordinates. There are 10 classes of samples in CIFAR10-DVS, and we divide each class of samples into training and test sets in the ratio of 9:1 to evaluate the model performance, the same as the existing work~\citep{TRT,Shi_2024_CVPR,yao2023spikedriven}.

DVS-Gesture~\citep{DVS-Gesture} dataset contains event samples for 11 gestures, of which 1176 are used for training and 288 are used for testing. The dimension of each event sample is $[t,p,x,y]$ and the spatial dimension size is $128\times128$.

N-Caltech101~\citep{N-Caltech101} contains event stream data for 101 objects, each sample with a spatial size of $180\times240$. There are 8709 samples in N-Caltech101, and we divide the training set and the test set at a ratio of 9:1.

Since the event data is of high temporal resolution, we use the SpikingJelly~\citep{SpikingJelly} framework to integrate each event sample into $T$ event frames, where $T$ corresponds to the timestep of the SNN. In addition, event frames are downsampled to a spatial size of $48\times48$ before being input to the SNN.

The Spiking Heidelberg Digits (SHD)~\citep{SHD} dataset contains 1000 spoken digits in 20 categories (from 0 to 9 in English and German) for the speech recognition task. For processing the SHD dataset, we followed~\citep{DHLIF}.

ModelNet10~\citep{Wu_2015_CVPR} and ModelNet40~\citep{Wu_2015_CVPR} are benchmark datasets for 3D point cloud classification. ModelNet10 contains point cloud data for 4899 objects in ten categories; ModelNet40 contains data for 12311 objects in 40 categories. For point cloud data preprocessing, we follow~\citep{ren2023spiking}. We uniformly sample 1024 points on the mesh faces and input them into the SNN after normalizing them to a unit sphere.


\subsubsection{Training Setting}

Our experiments are based on the PyTorch package, using the Nvidia RTX 4090 GPU. By default, we use the VGG-9~\citep{TRT} architecture on the neuromorphic datasets. For ResNet-18, we use the architecture settings described in~\citep{ding2024shrinking}. We trained the model for 100 epochs using a stochastic gradient descent optimizer with an initial learning rate of 0.1 and a tenfold decrease every 30 epochs. We trained the VGG-9 and ResNet-18 models without using any data augmentation techniques, and the weight decay value was 1e-3. The batch size during training is 64. The firing threshold $\vartheta$ and membrane potential time constant $\tau$ of spiking neurons were 1.0 and 2.0, respectively.

When our method is used for the Transformer architecture SNN, we use the SpikingResformer~\citep{Shi_2024_CVPR} architecture. At this point, our training strategy is exactly the same as in the original paper, and we use the officially released training code directly.

Our method can be combined with the knowledge transfer strategy~\citep{he2024efficient} to maximize performance gains, and we conducted experiments on N-Caltech101 using the officially released code. At this point, our training strategy is exactly the same as A, but we set the batch size to 48 to reduce the memory overhead.

For the 1D SHD speech recognition task, we follow the training strategy and architecture of~\citep{DHLIF}. We use the one-layer two-branch recurrent network architecture defined in~\citep{DHLIF} to which the proposed method is applied.

For the 3D point cloud classification task, we use the spiking version of the lightweight PointNet++~\citep{NIPS2017_d8bf84be} architecture. We directly use the code released by~\citep{ren2023spiking}, and thus our training strategy is exactly the same as that of~\citep{ren2023spiking}.

To reduce randomness, we report the average results of three independent experiments in our experiments, in addition to the experiments performed on Table~\ref{tab:Timestep} (randomly selected individual models) and on the large-scale ImageNet.

\subsection{Additional Ablation Study}
\label{shd_ablation}

In this section, we evaluate the proposed method on the neuromorphic speech recognition dataset SHD~\citep{SHD}. Taking DH-LIF~\citep{DHLIF} as the baseline, the experimental results of the proposed method for ablation are shown in Table~\ref{tab:ablation_shd}. The results show that our proposed method still improves the performance of the baseline in speech recognition tasks, demonstrating the generality of our method.


Furthermore, the experiments show that this view of an SNN as an ensemble of multiple subnetworks also holds for time-dependent tasks. Although time-dependent tasks require more time-varying information than static tasks, too much variance can still negatively affect their performance. Our method reduces excessive variance across timesteps, improving ensemble stability and overall performance. It is worth noting that our method does not completely eliminate variance, thus preserving the necessary dynamic information and allowing its application to time-dependent tasks.

\begin{table}[h]
\caption{Ablation study results of the proposed method on neuromorphic speech recognition dataset SHD.}
\label{tab:ablation_shd}
\tabcolsep=0.013\columnwidth
\begin{center}
 \scalebox{0.9}{
  \begin{tabular}
  {lc}
  \toprule
  Method & Accuracy (\%)\\
  \midrule
  Baseline (DH-LIF~\citep{DHLIF}) & 89.86 \\ 
  +Smooth & $90.33_{+0.47}$\\ 
  +Guidance & $90.46_{+0.60}$ \\ 
  \rowcolor{ourcolor}+Both & $\textbf{91.19}_{+1.33}$\\
  \bottomrule
 \end{tabular}
}
\end{center}
\end{table}

Ablation experiments on the 3D point cloud classification dataset ModelNet10 using the PointNet++ architecture and with a timestep of 2 are shown in Table~\ref{tab:ablation_point}, and the results further confirm the effectiveness of our method. We also compare the performance when trained with T=2 but with inference T=1. Our method is able to achieve 94.39\% accuracy in 1 timestep inference, surpassing the performance of the vanilla SNN by 1.78\% for the same training and inference timestep.

\begin{table}[h]
\caption{Ablation study results of the proposed method on 3D point cloud classification dataset ModelNet10.}
\label{tab:ablation_point}
\tabcolsep=0.03\columnwidth
\begin{center}
 \scalebox{0.9}{
  \begin{tabular}
  {cc|cc}
  \toprule
  Method & Inference T=2 Acc. (\%) & Method & Inference T=1 Acc. (\%)\\
  \midrule
  Baseline & 93.75 & Direct training & 91.62 \\ 
  +Smooth & $94.23_{+0.48}$ & Vanilla SNN Training T=2 & 92.61\\ 
  +Guidance & $94.05_{+0.30}$ & - & -\\ 
  \rowcolor{ourcolor}+Both & $\textbf{94.54}_{+0.79}$ & Ours SNN Training T=2 & \textbf{94.39}\\
  \bottomrule
 \end{tabular}
}
\end{center}
\end{table}

\newpage
\subsection{Object Detection Experiment}
\label{detection}

We perform object detection on PASCAL VOC 2012 and COCO 2017 to evaluate whether the proposed method can provide performance gains on non-classification tasks. When our method is used for non-classification tasks (e.g., regression), we can compute the MSE loss between outputs instead of the KL divergence. We take SpikeYOLO~\citep{SpikeYOLO} as the baseline and compute the guidance loss using MSE for its predicted coordinates, and the results for the PASCAL VOC 2012 and COCO 2017 datasets are shown in Table~\ref{tab:com_PASCAL} and Table~\ref{tab:com_coco}, respectively (The training setup follows~\citep{SpikeYOLO} and a total of 80 epochs are trained). The results show that our method can be applied to the object detection task and improve the performance of the baseline model, and should be similarly applicable to other regression tasks.

Note that SpikeYOLO~\citep{SpikeYOLO} uses multi-bit neurons during training so that it can achieve satisfactory performance at lower timesteps. However, this slightly affects the temporal correlation of the SNN. Therefore, our method may be able to achieve more significant facilitation when used in other SNNs (using vanilla single-bit multi-timestep neurons).


\begin{table}[h]
 \centering
 \caption{Comparative results with existing methods on PASCAL VOC 2012. $T\times D$ indicates that $T$ timesteps are set and each timestep is expanded $D$ times. $D$ is set to 1 by default in the comparative methods.}
 \label{tab:com_PASCAL}
 \begin{threeparttable}
 \scalebox{0.9}{
 \begin{tabular}{ccccc}
  \toprule
  Method & \#Param (M) & $T\times D$ & mAP@50 (\%) & mAP@50:95 (\%)\\
  \midrule
  SpikeYOLO~\citep{SpikeYOLO} & 13.2 & $2\times4$ & 48.0 & 30.9\\
  \hline
  \rowcolor{ourcolor}\textbf{Ours} & 13.2 & $2\times4$ & \textbf{48.7} & \textbf{31.5}\\
  \bottomrule
 \end{tabular}
 }
 \end{threeparttable}
\end{table}

\begin{table}[h]
 \centering
 \caption{Comparative results with existing methods on COCO 2017 val. $T\times D$ indicates that $T$ timesteps are set and each timestep is expanded $D$ times. $D$ is set to 1 by default in the comparative methods.}
 \label{tab:com_coco}
 \begin{threeparttable}
 \scalebox{0.9}{
 \begin{tabular}{ccccc}
  \toprule
  Method & \#Param (M) & $T\times D$ & mAP@50 (\%) & mAP@50:95 (\%)\\
  \midrule
  Spiking-YOLO~\citep{spikingyolo} & 10.2 & 3500 & - & 25.7\\
  EMS-YOLO~\citep{EMSYOLO} & 26.9 & 4 & 50.1 & 30.1\\
  Meta-Spikeformer (YOLO)~\citep{yao2024spikedriven} & 16.8 & 4 & 50.3 &-\\
  SpikeYOLO~\citep{SpikeYOLO} & 13.2 & $2\times4$ & 53.6 & 37.9\\
  \hline
  \rowcolor{ourcolor}\textbf{Ours} & 13.2 & $2\times4$ & \textbf{54.0} & \textbf{38.4}\\
  \bottomrule
 \end{tabular}
 }
 \end{threeparttable}
\end{table}

\newpage
\newpage
\subsection{Timestep and Power Consumption Analysis of Spiking Transformer Model}
\label{spiking_Transformer_power}

The timestep of the SNN is proportional to the training and inference overhead. To balance performance and overhead, the timestep in this paper on the neuromorphic dataset was set to 5. However, since the typical spiking Transformer~\citep{Shi_2024_CVPR} model uses larger timesteps such as 10 and 16 on the neuromorphic datasets, we explore here the performance of our method for the Transformer architecture at different timesteps. To evaluate the influence of the time-step hyperparameter on the performance, we conducted experiments on DVS-Gesture based on the SpikingResformer~\citep{Shi_2024_CVPR} architecture, and the results are shown in Table~\ref{tab:power}. The results show that even at larger timesteps, our method is still able to improve the performance of the baseline SpikingResformer model, although the effect gradually decreases as the performance of the model saturates.

\begin{table}[h]
 \centering
 \caption{Comparison of performance and power consumption at different timesteps. Experiments were performed on DVS-Gesture with the SpikingResformer architecture.}
 \label{tab:power}
 \begin{small}
 \begin{tabular}{ccccccc}
  \toprule
  & & T=4 & T=5 & T=8 & T=10 & T=16 \\
  \midrule
  \multirow{2}{*}{Acc. (\%)}
  & Baseline & 89.93 & 90.63 & 92.89 & 93.58 & 96.99\\ 
  & \cellcolor{ourcolor}+Smoothing & \cellcolor{ourcolor}$91.67_{+1.74}$ & \cellcolor{ourcolor}$94.44_{+3.81}$ & \cellcolor{ourcolor}$94.32_{+1.43}$ & \cellcolor{ourcolor}$94.33_{+0.75}$ & \cellcolor{ourcolor}$97.23_{+0.24}$ \\
 \hline
  \multirow{2}{*}{Power (mJ)}
  & Baseline & 0.3699 & 0.4796 & 0.8119 & 1.0374 & 1.5788\\ 
  & \cellcolor{ourcolor}+Smoothing & \cellcolor{ourcolor}0.3869 & \cellcolor{ourcolor}0.4874 & \cellcolor{ourcolor}0.8079 & \cellcolor{ourcolor}1.0199 & \cellcolor{ourcolor}1.5769\\
  \bottomrule
 \end{tabular}
 \end{small}
\end{table}

In addition, Table~\ref{tab:power} compares the power consumption of our method with that of the baseline model. The calculation of the power consumption follows~\citep{Shi_2024_CVPR} (Power consumption is positively correlated with the number of spikes). The results show that our method only slightly increases the power consumption when the timestep is small, and our method with lower power consumption when the timestep is large. In particular, the low number of spikes of the SNN at low timesteps tends to lead to an inadequate feature representation. Our method generates slightly more spikes at low timesteps to enhance the representation quality and thus the performance. In contrast, when the timestep is large, the vanilla SNN suffers from redundant spikes, and our method reduces the redundant spikes, thus improving performance and reducing power consumption. Overall, the difference in power consumption between our method and the baseline model is negligible, and therefore hardly affects the power consumption of the SNN model.

\newpage
\subsection{Experiments on Static Image Datasets}
\label{static_exp}

In addition to neuromorphic datasets, we also conducted experiments on the static object recognition task to demonstrate the generalizability of our method. The comparative results for static datasets are shown in Table~\ref{com_cifar}, where we achieved 96.16\% and 79.22\% accuracy for CIFAR10 and CIFAR100, respectively, outperforming the other methods.

\begin{table*}[h]
 \centering
 \vskip -0.2in
 \caption{Comparative results (\%) on static datasets. * denotes self-implementation results.}
 \label{com_cifar}
 \tabcolsep=0.01\columnwidth
 \begin{threeparttable}
 \scalebox{0.9}{
 \begin{tabular}{ccccccc}
  \toprule
  Method  & Architecture & Param (M) & T & CIFAR10 & CIFAR100\\
  \midrule
  CLIF~\citep{huang2024clif} &  ResNet-18 & 11.21 & 4 & 94.89 & 77.00\\
  RMP-Loss~\citep{Guo_2023_ICCV} &  ResNet-19 & 12.54 & 4 & 95.51 & 78.28\\
NDOT~\citep{NDOT} & VGG-11 & 9.23 &  4 & 94.86 & 76.12\\
TAB~\citep{TAB} &  ResNet-19 & 12.54 & 4 & 94.76 & 76.81\\
  SLT-TET~\citep{SLT} &  ResNet-19 & 12.54 & 4 & 95.18 & 75.01\\
Spikformer~\citep{zhou2023spikformer} & Spiking Transformer-4-384 & 9.28 & 4 & 95.19 & 77.86\\
  SpikingResformer~\citep{Shi_2024_CVPR} & SpikingResformer-Ti & 10.79 & 4 & 95.93\tnote{*} & 78.23\tnote{*}\\
SDT~\citep{yao2023spikedriven} & Spiking Transformer-2-512 & 10.21 &  4 & 95.60 & 78.40 \\
  \hline
 \rowcolor{ourcolor}\textbf{Ours} & SpikingResformer-Ti & 10.79 & 4 & \textbf{96.16} & \textbf{79.22}\\
  \bottomrule
 \end{tabular}
 }
 \end{threeparttable}
\end{table*}

To validate the scalability of our method, we performed experiments on Tiny-ImageNet, ImageNet-Hard~\citep{imagenethard}, and ImageNet.

The comparative results with existing methods on Tiny-ImageNet are shown in Table~\ref{tab:com_tinyimagenet}. Our method achieves an accuracy of 58.04\% in only 4 timesteps, surpassing other comparative methods.

\begin{table}[!h]
 \centering
 \vskip -0.25in
 \caption{Comparative results on the Tiny-ImageNet dataset.}
 \label{tab:com_tinyimagenet}
 \begin{threeparttable}
 \begin{tabular}{cccc}
  \toprule
  Method & Architecture & T & Accuracy (\%)\\
  \midrule
  Online LTL~\citep{Localtandem}  &  VGG-16 & 16 & 56.87\\
  ASGL~\citep{ASGL}  &  VGG-13 & 8 & 56.81\\
  Joint A-SNN~\citep{guo2023joint}  &  VGG-16 & 4 & 55.39\\
  \hline
 \rowcolor{ourcolor}\textbf{Ours} & VGG-13 & 4 & \textbf{58.04}\\
  \bottomrule
 \end{tabular}
 \end{threeparttable}
\end{table}

ImageNet-Hard is slightly smaller in scale than ImageNet, but more challenging. We take~\citep{PSN} as the baseline and employ the same training strategy as~\citep{PSN}. The comparative results on ImageNet-Hard are shown in Table~\ref{tab:com_imagenet_hard}. The results show that our method can still be effective for this challenging dataset.

\begin{table}[!h]
 \centering
 \vskip -0.25in
 \caption{Comparative results with~\citep{PSN} on the challenging ImageNet-Hard dataset.}
 \label{tab:com_imagenet_hard}
 \begin{threeparttable}
 \begin{tabular}{ccccc}
  \toprule
  Method & Architecture & T & Top-1 Acc. (\%)& Top-5 Acc. (\%)\\
  \midrule
  PSN~\citep{PSN}  &  SEW-ResNet18 & 4 & 44.32 & 52.57\\
  \hline
 \rowcolor{ourcolor}\textbf{Ours} & SEW-ResNet18 & 4 & \textbf{45.89} & \textbf{53.16}\\
  \bottomrule
 \end{tabular}
 \end{threeparttable}
\end{table}

Since ImageNet requires more training resources and time, we show the performance of training 250 epochs in Table~\ref{tab:com_imagenet}. The results show that our method is already able to achieve competitive performance even with only 250 epochs of training.

\begin{table}[!h]
 \centering
 \vskip -0.2in
 \caption{Comparative results with existing methods on ImageNet dataset.}
 \label{tab:com_imagenet}
 \begin{threeparttable}
 \begin{tabular}{cccc}
  \toprule
  Method & Architecture & T & Accuracy (\%)\\
  \midrule
  RecDis-SNN~\citep{RecDis} & ResNet-34 & 6 & 67.33\\
  RMP-Loss~\citep{Guo_2023_ICCV} & ResNet-34 & 4 & 65.17\\
  SSCL~\citep{zhang2024enhancing} & ResNet-34 & 4 & 66.78\\
  TAB~\citep{TAB} & ResNet-34 & 4 & 67.78\\
  SEW-ResNet~\citep{fang2021deep} &  SEW-ResNet34 & 4 & 63.18\\
  \hline
 \rowcolor{ourcolor}\textbf{Ours} & SEW-ResNet34 & 4 & \textbf{69.03}\\
  \bottomrule
 \end{tabular}
 \end{threeparttable}
\end{table}

\newpage
\subsection{Additional Visualizations}
\label{addvis}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.99\linewidth]{cos_mps.pdf}
  \caption{Comparison of the cosine similarity of membrane potential distributions for adjacent timesteps between the vanilla SNN and our method. $T(a-b)$ denotes the similarity of the distribution between timestep $a$ and timestep $b$. Our method significantly improves the similarity of the membrane potential distribution across timesteps, thus facilitating the ensemble performance.}
  \label{fig:cossim}
\end{figure}


\begin{figure}[t]
  \centering
  \includegraphics[width=0.2\linewidth]{vanilla2_T0.pdf}\hspace{-1.34mm}
  \includegraphics[width=0.2\linewidth]{vanilla2_T1.pdf}\hspace{-1.34mm}
  \includegraphics[width=0.2\linewidth]{vanilla2_T2.pdf}\hspace{-1.34mm}
  \includegraphics[width=0.2\linewidth]{vanilla2_T3.pdf}\hspace{-1.34mm}
  \includegraphics[width=0.2\linewidth]{vanilla2_T4.pdf}
  \\
  \includegraphics[width=0.2\linewidth]{our2_T0.pdf}\hspace{-1.34mm}
  \includegraphics[width=0.2\linewidth]{our2_T1.pdf}\hspace{-1.34mm}
  \includegraphics[width=0.2\linewidth]{our2_T2.pdf}\hspace{-1.34mm}
  \includegraphics[width=0.2\linewidth]{our2_T3.pdf}\hspace{-1.34mm}
  \includegraphics[width=0.2\linewidth]{our2_T4.pdf}\\
  \caption{Additional visualization of the membrane potential distribution on CIFAR10-DVS for comparison ($(\mu,\sigma)$ denotes the mean and standard deviation of the distribution). \textbf{Top}: Vanilla SNN. \textbf{Bottom}: Our method allows for a more stable distribution with smaller differences across timesteps.}
  \label{fig:distribution2}
\end{figure}

In this section, we provide additional visualizations to demonstrate the effectiveness of our method in enhancing the similarity of membrane potential distributions across timesteps.


The cosine similarity of the membrane potential distribution across timesteps for the eight convolutional layer neurons in VGG-9 is shown in Fig.~\ref{fig:cossim}. Since the membrane potential is initialized to 0, the similarity of the membrane potential distribution between the 0th timestep and the 1st timestep is 0, and we ignore this term in the figure. It can be seen that our method consistently shows a higher similarity compared to the vanilla SNN, which mitigates the difference in the distribution of membrane potentials across timesteps. Although our method has lower similarity than the vanilla SNN for layer 7 timestep 1 and timestep 2, this exception does not affect our overall role in reducing distributional differences.


In addition, additional visualizations of the membrane potential distribution on CIFAR10-DVS are shown in Fig. \ref{fig:distribution2} to illustrate the effect of our method in smoothing the membrane potential distribution. The results of the membrane potential visualization on DVS-Gesture are shown in Fig.~\ref{fig:distributiongesture}, where again our method shows a more consistent distribution.


\begin{figure}[t]
  \centering
  \includegraphics[width=0.2\linewidth]{vanilla_gesture_T0.pdf}\hspace{-1.34mm}
  \includegraphics[width=0.2\linewidth]{vanilla_gesture_T1.pdf}\hspace{-1.34mm}
  \includegraphics[width=0.2\linewidth]{vanilla_gesture_T2.pdf}\hspace{-1.34mm}
  \includegraphics[width=0.2\linewidth]{vanilla_gesture_T3.pdf}\hspace{-1.34mm}
  \includegraphics[width=0.2\linewidth]{vanilla_gesture_T4.pdf}
  \\
  \includegraphics[width=0.2\linewidth]{our_gesture_T0.pdf}\hspace{-1.34mm}
  \includegraphics[width=0.2\linewidth]{our_gesture_T1.pdf}\hspace{-1.34mm}
  \includegraphics[width=0.2\linewidth]{our_gesture_T2.pdf}\hspace{-1.34mm}
  \includegraphics[width=0.2\linewidth]{our_gesture_T3.pdf}\hspace{-1.34mm}
  \includegraphics[width=0.2\linewidth]{our_gesture_T4.pdf}
  \\
  \caption{Membrane potential distribution in spiking VGG-9 on DVS-Gesture ($(\mu,\sigma)$ denotes the mean and standard deviation of the distribution). \textbf{Top}: Vanilla SNN. \textbf{Bottom}: The overall membrane potential distribution of our method is more stable.}
  \label{fig:distributiongesture}
\vspace{-0.5cm}
\end{figure}


\end{document}
