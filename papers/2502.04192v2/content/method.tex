In this section, we describe our two benchmarks that augment previously released vision centric benchmarks focusing on the shortcomings of MLLMs. Furthermore, we detail our probing techniques to study the available pixel-level MLLMs. Finally, we describe the baseline methods for extracting segmentation from MLLMs that were never trained with pixel-level grounding supervision.

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{images/pixelmllms_failures/failures_pixmllms_1.drawio.pdf}
\vspace{-2em}
\caption{First shortcoming of pixel-level MLLMs is the degraded performance in visual question answering. Predicted segmentations corresponding to the [SEG] token are highlighted in red.} 
\vspace{-1em}
\label{fig:shortcoming1}
\end{figure*}

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{images/pixelmllms_failures/failures_pixmllms_2.drawio.pdf}
\vspace{-2em}
\caption{Second shortcoming of pixel-level MLLMs is the degraded performance in instruction following, where the question is instructing the model to generate one letter from the options.} 
\vspace{-1.5em}
\label{fig:shortcoming2}
\end{figure}


\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{images/pixelmllms_failures/failures_pixmllms_3.drawio.pdf}
\caption{Third shortcoming of pixel-level MLLMs is the degraded performance in pixel-level visual grounding in certain models. Predicted segmentations are highlighted in red.} 
\vspace{-0.5em}
\label{fig:shortcoming3}
\end{figure*}


\subsection{Benchmarks}
\textbf{PixMMVP benchmark:} We build upon the recently released MMVP~\cite{tong2024eyes} which identified clip blind pairs and used them to build a challenging benchmark with the corresponding questions and choices. We augment the aforementioned dataset and manually annotate each question with the corresponding object of interest referring expression, e.g. an elderly person or the butterfly's feet. There are seven questions only that are not designed to inquire about a specific object in the scene, they are labeled for `None' to indicate no object. Examples include questions inquiring on the view direction of the camera and so forth which are not tied to a specific entity. Our manual referring expression annotations are as fine-grained and descriptive as possible corresponding to what is required to ground in the image in order to properly answer the question. Afterwards, we manually label these objects of interest with polygonal annotations using the VGG annotator~\cite{dutta2016via}.

\textbf{PixCV-Bench benchmark:} For this benchmark we build upon the 2D component of the recently released CV-Bench~\cite{tong2024cambrian}. We specifically select the 2D component, since they are sourced from segmentation datasets (i.e., ADE20K~\cite{zhou2017scene} and COCO~\cite{lin2014microsoft}), which can be used in our proposed benchmark. However, the publicly released CV-Bench only contains the mapping to the original images in the corresponding datasets, but it does not identify the objects in question and their corresponding segmentation. As such we use GPT-4o to parse the questions and identify the objects of interest automatically, followed by manual inspection and correction. Specifically, we collect the classes in each image from the corresponding dataset and construct a list of class choices ``1. $<$CLS1$>$, 2. $<$CLS2$>$, ...''. Then we prompt GPT-4o with the following, ``Provide a number only as answer. Identify the objects of interest in the following question: $<$QUESTION$>$ ? 1. $<$CLS1$>$, 2. $<$CLS2$>$, ... ''.  This provides us with the categories per question that highlights the objects of interest. While seemingly these are categorical annotations not referring expressions, except in certain situations in CV-Bench it is still considered as referring expressions. Specifically, in the relative positioning task all the questions that include an object annotated by a red box in the image are annotated with the referring expression, ``(annotated by a red box)'', as such going beyond simple categorical annotations.

Afterwards, we use the selected categories from GPT-4o to retrieve the corresponding segmentation mask/s per image. Furthermore, we use a custom annotation tool to manually filter the objects in the question, e.g. selecting only the object mask annotated by the red box when referred to it and filtering out the other instances for that same class. Another example that needs manual filtration when the class in question is a broader category than what is inquired upon, e.g. ``Pendant Lamp'' which is under the category of ``Lamp'' in ADE20K yet we still need to filter out the masks of other types such as ``Table Lamp''. Moreover, we identified missing annotations in rare occasions that required additional intervention. Examples include transparent objects such as wine glass which was partially out of the frame, yet the ground-truth answer of CV-Bench question included that object in the count. As such, we use VGG annotator for these missing annotations that were not part of the original segmentation masks. We provide the final PixCV-Bench with referring expressions and their segmentation annotations that can be used to evaluate the grounding ability in relation to the original visual question answering task.

\subsection{A Pixel-level MLLMs Study}

We utilize the two proposed benchmarks, PixMMVP and PixCV-Bench, to evaluate how the current trend in pixel-level MLLMs that relies on training with grounding supervision perform in such challenging tasks. Furthermore, we aim to inspect the shortcomings and failures of these pixel-level MLLMs both qualitatively and quantitatively. Finally, we explore simple approaches to pixel-level understanding from MLLMs that overcome the previous shortcomings. %We aim to answer two major research questions in our study; ``How do Pixel-level MLLMs perform in challenging pixel-level visual grounding tasks?'' and ``Whether grounding can be extracted from MLLMs that were not necessarily trained with full supervision as a simpler but more powerful means? and When does that grounding emerge?''

\textbf{Pixel-level MLLMs Shortcomings.} We highlight the failures for the current state-of-the-art pixel-level MLLMs through three probing techniques. First, we highlight the degraded performance in visual question answering from most of the pixel-level MLLMs that are trained with full grounding supervision. As such we use for the first prompt ``$<$IMG$>$$<$QUESTION$>$? $<$OPTION1$>$ $<$OPTION2$>$...'', as shown in Figure~\ref{fig:shortcoming1}. Certain pixel-level MLLMs tend to answer the aforementioned question while outputting a corresponding segmentation mask/s for the objects of interest. Notably, the worst two models in this task, LISA~\cite{lai2024lisa} and GLAMM~\cite{rasheed2024glamm}, are not able to provide an answer and rather refer to a segmentation mask. On the other hand, OMG-LLava~\cite{zhang2024omg} provides an answer to the question showing better ability in visual question answering.%, yet the answer is not necessarily the correct one.

Second, we highlight another shortcoming in these pixel-level MLLMs as they show degraded ability to follow instructions. In order to probe this we use the following prompt: ``$<$IMG$>$$<$QUESTION$>$? a.$<$OPTION1$>$ b.$<$OPTION2$>$... Answer with the option's letter from the given.'' Figure~\ref{fig:shortcoming2} shows an example with the answers from the worst two models in this aspect which are LISA~\cite{lai2024lisa} and LLava-G~\cite{zhang2025llava}. Both are not capable of following the instruction to generate the option letter, yet LLava-G shows better ability to understand the question and a trial to tackle it unlike LISA which is completely incapable of answering the question. On the other hand, OMG-Llava shows better ability to follow the instruction and answer the question. 

The third shortcoming we discuss is their degraded ability to visually ground objects. Surprisingly, although they were trained with grounding supervision not all of these models show superior grounding performance. Figure~\ref{fig:shortcoming3} shows the third prompt to generate a segmentation mask for the ground-truth referring expression. The purpose of this probing is to understand whether the failure in these models is purely on the visual question answering task, or its inability to ground the objects of interest in the corresponding question or both. Figure~\ref{fig:shortcoming3} shows the worst two models in this aspect which are GLAMM, the region captioning variant, and LLava-G. Both fail to segment the specific object in question, while OMG-LLava yet again shows better performance in grounding.


\textbf{Baselines.} In addition to evaluating state-of-the-artpixel-level MLLMs, we propose three baselines. The first of which is inspired by a concurrent work~\cite{cao2024emerging} that identified the emergent grounding in MLLMs without the need for any pixel-level grounding supervision. Specifically, we use their attend and segment meta architecture as one of our baselines. However, we are the first to discuss when does such grounding emerge in these models. We identify interesting connection between the identified output tokens and the output grounding from the attention maps that gives insights on how these models reason to follow the instruction and answer the question. 

The attend and segment meta-architecture relies on using attention maps corresponding to each output token from a transformer based LLM. It uses the raw attention map for the $i^{th}$ output token, $A_i \in [0, 1]^{n_{\text{layer}} \times n_{\text{head}} \times (x+hw+y+i-1)}$, where $n_{\text{layer}}, n_{\text{head}}$  are the number of layers and heads, resp. Then, $x,y$ are the number of input language tokens before and after the visual tokens respectively, while $hw$ are the height and width of the input image. Only the attention corresponding to the visual tokens of length $hw$ are used, and these attention maps are averaged across the layers and heads, resulting in $\bar{A}_i \in [0, 1]^{h \times w}$. This is further normalized across all the output tokens, $\tilde{A}_i = \bar{A}_i - \frac{1}{N} \sum_{j=1}^{N}{\bar{A}_j}$ for $N$ output tokens. The attend and segment depends on using spaCy natural language processing tool~\cite{spaCy} to identify the noun phrases and associate them to the ground-truth referring expressions. Thus, the spaCy embeddings closest to the ground-truth expression are used in the mask selection. This is followed by extracting the maximum attention point to feed into SAM~\cite{kirillov2023segment} as a point prompt.

\begin{table*}[t]
\centering
\begin{tabular}{lc|cccc|c}
\hline
\textbf{Method} & \textbf{Gr. Sup.} & \multicolumn{4}{|c|}{\textbf{MMVP \& PixMMVP}}  \\ 
                &                  & $\mathcal{A}\dagger$  & $\mathcal{A}$ & $\mathcal{M}$ & $\mathcal{M}\dagger$ & $\mathcal{S}$\\\hline
LLava 1.5 (7B)~\cite{liu2024visual}  &   \xmark      &     \textbf{27.3}       & \textbf{27.3}      &     -      &     -     & -\\
LLava 1.5 (13B)~\cite{liu2024visual} &   \xmark      &     \textbf{39.3}       & \textbf{30.7}      &     -      &     -     & -\\
Cambrian (8B)*~\cite{tong2024cambrian}  &   \xmark   & \textbf{53.3}  & \textbf{51.3} &     -      &     -     & -\\
OMG LLava (7B)~\cite{zhang2024omg}  &   \checkmark   &     12.0       & 12.0      &    17.8    &     38.0  & 25.0\\
GLAMM (7B)~\cite{rasheed2024glamm} &   \checkmark    &      -         &   2.7     &    \textbf{31.5}    &     \textbf{47.4}  & \textbf{25.1}\\
GLAMM - RegCap (7B)~\cite{rasheed2024glamm} &   \checkmark    &      12.7              &    6.7    &    14.5    &     18.6  & 15.7\\
LISA (7B)~\cite{lai2024lisa}       &   \checkmark    &       6.7              &    -      &     -      &    42.9   & 24.8\\
LLava-G (7B)~\cite{zhang2025llava}    &    \checkmark   &        8.7             &    -      &     17.8   &     13.5  & 13.3\\

LLava 1.5 (7B) + (a+s)~\cite{cao2024emerging}  &  \xmark &       \textbf{27.3}   &     \textbf{27.3}  &   11.1  &  11.2 & 19.3 \\ 
LLava 1.5 (13B) + (a+s)~\cite{cao2024emerging} &  \xmark &        \textbf{39.3}  &    \textbf{30.7}   &    9.8  &  11.4 & 25.4\\ 
Cambrian (8B)* + (a+s)~\cite{cao2024emerging}  &  \xmark &      \textbf{53.3}            & \textbf{51.3}  &    13.0 &  13.2 &  33.3\\ \hline

PixFoundation (LLava (7B)) (Ours)    & \xmark  &    \textbf{27.3}  &   \textbf{27.3} &  17.7  & 19.2 & 23.3\\ 
PixFoundation (LLava (13B)) (Ours)    & \xmark  &    \textbf{39.3}  &     \textbf{30.7}  &   15.1 & 17.2 & \textbf{28.3}\\ 
PixFoundation (Cambrian (8B)*) (Ours)    & \xmark  &  \textbf{53.3} &  \textbf{51.3}  &   17.7 & 19.1 & \textbf{36.2}\\ \hline
\multicolumn{7}{l}{\textbf{Upper Bound - Oracle Selection}} \\ \hline
PixFoundation$\dagger$ (LLava (7B)) (Ours)    & \xmark  &     27.3  &    27.3 &  \textbf{\textcolor{red}{26.1}}   & \textbf{\textcolor{red}{38.0}}   & \textcolor{red}{\textbf{32.7}}\\ 
PixFoundation$\dagger$ (LLava (13B)) (Ours)    & \xmark  &    39.3  &   30.7  &    \textbf{\textcolor{red}{23.6}} &   \textbf{\textcolor{red}{38.2}} & \textcolor{red}{\textbf{38.8}}\\ 
PixFoundation$\dagger$ (Cambrian (8B)*) (Ours)    & \xmark  &  53.3 & 51.3 &   \textbf{\textcolor{red}{51.5}}  &   \textbf{\textcolor{red}{56.5}} &  \textcolor{red}{\textbf{54.9}}\\ \hline

\end{tabular}
\caption{Evaluation of the various pixel-level MLLMs and the different baselines on PixMMVP benchmark. We evaluate the visual question answering accuracy and accuracy using GPT-4o in the first two protocols (i.e., $\mathcal{A}, \mathcal{A}\dagger$ resp.). Additionally, we evaluate pixel-level visual grounding ability with output segmentation masks in the first and third protocols (i.e., $\mathcal{M}, \mathcal{M}\dagger$ resp.). *: models using Llama 3 (8B) unlike the rest that are relying on Vicuna (7B and 13B) for the base LLM. - : indicates either the model can not be evaluated in that setting, or has low results below 1\% showing complete failure in that setting. $\mathcal{S}$: denotes the score of the MLLM that is average of $\text{max}(\mathcal{A}, \mathcal{A}\dagger)$ and $\text{max}(\mathcal{M}, \mathcal{M}\dagger)$. The oracle results are highlighted in red, and the best in each variant (7B, 13B, and 8B) are bolded.}
\label{tab:pixmmvp}
\end{table*}

For our two baselines, we build upon the previous pipeline and build an \textit{oracle} baseline and an \textit{automatic} baseline. We introduce two major modifications to account for our observation that the correct grounding can occur with different output tokens describing the object not necessarily aligning with the exact ground-truth expression. The first modification is to go through all the potential output tokens without relying on spaCy embeddings. In the \textit{oracle} baseline we rely on the ground-truth mask to select the correct token and its corresponding attention map as an upper bound, hence why we call it an \textit{oracle}. The other baseline, \textit{automatic}, uses a simple but powerful mechanism where we overlay the predicted masks on the original image to highlight the potential object of interest. This is followed by feeding these images to a multi-modal LLM inquiring on which is best in highlighting the object of interest. Specifically, we use the following prompt ``Select the image that has $<$EXPR$>$ best highlighted in red color than the others? Answer with a number from 1 to $<$N$>$ and mention the number only. $<$IMG$>$'', where  $<$EXPR$>$ and $<$IMG$>$ are the ground-truth expression and the image tokens respectively. In our experiments we rely on the open source Cambrian-1(8B)~\cite{tong2024cambrian} model in our automatic baseline for the mask selection. The second major modification since SAM model has a good understanding of the ambiguity from point prompting, as such we process three potential output masks for each prompt instead of a single one. This enables us to utilize the power of SAM in identifying fine-grained objects and referring expressions that tends to surpass what other MLLMs do, even if trained with full grounding supervision. %These baselines serve the main purpose that the correct grounding is already embedded in these MLLMs without any grounding supervision and the \textit{oracle} upper bound shows it surpasses any other MLLM with a significant margin. Thus, our proposed benchmarks and baselines confirm that pixel-level grounding is already embedded in such MLLMs and there is still plenty of opportunity to improve the grounding output further if equipped with the right tool to identify when grounding emerges.


