
\begin{figure}[h]
\centering
    \includegraphics[width=0.52\textwidth]{figures/ICML25PixFoundation.drawio.pdf}
    \vspace{-1em}
    \caption{The two major research questions we explore: (i) the grounding/VQA ability of pixel-level MLLMs in challenging scenarios (first row), (ii) the ability of vanilla MLLMs to perform grounding and when does it emerge (second row). Second row shows the noun phrase and its corresponding segmentation mask, highlighted in red, extracted from Llava 1.5~\cite{liu2024improved} attention maps with three possible output masks to accommodate ambiguity in the point prompt, highlighted as a black dot.}
    \vspace{-1em}
    \label{fig:overview}
\end{figure}

% Remember to argue against why simple referring segmentation benchmarks might not be the same challenge as vision centric ones. Perhaps reword the term vision centric.
There have been numerous advancements in pixel-level image and video understanding, including tasks such as image/video segmentation~\cite{zhou2022survey,minaee2021image,kirillov2023segment,ravi2024sam}, pixel-level visual grounding and reasoning~\cite{rasheed2024glamm,lai2024lisa}, depth estimation~\cite{yang2024depth} and tracking~\cite{wang2023tracking}. The majority of these have been transformed with the emergence of foundation models~\cite{bommasani2021opportunities}, specifically multi-modal large language models (MLLMs)~\cite{liu2024visual,dai2023instructblip}. Nonetheless, pixel-level MLLMs have shown degradation in their capabilities in chat performance~\cite{lai2024lisa}. Recent models tried to address this gap~\cite{zhang2024omg,zhang2025llava}, yet they relied on standard evaluation benchmarks overlooking the shortcomings of current MLLMs.

Recent efforts explored the shortcomings of MLLMs in vision centric benchmarks~\cite{tong2024eyes,tong2024cambrian}. Such benchmarks focused on challenging visual patterns and tasks such as counting and relative positioning. Nonetheless, these benchmarks did not evaluate the recent pixel-level MLLMs. In this work we propose challenging vision centric benchmarks that are dedicated to evaluate these models. Through these, we answer the first research question; \textit{``Are the current pixel-level MLLMs trained with full grounding supervision heading in the right direction to improve both grounding and visual question answering (VQA)?''.} Our findings show that the majority of pixel-level MLLMs still fall short in such a challenging setting. While evidently, some of these show superior performance in visual grounding, we show that MLLMs that were not trained with pixel-level grounding can have better performance without degrading their chat capabilities.

There have been recent works showing training-free open vocabulary segmentation emerging from vision language models~\cite{wang2025sclip,luo2024emergent,hajimiri2024pay}. Concurrent work has specifically explored emerging grounding in MLLMs~\cite{cao2024emerging}. We are inspired by the previous work to establish a baseline on our proposed benchmarks. However, unlike their method we focus on the second research question of \textit{``When does grounding emerge in MLLMs that are not trained with full grounding supervision?''}. Previous efforts studying emergent segmentation and/or grounding assumed correspondence with the specific language tokens (i.e., nouns or noun phrases) of the objects of interest. However, our work documents that emerging grounding in MLLMs does not necessarily coincide with the exact language tokens of the object. We show the most frequent emergence in PixMMVP happens in the last 40-60\% of the output, and can coincide with concepts about object parts, position, color or context of these objects. Fig.~\ref{fig:overview} summarizes our research questions. 

In summary, our contributions include: (i) Proposing pixel-level vision centric benchmarks, PixMMVP and PixCV-Bench, with segmentation annotations and referring expression of the object of interest in the corresponding questions~\cite{tong2024cambrian,tong2024eyes}. (ii) Benchmarking recent efforts in pixel-level MLLMs where we show that they degrade VQA capabilities. More importantly, some of them lag in visual grounding with respect to simple techniques of extracting the segmentation from vanilla MLLMs, i.e., MLLMs that are not trained for pixel-level grounding. (iii) We provide a simple mechanism for extracting segmentation from vanilla MLLMs, with an understanding of when grounding emerges. Our mechanism uses the observation that grounding can emerge corresponding to different output tokens describing the object's appearance or location, not necessarily the exact text of the object of interest. We call it PixFoundation, since it can be plugged with any MLLM to mine for pixel-level understanding.

