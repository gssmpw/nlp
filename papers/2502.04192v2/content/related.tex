\textbf{Pixel-level Vision Foundation Models.} There have been various vision foundation models released that were trained with supervision for the segmentation task (e.g., SAM, SAM 2.0)~\cite{kirillov2023segment, ravi2024sam}. Orthogonal to this, some methods discussed the ability of vision foundation models such as CLIP and BLIP in image segmentation without any segmentation supervision~\cite{luo2024emergent,hajimiri2024pay,wang2025sclip}. Yet, they relied on earlier foundation models that did not incorporate the power of large language models. Combining large language models with vision has been extensively researched with pioneering works such as Llava~\cite{liu2024visual, liu2024improved} and instruct-BLIP~\cite{dai2023instructblip}. Multiple works afterwards focused on pixel-level visual grounding in these MLLMs with full supervision~\cite{lai2024lisa,rasheed2024glamm,zhang2025llava,zhang2024omg,zhang2025llava,zhang2024omg}. However, these methods were lagging in their chat performance. Notably, pixel-level MLLMs were not evaluated on the challenging benchmarks that focused on the shortcomings of MLLMs~\cite{tong2024eyes,tong2024cambrian}. Hence, it is still unclear if the pixel-level grounding supervision helped to improve their ability on these challenging tasks or not. In this work, we focus on the previous question to have a better understanding of their performance. Concurrent work, has shown that without pixel-level supervision there is an emerging ability to perform pixel-level grounding~\cite{cao2024emerging}. We rely on this method as our baseline, but unlike previous works we provide an insight on when grounding emerges in such MLLMs. We propose another baseline that uses a novel and simple mechanism to perform mask selection while taking the previous insight into consideration. Our baseline outperforms the previous methods trained with full grounding supervision and the ones without it.


\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{images/pixelmllms_failures/failures_pixmllms_1.drawio.pdf}
\vspace{-2em}
\caption{First shortcoming of pixel-level MLLMs is the degraded performance in visual question answering. The Predicted segmentation masks corresponding to the [SEG] token/s are highlighted in red.} 
\vspace{-1em}
\label{fig:shortcoming1}
\end{figure*}

\textbf{Benchmarking Multi-modal Large Language Models.}
 There is an abundance of standard benchmarks used for evaluating MLLMs (e.g., MMU~\cite{yue2024mmmu}) and pixel-level benchmarks (e.g., refCOCO/+/g~\cite{yu2016modeling,kazemzadeh2014referitgame}). These have pushed the limits on MLLMs capabilities in terms of VQA and visual grounding. Nonetheless, there have been various works that discussed the shortcomings of MLLMs. One of them discussed the shortcomings in CLIP~\cite{radford2021learning}, which is used in various MLLMs as a visual backbone. They proposed a benchmark, MMVP~\cite{tong2024eyes}, that is focused on the visual aspects within a VQA task. More recently, CV-Bench~\cite{tong2024cambrian} focused on two major tasks that are vision focused which are counting and relative positioning. Both were proposed to evaluate MLLMs that do not have the ability to generate segmentation output. Nonetheless, they still provide quite challenging scenarios that can act as a strong benchmark for the pixel-level MLLMs counterpart. In this work, we extend these two benchmarks to augment it with pixel-level annotations and referring expressions that correspond to the object of interest within their VQA task. These help to understand if failures occurring in these benchmarks stem from grounding or VQA capabilities.