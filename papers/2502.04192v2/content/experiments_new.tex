\begin{figure}[t]
\centering
\input{graphs/ablation_point_prompts}
\vspace{-1em}
\caption{Ablation on the point prompt used paired with the \textit{oracle} selection on PixMMVP. We evaluate using the second probing reporting mean intersection over union, $\mathcal{M}$.}
\label{fig:ablation}
\vspace{-1em}
\end{figure}

\subsection{Experimental Setup}

\textbf{Evaluation benchmarks, protocols and metrics.}
PixMMVP is composed of 300 images paired with questions, choices, referring expressions and segmentation masks, while PixCV-Bench has 1,438 images with their corresponding annotations similarly. On each benchmark we evaluate the VQA and visual grounding capabilities following three probing techniques and reporting their metrics. The first probing is to evaluate the VQA ability, where the accuracy is computed using GPT-4o following~\cite{tong2024eyes} as, $\mathcal{A}\dagger$. If the model generates a segmentation without explicitly asking it to, it is evaluated with respect to the ground-truth referring segmentation in terms of mean intersection over union as, $\mathcal{M}\dagger$. The second probing prompts the model to identify the referred expression then evaluates the mean intersection over union reported as, $\mathcal{M}$. The third probing following~\cite{tong2024cambrian} instructs the model to generate a single option letter and evaluate the accuracy directly without GPT-4o, reported as, $\mathcal{A}$. There is a need for the first probing since some of the recent pixel-level MLLMs face challenges in following instructions. We evaluate the score of each model, $\mathcal{S}$, which is the harmonic mean across the maximum of both pixel-level visual grounding and VQA,
\begin{equation}
\mathcal{S} = \frac{2}{\frac{1}{\text{max}(\mathcal{A}, \mathcal{A}\dagger)} + \frac{1}{\text{max}(\mathcal{M}, \mathcal{M}\dagger)}}.
\end{equation}

We mainly focus on evaluating four state-of-the-art pixel-level MLLMs; LISA~\cite{lai2024lisa}, GLAMM~\cite{rasheed2024glamm}, OMG-Llava~\cite{zhang2024omg} and Llava-G~\cite{zhang2025llava}. For GLAMM we use two variants; the original model (GLAMM) and the one fine-tuned for region-level captioning, (GLAMM-RegCap). For details on the models weights, refer to App.~\ref{app:impdetails}.

\textbf{Baselines and upper bound implementation details.} We evaluate: (i) the attend and segment (a+s), (ii) the \textit{oracle} selection relying on the highest intersection over union in selecting the correctly predicted masks (PixFoundation$\dagger$), and (iii) the \textit{automatic} selection, (PixFoundation). These are implemented on top of three base MLLMs which are, Llava 1.5 (7B, 13B)~\cite{liu2024improved} and Cambrian-1(8B)~\cite{tong2024cambrian}. The automatic selection is implemented using GPT-4o, refer to App.~\ref{app:impdetails} for more details.

\subsection{Are the current pixel-level MLLMs heading in the right direction?}
In order to answer this, we evaluate each of these pixel-level MLLMs VQA capabilities in challenging tasks. Additionally, we evaluate their ability to visually ground the objects of interest in these questions. Table~\ref{tab:pixmmvp} shows the results on the challenging PixMMVP and PixCV-Bench. From the accuracy of VQA, MLLMs that are not trained with pixel-level grounding surpass their pixel-level counterpart with up to 16\%. The best in pixel-level MLLMs score in this aspect is GLAMM-RegCap~\cite{zhang2024omg} yet it has degraded ability to generate segmentation. On the other hand, when looking at pixel-level visual grounding we find the best model, GLAMM~\cite{rasheed2024glamm}, has a weak ability in VQA or following instructions. Moreover, it shows LISA and Llava-G are mostly incapable of following the instruction to output the option letter reported in, $\mathcal{A}$. OMG-Llava strikes the right balance in both VQA and pixel-level grounding with the highest score, $\mathcal{S}$, within pixel-level MLLMs. However, looking at the bottom three rows, the \textit{oracle}, PixFoundation$\dagger$, confirms that MLLMs that were never trained with pixel-level grounding have the correct grounding within their learned attention maps yielding higher score, $\mathcal{S}$, than OMG-Llava. Qualitative comparison is provided in Fig.~\ref{fig:qual} and additional examples are in App.~\ref{app:qual}. Additionally, the \textit{automatic} baseline outperforms OMG-Llava in PixMMVP with a considerable margin and is on-par to it on PixCV-Bench. Furthermore, the attend and segment baseline~\cite{cao2024emerging} lags behind our \textit{automatic} method with more than 10\% in both datasets. Refer to App.~\ref{app:failure} for a failure case analysis.

Finally, we evaluate whether the failures of these MLLMs occur in visual grounding, VQA or both. Figure~\ref{fig:acciou-mmvp} shows the frequency of failures per category, where the majority stem from failures in both especially in the pixel-level MLLMs. While the vanilla MLLMs perform better in the VQA and tend to fail more in the grounding task.

\textbf{Summary.} In summary, pixel-level grounding supervision degrades MLLMs ability in VQA and sometimes even their generalization in grounding. We show that MLLMs trained with pixel-level supervision lag behind vanilla MLLMs using simple mechanisms to extract grounding, and the \textit{oracle} indicates there is an opportunity to improve this. Moreover, we show that grounding might not coincide with the noun phrase most similar to the referred expression, where our \textit{oracle} upper bound and \textit{automatic} baseline are both surpassing the attend and segment. %We also show our proposed automatic baseline provides a simple but powerful technique to overcome this.

\subsection{When does grounding emerge in MLLMs?}
\textbf{When - location.} Taking into account the powerful performance of the \textit{oracle} upper bound, it begs the important question of when grounding emerges. We start by looking at when it emerges in terms of the location within the output text. We analyze the word/phrase location with respect to the full output text in terms of a percentage from its total length, (i.e., 0\% means the beginning of the text). Accordingly, Fig.~\ref{fig:tokenloc} shows the location percentages histogram, binned at 10\%, for the three base MLLMs reporting the oracle selection and evaluating on PixMMVP benchmark using the second probing. In the Llava 1.5 variants, the highest grounding emerges at the last 40\% of the output text, while for Cambrian it emerges at the last 60\%. 

\textbf{When - concept.} For the second analysis we look into the concept category that the correct output word/phrase corresponds to. The previous assumption in other works is that grounding emerges in the exact noun/noun phrase of the object of interest. Except our analysis confirms that this is not necessarily the case. We take the correct noun/noun phrase where the grounding emerges based on the \textit{oracle} from all the three variants, then we pass it to GPT-4o to request a grouping of these concepts. It resulted into six main groups, which are: (i) Color and appearance, (ii) location and position, (iii) object parts, (iv) context and setting, (v) objects and entities, and (vi) State. We then prompt for each of the noun/noun phrase, GPT-4o, to categorize it within these six categories. The histogram of the occurrences of these concept categories is shown in Fig.~\ref{fig:tokenconcept}. It clearly conveys that in certain scenarios the correct output when grounding emerges can be describing the position or the color of the object of interest not necessarily the exact ground-truth referring expression. Fig.~\ref{fig:when_imgs} shows qualitative examples of these scenarios from PixMMVP using the second probing. Additional results are provided in App.~\ref{app:qual}.

\textbf{Random vs. best.} All the results of our baselines and our findings hinge on the fact that we are using the maximum attention per output noun phrase to prompt SAM for the segmentation mask. Nonetheless, as a lower bound analysis, we evaluate the performance if we use a random point as prompt to SAM instead. For fair comparison, we generate random points with the count of output masks that the oracle has to select among (i.e., the number of the output noun phrases). We conduct this ablation on PixMMVP using Llava 1.5 (7B) base MLLM, with random point prompts followed by the \textit{oracle} selection among their SAM masks. Figure~\ref{fig:ablation} shows that this random + oracle baseline is a strong baseline, yet it lags behind the correct one using the maximum point (i.e., First) with around 12\%. More importantly, we confirm the stability of the results if we select the second best or third best attention (i.e., Second and Third), which are on-par to the maximum point. Thus, even with the oracle selection, using the wrong point prompts lags with a considerable margin behind using the correct ones. 

\textbf{Summary.} In summary, we found that highest frequent emergence of grounding is in the last 40-60\% of the output text on PixMMVP which might indicate that the reasoning of the model on this challenging task to provide a response impacts when the correct visual grounding can occur. More importantly, we show that grounding in MLLMs can emerge in the noun phrase that corresponds to color, position or other characteristics of the object of interest and not necessarily the exact referring expression. 