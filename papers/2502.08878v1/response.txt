\section{Related Work}
Our algorithms are in the area of privacy preserving algorithms with differential privacy guarantee which is the de facto standard of privacy (we refer to Dwork et al., "Differential Privacy" for an introduction to this area). As we covered the application and prior work on private partition selection in the introduction, we now provide more details on the work most related to our paper.

The differentially private partition selection problem was first studied in Hardt et al., "A Study of Differential Privacy" They utilized the now-standard approach of subsampling to limit the number of items in each user's set, constructing weights over items, and thresholding noised weights to produce the output. They proposed a version of the basic weighting algorithm which uses the Laplace mechanism rather than the Gaussian mechanism. This algorithm was also used in McSherry and Talwar, "Differential Privacy with Exponential Mechanism" within the context of a private SQL system.
The problem received renewed study in Vadhan et al., "Private Selection from Private Data" where the authors propose a generic class of greedy, sequential weighting algorithms which empirically outperform basic weighting (with either the Laplace or Gaussian mechanism). Kairouz et al. gave an alternative greedy, sequential weighting algorithm which leverages item frequencies in cases where each user has a multiset of items.
Bassily and Smith analyzed in depth the optimal strategy when each user has only a single item (all sets have cardinality one). This is the only work that does not utilize the weight and threshold approach, but it is tailored only for this special case.
The work most related to ours is DP-SIPS Dwork et al., "DP-SIPS: Distributed Private Selection with Iterative Splitting" which proposes the first algorithm other than basic weighting which is amenable to implementation in a parallel environment. DP-SIPS splits the privacy budget over a small number of rounds, runs the basic algorithm as a black box each round, and iteratively removes the items found in previous rounds for future computations. This simple idea leads to large empirical improvements, giving a scalable algorithm that has competitive performance with sequential algorithms.