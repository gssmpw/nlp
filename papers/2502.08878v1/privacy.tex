We now prove the privacy of our main algorithm by bounding its $\ell_2$ and novel $\ell_\infty$ sensitivities.
% New theorem for the biased algorithm
\begin{theorem}[Privacy of \ouralgotworounds{}]\label{thm:dp-two-rounds}
\cref{alg:max-deg-two-round} is $(\eps_1 + \eps_2, \delta_1 + \delta_2)$-DP.
\end{theorem}

The rest of this section will be devoted to proving this theorem. We first state as a corollary that \ouralgo{} run in a single round without biases is also private.

\begin{corollary}[Privacy of \ouralgo{}]\label{thm:dp-one-round}
Releasing the output $U$ from \cref{alg:generic} run with unbiased \ouralgo{} (\cref{alg:max-deg}) as the weighting algorithm and with $h(t) = \frac{1}{\sqrt{t}}$ is $(\eps, \delta)$-DP.
\end{corollary}
\begin{proof}
This follows directly from \cref{thm:dp-two-rounds} by setting $(\epsilon_1, \delta_1) = (\epsilon, \delta)$ and $(\epsilon_2, \delta_2) = (0,0)$.
\end{proof}

To prove privacy, consider a weight vector $w$ returned by \cref{alg:max-deg} for an input $\sets = \{(u, S_u)\}_{u=1}^n$, $\tau$, $\dmax$, $b$, $\bmin$, $\bmax$ and the output $w'$ for an input $\sets' = \sets \cup \{(v, S_v)\}$, $\tau$, $\dmax$, $b'$, $\bmin$, $\bmax$ which includes a new user $v$ not in the original input.
Let $d_v$ be the degree of the new user.
Let $T = S_v \setminus \cup_{u=1}^n S_u$ be the subset of items which appear only in $S_v$ and not in any of the original user sets, and let $t = |T|$. The vectors of biases $b$ and $b'$ are defined on the set of items $\cup_{u=1}^n S_u$ and 
$\left(\cup_{u=1}^n S_u \right) \cup T$ respectively. 
We remind the notation used in \cref{alg:max-deg} for vector $w^v_b$ to denote the biased weight vector the new user $v$ computes by calling \cref{alg:user-biased}, i.e. \userweights.
We state some properties of the biased weights $w_b$ which will be helpful in the proof.
\begin{lemma}\label{lem:l2-bound-user-weights}
Let $S_u, b, \bmin, \bmax$ be valid inputs to \cref{alg:user-biased}, and let $w_b$ be the weight vector returned by the algorithm.
Let $d = |S_u|$.
Then, the following hold:
\begin{itemize}
    \item $w_b(i) = 0$ for all $i \in \U \setminus S_u$
    \item $\frac{\bmin}{\sqrt{d}} \leq w_b(i) \leq \frac{\bmax}{\sqrt{d}}$ for all $i \in S_u$
    \item $\|w_b\|_2 \leq 1$
\end{itemize}
\end{lemma}

\begin{proof}
The first claim holds as the weight vector is initialized with zeros and only indices $i \in S_u$ are updated by the algorithm.

To simplify the notation, we define $d = |S_u| = |S_{biased}| + |S_{unbiased}|$.
As $b(i) \leq 1$ for all $i \in \U$, the initial weights given to items in $S_{biased}$ are between $\frac{\bmin}{\sqrt{d}}$ and $\frac{1}{\sqrt{d}}$. We also know that $\frac{1}{\sqrt{d}} \leq \frac{\bmax}{\sqrt{d}}$.
The sum of squares of these weights will thus be between $\frac{\bmin^2 |S_{biased}|}{d}$ and $\frac{|S_{biased}|}{d}$.
Call this value $k$.

Weights of items in $S_{unbiased}$ are given by
\begin{equation*}
    \min\bc{\frac{\bmax}{\sqrt{d}}, \sqrt{\frac{1 - k}{|S_{unbiased}|}}}
\end{equation*}
We show that the minimum of these two terms is at least $\frac{1}{\sqrt{d}} \geq \frac{\bmin}{\sqrt{d}}$. The first term is at least $\frac{1}{\sqrt{d}}$ since $\bmax \geq 1$. To observe the same for the second term, one should   plugg in the upper bound of $k \leq \frac{|S_{biased}|}{d}$. 
%Note that the weights will be at least $\frac{1}{\sqrt{d}} \geq \frac{\bmin}{\sqrt{d}}$ plugging in the upper bound of $k \leq \frac{|S_{biased}|}{d}$ to the second term of the minimization and as $\bmax \geq 1$.
By construction, the weights are upper bounded by $\frac{\bmax}{\sqrt{d}}$.
Furthermore, note that the sum of squares of the entire weight vector at this point is upper bounded by $1$. In particular, it is equal to $1$ for the second term of the minimization:
\begin{equation*}
    k + \sum_{i \in S_{biased}} \p{\frac{1 - k}{|S_{unbiased}|}}
    = k + (1 - k)
    = 1.
\end{equation*}

In the remainder of the algorithm, sum of the weights of items in $S_{small} \subseteq S_{unbiased}$ may increase if the $\ell_2$ norm of the weight vector is strictly less than $1$.
Consider the weights after any such update by a multiplicative factor $C$ defined as
\begin{equation*}
    C = \min\bc{
    \frac{\bmax/\sqrt{d}}{\max_{i \in S_{small}} w_b(i)},
    \sqrt{1 + \frac{1 - \sum_{i \in S_u} w_b(i)^2}{\sum_{i \in S_{small}} w_b(i)^2}}}.
\end{equation*}
Note that $C > 1$ by definition of $S_{small}$ and the stopping criteria of the while loop. Therefore, none of the final weights will be less than $\frac{\bmin}{\sqrt{d}}$.
Consider the first case of the minimization.
Any updated weight $C \cdot w_b(i)$ for $i \in S_{small}$ will be at most $\frac{\bmax}{\sqrt{d}}$ as
\begin{equation*}
    \frac{\bmax/\sqrt{d}}{\max_{j \in S_{small}} w_b(j)} \cdot w_b(i)
    \leq 
    \max_{i^* \in S_{small}} \frac{\bmax/\sqrt{d}}{w_b(i^*)} w_b(i^*)
    = \bmax/\sqrt{d}.
\end{equation*}
Now, consider the second case of the maximization.
Then, the squared $\ell_2$ norm of the weight vector will be
\begin{equation*}
    \sum_{i \in S_u \setminus S_{small}} w_b(i)^2
    + \sum_{i \in S_{small}} \p{1 + \frac{1 - \sum_{j \in S_u} w_b(j)^2}{\sum_{j \in S_{small}} w_b(j)^2}} w_b(i)^2
    = \p{\sum_{i \in S_u} w_b(i)^2} + \p{1 - \sum_{j \in S_u} w_b(j)^2}
    = 1.
\end{equation*}
As $C$ is taken to be the minimum of these two values, the final weight vector will satisfy all of the required bounds.
\end{proof}


\begin{lemma}[Novel $\ell_\infty$ sensitivity with biases]\label{lem:linf-sensitivity-biased}
Assume that the adaptive threshold is $\tau \geq 1$, and the maximum adaptive degree is $\dmax \geq 4$. Then, \cref{alg:max-deg} has novel $\ell_\infty$ sensitivity bounded by
\[
    \Delta_\infty(t) \leq \frac{\bmax}{\sqrt{t}}.
\]
where $t = |T|$ is the cardinality of $T$, the set of novel items.
\end{lemma}
\begin{proof}
Unpacking \cref{def:linf-sensitivity}, it suffices to show that
\[
    \max_{i \in T} w'(i) \leq \frac{\bmax}{\sqrt{t}}.
\]
Note that this is trivially true if $d_v = |S_v| > \dmax$ or $d_v < \ceil{\frac{1}{(\bmin)^2}}$ since $v$ does not participate in adaptivity due to its too low or high degree. We will proceed by assuming this is not the case.

Consider the final weight of an item $i$ in the set of novel items $T \subseteq S_v$:
\begin{equation*}
    w'(i) = w'_{trunc}(i) + w'_{reroute}(i) + w^v_b(i) - w'_{init}(i). 
\end{equation*}
Note that for all novel items $i \in T$, $w'_{init}(i) = \frac{1}{d_v} \leq \tau$ as $v$ is the sole contributor to the weight of item $i$.
Therefore, no weight is truncated or rerouted from these items: $w'_{trunc}(i) = w'_{init}(i) =  \frac{1}{d_v}$.
Expanding the definition of rerouted weight,
\begin{align*}
    w'_{reroute}(i) &= \frac{\alpha e'_v}{\dmax} \\
    &= \frac{\alpha}{\dmax d_v} \sum_{j \in S_v \setminus T} r'(j) \\
    &= \frac{\alpha}{\dmax d_v} \sum_{j \in S_v \setminus T} \frac{w'_{init}(j) - w'_{trunc}(j)}{w'_{init}(j)} \\
    &\leq \frac{\alpha}{\dmax d_v} \p{d_v - t}\\
    &= \frac{\alpha}{\dmax}\p{1 - \frac{t}{d_v}}.
\end{align*}
Note that $w_b^v(i) \leq \frac{\bmax}{\sqrt{d_v}}$ by construction (see \cref{lem:l2-bound-user-weights}).
Overall, we can bound $w'(i)$ as
\begin{align}
    w'(i) &= w'_{trunc}(i) + w'_{reroute}(i) + w^v_b(i) - w'_{init}(i) \notag \\
    &\leq \frac{1}{d_v} + \frac{\alpha}{\dmax}\p{1 - \frac{t}{d_v}} + \frac{\bmax}{\sqrt{d_v}} - \frac{1}{d_v} \notag \\
    &= \frac{\alpha}{\dmax}\p{1 - \frac{t}{d_v}} + \frac{\bmax}{\sqrt{d_v}}. \label{eq:linf-upper-bound}
\end{align}

In the rest of the proof, we will show that the upper bound \cref{eq:linf-upper-bound} is maximized when $d_v = t$, i.e., when the first term is zero.
Recall that $t \leq d_v \leq \dmax$.
Consider the partial derivative with respect to $d_v$:
\begin{equation*}
    \frac{\partial}{\partial d_v} \p{\frac{\alpha}{\dmax}\p{1 - \frac{t}{d_v}} + \frac{\bmax}{\sqrt{d_v}}}
    = \frac{\alpha}{\dmax} \frac{t}{d_v^2} - \frac{\bmax}{2d_v^{3/2}}.
\end{equation*}
Consider the condition of the derivative being non-positive:
\begin{align*}
    &0 \geq \frac{\alpha}{\dmax} \frac{t}{d_v^2} - \frac{\bmax}{2d_v^{3/2}} \\
    \iff &\bmax \geq \frac{2\alpha t}{\dmax \sqrt{d_v}} \\
    \Longleftarrow\; &\bmax \geq \frac{2\alpha\sqrt{t}}{\dmax} \\
    \Longleftarrow\; &\bmax \geq \frac{2\alpha}{\sqrt{\dmax}}.
\end{align*}
The final condition holds as $\alpha \leq 1$ and by the assumption that $\dmax \geq 4$. We note that $\bmax$ is always set to be at least $1$.
As the derivative is non-positive, the right side of \cref{eq:linf-upper-bound} is maximized when $d_v$ is minimized at $d_v=t$.
Then, $\Delta_\infty(t) \leq \frac{\bmax}{\sqrt{t}}$, as required.
\end{proof}



We will prove a useful fact that \cref{alg:max-deg} is monotone in the sense that weights when run on $\sets'$ will only increase compared to when run on only $\sets$. We apply this proposition in upper bounding the $\ell_2$ sensitivity of \cref{alg:max-deg} in \cref{lem:l2-sensitivity-biased-weights}.
\begin{proposition}[Monotonicity]\label{lem:monotone}
For all $i \in \cup_{u \in \{1, \ldots, n, v\}} S_u$,
\begin{align*}
    w'_{init}(i) &\geq w_{init}(i) \\
    w'_{trunc}(i) &\geq w_{trunc}(i) \\
    w'_{reroute}(i) &\geq w_{reroute}(i).
\end{align*}
\end{proposition}
\begin{proof}
Fix any item $i$.
As all increments to the initial $\ell_1$ bounded weights are positive and non-adaptive,
\[
    w'_{init}(i) \geq w_{init}(i).
\]
In fact, the two weights are either equal or differ by a factor of $1/d_v$ depending on whether $i \in S_v$.
The calculations of the fraction of excess weight that exceeds the threshold, the truncated weights, the excess weight returned to each user, and the rerouted weights are all monotonically non-decreasing with the initial weights.
Therefore,
\[
    w'_{trunc}(i) \geq w_{trunc}
\]
and
\[
    w'_{reroute}(i) \geq w_{reroute}.
\]
\end{proof}


Before bounding the $\ell_2$ sensitivity of \ouralgo{}, we will need the following basic proposition and technical lemma.

\begin{proposition}[Taylor expansion of $\sqrt{1+x}$ as $x \to 0$]\label{prop:taylor-sqrt}
\[
\lim_{x \to 0} \sqrt{1 + x} = \sum_{n=0}^\infty \frac{\prod_{k=1}^n \left(\frac{3}{2} - k\right)}{n!} x^n.
\]
\end{proposition}


\begin{lemma}\label{lem:gen-limit}
For a constant $1\leq C \leq 2$,
consider the following function of $x$ parameterized by an auxiliary variable $y$:
\begin{equation}\label{eq:fxy}
    f(x;y) = \frac{\sqrt{y}}{x}\left(1 - \sqrt{1 - \frac{Cx}{\sqrt{y}} + \frac{x}{y}}\right).
\end{equation}
For any $y > 1$,
\[
    \inf_{x \in (0,1]} f(x;y)
    = \frac{C}{2} - \frac{1}{2\sqrt{y}}.
\]
\end{lemma}
\begin{proof}
To minimize $f$, we will evaluate the function at any stationary points (in terms of $x$) as well as the boundaries $x = 1$ and $x \to 0$.
Consider the derivative
\begin{equation*}\label{eq:derivative}
    \frac{d}{d x}f(x;y) = - \frac{\sqrt{y}}{x^2}\left(1 - \sqrt{1 - \frac{C x}{\sqrt{y}} + \frac{x}{y}}\right) + \frac{\sqrt{y}}{x}
    \left(\frac{\frac{C}{\sqrt{y}} - \frac{1}{y}}{2\sqrt{1 - \frac{C x}{\sqrt{y}} + \frac{x}{y}}}\right).
\end{equation*}
Let $A = \sqrt{1 - \frac{C x}{\sqrt{y}} + \frac{x}{y}}$.
To look for stationary points and will set the derivative of $f$ to zero:
\[
 \frac{d}{d x}f(x;y) = 0 \iff
 -\frac{1}{x} (1-A) + \frac{\frac{C}{\sqrt{y}} - \frac{1}{y}}{2A} = 0 \iff
 x \left(\frac{C}{\sqrt{y}} - \frac{1}{y}\right) <  2A(1-A)
\]
We expand $A^2$ to get the simpler form:
\begin{align*}
    \frac{C x}{\sqrt{y}} - \frac{x}{y} = 2A - 2\left(1 - \frac{C x}{\sqrt{y}} + \frac{x}{y}\right)
    &\iff 0 = 2A - 2 + \frac{C x}{\sqrt{y}} - \frac{x}{y} = -A^2 + 2A - 1 = -(A - 1)^2 \\
    &\iff A = 1.
\end{align*}
From the definition of $A$,
\[
    A = 1 \iff
    1 - \frac{C x}{\sqrt{y}} + \frac{x}{y} = 1 \iff
    C x \sqrt{y} = x \iff
    y = \frac{1}{C^2}.
\]
As $C \geq 1$, in the parameter regime $y > 1$, $f$ has no stationary points.

It remains to check the boundary point $x = 1$ and the function $f$ in the limit as $x \to 0$.
For $x=1$, the claim is reduced to this simple inequality:
\begin{align*}
    \sqrt{y}\left(1 - \sqrt{1 - \frac{C}{\sqrt{y}} + \frac{1}{y}}\right) \geq \frac{C}{2} - \frac{1}{2\sqrt{y}}
    &\iff 2y\left(1 - \sqrt{1 - \frac{C}{\sqrt{y}} + \frac{1}{y}}\right) \geq C\sqrt{y} - 1 \\
    &\iff (2y - C\sqrt{y} + 1)^2 \geq  4y^2 \left(1 - \frac{C}{\sqrt{y}} + \frac{1}{y}\right) \\
    &\iff 4y^2 + C^2y + 1 - 4Cy\sqrt{y} + 4y - 2C\sqrt{y} \geq 4y^2 - 4Cy\sqrt{y} + 4y \\
    &\iff 1 + C^2y -2C\sqrt{y} \geq 0 \\
    &\iff (C\sqrt{y}-1)^2 \geq 0
\end{align*}
which holds for any value of $y$.
In the rest of the proof, we focus on the limit as $x \to 0$.

Via the Taylor expansion of \cref{prop:taylor-sqrt},
\begin{align*}
    \lim_{x \to 0} f(x;y) &= \lim_{x \to 0} \frac{\sqrt{y}}{x} \left(1 - \sum_{n=0}^\infty \frac{\prod_{k=1}^n \left(\frac{3}{2} - k\right)}{n!} \left(-\frac{Cx}{\sqrt{y}} + \frac{x}{y}\right)^n \right)\\
    &= \lim_{x \to 0} \frac{\sqrt{y}}{x} \sum_{n=1}^\infty - \frac{\prod_{k=1}^n \left(\frac{3}{2} - k\right)}{n!} \left(-\frac{Cx}{\sqrt{y}} + \frac{x}{y}\right)^n.
\end{align*}
Note that the coefficients in the summation are upper bounded in magnitude by $1$ as the sequence of terms in the descending factorial in the numerator is dominated by the sequence in the factorial in the denominator.
We also note that the absolute value of the coefficient of the first term is a constant, i.e. $\frac{1}{2}$.
So, in the limit, the summation is dominated by the lowest order terms with respect to $x$ which correspond to $n=1$. In this case, the coefficient is $-\frac{1}{2}$ and the limit evaluates to
\begin{equation*}
    \lim_{x \to 0} f(x;y) = \frac{\sqrt{y}}{{x}} \left(\frac{Cx}{2\sqrt{y}} - \frac{x}{2y}\right) = \frac{C}{2} - \frac{1}{2\sqrt{y}}.
\end{equation*}
\end{proof}

\begin{lemma}[$\ell_2$ sensitivity with biased weights]\label{lem:l2-sensitivity-biased-weights}
\cref{alg:max-deg} has $\ell_2$ sensitivity upper bounded by $1$.
\end{lemma}

\begin{proof}
Note that this is trivially true by $\cref{lem:l2-bound-user-weights}$ if $|S_v| = d_v > \dmax$ or $d_v < \ceil{\frac{1}{(\bmin)^2}}$ since $v$ does not participate in adaptivity in this case.
We will proceed by assuming this is not the case.

Our goal is to bound the $\ell_2$ norm of the difference $\Delta = w' - w$, the difference in final weights with and without the new user $v$. We will use the notation $\Delta_{subscript} = w'_{subscript} - w_{subscript}$.
Note that $\Delta_{reroute} = w'_{reroute} - w_{reroute}$ is the additional rerouted weight after adding user $v$ and let $\Delta_{user} = \Delta - \Delta_{reroute}$ be the rest of the difference.
Note that $\Delta_{user}$ is $d_v$-sparse and only has nonzero entries on $S_v$, the items of the new user.
Our goal will be to bound the $\ell_2$ norms of $\Delta_{reroute}$ and $\Delta_{user}$, thus bounding the $\ell_2$ sensitivity of the algorithm by triangle inequality.

We start by tracking the excess weight created by $v$ which will be useful in bounding the $\ell_2$ norms of both $\Delta_{reroute}$ and $\Delta_{user}$.
It is the total amount of weight added by $v$ to items that exceed the threshold\footnote{If if an item $i$ only exceeds the threshold due to the addition of $v$, we only consider the allocated weight to $i$ above the threshold. }:
\begin{equation}\label{eq:excess}
    \gamma = \|\Delta_{init} - \Delta_{trunc}\|_1
\end{equation}
(Note that this is not the same as $e'_v$ which is the amount of weight from $v$ gets returned to reroute.)

The total amount of weight that is returned to users to reroute is equal to the amount of weight truncated, i.e., the sum of $w_{init} - w_{trunc}$:
\begin{align*}
    \sum_{u=1}^n e_u &= \sum_{u=1}^n (1/|S_u|) \sum_{i \in S_u} r(i) \\
    &= \sum_{u=1}^n (1/|S_u|) \sum_{i \in S_u} \max\left\{0, \frac{w_{init}(i) - \tau}{w_{init}(i)}\right\} \\
    &= \sum_{u=1}^n (1/|S_u|) \sum_{i \in S_u} \frac{w_{init}(i) - w_{trunc}(i)}{w_{init}(i)} \\
    &= \sum_{u=1}^n (1/|S_u|) \sum_{i \in S_u} \frac{w_{init}(i) - w_{trunc}(i)}{\sum_{w : i \in S_w} 1/|S_w|} \\
    &= \sum_{i \in \U} \frac{(w_{init}(i) - w_{trunc}(i)) \sum_{u: i \in S_u} 1/|S_u|}{\sum_{w : i \in S_w} 1/|S_w|} \\
    &= \sum_{i \in \U} w_{init}(i) - w_{trunc}(i).
\end{align*}

For notational parsimony, let $e_v = 0$ (as $v$ does not appear in the original input $\sets$).
Note that $e_u$ is monotonically increasing with $w_{init}$: if any coordinate of the initial weight increases, the excess ratio of any user will never decrease.
With monotonicity of $w_{init}$ from \cref{lem:monotone}, it follows that $w'_{init} - w'_{trunc} \geq w_{init} - w_{trunc}$ since the threshold $\tau$ in the capping formula  $w_{trunc}(i) \gets \min\{w_{init}(i), \tau\}$ stays the same after adding the new user. Consequently, we have:
\begin{align*}
    \gamma
    &= \|\p{w'_{init} - w'_{trunc}} - \p{w_{init} - w_{trunc}}\|_1 \\
    &= \p{\sum_{i \in \U'} w'_{init} - w'_{trunc}} - \p{\sum_{i \in \U} w_{init} - w'_{trunc}} \\
    &= \sum_{u \in \{1, \ldots, n, v\}} e'_u - e_u.
\end{align*}

Now, we will consider $\Delta_{reroute}$.
Recall that $|S_u| \leq \dmax$ for all $u$ participating in adaptivity. For all other users, the terms $e_u$ and $e'_u$ are zero.
\begin{align*}
    \|\Delta_{reroute}\|_1 &= \sum_{u \in \{1, \ldots, n, v\}} \sum_{i \in S_u} (\alpha / \dmax) (e'_u - e_u) \\
    &= \sum_{u \in \{1, \ldots, n, v\}} |S_u| (\alpha / \dmax) (e'_u - e_u) \\
    &\leq \sum_{u \in \{1, \ldots, n, v\}} \alpha(e'_u - e_u) \\
    &= \alpha \gamma.
\end{align*}
Furthermore, we can bound the $\ell_\infty$ norm of $\Delta_{reroute}$ as:
\begin{equation*}
    w'_{reroute}(i) - w_{reroute}(i) \leq (\alpha / \dmax) \sum_{u \in \{1, \ldots, n, v\}} e'_u - e_u = \alpha \gamma / \dmax.
\end{equation*}
By H\"older's inequality,
\begin{equation}\label{eq:reroute-l2}
    \|\Delta_{reroute}\|_2 \leq \sqrt{\|\Delta_{reroute}\|_1 \|\Delta_{reroute}\|_\infty}
    = \sqrt{\alpha^2 \gamma^2 / \dmax} = \frac{\alpha \gamma}{\sqrt{\dmax}}.
\end{equation}



%\justin{WIP from here: unify with notation for biased weights and min biases above}
Consider the rest of the difference $\Delta_{user}$. For $i \in S_v$, a single coordinate of $\Delta_{user}$ will be comprised of the sum
\begin{equation*}
    \Delta_{user}(i) = \Delta_{trunc}(i) + w_b^v(i) - 1/d_v.
\end{equation*}
Note that $\Delta_{trunc}(i) \in [0,1/d_v]$, so
\begin{equation*}
    \Delta_{user}(i) \in \left[w_b^v(i) - 1/d_v, w_b^v(i)\right].
\end{equation*}
Furthermore, as $\Delta_{init}(i) = 1/d_v$,
\begin{equation*}
    \|\Delta_{user}\|_1 = 
    \sum_{i \in S_v} w_b^v(i) + \Delta_{trunc}(i) - \Delta_{init}(i)
    = \left(\sum_{i \in S_v} w_b^v(i) \right) - \gamma.
\end{equation*}

Let $x: S_v \rightarrow \mathbb{R}$ and $y: S_v \rightarrow \mathbb{R}$ be two sets of weights over $S_v$ such that $x(i) = w_b^v(i) - 1/d_v$, $y(i) \in [0, 1/d_v]$, and $\sum_{i \in S_v} x(i) + y(i) = \left( \sum_{i \in S_v} w_b^v(i) \right) - \gamma$.
Then,
\begin{equation*}
    \|\Delta_{user}\|_2 \leq \max_y \|x + y\|_2
\end{equation*}
as any valid $\Delta_{user}$ can be expressed as the sum of $x$ and a choice of $y$ satisfying the above constraints.
Note that
\begin{equation*}
    \|y\|_1 = \left(\sum_{i \in S_v} w_b^v(i) \right) - \gamma - \sum_{i \in S_v} x(i)
    = \left(\sum_{i \in S_v} w_b^v(i) \right) - \gamma - \sum_{i \in S_v} w_b^v(i) + 1
    = 1 - \gamma
\end{equation*}
and by H\"older's inequality, 
\begin{equation*}
    \|y\|_2 \leq \sqrt{\|y\|_1 \|y\|_\infty} = \sqrt{\frac{1 - \gamma}{d_v}}.
\end{equation*}
Then,
\begin{align*}
    \|x + y\|_2^2 &= \sum_{i \in S_v} \left(w_b^v(i) - 1/d_v + y(i)\right)^2 \\
    &= \sum_{i \in S_v} w_b^v(i)^2 + \frac{1}{d_v^2} + y(i)^2 - \frac{2 w_b^v(i)}{d_v} + 2 y(i) \cdot w_b^v(i) - \frac{2y(i)}{d_v} \\
    &= 1 + \frac{1}{d_v} + \|y\|_2^2 - \frac{2}{d_v} \cdot \|w_b^v\|_1  + 2 \langle y, w_b^v \rangle - \frac{2}{d_v} \cdot \|y\|_1  \\
    &\leq 1 + \frac{1}{d_v} + \frac{1-\gamma}{d_v} - \frac{2}{d_v} \cdot \|w_b^v\|_1  + 2 \langle y, w_b^v \rangle - \frac{2(1-\gamma)}{d_v}  \\
    &= 1 + \frac{1 + 1-\gamma - 2(1-\gamma)}{d_v}  - \frac{2}{d_v} \cdot \|w_b^v\|_1  + 2 \langle y, w_b^v \rangle   \\
    &= 1  + \frac{\gamma}{d_v} - \frac{2}{d_v} \cdot \|w_b^v\|_1  + 2 \langle y, w_b^v \rangle
\end{align*}
Every $y(i)$ can be written as $1/d_v - z(i)$ for some non-negative residual $z(i)$ with $\sum_{i \in S_v} z(i) = \gamma$. So we continue the above equations as follows: 
\begin{align*}
    \|x + y\|_2^2 
    &\leq  1  + \frac{\gamma}{d_v} - \frac{2}{d_v} \cdot \|w_b^v\|_1  + 2 \langle y, w_b^v \rangle\\
    &=  1  + \frac{\gamma}{d_v} - \frac{2}{d_v} \cdot \|w_b^v\|_1  + \frac{2}{d_v}  \cdot \|w_b^v\|_1 -  2\sum_{i \in S_v} z(i) w_b^v(i) \\
    &=  1  + \frac{\gamma}{d_v}  -  2\sum_{i \in S_v} z(i) w_b^v(i) \\
    &\leq  1  + \frac{\gamma}{d_v}  -  2 \cdot \frac{\bmin}{\sqrt{d_v}} \cdot \sum_{i \in S_v} z(i)  \\
    &= 1 - \frac{2 \bmin \gamma}{\sqrt{d_v}} + \frac{\gamma}{d_v}.
\end{align*}
The last inequality holds because   \cref{lem:l2-bound-user-weights} implies that 
$w_b^v(i) \geq \frac{\bmin}{\sqrt{d_v}}$. We conclude that: 
\[
\|\Delta_{user}\|_2 \leq \sqrt{1 - \frac{2 \bmin \gamma}{\sqrt{d_v}} + \frac{\gamma}{d_v}}.
\]
We can now bound the $\ell_2$ sensitivity of the entire algorithm as
\begin{equation*}
    \|\Delta\|_2 \leq \|\Delta_{reroute}\|_2 + \|\Delta_{user}\|_2 \leq 
    \frac{\alpha \gamma}{\sqrt{\dmax}} + \sqrt{1 - \frac{2 \bmin \cdot \gamma}{\sqrt{d_v}} + \frac{\gamma}{d_v}}.
\end{equation*}
As the expression $-\frac{2 \bmin}{\sqrt{d_v}} + \frac{1}{d_v}$ is increasing for $d_v \geq \frac{1}{\bmin^2}$, the right hand side is maximized with $d_v = \dmax$:
\begin{equation*}
    \|\Delta\|_2 \leq \frac{\alpha \gamma}{\sqrt{\dmax}} + \sqrt{1 - \frac{2 \bmin \cdot \gamma}{\sqrt{\dmax}} + \frac{\gamma}{\dmax}}.
\end{equation*}
Our goal is to choose $\alpha \in [0,1]$ such that the right hand side is upper bounded by $1$.
We note that for $\gamma = 0$, the above inequality proves this desired upper bound.
For other cases, $\gamma \in (0,1]$, it is achieved when,
\begin{equation}
    \alpha \leq \frac{\sqrt{\dmax}}{\gamma} \left(1 - \sqrt{1 - \frac{2 \bmin \cdot \gamma}{\sqrt{\dmax}} + \frac{\gamma}{\dmax}} \right).
\end{equation}
By \cref{lem:gen-limit} with $C=2 \bmin$ and using the restrictions $\dmax > 1$ and $\frac{1}{2} \leq \bmin \leq 1$, it suffices to choose
\begin{equation}\label{eq:alpha}
    \alpha = \bmin - \frac{1}{2\sqrt{\dmax}}.
\end{equation}
\end{proof}

After having bounded the novel $\ell_\infty$ and $\ell_2$ sensitivities of \ouralgo{}, we are now ready to conclude the privacy analysis.
\begin{proof}[Proof of \cref{thm:dp-two-rounds}]
Note that both rounds of \ouralgotworounds{} (\cref{alg:max-deg-two-round}) correspond to running the \selectalgo{} meta-algorithm (\cref{alg:generic}) with privacy parameters $(\eps_1, \delta_1)$ and $(\eps_2, \delta_2)$, respectively.
The only difference is that we only materialize $\tilde{w}_1$ rather than the entire vector $\tilde{w}_{ext}$ from the first round. The functionality of our algorithm would be equivalent if we instead materialized the full vector as we only query weights on items in $\U$ and we never output the vector.
Therefore, we will invoke \cref{thm:generic-privacy} twice and apply basic composition to prove the privacy of \ouralgotworounds{}.
By \cref{thm:generic-privacy}, it suffices to show that the $\ell_2$ and novel $\ell_\infty$ sensitivities of the weight algorithm \ouralgo{} are bounded by $1$ and $\frac{\bmax}{\sqrt{t}}$, respectively.
This follows directly from \cref{lem:linf-sensitivity-biased}, and \cref{lem:l2-sensitivity-biased-weights}.
\end{proof}


