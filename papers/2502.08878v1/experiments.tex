\begin{table}[ht]
\begin{centering}
\footnotesize
\begin{tabular}{| l | r | r | r | }
\hline
Dataset     & Users  & Items  & Entries  \\
\hline
Higgs       & \num{2.8e5}   & \num{5.9e4}     & \num{4.6e5} \\
IMDb        & \num{5.0e4}    & \num{2.0e5}    & \num{7.6e6} \\
Reddit      & \num{2.2e5}   & \num{1.5e5}    & \num{7.9e6} \\
Finance     & \num{1.4e6}  & \num{2.7e5}    & \num{1.7e7} \\
Wiki        & \num{2.5e5}   & \num{6.3e5}    & \num{1.8e7} \\
Twitter     & \num{7.0e5}   & \num{1.3e6}   & \num{2.7e7} \\
Amazon      & \num{4.0e6}  & \num{2.5e6}   & \num{2.4e8} \\
Clueweb     & \num{9.6e8} & \num{9.4e8} & \num{4.3e10} \\
Common Crawl & \num{2.9e9} & \num{1.8e9} & \num{7.8e11} \\
\hline
\end{tabular}
\caption{Number of distinct users, distinct items, and total entries (user, item pairs). The number of entries is the sum of the sizes of all the users' sets.}
\label{table:datasets}
\end{centering}
\end{table}


\sisetup{
text-series-to-math = true ,
propagate-math-font = true
}

\begin{table}[ht]
\begin{centering}
\footnotesize
\begin{tabular}{| l | l  l  l  l | l  l |}
\hline
\multirow{2}{*}{Dataset} & \multicolumn{4}{c |}{\emph{Parallel Algorithms}} & \multicolumn{2}{c |}{\emph{Sequential Algorithms}} \\
 & \ouralgo{} (ours) & \ouralgotworounds{} (ours) & \basicalgo{} & DP-SIPS & PolicyGaussian & GreedyUpdate \\
\hline
Higgs & $\textbf{\num{1807}} \; \err{13}$ & $\num{1767} \; \err{15}$ & $\num{1791} \; \err{18}$ & $\num{1743} \; \err{8}$ & $\num{1923} \; \err{18}$ & $\underline{\num{2809}} \; \err{11}$ \\
IMDb & $\num{2516} \; \err{12}$ & $\textbf{\num{3369}} \; \err{19}$ & $\num{2504} \; \err{7}$ & $\num{3076} \; \err{16}$ & $\underline{\num{3578}} \; \err{19}$ & $\num{1363} \; \err{11}$ \\
Reddit & $\num{4162} \; \err{19}$ & $\textbf{\num{6215}} \; \err{18}$ & $\num{4062} \; \err{21}$ & $\num{5784} \; \err{30}$ & $\underline{\num{7170}} \; \err{39}$ & $\num{6340} \; \err{16}$ \\
Finance & $\num{12759} \; \err{16}$ & $\textbf{\num{17785}} \; \err{28}$ & $\num{12412} \; \err{50}$ & $\num{16926} \; \err{18}$ & $\num{20100} \; \err{49}$ & $\underline{\num{23556}} \; \err{27}$ \\
Wiki & $\num{7812} \; \err{12}$ & $\textbf{\num{10554}} \; \err{41}$ & $\num{7753} \; \err{36}$ & $\num{9795} \; \err{21}$ & $\underline{\num{11455}} \; \err{21}$ & $\num{4739} \; \err{14}$ \\
Twitter & $\num{9074} \; \err{23}$ & $\textbf{\num{14064}} \; \err{13}$ & $\num{8859} \; \err{22}$ & $\num{13499} \; \err{50}$ & $\num{15907} \; \err{30}$ & $\underline{\num{15985}} \; \err{29}$ \\
Amazon & $\num{35797} \; \err{63}$ & $\textbf{\num{67086}} \; \err{59}$ & $\num{35315} \; \err{69}$ & $\num{66126} \; \err{57}$ & $\num{77846} \; \err{127}$ & $\underline{\num{86841}} \; \err{95}$ \\
Clueweb & $\num{34692178}$ & $\textbf{\num{36073952}}$ & $\num{34603077}$ & $\num{34889208}$ & -- & -- \\
Common Crawl & $\num{15815452}$ & $\textbf{\num{29373829}}$ & $\num{15734148}$ & $\num{28328613}$ & -- & -- \\
\hline
\end{tabular}
\caption{Comparison of output size of DP partition selection algorithms with $\eps = 1$, $\delta=10^{-5}$, and $\Delta_0 = 100$. A standard hyperparameter setting is fixed for each algorithm, other than DP-SIPS, where the best result is taken from privacy splits $[0.1, 0.9]$ and $[0.05, 0.15, 0.8]$. For smaller datasets, sequential algorithms are also reported as oracles and results are averaged over $5$ trials with one standard deviation reported parenthetically. For each dataset, the best parallel result is bolded and the best sequential result is underlined.}
\label{table:results}
\end{centering}
\end{table}
    
We now compare the empirical performance of \ouralgo{} and \ouralgotworounds{} against two parallel (\basicalgo{}, DP-SIPS) and two sequential algorithms (PolicyGaussian and GreedyUpdate) for the partition selection. We observe that our algorithms output most items (at parity of privacy parameter) among the parallel algorithms for every dataset and across various parameter regimes. Moreover, parallelization allows us to analyze datasets with up to $800$ billion entries, orders of magnitude larger than sequential algorithms. In the rest of the section we describe the datasets used, the algorithms compared and the computational  architecture used, before presenting our empirical results.


\subsection{Datasets}
We consider 9 datasets with statistics detailed in \cref{table:datasets}.
First, we consider small-scale datasets that are suitable for fast processing by sequential algorithms in a single-core architecture. These includes, for the sake of replicability, datasets used in prior works~\cite{gopi2020dpunion, carvalho2022incorporatingitem, swanberg2023dpsips}: Higgs, IMDb, Reddit, Finance, Wiki, Twitter, Amazon. These datasets have up to $3$ million distinct items and $300$ million entries.

We also consider two very-large publicly-available datasets Clueweb~\cite{BRSLLP} and Common Crawl\footnote{\url{https://www.commoncrawl.org/}}. The latter has approximately $2$ billion distinct items and $800$ billion entries. This is $3$ orders of magnitude larger than the largest dataset used in prior work.

Higgs~\cite{snapnets} is a dataset of Tweets during the discovery of the Higgs.
IMDb~\cite{imdb} is a dataset of movie reviews, Reddit~\cite{gopi2020dpunion} is a dataset of posts to \texttt{r/askreddit}, Finance~\cite{finance} is dataset of financial headlines, Wiki~\cite{wiki} is a dataset of Wikipedia abstracts, Twitter~\cite{twitter} is a dataset of customer support tweets, and Amazon~\cite{amazon1, amazon2} is a dataset of product reviews.  Common Crawl is a very-large dataset of crawled web pages used in LLM research.
For each of these text-based datasets we replicate prior methodology~\cite{gopi2020dpunion, carvalho2022incorporatingitem} where items represent the words used in a document (obtained by tokenizer) and each document corresponds to a user (in some datasets, actual users are tracked across documents, in which case, we use combine the users' documents into one document).
Finally, for Clueweb~\cite{BRSLLP} which is a dataset of web pages and their hyper-links, items corresponds to the hyperlinks on a web page and each page corresponds to a user. 



\subsection{Algorithms and Parameters}

We compare our results to both sequential and parallel algorithms from prior work. The sequential algorithms we compare against are PolicyGaussian~\cite{gopi2020dpunion} and GreedyUpdate~\cite{carvalho2022incorporatingitem}. Like our algorithm, both algorithms set an adaptive threshold $\tau$ greater than the true threshold $\rho$. They try to maximize weight assigned to items up to but not exceeding $\tau$. PolicyGaussian goes through each user set one by one and adds $\ell_2$ bounded weight to minimize the $\ell_2$ distance between the current weight and the all $\tau$ vector, $w(i) = \tau \; \forall i \in \U$. GreedyUpdate goes through each user set one by one and increments the weight of a single item in the set by one, choosing an item whose weight is currently below $\tau$\footnote{Unlike all of the other algorithm, this algorithm does not do a first step of bounding users' degrees by $\Delta_0$ as it only assigns weight to a single item per user by design.}.
As observed before~\cite{swanberg2023dpsips}, sequential algorithms can have arbitrary long adpativity chains (the processing of each user can depend on all prior users processed) thus allowing larger output sizes than parallel algorithms. This, however, comes at the cost of not being parallelizable (as we observe in our experiments on the larger datasets).     
The parallel baselines we compare against are \basicalgo{}~\cite{korolova2009releasing, gopi2020dpunion} and DP-SIPS~\cite{swanberg2023dpsips}. In DP-SIPS, the privacy budget is split into a distribution over rounds. In each round, the basic algorithm is run with the corresponding privacy budget. Items found in previous rounds are removed from all user's sets for the next rounds.

We make parameter choices which are consistent with prior work and generally work well across datasets.
We include an evaluation of performance as several parameters vary in Figures~\ref{fig:eps},~\ref{fig:delta},~\ref{fig:cap}, and~\ref{fig:dmax}.
We use $\eps = 1$ and $\delta=10^{-5}$ for the privacy parameters\footnote{We report these parameter settings for consistency with prior work in the literature, but observe the results are consistent across various choices. For real production deployments on large-scale sensitive data, $\delta$ is usually smaller.} and $\Delta_0 = 100$.
For PolicyGaussian and GreedyUpdate, we set the $\beta=4$ to be the number of standard deviations of noise to add to the base threshold to set the adaptive threshold.
For DP-SIPS, we take the best result of running with a privacy split of $[0.1, 0.9]$ and $[0.05, 0.15, 0.8]$\footnote{As this choice can have a significant effect on performance, we choose the best-performing to give this baseline the benefit of the doubt.}.
For \ouralgo{} and \ouralgotworounds{}, we set $\dmax=50$ and $\beta=2$.
For \ouralgotworounds{}, we set the privacy split of $[0.1,0.9]$, $\bmin=0.5$, $\bmax=2$, $C_{lb}=1$, and $C_{ub}=3$.

\subsection{Experimental Architecture}
We perform experiments in two different computational settings. First we implement a sequential, in-memory version of all algorithms (including the parallel ones) using Python.
For PolicyGaussian and GreedyUpdate we use the Python implementations from prior work~\cite{gopi2020dpunion, carvalho2022incorporatingitem}.
This allows us to fairly test the scalability of the algorithms not using parallelism. As we observe next, this approach does not scale to the two largest datasets we have (Clueweb, Common Crawl). 

Then, we implement all parallel algorithms (\ouralgo{}, \ouralgotworounds{}, \basicalgo{}, DP-SIPS) using C++ in a modern multi-machine massively parallel computation framework in our institution. This framework allows to use a fleet of shared (x86\_64) architecture machines with 2.45GHz clocks. The machines are shared by several projects and can have up to 256 cores and up to 512GB of RAM. The jobs are dynamically allocated RAM, machines and cores depending on need and availability. 
As we observe, all parallel algorithm are very scalable and run on these huge datasets within 4 hours of wall-clock time. On the other hand, both sequential algorithms cannot exploit this architecture and could not complete in 16 hours on the Clueweb dataset (we estimate they would take several days to complete on the Common Crawl dataset even assuming access to enough memory).

\subsection{Results}
\cref{table:results} displays the output size of the DP partition selection algorithms (i.e., the number of privatized items output).
Among parallel algorithms, either \ouralgo{} or \ouralgotworounds{} achieves the best performance with \ouralgotworounds{} achieving the best result on all but one dataset\footnote{The Higgs dataset is the smallest dataset and the average size of a user set is less than $2$. It is an outlier among the datasets.}.
Directly comparing \ouralgo{} with \basicalgo{}, \ouralgo{} is always better, corroborating our theoretical result on stochastic dominance.
Comparing \ouralgotworounds{} with DP-SIPS, \ouralgotworounds{} is always significantly better, by up to a factor of a $9.5\%$ improvement on the IMDb dataset.

On the small scale datasets where we can run sequential algorithms, as expected from prior work~\cite{swanberg2023dpsips}, one of the two sequential algorithms yield the best results across all algorithms with PolicyGaussian consistently outperforming all parallel baselines (GreedyUpdate's performance is heavily dataset dependent, sometimes performing the best and sometimes the worst out of all algorithms).  This is expected as the sequential algorithms utilize much more adaptivity than even our adaptive parallel algorithm at the cost of limiting scalability. Our algorithm is still competitive, never outputting fewer than $86\%$ of the items of PolicyGaussian (and outperforming GreedyUpdate on many datasets). 
For massive datasets, where it is simply infeasible to run the sequential algorithms, however \ouralgotworounds{} has the best results of all parallel algorithms.

Additional figures comparing output sizes with varying privacy parameters $\eps$, $\delta$, maximum cardinality $\Delta_0$, and our adaptive degree parameter $\dmax$ are included in Figures~\ref{fig:eps},~\ref{fig:delta},~\ref{fig:cap}, and~\ref{fig:dmax}. The relative performance of the algorithms is the same across parameter choices.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\textwidth]{images/epsilon.pdf}
    \caption{Comparison of output size across parallel algorithms while varying privacy parameter $\eps$ on the Reddit dataset. Other parameters are fixed as described above with a fixed privacy split of $[0.1,0.9]$ for DP-SIPS and \ouralgotworounds{}. The relative performance of algorithms does not change with this parameter. Increasing $\eps$ significantly improves performance at the cost of privacy by lowering the required noise and threshold.}
    \label{fig:eps}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\textwidth]{images/delta.pdf}
    \caption{Comparison of output size across parallel algorithms while varying privacy parameter $\delta$ on a log-scale on the Reddit dataset. Other parameters are fixed as described above with a fixed privacy split of $[0.1,0.9]$ for DP-SIPS and \ouralgotworounds{}. The relative performance of algorithms does not change with this parameter. Increasing $\delta$ significantly improves performance at the cost of privacy by lowering the required noise and threshold.}
    \label{fig:delta}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\textwidth]{images/max_items.pdf}
    \caption{Comparison of output size across parallel algorithms while varying maximum set size parameter $\Delta_0$ on the Reddit dataset. Other parameters are fixed as described above with a fixed privacy split of $[0.1,0.9]$ for DP-SIPS and \ouralgotworounds{}. The relative performance of algorithms does not change with this parameter, and good results are achieved as long as it is not too small.}
    \label{fig:cap}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{images/max_adaptive_degree.pdf}
    \caption{Comparison of output size across parallel algorithms while varying the parameter $\dmax$ of our algorithms on the Reddit dataset. As this parameter is only used by \ouralgo{} and \ouralgotworounds{}, the performance of the baselines is fixed. Other parameters are fixed as described above with a fixed privacy split of $[0.1,0.9]$ for DP-SIPS and \ouralgotworounds{}. The performance of our algorithm is relatively insensitive to this parameter.}
    \label{fig:dmax}
\end{figure}