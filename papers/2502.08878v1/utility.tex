\subsection{Stochastic Dominance}

In \cref{thm:maxalgo-perf-informal}, 
we show that for any input, \ouralgo{}'s performance stochastically dominates \basicalgo{}: the probability that an item is output by \basicalgo{} is upper bounded by the probability it is output by \ouralgo{}.

\begin{theorem}\label{thm:maxalgo-perf}
Let $\beta \geq 0$ be the parameter controlling the adaptive threshold excess.
Let $U$ be the set of items output when using \basicalgo{} as the weighting algorithm and let $U^*$ be the set of items output when using unbiased \ouralgo{} as the weighting algorithm.
Then, for items $i \in \U$,
\begin{itemize}
    \item If $\Pr\p{i \in U} < \Phi(\beta)$, then $\Pr\p{i \in U^*} \geq \Pr\p{i \in U}$.
    \item Otherwise, $\Pr\p{i \in U^*} \geq \Phi(\beta)$.
\end{itemize}
\end{theorem}

\begin{proof}[Proof of \cref{thm:maxalgo-perf}]
Let $w$ and $w^*$ be the weights produced by \basicalgo{} and \ouralgo{}, respectively.
Let $I_{adapt} = \{u \in [n] : |S_u| \leq \dmax\}$ be the items which participate in adaptive rerouting.
For any item $i \in \U$, we will consider it's initial weight under the adaptive algorithm:
\[
    w^*_{init}(i) = \sum_{u \in I_{adapt}: i \in S_u} \frac{1}{|S_u|}.
\]
We will proceed by cases.
\paragraph{Case 1:} $w^*_{init}(i) \leq \tau$. In this case, $w^*(i) \geq w(i)$. In the adaptive algorithm, no weight is truncated from the initial weights, and so each user contributes to the final weight of an item $\frac{1}{\sqrt{|S_u|}}$ plus rerouted weight from other items. As the weight on item $i$ only increases for the adaptive algorithm compared to the basic algorithm, the probability of outputting $i$ also can only increase.

\paragraph{Case 2:} $w^*_{init}(i) > \tau$. In this case in the adaptive algorithm, the initial weight is truncated to $\tau$, excess weight is rerouted, and a final addition of $\frac{1}{\sqrt{|S_u|}} - \frac{1}{|S_u|}$ is added. As $|S_u| \geq 1$, the final weight $w^*(i) \geq \tau$.
Then, $i \in U^*$ if the added Gaussian noise does not drops the weight below the threshold $\rho$, i.e., if the noise is greater than or equal to 
\[
    \rho - \tau = \rho - (\rho + \beta \sigma) = -\beta \sigma.
\]
As the noise has zero mean and standard deviation $\sigma$, this probability is exactly $1 - \Phi(-\beta) = \Phi(\beta)$.
\end{proof}

\subsection{Example showing a gap between \ouralgo{} and parallel baselines}
While \cref{thm:maxalgo-perf} bounds the worst-case behavior of our algorithm compared to the basic algorithm, as \ouralgo{} increases the weight of items below $\tau$ compared to the basic algorithm, it will often have a larger output. We show a simple, explicit example where our algorithm will substantially increase the output probability of all but one item.

Here, we demonstrate an explicit setting where \ouralgo{} outperforms the parallel baselines: \basicalgo{} \cite{korolova2009releasing, gopi2020dpunion} and DP-SIPS \cite{swanberg2023dpsips}.
There are $n$ users each with degree $3$ as well as a single heavy item $i^*$ and $m$ light items. Each user's set is comprised of $i^*$ as well as two random light items. Under the basic algorithm, each user will contribute $\nicefrac{1}{\sqrt{3}}$ to each of their items. Therefore, the weights under \basicalgo{} are
\begin{equation*}
    w(i) = 
    \begin{cases}
        \frac{n}{\sqrt{3}} & \text{if } i = i^* \\
        \frac{2n}{\sqrt{3}m} < \frac{1.16n}{m} & o.w.
    \end{cases}.
\end{equation*}

On the other hand, assuming $n >> \tau$, \ouralgo{} will reroute almost all of the initial weight on the heavy item back to the users, so each user will have excess weight approximately $\nicefrac{1}{3}$. For $\dmax=3$, we get discount factor $\alpha > 0.5$. So, each user will send approximately $\nicefrac{1}{18}$ weight to each of its items. The weights under \ouralgo{} are
\begin{equation*}
    w(i) = 
    \begin{cases}
        \tau + \frac{n}{18} & \text{if } i = i^* \\
        \frac{2n}{m}\left(\frac{1}{\sqrt{3}} + \frac{1}{18}\right) < \frac{1.27n}{m} & o.w.
    \end{cases}.
\end{equation*}

In this setting, our algorithm will assign close to $10\%$ more weight to the light items  (resulting in substantially higher probability of output) compared to the basic algorithm.
If $\nicefrac{n}{m}$ is close to the true threshold $\rho$, this gap will have a large effect on the final output size. We empirically validate this for $n=\num{15000}, m=1000, \eps=1, \delta=10^{-5}$. Our algorithm returns $610$ items on average. The basic algorithm returns $519$ items while DP-SIPS with a privacy split of $5\%, 15\%, 80\%$ or $10\%, 90\%$ returns $332$ or $514$ items, respectively. In all cases, as expected, our algorithm has significantly higher average output size. 