\subsection{Weight and Threshold Meta-Algorithm}

% \ale{I think it is good to start with a summary of what we want to do in this section. Are we first going to present a standard solution in the literature? If so we should say that we first present teh algorithm of XYZ ... (or a meta algorithm describing the algorithms ...)}. 


\ale{where are these algorithms? We need to move them here} 
Solutions to the private set union problem follow a weight and threshold meta-algorithm. Input is given as a set of user sets $\sets=\{(u, S_u)\}_{u \in [n]}$, privacy parameters $\eps$ and $\delta$, a maximum degree cap $\Delta_0$, and a weighting algorithm \weightalgo.
First, each user's set is randomly subsampled so that the size of each resulting set is at most $\Delta_0$ (the necessity of this step will be further explicated).
Then, the \weightalgo{} takes in the cardinality-capped sets and produces a set of weights over all items in the union.
Independent Gaussian noise with standard deviation $\sigma$ is added to each coordinate of the weights, and items with weight above a certain threshold $\rho$ are output.

The privacy of this algorithm depends on certain ``sensitivity'' properties of the \weightalgo{} as well as our choice of $\sigma$ and $\rho$. Consider any pair of neighboring inputs $\sets$ and $\sets' = \sets \cup \{(v, S_v)\}$, let $\U$ and $\U'$ be the corresponding unions, and let $w$ and $w'$ be the item weights assigned by \weightalgo{} on the two inputs, respectively. The $\ell_2$ sensitivity of \weightalgo{} is defined as an upper bound on the Euclidean distance between $w$ and $w'$: $\sqrt{\sum_{i \in \U} \left(w(i) - w'(i)\right)^2}$.
Given bounded $\ell_2$ sensitivity, choosing the scale of noise $\sigma$ appropriately for the Gaussian mechanism in~\cref{prop:gaussian} ensures that outputting the noised weights on items in $\U$ satisfies $\left(\eps, \frac{\delta}{2} \right)$-DP.
So if we knew $\U$, then the output of the algorithm after thresholding would be private via post-processing.

However, knowledge of the union $\U$ is exactly the problem we want to solve. The challenge is that there may be items in $\U'$ which do not appear in $\U$. Let $T = \U' \setminus \U$ be these ``novel'' items with $t = |T|$. As long as the probability that any of these items are output by the algorithm is at most $\frac{\delta}{2}$, $(\eps, \delta)$-DP will be maintained. Consider a single item $i \in T$ which has zero probability of being output by a weight and threshold algorithm run on $\sets$ but is given some weight $w'(i)$ when the \weightalgo{} is run on $\sets'$. The item will be output only if after adding the Gaussian noise with standard deviation $\sigma$, the noised weight exceeds $\rho$. The probability that any item in $T$ is output follows from a union bound. In order to union bound only over finitely many events, we rely on the fact that $t \leq \Delta_0$; this is why the cardinalities must be capped. The second important sensitivity measure of a \weightalgo{} is the novel $\ell_\infty$ sensitivity, which is an upper bound, parameterized by $t$, on the value of $w'(i)$ for $i \in T$. Then, the calculation of $\rho$ to obtain $(\eps, \delta)$-DP is obtained based on the novel $\ell_\infty$ sensitivity, $\delta$, $\sigma$, and $\Delta_0$.

It is straightforward to show that for the uniform weighting algorithm \basicalgo{} in which each user $u$ contributes $1/\sqrt{|S_u|}$ weight to each of the items in $S_u$ has $\ell_2$ sensitivity equal to $1$ and novel $\ell_\infty$ sensitivity equal to $1/\sqrt{t}$.

\subsection{Max Degree Adaptive Weighting}

Our main result is an adaptive weighting algorithm \ouralgo{} which is amenable to parallel implementations and has the exact same $\ell_2$ and novel $\ell_\infty$ sensitivities as \basicalgo{}. Therefore, within the weight and threshold meta-algorithm, both algorithms utilize the same noise $\sigma$ and threshold $\rho$ to maintain privacy.

Our algorithm takes two additional parameters: a maximum adaptive degree $\dmax \in [1, \Delta_0]$ and an adaptive threshold $\tau = \rho + \beta \sigma$ for a free parameter $\beta \geq 0$. Users with set cardinalities greater than $\dmax$ are set aside and contribute basic uniform weights to their items at the end of the algorithm. The rest of the users participate in adaptive reweighting. We start from a uniform weighting where each user sends $1/|S_u|$ weight to each of their items. Items have their weights truncated to $\tau$ and any excess weight is sent back to the users proportional to the amount they contributed. Users then reroute a carefully chosen fraction (depending on $\dmax$) of this excess weight across their items. Finally, each user adds $1/\sqrt{|S_u|} - 1/|S_u|$ to the weight of each of their items. Each of these steps are straightforward to implement within a parallel framework.

Bounding the sensitivity of this weighting algorithm is significantly more involved than for basic weighting. Algorithm design choices such as using an initial uniform weighting inversely proportional to cardinality rather than square root of cardinality, using a maximum adaptive degree, and the fraction of how much excess weight to reroute are all required for the following theorem.

\begin{theorem}[Informal]
\ouralgo{} has $\ell_2$ sensitivity equal to 1 and novel $\ell_\infty$ sensitivity equal to $1/\sqrt{t}$. Thus, using this algorithm within the weight and threshold meta-algorithm maintains $(\eps, \delta)$-DP.
\end{theorem}

As a direct result of the design of \ouralgo{}, for items with weight less than $\tau$, our algorithm dominates the basic weighting algorithm in the sense that the weights on those items will only increase under our algorithm. While weights on overallocated items may decrease due to truncation, this only marginally affects the probability that these items are output as long as $\beta$ is large enough, formalized in the following theorem statement.

\begin{theorem}[Informal]
Let $U$ be the set of items output when using \basicalgo{} as the weighting algorithm in the weight and threshold meta-algorithm and let $U^*$ be the set of items output when using \ouralgo{} as the weighting algorithm.
Then, for items $i \in \U$, $\Pr\left(i \in U^*\right) \geq \min\{\Pr\left(i \in U\right), \Phi(\beta)\}$.
\end{theorem}