

The availability of large amounts of user data has been one of the driving factors for the widespread adoption and rapid development of modern machine learning and data analytics. 
Consider the example of a system releasing information on the queries asked to a search engine over a period of time~\cite{korolova2009releasing, bavadekar2021google}. Such a system can provide valuable insights to researchers and the public (for instance on health concerns~\cite{bavadekar2021google}) but care is needed in ensuring that the queries output do not leak private and sensitive user information.

In this paper, we focus on the problem of {\it private partition selection}~\cite{desfontaines2022dppartition,gopi2020dpunion} which models the challenge of  extracting as much data as possible from such a dataset while respecting user privacy. 
More formally, the setting of the problem  (which is also known as private set union or private key discovery) is that each user has a private subset of items (e.g., the queries issued by the user) from an unknown and unbounded universe of items (e.g., all strings). The goal is to output as many of the items in the users' sets as possible (e.g., the queries issues by the users), while  providing a strong notion of privacy---User-level Differential Privacy (DP)~\cite{dwork2014book}.


Private partition selection models many challenges beyond the example above, including extracting the vocabulary (words, tokens or $n$-grams) present in a private corpus~\cite{zhang2022federated,kim2021differentially}. This task is a fundamental prerequisite for many privacy-preserving natural language processing algorithms~\cite{wilson2019differentially,gopi2020dpunion}, such as in training language models for sentence completion and response generation for emails~\cite{kim2021differentially}. Similarly,  learning embedding models over categorical data often requires identification of the categories present in a private dataset~\cite{ghazi2023sparsitypreserving, ghazi2024dpsparsegrad}. Partition selection underpins other applications including analyzing private streams~\cite{cardoso2022differentially,zhang2023differentially}, learning sparse histograms~\cite{boneh2021lightweight}, answering SQL queries~\cite{desfontaines2022dppartition}, and sparsifying the gradients in the DP SGD method~\cite{ghazi2023sparsitypreserving}.
Unsurprisingly given their numerous applications, private partition selection algorithms \cite{korolova2009releasing} are a core building block of many standard differentially private libraries such as PyDP~\cite{pydp_partition}, Google's DP Libraries~\cite{googledplibrary,amin2022plume}, and OpenMined DP Library~\cite{openmined}. 

Real-world datasets for these applications can be massive, potentially containing hundreds of billions of data points, thus requiring algorithms for partition selection that can be efficiently run in large-scale data processing infrastructures such as MapReduce~\cite{dean2004mapreduce}, Hadoop~\cite{hadoop}, or Spark~\cite{zaharia2016spark}. In our work, we design a highly parallelizable algorithm for this problem which requires constant parallel rounds in the Massively Parallel Computing model~\cite{karloff2010model} and does not assume the input fits in memory.  This contrasts to prior algorithms such as \cite{gopi2020dpunion, carvalho2022incorporatingitem} which require (with the key exception of the uniform weighting method described below) processing all the data sequentially on a single machine and storing the input in-memory, precluding efficient parallelization.

\subsection{Weight and Threshold Approach}
Before introducing our algorithm, we review the popular weighting-based approach to partition selection which is used in many algorithms~\cite{korolova2009releasing, gopi2020dpunion, carvalho2022incorporatingitem, swanberg2023dpsips}. This approach is of interest in the context of large-scale data as some of its variants can be parallelized efficiently~\cite{korolova2009releasing,swanberg2023dpsips}. 

Notice that differential privacy prohibits outputting any item which is owned by only a single user. However, it is possible for a private algorithm to output items which appear in {\it many} different sets. This intuition is at the basis of the weighting-based algorithms.

Algorithms in this framework start by subsampling each user's set to bound the maximum number of items per user. Then, for each user, the weights associated with the user's items are increased via a weighting function. Finally, the algorithm adds Gaussian (or Laplace) noise to the total accumulated weight of each item, and outputs all items with noised weight above a certain threshold~\cite{korolova2009releasing, gopi2020dpunion, carvalho2022incorporatingitem, swanberg2023dpsips}. The amount of noise and value of the threshold depends on the privacy parameters and  \emph{crucially} on the sensitivity of the weighting function to the addition or removal of any individual user's set. Loosely speaking, the contribution of each user to the item weights must be bounded in order to achieve differential privacy. Algorithms within this framework differ in the choice of how to assign item weights, but in all designs the key goal is that of limiting the sensitivity of the weighting function.

A basic strategy is {\it uniform weighting}~\cite{korolova2009releasing} where each user  contributes equal weight to each of the items in their set. It is easy to bound the sensitivity of this basic weighting and thus to prove differential privacy. Because of its simplicity, the basic uniform weighting algorithm is extremely parallelizable requiring only basic counting operations over the items in the data.  

Unfortunately, however, uniform weighting is lossy in that it may overallocate weight far above the threshold to high frequency items, missing an opportunity to boost the weight of items closer to the decision boundary. This has inspired the design of greedy weighting schemes~\cite{gopi2020dpunion, carvalho2022incorporatingitem} where each user's allocations depend on data of previously analyzed users. All of these algorithms are inherently sequential and require memory proportional to the items present in the data. 

To our knowledge, uniform weighting~\cite{korolova2009releasing} is essentially the only known solution to the private partition selection problem which is amenable to implementation in a massively parallel computation framework. The sole exception is the scalable, iterative partition selection (DP-SIPS) scheme of~\cite{swanberg2023dpsips} which uses, as its core computation, repeated invocations of the uniform weighting algorithm.

\subsection{Our Contributions}

In this work, we design the first, adaptive, non-uniform weighting algorithm that is amenable to massively parellel implementations. Our algorithm, called \ouralgolong{} (\ouralgo{}), requires linear work in the size of the input and can be implemented in a constant number of rounds in a parallel framework.
The algorithm is based on a careful rerouting of overallocated weight to less frequent items. With a delicate sensitivity analysis, we show that this algorithm has no privacy loss compared to uniform weighting. This means that---given the same privacy parameters---both algorithms utilize exactly the same amount of noise and the same threshold (but our algorithm can better allocate the weight). As a result, we are able to prove that our algorithm stochastically dominates the uniform weighting strategy (we will refer to this as the basic algorithm).

We extend our result to multiple rounds in \ouralgotworoundslong{} (\ouralgotworounds{}), splitting our privacy budget across the rounds, running \ouralgo{} in each round, and outputting the union of items found in both rounds.
Similar to DP-SIPS~\cite{swanberg2023dpsips}, in the second round, we remove from the input any items found in the first round (this is private by post-processing).
By a careful generalization of the privacy analysis of the weight and threshold approach, we show that it is possible to also reuse the noisy weights from the prior round (in a limited capacity).
We leverage this in two ways. First, we remove items which have very small weights from the first round: these have little chance of being output in the second round. Second, we bias the weighting produced by \ouralgo{} in the second round to further limit overallocation to items which received large weights in the first round. The combination of these ideas yields significant empirical improvements over both the basic algorithm and DP-SIPS.

In more detail, in our algorithm \ouralgo{}, users with a too small or large of a cardinality (we equivalently refer to this as the user's degree) are labeled non-adaptive: these users will add uniform weight to their items (or biased weight in the case of \ouralgotworounds{}). 
The rest of the users participate in adaptive reweighting with the privacy analysis making use of the upper bound on their degree. Initially each of these users sends a small amount of initial weight uniformly among their items (the total amount of weight sent per user is bounded by $1$ rather than the square root of their degree, the latter is the case for the basic weighting algorithm). Then, items with weight significantly above the threshold are truncated to only have weight slightly above the threshold (we do not want to truncate all the way to the threshold as the added noise can decrease weights). The weight removed via truncation is returned to each user proportional to their initial contributions. Then, users reroute a carefully chosen fraction of this ``excess'' weight back to their items. Finally, users add additional uniform weight to their items to make up for the small amount of weight that was initially sent.

% \todo{we need something about the thresholding done now}
Bounding the sensitivity of \ouralgo{} requires a careful analysis and is significantly more involved than for basic weighting. Several design choices made in our algorithm, such as using an initial uniform weighting inversely proportional to cardinality rather than square root of cardinality, using a minimum and maximum adaptive degree, and choosing the fraction of how much excess weight to reroute are all required for the following theorem to hold. 
Furthermore, we generalize the analysis of the weight and threshold paradigm to allow us to use noisy weights from first round to bias weights in the second round for \ouralgotworounds{}.
This biasing further complicates the sensitivity analysis of \ouralgo{} which we address by putting limits on the minimum and maximum bias.

\begin{theorem}[Privacy, Informal version of \cref{thm:dp-two-rounds} and \cref{thm:dp-one-round}]
Using \ouralgo{} as the weighting algorithm achieves $(\eps, \delta)$-DP with the exact same noise and threshold parameters as the basic algorithm.
Running \ouralgo{} in two rounds with biases via \ouralgotworounds{} is $(\eps, \delta)$-DP.
\end{theorem}

Within \ouralgo{}, items have their weight truncated if it exceeds an ``adaptive threshold'' $\tau$ after adding the initial weights. $\tau$ is set to be $\beta$ standard deviations of the noise above the actual threshold that will be used to determine the output. $\beta \geq 0$ is a free parameter of the algorithm. By design, before adding noise, every item which receives at least weight $\tau$ in the basic algorithm will also receive weight at least $\tau$ by \ouralgo{}. Furthermore, the weights on all other items will only be increased under \ouralgo{} compared to the basic algorithm. Taking the final step of adding noise, we show the following theorem.

\begin{theorem}[Stochastic Dominance, Informal version of \cref{thm:maxalgo-perf}]\label{thm:maxalgo-perf-informal}
Let $U$ be the set of items output when using the basic algorithm and let $U^*$ be the set of items output when using \ouralgo{} as the weighting algorithm.
Then, for items $i \in \U$ and a free parameter $\beta \geq 0$,
\begin{itemize}
    \item If $\Pr\p{i \in U} < \Phi(\beta)$, then $\Pr\p{i \in U^*} \geq \Pr\p{i \in U}$.
    \item Otherwise, $\Pr\p{i \in U^*} \geq \Phi(\beta)$.
\end{itemize}
% \begin{equation*}
%     \Pr\left(i \in U^*\right) \geq \min\{\Pr\left(i \in U\right), \Phi(\beta)\},
% \end{equation*}
where $\Phi$ is the standard Gaussian cdf.
\end{theorem}

Compared to the basic algorithm, \ouralgo{} has a higher probability of outputting any item that does not reach the adaptive threshold in its initial stage as it reroutes excess weight to these items.
The theorem shows that \ouralgo{} \emph{stochastically dominates} the basic algorithm on these items.
For the remaining items, they already have an overwhelming probability of being output as their final weight before adding noise is at least several standard deviations above the threshold (this is quantitatively controlled by the parameter $\beta$).
In \cref{sec:utility}, we also describe a simple, concrete family of instances where \ouralgo{} significantly improves upon the baselines.


Finally, we conduct experiments on several publicly-available datasets with up to 800 billions of (user, item) pairs, up to three orders of magnitude larger than prior datasets used to evaluate sequential algorithms. Our algorithm performs the \emph{best out of all parallel baselines} on every dataset and is competitive with the sequential baselines.

\subsection{Related Work}
Our algorithms are in the area of privacy preserving algorithms with differential privacy guarantee which is the de facto standard of privacy (we refer to~\cite{dwork2014book} for an introduction to this area). As we covered the application and prior work on private partition selection in the introduction, we now provide more details on the work most related to our paper.

The differentially private partition selection problem was first studied in~\cite{korolova2009releasing}. They utilized the now-standard approach of subsampling to limit the number of items in each user's set, constructing weights over items, and thresholding noised weights to produce the output. They proposed a version of the basic weighting algorithm which uses the Laplace mechanism rather than the Gaussian mechanism. This algorithm was also used in~\cite{wilson2020dpsql} within the context of a private SQL system.
The problem received renewed study in~\cite{gopi2020dpunion} where the authors propose a generic class of greedy, sequential weighting algorithms which empirically outperform basic weighting (with either the Laplace or Gaussian mechanism). \cite{carvalho2022incorporatingitem} gave an alternative greedy, sequential weighting algorithm which leverages item frequencies in cases where each user has a multiset of items.
\cite{desfontaines2022dppartition} analyzed in depth the optimal strategy when each user has only a single item (all sets have cardinality one). This is the only work that does not utilize the weight and threshold approach, but it is tailored only for this special case.
The work most related to ours is DP-SIPS~\cite{swanberg2023dpsips} which proposes the first algorithm other than basic weighting which is amenable to implementation in a parallel environment. DP-SIPS splits the privacy budget over a small number of rounds, runs the basic algorithm as a black box each round, and iteratively removes the items found in previous rounds for future computations. This simple idea leads to large empirical improvements, giving a scalable algorithm that has competitive performance with sequential algorithms.

\subsection{Paper Organization}

In \cref{sec:prelim}, we formally define the private partition selection problem as well as differential privacy. In \cref{sec:meta-algo}, we describe the general weight and threshold approach to the problem as well as what properties of weighting algorithm are required to achieve privacy. In \cref{sec:algo}, we describe our algorithms \ouralgo{} and \ouralgotworounds{}. We analyze their privacy and utility theoretically in \cref{sec:privacy} and \cref{sec:utility}. We conclude with the empirical results in \cref{sec:experiments}.