\section{Related Work}
In this section, we review approaches most related to our method.
For readers who want to know more about GANs, please refer to____ and____ for more comprehensive surveys.

\subsection{Few-shot Generative Adversarial Networks}
Training GANs under limited data often results in volatility and overfitting issues, leading to low fidelity and quality of synthesized images.
%
\revise{Extensive approaches have been proposed to overcome these drawbacks and improve the synthesis quality}.
%
These techniques can be roughly divided into two categories based on whether pre-trained models are required: 1) approaches based on transfer learning and 2) approaches train from scratch.
%
\revise{The former assumes that one can facilitate the generalization of the target domain by leveraging the knowledge from the source domain, where massive training data is available for pre-training}.
%
TransferGAN____, Scale/Shift____ and FreezeD____ build baselines for adapting knowledge to the target domain by finetuning the pre-trained GANs.
%
MineGAN____ and MineGAN++____ acquire beneficial information with a well-designed miner network.
%
ElasticGAN____ analyzes the weight importance and regularizes the weight changes quantitatively during the adaptive process.
%
Similarly, FSGAN learns to adapt the pre-trained weights' statistics (singular values) to transfer knowledge____.
Although these approaches have promoted the fidelity and diversity of GANs in low-data regimes, they still require pre-training on sufficient data.
\revise{Besides, when the domain shift between the source and target domain is large, the performance degrades significantly due to negative transfer}.

Approaches directly trained on limited data alleviate the overfitting problem by adopting regularization or data augmentation techniques.
Liu~\etal____ design FastGAN with a skip-layer excitation (SLE) block to fuse low-resolution information into high-resolution feature maps. They further propose a self-supervised discriminator as a regularization to reconstruct training images.
LeCam-GAN improves the performance and stability by adding a regularized loss____.
Differently, Projected-GAN____ and Aided-GAN____ improve the synthesis performance by leveraging off-the-shelf discriminative models as discriminators.
%
Other approaches employ various data augmentation techniques to increase training samples, we briefly review them in~\ref{Sec:RelatedDA}.

\subsection{Data Augmentation in GANs}
\label{Sec:RelatedDA}
Inspired by the great success of data augmentation in training discriminant models____, researchers have investigated the effectiveness of applying data augmentation in training GANs recently.
Zhao~\etal~____ provide guidelines on augmenting training samples for both vanilla GAN and GANs with improved techniques such as consistency regularization____.
Borrowing the consistency principle from semi-supervised learning, CR-GAN penalizes the sensitivity of the discriminator to the augmentations on real images____, and ICR-GAN further improves the performance by augmenting both real and generated images____.
Zhao~\etal~____ design a differentiable augmentation approach to stabilize training, and Karras~\etal~____ perform an adaptive strategy to control the strength of augmentation.
ContraD____ and InsGEN____ train GANs with strong augmentations in a self-supervised way to improve the representation ability of $D$.
Theoretically, Tran~\etal____ prove that classical DA (\emph{e.g.}, rotation, cropping) may mislead the generator and propose a DAG framework for performing data augmentation in GANs.
\revise{Unlike prior approaches that augment images in the original image level, we enlarge the training sets implicitly in semantic feature space, without altering the data distribution and produce representative semantic features for augmentation}.