\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
\usepackage[preprint]{neurips_2024}
\usepackage{caption}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx} % 引入必要的包来处理图片
\usepackage{colortbl}
\usepackage{color}
\usepackage{pifont}
\usepackage{wrapfig}
\usepackage[font=footnotesize]{caption}
\usepackage{wrapfig}
\usepackage{placeins}

\usepackage{amssymb}
% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage{amssymb}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage[dvipsnames]{xcolor}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{geometry}
\usepackage{tcolorbox}
\usepackage{array}
\usepackage{colortbl}
\usepackage{graphicx}

\usepackage{lscape} % 用于旋转表格
\usepackage{subfig} % 用于子图
\usepackage{caption} % 用于图注
\usepackage{pifont} % 导入 pifont 包以使用 \ding{51}
% 定义 \cm 为 \ding{51}，即打勾符号
\newcommand{\cm}{\ding{51}}
\newcommand{\yijun}[1]{\textcolor{blue}{{#1}}}

% \usepackage[nonatbib]{neurips_2024}

\newcommand{\yj}[1]{\textcolor{red}{\textbf{\footnotesize [Yijun: #1]}}}

\newcommand{\huanqia}[1]{\textcolor{red}{\textbf{\footnotesize [huanqia: #1]}}}

\newcommand{\cmark}{\color{blue}{\ding{51}}}
\newcommand{\xmark}{\color{red}{\ding{55}}}

\usepackage{tcolorbox}
\tcbuselibrary{theorems}
% \usepackage{xcolor}

\definecolor{lightblueshade}{rgb}{0.8,0.9,1}
\definecolor{bluex}{rgb}{0.27, 0.42, 0.81}
\definecolor{purplex}{HTML}{9564bf}
\definecolor{red3}{HTML}{C52A20}
\definecolor{red2}{HTML}{B36A6F}
\definecolor{red1}{HTML}{FFb5b5}
\definecolor{purple}{HTML}{B36A6F}
\definecolor{darkyellow}{HTML}{D5BA82}
\definecolor{blue1}{HTML}{A0C0E0}
\definecolor{blue2}{HTML}{C4E4E3}
\definecolor{green1}{HTML}{A1D0C7}
\definecolor{green2}{HTML}{BFF6BA}
\definecolor{green3}{HTML}{028100}
\definecolor{teal}{HTML}{508AB2}
\definecolor{purple1}{HTML}{8d3a94}
\definecolor{olivegreen}{rgb}{0.33, 0.42, 0.18}


\newtcbtheorem[number within=section]{exmp}{Example}%
{colback=green2!5,colframe=blue1,fonttitle=\bfseries, left=.02in, right=.02in,bottom=.02in, top=.02in}{exmp}

\title{MM-IQ: Benchmarking Human-Like Abstraction and Reasoning in Multimodal Models}
%Are Your Multimodal Large Language Models Smart Enough?}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{Huanqia Cai$\thanks{caihuanqia19@mails.ucas.ac.cn}$ \quad
Yijun Yang \quad 
Winston Hu  
 \\~\\
Tencent Hunyuan Team\\
}





\begin{document}
\maketitle

\begin{figure}[h]
  \vspace{-3em}
    \centering
    \includegraphics[width=\textwidth]{imgs/combined_image1.pdf}
    \caption{\textbf{Left}: Performance (accuracy) of top-performing multimodal models and humans across eight reasoning paradigms of MM-IQ. \textbf{Right}: Visual examples of eight reasoning paradigms of MM-IQ (Detailed information can be found in Section~\ref{sec:reasoning pattern}).\looseness-1}
    % \vspace{-1em}
    \label{fig:MLLMs' performance}
  % \vspace{-0.2cm}
\end{figure}

\begin{abstract}
IQ testing has served as a foundational methodology for evaluating human cognitive capabilities, deliberately decoupling assessment from linguistic background, language proficiency, or domain-specific knowledge to isolate core competencies in abstraction and reasoning. Yet, artificial intelligence research currently lacks systematic benchmarks to quantify these critical cognitive dimensions in multimodal systems. To address this critical gap, we propose \textbf{MM-IQ}, a comprehensive evaluation framework comprising 2,710 meticulously curated test items spanning 8 distinct reasoning paradigms.

Through systematic evaluation of leading open-source and proprietary multimodal models, our benchmark reveals striking limitations: even state-of-the-art architectures achieve only marginally superior performance to random chance (27.49\% vs. 25\% baseline accuracy). This substantial performance chasm highlights the inadequacy of current multimodal systems in approximating fundamental human reasoning capacities, underscoring the need for paradigm-shifting advancements to bridge this cognitive divide.




\raisebox{-0.3\height}{\hspace{0.1cm}\includegraphics[width=0.41cm]{imgs/homepage.png}} \small \textbf{\mbox{Homepage:}} \href{https://acechq.github.io/MMIQ-benchmark/}{acechq.github.io/MMIQ-benchmark/}
% \vspace{0.3em}
% \raisebox{-0.2\height}{\includegraphics[width=0.45cm]{imgs/git_logo.png}} \small \textbf{\mbox{Code:}} \href{https://github.com/AceCHQ/MMIQ/tree/main}{github.com/AceCHQ/MMIQ/} \\
% \vspace{0.1em}
% \raisebox{-0.2\height}{\hspace{0.05cm}\includegraphics[width=0.4cm]{imgs/hf_logo.png}} \small \textbf{\mbox{Dataset:}} \href{https://huggingface.co/datasets/huanqia/MMIQ}{huggingface.co/datasets/huanqia/MMIQ}\\
% \vspace{1em}



\end{abstract}

\section{Introduction}
The rapid advancement of large multimodal models (LMMs) has intensified debates about their capacity for human-like abstraction and reasoning. While existing benchmarks evaluate specialized capabilities such as OCR, object localization, and medical image analysis~\cite{liu2023hidden,yue2024mmmu,liu2025mmbench}, these task-specific metrics fail to quantify the critical cognitive dimensions in multimodal systems. This limitation mirrors a long-standing challenge in human cognitive assessment: early methods conflated domain knowledge with innate reasoning ability until IQ testing emerged to isolate core cognitive competencies through language- and knowledge-agnostic evaluations~\cite{snow1984topography}. Inspired by this paradigm, we argue that multimodal intelligence evaluation should also similarly decouple linguistic proficiency and task-specific knowledge from the measurement of abstract reasoning capacities.



Abstract Visual Reasoning (AVR) offers a plausible solution to the above challenge. As shown in Figure~\ref{fig:2D geometry reasoning}, AVR problems usually contain visual puzzles with simple 2D/3D shapes. Solving these problems requires identifying and understanding the underlying abstract rules and generalizing them to novel configurations. 
% This approach has been fundamental in cognitive research, where AVR tasks serve as key components in IQ measurements~\cite{snow1984topography, carpenter1990one}. 
% Building upon this methodology, Chollet~\cite{chollet2019measure} developed the Abstraction and Reasoning Corpus (ARC) as a groundbreaking method to allow fair general intelligence comparisons between AI models and humans.
Although there exists a wide range of AVR benchmarks, e.g., RAVEN~\cite{zhang2019raven}, Bongard-LOGO~\cite{nie2020bongard}, and SVRT~\cite{fleuret2011comparing}, most of them have limited input modalities, reasoning paradigms, and restricted problem configurations, which can lead to biased evaluation results~\cite{van2021much}.
% For instance, RAVEN ignores necessary inductive biases, such as sensitivity within each row/column and incremental rule induction. Excelling in a single paradigm does not necessarily mean that a model has strong abstract reasoning ability, because it may exploit the bias of the dataset after training~\cite{hu2021stratified}.

To this end, we propose MM-IQ, a comprehensive AVR benchmark comprising 2,710 meticulously curated test items spanning 8 distinct reasoning paradigms. Like human IQ tests, MM-IQ fully eliminates domain-specific and linguistic biases while systematically diversifying problem configurations to prevent pattern memorization, presenting striking challenges for LMMs: even state-of-the-art models achieve only 27.49\% accuracy, marginally exceeding random chance (25\%) but far below human-level performance (51.27\%). This substantial performance chasm highlights the inadequacy of current LMMs in approximating fundamental human reasoning capacities, underscoring the need for paradigm-shifting advancements to bridge this cognitive divide.
By applying IQ-testing principles to multimodal models, MM-IQ fills a critical gap in existing multimodal benchmarks, e.g., MMBench~\cite{liu2025mmbench} and MMMU~\cite{yue2024mmmu} that focus on broad task coverage rather than core reasoning abilities. Our results demonstrate that current architectures lack the intrinsic abstraction abilities necessary for human-like intelligence, shedding light on potential directions toward developing systems capable of genuine cognitive adaptation.\looseness-1





\section{Related Work} 
\label{sec:Categories}
Following ~\cite{malkinski2023review,jiang2024marvel, malkinski2022deep}, all existing AVR benchmarks, including our MM-IQ, can be cataloged along three dimensions: input shape, problem configuration, and reasoning paradigm, as shown in Table ~\ref{tab:dataset}.
Input shape refers to the input forms of the objects in the given image, which contributes to evaluating models' cognition abilities of different shapes. Diverse problem configurations assess models' abstract reasoning capabilities across multi-dimensional aspects, including pattern recognition (Raven’s Progressive Matrices~\cite{raven2003raven}), analogical transfer ability (Visual Analogy~\cite{hill2019learning}), discrimination ability (Odd-one-out~\cite{mandziuk2019deepiq}), extrapolation and generalization ability (Visual Extrapolation~\cite{webb2020learning}), and numerical reasoning ability (Arithmetic Reasoning~\cite{zhang2020machine}), etc. MM-IQ's inclusion of diverse problem configurations ensures a thorough evaluation of multimodal models' abstract reasoning capabilities across various AVR problems. Reasoning paradigm is a more fine-grained category that evaluates LMMs' abstract reasoning capabilities, like logical deduction, temporal and spatial cognition, geometric, etc. It includes various reasoning paradigms such as temporal movement, spatial relationships, logical operations, and both 2D and 3D geometry, which are based on the internal forms, relationships, and numbers of objects in the given image. Existing benchmarks have only three paradigms on average except for MARVEL, which has five ones, but its quantity is relatively small. Although RAVEN~\citep{zhang2019raven},  G-set~\citep{mandziuk2019deepiq}, VAP~\citep{hill2019learning}, and DOPT~\citep{webb2020learning} have more than 1,000 instances, all of their data are generated by computer programs, which lack diversity and complexity~\cite{chollet2019measure}. MM-IQ comprises a total of 2,710 meticulously selected problems, 3x larger than MARVEL, and covers a diverse spectrum of 8 fine-grained reasoning paradigms.

\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{ll|c|c|c|c|c|c|c|c|c|c}
\toprule
\multicolumn{2}{c|}{}                                            & RAVEN$\ast$ & G-set$\ast$ & VAP$\ast$  & SVRT &  DOPT$\ast$ & ARC & MNS & IQTest & MARVEL & \textbf{MM-IQ} \\ \midrule
\multicolumn{1}{l}{\multirow{3}{*}{\textbf{Input Shape}}}   & Geometric            & \cm  & \cm   & \cm &                                  &\cm  & \cm  &\cm & \cm & \cm   & \cm  \\

\multicolumn{1}{l}{}                               & Abstract             &      &       &                                & \cm  &     &   &  & \cm  & \cm  & \cm    \\

\multicolumn{1}{l}{}                               & Concrete Object             &      &       &                                       &   &     &    &  & & & \cm     \\ 

\midrule

\multicolumn{1}{l}{\multirow{6}{*}{\textbf{Problem Configuration}}} & Raven’s Progressive Matrices~\cite{raven2003raven}             &  \cm    &\cm       &                                  &      &  &   &  & \cm &  \cm & \cm \\

\multicolumn{1}{l}{}                               & Visual Analogy~\cite{hill2019learning}               &      &     &   \cm                                     &      &     &   &  &  & \cm &\cm    \\

\multicolumn{1}{l}{}                               & Odd-one-out~\cite{mandziuk2019deepiq}               &  & \cm &                                        &      &     &   & & \cm & & \cm   \\

\multicolumn{1}{l}{}                               & Visual Extrapolation~\cite{webb2020learning}                 &      &       &                                     &    & \cm    &   & & \cm &\cm  & \cm    \\

\multicolumn{1}{l}{}                               & Arithmetic Reasoning~\cite{zhang2020machine}            &      &       &                                             &      &     &   &  \cm & \cm &  & \cm   \\ 

\multicolumn{1}{l}{}                               & Visual Grouping               &      &       &                                          & \cm     &     &   &  &  &  & \cm   \\ 

\midrule

\multicolumn{1}{l}{\multirow{9}{*}{\textbf{Reasoning Paradigm}}}       & Temporal Movement    & \cm  & \cm   &                                &      & \cm & \cm  &  & & \cm   & \cm  \\
\multicolumn{1}{l}{}                               & Spatial Relationship &      &       &                                        & \cm   &     & \cm   & & & \cm   & \cm  \\
% \multicolumn{1}{l}{}                               & Quantities          & \cm  & \cm   & \cm                           & \cm   & \cm  & \cm   & & \cm    & \cm \\
% % \multicolumn{1}{l}{}                               & Mathematical         & \cm  & \cm    & \cm &                                     &      &     & \cm & \cm     \\
\multicolumn{1}{l}{}                               & 2D-Geometry          &      &       &                               & \cm  &     &    &  &\cm & \cm   & \cm \\
\multicolumn{1}{l}{}                               & 3D-Geometry          &      &       &                                       &      &     & &  &\cm  & \cm  & \cm \\ 

\multicolumn{1}{l}{}                               & Logical Operation          &  \cm    &       & \cm                                 &      &     &   &  &  &  & \cm \\
\multicolumn{1}{l}{}                               & Concrete Object          &      &       &                                         &      &     &   &  &  &  & \cm \\
\multicolumn{1}{l}{}                               & Visual Instruction          &      &       &                                         &      &     &   &  &  &  & \cm \\
\multicolumn{1}{l}{}                               & mathematics           & \cm     &  \cm     &  \cm                                       & \cm     & \cm    & \cm  & \cm & \cm & \cm & \cm \\



\bottomrule
\multicolumn{2}{c|}{Dataset Size}                  & 14,000  & 1,500      &   100,000   &     23                         &  95,200    & 600     &  - & 228 & 770 & 2,710 \\ \bottomrule
\end{tabular}%

}
\caption{Comparison between our \textsc{MM-IQ} and related benchmarks: RAVEN$\ast$~\citep{zhang2019raven}, G-set$\ast$~\citep{mandziuk2019deepiq}, VAP$\ast$~\citep{hill2019learning}, SVRT~\citep{fleuret2011comparing}, DOPT$\ast$~\citep{webb2020learning},
ARC~\citep{chollet2019measure}, MNS~\cite{zhang2020machine},
IQTest~\cite{lu2023mathvista},
MARVEL~\cite{jiang2024marvel}. $\ast$ denotes that the dataset is automatically produced through procedural content generation.\looseness-1}
\label{tab:dataset}
\vspace{-0.3cm}
\end{table*}



\section{Construction of MM-IQ}

Two features distinguish MM-IQ from other existing benchmarks for LMMs: (1) MM-IQ adopts data from professional and authoritative examinations and performs rigorous quality control, which ensures its correctness and validity; (2) MM-IQ is a comprehensive AVR benchmark for evaluating the intelligence of LMMs, comprising a total of 2,710 problems and covering a diverse spectrum of 8 fine-grained reasoning paradigms.
% \subsection{Diverse Problem Configurations}
% \label{sec:task}

\subsection{Data Collection}
The collection of MM-IQ involves three stages. Initially, we examined existing AVR datasets~\cite{zhang2019raven, mandziuk2019deepiq, chollet2019measure, nie2020bongard} and discovered that most of them are generated by hand-coded procedures. Although programmatic synthesis can produce substantial amounts of data, it often lacks the necessary diversity. Hence, we chose to collect AVR problems from existing resources. Following ~\cite{liu2020logiqa, jiang2024marvel, zhang2024cmmmu}, we collected problems from publicly available questions of the National Civil Servants Examination of China. These problems are specifically designed to evaluate civil servant candidates’ critical thinking and problem-solving skills, and they meet our criteria for both quantity and diversity. The collected data underwent a rigorous filtering process conducted by two human annotators to eliminate any low-quality entries. The filtering principle is that the problems can be solved only by the extraction and utilization of high-level abstract reasoning information based on visual inputs.


% Therefore, we collected these data from the public resource website\footnote{https://v.huatu.com/tiku/} 

To create a systematic and comprehensive benchmark, we proceeded to the second stage, which involved classifying the data into different paradigms and further adding more problems to those with fewer instances. Based on the descriptions of collected problems, we classified them into the corresponding reasoning paradigms. Additionally, we identified the common attributes of each paradigm's problems, such as attributes and entity types, and supplemented those with fewer instances to ensure that each fine-grained attribute or entity type had sufficient problems.

The final stage involved a more thorough cleaning of the collected data through deduplication and extraction of the final answers. We performed deduplication in two ways. The first way was to employ the MD5 hashing algorithm to find the same images and removed them if their input text was the same. Secondly, we utilized the problems' corresponding information, where similar ones were considered suspected duplicates, and then reviewed by human annotators based on the input image and corresponding information to identify and eliminate duplications.

Additionally, the final answers were extracted by human annotators to facilitate efficient evaluation later. To further support the development of the open-source community, we also translated all content of questions and answers from Chinese to English based on GPT-4, resulting in a bilingual version of the dataset. All translations were verified by humans to ensure their correctness. Specifically, the data distribution of the reasoning paradigms is shown in the Fig.~\ref{fig:reasoning pattern distribution}, where concrete object and visual instruction are less than 2\% since they are rare in the existing data.

\subsection{Reasoning Paradigms of MM-IQ}
\label{sec:reasoning pattern}

For simplicity and consistency, we follow MARVEL, a dataset evaluating LMMs' AVR ability but 3x smaller than ours, and extend its taxonomy to 8 categories, including logical operation, mathematics, 2D-geometry, 3D-geometry, visual instruction, temporal movement, spatial relationship, and concrete object. Notably, we merge mathematical and quantity categories from MARVEL's taxonomy into mathematics to align more closely with our taxonomy.

\textbf{Logical Operation} refers to the application of logical operators, such as AND (conjunction), OR (disjunction), XOR (exclusive disjunction), etc. This reasoning process involves observing and summarizing the abstract logical operations represented in the given graphics to derive general logical rules, which can then be applied to identify the required graphics. An example of reasoning involving the AND operation is shown in Fig.~\ref{fig:Logical operation reasoning}. 


\textbf{2D-Geometry} encompasses two distinct categories. The first category involves understanding the attribute patterns of the provided 2D geometric graphics, such as symmetry, straightness, openness, and closure, and making analogies or extrapolations based on these attributes. The second category focuses on graphic splicing, which entails identifying a complete pattern that can be formed by assembling existing 2D geometric fragments. Together, these two types assess the capability of LMMs to perceive geometric shapes from both local and global perspectives. A visualized example of 2D-geometry reasoning concerning the symmetry property is shown in Fig.~\ref{fig:2D geometry reasoning}. 






\textbf{3D-Geometry} can be categorized into three categories. The first category assesses the capability of LMMs to perceive 3D geometry comprehensively by observing a polyhedron and identifying the required view from a specific direction. The second category is analogous to 2D graphic splicing, but it involves basic fragments and target objects that are three-dimensional in nature. The third category evaluates LMMs' comprehension of the interior structure of a 3D solid shape with the goal of identifying a cross-sectional view of the solid. An example of 3D-geometry reasoning for the specific directional view is shown in Fig.~\ref{fig:3D geometry reasoning}. 





\textbf{Visual Instruction} employs visual cues such as points, lines, and arrows to highlight key areas necessary for solving visual puzzles. Unlike other reasoning paradigms, this approach allows test-takers to concentrate solely on these visual indicators rather than requiring a comprehensive observation of the entire panel. A visualized example of visual instruction reasoning with arrows is shown in Fig.~\ref{fig:visual instruction reasoning}. 




\textbf{Temporal Movement} focuses on changes in position or movement, including translation, rotation, and flipping. This paradigm encompasses several problem configurations discussed in Section~\ref{sec:Categories}, including Raven’s Progressive Matrices, Visual Analogy, and Visual Extrapolation. A visualized example of temporal movement reasoning involving rotation is shown in Fig.~\ref{fig:temporal movement reasoning}. 




\textbf{Spatial Relationship} examines the static relative positional relationships among objects. This paradigm also encompasses various problem configurations, including Raven’s Progressive Matrices, Visual Analogy, Visual Extrapolation and Visual Grouping. An example of spatial relationship reasoning is shown in Fig.~\ref{fig:spatial relationship reasoning}. 




\textbf{Concrete Object} involves real-world objects, such as vases, leaves, or animals, and requires LMMs to categorize these objects based on their characteristics, which may require external knowledge to solve. A visualized example of concrete object reasoning is shown in Fig.~\ref{fig:concrete object reasoning}. 



\textbf{Mathematics} evaluates LMMs' ability to reason about quantity, numbers, and arithmetic operations through visual inputs. This paradigm contains two types of tasks. The first type involves perceiving basic graphical elements, such as points, angles, lines, and planes, and applying arithmetic operations to these elements. The second type involves identifying an arithmetic expression that is satisfied by the numbers in the given images and determining the missing number based on the four fundamental operators: addition, subtraction, multiplication, and division. Examples of the two types of mathematics reasoning are shown in Fig.~\ref{fig:quantity reasoning} and Fig.~\ref{fig:arithmetic visual reasoning}. The intersection point is the basic element that used in Fig.~\ref{fig:quantity reasoning}.



\begin{table*}[h]
    \centering
    \caption{\textbf{Model and Human Performance on MM-IQ (\%)}. Abbreviations adopted: \textbf{LO} for Logical Operation; \textbf{2D-G} for 2D-Geometry; \textbf{3D-G} for 3D-Geometry; \textbf{VI} for Visual Instruction; \textbf{TM} for Temporal Movement; \textbf{SR} for Spatial Relationship; \textbf{CO} for Concrete Object. }
    \label{tab:performance}
    \resizebox{\textwidth}{!}{ % Resize the table to fit the text width
        \begin{tabular}{lccccccccc}
            \toprule
            \rowcolor{gray!20} \textbf{Model} & \textbf{Mean} & \textbf{LO} & \textbf{Math}  & \textbf{2D-G} & \textbf{3D-G} & \textbf{VI} & \textbf{TM} & \textbf{SR} & \textbf{CO}  \\ 
            \midrule
            \rowcolor{yellow!20} \multicolumn{10}{c}{\textbf{Open-Source LMMs}} \\
            LLaVA-1.6-7B~\cite{liu2024llavanext} & 19.45 & 24.22	& 20.34	& 17.92	& 15.83	& 20.00 &	18.23	& 17.82	& 18.42 \\ 
            Deepseek-vl-7b-chat~\cite{bi2024deepseek} & 22.17 & 19.53 & 20.30 & 22.25	& 27.39	& 35.56 & 23.72	& 24.75	& 15.79 \\
            Qwen2-VL-72B-Instruct~\cite{Qwen2VL} & 26.38 & 24.74 & 24.40 & 28.60 &	27.39 &24.44 &26.93	&32.67 &23.68 \\
            QVQ-72B-Preview~\cite{qvq-72b-preview} & 26.94 & 28.91	& 25.59	&29.23 &26.38 &26.67 &25.43	&22.77	&34.21 \\
            \midrule
            \rowcolor{red!20} \multicolumn{10}{c}{\textbf{Proprietary LMMs}} \\
            GPT-4o~\cite{achiam2023gpt} & 26.87 & 25.52 & 25.70 & 28.32 & 27.64 &26.67 &25.69 &27.72	&50.00 \\
            Gemini-1.5-Pro-002~\cite{team2023gemini} & 26.86 & 19.53	&27.43	&28.03	&25.88	&24.44	&31.17	&25.74	&39.47 \\
            Claude-3.5-Sonnet~\cite{anthropic_claude3} & 27.49 & 23.41 &29.48 &26.60	&24.37	&35.56	&25.69	&27.72 &	42.11 \\
            \midrule
            Human Performance & 51.27 & 61.36 & 45.03  & 60.11 & 47.48 & 46.67 & 55.61  & 36.63 & 65.79  \\
            \bottomrule            
        \end{tabular}
    }
\end{table*}



\begin{figure*}[h!]
    \centering
    \begin{tcolorbox}[colframe=RoyalBlue, colback=white]
    \begin{minipage}[t]{0.45\textwidth}
        \textbf{Prompt:} Choose the most appropriate option from the given four choices to fill in the question mark, so that it presents a certain regularity:\\
        \hfill
        \begin{minipage}{0.7\textwidth}
            \centering
            \includegraphics[scale=.5]{imgs/logical_AND_2664.png}
        \end{minipage}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.5\textwidth}
        \textbf{LMM's Response:} 
        \begin{tcolorbox}[colframe=RoyalBlue, colback=gray!10]
        % \begin{minted}{}
The first two panels in each row have similar parts to the third panel. 

In the first and second rows, the third panel is obtained by removing the differences and finding the similarities between the first two panels; the same rule is applied to the third row, and the small circle is obtained by removing the differences and finding the similarities between the first two panels. Only option C meets the requirements. 

Therefore, option C is selected.
% \end{minted}
        \end{tcolorbox}
        \textbf{Ground Truth:} \textbf{C}
        % \textcolor{green}{{\Large \checkmark}}
    \end{minipage}
    \end{tcolorbox}
    \caption{A visualized example of logical operation paradigm.}
    \label{fig:Logical operation reasoning}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \begin{tcolorbox}[colframe=RoyalBlue, colback=white]
    \textbf{Prompt:} Choose the most appropriate option from the given choice: \\
    Option A: 4;  Option B: 5;  Option C: 6;  Option D: 7.
    \\
    % \hfill
    % \centering
    \begin{minipage}{0.5\textwidth}
        \centering
    \includegraphics[scale=0.8]{imgs/arithmetic_1133.png}
    \end{minipage}
    \\
    \\
    \textbf{LMM's Response:} 
    \begin{tcolorbox}[colframe=RoyalBlue, colback=gray!10]
    % \begin{minted}{}
By observing the numbers in the figure, we notice the following patterns: 6 - 4 = 8 - 6, 5 - 7 = 1 - 3, and 3 - 8 = -3 - 2. The pattern is that the number in the bottom right corner minus the number in the top left corner equals the number in the bottom left corner minus the number in the top right corner.

Using this pattern, we can set up the equation: ? - 9 = 6 - 8. Solving for ?, we find that ? = 7. 

Therefore, we choose option D.
% \end{minted}
\end{tcolorbox}

\textbf{Ground Truth:} \textbf{D}
    \end{tcolorbox}
    \caption{A visualized example of mathematics paradigm.}
    \label{fig:arithmetic visual reasoning}
\end{figure*}




\clearpage



\begin{figure*}[h!]
    \centering
    \begin{tcolorbox}[colframe=RoyalBlue, colback=white]
    \textbf{Prompt:} The option that best fits the given pattern of figures is ( ).\\
    \\
    % \hfill
    % \centering
    \begin{minipage}{0.6\textwidth}
        \centering
    \includegraphics[scale=0.19]{imgs/2D_sys_1036.png}
    \end{minipage}
    \\
    \\
    \textbf{LMM's Response:} 
    \begin{tcolorbox}[colframe=RoyalBlue, colback=gray!10]
    % \begin{minted}{}
To determine the option that best fits the given pattern of figures, let's analyze the symmetry of each figure in the sequence:

The first panel is a cylinder, which has rotational symmetry around its vertical axis.
The second panel is a circle with two vertical lines inside, which has vertical symmetry.
The third panel is a rectangle inside a square, which have both vertical and horizontal symmetry.
The fourth panel is a circle with two small circles inside, which has rotational symmetry around its center.
Now, let's consider the options:

A. A trapezoid with an X inside, which has vertical symmetry.

B. A recycling symbol, which has no symmetry.

C. A cube, which has multiple planes of symmetry.

D. A spiral, which has no symmetry.

Based on the symmetry analysis, the option that best fits the pattern of figures is A, as it has rotational symmetry similar to the other figures in the sequence.
% \end{minted}
\end{tcolorbox}

\textbf{Ground Truth:} \textbf{A}
    \end{tcolorbox}
    \caption{A visualized example of 2D-geometry paradigm.}
    \label{fig:2D geometry reasoning}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \begin{tcolorbox}[colframe=RoyalBlue, colback=white]
    \textbf{Prompt:} Choose the most appropriate option from the given four options to present a certain regularity:\\
    \\
    % \hfill
    % \centering
    \begin{minipage}{0.7\textwidth}
        \centering
    \includegraphics[scale=0.38]{imgs/Visual_instruction_arrow_2440.png}
    \end{minipage}
    \\
    \\
    \textbf{LMM's Response:} 
    \begin{tcolorbox}[colframe=RoyalBlue, colback=gray!10]
    % \begin{minted}{}
By looking at the images, we notice that each picture has arrows, so we focus on their direction. When we examine the figure in the question from left to right, we see that all the arrows point to acute angles. Among the four options, only option B meets the requirement that the arrows point to acute angles.

Therefore, option B is the correct choice.
% \end{minted}
\end{tcolorbox}

\textbf{Ground Truth:} \textbf{B}
    \end{tcolorbox}
    \caption{A visualized example of visual instruction paradigm.}
    \label{fig:visual instruction reasoning}
\end{figure*}



\begin{figure*}[h!]
    \centering
    \begin{tcolorbox}[colframe=RoyalBlue, colback=white]
    \textbf{Prompt:} Choose the most appropriate option from the given four options to present a certain regularity:\\
    \\
    % \hfill
    % \centering
    \begin{minipage}{0.7\textwidth}
        \centering
    \includegraphics[scale=0.6]{imgs/spatial_6160.png}
    \end{minipage}
    \\
    \\
    \textbf{LMM's Response:} 
    \begin{tcolorbox}[colframe=RoyalBlue, colback=gray!10]
    % \begin{minted}{}
As can be seen from the figure, the number of elements is the same, but the styles are different, mainly indicating that the positions and methods of placing multiple elements vary. We can identify the pattern in the three figures from the left box and then apply this pattern to the right box. All the figures in the square box on the left are composed of solid circles, triangles, and quadrilaterals. The positions of the solid circles are upper inner, lower, and lower inner, respectively; the positions of the triangles are upper outer, upper inner, and upper, respectively; and the positions of the quadrilaterals are lower, upper outer, and lower outer, respectively. By applying this pattern to the square box on the right, the figure at the question mark should be a solid trapezoid in the lower inner position, a rhombus in the upper position, and a circle in the lower outer position, which corresponds to option D.

Therefore, the correct option is option D.
% \end{minted}
\end{tcolorbox}

\textbf{Ground Truth:} \textbf{D}
    \end{tcolorbox}
    \caption{A visualized example of spatial relationship paradigm.}
    \label{fig:spatial relationship reasoning}
\end{figure*}






\begin{figure*}[h!]
    \centering
    \begin{tcolorbox}[colframe=RoyalBlue, colback=white]
    \begin{minipage}[t]{0.37\textwidth}
        \textbf{Prompt:} Choose the most appropriate option from the given four choices to fill in the question mark, so that it presents a certain regularity:\\
        \\
        \hfill
        \begin{minipage}{0.38\textwidth}
            \centering
            \includegraphics[scale=0.75]{imgs/concrete_object_6167.png}
        \end{minipage}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.6\textwidth}
        \textbf{LMM's Response:} 
        \begin{tcolorbox}[colframe=RoyalBlue, colback=gray!10]
        % \begin{minted}{}
To solve this puzzle, we need to identify the pattern or regularity in the given grid.

Looking at the grid:

1. The first row contains a hat, roller skates, and gloves, which are all wearable items.

2. The second row contains a radio, a gramophone, and a television, which are all related to audio-visual media.

3. The third row contains tools (screwdrivers), a stool, and the missing item.

The pattern seems to be categorization based on function or use. The third row appears to be related to household or utility items.

Given the options:
- A: Fire extinguisher
- B: Pills
- C: Soccer ball
- D: Padlock

The most appropriate choice to complete the pattern of household or utility items is the padlock (D).
% \end{minted}
        \end{tcolorbox}
        \textbf{Ground Truth:} \textbf{D}
        % \textcolor{green}{{\Large \checkmark}}
    \end{minipage}
    \end{tcolorbox}
    \caption{A visualized example of concrete object paradigm.}
    \label{fig:concrete object reasoning}
\end{figure*}






\begin{figure*}[h!]
    \centering
    \begin{tcolorbox}[colframe=RoyalBlue, colback=white]
    \textbf{Prompt:} The one that matches the top view is:\\
    \\
    % \hfill
    % \centering
    \begin{minipage}{0.5\textwidth}
        \centering
    \includegraphics[scale=0.13]{imgs/3D_view_1699.png}
    \end{minipage}
    \\
    \\
    \textbf{LMM's Response:} 
    \begin{tcolorbox}[colframe=RoyalBlue, colback=gray!10]
    % \begin{minted}{}
The top view is the view obtained by orthographic projection from the top of the object, also called the top view. Analysis options:

A: There is a hollow cylinder in the three-dimensional figure that is not reflected, so it is excluded;

B: The hollow cylinder in the three-dimensional figure is only partially reflected, which is incomplete, so it is excluded;

C: There is a hollow cylinder in the middle of the bottom of the three-dimensional figure. The visible lines are represented by solid lines, and the invisible but real lines are represented by dotted lines. This is correct;

D: When observing the top row of the three-dimensional figure from top to bottom, there should be only 3 solid line surfaces, so it is excluded.

Therefore, the final choice is option C.
% \end{minted}
\end{tcolorbox}

\textbf{Ground Truth:} \textbf{C}
    \end{tcolorbox}
    \caption{A visualized example of 3D-geometry reasoning.}
    \label{fig:3D geometry reasoning}
\end{figure*}



\begin{figure*}[h!]
    \centering
    \begin{tcolorbox}[colframe=RoyalBlue, colback=white]
    \begin{minipage}[t]{0.45\textwidth}
        \textbf{Prompt:} Choose the most appropriate option from the given four choices to fill in the question mark, so that it presents a certain regularity:\\
        \\
        \hfill
        \begin{minipage}{0.8\textwidth}
            \centering
            \includegraphics[scale=.22]{imgs/temporal_rotation_1379.png}
        \end{minipage}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.45\textwidth}
        \textbf{LMM's Response:} 
        \begin{tcolorbox}[colframe=RoyalBlue, colback=gray!10]
        % \begin{minted}{}
By observing the images, we can see that in the first row, image 1 is rotated 90 degrees clockwise to produce image 2, and then image 2 is flipped to the right to create image 3. The second row follows the same pattern.


Applying this same logic to the third row, we flip image 2 to the right, which allows us to obtain option C. 


Therefore, the final choice is option C.

% \end{minted}
        \end{tcolorbox}
        \textbf{Ground Truth:} \textbf{C}
        % \textcolor{green}{{\Large \checkmark}}
    \end{minipage}
    \end{tcolorbox}
    \caption{A visualized example of temporal movement paradigm.}
    \label{fig:temporal movement reasoning}
\end{figure*}







\begin{figure*}[h!]
    \centering
    \begin{tcolorbox}[colframe=RoyalBlue, colback=white]
    \begin{minipage}[t]{0.45\textwidth}
        \textbf{Prompt:} Choose the most appropriate option from the given four choices to fill in the question mark, so that it presents a certain regularity:\\
        \\
        \hfill
        \begin{minipage}{0.8\textwidth}
            \centering
            \includegraphics[scale=.25]{imgs/quantity_point_678.png}
        \end{minipage}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.46\textwidth}
        \textbf{LMM's Response:} 
        \begin{tcolorbox}[colframe=RoyalBlue, colback=gray!10]
        % \begin{minted}{}
        Based on the pattern observed, each figure in the sequence has 7 intersections. After analyzing the options, only option A meets this requirement. 
        
        
        Therefore, the correct answer is option A.
        % \end{minted}
        \end{tcolorbox}
        \textbf{Ground Truth:} \textbf{A}
        % \textcolor{green}{{\Large \checkmark}}
    \end{minipage}
    \end{tcolorbox}
    \caption{A visualized example of mathematics paradigm.}
    \label{fig:quantity reasoning}
\end{figure*}


\begin{figure*}[h!]
    \centering
    \begin{tcolorbox}[colframe=RoyalBlue, colback=white]
    \textbf{Prompt:} The option that best fits the given pattern of figures is ( ).\\
    \\
    % \hfill
    % \centering
    \begin{minipage}{0.5\textwidth}
        \centering
    \includegraphics[scale=0.23]{imgs/example_claude3.5.png}
    \end{minipage}
    \\
    \\
    \textbf{Response:} 
    \begin{tcolorbox}[colframe=RoyalBlue, colback=gray!10]
    % \begin{minted}{}
To solve this problem, we need to analyze the pattern in the given sequence of figures and then determine which of the options (A, B, C, or D) best continues that pattern.

Looking at the sequence:

1. We start with a simple line connecting two points.

2. Then we have a triangle with 3 points.

3. Next is a shape with 4 points, forming a square with crossed lines inside.

4. The fourth shape has 5 points, forming an irregular pentagon.


The pattern seems to be increasing the number of points by one each time, while also increasing the complexity of the connections between these points.

Following this pattern, the next shape should have 6 points.

Looking at the options:

A. A star shape with 5 points - doesn't fit the pattern as it doesn't increase to 6 points.

B. A house-like shape with 6 points - fits the number of points, but the connections are simpler than the previous shapes.

C. A hexagon with interconnected points - this fits both the number of points (6) and the complexity of connections.

D. An irregular shape with 6 points - fits the number of points, but the connections don't follow the increasing complexity.

Therefore, the option that best fits the given pattern is C. It has 6 points, continuing the increasing point count, and its interconnected structure follows the pattern of increasing complexity in how the points are connected.
% \end{minted}
\end{tcolorbox}

\textbf{Final Answer:} \textbf{C}  \textcolor{green}{{\Large \checkmark}}
    \end{tcolorbox}
    \caption{An example of Claude-3.5-Sonnet's prediction.}
    \label{fig:example of Claude-3.5-sonnet's prediction}
\end{figure*}


\clearpage

\section{Experiments}

\subsection{Experimental Setup}
We evaluate open-source and closed-source LMMs on the MM-IQ dataset with zero-shot prompting and employ the same question prompt for all models. The few-shot prompting results will be included in the future version of MM-IQ since how to design appropriate multimodal prompts is still an open problem~\cite{yin2023survey,tai2024link}. For open-source LMMs, we select widely used and state-of-the-art models, including QVQ-72B-Preview~\cite{qvq-72b-preview}, Qwen2-VL-72B-Instruct~\cite{Qwen2VL}, Deepseek-VL-7B-Chat~\cite{bi2024deepseek}, and LLaVA-1.6-7B~\cite{liu2024llavanext}. For closed-source LMMs, we adopt GPT-4o-2024-08-06~\cite{achiam2023gpt}, Gemini-1.5-Pro-002~\cite{team2023gemini}, and Claude-3.5-Sonnet-2024-06-20~\cite{anthropic_claude3}. For a fair comparison, we employ the same settings and default hyper-parameters for all LMMs (please refer to Table~\ref{tab:mllm_generating_params} for more details). Each model generates a single response to each problem in the dataset.
The evaluation process of LMMs consists of three steps: (1) response generation, (2) answer extraction, and (3) accuracy calculation. We extract the final answer using regular expression (regex) matching. For example, the final answer will be extracted from the response ``The correct answer is A.'' as ``A''. If there is no valid answer in the model’s response, it will be considered incorrect.



\begin{table*}[h]
    \centering
    \caption{Generating parameters for various LMMs.}
    \label{tab:mllm_generating_params}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{l|>{\raggedright\arraybackslash}p{0.7\textwidth}}
            \toprule
            \textbf{Model} & \textbf{Generation Setup} \\
            \midrule
            Claude-3.5-Sonnet-2024-06-20 & \texttt{temperature = 1.0, output\_token\_limit = 8,192, top\_p = 1.0} \\
            \midrule
            GPT-4o-2024-08-06 & \texttt{temperature = 1.0, output\_token\_limit = 16,384, top\_p = 1.0} \\
            \midrule
            Gemini-1.5-Pro-002 & \texttt{temperature = 1.0, output\_token\_limit = 8,192} \\
            \midrule
            DeepSeek-vl-7b-chat & \texttt{temperature = 1.0, output\_token\_limit = 2,048, do\_sample = False, top\_p = 1.0} \\
            \midrule
            LLaVA-1.6-7B & \texttt{temperature = 0, output\_token\_limit = 2,048} \\
            \midrule
            Qwen2-VL-72B-Instruct & \texttt{temperature = 1.0, output\_token\_limit = 8,192, top\_p = 0.001, top\_k = 1, do\_sample = True,} \\
            & \texttt{repetition\_penalty = 1.05} \\
            \midrule
            QVQ-72B-Preview & \texttt{temperature = 0.01, output\_token\_limit = 8,192, top\_p = 0.001, top\_k = 1, do\_sample = True,} \\
            & \texttt{repetition\_penalty = 1.0} \\
            \bottomrule
        \end{tabular}
    }
\end{table*}


\subsection{Overall Performance}
According to the results from Table~\ref{tab:performance}, we have the following conclusions. Firstly, human performance significantly outperforms all LMMs, achieving an average accuracy of 51.27\%, while the best LMM Claude-3.5-Sonnet only achieves 27.49\%. This substantial gap highlights LMMs' limitations in AVR tasks and underscores the necessity of our MM-IQ dataset. By comparing small LMMs (7B) with larger ones (72B), we find that increased model size improves performance, from an average accuracy of 20.81\% to 26.66\%. We further compare the performance between open-source and proprietary models and find that the 72B ones (averaging 26.66\%) can achieve comparable performance with proprietary models (averaging 27.07\%), highlighting the potential of the open-source community.

Secondly, several noteworthy phenomena are revealed by the more comprehensive analysis of the results across different reasoning paradigms. Among these paradigms, humans and closed-source LMMs perform better in object-concrete reasoning. Humans achieve an accuracy of 65.79\%, while GPT-4o achieves 50\%. Their scores are significantly higher than other models, especially the open-source ones. The object-concrete reasoning may require additional knowledge since the objects of the images are concrete. This observation may align with MMbench, which argues that proprietary models significantly outperform the open-source ones on tasks requiring additional knowledge, like celebrity recognition, physical property reasoning, natural relation reasoning, etc. The hardest paradigm for LMMs is the logical operation, which only scores at 23.69\% average, because the solving of logical operation needs to identify more fine-grained relationships between multiple objects and extract high-level abstract rules, like AND, OR, and XOR, raising a significant challenge to LMMs.

\subsection{Failure Analysis of LMMs on MM-IQ}



% \begin{wrapfigure}{r}{0.53\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{imgs/error_analysis_stacked.png}
%     \caption{\textbf{Distribution over three representative MLLMs' human-annotated errors.}}
%     \label{fig:error_types_proportions}
% \end{wrapfigure}


\begin{figure}[h]
    \centering
    \begin{minipage}[b]{0.6\textwidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/error_analysis_stacked.png}
        \caption{\textbf{Distribution over three representative LMMs' human-annotated errors.}}
        \label{fig:error_types_proportions}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.38\textwidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/data_distribution.png}
        \caption{\textbf{Data distribution of reasoning paradigms of MM-IQ.}}
        \label{fig:reasoning pattern distribution}
    \end{minipage}
\end{figure}





Table~\ref{tab:performance} demonstrates that the highest accuracy of LMMs (27.49\%) is almost equivalent to randomly guessing a correct answer among four options, which motivates us to ask: Does the strongest LMM, e.g., Claude, actually possess the reasoning abilities required by AVR tasks? To investigate this, we selected three representative models: Claude-3.5-Sonnet, Qwen2-VL-72B-Instruct, and LLaVA-1.6-7B, and examined their generated wrong responses through human-in-the-loop evaluation. We sampled a total of 90 predictions from each model for analysis. The 90 problems include 10 instances drawn from each reasoning paradigm and 20 instances from the mathematics paradigm, as the mathematics paradigm is significantly larger than the other paradigms, constituting 34.5\% of the entire MM-IQ dataset.

First of all, we take an in-depth look at the average length of predictions and their response styles. Compared to LLaVA-1.6-7B and Qwen2-VL-72B-Instruct, the best-performing LMM, Claude-3.5-Sonnet, tends to generate longer responses. Moreover, Claude-3.5-Sonnet's responses share  a consistent structure: they first offer a detailed caption of the given image and the possible abstract reasoning paradigms, and then discuss each option to identify the correct answer. A visual example of Claude-3.5-Sonnet's response is illustrated in Fig.~\ref{fig:example of Claude-3.5-sonnet's prediction}. In contrast, LLaVA-1.6-7B and Qwen2-VL-72B-Instruct fail to generate responses in a structured manner. These observations suggest that structured outputs may enhance reasoning performance.



Furthermore, we examined each wrong response and categorized them into three types: incorrect reasoning, incorrect visual understanding, and incorrect final answers, examples of which can be found in Fig.~\ref{fig:incorrect pattern reasoning}, Fig.~\ref{fig:image understanding error} and Fig.~\ref{fig:incorrect final answer}. As shown in Fig.~\ref{fig:error_types_proportions}, incorrect paradigm reasoning constitutes a major part of failures (32.3\% on average). In these responses, we observe that LMMs tend to solve problems by considering simpler wrong rules or focusing on more superficial changes rather than extracting higher-level abstract rules. Examples of simpler rules include objects in the image becoming progressively more compact or dense, and increasingly complex or detailed. A corresponding visualized example is provided in Fig.~\ref{fig:incorrect pattern reasoning}, where the red parts indicate the incorrect reasoning due to wrongly recognizing simpler rules. Enhancing LMMs' ability to perceive more fine-grained image details and accurately identify abstract paradigms is critical to address these shortcomings. 

Additionally, as shown in Fig.~\ref{fig:error_types_proportions}, Qwen2-VL-72B-Instruct and LLaVA-1.6-7B exhibit an additional error type compared to Claude-3.5-Sonnet: incorrect final answers, which accounts for nearly one-third of the errors. To further investigate whether the absence of explanations is a critical factor, we calculated the accuracy of all responses without explanations. Notably, for the top-performing model, Qwen2-VL-72B-Instruct, directly providing the final answer results in a performance drop of 4.7\% (from 26.9\% to 22.5\%) on average. Conversely, for LLaVA-1.6-7B, it leads to an improvement of 2.8\% (from 19.4\% to 22.2\%) on average. This underscores that generating detailed reasoning chains helps improve the performance of larger models.



\begin{figure*}[h]
\centering
\includegraphics[width=1\textwidth]{imgs/error_types_proportions.png}
\caption{\textbf{Proportions of incorrect visual understanding across eight reasoning paradigms.}\looseness-1}
% \vspace{-2.5em}
\label{fig:image understanding error}
\end{figure*}


\begin{figure*}[h!]
    \centering
    \begin{tcolorbox}[colframe=RoyalBlue, colback=white]
    \textbf{Prompt:} Which of the following options conforms to the given pattern of changes in the figures? \\
    \\
    % \hfill
    % \centering
    \begin{minipage}{0.8\textwidth}
        \centering
    \includegraphics[scale=0.6]{imgs/wrong_pattern_reasoning.png}
    \end{minipage}
    \\
    \\
    \textbf{LMM's Response:} 
    \begin{tcolorbox}[colframe=RoyalBlue, colback=gray!10]
To solve this problem, we need to analyze the pattern in the top row and then find which option in the bottom row follows that pattern.

In the top row, we see:
1. A circle inside a triangle inside a square
2. The same shapes, but the triangle is cut off on one side
3. The same shapes, but now the circle is cut off
4. A new pattern of intersecting squares
5. The same intersecting squares, but with an additional square added

\textcolor{red}{The pattern seems to be adding complexity or elements as we move from left to right.}

Now, looking at the options A, B, C, and D:

A: Shows two intersecting squares

B: Shows three intersecting squares

C: Shows four intersecting squares

D: Shows five intersecting squares

\textcolor{red}{Following the pattern of increasing complexity and number of elements, the correct answer should be D.} It shows the most complex arrangement with five intersecting squares, which follows the progression we see in the top row.

Therefore, the answer that best conforms to the given pattern is option D.
\end{tcolorbox}

\textbf{Final Answer:} \textbf{D} \textcolor{red}{{\Large \texttimes}}  \\
\textbf{Ground Truth:} \textbf{C} 
    \end{tcolorbox}
    \caption{An example of incorrect paradigm reasoning. The \textcolor{red}{red parts} are the incorrect reasoning due to wrongly recognizing simpler rules. The correct reasoning solution is that the intersection in the third panel is the sum of the intersections in the first two panels in the same box.}
    \label{fig:incorrect pattern reasoning}
\end{figure*}

\clearpage
\clearpage


\begin{figure*}[h!]
    \centering
    \begin{tcolorbox}[colframe=RoyalBlue, colback=white]
    \textbf{Prompt:} Choose the most appropriate one from the given four options to present a certain regularity: \\
    \\
    % \hfill
    % \centering
    \begin{minipage}{0.8\textwidth}
        \centering
    \includegraphics[scale=0.6]{imgs/error_analysis_wrong_visual_understanding.png}
    \end{minipage}
    \\
    \\
    \textbf{LMM's Response:} 
    \begin{tcolorbox}[colframe=RoyalBlue, colback=gray!10]
To solve this problem, we need to analyze the pattern in the given sequence of shapes and then determine which of the options A, B, C, or D best continues that pattern.

Let's examine the sequence:

1. Star (5-pointed)

2. Shape with two curved lines

3. Shape with two straight lines forming an acute angle

4. Shape with one curved line (quarter circle)

5. \textcolor{red}{Shape with one straight line}

The pattern seems to be a progression from more complex shapes to simpler ones, reducing the number of lines or points:  - From 5 points (star) to 2 lines, then to 1 line  - From curved lines to straight lines
Following this pattern, the next shape should be even simpler, possibly just a single straight line or a single point.

Looking at the options:

A: Curved line

B: Straight line

C: \textcolor{red}{Two intersecting straight lines}

D: \textcolor{red}{Single point}

Option B (straight line) or D (single point) would be the most logical continuations of the pattern. However, since we've already seen a straight line in the sequence, the single point (Option D) represents a further simplification and reduction, making it the most appropriate choice to continue the observed pattern.

Therefore, the best answer is D.
\end{tcolorbox}

\textbf{Final Answer:} \textbf{D} \textcolor{red}{{\Large \texttimes}}  \\
\textbf{Ground Truth:} \textbf{B} 
    \end{tcolorbox}
    \caption{An example of incorrect visual understanding. The \textcolor{red}{red parts} are the incorrect visual understanding. }
    \label{fig:incorrect visual understanding}
\end{figure*}




\begin{figure*}[h!]
    \centering
    \begin{tcolorbox}[colframe=RoyalBlue, colback=white]
    \textbf{Prompt:} Choose the only option from the four figures to fill in the question mark, so that it presents a certain regularity. \\
    \\
    % \hfill
    % \centering
    \begin{minipage}{0.8\textwidth}
        \centering
    \includegraphics[scale=0.2]{imgs/error_analysis_final_answer.png}
    \end{minipage}
    \\
    \\
    \textbf{LMM's Response:} 
    \begin{tcolorbox}[colframe=RoyalBlue, colback=gray!10]
The only option that presents a certain regularity is B.
\end{tcolorbox}

\textbf{Final Answer:} \textbf{B} \textcolor{red}{{\Large \texttimes}}  \\
\textbf{Ground Truth:} \textbf{A} 
    \end{tcolorbox}
    \caption{An example of incorrect final answers.}
    \label{fig:incorrect final answer}
\end{figure*}

% \clearpage

Finally, we conducted a deeper analysis of incorrect visual understanding, which subsequently leads to reasoning errors. As shown in Figure~\ref{fig:image understanding error}, we found that all three models perform consistently poorly on certain paradigms, such as logical operation, temporal movement, and spatial relationship, due to the graphics in the image being more complex. Moreover, we found that the proportion of incorrect visual understanding is inversely proportional to the performance of the model. For instance, Claude-3.5-Sonnet performs poorly on temporal movement and spatial relationship reasoning paradigms, and also performs worse on visual understanding of both paradigms. This underscores the necessity of enhancing the models' perceptual capacity to accurately interpret complex visual paradigms, thereby improving LMMs' reasoning capabilities.

In summary, our failure analysis of LMMs on the MM-IQ dataset highlights several critical points for further research and improvement in multimodal abstract reasoning: 1) Structured response generation: Models like Claude-3.5-Sonnet, which produce longer and more structured responses, tend to perform better, suggesting that enhancing the ability to generate structured and detailed reasoning chains can improve accuracy. 2) Abstract pattern recognition: A significant portion of errors stems from incorrect reasoning due to reliance on simpler rules. Improving models' ability to identify and apply high-level abstract paradigms is essential. 3) Visual understanding: All models exhibit poor performance on complex visual paradigms, such as logical operations and spatial relationships, indicating a need for enhanced perceptual capabilities to accurately interpret intricate visual details. 4) Explanatory vs. concise answers: The presence of detailed explanations can improve performance in stronger models but may not benefit weaker ones, highlighting the nuanced role of explanatory reasoning in model accuracy. Addressing these challenges is crucial for advancing the reasoning capabilities of LMMs.

\section{Conclusion}
We propose MM-IQ, a comprehensive benchmark for evaluating the abstract visual reasoning of LMMs. It covers a diverse range of 2,710 AVR problems across 8 distinct reasoning paradigms, enabling a rigorous assessment of LMMs' abstraction and reasoning capabilities. Experimental results reveal striking limitations in current state-of-the-art LMMs, with the leading models achieving only slightly above the accuracy of random guessing, far behind human performance. We conduct a thorough failure analysis that identifies several key points for improvement, including structured reasoning, abstract pattern recognition, visual understanding, and inference-time scaling. MM-IQ is expected to complement existing multimodal benchmarks and provide a valuable resource for steering progress in multimodal research and promoting the advancements of AGI.


% \clearpage

\bibliography{custom}


\bibliographystyle{plainnat}
\clearpage

\appendix
% \section{Appendix}

%Optionally include supplemental material (complete proofs, additional experiments and plots) in appendix.
% All such materials \textbf{SHOULD be included in the main submission.}


\end{document}