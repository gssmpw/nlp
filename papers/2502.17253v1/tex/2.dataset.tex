% 我们数据集的输入为表格、文本和相关的问题，输出为问题的回答
The input of \ourdataset consists of a question, the hybrid context including the table and text, and the output is the answer to the question. 
% 并且，我们为每个问题标注了rationale
Additionally, we annotate the rationale, which is the reasoning process of answering the question in \ourdataset. 
% 我们称每个问题，以及它对应的表格、文本、rationale和答案为一个instance
We refer to each question, along with its corresponding table, text, rationale, and answer, as an instance. 
% 我们为每个instance标注了10种非英语的多样的语言
For each instance, we annotate $11$ diverse languages.
% 首先，我们介绍我们数据集的构建过程，采用了自动翻译和人工纠错相结合的方式，following前人工作，如图所示
We first describe the construction process of \ourdataset, which combines automatic generation with manual error correction, following previous works \cite{peng-etal-2024-humanevalxl,singh-etal-2024-indicgenbench,MultiSpider}, as shown in Figure~\ref{fig:framework}.

\begin{figure*}
    \centering
    \includegraphics[width=.85\linewidth]{fig/framework.pdf}
    % \vspace{-0.5em}
    \caption{
    % 我们数据集构建的流程
    The process of constructing \ourdataset.
    % 蓝色框代表数据，白色实线框代表构造步骤
    The \colorbox{data_blue_light}{\datatext{blue}} boxes represent the data, and the white solid boxes represent the construction steps.
    }
    \label{fig:framework}
\end{figure*}

\begin{table*}[t]
    \centering
    \small
    \input{tab/answer_statistics}
    % \vspace{-0.5em}
    \caption{
    % 我们数据集中数据的分布
    The distribution of English data, including answer types and answer sources in \ourdataset, sourced from three mainstream datasets.
    % 表中列出的是每个数据集拥有的所有答案类型
    The listed answer types are the all answer types corresponding to each dataset.
    }
    \label{tab:answer_statistics}
\end{table*}


\subsection{Data Preparation}
We first collect English data from existing datasets and select languages to translate them.

\subsubsection{Source Data Collection}
% 我们分别选取了wikipedia，金融和科学领域的HybridQA, TAT-QA和SciTAT数据集作为我们的数据来源，因为这三个领域是目前TATQA任务主要分布的领域
We select HybridQA~\cite{chen-etal-2020-hybridqa}, TAT-QA~\cite{zhu-etal-2021-tat}, and SciTAT~\cite{zhang2024scitat} datasets from the Wikipedia, finance, and science domains as our data sources, as these three domains are the primary areas where TATQA tasks are currently distributed (see Table~\ref{tab:comparison_tat}). 
% 为了令我们的数据集在不同的答案来源和答案类型上均匀分布，我们按照表格中的比例从三个数据集共采样了250条数据，如表所示
To ensure an even distribution of different answer types and answer sources in \ourdataset, we sample a total of $250$ instances from the three datasets according to the proportions shown in Table~\ref{tab:answer_statistics}. 
% 其中，HybridQA数据集只采样了50条是由于其答案来源和类型较为单一
Among them, only $50$ instances are sampled from HybridQA due to its relatively limited answer sources and types. 

\subsubsection{Target Language Selection}
% 我们选取了除英语之外的10种语言覆盖8个语系，分别是Bengali (BN), Chinese (ZH), French (FR), German (DE), Japanese (JA), Russian (RU), Spanish (ES), Swahili (SW), Telugu (TE), and Thai (TH)，follow前人工作.
For \ourdataset, we select $11$ languages, covering $8$ language families: Bengali (bn), Chinese (zh), English (en), French (fr), German (de), Japanese (ja), Russian (ru), Spanish (es), Swahili (sw), Telugu (te), and Thai (th), following the previous benchmark \cite{shi2023MGSM}. 
% 并且，我们在所有语言中都保持了原数据集中答案的阿拉伯数字，以便评测（引）
Additionally, we preserve the Arabic numerals from the original datasets across all languages to facilitate evaluation \cite{shi2023MGSM}.

\subsection{Rationale Annotation}
\label{subsec:Rationale Annotation}
% \paragraph{Rationale Generation}
% 在本节，我们首先展示了如何使用LLM结合人工修改来标注rationale
We first demonstrate how to annotate English rationales by employing the large language model (LLM) in combination with manual refinement. 
% 我们首先使用gpt-4o完成英文rationale的标注，因为其强大的推理能力以及instruction-following的能力
We use \texttt{gpt-4o}~\cite{openai2024gpt4technicalreport} to complete \textbf{rationale generation} due to its strong reasoning and instruction-following capabilities. 
% 具体地，我们将问题，相关的表格和文本以及答案输入LLM，提示模型生成推理过程。
Specifically, we input the question, relevant tables and texts, and the answer into the LLM, prompting the LLM to generate the corresponding rationale.
% \paragraph{Human Refinement}
% 由于LLM不能保证推理的正确性，我们采用人工校验及修改
Since LLMs cannot guarantee the accuracy of reasoning, we employ \textbf{manual refinement}. 
% 对于模型生成的推理过程，我们提示标注者判断英语rationale的正确性，对于错误的进行修改
The annotators are instructed to evaluate the accuracy of the generated rationale and make corrections where necessary.

\subsection{Instance Translation}
\label{subsec:Instance Translation}
% 在本节，我们介绍使用LLM结合人工标注将英文instance翻译为10种语言
In this section, we describe the process of combining the LLM with human annotations to translate English instances into $10$ languages. 
% \paragraph{Machine Translation}
% 我们选择gpt-4o，因为其强大的翻译能力
For \textbf{machine translation}, we select \texttt{gpt-4o} because of its strong translation capabilities \cite{yan2024gpt4vshumantranslators,hu-etal-2024-gentranslate}. 
% 我们将instance输入LLM，分别提示模型翻译为10种目标语言
Specifically, we input each instance into the LLM, with prompts to translate it into the target languages, respectively. 
% \paragraph{Human Refinement}
% 为了评测翻译的正确性，我们follow前人工作使用gpt-4o将翻译成目标语言的instance翻译回英语，计算其和原本英语版本的F1
To assess the accuracy of the translations, we use \texttt{gpt-4o} to translate the target language instances back into English, and calculate the F1 score between the back-translated version and the original English instance following previous works \cite{peng-etal-2024-humanevalxl}. 
% 对于F1<0.6的instance，我们提示标注者使用谷歌翻译重新翻译并检查，直至回译和原始英文版本一致
For instances with an F1 score below $0.6$, we prompt annotators to complete \textbf{manual refinement} by using Google Translation for a new translation.
% and verification, iterating until the back-translated instances align with the original English version.

% \subsection{Initial Annotation}
% % 我们首先使用LLM完成英文rationale的标注，以及将英文instance翻译为10种语言
% We first employ large language models (LLMs) to annotate the English rationales and translate the English instances into ten languages. 
% % 具体地，我们使用的是gpt-4o，因为其强大的推理能力，instruction-following的能力，以及翻译能力
% Specifically, we use \texttt{gpt-4o}~\cite{openai2024gpt4technicalreport}, given its strong reasoning ability, proficiency in instruction-following, and translation capabilities.
% % 1. 对于rationale的标注，我们将问题，相关的表格和文本以及答案输入LLM，提示模型生成推理过程。
% (\emph{i})~For rationale annotation, we input the question, relevant tables and text, and the answer into the LLM, prompting the model to generate the reasoning process. 
% % 在经过人工校验及修改后（过程见S2.3），我们将每个instance翻译为目标语言。
% After manual verification and modification (as described in Section \S\ref{subsec:Human Annotation}), we translate each instance into the target languages.
% % 2. 对于instance的翻译，我们将表格，文本，问题，rationale和答案都输入LLM，分别提示模型翻译为10种目标语言
% (\emph{ii})~For instance translation, we input the tables, text, question, rationale, and answer into the LLM, prompting the model to translate them into the target languages, respectively.
% % 我们标注过程中使用的prompt在附录中提供
% % The prompts used during the annotation process are provided in Appendix.

% \subsection{Human Refinement}
% \label{subsec:Human Refinement}
% % 由于LLM不能保证推理以及翻译的正确性，我们采用人工校验及修改
% Since LLMs cannot guarantee the accuracy of reasoning and translation, we employ manual verification and correction. 
% % 对于推理过程，我们提示标注者判断英语rationale的正确性，对于错误的进行修改
% (\emph{i})~For the rationale, we instruct the annotators to assess the correctness of the English rationale, making corrections where necessary. 
% % 对于翻译过程，我们follow前人工作采用回译进行判断。
% (\emph{ii})~For the translation, we follow previous works by using back-translation for validation \cite{peng-etal-2024-humanevalxl}. 
% % 具体来说，我们使用gpt-4o将翻译成目标语言的instance翻译回英语，计算其和原本英语版本的F1
% Specifically, we utilize \texttt{gpt-4o} to translate target language instances back into English and compute the F1 score between the back-translated version and the original English version. 
% % 对于F1<0.6的instance，我们提示标注者使用谷歌翻译重新翻译并检查，直至回译和原始英文版本一致
% For instances with an F1 score below $0.6$, we instruct the annotators to use Google Translation for a new translation and review it until the back-translated version matches the original English text.

\subsection{Quality Control}
% 为了保证我们的数据集中数据的质量，我们采取了严格的质量控制策略
To ensure the quality of \ourdataset, we implement rigorous quality control strategies.
% 首先，回译是广泛被前人采用的评价翻译质量的方法。
% First, back-translation is a widely adopted method for evaluating translation quality. 
% 并且，我们的方法是结合人工校验的gpt-4o和谷歌翻译的融合学习的结果，有着优越的翻译性能
% Our translation process, which combines the results of human-validated \texttt{gpt-4o} and Google Translation through ensemble learning, demonstrates superior translation performance. 
% \paragraph{Competent Annotators}
% 我们雇佣的标注者均为研究生及以上学历，精通英语，并支付给他们每条数据$1的费用
The annotators we hire hold graduate-level degrees, are proficient in English and are compensated with $\$1$ per data instance. 
% 我们首先对标注者进行培训，使其了解标注的要求以及标注工具的使用（见附录）
We first train the annotators to familiarize them with the annotation requirements and the use of the annotation tool (see Appendix~\ref{subsec:Annotator Training Process}). 
% 然后，我们令其试标注20条数据，检查他们标注的结果并给出反馈以及修改意见
Then, they try to annotate $20$ instances, and we review their annotations, providing feedback and suggestions for revisions.

% \paragraph{Quality Inspection}
% 为了进一步确保翻译的质量，我们选择了低资源语言，包括孟加拉语、斯瓦希里语以及泰卢固语，对我们的数据集的翻译进行评测
% To further ensure the quality of the translations, we select low-resource languages, including Bengali, Swahili, and Telugu, to evaluate the translations in \ourdataset. 
% % 具体来说，我们分别请了母语为这三种语言且精通英语的标注者分别对这些语言在我们数据集中的问题，对照英文的原始问题对翻译的流畅性、充分性和一致性进行打分
% Specifically, native speakers of these languages, who are also proficient in English, are asked to rate the fluency, adequacy, and consistency of the translated questions by comparing them to the original English questions in \ourdataset. 
% % 最终平均得分分别为，证明了我们数据集翻译的质量
% The final average scores is , demonstrating the high quality of \ourdataset. 
% % 我们将具体分值放在了附录
% Detailed annotation process and scoring information are provided in Appendix.

% \begin{table}[t]
%     \centering
%     \small
%     \input{tab/basic_statistics}
%     \vspace{-0.5em}
%     \caption{
%     % 我们数据集中数据的分布
%     The basic statistics of \ourdataset.
%     }
%     \label{tab:basic_statistics}
% \end{table}

\subsection{Data Analysis}
% 我们展示了我们数据集的基本数据的统计结果，如表1和表2所示
We show the data distribution of \ourdataset in Table~\ref{tab:answer_statistics}.
% 我们数据集中的250个问题涉及233个混合上下文，其中每个混合上下文包括1个表格和平均5.3个段落。其中每个表格平均有10.2行和4.7列
The $250$ questions in \ourdataset involve $233$ hybrid contexts, each of which includes $1$ table and an average of $5.3$ paragraphs. 
Each table has an average of $10.2$ rows and $4.7$ columns.

\begin{figure*}[t]
    \centering
    \includegraphics[width=.95\linewidth]{fig/method.pdf}
    % \vspace{-0.5em}
    \caption{
    % 我们方法的示意图，我们方法分为两步：
    The overview of \ourmethod, which includes two modules:
    % 1. Linking根据问题链接到表格或文本中相关的信息
    (\emph{i})~\textbf{Linking}: Mapping the entities in the question to the relevant information in tables or text, which are marked with \datatext{\textbf{blue}} in the left part.
    % 2. Reasoning根据相关信息生成代码求解
    (\emph{ii})~\textbf{Reasoning}: Generating programs to solve the question using the information.
    % 例子和图一中的例子一致
    % The example is consistent with the Chinese one shown in Figure~\ref{fig:intro}.
    % 我们以中文TATQA的输入为例，括号中的灰色文字为对应的英文
    We take the Chinese TATQA input as an example, with the corresponding English text provided in \colorbox{gray_light}{\textgray{(gray)}}.
    % within parentheses.
    }
    \label{fig:method}
\end{figure*}

