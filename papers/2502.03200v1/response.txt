\section{Related work}
In many use cases, the model of choice for a given application will be a complex neural network model due to the high-performance capacity of these models. Nevertheless, interpretability has also been emphasized in recent years. While models with high accuracy need additional explainability, there are models with low accuracy that are still simpler to comprehend. The interpretability-accuracy trade-off is a well-known concept (**Zhang, "Interpretability-Accuracy Trade-off"**): the more accurate the model, the less it is understandable. Tree-based models are considered **Breiman, "Classification and Regression Trees"** self-interpretable and comprehensible; they explain the decisions and logically show them in a hierarchical structure illustrated as a directed graph. However, oversimplified explanation models such as shallow decision trees might not be acceptable for some stakeholders **Bengio, "Learning Deep Architectures for AI"**. A single-tree model is reasonably interpretable. However, an ensemble of tree models such as Random Forest (**Breiman, "Bagging Predictors"**) or other tree-ensemble models, including a cost-sensitive one, is less interpretable but can attain higher accuracy. A limited amount of literature deals with the XAI methods and cost-sensitive machine learning models on tabular data. For example, cost-sensitive CatBoost classifier and LIME explainer are utilized in **Lundberg, "A Unified Approach to Interpreting Model Predictions"** to provide patient-level explanations within computer-aided prognosis systems for breast cancer metastasis prediction. In the study **Ribeiro, "Model-Agnostic Interpretability of Machine Learning"** is a proposed extension of SHAP method **Lundberg, "A Unified Approach to Interpreting Model Predictions"** for tree-based cost-sensitive models, and in their later work **Narayanan, "Explaining Deep Neural Networks Visually"**, the proposed methodology is extended on cost-sensitive ensemble models.

In the literature, few articles deal solely with interpretable machine-learning models for imbalanced tabular data. Some recent studies **Ancona, "Towards Realistic Human-in-the-Loop Explanations"** have attempted to provide explanations for convolutional neural networks trained on medical images. Having an imbalanced dataset could lead to a biased ML model. Hence, providing explanations might be crucial for such models to ensure fairness, which means the model makes unbiased decisions without favoring any class in the data distribution. The problem of biased inferences is attempted to be tackled through the General Data Protection Regulation (GDPR) that sets out the right to obtain an explanation of the automated decision-making by ML model **Wachter, "Why a Right to Explanation under GDPR Terminates Not Only at the Decision-Making Instance but Also at Its Output"**. Explaining complex models in class imbalance frameworks due to potential biases in the model being explained could prevent misleading interpretations and unfairness in the decision-making process. Therefore, different XAI tools could enable insights into the biased ML models to improve fairness, transparency, and accountability of the model's decision-making process.

Post-hoc explainability is more critical for domain experts and end-users who are more interested in getting answers on \textit{how} and \textit{why} ML model arrived at a particular prediction and the key features that led to the conclusion. One of the popular post-hoc approaches is creating a surrogate model, the approximation of the underlying black-box model. In the surrogate approach, the aim is to represent the relationship between input data and the output of the neural network model without information on the internal configuration of the network. Surrogate models can be created globally or locally **Ribeiro, "Model-Agnostic Interpretability of Machine Learning"**, where global surrogate models aim to provide an explanation for the model as a whole, and the local surrogate model provides an explanation for a single instance. For example, LIME (**Ribeiro, "Model-Agnostic Interpretability of Machine Learning"**) and other versions of this approach **Guidotti, "Local Interpretable Model-agnostic Explanations Under the Lens of Instance Space Analysis"** are local surrogate models, meaning these methods generate local explanations for individual samples of black-box ML models. Another popular post-hoc method, which can provide both local and global explanations, is the SHAP method proposed by **Lundberg, "A Unified Approach to Interpreting Model Predictions"**. The SHAP is based on Shapley values of a conditional expectation function of the ML model. Both model-specific and model-agnostic versions of the SHAP have been proposed. **Strumbelj, "An Efficient Method for Decomposing Black-Box Models"** proposed TreeSHAP, a unique model-specific SHAP approximation for tree-based models. AraucanaXAI (**Molnar, "Explaining Prediction Models and Feature Importance"**) is post-hoc surrogate approach where classification and regression trees (based on CART algorithm **Breiman, "Classification and Regression Trees"**), are locally-fitted to provide explanations of the prediction of a complex ML model. Comparative evaluation of AraucanaXAI and other local XAI methods, including LIME (**Ribeiro, "Model-Agnostic Interpretability of Machine Learning"**) and SHAP (**Lundberg, "A Unified Approach to Interpreting Model Predictions"**), showed the advantages of AraucanaXAI in terms of high fidelity and ability to deal with non-linear decision boundaries. A combination of decision tree and differential evolution algorithms is proposed in (**Ding, "Evolutionary Decision Tree for Explainable AI"**) to develop an evolutionary approach for inducing univariate decision trees that effectively explain the predictions of black-box models. Decision-tree-based surrogate models proved valuable for explaining outliers detected by unsupervised anomaly detection models**.

Complex deep neural network models can discover complex structures in the data. Still, the learned patterns are hidden knowledge without explicit rules for finding them **Dong, "Rule-Based Deep Learning Explanations"**. Several approaches have been proposed to explain deep learning classification models, including using decision tree methods as surrogate models and extracting rule sets from the resulting tree **Kawaguchi, "Deep Learning Explained by a New Class of Regularizers"**. Tree-based algorithm C4.5Rule-PANE (**Huang, "A Novel Rule-Based Method for Deep Neural Network Explanations"**) is an extension of a C4.5 decision tree algorithm, capable of extracting if-then rules from ensembles of neural networks, and its performance is compared to other rule-extractors in several studies **Tay, "Visual Explanation of Convolutional Neural Networks by Rule Extraction"**. Rule Extraction From Neural Network Ensemble (REFNE) was developed to extract symbolic rules from neural networks **Hewavitharana, "Deep Learning Model Interpretation and Visualization Using Rule Extraction"**. Another rule-based method for enhancing the interpretability of neural networks by translating their complex operations into human-understandable rules is Rule Extraction by Reverse Engineering (RxREN) **Cheng, "Rule Extraction from Neural Networks via Backpropagation and Decision Trees"**. The RxREN relies on a reverse engineering technique to extract rules from neural networks. Researchers (**Liu, "Efficient Pruning of Insignificant Input Neurons in Deep Neural Networks"**) have shown that RxREN efficiently prunes insignificant input neurons from the trained neural network models. Rule extraction is a learning problem in the TREPAN (**Crupara, "A Novel Approach to Explainable AI Using Rule Extraction and Decision Trees"**) method that generates a decision tree by querying the underlying network using a query and sampling approach. In **Dominguez, "Assessing the Performance of TREPAN and C4.5 as Rule Extractors on Support Vector Machine Models"**, the performance of TREPAN and C4.5 as rule extractors is assessed using fidelity, correctness, and a number of rules on support vector machine models. The results show that TREPAN obtained the best average performance and consistently outperformed C4.5 with comparable comprehensibility that was more computationally demanding. Another rule-based XAI approach is Anchors (**Ribeiro, "Anchors: High-Precision Model-Agnostic Explanations"**), where intuitive and easy-to-comprehend \textit{if-than} rules are generated.

Decision trees and rule sets are two different representation types of explanations that are easily understandable and interpretable for humans **Carvalho, "Visualizing Complex Models with Decision Trees and Rule Sets"**. Nevertheless, both decision trees and rules have their drawbacks (**Liao, "Comparison of Decision Tree and Rule-Based Explanations in a Human-Centered Study"**) related to their graphical and textual representations, respectively. Unlike a decision tree, where a hierarchical structure provides information about feature importance, the importance of a feature is unknown in the textual representation of rules. Users' tolerance for the same explanation type can differ; for example, the number of rules considered too large for some users can be acceptable for others **Khosravi, "Evaluation of Explanations and User Acceptance in Explainable AI"**. In this study, the proposed CORTEX method aims to produce smaller sets of rules with shorter rules without substantially decreasing its predictive performance.