\section{Related work}
In many use cases, the model of choice for a given application will be a complex neural network model due to the high-performance capacity of these models. Nevertheless, interpretability has also been emphasized in recent years. While models with high accuracy need additional explainability, there are models with low accuracy that are still simpler to comprehend. The interpretability-accuracy trade-off is a well-known concept (\cite{Rivera2024, ribeiro2016, darpa, RizzoL18Explainability}): the more accurate the model, the less it is understandable. Tree-based models are considered \cite{ali} self-interpretable and comprehensible; they explain the decisions and logically show them in a hierarchical structure illustrated as a directed graph. However, oversimplified explanation models such as shallow decision trees might not be acceptable for some stakeholders \citep{freitas}. A single-tree model is reasonably interpretable. However, an ensemble of tree models such as Random Forest \citep{RF} or other tree-ensemble models, including a cost-sensitive one, is less interpretable but can attain higher accuracy. A limited amount of literature deals with the XAI methods and cost-sensitive machine learning models on tabular data. For example, cost-sensitive CatBoost classifier and LIME explainer are utilized in \citep{Maouche2023} to provide patient-level explanations within computer-aided prognosis systems for breast cancer metastasis prediction. In the study \citep{kopanja2023} is a proposed extension of SHAP method \citep{shap} for tree-based cost-sensitive models, and in their later work \citep{kopanja} the proposed methodology is extended on cost-sensitive ensemble models. 

In the literature, few articles deal solely with interpretable machine-learning models for imbalanced tabular data. Some recent studies \citep{Mustari2023, Dablain2023} have attempted to provide explanations for convolutional neural networks trained on medical images. Having an imbalanced dataset could lead to a biased ML model. Hence, providing explanations might be crucial for such models to ensure fairness, which means the model makes unbiased decisions without favoring any class in the data distribution. The problem of biased inferences is attempted to be tackled through the General Data Protection Regulation (GDPR) that sets out the right to obtain an explanation of the automated decision-making by ML model \citep{vilone_review}. Explaining complex models in class imbalance frameworks due to potential biases in the model being explained could prevent misleading interpretations and unfairness in the decision-making process. Therefore, different XAI tools could enable insights into the biased ML models to improve fairness, transparency, and accountability of the model's decision-making process.

Post-hoc explainability is more critical for domain experts and end-users who are more interested in getting answers on \textit{how} and \textit{why} ML model arrived at a particular prediction and the key features that led to the conclusion. One of the popular post-hoc approaches is creating a surrogate model, the approximation of the underlying black-box model. In the surrogate approach, the aim is to represent the relationship between input data and the output of the neural network model without information on the internal configuration of the network. Surrogate models can be created globally or locally \citep{ali}, where global surrogate models aim to provide an explanation for the model as a whole, and the local surrogate model provides an explanation for a single instance. For example, LIME \citep{lime} and other versions of this approach \citep{alime, tree_alime} are local surrogate models, meaning these methods generate local explanations for individual samples of black-box ML models. Another popular post-hoc method, which can provide both local and global explanations, is the SHAP method proposed by \citet{shap}. The SHAP is based on Shapley values of a conditional expectation function of the ML model. Both model-specific and model-agnostic versions of the SHAP have been proposed. \citet{TreeSHAP} proposed TreeSHAP, a unique model-specific SHAP approximation for tree-based models. AraucanaXAI \citep{Araucana} is post-hoc surrogate approach where classification and regression trees (based on CART algorithm \citep{CART}), are locally-fitted to provide explanations of the prediction of a complex ML model. Comparative evaluation of AraucanaXAI and other local XAI methods, including LIME and SHAP, showed the advantages of AraucanaXAI in terms of high fidelity and ability to deal with non-linear decision boundaries. A combination of decision tree and differential evolution algorithms is proposed in (\citep{Rivera2024}) to develop an evolutionary approach for inducing univariate decision trees that effectively explain the predictions of black-box models. Decision-tree-based surrogate models proved valuable for explaining outliers detected by unsupervised anomaly detection models~\citep{Savic2022}.

Complex deep neural network models can discover complex structures in the data. Still, the learned patterns are hidden knowledge without explicit rules for finding them \cite{ali}. Several approaches have been proposed to explain deep learning classification models, including using decision tree methods as surrogate models and extracting rule sets from the resulting tree \citep{DT_surrogate}. Tree-based algorithm C4.5Rule-PANE \citep{c45} is an extension of a C4.5 decision tree algorithm, capable of extracting if-then rules from ensembles of neural networks, and its performance is compared to other rule-extractors in several studies \citep{giulia_2020, giulia_2021}. Rule Extraction From Neural Network Ensemble (REFNE) was developed to extract symbolic rules from neural networks \citep{refne}. Another rule-based method for enhancing the interpretability of neural networks by translating their complex operations into human-understandable rules is Rule Extraction by Reverse Engineering (RxREN) \citep{rxren}. The RxREN relies on a reverse engineering technique to extract rules from neural networks. Researchers \citep{rxren} have shown that RxREN efficiently prunes insignificant input neurons from the trained neural network models. Rule extraction is a learning problem in the TREPAN \citep{trepan} method that generates a decision tree by querying the underlying network using a query and sampling approach. In \citep{svm_rules}, the performance of TREPAN and C4.5 as rule extractors is assessed using fidelity, correctness, and a number of rules on support vector machine models. The results show that TREPAN obtained the best average performance and consistently outperformed C4.5 with comparable comprehensibility that was more computationally demanding. Another rule-based XAI approach is Anchors \citep{Anchors}, where intuitive and easy-to-comprehend \textit{if-than} rules are generated. 

Decision trees and rule sets are two different representation types of explanations that are easily understandable and interpretable for humans \citep{Guidotti2019}. Nevertheless, both decision trees and rules have their drawbacks \citep{ribeiro2016} related to their graphical and textual representations, respectively. Unlike a decision tree, where a hierarchical structure provides information about feature importance, the importance of a feature is unknown in the textual representation of rules. Users' tolerance for the same explanation type can differ; for example, the number of rules considered too large for some users can be acceptable for others \citep{freitas}. However, some stakeholders might prefer rules as an explanation type instead of the decision tree, and others, depending on their background, can favor decision trees as more comprehensible \citep{ribeiro2016}. Additional experimental studies should be conducted to compare user preferences for various representations of explanations, as none can be considered the best for all applications.

With many XAI methods available, the evaluation process of generated explanations enables practical assessment based on criteria such as transparency, fidelity, robustness, and usability. Different metrics measure different aspects of explanations depending on the type of explanation. For example, suppose explanations are generated in the form of rules. In that case, evaluation metrics such as a number of rules or an average number of antecedents in the rule can be used \citep{giulia_2020}. In the study by \citet{ribeiro2016}, it is argued that average rule length is a fair measure of the simplicity of a rule set. The small input changes should not significantly affect the AI system's behavior. Hence, robustness represents one of the critical measures of the XAI method, where robustness refers to the sensitivity of the AI-driven model output to a change in the input. Beyond these metrics, other quantitative validation factors must be fulfilled by every type of explanation automatically generated by an XAI method \citep{giulia_2021}, including the correctness measured as the portion of samples correctly classified by a given XAI method. 

The effectiveness of explanations generated by some XAI methods must be assessed according to how the explanations aid human users. Therefore, there is an implicit requirement for human-in-the-loop evaluation of AI-driven system reasoning.
Apart from quantitative comprehensive assessment, the AI systems can also be validated by users \citep{ali}. Taxonomy for evaluation of XAI methods is divided into computer-centred and human-centred by \citet{Lopes_2022}, where the former involves methods to obtain a measure of interpretability and fidelity to evaluate the quality of explanations. In contrast, the letter consists of conducting user experiments with human subjects. Human-centred evaluation of explanations in the form of if-than rules is undertaken in the study \citep{Huysmans_2011}. In a more recent study, \citet{dragoni_2020} created a questionnaire to collect feedback from participants on the persuasiveness of automatically generated explanations. Similarly, \citet{giulia_test} have developed and evaluated a novel questionnaire designed to assess the explanations generated by XAI methods reliably. The questionnaire is based on close-ended questions for testing rule-based explanations and tested on argument-based and decision-tree explanations of deep neural networks trained on three datasets over six groups of human participants. Another human-centred study \citep{Anchors} showed that XAI methods could enable users' understanding of the model’s behavior measured by users predicting how a model would behave on unseen samples with less effort and higher precision.

While significant progress has been made within the XAI field in developing new methods and evaluation frameworks, many challenges remain, particularly in existing post-hoc model-agnostic XAI approaches. Surrogate models created by tree-based and rule-based methods can have good predictive capacity at the expense of extensive and, therefore, ineffective rule sets. In this study, the proposed CORTEX method aims to produce smaller sets of rules with shorter rules without substantially decreasing its predictive performance. In the following section, the CORTEX method is briefly described.