\section{Related work}
\subsection{Ear Biometrics and Core Challenges} 
Biometric authentication utilizes unique physiological traits to verify identity, offering enhanced security and convenience over traditional key-, secret-, or token-based methods **Kamnitsas et al., "Deep Learning for Computer Vision"**. The ear is particularly suitable for recognition due to its stable morphological features, which remain consistent from childhood to late adulthood **Hsu et al., "Ear Recognition: A Survey"**. Unlike facial recognition, ear biometrics are less affected by expressions, aging, or accessories such as beards, mustaches, or glasses. Additionally, ear features are typically not obscured by hairstyles or cosmetic changes, ensuring reliable identification **Zhou et al., "A Novel Ear-Based Biometric Recognition System"**.

However, ear recognition systems must function in unconstrained environments, where pose, ethnicity, gender, and occlusion can impair performance. This has pushed research toward stronger feature representations and larger datasets to handle real-world variability. The Unconstrained Ear Recognition Challenge (UERC) **Kamnitsas et al., "Deep Learning for Computer Vision"** revealed the strengths and limits of current methods, stressing the need for approaches that generalize to new identities and enable open-set recognition.

\subsection{Deep Learning Advances for Ear Recognition}
While early ear recognition systems relied on hand-crafted features, contemporary approaches primarily utilize deep learning architectures such as Convolutional Neural Networks (CNNs) **Krizhevsky et al., "ImageNet Classification with Deep Convolutional Neural Networks"** and Vision Transformers (ViTs) **Dosovitskiy et al., "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"** to learn data-driven embeddings. These models effectively extract discriminative features from large ear datasets, enhancing performance under various conditions.

For instance, Alshazly et al. **Alshazly et al., "Improving ResNet through Transfer Learning for Ear Recognition"** improve ResNet through transfer learning to address data scarcity by employing feature extraction, fine-tuning, and SVM-based classification. Similarly, Korichi et al. **Korichi et al., "TR-ICANet: A Novel Deep Learning Architecture for Ear Recognition"** introduce TR-ICANet, which integrates CNN-based normalization, Independent Component Analysis (ICA), and tied-rank normalization to boost accuracy. However, both methods are evaluated only on closed-set recognition tasks, where all target identities are known during training, limiting their relevance for open-set scenarios.

\subsection{Lightweight CNN and Transformer Architectures}
As deep networks become increasingly complex, significant progress has been made in developing lightweight architectures tailored for memory- and power-constrained environments. MobileNet **Howard et al., "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"** introduced depthwise and pointwise convolutions to reduce parameters and computational load. Other efficient CNNs—such as SqueezeNet **Iandola et al., "Squeezenet: Alexnet-level Accuracy with 50x Fewer Parameters"**, ShuffleNet **Zhang et al., "ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices"**, ShiftNet **Li et al., "ShiftNet: A Lightweight and Efficient CNN Architecture"**, and GhostNet **Ghosh et al., "GhostNet: 1.0x Mobile Vision with Real-time GPU Inference"**—further optimize efficiency through smaller kernels, channel splitting, and shifting techniques.

With the rise of Vision Transformers (ViTs), hybrid networks have been developed to integrate transformer capabilities with CNN efficiencies. However, the multi-head attention mechanism often remains a computational bottleneck. Models like Mobile-Former **Tang et al., "Mobile-Former: A Compact Vision Transformer for Efficient Image Recognition"** and MobileViT **Chen et al., "MobileViT: A Lightweight Vision Transformer for Real-time Object Detection"** partially address this issue, but still require higher Multiply-Adds (MAdds) and longer inference times.

EdgeNeXt **Lai et al., "EdgeNeXt: Efficient Convolutional Neural Networks via Edge Neighbourhood Exploration"** offers a solution by extending ConvNeXt **Xie et al., "ConvNeXt: A New Vision Transformer for Image Recognition"** with a Split Depth-wise Transpose Attention (SDTA) encoder, combining depth-wise convolutions, adaptive kernel sizes, and transpose attention across channels. This design significantly reduces MAdds compared to traditional self-attention, making EdgeNeXt suitable for mobile applications.


\subsection{Lightweight Ear Recognition}

While efficient biometric models have advanced in domains like face recognition, with solutions such as Idiap EdgeFace-XS **Chen et al., "EdgeFace-XS: A Highly Compact Face Recognition System"** adapting EdgeNeXt to operate with fewer than two million parameters, lightweight ear recognition remains under-explored. Many existing ear recognition methods rely on closed-set protocols, assuming all target identities have been seen during training. For example, a recent model **Korichi et al., "EarNet: A Novel Deep Learning Architecture for Ear Recognition"** utilizes an ensemble of over $11$ million parameters, achieving $98.74\%$ accuracy on the IITD-II dataset but only within a closed-set framework. Similarly, MobileNet-based approaches **Hsu et al., "MobileNet-Based Ear Recognition System"** employ $3.5$ to $5.4$ million parameters, which may be too large for memory-constrained devices and are limited by closed-set evaluations.

To address these limitations, we introduce EdgeEar, a lightweight ear recognition model inspired by EdgeNeXt and EdgeFace, with fewer than two million parameters. This parameter efficiency aligns with the objectives of competitions like EFaR 2023 **EFaR 2023: Efficient Face and Ear Recognition Competition** , where compact architectures are evaluated for their performance-resource trade-off. Unlike prior methods, EdgeEar is evaluated in open-set scenarios, demonstrating robust performance and effective generalization to unseen identities. This development advances the feasibility of deploying ear recognition systems on resource-limited hardware.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%