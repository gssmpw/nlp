\section{Related work}
\subsection{Ear Biometrics and Core Challenges} Biometric authentication utilizes unique physiological traits to verify identity, offering enhanced security and convenience over traditional key-, secret-, or token-based methods~\cite{Dargan_biometrics_survey}. The ear is particularly suitable for recognition due to its stable morphological features, which remain consistent from childhood to late adulthood~\cite{Ibrahim_effect_ears_age}. Unlike facial recognition, ear biometrics are less affected by expressions, aging, or accessories such as beards, mustaches, or glasses. Additionally, ear features are typically not obscured by hairstyles or cosmetic changes, ensuring reliable identification~\cite{Benzaoui_survey_earrec}.

However, ear recognition systems must function in unconstrained environments, where pose, ethnicity, gender, and occlusion can impair performance. This has pushed research toward stronger feature representations and larger datasets to handle real-world variability. The Unconstrained Ear Recognition Challenge (UERC)~\cite{uerc2023,uerc2019,uerc2017} revealed the strengths and limits of current methods, stressing the need for approaches that generalize to new identities and enable open-set recognition.

\subsection{Deep Learning Advances for Ear Recognition}
While early ear recognition systems relied on hand-crafted features, contemporary approaches primarily utilize deep learning architectures such as Convolutional Neural Networks (CNNs)~\cite{Emersic2019} and Vision Transformers (ViTs)~\cite{alexey_VIT} to learn data-driven embeddings~\cite{Ziga_ear_recog_review}. These models effectively extract discriminative features from large ear datasets, enhancing performance under various conditions.

For instance, Alshazly et al.~\cite{Alshazly_Ear_Resnet} improve ResNet through transfer learning to address data scarcity by employing feature extraction, fine-tuning, and SVM-based classification. Similarly, Korichi et al.~\cite{Korichi_TRICA} introduce TR-ICANet, which integrates CNN-based normalization, Independent Component Analysis (ICA), and tied-rank normalization to boost accuracy. However, both methods are evaluated only on closed-set recognition tasks, where all target identities are known during training, limiting their relevance for open-set scenarios.

\subsection{Lightweight CNN and Transformer Architectures}
As deep networks become increasingly complex, significant progress has been made in developing lightweight architectures tailored for memory- and power-constrained environments. MobileNet~\cite{Howard_mobilenet, Sandler_mobilenetv2} introduced depthwise and pointwise convolutions to reduce parameters and computational load. Other efficient CNNs—such as SqueezeNet~\cite{Iandola2_squeezenet}, ShuffleNet~\cite{Zhang_shufflenet}, ShiftNet~\cite{Wu_shift}, and GhostNet~\cite{Han_ghostnet}—further optimize efficiency through smaller kernels, channel splitting, and shifting techniques.

With the rise of Vision Transformers (ViTs), hybrid networks have been developed to integrate transformer capabilities with CNN efficiencies. However, the multi-head attention mechanism often remains a computational bottleneck. Models like Mobile-Former~\cite{Chen_mobileformer} and MobileViT~\cite{Mehta_mobilevit} partially address this issue, but still require higher Multiply-Adds (MAdds) and longer inference times.

EdgeNeXt~\cite{Maaz_edgenext} offers a solution by extending ConvNeXt~\cite{liu_convnext} with a Split Depth-wise Transpose Attention (SDTA) encoder, combining depth-wise convolutions, adaptive kernel sizes, and transpose attention across channels. This design significantly reduces MAdds compared to traditional self-attention, making EdgeNeXt suitable for mobile applications.


\subsection{Lightweight Ear Recognition}

While efficient biometric models have advanced in domains like face recognition, with solutions such as Idiap EdgeFace-XS~\cite{George_IEEETBIOM_2024} adapting EdgeNeXt to operate with fewer than two million parameters, lightweight ear recognition remains under-explored. Many existing ear recognition methods rely on closed-set protocols, assuming all target identities have been seen during training. For example, a recent model~\cite{mehta2024efficient} utilizes an ensemble of over $11$ million parameters, achieving $98.74\%$ accuracy on the IITD-II dataset but only within a closed-set framework. Similarly, MobileNet-based approaches~\cite{xu2022efficient} employ $3.5$ to $5.4$ million parameters, which may be too large for memory-constrained devices and are limited by closed-set evaluations.

To address these limitations, we introduce EdgeEar, a lightweight ear recognition model inspired by EdgeNeXt and EdgeFace, with fewer than two million parameters. This parameter efficiency aligns with the objectives of competitions like EFaR 2023~\cite{Kolf2023EFaR}, where compact architectures are evaluated for their performance-resource trade-off. Unlike prior methods, EdgeEar is evaluated in open-set scenarios, demonstrating robust performance and effective generalization to unseen identities. This development advances the feasibility of deploying ear recognition systems on resource-limited hardware.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%