% CVPR 2025 Paper Template; see https://github.com/cvpr-org/author-kit

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage{cvpr}              % To produce the CAMERA-READY version
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Import additional packages in the preamble file, before hyperref
\input{preamble}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, 
% e.g. with the file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete *.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}
\usepackage{graphicx} % Required for inserting images
\usepackage{multicol}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}
\usepackage[capitalize]{cleveref}
\usepackage{fnpct}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\input{math_commands}
%% dummy
% \usepackage{lipsum}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{12677} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2025}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Enhancing Quantum-ready QUBO-based Suppression for Object Detection with Appearance and Confidence Features}

%%%%%%%%% AUTHORS - PLEASE UPDATE
\author{
Keiichiro Yamamura$^1$\thanks{Corresponding author.} 
% Institute of Integrated Research, Institute of Science Tokyo\\
% {\tt\small yamamura.k@first.iir.ac.jp}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Toru Mitsutake$^1$
% Institute of Integrated Research, Institute of Science Tokyo\\
% {\tt\small }
\and
Hiroki Ishikura$^1$
% Institute of Integrated Research, Institute of Science Tokyo\\
% {\tt\small }
\and
Katsuki Fujisawa$^1$
% {\tt\small }
\and
Daiki Kusuhara$^2$
% Kyushu University
% {\tt\small }
\and
Akihiro Yoshida$^2$
% {\tt\small }
\and
$^1$ Institute of Integrated Research, Institute of Science Tokyo
\and
$^2$ Kyushu University
}

\begin{document}
\maketitle
% Sample paper
% \input{_sec/0_abstract}
% \input{_sec/1_intro}
% \input{_sec/2_formatting}
% \input{_sec/3_finalcopy}
% \input{_sec/X_suppl}
\input{sec/abstract}
\section{Introduction}
\input{sec/introduction}
\section{Preliminaries}
\input{sec/preliminaries.tex}
\section{Method}
\input{sec/method}
\section{Experiments}
\input{sec/experiments/setting}
\input{sec/experiments/classical_computer}
\input{sec/experiments/qaqs}
\input{sec/experiments/ssim}
\section{Discussion}
\input{sec/discussion}
\section{Related Work}
% \input{sec/related_work}
\input{sec/related_work}
\section{Conclusion}
\input{sec/conclusion}

\appendix
% \onecolumn
% \input{sec/appendix/appendix}
\section*{Appendix}

\section{Example usage of our software with quantum circuits}
\label{appendix:software}
% 移植可能性を高めるため、私たちは提案手法をモジュラー化されたソフトウェアとして実装した。
% \Cref{fig:example}に最小限のサンプルコードを記載する。私たちの実装は、QAQS以外にもQAQS, QAQS-Cにも対応している。
% QUBO-based suppressionは、\cref{fig:example}に示すようにインスタンス化する必要がある。
% インスタンスの初期化時に、solver引数を与える。対応しているソルバーは、分枝限定法に基づく古典ソルバーであるGurobi Optimizer (\cref{fig:gurobi})と、量子回路シミュレータのqiskitを用いて実装されたQAOAアルゴリズム (\cref{fig:qiskit})である。
\begin{figure}[H]
    \centering
    % \includegraphics[width=0.8\linewidth]{fig/miscellaneous/sample.pdf}
    \begin{subfigure}[b]{\linewidth}
         \centering
         \includegraphics[width=\linewidth]{fig/gurobi.pdf}
         \caption{Run QAQS with Gurobi Optimizer.}\label{fig:gurobi}
     \end{subfigure}
    \begin{subfigure}[b]{\linewidth}
         \centering
         \includegraphics[width=\linewidth]{fig/qiskit.pdf}
         \caption{Run QAQS with QAOA, which depends on the quantum circuit.}\label{fig:qiskit}
     \end{subfigure}
    \caption{Example usage of our software.}
    \label{fig:example}
\end{figure}
We implement our proposed methods as modularized software for portability.
A minimal sample codes are shown in \cref{fig:example}. Our implementation supports not only QAQS but also QSQS and QAQS-C.
QUBO-based suppression must be instantiated as shown in \cref{fig:example}.
The solver argument should be passed to initialize the instance. The supported solvers are the Gurobi Optimizer (\cref{fig:gurobi}) and the QAOA (\cref{fig:qiskit}). We implement QAOA using qiskit~\cite{qiskit2024}.
% \Cref{fig:qiskit-solver_impl}にqiskitを用いたQAOAアルゴリズムによってQUBOを解くためのクラスの実装を示す。赤い点線の矩形で囲まれた部分のSamplerによって、量子回路を実行するハードウェアが指定される。この実装ではGPU上で動作する量子回路シミュレータを採用しているが、IBMが公開しているQPUをバックエンドに指定することもできる。\footnote{https://docs.quantum.ibm.com/}
Qiskit enables us to switch types of quantum circuits from simulators to actual QPUs.
The class implementation for solving QUBO by the QAOA using qiskit is shown in \cref{fig:qiskit-solver_impl}. The backend of the quantum circuit is specified as the \textit{Sampler} in the area enclosed by the red dotted rectangle. This implementation uses a quantum circuit simulator running on a GPU, but a QPU published by IBM can also be specified as the backend.\footnote{https://docs.quantum.ibm.com/}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{fig/qiskit_solver_impl.pdf}
    \caption{Implementation of QiskitSolver used to solve QUBO using QAOA algorithm.}
    \label{fig:qiskit-solver_impl}
\end{figure}

\section{Experiments including soft-scoring after solving QUBO}
\label{appendix:exp_original_impl}
% 本節では、\cref{tab:exp_setting}に示す、本文とは異なる3つの設定における実験結果を示す。
% QSQSの実行プロセスは、\cref{algo:qsqs}の入力であるpredictionsに対する前処理 (pre-processing), QUBOを解く, Soft-NMSの三段階に分解できる。
% \Cref{tab:exp_setting}は、pre-processingの種類とSoft-NMSの有無を実験番号と紐つけたものである。
% ここでいうSoft-NMSは、\cref{algo:qsqs}の青でハイライトされた部分の処理を指す。
% 実験1, 3では、\citet{li2020qsqs}に倣い、NMSのIoU閾値を$0.5$, Soft-NMSのハイパーパラメータを$\sigma=0.5, O_t=0.01$で実験を行った。
% 実験番号1, 2, 3に対応する実験結果は、それぞれ\cref{tab:original_qsqs_v1,tab:original_qsqs_v2,tab:original_qsqs_v3}に記載されている。
% 本実験の主要な結果は以下のように要約される。
In this section, we present experimental results in three different settings from the main text, shown in \cref{tab:exp_setting}.
The QSQS procedure can be decomposed into three stages: pre-processing of predictions, solving QUBO, and Soft-NMS.
\Cref{tab:exp_setting} associates the type of pre-processing and the presence or absence of Soft-NMS with the experiment number (No. column in the table).
Here, Soft-NMS refers to the processing in the blue-highlighted part of the \cref{algo:qsqs}.
For these experiments, we use the same datasets, models, and computer specifications. The hyperparameters of QUBO-based suppressions are also the same.
In experiments No. 1 and 3, we use the same parameters used by~\citet{li2020qsqs}. The IoU threshold of NMS is $0.5$, and the hyperparameters of Soft-NMS are $\sigma=0.5, O_t=0.01$.
The results of the experiments corresponding to experiments No. 1, 2, and 3 can be found in~\cref{tab:original_qsqs_v1,tab:original_qsqs_v2,tab:original_qsqs_v3}, respectively.
The main results of this experiment are summarized as follows.

% \begin{enumerate}
%     \item スコア閾値0.25で前処理→QUBO→スコア閾値0.01のSoftNMS→評価 \cref{tab:original_qsqs_v1}
%     \item 閾値0.5のNMS→QUBO→評価 \cref{tab:original_qsqs_v2}
%     \item 閾値0.5のNMS→QUBO→Soft-NMS→評価  \cref{tab:original_qsqs_v3}
% \end{enumerate}
\begin{itemize}
    % \item どの前処理手法を採用した場合でも、QAQS-CはSoft-NMSの有無に対して頑健である一方、QSQSはSoft-NMSの有無による影響を大きく受ける（main text記載の実験結果 vs 実験番号1, 実験番号2 vs 3の比較に基づく）
    % \item 前処理手法がconfidence scoreに基づく場合、Soft-NMSの追加によって異なるQUBO定式化間の優劣がわからなくなる (main text記載の実験結果 vs 実験番号1)
    % \item NMSに基づく前処理を適用した場合、Soft-NMSの有無によらず、QAQS-CはQSQSよりも高い性能を示す傾向にある。 (実験番号2 vs 3)
    % \item NMSに基づく前処理を適用し、Soft-NMSを行わない場合、QAQSはQSQSよりも極めて高い性能を示す。NMSに基づく前処理を適用し、かつSoft-NMSを行う場合、QAQSはQSQSとcompetitiveであるが、僅かにmARを改善する。(実験番号2 vs 3)
    \item Regardless of the type of preprocessing before solving QUBO, QAQS-C is robust to the presence or absence of Soft-NMS, while QSQS is significantly affected by the presence or absence of Soft-NMS. (The experiments in the main text and experiment No. 1, experiments No. 2 and 3)
     \item With the confidence score-based preprocessing before solving QUBO, Soft-NMS obscures the superiority between the different QUBO formulations. (The experiments in the main text and experiment No. 1)
    \item With NMS-based preprocessing before solving QUBO, QAQS-C tends to perform better than QSQS, regardless of the presence or absence of Soft-NMS after solving QUBO. (Experiments No. 2 and 3)
    \item With NMS-based preprocessing before solving QUBO and without Soft-NMS after QUBO, QAQS performs significantly better than QSQS. (Experiment No. 2)
    \item  With NMS-based preprocessing before solving QUBO and Soft-NMS after QUBO, QAQS is competitive with QSQS, but slightly improves mAR. (Experiment No. 3)
\end{itemize}
While the performance of QSQS relies heavily on Soft-NMS, QAQS-C can achieve high performance without Soft-NMS. This supports the superiority of our new QUBO formulations.
% QSQSは性能がSoft-NMSに大きく依存している一方、QAQS-CはQUBOの解をそのまま利用するだけで高い性能を得ることができる。これは私たちの新しいQUBO定式化の優位性を支持する。
\begin{table}[]
    \centering
    \small
    \begin{tabular}{c ccc}
        \toprule
        No. & Before solving QUBO & After solving QUBO \\
        \midrule
        1. & Confidence score $\geq 0.25$ & Soft-NMS \\
        2. & NMS (IoU threshold $= 0.5$)  & N/A \\
        3. & NMS (IoU threshold $= 0.5$)  & Soft-NMS \\
        \bottomrule
    \end{tabular}
    \caption{Experimental settings.}
    \label{tab:exp_setting}
\end{table}
\begin{algorithm}
\centering
    \caption{QSQS-based suppression}
    \label{algo:qsqs}
    \begin{algorithmic}
        \Require{Predictions: $B\gets\{\bm{b}_1,\bm{b_2},\ldots,\bm{b}_n\}$, Confidence scores: $V\gets\{v_1,v_2,\ldots,v_n\}$, Image: $X\in\R^{C\times W\times H}$ \textcolor{blue}{Hyperparameters of soft-scoring: $O_t$ and $\sigma$}}
        \Ensure{Suppressed predictions: $D\subset B$}
        \State{Prepare $Q\in\R^{n\times n}$ using $B, V$, and $X$.}
        \State{$\displaystyle \bm{x}^* \gets\underset{\bm{x}\in\{0,1\}^{n\times 1}}{\mathrm{argmin}}\bm{x}^\top Q \bm{x}$.}\Comment{Solve QUBO}
        \State{$B_{kept}\gets\{\bm{b}_i\in B\mid \bm{x}^{*}_i = 1\}$}
        \State{$B_{soft}\gets\{\bm{b}_i\in B\mid \bm{x}^{*}_i = 0\}$}
        \State{$D\gets D\cup B_{kept}$}
        \textcolor{blue}{
        \For{$\bm{b}_i\in B_{soft}$}
            \State{$\displaystyle \bm{b}_{m}\gets\underset{\bm{b}_m\in B_{kept}}{\mathrm{argmax}}IoU(\bm{b}_i, \bm{b}_m)$}
            \State{$v_i\gets v_i \exp\left(-\frac{IoU(\bm{b}_i, \bm{b}_m)^2}{\sigma}\right)$}
            \If{$v_i\geq O_t$}
                \State{$D\gets D\cup \{\bm{b}_{i}\}$}
            \EndIf
        \EndFor
        }
    \end{algorithmic}
\end{algorithm}
\setlength\tabcolsep{0.75mm} 
\begin{table}[tbh]
    \centering
    \small
    \begin{tabular}{l ccc ccc}
    \toprule
     & \multicolumn{3}{c}{COCO 2017} & \multicolumn{3}{c}{CrowdHuman} \\
        \cmidrule(lr){1-4}
        \cmidrule(lr){5-7}
        Method    & QSQS  & QAQS  & QAQS-C & QSQS  & QAQS  & QAQS-C  \\
        \cmidrule(lr){1-4}
        \cmidrule(lr){5-7}
        mAP       & 35.36 & 35.36 & 35.36  & 35.77 & 35.77 & 35.77  \\
        mAP@50    & 55.51 & 55.51 & 55.51  & 62.44 & 62.44 & 62.44  \\
        mAP@75    & 38.24 & 38.24 & 38.24  & 36.14 & 36.14 & 36.14  \\
        mAP@S     & 14.84 & 14.84 & 14.84  & 10.08 & 10.08 & 10.08  \\
        mAP@M     & 33.99 & 33.99 & 33.99  & 30.38 & 30.38 & 30.38  \\
        mAP@L     & 46.81 & 46.81 & 46.81  & 51.81 & 51.81 & 51.81  \\
        \cmidrule(lr){1-4}
        \cmidrule(lr){5-7}
        mAR@1     & 29.26 & 29.26 & 29.26  &  3.36 &  3.36 &  3.36  \\
        mAR@10    & 44.39 & 44.39 & 44.39  & 24.46 & 24.46 & 24.46  \\
        mAR@100   & 45.92 & 45.92 & 45.92  & 42.91 & 42.91 & 42.91  \\
        mAR@S     & 23.30 & 23.30 & 23.30  & 17.78 & 17.78 & 17.78  \\
        mAR@M     & 45.18 & 45.18 & 45.18  & 39.59 & 39.59 & 39.59  \\
        mAR@L     & 56.57 & 56.57 & 56.57  & 58.55 & 58.55 & 58.55  \\
    \bottomrule
    \end{tabular}
    \caption{Results of experiment No. 1, with the confidence-based preprocessing and Soft-NMS. All the compared methods show the same performance on each dataset.}
    \label{tab:original_qsqs_v1}
\end{table}
\setlength\tabcolsep{0.75mm} 
\begin{table}[tbh]
    \centering
    \small
    \begin{tabular}{l ccc ccc}
    \toprule
     & \multicolumn{3}{c}{COCO} & \multicolumn{3}{c}{CrowdHuman} \\
        \cmidrule(lr){1-4}
        \cmidrule(lr){5-7}
        Method    & QSQS  & QAQS  & QAQS-C & QSQS  & QAQS  & QAQS-C  \\
        \cmidrule(lr){1-4}
        \cmidrule(lr){5-7}
        mAP       & 34.62 & \underline{35.97} & \textbf{36.30}  & 31.39 & \underline{36.15} & \textbf{36.87}   \\
        mAP@50    & 54.20 & \underline{56.87} & \textbf{57.51}  & 53.92 & \underline{63.69} & \textbf{65.50}   \\
        mAP@75    & 37.66 & \underline{38.77} & \textbf{38.96}  & 32.30 & \underline{36.54} & \textbf{36.81}   \\
        mAP@S     & 15.35 & \underline{15.52} & \textbf{15.63}  &  9.78 & \underline{10.62} & \textbf{10.90}   \\
        mAP@M     & 33.61 & \underline{34.59} & \textbf{34.99}  & 27.28 & \underline{30.88} & \textbf{31.58}   \\
        mAP@L     & 44.93 & \underline{47.42} & \textbf{47.84}  & 43.78 & \underline{51.62} & \textbf{52.58}   \\
        \cmidrule(lr){1-4}
        \cmidrule(lr){5-7}
        mAR@1     & 30.19 & \underline{30.61} & \textbf{30.62}  &  \underline{3.34} & \textbf{ 3.36} & \textbf{ 3.36}   \\
        mAR@10    & 44.56 & \underline{47.13} & \textbf{48.22}  & 23.36 & \underline{24.46} & \textbf{24.49}   \\
        mAR@100   & 45.44 & \underline{48.67} & \textbf{50.54}  & 36.51 & \underline{43.58} & \textbf{45.19}   \\
        mAR@S     & 26.10 & \underline{27.48} & \textbf{28.87}  & 16.72 & \underline{19.68} & \textbf{21.17}   \\
        mAR@M     & 45.31 & \underline{48.08} & \textbf{50.10}  & 34.18 & \underline{40.53} & \textbf{42.18}   \\
        mAR@L     & 54.12 & \underline{58.89} & \textbf{60.86}  & 48.57 & \underline{58.38} & \textbf{59.99}   \\
    \bottomrule
    \end{tabular}
    \caption{Results of experiment No. 2 with NMS-based preprocessing and without Soft-NMS. The best values are shown in \textbf{bold}, and the second-best values are \underline{underlined}. The proposed methods outperform QSQS.}
    \label{tab:original_qsqs_v2}
\end{table}
\setlength\tabcolsep{0.75mm} 
\begin{table}[tbh]
    \centering
    \small
    \begin{tabular}{l ccc ccc}
    \toprule
     & \multicolumn{3}{c}{COCO} & \multicolumn{3}{c}{CrowdHuman} \\
        \cmidrule(lr){1-4}
        \cmidrule(lr){5-7}
        Method    & QSQS  & QAQS  & QAQS-C & QSQS  & QAQS  & QAQS-C  \\
        \cmidrule(lr){1-4}
        \cmidrule(lr){5-7}
        mAP       & 36.25 & 36.25 & \textbf{36.30}  & 36.76 & 36.76 & \textbf{36.87}   \\
        mAP@50    & 57.41 & \underline{57.42} & \textbf{57.52}  & 65.09 & 65.09 & \textbf{65.49}   \\
        mAP@75    & 38.92 & \underline{38.93} & \textbf{38.96}  & \textbf{36.81} & \textbf{36.81} & \textbf{36.81}   \\
        mAP@S     & 15.61 & 15.61 & \textbf{15.63}  & 10.89 & 10.89 & \textbf{10.90}   \\
        mAP@M     & 34.93 & 34.93 & \textbf{34.99}  & 31.58 & 31.58 & \textbf{31.58}   \\
        mAP@L     & 47.80 & \underline{47.81} & \textbf{47.85}  & 52.52 & 52.52 & \textbf{52.58}   \\
        \cmidrule(lr){1-4}
        \cmidrule(lr){5-7}
        mAR@1     & \textbf{30.62} & \textbf{30.62} & \textbf{30.62}  &  \textbf{3.36} &  \textbf{3.36} &  \textbf{3.36}   \\
        mAR@10    & 47.97 & \underline{47.98} & \textbf{48.22}  & \textbf{24.49} & \textbf{24.49} & \textbf{24.49}   \\
        mAR@100   & 50.20 & \underline{50.23} & \textbf{50.57}  & 45.05 & \underline{45.06} & \textbf{45.19}   \\
        mAR@S     & 28.64 & \underline{28.67} & \textbf{28.88}  & 21.01 & \underline{21.04} & \textbf{21.17}   \\
        mAR@M     & 49.67 & \underline{49.68} & \textbf{50.11}  & 42.06 & \underline{42.07} & \textbf{42.18}   \\
        mAR@L     & 60.58 & \underline{60.63} & \textbf{60.90}  & 59.83 & 59.83 & \textbf{59.99}   \\
    \bottomrule
    \end{tabular}
    \caption{Results of experiment No. 3 with NMS-based preprocessing and Soft-NMS. The best values are shown in \textbf{bold}, and the second-best values are \underline{underlined}. However, the value of the second-best tie is \textbf{not underlined}. The proposed methods show slightly better performance than QSQS.}
    \label{tab:original_qsqs_v3}
\end{table}


\section{Details and experimental results for Section 5.2 in the main text}
\label{appendix:discussion}
\begin{table*}[tbh]
    \centering
    \small
    % \begin{tabular}{l | cc|cc|cc| cc|cc|cc}
    \begin{tabular}{l cc c cc c cc c  cc c cc c cc}
    \toprule
     & \multicolumn{8}{c}{COCO} && \multicolumn{8}{c}{CrowdHuman} \\
        \cmidrule(lr){1-1}
        \cmidrule(lr){2-9}
        \cmidrule(lr){10-18}
        % Method    & QSQS & QSQS$^*$ & QAQS & QAQS$^*$ & QAQS-C & QAQS-C$^*$ & QSQS & QSQS$^*$ & QAQS & QAQS$^*$ & QAQS-C & QAQS-C$^*$ \\
        Method    & \multicolumn{2}{c}{QSQS} && \multicolumn{2}{c}{QAQS} && \multicolumn{2}{c}{QAQS-C} && \multicolumn{2}{c}{QSQS} && \multicolumn{2}{c}{QAQS} && \multicolumn{2}{c}{QAQS-C} \\
        % \cmidrule(lr){1-7}
        % \cmidrule(lr){8-13}
        \cmidrule(lr){1-1}
        \cmidrule(lr){2-3}
        \cmidrule(lr){5-6}
        \cmidrule(lr){8-9}
        \cmidrule(lr){11-12}
        \cmidrule(lr){14-15}
        \cmidrule(lr){17-18}
        Time [s]  & 108   & 59    && 122   & 63    && 121   & 58     && 140   &    99 && 163   &   100 && 155   & 95    \\
        Enforce sparsity$^*$  & -   & $\checkmark$    &&  -   & $\checkmark$   && -   & $\checkmark$      &&  -   & $\checkmark$  &&  -   & $\checkmark$  &&  -   & $\checkmark$    \\
        % \cmidrule(lr){1-7}
        % \cmidrule(lr){8-13}
        \cmidrule(lr){1-1}
        \cmidrule(lr){2-3}
        \cmidrule(lr){5-6}
        \cmidrule(lr){8-9}
        \cmidrule(lr){11-12}
        \cmidrule(lr){14-15}
        \cmidrule(lr){17-18}
        mAP       & 34.04 & 34.02 && 35.25 & 35.27 && 35.35 & 35.36  && 31.23 & 32.92 && 35.62 & 35.73 && 35.77 & 35.77 \\
        mAP@50    & 52.94 & 53.07 && 55.33 & 55.37 && 55.50 & 55.50  && 53.28 & 56.71 && 61.89 & 62.49 && 62.44 & 62.44 \\
        mAP@75    & 37.19 & 37.15 && 38.17 & 38.18 && 38.22 & 38.23  && 32.32 & 34.34 && 36.15 & 36.15 && 36.14 & 36.14 \\
        mAP@S     & 14.63 & 14.65 && 14.79 & 14.79 && 14.83 & 14.83  &&  9.42 &  9.63 && 10.09 & 10.08 && 10.08 & 10.08 \\
        mAP@M     & 32.91 & 33.07 && 33.88 & 33.91 && 34.02 & 34.02  && 27.11 & 28.41 && 30.35 & 30.41 && 30.38 & 30.38 \\
        mAP@L     & 44.43 & 44.18 && 46.64 & 46.65 && 46.78 & 46.79  && 43.79 & 46.63 && 51.55 & 51.54 && 51.81 & 51.81 \\
        % \cmidrule(lr){1-7}
        % \cmidrule(lr){8-13}
        \cmidrule(lr){1-1}
        \cmidrule(lr){2-3}
        \cmidrule(lr){5-6}
        \cmidrule(lr){8-9}
        \cmidrule(lr){11-12}
        \cmidrule(lr){14-15}
        \cmidrule(lr){17-18}        
        mAR@1     & 28.92 & 28.79 && 29.26 & 29.26 && 29.26 & 29.26  &&  3.34 &  3.35 &&  3.36 &  3.36 &&  3.36 &  3.36 \\
        mAR@10    & 42.13 & 42.14 && 44.19 & 44.21 && 44.38 & 44.39  && 23.38 & 23.73 && 24.45 & 24.45 && 24.46 & 24.46 \\
        mAR@100   & 42.90 & 43.04 && 45.50 & 45.57 && 45.90 & 45.91  && 36.19 & 38.45 && 42.57 & 42.67 && 42.91 & 42.91 \\
        mAR@S     & 22.10 & 22.22 && 23.01 & 23.05 && 23.30 & 23.30  && 15.55 & 16.08 && 17.56 & 17.61 && 17.78 & 17.78 \\
        mAR@M     & 42.47 & 42.81 && 44.76 & 44.85 && 45.17 & 45.17  && 33.86 & 35.81 && 39.29 & 39.38 && 39.59 & 39.59 \\
        mAR@L     & 52.14 & 52.00 && 56.07 & 56.11 && 56.52 & 56.53  && 48.66 & 52.08 && 58.13 & 58.26 && 58.55 & 58.55 \\
    \bottomrule
    \end{tabular}
    \\\footnotesize{$*$ Sparse coefficient matrix using $P_1$ and $P_2$ defined in \cref{eq:sparse_p1,eq:sparse_p2}.}
    \caption{Comparison with default coefficient matrix and sparse coefficient matrix.}
    \label{tab:default_vs_sparse}
\end{table*}
% 本節ではmain textのsection 5.2で触れた、係数行列のスパース性の向上に基づく計算速度の改善について、より詳細に議論する。
% まずは係数行列のスパース性を向上させる、係数行列の計算方法の工夫について説明する。
In this section, we will discuss in more detail the acceleration based on improving the sparsity of the coefficient matrix, which is mentioned in Section 5.2 of the main text.
First, we explain the method of calculating the coefficient matrix to improve its sparsity.
Generalized IoU (GIoU) is defined as follows.
\begin{equation}
    GIoU(A, B) = IoU(A, B) - \frac{|C(A, B)\setminus A\cup B|}{|C(A, B)|},
\end{equation}
where $C(A, B)$ is the minimum convex hull that covers $A\cup B$.
% 係数行列のスパース性を向上させるため、IoUの代わりにこのGIoUを用いる。
% IoUが0以上の値しか取らないのに対し、GIoUは負の値を取る場合がある。
% そこで、私たちはGIoUの最小値が0になるようにクリップする。
% GIoUの定義から、$GIoU(A, B)\leq 0$ならば$0$でクリップする、という処理は、IoU閾値を$\dfrac{|C\setminus A\cup B|}{|C|}$としてボックスの重なり判定を行うことと等価である。
We use this GIoU instead of IoU to improve the sparsity of the coefficient matrix.
Although IoU is greater than or equal to 0 for any inputs, GIoU can take negative values.
To adjust the range of coefficient to the original IoU version, we clip the negative value of GIoU to 0.
From the definition of GIoU, the process of clipping at $0$ if $GIoU(A, B) \leq 0$ is equivalent to ignoring prediction overlap when IoU is smaller than the threshold of $\dfrac{|C\setminus A\cup B|}{|C|}$.
% また、IoUを用いた元々の定式化は、IoUが0になる時spatial featureで定義される$P_2$も0になるという特徴を持つ。
% これとの整合性を取るため、GIoUが0になるような組に対して、対応する$P_2$の成分も0に置き換えることにした。
% 以上をまとめると、$P_1$と$P_2$はそれぞれ以下のように定義される。
Also, in the original formulation using IoU, the components of the spatial feature $(P_2)_{ij}$ is 0 when $(P_1)_{ij}=0$, i.e., IoU equals 0.
To maintain consistency with this, we decide to replace the corresponding components of $P_2$ with 0 for pairs that make GIoU equal to 0.
To summarize the above, the sparse version of $P_1$ and $P_2$ are defined as follows.
\begin{align}
    (P_1)_{ij} &= \left\{
        \begin{array}{cl}
            IoU(b_i, b_j) & \mathrm{if}~IoU(b_i, b_j) \geq \dfrac{|C(b_i, b_j)\setminus b_i\cup b_j|}{|C(b_i, b_j)|}\\
            0 & \mathrm{otherwise}
        \end{array}\right.\label{eq:sparse_p1}\\
    (P_2)_{ij} &= \left\{
        \begin{array}{cl}
            \dfrac{|b_i\cap b_j|}{\sqrt{|b_i||b_j|}} & \mathrm{if}~IoU(b_i, b_j) \geq \dfrac{|C(b_i, b_j)\setminus b_i\cup b_j|}{|C(b_i, b_j)|}\\
            0 & \mathrm{otherwise}
        \end{array}\right.\label{eq:sparse_p2}
\end{align}
% $C\neq A\cup B$なら必ず$IoU(A, B)\leq GIoU(A, B)$なので、この修正によって得られる係数行列の方が元の係数行列よりも非ゼロ成分の数が等しいか少ない。従って、この修正によって係数行列の疎性が向上する。
% \Cref{tab:default_vs_sparse}に実験結果を示す。データセットによらず、係数行列の疎性が高いほど計算時間が短縮されており、最大で約半分の実行時間である。
% QSQSがCOCOデータセットで、わずかな性能低下と引き換えに計算速度を向上させたことを除くと、他のケースでは性能を保ったまま、もしくは僅かな改善とともに計算速度の改善が観察された。
The modified coefficient matrix has the same or fewer non-zero components than the original because $IoU(A, B)\leq GIoU(A, B)$ when $C\ne A\cup B$. This means the modified coefficient matrix is more sparse.
The experimental results are shown in \cref{tab:default_vs_sparse}.
The experimental settings are the same as in the main text.
Regardless of the dataset, the higher the sparsity of the coefficient matrix, the shorter the computation time, with up to around 50\% reduction of the execution time.
In most cases, we observe improvements in computation speed while maintaining performance or with a slight improvement.
One exception is QSQS with the COCO dataset. QSQS improves the computation speed at the expense of a slight decrease in performance.

{
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{reference}
}

% WARNING: do not forget to delete the supplementary pages from your submission 
% \input{sec/X_suppl}
% \clearpage
% \appendix
% \input{sec/appendix/appendix}

\end{document}
