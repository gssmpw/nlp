\subsection{Overview}
\label{overview}
The goal of 3D instance segmentation is to determine the categories and binary masks of all foreground objects in the scene. 
%
The architecture of our method is illustrated in Figure~\ref{framework}.
%
Assuming that the input point cloud
has $N$ points, each point contains position $(x, y, z)$, color
$(r, g, b)$ and normal $(n_x, n_y, n_z)$ information.
Initially, we utilize a Sparse UNet~\cite{spconv2022} to extract per-point features $F$. Next, we perform farthest point sampling (FPS) on the entire point cloud coordinates to obtain $\mathcal{S}$ sampled points $Q^p$, representing position queries. Subsequently, we input these sampled points $Q^p$ into the Agent-Interpolation Initialization Module (in Section~\ref{aiim}) to interpolate and obtain corresponding content queries $Q^c$. Finally, we feed $Q^p$ and $Q^c$ together into the Hierarchical Query Fusion Decoder (in Section~\ref{hqfd}) for decoding, resulting in the final instance predictions.


\begin{figure}[!t]
    \vspace{-2em}
    \begin{center}
        \includegraphics[width=1\textwidth]{sup/framework.pdf}
        \caption{\textbf{The overall framework of our method BFL.} The Agent-Interpolation Initialization Module is meticulously crafted to synergize the strengths of FPS and learnable queries, producing object queries better suited for complex and dynamic environments. The Hierarchical Query Fusion Decoder is utilized to retain low overlap queries that aid in recall rate.\vspace{-1.0em}
        }
        \label{framework}
    \end{center}
    \vspace{-1.em}
\end{figure}
\subsection{Feature Extraction}
%我们使用sparse unet作为backbone来提取特征$f \in N*C$，which is the same as spformer， maft。接下来we aggregate the point-level features F into superpoint-level features Fsup using average pooling， which will be uesd as transformer decoder layer（in section 3）中cross attention 的 key和value。紧接着，we perform farthest point sampling (FPS) on the entire point cloud coordinates to obtain $M$ sampled points $Q^p$.
We employ Sparse UNet as the backbone for feature extraction, yielding features $F \in \mathbb{R} ^{N\times C}$, which is consistent with SPFormer~\cite{sun2023superpoint} and Maft~\cite{lai2023mask}. Next, we aggregate the point-level features $F$ into superpoint-level features $F_{\text{sup}}$ using average pooling, which will serve as the key and value for cross-attention in the transformer decoder layer (Section~\ref{hqfd}). Subsequently, we perform FPS on the entire point cloud coordinates to obtain $\mathcal{S}  $ sampled points $Q^p$.
\subsection{Agent-Interpolation Initialization Module}
\label{aiim}
\subsubsection{Discussion} 
\begin{table}[!ht]
  \begin{center}
    \footnotesize
    \vspace{-1em}
    \setlength\tabcolsep{3pt}
    \caption{\textbf{ The mean distance between the coordinates of FPS sampling points and the center points of the final predicted instances on ScanNetV2 validation set. 
}\vspace{-0.5em}}
    \label{table:meandistance}
    \begin{tabular}{ccc}
      \toprule
    X&	Y	&Z\\
    \midrule
    0.2262m&	0.2145m	&0.2367m\\
      \bottomrule
    \vspace{-2.5em}
    \end{tabular}
  \end{center}

\end{table}

% \begin{table}[!t]
%   \begin{center}
%     \footnotesize
%     \setlength\tabcolsep{3pt}
%     \vspace{-2em}
%     \caption{\textbf{The foreground recall rate of the first layer predictions on ScanNetV2 validation set.
% }}
%     \label{table:foregroundrecall}
%     \begin{tabular}{cc}
%       \toprule
% Position Initialization & Recall@50 \\
%     \midrule
% Learnable-based  &82.4\\
% FPS-based  &83.8 \\
%       \bottomrule
%       %\vspace{-2.4em}
%     \end{tabular}
%   \end{center}
% \end{table}
% \begin{table}[!t]
%   \begin{center}
%     \footnotesize
%     \setlength\tabcolsep{3pt}
%     \vspace{-2em}
%     \caption{\textbf{ The performance of the first layer predictions on ScanNetV2 validation set.
% }}
%     \label{table:firstlayerpredictions}
%     %\vspace{-1em}
%     % \normalsize
%     \begin{tabular}{c|ccc}
%       \toprule
%       Position + Content&	mAP	&AP@50&	AP@25\\
%       \midrule
%     FPS + Learnable	&37.2	&50.1&	56.9\\
%     FPS + Zero	&39.2&	51.4	&58.5\\
%     FPS + Agent (Ours)&	\textbf{43.1}&	\textbf{55.7}&\textbf{62.7}\\
%       \bottomrule
%       \vspace{-2.4em}
%     \end{tabular}
%   \end{center}
% \end{table}
\textbf{(a) Position Information:} Our method follows QueryFormer~\cite{lu2023query} and Maft~\cite{lai2023mask}, achieving a strong correlation between the positions of sampling points and the positions of the corresponding predicted instances. The details can be found in the supplemental materials~\ref{Morediscussion}. As shown in Table~\ref{table:meandistance}, we calculate the mean distance between the coordinates of FPS sampling points and the center points of the final predicted instances. The results show that the distances are small relative to the scale of the scene, validating the strong correlation between the FPS positions and the predicted instance positions. This is why we use FPS to initialize the position embedding of the query—it can sample nearly 100\% of foreground instances. In contrast, the learnable-based method of Maft is prone to empty sampling initially. As shown in the second column of Table~\ref{table:first}, we have recorded the foreground recall rate of the first layer predictions, which supports the above viewpoint.

\textbf{(b) Content Information:} In our method, the primary role of content embedding is to provide a strong global inductive bias. This global inductive bias offers specific information about the dataset: \textbf{Firstly}, the dataset being an indoor scene, resulting in biased distributions of point cloud coordinates (XYZ) and color (RGB). \textbf{Secondly}, this task is instance segmentation, so the query needs to focus more on positional information (unlike semantic segmentation, which only requires attention to semantics). 

And similar to most transformer-based methods, the decoder's input (query) includes position embedding and content embedding. The position embedding represents the query's location in the scene, encoding positional information, while the content embedding is mainly used for subsequent instance prediction by being input into the cls head and mask head for predictions. Notably, in the transformer's attention operation, position information converges into the content embedding. Next, we will introduce several design schemes for the combination of position embedding and content embedding, discussing their advantages and disadvantages.

\textbf{FPS + Zero.} This scheme only includes information from a single scene through FPS, lacking the necessary global inductive bias (just like how image preprocessing typically normalizes using the mean and standard deviation of ImageNet~\cite{deng2009imagenet}).

\textbf{FPS + Learnable.} Although learnable embedding can capture global inductive bias, the positions obtained by FPS for different scenes are entirely different, while the learnable embedding is shared across all scenes. Therefore, there is a lack of correspondence between position and learnable embedding.

\textbf{Learnable + Learnable/Zero.} Although this approach ensures correspondence between position embedding and content embedding, it loses the prior knowledge of a single scene. (FPS can obtain the prior of a single scene, i.e., higher foreground coverage for the current scene. Given the wide, sparse, and diverse distribution of point cloud, it is challenging for learnable embedding to cover instances effectively.)

\textbf{FPS + Agent (Interpolation)—Our Method.} Firstly, we use FPS to obtain the prior for the current scene. Next, we use interpolation to acquire the global inductive bias. Since the agent contains corresponding position embedding and content embedding, our method balances single scene priors, global inductive bias, and correspondence. To validate this, as shown in the 3 to 5 column of Table~\ref{table:first}, we record the APs of the first layer predictions (the main difference among the three setups lies in the content embedding). Our agent-based interpolation method can acquire richer content information (strong global inductive bias), thereby improving the APs metrics.
\subsubsection{Method Details} 
In this section, we will introduce the process of obtaining content queries through agent interpolation. Firstly, we initialize $L$ agents, which contain $L$ learnable position coordinates $Q^p_0\in[0,1]^{L\times 3}$ and $L$ learnable content queries $Q^c_0\in\mathbb{R} ^{L\times C}$. Given the significant variation in the range of points among different scenes, we perform a scene-specific refinement on the normalized $Q^p_0$, 
\begin{equation}
  \label{refinement}
  \widehat{Q^p_0} = Q^p_0\cdot (p_{max}-p_{min}) + p_{min},
\end{equation}
where $p_{max}\in\mathbb{R}^3$, $p_{min}\in\mathbb{R}^3$ represent the maximum and minimum coordinates of the input scene respectively.
%接下来，its turn to 基于 agents 和sampled points $Q^p$插值获取content queries $Q^c$。
%具体来说， 我们首先计算\widehat{Q^p_0}集合中距离每个采样点$Q^p$最近的k个点，
Next, it's time to interpolate content queries $Q^c$ based on agents and sampled points $Q^p$. Specifically, we first compute the nearest $K$ agents in the $\widehat{Q^p_0}$ set to each sampled point $Q^p$,
\begin{equation}
  \label{knn}
  dis, idx = \text{KNN}(\widehat{Q^p_0}, Q^p),
\end{equation}
where $dis \in \mathbb{R}^{\mathcal{S} \times K}$, $idx \in \mathbb{N}^{\mathcal{S} \times K}$.
%紧接着，我们会基于dis计算权重W，
Following that, we calculate weights $\text{W}\in[0,1]^{\mathcal{S} \times K}$ based on the distance $dis$,
\begin{equation}
  \label{weight}
  \text{W}_{i,j} = \frac{dis^{-1}_{i,j}}{\sum_{j = 1}^{K}dis^{-1}_{i,j} },
\end{equation}
where $i$, $j$ represent the $i$-th sampled point and the $j$-th agent.
%最终,我们利用Q^c_0加权得到sampled 点所对应的content queries $Q_c$,
Finally, we weight $Q^c_0$ to obtain the content queries $Q^c$ corresponding to the sampled points $Q^p$,
\begin{equation}
  \label{Q^c}
  Q^c_i = \sum_{j = 1}^{K}\text{W}_{i,j}\text{Gather}(Q^c_0,idx)_{i,j},
\end{equation}
where $\text{Gather}$~\cite{paszke2019pytorch} is used to collect values from an input tensor according to specified indices.

%在获得$Q^c$后，我们会将$Q^c$，$Q^p$一起输送到Hierarchical Query Fusion Decoder做instance 预测。but值得注意的是，如果我们直接将$Q^p$输送进去，我们无法通过梯度反传直接更新learnable position coordinates $Q^p_0$，only  $Q^c_0$会被更新。所以为了$Q^p_0$也能随着网络训练不断更新，我们对$Q^p$做了一些修改：
After obtaining $Q^c$, we feed $Q^c$ and $Q^p$ together into the Hierarchical Query Fusion Decoder for instance prediction. However, it is worth noting that if we directly feed $Q^p$ in, we cannot update the learnable position coordinates $Q^p_0$ through gradient backpropagation; only $Q^c_0$ can be updated. Therefore, to ensure that $Q^p_0$ can also be continuously updated along with the network training, we make some modifications to $Q^p$,
\begin{equation}
  \label{sg}
  \widehat{Q^p} = \text{SG}(Q^p-\Phi (\text{W},Q^p_0,idx)) + \Phi(\text{W},Q^p_0,idx),
\end{equation}
where $\text{SG}$~\cite{van2017neural} refers to stop gradient, $\Phi$ achieves the same functionality with Equation~\ref{Q^c}. With this ingenious design, the values of $\widehat{Q^p}$ equal $Q^p$, and $Q^p_0$ remain updatable. %为了书写简洁，后续我们依旧将\widehat{Q^p}写作Q^p。
To maintain brevity in our writing, we will continue to use $Q^p$ to represent $\widehat{Q^p}$ in subsequent modules.
\subsection{Hierarchical Query Fusion Decoder}
\label{hqfd}
The purpose of this section is to generate final instance predictions through decoding. 
%在过去的方法中，multi decoder layers are utilized to refine queries and 对于每一层输出的queries我们都会通过MLP获取对应的实例类别以及掩码，获取的实例类别以及掩码将会采用匈牙利算法和GT进行匹配并且采用per-layer auxiliary loss进行监督。
In previous approaches, multiple decoder layers are employed to refine queries. For output queries of each layer, we utilize MLPs to obtain the corresponding instance categories and masks. The acquired instance categories and masks are matched with the ground truth using the Hungarian Matching algorithm~\cite{kuhn1955hungarian} and supervised using per-layer auxiliary loss. 
%在这个过程中，由于noisy features的存在，query优化的方向不稳定，进而造成匈牙利匹配结果的不稳定性，最终造成那些难预测实例无法得到持续稳定的优化，最终造成recall的降低。
In this process, the presence of noisy features leads to unstable directions in query optimization, resulting in instability in Hungarian Matching results, especially for those hard-to-predict instances. Consequently, those hard-to-predict instances are difficult to acquire better mask quality through multiple decoder layers, ultimately leading to \textit{\textbf{Object Disappearance}} and decreased recall (as shown in Figure~\ref{motivation} (b)).

%因此，为了避免部分query因不稳定的优化带来recall的降低，我们会保留住那些更新前后状态发生较大变化的query的更新前状态。
Therefore, to mitigate this problem, we merge specific queries from different layers, retaining pre-update queries that exhibit a low overlap compared to post-update queries.
%具体来说，假设第l-1层输出的query QP QC经过第l层的更新变成qp，qc。
Specifically, suppose the queries $Q^p_{l-1}$ and $Q^c_{l-1}$, outputted from the ($l$-1)-th layer, is updated to $Q^p_l$ and $Q^c_l$ after the update in the $l$-th layer.
%我们首先会计算$Q^c_{l-1}$和$Q^c_l$所对应的实例mask。
We first calculate the instance masks $\mathbf{M}_{l-1}$ and $\mathbf{M}_l$ corresponding to $Q^c_{l-1}$ and $Q^c_l$.
%接下来我们会计算$\mathbf{M}_{l-1}$和 $\mathbf{M}_l$之间的IoU,
Next, we compute the $\text{IoU} \in [0,1]^{\mathcal{S} _{l-1}\times \mathcal{S} _{l}}$ between $\mathbf{M}_{l-1}$ and $\mathbf{M}_l$. 
%我们会统计第l-1层的每一个mask与第l层mask的iou最大值。
We calculate the maximum IoU between each mask from layer ($l-1$) and the masks from layer $l$,
\begin{equation}
  \label{iou}
  \mathbf{U}_i = \max_j(\text{IoU}_{i,j}),
\end{equation}
%最后我们会对$\mathbf{U}$做Bottom K的操作，即选取\mathbf{U}中值最小的索引。
Finally, we perform a Bottom-K operation on $\mathbf{U}$, selecting the indices $\mathcal{I} \in\mathbb{N} ^{\mathcal{D}_1\times 1}$ corresponding to the smallest $\mathcal{D}_1$ values in $\mathbf{U}$. 
%我们会根据索引 $\mathcal{I}$去查询所对应的第l-1层的query，并将这些query与第l层的query concatenate起来，一起输入到第l+1层中。
We utilize the indices $\mathcal{I}$ to retrieve the corresponding queries from the ($l$-1)-th layer. These queries are concatenated with those from the $l$-th layer and collectively fed into the ($l$+1)-th layer.
%通过这种选择机制，query有了重新更新的机会，一旦query更新的很不理想，更新前的query便会保留下来，输送到下一层重新更新，如果更新的适中或比较理想，那么更新前的query保不保留都可以接受。recall也能够得到逐层稳步的提升。值得注意的是，query数目的增加对耗时的负担非常有限， with only a 7.8\% increase in runtime.
%关于transformer decoder layer的细节，可以参考补充材料。
For details regarding the transformer decoder layer, please refer to the supplemental materials.

Through this selection mechanism, queries are given the opportunity for re-updating. If the updated queries perform poorly, the pre-update queries will be retained and passed to the next layer for re-updating. If the update is moderate or reasonably satisfactory, whether to retain the pre-update queries or not is acceptable. Recall also experiences a gradual and steady improvement layer by layer. To be more specific, we introduce the details in the supplemental materials~\ref{Morediscussion}. It is worth noting that the increase in the number of queries imposes a limited burden on runtime, with a 7.8\% increase. One final point to add is that since the queries in the earlier layers have not aggregated enough instance information, we do not perform the aforementioned fusion operation. Instead, we only conduct the fusion operation at the final $\mathcal{D}_2$ layers.  Here, $\mathcal{D}_2$ indicates the layers where the fusion operation is performed. For example, 
$\mathcal{D}_2$=3 means we perform the fusion operation in the last 3 layers. 


\subsection{Model Training and Inference}
\label{training inference}
Following Maft~\cite{lai2023mask}, the training loss we utilize contains five aspects, 
\begin{equation}
  \label{lall}
  L_{all} = \lambda_1L_{ce} + \lambda_2L_{bce} + \lambda_3L_{dice}  + \lambda_4L_{center} + \lambda_5L_{score}, 
\end{equation}
where $\lambda_1$, $\lambda_2$, $\lambda_3$, $\lambda_4$, $\lambda_5$ are hyperparameters. It is worth noting that we apply $L_{all}$ supervision to the output of each layer.
During the model inference phase, we use the predictions from the final layer as the final output. In addition to the normal forward pass through the network, we also employ NMS on the final output as a post-processing operation. A further discussion on NMS is provided in the supplementary materials.