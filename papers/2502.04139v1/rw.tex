In this section, we briefly overview related works on 3D instance segmentation, including proposal-based methods~\cite{yi2019gspn,hou20193d,yang2019learning}, grouping-based methods~\cite{engelmann20203d,liu2020learning,wang2018sgpn,wang2019associatively,lahoud20193d,jiang2020pointgroup,engelmann20203d,han2020occuseg,jiang2020pointgroup,jiang2020end,chen2021hierarchical,liang2021instance,vu2022softgroup}, and instance segmentation with transformer~\cite{cheng2021per, cheng2022masked,schult2022mask3d,sun2023superpoint,lu2023query,lai2023mask}.
%\vspace{-1em}

{\bf Proposal-based Methods.}
%
Existing proposal-based methods are heavily influenced by the success of Mask R-CNN~\cite{he2017mask} for 2D instance segmentation. GSPN~\cite{yi2019gspn} adopts an analysis-by-synthesis strategy to generate high-quality 3D proposals, refined by a region-based PointNet~\cite{qi2017pointnet}. 3D-BoNet~\cite{yang2019learning} employs PointNet++\cite{qi2017pointnet++} for feature extraction from point clouds and applies Hungarian Matching\cite{kuhn1955hungarian} to generate 3D bounding boxes. These methods set high expectations for proposal quality.

{\bf Grouping-based Methods.}
%
Grouping-based methods make per-point predictions, such as semantic categories and geometric offsets, then group points into instances. 
%MTML~\cite{lahoud20193d} employs a multi-task learning strategy for grouping points. 
PointGroup~\cite{jiang2020pointgroup} segments objects on original and offset-shifted point clouds and employs ScoreNet for instance score prediction. %HAIS~\cite{chen2021hierarchical} extends PointGroup by incorporating surrounding fragments of instances and refining instances based on intra-instance prediction.
SSTNet~\cite{liang2021instance} constructs a tree network from pre-computed superpoints and splits non-similar nodes to obtain object instances. 
SoftGroup~\cite{vu2022softgroup} groups based on soft semantic scores instead of hard semantic predictions and refines proposals to enhance positive samples while suppressing negatives. However, grouping-based methods require manual selection of geometric properties and parameter adjustments, which can be challenging in complex and dynamic point cloud scenes.

{\bf Instance Segmentation with Transformer.}
Transformer~\cite{vaswani2017attention} has been widely applied in computer vision tasks such as image classification~\cite{dosovitskiy2020image, chen2021crossvit}, object detection~\cite{carion2020end, ding2019learning,wang2023long,deng2024diff3detr}, and segmentation~\cite{zheng2021rethinking, deng2025quantity, cheng2021per,cheng2022masked,lu2024bsnet,li2024mamba24} due to the self-attention mechanism, which models long-range dependencies. Recently, DETR~\cite{carion2020end} has been proposed as a new paradigm using object queries for object detection in images. Building on the set prediction mechanism introduced by DETR, Mask2Former~\cite{cheng2022masked} employs mask attention to impose semantic priors, thereby accelerating training for segmentation tasks. The success of transformer has also become prominent in 3D instance segmentation. Following Mask2Former, each object instance is represented as an instance query, with query features learned through a vanilla transformer decoder, and the output from the final layer serving as the final prediction. Mask3D~\cite{schult2022mask3d} and SPFormer~\cite{sun2023superpoint} are the first works to utilize the transformer framework for 3D instance segmentation. They respectively employ FPS and learnable queries as query initialization. QueryFormer~\cite{lu2023query} and Maft~\cite{lai2023mask} are improvements upon Mask3D and SPFormer, but still utilize FPS and learnable queries for query initialization. Our approach combines FPS and learnable queries, employing the Agent-Interpolation Initialization Module to produce object queries better suited for complex and dynamic environments. Additionally, we utilize the Hierarchical Query Fusion decoder to retain low overlap queries that aid in recall rate.
