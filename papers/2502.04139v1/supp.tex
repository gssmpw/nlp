\subsection{Overview}

This supplementary material provides more model and experimental details to understand our proposed method. After that, we present more experiments to demonstrate the effectiveness of our methods. Finally, we show a rich visualization of our modules. 

\subsection{More Model Details}
\textbf{Sparse UNet.}  
For ScanNetV2~\cite{dai2017scannet}, ScanNet200~\cite{rozenberszki2022language}, and ScanNet++~\cite{yeshwanth2023scannet++}, we employ a 5-layer U-Net as the backbone, with the initial channel set to 32. 
%
Unless otherwise specified, we utilize coordinates, colors, and normals as input features. 
%
Our method incorporates 6 layers of Transformer decoders, with the head number set to 8, and the hidden and feed-forward dimensions set to 256 and 1024, respectively.
%
For S3DIS~\cite{armeni20163d}, following Mask3D~\cite{schult2022mask3d}, we utilize Res16UNet34C~\cite{choy20194d} as the backbone and employ 4 decoders to attend to the coarsest four scales. 
%
This process is repeated 3 times with shared parameters. The dimensions for the decoder's hidden layer and feed-forward layer are set to 128 and 1024, respectively.

\textbf{Transformer Decoder Layer.} In this layer, we use superpoint-level features $F_{\text{sup}}$ and their corresponding positions $P_{\text{sup}}$ as key and value, with content queries $Q^c$ and position queries $Q^p$ as query. 
%Ju具体的网络结构可见Figure~\ref{decoder}, which is the same with Maft's transformer decoder layer. 所以如果想知道相关方程及细节，可以直接查询maft的正文。
The specific network architecture can be seen in Figure~\ref{decoder}, which is identical to Maft's~\cite{lai2023mask} transformer decoder layer. Therefore, more relevant equations and details can be directly referred to Maft's main text.
\begin{figure}[!ht]
  %\vspace{-2.0em}
  \begin{center}
      \includegraphics[width=0.95\textwidth]{graph/decoder_layer.pdf}
      \caption{\textbf{The architecture of the transformer decoder layer.} The figure is taken from the main text of Maft. }
      \label{decoder}
  \end{center}
  %\vspace{-1.0em}
\end{figure}

\textbf{Matching and Loss.} 
Existing methods depend on semantic predictions and binary masks for matching queries with ground truths. 
%
Building upon Maft~\cite{lai2023mask}, our approach integrates center distance into Hungarian Matching~\cite{kuhn1955hungarian}. 
%
To achieve this, we modify the formulation of matching costs as follows:
\begin{gather}
  \label{cost}
  \mathcal{C}_{cls} (p, \overline{p} ) = CE(CLASS_p, CLASS_{\overline{p}}), \\
  \mathcal{C}_{dice} (p, \overline{p} ) = DICE(MASK_p, MASK_{\overline{p}}), \\
  \mathcal{C}_{bce} (p, \overline{p} ) = BCE(MASK_p, MASK_{\overline{p}}), \\
  \mathcal{C}_{center} (p, \overline{p} ) = L1(Center_p, Center_{\overline{p}}), \\
  \mathcal{C}(p, \overline{p} ) = \lambda_{cls}\mathcal{C}_{cls} (p, \overline{p} ) + \lambda_{dice}\mathcal{C}_{dice} (p, \overline{p} ) + \lambda_{bce}\mathcal{C}_{bce} (p, \overline{p} ) +\lambda_{center}\mathcal{C}_{center} (p, \overline{p} ), 
\end{gather}
where $p$ and $\overline{p}$ denotes a predicted and ground-truth instance, $\mathcal{C}$ represents the matching cost matrix, and $\lambda_{cls}, \lambda_{dice}, \lambda_{bce}, \lambda_{center}$ are the hyperparameters. Here, $\lambda_{cls}, \lambda_{dice}, \lambda_{bce}, \lambda_{center}$ are the same as $\lambda_{1}, \lambda_{2}, \lambda_{3}, \lambda_{4}$.
%接下来我们会对$\mathcal{C}$进行匈牙利匹配，最后我们会对匈牙利匹配结果按照Eqution~\ref{lall}进行监督。
Next, we perform Hungarian Matching on $\mathcal{C}$, and then supervise the Hungarian Matching results according to Equation~\ref{lall}



\textbf{Non-Maximum Suppression.} Non-maximum suppression (NMS) is a common post-processing operation used in instance segmentation. In fact, for some previous methods, applying NMS to the final layer predictions has consistently led to performance improvements, as shown in Table~\ref{table:nms}. However, if we apply NMS to the concatenated outputs, as described in Section~\ref{sec:intro} lines 63-65, a significant decrease in performance occur.
%具体原因我们猜测一方面是因为NMS非常依赖于置信度，重复的mask中只有置信度高的mask会被保留下来，但是这种置信度很不准确，所以往往保留下来的mask并不是质量最好的。而concatenated outputs中重复的mask又特别多，所以最终性能会降低。另一方面是因为需要手工选取阈值，阈值过高，无法有效滤除重复的mask，阈值过低，往往会把有用的mask错误丢掉。而越复杂的输出，阈值选取难度越大，因此对于concatenated outputs，很难找到最优的阈值进行有效过滤。
The specific reasons for this performance decrease are twofold. Firstly, NMS heavily relies on confidence scores, retaining only the masks with the highest confidence among the duplicates. However, these confidence scores are often inaccurate, leading to the retention of masks that are not necessarily of the best quality. Since the concatenated outputs contain a large number of duplicate masks (almost every mask has duplicates), this results in a significant reduction in performance. Secondly, NMS requires manual selection of a threshold. If the threshold is set too high, it cannot effectively filter out duplicate masks; if it is set too low, it tends to discard useful masks. The more complex the output, the more challenging it becomes to select an optimal threshold. Therefore, for concatenated outputs, it is difficult to find an optimal threshold for effective filtering.
\begin{table}[!ht]
  \begin{center}
    \footnotesize
    \setlength\tabcolsep{5pt}
    \centering
    \caption{\textbf{The effectiveness of the NMS.} COE refers to concatenating the outputs of each layer and then conducting NMS.}
    \label{table:nms}
    \begin{tabular}{c|ccc}
      \toprule
      % \hlin
     Method&mAP&AP@50&AP@25\\
    \midrule
      %\cline{2-9}
      SPFormer &56.7  &74.8 &82.9 \\
      SPFormer+NMS &57.2 &75.9& 83.5\\
      SPFormer+COE &55.7 & 73.4  &81.8\\
      \midrule
      Maft &58.4 &75.2 &83.5 \\
      Maft+NMS&59.0& 76.1 &84.3\\
      SPFormer+COE &57.3& 73.5& 81.8\\
      \midrule
      Ours &61.1& 78.2& 85.6\\
      Ours+NMS&\textbf{61.7} &\textbf{79.5}& \textbf{86.5}\\
      \bottomrule 
    \end{tabular}
  \end{center}
\end{table}

\subsection{More discussion}
\label{Morediscussion}
\textbf{Details on achieving a strong correlation.} The positions of sampling points in Mask3D are not related to the positions of the corresponding predicted instances. In fact, this lack of correlation results in the query's lack of interpretability, we cannot clearly understand why this query predicts this object, thus hindering intuitive optimization. Both QueryFormer and Maft address this by adding a  $\mathcal{C}_{center}$
term when calculating the Hungarian matching cost matrix, which represents the distance between the query coordinates and the ground truth instance center. Additionally, they update the query coordinates layer by layer, making the matched query progressively closer to the GT instance center. With this design, the position of the query becomes correlated with the position of the corresponding predicted instance, facilitating intuitive improvements in the distribution of query initialization by QueryFormer and Maft (Query Refinement Module and Learnable Position Query).

\textbf{Detail classification on Hierarchical Query Fusion Decoder.} We aim to give poorly updated queries a new opportunity for updating. It is important to note that this is a copy operation, so we retain both pre-updated and post-updated queries, thus not "limiting the transformer decoder in its ability to swap objects." This approach provides certain queries with an opportunity for entirely new feature updates and offers more diverse matching options during Hungarian matching. This re-updating and diverse selection mechanism clearly enhances recall rates because our design implicitly includes a mechanism: for instances that are difficult to predict or poorly predicted, if the updates are particularly inadequate, the corresponding queries will be retained and accumulated into the final predictions. For example, if a query $ Q_{i}^3$ from the third layer is updated in the fourth layer to become $ Q_{i}^4$ and experiences a significant deviation, the network will retain $ Q_{i}^3$ and pass both $ Q_{i}^3$ and $ Q_{i}^4$ to the fifth layer. After being updated in the fifth layer, $ Q_{i}^3$ becomes $ \hat{Q_{i}^3}$. If $ \hat{Q_{i}^3}$ does not significantly differ from $ Q_{i}^3$, the model will not retain $ Q_{i}^3$ further and will only pass $ \hat{Q_{i}^3}$ to the sixth layer. If $ \hat{Q_{i}^3}$ shows a significant difference from $ Q_{i}^3$ , the model will continue to retain $ Q_{i}^3$. Through this process, teh model can continuously retain the queries that are poorly updated, accumulating them into the final prediction.

\subsection{More Implementation Details}
On ScanNet200~\cite{rozenberszki2022language}, we train our model on a single RTX3090 with a batch size of 8 for 512 epochs. 
%
We employ AdamW~\cite{loshchilov2017decoupled} as the optimizer and PolyLR as the scheduler, with a maximum learning rate of 0.0002.
%
Point clouds are voxelized with a size of 0.02m.
%
For hyperparameters, we tune $\mathcal{S}, L, K, \mathcal{D}_1, \mathcal{D}_2$ as 500, 500, 3, 40, 3 respectively.
$\lambda_1 ,\lambda_2 ,\lambda_3 ,\lambda_4 ,\lambda_5$ in Equation~\ref{lall} are set as 0.5, 1, 1, 0.5, 0.5.
On ScanNet++~\cite{yeshwanth2023scannet++}, we train our model on a single RTX3090 with a batch size of 4 for 512 epochs. The other settings are the same as ScanNet200.
On S3DIS~\cite{armeni20163d}, we train our model on a single A6000 with a batch size of 4 for 512 epochs and adopt onecycle scheduler.
For hyperparameters, we tune $\mathcal{S}, L, K, \mathcal{D}_1, \mathcal{D}_2$ as 400, 400, 3, 40, 3 respectively.
$\lambda_1 ,\lambda_2 ,\lambda_3 ,\lambda_4 ,\lambda_5$ in Equation~\ref{lall} are set as 2, 5, 1, 0.5, 0.5.
\subsection{Detailed Results}
The detailed results for each category on ScanNetV2 validation set are reported in Table~\ref{table:ScanNetV2val}. As the table illustrates, our method achieves the best performance in 16 out of 18 categories.
%
The detailed results for certain categories on ScanNet++ test set are presented in Table~\ref{table:ScanNetppmap}. As indicated by the table, the significant performance improvement highlights the effectiveness of our method in managing denser point cloud scenes across a broader range of categories.
\begin{table}[!ht]
  \begin{center}
    \small
    \setlength\tabcolsep{3.0pt}
    \caption{\textbf{Full quantitative results of mAP on ScanNetV2 validation set.} Best performance is in boldface.}
    \label{table:ScanNetV2val}
    %\vspace{0.5em}
    % \normalsize
    \scalebox{0.74}{\begin{tabular}{c|c|cccccccccccccccccccc}
      \toprule
      Method &  mAP & \rotatebox{90}{bathtub} &\rotatebox{90}{bed} &\rotatebox{90}{bookshe.}	 &\rotatebox{90}{cabinet}&\rotatebox{90}{chair}	&\rotatebox{90}{counter}	&\rotatebox{90}{curtain}&\rotatebox{90}{desk}	&\rotatebox{90}{door}&\rotatebox{90}{other}&\rotatebox{90}{picture}&\rotatebox{90}{frige}&\rotatebox{90}{s. curtain}&\rotatebox{90}{sink}&\rotatebox{90}{sofa}&\rotatebox{90}{table}&\rotatebox{90}{toilet}&\rotatebox{90}{window}\\
      \midrule
      SoftGroup~\cite{vu2022softgroup} & 45.8 &  66.6  & 48.4 & 32.4 & 37.7 &72.3&14.3&37.6&27.6&35.2& 42.0&34.2&56.2&56.9&39.6& 47.6& 54.1 & 88.5&33.0  \\
      DKNet~\cite{wu20223d}& 50.8 & 73.7  & 53.7 & 36.2 &42.6&80.7&22.7&35.7&35.1&42.7&46.7&51.9&39.9&57.2&52.7&52.4&54.2&91.3&37.2\\   	
      Mask3D~\cite{schult2022mask3d}  & 55.2 & 78.3  & 54.3 & 43.5 & 47.1&82.9& 35.9&48.7 &37.0&54.3&59.7 &53.3& 47.7&47.4  &55.6&48.7&63.8 &94.6&39.9\\
      QueryFormer~\cite{lu2023query} & 56.5& 81.3  & 57.7 & 45.0 & 47.2& 82.0 &37.2&43.2&43.3&54.5&60.5&52.6&54.1&62.7& 52.4 &49.9&60.5 &94.7 &37.4\\
      Maft~\cite{lai2023mask}&58.4&80.1& 58.1&41.8 &48.3 &82.2&34.4& 55.1 &\textbf{44.3}& 55.0&57.9&61.6&56.4& 63.7&54.4&53.0 &66.3& \textbf{95.3}&\textbf{42.9}\\
      Ours&\textbf{61.7}&\textbf{83.5}&\textbf{62.3} & \textbf{48.1} &\textbf{50.6}&\textbf{84.1}& \textbf{45.0}& \textbf{57.4}&42.1& \textbf{57.3}&\textbf{61.8}&\textbf{67.8}& \textbf{59.9}& \textbf{68.8}&\textbf{61.1} &\textbf{55.3} & \textbf{66.6} &\textbf{95.3}&42.6\\
      \bottomrule
    \end{tabular}}
  \end{center}
\end{table}
\begin{table}[!ht]
  \begin{center}
    \small
    \setlength\tabcolsep{2.5pt}
    \caption{\textbf{Full quantitative results of mAP on the ScanNetV2 test set. Best performance is in boldface.}}
    \label{table:ScanNetV2map}
    \vspace{0.5em}
    % \normalsize
    \scalebox{0.74}{\begin{tabular}{c|c|cccccccccccccccccccc}
      \toprule
      Method &  mAP & \rotatebox{90}{bathtub} &\rotatebox{90}{bed} &\rotatebox{90}{bookshe.}	 &\rotatebox{90}{cabinet}&\rotatebox{90}{chair}	&\rotatebox{90}{counter}	&\rotatebox{90}{curtain}&\rotatebox{90}{desk}	&\rotatebox{90}{door}&\rotatebox{90}{other}&\rotatebox{90}{picture}&\rotatebox{90}{frige}&\rotatebox{90}{s. curtain}&\rotatebox{90}{sink}&\rotatebox{90}{sofa}&\rotatebox{90}{table}&\rotatebox{90}{toilet}&\rotatebox{90}{window}\\
      \midrule
    
      % 3D-BoNet~\cite{qi2018frustum}   &25.3& 51.9& 32.4 &25.1& 13.7& 34.5& 3.1& 41.9& 6.9& 16.2& 13.1& 5.2& 20.2& 33.8& 14.7& 30.1& 30.3& 65.1& 17.8 \\
     
      % MTML~\cite{lahoud20193d}        &28.2& 57.7& 38.0& 18.2& 10.7& 43.0& 0.1& 42.2& 5.7 &17.9& 16.2& 7.0& 22.9& 51.1& 16.1 &49.1& 31.3& 65.0& 16.2 \\
      % %GICN~\cite{liu2020learning}     &34.1& 58.0& 37.1& 34.4& 19.8& 46.9& 5.2& 56.4& 9.3& 21.2& 21.2& 12.7& 34.7& 53.7& 20.6& 52.5& 32.9& 72.9& 24.1 \\
      % 3D-MPA~\cite{engelmann20203d}   &35.5& 45.7& 48.4& 29.9& 27.7& 59.1& 4.7& 33.2 &21.2& 21.7& 27.8& 19.3& 41.3& 41.0& 19.5& 57.4& 35.2& 84.9& 21.3\\
      % %DyCo3D~\cite{he2021dyco3d}      &39.5& 64.2& 51.8& 44.7& 25.9& 66.6& 5.0& 25.1& 16.6& 23.1& 36.2& 23.2& 33.1& 53.5& 22.9& 58.7& 43.8& 85.0 &31.7 \\
      % PE~\cite{zhang2021point}        &39.6 &66.7& 46.7& 44.6& 24.3& 62.4& 2.2& 57.7 &10.6& 21.9 &34.0& 23.9& 48.7& 47.5& 22.5& 54.1& 35.0 &81.8& 27.3\\
      PointGroup~\cite{jiang2020pointgroup} &40.7& 63.9 &49.6& 41.5& 24.3& 64.5& 2.1 &57.0& 11.4& 21.1& 35.9& 21.7& 42.8& 66.6 &25.6 &56.2& 34.1& 86.0& 29.1 \\
      MaskGroup~\cite{zhong2022maskgroup}   &43.4& 77.8& 51.6 &47.1& 33.0 &65.8 &2.9& 52.6 &24.9 &25.6 &40.0& 30.9 &38.4&29.6& 36.8 &57.5 &42.5& 87.7& 36.2 \\
      OccuSeg~\cite{han2020occuseg}         & 48.6 &80.2& 53.6 &42.8 &36.9 &70.2 &20.5 &33.1 &30.1 &37.9 &47.4& 32.7& 43.7 &\textbf{86.2} &48.5 &60.1 &39.4 &84.6 &27.3  \\
      HAIS~\cite{chen2021hierarchical}      &45.7 &70.4& 56.1 &45.7 &36.4 &67.3 &4.6& 54.7& 19.4& 30.8& 42.6& 28.8& 45.4& 71.1& 26.2& 56.3& 43.4& 88.9& 34.4 \\
      SSTNet~\cite{liang2021instance}       & 50.6 &73.8& 54.9 &49.7& 31.6& 69.3& 17.8& 37.7& 19.8& 33.0& 46.3& 57.6& 51.5& 85.7& \textbf{49.4}& 63.7& 45.7& 94.3& 29.0 \\
      %SoftGroup~\cite{vu2022softgroup}      &50.4& 66.7 &57.9& 37.2& 38.1& 69.4& 7.2& 67.7& 30.3& 38.7& 53.1& 31.9& 58.2& 75.4& 31.8& 64.3& 49.2& 90.7& 38.8 \\
      DKNet~\cite{wu20223d}                 &53.2 &81.5& 62.4 &\textbf{51.7}& 37.7& 74.9& 10.7& 50.9& 30.4 &43.7& 47.5& 58.1& 53.9& 77.5& 33.9& 64.0& 50.6& 90.1& 38.5 \\   	
      %Mask3D~\cite{schult2022mask3d}& 56.6& 92.6 &	59.7 &	40.8 &42.0&	73.7&	23.9 &59.8 &	38.6 &	45.8 &	54.9&	56.8&	71.6 &	60.1 &	48.0 &	64.6 &	57.5 &92.2 &	36.4\\
      %QueryFormer &58.6 &	92.6 &	70.2 &	39.3 &	\textbf{50.5} &	73.7 &	27.7 &	58.3 &	37.5 &	47.9 &	53.5 &	56.8 &	61.5 &	72.0 &	48.1 &	74.5 &59.2 & 95.8 &	36.1 \\
      SPFormer~\cite{sun2023superpoint}&54.9 &	74.5 &	64.0 &	48.4  &	39.5 &	73.9  &	\textbf{31.1}  &	56.6  &	33.5  &	46.8  &	49.2  &	55.5  &	47.8  &	74.7  &	43.6  &	71.2  &	54.0  &	89.3  &	34.3  \\
      Maft~\cite{lai2023mask}&59.6 &	88.9 &	\textbf{72.1} &	44.8 &	46.0 &	\textbf{76.8} &	25.1 &	55.8 &	\textbf{40.8} &	\textbf{50.4} &	53.9 &	61.6 &	61.8 &	85.8 &	48.2 &	68.4 &	55.1 &	93.1 &	\textbf{45.0} \\
      \midrule
      %Spherical Mask&61.6 &	\textbf{94.6} &	65.4 &	55.5 &	43.4 &	76.9 &	27.1 &	60.4 &	44.7 &	50.5 &	54.9 &	69.8 &	71.6 &	77.5 &	48.0 &	74.7 &	57.5 &	92.5 &	43.6 \\
      Ours& \textbf{60.6} &	\textbf{92.6} &70.2 &	51.5&	\textbf{50.2} &	73.2 &	28.2 &	\textbf{59.8} &	38.6 &	48.9 &	\textbf{54.2} &	\textbf{63.5} &	\textbf{71.6} &	75.1 &47.6 &	\textbf{74.3} &	\textbf{58.7} &	\textbf{95.8} &36.0 \\
      \bottomrule
    \end{tabular}}
  \end{center}
\end{table}

\begin{table}[!ht]
  \begin{center}
    \small
    \setlength\tabcolsep{2.5pt}
    \caption{\textbf{Full quantitative results of AP@50 on the ScanNetV2 test set. Best performance is in boldface.}}
    \label{table:ScanNetV250}
    \vspace{0.5em}
    % \normalsize
   \scalebox{0.74}{ \begin{tabular}{c|c|cccccccccccccccccccc}
      \toprule
      Method &  AP@50 & \rotatebox{90}{bathtub} &\rotatebox{90}{bed} &\rotatebox{90}{bookshe.}	 &\rotatebox{90}{cabinet}&\rotatebox{90}{chair}	&\rotatebox{90}{counter}	&\rotatebox{90}{curtain}&\rotatebox{90}{desk}	&\rotatebox{90}{door}&\rotatebox{90}{other}&\rotatebox{90}{picture}&\rotatebox{90}{frige}&\rotatebox{90}{s. curtain}&\rotatebox{90}{sink}&\rotatebox{90}{sofa}&\rotatebox{90}{table}&\rotatebox{90}{toilet}&\rotatebox{90}{window}\\
      \midrule
        
      % 3D-BoNet~\cite{qi2018frustum}   &48.8 &	100.0  & 67.2  &	59.0  &	30.1  &	48.4  &	9.8  &	62.0  &	30.6  &	34.1  &	25.9  &	12.5  &	43.4 &	79.6 &	40.2 &	49.9 &	51.3  &	90.9 &	43.9 \\
      % MTML~\cite{lahoud20193d}        &54.9 &	100.0 &	80.7 &	58.8 &	32.7 &	64.7 &	4 &	81.5 &	18.0 &	41.8 &	36.4 &	18.2 &	44.5 &	100.0 &	44.2 & 68.8 & 57.1 &	100.0 & 39.6 \\
      % %GICN~\cite{liu2020learning}     &78.8 &	100 &	97.8 &	86.7&	78.1 &	83.3 &	52.7 &	82.4 &	86 &	54.9 &	59.6 &	55.1 &	70 &	100 &	85.3 &	\textbf{93.5} &	73.3 &	100 &	65.1 \\
      % 3D-MPA~\cite{engelmann20203d}   &61.1 &	100.0 & 83.3 &	76.5 &	52.6 &	75.6 &	13.6 &	58.8 &	47.0 &43.8 &	43.2 &	35.8 &	65.0 &	85.7 &	42.9 &	76.5 &	55.7 &	100.0 &	43.0\\
      % DyCo3D~\cite{he2021dyco3d}      &64.1 &	100.0 & 84.1 &	89.3 &	53.1 &	80.2 &	11.5 &	58.8 &	44.8 &	43.8 &	53.7 &	43.0 &	55.0 &	85.7 &	53.4 &	76.4 &	65.7 &	98.7 &	56.8 \\
      % PE~\cite{zhang2021point}        &64.5  &	100.0  &	77.3  &	79.8  &	53.8  &	78.6  &	8.8  &	79.9  &	35.0  &	43.5  &	54.7  &	54.5  &	64.6  &	93.3  &	56.2  &	76.1  &	55.6  &	99.7  &	50.1\\
      PointGroup~\cite{jiang2020pointgroup} &63.6 & 100.0 &	76.5 &	62.4 &	50.5 &	79.7 &	11.6 &	69.6 &	38.4 &	44.1 &	55.9 &	47.6 &	59.6 &	100.0 &	66.6 &	75.6 &	55.6 &	99.7 &	51.3 \\
      MaskGroup~\cite{zhong2022maskgroup}   &66.4  &	100.0 &	82.2 &	76.4 &	61.6 &	81.5 &	13.9 &	69.4 &	59.7 &	45.9 &	56.6 &	59.9 &	60.0 &	51.6 &	71.5 &	81.9 &	63.5 &	100.0 &	60.3 \\
      OccuSeg~\cite{han2020occuseg}         & 67.2 &	100.0&	75.8 &	68.2 &	57.6 &	84.2 &	47.7 & 50.4 &	52.4 &	56.7 &	58.5 &	45.1 &	55.7 &	100.0 &	75.1 &	79.7 &	56.3 &	100.0 &	46.7\\
      HAIS~\cite{chen2021hierarchical}      &69.9 &	100.0 &	84.9 &	82.0 &	67.5 &	80.8 &	27.9 &	75.7 &	46.5 &	51.7 &	59.6 &	55.9 &	60.0 &	100.0 &	65.4 &	76.7&	67.6 &	99.4 &	56.0\\
      SSTNet~\cite{liang2021instance}       & 69.8 &	100.0 &	69.7 &	\textbf{88.8} &	55.6 &	80.3 &	38.7 &	62.6 &	41.7 &	55.6 &	58.5 &	70.2 &	60.0 &	100.0 &	\textbf{82.4} & 72.0 &	69.2 &	100.0 &	50.9\\
      %SoftGroup~\cite{vu2022softgroup}      &76.1 &	100.0 &	80.8 &	84.5 & 71.6 &	86.2 &	24.3 &	\textbf{82.4} &	65.5 &	62.0 & 73.4 &	69.9 &	79.1 &	98.1 &	71.6 &	84.4 & 76.9 &	100.0 &	59.4\\
      DKNet~\cite{wu20223d} &71.8 &	100.0 &	81.4 &	78.2 &	61.9 &	87.2 &	22.4 &	75.1 &	56.9 &	67.7 &	58.5 &	72.4 &	63.3 &	98.1 &	51.5 &	81.9 &	73.6 &	100.0 &	61.7\\   	
      %Mask3D~\cite{schult2022mask3d}& 78.0 &	100.0 &	78.6 &	71.6 &	69.6 &	88.5 &	50.0 &	71.4 &	\textbf{81.0} &	67.2 &	71.5 &	67.9 &	80.9 &	100.0 &	\textbf{83.1} &	83.3 &	78.7 &	100.0 &	60.2\\
      %QueryFormer & 78.4	&100.0&	93.3&	60.1&	\textbf{75.4}&	88.5	&56.4&	67.7&	66.6&	66.4	&71.6&	67.9	&\textbf{82.0}	&100.0	&83.0&	89.7&	80.4 &	100.0	&62.2\\
      SPFormer~\cite{sun2023superpoint}&77.0 &	90.3 &	90.3 &	80.6 &	60.9 &	88.6 &	56.8 &	\textbf{81.5} &	70.5 &	71.1 &	65.5 &	65.2 &	68.5&	100.0 &	78.9 &	80.9 &	77.6 &	100.0 &	58.3 \\
      Maft~\cite{lai2023mask}&78.6 &	100.0 &89.4 &	80.7 &	69.4 &	\textbf{89.3} &	48.6 &	67.4 &	74.0 &	\textbf{78.6} &	70.4 &	\textbf{72.7} &	73.9 &	100.0 &	70.7 &	84.9 &	75.6 &	100.0 &	\textbf{68.5} \\ 
      \midrule
      Ours&\textbf{81.0} &	\textbf{100.0} &	\textbf{93.4} &85.4 &	\textbf{74.3} &88.9 &	\textbf{57.5} &	71.4 &	\textbf{81.0} &	66.9&	\textbf{72.9}&	70.7 &	\textbf{80.9}&	\textbf{100.0} &	81.4 &	\textbf{90.2}&	\textbf{81.4} &	\textbf{100.0} &	62.5 \\
      
      \bottomrule
    \end{tabular}}
  \end{center}
\end{table}

\begin{table}[!ht]
  \begin{center}
    \small
    \setlength\tabcolsep{2.5pt}
    \caption{\textbf{Full quantitative results of AP@25 on the ScanNetV2 test set. Best performance is in boldface.}}
    \label{table:ScanNetV225}
    \vspace{0.5em}
    % \normalsize
    \scalebox{0.74}{\begin{tabular}{c|c|cccccccccccccccccccc}
      \toprule
      Method &  AP@25 & \rotatebox{90}{bathtub} &\rotatebox{90}{bed} &\rotatebox{90}{bookshe.}	 &\rotatebox{90}{cabinet}&\rotatebox{90}{chair}	&\rotatebox{90}{counter}	&\rotatebox{90}{curtain}&\rotatebox{90}{desk}	&\rotatebox{90}{door}&\rotatebox{90}{other}&\rotatebox{90}{picture}&\rotatebox{90}{frige}&\rotatebox{90}{s. curtain}&\rotatebox{90}{sink}&\rotatebox{90}{sofa}&\rotatebox{90}{table}&\rotatebox{90}{toilet}&\rotatebox{90}{window}\\
      \midrule

      % 3D-BoNet~\cite{qi2018frustum}   &68.7 &	100.0 &	88.7 &	83.6 &	58.7 &	64.3 &	55.0 &	62.0 &	72.4 &	52.2&	50.1 &	24.3 &	51.2 &	100.0 &	75.1 &	80.7 &	66.1 &	90.9 &	61.2 \\
      % MTML~\cite{lahoud20193d}        &73.1  &	100.0 &99.2  &	77.9  &	60.9  &	74.6  &	30.8  &	86.7 &	60.1  &	60.7  &	53.9  &	51.9  &	55.0  &	100.0  &	82.4  &	86.9 &	72.9 &	100.0 &	61.6 \\
      % %GICN~\cite{liu2020learning}     &78.8 &	100.0 &	97.8 &	86.7&	78.1 &	83.3 &	52.7 &	82.4 &	80.6 &	54.9 &	59.6 &	55.1 &	70.0 &	100.0 &	85.3 &	\textbf{93.5} &	73.3 &	100.0 &	65.1 \\
      % 3D-MPA~\cite{engelmann20203d}   &73.7 &100.0 &	93.3 &	78.5&	79.4 &	83.1 &	27.9 &58.8 &	69.5 &	61.6 &	55.9 &55.6 &	65.0 &	100.0 &	80.9 &	87.5 &	69.6 &	100.0 &	60.8\\
      % DyCo3D~\cite{he2021dyco3d}      &76.1 &	100.0 &	93.5 &	89.3 &	75.2 &	86.3 &	60.0 &	58.8 &	74.2&	64.1 &	63.3 &	54.6 &	55.0 &	85.7 &	78.9 &	85.3 &	76.2 &	98.7 &	69.9 \\
      % PE~\cite{zhang2021point}        &77.6 &100.0 &90.0&	86.0 &	72.8 &	86.9&	40.0 &	85.7 &	77.4 &	56.8 &	70.1 &60.2 &	64.6 &	93.3 &	84.3 &	89.0&	69.1 &	99.7 &	70.9\\
      PointGroup~\cite{jiang2020pointgroup} &77.8 &	100.0 &	90.0 &	79.8 &	71.5 &	86.3 &	49.3 &	70.6 &	89.5 &	56.9 &	70.1 &	57.6 &	63.9 &	100.0 &88.0 &85.1 &	71.9 &	99.7 &	70.9 \\
      MaskGroup~\cite{zhong2022maskgroup}   &79.2&	100.0 &	96.8 &	81.2 &	76.6 &	86.4 &	46.0 &	81.5&	88.8 &	59.8 &	65.1 &	63.9 &	60.0 &	91.8 &	94.1 &89.6 &	72.1 &	100.0 &	72.3 \\
      OccuSeg~\cite{han2020occuseg}         & 74.2  &100.0 &	92.3  &	78.5  &	74.5  &	86.7  &	55.7  &	57.8  &	72.9  &	67.0  &	64.4  &	48.8  &	57.7 &	100.0  &	79.4  &	83.0  &62.0 &	100.0  &55.0\\
      HAIS~\cite{chen2021hierarchical}      &80.3 &	100.0 &	\textbf{99.4} &	82.0 &	75.9 &	85.5 &	55.4 &	88.2&	82.7 &	61.5 &67.6 &	63.8 &64.6 &	100.0 &	91.2&	79.7 &	76.7 &	99.4 &	72.6 \\
      SSTNet~\cite{liang2021instance}       & 78.9&	100.0 &84.0 &	\textbf{88.8} &	71.7 &	83.5 &	71.7 &68.4&	62.7 &	72.4 &	65.2 &	72.7 &	60.0 &	100.0 &	91.2 &	82.2 &	75.7 &	100.0 &	69.1\\
      %SoftGroup~\cite{vu2022softgroup}      &86.5&	100.0 &	96.9 &	86.0 &	86.0 &	91.3 &	55.8 &	\textbf{89.9} &	91.1 &76.0 &	\textbf{82.8} &	73.6 &	80.2 &98.0 &	91.9 &	87.5 &	87.7 &	100.0&	82.0  \\
      DKNet~\cite{wu20223d} &81.5 &	100.0 &	93.0 &	84.4 &	76.5 &	91.5 &	53.4 &	80.5 &	80.5 &	80.7 &	65.4 &	76.3 &65.0 &100.0 &79.4 &	88.1 &	76.6 &	100.0 &	75.8 \\   	
      SPFormer~\cite{sun2023superpoint}&85.1 &	100.0 &	99.4&	80.6 &77.4 &	94.2 &	63.7&	\textbf{84.9} &	85.9 &88.9 &72.0 &73.0&	66.5 &	100.0 &	91.1 &	86.8 &	87.3 &	100.0 &	79.6 \\
      %Mask3D~\cite{schult2022mask3d}& 87.0 &	100.0 &	98.5 &	78.2 &	81.8 &	93.8 &	76.0 &	74.9 &	92.3&	87.7 &	76.0&	78.5 &	82.0 &	100.0 &	91.2 &	86.4 &	87.8 &	98.3 &	82.5\\
      %QueryFormer &87.3 &	100.0 &	97.8 &	80.9 &	87.6&	93.7 &	70.2 &	74.9 &	88.4 &	87.5 &	75.5 &\textbf{78.5} &	\textbf{83.5} &	100.0 &	91.2 &	91.6&	86.9 &	100.0 &	82.5\\
        Maft~\cite{lai2023mask}&86.0 &	100.0 &	99.0 &	81.0 &	82.9 &	\textbf{94.9} &	\textbf{80.9} &	68.8 &	83.6 &	\textbf{90.4} &	75.1 &	\textbf{79.6} &	74.1 &	100.0 &	86.4 &	84.8&	83.7 &	100.0 &	\textbf{82.8} \\
        \midrule
        Ours&\textbf{88.2} &	\textbf{100.0} &	97.9 &	88.2 &\textbf{87.9} &	93.7 &	70.3 &	74.9 &	\textbf{91.5} &	87.5 &	\textbf{79.5}&	74.0 &	\textbf{82.0} &	\textbf{100.0} &	\textbf{99.4}&	\textbf{92.3}&	\textbf{89.1} &\textbf{100.0} &	78.8\\
      \bottomrule
    \end{tabular}}
  \end{center}
\end{table}
% \begin{figure}[!h]
%   \begin{center}
%       \includegraphics[width=0.77\textwidth]{map.png}
%       \caption{\textbf{The mAP result of our method on ScanNetV2 test set.
%       }}
%       \label{ap_screenshot}
%   \end{center}
% \end{figure}
% \begin{figure}[!h]
%   \begin{center}
%       \includegraphics[width=0.77\textwidth]{ap_50.png}
%       \caption{\textbf{The AP@50 result of our method on ScanNetV2 test set.
%       }}
%       \label{ap50_screenshot}
%   \end{center}
% \end{figure}
% \begin{figure}[!h]
%   \begin{center}
%       \includegraphics[width=0.77\textwidth]{ap_25.png}
%       \caption{\textbf{The AP@25 result of our method on ScanNetV2 test set.
%       }}
%       \label{ap25_screenshot}
%   \end{center}
% \end{figure}
% \begin{figure}[!h]
%   \begin{center}
%       \includegraphics[width=0.77\textwidth]{ICLR 2025 Template/sup/scannet200ap.png}
%       \caption{\textbf{The mAP result of our method on ScanNet200 test set.
%       }}
%       \label{ap_screenshot}
%   \end{center}
% \end{figure}
% \begin{figure}[!h]
%   \begin{center}
%       \includegraphics[width=0.77\textwidth]{ICLR 2025 Template/sup/scannet200ap50.png}
%       \caption{\textbf{The AP@50 result of our method on ScanNet200 test set.
%       }}
%       \label{ap50_screenshot}
%   \end{center}
% \end{figure}
% \begin{figure}[!h]
%   \begin{center}
%       \includegraphics[width=0.77\textwidth]{ICLR 2025 Template/sup/scannet200ap25.png}
%       \caption{\textbf{The AP@25 result of our method on ScanNet200 test set.
%       }}
%       \label{ap25_screenshot}
%   \end{center}
% \end{figure}

\begin{table}[!ht]
  \begin{center}
    \footnotesize
    \setlength\tabcolsep{3.0pt}
    \caption{\textbf{Full quantitative results of mAP on ScanNet++ test set.} Best performance is in boldface.}
    \label{table:ScanNetppmap}
    %\vspace{0.5em}
    % \normalsize
    \scalebox{0.78}{\begin{tabular}{c|c|cccccccccccccccccccc}
      \toprule
      Method &  mAP & \rotatebox{90}{	bottle} &\rotatebox{90}{box} &\rotatebox{90}{ceiling l.}	 &\rotatebox{90}{cup}&\rotatebox{90}{monitor	}	&\rotatebox{90}{office c.}	&\rotatebox{90}{white. e.}&\rotatebox{90}{tv}	&\rotatebox{90}{white.}&\rotatebox{90}{telephone}&\rotatebox{90}{tap}&\rotatebox{90}{tissue b.}&\rotatebox{90}{trash c.}	&\rotatebox{90}{window}&\rotatebox{90}{sofa}&\rotatebox{90}{pillow}&\rotatebox{90}{plant}&...\\
      \midrule
      PointGroup~\cite{wu20223d}& 8.9 & 0.8 & 2.1&57.3& 13.2 &37.8	&82.8 &0 &39.0& 54.7	&0	&0&0&37.2&3.5&35.7&10.1&22.5&...\\   	
      HAIS~\cite{schult2022mask3d}  & 12.1 & 3.4 &3.8 & 55.9 & 16.8& 49.5	&\textbf{87.1} &0 &64.1 &72.5	&7.2&	0&0&29.5&4.0&49.0&14.9&25.0	&...\\
      SoftGroup~\cite{vu2022softgroup} &16.7& 9.4 &  6.2  & 46.7 & 23.2 &42.8 &81.3&	0& 67.3& 71.6&	10.9	&	14.0&\textbf{2.9}&32.9	&8.1&46.4&17.0&\textbf{60.0}&...\\
      Ours&\textbf{22.2}&\textbf{13.2}&\textbf{12.7} & \textbf{63.7} &\textbf{38.1} &\textbf{69.3} &86.0 &\textbf{38.9}& \textbf{90.6}	&\textbf{86.8}& \textbf{26.7} &\textbf{20.6}&2.0&\textbf{60.0}&\textbf{9.4}&\textbf{63.7}&\textbf{45.3}&52.5&...\\
      \bottomrule
    \end{tabular}}
  \end{center}
\end{table}
\subsection{More Ablation Studies}
\textbf{Difference in Recall and AP across different decoder layers.} As depicted in Table~\ref{table:DifferenceRecallAP}, we conduct an ablation study on ScanNetV2 validation set to examine the impact of our proposed HQFD on recall and AP. 
%
From the table, it is evident that the recall of Maft decreases at the fifth layer, consequently leading to a decline in the corresponding AP and influencing the final prediction results. 
%
In contrast, our approach, which incorporates HQFD, ensures a steady improvement in recall, thereby guaranteeing a consistent enhancement in AP. 
%
This favorable effect on the final output results is attributed to the design of this moudle.
\begin{table}[!ht]
  \begin{center}
    \footnotesize
    \setlength\tabcolsep{5pt}
    \centering
    \caption{\textbf{Difference in Recall and AP across different decoder layers.} (\textcolor{red}{+}) indicates an increase compared to the previous layer, while (\textcolor{blue}{-}) indicates a decrease compared to the previous layer.}
    \label{table:DifferenceRecallAP}
    \begin{tabular}{c|c|ccc||c|ccc}
      \toprule
      % \hline
      
      \multirow{2}*{Layer}&\multicolumn{4}{c}{Ours}&\multicolumn{4}{c}{Maft}\\
      %\cline{2-9}
      & Recall@50 & mAP &AP@50&AP@25&Recall@50 & mAP &AP@50&AP@25\\
      \midrule
      3 & 87.5 & 59.4 &  76.7  & 84.9&85.7&56.9 &  73.9 &  82.5\\
      4& 87.8 (\textcolor{red}{+}) &  59.7 (\textcolor{red}{+})   &77.1 (\textcolor{red}{+}) &  85.1 (\textcolor{red}{+})&86.6 (\textcolor{red}{+})&58.5 (\textcolor{red}{+}) &  75.5 (\textcolor{red}{+}) &  83.7 (\textcolor{red}{+})\\
      5& 87.9  (\textcolor{red}{+})& 59.9 (\textcolor{red}{+}) & 77.3 (\textcolor{red}{+})& 85.3 (\textcolor{red}{+})&85.8 (\textcolor{blue}{-})&58.2 (\textcolor{blue}{-}) & 75.0 (\textcolor{blue}{-}) & 83.5 (\textcolor{blue}{-})\\
      6& \textbf{88.1} (\textcolor{red}{+})& \textbf{60.9} (\textcolor{red}{+}) &\textbf{78.1} (\textcolor{red}{+})&\textbf{85.7} (\textcolor{red}{+})&86.6 (\textcolor{red}{+})&59.0 (\textcolor{red}{+}) & 76.1 (\textcolor{red}{+}) & 84.3 (\textcolor{red}{+})\\
      \bottomrule 
    \end{tabular}
  \end{center}
\end{table}
% \begin{table}[!ht]
%   \begin{center}
%     \footnotesize
%     \setlength\tabcolsep{5pt}
%     \centering
%     \caption{\textbf{Difference in Recall and AP across different decoder layers.} (\textcolor{red}{+}) indicates an increase compared to the previous layer, while (\textcolor{blue}{-}) indicates a decrease compared to the previous layer.}
%     \label{table:DifferenceRecallAP}
%     \begin{tabular}{c|c|ccc||c|ccc}
%       \toprule
%       % \hline
      
%       \multirow{2}*{Layer}&\multicolumn{4}{c}{Ours}&\multicolumn{4}{c}{Maft}\\
%       %\cline{2-9}
%       & Recall@50 & mAP &AP@50&AP@25&Recall@50 & mAP &AP@50&AP@25\\
%       \midrule
%       3 & 92.3 & 60.0 &  78.3  & 85.8&85.7&56.9 &  73.9 &  82.5\\
%       4& 92.7 (\textcolor{red}{+}) & 60.7 (\textcolor{red}{+})   &78.5 (\textcolor{red}{+}) &  86.1 (\textcolor{red}{+})&86.6 (\textcolor{red}{+})&58.5 (\textcolor{red}{+}) &  75.5 (\textcolor{red}{+}) &  83.7 (\textcolor{red}{+})\\
%       5& 92.9 (\textcolor{red}{+})& 61.1 (\textcolor{red}{+}) & 78.9 (\textcolor{red}{+})& 86.2 (\textcolor{red}{+})&85.8 (\textcolor{blue}{-})&58.2 (\textcolor{blue}{-}) & 75.0 (\textcolor{blue}{-}) & 83.5 (\textcolor{blue}{-})\\
%       6& \textbf{93.1} (\textcolor{red}{+})& \textbf{61.7} (\textcolor{red}{+}) &\textbf{79.5} (\textcolor{red}{+})&\textbf{86.5} (\textcolor{red}{+})&86.6 (\textcolor{red}{+})&59.0 (\textcolor{red}{+}) & 76.1 (\textcolor{red}{+}) & 84.3 (\textcolor{red}{+})\\
%       \bottomrule 
%     \end{tabular}
%   \end{center}
% \end{table}
\textbf{Ablation study on $\mathcal{D}_1$ and $\mathcal{D}_2$ of the Hierarchical Query Fusion Decoder.} $D_1$ represents the number of new added queries in each layer compared to the previous layer, while $D_2$ indicates the layers where the fusion operation is performed. From the table data, we can see that performance decreases significantly when $D_2$=4 compared to $D_2$=3. As analyzed in lines 334-336 in the main text, the queries in the earlier layers have not aggregated enough instance information. Therefore, if $D_2$=4, it means that the queries in the second layer will also participate in the fusion operation, but these queries have only undergone two rounds of feature aggregation, resulting in inaccurate mask predictions. This can affect the operation of the Hierarchical Query Fusion Decoder (HQFD). To ensure the effectiveness of HQFD, we recommend performing the fusion operation on the last half of the decoder layers. In fact, we follow this approach in other datasets as well.

\begin{table}[!ht]
  \begin{center}
    \footnotesize
    \setlength\tabcolsep{5pt}
    \centering
    \caption{\textbf{Ablation study on $\mathcal{D}_1$ and $\mathcal{D}_2$ of the Hierarchical Query Fusion Decoder.}}
    \label{table:d1d2}
    \begin{tabular}{cc|ccc}
      \toprule
      % \hline
      $\mathcal{D}_1$ & $\mathcal{D}_2$& mAP & AP@50 &AP@25\\
      \midrule
      50 &2&61.4& 78.9&86.1 \\
      50 &3&61.5 &79.2 &86.3 \\
      50 &4&61.0& 78.5&85.6 \\
      40 &3&\textbf{61.7} &\textbf{79.5} &\textbf{86.5} \\
      60 &3&61.3& 78.8&85.9 \\
      % \hline
      % \ding{51}&\ding{51}&\ding{51}&\ding{51}&\ding{51}&\ding{51}&55.11&37.94&65.43&48.81\\
      % \hline
      \bottomrule
    \end{tabular}
  \end{center}
\end{table}



\textbf{The effectiveness of the SG in Equation~\ref{sg}.} As illustrated in Table~\ref{table:SG}, we performed an ablation study on ScanNetV2 validation set to examine the impact of the SG operation in Equation~\ref{sg}.
%
If we do not utilize SG, $Q^p_0$ remains fixed, which hinders its ability to adaptively learn a distribution suitable for all scenarios, thus impacting the overall performance.
\begin{table}[!ht]
  \begin{center}
    \footnotesize
    \setlength\tabcolsep{5pt}
    \centering
    \caption{\textbf{The effectiveness of the SG in Equation~\ref{sg}.}}
    \label{table:SG}
    \begin{tabular}{c|cc}
      \toprule
      % \hline
      
     Setting&mAP&AP@50\\
      %\cline{2-9}
      W SG &61.4&79.0\\
      W/o SG &\textbf{61.7}&\textbf{79.5}\\
      \bottomrule 
    \end{tabular}
  \end{center}
\end{table}

\textbf{Ablation Study on the hyperparameters in Equation~\ref{lall}.} We perform the experiment in Table~\ref{table:hyperparameters}. 
%
Based on the results, we find that the combination 0.5, 1, 1, 0.5, 0.5 yields the best performance.
\begin{table}[!htbp]
  \begin{center}
    \footnotesize
    %\setlength\tabcolsep{3pt}
    %\renewcommand{\thetable}{{\Roman{table}}}
    \caption{\textbf{Ablation Study on the hyperparameters in Equation~\ref{lall} on ScanNetV2 validation set.}}
    \label{table:hyperparameters}
    % \normalsize
    \scalebox{1}{\begin{tabular}{ccccc|c}
      \toprule
      $\lambda_1$ & $\lambda_2$ & $\lambda_3$& $\lambda_4$ & $\lambda_5$ & mAP \\
      \midrule
     
      1& 1 & 1 &0.5&0.5& 61.1 \\
      0.5& 1 & 1 &0.5&0.5& \textbf{61.7} \\
      1.5& 1 & 1 &0.5&0.5& 61.4 \\
      0.5& 0.5 & 1 &0.5&0.5& 60.8 \\
      0.5& 1.5 & 1 &0.5&0.5& 61.5 \\
      0.5& 1 & 0.5 &0.5&0.5& 61.0 \\
      0.5& 1 & 1.5 &0.5&0.5& 61.2 \\
      0.5& 1 & 1 &1&0.5& 61.0 \\
      0.5& 1 & 1 &0.5&1& 61.5 \\
      \bottomrule
    \end{tabular}}

  \end{center}
\end{table}

\subsection{Assets Availability}
\label{asset}
The datasets that support the findings of this study are available in the following repositories: 

ScanNetV2~\cite{dai2017scannet} at \url{http://www.scan-net.org/changelog#scannet-v2-2018-06-11} under the \href{http://kaldir.vc.in.tum.de/scannet/ScanNet_TOS.pdf}{ScanNet Terms of Use}.
%
ScanNet200~\cite{rozenberszki2022language} at \url{https://github.com/ScanNet/ScanNet} under the \href{http://kaldir.vc.in.tum.de/scannet/ScanNet_TOS.pdf}{ScanNet Terms of Use}.
%
ScanNet++~\cite{yeshwanth2023scannet++} at \url{https://kaldir.vc.in.tum.de/scannetpp} under the \href{https://kaldir.vc.in.tum.de/scannetpp/static/scannetpp-terms-of-use.pdf}{ScanNet++ Terms of Use}.
%
S3DIS~\cite{armeni20163d} at \url{http://buildingparser.stanford.edu/dataset.html} under Apache-2.0 license.
%
The code of our baseline~\cite{lai2023mask,sun2023superpoint} is available at \url{https://github.com/dvlab-research/Mask-Attention-Free-Transformer} and \url{https://github.com/sunjiahao1999/SPFormer} under MIT license.

\subsection{More Visual Comparison}
In Figure~\ref{VisualizationComparison}, we visualize and compare the results of several methods. As shown in this figure's red boxes, our method produces finer segmentation results.
\begin{figure}[!ht]
  \begin{center}
      \includegraphics[width=0.77\textwidth]{graph/compare_supp.pdf}
      \caption{\textbf{Additional Visual Comparison on ScanNetV2 validation set.} The red boxes highlight the key regions.
      }
      \label{VisualizationComparison}
  \end{center}
\end{figure}

\begin{figure}[!ht]
  \begin{center}
      \includegraphics[width=0.77\textwidth]{sup/bfl_intro_1.pdf}
      \caption{\textbf{Visual comparisons between the baseline and our method across different decoder layers on ScanNetV2 validation set.} The red boxes highlight the key regions.
      }
      \label{VisualizationComparison1}
  \end{center}
\end{figure}
\begin{figure}[!ht]
  \begin{center}
      \includegraphics[width=0.77\textwidth]{sup/bfl_intro_2.pdf}
      \caption{\textbf{Visual comparisons between the baseline and our method across different decoder layers on ScanNetV2 validation set.} The red boxes highlight the key regions.
      }
      \label{VisualizationComparison2}
  \end{center}
\end{figure}
\begin{figure}[!ht]
  \begin{center}
      \includegraphics[width=0.77\textwidth]{sup/bfl_intro_3.pdf}
      \caption{\textbf{Visual comparisons between the baseline and our method across different decoder layers on ScanNetV2 validation set.} The red boxes highlight the key regions.
      }
      \label{VisualizationComparison3}
  \end{center}
\end{figure}