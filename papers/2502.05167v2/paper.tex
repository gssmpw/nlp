%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage[table]{xcolor}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

%toggle comments
\newcommand{\hanieh}[1]{\textcolor{blue}{~Hanieh:~#1}}
\newcommand{\ryan}[1]{\textcolor{re}{~Ryan:~#1}}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% ADDED PACKAGES/COMMANDS by Authors
\usepackage{xspace}
\usepackage{multirow}
\newcommand{\framework}{\textsc{NoLiMa}\xspace}
% \usepackage[dvipsnames]{xcolor}
\definecolor{shadedRed}{HTML}{faa7a7}
\usepackage{float}

\usepackage{array} % For advanced table formatting
\usepackage{ragged2e} % For custom alignment
\usepackage{makecell} % For manual line breaks in table cells



% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{\framework: Long-Context Evaluation Beyond Literal Matching}

\newcounter{notecounter}
\newcommand{\enotesoff}{\long\gdef\enote##1##2{}}
\newcommand{\enoteson}{\long\gdef\enote##1##2{{
\stepcounter{notecounter}
{\large\bf
\hspace{0cm}\arabic{notecounter} $<<<$ ##1: ##2
$>>>$\hspace{1cm}}}}}

% \enoteson
\enotesoff

\begin{document}

\twocolumn[
\icmltitle{\framework: Long-Context Evaluation Beyond Literal Matching}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}
\icmlsetsymbol{internship}{*}

\begin{icmlauthorlist}
\icmlauthor{Ali Modarressi}{LMU,MCML,internship}
\icmlauthor{Hanieh Deilamsalehy}{Adobe}
\icmlauthor{Franck Dernoncourt}{Adobe}
\icmlauthor{Trung Bui}{Adobe}
\icmlauthor{Ryan Rossi}{Adobe}
\icmlauthor{Seunghyun Yoon}{Adobe}
\icmlauthor{Hinrich Sch√ºtze}{LMU,MCML}

% \icmlauthor{Firstname3 Lastname3}{comp}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{LMU}{Center for Information and Language Processing, LMU Munich, Germany}
\icmlaffiliation{MCML}{Munich Center for Machine Learning (MCML)}
\icmlaffiliation{Adobe}{Adobe Research}

\icmlcorrespondingauthor{Ali Modarressi}{amodaresi@cis.lmu.de}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution 
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
\makeatletter\renewcommand{\Notice@String}{}\makeatother
\printAffiliationsAndNotice{\textsuperscript{*}Work done during an internship at Adobe Research.}

\begin{abstract}
Recent large language models (LLMs) support long contexts ranging from 128K to 1M tokens. A popular method for evaluating these capabilities is the needle-in-a-haystack (NIAH) test, which involves retrieving a ``needle'' (relevant information) from a ``haystack'' (long irrelevant context). Extensions of this approach include increasing distractors, fact chaining, and in-context reasoning. However, in these benchmarks, models can exploit existing literal matches between the needle and haystack to simplify the task. 
%Needle set definition
To address this, we introduce \framework, a benchmark extending NIAH with a carefully designed needle set, where questions and needles have minimal lexical overlap, requiring models to infer latent associations to locate the needle within the haystack. We evaluate 12 popular LLMs that claim to support contexts of at least 128K tokens. While they perform well in short contexts ($<$1K), performance degrades significantly as context length increases. At 32K, for instance, 10 models drop below 50\% of their strong short-length baselines. Even GPT-4o, one of the top-performing exceptions, experiences a reduction from an almost-perfect baseline of 99.3\% to 69.7\%. Our analysis suggests these declines stem from the increased difficulty the attention mechanism faces in longer contexts when literal matches are absent, making it harder to retrieve relevant information. 
We publicly release the dataset and evaluation code at \href{https://github.com/adobe-research/NoLiMa}{https://github.com/adobe-research/NoLiMa}.
\end{abstract}

\section{Introduction}
In recent years, large language models (LLMs) have made remarkable advancements in handling long-context inputs \cite{chen2023extending, xiong-etal-2024-effective, peng2024yarn}. This capability has unlocked new possibilities in various NLP tasks that require understanding or generating content over extended documents. Examples include long- or multi-document question answering (QA), summarization, and many-shot in-context learning \cite{lee2024longcontextlanguagemodelssubsume, chang2024booookscore, agarwal2024manyshot}. To evaluate these models' effectiveness in handling long contexts, several benchmarks have been developed.
One prominent benchmark is Needle-in-a-Haystack (NIAH), which tests a model's ability to search for and retrieve a specific fact (the ``needle'') hidden within irrelevant information (the ``haystack'')\cite{kamradt2023needle, mohtashami2023randomaccess}. While the baseline NIAH task assesses surface-level retrieval capabilities, recent adaptations have increased its complexity. These enhancements include introducing multiple needles, incorporating additional distractor material, and interconnecting facts to necessitate in-context reasoning (e.g., fact-chaining) \cite{hsieh2024ruler, levy-etal-2024-task, kuratov2024babilong}. 
Other benchmarks, such as long-, multi-document QA, and long conversation understanding, have also been proposed to evaluate long-context comprehension in a more downstream task manner \cite{liu-etal-2024-lost, yen2024helmet, zhang2024inftybenchextendinglongcontext, dong-etal-2024-bamboo, wang-etal-2024-leave, maharana-etal-2024-evaluating}.

Arguably, these tasks share a common foundation: the ability to recall previously seen information \cite{goldman-etal-2024-really}. This broader category, termed association recall tasks, has been extensively studied in machine learning \cite{graves2014neuralturingmachines, ba2016using}. A key argument is that the attention mechanism, which is the underlying foundation of many LLMs, is inherently adept at identifying and recalling associations present in the input \cite{olsson2022incontextlearninginductionheads, arora2024zoology}. However, this raises an important question: Long-context benchmarks feature tasks where the queried input (e.g., a question or a task) has literal matches with the provided context. \textit{Do such literal
matches make it easier for language models to locate relevant information and
output correct answers?}

We argue that many existing long-context benchmarks either explicitly (e.g., synthetic tasks or NIAH-based) or implicitly (e.g., multi-document or long-document QA) contain such literal matches. To address this, we introduce \textbf{\framework}, a benchmark designed to minimize literal overlap between questions and their corresponding needles. In \framework, questions and needles contain keywords that are related through associative links, such as real-world knowledge or commonsense facts. By embedding these needles in a haystack, \framework challenges models to leverage latent associative reasoning capabilities rather than relying on surface-level matching.

We evaluate \framework over 12 state-of-the-art language models, all claiming to support token lengths of at least 128K, including GPT-4o, Gemini 1.5 Pro, and Llama 3.3 70B \cite{hurst2024gpt, team2024gemini, meta2024llama3_3}. Unlike NIAH-based evaluations, which contain literal matches and exhibit near-saturated performance, \framework presents a more demanding challenge that highlights the limitations of these models. We evaluate 12 state-of-the-art language models, all of which claim to support token lengths of at least 128K, including GPT-4o, Gemini 1.5 Pro, and Llama 3.3 70B \cite{hurst2024gpt, team2024gemini, meta2024llama3_3}. Their performance declines noticeably as context length increases, with considerable drops even at 2K‚Äì8K tokens. For instance, at 32K tokens, 10 out of 12 models achieve only half of their short-context performance.

We conduct extensive analyses using \framework, yielding the following insights:
\begin{itemize}
    \item \textbf{Impact of Latent Hops and Fact Direction}: We demonstrate how the number of associative reasoning steps (latent hops) and the ordering of elements within a fact statements influence task performance.
    \item \textbf{Context Length vs. Needle Position}: Our aligned-depth analysis shows that as latent reasoning complexity grows, performance depends more on context length than needle position. Without surface-level cues, longer contexts overwhelm the attention mechanism.
    \item \textbf{Ablation Tests}: We confirm that the presence of literal matches significantly simplifies the task, enabling models to achieve high accuracy in answering questions. In contrast, when literal matches serve as distractors, they severely impair accuracy.
    \item \textbf{Chain-of-Thought (CoT) Prompting and Reasoning-based Models}: While CoT prompting or reasoning-based models such as GPT-o1 \cite{jaech2024openai} improve performance by encouraging step-by-step reasoning, they fail to fully mitigate the challenge, particularly in contexts exceeding 16K tokens.
\end{itemize}
Through \framework, we reveal the limitation of literal matching in long-context benchmarks and introduce a novel approach for evaluating models' latent reasoning in longer contexts.

\section{Related Work}
% To extend long-context evaluation beyond simple retrieval, various efforts have been made. Kamradt \cite{kamradt2023needle} and Hsieh et al. \cite{hsieh2024ruler} expand on this by injecting multiple factual statements (multi-needle), where all needles share a certain pattern that does also appear in the question. Then answering the question requires gathering information from all of those injected facts.
% Another step in increasing task complexity involves introducing inter-needle reasoning. Rather than having a single shared query that matches each needle, one could develop a question that requires one or more reasoning hops across multiple needles, using multiple queries \cite{levy-etal-2024-task, arorazoology}. This can be extended further by filling the entire context with interlinked facts and posing a question that demands a large number of reasoning steps (e.g. ancestral trace challenge or long arithmetic tasks) \cite{zhang-etal-2024-bench, li2024needlebench}, which may require reasoning-facilitating techniques like Chain-of-Thought prompting.

% Some benchmarks increase difficulty by extending contexts with additional facts \cite{kuratov2024babilong, li2024needlebenchllmsretrievalreasoning}. Also in the following we show that many benchmarks contain superficial literal matches between questions and relevant information which eases out the tasks. These approaches often fail to provide a true sense of length generalization capabilities over long contexts. In contrast, \framework keeps task complexity consistent across all context lengths. It avoids literal matches and instead emphasizes more nuanced reasoning. This makes \framework a more challenging evaluation for long-context language models, addressing a gap in existing benchmarks.
% \cite{xiong2024artificial}
% \cite{xu2024stresstesting}
% \cite{an2023eval}
% \cite{yuan2024lv}
% \cite{liu2024repoqa}
% \cite{mohtashami2023randomaccess}
% \cite{bai2025longbenchv2deeperunderstanding}
% \cite{li-etal-2024-loogle}

\input{Tables/literal_match_benchmarks_rouge}
With the increasing popularity of long-context language modeling, numerous benchmarks have been introduced to evaluate this capability. Needle-in-a-Haystack (NIAH) is the most well-known and widely used benchmark \cite{mohtashami2023randomaccess, kamradt2023needle}. However, due to performance saturation, various extensions have been proposed. These include increasing complexity by adding more needles, chaining needles to require inter-needle reasoning (fact-chaining), or incorporating arithmetic or code reasoning \cite{kamradt2023needle, hsieh2024ruler, levy-etal-2024-task, kuratov2024babilong, hengle2024multilingual, zhang2024inftybenchextendinglongcontext, vodrahalli2024michelangelolongcontextevaluations}.
Some tasks increase the complexity to such an extent that they become overly difficult even in short-context scenarios. For instance, BABILong includes tasks that perform poorly (e.g. the counting task achieves 28\% accuracy) even without any irrelevant background text (0K) \cite{kuratov2024babilong}. Similarly, the Ancestral Tree Challenge (ATC) employs extensive fact-chaining, resulting in tasks that are overly complex even for short contexts ($<$1K) \cite{li2024needlebenchllmsretrievalreasoning}.
While such tasks challenge language models in long contexts, they raise the question of whether the tasks are inherently too complex for models to handle, regardless of context length.

\input{Tables/needle_set}
\paragraph{Literal Matching in Long-Context Benchmarks.}
\label{sec:related_literal_matching}
Another frequent pattern in many long-context benchmarks is the presence of literal matches between the facts required to answer a question and the question itself. This fact is not limited to synthetic recall-based tasks (e.g., vanilla NIAH, RULER retrieval-based sets) but also affects downstream-like QA-based benchmarks \cite{hsieh2024ruler, liu-etal-2024-lost, zhang2024inftybenchextendinglongcontext, bai-etal-2024-longbench, yen2024helmet}, which often implicitly include literal matches between the relevant document and the question. Although many of these studies introduce complexity by adding similar documents as distractors, literal matches can still provide cues. These cues may help models focus on potential relevant facts based on matches, as attention mechanisms excel at recalling repetitive patterns \cite{olsson2022incontextlearninginductionheads, arora2024zoology}.
We later demonstrate to what extent literal matches simplify recall-based questions (cf. \ref{sec:literal_match_effect}). To quantify the prevalence of these matches in popular benchmarks, we compute ROUGE (R-1, R-2, and R-L) precision scores\footnote{We use precision as our metric to measure how many of the question's tokens exist in the relevant context, rather than the reverse.} \citep{lin-2004-rouge} between the needle (in recall-based tasks), the relevant document (in multi-document setups), or the full document (in long-document QA) and the corresponding question. This analysis measures the degree of literal overlap between the question and the context.
Based on the scores in Table \ref{tab:literal_match_rouge}, \framework demonstrates significantly less literal overlap compared to other datasets.

\section{\framework}
\label{sec:NOLIMA}
The goal of \framework is to design a task that is
inherently simple to solve through associative reasoning,
but for which surface-level matching has zero utility. As a
result, \framework allows us to cleanly investigate
associative reasoning in long-context scenarios without
confounding from surface-level effects.

The main elements of \framework are similar to vanilla NIAH.
A ``needle'' -- a single key piece of information --
is placed within a ``haystack'', i.e., a long irrelevant
text (in our case, snippets from books). Given a
question, the model is then tested on its ability to find
the needle.    The needle is designed to be a clearly relevant
answer to the question.  In contrast to existing NIAH tasks, we
impose the condition that the question have minimal literal match
with the needle. To achieve this, we design a set of needles 
and corresponding questions, collectively referred to as
a ``needle set.''  Table \ref{tab:needle_set_one} presents
one of the constructed needle set templates (see Appendix
\ref{sec:appx_needle_set} for the full list).  Each needle
consists of a unique character and specific information
about them. Example:
\begin{center}
Actually, Yuki lives next to the \textbf{Semper Opera House}. 
\end{center}
The needle 
contains a keyword ($W_n$, here ``Semper Opera House'') that serves as the critical link
between needle and question. The question is
designed to retrieve this information by asking which
character possesses a specific attribute $W_q$, ``Dresden''
in the example:
\begin{center}
Which character has been to \textbf{Dresden}?
\end{center}
The Semper Opera House is located in Dresden.
Thus, the
model should 
be able to identify the latent association link between
$W_q$ (``Dresden'') in the question
and $W_n$ (``Semper Opera House'')
in the needle.
Since there is no literal overlap between
needle and question, the model
must rely on this latent association link to
retrieve ``Yuki'', the correct answer.
For some of our needles,
the association involves commonsense
reasoning instead of world knowledge. Example: ``Then Yuki
mentioned that he has been vegan for years.'' $\rightarrow$
``Which character cannot eat fish-based meals?''  To push the
limits of the model‚Äôs ability to identify hidden
associations, we include questions that require
two hops to connect $W_q$ with $W_n$, for example:
\begin{center}
Which character has been to \textbf{the state of Saxony}?
\end{center}
Here, the model should tap into its knowledge that
Dresden (and hence the Semper Opera) is located
in the state of Saxony.
This two-hop setup further increases the difficulty of identifying
the latent association of $W_q$ with $W_n$.

To make \framework an effective benchmark for evaluating LLM
long-context abilities, we impose several constraints on
the needle set.  (i) We select keywords that
ensure simplicity -- so that, without irrelevant context,
the associations are clear and the model can identify the
correct answer.  (ii) We randomize the assignment of
character names from a diverse pool to minimize sensitivity
to tokenization problems and mitigate ethnic bias \cite{biasesinLLMs2023Navigli, jiang-etal-2024-peek}. Names
already occurring in the haystacks are excluded.  (iii) We
ensure $W_n$ is uniquely associated with $W_q$, avoid
language-based cues and employ preface phrases to isolate
needles from preceding context.  See Appendix
\ref{sec:appx_needle_set} for details.
\input{Figures/haystack_pipeline.tex}
\subsection{Haystack Filtering Pipeline}
\label{sec:haystack_filtering}
To ensure that the haystack does not contain: (1) Any distracting words that have extreme literal or high semantic similarities with the key points mentioned in the question (2) Any information that explicitly or in an inferrable case be a potential false answer to the question, we devise a filtering process.

\paragraph{Distractor Filtering.} For this step, we use an embedding function, Contriever \citep{izacard2022unsupervised}, to find similar words in the haystack to the keywords of the questions. First we gather all words in the haystack and compute their respective embedding. Then using dot-product similarity we compute their similarity to the question keywords. 
% TODO: Double check the top-k number
We manually inspect the top-20 similar words per each $W_q$ and flag those with high semantic or substring similarity for removal.
% we flag them for removal. 
In the removal process those sentences that contain flagged words are removed from the haystack. 
This initial filtering step helps to avoid an uncontrolled set of superficial distractors that could undesirably disrupt the experimental results. We will discuss the impact of distractors on the model performance in our analysis (Section \ref{sec:literal_match_effect}).

\paragraph{Conflicting Information Filtering.} In this step, we implement a semi-automatic redaction process to detect and remove such conflicting information. As shown in Figure \ref{fig:haystack_pipeline}, this process takes the haystack text‚Äîalready filtered for distractors‚Äîalong with questions from our needle set as input.
Assuming the model should infer cases within short contexts, we scan the input texts in smaller chunks.\footnote{With an 800-character stride and a 1000-character chunk size ($\sim$250 tokens).} To identify potential answers within a chunk, we pair each question with the chunk and input them into an instruction-tuned language model, along with a short instruction and few-shot examples. The model responds with either ``N/A'' (indicating no relevant information was found) or an explanation identifying a possible conflict. Flagged examples are manually reviewed\footnote{All manual reviews---in both filtering steps---were conducted by one of the authors.} to determine whether the identified information should be removed. If no conflicts are found, the text remains unchanged. This process is repeated across all selected haystacks until no further removals are necessary.


\section{Experiments}
% maybe worth providing an overview, or insight into what the goal of the experiments is, then we do just that

\subsection{Dataset Configuration}
In \framework, we use 5 groups of needles, each with two fact-order variations: default and inverted. In the default order, the answer character always precedes the needle keyword $W_n$. In the inverted order, the fact is conveyed with the character name placed after $W_n$. Each group includes 2‚Äì6 keyword sets, with some sets containing multiple $W_q$ items to produce both one-hop and two-hop examples. This setup results in 58 question-needle pairs in total.
To generate the haystacks, we select 10 open-licensed books, ensuring each covers at least 50K tokens. Using the filtering mechanism described in Section \ref{sec:haystack_filtering}, we process the text to prepare it for haystack construction. To mitigate potential memorization issues‚Äîsince these books are publicly available‚Äîwe construct haystacks by concatenating short snippets. Specifically, we iteratively and randomly select a book, extract a continuous snippet (under 250 tokens), and append it to the haystack until it exceeds 2K lines, resulting in haystacks exceeding 60K tokens.
In all experiments, each needle is placed 26 times at equal intervals across the evaluated context length. With 5 randomly generated haystacks, 58 question-needle pairs, and 26 placements per context length, this setup results in 7,540 tests per context length experiment.

\input{Tables/models_results}
\subsection{Models}
For the filtering process, we opted using Llama 3.3 70b instruction tuned model \citep{meta2024llama3_3}. As a control test, for each question, we place its needle in 100 randomly selected chunks to verify whether the model (1) understands the filtering task and (2) is familiar with the facts and capable of inferring the answer. The model achieves a score of 99.8\% in this test, indicating its ability to effectively flag conflicting information from the haystacks.

For the evaluation process, we select five closed-source models: GPT-4o, GPT-4o Mini \cite{hurst2024gpt}, Gemini 1.5 Pro, Flash \cite{team2024gemini}, and Claude 3.5 Sonnet \cite{anthropic2024claude}, along with seven open-weight Llama models: The Llama 3.x model family (3.1 8B, 70B, 405B, and 3.3 70B) \cite{dubey2024llama, meta2024llama3_3}, Mistral Large \cite{mistralLarge2411}, Command R+ \cite{cohere_for_ai_2024}, and Jamba 1.5 Mini \cite{team2024jamba}. All these models are well-known and widely used in long-context setups. 
In our analysis on reasoning-based prompting and models, we evaluate GPT-o1, GPT-o3 Mini \cite{jaech2024openai, o3Mini2024SystemCard}, and DeepSeek-R1 Distill-Llama-70B \cite{deepseekai2025deepseekr1incentivizingreasoningcapability}.
More details regarding model versions and deployment details are described in Appendix \ref{sec:appx_models}.

\subsection{Evaluation Setup \& Metric}
During inference, we use a task template (see Appendix \ref{sec:appx_task_prompts}) that instructs the model to answer the question based on the provided text. Since all questions seek the name of the character mentioned in the needle, any returned answer containing the correct name is considered accurate. Accuracy is reported as the proportion of tests with correct answers.

Models are evaluated on all tasks over context lengths of 250, 500, 1K, 2K, 4K, 8K, 16K, and 32K.
To take into account how models would perform on \framework regardless of
long-context scenario, we control the difficulty of the task by by reporting a \textbf{base score}. Evaluations at context lengths of 250, 500, and 1K are used to compute the base score. These three are the shortest contexts. If a model can solve the task at these lengths, then any deterioration of its performance at greater lengths is expected to be solely due to its difficulties with generalizing over long contexts.
For each question-needle example, we compute the average score over 5 haystacks, then take the maximum score of that example across the 250, 500, and 1K tests. The final base score is obtained by averaging these maximum scores across all question-needle examples.
Inspired by \citet{hsieh2024ruler}, we also report the \textbf{effective length} of each model. While they use the performance of Llama 2 model at a 4K length as a threshold (85.6\%), we define the threshold as 85\% of the base score. Thus, the effective length of a model is the largest tested length that exceeds this threshold.
Additionally, some plots show the \textbf{normalized score}, calculated by dividing the accuracy score by the base score.

\input{Figures/depth_plots}
\subsection{Results}
Table \ref{tab:models_performance} presents the performance results of all \framework tests on the selected models. Most models achieve high base scores, indicating that the designed needle set is relatively simple to answer in shorter contexts.
Even models with base scores exceeding 90.0\% exhibit a significantly shorter effective length than their claimed lengths, generally limited to $\leq$2K tokens, with GPT-4o being an exception. While GPT-4o demonstrates strong overall performance, it fails to generalize effectively beyond 8K tokens. Out of the 12 models, 10 exhibit performance at 32K lengths that is half or less of their base scores. For comparison, in other benchmarks with similar settings, such as BABILong (QA1) \cite{kuratov2024babilong} and RULER \cite{hsieh2024ruler}, Llama 3.1 70B achieves effective lengths of 16K\footnote{In BABILong, the effective length is also based on 85\% of the 0K base performance threshold} and 32K, respectively. However, in \framework, Llama 3.1 70B has an effective length of only 2K and shows a significant drop in performance at 32K lengths (42.7\% vs. 94.3\% base score).
Models such as Claude 3.5 Sonnet, Gemini 1.5 Flash, GPT-4o mini, and Llama 3.1 8B may have weaker base scores, but their effective lengths are calculated relative to these scores. This reveals an interesting observation: a model like Claude 3.5 Sonnet, despite having a lower base score, may underperform in shorter contexts but demonstrate better length generalization than models with higher base scores, such as Llama 3.1 70B and Llama 3.3 70B. In fact, Sonnet even achieves higher raw scores in 4K-token experiments compared to some higher-base-score models.

Model scaling generally improves performance, as seen in the progression from Llama 3.1 8B to 70B, Gemini 1.5 Flash to Pro, or GPT-4o mini to GPT-4o. However, the benefits of scaling diminish at larger scales; for example, the performance gap between Llama 3.1 70B and 405B is smaller (and sometimes worse) than that between 8B and 70B. In general, ``lite'' models such as Gemini 1.5 Flash, GPT-4o mini, and Llama 3.1 8B perform well in shorter contexts ($<$1K tokens) but fail to generalize effectively in longer contexts.

% \input{Figures/normal_vs_inverted}
\subsubsection{Latent hops \& Inversion}
As discussed in Section \ref{sec:NOLIMA}, our needle set also includes examples requiring two-hop associative linking from the question keyword to the needle keyword. To evaluate the impact on length generalization, Figure \ref{fig:one_vs_twohop} presents the normalized performance of two top-performing models on one-hop and two-hop tasks. It is evident that, for the same context lengths, questions involving two-hop latent reasoning steps are more challenging than those requiring one-hop reasoning. Notably, the performance gap between one-hop and two-hop tasks widens with increasing context lengths. GPT-4o demonstrates impressive generalization performance, handling both types of examples effectively even at context lengths up to 4K.

Each group of needles includes both a default and an inverted template and Figure~\ref{fig:normal_vs_inverted} shows that inverted examples are more challenging to answer. We argue this difficulty arises from the model‚Äôs causal attention mechanism, particularly in longer contexts where attention signals weaken. 
In the default template, the question or particularly $W_q$, can link directly to $W_n$, which could contain information about the character‚Äôs name since the name appears earlier in the sequence. This allows the model to backtrace effectively from $W_q$ through $W_n$ to the character. In the inverted template, $W_q$ may still attend to $W_n$, but since the fact is incomplete (the character hasn‚Äôt been stated yet), the model cannot use that attention to resolve the question. Instead, it must rely on weaker signals encoded in the character‚Äôs name to establish the link, which becomes harder with longer contexts due to diminishing attention strength. While these findings shed light on the challenge, deeper mechanistic analysis is beyond the scope of this paper and requires further study.
\input{Figures/hops_inversion_plots}

% The only clue the model can use to identify the needle is $W_q$. The most semantically relevant token the model could match it to is $W_n$. The tokens following $W_n$ would then need to contain traces of information related to $W_n$.
% In the default template, the model can link $W_q$ to $W_n$, and since the character's name appears before $W_n$, it can trace back to the name and provide it as the answer. However, in the inverted template, while the model can still link $W_q$ to $W_n$, no information about the answer character is present before $W_n$. Consequently, to answer the question, the model must trace the relevant information through the answer character's name (or any following tokens.) This implies that the answer character's name or subsequent tokens must encode sufficient information about $W_n$ for the model to establish a link with $W_q$.
% While shorter contexts make it easier for the model to retrieve this information, longer contexts present greater challenges, as the results in Figure \ref{fig:normal_vs_inverted} also suggest.

\input{Figures/full_vs_last2K_demonstrate}
\subsubsection{Needle Placement Depth Analysis}
A common evaluation across NIAH-based benchmarks \cite{kamradt2023needle} examines the impact of needle placement within the context window. In Figure \ref{fig:depth_full_onehop}, we observe a "lost-in-the-middle" effect \cite{liu-etal-2024-lost} in 32K, where model performance dips when the needle appears in the middle of longer contexts.

Additionally, Figure \ref{fig:depth_full_twohop} reveals a key phenomenon: longer contexts in more complex (two-hop) examples dampens the performance distribution over the full sweep depending on their length. In vanilla multi-document or NIAH-based benchmarks \cite{kamradt2023needle, liu-etal-2024-lost}, models perform consistently well when the needle (or gold document) appears at the very beginning or end of the context window, with minimal impact from context length. 
However, in \framework, as task complexity increases in two-hop scenarios, larger context sizes shift the entire trendline downward toward zero, with performance declining even at the edges of the context window.

To further investigate this issue, we devise an alternative setup that focuses on analyzing the last 2K tokens instead of sweeping across the full context. Therefore, we align the placement positions in the last 2K tokens for all context lengths (see Figure \ref{fig:full_vs_last2K_demonstrate}).
% To ensure consistent aligned placements across all context lengths, we adjust the starting point of the context window for each length. For example, when testing a 4K context length, we shift the starting point of the given haystack by 28K tokens further. To align the place begin sweeping from the 50\% depth point (see Figure \ref{fig:full_vs_last2K_demonstrate}).
% This setup ensures that while the total context length is 4K tokens, needle placement starts within the last 2K tokens and progresses toward the end of the window (with 50 evenly spaced placements).
This makes that for a certain token depth the only changing factor in each plotline would be the context length, which in turn means that the model has more tokens that can be attended to.
% are evaluating on the same needles, the same tasks, and the same haystacks. The only changing factor in each plot would be the context length, which in turn means that the model has more tokens that can be attended to.

Based on the final 2K results in Figure \ref{fig:last_2k_onehop}, the one-hop setup confirms our earlier observations from the full-sweep plots. The ‚Äúlost-in-the-middle‚Äù phenomenon---where performance dips toward the center of the context---primarily appears in simpler tasks. Each plotline drops as it moves toward the center, reflecting its dependence on placement position and the way the model encodes positional information.
In contrast, the two-hop scenario appears to be influenced more by attention limitations than by position encoding alone. Figure \ref{fig:last_2k_twohop} reveals that, rather than depth exacerbating performance drops, the plot lines remain relatively stable over the last 2K positions. However, context length significantly reduces the overall performance trends observed in this range.
Llama 3.x models, like many other recent language models, features rotary position embeddings (RoPE) which is a relative PE \cite{su2024roformer}. 
For each token depth in Figure \ref{fig:last_2k_twohop}, as the relative distance between question and fact remains the same regardless of context length, position encoding does not explain the performance drop.
Instead, the main limiting factor is the increased context length: as the number of tokens grows, the attention mechanism struggles to process information effectively. In the absence of strong surface-level cues (e.g., literal matches), locating relevant facts becomes challenging for the model, regardless of their position within long contexts.

\input{Tables/Results_w_reasoning}
\subsubsection{CoT Prompting}
Since \framework examples require an associative reasoning between the needle and question keywords to retrieve the correct answer, in this part we evaluate when the model is prompted to reason in a Chain-of-Thought (CoT) style \citep{CoT-Wei-2022} before returning a final answer (see Appendix \ref{sec:appx_task_prompts} for more details). 
In Table \ref{tab:results_w_reasoning}, we present the results when asked for CoT compared to asking directly for the final answer.
CoT prompting shows improvements over long-context tests and it shows a higher rate of improvement in two-shot. Despite the improvements, the tasks seem to remain challenging. For example, two-hop examples with CoT prompting barely achieve the scores of one-hop examples without CoT and continue to perform poorly on texts 16K tokens or longer. The challenge with CoT prompting is that the questions in \framework are straightforward. They are mentioning a singular clue to the answer, meaning they cannot be further decomposed into simpler steps. This limits the benefits of CoT prompting. However, the difficulty lies in reasoning through the association between the question and the needle, which remains a significant challenge for the model.

To assess the performance of reasoning-based models (e.g., GPT-o1) on \framework, we selected the 10 most challenging needle-question pairs from the 58 available, based on the results summarized in Table \ref{tab:models_performance}.
We refer to this subset as \framework-Hard and present the evaluation results in Table \ref{tab:results_w_reasoningmodels}.
While reasoning-based models outperform CoT prompting on Llama 3.3, they still fail to achieve full-length generalization on this subset. Across all models, performance drops below the 50\% mark at 32K context length.
Notably, base scores are nearly perfect, demonstrating the simplicity of the task‚Äîeven within this designated ``hard'' subset. 
This means that even with intermediate reasoning steps, models still struggle to link the needle to the question in long contexts without surface-level cues.

\input{Tables/Results_w_reasoning_models}
\subsubsection{Ablation Study: Literal match effect}
\label{sec:literal_match_effect}
To examine the simplifying impact of literal matches on results, we define two new sets of tests: (1) \textbf{Direct}: questions that explicitly ask about the fact stated in the needle by stating $W_n$ in the question, resembling a vanilla NIAH evaluation \cite{kamradt2023needle}. (2) \textbf{Multiple Choice (MC)}: questions that maintain the required latent associative reasoning while incorporating literal matches. In this setup, the question includes four character names as answer options‚Äîthree from the haystack and one correct answer from the needle.

As expected, Table \ref{tab:literal_match_ablation} shows that direct examples with a high degree of literal overlap between the question and the needle are straightforward for the model to answer, even in long contexts, consistent with prior findings in RULER \cite{hsieh2024ruler}. Additionally, literal matches significantly aid the model when the questions remain unchanged, and only the multiple-choice format is introduced. The inclusion of literal matches in the multiple-choice setup provides significant guidance to the model. By offering the character names as answer options, including the correct name from the needle, the model can focus its search within a smaller scope. This dramatically simplifies the task of identifying the correct answer, as the literal match serves as a direct hint, reducing ambiguity in the reasoning process.
\input{Tables/literal_match_ablation_results}

\input{Figures/distractor_effect}
\paragraph{Distracting Literal Matches.} While literal matches could serve as cues if they are part of the relevant fact, they can also act as distractors if they are irrelevant to the answer. In Section \ref{sec:related_literal_matching}, we noted that some related benchmarks include similar documents in the context as distractors to test the model‚Äôs ability to discern the correct answer from irrelevant ones. This setup creates matches between the query and both relevant and irrelevant documents or facts. In contrast, \framework allows us to explore a different scenario: when the context contains distracting words overlapping with the question, while the relevant fact has minimal overlap with the query.
We insert a distractor sentence into the haystack (details in Appendix \ref{sec:appx_distractors}) containing $W_q$ but entirely irrelevant to both the needle and the question‚Äôs intent. This setup poses a significant challenge, requiring the model to disregard irrelevant literal overlaps while identifying a relevant fact with no meaningful overlap with the query.
As shown in Figure \ref{fig:distractors}, such distractors have a substantial impact on degrading length generalization. GPT-4o now demonstrates an effective length of just 1K, while Llama 3.3 70B performs even worse. While adding distractors slightly lowers base scores (GPT-4o: 93.8, Llama 3.3 70B: 84.4), the normalized plots still clearly illustrate a performance drop at longer lengths. These results highlight the challenge of resolving queries in contexts where irrelevant overlaps mislead the model, and the relevant fact shares no overlap with the question.

\section{Conclusion}
\framework provides a challenging benchmark for evaluating the reasoning capabilities of large language models in long-context settings. By removing literal overlaps between questions and relevant information, the benchmark tests models' ability to infer and link information within extensive irrelevant content. Our findings show that even state-of-the-art models struggle, especially as context length increases, revealing serious limits in their attention mechanism. While causal attention should theoretically access all previous tokens, models often rely on surface-level cues in longer contexts. This vulnerability becomes more pronounced when the context contains literal matches that fail to connect with the truly relevant fact, causing models to overlook the correct information and focus instead on superficial signals. 
We believe our findings with \framework are likely to extend to downstream applications. For instance, in search engines or RAG systems, a relevant document containing the correct answer may have a lexical gap with the query.
So, even if such a document is retrieved alongside others that likely have higher lexical similarity, language models may struggle to extract the correct answer, as they can become distracted by the lexical overlaps in those other documents.
This work highlights the need for benchmarks that go beyond surface-level retrieval to assess deeper reasoning. 
\framework sets a new standard for evaluating long-context comprehension and emphasizes the importance of developing approaches capable of handling complex reasoning in long contexts.


\section*{Impact Statement}
This paper presents work aimed at advancing the field of long-context language modeling by evaluating and analyzing the most commonly used LLMs. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.
% This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

\section*{Acknowledgments}
We thank Abdullatif K√∂ksal, Leonie Weissweiler, and Amir Hossein Kargaran for their valuable feedback and support, particularly in the early stages of this project.
\bibliography{paper_models, paper_related, paper_other}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Needle Set Design \& Considerations}
\label{sec:appx_needle_set}
In Table \ref{tab:needle_set_full}, we demonstrate the full needle set that we use in \framework. In designing the needle templates, there are multiple considerations involved.
First, all templates in the needle set begin with a small introductory phrase or at least one word (e.g., ``Actually,'' ``In 2013,'') to distinguish themselves from the preceding context. This ensures that the needle's keyword or character is not inadvertently linked to the prior context. Since a newline is appended at the end of each needle, this issue is mitigated if the keyword or character appears at the end of the needle.

\input{Tables/needle_set_full}

Another consideration is that the needle keyword should be uniquely associated with the query keyword. For instance, in the following sentence:
\begin{center}
There was an engineer living in \emph{Cambridge}, named Yuki.
\end{center}

Although the term "Cambridge" is commonly associated with the "United Kingdom," it is not uniquely so; it could also refer to cities in the United States, Canada, or other countries.
Additionally, we aim to avoid relying on language-specific markers. Many cities have distinctive elements in their names, such as orthographic features, morphological structures, or cultural naming conventions, that hint at their linguistic or geographic origins. By minimizing the influence of such markers, the needle design ensures a more rigorous evaluation of the model‚Äôs ability to make meaningful connections based on learned knowledge rather than surface-level linguistic cues.
For each template, we manually curated 2-6 keyword pairs, resulting in a total of 28 keyword pairs. Taking into account the order of fact statements, this generates 58 needle-question pairs.

\section{Models}
\label{sec:appx_models}
In Table \ref{tab:model_details}, we list all the models selected for evaluation. Models that are open weights were deployed using the vLLM library \cite{kwon2023efficient}, with weights obtained from HuggingFace \cite{wolf-etal-2020-transformers}.

\input{Tables/models_details}

\section{Task Prompt Templates \& Inference Settings}
\label{sec:appx_task_prompts}
In Table \ref{tab:prompt_templates}, we present the task prompts used across all evaluations. While we do not employ the commonly used "Let's think step by step" prompt in the Chain-of-Thought (CoT) setup \cite{kojima2022large}, our prompt encourages the model to elaborate and expand its reasoning sufficiently before producing a final answer. To manage the extensive testing scope‚Äî7,540 tests per context length‚Äîwe limit reasoning to three sentences or a maximum of 192 generated tokens.
In the CoT setup, a test is considered successful if the final answer (on the newline) includes the correct answer. This differs with the non-CoT setup, where success is determined based on whether the correct answer is present within the generated output.
For all standard instruction-tuned models, we use greedy decoding during generation. For reasoning-based models, we utilize the default sampling decoding mechanism for GPT-o1 and GPT-o3 Mini, while R1-based models employ top-P sampling with p = 0.95 and a temperature of 0.6. In addition, we cap the maximum number of generated tokens in reasoning-based models at 1536 tokens, including both reasoning and output tokens.
In all models, we apply each model's instruction-tuned chat templates.
\input{Tables/task_prompt_templates}

\section{Distractor Design}
\label{sec:appx_distractors}
To construct and integrate the distractor sentences mentioned in Section \ref{sec:literal_match_effect}, we devised two templates, applied uniformly across all needle-question pairs. Depending on the $W_q$, we use one of the following templates:
\begin{center} 
There was an article about $W_q$ in the daily newspaper.

or

There was a photo of $W_q$ in the daily newspaper.
\end{center}
Some instances of $W_q$ may naturally include an article (e.g., "a" or "an"), making them better suited for the second template, while others fit the first. Regardless of the choice, the templates are designed to remain neutral and unrelated to the intent of the question or the fact stated by any needle.

To minimize interference with the needle, we randomly place the distractor sentence while ensuring a token distance of at least 20\% of the context length. For example, in a 1K-token test, the distractor must be at least 200 tokens away from the needle.
Additionally, to avoid any advantage from proximity to the beginning or end of the context (which may gain extra attention), we restrict placement to between the 20\% and 80\% marks of the context length.
Together, these two constraints leave a span of 40\%-60\% of the context length available for random placement of the distractor sentence.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
