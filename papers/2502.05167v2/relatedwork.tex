\section{Related Work}
% To extend long-context evaluation beyond simple retrieval, various efforts have been made. Kamradt \cite{kamradt2023needle} and Hsieh et al. \cite{hsieh2024ruler} expand on this by injecting multiple factual statements (multi-needle), where all needles share a certain pattern that does also appear in the question. Then answering the question requires gathering information from all of those injected facts.
% Another step in increasing task complexity involves introducing inter-needle reasoning. Rather than having a single shared query that matches each needle, one could develop a question that requires one or more reasoning hops across multiple needles, using multiple queries \cite{levy-etal-2024-task, arorazoology}. This can be extended further by filling the entire context with interlinked facts and posing a question that demands a large number of reasoning steps (e.g. ancestral trace challenge or long arithmetic tasks) \cite{zhang-etal-2024-bench, li2024needlebench}, which may require reasoning-facilitating techniques like Chain-of-Thought prompting.

% Some benchmarks increase difficulty by extending contexts with additional facts \cite{kuratov2024babilong, li2024needlebenchllmsretrievalreasoning}. Also in the following we show that many benchmarks contain superficial literal matches between questions and relevant information which eases out the tasks. These approaches often fail to provide a true sense of length generalization capabilities over long contexts. In contrast, \framework keeps task complexity consistent across all context lengths. It avoids literal matches and instead emphasizes more nuanced reasoning. This makes \framework a more challenging evaluation for long-context language models, addressing a gap in existing benchmarks.
% \cite{xiong2024artificial}
% \cite{xu2024stresstesting}
% \cite{an2023eval}
% \cite{yuan2024lv}
% \cite{liu2024repoqa}
% \cite{mohtashami2023randomaccess}
% \cite{bai2025longbenchv2deeperunderstanding}
% \cite{li-etal-2024-loogle}

\input{Tables/literal_match_benchmarks_rouge}
With the increasing popularity of long-context language modeling, numerous benchmarks have been introduced to evaluate this capability. Needle-in-a-Haystack (NIAH) is the most well-known and widely used benchmark \cite{mohtashami2023randomaccess, kamradt2023needle}. However, due to performance saturation, various extensions have been proposed. These include increasing complexity by adding more needles, chaining needles to require inter-needle reasoning (fact-chaining), or incorporating arithmetic or code reasoning \cite{kamradt2023needle, hsieh2024ruler, levy-etal-2024-task, kuratov2024babilong, hengle2024multilingual, zhang2024inftybenchextendinglongcontext, vodrahalli2024michelangelolongcontextevaluations}.
Some tasks increase the complexity to such an extent that they become overly difficult even in short-context scenarios. For instance, BABILong includes tasks that perform poorly (e.g. the counting task achieves 28\% accuracy) even without any irrelevant background text (0K) \cite{kuratov2024babilong}. Similarly, the Ancestral Tree Challenge (ATC) employs extensive fact-chaining, resulting in tasks that are overly complex even for short contexts ($<$1K) \cite{li2024needlebenchllmsretrievalreasoning}.
While such tasks challenge language models in long contexts, they raise the question of whether the tasks are inherently too complex for models to handle, regardless of context length.

\input{Tables/needle_set}
\paragraph{Literal Matching in Long-Context Benchmarks.}
\label{sec:related_literal_matching}
Another frequent pattern in many long-context benchmarks is the presence of literal matches between the facts required to answer a question and the question itself. This fact is not limited to synthetic recall-based tasks (e.g., vanilla NIAH, RULER retrieval-based sets) but also affects downstream-like QA-based benchmarks \cite{hsieh2024ruler, liu-etal-2024-lost, zhang2024inftybenchextendinglongcontext, bai-etal-2024-longbench, yen2024helmet}, which often implicitly include literal matches between the relevant document and the question. Although many of these studies introduce complexity by adding similar documents as distractors, literal matches can still provide cues. These cues may help models focus on potential relevant facts based on matches, as attention mechanisms excel at recalling repetitive patterns \cite{olsson2022incontextlearninginductionheads, arora2024zoology}.
We later demonstrate to what extent literal matches simplify recall-based questions (cf. \ref{sec:literal_match_effect}). To quantify the prevalence of these matches in popular benchmarks, we compute ROUGE (R-1, R-2, and R-L) precision scores\footnote{We use precision as our metric to measure how many of the question's tokens exist in the relevant context, rather than the reverse.} \citep{lin-2004-rouge} between the needle (in recall-based tasks), the relevant document (in multi-document setups), or the full document (in long-document QA) and the corresponding question. This analysis measures the degree of literal overlap between the question and the context.
Based on the scores in Table \ref{tab:literal_match_rouge}, \framework demonstrates significantly less literal overlap compared to other datasets.