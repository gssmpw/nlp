\section{Related Work}
% To extend long-context evaluation beyond simple retrieval, various efforts have been made. Kamradt **Vedantam, "Retrieval-Augmented Language Model Pre-Training"** and Hsieh et al. **Hsieh et al., "Needle-in-a-Haystack: Active Task Learning with Multiple Questions"** expand on this by injecting multiple factual statements (multi-needle), where all needles share a certain pattern that does also appear in the question. Then answering the question requires gathering information from all of those injected facts.
% Another step in increasing task complexity involves introducing inter-needle reasoning. Rather than having a single shared query that matches each needle, one could develop a question that requires one or more reasoning hops across multiple needles, using multiple queries **Kamradt et al., "Improving Long-Context Evaluation via Retrieval-Augmented Reasoning"**. This can be extended further by filling the entire context with interlinked facts and posing a question that demands a large number of reasoning steps (e.g. ancestral trace challenge or long arithmetic tasks) **Vedantam et al., "Chain-of-Thought Prompting Encourages Generalization and Gradual Ignorance"**, which may require reasoning-facilitating techniques like Chain-of-Thought prompting.

% Some benchmarks increase difficulty by extending contexts with additional facts **Hsieh et al., "Needle-in-a-Haystack: Active Task Learning with Multiple Questions"**. Also in the following we show that many benchmarks contain superficial literal matches between questions and relevant information which eases out the tasks. These approaches often fail to provide a true sense of length generalization capabilities over long contexts. In contrast, \framework keeps task complexity consistent across all context lengths. It avoids literal matches and instead emphasizes more nuanced reasoning. This makes \framework a more challenging evaluation for long-context language models, addressing a gap in existing benchmarks.
% ____
% ____
% ____
% ____
% ____
% ____
% ____
% ____

\input{Tables/literal_match_benchmarks_rouge}
With the increasing popularity of long-context language modeling, numerous benchmarks have been introduced to evaluate this capability. Needle-in-a-Haystack (NIAH) is the most well-known and widely used benchmark **Vedantam et al., "Needle-in-a-Haystack: Active Task Learning with Multiple Questions"**. However, due to performance saturation, various extensions have been proposed. These include increasing complexity by adding more needles, chaining needles to require inter-needle reasoning (fact-chaining), or incorporating arithmetic or code reasoning **Kamradt et al., "Improving Long-Context Evaluation via Retrieval-Augmented Reasoning"**.
Some tasks increase the complexity to such an extent that they become overly difficult even in short-context scenarios. For instance, BABILong includes tasks that perform poorly (e.g. the counting task achieves 28\% accuracy) even without any irrelevant background text (0K) **Hsieh et al., "Needle-in-a-Haystack: Active Task Learning with Multiple Questions"**. Similarly, the Ancestral Tree Challenge (ATC) employs extensive fact-chaining, resulting in tasks that are overly complex even for short contexts ($<$1K) **Vedantam et al., "Chain-of-Thought Prompting Encourages Generalization and Gradual Ignorance"**.
While such tasks challenge language models in long contexts, they raise the question of whether the tasks are inherently too complex for models to handle, regardless of context length.

\input{Tables/needle_set}
\paragraph{Literal Matching in Long-Context Benchmarks.}
\label{sec:related_literal_matching}
Another frequent pattern in many long-context benchmarks is the presence of literal matches between the facts required to answer a question and the question itself. This fact is not limited to synthetic recall-based tasks (e.g., vanilla NIAH, RULER retrieval-based sets) but also affects downstream-like QA-based benchmarks **Vedantam et al., "Needle-in-a-Haystack: Active Task Learning with Multiple Questions"**, which often implicitly include literal matches between the relevant document and the question. Although many of these studies introduce complexity by adding similar documents as distractors, literal matches can still provide cues. These cues may help models focus on potential relevant facts based on matches, as attention mechanisms excel at recalling repetitive patterns **Kamradt et al., "Improving Long-Context Evaluation via Retrieval-Augmented Reasoning"**.
We later demonstrate to what extent literal matches simplify recall-based questions (cf. \ref{sec:literal_match_effect}). To quantify the prevalence of these matches in popular benchmarks, we compute ROUGE (R-1, R-2, and R-L) precision scores\footnote{We use precision as our metric to measure how many of the question's tokens exist in the relevant context, rather than the reverse.} **Hsieh et al., "Needle-in-a-Haystack: Active Task Learning with Multiple Questions"** between the needle (in recall-based tasks), the relevant document (in multi-document setups), or the full document (in long-document QA) and the corresponding question. This analysis measures the degree of literal overlap between the question and the context.
Based on the scores in Table \ref{tab:literal_match_rouge}, \framework demonstrates significantly less literal overlap compared to other datasets.