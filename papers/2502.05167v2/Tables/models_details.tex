\begin{table*}[h]
        \small
	\setlength\tabcolsep{5pt}
	\centering
	\begin{tabular}{lccl}
		\toprule
        \textbf{Model}  &   Context Length  & Open Weights? &   Model Revision \\
        \midrule
        GPT-4o              & 128K & No & \texttt{gpt-4o-2024-11-20} \\
        GPT-4o mini         & 128K & No & \texttt{gpt-4o-mini-20240718} \\
        \midrule
        Llama 3.3 70B       & 128K & Yes & \texttt{meta-llama/Llama-3.3-70B-Instruct} \\
        Llama 3.1 405B      & 128K & Yes & \texttt{meta-llama/Llama-3.1-405B-Instruct} \\
        Llama 3.1 70B       & 128K & Yes & \texttt{meta-llama/Llama-3.1-70B-Instruct} \\
        Llama 3.1 8B        & 128K & Yes & \texttt{meta-llama/Llama-3.1-8B-Instruct} \\
        \midrule
        Gemini 1.5 Pro      & 2M   & No     & \texttt{gemini-1.5-pro-002} \\
        Gemini 1.5 Flash    & 1M   & No     & \texttt{gemini-1.5-flash-002} \\ 
        \midrule
        Claude 3.5 Sonnet   & 200K & No     & \texttt{anthropic.claude-3-5-sonnet-20241022-v2} \\ 
        \midrule
        Jamba 1.5 Mini      & 256K & Yes    & \texttt{ai21labs/AI21-Jamba-1.5-Mini} \\ 
        Command R+          & 128K & Yes    & \texttt{CohereForAI/c4ai-command-r-plus-08-2024} \\ 
        Mistral Large 2     & 128K & Yes    & \texttt{mistralai/Mistral-Large-Instruct-2411} \\
        \midrule
        \multicolumn{4}{c}{\emph{Reasoning-based models}} \\
        GPT-o1              & 128K & No    & \texttt{gpt-o1-2024-12-17} \\ 
        GPT-o3 Mini         & 128K & No    & \texttt{gpt-o3-mini-2025-01-31} \\ 
        DeepSeek R1-DL-70b  & 128K & Yes    & \texttt{deepseek-ai/DeepSeek-R1-Distill-Llama-70B} \\
	\bottomrule
	\end{tabular}
	\caption{Details of the selected models used for evaluation.}
	\label{tab:model_details}
\end{table*}
