% \section{Experiments}
% In this section, we present our experimental setup, empirical results, and subsequent analysis.
% \subsection{Experimental Setup}
\section{Experimental Setup}
\subsection{Dataset}
Baidu-ULTR dataset\footnote{\url{https://github.com/ChuXiaokai/baidu_ultr_dataset/}} provides search sessions with clicks collected from a Chinese search engine, Baidu, and a test set with human annotations. We train all the models with a subset of the click data used in the NTCIR-17 ULTRE-2 task \cite{niu2023overview}. Each query-document pair is represented by 14 scalar features, including term-based features (e.g., BM25) and the relevance score generated by the pre-trained BERT-based ranking model\footnote{\url{https://github.com/lixsh6/Tencent_wsdm_cup2023}}. After excluding sessions with fewer than 10 results or without clicks at the top 10 positions, 485,342 query sessions are kept for training. We split the test set and used 1,446 for validation and 5,201 for testing.
%Here, we used the AllPairs intervention harvesting method \cite{agarwal2019estimating} to estimate propensities. We trained a LambdaMART model to estimate the logging policy scores used in it.
\subsection{Baselines}
To demonstrate the effectiveness and necessity of our proposed method, we compare the following methods. \textbf{BM25:} BM25 \cite{robertson1994some} ranks documents based on term matching. \textbf{Naive:} It directly uses raw click data to train a ranking model without debiasing. \textbf{IPW:} Inverse Propensity Weighting (IPW) \cite{joachims2017unbiased} uses the estimated propensity based on positions to re-weight clicks. \textbf{DLA:} Dual Learning Algorithm (DLA) \cite{ai2018unbiased} simultaneously learns an unbiased ranking model and an unbiased propensity model. \textbf{PRS:} Propensity Ratio Scoring \cite{wang2021non} assigns higher weights to pairs formed with a click and non-clicks with higher observation probability. \textbf{GradRev} and \textbf{Drop:} These two methods \cite{zhang2023towards} mitigate the negative confounding effect between relevance and position by unlearning the relevance in the propensity model. \textbf{UPE:} Unconfounded Propensity Estimation (UPE) \cite{luo2024unbiased} leverages backdoor adjustment to mitigate the over-estimation of propensities. \textbf{IOBM:} Interactional Obsevation-Based Model (IOBM) \cite{chen2021adapting} leverages click information besides positions to estimate propensities.

% \textbf{Evaluation Metrics.} We use normalized Discounted Cumulative Gain (nDCG) \cite{jarvelin2002cumulated} and Expected Reciprocal Rank (ERR) \cite{chapelle2009expected} at 1, 3, 5, and 10. Experimental results are averaged over 5 runs with different random seeds.
\subsection{Evaluation Metrics}
We use normalized Discounted Cumulative Gain (nDCG) and Expected Reciprocal Rank (ERR) at 1, 3, 5, and 10. Experimental results are averaged over 5 runs with different random seeds.


\subsection{Implementation Details} We established our baselines using ULTRA \cite{tran2021ultra}. The input features were initially projected to 64 dimensions, and the architecture of the ranking model is a Deep Neural Network (DNN) with three hidden layers of sizes [64, 32, 16]. For the LSTM block in the query-level click propensity model, we tuned the hidden size to $\{4,8,16\}$, and the number of hidden layers to $\{1,2\}$. We used the optimizer AdamW with a learning rate ranging from 2e-6 to 6e-6. We set the batch size as 30 (the number of queries) and trained 2 epochs. We fixed the size of the ranking list to 10 in the training stage.
% The FFN component in the weight model has one hidden layer with a size equal to half of the hidden size of the LSTM.
\section{Experimental Results}
\subsection{Overall Performance}
% Tab. \ref{real_perform} presents the overall performance of existing ULTR methods and our \m~on the Baidu-ULTR dataset. 
From Tab. \ref{real_perform}, we observe: \textbf{1)} For real-world click data with more complex biases, estimating propensities related only to positions (IPW, DLA) does not outperform the Naive method. 
% \textbf{2)} Methods such as IOBM---which introduces click information to estimate propensities---along with UPE, GradRev, and Drop, which aim to mitigate the negative confounding effect between relevance and positions, can consistently maintain their effectiveness. 
\textbf{2)} Methods that model complex user behaviors (i.e., IOBM) and disentangle observation and relevance for strong logging policies (i.e., UPE, GradRev, and Drop) are effective on real-world click data.  
% \textbf{3)} PRS performs the worst, indicating that its assumption---non-clicked documents with lower observation probability are more likely to be relevant---indeed does not hold in real-world click data. 
\textbf{3)} PRS performs the worst, indicating that its assumption---non-clicked documents with higher observation probability are more likely to be true negatives---does not hold in real-world click data. 
\textbf{4)} Our \m~achieves significant improvements over the above best-performing method IOBM, validating the dual click propensity hypothesis in mitigating complex biases in real-world click data. 
\begin{table}[t]
\setlength{\tabcolsep}{0.2pt}
\footnotesize
 \centering
  % \caption{Performance comparison among \m, separate components of \m, and all the baselines, displaying averaged results over 5 runs with different random seeds. The best method is in \textbf{bold}, and the best baseline is \uline{underlined}. ``$*$'' and ``$\dag$'' indicate significant improvements ($p\leq0.05$) over the Naive method, and the best baseline, respectively. ``$-$'' indicates significant degradations ($p\leq0.05$) over the \m.}
  \caption{Average performance from runs with 5 random seeds. \m, its ablations, and baselines are compared. The best overall and baseline performance is marked in \textbf{bold} and \uline{underlined}. Significant improvements over Naive, the best baseline, and significant degradations from \m~are marked with ``$*$'', ``$\dag$'', and ``$-$'' respectively ($p\leq0.05$).}
 \begin{tabular}{cllllllll}\toprule
   \multirow{2}{*}{Methods} & \multicolumn{4}{c}{nDCG@K} & \multicolumn{4}{c}{ERR@K}
    \\\cmidrule(r){2-5}\cmidrule{6-9}
             & K=1 & K=3 & K=5 & K=10 & K=1 & K=3 & K=5 & K=10\\\midrule
BM25&0.4139&0.4290&0.4445&0.4769&0.1395&0.2203&0.2480&0.2696\\\hline
Naive&0.4365&0.4518&0.4654&0.4948&0.1522&0.2359&0.2631&0.2838\\\hline
PRS&0.3382&0.3554&0.3713&0.4085&0.1215&0.1929&0.2185&0.2408\\\hline
IPW&0.4349&0.4492&0.4627&0.4925&0.1532&0.2365&0.2633&0.2839\\
DLA&${0.4392}^*$&0.4525&0.4660&0.4952&0.1527&0.2361&0.2633&0.2840\\\hline
GradRev&${0.4377}$&${0.4546}^*$&${0.4679}^*$&${0.4974}^*$&${0.1525}$&${0.2372}^*$&${0.2643}^*$&${0.2849}^*$\\
Drop&${0.4391}^*$&${0.4535}^*$&${0.4667}^*$&${0.4961}^*$&${0.1528}$&${0.2367}^*$&${0.2639}^*$&${0.2845}^*$\\
UPE&${0.4407}^*$&${0.4557}^*$&${0.4684}^*$&${0.4983}^*$&$\uline{0.1540}^*$&$\uline{0.2382}^*$&${0.2652}^*$&${0.2859}^*$\\\hline
IOBM&$\uline{0.4413}^*$&$\uline{0.4566}^*$&$\uline{0.4701}^*$&$\uline{0.5002}^*$&${0.1537}^*$&${0.2381}^*$&$\uline{0.2654}^*$&$\uline{0.2861}^*$\\\hline
\m&$\textbf{0.4429}^{*}$&$\textbf{0.4582}^{*\dag}$&$\textbf{0.4719}^{*\dag}$&$\textbf{0.5020}^{*\dag}$&$\textbf{0.1545}^{*}$&$\textbf{0.2391}^{*\dag}$&$\textbf{0.2664}^{*\dag}$&$\textbf{0.2871}^{*\dag}$\\
${-\textrm{PL-IPW}}$&${0.4391}^-$&${0.4556}^-$&${0.4691}^-$&${0.4990}^-$&${0.1540}$&${0.2383}$&${0.2654}^-$&${0.2861}^-$\\
${-\textrm{QL-IPW}}$&${0.4392}^-$&${0.4525}^-$&${0.4660}^-$&${0.4952}^-$&${0.1527}^-$&${0.2361}^-$&${0.2633}^-$&${0.2840}^-$
    \\\bottomrule
 \end{tabular}
 \label{real_perform}
\end{table}

\subsection{Ablation Study}
\m~comprises QL-IPW and PL-IPW. To validate the necessity of introducing both components, we conduct an ablation study, as shown in Tab. \ref{real_perform}. The PL-IPW here is essentially DLA. We can see that \m~significantly outperforms either component alone, confirming the importance of their combination.

\subsection{Fine-grained Comparison}
We further evaluate the performance of different ULTR methods on three levels of search frequencies. Since almost all metrics show similar trends, we take nDCG@10 as an example as shown in Fig. \ref{freq}. \m~demonstrates significant performance improvement on low-frequency queries, while the improvement on mid-frequency queries is marginal. On high-frequency queries, \m's performance is on par with methods other than IPW.


\subsection{Click Weight Analysis}
We compare the click weight learned by different ULTR methods: IPW learned by DLA and UPE, and combined IPW learned by \m, as shown in Fig. \ref{weight1}. Since the PL-IPW learned by \m~and the IPW learned by DLA exhibit minimal differences, we omit the presentation of the former. Compared to the IPW learned on synthetic click data, which increases with position, the IPW learned by DLA on the real-world click data differs significantly, whereas UPE shows minimal difference. Compared to DLA, \m~adjusts the click weight by increasing it for clicks at the top ranks and decreasing it for lower ranks. Since queries with more relevant documents have higher click propensities, the click weight adjustment indicates that queries with a single click at lower ranks tend to have more relevant documents.

\begin{figure}[!t]
\centering
\subfigure{
\includegraphics[width=0.23\textwidth]{figures/freq.pdf}
\label{freq}}\subfigure{
\includegraphics[width=0.25\textwidth]{figures/weight2.pdf}
\label{weight1}}
% \caption{Fine-grained comparison of different ULTR methods. (a) Performance comparison of different ULTR methods versus three levels of search frequencies. (b) Normalized click weight of different ULTR methods.}
\caption{Fine-grained analysis and click weights.}
\end{figure}

