\section{Preliminary}
\subsection{Dual Learning Algorithm (DLA)}
% DLA \cite{ai2018unbiased} is an effective method for mitigating position bias. Based on the examination hypothesis \cite{richardson2007predicting} as follows, where $c_d$, $r_d$, and $o_d$ represent whether a document d is clicked, relevant, and observed,
% \begin{equation}
% p(c_d=1)=p(r_d=1)\cdot p(o_d=1)\text{.}
% \end{equation}
DLA \cite{ai2018unbiased} is an effective method for mitigating position bias based on the user examination hypothesis \cite{richardson2007predicting}: 
\begin{equation}
p(c_d=1)=p(r_d=1)\cdot p(o_d=1)\text{,}
\end{equation}
where $c_d$, $r_d$, and $o_d$ represent whether a document d is clicked, relevant, and observed.
% DLA treats the problem of unbiased learning to rank and observation probability (i.e., propensity) estimation as a dual problem. Given a query $q$ from the total query set $Q$ with its document list $\pi_q$, similar to the core idea of inverse propensity weighting (IPW) \cite{joachims2017unbiased}---optimizing a ranking model with inverse propensity weighted loss---DLA jointly learns an unbiased ranking model $f$ and an unbiased propensity model $g$ by optimizing the local losses as:
DLA treats the estimation of relevance and propensity as a dual problem. Given a query $q$ and its document list $\pi_q$, DLA alternates the learning of an unbiased ranking model $f$ and an unbiased propensity model $g$ by inverse propensity weighting (IPW) and inverse relevance weighting (IRW):
\begin{small}
\begin{equation}
l_{IPW}(f,q)=\!\!\!\!\!\!\!\!\sum_{d\in \pi_q, c_d=1}\!\!\!\!\frac{l(f(x_d),c_d)}{p(o_d=1)}\text{, }l_{IRW}(g,q)=\!\!\!\!\!\!\!\!\sum_{d\in \pi_q, c_d=1}\!\!\!\!\frac{l(g(k_d),c_d)}{p(r_d=1)}\text{,}
\end{equation}\end{small}
where $k_d$, $x_d$, and $l$ denote the document at position $k$, features of $d$ regarding $q$, and the loss function, respectively.
% \subsection{Preliminary Analysis}
\subsection{Pilot Study}
\label{data_ref}
% To mitigate relevance saturation bias, we first investigate how to distinguish its severity across different queries, followed by exploring representative features to accurately characterize it.

% To mitigate relevance saturation (RS) bias, we first investigate how it varies across different queries, and then explore discriminative features to characterize it.

% \textbf{The severity of relevance saturation bias can be distinguished by click positions of queries.} Given users' limited patience, queries with more severe relevance saturation bias have more false negatives (relevant but non-clicked) in their result lists, thus, leading to poorer performance when trained on them. We found it hard to distinguish the severity of this bias among different queries through several unsupervised query performance prediction (QPP) methods. Eventually, we discovered that click positions of queries could effectively distinguish the severity. To illustrate the discovery more intuitively, We conducted experiments on single-click queries, which make up two-thirds of the click data. These queries were categorized into 10 groups based on click positions, with a separate ranking model trained for each group. Each experiment was run 5 times with different random seeds. The average performance (nDCG@10) of each model on the test set, along with the proportion of queries in each group, is shown in Fig. \ref{single}. Though groups with single clicks beyond the fifth position may be insufficient to train ranking models, the performance difference among models trained on single clicks at the first four positions can reflect the different severity of relevance saturation bias.
\subsubsection{The Relationship Between Click Positions and Relevance Saturation (RS) Bias:} A more severe relevance saturation (RS) bias leads to a higher number of false negatives (relevant but non-clicked results), which in turn can degrade model performance due to inaccurate labels. To explore how RS bias varies across query sessions, we divide the Baidu-ULTR query sessions into subsets and evaluate models trained on each subset. Since we have limited query-level information, we use click positions within a query as the partition criterion. For simplicity, we focus on single-click sessions, which account for two-thirds of the sessions with clicks. 

Fig. \ref{single} shows the average nDCG@10 for models trained on single-click sessions at different positions, using results from 5 random seeds. We focus primarily on the first four positions, as they constitute about 92\% of all the single-click sessions. The results show that sessions with single clicks at positions 1 and 4 perform significantly better than those at positions 2 and 3, suggesting less severe RS bias and fewer false negatives. Notably, the model trained on single-click sessions at position 4, despite having much less training data, still performs relatively well. This could be attributed to the way search results are displayed on mobile devices, where the first three results typically fit on the screen. Users may scroll down and click the fourth result when the first three are irrelevant, while clicking the second or third result does not imply that the results before them are irrelevant.

% \textbf{Features for estimating relevance saturation bias.} We then explore how to estimate relevance saturation bias for queries with different click positions, i.e., for different click sequences. We can directly use the model performance on the test set to measure it, which is limited by the constraint of the test set. We discover a representative feature, the divergence of the distribution of maximal-score positions from its original click sequence. We first trained a ranking model with DLA and used it as a surrogate model to predict relevance scores. 
\subsubsection{Features for Estimating Relevance Saturation Bias:} We then explore the features that can distinguish RS bias for queries with different click sequences. We attempt to estimate result quality, correlated to RS bias, with a surrogate ranking model. We train the model with DLA using sessions with clicks and use it to predict relevance scores. We tried several unsupervised query performance prediction (QPP) methods to estimate result quality but they are not distinctive regarding the single-click sessions at different positions. We then explore and identify another discriminative feature. For a click sequence, $\mathbf{cs}$, with a single click, e.g., ($1,0,\dots,0$), we calculate the proportions of the maximal score of the list occurring at each position and obtain a distribution of the maximal-score positions, denoted as $D_{mp}^{\mathbf{cs}}$ according to: 
% Then, we extracted the distribution of maximal-score positions $D_{mp}^{\mathbf{cs}}$ for a click sequence $\mathbf{cs}$ (e.g., the $\mathbf{cs}$ of queries with clicks at position one is ($1,0,\dots,0$)) as shown in Eq. \ref{dms} and Fig. \ref{whole} (left),
\begin{small}
\begin{equation}
\label{dms}
    D_{mp}^{\mathbf{cs}}=(\frac{\sum_{q\in Q,\mathbf{cs}^q=\mathbf{cs}}\mathbb{I}(i ==\arg\max_{j \in \{1,\dots, 10\}} s_j)}{\sum_{q\in Q}\mathbb{I}(\mathbf{cs}^q==\mathbf{cs})})_{i=1,\dots,10}\text{,}
\end{equation}\end{small}where $\mathbf{cs}^q$ indicates the click sequence of query $q$, $s_j$ represents the relevance score of the document at position $j$ predicted by the surrogate model, and $\mathbb{I}$ is an indicator function. Fig. \ref{whole} (left) also illustrates how $D_{mp}^{\mathbf{cs}}$ is calculated. 

% Due to the obvious performance difference in the models trained with single-click sequences at the first four positions, we extracted their corresponding $D_{mp}^{\mathbf{cs}}$. By comparing the performance in Fig. \ref{single} with the distribution in Fig. \ref{single_feature}, we observed that for a click sequence, the more consistent $D_{mp}^{\mathbf{cs}}$ and $\mathbf{cs}$ are, the better the performance, i.e., less severe relevance saturation bias. Thus, we can leverage the divergence of $D_{mp}^{\mathbf{cs}}$ from its original $\mathbf{cs}$ to estimate relevance saturation bias for different click sequences.

We extracted \( D_{mp}^{\mathbf{cs}} \) for single-click sessions at the first four positions and present the four distributions in Fig. \ref{single_feature}. By comparing Fig. \ref{single} and Fig. \ref{single_feature}, we observe that the more consistent \( D_{mp}^{\mathbf{cs}} \) is with \( \mathbf{cs} \), the better the model's performanceâ€”indicating less severe relevance saturation bias. This suggests that we can use the divergence of \( D_{mp}^{\mathbf{cs}} \) from its original \( \mathbf{cs} \) to estimate relevance saturation bias across different click sequences.

\begin{figure}[!t]
\centering
\hspace{-1mm}
\subfigure{
\includegraphics[width=0.23\textwidth]{figures/single.pdf}
\label{single}}\subfigure{
% \caption{22}
\includegraphics[width=0.24\textwidth]{figures/feature.pdf}
\label{single_feature}}
% \caption{Experiments using query sessions with a single click. (a) The proportion of single-click sessions at each position and performance of different ranking models trained by 10 single-click groups. (b) The distribution of maximal-score positions of single-click sessions at positions 1, 2, 3, and 4.}
\caption{(a) The distribution of single-click sessions at each position and performance of models trained by 10 single-click groups. (b) The distribution of maximal-score positions of single-click sessions at positions 1, 2, 3, and 4.}
\end{figure}


% \begin{minipage}[c]{0.24\linewidth}
% \includegraphics[width=\linewidth]{figures/single.pdf}
% % \caption{1}
% \label{single}
% \end{minipage}
% \begin{minipage}[c]{0.22\linewidth}
% \includegraphics[width=\linewidth]{figures/feature.pdf}
% % \caption{2}
% \label{single_feature}
% \end{minipage}
% \begin{minipage}[c]{0.5\linewidth}
% \includegraphics[width=\linewidth]{figures/whole1.png}
% \caption{3}
% \label{whole}
% \end{minipage}