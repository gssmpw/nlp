\section{Methodology}
% We can add this paragraph back in the cameral-ready version if more space is allowed.
% We first propose a dual click propensity hypothesis to characterize relevance saturation bias. Then we propose the Dual Inverse Propensity Weighting (\m) method to simultaneously mitigate relevance saturation bias and position bias. We theoretically analyze that \m~can learn unbiased ranking and propensity models.
% \subsection{Bias Analysis and Mitigation}
\subsection{Dual Click Propensity Hypothesis}
\label{theory}
We assume that queries with more relevant documents are more likely to receive clicks, although only one or two. More relevant results lead to more severe relevance saturation bias. We refer to the probability of a query $q$ receiving clicks as the query-level click propensity (denoted as $cp_q$). Based on this assumption, we propose a new click hypothesis, where $c_q$ represents whether a query $q$ receives clicks:\looseness=-1
\begin{equation}
\label{hypothesis}
p(c_d=1)=p(o_d=1)\cdot p(r_d=1)\cdot p(c_q=1)\text{.}
\end{equation}
To mitigate relevance saturation bias and position bias, we propose Dual Inverse Propensity Weighting (\m) with inverse query-level and position-level propensity to compute loss:
\begin{equation}
\mathcal{L}_{\m}(f)=\int_{q\in Q} \sum_{d\in \pi_q, c_d=1}\frac{ l(f(x_d),c_d)\text{d}p(q)}{p(o_d=1)\cdot p(c_q=1)}\text{.}
\end{equation}
% where $q$ is an instance from $Q$ with $q\sim p(q)$.
where $q$ is drawn from $Q$ according to $q\sim p(q)$.
Theoretically, $\mathcal{L}_{\m}(f)$ is an unbiased estimation of the global loss $\mathcal{L}(f)$, where $\mathbf{o}^q=(o_d)_{d\in \pi_q}$, and $\mathbf{c}=(c_q)_{q\in Q}$,
\begin{footnotesize}
\label{proof}
\begin{align}
\mathbb{E}_{\mathbf{o}^q,\mathbf{c}}[\mathcal{L}_{\m}(f)]
&=\mathbb{E}_{\mathbf{c}}[\int_{q\in Q}\mathbb{E}_{\mathbf{o}^q}[\sum_{\text{d}\in \pi_q, c_d=1}\frac{ l(f(x_d),c_d)\text{d}p(q)}{p(o_d=1)\cdot p(c_q=1)}]]\nonumber\\
&=\mathbb{E}_{\mathbf{c}}[\int_{q\in Q}\mathbb{E}_{\mathbf{o}^q}[\sum_{d\in \pi_q, r_d=1}\frac{o_d\cdot c_q\cdot l(f(x_d),r_d)\text{d}p(q)}{p(o_d=1)\cdot p(c_q=1)}]]\nonumber\\
&=\mathbb{E}_{\mathbf{c}}[\int_{q\in Q}\sum_{d\in \pi_q, r_d=1}\frac{c_q\cdot l(f(x_d),r_d)\text{d}p(q)}{p(c_q=1)}]\\
&=\int_{q\in Q} \sum_{d\in \pi_q,r_d=1} l(f(x_d),r_d)\text{d}p(q)=\mathcal{L}(f)\nonumber\text{.}
\end{align}\end{footnotesize}The proof consists of two levels: 1) Inner Level: mitigating position bias at the position level within each query with inverse position-level propensity weighting (\textbf{PL-IPW}), and 2) Outer Level: mitigating relevance saturation bias at the query level across different queries with inverse query-level click propensity weighting (\textbf{QL-IPW}). Since they are isolated, we can similarly prove that the \m~can learn an unbiased propensity model as shown in \cite{ai2018unbiased}.

\subsection{Query-Level Click Propensity Estimation}
% To mitigate relevance saturation bias, we propose \ql. From Section \ref{data_ref}, by leveraging the divergence of $D_{mp}^{\mathbf{cs}}$ from its original click sequence $\mathbf{cs}$, we can estimate relevance saturation bias for each click sequence, allowing us to estimate the query-level click propensity as well. In this way, however, we need to extract $D_{mp}^{\mathbf{cs}}$ for each click sequence, which is cumbersome as a 0/1 click sequence of n positions has $2^n$ possible combinations. For simplicity, when estimating query-level click propensities of multi-click sequences, we treat multi-click sequences as combinations of multiple single-click sequences. Thus, given a click sequence $\mathbf{cs}$, its query-level click propensity can be calculated as follows, where the subscript $i$ represents the single-click sequence with a click at position i,
Section \ref{data_ref} shows that the divergence of $D_{mp}^{\mathbf{cs}}$ from its original click sequence $\mathbf{cs}$ can indicate relevance saturation bias, so we use the divergence between them to estimate the query-level click propensity. Since the click sequence of $n$ positions has $2^n$ possible combinations, we need to calculate $2^n$ distributions for all possible sequences, which is cumbersome. For simplicity, we treat multi-click sequences as combinations of multiple single-click sequences. Thus, given a click sequence $\mathbf{cs}$, its query-level click propensity can be calculated as the average propensity of single-click sequences, where the subscript $i$ represents the single-click sequence with a click at position i,
\begin{equation}
cp_{\mathbf{cs}}=\frac{\sum_{i=1,cs_i=1}^{10}cp_{\mathbf{cs}^i}}{\sum_{i=1}^{10}\mathbb{I}(cs_i==1)}\text{.}
\end{equation}
Then we only need to estimate the click propensities of single-click sequences. 
Considering that the divergence at each position contributes differently to the final query-level click propensity, we use the log-ratio terms, ${(cs}_ilog\frac{{cs}_i}{D_{mp}^{\mathbf{cs}}(i)})_{i=1,\dots,10}$, as the listwise input. 
% Then, we construct an LSTM model to adaptively learn the interacted hidden representation of these log-ratio terms, extract the hidden state of the last position (considering that documents in lower positions being relevant can reflect more severe relevance saturation bias), and employ a feed-forward network (FFN) to predict the query-level click propensity as shown in Fig. \ref{whole} (right). 
Then, as shown in Fig. \ref{whole} (right), we employ an LSTM model to encode the hidden representations of these log-ratio terms and use a feed-forward network (FFN) on the last hidden state of the sequence to predict the query-level click propensity. We use the last state since lower positions could be more important in reflecting relevance saturation bias. We smooth the original click sequence with softmax: $\mathbf{cs}'=\textrm{softmax}(\mathbf{cs}/\tau)$. Formally, the output of the query-level click propensity model for a specific click sequence $\mathbf{cs}$ is as follows:
\begin{equation}
\label{qw}
h(\mathbf{cs})\!=\!\textrm{FFN}(\textrm{LSTM}(t_1,\dots,t_{10})[-1])\text{, }t_i\!=\!{cs}_i'log\frac{{cs}'_i}{D_{mp}^{\mathbf{cs}}(i)}\text{,}
\end{equation}
where $h$ denotes the query-level click propensity model. The final predicted query-level click propensity of each single-click sequence is calibrated by the $\textrm{softmax}$ function as,
\begin{equation}
({cp}_{\mathbf{cs}^1},\dots,{cp}_{\mathbf{cs}^{10}})=\textrm{softmax}(h(\mathbf{cs}^1),\dots,h(\mathbf{cs}^{10}))\text{.}
\end{equation}
\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/model.png}
    \caption{Query-level click propensity estimation.}
    \label{whole}
\end{figure}
\subsection{Learning Algorithm}
% As analyzed in Section \ref{theory}, an unbiased ranking model $f$ and an unbiased propensity model $g$ can be learned by optimizing Eq. \ref{ipw} and Eq. \ref{irw}. We leverage the dual learning algorithm (DLA) to jointly train the ranking model and propensity model. 
% To ensure that the two propensities mitigate biases at different levels, the query-level click propensity model is fixed during position-level propensity model learning. Since the inverse query-level click propensity adjusts weights across queries, which is independent of relevance learning within a query. Moreover, the input features of the query-level click propensity model and the ranking model are independent. Therefore, we update the query-level click propensity model during ranker learning.
In contrast to DLA which learns a position-level propensity model and a relevance model, \m~ has an additional query-level propensity model $h(\mathbf{cs})$ in Eq. \ref{qw} to learn. Since the query-level propensities use listwise information of a click sequence independent of LTR features, we update $h(\mathbf{cs})$ and relevance model $f$ simultaneously according to Eq. \ref{ipw}. To separate the effect of query-level and position-level propensity, we freeze $h(\mathbf{cs})$ during the position-level propensity model $g$ learning, in Eq. \ref{irw}:
\begin{small}
\begin{align}
    l(f,h,q)&=-\frac{cp_{\mathbf{cs}^1}}{cp_{\mathbf{cs}^q}}\cdot\sum_{d\in \pi_q,c_d=1}\frac{g(k_1)}{g(k_d)}\cdot \textrm{log}\frac{e^{f(x_d)}}{\sum_{d'\in \pi_q} e^{f(x_{d'})}}\text{,}\label{ipw} \\
    l(g,q)&=-\frac{cp_{\mathbf{cs}^1}}{cp_{\mathbf{cs}^q}}\cdot\sum_{d\in \pi_q,c_d=1}\frac{f(x_1)}{f(x_d)}\cdot \textrm{log}\frac{e^{g(k_d)}}{\sum_{d'\in \pi_q} e^{g(k_{d'})}}\text{.} \label{irw}
\end{align}
\end{small}

