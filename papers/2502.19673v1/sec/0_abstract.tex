\begin{abstract}

%This work proposes a training-free approach to diffusion based personalization. 
%We strive for optimal content-style composition and develop a highly synergized set of constraints to fuse content and style information without any leakage.
%We leverage from domain-expert models specialized in style-description and subject classification to faithfully encode the desired style characteristics while preserving subjects identity during the generation process. 

Diffusion models are increasingly popular for generative tasks, including personalized composition of subjects and styles. While diffusion models can generate user-specified subjects performing text-guided actions in custom styles, they require fine-tuning and are not feasible for personalization on mobile devices. Hence, tuning-free personalization methods such as IP-Adapters have progressively gained traction. However, for the composition of subjects and styles, these works are less flexible due to their reliance on ControlNet, or show content and style leakage artifacts. To tackle these, we present SubZero, a novel framework to generate any subject in any style, performing any action without the need for fine-tuning. We propose a novel set of constraints to enhance subject and style similarity, while reducing leakage. Additionally, we propose an orthogonalized temporal aggregation scheme in the cross-attention blocks of denoising model, effectively conditioning on a text prompt along with single subject and style images. We also propose a novel method to train customized content and style projectors to reduce content and style leakage. Through extensive experiments, we show that our proposed approach, while suitable for running on-edge, shows significant improvements over state-of-the-art works performing subject, style and action composition. 

\vspace{- 1.2 em}

% Diffusion models have become increasingly popular for performing generative tasks, including personalized composition of subjects and styles. PEFT-based methods are utilized for generating custom objects performing text-driven actions in custom styles. However, these methods require fine-tuning and are not feasible to use for personalization on mobile devices.
% Hence, tuning-free personalization methods such as IP-Adapters have progressively gained traction. However, for the composition of subjects and styles, these works are either limited to less flexibility in text generation process due to their reliance on controlnet, or display content and style leakage artifacts, observed in DDIM inversion. Hence, we present SubZero, a novel framework which given single reference images, generates any subject in any style, performing any action without the need for finetuning. We propose novel latent objective functions to enhance subject and style similarity, while reducing leakage. Additionally, we propose an orthogonalized temporal aggregation scheme in the cross-attention blocks of denoising networks, effectively conditioning on a text prompt along with single subject and style images. Furthermore, we propose a novel method to train customized content and style projectors in a way which reduces content and style leakage. Our extensive experiments show a tremendous improvement over previous works performing subject, style and action composition. Finally, we show that the SubZero algorithm is feasible to run on a mobile device without the need for online compute. 

\end{abstract}