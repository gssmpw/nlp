
\vspace{- 0.7 em}
\section{Proposed Approach}
\label{sec:method}

%In this Section, we provide a detailed description of our approach. We start with the method preliminaries in \ref{subsec:background}, followed by our proposed Disentangled Stochastic Optimal Controller and orthogonal Temporal Aggregation schemes for subject and style composition. While SubZero works out-of-the-box on existing adapters, we provide further insight into training targeted projectors for object and style composition. 

In this Section, we provide a detailed description of our approach. We briefly summarize preliminaries in Sec.\ref{subsec:background}. In Sec.\ref{subsec:dsoc}, we elaborate on the Disentangled Stochastic Optimal Controller to reduce subject and style leakage while preserving identity. To further facilitate effective information composition, we propose orthogonal Temporal Aggregation schemes in Sec.\ref{subsec:ota}. While SubZero works out-of-the-box on existing adapters, we provide additional insight into training targeted projectors for object and style composition in Sec. \ref{subsec:train}. Finally, we propose an extension of our work to Zero-Order Stochastic Optimal Control in Sec.~\ref{subsec:zo}.

%In the following, we provide a detailed description of our approach. We start with the method overview, followed by our proposed latent objective functions, with further discussions on our adaptive weighted attention strategy.


\subsection{Preliminaries}
\label{subsec:background}
\vspace{- 0.2 em}

\textbf{Text-to-Image Generation:} Diffusion-based models such as~\cite{rombach2022high, podellsdxl, perniaswurstchen} are widely adopted for text-to-image generation. As they usually require 20-30 inference steps, recent works such as~\cite{lin2024sdxl} have also been adopted to speed up their latent denoising process. Our approach is developed on two efficient foundational models: SDXL-Lightning~\cite{lin2024sdxl} (4-step) and W\"{u}rstchen~\cite{perniaswurstchen}. The goal is to model a denoising operation given a forward noising process:
\begin{equation}
\label{eq:fwdproc}
    x_t = \sqrt{\alpha_t}x_0 + \sqrt{1-\alpha_t}\epsilon, \quad  \epsilon \sim N(0, 1)
\end{equation}

Here, $x_t$ represents the state at time $t \in [0, \infty) $, given the original input $x_0$, and $\alpha_t$ is computed by a scheduler.

Current methods~\cite{rombach2022high, podellsdxl, perniaswurstchen} are developed with the objective of reversing the equation~\ref{eq:fwdproc}. They consist of an Encoder-Decoder model $\mathbf{V_e}, \mathbf{V_d}$ which transforms images to and from the latent representation $x_t$, and denoising model $\mathbf{U}$ which progressively de-noises input latents to estimate the noise at every timestep. For SDXL, we denote the Unet as $\mathbf{U}$, and VAE decoder as $\mathbf{V_d}$. For W\"{u}rstchen, we denote the StageC denoiser and the StageA VAE as $\mathbf{U}$ and $\mathbf{V_d}$ respectively. To produce a text-conditioning for the denoising model, the text prompt $\mathbf{p}$ is tokenized and encoded via a text encoder $\bf{\phi_p}$ (i.e. clip \cite{clip}). The output embeddings are fed to $\mathbf{U}$ as keys and values in stage-wise cross-attention modules. The queries to each cross-attention module are the intermediate latent features from $\mathbf{U}$.


\noindent \textbf{Stochastic Optimal Control:} RB-Modulation~\cite{rout2024rb} recently developed latent optimization with stochastic optimal control to effectively adapt intermediate latents produced by $\mathbf{U}$ to inject a reference style $r_{sty}$. For accurate measurement of style, they used the Contrastive Style Descriptor (CSD) network~\cite{somepalli2024measuring} $\mathbf{\psi}$. To perform stochastic optimal control, the intermediate latent $x_t$ at timestep $t$ is used to predict de-noised latent $\hat{x}_0$ as follows:
\begin{equation}
\label{eq:pred_x0}
    \hat{x}_0 = \frac{x_t}{\alpha_t} + \frac{(1-\sqrt{\bar{\alpha_t}})}{\sqrt{\bar{\alpha_t}}}\mathbf{U}(x_t, t, \mathbf{p});
\end{equation}

Keeping only $\hat{x}_0$ as tunable, the denoised image is predicted as $\hat{y} = \mathbf{V_d}(\hat{x_0})$. A style objective $\mathcal{L} = \|\mathbf{\psi} (\hat{y}) - \mathbf{\psi} (r_{sty})\|^{2}_{2}$ is then computed as the terminal cost.
% $\ell_\mathrm{style} = \|\mathbf{\psi} (\hat{y}) - \mathbf{\psi} (y)\|^{2}_{2}$
Finally, the Adam optimizer is used to update $\hat{x_0}$ to reduce the style objective for $M$ iterations. The updated $\hat{x_0}$ is now used to compute denoised latent for the previous time-step $x_{t-1}$. 

\noindent \textbf{Reference Image Conditioning:}
To condition the denoising model using reference subject image $r_{sub}$ and style image $r_{sty}$, there have been various lines of work. For example, training additional customized key and value projections in the cross-attention blocks of $\mathbf{U}$ for reference images of concepts, such as IP-Adapter~\cite{ye2023ip-adapter} and PulID~\cite{guo2024pulid}. Another line of work, such as the Attention Feature Aggregation (AFA) proposed by RB-Modulation, pass the reference image through the clip-image encoder $\bf{\phi_i}$ to encode reference images, and use the key/value projections already available in the base model for conditioning. This method is however, native only to the W\"{u}rstchen model, as it contains already learnt clip text and image projectors. Hence for fair comparison with all baselines, we use IP-Adapter-based projections to encode reference conditions in SDXL experiments, and AFA-based conditioning in W\"{u}rstchen \cite{perniaswurstchen}.

For the methods discussed above, queries from $\mathbf{U}$ are attended separately by key-value projections from all modalities (text, style, subject) or an aggregation of key-value projections in these modalities. In our work, we denote the updated features as $f_{text}$, $f_{style}$ and $f_{sub}$. After feature aggregation, the updated features after aggregating cross-attention outputs from all modalities is denoted as $f_{agg}$. 

\subsection{Disentangled Stochastic Optimal Controller}
\label{subsec:dsoc}
% 
% \textbf{Preliminaries}
% For T2I generation, let $s(x_t; t,\theta)$ be the neural network from which we sample the score function of the joint predictive distribution $\nabla p(X_t;y)$. For personalization in diffusion, we follow the optimal control method in RB modulation. Given a reference image $X^f_0$, and sampled latent from the diffusion process $X^u_t$, terminal cost is defined as the l2 term 
% \begin{math}
% \centering
% \mathcal{L}(x_0) = ||\psi(z_0) - \psi(x_0)||^{2}_{2}
% \end{math}, 
% where the style features are computed by the Contrastive Style Descriptor (CSD) model $\Psi(.)$~\cite{somepalli2024measuring}.

RB Modulation showed that direct feature injection can cause subject leakage from style reference images.  However, our studies show that the stochastic optimal controller and AFA modules are not able to alleviate the subject leakage problem. This has also been observed by the community~\cite{RB-Issues}. Additionally, the approach is not able to preserve necessary characteristics of faces for face personalization (see Fig.~\ref{fig:wurschten_comp}). 
Hence, we propose the Disentangled Stochastic Optimal Controller to alleviate subject and style leakage, while preserving key features of the subjects along with styles. Algorithm~\ref{alg:inference} provides pseudo-code for the proposed Disentangled Stochastic Optimal Controller.

\noindent \textbf{Subject and Style Descriptors}: As discussed in Sec.~\ref{subsec:background}, RB-Modulation optimizes latents for style descriptor $\psi$. Their terminal cost however does not take into account the personalized features of the subject image. Hence, we propose an additional term for personalization of the reference image, computed by a subject descriptor $\rho$. For face stylization experiments, we replace $\rho$ with a facial descriptor $\delta$. Throughout this paper, we use style descriptors $\psi$ from the CSD network~\cite{somepalli2024measuring}, the subject descriptor network as DINO~\cite{Caron_2021_ICCV}, and the facial descriptor as $\delta$ as the facial embedding extractor trained by~\cite{ye2023ip-adapter}, using  Arc-Face~\cite{deng2019arcface}. %We provide further details of all the descriptors in  \textcolor{red}{ Section~\ref{}}.

%We add the maximizing the distance between the style descriptors of the reference concept image $r_con$. In addition, to minimize the leakage of style from the reference content image we apply 
We also propose negative criteria aiming to reduce content and style leakage between networks. This is achieved by maximizing descriptors from $\rho$ for $r_{sty}$ and maximizing descriptors from $\psi$ for $r_{con}$. The terminal cost is hence a combination of four objectives, see Fig.~\ref{fig:latent_objectives}.

\noindent \textbf{Terminal Cost}: We define the terminal cost $\mathcal{L}$ as, 
% \vspace{-0.5 em}
\begin{align}
\begin{split}
\label{eq:dsoc}
    \mathcal{L} = \underbrace{\|\mathbf{\rho} (\hat{y}) - \mathbf{\rho} (r_{sty})\|^{2}_{2}}_{\text{subject descriptor constraint } \mathcal{L}_{c}} 
     + \underbrace{\|\mathbf{\psi} (\hat{y}) - \mathbf{\psi} (r_{sub})\|^{2}_{2}}_{\text{style descriptor constraint } \mathcal{L}_{s}} \\ 
     -\gamma_{nc}\underbrace{\|\mathbf{\psi} (\hat{y}) - \mathbf{\psi} (r_{sty})\|^{2}_{2}}_{\text{subject leakage constraint } \mathcal{L}_{nc}} -\gamma_{ns} \hfill\underbrace{\|\mathbf{\rho} (\hat{y}) - \mathbf{\rho} (r_{sub})\|^{2}_{2}}_{\text{style leakage constraint } \mathcal{L}_{ns}}
\end{split}
 \vspace{-0.8 em}
\end{align}

where $\hat{y}$ is the estimated denoised image $\mathbf{V_d}(\hat{x}_0)$, $\mathbf{V_d}$ is a TinyVAE decoder~\cite{TinyVAE}, $\gamma_{ns}$ and $\gamma_{nc}$ are weighting terms for style and content leakage and are used as hyperparameters, their values are provided in the Appendix. 
 

% \begin{align}
% \begin{split}
%     \mathcal{L}(x_0) = \underbrace{min_{u \in \mathcal{U}} ||\Gamma(z_0) - \Gamma(X^u_0)||^{2}_{2}}_{\text{facial descriptor constraint}}  \\
%      + \hfill \underbrace{min_{u \in \mathcal{U}} ||\Phi(z_0) - \Phi(X^u_0)||^{2}_{2}}_{\text{concept descriptor constraint}} 
% \end{split}
% \end{align}
%Where, $\Phi(.)$ provides the concept embedding and the   $\Gamma(.)$ provides the facial descriptor embedding. We use CLIP Image encoder to extract visual features further project it using a QFormer model initialized using IP-Adapter weights to extract facial features

%To maintain high fidelity of facial content and the concept, We extend the controller cost $\mathcal{L}(x_0)$, to add the concept criterion as 


% \begin{math}
% \mathcal{L}(x_0) := ||\psi(z_0) - \psi(x_0)||^{2}_{2} + \lambda ||x_0 - \mathrm{E}[X^{u}_{0}|X^{u}_{t}]||^{2}_{2} + ||\partial(z_0) - \partial(x_0)||^{2}_{2}
% \end{math}

% \subsection{Latent Objective Functions}
% RB Modulation, showed that direct feature injection can cause content leakage from style reference images and introduced Attention feature aggregation (AFA).  AFA module decouples style from the content, we show that it cannot retain concept/face alignment. We show that adding a negative terminal cost in addition to the cost introduced by the face and style descriptors can alleviate leakage problem further. 
% We add the maximizing the distance between the style descriptors of the reference concept image $r_con$. In addition, to minimize the leakage of style from the reference content image we apply 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/subzero_latent_mod.jpg}
    %\vspace{- 1.2 em}
    \caption{\textbf{Disentangled Stochastic Optimal Controller}.}%, and suffer from mode collapse. However, our proposed FouRA produces more diverse images.}
    \label{fig:latent_objectives}
    \vspace{- 1 em}
\end{figure}

%\subsection{Adaptive Weighted Attention}

\subsection{Orthogonal Temporal Attention Aggregation}
\label{subsec:ota}
As discussed in Section~\ref{subsec:background}, within our denoising model $\mathbf{U}$, we obtain the updated features $f_{text}$, $f_{style}$ and $f_{sub}$ from three sources of conditioning after cross attention. Previous works~\cite{rout2024rb, ye2023ip-adapter} have proposed a weighted addition of these updated features, to obtained aggregated features $f_{agg}$. However, we observe that this leads to subject leakage in the generated image, as discussed in the Appendix.

% \noindent \textbf{Orthogonal features:} Subject features need to update local regions of the latent. Hence, SubZero uses an orthogonal projection of the subject query, $\hat{f}_{sub}$, over the original text features. This preserves key aspects of the text prompt, such as any actions described for the subject and generates robust images depending on text conditioning. On the other hand, style is a global construct. Hence we directly add the style query without orthogonalization, allowing the style condition to update the image holistically instead of a local area. 

\noindent \textbf{Orthogonal features:} 
The text and style features contribute to the global structure, while the subject features update local regions of the latent space. 
To prevent distortion between various sources of information in the latent space, we apply an orthogonal projection of the subject query, $\hat{f}_{sub}$, onto the original text  to update local regions. 
Meanwhile, the style query is directly added to the text features to update the image holistically, as shown in Fig \ref{fig:orthogonal_queries}. 
This approach preserves key aspects of each component, such as actions described for the subject in the text prompt, and generates robust images based on text and image conditioning.


\begin{figure}[ht]
    \centering
    \vspace{- 0.5 em}
    \includegraphics[width=0.7\linewidth]{figures/subzero_ortho_proj.jpg}
    \caption{\textbf{Orthogonal Temporal Aggregation}.}
    \label{fig:orthogonal_queries}
    \vspace{- 0.5 em}
\end{figure}


\noindent \textbf{Temporal Weighting:} To reduce the subject leakage problem, we propose a temporal weighting strategy. To weigh the updated queries, we use a novel temporal-adaptive weighting mechanism. As style is a global construct, it should not decide the shape of objects generated in the image. The shapes should be decided based on text-conditioning features and subject-conditioning features. Hence, at the start of the denoising process, when shapes are being generated, we fix a lower weight for style features $f_{style}$ and a higher scale for subject features $f_{sub}$. As the denoising process progresses, we increase the style scale gradually based on two factors: direct proportionality to the style descriptor constraint $\mathcal{L}_{s}$ and inverse proportionality to the subject leakage constraint $\mathcal{L}_{ns}$, determined in Equation~\ref{eq:dsoc}. At timestep $t$, the temporal style weights are denoted as $\mu_{s,t}$, and subject weights are denoted as $\mu_{c}$. Algorithm~\ref{alg:inference} provides pseudo-code for $\mu_{s,t}$. 

Finally, the Orthogonal Temporal Aggregation (OTA) features are calculated as $f_{agg} = f_{text} + \mu_{s,t}f_{style} + \mu_{c}f_{sub}$.


\begin{algorithm}[ht]
\caption{SubZero: Disentangled Controller and Temporal Aggregation}
\label{alg:inference}
%\begin{algorithmic}[1] 
\textbf{Input}: Reference subject image $r_{sub}$, reference style image $r_{sty}$, style descriptor $\psi$, Subject extractor $\rho$, text prompt $\mathbf{p}$, Denoising Network $\mathbf{U}$, TAE decoder $\mathbf{V_d}$ \\
\textbf{Tunable Parameter}: Step size $\eta$, Optimization steps $M$,  Initial style scale $\mu_{s,0}$, Style tuner $\zeta$ \\
\SetAlgoLined
Initialize $x_T \gets \mathcal{N}(0,I_d)$;

\For{t=T to 1}
{
    \textit{Compute Predicted latent}: \\ 
    $\hat{x}_0 = \frac{x_t}{\alpha_t} + \frac{(1-\sqrt{\bar{\alpha_t}})}{\sqrt{\bar{\alpha_t}}}\mathbf{U}(x_t, t, \mathbf{p})$\;
    \textit{Initialize} $z_0 \to \hat{x}_0$\;
    \For{t=M to 1}
    {
       $\hat{y} = \mathbf{V_d}(\hat{x}_0)$\;
       \textit{Compute disentangled control objective}: \\
        $\mathcal{L} = \mathcal{L}_s + \mathcal{L}_c - \gamma_{nc}\mathcal{L}_{nc} - \gamma_{ns}\mathcal{L}_{ns}$
        $= \|\mathbf{\rho} (\hat{y}) - \mathbf{\rho} (r_{sty})\|^{2}_{2} 
     + \|\mathbf{\psi} (\hat{y}) - \mathbf{\psi} (r_{sub})\|^{2}_{2}
     -\gamma_{nc}\|\mathbf{\rho} (\hat{y}) - \mathbf{\rho} (r_{sty})\|^{2}_{2} -\gamma_{ns}\|\mathbf{\psi} (\hat{y}) - \mathbf{\psi} (r_{sub})\|^{2}_{2}$\;
    \textit{Update optimization variable $z_0$}: \\
    $z_0$ = $z_0 - \eta$ $\nabla_{z_{0}}$ $\mathcal{L}$ $(z_0)$\;
    } 
    $\hat{x}_0 \to z_0$\;
    \textit{Set temporal weighting term:}
    $\mu_{s,t-1} = \mu_{s,t-1} + \zeta\mathcal{L}_s(1-\mathcal{L}_{nc})$\;
    \textit{Compute previous state:}
    $x_{t-1}=DDIM(\hat{x}_0, x_t)$
}
\textbf{Output}: Denoised Image $y = \mathbf{V_d}(x_0)$

%\end{algorithmic}
\end{algorithm}

 % \vspace{- 0.7 em}

% pmandke - WIP

% \begin{tcolorbox}[fonttitle=\bfseries, title=SubZero: Algorithm for latent objective]
% \end{tcolorbox}

% using mezo in latent udpate
% \textbf{ZO for Latent Update:}
% SPSA based ZO methods approximate the gradient by perturbing the weight parameters by a small amount based on some random noise.
% We perform preliminary experiments by leveraging the ZO-Adam scheme described in MeZO ~\cite{malladi2024finetuninglanguagemodelsjust} and extend it to update the latent.
% One of the key challenges with ZO methods is slow convergence primarily resulting from high variance of the noisy gradient approximations.
% In our experiments we observe that while ZO-Adam is not at par with gradient descent, it shows promising performance - achieving a competitive personalization distance given enough training iterations.
% However, the memory savings resulting from ZO-Adam are significant.
% Thus, we suggest the use of ZO techniques for the latent update in scenarios where one can afford to trade training time for a more favorable memory budget.
% Our experiments in ZO are preliminary, and moving forward we intend to explore this area in much more detail.

% [optional] some results: just to show that we achieved decent convergence and that ZO has promise

% @sborse feel free to modify or add more info

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/subzero_training.jpg}
    %\vspace{- 1.2 em}
    \caption{\textbf{Training Pipeline for StyleZero and ObjectZero projectors}. To train disentangled projectors, we use a weighted combination of the denoising diffusion loss along with a targeted loss to help extract only relevant information from styles and objects.}
    \label{fig:training_pipe}
    \vspace{- 0.5 em}
\end{figure*}

\subsection{Targeted Style and Object Projectors}
\label{subsec:train}
%For content and style, we train two separate adapters that project the DINO features of images into the same space as the text features in the pretrained diffusion model. These adapters are followed by a layer norm and two other projection layers for K and V. The style adapter is trained on the ContraStyles dataset~\cite{somepalli2024measuring}. The content adapter is trained on the MS COCO dataset~\cite{Lin2014MicrosoftCC}. 

While our proposed SubZero algorithm works out-of-the-box on existing IP-Adapters~\cite{ye2023ip-adapter, guo2024pulid, wang2024instantstyle_plus}, we further propose a method to train new style and object projectors. Here, the aim is to disentangle and extract only the relevant information from subjects and styles because IP-Adapters are also known to cause subject leakage. To this end, we utilize the subject and style descriptor models ($\rho$ and $\psi$) to train targeted projectors for objects and styles. 

To train our proposed projectors, we set them as tunable and attach them to every cross-attention block in the denoising model $\mathbf{U}$, which is kept frozen. During each training iteration, we randomly sample the timestep $t$ and compute the noisy latent $x_t$ using the scheduler. We compute the diffusion loss $\ell_\mathrm{denoising}$ on the predicted noise during training.

\noindent\textbf{StyleZero:} We illustrate the training setup for our style projector (StyleZero) in Fig.~\ref{fig:training_pipe}. We use images $y$ from the recent ContraStyles dataset~\cite{somepalli2024measuring} as ground-truth. We first employ the style descriptor (CSD) $\psi$ to extract style embeddings of the reference style image. Next, we pass these descriptors through a Style Projection Network, before passing through key-value projections. These are fed to a cross-attention module, with query projections directly from intermediate features of $\mathbf{U}$. Given noisy image at timestep $t$, we first predict $\hat{x}_0$ using Equation~\ref{eq:pred_x0}. Next, we pass it to the VAE decoder to obtain de-noised prediction $\hat{y} = \mathbf{V_d}(\hat{x_0})$. Similar to the stochastic objective $\mathcal{L}_{s}$, we compute the style loss $\ell_\mathrm{style} = \|\mathbf{\psi} (\hat{y}) - \mathbf{\psi} (y)\|^{2}_{2}$. Hence, the final loss for StyleZero is $\ell_\mathrm{final} = \ell_\mathrm{denoising} + \gamma\ell_\mathrm{style} \ $.

% \begin{equation}
% \ell_\mathrm{final} = \ell_\mathrm{denoising} + \gamma\ell_\mathrm{style} \ .
% \label{eqn:final_style_loss}
% \end{equation}


\noindent\textbf{ObjectZero:} We illustrate the training setup for our object projector (ObjectZero) in Fig.~\ref{fig:training_pipe}. We use images $y$ from MSCOCO~\cite{Lin2014MicrosoftCC} as ground-truth. Similar to StyleZero, we first employ an object descriptor $\rho$ (DINO encoder) to project object embeddings. Similar to the stochastic objective $\mathcal{L}_{c}$, we compute the object loss $\ell_\mathrm{object} = \|\mathbf{\rho} (\hat{y}) - \mathbf{\rho} (y)\|^{2}_{2}$. Hence, the final loss function for ObjectZero is $\ell_\mathrm{final} = \ell_\mathrm{denoising} + \gamma\ell_\mathrm{object} \ $. 

% \begin{equation}
% \ell_\mathrm{final} = \ell_\mathrm{denoising} + \gamma\ell_\mathrm{object} \ .
% \label{eqn:final_content_loss}
% \end{equation}

Once trained, we get StyleZero and ObjectZero projectors for disentangling style and object features, respectively, from the corresponding reference images. These newly trained projectors are used in conjunction with the rest of SubZero latent modulation approach. See Appendix for training hyperparameters of StyleZero and ObjectZero.%$ are provided in Appendix.


\subsection{Extension: Zero-Order Stochastic Control}
\label{subsec:zo}
 %\vspace{-0.5 em}

Even though our method does not involve updating any parameters of the descriptor models $\psi$ and $\rho$, the optimal controller entails the need to cache intermediate activations and gradient computations as part of the chain rule, during the update of $\hat{x}_0$. Zero Order (ZO) approximation has been gaining popularity in order to alleviate the memory requirements of back-propagation. While most efforts in the context of ZO have been in the area of language modeling, we attempt to leverage ZO techniques for the latent update. To achieve zero-order optimal control, we perform our experiments by leveraging the ZO-Adam scheme described in MeZO ~\cite{malladi2024finetuninglanguagemodelsjust} and extend it to update the latent. More details and experiments are in the Appendix.


%\subsection{Visual Prompting for Text Conditioning}

%Additionally, we propose to use a contrastive loss $\ell_\mathrm{contrastive} = \|\mathbf{\rho} (y') - \mathbf{\rho} (y)\|^{2}_{2}$, to match  together.

%We provide more details regarding Projector architectures in the Appendix.

%To ensure the proper style transfer, we compute the \(X_0\) at each time step of training using the DDPM formulation~\cite{ddpm} and then use a cosine similarity loss between the CSD~\cite{somepalli2024measuring} embeddings of \(x_0\) and the ground truth image:



% \begin{equation}
% \ell_\mathrm{style} = || \Psi(X_0) - \Psi(X_0|X_t)|| \ ,
% \label{eqn:style_loss}
% \end{equation}

% where \(\Psi\) is the CSD encoder. The final loss for training the style embedding is as follow:


% We follow the same process for the content adapter as the style transfer. 

% \begin{equation}
% \ell_\mathrm{content} = || \Phi(X_0) - \Phi(X_0|X_t)|| \ ,
% \label{eqn:content_loss}
% \end{equation}

% where we use DINO encoder as \(\Phi\). In addition, we propose other loss terms to ensure the content consistency. At each iteration, we sample another sets of noise for each time step using the same text and content conditioning, \(X_t^2\). We compute \(\ell_\mathrm{content}\) for the second batch as well to ensure the content consistency. Moreover, we add another term to our loss to ensure consistency between batch one \(X_t\) and batch two \(X_t^2\):

% \begin{equation}
% \ell_\mathrm{contrastive} = || \Phi(X_0|X_t) - \Phi(X_0|X_t^2)|| \ .
% \label{eqn:contrastive_loss}
% \end{equation}

% Finally, we also compute the loss terms \(\ell_\mathrm{content}\) and \(\ell_\mathrm{contrastive}\) on the foreground objects in each image to avoid background content leakage into the generation. We used SAM~\cite{sam} to separate the foreground and background.


% \paragraph{Composition.}
% We have multiple options to integrate content and style features within the cross attention layers of Stable Diffusion model. One option is to use weighted sum, similar to \cite{ye2023ip-adapter}.

% \begin{equation}
% Z_{composition} = A_{text} + \alpha_1.A_{style} + \alpha_2.A_{content}\ ,
% \label{eqn:sum}
% \end{equation}

% where \(A_{text}\), \(A_{style}\), and \(A_{content}\) are the cross-attentions from text, style and content images, respectively. 

% Another approach could be to follow the same process as \cite{rout2024rb} and use Attention Feature Aggregation (AFA): 

% \begin{equation}
% Z_{composition} = Average(A_{text}, A_{style}, A_{content})\ .
% \label{eqn:afa}
% \end{equation}

% Finally, Adaptive Instance Normalization (AdaIN)~\cite{adain} is another popular choice in style transfer literature. 

% \begin{equation}
% {AdaIN}(x,y) = \sigma(y).(\frac{x-\mu(x)}{\sigma(x)}) + \mu(y)\ .
% \label{eqn:adain}
% \end{equation}

% AdaIN can be used to normalize the \(A_{text}\) by either using  \(A_{style}\) or \(A_{content}\).


