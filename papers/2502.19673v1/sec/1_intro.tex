\section{Introduction}
\label{sec:intro}

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/subzero_faces_stylized.jpg}
    \vspace{- 2 em}
    \caption{
    % \textbf{Stylized Faces with SubZero}. Stylized Faces with SubZero.
Various stylized face images generated using our proposed SubZero method applied to pre-trained text-to-image diffusion models without any tuning. 
SubZero produces high-quality, diverse stylized images while maintaining facial features.
    }%, and suffer from mode collapse. However, our proposed FouRA produces more diverse images.}
    \label{fig:faces_stylized}
   \vspace{- 1.2 em}
\end{figure}


Large Text-to-Image (T2I) generative models based on diffusion have gained traction~\cite{ddpm, podellsdxl, rombach2022high}, surpassing other existing methods \cite{goodfellow2014generative}. While these models can generate high-fidelity and diverse images\cite{esser2024scaling}, gaining control over synthesized images by ensuring consistent subjects or styles remains a significant challenge~\cite{blora, shah2025ziplora}.
%Stable Diffusion has revolutionized computer vision by bringing the ability to generate novel images to the masses. One of the key developments in this field are Generative AI methods that allow users to personalize images either to contain a specific subject (e.g., a specific person or common objects/pets used in DreamBooth~\cite{ruiz2023dreambooth}) or by editing generated images to have certain styles~\cite{hertz2024style, ye2023ip-adapter}. For example, successful style transfer is commonly accomplished using the popular Low Rank Adaptation (LoRA) technique~\cite{lora} and its variants~\cite{shira, foura, blora}. Recently, \textit{subject-style composition} has emerged which integrates both subject and styles into text-to-image diffusion models~\cite{shah2025ziplora,gu2024mix,blora,wang2024instantstyle}. These methods either (\textit{i})~rely on training subject- or style-specific LoRAs and then blending them together (e.g., ZipLoRA~\cite{shah2025ziplora}, Mix-of-Show~\cite{gu2024mix}, B-LoRA~\cite{blora}), or (\textit{ii})~exploit expensive models architectures (e.g., InstantStyle-Plus~\cite{wang2024instantstyle}, ControlNet~\cite{controlnet}), or DDIM inversion (e.g., StyleAligned~\cite{hertz2024style}) to directly work with a single reference image, or (\textit{iii})~leak irrelevant content from reference images into the generated images (e.g., IP-Adapter~\cite{ye2023ip-adapter}). 


To address this issue, recent studies have proposed fine-tuning diffusion models using reference images~\cite{blora, shah2025ziplora, gu2024mix, foura, shira}. 
They utilize LoRA~\cite{lora} for efficient training while preserving original models capability. While this approach has demonstrated a remarkable ability to control the style or content of generative model, it lacks generalization and requires availability of multiple training samples incurring additional memory and time for adaptation. 
Moreover, these methods require fine-tuning a dedicated adapter each time we need to support new styles or subject images, which is a significant drawback for resource-constrained on-device applications. This key limitation has led to an emergence of training-free methods that can generalize to any reference subject or style images.

Recent training-free methods for \textit{subject-style composition} rely on DDIM inversion-based approaches~\cite{wang2024instantstyle_plus, hertz2024style}, ControlNet-based methods~\cite{controlnet, wang2024instantstyle}, and shared attention techniques~\cite{rout2024rb, hertz2024style}. 
These methods eliminate the need for fine-tuning a different adapter for each subject/style but struggle to properly disentangle content and style information or to preserve subject fidelity. For instance, the DDIM inversion-based methods adapt the noise from the subject image by injecting style information, which can lead to subject leakage from the style image. 
ControlNet-based methods offer good personalization but lack flexibility. Both DDIM inversion and ControlNet based methods perform poorly on generating a diverse range of images. Hence, they also fail when action prompts are added. Moreover, both the techniques are computationally expensive. Other methods such as IP-Adapter~\cite{ye2023ip-adapter} are efficient. However, all the above methods result in \textit{irrelevant subject leakage} (e.g., background from reference subject images leaking into generated images). To tackle subject leakage, RB-modulation~\cite{rout2024rb} elegantly proposed the stochastic optimal control scheme which directly optimizes the diffusion latent. However, our experiments show that RB-modulation fails to effectively align the content with style in the loss and hence results in irrelevant subject leakage. This has also been recently observed by the community~\cite{RB-Issues}.

To enable effective and privacy-preserving subject-style composition on-edge devices, we aim to create a \textit{robust yet efficient} subject, style and action composition method that can (\textit{i})~clearly \textit{disentangle} the subject and style, (\textit{ii})~generate a wide range of images controlled by the text prompt, (\textit{iii})~work with just a \textit{single reference} subject and/or style image instead of training a new adapter for each scenario, and (\textit{iv})~reduce irrelevant subject leakage (e.g., background from subject reference image) into the generated image. 
We propose SubZero, a robust zero-shot solution to subject, style and action composition. At the core of our approach is a novel latent modulation objective formulation, orthogonal and temporally-adaptive blending of subject and style information inside the cross-attention modules, generalized adapters trained to specifically disentangle subjects and styles while limiting irrelevant leakage. With these new ideas, we show high quality subject, style and action composition and face personalization applications (e.g., see Fig.~\ref{fig:faces_stylized}) that are particularly suited for efficient execution due to their low compute costs.

Overall, we make the following \textbf{key contributions}:
\begin{enumerate}
    \item We propose SubZero, a robust \underline{Sub}ject-Style Composition framework with \underline{Zero} training for new concepts.
    \item We propose the disentangled stochastic optimal controller containing novel latent modulation objectives that effectively align subject and style during inference. 
    \item We propose a temporally-adaptive and orthogonal aggregation method to effectively combine attention features originating from subject, style and text conditioning.
    \item We train custom subject and style adapters with novel training techniques and losses, and demonstrate how these new adapters significantly limit irrelevant content leakage compared to the prior art.
    \item Our extensive experiments clearly set a new state-of-the-art on subject-style composition (e.g., for objects such as items or pets, as well as face personalization) as well as subject-style-action composition.
    %\item Finally, we discuss how SubZero can be applied in a completely gradient-free manner in the future using Zero-Order training and provide a proof-of-concept.
\end{enumerate}

%This paper is organized as follows. Section~\ref{sec:related} discusses the related work while Section~\ref{sec:method} proposes our technique. Then, we conduct extensive experiments in Section~\ref{sec:exp} and conclude the paper in Section~\ref{sec:conc}.

% Training free
% DDIM inversion : contents leakage from style (RB mention in paper)
% Intermediate control (controlnet) methods provide good personalization but less flexibility cannot 
% Shared-Attention or CLIPspace methods provide good flexibility but less personalization
% RB module/ dreambooth -> content,style together 
% Advantage Disadvantage
% Good style but cannot keep the content information
% ur solution
% similairt (Dino, other obj, face sim for human)

