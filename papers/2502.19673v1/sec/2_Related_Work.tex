\section{Related Work}
\label{sec:related}



Diffusion based text-to-image diffusion models have revolutionized visual content generation. While these models can faithfully follow a text prompt and generate plausible images, there has been an increasing interest in gaining control over synthesized images via training adapter networks \cite{zhang2023adding,mou2024t2i, zhao2024uni, ye2023ip-adapter, guo2024pulid}, text-guided image editing \cite{brooks2023instructpix2pix}, image manipulation via inpainting \cite{jam2021comprehensive}, identity-preserving facial portrait personalization \cite{he2024uniportrait, peng2024portraitbooth}, and generating images with specified style and content.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/subzero_inference.jpg}
    %\vspace{- 1.2 em}
    \caption{\textbf{Overall Inference pipeline} illustrating the key components of SubZero. Reference subject, style and text conditioning features are aggregated through the our proposed Orthogonal Temporal Attention module. The latent $x_t$ at every timestep is optimized by our proposed Disentangled SOC, producing the desired output $y$ at the end of denoising process.}
    \label{fig:inference_pipe}
    \vspace{- 0.5 em}
\end{figure*}



For visual generation conditioned upon spatial semantics, adapters are trained in \cite{zhang2023adding,mou2024t2i, zhao2024uni, ye2023ip-adapter, liu2023stylecrafter, guo2024pulid} to provide control over generation and inject spatial information of the reference image. ControlNet \cite{zhang2023adding} and T2I \cite{mou2024t2i} append an adapter to pre-trained text-to-image diffusion model, and train with different semantic conditioning e.g., canny edge, depth-map, and human pose. Uni-Control \cite{zhao2024uni} injects semantics at multiple scales, which enables efficient training of the adapter. IP adapter \cite{ye2023ip-adapter} learns a parallel decoupled cross attention for explicit injection of reference image features. Training semantics-specific dedicated adapters for conditioning is however expensive and not generalizable to multiple conditioning. 

Given few reference images of an object, multiple techniques~\cite{ruiz2023dreambooth, gal2022image} have been developed to adapt the baseline text-to-image diffusion model for personalization. 
Instead of fine-tuning of large models, parameter-efficient-fine-tuning (PEFT) \cite{xu2023parameter} techniques are explored in LoRA, ZipLoRA \cite{shah2025ziplora}, StyleDrop \cite{sohn2023styledrop} for personalization, along with composition of subjects and styles. 
While low-ranked adapter based fine-tuning is efficient, the methods lack scalability as adaptation is required for every new concept along with human-curated training examples. Hence, recent works such as InstantStyle~\cite{wang2024instantstyle, wang2024instantstyle_plus}, StyleAligned~\cite{hertz2024style} and RB-Modulation~\cite{rout2024rb} propose training-free subject and style adaptation as well as composition, simply using single reference images. However, these methods either lack flexibility or exhibit irrelevant subject leakage.

Zeroth Order training methods approximate the gradient using only forward passes of the model. Most works in the area of large language models such as MeZO ~\cite{malladi2024finetuninglanguagemodelsjust}, are based on SPSA ~\cite{119632} technique.
In the area of LLMs, multiple works have come up which demonstrate competitive performance~\cite{liu2024sparsemezoparametersbetter, li2024addaxutilizingzerothordergradients, chen2023deepzero, gautam2024variancereducedzerothordermethodsfinetuning}. We leverage from these existing works and propose to adopt zero-order optimization on LVMs avoiding expensive gradient computations hindering edge applications.
%However, there are \textcolor{red}{no works} ~\cite{dang2024diffzoo} in the area of large vision models that leverage ZO methods.%, that we are aware of.
