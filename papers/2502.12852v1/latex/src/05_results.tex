\section{Results and Discussion}
\label{sec:results}

\begin{table*}[ht]
  \centering
  \begin{adjustbox}{width=\textwidth,center}
     \setlength{\tabcolsep}{8pt}
    \begin{tabular}{
      @{\extracolsep{\fill}} 
      l  !{\vrule}
      c c c !{\vrule}
      c c c !{\vrule}
      c c c !{\vrule}
      c c c !{\vrule}
      c c c 
    }
      \toprule
      % Resource categories (each now with 3 columns)
       \multicolumn{1}{c}{\textbf{Resourceness}} & \multicolumn{7}{r}{$\xleftarrow{\hspace{3cm}}$ \textbf{High} \rule[0.6ex]{3cm}{0.4pt}} & \multicolumn{1}{c}{\textbf{Mid}} & \multicolumn{7}{l}{\rule[0.6ex]{3cm}{0.4pt} \textbf{Low} $\xrightarrow{\hspace{3cm}}$} \\ \midrule
      \multicolumn{1}{c}{} &
      \multicolumn{3}{c}{\textbf{\textsc{English}}} &
      \multicolumn{3}{c}{\textbf{\textsc{Tier 4} (26)}} &
      \multicolumn{3}{c}{\textbf{\textsc{Tier 3} (32)}} &
      \multicolumn{3}{c}{\textbf{\textsc{Tier 2} (96)}} &
      \multicolumn{3}{c}{\textbf{\textsc{Tier 1} (51)}} \\
      \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13} \cmidrule(lr){14-16}
      % Headings for k=1,3,5 with single columns for each k
      \multicolumn{1}{c !{\vrule}}{\textbf{\textit{k} References}} &
      \textbf{1} &
      \textbf{3} &
      \textbf{5} &
      \textbf{1} &
      \textbf{3} &
      \textbf{5} &
      \textbf{1} &
      \textbf{3} &
      \textbf{5} &
      \textbf{1} &
      \textbf{3} &
      \textbf{5} &
      \textbf{1} &
      \textbf{3} &
      \textbf{5} \\
      \midrule \hline
      \rowcolor{gray!10} 
      \multicolumn{16}{l}{\rule{0pt}{2.5ex} \textbf{\textit{Images-To-Sentence:}} \textit{Select 1 of 4 sentences topically matching $k$ reference images}} \\ \hline

mSigLIP-base & 57.7 & 64.6 & 66.4 & 
53.3 & 58.6 & 59.7 & 
51.4 & 56.2 & 57.1 & 
38.9 & 41.2 & 41.7 & 
36.1 & 37.6 & 38.0 \\ \hline

Qwen2-VL 2B & 36.3 & 34.8 & 34.9 & 
35.5 & 35.3 & 34.1 & 
34.5 & 34.3 & 33.2 & 
31.0 & 30.6 & 30.0 & 
29.5 & 29.0 & 28.6 \\
Qwen2-VL 7B & 65.8 & 63.1 & 58.9 & 
57.7 & 56.5 & 51.7 & 
55.4 & 54.5 & 49.6 & 
44.3 & 44.4 & 40.5 & 
39.6 & 39.7 & 36.5 \\
InternVL 2.5 4B & 52.5 & 49.2 & 48.1 & 
50.3 & 46.6 & 47.7 & 
48.6 & 45.3 & 46.1 & 
38.7 & 37.1 & 37.3 & 
35.4 & 34.5 & 34.5 \\
InternVL 2.5 8B & \underline{67.7} & \underline{67.9} & \underline{68.7} & 
\underline{64.6} & \underline{64.9} & \underline{65.7} & 
\underline{61.2} & \underline{60.8} & \underline{61.6} & 
\underline{51.0} & \underline{51.4} & \underline{51.8} & 
\underline{46.1} & \underline{46.0} & \underline{46.3} \\
Centurio Qwen & 54.8 & 60.0 & 62.4 & 
54.2 & 59.2 & 60.6 & 
53.4 & 58.1 & 58.9 & 
46.6 & 48.9 & 49.2 & 
43.0 & 44.2 & 44.7 \\
GPT-4o-mini & \textbf{68.3} & \textbf{78.1} & \textbf{77.4} & 
\textbf{71.6} & \textbf{79.0} & \textbf{78.1} & 
\textbf{72.0} & \textbf{78.9} & \textbf{77.7} & 
\textbf{63.5} & \textbf{68.0} & \textbf{66.4} & 
\textbf{56.9} & \textbf{60.3} & \textbf{58.7} \\
      \midrule \hline
      \rowcolor{gray!10} 
      \multicolumn{16}{l}{\rule{0pt}{2.5ex} \textbf{\textit{Sentences-To-Image:}} \textit{Select 1 of 4 images topically matching $k$ reference sentences}} \\
      \hline

mSigLIP-base & 56.3 & 66.0 & \underline{69.6} & 
51.8 & 61.6 & \underline{64.0} & 
49.1 & 58.3 & 60.2 & 
36.0 & 40.4 & 41.2 & 
32.9 & 36.3 & 36.9 \\ \hline

Qwen2-VL 2B & 41.9 & 43.1 & 43.4 & 
41.6 & 42.5 & 42.7 & 
40.8 & 42.4 & 42.4 & 
33.7 & 35.6 & 35.5 & 
31.0 & 32.7 & 32.8 \\
Qwen2-VL 7B & \underline{71.7} & \underline{70.4} & 68.6 & 
\underline{65.5} & \underline{65.5} & 63.5 & 
\underline{64.4} & \underline{65.3} & \underline{64.1} & 
\underline{50.3} & \underline{52.5} & \underline{52.9} & 
\underline{43.5} & \underline{45.9} & \underline{46.6} \\
InternVL 2.5 4B & 47.7 & 44.5 & 43.0 & 
38.0 & 40.3 & 40.4 & 
36.7 & 39.6 & 40.3 & 
30.7 & 34.4 & 35.7 & 
28.8 & 32.1 & 33.7 \\
InternVL 2.5 8B & 66.2 & 69.0 & 68.7 & 
57.5 & 62.5 & 61.6 & 
52.9 & 58.5 & 58.1 & 
43.4 & 49.8 & 49.7 & 
39.7 & 45.9 & 46.2 \\
Centurio Qwen    & 35.3 & 36.1 & 35.6 & 
31.1 & 32.9 & 33.3 & 
31.0 & 32.8 & 33.1 & 
28.7 & 29.7 & 29.8 & 
28.1 & 28.7 & 28.7 \\
GPT-4o-mini & \textbf{77.5} & \textbf{86.4} & \textbf{89.1} & 
\textbf{77.2} & \textbf{86.5} & \textbf{88.6} & 
\textbf{77.1} & \textbf{86.1} & \textbf{88.4} & 
\textbf{68.4} & \textbf{79.8} & \textbf{82.7} & 
\textbf{61.7} & \textbf{74.0} & \textbf{77.2} \\
      \bottomrule
    \end{tabular}
  \end{adjustbox}
  \caption{\textbf{Cross-modal Topic Matching:} LVLMs must select the candidate sentence (image) from 4 choices that topically align with $k$ reference images (sentences). Prompts provided in \S\ref{appendix:prompts}. Languages are tiered by Wikipedia sizes (cf. \S\ref{sec:results}). Number of languages in parentheses. \textbf{Metric:} share of responses starting with correct option letter. Details in \S\ref{sec:experimental-setup}. In each column, the best model is emphasized in \textbf{bold}, the second-best model is \underline{underlined}.}
  \label{tab:main-results-image-text}
  \vspace{-0.2cm}
\end{table*}

\begin{table*}[ht]
  \centering
  \begin{adjustbox}{width=\textwidth,center}
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{
      @{\extracolsep{\fill}} 
      l  !{\vrule}
      c c c c  % English
      c c c c  % High (53)
      c c c c  % Low (151)
      c c c c  % Non-En (199)
      c c c c  % All
    }
      \toprule
       \multicolumn{1}{c}{\textbf{Resourceness}} & \multicolumn{9}{r}{$\xleftarrow{\hspace{3cm}}$ \textbf{High} \rule[0.6ex]{4cm}{0.4pt}} & \multicolumn{2}{c}{\textbf{Mid}} & \multicolumn{9}{l}{\rule[0.6ex]{4cm}{0.4pt} \textbf{Low} $\xrightarrow{\hspace{3cm}}$} \\ \midrule
      \multicolumn{1}{c}{} &
      \multicolumn{4}{c}{\textbf{\textsc{English}}} &
      \multicolumn{4}{c}{\textbf{\textsc{Tier 4} (26)}} &
      \multicolumn{4}{c}{\textbf{\textsc{Tier 3} (32)}} &
      \multicolumn{4}{c}{\textbf{\textsc{Tier 2} (96)}} &
      \multicolumn{4}{c}{\textbf{\textsc{Tier 1} (51)}} \\
      \cmidrule(lr){2-5} \cmidrule(lr){6-9} \cmidrule(lr){9-13} \cmidrule(lr){14-17} \cmidrule(lr){18-21}
      % Headings for k=1,3,5 with single columns for each k
      \multirow{2}{*}{\textbf{Task}} &
      \textbf{Topic-To} &
      \multicolumn{3}{c}{\textbf{Sentences}} &
      \textbf{Topic-To} &
      \multicolumn{3}{c}{\textbf{Sentences}} &
      \textbf{Topic-To} &
      \multicolumn{3}{c}{\textbf{Sentences}} &
      \textbf{Topic-To} &
      \multicolumn{3}{c}{\textbf{Sentences}} &
      \textbf{Topic-To} &
      \multicolumn{3}{c}{\textbf{Sentences}} \\
        &
      \textbf{Sentence} &
      \multicolumn{3}{c}{\textbf{To-Topic}} &
      \textbf{Sentence} &
      \multicolumn{3}{c}{\textbf{      To-Topic}} &
      \textbf{Sentence} &
      \multicolumn{3}{c}{\textbf{      To-Topic}} &
      \textbf{Sentence} &
      \multicolumn{3}{c}{\textbf{      To-Topic}} &
      \textbf{Sentence} &
      \multicolumn{3}{c}{\textbf{      To-Topic}} \\
      
      \cmidrule(lr){2-2} \cmidrule(lr){3-5} 
      \cmidrule(lr){6-6} \cmidrule(lr){7-9} 
      \cmidrule(lr){10-10} \cmidrule(lr){11-13} 
      \cmidrule(lr){14-14} \cmidrule(lr){15-17}
      \cmidrule(lr){18-18} \cmidrule(lr){19-21}
      % Fourth header row: sub-column labels (e.g., k values)
      \textbf{\textit{k} References} &
      \textbf{1} & 
      \textbf{1} & 
      \textbf{3} &
      \textbf{5} & 
      \textbf{1} & 
      \textbf{1} & 
      \textbf{3} &
      \textbf{5} &
      \textbf{1} & 
      \textbf{1} & 
      \textbf{3} &
      \textbf{5} & 
      \textbf{1} &
      \textbf{1} &
      \textbf{3} & 
      \textbf{5} &
      \textbf{1} &
      \textbf{1} &
      \textbf{3} &
      \textbf{5} \\
      \midrule

    Qwen2-VL 2B &
                56.7 & 86.1 & 96.5 & 98.2 & 
                49.1 & 78.4 & 92.0 & 95.2 & 
                45.2 & 73.7 & 89.6 & 93.6 & 
                36.4 & 53.7 & 69.3 & 76.4 & 
                33.0 & 46.4 & 60.7 & 68.4 \\
Qwen2-VL 7B & 
            85.7 & 89.1 & 95.3 & 97.5 & 
            81.8 & 84.1 & 93.3 & 96.1 & 
            80.5 & 84.1 & 93.5 & 96.6 & 
            63.7 & 67.8 & 81.3 & 86.7 & 
            55.3 & 59.1 & 73.7 & 79.8 \\
InternVL 2.5 4B &
            81.4 & 90.6 & \underline{98.0} & \textbf{99.1} & 
            72.7 & 85.2 & 95.8 & 97.9 & 
            68.2 & 83.7 & 95.5 & 97.7 & 
            50.2 & 67.8 & 83.4 & 87.9 & 
            44.6 & 60.7 & 77.4 & 83.1 \\
InternVL 2.5 8B &
            \underline{87.0} & \underline{91.7} & \textbf{98.1} & \underline{99.0} & 
            83.0 & 86.3 & \underline{96.3} & \underline{98.3} & 
            79.1 & 82.4 & 94.1 & 96.7 & 
            65.4 & 66.8 & 81.8 & 86.9 & 
            58.1 & 58.3 & 74.6 & 80.9 \\
Centurio Qwen &
            85.4 & 89.9 & 96.7 & 97.7 & 
            \underline{83.6} & \underline{88.0} & 95.8 & 97.7 & 
            \underline{82.6} & \underline{87.7} & \underline{96.0} & \underline{97.9} & 
            \underline{70.4} & \underline{73.1} & \underline{86.8} & \underline{90.7} & 
            \underline{64.0} & \underline{66.1} & \underline{81.2} & \underline{85.7} \\
GPT-4o-mini &
            \textbf{88.5} & \textbf{92.4} & \textbf{98.1} & \textbf{99.1} & 
            \textbf{89.3} & \textbf{91.6} & \textbf{98.4} & \textbf{99.3} & 
            \textbf{89.3} & \textbf{91.6} & \textbf{98.4} & \textbf{99.3} & 
            \textbf{80.9} & \textbf{82.1} & \textbf{93.3} & \textbf{95.7} & 
            \textbf{73.9} & \textbf{74.3} & \textbf{88.2} & \textbf{92.1} \\
      \bottomrule
    \end{tabular}
  \end{adjustbox}
  \caption{\textbf{Text-only Topic Matching:} LVLMs must select the candidate sentence (topic) of 4 choices that aligns topically with the reference topics ($k$ reference sentences). See Table \ref{tab:main-results-image-text} for further details.}
  \label{tab:main-results-text}
  \vspace{-0.2cm}
\end{table*}

We categorize the languages in MVL-SIB based on their `resourceness'. To do so, we reorganize the language groups from \citet{joshi-etal-2020-state} into four tiers. We first rank the tiers w.r.t. Wikipedia size and then merge (i) the two highest-resource tiers and (ii) the two lowest-resource tiers.\footnote{Sorting by Wikipedia size (in number of pages) swaps tiers 1 and 2; we then merge Tier 0 with the new Tier 1, as well as tiers 4 and 5.} This both better reflects current corpus availability for LLM pre-training \citep{xue2021mt5,kudugunta2023madlad400},
% thanks to extensive data collection efforts
and aligns with downstream performance (cf. Appendix \ref{app:tier-perf-plot}). We isolate English from Tier 4, since it is the pivotal language in NLP. The full per-language results by task and model are provided in Appendix \ref{app:per-language-results}.

\subsection{Cross-modal Topic Matching}

\rparagraph{Images-To-Sentence (\its)} The upper segment of Table \ref{tab:main-results-image-text} displays the results for \its, in which the LVLMs must pick the candidate sentence that topically matches the $k$ reference images. 

\iparagraph{English} The performance on \its with English candidate sentences scales well with model size. The small Qwen2-VL 2B performs only slightly better than chance (25\% vs. ca. 35\%).  The comparably sized Qwen2-VL 7B, InternVL 2.5 8B, and Centurio-Qwen 8B peak around 62\% to 68\% at various $k$. These models nevertheless non-negligibly trail GPT-4o-mini (78.1\%). Among LVLMs, only InternVL 2.5 8B, Centurio-Qwen, and GPT-4o-mini benefit from multiple reference images. When the number of references $k$ increases from 3 to 5, GPT-4o-mini declines slightly in performance, while InternVL and Centurio-Qwen continue to improve marginally (ca. +1\%) and more notably (ca. +3-6\%), respectively. All other LVLMs deteriorate materially with more reference images (ca. -3{-}4\%).
mSigLIP indeed is a strong baseline, trailing only GPT-4o-mini and InternVL 2.5 8B at $k{=}5$. The ViT yields large gains with 4 more images (+9.7\%).

\iparagraph{Tiers} The performance gap of other languages to English correlates well with their resource levels by language tier. When presented with candidate sentences in non-English high-resource languages (cf. Tier 4), GPT-4o-mini even performs better slightly better. For very low-resource languages in Tier 1, such as N'Koo or Tamazight, all models fail to perform better than chance (cf. Appendix \ref{app:images-to-sentences}). Among LVLMs, only GPT-4o-mini remains overall robust for topically matching sentences of low-resource languages to images, whereas other models drop severely in performance (ca. 15-20\%). While mSigLIP still performs well, it declines more notably than LVLMs on lower-resource languages.

% The results show that open-weight models struggle to utilize further context from additional reference images in cross-modal topic matching.

\rparagraph{Sentences-To-Image (\sti)} The lower part of Table \ref{tab:main-results-image-text} presents the results for \sti. Here, the models select the candidate image among four options that topically fits the $k$ reference sentences.

\iparagraph{English} Performance again correlates well with model capacity. However, in \sti, only GPT-4o-mini significantly seizes on additional references (+13\%) to excel with 89\%, while models like Qwen2-VL 7B and InternVL 2.5 8B exhibit peak performance at $k{=}1$ and $k{=}3$, respectively, that taper slightly with more sentence references. Centurio-Qwen performs only slightly better than random (25\% vs. ca. 35\%). In \sti, mSigLIP is again very strong, second only to GPT-4o-mini at $k{=}5$. The encoder once more seizes sizable gains from additional references (+13.3\%).

\iparagraph{Tiers} In non-English evaluations, the overall trend remains similar, though absolute performance is lower. The gap between high- and low-resource language tiers is evident, as all models yield higher scores across Tiers 4 and 3. GPT-4o-mini maintains robust performance even in the most challenging Tier 1 when provided multiple references (ca.75\%).

% The \sti results suggest that, again, only GPT-4o-mini utilizes additional references effectively. 
% This likely stems from insufficient pre-training and post-hoc VL instruction fine-tuning to enable models to perform higher-level semantic reasoning with diverse texts to images successfully.

% what does it show us?
\vspace{0.2cm}
\noindent \textbf{In sum}, both the training protocol and the model size collectively determine whether models favor \its or \sti. For instance, InternVL 2.5 8B outperforms Qwen2-VL 7B on \its across the board, while trailing on \sti.  Moreover, only GPT-4o-mini consistently seizes on additional references and remains largely robust to the lowest-resource languages on both tasks. This likely stems from insufficient training to enable open-weight LVLMs to perform higher-level VL reasoning with diverse texts and multiple images successfully. For instance, Centurio Qwen was mostly trained on data that prefixes images to text, resulting in low performance when images are postfixed to the context.

\subsection{Text-only Topic Matching}
\label{subsec:text-only-topic-matching}

Table \ref{tab:main-results-text} lists the results for text-only topic matching, in which the images are exchanged with their topic label. These tasks denominate `upper-bounds' for their cross-modal counterparts to enable ablations of language support and VL support in LVLMs.

\rparagraph{Topic-to-Sentence (\tts)}  In this task, LVLMs choose the sentence that best suits the reference topic. Barring Qwen2-VL 2B, all models perform well on the task. Notably, 5 images should capture the underlying topic well for all models (cf. Appendix \ref{app:images-to-topics}). Despite that, the gap between text-only and vision-language tasks (cf. Table \ref{tab:main-results-image-text}) is sizable across all open-weights models for `English' (ca. 20\%+). In English, GPT-4o-mini and InternVL 2.5 8B achieve the highest accuracies, indicating strong topic comprehension. For non-English languages, while the overall scores are reduced, high-resource languages benefit from richer training signals compared to their low-resource counterparts -- models like Qwen2-VL 2B and Centurio-Qwen 8B show a more pronounced drop in the latter, underscoring the impact of language resources.

\rparagraph{Sentences-to-Topic (\stt)} In \stt, where models choose the topic that best aligns with $k$ reference sentences, performance scales both with model size and the number of references. The gains from additional context ($3{-}5~k$) are particularly notable for larger LVLMs. In English, GPT-4o-mini improves markedly from 92.4\% at $k{=}1$ to 98.1\% at $k{=}3$. In non-English languages, similar patterns emerge: high-resource languages consistently yield higher accuracies than low-resource ones, with GPT-4o-mini and InternVL 2.5 8B exhibiting the most stable improvements across varying $k$. This reinforces the role of model capacity and training data diversity in effective cross-lingual  topic matching.

% Comparing the results from cross-modal and text-only topic matching offers further insight into the underlying VL interactions in LVLMs. For lower-resource languages, the performance gap between matching sentences to reference images versus textual topics narrows regardless of the number of references ($k$). This discrepancy likely stems from the limited support of lower-resource languages in LVLMs. In contrast, for lower-resource languages with limited vision-language support, the spread in performance between selecting the image or the topic for reference sentences becomes much more pronounced, most notably at $k{=}5$. This shows that VL support drops disproportionally t textual support for a language in LVLMs.

Comparing the results of cross-modal and text-only topic matching sheds further light on the VL interactions for lower-resource languages in LVLMs. The performance gap between matching sentences to reference images and matching them to textual topics tends to narrow regardless of the number of references, likely reflecting their limited textual support in LVLMs. In contrast, the discrepancy between selecting an image versus a topic for reference sentences becomes much more pronounced, especially at $k{=}5$. These findings suggest that VL support degrades more sharply than textual support for lower-resource languages in LVLMs.

\begin{figure*}[htb!]
  \centering
  % \captionsetup{labelformat=empty} % Remove the prefix for this table
  \begin{adjustbox}{width=\textwidth,center}
    \begin{tabular}{cc}
      % \toprule
      % \cmidrule(lr){1-2}
      % \multicolumn{2}{c}{} \\
      \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth,trim={0 0 0 0.6cm},clip]{latex/gfx/task_correlation/img2sent.pdf}
        \vspace{-1cm}
        \captionof{figure}{\its with $k{=}3$.}
        \label{fig:img2sent}
      \end{minipage} &
      \hspace{1cm}
      \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth,trim={0 0 0 0.6cm},clip]{latex/gfx/task_correlation/sent2img.pdf}
        \vspace{-1cm}
        \captionof{figure}{\sti with $k{=}3$.}
        \label{fig:sent2img}
      \end{minipage} \\
      % \bottomrule
    \end{tabular}
  \end{adjustbox}
   \vspace{-0.1cm}
   \caption*{\textbf{Correlations Between MVL-SIB \& Multilingual VL Benchmarks.} Pearson correlation coefficients obtained by regressing MVL-SIB performance against performance on multilingual VL tasks on languages common to both datasets, respectively. An asterisk (*) indicates whether the coefficient is statistically significant at $p \leq 0.05$.}
   \vspace{-0.4cm}
\end{figure*}

\subsection{Further Analyses}
\label{subsec:further-analyses}

\rparagraph{Task Correlation} To compare MVL-SIB with other tasks, we access the results for Qwen2-VL and InternVL 2.5 models on several established multilingual VL benchmarks from \citet{geigle2025centurio}.\footnote{We omit Centurio-Qwen, since it degrades on the \sti task. We further provide details on all the multilingual vision-language benchmarks we correlate MVL-SIB with in Appendix \ref{app:summary-mvl-benchmarks}}
Next, we align the results across languages for the MVL-SIB tasks and the other benchmarks. Finally, we plot the linear regressions of performance on \its and \sti with $k{=}3$ against the performance on the VL benchmarks, respectively, pooled over models, in Figures \ref{fig:img2sent} and \ref{fig:sent2img}.\footnote{Note that we include a constant in our regression model to bridge task-specific scales of results.}

MVL-SIB positively correlates with all tasks in both the \its and \sti evaluations. However, both the magnitude and statistical significance of these linear relationships vary across VL benchmarks. Both \its and \sti exhibit the strongest connections with XVNLI (visual inference),  BIN-MC (multiple-choice image classification), MaRVL, and M5-VGR (both visually-grounded boolean reasoning). Since these tasks restrict valid answers to a small set of fixed options (i.e., choice letters or `yes/no/maybe'), LVLMs must engage in higher‐level vision-language disambiguation rather than relying solely on lower‐level visual cues to solve these tasks.
In addition, xMMMU, M3Exam, and M5-VLOD are significantly related only to \its, whereas xGQA is significantly aligned solely with \sti. The former tasks are structurally similar to \its, typically presenting one or more images that LVLMs are given as visual context to answer multiple-choice questions.
We hypothesize that only \sti is significantly correlated with xGQA, since \sti is more analogous to object-centric benchmarks. In \sti, LVLMs likely leverage targeted semantic cues (e.g., keywords or phrases) from the reference sentences to better disambiguate candidate images by topic. This behavior aligns with lower-level VL tasks such as xGQA (cross-lingual short-form QA) or BIN-MC (multiple-choice object classification), where texts and images are more deliberately connected. 
In contrast, MTVQA, SMPQA‐name, and SMPQA‐ground show only weak or statistically insignificant correlations with the MVL‐SIB tasks. Since these tasks require LVLMs to comprehend text embedded in images, a low‐level, fine‐grained VL task, they differ substantially from the higher‐level VL reasoning evaluated by MVL‐SIB.

Overall, the regression analysis indicates that \its and \sti capture distinct yet complementary aspects of VL understanding, collectively exhibiting a strong relationship with a broad set of VL tasks. This aligns with our main results (cf. Table \ref{tab:main-results-image-text}), which show that different LVLMs may favor one MVL-SIB task over the other. This renders MVL-SIB as a suitable benchmark for evaluating \textit{universal} VL understanding (across 205 languages). It also enables to ablate performance across the key axes of analysis, the task formulation (\its vs. \sti), the language vs. vision-language support for 205 languages, and the number of images in context.\footnote{In \sti, candidates could comprise more than a single image.}

\begin{figure*}[ht]
  \begin{adjustbox}{width=\textwidth,center}
        \includegraphics[width=\textwidth, trim={0 1.65cm 0 0.2cm},clip]{latex/gfx/subgroups_cross_modal.pdf}
  \end{adjustbox}
  \begin{adjustbox}{width=\textwidth,center}
        \includegraphics[width=\textwidth, trim={0 0 0 0.2cm},clip]{latex/gfx/subgroups_text_only.pdf}
  \end{adjustbox}
  \vspace{-0.75cm}
  \caption{\textbf{Larger LVLMs on Subsampled Tiers.} We extract 3 languages per tier that mimic avg. performance full language groups (cf. \S\ref{subsec:further-analyses}) and evaluate LVLMs across all model sizes on \{\its,\sti,\tts,\stt\} @~$k{=}3$ (cf. \S\ref{sec:tasks}).}
  \label{fig:perf-subgroups}
  \vspace{-0.4cm}
\end{figure*}

\rparagraph{Larger LVLMs} To evaluate larger LVLMs on MVL-SIB, we construct language-tier subsets that reliably estimate performance while mitigating excessive computational overhead (cf. \S\ref{sec:results}). For both \its and \sti, we identify the three languages in each tier that best replicate the average performance of the tier. First, we compute the average performance per language tier for both \its and \sti with $k{=}3$, pooling results across models (cf. \S\ref{sec:results}). Then, for each tier, we select the three languages whose performance deviates least from the tier mean. The languages chosen for each tier by task are detailed in Appendix \ref{app:subsets-full-results}. Finally, we test GPT-4o, InternVL 2.5 \{26,32,72\}B, and Qwen 2.5 VL \{32,70\}B on these subsets to assess how well larger models perform across language tiers.

Figure \ref{fig:perf-subgroups} displays the results for both \its and \sti with $k{=}3$.\footnote{Qwen2-VL 72B frequently responded with the first letter of the correct topic. If that letter uniquely identifies the correct choice, the answer is considered correct.} We observe that larger models catch up to GPT-4o on \its and even outperform it on \sti for higher-resource languages. Since open-weight LVLMs have more limited language support for low-resource languages, GPT-4 and GPT-4o-mini outperform all other models on \its for languages in tier 1 and 2. Increasing model capacity yields the largest gains on \sti, for which the models exceed GPT-4o up to the lowest-resource Tier 1. Moreover, unlike smaller models, all larger LVLMs (+26B) more effectively leverage multiple image references to improve performance (cf. Appendix \ref{app:subsets-full-results}). They reap the largest benefit when the number of references increases from 1 to 3 (ca. +3\%), with further improvements at $k=5$ (ca. +1.5\%). These results suggest that larger LVLMs have fundamentally better VL support irrespective of the evaluated language (cf. \S\ref{subsec:text-only-topic-matching}). The results on text-only topic matching further underscore this notion (cf. lower segment of Figure \ref{fig:perf-subgroups}). Larger LVLMs near perfectly match both the reference topic to the correct sentence (ca. 90\%) and references sentences to the right topic (ca. 98\%). Notably, as model size increases, the performance gap between cross-modal and text-only matching narrows.  Collectively, these results further indicate that VL support relative to text-only support improves with increasing model capacity in LVLMs.
