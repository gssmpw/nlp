
\clearpage

\subsection{Overview of Multilingual Vision-Language Benchmarks}
\label{app:summary-mvl-benchmarks}

\begin{table*}[ht]
    \centering
\adjustbox{max width=\textwidth}{%
\large
\begin{tabular}{lcccccc}
\toprule
\textbf{Task} & \textbf{Dataset} & \textbf{Visual Input} & \textbf{Textual Input} & \textbf{Target Output} & \textbf{Metric} & \textbf{\#Lang.} \\
\midrule
\multirow{2}{*}{Captioning} 
    & \multirow{2}{*}{XM3600} & \multirow{2}{*}{Single-Image} & \multirow{2}{*}{Prompt (English)} & \multirow{2}{4cm}{\centering Caption\\(Target Language)} & CIDEr & $36$ \\
   & & & & & &  \\ 
\midrule
\multirow{2}{4cm}{Multiple-Choice Visual Question Answering} 
    & \multirow{2}{*}{BabelImageNet-MC} 
    & \multirow{2}{*}{Single-Image} 
    & \multirow{2}{*}{Question (Target Language)} 
    & \multirow{2}{*}{Letter of the correct Choice} 
    & \multirow{2}{*}{Relaxed Accuracy} 
    & \multirow{2}{*}{$20$} \\
   & & & & & &  \\ 
\midrule
\multirow{3}{4cm}{Text-Heavy Multiple-Choice Visual Question Answering} 
    & M3Exam 
    & \multirow{3}{*}{Single or Multi-Image} 
    & Question (Target Language) 
    & \multirow{3}{*}{Letter of the correct Choice} 
    & \multirow{3}{*}{Relaxed Accuracy} 
    & $7$ \\
    & MMMU     
    &  % Empty cell because of multirow
    & Question (English)        
    &  % Empty cell because of multirow
    &  % Empty cell because of multirow
    & $1$ \\
    & xMMMU    
    &  % Empty cell because of multirow
    & Question (Target Language) 
    &  % Empty cell because of multirow
    &  % Empty cell because of multirow
    & $7$ \\
\midrule
\multirow{2}{4cm}{Text-Heavy Visual\\Question Answering} 
    & MTVQA       
    & \multirow{2}{*}{Single-Image} 
    & \multirow{2}{*}{Question (Target Language)} 
    & \multirow{2}{*}{Word or Phrase (Target Language)} 
    & \multirow{2}{*}{Exact Accuracy} 
    & $9$ \\
    & SMPQA - Name 
    &  % empty cell for Single-Image (multirow)
    &  % empty cell for Question (Target Language) (multirow)
    &  % empty cell for Word or Phrase (Target Language) (multirow)
    &  % empty cell for Exact Accuracy (multirow)
    & $11$ \\
\midrule
\multirow{2}{4cm}{Text-Heavy Visually\\ Grounded Reasoning}  & \multirow{2}{*}{SMPQA - Ground} & \multirow{2}{*}{Single-Image} & \multirow{2}{*}{Question (Target Language)} & \multirow{2}{*}{'yes' / 'no'} & \multirow{2}{*}{Exact Accuracy} & \multirow{2}{*}{$11$} \\
& & & & & &  \\ 
\midrule
\multirow{2}{4cm}{Visio-Linguistic \\ Outlier Detection} 
    & \multirow{2}{*}{M5B-VLOD} & \multirow{2}{*}{Multi-Image} & \multirow{2}{*}{Hypothesis (Target Language)} & \multirow{2}{*}{Letter of the correct Choice} & \multirow{2}{*}{Relaxed Accuracy} & \multirow{2}{*}{$12$} \\
& & & & & &  \\ 
\midrule
\multirow{2}{4cm}{Visual Natural\\ Language Inference} 
    & \multirow{2}{*}{XVNLI} & \multirow{2}{*}{Single-Image} & \multirow{2}{*}{Hypothesis (Target Language)} & \multirow{2}{*}{'yes' / 'no' / 'maybe'} & \multirow{2}{*}{Exact Accuracy} & \multirow{2}{*}{$5$} \\
   & & & & & &  \\ 
\midrule
\multirow{2}{*}{Visual Question Answering} 
    & MaXM 
    & \multirow{2}{*}{Single-Image} 
    & \multirow{2}{*}{Question (Target Language)} 
    & Word or Phrase (Target Language) 
    & \multirow{2}{*}{Exact Accuracy} 
    & $6$ \\
    & xGQA 
    &  % empty cell because of multirow in column 3
    &  % empty cell because of multirow in column 4
    & Word or Phrase (English) 
    &  % empty cell because of multirow in column 6
    & $8$ \\ \midrule
    \multirow{2}{*}{Visually Grounded Reasoning} 
    & M5B-VGR
    & \multirow{2}{*}{Multi-Image} 
    & \multirow{2}{*}{Hypothesis (Target Language)} 
    & \multirow{2}{*}{'yes' / 'no'} 
    & \multirow{2}{*}{Exact Accuracy} 
    & $12$ \\
    & MaRVL   
    &  % empty cell (Multi-Image)
    &  % empty cell (Hypothesis (Target Language))
    &  % empty cell ('yes' / 'no')
    &  % empty cell (Exact Accuracy)
    & $6$ \\

\bottomrule
\end{tabular}    }
    \caption{Summary of multilingual vision-language benchmarks we correlate MVL-SIB against. Relaxed 
 denotes responses that start with the correct option letter (cf. \ref{sec:experimental-setup}).}
    \label{appendix:tab:eval_suite_datasets}
\end{table*}

\rparagraph{xGQA} The xGQA dataset~\citep{pfeiffer-etal-2022-xgqa} is a cross-lingual visual question-answering resource. It extends the well-known English-only GQA dataset~\citep{hudson_gqa_2019} by providing manual translations of the questions in the balanced \textit{test-dev} set. The dataset contains $9666$ questions available in eight languages across five scripts, while the answers remain in English. In addition, it features $300$ unique images from Visual Genome~\citep{krishna_visual_2017}.

%%%%%%%%%%%%%
%
\rparagraph{MaXM}
%
MaXM, introduced by~\citet{changpinyo-etal-2023-maxm}, is a VQA dataset covering seven languages written in five scripts. In this dataset, both the questions and their corresponding answers are presented in the same language. The images are drawn from a subset of the XM3600~\citep{thapliyal_xm3600_2022} dataset and are selected to correspond to regions where the question-answer pair’s language is spoken, ensuring both linguistic and cultural diversity.

%%%%%%%%%%%%%%
%
\rparagraph{XVNLI}
%
The XVNLI dataset~\citep{bugliarello-etal-2022-iglue} introduces the task of Cross-lingual Visual Natural Language Inference, where a model must determine if a textual hypothesis \textit{entails}, \textit{contradicts}, or is \textit{neutral} with respect to a visual premise. This dataset spans five languages across three scripts and includes $357$ unique images from Visual Genome. It is built upon a combination of the text-only SNLI~\citep{bowman_large_2015} dataset and its cross-lingual~\citep{agic_cli_2018} and cross-modal~\citep{xie_visual_entailment_2019} counterparts.

%%%%%%%%%%%%%%%
%
\rparagraph{MaRVL}
%
The MaRVL dataset~\citep{liu-etal-2021-visually} is designed to benchmark models on Multicultural Reasoning over Vision and Language. Each sample consists of two images, a textual statement, and a binary (true/false) answer grounded in the images. Covering five languages across three scripts, MaRVL includes $4914$ culturally diverse images that align with the respective languages. The images in each sample are selected to reflect the culture of the annotator who composed the textual statement in their native language.

%%%%%%%%%%%%%%%
%
\rparagraph{XM3600}
%
The XM3600 dataset~\citep{thapliyal_xm3600_2022} is an extensive multilingual image captioning resource encompassing 36 languages. It contains $261375$ captions across 13 scripts, with 100 unique images per language. The images are chosen to reflect the cultural background of the language, ensuring both cultural and linguistic diversity. All captions were manually produced by professional native speakers rather than being automatically generated. Due to the dataset's large size, we evaluate XM3600 using a randomly selected subset of 500 images per language.

%%%%%%%%%%%%%%
%
\rparagraph{Babel-ImageNet (multiple-choice) (BIN-MC)}
%
Babel-ImageNet~\cite{geigle-etal-2024-babel} translates ImageNet’s~\cite{deng_imagenet_2009} labels into nearly 300 languages, allowing us to assess whether models can recognize and correctly link diverse ImageNet objects to their labels in the target language. Given the computational cost, we focus on languages that appear in only one or two other datasets, in addition to English and a select few high-resource languages, and we use 10 images per class instead of 50. We follow \citet{geigle-etal-2024-african} and frame the task as a multiple-choice problem by mining hard negative options from the complete label pool. This approach avoids the ambiguity inherent in traditional open-ended VQA formats. Negatives are selected based on the English labels, filtering out candidates not translated by Babel-ImageNet into the target language, and ultimately choosing the three most similar negative labels available.

%%%%%%%%%%%%%%
%
\rparagraph{SMPQA}
%
\citet{geigle2025centurio} introduce SMPQA (Synthetic Multilingual Plot QA) as a test dataset for evaluating multilingual OCR capabilities for bar plots and pie charts, covering 11 languages and various scripts and resource levels.

%%%%%%%%%%%%%%%%%%%%
%
\rparagraph{M5B-VGR}
%
The M5B-VGR dataset, presented by~\cite{schneider-sitaram-2024-m5}, is a visually grounded reasoning benchmark akin to MaRVL. Each sample comprises two images, a textual statement, and a binary (true/false) answer based on the images. It spans 12 languages across 7 scripts and features culturally diverse photos from regions where the respective languages are spoken. The images are sampled from the Dollar Street~\cite{gaviria2022dollar} dataset, with 120 samples provided per language.

%%%%%%%%%%%%%%%%%%%%
%
\rparagraph{M5B-VLOD}
%
The M5B-VLOD (Visio-Linguistic Outlier Detection) dataset, also introduced by~\cite{schneider-sitaram-2024-m5}, consists of samples containing five images paired with a textual statement that is true for all but one image. The task is to identify the outlier image that does not match the statement. This dataset covers the same 12 languages as M5B-VGR, with images sampled in a similar manner from the same source, and provides 120 samples per language.

%%%%%%%%%%%%%%%%%%%%%%%
%
\rparagraph{MTVQA}
%
The MTVQA dataset, introduced by~\cite{tang2024mtvqa}, features text-heavy visual question answering tasks. It includes expert human annotations in 9 diverse languages, comprising 6778 question-answer pairs across 2116 images. The images predominantly contain text in the corresponding language, with questions and answers closely tied to that text. These images are sourced from various publicly available datasets.

%%%%%%%%%%%%%%%%%%%%%%
%
\rparagraph{CVQA}
%
The CVQA dataset, introduced by~\cite{romero2024cvqa}, is a multilingual and culturally nuanced VQA benchmark that includes a broad array of languages, many of which are underrepresented in NLP. It consists of 10000 questions spanning 30 countries and 31 languages, forming 39 distinct country-language pairs (for instance, Spanish appears in 7 different splits corresponding to 7 Spanish-speaking countries). The images were manually collected by human annotators to accurately depict the culture associated with each country-language pair. Each sample includes one image and a question in the respective language. Although the test set is not publicly available, the authors permit up to 5 daily leaderboard submissions for evaluation.

%%%%%%%%%%%%%%%%%%%%%%
%
\rparagraph{M3Exam}
%
The M3Exam dataset, presented by~\cite{zhang_m3exam_2023}, contains real-world exam questions in 9 languages, available as either text-only or multimodal samples. For our evaluation, we only include samples that require at least one image. Due to the limited number of samples for Swhalili and Javanese, we focus on the remaining 7 languages. The selected samples consist of multiple-choice questions in the target language, accompanied by up to 8 images that may appear in both the question and the answer options, with the number of choices ranging from 4 to 8 per sample.

%%%%%%%%%%%%%%%%%%%%%%
%
\rparagraph{xMMMU}
%
xMMMU, introduced by~\cite{yue2024pangea}, comprises college-level multiple-choice VQA samples in seven languages. It was automatically translated using GPT4o from a randomly selected subset of 300 questions from the MMMU~\cite{yue_mmmu_2023} validation split.

\subsection{Prompts}
\label{appendix:sec:evaluation:prompts}
%
We list the prompts for each dataset in our test suite used for all models in Figure~\ref{appendix:fig:evaluation:prompts}.
%
\begin{figure*}[ht!]\tiny
    \centering
    %
    \begin{promptbox}{SMPQA}
    <IMG>\{QUESTION\}\textbackslash{n}Answer the question using a single word or phrase.
    \end{promptbox}
    %
    %
    \begin{promptbox}{CVQA}
    <IMG>\{QUESTION\}\textbackslash{n}There are several options:\textbackslash{n}A. \{OPTION A\}\textbackslash{n}B. \{OPTION B\}\textbackslash{n}C. \{OPTION C\}\textbackslash{n}D. \{OPTION D\}\textbackslash{n}Answer with the option's letter from the given choices directly.
    \end{promptbox}
    %
    %
    \begin{promptbox}{xMMMU}
    \{QUESTION\}\textbackslash{n}There are several options:\textbackslash{n}A. \{OPTION A\}\textbackslash{n}B. \{OPTION B\}\textbackslash{n}C. \{OPTION C\}\textbackslash{n}D. \{OPTION D\}\textbackslash{n}Answer with the option's letter from the given choices directly.
    \end{promptbox}
    %
    %
    \begin{promptbox}{MTVQA}
    <IMG>\{QUESTION\}\textbackslash{n}Answer the question using a single word or phrase.\textbackslash{n}Answer in \{LANGUAGE\}.
    \end{promptbox}
    %
    %
    \begin{promptbox}{M3Exam}
    \{QUESTION\}\textbackslash{n}Options:\textbackslash{n}A. \{OPTION A\}\textbackslash{n}B. \{OPTION B\}\textbackslash{n}C. \{OPTION C\}\textbackslash{n}D. \{OPTION D\}\textbackslash{n} Answer with the option's letter from the given choices directly.
    \end{promptbox}
    %
    %
    \begin{promptbox}{BIN-MC}
    <IMG>Which of these choices (in English) is shown in the image?\textbackslash{n} Choices:\textbackslash{n}A. \{CHOICE A\}\textbackslash{n}B. \{CHOICE B\}\textbackslash{n}C. \{CHOICE C\}\textbackslash{n}D. \{CHOICE D\}\textbackslash{n} Answer with the letter from the given choices directly.
    \end{promptbox}
    %
    \begin{promptbox}{xGQA}
    <IMG>\{QUESTION\}?\textbackslash{n}Answer the question using a single word or phrase.\textbackslash{n}Answer in English.
    \end{promptbox}
    %
    \begin{promptbox}{MaXM}
    <IMG>\{QUESTION\}?\textbackslash{n}Answer the question using a single word or phrase.\textbackslash{n}Answer in \{LANGUAGE\}.
    \end{promptbox}
    %
    \begin{promptbox}{MaRVL}
    <IMG>Given the two images <IMG><IMG>, is it correct to say ``\{HYPOTHESIS\}''? Answer yes or no.'
    \end{promptbox}
    %
    \begin{promptbox}{XVNLI}
    <IMG>Is it guaranteed true that ``\{HYPOTHESIS\}''? Yes, no, or maybe? Answer in English.
    \end{promptbox}
    %
    \begin{promptbox}{M5-VGR}
    Given the two images <IMG><IMG>, is it correct to say ``\{HYPOTHESIS\}''? Answer yes or no.'
    \end{promptbox}
    %
    \begin{promptbox}{M5-VLOD}
    Based on the 5 images <IMG><IMG><IMG><IMG><IMG> ordered from top-left to bottom-right, which image does not match the hypothesis ``\{HYPOTHESIS\}''? Choose one from [A, B, C, D, E] and only output a single letter:
    \end{promptbox}
    %
    \begin{promptbox}{XM3600}
    Briefly describe the image in \{LANGUAGE\} in one sentence.
    \end{promptbox}
    %
    %
    \caption{Prompts used for the different datasets of our test suite. For M3Exam and xMMMU, the questions contain images at individual positions, and also the options can consist of images. In total, a sample of M3Exam can contain up to 8 images and 8 options, and a sample of xMMMU can contain up to 4 images and 4 options.}
    \label{appendix:fig:evaluation:prompts}
\end{figure*}