\clearpage
\subsection{Further Details}
\label{app:subsec:exp-details}

\rparagraph{Models} We test state-of-the-art instruction fine-tuned LVLMs across various sizes. Smaller models are evaluated on all languages, while larger LVLMs (26B+) are tested on subsets of the MVL-SIB languages (cf. \S\ref{subsec:further-analyses}).

\iparagraph{GPT-4o} We evaluate MVL-SIB on GPT-4o-mini-\textit{2024-07-18} and GPT-4o-\textit{2024-08-06}. We set the image detail in the API to `low', since our tasks require high-level reasoning that does not depend on finer image details.\footnote{We use GPT-4o-mini because evaluating GPT-4o would be too expensive.} Prior works show that GPT-4o is the best-performing multilingual LVLM \cite{schneider-sitaram-2024-m5, vayani2024alm}.

\iparagraph{Qwen2-VL} Qwen2-VL ties a 675M parameter vision-transformer (ViT) into Qwen2 LLMs \cite{wang2024qwen2vl}. An MLP compresses adjacent 2×2 visual tokens embedded by the ViT into one token representation, which is then input to the LLM. 

\iparagraph{InternVL 2.5} Depending on the model size, InternVL uses Qwen2.5 or InternLM as its LLM backbone  \cite{chen2024internvl}. The model embeds images either by a 6B or by a distilled 300M ViT pretrained with CLIP \cite{radford2021clip}. The resulting image patch encodings are downsampled by factor 4 and fed through an MLP to the LLM.

\iparagraph{Centurio} Centurio is the latest massively multilingual LVLM trained on 100 languages \cite{geigle2025centurio}, outperforming alternatives like Parrot \cite{sun2024parrot} or Pangea \cite{yue2024pangea}. It employs Qwen2.5 as its LLM \cite{yang2024qwen2technicalreport} and \texttt{SigLIP SO400/384} as its ViT \cite{zhai2023sigmoid}. The model mixes resolutions by stacking the encodings of the full image and those of 2×2 tiles along the features. The combined embedding is then projected via an MLP to the LLM's input space.

Besides architectures, the LVLMs crucially differ in dataset mixtures on which they were trained. Centurio translates image-caption, VQA, OCR, and a few multi-image datasets to 100 languages with NLLB \cite{nllbteam2022language} to mix 50:50 with the original English data. Qwen2-VL and InternVL, however, were trained on much larger, more diverse datasets that comprise sizable multi-image comparison and video understanding datasets. Moreover, assuming that the LLMs of Qwen2-VL, InternVL, and GPT-4o were pretrained on Flores, their performance would be overly optimistic.


\begin{figure*}[ht]
  \subsubsection{Performance by Model over Languages grouped by Language Tier}
  \label{app:tier-perf-plot}
  \centering
  \adjustbox{max width=\textwidth}{%
    \includegraphics{latex/gfx/tier_img2sent.pdf}%
  }
  \vspace{-0.8cm}
  \caption{\textbf{Images-To-Sentences~@~$k{=}3$.} The English prompt describes the cross-modal  topic matching task, lists all topics, and provides both $k{=}3$ reference images and 4 sentences in the corresponding language \{\texttt{eng\_Latn}, $\dots$, \texttt{nqo\_Nkoo}\}. LVLMs must select the sentence of 4 options that topically fits $k{=}3$ reference images. The sentences spanning 205 languages and 7 topics are drawn from SIB-200 \cite{adelani-etal-2024-sib}, while images for the topics were hand-selected (cf. Appendix \ref{app:images-per-topic}). An example prompt is shown in Appendix \ref{app:images-to-sentences}; further details are in \S\ref{sec:experimental-setup}. \\ \textbf{Plot.} The x-axis orders the languages of the candidate sentences \{\texttt{eng\_Latn}, $\dots$, \texttt{nqo\_Nkoo}\}, respectively, by descending performance (y-axis). The top x-axis indicates the running index of each language $L_i$ ($i \in \{1, \dots, 205\}$). \\
  \textbf{Tiers.} The languages are grouped by tiers derived from \citet{joshi-etal-2020-state} (cf. \S\ref{sec:results}).}
  \vspace{-0.1cm}
  \label{fig:tier-img2sent}
\end{figure*}


\begin{figure*}[ht]
 \subsubsection{Calibration Analysis of Cross-Modal Topic Matching}
 \label{app:fig:calibration}
 \centering
 \adjustbox{max width=\textwidth}{%
   \includegraphics{latex/gfx/calibration.pdf}%
 }
 \caption{ \textbf{Calibration Analysis of Cross-Modal Topic Matching for InternVL 2.5 8B.} \textbf{Analysis:} To assess the reliability of cross-modal topic matching with fewer samples than our full dataset (1004 samples per language), we randomly select 500 trajectories. We then compute performance metrics on cumulative subsets, incrementing by 50 examples at each step. The difference in performance between the full dataset and each subset is calculated to quantify the deviation at each sample size. \textbf{Plot:} The plot displays the average absolute spread in performance (averaged over all languages) along with the standard deviation for InternVL 2.5 8B. We restrict ourselves to a single open-weight LVLM, since the analysis yields identical results across all combinations of LVLMs and language tiers. \textbf{Insights:} Our analysis shows that performance stabilizes rapidly, with deviations of only about 1\% observed at 1,004 instances -- same size as the subset from which the dataset was created. This indicates that reliable evaluation of cross-modal topic matching can be achieved with far fewer than 3,012 samples. }
 \vspace{-0.1cm}
\end{figure*}