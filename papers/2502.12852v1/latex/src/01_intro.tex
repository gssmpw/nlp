\section{Introduction}
\label{sec:intro}

\begin{figure}[ht]
\begin{adjustbox}{width=\columnwidth,center}
\begin{tcolorbox}[
    colback=white!95!black,    % Background color
    colframe=black,            % Frame color
    title=Images-To-Sentences (\its), % Box title
    fonttitle=\bfseries\Large,       % Title font styling
    boxrule=0.5pt,             % Frame thickness
    arc=4pt,                   % Rounded corners
    outer arc=4pt,
    width=\textwidth,          % Box width
    % boxsep=-2mm,
    boxsep=1pt,
    left=7pt,
    right=7pt,
    enlarge left by=0mm,
    enlarge right by=0mm,
    before skip=0.5em,           % Space before the box
    after skip=0.5em,            % Space after the box
]

Which sentence best matches the topic of the images? The images and the sentences each belong
to one of the following topics: "entertainment", "geography", "health", "politics", "science and technology", "sports", or "travel". Choose one sentence from A, B, C, or D. Output only 
a single letter!

\medskip

\# Images

\begin{center}
    \begin{minipage}{0.18\textwidth}
        \centering
        \includegraphics[width=\linewidth]{latex/gfx/i2s/i2s_image1.jpg}
        % \vspace{0.5em}
        % \textbf{(a)}
    \end{minipage}
    \hfill
    \begin{minipage}{0.18\textwidth}
        \centering
        \includegraphics[width=\linewidth]{latex/gfx/i2s/i2s_image2.jpg}
        % \vspace{0.5em}
        % \textbf{(b)}
    \end{minipage}
    \hfill
    \begin{minipage}{0.18\textwidth}
        \centering
        \includegraphics[width=\linewidth]{latex/gfx/i2s/i2s_image3.jpg}
        % \vspace{0.5em}
        % \textbf{(c)}
    \end{minipage}
    \hfill
    \begin{minipage}{0.18\textwidth}
        \centering
        \includegraphics[width=\linewidth]{latex/gfx/i2s/i2s_image4.jpg}
        % \vspace{0.5em}
        % \textbf{(d)}
    \end{minipage}
    \hfill
    \begin{minipage}{0.18\textwidth}
        \centering
        \includegraphics[width=\linewidth]{latex/gfx/i2s/i2s_image5.jpg}
        % \vspace{0.5em}
        % \textbf{(e)}
    \end{minipage}
\end{center}

\medskip

\# Sentences

\begin{enumerate}[label=\Alph*., itemsep=0pt, topsep=0pt]
    \item \textasciigrave\textasciigrave\textasciigrave Maroochydore führte am Ende die Rangfolge an, mit sechs Punkten Vorsprung vor Noosa als Zweitem.\textasciigrave\textasciigrave\textasciigrave
    \item \textasciigrave\textasciigrave\textasciigrave 
    Es wurden keine schwere Verletzungen gemeldet, jedoch mussten mindestens fünf der zur Zeit der Explosion Anwesenden aufgrund von Schocksymptomen behandelt werden.\textasciigrave\textasciigrave\textasciigrave
    \item \textasciigrave\textasciigrave\textasciigrave 
    Finnland ist ein großartiges Reiseziel für Bootstouren. Das „Land der tausend Seen“ hat auch Tausende von Inseln – in den Seen und in den Küstenarchipelen.\textasciigrave\textasciigrave\textasciigrave
    \item \textasciigrave\textasciigrave\textasciigrave 
    Es ist auch nicht erforderlich, dass Sie eine lokale Nummer von der Gemeinde erhalten, in der Sie leben. Sie können eine Internetverbindung über Satellit in der Wildnis v on Chicken in Alaska erhalten und eine Nummer auswählen, die vorgibt, dass Sie im sonnigen Arizona sind.\textasciigrave\textasciigrave\textasciigrave
\end{enumerate}

Your answer letter: 
\end{tcolorbox}
\end{adjustbox}
\vspace{-0.7cm}
\caption{Cross-modal topic matching `Images-To-Sentence' for German with $k{=}5$ reference images.} 
\vspace{-0.8cm}
\label{fig:intro-prompt-i2s}
\end{figure}

 \begin{figure*}[ht]
   \centering
   \adjustbox{max width=\textwidth,trim={0 0 0 0.4cm},clip}{%
     \includegraphics{latex/gfx/intro_img2sent.pdf}%
   }
   \vspace{-0.8cm}
   \caption{\textbf{Images-To-Sentences~@~$k{=}3$.} The English prompt describes the cross-modal topic matching task, lists all topics, and provides both $k{=}3$ reference images and 4 sentences in the corresponding language \{\texttt{eng\_Latn}, $\dots$, \texttt{nqo\_Nkoo}\}. LVLMs must select the sentence of 4 options that topically fits $k{=}3$ reference images. The sentences spanning 205 languages and 7 topics are drawn from SIB-200 \cite{adelani-etal-2024-sib}, while images for the topics were hand-selected (cf. Appendix \ref{app:images-per-topic}). An example prompt is shown in Appendix \ref{app:images-to-sentences}; further details are in \S\ref{sec:experimental-setup}. \\ \textbf{Plot.} The x-axis orders the languages of the candidate sentences \{\texttt{eng\_Latn}, $\dots$, \texttt{nqo\_Nkoo}\}, respectively, by descending performance (y-axis). The top x-axis indicates the running index of each language $L_i$ ($i \in \{1, \dots, 205\}$).}
   \vspace{-0.3cm}
   \label{fig:intro-img2sent}
 \end{figure*}

Large Vision-Language Models (LVLMs) extend Large Language Models (LLMs) to take images as inputs, leveraging their advanced language capabilities for vision-language (VL) tasks like image captioning and visual question answering (VQA). However, LVLMs are typically trained mainly on English data, leading to significant limitations despite the base LLMs' multilingual abilities. They may fail to follow instructions or struggle to interpret text within images in Non-English languages \cite{schneider-sitaram-2024-m5,tang2024mtvqa}. \\
Although many multilingual VL benchmarks exist, they typically cover at most 10 languages \cite[\textit{inter alia}]{bugliarello-etal-2022-iglue,liu-etal-2021-visually,tang2024mtvqa}. Only concurrent work has scaled VL evaluation to 100 languages using machine translation (MT) with human post-editing \cite{vayani2024alm}. Nevertheless, benchmarks constructed using semi-manual MT cannot support truly low-resource languages adequately, as current MT models lack the necessary quality for these languages. Moreover, existing benchmarks primarily assess lower-level VL semantics through concrete text-image relationships, such as those found in VQA. This underscores the need for VL benchmarks that cover truly low-resource languages and evaluate more abstract VL interactions. 

To address these challenges, we introduce the massively multilingual vision-language SIB (MVL-SIB) dataset, which extends the topic labels of the multi-way parallel sentences from SIB-200 \cite{adelani-etal-2024-sib} by associating each topic with hand-selected images. MVL-SIB evaluates cross-modal image-text topic matching in 205 languages: LVLMs must select one of 4 candidate sentences that best matches the topic of the reference images (`images-to-sentence', cf. Figure \ref{fig:intro-prompt-i2s}) or, conversely, choose one of 4 candidate images corresponding to the topic of the reference sentences (`sentences-to-image'). Figure~\ref{fig:intro-img2sent} displays the `images-to-sentence' performance for across all languages in MVL-SIB, sorted in descending order.
Notably, GPT-4o-mini performs robustly on the top 125 languages. However, beyond that range its performance declines sharply, falling to chance levels in the lowest-resource languages, such as N'Koo. %It, however, declines sharply beyond that range, dropping to chance-level performance for the lowest-resource languages such as N'Koo.
Bridging these gaps is crucial for developing genuinely inclusive VL technology.

% benchmark
% evaluation



\rparagraph{Contributions} \textbf{1)} MVL-SIB supports parallel VL evaluation in 205 languages on professionally translated texts, a 105 more languages than any other VL benchmark. The tasks, images-to-sentence and sentences-to-images with prefix and postfix images in context, respectively, allows for fine-grained analysis of VL interactions. We also define corresponding text-only tasks by replacing the images with the topic label to compare the VL support (by language) of LVLMs against the text-only support of their underlying LLMs. Both tasks allow to vary the number of included images to analyze the shift from single to multi-image support in LVLMs.
\textbf{2)} We thoroughly evaluate LVLMs on cross-modal image-text topic matching, finding that task performance is closely associated with both model size and the size of pre-training corpora of the respective languages. % though ultimately dropping to random chance for the lowest resource languages part of MVL-SIB.
We further find that only GPT-4o-mini seizes on multiple references in both cross-modal tasks. Open-weights LVLMs, moreover, favor one of the two tasks, highlighting the asymmetry in their VL support.
\textbf{3)} We analyze the relationship between stand-alone text and vision-language support in LVLMs by also benchmarking LVLMs on text-only topic matching. The performance gap between matching sentences to reference images or the topic tends to be larger the better the LVLM supports the underlying language. Conversely, the spread in performance between picking the fitting image or topic for reference sentences increases the worse the vision-language support of the LVLM for the evaluated language is.
\textbf{4)} We correlate MVL-SIB with established multilingual VL benchmarks on the languages shared between the respective pairs of datasets, showing that the MVL-SIB tasks align well with all VL tasks except OCR. We further show that images-to-sentence and sentence-to-image probe distinct aspects of VL interaction, as certain benchmarks correlate more strongly with one task than with the other. This analysis shows that MVL-SIB constitutes a reliable and comprehensive VL benchmark for the lower-resource languages that are not covered by other datasets.
