\begin{abstract}

% Existing multilingual vision-language benchmarks often only cover a handful of languages and focus on concrete facets of vision-language interaction in tasks like vision-grounded question answering or cultural reasoning. Consequently, evaluations of large vision-language models (LVLMs) are constrained twofold: they predominantly target high-resource languages and rely on more narrowly defined tasks. However, LVLMs are trained primarily in English, which hampers their effectiveness especially on lower-resource languages. This underscores the need for evaluation data for such languages. To address these limitations, we introduce MVL-SIB, a massively multilingual vision-language benchmark that evaluates both cross-modal and text-only topic matching across 205 languages -- over 100 more than existing datasets. We comparatively evaluate open-weights LVLMs and GPT-4o(-mini). Our results reveal that LVLMs struggle with cross-modal topic matching in lower-resource languages, performing no better than chance on languages like N'Koo. We further observe that open-weight LVLMs do not benefit from representing a topic with more than one image, demonstrating that these models remain ineffective at handling multi-image tasks. Our analysis also shows that vision-language support in LVLMs declines disproportionately relative to textual support for lower-resource languages, as evidenced by the outcomes of both cross-modal and text-only topic matching tasks. Finally, by correlating performance on MVL-SIB with other multilingual vision-language benchmarks, we highlight that MVL-SIB serves as a comprehensive probe of vision-language understanding in LVLMs.
Existing multilingual vision-language (VL) benchmarks often only cover a handful of languages. Consequently, evaluations of large vision-language models (LVLMs) predominantly target high-resource languages, underscoring the need for evaluation data for low-resource languages. To address this limitation, we introduce MVL-SIB, a massively multilingual vision-language benchmark that evaluates both cross-modal and text-only topical matching across 205 languages---over 100 more than the most multilingual existing VL benchmarks encompass. 
%%
We then benchmark a range of of open-weight LVLMs together with GPT-4o(-mini) on MVL-SIB. Our results reveal that LVLMs struggle in cross-modal topic matching in lower-resource languages, performing no better than chance on languages like N'Koo. Our analysis further reveals that VL support in LVLMs declines disproportionately relative to textual support for lower-resource languages, as evidenced by comparison of cross-modal and text-only topical matching performance. 
We further observe that open-weight LVLMs do not benefit from representing a topic with more than one image, suggesting that these models are not yet fully effective at handling multi-image tasks.
By correlating performance on MVL-SIB with other multilingual VL benchmarks, we highlight that MVL-SIB serves as a comprehensive probe of multilingual VL understanding in LVLMs.
\end{abstract}