\section{Conclusion}
\label{sec:conclusion}
%

We present the massively multilingual vision-language benchmark MVL-SIB for cross-modal (and text-only topic matching) in 205 languages that offers key advantages over prior multilingual VL benchmarks. Notably, it covers over 100 additional languages without relying on machine translation. MVL-SIB allows for a clear separation between textual language support and vision-language support in LVLMs by comparing performance on mirrored cross-modal and text-only tasks. Moreover, it allows us to study how LVLMs handle single-image versus multi-image formulations of cross-modal topic matching by varying the number of images provided.
In our comparative evaluation of state-of-the-art LVLMs on MVL-SIB, we find that model performance is strongly correlated with both model size and the volume of available pre-training data for each language. However, all LVLMs experience a dramatic performance drop on the lowest-resource languages. Our analysis further reveals that vision-language support deteriorates disproportionately relative to language support, highlighting the need to incorporate low-resource languages into VL training. Moreover, providing multiple images does not benefit open-weight LVLMs in cross-modal topic matching, suggesting that LVLMs are not yet fully effective in multi-image tasks. Lastly, we validate that MVL-SIB correlates well with existing multilingual VL benchmarks, underscoring its reliability as a source of evaluation data for 205 languages.