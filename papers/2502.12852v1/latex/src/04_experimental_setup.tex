\section{Experimental Setup}
\label{sec:experimental-setup}

% \rparagraph{Models} We test state-of-the-art LVLMs across various sizes. Smaller models are evaluated on all languages, while larger LVLMs (26B+) are tested on subsets of the MVL-SIB languages (cf. \S\ref{subsec:further-analyses}).

% \iparagraph{GPT-4o} We evaluate MVL-SIB on GPT-4o-mini-\textit{2024-07-18} and GPT-4o-\textit{2024-08-06}. We set the image detail in the API to `low' because our tasks only require high-level reasoning that does not depend on finer image details.\footnote{We use GPT-4o-mini because evaluating GPT-4o would be too expensive.} Prior works show that GPT-4o is the best-performing multilingual LVLM \cite{schneider-sitaram-2024-m5, vayani2024alm}.

% \iparagraph{Qwen2-VL} Qwen2-VL ties a 675M parameter vision-transformer (ViT) into Qwen2 LLMs \cite{wang2024qwen2vl}. An MLP compresses adjacent 2×2 visual tokens embedded by the ViT into one token representation, which is then input to the LLM. 

% \iparagraph{InternVL 2.5} Depending on the model size, InternVL uses Qwen2.5 or InternLM as its LLM backbone  \cite{chen2024internvl}. The model embeds images either by a 6B or by a distilled 300M ViT pretrained with CLIP \cite{radford2021clip}. The resulting image patch encodings are downsampled by factor 4 and fed through an MLP to the LLM.


% \iparagraph{Centurio} Centurio is the latest massively multilingual LVLM explicitly trained on 100 languages \cite{geigle2025centurio}, outperforming alternatives like Parrot \cite{sun2024parrot} or Pangea \cite{yue2024pangea}. It employs Qwen2.5 as its LLM \cite{yang2024qwen2technicalreport} and \texttt{SigLIP SO400/384} as its ViT \cite{zhai2023sigmoid}. The model mixes resolutions by stacking the encodings of the full image and those of 2×2 tiles along the features. The combined embedding is then projected via an MLP to the LLM's input space.

% Besides architectures, the LVLMs most crucially differ in dataset mixtures on which they were trained. Centurio translates image-caption, VQA, OCR, and a few multi-image datasets to 100 languages with NLLB \cite{nllbteam2022language} to mix 50:50 with the original English data. Qwen2-VL and InternVL, however, were trained on much larger, more diverse datasets that comprise sizable multi-image comparison and video understanding datasets. Moreover, assuming that the LLMs of Qwen2-VL, InternVL, and GPT-4o were pretrained on Flores, their performance would be overly optimistic.

\rparagraph{Models} We test state-of-the-art LVLMs Qwen2-VL \cite{wang2024qwen2vl}, InternVL 2.5 \cite{chen2024internvl}, Centurio-Qwen \cite{geigle2025centurio}, and GPT-4o(-mini) across available sizes.\footnote{We provide details on the LVLMs in Appendix \ref{app:subsec:exp-details}.} Smaller LVLMs are evaluated on all languages, while larger ones (26B+) are tested on subsets (cf. \S\ref{subsec:further-analyses}). For cross-modal topic matching, we also evaluate on mSigLIP-base \cite{zhai2023sigmoid}. Trained explicitly for semantic similarity on multilingual image-caption pairs, the ViT represents a strong baseline.\footnote{The model is available on Huggingface at: \href{https://huggingface.co/google/siglip-base-patch16-256-multilingual}{google/siglip-base-patch16-256-multilingual}.} Its prediction denotes the choice that has the highest average cosine similarity to the $k$ references.

\rparagraph{Image preprocessing} We downsample the images to 640×480 pixels, as the tasks rely on higher-level visual cues for topically associating images and texts rather than finer image details. This can significantly reduce the number of visual tokens input to LVLMs, enabling more efficient inference.

\rparagraph{Hyperparameters} We decode text greedily with temperature set to $0.0$ to ensure reproducibility.

\rparagraph{Metric} We compute the share of prompts for which responses begin with the right letter. If the label is "A", a response such as "A." is also correct.