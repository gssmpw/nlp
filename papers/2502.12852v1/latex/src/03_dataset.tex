\section{Dataset and Tasks}

\subsection{Dataset}
\label{sec:dataset}

For MVL-SIB, we extend the following Flores-based datasets to create a massively multilingual, multi-way parallel VL benchmark for identifying topical associations between images and sentences.

\rparagraph{Flores} Flores is a machine translation benchmark containing 3,001 sentences from English Wikipedia paragraphs \cite{nllbteam2022language}, professionally translated into over 200 languages.\footnote{Flores splits 3,001 sentences into \textsc{dev} (997), \textsc{devtest} (1,012), and \textsc{test} (992) sets. The \textsc{test} set was not released.}

\rparagraph{SIB-200} \citet{adelani-etal-2024-sib} grouped the coarse topical annotations for sentences in the \textsc{dev} and \textsc{devtest} subsets of Flores into 7 higher-level topics.\footnote{The topics are \texttt{entertainment}, \texttt{geography}, \texttt{health}, \texttt{politics}, \texttt{science/technology}, \texttt{sports}, and \texttt{travel}.} The resulting SIB-200 dataset is a benchmark for topical classification with 1,004 parallel examples for 205 language variants.

\rparagraph{MVL-SIB} For each topic, we first manually collect 10 permissively licensed images that distinctly represent the topic (e.g., \texttt{sports}) with minimal overlap or ambiguity (cf. Appendix \ref{app:images-per-topic}). We verify that all LVLMs in our study correctly classify the topics for the images when prompted (cf. Appendix \ref{app:images-to-topics}). We next create 3 different MVL-SIB instances from each of the 1,004 SIB sentences, totaling 3,012 MVL-SIB instances.
%, to increase the number of image-text combinations. 
For each MVL-SIB instance, we couple the respective SIB sentence with (1) a random selection of 5 positive images (same topic) and 4 additional sentences from the same category as the original sentence, as well as (2) 3 negative images and sentences randomly sampled from different topics compared to the starting SIB sentence. The set of sampled sentences and images by instance is maintained across languages. 

\subsection{Cross-modal \& Text-only Topic Matching}
\label{sec:tasks}

We formulate both cross-modal and text-only topic matching tasks based on MVL-SIB. In every task, we present the model with the list of topics that images and sentences may be associated with.\footnotemark[\value{footnote}] Otherwise, it would be unclear along which dimension the model should match images and sentences. The portion of the prompt that introduces the task is provided in English, while the sentences to be topically aligned with images are presented in one of the 205 languages included in MVL-SIB. LLMs reliably perform tasks described in English, even when task-related information is conveyed in other languages \cite{muennighoff2022crosslingual,romanou2024include}. This ensures a fair comparison across all 205 languages, where MT would not accurately preserve the meaning of the prompts. We detail prompts for each task in Appendix \ref{appendix:prompts}. 

\rparagraph{Cross-modal Topic Matching} 
Using our text-image samples (cf. \S\ref{sec:dataset}), we define two cross-modal topic matching tasks: Images-To-Sentence (\its) and Sentences-To-Image (\sti). In \its, the model must select, from 4 candidates, the sentence that matches the topic of $k$ reference images. Conversely, in \sti, the model chooses, from 4 options, the image that shares the topic with $k$ reference sentences. In both tasks, we present the model with $k \in \{1, 3, 5\}$ references, respectively. These tasks evaluate the modelâ€™s ability to align high-level visual and textual cues on topics.

\rparagraph{Text-only Topic Matching} 
We construct two tasks by replacing the images in \its and \sti, that represent the topics, with their corresponding labels (e.g., \texttt{sports}). The resulting unimodal tasks, Topic-To-Sentence (\tts) and Sentences-To-Topic (\stt), mirror the cross-modal tasks, \its and \sti, respectively. For \tts, we evaluate only $k{=}1$, since repeating the topic label adds no information. These baseline tasks allow us to delineate between language support and vision-language understanding in LVLMs.

\rparagraph{MVL-SIB offers 4 crucial advantages over prior benchmarks} \textbf{1)} It supports evaluation in 205 languages, covering over 100 more languages than existing benchmarks for which MT models fail to synthesize reliable evaluation data. \textbf{2)} MVL-SIB supports ablating language understanding and multimodal reasoning of LVLMs by comparatively evaluating the mirroring text-only and cross-modal topic matching tasks (cf. \S\ref{sec:tasks}). \textbf{3)} MVL-SIB enables intricate analysis of single- and multi-image VL interactions in LVLMs by allowing topics to be represented by varying numbers of images in cross-modal tasks. \textbf{4)} MVL-SIB comprises higher-level VL reasoning tasks,  pairing varied images and diverse texts to test nuanced VL understanding. 