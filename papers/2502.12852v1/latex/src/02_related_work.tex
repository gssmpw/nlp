\section{Related Work}
\label{sec:related}
%

\rparagraph{Multilingual Vision-Language Models} Researchers have extended more English-centric LVLMs like BLIP-2 or LLaVA by continuing to train on multilingual data. Google’s PaLI models were the earliest closed-weight models trained on multilingual captions and VQA data; their open-weight PaliGemma followed a comparable strategy. Meanwhile, modern LLMs (e.g., Qwen 2.5, Llama 3, Gemma 2, Aya) have improved in multilingual tasks but frequently fail to respond consistently in non-English languages, particularly in low-resource settings \cite{schneider-sitaram-2024-m5}. % Research on multilingual instruction tuning underscores its importance even for models pre-trained on multilingual data mixtures  and indicates that only a small set of languages can yield significant cross-lingual gains \cite{weber-etal-2024-investigating,geigle2025centurio}. 
However, foundational models tend to focus on higher-resource languages and do not fully account for broader linguistic contexts in vision-language tasks. mBLIP is the first open multilingual LVLM trained on image captions and a small set of translated instruct data in 98 languages \cite{geigle-etal-2024-mblip}. Pangea incorporates multicultural dimensions by blending machine-translated data, existing multilingual resources, and synthetic data, on 39 languages \cite{yue2024pangea}. Most recently, \citet{geigle2025centurio} studied the composition of training data for multilingual adaptation of LVLMs, observing that only 25-50\% of the data needs to be in English. The authors apply their findings when training `Centurio', which achieves state-of-the-art performance on 14 multilingual VL benchmarks.

\rparagraph{Multilingual Vision-Language Benchmarks}
Existing datasets span VQA, natural language inference (NLI), image captioning, outlier detection, and culturally grounded QA: 

\iparagraph{VQA} xGQA extends the questions of the GQA dataset into 8 languages (5 scripts), but the answers in English \cite{pfeiffer-etal-2022-xgqa}. MaXM offers short-form QA fully in 7 languages, pairing culturally aligned images with same-language QA pairs \cite{changpinyo-etal-2023-maxm}. MTVQA focuses on text-heavy VQA in 9 languages \cite{tang2024mtvqa}.

\iparagraph{Culturally-grounded VQA} CVQA collects culturally diverse images and queries in 31 languages with multiple country-specific variants \cite{romero2024cvqa}. The concurrent ALM-Bench covers 100 languages via MT with GPT-4o followed by human post editing with both generic and cultural multiple-choice and `true or false' questions, as well as free-form VQA \cite{vayani2024alm}.

\iparagraph{Visual Reasoning \& NLI}
XVNLI evaluates cross-lingual visual NLI on 5 languages \cite{bugliarello-etal-2022-iglue}. Binary reasoning tasks include MaRVL \cite{liu-etal-2021-visually} and M5B-VGR \cite{schneider-sitaram-2024-m5}, each using linguistically specific images and textual statements. M5B-VLOD presents an outlier detection challenge, where a statement holds true for all but one image \cite{schneider-sitaram-2024-m5}.

\iparagraph{Multiple-Choice QA}
Babel-ImageNet \cite{geigle-etal-2024-babel} translates ImageNet labels into nearly 300 languages for multiple-choice object classification.  M3Exam and xMMMU also feature multiple-choice VQA in 9 and 7 languages, respectively.

\vspace{1.2mm}
\noindent MVL‐SIB fills the gaps in existing multilingual VL benchmarks. It provides test data professionally translated to 205 languages, covering over 105 more languages than other benchmarks for which MT cannot synthesize reliable data for. Other VL datasets that eschew MT, such as culturally-grounded VQA benchmarks,  typically construct more language-specific non-parallel data, that does not support comparative evaluation across languages. The cross-modal topic matching tasks can be also framed text-only (cf. \S\ref{sec:tasks}), replacing the images that represent topics with the explicit topic labels. Thereby, MVL-SIB enables to ablate the vision-language support from the textual support for a language. Finally, the benchmark allows to vary the number of images provided LVLMs to analyze the support for multi‐image reasoning.

% The benchmarks nevertheless collectively fall short of truly inclusive massively multilingual VL evaluation. Benchmarks that comprise text in more than 10 languages often rely on MT, which can inadvertently distort the original evaluation data \cite{artetxe-etal-2020-translation,panickssery2024llmevaluatorsrecognizefavor}. Conversely, datasets that eschew MT, such as culturally-grounded VQA benchmarks,  typically construct more language-specific non-parallel evaluation data, which does not allow for comparative multilingual evaluation. MVL‐SIB addresses gaps in existing multilingual VL benchmarks by providing evaluation data in 205 languages -- over 105 more than current benchmarks, particularly for languages where machine translation falls short in generating reliable synthetic data. Unlike existing benchmarks, MVL-SIB also enables to ablate the vision-language support from the textual support by language. The cross-modal topic-matching tasks can be also framed text-only (cf. \S\ref{sec:tasks}), replacing the images that represent topics with the explicit topic labels. Finally, the benchmark allows to vary the number of images provided LVLMs to analyze the differences between single‐ and multi‐image reasoning.