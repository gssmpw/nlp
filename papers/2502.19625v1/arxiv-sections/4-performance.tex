\section{Impact of Non-Adherence Bias on Downstream Performance}
\label{sec:performance}
In this section, we evaluate the impact of treatment non-adherence bias on downstream model performance. We first define the outcome of interest and demonstrate the effect of non-adherence on the outcome in Section~\ref{sec:setup}. We then analyze its impact on treatment effect estimation for causal inference in Section~\ref{sec:causal_inference} and on ML model performance in Section~\ref{supervised_learning}.


\subsection{Treatment non-adherence leads to worse blood pressure outcomes}
\label{sec:setup}
Blood pressure reduction is the primary outcome when evaluating hypertension medications. To investigate the impact of treatment non-adherence on downstream performances, we begin by extracting pairs of visits where blood pressure measurements are available for both visits. To ensure a sufficient number of encounters for each medication, we focus on the top five most commonly prescribed medications: amlodipine, lisinopril, losartan, hydrochlorothiazide and metoprolol. We only include pairs where the duration between visits is less than six months to minimize the influence of other factors that could affect blood pressure over longer intervals. This results in 1732 pairs of encounters in total with 303 non-adherent pairs.

We begin by showcasing the impact of treatment non-adherence on blood pressure reduction between visits using t-tests. The results presented in Table~\ref{tab:treatment_adherence_outcome} indicate that non-adherence leads to smaller blood pressure reduction, with 1.96 mmHg less systolic reduction ($p=0.011$) and 3.93 mmHg less diastolic reduction ($p=0.001$) compared to adherence.

\begin{table}[h]
\centering
\footnotesize 
\caption{Results of the t-tests assessing the effect of treatment non-adherence on blood pressure reduction. Treatment non-adherence is statistically significant for both systolic and diastolic reduction, with non-adherence leading to smaller reductions in systolic and diastolic blood pressure.}
\label{tab:treatment_adherence_outcome}
\begin{tabular}{@{}l@{}c@{\hspace{5pt}}c@{\hspace{5pt}}c@{}}
\toprule
\textbf{Outcome} &  \textbf{Mean Difference} &  \textbf{95\% CI} &\textbf{$p$-value} \\
\midrule
Systolic Reduction &  -1.96 & (-3.47 to -0.46) &    \textbf{0.011} \\
Diastolic Reduction &  -3.93 & (-6.23 to -1.63) &    \textbf{0.001} \\
\bottomrule
\end{tabular}
\end{table}


\subsection{Causal inference for treatment effect estimation}
\label{sec:causal_inference}
Amlodipine and Lisinopril are the most commonly prescribed medications for hypertension, leading to numerous randomized controlled trials (RCTs) comparing their treatment effects \citep{cappuccio1993amlodipine, naidu2000evaluation}. However, due to the high cost of RCTs, various causal inference methods have been developed to estimate treatment effects from observational data \citep{pearl2009causality, austin2011introduction, shalit2017estimatingindividualtreatmenteffect, K_nzel_2019}. Among them, Inverse Probability Weighting (IPW) is one of the most widely used methods \citep{austin2011introduction}, providing an unbiased estimation of the Average Treatment Effect (ATE) by adjusting for confounding. We start by demonstrating the impact of treatment non-adherence bias on ATE estimation using IPW.
\\
\newline
\textbf{Experiment Setup.} 
We compare the ATE estimation with and without including treatment non-adherent data. Demographic and clinical factors are included as confounders and detailed in Appendix~\ref{apd:feature}. Patients prescribed lisinopril act as the control group, while those taking amlodipine are considered the treated group. The treatment effect is assessed based on the reduction in diastolic and systolic blood pressure between two visits.
\\
\newline
\textbf{Results.} 
The results are presented in Table~\ref{tab:ipw_ate}. Without filtering for non-adherent data, amlodipine lowers diastolic blood pressure by 1.75 mmHg but increases systolic blood pressure by 0.06 mm Hg compared to lisinopril. After excluding non-adherent data, amlodipine lowers diastolic blood pressure by 1.40 mmHg and also reduces systolic blood pressure by 0.11 mmHg compared to lisinopril. This result shows a reversal in the estimated treatment effect for systolic blood pressure reduction before and after excluding non-adherent data.
\begin{table}[h]
\centering
\footnotesize 
\caption{Estimated ATE of medication on blood pressure reduction using IPW. Notably, excluding non-adherent data reverses the conclusion on the treatment effect of systolic blood pressure reduction, causing the estimate to flip from -0.06 to 0.11 mmHg.}
\label{tab:ipw_ate}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Dataset} & \multicolumn{2}{c}{\textbf{Blood Pressure Reduction}} \\ 
\cmidrule(lr){2-3}
 & \textbf{Diastolic} & \textbf{Systolic} \\ 
\midrule
Full Dataset & 1.75 & -0.06 \\
Adherent Data Only & 1.40 & 0.11 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Supervised learning for treatment outcome prediction}
\label{supervised_learning}
We now demonstrate the impact of treatment non-adherence bias on predictive modeling performance. Following a common setup in the literature \citep{mroz2024predicting, yi2024development}, we use patients' EHR data with treatment prescriptions and blood pressure measurements from their first visit as covariates. The target to predict is whether the blood pressure will be normal at their second visit. Following the guidelines of the American Heart Association \citep{heartUnderstandingBlood}, we define normal blood pressure as having a systolic value of less than 120 and a diastolic value of less than 80. To evaluate model performance in predicting outcomes, we use 500 adherent samples as the test set in all subsequent experiments. We test exclusively on adherent patients since our goal is to evaluate model performance in scenarios where treatments are followed as prescribed, representing the intended clinical use case. A detailed description of the features used is provided in Appendix~\ref{apd:feature}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/treatment_non_adherence_ratio.png}
    \caption{Results of varying treatment non-adherence data percentage on model performance and fairness. Increasing the proportion of non-adherent data in the training set degrades predictive performance and increases fairness disparities between Black and non-Black patients, as measured by demographic parity and the equal odds criterion (true positive rate and false positive rate differences). Results are averaged over 100 seeds, with error bars representing the standard error of the mean.}
    \label{fig:treatment_non_adherence_ratio}
    \includegraphics[width=0.9\textwidth]{figures/remove_non_adherence_rf.png}
    \caption{Results of removing non-adherent data on model performance and fairness. The black curve represents training on 75\% of the full dataset, which only consists of adherent encounters. Removing non-adherent data improves model performance, with greater gains observed as sample size increases. It also decreases fairness disparities between Black and non-Black patients, as measured by demographic parity and the equal odds criterion (true positive rate and false positive rate differences). Results are averaged over 100 seeds, with error bars representing the standard error of the mean.}
    \label{fig:remove_non_adherence_rf}
\end{figure}


\subsubsection{Effect of varying treatment non-adherence data ratios on model performance and fairness}
\label{sec:very_ratio}
We begin by showing the harmful impact of treatment non-adherence bias by varying the proportion of non-adherent data in the training set.
\\
\newline
\textbf{Experiment Setup.}  
We fix the training set size at 300 and vary the proportion of non-adherent data in the training set across \(\{0\%, 10\%, 30\%, 50\%, 70\%, 90\%\}\) to evaluate the impact of treatment non-adherence bias on model performance. We train logistic regression and random forest models, both commonly used for modeling tabular EHR data and assess performance using AUROC on the test set.  Beyond performance, ensuring fair decision-making is also a critical consideration in healthcare. Let \( A \) denote the sensitive attribute (e.g., race), \( Y \) represent the true outcome, and \( \hat{Y} \) denote the predicted outcome. Demographic parity \citep{dwork2012fairness} difference measures the disparity in the likelihood of receiving a positive prediction between groups, i.e.,
\begin{equation*}
    |P(\hat{Y} = 1 \mid A = 1) - P(\hat{Y} = 1 \mid A = 0)|
\end{equation*}
Equal odds \citep{hardt2016equality} difference compares both true positive rates and false positive rates across groups, i.e., 
\begin{align*}
    |P(\hat{Y} = 1 \mid Y = 1, A = 1) - P(\hat{Y} = 1 \mid Y = 1, A = 0)|\\
    |P(\hat{Y} = 1 \mid Y = 0, A = 1) - P(\hat{Y} = 1 \mid Y = 0, A = 0)|
\end{align*}
We therefore measure the differences in demographic parity and equal odds across racial groups to assess fairness.  
\\
\newline
\textbf{Results.} 
We present the results in Figure~\ref{fig:treatment_non_adherence_ratio}, showing that increasing the percentage of non-adherent data in the training set degrades performance, with a 3\% drop for logistic regression and a 5\% drop for random forest in AUROC. Additionally, a higher proportion of non-adherent data increases fairness disparities between Black and non-Black patients under both the demographic parity and equal odds criteria. For instance, the false positive rate disparity of the random forest doubles, increasing from 0.125 to 0.25 as the percentage of non-adherent data increases. Similar trends are observed for other racial groups, and we provide full results in Appendix~\ref{apd:fairness}. These findings consistently highlight the harmful impact of treatment non-adherence bias.




\subsubsection{
Effect of removing non-adherent data on model performance and fairness}
\label{sec:drop_data}
We now emphasize the importance of addressing treatment non-adherence bias by showing that a simple approach to remove non-adherent data can improve predictive performance and lead to fairer predictions.
\\
\newline
\textbf{Experiment setup.}
We fix the non-adherent data ratio at 25\% and compare the performance of random forests trained on the entire dataset versus those trained only on adherent data by removing a quarter of data that are non-adherent. We report the test AUROC as well as demographic and equal odds differences while varying the full training set size in \(\{600, 800, 1000, 1200\}\) before removing non-adherent data.
\\
\newline
\textbf{Results.} 
The results are presented in Figure~\ref{fig:remove_non_adherence_rf}. While the traditional ML perspective suggests that more data generally improves performance, our findings show that using only the adherent 75\% of the data leads to better model performance, with the improvement becoming more significant as the sample size increases. For instance, with a training size of 1,200, the model achieves an AUROC of 0.695 when using all data, whereas dropping non-adherent data improves AUROC to 0.71. Additionally, we find removing non-adherent data reduces racial disparities between Black and non-Black patients, as both demographic parity and equal odds differences are consistently smaller across sample sizes. Similar trends are observed for other racial groups, and we provide full results in Appendix~\ref{apd:fairness}. These findings further highlight the importance of addressing treatment non-adherence bias to achieve better and fairer model performance.
