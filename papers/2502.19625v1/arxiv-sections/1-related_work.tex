\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/Figure1.pdf}
\caption{Illustration of cohort selection, LLM non-adherence extraction, and non-adherence analysis. (a) We select 3,623 hypertension patients and pair their visits, with hypertension medication prescribed at the first visit and clinical notes extracted from the second. (b) These notes are then processed by an LLM to identify treatment non-adherence, with outputs validated through clinician annotations. (c) We further perform topic modeling to uncover reasons for non-adherence and assess the harmful impact of ignoring this bias on predictive modeling performance and treatment effect estimation.}
    \label{fig:diagram}
\end{figure}


\section{Related Work}
\label{sec:related_work}
\subsection{Treatment adherence analysis in hypertension}
Multiple studies have investigated treatment adherence among patients with hypertension \citep{boratas2018evaluation, uchmanowicz2018factors, algabbani2020treatment, najjuma2020adherence, schober2021high}. These studies are mainly cross-sectional, with a cohort of admitted patients collected at a fixed time point, and treatment adherence is typically measured through questionnaires and interviews. For instance, \citet{algabbani2020treatment} conducted a study in Saudi Arabia involving 306 hypertensive outpatients, finding that only 42.2\% of participants adhered to their antihypertensive medications. \citet{boratas2018evaluation} conducted a similar study of 147 hypertensive patients, identifying factors such as age and duration of hypertension to be significant. However, due to their reliance on questionnaire and interview data, they often have small sample sizes (e.g., less than 300 patients) and self-reporting bias \citep{adams1999evidence, stirratt2015self}, which limits their representativeness and can even lead to contradictory conclusions. In contrast, our work conducts the first large-scale analysis utilizing EHR, with a significantly larger sample size of 3,623 patients.







\subsection{Machine learning and treatment adherence}
Machine learning has been used to identify individual risk factors associated with treatment non-adherence \citep{koesmahargyo2020accuracy, gichuhi2023machine, burgess2023using}. \citet{gichuhi2023machine} developed ML algorithms and found SVM achieved 91.28\% accuracy in predicting tuberculosis treatment non-adherence. Instead of predicting treatment adherence, our work focuses on analyzing the impact of treatment non-adherence bias on downstream model performance. Other studies have applied natural language processing (NLP) to analyze surveys to better understand treatment non-adherence \citep{anglin2021natural, lin2022extraction, chan2024patient}. \citet{chan2024patient} applied NLP to free-text responses from questionnaires completed by type 2 diabetes patients, identifying key reasons for non-adherence. Unlike questionnaires, our work leverages treatment adherence information extracted from clinical notes using LLMs. Lastly, \citet{zhong2022use} applied ML while accounting for adherence information when analyzing treatment effects in a randomized controlled trial. To our knowledge, our study is the first to leverage LLMs for extracting treatment adherence information from clinical notes and evaluating its impact on downstream causal inference and predictive model performance.





