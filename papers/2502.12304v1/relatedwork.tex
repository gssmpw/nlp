\section{Related work}
Guiding generative models with intermediate steps has been widely explored to enhance coherence, interpretability, and task performance. Existing approaches can be categorized into explicit human-readable intermediate steps, and structured weakly supervised intermediate steps.

\paragraph{Human readable intermediate steps}
Plan-and-Write \cite{Yao_Peng_Weischedel_Knight_Zhao_Yan_2019} introduces storyline-based planning, where a model first generates a structured sequence of key events before expanding them into a full story, improving coherence and creativity. %In summarization, 
\citeauthor{amplayo2020unsupervisedopinionsummarizationcontent} \citeyearpar{amplayo2020unsupervisedopinionsummarizationcontent} employ content planning, explicitly modelling aspect and sentiment distributions to guide summary generation, thereby enhancing readability and informativeness.

Similarly, \citeauthor{wolfson-etal-2022-weakly} \citeyearpar{wolfson-etal-2022-weakly} propose Question Decomposition Meaning Representations (QDMR), which break down complex questions into sequences of reasoning steps. These decompositions serve as an explicit intermediate representation, improving interpretability and guiding Text-to-SQL parsing by systematically mapping natural language queries to SQL.
\citeauthor{baziotis-etal-2019-seq} \citeyearpar{baziotis-etal-2019-seq}  introduce a sequence-to-sequence-to-sequence model (SEQÂ³), where the intermediate step is a compressed version of the input sentence, explicitly represented in natural language. 


% Other works rely on latent representations, where intermediate steps remain internal to the model rather than explicitly interpretable. \citeauthor{herzig2021unlockingcompositionalgeneralizationpretrained} \citeyearpar{herzig2021unlockingcompositionalgeneralizationpretrained} introduce reversible latent structures to improve compositional generalization, but these representations do not correspond to explicit human-understandable reasoning steps. 




\paragraph{Structured intermediate steps}
Some models introduce structured but weakly supervised intermediate steps, where the intermediate representations are partially interpretable but not explicitly labelled during training. \citeauthor{cheng-etal-2017-learning} \citeyearpar{cheng-etal-2017-learning} generate predicate-argument structures, which serve as an intermediate step in semantic parsing. Unlike explicit intermediate representations, these structures are learned through optimization-based search rather than direct supervision.
Similarly, \citeauthor{jambor-bahdanau-2022-lagr} \citeyearpar{jambor-bahdanau-2022-lagr} propose Label Aligned Graphs (LAGr), where models predict node and edge labels to construct structured meaning representations aligned with input text, improving systematic generalization in semantic parsing. These representations enhance compositional generalization but still depend on predefined structural mappings.
\citeauthor{herzig2021unlockingcompositionalgeneralizationpretrained} \citeyearpar{herzig2021unlockingcompositionalgeneralizationpretrained} introduce intermediate representations that transform meaning representations (e.g., SPARQL or SQL queries) into structured forms that improve compositional generalization while maintaining reversibility. 
While these methods balance interpretability and generalization, they still rely on task-specific constraints rather than fully flexible intermediate representations.

\paragraph{Reinforcement learning in NLP}
RL has also been applied in NLP to optimize model generation beyond traditional supervised learning for text summarization~\citep{paulus2018a}, dialogue generation \cite{li-etal-2016-deep} and machine translation \cite{wu-etal-2018-study}. More recently, Reinforcement Learning from Human Feedback (RLHF) \cite{NIPS2017_d5e2c0ad} has been instrumental in aligning large-scale language models with human preferences, demonstrating the effectiveness of RL-based fine-tuning. While RL provides a strong optimization framework, it does not inherently generate structured intermediate representations, instead refining model behaviour through reward-based learning.


% While these methods show that intermediate steps improve performance, they either depend on predefined structured constraints or lack interpretability, making them less adaptable across tasks.

% More recently, researchers have explored self-supervised approaches to autonomously learn intermediate steps without human annotations. \citeauthor{chen2024selfplayfinetuningconvertsweak} \citeyearpar{chen2024selfplayfinetuningconvertsweak} propose self-reinforcement learning, where models iteratively generate and refine synthetic intermediate data to improve performance. However, such approaches still depend on task-specific setups or external reward signals, constraining their generalizability.

%In contrast, our 
In this paper, we learn intermediate steps freely, without predefined formats, constraints, search procedures, external supervision, annotated datasets or task-specific designs.
Inspired by RL principles, our method integrates intermediate step/initial state generation and final output optimization into a unified framework. 
Our approach generalizes across tasks such as translation, summarization, and multi-choice question answering logical reasoning, and architectures, providing a flexible, scalable, and theoretically grounded solution for improving the quality of generation.


% Guiding generative models with intermediate steps, such as keywords, outlines, or reasoning chains, has been extensively studied across various domains to improve task performance, coherence, and interpretability. Early work, such as \cite{keskar2019ctrlconditionaltransformerlanguage}, introduced controllable text generation by leveraging control codes to guide the generation process. These approaches demonstrated the utility of external guidance in producing coherent and contextually relevant outputs. Similarly, Plan-and-Write \cite{Yao_Peng_Weischedel_Knight_Zhao_Yan_2019} emphasized hierarchical planning in story generation by generating a sequence of keywords as an intermediate step before crafting the full narrative, enhancing both creativity and coherence.

% The use of intermediate steps extends beyond story generation. In summarization, \citeauthor{paulus2018a} (\citeyear{paulus2018a}) proposed RL-based methods to optimize abstractive summaries by generating intermediate representations. These efforts showcased how intermediate structures could bridge input and output for more accurate and interpretable outputs. Similarly, \citeauthor{amplayo2020unsupervisedopinionsummarizationcontent} employed content planning for unsupervised opinion summarization, organizing key elements into structured summaries that maintain coherence and relevance.

% In reasoning tasks, the advent of COT prompting  ~\cite{wei2023chainofthoughtpromptingelicitsreasoning} highlighted the importance of intermediate logical reasoning steps. While COT focuses on prompt-based reasoning, fine-tuning approaches such as \citeauthor{creswell2022faithfulreasoningusinglarge} (\citeyear{creswell2022faithfulreasoningusinglarge}) demonstrated how incorporating intermediate steps during training improves model interpretability and task performance. These efforts underscore the general utility of intermediate steps but often rely on predefined formats and annotated datasets, which limit scalability.

% More recent works explore the autonomous generation of intermediate steps. For instance, self-reinforcement methods, such as \citeauthor{chen2024selfplayfinetuningconvertsweak} (\citeyear{chen2024selfplayfinetuningconvertsweak}), allow models to iteratively generate and refine synthetic data or guiding contexts without requiring human intervention. These approaches align with RL principles, emphasizing iterative optimization to enhance performance. However, these methods still often depend on specific task setups or external rewards, constraining their generalizability.

% Several studies have explored intermediate representations in sequence-to-sequence learning, often relying on structured latent variables. \citeauthor{cheng-etal-2017-learning} (2017) used an optimization-based search process, where the model generates a latent predicate-argument structure and iterates over possible representations to maximize final output accuracy. \citeauthor{herzig2021unlockingcompositionalgeneralizationpretrained} (2021) enforced reversible intermediate representations in pre-trained models, ensuring compositional generalization but requiring explicitly defined mappings. \citeauthor{baziotis-etal-2019-seq} (2019) proposed a sequence-to-sequence-to-sequence autoencoder, where an intermediate compressed representation is learned for sentence compression using an explicit autoencoding objective.

% In contrast, our method learns intermediate steps freely, without predefined constraints, search procedures, or external supervision, making it more flexible and adaptable across tasks like summarization, translation, and reasoning.

% In contrast, we introduce a task-agnostic framework that enables models to generate and optimize intermediate steps, or the initial states without relying on predefined formats or external supervision. Inspired by RL principles, our method integrates intermediate step/initial state generation and final output optimization into a unified framework. Unlike prior work that depends on annotated datasets or task-specific designs, our approach generalizes across tasks such as translation, summarization, and multi-choice question answering for both reading comprehension and logical reasoning, and architectures, providing a flexible, scalable, and theoretically grounded solution for improving the quality of generation.
% both training and inference.