% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{array}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{multirow}
\usepackage{graphicx}
% Standard package includes
\usepackage{times}
\usepackage{tabularx}
\usepackage{latexsym}
\usepackage{xcolor}
\definecolor{softred}{rgb}{0.9, 0.4, 0.4}
\definecolor{softblue}{rgb}{0.4, 0.4, 0.9}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\newcommand{\jiayi}[1]{{\color{red}[Jiayi: {#1}]}}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Warmup Generations: A Task-Agnostic Approach for Guiding Sequence-to-Sequence Learning with
Unsupervised Initial State Generation }

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
    Senyu Li$^{1,2}$\qquad 
    Zipeng Sun$^{1,2}$\qquad 
    Jiayi Wang$^3$\qquad \\
    \textbf{Xue Liu}$^{1,2}$\qquad 
    \textbf{Pontus Stenetorp}$^3$\qquad 
    \textbf{Siva Reddy}$^{1,2,4}$\qquad 
    \textbf{David Ifeoluwa Adelani}$^{1,2,4}$\\
    $^1$Mila - Quebec AI Institute, $^2$McGill University, $^3$University College London,\\  $^4$Canada CIFAR AI Chair \\
    \texttt{\{senyu.li,  siva.reddy, david.adelani\}@mila.quebec }\\
    \texttt{\{zipeng.sun, xueliu\}@mail.mcgill.ca}\\
    \texttt{\{jiaywang, p.stenetorp\}@cs.ucl.ac.uk}\\
    % \texttt{david.adelani@mcgill.ca}
    }


% \author{Senyu Li \\
%   McGill University \\
%   Mila - Quebec AI Institute \\
%   \texttt{senyu.li@mila.quebec} \\
%   \And
%   Zipeng Sun \\
%   McGill University \\
%   \texttt{zipeng.sun@mail.mcgill.ca} \\}


%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
% \linenumbers

\maketitle
\begin{abstract}
% Traditional training strategies for natural language generation tasks typically train models to generate target text directly. In fields like story generation, research has shown that providing models with additional keywords can enhance both training and inference.
% With the recent rise of Large Language Models, techniques like in-context learning highlight the value of incorporating supplementary information to improve the quality of generated text.
% However, for real-world applications, additional information like informative context or keywords is not always available. Moreover, researchers don’t always have access to datasets that provide this helpful information to support the model's learning process. In this work, we propose a two-stage training approach for natural language generation tasks, focusing on enabling models to automatically generate beneficial context without the need for supervised data for keyword or context generation of the target task. The model learns to generate a broad, unrestricted format of guiding information on its own. Leveraging these self-generated contexts to guide the model, 
% we can improve and stabilize
% both the training and 
% the inference processes, resulting in higher-quality outputs.
Traditional supervised fine-tuning (SFT) strategies for sequence-to-sequence tasks often train models to directly generate the target output.
Recent work has shown that guiding models with intermediate steps—such as keywords, outlines, or reasoning chains—can significantly improve performance, coherence, and interpretability. However, these methods often depend on predefined intermediate formats and annotated data, limiting their scalability and generalizability. In this work, we introduce a task-agnostic framework that enables models to generate intermediate ``warmup'' sequences. These warmup sequences, serving as an initial state for subsequent generation, are optimized to enhance the probability of generating the target sequence without relying on external supervision or human-designed structures. Drawing inspiration from reinforcement learning principles, our method iteratively refines these intermediate steps to maximize their contribution to the final output, similar to reward-driven optimization in reinforcement learning with human feedback. Experimental results across tasks such as translation, summarization, and multi-choice question answering for logical reasoning show that our approach outperforms traditional SFT methods, and offers a scalable and flexible solution for sequence-to-sequence tasks\footnote{We will release our code after the paper is published.}.
% This provides a flexible and efficient alternative to traditional supervised techniques that rely on annotated intermediate data or task-specific designs.



\end{abstract}

\section{Introduction}

% Natural language generation (NLG) \jiayi{why NLG?} has seen significant advancements in recent years, primarily driven by the rise of large-scale pre-trained language models, or Large Language Models (LLM), such as the  T5 \cite{DBLP:journals/corr/abs-1910-10683}, GPT \cite{brown2020languagemodelsfewshotlearners}, and Llama series \cite{touvron2023llamaopenefficientfoundation}. These models, built based on the transformer architecture \cite{vaswani2017attention}, have evolved the field through their powerful text-generation capabilities across diverse tasks, including machine translation, summarization, and dialogue generation. One of the key breakthroughs in this domain is the ability of these models to generate coherent and contextually relevant responses, largely due to their ability to model dependencies across long sequences of text.
Recent advancements in large-scale pre-trained language models (LLMs), such as T5 \cite{DBLP:journals/corr/abs-1910-10683}, GPT 
 \cite{brown2020languagemodelsfewshotlearners}, and LLaMA \cite{touvron2023llamaopenefficientfoundation}, have significantly improved performance on both predictive tasks (e.g., multi-choice question answering) and generative tasks (e.g., machine translation and summarization). These models have demonstrated exceptional capabilities in generating coherent and contextually relevant outputs by modelling dependencies across long sequences of data. Despite their successes, traditional 
 % training methods 
supervised fine-tuning (SFT) methods
 for such models
 % \jiayi{what are training methods? Pretrain or SFT? Not clear. I think here it should be SFT} 
 % added
 often focus on directly generating the target output without leveraging the benefits of intermediate steps or initial guidance \cite{sutskever2014sequencesequencelearningneural}.
 % \jiayi{need a reference}.
% added
% However, traditional approaches to training NLG models often focus on directly generating the target text without leveraging the power of additional guiding information. 
% Recent research in fields such as story generation has shown that offering models supplementary information, such as keywords or intermediate prompts, can enhance both the quality and coherence of the generated text. 
% In-context learning (ICL) \cite{NEURIPS2020_1457c0d6} has also shown great success in various tasks, as shown by the use of examples during inference. 
% This demonstrates the potential of using contextual cues to guide text generation, making the outputs more relevant and structured. 
% Unfortunately, in many real-world applications, such informative context (e.g., keywords or external cues) is not always readily available, and datasets often lack the labelled information needed to explicitly train models with these guiding elements.

Research has shown that guiding models with intermediate steps, such as outlines, keywords, or reasoning chains, can significantly improve performance, coherence, and interpretability across tasks \cite{wang2022rationaleaugmentedensembleslanguagemodels, creswell2022faithfulreasoningusinglarge}.
For instance, hierarchical frameworks for tasks such as story generation \cite{fan-etal-2019-strategies} and summarization \cite{amplayo2020unsupervisedopinionsummarizationcontent} often first generate high-level structures, such as outlines or reasoning steps, before producing detailed outputs. 
These approaches highlight the utility of intermediate guidance in organizing complex tasks. Similarly, chain-of-thought (COT) \cite{wei2023chainofthoughtpromptingelicitsreasoning} reasoning for predictive tasks extends this concept by demonstrating the value of logical steps by decomposing complex predictive tasks into explicit logical steps, demonstrating how structured reasoning between inputs and outputs can improve model performance.
\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth, height=0.11\textheight]{intermediate_tokens.drawio.png}
    \vspace{-7mm}
    \caption{\textbf{High-level workflow of Warmup Generations. } The input is first used to generate an intermediate ``warmup'' sequence, which acts as a guiding context to improve the generation of the final target output for sequence-to-sequence tasks.}
    \label{fig:intuitve}
\end{figure}
% \jiayi{replace "that bridges input and target outputs, leading to improved downstream outcomes" with "by decomposing complex predictive tasks into explicit logical steps, demonstrating how structured reasoning between inputs and outputs can enhance model performance."}. 
% replaced
% However, these approaches highly depend on predefined intermediate formats and annotated data, which are human annotation costly to create and often task-specific, limiting their scalability and adaptability to broader applications.
However, these approaches heavily rely on predefined intermediate formats and annotated data, which are costly to create due to human annotation efforts and often task-specific, thus limiting their scalability and adaptability to broader applications.

In this work, we address these limitations by introducing a framework that enables models to generate an initial state, which we refer to as ``warmup sequence.'' These warmup sequences act as preparatory steps, priming the model for the main generation task. Drawing inspiration from reinforcement learning (RL) principles, our method treats these steps as actions within a reward-driven framework, optimizing them to maximize their utility in improving the quality and coherence of the final target output. Importantly, this process operates without relying on predefined formats or external annotations, making it adaptable to a wide range of tasks and model architectures.
% This framework eliminates reliance on annotated intermediate data, generalizes across tasks, and unifies optimization of intermediate and final outputs for improved stability and performance during training and inference.
This approach eliminates dependence on annotated data for intermediate steps, achieves generalization across tasks, and unifies the optimization of intermediate and final outputs, leading to improved 
% stability
% \jiayi{robustness? what do you mean by stability?} 
% edited
final performance of the models.
% during both training and inference.

Through experiments, we demonstrate that our method improves output quality across translation, summarization,
and multi-choice question answering for logical reasoning, and is compatible with various model architectures, including encoder-decoder models like T5 \cite{2020t5} and mT5 \cite{xue2021mt5massivelymultilingualpretrained}, as well as decoder-only models like Llama \cite{touvron2023llamaopenefficientfoundation}. In addition, our method is simple to implement, requiring only about 10 additional lines of codes, without modifications
% \jiayi{not clear. what are complex modifications? I would say without modifications} 
% edited
to existing model architectures or reliance on task-specific annotations, and is grounded in a solid theoretical framework. These contributions establish an approach where models can autonomously discover and leverage a helpful
% \jiayi{how helpful?} 
initial state that increases the probability of the target sequence 
% \jiayi{why did you mention "intermediate steps" in the last paragraph (the first sentence), but say "initial step" here?}
% edited
across diverse tasks to enhance the quality of the final generation.
% \jiayi{"to .....". Here we need a sentence to emphasize the altimate objective of this new framework.}
% added

\begin{figure*}
    \centering
    \includegraphics[width=1\textwidth, height=0.12\textheight]{warmupgeneration.drawio_new.png}
    \caption{\textbf{Comparison of the traditional supervised fine-tuning methods (left) and our proposed method (right)}. The traditional method directly optimizes the mapping from input to output using annotated data, while our method dynamically generates and optimizes warmup sequences to guide the final output.}
    \label{fig:workflow}
\end{figure*}

\section{Related work}
Guiding generative models with intermediate steps has been widely explored to enhance coherence, interpretability, and task performance. Existing approaches can be categorized into explicit human-readable intermediate steps, and structured weakly supervised intermediate steps.

\paragraph{Human readable intermediate steps}
Plan-and-Write \cite{Yao_Peng_Weischedel_Knight_Zhao_Yan_2019} introduces storyline-based planning, where a model first generates a structured sequence of key events before expanding them into a full story, improving coherence and creativity. %In summarization, 
\citeauthor{amplayo2020unsupervisedopinionsummarizationcontent} \citeyearpar{amplayo2020unsupervisedopinionsummarizationcontent} employ content planning, explicitly modelling aspect and sentiment distributions to guide summary generation, thereby enhancing readability and informativeness.

Similarly, \citeauthor{wolfson-etal-2022-weakly} \citeyearpar{wolfson-etal-2022-weakly} propose Question Decomposition Meaning Representations (QDMR), which break down complex questions into sequences of reasoning steps. These decompositions serve as an explicit intermediate representation, improving interpretability and guiding Text-to-SQL parsing by systematically mapping natural language queries to SQL.
\citeauthor{baziotis-etal-2019-seq} \citeyearpar{baziotis-etal-2019-seq}  introduce a sequence-to-sequence-to-sequence model (SEQ³), where the intermediate step is a compressed version of the input sentence, explicitly represented in natural language. 


% Other works rely on latent representations, where intermediate steps remain internal to the model rather than explicitly interpretable. \citeauthor{herzig2021unlockingcompositionalgeneralizationpretrained} \citeyearpar{herzig2021unlockingcompositionalgeneralizationpretrained} introduce reversible latent structures to improve compositional generalization, but these representations do not correspond to explicit human-understandable reasoning steps. 




\paragraph{Structured intermediate steps}
Some models introduce structured but weakly supervised intermediate steps, where the intermediate representations are partially interpretable but not explicitly labelled during training. \citeauthor{cheng-etal-2017-learning} \citeyearpar{cheng-etal-2017-learning} generate predicate-argument structures, which serve as an intermediate step in semantic parsing. Unlike explicit intermediate representations, these structures are learned through optimization-based search rather than direct supervision.
Similarly, \citeauthor{jambor-bahdanau-2022-lagr} \citeyearpar{jambor-bahdanau-2022-lagr} propose Label Aligned Graphs (LAGr), where models predict node and edge labels to construct structured meaning representations aligned with input text, improving systematic generalization in semantic parsing. These representations enhance compositional generalization but still depend on predefined structural mappings.
\citeauthor{herzig2021unlockingcompositionalgeneralizationpretrained} \citeyearpar{herzig2021unlockingcompositionalgeneralizationpretrained} introduce intermediate representations that transform meaning representations (e.g., SPARQL or SQL queries) into structured forms that improve compositional generalization while maintaining reversibility. 
While these methods balance interpretability and generalization, they still rely on task-specific constraints rather than fully flexible intermediate representations.

\paragraph{Reinforcement learning in NLP}
RL has also been applied in NLP to optimize model generation beyond traditional supervised learning for text summarization~\citep{paulus2018a}, dialogue generation \cite{li-etal-2016-deep} and machine translation \cite{wu-etal-2018-study}. More recently, Reinforcement Learning from Human Feedback (RLHF) \cite{NIPS2017_d5e2c0ad} has been instrumental in aligning large-scale language models with human preferences, demonstrating the effectiveness of RL-based fine-tuning. While RL provides a strong optimization framework, it does not inherently generate structured intermediate representations, instead refining model behaviour through reward-based learning.


% While these methods show that intermediate steps improve performance, they either depend on predefined structured constraints or lack interpretability, making them less adaptable across tasks.

% More recently, researchers have explored self-supervised approaches to autonomously learn intermediate steps without human annotations. \citeauthor{chen2024selfplayfinetuningconvertsweak} \citeyearpar{chen2024selfplayfinetuningconvertsweak} propose self-reinforcement learning, where models iteratively generate and refine synthetic intermediate data to improve performance. However, such approaches still depend on task-specific setups or external reward signals, constraining their generalizability.

%In contrast, our 
In this paper, we learn intermediate steps freely, without predefined formats, constraints, search procedures, external supervision, annotated datasets or task-specific designs.
Inspired by RL principles, our method integrates intermediate step/initial state generation and final output optimization into a unified framework. 
Our approach generalizes across tasks such as translation, summarization, and multi-choice question answering logical reasoning, and architectures, providing a flexible, scalable, and theoretically grounded solution for improving the quality of generation.


% Guiding generative models with intermediate steps, such as keywords, outlines, or reasoning chains, has been extensively studied across various domains to improve task performance, coherence, and interpretability. Early work, such as \cite{keskar2019ctrlconditionaltransformerlanguage}, introduced controllable text generation by leveraging control codes to guide the generation process. These approaches demonstrated the utility of external guidance in producing coherent and contextually relevant outputs. Similarly, Plan-and-Write \cite{Yao_Peng_Weischedel_Knight_Zhao_Yan_2019} emphasized hierarchical planning in story generation by generating a sequence of keywords as an intermediate step before crafting the full narrative, enhancing both creativity and coherence.

% The use of intermediate steps extends beyond story generation. In summarization, \citeauthor{paulus2018a} (\citeyear{paulus2018a}) proposed RL-based methods to optimize abstractive summaries by generating intermediate representations. These efforts showcased how intermediate structures could bridge input and output for more accurate and interpretable outputs. Similarly, \citeauthor{amplayo2020unsupervisedopinionsummarizationcontent} employed content planning for unsupervised opinion summarization, organizing key elements into structured summaries that maintain coherence and relevance.

% In reasoning tasks, the advent of COT prompting  ~\cite{wei2023chainofthoughtpromptingelicitsreasoning} highlighted the importance of intermediate logical reasoning steps. While COT focuses on prompt-based reasoning, fine-tuning approaches such as \citeauthor{creswell2022faithfulreasoningusinglarge} (\citeyear{creswell2022faithfulreasoningusinglarge}) demonstrated how incorporating intermediate steps during training improves model interpretability and task performance. These efforts underscore the general utility of intermediate steps but often rely on predefined formats and annotated datasets, which limit scalability.

% More recent works explore the autonomous generation of intermediate steps. For instance, self-reinforcement methods, such as \citeauthor{chen2024selfplayfinetuningconvertsweak} (\citeyear{chen2024selfplayfinetuningconvertsweak}), allow models to iteratively generate and refine synthetic data or guiding contexts without requiring human intervention. These approaches align with RL principles, emphasizing iterative optimization to enhance performance. However, these methods still often depend on specific task setups or external rewards, constraining their generalizability.

% Several studies have explored intermediate representations in sequence-to-sequence learning, often relying on structured latent variables. \citeauthor{cheng-etal-2017-learning} (2017) used an optimization-based search process, where the model generates a latent predicate-argument structure and iterates over possible representations to maximize final output accuracy. \citeauthor{herzig2021unlockingcompositionalgeneralizationpretrained} (2021) enforced reversible intermediate representations in pre-trained models, ensuring compositional generalization but requiring explicitly defined mappings. \citeauthor{baziotis-etal-2019-seq} (2019) proposed a sequence-to-sequence-to-sequence autoencoder, where an intermediate compressed representation is learned for sentence compression using an explicit autoencoding objective.

% In contrast, our method learns intermediate steps freely, without predefined constraints, search procedures, or external supervision, making it more flexible and adaptable across tasks like summarization, translation, and reasoning.

% In contrast, we introduce a task-agnostic framework that enables models to generate and optimize intermediate steps, or the initial states without relying on predefined formats or external supervision. Inspired by RL principles, our method integrates intermediate step/initial state generation and final output optimization into a unified framework. Unlike prior work that depends on annotated datasets or task-specific designs, our approach generalizes across tasks such as translation, summarization, and multi-choice question answering for both reading comprehension and logical reasoning, and architectures, providing a flexible, scalable, and theoretically grounded solution for improving the quality of generation.
% both training and inference.

\section{Formulation and Derivation}
We reformulate the process of text generation by assuming that given a specific \textbf{input} \(x\) and the \textbf{target text} \(y_{\text{target}}\), there exists an intermediate sequence, or the \textbf{initial state} \(c_{\text{init}}\) preceding \(y_{\text{target}}\), where \(\text{length}(c_{\text{init}}) \geq 0\). To be more specific, \( c_{\text{init}} = \{c_1, c_2, \ldots, c_k\} \) is a sequence of tokens, where \( k \geq 0 \). The intermediate sequence \( c_{\text{init}} \) serves as a latent variable that conditions the generation of \( y_{\text{target}} \). When \( k = 0 \), \( c_{\text{init}} \) is an empty sequence, reducing the framework to the traditional sequence-to-sequence paradigm:
\vspace{-2mm}
\[
P(y_{\text{target}}|x) = \sum_{c_{\text{init}}} P(c_{\text{init}}, y_{\text{target}}|x)
\]

\vspace{-2mm}
Using the chain rule, this can be decomposed as:
\vspace{-2mm}
\[
P(y_{\text{target}}|x) = \sum_{c_{\text{init}}} P(y_{\text{target}}|c_{\text{init}}, x)P(c_{\text{init}}|x)
\]
\vspace{-2mm}
Which can be rewritten in the form:
\vspace{-3mm}

\[
P(y_{\text{target}}|x) = \mathbb{E}_{c_{\text{init}} \sim P(c_{\text{init}}|x)}[P(y_{\text{target}}|c_{\text{init}}, x)]
\]

Our objective is to maximize the probability of the target sequence \(y_{\text{target}}\) given the input \(x\). Traditionally, the loss for maximizing \(P(y_{\text{target}}|x)\) is:

\vspace{-5mm}
\[
L_{y_{\text{target}}} = -\log(P(y_{\text{target}}|x))
\]

Which is equivalent to:

\[
L_{y_{\text{target}}} =  -\log(\mathbb{E}_{c_{\text{init}} \sim P(c_{\text{init}}|x)}[P(y_{\text{target}}|c_{\text{init}}, x)])
\]
where \(c_{\text{init}}\) represents any possible initial state conditioning \(y_{\text{target}}\). This expectation implies maximizing \(P(y_{\text{target}})\) across all initial states, weighted by their probability \(P(c_{\text{init}}|x)\).

\section*{Reward-Based Initial State Optimization}

Since we lack labels for \(c_{\text{init}}\), we train the model to generate \(c_{\text{init}}\) using reward-based optimization. A good initial state \(c_{\text{init}}\) increases the probability of \(y_{\text{target}}\), while a poor \(c_{\text{init}}\) reduces it. The reward \( R(c_{\text{init}}) \) quantifies the quality of \( c_{\text{init}} \) in terms of its contribution to generating \( y_{\text{target}} \) given \( x \):
\vspace{-2mm}
\[
R(c_{\text{init}}) = P(y_{\text{target}}|c_{\text{init}}, x)
\]

\vspace{-2mm}
Under a RL framework, we aim to maximize:

\vspace{-2mm}
\[
\mathbb{E}_{c_{\text{init}} \sim P(c_{\text{init}}|x)}[R(c_{\text{init}})]
\]
%\vspace{-2mm}
which is equivalent to:

\vspace{-2mm}
\[
\mathbb{E}_{c_{\text{init}} \sim P(c_{\text{init}}|x)}[P(y_{\text{target}}|c_{\text{init}}, x)]
\]

The loss for training \(c_{\text{init}}\) generation is defined as the negative log of the expected reward:

\vspace{-3mm}
\begin{align*}
L_{c_{\text{init}}} &= -\log\big(\mathbb{E}_{c_{\text{init}} \sim P(c_{\text{init}}|x)}[R(c_{\text{init}})]\big) \\
&= -\log\big(\mathbb{E}_{c_{\text{init}} \sim P(c_{\text{init}}|x)}[P(y_{\text{target}}|c_{\text{init}}, x)]\big)
\end{align*}

% The use of a negative log maps this loss to the same logarithmic scale as $L_{y_{\text{target}}}$, and a negative log works better for sparse reward scenarios (we only have a single reward for each sequence generated) since it provides a smoother gradient signal than directly using negative expected reward as the loss function, which can help during optimization.


\begin{algorithm}

\caption{Warmup Generations}
\label{algo:1}
\KwIn{Training Data,  Maximum Epochs $E$, Number of Samples $n$, Model Parameters $\theta$}
\KwOut{Trained $\theta$}

\SetKwBlock{Initialize}{Initialize:}{}
\Initialize{
    Model $\theta$ with pretrained weights\;
}

\For{epoch $t = 1$ to $E$}{
    \For{each input $x$ in Training Data}{
        
        Initialize total loss $\mathcal{L}_{\text{total}} \leftarrow 0$\;
        \For{$i = 1$ to $n$}{
           
            Sample $c_{\text{init}}^{(i)} \sim P(c_{\text{init}}|x; \theta)$ \;
            
            
             
            
            
            Compute loss $\mathcal{L}^{(i)}$ for $y_{\text{target}}^{(i)}$\ conditioned on $c_{\text{init}}^{(i)}$ and $x$\;
            $\mathcal{L}_{\text{total}} \leftarrow \mathcal{L}_{\text{total}} + \mathcal{L}^{(i)}$\;
        }
       
        $\mathcal{L}_{\text{avg}} \leftarrow \mathcal{L}_{\text{total}} / n$\;
        
       
        Update $\theta$ using gradient descent\;
    }
    

 
}
\Return{Trained $\theta$}

\end{algorithm}


As we can observe, this formulation aligns the optimization of \(y_{\text{target}}\) and \(c_{\text{init}}\) under the same loss function.
\vspace{-2mm}
\[
L_{c_{\text{init}}} = L_{y_{\text{target}}}
\]


Directly minimizing $ L_{c_{\text{init}}}$ or $L_{y_{\text{target}}}$ is computationally infeasible due to numerical underflow for long sequences. Instead, we minimize the expected cross-entropy loss based on Jensen's inequality:

% \[
% \mathbb{E}_{c_{\text{init}} \sim P(c_{\text{init}}|x)}[-\log(P(y_{\text{target}}|c_{\text{init}}, x))]
% \]

% % \section*{Using Jensen's Inequality}

% Since \(-\log(\cdot)\) is a convex function, Jensen's inequality implies:

\vspace{-5mm}

\begin{align*}
-\log\big(&\mathbb{E}_{c_{\text{init}} \sim P(c_{\text{init}}|x)}[P(y_{\text{target}}|c_{\text{init}}, x)]\big) \\
&\leq \mathbb{E}_{c_{\text{init}} \sim P(c_{\text{init}}|x)}[-\log(P(y_{\text{target}}|c_{\text{init}}, x))]
\end{align*}

Minimizing the expected cross-entropy loss indirectly minimizes an upper bound on \(-\log(P(y_{\text{target}}|x))\), bringing us closer to maximizing \(P(y_{\text{target}}|x)\).

% \section*{Implementation via Monte Carlo Sampling}

To approximate the expected value, we use Monte Carlo sampling with \(n\) samples of \(c_{\text{init}}\):

\begin{align*}
L_{\text{final}} &= \mathbb{E}_{c_{\text{init}} \sim P(c_{\text{init}}|x)}[-\log(P(y_{\text{target}}|c_{\text{init}}, x))] \\
&\approx \frac{1}{n} \sum_{i=1}^{n} -\log(P(y_{\text{target}}|c_{\text{init}, i}, x))
\end{align*}


% Minimizing the expected cross-entropy loss is a theoretically sound and computationally feasible proxy for maximizing \(P(y_{\text{target}}|x)\):

% \begin{itemize}
%     \item It aligns with the goal of maximizing the likelihood of \(y_{\text{target}}\) across all contexts.
%     \item Despite the approximation introduced by Jensen's inequality, it effectively reduces the upper bound on \(-\log(P(y_{\text{target}}|x))\), achieving a similar outcome.
% \end{itemize}

Thus, minimizing the expected cross-entropy loss over sampled contexts is an effective approach to optimize text generation tasks.

\section{Warmup Generations Approach}
A general overview of our method is illustrated in Figure \ref{fig:workflow}. Unlike traditional SFT methods, where the loss is computed solely based on the input, our approach introduces an intermediate generation step. After receiving the input, the model first generates $n$ warmup sequences. The final loss is then computed as the average of $n$ individual losses, each conditioned on both the input and one of the generated warmup sequences. The pseudo-code outlining the implementation of our method is provided in %Algorithm 
\autoref{algo:1}.


\subsection{Implementation for Encoder-Decoder Models}
Encoder-decoder structured models have a clear separation between input and output. For this type of model, the input is processed through the encoder, and then $n$ $c_{init}$ are generated using beam search with sampling. Each generated $c_{init}$ is followed by a separator and fed into the decoder. Subsequently, the cross-entropy loss of $y_{target}$ is calculated $n$ times, conditioned on the input and each of the $n$ generated $c_{init}$. Finally, the average of these $n$ losses is taken as the final loss.

The inference process follows the same logic shown in Figure \autoref{fig:workflow}, given an input sequence, the model first generates a warmup sequence. This sequence is then concatenated with a separator and fed back into the beginning of the decoder. The model then generates the target sequence conditioning on both the original input and the generated warmup sequence. The final output consists of the tokens generated after the concatenated separator.

\subsection{Implementation for Decoder-Only Models}
Similar to encoder-decoder models, the input is first fed into the model, and $n$ $c_{init}$ are sampled using beam search. The input is then concatenated with $n$ $c_{init}$ sequences, followed by a separator, and fed back into the model. The final loss is the average cross-entropy loss of $y_{target}$ conditioned on the $n$ $c_{init}$ sequences and the input.

During inference, similar to encoder-decoder models, the warmup sequence is first generated and then appended to the input sequence, followed by a separator. The combined sequence is then fed back into the model to generate the target sequence. The final output consists of the tokens produced after the concatenated separator.
\subsection{Rationale Behind Using a Separator Between $c_{init}$ and $y_{target}$}
The inclusion of separators helps the model to distinguish the boundary between $c_{init}$ and $y_{target}$, preventing $y_{target}$ from being treated as a continuation of $c_{init}$. This enhances both the stability and efficiency of training. Since the separators are deterministically appended to the end of each $c_{init}$, the probability distributions of $c_{init}$ and $c_{init}$ followed by the separator remain the same. Additionally, since the model is rewarded based on the generation of $c_{init}$, the inclusion of separators does not disrupt this training process.
% \subsection{Implmentation for Encoder-only Models}
% Encoder-only models are generally not used as generative models, unlike encoder-decoder and decoder-only models. In this approach, a separator token is first appended to the original input, followed by $m$ mask tokens. Next, $n$ sets of predictions for the $m$ mask tokens are sampled using Monte Carlo dropout. The original input, concatenated with the separator and each of the $n$ sets of $m$ tokens, is then fed back into the model as the new input. The final loss is calculated as the average loss across the downstream tasks for each of the $n$ different final input sequences.






\begin{table}[t]
\centering
%\setlength{\tabcolsep}{2.8pt} 
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l c | c c}
\toprule
\textbf{Model} & \textbf{Warmup} &\textbf{Macro F1 } & \textbf{Accuracy} \\ 
% \midrule
%  \multicolumn{4}{c}{\textbf{\large Logiqa2}} \\
 \midrule
\multirow{2}{*}{T5-base} & \checkmark & \textbf{50.00} & \textbf{50.06} \\ 
 & $\times$ &  49.16 & 49.19 \\ 
% \midrule
\multirow{2}{*}{T5-large}& \checkmark &  \textbf{55.50} & \textbf{55.62} \\ 
 & $\times$ & 54.46 & 54.54 \\ 

\multirow{2}{*}{Llama-3.2-1B}& \checkmark &  \textbf{32.63} & \textbf{33.55} \\ 
 & $\times$ & 30.12 & 31.70 \\ 
\midrule

\end{tabular}
}
\caption{Performance of each model on LogiQA2 for Macro F1 and Accuracy.}
\label{table:logiqa2}
\end{table}




\begin{table*}[htbp]
\centering
%\resizebox{\textwidth}{!}{%
%\setlength{\tabcolsep}{4.4pt}
\scalebox{0.8}{
\begin{tabular}{l c c| r r r r| r r r r |c}
\toprule
\textbf{Model} & \textbf{Warmup} &\textbf{Metric} & \textbf{de-en} & \textbf{ru-en} & \textbf{zh-en} & \textbf{fr-en} & \textbf{en-de} & \textbf{en-ru} & \textbf{en-zh} & \textbf{en-fr} & \textbf{Avg} \\ 
\midrule
\multirow{6}{*}{mt5-base}       & \checkmark &\multirow{2}{*}{BLEU} &  \textbf{29.64}	&  \textbf{20.78}	&  \textbf{13.31}	&  \textbf{30.64}	&  \textbf{20.43}	&  \textbf{11.72}	&  \textbf{22.64}	&  \textbf{27.95}	&\textbf{22.14} \\ 
  & $\times$ & &    27.89	&   19.41	&  12.66	&   28.73	&   18.10&  10.64	&   20.95	&   26.14 & 20.57\\ 
      & \checkmark &\multirow{2}{*}{COMET} &  \textbf{83.12}	&  \textbf{78.19}	&  \textbf{77.05}	&  \textbf{82.99}	&  \textbf{75.11}	&  \textbf{69.64}&\textbf{75.89}	&  \textbf{76.89}& \textbf{77.36}\\ 
   & $\times$ &  &   82.21	&  77.60	&  75.50	&  81.95	&  72.88	& 68.13	&  74.45	&  75.60 &76.04\\ 
       & \checkmark &\multirow{2}{*}{ChrF++} &  \textbf{55.15}	&\textbf{46.48}	& \textbf{39.48}	&  \textbf{55.64}	&  \textbf{47.78}	&  \textbf{32.09}	&  \textbf{25.05}	&  \textbf{52.66}	 &\textbf{44.29}\\ 
       & $\times$ &  &    53.63& 45.57	&  38.01	&  53.84	&  45.43	&  30.71	&  23.44	&  50.87		&  42.69\\ 
\midrule
\multirow{6}{*}{mt5-large}       & \checkmark &\multirow{2}{*}{BLEU} &    \textbf{34.71}	&  \textbf{25.76}	&  \textbf{18.66}	& \textbf{35.63} &  \textbf{26.84}	&  \textbf{16.30}	&  \textbf{27.89}	&  \textbf{36.29}	&		  \textbf{27.76}\\ 
  & $\times$ & &    34.12	&  24.93	& 17.76&  34.84	& 24.91	&  14.29	&  27.29	&  30.39&26.07\\ 
      & \checkmark &\multirow{2}{*}{COMET} &  \textbf{86.74}	& \textbf{82.51}	&  \textbf{82.54}	& \textbf{86.37}		&   \textbf{82.67}	&   \textbf{77.43}	&  \textbf{82.19}	&  \textbf{83.50}		 &\textbf{82.99}\\ 
   & $\times$ & &    86.28&  82.30	&  81.83	&  86.12	&  81.47	& 74.43	&  82.18	& 80.91	 &81.94\\  
       & \checkmark &\multirow{2}{*}{ChrF++} &    \textbf{59.65}	&  \textbf{51.15}	& \textbf{45.45}& \textbf{59.39}		&  \textbf{53.19}	& \textbf{38.34}	&  \textbf{29.72}	&  \textbf{59.28}			&\textbf{49.52} \\ 
       & $\times$ &  &   58.75	& 50.80	& 44.21	&  58.87	&  51.44	&  34.47	&  29.17	&  54.33			& 47.75\\ 

\bottomrule
\end{tabular}
}
\caption{Performance on translation tasks with the comparison of the BLEU score, COMET score and ChrF++ score across different language pairs. For example, ``de-en'' denotes the source language to be German, and the target language to be English. }
\label{table:bleu_scores}
\end{table*}

\section{Experiments}

In this section, we present the tasks and corresponding datasets used,  the models selected for the experiments and the results obtained.

\subsection{Tasks and Datasets}
We evaluated our approach on three datasets spanning three tasks: FLORES~\cite{nllbteam2022languageleftbehindscaling} for testing, WMT for training for the translation task; LogiQA2~\cite{10174688} for logical reasoning multi-choice QA; and XSum~\cite{Narayan2018DontGM} for summarization;  
Specifically, we used WMT19 datasets \cite{barrault-etal-2019-findings} for the fine-tuning of de-en (en-de), ru-en (en-ru), and zh-en (en-zh) and the fr-en (en-fr) data from the WMT14 dataset \cite{bojar-etal-2014-findings}.

\subsection{Models}
We used T5-base (223M) and T5-large (738M) for summarization, T5-base, T5-large, and Llama-3.2-1B (1.24B) for multiple-choice logical reasoning, and mT5-base (582M) and mT5-large (1.23B) for translation. These models, covering both encoder-decoder and decoder-only architectures, serve as well-established benchmarks in their respective categories and are widely recognized for their effectiveness.
% By selecting models from different architectures, we aimed to showcase the generalizability of our method. Due to limited computational resources, we focused on smaller model variants while ensuring they sufficiently demonstrate the effectiveness of our method.

\subsection{Metrics}
We used the BLEU score \cite{papineni-etal-2002-bleu}, COMET score \footnote{The implementation of COMET was from huggingface.}  \cite{rei-etal-2020-comet}, and ChrF++ score \cite{popovic-2015-chrf} for translation. For logical reasoning multiple-choice, we used macro F1 and accuracy, and for summarization, we employed ROUGE score (1, 2, L) \cite{lin-2004-rouge} and BERTScore \footnote{The implementation of BERTScore was from huggingface, with the model type set to ``roberta-large''.} \cite{Zhang*2020BERTScore:}. 
% These metrics encompass not only traditional word- and character-level evaluations but also leverage the advantages of pre-trained language models for semantic-level assessment. 
\subsection{Experimental Settings}
For fine-tuning all tasks, we used a learning rate of 2e-5, with the warmup sequence's maximum sampled length capped at 8 tokens. Models were trained for 10 epochs. Due to computational constraints, we randomly selected 50,000 samples from the training set for fine-tuning in translation and summarization tasks. During fine-tuning, warmup sequences were generated using a beam size of 4, with 4 warmup sequences sampled per training sample for each loss calculation. 

For each task, we selected the checkpoint that achieved the highest metric score on the validation set and reported its performance on the test set. Specifically, for translation, checkpoints were selected based on the COMET score; for summarization, based on BERTScore; and for logical reasoning multiple-choice, based on the macro F1 score. For LogiQA2, each model was fine-tuned 3 times with different random seeds, and we report the average performance of the selected checkpoints.
% For GPT-2, input lengths were truncated to 750 tokens, and label sequences to 250 tokens due to the context window limitations of GPT-2 (1024 tokens). For translation tasks, fine-tuning was conducted over 10 epochs, with both input and output lengths truncated to 256 tokens. A beam size and sampling number of 4 were used for these tasks.





% \{Input Sequence\}\textbackslash n\{Output Field Name\} \}\textbackslash n Context:\{Warmup Sequence\}\}\textbackslash n\{Output Field Name\}:\{Label\}

% Here, \{Output Field Name\} was defined by the dataset. For example, in the XSum dataset, the \{Output Field Name\} was set to ``Summary.'' This approach addresses the absence of a clear separation between input and output in decoder-only models. By distinguishing the warmup sequence from the input sequence and establishing a connection with the output field, the template ensures coherence, as both the warmup and the output field are part of the ``output.''





% Bold values indicate better performance compared to the traditional task.  


\begin{table}[t]
\centering
%\setlength{\tabcolsep}{0.7pt}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{llrrrr}
\toprule
 & &\multicolumn{3}{c}{\textbf{Rouge}} &\textbf{BERT}\  \\ 
\textbf{Model} & \textbf{Warmup} & \textbf{R1} & \textbf{R2} & \textbf{RL} &\textbf{Score}\  \\ 
\midrule
\multirow{2}{*}{T5-base}& \checkmark & \textbf{37.63} &15.51 &30.32& \textbf{90.50}\\ 
& $\times$ & 37.18 & 15.19 & 29.96 & 90.40\\ 
% \midrule
\midrule
\multirow{2}{*}{T5-large}& \checkmark & \textbf{40.67} &18.23& 33.21& \textbf{91.11}\\ 
 &  $\times$ & 40.21 &17.84 & 32.75& 91.05\\ 
\bottomrule
\end{tabular}
}
\caption{Performance on summarization tasks for Rouge (R1, R2 and RL) and BERTScore. }
\label{table:summarization}
\end{table}
% Bold values indicate better performance compared to the traditional task. 



% \begin{table*}[h!]
%     \centering
    
%     \begin{tabular}{@{}>{\centering\arraybackslash}p{4cm}>{\centering\arraybackslash}p{2cm}>{\raggedright\arraybackslash}p{8cm}@{}}
%         \toprule
%         \textbf{Initial States} & \textbf{Type} & \multicolumn{1}{c}{\textbf{Content}} \\ % Header for "Content" centered
%         \midrule
%         \multicolumn{3}{c}{\textbf{\large Direct Core Phrases}} \\ % Centered "Direct Core Phrases"
%         \midrule
%         \textcolor{softred}{a British traveller in} & output & Similarly, \textcolor{softred}{a British traveller in} Spain could confuse a sign of a warning ... \\
%         & label & Similarly, \textcolor{softred}{a British traveller in} Spain may mistake a wave goodbye involving the palm ... \\
%         \midrule
%         \textcolor{softred}{a series of events that} & output & We live the time as \textcolor{softred}{a series of events that} go from the future to the present through the present. \\
%         & label & We experience time as \textcolor{softred}{a series of events} passing from the future through the present to the past. \\
%         \midrule
%         \multicolumn{3}{c}{\textbf{\large Similar Phrases}} \\ % Centered "Similar Phrases"
%         \midrule
%         \textcolor{softblue}{Please contact us directly} & output & In all cases, you must \textcolor{softblue}{reserve by telephone directly} from the aircompany. \\
%         & label & In all cases, you must \textcolor{softblue}{book by phone directly} with the airline. \\
%         \midrule
%         \textcolor{softblue}{when they are in danger} & output & They are not aggressive, but defend themselves if \textcolor{softblue}{they are in danger}. \\
%         & label & Moose (also known as elk) aren’t inherently aggressive, but will defend themselves if \textcolor{softblue}{they perceive a threat}. \\
%         %  \midrule
%         % \multicolumn{3}{l}{\textbf{Average Overlap Rate: 0.5367}} \\
%         \bottomrule
%     \end{tabular}
    
%     \caption{Qualitative analysis performed on the fr-en translation task to investigate the role of initial states. Two main categories are identified: Direct Core Phrases and Similar Phrases. The former means that those initial states can be directly identified in both the labels and the generated outputs, and the latter represent expressions that are semantically similar to important components in the labels and outputs.}
%     \label{table:casestudy}
% \end{table*}

% \begin{table*}[h!]
%     \centering
%     \renewcommand{\arraystretch}{1.3} % Adjust row height
%     \setlength{\tabcolsep}{6pt} % Adjust column spacing
    
%     \begin{small} % Reduce font size for the table
%     \begin{tabular}{@{}>{\centering\arraybackslash}p{4cm}>{\centering\arraybackslash}p{2cm}>{\raggedright\arraybackslash}p{8cm}@{}}
%         \toprule
%         \textbf{Initial States} & \textbf{Type} & \multicolumn{1}{c}{\textbf{Content}} \\ % Centered header for "Content"
%         \midrule
%         \multicolumn{3}{c}{\textbf{Direct Core Phrases}} \\ % Centered section title
%         \midrule
%         \textcolor{softred}{a British traveller in} & output & Similarly, \textcolor{softred}{a British traveller in} Spain could confuse a sign of a warning ... \\
%         & label & Similarly, \textcolor{softred}{a British traveller in} Spain may mistake a wave goodbye involving the palm ... \\
%         \midrule
%         \textcolor{softred}{a series of events that} & output & We live the time as \textcolor{softred}{a series of events that} go from the future to the present through the present. \\
%         & label & We experience time as \textcolor{softred}{a series of events} passing from the future through the present to the past. \\
%         \midrule
%         \multicolumn{3}{c}{\textbf{Similar Phrases}} \\ % Centered section title
%         \midrule
%         \textcolor{softblue}{Please contact us directly} & output & In all cases, you must \textcolor{softblue}{reserve by telephone directly} from the aircompany. \\
%         & label & In all cases, you must \textcolor{softblue}{book by phone directly} with the airline. \\
%         \midrule
%         \textcolor{softblue}{when they are in danger} & output & They are not aggressive, but defend themselves if \textcolor{softblue}{they are in danger}. \\
%         & label & Moose (also known as elk) aren’t inherently aggressive, but will defend themselves if \textcolor{softblue}{they perceive a threat}. \\
%         \bottomrule
%     \end{tabular}
%     \end{small} % End reduced font size
    
%     \caption{Qualitative analysis of the fr-en translation task, investigating the role of initial states. Two categories are identified: Direct Core Phrases, which are directly found in both labels and generated outputs, and Similar Phrases, which represent semantically similar expressions.}
%     \label{table:casestudy}
% \end{table*}


% \begin{table*}[]
% \centering
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{cccl}
% \hline
% Languages              & Initial States                                                        & Type & \multicolumn{1}{c}{Content}                                                                                                         \\ \hline
% \multicolumn{4}{c}{Direct Core Phrases}                                                                                                                                                                                                     \\ \hline
% \multirow{2}{*}{de-en} & \multirow{2}{*}{pyramid is the only one of}                           & MG   & The Cheops pyramid is the only one of the seven world wonders ...                                                                   \\
%                        &                                                                       & GT   & The Great Pyramid at Giza is the only one of the seven wonders ...                                                                  \\
% \multirow{2}{*}{fr-en} & \multirow{2}{*}{a British traveller in}                               & MG   & \begin{tabular}[c]{@{}l@{}}Similarly, a British traveller in Spain could confuse a sign of a\\ warning ...\end{tabular}             \\
%                        &                                                                       & GT   & \begin{tabular}[c]{@{}l@{}}Similarly, a British traveller in Spain may mistake a wave\\ goodbye involving the palm ...\end{tabular} \\
% \multirow{2}{*}{zh-en} & \multirow{2}{*}{a search on the Internet for}                         & MG   & Search on the Internet for a response to hostile environment courses ...                                                            \\
%                        &                                                                       & GT   & A search of the Internet for 'Hostile environment course' ...                                                                       \\
% \multirow{2}{*}{ru-en} & \multirow{2}{*}{because there was no national}                        & MG   & ... and because there was no national executive or judicial power, ...                                                              \\
%                        &                                                                       & GT   & ... and, because there was no national executive or judiciary, ...                                                                  \\ \hline
% \multicolumn{4}{c}{Similar Phrases}                                                                                                                                                                                                         \\ \hline
% \multirow{2}{*}{de-en} & \multicolumn{1}{l}{\multirow{2}{*}{female travelers are recommended}} & MG   & Women: It is recommended that all women travelling claim to be ...                                                                  \\
%                        & \multicolumn{1}{l}{}                                                  & GT   & Women: It is recommended that any women travellers say that ...                                                                     \\
% \multirow{2}{*}{fr-en} & \multirow{2}{*}{Please contact us directly}                           & MG   & \begin{tabular}[c]{@{}l@{}}In all cases, you must reserve by telephone directly from the\\ aircompany.\end{tabular}                 \\
%                        &                                                                       & GT   & In all cases, you must book by phone directly with the airline.                                                                     \\
% \multirow{2}{*}{zh-en} & \multirow{2}{*}{shows a changing temperature}                         & MG   & The ultraviolet image shows that the changes in the night temperature suggest that ...                                              \\
%                        &                                                                       & GT   & Infrared images show that the temperature variations from night and day show that ...                                               \\
% \multirow{2}{*}{ru-en} & \multirow{2}{*}{According to the Japanese nuclear}                    & MG   & According to the nuclear authority of Japan, radioactive ...                                                                        \\
%                        &                                                                       & GT   & According to Japan's nuclear agency, radioactive ...                                                                                \\ \hline
% \end{tabular}%
% }
% \caption{}
% \label{tab:my-table}
% \end{table*}
% \begin{table*}[ht!]
% \centering
% \renewcommand{\arraystretch}{1.4} % 增大行距
% \setlength{\tabcolsep}{2.5pt} % 调整列间距
% \begin{tabular}{cccl}
% \hline
% \textbf{Languages} & \textbf{Initial States} & \textbf{Type} & \multicolumn{1}{c}{\textbf{Content}} \\ \hline
% \multicolumn{4}{c}{\textbf{Direct Core Phrases}} \\ \hline
% \multirow{2}{*}{de-en} & \multirow{2}{*}{pyramid is the only one of} & MG & The Cheops pyramid is the only one of the seven world wonders ... \\
%                        &                                            & GT & The Great Pyramid at Giza is the only one of the seven wonders ... \\ \hline
% \multirow{2}{*}{fr-en} & \multirow{2}{*}{a British traveller in}    & MG & Similarly, a British traveller in Spain could confuse a sign of a warning ... \\
%                        &                                            & GT & Similarly, a British traveller in Spain may mistake a wave goodbye involving the palm ... \\ \hline
% \multirow{2}{*}{zh-en} & \multirow{2}{*}{a search on the Internet for} & MG & Search on the Internet for a response to hostile environment courses ... \\
%                        &                                            & GT & A search of the Internet for 'Hostile environment course' ... \\ \hline
% \multirow{2}{*}{ru-en} & \multirow{2}{*}{because there was no national} & MG & ... and because there was no national executive or judicial power, ... \\
%                        &                                            & GT & ... and, because there was no national executive or judiciary, ... \\ \hline
% \multicolumn{4}{c}{\textbf{Similar Phrases}} \\ \hline
% \multirow{2}{*}{de-en} & female travelers are recommended           & MG & Women: It is recommended that all women travelling claim to be ... \\
%                        &                                            & GT & Women: It is recommended that any women travellers say that ... \\ \hline
% \multirow{2}{*}{fr-en} & Please contact us directly                 & MG & In all cases, you must reserve by telephone directly from the aircompany. \\
%                        &                                            & GT & In all cases, you must book by phone directly with the airline. \\ \hline
% \multirow{2}{*}{zh-en} & shows a changing temperature               & MG & The ultraviolet image shows that the changes in the night temperature suggest that ... \\
%                        &                                            & GT & Infrared images show that the temperature variations from night and day show that ... \\ \hline
% \multirow{2}{*}{ru-en} & According to the Japanese nuclear          & MG & According to the nuclear authority of Japan, radioactive ... \\
%                        &                                            & GT & According to Japan's nuclear agency, radioactive ... \\ \hline
% \end{tabular}
% \caption{Examples of Direct Core Phrases and Similar Phrases for different languages.}
% \label{tab:my-table}
% \end{table*}
% \begin{table*}[ht!]
% \centering
% \renewcommand{\arraystretch}{1.4} % 增大行距
% \setlength{\tabcolsep}{5pt} % 调整列间距
% \begin{tabular}{cccl}
% \hline
% \textbf{Languages} & \textbf{Initial States} & \textbf{Type} & \multicolumn{1}{p{0.55\textwidth}}{\textbf{Content}} \\ \hline
% \multicolumn{4}{c}{\textbf{Direct Core Phrases}} \\ \hline
% \multirow{2}{*}{de-en} & \multirow{2}{*}{pyramid is the only one of} & MG & The Cheops pyramid is the only one of the seven world wonders ... \\
%                        &                                            & GT & The Great Pyramid at Giza is the only one of the seven wonders ... \\ \hline
% \multirow{2}{*}{fr-en} & \multirow{2}{*}{a British traveller in}    & MG & Similarly, a British traveller in Spain could confuse a sign of a warning ... \\
%                        &                                            & GT & Similarly, a British traveller in Spain may mistake a wave goodbye involving the palm ... \\ \hline
% \multirow{2}{*}{zh-en} & \multirow{2}{*}{a search on the Internet for} & MG & Search on the Internet for a response to hostile environment courses ... \\
%                        &                                            & GT & A search of the Internet for 'Hostile environment course' ... \\ \hline
% \multirow{2}{*}{ru-en} & \multirow{2}{*}{because there was no national} & MG & ... and because there was no national executive or judicial power, ... \\
%                        &                                            & GT & ... and, because there was no national executive or judiciary, ... \\ \hline
% \multicolumn{4}{c}{\textbf{Similar Phrases}} \\ \hline
% \multirow{2}{*}{de-en} & female travelers are recommended           & MG & Women: It is recommended that all women travelling claim to be ... \\
%                        &                                            & GT & Women: It is recommended that any women travellers say that ... \\ \hline
% \multirow{2}{*}{fr-en} & Please contact us directly                 & MG & In all cases, you must reserve by telephone directly from the aircompany. \\
%                        &                                            & GT & In all cases, you must book by phone directly with the airline. \\ \hline
% \multirow{2}{*}{zh-en} & shows a changing temperature               & MG & The ultraviolet image shows that the changes in the night temperature suggest that ... \\
%                        &                                            & GT & Infrared images show that the temperature variations from night and day show that ... \\ \hline
% \multirow{2}{*}{ru-en} & According to the Japanese nuclear          & MG & According to the nuclear authority of Japan, radioactive ... \\
%                        &                                            & GT & According to Japan's nuclear agency, radioactive ... \\ \hline
% \end{tabular}
% \caption{Examples of Direct Core Phrases and Similar Phrases for different languages.}
% \label{tab:my-table}
% \end{table*}
\begin{table*}[ht!]
\centering
\renewcommand{\arraystretch}{1.6}
%\setlength{\tabcolsep}{6pt} 
%\small
\footnotesize
%\resizebox{\textwidth}{!}{%
\scalebox{0.9}{
\begin{tabular}{ccl}
\hline
\textbf{Lang Pair} & \textbf{Warmup Sequence} & \multicolumn{1}{p{0.5\textwidth}}{\textbf{\hspace{50mm}Content}} \\ \hline
\multicolumn{3}{c}{\textbf{Direct Core Phrases}} \\ \hline
\multirow{2}{*}{de-en} & \multirow{2}{*}{\parbox{0.2\textwidth}{\textcolor{softred}{``pyramid is the only one of''}}} & T: The Cheops \textcolor{softred}{pyramid is the only one of} the seven world wonders ... \\
                       &                                                                 & G: The Great \textcolor{softred}{Pyramid} at Giza \textcolor{softred}{is the only one of} the seven wonders ... \\ \hline
\multirow{2}{*}{fr-en} & \multirow{2}{*}{\parbox{0.2\textwidth}{\textcolor{softred}{``a British traveller in''}}}    & T: Similarly, \textcolor{softred}{a British traveller in} Spain could confuse ... \\
                       &                                                                 & G: Similarly, \textcolor{softred}{a British traveller in} Spain may mistake ... \\ \hline
\multirow{2}{*}{zh-en} & \multirow{2}{*}{\parbox{0.2\textwidth}{\textcolor{softred}{``a search on the Internet for''}}} & T: \textcolor{softred}{Search on the Internet for} a response to hostile environment courses ... \\
                       &                                                                 & G: \textcolor{softred}{A search of the Internet for} `Hostile environment course' ... \\ \hline
\multirow{2}{*}{ru-en} & \multirow{2}{*}{\parbox{0.2\textwidth}{\textcolor{softred}{``because there was no national''}}} & T: ... and \textcolor{softred}{because there was no national} executive or judicial power, ... \\
                       &                                                                 &G: ... and, \textcolor{softred}{because there was no national} executive or judiciary, ... \\ \hline
\multicolumn{3}{c}{\textbf{Similar Phrases}} \\ \hline
\multirow{2}{*}{de-en} & \multirow{2}{*}{\parbox{0.2\textwidth}{\textcolor{softblue}{``female travelers are recommended''}}}           & T: \textcolor{softblue}{Women: It is recommended that all women travelling} claim to be married, ... \\
                       &                                                                 & G: \textcolor{softblue}{Women: It is recommended that any women travellers} say that they are ... \\ \hline
\multirow{2}{*}{fr-en} & \multirow{2}{*}{\parbox{0.2\textwidth}{\textcolor{softblue}{``Please contact us directly''}}}                 & T: In all cases, you must \textcolor{softblue}{reserve by telephone directly} from the aircompany. \\
                       &                                                                 & G: In all cases, you must \textcolor{softblue}{book by phone directly} with the airline. \\ \hline
\multirow{2}{*}{zh-en} & \multirow{2}{*}{\parbox{0.2\textwidth}{\textcolor{softblue}{``shows a changing temperature''}}}               & T: The ultraviolet image \textcolor{softblue}{shows that the changes in the night temperature}  ... \\
                       &                                                                 & G: Infrared images \textcolor{softblue}{show that the temperature variations} from  night and day ... \\ \hline
\multirow{2}{*}{ru-en} & \multirow{2}{*}{\parbox{0.2\textwidth}{\textcolor{softblue}{``According to the Japanese nuclear''}}}          & T: \textcolor{softblue}{According to the nuclear authority of Japan}, radioactive cezai and ... \\
                       &                                                                 & G: \textcolor{softblue}{According to Japan's nuclear agency}, radioactive caesium and iodine ... \\ \hline
\end{tabular}
}
\caption{Examples of Direct Core Phrases and Similar Phrases for different language pairs. ``T'' indicates translations that are model-generate, and ``G'' indicates the golden label.}
\label{table:casestudy}
\end{table*}

\begin{table}[t]
\centering
% \renewcommand{\arraystretch}{1.3} % 增大行距
%\setlength{\tabcolsep}{12pt} % 调整列间距
\scalebox{0.8}{
\begin{tabular}{c| c c}
\hline
\textbf{Lang Pair} & \textbf{Devtest (\%)} & \textbf{Dev (\%)} \\ 
\hline
de-en                  & 55.05               & 56.57              \\ 
en-de                  & 32.71                  & 33.41              \\ 
fr-en                  & 40.94                 & 42.75              \\ 
en-fr                  & 34.42               & 32.97              \\ 
\midrule
zh-en                  & 43.91             & 42.90              \\ 
en-zh                  & 63.03                & 63.54             \\ 
ru-en                  & 46.13                  & 48.80              \\ 
en-ru                  & 33.51                 & 33.48              \\ 
\hline
\end{tabular}
}
\caption{Overlap rates of the warmup sequence for different language pairs on Flores200 devtest and dev datasets, generated by the mT5-base model.}

\label{tab:overlap-rate}
\end{table}





% \section{Analysis}
\subsection{Results and Discussions}
We put our experiment results in Table \ref{table:logiqa2}, \ref{table:bleu_scores}, and \ref{table:summarization}.
\paragraph{Warmup generations consistently enhance performance across tasks}
Across all three tasks, models utilizing warmup generations outperform those employing traditional SFT methods. The most significant gains are observed in translation tasks, where mT5-base achieves an average improvement of 1.57 BLEU, 1.32 COMET, and 1.60 ChrF++ scores across 8 language pairs.
For multiple-choice logical reasoning, the T5-base model trained with warmup generations achieves 0.84 higher macro F1 and 0.87 higher accuracy compared to models using traditional SFT. A similar trend is observed in summarization, where the T5-base model with warmup generations yields gains of 0.45 ROUGE-1, 0.32 R2, 0.36 RL, and 0.1 BERTScore. %on a scale of 100.
\paragraph{Performance gains are robust to increases in model size}
When scaling the models from base to large, we observe similar or even greater performance gains. In translation, mT5-large exhibits a greater average improvement than mT5-base, with a higher BLEU gain of 1.69, a COMET increase of 1.05 (slightly lower but still comparable), and a greater ChrF++ gain of 1.77.
For logical reasoning, T5-large benefits more from warmup generations than T5-base, achieving performance gains of 1.04 in Macro F1 and 1.08 in Accuracy. Similarly, in summarization, T5-large outperforms its counterpart without warmup generations, with improvements of 0.46, 0.39, and 0.46 in ROUGE-1, ROUGE-2, and ROUGE-L scores, respectively.  
\paragraph{Performance gain extends to decoder-only architecture}
The decoder-only model, Llama-3.2-1B, gains from warmup generations in multiple-choice logical reasoning, achieving performance improvements of 2.51 in Macro F1 and 1.85 in Accuracy.
\paragraph{Warmup generations improve lexical alignment more than semantic richness in summarization}
For summarization, using warmup generations achieves BERTScore increases by 0.06–0.1 points on a scale of 100, indicating that while warmup sequences enhance word selection and fluency, they do not significantly impact semantic richness. This suggests that, for summarization, warmup sequences help the model better mimic the word choices made in the reference summaries, leading to higher word-level alignment (ROUGE scores). However, they do not push the model to generate additional semantic content beyond what it would traditionally learn to extract through standard training, which explains the smaller improvement in BERTScore.

% \jiayi{what are the main results?}
% , warmup generations consistently enhance model performance across all tasks being tested, but the extent of improvement varies based on the task and model size.

% For logical reasoning, our method gains 0.84 macro F1 and 0.87 accuracy for T5-base, 1.04 macro F1 and 1.08 accuracy for T5-large,  2.52  macro F1 and 1.85 accuracy for Llama-3.2-1B. 
% The greater improvement in the larger model suggests that T5-large leverages intermediate reasoning steps more effectively, possibly due to its higher capacity for modelling logical dependencies.

% For summarization, 
% % warmup generations consistently improve performance across models. 
% T5-base achieves a gain of 0.45 ROUGE-1, 0.32 ROUGE-2, and 0.36 ROUGE-L, while T5-large exhibits slightly higher improvements of 0.46, 0.39, and 0.46, respectively. 
% Additionally, 
% In contrast, warmup sequences lead to notable COMET score improvements for translation, suggesting that the effectiveness of warmup sequences is task-dependent. Since translation requires precise word choices and alignment, warmup sequences may play a stronger role in refining token-level decisions, leading to greater semantic and adequacy-based improvements as captured by COMET. 

% For translation, 
% warmup generations improve performance across both mT5-base and mT5-large, but 
% the extent of improvement varies significantly depending on the language pair.
% while the performance gain of warmup generation on mT5-large is greater on average than on mT5-base, the trend differs between language pairs.
% For some language pairs, mT5-large benefits more from warmup than mT5-base. The en-fr pair sees the largest difference, with mT5-large improving by 4.09 BLEU more than mT5-base.
% , 
% highlighting that warmup remains beneficial even for high-resource, structurally similar languages
% The en-ru pair also benefits more in mT5-large, with a 0.93 more BLEU improvement over the mT5-base. Similarly, warmup on mT5-large gains 0.25 more BLEU for zh-en compared to mT5-base. 
% However, for other language pairs, mT5-base benefits more from warmup than mT5-large. 
% In de-en and en-de, mT5-large gains less from warmup than mT5-base, with BLEU score improvements being 1.16 and 0.40 points lower respectively. Similarly, fr-en, ru-en, and en-zh all show smaller warmup improvements in mT5-large compared to mT5-base.
% with differences of -1.12, -0.54, and -1.09 BLEU, respectively.

Overall, warmup sequences consistently improve performance across tasks, models, and architectures, but their effectiveness is task-dependent. 
For encoder-decoder models, 
logical reasoning benefits more in 
larger
%  encoder-decoder 
models, 
while translation shows variable gains depending on the language pair. Summarization, in contrast, benefits uniformly across scales. 
While the decoder-only model, LLaMA-3.2-1B can leverage warmup sequences effectively, its relative performance remains lower than encoder-decoder models like T5.

\subsection{Ablation Studies}



% \begin{table}[htbp]
% \centering
% \begin{tabular}{ c|  c c}
% \toprule
% \textbf{Sample} &\textbf{Macro F1}& \textbf{Accuracy}   \\ 
% \midrule
% 4 &29.64  & 83.12\\ 
% \midrule
% 2 & 28.51 &82.45 \\ 
% % \midrule
% 6 & \textbf{29.70}& \textbf{83.24}\\ 
% 8 & 29.62  &83.06 \\ 
% \bottomrule
% \end{tabular}
% \caption{The performance of t5-base models trained with different numbers of warmup sequences sampled during training for the Logiqa2 dataset.}
% \label{table:seqnum_logiqa2}
% \end{table}


\begin{figure*}
    \centering
    \includegraphics[width=0.48\textwidth, height=0.18\textheight]{figure_logiqa2.jpg}
    \includegraphics[width=0.48\textwidth, height=0.18\textheight]{figure_translation.jpg}
    \caption{\textbf{Models' training loss after each epoch of fine-tuning}. The left figure represents T5-base on the LogiQA2 dataset, while the right graph represents mT5-base on the WMT19 dataset for the de-en language pair.}
    \label{fig:loss_logiqa2}
\end{figure*}


% \subsubsection{enc-dec vs. dec only}
% \subsubsection{Base vs. Large}
% As the size of the model increase from ``base'' to ``large'', we observe a change in the amount of performance gain
\subsubsection{Number of Samples}

% \begin{table}[htbp]
% \centering
% \begin{tabular}{ c| c c c}
% \toprule
% \textbf{Sample} & \textbf{BLEU}&\textbf{COMET}&\textbf{ChrF++}   \\ 
% \midrule
% 4 &29.64  & 83.12&55.15 \\ 
% \midrule
% 2 & 28.51 &82.45 &54.07\\ 
% % \midrule
% 6 & \textbf{29.70}& \textbf{83.24}& 55.26\\ 
% 8 & 29.62  &83.06 &  \textbf{55.38}\\ 
% \bottomrule
% \end{tabular}
% \begin{tabularx}{  c| X c X}

% \toprule

%  \textbf{Sample} &\textbf{Macro F1} & & \textbf{Accuracy}\\
% \midrule
%   4 & 50.00 & &  50.06 \\ 

% \midrule
%  2 & 50.09 & &  50.17 \\ 
%   6 & 50.19 & & 50.19 \\ 
%   8 & \textbf{50.99} & &  \textbf{50.95} \\ 
% \bottomrule
% \end{tabularx}



% \caption{The performance of mT5-base (de-en translation, upper) and T5-base (LogiQA2, lower) models trained with varying numbers of warmup sequences sampled during training. }
% \label{table:seqnum_translation}
% \end{table}

% \begin{table}[htbp]
% \centering
% \renewcommand{\arraystretch}{1.2} % Adjust row height for better readability
% \setlength{\tabcolsep}{6pt} % Adjust column spacing

% \begin{tabularx}{\columnwidth}{c| >{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X} 
% \toprule
% \textbf{Sample} & \textbf{BLEU} & \textbf{COMET} & \textbf{ChrF++} & \textbf{Macro F1} & \textbf{Accuracy} \\ 
% \midrule
% 4 & 29.64  & 83.12 & 55.15 & 50.00 & 50.06 \\ 
% 2 & 28.51  & 82.45 & 54.07 & 50.09 & 50.17 \\ 
% 6 & \textbf{29.70} & \textbf{83.24} & 55.26 & 50.19 & 50.19  \\ 
% 8 & 29.62  & 83.06  & \textbf{55.38} & \textbf{50.99} & \textbf{50.95} \\ 
% \bottomrule
% \end{tabularx}

% \caption{The performance of mT5-base (de-en translation) and T5-base (LogiQA2) models trained with varying numbers of warmup sequences sampled during training.}
% \label{table:seqnum_translation}
% \end{table}


\begin{table}[t]
\centering
%\setlength{\tabcolsep}{0.7pt}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{c|ccc|cc}
\toprule
 & \multicolumn{3}{c}{\textbf{de-en translation}} & \multicolumn{2}{c}{\textbf{LogiQA2}}\  \\ 
\textbf{Sample} & \textbf{BLEU} & \textbf{COMET} & \textbf{ChrF++} & \textbf{Macro F1} &\textbf{Accuracy}\  \\ 
\midrule
4 & 29.64  & 83.12 & 55.15 & 50.00 & 50.06 \\ 
2 & 28.51  & 82.45 & 54.07 & 50.09 & 50.17 \\ 
6 & \textbf{29.70} & \textbf{83.24} & 55.26 & 50.19 & 50.19  \\ 
8 & 29.62  & 83.06  & \textbf{55.38} & \textbf{50.99} & \textbf{50.95} \\ 
\bottomrule
\end{tabular}
}
\caption{Performance of mT5-base (de-en translation) and T5-base (LogiQA2) models trained with varying numbers of warmup sequences sampled during training. }
\label{table:seqnum_study}
\end{table}



To assess the impact of the number of sampled warmup sequences during training, we analyze both training loss trends and test set performance across different sample numbers.

As shown in Figure \ref{fig:loss_logiqa2}, increasing the number of sampled warmup sequences generally accelerates convergence and reduces final training loss. 
% Models trained with 4, 6, or 8 sampled sequences converge faster and reach lower final loss values than those trained with 2 sequences or the traditional approach. 
However, the differences between 4 and 6 sequences for LogiQA2, and 4, 6, and 8 sequences for de-en translation are relatively small, suggesting that beyond a certain threshold, additional samples do not significantly reduce training loss further.

The results for LogiQA2 in Table \ref{table:seqnum_study} indicate that increasing the number of samples does not lead to strictly monotonic improvements. The default setting of 4 samples achieves 50.00 Macro F1 and 50.06 Accuracy while increasing to 6 samples provides only a slight improvement (50.19 Macro F1 and 50.19 Accuracy). However, moving from 6 to 8 samples results in the largest jump, with Macro F1 increasing to 50.99 and Accuracy to 50.95. Notably, reducing the number of samples to 2 still achieves 50.09 Macro F1 and 50.17 accuracy,
% which is slightly higher than using 4 samples.
this suggests that even a small number of warmup samples can provide meaningful improvements.

The results for de-en translation exhibit a similar trend. While increasing the number of samples improves translation quality up to 6 sequences, beyond this point, the gains become marginal or even slightly decrease. Specifically, BLEU improves from 28.51 (2 samples) to 29.70 (6 samples) but slightly drops to 29.62 with 8 samples. COMET score follows a similar pattern, increasing from 82.45 to 83.24 before slightly decreasing to 83.06, while ChrF++ continues improving slightly, though the changes are minimal beyond 6 samples.

These findings suggest that while increasing the number of warmup sequences reduces training loss and improves test-time performance, there exists an optimal range—such as 6 to 8 samples for de-en translation—beyond which the benefits diminish. The initial improvements stem from better approximations of the probability distribution of warmup sequences, leading to more effective learning. However, adding too many samples can introduce redundancy or increased variance, limiting further performance gains.


\subsection{Qualitative Analysis}
We performed a qualitative analysis of the translation task to investigate the role of warmup sequences. 
As the results shown in Table \ref{table:casestudy}, we found that these warmup sequences can be primarily categorized into two types:
\begin{itemize}
\item Direct Core Phrases: These warmup sequences can be directly identified in both the labels and the generated outputs.
\item Similar Phrases: Expressions that are semantically similar to important components in the labels and outputs.
\end{itemize}
% Direct Core Phrases: These initial states can be directly identified in both the labels and the generated outputs.

% Similar Phrases: These initial states represent expressions that are semantically similar to important components in the labels and outputs.


For example, ``a British traveller in''  and ``a series of events that'' can be directly found in both the output and ground-truth labels. This indicates that for certain scenarios, initial states function as core-information extractors, guiding the model to generate outputs focusing on these core concepts.

Meanwhile, in other cases, the warmup sequences are more semantically related rather than exact phrase matches. For instance, ``when they are in danger'' is semantically related to ``they perceive a threat'' in the label, and ``Please contact us directly'' aligns with ``book by phone directly with the airline.'' In such scenarios, the initial states serve as semantic guides, enabling the model to generate outputs that capture the intended meaning without relying on exact phrase matching.

To further evaluate this dual role, we calculated the overlapping rate of the words in the initial states that also appear in the labels in Table \ref{tab:overlap-rate}.
As we can see, ``X-eng'' all achieve a overlapping rate over 40\%, with ``de-en'' reaching 56.57\% at the most.
On ``Eng-X'', the overlapping rate also reach at least 32.71\% on ``en-de'', and get to the highest on ``en-zh'' with the rate of 63.53\%. 
This means that generally, over 40\% of the words in the warmup sequence could be found in the ground-truth, indicating a strong alignment between the initial states and the ground-truth data. 
% This high overlap reinforces the idea that initial states effectively capture key elements of the labels, providing a balance between precision and contextual understanding. 
By acting as both direct extractors and semantic interpreters, the initial states ensure the generated outputs remain closely aligned with the intended semantics and structure of the target language.



\section{Conclusions}
In this work, we introduced a task-agnostic framework with theoretical proof and derivation, for improving sequence-to-sequence learning through warmup generations, where models learn to generate intermediate sequences to enhance final output quality. Unlike traditional approaches, our method learns intermediate steps in an unsupervised manner, improving performance across diverse tasks without requiring task-specific annotations.
Experiments demonstrate that warmup sequences consistently benefit both encoder-decoder and decoder-only models across different sizes. Analysis reveals that warmup sequences aid generation by extracting key phrases and providing semantically related guidance, resulting in more fluent and contextually accurate outputs. Additionally, increasing the number of sampled warmup sequences accelerates convergence and enhances test-time performance, though gains diminish beyond a certain threshold.
However, the performance gains vary across tasks and architectures, highlighting the need for further investigation into how different task types and model structures influence warmup effectiveness.
Overall, by introducing and demonstrating the effectiveness of warmup sequences across multiple seq2seq tasks, this work lays the groundwork for further research into leveraging intermediate generations to enhance model training and generation.

\section*{Limitations}
While our proposed framework demonstrates improvements across various tasks, there are several limitations to address.
The first is increased training time. The framework relies on sampling multiple initial states during training, introducing computational overhead compared to traditional supervised fine-tuning methods. This can make training more resource-intensive, particularly for large-scale datasets or deployment in constrained environments. Future work could explore more efficient sampling strategies or adaptive selection methods to mitigate this cost.
Another limitation is that warmup sequences primarily enhance the model's lexical-level understanding rather than deeper reasoning or structural-level improvements. 
As shown in our experiments, warmup generation aids the model in selecting key phrases and improving word choice, but it does not explicitly introduce or infer new knowledge beyond what is present in the input. Future research could explore how warmup sequences might be adapted to facilitate higher-level abstraction or knowledge augmentation, potentially bridging gaps in implicit reasoning.
Finally, our framework has not been tested on decoder-only models for generative tasks. While experiments on LogiQA2 demonstrate improvements for decoder-only architectures, the application of warmup sequences to open-ended text generation (e.g., summarization or translation) in decoder-only models remains unexplored. This poses potential challenges, as decoder-only models lack explicit input-output alignments found in sequence-to-sequence tasks, making it unclear whether warmup sequences would be equally effective. Investigating warmup generation within causal language models is an important direction for future work.


% \section*{Contributions}
% Senyu came up with the idea and implemented the codes, Zipeng and Senyu ran the experiments and wrote this report together together.  
% To produce a PDF file, pdf\LaTeX{} is strongly recommended (over original \LaTeX{} plus dvips+ps2pdf or dvipdf). Xe\LaTeX{} also produces PDF files, and is especially suitable for text in non-Latin scripts.

% \section{Preamble}

% The first line of the file must be
% \begin{quote}
% \begin{verbatim}
% \documentclass[11pt]{article}
% \end{verbatim}
% \end{quote}

% To load the style file in the review version:
% \begin{quote}
% \begin{verbatim}
% \usepackage[review]{acl}
% \end{verbatim}
% \end{quote}
% For the final version, omit the \verb|review| option:
% \begin{quote}
% \begin{verbatim}
% \usepackage{acl}
% \end{verbatim}
% \end{quote}

% To use Times Roman, put the following in the preamble:
% \begin{quote}
% \begin{verbatim}
% \usepackage{times}
% \end{verbatim}
% \end{quote}
% (Alternatives like txfonts or newtx are also acceptable.)

% Please see the \LaTeX{} source of this document for comments on other packages that may be useful.

% Set the title and author using \verb|\title| and \verb|\author|. Within the author list, format multiple authors using \verb|\and| and \verb|\And| and \verb|\AND|; please see the \LaTeX{} source for examples.

% By default, the box containing the title and author names is set to the minimum of 5 cm. If you need more space, include the following in the preamble:
% \begin{quote}
% \begin{verbatim}
% \setlength\titlebox{<dim>}
% \end{verbatim}
% \end{quote}
% where \verb|<dim>| is replaced with a length. Do not set this length smaller than 5 cm.

% \section{Document Body}

% \subsection{Footnotes}

% Footnotes are inserted with the \verb|\footnote| command.\footnote{This is a footnote.}

% \subsection{Tables and figures}

% See Table~\ref{tab:accents} for an example of a table and its caption.
% \textbf{Do not override the default caption sizes.}

% \begin{table}
% \centering
% \begin{tabular}{lc}
% \hline
% \textbf{Command} & \textbf{Output}\\
% \hline
% \verb|{\"a}| & {\"a} \\
% \verb|{\^e}| & {\^e} \\
% \verb|{\`i}| & {\`i} \\ 
% \verb|{\.I}| & {\.I} \\ 
% \verb|{\o}| & {\o} \\
% \verb|{\'u}| & {\'u}  \\ 
% \verb|{\aa}| & {\aa}  \\\hline
% \end{tabular}
% \begin{tabular}{lc}
% \hline
% \textbf{Command} & \textbf{Output}\\
% \hline
% \verb|{\c c}| & {\c c} \\ 
% \verb|{\u g}| & {\u g} \\ 
% \verb|{\l}| & {\l} \\ 
% \verb|{\~n}| & {\~n} \\ 
% \verb|{\H o}| & {\H o} \\ 
% \verb|{\v r}| & {\v r} \\ 
% \verb|{\ss}| & {\ss} \\
% \hline
% \end{tabular}
% \caption{Example commands for accented characters, to be used in, \emph{e.g.}, Bib\TeX{} entries.}
% \label{tab:accents}
% \end{table}

% \subsection{Hyperlinks}

% Users of older versions of \LaTeX{} may encounter the following error during compilation: 
% \begin{quote}
% \tt\verb|\pdfendlink| ended up in different nesting level than \verb|\pdfstartlink|.
% \end{quote}
% This happens when pdf\LaTeX{} is used and a citation splits across a page boundary. The best way to fix this is to upgrade \LaTeX{} to 2018-12-01 or later.

% \subsection{Citations}

% \begin{table*}
% \centering
% \begin{tabular}{lll}
% \hline
% \textbf{Output} & \textbf{natbib command} & \textbf{Old ACL-style command}\\
% \hline
% \citep{Gusfield:97} & \verb|\citep| & \verb|\cite| \\
% \citealp{Gusfield:97} & \verb|\citealp| & no equivalent \\
% \citet{Gusfield:97} & \verb|\citet| & \verb|\newcite| \\
% \citeyearpar{Gusfield:97} & \verb|\citeyearpar| & \verb|\shortcite| \\
% \hline
% \end{tabular}
% \caption{\label{citation-guide}
% Citation commands supported by the style file.
% The style is based on the natbib package and supports all natbib citation commands.
% It also supports commands defined in previous ACL style files for compatibility.
% }
% \end{table*}

% Table~\ref{citation-guide} shows the syntax supported by the style files.
% We encourage you to use the natbib styles.
% You can use the command \verb|\citet| (cite in text) to get ``author (year)'' citations, like this citation to a paper by \citet{Gusfield:97}.
% You can use the command \verb|\citep| (cite in parentheses) to get ``(author, year)'' citations \citep{Gusfield:97}.
% You can use the command \verb|\citealp| (alternative cite without parentheses) to get ``author, year'' citations, which is useful for using citations within parentheses (e.g. \citealp{Gusfield:97}).

% \subsection{References}

% \nocite{Ando2005,borschinger-johnson-2011-particle,andrew2007scalable,rasooli-tetrault-2015,goodman-etal-2016-noise,harper-2014-learning}

% The \LaTeX{} and Bib\TeX{} style files provided roughly follow the American Psychological Association format.
% If your own bib file is named \texttt{custom.bib}, then placing the following before any appendices in your \LaTeX{} file will generate the references section for you:
% \begin{quote}
% \begin{verbatim}
% \bibliographystyle{acl_natbib}
% \bibliography{custom}
% \end{verbatim}
% \end{quote}

% You can obtain the complete ACL Anthology as a Bib\TeX{} file from \url{https://aclweb.org/anthology/anthology.bib.gz}.
% To include both the Anthology and your own .bib file, use the following instead of the above.
% \begin{quote}
% \begin{verbatim}
% \bibliographystyle{acl_natbib}
% \bibliography{anthology,custom}
% \end{verbatim}
% \end{quote}

% Please see Section~\ref{sec:bibtex} for information on preparing Bib\TeX{} files.

% \subsection{Appendices}

% Use \verb|\appendix| before any appendix section to switch the section numbering over to letters. See Appendix~\ref{sec:appendix} for an example.

% \section{Bib\TeX{} Files}
% \label{sec:bibtex}

% Unicode cannot be used in Bib\TeX{} entries, and some ways of typing special characters can disrupt Bib\TeX's alphabetization. The recommended way of typing special characters is shown in Table~\ref{tab:accents}.

% Please ensure that Bib\TeX{} records contain DOIs or URLs when possible, and for all the ACL materials that you reference.
% Use the \verb|doi| field for DOIs and the \verb|url| field for URLs.
% If a Bib\TeX{} entry has a URL or DOI field, the paper title in the references section will appear as a hyperlink to the paper, using the hyperref \LaTeX{} package.

% \section*{Acknowledgements}

% This document has been adapted
% by Steven Bethard, Ryan Cotterell and Rui Yan
% from the instructions for earlier ACL and NAACL proceedings, including those for 
% ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
% NAACL 2019 by Stephanie Lukin and Alla Roskovskaya, 
% ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu, 
% NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
% Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
% ACL 2017 by Dan Gildea and Min-Yen Kan, 
% NAACL 2017 by Margaret Mitchell, 
% ACL 2012 by Maggie Li and Michael White, 
% ACL 2010 by Jing-Shin Chang and Philipp Koehn, 
% ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui, 
% ACL 2005 by Hwee Tou Ng and Kemal Oflazer, 
% ACL 2002 by Eugene Charniak and Dekang Lin, 
% and earlier ACL and EACL formats written by several people, including
% John Chen, Henry S. Thompson and Donald Walker.
% Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
% \bibliographystyle{acl_natbib}

\appendix
% \section{Templates of Data Used for Fine-Tuning}
\label{sec:templates}
\section{Selection of Seperator}
The separator token for T5 and mT5 was set to `` || '', as this symbol is rarely used in natural text, making it an ideal choice for separating different parts of the sequence. 
\section{Warmup Sequence for Summarization and Logical Reasoning}

The warmup sequences for summarization follow a similar pattern to those in translation, predominantly consisting of either Direct Core Phrases or Similar Phrases. For logical reasoning, the warmup sequence is identical to the target sequence, representing only the final letter choice that indicates the predicted answer.



% \section{Example Appendix}
% \label{sec:appendix}

% This is an appendix.

\end{document}

