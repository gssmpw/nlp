
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage[pdftex,colorlinks,linkcolor=gray,citecolor=gray,filecolor=gray,urlcolor=gray]{hyperref}\usepackage{url}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%%% TAL ADDED %%%

%\usepackage{hyperref}       % hyperlinks
%\usepackage[pdftex,colorlinks,linkcolor=blue,citecolor=blue,filecolor=blue,urlcolor=blue]{hyperref}
%\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
%\usepackage[dvipsnames]{xcolor}         % colors

\usepackage{tikz}
\usepackage{graphicx}
\usepackage{nicefrac}
\usepackage{aliascnt}
%\usepackage{amssymb,amsmath,amsthm,sectsty,url}
\usepackage{amssymb,amsmath,amsthm,url}
\usepackage{cleveref}
\usepackage{color}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{multicol}
\usepackage{subcaption}

% Numbering equations inside align*
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}


% ----------- algorithm tweaking -----------
% (From Sacha)
%\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
%\SetCommentSty{mycommfont}
%
\SetKwInput{KwInput}{input}
\SetKwInput{KwOutput}{Output}
\SetKwInput{KwReturn}{return}
\SetKwInput{KwPublish}{publish}
\SetKwFunction{procadd}{Add}
\SetKwFunction{proccount}{Count}
\SetKwProg{myinit}{Global initialization}{}{}
\SetKwProg{myrandomizer}{Randomizer}{}{}
\SetKwProg{myanalyzer}{Analyzer}{}{}
\SetKwProg{myquery}{KDE Query}{}{}


%Environments
\newtheorem{theorem}{Theorem}[section]
\crefname{theorem}{Theorem}{Theorems}

\newaliascnt{lemma}{theorem}
\newtheorem{lemma}[lemma]{Lemma}
\aliascntresetthe{lemma}
\crefname{lemma}{Lemma}{Lemmas}

\newaliascnt{proposition}{theorem}
\newtheorem{proposition}[proposition]{Proposition}
\aliascntresetthe{proposition}
\crefname{proposition}{Proposition}{Propositions}

\newaliascnt{corollary}{theorem}
\newtheorem{corollary}[corollary]{Corollary}
\aliascntresetthe{corollary}
\crefname{corollary}{Corollary}{Corollaries}

\newaliascnt{fact}{theorem}
\newtheorem{fact}[fact]{Fact}
\aliascntresetthe{fact}
\crefname{fact}{Fact}{Facts}

\newaliascnt{definition}{theorem}
\newtheorem{definition}[definition]{Definition}
\aliascntresetthe{definition}
\crefname{definition}{Definition}{Definitions}

\newaliascnt{remark}{theorem}
\newtheorem{remark}[remark]{Remark}
\aliascntresetthe{remark}
\crefname{remark}{Remark}{Remarks}

\newaliascnt{conjecture}{theorem}
\newtheorem{conjecture}[conjecture]{Conjecture}
\aliascntresetthe{conjecture}
\crefname{conjecture}{Conjecture}{Conjectures}

\newaliascnt{claim}{theorem}
\newtheorem{claim}[claim]{Claim}
\aliascntresetthe{claim}
\crefname{claim}{Claim}{Claims}

\newaliascnt{question}{theorem}
\newtheorem{question}[question]{Question}
\aliascntresetthe{question}
\crefname{question}{Question}{Questions}

\newaliascnt{exercise}{theorem}
\newtheorem{exercise}[exercise]{Exercise}
\aliascntresetthe{exercise}
\crefname{exercise}{Exercise}{Exercises}

\newaliascnt{example}{theorem}
\newtheorem{example}[example]{Example}
\aliascntresetthe{example}
\crefname{example}{Example}{Examples}

\newaliascnt{notation}{theorem}
\newtheorem{notation}[notation]{Notation}
\aliascntresetthe{notation}
\crefname{notation}{Notation}{Notations}

\newaliascnt{problem}{theorem}
\newtheorem{problem}[problem]{Problem}
\aliascntresetthe{problem}
\crefname{problem}{Problem}{Problems}

\newcommand{\aref}[1]{\autoref{#1}}

%General
\newcommand{\abs}[1]{\left|#1\right|} 
\newcommand{\norm}[1]{\lVert#1\rVert}

%Probability
\def\E{\mathbb E}
\newcommand{\Exp}{\mathop{\mathrm E}\displaylimits} % expectation
%\newcommand{\Var}{{\bf Var}}
%\newcommand{\Cov}{{\bf Cov}}
%\newcommand{\1}{{\rm 1\hspace*{-0.4ex}%
%\rule{0.1ex}{1.52ex}\hspace*{0.2ex}}}

%Algebraic Structures
\newcommand{\U}{\mathbf U}
\newcommand{\N}{\mathbb N}
%\newcommand{\R}{\mathbb R}
\newcommand{\Z}{\mathbb Z}
\newcommand{\C}{\mathbb C}
\newcommand{\F}{\mathbb F}
\newcommand{\GF}{\mathbb{GF}}
\newcommand{\B}{\{ 0,1 \}}
\newcommand{\BM}{\{ -1,1 \}}

% Misc
\newcommand\polylog[1]{\ensuremath{\mathrm{polylog}\left(#1\right)}}
%\newcommand\argmin{\ensuremath{\mathrm{argmin}}}
\newcommand\fsnorm[1]{\ensuremath{\norm{#1}_F^2}} % Square-Frobenius norm
%\newcommand{\1}{\mathbf1}
% Our algorithms
\newcommand{\kk}{\ensuremath{\mathrm{\mathbf{k}}}}


%%%%%%

\newenvironment{CompactEnumerate}{
\begin{list}{\arabic{enumi}.}{%
\usecounter{enumi}
\setlength{\leftmargin}{4pt}
\setlength{\itemindent}{1pt}
\setlength{\topsep}{-1pt}
\setlength{\itemsep}{-2pt}
}}
{\end{list}}

\newenvironment{CompactItemize}{
\begin{list}{\tiny$\bullet$}{%
\setlength{\leftmargin}{10pt}
\setlength{\itemindent}{0pt}
\setlength{\topsep}{-1pt}
\setlength{\itemsep}{0pt}
}}
{\end{list}}

\definecolor{goodgreen}{RGB}{74,146,59}
\definecolor{badred}{RGB}{196,46,34}

\usepackage{makecell}
\usepackage{chngpage}



%\title{Learning from End User Data with \\ Shuffled
%Differential Privacy}
\title{Learning from End User Data with Shuffled
Differential Privacy over Kernel Densities}

%\title{Distributed Densities with Shuffled Differential Privacy}

%\title{Private Classification and Class Decoding from Shuffled Distributed Densities}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Tal Wagner\\ %\thanks{Also with Amazon. This work is not associated with Amazon.} \\
  The Blavatnik School of Computer Science and AI\\
  Tel-Aviv University\\
  \texttt{tal.wagner@gmail.com} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle


\begin{abstract}
We study a setting of collecting and learning from private data distributed across end users.
In the \emph{shuffled} model of differential privacy, the end users partially protect their data locally before sharing it, and their data is also anonymized during its collection to enhance privacy. 
This model has recently become a prominent alternative to central DP, which requires full trust in a central data curator, and local DP, where fully local data protection takes a steep toll on downstream accuracy. 


Our main technical result is a shuffled DP protocol for privately estimating the kernel density function of a distributed dataset, with accuracy essentially matching central DP. 
We use it to privately learn a classifier from the end user data, by learning a private density function per class. 
Moreover, we show that the density function itself can recover the semantic content of its class, despite having been learned in the absence of any unprotected data. 
Our experiments show the favorable downstream performance of our approach, and highlight key downstream considerations and trade-offs in a practical ML deployment of shuffled DP. 
\end{abstract}


\section{Introduction}\label{sec:intro}
Collecting statistics on end user data is commonly required in data analytics and machine learning. As it could leak private user information, privacy guarantees need to be incorporated into the data collection pipeline. Differential Privacy (DP) \citep{dwork2006calibrating} currently serves as the gold standard for privacy in machine learning. Most of its success has been in the \emph{central} DP model, where a centralized data curator holds the private data of all the users and is charged with protecting their privacy. However, this model does not address how to collect the data from end users in the first place. 
The \emph{local} DP model \citep{kasiviswanathan2011can}, where end users protect the privacy of their data locally before sharing it, is often used for private data collection \citep{erlingsson2014rappor,ding2017collecting,apple2017learning}. However, compared to central DP, local DP often comes at a steep price of degraded accuracy in downstream uses of the collected data.

%with the centralized party.


%requires the users to entrust a central authority with their sensitive data. 
%The classical alternative is \emph{local} DP \citep{kasiviswanathan2011can}, where users protect the privacy of their data locally before sharing it, obviating the need to trust a central authority. However, this often comes at a steep price of degraded accuracy in downstream uses of the privatized data.

%Protecting the privacy of user data in a training set is among the most prominent challenges in real-world deployment of machine learning. 
%Differential Privacy (DP) \citep{dwork2006calibrating} currently serves as the gold standard for privacy in machine learning, due to its capacity to combine accuracy, efficiency and rigorous privacy in many cases. 
%Most of its success has been in the 
%\emph{central} DP model, where a single data curator holds the private data of all the users and is charged with protecting their privacy. However, this model requires the users to entrust a central authority with their sensitive data. 
%The classical alternative is \emph{local} DP \citep{kasiviswanathan2011can}, where users protect the privacy of their data locally before sharing it, obviating the need to trust a central authority. However, this often comes at a steep price of degraded accuracy in downstream uses of the privatized data.

The \emph{shuffled} DP model \citep{bittau2017prochlo,cheu2019distributed,erlingsson2019amplification} has recently emerged as a prominent intermediate alternative. 
In this model, the users partially protect their data locally, and then entrust a centralized authority---called the ``shuffler''---with the single operation of shuffling (or anonymizing) the data from all participating users. Data collection protocols in this model are designed so that the composition of shuffling over local user computations rigorously ensures DP. 
The appeal of shuffled DP lies in the convergence of theoretical and practical properties:  
mathematically, recent work has proved that shuffling can boost the accuracy of local DP to levels that may reach those of central DP \citep{erlingsson2019amplification,balle2019privacy,balle2020privacy,koskela2021tight,girgis2021renyi,feldman2022hiding,feldman2023stronger,zhou2022power}.  
At the same time, the strictly limited functionality of the shuffler lends itself to realistic secure implementations, and a trusted shuffler can be implemented using techniques from secure computation and cryptography, like mixnets, onion routing, trusted execution environments (TEEs), and secure aggregation (SecAgg) \citep{ishai2006cryptography,bittau2017prochlo,gordon2022spreading,kairouz2021distributed,kairouz2021advances}. 
 

There is by now a well-developed body of work on basic operations under shuffled DP, primarily summation (see \Cref{sec:prelim_bitsum}). 
Work on machine learning has mostly focused on iterative settings (see \Cref{sec:related}), where distributed parties contribute local computations on their sensitive data, like gradients, over multiple rounds of shuffled DP communication. 
This is compatible with \emph{distributed (or federated) training} scenarios, in which a known set of parties collaborate in a training process that unfolds over time, typically with each contributing a local dataset and local computational resources (say, for computing local gradients). For example, the parties could be local branches of a large corporation (e.g., a bank), each holding the private data of multiple local customers.

Unfortunately, this is incompatible with \emph{data collection} scenarios, where a ``snapshot'' of user data is collected in one shot from a pool of uncommitted users who hold a single training point (their own private data), which they may opt in or out of sharing, and who do not participate computationally in the training process beyond possibly contributing their data. For example, the users could be end customers of a smartphone app, prompted to privately share statistics about their app activity.

In this work, we study the \emph{data collection} scenario under shuffled DP. We propose a private learning approach which can intuitively be seen as a shuffled DP analog of a nearest neighbor (kNN) classifier. 
A distributed training set of sensitive labeled data is privately collected from users, and like in kNN, subsequent test points are classified according to the most similar training examples. Since using a small number of neighbors in classification may violate their privacy, our classifier uses kernel density estimation (KDE) as a ``smooth'' alternative to kNN, which can be realized with shuffled DP. 
%By building on recent advances in binary summation under shuffled DP and in KDE under central DP, 
It thus labels test points as the class where their privately estimated density is maximized. 
%we obtain a shuffled DP protocol for KDE. Test points can then be labeled according to the class where their privately estimated density is maximized.

%In this work, we study an approach to private distributed learning, which intuitively can be seen as a shuffled DP analog of a nearest neighbor (kNN) classifier. 
%Unlike prior iterative approaches (see \Cref{sec:related}), in our setting, 
%A distributed training set of sensitive labeled data is privately collected from users once, and subsequent test points are classified according to the most similar training examples. Since classifying according to a small number of near neighbors is incompatible with privacy, the classifier uses kernel density estimation (KDE) as a ``smooth'' alternative to kNN. 
%By utilizing recent advances on KDE in central DP and on binary summation in shuffled DP, we obtain a shuffled DP protocol for KDE. Test points can then be labeled according to the class where their privately estimated density is maximized. 

Moreover, our classifier produces a function representation of each class. We show this representation can be used to recover the semantics of the class---for example, a list of terms that captures the topic of a class in textual data---even though the learner did not observe any unprotected text record from the class before privacy was imposed. We refer to this as \emph{private class decoding}. 

\noindent\textbf{Our results.}
Formally, we consider the following learning setting. Training data is distributed across $n$ users, each holding a single private training point $(x,c)$, where $x\in\R^d$ is a feature vector and $c\in[m]$ is a class label. 
The learner collects data from the users through shuffled DP, and uses them to construct a classifier, which can then be used to classify feature vectors $y\in\R^d$ from an unlabeled test set. The classifier itself needs to be private w.r.t.~the collected dataset; %(whereas the test queries $y$ are available to the analyzer in the clear, and need not be protected). 
this enables labeling an unbounded number of test points without additional loss of privacy. 
%See \Cref{sec:prelim} for formal definitions.

To address this setting, 
our main theoretical result is a shuffled DP protocol for KDE estimation, that learns a private KDE function from distributed user data, which can then be used to estimate densities of test points. The utility guarantee is given in terms of the supremum mean squared error over all test points in $\R^d$, so that test points need not be known to the protocol in advance. The proof goes through a reduction to binary summation (abbrev.~\emph{bitsum}), which is among the most well-studied problems in shuffled DP, with a variety of available protocols to employ. % optimizing for different desiderata.  Our KDE protocol can use any of these bitsum protocols internally, directly capitalizing on that extensive body of work. 
%In contrast, most prior work on machine learning with shuffled DP (see \Cref{sec:related}) had to develop specialized ad-hoc protocols to handle the  specific learning settings. 

Experimentally, we evaluate our method with various combinations of kernels and bitsum protocols, yielding different trade-offs between privacy, accuracy and communication, and highlighting key downstream considerations for shuffled DP compared to central DP and local DP baselines. 

%Our experiments show that our shuffled DP KDE protocol leads to private and accurate classification and class decoding. 
%Our learner learns a private KDE function per class, and labels new test points as the class that maximizes their approximated density. 
%This approach to learning with shuffled DP is markedly different than previous ones, which have been iterative, see a discussion of prior work \Cref{sec:related}. 
%We conduct extensive experiments with 
%We evaluate
%different combinations of kernels and bitsum protocols, and show that their theoretical properties and trade-offs transfer well to empirical downstream performance. 

%\noindent\textbf{Summary of contributions.}
%Our contributions can be summarized as follows:
%\begin{CompactItemize}
%    \item Theoretically, we formulate the KDE problem in shuffled DP (\Cref{def:shufdpkde}), and present a solution for notable families of kernels (\Cref{thm:main}), including the Gaussian kernel (\Cref{thm:gaussian}). 
%    \item We show how to use this KDE primitive for private classification and class decoding from distributed user data, in what can be seen as a kNN classifier analog in the shuffled DP model. %shuffled DP analog of a kNN classifier. %, and validate the efficacy of the approach on real data. 
%    \item Experimentally, we validate the effectiveness of our approach on real data, presenting a comprehensive empirical study with various combinations of kernels and bitsum protocols.  
    %\item Our experiments also include comprehensive implementation, evaluation and comparison of various shuffled DP bitsum protocols on downstream learning tasks. %, complementing prior work that has evaluated their additive error directly for the summation problem.
%\end{CompactItemize}
%Along the way, our experiments also include comprehensive implementation, evaluation and comparison of various shuffled DP bitsum protocols on downstream learning tasks. As most prior work on them has been either theoretical or studied them , thus empirical study may be of independent interest. 

%Prior work on these protocols has been mostly theoretical, or simulated and measured their noise without data.

%Notably, even though we implement them with the same hidden constants from the proofs, which are typically not optimized but rather chosen to just be ``good enough'' (e.g., sufficiently small or sufficiently large) for the asymptotic analysis to go through, they still work great.

%Training data is distributed across $n$ users, each a single pair $(x_i,c_i)$, considered as that user's private data, where $x\in\R^d$ is a feature vector and $c\in[m]$ is a class label. In a shuffled DP protocol, the users send messages containing noisy versions of their private data to a central \emph{shuffler}, who removes the sender identities from those messages and forwards them to the \emph{analyzer}. The analyzer uses those anonymized noisy messages to construct a classifier, which can then be used to classify feature vectors $y\in\R^d$ from an unlabeled test set. The classifier itself needs to be private w.r.t.~the user data $\{(x_i,c_i)\}$; %(whereas the test queries $y$ are available to the analyzer in the clear, and need not be protected). 
%note that this enables labeling an unbounded number of queries without additional loss of privacy. 
%See \Cref{sec:prelim} for formal definitions. 

%\vspace{-5pt}
\section{Background and Preliminaries}\label{sec:prelim}
%\vspace{-5pt}
\subsection{Central, Local and Shuffled DP}\label{sec:prelim_shuffle}
We review models of differential privacy.
Let $\mathcal X$ be a universe of data elements. A \emph{dataset} is an $n$-tuple $X\in\mathcal X^n$. 
Two datasets $X,X'$ are called \emph{neighboring} if they differ on at most one coordinate. A randomized algorithm $M$, that maps an input dataset to an output from a range of outputs $\mathcal T$, is $(\varepsilon,\delta)$-DP if for every pair of neighboring datasets $X,X'$ and every $T\subset\mathcal T$, it satisfies
\begin{equation}\label{eq:dp}
    \Pr[M(X)\in T] \leq e^\varepsilon\cdot\Pr[M(X')\in T] + \delta.
\end{equation}

In \emph{central} DP, a single data curator holds a dataset $X\in\mathcal X^n$ containing the data of $n$ users, with each coordinate in the $n$-tuple $X$ representing a user. The curator runs $M$ and releases its output. 

In \emph{local} DP, each user holds her own data element, on which she runs $M$ locally, and releases its output. Here, $M$ operates on a single data element (or $1$-tuple), and needs to satisfy \cref{eq:dp} for every $X,X'\in\mathcal X$ (every pair of single elements is neighboring). A central \emph{analyzer} collects the already ``privatized'' outputs from all users and performs an aggregate computation. 
In this model, there is no trusted central party at all, yielding a stronger form of privacy, albeit at the cost of accuracy. 


%This is a stronger form of privacy than central DP, since users never trust their raw data with another party. However, its formal privacy/accuracy trade-offs are typically much weaker (e.g., \cite{duchi2013local,kairouz2014extremal,kairouz2016discrete,bassily2017practical}).
%than the central DP model.% (e.g., \cite{duchi2013local,kairouz2014extremal,kairouz2016discrete,bassily2017practical}).

The \emph{shuffled} DP model \citep{bittau2017prochlo,cheu2019distributed,erlingsson2019amplification} bridges the central and local DP models, by introducing a \emph{limited} trusted central party---called a ``shuffler''---whose only function is to anonymize (or randomly permute) the users' outputs before they are shown to the analyzer. The analyzer is considered untrusted, simialrly to local DP and unlike central DP. Formally, a shuffled DP protocol $\Pi$ consists of three randomized algorithms $\Pi=(\Pi_R,\Pi_S,\Pi_A)$:
\begin{CompactItemize}
    \item \emph{Randomizer} $\Pi_R$, which maps a single element from $X$ to some sequence of messages. Each user runs $\Pi_R$ locally on her data element, and forwards the output messages to the shuffler.
    \item \emph{Shuffler} $\Pi_S$, which collects the messages from all users and forwards them to the analyzer in a uniformly random order (thus, intuitively, removing sender identities).
    \item \emph{Analyzer} $\Pi_A$, which receives the permuted messages from the shuffler and outputs the result of an aggregate computation. 
\end{CompactItemize}
The protocol is $(\varepsilon,\delta)$-DP in the shuffled DP model if the output of $\Pi_S$ satisfies \cref{eq:dp} (i.e., it holds with $M(X):=\Pi_S(\cup_{i=1}^n\Pi_R(X_i))$). Due to the DP post-processing property \citep{dwork2014algorithmic}, the output of $\Pi_A$ is $(\varepsilon,\delta)$-DP as well.
%Note that if the shuffler is skipped, we again obtain the local DP model.
%As in prior work \cite{kairouz2021distributed,ghazi2019private,wang2020d2p}, we assume that 
The parties in the protocol also have access to a source of shared public randomness, which is considered publicly known, and thus cannot be exploited to compromise privacy (see \citet{kairouz2021distributed}).

%The shuffled DP model has become popular thanks to the convergence of theoretical and practical advantages. On the theory side, it has been proven \cite{erlingsson2019amplification,balle2019privacy,girgis2021renyi,feldman2022hiding,zhou2022power} that the introduction of the shuffler asymptotically amplifies the privacy/accuracy trade-off of every local DP protocol, in some cases nearly matching that of the central DP model. Thus, the shuffling step is strongly theorerically grounded. 
%On the practical side, a trusted shuffler can be securely implemented using various techniques in secure computation and cryptography, like mixnets, onion routing, trusted execution environments (TEEs), and secure aggregation (SecAgg); see the discussions in \cite{ishai2006cryptography,bittau2017prochlo,gordon2022spreading,kairouz2021distributed,kairouz2021advances}.  

%\noindent\textbf{Honesty limitation.}
%A noteworthy limitation of the shuffled DP model, compared to local DP, is that its privacy relies on all $n$ users being honest, i.e., correctly executing the protocol on their data. 
%Namely, the local randomizer $\Pi_R$ generally relies on the number of participating users $n$ (e.g., the larger $n$ is, the less noise $\Pi_R$ adds locally). Thus, if some users drop out, the protocol would fail to meet its target privacy guarantee. 
%In \cite{cheu2019distributed,kairouz2021distributed} this is partially addressed by observing that dishonest users cannot harm the privacy guaranteed by a ``coalition'' of honest users in a shuffled DP protocol. Thus, its privacy guarantees still hold with $n$ being a lower bound on the number of honest users. 


%\noindent\textbf{Honesty limitation.}
%A noteworthy limitation of the shuffled DP model, compared to local DP, is that its privacy guarantees rely on all $n$ users being honest, i.e., correctly executing the protocol on their data. 
%Namely, the local randomizer $\Pi_R$ generally relies on knowing the number of participating users $n$ (say, the larger $n$ is, the less noise $\Pi_R$ adds locally), and this is essential for gaining in accuracy over local DP. Thus, if some users drop out, the protocol would fail to meet its target privacy guarantee. 
%In \cite{cheu2019distributed,kairouz2021distributed} this is partially addressed by observing that dishonest users cannot harm the privacy guaranteed by a ``coalition'' of honest users in a shuffled DP protocol. Thus, its privacy guarantees still hold with $n$ being a lower bound on the guaranteed number of honest users. 
%, and degrade gracefully as this number decreases. 


%Cheu et al.~\cite{cheu2019distributed} partially address this by observing that while dishonest users can destroy the accuracy of a shuffled DP protocol, they cannot harm the privacy guaranteed by a ``coalition'' of honest users. Thus, in settings where dishonest users may be present, the privacy guarantees of a shuffled DP protocol hold with $n$ being a lower bound on the guaranteed number of honest users. 

%\noindent\textbf{Shared public randomness.}
%As in prior work \cite{kairouz2021distributed,ghazi2019private,wang2020d2p}, we assume that the parties in the protocol have access to a source of shared public randomness. 
%The randomness is considered publicly known, and thus cannot be exploited to compromise the privacy of the protocol (cf.~the discussion in  \cite{kairouz2021distributed}).

%Since (in either case) it is considered public, it cannot be exploited to compromise the privacy of the protocol (cf.~the discussion in  \cite{kairouz2021distributed}).

\subsection{Bitsum Protocols in the Shuffled DP Model}\label{sec:prelim_bitsum}
Binary summation, which we refer to throughout as \emph{\textbf{bitsum}}, is a fundamental and well-studied problem in DP, and particularly in shuffled DP. Each of $n$ users holds a private bit $X_i\in\{0,1\}$, and the goal is to compute a DP estimate of the sum $S=\sum_{i=1}^nX_i$. 
The accuracy of a randomized estimate $\widetilde S$ is often quantified by its root mean squared error (RMSE), $(\E[(\widetilde S-S)^2])^{1/2}$. 
 
 %The root mean squared error (RMSE) of a randomized estimate $\widetilde S$ is $(\E[(\widetilde S-S)^2])^{1/2}$. 


A long line of work on shuffled DP bitsums  \citep{cheu2019distributed,cheu2021pure,ghazi2020private,ghazi2020pure,ghazi2021differentially,ghazi2023pure} had yielded protocols whose RMSE essentially matches central DP, and is significantly better than local DP, along with additional desirable properties, like low communication and pure DP (i.e., $\delta=0$). 
%
We will use these protocols as black-boxes and not require familiarity with their details. For completeness and intuition, we describe how they work in \Cref{sec:bitsumappendix}. 
%
%One bitsum protocol is the classical \emph{randomized response} \citep{warner1965randomized}: each user flips her bit with some probability and sends it to the shuffler. While originally introduced for local DP, \cite{cheu2019distributed}
% showed that in the shuffled DP model, the flip probability can be significantly smaller, leading to much better accuracy. 
%Even though its error is not optimal, this protocol has the advantages of a formal local DP guarantee (with a larger $\varepsilon$) on top of the shuffled DP guarantee, forming another layer of privacy, and of optimal communication, with each user communicating a single bit. 
%
%Another popular technique for shuffled DP bitsums is \emph{noise divisibility} \citep{goryczka2015comprehensive,balle2019privacy,balle2020private,ghazi2020private,ghazi2020pure,ghazi2021differentially,kairouz2021distributed}: each user $i$ locally adds noise $\nu_i$, sampled from a distribution carefully chosen so that the aggregate noise $\sum_i\nu_i$ from all users, after shuffling, is distributed in a way that ensures central DP (e.g., $\sum_i\nu_i$ could be distributed like a discrete Laplace). 
%Thus, the shuffled DP protocol simulates central DP, by having each user contribute a piece of the total ``divisible'' requisite noise.
%by rendering the total requisite noise ``divisible'' across the users. 
%Ensuring the requisite 
%Rigorously arguiging about noise aggregation through shuffling can be done either by using a cryptographic result of \cite{ishai2006cryptography}, as in \cite{balle2020private}, or by using integral noise to send a random number of identical messages, as in \cite{ghazi2020private,ghazi2020pure,ghazi2023pure}.

%This line of work on shuffled DP bitsums has been either theoretical, or in some cases empirically evaluated the \cite{ghazi2020private,balle2020private,ghazi2021differentially}


%Our reduction from KDE to bitsums (\Cref{thm:main}) can work with real summation as well; see \Cref{sec:bitvsreal}.
%. We discuss the merits of opting for bitsum protocols in \Cref{sec:bitvsreal}.

%$\sqrt{\E[(\widetilde S-S)^2]}$. 

\subsection{Private Kernel Density Estimation}\label{sec:prelim_lsq}
Let $\kk:\R^d\times\R^d\rightarrow\R$ be a kernel, such as the Gaussian kernel $\kk(x,y)=\exp(-\norm{x-y}_2^2)$. The kernel density estimation (KDE) map $KDE_X:\R^d\rightarrow\R$, associated with a multiset $X\subset\R^d$ of size $n$, is defined as $KDE_X(y)=\frac1n\sum_{x\in X}\kk(x,y)$. %\citep{shawe2004kernel,hofmann2008kernel}.

Numerous works studied KDE in the central DP model \citep{hall2013differential,wang2016differentially,alda2017bernstein,coleman2020one,wagner2023fast,backurs2024efficiently}, mostly in a setting known as \emph{function release}. In this setting, the data curator holds all of $X$, and her goal is to release a function description $\widetilde K(\cdot)$ which is DP w.r.t.~$X$, such that $\widetilde K(y)$ is an accurate estimate of $KDE_X(y)$ for every $y\in\R^d$. 
%
We will adapt this problem to the shuffled DP model in \Cref{def:shufdpkde}.
Our approach to this problem will use the following notion of \emph{locality-sensitive quantization} (LSQ), recently introduced in \citet{wagner2023fast} for KDE in the central DP model. 

\begin{definition}[\citet{wagner2023fast}]\label{def:lsq}
    Let $Q,R,S,\beta>0$. Let $\mathcal Q$ be a distribution over pairs of functions $f,g:\R^d\rightarrow[-R,R]^Q$. We say that $\mathcal Q$ is a \emph{$\beta$-approximate $(Q,R,S)$-locality sensitive quantization (abbrev.~LSQ) family} for the kernel $\kk$, if the following % two properties 
    are satisfied for all $x,y\in\R^d$:
    \begin{CompactItemize}
        \item $|\kk(x,y) - \E_{(f,g)\sim\mathcal Q}[f(x)^Tg(y)]|\leq\beta$.
        \item $f(x)$ and $g(y)$ have each at most $S$ non-zero coordinates.
    \end{CompactItemize}
If this holds, then $\kk$ is \emph{$\beta$-approximate $(Q,R,S)$-LSQable}. If $\beta=0$, then $\kk$ is    \emph{$(Q,R,S)$-LSQable}. 
\end{definition}
%For example, \cite{wagner2023fast} observe that the Gaussian kernel is LSQable with $Q,R,S=O(1)$ by random Fourier features \cite{rahimi2007random}.
For example, \citet{wagner2023fast} observed that the Gaussian kernel is $(1,\sqrt2,1)$-LSQable by random Fourier features \citep{rahimi2007random}, and the Laplacian and exponential kernels are $\beta$-approximate $(O(\beta^{-1}),1,1)$-LSQable for all $\beta>0$ by locality sensitive hashing \citep{indyk1998approximate}. 
%Loosely speaking, t
They proved that LSQable kernels admit efficient KDE mechanisms in the central DP model. We will prove an analogous result for shuffled DP. %, provided appropriate bitsum protocols. 
While we draw on ideas from their central DP mechanism, our proofs will be different and self-contained.

\subsection{Additional Related Work}\label{sec:related}
Prior work on machine learning with shuffled DP has mostly focused on two iterative learning settings: distributed and federated model training \citep{cheu2021shuffle,girgis2021shuffled,NEURIPS2021_f44ec26e,liu2021flame,kairouz2021distributed}, where users share privately computed gradients; and multi-armed and contextual bandits \citep{tenenbaum2021differentially,chowdhury2022shuffle,zhou2023differentially,tenenbaum2023concurrent}, where users share private contexts and rewards. 
The main difference from our setting is their iterative nature, % of these learning settings, 
in which the users communicate with the analyzer over multiple rounds of a shuffled DP protocol. 
Of these, \citet{tenenbaum2021differentially} is somewhat akin to us in that they also reduce their problem to bitsums, although in their case the connection is more direct as they assume binary rewards in their multi-armed bandits problem. 
%
%\cite{kairouz2021distributed} notably include a thorough discussion of realistic considerations of shuffled DP and shuffler implementation, with ramifications for discretization and communication. While their setting is different, their discussion informs our design choices as well. 

Beyond bitsums, there has been much work on shuffled DP protocols for integer and real summation \citep{cheu2019distributed,cheu2022differentially,balle2019privacy,balle2019improved,balle2020private,ghazi2020private,ghazi2020pure,ghazi2021differentially,balcer2021connecting} and other basic operations \citep{balcer2019separating,ghazi2019private,ghazi2021power,chen2020distributed,chang2021locally,scott2021applying,tenenbaum2023concurrent}. 
We discuss real summation in the context of our work in more detail in \Cref{sec:bitvsreal}.

Outside shuffled DP, a relevant work in the central DP model is \citet{backurs2024efficiently}, who also suggested a classifier that maximizes the privately computed similarity to a class. They presented results for the CIFAR-10 dataset by measuring distances to class means. Our experiments in \Cref{sec:experiments} include the same data in a distributed setup, evaluated with our shuffled DP protocol. 

%\cite{kairouz2021distributed} include a thorough discussion of the realistic implementation of shuffled DP, and its ramifications on discretization and communication, which informs our work as well. 
%They study federated learning, where users cooperate on training a model by locally computing private gradients and sharing them with a central learner via shuffled DP. 
%They base their protocol on a noise divisibility result for discrete Gaussians  \cite{canonne2020discrete} combined with clipping, flattening and discretization to render gradients efficiently and accurately communicable with shuffled DP.

%Furthermore, they mostly design ad-hoc shuffled DP protocols for their settings rather than using 

%cannot directly use fundamental results in shuffled DP (like bitsums) and instead design ad-hoc protocols. One exception is \cite{tenenbaum2021differentially}, who like us reduce to generic binary summation, though their connection is more direct as they assume binary rewards in their multi-armed bandits problem. 

%These issues are made particularly explicit in  \cite{kairouz2021distributed}, who study a federated learning setting, where users cooperate on model training by locally computing gradients on their private data and sharing them with a central learner via shuffled DP. 
%\cite{kairouz2021distributed} include a thorough discussion on the reaslitic deployments of shuffled DP, and how they affect discretization and communication. They develop a protocol based on a noise divisibility result for discrete Gaussian noise, and combine with clipping, flattening and discretizing the user gradients to render them efficiently and accurately communicable with shuffled DP. 




%sandeep \cite{backurs2024efficiently}
%thomas FL \cite{kairouz2021distributed}, predecessors \cite{agarwal2018cpsgd,girgis2021shuffled,truex2019hybrid}, dpsgd \cite{song2013stochastic,abadi2016deep}

%Work on its adaption to machine learning so far has focused on iterative settings of distributed and federated optimization \cite{cheu2021shuffle,girgis2021shuffled,NEURIPS2021_f44ec26e,liu2021flame,kairouz2021distributed} and on multi-armed and contextual bandits \cite{tenenbaum2021differentially,chowdhury2022shuffle,zhou2023differentially,tenenbaum2023concurrent}. Due to the rather involved nature of these settings, they necessitated the design of specialized and ad-hoc shuffled DP protocols, and 


%\section{Privacy Preserving Classifier from Distributed User Data}
%\section{Similarity-Based Classifier for Private Distributed User Data}
\section{Data Collection and Classification with Shuffled DP}
In this section we present our private data collection and learning protocol. In \Cref{sec:shuffled_dp_considerations}, we discuss certain practical considerations with shuffled DP, that would inform the design of our method. In \Cref{sec:shufdpkde}, we give our shuffled DP result for KDE, which is the main building block in our classifier. In \Cref{sec:hdc}, we use it to privately learn a classifier from collected user data.



%The main building block is a KDE protocol in the shuffle DP model, given in \Cref{sec:shufdpkde}. In \Cref{sec:hdc}, we use it to privately learn a classifier.

\subsection{Practical Considerations with Shuffled DP}\label{sec:shuffled_dp_considerations}


\noindent\textbf{User counts.}
A key limitation of shuffled DP is that protocols are required to know in advance the number of participating users $n$. Technically, the noise added by each local randomizer $\Pi_R$ generally decreases with $n$. This is crucial for boosted accuracy, albeit if some users drop out, the protocol fails to meet its DP guarantee for the remaining users. This limitation may be acceptable in the \emph{distributed training} scenario from \Cref{sec:intro}, where a predetermined group of parties is expected to collaborate on training and reliably execute the protocol. However, in our \emph{data collection} scenario, it would make less sense to assume that the number of participating users is known in advance. We will therefore designate a preliminary communication round for allowing users to opt into participation.

%To handle this, we will use a preliminary communication where users opt into the shuffled DP data collection protocol. To avoid a chicken-and-egg problem, this round would use local DP. While local DP has decreased accuracy, it is sufficiently accurate for only privately computing user counts.  

%In \citet{cheu2019distributed,kairouz2021distributed} this is partially addressed by observing that dishonest users cannot harm the privacy guaranteed by a ``coalition'' of honest users in a shuffled DP protocol. Thus, its privacy guarantees still hold with $n$ being a lower bound on the guaranteed number of honest users. 
%, and degrade gracefully as this number decreases.

%\noindent\textbf{Honesty limitation.}
%A noteworthy limitation of the shuffled DP model, compared to local DP, is that its privacy guarantees rely on all $n$ users being honest, i.e., correctly executing the protocol on their data. 
%Namely, the local randomizer $\Pi_R$ generally relies on knowing the number of participating users $n$ (say, the larger $n$ is, the less noise $\Pi_R$ adds locally), and this is essential for gaining in accuracy over local DP. Thus, if some users drop out, the protocol would fail to meet its target privacy guarantee. 
%In \cite{cheu2019distributed,kairouz2021distributed} this is partially addressed by observing that dishonest users cannot harm the privacy guaranteed by a ``coalition'' of honest users in a shuffled DP protocol. Thus, its privacy guarantees still hold with $n$ being a lower bound on the guaranteed number of honest users. 
%, and degrade gracefully as this number decreases. 

\noindent\textbf{Privacy threat models.} 
There are several possible places to impose DP in an ML pipeline. \citet{ponomareva2023dp} outline three options, from the most stringent to most lenient form of privacy: \emph{input/data-level DP}, where the adversary has access to the data used to train the ML model; \emph{model-level DP}, where the adversary has full access to the weights of the trained model; and \emph{prediction-level DP}, where the adversary has access only to model outputs when presented with test points. 

We will consider the first two of these options, adapted to shuffled DP. 
Input/data-level DP means the adversary sees all communication sent to the analyzer (equivalently, the analyzer itself is the adversary). Note that communication from the users to the shuffler is never exposed (that would void the premise of shuffled DP); in practice, this line of communication is implemented cryptographically, exploiting on the restricted nature of the shuffler \citep{kairouz2021distributed}. However, the adversary can see all communication between the shuffler and the analyzer, as well as all direct communication (if any) between the users and the analyzer. We refer to this as the \emph{communication-threat model}. 
In model-level DP, a weaker adversary sees only the trained model released by the analyzer after the protocol execution is complete; we refer to this as the \emph{model-threat model}. These threat models are typically not differentiated in prior work on shuffled DP, since they often coincide; however, in our case, the trained model would leak less privacy than the communication used to learn it. 

\noindent\textbf{Bit-width and discretization.} 
\citet{kairouz2021distributed} emphasize that in practice, the shuffler implementation often requires modular arithmetics for 
cryptographic secure aggregation. Therefore, the shuffled DP protocol's numerical values must be discretized, and its bit precision (called \emph{bit-width}) needs to be explicitly bounded. Neglecting to account for the bit-width may lead to impractical communication costs and to larger errors (due to discretization) than a real-valued analysis predicts.
We will therefore incorporate discretization into our protocol and account for it in the error analysis.

\begin{algorithm}[t]
 \caption{Shuffled DP KDE protocol from bitsums} 
\label{alg:shufdpkde}
 \begin{multicols}{2}
 \DontPrintSemicolon
  %\LinesNumbered
  %\KwOutput{Rank-$k$ approximation $\tilde A\in\R^{n\times d}$ of $A$.}
  \myinit{\textit{$\;\;$// all data here is public}}
  {
    \KwInput{shuffled DP bitsum protocol $\Pi$; $(Q,R,S)$-LSQ family $\mathcal Q$; integer $I>0$}
    \For{$i=1,\ldots,I$}{
      $(f_i,g_i)\leftarrow$ independent sample from $\mathcal Q$ \textit{$\;\;\;\;$// using shared/public randomness}\;
      \For{$j=1,\ldots,Q$}{
        $\Pi_{ij}\leftarrow$ independent instance of $\Pi$\;
      }
    }
    \KwPublish{$(f_i,g_i)$ for all $i=1,\ldots,I$}
  }
  \myrandomizer{\textit{$\;\;$// each user runs this locally with private randomness}}{
    \KwInput{private data point $x\in\R^d$}
     \For{$(i,j)\in[I]\times[Q]$}{
       %$p_{ij}\leftarrow 0.5(1+f_{ij}(x)/R)$\;
       %$b_{ij}\leftarrow 1$ w.p.~$p_{ij}$ and 0 w.p.~$1-p_{ij}$\; 
       $b_{ij}\leftarrow\mathrm{Bernoulli}(0.5(1+(f_i(x))_j/R))$\;
       $\Gamma_{ij}\leftarrow$ run the randomizer of $\Pi_{ij}$ on $b_{ij}$\;
       \For{message $\gamma$ in $\Gamma_{ij}$}{
         send $(\gamma, (i,j))$ to the shuffler 
       }
    }
  }
  \myanalyzer{\textit{$\;\;$// runs after the shuffler}}{
    \KwInput{shuffled sequence of messages $\widetilde\Gamma$ from $n$ users}
    %$\forall_{i,j}\;\;\widetilde\Gamma_{ij}\leftarrow$ empty sequence\;
    \For{$(i,j)\in[I]\times[Q]$}{
        $\widetilde\Gamma_{ij}\leftarrow$ empty sequence\;
    }
    \For{message $(\gamma,(i,j))$ in $\widetilde\Gamma$}{
        append $\gamma$ to $\widetilde\Gamma_{ij}$\;
    }
    \For{$(i,j)\in[I]\times[Q]$}{
        $\widetilde B_{ij}\leftarrow$ run the analyzer of $\Pi_{ij}$ on $\widetilde\Gamma_{ij}$\;
        $\widetilde F_{ij}\leftarrow (2\widetilde B_{ij}-n)R$\;
    }
    \KwPublish{$\widetilde F_{ij}$ for all $i,j$}
  }
  \vspace{2pt}
  \myquery{\textit{$\;\;$// runs on the analyzer's published output arbitrarily many times}}{
    \KwInput{query point $y\in\R^d$}
    \KwReturn{$\frac{1}{nI}\sum_{i=1}^I\sum_{j=1}^Q\widetilde F_{ij}\cdot(g_i(y))_j$}
  }
  \end{multicols}
\end{algorithm}




\subsection{Shuffled DP KDE from Bitsum Protocols}\label{sec:shufdpkde}

We now present the theoretical backbone of our private learning approach, a shuffled DP protocol for KDE. 
We start by defining the KDE problem in the shuffled DP model.

\begin{definition}[shuffled DP KDE]\label{def:shufdpkde}
In the shuffled DP KDE problem, a dataset $X\in(\R^d)^n$ of $n$ points in $\R^d$ is distributed across $n$ users, one point per user. The goal is to devise a shuffled DP protocol $\Pi_{\mathrm{KDE}}$ in which the analyzer releases a function description $\widetilde K(\cdot)$, required to be $(\varepsilon,\delta)$-DP w.r.t.~$X$. The supremum root mean square error (abbrev.~supRMSE) of the protocol is defined as 
\[ \mathrm{supRMSE}(\Pi_{\mathrm{KDE}}) := \sup_{y\in\R^d}\sqrt{\E\left[\left(\widetilde K(y) - KDE_X(y)\right)^2\right]} . \]
\end{definition}

Our main technical result is the following theorem, which is a reduction from KDE to bitsum protocols in the shuffled DP model, for kernels with the LSQ property defined in \Cref{def:lsq}. 
The resulting shuffled DP KDE protocol is given in Algorithm~\ref{alg:shufdpkde}. 
%The public shared randomness is used in a global initialization step, while the randomness used by the randomizers is private.
%We prove the theorem in \Cref{sec:mainproof}.



%\begin{theorem}\label{thm:main}
%Let $\kk$ be a $\beta$-approximate $(Q,R,S)$-LSQable kernel (cf.~\Cref{def:lsq}). 
%Suppose we have an $(\varepsilon_0,\delta_0)$-DP bitsum protocol $\Pi$ in the shuffled DP model, with RMSE $\mathcal E_{\Pi}$, running time $T_{\Pi}^{U}$ per user, communication $C_{\Pi}$ per user, and analyzer running time $T_{\Pi}^{A}$. 
%Then, for every $\delta'>0$ and integer $I\gtrsim something$, there is an $(\varepsilon,\delta)$-DP KDE protocol in the shuffled DP model (cf.~\Cref{def:shufdpkde}), where $\varepsilon=\varepsilon_0(e^{\varepsilon_0}-1)IQ + \sqrt{2IQ\ln(1/\delta')}\cdot\varepsilon_0$ and $\delta=IQ\delta_0+\delta'$, 
%with supRMSE $O(1/I)$, 
%with and the error is something. Also, the runtimes and communication are something and something. 
%\end{theorem}

\begin{theorem}\label{thm:main}
Let $\kk$ be a $\beta$-approximate $(Q,R,S)$-LSQable kernel (cf.~\Cref{def:lsq}). 
Suppose we have an unbiased $(\varepsilon_0,\delta_0)$-DP bitsum protocol $\Pi$ in the shuffled DP model, with RMSE $\mathcal E_{\Pi}$. 
Then, for every $\delta'>0$ and integer $I>0$, Algorithm~\ref{alg:shufdpkde} is a shuffled DP KDE protocol, which is $(\varepsilon,\delta)$-DP in the communication-threat model, where $\varepsilon=\varepsilon_0S(e^{\varepsilon_0S}-1)I + \varepsilon_0S\sqrt{2I\ln(1/\delta')}$ and $\delta=IS\delta_0+\delta'$, 
with supRMSE $\sqrt{4\beta^2 + I^{-1}\cdot 16R^4S \left(S + (\mathcal E_{\Pi}/n)^2\right)}$. The protocol has optimal bit-width $1$.
\end{theorem}

Note that $\varepsilon,\delta$ take the familiar ``advanced composition'' form \citep{dwork2014algorithmic} of $I$ instances of an $(\varepsilon_0S,\delta_0S)$-DP mechanism. % (in this case, the bitsum protocol $\Pi$). Similarly, t
%The computational parameters of the DP KDE protocol are $IQ$ times the respective quantities of the bitsum protocol $\Pi$, plus some overhead inherited from the LSQ family. %We detail them in \Cref{thm:computational}. 
%
The proof of \Cref{thm:main} goes by showing that the LSQ coordinates can be discretized essentially without loss of accuracy, and invoking the bitsum protocol on the discretized coordinates, using a careful probabilistic analysis to bound the overall supRMSE from their individual RMSEs. It is given in full in \Cref{sec:mainproof}.
\Cref{sec:variants} also discusses additional variants of \Cref{thm:main}, for bitsum protocols whose error guarantee is given in other terms than the RMSE (like \citet{cheu2019distributed}), or that achieve pure DP (like \citet{ghazi2020pure,ghazi2023pure}). 

As a concrete instantiation of \Cref{thm:main}, we get the following result for the Gaussian kernel, by plugging the shuffled DP protocol from \citet{ghazi2020private} as the bitsum protocol, and random Fourier features \citep{rahimi2007random} as the LSQ family. The proof is in \Cref{sec:fullgaussian}. 
For completeness, the protocol for this special case is fully detailed in Algorithm~\ref{alg:shufdpkdegaussian} in the appendix.

\begin{theorem}[shuffled DP Gaussian KDE]\label{thm:gaussian}
There are constants $C,C'>0$ such that the following holds. 
    Let $\delta\in(0,1)$ and $\varepsilon\leq C\log(1/\delta)$. 
    For every $\alpha\geq C'\sqrt{\log(1/\delta)}/(\varepsilon n)$, there is an $(\varepsilon,\delta)$-DP Gaussian KDE protocol in the shuffled DP model (under the communication-threat model) with $n$ users and inputs from $\R^d$, which has: supRMSE $\alpha$, user running time $\min(O(d/\alpha^2), \tilde O(d+1/\alpha^4))$, expected communication of $\tilde O(1/\alpha^2)$ bits per user, expected analyzer running time $O(n/\alpha^2)$, KDE query time $\min(O(d/\alpha^2), \tilde O(d+1/\alpha^4))$, and optimal bit-width $1$. 
\end{theorem}

%\begin{theorem}[shuffled DP Gaussian KDE]\label{thm:gaussian}
%There are constants $C,C'>0$ such that the following holds. 
%    Let $\varepsilon,\delta$ be such that $\delta\in(0,1)$ and $\varepsilon\leq C\log(1/\delta)$. 
%    For every $\alpha\geq C'\sqrt{\log(1/\delta)}/(\varepsilon n)$, there is an $(\varepsilon,\delta)$-DP Gaussian KDE protocol in the shuffled DP model with $n$ users and inputs from $\R^d$, which has: supRMSE $\alpha$, user running time $O(d/\alpha^2)$, expected communication of $O(1/\alpha^2)$ bits per user, analyzer running time $O(n/\alpha^2)$, and KDE query time $O(d/\alpha^2)$. 
%\end{theorem}

%expected communication of $(1+O(\frac{\log^2(1/(\alpha\delta))}{\varepsilon^2 n})/\alpha^2$ bits per user


\subsection{Private Learning, Classification and Class Decoding}\label{sec:hdc}
We now describe our private learning approach for classification and class decoding. 
Recall that each user holds a private data point $x\in\R^d$ and a corresponding label $c\in[m]$. 
%The analyzer aims to learn a KDE function representation per class. 

%how to apply \Cref{thm:main} for classification and class decoding.
%{sec:shuffled_dp_considerations}

\noindent\textbf{Learning.}
The learner will aim to learn a KDE function representation per class, using the shuffled DP protocol from \Cref{thm:main}. As discussed in \Cref{sec:shuffled_dp_considerations}, this requires knowing in advance the number of participating users per class. We therefore start with a preliminary communication round designated to privately obtain these counts. This could be done with an off-the-shelf shuffled DP histogram protocol (e.g., \citet{ghazi2020private}); however, this again requires prior knowledge of the total number of users. To avoid this chicken-and-egg issue, we will use vanilla local DP for the preliminary communication round. It is a stronger form of privacy that requires no prior knowledge, and its accuracy, while degraded, is still sufficient for the simple task of private user counts. 


Formally, let $\varepsilon_0,\delta_0,\varepsilon_{\mathrm{lbl}}>0$ be privacy parameters. %Recall that each user holds a private data point $x\in\R^d$ and a corresponding label $c\in[m]$. %They will be protected separately, with $\varepsilon_0,\delta_0$ being the privacy parameters for $x$, and $\varepsilon_{\mathrm{lbl}}$ being the privacy parameter for $c$.
%
Learning proceeds as follows. 
First, each user locally protects her label $c$ and publishes a privatized label $\tilde c$, using $m$-ary randomized response \citep{kairouz2014extremal,kairouz2016discrete}. Thus, $\tilde c$ is set to $c$ with probability $e^{\varepsilon_{\mathrm{lbl}}}/(e^{\varepsilon_{\mathrm{lbl}}}-1+m)$, and to a uniformly random label from $[m]\setminus\{c\}$ otherwise. This ensures that $\tilde c$ is $\varepsilon_{\mathrm{lbl}}$-DP, without shuffling.

Based on the published labels, the learner groups the users into their reported classes, and publishes the count of users $\tilde n_{\tilde c}$ in each reported class $\tilde c\in[m]$. These counts are $\varepsilon_{\mathrm{lbl}}$-DP by post-processing, and thus safe to publish. 
The users in each reported class then execute the shuffled DP KDE protocol in Algorithm~\ref{alg:shufdpkde}, using $\varepsilon_0,\delta_0$ as the privacy parameters for each instance of the bitsum protocol $\Pi$.
%\footnote{Recall that in order to execute a shuffled DP protocol, while each user does not need to know the identities of the other participating users, she does need to know their total number. This is why our learner publishes the reported class sizes $\{\tilde n_{\tilde c}:\tilde c\in[m]\}$ prior to executing the shuffled DP protocol per reported class.}
The learner acts as the analyzer in all these protocols, and through them learns an approximate KDE function $\widetilde K_c(\cdot)$ for each label $c\in[m]$. 
From \Cref{thm:main} and from basic DP composition, we have the following privacy guarantee:

\begin{corollary}
    The above learning protocol is $(\varepsilon,\delta)$-DP in the model-threat model, and $(\varepsilon+\varepsilon_{\mathrm{lbl}},\delta)$-DP in the communication-threat model, where $\varepsilon,\delta$ are given by $\varepsilon_0,\delta_0$ as stated in \Cref{thm:main}.
\end{corollary}

%These functions are $(\varepsilon,\delta)$-shuffled DP w.r.t.~the user data points, where $\varepsilon,\delta$ are as stated in \Cref{thm:main}, and  $\varepsilon_{\mathrm{lbl}}$-local DP w.r.t.~the user labels.

%Note that this requires that each user knows $\tilde n_{\tilde c}$ for her reported class $\tilde c$, as per the discussion in \Cref{sec:prelim}.) 
%The final shuffled DP privacy parameters $\varepsilon,\delta$ are as stated in \Cref{thm:main}. 
%The learner acts as the analyzer in these protocols, and through them learns an $(\varepsilon,\delta)$-DP approximate DP KDE function $\widetilde K_c(\cdot)$ for each label $c\in[m]$. 

%Based on the first round, the users are grouped into their reported classes. In the second round, the users in each reported class execute the DP KDE protocol in Algorithm~\ref{alg:shufdpkde}, with $\varepsilon_0,\delta_0$ used as the privacy parameters for each instance of the bitsum protocol $\Pi$. The final privacy parameters $\varepsilon,\delta$ for this round are as stated in \Cref{thm:main}. The learner acts as the analyzer in these protocols. At the end of the second round, the learner has an approximate DP KDE function $\widetilde K_c(\cdot)$ for each label $c\in[m]$. 

\noindent\textbf{Classification.}
The learner classifies a test point $y\in\R^d$ as the class where its private density estimate is maximized, namely as $c_y=\mathrm{argmax}_{c\in[m]}\widetilde K_c(y)$. 
%
We refer to this as the \emph{highest density class} (HDC) classifier. It can be viewed as generalizing the $k$-nearest neighbor (kNN) classifier, where the density of $y$ w.r.t.~class $c$ is measured by the number of its $k$-nearest neighbors labeled $c$, and of the nearest class center (NCC) classifier \citep{papyan2020prevalence}, where the density is measured by the distance between $y$ to the mean of all data points labeled $c$. Note that the kNN classifier is incompatible with DP, since its output relies on a small number of training points, while the NCC classifier is a special case of HDC, which we include in the experiments in the next section.
%(indeed, it was used in \cite{backurs2024efficiently} for classification with central DP)

\noindent\textbf{Private class decoding.}
To illustrate class decoding, Suppose that the data points are embeddings of text documents (even though the notion extends to other data modalities as well). Let $V\subset\R^d$ be a fixed public ``vocabulary'', say the embeddings of all words in an English dictionary. To ``decode'' a class $c$, the learner returns the top few vocabulary words $v\in V$ that maximize $\widetilde K_c(v)$. 
The goal is for those words to capture and convey the semantic meanings of texts from class $c$.  
%If the learned density function $\widetilde K_c$ is sufficiently accurate, then the list of top scoring vocabulary terms conveys the semantic content of the class, even though the learner has never seen any of its documents (nor embeddings) in the clear.

To underline the distinction between classification and class decoding: classification relies on the ability of the collection of functions $\{\widetilde K_c\}_{c\in[m]}$ to produce meaningfully rankable scores over the different classes for a \emph{fixed input} $y\in\R^d$; class decoding relies on the ability of each \emph{fixed function} $\widetilde K_c$ to produce meaningfully rankable scores over a large collection of inputs $V\subset\R^d$. 
In general, we expect a learned representation of a class to encompass its semantic meaning and to be decodable. The particular challenge in shuffled DP is that the representation was learned without observing any raw training example from the class, i.e., prior to imposing differential privacy on the training data.

\section{Experiments}\label{sec:experiments}
We evaluate our method with several combinations of kernels and bitsum protocols. 
Our code is enclosed in the supplementary material and available online.

\noindent\textbf{Datasets.}
We use three textual datasets and one image dataset:
\begin{CompactItemize}
    \item \emph{DBPedia-14} \citep{NIPS2015_250cf8b5}: Text documents containing summaries of Wikipedia articles. Training examples: 560K, test examples: 70K, classes: 14, task: topic classification.
    \item \emph{AG news} \citep{NIPS2015_250cf8b5}: Text documents containing news articles. Training examples: 120K, test examples: 7.6K, classes: 4, task: topic classification.
    \item \emph{SST2} \citep{socher-etal-2013-recursive}: Sentences extracted from movie reviews. Training examples: 67.3K, test examples: 1.82K, classes: 2, task: sentiment classification (positive/negative).
    \item \emph{CIFAR-10} \citep{Krizhevsky09learningmultiple}: Images from different object categories. Training examples: 50K, test examples: 10K, classes: 10, task: depicted object classification.
\end{CompactItemize} 
The datasets are embedded in $\R^d$ using standard pretrained models. 
The textual datasets are embedded into 768 dimensions with the SentenceBERT ``all-mpnet-base-v2'' model \citep{reimers2019sentence}. CIFAR-10 is embedded into 6144 dimensions with the SimCLR ``r152\_3x\_sk1'' model \citep{chen2020simple}, pre-trained on Imagenet (these are the same embeddings used in \citet{backurs2024efficiently} for their central DP experiment). 
All embedding vectors are normalized to unit length.
We note that the datasets are not included in the pretraining set of the respective embedding models used to embed them, ensuring that the pretraining set does not leak privacy in our experiments. 


\noindent\textbf{Kernels.}
We experiment with two kernels that fit into the framework of \Cref{thm:main}:
\begin{CompactItemize}
    \item\textbf{Gaussian:} $\kk(x,y)=\exp(-\norm{x-y}_2^2)$. As noted in \Cref{sec:prelim_lsq}, it is $(1,\sqrt2,1)$-LSQable by letting the functions in $\mathcal Q$ be random Fourier features, leading to \Cref{thm:gaussian}.
    \item\textbf{IP:} The inner product kernel $\kk(x,y)=x^Ty$. Since our embeddings are normalized, it is trivially $(d,1,d)$-LSQable. %, by letting the functions in $\mathcal Q$ be the identity. 
    It is also $(1,\sqrt d,1)$-LSQable by standard dimensionality reduction arguments (see \Cref{sec:iplsq}), 
    which leads to better parameters in \Cref{thm:main}. %which enables better control of the privacy parameters and computational cost in \Cref{thm:main}. 
    %
    Note that for a subset $X'$ of training points, 
    the IP KDE at $y$  
    %the KDE w.r.t.~the IP kernel 
    is $\tfrac1{|X'|}\sum_{x\in X'|}y^Tx=y^T(\tfrac1{|X'|}\sum_{x\in X'}x)$. Thus, the HDC classifier labels $y$ by the most similar class mean, as the NCC classifier discussed in \Cref{sec:hdc}.
\end{CompactItemize}
%Note that since our embeddings are normalized, $x^Ty=1-\tfrac12\norm{x-y}_2^2$ for all $x,y$, thus the Gaussian kernel is equivalent to the exponent of the IP kernel. 
%
To equalize the computational costs of the two kernels, we set the number of repetitions $I$ in Algorithm~\ref{alg:shufdpkde} to $d$ (the embedding dimension). % for both kernels. 
%we set the number of repetitions $I$ in the Gaussian kernel to $d$. 
Since the embeddings have unit length, there is no need to clip the vectors at a hyperparameter (as in \citet{kairouz2021distributed,backurs2024efficiently}) for the IP kernel, nor to optimize a bandwidth for the Gaussian kernel (it is just set to $1$). 

\noindent\textbf{Bitsum protocols.}
We use three shuffled DP bitsum protocols from the literature, each optimal in a different measure --- efficiency, accuracy and privacy, respectively (see also \Cref{sec:bitsumappendix}):
\begin{CompactItemize}
    \item \textbf{RR}: The classical randomized response protocol, as adapted to shuffled DP by \citet{cheu2019distributed}. This protocol has optimal communication efficiency of a single one-bit message per user.\footnote{See \Cref{sec:communication} for a detailed discussion on communication costs.}
  %\item \textbf{RR}: The randomized response protocol \citep{cheu2019distributed}, described in \Cref{sec:prelim_bitsum}. This protocol has optimal communication efficiency of a single one-bit message per user.\footnote{See \Cref{sec:communication} for a detailed discussion on communication costs.}
  %\footnote{The other two protocols are based on noise divisibility (see \Cref{sec:prelim_bitsum}), and use multiple messages per user (each user sends a different number of messages which is chosen at random), with more bits per message.} % As \cite{ghazi2020private} prove, this is required for their better accuracy ansadaksdlkjaslkj
  \item \textbf{3NB}: The correlated noise protocol of \citet{ghazi2020private}, which has asymptotically optimal accuracy. We call it 3NB since it relies on three samples from a negative binomial distribution.\footnote{See $\psi_1,\psi_2,\psi_3$ in Algorithm~\ref{alg:shufdpkdegaussian} in the appendix.}
  %(cf.~$\psi_1,\psi_2,\psi_3$ in Algorithm~\ref{alg:shufdpkdegaussian} in the appendix). 
  \item \textbf{Pure}: The pure DP ($\delta=0$) protocol of \citet{ghazi2023pure}. The other protocols use $\delta>0$.\footnote{To maintain purity in Algorithm~\ref{alg:shufdpkde} when composing instances of Pure, they are composed with ``basic'' (pure) rather than ``advanced'' (approximate) DP composition \citep{dwork2014algorithmic}. See \Cref{sec:variants}.}
\end{CompactItemize}
%Internal parameters for these protocols are set the same as in the papers that introduced them.
%In the appendix we include experiments with additional bitsum protocols from the literature. 
%: The Poisson and the negative binomial protocols from \cite{ghazi2020private}, and the Pure protocol when composed with advanced (approximate DP) composition instead of pure DP composition. 

\noindent\textbf{Privacy parameters.} We follow the guidelines given in \citet{ponomareva2023dp}, who cite current machine learning deployments of DP as using $\varepsilon$ generally between $5$ to $15$, and advocate for $\varepsilon\leq10$ as an acceptable privacy regime. We use $\varepsilon\in(0,10)$ to protect the training point $x\in\R^d$ with $(\varepsilon,\delta)$-shuffle DP, and $\varepsilon_{\mathrm{lbl}}\in\{3,5,7,10\}$ to protect the label $c\in[m]$ with $(\varepsilon_{\mathrm{lbl}},0)$-local DP.  %(recall this is a stronger notion of privacy, being both local and pure). 
We use $\delta=10^{-6}$ for DBPedia-14 and AG news, and $\delta=10^{-5}$ for SST2 and CIFAR-10, accounting for the different dataset sizes. For RR and 3NB, the $\delta$ ``budget'' in \Cref{thm:main} is split equally between the advanced composition parameter $\delta'$ and the total $IQ\delta_0$ term of the bitsum protocol instances. 


 

%\noindent\textbf{Other parameters.} We use the IP kernel with given dimension $d$. To equalize the computational cost between the two tested kernels, we set the number of repetitions $I$ in the Gaussian kernel also to $d$. Since we use normalized embeddings, there are no further hyperparameters to set: for the IP kernel, normalization ensures that the coordinates are bounded in $[-1,1]$ (whereas non-normalized vectors require clipping at a hyperparameters, e.g., \cite{kairouz2021distributed,backurs2024efficiently}), and for the Gaussian kernel, they obviate the need to choose a bandwidth hyperparameter (it is simply set to $1$). 

\subsection{Private Classification Results}
%\subsection{Results}

\begin{figure}[t]
\centering

\begin{subfigure}[b]{0.28\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=3.6cm]{figures/dbpedia_14_rr_5.png}
    %\caption{DBPedia-14, RR}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.28\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=3.6cm]{figures/dbpedia_14_3negbin_5.png}
    %\caption{DBPedia-14, 3NB}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.28\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=3.6cm]{figures/dbpedia_14_pure_5.png}
    %\caption{DBPedia-14, Pure}
\end{subfigure}\\

\begin{subfigure}[b]{0.28\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=3.6cm]{figures/ag_news_rr_5.png}
    %\caption{AG news, RR}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.28\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=3.6cm]{figures/ag_news_3negbin_5.png}
    %\caption{AG news, 3NB}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.28\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=3.6cm]{figures/ag_news_pure_5.png}
    %\caption{AG news, Pure}
\end{subfigure}\\

\begin{subfigure}[b]{0.28\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=3.6cm]{figures/sst2_rr_5.png}
    %\caption{SST2, RR}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.28\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=3.6cm]{figures/sst2_3negbin_5.png}
    %\caption{SST2, 3NB}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.28\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=3.6cm]{figures/sst2_pure_5.png}
    %\caption{SST2, Pure}
\end{subfigure} \\

\begin{subfigure}[b]{0.28\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=3.6cm]{figures/cifar10_rr_5.png}
    %\caption{CIFAR-10, RR}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.28\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=3.6cm]{figures/cifar10_3negbin_5.png}
    %\caption{CIFAR-10, 3NB}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.28\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=3.6cm]{figures/cifar10_pure_5.png}
    %\caption{CIFAR-10, Pure}
\end{subfigure} \\

\begin{subfigure}[b]{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/legend.png}
    %\caption{(legend)}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.28\textwidth}
    \centering
\end{subfigure}
\caption{Classification results with $\varepsilon_{\mathrm{lbl}}=5$}
\label{fig:epslbl5_primary}
\vspace{-0.1 in}
\end{figure}


%\noindent\textbf{Private classification.}
We evaluate the HDC classification accuracy for each combination of kernel and bitsum protocol, $\{$Gaussian, IP$\}\times\{$RR, 3NB, Pure$\}$, on each the dataset. Figure~\ref{fig:epslbl5_primary} shows results for $\varepsilon_{\mathrm{lbl}}=5$ (solid lines). Results for other values of $\varepsilon_{\mathrm{lbl}}$ are similar and appear in the appendix  (\Cref{fig:epslbl10_primary,fig:epslbl7_primary,fig:epslbl5_again,fig:epslbl3_primary}).

As points of reference, we include the following two baselines for each kernel:
\begin{CompactItemize}
    \item\emph{$(\varepsilon,\delta)$-central DP (dotted lines)}: HDC with the bitsum protocols in Algorithm~\ref{alg:shufdpkde} replaced by the standard Gaussian DP mechanism (see Section A in~\citet{dwork2014algorithmic}). 
    \item\emph{No privacy (dash-dot lines)}: HDC with the bitsum protocols replaced by exact summation. 
\end{CompactItemize}
In the appendix (Figure~\ref{fig:ldp}), we also include an ablation against a local DP baseline.

The results exhibit consistent behavior. Without privacy, the IP kernel outperforms the Gaussian kernel in all settings. This corroborates the known effectiveness of the NCC classifier on neural embeddings (see \citet{papyan2020prevalence}). Moreover, central DP closely matches the corresponding non-DP downstream accuracy already at small values of $\varepsilon$, which corroborates a similar empirical finding reported in \citet{backurs2024efficiently}.

Under shuffled DP, however, this behavior varies in different settings. The Gaussian kernel is more resilient to errors than IP, and thus matches its central DP and non-DP HDC performance at broader parameter regimes. As a result, it often achieves better overall accuracy than IP, even though its baseline (non-DP) accuracy is lower. This phenomenon is more expressed the more error-prone the setting is -- both with lower privacy budgets (lower $\varepsilon$), and when the bitsum protocol is less optimized for accuracy (i.e., RR and Pure vs.~3NB). 
The upshot is that the shuffled DP model, due its more delicate interplay between communication, privacy and accuracy compared to the central DP and non-DP settings, may require different and more error-resilient mechanisms for better downstream performance, particularly under tighter privacy and communication constraints. 

%The results are shown in Figure X. We plot the classification accuracy (rate of test points classified correctly) of the shuffled DP protocols in solid lines. For reference, we plot the accuracy of HDC without any privacy (i.e., with bitsum protocols replaced by exact sums) in dashed lines. Error bars are calculated on 5 independent repetitions of each experiment.
%
%Without privacy, the IP kernel outperforms the Gaussian kernel in all settings. This aligns with the known superiority of the NCC classifier on neural embeddings \cite{papyan2020prevalence}. When DP is imposed, this behavior varies with how much accuracy is impacted. 
%The IP kernel continues to be better with the 3NB protocol, which we recall is optimized for accuracy. The situation is reserved in the less accurate settings---with the RR and Pure protocols, which are less accurate than 3NB (optimizing instead for efficiency and DP purity, respectively), and, in some cases, in the lower $\varepsilon$ regimes---due to the higher tolerance of the Gaussian kernel to errors. This pattern of behavior extends to the additional bitsum protocols evaluated in the appendix. Thus, the choice of kernel should depend on the desired accuracy/efficiency/privacy balance of the target use case. 


\subsection{Communication Cost}\label{sec:communication}
The communication cost of our learning protocol depends on that of the bitsum protocol used within it. Specifically, for both the Gaussian and IP kernels, the communication cost is as follows:
\begin{CompactItemize}
    \item With RR: each user sends exactly $d$ messages.
    \item With 3NB: the number of messages sent by each user is a random variable (different per user), with expectation $(1+o(1))d$. (The $o(1)$ term vanishes as either $n$ or $\varepsilon$ grows.) 
    \item With Pure: the number of messages sent by each user is a random variable (different per user), with expectation $O(d^2\log(n)/\varepsilon_0)$. 
\end{CompactItemize}
With all three protocols, each message is of size $\lceil \log_2(d)\rceil+1$ bits.

Figure~\ref{fig:communication} displays the empirical number of messages on each dataset, for RR (whose communication is constant, as per above) and 3NB (whose communication is a random variable). 
Note while the cost of 3NB is asymptotically near-similar to RR, in practice its cost can be a few times larger, which may be significant in applications.  
Pure sends orders of magnitude more messages (as per above), which may render it impractical in tight communication settings, and cannot fit on the same plots.

\begin{figure}[t]
\centering

\begin{subfigure}[b]{0.21\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=2.5cm]{figures/communication_dbpedia14.png}
    %\caption{DBPedia-14, RR}
\end{subfigure} \hspace{0.1in}
\begin{subfigure}[b]{0.21\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=2.5cm]{figures/communication_ag_news.png}
    %\caption{DBPedia-14, 3NB}
\end{subfigure} \hspace{0.1in}
\begin{subfigure}[b]{0.21\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=2.5cm]{figures/communication_sst2.png}
    %\caption{DBPedia-14, Pure}
\end{subfigure} \hspace{0.1in}
\begin{subfigure}[b]{0.21\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=2.5cm]{figures/communication_cifar10.png}
    %\caption{DBPedia-14, Pure}
\end{subfigure}
\caption{Empirical communication}
\label{fig:communication}
\vspace{-0.1 in}
\end{figure}

\subsection{Private KDE results}
We also directly evaluate \Cref{thm:gaussian} for the standalone task of private Gaussian KDE (without subsequent classification). The results are shown in Figure~\ref{fig:direct_kde}, with accuracy measured over 1K random queries from the query set of each dataset. They show that the KDE error generally tracks with the downstream classification accuracy reported above, with 3NB being the most accurate variant with error vanishing nearly as fast as central DP, followed by RR and Pure.


\begin{figure}[t]
\centering

\begin{subfigure}[b]{0.21\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=2.2cm]{figures/kde_dbpedia14.png}
    %\caption{DBPedia-14, RR}
\end{subfigure} \hspace{0.1in}
\begin{subfigure}[b]{0.21\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=2.2cm]{figures/kde_ag_news.png}
    %\caption{DBPedia-14, 3NB}
\end{subfigure} \hspace{0.1in}
\begin{subfigure}[b]{0.21\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=2.2cm]{figures/kde_sst2.png}
    %\caption{DBPedia-14, Pure}
\end{subfigure} \hspace{0.1in}
\begin{subfigure}[b]{0.21\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=2.2cm]{figures/kde_cifar10.png}
    %\caption{DBPedia-14, Pure}
\end{subfigure}
\caption{Gaussian KDE accuracy}
\label{fig:direct_kde}
\vspace{-0.1 in}
\end{figure}

\subsection{Private Class Decoding Results}
%\noindent\textbf{Private class decoding.}
We perform class decoding, as described in \Cref{sec:hdc}, on the three textual datasets. 
As the public vocabulary $V$ we use GloVe 6B \citep{pennington2014glove}, %\footnote{Available online at \url{https://nlp.stanford.edu/projects/glove/}.} 
%a vocabulary 
consisting of 400K words extracted from public sources. 
Rather than using the embeddings from \citet{pennington2014glove}, we embed the terms in $\R^d$ with the same pre-trained SentenceBERT model used to embed the datasets. Then, for each class $c\in[m]$ of each dataset, we rank all vocabulary terms according to their density as reported by the function $\widetilde K_c(\cdot)$ privately learned by our shuffled KDE protocol for that class, and report the top-3 scoring terms. We repeat this for every combination of kernel and bitsum protocol. 

We make no attempt at quantifying a measure of class decoding performance, since semantic relatedness is inherently somewhat subjective, and may furthermore depend on external knowledge (for example, when the ``artist'' or ``athlete'' classes are decoded into names of specific artists or athletes). 
Rather, our goal in this experiment to is gain qualitative insight into what the shuffled DP KDE protocol succeeds in learning, despite its lack of access to unprotected training examples, and to complement the quantitative classification accuracy results. 

\Cref{tbl:decodingmain} includes decoding results with $\varepsilon\approx3.2$ and $\varepsilon_{\mathrm{lbl}}=5$ (the $\varepsilon$ values are slightly different across the datasets and bitsum protocols, due to the use of different $\delta$ values and composition theorems, as detailed earlier in this section). It only includes some classes from each dataset, due to space limits; results for all classes, and for other values of $\varepsilon$, are in the appendix. 

Qualitatively, in settings where classification accuracy is nontrivial (per Figure~\ref{fig:epslbl5_primary}), the 
class decoding results in \Cref{tbl:decodingmain} also yield vocabulary words that are aligned with the topic of the class. 
This demonstrates that the class representations learned by shuffled DP KDE protocol capture the semantic meaning of the classes, and preserve the ability to rank not only inter-class similarities (as needed for classification), but also intra-class similarities (as needed for decoding a specific class). 



\begin{table*}
\caption{Private class decoding results with $\varepsilon\approx3.2$ and $\varepsilon_{\mathrm{lbl}}=5$} \label{tbl:decodingmain}
%{\renewcommand{\arraystretch}{1.5}
\begin{centering}
%\setlength\extrarowheight{-3pt}
\scriptsize
%\begin{adjustwidth}{-0.3in}{-0.1in}
\begin{tabular}
{p{0.08\linewidth}p{0.06\linewidth}p{0.03\linewidth}p{0.34\linewidth}p{0.34\linewidth}}
\toprule
 \textbf{Dataset} & \textbf{Class} & \textbf{Bitsum} & \textbf{Gaussian KDE class decoding} & \textbf{IP KDE class decoding} \\
\midrule

\multirow{2}{*}{DBPedia-14} & \multirow{3}{0.1\linewidth}{Company} & RR & vendors, gencorp, servicers & firesign, wnews, usos \\
 &  & 3NB & molycorp, newscorp, mediacorp & companys, alicorp, interactivecorp \\
 &  & Pure & ameritech, alicorp, newscorp & alibabacom, oscorp, companies \\
%\cmidrule{2-5}
% & \multirow{3}{0.1\linewidth}{Artist} & RR & author, aristizabal, levesongower & catalani, macki, bacashihua \\
% &  & 3NB & artist, lyricists, musician & author, musician, roberthoudin \\
% &  & Pure & author, originator, mikhaylovsky & musician, artist, jacquesfranois \\
%\cmidrule{2-5}
% & \multirow{3}{0.1\linewidth}{Office holder} & RR & louislegrand, legislator, lawmaker & sulphide, exclude, sulked \\
% &  & 3NB & patiashvili, kumaritashvili, biographers & ministerpresident, legislator, congressperson \\
% &  & Pure & polinard, bobrzaski, politician & ministerpresident, politician, polian \\
%\cmidrule{2-5}
% & \multirow{3}{0.1\linewidth}{Building} & RR & reisterstown, benenson, hellertown & opened, mastered, poegaslavonia \\
% &  & 3NB & frenchtown, brookeville, kenansville & beaconhouse, manorville, reisterstown \\
% &  & Pure & huntingtonwhiteley, wrightstown, randallstown & hyattsville, roxboro, reisterstown \\
%\cmidrule{2-5}
% & \multirow{3}{0.1\linewidth}{Village} & RR & lalganj, balrampur, manikganj & baluchestan, jagiellonia, nidderdale \\
% &  & 3NB & pazardzhik, tzintzuntzan, khuzistan & kyrghyzstan, kalinske, kalinski \\
% &  & Pure & poniewozik, mieszkowice, czerniewice & kazemabad, diyarbakir, khoramabad \\
%\cmidrule{2-5}
% & \multirow{3}{0.1\linewidth}{Plant} & RR & chaetophoraceae, gentianaceae, rutaceae & chilensis, surinamensis, tampines \\
% &  & 3NB & cupressaceae, chaetophoraceae, podocarpaceae & chenopodiaceae, araucariaceae, loranthaceae \\
% &  & Pure & asclepiadaceae, cupressaceae, gentianaceae & chaetophoraceae, chenopodiaceae, araucariaceae \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Film} & RR & biopic, movie, screenplay & kaptai, kakhi, kaloi \\
 &  & 3NB & filmography, vanya, ghostbusters & movie, filmography, screenplay \\
 &  & Pure & filme, movie, videodrome & movie, film, filmmakers \\
\midrule
\multirow{2}{*}{AG news} & \multirow{3}{0.1\linewidth}{Sports} & RR & vizner, runnerups, dietrichson & ongeri, grandi, zarate \\
 &  & 3NB & injury, semifinalists, finalists & semifinalists, championship, standings \\
 &  & Pure & pensford, rematches, undefeated & chauci, teammates, nith \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Business} & RR & repurchases, downtrend, equitywatch & sneed, timesnews, anxiousness \\
 &  & 3NB & enrononline, investcorp, comcorp & stockholders, nasdaq, marketwatchcom \\
 &  & Pure & corporations, consolidations, consolidated & merger, divestiture, stockholders \\
%\cmidrule{2-5}
% & \multirow{3}{0.1\linewidth}{World} & RR & iraqi, hamas, darfur & tym, asg, tyo \\
% &  & 3NB & hezbollah, hamas, iraqstld & hamas, terrorists, baghdadi \\
% &  & Pure & kutayev, qaeda, yanukovych & shamkir, barricading, samaritans \\
%\cmidrule{2-5}
% & \multirow{3}{0.1\linewidth}{Sci/Tech} & RR & snopes, cyberworks, hacktivists & meanings, collegefootballnewscom, multipolarity \\
% &  & 3NB & ibm, thermedics, flextech & microsoft, ibm, accenture \\
% &  & Pure & feedbacks, companywide, eurogroup & movedtech, techcrunch, swindlers \\
\midrule
\multirow{2}{*}{SST2} & \multirow{3}{0.1\linewidth}{Negative} & RR & beguile, inception, shallow & manipulating, uncouple, dissects \\
 &  & 3NB & melodrama, rawness, blandness & comedy, tastelessness, uneasiness \\
 &  & Pure & chumminess, meaningfulness, mootness & absurdities, chastisement, absurdity \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Positive} & RR & kindliness, pleasantness, entertaining & enjoyments, academie, amusements \\
 &  & 3NB & salacious, movie, majestic & salaciousness, theatricality, memorability \\
 &  & Pure & spiritedness, spirited, perspicacious & exorcisms, fairytales, revisiting \\

\bottomrule
\end{tabular}
%\end{adjustwidth}
\end{centering}%}
\end{table*}

\section{Conclusion}
We showed how to use shuffled DP for ``one-shot'' data collection and learning from an undetermined pool of uncommitted end users, in contrast to prior work on ML with shuffled DP, which mostly focused on collaborative distributed training across committed parties over time. Due to the desirable accuracy of shuffled DP, our method is able to learn intricate data semantics while adhering to a distributed notion of privacy. Our experimental results highlight practical downstream considerations related to the delicate interplay between privacy, accuracy and communication cost. 

Future work would explore further ways to deploy shuffled DP in ML pipelines, and extend to more challenging settings, such as privately and continuously monitoring end user data over time. 

\subsubsection*{Acknowledgments}
We thank the anonymous reviewers for helpful feedback. 
Work supported by Len Blavatnik and the Blavatnik Family foundation and by an Alon Scholarship of the Israeli Council for Higher Education. Author is also with Amazon. 
%This work does not relate to the author's position at Amazon. 
This work is not associated with Amazon.


\bibliography{shufbib}
\bibliographystyle{iclr2025_conference}

\newpage
\appendix

\section{Proofs}\label{sec:proofs}
\subsection{Proof of \Cref{thm:main}}\label{sec:mainproof}

We restate the theorem for convenience:
\begin{theorem}[\Cref{thm:main}, restated]
Let $\kk$ be a $\beta$-approximate $(Q,R,S)$-LSQable kernel (cf.~\Cref{def:lsq}). 
Suppose we have an unbiased $(\varepsilon_0,\delta_0)$-DP bitsum protocol $\Pi$ in the shuffled DP model, with RMSE $\mathcal E_{\Pi}$. 
Then, for every $\delta'>0$ and integer $I>0$, Algorithm~\ref{alg:shufdpkde} is a shuffled DP KDE protocol, which is $(\varepsilon,\delta)$-DP in the communication-threat model, where $\varepsilon=\varepsilon_0S(e^{\varepsilon_0S}-1)I + \varepsilon_0S\sqrt{2I\ln(1/\delta')}$ and $\delta=IS\delta_0+\delta'$, 
with supRMSE $\sqrt{4\beta^2 + I^{-1}\cdot 16R^4S \left(S + (\mathcal E_{\Pi}/n)^2\right)}$. The protocol has optimal bit-width $1$. 
\end{theorem}

The proof proceeds in three steps: (i) discretize the LSQ coordinates of $f_i(x)$ locally at each user from $[-R,R]$ to $\{-R,R\}$, using randomized rounding to maintain the LSQ property; (ii) use the bitsum protocol to estimate the sum of each discretized coordinate (with shifting and scaling to turn bitsums into $\pm R$-sums); (iii) use the LSQ property together with the RMSE bound of the given bitsum protocol to bound the total error of any output KDE estimate.

%We start with discretization.
\subsubsection{Discretization}\label{sec:proof_discretization}
We will index the users in the protocol by $u=1,\ldots,n$.
Fix $i\in[I]$. Let $(f_i,g_i)$ be the pair sampled from $\mathcal Q$ in the global initialization step of Algorithm~\ref{alg:shufdpkde}, and recall that $f_i,g_i:\R^d\rightarrow[-R,R]^Q$. 
Consider a user $u\in[n]$ with input $x_u\in\R^d$. 
In the randomizer of Algorithm~\ref{alg:shufdpkde}, for every $j\in[Q]$, the user samples $b_{ij}\sim\mathrm{Bernoulli}((f_i(x_u)_j+R)/2R)$ using private randomness, independently of the other users and of the other coordinates. 
To refer to local samples of different users, in this proof we will denote $b_{ij}$ by $b_{ij}^{(u)}$. 

Define $\bar f_i^{(u)}\in\{-R,R\}^Q$ by letting $\bar f_{ij}^{(u)}=(2b_{ij}^{(u)}-1)R$ for every $j$.


\begin{claim}\label{clm:discretization}
  For every $i\in [I]$, $u\in[n]$ and $y\in\R^d$,
  \[ \left|\E_{(f_i,g_i),\{b_{ij}^{(u)}\}_{j=1}^Q}\left[(\bar f_i^{(u)})^Tg_i(y)\right] - \kk(x,y)\right| \leq \beta, \]
  where the expectation is over both the sampling of $(f_i,g_i)\sim\mathcal Q$ in the global initialization part and the sampling of $\{b_{ij}^{(u)}:j\in[Q]\}$ in the randomizer part of Algorithm~\ref{alg:shufdpkde}. 
\end{claim}
\begin{proof}
It is immediate to check that $\E_{b_{ij}^{(u)}}[\bar f_{ij}^{(u)} \;\; | \;\; f_i]=f_i(x_u)_j$ for every $j$, hence,
    \begin{align*}
        \E_{(f_i,g_i),\{b_{ij}^{(u)}\}}\left[(\bar f_i^{(u)})^Tg_i(y)\right] &=  \E_{(f_i,g_i)}\left[\E_{\{b_{ij}^{(u)}\}}\left[(\bar f_i^{(u)})^Tg_i(y) \;\; | \;\; (f_i,g_i)\right]\right] \\
        &= \E_{(f_i,g_i)}\left[f_i(x)^Tg_i(y) \right],
    \end{align*}
and the claim follows from the LSQ property (\Cref{def:lsq}).
\end{proof}

%Next, f
\subsubsection{Instances of the Bitsum Protocol}
Fix $(i,j)\in[I]\times[Q]$. 
Let $\bar F_{ij}=\sum_{u=1}^n\bar f_{ij}^{(u)}$ and $B_{ij}=\sum_{u=1}^nb_{ij}^{(u)}$. 
The shuffled DP protocol in Algorithm~\ref{alg:shufdpkde} executes an independent instance of the given shuffled DP bitsum protocol $\Pi$ to estimate $B_{ij}$, and this estimate is denoted by $\widetilde B_{ij}$ in the analyzer in Algorithm~\ref{alg:shufdpkde}. 
We will denote this instance of $\Pi$ by $\Pi_{ij}$. 
Recall that $\Pi$ is an unbiased bitsum protocol and has RMSE $\mathcal E_{\Pi}$. Since $B_{ij}$ itself is a random variable determined by the sampling of $(f_i,g_i)\sim\mathcal Q$ and on the local randomized rounding by the users, conditioning on these, we have 
\[ \E_{\Pi_{ij}}[\widetilde B_{ij} - B_{ij} \;|\; (f_i,g_i),B_{ij}]=0 \;\;\;\; \text{and} \;\;\;\; \E_{\Pi_{ij}}[|\widetilde B_{ij} - B_{ij}|^2 \;|\; (f_i,g_i),B_{ij}]=\mathcal E_{\Pi}^2 . \]

The analyzer computes and publishes $\widetilde F_{ij}=(2\widetilde B_{ij}-n)R$. Considering this as an estimate of $\bar F_{ij}$, we denote
\[ E_{ij} := \widetilde F_{ij} - \bar F_{ij} . \]

\begin{claim}\label{clm:errorij}
$\{E_{ij} \;|\; (f_i,g_i),B_{ij}\}_{i,j}$ are independent random variables, and each satisfies
\[\E_{\Pi_{ij}}\left[E_{ij}\;|\; (f_i,g_i),B_{ij}\right]=0 \;\;\;\; \text{and} \;\;\;\;
   \E_{\Pi_{ij}}\left[\left|E_{ij}\right|^2\;|\; (f_i,g_i),B_{ij}\right]=(2R\cdot \mathcal E_{\Pi})^2 .
\]
%has zero mean and second moment $\E\left[\left|E_{ij}\right|^2\right]=(2R\cdot \mathcal E_{\Pi})^2$.
\end{claim}
\begin{proof}
Probabilistic independence holds since we use independent randomness in the instance $\Pi_{ij}$ of $\Pi$ for different pairs $i,j$. 
For the first and second moments, recall that $\bar f_{ij}^{(u)}=(2b_{ij}^{(u)}-1)R$ for every user $u$, which implies $\bar F_{ij}=(2B_{ij}-n)R$ when summing over the users. Also recall from above that $\widetilde F_{ij}=(2\widetilde B_{ij}-n)R$. 
Thus,
\begin{align*}
  \E_{\Pi_{ij}}
\left[E_{ij}\;|\; (f_i,g_i),B_{ij}\right] &=  \E_{\Pi_{ij}}
\left[\widetilde F_{ij} - \bar F_{ij}\;|\; (f_i,g_i),B_{ij}\right] \\
  &= \E_{\Pi_{ij}}
\left[(2\widetilde B_{ij}-n)R - (2B_{ij}-n)R\;|\; (f_i,g_i),B_{ij}\right] \\
  &= 2R\cdot \E_{\Pi_{ij}}
\left[\widetilde B_{ij}-B_{ij}\;|\; (f_i,g_i),B_{ij}\right] \\
  &= 0,
\end{align*}
and
\begin{align*}
    \E_{\Pi_{ij}}
\left[\left|E_{ij}\right|^2\;|\; (f_i,g_i),B_{ij}\right] &= \E_{\Pi_{ij}}
\left[\left|\widetilde F_{ij} - \bar F_{ij}\right|^2\;|\; (f_i,g_i),B_{ij}\right] \\
    &= \E_{\Pi_{ij}}
\left[\left|(2\widetilde B_{ij}-n)R - (2B_{ij}-n)R\right|^2\;|\; (f_i,g_i),B_{ij}\right] \\
    &= (2R)^2\cdot \E_{\Pi_{ij}}
\left[\left|\widetilde B_{ij}-B_{ij}\right|^2\;|\; (f_i,g_i),B_{ij}\right] \\
    &= (2R\cdot \mathcal E_{\Pi})^2. 
\end{align*}
\end{proof}

\subsubsection{Bounding the supRMSE}
To bound the supRMSE of Algorithm~\ref{alg:shufdpkde}, fix $y\in\R^d$. The KDE query part of the protocol uses the analyzer's published output to estimate $KDE_X(y)$ by $\frac{1}{nI}\sum_{i=1}^I\sum_{j=1}^Q\widetilde F_{ij}g_i(y)_j$. We now bound the RMSE of this estimate. Substituting $E_{ij} := \widetilde F_{ij} - \bar F_{ij}$, we have
\begin{align*}
    & \E\left[\left| KDE_X(y) - \frac{1}{nI}\sum_{i=1}^I\sum_{j=1}^Q\widetilde F_{ij}g_i(y)_j \right|^2\right] \\
    & = \E\left[\left| KDE_X(y) - \frac{1}{nI}\sum_{i=1}^I\sum_{j=1}^Q\bar F_{ij}g_i(y)_j + \frac{1}{nI}\sum_{i=1}^I\sum_{j=1}^Q E_{ij}g_i(y)_j\right|^2\right] \\
    &\leq 2\E\left[\left| KDE_X(y) - \frac{1}{nI}\sum_{i=1}^I\sum_{j=1}^Q\bar F_{ij}g_i(y)_j\right|^2\right] + 2\E\left[\left|\frac{1}{nI}\sum_{i=1}^I\sum_{j=1}^Q E_{ij}g_i(y)_j\right|^2\right] . \numberthis \label{eq:twosummands}
\end{align*}

We handle the two summands in turn. 

\subsubsection{First Summand: Discretized LSQ Approximation Error}
%Moving on to the first summand in \Cref{eq:twosummands}, 
For every $i$, let $\bar F_i\in\R^Q$ denote the vector with coordinates $\bar F_{ij}$. Recalling that $\bar F_{ij}=\sum_{u=1}^n\bar f_{ij}^{(u)}$, we have $\bar F_i=\sum_{u=1}^n\bar f_i^{(u)}$. 
We can thus write,
\begin{align*}
    \E\left[\left| KDE_X(y) - \frac{1}{nI}\sum_{i=1}^I\sum_{j=1}^Q\bar F_{ij}g_i(y)_j\right|^2\right] &= \E\left[\left| KDE_X(y) - \frac{1}{nI}\sum_{i=1}^I\bar F_i^Tg_i(y)\right|^2\right] \\
    &= \frac1{I^2}\E\left[\left| \sum_{i=1}^I \left(KDE_X(y) - \frac{1}{n}\bar F_i^Tg_i(y)\right)\right|^2\right] . \numberthis \label{eq:fullz}
\end{align*}
Denote the random variables,
\[ Z_i := KDE_X(y) - \frac{1}{n}\bar F_i^Tg_i(y) , \]
and for every $u\in[n]$,
\[ Z_{i,u} := \kk(x_u,y) - (\bar f_i^{(u)})^Tg_i(y) . \]
Observe that $Z_i=\frac1n\sum_{u=1}^nZ_{i,u}$, and that the rightmost side of \Cref{eq:fullz} is $\frac1{I^2}\E[|\sum_{i=1}^IZ_i|^2]$. Due to the probabilistic independence of samples for different values $i\in[I]$, we can expand this as
\begin{align*}
\E\left[\left|\sum_{i=1}^IZ_i\right|^2\right] &= \left|\sum_{i=1}^I\E[Z_i^2] + \sum_{i=1}^I\sum_{i'\neq i}\E[Z_i]\cdot\E[Z_{i'}]\right| \\
&\leq \sum_{i=1}^I\E[Z_i^2] + \sum_{i=1}^I\sum_{i'\neq i}|\E[Z_i]|\cdot|\E[Z_{i'}]|. \numberthis \label{eq:zexpand}
\end{align*}
\begin{claim}\label{clm:zbounds}
    For every $i$ we have $|\E[Z_i]|\leq\beta$ and $\E[Z_i^2]\leq(\beta + 2R^2S)^2$. 
\end{claim}
\begin{proof}
    For the first bound in the claim, observe that \Cref{clm:discretization} can be rewritten as $|\E[Z_{i,u}]|\leq\beta$ for every $i,u$. Since $Z_i=\frac1n\sum_{u=1}^nZ_{i,u}$, we get $|\E[Z_i]| \leq \frac1n\sum_{u=1}^n\E|Z_{i,u}| \leq \beta$.

    For the second bound in the claim, recall that by \Cref{def:lsq}, for every supported function pair $(f,g)$ in the LSQ family $\mathcal Q$, and every $x,y\in\R^d$, we have that $f(x)$ and $g(y)$ have coordinates in $[-R,R]$, and have at most $S$ non-zero coordinates each. Thus, $|f(x)^Tg(y)|\leq R^2S$. By recalling that $\bar f_i^{(u)}$ was generated from $f_i(x_u)$ by rounding its coordinates to $\{-R,R\}$, this implies in particular that $|(\bar f_i^{(u)})^Tg_i(y)|\leq  R^2S$ for every $u$. Moreover, since by \Cref{def:lsq} we have $|\kk(x,y)-\E_{(f,g)\sim\mathcal Q}[f(x)^Tg(y)]|\leq\beta$, this also implies that $|\kk(x,y)|\leq R^2S+\beta$. Therefore, unconditionally, 
    \begin{align*}
        |Z_i| &\leq \frac1n\sum_{u=1}^n|Z_{i,u}| \\
        &= \sum_{u=1}^n\left|\kk(x_u,y) - (\bar f_i^{(u)})^Tg_i(y)\right| \\
        &\leq \frac1n\sum_{u=1}^n\left(\left|\kk(x_u,y)\right| + \left|(\bar f_i^{(u)})^Tg_i(y)\right|\right) \\
        &\leq \beta + 2R^2S ,
    \end{align*}
    which implies in particular $\E[Z_i^2]\leq(\beta + 2R^2S)^2$.
\end{proof}

We now have,
\begin{align*}
    &\E\left[\left| KDE_X(y) - \frac{1}{nI}\sum_{i=1}^I\sum_{j=1}^Q\bar F_{ij}g_i(y)_j\right|^2\right] & \\
    \;\;\;\;\;\;\;\;\;\;\;\; &= \frac1{I^2}\E\left[\left| \sum_{i=1}^I \left(KDE_X(y) - \frac{1}{n}\bar F_i^Tg_i(y)\right)\right|^2\right] & \text{\Cref{eq:fullz}} \\
    &= \frac1{I^2}\E\left[\left|\sum_{i=1}^IZ_i\right|^2\right] & \text{definition of $Z_i$} \\
    &\leq \frac1{I^2}\left(\sum_{i=1}^I\E[Z_i^2] + \sum_{i=1}^I\sum_{i'\neq i}|\E[Z_i]|\cdot|\E[Z_{i'}]|.\right) & \text{\Cref{eq:zexpand}} \\
    &\leq
     \frac1{I^2} \left( I  (\beta + 2R^2S)^2 + I(I-1)\beta^2\right) & \text{\Cref{clm:zbounds}} \\
     &\leq \frac{8R^4S^2}{I} + 2\beta^2 . &
\end{align*}

This is our bound for the first summand in \Cref{eq:twosummands}.

\subsubsection{Second Summand: Total Bitsum Protocol Error}\label{sec:secondsummand}

%We start with the second summand. 
For $i\in[I]$, let $Y_i$ be the random variable
\[ Y_i=\sum_{j=1}^QE_{ij}g_i(y)_j . \]
Note that the second summand in \Cref{eq:twosummands} equals $2(\frac1{nI})^2\E[(\sum_{i=1}^IY_i)^2]$. 

By \Cref{clm:errorij}, $\{E_{ij} \;|\; (f_i,g_i),B_{ij}\}_{i,j}$ are independent random variables. Each $Y_i$, when conditioned on $(f_i,g_i),\{B_{ij}\}_{j\in[Q]}$, is a linear combination of a subset of these random variables and the subsets are disjoint for $i\neq i'$, hence $\{Y_i \;|\; (f_i,g_i),\{B_{ij}\}_{j\in[Q]}\}_{i\in[I]}$ are also independent random variables. Furthermore, for every $i\in[I]$ we have
\begin{align*}
    \E_{\{\Pi_{ij}\}_{j\in[Q]}}\left[Y_i\;|\; (f_i,g_i),\{B_{ij}\}_{j\in[Q]}\right] &= \E_{\{\Pi_{ij}\}_{j\in[Q]}}\left[\sum_{j=1}^QE_{ij}g_i(y)_j\;|\; (f_i,g_i),\{B_{ij}\}_{j\in[Q]}\right] \\
    &= \sum_{j=1}^Qg_i(y)_j\E_{\Pi_{ij}}\left[E_{ij}\;|\; (f_i,g_i),B_{ij}\right] \\
    &= 0 ,
\end{align*}
and
\begin{align*}
    \E_{\{\Pi_{ij}\}_{j\in[Q]}}\left[Y_i^2\;|\; (f_i,g_i),\{B_{ij}\}_{j\in[Q]}\right] &= \E_{\{\Pi_{ij}\}_{j\in[Q]}}\left[\left(\sum_{j=1}^QE_{ij}g_i(y)_j\right)^2\;|\; (f_i,g_i),\{B_{ij}\}_{j\in[Q]}\right] \\
    &= \sum_{j=1}^Q(g_i(y)_j)^2\E_{\Pi_{ij}}\left[E_{ij}^2\;|\; (f_i,g_i),B_{ij}\right] \\
    &= \sum_{j=1}^Q(g_i(y)_j)^2(2R\mathcal E_\Pi)^2 ,
\end{align*}
having used $\E_{\Pi_{ij}}\left[E_{ij}\;|\; (f_i,g_i),B_{ij}\right]=0$ and $\E_{\Pi_{ij}}\left[\left|E_{ij}\right|^2\;|\; (f_i,g_i),B_{ij}\right]=(2R\cdot \mathcal E_{\Pi})^2$ from \Cref{clm:errorij}. 
Since by \Cref{def:lsq} $g_i(y)$ has at most $S$ non-zero entries and each is bounded in absolute value by $R$,
\[
  \E_{\{\Pi_{ij}\}_{j\in[Q]}}\left[|Y_i|^2\;|\; (f_i,g_i),\{B_{ij}\}_{j\in[Q]}\right] \leq 4SR^4\cdot \mathcal E_\Pi^2 .
\]
Therefore,
\begin{align*}
    & \E_{\{\Pi_{ij}\}_{i\in[I],j\in[Q]}}\left[\left(\sum_{i=1}^IY_i\right)^2 \;\; | \;\; \{(f_i,g_i),B_{ij}\}_{i\in[I],j\in[Q]} \right] \\
    &= \sum_{i=1}^I\E_{\{\Pi_{ij}\}_{j\in[Q]}}\left[Y_i^2 \;\; | \;\; (f_i,g_i),\{B_{ij}\}_{j\in[Q]} \right] \\
    &\leq I \cdot 4SR^4\cdot \mathcal E_\Pi^2 .
\end{align*}

Now we can bound the second summand in \Cref{eq:twosummands} as
\begin{align*}
& \frac{2}{n^2I^2}\E_{\{(f_i,g_i),B_{ij},\Pi_{ij}\}_{i\in[I],j\in[Q]}}\left[\left(\sum_{i=1}^IY_i\right)^2 \right] \\
 &= \frac{2}{n^2I^2}\E_{\{(f_i,g_i),B_{ij}\}_{i\in[I],j\in[Q]}}\left[\E_{\{\Pi_{ij}\}_{i\in[I],j\in[Q]}}\left[\left(\sum_{i=1}^IY_i\right)^2 \;\; | \;\; \{(f_i,g_i),B_{ij}\}_{i\in[I],j\in[Q]} \right]\right] \\
 &\leq \frac{8SR^4\mathcal E_{\Pi}^2}{n^2I} .
\end{align*}

\subsubsection{Finishing the Proof of \Cref{thm:main}}\label{sec:finishing}
\textbf{Accuracy:} 
By putting together the bounds on both summands in \Cref{eq:twosummands}, we get that the RMSE of estimating $KDE_X(y)$ is at most $\sqrt{4\beta^2 + \frac{16R^4S}{I}\left(S + \frac{\mathcal E_{\Pi}^2}{n^2}\right)}$. Since this holds for every $y\in\R^d$, this is a bound on the supRMSE.

\textbf{Privacy:} for every $i\in[I]$ and $j\in[Q]$, let $O_{ij}$ denote the output of the shuffler in protocol instance $\Pi_{ij}$. The fact that $\Pi_{ij}$ is an instance of the $(\varepsilon_0,\delta_0)$-DP protocol $\Pi$ means (by the definition of the shuffled DP model) that $O_{ij}$ is $(\varepsilon_0,\delta_0)$-DP w.r.t.~the collection of user inputs. 

First, fix $i\in[I]$. Recall the sparsity property of LSQ (\Cref{def:lsq}), namely that each $f_i$ has at most $S$ non-zero entries per user. This means that if the input of one user is omitted from the dataset, the inputs of at most $S$ of the $Q$ protocols $\{\Pi_{ij}\}_{j=1}^Q$ are changed. Since these protocol instances use independent randomness, then by standard (``basic'') DP composition arguments \citep{dwork2014algorithmic}, the collection $\{O_{ij}\}_{j=1}^Q$ is $(\varepsilon_0S,\delta_0S)$-DP. In other words, the protocol $\Psi_i$ obtained by composing the protocols $\{\Pi_{ij}\}_{j=1}^Q$ is $(\varepsilon_0S,\delta_0S)$-DP in the shuffled model (see \citet{cheu2019distributed} for the definition of protocol composition in the shuffled DP model). 

Now, by ``advanced'' composition for shuffled DP protocols (Lemma 3.6 in \citet{cheu2019distributed}) over the $I$ protocols $\{\Psi_i\}$, we get that the collection of shuffler outputs $\{O_{ij}:(i,j)\in[I]\times[Q]\}$ is $(\epsilon,\delta)$-DP, with $\epsilon,\delta$ as stated in \Cref{thm:main}. Since the analyzer in Algorithm~\ref{alg:shufdpkde} is a post-processing of these shuffler outputs, the protocol in Algorithm~\ref{alg:shufdpkde} is $(\epsilon,\delta)$-DP in the shuffled model.

\textbf{Efficiency:}
The computational parameters of Algorithm~\ref{alg:shufdpkde} is straightforward to calculate from those of the bitsum protocol $\Pi$ and the LSQ family $\mathcal Q$: the global initialization samples $I$ pairs $(f_i,g_i)$ from $\mathcal Q$; each user evaluates $f_i(x)$ on her input $x$ for every $i$; the users, the shuffler and the analyzer perform $IQ$ instances of $\Pi$, thus incurring $IQ$ times its computational and communication cost; the the final KDE evaluation part evaluates $g_i$ on the query $y$ for every $i\in[I]$. \qed

\subsubsection{Variants of \Cref{thm:main}}\label{sec:variants}
The foregoing proof of \Cref{thm:main} can be adapted in various ways to accommodate bitsum protocols with different properties than those stated in the theorem. For example,
\begin{CompactItemize}
    \item If the accuracy guarantee of $\Pi$ is given in terms of absolute error rather than RMSE (as in \citet{cheu2019distributed}), the proof can be repeated with bounding the supremum absolute error of Algorithm~\ref{alg:shufdpkde} instead of its supRMSE (this yields a very similar and somewhat simpler version of the proof given above).
    \item If $\Pi$ has a pure DP guarantee, the advanced composition step in the privacy analysis from \Cref{sec:finishing} can be replaced by standard pure composition, resulting in $\varepsilon=IS\varepsilon_0$ (compare this to $\varepsilon\sim\sqrt{I}S\varepsilon_0$ in \Cref{thm:main}) and $\delta=0$. In the analogous instantiation of \Cref{thm:gaussian}, the lower bound on the error $\alpha$ changes from $\sqrt{\log(1/\delta)}/(\varepsilon n)$ to $1/\sqrt{\varepsilon n}$. 
    \item If $\Pi$ is not unbiased, the proof (specifically \Cref{sec:secondsummand}) can be slightly modified to accommodate its bias, resulting in a corresponding term in the final supRMSE bound.
\end{CompactItemize}


\subsection{Proof of \Cref{thm:gaussian}}\label{sec:fullgaussian}
\begin{algorithm}[t]
 \caption{Shuffled DP Gaussian KDE protocol, based on either RR or 3NB bitsum protocol} 
\label{alg:shufdpkdegaussian}
 \begin{multicols}{2}
 \DontPrintSemicolon
  %\LinesNumbered
  %\KwOutput{Rank-$k$ approximation $\tilde A\in\R^{n\times d}$ of $A$.}
  \myinit{\textit{$\;\;$// all data here is public}}
  {
    \KwInput{integer $I>0$; \\
    parameters for RR bitsum: $p_{RR}\in(0,1)$; \\
    parameters for 3NB bitsum: $r,r',p,p'>0$}
    \vspace{5pt}
    \For{$i=1,\ldots,I$}{
      \textit{// i.i.d.~samples using shared/public randomness:}\;
      $\omega_i\sim N(0,\mathbb I_d)\;\;\;\;$ \textit{// $d$-dim normal r.v.}\;
      $\beta_i\sim\mathrm{Uniform}[0,2\pi)$\;
    }
    \KwPublish{$\omega_i$ and $\beta_i$ for all $i$}
  }
  \vspace{5pt}
  \myrandomizer{\textit{$\;\;$// each user runs this locally with private randomness}}{
    \KwInput{private data point $x\in\R^d$}
     \For{$i=1,\ldots,I$}{
       $\varphi_i\leftarrow\cos(\sqrt2\omega_i^Tx + \beta_i)$\;
       $b_i\sim\mathrm{Bernoulli}((1+\varphi_i)/2)$\;
       \If{bitsum protocol is RR}{
         $b_i\leftarrow$ flip with probability $p_{RR}$\;
         send $(b_i,i)$ to the shuffler\;
       }
       \If{bitsum protocol is 3NB}{
         $\psi_1\sim \mathrm{NegativeBinomial}(r,p)$\;
         $\psi_2\sim \mathrm{NegativeBinomial}(r,p)$\;
         $\psi_3\sim \mathrm{NegativeBinomial}(r',p')$\;
         \For{$j=1,\ldots,b_i+\psi_1+\psi_3$}{
           send $(1,i)$ to the shuffler\;
         }
         \For{$j=1,\ldots,\psi_2+\psi_3$}{
           send $(-1,i)$ to the shuffler\;
         }
       }
    }
  }
  \myanalyzer{\textit{$\;\;$// runs after the shuffler; analyzer is the same for both RR and 3NB bitsum protocols}}{
    \KwInput{shuffled sequence of messages $\widetilde\Gamma$ from $n$ users}
    \For{$i=1,\ldots,I$}{
        $\widetilde B_i\leftarrow0$\;
    }
    \For{message $(\gamma,i)$ in $\widetilde\Gamma$}{
        $\widetilde B_i\leftarrow\widetilde B_i + \gamma$\;
    }
    \For{$i=1,\ldots,I$}{
        $\widetilde F_i\leftarrow 2\widetilde B_i-n$\;
    }
    \KwPublish{$\widetilde F_i$ for all $i$}
  }
  \vspace{5pt}
  \myquery{\textit{$\;\;$// runs on the analyzer's published output arbitrarily many times}}{
    \KwInput{query point $y\in\R^d$}
    \KwReturn{$\frac{2}{nI}\sum_{i=1}^I\widetilde F_i\cdot\cos(\sqrt2\omega_i^Ty + \beta_i)$}
  }
  \end{multicols}
\end{algorithm}

We restate \Cref{thm:gaussian} and prove it as a corollary of \Cref{thm:main}.
The corresponding protocol for Gaussian KDE is Algorithm~\ref{alg:shufdpkdegaussian} with the choice of 3NB as the bitsum protocol.\footnote{For completeness, Algorithm~\ref{alg:shufdpkdegaussian} also specifies how to use RR as the bitsum protocol. The flip probability $p_{RR}$ should be set according to Lemma 4.8 in \citet{chen2020distributed}.}
\begin{theorem}[\Cref{thm:gaussian}, restated]
There are constants $C,C'>0$ such that the following holds. 
    Let $\delta\in(0,1)$ and $\varepsilon\leq C\log(1/\delta)$. 
    For every $\alpha\geq C'\sqrt{\log(1/\delta)}/(\varepsilon n)$, there is an $(\varepsilon,\delta)$-DP Gaussian KDE protocol in the shuffled DP model (under the communication-threat model) with $n$ users and inputs from $\R^d$, which has: supRMSE $\alpha$, user running time $\min(O(d/\alpha^2), \tilde O(d+1/\alpha^4))$, expected communication of $\tilde O(1/\alpha^2)$ bits per user, expected analyzer running time $O(n/\alpha^2)$, KDE query time $\min(O(d/\alpha^2), \tilde O(d+1/\alpha^4))$, and optimal bit-width $1$. 
\end{theorem}
\begin{proof}
Recall that the Gaussian kernel has an LSQ family with $\beta=0$, $Q=S=1$, $R=\sqrt{2}$ by random Fourier features. 
For clarity, we mostly suppress constants in this proof. % and scale them appropriately in the end. 
For the given $\varepsilon,\delta,\alpha$ in \Cref{thm:gaussian}, we set the parameters in \Cref{thm:main} as follows: 
\[ I = \lceil\frac{1}{\alpha^2}\rceil \;\; ; \;\; \varepsilon_0 = \frac{\varepsilon}{\sqrt{I\log(1/\delta)}} \;\; ; \;\; \delta_0 = \frac{\delta}{2I} \;\; ; \;\; \delta'=\delta/2 . \]

\textbf{Privacy:}
It can be easily checked that plugging the above setting of parameters into the composed privacy parameters in \Cref{thm:main} yields an $(\varepsilon,\delta)$-DP guarantee in the shuffled model, provided that the given bitsum protocol $\Pi$ is $(\varepsilon_0,\delta_0)$-DP. 

To this end, we use the 3NB bitsum protocol from \citet{ghazi2020private} as $\Pi$, since it has near-optimal accuracy with low communication overhead. 
3NB has four parameters $r,r',p,p'$ (see Algorithm~\ref{alg:shufdpkdegaussian}) that \citet{ghazi2020private} show how to set to ensure the protocol is $(\varepsilon_0,\delta_0)$-DP in the shuffled model. Namely, they prove that setting $r=1/n$, $p=e^{-0.99\varepsilon_0}$, $r'=3(1+\log(2e^{0.99\varepsilon_0}/\delta_0))$, $p'=e^{-\Theta(1)\cdot\varepsilon_0/(\varepsilon_0 + \log(1/\delta_0))}$ guarantees 3NB is $(O(\varepsilon_0),O(\delta_0))$-DP, and the constants can be scaled so it is $(\varepsilon_0,\delta_0)$-DP. 

\textbf{Accuracy:}
The 3NB protocol is unbiased and has RMSE $\Theta(1/\varepsilon_0)=\Theta(\sqrt{\log(1/\delta)}/(\alpha\varepsilon))$. Plugging this into the supRMSE in \Cref{thm:main}, we get supRMSE $O\left(\sqrt{\alpha^2(1 + \log(1/\delta)/(\alpha\varepsilon n)^2)}\right)$ in Algorithm~\ref{alg:shufdpkdegaussian}. By the bound on $\alpha$ in the statement of \Cref{thm:gaussian}, this supRMSE is at most $O(\alpha)$, and we can scale the constants to get supRMSE $\alpha$. 

\textbf{Efficiency:}
We recall that in the 3NB protocol from \citet{ghazi2020private}, each user runs in $O(1)$ time and sends an expected nunber of $1+o(1)$ messages of $O(1)$ bits each, which the analyzer iterates over in time $O(n)$. Since we have $I=O(1/\alpha^2)$ instances of this protocol, each message needs to include $O(\log(1/\alpha))$ additional bits to identify which protocol instance it belongs to, yielding $O(\log(1/\alpha)/\alpha^2)$ expected bits of communication per user, and expected analyzer running time $O(n/\alpha^2)$. 

Each user also needs to compute the inner product $\omega_i^Tx$ for every $i\in[I]$. Similarly, the KDE query algorithm needs to compute $\omega_i^Ty$ for every $i\in[I]$. This takes time $O(d)$ per inner product, for a total of $O(dI)=O(d/\alpha^2)$ time. If $d\gg1/\alpha^2$, this time bound can be improved by using the faster preprocessing result of \citet{backurs2024efficiently}, who showed that one can first do a random projection of $x$ (for each user input $x$) and $y$ (for each KDE query $y$) onto $O(\log^2(1/\alpha)/\alpha^2)$ dimensions, and thus only distort the final DP KDE error up to a multiplicative constant (that can again be scaled). As shown by \citet{backurs2024efficiently}, the random projection can be done in time $\tilde O(d+1/\alpha^2)$ by the fast Johnson-Lindenstrauss transform \citep{ailon2009fast}, and then each of the $I=O(1/\alpha^2)$ inner products takes time $\tilde O(1/\alpha^2)$, for a total of $\tilde O(d+1/\alpha^4)$ time per user and per KDE query.
\end{proof}

\subsection{Inner Product LSQ}\label{sec:iplsq}
The inner product kernel $\kk(x,y)=x^Ty$ is trivially $(d,1,d)$-LSQable for unit length embeddings, by letting the LSQ family include a single pair of functions $(f,g)$ such that both are the identity over $\R^d$. We now observe it is also $(1,\sqrt d,1)$-LSQable. This allows better control over the privacy parameters and computational cost of the protocol in \Cref{thm:main}, since they depend on the parameters $S$ and $Q$ (respectively) of the $(Q,R,S)$-LSQ family.

To sample a pair $(f,g)\sim\mathcal Q$, we sample a vector $(\sigma_1,\ldots,\sigma_d)\in\{-1,1\}^d$ of i.i.d.~uniformly random signs, and let both $f$ and $g$ be the function $\R^d\rightarrow\R$ that maps $x=(x_1,\ldots,x_i)$ to $\sum_{i=1}^d\sigma_ix_i$. 
It is straightforward to check that $\E[f(x)^Tg(y)]=x^Ty$ for every $x,y\in\R^d$. To determine the upper bound $R$ on the only coordinate of $f(x)$, we observe, $|f(x)|=|\sum_{i=1}^d\sigma_ix_i|\leq\norm{x}_1\leq\sqrt{d}\norm{x}_2=\sqrt{d}$, since $x$ is unit length (in Euclidean norm). 

\section{More on Shuffled DP Summation}

\subsection{Bitsum Protocols}\label{sec:bitsumappendix}
In this section we describe some common techniques behind shuffled DP bitsum protocols, including the ones we use in our experiments (RR, 3NB and Pure). 

One bitsum protocol is the classical \emph{randomized response} (RR) \citep{warner1965randomized}: each user $i$ locally flips her bit $b_i$ with some probability, and sends the resulting bit to the shuffler. The analyzer received the anonymized received bits from the shuffler, and simply releases their sum. While originally introduced for local DP, \cite{cheu2019distributed} showed that in the shuffled DP model, the flip probability can be significantly smaller, leading to much better accuracy. 

Another popular technique for shuffled DP bitsums, which is the one underlying 3NB and Pure, is \emph{noise divisibility} \citep{goryczka2015comprehensive,balle2019privacy,balle2020private,ghazi2020private,ghazi2020pure,ghazi2021differentially,kairouz2021distributed}. Each user $i$ locally adds noise $\nu_i$, sampled from a distribution carefully chosen so that the aggregate noise $\sum_i\nu_i$ from all users, after shuffling, is distributed in a way that ensures central DP. %(e.g., $\sum_i\nu_i$ could be distributed like a discrete Laplace). 
Thus, the shuffled DP protocol simulates central DP, by having each user contribute a piece of the total ``divisible'' requisite noise. 

To be concrete, we describe the single-distribution protocol from \citet{ghazi2020private}. In this protocol, each user $i$ samples a non-negative integer noise random variable $\nu_i$, and sends to the shuffler a stream of $b_i+\nu_i$ identical content-less messages (where $b_i$ is user $i$'s private bit). The analyzer receives the unified streams of messages from all users after shuffling. Since the messages are identical and are now stripped of both content and sender identities, the only information they convey is their count $\sum_{i}(b_i+\nu_i$), which is released as the bitsum estimate. This equals the true bitsum $\sum_{i}b_i$ plus a total noise of $\sum_i\nu_i$. Thus, to ensure shuffled DP, it suffices for the $\nu_i$s to be such that their sum $\sum_i\nu_i$ is distributed in a way that ensures central DP for $\sum_{i}b_i$. \citet{ghazi2020private} show this can be achieved by either a Poisson or a negative binomial distribution. The 3NB and Pure bitsum protocols are more involved applications of this basic technique, designed to achieve better accuracy, lower communication cost, and (in the case of Pure) a pure DP guarantee. 

\subsection{Bitsums vs.~Real Sums}\label{sec:bitvsreal}
As mentioned in \Cref{sec:related}, there is also ample work on shuffled DP protocol for real number summation (abbrev.~\emph{realsum}), where each user holds an input number in a bounded range (say, $[-1,1]$). In this appendix we expand on the choice to base our approach on bitsum rather than realsum protocols. The answer has two parts: (1) why there is little potential gain in real summation, (2) why there is substantial advatnage in bit summation.

\noindent\textbf{Little gain in realsums.} 
Ostensibly, Algorithm~\ref{alg:shufdpkde} and \Cref{thm:main} could have used realsum instead of bitsum protocols, obviating the need to discretizate the LSQ coordinates with randomized rounding. When summing $\ell$ real numbers in $[-1,1]$, discretization with randomized rounding generally leads to a Hoeffding-like error of order $\sqrt{\ell}$, which our protocol incurs. There are parameter regimes where shuffled DP realsum protocols are more accurate than bitsum protocols, avoiding this Hoeffding-like error, and thus it may seem like an avenue to improve \Cref{thm:main}. 

However, this is in fact not the case. In our protocol, summation serves as a subroutine. The true sum is not the target quantity; rather, the true sum is a random variable (sampled according to the LSQ family), which only approximates the target quantity against which error is measured (the true KDE). This LSQ approximation already incurs the Hoeffding-like error
 (it is “built-into” LSQ). Thus, if the discretized bitsums were to be replaced with a shuffled DP realsum protocol, the final KDE error would still be dominated by the Hoeffding-like error, and any improvement would be restricted to low-order terms. Improving the final KDE error asymptotically, if this is indeed possible, would require a different approach than LSQ to private KDE, and we are currently not aware of a way to improve the KDE error in the shuffled DP model. 

\noindent\textbf{Advangate of bitsums.} 
At the same time, discretization has its own important advantages in shuffled DP and distributed learning. This was discussed in \Cref{sec:shuffled_dp_considerations}. To recap, in practical applications of shuffled DP, numerical values need to be discretized, and their bit-width bounded, in order to properly control their accuracy and communication cost. This was among the main motivations of \citet{kairouz2021distributed} in developing their Distributed Discrete Gaussian (DDG) protocol for shuffled DP realsum, rather than using prior protocols for this task. The DDG facilitates bounding the bit-width (though it is not as low as $1$), and its error analysis accounts for discretization errors. Our approach, which does not need to solve generic real summation, but only the specific case of KDE, attains the optimal bit-width of $1$
 by using binary discretization followed by bit summation, and our error analysis too accounts for the discretization error.


\section{Full Experimental Results}

\paragraph{Ablation: Local DP.}

Figure~\ref{fig:ldp} displays a comparison of our shuffled DP method with local DP. The local DP baseline is obtained by taking our private KDE protocol (\Cref{thm:main}), and replacing the shuffled DP bitsum with classical randomized response, which satisfies local DP. For the most direct comparison, the local DP plots are displayed compared to the RR plots (i.e., the leftmost column) in Figure~\ref{fig:epslbl5_primary}. 

We recall that the difference between the methods is that in classical local DP RR (the dashed lines in the plot) \citep{warner1965randomized}, each user locally flips her bit with probability that depends on the desired privacy parameter $\varepsilon$, but is independent of the overall number of user in the protocol (local DP RR makes no assumptions on other participating users). In contrast, in shuffled DP RR (the solid plots) \citep{cheu2019distributed}, each user flips her bit with probability that depends both on $\varepsilon$ and on the total number of participating user $n$; the larger $n$ is, the smaller the flip probability needs to be, since in shuffled DP, the user assumes her bit would also be anonymized and ``hidden'' among the bits received from the other $n-1$ users. 

The results in Figure~\ref{fig:ldp} show that as expected, shuffled DP attains considerably higher downstream accuracy than local DP. 

\paragraph{Ablation: Effect of $\varepsilon_{\mathrm{lbl}}$.}
\Cref{tbl:epslbl_dbpedia,tbl:epslbl_agnews,tbl:epslbl_sst2,tbl:epslbl_cifar10} show the effect of varying $\varepsilon_{\mathrm{lbl}}$ on various settings on the four datasets, respectively.

\paragraph{Additional parameter settings.}
\Cref{fig:epslbl10_primary,fig:epslbl7_primary,fig:epslbl5_again,fig:epslbl3_primary} display private classification accuracy results for $\varepsilon_{\mathrm{lbl}}=10,7,5,3$ respectively (\Cref{fig:epslbl5_again} repeats \Cref{fig:epslbl5_primary} from the main paper for convenience).

\Cref{tbl:decoding5,tbl:decoding4,tbl:decoding3} present private class decoding results with $\varepsilon_{\mathrm{lbl}}=5$ and $\varepsilon\approx5.7,4.4,3.2$ respectively (\Cref{tbl:decoding3} is the full version of \Cref{tbl:decodingmain} from the main paper).

%%% Local DP %%%

\begin{figure}[ht]
\centering

\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/ldp_dbpedia_14.png}
    %\caption{DBPedia-14, RR}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/ldp_ag_news.png}
    %\caption{DBPedia-14, Pure}
\end{subfigure}\\

\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/ldp_sst2.png}
    %\caption{AG news, RR}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/ldp_cifar10.png}
    %\caption{AG news, Pure}
\end{subfigure}
\caption{Classification accuracy comparison with a local DP baseline (overlaid on the shuffled DP RR plots with $\varepsilon_{\mathrm{lbl}}=5$, from the leftmost column in Figure~\ref{fig:epslbl5_primary}).}
\label{fig:ldp}
\vspace{-0.1 in}
\end{figure}


%%% For ICLR reviewer %%%
\iffalse
\begin{figure}[ht]
\centering

\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/dbpedia_14_asym.png}
    %\caption{DBPedia-14, RR}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/ag_news_asym.png}
    %\caption{DBPedia-14, Pure}
\end{subfigure}\\

\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/sst2_asym.png}
    %\caption{AG news, RR}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/cifar10_asym.png}
    %\caption{AG news, Pure}
\end{subfigure}
\caption{Comparison of Gaussian KDE empirical error vs.~asymptotic bound.}
\label{fig:empirical_vs_theoretical}
\vspace{-0.1 in}
\end{figure}

\begin{figure}[ht]
\centering

\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/dbpedia_14_ip.png}
    %\caption{DBPedia-14, RR}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/ag_news_ip.png}
    %\caption{DBPedia-14, Pure}
\end{subfigure}\\

\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/sst2_ip.png}
    %\caption{AG news, RR}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/cifar10_ip.png}
    %\caption{AG news, Pure}
\end{subfigure}
\caption{DP KDE accuracy comparison of the $(d,1,d)$-LSQ and $(1,\sqrt d,1)$-LSQ families of the IP kernel.}
\label{fig:ip_lsqs}
\vspace{-0.1 in}
\end{figure}
\fi
%%%

\begin{table*}[ht]
{\renewcommand{\arraystretch}{2}
\caption{Effect of $\varepsilon_{lbl}$ on accuracy, DBPedia-14.}
\label{tbl:epslbl_dbpedia}
\begin{centering}
\small
\begin{tabular}{clc|lllll}
\toprule
   & & & \multicolumn{5}{c}{\% Classification accuracy ($\pm$std) with $\varepsilon_{lbl}$:}  \\
Kernel & Bitsum & $\varepsilon$   & $\infty$ & 7 & 5 & 3 & 1 \\ 
\midrule
\multirow{6}{*}{Gaussian} & RR & 2 & $75.5_{\pm1.1}$ & $76.1_{\pm1.8}$ & $73.6_{\pm1.1}$ & $57.1_{\pm2.6}$ & $15.2_{\pm1.0}$  \\
 & RR & 4.5 & $79.7_{\pm1.3}$ & $79.3_{\pm1.9}$ & $76.1_{\pm0.9}$ & $61.9_{\pm2.5}$ & $15.9_{\pm0.7}$  \\
 & RR & 7 & $79.5_{\pm1.5}$ & $79.1_{\pm0.8}$ & $77.4_{\pm1.7}$ & $63.6_{\pm1.5}$ & $17.6_{\pm0.7}$  \\
 & 3NB & 2 & $79.2_{\pm1.6}$ & $79.6_{\pm1.6}$ & $79.4_{\pm0.8}$ & $65.0_{\pm1.0}$ & $15.8_{\pm1.0}$  \\
 & 3NB & 4.5 & $80.3_{\pm1.5}$ & $78.4_{\pm0.9}$ & $76.9_{\pm1.1}$ & $64.7_{\pm1.3}$ & $15.9_{\pm1.4}$  \\
 & 3NB & 7 & $80.0_{\pm0.8}$ & $79.1_{\pm1.0}$ & $77.5_{\pm0.8}$ & $65.2_{\pm2.1}$ & $17.0_{\pm1.9}$   \\
% & Pure & 2 & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$  \\
% & Pure & 4.5 & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$  \\
% & Pure & 7 & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$  \\
\midrule
\multirow{6}{*}{IP} & RR & 2 & $27.0_{\pm0.8}$ & $27.3_{\pm4.9}$ & $23.0_{\pm1.8}$ & $15.4_{\pm1.8}$ & $7.8_{\pm1.2}$  \\
 & RR & 4.5 & $50.5_{\pm3.5}$ & $51.3_{\pm1.6}$ & $46.2_{\pm3.1}$ & $30.0_{\pm2.8}$ & $10.7_{\pm2.0}$  \\
 & RR & 7 & $65.4_{\pm2.1}$ & $66.7_{\pm2.0}$ & $63.0_{\pm1.6}$ & $45.2_{\pm3.3}$ & $12.0_{\pm1.8}$  \\
 & 3NB & 2 & $92.7_{\pm0.1}$ & $92.8_{\pm0.2}$ & $92.5_{\pm0.2}$ & $91.0_{\pm0.4}$ & $58.9_{\pm1.4}$  \\
 & 3NB & 4.5 & $93.1_{\pm0.1}$ & $93.1_{\pm0.0}$ & $93.1_{\pm0.1}$ & $92.5_{\pm0.3}$ & $72.2_{\pm1.2}$  \\
 & 3NB & 7 & $93.1_{\pm0.2}$ & $93.1_{\pm0.2}$ & $93.1_{\pm0.1}$ & $92.6_{\pm0.2}$ & $72.9_{\pm1.7}$  \\
% & Pure & 2 & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$  \\
% & Pure & 4.5 & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$  \\
% & Pure & 7 & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$  \\
\bottomrule
\end{tabular}
\end{centering}}
\end{table*}


\begin{table*}[ht]
{\renewcommand{\arraystretch}{2}
\caption{Effect of $\varepsilon_{lbl}$ on accuracy, AG News.}
\label{tbl:epslbl_agnews}
\begin{centering}
\small
\begin{tabular}{clc|lllll}
\toprule
   & & & \multicolumn{5}{c}{\% Classification accuracy ($\pm$std) with $\varepsilon_{lbl}$:}  \\
Kernel & Bitsum & $\varepsilon$   & $\infty$ & 7 & 5 & 3 & 1 \\ 
\midrule
\multirow{6}{*}{Gaussian} & RR & 2 & $60.5_{\pm3.6}$ & $60.4_{\pm2.4}$ & $61.7_{\pm0.8}$ & $57.2_{\pm2.4}$ & $36.3_{\pm2.2}$  \\
 & RR & 4.5 & $66.8_{\pm1.1}$ & $66.7_{\pm1.7}$ & $66.8_{\pm1.4}$ & $61.8_{\pm2.8}$ & $37.1_{\pm1.0}$  \\
 & RR & 7 & $67.7_{\pm0.8}$ & $67.9_{\pm1.6}$ & $67.9_{\pm1.4}$ & $62.9_{\pm2.7}$ & $40.7_{\pm1.6}$   \\
 & 3NB & 2 & $68.2_{\pm1.2}$ & $69.5_{\pm1.4}$ & $67.0_{\pm1.0}$ & $63.4_{\pm2.3}$ & $41.7_{\pm0.9}$  \\
 & 3NB & 4.5 & $68.7_{\pm1.1}$ & $69.1_{\pm1.2}$ & $68.5_{\pm1.1}$ & $63.3_{\pm2.5}$ & $41.0_{\pm2.6}$  \\
 & 3NB & 7 & $67.8_{\pm2.2}$ & $68.2_{\pm0.7}$ & $68.7_{\pm1.3}$ & $62.8_{\pm1.7}$ & $40.5_{\pm2.0}$   \\
% & Pure & 2 & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$  \\
% & Pure & 4.5 & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$  \\
% & Pure & 7 & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$  \\
\midrule
\multirow{6}{*}{IP} & RR & 2 & $30.3_{\pm5.8}$ & $33.5_{\pm1.7}$ & $35.7_{\pm2.7}$ & $31.1_{\pm2.0}$ & $25.4_{\pm3.0}$  \\
 & RR & 4.5 & $45.5_{\pm3.7}$ & $41.4_{\pm6.1}$ & $42.2_{\pm7.6}$ & $43.6_{\pm4.6}$ & $28.9_{\pm6.0}$   \\
 & RR & 7 & $50.5_{\pm3.7}$ & $48.8_{\pm4.8}$ & $50.9_{\pm4.4}$ & $41.9_{\pm3.9}$ & $31.5_{\pm3.3}$  \\
 & 3NB & 2 & $84.9_{\pm0.1}$ & $84.9_{\pm0.3}$ & $85.0_{\pm0.5}$ & $84.1_{\pm0.8}$ & $73.9_{\pm1.5}$  \\
 & 3NB & 4.5 & $85.9_{\pm0.4}$ & $85.9_{\pm0.4}$ & $85.6_{\pm0.3}$ & $85.6_{\pm0.2}$ & $79.5_{\pm1.6}$   \\
 & 3NB & 7 & $86.2_{\pm0.2}$ & $86.1_{\pm0.2}$ & $85.9_{\pm0.3}$ & $85.9_{\pm0.3}$ & $79.9_{\pm1.4}$   \\
% & Pure & 2 & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$  \\
% & Pure & 4.5 & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$  \\
% & Pure & 7 & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$  \\
\bottomrule
\end{tabular}
\end{centering}}
\end{table*}


\begin{table*}[ht]
{\renewcommand{\arraystretch}{2}
\caption{Effect of $\varepsilon_{lbl}$ on accuracy, SST2.}
\label{tbl:epslbl_sst2}
\begin{centering}
\small
\begin{tabular}{clc|lllll}
\toprule
   & & & \multicolumn{5}{c}{\% Classification accuracy ($\pm$std) with $\varepsilon_{lbl}$:}  \\
Kernel & Bitsum & $\varepsilon$   & $\infty$ & 7 & 5 & 3 & 1 \\ 
\midrule
\multirow{6}{*}{Gaussian} & RR & 2 & $68.2_{\pm2.9}$ & $66.7_{\pm3.3}$ & $69.0_{\pm2.8}$ & $69.1_{\pm1.7}$ & $61.2_{\pm3.7}$  \\
 & RR & 4.5 & $70.0_{\pm4.0}$ & $72.8_{\pm1.8}$ & $70.3_{\pm2.6}$ & $71.4_{\pm2.7}$ & $57.9_{\pm4.9}$   \\
 & RR & 7 & $73.1_{\pm1.3}$ & $74.3_{\pm2.5}$ & $72.9_{\pm3.2}$ & $72.1_{\pm1.2}$ & $61.9_{\pm1.8}$  \\
 & 3NB & 2 & $72.6_{\pm1.1}$ & $71.8_{\pm4.1}$ & $71.1_{\pm2.7}$ & $70.8_{\pm2.9}$ & $64.6_{\pm2.4}$  \\
 & 3NB & 4.5 & $71.5_{\pm3.7}$ & $74.1_{\pm2.0}$ & $72.2_{\pm1.0}$ & $71.5_{\pm2.2}$ & $61.1_{\pm4.6}$  \\
 & 3NB & 7 & $70.0_{\pm4.8}$ & $70.9_{\pm1.7}$ & $72.7_{\pm2.8}$ & $71.1_{\pm3.3}$ & $63.6_{\pm2.4}$  \\
% & Pure & 2 & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$  \\
% & Pure & 4.5 & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$  \\
% & Pure & 7 & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$  \\
\midrule
\multirow{6}{*}{IP} & RR & 2 & $27.0_{\pm0.8}$ & $27.3_{\pm4.9}$ & $23.0_{\pm1.8}$ & $15.4_{\pm1.8}$ & $7.8_{\pm1.2}$  \\
 & RR & 4.5 & $50.5_{\pm3.5}$ & $51.3_{\pm1.6}$ & $46.2_{\pm3.1}$ & $30.0_{\pm2.8}$ & $10.7_{\pm2.0}$  \\
 & RR & 7 & $65.4_{\pm2.1}$ & $66.7_{\pm2.0}$ & $63.0_{\pm1.6}$ & $45.2_{\pm3.3}$ & $12.0_{\pm1.8}$  \\
 & 3NB & 2 & $92.7_{\pm0.1}$ & $92.8_{\pm0.2}$ & $92.5_{\pm0.2}$ & $91.0_{\pm0.4}$ & $58.9_{\pm1.4}$  \\
 & 3NB & 4.5 & $93.1_{\pm0.1}$ & $93.1_{\pm0.0}$ & $93.1_{\pm0.1}$ & $92.5_{\pm0.3}$ & $72.2_{\pm1.2}$  \\
 & 3NB & 7 & $93.1_{\pm0.2}$ & $93.1_{\pm0.2}$ & $93.1_{\pm0.1}$ & $92.6_{\pm0.2}$ & $72.9_{\pm1.7}$  \\
% & Pure & 2 & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$  \\
% & Pure & 4.5 & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$  \\
% & Pure & 7 & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$ & $0.00_{\pm0.00}$  \\
\bottomrule
\end{tabular}
\end{centering}}
\end{table*}


\begin{table*}[ht]
{\renewcommand{\arraystretch}{2}
\caption{Effect of $\varepsilon_{lbl}$ on accuracy, CIFAR-10}
\label{tbl:epslbl_cifar10}
\begin{centering}
\small
\begin{tabular}{clc|lllll}
\toprule
   & & & \multicolumn{5}{c}{\% Classification accuracy ($\pm$std) with $\varepsilon_{lbl}$:}  \\
Kernel & Bitsum & $\varepsilon$   & $\infty$ & 7 & 5 & 3 & 1 \\ 
\midrule
\multirow{6}{*}{Gaussian} & RR & 1.5 & $24.1_{\pm1.8}$ & $21.6_{\pm1.6}$ & $21.4_{\pm1.6}$ & $16.9_{\pm2.4}$ & $11.6_{\pm0.6}$  \\
 & RR & 3 & $34.8_{\pm3.7}$ & $34.0_{\pm2.4}$ & $36.1_{\pm2.2}$ & $27.5_{\pm0.9}$ & $11.3_{\pm1.2}$  \\
 & RR & 4.7 & $46.7_{\pm0.2}$ & $45.2_{\pm2.1}$ & $44.3_{\pm4.0}$ & $36.8_{\pm2.0}$ & $16.7_{\pm1.3}$  \\
 & 3NB & 1.5 & $73.4_{\pm1.0}$ & $71.3_{\pm2.0}$ & $72.9_{\pm1.6}$ & $67.8_{\pm1.3}$ & $39.3_{\pm5.4}$  \\
 & 3NB & 3 & $73.1_{\pm0.9}$ & $72.2_{\pm1.5}$ & $71.9_{\pm2.2}$ & $67.7_{\pm1.2}$ & $42.8_{\pm3.5}$  \\
 & 3NB & 4.7 & $71.5_{\pm1.7}$ & $73.3_{\pm1.6}$ & $72.8_{\pm1.6}$ & $70.4_{\pm1.3}$ & $40.8_{\pm4.1}$  \\
\midrule
\multirow{6}{*}{IP} & RR & 1.5 & $11.4_{\pm1.6}$ & $10.7_{\pm1.4}$ & $10.6_{\pm2.1}$ & $12.0_{\pm1.6}$ & $10.3_{\pm2.0}$  \\
 & RR & 3 & $11.1_{\pm2.6}$ & $9.1_{\pm2.0}$ & $10.6_{\pm1.9}$ & $9.3_{\pm1.3}$ & $10.0_{\pm1.8}$  \\
 & RR & 4.7 & $12.0_{\pm1.7}$ & $8.6_{\pm1.8}$ & $10.7_{\pm1.8}$ & $8.7_{\pm1.8}$ & $9.8_{\pm1.5}$  \\
 & 3NB & 1.5 & $18.8_{\pm3.3}$ & $19.6_{\pm0.9}$ & $17.9_{\pm3.8}$ & $13.8_{\pm4.2}$ & $12.3_{\pm3.0}$  \\
 & 3NB & 3 & $30.4_{\pm4.0}$ & $28.7_{\pm2.4}$ & $26.6_{\pm5.1}$ & $24.4_{\pm2.2}$ & $12.2_{\pm2.1}$  \\
 & 3NB & 4.7 & $36.8_{\pm7.8}$ & $37.1_{\pm5.9}$ & $37.8_{\pm2.8}$ & $25.3_{\pm3.6}$ & $13.2_{\pm3.8}$  \\
\bottomrule
\end{tabular}
\end{centering}}
\end{table*}

%%% EPS LABEL 10 %%%

\begin{figure}[ht]
\centering

\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/dbpedia_14_rr_10.png}
    %\caption{DBPedia-14, RR}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/dbpedia_14_3negbin_10.png}
    %\caption{DBPedia-14, 3NB}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/dbpedia_14_pure_10.png}
    %\caption{DBPedia-14, Pure}
\end{subfigure}\\

\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/ag_news_rr_10.png}
    %\caption{AG news, RR}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/ag_news_3negbin_10.png}
    %\caption{AG news, 3NB}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/ag_news_pure_10.png}
    %\caption{AG news, Pure}
\end{subfigure}\\

\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/sst2_rr_10.png}
    %\caption{SST2, RR}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/sst2_3negbin_10.png}
    %\caption{SST2, 3NB}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/sst2_pure_10.png}
    %\caption{SST2, Pure}
\end{subfigure} \\

\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/cifar10_rr_10.png}
    %\caption{CIFAR-10, RR}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/cifar10_3negbin_10.png}
    %\caption{CIFAR-10, 3NB}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/cifar10_pure_10.png}
    %\caption{CIFAR-10, Pure}
\end{subfigure} \\

\begin{subfigure}[b]{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/legend.png}
    %\caption{(legend)}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
\end{subfigure}
\caption{Classification results with $\varepsilon_{\mathrm{lbl}}=10$}
\label{fig:epslbl10_primary}
\vspace{-0.1 in}
\end{figure}

%%% EPS LABEL 7 %%%

\begin{figure}[ht]
\centering

\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/dbpedia_14_rr_7.png}
    %\caption{DBPedia-14, RR}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/dbpedia_14_3negbin_7.png}
    %\caption{DBPedia-14, 3NB}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/dbpedia_14_pure_7.png}
    %\caption{DBPedia-14, Pure}
\end{subfigure}\\

\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/ag_news_rr_7.png}
    %\caption{AG news, RR}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/ag_news_3negbin_7.png}
    %\caption{AG news, 3NB}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/ag_news_pure_7.png}
    %\caption{AG news, Pure}
\end{subfigure}\\

\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/sst2_rr_7.png}
    %\caption{SST2, RR}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/sst2_3negbin_7.png}
    %\caption{SST2, 3NB}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/sst2_pure_7.png}
    %\caption{SST2, Pure}
\end{subfigure} \\

\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/cifar10_rr_7.png}
    %\caption{CIFAR-10, RR}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/cifar10_3negbin_7.png}
    %\caption{CIFAR-10, 3NB}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/cifar10_pure_7.png}
    %\caption{CIFAR-10, Pure}
\end{subfigure} \\

\begin{subfigure}[b]{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/legend.png}
    %\caption{(legend)}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
\end{subfigure}
\caption{Classification results with $\varepsilon_{\mathrm{lbl}}=7$}
\label{fig:epslbl7_primary}
\vspace{-0.1 in}
\end{figure}

%%% EPS LABEL 5 %%%

\begin{figure}[ht]
\centering

\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/dbpedia_14_rr_5.png}
    %\caption{DBPedia-14, RR}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/dbpedia_14_3negbin_5.png}
    %\caption{DBPedia-14, 3NB}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/dbpedia_14_pure_5.png}
    %\caption{DBPedia-14, Pure}
\end{subfigure}\\

\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/ag_news_rr_5.png}
    %\caption{AG news, RR}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/ag_news_3negbin_5.png}
    %\caption{AG news, 3NB}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/ag_news_pure_5.png}
    %\caption{AG news, Pure}
\end{subfigure}\\

\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/sst2_rr_5.png}
    %\caption{SST2, RR}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/sst2_3negbin_5.png}
    %\caption{SST2, 3NB}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/sst2_pure_5.png}
    %\caption{SST2, Pure}
\end{subfigure} \\

\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/cifar10_rr_5.png}
    %\caption{CIFAR-10, RR}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/cifar10_3negbin_5.png}
    %\caption{CIFAR-10, 3NB}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/cifar10_pure_5.png}
    %\caption{CIFAR-10, Pure}
\end{subfigure} \\

\begin{subfigure}[b]{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/legend.png}
    %\caption{(legend)}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
\end{subfigure}
\caption{Classification results with $\varepsilon_{\mathrm{lbl}}=5$ (this is a copy of Figure~\ref{fig:epslbl5_primary} for convenience)}
\label{fig:epslbl5_again}
\vspace{-0.1 in}
\end{figure}

%%% EPS LABEL 3

\begin{figure}[ht]
\centering

\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/dbpedia_14_rr_3.png}
    %\caption{DBPedia-14, RR}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/dbpedia_14_3negbin_3.png}
    %\caption{DBPedia-14, 3NB}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/dbpedia_14_pure_3.png}
    %\caption{DBPedia-14, Pure}
\end{subfigure}\\

\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/ag_news_rr_3.png}
    %\caption{AG news, RR}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/ag_news_3negbin_3.png}
    %\caption{AG news, 3NB}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/ag_news_pure_3.png}
    %\caption{AG news, Pure}
\end{subfigure}\\

\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/sst2_rr_3.png}
    %\caption{SST2, RR}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/sst2_3negbin_3.png}
    %\caption{SST2, 3NB}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/sst2_pure_3.png}
    %\caption{SST2, Pure}
\end{subfigure} \\

\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/cifar10_rr_3.png}
    %\caption{CIFAR-10, RR}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/cifar10_3negbin_3.png}
    %\caption{CIFAR-10, 3NB}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{figures/cifar10_pure_3.png}
    %\caption{CIFAR-10, Pure}
\end{subfigure} \\

\begin{subfigure}[b]{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/legend.png}
    %\caption{(legend)}
\end{subfigure} \hspace{0.2in}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
\end{subfigure}
\caption{Classification results with $\varepsilon_{\mathrm{lbl}}=3$}
\label{fig:epslbl3_primary}
\vspace{-0.1 in}
\end{figure}


%%%%%%

\clearpage


\begin{table*}
\caption{Private class decoding results with $\varepsilon_{\mathrm{lbl}}=5$ and $\varepsilon\approx5.7$} \label{tbl:decoding5}
%{\renewcommand{\arraystretch}{1.5}
\begin{centering}
%\setlength\extrarowheight{-3pt}
\scriptsize
%\begin{adjustwidth}{-0.3in}{-0.1in}
\begin{tabular}
{p{0.08\linewidth}p{0.06\linewidth}p{0.03\linewidth}p{0.34\linewidth}p{0.34\linewidth}}
\toprule
 \textbf{Dataset} & \textbf{Class} & \textbf{Bitsum} & \textbf{Gaussian KDE class decoding} & \textbf{IP KDE class decoding} \\
\midrule

\multirow{14}{*}{DBPedia-14} & \multirow{3}{0.1\linewidth}{Company} & RR & seacorp, gencorp, southcorp & xuande, xinmi, xue \\
 &  & 3NB & europacorp, telekomunikasi, southcorp & companys, railcorp, interactivecorp \\
 &  & Pure & storebrand, railcorp, onbancorp & companys, companies, nokiacorp \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Artist} & RR & mandolinist, bosacki, cofounder & ricketson, attributor, vijayendra \\
 &  & 3NB & author, novelist, writer & author, musician, biographically \\
 &  & Pure & raymonde, bertogliati, bonifassi & author, roberthoudin, musician \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Office holder} & RR & legislator, politician, mulroney & cabinetmaker, chatham, provost \\
 &  & 3NB & ministerpresident, legislator, pastpresident & ministerpresident, legislator, congressperson \\
 &  & Pure & legislator, reelections, senatorial & ministerpresident, governorsgeneral, legislator \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Building} & RR & proctorville, connellsville, fargomoorhead & friedrichwilhelmsuniversitt, nordwestmecklenburg, brandenburgbayreuth \\
 &  & 3NB & galehouse, hyannisport, beaconhouse & churchville, reisterstown, jeffersontown \\
 &  & Pure & headquarters, northcote, northvale & holyroodhouse, reisterstown, beaconhouse \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Village} & RR & szewczenko, przodkowo, wodiczko & manasse, jeram, esfahan \\
 &  & 3NB & khuzistan, wojciechowice, szczawnica & khairabad, kyrghyzstan, kishanganj \\
 &  & Pure & kleveland, kurdamir, diyarbakirspor & kyrghyzstan, khairabad, yusefabad \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Plant} & RR & araucariaceae, rubiaceae, araceae & succulents, cunoniaceae, chaetophoraceae \\
 &  & 3NB & celastraceae, rubiaceae, cactaceae & chenopodiaceae, chaetophoraceae, araucariaceae \\
 &  & Pure & sapindaceae, violaceae, chenopodiaceae & chenopodiaceae, loranthaceae, gesneriaceae \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Film} & RR & screenplay, movie, filmore & dishonoring, dishonor, inglorious \\
 &  & 3NB & toyland, musketeers, imdb & movie, screenplay, biopic \\
 &  & Pure & screenplay, casablanca, filmmakers & movie, sicario, screenplay \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Educational institution} & RR & schoolcollege, secondaryschool, boardingschool & humboldtuniversitt, everetts, aleksandrw \\
 &  & 3NB & bryancollege, boardingschool, schoolship & schoolcollege, boardingschool, polytechnic \\
 &  & Pure & boardingschool, publicschool, allschool & schoolcollege, boardingschool, polytechnic \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Athlete} & RR & ajanovic, jovanovski, miloevi & bohuslav, denverbased, petersen \\
 &  & 3NB & sportsperson, gillenwater, khairuddin & sportsperson, footballer, handballer \\
 &  & Pure & alifirenko, kovalenko, ilyushenko & laliashvili, jamalullail, footballer \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Mean of transport} & RR & warship, troopships, aircrafts & curtisswright, veteran, pilotless \\
 &  & 3NB & fleetness, warship, shipmasters & warship, frigate, torpedoboat \\
 &  & Pure & warship, battleships, sailed & warship, landcruiser, torpedoboat \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Natural place} & RR & beringen, freshwater, merideth & bernardini, intermountain, varangians \\
 &  & 3NB & villeurbanne, riverina, curwensville & river, danube, rivermaya \\
 &  & Pure & fergushill, danube, waldenburg & floodplain, river, rivermaya \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Animal} & RR & coraciidae, caeciliidae, cicadellidae & columbellidae, marginellidae, caractacus \\
 &  & 3NB & carangidae, fasciolariidae, scolopacidae & leiothrichidae, marginellidae, phasianellidae \\
 &  & Pure & coccinellidae, coraciidae, cardinalidae & dendrobatidae, margaritidae, catostomidae \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Album} & RR & discography, vocals, stereophonics & album, korn, groupie \\
 &  & 3NB & album, instrumentals, tracklist & album, discography, tracklist \\
 &  & Pure & discography, pledgemusic, vocals & album, discography, allmusic \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Written work} & RR & biographies, storybook, author & booksurge, huilai, huizhou \\
 &  & 3NB & bibliography, biographies, autobiography & nonfiction, author, biographies \\
 &  & Pure & wittgenstein, werman, fangoria & nonfiction, author, bibliography \\
\midrule
\multirow{4}{*}{AG news} & \multirow{3}{0.1\linewidth}{Sports} & RR & winningest, standings, playoff & mccolm, mccartt, inconclusive \\
 &  & 3NB & huels, kurkjian, darrington & playoff, championship, standings \\
 &  & Pure & gallardo, unfit, basketball & playoff, postseason, runsgriffey \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Business} & RR & theba, buybacks, kulikowski & surcharging, nonhazardous, surcharges \\
 &  & 3NB & retrials, clawbacks, revaluation & nasdaq, enron, divestitures \\
 &  & Pure & nasdaq, exxonmobil, exxon & nasdaq, divestitures, nyse \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{World} & RR & terrorist, bombings, ilghazi & ppas, atpranking, deng \\
 &  & 3NB & hostages, baghdadi, nabaa & terrorists, militants, qaeda \\
 &  & Pure & qaeda, iraqstld, antifur & qaeda, ceasefire, intifadas \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Sci/Tech} & RR & ibm, loango, launchpads & ough, iebc, oul \\
 &  & 3NB & microsoft, viacom, protv & microsoft, ibm, lucenttech \\
 &  & Pure & edgeware, proximity, shareware & ibm, infotrends, suntec \\
\midrule
\multirow{2}{*}{SST2} & \multirow{3}{0.1\linewidth}{Negative} & RR & overstating, dreadful, awfulness & gameplay, tactics, underplaying \\
 &  & 3NB & derailments, vagueness, breakage & blandness, dramaturgy, comedy \\
 &  & Pure & perversities, sentimentalism, overthinking & tragedy, blandness, melodrama \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Positive} & RR & fervor, phenomenom, pageantry & embeddable, imbedding, embed \\
 &  & 3NB & cinema, screenplays, films & evocative, salaciousness, theatricality \\
 &  & Pure & majestically, dramatization, shrewdness & memorability, evocative, masterpieces \\
\bottomrule
\end{tabular}
%\end{adjustwidth}
\end{centering}%}
\end{table*}


\begin{table*}
\caption{Private class decoding results with $\varepsilon_{\mathrm{lbl}}=5$ and $\varepsilon\approx4.4$} \label{tbl:decoding4}
%{\renewcommand{\arraystretch}{1.5}
\begin{centering}
%\setlength\extrarowheight{-3pt}
\scriptsize
%\begin{adjustwidth}{-0.3in}{-0.1in}
\begin{tabular}
{p{0.08\linewidth}p{0.06\linewidth}p{0.03\linewidth}p{0.34\linewidth}p{0.34\linewidth}}
\toprule
 \textbf{Dataset} & \textbf{Class} & \textbf{Bitsum} & \textbf{Gaussian KDE class decoding} & \textbf{IP KDE class decoding} \\
\midrule

\multirow{14}{*}{DBPedia-14} & \multirow{3}{0.1\linewidth}{Company} & RR & companys, manufacturera, comcorp & airgroup, aerosystems, bluepoint \\
 &  & 3NB & manufactories, subsidiaries, originators & companys, railcorp, baycorp \\
 &  & Pure & railcorp, companys, comapny & companys, companies, manufactories \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Artist} & RR & singer, musician, ewan & marxer, auditioner, surinder \\
 &  & 3NB & balladeer, musician, artiste & author, musician, novelist \\
 &  & Pure & author, artist, composers & author, biographically, musician \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Office holder} & RR & mcclelland, mclellan, dreiberg & janetta, janette, jadakiss \\
 &  & 3NB & representant, representan, representantes & legislator, ministerpresident, congressperson \\
 &  & Pure & bashiruddin, ministerpresident, lazarescu & politician, ministerpresident, liberhan \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Building} & RR & woodville, marksville, douglassville & poteet, hocutt, chestnutt \\
 &  & 3NB & stationhouse, randallstown, dovercourt & churchville, chapeltown, beaconhouse \\
 &  & Pure & headquarter, hyattsville, weaverville & holyroodhouse, fenchurch, charleswood \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Village} & RR & krakowiak, krzynowoga, lubliniec & kieslowski, radiolocation, blenkiron \\
 &  & 3NB & kyrghyzstan, khazakstan, diyarbakir & kyrghyzstan, khairabad, khuzistan \\
 &  & Pure & taleyarkhan, voivodeship, mieszkowice & khuzestan, kyrghyzstan, diyarbakir \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Plant} & RR & saxifragaceae, loranthaceae, sapotaceae & lauraceae, loganiaceae, annonaceae \\
 &  & 3NB & orobanchaceae, mycenaceae, bromeliaceae & chenopodiaceae, araucariaceae, loranthaceae \\
 &  & Pure & chenopodiaceae, podocarpaceae, cactaceae & cupressaceae, rubiaceae, chaetophoraceae \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Film} & RR & biopic, imdb, screenplay & dollywood, isoroku, rakotomanana \\
 &  & 3NB & biopic, movie, silmarillion & movie, screenplay, biopic \\
 &  & Pure & biopic, cinemax, films & movie, sicario, biopic \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Educational institution} & RR & ucda, madrassa, polytechnic & write, reflectometry, chathams \\
 &  & 3NB & schoolcollege, polytechnic, fachhochschule & schoolcollege, boardingschool, polytechnic \\
 &  & Pure & eduniversal, universits, universitat & schoolcollege, boardingschool, publicschool \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Athlete} & RR & borgne, romanowski, brzezinski & sobolewski, khatemi, wlosowicz \\
 &  & 3NB & laliashvili, gianluigi, pirlo & sportsperson, handballer, ivanovic \\
 &  & Pure & pejaevi, milanovic, tomashova & konashenkov, laliashvili, kalynychenko \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Mean of transport} & RR & battleships, navymarine, landcruiser & pinezhsky, pisetsky, ilyinsky \\
 &  & 3NB & spitfires, troopships, maersk & warship, landcruiser, frigate \\
 &  & Pure & warship, frigate, torpedo & warship, frigate, landcruiser \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Natural place} & RR & krauchanka, gaucelm, kotonowaka & halethorpe, mapplethorpe, chloropaschia \\
 &  & 3NB & danube, tributary, vilfredo & river, rivermaya, rivervale \\
 &  & Pure & soligorsk, vassilakis, nordgau & floodplain, danube, river \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Animal} & RR & coraciidae, glareolidae, phyllostomidae & mollusc, motacillidae, molluscan \\
 &  & 3NB & paludomidae, acrolepiidae, discodorididae & leiothrichidae, marginellidae, catostomidae \\
 &  & Pure & marginellidae, riodinidae, orthogoniinae & mantellidae, catostomidae, coraciidae \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Album} & RR & album, vanilli, europop & melancholy, poetica, majra \\
 &  & 3NB & allmusic, remixes, remixed & album, discography, allmusic \\
 &  & Pure & housemusic, tracklist, musicology & album, discography, tracklist \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Written work} & RR & booknotes, pulitzerprize, apocryphally & terrors, sarkies, dementyeva \\
 &  & 3NB & novelistic, magazine, novelist & nonfiction, author, novelist \\
 &  & Pure & authorites, novelette, novelist & nonfiction, bibliography, author \\
\midrule
\multirow{4}{*}{AG news} & \multirow{3}{0.1\linewidth}{Sports} & RR & lose, playoff, sportschannel & eddard, frankenfish, paeonian \\
 &  & 3NB & cbssportscom, hof, injuries & playoff, semifinalists, championship \\
 &  & Pure & byrd, garvin, deq & injury, guardino, tiedown \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Business} & RR & gencorp, walkout, comstock & citywest, epson, arkwright \\
 &  & 3NB & opec, sirri, toyota & nasdaq, stockholders, divestitures \\
 &  & Pure & nonactors, goldcorp, archconfraternity & outbids, stockpiling, outselling \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{World} & RR & collusion, dahle, kejie & yawner, oswalt, russ \\
 &  & 3NB & islamiah, taliban, hurghada & hamas, qaeda, taliban \\
 &  & Pure & bomb, nomination, warmongering & baghdadi, occupiers, militants \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Sci/Tech} & RR & baidu, microsoft, tencent & multicamera, intercambio, videoconferences \\
 &  & 3NB & ibm, httpwwwdaimlerchryslercom, computerware & microsoft, ibm, lucenttech \\
 &  & Pure & infotrends, ati, infogear & redesigns, ibm, lucenttech \\
\midrule
\multirow{2}{*}{SST2} & \multirow{3}{0.1\linewidth}{Negative} & RR & portrayal, dreariness, bleakness & repeated, twicetobeat, ringed \\
 &  & 3NB & murkiness, unexciting, vapidity & comedy, dramaturgy, dramaturgical \\
 &  & Pure & dramaturgy, portrayals, dramatising & unpleasantries, unpleasantness, deadness \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Positive} & RR & reworded, comedies, critiques & rastignac, ruderman, gritschuk \\
 &  & 3NB & memorability, rausing, miserables & masterpieces, salaciousness, evocative \\
 &  & Pure & intiative, artistical, screenplays & vividness, evocative, presence \\

\bottomrule
\end{tabular}
%\end{adjustwidth}
\end{centering}%}
\end{table*}




\begin{table*}
\centering
\caption{Private class decoding results with $\varepsilon_{\mathrm{lbl}}=5$ and $\varepsilon\approx3.2$ (full version of \Cref{tbl:decodingmain})} \label{tbl:decoding3}
%{\renewcommand{\arraystretch}{1.5}
\begin{centering}
%\setlength\extrarowheight{-3pt}
\scriptsize
%\begin{adjustwidth}{-0.3in}{-0.1in}
\begin{tabular}
{p{0.08\linewidth}p{0.06\linewidth}p{0.03\linewidth}p{0.34\linewidth}p{0.34\linewidth}}
\toprule
 \textbf{Dataset} & \textbf{Class} & \textbf{Bitsum} & \textbf{Gaussian KDE class decoding} & \textbf{IP KDE class decoding} \\
\midrule

\multirow{14}{*}{DBPedia-14} & \multirow{3}{0.8\linewidth}{Company} & RR & vendors, gencorp, servicers & firesign, wnews, usos \\
 &  & 3NB & molycorp, newscorp, mediacorp & companys, alicorp, interactivecorp \\
 &  & Pure & ameritech, alicorp, newscorp & alibabacom, oscorp, companies \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Artist} & RR & author, aristizabal, levesongower & catalani, macki, bacashihua \\
 &  & 3NB & artist, lyricists, musician & author, musician, roberthoudin \\
 &  & Pure & author, originator, mikhaylovsky & musician, artist, jacquesfranois \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Office holder} & RR & louislegrand, legislator, lawmaker & sulphide, exclude, sulked \\
 &  & 3NB & patiashvili, kumaritashvili, biographers & ministerpresident, legislator, congressperson \\
 &  & Pure & polinard, bobrzaski, politician & ministerpresident, politician, polian \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Building} & RR & reisterstown, benenson, hellertown & opened, mastered, poegaslavonia \\
 &  & 3NB & frenchtown, brookeville, kenansville & beaconhouse, manorville, reisterstown \\
 &  & Pure & huntingtonwhiteley, wrightstown, randallstown & hyattsville, roxboro, reisterstown \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Village} & RR & lalganj, balrampur, manikganj & baluchestan, jagiellonia, nidderdale \\
 &  & 3NB & pazardzhik, tzintzuntzan, khuzistan & kyrghyzstan, kalinske, kalinski \\
 &  & Pure & poniewozik, mieszkowice, czerniewice & kazemabad, diyarbakir, khoramabad \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Plant} & RR & chaetophoraceae, gentianaceae, rutaceae & chilensis, surinamensis, tampines \\
 &  & 3NB & cupressaceae, chaetophoraceae, podocarpaceae & chenopodiaceae, araucariaceae, loranthaceae \\
 &  & Pure & asclepiadaceae, cupressaceae, gentianaceae & chaetophoraceae, chenopodiaceae, araucariaceae \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Film} & RR & biopic, movie, screenplay & kaptai, kakhi, kaloi \\
 &  & 3NB & filmography, vanya, ghostbusters & movie, filmography, screenplay \\
 &  & Pure & filme, movie, videodrome & movie, film, filmmakers \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Educational institution} & RR & schoolcollege, boardingschool, allschool & eastern, marykane, kbe \\
 &  & 3NB & boardingschool, schoolcollege, publicschool & schoolcollege, boardingschool, polytechnic \\
 &  & Pure & schoolcollege, polytechnic, qschool & schoolcollege, publicschool, boardingschool \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Athlete} & RR & kovaleski, kaessmann, miroshnichenko & torstensson, torstenson, torlakson \\
 &  & 3NB & fabianski, tarnowski, bochenski & sportsperson, laliashvili, konashenkov \\
 &  & Pure & rightfielder, leftfielder, konashenkov & lukasiewicz, sportsperson, marcinkiewicz \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Mean of transport} & RR & warship, frigate, steamships & latrodectus, laax, herx \\
 &  & 3NB & landcruiser, warship, landships & warship, frigate, landcruiser \\
 &  & Pure & battlecruiser, warship, hmso & warship, landcruiser, connaught \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Natural place} & RR & tributary, riverina, river & bimota, miercoles, mientras \\
 &  & 3NB & langenlonsheim, nordwestmecklenburg, schweinfurt & rivermaya, river, danube \\
 &  & Pure & riverbeds, azkoitia, zaporozhian & lakernotes, river, riverina \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Animal} & RR & carangidae, caeciliidae, arctiidae & taricani, tardio, kambona \\
 &  & 3NB & leiothrichidae, coleoptera, acrolepiidae & marginellidae, limoniidae, catostomidae \\
 &  & Pure & coraciidae, poeciliidae, acrolepiidae & caeciliidae, heliozelidae, lasiocampidae \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Album} & RR & discography, sevenfold, album & roadster, approximant, pantocrator \\
 &  & 3NB & discography, album, allmusic & album, discography, allmusic \\
 &  & Pure & discography, album, allmusic & album, decemberists, song \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Written work} & RR & nonfiction, encyclopedia, nonfictional & sociolinguist, becc, sociolegal \\
 &  & 3NB & author, magazine, novelist & nonfiction, author, biographies \\
 &  & Pure & reganbooks, novelettes, fourbook & synopsis, nonfiction, biographies \\
\midrule
\multirow{4}{*}{AG news} & \multirow{3}{0.1\linewidth}{Sports} & RR & vizner, runnerups, dietrichson & ongeri, grandi, zarate \\
 &  & 3NB & injury, semifinalists, finalists & semifinalists, championship, standings \\
 &  & Pure & pensford, rematches, undefeated & chauci, teammates, nith \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Business} & RR & repurchases, downtrend, equitywatch & sneed, timesnews, anxiousness \\
 &  & 3NB & enrononline, investcorp, comcorp & stockholders, nasdaq, marketwatchcom \\
 &  & Pure & corporations, consolidations, consolidated & merger, divestiture, stockholders \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{World} & RR & iraqi, hamas, darfur & tym, asg, tyo \\
 &  & 3NB & hezbollah, hamas, iraqstld & hamas, terrorists, baghdadi \\
 &  & Pure & kutayev, qaeda, yanukovych & shamkir, barricading, samaritans \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Sci/Tech} & RR & snopes, cyberworks, hacktivists & meanings, collegefootballnewscom, multipolarity \\
 &  & 3NB & ibm, thermedics, flextech & microsoft, ibm, accenture \\
 &  & Pure & feedbacks, companywide, eurogroup & movedtech, techcrunch, swindlers \\
\midrule
\multirow{2}{*}{SST2} & \multirow{3}{0.1\linewidth}{Negative} & RR & beguile, inception, shallow & manipulating, uncouple, dissects \\
 &  & 3NB & melodrama, rawness, blandness & comedy, tastelessness, uneasiness \\
 &  & Pure & chumminess, meaningfulness, mootness & absurdities, chastisement, absurdity \\
\cmidrule{2-5}
 & \multirow{3}{0.1\linewidth}{Positive} & RR & kindliness, pleasantness, entertaining & enjoyments, academie, amusements \\
 &  & 3NB & salacious, movie, majestic & salaciousness, theatricality, memorability \\
 &  & Pure & spiritedness, spirited, perspicacious & exorcisms, fairytales, revisiting \\

\bottomrule
\end{tabular}
%\end{adjustwidth}
\end{centering}%}
\end{table*}


%%%%
\clearpage


\end{document}