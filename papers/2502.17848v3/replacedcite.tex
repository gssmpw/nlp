\section{Related Work}
% \subsection{Reasoning with LLMs}
% \subsection{Reflective Reasoning Evaluation}
\subsection{Reflection Capabilities of LLMs}
The impressive reasoning capabilities of LLMs have naturally led to increased interest in reflection, a more sophisticated and human-like form of reasoning. Previous studies primarily focus on evaluating LLMs' abilities to rectify their response based on explicit feedback, including self-improvement with critique prompts ____, leveraging external tools such as code interpreters or search engines ____, and engaging in multi-LLM interaction through debating ____. However, these works mainly evaluate LLMs' behaviors in response to feedback. They fail to assess LLMs' capabilities to spontaneously engage in the complete reflection process for complex reasoning tasks. Our proposed LR$^2$Bench provides scenarios necessitating capabilities, such as making assumptions, verification, backtracking, and self-refinement, thus filling a critical gap in evaluating LLMs' intrinsic reflective reasoning abilities.

% \subsection{Complex Reasoning Tasks for LLMs}
\subsection{Puzzle-solving for LLMs}
Puzzle-solving ____ offers valuable insight for evaluating the complex reasoning capabilities of LLMs across diverse scenarios. ____ explore Sudoku solving strategies with answer set programming. ____ leverages reinforcement learning and Monte Carlo Tree Search to solve problems like Game of 24, 8-Puzzle, and Pocket Cube. ____ introduces "Tree of Thought" to enable self-evaluating and backtracking for Game of 24 and Crosswords. ____ combines LLMs with symbolic solvers and program interpreters to complete first-order combinatorial reasoning problems. ____ focuses on grid puzzles to evaluate the generated reasoning chains of LLMs. Moreover, existing studies have also investigate Board game ____, Chess ____ and social games ____. However, these studies primarily leverage external tools or specialized algorithms to develop task-specific solutions within limited puzzle domains. In contrast, LR$^2$Bench provides diverse tasks and difficulty levels and focuses on evaluating the intrinsic reflective reasoning capabilities of LLMs.