\begin{table}[!h]
    \centering
    \resizebox{1.0\columnwidth}{!}{
    % \setlength{\tabcolsep}{3pt}
    \begin{tabular}{l|ccccc}
        \toprule
        \textbf{Model} & \textbf{CR} & \textbf{S-Acc} & \textbf{EM} & \textbf{PM-0.5} & \textbf{Tokens} \\
        \midrule
        \rowcolor{gray!15} \multicolumn{6}{l}{\textit{\textbf{Open-source LLMs}}}\\
        \midrule 
        Llama-3.1-8B-Instruct & 42.6 & 9.9 & 0.0 & 3.8 & 2,478 \\
        Llama-3.1-70B-Instruct & 71.8 & 27.4 & 0.5 & 21.9 & 2,090 \\
        Llama-3.3-70B-Instruct & 92.4 & 33.1 & 1.3 & 25.8 & 1,842 \\
        Mistral-7B-Instruct-v0.3 & 85.8 & 12.1 & 0.0 & 2.3 & 2,736 \\
        Mistral-Small-Instruct-2409 & 91.0 & 23.1 & 0.2 & 13.3 & 2,273 \\
        Mistral-Large-Instruct-2411 & 96.1 & \underline{36.4} & 2.5 & \underline{30.0} & 2,313 \\
        Qwen2.5-7B-Instruct & 85.1 & 17.7 & 0.3 & 5.1 & 2,086 \\
        Qwen2.5-32B-Instruct & \underline{96.2} & 29.9 & 0.6 & 14.8 & 1,924 \\
        Qwen2.5-72B-Instruct & 95.0 & 33.9 & 0.9 & 20.8 & 2,359 \\
        QwQ-32B-Preview & 65.0 & 26.6 & \underline{8.5} & 19.3 & 6,709 \\
        DeepSeek-R1 & \textbf{100.0} & \textbf{58.4} & \textbf{20.0} & \textbf{62.0} & 9,856 \\
        \midrule
        \rowcolor{blue!15} \multicolumn{6}{l}{\textit{\textbf{Closed-source LLMs}}}\\
        \midrule
        Gemini-2.0-flash & 81.1 & 37.0 & 2.4 & 34.5 & 2,637 \\
        Gemini-2.0-flash-thinking & 88.2 & 39.4 & 4.3 & 35.0 & 3,725 \\
        GPT-4o & \textbf{99.8} & \underline{43.7} & 3.2 & \underline{41.7} & 1,486 \\
        o1-mini & \underline{97.7} & 41.3 & \underline{9.1} & 32.8 & 9,576 \\
        o1-preview & 96.3 & \textbf{58.7} & \textbf{23.6} & \textbf{61.7} & 11,436 \\ 
        \bottomrule
    \end{tabular}}    
    \caption{Average performance (\%) across six tasks on LR${}^{2}$Bench. The best and second-best results are highlighted in \textbf{bold} and \underline{underlined}, respectively. "Tokens" denotes the average number of generated tokens.}
    \label{tab:main_exp_sum}
    % \vspace{-2mm}
\end{table}