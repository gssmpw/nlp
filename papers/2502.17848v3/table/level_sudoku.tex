\begin{table*}[t]
    \centering
    \resizebox{1.0\textwidth}{!}{
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{l|ccccc|ccccc|ccccc|ccccc}
        \toprule
        \multirow{2}{*}{\textbf{Model}} &  \multicolumn{5}{c|}{\textbf{Sudoku - $4\times4$, Easy}} & \multicolumn{5}{c|}{\textbf{Sudoku - $4\times4$, Hard}}  & \multicolumn{5}{c}{\textbf{Sudoku - $9\times9$, Easy}} & \multicolumn{5}{c}{\textbf{Sudoku - $9\times9$, Hard}} \\
        \cmidrule(l){2-21}
        & CR & S-Acc & EM & PM-0.5 & Tokens & CR & S-Acc & EM & PM-0.5 & Tokens & CR & S-Acc & EM & PM-0.5 & Tokens & CR & S-Acc & EM & PM-0.5 & Tokens \\
        \midrule
        \rowcolor{gray!15} \multicolumn{21}{l}{\textit{\textbf{Open-source LLMs}}}\\
        \midrule 
Llama-3.1-8B-Instruct & 4.0 & 0.8 & 0.0 & 0.0 & 931 & 6.0 & 1.3 & 0.0 & 0.0 & 1468 & 12.0 & 1.6 & 0.0 & 0.0 & 5051 & 8.0 & 1.2 & 0.0 & 0.0 & 3678 \\
Llama-3.1-70B-Instruct & 68.0 & 32.4 & 0.0 & 38.0 & 1142 & 60.0 & 31.5 & 4.0 & 32.0 & 1343 & 74.0 & 16.4 & 0.0 & 0.0 & 2514 & 76.0 & 16.7 & 0.0 & 0.0 & 2761 \\
Llama-3.3-70B-Instruct & 84.0 & 44.2 & 10.0 & 48.0 & 1253 & 90.0 & 48.9 & 18.0 & 42.0 & 1200 & \textbf{100.0} & 24.7 & 0.0 & 0.0 & 917 & \textbf{100.0} & 21.5 & 0.0 & 0.0 & 877 \\
Mistral-7B-Instruct-v0.3 & 96.0 & 19.6 & 0.0 & 4.0 & 1333 & {\ul 92.0} & 17.5 & 0.0 & 2.0 & 1413 & 78.0 & 6.0 & 0.0 & 0.0 & 4725 & 70.0 & 4.4 & 0.0 & 0.0 & 4962 \\
Mistral-Small-Instruct-2409 & \textbf{100.0} & 33.2 & 0.0 & 18.0 & 1351 & \textbf{100.0} & 30.2 & 2.0 & 12.0 & 1272 & 80.0 & 9.5 & 0.0 & 0.0 & 2400 & 76.0 & 9.1 & 0.0 & 0.0 & 2850 \\
Mistral-Large-Instruct-2411 & 94.0 & {\ul 68.2} & 26.0 & {\ul 78.0} & 1204 & {\ul 92.0} & 54.2 & 14.0 & 54.0 & 1235 & 86.0 & 21.0 & 0.0 & 2.0 & 2910 & 70.0 & 14.5 & 0.0 & 0.0 & 2472 \\
Qwen2.5-7B-Instruct & {\ul 98.0} & 42.0 & 2.0 & 42.0 & 937 & \textbf{100.0} & 36.2 & 4.0 & 16.0 & 951 & {\ul 96.0} & 25.5 & 0.0 & 2.0 & 2022 & 84.0 & 17.1 & 0.0 & 0.0 & 2034 \\
Qwen2.5-32B-Instruct & \textbf{100.0} & 53.8 & 12.0 & 60.0 & 1095 & \textbf{100.0} & 50.2 & 2.0 & 46.0 & 1158 & \textbf{100.0} & {\ul 34.9} & 0.0 & {\ul 8.0} & 1435 & \textbf{100.0} & {\ul 32.5} & 0.0 & {\ul 8.0} & 1119 \\
Qwen2.5-72B-Instruct & \textbf{100.0} & 60.8 & 16.0 & 72.0 & 1380 & \textbf{100.0} & 55.6 & 6.0 & 58.0 & 1520 & 94.0 & 28.4 & 0.0 & 2.0 & 2543 & {\ul 96.0} & 27.3 & 0.0 & 4.0 & 2610 \\
QwQ-32B-Preview & 66.0 & 63.8 & {\ul 60.0} & 64.0 & 3217 & 76.0 & {\ul 72.0} & {\ul 66.0} & {\ul 72.0} & 5119 & 44.0 & 15.3 & 0.0 & 6.0 & 12575 & 32.0 & 9.5 & 0.0 & 0.0 & 12613 \\
DeepSeek-R1 & \textbf{100.0} & \textbf{100.0} & \textbf{100.0} & \textbf{100.0} & 2878 & \textbf{100.0} & \textbf{100.0} & \textbf{100.0} & \textbf{100.0} & 3620 & \textbf{100.0} & \textbf{46.6} & 0.0 & \textbf{42.0} & 14491 & \textbf{100.0} & \textbf{34.5} & 0.0 & \textbf{14.0} & 12117 \\
        \midrule
        \rowcolor{blue!15} \multicolumn{21}{l}{\textit{\textbf{Closed-source LLMs}}}\\
        \midrule
Gemini-2.0-flash & {\ul 94.0} & 57.8 & 24.0 & 66.0 & 1437 & 92.0 & 58.9 & 26.0 & 60.0 & 1454 & {\ul 94.0} & 33.8 & 0.0 & 10.0 & 4194 & {\ul 92.0} & 30.6 & 0.0 & {\ul 14.0} & 4287 \\
Gemini-2.0-flash-thinking & 70.0 & 62.8 & 44.0 & 68.0 & 2009 & 66.0 & 50.0 & 22.0 & 54.0 & 2350 & 92.0 & \textbf{41.7} & 0.0 & \textbf{36.0} & 5511 & 90.0 & {\ul 31.3} & 0.0 & 6.0 & 5541 \\
GPT-4o & \textbf{100.0} & 68.0 & 26.0 & 84.0 & 1015 & \textbf{100.0} & 63.3 & 32.0 & 64.0 & 1039 & \textbf{100.0} & {\ul 39.7} & 0.0 & {\ul 26.0} & 1234 & \textbf{100.0} & \textbf{37.9} & 0.0 & \textbf{18.0} & 1127 \\
o1-mini & \textbf{100.0} & {\ul 85.4} & {\ul 66.0} & {\ul 88.0} & 3059 & {\ul 96.0} & {\ul 68.6} & {\ul 42.0} & {\ul 72.0} & 3403 & \textbf{100.0} & 32.9 & 0.0 & 10.0 & 4428 & \textbf{100.0} & 26.7 & 0.0 & 2.0 & 4953 \\
o1-preview & \textbf{100.0} & \textbf{100.0} & \textbf{100.0} & \textbf{100.0} & 5616 & \textbf{100.0} & \textbf{100.0} & \textbf{100.0} & \textbf{100.0} & 6191 & 86.0 & 34.5 & 0.0 & 16.0 & 10328 & 80.0 & 25.7 & 0.0 & 6.0 & 10111 \\
        \bottomrule
    \end{tabular}}    
    \caption{Performance (\%) of LLMs on Sudoku across all difficulty levels. The best and second-best results are highlighted in \textbf{bold} and \underline{underlined}, respectively. "Tokens" denotes the average number of generated tokens.}
    \label{level_sudoku}
\end{table*}