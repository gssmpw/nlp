\begin{table*}[t]
    \centering
    \resizebox{1.0\textwidth}{!}{
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{l|ccccc|ccccc|ccccc|ccccc}
        \toprule
        \multirow{2}{*}{\textbf{Model}} &  \multicolumn{5}{c|}{\textbf{Acrostic - Easy}} & \multicolumn{5}{c|}{\textbf{Acrostic - Hard}} & \multicolumn{5}{c|}{\textbf{Cryptogram - Easy}} & \multicolumn{5}{c|}{\textbf{Cryptogram - Hard}}\\
        \cmidrule(l){2-21}
        & CR & S-Acc & EM & PM-0.5 & Tokens & CR & S-Acc & EM & PM-0.5 & Tokens & CR & S-Acc & EM & PM-0.5 & Tokens & CR & S-Acc & EM & PM-0.5 & Tokens \\
        \midrule
        \rowcolor{gray!15} \multicolumn{21}{l}{\textit{\textbf{Open-source LLMs}}}\\
        \midrule 
Llama-3.1-8B-Instruct & 56.0 & 8.9 & 0.0 & 0.0 & 2587 & 30.0 & 2.2 & 0.0 & 0.0 & 4838 & 42.0 & 2.3 & 0.0 & 0.0 & 1977 & 44.0 & 2.2 & 0.0 & 0.0 & 2159 \\
Llama-3.1-70B-Instruct & 88.0 & 41.6 & 0.0 & 34.0 & 2504 & 80.0 & 30.0 & 0.0 & 8.0 & 4626 & 64.0 & 7.7 & 0.0 & 0.0 & 1465 & 60.0 & 6.0 & 0.0 & {\ul 2.0} & 1130 \\
Llama-3.3-70B-Instruct & \textbf{100.0} & {\ul 46.6} & 0.0 & {\ul 46.0} & 2514 & {\ul 94.0} & 35.0 & 0.0 & {\ul 10.0} & 4655 & {\ul 98.0} & 16.3 & 0.0 & 0.0 & 1135 & \textbf{100.0} & {\ul 12.3} & 0.0 & {\ul 2.0} & 1140 \\
Mistral-7B-Instruct-v0.3 & 84.0 & 9.5 & 0.0 & 0.0 & 3097 & 66.0 & 6.2 & 0.0 & 0.0 & 6103 & {\ul 98.0} & 3.4 & 0.0 & 0.0 & 1179 & \textbf{100.0} & 5.1 & 0.0 & 0.0 & 1012 \\
Mistral-Small-Instruct-2409 & 84.0 & 8.1 & 0.0 & 0.0 & 2839 & 50.0 & 2.9 & 0.0 & 0.0 & 5503 & 94.0 & 8.4 & 0.0 & 0.0 & 1307 & 96.0 & 5.6 & 0.0 & 0.0 & 1159 \\
Mistral-Large-Instruct-2411 & {\ul 96.0} & 44.8 & 0.0 & 36.0 & 2892 & \textbf{100.0} & 34.0 & 0.0 & 4.0 & 5667 & 94.0 & {\ul 18.3} & 0.0 & {\ul 2.0} & 1250 & {\ul 98.0} & 9.1 & 0.0 & 0.0 & 1158 \\
Qwen2.5-7B-Instruct & 58.0 & 5.5 & 0.0 & 0.0 & 2896 & 26.0 & 1.7 & 0.0 & 0.0 & 5422 & 84.0 & 4.8 & 0.0 & 0.0 & 1205 & 78.0 & 2.1 & 0.0 & 0.0 & 1158 \\
Qwen2.5-32B-Instruct & \textbf{100.0} & 32.3 & 0.0 & 2.0 & 2763 & \textbf{100.0} & 31.2 & 0.0 & 2.0 & 5383 & 88.0 & 9.9 & 0.0 & 0.0 & 1404 & 90.0 & 9.8 & 0.0 & 0.0 & 1202 \\
Qwen2.5-72B-Instruct & \textbf{100.0} & 41.7 & 0.0 & 30.0 & 2793 & \textbf{100.0} & {\ul 36.9} & 0.0 & 6.0 & 5429 & 86.0 & 14.9 & 0.0 & 0.0 & 1743 & 84.0 & 8.6 & 0.0 & 0.0 & 1710 \\
QwQ-32B-Preview & \textbf{100.0} & 36.4 & 0.0 & 12.0 & 3380 & {\ul 94.0} & 26.9 & 0.0 & 0.0 & 6549 & 42.0 & 4.1 & 0.0 & 0.0 & 6432 & 52.0 & 3.1 & 0.0 & 0.0 & 6551 \\
DeepSeek-R1 & \textbf{100.0} & \textbf{66.5} & 0.0 & \textbf{90.0} & 9095 & \textbf{100.0} & \textbf{57.8} & 0.0 & \textbf{76.0} & 11058 & \textbf{100.0} & \textbf{36.7} & \textbf{6.0} & \textbf{32.0} & 10404 & \textbf{100.0} & \textbf{15.3} & \textbf{2.0} & \textbf{10.0} & 10284 \\
        \midrule
        \rowcolor{blue!15} \multicolumn{21}{l}{\textit{\textbf{Closed-source LLMs}}}\\
        \midrule
Gemini-2.0-flash & \textbf{100.0} & 50.6 & 0.0 & 60.0 & 2722 & {\ul 96.0} & 45.4 & 0.0 & 36.0 & 5318 & 42.0 & 10.3 & 0.0 & 2.0 & 1844 & 52.0 & 6.7 & 0.0 & 0.0 & 1327 \\
Gemini-2.0-flash-thinking & \textbf{100.0} & 46.2 & 0.0 & 36.0 & 3027 & 84.0 & 35.3 & 0.0 & 18.0 & 5486 & 68.0 & 14.9 & 0.0 & 4.0 & 4244 & 68.0 & 7.5 & 0.0 & 0.0 & 4090 \\
GPT-4o & \textbf{100.0} & {\ul 58.5} & 0.0 & {\ul 74.0} & 2288 & \textbf{100.0} & {\ul 53.5} & 0.0 & {\ul 60.0} & 4171 & \textbf{100.0} & 25.4 & 0.0 & 8.0 & 750 & \textbf{100.0} & {\ul 15.9} & 0.0 & 2.0 & 729 \\
o1-mini & 98.0 & 37.5 & 0.0 & 24.0 & 10018 & {\ul 96.0} & 31.8 & 0.0 & 0.0 & 11885 & \textbf{100.0} & {\ul 30.9} & {\ul 2.0} & {\ul 20.0} & 11689 & \textbf{100.0} & 14.5 & 0.0 & {\ul 6.0} & 10727 \\
o1-preview & \textbf{100.0} & \textbf{68.3} & 0.0 & \textbf{90.0} & 13096 & \textbf{100.0} & \textbf{66.2} & 0.0 & \textbf{90.0} & 16598 & {\ul 96.0} & \textbf{48.9} & \textbf{18.0} & \textbf{42.0} & 12096 & {\ul 88.0} & \textbf{20.7} & \textbf{8.0} & \textbf{16.0} & 13039 \\
        \bottomrule
    \end{tabular}}    
    \caption{Performance (\%) of LLMs on Acrostic and Cryptogram across all difficulty levels. The best and second-best results are highlighted in \textbf{bold} and \underline{underlined}, respectively. "Tokens" denotes the average number of generated tokens.}
    \label{level_acrostic_cryptogram}
\end{table*}