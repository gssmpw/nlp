\begin{table*}[!t]
    \centering
    \renewcommand{\arraystretch}{1.0}\resizebox{1.0\textwidth}{!}{
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{l|ccccc|ccccc|ccccc}
        \toprule
        \multirow{2}{*}{\textbf{Model}} &  \multicolumn{5}{c|}{\textbf{Crossword}} & \multicolumn{5}{c|}{\textbf{Acrostic}}  & \multicolumn{5}{c}{\textbf{Logic Puzzle}} \\
        \cmidrule(l){2-16}
        & CR & S-Acc & EM & PM-0.5 & Tokens & CR & S-Acc & EM & PM-0.5 & Tokens & CR & S-Acc & EM & PM-0.5 & Tokens \\
        \midrule
        \rowcolor{gray!15} \multicolumn{16}{l}{\textit{\textbf{Open-source LLMs}}}\\
        \midrule 
        % \rowcolor{blue!15} GPT-4o-2024-08-06 & 54.82 & 898 & 29.71 & 457 & \textbf{42.88} & 1436 & \textbf{67.49} & 1581 & 47.87 & 1121 & \textbf{48.55} & 1098 \\
        Llama-3.1-8B-Instruct & 61.3 & 23.3 & 0.0 & 14.0 & 2,888 & 43.0 & 5.6 & 0.0 & 0.0 & 3,712 & 57.0 & 16.0 & 0.0 & 8.0 & 1,293 \\
        Llama-3.1-70B-Instruct & 77.3 & 46.8 & 0.0 & 62.0 & 3,072 & 84.0 & 35.8 & 0.0 & 21.0 & 3,565 & 56.0 & 22.8 & 2.0 & 18.0 & 1,165 \\
        Llama-3.3-70B-Instruct & 85.3 & 47.6 & 0.0 & 65.3 & 2,613 & 97.0 & \underline{40.8} & 0.0 & \underline{28.0} & 3,584 & 80.5 & 32.2 & 1.0 & 25.0 & 1,738 \\
        Mistral-7B-Instruct-v0.3 & 94.0 & 23.0 & 0.0 & 6.7 & 3,655 & 75.0 & 7.9 & 0.0 & 0.0 & 4,600 & 97.0 & 19.1 & 0.0 & 4.5 & 1,618 \\
        Mistral-Small-Instruct-2409 & 98.7 & 48.3 & 0.0 & 54.0 & 3,135 & 67.0 & 5.5 & 0.0 & 0.0 & 4,171 & \underline{99.5} & 30.7 & 0.5 & 12.5 & 1,514 \\
        Mistral-Large-Instruct-2411 & \underline{99.3} & \underline{62.8} & \underline{2.0} & \underline{86.0} & 3,237 & \underline{98.0} & 39.4 & 0.0 & 20.0 & 4,279 & \textbf{100.0} & 38.3 & 3.0 & 30.5 & 1,637 \\
        Qwen2.5-7B-Instruct & 98.7 & 21.1 & 0.0 & 3.3 & 2,441 & 42.0 & 3.6 & 0.0 & 0.0 & 4,159 & 96.5 & 25.8 & 0.0 & 8.5 & 1,396 \\
        Qwen2.5-32B-Instruct & \textbf{100.0} & 34.6 & 0.0 & 20.0 & 2,560 & \textbf{100.0} & 31.8 & 0.0 & 2.0 & 4,073 & 93.0 & 32.2 & 0.0 & 22.5 & 1,208 \\
        Qwen2.5-72B-Instruct & \textbf{100.0} & 44.1 & 0.0 & 36.7 & 2,735 & \textbf{100.0} & 39.3 & 0.0 & 18.0 & 4,111 & 93.5 & 34.0 & 0.0 & 23.0 & 1,810 \\
        QwQ-32B-Preview & 80.0 & 30.2 & 0.0 & 18.0 & 4,817 & 97.0 & 31.6 & 0.0 & 6.0 & 4,964 & 78.5 & \underline{46.3} & \underline{19.5} & \underline{48.0} & 9,524 \\
        DeepSeek-R1 & \textbf{100.0} & \textbf{75.4} & \textbf{16.7} & \textbf{94.0} & 9,810 & \textbf{100.0} & \textbf{62.2} & 0.0 & \textbf{83.0} & 10,077 & \textbf{100.0} & \textbf{69.4} & \textbf{42.5} & \textbf{68.0} & 9,205\\
        \midrule
        \rowcolor{blue!15} \multicolumn{16}{l}{\textit{\textbf{Closed-source LLMs}}}\\
        \midrule
        Gemini-2.0-flash & \underline{98.7} & 61.6 & 0.0 & 83.3 & 2,555 & \underline{98.0} & 48.0 & 0.0 & 48.0 & 4,020 & 58.0 & 24.2 & 2.0 & 20.0 & 2,104 \\
        Gemini-2.0-flash-thinking & 94.7 & 57.7 & \underline{1.3} & 79.3 & 2,648 & 92.0 & 40.7 & 0.0 & 27.0 & 4,257 & \underline{99.0} & 45.9 & 8.0 & 37.5 & 4,038 \\
        GPT-4o & \textbf{100.0} & \underline{66.0} & \underline{1.3} & \underline{86.7} & 1,726 & \textbf{100.0} & \underline{56.0} & 0.0 & \underline{67.0} & 3,229 & \textbf{100.0} & 39.3 & 3.5 & 29.5 & 953 \\
        o1-mini & 95.3 & 45.5 & \underline{1.3} & 54.0 & 7,840 & 97.0 & 34.7 & 0.0 & 12.0 & 10,952 & \underline{99.0} & \underline{57.2} & \underline{23.5} & \underline{53.5} & 10,242 \\
        o1-preview & 98.0 & \textbf{77.7} & \textbf{24.7} & \textbf{89.3} & 10,098 & \textbf{100.0} & \textbf{67.2} & 0.0 & \textbf{90.0} & 14,847 & \underline{99.0} & \textbf{68.8} & \textbf{41.0} & \textbf{68.5} & 9,449 \\ 
        \bottomrule
    \end{tabular}}
\end{table*}
\begin{table*}[!t]
    \centering
    \renewcommand{\arraystretch}{1.0}\resizebox{1.0\textwidth}{!}{
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{l|ccccc|ccccc|ccccc}
        \toprule
        \multirow{2}{*}{\textbf{Model}} &  \multicolumn{5}{c|}{\textbf{Cryptogram}} & \multicolumn{5}{c|}{\textbf{Sudoku}}  & \multicolumn{5}{c}{\textbf{Drop Quote}} \\
        \cmidrule(l){2-16}
        & CR & S-Acc & EM & PM-0.5 & Tokens & CR & S-Acc & EM & PM-0.5 & Tokens & CR & S-Acc & EM & PM-0.5 & Tokens \\
        \midrule
        \rowcolor{gray!15} \multicolumn{16}{l}{\textit{\textbf{Open-source LLMs}}}\\
        \midrule 
        % \rowcolor{blue!15} GPT-4o-2024-08-06 & 54.82 & 898 & 29.71 & 457 & \textbf{42.88} & 1436 & \textbf{67.49} & 1581 & 47.87 & 1121 & \textbf{48.55} & 1098 \\
        Llama-3.1-8B-Instruct & 43.0 & 2.3 & 0.0 & 0.0 & 2,068 & 7.5 & 1.2 & 0.0 & 0.0 & 2,782 & 44.0 & 11.2 & 0.0 & 1.0 & 2,123 \\
        Llama-3.1-70B-Instruct & 62.0 & 6.9 & 0.0 & \underline{1.0} & 1,298 & 69.5 & 24.2 & 1.0 & 17.5 & 1,940 & 82.0 & 27.7 & 0.0 & 12.0 & 1,498 \\
        Llama-3.3-70B-Instruct & \underline{99.0} & \underline{14.3} & 0.0 & \underline{1.0} & 1,137 & 93.5 & 34.8 & 7.0 & 22.5 & 1,062 & \underline{99.0} & 29.0 & 0.0 & 13.0 & 918 \\
        Mistral-7B-Instruct-v0.3 & \underline{99.0} & 4.3 & 0.0 & 0.0 & 1,096 & 84.0 & 11.9 & 0.0 & 1.5 & 3,108 & 66.0 & 6.6 & 0.0 & 1.0 & 2,337 \\
        Mistral-Small-Instruct-2409 & 95.0 & 7.0 & 0.0 & 0.0 & 1,233 & 89.0 & 20.5 & 0.5 & 7.5 & 1,968 & 97.0 & 26.9 & 0.0 & 6.0 & 1,615 \\
        Mistral-Large-Instruct-2411 & 96.0 & 13.7 & 0.0 & \underline{1.0} & 1,204 & 85.5 & 39.5 & 10.0 & 33.5 & 1,955 & 98.0 & 24.7 & 0.0 & 9.0 & 1,566 \\
        Qwen2.5-7B-Instruct & 81.0 & 3.5 & 0.0 & 0.0 & 1,181 & 94.5 & 30.2 & 1.5 & 15.0 & 1,486 & 98.0 & 21.9 & 0.0 & 4.0 & 1,852 \\
        Qwen2.5-32B-Instruct & 89.0 & 9.8 & 0.0 & 0.0 & 1,303 & \textbf{100.0} & 42.8 & 3.5 & 30.5 & 1,202 & 95.0 & 28.4 & 0.0 & \underline{14.0} & 1,197 \\
        Qwen2.5-72B-Instruct & 85.0 & 11.8 & 0.0 & 0.0 & 1,727 & \underline{97.5} & \underline{43.0} & 5.5 & 34.0 & 2,013 & 94.0 & \underline{30.9} & 0.0 & 13.0 & 1,757 \\
        QwQ-32B-Preview & 47.0 & 3.6 & 0.0 & 0.0 & 6,492 & 54.5 & 40.1 & \underline{31.5} & \underline{35.5} & 8,381 & 33.0 & 7.5 & 0.0 & 8.0 & 6,078 \\
        DeepSeek-R1 & \textbf{100.0} & \textbf{26.0} & \textbf{4.0} & \textbf{21.0} & 10,344 & \textbf{100.0} & \textbf{70.3} & \textbf{50.0} & \textbf{64.0} & 8,277 & \textbf{100.0} & \textbf{47.3} & \textbf{7.0} & \textbf{42.0} & 11,422 \\
        \midrule
        \rowcolor{blue!15} \multicolumn{16}{l}{\textit{\textbf{Closed-source LLMs}}}\\
        \midrule
        Gemini-2.0-flash & 47.0 & 8.5 & 0.0 & 1.0 & 1,585 & 93.0 & 45.3 & 12.5 & 37.5 & 2,843 & 92.0 & 34.3 & 0.0 & 17.0 & 2,717 \\
        Gemini-2.0-flash-thinking & 68.0 & 11.2 & 0.0 & 2.0 & 4,167 & 79.5 & 46.5 & 16.5 & 41.0 & 3,853 & 96.0 & \underline{34.4} & 0.0 & \underline{23.0} & 3,386 \\
        GPT-4o & \textbf{100.0} & 20.7 & 0.0 & 5.0 & 740 & \textbf{100.0} & 52.2 & 14.5 & \underline{48.0} & 1,104 & \textbf{99.0} & 31.1 & 0.0 & 14.0 & 1,165 \\
        o1-mini & \textbf{100.0} & \underline{22.7} & \underline{1.0} & \underline{13.0} & 11,208 & \underline{99.0} & \underline{53.4} & \underline{27.0} & 43.0 & 3,961 & 96.0 & 34.3 & \underline{2.0} & 21.0 & 13,255 \\
        o1-preview & \underline{92.0} & \textbf{34.8} & \textbf{13.0} & \textbf{29.0} & 12,567 & 91.5 & \textbf{65.1} & \textbf{50.0} & \textbf{55.5} & 8,062 & \underline{97.0} & \textbf{38.8} & \textbf{13.0} & \textbf{38.0} & 13,595 \\ 
        \bottomrule
    \end{tabular}}    
    \caption{Performance (\%) of LLMs on six tasks. The best and second-best results are highlighted in \textbf{bold} and \underline{underlined}, respectively. "Tokens" denotes the average number of generated tokens.}
    \label{tab:main_exp}
    % \vspace{-2mm}
\end{table*}