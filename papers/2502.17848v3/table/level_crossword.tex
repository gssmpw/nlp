\begin{table*}[t]
    \centering
    \resizebox{0.9\textwidth}{!}{
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{l|ccccc|ccccc|ccccc}
        \toprule
        \multirow{2}{*}{\textbf{Model}} &  \multicolumn{5}{c|}{\textbf{Crossword - $5\times5$}} & \multicolumn{5}{c|}{\textbf{Crossword - $10\times10$}}  & \multicolumn{5}{c}{\textbf{Crossword - $15\times15$}} \\
        \cmidrule(l){2-16}
        & CR & S-Acc & EM & PM-0.5 & Tokens & CR & S-Acc & EM & PM-0.5 & Tokens & CR & S-Acc & EM & PM-0.5 & Tokens \\
        \midrule
        \rowcolor{gray!15} \multicolumn{16}{l}{\textit{\textbf{Open-source LLMs}}}\\
        \midrule 
Llama-3.1-8B-Instruct & 64.0 & 29.6 & 0.0 & 32.0 & 743 & 50.0 & 19.2 & 0.0 & 10.0 & 2433 & 70.0 & 21.2 & 0.0 & 0.0 & 5488 \\
Llama-3.1-70B-Instruct & 72.0 & 46.6 & 0.0 & 64.0 & 724 & 74.0 & 47.8 & 0.0 & 66.0 & 2554 & 86.0 & 46.1 & 0.0 & 56.0 & 5938 \\
Llama-3.3-70B-Instruct & {\ul 92.0} & 59.2 & 0.0 & 78.0 & 817 & {\ul 78.0} & 48.5 & 0.0 & 72.0 & 2284 & 86.0 & 35.3 & 0.0 & 46.0 & 4738 \\
Mistral-7B-Instruct-v0.3 & \textbf{100.0} & 27.2 & 0.0 & 18.0 & 970 & \textbf{100.0} & 27.9 & 0.0 & 2.0 & 3134 & 82.0 & 13.8 & 0.0 & 0.0 & 6863 \\
Mistral-Small-Instruct-2409 & \textbf{100.0} & 52.8 & 0.0 & 66.0 & 803 & \textbf{100.0} & 52.0 & 0.0 & 64.0 & 2469 & 96.0 & 40.0 & 0.0 & 32.0 & 6133 \\
Mistral-Large-Instruct-2411 & \textbf{100.0} & {\ul 68.8} & {\ul 6.0} & {\ul 92.0} & 804 & \textbf{100.0} & {\ul 66.2} & 0.0 & 96.0 & 2634 & {\ul 98.0} & {\ul 53.2} & 0.0 & {\ul 70.0} & 6273 \\
Qwen2.5-7B-Instruct & \textbf{100.0} & 27.8 & 0.0 & 10.0 & 659 & \textbf{100.0} & 21.7 & 0.0 & 0.0 & 2022 & 96.0 & 13.8 & 0.0 & 0.0 & 4643 \\
Qwen2.5-32B-Instruct & \textbf{100.0} & 42.6 & 0.0 & 44.0 & 678 & \textbf{100.0} & 37.8 & 0.0 & 16.0 & 2132 & \textbf{100.0} & 23.4 & 0.0 & 0.0 & 4871 \\
Qwen2.5-72B-Instruct & \textbf{100.0} & 51.0 & 0.0 & 64.0 & 720 & \textbf{100.0} & 47.0 & 0.0 & 44.0 & 2167 & \textbf{100.0} & 34.4 & 0.0 & 2.0 & 5318 \\
QwQ-32B-Preview & 80.0 & 35.2 & 0.0 & 32.0 & 2965 & {\ul 78.0} & 31.9 & 0.0 & 22.0 & 4614 & 82.0 & 23.6 & 0.0 & 0.0 & 6872 \\
DeepSeek-R1 & \textbf{100.0} & \textbf{87.2} & \textbf{50.0} & \textbf{98.0} & 8565 & \textbf{100.0} & \textbf{78.5} & 0.0 & \textbf{100.0} & 10314 & \textbf{100.0} & \textbf{60.4} & 0.0 & \textbf{84.0} & 10552 \\
        \midrule
        \rowcolor{blue!15} \multicolumn{16}{l}{\textit{\textbf{Closed-source LLMs}}}\\
        \midrule
Gemini-2.0-flash & \textbf{100.0} & 65.6 & 0.0 & 84.0 & 622 & \textbf{100.0} & 69.4 & 0.0 & {\ul 98.0} & 2079 & {\ul 96.0} & {\ul 49.8} & 0.0 & {\ul 68.0} & 4964 \\
Gemini-2.0-flash-thinking & {\ul 98.0} & 65.0 & {\ul 4.0} & 90.0 & 744 & 98.0 & 64.8 & 0.0 & 92.0 & 2241 & 88.0 & 43.4 & 0.0 & 56.0 & 4961 \\
GPT-4o & \textbf{100.0} & {\ul 73.2} & {\ul 4.0} & {\ul 96.0} & 597 & \textbf{100.0} & {\ul 74.1} & 0.0 & {\ul 98.0} & 1679 & \textbf{100.0} & 41.7 & 0.0 & 66.0 & 2902 \\
o1-mini & \textbf{100.0} & 58.8 & {\ul 4.0} & 80.0 & 7243 & 92.0 & 49.0 & 0.0 & 68.0 & 7292 & 94.0 & 28.6 & 0.0 & 14.0 & 8986 \\
o1-preview & {\ul 98.0} & \textbf{95.1} & \textbf{74.0} & \textbf{98.0} & 8006 & \textbf{100.0} & \textbf{80.8} & 0.0 & \textbf{100.0} & 9971 & {\ul 96.0} & \textbf{57.1} & 0.0 & \textbf{70.0} & 12317 \\
        \bottomrule
    \end{tabular}}    
    \caption{Performance (\%) of LLMs on Crossword across all difficulty levels. The best and second-best results are highlighted in \textbf{bold} and \underline{underlined}, respectively. "Tokens" denotes the average number of generated tokens.}
    \label{level_crossword}
\end{table*}