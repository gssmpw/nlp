\begin{table*}[t]
    \centering
    \resizebox{1.0\textwidth}{!}{
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{l|ccccc|ccccc|ccccc|ccccc}
        \toprule
        \multirow{2}{*}{\textbf{Model}} &  \multicolumn{5}{c|}{\textbf{Logic Puzzle - $4\times4$}} & \multicolumn{5}{c|}{\textbf{Logic Puzzle - $4\times5$}}  & \multicolumn{5}{c}{\textbf{Logic Puzzle - $4\times6$}} & \multicolumn{5}{c}{\textbf{Logic Puzzle - $4\times7$}} \\
        \cmidrule(l){2-21}
        & CR & S-Acc & EM & PM-0.5 & Tokens & CR & S-Acc & EM & PM-0.5 & Tokens & CR & S-Acc & EM & PM-0.5 & Tokens & CR & S-Acc & EM & PM-0.5 & Tokens \\
        \midrule
        \rowcolor{gray!15} \multicolumn{21}{l}{\textit{\textbf{Open-source LLMs}}}\\
        \midrule 
Llama-3.1-8B-Instruct & 86.0 & 30.7 & 0.0 & 28.0 & 1049 & 52.0 & 12.8 & 0.0 & 0.0 & 1214 & 46.0 & 10.6 & 0.0 & 2.0 & 1225 & 44.0 & 9.8 & 0.0 & 2.0 & 1683 \\
Llama-3.1-70B-Instruct & 66.0 & 35.2 & 8.0 & 42.0 & 989 & 58.0 & 20.0 & 0.0 & 10.0 & 1126 & 54.0 & 18.9 & 0.0 & 12.0 & 1109 & 46.0 & 17.1 & 0.0 & 8.0 & 1435 \\
Llama-3.3-70B-Instruct & 88.0 & 46.7 & 4.0 & 52.0 & 1258 & 82.0 & 35.6 & 0.0 & 26.0 & 1576 & 90.0 & 25.4 & 0.0 & 12.0 & 1931 & 62.0 & 21.2 & 0.0 & 10.0 & 2188 \\
Mistral-7B-Instruct-v0.3 & \textbf{100.0} & 26.8 & 0.0 & 16.0 & 1289 & {\ul 98.0} & 18.0 & 0.0 & 2.0 & 1590 & 96.0 & 15.0 & 0.0 & 0.0 & 1846 & {\ul 94.0} & 16.8 & 0.0 & 0.0 & 1746 \\
Mistral-Small-Instruct-2409 & \textbf{100.0} & 38.7 & 2.0 & 34.0 & 1358 & \textbf{100.0} & 33.6 & 0.0 & 12.0 & 1424 & {\ul 98.0} & 23.7 & 0.0 & 2.0 & 1554 & \textbf{100.0} & 26.7 & 0.0 & 2.0 & 1721 \\
Mistral-Large-Instruct-2411 & \textbf{100.0} & 53.2 & 10.0 & 62.0 & 1293 & \textbf{100.0} & 40.4 & 2.0 & 30.0 & 1532 & \textbf{100.0} & {\ul 30.6} & 0.0 & 18.0 & 1772 & \textbf{100.0} & {\ul 29.1} & 0.0 & 12.0 & 1950 \\
Qwen2.5-7B-Instruct & \textbf{100.0} & 33.5 & 0.0 & 20.0 & 1133 & {\ul 98.0} & 28.8 & 0.0 & 14.0 & 1254 & 96.0 & 21.0 & 0.0 & 0.0 & 1494 & 92.0 & 19.7 & 0.0 & 0.0 & 1701 \\
Qwen2.5-32B-Instruct & \textbf{100.0} & 45.5 & 0.0 & 54.0 & 1070 & 92.0 & 34.3 & 0.0 & 20.0 & 1137 & 94.0 & 24.1 & 0.0 & 6.0 & 1241 & 86.0 & 25.0 & 0.0 & 10.0 & 1383 \\
Qwen2.5-72B-Instruct & {\ul 94.0} & 48.3 & 0.0 & 56.0 & 1490 & {\ul 98.0} & 34.8 & 0.0 & 16.0 & 1641 & 92.0 & 26.3 & 0.0 & 14.0 & 1931 & 90.0 & 26.7 & 0.0 & 6.0 & 2177 \\
QwQ-32B-Preview & {\ul 94.0} & {\ul 76.3} & {\ul 56.0} & {\ul 80.0} & 4766 & 86.0 & {\ul 52.7} & {\ul 14.0} & {\ul 64.0} & 8966 & 68.0 & 30.2 & {\ul 4.0} & {\ul 28.0} & 11292 & 66.0 & 25.8 & {\ul 4.0} & {\ul 20.0} & 13070 \\
DeepSeek-R1 & \textbf{100.0} & \textbf{94.2} & \textbf{90.0} & \textbf{94.0} & 4724 & \textbf{100.0} & \textbf{70.5} & \textbf{36.0} & \textbf{68.0} & 8907 & \textbf{100.0} & \textbf{50.2} & \textbf{16.0} & \textbf{50.0} & 11860 & \textbf{100.0} & \textbf{62.6} & \textbf{28.0} & \textbf{60.0} & 11329 \\
        \midrule
        \rowcolor{blue!15} \multicolumn{21}{l}{\textit{\textbf{Closed-source LLMs}}}\\
        \midrule
Gemini-2.0-flash & 70.0 & 35.3 & 6.0 & 36.0 & 1569 & 56.0 & 22.8 & 2.0 & 16.0 & 2283 & 62.0 & 23.8 & 0.0 & 18.0 & 2254 & 44.0 & 14.8 & 0.0 & 10.0 & 2308 \\
Gemini-2.0-flash-thinking & \textbf{100.0} & 63.8 & 28.0 & 66.0 & 3310 & {\ul 98.0} & 44.0 & 2.0 & 34.0 & 4073 & \textbf{100.0} & 37.8 & 0.0 & 30.0 & 4311 & {\ul 98.0} & 38.1 & 2.0 & 20.0 & 4458 \\
GPT-4o & \textbf{100.0} & 55.0 & 14.0 & 62.0 & 871 & \textbf{100.0} & 41.9 & 0.0 & 30.0 & 907 & \textbf{100.0} & 29.9 & 0.0 & 14.0 & 974 & \textbf{100.0} & 30.3 & 0.0 & 12.0 & 1061 \\
o1-mini & {\ul 98.0} & {\ul 79.5} & {\ul 66.0} & {\ul 82.0} & 5572 & \textbf{100.0} & {\ul 60.5} & {\ul 16.0} & {\ul 62.0} & 9522 & {\ul 98.0} & {\ul 42.9} & {\ul 4.0} & {\ul 36.0} & 12121 & \textbf{100.0} & {\ul 45.9} & {\ul 8.0} & {\ul 34.0} & 13753 \\
o1-preview & \textbf{100.0} & \textbf{96.7} & \textbf{92.0} & \textbf{98.0} & 6199 & {\ul 98.0} & \textbf{74.5} & \textbf{48.0} & \textbf{74.0} & 9129 & \textbf{100.0} & \textbf{53.8} & \textbf{14.0} & \textbf{56.0} & 11140 & {\ul 98.0} & \textbf{50.1} & \textbf{10.0} & \textbf{46.0} & 11330 \\
        \bottomrule
    \end{tabular}}    
    \caption{Performance (\%) of LLMs on Logic Puzzle across all difficulty levels. The best and second-best results are highlighted in \textbf{bold} and \underline{underlined}, respectively. "Tokens" denotes the average number of generated tokens.}
    \label{level_logic}
\end{table*}