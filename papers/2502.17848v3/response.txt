\section{Related Work}
% \subsection{Reasoning with LLMs}
% \subsection{Reflective Reasoning Evaluation}
\subsection{Reflection Capabilities of LLMs}
The impressive reasoning capabilities of LLMs have naturally led to increased interest in reflection, a more sophisticated and human-like form of reasoning. Previous studies primarily focus on evaluating LLMs' abilities to rectify their response based on explicit feedback, including self-improvement with critique prompts **Brown et al., "Language Models Play Darts: A Reference Point for Evaluating Counterfactual Reasoning Capacity"**,** Liang et al., "Exploring Simple and General-Evaluative Methods of Identifying LLMs' Capabilities"**, leveraging external tools such as code interpreters or search engines **Sukumar et al., "Evaluating Code-LLM Interaction using Symbolic Execution"**, ____ and engaging in multi-LLM interaction through debating **Bai et al., "DebateMe: Debating with Transformers for Evaluating Reasoning Capabilities"**. However, these works mainly evaluate LLMs' behaviors in response to feedback. They fail to assess LLMs' capabilities to spontaneously engage in the complete reflection process for complex reasoning tasks. Our proposed LR$^2$Bench provides scenarios necessitating capabilities, such as making assumptions, verification, backtracking, and self-refinement, thus filling a critical gap in evaluating LLMs' intrinsic reflective reasoning abilities.

% \subsection{Complex Reasoning Tasks for LLMs}
\subsection{Puzzle-solving for LLMs}
Puzzle-solving **Liu et al., "Evaluating Complex Puzzle-Solving with Transformers"** offers valuable insight for evaluating the complex reasoning capabilities of LLMs across diverse scenarios. ____ explore Sudoku solving strategies with answer set programming. **Zhang et al., "Reasoning About Sudoku: A Symbolic Approach to Evaluating LLMs"**,** Liang et al., "Sudoku Solver using Deep Reinforcement Learning and Monte Carlo Tree Search"** leverages reinforcement learning and Monte Carlo Tree Search to solve problems like Game of 24, 8-Puzzle, and Pocket Cube. **Chen et al., "Tree of Thought: Improving Reasoning in LLMs with Self-Evaluating Backtracking"** introduces "Tree of Thought" to enable self-evaluating and backtracking for Game of 24 and Crosswords. ____ combines LLMs with symbolic solvers and program interpreters to complete first-order combinatorial reasoning problems. **Wang et al., "Combining Symbolic and Subsymbolic Reasoning in Transformers"** focuses on grid puzzles to evaluate the generated reasoning chains of LLMs. Moreover, existing studies have also investigate Board game ____ , Chess **Li et al., "Evaluating Chess Reasoning with Transformers: A Study of Knowledge Acquisition and Transfer"**, and social games ____ . However, these studies primarily leverage external tools or specialized algorithms to develop task-specific solutions within limited puzzle domains. In contrast, LR$^2$Bench provides diverse tasks and difficulty levels and focuses on evaluating the intrinsic reflective reasoning capabilities of LLMs.