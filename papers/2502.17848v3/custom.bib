% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@misc{qwq-32b-preview,
    title = {QwQ: Reflect Deeply on the Boundaries of the Unknown},
    url = {https://qwenlm.github.io/blog/qwq-32b-preview/},
    author = {Qwen},
    month = {November},
    year = {2024}
}

@misc{o1,
  author = {OpenAI},
  title = {Learning to Reason with LLMs},
  year = {2024},
  url = {https://openai.com/index/learning-to-reason-with-llms}
}

@misc{deepseek2024r1lite,
  author = {DeepSeek},
  title = {DeepSeek-R1-Lite-Preview: Unleashing Supercharged Reasoning Power},
  year = {2024},
  howpublished = {\url{https://api-docs.deepseek.com/news/news1120}},
  note = {Accessed: 2024-12-29},
}

@misc{gemini-2.0,
  author = {Sundar Pichai, Demis Hassabis, and Koray Kavukcuoglu},
  title = {Introducing Gemini 2.0: our new AI model for the agentic era},
  year = {2024},
  url = {https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024}
}

@article{zhong2024evaluation,
  title={Evaluation of openai o1: Opportunities and challenges of agi},
  author={Zhong, Tianyang and Liu, Zhengliang and Pan, Yi and Zhang, Yutong and Zhou, Yifan and Liang, Shizhe and Wu, Zihao and Lyu, Yanjun and Shu, Peng and Yu, Xiaowei and others},
  journal={arXiv preprint arXiv:2409.18486},
  year={2024}
}

@article{gao2024omni,
  title={Omni-math: A universal olympiad level mathematic benchmark for large language models},
  author={Gao, Bofei and Song, Feifan and Yang, Zhe and Cai, Zefan and Miao, Yibo and Dong, Qingxiu and Li, Lei and Ma, Chenghao and Chen, Liang and Xu, Runxin and others},
  journal={arXiv preprint arXiv:2410.07985},
  year={2024}
}


@article{wang2024planning,
  title={On The Planning Abilities of OpenAI's o1 Models: Feasibility, Optimality, and Generalizability},
  author={Wang, Kevin and Li, Junbo and Bhatt, Neel P and Xi, Yihan and Liu, Qiang and Topcu, Ufuk and Wang, Zhangyang},
  journal={arXiv preprint arXiv:2409.19924},
  year={2024}
}

@article{wu2024comparative,
  title={A Comparative Study on Reasoning Patterns of OpenAI's o1 Model},
  author={Wu, Siwei and Peng, Zhongyuan and Du, Xinrun and Zheng, Tuney and Liu, Minghao and Wu, Jialong and Ma, Jiachen and Li, Yizhi and Yang, Jian and Zhou, Wangchunshu and others},
  journal={arXiv preprint arXiv:2410.13639},
  year={2024}
}

@article{nguyen2014reflection,
  title={What is reflection? A conceptual analysis of major definitions and a proposal of a five-component model},
  author={Nguyen, Quoc Dinh and Fernandez, Nicolas and Karsenti, Thierry and Charlin, Bernard},
  journal={Medical education},
  volume={48},
  number={12},
  pages={1176--1189},
  year={2014},
  publisher={Wiley Online Library}
}



@inproceedings{
lan2024criticeval,
title={CriticEval: Evaluating Large-scale Language Model as Critic},
author={Tian Lan and Wenwei Zhang and Chen Xu and Heyan Huang and Dahua Lin and Kai Chen and Xian-Ling Mao},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=ZsxZ65YqL1}
}

@inproceedings{
gou2024critic,
title={{CRITIC}: Large Language Models Can Self-Correct with Tool-Interactive Critiquing},
author={Zhibin Gou and Zhihong Shao and Yeyun Gong and yelong shen and Yujiu Yang and Nan Duan and Weizhu Chen},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=Sx038qxjek}
}

@article{sun2024critique,
  title={The critique of critique},
  author={Sun, Shichao and Li, Junlong and Yuan, Weizhe and Yuan, Ruifeng and Li, Wenjie and Liu, Pengfei},
  journal={arXiv preprint arXiv:2401.04518},
  year={2024}
}

@inproceedings{lin-etal-2024-criticbench,
    title = "{C}ritic{B}ench: Benchmarking {LLM}s for Critique-Correct Reasoning",
    author = "Lin, Zicheng  and
      Gou, Zhibin  and
      Liang, Tian  and
      Luo, Ruilin  and
      Liu, Haowei  and
      Yang, Yujiu",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.91/",
    doi = "10.18653/v1/2024.findings-acl.91",
    pages = "1552--1587",
    abstract = "The ability of Large Language Models (LLMs) to critique and refine their reasoning is crucial for their application in evaluation, feedback provision, and self-improvement. This paper introduces CriticBench, a comprehensive benchmark designed to assess LLMs' abilities to critique and rectify their reasoning across a variety of tasks. CriticBench encompasses five reasoning domains: mathematical, commonsense, symbolic, coding, and algorithmic. It compiles 15 datasets and incorporates responses from three LLM families. Utilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in generation, critique, and correction reasoning, i.e., GQC reasoning. Our findings reveal: (1) a linear relationship in GQC capabilities, with critique-focused training markedly enhancing performance; (2) a task-dependent variation in correction effectiveness, with logic-oriented tasks being more amenable to correction; (3) GQC knowledge inconsistencies that decrease as model size increases; and (4) an intriguing inter-model critiquing dynamic, where stronger models are better at critiquing weaker ones, while weaker models can surprisingly surpass stronger ones in their self-critique. We hope these insights into the nuanced critique-correct reasoning of LLMs will foster further research in LLM critique and self-improvement."
}

@article{li2024reflection,
  title={Reflection-Bench: probing AI intelligence with reflection},
  author={Li, Lingyu and Wang, Yixu and Zhao, Haiquan and Kong, Shuqi and Teng, Yan and Li, Chunbo and Wang, Yingchun},
  journal={arXiv preprint arXiv:2410.16270},
  year={2024}
}

@inproceedings{li2024hindsight,
  title={When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models},
  author={Li, Yanhong and Yang, Chenghao and Ettinger, Allyson},
  booktitle={Findings of the Association for Computational Linguistics: NAACL 2024},
  pages={3741--3753},
  year={2024}
}

@inproceedings{
huang2024large,
title={Large Language Models Cannot Self-Correct Reasoning Yet},
author={Jie Huang and Xinyun Chen and Swaroop Mishra and Huaixiu Steven Zheng and Adams Wei Yu and Xinying Song and Denny Zhou},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=IkmD3fKBPQ}
}

@inproceedings{
chen2024teaching,
title={Teaching Large Language Models to Self-Debug},
author={Xinyun Chen and Maxwell Lin and Nathanael Sch{\"a}rli and Denny Zhou},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=KuPixIqPiq}
}

@article{madaan2024self,
  title={Self-refine: Iterative refinement with self-feedback},
  author={Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{shinn2024reflexion,
  title={Reflexion: Language agents with verbal reinforcement learning},
  author={Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{liang-etal-2024-encouraging,
    title = "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate",
    author = "Liang, Tian  and
      He, Zhiwei  and
      Jiao, Wenxiang  and
      Wang, Xing  and
      Wang, Yan  and
      Wang, Rui  and
      Yang, Yujiu  and
      Shi, Shuming  and
      Tu, Zhaopeng",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.992/",
    doi = "10.18653/v1/2024.emnlp-main.992",
    pages = "17889--17904",
    abstract = "Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively. However, our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD) framework, in which multiple agents express their arguments in the state of {\textquotedblleft}tit for tat{\textquotedblright} and a judge manages the debate process to obtain a final solution. Clearly, our MAD framework encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation. Experiment results on two challenging datasets, commonsense machine translation and counter-intuitive arithmetic reasoning, demonstrate the effectiveness of our MAD framework. Extensive analyses suggest that the adaptive break of debate and the modest level of {\textquotedblleft}tit for tat{\textquotedblright} state are required for MAD to obtain good performance. Moreover, we find that LLMs might not be a fair judge if different LLMs are used for agents."
}


@inproceedings{talmor2019commonsenseqa,
  title={CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge},
  author={Talmor, Alon and Herzig, Jonathan and Lourie, Nicholas and Berant, Jonathan},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4149--4158},
  year={2019}
}

@inproceedings{rajani2019explain,
  title={Explain Yourself! Leveraging Language Models for Commonsense Reasoning},
  author={Rajani, Nazneen Fatema and McCann, Bryan and Xiong, Caiming and Socher, Richard},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4932--4942},
  year={2019}
}

@inproceedings{zellers2019hellaswag,
  title={HellaSwag: Can a Machine Really Finish Your Sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4791--4800},
  year={2019}
}

@article{gsm8k,
  title={Training Verifiers to Solve Math Word Problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  journal={arXiv:2110.14168},
year = {2021}
}

@article{hendrycksmath2021,
  title={Measuring Mathematical Problem Solving With the MATH Dataset},
  author={Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},
  journal={NeurIPS},
  year={2021}
}

@inproceedings{
lightman2024lets,
title={Let's Verify Step by Step},
author={Hunter Lightman and Vineet Kosaraju and Yuri Burda and Harrison Edwards and Bowen Baker and Teddy Lee and Jan Leike and John Schulman and Ilya Sutskever and Karl Cobbe},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=v8L0pN6EOi}
}


@inproceedings{kwon2023efficient,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}

@article{giadikiaroglou2024puzzle,
  title={Puzzle Solving using Reasoning of Large Language Models: A Survey},
  author={Giadikiaroglou, Panagiotis and Lymperaiou, Maria and Filandrianos, Giorgos and Stamou, Giorgos},
  journal={arXiv preprint arXiv:2402.11291},
  year={2024}
}

@inproceedings{ishay2023leveraging,
  title={Leveraging large language models to generate answer set programs},
  author={Ishay, Adam and Yang, Zhun and Lee, Joohyung},
  booktitle={Proceedings of the 20th International Conference on Principles of Knowledge Representation and Reasoning},
  pages={374--383},
  year={2023}
}

@article{ding2023everything,
  title={Everything of thoughts: Defying the law of penrose triangle for thought generation},
  author={Ding, Ruomeng and Zhang, Chaoyun and Wang, Lu and Xu, Yong and Ma, Minghua and Zhang, Wei and Qin, Si and Rajmohan, Saravan and Lin, Qingwei and Zhang, Dongmei},
  journal={arXiv preprint arXiv:2311.04254},
  year={2023}
}

@article{yao2024tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{mittal2024puzzlebench,
  title={PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?},
  author={Mittal, Chinmay and Kartik, Krishna and Singla, Parag and others},
  journal={arXiv preprint arXiv:2402.02611},
  year={2024}
}

@inproceedings{tyagi2024step,
  title={Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?},
  author={Tyagi, Nemika and Parmar, Mihir and Kulkarni, Mohith and Rrv, Aswin and Patel, Nisarg and Nakamura, Mutsumi and Mitra, Arindam and Baral, Chitta},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={19898--19915},
  year={2024}
}

@article{kazemi2024boardgameqa,
  title={Boardgameqa: A dataset for natural language reasoning with contradictory information},
  author={Kazemi, Mehran and Yuan, Quan and Bhatia, Deepti and Kim, Najoung and Xu, Xin and Imbrasaite, Vaiva and Ramachandran, Deepak},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{feng2024chessgpt,
  title={Chessgpt: Bridging policy learning and language modeling},
  author={Feng, Xidong and Luo, Yicheng and Wang, Ziyan and Tang, Hongrui and Yang, Mengyue and Shao, Kun and Mguni, David and Du, Yali and Wang, Jun},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{light2023avalonbench,
  title={Avalonbench: Evaluating llms playing the game of avalon},
  author={Light, Jonathan and Cai, Min and Shen, Sheng and Hu, Ziniu},
  booktitle={NeurIPS 2023 Foundation Models for Decision Making Workshop}
}

@article{wang2023avalon,
  title={Avalon's game of thoughts: Battle against deception through recursive contemplation},
  author={Wang, Shenzhi and Liu, Chang and Zheng, Zilong and Qi, Siyuan and Chen, Shuo and Yang, Qisen and Zhao, Andrew and Wang, Chaofei and Song, Shiji and Huang, Gao},
  journal={arXiv preprint arXiv:2310.01320},
  year={2023}
}

@article{xu2023exploring,
  title={Exploring large language models for communication games: An empirical study on werewolf},
  author={Xu, Yuzhuang and Wang, Shuo and Li, Peng and Luo, Fuwen and Wang, Xiaolong and Liu, Weidong and Liu, Yang},
  journal={arXiv preprint arXiv:2309.04658},
  year={2023}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{yang2024qwen2,
  title={Qwen2. 5 technical report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@misc{deepmind_gemini_flash_thinking,
author={Google DeepMind},
title={Gemini 2.0 Flash Thinking Experimental},
url={https://deepmind.google/technologies/gemini/flash-thinking/},
year={2024}
}

@misc{deepmind_gemini_flash,
author={Google DeepMind},
title={Gemini 2.0 Flash Experimental},
url={https://deepmind.google/technologies/gemini/flash/},
year={2024}
}

@Misc{gpt4o,
title = {OpenAI GPT-4o},
author={OpenAI},
url={https://platform.openai.com/docs/models/gpt-4o},
year = {2024}
}

@book{dechter2003constraint,
  title={Constraint Processing},
  author={Dechter, Rina},
  year={2003},
  publisher={Morgan Kaufmann}
}