@article{ding2023everything,
  title={Everything of thoughts: Defying the law of penrose triangle for thought generation},
  author={Ding, Ruomeng and Zhang, Chaoyun and Wang, Lu and Xu, Yong and Ma, Minghua and Zhang, Wei and Qin, Si and Rajmohan, Saravan and Lin, Qingwei and Zhang, Dongmei},
  journal={arXiv preprint arXiv:2311.04254},
  year={2023}
}

@article{feng2024chessgpt,
  title={Chessgpt: Bridging policy learning and language modeling},
  author={Feng, Xidong and Luo, Yicheng and Wang, Ziyan and Tang, Hongrui and Yang, Mengyue and Shao, Kun and Mguni, David and Du, Yali and Wang, Jun},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{giadikiaroglou2024puzzle,
  title={Puzzle Solving using Reasoning of Large Language Models: A Survey},
  author={Giadikiaroglou, Panagiotis and Lymperaiou, Maria and Filandrianos, Giorgos and Stamou, Giorgos},
  journal={arXiv preprint arXiv:2402.11291},
  year={2024}
}

@inproceedings{ishay2023leveraging,
  title={Leveraging large language models to generate answer set programs},
  author={Ishay, Adam and Yang, Zhun and Lee, Joohyung},
  booktitle={Proceedings of the 20th International Conference on Principles of Knowledge Representation and Reasoning},
  pages={374--383},
  year={2023}
}

@article{kazemi2024boardgameqa,
  title={Boardgameqa: A dataset for natural language reasoning with contradictory information},
  author={Kazemi, Mehran and Yuan, Quan and Bhatia, Deepti and Kim, Najoung and Xu, Xin and Imbrasaite, Vaiva and Ramachandran, Deepak},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{li2024hindsight,
  title={When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models},
  author={Li, Yanhong and Yang, Chenghao and Ettinger, Allyson},
  booktitle={Findings of the Association for Computational Linguistics: NAACL 2024},
  pages={3741--3753},
  year={2024}
}

@article{li2024reflection,
  title={Reflection-Bench: probing AI intelligence with reflection},
  author={Li, Lingyu and Wang, Yixu and Zhao, Haiquan and Kong, Shuqi and Teng, Yan and Li, Chunbo and Wang, Yingchun},
  journal={arXiv preprint arXiv:2410.16270},
  year={2024}
}

@inproceedings{liang-etal-2024-encouraging,
    title = "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate",
    author = "Liang, Tian  and
      He, Zhiwei  and
      Jiao, Wenxiang  and
      Wang, Xing  and
      Wang, Yan  and
      Wang, Rui  and
      Yang, Yujiu  and
      Shi, Shuming  and
      Tu, Zhaopeng",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.992/",
    doi = "10.18653/v1/2024.emnlp-main.992",
    pages = "17889--17904",
    abstract = "Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively. However, our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD) framework, in which multiple agents express their arguments in the state of {\textquotedblleft}tit for tat{\textquotedblright} and a judge manages the debate process to obtain a final solution. Clearly, our MAD framework encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation. Experiment results on two challenging datasets, commonsense machine translation and counter-intuitive arithmetic reasoning, demonstrate the effectiveness of our MAD framework. Extensive analyses suggest that the adaptive break of debate and the modest level of {\textquotedblleft}tit for tat{\textquotedblright} state are required for MAD to obtain good performance. Moreover, we find that LLMs might not be a fair judge if different LLMs are used for agents."
}

@inproceedings{light2023avalonbench,
  title={Avalonbench: Evaluating llms playing the game of avalon},
  author={Light, Jonathan and Cai, Min and Shen, Sheng and Hu, Ziniu},
  booktitle={NeurIPS 2023 Foundation Models for Decision Making Workshop}
}

@inproceedings{lin-etal-2024-criticbench,
    title = "{C}ritic{B}ench: Benchmarking {LLM}s for Critique-Correct Reasoning",
    author = "Lin, Zicheng  and
      Gou, Zhibin  and
      Liang, Tian  and
      Luo, Ruilin  and
      Liu, Haowei  and
      Yang, Yujiu",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.91/",
    doi = "10.18653/v1/2024.findings-acl.91",
    pages = "1552--1587",
    abstract = "The ability of Large Language Models (LLMs) to critique and refine their reasoning is crucial for their application in evaluation, feedback provision, and self-improvement. This paper introduces CriticBench, a comprehensive benchmark designed to assess LLMs' abilities to critique and rectify their reasoning across a variety of tasks. CriticBench encompasses five reasoning domains: mathematical, commonsense, symbolic, coding, and algorithmic. It compiles 15 datasets and incorporates responses from three LLM families. Utilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in generation, critique, and correction reasoning, i.e., GQC reasoning. Our findings reveal: (1) a linear relationship in GQC capabilities, with critique-focused training markedly enhancing performance; (2) a task-dependent variation in correction effectiveness, with logic-oriented tasks being more amenable to correction; (3) GQC knowledge inconsistencies that decrease as model size increases; and (4) an intriguing inter-model critiquing dynamic, where stronger models are better at critiquing weaker ones, while weaker models can surprisingly surpass stronger ones in their self-critique. We hope these insights into the nuanced critique-correct reasoning of LLMs will foster further research in LLM critique and self-improvement."
}

@article{madaan2024self,
  title={Self-refine: Iterative refinement with self-feedback},
  author={Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{mittal2024puzzlebench,
  title={PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?},
  author={Mittal, Chinmay and Kartik, Krishna and Singla, Parag and others},
  journal={arXiv preprint arXiv:2402.02611},
  year={2024}
}

@article{shinn2024reflexion,
  title={Reflexion: Language agents with verbal reinforcement learning},
  author={Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{tyagi2024step,
  title={Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?},
  author={Tyagi, Nemika and Parmar, Mihir and Kulkarni, Mohith and Rrv, Aswin and Patel, Nisarg and Nakamura, Mutsumi and Mitra, Arindam and Baral, Chitta},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={19898--19915},
  year={2024}
}

@article{wang2023avalon,
  title={Avalon's game of thoughts: Battle against deception through recursive contemplation},
  author={Wang, Shenzhi and Liu, Chang and Zheng, Zilong and Qi, Siyuan and Chen, Shuo and Yang, Qisen and Zhao, Andrew and Wang, Chaofei and Song, Shiji and Huang, Gao},
  journal={arXiv preprint arXiv:2310.01320},
  year={2023}
}

@article{xu2023exploring,
  title={Exploring large language models for communication games: An empirical study on werewolf},
  author={Xu, Yuzhuang and Wang, Shuo and Li, Peng and Luo, Fuwen and Wang, Xiaolong and Liu, Weidong and Liu, Yang},
  journal={arXiv preprint arXiv:2309.04658},
  year={2023}
}

@article{yao2024tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

