\section{Related Work}
% \subsection{Reasoning with LLMs}
% \subsection{Reflective Reasoning Evaluation}
\subsection{Reflection Capabilities of LLMs}
The impressive reasoning capabilities of LLMs have naturally led to increased interest in reflection, a more sophisticated and human-like form of reasoning. Previous studies primarily focus on evaluating LLMs' abilities to rectify their response based on explicit feedback, including self-improvement with critique prompts \citep{lan2024criticeval, li2024reflection, lin-etal-2024-criticbench, li2024hindsight, madaan2024self}, leveraging external tools such as code interpreters or search engines \citep{gou2024critic, chen2024teaching, shinn2024reflexion}, and engaging in multi-LLM interaction through debating \citep{liang-etal-2024-encouraging, huang2024large}. However, these works mainly evaluate LLMs' behaviors in response to feedback. They fail to assess LLMs' capabilities to spontaneously engage in the complete reflection process for complex reasoning tasks. Our proposed LR$^2$Bench provides scenarios necessitating capabilities, such as making assumptions, verification, backtracking, and self-refinement, thus filling a critical gap in evaluating LLMs' intrinsic reflective reasoning abilities.

% \subsection{Complex Reasoning Tasks for LLMs}
\subsection{Puzzle-solving for LLMs}
Puzzle-solving \citep{giadikiaroglou2024puzzle} offers valuable insight for evaluating the complex reasoning capabilities of LLMs across diverse scenarios. \citet{ishay2023leveraging} explore Sudoku solving strategies with answer set programming. \citet{ding2023everything} leverages reinforcement learning and Monte Carlo Tree Search to solve problems like Game of 24, 8-Puzzle, and Pocket Cube. \citet{yao2024tree} introduces "Tree of Thought" to enable self-evaluating and backtracking for Game of 24 and Crosswords. \citet{mittal2024puzzlebench} combines LLMs with symbolic solvers and program interpreters to complete first-order combinatorial reasoning problems. \citet{tyagi2024step} focuses on grid puzzles to evaluate the generated reasoning chains of LLMs. Moreover, existing studies have also investigate Board game \citep{kazemi2024boardgameqa}, Chess \citep{feng2024chessgpt} and social games \citep{light2023avalonbench, wang2023avalon, xu2023exploring}. However, these studies primarily leverage external tools or specialized algorithms to develop task-specific solutions within limited puzzle domains. In contrast, LR$^2$Bench provides diverse tasks and difficulty levels and focuses on evaluating the intrinsic reflective reasoning capabilities of LLMs.