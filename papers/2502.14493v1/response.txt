\section{Related Works}
In this section, we first review representative deep learning-based IVIF methods and then discuss the challenges posed by OOD data in real-world applications.

\subsection{Deep Learning-based IVIF}
Deep learning-based IVIF methods primarily includes those based on AE, CNN, GAN, and Transformer as well as CNN-Transformer.

AE-based methods leverage pre-trained autoencoders on large-scale datasets for feature extraction and image reconstruction. These approaches integrate deep features from different modality images through manually designed fusion rules. Li et al., "Densely Connected Encoder Network for Feature Extraction"**, proposed Densefuse**, which introduces a densely connected encoder network for feature extraction, while the corresponding decoder network produces the fusion result based on a manually crafted fusion strategy. Ren et al., "Variational Autoencoders for Image Fusion"**, developed an image fusion network based on Variational Autoencoders (VAE)**. Attention mechanisms have also been incorporated into AE-based fusion frameworks to better focus on characteristics from source images**. However, the performance of these methods is constrained by the reliance on manually designed fusion rules.

In CNN-based image fusion methods**, end-to-end fusion architectures** achieve single-step inference for feature extraction, fusion, and image reconstruction. The incorporation of various network modules and loss functions enhances feature extraction and fusion, resulting in high-quality fused images. Furthermore, in light of integrating fusion with advanced visual tasks, Tang et al., "Scene-Aware Image Fusion"**, proposed SeAFusion** to explore multi-task learning in high-level vision. Yao et al., "Nighttime Image Fusion with Low-Light Enhancement Technology"**, further address the challenge of nighttime image fusion with low-light enhancement technology and knowledge distillation. Additionally, HG-LPFN**, present an novel laplacian pyramid network with hierarchical guidance to explore end-to-end IVIF. However, CNN-based architectures have limitations in extracting out-of-distribution features, posing difficulties in generating fused image with complete content.

GAN-based methods** address the challenge of insufficient supervision information in image fusion through adversarial learning. FusionGAN**, first models the image fusion problem as an adversarial game between a generator and a discriminator, leveraging the distribution properties of source images to generate fused results. To maintain a balance of information from different modality, dual-discriminator structures** and attention mechanisms** have been introduced into fusion methods. However, persistent issues of training instability and interpretability limitations continue to affect these methods, resulting in artifacts and blurred details in the generated images.

The powerful modeling capability of the Transformer**, in handling long-range dependencies has led to its widespread adoption in visual tasks. Recently, architectures incorporating Transformer or its combination with CNN have shown competitive results in image fusion**.
% Wang et al., "SwinFuse: Residual Swin Transformer Network for Image Fusion"**, introduced SwinFuse**. 
Tang et al., "Y-Shape Dynamic Transformer for Image Fusion"**, that comprehensively maintains local features and significant context information through a Y-shape dynamic Transformer.
Zhao et al., "CNN-Transformer Based Fusion Framework"**, proposed a fusion framework based on CNN-Transformer to enhance fusion performance**.

\subsection{OOD Data Challenges in Real-world}
In practical applications, learning-based models often encounter unforeseen samples that deviate from the training distribution, referred to as OOD data. Consequently, model performance on these OOD data is suboptimal, posing a critical challenge for ensuring the reliability and safety of practical applications**. Various techniques, such as domain generalization**, and domain adaptation**, have been developed to enable models to adapt to distribution shifts and improve OOD performance. Injecting domain knowledge through data augmentation or self-supervised pre-training can further enhance domain generalization and improve model adaptability. 

Deep learning-based fusion models are increasingly applied in open-world scenarios, where OOD data challenges are becoming prominent**. In practical applications, IVIF models are particularly susceptible to the influence of OOD data due to environmental changes, variations in objects and scenes, and differences in sensors. Despite continuous improvements in the performance of current IVIF models, there is a notable lack of studies addressing the challenges posed by OOD scenarios during model deployment. This oversight contributes to unresolved issues, including potential performance degradation and increased uncertainty in open-world applications.

To address challenges posed by OOD data in real-world scenarios, we propose a multi-view data augmentation, aiming to enhance the robustness and generalization of the model.