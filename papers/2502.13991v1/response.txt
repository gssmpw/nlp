\section{Related works and Preliminary}
% \textcolor{red}{We can move this part into the beginning of methods. Or we can include it in a preliminary part. Both are fine.}
% \subsection{Problem Formulation}
% We study the problem of gene expression task in DNA field. Formally, given each DNA sequence $X_{seq}=\{x_{seq,1},...,x_{seq,n}\}$ centered on the gene promoters of specific protein-coding genes, our goal is to predict the gene expression levels associated with these promoters. This task is formulated as a sequence regression problem, where we aim to map each input DNA sequence to its corresponding gene expression value.
% To enhance the prediction accuracy, we also incorporate additional DNA epigenomic signals, such as DNase-seq and H3K27ac data, denoted as $X_{sig}=\{x_{sig,1},...,x_{sig,n}\}$, which are of the same length and derived from lab experiments. The signals serve as supplementary features for improving the model's performance in predicting gene expression.

\subsection{Task Description}
% \textbf{Notations.} 
Let $X\textsubscript{seq} = [x_1, \cdots, x_L]$ denote the DNA sequence with length $L$, where each token $x_i \in \mathbb{R}^{4\times 1}$ is a one-hot vector representing a nucleotide from the set \{A, C, G, T\}. For this DNA sequence, the corresponding epigenomic  signals are denoted as $X\textsubscript{sig} = [s_1, \cdots, s_L]$, where $s_i \in \mathbb{R}^{d \times 1}$ represents $d$ different signals. 
By using both the DNA sequence and epigenomic signals, the task aims to predict the target gene expression denoted as $Y \in \mathbb{R}$.
To achieve this target, we propose our framework to extract the active regulatory elements by learning a token-level binary mask $M = [m_1, \cdots, m_L]$, where $m_i \in \{0, 1\}$ or a soft mask $M$ where $m_i \in [0, 1]$.

Specifically, in our implementation, each example contains one target gene. We first identify the transcription start site (TSS) of the target gene, then select input sequences $X\textsubscript{seq}$ and $X\textsubscript{sig}$ consist of $L = 200,000$ base pairs, centered on the TSS.
Then, the entire sequences provide sufficient contextual information for accurate prediction of the target gene expression value $Y$.

\subsection{Related Works}
\noindent\textbf{DNA language model} has been proposed recently to apply language machine learning models to long DNA sequences**Kamath, "Hybrid Models for Long Sequence Modeling"** and solve various downstream tasks.
Two notable methods in this area are **Makinen et al., "Caduceus: Bidirectional Mamba Transformer for Long Sequence Modeling"**, **Shu et al., "Long-DNA: A Language Model for Long DNA Sequences"**. 
**Kamath, "Hybrid Models for Long Sequence Modeling** utilizes the Hyena operator  to process long DNA sequences.
**Makinen et al., "Caduceus: Bidirectional Mamba Transformer for Long Sequence Modeling" introduces bidirectional Mamba ** for DNA sequences, providing linear complexity for long sequence modeling while also considering the reverse complement of the DNA sequences. 
% Although neither method is specifically designed for gene expression prediction, these language models can be easily fine-tuned for such tasks. 
% However, those models rely solely on DNA sequences as input.
These methods offer a powerful approach for modeling long sequence data, such as DNA, and can be fine-tuned for tasks like gene expression prediction. However, they usually only considers DNA sequences as input, and do not explicitly consider the additional epigenomic signals during the prediction. 
Since these signals often carry meaningful information, such as physical interaction frequency and functional activity, incorporating them into the model could further enhance its performance on the gene expression prediction task.
% Since gene expression varies across different cell types, incorporating cell type-specific information, such as epigenomic signals, is crucial to improving the accuracy of gene expression predictions.

\noindent\textbf{Gene expression prediction} is one of the fundamental tasks in bioinformatics**Friedlaender et al., "Enformer: A Language Model for Gene Expression Prediction"**. 
Numerous studies**Chen et al., "GraphReg: A Graph Attention Network for Gene Expression Prediction"** have attempted to predict gene expression values directly from DNA sequences. 
**Friedlaender et al., "Enformer: A Language Model for Gene Expression Prediction"**, for instance, tries to only encode DNA sequences as input and employs convolutional and transformer blocks to predict 5,313 human genomic tracks and 1,643 mouse tracks. 
In contrast, **Chen et al., "GraphReg: A Graph Attention Network for Gene Expression Prediction"** incorporates a graph attention network to account for Hi-C/HiChIP interactions between DNA sub-sequences, improving gene expression predictions by considering physical interaction frequencies.
However, both methods either rely on epigenomic signals or DNA sequences as input data, without integrating both data types.  
Recently, **Zhang et al., "EPInformer: A Framework for Integrating DNA Sequences and Epigenomic Signals"** has advanced this approach by integrating both DNA sequences and epigenomic signals for gene expression prediction. EPInformer first identifies enhancer regions from the DNA sequences based on DNase-seq signals, treating epigenomic signals as enhancer features, and then use promoter-enhancer interactions for gene expression prediction. 
Despite this progress, EPInformer selects enhancer regions solely based on epigenomic signal peaks, overlooking the complex relationships between DNA sequences, epigenomic signals, and predicted gene expression values. This highlights the need for machine learning methods capable of learning to extract relevant regions in a more comprehensive manner.

% However, most of these methods rely solely on DNA sequences as input. Since gene expression varies across different cell types, it is essential to incorporate cell type-specific information, such as epigenomic signals, to enhance the accuracy of gene expression predictions. In this work, we propose to incorporate the epigenomic signals in the process of finding the regulatory elements that have the most significant impact on gene expression values. The work most similar to ours is EPInformer. However, while EPInformer selects enhancer regions based solely on the peaks of epigenomic signals, we propose a method that learns to select regulatory elements specifically for the gene expression prediction task, taking into account the complex relationships between DNA sequences, epigenomic signals, and predicted gene expression values.

% \noindent\textbf{Rationalize Neural Predictions.}
% Clarify the key to find the promoter-related regions, such as the interaction between the promoters and the interactions between the promoter and enhancer.

% Equation~\ref{IB} describes how to learn compressed representations $Z$ rather than selecting specific sub-sequences. To directly select regulatory elements, we define the latent representations as $Z = m \odot X$, where $m$ is a binary variable controlling the selection of each DNA base pair. We assume that each selection is independent given the input sequence $X$, i.e., $p(m|X) = \prod_i p(m_i|X)$. Following the method of **Srivastava et al., "DeepDREAM: A Deep Learning Framework for Sequence Selection"**, the objective becomes:
% \begin{equation}
% \label{obj}
%     L \approx \frac{1}{N} \sum_{i=1}^N \mathbb{E}_{p_{\theta}(m_i|x_i)} [-log q_{\phi} (y_i|m_i\odot x_i)] + \beta KL[p_{\theta}(m_i|x_i), r(m_i)],
% \end{equation}
% where the first term is the task-specific loss, such as mean square error in DNA gene expression prediction, and the second term imposes a constraint on the learned mask $m$, aligning it with the predefined distribution $r(m)$ without conditioning on any specific sequence $x$. In our case, we use this second term to enforce sparsity in the learned regulatory elements.