
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{amsthm}
\usepackage{graphicx}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{assumption}{Assumption}
\newtheorem{proposition}{Proposition}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{wrapfig}
\iclrfinalcopy
\urlstyle{same}
\newcommand{\redtext}[1]{\textcolor{red}{#1}}

\title{Learning to Discover Regulatory Elements for Gene Expression Prediction}
% Hi, Dr. Zhi, we are checking and adding necessary citations to suitable places in introduction and related works. If you find any reference is not in a suitable positions or any important works needs to be cited, feel free to modify the papars. Thanks! 

% sure. We can use the chatbox at the right?
% Yes. Feel free to add comments. By the way, we are trying to change a new name for our methods. Currently, we have Seq2Exp, DNA2Exp and Reg2Exp. If you have any ideas about the suitable names, welcome to propose.
% I mean the chat, not the comment. Chat is at the upper right corner
% Sure. Just find it.

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Xingyu Su\textsuperscript{\textnormal{1*}}, Haiyang Yu\textsuperscript{\textnormal{1*}}, Degui Zhi\textsuperscript{\textnormal{2}} \& Shuiwang Ji\textsuperscript{\textnormal{1\dag}} 
% \thanks{ Use footnote for providing further information
% about author (webpage, alternative address)---\emph{not} for acknowledging
% funding agencies.  Funding acknowledgements go at the end of the paper.} 
\\
% Department of Computer Science\\
\textsuperscript{1}Texas A\&M University, \textsuperscript{2}The University of Texas Health Science Center at Houston\\
\texttt{\{xingyu.su,haiyang,sji\}@tamu.edu,\{degui.zhi\}@uth.tmc.edu} \\
% \And
% Ji Q. Ren \& Yevgeny LeNet \\
% Department of Computational Neuroscience \\
% University of the Witwatersrand \\
% Joburg, South Africa \\
% \texttt{\{robot,net\}@wits.ac.za} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{Equal contribution.}
\footnotetext[2]{Corresponding author.}


% (
% What is our task.
% What is the gap of current method. 
% What we have proposed. 
% What is the internal mechanism that make our method different and powerful. 
% (Take away.) (concretely for at least two or three points)
% What performance we have achieved.
% Any insight for following research.
% )
\begin{abstract}
We consider the problem of predicting gene expressions from DNA sequences. 
A key challenge of this task is to find the regulatory elements that control gene expressions.
Here, we introduce Seq2Exp, a \textbf{Seq}uence to \textbf{Exp}ression network explicitly designed to discover and extract regulatory elements that drive target gene expression, enhancing the accuracy of the gene expression prediction.
Our approach captures the causal relationship between epigenomic signals, DNA sequences and their associated regulatory elements. 
% Specifically, we frame the learning of these regulatory elements within the information bottleneck principle. We propose to decouple the epigenomic signals and DNA sequences to extract regulatory elements, employing a Beta distribution to determine their positions and filter out non-causal components.
% Specifically, we employ the information bottleneck principle to guide the learning of regulatory elements, proposing a decoupling of epigenomic signals and DNA sequences for more effective extraction. A Beta distribution is used to pinpoint the regulatory element positions and filter out non-causal components.
Specifically, we propose to decompose the epigenomic signals and the DNA sequence conditioned on the causal active regulatory elements, and apply an information bottleneck with the Beta distribution to combine their effects while filtering out non-causal components.
% And apply the information bottleneck with beta distribution to combine the effect of these two and filter out non-causal components.
% Our approach explicitly integrates both epigenomic signals and DNA sequences via modeling their causal relationship to the active regulatory elements, and filtering out non-causal components by information bottleneck. 
% Our experiments demonstrate that RegExNet outperforms existing baselines by focusing on regulatory element extraction, highlighting its effectiveness in identifying critical regulatory regions.
Our experiments demonstrate that Seq2Exp outperforms existing baselines in gene expression prediction tasks and discovers influential regions compared to commonly used statistical methods for peak detection such as MACS3.
The source code is released as part of the AIRS library (\url{https://github.com/divelab/AIRS/}).
% Our framework provide a gene expression prediction enhancement and offers insights into the underlying regulatory mechanisms, 
% Our framework not only improves gene expression prediction but also offers insights into the underlying regulatory mechanisms, offering new opportunities for further exploration in epigenomic research and functional genomics.
\end{abstract}

% I need to provide several version to choose from.


\section{Introduction}
% The flow of introduction
% What deep learning have achieved in the gene expression prediction
% Why extracting related region is important in these tasks
% Here, we need to show two things. 
% 1. The ratio of mask;
% 2. the F1 that's to say, although we find some mask with statistic methods, many of them are not related at all.
% Here, we describe the kind of data with causal relationship
% Guided by this causal relationship, we propose our framework, with several modules: module name and their target.
% summarization of our contributions.
% We illustrate the causal relationships between the .
% We provide a framework based on information bottleneck to .
% We achieve performance, and comparsion to statistical methods.
% \textcolor{red}{Draft introduction.}
Gene expression serves as a fundamental process that dictates cellular function and variability, providing insights into the mechanisms underlying development~\citep{pratapa2020benchmarking}, disease~\citep{cookson2009mapping, emilsson2008genetics}, and responses to external factors~\citep{schubert2018perturbation}. 
Despite the critical importance of gene expression, predicting it from genomic sequences remains a challenging task due to the complexity and variability of regulatory elements involved. 
Recent advances in deep learning techniques~\citep{enformer, gu2023mamba, hyenaDNA, badia2023gene} have shown remarkable capabilities and performance in modeling long sequential data like language and DNA sequence. 
By capturing intricate dependencies within genomic data, these techniques provide a deeper understanding of how regulatory elements contribute to transcription
% These techniques offer a powerful approach to enhance the performance of gene expression prediction, facilitating a deeper understanding of how regulatory elements contribute to transcription
~\citep{aristizabal2020biological}.
% (\textcolor{red}{Can you help me add more background information or related works in this place?})
% Deep learning has shown its dramatic power in gene expression prediction, offering models that can process vast and complex biological 
% data with greater accuracy than traditional approaches.
% Recent advances, such as the SSM Mamba model, have shown remarkable capabilities in handling long sequences with linear complexity, making them particularly powerful for applications like DNA sequence analysis.



To predict gene expression, DNA language models are usually applied to encode long DNA sequences with a subsequent predictor to estimate the gene expression values~\citep{enformer, hyenaDNA, gu2023mamba, caduceus}. However, those language models are typically designed to encode DNA sequences alone, overlooking the specific environments like different cell types, which leads to suboptimal performance. 
% Other approaches attempt to enhance prediction accuracy by incorporating epigenomic signals. 
Instead of predicting the gene expression only using DNA sequence, which is invariant across cell types, a more biological relevant formulation is to predict gene expression levels using both DNA sequence and epigenomic signals.
For example, GraphReg~\citep{graphreg} uses epigenomic signals as input data to predict gene expression values. 
However, it does not integrate DNA sequences and epigenomic signals in a unified manner to improve gene expression prediction.
% , but it does not consistently yield improved results, indicating that directly using epigenomic signals as input sequences does not always result in positive gains. 
% EPInformer~\citep{ABC, epinformer} focuses on regulatory elements identified by statistical methods from epigenomic signals, it neglects the complex interactions between DNA sequences, epigenomic signals, and regulatory elements, which can further influence gene expression.
EPInformer~\citep{epinformer} uses statistical methods to identify the epigenomic signal peaks, and focuses on regulatory elements identified by those peaks. Although obtaining better results, EPInformer still neglects the complex relationship between DNA sequences, epigenomic signals and regulatory elements, which is essential for improving prediction accuracy.


% A key challenge that arises from these complex relationships is to extract the specific regulatory element regions that influence gene activity.
% % One key challenge to improve the gene expression prediction is to extract relevant regions of genomic data that influence gene activity.
% % Since gene expression is not solely dependent on the genome's DNA sequence, it is also affected by factors such as cell type and tissue environment, making the task of selecting these regions complex~\citep{segal2003module}. 
% Gene expression is not solely dependent on DNA sequences but is also affected by factors such as cell type and tissue environment~\citep{segal2003module}.
% % Meanwhile, the relevant regions are usually sparse and may involve long-range interactions. 
% Moreover, these relevant regulatory regions are often sparse and may involve long-range interactions, making the task of extracting and integrating these regions into predictive models particularly difficult. This highlights the need for models capable of capturing regulatory interactions in the context of long DNA sequences.


% Therefore, one of the key challenges in understanding gene regulation is discovering the specific regulatory regions that influence gene activity.
The task of predicting gene expression levels given the DNA sequences and epigenomic signals presents several challenges. First, epigenomic signals can be measured by a variety of experimental techniques, including 
% methylation array, 
ChIP-seq, DNase-seq, Hi-C, each with their own biases and limitations~\citep{encode2012integrated, bernstein2010nih, encode2020expanded}.
% The task of predicting gene expression presents several challenges. First, gene expression is influenced not only by DNA sequences but also by factors such as cell types and experimental conditions~\citep{yoshida2019cis, segal2003module}. 
Additionally, the regulatory elements influencing target gene expression are often sparse and may involve long-range interactions, making them challenging to identify and integrate into predictive models. These complexities highlight the need for models that can effectively discover the actively interacted regulatory elements with the target gene on long DNA sequences.


In response to these challenges, we propose Seq2Exp (\textbf{Seq}uence to \textbf{Exp}ression), a novel framework designed to improve gene expression prediction by selectively extracting relevant sub-sequences from both DNA sequences and epigenomic  signals. 
Since DNA sequences and epigenomic signals capture different aspects of biological information, their integration offers deeper insights.
For example, Hi-C/HiChIP data reveals the physical interaction frequency between distal DNA regions, and DNase-seq reflects the functional activity of regulatory elements. 
Effectively incorporating these signals along with DNA sequences can be highly beneficial for addressing the above challenges for gene expression prediction task.
Specifically, in this work, we suggest the causal relationship between genomic data and gene expression to guide the learning process as depicted in Figure~\ref{fig:causal_relationship}.
Inspired by the causal relationship, we decompose the mask learning process into two components: one based on DNA sequences and the other on epigenomic  signals. 
The proposed Seq2Exp first employs a generator module to learn a token-level mask based on both DNA sequences and epigenomic  signals, to extract DNA sub-sequences. Then, the predictor module is applied on these extracted sub-sequences to predict gene expression.
With information bottleneck, Seq2Exp can effectively filter out non-causal parts by constraining the mask size, ensuring that only the most relevant regions are extracted.
Overall, the incorporation of the DNA sequences and epigenomic  signals systematically discovers regions that are likely to influence gene expression.
% Specifically, we suggest the causal relationships between epigenomic  data and gene expression, which guide the learning process. 
% % , as depicted in Figure~\ref{fig:causal_relationship}.
% Using the causal relationship, we decompose the mask learning process into two components: one based on DNA sequences and the other on epigenomic  signals. 

% % The mask information learned from epigenomic  signals are combined with those derived from DNA sequences to discover regions likely to impact gene expression.

% To model the mask distribution, we employ a Beta distribution to quantify the probability of selecting specific tokens from the DNA sequences. 
% These masks are used to select informative sub-sequences, which are subsequently fed into the predictor module to estimate gene expression values.



% We then use Beta distribution to model the mask distribution and sample from the distribution to get the mask.
% Finally, we make the prediction based on the sub-sequences selected by the mask.

% Epigenomic data and Hi-C signals have been introduced to provide insights into gene activity in the current environment and the frequency of epigenomic  interactions. These mask probability learned from these signals are combined with mask learned from DNA sequence to discover the regions that may influence gene expression.
We summarize our contributions here:
\begin{itemize}[left=2pt]
 \item We propose a framework articulating the causal relationship between epigenomic signals, DNA sequences, target gene expression and related regulatory elements.
 \item Based on the causal relationships, our framework is proposed to combine the mask probability distribution from DNA sequences and epigenomic signals, and filtering out non-causal region via information bottleneck.
 \item The proposed Seq2Exp achieves SOTA performances compared to previous gene expression prediction baselines, and demonstrates the extracted regulatory elements serve as a better sub-sequences compared to statistical peaks calling methods of epigenomic signals such as MACS3.
\end{itemize}

\section{Related works and Preliminary}

% \textcolor{red}{We can move this part into the beginning of methods. Or we can include it in a preliminary part. Both are fine.}
% \subsection{Problem Formulation}
% We study the problem of gene expression task in DNA field. Formally, given each DNA sequence $X_{seq}=\{x_{seq,1},...,x_{seq,n}\}$ centered on the gene promoters of specific protein-coding genes, our goal is to predict the gene expression levels associated with these promoters. This task is formulated as a sequence regression problem, where we aim to map each input DNA sequence to its corresponding gene expression value.
% To enhance the prediction accuracy, we also incorporate additional DNA epigenomic signals, such as DNase-seq and H3K27ac data, denoted as $X_{sig}=\{x_{sig,1},...,x_{sig,n}\}$, which are of the same length and derived from lab experiments. The signals serve as supplementary features for improving the model's performance in predicting gene expression.

\subsection{Task Description}
% \textbf{Notations.} 
Let $X\textsubscript{seq} = [x_1, \cdots, x_L]$ denote the DNA sequence with length $L$, where each token $x_i \in \mathbb{R}^{4\times 1}$ is a one-hot vector representing a nucleotide from the set \{A, C, G, T\}. For this DNA sequence, the corresponding epigenomic  signals are denoted as $X\textsubscript{sig} = [s_1, \cdots, s_L]$, where $s_i \in \mathbb{R}^{d \times 1}$ represents $d$ different signals. 
By using both the DNA sequence and epigenomic signals, the task aims to predict the target gene expression denoted as $Y \in \mathbb{R}$.
To achieve this target, we propose our framework to extract the active regulatory elements by learning a token-level binary mask $M = [m_1, \cdots, m_L]$, where $m_i \in \{0, 1\}$ or a soft mask $M$ where $m_i \in [0, 1]$.

Specifically, in our implementation, each example contains one target gene. We first identify the transcription start site (TSS) of the target gene, then select input sequences $X\textsubscript{seq}$ and $X\textsubscript{sig}$ consist of $L = 200,000$ base pairs, centered on the TSS.
Then, the entire sequences provide sufficient contextual information for accurate prediction of the target gene expression value $Y$.

\subsection{Related Works}
\noindent\textbf{DNA language model} has been proposed recently to apply language machine learning models to long DNA sequences~\citep{hyenaDNA,gu2023mamba,caduceus} and solve various downstream tasks.
Two notable methods in this area are HyenaDNA \citep{hyenaDNA} and Caduceus \citep{caduceus}. 
HyenaDNA utilizes the Hyena operator \citep{hyena_op} to process long DNA sequences.
Caduceus introduces bidirectional Mamba~\citep{gu2023mamba} for DNA sequences, providing linear complexity for long sequence modeling while also considering the reverse complement of the DNA sequences. 
% Although neither method is specifically designed for gene expression prediction, these language models can be easily fine-tuned for such tasks. 
% However, those models rely solely on DNA sequences as input.
These methods offer a powerful approach for modeling long sequence data, such as DNA, and can be fine-tuned for tasks like gene expression prediction. However, they usually only considers DNA sequences as input, and do not explicitly consider the additional epigenomic signals during the prediction. 
Since these signals often carry meaningful information, such as physical interaction frequency and functional activity, incorporating them into the model could further enhance its performance on the gene expression prediction task.
% Since gene expression varies across different cell types, incorporating cell type-specific information, such as epigenomic signals, is crucial to improving the accuracy of gene expression predictions.

\noindent\textbf{Gene expression prediction} is one of the fundamental tasks in bioinformatics~\citep{segal2002promoter}. 
Numerous studies~\citep{Xpresso,graphreg,enformer,epinformer} have attempted to predict gene expression values directly from DNA sequences. 
Enformer~\citep{enformer}, for instance, tries to only encode DNA sequences as input and employs convolutional and transformer blocks to predict 5,313 human genomic  tracks and 1,643 mouse tracks. 
In contrast, GraphReg~\citep{graphreg}, incorporates a graph attention network to account for Hi-C/HiChIP interactions between DNA sub-sequences, improving gene expression predictions by considering physical interaction frequencies.
However, both methods either rely on epigenomic  signals or DNA sequences as input data, without integrating both data types.  
Recently, EPInformer~\citep{epinformer} has advanced this approach by integrating both DNA sequences and epigenomic signals for gene expression prediction. EPInformer first identifies enhancer regions from the DNA sequences based on DNase-seq signals, treating epigenomic signals as enhancer features, and then use promoter-enhancer interactions for gene expression prediction. 
Despite this progress, EPInformer selects enhancer regions solely based on epigenomic signal peaks, overlooking the complex relationships between DNA sequences, epigenomic signals, and predicted gene expression values. This highlights the need for machine learning methods capable of learning to extract relevant regions in a more comprehensive manner.

% However, most of these methods rely solely on DNA sequences as input. Since gene expression varies across different cell types, it is essential to incorporate cell type-specific information, such as epigenomic signals, to enhance the accuracy of gene expression predictions. In this work, we propose to incorporate the epigenomic signals in the process of finding the regulatory elements that have the most significant impact on gene expression values. The work most similar to ours is EPInformer. However, while EPInformer selects enhancer regions based solely on the peaks of epigenomic signals, we propose a method that learns to select regulatory elements specifically for the gene expression prediction task, taking into account the complex relationships between DNA sequences, epigenomic signals, and predicted gene expression values.

% \noindent\textbf{Rationalize Neural Predictions.}
% Clarify the key to find the promoter-related regions, such as the interaction between the promoters and the interactions between the promoter and enhancer.

% (Interpretable and singal enhancement for the selection of input sequence)
% Sequence-only, signal based interaction including HiC - sequence interaction, DHS, H3 - denoting the region belongs to regulatory elements. papers to support. 
% High level idea: such signal provides a biology information about interactions and activities related to and beyond the gene expression. Specifically considering these . 
% While the information avoid the dummy region that introduce additional noisy to the final regression problem.


%% Introduce some in the introduction, and point out the difference and relationship to this work.% Rationalizing Neural Predictions (RNP) has been a longstanding research topic focused on identifying the most influential sentences that contribute to sentence classification outcomes. While the primary goal of RNP is model interpretability, its framework is applicable to our problem setting. Early methods introduced the foundational RNP framework, which consists of a predictor that learns a sentence mask and an encoder that predicts classification results. Subsequent approaches have explored various strategies to avoid the use of reinforcement learning. XX incorporates information bottleneck theory to learn a sentence mask. Although these works are rooted in natural language processing, we are the first to implement RNP in the field of DNA sequence modeling, addressing the integration of DNA sequences and epigenomic signals.

% INformation bottlneck and relationale extraction. \cite One more step is need to encode the biological information into the bottleneck part.


\subsection{Background of Information Bottleneck}
% Add citation here
To effectively extract active regulatory elements from DNA sequences, it is important to understand the concept of the information bottleneck. 
The information bottleneck method is a widely used technique in machine learning on tasks for images~\citep{alemi2016deep, chen2018learning}, language data~\citep{belinkov2020variational, lei2016rationalizing, rnp_ib, bastings2019interpretable, jain2020learning} or graph data~\citep{wu2020graph, miao2022interpretable}.
Its goal is to maximize the mutual information between compressed representations $Z$ and the target variable $Y$, expressed as $I(Z; Y)$, while controlling the information extracted from the input $X$. Note that in the proposed method, $Y$ represents the target gene expression.
A straightforward approach would be to set $Z = X$, but this retains the full complexity of $X$, which makes the optimization process challenging, especially with the long and noisy nature of DNA sequences. 

To address this, researchers impose a constraint on the information transferred from $X$ to $Z$, ensuring that $I(X;Z) \leq I_c$, where $I_c$ is an information constraint that allows us to capture only the most critical compressed representations. The information bottleneck objective becomes maximizing:
\begin{equation}
\label{prelim:MI}
    L = I(Z;Y) - \beta I(X;Z),
\end{equation}
where $\beta$ is a hyperparameter that balances the trade-off between compression and relevance. However, directly optimizing this objective is challenging. To overcome this, \citet{chen2018learning} proposes to maximize a lower bound approximation, which leads to minimizing the following expression:
\begin{equation}
\label{IB}
    L \approx \frac{1}{N} \sum_{i=1}^N \mathbb{E}_{p_{\theta}(Z|x_i)} [-\log q_{\phi} (y_i|Z)] + \beta KL[p_{\theta}(Z|x_i), r(Z)],
\end{equation}
where $p_{\theta}(Z|x_i)$ is a parametric approximation of $Z$, $q_{\phi}(y_i|Z)$ is a variational approximation of the true distribution $p(y_i|Z)$, and $r(Z)$ approximates the marginal distribution $p(Z)$.


% Equation~\ref{IB} describes how to learn compressed representations $Z$ rather than selecting specific sub-sequences. To directly select regulatory elements, we define the latent representations as $Z = m \odot X$, where $m$ is a binary variable controlling the selection of each DNA base pair. We assume that each selection is independent given the input sequence $X$, i.e., $p(m|X) = \prod_i p(m_i|X)$. Following the method of \citet{rnp_ib}, the objective becomes:
% \begin{equation}
% \label{obj}
%     L \approx \frac{1}{N} \sum_{i=1}^N \mathbb{E}_{p_{\theta}(m_i|x_i)} [-log q_{\phi} (y_i|m_i\odot x_i)] + \beta KL[p_{\theta}(m_i|x_i), r(m_i)],
% \end{equation}
% where the first term is the task-specific loss, such as mean square error in DNA gene expression prediction, and the second term imposes a constraint on the learned mask $m$, aligning it with the predefined distribution $r(m)$ without conditioning on any specific sequence $x$. In our case, we use this second term to enforce sparsity in the learned regulatory elements.



\section{Proposed Methods}
% In this section, we will show the process of solving the gene expression prediction task. We will first cast the prediction task into the framework of extracting regulatory elements in DNA. Then, we will describe the model architectures. 
% \subsection{Regulatory Elements Extraction}
% To get a good gene expression prediction from a long and noisy DNA sequence, we try to extract the critical elements that highly influence the gene expression. While the epigenomic signals are indicators to show which elements are critical for the gene expression, a natural question is: how to extract the critical elements based on the DNA sequences and epigenomic signals?
% WHY RNP ???

In this section, we present our framework Seq2Exp. We first present our motivation for predicting gene expression with learnable extraction of effective regulatory elements.
We illustrate the causal relationship among regulatory elements, epigenomic  signals and DNA sequences as shown in Figure~\ref{fig:causal_relationship}.
Motivated by this structural causal model (SCM)~\citep{pearl2009causal,pearl2000models, wudiscovering}, our framework provides a learnable approach to effectively extract effective regulatory elements, considering both DNA sequences and epigenomic signals, through an information bottleneck mechanism. 




% \textbf{Notations.} Let $X\textsubscript{seq} = [x_1, \cdots, x_L]$ denote the DNA sequence with length $L$, where each token $x_i \in \mathbb{R}^{4\times 1}$ is a one-hot vector representing a nucleotide from the set \{A, T, G, C\}. For this DNA sequence, the corresponding epigenomic signals are denoted as $X\textsubscript{sig} = [s_1, \cdots, s_L]$, where $s_i \in \mathbb{R}^{d \times 1}$ represents $d$ different signals. 
% By using both the DNA sequence and epigenomic signals, the task aims to predict the target gene expression denoted as $Y \in \mathbb{R}$. To achieve this target, we propose our framework to extract the active regulatory element by learning a token-level mask $M = [m_1, \cdots, m_L]$, where $m_i \in \{0, 1\}$.

% In this work, we cast the prediction task within the framework of extracting regulatory elements. Given a sequence $X=\{X\textsubscript{seq}, X\textsubscript{sig}\}$, we aim to learn the token-level mask $m$ on the input sequences and get several sub-sequences $Z=m\odot X$, and then make predictions $Y$ on these sub-sequences. On the one hand, these sub-sequences improve the accuracy of gene expression predictions by retaining only the most critical elements to enhance generalization; on the other hand, those critical elements should be interpretable as regulatory elements, offering insights into the factors influencing gene expression in the DNA field.

% We will first review the background of information bottleneck. Then, we try to decouple the DNA sequences and epigenomic signals of the input data; thus modeling the mask learning process.

\begin{figure}[t]
    \vspace{-5mm}
    \centering
    \includegraphics[width=0.85\linewidth]{figs/causal_figure.jpg}
    \caption{Causal relationships between epigenomic signals, sequence, gene expression $Y$ and related regulatory elements.}
    \label{fig:causal_relationship}
    \vspace{-4mm}
\end{figure}
\subsection{causal relationship among regulatory elements, DNA sequence and epigenomic  signals}
\label{causal}

The interactions between target gene and regulatory elements are complex, particularly when multiple potential regulatory elements are involved. Meanwhile, long sequences and distal interactions require a large search region, further complicating the discovery of effective regulatory elements that influence target gene expression. 
In this study, we take use of epigenomic signals $X\textsubscript{sig}$ from laboratory experiments as well as the DNA sequence $X\textsubscript{seq}$ for target gene expression $Y$, and formulate their relationships with the proposed three categories of regulatory elements.
\begin{itemize}[left=2pt]
    \item $R\textsubscript{g}$: Regulatory elements that have the potential to interact with target gene. However, they might not influence target gene expression if they are inactive in a specific cell type or are distant from the target gene.
    \item $R\textsubscript{m}$: Regulatory elements discovered from measurement. Typically, the region with strong measured epigenomic signals, such as peaks in DNase-seq, are more likely to influence the gene expression. However, there are usually multiple genes within a sequence and the association of $R\textsubscript{m}$ with target gene remains unknown.
    \item $R\textsubscript{ag}$: Regulatory elements actively interacted with target gene. It is identified as the causal component for the final target gene expression $Y$.
\end{itemize}

The causal relationship between these variables is depicted in Figure~\ref{fig:causal_relationship}. Note that each variable corresponds to a distribution and link represents a causal connection. The flow of this SCM illustrates the perspective of data generation.

% \textcolor{red}{There are still some problems. And I will write from my view. There still need to introduce what is SCM, and how does the variables are connected from the view of data generation.}
\begin{itemize}[left=2pt]
    \item $X\textsubscript{seq} \longleftarrow R\textsubscript{g}$. The DNA sequence consists of $R\textsubscript{g}$ and other non-causal parts.
    
    \item $R\textsubscript{ag} \longrightarrow Y$. The causal part $R\textsubscript{ag}$ directly influences the final gene expression. For example, an active enhancer interacting with a gene can significantly impact its expression.
    

    \item $R\textsubscript{g} \longleftarrow R\textsubscript{ag} \longrightarrow R\textsubscript{m}$. The key causal component $R\textsubscript{ag}$ is shared by both $R\textsubscript{g}$ and $R\textsubscript{m}$. It can be detected through epigenomic signals in laboratory experiments and also participates in interactions with the target gene.

    
    \item $R\textsubscript{m} \longrightarrow X\textsubscript{sig}$. $X\textsubscript{sig}$ usually contains strong observable signals, such as peaks in DNase-seq, whereas regions without such signals often provide limited useful information.
    % whereas signals in other regions tend to contain limited information.
    
\end{itemize}


% The interaction of DNA sequence is complicated, especially consider relationship of multiple region. Long sequence data, and long range sequence are causing disaster to understand the the reason for the target. 
% In this work, we try to enc, and provide three different kinds of regulatory elements that will affect the final .



% However, we can consider different types of data from the perspective of data generation. 
% Here, we consider from the perspective of data generation. 
% (I need to think about this)
% (Figure here)
% (explain each component within the figure, and how to build the sequence data once we already have the corresponding key components)

% In a summarization, XX.


% \subsection{Background of Information Bottleneck}
% % Add citation here
% To obtain informative sub-sequences $Z$, we aim to maximize the mutual information between $Z$ and our target $Y$, represented as $I(Z;Y)$. A straightforward solution to this is to set $Z = X$. However, incorporating the full sequence may not necessarily lead to better predictions, as DNA sequences are extremely long and contain a significant amount of irrelevant or noisy information, making optimization difficult. Thus, similar to the information bottleneck principle, we impose a constraint on the information transferred from $X$ to $Z$, ensuring that $I(X;Z) \leq I_c$, where $I_c$ is an information constraint that allows us to capture only the most critical DNA sub-sequences. The information bottleneck objective becomes maximizing:
% \begin{equation}
%     L = I(Z;Y) - \beta I(X;Z),
% \end{equation}
% where $\beta$ is a hyperparameter. However, directly optimizing this objective is challenging. To address this, researchers often maximize a lower bound approximation, which leads to minimizing the following expression:
% \begin{equation}
% \label{IB}
%     L \approx \frac{1}{N} \sum_{i=1}^N \mathbb{E}_{p_{\theta}(Z|x_i)} [-log q_{\phi} (y_i|Z)] + \beta KL[p_{\theta}(Z|x_i), r(Z)],
% \end{equation}
% where $p_{\theta}(Z|x_i)$ is a parametric approximation of $Z$, $q_{\phi}(y_i|Z)$ is a variational approximation of the true distribution $p(y_i|Z)$, and $r(Z)$ approximates the marginal distribution $p(Z)$.



% Equation~\ref{IB} describes how to learn compressed representations $Z$ rather than selecting specific sub-sequences. To directly select regulatory elements, we define the latent representations as $Z = m \odot X$, where $m$ is a binary variable controlling the selection of each DNA base pair. We assume that each selection is independent given the input sequence $X$, i.e., $p(m|X) = \prod_i p(m_i|X)$. Following the method of \citet{rnp_ib}, the objective becomes:
% \begin{equation}
% \label{obj}
%     L \approx \frac{1}{N} \sum_{i=1}^N \mathbb{E}_{p_{\theta}(m_i|x_i)} [-log q_{\phi} (y_i|m_i\odot x_i)] + \beta KL[p_{\theta}(m_i|x_i), r(m_i)],
% \end{equation}
% where the first term is the task-specific loss, such as mean square error in DNA gene expression prediction, and the second term imposes a constraint on the learned mask $m$, aligning it with the predefined distribution $r(m)$ without conditioning on any specific sequence $x$. In our case, we use this second term to enforce sparsity in the learned regulatory elements.



\subsection{Task Objective}
Based on information bottleneck, Equation~\ref{IB} describes how to learn compressed representations $Z$ rather than selecting specific sub-sequences. To directly select regulatory elements, we define the latent representations as $Z = M \odot X$, where $M$ is a binary variable controlling the selection of each DNA base or a soft mask $M$ indicating the importance of each DNA base. We assume that each selection is independent given the input sequence $X$, i.e., $p(M|X) = \prod_i p(m_i|X)$. Following the method of \citet{rnp_ib}, the objective becomes:
\begin{equation}
\label{obj}
    L \approx \frac{1}{N} \sum_{i=1}^N \mathbb{E}_{p_{\theta}(m_i|x_i)} [-\log q_{\phi} (y_i|m_i\odot x_i)] + \beta KL[p_{\theta}(m_i|x_i), r(m_i)],
\end{equation}
where the first term is the task-specific loss, such as mean square error in DNA gene expression prediction, and the second term imposes a constraint on the learned mask $m$, aligning it with the predefined distribution $r(m)$ without conditioning on any specific sequence $x$. In our case, we use this second term to enforce sparsity in the learned regulatory elements.




\subsection{Decomposition of Sequences and Signals}
By using information bottleneck shown in Equation~\ref{obj}, our primary focus is on estimating $p_{\theta}(M|X)$, i.e., learning the mask from the input sequences. Given that the input $X$ consists of both DNA sequences and epigenomic signals, we need to estimate $p_{\theta}(M|\{X_{seq},X_{sig}\})$.

\begin{assumption}[Conditional Independence of Sequences and Signals]
\label{inde_assume}
We assume that, conditioned on the selection of regulatory elements $M$, the DNA sequences and epigenomic signals are conditional independent to each other, i.e.,
    \begin{equation}
        p(X_{sig},X_{seq}|M)=p(X_{sig}|M)p(X_{seq}|M)
    \end{equation}
\end{assumption}

Assumption~\ref{inde_assume} is based on the causal relationships outlined in Section~\ref{causal}. The selected sub-sequences of a full given sequence, represented by $M\odot X$, can be viewed as the optimal regulatory elements ($R_{ag}$) for a specific gene in a particular cell type. 
From a data generation perspective, both the regulatory elements detected through measurements ($R_m$) and those interacting with the gene ($R_g$) originate from the optimal regulatory elements ($R_{ag}$). Therefore, given the optimal regulatory elements, the distributions $p(X_{sig}|M)$ and $p(X_{seq}|M)$ should be independent of each other.

% From a data generation perspective, the  the optimal regulatory elements are determined by both the DNA sequences and the environment (e.g., cell type, specific gene, etc.). The epigenomic signals, on the other hand, reflect the activity and interaction frequency of these regulatory elements. Therefore, conditioned on $m$, the DNA sequences should be independent of the epigenomic signals.

\begin{proposition}
\label{propo_decom}
    Based on Assumption~\ref{inde_assume}, the estimation of \( p_{\theta}(M|X) \) can be decomposed into terms involving \( X_{seq} \) and \( X_{sig} \). Specifically, we have
    \begin{equation}
        p_{\theta}(M|X) \propto p_{\theta_1}(M|X_{seq}) p_{\theta_2}(M|X_{sig}),
    \end{equation}
    where \( p_{\theta_1}(M|X_{seq}) \) and \( p_{\theta_2}(M|X_{sig}) \) represent the contributions from the DNA sequence and the epigenomic signals, respectively.
\end{proposition}
The detailed proof of this decomposition is provided in Appendix~\ref{decom}. Proposition~\ref{propo_decom} allows us to factorize the estimation of \( p_{\theta}(M|X) \) into two independent components, corresponding to the DNA sequence \( X_{seq} \) and the epigenomic signals \( X_{sig} \). As a result, we can independently estimate \( p_{\theta_1}(M|X_{seq}) \) and \( p_{\theta_2}(M|X_{sig}) \), which simplifies the overall estimation process. This decomposition is based on the assumption that, conditioned on the selection of regulatory elements \( m \), the DNA sequences and epigenomic signals are independent, thus enabling more efficient and targeted modeling of each component.


\subsection{Mask Distribution}
\label{sec:mask_dist}

With the conditional independence property shown in Proposition~\ref{propo_decom}, the estimation of the mask \( M \) can be decomposed into two components: one based on DNA sequences \( p_{\theta_1}(M|X_{seq}) \) and the other on epigenomic signals \( p_{\theta_2}(M|X_{sig}) \). 
We assume that both components follow the Beta distribution, as described in Assumption~\ref{assumption:mask_dist}. 
The sampled values from the Beta distribution represent the probability of selecting specific base pairs from a DNA sequence.



\begin{assumption}[Mask Distribution]
    We assume that the soft mask \( m_s \) follows the Beta distribution, i.e., \( m_s \sim \text{Beta}(\alpha, \beta) \).
    \label{assumption:mask_dist}
\end{assumption}
Unlike the binary hard mask $M$, the soft mask $m_s$ takes values between 0 and 1, making it more suitable for the Beta distribution. The hard mask $M$ can then be obtained by applying a threshold to the soft mask.
For the implementation, we apply both hard mask version and soft mask version.

There are several reasons for choosing the Beta distribution. 
First, the Beta distribution typically quantifies success rates~\citep{degroot2013probability,gelman2013bayesian}.
The input parameters \( \alpha \) and \( \beta \) represent the weights for selecting and not selecting the base pair, respectively. Therefore, when \( \alpha > \beta \), the base pair is more likely to be selected, and vice versa. 
% when \( \alpha < \beta \), it is less likely to be selected. 
Second, as both \( \alpha \) and \( \beta \) increase simultaneously, the selection process will exhibit lower variance, indicating more confidence in the selection. 
% Third, Beta distribution is conjugate, meaning that when it is used as the prior distribution, the posterior distribution will also be a Beta distribution. 
Third, the product of two Beta distributions, when properly normalized, results in another Beta distribution.
This ensures that the distributions within the framework remain in the same family, simplifying subsequent mathematical calculations and providing consistent fitting objectives for the models.

% Intuitively, we can interpret the value drawn from a Beta distribution as the probability of selecting a specific base pair from a DNA sequence. 

Based on these properties of the Beta distribution, we assume that both \( p_{\theta_1}(m_s|X_{seq}) \) and \( p_{\theta_2}(m_s|X_{sig}) \) follow Beta distributions, but with different parameters \( \alpha \) and \( \beta \). 
% At the same time, we take the mask prior distribution \( r(m) \) as Beta distribution as well.

\begin{proposition}
\label{beta_product}
    Given \( p_{\theta_1}(m_s|X_{seq}) \sim \text{Beta}(\alpha_1, \beta_1) \) and \( p_{\theta_2}(m_s|X_{sig}) \sim \text{Beta}(\alpha_2, \beta_2) \), the product of these distributions also follows a Beta distribution, with parameters:
    \begin{equation}
        p_{\theta_1}(m_s|X_{seq}) p_{\theta_2}(m_s|X_{sig}) \sim \text{Beta}(\alpha_1 + \alpha_2 - 1, \beta_1 + \beta_2 - 1)
    \end{equation}
\end{proposition}

The proof of Proposition~\ref{beta_product} is provided in Appendix~\ref{app_beta}.
When combining the probability distributions learned from the DNA sequence and signals, Proposition~\ref{beta_product} ensures that the resulting distribution remains within the same family.
And the final mask $m_s$ is then obtained through the combined Beta distribution.
Specifically, deep learning models are applied in our framework to learn these two distributions by predicting the parameters \( \alpha \) and \( \beta \).

% Based on these steps, we can model \( p_{\theta}(m|X) \) using a Beta distribution.

% , the mask \( m \) is a hard mask consisting of binary values, either 0 or 1. 
% To make sure the overall framework is differentiable, we apply a straight-through estimator. 
% That's to say,  the hard mask \( m \) is derived from a soft mask \( m_s \), where \( m_s \in [0,1] \). A common approach is to apply a threshold to the soft mask to generate the hard mask, i.e., \(m = \mathbb{I}(m_s \geq C_m)\), where $C_m$ is the threshold value (e.g., 0.5).

\subsection{Sparse Objective}
% The previous sections primarily focus on modeling \( p_{\theta}(m|X) \). Here, we describe the prior distribution \( r(m) \). We continue to work with the soft mask \( r(m_s) \) instead of the hard mask. Note that \( r(m_s) \) has no dependency on the input \( X \); hence, it can be interpreted as a predefined distribution that the mask should follow. 
% \textcolor{red}{suggestion: try to aviod use the sentence like "we will do". You can change that to "what can be done".}
In this part, we focus on the mask prior distribution  \( r(m) \).
From the objective in Equation~\ref{obj}, the KL divergence between \( p_{\theta}(m_i|x_i) \) and \( r(m_i) \) needs to be computed. 
% To simplify this calculation, we assume that both distributions follow a Beta distribution. 
To simplify this calculation, we assume the prior distribution of the soft mask \( r(m_s) \) follows the Beta distribution as well.
% As stated in Section~\ref{sec:mask_dist}, both distributions are assumed to follow a Beta distribution.
Therefore, we have \( r(m_s) \sim \text{Beta}(\alpha_3, \beta_3) \), where \( \alpha_3 \) and \( \beta_3 \) are related to the sparsity of mask.

% For better interpretability, instead of directly specifying the parameters \( \alpha_3 \) and \( \beta_3 \), we can define the mean $\mu$ of the Beta distribution and multiple the parameters by a scale factor $C$. The expectation of the Beta distribution is given by:

The expectation of the Beta distribution is
\begin{equation}
    \mathbb{E}[m_s] = \mu = \frac{\alpha_3}{\alpha_3 + \beta_3},
\end{equation}


where \( \mu \) approximately represents the proportion of regulatory elements within the DNA sequences. 
Therefore, by setting the hyperparameters  \( \alpha_3 \) and \( \beta_3 \), the sparsity of the mask is taken into consideration, acting as a bottleneck to filter out non-causal parts.
% To adjust the confidence in the predefined percentage of regulatory elements, we introduce a scale factor \( C \), which multiplies both \( \alpha_3 \) and \( \beta_3 \). A higher value of \( C \) indicates greater confidence in the predefined percentage of regulatory elements.





\section{Model Designs}
% \begin{wrapfigure}{l}{0.55\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{figs/Pipeline.jpg}
%     \caption{Pipeline of proposed architectures. The input data contains DNA sequence $X\textsubscript{seq}$ and signals $X\textsubscript{sig}$, which is then used to form the combined beta distribution to generate the mask for actively interacted regulatory elements. Then, the selected elements is sent to the predictor model $g_{\phi}$ to get the final target gene expression.}
%     \label{fig:pipeline}
% \end{wrapfigure}
\begin{figure}[t]
    \vspace{-5mm}
    \centering
    \includegraphics[width=0.65\linewidth]{figs/Pipeline.jpg}
    \caption{Pipeline of proposed architectures. The input data contains the DNA sequence $X\textsubscript{seq}$ and epigenomic signals $X\textsubscript{sig}$. 
    A deep learning model $f_{\theta}$ is then applied to $X\textsubscript{seq}$ to learn the corresponding parameters for the Beta distribution $\alpha_1, \beta_1$, while $\alpha_2, \beta_2$ are obtained from $X\textsubscript{sig}$ in a non-parameterized manner.
    By combining these two beta distributions,  $p(m_s | X)$ is obtained and used to generate the mask for actively interacted regulatory elements. The selected elements are then fed into the predictor model $g_{\phi}$ to provide the final target gene expression.}
    \label{fig:pipeline}
    \vspace{-4mm}
\end{figure}

\subsection{Architecture}
% (Any information about where to encode the signal)
As shown in Figure~\ref{fig:pipeline}, our proposed model generate the mask distribution  $p_\theta(M|X)$ from the DNA sequences and epigenomic signals $X=\{X_{seq},X_{sig}\}$, and an predictor, $q_\phi(Y|M\odot X)$, provides gene expression values from the masked sequences $Z=M\odot X$. Those two modules will be trained together.

\textbf{Generator.}
As outlined in Section~\ref{sec:mask_dist}, we aim to generate a mask \( M \) to identify the critical regions within the DNA sequences. To achieve this, we first learn a soft mask \( m_s \), which is a probabilistic representation of each base pair's relevance, where \( m_s \in [0,1] \). The soft mask is modeled using the Beta distribution, whose parameters—\( \alpha_1 \), \( \alpha_2 \), \( \beta_1 \), and \( \beta_2 \)—are estimated from the combination of DNA sequences and epigenomic signals, as detailed in Proposition~\ref{beta_product}.


% Seq2Exp is applied to predict the hyperparamaters of the Beta distribution. 
For the parameters derived from the DNA sequences, the neural network \( f_\theta \) is used to predict \( \alpha_1 \) and \( \beta_1 \). Specifically, we have
\begin{equation}
    \alpha_1, \beta_1 = f_\theta(X_{seq}),
    % p_{\theta_1}(m_s|X_{seq}) = \text{Beta}(f_{\alpha_1}(X_{seq}), f_{\beta_1}(X_{seq})),
\end{equation}
where the network \( f_\theta \) outputs the \( L \)-dimensional parameters \( \alpha_1 \) and \( \beta_1 \), with \( L \) being the length of the input DNA sequence. Each position in the sequence is associated with a pair of values \( \alpha_1 \) and \( \beta_1 \), which parameterize the Beta distribution for that base pair. 


For the parameters related to epigenomic signals, we use the intuition that higher signal values increase the likelihood of selecting the corresponding base pair. To capture this, we directly use the epigenomic signal values as the parameter \( \alpha_2 \), which influences the selection weight for each base pair. The parameter \( \beta_2 \), representing a selection threshold, is set as a fixed constant. Specifically, we define
\begin{equation}
\alpha_2 = X_{sig};\beta_2 = C_{\beta}.
% \begin{aligned}
%     \alpha_2 &= X_{sig} \\
%     \beta_2 &= C_{\beta}.
% \end{aligned}
\end{equation}
By the above modeling procedure, we simplify the modeling process, making the learning of $\alpha_2$ and $\beta_2$ non-parametric while maintaining the influence of signal strength without introducing additional learnable parameters.

After estimating the parameters, based on Proposition~\ref{beta_product}, the soft mask \( m_s \) is sampled from the combined Beta distribution, $p_{\theta_1}(m_s|X_{seq}) p(m_s|X_{sig})\sim \text{Beta}(\alpha_1+\alpha_2-1, \beta_1+\beta_2-1)$, which represents the probability of selecting each base pair in the sequence. This probabilistic formulation allows us to model the selection process effectively.
% % while the neural network \( f \) adapts the distribution based on the sequence features.
% Then, with the learned combined probability distribution,  
% the hard mask is sampled to extract the important regions.
% % \textcolor{red}{verify.}
% Specifically, a soft mask \( m_s \) is learned from the combination of DNA sequences and signals, where \( m_s \in [0,1] \). 


Finally, for the hard mask version, a threshold is applied to the soft mask to generate the hard mask, \( M = \mathbb{I}(m_s \geq C_m) \), where \( C_m \) is the threshold (e.g., 0.5). The hard mask \( M \) provides a binary decision for selecting or ignoring specific base pairs. Through this approach, we model the mask generation process by leveraging both DNA sequences and epigenomic  signals, combining parametric and non-parametric methods for more efficient region selection.
% To make sure the overall framework is differentiable, we apply a straight-through estimator. 



% Finally, after obtaining the soft mask \( m_s \), we apply a threshold to convert it into a hard mask \( m \), representing the binary decision of whether to select a specific base pair or not. Through this approach, we successfully model the mask generation process based on both DNA sequences and epigenomic signals, combining parametric and non-parametric methods for effective mask selection.







\textbf{Predictor.}
After obtaining the mask \( M \), we apply it to the input sequences to extract the relevant sub-sequences, represented as \( M \odot X \).
% This operation tends to select the  that the model has access to the selected parts of the input. 
% The extracted sub-sequences are then fed into a secondary neural network $g$ to estimate probability distribution of target $Y$ as \( q_\phi(Y|m \odot X) \) based solely on the selected sub-sequences.
The extracted sub-sequences are then fed into a secondary neural network, denoted by \( g_\phi \), to estimate the probability distribution of the target gene expression \( Y \). The conditional distribution is expressed as \( q_\phi(Y | M \odot X) \), where \( \phi \) represents the parameters of the network, and \( M \odot X \) refers to the masked input sequences.



To incorporate epigenomic  signals alongside the DNA sequences, the input \( X \) is formed by concatenating the one-hot encoded DNA sequence embeddings with the epigenomic  signal values. This combined input allows the model to leverage both DNA sequence information and epigenomic  signals, enhancing the model's predictive capability during the estimation process.

% \redtext{
% Both generator $p_\theta$ and predictor $q_\phi$ are based on Caduceus architecture~\citep{caduceus}, an advanced long-sequence model considering the bi-direction and RC-equivariance for DNA. 
% Specifically, we train for 50,000 steps on a 4-layer Caduceus architecture from scratch with a hidden dimension of 128, and more hyperparameters can be found in the Appendix~\ref{sec:experiment_setup}}



\subsection{Optimization}
% straight through
% dist rsample & mean
% dist differentiable
% \textcolor{red}{Please add citation to support this.}
To optimize the loss function introduced in Equation~\ref{obj}, it is essential that every step remains differentiable to allow for gradient-based optimization during training. After obtaining the parameters of the Beta distribution through the neural network \( p_\theta \), we generate the soft mask \( m_s \) by sampling from this distribution. To maintain differentiability, we treat the Beta distribution as a special case of the Dirichlet distribution~\citep{figurnov2018implicit, bishop2006pattern}. Using the reparameterization trick, we achieve differentiable sampling from the Dirichlet distribution by first sampling from the Gamma distribution and then normalizing the results~\citep{figurnov2018implicit}. This method ensures that the sampling process remains differentiable with respect to the parameters \( \alpha \) and \( \beta \), allowing for efficient backpropagation and optimization.

During inference, instead of sampling from the Beta distribution, we directly use the \textit{expected value} of the Beta distribution as the soft mask \( m_s \) for each base pair. The expected value of a Beta distribution with parameters \( \alpha \) and \( \beta \) is given by \(\mathbb{E}[m_s] = \frac{\alpha}{\alpha + \beta}\), which provides a deterministic and efficient way to generate the soft mask without introducing randomness during inference, thus stabilizing the model’s predictions.

For the soft mask version, we multiply the soft mask value. And for the hard mask version, when the soft mask \( m_s \) is obtained, we need to convert it into a hard binary mask \( M \) to make final selections for each base pair. To retain differentiability in this process, we apply the \textit{straight-through estimator (STE)} commonly used in Gumbel-Softmax~\citep{gumbel}. The STE allows us to make the forward pass non-differentiable by applying a hard threshold (e.g., setting values greater than 0.5 to 1 and others to 0), while during the backward pass, the gradient is propagated through the soft mask as if it were continuous. This approach ensures that the model can learn effectively while using discrete decisions during the forward pass, preserving differentiability in the overall optimization process.

% \subsection{Loss function}
% (put it here or in appendix)


\section{Experiments}
\subsection{Settings}

\subsubsection{Datasets and Input Features} 
In this study, we aim to predict gene expression by modeling CAGE values, as it serves as key indicators of gene expression levels. Since gene expression varies across different cell types, we focus on two well-studied cell types: K562 and GM12878, both commonly used in biological research. The CAGE data are sourced from the ENCODE project~\citep{encode2012integrated}, and we follow the methodology of \citet{epinformer} to predict gene expression values for 18,377 protein-coding genes. 

For the input data, we utilize the HG38 human reference genome to provide the reference DNA sequences. Additionally, the model incorporates several types of epigenomic signals: 
\begin{itemize}[left=0pt, noitemsep]
    \item \textbf{DNase-seq} data is used to capture chromatin accessibility by identifying regions of the genome that are open and accessible to transcription factors and other regulatory proteins. The signals are extracted from bigWig files, providing genome-wide distributions of chromatin accessibility.
    \item \textbf{H3K27ac} ChIP-seq data is used to detect histone modifications, specifically the acetylation of lysine 27 on histone H3, which is often associated with active enhancers and promoters. This data is also extracted from bigWig files to provide genome-wide information on histone modification patterns.
    \item \textbf{Hi-C} data is processed to calculate the contact frequencies between each base pair and the target transcription start site (TSS), following the ABC pipeline method as described by \citet{ABC}. 
\end{itemize}

Furthermore, we incorporate additional features such as mRNA half-life and promoter activity from previous studies~\citep{epinformer}, which improve the model’s prediction accuracy on gene expression levels. The detailed information about these signals can be found in Appendix~\ref{app_data}.

A detailed description of data acquisition, preprocessing, and extraction, including downloading, filtering, and alignment, is provided in Appendix~\ref{app_data}.




\subsubsection{Baselines} 
To benchmark our model’s performance, we compare it against several well-established baselines in gene expression prediction: 
\begin{itemize}[left=0pt, noitemsep]
    \item \textbf{Enformer}~\citep{enformer}: 
    A widely used deep learning model for gene expression prediction, designed to capture long-range interactions across DNA sequences. Enformer employs the CNN and transformer architecture to model the DNA sequence to get the gene expression.
    \item \textbf{HyenaDNA}~\citep{hyenaDNA}: 
    A cutting-edge method for modeling long DNA sequences, building on the Hyena~\citep{hyena_op} operator, which introduces a novel way to handle long-range dependencies efficiently. HyenaDNA is designed to maintain high accuracy while significantly reducing computational complexity compared to traditional transformer-based models.
    \item \textbf{Mamba}~\citep{gu2023mamba}: 
    A long-sequence modeling approach based on the state space model (SSM), offering linear computational complexity. Mamba is specifically tailored for efficiently handling long sequences, making it highly scalable while retaining strong predictive performance.
    \item \textbf{Caduceus}~\citep{caduceus}: 
    The state-of-the-art model for long genomic sequence modeling, built upon the Mamba architecture. Caduceus is optimized for learning rich representations of genomic sequences. In our study, we utilize Caduceus-Ph. A classification layer is appended to evaluate its performance on our specific task.
    \item \textbf{EPInformer}~\citep{epinformer}: 
    A recently developed model extends the Activity-By-Contact (ABC) model~\citep{ABC} for gene expression prediction. EPInformer utilizes DNase-seq peak data to define potential regulatory regions and applies an attention mechanism to aggregate enhancer signals. By leveraging both epigenomic and spatial information, EPInformer effectively models the enhancer information for gene expression prediction.

\end{itemize}
Note that the size of the field of view of Enformer, HyenaDNA, Mamba, Caduceus are long-range DNA sequence, while the EPInformer is the extracted potential enhancer candidates based on DNase-seq measurement following ABC model [5]. Moreover, our proposed Seq2Exp also has the view of long-range DNA sequence, since the generator will take the long-range DNA sequence to extract the relevant sub-sequences for the prediction.



\subsubsection{Evaluation Metrics} We employ the following evaluation metrics to assess the performance of our model and baselines on the gene expression regression task: Mean Squared Error (MSE) measures the average squared difference between the predicted and target gene expression values, capturing large deviations strongly. Mean Absolute Error (MAE) evaluates the absolute differences between predicted and actual values, providing a more direct measure of average prediction error. Pearson Correlation is used to assess the linear correlation between the predicted and actual gene expression values, reflecting how well the model captures the relative ordering of gene expression. While MSE and MAE focus on the absolute errors in predictions, Pearson Correlation assess the model’s ability to capture relative ranking and overall trends in the data.

% We employ the following evaluation metrics to assess the performance of our model and baselines on the gene expression regression task: Mean Squared Error (MSE) measures the average squared difference between the predicted and target gene expression values, capturing large deviations strongly. Mean Absolute Error (MAE) evaluates the absolute differences between predicted and actual values, providing a more direct measure of average prediction error. Pearson Correlation is used to assess the linear correlation between the predicted and actual gene expression values, reflecting how well the model captures the relative ordering of gene expression. Finally, the Coefficient of Determination ($R^2$) evaluates the proportion of variance in the gene expression data that is explained by the model's predictions, offering a measure of overall goodness-of-fit. While MSE and MAE focus on the absolute errors in predictions, Pearson Correlation and $R^2$ assess the model’s ability to capture the relative ranking and overall trends in the data.



\subsubsection{Implementation Details}
We evaluate model performance using a cross-chromosome validation strategy. The model is trained on all chromosomes except those designated for validation and testing. Specifically, chromosomes 3 and 21 are used as the validation set, and chromosomes 22 and X are reserved for the test set. The inclusion of chromosome X is particularly challenging due to its unique biological characteristics compared to autosomes, thus providing a more stringent test of the model's robustness.


Both generator $p_\theta$ and predictor $q_\phi$ are based on Caduceus architecture~\citep{caduceus}, an advanced long-sequence model considering the bi-direction and RC-equivariance for DNA. 
Specifically, we train for 50,000 steps on a 4-layer Caduceus architecture from scratch with a hidden dimension of 128, and more hyperparameters can be found in the Appendix~\ref{sec:experiment_setup}


All experiments were conducted on a system equipped with an NVIDIA A100 80GB PCIe GPU.

% Our model implementation is based on the Caduceus architecture~\citep{caduceus}, where both the generator $p_\theta$ and predictor $q_\phi$ are modeled using neural networks from the Caduceus framework. 

The input sequences consist of 200,000 base pairs, centered around the promoter regions of the target genes, providing sufficient contextual information for accurate gene expression prediction.


\subsection{Results of Gene Expression Prediction}



% \begin{table}
%     \caption{Performance on Gene Expression CAGE Prediction.}
%     \label{result:cage}
%     \centering
%     \scalebox{0.88}{
%     \begin{tabular}{c|ccc||cccc}
%         \toprule
%         & \multicolumn{3}{c||}{K562} & \multicolumn{3}{c}{GM12878} \\
%         & MSE $\downarrow$ & MAE $\downarrow$ &  Pearson $\uparrow$ & MSE $\downarrow$ & MAE $\downarrow$  & Pearson $\uparrow$ \\
%         \midrule 
%         Enformer & $0.2920 $\pm$ 0.0050$ & $0.4056 $\pm$ 0.0040$  & $0.7961 $\pm$ 0.0019$ & $0.2889$\pm$ 0.0009$ & $0.4185$\pm$ 0.0013$& $0.8327 $\pm$ 0.0025$ \\
%         HyenaDNA & 0.2230 & 0.3475  & 0.8428 & 0.2249 & 0.3582 &  0.8700 \\
%         Mamba & 0.2301 & 0.3494  & 0.8392 & 0.2191 & 0.3516 & 0.8751 \\
%         Caduceus & 0.2217 & 0.3385  & 0.8454 & 0.2143 & 0.3442 &  0.8775 \\
%         \midrule
%         Caduceus w/ signals & $\underline{0.1959} $\pm$ 0.0036$ & $\underline{0.3187} $\pm$ 0.0036$ & $\underline{0.8630}$\pm$ 0.0008$ & $\underline{0.1942}$\pm$ 0.0058$ & ${0.3269}$\pm$ 0.0048$  & $\underline{0.8928}$\pm$ 0.0017$ \\
%         \midrule
%         EPInformer & ${0.2140}$\pm$ 0.0042$ & ${0.3291}$\pm$ 0.0031$ & ${0.8473}$\pm$ 0.0017$ & ${0.1975}$\pm$ 0.0031$ & $\underline{0.3246}$\pm$ 0.0025$ & ${0.8907} $\pm$ 0.0011$ \\
%         \midrule
%         Seq2Exp-hard & {0.1951} & {0.3150} & {0.8623} & {0.1900} & {0.3221} & {0.8942} \\
%         Seq2Exp-soft & $\textbf{0.1856}$\pm$ 0.0032$ & $\textbf{0.3054}$\pm$ 0.0024$ & $\textbf{0.8723}$\pm$ 0.0012$ & $\textbf{0.1873}$\pm$ 0.0044$ & $\textbf{0.3137}$\pm$ 0.0028$ & $\textbf{0.8951}$\pm$ 0.0038$ \\
%         \bottomrule
%     \end{tabular}
%     }
% \end{table}


\begin{table}
    \vspace{-3mm}
    \caption{Performance on Gene Expression CAGE Prediction. The top performance over all the methods are highlighted in \textbf{bold}. \underline{Underline} indicates that the best performance over all the baselines.}
    \label{result:cage}
    \centering
    \scalebox{0.88}{
    \begin{tabular}{c|ccc||ccc}
        \toprule
        & \multicolumn{3}{c||}{K562} & \multicolumn{3}{c}{GM12878} \\
        & MSE $\downarrow$ & MAE $\downarrow$ & Pearson $\uparrow$ & MSE $\downarrow$ & MAE $\downarrow$ & Pearson $\uparrow$ \\
        \midrule 
        Enformer & 0.2920 & 0.4056 & 0.7961 & 0.2889 & 0.4185 & 0.8327 \\
        HyenaDNA & 0.2265 & 0.3497 & 0.8425 & 0.2217 & 0.3562 & 0.8729 \\
        Mamba & 0.2241 & 0.3416 & 0.8412 & 0.2145 & 0.3446 & 0.8788 \\
        Caduceus & 0.2197 & 0.3327 & 0.8475 & 0.2124 & 0.3436 & 0.8819 \\
        \midrule
        Caduceus w/ signals & \underline{0.1959} & \underline{0.3187} & \underline{0.8630} & \underline{0.1942} & {0.3269} & \underline{0.8928} \\
        \midrule
        EPInformer & ${0.2140}$ & ${0.3291}$ & ${0.8473}$ & ${0.1975}$ & $\underline{0.3246}$ & ${0.8907}$ \\
        \midrule
        Seq2Exp-hard & {0.1863} & {0.3074} & {0.8682} & {0.1890} & {0.3199} & {0.8916} \\
        Seq2Exp-soft & $\textbf{0.1856}$ & $\textbf{0.3054}$ & $\textbf{0.8723}$ & $\textbf{0.1873}$ & $\textbf{0.3137}$ & $\textbf{0.8951}$ \\
        \bottomrule
    \end{tabular}
    }
    \vspace{-3mm}
\end{table}


% \begin{table}
%     \caption{Performance on Gene Expression CAGE Prediction.}
%     \label{result:cage}
%     \centering
%     \scalebox{0.88}{
%     \begin{tabular}{c|cccc||cccc}
%         \toprule
%         & \multicolumn{4}{c||}{K562} & \multicolumn{4}{c}{GM12878} \\
%         & MSE $\downarrow$ & MAE $\downarrow$ & $R^2$ $\uparrow$ & Pearson $\uparrow$ & MSE $\downarrow$ & MAE $\downarrow$ & $R^2$ $\uparrow$ & Pearson $\uparrow$ \\
%         \midrule 
%         Enformer & 0.3629 & 0.4714 & 0.6304 & 0.7940 & 0.3668 & 0.4721& 0.6386 & 0.7991 \\
%         HyenaDNA & 0.2230 & 0.3475 & 0.7103 & 0.8428 & 0.2249 & 0.3582 & 0.7570 & 0.8700 \\
%         Mamba & 0.2301 & 0.3494 & 0.7043 & 0.8392 & 0.2191 & 0.3516 & 0.7659 & 0.8751 \\
%         Caduceus & 0.2217 & 0.3385 & 0.7146 & 0.8454 & 0.2143 & 0.3442 & 0.7701 & 0.8775 \\
%         \midrule
%         Caduceus w/ signals & 0.2411 & 0.3747 & 0.6884 & 0.8297 & 0.2080 & \underline{0.3441} & 0.7773 & 0.8816 \\
%         \midrule
%         EPInformer & \underline{0.2177} & \underline{0.3323} & \underline{0.7253} & \underline{0.8517} & \underline{0.1941} & \textbf{0.3221} & \underline{0.7959} & \underline{0.8921} \\
%         \midrule
%         Seq2Exp & \textbf{0.1951} & \textbf{0.3150} & \textbf{0.7435} & \textbf{0.8623} & \textbf{0.1900} & \textbf{0.3221} & \textbf{0.7997} & \textbf{0.8942} \\
%         \bottomrule
%     \end{tabular}
%     }
% \end{table}

% \begin{table}
%     \caption{Performance on Gene Expression RNA-seq Prediction.}
%     \label{result_rna}
%     \centering
%     \scalebox{0.95}{
%     \begin{tabular}{c|cccc||cccc}
%         \toprule
%         & \multicolumn{4}{c||}{K562} & \multicolumn{4}{c}{GM12878} \\
%         & MSE $\downarrow$ & MAE $\downarrow$ & $R^2$ $\uparrow$ & Pearson $\uparrow$ & MSE $\downarrow$ & MAE $\downarrow$ & $R^2$ $\uparrow$ & Pearson $\uparrow$ \\
%         \midrule 
%         Enformer & 0.4311 & 0.5148 & 0.5923 & 0.7696 & 0.3869 & 0.4701 & 0.6123 & 0.7825 \\
%         EPInformer & 0.432 & 0.4203 & 0.643 & 0.8019 & 0.2874 & 0.3808 & 0.7339 & 0.8567 \\
%         Caduceus &  &  &  &  &  &  &  &  \\
%         \midrule
%         Ours &  &  &  &  &  &  &  &  \\
%         \bottomrule
%     \end{tabular}
%     }
% \end{table}

Table~\ref{result:cage} presents the gene expression results based on CAGE values. Enformer, HyenaDNA, Mamba, and Caduceus are all DNA sequence-based methods, relying solely on DNA sequences without incorporating epigenomic  signals. Among these, Caduceus achieves the best performance. We further evaluate Caduceus by incorporating epigenomic signals, concatenated with the one-hot DNA sequence embeddings as input features. 
% For the GM12878 cell type, this addition improves performance; however, for the K562 cell type, the inclusion of epigenomic signals leads to a decline in performance. This suggests that directly adding epigenomic signals to the input data does not consistently enhance prediction accuracy and may introduce unnecessary or noisy information.
EPInformer, which explicitly extracts enhancer regions based on DNase-seq measurements, outperforms other baselines. This highlights 
% that the relationship between DNA sequences and epigenomic signals is not strictly linear, and 
that selecting key regions based on epigenomic signals yields better results.

Finally, our proposed model, Seq2Exp, achieves the best performance overall. By using the Caduceus sequence model as both the generator and predictor, and incorporating epigenomic signals as additional features to the predictor, Seq2Exp explicitly learns the positions of regulatory elements from both DNA sequences and epigenomic signals, resulting in superior performance. We propose two versions of Seq2Exp. Seq2Exp-hard is to have a binary mask, and Seq2Exp-soft takes use of soft mask values to denote the importance, resulting in an even better performances regarding the CAGE prediction task.



% \subsection{Comparison with peak detection method}
% \begin{table}
%     \caption{Comparison with MACS3 on Gene Expression CAGE Prediction.}
%     \label{result:macs}
%     \centering
%     \scalebox{0.74}{
%     \begin{tabular}{c|cccc||cccc}
%         \toprule
%         & \multicolumn{4}{c||}{K562} & \multicolumn{4}{c}{GM12878} \\
%         & MSE $\downarrow$ & MAE $\downarrow$ &  Pearson $\uparrow$ & Mask Ratio & MSE $\downarrow$ & MAE $\downarrow$ &  Pearson $\uparrow$ & Mask Ratio \\
%         \midrule 

%         Seq2Exp-hard & {0.1951} & {0.3150}  & {0.8623} & 6.86\% & {0.1900} & {0.3221} & {0.8942} & 6.25\% \\
%         Seq2Exp-retrain & $0.2001$\pm$ 0.0058$ & $0.3181$\pm$ 0.0056$ & $0.8612$\pm$ 0.0026$ & 10.00\% & $0.1880$\pm$ 0.0026$ & $0.3172$\pm$ 0.0018$ & $0.8960$\pm$ 0.0009$ & 10.00\% \\
%         MACS3 & $0.2195$\pm$ 0.0023$ & $0.3455$\pm$ 0.0018$ & $0.8435$\pm$ 0.0013$ & 13.61\% & $0.2340$\pm$ 0.0028$ & $0.3654$\pm$ 0.0017$ & $0.8634$\pm$ 0.0020$ & 15.95\% \\
        


%         \bottomrule
%     \end{tabular}
%     }
% \end{table}






\subsection{Comparison with peak detection method}
\begin{table}
    \caption{Comparison with MACS3 on Gene Expression CAGE Prediction.}
    \label{result:macs}
    \centering
    \scalebox{0.74}{
    \begin{tabular}{c|cccc||cccc}
        \toprule
        & \multicolumn{4}{c||}{K562} & \multicolumn{4}{c}{GM12878} \\
        & MSE $\downarrow$ & MAE $\downarrow$ &  Pearson $\uparrow$ & Mask Ratio & MSE $\downarrow$ & MAE $\downarrow$ &  Pearson $\uparrow$ & Mask Ratio \\
        \midrule 

        Seq2Exp-hard & {0.1863} & {0.3074} & {0.8682} & 6.88\% & {0.1890} & {0.3199} & {0.8916} & 6.32\% \\
        Seq2Exp-retrain & $0.1979$ & $0.3168$ & $0.8623$ & 10.00\% & $0.1887$ & $0.3177$ & $0.8941$ & 10.00\% \\
        MACS3 & $0.2195$ & $0.3455$ & $0.8435$ & 13.61\% & $0.2340$ & $0.3654$ & $0.8634$ & 15.95\% \\
        \bottomrule
    \end{tabular}
    }
    \vspace{-5mm}
\end{table}

Table~\ref{result:macs} compares the performance of Seq2Exp with regions identified through peak calling by MACS3~\citep{macs} on DNase-seq epigenomic signals. 
While DNase-seq is a crucial technique for identifying the positions of regulatory elements, statistical peak-calling methods, such as MACS3, can be considered a simple approach for measuring these elements. 
% This methodology has been utilized by models like the ABC model~\citep{ABC} and EPInformer~\citep{epinformer}.
Our results show that Seq2Exp significantly outperforms MACS3 in terms of predictive performance. 
Seq2Exp-hard utilizes a hard binary mask. Seq2Exp-retrain builds on a soft mask, and explicitly select the top 10\% of base pairs and retrain the predictor model using only the selected ones. Both models outperform MACS3, suggesting the ability of discovering regulatory elements.

% One of the key reasons for this improvement is the quality of selected regulatory regions. While MACS3 selects a broader set of regions based on general peak-calling methods, Seq2Exp extracts more accurate regions, retaining only 6.86\% and 6.25\% of the full DNA sequences for K562 and GM12878 cell type, respectively. The differences highlight Seq2Exp’s ability to focus on the most critical regulatory regions, resulting in an improved prediction accuracy.


% \subsection{Perturbation Experiment}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figs/exp_perturb.png}
%     \caption{Perturbation effects on gene expression changes for Seq2Exp and CRISPRi-FlowFISH experiments targeting the KLF1 gene. The figure illustrates the fraction change in gene expression on CRISPRi-FlowFISH defined regions.}

%     \label{fig:perturb_bio}
% \end{figure}




\section{Conclusion}

% In this work, we investigate the task of gene expression prediction by learning critical regulatory elements from both DNA sequences and epigenomic signals. We propose the Seq2Exp framework, which integrates information from sequences and signals to generate a binary mask that identifies critical regions of the sequences. Seq2Exp then makes predictions based solely on the extracted sub-sequences. Our experiments demonstrate the effectiveness of the proposed approach in identifying important regulatory elements, suggesting that it can potentially reduce the complexity of the input data and achieve better predictive performance.

% \textbf{Limitations.} Due to data limitations, we have currently evaluated Seq2Exp on only two cell types. Future work should expand testing across a broader range of cell types to fully validate the generalizability of the framework. Additionally, while we have used three specific types of epigenomic signals, our method is not limited to these. More biological experiments including diverse epigenomic data could enhance the applicability and robustness of the approach.

% \textbf{Broader Impacts.} Gene expression prediction is a long-standing challenge in both computer science and biology. While much of the current research focuses on encoding long sequences to capture more information, our approach takes a different path by concentrating on reducing the input space to the most critical subsequences. Given the complexity and noisiness inherent in DNA sequences, we hope that our work can inspire a shift in perspective toward sequence compression rather than longer encoding. This approach could simplify model architectures while enhancing their clarity and understanding.

% \textbf{Future Directions.} DNA sequences play a crucial role in many downstream prediction tasks beyond gene expression. Expanding our approach to other tasks related to regulatory element discovery and sequence analysis would be a promising research direction. Furthermore, the development of pretraining models specifically designed for regulatory element extraction could provide significant advances in this field, offering more robust generalization across multiple genomic tasks.


In this work, we introduced Seq2Exp, a framework for gene expression prediction that learns critical regulatory elements from both DNA sequences and epigenomic signals. By generating a binary mask to identify relevant sub-sequences, Seq2Exp reduces input complexity and focuses on key regions for prediction. Our experiments demonstrate its effectiveness in identifying important regulatory elements and improving predictive performances, though current evaluations are limited to two cell types and several epigenomic signals.

For the future direction, expanding the framework to more cell types and integrating diverse epigenomic data will be important for validating its generalizability. Beyond gene expression, applying this approach to other tasks related to regulatory element discovery and sequence analysis presents exciting research opportunities. Developing pretraining models focused on regulatory element extraction could also advance the field, enhancing generalization across genomic tasks.






\newpage

\section*{Acknowledgments}
Research reported in this publication was supported in part by the National Institute on Aging of the National Institutes of Health under Award Number U01AG070112 and ARPA-H under Award Number 1AY1AX000053. The content is solely the responsibility of the authors and does not necessarily represent the official views of the funding agencies.

% \section{Submission of conference papers to ICLR 2025}

% ICLR requires electronic submissions, processed by
% \url{https://openreview.net/}. See ICLR's website for more instructions.

% If your paper is ultimately accepted, the statement {\tt
%   {\textbackslash}iclrfinalcopy} should be inserted to adjust the
% format to the camera ready requirements.

% The format for the submissions is a variant of the NeurIPS format.
% Please read carefully the instructions below, and follow them
% faithfully.

% \subsection{Style}

% Papers to be submitted to ICLR 2025 must be prepared according to the
% instructions presented here.

% %% Please note that we have introduced automatic line number generation
% %% into the style file for \LaTeXe. This is to help reviewers
% %% refer to specific lines of the paper when they make their comments. Please do
% %% NOT refer to these line numbers in your paper as they will be removed from the
% %% style file for the final version of accepted papers.

% Authors are required to use the ICLR \LaTeX{} style files obtainable at the
% ICLR website. Please make sure you use the current files and
% not previous versions. Tweaking the style files may be grounds for rejection.

% \subsection{Retrieval of style files}

% The style files for ICLR and other conference information are available online at:
% \begin{center}
%    \url{http://www.iclr.cc/}
% \end{center}
% The file \verb+iclr2025_conference.pdf+ contains these
% instructions and illustrates the
% various formatting requirements your ICLR paper must satisfy.
% Submissions must be made using \LaTeX{} and the style files
% \verb+iclr2025_conference.sty+ and \verb+iclr2025_conference.bst+ (to be used with \LaTeX{}2e). The file
% \verb+iclr2025_conference.tex+ may be used as a ``shell'' for writing your paper. All you
% have to do is replace the author, title, abstract, and text of the paper with
% your own.

% The formatting instructions contained in these style files are summarized in
% sections \ref{gen_inst}, \ref{headings}, and \ref{others} below.

% \section{General formatting instructions}
% \label{gen_inst}

% The text must be confined within a rectangle 5.5~inches (33~picas) wide and
% 9~inches (54~picas) long. The left margin is 1.5~inch (9~picas).
% Use 10~point type with a vertical spacing of 11~points. Times New Roman is the
% preferred typeface throughout. Paragraphs are separated by 1/2~line space,
% with no indentation.

% Paper title is 17~point, in small caps and left-aligned.
% All pages should start at 1~inch (6~picas) from the top of the page.

% Authors' names are
% set in boldface, and each name is placed above its corresponding
% address. The lead author's name is to be listed first, and
% the co-authors' names are set to follow. Authors sharing the
% same address can be on the same line.

% Please pay special attention to the instructions in section \ref{others}
% regarding figures, tables, acknowledgments, and references.


% There will be a strict upper limit of 10 pages for the main text of the initial submission, with unlimited additional pages for citations. 

% \section{Headings: first level}
% \label{headings}

% First level headings are in small caps,
% flush left and in point size 12. One line space before the first level
% heading and 1/2~line space after the first level heading.

% \subsection{Headings: second level}

% Second level headings are in small caps,
% flush left and in point size 10. One line space before the second level
% heading and 1/2~line space after the second level heading.

% \subsubsection{Headings: third level}

% Third level headings are in small caps,
% flush left and in point size 10. One line space before the third level
% heading and 1/2~line space after the third level heading.

% \section{Citations, figures, tables, references}
% \label{others}

% These instructions apply to everyone, regardless of the formatter being used.

% \subsection{Citations within the text}

% Citations within the text should be based on the \texttt{natbib} package
% and include the authors' last names and year (with the ``et~al.'' construct
% for more than two authors). When the authors or the publication are
% included in the sentence, the citation should not be in parenthesis using \verb|\citet{}| (as
% in ``See \citet{Hinton06} for more information.''). Otherwise, the citation
% should be in parenthesis using \verb|\citep{}| (as in ``Deep learning shows promise to make progress
% towards AI~\citep{Bengio+chapter2007}.'').

% The corresponding references are to be listed in alphabetical order of
% authors, in the \textsc{References} section. As to the format of the
% references themselves, any style is acceptable as long as it is used
% consistently.

% \subsection{Footnotes}

% Indicate footnotes with a number\footnote{Sample of the first footnote} in the
% text. Place the footnotes at the bottom of the page on which they appear.
% Precede the footnote with a horizontal rule of 2~inches
% (12~picas).\footnote{Sample of the second footnote}

% \subsection{Figures}

% All artwork must be neat, clean, and legible. Lines should be dark
% enough for purposes of reproduction; art work should not be
% hand-drawn. The figure number and caption always appear after the
% figure. Place one line space before the figure caption, and one line
% space after the figure. The figure caption is lower case (except for
% first word and proper nouns); figures are numbered consecutively.

% Make sure the figure caption does not get separated from the figure.
% Leave sufficient space to avoid splitting the figure and figure caption.

% You may use color figures.
% However, it is best for the
% figure captions and the paper body to make sense if the paper is printed
% either in black/white or in color.
% \begin{figure}[h]
% \begin{center}
% %\framebox[4.0in]{$\;$}
% \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
% \end{center}
% \caption{Sample figure caption.}
% \end{figure}

% \subsection{Tables}

% All tables must be centered, neat, clean and legible. Do not use hand-drawn
% tables. The table number and title always appear before the table. See
% Table~\ref{sample-table}.

% Place one line space before the table title, one line space after the table
% title, and one line space after the table. The table title must be lower case
% (except for first word and proper nouns); tables are numbered consecutively.

% \begin{table}[t]
% \caption{Sample table title}
% \label{sample-table}
% \begin{center}
% \begin{tabular}{ll}
% \multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
% \\ \hline \\
% Dendrite         &Input terminal \\
% Axon             &Output terminal \\
% Soma             &Cell body (contains cell nucleus) \\
% \end{tabular}
% \end{center}
% \end{table}

% \section{Default Notation}

% In an attempt to encourage standardized notation, we have included the
% notation file from the textbook, \textit{Deep Learning}
% \cite{goodfellow2016deep} available at
% \url{https://github.com/goodfeli/dlbook_notation/}.  Use of this style
% is not required and can be disabled by commenting out
% \texttt{math\_commands.tex}.


% \centerline{\bf Numbers and Arrays}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1in}p{3.25in}}
% $\displaystyle a$ & A scalar (integer or real)\\
% $\displaystyle \va$ & A vector\\
% $\displaystyle \mA$ & A matrix\\
% $\displaystyle \tA$ & A tensor\\
% $\displaystyle \mI_n$ & Identity matrix with $n$ rows and $n$ columns\\
% $\displaystyle \mI$ & Identity matrix with dimensionality implied by context\\
% $\displaystyle \ve^{(i)}$ & Standard basis vector $[0,\dots,0,1,0,\dots,0]$ with a 1 at position $i$\\
% $\displaystyle \text{diag}(\va)$ & A square, diagonal matrix with diagonal entries given by $\va$\\
% $\displaystyle \ra$ & A scalar random variable\\
% $\displaystyle \rva$ & A vector-valued random variable\\
% $\displaystyle \rmA$ & A matrix-valued random variable\\
% \end{tabular}
% \egroup
% \vspace{0.25cm}

% \centerline{\bf Sets and Graphs}
% \bgroup
% \def\arraystretch{1.5}

% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle \sA$ & A set\\
% $\displaystyle \R$ & The set of real numbers \\
% $\displaystyle \{0, 1\}$ & The set containing 0 and 1 \\
% $\displaystyle \{0, 1, \dots, n \}$ & The set of all integers between $0$ and $n$\\
% $\displaystyle [a, b]$ & The real interval including $a$ and $b$\\
% $\displaystyle (a, b]$ & The real interval excluding $a$ but including $b$\\
% $\displaystyle \sA \backslash \sB$ & Set subtraction, i.e., the set containing the elements of $\sA$ that are not in $\sB$\\
% $\displaystyle \gG$ & A graph\\
% $\displaystyle \parents_\gG(\ervx_i)$ & The parents of $\ervx_i$ in $\gG$
% \end{tabular}
% \vspace{0.25cm}


% \centerline{\bf Indexing}
% \bgroup
% \def\arraystretch{1.5}

% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle \eva_i$ & Element $i$ of vector $\va$, with indexing starting at 1 \\
% $\displaystyle \eva_{-i}$ & All elements of vector $\va$ except for element $i$ \\
% $\displaystyle \emA_{i,j}$ & Element $i, j$ of matrix $\mA$ \\
% $\displaystyle \mA_{i, :}$ & Row $i$ of matrix $\mA$ \\
% $\displaystyle \mA_{:, i}$ & Column $i$ of matrix $\mA$ \\
% $\displaystyle \etA_{i, j, k}$ & Element $(i, j, k)$ of a 3-D tensor $\tA$\\
% $\displaystyle \tA_{:, :, i}$ & 2-D slice of a 3-D tensor\\
% $\displaystyle \erva_i$ & Element $i$ of the random vector $\rva$ \\
% \end{tabular}
% \egroup
% \vspace{0.25cm}


% \centerline{\bf Calculus}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1.25in}p{3.25in}}
% % NOTE: the [2ex] on the next line adds extra height to that row of the table.
% % Without that command, the fraction on the first line is too tall and collides
% % with the fraction on the second line.
% $\displaystyle\frac{d y} {d x}$ & Derivative of $y$ with respect to $x$\\ [2ex]
% $\displaystyle \frac{\partial y} {\partial x} $ & Partial derivative of $y$ with respect to $x$ \\
% $\displaystyle \nabla_\vx y $ & Gradient of $y$ with respect to $\vx$ \\
% $\displaystyle \nabla_\mX y $ & Matrix derivatives of $y$ with respect to $\mX$ \\
% $\displaystyle \nabla_\tX y $ & Tensor containing derivatives of $y$ with respect to $\tX$ \\
% $\displaystyle \frac{\partial f}{\partial \vx} $ & Jacobian matrix $\mJ \in \R^{m\times n}$ of $f: \R^n \rightarrow \R^m$\\
% $\displaystyle \nabla_\vx^2 f(\vx)\text{ or }\mH( f)(\vx)$ & The Hessian matrix of $f$ at input point $\vx$\\
% $\displaystyle \int f(\vx) d\vx $ & Definite integral over the entire domain of $\vx$ \\
% $\displaystyle \int_\sS f(\vx) d\vx$ & Definite integral with respect to $\vx$ over the set $\sS$ \\
% \end{tabular}
% \egroup
% \vspace{0.25cm}

% \centerline{\bf Probability and Information Theory}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle P(\ra)$ & A probability distribution over a discrete variable\\
% $\displaystyle p(\ra)$ & A probability distribution over a continuous variable, or over
% a variable whose type has not been specified\\
% $\displaystyle \ra \sim P$ & Random variable $\ra$ has distribution $P$\\% so thing on left of \sim should always be a random variable, with name beginning with \r
% $\displaystyle  \E_{\rx\sim P} [ f(x) ]\text{ or } \E f(x)$ & Expectation of $f(x)$ with respect to $P(\rx)$ \\
% $\displaystyle \Var(f(x)) $ &  Variance of $f(x)$ under $P(\rx)$ \\
% $\displaystyle \Cov(f(x),g(x)) $ & Covariance of $f(x)$ and $g(x)$ under $P(\rx)$\\
% $\displaystyle H(\rx) $ & Shannon entropy of the random variable $\rx$\\
% $\displaystyle \KL ( P \Vert Q ) $ & Kullback-Leibler divergence of P and Q \\
% $\displaystyle \mathcal{N} ( \vx ; \vmu , \mSigma)$ & Gaussian distribution %
% over $\vx$ with mean $\vmu$ and covariance $\mSigma$ \\
% \end{tabular}
% \egroup
% \vspace{0.25cm}

% \centerline{\bf Functions}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle f: \sA \rightarrow \sB$ & The function $f$ with domain $\sA$ and range $\sB$\\
% $\displaystyle f \circ g $ & Composition of the functions $f$ and $g$ \\
%   $\displaystyle f(\vx ; \vtheta) $ & A function of $\vx$ parametrized by $\vtheta$.
%   (Sometimes we write $f(\vx)$ and omit the argument $\vtheta$ to lighten notation) \\
% $\displaystyle \log x$ & Natural logarithm of $x$ \\
% $\displaystyle \sigma(x)$ & Logistic sigmoid, $\displaystyle \frac{1} {1 + \exp(-x)}$ \\
% $\displaystyle \zeta(x)$ & Softplus, $\log(1 + \exp(x))$ \\
% $\displaystyle || \vx ||_p $ & $\normlp$ norm of $\vx$ \\
% $\displaystyle || \vx || $ & $\normltwo$ norm of $\vx$ \\
% $\displaystyle x^+$ & Positive part of $x$, i.e., $\max(0,x)$\\
% $\displaystyle \1_\mathrm{condition}$ & is 1 if the condition is true, 0 otherwise\\
% \end{tabular}
% \egroup
% \vspace{0.25cm}



% \section{Final instructions}
% Do not change any aspects of the formatting parameters in the style files.
% In particular, do not modify the width or length of the rectangle the text
% should fit into, and do not change font sizes (except perhaps in the
% \textsc{References} section; see below). Please note that pages should be
% numbered.

% \section{Preparing PostScript or PDF files}

% Please prepare PostScript or PDF files with paper size ``US Letter'', and
% not, for example, ``A4''. The -t
% letter option on dvips will produce US Letter files.

% Consider directly generating PDF files using \verb+pdflatex+
% (especially if you are a MiKTeX user).
% PDF figures must be substituted for EPS figures, however.

% Otherwise, please generate your PostScript and PDF files with the following commands:
% \begin{verbatim}
% dvips mypaper.dvi -t letter -Ppdf -G0 -o mypaper.ps
% ps2pdf mypaper.ps mypaper.pdf
% \end{verbatim}

% \subsection{Margins in LaTeX}

% Most of the margin problems come from figures positioned by hand using
% \verb+\special+ or other commands. We suggest using the command
% \verb+\includegraphics+
% from the graphicx package. Always specify the figure width as a multiple of
% the line width as in the example below using .eps graphics
% \begin{verbatim}
%    \usepackage[dvips]{graphicx} ...
%    \includegraphics[width=0.8\linewidth]{myfile.eps}
% \end{verbatim}
% or % Apr 2009 addition
% \begin{verbatim}
%    \usepackage[pdftex]{graphicx} ...
%    \includegraphics[width=0.8\linewidth]{myfile.pdf}
% \end{verbatim}
% for .pdf graphics.
% See section~4.4 in the graphics bundle documentation (\url{http://www.ctan.org/tex-archive/macros/latex/required/graphics/grfguide.ps})

% A number of width problems arise when LaTeX cannot properly hyphenate a
% line. Please give LaTeX hyphenation hints using the \verb+\-+ command.

%\subsubsection*{Author Contributions}
%If you'd like to, you may include  a section for author contributions as is done
%in many journals. This is optional and at the discretion of the authors.

%\subsubsection*{Acknowledgments}
%Use unnumbered third level headings for the acknowledgments. All
%acknowledgments, including those to funding agencies, go at the end of the paper.


\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage

\appendix
\section{Appendix}
\subsection{Derivation of Sequence and Signal Decomposition}
\label{decom}

For the mask distribution \( p_\theta(m|X) \), we aim to decompose it. For simplicity, we omit the parameter \( \theta \) in the following derivation. By applying Bayes' theorem, we obtain
\begin{equation}
\begin{aligned}
    p(m|X) &= p(m|X_{seq}, X_{sig}) \\
     & = \frac{p(X_{seq}, X_{sig}|m) p(m)}{p(X_{seq}, X_{sig})} \\
     & \propto p(X_{seq}|m) p(X_{sig}|m) p(m),
\end{aligned}
\end{equation}
where \( p(X_{seq}, X_{sig}|m) = p(X_{seq}|m) p(X_{sig}|m) \) is based on Assumption~\ref{inde_assume}, and \( p(X_{seq}, X_{sig}) \) represents the input data, which is independent of the learning process. 

Applying Bayes' theorem again to \( p(X_{seq}|m) \) and \( p(X_{sig}|m) \), we have
\begin{equation}
    \begin{aligned}
        p(m|X) & \propto p(X_{seq}|m) p(X_{sig}|m) p(m) \\
         & = \frac{p(m|X_{seq}) p(X_{seq})}{p(m)} \frac{p(m|X_{sig}) p(X_{sig})}{p(m)} p(m) \\
         & \propto \frac{p(m|X_{seq}) p(m|X_{sig})}{p(m)},
    \end{aligned}
\end{equation}
where we can safely omit \( p(X_{seq}) \) and \( p(X_{sig}) \). For the marginal distribution \( p(m) \), we make it to be a prior distribution with constant predefined parameters, allowing us to omit it as well. Thus, we derive
\begin{equation}
    p(m|X) \propto p(m|X_{seq}) p(m|X_{sig}),
\end{equation}
which corresponds to Proposition~\ref{propo_decom}.

\subsection{Beta Distribution Product}
\label{app_beta}

The probability density function for a Beta distribution is given by
\begin{equation}
    \text{Beta}(x;\alpha, \beta) \propto x^{\alpha-1} (1-x)^{\beta - 1}.
\end{equation}
Given that both \( p(m_s|X_{seq}) \) and \( p(m_s|X_{sig}) \) follow a Beta distribution, we have
\begin{equation}
    \begin{aligned}
        p(m_s|X_{seq}) & \propto x^{\alpha_1-1} (1-x)^{\beta_1-1}, \\ 
        p(m_s|X_{sig}) & \propto x^{\alpha_2-1} (1-x)^{\beta_2-1}.
    \end{aligned}
\end{equation}
Multiplying these distributions yields
\begin{equation}
    \begin{aligned}
        p(m_s|X_{seq}) p(m_s|X_{sig}) & \propto x^{\alpha_1 + \alpha_2 - 2} (1-x)^{\beta_1 + \beta_2 - 2} \\ 
        & \sim \text{Beta}(m_s; \alpha_1 + \alpha_2 - 1, \beta_1 + \beta_2 - 1).
    \end{aligned}
\end{equation}
Note that the parameters of a Beta distribution must lie within the range \( (0, \infty) \), thus we require \( \alpha_1 + \alpha_2 > 1 \) and \( \beta_1 + \beta_2 > 1 \) to ensure a valid distribution.



\subsection{Data Processing}
\label{app_data}

The gene expression is different for different cell types. In this work, we consider the well-studied cell type K562 and GM12878. 


\textbf{CAGE.}
Cap Analysis of Gene Expression (CAGE) is the primary target for prediction in this work. Each gene is assigned a CAGE value to quantify its expression level. CAGE is a high-throughput sequencing technique primarily used to map transcription start sites (TSS) and quantify gene expression. It provides a comprehensive profile of promoter usage and alternative TSS across different genes, quantifying the number of RNA molecules initiating at each TSS, thereby reflecting gene transcriptional activity.

In this study, we use CAGE data from the FANTOM5 project~\citep{fantom5} (K562: CNhs11250; GM12878: CNhs12333). We follow the procedures outlined in \citet{Xpresso} and \citet{epinformer} to derive the target values for each gene.




\textbf{DNase-seq.}
DNase-seq (DNase I hypersensitive site sequencing) is a technique used to identify regions of open chromatin within the genome. It pinpoints areas that are less compacted by nucleosomes, typically corresponding to promoters, enhancers, and transcription factor binding sites. The value derived from DNase-seq represents the frequency of DNase I cleavage at specific sites, with higher values indicating regions that are more accessible to regulatory elements.

We obtained the DNase-seq data from the ENCODE project~\citep{encode2012integrated} (K562: ENCFF414OGC; GM12878: ENCFF960FMM). We directly downloaded the data in bigWig format, as it provides a genome-wide distribution of DNase-seq values.


\textbf{H3K27ac.}
H3K27ac refers to the acetylation of lysine 27 on histone H3, a post-translational modification associated with active enhancers and promoters. High levels of H3K27ac in a genomic region indicate that it is likely an active enhancer or promoter, playing a significant role in gene expression regulation.

We also obtained H3K27ac data from the ENCODE project~\citep{encode2012integrated} (K562: ENCFF465GBD; GM12878: ENCFF798KYP), again in bigWig format, which provides the value distribution across the genome.

\textbf{Hi-C.}
Hi-C measures the three-dimensional (3D) organization of genomes by capturing physical interactions between different regions of DNA. This technique helps researchers understand how DNA is folded and structured within the nucleus. Hi-C data provides information about genome contacts, but due to technical limitations, it often has low resolution (typically at 5,000 base pairs), meaning we can only observe interactions between two regions of DNA of at least this length.

In this work, we follow previous studies~\citep{ABC}, calculating the frequency of contacts between a specific region (TSS) and all other regions, generating a Hi-C frequency distribution across the genome.

The Hi-C data were sourced from the 4D Nucleome project~\citep{4DN} (K562: 4DNFITUOMFUQ; GM12878: 4DNFI1UEG1HD).

\begin{table}
    \caption{Hyperparameter values and their search space (final choices are highlighted in \textbf{bold}).}
    \label{app:hyper}
    \centering
    \scalebox{1.0}{
    \begin{tabular}{c|c}
        \toprule
        Hyperparameters & Values \\
        \midrule 
        \# Layers of Generator & 4 \\
        \# Layers of Predictor & 4 \\
        Hidden dimensions & 128 \\
        $\alpha_3, \beta_3$ & $[1,9], \textbf{[10,90]}, [10,190],[10,10],[10,1.11]$ \\
        % $\beta_3$ & 90 \\
        \# training steps & $\textbf{50000}, 85000$ \\
        Batch size & 8 \\
        Learning rate & $1e-3, \textbf{5e-4}, 1e-4, 5e-5$ \\
        Scheduler strategy & CosineLR with Linear Warmup \\
        Initial warmup learning rate & 1e-5 \\
        Min learning rate & 1e-4 \\
        Warmup steps & 5,000 \\
        Validation model selection criterion & validation MSE \\
        \bottomrule
    \end{tabular}
    \label{tab:hyperparameters}
    }
\end{table}



\textbf{mRNA half-life and promoter activity features.}
When predicting the CAGE values, following the implementation of \citet{epinformer}, we use the promoter activity feature and mRNA half-life features as supplementary for fair comparison and further improvement. The promoter activity feature is defined as the square root of the product of DNase-seq and H3K27ac signal values. The mRNA features include G/C contents, lengths of functional regions, intron length, and exon junction density within the coding region. Specifically, the features are
\begin{itemize}
    \item The log-transformed z-score of the 5' UTR (untranslated region) length. 
    \item The log-transformed z-score of the CDS (coding sequence) length.
    \item The log-transformed z-score of the 3' UTR (untranslated region) length.
    \item The GC content of the 5' UTR, expressed as the proportion of G and C bases.
    \item The GC content of the CDS.
    \item The GC content of the 3' UTR.
    \item The log-transformed z-score of the total intron length for a gene.
    \item The exon density within the open reading frame (ORF), reflecting the number of exon junctions per unit length of the ORF.
\end{itemize}





\subsection{Experiment setup}
\label{sec:experiment_setup}
Here we present some hyperparameters values and their search space in Table~\ref{tab:hyperparameters}.

\subsection{Experiment results}
Based on Table \ref{result:cage} and Table \ref{result:macs}, here we present the experimental results with standard deviation in Table \ref{app:k562} and Table \ref{app:gm12878}. The results are from five runs of different random seeds: \{2,22,222,2222,22222\}.

\begin{table}
    \vspace{-3mm}
    \caption{Performance on Gene Expression CAGE Prediction with Standard Deviation for Cell Type K562.}
    \label{app:k562}
    \centering
    \scalebox{1.0}{
    \begin{tabular}{c|ccc}
        \toprule
        % & \multicolumn{3}{c||}{K562} & \multicolumn{3}{c}{GM12878} \\
        & MSE $\downarrow$ & MAE $\downarrow$ & Pearson $\uparrow$ \\
        \midrule 
        Enformer & 0.2920 $\pm$ 0.0050 & 0.4056 $\pm$ 0.0040 & 0.7961 $\pm$ 0.0019  \\
        HyenaDNA & 0.2265 $\pm$ 0.0013 & 0.3497 $\pm$ 0.0012 & 0.8425 $\pm$ 0.0008 \\
        Mamba & 0.2241 $\pm$ 0.0027 & 0.3416 $\pm$ 0.0026 & 0.8412 $\pm$ 0.0021 \\
        Caduceus & 0.2197 $\pm$ 0.0038 & 0.3327 $\pm$ 0.0070 & 0.8475 $\pm$ 0.0014 \\
        \midrule
        Caduceus w/ signals & \underline{0.1959 $\pm$ 0.0036} & \underline{0.3187 $\pm$ 0.0036} & \underline{0.8630 $\pm$ 0.0008} \\
        \midrule
        EPInformer & {0.2140 $\pm$ 0.0042} & {0.3291 $\pm$ 0.0031} & {0.8473 $\pm$ 0.0017} \\
        \midrule
        MACS3 & $0.2195 \pm 0.0023$ & $0.3455 \pm 0.0018$ & $0.8435 \pm 0.0013$ \\
        \midrule
        Seq2Exp-hard & {0.1863 $\pm$ 0.0051} & {0.3074 $\pm$ 0.0036} & {0.8682 $\pm$ 0.0045} \\
        Seq2Exp-soft & $\textbf{0.1856 $\pm$ 0.0032}$ & $\textbf{0.3054 $\pm$ 0.0024}$ & $\textbf{0.8723 $\pm$ 0.0012}$ \\
        \bottomrule
    \end{tabular}
    }
    \vspace{-3mm}
\end{table}


\begin{table}
    \vspace{-3mm}
    \caption{Performance on Gene Expression CAGE Prediction with Standard Deviation for Cell Type GM12878.}
    \label{app:gm12878}
    \centering
    \scalebox{1.0}{
    \begin{tabular}{c|ccc}
        \toprule
        % & \multicolumn{3}{c||}{K562} & \multicolumn{3}{c}{GM12878} \\
        & MSE $\downarrow$ & MAE $\downarrow$ & Pearson $\uparrow$ \\
        \midrule 
        Enformer & 0.2889 $\pm$ 0.0009 & 0.4185 $\pm$ 0.0013 & 0.8327 $\pm$ 0.0025 \\
        HyenaDNA & 0.2217 $\pm$ 0.0018 & 0.3562 $\pm$ 0.0012 & 0.8729 $\pm$ 0.0010 \\
        Mamba & 0.2145 $\pm$ 0.0021 & 0.3446 $\pm$ 0.0022 & 0.8788 $\pm$ 0.0011 \\
        Caduceus & 0.2124 $\pm$ 0.0037 & 0.3436 $\pm$ 0.0031 & 0.8819 $\pm$ 0.0009 \\
        \midrule
        Caduceus w/ signals & \underline{0.1942 $\pm$ 0.0058} & {0.3269 $\pm$ 0.0048} & \underline{0.8928 $\pm$ 0.0017} \\
        \midrule
        EPInformer & {0.1975 $\pm$ 0.0031} & \underline{0.3246 $\pm$ 0.0025} & {0.8907 $\pm$ 0.0011} \\
        \midrule
        MACS3 & $0.2340 \pm 0.0028$ & $0.3654 \pm 0.0017$ & $0.8634 \pm 0.0020$ \\
        \midrule
        Seq2Exp-hard & {0.1890 $\pm$ 0.0045} & {0.3199 $\pm$ 0.0040} & {0.8916 $\pm$ 0.0027} \\
        Seq2Exp-soft & \textbf{0.1873 $\pm$ 0.0044} & $\textbf{0.3137 $\pm$ 0.0028}$ & $\textbf{0.8951 $\pm$ 0.0038}$ \\
        \bottomrule
    \end{tabular}
    }
    \vspace{-3mm}
\end{table}


% \subsection{Results on Perturbation Experiments}

% % We conducted perturbation experiments on selected regions. For each continuous region, we manually masked it to observe the resulting changes in the model’s predictions. Our results were compared to CRISPRi-FlowFISH experiments~\citep{ABC}, where enhancer regions were pre-defined based on epigenomic signals and biologically removed to evaluate their impact on gene expression. Following prior work~\citep{epinformer}, we used the KLF1 gene as a case study. The comparison of results is shown below.



% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figs/perturb_full.png}
%     \caption{Perturbation results between Seq2Exp and CRISPRi-FlowFISH on KLF1 Gene Type.}
%     \label{fig:perturb_full}
% \end{figure}

% % From Figure~\ref{fig:perturb_full}, we can observe the differences in perturbation results. We can see that Seq2Exp focuses more on the middle regions, which is near the TSS point, thus has high influences to the gene expression values prediction. By biological experiment, CRISPRi-FlowFISH detects a boarder region of enhancers, but the change in gene expression is often too small, and ignoring many significant regions. This means that regions selected by~\citet{ABC} and~\citet{epinformer} ignore some parts that contribute significantly to the final gene expression predictions. If we zoom in the central part, we can observe more clearly.

% % As seen in Figure~\ref{fig:perturb_full}, the perturbation results highlight clear differences between the methods. Seq2Exp focuses more on regions near the transcription start site (TSS), which have a significant influence on gene expression prediction. In contrast, CRISPRi-FlowFISH identifies a broader range of enhancer regions, but the resulting changes in gene expression are often small, overlooking several key regions. This suggests that the regions selected by~\citet{ABC} and~\citet{epinformer} may miss critical areas that contribute meaningfully to gene expression predictions. Zooming in on the central region provides further insights.

% % \begin{figure}[t]
% %     \centering
% %     \includegraphics[width=0.9\linewidth]{figs/perturb_70_130.png}
% %     \caption{Zoomed-in comparison of perturbation results between Seq2Exp and CRISPRi-FlowFISH for the KLF1 gene.}
% %     \label{fig:perturb_zoom}
% % \end{figure}
% % In Figure~\ref{fig:perturb_zoom}, the blue regions identified by Seq2Exp encompass a larger area surrounding the TSS, indicating the potential presence of regulatory elements that may have a significant impact on gene expression predictions. These regions, with their strong influence on the predicted gene expression values, suggest that they could play a crucial role in transcriptional regulation. However, while our model highlights these areas as important, further biological experiments are necessary to definitively confirm their regulatory function. These additional experiments will be essential for validating whether these regions truly act as enhancers or other regulatory elements that contribute to gene expression modulation.

% % You may include other additional sections here.



% \subsection{\redtext{Beta distribution}}
% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\linewidth]{figs/Beta distribution.png}
%     \caption{When the ratio between $\alpha$ and $\beta$ is fixed, reducing the values of the parameters results in a flatter distribution with less sharpness and increased variance indicating more sampled points closer to 0 and 1.}
%     \label{fig:beta_distribution}
% \end{figure}
\end{document}
