\section{Related Work}
\subsection{Fine-tuning LLMs with Human Knowledge}
Large language models learn extensive knowledge from human society and carry out various tasks based on this knowledge ____. Among mainstream techniques, supervised fine-tuning (SFT) aligns LLM with real-world human knowledge, playing a critical role in fulfilling the tasks expected of LLM by humans____. After gathering a large annotated corpus for natural language processing, researchers fine-tune pre-trained language models, enabling LLMs to learn from the carefully selected and annotated human knowledge and apply it to unseen tasks ____. During the SFT process, enhancing model performance can be achieved by expanding the size of fine-tuning data ____ and gathering complex data across various domains and tasks ____, however, it may sacrifice generality by focusing on specific domain fine-tuning. Beyond different data, instruction tuning methods design various instruction-response pairs and enable models to learn response patterns and content more in line with human instruction expectations, making LLMs more truthful, credible, and helpful ____. However, LLMs in instruction tuning are not robust to changes in instruction texts, often generating inconsistent answers to slightly rephrased instructions ____. ____ propose a contrastive instruction tuning approach that maximizes similarity between semantically equivalent instruction pairs and minimizes similarity between semantically distinct pairs, significantly enhancing robustness in instruction fine-tuning.

\subsection{Mitigating Hallucination with LLM’s Knowledge}
LLMs accumulate substantial knowledge during training and fine-tuning, yet unavoidably encounter data compression challenges during training, where the training data size is several times larger than the model's parameters ____. This makes it challenging for LLMs to fully restore the original knowledge, leading to hallucinations ____. Moreover, when the model lacks sufficient information to respond to questions, it tends to guess, producing inaccurate outputs ____. Hallucinations typically involve logical fallacies (errors in reasoning by the model) ____, factual errors (when the model confidently asserts non-existent facts) ____, and data-driven biases (when certain data prevails, potentially skewing the model's output in certain directions) ____. Numerous studies focus on enhancing the models' utilization of knowledge representation to mitigate hallucination issues. ____ employ the RAG approach, bolstering knowledge retrieval and self-feedback, thereby alleviating the model's hallucinations. ____ propose a decoding strategy, which contrasts the differences in tokens’ logits obtained from different layers for reducing hallucinations. ____ are also exploring the issue of model honesty, asserting that when the model can honestly respond ``I don't know'' to unknown knowledge, it enhances the model's credibility.