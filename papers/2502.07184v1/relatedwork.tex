\section{Related Work}
\subsection{Fine-tuning LLMs with Human Knowledge}
Large language models learn extensive knowledge from human society and carry out various tasks based on this knowledge \citep{DBLP:conf/iclr/BurnsYKS23,DBLP:conf/nips/Ouyang0JAWMZASR22, DBLP:journals/corr/abs-2307-09007, DBLP:journals/corr/abs-2308-10855}. Among mainstream techniques, supervised fine-tuning (SFT) aligns LLM with real-world human knowledge, playing a critical role in fulfilling the tasks expected of LLM by humans~\cite{DBLP:conf/acl/LiXC0LMJLZZS24, DBLP:conf/aaai/YuJLHWLCLLTZZXH24, DBLP:conf/emnlp/DuW0D0LZVZSZGL024, DBLP:journals/eswa/LiMCHHLZS25}. After gathering a large annotated corpus for natural language processing, researchers fine-tune pre-trained language models, enabling LLMs to learn from the carefully selected and annotated human knowledge and apply it to unseen tasks \citep{DBLP:conf/iclr/WeiBZGYLDDL22,DBLP:conf/iclr/SanhWRBSACSRDBX22}. During the SFT process, enhancing model performance can be achieved by expanding the size of fine-tuning data \citep{DBLP:journals/corr/abs-2210-11416} and gathering complex data across various domains and tasks \citep{DBLP:journals/corr/abs-2305-14705,DBLP:conf/icml/LongpreHVWCTZLZ23}, however, it may sacrifice generality by focusing on specific domain fine-tuning. Beyond different data, instruction tuning methods design various instruction-response pairs and enable models to learn response patterns and content more in line with human instruction expectations, making LLMs more truthful, credible, and helpful \citep{DBLP:conf/iclr/0131Z00H24,DBLP:conf/iclr/MuennighoffLZZH24}. However, LLMs in instruction tuning are not robust to changes in instruction texts, often generating inconsistent answers to slightly rephrased instructions \citep{DBLP:conf/iclr/LiuLLWYW24}. \citet{DBLP:conf/acl/YanWHZYG0C24} propose a contrastive instruction tuning approach that maximizes similarity between semantically equivalent instruction pairs and minimizes similarity between semantically distinct pairs, significantly enhancing robustness in instruction fine-tuning.

\subsection{Mitigating Hallucination with LLM’s Knowledge}
LLMs accumulate substantial knowledge during training and fine-tuning, yet unavoidably encounter data compression challenges during training, where the training data size is several times larger than the model's parameters \citep{DBLP:journals/corr/abs-2308-07633}. This makes it challenging for LLMs to fully restore the original knowledge, leading to hallucinations \citep{gekhman2024does}. Moreover, when the model lacks sufficient information to respond to questions, it tends to guess, producing inaccurate outputs \citep{DBLP:conf/iclr/MundlerHJV24,DBLP:conf/icml/ZhangPMLS24}. Hallucinations typically involve logical fallacies (errors in reasoning by the model) \citep{DBLP:conf/iclr/Mu024}, factual errors (when the model confidently asserts non-existent facts) \citep{DBLP:conf/icml/Lv0C00024,DBLP:conf/acl/LiCRCZNW24}, and data-driven biases (when certain data prevails, potentially skewing the model's output in certain directions) \citep{DBLP:journals/corr/abs-2405-18654,DBLP:conf/acl/ZhangGLY24}. Numerous studies focus on enhancing the models' utilization of knowledge representation to mitigate hallucination issues. \citet{DBLP:conf/iclr/AsaiWWSH24,DBLP:conf/acl/NiuWZXSZS024} employ the RAG approach, bolstering knowledge retrieval and self-feedback, thereby alleviating the model's hallucinations. \citet{DBLP:conf/iclr/ChuangXLKGH24} propose a decoding strategy, which contrasts the differences in tokens’ logits obtained from different layers for reducing hallucinations. \citet{DBLP:conf/icml/ChengSLZYLLH0Q24,DBLP:conf/naacl/ZhangDLFL0CJZ24} are also exploring the issue of model honesty, asserting that when the model can honestly respond ``I don't know'' to unknown knowledge, it enhances the model's credibility.