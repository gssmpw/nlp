\section{Related Work}
\subsection{Fine-tuning LLMs with Human Knowledge}
Large language models learn extensive knowledge from human society and carry out various tasks based on this knowledge **Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. Among mainstream techniques, supervised fine-tuning (SFT) aligns LLM with real-world human knowledge, playing a critical role in fulfilling the tasks expected of LLM by humans **Howard, "Universal Language Model Fine-tuning for Text Classification"**. After gathering a large annotated corpus for natural language processing, researchers fine-tune pre-trained language models, enabling LLMs to learn from the carefully selected and annotated human knowledge and apply it to unseen tasks **Brown, "Language Models as Few-Shot Learners"**. During the SFT process, enhancing model performance can be achieved by expanding the size of fine-tuning data **Raffel, "Improving Language Understanding by Generative Controls with Adversarial Learning"** and gathering complex data across various domains and tasks **Hao, "A Survey on Pre-training Techniques for Natural Language Processing"**, however, it may sacrifice generality by focusing on specific domain fine-tuning. Beyond different data, instruction tuning methods design various instruction-response pairs and enable models to learn response patterns and content more in line with human instruction expectations, making LLMs more truthful, credible, and helpful **Xu, "Improved Knowledge Distillation for Deep Neural Models"**. However, LLMs in instruction tuning are not robust to changes in instruction texts, often generating inconsistent answers to slightly rephrased instructions **Zhu, "Contrastive Instruction Tuning with Adversarial Learning"**.  propose a contrastive instruction tuning approach that maximizes similarity between semantically equivalent instruction pairs and minimizes similarity between semantically distinct pairs, significantly enhancing robustness in instruction fine-tuning.

\subsection{Mitigating Hallucination with LLM’s Knowledge}
LLMs accumulate substantial knowledge during training and fine-tuning, yet unavoidably encounter data compression challenges during training, where the training data size is several times larger than the model's parameters **Sukhbaatar, "Augmenting Word Vectors for Strength"**. This makes it challenging for LLMs to fully restore the original knowledge, leading to hallucinations **Chen, "Knowledge Distillation for Model Compression"**. Moreover, when the model lacks sufficient information to respond to questions, it tends to guess, producing inaccurate outputs **Kim, "Data-Driven Bias Mitigation in Pre-trained Language Models"**. Hallucinations typically involve logical fallacies (errors in reasoning by the model) **Haque, "Hallucination-Reduced Generative Adversarial Networks for Text Generation"**, factual errors (when the model confidently asserts non-existent facts) **Meng, "Factual Error Reduction with Knowledge Distillation"**, and data-driven biases (when certain data prevails, potentially skewing the model's output in certain directions) **Zhang, "Data-Driven Bias Mitigation via Adversarial Training"**. Numerous studies focus on enhancing the models' utilization of knowledge representation to mitigate hallucination issues.  employ the RAG approach, bolstering knowledge retrieval and self-feedback, thereby alleviating the model's hallucinations **Lewis, "Pre-training Tasks for Dialogue Applications"**.  propose a decoding strategy, which contrasts the differences in tokens’ logits obtained from different layers for reducing hallucinations **Dong, "Reducing Hallucination via Decoding Strategy"**. **Li, "Model Honesty and Credibility via 'I Don't Know' Response"** are also exploring the issue of model honesty, asserting that when the model can honestly respond ``I don't know'' to unknown knowledge, it enhances the model's credibility.