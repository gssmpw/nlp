\section{Related Work}
% \subsection{Sequential Recommender Systems}
\noindent\textbf{Sequential Recommender Systems. }
Recommendation systems primarily focus on capturing collaborative filtering (CF) to identify similar items/users. Matrix Factorization-based approaches, achieved notable success by constructing CF knowledge in a latent space \cite{mnih2007probabilistic, chaney2015probabilistic, he2017neural}. However, in conjunction with CF knowledge, understanding dynamic evolution in temporal user preferences has become a powerful tool, leading to the development of collaborative filtering-based sequential recommenders (CF-SRec) \cite{kang2018self, sun2019bert4rec, wu2019session, kim2023task, hidasi2015session,oh2023muse}.
Initial approaches combined Matrix Factorization with Markov Chains to model temporal dynamics \cite{rendle2010factorizing}. Subsequently, neural network-based methods advanced sequential recommender systems, with GRU4Rec \cite{hidasi2015session} leveraging recurrent architectures, while methods such as Caser \cite{tang2018personalized} and NextItNet \cite{yuan2019simple} adopted Convolutional Neural Networks \cite{krizhevsky2012imagenet}. More recently, models such as SASRec \cite{yuan2019simple} and BERT4Rec \cite{sun2019bert4rec}, based on attention mechanisms, have demonstrated superior performance by focusing on the more relevant interaction sequences. These advancements underscore the importance of effectively modeling user behavior dynamics for improved recommendation accuracy.

% \subsection{LLM-based Recommender Systems}
\smallskip
\noindent\textbf{LLM-based Recommender Systems. }
% TALLRec, LLaRA, CoLLM, A-LLMRec
LLMs have recently gained attention in recommendation systems \cite{yue2023llamarec, harte2023leveraging, dai2023uncovering, wu2024coral}, leveraging their reasoning ability and textual understanding for novel approaches such as zero-shot recommendation \cite{hou2024large} and conversational recommendation \cite{sanner2023large}. However, TALLRec \cite{bao2023tallrec} highlights the gap between LLMs' language modeling tasks and recommendation tasks, proposing a fine-tuning approach through Parameter-Efficient Fine-Tuning (PEFT) to adapt LLMs to recommendation tasks.
More recently, LLaRA \cite{10.1145/3626772.3657690}, CoLLM \cite{zhang2023collm}, and A-LLMRec \cite{10.1145/3637528.3671931} have been proposed. LLaRA and CoLLM combine CF-SRec item embeddings with text embeddings from item titles, enabling LLMs to utilize CF knowledge. A-LLMRec further incorporates item descriptions into a latent space, enabling the model to demonstrate robust performance in various scenarios.
Despite these advancements, prior methods fail to capture dynamic user preferences inherent in user interaction sequences as shown in Sec.~\ref{sec: sequence exp}. 

% \vspace{-1ex}