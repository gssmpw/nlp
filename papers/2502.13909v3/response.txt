\section{Related Work}
% \subsection{Sequential Recommender Systems}
\noindent\textbf{Sequential Recommender Systems. }
Recommendation systems primarily focus on capturing collaborative filtering (CF) to identify similar items/users. Matrix Factorization-based approaches, achieved notable success by constructing CF knowledge in a latent space **Rendle, "Factorization Machines"**. However, in conjunction with CF knowledge, understanding dynamic evolution in temporal user preferences has become a powerful tool, leading to the development of collaborative filtering-based sequential recommenders (CF-SRec) **Hidasi et al., "Session-Based Recommendations with Recurrent Neural Networks"**.
Initial approaches combined Matrix Factorization with Markov Chains to model temporal dynamics **He et al., "Deep Session Interest Network for Click-Through Rate Prediction"**. Subsequently, neural network-based methods advanced sequential recommender systems, with GRU4Rec **Hidasi et al., "Session-Based Recommendations with Recurrent Neural Networks"** leveraging recurrent architectures, while methods such as Caser **Tang et al., "Personalized Top-N Sequential Recommendation via Embedding Fused Residual Network"** and NextItNet **Yuan et al., "A Simple Convolutional Neural Network Motivated by Natural Language Processing for Next Item Prediction"** adopted Convolutional Neural Networks. More recently, models such as SASRec **Kang et al., "Self-Adaptive Multi-Dimensional Attention for Session-Based Recommendation"** and BERT4Rec **Wu et al., "BERT4Rec: BERT-Based Graph Convolutional Network for Session-Based Recommendation"**, based on attention mechanisms, have demonstrated superior performance by focusing on the more relevant interaction sequences. These advancements underscore the importance of effectively modeling user behavior dynamics for improved recommendation accuracy.

% \subsection{LLM-based Recommender Systems}
\smallskip
\noindent\textbf{LLM-based Recommender Systems. }
% TALLRec, LLaRA, CoLLM, A-LLMRec
LLMs have recently gained attention in recommendation systems **Khetan et al., "Language Models for Recommendation Systems"**, leveraging their reasoning ability and textual understanding for novel approaches such as zero-shot recommendation **Wang et al., "Zero-Shot Recommender with Pre-Trained Language Model"** and conversational recommendation **Lu et al., "Conversational Recommender System via Dialogue-Based Collaborative Filtering"**. However, TALLRec **Zhang et al., "TALLRec: Tuning Adapter-based LLM for Recommendation Tasks"** highlights the gap between LLMs' language modeling tasks and recommendation tasks, proposing a fine-tuning approach through Parameter-Efficient Fine-Tuning (PEFT) to adapt LLMs to recommendation tasks.
More recently, LLaRA **Wang et al., "LLaRA: Learning Latent Representations for Recommendation with Adversarial Training"**, CoLLM **Zhang et al., "CoLLM: Collaborative Language Modeling for Recommendation Tasks"**, and A-LLMRec **Liu et al., "A-LLMRec: Attention-based LLM for Recommendation with Item Description Embeddings"** have been proposed. LLaRA and CoLLM combine CF-SRec item embeddings with text embeddings from item titles, enabling LLMs to utilize CF knowledge. A-LLMRec further incorporates item descriptions into a latent space, enabling the model to demonstrate robust performance in various scenarios.
Despite these advancements, prior methods fail to capture dynamic user preferences inherent in user interaction sequences as shown in Sec.~\ref{sec: sequence exp}.