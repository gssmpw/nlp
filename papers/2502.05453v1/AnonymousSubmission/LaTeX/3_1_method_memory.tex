\subsection{Problem Setting}\label{sec:setting} 
Our goal is to demonstrate that Large Language Models (LLMs) can effectively plan, coordinate, and execute tasks in a multi-agent environment where collaboration and resource management are critical. % In our extension of the Crafter environment \cite{hafner2021benchmarking}, 
We consider an environment model that follows a Decentralized Partially Observable Markov Decision Process (Dec-POMDP)~\cite{bernstein2002complexity,chen2024rgmcomm}, as is common in cooperative MARL, where agents lack complete information about the environment and have only local observations. Figure \ref{fig:framework} gives an overview of this framework. We model the environment as a Dec-POMDP with communication as a tuple $D=\langle I, n, S, A, P, \Omega, O, g, R \rangle$, where $I = \{1,2,\dots,n\}$ is a set of $n$ agents, $S$ is the joint \textbf{state} space, and $A=A_1\times A_2 \times \dots \times A_n$ is the joint \textbf{action} space, where $\boldsymbol{a}=(a_1,a_2,\dots,a_n)\in A$ denotes the joint action of all agents. $P(\boldsymbol{s}'|\boldsymbol{s},\boldsymbol{a}): S \times A \times S \to [0,1] $ is the \textbf{state transition function} that describes how the environment state evolves, given the actions taken by the agents.

We consider an episode that is divided into a series of timeslots $t = 1,2,\ldots$; at the start of each episode, agents respawn in the center of the map. Within each timeslot, each agent can take an \textit{action}, e.g., sharing resources with another agent or working towards a goal. 
%\carlee{is this right? Or is the action simply deciding which direction to travel in?} \hq{Yes. The action space includes moving to different directions, sharing, do, crafting tools, etc.}
Agents decide their action based on their observations, which are contained in the \textbf{observation} space $\Omega$, and $O(\boldsymbol{s}, i): S \times I \to \Omega$ denotes the function that maps from the joint state space to distributions of observations for each agent $i$.
Each agent's observations, as shown in Figure~\ref{fig:framework}, include its own environment input, as well as communication messages from the other agents. We use $g: \Omega \to M$ to denote the \textbf{communication message generation function} that each agent $j$ uses to encode its local observation $o_j$ into a communication message for other agents $i \neq j$. 
We use $\boldsymbol{m_{-i}}=\{m_{j}=g(o_j), \forall j \neq i\}$ to denote the collection of messages agent $i$ receives from all other agents $j \neq i$. 

 
% $\Omega$ is the \textbf{observation} space. $O(\boldsymbol{s}, i): S \times I \to \Omega$ is a function that maps from the joint state space to distributions of observations for each agent $i$. 
In deciding which actions to take, the agents' goal is to maximize the long-term reward. More formally, they aim to find a policy $\pi$ that maximizes the average expected return $\lim_{T \to \infty} (1/T) E_{\pi} [{\sum_{t=0}^T R_{t}}]$, where $R(\boldsymbol{s}, \boldsymbol{a}): S \times A \to \mathbb{R}$ is the reward of the current state $\boldsymbol{s}$ and joint action $\boldsymbol{a}$ and $R_t$ is the reward incurred in timeslot $t$. As shown in Figure~\ref{fig:framework}, this policy goal is enforced in our framework by including it in a prompt that is fed to a \textbf{multi-modal large language model (MLLM)} along with a prompt to generate plans and actions for the current timestep, thus forming the policy $\pi$. For example, Agent 6 in Figure~\ref{fig:framework} is told to find a diamond.
% and $\gamma$ is the discount factor. 
To ensure the LLM finds a good policy based on historical data, each agent maintains its own memory, consisting of  both \textbf{Short-Term Working Memory (\textbf{STWM})} and \textbf{Long-Term Memory (LTM)}. The STWM holds information for decision-making at the current timestep, combining current environmental perceptions with relevant information retrieved from LTM. The STWM is then included in the MLLM prompt. % fed into a \textbf{multi-modal large language model (MLLM)} along with a prompt to generate plans and actions for the current timestep, thus forming the policy $\pi$. 
The STWM and MLLM responses are then consolidated into the agent’s LTM, enabling agents to make strategic decisions based on historical context.

% In Dec-POMDP with communications, each agent $i$ considers an individual policy $\pi_i(a_i|o_i,\boldsymbol{m_{-i}})$ conditioned on local observation $o_i$ and messages $\boldsymbol{m_{-i}}$, i.e., $\pi=[\pi_i(a_i|o_i,\boldsymbol{m_{-i}}),\forall i]$.
% The objective is to find a policy $\pi$ that maximizes the average expected return $J(\pi) =\lim_{T \to \infty} (1/T) E_{\pi} [{\sum_{t=0}^T R_{t}}]$. The core system is structured to enable agents to learn from past interactions and \textcolor{orange}{transfer} those experiences to \textcolor{orange}{learn to collaborate in }new scenarios.

% \textcolor{orange}{Jingdi: I added this section, but I think the message generation function could not be highlighted here.}
% \carlee{Maybe we can define it and say it is connected to the communication protocol}

\begin{figure}[h]
  \centering
  \includegraphics[width=1\linewidth]{AnonymousSubmission/LaTeX/figures/framework.pdf}
  \caption{Framework Overview. Multiple agents respawn on the map and interact with each other through a memory system and communication protocol, aiming to collect a diamond as fast as possible.}
  \label{fig:framework}
  %\Description{Framework Overview.}
\end{figure}

% \subsection{Framework Overview}
% Our decentralized cooperative agents operate within a modified \textit{Dec-POMDP} framework, where each agent receives partial observations and makes decisions independently. Agents collaborate by sharing resources and updating each other on their goals and progress. The core system is structured to enable agents to learn from past interactions and \textcolor{orange}{transfer} those experiences to \textcolor{orange}{learn to collaborate in }new scenarios.

% \carlee{Integrate this with Section 3.1 (this seems to explain how agents take actions, while 3.1 explains how the environment evolves)}
% At the core of our framework is the interaction between working memory and long-term memory. Figure \ref{fig:framework} shows the framework. At the start of each episode, agents respawn in the center of the map. Each agent maintains its own memory, consisting of two components:  \textcolor{orange}{\textbf{Short-Term Working Memory (\textbf{STWM})}} and \textcolor{orange}{\textbf{Long-Term Memory (LTM)}}. The \textbf{STWM} holds information for decision-making at the current timestep, combining current environmental perceptions with relevant information retrieved from LTM. The \textbf{STWM} is fed into a \textbf{multi-modal large language model (MLLM)} along with a prompt to generate plans and actions for the current timestep. The \textbf{STWM} and MLLM responses are then consolidated into the agent’s LTM, enabling agents to make strategic decisions based on historical context. \carlee{refer to Sections 3.3 and 3.4 here for details of the memory structure}

\subsection{Adaptive Knowledge Graph Memory System}\label{sec:memory}
Recent work in multi-task learning has demonstrated the benefits of integrating heterogeneous data sources for optimized decision-making \cite{baltruvsaitis2018multimodal, ngiam2011multimodal, xu2024predicting}. In the proposed \textbf{Adaptive Knowledge Graph Memory System (A-KGMS)}, inspired by human cognitive processes \cite{sumers2023cognitive}, each agent uses a \textit{multi-modal memory system} combining short-term and long-term memories that facilitates storing and retrieving experiences across different memory types. While existing memory systems focus on aspects like semantic understanding \cite{li2024optimus}, our system is goal-oriented.
%\carlee{How does this compare to existing LLM memory systems (do any exist)?} 
This memory system allows agents to learn from past experiences, facilitating task completion in open-world environments. %\carlee{how is the memory shared across agents?} %is essential for enabling agents to learn from past interactions and apply those experiences to new scenarios.
% \carlee{refer to the figure more in explaining short term and long term memory}

%As illustrated in Figure~\ref{fig:memory_system}, the memory system is divided into two main components: \textit{working memory} and \textit{long-term memory}. The working memory captures immediate environmental inputs, while the long-term memory retains historical experiences and knowledge. We describe the components of our memory system in detail:

\begin{figure}[h]
  \centering
  \includegraphics[width=1\linewidth]{AnonymousSubmission/LaTeX/figures/memory.pdf}
  \caption{Memory System. 
  The system consists of \textit{working memory} and \textit{long-term memory}. \textit{Sensory inputs} (1) are captured in \textit{working memory} (2), alongside relevant information retrieved from \textit{long-term memory} (4). The agent 'thinks' using an \textit{MLLM} (3) to generate responses and action plans, which are then stored in long-term memory. A \textit{consolidation process} updates the \textit{goal-oriented hierarchical knowledge graph} (5), linking new experiences to past events. This graph comprises \textit{experience nodes} $E$, \textit{goal nodes} $G$, and \textit{long-term goal nodes} $LTG$.}
  \label{fig:memory_system}
  \vspace{-0.1in}
  %\Description{Memory System.}
\end{figure}
%The system is divided into two main components: working memory and long-term memory. The \textit{sensory inputs} (1) from the environment are captured in the \textit{working memory} (2), along with relevant information retrieved from the long-term memory (4). The agent then "thinks" by feeding the working memory into a \textit{MLLM} (3) along with a prompt to generate responses and action plans. The responses, along with the working memory, are then stored in the experience pool in \textit{long-term memory} (4). A consolidation process is then triggered to update the \textit{goal-oriented hierarchical knowledge graph} (5), connecting the current experience with past events. The knowledge graph consists of experience nodes $E$, goal nodes $G$, and long-term goal nodes $LTG$.
\textbf{Experience.} The \textbf{experience} for each time step in a learning episode consists of two stages: \textbf{pre-stage} and \textbf{post-stage}, as shown in Parts 2 and 3 of Figure~\ref{fig:memory_system} The \textbf{pre-stage} refers to the information available to the agent at the current timestep for decision-making. The \textbf{post-stage} is the thought process generated by the language model, then consolidated into \textbf{Long-Term Memory}. The post-stage contains full information, including environment cues and the agent's thoughts, which help generalize actions in similar scenarios by emphasizing decision-making and consequences.
%\carlee{explain the rationale for this design}

%Each episode consists of multiple timesteps, and during each timestep, an \textbf{experience} is created, which consists of two stages: \textbf{pre-stage} and \textbf{post-stage}. The information that is available for the agent to use in making a decision at the current timestep is the pre-stage. The thought process, which is the response generated from the language model \textcolor{blue}{[what types of questions does the prompt ask the agents?]}, is referred to as the post-stage, which is then consolidated into the long-term memory.

\textbf{Short-Term Working Memory (STWM, Part 2 of Figure~\ref{fig:memory_system}).} STWM refers to the pre-stage experience and consists of four parts: (\rom{1}). \textbf{Sensory memory} captures raw environmental observations, such as visual inputs and communication messages; (\rom{2}). \textbf{Episodic memory} stores contextual details, including the agent's health, location, time, and inventory; (\rom{3}). \textbf{Feedback}, retrieved from long-term semantic and procedural memory, provides available actions and their prerequisites; (\rom{4}). \textbf{Retrospection} offers context from the hierarchical knowledge graph, including recent events, achievements, goals, and progress. STWM, along with a prompt, is processed by a multi-modal large language model (MLLM) to help the agent `think' and `plan' its next action.

%\textbf{\textcolor{orange}{Short-Term Working Memory (\textbf{STWM})}} The working memory is also referred to as the pre-stage experience. The \textbf{sensory memory} refers to the raw observations from the environment, including visual input and communication messages. The \textbf{episodic memory} stores the \textcolor{blue}{episode's} contextual information, including the agent's health stats, location, time, and inventory items. The \textbf{feedback} \textcolor{blue}{is the agent's available actions and these action's prerequisites. This information} is retrieved from the long-term memory, specifically from the semantic memory and procedural memory. %, which provide the agent's available actions and their prerequisites. 
%Finally, \textbf{retrospection} contains information retrieved from the hierarchical knowledge graph from the long-term memory, providing more contextual information, such as recent events, past accomplishments, goals, and current progress. The working memory is fed into a multi-modal large language model (MLLM) along with a prompt, allowing the agent to "think" and make plans to determine the action to take.

\textbf{Long-Term Memory (LTM, Part 4 of Figure~\ref{fig:memory_system}).} LTM consists of an experience pool of post-stage experiences. A consolidation process updates the goal-oriented hierarchical knowledge graph (further explained below) by organizing experiences according to their goals, connecting current experiences with past events and allowing agents to access memories useful to their short- and long-term goals.
%\carlee{and allowing agents to access memories useful to their short- and long-term goals}. 
\textbf{Semantic memory} holds factual knowledge, specifically the hierarchical crafting tree of the environment, which is programmed explicitly using logical expressions. This factual knowledge provides accurate feedback on action prerequisites, 
%\textcolor{blue}{[what kinds of factual knowledge, and how was it obtained?]} about the environment, providing accurate feedback on action prerequisites, 
while \textbf{procedural memory} stores all available actions. The consolidation process is triggered whenever a new experience is added, updating the hierarchical knowledge graph.

%\textcolor{orange}{\textbf{Long-Term Memory (LTM)}} The long-term memory is composed of an \textbf{experience pool} of post-stage experiences. A consolidation process is then triggered to update the \textbf{goal-oriented hierarchical knowledge graph} \textcolor{blue}{through organizing the post-stage experiences according to their respective goals}, hence connecting the current experience with past events. The \textbf{semantic memory} consists of factual knowledge \textcolor{blue}{[what kinds of factual knowledge, and how was it obtained?]} about the environment that can provide accurate feedback to the agent on the prerequisites of actions. The \textbf{procedural memory} retains all available actions in the environment. The consolidation process happens when a new experience is added to the long-term memory, which involves updating the hierarchical knowledge graph.

\textbf{Goal-Oriented Hierarchical Knowledge Graph (Part 5 of Figure~\ref{fig:memory_system}).} % As shown in Figure~\ref{fig:memory_system}, 
The agent maintains an adaptive goal-oriented hierarchical knowledge graph within its LTM. Each node represents an experience ($E$), and nodes are linked sequentially based on goal-related sequences, reflecting the agent's progress. We link each experience node to a goal node corresponding to the goal it tries to achieve, derived from the LLM output.
%\carlee{We link each experience node to a goal node corresponding to the goal it tries to achieve, derived from the LLM output.} % A specific algorithm \textcolor{blue}{[state the algorithm or technique that helps us link the experience according to goals]} links \textbf{experience} to goals. 
When a new goal begins, a new \textbf{goal node} ($G$) is created and connected to the previous one, forming a sequence that tracks the agent's journey. A higher-level \textbf{Long-Term Goal node} ($LTG$) is generated from goal nodes, providing an overview of the agent’s long-term progress. At the end of the \textbf{consolidation process}, a summary is updated for the most recent goal node, including the long-term goal, current goal, past goals, and recent experiences. \textbf{At the planning stage}, the agent retrieves information from the most recent goal node ($G$) and combines it with pre-stage experiences $\boldsymbol{E}$ to form its STWM. This enables the agent to reason and make decisions by integrating past and present data, as well as adjusting strategies in real-time to optimize progress toward current and long-term goals.

%\textbf{Goal-Oriented Hierarchical Knowledge Graph.} As depicted in Figure~\ref{fig:memory_system}, the agent maintains an adaptive goal-oriented hierarchical knowledge graph (Part 5 of Figure~\ref{fig:memory_system}) within its long-term memory. Each node in the knowledge graph represents an experience ($E$), and nodes are connected based on goal-related sequences. When the agent is working on a goal, experience nodes ($E$) are linked sequentially, \textcolor{blue}{[state the algorithm or technique that helps us link the experience according to goals]} reflecting the agent's progress on that goal. Upon starting a new goal, a goal node ($G$) is created and linked to the previous goal node, forming a sequence of goals that records the agent’s journey. On top of the goal node is the long-term goal node ($LTG$), which is generated based on goal nodes in a similar way. Long-term goal nodes provide a higher-level view of the agent’s overall progress, guiding the agent toward its long-term objectives. At the end of the consolidation process, a summary will be updated for the most recent goal node: long-term goal, current goal, past accomplished goal, and recent experiences toward completing the current goal.

%When planning, the agent retrieves relevant information from the most recent goal node and combines it with pre-stage experiences to form the working memory. This helps the agent reason and make decisions based on both past and present information, adjusting strategies in real-time and optimizing progress toward both current and long-term goals.

\subsection{Structured Reasoning Output}\label{sec:output}
%Converting unstructured inputs into structured data is crucial for developing multi-step agent workflows that enable LLMs to perform actions \cite{pokrass2023structured}. Structured outputs provide a framework that constrains language models to adhere to predefined schemas. In our reasoning process, we utilize structured prompting techniques to achieve this. We employ a carefully tuned structured output format along with an environment explanation as the prompt. This prompt organizes the working memory components into actionable insights, enabling the agent to generate well-informed decisions.
% \carlee{Is this about how agents process the outputs of the LLM?}
%\textcolor{blue}{[maybe some brief examples of what unstructured input (free flow text?) vs structured data/outputs are. cause the above paragraph is kinda abstract. an alternative is to put the specifics described below first.]}
%\textcolor{blue}{[if this structured output helps reduce communication required, we could mention it too.]}

Converting unstructured inputs, such as free-form text, into structured data is crucial for developing multi-step agent workflows that enable LLMs to perform actions \cite{pokrass2023structured}. Structured outputs provide a framework that constrains language models to follow predefined \textbf{schemas}. For example, instead of processing unstructured text like \textit{`The agent moved north to pick up a key'}, we format it into structured data such as \textit{`[Action: Move North, Reason: Pick up a key]'}. We utilize structured prompting techniques, combining a carefully tuned output format with environment explanations, to organize working memory into actionable insights. This reduces communication needs and helps the agent make well-informed decisions. Meanwhile, the number of output tokens is significantly reduced due to formatted and focused responses, resulting in faster generation speed.

\textbf{Schemas.} The schemas are built around three core components: (\rom{1}) \textbf{Reflection}, which enables agents to review recent actions, summarize outcomes, and reflect on lessons learned to adjust future strategies; (\rom{2}) \textbf{Goal}, which tracks both current and long-term objectives, including sub-goals and progress updates, helping the agent stay focused and break down tasks into manageable steps; and (\rom{3}) \textbf{NextAction}, which determines the agent’s upcoming actions and the reasoning behind them, evaluating prerequisites and ensuring alignment with both short-term and long-term goals. Each component is represented by a data class with fields specifying required responses and data types, using the Python \textit{Pydantic} library.

%\textbf{Schemas.} The schemas are defined by three core components: NextAction, Reflection, and Goal. Each component is represented by a data class with fields specifying required responses and data types using the Python Pydantic library.

% \mycodebox[red!20!white]{
% class NextAction(BaseModel):\\
% \hspace*{5mm}next\_action: ActionType\\
% \hspace*{5mm}next\_action\_reason: str\\
% \hspace*{5mm}final\_next\_action: ActionType\\
% \hspace*{5mm}final\_next\_action\_reason: str\\
% }

% \mycodebox[blue!20]{%
% class Reflection(BaseModel):\\
% \hspace*{5mm}vision: list[MaterialType]\\
% \hspace*{5mm}last\_action: ActionType\\
% \hspace*{5mm}last\_action\_result: ResultType\\
% \hspace*{5mm}last\_action\_result\_reflection: str\\
% \hspace*{5mm}last\_action\_repeated\_reflection: str\\
% }

% \mycodebox[yellow!20]{%
% class Goal(BaseModel):\\
% \hspace*{5mm}ultimate\_goal: LongTermGoalType\\
% \hspace*{5mm}long\_term\_goal: LongTermGoalType\\
% \hspace*{5mm}long\_term\_goal\_subgoals: str\\
% \hspace*{5mm}long\_term\_goal\_progress: GoalType\\
% \hspace*{5mm}current\_goal: GoalType\\
% \hspace*{5mm}current\_goal\_reason: str\\
% }

%The \textbf{Reflection} component enables agents to utilize their working memory by reviewing their most recent actions. It summarizes these actions, their outcomes, and the agent’s reflections on the results, helping identify lessons learned and adapt future strategies accordingly. The \textbf{Goal} component tracks both the agent’s current and long-term objectives, including sub-goals and progress updates. This helps the agent stay focused on overarching goals while managing immediate tasks. By linking current objectives with long-term plans, this component allows agents to articulate their goals and break them down into manageable sub-goals. The \textbf{NextAction} component determines the agent's upcoming actions and the reasoning behind them. It allows agents to evaluate the prerequisites for their next move, why they chose it, and how it aligns with both their current and long-term goals.