
%Multi-agent cooperation and communication have been pivotal research areas for decades \cite{stone2000multiagent}. Numerous works have focused on optimizing communication efficiency between agents \cite{jiang2018learning, das2019tarmac, wang2021tom2c, wan2022handmethat}, enabling collaboration in visually rich environments \cite{jain2020cordial}, and grounding communication within specific environmental contexts \cite{patel2021interpretation, mandi2024roco, narayan2019collaborative}.

%Many existing approaches either neglect communication entirely \cite{jaderberg2019human, samvelyan2019starcraft, carroll2019utility, puig2020watch} or rely on communication strategies involving uninterpretable vectors \cite{jiang2018learning, das2019tarmac} or limited discrete symbols \cite{lowe2017multi, jaques2019social, jain2020cordial, patel2021interpretation, resnick2018pommerman}. 



%%%%%%%%%%%%%%%%





\textbf{Multi-Agent Cooperation and Communication.} Multi-agent reinforcement learning (MARL) has advanced decision-making in communication-reliant tasks~\cite{6303906}, with approaches including \textit{continuous}~\cite{dial,commnet,maddpg,atoc,wang2019learning,sarnet,chen2023real,chen2023distributional} and \textit{discrete communication}~\cite{emergentlan,freed2020sparse,lazaridou2020emergent,9812285,tucker2022trading,chen2024rgmcomm}. Continuous communication, though expressive, suffers from explainability and high overhead~\cite{chen2023ride}, while discrete methods~\cite{maddpg,commnet} limit relational learning. Learnable messages~\cite{tucker2021emergent} improve but lack Dec-POMDP guarantees and require large vocabularies.
%Multi-agent reinforcement learning (MARL) has advanced decision-making in communication-reliant tasks~\cite{6303906}, with approaches including \textit{continuous}~\cite{dial,commnet,maddpg,atoc,wang2019learning,sarnet,chen2023real,chen2023distributional} and \textit{discrete communication}~\cite{emergentlan,freed2020sparse,lazaridou2020emergent,9812285,tucker2022trading,chen2024rgmcomm}. Continuous communication, though expressive, suffers from low explainability and high overhead~\cite{chen2023ride}, while discrete methods, like one-hot vectors~\cite{maddpg,commnet}, limit relational learning. Learnable messages~\cite{tucker2021emergent} improve this but lack Dec-POMDP guarantees and require large vocabularies.

Recent work uses LLMs to enhance communication in embodied tasks, e.g., CoELA’s free-form natural language generation~\cite{zhang2023building}. However, these approaches lack structured planning or reasoning about actions’ consequences. LLM-based agents like SPRING leverage external knowledge for planning, but often do not learn from in-environment interactions~\cite{wu2024spring}. \hq{LLM agents have also been used to simulate human behavior and feedback but rely solely on the language model rather than structured reasoning~\cite{yang2024llm}.} Our work addresses these by proposing a structured communication protocol and a knowledge-graph memory system that allows agents to learn from each other’s experiences.



%\textbf{Multi-Agent Cooperation and Communication:} Multi-agent reinforcement learning (MARL) has recently advanced as a way to make decisions in communication-reliant scenarios~\cite{6303906}. Approaches include \textit{continuous}~\cite{dial,commnet,maddpg,atoc,wang2019learning,sarnet} and \textit{discrete communication}~\cite{emergentlan,freed2020sparse,lazaridou2020emergent,9812285,tucker2022trading}. Continuous communication uses numerical vectors, offering low explainability and high overhead~\cite{chen2023ride}. Discrete communication, often via one-hot vectors~\cite{maddpg,commnet,freed2020sparse}, limits relational learning between messages. Recent efforts, like learnable messages or autoencoder bottlenecks~\cite{tucker2021emergent,tucker2022trading}, improve upon these but lack Dec-POMDP guarantees and still require large vocabularies.


\iffalse

Pioneering work has leveraged LLMs to facilitate communication and decision-making in embodied tasks. For instance, CoELA uses free-form natural language generation to enable communication between agents \cite{zhang2023building}. However, these works primarily focus on communication without addressing the need for structured planning or reasoning about the consequences of actions.
An ongoing discussion compares LLM-based agents with traditional RL agents. While RL algorithms improve through trial and error, they often rely on carefully crafted reward functions, which require significant expert input \cite{hafner2021benchmarking}. In contrast, LLM-based agents, like SPRING, leverage external knowledge for planning but do not often learn from interactions within the environment \cite{wu2024spring}.
In contrast, our work proposes a structured approach to interaction and communication. We introduce effective communication protocols, %that mirror real-world scenarios, 
and a knowledge-graph based memory system, where distributed agents interact and learn from each others' past experiences. %must interact efficiently, emphasizing not only communication but also planning and the consequences of actions.

%\textcolor{blue}{[M: I took out the subsection on "multi-agent environments" (which discuss atari, mindcraft etc). since proposing crafter is not our contribution. we extended crafter, which is alr mentioned in the intro.]}
%\subsection{Multi-Agent Environments}

%Several platforms have been developed to facilitate multi-agent tasks, ranging from competitive to collaborative environments \cite{lowe2017multi, resnick2018pommerman, shu2018m, jaderberg2019human, samvelyan2019starcraft, baker2019emergent, bard2020hanabi, carroll2019utility, smith2024concordia}. These environments allow researchers to benchmark multi-agent coordination, communication, and decision-making in a variety of contexts.

%The Atari environment \cite{bellemare2013arcade} has long been a benchmark for reinforcement learning, but its computation-heavy protocols and deterministic nature limit its adaptability to real-time scenarios \cite{machado2018revisiting}. Minecraft, as a well-known open-ended environment, has gained significant attention in recent years. Its rich, dynamic world provides a challenging testbed for researchers, particularly in multi-agent learning, long-horizon planning, and collaboration. Previous work in Minecraft has involved agents performing simple tasks using policy models \cite{fan2022minedojo, baker2022video, lifshitz2024steve}. However, these agents often struggle with more complex tasks due to limitations in understanding instructions and planning. Recent work has integrated LLMs for planning and reflection modules to build more capable agents \cite{wang2023voyager, zhu2023ghost, li2024optimus}. While Minecraft-inspired environments such as Malmo \cite{johnson2016malmo} and MineRL \cite{milani2020minerl} provide open-world challenges with complex terrains and resources, they are often too computationally intensive to solve fully, making evaluation difficult \cite{milani2020retrospective}.

%In contrast, Crafter \cite{hafner2021benchmarking} offers a simplified version of Minecraft, providing a faster environment with more accessible and meaningful evaluation metrics for reinforcement learning. While Crafter strikes a balance between complexity and speed, it remains a single-agent platform, and its reward structure presents challenges in multi-agent scenarios.

%Existing environments often focus heavily on language, oversimplifying action consequences, or conversely emphasize actions with minimal focus on language. To address these limitations, we propose a multi-agent variant of Crafter that balances communication, planning, and action consequences, making it more suitable for complex multi-agent tasks.

\fi

\textbf{Generative Agents. }The use of LLMs for decision-making and planning in dynamic environments has grown rapidly \cite{yang2023foundation, wang2024survey, xi2023rise, sumers2023cognitive}. Although LLMs face challenges in handling complex reasoning tasks \cite{bubeck2023sparks}, they have shown significant promise in guiding agents in real-time environments \cite{park2023generative, sharma2021skill, raman2024cape, pallagani2022plansformer, gramopadhye2023generating, yuan2023distilling, li2022pre, wang2023describe}. Some approaches have integrated LLMs for real-time planning and decision-making \cite{li2023behavior, padmakumar2022teach, kolve2017ai2, shridhar2020alfred, misra2018mapping, zhu2017visual, brodeur2017home, xia2018gibson, savva2019habitat, huang2022language}. %, while others employ prompting techniques and inner monologue systems to incorporate environmental feedback \cite{huang2022language}. 
Recent research has also explored %cooperative multi-agent 
systems where multiple LLMs collaborate or debate to enhance problem-solving capabilities \cite{li2023camel, du2023improving, wang2023unleashing}.
LLMs have demonstrated effectiveness in high-level planning for simpler tasks, where the action space and trajectory length are limited \cite{huang2022language, ahn2022can}.
There has been less work on LLM-agents in open-world game environments.
Scaling LLM-based planning to open-world environments \cite{du2023improving, wang2023unleashing} like Crafter presents challenges such as long-horizon planning and the management of multiple objectives simultaneously, increasing the complexity of planning. 
\cite{wu2024spring, li2024optimus, wang2023voyager} involve single agent systems in open-world games. We present a novel framework, harnessing the interaction of multiple LLM-agents to tackle multiple objectives with dependencies in open-world exploration games.
%Open-world games like Crafter require agents to pursue multiple objectives simultaneously, significantly increasing the complexity of planning.
%Our approach leverages robust structured prompting techniques \cite{pokrass2023structured} for long-horizon, multi-objective tasks, eliminating the need for demonstration data. \textcolor{blue}{[M: what is demonstration data?]}

%\subsection{Knowledge Graphs and Memory Systems}
\textbf{Knowledge Graph-aided LLM agents. }Knowledge graphs help in organizing information and facilitating structured reasoning for large language model agents \cite{ji2021survey, hogan2021knowledge, edge2024local}. They allow agents to store, retrieve, and update knowledge. %in real time, supporting decentralized decision-making and enhancing understanding by capturing relationships between tasks and tracking long-term goals.
Recent works have explored applications of knowledge graphs in decision-making. For example, SPRING uses a knowledge graph to structure decision-making processes \cite{wu2024spring}, while Optimus leverages a knowledge graph to model goal hierarchies \cite{li2024optimus}. %enabling agents to navigate and prioritize tasks more effectively \cite{li2024optimus}. 
While these approaches use knowledge graphs for reasoning and goal understanding, 
\cite{wu2024spring}'s graph is static,
%While these approaches use knowledge graphs for reasoning and goal understanding, 
and neither are designed for effective communication in multi-agent systems. % \textcolor{blue}{Furthermore, their graph structures were static, and not dynamic and updated upon new information received.}
In our work, the graph functions as the long-term memory, which agents dynamically update and build, providing other agents with contextual awareness. %build goal-driven hierarchical knowledge graphs from past experiences, providing them with contextual awareness of their progress and their ongoing tasks.

%Our work introduces an adaptive knowledge graph framework that tightly integrates with memory-driven decision-making in decentralized systems. In this system, the knowledge graph is coupled with the memory architecture, handling sensory perceptions and episodic experiences, effectively functioning as the long-term memory. Agents dynamically build goal-driven hierarchical knowledge graphs from past experiences, providing them with contextual awareness of their progress and their ongoing tasks. This aids in efficient memory retrieval and supports decision-making in long-horizon, complex tasks.

%By reducing the need for repeated retrievals and unnecessary communication, our approach empowers agents with an evolving knowledge base that enhances cooperation and coordination in decentralized multi-agent environments. The adaptive knowledge graph enables agents to maintain accurate situational awareness, significantly improving the efficiency and effectiveness of collaborative problem-solving.