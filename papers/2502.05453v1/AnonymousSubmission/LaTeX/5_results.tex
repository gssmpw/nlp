\subsection{Experiment Setup and Baselines}

\textbf{Experiment Setup.} We evaluate our framework on the proposed Multi-agent Crafter environment (Sec.~\ref{sec:crafter}) to test agents' ability to plan and complete hierarchical tasks through cooperation and assess how well our framework scales in multi-agent settings. The final goal is for agents to collect a diamond, which requires completing a series of tasks: collecting stone, iron, and coal, crafting a furnace, and making an iron pickaxe (task hierarchy shown in Figure \ref{fig:crafter}). With our framework's memory systems and communication module for multi-agent collaboration, agents efficiently collect a diamond by distributing tasks and gathering resources in parallel. To evaluate the performance of our framework and analyze the effectiveness of the memory system and communication, we conducted experiments with $n=1,2,6$ agents. The LLM used in our framework is GPT-4o, deployed via the Azure platform.

%\textbf{Multi-agent Crafter. }The multi-agent Crafter is designed to evaluate the ability of agents to plan and complete hierarchical tasks through cooperation. In order to collect a diamond in the environment, agents must complete a sequence of tasks. To collect a diamond, agents need an iron pickaxe, which requires crafting a furnace and collecting coal, iron, and stone. The detailed task hierarchy is shown in Figure \ref{fig:crafter}. Agents can efficiently gather a diamond by strategically distributing tasks and gathering resources in parallel.
%\carlee{need to explain the sequence of tasks here and how they test long-term planning and reasoning. We should also define the ``achievement score'' as a metric} \hq{done}

%In this section, we present the experimental results and the impact of our proposed multi-agent framework, which incorporates memory systems and a communication policy.


\textbf{Baselines.} We compare our framework with RL/MARL and LLM baselines: \textbf{(1). RL/MARL baselines. }Since we have modified an environment originally designed for reinforcement learning (RL) agents, we test it with RL agents to evaluate if they can learn from the environment in both single-agent and multi-agent setups. We use \textbf{Proximal Policy Optimization (PPO)} for the single-agent case and \textbf{Multi-Agent Deep Deterministic Policy Gradient (MADDPG)} for the multi-agent case, MADDPG is recognized as state-of-the-art by BenchMARL~\cite{bettini2024benchmarlbenchmarkingmultiagentreinforcement}. The objective of the environment is to gather a diamond as quickly as possible. The reward is structured so that each item in the hierarchy tree (Figure \ref{fig:crafter}) is assigned a score based on its depth, along with a time penalty. \textbf{(2). LLM baselines. }
We also includes LLM-based baselines under difference settings: \textbf{(\rom{1}). LLM basic,} a \textbf{basic LLM Agent} that employs the structured output prompt combined with a basic memory approach in a single-agent setting, where the agent is aware only of its previous actions; \textbf{(\rom{2}). LLM Mem,} a structured output prompt with memory only (Mem only); \textbf{(\rom{3}). LLM MemComm} a structured output prompt with both memory (Mem) and communication (Comm). These different LLM agents' configurations allowed us to evaluate the contribution of each component of \framework. 

\textbf{Metrics.} Algorithm performance across all LLM-based settings was measured using the Average number of time Steps (AS) within an episode required for at least one agent to complete each task, as displayed in Figure \ref{fig:agents_results} and Table \ref{tab:result_table}.



%\textcolor{orange}{TO-DO, add two results to result section}

%Performance across all settings was measured using the Average Steps (AS) required to complete each task, as displayed in Figures \ref{fig:two_agents} and \ref{fig:six_agents}. The detailed values are provided in Table \ref{tab:result_table}. 

%\textbf{RL Agent. }Since we have modified the environment originally designed for reinforcement learning (RL) agents, we test it with RL agents to evaluate if they can learn from the environment in both single-agent and multi-agent setups. We use Proximal Policy Optimization (PPO) for the single-agent case and Multi-Agent Deep Deterministic Policy Gradient (MADDPG) for the multi-agent case. The objective of the environment is to gather a diamond as quickly as possible. The reward is structured so that each item in the hierarchy tree (Figure \ref{fig:crafter}) is assigned a score based on its depth, along with a time penalty.

%\carlee{what was the objective here? also, we should show the single-agent training curve too} \hq{will update the single agent training.}
%\carlee{I think we can call the agents without prompts or memory ``baselines'' as well} \hq{done}

\subsection{Experimental Results}
\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.47\linewidth}
        \includegraphics[width=\linewidth]{AnonymousSubmission/LaTeX/figures/single-agent.png}
        \caption{Single-agent. PPO.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\linewidth}
        \raisebox{0.02cm}{\includegraphics[width=\linewidth]{AnonymousSubmission/LaTeX/figures/multi-agent.png}}
        \caption{Two-agents. MADDPG.}
    \end{subfigure}
    \caption{%\hq{RL agents are training in the environment, and increasing rewards indicate learning. However, the learned policy quickly hits a bottleneck, as further reward improvements require the agent to acquire advanced skills following a hierarchical order. Learning remains prohibitively slow.} 
    \jd{Evaluation of $n$-RL-trained agents in MAC: Both PPO-trained and MADDPG-trained agents initially show increasing total rewards, indicating active learning. However, they fail to achieve higher rewards as further improvements require acquiring advanced skills in a hierarchical order. Learning remains prohibitively slow for both RL agents.}}
    \label{fig:training}
      \vspace{-0.1in}
\end{figure}

%\textbf{Training. }The training results are shown in Figure \ref{fig:training}. For each training session, we trained for 2000 episodes. In both cases, we applied a Convolutional Neural Network (CNN) policy for the actor and critic networks. The rewards for both cases show a clear increasing trend, indicating that the environment is suitable for RL algorithms and for researchers to develop advanced RL and MARL algorithms. However, it is evident that after 2000 episodes of training, the RL agents still underperform significantly, highlighting the limitations of RL methods in complex, dynamic environments. The MARL agents require significant training time, taking over 22 hours to complete 2000 episodes. To demonstrate the feasibility of learning in this multi-agent environment, we present a training curve that highlights both the agents' learning progress and the limitations inherent in traditional RL approaches for such complex scenarios. 
%\carlee{how many hours of training did this take?} \hq{done}


\textbf{Evaluating RL/MARL agents. }The training results are shown in Figure \ref{fig:training}. 
We trained for 1,000 episodes using a Convolutional Neural Network (CNN) policy for both the actor and critic networks. The reward trends indicate improvement, confirming that the environment is suited for researchers developing RL and MARL methods. However, after 1,000 episodes, the RL agents still perform suboptimally. This highlights both the progress made by the agents and the limitations of traditional RL approaches in such scenarios.

%\textbf{LLM Agent.} To evaluate the performance of our framework and analyze the effectiveness of the memory system and communication, we conducted experiments with single, two, and six agents under different settings. These settings included (1) using a structured output prompt with memory only (Mem only) and (2) using a structured output prompt with both memory and communication (Mem + Comm). These configurations allowed us to evaluate the contribution of each component. For the \textbf{baseline LLM Agent}, we employed the structured output prompt combined with a basic memory approach in a single-agent setting, where the agent is aware only of its previous actions. Performance across all settings was measured using the Average Steps (AS) required to complete each task, as displayed in Figures \ref{fig:two_agents} and \ref{fig:six_agents}. The detailed values are provided in Table \ref{tab:result_table}. 

\begin{figure}[!b]
    \centering
    \includegraphics[width=0.95\linewidth]{AnonymousSubmission/LaTeX/figures/two_six_agents.png}
    \caption{\textbf{Two agents} with communication complete tasks faster than two agents without communication, who complete tasks at about the same speed as a single agent. The basic agent is slower than agents with our memory system. \textbf{Six agents} with communication complete tasks faster than six agents without communication. They are also faster than two agents with communication.}
    \label{fig:agents_results}
\end{figure}

% \begin{figure}[!b]
%     \centering
%     \includegraphics[width=1\linewidth]{AnonymousSubmission/LaTeX/figures/two_agents.png}
%     \caption{Two agents with communication complete tasks faster than two agents without communication, who complete tasks at about the same speed as a single agent. The basic agent is slower than agents with our memory system.}
%     \label{fig:two_agents}
% \end{figure}

% \begin{figure}[!b]
%     \centering
%     \includegraphics[width=1\linewidth]{AnonymousSubmission/LaTeX/figures/six_agents.png}
%     \caption{Six agents with communication complete tasks faster than six agents without communication. They are also faster than two agents with communication (Figure~\ref{fig:two_agents}).}
%     \label{fig:six_agents}
% \end{figure}

\begin{table*}[t]
	\caption{Number of average steps to complete each task in Multi-agent Crafter across five different environments. Communication greatly accelerates agents' time to achieve each task. Results are reported with standard deviations over ten runs.}
	\label{tab:result_table}
	\centering
    \resizebox{\textwidth}{!}{%
	\begin{tabular}{lp{2cm}cccccccccc}\toprule
		\textit{Setup} & \textit{Setting} & \textit{Collect wood} & \textit{Place table} & \textit{Make wood pickaxe} & \textit{Collect stone} & \textit{Make stone pickaxe} & \textit{Collect iron} & \textit{Collect coal} & \textit{Place furnace} & \textit{Make iron pickaxe} & \textit{Collect diamond} \\ \midrule
            Baseline & Simple Mem & $6.2 \pm 0.82$ & $14.0 \pm 3.34$ & $30.2 \pm 9.45$ & $35.4 \pm 9.83$ & $54.2 \pm 19.62$ & $76.6 \pm 15.11$ & $76.2 \pm 22.13$ & $197.5 \pm 109.79$ & $281.0 \pm 87.48$ & $334.67 \pm 95.07$ \\ \midrule
		Single & Mem          & $5.2 \pm 0.84$ & $9.6 \pm 0.55$ & $16.2 \pm 1.48$ & $23.4 \pm 6.47$   & $32.2 \pm 12.09$ & $55.2 \pm 23.96$ & $84.0 \pm 42.64$ & $112.2 \pm 36.79$ & $118.8 \pm 36.31$ & $140.0 \pm 35.94$ \\ \midrule
		2 Agents & Mem        & $5.0 \pm 1.00$ & $9.8 \pm 0.45$ & $16.6 \pm 1.82$ & $24.6 \pm 7.30$   & $36.8 \pm 12.91$ & $68.0 \pm 18.06$ & $75.8 \pm 27.82$ & $117.4 \pm 35.15$ & $119.6 \pm 34.41$ & $150.4 \pm 32.21$ \\
		 & Mem+Comm & $5.0 \pm 1.00$ & $8.0 \pm 1.22$ & $16.6 \pm 4.04$ & $21.8 \pm 4.97$   & $33.2 \pm 13.44$ & $64.2 \pm 21.39$ & $88.0 \pm 27.23$ & $107.0 \pm 25.70$ & $112.0 \pm 29.33$ & $121.0 \pm 30.27$ \\  \midrule
		6 Agents & Mem        & $3.0 \pm 1.00$ & $8.6 \pm 1.14$ & $16.6 \pm 2.70$ & $23.8 \pm 5.54$   & $40.2 \pm 15.11$ & $63.4 \pm 23.86$ & $79.8 \pm 37.93$ & $107.4 \pm 31.84$ & $119.2 \pm 31.61$ & $140.6 \pm 21.89$ \\
		 & Mem+Comm & $4.0 \pm 1.22$ & $9.2 \pm 2.95$ & $12.0 \pm 4.95$ & $20.6 \pm 11.95$  & $28.8 \pm 18.27$ & $62.6 \pm 23.20$ & $67.2 \pm 17.58$ & $80.8 \pm 19.69$ & $81.8 \pm 19.69$ & $85.4 \pm 18.04$ \\ \bottomrule
	\end{tabular}
    }
\end{table*}

\textbf{Evaluating LLM basic agents. } Figure \ref{fig:agents_results} shows a comparison between the LLM basic agent in the single-agent, the two-agent, and six-agent scenarios. The LLM basic agent, which only relies on its past actions, without having the goal-oriented memory system, demonstrates an impressive ability to plan and execute tasks using the structured output prompt. However, the LLM basic agent struggles significantly with more complex tasks that have additional prerequisites. The success rate (i.e., fraction of episodes for which the task is successfullly executed) for the LLM basic agent is 100\% for the first seven tasks, but it drops to 60\% for the last three tasks. One major factor slowing down the agent, if not preventing it from progressing altogether, when completing more complex tasks is that, as the number of past actions increases, the agent loses track of what has already been done. This leads to inefficient behavior, such as placing multiple tables, which wastes resources and time (Table~\ref{tab:result_table}). Additionally, the LLM basic agent struggles to understand whether its actions have been successful.

\subsubsection{Evaluating DAMCS Agents and Ablation Study}

\paragraph{\textbf{Single-agent scenarios.}} With the goal-oriented memory system A-KGMS, the LLM Mem agent understands the hierarchy and works toward gathering a diamond. LLM Mem agent is able to unlock tasks in a hierarchical order, completing each of them significantly faster than the LLM basic (Table~\ref{tab:result_table}). This is largely due to the efficient, relevant memory retrieval and reliable semantic memory feedback, which we can see from the fact that the relative progress of the single agent relative  to the basic one accelerates for tasks further in the hierarchy: the single agent collects the diamond almost 3x faster. For example, once the agent sets the goal of placing a table, the semantic memory informs the agent that placing a table requires two pieces of wood. Although this information is available in the environment description text, the LLM basic struggles to understand it when there is too much of this information. With the memory system, the agent is able to set goals in an achievable order. The memory system prevents the agent from repeating efforts.

\paragraph{\textbf{Two-agent scenarios.}} In the two-agent scenario, LLM Mem agents without communication perform similarly to the LLM Mem agent in single-agent scenario (Figure~\ref{fig:agents_results}). However, in a cooperative scenario with communication, the LLM MemComm agents are able to distribute tasks more efficiently. On average, LLM MemComm agents collect a diamond in 121 steps, compared to 140 steps for a single agent, resulting in 13.6\% fewer steps to achieve the goal, and 63\% fewer steps compared to the LLM basic agent (Table~\ref{tab:result_table}). Notably, two LLM Mem agents without communication take an average of 150 steps to obtain the diamond. This is due to conflicts of interest, where both LLM Mem agents may work on the same task or compete for the same resource. For example, two LLM Mem might attempt to gather the same piece of wood, but only one will successfully collect it. Another interesting finding that contributes to the higher step count for diamond collection is the \textit{butterfly effect}. Since Multi-agent Crafter is a procedurally generated, sequential world, the placement of a crafting station—such as a table—in a particular location can influence subsequent gameplay, impacting the agents' performance.

\paragraph{\textbf{Six-agent scenarios.}} Figure \ref{fig:agents_results} illustrates the six-agent scenario. Without communication, the six LLM Mem agents take approximately the same number of steps to collect a diamond as a single agent. Unlike the two-agent scenario, where interference is limited to just two agents, having more agents increases the potential for conflicts. However, the presence of more agents also provides a greater opportunity to explore different areas, increasing the likelihood of finding locations where resources like stone, coal, iron, and diamonds are clustered together, which reduces the number of steps needed to collect the diamond. \hq{Each agent's \textbf{A-KGMS} is detailed in Appendix \ref{appendix:agent_memories}.}

LLM MemComm with communication significantly outperforms all other methods in the six-agent scenarios. Using our communication protocol \textbf{S-CS}, the LLM MemComm agents can efficiently distribute tasks, work independently on simpler objectives, and collaborate on more complex tasks, reducing the total number of steps required. With communication, the six LLM MemComm agents are able to collect a diamond using 39\% fewer steps compared to a single LLM Mem agent with memory and 74\% fewer steps compared to the LLM basic agent.

% \paragraph{\textbf{Ablation Study. }}These findings highlight the crucial role of both the memory system (\textbf{A-KGMS}) and communication protocol (\textbf{S-CS}) for efficient multi-agent cooperation. In our ablation study, (LLM Mem) agents with only memory could plan better using past experiences but struggled to coordinate efficiently (Figure \ref{fig:agents_results}), and also takes 39\% more steps to achieve the final goal compared (LLM MemComm) agents, while agents with only communication (LLM Comm) shared information but lacked deeper reasoning. The full LLM MemComm framework, combining both memory and structured communication, significantly outperformed both, reducing redundant actions and optimizing task distribution. This resulted in a 74\% reduction in steps compared to LLM basic agents, showing that both memory and communication are essential for effective collaboration.

% Our structured memory approach is particularly effective in cooperative tasks requiring shared knowledge. However, evaluating how memory persistence affects performance over time remains difficult. Unlike MARL agents that optimize predefined reward functions, LLM agents engage in \textit{open-ended reasoning}, making evaluation subjective. We assess adaptability by analyzing how agents respond to dynamic conditions (e.g., resource depletion, unexpected obstacles), but future work should explicitly link structured memory to adaptability in evolving multi-agent scenarios.

% \paragraph{Scalability in Large-Agent Systems}
% While our experiments with $n = \{1, 2, 6\}$ agents demonstrate efficiency gains using structured communication (S-CS), larger-scale systems pose additional challenges. Increased agent complexity can introduce \textit{bottlenecks in communication}, redundant task execution, or inefficiencies in LLM inference. Future studies should analyze whether emergent cooperation mechanisms remain stable as the number of interacting agents grows.

%These findings highlight the importance of both the memory system (\textbf{A-KGMS}) and the communication protocol (\textbf{S-CS}), as the combination of memory and structured communication is crucial for efficient multi-agent cooperation and task completion.


% \carlee{Since the memory system is one of our contributions, we should have some comparison to agents without memory} \hq{done}
%\hq{appendix: prompt and structured output format, running example}
