\begin{figure}[h]
  \centering
  \includegraphics[width=1\linewidth]{AnonymousSubmission/LaTeX/figures/crafter.pdf}
  \caption{The Multi-agent Crafter Environment. Agents spawn in a shared environment and interact to collect a diamond as quickly as possible, terminating the session upon success. To achieve this, they must craft tools in a hierarchical order while maintaining their health stats.}
  \label{fig:crafter}
\end{figure}

%In recent years, human-centric decision-making \textcolor{blue}{[since our framework doesn't involve humans, should we change this to "joint decision making over multiple-users"]} has emerged as a critical area of research, driven by its potential to fundamentally reshape decision processes across various domains, which can be modeled as multi-agent decision-making problems. Multi-agent reinforcement learning (MARL) has been applied to learn effective control policies for complex multi-agent tasks where agents possess only partial observability of the environment~\cite{maddpg,sukhbaatar2016learning,chen2024rgmcomm}. Such tasks span domains from simulated multi-player games (e.g., DoTA, StarCraft) to real-world settings, such as robot soccer, autonomous vehicle planning~\cite{6303906}, smart grid control~\cite{4840087}, and multi-robot search-and-rescue~\cite{10.5555/646288.686470}.

%MARL~\cite{busoniu2008comprehensive} has gained significant attention in the field of Reinforcement Learning (RL). This paper focuses on cooperative MARL, which is typically modeled as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP)\cite{bernstein2002complexity}, where all agents aim to maximize the long-term return by sharing a reward. The Centralized Training with Decentralized Execution (CTDE) paradigm\cite{maddpg} is widely used in cooperative MARL. Many MARL algorithms, including COMA~\cite{foerster2018counterfactual}, MAAC~\cite{iqbal2019actor}, VDN~\cite{sunehag2017value}, QMIX~\cite{rashid2018qmix}, MD-MADDPG~\cite{MDMADDPG}, FAC-MADDPG~\cite{charlot2020third}, MA-PPO~\cite{mappo}, DOP~\cite{wang2020dop}, FOP~\cite{zhang2021fop}, and PMIC~\cite{li2022pmic}, adopt CTDE paradigms to improve performance in cooperative MARL. However, CTDE relies on centralized modules, which are often impractical in real-world scenarios due to network constraints and privacy concerns. These challenges are further compounded by scalability issues, computational complexity, non-stationarity, long-horizon planning, communication, coordination, and performance evaluation \cite{huh2023multi, busoniu2008comprehensive}. Additionally, the design of reliable interactive environments remains difficult, limiting the progress of learning-based research \cite{guo2024large}. As a result, decentralized approaches are gaining attention for their flexibility and scalability \cite{zhang2018fully, zhou2023decentralized}, but fully decentralized MARL is still in its early stages, facing issues like suboptimal joint policies, high sample complexity, interpretability, and effective coordination \cite{jiang2024fully}.


In recent years, joint decision-making by multiple users has become a critical research area, with applications like robot soccer, autonomous vehicles~\cite{6303906}, cyber security~\cite{chen2023explainable,chen2023ride}, smart grid control~\cite{4840087}, and search-and-rescue~\cite{10.5555/646288.686470} modeled as multi-agent decision-making problems~\cite{chen2021bringing,chen2024rgmdt}. Automating decisions in such scenarios may significantly improve efficiency, cost, and safety, yet they often require agents to make joint long-term planning and reasoning decisions under uncertainty, often in large-scale systems with high volumes of dynamic information.
% Multi-agent reinforcement learning (MARL) has been applied to learn control policies for complex tasks in both simulations (e.g., DoTA, StarCraft) and real-world applications like robot soccer, autonomous vehicles~\cite{6303906}, smart grid control~\cite{4840087}, and search-and-rescue~\cite{10.5555/646288.686470}. % \carlee{I think we should frame this as general multi-agent decision-making problems, and then say MARL is a common, perhaps the most common, solution, but that it has significant drawbacks} 
Much recent work aims to use AI to tackle these challenges by formally modeling such decision-making problems as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP)~\cite{bernstein2002complexity}. Centralized Training and Decentralized Execution (CTDE) is a popular solution framework in which agents centrally train a policy that learns how they should make decisions from historical observations. This centralized training allows agents to learn how to cooperate, but the policy can still be executed distributedly by each agent at test time~\cite{maddpg,li2022pmic,chen2024rgmcomm}. In multi-agent reinforcement learning (MARL), 
%for example, such CTDE frameworks include COMA~\cite{foerster2018counterfactual}, MAAC~\cite{iqbal2019actor}, VDN~\cite{sunehag2017value}, QMIX~\cite{rashid2018qmix}, MD-MADDPG~\cite{MDMADDPG}, MADDPG~\cite{maddpg}, MA-PPO~\cite{mappo}, DOP~\cite{wang2020dop}, FOP~\cite{zhang2021fop}, and PMIC~\cite{li2022pmic}. 
%However, 
CTDE still faces challenges like constraints on agent communication, difficulty in adapting to non-stationary environments, and scalability of the centralized training to a large number of agents
%privacy concerns, scalability, non-stationarity, and communication
~\cite{huh2023multi}.



%Open-world games such as Minecraft~\cite{fan2022minedojo} or Crafter~\cite{hafner2021benchmarking} involve large, expansive worlds, where agents are free to roam around and complete tasks, without a pre-specified path. Thus, they serve as a common benchmark for assessing the planning capabilities of AI agents. However, traditional MARL frameworks and other existing CTDE multi-agent approaches struggle in such open-world games because they require centralized, long-term reasoning and planning, which is difficult to achieve without custom-designed reward functions. Furthermore, open-world games often involve multi-modal data, such as vision and text, which adds another layer of complexity for MARL agents to process. These agents need to integrate and reason across different types of information, which generally demands significant amounts of training time and data. Optimizing policies for distributed tasks in a centralized manner becomes inefficient as the complexity of the environment increases. CTDE approaches also face scalability issues, as they rely on centralized modules for training, which scale poorly to large numbers of agents. Additionally, these methods assume fixed cooperation strategies, making them inflexible in dynamic environments where agents must adapt and plan independently.

Open-world games like Minecraft~\cite{fan2022minedojo} and Crafter~\cite{hafner2021benchmarking} feature large, expansive worlds where agents are free to roam and complete tasks without a pre-specified path, serving as benchmarks for AI agent capabilities. However, traditional MARL and CTDE approaches struggle in such environments due to the need for centralized, long-term reasoning, which is difficult without custom rewards. Additionally, processing multi-modal data (e.g., vision, text) in open-world games increases complexity and requires extensive training. As environments grow, centralized training for optimizing distributed tasks becomes inefficient, and CTDE methods face scalability issues, relying on fixed cooperation strategies that hinder adaptability in dynamic settings.


%Applying MARL \textcolor{blue}{and other existing CTDE multi-agent approaches} to such games is often difficult, with agents often unable to achieve human-like performance. Indeed, these games often exacerbate MARL limitations, as achieving specified goals in such open worlds requires long-term reasoning and planning that is difficult to enforce without custom-designed reward functions. Such worlds also generally require processing \textit{multi-modal}, e.g., vision and text, data about their environments and then relating it to optimal agent policies, which generally requires significant training time and data.



%\textcolor{blue}{\textbf{Open-world games} such as Mindcraft~\cite{fan2022minedojo} or Crafter~\cite{hafner2021benchmarking} involve large, expansive worlds, where agents are free to roam around and complete tasks, without a pre-specified path. Thus, they serve as a common benchmark for assessing the planning capabilities of AI agents.
% They serve as a benchmark for assessing AI capabilities in planning, resource allocation, and multi-timescale decision making under uncertainty, in large scale systems with high volumes of dynamic information.  Such challenges are valuable for advancing AI in real-world applications, such as search and rescue operations.
%}
%Applying MARL \textcolor{blue}{and other existing CTDE multi-agent approaches} to such games is often difficult, with agents often unable to achieve human-like performance. Indeed, these games often exacerbate MARL limitations, as achieving specified goals in such open worlds requires long-term reasoning and planning that is difficult to enforce without custom-designed reward functions. Such worlds also generally require processing \textit{multi-modal}, e.g., vision and text, data about their environments and then relating it to optimal agent policies, which generally requires significant training time and data. % including high sample complexity, poor scalability to multiple agents or large action spaces, and difficulty incorporating prior knowledge, preventing agents from achieving human-like performance. 
%These challenges are often due to (1) \textit{inefficient and non-generalizable learning} in online settings, resulting in a need for vast amounts of training data particularly when multiple agents are present; (2) a \textit{diverse action space} requiring multi-tasking (e.g., across tasks like gathering resources, building structures), thus complicating the policy that needs to be learned; (3) difficulty in fostering \textit{consistent cooperation} across agents; (4) \textit{long-term planning demands} \textcolor{blue}{given the scale of open-world games}, and (5) \textit{lack of interpretability} in cooperative mechanisms. % Addressing these challenges would advance \textcolor{blue}{multi-agent solutions.}
% Such challenges are particularly apparent for multi-agent tasks with partial observability, \textcolor{blue}{where each agent does not observe the full state of the environment} and communication is therefore crucial for task completion.% \carlee{We should only cite challenges here that we can actually solve (or at least, that don't appear for in LLM-based agents)} 
% Decentralized MARL is gaining attention for its flexibility \carlee{it's not clear what ``flexibility'' means in this context}, but it still faces issues with suboptimal policies, high sample complexity, and coordination~\cite{zhang2018fully,jiang2024fully}. \carlee{I think this paragraph should be focused on explaining the decision-making framework in general (agents observe part of the environment, possibly talk with other agents, and then take an action that may affect the environment) and giving some examples of why it's relevant. After we establish that, we can discuss MARL as a solution and its drawbacks}

% In multi-agent tasks with partial observability, \textcolor{blue}{where each agent does not observe the full state of the environment,} communication is also crucial for task completion. \carlee{need to explain what partial observabilty means}  \carlee{This feels like it belongs in the related work section--it's too much detail for the introduction, since this isn't a MARL paper}



%While MARL has demonstrated impressive results, it faces several limitations, including high sample complexity, \textcolor{blue}{scalability challenges in large scale systems,} and challenges in incorporating prior knowledge, which prevents agents from achieving human-like performance. These limitations are \textcolor{blue}{further exacerbated} %particularly problematic 
%In addition, when applying MARL to complex open-world survival games like Crafter~\cite{hafner2021benchmarking} or Minecraft~\cite{fan2022minedojo}, it faces several limitations, including high sample complexity, \textcolor{blue}{scalability challenges in large scale systems,} and challenges in incorporating prior knowledge, which prevents agents from achieving human-like performance. These limitations are \textcolor{blue}{further exacerbated} when numerous challenges arise: (1) learning in online settings is neither sample-efficient nor easily generalizable, (2) a diverse action space requires multi-tasking capabilities \textcolor{blue}{(e.g. across tasks like gathering resources and building structures)}, (3) fostering consistent cooperative behavior across tasks is difficult, (4) the technology tree demands long-term planning and deep exploration, \textcolor{blue}{as each human level action in open-world exploration can be brokend down into multiple steps in the action space}, and (5) the cooperative mechanisms that emerge \textcolor{blue}{from MARL algorithms} are often not human-interpretable. 
%\textcolor{blue}{[M: machine-interpretable, not human interpretable right?]}. 
%Addressing these challenges is crucial for advancing MARL towards more sample-efficient and robust solutions.
%\textcolor{blue}{[M: We dont advance/ build upon MARL in this paper specifically right? How about "It would therefore be beneficial to address these challenges through a novel framework."]}



% \carlee{need to motivate why we care about open-world games. I think we can make this paragraph about challenges to existing approaches generally (of which MARL is one), setting up LLM agents as a solution}

% When applying MARL \textcolor{blue}{and other existing multi-agent approaches} to complex open-world games like Crafter~\cite{hafner2021benchmarking} or Minecraft~\cite{fan2022minedojo} , several limitations arise, including high sample complexity, scalability issues, and difficulty incorporating prior knowledge, preventing agents from achieving human-like performance. These challenges are compounded by: (1) inefficient and non-generalizable learning in online settings, (2) a diverse action space requiring multi-tasking (e.g., across tasks like gathering resources, building structures), (3) difficulty in fostering consistent cooperation, (4) long-term planning demands \textcolor{blue}{given the scale of open-world games}, and (5) lack of interpretability in cooperative mechanisms. Addressing these challenges would advance \textcolor{blue}{multi-agent solutions.} %towards more sample-efficient, robust solutions.
%\textcolor{blue}{[M: machine-interpretable, not human interpretable right?]}. 
%\textcolor{blue}{[M: We dont advance/ build upon MARL in this paper specifically right? How about "It would therefore be beneficial to address these challenges through a novel framework."]}






%Humans are naturally skilled at collaboration and communication to solve complex tasks in dynamic environments \cite{woolley2010evidence}. Replicating these abilities in AI agents that can act consistently based on their past experiences and collaborate effectively in changing environments poses significant challenges. 

%A common approach in multi-agent systems is Centralized Training with Decentralized Execution (CTDE), but this relies on centralized modules, which are often impractical in real-world scenarios due to network constraints and privacy concerns. These challenges are further compounded by scalability issues, computational complexity, non-stationarity, long-horizon planning, communication, coordination, and performance evaluation \cite{huh2023multi, busoniu2008comprehensive}. Additionally, the design of reliable interactive environments remains difficult, limiting the progress of learning-based research \cite{guo2024large}. As a result, decentralized approaches are gaining attention for their flexibility and scalability \cite{zhang2018fully, zhou2023decentralized}, but fully decentralized MARL is still in its early stages, facing issues like suboptimal joint policies, high sample complexity, interpretability, and effective coordination \cite{jiang2024fully}.

%Large language models (LLMs)~\cite{brown2020language} have demonstrated remarkable success when prompted for various tasks, including embodied planning and acting~\cite{ahn2022can, du2023improving, wang2023voyager}, question answering or dialogue~\cite{bubeck2023sparks}, and general problem-solving~\cite{brown2020language, bubeck2023sparks}. Their unique capabilities in planning~\cite{ahn2022can}, reasoning~\cite{shinn2024reflexion}, and problem-solving~\cite{bubeck2023sparks} make them a promising candidate for incorporating prior knowledge and in-context reasoning into game-based problem-solving, particularly in addressing the aforementioned limitations of RL. 

%However, scaling these capabilities to decentralized multi-agent settings, where cooperation and communication are critical, remains a significant challenge. In multi-agent environments, communication is often costly, requiring agents to communicate selectively and strategically~\cite{guo2024large, zhang2023building}. Addressing this challenge is essential to extend the success of LLMs from single-agent to multi-agent systems and to enable effective cooperation and coordination in decentralized environments.

%Large language models (LLMs)~\cite{brown2020language} have demonstrated remarkable success across a variety of tasks, including natural language understanding, dialogue generation, complex reasoning~\cite{liang2022holistic, achiam2023gpt, touvron2023llama}, and embodied planning and acting~\cite{ahn2022can, du2023improving, wang2023voyager}.\textcolor{blue}{Due to their natural language abilities, their} capabilities in planning~\cite{ahn2022can}, reasoning~\cite{shinn2024reflexion}, and problem-solving~\cite{bubeck2023sparks} make them promising candidates for incorporating prior knowledge and in-context reasoning into game-based \textcolor{blue}{open-world} problem-solving, particularly in addressing the limitations of RL.} While LLMs have shown promise in single-agent tasks using zero-shot and few-shot prompting~\cite{huang2022language, song2023llm}, scaling these capabilities to decentralized multi-agent settings, \textcolor{blue}{such as jointly exploring and performing tasks in an open-world game still possesses several challenges:} %where cooperation is important and communication is limited, remains a significant challenge
%a) In multi-agent environments, communication is often costly, requiring agents to communicate selectively and strategically~\cite{guo2024large, zhang2023building},
%\textcolor{blue}{b) How can we effectively fuse multi-modal partially observable or potentially duplicated information in open-world games c) Agent states and observations are not only multi-modal, but also dynamic, d) How can LLM agents cooperate across tasks with complex dependencies?
%In multi-agent environments, communication is often costly, requiring agents to communicate selectively and strategically~\cite{guo2024large, zhang2023building}. 
%Addressing this challenge is essential to extend the success of LLMs from single-agent to multi-agent systems and to enable effective cooperation and coordination in decentralized environments.

% \carlee{Start this paragraph by giving a one- or two-sentence motivation about why LLMs can solve existing multi-agent decision-making challenges, e.g., ``Much of the complexity in MARL-based solutions comes from the vast amount of information that needs to be encapsulated by the policy, which is generally trained from scratch. Recently, large language models (LLMs) have demonstrated the ability to encapsulate some of this information through their ability to communicate in natural language, e.g., with impressive performance on tasks like...''}

Many challenges in MARL-based solutions stem from the vast amount of information agents must encapsulate in their learned policies, which are typically trained from scratch. Recently, large language models (LLMs)~\cite{brown2020language} have emerged as a potential solution, achieving impressive performance in dialogue generation, complex reasoning~\cite{liang2022holistic, achiam2023gpt, touvron2023llama,warner2023utilizing}, and embodied planning~\cite{ahn2022can, du2023improving, wang2023voyager}. Their natural interpretability, due to working in natural language, along with strengths in planning~\cite{ahn2022can}, reasoning~\cite{shinn2024reflexion}, and problem-solving~\cite{bubeck2023sparks}, suggests they can incorporate prior knowledge and in-context reasoning into open-world problem-solving, potentially overcoming RL limitations. Yet, while LLMs excel in single-agent tasks via zero-shot and few-shot prompting~\cite{huang2022language, song2023llm}, scaling to decentralized multi-agent settings introduces new challenges, particularly in open-world games where cooperation and limited communication are critical. These include: a) fusing partially observable or duplicated information across agents; b) handling dynamic, multi-modal agent states and observations in evolving environments; c) enabling selective and strategic communication in costly multi-agent settings~\cite{guo2024large, zhang2023building}; and d) fostering cooperation across dependent tasks, e.g., accomplishing multiple sub-goals to achieve a long-term and shared objective.
% tasks with complex dependencies \textcolor{blue}{(e.g. hierarchical item production systems.)}
% \carlee{I like this set of challenges; we might want to expand on and highlight them and then structure our contributions around solving them.}

%Recent advancements in large language models (LLMs) have demonstrated remarkable capabilities across various tasks, including natural language understanding, dialogue generation, and complex reasoning~\cite{liang2022holistic, achiam2023gpt, touvron2023llama, brown2020language}. LLMs have shown promise in single-agent tasks using zero-shot and few-shot prompting~\cite{huang2022language, song2023llm}; however, scaling these capabilities to decentralized multi-agent settings, where cooperation and communication are critical, remains a significant challenge. In multi-agent environments, communication is often costly, requiring agents to communicate selectively and strategically~\cite{guo2024large, zhang2023building}. Addressing this challenge is essential to extend the success of LLMs from single-agent to multi-agent systems and to enable effective cooperation and coordination in decentralized environments.



%Recent advancements in Large Language Models (LLMs) have shown remarkable capabilities across a variety of tasks, including natural language understanding, dialogue generation, and complex reasoning \cite{liang2022holistic, achiam2023gpt, touvron2023llama, brown2020language}. While LLMs have been successfully applied to single-agent tasks using zero-shot and few-shot prompting \cite{huang2022language, song2023llm}, scaling these capabilities to decentralized multi-agent settings, where cooperation and communication are critical, remains a challenge. In such settings, communication is costly, making it essential for agents to communicate selectively and strategically \cite{guo2024large, zhang2023building}.




In this work, we propose Decentralized Adaptive Knowledge Graph Memory and Structured Communication System (\textbf{DAMCS}) in a novel \textbf{Multi-agent Crafter} environment to address these fundamental challenges that enable agents to collaborate, and dynamically adapt in complex, open-ended environments like open-world survival games.
% \textcolor{blue}{\cite{wu2024spring, li2024optimus, wang2023voyager} involve single agent systems in open-world games. 
% We present a novel framework, harnessing the interaction of multiple LLM-agents to tackle multiple objectives with dependencies in open-world exploration:
%}
\textbf{DAMCS} is a novel decentralized multi-agent framework that leverages LLMs, featuring two key components: an Adaptive Knowledge Graph Memory System (\textbf{A-KGMS})and a Structured Communication System (\textbf{S-CS}) to enhance agent cooperation. 


\textbf{Adaptive Knowledge Graph Memory System (A-KGMS)} effectively fuses information across agents in dynamic environments, thus addressing the first two challenges of multi-agent LLMs above. Unlike LLM-based agents like SPRING, which uses external knowledge for planning but rarely learns from in-environment interactions \cite{wu2024spring}, A-KGMS enables agents to interact and learn from each other's experiences, through a hierarchical adaptive knowledge graph. Each agent maintains and dynamically updates its own graph-based in part on messages from other agents.
A-KGMS integrates sensory, episodic, and procedural memory, enabling agents to perceive cues and retrieve relevant experiences for contextual knowledge. Through facilitating interaction between the joint long-term and short-term memory across agents, agents can efficiently learn, retrieve, and generalize task-relevant knowledge by categorizing and linking experiences. This system allows agents to cooperate across a variety of tasks, adapt to dynamic environments, and handle partial observations while minimizing unnecessary communication. 

\textbf{Structured Communication System (S-CS)} ensures that agents exchange only the most relevant information, unlike free-form natural language approaches such as CoELA~\cite{zhang2023building}. Agents share key data—such as their current status, resources, and observations—with specific target agents that they aim to assist, following a structured schema to ensure clarity and ease of interpretation. This structured communication focuses on task-relevant information, such as resource availability or task progress, enabling agents to coordinate efficiently across diverse tasks. By minimizing unnecessary data exchange, agents can better align their actions, optimize resource-sharing, and ensure smooth cooperation for achieving shared goals. This protocol fosters collaboration while reducing communication overhead, particularly in complex, dynamic environments.










%Our two key technical innovations are an \textbf{Adaptive Knowledge Graph Memory} system \carlee{we should name this...} and \textbf{structured communication protocol} for agent cooperation. \carlee{When making decisions, agents use their memory systems and information communicated from other agents to craft a LLM prompt, and the LLM can then recommend an action (decision).} 

%Our \textit{memory system} effectively fuses information across agents in dynamic environments, thus addressing the first two challenges of multi-agent LLMs above. \textcolor{blue}{Unlike LLM-based agents like SPRING which use external knowledge for planning but rarely learn from in-environment interactions \cite{wu2024spring}, our approach enables agents to interact and learn from each other's experiences, through a \textit{hierarchical adaptive knowledge graph}. Each agent maintains and dynamically updates its own graph based in part on messages from other agents.} 
%The Adaptive Knowledge Graph Memory integrates sensory, episodic, and procedural memory, enabling agents to perceive cues and retrieve relevant experiences for contextual knowledge. % \carlee{maybe discuss how this knowledge graph is shared across agents}
%
% By combining generative models with decentralized control, our \textcolor{blue}{novel framework} supports both high-level planning and real-time decision-making.
%Through facilitating interaction between \textcolor{blue}{the joint} long-term and short-term memory \textcolor{blue}{across agents}, agents can efficiently learn, retrieve, and generalize task-relevant knowledge by categorizing and linking experiences. % This graph helps agents to reason about task dependencies, consequences of actions, and long-term goals, enhancing both communication and planning. 
%This system allows agents to cooperate across a variety of tasks, adapt to dynamic environments, and handle partial observations while minimizing unnecessary communication. 



%%%%%COMM module




%Our \textit{communication protocol} %enables low-cost, life-long learning 
% through \textbf{structured communication}, 
%allows agents to exchange only the most relevant information, %and reducing communication overhead, 
%unlike free-form natural language approaches like CoELA~\cite{zhang2023building}. Agents share their current status and observations with certain target agents that they wish to help, according to a structured schema for ease of interpretability.
%\carlee{Agents share their current status and observations with certain target agents that they wish to help, according to a structured schema for ease of interpretability.} 
%\carlee{talk more about what is communicated here and how it enables cooperation across dependent tasks}
% \carlee{This can be restructured a bit to explain which parts are new and which parts address the challenges we list above. It may be good to briefly explain existing multi-agent LLM work too}

% \textcolor{blue}{[I restructured the above 2 paragraphs, to reduce duplicated information. feel free to further edit]}
%We also integrate an \textbf{adaptive knowledge graph}, which allows agents to reason about task dependencies, consequences of actions, and long-term goals, enhancing both communication and strategic planning. 
%The Adaptive Knowledge Graph Memory integrates sensory, episodic, and procedural memory, enabling agents to perceive cues and retrieve relevant experiences for contextual knowledge.
%By combining generative models with decentralized control, our system supports both high-level planning and real-time decision-making. %This approach optimizes task execution while minimizing communication overhead. 
% \carlee{emphasize in this paragraph that these ideas are novel}

%To enable low-cost, life-long learning communication between agents, our framework employs \textbf{structured communication}, ensuring that agents exchange only the most relevant information, reducing communication overhead. In contrast, approaches like CoELA focus on free-form natural language communication \cite{zhang2023building}. Additionally, we integrate planning mechanisms with an \textbf{adaptive knowledge graph}, enabling agents to reason about task dependencies, and hence the consequences of their actions and long-term goals—going beyond simple communication. %Our approach emphasizes task-specific optimizations, such as resource-sharing mechanisms and task-dependency tracking, with a focus on communication for strategic planning and understanding action consequences. 
%This combination of communication, strategic planning, and reasoning about the impact of actions leads to more efficient collaboration and task execution in real-time environments.

%The key insight is that our framework integrates generative models with decentralized control, enabling agents to perform both high-level planning and real-time decision-making. The Adaptive Knowledge Graph Memory system incorporates sensory, episodic, and procedural memory, allowing agents to perceive environmental cues and retrieve relevant past experiences to form contextual knowledge. By strategically managing communication and focusing on long-term collaborative goals, our agents optimize task completion and reduce communication overhead.


%%%%%%%%%%%%%%evaluation jingdi
%To evaluate our framework, we are the first to extend the \textit{Crafter} environment~\cite{hafner2021benchmarking} to a multi-agent setting, creating a valuable platform for testing LLM models on MARL tasks. Originally designed for single-agent benchmarking, Crafter’s procedurally generated world—with its complex resource dependencies and open-ended tasks—now enables the study of multi-agent collaboration, communication, and long-term planning. Figure~\ref{fig:crafter} illustrates our multi-agent Crafter environment, where agents can interact using natural language while navigating the visual environment to achieve both long- and short-term goals.

%Crafter strikes a balance between complex environments like Minecraft, which are often too challenging for existing RL-based agents, and simpler games like Atari, which lack the depth of combined short- and long-term objectives. By extending Crafter to multiple agents, we provide an ideal setting to evaluate core tasks such as collaboration and long-term planning in a dynamic, procedurally generated environment.


To evaluate our framework, we develop \textbf{Multi-Agent Crafter (MAC)} extended form environments~\cite{hafner2021benchmarking}, providing a new platform for testing LLM models on MARL tasks. 
%Multi-Agent Crafter’s procedurally generated world, with complex resource dependencies and open-ended tasks, allows for studying multi-agent collaboration, communication, and long-term planning. 
The framework is different from Multi-agent systems such as environments that simulate group tasks like mining. Our proposed MAC addresses specific gaps that remain underserved by current developed benchmarks. It is designed to balance accessibility and complexity, providing a computationally efficient yet challenging testbed suitable for labs with limited resources. Unlike Minecraft~\cite{fan2022minedojo} which is highly complex and resource-intensive, MAC is streamlined while still requiring significant long-term planning and collaboration.
Compared to other benchmarks like SMAC~\cite{samvelyan2019starcraftmultiagentchallenge}, which primarily emphasize micro-management in scenarios, MAC focuses on macro-management and challenges like hierarchical task completion, resource sharing, and communication. This includes the need for agents to plan intermediate goals over extended time horizons, which SMAC does not emphasize.
Additionally, MAC builds on recent efforts such as Craftax~\cite{matthews2024craftaxlightningfastbenchmarkopenended}, leveraging improvements in runtime performance and task complexity while tailoring the environment for the multi-agent community. By focusing on collaboration and communication dynamics in multi-agent systems, it provides unique value as a benchmark for both MARL and LLM-based approaches. 
Figure~\ref{fig:crafter} shows an example where agents interact using natural language to complete both short- and long-term goals. MAC offers a balanced testbed, bridging the gap between overly complex environments like Minecraft and simpler games like Atari, enabling focused evaluation of cooperation and planning in dynamic settings. We will release the codebase of MAC publicly to provide a testbed for other works to evaluate real-time cooperative multi-agent scenarios involving LLMs.
Our key \textbf{contributions} are as follows:
\begin{itemize}
    \item We propose a Decentralized Adaptive Knowledge Graph Memory and Structured Communication System (\textbf{DAMCS}), which is a decentralized cooperative framework for generative agents. This decentralized multi-agent framework leverages LLMs and an Adaptive Knowledge Graph Memory System (A-KGMS), enabling agents to plan, cooperate, and dynamically adapt in open-ended, complex environments.
    
    \item We propose a Structured Communication System (S-CS) that optimizes the exchange of relevant information among agents, minimizing unnecessary communication overhead, in contrast to free-form language approaches.
    
%    \item Our framework integrates \textbf{planning mechanisms} with an adaptive knowledge graph, allowing agents to reason about task dependencies, long-term goals, and the consequences of their actions, which enhances task-specific optimizations such as resource-sharing and task-dependency tracking. \carlee{should this go before the communication strategy?}
    
    \item We develop the Multi-Agent Crafter for MARL tasks, making it an ideal testbed for evaluating decentralized cooperation, communication, and long-term planning in real-time, multi-step tasks. Evaluation results show that \textbf{DAMCS} outperforms both MARL and LLM baselines.
    
    % \item Our approach combines efficient communication, strategic planning, and reasoning about action consequences, resulting in improved collaboration and task execution in dynamic, real-time environments.
\end{itemize}
In Section~\ref{sec:related}, we provide a brief overview of related work. Next, in Section~\ref{sec:method}, we present our \textbf{DAMCS} framework, detailing its memory system, planning mechanism, and communication protocol. In Section~\ref{sec:crafter}, we describe how we develop Multi-agent Crafter for MARL tasks. We then evaluate \textbf{DAMCS} on Multi-agent Crafter in Section~\ref{sec:evaluation} and conclude our findings in Section~\ref{sec:conclusion}.




%After giving a brief overview of related work in Section~\ref{sec:related}, we outline our \framework~framework, including our memory system, planning mechanism, and communication protocol in Section~\ref{sec:method}. We then describe our extension of Crafter to a multi-agent environment in Section~\ref{sec:crafter}, evaluate \framework~on this environment in Section~\ref{sec:evaluation}, and conclude in Section~\ref{sec:conclusion}.
