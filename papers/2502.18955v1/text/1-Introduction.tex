\section{Introduction}

Offline reinforcement learning (RL)~\citep{levine2020offline} has marked a paradigm shift in artificial intelligence. Unlike traditional RL~\citep{sutton2018reinforcement} that relies on real-time interaction with the environment, offline RL utilizes pre-collected datasets to learn decision-making policies~\citep{yang2021believe,janner2021offline}. This approach is increasingly favored for its practicality in scenarios where real-time data acquisition is impractical or could damage physical assets. Moreover, offline learning can avoid the significant time and complexity involved in online sampling and environment construction. This streamlines the learning process and expands the potential for deploying RL across a more comprehensive array of applications~\citep{yuan2022offline, zhou2023real, nambiar2023deep}.

% However, in real-world applications, we often need to deploy offline datasets to low-power devices, such as micro-robot~\cite{jahangir2016persistence, fioranelli2015classification}.
% Reduced datasets can enable learning on relatively low-resource computational environments without requiring a large number of GPU and CPU servers. % (Data subset selection enables efficient learning at multiple levels. First, by using a subset of a large dataset, we can enable learning on relatively low resource computational environments without requiring a large number of GPU and CPU servers. 

However, offline reinforcement learning relies on large pre-collected datasets, which can result in substantial computational costs during policy learning~\citep{lu2022challenges}, especially when the algorithm model requires extensive parameter tuning~\citep{sharir2020cost}.
Moreover, additional data may not always improve performance, as suboptimal data can exacerbate the distribution shift problem, potentially degrading the policy~\citep{hu2022role}.
In this work, we attempt to explore effective offline reinforcement learning methods through a data subset selection mechanism and address the following question:

\begin{center}
    \it{How do we determine the subset of the offline dataset to improve algorithm performance and accelerate algorithm training?}
\end{center}

% Therefore, a critical yet under-investigated aspect of offline RL is determining the subset of the offline dataset while ensuring superior performance for offline RL.

% Despite its growing adoption, a critical but yet under-investigated aspect of offline RL is determining the optimal size of the offline dataset necessary for efficient policy training. 
% Currently, the majority of research in this field uses very large datasets comprising millions of data points, like the D4RL datasets~\cite{fu2020d4rl}. These large datasets have been crucial in driving many significant research breakthroughs~\cite{kumar2020conservative,kidambi2020morel,kostrikov2021offline,zheng2023semi,rosete2023latent}. However, existing approaches largely leave untouched the scenario where generating or accessing such extensive datasets is not feasible. This gap brings us to a key question: What is the minimum dataset size that can still ensure training performance for offline RL? 

% and establishing high-performance benchmarks
%By addressing this question, we will ease the training demands on current large-scale offline datasets and offer essential support for determining the sample size in the development of offline datasets for new settings.

% This paper studies this question. 
In this paper, we formulate the dataset selection challenge as a gradient approximation optimization problem. 
The underlying rationale is that if the weighted gradients of the TD loss on the reduced dataset can closely approximate those on the original dataset, the dataset reduction process should not lead to significant performance degradation.
However, directly solving this data selection problem is NP-Hard~\citep{killamsetty2021glister,killamsetty2021retrieve}.
To this end, we first prove that the common actor-critic framework can be transformed into a submodular optimization problem~\citep{mirzasoleiman2020coresets}.
Based on this insight, we adopt the Orthogonal Matching Pursuit~(OMP)~\citep{elenberg2018restricted} to solve the data selection problem.
On the other hand, different from supervised learning, target values in offline RL evolve with policy updates, resulting in unstable gradients that affect the quality of the selected data subset.
To solve this issue, we stabilize the learning process by making several essential modifications to the OMP.

% We frame gradient approximation as a nested optimization problem. The outer layer of this problem focuses on determining the size of the reduced dataset, while the inner one aims to identify a data subset with a given size that minimizes the accumulated gradient difference compared to the original dataset.

% Directly solving this two-level optimization problem is at least NP-Hard~\cite{killamsetty2021glister,killamsetty2021retrieve}, and we propose a method that finds solutions with provable approximation bounds by unveiling a subtle yet important link between the inner and outer optimization problem.

% To be specific, the inner problem can be solved by extending the orthogonal matching pursuit (OMP) algorithm to offline RL. OMP is an iterative method that selects a new sample as a basis vector per iteration so that the combination of basis vectors can minimize a specified objective. However, this objective must be submodular~\cite{iyer2021submodular}. To meet this requirement, we first construct an objective that is provably submodular and equivalent to minimizing the gradient difference compared to the original dataset. 

Theoretically, we provide a comprehensive analysis of the convergence properties of our algorithm and establish an approximation bound for its solutions. 
We then prove the objective function can be upper-bounded if the selected data is sufficiently diverse.
Empirically, we evaluate \name~on the D4RL benchmark~\citep{fu2020d4rl}. Comparison against various baselines and ablations shows that the data subsets constructed by the \name~can significantly improve algorithm performance with low computationally expensive.
To the best of our knowledge, our work is the first study analyzing the reduced dataset in offline reinforcement learning.

% This insight allows us to run clustering algorithms with an approximation guarantee like $k$-means++~\cite{makarychev2020improved} to get a good initial solution for the outer problem and proceed by solving the inner problem via selecting data within each cluster through an enhanced OMP method initialized with the cluster center. 
% The OMP is specifically optimized for offline RL and addresses unique challenges in this setting such as learning stability and time efficiency.

% In addition, we delve into a comprehensive analysis of the convergence properties exhibited by our algorithm to further demonstrate reliability of our study.
%enhance the depth of our study.

% When used with $k$-means++, the gradient approximation error achieved by our method is at most $5(\ln K+2)$ times the error of the optimal solution, where $K$ is the number of clusters. This analysis provides a theoretical foundation for understanding the efficacy of the data subsets selected by our method.


% Visualization of the selected data reveals the rationale of \name~data reduction. The embedding of the complete dataset typically consists of different components, each representing a separate phase or skill required for task completion. The data selected by \name~not only covers these components but also interlinks them, thereby ensuring the task's complete and coherent execution (see Figure~\ref{fig: t-sne}). 

% Moreover, our research findings clear the path for the advancement of offline reinforcement learning techniques into real-world settings. Academic datasets, such as those provided by D4RL, typically encompass millions of samples. However, acquiring such extensive data for practical applications is a labor-intensive process. Our compression experiments reveal that the dataset size necessary for training varies with the complexity of the task. Remarkably, a task of moderate difficulty, akin to the hopper scenario, can be effectively trained with just 3$\%$ of the dataâ€”amounting to roughly 30,000 samples. This discovery bolstering the transition of offline reinforcement learning from academic research to practical implementations, inspiring greater confidence in its potential for real-world adoption.




% In general, we ease the training demands on current large-scale offline datasets and offer essential support for determining the sample size in the development of offline datasets for new settings.


\section{Related Works}
\textbf{Offline Reinforcement Learning.}\ \
% Offline RL can execute policy training entirely based on static datasets without further interaction with the environment~\citep{levine2020offline}.
% Therefore, it faces challenges such as distribution shift and value overestimation.
Current offline RL methods attempted to constrain the learned policy and behavior policy by limiting the action difference~\citep{fujimoto2019off}, adding KL-divergence~\citep{yang2021believe, nair2020awac,peng2019advantage,wu2019behavior}, regularization~\citep{kumar2019stabilizing}, conservative estimates~\citep{yang2023flow,ma2021offline, kumar2020conservative,ma2021conservative} or penalizing uncertain actions~\citep{janner2019trust,yu2021combo,kidambi2020morel}.
% There are also some new attempts, such as lightweight implementation~\citep{fujimoto2021minimalist} or avoiding distribution shift by single-step policy iteration~\citep{kostrikov2021offline}.
These studies provide a solid foundation for implementing and transferring reinforcement learning to real-world tasks.
% Differently, our work aims to figure out the ideal size of the dataset needed for effective policy training.

\textbf{Offline Dataset.}
Some works attempted to explore which dataset characteristics dominate in offline RL algorithms~\citep{schweighofer2021understanding, swazinna2021measuring, chen2020bail, yue2022boosting} or investigate the data generation~\citep{yarats2022don}.
Recently, some researchers attempted to solve the sub-optimal trajectories issue by constraining policy to good data rather than all actions in the dataset~\citep{hong2023beyond} or re-weighting policy~\citep{hong2023harnessing}.
However, limited research has addressed considerations related to the reduced dataset in offline RL.

\textbf{Data Subset Selection.}\ \ The research on identifying crucial samples within datasets is concentrated on supervised learning.
Some prior works use uncertainty of samples~\citep{coleman2019selection,paul2021deep} or the frequency of being forgotten~\citep{toneva2018empirical} as the proxy function to prune the dataset.
Another research line focuses on constructing weighted data subsets to approximate the full dataset~\citep{feldman2020core}, which often transforms the subset selecting to the submodular set cover problem~\citep{wei2015submodularity,kaushal2019learning}.
% Specifically, several works adopted loss functions as the optimization target~\citep{lucic2018training,campbell2018bayesian}, while recent research finds that approximating full gradient is more efficient~\citep{mirzasoleiman2020coresets, killamsetty2021grad,killamsetty2021glister,killamsetty2021retrieve}.
These studies establish the importance of selecting critical samples from datasets for practical training. 
However, unlike supervised learning, target values in offline RL evolve as policies update, leading to unstable gradients that significantly complicate the learning process.

% However, the different learning objectives and training methodologies between reinforcement learning and computer vision mean that these techniques cannot be directly applied to Offline RL.