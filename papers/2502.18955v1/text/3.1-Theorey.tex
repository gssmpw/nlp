\section{Theoretical Analysis}\label{sec:theory}
% Recall that the proposed \name~learning framework aims to find Q-function parameters $\theta$ that can closely approximate the gradients of Q-function evaluated on 

In this section, we study the convergence property of our method and the error bounds of the solutions it finds. We work with mild assumptions that the gradient of the TD loss is Lipschitz smooth with constant $L$: $\|\nabla \mathcal L(\theta') - \nabla \mathcal L(\theta)\| \leq L\|\theta' - \theta\|$, and that the gradient is bounded by $\sigma$: $\| \nabla \mathcal L(\theta) \| \leq \sigma$.

% Let $L_{T}(\theta_{t}; \mathcal{D})$ denote the training 
% \begin{align}
%     L_{T}(\theta_{t}; \mathcal{D}) = 
%     \mathcal L(\theta)=\sum_{i \in \mathcal{D}}\mathcal L^i(\theta) = \sum_{i \in \mathcal{D}}\mathcal L_{\mathtt{TD}}(s_{i}, a_{i}, r_i, s'_{i}, \theta)
% \end{align}
% loss on the original dataset $\mathcal{D}$.
% Then, we suppose the training loss $L_T$ and 
%  and 
% Similarly, we let $L_{T}(\theta_{t}; \mathcal{S})$ denote the training loss on the reduced subset $\mathcal{S}$ with the same smooth assumption.
% \begin{align}
% \mathcal L_{\rdcshort}(\vw,\theta) = \sum\nolimits_{i \in \mathcal{S}} w_i\mathcal L^i(\theta)
% \end{align}
% Let $\Theta_t$ be the angle between $\nabla_{\theta} L_{T}(\theta_t; \mathcal{D})$ and $\nabla_{\theta} L_{T}(\theta_{t}; \mathcal{S})$.
% The cosine similarity of $\Theta_t$ is $\cos\Theta_t = \frac{\nabla_{\theta} L_{T}(\theta_t; \mathcal{D})^T\nabla_{\theta} L_{T}(\theta_{t}; \mathcal{S})}{\|\nabla_{\theta} L_{T}(\theta_t; \mathcal{S})\|\|\nabla_{\theta} L_{T}(\theta_{t}; \mathcal{S})\|}$.
Firstly, we show that the TD loss of the offline Q function $Q^{\pi_\mathcal{S}}$ trained on the reduced dataset $\mathcal{S}$ can converge.
\begin{restatable}{theorem}{convergence}\label{thm:convergence}
    \label{thm:convergence}
    Let $\theta^*$ denote the optimal $Q^{\pi_\mathcal{S}}$ parameters, $\theta_t$ the parameters after $t$ training steps. We have
    \begin{align}
        \min_{t=1:G}\mathcal{L}(\theta_t)\leq \mathcal{L}(\theta^*) + \frac{D\sigma}{\sqrt{G}} + \frac{D}{G}\sum_{t=1}^{G-1}\varepsilon.
    \end{align}
    Here 
    $\mathcal{L}(\theta)=\sum_{i \in \mathcal{D}}\mathcal L_{\mathtt{TD}}(s_{i}, a_{i}, r_i, s'_{i}, \theta)$ is the TD loss, $G$ is the number of total training steps, $D=\|\theta^*-\theta_t\|$, and $\varepsilon=\operatorname{Err}\left(\vw, \mathcal{S}, \mathcal L, \theta_t\right)$ is the gradient approximation errors.
\end{restatable}
\begin{proof}
    Please refer to Appendix~\ref{appendix: convergence} for detailed proof. 
\end{proof}

We assume the gradients of selected data are diverse and they can be divided into $K$ clusters $\{\mathcal{C}_1,\cdots,\mathcal{C}_K\}$ with the cluster centers set $\mathcal C=\{c_1,\cdots,c_K\}$.
Then, we prove the residual error $\operatorname{Err}\left(\vw, \mathcal{S}, \mathcal L, \theta\right)$ can be upper bounded:

% The cornerstone of this approach, i.e., the relationship between the dataset selection problem and the clustering problem, is shown in the following theorem.
\begin{restatable}{theorem}{cluster}\label{thm:cluster}
% \begin{theorem}\label{thm:cluster}
The residual error $\operatorname{Err}\left(\vw, \mathcal{S}, \mathcal L, \theta\right)$ is upper bounded according to the sample's gradient of TD loss:
\begin{align}
    \min_{\mathcal C}\sum_{i\in\mathcal D} \min_{c\in \mathcal C}\|\nabla_{\theta} \mathcal L^i\left(\theta\right) - \nabla_{\theta} \mathcal L^c\left(\theta\right) \|_2. 
\end{align}
% \end{theorem}
\end{restatable}
\begin{proof}
Please refer to Appendix~\ref{appendix: cluster theory} for detailed proof.
\end{proof}

We then prove that the reduced dataset selected by our method can achieve a good approximation for the gradient calculated on the complete dataset, which also means $\varepsilon=\operatorname{Err}\left(\vw, \mathcal{S}, \mathcal L, \theta_t\right)$ in Theorem~\ref{thm:convergence} is bounded.

\begin{corollary}[Approximation Error Bound of the Reduced Dataset]\label{thm:c_bound}
    The expected gradient approximation error achieved by our method is at most $5(\ln K+2)$ times the error of the optimal solution $\mathcal{S}^*$:
    \begin{align}
        \operatorname{Err}\left(\vw, \mathcal{S}, \mathcal L, \theta\right) \le 5(\ln K+2)\operatorname{Err}\left(\vw, \mathcal{S}^*, \mathcal L, \theta\right).
    \end{align}
\end{corollary}
\begin{proof}
The proof is derived by applying Theorem~\ref{thm:cluster} along with Theorem 4.3 from~\citep{makarychev2020improved}, by observing that cluster centers are included in the reduced dataset. 
% This gradient approximation error bound ensures that our reduced dataset will not result in significant performance degradation.
\end{proof}
% \vspace{-1cm}

\paragraph{Discussion}
{The aforementioned theoretical analysis has the following limitations: 
First, we assume that the gradients are uniformly bounded.
Therefore, if the gradients of the algorithm diverge in practice, it would contradict our assumptions, and the selected data subset would no longer be valuable. 
Current offline RL methods can only ensure that the Q-values do not diverge~\cite{kumar2020conservative, fujimoto2021minimalist}.
Although this can, to some extent, reflect the gradients of the Q-network that have not diverged, there is no rigorous proof that the bounds of the gradients can be guaranteed. 
Second, the above theoretical analysis is based on the classic TD loss.
However, to provide a consistent learning signal and mitigate instability caused by changing target value, the techniques in Section~\ref{sec:method:outer} adopt a fixed target rather than TD loss.}
