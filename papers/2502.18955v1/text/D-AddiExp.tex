\clearpage
\section{Ablation Study}
\label{sec: ablation}
To study the contribution of each component in our learning framework, we conduct the following ablation study. 
\nameq: We replace the empirical returns used to update Q functions with the standard target Q function in the TD loss function. 
\namei: We set the number of data selection rounds to 1 and study the function of multi-round data selection.
The experimental results in Figure~\ref{fig: modular ablation}show that removing any of these two modules will worsen the performance of \name. In case like $\texttt{walker2d-medium}$, ablation \namei~even decrease the performance by over 80\%, and ablation \nameq~results in a 95\% performance drop in $\texttt{walker2d-expert}$. Furthermore, we also find that in the $\texttt{halfcheetah}$ tasks, the impact of removing the two modules is relatively small. This result can be attributable to the fact that this task has a limited state space, and we can directly apply OMP to the entire dataset and identify important and diverse data.

\begin{figure}[H]
    \centering
    \subfigure{\includegraphics[scale=0.27]{ablation_moduler1.pdf}}
    \hspace{0.3cm}\subfigure{\includegraphics[scale=0.27]{ablation_moduler2.pdf}}
    \caption{Ablation results on D4RL~(Hard) tasks with the normalized score metric.}
    \label{fig: modular ablation}
\end{figure}


\section{Computational Complexity}
\label{appendix: computation complexity}
We report the computational overhead of \name~on various datasets. 
All experiments are conducted on the same computational device (GeForce RTX 3090 GPU). 
The results in the following Table indicate that even on datasets containing millions of data points, the computational overhead remains low. 
This low computational complexity can be attributed to the trajectory-based selection technique in Sec.~\ref{sec: offline omp}~(II) and the regularized constraint technique in Sec.~\ref{sec:method:outer}, making our method easily scalable to large-scale datasets. 

\begin{table*}[h]
    \centering
    \begin{tabular}{c|cc}
    \toprule
    Env & Data Number & \name \\
    \midrule
    Hopper-medium-v0 & 999981 & 8m \\
    Walker2d-medium-v0 & 999874 & 8m \\
    Halfcheetah-medium-v0 & 998999 & 8m \\
    Hopper-expert-v0 & 999034 & 8m \\
    Walker2d-expert-v0 & 999304 & 8m \\
    Halfcheetah-expert-v0 & 998999 & 8m\\
    Hopper-medium-expert-v0 & 1199953 & 8m\\
    Walker2d-medium-expert-v0 & 1999179  & 13m\\
    Halfcheetah-medium-expert-v0 & 1997998 & 14m\\
    Hopper-medium-replay-v0 & 200918 & 3m\\
    Walker2d-medium-replay-v0 & 100929  & 3m\\
    Halfcheetah-medium-replay-v0 & 100899 & 3m\\
    \bottomrule
    \end{tabular}
    \label{tab: cc}
    \caption{The computational complexity associated with \name~in various datasets. $m$ represents minutes.} 
\end{table*}

% \subsection{Trajectory-based selection}
% \label{appendix: trajectory}

% Experimental results in Figure~\ref{fig: d4rl topbc} show that \name~maintains its superiority in this setting with suboptimal (e.g., \texttt{medium}) datasets. This evidence suggests that \name~provides a valuable strategy for selecting data conducive to effective training under conditions of compromised data quality.

% \subsection{Generalizability of \name~ to other domains}
% \label{appendix: other domain}
% We evaluate our algorithm on robotic manipulation (Adroit) and sparse reward (Antmaze) tasks. 
% The experimental results in Table~\ref{tab: other domain} indicate that in sparse reward tasks, \name~ achieves comparable performance close to that on the full dataset with only 20\% of the data. In the robotic manipulation tasks, \name~ requires even less data.

% \begin{table}[h]
%     \centering
%     \caption{Experimental results of \name~ with various dataset sizes ($x\%$) in Antmaze and Adroit tasks. 
%     Highlighted is the performance comparable to training TD3+BC with the complete dataset. } 
%     \label{tab: other domain}
%     \begin{tabular}{c|cccc}
%     \toprule
%         Env & 10\% & 20\% & 30\% & All Data \\
%         \midrule
%         Antmaze-umaze-v0 & 70.2$\pm$3.6 & 75.1$\pm$2.5 & 84.7$\pm$3.3 & 87.5$\pm$1.3 \\
%         Antmaze-umaze-diverse-v0 & 44.7$\pm$2.7 & 46.3$\pm$1.9 & 47.7$\pm$2.2 & 62.2$\pm$2.0 \\
%         Antmaze-medium-play-v0 & 2.1$\pm$1.3 & 59.3$\pm$1.6 & 60.3$\pm$2.9 &	71.2$\pm$2.2 \\
%         Antmaze-medium-diverse-v0 &	7.3$\pm$3.1 & 43.6$\pm$2.7 & 64.9$\pm$3.8 & 70.0$\pm$1.6 \\
%         % Antmaze-large-play-v0 &	2.0$\pm$0.5 & 3.7$\pm$0.7 & 16.0$\pm$3.5 & 39.6$\pm$3.6 \\
%         % Antmaze-large-diverse-v0 & 2.4$\pm$1.0 & 16.0$\pm$3.6 & 20.5$\pm$3.7 & 47.5$\pm$1.1 \\
%         Pen-expert-v0 & 121.8$\pm$1.6 & 121.5$\pm$1.0 & 119.3$\pm$2.2 & 136.7$\pm$2.5 \\
%         Hammer-expert-v0 & 127.0$\pm$1.3 & 126.9$\pm$1.6 & 119.4$\pm$2.1 & 121.5$\pm$1.1 \\
%         Relocate-expert-v0 & 103.5$\pm$3.7 & 106.8$\pm$2.7 & 106.3$\pm$2.3 & 108.8$\pm$3.5\\ 
%         Door-expert-v0 & 105.1$\pm$2.7 & 105.2$\pm$1.9 & 105.4$\pm$3.6 & 106.3$\pm$2.9\\
%     \bottomrule
%     \end{tabular}
% \end{table}

% \clearpage
% \subsection{Generalizability of \name~ to other algorithms}
% \label{appendix: other algorithm}

% We add IQL~\cite{kostrikov2021offline} as a baseline and apply \name~to IQL by using the gradient of the training loss of the V-function in IQL as the criterion. 
% The experimental results in Table~\ref{tab: other algorithm} demonstrate both \name$_{\rm IQL}$ and \name$_{\rm TD3+BC}$ can achieve performance close to Complete Dataset (Best) with a small amount of data.

% \begin{table}[h]
%     \centering
%     \caption{Experimental results of applying \name~ to IQL.} 
%     \label{tab: other algorithm}
%     \begin{tabular}{c|cccc}
%     \toprule
%     Env & \name$_{\rm IQL}$ & \name$_{\rm TD3+BC}$ & IQL~(All Data) & TD3+BC~(All Data) \\
%     \midrule
%     Hopper-medium-v0 & 91.7$\pm$1.3 & 93.3$\pm$2.5 & 98.7$\pm$1.2 & 99.5$\pm$1.0\\
%     Walker2d-medium-v0 & 63.2$\pm$2.3 & 64.3$\pm$2.2 & 70.5$\pm$1.7 & 79.7$\pm$1.8\\
%     Halfcheetah-medium-v0 & 32.5$\pm$0.7 & 33.0$\pm$0.8 & 40.2$\pm$0.5 & 42.8$\pm$0.3\\
%     Hopper-medium-expert-v0 & 106.7$\pm$0.3 & 103.0$\pm$0.5 & 112.0$\pm$1.0 & 112.2$\pm$0.2\\
%     Walker2d-medium-expert-v0 & 84.0$\pm$8.1 & 77.0$\pm$8.6 & 105.0$\pm$4.7 & 101.1$\pm$9.3\\
%     Halfcheetah-medium-expert-v0 & 78.6$\pm$3.2 & 80.5$\pm$6.0 & 92.1$\pm$4.6 & 97.9$\pm$4.4\\
%     Hopper-expert-v0 & 112.2$\pm$0.3 & 108.6$\pm$0.8 & 112.2$\pm$0.6 & 112.2$\pm$0.2\\
%     Walker2d-expert-v0 & 83.0$\pm$4.5 & 83.8$\pm$4.2 & 106.8$\pm$2.6 & 105.7$\pm$2.7\\
%     Halfcheetah-expert-v0 & 78.9$\pm$1.7 & 85.6$\pm$1.2 & 107.0$\pm$2.1 & 105.7$\pm$1.9\\
%     \bottomrule
%     \end{tabular}
% \end{table}

% \clearpage

% \subsection{Generalizability of subset selecting by~\name}
% \label{appendix: tb3bc2iql}
% To test the generalizability of the dataset selected by~\name, we select subset by applying~\name~to TD3+BC.
% Then we evaluate the performance of IQL on the selected subset. 
% The experimental results in Table~\ref{tab: td3bc2iql} demonstrate that the selected subset based on TD3+BC is effectively applicable to IQL.
%  Across all tasks, the subset size is 10\% of the entire dataset.

% \begin{table}[h]
%     \centering
%     \caption{The performance of IQL on the subset selected based on TD3+BC.}
%     \label{tab: td3bc2iql}
%     \begin{tabular}{c|cc}
%     \toprule
%     Env & \name$_{\rm TD3+BC\rightarrow IQL}$ & IQL (All Data) \\
%     \midrule
%     Hopper-medium-v0 & 88.9$\pm$8.7 & 98.7$\pm$1.2\\
%     Walker2d-medium-v0 & 59.1$\pm$6.9 & 70.5$\pm$1.7\\
%     Halfcheetah-medium-v0 & 37.0$\pm$0.1 & 40.2$\pm$0.5 \\
%     Hopper-medium-expert-v0 & 100.5$\pm$1.6 & 112.0$\pm$1.0\\
%     Walker2d-medium-expert-v0 & 82.1$\pm$4.8 & 105.0$\pm$4.7 \\
%     Halfcheetah-medium-expert-v0 & 50.4$\pm$0.1 & 92.1$\pm$4.6 \\
%     Hopper-expert-v0 & 110.9$\pm$0.6 & 112.2$\pm$0.6\\
%     Walker2d-expert-v0 & 83.5$\pm$4.2 & 106.8$\pm$2.6\\
%     Halfcheetah-expert-v0 & 92.5$\pm$1.4 & 107.0$\pm$2.1\\
%     \bottomrule
%     \end{tabular}
% \end{table}

% \clearpage
% \subsection{Ablation study for cluster number}
% \label{appendix: cluster number}
% We evaluate the performance of \name~with various cluster numbers. The experimental results in Table~\ref{tab: cluster number} show that the suitable cluster number is between 25 and 50. Too few clusters (e.g., less than 5) are detrimental to the algorithm.

% \begin{table}[h]
%     \centering
%     \caption{Ablation study with the cluster number.} 
%     \label{tab: cluster number}
%     \begin{tabular}{c|cccccc}
%     \toprule
%     Cluster Number & 1 & 5 & 15 & 25 & 50\\
%     \midrule
%     Hopper-medium-v0 & 47.6$\pm$1.6 & 81.7$\pm$3.0 & 96.2$\pm$2.0 & 99.1$\pm$3.3 & 92.6$\pm$3.0 \\
%     Walker2d-medium-v0 & 9.5$\pm$1.1 & 5.9$\pm$3.6 & 32.2$\pm$2.4 & 64.1$\pm$1.9 & 57.9$\pm$3.6 \\
%     Halfcheetah-medium-v0 & 40.4$\pm$0.2 & 41.2$\pm$0.7 & 41.3$\pm$0.4 & 41.4$\pm$0.2 & 40.9$\pm$0.1\\
%     Hopper-expert-v0 & 97.5$\pm$1.9 & 112.2$\pm$1.4 & 111.3$\pm$2.1 & 111.5$\pm$1.6 & 110.6$\pm$1.9 \\
%     Walker2d-expert-v0 & 76.9$\pm$3.2 & 80.8$\pm$5.2 & 81.7$\pm$3.4 & 80.8$\pm$2.8 & 84.4$\pm$5.0 \\
%     Halfcheetah-expert-v0 & 84.3$\pm$2.7 & 82.9$\pm$2.8 & 83.0$\pm$3.2 & 82.3$\pm$1.9 & 84.3$\pm$3.5 \\
%     Hopper-medium-expert-v0 & 112.0$\pm$0.7 & 112.1$\pm$0.2 & 112.1$\pm$0.6 & 112.3$\pm$0.3 & 112.4$\pm$0.3 \\
%     Walker2d-medium-expert-v0 & 78.6$\pm$3.6 & 82.5$\pm$3.2 & 85.0$\pm$2.8 & 84.6$\pm$2.9 & 85.4$\pm$5.3 \\
%     Halfcheetah-medium-expert-v0 & 63.5$\pm$3.3 & 66.7$\pm$3.9 & 84.1$\pm$4.2 & 85.0$\pm$5.2 & 86.2$\pm$5.0 \\
%     \bottomrule
%     \end{tabular}
% \end{table}

% \subsection{Ablation study for approximation bounds}
% \label{appendix: approx bound} 
% We evaluate the performance of \name~with various approximation bounds (from 0.0001 to 0.05). 
% A smaller approximation bound represents a larger reduced dataset. The experimental results in Table~\ref{tab: approx bound} show that similar to the ablation of the size of the reduced dataset, \name~requires only a 0.01 approximation bound to obtain good performance.

% \begin{table}[h]
%     \centering
%     \caption{Ablation study with the approximation bounds.} 
%     \label{tab: approx bound}
%     \begin{tabular}{c|ccccc}
%     \toprule
%     Approximation Bounds & 0.0001 & 0.001 & 0.01 & 0.05 & All Data\\
%     \midrule
%     Hopper-medium-v0 & 97.9$\pm$1.3 & 94.6$\pm$0.8 & 92.2$\pm$1.1 & 31.9$\pm$1.9 & 99.5$\pm$1.0 \\
%     Walker2d-medium-v0 & 75.7$\pm$0.9 & 70.5$\pm$2.2 & 36.3$\pm$1.6 & 1.3$\pm$0.5 & 79.7$\pm$1.8\\
%     Halfcheetah-medium-v0 & 42.0$\pm$0.8 & 41.2$\pm$0.7 & 40.7$\pm$0.5 & 30.6$\pm$0.9 & 42.8$\pm$0.3 \\
%     Hopper-expert-v0 & 112.3$\pm$0.1 & 112.5$\pm$0.3 & 111.1$\pm$0.2 & 26.7$\pm$0.5 & 112.2$\pm$0.2\\
%     Walker2d-expert-v0 & 102.9$\pm$2.5 & 98.9$\pm$2.2 & 79.4$\pm$1.6 & 0.6$\pm$0.1 & 105.7$\pm$2.7\\
%     Halfcheetah-expert-v0 & 102.9$\pm$1.1 & 98.9$\pm$1.4 & 70.9$\pm$1.9 & 1.2$\pm$0.2 & 105.7$\pm$1.9\\
%     Hopper-medium-expert-v0 & 112.5$\pm$0.3 & 112.5$\pm$0.8 & 110.2$\pm$0.5 & 6.4$\pm$0.3 & 112.2$\pm$0.2\\
%     Walker2d-medium-expert-v0 & 101.2$\pm$5.7 & 98.8$\pm$7.4 & 82.3$\pm$6.9 & 1.2$\pm$0.6 & 101.1$\pm$9.3\\
%     Halfcheetah-medium-expert-v0 & 95.1$\pm$5.6 &	89.1$\pm$3.4 & 76.8$\pm$4.7 & 1.3$\pm$0.3 & 97.9$\pm$4.4\\
%     \bottomrule
%     \end{tabular}
% \end{table}