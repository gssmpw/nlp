\clearpage
\section{Proofs of theoretical analysis}
{\subsection{Notations}}

\begin{table}[ht]
    \centering
    {\begin{tabular}{ll}
    \toprule
    Notation & Explanation \\
    \midrule
    \hspace{0.3cm} $U_\mathtt{TD}$ & Bound of TD Loss \\
    \hspace{0.3cm} $U_{\nabla Q}$ & Bound of Gradient \\
    \hspace{0.3cm} $U_{\nabla a}$ & Bound of Gradient \\
    \hspace{0.3cm} $U_a$ & Bound of Action Difference \\
    \hspace{0.3cm} $U_\pi$ & Bound of Action \\
    \hspace{0.3cm} $\mathcal{D}$ & Complete Dataset \\
    \hspace{0.3cm} $\mathcal{S}$ & Reduced Dataset \\
    \hspace{0.3cm} $N$ & Size of Reduced Dataset \\
    \hspace{0.3cm} $\lambda$ & Minimum Eigenvalues \\
    \hspace{0.3cm} $\mathcal{C}$ & Cluster \\
    \hspace{0.3cm} $G$ & Total Training Steps \\
    \hspace{0.3cm} $\epsilon$ & Gradient Approximation Errors \\
    \hspace{0.3cm} $\theta_t$ & Updated parameter at the $t^{th}$ epoch \\
    \hspace{0.3cm} $\theta_t^*$ & Optimal model parameter \\
    \bottomrule
    \end{tabular}}
    {\caption{Organization of the notations used througout this paper}}
    \label{tab: notation}
\end{table}

\subsection{Submodular}
\label{appendix: submodular}

% First, we restate Theorem \ref{thm: submodular}.
% \begin{theorem}
%     For any $\mathcal S$ with $|\mathcal S| \leq N$ and sample $(s_i,a_i,r_i,s'_i)\in \mathcal{D}$, suppose that the TD loss and gradients are bounded: $|\mathcal{L}^i(\theta)| \leq U_\mathtt{TD}$, $ \|\nabla_\theta Q_\theta(s_i,a_i)\|_2 \leq U_{\nabla Q}$, $\|\nabla_{\pi_{\phi}(s_i)}Q_\theta(s_i,\pi_{\phi}(s_i))\|_2 \leq U_{\nabla a}$, $\|\pi_{\phi}(s_i)-a_i\|_2 \leq U_a$, $\|\pi_{\phi}(s_i)\|_2\leq U_\pi$, and $\|\nabla_\phi \pi_{\phi}(s_i)\|_2 \leq U_{\nabla\pi}$, then $F_\lambda^Q(\mathcal{S})$ is $\delta$-weakly submodular, with
%     \begin{align}
%         \delta \geq \frac{\lambda}{\lambda+4 N (U_\mathtt{TD}U_{\nabla Q})^2},\nonumber
%     \end{align}
%     and $F_\lambda^\pi(\mathcal{S})$ is $\delta$-weakly submodular, with 
%     \begin{align}
%         \delta \geq \frac{\lambda}{\lambda + N(U_{\nabla a}/\alpha+2U_a U_\pi)^2 U_{\nabla\pi}^2}.\nonumber
%     \end{align}
%     \label{thm: submodular}
% \end{theorem}
\submodular*

\begin{proof}
As mentioned in Section \ref{sec: preliminary}, we use the TD3+BC algorithm as the basic offline RL algorithm. 
TD3+BC follows the actor-critic framework, which trains policy and value networks separately. 
For a single sample $(s_i,a_i,r_i,s'_i)$, the loss of the value network is also named as TD error, which is defined by:
\begin{align}
    & \mathcal L_{Q}^i(\theta) = (y_i - Q_\theta(s_i,a_i))^2   \\
    & \text{where}\quad y_i = r_i + \gamma Q_{\theta'}(s'_i,\pi_{\phi'}(s'_i)+\epsilon)  \\
\end{align}

The gradient is:

\begin{align}
    -\frac{1}{2} \nabla_{\theta} \mathcal L^i_Q(\theta)=(y_i- Q_\theta(s_i,a_i))\nabla_\theta Q_\theta(s_i,a_i)
    \label{eq: td_gradient}
\end{align}

Offline RL algorithms attempt to minimize the TD error and compute the Q-value through a neural network.
Therefore, we assume the upper bound of the TD error is $\max_i\|y_i- Q_\theta(s_i,a_i)\|_2\leq U_\mathtt{TD}$.
The upper bound of the gradient of the value network is $\max_i \|\nabla_\theta Q_\theta(s_i,a_i)\|_2\leq U_{\nabla Q}$.
Then, Equation~\ref{eq: td_gradient} can be transformed into:
\begin{equation}
    \|\nabla_\theta \mathcal L^i_Q(\theta)\|_2 \leq 2U_\mathtt{TD} U_{\nabla Q}
\end{equation}

Similarly, for a single sample$(s_i,a_i,r_i,s'_i)$, the loss of the policy network is
\begin{align}
    \mathcal L_{\pi}^i(\phi) &= -\frac{1}{\alpha} Q_\theta(s_i, \pi_{\phi}(s_i))+\|\pi_{\phi}(s_i)-a_i\|_2^2 \\
\end{align}

The gradient is:

\begin{align}
    \nabla_{\phi} \mathcal L_{\pi}^i(\phi) &= \frac{\partial \mathcal L_{\pi}^i(\phi)}{\partial \pi_{\phi}(s_i)}\times \frac{\partial \pi_{\phi}(s_i)}{\partial \phi}   \\
    &= [-\frac{1}{\alpha} \nabla_{\pi_{\phi}(s_i)}Q_\theta(s_i,\pi_{\phi}(s_i))+2(\pi_{\phi}(s_i)-a_i)^\top \pi_{\phi}(s_i)] \times \nabla_\phi \pi_{\phi}(s_i)
    \label{eq: policy_gradient}
\end{align}

Here $\alpha$ is used to balance the conservatism and generalization in Offline RL, which is defined by:

\begin{align}
    \alpha= \frac{\mathbb{E}_{(s_i,a_i)}[|Q(s_i,a_i)|]}{\kappa}
\end{align}

where $\kappa$ is a hyper-parameter in TD3+BC.
Note that although $\alpha$ includes $Q$, it is not differentiated over. 

Offline RL algorithms attempt to limit the deviation of the current learned policy from the behavior policy while maximizing the Q-value of the optimized policy.
Therefore, we assume the upper bound of the gradient of the value network is $\max_i\|\nabla_{\pi_{\phi}(s_i)}Q_\theta(s_i,\pi_{\phi}(s_i))\|_2 \leq U_{\nabla a}$.
The upper bound of the action error is $\max_i\|\pi_{\phi}(s_i)-a_i\|_2\leq U_a$.
The upper bound of the output of the policy is $\max_i\|\pi_{\phi}(s_i)\|_2 \leq U_\pi$.
The upper bound of the gradient of the policy network is
$\max_i\|\nabla_\phi \pi_{\phi}(s_i)\|_2\leq U_{\nabla \pi}$.

Then, Equation~\ref{eq: policy_gradient} can be bound:
\begin{equation}
    \|\nabla_\phi \mathcal L_{\pi}^i(\phi)\|_2 \leq (U_{\nabla a}/\alpha+2U_a U_\pi)U_{\nabla \pi}
\end{equation}

We can define two functions $l_Q(\mathbf{\beta}), l_\pi(\mathbf{\beta}): \mathbb{R}^{|\mathcal{D}|} \rightarrow \mathbb{R}$
\begin{equation}
\begin{aligned}
    l_Q(\mathbf{\beta}) &= -\|\sum_{i=1}^{{|\mathcal{D}|}} \beta_i\nabla_\theta \mathcal L_Q^i(\theta)-\nabla_\theta \mathcal L(\theta)\|_2 - \lambda\|\beta\|_2^2 \\
    l_\pi(\mathbf{\beta}) &= -\|\sum_{i=1}^{{|\mathcal{D}|}} \beta_i\nabla_\phi \mathcal L^i_\pi(\phi)-\nabla_\phi \mathcal L(\phi)\|_2 - \lambda\|\beta\|_2^2
\end{aligned}
\end{equation}

We assume $\beta$ is a $N$-sparse vector that is 0 on all but $N$ indices.
Then we can transform maximizing $F^Q_\lambda(\mathcal{S}), F^\pi_\lambda(\mathcal{S})$ into maximizing $l(\beta)-l(\mathbf{0})$:
\begin{equation}
\begin{aligned}
    \max_{\mathcal{S}:|\mathcal{S}| \leq N} F^Q_\lambda(\mathcal{S}) &\xleftrightarrow{} \max_{\substack{\beta:\beta_{S^c=0} \\|\mathcal{S}|\leq N}} l_Q(\mathbf{\beta})-l_Q(\mathbf{0}) \\
    \max_{\mathcal{S}:|\mathcal{S}| \leq N} F^\pi_\lambda(\mathcal{S}) &\xleftrightarrow{} \max_{\substack{\beta:\beta_{S^c=0} \\|\mathcal{S}|\leq N}} l_\pi(\mathbf{\beta})-l_\pi(\mathbf{0})
\end{aligned}
\end{equation}
where ${S^c}$ means the complementary set of $S$, and $\beta_{S^c}=0$ means $\beta$ is 0 on all but indices $i$ that $i \in S$. 
$l(\mathbf{0})$ means the value of $l(\cdot)$ when input is zero vector $\mathbf{0}$, it serves as a basic value.
Since $l_Q(\beta)\leq 0, l_\pi(\beta) \leq 0$,  we can easily find that the minimum eigenvalues of $-l_Q(\beta)$ and $-l_\pi(\beta)$ are both at least $\lambda$. 

Next, the maximum eigenvalues of $-l_Q(\beta)$ and $-l_\pi(\beta)$ are
\begin{equation}
\begin{aligned}
\Lambda_{\max}(-l_Q(\beta))&=
\lambda+\operatorname{Trace}\left(\left[\begin{array}{c}
\beta_1\nabla_\theta \mathcal L_Q^{1 \top}\left(\theta\right) \\
\beta_2\nabla_\theta \mathcal L_Q^{2 \top}\left(\theta\right) \\
\ldots\\
\beta_{|\mathcal{D}|}\nabla_\theta \mathcal L_Q^{|\mathcal{D}| \top}\left(\theta_t\right)
\end{array}\right]\left[\begin{array}{c}
\beta_1\nabla_\theta \mathcal L_Q^{1 \top}\left(\theta\right) \\
\beta_2\nabla_\theta \mathcal L_Q^{2 \top}\left(\theta\right) \\
\ldots \\
\beta_{|\mathcal{D}|}\nabla_\theta \mathcal L_Q^{|\mathcal{D}| \top}\left(\theta\right)
\end{array}\right]^{\top}\right)    \\
&=\lambda + \sum_{i=1}^{{|\mathcal{D}|}} \beta_i^2 \| \nabla_\theta \mathcal L_Q^{i}(\theta) \|^2\\ &\leq \lambda+4 N (U_\mathtt{TD}U_{\nabla Q})^2 \\
\Lambda_{\max}(-l_\pi(\beta))&\leq \lambda + N(U_{\nabla a}/\alpha+2U_a U_\pi)^2 U_{\nabla\pi}^2
\end{aligned}
\end{equation}

Following the Theorem~1 in \cite{elenberg2018restricted}, we can derive that $F_\lambda^Q(\mathcal{S})$ is $\delta$-weakly submodular with $\delta \geq \frac{\lambda}{\lambda+4 N (U_\mathtt{TD}U_{\nabla Q})^2}$. 
And $F_\lambda^\pi(\mathcal{S})$ is $\delta$-weakly submodular with $\delta \geq \frac{\lambda}{\lambda + N(U_{\nabla a}/\alpha+2U_a U_\pi)^2 U_{\nabla\pi}^2}$.
\end{proof}

\subsection{Upper Bound of Residual Error}
\label{appendix: cluster theory}

\cluster*

\begin{proof}
The residual error is no larger than the special case where all $w_i$ are $|\mathcal{D}|/|\mathcal{S}|$:
\begin{align}
\operatorname{Err}\left(\vw, \mathcal{S}, \mathcal L, \theta\right)\le\|\frac{|\mathcal D|}{|\mathcal S|}\sum_{i\in \mathcal S}\nabla_{\theta} \mathcal L^i\left(\theta\right) - \sum_{i\in \mathcal D}\nabla_{\theta} \mathcal L^i\left(\theta\right) \|_2. \nonumber
\end{align}
Using Jensen's inequality, we have
\begin{align}
\operatorname{Err}\left(\vw, \mathcal{S}, \mathcal L, \theta\right)\le\sum_{i\in \mathcal D} \|\nabla_{\theta} \mathcal L^i\left(\theta\right) - \frac{1}{|\mathcal S|}\sum_{s\in\mathcal S}\nabla_{\theta}\mathcal L^s\left(\theta\right) \|_2. \nonumber
\end{align}
% In our formulation, samples are selected in mini-batches, and the gradient of sample $i$ from mini-batch $\mathcal B_{j_i}^{k_i}$ is approximated by those of $\mathcal S_{j_i}^{k_i}$ (Eq.~\ref{eq: batch gradient approx}). Therefore, we have
% \begin{align}
%     \operatorname{Err}\left(\vw, \mathcal{S}, \mathcal L, \theta\right)\le\sum_{i\in \mathcal D}\|\nabla_{\theta} \mathcal L^i\left(\theta\right) - \frac{1}{|\mathcal S_{j_i}^{k_i}|}\sum_{s\in\mathcal S_{j_i}^{k_i}}\nabla_{\theta}\mathcal L^s\left(\theta\right) \|_2.\nonumber
% \end{align}
According to the monotone property of submodular functions, adding more samples to $S^k$ reduces the residual error. We assume $S^k$ starts with the cluster center $\{c_k\}$, it follows that
    \begin{align}
    \operatorname{Err}&\left(\vw, \mathcal{S}, \mathcal L, \theta\right) \le \sum_{i\in \mathcal D}\|\nabla_{\theta} \mathcal L^i\left(\theta\right) - \nabla_{\theta}\mathcal L^{c_k}\left(\theta\right) \|_2\nonumber\\
    =& \sum_{i\in\mathcal D} \min_{c\in \mathcal C}\|\nabla_{\theta} \mathcal L^i\left(\theta\right) - \nabla_{\theta} \mathcal L^c\left(\theta\right) \|_2.\label{equ:cluster_obj}
\end{align}
Eq.~\ref{equ:cluster_obj} is exactly the optimization objective typical of the clustering problem.
\end{proof}

\subsection{Convergence Analysis}
\label{appendix: convergence}

% \begin{theorem}
%     Let $\theta^*$ denote the optimal $Q^{\pi_\mathcal{S}}$ parameters, $\theta_t$ the parameters after $t$ training steps. The TD loss satisfies 
%     \begin{align}
%         \min_{t=1:G}\mathcal{L}(\theta_t)\leq \mathcal{L}(\theta^*) + \frac{D\sigma}{\sqrt{G}} + \frac{D}{G}\sum_{t=1}^{G-1}\varepsilon.\nonumber
%     \end{align}
%     Here 
%     $\mathcal{L}(\theta)=\sum_{i \in \mathcal{D}}\mathcal L_{\mathtt{TD}}(s_{i}, a_{i}, r_i, s'_{i}, \theta)$, $G$ is the total training steps, $\varepsilon=\operatorname{Err}\left(\vw, \mathcal{S}, \mathcal L, \theta_t\right)$ is the gradient approximation errors that are bounded in Corollary~\ref{thm:c_bound}.
% \end{theorem}
\convergence*

\begin{proof}
    From the definition of Gradient Descent, we have:

    \begin{align}
    \nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t)^T(\theta_t - \theta^*) &= \frac{1}{\alpha_t}(\theta_t-\theta_{t+1})^T(\theta_t-\theta^*) \\
    \nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t)^T(\theta_t - \theta^*) &= \frac{1}{2\alpha_t}\left(\|\theta_t-\theta_{t+1}\|^2 + \|\theta_t-\theta^*\|^2 - \|\theta_{t+1}-\theta^*\|^2\right) \\
    \nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t)^T(\theta_t - \theta^*) &= \frac{1}{2\alpha_t}\left(\|\alpha_t \nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t)\|^2 + \|\theta_t-\theta^*\|^2 - \|\theta_{t+1}-\theta^*\|^2\right)
    \end{align}

    Then, we rewrite the function $\nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t)^T(\theta_t - \theta^*)$ as follows:

    \begin{align}
        \nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t)^T(\theta_t - \theta^*) = \nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t)^T(\theta_t - \theta^*) - \nabla_{\theta} \mathcal{L}(\theta_t)^T(\theta_t - \theta^*) + \nabla_{\theta} \mathcal{L}(\theta_t)^T(\theta_t - \theta^*)
    \end{align}

    Combining the above equations we have:

    \begin{align}
    \nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t)^T(\theta_t - \theta^*) - \nabla_{\theta} \mathcal{L}(\theta_t)^T(\theta_t - \theta^*) + \nabla_{\theta} \mathcal{L}(\theta_t)^T(\theta_t - \theta^*) = \\
    \frac{1}{2\alpha_t}\left(\|\alpha_t\nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t)\|^2 + \|\theta_t - \theta^*\|^2 - \|\theta_{t+1} - \theta^*\|^2\right)
    \end{align}

    \begin{align}
    \nabla_{\theta} \mathcal{L}(\theta_t)^T(\theta_t - \theta^*) =
    \frac{1}{2\alpha_t}\left(\|\alpha_t\nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t)\|^2 + \|\theta_t - \theta^*\|^2 - \|\theta_{t+1} - \theta^*\|^2\right) - \\ (\nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t) - \nabla_{\theta} \mathcal{L}(\theta_t))^T(\theta_t - \theta^*)
    \end{align}

    Summing up the above equation for different value of $t\in [0,G-1]$ and the learning rate $\alpha_t$ is a constant $\alpha$, then we have:

    \begin{align}
    \sum_{t=0}^{G-1} \nabla_{\theta} \mathcal{L}(\theta_t)^T(\theta_t - \theta^*) = \frac{1}{2\alpha} \|\theta_0 - \theta^*\|^2 - \|\theta_G - \theta^*\|^2 + \sum_{t=0}^{G-1}\left(\frac{1}{2\alpha}\|\alpha\nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t)\|^2\right) \\
    + \sum_{t=0}^{G-1}\left((\nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t) - \nabla_{\theta} \mathcal{L}(\theta_t) )^T(\theta_t - \theta^*)\right)
    \end{align}

    Since $\|\theta_G - \theta^*\|^2 \geq 0$, we have:

    \begin{align}
        \sum_{t=0}^{G-1} \nabla_{\theta} \mathcal{L}(\theta_t)^T(\theta_t - \theta^*) \leq \frac{1}{2\alpha} \|\theta_0 - \theta^*\|^2 + \sum_{t=0}^{G-1}\left(\frac{1}{2\alpha}\|\alpha\nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t)\|^2\right) \\
        + \sum_{t=0}^{G-1}\left((\nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t) - \nabla_{\theta} \mathcal{L}(\theta_t) )^T(\theta_t - \theta^*)\right)
        \label{eq: sum}
    \end{align}

From the convexity of function $\mathcal{L}(\theta)$, we have:

\begin{align}
    \mathcal{L}(\theta_t) - \mathcal{L}(\theta^*) \leq \nabla_{\theta} \mathcal{L}(\theta_t)^T(\theta_t - \theta^*)
    \label{eq: convexity}
\end{align}

Combining the Equation~\ref{eq: sum} and Equation~\ref{eq: convexity}, we have:

\begin{align}
    \sum_{t=0}^{G-1}\mathcal{L}(\theta_t) - \mathcal{L}(\theta^*) \leq \frac{1}{2\alpha} \|\theta_0 - \theta^*\|^2 + \sum_{t=0}^{G-1}\left(\frac{1}{2\alpha}\|\alpha\nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t)\|^2\right) \\
    + \sum_{t=0}^{G-1}\left((\nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t) - \nabla_{\theta} \mathcal{L}(\theta_t))^T(\theta_t - \theta^*)\right)
\end{align}

We assume that $\|\theta - \theta^*\|\leq D$.
Since $\| \nabla \mathcal L(\theta) \| \leq \sigma$, we have:

\begin{align}
    \sum_{t=0}^{G-1}\mathcal{L}(\theta_t) - \mathcal{L}(\theta^*) \leq \frac{D^2}{2\alpha} + \frac{G\alpha\sigma^2}{2}
    + \sum_{t=0}^{G-1}D(\|\nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t) - \nabla_{\theta} \mathcal{L}(\theta_t)\|)
\end{align}

Then:

\begin{align}
    \frac{\sum_{t=0}^{G-1}\mathcal{L}(\theta_t) - \mathcal{L}(\theta^*)}{G} \leq \frac{D^2}{2\alpha G} + \frac{\alpha\sigma^2}{2}
    + \sum_{t=0}^{G-1}\frac{D}{G}(\|\nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t) - \nabla_{\theta} \mathcal{L}(\theta_t)\|)
\end{align}

Since $\min(\mathcal{L}(\theta_t) - \mathcal{L}(\theta^*))\leq \frac{\sum_{t=0}^{G-1}\mathcal{L}(\theta_t) - \mathcal{L}(\theta^*)}{G}$, we have:

\begin{align}
    \min(\mathcal{L}(\theta_t) - \mathcal{L}(\theta^*))\leq \frac{D^2}{2\alpha G} + \frac{\alpha\sigma^2}{2}
    + \sum_{t=0}^{G-1}\frac{D}{G}(\|\nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t) - \nabla_{\theta} \mathcal{L}(\theta_t)\|)
\end{align}

We adopt $\varepsilon$ to denote $\|\nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t) - \nabla_{\theta} \mathcal{L}(\theta_t)\|$, then we have:

\begin{align}
    \min(\mathcal{L}(\theta_t) - \mathcal{L}(\theta^*))\leq \frac{D^2}{2\alpha G} + \frac{\alpha\sigma^2}{2}
    + \sum_{t=0}^{G-1}\frac{D}{G}\varepsilon
\end{align}
     
\end{proof}


\begin{theorem}\label{thm:monotone}
    The training loss on original dataset always monotonically decreases with every training epoch $t$, $\mathcal{L}(\theta_{t+1}) \leq \mathcal{L}(\theta_t)$ if it satisfies the condition that $\nabla_{\theta} \mathcal{L}(\theta_t)^T\nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t) \geq 0$ for $0\leq t \leq G$ and the learning rate $\alpha \leq \min_{t} \frac{2}{L}\frac{\nabla_{\theta} \mathcal{L}(\theta_t)^T\nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t)}{\nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t)^T\nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t)}$.
\end{theorem}

% Let $L_{T}(\theta_{t}; \mathcal{D})$ denote the training loss on the original dataset $\mathcal{D}$.
% Then, we suppose the training loss $L_T$ is Lipschitz smooth with constant $\mathcal{L}$, and the gradient is bounded by $\sigma_T$:
% $\|\nabla L_{T}(\theta_{t+1}) - \nabla L_{T}(\theta_{t})\| \leq \mathcal{L}\|\theta_{t+1} - \theta_{t}\|$ and $\| \nabla L_{T}(\theta_{t}) \| \leq \sigma_T$.
% Similarly, we let $L_{T}(\theta_{t}; \mathcal{S})$ denote the training loss on the reduced subset $\mathcal{S}$ with the same smooth assumption.

\begin{proof}
    Since the training loss $\mathcal{L}(\theta)$ is lipschitz smooth, we have:
\begin{align}
    \mathcal{L}(\theta_{t+1}) & \leq \mathcal{L}(\theta_t) + \nabla_{\theta} \mathcal{L}(\theta_t)^T\Delta \theta + \frac{L}{2}\|\Delta\theta\|^2, \\
    &\text{where} \qquad \Delta\theta=\theta_{t+1} - \theta_{t}.
\end{align}

Since, we are using SGD to optimize the reduced subset training loss $\mathcal L_{\rdcshort}(\theta_t)$ model parameters.
The update equation is:

\begin{align}
    \theta_{t+1} = \theta_{t} - \alpha \nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t)
\end{align}

Combining the above two equations, we have:
\begin{align}
    \mathcal{L}(\theta_{t+1}) \leq \mathcal{L}(\theta_t) + \nabla_{\theta} \mathcal{L}(\theta_t)^T(- \alpha \nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t)) + \frac{L}{2}\|- \alpha \nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t)\|^2
\end{align}

Next, we have:

\begin{align}
    \mathcal{L}(\theta_{t+1}) - \mathcal{L}(\theta_t) \leq \nabla_{\theta} \mathcal{L}(\theta_t)^T(- \alpha \nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t)) + \frac{L}{2}\|- \alpha \nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t)\|^2
\end{align}

From the above equation, we have:

\begin{align}
    \mathcal{L}(\theta_{t+1}) \leq \mathcal{L}(\theta_{t}), \quad  \text{if} \quad \nabla_{\theta} \mathcal{L}(\theta_{t})^T\nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t) 
    - \frac{\alpha L}{2}\|\nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t)\|^2 \geq 0
\end{align}

Since $\|\nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t)\|^2\geq 0$, we will have the necessary condition $\nabla_{\theta} \mathcal{L}(\theta_{t})^T\nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t) \geq 0$.
Next, we rewrite the above condition as follows:

\begin{align}
    \nabla_{\theta} \mathcal{L}(\theta_{t})^T\nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t) \geq \frac{\alpha L}{2}\|\nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t)\|^2
\end{align}

Therefore, the necessary condition for the learning rate $\alpha$ is:

\begin{align}
    \alpha \leq \frac{2}{L}\frac{\nabla_{\theta} \mathcal{L}(\theta_t)^T\nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t)}{\nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t)^T\nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t)}
\end{align}

Since the above condition needs to be true for all values for $t$, we have the following conditions for the learning rate:

\begin{align}
    \alpha \leq \min_{t} \frac{2}{L}\frac{\nabla_{\theta} \mathcal{L}(\theta_t)^T\nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t)}{\nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t)^T\nabla_{\theta} \mathcal L_{\rdcshort}(\theta_t)}
\end{align}

\end{proof}