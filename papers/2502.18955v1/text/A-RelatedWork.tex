\clearpage
\section{Related works}
\label{appx:related_work}
% As deep learning usually trains on abundant data, a considerable number of works have focused on identifying important training samples and figuring out the ideal size of the dataset. However, the employment in policy training tasks is under-explored. Our work is closely related to offline RL and data subset selection.

\textbf{Offline Reinforcement Learning.}\ \
Offline RL can execute policy training entirely based on static datasets without further interaction with the environment~\cite{levine2020offline}.
Therefore, it faces challenges such as distribution shift and value overestimation.
To address this issue, some prior works attempted to constrain the learned policy and behavior policy by limiting the action difference~\cite{fujimoto2019off}, adding KL-divergence~\cite{nair2020awac,peng2019advantage,wu2019behavior}, or regularization~\cite{kumar2019stabilizing}.
Other works consider employing conservative estimates of future values~\cite{kumar2020conservative,ma2021conservative} or penalizing uncertain actions~\cite{janner2019trust,yu2021combo,kidambi2020morel} by uncertainty.
There are also some new attempts, such as lightweight implementation~\cite{fujimoto2021minimalist} or avoiding distribution shift by single-step policy iteration~\cite{kostrikov2021offline}.
These studies provide a solid foundation for implementing and transferring reinforcement learning to real-world tasks.
However, there has been limited research addressing considerations related to the dataset.
Some works attempted to explore which dataset characteristics dominate in offline RL algorithms~\cite{schweighofer2021understanding, swazinna2021measuring} or investigate the data generation~\cite{yarats2022don}.
Recently, some researchers attempted to solve the sub-optimal trajectories issue by constraining policy to good data rather than all actions in the dataset~\cite{hong2023beyond} or re-weighting policy~\cite{hong2023harnessing}.
Differently, our work aims to figure out the ideal size of the dataset needed for effective policy training.

\textbf{Data subset selection.}\ \ The research on identifying crucial samples within datasets is concentrated in the field of computer vision.
Some prior works use uncertainty of samples~\cite{coleman2019selection,paul2021deep} or the frequency of being forgotten~\cite{toneva2018empirical} as the proxy function to prune the dataset.
Another research line focuses on constructing weighted data subsets to approximate the full dataset~\cite{feldman2020core}, which often transforms the subset selecting to the submodular set cover problem~\cite{wei2015submodularity,kaushal2019learning}.
Specifically, several works adopted loss functions as the optimization target~\cite{lucic2018training,campbell2018bayesian}, while recent research finds that approximating full gradient is more efficient~\cite{mirzasoleiman2020coresets, killamsetty2021grad,killamsetty2021glister,killamsetty2021retrieve}.
These studies establish the critical importance of selecting key samples from datasets for effective training. However, the different learning objectives and training methodologies between reinforcement learning and computer vision mean that these techniques cannot be directly applied to Offline RL.