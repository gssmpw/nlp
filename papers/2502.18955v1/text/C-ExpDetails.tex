% \clearpage
% \section{ELBOW Experiments}
% \label{appendix: elbow}

% Determining the cluster number is crucial as it is used to solve the outer optimization issue in Equation~\ref{eq: gradient approx}.
% For this reason, we adopt the simple yet efficient elbow method to solve this issue.
% As shown in Figure~\ref{fig: elbow}, the appropriate number of clusters for these tasks tends to concentrate between 75 and 100.

% \begin{figure*}[h]
%     \centering
%     \subfigure{\includegraphics[scale=0.5]{elbow/hopper-medium-v0.pdf}}\subfigure{\includegraphics[scale=0.5]{elbow/hopper-medium-expert-v0.pdf}}\subfigure{\includegraphics[scale=0.5]{elbow/hopper-expert-v0.pdf}}\\\subfigure{\includegraphics[scale=0.5]{elbow/halfcheetah-medium-v0.pdf}}\subfigure{\includegraphics[scale=0.5]{elbow/halfcheetah-medium-expert-v0.pdf}}\subfigure{\includegraphics[scale=0.5]{elbow/halfcheetah-expert-v0.pdf}}
%     \\\subfigure{\includegraphics[scale=0.5]{elbow/walker2d-medium-v0.pdf}}\subfigure{\includegraphics[scale=0.5]{elbow/walker2d-expert-v0.pdf}}
%     \caption{The sum of squared errors of the data points~(named distance) with various cluster numbers.}
%     \label{fig: elbow}
% \end{figure*}

\clearpage
\section{Experimental Details}
\label{appendix: exp details}

\paragraph{Hyper-parameters.}
For the Mujoco tasks, we adopt the TD3+BC as the backbone of the offline algorithms.
For the Antmaze tasks, we adopt the IQL as the backbone of the offline algorithms.
We outline the hyper-parameters used by \name~ in Table~\ref{tab: parameters mujoco}.

\begin{table}[ht]
    \centering
    \begin{tabular}{ll}
    \toprule
    Hyperparameter & Value \\
    \midrule
    \hspace{0.3cm} Optimizer & Adam \\
    \hspace{0.3cm} Critic learning rate & 3e-4 \\
    \hspace{0.3cm} Actor learning rate & 3e-4 \\
    \hspace{0.3cm} Mini-batch size & 256 \\
    \hspace{0.3cm} Discount factor & 0.99 \\
    \hspace{0.3cm} Target update rate & 5e-3 \\
    \hspace{0.3cm} Policy noise & 0.2 \\
    \hspace{0.3cm} Policy noise clipping & (-0.5, 0.5) \\
    \hspace{0.3cm} TD3+BC regularized parameter & 2.5 \\
    \midrule
    Architecture & Value \\
    \midrule
    \hspace{0.3cm} Critic hidden dim & 256 \\
    \hspace{0.3cm} Critic hidden layers & 2 \\
    \hspace{0.3cm} Critic activation function & ReLU \\
    \hspace{0.3cm} Actor hidden dim & 256 \\
    \hspace{0.3cm} Actor hidden layers & 2 \\
    \hspace{0.3cm} Actor activation function & ReLU \\
    \midrule
    \name~Parameters & Value \\
    \midrule
    \hspace{0.3cm} Training rounds $T$ & 50 \\
    \hspace{0.3cm} $m$ & 50 \\
    \hspace{0.3cm} $\epsilon$ & 0.01 \\
    \bottomrule
    \end{tabular}
    \caption{Hyper-parameters sheet of ~\name}
    \label{tab: parameters mujoco}
\end{table}

% \section{Discussion of Limitations}
% \label{appendix: limitation}
% In this work, we consider using the TD loss gradient as the data subset selection criterion. 
% This is because if the gradients of the loss function used to train the $Q$ function are similar, the differences between $Q$ functions are also relatively small, thus making the policy on the data subset closer to that on the full dataset. 
% However, the theoretical framework does not directly present the relationship between the solution in the subset and the optimal solution for the original empirical Markov Decision Process (MDP).
% Nonetheless, our experimental results demonstrate the effectiveness of the proposed approach.

% \section{Broader Impacts}
% \label{appendix: impacts}
% This paper introduces a new perspective and pioneers a new path in the research of offline reinforcement learning. 
% This paper not only offers a reliable method for reducing dataset size, substantiated by sufficient proof, but also delineates the thresholds between adequate and inadequate dataset sizes through experiments, which provides considerable societal importance. 
% Our method can significantly reduce the burdens of training and storage by identifying a more compact subset of data. 
% % Conversely, in more societal domains where accumulating vast amounts of data is impractical, our approach offers guidance on the sufficient amount of data required. 
% This has the potential to expand the current boundaries of application in the field of offline reinforcement learning, making it more accessible and applicable in a broader range of societal contexts.

% % As for ethical aspects, to the best of our knowledge, the research presented in this paper does not directly engage with them. 
% % However, we acknowledge the importance of ethical considerations in machine learning research and strive to ensure that our work aligns with general ethical standards.