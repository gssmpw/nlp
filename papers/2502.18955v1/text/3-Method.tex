\section{Method}
% In this section, we introduce how to identify the reduced datasets for offline RL.
For the data subset selection problem, RL and supervised learning are significantly different in two aspects:
(1) In supervised learning, the loss value is the primary criterion for selecting data.
However, the loss value in RL is unrelated to the policy performance.
Therefore, we need to consider new criteria for selecting data in RL.
(2) Compared with the fixed learning objective in supervised learning, the learning objective in offline RL evolves as policies update, significantly complicating the data selection process.
To solve these issues, we first formulate the data selection problem in offline RL as the constrained optimization problem in Sec.~\ref{subsec: grad approx optim}.
Then, we present how to effectively solve the optimization problem in Sec.~\ref{sec: offline omp}.
Finally, we balance the data quantity with performance in Sec.~\ref{sec:method:outer}.
The algorithm framework is shown in Algorithm~\ref{alg: offline data selection}.


\subsection{Gradient Approximation Optimization}
\label{subsec: grad approx optim}

We first approximate the optimization problem \ref{eq: opt prob}, using the Q-function $Q^{\pi}(s,a)$ as the performance measure $J(\pi) = Q^{\pi}(s_0, a_0)$ and requiring that $Q^{\pi_{\mathcal{D}}}$ and $Q^{\pi_{\mathcal{S}}}$ to be approximately equal for any action-state pair $(s,a)$:
\begin{align}
\label{equ:opt_prob-Q}
    \mathcal{S}^* = \mathop{\arg\min}\limits_{\mathcal{S}\subseteq \mathcal{D}}|\mathcal{S}|, \quad
    \text{s.t.} \quad \|Q^{\pi_{\mathcal{D}}}(s,a) - Q^{\pi_{\mathcal{S}}}(s,a) \|_\infty \leq \delta.
\end{align}

We use \emph{gradient approximation optimization} to deal with the constraint in the optimization problem~\ref{equ:opt_prob-Q}. Suppose that Q-functions are represented by networks with learnable parameters $\theta$ and updated by gradients of loss function $\mathcal{L}(\theta)$, e.g., the TD loss~\citep{mnih2015human}. If we can identify a reduced training set $\mathcal{S}$ such that the weighted sum of the gradients of its elements closely approximates the full gradient over the complete dataset $\mathcal{D}$, then we can train on $\mathcal{S}$ and converge to a Q-function that is nearly identical to the one trained on $\mathcal{D}$.

Formally, 
\begin{align}
    \mathcal L(\theta)=\sum_{i \in \mathcal{D}}\mathcal L^i(\theta) = \sum_{i \in \mathcal{D}}\mathcal L_{\mathtt{TD}}(s_{i}, a_{i}, r_i, s'_{i}, \theta)
\end{align}
is the standard Q-learning TD loss, and
\begin{align}
\mathcal L_{\rdcshort}(\vw,\theta) = \sum\nolimits_{i \in \mathcal{S}} w_i\mathcal L^i(\theta)
\end{align}
is the loss on the reduced subset $\mathcal{S} \subseteq \mathcal{D}$.
In order to better approximate the gradient for the full dataset, we use the weighted data subset. Specifically, $w_i$ is the per-element weight in coreset $\mathcal{S}$. During the learning process, we approximate the entire dataset's gradient by multiplying the samples' gradient in coreset by their weights.
We define the following error term:
\begin{align}
   \operatorname{Err}\left(\vw, \mathcal{S}, \mathcal L, \theta\right) =  \| \sum_{i\in \mathcal{S}}w_{i}\nabla_{\theta}\mathcal L^i\left(\theta\right) - \nabla_{\theta} \mathcal L\left(\theta\right)\|_2.
   \label{eq: omp error}
\end{align}
Minimizing Eq.~\ref{eq: omp error} ensures the dataset selection procedure can maintain or even improve the policy performance.
Similarly, define the regularized version of $\operatorname{Err}\left(\vw, \mathcal{S}, \mathcal L, \theta\right)$ as
\begin{align}
    \operatorname{Err}_{\lambda}\left(\vw, \mathcal{S}, \mathcal L, \theta\right) =
    \operatorname{Err}\left(\vw, \mathcal{S}, \mathcal L, \theta\right) + \lambda \|\vw\|_2^2.
\label{eq: regular opt prob}
\end{align}
Then, the optimization problem \ref{eq: opt prob} is transformed into:
\begin{align}
\vw, \mathcal S= \mathop{\arg\min}\limits_{\vw, \mathcal{S}}\operatorname{Err}_{\lambda}\left(\vw, \mathcal{S}, \mathcal L, \theta\right).
\label{eq: gradient approx}
\end{align}


%We also introduce a normalization component:

%\begin{align}
%\operatorname{Err_{\lambda}}\left(\mathbf{w}^{t}, \mathcal{X}^{t}_1, \cdots, \mathcal{X}^{t}_K, L, L_{T}, \theta_{t}\right)\\
%=\operatorname{Err}\left(\mathbf{w}^{t}, \mathcal{X}^{t}_1, \cdots, \mathcal{X}^{t}_K, L, L_{T}, \theta_{t}\right) + \lambda \|w \|^2
%\end{align}

% The data selection optimization problem then is:

% \begin{equation}
% \begin{align}
% N^t, \mathbf{w}^{t}, \mathcal{X}^{t}=&\underset{N}{\operatorname{argmin}}\underset{\mathbf{w}, \mathcal{X}_1, \cdots, \mathcal{X}_K:\sum_{k \in \{1, \cdots, K\}}|\mathcal{X}_k| \leq N}{\operatorname{argmin}}
% \\& \operatorname{Err}_{\lambda}\left(\mathbf{w}, \mathcal{X}_1, \cdots, \mathcal{X}_K, L, L_{T}, \theta_{t}\right)
% \end{align}
% \end{equation}

% or?

% \begin{equation}
% \begin{align}
% N^t, \mathbf{w}^{t}, \mathcal{X}^{t}=&\underset{N}{\operatorname{argmin}}\underset{\mathbf{w}, \mathcal{X}_1, \cdots, \mathcal{X}_K:\sum_{k \in \{1, \cdots, K\}}|\mathcal{X}_k| \leq N}{\operatorname{argmin}} 
% \\& f(N)\operatorname{Err}_{\lambda}\left(\mathbf{w}, \mathcal{X}_1, \cdots, \mathcal{X}_K, L, L_{T}, \theta_{t}\right),
% \end{align}
% \end{equation}
% where $f(N)$ is a function based on $N$, such as $f(N)=N$ or $f(N)=N^2$

% Optimization problem~\ref{eq: gradient approx} consists of an outer optimization problem that determines the size of the reduced dataset and an inner optimization problem that finds a dataset of a fixed size minimizing the error term $\operatorname{Err}_{\lambda}$. 


\subsection{Orthogonal Matching Pursuit for Offline RL}\label{sec: offline omp}
Directly solving problem \ref{eq: gradient approx} is NP-hard~\citep{killamsetty2021glister,killamsetty2021retrieve} and computationally intractable.
To solve the issue, we consider using the iterative approach, which selects data one by one to reduce $\operatorname{Err}_{\lambda}\left(\vw, \mathcal{S}, \mathcal L, \theta\right)$.
To ensure newly selected data are informative, we prove the optimized problem~\ref{eq: gradient approx} can be transformed into the submodular function.

Specifically, we introduce a constant $L_{\max}$ and define 
$F_{\lambda}(\mathcal{S})=L_{\max} - \min_{\vw}\operatorname{Err}_{\lambda}\left(\vw, \mathcal{S}, \mathcal L, \theta\right)$. 
Then, we consider the common actor-critic framework in data subset selection, which has an actor-network $\pi_{\phi}(s)$ and a critic network $Q_{\theta}(s, a)$ that influence the TD loss and thus the function $F_{\lambda}(\mathcal{S})$. Therefore, the submodularity analysis of  $F_{\lambda}(\mathcal{S})$ involves two components: $F_\lambda^Q(\mathcal{S})$ that depends on the critic loss $\mathcal{L}_Q(\theta)$, and $F_\lambda^\pi(\mathcal{S})$ that depends on the actor loss $\mathcal{L}_\pi(\phi)$. The following theorem shows that both $F_\lambda^Q(\mathcal{S})$ and $F_\lambda^\pi(\mathcal{S})$ are weakly submodular.

% We now introduce how to find an approximation solution to the problem.
% However, this attempt encounters four major challenges. In this section, we discuss and address these challenges.
% We start with dealing with 
% \textbf{(I) Submodular objective}. As discussed in Sec.~\ref{sec:omp_description}, OMP is only applicable to objectives that can be transformed into a weakly submodular function. 
% To construct a submodular function from Eq.~\ref{eq: gradient approx}, 

% We now prove that $F_{\lambda}(\mathcal{S})$ is weakly submodular. 

% Based on the linear function approximation assumption, the value function can be modeled by a linear function of features of the state. Concretely, $V_{\theta}(s)=\phi(s)^{\top} \theta$, where $\phi(s)$ are known features of the state and $\theta$ are the weights of the function approximator. TD loss is defined based on the discount factor $\gamma$. We have the following theorem to demonstrate $F_{\lambda}(\mathcal{S})$ is weakly submodular:

\begin{restatable}[Submodular Objective]{theorem}{submodular}
    For $|\mathcal S| \leq N$ and sample $(s_i,a_i,r_i,s'_i)\in \mathcal{D}$, suppose that the TD loss and gradients are bounded: $|\mathcal{L}^i(\theta)| \leq U_\mathtt{TD}$, $ \|\nabla_\theta Q_\theta(s_i,a_i)\|_2 \leq U_{\nabla Q}$, $\|\nabla_{\pi_{\phi}(s_i)}Q_\theta(s_i,\pi_{\phi}(s_i))\|_2 \leq U_{\nabla a}$, $\|\pi_{\phi}(s_i)-a_i\|_2 \leq U_a$, $\|\pi_{\phi}(s_i)\|_2\leq U_\pi$, and $\|\nabla_\phi \pi_{\phi}(s_i)\|_2 \leq U_{\nabla\pi}$, then $F_\lambda^Q(\mathcal{S})$ is $\delta$-weakly submodular, with
    \begin{align}
        \delta \geq \frac{\lambda}{\lambda+4 N (U_\mathtt{TD}U_{\nabla Q})^2},
    \end{align}
    and $F_\lambda^\pi(\mathcal{S})$ is $\delta$-weakly submodular, with 
    \begin{align}
        \delta \geq \frac{\lambda}{\lambda + N(U_{\nabla a}/\alpha+2U_a U_\pi)^2 U_{\nabla\pi}^2}.
    \end{align}
    \label{thm: submodular}
\end{restatable}
Please refer to Appendix~\ref{appendix: submodular} for detailed proof. 

% We start with dealing with the inner optimization problem by attempting to extend the OMP algorithm to the offline RL setting.

Based on the above theoretical analysis, let $\operatorname{Err}_{\lambda}\left(\vw, \mathcal{S}_{j-1}, \mathcal L, \theta\right)$ represent the residual error at iteration $j$.
Then, we adopt the Orthogonal Matching Pursuit~(OMP) algorithm~\citep{elenberg2018restricted}, which selects a new data sample $i$ and takes its gradient $\nabla_{\theta}\mathcal L^i\left(\theta\right)$ as the new basis vector to minimize this residual error.
In this way, we update the residual to $\operatorname{Err}_{\lambda}\left(\vw, \mathcal{S}_{j}, \mathcal L, \theta\right)$, where $\mathcal{S}_j = \mathcal{S}_{j-1} \cup \{i\}$.
However, the dynamic nature of offline RL poses a challenge when using OMP, leading to unstable learning.
To address the unique challenges of offline RL, we propose the following novel techniques to enhance gradient matching:

% If $|S| \leq N$, reward $R$ is in the set $[0, 1]$, $||\phi(s)||_2 < \Phi, \forall {s}$, $||\theta||_2 < \Theta$, then $F_{\lambda}(\mathcal{S})$ is $\alpha$-weakly submodular, with $\alpha \geq \frac{\lambda}{\lambda+N ((1+(1+\gamma)\Phi\Theta)\Phi)^{2}}$
% Since $F_{\lambda}(\mathcal{S})$ is approximately submodular, we consider adopt the orthogonal matching pursuit algorithm to solve it.
% However, employing OMP in offline RL datasets entails the following challenges:
% \begin{enumerate}
%     \item Should we directly employ the gradient of the standard Offline Q-function learning loss to conduct OMP?
%     \item OMP requires traversing the entire dataset to select elements from a subset, and standard offline RL datasets are often very large, for example, 1M in size, which leads to very high computational complexity.
%     \item Since OMP is gradient-based and operates on the training loss, how can we ensure data diversity within the dataset subset to prevent convergence to a local solution based on a single gradient?
% \end{enumerate}
% To address the aforementioned issues, we take TD3+BC as an example to employ the following measures.



\textbf{(I) Stabilizing Learning with Changing Targets.} 
In supervised learning, the stability of training targets leads to stable gradients. However, in offline RL, target values evolve with policy updates, resulting in unstable gradients in Eq.~\ref{eq: omp error} that affect the quality of the selected data subset. To address this issue, we will stabilize the learning process by using \textbf{empirical returns from trajectories} to smooth the gradient updates. This provides a more consistent learning signal and mitigates instability caused by changing target values.
Specifically, rather than adopt the gradient of the TD loss, we calculate gradient $\nabla_{\theta}\mathcal L\left(\theta\right)$ from the following equation
\begin{align}
\label{eq: q-target value}
    \nabla_{\theta}\mathcal L\left(\theta\right) = \nabla_{\theta}\mathbb{E}_\mathcal{D}[(y - Q_{\theta}(s_t,a_t))^2], \quad
    y = \sum_{k=0}^{H-t} \gamma^k r(s_{t+k}, a_{t+k}).
\end{align}

% Dealing with optimization problem~\ref{eq: gradient approx} necessitates training a Q function and a policy to estimate the TD loss. Typically, target functions $Q_{\theta'}$ and $\pi_{\phi'}$ are used to update the Q-networks:
% \begin{align}
%     \theta = {\arg\min}_{\theta}\mathbb{E}_\mathcal{D}[(y - Q_{\theta}(s_t,a_t))^2], \quad
%     y = r(s_t,a_t) + \gamma Q_{\theta'}{(s_{t+1}, \pi_{\phi'}(s_{t+1})+\epsilon)},
%     \label{eq: td3+bc q}
% \end{align}
% and the policy:
% \begin{align}
%     \pi_\phi = \arg\max_{\phi}\mathbb{E}_\mathcal{D}[\frac{Q_{\theta}(s_t,\pi_\phi(s_t))}{\alpha} \shortn (\pi_\phi(s_t)\shortn a_t)^2].\label{eq: td3+bc pi}
% \end{align}
% When using the gradient of the TD loss (Eq.~\ref{eq: td3+bc q}) to minimize the residual error in OMP, we found that due to the continuously changing targets $Q_{\theta'}$, gradients are unstable even for the same data across consecutive iterations, leading to poor offline training performance on the selected data subset. This problem is unique in the offline RL setting, as we do not have supervised learning signals and cannot get accurate target networks with limited training data observable in a few OMP iterations. To address this problem, we propose to use the cumulative discounted rewards from trajectory data to calculate the Q-learning error, which leads to more stable gradients.

Furthermore, we will adopt a \textbf{multi-round selection strategy} where data selection occurs over multiple rounds \( T \). In each round, a portion of the data is selected based on the updated Q-values, reducing variance and ensuring that the subset captures the most critical information. This multi-round approach allows for dynamic adjustment of the selected subset as learning progresses, improving stability and reducing the risk of overfitting to specific trajectories.
Specifically, we calculate $\nabla_{\theta_t}\mathcal L\left(\theta_t\right)$ at each round based on Eq.~\ref{eq: q-target value}, where $\theta_t$ is the parameter updated in the $t$-round.
In practice, we pre-store parameters $\theta_t$ with various rounds $t$ and load them during training.

\textbf{(II) Trajectory-based Selection.} 
In offline RL, collected data is often stored in trajectories, which are coherent and more valuable than individual data points.
For this reason, we modify OMP to the trajectory-based gradient matching.
Specifically, we select a new trajectory $i$ of length $K$ and take the mean of gradients $\nabla_{\theta_t}\mathcal L^i_{\text{Traj}}\left(\theta_t\right)=\sum_{k=1}^{K}\nabla_{\theta_t}\mathcal L^k\left(\theta_t\right)/K$  as the new basis vector to minimize the residual error.
Then, we update the residual to $\operatorname{Err}_{\lambda}\left(\vw, \mathcal{S}_{j}, \mathcal L, \theta\right)$, where $\mathcal{S}_j = \mathcal{S}_{j-1} \cup \{\text{Trajectory}_i\}$.



% \textbf{(II) Multi-round selection.} Training with empirical returns as in Eq.~\ref{eq: q-target value} might suffer from high variance. To further enhance stability, we adopt a strategy where data is selected across $T$ rounds to reduce variance. In each round, we first train $Q_\theta$ and $\pi_\phi$ with $K_T$ samples and then select $\frac{N}{T}$ data points from the original dataset using the updated Q function and policy. The selection process is cumulative, with the data chosen in each round collectively constituting the reduced offline dataset.

% according to Equation~\ref{eq: batch gradient approx}.

% In contrast to data selection in fields like computer vision that typically deals with datasets of around 60K instances~\cite{krizhevsky2009learning}, offline RL datasets require the OMP to process millions of trajectories. When $|\mathcal{D}|=M$, the execution of OMP has a complexity of $\mathcal{O}(MN)$, which is intractable in practice.

% We propose to solve this problem by dividing samples into mini-batches. In each mini-batch $\mathcal{B}$, we select $\frac{|\mathcal{B}|N}{M}$ samples with the objective in the mini-batch format:
% \begin{align}
% \label{eq: batch gradient approx}
% N_j, &\vw_j, \mathcal{S}_j=  \mathop{\arg\min}\limits_{N_j<\frac{|\mathcal{B}|N}{M}}\mathop{\arg\min}\limits_{\vw_j, \mathcal{S}_j: |\mathcal{S}_j|\leq N_j}
% \operatorname{Err}_{\lambda}^B\left(\vw_j, \mathcal{S}_j, \mathcal L^B, \theta\right),
% \end{align}
% where $\operatorname{Err}^{B}_{\lambda}\left(\vw_j, \mathcal{S}_j, \mathcal L^B, \theta\right)$ is the gradient approximation error on the mini-batch $\mathcal{B}$, and $j$ is the batch index. In this way, the computational complexity can  be reduced to $\mathcal{O}(|\mathcal{B}|N)=\mathcal{O}(\frac{|\mathcal{B}|^2N}{M}\frac{M}{|\mathcal{B}|})$. In practice, we sample $\frac{M}{T|\mathcal{B}|}$ mini-batches in each round and select data from them.

% The selected dataset is the union of all $\frac{M}{|\mathcal{B}|}$ batch-wise subsets.

% Eq.~\ref{eq: omp error} in the mini-batch format is:
% \begin{align}
%    \operatorname{Err}^{B}\left(\mathbf{w}_i, \mathcal{S}_i, L^B, L^B_T, \theta\right) =  \| \sum_{j\in \mathcal{S}_i}w_{i,j}\nabla_{\theta}L_{T}^{j, B}\left(\theta\right) - \nabla_{\theta} L^B\left(\theta\right)\|,
%    \label{eq: batch omp error}
% \end{align}
% $i$ denotes the index of the samples. 
% $\{S_i\}_{i=1}^{N/|\mathcal{B}|}$.

\subsection{Balancing Data Quantity with Performance}\label{sec:method:outer}
In offline RL, while additional data can help generalization, suboptimal data may lead to significant performance degradation due to distribution shifts. To address this, we will introduce a \textbf{constraint term} that biases the TD-gradient matching method toward selecting data with higher return estimates.
Then, based on the design in the Sec.~\ref{sec: offline omp}~(I), the Equation~\ref{eq: q-target value} is transformed into
\begin{equation}
\begin{aligned}
\label{eq: q-target value constraint}
    \nabla_{\theta}\mathcal L\left(\theta\right) &= \nabla_{\theta}\mathbb{E}_\mathcal{D}[(y - Q_{\theta}(s_t,a_t))^2], \quad
    y = \sum_{k=0}^{H-t} \gamma^k r(s_{t+k}, a_{t+k}),\\
    & \text{s.t.} \quad y > \text{\rm Top}~m\%(\{\text{Return}(\text{Trajectory}_j)\}_{j=1}^{|\mathcal{D}|}).
\end{aligned}
\end{equation}

This regularized constraint selection approach ensures that the selected subset not only reduces computational costs but also focuses on data points that are aligned with the learned policy, avoiding performance degradation caused by suboptimal trajectories.

\begin{algorithm}[t]
    \caption{Reduce Dataset for Offline RL~(\name)}
    \label{alg: offline data selection}
    \begin{algorithmic}[1]
        \STATE {\bf Require}: Complete offline dataset $\mathcal{D}$
        \STATE Initialize parameters of the offline agent for data selection $Q_{\theta}, \pi_{\phi}$
        \FOR{$t=1, \cdots, T$}
        \STATE Load parameter $\theta_t$ for $Q_{\theta_t}$
        \STATE Calculate $\nabla_{\theta_t}\mathcal{L}(\theta_t), \nabla_{\theta_t}\mathcal L_{\text{Traj}}(\theta_t)$ based on Equation~\ref{eq: q-target value constraint}
        \STATE $\mathcal{S}_{t}, \vw_{t}$ = OMP$(\nabla_{\theta_t}\mathcal{L}(\theta_t), \nabla_{\theta_t}\mathcal L_{\text{Traj}}(\theta_t), \theta_t)$
        \ENDFOR
    \STATE Reduced offline dataset $\mathcal S\leftarrow\cup_{t\in[T]}\mathcal S_t$
    \STATE Initialize parameters of the offline agent for training on the reduced offline dataset $Q_{\vartheta},\pi_{\varphi}$
    \STATE Train $Q_{\vartheta},\pi_{\varphi}$ based on $\mathcal{S}$ and $\vw$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[t]
    \caption{OMP algorithm}
    \label{alg: omp}
    \begin{algorithmic}[1]
        \STATE {\bf Require}: $\nabla_{\theta_t}\mathcal{L}(\theta_t), \nabla_{\theta_t}\mathcal L_{\text{Traj}}(\theta_t), \theta_t$, regularization coefficient $\lambda$
        \STATE $r\leftarrow\operatorname{Err}_{\lambda}\left(\vw_t, \mathcal{S}_t, \mathcal L, \theta_t\right)$
        \WHILE{$r{\geq}\epsilon$}
        \STATE $e=\arg\max_{i\notin\mathcal{S}_t}|\langle\nabla_{\theta_t}\mathcal L_{\text{Traj}}^i(\theta_t), r\rangle|$
        \STATE $\mathcal{S}_t\leftarrow\mathcal{S}_t\cup\{\text{Trajectory}_e\}$
        \STATE $\vw_t\leftarrow\arg\min_{\vw_t}\operatorname{Err}_{\lambda}\left(\vw_t, \mathcal{S}_t, \mathcal L, \theta_t\right)$
        \STATE $r\leftarrow\operatorname{Err}_{\lambda}\left(\vw_t, \mathcal{S}_t, \mathcal L, \theta_t\right)$
        \ENDWHILE\\
    \STATE \textbf{Return} $\mathcal{S}_t$ and $\vw_t$
    \end{algorithmic}
\end{algorithm}

% We have shown how to find an offline data subset using the improved OMP when the subset size $N$ is given. In this section, we discuss how to solve the outer optimization problem in~\ref{eq: gradient approx} to find the minimum value of $N$, which is an NP-hard problem.

% Our approach entails transforming the problem of identifying the minimum value of $N$ into the problem of determining the number of minimal cluster centers. This approach is underpinned by Theorem~\ref{thm:cluster} proving that the objective of the clustering problem is an upper bound for the residual error $\operatorname{Err}\left(\vw, \mathcal{S}, \mathcal L, \theta\right)$. As a result, a clustering algorithm effectively constitutes an initial step towards resolving the optimization problem~\ref{eq: gradient approx}, with a theoretically guaranteed bounded error specified in Theorem~\ref{thm:c_bound}. Empirically, we can use the elbow method~\cite{milligan1985examination} to determine the optimal number of clusters, thereby offering a proximate solution to the outer optimization problem.

% Specifically, we first run $k$-means++ on the original dataset $\mathcal D$ and obtain $K$ clusters $\{\mathcal{C}_1,\cdots,\mathcal{C}_K\}$. The cluster centers form a set $\mathcal C=\{c_1,\cdots,c_K\}$. Then we divide each cluster $k$ into $J$ mini-batches $\{\mathcal{B}^k_1,\cdots,\mathcal{B}^k_J\}$. On each mini-batch $j$, we initialize the reduced dataset with the cluster center $\{c_k\}$ and run our improved version of OMP to get selected samples $\mathcal{S}_j^k$. The final reduced offline dataset is $\mathcal S=\cup_{j\in[J],k\in[K]}\mathcal S_j^k$ (Algorithm~\ref{alg: offline data selection} in Appendix~\ref{appendix: alg}).

% \begin{figure*}[t]
%     \centering
%     \subfigure{\includegraphics[scale=0.24]{mujoco/hopper-medium-v0.pdf}}\subfigure{\includegraphics[scale=0.24]{mujoco/hopper-medium-expert-v0.pdf}}\subfigure{\includegraphics[scale=0.24]{mujoco/hopper-expert-v0.pdf}}\subfigure{\includegraphics[scale=0.24]{mujoco/walker2d-medium-v0.pdf}}
%     \subfigure{\includegraphics[scale=0.24]{mujoco/walker2d-medium-expert-v0.pdf}}\subfigure{\includegraphics[scale=0.24]{mujoco/walker2d-expert-v0.pdf}}\subfigure{\includegraphics[scale=0.24]{mujoco/halfcheetah-medium-expert-v0.pdf}}\subfigure{\includegraphics[scale=0.24]{mujoco/halfcheetah-expert-v0.pdf}}
%     \caption{Experiments between several baselines and \name~.
%     The reference line is the performance of TD3+BC in the original dataset.
%     The experimental results are averaged with five random seeds.}
%     \label{fig: d4rl minimal ratio}
% \end{figure*}

% Therefore, we can use the cluster method to deal with $\arg\min_{N}$ term.
% Specifically, before we proceed with reducing the number of training examples, 
% our first step involves clustering the batch data $\mathcal{B}$ into $K$ clusters. 
% After that, we will select a subset $\mathcal{S}_i^k$ from each cluster $\mathcal{B}^k$. 
% Therefore, Equation~\ref{eq: batch omp error} transforms into



% \begin{align}
%     &\operatorname{Err}^B\left(\mathbf{w}_i, \mathcal{S}^1_i, \cdots, \mathcal{S}^K_i, L^B, L_{T}^B, \theta\right) \\
%     &= \left\|\sum_{k \in \{1, \cdots, K\}}\sum_{j \in \mathcal{S}_i^k} w_{i, k, j} \nabla_{\theta} L_{T}^{k,j,B}\left(\theta\right)-\nabla_{\theta} L^{k,B}\left(\theta\right)\right\| \nonumber
% \end{align}

% Then, the optimization target in Equation~\ref{eq: batch gradient approx} transforms into:
% \begin{align}
% N_i, \mathbf{w}_i, \{\mathcal{S}_i^k\}_{k=1}^{K}&=\underset{N_i}{\arg\min}\underset{\mathbf{w}_i, \mathcal{S}^1_i, \cdots, \mathcal{S}^K_i:\sum_{k \in \{1, \cdots, K\}}|\mathcal{S}^k_i| \leq \frac{|\mathcal{B}|N}{M}}{\arg\min} \nonumber \\
% &\operatorname{Err}^B_{\lambda}\left(\mathbf{w}_i, \mathcal{S}^1_i, \cdots, \mathcal{S}^K_i, L^B, L_{T}^B, \theta\right)
% \end{align}
% where $\mathcal{S}_i = \{S^k_i\}_{k=1}^{K}$ and $\mathbf{w}_i=\{w_{i, k}\}_{k=1}^{K}$.
% By combining the multiple iterations data selection technique, we obtain the final data subset $\mathcal{S}=\{S_i\}_{i=1}^{M/|\mathcal{B}|}$ and its associated weights $\mathbf{w}=\{\mathbf{w}_i\}_{i=1}^{M/|\mathcal{B}|}$.
% Then, we update the parameter of Q-function as follows:

% \begin{align}
%     \theta_{t+1}=\theta_{t}-\alpha \sum_{k \in \{1, \cdots, K\}}\sum_{j \in \mathcal{S}_i^k} w_{i, k, j} \nabla_{\theta} L_{T}^{k,j,B}\left(\theta_t\right)
%     \label{eq: apply omp}
% \end{align}

% The entire training process and pseudocode are presented in the Algorithm~\ref{alg: offline data selection}.

