\section{Background}\label{sec: preliminary}
\textbf{Reinforcement Learning~(RL)} deals with Markov Decision Processes~(MDPs). A MDP can be modeled by a tuple~($S, A, r, p, \gamma$), with the state space $S$, the action space $A$, the reward function $r(s, a)$, the transition function $p(s'|s,a)$, and the discount factor $\gamma$.
We follow the common assumption that the reward function is positive and bounded: $\forall s \in S, a\in A, 0\leq r(s,a) \leq R_{\rm max}$, where $R_{\rm max}$ is the maximum possible reward. RL aims to find a policy $\pi(a\mid s)$ that maximizes the cumulative discounted return:
\begin{equation}
    \pi^* = \arg\max_{\pi} J(\pi) = \arg\max_{\pi}\mathbb{E}_{\pi}[\sum_{t=0}^{H}\gamma^t r(s_t, a_t)],
\end{equation}
where $H$ is the horizon length.
For any policy $\pi$, the action value function is $Q^{\pi}(s_t,a_t)=\mathbb{E}_{\pi}[\sum_{k=0}^{H-t}\gamma^k r(s_{t+k}, a_{t+k})| s_t\shorte s, a_t\shorte a]$.
The state value function is $V^{\pi}(s_t)=\mathbb{E}_{\pi}[\sum_{k=0}^{H-t}\gamma^k r(s_{t+k}, a_{t+k})| s_t\shorte s]$.
It follows from the Bellman equation that $V^{\pi}(s_t) = \sum_{a\in A}\pi(a|s)Q^{\pi}(s_t,a_t)$.


\textbf{Offline RL} learns a policy $\pi$ without interacting with an environment. Rather, the learning is based on a dataset $\mathcal{D}$ generated by a behavior policy $\pi_{\beta}$. One of the major challenges in offline RL is the issue of distributional shift~\citep{fujimoto2019off}, where the learned policy is different from the behavioral policy. Existing offline RL methods apply various forms of regularization to limit the deviation of the current learned policy:
\begin{equation}
    \pi^* = \arg\max_{\pi}\left[ J_{\mathcal{D}}(\pi) - \alpha D(\pi, \pi_{\beta})\right],
    \label{eq: offline opt}
\end{equation}
where $J_{\mathcal{D}}(\pi)$ is the cumulative discounted return of policy $\pi$ on the empirical MDP induced by the dataset $\mathcal{D}$, and $D(\pi, \pi_{\beta})$ is a divergence measure between $\pi$ and $\pi_{\beta}$. In this paper, we base our study on TD3+BC~\citep{fujimoto2021minimalist}, which follows this regularized learning scheme.

%In this case, the learned policy might overestimate certain actions more than the behavioral policy does, primarily because these actions are underrepresented in the training dataset. 

We introduce the concept of \textbf{offline data subset selection}.
% selecting a small subset from the original dataset that minimizes performance degradation for offline training.
Specifically, let $\mathcal{D}=\{(s_{i}, a_{i}, r_i, s_{i}')\}_{i=1}^{M}$ denote the complete offline dataset, and let $\mathcal{S}\subseteq\mathcal{D}$, indexed by $j$, represent the reduced dataset. 
% In offline learning, since we cannot interact with the environment, it is hard to know which data are important to improve policy.
% For this reason, we first consider the subset that performs similarly with the original dataset, and then enhance the policy by improving the subset.
We formulate the subset selection as:
\begin{align}
\label{eq: opt prob}
    \mathcal{S}^* = \mathop{\arg\min}\limits_{\mathcal{S}\subseteq \mathcal{D}}|\mathcal{S}|, \quad
    \text{s.t.} \quad J(\pi_{\mathcal{S}}) \geq J(\pi_{\mathcal{D}}) + c,
\end{align}
where $\pi_{\mathcal{D}}$ and $\pi_{\mathcal{S}}$ are the policy trained using Eq.~\ref{eq: offline opt} with dataset $\mathcal{D}$ and $\mathcal{S}$, respectively. 
$c\geq0$ is the policy performance gain.

\textbf{Compact Subset Selection} \label{sec:omp_description} for offline reinforcement learning remains largely under-explored in existing literature. However, research efforts have been directed toward reducing the size of training samples in other deep learning fields like supervised learning~\citep{killamsetty2021grad, killamsetty2021glister, mirzasoleiman2020coresets}.

%Identifying a compact training dataset suitable

Specifically, there are some research explorations on transforming the subset selection problem into the submodular set cover problem~\citep{mirzasoleiman2020coresets}. The submodular set cover problem is defined as finding the smallest set $\mathcal{S}$ that achieves utility $\rho$:
\begin{align}
    \mathcal{S}^* = \mathop{\arg\min}\limits_{\mathcal{S}\subseteq \mathcal{D}}|\mathcal{S}|, \quad \text{s.t.} \quad F(\mathcal{S})\geq \rho,
\end{align}
where we slightly abuse the notation and use $\mathcal{D}$ to denote the complete supervised learning dataset. We require $F$ to be a \emph{submodular} function like set cover and concave cover modular~\citep{iyer2021submodular}. 
A function $F$ is submodular if it satisfies the \emph{diminishing returns property}: for subsets $\mathcal{S}\subseteq\mathcal{T}\subseteq\mathcal{D}$ and $j\in \mathcal{D} \setminus \mathcal{T}$, $ F(j\mid \mathcal{S})\triangleq F(\mathcal{S}\cup j)-F(\mathcal{S})\geq F(j\mid \mathcal{T})$ and the \emph{monotone property}: $F(j\mid \mathcal{S})\geq 0$ for any $j\in \mathcal{D} \setminus \mathcal{S}$ and $\mathcal{S}\subseteq \mathcal{D}$.


% Further, the submodular cover problem can be solved by the greedy algorithm.It can achieve the logarithmic approximation by selecting an element $j\in\mathcal{D}$ to maximizes $f(j\mid \mathcal{S}_{i-1})$ at each iteration $i$, where $\mathcal{S}_i = \mathcal{S}_{i-1} \cup \{\arg\max_{j\in\mathcal{D}} f(j\mid \mathcal{S}_{i-1})\}$.

% \emph{Orthogonal Matching Pursuit}~(OMP) is a popular method to solve the submodular set cover problem~\citep{pati1993orthogonal, elenberg2018restricted}. It is an iterative method that selects basis vectors to best approximate the observed data. At each iteration, OMP picks a new basis vector to minimize the residual error, that is the difference between the observed data and the combination of the selected basis vectors. 


% For more related works, please refer to Section~\ref{appx:related_work}.

% maximize the inner product with the current residual . In this way, OMP attempts to select the basis vectors that 
% Therefore, 
% OMP is appealing since we can obtain the subset with high marginal utility by employing OMP to solve the submodular cover problem iteratively, similar to the offline data subset selection setting.

% \tw{I suggest that we discuss related works here because there are only  a few closely related works. This position is preferred because we just "introduced" the concept of reducing offline dataset.} 
% \tw{We can introduce submodular and OMP here}.