\section{Experiment}\label{sec: exp}
In this section, we assess the efficacy of our algorithm by addressing the following key questions. 
(1) Can offline RL algorithms achieve stronger performance on the reduced datasets selected by~\name?
(2) How does \name~perform compare to other offline data selection methods? 
(3) What are the factors that contribute to \name's effectiveness?

\begin{figure}[t]
    \centering
    \subfigure{\includegraphics[scale=0.24]{d4rl-hard/walker2d-medium-v0-hard.pdf}}
    \hspace{0.2cm}
    \subfigure{\includegraphics[scale=0.24]{d4rl-hard/walker2d-expert-v0-hard.pdf}}
    \hspace{0.2cm}
    \subfigure{\includegraphics[scale=0.24]{d4rl-hard/walker2d-medium-replay-v0-hard.pdf}}
    % \subfigure{\includegraphics[scale=0.20]{d4rl-hard/walker2d-medium-expert-v0-hard.pdf}}
    \subfigure{\includegraphics[scale=0.24]{d4rl-hard/hopper-medium-v0-hard.pdf}}
    \hspace{0.2cm}
    \subfigure{\includegraphics[scale=0.24]{d4rl-hard/hopper-expert-v0-hard.pdf}}
    \hspace{0.2cm}
    \subfigure{\includegraphics[scale=0.24]{d4rl-hard/hopper-medium-replay-v0-hard.pdf}}
    % \subfigure{\includegraphics[scale=0.20]{d4rl-hard/hopper-medium-expert-v0-hard.pdf}}
    \subfigure{\includegraphics[scale=0.24]{d4rl-hard/halfcheetah-medium-expert-v0-hard.pdf}}
    \hspace{0.2cm}
    \subfigure{\includegraphics[scale=0.24]{d4rl-hard/halfcheetah-expert-v0-hard.pdf}}
    \hspace{0.2cm}
    \subfigure{\includegraphics[scale=0.24]{d4rl-hard/halfcheetah-medium-replay-v0-hard.pdf}}
    % \subfigure{\includegraphics[scale=0.20]{d4rl-hard/halfcheetah-medium-v0-hard.pdf}}
    \caption{Experimental results on the D4RL (Hard) offline datasets. All experiment results were averaged over five random seeds. Our method achieves better or
    comparable results than the baselines with lower computational complexity.}
    \label{fig: d4rl hard}
    \vspace{-0.5cm}
\end{figure}

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=\linewidth]{mujoco/fig1.pdf}
%     \vspace{-2em}
%     \caption{Sample-based selection performance of several baselines and \name~with different selected subset sizes~($x\%$).
%     The horizontal line is the performance of TD3+BC trained with the original dataset.}
%     \label{fig: d4rl minimal ratio}
%     \vspace{-1em}
% \end{figure*}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{mujoco/traj.pdf}
%     \caption{In trajectory-based selection, \name~outperforms behavior cloning (\nameh) using trajectories with the highest accumulative returns, presenting a robust method for selecting the most useful data from training sets of compromised quality.}
%     \label{fig: d4rl topbc}
%     \vspace{-1em}
% \end{figure}

\subsection{Setup}
We evaluate algorithms on the offline RL benchmark D4RL~\citep{fu2020d4rl} to answer the aforementioned questions.
In addition, we consider a more challenging scenario where we add additional low-quality data to the dataset to simulate noise in real-world tasks, named D4RL~(hard).
The evaluation process commences with the selection of offline data, followed by the training of a widely recognized offline RL algorithm, TD3+BC~\citep{fujimoto2021minimalist}, on this reduced dataset for 1 million time steps.
To ensure a fair comparison, we apply the same offline RL algorithm to data subsets obtained by different algorithms. 
Evaluation points are set at every 5,000 training time steps and involve calculating the return of 10 episodes per point.
The results, comprising averages and standard deviations, are computed with five independent random seeds.
On the other hand, we can also incorporate our method into offline model-based approaches, such as MOPO~\citep{yu2020mopo} and MoERL~\citep{kidambi2020morel}.
Similarly, we only need to replace the current offline loss with the corresponding policy and model loss.

\textbf{Baselines}. 
We compare \name~with data selection methods in RL.
Specifically, previous work on prioritized experience replay for online RL~\citep{schaul2015prioritized} aligns closely with our objective. 
We make this a baseline \namep~where samples with the highest TD losses form the reduced dataset. 
Baseline \nameo~presents the performance by training TD3+BC with the original, complete dataset. 
Baseline \namer~randomly selects subsets from the D4RL dataset that are of the same size as \name.
We also compare our method with general dataset reduction techniques from supervised learning.
Specifically, we adopt the coherence criterion from Kernel recursive least squares~($\mathtt{KRLS}$)~\citep{engel2004kernel}, the log det criterion by forward selection in informative vector machines~($\mathtt{LogDet}$)~\citep{seeger2004greedy} and the adapting kernel representation~($\mathtt{BlockGreedy}$)~\citep{schlegel2017adapting} as our baselines.

%Specifically, we consider randomly selecting offline coreset as our baseline algorithms.
% In addition, we consider separately selecting high-reward offline datasets and low-reward offline datasets as our baseline algorithms.

\subsection{Experimental Results}
\label{sec:exp_perf}
% To compare the performance of different algorithms, we adopt two data selection schemes: sample-based selection and trajectory-based selection. They differ in the smallest unit of selection: the first selects samples in each iteration, while the second selects trajectories.

% As for the trajectory-based selection, prioritized sampling is no loner applicable. As an alternative, we compare with \nameh, which selects trajectories with the highest accumulative reward from the complete dataset. We again compare with the \nameo~as the reference to an upper limit of performance.

\begin{table*}[t]
    \centering
    \begin{tabular}{c|cccc}
    \toprule
    & KRLS & Log-Det & BlockGreedy & \name \\
    \midrule
    Hopper-medium-v0 & 69.4$\pm$2.5 & 58.4$\pm$3.6 & 83.7$\pm$2.2 & \textbf{94.3$\pm$4.6}\\
    Hopper-expert-v0 & 91.0$\pm$1.1 & 90.7$\pm$1.3 & 98.7$\pm$0.5 & \textbf{110.0$\pm$0.5}\\
    Hopper-medium-replay-v0 & 28.5$\pm$3.2 & 29.4$\pm$1.2 & 30.5$\pm$2.4 & \textbf{35.3$\pm$3.2}\\
    Walker2d-medium-v0 & 49.1$\pm$2.8 & 47.5$\pm$3.4 & 53.3$\pm$3.6 & \textbf{80.5$\pm$2.9}\\
    Walker2d-expert-v0 & 68.4$\pm$3.2 & 67.5$\pm$5.6 & 74.8$\pm$3.4 & \textbf{104.6$\pm$2.5}\\
    Walker2d-medium-replay-v0 & 14.3$\pm$1.2 & 15.2$\pm$2.2 & 16.7$\pm$1.3 & \textbf{21.1$\pm$1.8}\\
    Halfcheetah-medium-v0 & 23.4$\pm$0.5 & 21.9$\pm$0.9 & 27.5$\pm$0.7 & \textbf{41.0$\pm$0.2}\\
    Halfcheetah-expert-v0 & 73.9$\pm$1.4 & 72.1$\pm$2.2 & 79.2$\pm$1.8 & \textbf{88.5$\pm$2.4}\\
    Halfcheetah-medium-replay-v0 & 39.5$\pm$0.3 &39.9$\pm$0.5 & 40.5$\pm$1.0 & \textbf{41.1$\pm$0.4}\\
    \bottomrule
    \end{tabular}
    \caption{Experimental results on the D4RL~(Hard) offline datasets. All experiment results were averaged over five random seeds. Our method performs better than the dataset reduction baselines.}
    \label{tab: varied performance}
\end{table*}

\begin{figure}[t]
    \centering
    \subfigure{\includegraphics[scale=0.20]{d4rl/halfcheetah-medium-expert-v0.pdf}}
    \subfigure{\includegraphics[scale=0.20]{d4rl/hopper-medium-v0.pdf}}
    \subfigure{\includegraphics[scale=0.20]{d4rl/hopper-medium-expert-v0.pdf}}
    \subfigure{\includegraphics[scale=0.20]{d4rl/walker2d-medium-expert-v0.pdf}}
    \caption{Experimental results on the D4RL offline datasets. All experiment results were averaged over five random seeds. Our method achieves better or comparable results than the baselines consistently.}
    \label{fig: d4rl original}
\end{figure}

\paragraph{Answer of Question 1:}
To show that \name~can improve offline RL algorithms, we compare \name~with Complete Dataset, Prioritized, and Random in the Mujoco domain.
The experimental results in Figure~\ref{fig: d4rl hard} show that our method achieves superior performance than baselines.
By leveraging the reduced dataset generated from \name, the agent can learn much faster than learning from the complete dataset.
Further, the results in Figure~\ref{fig: d4rl original} show that \name~also performs better than the complete dataset and data selection RL baselines in the standard D4RL datasets. 
This is because prior methods select data in a random or loss-priority manner, which lacks guidance for subset selection and leads to degraded performance for downstream tasks.

In addition, to test \name's generality across various offline RL algorithms on various domains, we also conduct experiments on Antmaze tasks.
We use IQL~\citep{kostrikov2021offline} as the backbone of offline RL algorithms.
The experimental results in Table~\ref{tab: other domain2} show that our method achieves stronger performance than baselines.
In the antmaze tasks, the agent is required to stitch together various trajectories to reach the target location.
In this scenario, randomly removing data could result in the loss of critical data, thereby preventing complete the task.
Differently, \name~extracts valuable subset by balancing data quantity with performance, achieving a stronger performance than the complete dataset.

% In Figure~\ref{fig: d4rl minimal ratio}, we show the performance of different algorithms with the sample-based selection scheme. The experimental results show that \name~can achieve performance close to \nameo~with a small amount of data. For example, we use only $3\%$ of the original dataset in the Hopper tasks. \namer~and \namep, on the other hand, present a stark contrast, even not showing a stable learning trend with the same amount of training data. 
% In addition, we also evaluate the performance on the trajectory-based selection setting. Please refer to Appendix~\ref{appendix: trajectory} for the detailed experimental results.
% For the trajectory-based selection, experimental results in Figure~\ref{fig: d4rl topbc} show that \name~maintains its superiority in this setting with suboptimal (e.g., \texttt{medium}) datasets. This evidence suggests that \name~provides a valuable strategy for selecting data conducive to effective training under conditions of compromised data quality.

\paragraph{Answer of Question 2:}
To test whether \name~can select more valuable data than the data selection algorithms in supervised learning, we compare our method with KRLS~\citep{engel2004kernel}, Log-Det~\citep{seeger2004greedy} and BlockGreedy~\citep{schlegel2017adapting} in the D4RL~(Hard) datasets.
The experimental results in Table~\ref{tab: varied performance} show that our method generally outperforms baselines.
We hypothesize that supervised learning is static with fixed learning objectives, while offline RL's dynamic nature makes the target values evolve with policy updates, complicating the data selection process.
Therefore, the data selection methods in supervised learning cannot be directly applied to offline RL scenarios.

% Additionally, we observe that  $\texttt{Random}$ performs better than $\texttt{Q-diff}$.
% We attribute this phenomenon to the broader data coverage of $\texttt{Random}$, while the data coverage of $\texttt{Q-diff}$ is narrow.
% However, we also note that in some tasks, such as $\texttt{Hopper-medium-expert-v0}$, $\texttt{Hopper-expert-v0}$ and $\texttt{Walker2d-expert-v0}$, $\texttt{Random}$ initially performs well, but as training progresses, its performance starts to decline.
% We find that this coincides with unstable Q-values, which can be attributed to the increased extrapolation error caused by the reduced training dataset.
% In contrast, \name~performs better since it closely approximates the original gradients, thus preventing Q-values from diverging.


% For this reason, when the dataset quality is high~(e.g., \texttt{medium-expert} dataset), TopBC performs comparably to \name.

% \begin{table*}[t]
%     \centering
%     \caption{\name~with varying dataset sizes~($x\%$). Highlighted is the performance comparable to training TD3+BC with the complete dataset. \name~typically achieves good results with 5\%-15\% data, indicating that existing offline RL datasets contain a high degree of redundancy.
%     We adopt the normalized score metric proposed by the D4RL benchmark. Scores roughly range from 0 to 100, where 0 corresponds to the performance of a random policy and 100 indicates the performance of an expert.} 
%     \label{tab: varied performance}
%     \begin{tabular}{c|cccc}
%     \toprule
%         & 5\% & 10\% & 15\% & 20\% \\
%         \midrule
%         Hopper-medium-v0 & 91.8$\pm$3.6 & 92.6$\pm$3.0 & 94.0$\pm$4.8 & 95.2$\pm$1.6\\
%         Walker2d-medium-v0 & 14.8$\pm$7.3 & 57.9$\pm$3.6 & 69.3$\pm$4.0 & 71.7$\pm$1.9 \\
%         Halfcheetah-medium-v0 & 40.5$\pm$0.0 & 40.9$\pm$0.1 & 41.3$\pm$0.1 & 41.2$\pm$0.5 \\
%         Hopper-expert-v0 & 111.6$\pm$0.9 & 110.6$\pm$1.9 & 112.7$\pm$0.1 & 112.4$\pm$0.1 \\
%         Walker2d-expert-v0 & 74.5$\pm$6.4 & 84.4$\pm$5.0 & 97.6$\pm$3.1 & 100.2$\pm$1.0 \\
%         Halfcheetah-expert-v0 & 57.5$\pm$6.4 & 84.3$\pm$2.7 & 97.8$\pm$0.8 & 100.1$\pm$3.0 \\
%         Hopper-medium-expert-v0 & 108.1$\pm$1.1 & 112.4$\pm$0.3 & 112.3$\pm$0.05 & 112.8$\pm$0.1\\
%         Walker2d-medium-expert-v0 & 79.3$\pm$2.1 & 85.4$\pm$5.3 & 96.2$\pm$6.7 & 101.4$\pm$3.6 \\
%         Halfcheetah-medium-expert-v0 & 67.5$\pm$0.5 & 86.2$\pm$5.0 & 85.8$\pm$1.5 & 92.4$\pm$1.3\\
%     \bottomrule
%     \end{tabular}
% \end{table*}


% \subsection{Ablation Study}\label{sec:exp_ab}
% \textbf{Varying dataset size}.\ \ In Table~\ref{tab: varied performance}, we show the performance of \name~with varying dataset sizes ranging from $5\%$ to $20\%$.
% The results demonstrate that \name~requires only $5\%$ or $10\%$ of the original dataset to obtain good performance.
% Further, \name~can achieve similar performance with \nameo~with $20\%$ data of the original dataset.
% This indicates that existing offline RL datasets are characterized by a high degree of redundancy.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.97\linewidth]{visual.jpg}
    \caption{Visualization of the \textcolor{blue}{complete dataset} and the \textcolor{orange}{reduced dataset} in \texttt{halfcheetah} task. The higher opacity of a point represents a large time step towards the end of an episode. The dataset embedding is characterized by its division into different components. 
    % In \texttt{walker2d} (upper), components vary with time steps.
     Samples selected by \name~connect different components by focusing on the data related to the task.}
    \label{fig: t-sne}
\end{figure}

\begin{table}[t]
    \centering 
    \begin{tabular}{c|cccc}
    \toprule
        Env & Random & Prioritized & Complete Dataset & \name\\
        \midrule
        Antmaze-umaze-v0 & 75.1$\pm$2.5 & 70.2$\pm$3.6 & 87.5$\pm$1.3 & \textbf{90.7$\pm$3.3}\\
        Antmaze-umaze-diverse-v0 & 46.3$\pm$1.9 & 44.7$\pm$2.7 & 62.2$\pm$2.0 & \textbf{76.7$\pm$2.2} \\
        Antmaze-medium-play-v0 & 59.3$\pm$1.6 & 60.3$\pm$2.9 & 71.2$\pm$2.2 & \textbf{80.3$\pm$2.9}\\
        Antmaze-medium-diverse-v0 & 43.6$\pm$2.7 & 46.9$\pm$3.8 & 70.0$\pm$1.6 & \textbf{84.9$\pm$3.8}\\
        Antmaze-large-play-v0 &	3.7$\pm$0.7 & 15.0$\pm$3.5 & 39.6$\pm$3.6 & \textbf{46.0$\pm$3.5}\\
        Antmaze-large-diverse-v0 & 16.0$\pm$3.6 & 20.5$\pm$3.7 & 47.5$\pm$1.1 & \textbf{52.0$\pm$3.7}\\
    \bottomrule
    \end{tabular}
    \caption{Experimental results on the Antmaze offline datasets. All experiment results were averaged over five random seeds. Our method performs better than baselines. }
    \label{tab: other domain2}
\end{table}

% \begin{figure*}[t]
%     \centering
%     \subfigure{\includegraphics[scale=0.27]{ablation_moduler1.pdf}}
%     \hspace{0.3cm}\subfigure{\includegraphics[scale=0.27]{ablation_moduler2.pdf}}
%     \caption{Ablation results on D4RL~(Hard) tasks with the normalized score metric.}
%     \label{fig: modular ablation}
% \end{figure*}

% In this subsection, we conduct ablation studies to study the effect of different modules and import hyper-parameters.


\paragraph{Answer of Question 3:}
To study the contribution of each component in our learning framework, we conduct the following ablation study. 
\nameq: We replace the empirical returns used to update Q functions with the standard target Q function in the TD loss function. 
\namei: We set the number of data selection rounds to 1 and study the function of multi-round data selection.
The experimental results in Figure~\ref{fig: modular ablation} in Appendix~\ref{sec: ablation} show that removing any of these two modules will worsen the performance of \name. In case like $\texttt{walker2d-medium}$, ablation \namei~even decrease the performance by over 80\%, and ablation \nameq~results in a 95\% performance drop in $\texttt{walker2d-expert}$. Furthermore, we also find that in the $\texttt{halfcheetah}$ tasks, the impact of removing the two modules is relatively small. This result can be attributable to the fact that this task has a limited state space, and we can directly apply OMP to the entire dataset and identify important and diverse data.

We visualize the selected data by \name~to better understand how it works. 
Figure~\ref{fig: t-sne} displays the t-SNE low-dimensional embeddings, with the complete dataset in blue and the selected data in orange. 
The higher opacity of a point indicates a larger time step. The dataset's structure is revealed by its segmentation into diverse components: 
In \texttt{halfcheetah}, each component reflects a distinct skill of the agent.
For example, from 1 to 7, they represent falling, leg lifting, jumping, landing, leg swapping, stepping, and starting, respectively.
We can observe that the selected samples by \name~ not only cover each component of the dataset but also effectively bridge the gaps between them, enhancing the dataset's versatility and coherence. 
Moreover, we find that \name~is less concerned with the falling data and instead focuses on the data related to the task.
This observation can explain the improved performance of \name. For additional visualizations, please refer to Appendix~\ref{appendix: visual}.

% \textbf{Generalizability of \name}. \ \
% We evaluate the generalizability of \name~from two perspectives.
% First, we add IQL~\cite{kostrikov2021offline} as a baseline and apply \name~to IQL by using the gradient of the training loss of the V-function in IQL as the criterion.
% On the other hand, we evaluate \name~on the other domains, such as robotic manipulation (Adroit) and sparse reward (Antmaze) tasks.
% The experiments in Appendix~\ref{appendix: other domain} and Appendix~\ref{appendix: other algorithm} show that \name~is not only applicable to other algorithms, such as IQL~\cite{kostrikov2021offline}, but also to other domains.

% \textbf{Generalizability of subset}. \ \
% To test the generalizability of the dataset selected by~\name, we select subset by applying~\name~to TD3+BC.
% Then we evaluate the performance of IQL on the selected subset. 
% The experimental results in Table~\ref{tab: td3bc2iql} in Appendix~\ref{appendix: tb3bc2iql} demonstrate that the selected subset based on TD3+BC is effectively applicable to IQL.

% \textbf{Sensitivity for hyperparameter}. \ \
% We evaluate the performance of \name~with various cluster numbers~(from 1 to 50) and approximation bounds~(from 0.0001 to 0.05).
% The experimental results in Appendix~\ref{appendix: cluster number} and Appendix~\ref{appendix: approx bound} show that the suitable cluster number is between 25 and 50.
% Too few clusters (e.g., less than 5) are detrimental to the algorithm.
% In addition, a smaller approximation bound represents a larger reduced dataset.
% Similar to the ablation of the size of the reduced dataset in Table~\ref{tab: varied performance}, \name~requires only a 0.01 approximation bound to obtain good performance.

\subsection{Computational complexity}
We report the computational overhead of \name~on various datasets. 
All experiments are conducted on the same computational device (GeForce RTX 3090 GPU). 
The results in Appendix~\ref{appendix: computation complexity} indicate that even on datasets containing millions of data points, the computational overhead of our method remains low~(e.g., several minutes).
This low computational complexity can be attributed to the trajectory-based selection technique in Sec.~\ref{sec: offline omp}~(II) and the regularized constraint technique in Sec.~\ref{sec:method:outer}, making our method easily scalable to large-scale datasets. 

% This low computational complexity can be attributed to the batch mechanism designed in section 3.2 (IV), which reduces the computational complexity from $O(MN)$ to $O(|\mathcal{B}|N)$, making our method easily scalable to large-scale datasets. $M, N, |\mathcal{B}|$ are the size of the full dataset, reduced dataset, and batch respectively.

% We conduct t-SNE based dimensionality reduction to the cluster centroids and these five trajectories.
% The experimental results are shown in the , where darker colors indicate moving towards the end of the trajectory.

% From the experimental results, we find that in the walker2d task, \name~ tends to select more low-reward but more diverse data points ~(upper right) while selecting a few high-reward data points~(left and bottom).
% We attribute this phenomenon to the narrow distribution of the high-reward points, allowing us to approximate the original gradients with only a few points. 
% In the halfcheetah task, \name~ connects useful information while ignoring low-quality data~(e.g., data point \texttt{1}).
