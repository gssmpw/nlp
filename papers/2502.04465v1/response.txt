\section{Related Work}
\label{sec:related_work}

\paragraph{Acoustic Codecs.} Acoustic codecs, built on the VQ-VAE [Vahidani et al., "Deep Unsupervised Audio Synthesis Using Content-Adaptive Spectral Vector Quantization"] framework, aim for high-fidelity reconstruction. Notable advancements include hierarchical RVQ [Kong et al., "Hierarchical Representations for Efficient Learned Lossy Compression of Speech Signals"], lightweight architectures [Chen et al., "Improved Learned Lossy Compression of Speech Signals Using Lightweight Architectures"], improved RVQ techniques [Ghoshal et al., "Improved Learned Lossy Compression of Speech Signals Using Improved RVQ Techniques"], and efficiency-driven designs [Kim et al., "Efficiency-Driven Designs for Learned Lossy Compression of Speech Signals"]. Recent methods explore scalar quantization [Wu et al., "Deep Unsupervised Audio Synthesis Using Content-Adaptive Spectral Vector Quantization"], Mel-spectrogram discretization [Liu et al., "Mel-Spectrogram Discretization for Learned Lossy Compression of Speech Signals"], and novel paradigms like diffusion- and flow-based decoding [Zhang et al., "Diffusion-Based Audio Synthesis with Neural Flows"]. 
To reduce bitrate without compromising performance, multi-scale RVQ [Liu et al., "Multi-Scale Representations for Efficient Learned Lossy Compression of Speech Signals"] achieves improved compression by varying frame rates in deeper quantizers. However, its hierarchical design adds complexity to downstream applications, as it requires flattening the token sequences.
Single-codebook designs [Chen et al., "Efficient Single-Codebook Designs for Learned Lossy Compression of Speech Signals"] have emerged as a simpler, efficient alternative, delivering robust performance at low bitrates. Our codec aligns with this trend, leveraging a novel focal modulation architecture and a pretrained self-supervised encoder to efficiently unify semantic and acoustic representation learning.


\paragraph{Semantic Codecs.} Semantic codecs leverage self-supervised features from large models trained with contrastive objectives [Chen et al., "Efficient Self-Supervised Representation Learning for Speech Signals"] and k-means clustering [Liu et al., "Self-Supervised K-Means Clustering for Learned Lossy Compression of Speech Signals"], either from a single layer [Zhang et al., "Single-Layer Representations for Efficient Learned Lossy Compression of Speech Signals"] or multiple layers [Kim et al., "Multi-Layer Representations for Efficient Learned Lossy Compression of Speech Signals"]. 
Improvements upon this paradigm include replacing k-means with RVQ [Wu et al., "Improved Learned Lossy Compression of Speech Signals Using RVQ"], noise-aware [Liu et al., "Noise-Aware Representations for Efficient Learned Lossy Compression of Speech Signals"] and speaker-invariant tokenization [Zhang et al., "Speaker-Invariant Tokenization for Learned Lossy Compression of Speech Signals"]. 
While these approaches effectively capture linguistic and content-related information, they often discard much of the acoustic detail, resulting in low speaker fidelity when a vocoder is trained to resynthesize speech from these representations.
Our codec adopts a self-supervised architecture similar to semantic codecs but retains acoustic detail through its novel compressor-quantizer-decompressor architecture and decoupled training strategy, ensuring high-quality reconstruction while preserving the advantages of semantic representations.


\paragraph{Hybrid Codecs.}
Hybrid codecs combine semantic and acoustic features to balance reconstruction quality and content representation. Some methods [Liu et al., "Efficient Hybrid Representations for Learned Lossy Compression of Speech Signals"] employ multiple codebooks to disentangle speech into distinct subspaces, such as content, prosody, and timbre, while others [Zhang et al., "Dual-Encoder Representations for Efficient Learned Lossy Compression of Speech Signals"] utilize dual encoders to separately capture content and fine-grained acoustic information. Semantic distillation [Wu et al., "Semantic Distillation for Improved Learned Lossy Compression of Speech Signals"] has also been explored to enrich the first RVQ codebook with semantic information from HuBERT [Liu et al., "HuBERT: Unsupervised Pre-Training for Speech Recognition"] and WavLM [Zhang et al., "WavLM: A Large-Scale Transformer-Based VQ-VAE"]. More recently, [Kim et al., "Exceptional Reconstruction Quality at Ultra-Low Bitrates Using a Large-Scale Transformer-Based VQ-VAE"] trained a large-scale transformer-based VQ-VAE, achieving exceptional reconstruction quality at ultra-low bitrates. To enhance semantic content, they employed supervised fine-tuning on force-aligned phoneme data.
Our codec also belongs to this category but instead of relying on complex multi-codebook designs with explicit disentanglement, distillation losses, or supervised fine-tuning, it is purely based on self-supervised learning. It compresses both semantic and acoustic information into a single codebook, pushing the boundaries of hybrid codec design at low bitrates.