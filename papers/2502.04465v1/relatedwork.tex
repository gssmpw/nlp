\section{Related Work}
\label{sec:related_work}

\paragraph{Acoustic Codecs.} Acoustic codecs, built on the VQ-VAE~\cite{vandenord2017vqvae} framework, aim for high-fidelity reconstruction. Notable advancements include hierarchical RVQ~\cite{zeghidour2021soundstream}, lightweight architectures~\cite{defossez2023encodec}, improved RVQ techniques~\cite{kumar2023dac}, and efficiency-driven designs~\cite{yang2023hifi, yong2024ticodec, ai2024apcodec}. Recent methods explore scalar quantization~\cite{mentzer2024finite, yang2024sqcodec}, Mel-spectrogram discretization~\cite{bai2024dmel}, and novel paradigms like diffusion- and flow-based decoding~\cite{wu2024scoredec, yang2024ladiff, pia2024flowmac}.
To reduce bitrate without compromising performance, multi-scale RVQ~\cite{siuzdak2024snac, qiu2024efficient} achieves improved compression by varying frame rates in deeper quantizers. However, its hierarchical design adds complexity to downstream applications, as it requires flattening the token sequences.
Single-codebook designs~\cite{li2024singlecodec, guo2024lscodec, ji2024wavtokenizer, xin2024bigcodec, wu2024ts3codec} have emerged as a simpler, efficient alternative, delivering robust performance at low bitrates. Our codec aligns with this trend, leveraging a novel focal modulation architecture and a pretrained self-supervised encoder to efficiently unify semantic and acoustic representation learning.


\paragraph{Semantic Codecs.} Semantic codecs leverage self-supervised features from large models trained with contrastive objectives~\cite{baevski2020wav2vec2, hsu2021hubert, chen2022wavlm} and k-means clustering~\cite{lloyd1982kmeans} for quantization, either from a single layer~\cite{polyak2021discrete, wang2024selm} or multiple layers~\cite{mousavi2024how, shi2024mmm}.
Improvements upon this paradigm include replacing k-means with RVQ~\cite{huang2024repcodec}, noise-aware~\cite{messica24nast} and speaker-invariant tokenization~\cite{chang2024dcspins}. 
While these approaches effectively capture linguistic and content-related information, they often discard much of the acoustic detail, resulting in low speaker fidelity when a vocoder is trained to resynthesize speech from these representations.
Our codec adopts a self-supervised architecture similar to semantic codecs but retains acoustic detail through its novel compressor-quantizer-decompressor architecture and decoupled training strategy, ensuring high-quality reconstruction while preserving the advantages of semantic representations.


\paragraph{Hybrid Codecs.}
Hybrid codecs combine semantic and acoustic features to balance reconstruction quality and content representation. Some methods~\cite{ju2024facodec, jiang2024unicodec, zheng2024freecodec} employ multiple codebooks to disentangle speech into distinct subspaces, such as content, prosody, and timbre, while others~\cite{liu2024semanticodec} utilize dual encoders to separately capture content and fine-grained acoustic information. Semantic distillation~\cite{zhang2024speechtokenizer, defossez2024moshi} has also been explored to enrich the first RVQ codebook with semantic information from HuBERT~\cite{hsu2021hubert} and WavLM~\cite{chen2022wavlm}. More recently, \citet{parker2024scaling} trained a large-scale transformer-based VQ-VAE, achieving exceptional reconstruction quality at ultra-low bitrates. To enhance semantic content, they employed supervised fine-tuning on force-aligned phoneme data.
Our codec also belongs to this category but instead of relying on complex multi-codebook designs with explicit disentanglement, distillation losses, or supervised fine-tuning, it is purely based on self-supervised learning. It compresses both semantic and acoustic information into a single codebook, pushing the boundaries of hybrid codec design at low bitrates.