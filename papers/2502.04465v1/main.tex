%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{xspace}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% Custom packages
\usepackage{multirow, makecell}
\usepackage{enumitem}


% Custom commands
\makeatletter
\newcommand*{\eg}{e.g.\@\xspace}
\newcommand*{\ie}{i.e.\@\xspace}
\newcommand*{\etal}{et al.\@\xspace}
\newcommand*{\etc}{etc.\@\xspace}
\makeatother


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks}

\begin{document}
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}

\makeatletter
\renewcommand{\paragraph}{%
  \@startsection{paragraph}{4}%
  {\z@}{0.25ex \@plus 3ex \@minus .2ex}{-1em}%
  {\normalfont\normalsize\bfseries}%
}
\makeatother


\twocolumn[
\icmltitle{FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks}
% FocalCodec: A Low-Bitrate Single-Codebook Speech Codec \\ with Rich Semantic and Acoustic Representations

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Luca {Della Libera}}{concordia,mila}
\icmlauthor{Francesco Paissan}{fbk,mila}
\icmlauthor{Cem Subakan}{laval,concordia,mila}
\icmlauthor{Mirco Ravanelli}{concordia,mila}
%\icmlauthor{Firstname4 Lastname4}{sch}
%\icmlauthor{Firstname5 Lastname5}{yyy}
%\icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
%\icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
%\icmlauthor{Firstname8 Lastname8}{sch}
%\icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{concordia}{Concordia University, Montreal, Canada}
\icmlaffiliation{mila}{Mila-Quebec AI Institute, Montreal, Canada}
\icmlaffiliation{fbk}{Fondazione Bruno Kessler, Trento, Italy}
\icmlaffiliation{laval}{Université Laval, Québec, Canada}

\icmlcorrespondingauthor{Luca {Della Libera}}{luca.dellalibera@mail.concordia.ca}
%\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Speech coding, Discrete representations, Vector quantization, Generative models}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Large language models have revolutionized natural language processing through self-supervised pretraining on massive datasets.
Inspired by this success, researchers have explored adapting these methods to speech by discretizing continuous audio into tokens using neural audio codecs. However, existing approaches face limitations, including high bitrates, the loss of either semantic or acoustic information, and the reliance on multi-codebook designs when trying to capture both, which increases architectural complexity for downstream tasks.
To address these challenges, we introduce FocalCodec, an efficient low-bitrate codec based on focal modulation that utilizes a single binary codebook to compress speech between 0.16 and 0.65 kbps.
FocalCodec delivers competitive performance in speech resynthesis and voice conversion at lower bitrates than the current state-of-the-art, while effectively handling multilingual speech and noisy environments.
Evaluation on downstream tasks shows that FocalCodec successfully preserves sufficient semantic and acoustic information, while also being well-suited for generative modeling. Demo samples, code and checkpoints are available at \href{https://lucadellalib.github.io/focalcodec-web/}{https://lucadellalib.github.io/focalcodec-web/}.
\end{abstract}

\newcommand{\cem}[1]{{\color{blue}\textit{cem: #1}}}
\newcommand{\fp}[1]{{\color{teal}\textit{fp: #1}}}

\section{Introduction}
\label{sec:introduction}
Recent advancements in large language models~\cite{openai2023gpt4, chowdhery2024palm, jiang2024mixtralexperts, dubey2024llama3herdmodels} have led to significant progress in natural language processing, enabling breakthroughs in tasks such as summarization, translation, question answering, code generation, and retrieval. Building on this success, the research community has extended these methods to other modalities, with speech emerging as a major area of interest. The impressive performance of text-conditioned audio and speech generation models~\cite{borsos2023audiolm, copet2023musicgen, kreuk2023audiogen, wang2023valle, kim2024clamtts}, along with recent speech language models~\cite{zhang2023speechgpt, hassid2023textually, defossez2024moshi, nguyen2024spiritlminterleavedspokenwritten}, highlights the potential of token-based approaches for speech processing.

A key component of these pipelines is the \emph{neural audio codec}, which compresses speech into tokens that downstream models can process. These tokens must preserve acoustic and semantic information to ensure effective representations for downstream tasks while maintaining high reconstruction quality. Another important requirement is a low token rate. As sequence length increases, capturing long-term dependencies becomes more challenging, and computational costs increase.

Despite recent progress, current codecs still face several challenges. Acoustic codecs~\cite{defossez2023encodec, kumar2023dac, ji2024wavtokenizer, xin2024bigcodec} achieve high-quality reconstruction but often rely on multiple codebooks, adding complexity to the design of downstream models. Additionally, they typically lack strong semantic representations. Hybrid codecs~\cite{zhang2024speechtokenizer, liu2024semanticodec, defossez2024moshi, parker2024scaling} aim to combine both acoustic and semantic information while maintaining high-quality resynthesis. Still, they often depend on complex multi-codebook designs, explicit disentanglement, distillation losses, or supervised fine-tuning. Single-codebook designs~\cite{li2024singlecodec, guo2024lscodec, ji2024wavtokenizer, xin2024bigcodec, wu2024ts3codec} offer a simpler architecture but struggle to balance compression while maintaining both reconstruction quality and effective representations for downstream tasks, especially at low bitrates.
To address these limitations, we introduce FocalCodec, an efficient low-bitrate codec based on focal modulation~\cite{yang2022focalnets} that compresses speech into the space of a single binary codebook. FocalCodec achieves competitive performance in reconstruction at lower bitrates than the current state-of-the-art under a variety of conditions while also preserving sufficient semantic and acoustic information for downstream tasks.


\begin{figure*}[t!]
  \centering
\includegraphics[width=0.85\textwidth]{figures/focalcodec.pdf}
   \vspace{-0.2cm}
  \caption{FocalCodec architecture. The encoder extracts features containing both acoustic and semantic information. These features are then mapped to a low-dimensional space by the compressor, binary quantized, and projected back by the decompressor. The decoder resynthesizes the waveform from these features.}
  \label{fig:focaldodec}
  \vskip -0.15in
\end{figure*}


Our contributions are as follows:
\begin{itemize}[topsep=0pt, leftmargin=15pt]
    \item We introduce FocalCodec, a novel hybrid codec featuring a unique compressor-quantizer-decompressor architecture that compresses speech using a single binary codebook at ultra-low bitrates (0.16 to 0.65 kbps).
    \item We propose a focal modulation-based architecture with strong inductive biases for speech, offering an efficient and scalable solution for tokenization.
    \item We demonstrate the versatility of FocalCodec through comprehensive evaluations of reconstruction quality and performance in downstream tasks, highlighting its potential for both discriminative and generative speech modeling.
\end{itemize}
Demo samples, code and checkpoints are available on our project page\footnote{\href{https://lucadellalib.github.io/focalcodec-web/}{https://lucadellalib.github.io/focalcodec-web/}}.




\section{Related Work}
\label{sec:related_work}

\paragraph{Acoustic Codecs.} Acoustic codecs, built on the VQ-VAE~\cite{vandenord2017vqvae} framework, aim for high-fidelity reconstruction. Notable advancements include hierarchical RVQ~\cite{zeghidour2021soundstream}, lightweight architectures~\cite{defossez2023encodec}, improved RVQ techniques~\cite{kumar2023dac}, and efficiency-driven designs~\cite{yang2023hifi, yong2024ticodec, ai2024apcodec}. Recent methods explore scalar quantization~\cite{mentzer2024finite, yang2024sqcodec}, Mel-spectrogram discretization~\cite{bai2024dmel}, and novel paradigms like diffusion- and flow-based decoding~\cite{wu2024scoredec, yang2024ladiff, pia2024flowmac}.
To reduce bitrate without compromising performance, multi-scale RVQ~\cite{siuzdak2024snac, qiu2024efficient} achieves improved compression by varying frame rates in deeper quantizers. However, its hierarchical design adds complexity to downstream applications, as it requires flattening the token sequences.
Single-codebook designs~\cite{li2024singlecodec, guo2024lscodec, ji2024wavtokenizer, xin2024bigcodec, wu2024ts3codec} have emerged as a simpler, efficient alternative, delivering robust performance at low bitrates. Our codec aligns with this trend, leveraging a novel focal modulation architecture and a pretrained self-supervised encoder to efficiently unify semantic and acoustic representation learning.


\paragraph{Semantic Codecs.} Semantic codecs leverage self-supervised features from large models trained with contrastive objectives~\cite{baevski2020wav2vec2, hsu2021hubert, chen2022wavlm} and k-means clustering~\cite{lloyd1982kmeans} for quantization, either from a single layer~\cite{polyak2021discrete, wang2024selm} or multiple layers~\cite{mousavi2024how, shi2024mmm}.
Improvements upon this paradigm include replacing k-means with RVQ~\cite{huang2024repcodec}, noise-aware~\cite{messica24nast} and speaker-invariant tokenization~\cite{chang2024dcspins}. 
While these approaches effectively capture linguistic and content-related information, they often discard much of the acoustic detail, resulting in low speaker fidelity when a vocoder is trained to resynthesize speech from these representations.
Our codec adopts a self-supervised architecture similar to semantic codecs but retains acoustic detail through its novel compressor-quantizer-decompressor architecture and decoupled training strategy, ensuring high-quality reconstruction while preserving the advantages of semantic representations.


\paragraph{Hybrid Codecs.}
Hybrid codecs combine semantic and acoustic features to balance reconstruction quality and content representation. Some methods~\cite{ju2024facodec, jiang2024unicodec, zheng2024freecodec} employ multiple codebooks to disentangle speech into distinct subspaces, such as content, prosody, and timbre, while others~\cite{liu2024semanticodec} utilize dual encoders to separately capture content and fine-grained acoustic information. Semantic distillation~\cite{zhang2024speechtokenizer, defossez2024moshi} has also been explored to enrich the first RVQ codebook with semantic information from HuBERT~\cite{hsu2021hubert} and WavLM~\cite{chen2022wavlm}. More recently, \citet{parker2024scaling} trained a large-scale transformer-based VQ-VAE, achieving exceptional reconstruction quality at ultra-low bitrates. To enhance semantic content, they employed supervised fine-tuning on force-aligned phoneme data.
Our codec also belongs to this category but instead of relying on complex multi-codebook designs with explicit disentanglement, distillation losses, or supervised fine-tuning, it is purely based on self-supervised learning. It compresses both semantic and acoustic information into a single codebook, pushing the boundaries of hybrid codec design at low bitrates.




\section{FocalCodec}
\label{sec:focalcodec}

\subsection{Architecture}\label{sec:arc}
The proposed codec is largely based on the VQ-VAE framework but incorporates novel \textbf{compressor} and \textbf{decompressor} modules between the encoder and decoder (see \cref{fig:focaldodec}). The {discriminator} is used only during training and is discarded afterward.


\paragraph{Encoder.}
\label{subsec:encoder}
To build a hybrid codec with a simple design, without relying on distillation losses or multiple encoders, the encoder must capture both acoustic and semantic information. This ensures high-quality reconstructions and expressive tokens for training downstream models.
Self-supervised models like HuBERT and WavLM retain significant acoustic information in their lower layers~\cite{chen2022wavlm}, making them suitable for hybrid codecs. For instance, \citet{baas2023knnvc} show that a high-quality vocoder can be trained using continuous representations from layer-6 of WavLM-large. Following this approach, we use the \textbf{first 6 layers of WavLM-large}\footnote{\href{https://github.com/microsoft/unilm/tree/master/wavlm}{https://github.com/microsoft/unilm/tree/master/wavlm}} as our encoder.
However, effective quantization is critical for approximating continuous representations with sufficient granularity. Standard k-means clustering typically fails to preserve essential acoustic details~\cite{vannieakerk2022soft}. To address this, we introduce the novel compressor-quantizer-decompressor design based on focal modulation, which allows for granular quantization that preserves both semantic and acoustic information.



\paragraph{Compressor.}
The compressor maps the encoder representations to a compact, low-dimensional latent space. Optionally, it can perform temporal downsampling to further reduce the frame rate.
Prior work typically relies on convolutional, recurrent, or transformer-based architectures for compression. In contrast, we introduce a novel \textbf{focal downscaling} module, which combines a downscaling operation with a focal block.
The downscaling step applies a linear projection to compress the feature dimension, while a 1D convolution can be used instead to additionally downsample along the time dimensions. To better capture periodic patterns, we follow \cite{kumar2023dac} and apply Snake activations~\cite{ziyin2020snake} after the projection.

To build a {focal block}, we replace the self-attention mechanism in the standard transformer block with focal modulation.
Focal modulation~\cite{yang2022focalnets} is an efficient alternative to self-attention that enables fine-to-coarse modeling and introduces useful inductive biases such as translation equivariance, explicit input dependency, time and channel specificity, and decoupled feature granularity. While originally designed for image and video processing, these properties also benefit speech modeling~\cite{dellaliber2024focal}. Unlike self-attention, which directly computes token-wise interactions, focal modulation first aggregates the global context and then modulates local interactions based on this aggregated representation. This ensures that interactions are guided by the overall context rather than being dominated by individual tokens.
Formally, focal modulation computes output representation $\mathbf{y}_i$ for each input feature $\mathbf{x}_i$ in sequence $\mathbf{x}_{1:n}$ as:
\begin{equation}
\mathbf{y}_i = q(\mathbf{x}_i) \odot h \left( \sum_{\ell=1}^{L+1} \mathbf{z}^\ell_i \odot \mathbf{g}^\ell_i \right)
\end{equation}
where $q(\cdot)$ and $h(\cdot)$ are linear projections, and $\mathbf{z}^\ell_i \in \mathbf{z}_{1:n}^\ell$ and $\mathbf{g}^\ell_i \in \mathbf{g}_{1:n}^\ell$ are the context and gating vectors at position $i$ and focal level $\ell \in \{1, \dots, L + 1\}$, with $\odot$ denoting element-wise multiplication. The context sequence $\mathbf{z}_{1:n}$ is obtained via a stack of depth-wise convolutions with exponentially increasing kernel sizes to capture dependencies from short to long range, with average pooling applied to the last level feature map to incorporate global information.
Then, for each focal level, a point-wise convolution is used to compute the gating sequence $\mathbf{g}_{1:n}$. This hierarchical approach, operating at multiple granularities, makes focal modulation well-suited for processing speech features, enabling efficient and scalable representation learning in linear time while preserving long-range dependencies.




\paragraph{Quantizer.}
FocalCodec maps latent representations from the compressor into the codebook space of a \textbf{single quantizer}, eliminating the need for hierarchical designs in downstream models. To achieve this, while maintaining both reconstruction quality and efficiency, the quantizer should satisfy the following requirements: 1) given that the original waveform is already significantly compressed into a short sequence of latents, the quantizer must compensate by using a sufficiently large codebook size to reduce the quantization error; 2) the quantizer should make efficient use of the codebook capacity, avoiding under-utilization; 3) code lookup must remain efficient, despite the increased codebook size, to ensure fast inference.

To address these challenges, we employ \textbf{binary spherical quantization} (BSQ)~\cite{zhao2024bsq}, originally introduced for compression of images and videos. To the best of our knowledge, this is the first successful application of binary quantization in the speech domain.
BSQ belongs to the category of lookup-free quantization (LFQ) methods~\cite{yu2024lfq}, \ie it utilizes an implicit codebook, defined as:
\begin{equation}
\mathcal{C} = \left\{-\frac{1}{\sqrt{L}}, \frac{1}{\sqrt{L}}\right\}^L,
\end{equation}
which represents an \(L\)-dimensional hypercube projected onto a unit hypersphere.
The codebook size is determined by the latent representation dimension $L$ as $\lvert \mathcal{C} \rvert = 2^L$. For example, latent representations of dimension 13 correspond to a codebook size of 8192.
The quantization process consists of two steps. First, the input vector $\mathbf{v}$ of dimension $L$ is normalized to lie on the unit hypersphere:
\begin{equation}
\mathbf{u} = \frac{\mathbf{v}}{\, \, \, \left\| \mathbf{v} \right\|_2}.
\end{equation}
Second, binary quantization with a normalization factor of $\sqrt{L}$ is applied independently to each dimension of \(\mathbf{u}\):
\begin{equation}
\hat{\mathbf{u}} = \frac{\mathrm{sign}(\mathbf{u})}{\sqrt{L}},
\end{equation}
where \(\mathrm{sign}(\cdot)\) denotes the sign function, with \(\mathrm{sign}(0)\) remapped to 1 to ensure the output always lies on the hypersphere.
To make the quantization differentiable, we use the straight-through estimator~\cite{bengio2013ste}.
BSQ offers several advantages over traditional quantization methods. First, the parameter-free implicit codebook is lightweight and computationally efficient. Second, empirical evidence~\cite{zhao2024bsq} shows that the binary quantization bottleneck encourages high codebook utilization, even for large values of $L$. Third, the quantization error is bounded, resulting in faster convergence compared to vanilla LFQ, which does not normalize the representations. Finally, tying the codebook size to the latent dimension helps prevent performance degradation in downstream generative models when using larger codebooks~\cite{yu2024lfq}.




\paragraph{Decompressor.}
The decompressor reconstructs the encoder continuous representations from the quantizer output. It closely mirrors the structure of the compressor, with the downscaling layers replaced by upscaling layers.


\paragraph{Decoder.}
Most codecs use symmetric architectures, where the decoder mirrors the encoder. However, some works~\cite{bai2024dmel, ji2024wavtokenizer, liu2024semanticodec} explore asymmetric designs with larger decoders to improve reconstruction quality. In this work, we adopt an asymmetric design but prioritize the encoder, allocating $\sim \text{5}$x more parameters to it than the decoder. We argue that a strong encoder is essential for extracting robust, disentangled representations for downstream tasks. Even with a high compression rate, a smaller decoder can still generate high-quality audio while offering faster inference, which is beneficial for streaming applications.
For the decoder, we choose the more efficient Vocos~\cite{siuzdak2023vocos} architecture over HiFi-GAN~\cite{kong2020hifigan}. Vocos maintains consistent feature resolution and uses inverse STFT for upsampling, minimizing aliasing and improving computational efficiency. The decoder processes features through ConvNeXt~\cite{liu2022convnext} blocks and projects the sequence of hidden representations to Fourier coefficients for waveform reconstruction. The final audio is synthesized using inverse STFT.





\paragraph{Discriminator.}
Following \cite{kong2020hifigan}, we employ a multi-period discriminator and a multi-scale discriminator. This approach slightly differs from prior work~\cite{zeghidour2021soundstream, defossez2023encodec, siuzdak2023vocos, kumar2023dac, ji2024wavtokenizer}, which utilize multi-resolution and/or STFT-based discriminators in place of a multi-scale discriminator. The multi-resolution and STFT-based discriminators are particularly useful for mitigating over-smoothing artifacts in high-frequency components~\cite{kumar2023dac}, which are more critical for music and environmental sounds. Since our focus is on speech (\ie medium frequency range), we stick to the simpler HiFi-GAN setup.



\subsection{Training}
\label{subsec:training_procedure}
The training process consists of two stages. In the \textbf{first stage}, the compressor, quantizer, and decompressor are jointly trained to reconstruct the encoder continuous representations, ensuring that the tokens retain both semantic and acoustic information from the encoder, which is kept frozen.
The training objective includes reconstruction loss and entropy loss.
The reconstruction loss is computed as the squared L2 distance between the reconstructed and original encoder features.
The entropy loss, defined as in \cite{yu2024lfq, zhao2024bsq}, encourages both confident predictions and uniform code utilization.
Note that we omit the commitment loss used in standard VQ, as for BSQ there is no concern of embedding divergence (quantization error is bounded).

In the \textbf{second stage}, the decoder is trained to resynthesize audio from the encoder continuous representations.
This approach enables us to perform this stage in parallel with the first, simplifying the training setup.
The training objective includes adversarial loss, reconstruction loss, and feature matching loss, as in \cite{kong2020hifigan}. However, following \cite{zeghidour2021soundstream}, we use a hinge loss formulation instead of least squares. The reconstruction loss is computed as the L1 distance between the reconstructed and original log-Mel spectrograms, while the feature matching loss is the mean of the distances between the $l$-th feature maps of the $k$-th subdiscriminator.

This decoupled training approach ensures that both semantic and acoustic information are preserved in the tokens, which is crucial for downstream tasks while maintaining high reconstruction quality. If trained end-to-end without additional constraints on the hidden representations (\eg distillation loss), the reconstruction loss prioritizes acoustic features, as observed in \cite{defossez2023encodec,kumar2023dac}.


\section{Experiments}
\label{sec:experiments}

\subsection{FocalCodec}
\label{subsec:focalcodec_training}
We train FocalCodec on LibriTTS~\cite{zen2019libritts}, resampled to 16 kHz.
We train three variants of the model with a codebook size of 8192 and token rates of 50 Hz, 25 Hz, and 12.5 Hz by adjusting the temporal downsampling factors in the compressor layers to (1, 1, 1), (2, 1, 1), and (2, 2, 1), respectively. These patterns are mirrored in the decompressor layers for upsampling.
Information about hyperparameters and training details can be found in \cref{subsec:focalcodec_hyperparameters}.


\subsection{Baselines}
We compare our models to recent state-of-the-art low-bitrate codecs across acoustic, semantic, and hybrid categories. Since the paper focuses on low-bitrate codecs, when multiple quantizers are available, we configure them to achieve a bitrate below 1.50 kbps, ensuring a fair comparison.
For acoustic codecs, we compare against \textbf{EnCodec}~\cite{defossez2023encodec}, \textbf{DAC}~\cite{kumar2023dac}, \textbf{WavTokenizer}~\cite{ji2024wavtokenizer}, and \textbf{BigCodec}~\cite{xin2024bigcodec}. Among these, BigCodec is the current state-of-the-art for low-bitrate speech reconstruction quality~\cite{wu2024ts3codec}. We use the official checkpoints for these models.
We do not include the recent TS3-Codec~\cite{wu2024ts3codec}, which matches BigCodec performance at an even lower bitrate, as it is not publicly available. However, we contacted the authors to request reconstructed samples for comparison. Additional results related to TS3-Codec can be found in \cref{subsec:appendix_ts3_codec}.

For semantic codecs, we adopt the approach introduced in \cite{wang2024selm}, which quantizes layer-6 representations from WavLM-large using k-means clustering with 512 centroids. These representations are fed into a Conformer~\cite{gulati2020conformer} encoder to reconstruct continuous representations, followed by a HiFi-GAN decoder. This baseline, referred to as \textbf{WavLM6-KM}, provides a direct comparison between our codec and another model leveraging WavLM layer-6 features but differing in design and training methodology. Since the code and checkpoints for WavLM6-KM are not publicly available, we reimplemented the model using a subset of LibriSpeech~\cite{panayotov2015librispeech}.
Note that we do not include additional baselines from this category, as semantic codecs typically underperform in terms of reconstruction quality~\cite{parker2024scaling} or require much higher bitrates to be competitive in this regard~\cite{mousavi2024dasb}. Furthermore, most hybrid codecs are already built on top of semantic representations. Therefore, we prioritize the hybrid category, to which our codec also belongs.
For hybrid codecs, we compare against \textbf{SpeechTokenizer}~\cite{zhang2024speechtokenizer}, \textbf{SemantiCodec}~\cite{liu2024semanticodec}, \textbf{Mimi}~\cite{defossez2024moshi}, and \textbf{Stable Codec}~\cite{parker2024scaling}, using their official checkpoints.
The configurations and details of each model are summarized in \cref{tab:baselines}. Multiply-accumulate operations per second (MACs) are measured using ptflops\footnote{\href{https://pypi.org/project/ptflops/0.7.4/}{https://pypi.org/project/ptflops/0.7.4/}}.
Additional information about the baselines is provided in \cref{sec:appendix_baselines_codecs}.




\begin{table}[t!]
\setlength{\tabcolsep}{3pt}
\vskip -0.1in
\caption{Compared codecs.}
\label{tab:baselines}
\vskip -0.1in
\begin{center}
\begin{footnotesize}
\resizebox{0.495\textwidth}{!}{%
\begin{tabular}{l|c|c|c|c|c|c|c}
\toprule
\textbf{Codec} & \makecell{\textbf{Bitrate} \\ \textbf{(kbps)}} & \makecell{\textbf{Sample} \\ \textbf{Rate} \\ \textbf{(kHz)}} & \makecell{\textbf{Token} \\ \textbf{Rate} \\ \textbf{(Hz)}} & \makecell{\textbf{Codebooks}} & \makecell{\textbf{Code} \\ \textbf{Size}} & \makecell{\textbf{Params} \\ \textbf{(M)}} & \makecell{\textbf{MACs} \\ \textbf{(G)}} \\
\midrule
EnCodec & 1.50 & 24 & 75.0 & 2 $\times$ 1024 & 128 & 15 & 2 \\
DAC & 1.00 & 16 & 50.0 & 2 $\times$ 1024 & 8 & 74 & 56 \\
WavLM6-KM & 0.45 & 16 & 50.0 & 1 $\times$ 512 & 1024 & 127 & 28 \\
SpeechTokenizer & 1.00 & 16 & 50.0 & 2 $\times$ 1024 & 1024 & 108 & 17 \\
SemantiCodec & 0.65 & 16 & 25.0 & 2 $\times$ 8192 & 1536 & 1033 & 1599 \\
Mimi & 0.69 & 24 & 12.5 & 5 $\times$ 2048 & 256 & 82 & 11 \\
WavTokenizer & 0.48 & 24 & 40.0 & 1 $\times$ 4096 & 512 & 85 & 3 \\
BigCodec & 1.04 & 16 & 80.0 & 1 $\times$ 8192 & 8 & 160 & 61 \\
Stable Codec & 0.70 & 16 & 25.0 & 2 $\times$ 15625 & 6 & 95 & 37 \\
\midrule
\textbf{FocalCodec@50} & 0.65 & 16 & 50.0 & 1 $\times$ 8192 & 13 & 142 & 9 \\
\textbf{FocalCodec@25} & 0.33 & 16 & 25.0 & 1 $\times$ 8192 & 13 & 144 & 9 \\
\textbf{FocalCodec@12.5} & 0.16 & 16 & 12.5 & 1 $\times$ 8192 & 13 & 145 & 8 \\
\bottomrule
\end{tabular}
}
\end{footnotesize}
\end{center}
\vskip -0.3in
\end{table}






\subsection{Speech Resynthesis}
\label{subsec:speech_resythesis}
We evaluate FocalCodec on speech resynthesis, considering both English and multilingual speech. For English speech, we use \textbf{LibriSpeech}~\cite{panayotov2015librispeech} \texttt{test-clean}. For multilingual speech, following \cite{xin2024bigcodec}, we randomly select 100 utterances from each of the 7 foreign languages in \textbf{Multilingual LibriSpeech}~\cite{pratap2020mls} (Dutch, French, German, Italian, Polish, Portuguese, and Spanish), resulting in a total of 700 utterances.
We also consider the more realistic scenario of speech contaminated with environmental noise. For this, we use the test splits of \textbf{VoiceBank}~\cite{valentinibotinhao2016voicebank} and the more challenging \textbf{Libri1Mix}, which is constructed by mixing clean utterances from the first speaker of LibriMix~\cite{cosentino2020librimix} with noise from WHAM!~\cite{wichern2019wham}.




\begin{table}[t!]
\setlength{\tabcolsep}{3.2pt}
\vskip -0.1in
\caption{Clean speech resynthesis.}
\label{tab:speech_clean}
\vskip -0.1in
\begin{center}
\begin{footnotesize}
\resizebox{0.495\textwidth}{!}{%
\begin{tabular}{l|c|c|c|c|c|c}
\toprule
\multirow{3}{*}{\textbf{Codec}} & \makecell{\textbf{UTMOS} $\uparrow$} & \makecell{\textbf{dWER} $\downarrow$} & \makecell{\textbf{Sim}} $\uparrow$ & \makecell{\textbf{Code} \\ \textbf{Usage}} $\uparrow$ & \makecell{\textbf{Norm} \\ \textbf{Entropy}} $\uparrow$ & \makecell{\textbf{RTF} $\uparrow$} \\
\cmidrule{2-7}
& \multicolumn{6}{c}{\textit{LibriSpeech test-clean}} \\
\cmidrule{1-7}
Reference & 4.09 & 0.00 & 100.0 & --- & --- & --- \\
EnCodec & 1.58 & 8.08 & 93.8 & 93.4 & 82.1  & 109 \\
DAC & 1.29 & 20.04  & 89.2 & \textbf{100.0} & 91.7 & 89 \\
WavLM6-KM & 3.75 & 6.20 & 90.0 & 26.4  & 95.4 & 85 \\
SpeechTokenizer & 2.28 & 5.14 & 91.6 & 95.9 & 97.0 & 63 \\
SemantiCodec & 2.91 & 8.97  & 96.0 & 75.9  & 94.4 & 0.62 \\
Mimi & 3.29 & 5.73  & 96.0  & 95.6 & 91.8 & 137 \\
WavTokenizer & 3.78 & 11.55 & 95.4 & \textbf{100.0} & 96.7 & 181 \\
BigCodec & 4.11 & 2.55  & \textbf{98.5} & \textbf{100.0} & 98.6 & 22 \\
Stable Codec  & \textbf{4.32} & 4.97 & 94.7 & 98.5 & 94.7 & 103 \\
\midrule
\textbf{FocalCodec@50} & 4.05 & \textbf{2.18}  & 97.4 & \textbf{100.0} & \textbf{98.9} & 185 \\
\textbf{FocalCodec@25} & 4.14 & 3.30 & 96.3 & 99.8 & 98.4  & 195 \\
\textbf{FocalCodec@12.5} & {4.22} & 7.94  & 93.9 & 98.2 & 97.4 & \textbf{208} \\
\midrule
& \multicolumn{6}{c}{\textit{Multilingual LibriSpeech 700}} \\
\cmidrule{1-7}
Reference & 2.84 & 0.00 & 100.0 & --- & --- & --- \\
EnCodec & 1.33 & 29.60 & 95.5 & 93.4 & 79.2 & 140 \\
DAC & 1.24 & 56.08 & 89.1 & \textbf{100.0} & 90.0 & 97 \\
WavLM6-KM & 2.97 & 44.54 & 89.5 & 28.1 & 0.91 & 125 \\
SpeechTokenizer & 1.55 & 56.32 & 92.0 & 96.1 & 94.0 & 74 \\
SemantiCodec & 1.87 & 36.21 & 97.7 & 76.4 & 94.7 & 0.74 \\
Mimi & 2.08 & 30.96 & 96.7 & 95.9 & 89.0 & 239 \\
WavTokenizer & 2.64 & 49.73 & 97.0 & 97.6 & 95.6 & 290 \\
BigCodec & 2.86 & 15.24 & \textbf{99.1} & \textbf{100.0} & 97.9 & 24 \\
Stable Codec & \textbf{3.47} & 56.99 & 95.9 & 92.9 & 93.8 & 144 \\
\midrule
\textbf{FocalCodec@50} & 2.96 & \textbf{12.57} & 98.3 & \textbf{100.0} & \textbf{98.1} & 269 \\
\textbf{FocalCodec@25} & 3.16 & 19.78 & 97.3 & 99.2 & 97.4 & 292 \\
\textbf{FocalCodec@12.5} & 3.37 & 54.15 & 95.2 & 96.4 & 96.9 & \textbf{296} \\
\bottomrule
\end{tabular}
}
\end{footnotesize}
\end{center}
\vskip -0.2in
\end{table}


\begin{table}[t!]
\setlength{\tabcolsep}{3.2pt}
\vskip -0.05in
\caption{Noisy speech resynthesis.}
\label{tab:speech_noisy}
\vskip -0.1in
\begin{center}
\begin{footnotesize}
\resizebox{0.495\textwidth}{!}{%
\begin{tabular}{l|c|c|c|c|c|c}
\toprule
\multirow{3}{*}{\textbf{Codec}} & \makecell{\textbf{DNSMOS} $\uparrow$} & \makecell{\textbf{dWER} $\downarrow$} & \makecell{\textbf{Sim}} $\uparrow$ & \makecell{\textbf{Code} \\ \textbf{Usage}} $\uparrow$ & \makecell{\textbf{Norm} \\ \textbf{Entropy}} $\uparrow$ & \makecell{\textbf{RTF} $\uparrow$} \\
\cmidrule{2-7}
& \multicolumn{6}{c}{\textit{VoiceBank test}} \\
\cmidrule{1-7}
Reference & 3.56 & 0.00 & 100.0 & --- & --- & --- \\
EnCodec & 2.76 & 28.16 & 87.7 & 77.5 & 78.1 & 44 \\
DAC & 2.72 & 63.90 & 79.8 & 98.7 & 88.4 & 48 \\
WavLM6-KM & 3.06 & 20.67 & 82.9 & 24.8 & 92.3 & 44 \\
SpeechTokenizer & 2.74 & 34.51 & 82.2 & 88.1 & 88.4 & 42 \\
SemantiCodec & 3.13 & 31.46 & 90.6 & 52.4 & 92.6 & 0.28 \\
Mimi & 3.01 & 28.00 & 87.8 & 78.6 & 85.5 & 47 \\
WavTokenizer & 3.09 & 42.12 & 89.8 & 94.8 & 94.0 & 63 \\
BigCodec & 3.19 & 20.67 & \textbf{92.3} & \textbf{99.8} & \textbf{96.8} & 17 \\
Stable Codec & \textbf{3.33} & 20.32 & 88.8 & 75.7 & 95.4 & 39 \\
\midrule
\textbf{FocalCodec@50} & 3.16 & \textbf{8.08} & 91.3 & 98.0 & 96.2 & 80 \\
\textbf{FocalCodec@25} & 3.17 & 11.75 & 90.1 & 89.6 & 96.0 & \textbf{81} \\
\textbf{FocalCodec@12.5} & {3.22} & 27.97 & 84.7 & 77.3 & 95.5 & 79 \\
\midrule
& \multicolumn{6}{c}{\textit{Libri1Mix test}} \\
\cmidrule{1-7}
Reference & 3.73 & 0.00 & 100.0 & --- & --- & --- \\
EnCodec & 2.40 & 55.17 & 86.3 & 84.4 & 78.7 & 97 \\
DAC & 2.40 & 90.92 & 76.6 & 99.1 & 88.8 & 91 \\
WavLM6-KM & 2.87 & 36.60 & 85.9 & 26.8 & 95.5 & 65 \\
SpeechTokenizer & 2.58 & 57.26 & 82.8 & 93.5 & 96.5 & 63 \\
SemantiCodec & 2.67 & 51.18 & 89.9 & 64.7 & 90.8 & 91 \\
Mimi & 2.65 & 49.14 & 89.4 & 90.8 & 90.1 & 104 \\
WavTokenizer & 2.53 & 70.10 & 86.3 & 96.4 & 95.4 & \textbf{165} \\
BigCodec & 2.75 & 53.26 & 88.3 & \textbf{100.0} & 98.2 & 19 \\
Stable Codec & 2.91 & 43.52 & 90.0 & 95.8 & 93.4 & 68 \\
\midrule
\textbf{FocalCodec@50} & \textbf{2.93} & \textbf{27.89} & \textbf{91.6} & \textbf{100.0} & \textbf{98.5} & 155 \\
\textbf{FocalCodec@25} & 2.91 & 34.27 & 90.7 & 99.6 & 97.9 & 161 \\
\textbf{FocalCodec@12.5} & 2.92 & 42.59 & 88.9 & 97.2 & 97.2 & 164 \\
\bottomrule
\end{tabular}
}
\end{footnotesize}
\end{center}
\vskip -0.25in
\end{table}



We evaluate the models using objective metrics. To measure naturalness, we employ \textbf{UTMOS}~\cite{saeki2022utmos} for clean speech and \textbf{DNSMOS}~\cite{reddy2022dnsmos} for noisy speech. Note that we do not include signal-level metrics such as SNR, PESQ~\cite{rix2001pesq}, or STOI~\cite{taal2011stoi}, as these metrics do not correlate well with perceived reconstruction quality~\cite{parker2024scaling, wang2024selm}.
To evaluate speaker fidelity, we compute the cosine similarity (\textbf{Sim}) between speaker embeddings extracted from the reconstructed audio and the target audio. These embeddings are obtained using WavLM-base~\cite{chen2022wavlm} fine-tuned for speaker verification\footnote{\href{https://huggingface.co/microsoft/wavlm-base-sv}{https://huggingface.co/microsoft/wavlm-base-sv}}.
To assess intelligibility, we compute the differential word error rate (\textbf{dWER})~\cite{wang2021dwer}, which measures the difference in word error rate between the reconstructed and target audio, using transcriptions from Whisper small\footnote{\href{https://huggingface.co/openai/whisper-small}{https://huggingface.co/openai/whisper-small}}~\cite{radford2022robust}.
To ensure fairness in evaluation, we do not use more powerful ASR models (\eg Whisper large-v3), as these models can correct pronunciation mistakes and are more robust to noise, potentially hiding flaws in the reconstruction.  
We also report \textbf{code usage}, \ie the ratio of unique tokens used to the codebook size (averaged over codebooks for multi-codebook models), and \textbf{normalized entropy}~\cite{thomas2006elements,parker2024scaling}, where higher values indicate more uniform codebook usage. For inference speed, we measure the real-time factor (\textbf{RTF}), \ie the ratio of the reconstructed audio duration to the processing time. An RTF greater than 1 indicates faster-than-real-time performance, measured on an NVIDIA V100 GPU with 32 GB of memory.





\paragraph{Discussion}
Results are presented in \cref{tab:speech_clean} and \cref{tab:speech_noisy}. FocalCodec shows strong performance across both clean and noisy speech resynthesis tasks. On clean speech, FocalCodec@50 achieves the best trade-off of quality, intelligibility, and efficiency. Notably, FocalCodec is the best in terms of dWER, surpassing BigCodec, which is currently state-of-the-art. It also generalizes well to multilingual speech, obtaining the lowest dWER and high Sim. Note that FocalCodec, WavLM6-KM, SpeechTokenizer, BigCodec and Stable Codec were trained exclusively on English speech. In noisy speech resynthesis, FocalCodec@50 again excels, achieving the lowest dWER by a large margin on both VoiceBank and Libri1Mix, while maintaining high speaker similarity. Meanwhile, FocalCodec@25 and FocalCodec@12.5 exhibit some degradation in dWER and speaker similarity, particularly in multilingual settings, due to their significantly lower bitrates. Nevertheless, despite operating at just 0.16 kbps, FocalCodec@12.5 remains competitive with several baselines that use much higher bitrates (\eg EnCodec).  
Finally, the high code usage and normalized entropy across all {FocalCodec} variants indicate efficient token utilization, contributing to their strong overall performance.
Additional results on reconstruction quality, including subjective evaluations, can be found in \cref{subsec:appendix_subjective_evaluation}.







\begin{table}[t!]
\setlength{\tabcolsep}{3.2pt}
\vskip -0.075in
\caption{One-shot voice conversion.}
\label{tab:voice_conversion}
\vskip 0.05in
\begin{center}
\begin{footnotesize}
\resizebox{0.36\textwidth}{!}{%
\begin{tabular}{l|c|c|c|c}
\toprule
\multirow{3}{*}{\textbf{Codec}} & \makecell{\textbf{UTMOS} $\uparrow$} & \makecell{\textbf{dWER} $\downarrow$} & \makecell{\textbf{Sim}} $\uparrow$ & \makecell{\textbf{RTF} $\uparrow$} \\
\cmidrule{2-5}
& \multicolumn{4}{c}{\textit{VCTK}} \\
\cmidrule{1-5}
Reference & 4.09 & 0.00 & 100.0 & --- \\
EnCodec & 1.24 & 86.52 & 72.2 & 57 \\
DAC & 1.25 & 104.00 & 67.2 & 60 \\
WavLM6-KM & 2.90 & 26.68 & 92.4 & 57 \\
SpeechTokenizer & 1.49 & \textbf{20.32} & 81.2 & 33 \\
SemantiCodec & 2.02 & 106.00 & 72.8 & 0.60 \\
Mimi & 2.40 & 110.00 & 89.7 & 71 \\
WavTokenizer & 3.13 & 43.15 & 73.4 & 89 \\
BigCodec & 1.31 & 99.96 & 68.9 & 13 \\
Stable Codec & \textbf{3.76} & 27.63 & 71.1 & 65 \\
\midrule
\textbf{FocalCodec@50} & 3.38 & 21.27 & 92.2 & 116 \\
\textbf{FocalCodec@25} & 3.40 & 23.59 &  \textbf{92.6} & \textbf{118} \\
\textbf{FocalCodec@12.5} & {3.43} & 29.93 & \textbf{92.6} & 117 \\
\bottomrule
\end{tabular}
}
\end{footnotesize}
\end{center}
\vskip -0.25in
\end{table}




\subsection{Voice Conversion}
We conduct one-shot voice conversion experiments to verify that FocalCodec can effectively disentangle speaker information from content despite its single-codebook design.  
This task involves converting speech from a source speaker to an arbitrary target speaker using reference speech from the target speaker.  
For single-codebook baselines, including FocalCodec, we use $k$-nearest neighbors search in the codec feature space, as in \cite{baas2023knnvc}. Specifically, we replace each frame in the reconstructed feature sequence (right before the decoder) with the average of the $k = \text{4}$ closest matches in terms of cosine distance from \emph{continuous} features extracted from the reference.
For multi-codebook baselines, instead, we follow the procedure in \cite{zhang2024speechtokenizer}. The source and reference speech are tokenized, and the first codebook tokens from the source are concatenated with the second-to-last codebook tokens from the reference. The resulting sequence is then forwarded to the decoder.
If sequence lengths differ, the reference is truncated or circularly padded as needed.
Effective disentanglement of content and speaker information between first and subsequent codebooks is expected to yield fair voice conversion performance.
We conduct voice conversion experiments on \textbf{VCTK}~\cite{veaux2017cstr}, which includes parallel utterances from different speakers. To create the test set, we randomly select an utterance from a source speaker, the corresponding utterance from a target speaker, and an utterance with different content from the same target speaker to act as the reference. Among available reference utterances, we select the longest to minimize padding issues. We repeat this process for each speaker, for each of the $\sim \text{24}$ parallel utterances, resulting in a dataset with 2521 samples.
To evaluate performance, we use UTMOS, dWER, Sim, and RTF as defined in \cref{subsec:speech_resythesis}.

\paragraph{Discussion}
As reported in \cref{tab:voice_conversion}, FocalCodec achieves the highest speaker similarity while maintaining good intelligibility, confirming its suitability for voice conversion tasks. This is particularly impressive, especially compared to other hybrid codecs like SpeechTokenizer and Mimi, which are explicitly optimized to disentangle semantic information in the first codebook and acoustic information in the following. Despite this, FocalCodec outperforms these models, excelling in both speaker identity preservation and intelligibility, striking a remarkable balance of quality, efficiency, and speaker similarity. WavLM6-KM ranks as the second-best performing model, which is expected since it shares the same encoder as FocalCodec. In contrast, acoustic codecs struggle with this task, as they do not separate speaker and content information.


 



\begin{table*}[t!]
\setlength{\tabcolsep}{8pt}
\vskip -0.1in
\caption{Evaluation on downstream tasks.}
\label{tab:downstream}
\vskip -0.1in
\begin{center}
\begin{footnotesize}
\resizebox{1.0\textwidth}{!}{%
\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c|c|c}
\toprule
\multirow{4}{*}{\textbf{Codec}} & \multicolumn{3}{c|}{\textbf{Discriminative Tasks}} & \multicolumn{9}{c}{\textbf{Generative Tasks}} \\
\cmidrule{2-13}
& \multicolumn{1}{c|}{\textbf{ASR}} & \multicolumn{1}{c|}{\textbf{SI}} & \multicolumn{1}{c|}{\textbf{SER}} & \multicolumn{3}{c|}{\textbf{SE}} & \multicolumn{3}{c|}{\textbf{SS}} & \multicolumn{3}{c}{\textbf{TTS}} \\
\cmidrule{2-13}
 & \textbf{WER} $\downarrow$ & \textbf{ER} $\downarrow$ & \textbf{ER} $\downarrow$ & \textbf{DNSMOS} $\uparrow$ & \textbf{dWER} $\downarrow$ & \makecell{\textbf{Sim}} $\uparrow$ & \textbf{DNSMOS} $\uparrow$ & \textbf{dWER} $\downarrow$ & \makecell{\textbf{Sim}} $\uparrow$ &
 \textbf{UTMOS} $\uparrow$ & \textbf{dWER} $\downarrow$ & \makecell{\textbf{Sim}} $\uparrow$ \\
\midrule
Reference & --- & --- & --- & 3.56 & 0.00 & 100.0 & 3.77 & 0.00 & 100.0 & 4.09 & 0.00 & 100.0  \\
EnCodec & 27.89 & 3.00 & 47.00 & 3.11 & 37.10 & 85.9 & 3.11 & 78.51 & 87.3 & 1.69 & 74.07 & 79.1 \\
DAC & 35.89 & 3.27 & 45.90 & 3.03 & 67.65 & 81.7 & 2.76 & 106.00 & 83.3 & 1.36 & 61.11 & 84.1 \\
WavLM6-KM & 19.04 & 22.30 & 42.90 & 3.52 & 22.85 & 83.6 & 3.49 & 76.91 & 85.0 & 3.71 & 48.51 & 88.2 \\
SpeechTokenizer & \textbf{14.97} & 2.73 & \textbf{41.50} & 3.21 & 29.82 & 85.9 & 3.13 & 83.99 & 87.3 & 2.63 & 47.81 & 88.3 \\
SemantiCodec & 41.42 & 15.90 & 51.60 & \textbf{3.59} & 102.00 & 83.3 & 3.59 & 123.00 & 84.4 & 2.72 & 59.85 & 90.8 \\
Mimi & 22.98 & 5.43 & 44.70 & 3.30 & 53.98 & 84.6 & 3.41 & 93.23 & 88.1 & 3.05 & 39.50 & \textbf{93.3} \\
WavTokenizer & 35.62 & 2.44 & 49.80 & 3.41 & 51.75 & 88.6 & 3.54 & 105.00 & 86.4 & 3.65 & 59.22 & 89.6 \\
BigCodec & 26.41 & \textbf{2.34} & 47.50 & 3.52 & 26.68 & \textbf{93.2} & 3.54 & 89.24 & \textbf{89.4} & 3.24 & 63.83 & 87.8 \\
Stable Codec & 16.85 & 16.50 & 46.54 & 3.55 & 35.57 & 82.8 & 3.61 & 103.00 & 78.2 & 2.86 & 56.97 & 84.3 \\ 
\midrule
\textbf{FocalCodec@50} & 17.63 & 4.48 & 45.60 & 3.47 & \textbf{10.93} & 91.4 & \textbf{3.71} & \textbf{73.87} & 89.0 & 4.05 & 39.58 & 92.9 \\
\textbf{FocalCodec@25} & 21.12 & 6.07 & 46.80 & 3.49 & 14.74 & 90.0 & 3.69 & 99.96 & 85.4 & 4.12 & 30.28 & 91.4 \\
\textbf{FocalCodec@12.5} & 33.24 & 11.69 & 46.30 & 3.58 & 36.98 & 86.9 & 3.57 & 116.00 & 80.8 & \textbf{4.16} & \textbf{29.91} & 91.5 \\
\bottomrule
\end{tabular}
}
\end{footnotesize}
\end{center}
\vskip -0.2in
\end{table*}








\subsection{Downstream Tasks}
\label{subsec:downstream_tasks}
To evaluate the quality of the learned discrete representations, we train downstream models on both discriminative and generative tasks.


\paragraph{Discriminative Tasks.}
We evaluate performance on automatic speech recognition (ASR), speaker identification (SI), and speech emotion recognition (SER).
These tasks allow us to assess token quality along three axes: semantic information retention (ASR), acoustic information retention (SI), and emotion information retention (SER, which requires a non-trivial combination of semantic and acoustic clues).
To focus on the disentanglement of learned representations, we employ shallow downstream models, aiming to stay as close as possible to linear probing. Following \cite{zhang2024speechtokenizer}, we employ a shallow BiLSTM for all tasks.
For \textbf{ASR}, we use LibriSpeech~\cite{panayotov2015librispeech} \texttt{train-clean-100} and \texttt{train-clean-360} for training, \texttt{dev-clean} for validation, and \texttt{test-clean} for testing. The word error rate (WER) is reported.
For \textbf{SI}, we also use LibriSpeech, grouping utterances from \texttt{train-clean-100} and \texttt{train-clean-360} by speaker ID. Data are randomly split into training, validation and test sets in a ratio of 80\% / 10\% / 10\%.
The speaker error rate (ER) is reported.
For \textbf{SER}, we use the IEMOCAP dataset~\cite{busso2008iemocap}, focusing on four emotions: sadness, happiness, anger, and neutral. Sessions 1–4 are used for training, session 5F for validation, and session 5M for testing. The emotion ER is reported.
Details about the model architecture, hyperparameters, and training procedure are provided in \cref{subsec:appendix_downstream}.

\paragraph{Discussion}
\cref{tab:downstream} shows the results. In ASR, FocalCodec@50 achieves the third lowest WER. While SpeechTokenizer and Stable Codec perform slightly better, the former operates at $\sim$1.5x higher bitrate using two codebooks, while the latter was fine-tuned on force-aligned phoneme data to enhance semantic representations. In contrast, our model is purely self-supervised.
In SI, FocalCodec@50 achieves a marginally higher error rate ($\sim$2\%) than codecs such as BigCodec and WavTokenizer. 
However, these models perform significantly worse in ASR due to being trained solely with reconstruction-based objectives.
On the other hand, the purely semantic WavLM-KM6 codec performs competitively in ASR but exhibits the highest ER in SI despite using the same encoder as FocalCodec. This further confirms the effectiveness of our codec design, as it improves WER over WavLM-KM6 while preserving speaker information.
Interestingly, Stable Codec also performs poorly in SI, likely because semantic fine-tuning tends to remove acoustic information from the representations.
In SER, no codec clearly excels, with FocalCodec@50 performing on par with the best models.
Overall, FocalCodec@50 shows competitive performance across all discriminative tasks, rivaling hybrid codecs with more complex multi-codebook designs and higher bitrates. The more compressed variants, FocalCodec@25 and FocalCodec@12.5, still achieve acceptable performance while operating at ultra-low bitrates.




\paragraph{Generative Tasks.}
We evaluate performance on speech enhancement (SE), speech separation (SS), and text-to-speech (TTS). For these tasks, we employ more powerful transformer-based downstream models, focusing on generation quality.
For \textbf{SE} we use VoiceBank~\cite{valentinibotinhao2016voicebank}. To form a validation set, we randomly select two speakers from the training set. The input tokens are extracted from noisy utterances, while the target tokens come from clean utterances. Performance metrics include DNSMOS, dWER, and Sim.
For \textbf{SS}, we use Libri2Mix~\cite{cosentino2020librimix} \texttt{train-100}, \texttt{dev}, and \texttt{test} sets. The setup mirrors that of speech enhancement: input tokens are derived from speech mixtures, while target tokens correspond to the two individual sources.
For \textbf{TTS}, we use LibriSpeech \texttt{train-clean-100} and \texttt{train-clean-360} for training, \texttt{dev-clean} for validation, and \texttt{test-clean} for testing. The input consists of character-based text tokens, while the target tokens are derived from the corresponding utterances. Performance is evaluated using UTMOS, dWER, and Sim.
Details about the model architecture, hyperparameters, and training procedure are provided in \cref{subsec:appendix_downstream}.


\paragraph{Discussion}
From \cref{tab:downstream}, we observe that in SE, FocalCodec@50 significantly outperforms all other baselines in terms of dWER. A similar trend is observed for SS, where FocalCodec@50 is consistently superior to the other baselines. However, the absolute performance is still far from practical utility, likely due to the loss of information crucial for SS during quantization. Additional results can be found in \cref{tab:ss_continuous}.
As with discriminative tasks, FocalCodec@25 and FocalCodec@12.5 show degraded performance, likely due to their ultra-low bitrates. However, this trend is reversed for TTS, with FocalCodec@12.5, followed closely by FocalCodec@25, achieving the best overall results. This can be attributed to the fact that, in autoregressive modeling, shorter sequences reduce the computational burden and simplify the task of predicting the next token. FocalCodec@12.5, operating at a frame rate close to that of text with a single codebook, makes next-token prediction easier and more computationally efficient than other methods. This highlights the importance of having compact, powerful, discrete representations for downstream tasks. It is important to note, however, that we trained on only 460 hours of speech, which explains why TTS performance is not state-of-the-art.








\subsection{Ablation Studies}
Due to limited computational resources, we perform ablation studies on a smaller variant of FocalCodec. This variant is similar to the 50 Hz model, with the main difference being the model size, as detailed in \cref{subsec:focalcodec_hyperparameters}. We focus on the speech resynthesis task.
The results are shown in \cref{tab:ablations}.
Replacing Vocos with HiFi-GAN significantly degrades UTMOS, despite achieving slightly better speaker fidelity. Also note that Vocos offers from 2x to 3x faster inference.
Replacing BSQ with vanilla LFQ results in the degradation of dWER and Sim despite similar UTMOS scores.
Replacing Snake activations with leaky ReLU causes only a minor degradation in performance.
The most significant performance drop occurs when the focal modulation blocks are replaced with Conformer~\cite{gulati2020conformer} blocks, anti-aliased multi-periodicity (AMP)~\cite{lee2023bigvgan} blocks, or linear layers, in this order. This leads to a notable decrease in both dWER and Sim. This analysis further validates our design choices, highlighting the importance of the selected components for achieving optimal performance.




\begin{table}[t!]
\setlength{\tabcolsep}{2.5pt}
\vskip -0.15in
\caption{Ablation studies.}
\label{tab:ablations}
\vskip -0.1in
\begin{center}
\begin{footnotesize}
\resizebox{0.49\textwidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c}
\toprule
\makecell{\textbf{Compression/} \\ \textbf{Decompression} \\ \textbf{Block}} & \makecell{\textbf{Down/Upscale} \\ \textbf{Activation}} & \makecell{\textbf{Quantizer}} & \makecell{\textbf{Decoder}} & \makecell{\textbf{UTMOS} $\uparrow$} & \makecell{\textbf{dWER} $\downarrow$} & \makecell{\textbf{Sim} $\uparrow$} \\
\cmidrule{1-7}
Focal modulation & Snake & BSQ & Vocos & \textbf{4.14} & \textbf{2.54} & 95.3  \\
Focal modulation & Snake & BSQ & \textbf{HiFi-GAN} & 3.73 & \textbf{2.54} & \textbf{95.7} \\
Focal modulation & Snake & \textbf{LFQ} & HiFi-GAN & 3.74 & 2.75 & 95.4 \\
Focal modulation & \textbf{Leaky ReLU} & LFQ & HiFi-GAN & 3.72 & 2.85 & 95.2 \\
\textbf{Conformer} & \textbf{Snake} & LFQ & HiFi-GAN & 3.74 & 3.58 & 94.3 \\
\textbf{AMP} & Snake & LFQ & HiFi-GAN & 3.70 & 4.52 & 94.3 \\
\textbf{Linear} & Snake & LFQ & HiFi-GAN & 2.55 & 9.37 & 82.5 \\
\bottomrule
\end{tabular}
}
\end{footnotesize}
\end{center}
\vskip -0.3in
\end{table}

 
\section{Conclusions}
\label{sec:conclusions}
In this work, we introduced FocalCodec, a low-bitrate single-codebook speech codec that employs a novel architecture based on focal modulation. It delivers competitive performance in speech resynthesis and voice conversion at low and ultra-low bitrates while maintaining robustness across diverse conditions, including multilingual and noisy speech. Furthermore, FocalCodec effectively preserves both semantic and acoustic information and provides powerful discrete representations for generative downstream tasks.



% Acknowledgements should only appear in the accepted version.
%\section*{Acknowledgements}

%\textbf{Do not} include acknowledgements in the initial version of
%the paper submitted for blind review.

%If a paper is accepted, the final camera-ready version can (and
%usually should) include acknowledgements.  Such acknowledgements
%should be placed at the end of the section, in an unnumbered section
%that does not count towards the paper page limit. Typically, this will 
%include thanks to reviewers who gave useful comments, to colleagues 
%who contributed to the ideas, and to funding agencies and corporate 
%sponsors that provided financial support.

%\section*{Impact Statement}
%This paper presents work whose goal is to advance the field of 
%Machine Learning. There are many potential societal consequences 
%of our work, none which we feel must be specifically highlighted here.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{bibliography}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn


\section{Limitations}
\label{sec:appendix_limitations}
The proposed codec comes with some inherent trade-offs.
From an architectural perspective, it is not causal. While it can be streamed in a chunk-wise fashion (see \cref{subsec:appendix_streaming}), its latency remains too high for real-time applications. Future work should explore alternative designs and/or training strategies to enable a fully causal architecture.
Regarding the dataset, aside from the 94,000 hours of pretraining data used for the WavLM encoder, our compressor, quantizer, decompressor, and decoder were trained only on a few hundred hours of clean English speech sampled at 16 kHz. Expanding the dataset to include more data, a broader range of domains (\eg multilingual speech, mixtures, \etc) and increasing the sampling rate (\eg 24 kHz) could further improve quality, robustness, and versatility of the model.
Additionally, the model is limited to speech and does not support high-fidelity reconstruction of music or environmental sounds.



\section{Datasets}
\label{sec:appendix_datasets}
The following datasets were used in this work:
\begin{itemize}
    \item \textbf{LibriSpeech}~\cite{panayotov2015librispeech} is a large-scale corpus of English read speech derived from audiobooks in the LibriVox project. It contains approximately 1000 hours of speech sampled at 16 kHz, with predefined training, validation, and test splits.

    \item \textbf{LibriTTS}~\cite{zen2019libritts} is a corpus designed for text-to-speech research, constructed from the same source as LibriSpeech. It consists of 585 hours of transcribed speech with predefined training, validation, and test splits.

    \item \textbf{Multilingual LibriSpeech}~\cite{pratap2020mls} is an extension of LibriSpeech to multiple languages, including English, German, Dutch, French, Spanish, Italian, Portuguese and Polish. It provides approximately 44,500 hours of transcribed English speech and about 6000 hours from other languages.

    \item \textbf{VoiceBank}~\cite{valentinibotinhao2016voicebank} is a dataset primarily used for speech enhancement, including 11,572 utterances from 28 speakers in the training set (noise at 0 dB, 5 dB, 10 dB, and 15 dB), and 872 utterances from 2 unseen speakers in the test set (noise at 2.5 dB, 7.5 dB, 12.5 dB, and 17.5 dB).

    \item \textbf{LibriMix}~\cite{cosentino2020librimix} is a dataset for speech separation and enhancement, created by mixing LibriSpeech utterances with noise from the WHAM!~\cite{wichern2019wham} corpus. It provides mixtures of two or three speakers at different signal-to-noise ratios.

    \item \textbf{VCTK}~\cite{veaux2017cstr} is a corpus of English speech recordings from 110 speakers with various accents. It is widely used for speaker adaptation, text-to-speech, and voice conversion tasks.

    \item \textbf{IEMOCAP}~\cite{busso2008iemocap} is a dataset designed for emotion recognition, consisting of scripted and improvised dialogues performed by 10 actors. It includes audio, video, and textual transcriptions with emotion labels such as happiness, sadness, and anger.
\end{itemize}




\section{Baselines}
\label{sec:appendix_baselines_codecs}
Additional information about the baseline codecs is provided in \cref{tab:appendix_baselines_codecs}.
For our WavLM6-KM~\cite{wang2024selm} reproduction, we use LibriSpeech \texttt{train-clean-100} and \texttt{train-clean-360}. First, we train a k-means quantizer with 512 centroids on top of layer-6 representations from WavLM-large. We train on audio chunks of 16,000 samples with a large batch size of 512 for improved stability, and we stop training when cluster centroids stop changing significantly.
Then, we train a dequantizer to minimize the L2 loss between quantized and original WavLM features. We employ a Conformer~\cite{gulati2020conformer} encoder with 6 layers, 4 attention heads, a hidden dimension of 512, and a feed-forward layer dimension of 512.
We train on audio chunks of 7040 samples with a batch size of 16.
We use the AdamW~\cite{loshchilov2019adamw} optimizer with an initial learning rate of 0.0005, $\beta_1$ of 0.8, $\beta_2$ of 0.99, weight decay of 0.01, and dropout of 0.1. The learning rate is reduced by a factor of 0.9 if validation loss does not improve within a margin of 0.0025. Gradients are clipped to a maximum L2 norm of 5. Training stops when validation loss does not decrease for several consecutive epochs.
Finally, we train a HiFi-GAN V1~\cite{kong2020hifigan} decoder on audio chunks of 7040 samples with a batch size of 16. We use the AdamW optimizer with an initial learning rate of 0.0002, $\beta_1$ of 0.8, $\beta_2$ of 0.99, and weight decay of 0.01. The learning rate follows an exponential decay schedule with a factor of 0.999. Training continues until perceived audio quality stops improving.



\vspace{-0.7cm}
\begin{table*}[ht!]
\setlength{\tabcolsep}{2pt}
\caption{Baseline codecs.}
\label{tab:appendix_baselines_codecs}
\vskip 0.05in
\begin{center}
\begin{footnotesize}
\resizebox{0.99\textwidth}{!}{%
\begin{tabular}{l|c|c|c|c|c|c}
\toprule
\textbf{Codec} & \textbf{Causal} & \textbf{Training Datasets} & \textbf{Hours} & \textbf{Multilingual} & \textbf{Audio Domain} & \textbf{Checkpoint} \\
\midrule
\multirow{2}{*}{EnCodec~\cite{defossez2023encodec}} & \multirow{2}{*}{Optional} & \multirow{2}{*}{DNS, CommonVoice, AudioSet, FSD50K, Jamendo} & \multirow{2}{*}{17,000+} & \multirow{2}{*}{Yes} & \multirow{2}{*}{General} & \multirow{2}{*}{\href{https://huggingface.co/facebook/encodec_24khz}{encodec\_24khz}} \\
& & & & & \\
\multirow{2}{*}{DAC~\cite{kumar2023dac}} & \multirow{2}{*}{No} & \multirow{2}{*}{DAPS, DNS, CommonVoice, VCTK, MUSDB, Jamendo} & \multirow{2}{*}{10,000+} & \multirow{2}{*}{Yes} & \multirow{2}{*}{General} & \multirow{2}{*}{\href{https://github.com/descriptinc/descript-audio-codec/releases/download/0.0.5/weights_16khz.pth}{weights\_16khz.pth}} \\
& & & & & \\
\multirow{3}{*}{WavLM6-KM~\cite{wang2024selm}} & \multirow{3}{*}{No} & \multirow{3}{*}{\makecell{Subset of LibriSpeech (in addition to Libri-Light, \\ GigaSpeech, and VoxPopuli English for WavLM pretraining)}} & \multirow{3}{*}{\makecell{460 \\ (+ 94,000)}} & \multirow{3}{*}{No} & \multirow{3}{*}{Speech} & \multirow{3}{*}{\href{https://huggingface.co/lucadellalib/discrete-wavlm-codec}{discrete-wavlm-codec}} \\
& & & & & \\
& & & & & \\
\multirow{2}{*}{SpeechTokenizer~\cite{zhang2024speechtokenizer}} & \multirow{2}{*}{No} & \multirow{2}{*}{LibriSpeech} & \multirow{2}{*}{960} & \multirow{2}{*}{No} & \multirow{2}{*}{Speech} & \multirow{2}{*}{\href{https://huggingface.co/fnlp/SpeechTokenizer/speechtokenizer_hubert_avg}{speechtokenizer\_hubert\_avg}} \\
& & & & & \\
\multirow{2}{*}{SemantiCodec~\cite{liu2024semanticodec}} & \multirow{2}{*}{No} & \multirow{2}{*}{\makecell{GigaSpeech, subset of OpenSLR, Million Song Dataset, \\ MedleyDB, MUSDB18, AudioSet, WavCaps, VGGSound}} & \multirow{2}{*}{20,000+} & \multirow{2}{*}{Yes} & \multirow{2}{*}{General} & \multirow{2}{*}{\href{https://huggingface.co/haoheliu/SemantiCodec/tree/main/semanticodec_tokenrate_50}{semanticodec\_tokenrate\_50}} \\
& & & & & \\
\multirow{3}{*}{Mimi~\cite{defossez2024moshi}} & \multirow{3}{*}{Yes} & \multirow{3}{*}{\makecell{Predominantly English speech (in addition to Libri-Light, \\ GigaSpeech, and VoxPopuli English for WavLM pretraining)}} & \multirow{3}{*}{\makecell{7,000,000 \\ (+ 94,000)}} & \multirow{3}{*}{Likely} & \multirow{3}{*}{Speech} & \multirow{3}{*}{\href{https://huggingface.co/kyutai/mimi}{mimi}} \\
& & & & & \\
& & & & & \\
\multirow{2}{*}{WavTokenizer~\cite{ji2024wavtokenizer}} & \multirow{2}{*}{No} & \multirow{2}{*}{\makecell{LibriTTS, VCTK, subset of CommonVoice, \\ subset of AudioSet, Jamendo, MUSDB}} & \multirow{2}{*}{8000} & \multirow{2}{*}{Yes} & \multirow{2}{*}{General} & \multirow{2}{*}{\href{https://huggingface.co/novateur/WavTokenizer-large-unify-40token}{WavTokenizer-large-unify-40token}} \\
& & & & & \\
\multirow{2}{*}{BigCodec~\cite{xin2024bigcodec}} & \multirow{2}{*}{No} & \multirow{2}{*}{LibriSpeech} & \multirow{2}{*}{960} & \multirow{2}{*}{No} & \multirow{2}{*}{Speech} & \multirow{2}{*}{\href{https://huggingface.co/Alethia/BigCodec/resolve/main/bigcodec.pt}{bigcodec.pt}} \\
& & & & & \\
\multirow{2}{*}{Stable Codec~\cite{parker2024scaling}} & \multirow{2}{*}{Optional} & \multirow{2}{*}{Libri-Light, Multilingual LibriSpeech English} & \multirow{2}{*}{105,000} & \multirow{2}{*}{No} & \multirow{2}{*}{Speech} & \multirow{2}{*}{\href{https://huggingface.co/stabilityai/stable-codec-speech-16k}{stable-codec-speech-16k}} \\
& & & & & \\
\bottomrule
\end{tabular}
}
\end{footnotesize}
\end{center}
\vspace{-0.5cm}
\end{table*}



\section{Hyperparameters and Training Details}
\label{sec:hyperparameters}

\subsection{FocalCodec}
\label{subsec:focalcodec_hyperparameters}
The compressor processes 1024-dimensional WavLM features and forwards them through 3 focal downscaling blocks with hidden dimensions of 1024, 512, and 256, respectively. Each block has two focal levels, a window size of 7, a focal factor of 2, and a layer scale initialization of 0.0001. A final projection maps the 256-dimensional hidden states to latent representations of dimension 13, which are then quantized with a binary spherical codebook of 2$^{13}$ $=$ 8192 codes. The decompressor mirrors the compressor, replacing focal downscaling blocks with focal upscaling blocks to reconstruct the 1024-dimensional continuous representations from the quantized latent codes. We use a weight of 1.0 for the reconstruction loss and a weight of 0.1 for the entropy loss. We train on LibriTTS~\cite{zen2019libritts} (585 hours from 2456 speakers) using full utterances rather than fixed-length chunks, which differs from related work. This approach allows us to fully exploit the unlimited receptive field of focal modulation. This is in line with our vision that the encoder should be as powerful as possible to extract high-quality representations, while the decoder can be lightweight and use limited context windows.
For this stage, we use the AdamW~\cite{loshchilov2019adamw} optimizer with an initial learning rate of 0.0005, $\beta_1$ of 0.8, $\beta_2$ of 0.99, and weight decay of 0.01. The learning rate is reduced by a factor of 0.9 if validation loss does not improve within a margin of 0.0025. Gradients are clipped to a maximum L2 norm of 5. Training stops when validation loss does not decrease for several consecutive epochs.

The decoder processes 1024-dimensional WavLM features and forwards them through 8 ConvNeXt blocks with a hidden dimension of 512, a feed-forward dimension of 1536, a kernel size of 7, and padding of 3. For the STFT, we set the FFT size to 1024 samples and the hop length to 320. The feature matching loss is calculated using 80-dimensional log-Mel spectrograms with the same STFT configuration. The discriminator adopts the convolutional architecture introduced in \cite{kong2020hifigan}.
We train on LibriTTS using audio chunks of 7040 samples with a batch size of 16. Due to resource constraints, our training is limited to the \texttt{train-clean-100} split. We found this amount of data sufficient to obtain high-quality reconstructions.
We use the AdamW optimizer with an initial learning rate of 0.0002, $\beta_1$ of 0.8, $\beta_2$ of 0.99, and weight decay of 0.01. The learning rate follows an exponential decay schedule with a factor of 0.999. Training continues until perceived audio quality stops improving, which occurs around 3M steps.

For the smaller variant of FocalCodec used in the ablation studies, we employ the same setup with the following modifications: the hidden sizes in the three focal downscaling blocks are reduced from 1024, 512, 256 to 512, 256, 128; the codebook size is decreased to 1024; the model is trained on LibriSpeech \texttt{train-clean-100} using a batch size of 4.



\subsection{Downstream Tasks}
\label{subsec:appendix_downstream}
\paragraph{Automatic Speech Recognition (ASR).}
The model architecture is a 2-layer BiLSTM with 512-dimensional hidden states. A CTC~\cite{graves2006connectionist} head is stacked on top and trained to predict either characters or BPE units. Experiments use characters and BPE vocabularies of sizes 250, 500, and 1000, with the best result reported. Note that for Mimi and FocalCodec@12.5, training on characters is infeasible due to the low token rate (12.5 Hz), which results in hidden sequences shorter than the target, making them incompatible with CTC loss.
For all models except Mimi, performance improves monotonically with increasing BPE sizes up to 1000, while Mimi achieves the best results with BPE-500.
If the codec employs multiple codebooks, we compute a weighted sum of the embeddings from each codebook, with the weights learned during training, as done in \cite{chen2022wavlm}. The embedding layer is initialized using the discrete embeddings from the codec quantizer.

\paragraph{Speaker Identification (SI).}
The SI setup closely mirrors that of ASR.
The only difference is that the BiLSTM output sequence is aggregated using statistics pooling, followed by a cross-entropy classification head.

\paragraph{Speech Emotion Recognition (SER).}
The SER setup is the same as SI, where only the number of output classes is different.


\paragraph{Speech Enhancement (SE).}
The model architecture is a Conformer~\cite{gulati2020conformer} encoder with 6 layers, 4 attention heads, a model dimension of 512, and a feed-forward layer dimension of 2048. Codecs with multiple codebooks use a weighted sum of embeddings for the input, with independent linear heads for each codebook in the output. The embedding layer is initialized using the discrete embeddings from the codec quantizer.
Training is performed using cross-entropy loss between predicted and target tokens.


\paragraph{Speech Separation (SS).}
The SS setup closely mirrors that of SE. The only difference is that training is performed using cross-entropy loss with permutation invariant training~\cite{kolbaek2017multitalker}, and the number of output heads is doubled to account for predicting two sources in parallel.

\paragraph{Text-to-Speech (TTS).}
The model architecture is an autoregressive Llama 3~\cite{dubey2024llama3herdmodels} decoder with 12 layers, 4 attention heads, 1 key-value head, a model dimension of 512, a feed-forward layer dimension of 2048, and a base RoPE frequency of 10,000. To provide speaker information, we extract speaker embeddings from the target utterance using WavLM-base~\cite{chen2022wavlm}, fine-tuned for speaker verification. The pooled speaker embedding is prepended to the text embeddings to condition the model on speaker identity. The embedding layer is initialized using the discrete embeddings from the codec quantizer. Training is performed with next-token prediction, where the input sequence consists of pooled speaker embedding, text embeddings, and speech token embeddings. The cross-entropy loss is computed only on speech tokens, while the text and speaker embeddings are excluded from loss computation. For inference, we use top-$p$ sampling with $p = \text{0.9}$ and temperature of 1.0.


\paragraph{Training Details.}
For all tasks, we use AdamW~\cite{loshchilov2019adamw} optimizer with a batch size of 16, an initial learning rate of 0.0001, $\beta_1$ = 0.8, $\beta_2$ = 0.99, weight decay of 0.01, and dropout of 0.1. The learning rate is reduced by a factor of 0.9 if validation loss does not improve within a margin of 0.0025. Gradients are clipped to a maximum L2 norm of 0.01. Training stops if validation loss does not decrease for several consecutive epochs.



\section{Implementation and Hardware}
Software for the experimental evaluation was implemented in Python using the SpeechBrain~\cite{ravanelli2021speechbrain, ravanelli2024open} toolkit.
Each model is trained on a single GPU, with the choice between V100 GPUs (16 or 32 GB) and A100 GPUs (40 GB), depending on cluster resource availability.


\section{Additional Results}
\label{sec:additional_results}


\subsection{Subjective Evaluation}
\label{subsec:appendix_subjective_evaluation}
We conduct a subjective test with 21 participants who rate a total of 10 reconstructions from LibriSpeech \texttt{test-clean}. Following prior work~\cite{defossez2023encodec,zhang2024speechtokenizer, liu2024semanticodec, parker2024scaling}, we employ the MUSHRA~\cite{schoeffler2018webmushra} format without hidden anchor.
Listeners compare multiple versions of an example at once, including a labeled reference and a hidden reference. They are asked the following question:
``\textit{Please evaluate the quality proximity between an audio sample and its reference. Please listen carefully to the reference audio and then rate the quality of each test audio clip compared to the reference. Use the scale where 0 indicates no resemblance to the reference, and 100 means perfectly the same as the reference.}"  
Participants were recruited online by sharing a link to the test across various public channels. To keep the subjective test short, we selected a subset of baselines based on their overall performance in objective metrics\footnote{All the baselines are presented on the project page to foster reproducibility and fair comparison.}. As showcased in \cref{fig:mushra}, FocalCodec can achieve extremely low bitrates without compromising performance significantly. We observe that FocalCodec is Pareto-optimal until 0.50 kbps. Its performance is superior to most baselines and remains comparable to BigCodec, Stable Codec and WavTokenizer. Finally, we emphasize that FocalCodec strikes the optimal trade-off between MOS and downstream task performance, balancing perceived audio quality and objective metrics.


%\vspace{-1cm}
\begin{figure*}[ht!]
    %\centering
    \begin{subfigure}
    \centering
    \includegraphics[width=0.47\textwidth]{figures/mos_bps.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}
    \centering
    \includegraphics[width=0.52\textwidth]{figures/mos.pdf}
    \end{subfigure}
    \vspace{-0.3cm}
    \caption{Subjective evaluation from 21 participants averaged over 10 samples. \textbf{Left.} Trade-off between mean opinion score and bitrate. The green dashed line highlights the reference score. \textbf{Right.} Distribution of mean opinion score. The red lines highlight the median. FocalCodec@50 median is marginally lower than BigCodec, Stable Codec and WavTokenizer. However, user preference remains comparable when accounting for variability.}
    \label{fig:mushra}
\end{figure*}



\subsection{Comparison to TS3-Codec}
\label{subsec:appendix_ts3_codec}
TS3-Codec~\cite{wu2024ts3codec} is a recent transformer-only architecture designed for low-bitrate streaming speech coding. Despite its lower bitrate and streamable architecture, it remains competitive with BigCodec, the current state-of-the-art.
Like FocalCodec, it utilizes a single quantizer. However, its fully transformer-based architecture prioritizes reconstruction, focusing on acoustic representations. The model was trained on Libri-Light~\cite{kahn2020libri}. Since the model is not publicly available, we reached out to the authors to obtain reconstructions of the LibriSpeech \texttt{test-clean} for comparison.
\cref{tab:ts3_performance} shows the results. FocalCodec@50 surpasses TS3-Codec across all evaluated metrics, while FocalCodec@25, despite operating at a significantly lower bitrate, still achieves superior performance in terms of UTMOS and dWER. These findings further highlight the effectiveness of the proposed models.


\begin{table*}[h]
\setlength{\tabcolsep}{4pt}
\vskip -0.1in
\caption{Clean speech resynthesis on LibriSpeech \texttt{test-clean}.}
\label{tab:ts3_performance}
\vskip 0.03in
\begin{center}
\begin{footnotesize}
\resizebox{0.8\textwidth}{!}{%
\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c}
\toprule
\textbf{Codec} & \makecell{\textbf{Bitrate} \\ \textbf{(kbps)}} & \makecell{\textbf{Sample} \\ \textbf{Rate} \\ \textbf{(kHz)}} & \makecell{\textbf{Token} \\ \textbf{Rate} \\ \textbf{(Hz)}} & \makecell{\textbf{Codebooks}} & \makecell{\textbf{Code} \\ \textbf{Size}} & \makecell{\textbf{Params} \\ \textbf{(M)}} & \makecell{\textbf{MACs} \\ \textbf{(G)}} & \makecell{\textbf{UTMOS} $\uparrow$} & \makecell{\textbf{dWER} $\downarrow$} & \makecell{\textbf{Sim} $\uparrow$} \\
\midrule
Reference & --- & --- & --- & --- & --- & --- & --- & 4.09 & 0.00 & 100.0 \\
TS3-Codec (X2) & 0.85 & 16 & 50.0 & 1 $\times$ 131072 & 16 & 204 & 8 & 3.84 & 4.51 & 97.1 \\
\midrule
\textbf{FocalCodec@50} & 0.65 & 16 & 50.0 & 1 $\times$ 8192 & 13 & 142 & 9 & 4.05 & \textbf{2.18} & \textbf{97.4} \\
\textbf{FocalCodec@25} & 0.33 & 16 & 25.0 & 1 $\times$ 8192 & 13 & 144 & 9 & {4.14} & 3.30 & 96.3 \\
\textbf{FocalCodec@12.5} & 0.16 & 16 & 12.5 & 1 $\times$ 8192 & 13 & 145 & 8 & \textbf{4.22} & 7.94 & 93.9 \\
\bottomrule
\end{tabular}
}
\end{footnotesize}
\end{center}
\vskip -0.2in
\end{table*}



\newpage

\subsection{Chunk-Wise Streaming Inference}
\label{subsec:appendix_streaming}
Although our codec is non-causal, it can be made streamable via chunked inference. This involves splitting the input signal into fixed-size chunks with a certain amount of overlap to reduce boundary artifacts. To assess the streamability of our codec, we use a left context of 48,000 samples (3 seconds) and an overlap of 250 samples while varying the chunk size. The reconstructed chunks are stitched together using the overlap–add method with linear fade-in/fade-out.
As shown in \cref{tab:streaming}, FocalCodec@50 maintains acceptable performance when streamed with a chunk size of 8000 (500 milliseconds). In contrast, for FocalCodec@25 and FocalCodec@12.5, performance drops significantly. This is because, due to the 2x and 4x downsampling along the time dimension, they require a much larger look-ahead.


\begin{table}[ht!]
\setlength{\tabcolsep}{4pt}
\vskip -0.1in
\caption{Offline vs chunk-wise streaming inference.}
\label{tab:streaming}
\vskip 0.03in
\begin{center}
\begin{footnotesize}
\resizebox{0.45\textwidth}{!}{%
\begin{tabular}{l|c|c|c|c}
\toprule
\multirow{3}{*}{\textbf{Codec}} & 
\multirow{3}{*}
{\makecell{\textbf{Chunk Size}}} & 
\makecell{\textbf{UTMOS} $\uparrow$}
& \makecell{\textbf{dWER} $\downarrow$} & \makecell{\textbf{Sim} $\uparrow$} \\
\cmidrule{3-5}
& & \multicolumn{3}{c}{\textit{LibriSpeech test-clean}} \\
\midrule
\textbf{FocalCodec@50} & Inf & 4.05 & \textbf{2.18} & \textbf{97.4} \\
\textbf{FocalCodec@25} & Inf & 4.14 & 3.30 & 96.3 \\
\textbf{FocalCodec@12.5} & Inf & \textbf{4.22} & 7.94 & 93.9 \\
\midrule
\textbf{FocalCodec@50} & 2000 (125 ms) & 2.17 & 6.06 & 95.9 \\
\textbf{FocalCodec@50} & 4000 (250 ms) & 2.71 & 4.62 & 96.6 \\
\textbf{FocalCodec@50} & 8000 (500 ms) & 3.16 & 4.55 & 96.9 \\
\textbf{FocalCodec@25} & 8000 (500 ms) & 2.95 & 12.17 & 95.6 \\
\textbf{FocalCodec@12.5} & 8000 (500 ms) & 2.84 & 47.43 & 91.8 \\
\bottomrule
\end{tabular}
}
\end{footnotesize}
\end{center}
\vskip -0.2in
\end{table}





\subsection{Speech Separation with Continuous Input}
\label{subsec:appendix_ss}
Although our codec outperforms the baselines in speech separation (see \cref{subsec:downstream_tasks}), its performance remains insufficient for practical use. We hypothesize that this limitation stems not from the lack of expressive power of the continuous representations -- since WavLM is pretrained on simulated noisy and overlapping speech, its lower layers retain the ability to represent mixtures~\cite{chen2022wavlm} -- but rather from the fact that our compressor, quantizer, decompressor, and decoder are not explicitly trained on mixtures. As a result, the quantized representations struggle to disentangle speakers.
To investigate this further, we replaced the discrete input representations with continuous representations from the encoder. We focused on FocalCodec@50 and WavLM-KM6, the top two models in terms of intelligibility of reconstructed speech, both of which share the same encoder (WavLM-large layer-6).
As shown in \cref{tab:ss_continuous}, using continuous representations as input instead of discrete ones significantly improves performance, with FocalCodec@50 outperforming WavLM-KM6 in both dWER and Sim. This suggests that WavLM-large layer-6 retains substantial information relevant to speech separation. Training the quantizer with mixtures could further improve the quality of the learned tokens for this task.



\begin{table}[h]
\setlength{\tabcolsep}{4pt}
\vskip -0.1in
\caption{Discrete vs continuous input features for speech separation.}
\label{tab:ss_continuous}
\vskip 0.03in
\begin{center}
\begin{footnotesize}
\resizebox{0.50\textwidth}{!}{%
\begin{tabular}{l|c|c|c|c}
\toprule
\multirow{3}{*}{\textbf{Codec}} & \multirow{3}{*}{\makecell{\textbf{Input} \\ \textbf{Features}}} & \makecell{\textbf{DNSMOS} $\uparrow$} & \makecell{\textbf{dWER} $\downarrow$} & \makecell{\textbf{Sim} $\uparrow$} \\
\cmidrule{3-5}
& & \multicolumn{3}{c}{\textit{Libri2Mix}} \\
\cmidrule{1-5}
Reference & --- & 3.77 & 0.00 & 100.0 \\
WavLM6-KM & Discrete & 3.49 & 76.91 & 85.0 \\
WavLM6-KM & Continuous & 3.68 & 23.09 & 89.4 \\
\midrule
\textbf{FocalCodec@50} & Discrete  & 3.71 & 73.87 & 89.0 \\
\textbf{FocalCodec@50} & Continuous  & \textbf{3.76} & \textbf{17.35} & \textbf{93.8} \\
\bottomrule
\end{tabular}
}
\end{footnotesize}
\end{center}
\vskip -0.2in
\end{table}


\newpage

\subsection{Mel-Spectrogram Analysis}
\cref{fig:spectrograms} shows examples of reconstructed Mel-spectrograms from LibriSpeech (left) and Libri1Mix (right), using the 3 top-performing codecs. The reconstructed speech from LibriSpeech is almost indistinguishable from the ground truth.
For Libri1Mix, the first row shows audio contaminated with noise, while the second row shows the original clean audio. It can be observed that BigCodec, a purely acoustic codec trained for reconstruction, attempts to reconstruct the noise, resulting in poor intelligibility. In contrast, Stable Codec and FocalCodec, which have semantically meaningful representations, are able to perform basic denoising. Notably, FocalCodec assigns more energy to the frequency bands corresponding to speech, even more than in the original clean audio, leading to improved intelligibility. On the other hand, Stable Codec, while providing good denoising, introduces some artifacts and static noise in the lower part of the spectrogram, which degrades intelligibility.



\begin{figure*}[ht!]
    \centering
    \begin{center}
    \begin{subfigure}{}
    \hspace{-0.15cm}
\includegraphics[width=0.35\textwidth]{figures/librispeech/ls_in.pdf}
    \end{subfigure}
    \begin{subfigure}{}
\includegraphics[width=0.35\textwidth]{figures/librimix/lm_in.pdf}
    \end{subfigure}
    \end{center}
    \begin{center}
    \begin{subfigure}{}
    \hspace{-0.15cm}
    \includegraphics[width=0.35\textwidth]{figures/librispeech/ls_clean_ref.pdf}
    \end{subfigure}
    \begin{subfigure}{}
    \includegraphics[width=0.35\textwidth]{figures/librimix/lm_clean_ref.pdf}
    \end{subfigure}
    \end{center}
    \begin{center}
    \begin{subfigure}{}
    \hspace{0.02cm}
    \includegraphics[width=0.35\textwidth]{figures/librispeech/ls_big.pdf}
    \end{subfigure}
    \begin{subfigure}{}
    \includegraphics[width=0.35\textwidth]{figures/librimix/lm_big.pdf}
    \end{subfigure}
    \vspace{0.2cm}
    \begin{subfigure}{}
    \includegraphics[width=0.35\textwidth]{figures/librispeech/ls_stab.pdf}
    \end{subfigure}
    \begin{subfigure}{}
    \includegraphics[width=0.35\textwidth]{figures/librimix/lm_stab.pdf}
    \end{subfigure}
    \end{center}
    \begin{center}
    \begin{subfigure}{}
    \hspace{-0.0cm}
    \includegraphics[width=0.35\textwidth]{figures/librispeech/ls_foc.pdf}
    \end{subfigure}
    \begin{subfigure}{}
    \includegraphics[width=0.35\textwidth]{figures/librimix/lm_foc.pdf}
    \end{subfigure}
    \vspace{-0.15cm}
    \end{center}
\caption{Reconstructed Mel-spectrograms from LibriSpeech (left) and Libri1Mix (right).}
\label{fig:spectrograms}
\vspace{-0.65cm}
\end{figure*}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
