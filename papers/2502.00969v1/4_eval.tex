\section{Experiments and Results}
%\subsection{Product Catalog Dataset}
\subsection{Experimental Setup}
\label{sec:setup}

%\paragraph{Data.}
We use a subset of the TREC product search dataset\footnote{\url{trec-product-search.github.io}}, which is a cleaned derivative of the Amazon product dataset \cite{mcauley2015image} as our product catalog. As we explained previously, we do not have any available real-user behavior data. We experiment with the Home \& Kitchen, Electronics, and Beauty \& Personal domains, which contain 135k, 54k, and 47k products respectively. 
%As Appendix Figure \ref{fig:stats} shows, each product is attached to several aspect-value pairs.
We remove non-intuitive product aspects, such as ``ASIN'', ``Date First Available'' and ``Is Discontinued By Manufacturer'' and normalize certain aspects, such as ``Brand Name'' to ``Brand'' and ``Colour'' to ``Color''. 
We also normalize the price range and customer review scores into ranges in the format of ``between \$10 and \$20'' and ``higher than 4.5 stars'' respectively. Finally, we discard products with fewer than two features, excluding customer reviews and price ranges.%\shervin{Do we mean EXCLUDING customer review and price range?}
%\shervin{Does the data size from the beginning of the paragraph change after filtering?}

%\paragraph{Models.} We experiment with two different LLMs: GPT-4 and LLaMA-2-70B-chat. As the \textit{chatbot-arena-leaderboard}\footnote{\url{https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard}} shows, GPT-4 is the best-performing LLM so far.
%However, as GPT-4 is proprietary, we also experiment with LLaMA-2-70B-chat, which is one of the best performing commercially available instruction-tuned LLMs. Due to the high GPU memory requirement of LLaMA-2, we use a 5-bit quantized version.\footnote{\url{https://github.com/abetlen/llama-cpp-python}}
%\url{https://huggingface.co/TheBloke/Llama-2-70B-Chat-GGML/resolve/main/llama-2-70b-chat.ggmlv3.q5_K_M.bin}


\subsection{Human Evaluation} 
\label{sec:human_evaluation}
Since there is no gold conversation to evaluate against as we explained \S\ref{sec:related_work}, we perform human evaluation at the conversation and utterance levels to examine the quality of the dialogues.
%\subsection{Evaluation Metrics}%\zhiyu{If we need to save space, we could just summarize metrics at high level (probably a small table) and put detailed explainations in appendix.}
%\jason{We should improve presentation here. Too much texts and hard to parse}
The metrics are summarized in Table \ref{tab:human_evaluation_metrics}. For conversation-level evaluations, judges are asked to score each conversation on a 5-point Likert scale. The definition of each score is shown in Appendix Table \ref{tab:score_interpretation}. Note that the \textit{overall naturalness} of 5 means the generated conversation is \textit{indistinguishable from real-life human conversations}. %\hl{Jason: I think this is too stretch. We never did pairwise test on real vs. generated dialogues}.
We also evaluate the conversations on a finer-grained utterance level where judges identify all unsatisfactory, non-script following utterances, as a binary labeling task. 

%\subsection{Human Evaluation Setting}
We evaluate conversations generated under 4 settings: GPT-4 single-pass generation, GPT-4 interactive generation, LLaMA-2-70B-Chat single-pass, and LLaMA-2-70B-Chat interactive generation. We begin by assessing inter-annotator agreement using Kendall's $\tau$. To achieve this, we provide detailed annotation instructions to expert judges divided into 5 groups, with 3 judges per group. Each judge is assigned 12 examples, spread across 3 domains with 4 configurations per example. Therefore, we have evaluation results of 180
%12 $\times$ 3 $\times$ 5 = 180 
instances for 60
%12 $\times$ 5 = 60 
conversations, allowing us to calculate inter-annotator agreement. Furthermore, we assign two of the judges to work on 24 additional examples each. These examples are divided into 3 domains, with 2 examples per domain, and each example involves 4 configurations. Thus, we evaluate 
%60 + 24 $\times$ 2 = 
108 dialogues in total. %The ratings provided by these judges will be averaged for analysis.\shervin{Why do we need this? The motivation is not clear.} %\zhiyu{Will revisit the human-eval setup on Monday after we finish annotation}

\begin{table}[t]
\small
\begin{center}
\setlength{\tabcolsep}{3pt} % Default value: 6pt
    \begin{tabular}{lp{0.6\linewidth} }
    \hline
\textbf{Metric} & \textbf{Description}\\ \hline
\textit{Conversation-Level} &  \\ \hline
Realism & Whether the conversation is likely to happen in the real world from a logical perspective. \\ \hline
Conciseness & Whether the conversation is too verbose. \\ \hline
Coherence & Whether the conversation is fluent and coherent with respect to the conversation history from the beginning. \\ \hline
Overall naturalness & Overall subjective impression of the conversation and whether it sounds natural. \\ \hline \hline
\textit{Utterance-level} &  \\ \hline
Realism & Whether the utterance is likely to be spoken in the real world from a logical perspective. \\ \hline
Script-following & Whether the utterances are consistent with the product features and customer preferences. \\ \hline
\end{tabular}
\vspace{-1em}
\caption{Human evaluation metrics.}
\label{tab:human_evaluation_metrics}
\end{center}
\vspace{-3em}
\end{table}

% We count the number of unrealistic or non-script-following utterances to measure the realism and script-following nature of utterances in Table \ref{tab:utterance_level_human_evaluation}.

\subsection{Results} \label{sec:human_evaluation_results}
Table \ref{tab:conversation_level_human_evaluation} shows the average scores of conversational-level human evaluation results, and Table \ref{tab:utterance_level_human_evaluation} shows the average number of unsatisfactory utterances marked by the judges. %\shervin{Why average? Why not the proportion of utterances? Does the comparison assume that the dialogues for each input have the same number of turns across the different models? Should we report the avg length per approach?}
As Table \ref{tab:conversation_level_human_evaluation} and \ref{tab:utterance_level_human_evaluation} show, we obtain a clear trend of scores across almost all metrics with high agreements (Average Kendall's $\tau=0.51$ \& $0.44$ respectively) %\shervin{Tau suddenly appears from the first time. It should be introduced and justified in section 4.2.} 
for both conversation-level and utterance-level evaluations. Unsurprisingly, GPT-4 outperforms LLaMA-2-70B-Chat, which is consistent with the existing leaderboard\footnote{\url{https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard}}.
 
The single pass generation approach consistently outperforms the interactive generation for both LLM types. This is because the single pass approach takes the global generation plan for both seller and customer agents and performs dialogue generation with a single pass, thus there are less likely to be contextual conflicts. %\shervin{This explanation is not very clear... the single pass vs interactive finding is a very important one, and needs to be clearly explained.} 
On the contrary, as our error analysis (\ref{sec:error_analysis}, coherence) \& Appendix Table \ref{tab:bad_coherence} show, in the interactive approach, the LLM-powered agents have no access to the generation plan beyond the next utterance. Due to the alternative generation paradigm, there is a greater potential for errors. The only exception is for LLaMA-2-70B-Chat, where the single pass approach has a higher average number of non-script-following utterances than the interactive setting. This is likely because LLaMA-2-70B-Chat tends to follow instructions less closely than GPT-4 when crafting a coherent conversation. %Additionally, it is generally a harder task to verbalize a series of product features in a conversation than to verbalize single product feature in an utterance.

\begin{table}[t] \small
\centering
\setlength{\tabcolsep}{2pt} % Default value: 6pt
\begin{tabular}{llllll}
\hline
\textbf{Model} & \textbf{Strategy} & \textbf{Real} & \textbf{Concise} & \textbf{Coherence} & \textbf{Natural}\\ \hline
GPT-4   & Single pass & \textbf{4.3} & \textbf{4.8} & \textbf{4.7} & \textbf{4.2}\\ \hline
GPT-4   & Interactive & 3.3 & 2.9 & 3.4 & 2.9 \\ \hline
LLaMA-2 & Single pass & 4.1 & 4.7 & 4.1 & 3.9\\ \hline
LLaMA-2 & Interactive & 2.5 & 3.0 & 3.3 & 2.5 \\ \hline

\end{tabular}
\vspace{-1em}
\caption{Conversational-level human evaluation results measured by Realism, Conciseness, Coherence and Naturalness. Average Kendall's $\tau = 0.51$.}
\label{tab:conversation_level_human_evaluation}
\vspace{-0.5em}
\end{table}

\begin{table}[t] \small
\centering
\setlength{\tabcolsep}{3pt} % Default value: 6pt
\begin{tabular}{llcc}
\hline
\textbf{Model} & \textbf{Strategy} & \textbf{Unrealistic $\downarrow$} & \textbf{Not Following $\downarrow$}\\ \hline
GPT-4 & single pass & \textbf{0.8} & \textbf{0.1}\\ \hline
GPT-4 & interactive & 2.0 & 0.4 \\ \hline
LLaMA-2 & single pass & 1.2 & 1.7 \\ \hline
LLaMA-2 & interactive & 2.9 & 1.1 \\ \hline

\end{tabular}
\vspace{-1em}
\caption{Utterance-level human evaluation results in terms of the number of labeled unrealistic and non-script-following utterances. The lower, the better. Average Kendall's $\tau = 0.44$.}
\label{tab:utterance_level_human_evaluation}
\vspace{-2em}
\end{table}

\begin{table}[t] \small
\centering
\setlength{\tabcolsep}{3pt} % Default value: 6pt
\begin{tabular}{cc}
\hline
\textbf{Total \# conv} & 3600\\ \hline
\textbf{Domains} & Electronics\\ 
 & Home \& Kitchen\\ 
 & Beauty \& Personal Care\\ \hline
\textbf{\# Conv per domain} & 1200\\ \hline
\textbf{avg \# utterance per conv} & 19.7\\ \hline
\textbf{avg \# search per conv} & 2.2\\ \hline
\textbf{avg generation time per conv} & 75.6s\\ \hline
\end{tabular}
\vspace{-1em}
\caption{Statistics of \textit{Wizard of Shopping} dataset.}
\label{tab:stats}
\vspace{-2em}
\end{table}

\subsection{Wizard-of-Shopping (WoS) Dataset}%\zhiyu{Move this section after human evaluation. Easier to justify why we generate with single-pass method}
%\jason{We should have a dedicated table for statistics on generated corpus (per domain). This is one of our main contribution. We also need to consider adding a dummy url as a pointer for data release}
%\nikhita{This is a main contribution, so we need more emphasis on the novelty of the dataset, how it differs from other existing datasets, and some additional analysis on the dataset features -- see MGShopDial paper for reference}
%\hl{Jason: This is one of our biggest contribution. Having at least one table about data statistics can help a lot here than saying out in text. With well-designed table, we can potentially free up more space by removing texts}

Since human evaluation results validate that \method with GPT-4 single pass generation can generate high-quality conversations, we propose to release an official, larger scale \textit{WoS} dataset to (1) promote more research in CPS and (2) study the effectiveness of generated datasets in real downstream tasks. Table \ref{tab:stats} shows the statistics of our \textit{WoS} dataset.
%Given the human evaluation results, we use the GPT-4 single pass generation variant of our approach TRACER to collect conversations for our proposed WoS dataset.
%\shervin{Before we describe what we did, we should justify WHY we do this. What is the purpose of the dataset generation and release? Make this clear.}
%We collect 3224 conversations
%We collect 1,200 conversations each for the Electronics, Home \& Kitchen, and Beauty \& Personal Care domains respectively, leading to a total of 3,600 conversations. %On average, each conversation contains 19.7 utterances, has through 2.2 searches %\zhiyu{Does this mean on average DT is called 1.2 times? Based on the algorithm, 1 search for PC then one search in each Decision-Tree call. I would expect conversation would be around 6 turns (1 for stage 1, 1.2 for stage 2, and 3 for stage 3) on average? } \Xiangci{See the updated LaTeX line 431. LLM translates each search results into several turns of conversation, since each search result returns several aspects together, and ideally each turn of conversation only discusses 1~2 product aspects.}
%and takes 75.6 seconds to generate. 
%There are roughly the same number of conversations per product domain: 1000, 1226, and 998 conversations for Electronics, Home \& Kitchen, and Beauty \& Personal Care respectively. 
A typical example of a generated conversation is shown in Appendix Table \ref{tab:good_example}. %\shervin{Consider adding more good examples in the appendix, I only saw problematic ones there.} 
In contrast to prior conversational e-commerce datasets that are either large-scale but unnatural \cite{zou2022learning}, or natural but small-scale \cite{Bernard:2023:SIGIR}, our \textit{WoS} dataset consisting of 3600 conversations is large-scale, and is designed to be human-like and realistic. Further, since our \method approach can be easily applied to other product domains to generate additional conversations, we will continue to expand the \textit{WoS} dataset in terms of quantity and number of product domains\footnote{We will release the dataset and dialogue generation scripts upon acceptance.}.
% \jason{Usually for a dataset release paper, people do much more analysis on the dataset itself. Maybe something to consider if time and space permits}

\subsection{Error Analysis of \textit{WoS} Dataset} \label{sec:error_analysis}
% \jason{let's move all these examples to appendix and bring back up result tables to main sections}
 Despite the high naturalness and coherence of the conversations generated by TRACER, we identify the following errors in them. 
 
 \paragraph{Verbosity.} In the interactive generation setting, the seller agent often ``hurries'' to include all the assigned product aspects in a single lengthy utterance. This behavior is primarily due to the seller agent's inability to anticipate the potential for multiple turns in the conversation, such that it could gradually discuss one or two product aspects per utterance, as instructed in the prompt. Consequently, the interactive setting often produces much lengthier utterances compared to the single-pass generation, adversely impacting realism, conciseness, and naturalness scores. As the examples in Tables \ref{tab:bad_conciseness} \& \ref{tab:bad_coherence} show, the seller tends to produce overly lengthy utterances.

 \paragraph{Coherence.} We encourage the customer agent to spontaneously ask clarification questions in the conversation in order to improve the naturalness of the conversation. In interactive generation settings, when the customer asks clarification questions, the product value selection is usually left unfinished due to the customer's confusion. In such cases, the seller agent usually also successfully responds to the customer's questions. However, the seller does not give the customer a second chance to make a value selection response and instead directly moves to asking about new product aspects (See sentences in bold in Table \ref{tab:bad_coherence}). Consequently, this abrupt transition significantly undermines the coherence, realism, and naturalness of the conversation.

 \paragraph{Bad features.} The extracted aspect-value pairs from the product catalog are not perfect~\cite{putthividhya2011bootstrapped,raju2009unsupervised,yang2022mave}, with some of them being very unnatural, despite the manual cleaning we performed and explained in \S\ref{sec:setup}. When such unnatural or unrealistic product aspects are selected by the decision tree, the LLMs are forced to verbalize them. In such cases, certain utterances may look awkward. For example, the inappropriately selected product aspects (in bold) in Table \ref{tab:bad_feature} make the conversation sound awkward.

 \paragraph{Not following instructions.} The LLMs may not always follow the prompts to verbalize selected product features and preferences faithfully. For example, the seller agent may ask about product aspects that have not been explicitly assigned from the catalog, based on the commonsense understanding of the LLM. Similarly, a customer agent may invent requirements that are not present in the assigned product values from the catalog. Additionally, the customer agent may not be precise in conveying their preferences, sometimes responding with statements like ``this is an optional feature for me'' instead of ``I do not want this feature''. In Table \ref{tab:not_instruction_following}, all the bolded unassigned product features are invented by the LLM. %\zhiyu{The comparison among different methods is unclear. e.g., which is them most instruction-following?  Same for other dimensions discussed.}
 We quantify this issue in the ``Not Following'' column in Table \ref{tab:utterance_level_human_evaluation}.

%\subsection{LLM-Generated vs. Human Generated Dataset}
\subsection{Comparison with Prior Dataset}\label{sec:mgshopdial_comparison}
%We observe that manually writing long and insightful product search conversations is not only time consuming but also very difficult. 

%Our LLM-generated dialogues not only include clarification and general knowledge questions from customers, but also allow the assistant to provide nuanced answers with detailed product knowledge and common-sense knowledge, such as the advantages and disadvantages of certain materials. These aspects make it exceedingly difficult for human annotators to replicate the complexity and naturalness of these conversations. 
%Although to the best of our knowledge, as we survey in \S\ref{sec:related_work}, there is no prior CPS dataset directly comparable to \textit{WoS}, 
This section compares \textit{WoS} to MG-ShopDial \cite{Bernard:2023:SIGIR} which is the only publicly available e-commerce dialogue dataset. % MG-ShopDial \cite{Bernard:2023:SIGIR}, the only human-generated CPR dataset. We use OpenAI GPT-4\footnote{gpt-4-0125-preview} to label each utterance with intents defined by \citet{Bernard:2023:SIGIR}. See more details in Appendix \ref{sec:intent_analysis} and Table \ref{tab:intent}.
\vspace{-0.5em}

\paragraph{Annotation Protocol.} MG-ShopDial is annotated by crowdsourcers, with some annotators acting as shopping assistants and others as customers. Each role is given a checklist of actions to complete, ensuring that the annotated dataset encompasses a variety of intents. While \textit{WoS} is initially generated by \method, we later demonstrate that through careful dialogue planning, our simulated dialogues can also encompass diverse intents. Additionally, because customer preferences are randomly sampled and vary in complexity, each dialogue may have a different ``checklist'', resulting in greater diversity at the corpus level compared to MG-ShopDial.
\vspace{-0.5em}
\paragraph{Product Catalog.} MG-ShopDial offers a curated catalog to annotators, which covers around 14 items per category. However, this limited selection may not accurately reflect the breadth of choices available in real-world shopping scenarios. As detailed in \S\ref{sec:setup}, we index over 236k product and the utterance generation in \textit{WoS} depends on the most recent search results. Therefore, the dialogue generation process of \method closely resembles the iterative ``search-and-refine'' procedure of typical shopping experiences.
\vspace{-0.5em}
%\paragraph{Settings \& strategies.}
\paragraph{Intent.} We use OpenAI GPT-4\footnote{gpt-4-0125-preview} to label each utterance with intents defined by \citet{Bernard:2023:SIGIR}. See more setup details in Appendix \ref{sec:intent_analysis}. As Table \ref{tab:intent} shows, similar to MG-ShopDial, our \textit{WoS} conversations exhibit diverse distributions of intents, which is consistent with the high naturalness observed in \S\ref{sec:human_evaluation_results}. Furthermore, MG-ShopDial has significantly more ``Recommend'' intent than \textit{WoS}, while \textit{WoS} has a dominating proportion of ``Elicit preferences'' intent. This correlates with our observation that the wizard (seller) in MG-ShopDial memorizes all candidate products due to a limited catalog size and starts the product recommendation stage early without explicit product search. In contrast, with the decision tree for dialogue planning, \textit{WoS} conversations aim to narrow down product aspect values for product search from a real product catalog and adopt the strategy of not selecting any product until the search is converged. As a result, \textit{WoS} conversations are significantly shorter than MG-ShopDial (19.7 vs. 34.3 utterances).



%\paragraph{Persona.} As the intents distribution in Figure 2 of \citet{Bernard:2023:SIGIR} shows, MG-ShopDial conversations have meta-communications (greeting \& interaction) throughout the conversation, which provides the wizard with more contextual information for recommendation. For example, the client brings up that he is a photographer when discussing a smartphone's camera, which also makes the conversation more vivid. On the contrary, since \textit{WoS} is generated with fixed prompt templates, even GPT-4-based conversations lack diverse personas across conversations, despite individual conversations being natural.

\vspace{-0.5em}
\paragraph{Conversation styles.} Since MG-ShopDial is generated by humans with keyboard typing, their sentences are usually more brief (8.5 words per utterance), with frequent absence of concluding punctuation. Specifically, only 256 of its 2,196 utterances conclude with a period, while 479 end with a question mark.  
In contrast, utterances in \textit{WoS} tend to be lengthier, averaging 22.8 words per utterance, characterized by near-perfect grammar (as illustrated in Appendix Table \ref{tab:good_example}). %since the LLM never gets tired and lazy. 
As a result, MG-ShopDial conversations tend to be more lengthy and casual than \textit{WoS}.


\paragraph{Expandability.} For integrating additional domains into MG-ShopDial, it is necessary to curate a new list of products and potentially retrain annotators. Our proposed \method can seamlessly extend to generating shopping dialogues for any new domains once the products are indexed.