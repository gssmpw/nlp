@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@inproceedings{eric-etal-2017-key,
    title = "Key-Value Retrieval Networks for Task-Oriented Dialogue",
    author = "Eric, Mihail  and
      Krishnan, Lakshmi  and
      Charette, Francois  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 18th Annual {SIG}dial Meeting on Discourse and Dialogue",
    month = aug,
    year = "2017",
    address = {Saarbr{\"u}cken, Germany},
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-5506",
    doi = "10.18653/v1/W17-5506",
    pages = "37--49",
    abstract = "Neural task-oriented dialogue systems often struggle to smoothly interface with a knowledge base. In this work, we seek to address this problem by proposing a new neural dialogue agent that is able to effectively sustain grounded, multi-domain discourse through a novel key-value retrieval mechanism. The model is end-to-end differentiable and does not need to explicitly model dialogue state or belief trackers. We also release a new dataset of 3,031 dialogues that are grounded through underlying knowledge bases and span three distinct tasks in the in-car personal assistant space: calendar scheduling, weather information retrieval, and point-of-interest navigation. Our architecture is simultaneously trained on data from all domains and significantly outperforms a competitive rule-based system and other existing neural dialogue architectures on the provided domains according to both automatic and human evaluation metrics.",
}

@inproceedings{wu-etal-2020-tod,
    title = "{TOD}-{BERT}: Pre-trained Natural Language Understanding for Task-Oriented Dialogue",
    author = "Wu, Chien-Sheng  and
      Hoi, Steven C.H.  and
      Socher, Richard  and
      Xiong, Caiming",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.66",
    doi = "10.18653/v1/2020.emnlp-main.66",
    pages = "917--929",
    abstract = "The underlying difference of linguistic patterns between general text and task-oriented dialogue makes existing pre-trained language models less useful in practice. In this work, we unify nine human-human and multi-turn task-oriented dialogue datasets for language modeling. To better model dialogue behavior during pre-training, we incorporate user and system tokens into the masked language modeling. We propose a contrastive objective function to simulate the response selection task. Our pre-trained task-oriented dialogue BERT (TOD-BERT) outperforms strong baselines like BERT on four downstream task-oriented dialogue applications, including intention recognition, dialogue state tracking, dialogue act prediction, and response selection. We also show that TOD-BERT has a stronger few-shot ability that can mitigate the data scarcity problem for task-oriented dialogue.",
}


@inproceedings{yang2022mave,
  title={MAVE: A product dataset for multi-source attribute value extraction},
  author={Yang, Li and Wang, Qifan and Yu, Zac and Kulkarni, Anand and Sanghai, Sumit and Shu, Bin and Elsas, Jon and Kanagal, Bhargav},
  booktitle={Proceedings of the fifteenth ACM international conference on web search and data mining},
  pages={1256--1265},
  year={2022}
}

@article{ghani2006text,
  title={Text mining for product attribute extraction},
  author={Ghani, Rayid and Probst, Katharina and Liu, Yan and Krema, Marko and Fano, Andrew},
  journal={ACM SIGKDD Explorations Newsletter},
  volume={8},
  number={1},
  pages={41--48},
  year={2006},
  publisher={ACM New York, NY, USA}
}

@article{zou2022learning,
  title={Learning to ask: Conversational product search via representation learning},
  author={Zou, Jie and Huang, Jimmy and Ren, Zhaochun and Kanoulas, Evangelos},
  journal={ACM Transactions on Information Systems},
  volume={41},
  number={2},
  pages={1--27},
  year={2022},
  publisher={ACM New York, NY}
}

@inproceedings{vedula2022matters,
  title={What matters for shoppers: Investigating key attributes for online product comparison},
  author={Vedula, Nikhita and Collins, Marcus and Agichtein, Eugene and Rokhlenko, Oleg},
  booktitle={European Conference on Information Retrieval},
  pages={231--239},
  year={2022},
  organization={Springer}
}

@inproceedings{mcauley2015image,
  title={Image-based recommendations on styles and substitutes},
  author={McAuley, Julian and Targett, Christopher and Shi, Qinfeng and Van Den Hengel, Anton},
  booktitle={Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval},
  pages={43--52},
  year={2015}
}

@inproceedings{Bernard:2023:SIGIR,
  author =    {Bernard, Nolwenn and Balog, Krisztian},
  title =     {MG-ShopDial: A Multi-Goal Conversational Dataset for e-Commerce},
  booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '23), July 23--27, 2023, Taipei, Taiwan},
  series =    {SIGIR '23},
  year =      {2023}
}

@inproceedings{bi2019conversational,
  title={Conversational product search based on negative feedback},
  author={Bi, Keping and Ai, Qingyao and Zhang, Yongfeng and Croft, W Bruce},
  booktitle={Proceedings of the 28th acm international conference on information and knowledge management},
  pages={359--368},
  year={2019}
}

@inproceedings{owoicho2023exploiting,
  title={Exploiting Simulated User Feedback for Conversational Search: Ranking, Rewriting, and Beyond},
  author={Owoicho, Paul and Sekulic, Ivan and Aliannejadi, Mohammad and Dalton, Jeffrey and Crestani, Fabio},
  booktitle={Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={632--642},
  year={2023}
}

@inproceedings{gung-etal-2023-natcs,
    title = "{N}at{CS}: Eliciting Natural Customer Support Dialogues",
    author = "Gung, James  and
      Moeng, Emily  and
      Rose, Wesley  and
      Gupta, Arshit  and
      Zhang, Yi  and
      Mansour, Saab",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.613",
    doi = "10.18653/v1/2023.findings-acl.613",
    pages = "9652--9677",
    abstract = "Despite growing interest in applications based on natural customer support conversations,there exist remarkably few publicly available datasets that reflect the expected characteristics of conversations in these settings. Existing task-oriented dialogue datasets, which were collected to benchmark dialogue systems mainly in written human-to-bot settings, are not representative of real customer support conversations and do not provide realistic benchmarks for systems that are applied to natural data. To address this gap, we introduce NatCS, a multi-domain collection of spoken customer service conversations. We describe our process for collecting synthetic conversations between customers and agents based on natural language phenomena observed in real conversations. Compared to previous dialogue datasets, the conversations collected with our approach are more representative of real human-to-human conversations along multiple metrics. Finally, we demonstrate potential uses of NatCS, including dialogue act classification and intent induction from conversations as potential applications, showing that dialogue act annotations in NatCS provide more effective training data for modeling real conversations compared to existing synthetic written datasets. We publicly release NatCS to facilitate research in natural dialog systems",
}

@inproceedings{zhang2018towards,
  title={Towards conversational search and recommendation: System ask, user respond},
  author={Zhang, Yongfeng and Chen, Xu and Ai, Qingyao and Yang, Liu and Croft, W Bruce},
  booktitle={Proceedings of the 27th acm international conference on information and knowledge management},
  pages={177--186},
  year={2018}
}

@inproceedings{ai2017learning,
  title={Learning a hierarchical embedding model for personalized product search},
  author={Ai, Qingyao and Zhang, Yongfeng and Bi, Keping and Chen, Xu and Croft, W Bruce},
  booktitle={Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={645--654},
  year={2017}
}

@inproceedings{xiao-etal-2021-end,
    title = "End-to-End Conversational Search for Online Shopping with Utterance Transfer",
    author = "Xiao, Liqiang  and
      Ma, Jun  and
      Dong, Xin Luna  and
      Mart{\'\i}nez-G{\'o}mez, Pascual  and
      Zalmout, Nasser  and
      Zhang, Chenwei  and
      Zhao, Tong  and
      He, Hao  and
      Jin, Yaohui",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.280",
    doi = "10.18653/v1/2021.emnlp-main.280",
    pages = "3477--3486",
    abstract = "Successful conversational search systems can present natural, adaptive and interactive shopping experience for online shopping customers. However, building such systems from scratch faces real word challenges from both imperfect product schema/knowledge and lack of training dialog data. In this work we first propose ConvSearch, an end-to-end conversational search system that deeply combines the dialog system with search. It leverages the text profile to retrieve products, which is more robust against imperfect product schema/knowledge compared with using product attributes alone. We then address the lack of data challenges by proposing an utterance transfer approach that generates dialogue utterances by using existing dialog from other domains, and leveraging the search behavior data from e-commerce retailer. With utterance transfer, we introduce a new conversational search dataset for online shopping. Experiments show that our utterance transfer method can significantly improve the availability of training dialogue data without crowd-sourcing, and the conversational search system significantly outperformed the best tested baseline.",
}


@inproceedings{10.1145/3539618.3591878,
author = {Liu, Yuanxing and Zhang, Weinan and Dong, Baohua and Fan, Yan and Wang, Hang and Feng, Fan and Chen, Yifan and Zhuang, Ziyu and Cui, Hengbin and Li, Yongbin and Che, Wanxiang},
title = {U-NEED: A Fine-Grained Dataset for User Needs-Centric E-Commerce Conversational Recommendation},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591878},
doi = {10.1145/3539618.3591878},
abstract = {Conversational recommender systems ( CRS s) aim to understand the information needs and preferences expressed in a dialogue to recommend suitable items to the user. Most of the existing conversational recommendation datasets are synthesized or simulated with crowdsourcing, which has a large gap with real-world scenarios. To bridge the gap, previous work contributes a dataset E-ConvRec, based on pre-sales dialogues between users and customer service staff in E-commerce scenarios. However, E-ConvRec only supplies coarse-grained annotations and general tasks for making recommendations in pre-sales dialogues. Different from it, we use real user needs as a clue to explore the E-commerce conversational recommendation in complex pre-sales dialogues, namely user needs-centric E-commerce conversational recommendation (UNECR).In this paper, we construct a user needs-centric E-commerce conversational recommendation dataset (U-NEED ) from real-world E-commerce scenarios. U-NEED consists of 3 types of resources: (i) 7,698 fine-grained annotated pre-sales dialogues in 5 top categories (ii) 333,879 user behaviors and (iii) 332,148 product knowledge tuples. To facilitate the research of UNECR, we propose 5 critical tasks: (i) pre-sales dialogue understanding (ii) user needs elicitation (iii) user needs-based recommendation (iv) pre-sales dialogue generation and (v) pre-sales dialogue evaluation. We establish baseline methods and evaluation metrics for each task. We report experimental results of 5 tasks on U-NEED . We also report results on 3 typical categories. Experimental results indicate that the challenges of UNECR in various categories are different.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2723–2732},
numpages = {10},
keywords = {dialogue corpus, conversational recommendation, user needs},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@inproceedings{li-etal-2022-controllable,
    title = "Controllable Dialogue Simulation with In-context Learning",
    author = "Li, Zekun  and
      Chen, Wenhu  and
      Li, Shiyang  and
      Wang, Hong  and
      Qian, Jing  and
      Yan, Xifeng",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.318",
    doi = "10.18653/v1/2022.findings-emnlp.318",
    pages = "4330--4347",
    abstract = "Building dialogue systems requires a large corpus of annotated dialogues. Such datasets are usually created via crowdsourcing, which is expensive and time-consuming. In this paper, we propose Dialogic, a novel dialogue simulation method based on large language model in-context learning to automate dataset creation. Seeded with a few annotated dialogues, Dialogic automatically selects in-context examples for demonstration and prompts GPT-3 to generate new dialogues and annotations in a controllable way. Our method can rapidly expand a small set of dialogue data with minimum or zero \textit{human involvement} and \textit{parameter update} and is thus much more cost-efficient and time-saving than crowdsourcing. Experimental results on the MultiWOZ dataset demonstrate that training a model on the simulated dialogues leads to even better performance than using the same amount of human-generated dialogues under the challenging low-resource settings, with as few as 85 dialogues as a seed. When the full training set is given, our method can still serve as an effective data augmentation method to further improve performance. Human evaluation results also show that our simulated dialogues have near-human fluency and annotation accuracy. The code and data are available at \textbf{ \url{https://github.com/Leezekun/dialogic} }.",
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{han2021multiwoz,
  title={Multiwoz 2.3: A multi-domain task-oriented dialogue dataset enhanced with annotation corrections and co-reference annotation},
  author={Han, Ting and Liu, Ximing and Takanabu, Ryuichi and Lian, Yixin and Huang, Chongxuan and Wan, Dazhen and Peng, Wei and Huang, Minlie},
  booktitle={Natural Language Processing and Chinese Computing: 10th CCF International Conference, NLPCC 2021, Qingdao, China, October 13--17, 2021, Proceedings, Part II 10},
  pages={206--218},
  year={2021},
  organization={Springer}
}

@article{zhang2023sgp,
  title={SGP-TOD: Building Task Bots Effortlessly via Schema-Guided LLM Prompting},
  author={Zhang, Xiaoying and Peng, Baolin and Li, Kun and Zhou, Jingyan and Meng, Helen},
  journal={arXiv preprint arXiv:2305.09067},
  year={2023}
}

@inproceedings{budzianowski-etal-2018-multiwoz,
    title = "{M}ulti{WOZ} - A Large-Scale Multi-Domain {W}izard-of-{O}z Dataset for Task-Oriented Dialogue Modelling",
    author = "Budzianowski, Pawe{\l}  and
      Wen, Tsung-Hsien  and
      Tseng, Bo-Hsiang  and
      Casanueva, I{\~n}igo  and
      Ultes, Stefan  and
      Ramadan, Osman  and
      Ga{\v{s}}i{\'c}, Milica",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1547",
    doi = "10.18653/v1/D18-1547",
    pages = "5016--5026",
    abstract = "Even though machine learning has become the major scene in dialogue research community, the real breakthrough has been blocked by the scale of data available.To address this fundamental obstacle, we introduce the Multi-Domain Wizard-of-Oz dataset (MultiWOZ), a fully-labeled collection of human-human written conversations spanning over multiple domains and topics.At a size of 10k dialogues, it is at least one order of magnitude larger than all previous annotated task-oriented corpora.The contribution of this work apart from the open-sourced dataset is two-fold:firstly, a detailed description of the data collection procedure along with a summary of data structure and analysis is provided. The proposed data-collection pipeline is entirely based on crowd-sourcing without the need of hiring professional annotators;secondly, a set of benchmark results of belief tracking, dialogue act and response generation is reported, which shows the usability of the data and sets a baseline for future studies.",
}

@inproceedings{zang2020multiwoz,
  title={MultiWOZ 2.2: A Dialogue Dataset with Additional Annotation Corrections and State Tracking Baselines},
  author={Zang, Xiaoxue and Rastogi, Abhinav and Sunkara, Srinivas and Gupta, Raghav and Zhang, Jianguo and Chen, Jindong},
  booktitle={Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI},
  pages={109--117},
  year={2020}
}

@article{peng-etal-2021-soloist,
    title = "Soloist: Building Task Bots at Scale with Transfer Learning and Machine Teaching",
    author = "Peng, Baolin  and
      Li, Chunyuan  and
      Li, Jinchao  and
      Shayandeh, Shahin  and
      Liden, Lars  and
      Gao, Jianfeng",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "9",
    year = "2021",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2021.tacl-1.49",
    doi = "10.1162/tacl_a_00399",
    pages = "807--824",
    abstract = "We present a new method, Soloist,1 that uses transfer learning and machine teaching to build task bots at scale. We parameterize classical modular task-oriented dialog systems using a Transformer-based auto-regressive language model, which subsumes different dialog modules into a single neural model. We pre-train, on heterogeneous dialog corpora, a task-grounded response generation model, which can generate dialog responses grounded in user goals and real-world knowledge for task completion. The pre-trained model can be efficiently adapted to accomplish new tasks with a handful of task-specific dialogs via machine teaching, where training samples are generated by human teachers interacting with the system. Experiments show that (i)Soloist creates new state-of-the-art on well-studied task-oriented dialog benchmarks, including CamRest676 and MultiWOZ; (ii) in the few-shot fine-tuning settings, Soloist significantly outperforms existing methods; and (iii) the use of machine teaching substantially reduces the labeling cost of fine-tuning. The pre-trained models and codes are available at https://aka.ms/soloist.",
}

@article{mosig2020star,
  title={Star: A schema-guided dialog dataset for transfer learning},
  author={Mosig, Johannes EM and Mehri, Shikib and Kober, Thomas},
  journal={arXiv preprint arXiv:2010.11853},
  year={2020}
}

@article{semnani2023wikichat,
  title={WikiChat: A Few-Shot LLM-Based Chatbot Grounded with Wikipedia},
  author={Semnani, Sina J and Yao, Violet Z and Zhang, Heidi C and Lam, Monica S},
  journal={arXiv preprint arXiv:2305.14292},
  year={2023}
}

@inproceedings{li-etal-2023-autoconv,
    title = "{A}uto{C}onv: Automatically Generating Information-seeking Conversations with Large Language Models",
    author = "Li, Siheng  and
      Yang, Cheng  and
      Yin, Yichun  and
      Zhu, Xinyu  and
      Cheng, Zesen  and
      Shang, Lifeng  and
      Jiang, Xin  and
      Liu, Qun  and
      Yang, Yujiu",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-short.149",
    doi = "10.18653/v1/2023.acl-short.149",
    pages = "1751--1762",
    abstract = "Information-seeking conversation, which aims to help users gather information through conversation, has achieved great progress in recent years. However, the research is still stymied by the scarcity of training data. To alleviate this problem, we propose AutoConv for synthetic conversation generation, which takes advantage of the few-shot learning ability and generation capacity of large language models (LLM). Specifically, we formulate the conversation generation problem as a language modeling task, then finetune an LLM with a few human conversations to capture the characteristics of the information-seeking process and use it for generating synthetic conversations with high quality. Experimental results on two frequently-used datasets verify that AutoConv has substantial improvements over strong baselines and alleviates the dependence on human annotation. In addition, we also provide several analysis studies to promote future research.",
}

@article{lu2023dialgen,
  title={DIALGEN: Collaborative Human-LM Generated Dialogues for Improved Understanding of Human-Human Conversations},
  author={Lu, Bo-Ru and Haduong, Nikita and Lee, Chia-Hsuan and Wu, Zeqiu and Cheng, Hao and Koester, Paul and Utke, Jean and Yu, Tao and Smith, Noah A and Ostendorf, Mari},
  journal={arXiv preprint arXiv:2307.07047},
  year={2023}
}

@article{ding2023enhancing,
  title={Enhancing Chat Language Models by Scaling High-quality Instructional Conversations},
  author={Ding, Ning and Chen, Yulin and Xu, Bokai and Qin, Yujia and Zheng, Zhi and Hu, Shengding and Liu, Zhiyuan and Sun, Maosong and Zhou, Bowen},
  journal={arXiv preprint arXiv:2305.14233},
  year={2023}
}

@article{kong2023large,
  title={Large Language Model as a User Simulator},
  author={Kong, Chuyi and Fan, Yaxin and Wan, Xiang and Jiang, Feng and Wang, Benyou},
  journal={arXiv preprint arXiv:2308.11534},
  year={2023}
}

@article{bitton2023q2d,
  title={q2d: Turning Questions into Dialogs to Teach Models How to Search},
  author={Bitton, Yonatan and Cohen-Ganor, Shlomi and Hakimi, Ido and Lewenberg, Yoad and Aharoni, Roee and Weinreb, Enav},
  journal={arXiv preprint arXiv:2304.14318},
  year={2023}
}

@article{yang2023refgpt,
  title={RefGPT: Reference-> Truthful \& Customized Dialogues Generation by GPTs and for GPTs},
  author={Yang, Dongjie and Yuan, Ruifeng and Fan, YuanTao and Yang, YiFei and Wang, Zili and Wang, Shushen and Zhao, Hai},
  journal={arXiv preprint arXiv:2305.14994},
  year={2023}
}

@inproceedings{cho-etal-2014-properties,
    title = "On the Properties of Neural Machine Translation: Encoder{--}Decoder Approaches",
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Bahdanau, Dzmitry  and
      Bengio, Yoshua},
    booktitle = "Proceedings of {SSST}-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W14-4012",
    doi = "10.3115/v1/W14-4012",
    pages = "103--111",
}
@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@inproceedings{lewis-etal-2020-bart,
    title = "{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
    author = "Lewis, Mike  and
      Liu, Yinhan  and
      Goyal, Naman  and
      Ghazvininejad, Marjan  and
      Mohamed, Abdelrahman  and
      Levy, Omer  and
      Stoyanov, Veselin  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.703",
    doi = "10.18653/v1/2020.acl-main.703",
    pages = "7871--7880",
    abstract = "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
}

@inproceedings{karpukhin-etal-2020-dense,
    title = "Dense Passage Retrieval for Open-Domain Question Answering",
    author = "Karpukhin, Vladimir  and
      Oguz, Barlas  and
      Min, Sewon  and
      Lewis, Patrick  and
      Wu, Ledell  and
      Edunov, Sergey  and
      Chen, Danqi  and
      Yih, Wen-tau",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.550",
    doi = "10.18653/v1/2020.emnlp-main.550",
    pages = "6769--6781",
    abstract = "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system greatly by 9{\%}-19{\%} absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
}

@article{young2010hidden,
  title={The hidden information state model: A practical framework for POMDP-based spoken dialogue management},
  author={Young, Steve and Ga{\v{s}}i{\'c}, Milica and Keizer, Simon and Mairesse, Fran{\c{c}}ois and Schatzmann, Jost and Thomson, Blaise and Yu, Kai},
  journal={Computer Speech \& Language},
  volume={24},
  number={2},
  pages={150--174},
  year={2010},
  publisher={Elsevier}
}

@inproceedings{mehta1996sliq,
  title={SLIQ: A fast scalable classifier for data mining},
  author={Mehta, Manish and Agrawal, Rakesh and Rissanen, Jorma},
  booktitle={Advances in Database Technology—EDBT'96: 5th International Conference on Extending Database Technology Avignon, France, March 25--29, 1996 Proceedings 5},
  pages={18--32},
  year={1996},
  organization={Springer}
}

@inproceedings{van2016learning,
  title={Learning latent vector spaces for product search},
  author={Van Gysel, Christophe and de Rijke, Maarten and Kanoulas, Evangelos},
  booktitle={Proceedings of the 25th ACM international on conference on information and knowledge management},
  pages={165--174},
  year={2016}
}


@article{vandic2012faceted,
  title={Faceted product search powered by the semantic web},
  author={Vandic, Damir and Van Dam, Jan-Willem and Frasincar, Flavius},
  journal={Decision Support Systems},
  volume={53},
  number={3},
  pages={425--437},
  year={2012},
  publisher={Elsevier}
}

@inproceedings{putthividhya2011bootstrapped,
  title={Bootstrapped named entity recognition for product attribute extraction},
  author={Putthividhya, Duangmanee and Hu, Junling},
  booktitle={Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing},
  pages={1557--1567},
  year={2011}
}


@inproceedings{raju2009unsupervised,
  title={An unsupervised approach to product attribute extraction},
  author={Raju, Santosh and Pingali, Prasad and Varma, Vasudeva},
  booktitle={Advances in Information Retrieval: 31th European Conference on IR Research, ECIR 2009, Toulouse, France, April 6-9, 2009. Proceedings 31},
  pages={796--800},
  year={2009},
  organization={Springer}
}

@inproceedings{tu2023ssp,
  title={SSP: Self-Supervised Post-training for Conversational Search},
  author={Tu, Quan and Gao, Shen and Wu, Xiaolong and Cao, Zhao and Wen, Ji-Rong and Yan, Rui},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={13237--13249},
  year={2023}
}

@article{kaddour2023challenges,
  title={Challenges and Applications of Large Language Models},
  author={Kaddour, Jean and Harris, Joshua and Mozes, Maximilian and Bradley, Herbie and Raileanu, Roberta and McHardy, Robert},
  journal={arXiv e-prints},
  pages={arXiv--2307},
  year={2023}
}

@inproceedings{chen-etal-2022-reinforced,
    title = "Reinforced Question Rewriting for Conversational Question Answering",
    author = "Chen, Zhiyu  and
      Zhao, Jie  and
      Fang, Anjie  and
      Fetahu, Besnik  and
      Rokhlenko, Oleg  and
      Malmasi, Shervin",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Industry Track",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-industry.36",
    doi = "10.18653/v1/2022.emnlp-industry.36",
    pages = "357--370",
    abstract = "Conversational Question Answering (CQA) aims to answer questions contained within dialogues, which are not easily interpretable without context. Developing a model to rewrite conversational questions into self-contained ones is an emerging solution in industry settings as it allows using existing single-turn QA systems to avoid training a CQA model from scratch. Previous work trains rewriting models using human rewrites as supervision. However, such objectives are disconnected with QA models and therefore more human-like rewrites do not guarantee better QA performance. In this paper we propose using QA feedback to supervise the rewriting model with reinforcement learning. Experiments show that our approach can effectively improve QA performance over baselines for both extractive and retrieval QA. Furthermore, human evaluation shows that our method can generate more accurate and detailed rewrites when compared to human annotations.",
}

@inproceedings{vakulenko2021question,
  title={Question rewriting for conversational question answering},
  author={Vakulenko, Svitlana and Longpre, Shayne and Tu, Zhucheng and Anantha, Raviteja},
  booktitle={Proceedings of the 14th ACM international conference on web search and data mining},
  pages={355--363},
  year={2021}
}


@inproceedings{yu2021few,
  title={Few-shot conversational dense retrieval},
  author={Yu, Shi and Liu, Zhenghao and Xiong, Chenyan and Feng, Tao and Liu, Zhiyuan},
  booktitle={Proceedings of the 44th International ACM SIGIR Conference on research and development in information retrieval},
  pages={829--838},
  year={2021}
}

@inproceedings{yu2020few,
  title={Few-shot generative conversational query rewriting},
  author={Yu, Shi and Liu, Jiahua and Yang, Jingqin and Xiong, Chenyan and Bennett, Paul and Gao, Jianfeng and Liu, Zhiyuan},
  booktitle={Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval},
  pages={1933--1936},
  year={2020}
}

@inproceedings{wu2022conqrr,
  title={CONQRR: Conversational Query Rewriting for Retrieval with Reinforcement Learning},
  author={Wu, Zeqiu and Luan, Yi and Rashkin, Hannah and Reitter, David and Hajishirzi, Hannaneh and Ostendorf, Mari and Tomar, Gaurav Singh},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={10000--10014},
  year={2022}
}

@inproceedings{xiao2021end,
  title={End-to-End Conversational Search for Online Shopping with Utterance Transfer},
  author={Xiao, Liqiang and Ma, Jun and Dong, Xin Luna and Mart{\'\i}nez-G{\'o}mez, Pascual and Zalmout, Nasser and Zhang, Chenwei and Zhao, Tong and He, Hao and Jin, Yaohui},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={3477--3486},
  year={2021}
}

@inproceedings{azzopardi2018measuring,
  title={Measuring the utility of search engine result pages: an information foraging based measure},
  author={Azzopardi, Leif and Thomas, Paul and Craswell, Nick},
  booktitle={The 41st International ACM SIGIR conference on research \& development in information retrieval},
  pages={605--614},
  year={2018}
}

@inproceedings{zhang2017evaluating,
  title={Evaluating web search with a bejeweled player model},
  author={Zhang, Fan and Liu, Yiqun and Li, Xin and Zhang, Min and Xu, Yinghui and Ma, Shaoping},
  booktitle={Proceedings of the 40th international ACM SIGIR conference on research and development in information retrieval},
  pages={425--434},
  year={2017}
}


@book{quinlan2014c4,
  title={C4. 5: programs for machine learning},
  author={Quinlan, J Ross},
  year={2014},
  publisher={Elsevier}
}

@inproceedings{hayati2020inspired,
  title={INSPIRED: Toward Sociable Recommendation Dialog Systems},
  author={Hayati, Shirley Anugrah and Kang, Dongyeop and Zhu, Qingxiaoyang and Shi, Weiyan and Yu, Zhou},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={8142--8152},
  year={2020}
}

@inproceedings{zhang2022multiple,
  title={Multiple choice questions based multi-interest policy learning for conversational recommendation},
  author={Zhang, Yiming and Wu, Lingfei and Shen, Qi and Pang, Yitong and Wei, Zhihua and Xu, Fangli and Long, Bo and Pei, Jian},
  booktitle={Proceedings of the ACM Web Conference 2022},
  pages={2153--2162},
  year={2022}
}

@inproceedings{deng2021unified,
  title={Unified conversational recommendation policy learning via graph-based reinforcement learning},
  author={Deng, Yang and Li, Yaliang and Sun, Fei and Ding, Bolin and Lam, Wai},
  booktitle={Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={1431--1441},
  year={2021}
}

@article{li2018towards,
  title={Towards deep conversational recommendations},
  author={Li, Raymond and Ebrahimi Kahou, Samira and Schulz, Hannes and Michalski, Vincent and Charlin, Laurent and Pal, Chris},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{liu2021durecdial,
  title={DuRecDial 2.0: A Bilingual Parallel Corpus for Conversational Recommendation},
  author={Liu, Zeming and Wang, Haifeng and Niu, Zheng-Yu and Wu, Hua and Che, Wanxiang},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={4335--4347},
  year={2021}
}

@inproceedings{liu2020towards,
  title={Towards Conversational Recommendation over Multi-Type Dialogs},
  author={Liu, Zeming and Wang, Haifeng and Niu, Zheng-Yu and Wu, Hua and Che, Wanxiang and Liu, Ting},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={1036--1049},
  year={2020}
}

@article{liu2019roberta,
  title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year={2019}
}

@article{al2010review,
  title={A review of factors influencing user satisfaction in information retrieval},
  author={Al-Maskari, Azzah and Sanderson, Mark},
  journal={Journal of the American Society for Information Science and Technology},
  volume={61},
  number={5},
  pages={859--868},
  year={2010},
  publisher={Wiley Online Library}
}

@inproceedings{chen-etal-2020-jddc,
    title = "The {JDDC} Corpus: A Large-Scale Multi-Turn {C}hinese Dialogue Dataset for {E}-commerce Customer Service",
    author = "Chen, Meng  and
      Liu, Ruixue  and
      Shen, Lei  and
      Yuan, Shaozu  and
      Zhou, Jingyan  and
      Wu, Youzheng  and
      He, Xiaodong  and
      Zhou, Bowen",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.58",
    pages = "459--466",
    language = "English",
    ISBN = "979-10-95546-34-4",
}


@inproceedings{zhao-etal-2022-jddc,
    title = "{JDDC} 2.1: A Multimodal {C}hinese Dialogue Dataset with Joint Tasks of Query Rewriting, Response Generation, Discourse Parsing, and Summarization",
    author = "Zhao, Nan  and
      Li, Haoran  and
      Wu, Youzheng  and
      He, Xiaodong",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.825",
    doi = "10.18653/v1/2022.emnlp-main.825",
    pages = "12037--12051",
}
