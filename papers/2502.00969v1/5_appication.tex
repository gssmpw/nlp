\section{Downstream Tasks}
\label{sec:downstream_tasks}
In this section, we show the effectiveness of the Wizard-of-Shopping dataset by applying it to two downstream tasks: Conversational Query Generation (CQG) and Conversational Product Ranking (CPR). For both tasks, we divided the \textit{WoS} conversations into 3,000 for training, 300 for validation, and 300 for testing, with each split containing an equal number of dialogues from the three domains. More details are available in Appendix \ref{sec:detailed_downstream_tasks}.

\subsection{Conversational Query Generation (CQG)}

Real product search conversations are likely to be verbose and redundant for the purpose of product ranking.  
Similar to the conversational search~\cite{yu2020few,vakulenko2021question,chen-etal-2022-reinforced,wu2022conqrr} where a reformulation model is used to generate a more informative query from the dialogue history, we address \textit{conversational query generation} (CQG) for product search. 
CQG aims to extract essential information such as the product category under discussion, desired features, undesired features, and optional product preferences from the customer. This extracted information can then be used as a query for a product search engine. And in fact, this is a reverse task of the LLM verbalization where we extract user preferences from the shopping dialogues. Similar to Dialogue State Tracking~\cite{lu2023dialgen}, CQG is helpful in tracing the interest of customers during the conversation. 
%can be extracted for product ranking and DST while the conversation is in progress.
%As the reverse task of the LLM verbalization, the CQG task condenses a long shopping conversation into a structured form that encodes the most essential information about conversational product search. %\shervin{What is the purpose of the task? what is it used for?} 
As \citet{bi2019conversational} suggests, utilizing positive and negative product features are crucial for training a conversational product ranking model. We train a conversational query generator on the \textit{WoS} data to extract the product category and the customer-preferred product features:
%\begin{equation*}
%\vspace{-1em}
\begin{equation}\label{eq:cqg}  \small
    [PC, Wanted, Unwanted, Optional] \gets CQG(Dialogue)
\end{equation}
\vspace{-2em}
%\hl{Jason: is this input same to preference?} % Xiangci: Yes.



\subsubsection{Approaches} \label{sec:conversational_query_generation_approaches}

\paragraph{Baseline.} As stated in \S\ref{sec:related_work}, to the best of our knowledge, \textit{WoS} is the only CPS dataset that can be used for training downstream tasks. Therefore as a simple baseline, we directly use the full conversation history as the predicted query.
%\vspace{-0.5em}
\paragraph{Dialogue --> Query (D2Q).} We use a seq2seq model, Longformer Encoder-Decoder \citep[LED;][]{beltagy2020longformer} fine-tuned on our \textit{WoS} dataset to predict the product category and the \textit{wanted}, \textit{unwanted} and \textit{optional} product features, given the full conversation history.

\paragraph{D2Q (GPT-4).} For reference of LLM performance, we few-shot prompt OpenAI gpt-4-turbo-2024-04-09 with the same inputs and outputs as D2Q.
%\paragraph{Utterance-level.} We assume that each utterance between the seller and customer encodes one or more product features. We use a seq2seq model to extract product features from each utterance, and concatenate the features to be the final query.

% \begin{table}[t]  \small
% \centering
% \setlength{\tabcolsep}{4pt}
% \begin{tabular}{lllllll}
% \hline
% \textbf{Approach} & \textbf{Features} & \textbf{F1} & \textbf{R-1} & \textbf{R-2} & \textbf{R-L}\\ \hline
% Baseline & + & 0 & 0.056 & 0.020 & 0.048 \\ 
% Baseline & +/-/? & 0.008 & 0.137 & 0.047 & 0.087  \\  \hline 
% %Utterance & BART & + & 0.698 & 0.592 & 0.482 & 0.562 \\ 
% %Utterance & BART & +/-/? & 0.656 & 0.664 & 0.489 & 0.592 \\  \hline
% %Utterance & LED & + & 0.704 & 0.610 & 0.509 & 0.578 \\ 
% %Utterance & LED & +/-/? & 0.748 & 0.720 & 0.561 & 0.657 \\  \hline
% D2Q & + & 0.834 & 0.899 & 0.822 & 0.873 \\ 
% D2Q & +/-/? & 0.873 & 0.900 & 0.768 & 0.840 \\  \hline
% \end{tabular}
% \vspace{-0.5em}
% \caption{Conversational Query Generation performance at the final turn measured by exact-F1, ROUGE-1, -2, and -L. We report the performance of only using the desired features (+) for downstream ranking, as well as using all features (wanted (+), unwanted (-), and optional (?)).}
% \label{tab:conversational_query_generation}
% \vspace{-2em}
% \end{table}

\begin{table}[t]  \small
\centering
\setlength{\tabcolsep}{4pt}
\begin{tabular}{llllll}
\hline
\textbf{Approach} & \textbf{F1} & \textbf{R-1} & \textbf{R-2} & \textbf{R-L}\\ \hline
%Baseline & 0 & 0.056 & 0.020 & 0.048 \\ 
Baseline & 0.008 & 0.137 & 0.047 & 0.087  \\  \hline  
%Utterance & BART & + & 0.698 & 0.592 & 0.482 & 0.562 \\ 
%Utterance & BART & +/-/? & 0.656 & 0.664 & 0.489 & 0.592 \\  \hline
%Utterance & LED & + & 0.704 & 0.610 & 0.509 & 0.578 \\ 
%Utterance & LED & +/-/? & 0.748 & 0.720 & 0.561 & 0.657 \\  \hline
D2Q & 0.834 & 0.899 & 0.822 & 0.873 \\ \hline
D2Q (GPT-4) & 0.553 & 0.793 & 0.628 & 0.734 \\ \hline
%D2Q & +/-/? & 0.873 & 0.900 & 0.768 & 0.840 \\  \hline
\end{tabular}
\vspace{-0.5em}
\caption{Conversational Query Generation performance at the final turn measured by exact-F1, ROUGE-1, -2, and -L.}
\label{tab:conversational_query_generation}
\vspace{-2em}
\end{table}

\subsubsection{Experimental Results}
Table \ref{tab:conversational_query_generation} shows the performance of the CQG task.
%We report results on two settings: \textit{wanted} feature-only setups concatenate the product category and \textit{wanted} features and all feature setups concatenate all features as defined in Eq.~\ref{eq:cqg}. 
As expected, the weak baseline using conversation history performs poorly, while our fine-tuned query generator performs much better. %Additionally, we observe that predicting product features utterance-by-utterance instead underperforms using the full conversation (\S\ref{sec:detailed_conversational_query_generation} \& Table \ref{tab:detailed_conversational_query_generation}), which is consistent with prior work of reformulating conversations into queries \cite{yu2020few, wu2022conqrr}. 
Although GPT-4's performance is slightly underestimated because few-shot demonstration examples do not show every edge case, D2Q by \textit{WoS}-finetuned LED shows its superiority of both performance and cost in terms of both latency and price over GPT-4 as an example of LLM.
%When comparing the utterance-level approaches, LED outperforms BART, since the training data for BART is slightly smaller than LED's. This is because we have to discard the input augmented conversation history that is too long for BART's maximum context window (1024 tokens). Also, the LED-based utterance level approach under-performs the conversation level approach, presumably because integrating the results from multiple inferences passes is more error-prone. 

\subsection{Conversational Product Ranking (CPR)}
As the core component of CPS, CPR directly ranks the candidate products given the shopping conversation. We index or embed each product's title and concatenate their aspect-value pairs according to the following approaches:

\subsubsection{Approaches}%\zhiyu{Define what is indexed for each document/product. Title ? Description ?}

\paragraph{Baseline} %Similar to the CQG task, we assume there is no conversational data available for training a ranker. Therefore, w
We directly feed the full conversation history as the query to a BM25 ranker.

%\paragraph{Dialogue --> Product (D2P)} We directly embed the full conversation history to a dense ranker to rank the candidate products, i.e. \textit{Products = Ranker(Dialogue)}. %\zhiyu{related to my previous comments. If products are ranked by Relevance = Ranker(dialogue, product\_info), what is product\_info here?}
\vspace{-0.5em}
\paragraph{Dialogue --> Query --> Product (D2Q2P)} We use D2Q setting in \S\ref{sec:conversational_query_generation_approaches} to generate queries given the full conversation history and apply the queries to a ranker, i.e. \textit{Products = Ranker(CQG(Dialogue))}. We experiment with both a sparse ranker, BM25, and a dense ranker \citep[fine-tuned RoBERTa;][]{liu2019roberta}.

\paragraph{D2Q2P (GPT-4)} We use D2Q (GPT-4) setting in \S\ref{sec:conversational_query_generation_approaches} and feed the predicted query to a BM25 ranker.

\begin{table}[t]  \small
\centering
\setlength{\tabcolsep}{2pt} % Default value: 6pt
\renewcommand{\arraystretch}{1.0} % Default value: 1
\begin{tabular}{llllllll}
\hline
\textbf{Approach} & \textbf{Ranker} & \textbf{MRR} & \textbf{H@1} & \textbf{H@10} & \textbf{H@100}\\ \hline
Baseline & BM25& 0.162 & 0.107 & 0.203 & 0.587 \\ \hline \hline
%D2P & - & longformer & 0.201 & 0.143  & 0.247 & 0.620 \\ \hline
%Utterance & BART & BM25& 0.629 & 0.540  & 0.733 & 0.900 \\
%Utterance & BART & Roberta& 0.217 & 0.163 & 0.270 & 0.477\\ \hline
%Utterance & LED & BM25& 0.667 & 0.593 & 0.740 & 0.900 \\
%Utterance & LED & Roberta& 0.207 & 0.163 & 0.250 & 0.460 \\ \hline
D2Q2P & BM25& \textbf{0.838} & \textbf{0.767}  & \textbf{0.927} & \textbf{0.993} \\
D2Q2P & Roberta & 0.675 & 0.583  & 0.780 & 0.937 \\ \hline
D2Q2P (GPT-4) & BM25 & 0.763 & 0.680 & 0.903 & 0.903 \\ \hline 

\end{tabular}
\vspace{-0.5em}
\caption{Downstream CPR performance at the final turn.}
\label{tab:ranker}
\vspace{-2em}
\end{table}

\subsubsection{Experimental Results}
Table \ref{tab:ranker} shows the mean reciprocal rank (MRR) and Hit$@k$ of all methods. 
%We observe that the D2P approach only slightly outperforms the baseline approach without training. We suspect that our DPR-Longformer is still under-fitted given 3000 conversations as the training set. On the contrary, by leveraging the query generators trained on our collected conversations, the ranking performance is greatly improved. 
%Similar to the trend observed in the conversational query generation task, the conversation-level approach outperforms the utterance-level approach that integrates multiple passes of inference outputs. 
Similar to the trend observed for CQG, our D2Q2P approach significantly outperforms the baseline and GPT-4. Interestingly, the sparse BM25 ranker greatly outperforms the dense RoBERTa-based ranker. This is because the product representations include feature names that are lexically similar to the gold queries. Consequently, BM25 exhibits a strong performance in this ranking task. Moreover, the dense ranker may be under-fitted. The query generators and dense rankers fine-tuned on our \textit{WoS} dataset significantly outperform the baseline that is not trained on our dataset.

