\section{Method}

%\glcomment{the paper lacked details in the methodology and procedures of the empirical study, in turn hurting the validity of its findings. This concern mainly stems from the lack of detail in the system, as the user workflow with the system from planning stage to accomplishing the task was unclear. My understanding of the system is that the execution stage does not actually execute the task, but only simulates it, meaning that the results may not accurately represent behaviors of AI agents if integrated into real applications}\ujcomment{IMPORTANT1. Concede that our setup is limited (discuss this in the limitations section)2. Argue why this is an ecologically valid representation of humans interacting with AI agents.}

\subsection{Overview of User Involvement in Plan-then-execute LLM Agents}

In our study, we adopted plan-then-execute LLM Agents~\cite{wang2023plan} as assistants to help users handle daily tasks, \eg itinerary planning and currency transactions. Figure~\ref{fig:illustration} illustrates how users collaborate with plan-then-execute LLM agents. 
First, the LLM agents will generate a step-wise plan based on a prompt specifying the plan format adopted from~\cite{huang2024planning}. 
Then, users will make necessary edits to the plan based on the provided edit tools (will be further detailed in Section~\ref{sec-method-planning}). 
After the user edit, we obtained the step-wise plan as outcomes of the planning stage. 
Next, the LLM agents will transform the step-wise plan into a sequence of action predictions, which will be served in a step-by-step manner. 
Users will join the real-time execution process and check whether they approve the current predicted action (\ie blue card shown in Figure~\ref{fig:illustration}) or they would like to modify the current action prediction. The user involvement in execution stages will be introduced in Section~\ref{sec-method-execution}. 
After the iterative execution of all steps, the task is solved. {The evaluation of task performance is mainly based on the plan quality and execution accuracy of the action sequences.}
% The benefits of such a setup are two-fold: (1) The step-by-step planning provides both clarity in the subsequent execution and control over the task decomposition. 
% (2) In the execution stage, users will follow the step-by-step execution process and provide necessary interventions to fix the errors of the LLM agent.\glcomment{Maybe it is better to visualize plan-then-execute with an illustration}
% \glcomment{motivation for plan-then-execute manner}. 
% Plan-then-execute LLM agent is first proposed by Wang \etal~\cite{wang2023plan}.
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.48\textwidth]{figures/plan_execute_illustration_2.pdf}
%     \caption{Illustration of the human-AI collaboration with plan-then-execute LLM agents.}%~\glcomment{Any suggestions to further improve this figure?}} 
%     \label{fig:illustration}
%     \Description{Illustration of the human-AI collaboration with plan-then-execute LLM agents. The planning stage mainly consists of two steps: (1) LLMs generate a draft plan (2) user edit the planning. Based on the edited plan, we get a step-wise plan (three steps in illustration). Then, in the execution stage, LLM agents will generate a sequence of actions, which is one-on-one mapping based on the plan's primary steps. Users will join this process to decide whether they approve the predicted action or choose to be involved (with manual specification of action or give text feedback to LLM agents). After the step-by-step execution of the plan, the task is solved.}
% \end{figure}

% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=0.8\textwidth]{figures/plan_execute_illustration_2.pdf}
%     \caption{Illustration of the human-AI collaboration with plan-then-execute LLM agents.}%~\glcomment{Any suggestions to further improve this figure?}} 
%     \label{fig:illustration}
%     \Description{Illustration of the human-AI collaboration with plan-then-execute LLM agents. The planning stage mainly consists of two steps: (1) LLMs generate a draft plan (2) user edit the planning. Based on the edited plan, we get a step-wise plan (three steps in illustration). Then, in the execution stage, LLM agents will generate a sequence of actions, which is one-on-one mapping based on the plan's primary steps. Users will join this process to decide whether they approve the predicted action or choose to be involved (with manual specification of action or give text feedback to LLM agents). After the step-by-steo execution of the plan, the task is solved.}
% \end{figure*}

\paratitle{Implementation details}. In our study, we adopted GPT-3.5-turbo as the backbone LLM to serve the plan-then-execute LLM agent. The backend LLM agent implementation is mainly based on the Langchain plan and execute agent.\footnote{\url{https://api.python.langchain.com/en/latest/plan_and_execute/langchain_experimental.plan_and_execute.agent_executor.PlanAndExecute.html}} \revise{The execution of tasks are based on a simulation environment, where all tools/actions of the LLM agents are pre-defined as backend APIs hosted with Flask\footnote{\url{https://github.com/pallets/flask}}.}
In the spirit of open science, all code and data analysis results can be found at Github.\footnote{\url{https://github.com/RichardHGL/CHI2025_Plan-then-Execute_LLMAgent}}

\subsection{Planning}
\label{sec-method-planning}
While LLMs can generate high-quality plans, there is no guarantee of their correctness and their further impact on the execution of the plan. 
Thus, involving users in the planning stage and controlling the plan quality would be essential to ensure successful subsequent execution. 

\begin{figure*}[htbp]
 \centering
  \subfigure[]%Illustration of user-involved execution of one primary step.]
  {\label{fig:execution-flow-chart}
  \centering
  \includegraphics[height=0.42\textwidth]{figures/execution_stages-new.pdf}
 }
  \subfigure[]%Screenshot of conversation interface for user-involved execution.]
  {\label{fig:execution-interface}
  \centering
  \includegraphics[height=0.42\textwidth,width=0.4\textwidth]{figures/execution-interface.png}
 }
 \centering
 \caption{User-involved execution flow chart and interface. \revise{Panel (a): a flow chart illustrating how each primary step is executed with two stages: action prediction and action execution.} Panel (b): a screenshot of the conversation interface for user-involved execution.
 }
 \label{fig-interface}
 \Description{User-involved execution flow chart and interface. In the left panel, a flow chart shows how each primary step is executed with two stages: action prediction and action execution. In the right panel, we show one example of a user-involved execution page, which follows a conversation to show the step-by-step execution process. After one action is predicted, users can choose from options `Proceed', `Feedback', and `Specify Action' to approve or involve in the real-time execution.}
\end{figure*}

\paratitle{Plan Format}. 
The step-wise plan in our study followed a hierarchical structure, adapted from a benchmark for LLM agents toolkit usage~\cite{huang2024planning}. 
The whole plan consists of multiple sub-steps, which are at most three levels (\eg 1., 1.x, 1.x.y where x,y are integers). 
All sub-steps started with the same prefix index are denoted as one primary step (\eg the three blocks of planning outcome in Figure~\ref{fig:illustration}). 
A high-level step (\eg 1.) will provide high-level instruction of the current primary step, while low-level steps \revise{(\eg 1.x, 1.x.y)} will provide subsequent details. 
In the execution stage, each primary step will be used as the execution unit. 
The LLM agent will transform one primary step into a predicted action filled with parameters. 
Thus, we ask participants to provide all necessary details in sub-steps of each primary step. {Each primary step will be transformed into \textbf{single action} in the follow-up execution stage}. 
If one primary step requires two actions to accomplish, it may cause a potential loss of one action. 
Thus, when a plan contains one primary step that contains information about two potential actions (\eg the initial plan in Figure~\ref{fig:illustration}), we consider it as a low-quality plan with `grammar errors.'\footnote{Note that this is not to be confused with the notion of grammar in language.} 
All these plan format designs are informed in our onboarding tutorial.


% \begin{minipage}[t]{.45\textwidth}
% \begin{lstlisting}[caption= initial plan for task 1,frame=tlrb,label=listing-1]{}
% 1. Log in to user account
% 1.1 Obtain account login information (Account ID: 54321, Account Password: PWD2023)
% 1.2 Log in to the account
% 1.3 Confirm successful 
% 2. Conduct foreign exchange transactions
% 2.1 Buy euros
% 2.1.1 Obtain information for buying euros (Currency Type: EUR, Purchase Amount: 10000)
% 2.1.2 Buy the specified amount of euros
% 2.1.3 Confirm successful euro purchase
% 2.2 Sell US dollars
% 2.2.1 Obtain information for selling US dollars (Currency Type: USD, Sell Amount: 5000)
% 2.2.2 Check the US dollar holdings
% 2.2.3 Obtain US dollar holdings information (Foreign Exchange Holdings Information: Obtained US dollar holdings information)
% 2.2.4 Sell the specified amount of US dollars
% 2.2.5 Confirm successful US dollar sale
% \end{lstlisting}
% \end{minipage}\hspace{0.8cm}
% \begin{minipage}[t]{.45\textwidth}
% \begin{lstlisting}[caption=edited plan for task 1,frame=tlrb,label=listing-2]{}
% 1. Log in to user account
% 1.1 Obtain account login information (Account ID: 54321, Account Password: PWD2023)
% 1.2 Log in to the account
% 1.3 Confirm successful 
% 2. Conduct foreign exchange transactions
% 2.1 Buy euros
% 2.1.1 Obtain information for buying euros (Currency Type: EUR, Purchase Amount: 10000)
% 2.1.2 Buy the specified amount of euros
% 2.1.3 Confirm successful euro purchase
% 3. Conduct second foreign exchange transactions
% 3.1 Sell US dollars
% 3.1.1 Obtain information for selling US dollars (Currency Type: USD, Sell Amount: 5000)
% 3.1.2 Check the US dollar holdings
% 3.1.3 Obtain US dollar holdings information (Foreign Exchange Holdings Information: Obtained US dollar holdings information)
% 3.1.4 Sell the specified amount of US dollars
% 3.1.5 Confirm successful US dollar sale
% \end{lstlisting}
% \end{minipage}

% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=0.75\textwidth]{figures/Screenshot-planning.png}
%     \caption{Screenshot of user-involved planning interface.} 
%     \label{fig:planning}
%     \Description{Screenshot of user-involved planning interface. At the top, we show the task description along with three buttons: show potential actions, plan edit instruction, and add one step. At the bottom, we show a step-wise plan for setting alarms. Users can click these buttons to achive the function we described in the user-involved planning to edit the plan.}
% \end{figure*}

\paratitle{User-involved Planning}. Figure~\ref{fig:planning} shows one screenshot of user-involved planning in our study. 
At the top of the interface, we provide a task description along with three buttons: `Show Potential Actions', `Plan Edit Instruction', and `Add one step'. 
By clicking `Show Potential Actions', we provide a prompt window to show concrete documentary descriptions of all potential actions (including action purpose and parameters) to be used in the execution stage. All instructions used in our tutorial are accessible with clicking the button `Plan Edit Instruction'. 
After users join the planning stage, an initial plan generated by LLM will be presented in the orange area. We allow users to edit the plan with following interactions:
\begin{itemize}
    \item Add step. By clicking `Add one step' button, users can insert a valid sub-step index into the whole plan, and then they can edit the plan text.
    \item Delete step. By clicking the `Delete step' button at the end of one step, all sub-steps associated with that step will be deleted from the plan.
    \item Edit step. By clicking the text input area in each step, users are allowed to edit the text with keyboard input.
    \item Split step. By clicking the `Split step' button associated with one step, we will split the original primary step into two primary steps. A new primary step will start the current step and contain all follow-up sub-steps. For example, if we click `Split step' for the plan show in Figure~\ref{fig:planning} at index `2.2'. We will generate a new blank step `3.' (where user input is expected) and re-index all sub-steps with `2.2.x' to `3.1.x'. At the same time, the original plan steps behind it will be automatically updated. Through this action, users can easily split one step that contains too much information into two primary steps. Figure~\ref{fig:illustration} shows an example of plan edit with `split step'.
\end{itemize}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.75\textwidth]{figures/Screenshot-planning.png}
%     \caption{Screenshot of user-involved planning interface.} 
%     \label{fig:planning}
%     \Description{Screenshot of user-involved planning interface. At the top, we show the task description along with three buttons: show potential actions, plan edit instruction, and add one step. At the bottom, we show a step-wise plan for setting alarms. Users can click these buttons to achive the function we described in the user-involved planning to edit the plan.}
% \end{figure}


\subsection{Execution}
\label{sec-method-execution}

After the planning stage, we obtain a plan with a step-wise structure. 
In the execution stage, the LLM agent executes the outcome of the planning stage \revise{(\ie a step-wise plan in text)} in a step-by-step manner. 
\revise{In each step, the LLM agent  translates a single step of the plan into one action, which is implemented with an API call in the backend. 
This setup is a simulation of real-world applications, which provide services with API calls (commonly implemented as langchain tools\footnote{\url{https://python.langchain.com/v0.1/docs/modules/tools/}}). Such a simulation setup is effective in developing and validating theory~\cite{davis2007developing} and has been widely adopted in existing research on agent-based modeling and HCI studies~\cite{olson2014ways}.} %\glcomment{Shall we highlight `HCI studies' instead of `crowdsourcing studies'?}
To provide a smooth user experience, we adopted a conversational interface to present the execution process. 
Figure~\ref{fig:execution-interface} shows one screenshot of user-involved execution in our study. 
As we can see, after a message of the first primary step of the plan, the LLM agent predicts one action `create\_alarm'. 
In our study, to provide a tidy view of the action prediction, we wrap the predicted action as one card (the blue area in Figure~\ref{fig:execution-interface}). 

% \begin{figure*}[htbp]
%  \centering
%   \subfigure[]%Illustration of user-involved execution of one primary step.]
%   {\label{fig:execution-flow-chart}
%   \centering
%   \includegraphics[width=0.48\textwidth]{figures/execution_stages-new.pdf}
%  }
%   \subfigure[]%Screenshot of conversation interface for user-involved execution.]
%   {\label{fig:execution-interface}
%   \centering
%   \includegraphics[height=0.46\textwidth,width=0.4\textwidth]{figures/execution-interface.png}
%  }
%  \centering
%  \caption{User-involved execution flow chart and interface. \revise{Panel (a): a flow chart illustrating how each primary step is executed with two stages: action prediction and action execution.} Panel (b): a screenshot of the conversation interface for user-involved execution.
%  }
%  \label{fig-interface}
%  \Description{User-involved execution flow chart and interface. In the left panel, a flow chart shows how each primary step is executed with two stages: action prediction and action execution. In the right panel, we show one example of a user-involved execution page, which follows a conversation to show the step-by-step execution process. After one action is predicted, users can choose from options `Proceed', `Feedback', and `Specify Action' to approve or involve in the real-time execution.}
% \end{figure*}

%\glcomment{A major concern I have in this research is the execution stage mentioned in section 3.3. The system appears to involve revealing the plan in the user interface (Fig. 3), but the paper does not specify how these tasks are actually accomplished. For instance, how is an alarm being set? Does the system generate code to create an alarm function and generate HTML/CSS code to provide an interface? Does the system fetch available flights and book them through API calls? What constitutes an “execution” of a task must be clearly defined to better understand the scope of this research, and this paper did not provide this information.}\ujcomment{We need to make two important points here: 1. We are focusing on the fundamental aspects of the human-AI interaction here. It is true that other aspects such as how the alarm is being set, the interfaces etc. can be relevant, but that is out of the scope of this work. Discuss, limitations. 2. Use of APIs, etc.}

\paratitle{User-involved Execution}. 
Figure~\ref{fig:execution-flow-chart} \revise{presents} a flow-chart to illustrate a primary step executed by the daily assistant (\ie LLM agent). First, given one primary step, the daily assistant predicts an action based on a given list of prepared actions \revise{(\ie pre-defined APIs in the backend)}. %\glcomment{Check our usage of action/tool/API, make it consistent. Or shall we just explain we use them interchangeably in this work?}
After users check the predicted action, they can choose from \revise{one of the following} three buttons to respond. \textbf{(1)`Proceed':} %\ujcomment{mention the other buttons?}. 
It indicates users agree that the predicted action is correct. After clicking this button, the LLM agent moves forward to execute it and shows the execution result of this action. 
\textbf{(2) `Feedback':} Users can give text feedback based on the message input area at the bottom of the conversational interface. This  triggers another action prediction based on the current primary step and user feedback. Then, users are  provided with the three options \revise{to proceed} again. \textbf{(3) `Specify Action':} Users can override the current action prediction with the manual specification of one action. If users choose this response, they are first asked to choose one action from the prepared action list and then fill in the parameters manually. The LLM agent directly executes the user-specified action. 
After one action is executed, if users are not satisfied with the results, they can choose to re-execute this step by providing text feedback \revise{(\ie by clicking button `Give feedback and try again')}, which works similarly to the `Feedback' option. 
If users are satisfied with the execution results, they can click the `Next Step' button and move to execute the next primary step. 
By iterating over this process through the step-wise plan, users \revise{can choose to either} approve or get involved in modifying the execution outcomes in each step. \revise{All actions are predicted and executed in the backend (\ie the respective API calls are triggered).} %Users will only follow the conversation to interact with LLM agents.}\glcomment{@Ujwal, if the last sent are not necessary, we can delete them}


% \begin{figure*}[htbp]
%  \centering
%   \subfigure[Illustration of user-involved execution of one primary step.]{\label{fig:execution-flow-chart}
%   \centering
%   \includegraphics[width=0.48\textwidth]{figures/execution_stages.pdf}
%  }
%   \subfigure[Screenshot of conversation interface for user-involved execution.]{\label{fig:execution-interface}
%   \centering
%   \includegraphics[height=0.42\textwidth,width=0.4\textwidth]{figures/execution-interface.png}
%  }
%  \centering
%  \caption{User-involved execution flow chart and interface. Panel (a): a flow chart shows how each primary step is executed with two stages: action prediction and action execution. Panel (b): a screenshot of the conversation interface for user-involved execution.
%  }
%  \label{fig-interface}
%  \Description{User-involved execution flow chart and interface. In the left panel, a flow chart shows how each primary step is executed with two stages: action prediction and action execution. In the right panel, we show one example of a user-involved execution page, which follows a conversation to show the step-by-step execution process. After one action is predicted, users can choose from options `Proceed', `Feedback', and `Specify Action' to approve or involve in the real-time execution.}
% \end{figure*}

\subsection{Hypotheses}
\label{sec:hypo}

Our experiment is designed to answer questions of how human involvement in the planning and execution stages will shape their trust and overall task performance. 
To analyze such impact, we regulate the levels of automation in the %~\glcomment{If we want to highlight delegation, we can also use full delegation in each stage as baselines. But I guess automation is easier to understand} 
LLM agent through the planning and execution stage as baselines for comparison. 
The automatic planning and execution denotes that the LLM agent directly generates the task outcomes without user involvement.

With user involvement in the planning stage, users have the opportunity to fix potential mistakes or issues in the plan generated by LLMs. 
Working on such plan editing tasks is similar to debugging, which has been argued to bring about a critical mindset~\cite{he2024err} to the generated plan. 
With a critical mindset, users may better calibrate their trust in the planning outcome. 
We also consider user involvement in planning to be beneficial to the plan quality, which can then contribute to the overall task performance. 
Thus, we hypothesize that:
\begin{framed}
\noindent\textbf{(H1)}: Compared to automatic planning, user-involved planning will result in a higher calibrated  trust {in the plan}.\\
\noindent\textbf{(H2)}: Compared to automatic planning, user-involved planning will result in better overall task performance.
\end{framed}

% Compared with automation by the AI system, user involvement may bring a sense of control, which can increase user trust in the task outcome.\glcomment{add reference} However, it does not indicate users can figure out when the AI system is trustworthy. 
% With user involvement in the execution, users will manually check the action prediction and execution results of each primary step. 
In the user-involved execution process, users manually check the action prediction and execution results of each primary step. 
% Such user involvement also increases user engagement in the step-by-step execution process. 
Such user involvement increases the chances of discovering potential mistakes of LLM agents.
Once users realize that the LLM agent made mistakes, they can get involved in modifying the execution outcome of the current step. 
By fixing these mistakes, the overall task performance gets improved. 
With such involvement in fixing potential errors, users will be more critical of trusting the task outcome. 
%\glcomment{Is this claim true? I find that calibrated trust here may not be very convincing. I find it can be users tend to trust the execution more, as they have a stronger sense of control}
% Such user involvement enables rich interactivity with the LLM agent and ensures that humans can fix the errors made by the LLM agent. 
Therefore, we hypothesize that:
\begin{framed}
\noindent\textbf{(H3)}: Compared to automatic execution, user-involved execution will result in a {higher} calibrated trust {in execution outcome}.\\
\noindent\textbf{(H4)}: Compared to automatic execution, user-involved execution will result in better overall task performance.
\end{framed}
