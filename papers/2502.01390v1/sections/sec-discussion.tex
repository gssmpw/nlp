\section{Discussion}
\label{sec:discussion}

%\glcomment{I would recommend inviting the authors to revise the paper to rewrite the results section with interpretations that are faithful to the otherwise frequentist approach of the paper. This would also lead to natural ways to update the discussion of the paper. For instance, I would appreciate a deeper discussion about how different approaches to providing human oversight could affect the results (since it is possible for example that more granular or more superficial representations of the plan could lead to different effects). \ujcomment{I think this comment remains unaddressed.} I believe that simplifying the reporting of the results this way, will lead to a more impactful paper that advances the discourse the authors are hoping to contribute to.}

\subsection{Key Findings}
\label{sec:discussion-findings}
Our experimental results show that user involvement in the plan-then-execute workflow with LLM agents can %bring benefits by fixing
help fix imperfect plans in planning and wrong action predictions in the step-by-step execution. 
However, user involvement does not ensure a consistently positive impact on calibrated trust and overall task performance across different tasks. 
%\glcomment{current organization of findings try to answer the RQs. If you find it helpful, we can also mention RQs here}\glcomment{To-do: connecting with existing work in presenting our findings}
% \glcomment{2AC comment 1: please explain the failure of some tasks. }

\paratitle{User Involvement Fails to Calibrate User Trust}. 
Overall, user involvement in the planning and execution does not significantly impact user trust and calibrated trust in planning and execution outcomes. 
As Table~\ref{tab:h1-res} shows, user involvement in planning can harm plan quality in tasks with a high-quality initial plan, which may potentially cause worse task performance in the subsequent execution stage. 
Our experimental results do not support \textbf{H1} or \textbf{H3}, which indicates user involvement does not necessarily help calibrate user trust in our study. 
Instead, with a task-specific correlation analysis (cf. Table~\ref{tab:correlation-2}), we found that the plan quality has a significant positive correlation with calibrated trust in both planning and execution outcomes. 
Combined with task-specific user trust and task-specific confidence, we can infer that users tend to trust the LLM agent overall. 
Such trust can be expected and calibrated in tasks with a high-quality plan. 
In contrast, users fail to calibrate their trust in the tasks where a low-quality plan is provided. {A potential cause of such miscalibrated trust is the plausibility of plans generated by LLMs (i.e., plans that appear to be likely correct).} 
In our study, all initial plans are formulated with a clear, logical structure, which covers most of the task requirements. 
At first glance, such high-quality text pieces seem quite plausible and trustworthy. 
We also received some open text feedback such as, --- ``The plans look nice, I do not find any space for improvement'' and ``the planning stage of the LLM assistant was helpful and trustworthy.'' 
Findings from recent work on LLM-assisted fact checking corroborate this, wherein authors found that convincing explanations provided by LLMs can cause over-reliance when LLMs are wrong~\cite{si2024large}.
% The existing work on LLM-assisted writing also provides some similar findings: when the LLMs generate high-quality text, users can easily be satisfied with them without further improvement.
%\glcomment{Shall we stop here and pick up trust calibration in the implications?} 

\paratitle{User Involvement can Benefit Task Performance}. 
User involvement in planning and execution can positively impact overall task performance, especially execution accuracy. 
As the results in Table~\ref{tab:h1-res} and Table~\ref{tab:h2-res} show, user involvement in planning can help address imperfect plans (\eg task-1 with grammar error). Doing so further contributes to improvements in the execution accuracy. 
After controlling the plan quality, we found that user involvement in the execution can provide the best execution accuracy among most tasks considered in our study (cf. Table~\ref{tab:h4-res}). 
Based on the failure analysis (Section~\ref{sec-failure-analysis}), LLM agents can make mistakes in executing high-quality plans, which can be attributed to prediction errors (\ie wrong action name or action parameters) and prediction failures (\ie failure to provide valid action prediction). 
In practice with deployed LLM services, there is no reliability guarantee for the generated plan in planning or predicted actions in execution. 
User involvement can play an important role in the plan quality control and risky action control, ensuring \revise{that} only correct and {safe} actions \revise{are} executed to obtain desirable task outcomes. %\glcomment{If you think it is better to show positive findings, we can move this before calibrated trust. Now I follow the order of research questions}

\paratitle{Other Findings}. %Besides the main impact of user involvement on calibrated trust and overall task performance. 
We also found some user factors and perceptions that affect user trust and task performance. 
As seen in Table~\ref{tab:correlation}, nearly all covariates show a significant positive correlation with user trust in the AI system. 
Some of these covariates also impact user trust in the planning and execution outcomes. 
Overall, these findings indicate that users who are familiar with such systems tend to show higher user trust. 
However, some factors also correlate negatively with the calibrated trust in the execution outcomes and risk perception of using the LLM agents as daily assistants. 
This reflects that these factors can cause miscalibrated trust and reduced risk perception when working with the LLM agent. 
{While we found that risk perception negatively correlated with user trust, calibrated trust, task performance, and confidence (cf. Table~\ref{tab:correlation-2}), it does not mean risk perception is harmful in the human-LLM agent collaboration. 
The main cause is that users may only notice the risks of using LLM agents when the task is provided with a relatively low-quality plan.} 
Risk perception is important to calibrate user trust in the planning and execution outcomes. Collaborative workflows should support users with the provision to take over control of planning and/or execution stages based on their perceived risk.
%And it may be beneficial to provide alternatives (\eg manual work) when users perceive risks in adopting an LLM agent for assistance.
 
\subsection{Implications}
% Our findings suggest that user involvement in planning and execution can fix imperfect plans and errors in the step-by-step execution. 
% Thus, human involvement shows effectiveness and improved task performance in this context. 
% However, user involvement in the LLM agent workflow is insufficient to calibrate user trust in the AI system and task outcome. 
% With a plan-then-execute style LLM agent, the plan plays an important role in human control over the LLM agent. 
% While LLMs can generate plausible, high-quality stepwise plans, their correctness is not guaranteed. 
% When users are presented with a plausible but misleading plan, they may show uncalibrated trust in the AI system, which can harm the task performance and lead to risky outcomes.
% Meanwhile, the LLMs can generate high-quality and plausible plans, which may mislead user trust in the AI system. 
%It implies that 
% Future work should look more closely at the effects of the plausibility of LLM generated plans and how we can handle such convincingly wrong LLM outcomes.
% \glcomment{shall we use ``convincingly wrong''? It is used in one of existing NLP literature of human-LLM collaboration}
%\glcomment{It would be great to discuss how we can handle plausible but imperfect/convincingly wrong LLM outcomes. But it seems we do not have any results to give implications about it. Shall we ignore it in the implications part and just take it as future directions?}\glcomment{I still add one paragraph about it. To check and discuss}\gdcomment{definitely future work, but could mention using model confidence scores to highlight a higher probability of mistakes which could calibrate trust better.}

\paratitle{The Impact of Convincingly Wrong LLM Outcomes}. 
% Our observations indicate that the convincingly wrong LLM outcomes (\eg plan) can be deceptive and misleading. 
As our study follows a plan-then-execute workflow for users to collaborate with LLM agents, users were not offered a chance to revise the plan after starting with execution. 
% This can be a disaster once users follow the wrong plan to solve the given task.
Users following a wrong plan can lead to negative outcomes. %\glcomment{Is this sentence not a formal expression?}\gdcomment{I tried to rephrase.}
Combined with existing work on algorithm aversion~\cite{dietvorst2015algorithm} and the impact of negative first impressions on user trust~\cite{tolmeijer2021second}, we can infer that such convincingly wrong content~\cite{si2024large} can bias user trust and reliance towards the %two opposite 
extremes. 
Before users take notice, they may develop an uncalibrated trust in the AI system, as observed through our findings in high-risk tasks (\ie tasks 1,2,3) and \revise{corroborating} work by~\citet{si2024large}. 
As a result, users over-rely on AI assistance, which is misuse akin to behavior that resonates with algorithm appreciation~\cite{logg2019algorithm}. 
Once users notice such phenomena, their trust in the LLM-based systems may sharply decrease, resulting in disuse due to algorithm aversion. 
%{One important cause of such bias impact is 
{This can be a result of the misalignment between perceived AI performance and actual AI performance.
% users lack performance feedback of the LLMs. 
Existing human-AI collaboration literature has provided potential solutions for such problems, ranging from performance feedback interventions~\cite{he2023knowing} to agreement-in-confidence heuristic~\cite{Lu-CHI-2021,pescetelli2021role}.
} 
Future work can combine these insights to explore effective interventions for user trust calibration with convincingly wrong LLM outcomes.

\paratitle{Insights for Effective Collaboration with Plan-then-execute LLM Agents}. 
Our work has important theoretical implications for effective human-AI collaboration with plan-then-execute LLM agents. 
% Through the empirical study, we found that the plan quality substantially impacts the subsequent execution process. 
On the one hand, user involvement can be necessary to achieve complementary team performance. 
Although LLM agents have shown promising planning and execution capabilities, they are never perfect due to probabilistic uncertainty. 
With user involvement in the planning, users can fix imperfect plans with grammar errors (cf. Table~\ref{tab:h1-res} task-1). 
With user involvement in the execution, users can fix uncertainty issues (\eg LLM agent predicts invalid actions) and prevent risky actions (\eg LLM agents choose an itinerary conflicting with task requirements, cf. Table~\ref{tab:h4-res}, task-6). 
On the other hand, user involvement may also bring uncertainty and even harm LLM agent performance. 
In tasks where the LLM agent provides a high-quality plan (cf. task 4, 5, 6 in Table~\ref{tab:h1-res} and Table~\ref{tab:h2-res}), user involvement can harm the plan quality, which further negatively impacts the execution accuracy. 
Moreover, user involvement in planning and execution poses a significantly higher cognitive load on users (cf. Figure~\ref{fig:cognitive_load}) and negatively impacts user confidence (cf. Figure~\ref{fig:confidence}). 
Thus, too much human involvement in collaboration with plan-then-execute LLM agents can be undesirable. 
User involvement in the execution process brings more consistent benefits than user involvement in the planning stage. 
As suggested by the participants, iterative LLM agent simulation may be one potential way to decide when users should be involved. 
The LLM agent may first conduct a plan-then-execute round to obtain a clear plan and execution results. 
With humans checking the whole process and simulated outcomes, humans can decide whether to be involved in revising the plan or the execution process. 
In this way, we can minimize user involvement while keeping highly effective task outcomes through LLM agents.
% \glcomment{Sth to further discuss. Instead of adjusting the text plan, it may be more important to adjust the outcome of each plan step. Then it may be more beneficial to combine plan-execute and iterate this process instead of separately adjusting the planning and execution.}
% This will be further discussed in Section~\ref{sec-limitation}.} 

% \paratitle{How to Handle Plausible but Imperfect LLM Outcome}. The experimental results suggest that user involvement is insufficient to calibrate user trust in the task outcomes. 
% One potential cause is that plausible plans may mislead users into imposing uncalibrated trust in the LLM agent.

\paratitle{\revise{Human Oversight and Designing Flexible Collaborative Workflows}}. 
\revise{In our study, we found that human oversight does not consistently lead to improved outcomes. 
One potential cause can be the disparity between the planning and execution of LLM agents. 
Specifically, it is unclear how one plan step will be transformed into one action. When users realize one plan step can be wrong during the execution stage, they may need to articulate it or manually override the agent action, posing a high cognitive load. 
Even worse, when users realize the LLM agent missed one action due to limited steps designed in the plan (in task-1), they do not have a chance to change the plan or add one extra step. 
To address such concerns, we may need a more flexible collaborative workflow where humans can fix planning and execution simultaneously.
% decide when to terminate or extend it. 
In this way, users can exercise more flexible control over the workflow and the task outcomes. 
% Based on such an idea, it may be more efficient to enable users to fix planning and execution on the same page. 
%An instance can be providing actionable planning. 
For instance, 
the action prediction from the LLM agent can be provided along with each step in the planning stage. 
Users can thereby be informed of the potential impact of their edited plan, which provides more straightforward feedback and helps users adjust the plan according to the expected actions.}
% Moreover, users do not need to work in a different mode during the planning and execution stages, which can also reduce their cognitive load. \ujcomment{We can drop the last sentence IMO.}
% }\glcomment{This idea connects with the last paragraph, but it is different. And I think it replies the reviewer comment about `a deeper discussion about how different approaches to providing human oversight could affect the results'. If you have any other ideas/points to add, feel free to comment here.}

\subsection{Caveats and Limitations}
\label{sec-limitation}
\paratitle{Limitations and Potentail Biases}.
To ensure reliable task outcomes, humans are expected to fix imperfect plans (\eg grammar errors, misleading action intents) in the planning stage. 
% Task 1 in our study shows that user-involved planning can fix grammar errors. 
However, not everyone in conditions UP-AE and UP-UE noticed such grammar errors and split the plan in task-1. 
Similarly, not everyone in conditions AP-UE and UP-UE noticed that the LLM agent chose an itinerary that conflicted with task requirements. 
%{One potential cause is that the crowd workers in our study do not show enough engagement in the tasks, which may be due to a lack of motivation.} 
%Moreover, 
%as we mentioned in the implications,
As discussed earlier, LLM agents can generate plausible plans, which may mislead user trust in the planning and execution outcomes. 
In that case, participants in our study may have easily ignored some convincingly wrong plan steps or execution actions. 
%To alleviate such bias or misleading impact, we can consider providing more rewards/punishments associated with task execution outcomes in the practice of human-LLM agent collaboration.
In our study, one primary step in the plan is only transformed into a single action. In practice, LLM agents can generate multiple actions for one specific goal. 
However, such action generation and execution modes are challenging for humans to get involved in and control, as the execution of the action sequences is automated by the LLM agent within one goal. 
Furthermore, using multiple actions to achieve one primary step (\ie goal) also results in higher task complexity and reduced task clarity, which may impact the task outcomes %in a crowdsourcing user study
~\cite{gadiraju2017clarity}. %With these concerns, we finally choose a one-by-one correspondence between the step-wise plan and the step-by-step execution process.

\paratitle{Transferability Concerns}. Although we selected representative tasks for daily scenarios, our study may not be enough to cover all potential cases of daily assistance with LLM agents. 
% For example, it is unclear how users can collaborate with LLM agents when the plans are not reliable. 
% However, in practice, we only adopt LLM agents for daily assistance when they show superior performance in these tasks. 
Some task characteristics (\eg task complexity, time consumption) may also impact how users are willing to rely on AI assistance. 
Meanwhile, full control over the plan-then-execute LLM agents may not be desirable for some simple tasks (\eg setting alarms). 
Once the efforts to control/interact with LLM agents are greater than the efforts to execute the tasks themselves, users will be unwilling to adopt such ``assistance." 
Future work can look into what daily user needs are suitable for LLM agents to support. 
% It may also help explain why some users prefer to do the tasks themselves (cf. comments in open text feedback in Table~\ref{tab:example-topic}). 
% However, we considered the benefits of daily assistance in our task selection process (\eg reduced information-seeking efforts). 
\revise{In our study, the execution of plans is conducted in a simulation environment. While it has been proven to be effective in prior work of agent-based modeling and HCI studies~\cite{olson2014ways}, more work is needed to understand how execution of tasks in real-world environments with additional dependencies and complexities can influence our findings.} %and implications may still differ from real practice, where misplaced trust will come at a cost. More work is required to understand how users and LLM agents can effectively collaborate in real tasks.}\glcomment{Added several sentences to address the simulation comment.}

Participants in our study only followed a relatively fixed mode in collaboration with LLM agents, they can determine when to get involved in the planning and execution stages. 
The experimental conditions considered in our study range from full automation (\ie AP-AE) to full user control (\ie UP-UE). 
Such a setup provides good flexibility, which simulates real-world practice. 
Our findings and implications provide valuable insights to guide future research on human-AI collaboration with LLM agents.
% \glcomment{I feel this reasoning is a bit weird, to further refine}
% \glcomment{To further expand limitation subsection}

% \paratitle{Potential Bias}