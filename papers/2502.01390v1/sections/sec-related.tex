\section{Background and Related Work}
Our work proposes to analyze how user involvement in the planning and execution stages of LLM agents shapes user trust in the LLM agents and the overall task performance of LLM agents. 
Thus, we position our work in \revise{three realms} of related literature: human-AI collaboration (\S~\ref{sec-rel-collaboration-LLM}), \revise{trust and reliance on AI systems (\S~\ref{sec-rel-trust-reliance}),} task support with LLMs and LLM agents (\S~\ref{sec-rel-LLM-agent}). %\glcomment{Be careful with the human-AI collaboration. I find it a bit distracting from our focus}

%\glcomment{the paper did not provide a strong review of literature surrounding AI trust. While I am not an expert in understanding trust in AI, I do know there is rich literature in this area. Because this review was not included, it is difficult to evaluate the originality of this work.}\ujcomment{Improve the section, but also point towards recent reviews for readers to get a more comprehensive view (e.g., Siddharth Mehrotra's recent trust review paper)}

\subsection{Human-AI Collaboration}
\label{sec-rel-collaboration-LLM}
% LLM agent in CHI~\cite{zhang2024s}
% \ujcomment{- General human-AI collaboration; delegation; AI-assisted decision making; trust/reliance; complementary performance; 
% - Metrics introduced in recent years;
% - What has actually worked or shown promise in facilitating optimal human-AI collaboration?
% - How do we position our work in this context?}

% \paratitle{Delegation, algorithm appreciation, algorithm aversion, control}. 
In recent decades, deep learning-based AI systems have shown promising performance across various domains~\cite{yang2022survey,fernando2021deep} and applications~\cite{pouyanfar2018survey,dong2021survey}. 
However, such AI systems are not good at dealing with out-of-distribution data~\cite{jia2017adversarial,mccoy-etal-2019-right}, and their intrinsic probabilistic nature brings much uncertainty in %practical service
practice~\cite{ghahramani2015probabilistic}. 
Such observations raise wide concerns about the accountability and reliability of AI systems~\cite{kaur2022trustworthy}. 
Under such circumstances, human-AI collaboration has been recognized as a well-suited approach %one promising approach 
to taking advantage of their promising predictive power and ensuring trustworthy outcomes~\cite{lai2021towards,jiang2021supporting}. 
While humans can provide more reliable and accountable task outcomes, too much user involvement to check and control AI outcomes is undesirable~\cite{lai2022human}. 
It goes against the premise that AI systems are introduced to reduce human workload. 
In that context, researchers have theorized and empirically analyzed when and where users could and should delegate to AI systems~\cite{lai2022human,lubars2019ask}. 

\paratitle{Task Delegation}. While humans prefer to play the leading role in human-AI collaboration~\cite{lubars2019ask}, delegating to AI systems can bring benefits like cost-saving and higher efficiency. 
Apart from manual delegation decisions, it is common to apply automatic rules for human delegation (\eg heuristics obtained from domain expertise or manually crafted rules~\cite{lai2022human}).
% Humans can delegate to AI decisions based on . 
Many user factors like trust~\cite{lubars2019ask}, human expertise domain~\cite{erlei2024understanding}, and AI knowledge~\cite{pinski2023ai}) have a substantial impact on human delegation behaviors. 
% With an empirical study, Erlei \etal~\cite{erlei2024understanding} found that the human expertise domain impacts human delegation behaviors, and the choice independence will be violated when users consider AI performance in an unrelated task.
% Erlei \etal conducted an empirical study to analyze the impact of choice independence and error type in the appropriate delegation behaviors~\cite{erlei2024understanding}. 
%Besides human delegation to AI systems, 
Another relevant stream of recent research has explored AI delegation to humans~\cite{madras2018predict,fugener2022cognitive,pinski2023ai}. 
Researchers have investigated the conditions under which AI systems should defer to a human decision maker, which may bring benefits of improved fairness~\cite{madras2018predict}, accuracy~\cite{narasimhan2022post}, and complementary teaming~\cite{ijcai2022p344}. 
Compared to human delegation, AI delegation has been observed to achieve more consistent benefits in team performance~\cite{fugener2022cognitive,hemmer2023human}. {In collaboration with LLM agents, users need to determine when they should be involved in high-level planning and real-time execution. Such involvement decisions are similar to the delegation choices made by users. While task delegation is not the focus of our study, future work can explore this further.}% within human-LLM agent collaboration.}


\paratitle{AI-assisted Decision Making} has attracted a lot of research focus in human-AI collaboration literature. 
Most existing work has conducted empirical studies~\cite{lai2021towards} and structured interviews~\cite{jiang2021supporting} to understand how factors surrounding the user, task, and AI systems affect human-AI collaboration. 
User factors like AI literacy~\cite{Chiang-IUI-2022}, cognitive bias~\cite{rastogi2022deciding}, and risk perception~\cite{fogliato2021impact,green2021algorithmic} have been observed to substantially impact user trust and reliance on the AI system. 
Similarly, task characteristics like task complexity and uncertainty~\cite{salimzadeh2023missing,salimzadeh2024dealing} and factors of the AI system (\eg performance feedback~\cite{bansal2019beyond,Lu-CHI-2021}, AI transparency~\cite{vossing2022designing} and confidence of AI advice~\cite{tomsett2020rapid,Zhang-FAT-2020}) also affect user trust and reliance on the AI system. 
For a more comprehensive survey of existing work on AI-assisted decision making, readers can refer to~\cite{lai2021towards}.

% Typically, user trust is operationalized as a subjective attitude toward AI systems/AI advice within the literature on human-AI collaboration. In comparison, user reliance on AI systems is based on user behaviors (\eg adoption of AI advice and modification of AI outcomes). 
% Such formulation can even be dated back to trust and reliance on automation systems~\cite{lee2004trust}.

% \paratitle{Calibrated Trust and Appropriate Reliance}. User trust in the context of human-AI collaboration is typically operationalized as a subjective attitude toward AI systems/AI advice~\cite{lee2004trust}. In comparison, user reliance on AI systems is based on user behaviors (\eg adoption of AI advice and modification of AI outcomes). 
% As pointed out by existing work on trust in algorithmic/automated systems, user trust can substantially affect user reliance~\cite{lee2004trust}. 
% While trust calibration is an important goal in human-AI collaboration, it may be not enough to ensure complementary team performance. 
% Through empirical user studies with different confidence levels of AI predictions, Zhang \etal~\cite{Zhang-FAT-2020} found that ``trust calibration alone is not sufficient to improve AI-assisted decision making''. 
% To achieve optimal human-AI collaboration, humans and AI systems need to play complementary roles~\cite{hemmer2021human,hemmer2024complementarity}, and humans need to know when they should adopt AI assistance. 
% In other words, humans should rely on AI advice when AI systems are correct and outperform them, and override AI advice when AI systems are incorrect or less capable than humans. 
% Such user reliance patterns are denoted as \textit{appropriate reliance}~\cite{schemmer2022should,schemmer2023appropriate}, which is the key to
% achieving complementary team performance. 

% Compared with human assistance, users can easily lose confidence in AI systems after seeing them make the same mistakes~\cite{dietvorst2015algorithm}. 
% Such algorithm aversion can be overcome by enabling users to modify the AI predictions~\cite{dietvorst2018overcoming}. 
% As a result of these 
% under-reliance (disuse AI assistance when AI systems outperform humans) and over-reliance (misuse AI assistance when AI systems are wrong or perform worse than humans).
% , users show contradicting attitudes towards AI assistance: algorithm appreciation~\cite{logg2019algorithm,hou2021expert} and algorithm aversion~\cite{dietvorst2015algorithm,dietvorst2018overcoming}. 

% \paratitle{User Trust}. 
% Most existing work has conducted empirical studies~\cite{lai2021towards} and structured interviews~\cite{jiang2021supporting} to understand user trust in AI systems. 



% As a result of uncalibrated trust, users also show sub-optimal reliance on the AI systems: under-reliance (disuse AI assistance when AI systems outperform humans) and over-reliance (misuse AI assistance when AI systems are wrong or perform worse than humans).\glcomment{Is this claim true: unexpected reliance due to uncalibrated trust?}

% To achieve optimal human-AI collaboration, humans and AI systems are supposed to play complementary roles~\cite{hemmer2021human,hemmer2024complementarity}, and humans know when they should adopt AI assistance. 
% In other words, humans should rely on AI advice when AI systems are correct and outperform them, and humans should override AI advice when AI systems are incorrect or less capable than humans. 
% Such user reliance patterns are denoted as appropriate reliance~\cite{schemmer2022should,schemmer2023appropriate}, which is the key to
% achieving complementary team performance. 
% Many factors like cognitive bias~\cite{he2023knowing}, 
% \paratitle{Interventions to Facilitate Calibrated Trust and Appropriate Reliance} 
% The main issues that lead to sub-optimal human-AI collaboration are: under-reliance (\ie disuse AI assistance when AI systems outperform humans) and over-reliance (\ie misuse AI assistance when AI systems are wrong or perform worse than humans)~\cite{schemmer2022should}. 
% %These reliance behaviors are also highly relevant to uncalibrated user trust. 
% Users with an uncalibrated trust in the AI system can be easily misled to disuse or misuse AI systems~\cite{jacovi2021formalizing}. 
%For example, compared with human assistance, users can easily develop a negative impression of AI systems and lose confidence in AI systems. Such phenomenon is called algorithm aversion~\cite{dietvorst2015algorithm}. 
%By contrast, some users were influenced more by algorithmic decisions instead of human decisions, and they first coined the notion of ``Algorithm Appreciation''~\cite{logg2019algorithm}. 
% Researchers have proposed various interventions to promote appropriate reliance~\cite{he2023knowing,Lu-CHI-2021,lu2024does,chiang2021you,Chiang-IUI-2022} and calibrate user trust in AI systems~\cite{buccinca2021trust,Zhang-FAT-2020}.  
% % We bring some representative interventions here.
% %\glcomment{Here is not good enough. I don't plan to bring too many examples. To check how to improve}
% For example, explainable AI methods have been shown to help reduce over-reliance~\cite{vasconcelos2023explanations} and under-reliance~\cite{wang2021explanations} in different scenarios albeit with little consistency across contexts. 
% Another example is tutorial interventions, which have shown effectiveness in user onboarding~\cite{lai2020chicago}, mitigating cognitive biases~\cite{he2023knowing} and developing AI literacy~\cite{Chiang-IUI-2022}. 
% For a more comprehensive overview of interventions to facilitate trust calibration and appropriate reliance, readers can refer to ~\cite{lai2021towards,eckhardt2024survey}.

% In this work, we analyze how user involvement in the planning and execution stages of LLM agents will shape user trust and affect overall task performance. 
% It is highly relevant to existing studies of human-AI collaboration about user trust and appropriate reliance. 
While machine learning and deep learning methods have been extensively analyzed in existing human-AI collaboration literature, to our knowledge, human-AI collaboration with LLM agents is still under-explored. 
Unlike previous studies where AI systems only follow a fixed mode to generate advice, LLM agents can be equipped with more logical clarity and can provide a step-wise plan and can follow a step-by-step execution. 
With such a plan-then-execute setup, LLM agents can bring high flexibility as well as uncertainty in high-level planning and real-time execution. Little is known about
%Meanwhile, it is unclear 
how well LLM agents can work as daily assistants while handling tasks entailing varying stakes and potential risks. %where wrong actions may cause a loss. 
In our study, we analyzed the impact of user involvement in such AI systems by adjusting their intermediate outcomes (plan and step-by-step execution) to calibrate their trust and improve task outcomes. 
Our findings and implications can help advance the understanding of the effectiveness of LLM agents in human-AI collaboration.
% with humans.

%\glcomment{I find the positioning of our work in the space of human-AI collaboration is a bit challenging. Our major claim is: human-AI collaboration with LLM agents is under-explored}

\subsection{\revise{Trust and Reliance on AI systems}}
\label{sec-rel-trust-reliance}
\revise{Trust and reliance have been important research topics since human adoption of automation systems~\cite{lee2004trust,dzindolet2003role}. Due to the widespread integration of AI systems in nearly all walks of society, %In recent years, 
there has been a growing interest in understanding user %researchers have developed a strong interest in 
trust~\cite{vereschak2021evaluate,mehrotra2024systematic} and reliance~\cite{eckhardt2024survey} on AI systems.}
User trust in the context of human-AI collaboration is typically operationalized as a subjective attitude toward AI systems/AI advice~\cite{lee2004trust}. 
In comparison, user reliance on AI systems is based on user behaviors (\eg adoption of AI advice and modification of AI outcomes). 
\revise{The two constructs have been shown to be highly related~\cite{lee1992trust,lee2004trust}: for example, user trust can substantially affect user reliance~\cite{lee2004trust}. 
However, they are intrinsically different and cannot be viewed as a direct reflection of each other~\cite{kahr2024understanding}. 
Most existing work has, therefore, studied the two constructs separately in terms of subjective trust and objective reliance.}
% As pointed out by existing work on trust in algorithmic/automated systems, user trust can substantially affect user reliance~\cite{lee2004trust}. 

\revise{%In an early analysis of human-AI trust, most literature 
Earlier work exploring human-AI trust primarily focused on the impact of different contextual factors surrounding user (\eg risk perception~\cite{green2021algorithmic}), task (\eg task complexity~\cite{salimzadeh2023missing}), and system (\eg stated accuracy~\cite{yin2019understanding,Zhang-FAT-2020}). 
% Among this literature, performance indicators of the AI system (\eg stated accuracy~\cite{yin2019understanding,Zhang-FAT-2020} and confidence~\cite{rechkemmer2022confidence}) have been extensively studied. 
% For trust calibration, researchers have proposed different interventions like explanations~\cite{Zhang-FAT-2020}, educational tutorials~\cite{Chiang-IUI-2022,chiang2021you,lai2020chicago}, competence comparison~\cite{ma2023should}. 
%According to empirical studies about AI-assisted decision making~\cite{yin2019understanding}, 
Empirical studies have shown that most users tend to trust AI systems that are perceived to be highly accurate~\cite{yin2019understanding}. 
Such trust is vulnerable, as the AI system may provide an illusion of competence with persuasive technology (\eg explanations~\cite{chromik2021think,He-IUI-2025}) or overclaimed performance~\cite{yin2019understanding}. 
Even if the AI systems are accurate on specific datasets, they still suffer from out-of-distribution data~\cite{liu2021understanding,chiang2021you}. 
The misplaced trust in the AI systems can lead to misuse of the systems.
% But such trust can be fragile. 
Several empirical studies~\cite{tolmeijer2021second} have shown that once users realize the AI system errs or performs worse than expected, their trust in the AI system can be violated, %In the extreme case, it can 
even resulting in the disuse of the AI system. 
Both the misuse and disuse of the AI system hinder optimal human-AI collaboration. 
}

\revise{%To calibrate user trust in the AI system, 
To address such concerns, researchers have explored how to help users calibrate their trust in the AI system. %researchers proposed 
Different techniques to help users realize the trustworthiness of the AI system have been proposed~\cite{kaur2022trustworthy,rechkemmer2022confidence,ma2023should}. 
For example, increasing the transparency of AI systems by providing confidence scores~\cite{Zhang-FAT-2020}, explanations~\cite{wang2021explanations}, trustworthiness cues~\cite{liao2022designing}, and uncertainty communication~\cite{Sunnie-FAccT-2024}. 
However, the actual trustworthiness of the AI system does not always align with user perception. 
As found by \citet{banovic2023being}, untrustworthy AI systems can deceive end users to gain their trust. 
Another example is that users can develop an illusion of explanatory depth brought by explainable AI techniques~\cite{chromik2021think}, which leads to uncalibrated trust in the AI system. 
Even if users have indicated trust in the AI system, they may turn to rely more on themselves in final decision-making. 
The reasons are complex, and many factors, such as accountability concerns~\cite{lima2021human,tolmeijer2022capable} and cognitive bias~\cite{he2023knowing}, may affect user reliance behaviors. %Much research effort is required to further our understanding of trust calibration in AI systems.
% On the other hand, research has dived deep into calibrating user trust in AI systems.
}

While trust calibration is an important goal in human-AI collaboration, it may be not enough to ensure complementary team performance. 
Through empirical user studies with different confidence levels of AI predictions, Zhang \etal~\cite{Zhang-FAT-2020} found that ``trust calibration alone is not sufficient to improve AI-assisted decision making''. 
To achieve optimal human-AI collaboration, humans and AI systems need to play complementary roles~\cite{hemmer2021human,hemmer2024complementarity}, and humans need to know when they should adopt AI assistance. 
In other words, humans should rely on AI advice when AI systems are correct and outperform them, and override AI advice when AI systems are incorrect or less capable than humans. 
Such user reliance patterns are denoted as \textit{appropriate reliance}~\cite{schemmer2022should,schemmer2023appropriate}, which is the key to
achieving complementary team performance. 

The main issues that lead to sub-optimal human-AI collaboration are: under-reliance (\ie disuse AI assistance when AI systems outperform humans) and over-reliance (\ie misuse AI assistance when AI systems are wrong or perform worse than humans)~\cite{schemmer2022should}. 
Users with an uncalibrated trust in the AI system can be easily misled to disuse or misuse AI systems~\cite{jacovi2021formalizing}. 
Researchers have proposed various interventions to promote appropriate reliance~\cite{he2023knowing,Lu-CHI-2021,lu2024does,chiang2021you,Chiang-IUI-2022} and calibrate user trust in AI systems~\cite{buccinca2021trust,Zhang-FAT-2020}.  
For example, explainable AI methods have been shown to help reduce over-reliance~\cite{vasconcelos2023explanations} and under-reliance~\cite{wang2021explanations} in different scenarios albeit with little consistency across contexts. 
Another example is tutorial interventions, which have shown effectiveness in user onboarding~\cite{lai2020chicago}, mitigating cognitive biases~\cite{he2023knowing} and developing AI literacy~\cite{Chiang-IUI-2022}. 
For a more comprehensive overview of interventions to facilitate trust calibration and appropriate reliance, readers can refer to ~\cite{lai2021towards,eckhardt2024survey,mehrotra2024systematic,kahr2024understanding}.

\revise{LLM agents~\cite{wang2024survey} have gained much popularity in recent years, distinguishing them from most prior AI systems. 
They can communicate through conversation, plan logically, and can be built to leverage powerful external tools to achieve complex functions.
% The interaction between human and AI systems is relatively limited. 
% Users mostly develop trust and reliance on the AI systems via provided information cues (\eg, explanations and references). 
While trust and reliance have been extensively analyzed in existing human-AI collaboration literature, it is still unclear how users trust and rely on AI systems powered by LLM agents. 
% This work addressed this gap with empirical studies to analyze user trust and reliance on the plan-then-execute LLM agents. 
In our work, calibrated trust is adopted as an important goal for human-AI collaboration in the planning and execution stage. 
Meanwhile, users are expected to fix potential errors in the planning and execution stages, reflecting their reliance on the AI system. 
Our work can substantially advance the understanding of trust and reliance on plan-then-execute LLM agents.
}

% \paratitle{position our work}.

\subsection{Task Support with LLMs and LLM Agents}
\label{sec-rel-LLM-agent}
% \glcomment{Do we need to give more context about LLM Agent? It seems more technical}
LLMs and LLM agents bring new opportunities and challenges to human-AI collaboration~\cite{bommasani2021opportunities}. 
%On the one hand, based on the text generation capability of LLMs, it would be possible for humans to directly give text responses and communicate with any AI systems that take LLMs as their backbone. 
%To this end, 
%LLMs and LLM agents bring new opportunities for more flexible and natural interactions with humans. 
% Meanwhile, the natural language understanding and learning capabilities also enable LLMs to evolve with user feedback. 
It is evident that their generation capabilities can help reduce the cognitive effort from humans. %On the other hand, the LLMs also bring new challenges like dealing with 
But LLMs are also riddled with challenges such as hallucination~\cite{ji2023survey} (\ie generated text seems plausible but is factually incorrect). 
% Fatal errors can be made if users get misled by such persuasive technology, resulting in unaffordable costs (\eg medical diagnosis and financial decisions).
Failure to handle such issues may bring fatal errors with unaffordable costs depending on the context (\eg medical diagnosis). 
%As our work is within the scope of human-AI collaboration, users can refer to corresponding literature reviews to obtain more technical background about LLMs~\cite{zhao2023survey} and LLM agents~\cite{xi2023rise,wang2024survey}.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/Screenshot-planning.png}
    \caption{Screenshot of user-involved planning interface.} 
    \label{fig:planning}
    \Description{Screenshot of user-involved planning interface. At the top, we show the task description along with three buttons: show potential actions, plan edit instruction, and add one step. At the bottom, we show a step-wise plan for setting alarms. Users can click these buttons to achive the function we described in the user-involved planning to edit the plan.}
\end{figure*}

% In recent years, LLMs have gained an explosion of popularity among academia and the industry community. 
Due to the capability of generating coherent, knowledgeable, and high-quality responses to diverse human input~\cite{wei2022emergent}, a wide community of human-computer interaction researchers has paid attention to large language models~\cite{liao2023ai}.
% With large volumes of data, large language models can obtain capabilities to help humans in writing. 
% Ideally, any complex task that can be modularized into a chain of different functions can also be achieved with chaining LLMs.
% LLMs can be used to generate dynamic user interface ~\cite{wang2023enabling}, support scientific writing~\cite{shen2023convxai}, and obtain high-quality data annotation~\cite{gilardi2023chatgpt,wang2024human,he2024if}. 
Researchers have actively explored how LLMs can assist users in various tasks like data annotation~\cite{wang2024human,he2024if}, programming~\cite{omidvar2024evaluating}, scientific writing~\cite{shen2023convxai}, and fact verification~\cite{si2024large}. 
All the above functions can be achieved with elaborate prompt engineering using a single LLM. 
By chaining multiple LLMs with different functions, humans can customize task-specific workflows to solve complex tasks~\cite{wu2022ai}. 
Apart from obtaining answers with a one-shot text generation, LLMs also provide convenient conversational interactions. 
Through empirical studies, such conversational interactions have been shown to be effective in human-AI collaboration with multiple applications, such as decision making~\cite{slack2023explaining,lin2024decision,ma2024towards}, scientific writing~\cite{shen2023convxai}, and mental health support~\cite{sharma2023human}. 
With the growing popularity of LLMs, more and more humans have begun to adopt LLMs (\eg ChatGPT) to boost their work efficiency and productivity %in their everyday work
~\cite{zhao2023survey}.

% Meanwhile, researchers also analyzed different system factors associated with LLMs. 
% For example, Kim \etal~\cite{Sunnie-FAccT-2024} found that the LLM's uncertainty Expression can decrease user trust in wrong AI advice, which helps reduce over-reliance.


LLM agents have been shown to have good planning, memory, and toolkit usage capabilities~\cite{xi2023rise, wang2024survey}. 
% External toolkits greatly increase the impact of human-AI collaboration on the real world. 
When suitable toolkits are provided, LLM agents can readily generate a task-specific plan and solve the tasks using toolkits. 
Attracted by the promise of LLM agents, there have been some early explorations~\cite{geissler2024concept,zheng2023synergizing,zhang2024s} of adopting them in human-AI collaboration contexts. 
%To our knowledge, only a few works~\cite{geissler2024concept,zheng2023synergizing,zhang2024s} have explored human-AI collaboration with LLM agents. 
% These works mainly analyzed how LLM agents can serve specific use cases (\eg design creation~\cite{geissler2024concept}) or conducted structured interviews to obtain expert insights~\cite{zhang2024s,zheng2023synergizing}. 
These works were mostly analyzed in specific use cases (\eg design creation~\cite{geissler2024concept}). 
% There is a lack of empirical studies on user trust and team performance in collaboration with LLM agents.
It is unclear how user trust and team performance are affected by user interactions with LLM agents in a sequential decision making setup (\ie solving a task by executing a sequence of actions) where users can be in control of the execution. %\glcomment{It is the only mention of sequential decision making. In my current framing, I try to give a very specific definition of what type of tasks we are focusing on. I mainly motivate LLM agents to provide daily assistance and facilitate daily life. Do you think we want to highlight sequential decision making?}
To fill this research gap and advance our understanding of user control over LLM agents, we carried out a quantitative empirical study. %to obtain empirical evidence.



% \ujcomment{- How do we position our work in this context?}

% \subsection{Human-agent collaboration}
% \glcomment{More traditional work of human-autonomous agent collaboration? Is it too far from our topic?}

% \subsection{User Trust and Reliance in Human-AI Collaboration}
% \label{sec-rel-trust}
