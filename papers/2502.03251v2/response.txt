\section{Related Work}
\vspace{-0.03in}
\subsubsection*{\textbf{Graph Neural Network \& Self-supervised Graph Learning.}}
Popular GNNs include graph convolutional nets and graph transformers.
The former conducts neighborhood aggregation with layer-wise message passing **Kipf, "Semi-Supervised Classification with Graph Convolutional Networks"**__**, while the latter leverages a transformer-like encoder __**Velickovic, et al., "Graph Attention Network"**__. Both of them are typically paired with a classification head or reconstructive loss on a specified graph.
Thus, a major shortcoming of traditional graph models is their limited generalization capability.
Self-supervised learning has been integrated into GNNs in recent years **Grover, et al., "Graph Attention Augmentation"**. 
Instead of coupling GNNs with downstream tasks, self-supervised learning conducts parameter training from the graph data itself via specialized pretext tasks. 
However, graph augmentation for self-supervised learning is nontrivial __**, and the parameters trained on one graph cannot be directly applied to another owing to the difference in attribute distribution.
In other words, existing graph models lack the universality, and are still far from being a foundation model.

  \vspace{-0.05in}
\subsubsection*{\textbf{Graph Foundation Model.}}
% Large language models demonstrate the remarkable success of pre-training a foundation model for multiple downstream tasks **Brown, et al., "Language Models are Few-Shot Learners"**.
% Recently, efforts have been made to replicate the success to the graph domain.
% __**Xiong, et al., "Graph Foundation Model"**__ design graph prompting to unify the downstream tasks, corresponding to the prompt of LLM.
% Up to date, the majority of GFMs are designed for text-attributed graphs assisted by the LLM.
% Specifically, LLM-based GFMs are fed with the graph's textual descriptions, e.g., describing nodes/edges throughout the graph __**, translating graphs into node sequences __**Zhang, et al., "Graph Translation"**, and re-framing textual attributes into language ____.

% Pioneering work **Xu, et al., "Graph Prompting"** designs the graph prompting to , the analogy to language prompt.
% GCOPE __**Sun, et al., "Graph Coordination for Multi-Domain Graphs"** further conducts model training on multi-domain graphs with coordinators in order to improve the generalization capacity to different datasets.
% Recent efforts have been made to integrate GNN into LLM. 
% For example, LLaGA __**Zeng, et al., "Learning Linear Augmentation for Graph Pre-training"** develops a graph translation technique that reshapes a graph into node sequences,
% while OFA ____  unifies different graph data by describing nodes and edges with natural language.

Recent efforts are generally categorized into two groups.
The first group enhances the vanilla GNNs to achieve better generalization capacity, e.g., unifying the downstream tasks with graph prompt __**, and training on multi-domain graphs with coordinators ____.
__**Kazi, et al., "Graph Neural Networks for Node Classification"** generalize SGC ____ for node classification on any graph.
The second group adapts LLM for analyzing graphs.
LLaGA ____ tailors graphs for the language model with node sequences, generated via graph translation,
while OFA ____  unifies different graph data by the language description of nodes and edges.
OpenGraph ____ re-frames textual attributes into language with a hierarchy.
Very recently, __**Li, et al., "Mixture of Graph Experts"** propose a mixture of graph experts.
Existing models  typically struggle to maintain the performance on graphs without textual attributes.
Also, they model graphs in Euclidean space, and tend to trivialize the structural complexity.
Distinguishing from the prior studies, we consider graphs in Riemannian geometry, and design the first GFM exploring graph substructures (structural vocabulary).
%, to the best of our knowledge.

% Today's graph foundation models can generally be categorized into two types. 
% The first type is based on traditional Graph Neural Networks (GNNs) or transformer models. [] proposed a task-specific foundation model for graphs across diverse domains. Additionally, [] introduced a graph prompting method, which imitates the prompting style of large language models to facilitate cross-domain graph pre-training.
% The second type leverages large language models (LLMs) to represent graphs through textual descriptions, such as describing nodes and edges separately [] and converting graphs into node sequences []. 

\begin{figure}[t]
    \centering
    \begin{subfigure}{0.31\linewidth}
        \centering
        \includegraphics[width=\linewidth]{tOriginal.pdf}
        \caption{Original}
        \label{fig:Original}
    \end{subfigure}
    \begin{subfigure}{0.31\linewidth}
        \centering
        \includegraphics[width=\linewidth]{tGCN.pdf}
        \caption{GCN}
        \label{fig:GCN}
    \end{subfigure}
    \begin{subfigure}{0.31\linewidth}
        \centering
        \includegraphics[width=\linewidth]{tRiemannGFM.pdf}
        \caption{RiemannGFM}
        \label{fig:RiemannGFM}
    \end{subfigure}
        \vspace{-0.1in}  
    \caption{Visualization on Cora}
    \label{fig:tSNE}
    \vspace{-0.15in}
\end{figure}

  \vspace{-0.05in}
\subsubsection*{\textbf{Riemannian Manifold \& Graphs.}}
Riemannian manifolds emerge as exciting alternatives for learning graphs __**Ganea, et al., "Hyperbolic Embeddings"**.
Concretely, hyperbolic space is well recognized for its alignment with tree-like (hierarchical) structures,
and hyperbolic GNNs show superior results to Euclidean counterparts **Nickel, et al., "Hyperspherical Space for Graph Clustering"**. 
The geometric analogy of cycles is hyperspherical space, whose advantage of embedding cyclical structures is reported __**Chami, et al., "Graph Convolutional Networks in Constant Curvature Spaces"**.
__**Mallon, et al., "Riemannian Manifolds for Graph Embedding"** formulate a graph convolutional net in constant curvature spaces.
Recent advances report the success of Riemannian manifolds in modeling dynamics  __**Vaswani, et al., "Graph Dynamics"** and clustering __**Zhou, et al., "Riemannian Manifolds for Clustering"**.
We notice that the product manifold has been introduced to study graphs recently, 
and advanced techniques are proposed for node embedding ____.
However, all of them lack the generalization capability to unseen graph structures, and consider node embeddings on the manifold, while we introduce the notion of tangent bundle to GFM.
%the input graph as a whole and thus lack the generalization capability to other graphs of different sizes.
So far, the potential of Riemannian geometry has not yet been released on GFM, and we are dedicated to bridging this gap.