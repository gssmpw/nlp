%!TEX root = ./main.tex
\section{Notation Table}
\vspace{-0.1in}
\begin{table}[h]
\centering
\caption{Importation Notations.}
\vspace{-0.15in}
\label{table. notation}
\begin{tabular}{c|c}
\hline
\textbf{Notation} & \textbf{Description}   \\
\hline
$\mathcal{M},\mathfrak{g}$        & A smooth manifold and Riemannian metric.        \\
\hline
$\mathcal{T}_x\mathcal{M}$        & The tangent space at $\boldsymbol{x}$.        \\
\hline
$\mathcal{T}\mathcal{M}$         & The tangent bundle surrounding the manifold.        \\
\hline
 $d, \kappa$  &  Dimension and curvature. \\
 \hline
 $\mathcal H$, $\mathcal S$ & Hyperbolic/Hyperspherical space. \\
  \hline
 $\mathcal L$ & A unified formalism of Lorentz/Spherical model.\\
 \hline
 $\boldsymbol{o}$ & North pole of the model space. \\
   \hline
 $\mathcal{G}=(\mathcal{V}, \mathcal{E})$ & A graph with nodes set $\mathcal{V}$ and edges set $\mathcal{E}$.\\
    \hline
 $\boldsymbol{p}\in \mathcal{L}$ & Node coordinate on the manifold.\\
  \hline
   $\boldsymbol{z} \in \mathcal{T}_{\boldsymbol{p}}\mathcal{L}$ & Node encoding in the tangent space.\\
  \hline
 $\phi:\mathcal{L}\times\mathcal{L}\rightarrow \mathbb{R}$ & A parameterized scalar map. \\
 \hline
 $f(\cdot):\mathcal{L}^m \rightarrow \mathcal{L}^n$ & Manifold-reserving linear operation. \\
 \hline
 $[\cdot || \cdot]$ & Vector concatenation. \\
 \hline
 $\operatorname{Exp}_{\boldsymbol{x}}(\cdot)$ & The exponential map at point $\mathbf{z}$ \\
 \hline
 $\operatorname{Log}_{\boldsymbol{x}}(\cdot)$ & The logarithmic map at point $\mathbf{z}$ \\
  \hline
 $\operatorname{PT}_{\boldsymbol x \rightarrow \boldsymbol y}(\cdot)$ & The parallel transport from $\boldsymbol x$ to $\boldsymbol y$\\
 \hline
  % $BC(\cdot)$ & The proposed bundle convolution. \\
  % \hline
\end{tabular}
\vspace{-0.1in}
\end{table}

\vspace{-0.1in}
\section{Proofs and Derivations}

In this section, we detail the proofs of Theorem 1 and 2, and show the derivation of the proposed bundle convolution.

\vspace{-0.03in}
\subsection{The Proposed Linear Operation}
% \newtheorem*{thm6}{Theorem 1 (Manifold-preserving of Proposed Operation)} 
% \begin{thm6}
% Given $\boldsymbol x \in \mathcal L_{\kappa }^{d_1}$ and $\kappa \neq 0$,  
% $f_{\boldsymbol W}(\boldsymbol x) \in \mathcal L_{\kappa }^{d_1}$ preserves on the manifold with  any $\boldsymbol W \in \mathbb R^{{d_1}\times{d_1}}$, and 
% $f_{\boldsymbol W}(\boldsymbol x) \in \mathcal L_{\kappa }^{d_2}$ holds for any $\boldsymbol W \in \mathbb R^{{d_1}\times{d_2}}$.
% \end{thm6}
% \vspace{-0.05in}
\begin{proof}
We give all the key equations, and do not list all the algebra for clarity.
The theorem holds if, with a given curvature $\kappa$, $\kappa \neq 0$, the proposed linear operation satisfies $f_{\boldsymbol W}:\mathcal L_{\kappa }^{d_1}\to \mathcal L_{\kappa }^{d_2}$ for  any $\boldsymbol W \in \mathbb R^{{d_1}\times{d_2}}$.
For $\boldsymbol x \in \mathcal L_{\kappa }^{d_1}$, we conduct the linear operation,
\vspace{-0.03in}
\begin{equation}
f_{\boldsymbol W}(\boldsymbol x)=
\left[\begin{array}{cc}
1 & \mathbf{0}^{\top} \\
\mathbf{0} & \alpha \boldsymbol W
\end{array}\right]
\left[\begin{array}{c}
x_t \\
\boldsymbol x_s
\end{array}\right]
= \left[\begin{array}{c}
x_t \\
\alpha \boldsymbol W\boldsymbol x_s
\end{array}\right].
\vspace{-0.05in}
\end{equation}
With the re-scaling factor $\alpha$ defined as $\frac{\sqrt{\kappa^{-1}-sgn(\kappa)x^2_t}}{\|\boldsymbol W\boldsymbol x_s\|^2}$, the result is yielded as follows
\vspace{-0.1in}
\begin{equation}
f_{\boldsymbol W}(\boldsymbol x)
= \left[\begin{array}{c}
x_t \\
\frac{\sqrt{\kappa^{-1}-sgn(\kappa)x^2_t}}{\|\boldsymbol W\boldsymbol x_s\|^2} \boldsymbol W\boldsymbol x_s
\end{array}\right].
\vspace{-0.05in}
\end{equation}
Given the equality of $sgn(\kappa)x_t^2+{\boldsymbol x}_s^\top{\boldsymbol x}_s=\frac{1}{\kappa}$, it is easy to verify the following equality
\vspace{-0.1in}
\begin{equation}
sgn(\kappa)x_t^2+{\boldsymbol x'}_s^\top{\boldsymbol x'}_s=\frac{1}{\kappa},  \quad
\boldsymbol x'_s=
\frac{\sqrt{\kappa^{-1}-sgn(\kappa)x^2_t}}{\|\boldsymbol W\boldsymbol x_s\|^2} \boldsymbol W\boldsymbol x_s,
\vspace{-0.03in}
\end{equation}
holds for any $\boldsymbol W \in \mathbb R^{{d_1}\times{d_2}}$.
That is, $f_{\boldsymbol W}(\boldsymbol x) \in \mathcal L_{\kappa }^{d_2}$ is ensured, completing the proof.
\end{proof}

\vspace{-0.15in}
\subsection{Geometric Midpoint}
% \newtheorem*{thm7}{Theorem 2 (Arithmetic Mean as Geometric Midpoint)} 
% \begin{thm7}
% %With a set of points and their weights $\{\boldsymbol x_i, \nu_i\}_{i \in \Omega}$, $\boldsymbol x_i \in \mathcal L_{\kappa }^{d}$, $\nu_i \in \mathbb R$,
% The arithmetic mean defined as 
% \vspace{-0.05in}
% \begin{equation}
% mid_\kappa(\{\boldsymbol x_i, \nu_{i}\}_{i \in \Omega})= 
% \frac{1}{\sqrt{|\kappa|} } \sum\nolimits_{i \in \Omega}\frac{\nu_{i} \boldsymbol x_i}{\left| \|\sum\nolimits_{j \in \Omega} \nu_{j} \boldsymbol x_j\|_\kappa \right|}, \ \kappa \neq 0,
% \label{eq:mid2}
% \end{equation}
%  is on the manifold $\boldsymbol c=mid_\kappa(\{\boldsymbol x_i, \nu_{i}\}_{i \in \Omega}) \in \mathcal L_{\kappa }^{d}$,
% and is the geometric midpoint  w.r.t. the squared distance $d$.
% \end{thm7}
\begin{proof}
The theorem claims two facts. The first  is the manifold-preserving of the given arithmetic mean, and the second is the equivalence between the mean and geometric midpoint.
We verify the manifold-preserving by manifold definition $sgn(\kappa)c_t^2+\boldsymbol c_s^\top\boldsymbol c_s=\frac{1}{\kappa}$, for any $\kappa$,
$\kappa \neq 0$.

We elaborate on the geometric midpoint (a.k.a. geometric centroid) before proving the equivalence.
Given the set of points of the manifold $\boldsymbol x_i \in \mathcal L_{\kappa }^{d}$ each attached with a weight $\nu_{i}$, $i \in \Omega$,
the geometric midpoint of squared distance in the manifold $\mathcal L_{\kappa }^{d}$  is given by the following optimization problem,
\vspace{-0.03in}
\begin{equation}
\boldsymbol c=\arg \min\nolimits_{\boldsymbol c \in \mathcal L_{\kappa }^{d}} \sum\nolimits_{i \in \Omega} \nu_{i} d^2_\kappa(\boldsymbol c, \boldsymbol x_i), \ \ \boldsymbol x_i \in \mathcal L_{\kappa }^{d}.
\label{eq:min}
\end{equation}
Now, we derive the geometric midpoint as follows.
Recall the fact that $\langle \boldsymbol{x}, \boldsymbol{x} \rangle_\kappa =\frac{1}{\kappa}$
and
$ d_{\kappa}^{2}\left(\boldsymbol{x}, \boldsymbol{y}\right) = \frac{2}{\kappa} - 2\langle \boldsymbol{x}, \boldsymbol{y} \rangle_\kappa$.
We equivalently transform the minimization of the midpoint in Eq. (\ref{eq:min}) to the maximization  as follows,
\vspace{-0.05in}
\begin{equation}
\boldsymbol c=\arg \max\nolimits_{\boldsymbol c \in \mathcal L_{\kappa }^{d}}   \langle \alpha \sum\nolimits_{i \in \Omega} \nu_{i}\boldsymbol{x}_{j}, \boldsymbol{c}\rangle_\kappa, \\
\end{equation}
where  $\alpha$ is a scaling coefficient so that $\alpha \sum\nolimits_{i \in \Omega} \nu_{i}\boldsymbol{x}_{j} \in \mathcal L_{\kappa }^{d}$ ($\alpha >0$). Note that, for any two points $\boldsymbol{x}, \boldsymbol{y} \in \mathcal L_{\kappa }^{d}$, 
we have the inequality 
$\langle \boldsymbol{x}, \boldsymbol{y} \rangle_\kappa< \frac{1}{\kappa}$
and $\langle \boldsymbol{x}, \boldsymbol{y} \rangle_\kappa = \frac{1}{\kappa}$ if and only if $\boldsymbol{x}=\boldsymbol{y}$.
That is, we need to find an $\alpha$ to satisfy 
$\alpha \sum\nolimits_{i \in \Omega} \nu_{i}\boldsymbol{x}_{j}= \boldsymbol{c}$. 
%Assume 
Let $\alpha_0 >0$ satisfies $\alpha_0 \sum\nolimits_{j \in \hat{\mathcal N}_i} \nu_{ij}\boldsymbol{h}_{j}= \boldsymbol{c}$. 
As the midpoint is required to live in the manifold, i.e., $\alpha_0 \sum\nolimits_{i \in \Omega} \nu_{i}\boldsymbol{x}_{j} \in  \mathcal L_{\kappa }^{d}$, 
we have the following equality
\vspace{-0.03in}
\begin{equation}
\langle \alpha_0 \sum\nolimits_{i \in \Omega} \nu_{i}\boldsymbol{x}_{j}, \alpha_0 \sum\nolimits_{i \in \Omega} \nu_{i}\boldsymbol{x}_{j}\rangle_\kappa = \frac{1}{\kappa},
\vspace{-0.01in}
\label{aggproof}
\end{equation}
according to the definition of the manifold in Eq. (\ref{manifold}), yielding the scaling coefficient as follows,
\vspace{-0.05in}
\begin{equation}
\alpha_0 =  \frac{1}{\sqrt{|\kappa|} \left| ||\sum\nolimits_{i \in \Omega} \nu_{i}\boldsymbol{x}_{j}||_\kappa \right|}>0.
\vspace{-0.03in}
\end{equation}
Consequently, the geometric midpoint is given as
\vspace{-0.03in}
\begin{equation}
mid_\kappa(\{\boldsymbol x_i, \nu_{i}\}_{i \in \Omega})= 
\frac{1}{\sqrt{|\kappa|} } \sum\nolimits_{i \in \Omega}\frac{\nu_{i} \boldsymbol x_i}{\left| \|\sum\nolimits_{j \in \Omega} \nu_{j} \boldsymbol x_j\|_\kappa \right|},
\vspace{-0.05in}
\end{equation}
completing the proof.
\end{proof}

\vspace{-0.1in}
\subsection{Bundle Convolution} 
The unified formalism for Bundle Convolution is given as follows,
\vspace{-0.05in}
\begin{align}
BC_{\boldsymbol p_t}(\{\boldsymbol p_i, \boldsymbol z_i\}_{i\in\Lambda})=\sum\limits_{i\in\Lambda}\left(\alpha_{it} \boldsymbol z_i-\frac{\kappa\alpha_{it}\langle\boldsymbol z_i, \boldsymbol p_t\rangle_\kappa}{1+\kappa\langle\boldsymbol p_i, \boldsymbol p_t\rangle_\kappa}(\boldsymbol p_i+\boldsymbol p_t)\right).
\label{eq:bconv2}
\vspace{-0.05in}
\end{align}
We leverage the equation above to aggregate the node encodings in the corresponding tangent spaces, which span the tangent bundle surrounding the manifold.
The key ingredient of the proposed convolution lies in the parallel transport, which solves the incompatibility issue among different tangent spaces.

The parallel transport w.r.t. the Levi-Civita connection $PT_{x\to y}$ transports a vector in $\boldsymbol v \in \mathcal T_x\mathcal L$ to another tangent space $\mathcal T_y\mathcal L$ with a linear isometry along the geodesic between $\boldsymbol x, \boldsymbol y \in \mathcal L$. 
Concretely, the unit speed geodesic from $\boldsymbol x$ to $\boldsymbol v$ is $\gamma_{\boldsymbol x, \boldsymbol v}(t)=\boldsymbol x\cos_\kappa(t) + \frac{1}{\sqrt{|\kappa|}}\sin_\kappa(t)\boldsymbol v$, for $t\in[0,1]$.
The generic form in $\mathcal L$ is given as
\vspace{-0.05in}
\begin{equation}
PT_{\boldsymbol p_i \to \boldsymbol p_t}(\boldsymbol z_i)=\boldsymbol z_i-\frac{\langle Log_{\boldsymbol p_i}^\kappa(\boldsymbol p_t), \boldsymbol z_i \rangle_{\boldsymbol x}}{d_{\mathcal L}(\boldsymbol p_i,\boldsymbol p_t)}\left(Log_{\boldsymbol p_i}^\kappa(\boldsymbol p_t)+Log_{\boldsymbol p_t}^\kappa(\boldsymbol p_i)\right),
\vspace{-0.03in}
\end{equation}
where $\langle \boldsymbol a, \boldsymbol b \rangle_{\boldsymbol x}=\boldsymbol a^\top\mathfrak{g}_{\boldsymbol x}\boldsymbol b$ is the inner product at the point $\boldsymbol x$, and $\mathfrak{g}_{\boldsymbol x}$

\noindent is  the Riemannian metric of $\mathcal L$ at $\boldsymbol x$.
Given the logarithmic map with curvature-aware cosine  as follows,
\begin{equation}
Log_{\boldsymbol p_i}^\kappa(\boldsymbol p_t)=\frac{\cos^{-1}_\kappa(\beta)}{\sqrt{\beta^2-1}}(\boldsymbol p_t-\beta\boldsymbol p_i), \quad \beta=\kappa\langle\boldsymbol p_i, \boldsymbol p_t\rangle_\kappa.
\end{equation}
The parallel transport in this case is derived as 
\begin{equation}
PT_{\boldsymbol p_i \to \boldsymbol p_t}(\boldsymbol z_i)=\boldsymbol z_i-\frac{\kappa \langle\boldsymbol z_i, \boldsymbol p_t\rangle_\kappa}{1+\kappa\langle\boldsymbol p_i, \boldsymbol p_t\rangle_\kappa}(\boldsymbol p_i+\boldsymbol p_t),  \quad \forall \boldsymbol p_i, \boldsymbol p_t \in \mathcal L,
\end{equation}
where the curvature-aware cosine is defined as $\cos_\kappa(\cdot)=\cos(\cdot)$ when $\kappa>0$, and $\cos_\kappa(\cdot)=\cosh(\cdot)$ with $\kappa>0$, and its superscript $-1$ denotes the inverse function.
Therefore, Eq. (\ref{eq:bconv2}) is given with aggregation over the set $\Lambda$.

\vspace{-0.05in}
\section{Algorithm}

We give the pseudocode of cross-geometry attention in Algo. 2. 


\vspace{-0.07in}
\section{Riemannian Geometry}
% A Riemannian manifold $(\mathcal M, \mathfrak g)$ is a smooth manifold $\mathcal M$ endowed with a Riemannian metric $\mathfrak g$. 
% Each point on the manifold is associated with a tangent space where the  metric $\mathfrak g$ is defined.
The curvature is a notion describing the extent of how a manifold derivatives from being ``flat''. 
It is typically viewed as a measure $R(X, Y)Z$ of the extent to which the operator 
$(X,Y) \to \nabla_X \nabla_YZ$ is symmetric, where $\nabla$ is a connection on $\mathcal M$ (where $X, Y, Z$ are vector fields, with $Z$ fixed).
Sectional curvature, defined on two independent vector unit in the tangent space, is often utilized.
The reason is the curvature operator $R$ can be recovered from the sectional curvature, when $\nabla$ is the canonical Levi-Civita connection induced by $\mathfrak g$.
A manifold is said to be a Constant Curvature Space (CCS) if the sectional curvature is constant scalar everywhere on the manifold.

Among Riemannian manifolds, there exist three types of CCSs: the negative curvature hyperbolic space, the positive curvature hyperspherical space, and the zero-curvature Euclidean space.
There are several model spaces of CCSs, e.g., Poincar\'{e} ball model, Poincar\'{e} half-plane, Klein model, Lorentz model, and Stereographical model, and they are equivalent to each other in essence\footnote{They are the same in structure and geometry but have different coordinate systems.}.
In this paper, we opt for the Lorentz/Spherical model\footnote{The Lorentz model of hyperbolic space corresponds to the Spherical model of hyperspherical space in account of the coordinate systems.}, and give a unified formalism,
\vspace{-0.1in}
\begin{equation}
\mathcal L_{\kappa }^{d}=\{ 
\left[\begin{array}{c}
x_t  \\
\boldsymbol x_s 
\end{array}\right] \in \mathbb R^{d+1} 
| \langle \boldsymbol x, \boldsymbol x \rangle_\kappa = \frac{1}{\kappa}, x_t >0, \boldsymbol x_s \in \mathbb R^d\},
\vspace{-0.02in}
\end{equation}
where  $d$ and $\kappa$ denote the dimension and curvature, respectively.
$x_t$ corresponds to the axis of symmetry of the hyperboloid and is termed the time-like dimension, while all other axes $\boldsymbol x_s$ are called space-like dimensions. 
In particular, $\mathcal L^d_\kappa$ becomes the Lorentz model of hyperbolic space under negative $\kappa$, and shifts to Spherical model of hyperspherical space when $\kappa>0$.
Note that, Euclidean space is not included in the formalism, and it requires $\kappa \neq 0$.
The induced hyperbolic space is a $d$-dimensional upper hyperboloid embedded $(d+1)$-dimensional Minkowski space, a.k.a. hyperboloid model. 
Similarly, the corresponding hyperspherical space is also expressed in a $(d+1)$-dimensional space.
All the mathematical construction in this paper is based on the Lorentz/Spherical model.
Accordingly, 
given a point in the manifold $\boldsymbol x \in \mathcal L_{\kappa }^{d}$, the exponential map projects a vector $\boldsymbol  v$ in the tangent space at $\boldsymbol  x$ to the manifold $Exp_{\boldsymbol x}(\boldsymbol v): \mathcal T_{\boldsymbol x}\mathcal L_{\kappa }^{d} \to \mathcal L_{\kappa }^{d}$, and the closed form expression is given as follows,
\vspace{-0.05in}
\begin{equation}
Exp_{\boldsymbol x}(\boldsymbol v) = \cos_{\kappa}(\sqrt{|\kappa|}\|\boldsymbol v\|_\kappa) \boldsymbol x + \sin_\kappa(\sqrt{|\kappa|}\|\boldsymbol v\|_\kappa) \frac{\boldsymbol v}{\sqrt{|\kappa|}\|\boldsymbol v\|_\kappa}. \\
\vspace{-0.03in}
\end{equation}
The logarithmic map $Log_{\boldsymbol x}(\boldsymbol y):  \mathcal L_{\kappa }^{d} \to \mathcal T_{\boldsymbol x}\mathcal L_{\kappa }^{d} $ projects a point $\boldsymbol  y$  in the manifold to the tangent space of $\boldsymbol  x$, serving as the inverse of the exponential map. 
It takes the form of 
\vspace{-0.05in}
\begin{equation}
Log_{\boldsymbol x}(\boldsymbol y) = \frac{\operatorname{cos}_\kappa^{-1}(-\kappa\langle \boldsymbol x, \boldsymbol y\rangle_\kappa)}{\sqrt{\kappa^2\langle \boldsymbol x, \boldsymbol y\rangle_\kappa^2 - 1}} \left(\boldsymbol y + \kappa\langle \boldsymbol x, \boldsymbol y\rangle_\kappa \boldsymbol x\right).
\vspace{-0.03in}
\end{equation}
%The parallel transport  maps between two tangent spaces, and its closed form expression is given in Appendix B.
% At each point $x \in M$, the tangent space $T_xM$ is a real vector space representing the set of all possible directions of curves passing through $x$. The Riemannian metric $g$ defines an inner product on each $T_xM$, thereby inducing the shape of the manifold. The collection of tangent spaces over the manifold is said to be tangent bundle, denoted as $T M$. 
% Both the exponential and logarithmic maps are defined locally around a reference point $x$, with their domains of definition dependent on the curvature and the injectivity radius of the manifold. The parallel transport carries the vector in one tangent space to another along the geodesic $PT_{x \to y} (v) $ : $ T_x M \to T_y M$. The parallel transport for a connection thus supplies a way of, in some sense, moving the local geometry of a manifold along a curve: that is, of connecting the geometries of nearby points.


% The geodesic is the shortest curve connecting two points in the manifold. The curvature $k_x$ is a local geometric property of a cure connecting two points in the manifold, measuring the extent how which a surface derives from being flat, and determines the shape of the manifold. Let ($M$, $g$) be a Riemannian manifold, and $TM$ be the space of all vector fields on $M$. We define the Riemannian curvature tensor as a map $TM \times TM \times TM \to TM$ by the following formula where $\nabla$ is the Levi-Civita connection:

% \vspace{-0.05in}
% \begin{equation}
% R(X,Y)Z = \nabla_X \nabla_Y Z - \nabla_Y \nabla_X Z - \nabla_{[X,Y]} Z
% \vspace{-0.03in}
% \end{equation}

% or equivalently

% \vspace{-0.05in}
% \begin{equation}
% R(X, Y) = [\nabla_X, \nabla_Y] - \nabla_{[X, Y]}
% \vspace{-0.03in}
% \end{equation}

% A manifold is referred to as a constant curvature space if and only if curvature $k_x$ is equal everywhere so that the closed-form metric is derived. There exists three types of constant curvature manifold:  hyperbolic space with negative curvature, hyperspherical space with positive curvature, and Euclidean space, which is indeed a special case of Riemannian manifold with zero curvature.

% The Lorentz model's foundation lies in the theory of special relativity, and consequently, it inherits terminology from this domain. Within the $(n+1)$-dimensional Minkowski space in which the hyperboloid model is embedded, the 0-th coordinate, denoted 

% It shifts to Spherical
% model of hyperspherical space when $\kappa$ > 0.

% Here, we give a unified formalism for representing hyperbolic space, a non-Euclidean geometry characterized by constant negative sectional curvature K. Several models facilitate computations within hyperbolic space, notably the PoincarÃ© ball model, the Klein model, and the Lorentz model. These models are isometrically equivalent; distance-preserving transformations exist between any two. For the foundation of our framework, we adopt the Lorentz model due to its advantageous properties for computations involving Minkowski space and its inherent connection to special relativity. An $n$-dimensional Lorentz model with negative constant curvature $\kappa$ ($\kappa \neq 0$) is a Riemannian manifold denoted by $\mathcal{L}_{k}^{d}$. Riemannian metric is given by $\mathfrak{g}_x= \operatorname{diag}(\operatorname{sgn}(k), 1, \ldots, 1)$. Each point in $\mathcal{L}_{k}^{d}$ can be represented as $\left[\begin{array}{c}
% x_t  \\
% \boldsymbol x_s 
% \end{array}\right]$ where $x \in R^{n+1}$, $x_t \in R$ and $x_s \in R^n$. The set of points, $\mathcal{L}_{k}^{d}$, that consitute the manifold are defined as 
% $\mathcal L_{\kappa }^{d}=\{ \langle \boldsymbol x, \boldsymbol x \rangle_\kappa = \frac{1}{\kappa}, x_t >0, \boldsymbol x_s \in \mathbb R^d\}$.
% The smooth manifold couple with the curvature aware inner product as follows,
% \vspace{-0.05in}
% \begin{equation}
%  \langle \boldsymbol x, \boldsymbol y \rangle_\kappa:=\operatorname{sgn}(\kappa) x_{t} y_{t}+\boldsymbol x_{s}^{\top}\boldsymbol y_{s}, \quad  \boldsymbol x, \boldsymbol y \in \mathcal L_{\kappa }^{d},
%  \vspace{-0.03in}
%  \label{manifold}
% \end{equation}



%\vspace{-0.05in}
%\begin{equation}
%P_{x \rightarrow y}^{\kappa}(v) = -%\frac{\lambda_{x}^{\kappa}}{\lambda_{y}^{\kappa}} %\left( y \oplus_{\kappa} -x \right) \oplus_{\kappa} %\left( y \oplus_{\kappa} \left( -x \oplus_{\kappa} v %\right) \right)
%\vspace{-0.03in}
%\end{equation}


\begin{algorithm}[t]
    \caption{Cross-geometry Attention in Hyperbolic Space}
    \label{alg. cross-att}
        \KwIn{A substructure, Node coordinates $\boldsymbol p^H$ and $\boldsymbol p^S$, Linear operation $f_{\boldsymbol W}$, A parameterized scalar map $\phi: \mathcal{L}\times\mathcal{L}\rightarrow \mathbb{R}$.}
        \KwOut{The updated node coordinates $\boldsymbol p^H$.}
           Compute the key, query and value via $\boldsymbol k_i=f_{\boldsymbol V}(\boldsymbol p^H_i)$, $\boldsymbol q_i=f_{\boldsymbol Q}(\boldsymbol p^S_i)$ and $\boldsymbol v_i=f_{\boldsymbol V}(\boldsymbol p^H_i)$, respectively;\\
           Compute the score of $\phi([\boldsymbol q_i, \boldsymbol k_j])$ for $i$, $j$ in the substructure;\\
            Derive attentional weight by the softmax of scores over the substructure $\alpha_{ij}=\frac{\exp(\phi([\boldsymbol q_i || \boldsymbol k_j]))}{\sum_{(i,t) \in \Omega}\exp(\phi([\boldsymbol q_i || \boldsymbol k_t]))}$;\\
           Update node coordinate by the weighted geometric midpoint $\boldsymbol v_i=mid_\kappa(\{\boldsymbol v_j, \alpha_{ij}\}_{(i,j) \in \Omega})$;
\end{algorithm}

\vspace{-0.1in}
\section{Experiment Details}

\subsection{Datasets}
We give the statistics in Table \ref{tab:datasets}, and introduce the datasets below.
\begin{itemize}
    \item  \textbf{Citeseer} \cite{citeseerandpubmed} consists of scientific publications in six classes. Nodes and edges denote publications and citation relationship, respectively. Each publication is described as a binary word vector from the dictionary of 3703 unique words.
    \item \textbf{Pubmed} \cite{citeseerandpubmed} is citation network among scientific publications in three classes. Each publication  is described by a TF/IDF weighted word vector from a dictionary  of 500 unique words.
    \item \textbf{GitHub} \cite{github} is a social network where nodes are developers who have starred at least 10 repositories, and edges denote mutual follower relationships. Node features are location, starred repositories, employer, and e-mail address.
    \item \textbf{Airports} \cite{kdd17struc2vec} is a  commercial air transportation network within the United States. The node corresponds to a distinct airport facility, and are stratified into four discrete classes. The edges indicate the existence of commercial flight routes.
    \item  \textbf{ogbn-arxiv} \cite{nips2020arxiv} is the citation network among Computer Science (CS) arXiv papers. Each paper is given as a 128-dimensional feature vector by averaging the embeddings of words in its title and abstract. 
    \item \textbf{Physics} \cite{physics_computers} is co-authorship graphs based on the Microsoft Academic Graph from the KDD Cup 2016 challenge. Nodes and edges denote authors and co-authored relationship, respectively.
    \item \textbf{AComp} (Amazon Computers dataset) \cite{physics_computers} is segments of the Amazon co-purchase graph. Nodes denote goods and edges indicate that two goods are frequently bought.
\end{itemize}


\begin{table}[t]
    \centering
    \caption{Summary of Datasets}
    \vspace{-0.05in}
    \label{tab:datasets}
    \begin{tabular}{l ccc}
    \toprule
    Dataset  & \#(Nodes) & \#(Edges) & 
    Feature Dim. \\
    \midrule
    Cora &   2,708 & 5,429 & 1,433 \\
    Pubmed & 19,717 & 44,338 & 500\\
    GitHub & 37,700 & 578,006& 0 \\
    Airports& 1,190 & 13,599 & 0\\
    ogbn-arxiv& 169,343   & 1,166,243    & 128  \\
    Physics   &  34,493  & 495,924  &  8,415 \\
    AComp& 13,752    & 491,722    & 767 \\
    \bottomrule
    \end{tabular}
     \vspace{-0.1in}
    \end{table}

\vspace{-0.15in}
\subsection{Baselines}
\begin{itemize}
\item \textbf{GCN} \cite{iclr17gcn}  resorts  neighborhood aggregation in spectral domain.
\item \textbf{DGI} \cite{iclr19dgi} introduces a self-supervised paradigm by maximizing the mutual information between the local node view and the global graph view.
\item  \textbf{GraphMAE2} \cite{www23graphmae2} conducts self-supervised learning in the reconstruction of masked node features with masked autoencoders.
\item  \textbf{OFA} \cite{iclr24ofa} describes all nodes and edges with natural language to feed into  LLMs, and  subsequently utilizes  graph prompting  that appends prompting substructures to the input graph.
\item  \textbf{LLaGA} \cite{icml24llaga} re-organizes graph nodes to  sequences and then maps the sequences into the token embedding space via a versatile projector in order to leverage the LLM for graph analysis.
\item  \textbf{OpenGraph} \cite{xia2024opengraph} is  trained on diverse datasets with a unified graph tokenizer, scalable graph transformer, and LLM-enhanced data augmentation, so as to  comprehend the nuances of diverse graphs.
\item  \textbf{GCOPE} \cite{kdd24gcope} is a graph pre-training framework designed to enhance the efficacy of downstream tasks by harnessing collective insights from multiple source domains. 
\item \textbf{GraphAny} \cite{zhao2024graphany} models the inference on a new graph as an analytical solution to a GNN with designs invariant to feature and label permutations and robust to dimension changes.
\end{itemize}


% \paragraph{\textbf{Few-shot settings.}}
% Few-shot learning is a machine learning framework that enables a pre-trained model to generalize to new categories of data that it has not seen during training, using only a few labeled samples for each class.
% This approach, which falls under the meta-learning paradigm (or learning to learn), enables an AI model to make accurate predictions even when trained on a limited amount of labeled data. 
% It is particularly useful in classification tasks where sufficient training data is scarce. 
% In our experiment settings, taking inspiration from previous works \cite{xia2024opengraph}, we retain up to $k$ training instances for labeled classes. This is crucial as it allows the model to learn from a limited number of examples, which is a common scenario in real-world applications where labeled data is scarce. For example, as we pre-train our model on ogbn-Arxiv \cite{nips2020arxiv}, Amazon Computers \cite{physics_computers}, and Coauthor Physics \cite{physics_computers} datasets, then we fetch $k$ samples per class on Citeseer \cite{citeseerandpubmed} and train the classification head. To create the label space for the four datasets, we first load the pre-trained Word2Vec \cite{iclr13word2vec} model like \emph{glove-wiki-gigaword-100}, \footnote{https://radimrehurek.com/gensim/downloader.html} then we map the raw text class names into dense label space. For the classification head, we transform the model output into label space and use cosine similarity to compute prediction scores.
\vspace{-0.11in}
\subsection{Reproducibility \& Implementation Notes}

\vspace{-0.01in}
\subsubsection{\textbf{On Few-shot Learning}}
Few-shot learning performance is significant to evaluate a pre-trained model. In particular, a pre-trained model is examined by classifying new data, which has not been seen during training, with only a few labeled samples for each class.
In our experiment, following the setting of \citet{xia2024opengraph}, we retain up to $k$ training instances for labeled classes.  
For example, we first pre-train our model on ogbn-Arxiv \cite{nips2020arxiv}, Amazon Computers \cite{physics_computers}, and Coauthor Physics \cite{physics_computers} datasets, and then fetch $k$ samples per class on Citeseer \cite{citeseerandpubmed} to train the classification head, so as to infer the classification results.

% Conversely, in link prediction, the k-shot setting translates to maintaining at most k links for each node within the training set. This approach emphasizes the need to make inferences about relationships between nodes with minimal training data, reflecting the challenges faced in dynamic or sparse networks
% For \textbf{zero-shot learning}, we perform tasks without having seen graph structures, node features and node labels during training. Refer to previous works [opengraph,opencitatin], the strategy we used removes class-related parameters. We consider the label classes as separate nodes in our graph and link the vanilla nodes to these class nodes based on their training labels.
% In node classification, the k-shot setting means keeping up to k training instances for each label class, while in link prediction, it involves maintaining at most k links for each node in the training set.
% Non-pre-training methods like GNNs are trained merely on the limited few-shot training data, while other baseline models that use a pre-training-and-tuning paradigm first undergo pre-training before being fine-tuned on the few-shot set.
% In pre-training period, we use the pre-rained model they provide initially. During the subsequent tuning phase, any parameters that cannot be directly transferred between datasets are re-learned. 
% \paragraph{\textbf{Optimizations.}}
% Our model is implemented using Pytorch.The optimization process ..., We use ... loss with ... The learnable parameters of ... are initialized using ...

\vspace{-0.05in}
\subsubsection{\textbf{Initialization and Configurations}}
% The default curvatures of hyperbolic space and hyper-spherical space are $-1$ and $1$. The mini-batch sample method is the same as GraphSAGE \cite{nips17GraphSAGE}. To generate trees by the seed nodes, we apply the BFS algorithm starting at each seed node, the tree edge direction is from children nodes to parent nodes. Then we batch the trees together into one graph for parallel computation. To get initial points on manifolds, there are two ways before using the exponential map.: one is to concatenate $0$ at the first position of the vector; the other one is to project the vector on the tangent space at the original points. For the fine-tuning task heads, we concatenate the raw attributes of the graph with the output of \texttt{RiemannGFM}, then use a simple head like GCN \cite{iclr17gcn} layer to get task-specified outputs.

For model initialization, we first compute the normalized graph Laplacian $\boldsymbol{L}=\boldsymbol{I} - \boldsymbol{D}^{-1/2}\boldsymbol{A}\boldsymbol{D}^{-1/2}$ of the given graph, where $\boldsymbol{A}$ is the adjacency matrix and $\boldsymbol{D}$ is the degree matrix.
Second, we conduct eigenvalue decomposition on $\boldsymbol{L}$  and utilize the largest $K$ eignvectors as node encodings, where $K$ is a predefined number.
Note that, the initialization process indeed normalizes different graphs with the $K$-dimensional encoding $\boldsymbol z$.
Subsequently, we induce  node coordinates via $\boldsymbol p= Exp_{\boldsymbol o}([0 || \boldsymbol z^\top]^\top)$ so that the coordinates are placed on the manifold $\boldsymbol p \in \mathcal L$, where the reference point is the north pole $\boldsymbol o$.
\texttt{RiemannGFM} allows for mini-batch training, and the mini-batch sampling strategy  is the same as that of  SAGE \cite{nips17GraphSAGE}.

\vspace{-0.05in}
\subsubsection{\textbf{Hyperparameters}}

The hyperparameters are  tuned with grid search. 
In particular, we set the dropout rate as $0.1$ to enhance the model robustness, and set learning rate of the pre-training as $0.01$ to balance convergence speed and stability. 
The  dimension of each factor in the product bundle is set as $32$, that is,  we instantiate \texttt{RiemannGFM} on  $\left(\mathcal H^{32}_{-1} \otimes \mathcal T\mathcal H^{32}_{-1}\right) \otimes \left(\mathcal S^{32}_{1} \otimes \mathcal T\mathcal S^{32}_{1}\right)$.
\texttt{RiemannGFM} is implemented with $2$ Riemannian layers.
The parameterized scalar map in cross-geometry attention is a multi-layer perceptions with one hidden layer, whose dimension is set as $256$.
The model is built on PyTorch, and the optimizer is Adam.
%further details are provided in the anonymous link of \url{https://anonymous.4open.science/r/Geo-GFM-1603}.

% In our experiment, we carefully tuned the model's hyperparameters to ensure optimal performance. Specifically, we configured the model with $2$ layers and included bias terms. To enhance the model's robustness, we implemented dropout with a rate of $0.1$. The embedding dimension of each component was set to $32$, while the hidden layer dimension was chosen to be $256$.
% During the pre-training phase, the learning rate was set to $0.01$ to balance convergence speed and stability. 





\begin{table}[t]
\centering % Center the table within the half-column width
\caption{Geometric ablation on Citeseer, Pubmed, and Airport datasets. Node classification results are reported in terms of AUC (\%). The results are given in the form of mean$\pm$std. $\mathcal R^{32}_{0}$ denotes the Euclidean space.}
         \vspace{-0.03in}
\label{tab:geo-ablation-2}
\begin{tabular}{cc|c c c}
\hline
\textbf{Trees} & \textbf{Cycles} & \textbf{Citeseer} & \textbf{Pubmed} & \textbf{Airport} \\
\hline
$\mathcal H^{32}_{-1}$&$\mathcal S^{32}_{1}$
& \textbf{66.38 $\pm$ 0.31}
& \textbf{76.20 $\pm$ 0.79}
& \textbf{55.29 $\pm$ 2.26} \\
$\mathcal H^{32}_{-1}$&$\mathcal R^{32}_{0}$
& 66.26 $\pm$ 1.45 & 73.10 $\pm$ 6.36 & 50.42 $\pm$ 1.48 \\
$\mathcal H^{32}_{-1}$&$\mathcal H^{32}_{-1}$
& 63.37 $\pm$ 1.69 & 72.26 $\pm$ 2.12 & 52.66 $\pm$ 1.46\\
\hline
$\mathcal H^{32}_{-1}$&$\mathcal S^{32}_{1}$
& \textbf{66.38 $\pm$ 0.31}
& \textbf{76.20 $\pm$ 0.79}
& \textbf{55.29 $\pm$ 2.26}\\
$\mathcal R^{32}_{0}$ &$\mathcal S^{32}_{1}$
& 65.52 $\pm$ 1.46 & 71.12 $\pm$ 8.73 & 50.17 $\pm$ 1.26 \\
$\mathcal S^{32}_{1}$ &$\mathcal S^{32}_{1}$
& 64.26 $\pm$ 1.09 & 71.46 $\pm$ 0.72 & 53.72 $\pm$ 0.46 \\
\hline
\end{tabular}
         \vspace{-0.05in}
\end{table}

    \begin{table}[t]
        \caption{Cross-domain node classification prformance on different pre-training datasets.}
        \vspace{-0.03in}
        \label{tab:diff-train-2}
        \centering
        \begin{tabular}{c  c | cc  c  }
        \hline
        & & \multicolumn{3}{c}{\textbf{Testing Datasets}}\\
        \textbf{Pre-training} & \textbf{Method}  & \textbf{Citeseer} & \textbf{Pubmed}  & \textbf{Airport}  \\
        \hline
        \multirow{3}{*}{Flickr}
        & OpenGraph 
        & 63.16\scriptsize{$\pm$4.45} 
        & 60.35\scriptsize{$\pm$5.53} 
        & 43.32\scriptsize{$\pm$2.23} \\
        & GCOPE &64.47\scriptsize{$\pm$2.87} &72.48\scriptsize{$\pm$0.97} & 36.74\scriptsize{$\pm$2.38} \\
        & \textbf{\texttt{RiemannGFM}}  &  \textbf{65.20\scriptsize{$\pm$1.73}} &\textbf{74.04\scriptsize{$\pm$0.53} }& \textbf{46.13\scriptsize{$\pm$2.78} }\\
        \hline
        \multirow{3}{*}{AComp}
        & OpenGraph 
        & 60.24\scriptsize{$\pm$1.25} 
        & 64.45\scriptsize{$\pm$1.24} 
        & 45.02\scriptsize{$\pm$4.25} \\
        & GCOPE &   63.79\scriptsize{$\pm$0.88} &72.80\scriptsize{$\pm$2.14} & 44.19\scriptsize{$\pm$1.53} \\
          & \textbf{\texttt{RiemannGFM}}  &  \textbf{64.80\scriptsize{$\pm$1.96}} &\textbf{77.00\scriptsize{$\pm$0.42}} & \textbf{49.41\scriptsize{$\pm$1.77} }\\
        \hline
        \multirow{3}{*}{WikiCS}
        & OpenGraph 
        & \textbf{67.54\scriptsize{$\pm$2.24}} 
        & 74.98\scriptsize{$\pm$3.25} 
        & 48.92\scriptsize{$\pm$1.22} \\
        & GCOPE &   65.47\scriptsize{$\pm$2.87} &75.38\scriptsize{$\pm$0.83} & 46.05\scriptsize{$\pm$2.51} \\
        & \textbf{\texttt{RiemannGFM}}  &  66.56\scriptsize{$\pm$1.15}&\textbf{75.78\scriptsize{$\pm$1.36}} & \textbf{51.25\scriptsize{$\pm$1.76}} \\
        \hline
        \end{tabular}
                \vspace{-0.05in}
        \end{table}


\section{Additional Results}
We show the additional results of the geometric ablation in Table \ref{tab:geo-ablation-2} and the impact of pre-training datasets in Table  \ref{tab:diff-train-2}.
The geometric ablation in node classification exhibits the similar pattern to that in link prediction, showing the alignment between trees and hyperbolic space  (and between cycles and hyperspherical space).
As shown in in Table \ref{tab:diff-train-2}, the stable performance of our model demonstrates the superiority of exploring GFM with structural vocabulary (i.e., the common substructures of trees and cycles underlying the graph domain).



