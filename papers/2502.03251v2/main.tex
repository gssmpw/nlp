%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf]{acmart}
\let\Bbbk\relax
\usepackage{makecell}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{caption}
% \usepackage{subfigure}
\usepackage{subcaption}
%\usepackage{pifont}
\usepackage{color} 
\usepackage{colortbl}
\usepackage{multirow}

% \usepackage{algorithm}
% \usepackage{algorithmic}

\usepackage{lipsum}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{lipsum} 


\usepackage[framemethod=tikz]{mdframed}
\definecolor{mygray}{RGB}{243,243,244}
\newmdenv[
  innertopmargin=0pt,
  backgroundcolor=mygray,
  linecolor=none,
  innerleftmargin=0pt,
  innerrightmargin=0pt,
  leftmargin=0pt
  ]{mymath}

\usepackage[ruled, linesnumbered,vlined]{algorithm2e}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.

% \setcopyright{acmlicensed}
% \copyrightyear{2018}
% \acmYear{2018}
%\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[ACM  TheWebConf'25]{ACM The Web Conference}{April 28--May 02,
%   2025}{Sydney, Australia}
% %%
% %%  Uncomment \acmBooktitle if the title of the proceedings is different
% %%  from ``Proceedings of ...''!
% %%
% %%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
% %%  June 03--05, 2018, Woodstock, NY}
% \acmISBN{978-1-4503-XXXX-X/18/06}


\copyrightyear{2025} 
\acmYear{2025} 
\setcopyright{acmlicensed}\acmConference[WWW '25]{Proceedings of the ACM Web Conference 2025}{April 28-May 2, 2025}{Sydney, NSW, Australia}
\acmBooktitle{Proceedings of the ACM Web Conference 2025 (WWW '25), April 28-May 2, 2025, Sydney, NSW, Australia}
\acmDOI{10.1145/3696410.3714952}
\acmISBN{979-8-4007-1274-6/25/04}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
% \title{Language-like or Structural: Learning a Graph Foundation Model from Structural Geometry}
\title{RiemannGFM: Learning a Graph Foundation Model from Riemannian Geometry}
%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.


\author{Li Sun}
\authornote{Corresponding Author: Li Sun, ccesunli@ncepu.edu.cn.}
\email{ccesunli@ncepu.edu.cn}
\affiliation{%
  \institution{North China Electric Power University}
  \city{Beijing}
  \country{China}
  \postcode{102206}
}

\author{Zhenhao Huang}
\email{huangzhenhao@ncepu.edu.cn}
\affiliation{%
  \institution{North China Electric Power University}
  \city{Beijing}
  \country{China}
  \postcode{102206}
}

\author{Suyang Zhou}
\email{zhousuyang@ncepu.edu.cn}
\affiliation{%
  \institution{North China Electric Power University}
  \city{Beijing}
  \country{China}
  \postcode{102206}
}


\author{Qiqi Wan}
\email{wanqiqi@ncepu.edu.cn}
\affiliation{%
  \institution{North China Electric Power University}
  \city{Beijing}
  \country{China}
  \postcode{102206}
}

\author{Hao Peng}
\email{penghao@buaa.edu.cn}
\affiliation{%
  \institution{Beihang University}
    \city{Beijing}
  \country{China}
}


\author{Philip  Yu}
\email{psyu@uic.edu}
\affiliation{%
  \institution{University of Illinois, Chicago}
  \country{USA}
}

% \author{Ben Trovato}
% \authornote{Both authors contributed equally to this research.}
% \email{trovato@corporation.com}
% \orcid{1234-5678-9012}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
% \affiliation{%
%   \institution{Institute for Clarity in Documentation}
%   \city{Dublin}
%   \state{Ohio}
%   \country{USA}
% }


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Li Sun et. al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
% and has achieved tremendous success in natural language processing, e.g., GPT-4, as well as computer vision. 

% but the foundation model encounters significant challenges in the graph domain due to its non-Euclidean nature.
% So far, existing graph foundation models are built upon the Large Language Model.
% % , which make efforts to align graph structures with the language and to seek advanced prompting techniques.
% % Consequently, the universality and transferability heavily 
% They either rely on the lingual vocabulary over textual features, 
% limiting the usage to a wide range of graphs other than the text-attributed ones, 
% or make efforts to align graph structures with the language and to seek advanced prompting techniques.
% Though this line of studies receive encouraging performance, 

%pretraining a single, universal model generalizable to different downstream tasks.
The foundation model has heralded a new era in artificial intelligence, 
pretraining a single model to offer cross-domain transferability on different datasets.
% Graphs are 
% , ranging from recommender systems to biochemical structures.
Graph neural networks excel at learning graph data, the omnipresent non-Euclidean structure, but often lack the generalization capacity.
Hence, \textbf{graph foundation model} is drawing increasing attention, and 
recent efforts have been made to leverage Large Language Models.
%, encouraged by the remarkable success of GPT-4.
On the one hand, existing studies primarily focus on text-attributed graphs, while a wider range of real graphs do not contain fruitful textual attributes.
On the other hand,  the sequential graph description tailored for the Large Language Model neglects the structural complexity, which is a predominant characteristic of the graph.
Such limitations motivate an important question: \textbf{Can we go beyond Large Language Models, and pretrain a universal model to learn the structural knowledge for any graph?}
The answer in the language or vision domain is a shared vocabulary.
We observe the fact that there also exist shared substructures underlying graph domain, 
and thereby open a new opportunity of graph foundation model with structural vocabulary.
% (by which any graph can be constructed).
The key innovation is the discovery of a simple yet effective structural vocabulary of trees and cycles, and we explore its inherent connection to Riemannian geometry.
Herein,  we present a universal pretraining model, \texttt{RiemannGFM}.
%, with geometric contrastive learning.
Concretely, we first construct a novel product bundle to incorporate the diverse geometries of the vocabulary. 
Then, on this constructed space, we stack Riemannian layers where the structural vocabulary, regardless of specific graph, is learned in Riemannian manifold offering  cross-domain transferability.
%and node encoding is generated in the tangent space for arbitrary input graph.
Extensive experiments show the effectiveness of \texttt{RiemannGFM} on a diversity of real graphs.
% in a novel augmented product manifold where each node has the coordinate in manifold factor, and is attached to node encoding in tangent bundle factor.
% We design the vocabulary learning module with cross-geometric attention to study how to place a tree (cycle) in , and global learning module to learn the structural knowledge for any graph.
% In other words, existing solutions neglect the fruitful information on the graph structure itself, which is the predominant character on graphs.
% A natural question arises that 
% \textbf{can we go beyond the Large Language Model, and pretrain a universal model to learn the structural knowledge for any graph?}
% In this paper, we approach this problem  by learning a graph foundation model from structural geometry.
% %, grounded by the solid theory of .
% Thus, we construct the graph foundation model on the product of Rimannian mainfolds
% (named \textbf{\texttt{RiemannGFM}})\footnote{Codes are available at ......}.
% Specifically, ..... grounded  on the theory of Riemannian harmonic analysis. 
\vspace{-0.1in}
\end{abstract}

%\footnote{Codes are available at ......} 



%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010257.10010293.10010294</concept_id>
<concept_desc>Computing methodologies~Neural networks</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010257.10010258.10010259</concept_id>
<concept_desc>Computing methodologies~Supervised learning</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10002951.10003227.10003351</concept_id>
<concept_desc>Information systems~Data mining</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Neural networks}
\ccsdesc[300]{Computing methodologies~Supervised learning}
\ccsdesc[300]{Information systems~Data mining}


%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Graph Foundation Model, Riemannian Geometry, Pretraining Model.}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.


% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\input{body}

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.


%%

\newpage
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{www25}


%%
%% If your work has an appendix, this is the place to put it.
\appendix

\input{appendix}

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
