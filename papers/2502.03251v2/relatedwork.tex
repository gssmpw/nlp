\section{Related Work}
\vspace{-0.03in}
\subsubsection*{\textbf{Graph Neural Network \& Self-supervised Graph Learning.}}
Popular GNNs include graph convolutional nets and graph transformers.
The former conducts neighborhood aggregation with layer-wise message passing \cite{icml19sgc,nips16cnn,nips17GraphSAGE}, while the latter leverages a transformer-like encoder \cite{arxiv22Gransformer}.
Both of them are typically paired with a classification head or reconstructive loss on a specified graph.
Thus, a major shortcoming of traditional graph models is their limited generalization capability.
Self-supervised learning has been integrated into GNNs in recent years \cite{www23graphmae2,iclr19dgi}.
Instead of coupling GNNs with downstream tasks, self-supervised learning conducts parameter training from the graph data itself via specialized pretext tasks. 
However, graph augmentation for self-supervised learning is nontrivial \cite{www2GraphAugmentation,arxiv24GraphContrastiveSurvey}, and the parameters trained on one graph cannot be directly applied to another owing to the difference in attribute distribution.
In other words, existing graph models lack the universality, and are still far from being a foundation model.

  \vspace{-0.05in}
\subsubsection*{\textbf{Graph Foundation Model.}}
% Large language models demonstrate the remarkable success of pre-training a foundation model for multiple downstream tasks \cite{2024gpt4}.
% Recently, efforts have been made to replicate the success to the graph domain.
% \citet{kdd23all-in-one,kdd24gcope} design graph prompting to unify the downstream tasks, corresponding to the prompt of LLM.
% Up to date, the majority of GFMs are designed for text-attributed graphs assisted by the LLM.
% Specifically, LLM-based GFMs are fed with the graph's textual descriptions, e.g., describing nodes/edges throughout the graph \cite{icml24llaga,iclr24ofa}, translating graphs into node sequences \cite{icml24llaga}, and re-framing textual attributes into language \cite{iclr24ofa}.

% Pioneering work \cite{kdd23all-in-one} designs the graph prompting to , the analogy to language prompt.
% GCOPE \cite{kdd24gcope} further conducts model training on multi-domain graphs with coordinators in order to improve the generalization capacity to different datasets.
% Recent efforts have been made to integrate GNN into LLM. 
% For example, LLaGA \cite{icml24llaga} develops a graph translation technique that reshapes a graph into node sequences,
% while OFA \cite{iclr24ofa}  unifies different graph data by describing nodes and edges with natural language.

Recent efforts are generally categorized into two groups.
The first group enhances the vanilla GNNs to achieve better generalization capacity, e.g., unifying the downstream tasks with graph prompt \cite{kdd23all-in-one}, and training on multi-domain graphs with coordinators \cite{kdd24gcope}.
\citet{zhao2024graphany} generalize SGC \cite{icml19sgc} for node classification on any graph.
The second group adapts LLM for analyzing graphs.
LLaGA \cite{icml24llaga} tailors graphs for the language model with node sequences, generated via graph translation,
while OFA \cite{iclr24ofa}  unifies different graph data by the language description of nodes and edges.
OpenGraph \cite{xia2024opengraph} re-frames textual attributes into language with a hierarchy.
Very recently, \citet{xia2024anygraph} propose a mixture of graph experts.
Existing models  typically struggle to maintain the performance on graphs without textual attributes.
Also, they model graphs in Euclidean space, and tend to trivialize the structural complexity.
Distinguishing from the prior studies, we consider graphs in Riemannian geometry, and design the first GFM exploring graph substructures (structural vocabulary).
%, to the best of our knowledge.

% Today's graph foundation models can generally be categorized into two types. 
% The first type is based on traditional Graph Neural Networks (GNNs) or transformer models. [] proposed a task-specific foundation model for graphs across diverse domains. Additionally, [] introduced a graph prompting method, which imitates the prompting style of large language models to facilitate cross-domain graph pre-training.
% The second type leverages large language models (LLMs) to represent graphs through textual descriptions, such as describing nodes and edges separately [] and converting graphs into node sequences []. 

\begin{figure}[t]
    \centering
    \begin{subfigure}{0.31\linewidth}
        \centering
        \includegraphics[width=\linewidth]{tOriginal.pdf}
        \caption{Original}
        \label{fig:Original}
    \end{subfigure}
    \begin{subfigure}{0.31\linewidth}
        \centering
        \includegraphics[width=\linewidth]{tGCN.pdf}
        \caption{GCN}
        \label{fig:GCN}
    \end{subfigure}
    \begin{subfigure}{0.31\linewidth}
        \centering
        \includegraphics[width=\linewidth]{tRiemannGFM.pdf}
        \caption{RiemannGFM}
        \label{fig:RiemannGFM}
    \end{subfigure}
        \vspace{-0.1in}  
    \caption{Visualization on Cora}
    \label{fig:tSNE}
    \vspace{-0.15in}
\end{figure}

  \vspace{-0.05in}
\subsubsection*{\textbf{Riemannian Manifold \& Graphs.}}
Riemannian manifolds emerge as exciting alternatives for learning graphs \cite{nips24sun,aaai24sun,icdm23sun}.
Concretely, hyperbolic space is well recognized for its alignment with tree-like (hierarchical) structures,
and hyperbolic GNNs show superior results to Euclidean counterparts \cite{nips18hnn,nips19hgcn}. 
The geometric analogy of cycles is hyperspherical space, whose advantage of embedding cyclical structures is reported \cite{22cikmSpherical,nips19sphericalText}. 
\citet{icml20Constant} formulate a graph convolutional net in constant curvature spaces.
Recent advances report the success of Riemannian manifolds in modeling dynamics  \cite{aaai21sun,cikm22sun,aaai23sun,aaai25sun,www23ye} and clustering \cite{icml24sun,www24sun,ijcai23sun}.
We notice that the product manifold has been introduced to study graphs recently, 
and advanced techniques are proposed for node embedding \cite{iclr19mixCurvature,iclr23lantentGraphProduct,aaai22selfMG,cikm24wang}.
However, all of them lack the generalization capability to unseen graph structures, and consider node embeddings on the manifold, while we introduce the notion of tangent bundle to GFM.
%the input graph as a whole and thus lack the generalization capability to other graphs of different sizes.
So far, the potential of Riemannian geometry has not yet been released on GFM, and we are dedicated to bridging this gap.

  \vspace{-0.1in}