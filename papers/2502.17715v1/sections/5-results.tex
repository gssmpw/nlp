To thoroughly assess the quality of the generated follow-up questions, we employ both automatic evaluation (\S\ref{sec:results:auto}) and human evaluation (\S\ref{sec:results:human}). As a first step for both evaluations, we automatically identify and remove ungrammatical questions based on syntactic parsing (see Appendix~\ref{appendix:ungrammatical_questions} for a complete description of the filtering process). Table~\ref{tab:filtered} shows the percent of ungrammatical questions that were filtered out for each model. \textit{AUG} (3.58\%) and \textit{FULL} (6.31\%) produce far fewer ungrammatical FQs compared to \textit{ORG}, demonstrating their ability to generate more well-formed outputs. We focus the rest of our evaluation on the grammatical questions retained after the filtering.

\input{figures/filtered}

% To address this, we implemented a multi-step validation framework that filters out errors in syntax, semantics, and wording. Using \texttt{spaCy}'s dependency parser, we check for proper WH-question indicators and auxiliary verbs. We also verify the presence of question marks and remove any artifacts or non-informative placeholders. Additionally, we use consecutive n-gram analysis to detect and eliminate excessive duplication between the original question-answer pair and the generated follow-up.

\subsection{Automatic Evaluation}
\label{sec:results:auto}
\input{figures/auto_eval}

% \textbf{\textit{Vered:I may add it back later but I think it's already implied -- please confirm}}
% \textit{These results highlight the limitations of automatic metrics for evaluating follow-up question generation. Unlike answer-aware QG, where factoid questions closely match reference answers, follow-up questions are open-ended, allowing for multiple valid generations. This lexical diversity lowers traditional n-gram-based scores, suggesting that future evaluations should incorporate human assessments or embedding-based similarity measures to better reflect the quality of generated questions.}

\paragraph{Diversity.} We measure the diversity of the set of follow-up questions generated for a particular (IQ, IA) input, averaged across the dataset. We report Distinct-n \cite{li-etal-2016-diversity}, which measures the average distinct n-gram across all groups of follow-up questions for a given (IQ, IA) pair. 
The results in Table~\ref{tab:auto_eval} show that \textit{AUG} and \textit{FULL} perform similarly, and outperform \textit{ORG} in generating more diverse unigrams and bigrams. 
% \textit{AUG} (77.36\%) and \textit{FULL} (77.09\%) performed similarly for unigram diversity and were able to generate more unique words in their follow-up questions compared to \textit{ORG} 66.06\%. For bigram diversity, \textit{FULL} (94.85\%) and \textit{AUG} (94.41\%) again outperformed \textit{ORG} (91.12\%), showing that \textit{AUG} and \textit{FULL} not only generate a wider variety of individual words but also exhibit greater phrase-level diversity in their responses. 

We also apply agglomerative clustering to the sentence embeddings of the FQs (generated with \texttt{all-mpnet-base-v2}) and report the number of clusters formed at a distance threshold of 1.0. We normalize it by the number of generated follow-up questions to quantify the diversity within that set.\footnote{The highest score (most diverse) is 1, and the lowest score (least diverse) is achieved for a large set of questions clustered together.} Again, Table~\ref{tab:auto_eval} shows that adding the augmented data substantially improves the diversity of the generated questions. 

% \label{sec:length_analysis}
\paragraph{Average question length.} We report the average question length in terms of the number of tokens. We hypothesize that shorter questions are generally more readable. Table~\ref{tab:auto_eval} summarizes key statistics: the average length, shortest and longest follow-ups, and standard deviation. The \textit{ORG} model shows the highest variation in question length (SD = 10.13) compared to \textit{FULL} (3.77) and \textit{AUG} (2.98). Notably, its longest follow-up (111 words) far exceeds those from \textit{FULL} (73) and \textit{AUG} (24). In contrast, \textit{AUG} maintains the most consistent length distribution, with a lower standard deviation and a maximum length of 24 words. 

Examining the generated follow-up questions, we find that \textit{AUG} and \textit{FULL} generally produce concise, well-formed queries, while \textit{ORG} sometimes generates unclear or uninformative short questions (e.g., ``So it's cultural?''). Meanwhile, the longer questions from \textit{ORG} and \textit{FULL} often include extraneous conversational elements, deviating from standard follow-up patterns. Overall, \textit{AUG} maintains structured, concise outputs, whereas \textit{FULL} and \textit{ORG} exhibit greater variability, occasionally producing overly long or conversational responses. More examples are provided in Section~\ref{sec:analysis:casestudy} and Appendix~\ref{app:additional_examples}.

\input{figures/auto_eval_ref}

\paragraph{Similarity to the references.} Finally, we use BLEU 1--4 \citep{papineni-etal-2002-bleu}, METEOR \citep{lavie-agarwal-2007-meteor}, and ROUGE-L \citep{lin-2004-rouge} to measure lexical overlap, and BERTScore \citep{bert-score} and embedding-based similarity based on the \texttt{all-mpnet-base-v2} Sentence Transformers \cite{reimers-gurevych-2019-sentence} to measure semantic similarity to the references. % and NLP similarity against human references. 
In all cases, we compute the maximum score between the human reference and all the generated follow-ups, and report the average for the entire dataset.  
% compute scores for all generated follow-ups against the human reference and retain the best score for each instance before averaging across the dataset. 
The automatic evaluation results are summarized in Table~\ref{tab:auto_eval_ref}, revealing a consistent preference to \textit{ORG} across models. This is unsurprising, given that the questions in the \textit{ORG} training set and the test set come from the same distribution (the original \fd). %However, this model also exhibits greater variability in question length (see Section~\ref{sec:length_analysis}), which may contribute to higher n-gram overlap scores.  
The low BLEU scores from the other models align with prior findings in open-ended question generation, where models generate plausible yet lexically diverse outputs that standard n-gram-based metrics fail to capture \citep{pan-etal-2021-zero}. Indeed, the gap between \textit{FULL} and \textit{AUG} is less pronounced in BERTScore and sentence similarity, which focus on the semantic alignment with human references and de-emphasize the style.  

\input{figures/tab_annotation_questions}
\input{figures/mean_variance}

\subsection{Human Evaluation}
\label{sec:results:human}

We conducted a human evaluation to assess the quality of generated follow-up questions across four key aspects: validity, complexity (the level of reasoning required), relevance, and informativeness (Table~\ref{tab:annotation_questions}). We randomly sampled 30 (IQ, IA) pairs from the \textsc{FollowupQG} testing set and verified the generated FQs from all models. 

The evaluation was carried out using Cloud Connect. To ensure high-quality annotations, we restricted participation to native English-speaking annotators with a minimum of 1,000 completed annotation tasks and an approval rating exceeding 90\%. Annotators interacted with a structured evaluation interface (see Appendix~\ref{app:interface}). Each task presented an original question, its corresponding answer, and a generated follow-up question. Annotators first assessed whether the follow-up question was valid. If deemed invalid, they proceeded directly to the next task. Otherwise, they answered four additional evaluation questions, as detailed in Table~\ref{tab:annotation_questions} (See Appendix~\ref{Model Evaluation - Human Annotation Guideline} for the complete annotation guidelines). 
Each task was annotated by 3 annotators, yielding substantial inter-annotator agreement, with an average Cohen's Kappa of $\kappa = 0.77$ \cite{mchugh2012interrater}.

\input{figures/example_question_long}
% \input{figures/followup_evaluation}

Table~\ref{tab:mean_variance} presents the mean and variance for each evaluation aspect. Overall, \textit{AUG} achieved the best results across all categories, with a statistically significant difference from the other models (tested with a one-way ANOVA). Over 90\% of the FQs generated by \textit{AUG} were considered valid, and these questions were judged as relevant, somewhat informative, and minimally to moderately complex. \textit{FULL} closely follows across aspects, while \textit{ORG} lags behind. The only aspect on which \textit{ORG} closely follows \textit{FULL} is relevance, aligning with the findings of \citet{meng-etal-2023-followupqg} that current models perform well in maintaining relevance. Overall, the results clearly prefer the questions generated by \textit{AUG}, which excel in validity, complexity, relevance, and informativenessâ€”key qualities for meaningful follow-up questions. 

% \paragraph{Validity.} \textit{AUG} achieved the highest percentage of valid follow-up questions, with 90.65\% classified as valid, closely followed by \textit{FULL} at 87.43\%. In contrast, \textit{ORG} (73.24\%) significantly lagged behind \textit{AUG}, indicating that the augmentation method effectively improves question quality. This result also aligns with the ungrammatical filtering in Table~\ref{tab:filtered}, further supporting the validity assessment of the generated questions.

% % \paragraph{Error.} \textit{FULL} produced the fewest errors in follow-up questions (13.93\%), followed by \textit{ORG}(17.69\%) and then \textit{AUG} (19.63\%). Across all models, over 80\% of questions contained no errors.

% \paragraph{Complexity.} The \textit{AUG} model achieved the highest reasoning score (1.48), indicating a level between minimal and moderate reasoning. It slightly outperformed \textit{FULL} (1.44) and significantly exceeded \textit{ORG} (0.93) by over 50\%. The low score of \textit{ORG} suggests its questions required little reasoning, often focusing on surface-level details rather than deeper inquiry. In contrast, \textit{AUG} and \textit{FULL} generated more thoughtful questions, which align better with human-like questioning behavior, demonstrating the effectiveness of augmentation in enhancing reasoning depth.

% \paragraph{Relevance.} All models generated generally relevant follow-up questions, aligning with \citet{meng-etal-2023-followupqg}'s conclusion that current models perform well in maintaining relevance. Nevertheless, the \textit{AUG} model, with an average relevance score of 2.09, outperformed \textit{FULL} (1.74) and \textit{ORG} (1.62), suggesting that our augmentation method further enhances relevance and refines question quality.

% \paragraph{Informativeness.} The \textit{AUG} model (1.45) generated the most informative follow-up questions, typically requiring some amount of new information. It outperformed the \textit{ORG} model (0.78) by over 85\% and also surpassed the \textit{FULL} model (1.3). This suggests that the \textit{AUG} model is more effective in generating questions that delve deeper, requiring additional insight or clarification, which aligns with the goal of enhancing the informativeness of follow-up questions.

The comparative results across models reveals key insights into the role of data quality versus quantity in the task of follow-up QG. Notably, \textit{AUG}, trained on the same number of instances as \textit{ORG} but using only GPT-4-generated questions, consistently outperform both \textit{ORG} and \textit{FULL} across most metrics. This suggests that data quality is more critical than dataset size. Despite having ten times more training data, \textit{FULL} failed to surpass \textit{AUG}, likely due to noise in the original dataset diluting learning effectiveness. In contrast, \textit{AUG} benefited from a curated subset of high-quality, reasoning-heavy examples, leading to greater validity, complexity, relevance, and informativeness. These findings challenge the assumption that larger datasets inherently improve performance, highlighting the importance of targeted augmentation with strict quality control. Future work should explore strategies that balance scale with rigorous data refinement to further optimize question generation.

