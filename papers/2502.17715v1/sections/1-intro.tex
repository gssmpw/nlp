Asking questions is a fundamental mechanism for humans to acquire new information, particularly when existing information is incomplete. While large language models (LLMs) excel at passively answering user queries, their ability to proactively guide conversations, by identifying and addressing gaps in information, remains underdeveloped \cite{liu-etal-2025-superficial}. Question generation (QG) has gained significant attention in NLP, particularly for its role in improving information-seeking dialogue systems \cite{chen-etal-2024-learning-retrieve}. By generating effective questions, these dialogue systems can make information-seeking more accurate and efficient \cite{qi-etal-2020-stay}, resolve ambiguities \cite{li2017learning}, and ultimately better understand the user's needs, providing suitable assistance across various domains, including education \cite{laban-etal-2022-quiz} and healthcare \cite{arslan2024accuracy, li2024mediq}.

While most existing QG tasks focus on generating questions directly answerable by a given context \cite{ zhao-etal-2018-paragraph, pan-etal-2020-semantic, ghanem-etal-2022-question}, which diverges from the human cognitive process of inferring and pursuing missing information, \citet{meng-etal-2023-followupqg} proposes that models must generate \emph{follow-up questions} that build on---but are not answerable by---an initial question-answer pair. \fd{} is more reasonable, but \citet{meng-etal-2023-followupqg} found that existing models struggle to replicate this human behavior, often producing repetitive or context-bound questions that fail to target unexplored information.

This core challenge can be formulated into two dimensions: (1) identifying information gaps, the unanswered aspects of the initial question, and (2) generating diverse questions that address these gaps. Traditional QG methods \cite{zhao-etal-2018-paragraph, pan-etal-2020-semantic, ghanem-etal-2022-question}, focusing on rephrasing or extracting information directly from the input, are less effective at generating questions that infer missing or implied content. In contrast, recent work \cite{mazzaccara-etal-2024-learning,liu-etal-2025-superficial} that attempts to generate follow-up questions that seek missing information lacks explicit mechanisms to model gaps or ensure diversity, resulting in questions that regurgitate existing context or exhibit limited novelty.

To mitigate these limitations, we propose an information gap-driven pipeline that generates follow-up questions by contrasting original answers with synthetically generated comprehensive answers. Our key insight is that comprehensive answers---hypothetical, LLM-generated ``complete'' responses to the initial question---reveal potential gaps when compared to the original, often incomplete answers. By analyzing these gaps, our method explicitly targets unanswered information, aligning with human cognitive strategies. For example, in Figure \hyperref[fig:teaser]{1}, if the original answer to ``how do clouds form?'' explains ``clouds form when water vapor cools,'' a comprehensive answer might add ``...and condenses around dust particles,'' exposing the gap ``What role do particles play in cloud formation?''. 

Specifically, we use GPT-4 ({\ttfamily 4o: 2024-02-15} \texttt{-preview}) to generate both the comprehensive answers and the information gap-driven follow-up questions and verified the high quality of the generated questions. We then augment the original \fd{} training set with more than 25,000 synthetic examples (approximately 10$\times$ the original size) and fine-tuned several language models on both the original dataset and our augmented dataset. Instead of using GPT-4 directly for follow-up question generation, we adopt a distillation approach: leveraging GPT-4 to generate high-quality training data, and then fine-tuning smaller models to achieve strong performance at a significantly lower cost. The extensive experimental results demonstrate significant improvements of the augmented dataset over the baselines, both in terms of quality (validity, relevance, informativeness, etc.) and diversity.
% This promising approach could be adapted by future work to augment information-seeking dialogues to reduce ambiguities and improve the accuracy of LLM answers.
Our contributions are as follows:

\begin{itemize}[leftmargin=10pt]
    \item We propose a novel approach that generates follow-up questions by identifying information gaps through contrastive analysis of original and generated comprehensive answers.
    \item We augment the \fd{} training set with 25,000 synthetic examples, which achieved 94\% validity according to human evaluation.
    \item Experimental results show that models fine-tuned on our augmented dataset outperform baselines in diversity and informativeness.
\end{itemize}