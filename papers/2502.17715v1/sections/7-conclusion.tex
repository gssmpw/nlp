In this work, we proposed a novel approach to enhance the diversity and informativeness of generated follow-up questions by directly modeling information gaps through a generated comprehensive answer. Training data was generated using GPT-4 and distilled into a smaller model. Our experiments demonstrate that training the smaller model on the augmented dataset significantly outperforms the baselines both in terms of quality and diversity, showing that this method can be effectively adopted to enhance information-seeking dialogues, reduce ambiguities, and improve the accuracy of LLM responses.   %Extensive evaluations demonstrate that GPT-4o can potentially serve as an alternative to human annotation for informativeness, though refinements are needed. 
Future work can explore methods for further optimizing the diversity of generated questions while reducing their redundancy, as well as applying our method to downstream applications that involve multi-turn dialogues. There is also room for developing more automated evaluation metrics to quantify the quality of generated questions, given the cost of human evaluation and the limitations of current automatic evaluation. 

% Our proposed approach introduces significant advancements in follow-up question generation, particularly in terms of increased follow-up diversity and evaluating the informativeness of follow-ups. These dimensions are essential for ensuring that follow-up questions enhance conversational engagement by introducing new perspectives while maintaining contextual relevance. Evaluation results indicate that our model surpasses the baselines in both aspects, exhibiting quantitative gains in diversity and informativeness metrics, as well as qualitative advantages confirmed through human annotation.