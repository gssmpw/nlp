\paragraph{Model Variants.} To evaluate the effectiveness of our proposed pipeline and augmented dataset, we train several variants of the same model on different data. We use BART-large \cite{lewis-etal-2020-bart} as our base model, motivated by its strong performance on the work of \fd{} \cite{meng-etal-2023-followupqg}. We report the performance of three variations trained on \fd{} training set. The \textit{ORG} variant follows the method presented in \citet{meng-etal-2023-followupqg}, trained on the 2,790 original instances from the training set, serving as the baseline model. It employs a seq2seq model that conditions on IQ and IA (\texttt{IQ <SEP> IA}) to generate FQ.\footnote{As the implementation from \newcite{meng-etal-2023-followupqg} was not publicly available, we tried to reproduce the performance based on the hyperparameters reported in their paper, however, we had to reduce the learning rate from 5e-5 to 2e-5 avoid training instability. The details about the hyperparameters and difference in performance are available in Appendix~\ref{appendix:baseline_reproduce}.} 

We similarly train the \textit{FULL} variant on all 27,874 instances in the augmented dataset. Finally, we also train the \textit{AUG} variant on a random sample of 2,790 of the GPT-generated questions (Sec~\ref{sec:data:augmentation}), to isolate the impact of our data augmentation technique from the effects of increased dataset size. For all variants, we used identical hyperparameters and evaluated them on the original \fd{} validation and test sets\footnote{While other follow-up question generation methods exist, we do not include direct comparisons due to their fundamentally different, non-comparable setups (e.g., \citet{mazzaccara-etal-2024-learning}) and the very recent release of concurrent work (e.g., \citet{liu-etal-2025-superficial}), making direct comparison neither straightforward nor feasible. However, we discuss their relevance in Section~\ref{sec:background}.}. %, we utilized the original \fd validation set across all configurations to ensure consistent evaluation.

% \subsection{Dataset Configurations}
% \label{dataset_config}


% \subsection{Training Protocol}
% We maintained consistent training protocols across all experiments to ensure a fair comparison. Each model configuration was trained using identical hyperparameters as established in our baseline reproduction. During training, we monitored the model's performance on the validation set but did not implement early stopping to maintain consistency with the baseline implementation.
% This setup ensures a rigorous evaluation framework for assessing both the effectiveness of our data augmentation technique while controlling for dataset size effects.

\paragraph{Decoding.} To generate diverse but contextually relevant follow-up questions, we input the initial question and answer into the model in the following format: \texttt{IQ <SEP> IA}, and generate 10 follow-up questions by applying beam search with a beam width of 20, selecting the top 10 candidates. We add a diversity penalty of 10 to encourage unique outputs across the groups and set the temperature to $t =1.0$ to maintain a balance between diversity and coherence. The maximum length for each generation is set to 1024 tokens. Duplicate generations are removed. %, although the average length of outputs will be analyzed in the "Experiment Result" section.

% This setup ensures that the generated questions vary significantly while remaining contextually aligned with the original question-answer pairs. The hyperparameters, including the diversity penalty and temperature, were optimized through iterative experimentation to achieve high diversity and coherence. Further evaluation of the generated outputs is discussed in the "Experiment Result" section. We performed both automatic evaluation and human evaluation with the follow-up question candidates generated by three versions of BART fine-tuned, as explained in Section \ref{dataset_config}

% \paragraph{Evaluation Metrics.}
% To comprehensively assess the quality of generated follow-up questions, we employ both automatic evaluation and human evaluation. For automatic evaluation, we compute scores for all generated follow-ups against the human reference and retain the best score for each instance before averaging across the dataset. Lexical alignment is measured using BLEU1â€“4 \citep{papineni-etal-2002-bleu}, METEOR \citep{lavie-agarwal-2007-meteor}, and ROUGE-L \citep{lin-2004-rouge}. Semantic similarity is assessed with BERTScore \citep{bert-score} and NLP similarity against human references. We also evaluate general readability based on sentence length and measure diversity by quantifying lexical and semantic variation using Distinct-n \cite{li-etal-2016-diversity} and agglomerative-clustering-based analysis semantic sentence embeddings. For human evaluation, we take a reference-free approach, using a structured annotation table to assess validity, reasoning, relevance, and informativeness.