Question generation (QG) focuses on automatically generating semantically meaningful and well-structured questions based on a given text \cite{ali-etal-2010-automatic}. While traditional QG techniques have made significant strides in domains such as machine comprehension \cite{du-etal-2017-learning, uto-etal-2023-difficulty}, e-commerce \cite{wang2021harvest}, and education \cite{luo-etal-2024-chain}, they primarily generate questions based on known answers. This approach contrasts sharply with human questioning behavior, which actively seeks new information from various perspectives. This limitation has led to the emergence of follow-up QG, a task whose goal is to generate questions that explore previously unanswered or underexplored aspects of a given text.

Follow-up QG has evolved from simpler methods, such as template-based and retrieval-driven approaches \cite{kumar2017incomplete, soni-roberts-2019-paraphrase, b-etal-2020-automatic}, to more advanced techniques that prioritize informativeness \cite{majumder-etal-2021-ask, mazzaccara-etal-2024-learning}. Knowledge-enhanced approaches, like those in \citet{ge-etal-2023-ask} and \citet{gupta-etal-2022-learning}, leverage entity-relation pairs and knowledge graphs to improve the depth of the generated questions. Further advancing this, \citet{liu-etal-2025-superficial} combined knowledge graphs with LLMs to increase question informativeness. Efforts to model human-like questioning behavior, such as InquisitiveQG \cite{ko-etal-2020-inquisitive}, have relied on crowd-sourced follow-up questions written for news articles rather than those naturally generated by humans, leading to a lack of depth and cognitive diversity.

We follow the setting of the \fd{} \cite{meng-etal-2023-followupqg}, which formalizes information-seeking follow-up question generation. Based on questions and answers from the ELI5 (explain like I'm 5) subreddit, follow-up questions in this dataset build upon---but are not answerable by---the initial question-answer pair, resembling real-world dialogues where follow-ups resolve ambiguities or deepen understanding. 

\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/data_augmentation_pipeline}
    \caption{\textbf{Data augmentation pipeline.} For a Q\&A pair, a comprehensive answer is first generated to the question. By comparing it with the initial answer, information gaps are identified. Finally, multiple follow-up questions are generated targeting those gaps.}
    \label{fig:data_aug_pipeline}
\end{figure*}

\newcite{meng-etal-2023-followupqg} found that models often produce questions that are either repetitive or fail to target unexplored information, thus lacking the cognitive diversity and variability seen in human questioning strategies \cite{sultan-etal-2020-importance}. While follow-up QG has made significant progress, existing approaches largely focus on generating questions directly, using various model architectures and knowledge enhancement techniques. Our work, however, takes a novel approach inspired by the human cognitive process that directly models information gaps and uses them to guide the follow-up question generation. 
% through comprehensive answer generation. This step serves as a foundation for follow-up question generation and shifts the focus from comparing model architectures to simulating human cognitive processes in questioning behavior. 
% As a result, we do not compare our method directly with existing models; instead, we evaluate its effectiveness by analyzing the informativeness and diversity of the questions generated by multiple models fine-tuned on variants of the \fd dataset.