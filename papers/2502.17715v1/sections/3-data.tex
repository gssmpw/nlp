Effective follow-up question generation (QG) requires models to infer and target gaps between the provided answer and the broader context of a conversation. We follow the task definition presented by the \fd{} \cite{meng-etal-2023-followupqg}: ``to generate follow-up questions that seek new information given the initial question and answer''. For simplicity, we denote the ``initial question'' as IQ, ``initial answer'' as IA, and the ``follow-up question'' as FQ. Critical limitations in the training dataset are identified, including quality issues, which are addressed through dataset cleaning (Sec~\ref{sec:data:cleaning}). The small scale (2,790 instances) and low diversity of the dataset are tackled by a novel data augmentation pipeline introduced in this paper (Sec~\ref{sec:data:augmentation}). Specifically, we augment the \fd{} training set with synthetic data generated by a pipeline that mimics human-like gap-driven questioning. This approach utilizes LLM-generated comprehensive answers to systematically identify missing information in initial answers and generate follow-up questions targeting those gaps. We demonstrate that the augmented data retains high quality (Sec~\ref{sec:data:quality}).

\subsection{Data Cleaning}
\label{sec:data:cleaning}

The \fd{} dataset is limited by its small scale, comprising 3,790 samples: 2,790 for training, 500 for validation, and 500 for testing. Within the 2,790 training instances, there are only 2,651 unique (IQ, IA, FQ) triplets, indicating duplication. Additionally, the number of 2,648 unique (IQ, IA) pairs suggest minimal follow-up question diversity, as 99.8\% of pairs have only one reference FQ. Further analysis also uncovered data quality issues, likely stemming from automated data collection (see Appendix~\ref{app:problematic_sample}). To improve the data quality, we did the following:

\begin{itemize}[leftmargin=10pt]
    \item \textbf{Deduplication.} We removed 139 duplicate (IQ, IA, FQ) triplet instances.
    \item \textbf{Reference quality check.} We manually filtered out 84 instances where the reference FQ diverged entirely from the initial question. 
    \item \textbf{Sensitive content removal.} We excluded 24 instances involving topics like self-harm or crime, which LLMs are likely to refuse to answer.
\end{itemize}

% \paragraph{Deduplication.} We removed 139 duplicate (IQ, IA, FQ) triplet instances. 

% \paragraph{Reference quality check.} We manually filtered out 84 instances where the reference FQ diverged entirely from the initial question. %This step ensures coherence in the augmented dataset.   

% \paragraph{Sensitive content removal.} We excluded 24 instances involving topics like self-harm or crime, which modern LLMs are likely to refuse to answer. %, avoiding pipeline failures during augmentation.

The cleaned dataset (2,543 instances) retained broad topic coverage (2,533 unique question-answer pairs).

\subsection{Augmentation Pipeline}
\label{sec:data:augmentation}

As discussed in Section~\ref{sec:data:cleaning}, the limited scale of the dataset and the lack of follow-up question diversity hinder the coverage of diverse questioning strategies, restricting model generalization. To address this, we design a GPT-4-based pipeline that augments the original dataset by generating additional follow-up questions. Our pipeline simulates human reasoning through three interconnected stages: comprehensive answer generation, information gap identification, and follow-up question generation\footnote{Please refer to Appendix~\ref{app:prompts} for the LLM prompts used for the following stages.}. %Each stage aims to enhance diversity, improve quality, and align with human strategies for effective information-seeking dialogues.

\paragraph{Comprehensive answer generation.} To identify gaps in the IA, we generate a comprehensive answer (CA) that represents a complete and thorough response to the IQ. As shown in Figure~\ref{fig:data_aug_pipeline}, we prompt GPT-4 iteratively to generate answers to IQ that target different perspectives, such as technical, ethical, and practical, and synthesize a unified CA.
% Prompts such as ``Generate a concise answer focused on a single perspective'' and ``Synthesize prior answers into a comprehensive explanation'' ensure the CA is broad yet cohesive. This process mirrors how humans consider multiple dimensions to intuit missing information. 

\paragraph{Information gap identification.} The next step is to identify key concepts or details discussed in the comprehensive answer (CA) but not covered in the initial answer (IA). This is done by prompting GPT-4. For example, in the example shown in Figure~\ref{fig:data_aug_pipeline}, the initial answer covers the topic of privacy issues but does not cover areas of cyber security (i.e. an information gap).
%The LLM uses the prompt: ``Identify specific explanations or concepts in the comprehensive answer absent from the initial answer.'' 

\paragraph{Follow-up question generation.} Using the identified information gaps, we prompt GPT-4 to generate follow-up questions that address those gaps while maintaining contextual relevance to the IQ and IA. The generated questions must meet three criteria: be (1) answerable by the CA, (2) unanswerable by the IA, and (3) grounded in terminology and context from the IQ. % This ensures that the follow-up questions are focused, relevant, and factually grounded, emulating human curiosity and reasoning.

% \paragraph{Dataset Reformation and Merging}
To match the format of the original follow-up questions in the \fd{} dataset, we automatically reformat the generated FQs to remove artifacts such as bullets or numbering. Each (IQ, IA) pair is enriched with multiple follow-up questions generated by our pipeline, averaging 10.95 questions per pair. The restructured dataset merges synthetic questions with cleaned human-generated examples, resulting in 27,874 samples---a 10$\times$ increase from the original dataset size. This expanded dataset captures the open-ended nature of human questioning, providing models with diverse and explicit signals to learn strategies for addressing information gaps effectively.

\subsection{Augmented Data Validation}
\label{sec:data:quality}

To assess the quality of the generated follow-up questions, we conducted a human evaluation study using Cloud Connect. To ensure high-quality annotations, we restricted participation to native English-speaking annotators with a minimum of 1,000 completed annotation tasks and an approval rating exceeding 90\%. A randomly sampled subset of 100 (IQ, IA, FQ) triplets was evaluated based on three key criteria: (1) whether the follow-up question was a valid question\footnote{A valid question must be in a question format and ask meaningful information, including Wh-questions (what/why/where/etc.), open-ended questions, probing questions and etc. \cite{meng-etal-2023-followupqg}}, (2) whether any component of the triplet contained sensitive information, and (3) the degree of relatedness between the follow-up question and the initial question-answer pair. The full survey format, including example annotations, is provided in Appendix~\ref{sec:augmented_data_annotation_guideline}. 
The results show that 94\% of the follow-up questions are labeled as valid, 92\% as not sensitive, and 91\% are related to the original (IQ, IA) pair. Inter-annotator agreement was moderate, with a Cohen's Kappa score of $\kappa = 0.73$ \cite{mchugh2012interrater}.
%, demonstrating strong inter-rater reliability in the evaluation process.

% \label{human_GPT}
