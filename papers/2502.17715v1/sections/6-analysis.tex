To further understand the strengths and limitations of our method, we present a qualitative comparison of follow-up questions generated by all models for the same (IQ, IA) pair (\S\ref{sec:analysis:casestudy}), as well as an analysis of the expected information gain from the generated follow-up questions (\S\ref{sec:analysis:infogain}). 

\subsection{Qualitative Analysis}
\label{sec:analysis:casestudy}

In Table~\ref{tab:example_question_long}, we compare follow-up questions generated by the \textit{ORG}, \textit{AUG}, and \textit{FULL} models for a given (IQ, IA) pair. The questions produced by the \textit{ORG} model are often either redundant, such as ``What is a heuristic?'', or tangential, like responding with ``How do you know if you can ELI5?'' to the original responder that mentioned they didn't know if they could explain it to a 5-year-old (ELI5)---diverging from the target concept of heuristics. While the \textit{FULL} model generates a variety of relevant questions, excelling in diversity, it sometimes includes tangential ones or redundant phrasing, such as ``How do heuristics help in segmenting and segmenting information for specific tasks?'', which can affect clarity. In contrast, the \textit{AUG} model offers the best balance of informativeness and diversity, producing focused and insightful questions, such as ``What are some examples of cognitive strategies that rely on heuristics?'' and ``How does the concept of a heuristic relate to the process of problem-solving?''. Additional examples can be found in Appendix~\ref{app:additional_examples}.

\subsection{Quantifying Information Gain}
\label{sec:analysis:infogain}

In Sec.~\ref{sec:results:human} we asked annotators to rate how informative each follow-up question is. Here we propose an alternative, automated method for effectively evaluating informativeness without human annotators---by leveraging the ``comprehensive answers'' generated from GPT-4 (CA; Sec.~\ref{sec:data:augmentation}). We consider CA as a proxy for the set of all and only the relevant information that can be discussed in the context of (IQ, IA) pair. Thus, an informative FQ cannot be answered by IA (otherwise, the answer to FQ would add no new information); but should be answerable from CA (otherwise, it may be irrelevant). Motivated by this, we prompt GPT-4 to evaluate the answerability of each model's generated follow-up questions using each IA and CA. %Follow-up questions are classified as informative if they are only answerable by CA but not by the IA.  

\input{figures/GPT_evaluation}

% We compared human-annotated informativeness scores with GPT-4's classification of informative questions across models (
Table~\ref{tab:auto_informative} reassess the findings from the human evaluation that \textit{AUG} produced the most informative follow-up questions (36\%), followed by \textit{FULL} (35\%) and \textit{ORG} (25\%). 

Comparison between the GPT-4 predictions and the human-annotated informativeness scores (\S\ref{sec:results:auto}) validates this automated approach by showing that annotators assigned slightly higher scores to questions classified by GPT-4 as informative (1.29) than to those classified as not informative (1.07). A T-test ($\textit{p-value} = 0.0011$) confirmed statistical significance, however, with a small magnitude (Cohen's $d = 0.215$) \cite{cohen2013statistical}. %, implying that the actual difference in informativeness scores is relatively minor. 

% These findings suggest that GPT-4's evaluation method could serve as a viable alternative to human annotation, offering potential benefits in reducing annotation time and cost. While it may not strongly differentiate between highly informative and weakly informative questions, its validity remains evident. To further improve the accuracy of informativeness assessment refinements could include redefining what constitutes a ``comprehensive answer'' and increasing the sample size to enhance robustness.

% while GPT-4's evaluation method is valid, it does not strongly differentiate between highly informative and weakly informative questions. offering  Given the small effect size, however, further refinements may be needed to improve the accuracy of informativeness assessment, such as redefining what constitutes a ``comprehensive answer'' and increasing the sample size to enhance robustness.

