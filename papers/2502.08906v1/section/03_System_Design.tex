\section{System Design Focus}
\label{sec:system_design}
Our goal is to finalize a system that assists blind people in exploring an indoor environment independently.
In this section, we describe the key design elements of the system.

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figure/device.png}
    \caption{Image of the robot and handle interface used in the study. Panel A-1 shows the robot used in the formative study, while Panel A-2 presents the robot used in the main study. Panels B-1 and B-2 illustrate the mapping of the handle interface buttons' functions, depending on the selected navigation mode.}
    \label{fig:device&ui}
    \Description{The image consists of four panels labeled A-1, A-2, B-1, and B-2. Panels A-1 and A-2 display a suitcase-shaped device used in the robot, while Panels B-1 and B-2 illustrate the button mapping of the handle interface for navigation. Panel A-1 depicts the initial robot used in the formative study. The robot is red and resembles a suitcase. The panel outlines six key components: The handle interface is located where a typical suitcase handle would be and includes five buttons. A smartphone is mounted on the back of the handle using a mounting device. A touch sensor, positioned under the handle, detects when users are touching it. Three RGBD sensors, situated on the front of the robot, are used for depth and color sensing to assist in obstacle detection and navigation. A 360-degree LiDAR sensor is mounted on top of the robot, on top of the three cameras. Motorized wheels at the base provide mobility, allowing the robot to move autonomously or in response to user input. Panel A-2 shows the updated robot used in the main study. The most significant change is the addition of a 1080p-resolution fisheye camera, positioned near the handle to capture images from a higher point of view. Panel C-1 illustrates the button mapping when the robot is in Automated Navigation Mode: The up button increases speed, and the right button decreases it. The left and right buttons adjust the level of detail. A long press on the middle button triggers conversation mode, while a single press switches to manual control mode. Panel B-2 displays the button mapping when the robot is in Manual Control Mode: The up button moves the robot forward, the back button moves it backward, the left button turns it left, and the right button turns it right. A long press on the middle button triggers conversation mode, while a single press returns the robot to automated navigation mode.
    }
\end{figure*}

\subsection{Device}
Assistance systems for blind people have been proposed in various devices, such as smartphones~\cite{presti2019watchout}, \red{handheld haptic devices~\cite{spiers2016outdoor,choiniere2016development,liu2021tactile},} wearable devices~\cite{li2016isana}, cane-like devices~\cite{ranganeni2023exploring} and robots~\cite{liu2024dragon}.
Each type of device offers unique advantages - Smartphones \red{and handheld haptic devices are portable; Smartphones} are also widely used by blind people~\cite{morris2014blind,martiniello2022exploring}; Wearable devices free the user's hands~\cite{lee2014wearable}; Cane-like devices resemble traditional canes~\cite{ranganeni2023exploring}; And robots are able to autonomously guide users~\cite{guerreiro2019cabot}.
\red{While handheld devices~\cite{presti2019watchout,spiers2016outdoor,choiniere2016development,liu2021tactile,ranganeni2023exploring} have often been used due to their portability, in exploration scenarios, they require users to point the devices in their directions of interest while navigating around unfamiliar locations and obstacles, which involve high cognitive load.
Thus}, we chose robots because of their autonomous navigation and obstacle avoidance capabilities. 
This allows users to concentrate on learning the environment~\cite{cai2024navigating,zhang2023follower,jain2023want}. 
In particular, we adopted a wheeled robot~\cite{guerreiro2019cabot,zhang2023follower,wang2022can}.
While wheeled robots are unable to navigate stairs like quadruped robots~\cite{cai2024navigating}, blind users often find wheeled robots more suitable due to their silence and stability~\cite{wang2022can}.
Our assumption is that the devices should ensure the users' safety during navigation and allow users to focus on exploration. 
As a result, the findings in our study can be extended to any similar devices other than wheeled robots. 

\subsection{Describing Scenes}
Previous navigation systems relied on hardcoded information~\cite{sato2019navcog3,Kaniwa2024ChitChatGuide} or simple image captioning models~\cite{saha2019closing} to provide scene descriptions. 
They only convey information related to navigating to destinations. 
In exploratory tasks, any information and details could be relevant. 
Therefore, we decided to use MLLM, a foundational model capable of recognizing a variety of objects and describing them in natural language. 
We injected MLLM into the system to periodically provide comprehensive information about the surroundings to inform blind users during exploration. 
In this paper, we investigate the appropriate presentation format, such as content types and lengths, and the quality of the responses from MLLMs through our user studies.

\subsection{Interaction} % This section is so hard to write... Plz gimme idea if there is any better way
The ability for users to select destinations and routes according to their interests, often referred to as autonomy, is particularly important for exploration~\cite{Kaniwa2024ChitChatGuide,kayukawa2022HowUsers}. 
In our system, to what extent users prefer to take control over the robot (\ie, interaction) remains unknown.
Based on the scene descriptions given by the system~\cite{Kaniwa2024ChitChatGuide}, some blind users may fully embrace letting the robot guide them automatically, while others may prefer to decide which way to go on their own.
Additionally, this preference may also be influenced by the robot's descriptions of the scenes. 
Given that the extent of user preference for autonomy remains unclear, we first conducted the formative study (Sec.~\ref{sec:study1}) to explore the requirement of autonomy based on interaction needs. 
Then, we conducted a full study (Sec.~\ref{sec:study2}) to evaluate the users' opinions on autonomy in our improved system, which integrated the feedback from the formative study.
