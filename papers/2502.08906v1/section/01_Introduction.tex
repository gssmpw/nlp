

\input{tables/related_work}
\section{Introduction}
Exploration is a fundamental skill that allows one to gain familiarity with novel environments that blind people do not know. 
Sighted people explore by visually perceiving points of interest (POI) and navigating to desirable destinations. 
However, blind people face significant challenges in independently exploring new environments~\cite{Engel2020travelling,muller2022traveling}. 
They typically rely on sighted assistants, such as friends or family members, to help them navigate and describe their surroundings.
Unfortunately, these assistants are not always readily available, resulting in limited opportunities for blind people to explore independently.

\red{
Over the past recent years, various guide systems~\cite{bineeth2020blindsurvey, sulaiman2021analysis, manjari2020survey}, that are aimed for navigation~\cite{sato2019navcog3,li2016isana} or exploration~\cite{Kaniwa2024ChitChatGuide,kayukawa2023enhancing}}, have been developed to guide blind people and provide details about surrounding POIs in the environment. 
These systems typically rely on prebuilt maps and localization infrastructure 
\red{(\eg, Bluetooth Low Energy (BLE) beacons~\cite{sato2019navcog3,murata2018smartphone,kim2016navigating,chen2015blindnavi,InclusiveNavi} and ultrawide-bandwidth beacons~\cite{lu2021assistive})}
that are highly customized to the environments to continually update their current locations and offer turn-by-turn navigation guidance.
\red{
As access to the prebuilt maps also allows these systems to convey information about nearby POIs while navigating, some systems are specialized in assisting exploration activity~\cite{Kaniwa2024ChitChatGuide,kayukawa2023enhancing}.
}
However, only a limited number of \red{guide systems (\eg, InclusiveNavi~\cite{InclusiveNavi} and BlindSquare~\cite{BlindSquare})} are publicly \red{deployed} because configuring and maintaining prebuilt maps and localization infrastructure is expensive, and it is infeasible for them to be deployed in unseen environments. 
\red{
Several systems that do not require maps, as well as remote sighted assistance (RSA)~\cite{kamikubo2020support,Aira,BeMyEyes}, have been developed to guide blind people in various locations~\cite{kuribayashi2023pathfinder,Kuribayashi2022CorridorWalker,fallah2012user,lacey2000context}. 
However, these systems primarily focus on navigation to target destinations, not exploration, thus providing only navigation-related information to users (\eg,  intersections~\cite{Kuribayashi2022CorridorWalker,kuribayashi2023pathfinder} and signs~\cite{kuribayashi2023pathfinder}). 
These systems are also not independent from human assistance.
}
To promote social inclusion and equality for blind people, there is a need to develop a \textit{map-less} guide system that assists blind people in exploring diverse novel locations without relying on prebuilt maps or infrastructure.

\red{
To bridge the gaps and address the shortcomings of existing systems, we developed a system with the following characteristics as shown in Table~\ref{tab:relatedwork}: 1. Our system does not rely on prebuilt maps or preinstalled infrastructure. 2. Our system focuses on exploration. 3. Our system does not require supplementary assistance from humans. 4. Our system can automatically guide users physically during exploration. None of the prior systems possess the combination of all these characteristics.
Given that the design space for a map-less exploration guide robot remains underexplored, this work aims to investigate and establish the key components of such a system.
We begin by selecting a wheeled robot platform as the device. 
The decision to use a wheeled robot is based on its ability to autonomously guide blind users. 
It alleviates the challenge of navigation, which is cognitively demanding while learning about the surrounding environment.
Additionally, we equip the robot system with the ability to convey real-time information about the surrounding environment to users using natural language, accomplished through a multimodal large language model (MLLM~\cite{GPT4o}). 
}

Using our prototype system, we employed an iterative process with the direct involvement of target users to develop our system. 
In the formative study, the participants were asked to follow the robot, which was controlled in a Wizard-of-Oz fashion~\cite{riek2012wizard-of-oz}, along predetermined routes while listening to the environment descriptions.
The study revealed three groups of user preferences in the system's descriptions with respect to varying levels of details in the descriptive information received.
It also revealed requirements in certain functionalities, such as revisiting locations where the system had mentioned, specifying directions to proceed, and obtaining in-depth information through question-and-answering (Q\&A) functionality.

In the second stage, taking the lessons learned from the first study, we present \textit{WanderGuide}, a map-less exploration system for blind people (Fig.~\ref{fig:teaser}).
Taking into consideration the previously discovered three groups of user preferences, the system offers three modes for describing the surroundings: (1) Detailed description --- in-depth information with high granularity, (2) Balanced-Length description --- balanced level of information, and (3) Concise description --- minimal but essential details for obtaining quick awareness. 
We also implemented various new features based on the feedback received from the first study, which includes adopting a high-resolution fisheye camera for better perception of the surrounding environment, allowing users to verbally interact to query about the environment and set explored POIs to be navigation destinations, and allowing users to use directional buttons to control the robot for navigation towards the direction of interest. 
Our system is also fully integrated with the automatic mapping, localization, map-less navigation, and obstacle avoidance functions of the wheeled mobile robot.

Finally, we conducted a main user study with five blind participants, who were asked to freely explore two floors of the science museum.
All participants appreciated the experience of wandering freely without a fixed destination, and they expressed their desire to use the system to explore both familiar and unfamiliar areas. 
Participants also highlighted the need to incorporate recognition of auditory cues from the environment.
Additionally, differences in how they interacted with the system were observed: one frequently used buttons to guide the robot towards their areas of interest, one passively followed the robot, and others often asked questions. 
We also identified a limitation in the system's MLLM when conveying detailed information about the surroundings, such as identifying specific names of objects, which suggests the need for further development in how we input information into the MLLM for exploration purposes.

\red{
To the best of our knowledge, our work is the first to investigate the design space of a map-less system for blind people to explore independently.
To this end, we made the following contributions.}
\begin{itemize}
    \item \red{We formulated the requirements for the system through a formative study, such as the ability to adjust the level of description based on user preferences and to guide users to previously visited locations of interest, thereby enhancing the exploration experience.}
    \item \red{
     We developed a full stack map-less exploration system that consists of a waypoint detection algorithm and an MLLM-based perception interaction system on top of an existing navigation guide robot. Additionally, we integrated several functionalities based on the formative study that facilitates the exploration experience.
    }
    \item \red{We confirmed key design requirements, such as varying the level of descriptions based on user preferences through a usability study. We also gained further insights into users' interaction preferences and into design implications for improving the system, including better recognition of audio cues.}
\end{itemize}

\rrred{
The codes of the system are publicly available in the following link: \url{https://github.com/chestnutforestlabo/WanderGuide}.
}