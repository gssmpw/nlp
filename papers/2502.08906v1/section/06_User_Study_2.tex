\section{Main User Study}
\label{sec:study2}
This study was conducted to validate WanderGuide and explore further design space.
Participants were recruited and compensated similarly to those in the formative study.
Similar to the formative study, in the recruitment email, we specified that participants unfamiliar with the experimental location would be eligible to participate.
We conducted this study on the same two floors of the science museum. 
Tab.~\ref{tab:demographics2} shows the demographics of the participants.
\red{
None of the participants from the formative study participated in this study.
Similar to the formative study, this study was conducted after business hours.
}

\subsection{Task and Procedure}
For each participant, we first conducted a pre-study interview similar to the formative study.
Then, the participant joined a 30-minute training session to get familiar with the robot system before the main tasks.
For the main tasks, they were asked to freely explore the floor for 20 minutes using the system from a fixed starting location, as illustrated in Fig.~\ref{fig:route}.
The ordering of the floors was counterbalanced to mitigate the order effect.
After the main tasks, we conducted a post-study interview to ask several seven-point Likert scale questions (1: Strongly Disagree, 4: Neutral, and 7: Strongly Agree) that measure their self-evaluated exploration performance, Raw Task Load Index (TLX)~\cite{byers1989traditional} to measure the task workload, and system usability scale (SUS)~\cite{brooke1996sus} to evaluate the usability of the system.
Finally, we asked open-ended questions to gather comments on the system.
Below, we report the results of the study.

\input{tables/activity_breakdown}
\input{tables/request_breakdown}

\subsection{Analysis of Participants Activity During The Task}
\label{sec:activity_breakdown}
We report the statistics of each participant's activity during the task by referring to the system's log and the video captured during the tasks. 
Tab.~\ref{tab:activity_breakdown} shows the analysis of their time spent on the three modes as specified in Sec.~\ref{sec:implementation_button}.
We noticed that the activation quantity and duration of each mode varied significantly among participants.
P11, P13, P14, and P15 frequently used the conversation mode.
Notably, P11 spent nearly 40\% of the total time engaging in conversation with the robot.
In contrast, P12 barely used the conversation mode and relied on the auto mode for 90\% of the total time.
%Secondly, there was also a difference in the usage of the Auto and Manual control modes. 
P15 was the only participant who actively used the manual control mode.


\input{tables/error_analysis}
\input{tables/description_level_analysis}

\subsection{Analysis of Requests from Participants During Within The Conversation Mode}
\label{sec:request_breakdown}
In Tab.~\ref{tab:request_breakdown}, we further report the statistics of requests from participants within the conversation mode. 
Note that the total count of conversations in Tab.~\ref{tab:request_breakdown} is bigger than the conversation mode counts in Tab.~\ref{tab:activity_breakdown}, as multiple turns of conversation could happen in one conversation mode interaction.
We classify each verbal request into three categories. 
\begin{description}
    \item[General Query] Request general information in the surrounding area or in a particular direction.
    \item[Specific Query] Request detailed information about a specific object in the environment.
    \item[Command Query] Issue command to guide to destination, triggering ``Take-Me-There'' functionality or direction specification via conversation.
\end{description}

Overall, we discovered that although our system constantly provided environmental descriptions in auto mode, users still preferred to ask for general information about their surroundings or in a specific direction in conversation mode. 
For example, P13 predominantly made General Queries (61.54\%). 
Users also had diverse preferences when using our system. 
Some users such as P11 (45.71\%), P13 (38.46\%) and P14 (46.43\%) were interested in learning the specifics of POIs, reflecting the takeaways obtained in Sec.~\ref{sec:info_needs}. 
Some users such as P11 (42.86\%), P12 (60.00\%), P14 (39.29\%), and P15 (66.67\%) favored using conversation mode to instruct the robot to guide them to their destinations. 
In particular, by referencing Tab.~\ref{tab:activity_breakdown}, we can see that P11, P12, and P14 preferred conversation mode over manual control mode to issue commands. 
This validates the extrapolated idea in Sec.~\ref{sec:implication_directionspecification}.

\subsection{Error Analysis of Scene Description and Q\&A Responses}
\label{sec:error_analysis}
In Tab.~\ref{tab:hallucinations}, we report the accuracy of MLLM responses both during auto and conversation modes.
We manually analyzed the text output generated by MLLM and compared it with the logs of the images saved.
We classified and counted the errors made by MLLM into six categories.
\begin{description}
    \item[Wrong Character Recognition] Misrecognition of text, such as misreading signs.
    \item[Wrong Object Recognition] Misidentification of objects in the scene.
    \item[Nonexistent Objects and Texts] Mistakenly recognizing objects or text that are not present. Note that this differs from the previous two categories, where some similar objects or text were actually present.
    \item[Misunderstanding User Input] Misinterpreting a user’s question in conversation mode, such as providing an environmental description when asked to read text from a panel.
    \item[Inaccurate User Input] Errors made when the user asked about objects or text that were not present.
    \item[No Error] Accurate responses with no errors.
\end{description}
\red{
When multiple errors occur in a single sentence, errors of the same type are grouped together and counted as one. 
Errors of different types are counted separately. 
For instance, if there are multiple text recognition errors in a single sentence, they are counted as one text recognition error. 
If a sentence contains both text recognition errors and object recognition errors, each is counted separately as one text recognition error and one object recognition error.
Thus, note that the total number of errors may not match the total number of outputs.}

The results showed that 28.6\% of the outputs contained some form of error during scene descriptions whereas 60.3\% of conversation mode outputs had errors.
This difference is likely because users in conversation mode often asked for more detailed explanations, which led MLLM to attempt more complex responses and, as a result, made more mistakes.
This was particularly evident in the \textit{Nonexistent Objects and Texts} category, which accounted for only 0.07\% of errors during scene descriptions but significantly higher at 28.3\% in conversation mode.
This means that MLLM often generated descriptions of objects or text that did not exist in the environment when asked for more detailed information.
Character recognition errors were common in both modes, likely due to MLLM’s limitation in reading distant text. 
In a general sense, instead of complete failures, MLLM often partially misread the text or misidentified objects with similar-looking ones (\eg, mistaking a tall table for a reception desk).
Nevertheless, over 70\% of responses in the auto mode were accurate, demonstrating the overall usefulness of the system.

\subsection{Analysis of Usage of Each Description Level}
\label{sec:description_level_analysis}
In Tab.~\ref{tab:description_level}, we report the statistics of how much time participants spend their time using each description level.
The result shows that there were three types of usage during the study. 
P15 only used Balanced-Length mode, P11 and P14 used Balanced-Length mode most of the time while sometimes using Detailed mode, and P12 and P13 used Detailed mode most of the time. 

\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{figure/boxplot.png}
    \caption{Box plot of evaluation with human experts in seven-point Likert points.}
    \Description{The figure displays box plots for four questions evaluating the generated descriptions: Q1. I think the generated descriptions are natural: The median score is 5, with a minimum of 2, a first quartile of 4, a third quartile of 6, and a maximum of 7. Q2. I think the generated descriptions are precise: The median score is 5, with a minimum of 1, a first quartile of 3, a third quartile of 6, and a maximum of 7. Q3. I think the generated descriptions are appropriate as descriptions provided by a navigation robot to blind people onsite: The median score is 4, with a minimum of 2, a first quartile of 3, a third quartile of 5, and a maximum of 7. Q4. I think the generated descriptions are appropriate as descriptions provided when I am there to explain to blind people onsite: The median score is 3, with a minimum of 1, a first quartile of 2, a third quartile of 4, and a maximum of 7.    
    }
    \label{fig:boxplot}
\end{figure*}

\input{tables/likert}
\input{tables/rawtlx}

\subsection{Scene Description Quality Evaluation}
\label{sec:quality_eval}
Finally, to analyze the quality of the MLLM-generated scene descriptions from the human expert perspective, we conducted a survey with human museum guides and asked them to evaluate using a seven-point Likert scale. 
The participants were presented with images captured by the robot, each accompanied by its corresponding generated description, and were asked to evaluate the descriptions in a survey, as shown in Fig.~\ref{fig:boxplot}. 
The survey was conducted in a counterbalanced manner to mitigate potential biases.
During the main study, 164 descriptions were \rrred{generated}, and we randomly sampled half (82) of the total descriptions for evaluation.
\rrred{
The randomly sampled descriptions contain mixed levels of detail.
}
Each description is evaluated by three to four participants.
In total, 56 museum guides participated in the evaluation, with each randomly assessing five descriptions. 
There were 32 males and 20 females, and four participants did not report their gender.
Their average age was 39.6 years, with an average of 5.9 years of experience as a museum guide. 
On seven-point Likert scale items, the median self-reported familiarity with museums was 5.0, and the familiarity with LLMs was 4.0 (1: very unfamiliar, 4: neutral, and 7: very familiar).
Our analysis revealed that the experts generally perceived the generated descriptions as somewhat natural (Q1) and precise in describing an image (Q2) as shown by their median of five.
Meanwhile, they found the generated descriptions less suitable as image descriptions for blind people (Q3) and as onsite descriptions provided by experts for blind people (Q4).



\subsection{Usability and Workload Evaluation}
In Tab.~\ref{tab:likert}, we report the results of seven-point Likert items. 
For Likert items, a median score of five or higher indicates that participants generally responded positively.
The total SUS for P11 to P15 were 72.5, 80, 90, 82.5, and 77.5, respectively, showing acceptable usability of all being above 70~\cite{bangor2009determining}. 
The total Raw TLX scores for P11 to P15 were 24, 15, 20, 26, and 28, respectively. 
We show the distribution of Raw TLX scores in Tab.~\ref{tab:tlx}.
Raw TLX~\cite{byers1989traditional}, a simplified version of NASA TLX~\cite{hart2006nasa}, is known to have a high correlation with NASA TLX, and the total NASA-TLX scores for people with special needs typically ranged from 26 to 48 in previous research~\cite{hertzum2021reference}. 
Overall, our total Raw TLX scores may suggest that participants did not experience a significant load during the task.
We also observed that the median value for mental, physical, and temporal demand was relatively lower, scoring 2. 
This is likely due to the robot navigating them, allowing participants to explore without being burdened by these demands. 
Nonetheless, a relatively higher median value was observed for Performance, Effort, and Frustration, indicating that some users experienced a lack of satisfaction with the exploration experience provided by the system. 

\subsection{Qualitative Analysis}
\subsubsection{Positive Feedback}
All participants expressed their appreciation for the experience of wandering around a building to explore without specific destinations in mind with the help of our system:
\newanswer[\label{P12IWantThis}]\textit{``
When the camera explains things it recognizes, like how bright the room is or what the floor looks like, or what objects are placed where, I found myself nodding in agreement multiple times, like, ``Oh, so this is how it looks.'' 
I remember when I first held the suitcase robot, I deeply empathized with guide dog users. I thought, ``Oh, so this is what it's like to have a guide dog.'' However, since I can't take care of a guide dog, I’ve given up on that option. 
And now, with this navigation system that explains various situations, it's exactly what I need. It’s not just about setting a destination and getting there but feeling the freedom to explore spontaneously. For example, the ability to roam a large shopping mall freely and explore on a whim feels like true freedom to me. Instead of pre-planning every move or relying on a guide, I could simply grab my suitcase and decide to venture out spontaneously.''} (P12)

\red{
The same participant, P12, who had been to the facility previously, noted that they still had new discoveries with the system:
\newanswer[\label{P12IHaveBeen}]\textit{``
I've been to this museum before, but when the guide explained things to me back then, it was more like a general explanation about the atmosphere and such. 
But earlier with the system, there was a very detailed explanation that came out of the suitcase. Like, about how bright sunlight comes [...] 
There were things I didn’t know that made me learn new stuff, even though I thought I knew about the facility.''} (P12)
Also, P12 and P14 noted the feeling of relief not relying on sighted assistance:
\newanswer[\label{P14ThereHaveBeenNoSystem}]\textit{``
I don’t think there has ever been a system that explains your surroundings while walking. [...]
When walking with other people, I often find myself feeling a sense of obligation. I worry that they’re putting in extra effort to describe things because I can’t see. And then I feel like I have to respond to them since they’re trying so hard—which can be exhausting. But with this system, I feel I can go strolling by myself.''} (P14)
}


Participants also noted the functionality to go to an aforementioned destination and Q\&A functionality particularly useful:
\newanswer[\label{P11TakeMeBack}]\textit{``(The ``Take-Me-There'' functionality is) I think it's wonderful. After all, spatial awareness is difficult, so going back to landmarks is very important. If it is accurate, I think it's great because it can be extremely helpful for spatial cognition.''} (P11) and
\newanswer[\label{P14Q&A}]\textit{``When engaging in a conversation, not knowing what kind of response you'll get, the feeling of unease and excitement that's both a plus and a minus, I think. But I found it really great that you can still ask questions. So even if the response you get doesn't answer your question, or even if it's just ``I don't know,'' the fact that you can at least ask is important.''} (P14) 

\subsubsection{Adjusting Detail of Description}
When we discussed their preference in the level of detail of descriptions, all participants described that it would rather depend on the scenario they are in:
\newanswer[\label{P12NormalMode}]\textit{``It might depend on the location, but I know I can get detailed information in Q\&A functionality. So, for familiar places, the Balanced-Length mode might be fine. However, there are parts where I'd want the Detailed Description mode for unfamiliar places. For example, switching between modes could be useful, like having Detailed Description mode first for explanations about the room's brightness and how easy it is to walk around. ''} (P12)

\subsubsection{Comments to Improve the System}
\label{sec:improve}
Participants suggested various improvements to the system.
One particular suggestion was to incorporate functionality for the robot to understand sounds. 
As the experiment location was a science museum, various exhibits emitted sounds.
P13 noted that they would like to inquire about the sound sources, which were not supported by the system:
\newanswer[\label{P13Sound}]\textit{``We are extremely sensitive to sounds, and it becomes a point of interest. At a place like the exhibition hall we're visiting this time, various sounds are coming from all directions. This prompts questions like, ''What's happening at that sound over there?'' Therefore, it would be advantageous if we could ask specific questions like, ``What's that sound coming from the right?'' ''} (P13)

Also, four participants (P11-P13 and P15) found the descriptions from the system still insufficient to explore, as described in the following comments:
\newanswer[\label{P15Insufficient}]\textit{``The place we did the task this time was quite out of the ordinary. Even if you were walking around with my family, I think they would also have difficulty explaining it. Therefore, I felt it might still be somewhat challenging for machines to handle this kind of thing. However, I did feel it was good that I got a sense of what was there. But when it comes to the actual detailed explanations, it was not there [...]''} (P15)

\subsubsection{Specification of Proceeding Direction}
While we introduced all functionality to participants within the training session, we observed that only P15 used the functionality to specify which way to proceed via a button or conversation.
P15 tended to use the functionality when P15 was interested in a specific object:
\newanswer[\label{P15DirectionSpecification}]\textit{``It seems that when I was told, ``There's something on the right,'' I tried to approach toward it because I wanted to get closer when I used something like that.''} (P15)