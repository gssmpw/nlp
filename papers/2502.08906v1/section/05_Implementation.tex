

\section{WanderGuide Implementation}

\label{sec:Implementation}
%This section describes the implementation of our system based on the previous investigation.
\red{
In this section, we provide the implementation of WanderGuide informed by the formative study.
Below is a summary of updates made from the implementation of the formative study.
\begin{itemize}
\item Attachment of a new fisheye camera for a better view (Sec.~\ref{sec:device_update})
\item Implementation of a waypoint detection algorithm for realizing autonomous map-less navigation (Sec.~\ref{sec:waypoint_detection})
\item Implementation of three levels of description based on user preferences (Sec.~\ref{sec:Implementation_description_mode})
\item Implementation of ``Take-Me-There'' Functionality (Sec.~\ref{sec:take_me_there})
\item Implementation of two navigation modes automatic navigation mode and manual control mode (Sec.~\ref{sec:implementation_button})
\item Implementation of an interface to adjust speed, level of description, and navigation mode (Sec.~\ref{sec:implementation_button})
\item Implementation of Q\&A Functionality (Sec.~\ref{sec:QA})
\end{itemize}
}

\subsection{Hardware Update} %Want to know if there is a better name for this section
\label{sec:device_update}
One of the notable user feedbacks was the need for more detailed information, such as the names of POIs.
However, the cameras in the prototype system were mounted at only 0.51 meters above the ground, had low resolution, and had a limited vertical field of view, making it difficult for the MLLM model to consistently capture details.
Thus, as illustrated in Fig.~\ref{fig:device&ui}--A-2, we attached a fisheye camera with 1080p resolution and a wide field of view to the higher part of the robot.

\subsection{Waypoint Detection and Navigation}
\label{sec:waypoint_detection}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figure/waypoint.png}
    \caption{Three steps of the waypoint detection algorithm. Step 1 shows the generated cost map, while Step 2 depicts the skeletonization process of the cost map along with the detection of intersection points. Finally, Step 3 highlights the selected intersection points, which are identified as waypoint candidates.}
    \Description{
    The image depicts three stages of a waypoint detection algorithm. On the left, Step 1 shows a simplified map where a red rectangular robot is moving toward an open area. The map highlights obstacles in black and the surrounding areas in gray. The white space represents the robot's navigable area. In the center, Step 2 illustrates a skeletonized version of the map from Step 1. Thin white lines trace the navigable paths, forming a network-like structure. At several points where the paths intersect, small white dots indicate intersection points. On the right, Step 3 marks the final waypoint candidates. The robot, represented by a red rectangle, is shown along with red dots that represent the selected waypoint candidates. Dotted black lines trace possible paths from the robot toward these waypoints, navigating around the obstacles.
    }
\label{fig:waypoint_detection}
\end{figure*}

%上原さんのアルゴリズム
In order to produce destinations to navigate to for the users, a waypoint detection algorithm (Fig.~\ref{fig:waypoint_detection}) is necessary to determine navigable points for the robots.
As no prebuilt maps were available, we first constructed a cost map, a two-dimensional occupancy grid that assigns costs based on obstacles, and updated it in real-time. 
We utilized the \red{existing open-source Cartographer package~\cite{Cartographer}, which is a real-time Simultaneous Localization and Mapping (SLAM) algorithm,} to generate the cost map.
Next, the cost map was skeletonized, and intersection points on the skeleton were identified based on a kernel-based corner detection algorithm~\cite{soille1999morphological}.
The intersection points, which are typically far from obstacles, were next used to select potential waypoints.
To maintain sparsity among waypoints, we applied the DBSCAN clustering algorithm~\cite{ester1996density} over the intersection points, and selected the centers of the clusters as potential waypoints.
In addition, coordinates three meters in front, behind, and to the sides of the robot were also considered potential destinations to address the case where no intersection points were detected through the algorithm.
As selecting a waypoint too far may be challenging for the robot to find a suitable path and a waypoint too close would lead to frequent destination changes, we filtered out candidates further than 50 meters and closer than one meter to the robot.
After filtering, the final list of candidate waypoints was set.

During navigation, the robot automatically selected its goal from the candidate waypoints. 
By default, priority was given to waypoints lying in the same forward direction as the robot's initial orientation, where it was placed and activated. 
If no forward waypoints are available, the robot selects the waypoint with the smallest absolute angle relative to its current orientation. 
Once a waypoint was chosen from the candidate list, the robot navigated to it by using the onboard open-source navigation algorithm~\cite{guerreiro2019cabot}.
Once the waypoint was reached, the next waypoint was chosen automatically using the same process.
It is important to note that prioritizing the robot's initial orientation was based on the assumption that users can adjust the general direction to proceed, such as starting from the entrance into the building.

\subsection{Scene Description Generation}
\label{sec:Implementation_description_mode}
The basic algorithm for scene description generation remains unchanged, but the description was conveyed only when the robot was moving.
Also, the MLLM took the overall view image from the fisheye view camera in addition to the three RGB images from the RGBD cameras.
According to the results of the formative study, we added three levels of detail in the scene description.
\begin{itemize}
    \item{\textit{Detailed Description}}
    This mode provided rich, immersive descriptions for blind users who wanted to explore their surroundings in detail. 
    The MLLM generated 3-4 sentences (120-240 characters), covering lighting, signs, layout, nearby people, and subjective descriptors like ``beautiful'' or ``modern''. 
    The description began with an overview, followed by details of the left, front, and right.
    \item{\textit{Balanced-Length Description}}
    This mode offered clear descriptions for users who preferred concise but informative content. 
    The MLLM generated 2-3 sentences (60-120 characters), focusing on relevant details like signs and layout, while omitting lighting conditions or subjective descriptors. 
    Descriptions covered the left, front, and right, without the overview.
    \item{\textit{Concise Description}}
    This mode provided brief, essential information for users who wanted quick guidance. 
    The MLLM generated 1-2 sentences (less than 60 characters), focusing only on key details needed to navigate, excluding unnecessary information. 
    Descriptions covered the left, front, and right, without the overview.
\end{itemize}
For MLLM, these three levels were controlled via prompts, which are shown in Appendix Sec.~\ref{appendix:prompt_main}.
All prompts shared the following instructions in common: to convey environmental information that assists blind people to explore, to refer to specific details such as genres or the names of objects, to encourage reading any text that helps users explore, to describe spaces for guide dogs to sit in restaurants, to provide information about potential hazards, and to use numbers to indicate the relative positions of surrounding objects.
To ensure that the MLLM adhered to the instructions provided in the prompt, we employed a two-stage inference process. 
First, we instructed MLLM to perform an initial inference, generating a description. 
Then, it self-supervised this generated description to verify if it met the given instructions. 
Finally, MLLM produced a revised version of the description to be presented to the user.
Although this approach resulted in longer inference times, the outputs produced follow complex prompt instructions.
\red{
The description is read aloud every 5-10 seconds after the previous description has been read out.
The processing time and cost to generate a description was 5.78 seconds and \$0.00811 for a Detailed Description, 4.75 seconds and \$0.00753 for a Balanced-Length Description, and 4.02 seconds and \$0.00734 for a Concise Description on average.
}


\subsection{``Take-Me-There'' Functionality}
\label{sec:take_me_there}
Acting on the feedback received from the formative study, we implemented a function that guided users to a destination verbally specified.
This feature was typically enabled by the robot's \textit{semantic map}~\cite{shafiullah2022clip,yokoyama2024vlfm,liu2024dragon}. 
In our case, we linked the images and generated descriptions, which had been saved as the robot had navigated, to the cost map of the robot.
Given a verbal cue from the user (\eg \textit{``I want to go to the blue sofa.''}), the system first used our selected MLLM model to extract the name of the target location (\eg, blue sofa).
Then, we calculated the embeddings of the target location, all saved captured images, and all saved generated descriptions. 
We took a dot-product similarity between the extracted target location and the embeddings of images and descriptions to find the closest match.
We used pre-trained feature extraction models: a fine-tuned SimCSE~\cite{gao2021simcse} model for generating sentence embeddings from text and a pre-trained CLIP~\cite{radford2021learning} model for creating image embeddings.
We used models that were trained in the native language where the study was conducted.
The coordinate linked to the closest matched image or description would be set as the destination.
If the user wanted to go back to the initial location, we used MLLM to detect the user's intent and set the destination to the initial point.
\red{
We note that a similar functionality, the ``Take-Me-Back'' functionality, which allows users to return to their initial location, has been implemented in the previous map-less navigation system PathFinder~\cite{kuribayashi2023pathfinder}. The ``Take-Me-Back'' functionality is specifically designed for navigation purposes, as it was motivated by the challenge blind individuals face in returning to their original location after navigating. In contrast, our functionality is tailored for exploration tasks, enabling users to return to any point of interest they identified during their exploration. Ultimately, our functionality encompasses the capabilities of the ``Take-Me-Back'' feature while extending its application to support exploratory activities.
}

\input{tables/demographics2}

\subsection{Navigation Mode and User Interface}
%ボタンUIと音声フィードバック
On the high level, we implemented button controls and conversation interaction methods for users to interact with the robot.

\subsubsection{Button Controls}
\label{sec:implementation_button}
\red{
We utilized the four directional buttons and the central button on the handle of the suitcase-shaped robot to enable users to control the robot's speed, adjust the level of descriptions, switch between automatic and manual control modes, and specify the direction of movement.
}
The mapping of the buttons is illustrated in Fig.~\ref{fig:device&ui}--B-1 and B-2.
The central button was used for mode changes.
The functions of the directional buttons would change depending on the robot's modes:  \textit{auto mode}, \textit{manual control mode}, and \textit{conversation mode}.
In auto mode, the robot navigated by determining the waypoint automatically.
The left and right buttons allowed the user to switch between three levels of description, where the default mode is the balanced-length description mode. 
The forward and backward buttons were used to adjust the robot’s speed.
Users can adjust the speed from zero to one meter per second, with increments of 0.05 meters per second.
In manual mode, users could specify directions on their own.
The robot would instruct the user to press the directional buttons to select the direction to proceed.
If there was a suitable waypoint in the specified direction, the robot would inform the users via voice feedback.
Otherwise, the robot conveyed that there were no navigable points in the specified direction.
In conversation mode, triggered by long-pressing the central button, the robot would pause, and all four directional buttons were disabled until the conversation was ended.
Users could manually end the conversation by long-pressing the central button again.
The details of the conversation mode are described below.

\subsubsection{Conversation}
\label{sec:QA}
The conversation mode allowed users to give commands or ask questions with verbal input via the smartphone attached to the robot (Fig.~\ref{fig:device&ui}).
When the user inputted their verbal cue, the system used MLLM to classify the user's intent into one of three categories: usage of ``Take-Me-There'' functionality, usage of Q\&A functionality, and direction specification.
If the detected intent was direction specification (\eg, \textit{``I want to go to right''}) the robot would navigate to the waypoint in the specified direction accordingly.
Finally, users could finish the conversation with an ending phrase such as \textit{``Thank you.'' }
