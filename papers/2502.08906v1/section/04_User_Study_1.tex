\vspace{-2mm}
\section{Formative Study}
\label{sec:study1}
We first conducted a formative study to investigate the requirements of the system, such as how the system should explain its surroundings and what potential interactions may happen between the robot and the user.
To conduct the study, we recruited ten participants through our existing email list.
Interestingly, our recruitment emails were shared among blind people, eventually reaching people not on our emailing list. 
In the recruitment email, we specified that those who are unfamiliar with the experimental location, \ie, even if they have had previous visiting experience, they do not have a clear understanding of the building or know what is there, would be eligible to participate.
Tab.~\ref{tab:demographics1} shows the demographics of the participants. 
All studies in this paper have been approved by our institution's review board.
Informed consent was read out to all participants in this paper and obtained from them. 
The study took approximately two hours, and the participants were compensated \$20 per hour and reimbursed for their transit costs.
Only one participant was present for each session. 

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figure/routes.png}
    \caption{Floor maps of the location of the study. The left panel shows the two floors of the science museum, \rrred{the fifth floor of Miraikan}, which feature exhibits on various topics, such as environmental issues and space exploration. On the right panel is a floor plan of a shopping mall, \rrred{the fourth floor of Toranomon Hills Station Tower}, which includes a variety of restaurants offering different cuisines, including French, Japanese, Chinese, and cafes.}
    \Description{
    The image contains three floor plans, each marked with dotted red lines and stars, indicating different routes and destinations. On the left side are two floors of a science museum, while the right side displays a single-floor plan of a shopping mall. The first map of the science museum highlights exhibits in light blue, connected by hallways in light gray. The floor has a rectangular shape with a main pathway running through the center. There are three round exhibits—two in the middle and one on the right—with additional exhibits placed along the left and right sides of the main walkway. A red dotted line shows the route followed in the first study, starting from the left side, circling the floor, and returning to the starting point. The starting point for the main study is located at the same spot as in the formative study. The second map represents the museum's second floor, also rectangular in shape and similarly structured to the previous floor but without the round exhibits. Again, a red dotted line marks the path followed in the formative study, beginning on the left side, looping around the floor, and returning to the starting point. However, the main study on this floor begins at a different location, on the right side of the floor. The last map on the right shows part of a shopping mall, featuring shops in blue and common areas in gray. The floor has a maze-like layout with several intersections and includes 21 restaurants. The route for the formative study begins in the center of the floor, loops around each restaurant, and returns to the starting point.
    }
    \label{fig:route}
\end{figure*}

\subsection{Prototype System}
We developed our prototype robot system according to Sec.~\ref{sec:system_design}. 
It was based on an open-source robot platform\footnote{https://github.com/CMU-cabot/cabot} and could guide users while explaining the surrounding environment. 
To ensure that the participants experienced the same level of autonomy, we used teleoperation, a Wizrd-of-Oz-based approach~\cite{riek2012wizard-of-oz}, to force the robot to be in full-automatic mode when guiding the participants.
We adopted a suitcase-shaped wheeled robot for this study.
The suitcase's appearance allows blind users to seamlessly blend into their environment, leading to higher social acceptance from users, surrounding pedestrians, and facility managers~\cite{kayukawa2022HowUsers}. 
As shown in Fig.~\ref{fig:device&ui}--A-1, the robot has a handle embedded with five buttons, a touch sensor beneath the handle, a 360$^\circ$ \red{Velodyne VLP-16 LiDAR sensor~\cite{Velodyne}} sensor, three RGBD cameras with resolutions of 640×360, \red{one RealSense D455 camera~\cite{RealSenseD455} mounted at the front, two RealSense D435 cameras~\cite{RealSenseD435} on the left and right}, and a pair of motorized wheels in differential drive configuration.
\red{
Inside the suitcase, it has Ruby R8 powered by an AMD Ryzen R7-4800U CPU~\cite{NUC}, and a Jetson Mate featuring multiple Jetson Xavier NX GPUs~\cite{JetsonMate}.
}
The RGBD cameras were attached 0.51 meters above the ground.
The touch sensor detects whether or not the user is holding the handle and moves only when it is being held by the user. 
The cameras combined have a horizontal field of view of approximately 180$^\circ$.
The weight of the robot is approximately 15kg.
We set the default speed of the robot to 0.5 meters per second to maintain a balance between a comfortable walking speed and a speed that allows sufficient time to absorb the scene description audios.
A smartphone is attached to the suitcase to provide audio feedback through a neck speaker worn by users, connected via Bluetooth.

To convey the surrounding information to the participants, we used GPT-4o~\cite{GPT4o}, a popular MLLM model.
We inputted the images from the three RGBD cameras into the MLLM model and asked the model to generate descriptions of the surrounding environment.
The robot was designed to describe surrounding information 5-10 seconds after the end of the previous description every time. 
We engineered the prompts to ask the MLLM model to first provide a general overview of the scene, followed by specific details on the left, front, and right. 
We asked the descriptions to include as many objects as possible and incorporate layout information, such as navigable directions and the presence of walls~\cite{jain2023want}. 
\red{The processing time and cost to generate a description was 6.087 seconds and \$0.00740 on average.}
We attach the full prompts in Appendix Sec.~\ref{appendix:prompt_formative}.

\input{tables/demographics1}

\subsection{Experimental Location}
To ensure the diversity of the findings we would obtain from this study, we conducted the study in two different locations. 
\red{We chose to conduct our studies in a science museum and a shopping mall, as these are locations where people typically engage in exploration, and they have been utilized in previous research~\cite{asakawa2019independent,asakawa2018present,Kaniwa2024ChitChatGuide}. 
A museum is generally a place for learning about exhibits, while a shopping mall often requires exploration both before and during visits to stores.
\rrred{Specifically, we used the fifth floor of Miraikan\footnote{\url{https://www.miraikan.jst.go.jp/en/}} for the science museum and the fourth floor of Toranomon Hills Station Tower\footnote{\url{https://www.toranomonhills.com/}} for the shopping mall.}
The floor map of the science museum is illustrated in the left panel of Fig.~\ref{fig:route}, which contains two floors, both primarily featuring science exhibits. 
For the studies, the order of the two floors was counterbalanced.
The study in the museum was conducted after business hours, during which customers were absent, but staff were present for their duties.
The floor map of the shopping mall is illustrated in the right panel of Fig.~\ref{fig:route}, a floor that contains several restaurants from various countries. 
The study in the shopping mall was conducted during regular business hours.}
As shown in Tab.~\ref{tab:demographics1}, the study with P01--P05 took place in the science museum, and the study with P06--P10 took place in the shopping mall.

\subsection{Procedure}
For each participant, we first conducted a pre-study interview to learn about their experience in exploring buildings, followed by an explanation that the study aimed to gather their opinions on a guide system designed to assist with exploration.
Then, participants were given a task to navigate the predetermined route (red arrow of Fig.~\ref{fig:route}) guided by the robot.
Adopting a Wizrd-of-Oz-based approach, an experimenter controlled the robot to navigate along the route and stop when there were nearby pedestrians. 
During exploration, the robot periodically generated descriptions of the scenes. 
We show an example of the generated description in Fig.~\ref{fig:study1example}.
After the exploration, we asked the participants if there were any additional things they wanted to do to partially simulate the potential interaction, such as going to additional places or going around the floor again for more exploration.
Finally, we conducted a post-interview session to gather their feedback on the system. 

\subsection{Result}
\subsubsection{Interests to Exploration}
All participants stated that totally independent exploration is challenging, but they expressed a desire for exploration if a guide system can help them do so. For example:
\newanswer[\label{P02Conditioned}]\textit{``I don't really explore much. I go out with a specific purpose in mind [...] The reason is that it's just too bothersome. But I do think it would be fun if I did [...]  I'm more of an old-timer, so exploration never really caught my interest. It's not that I didn't care at all, but perhaps I've been living this way (not to explore).}\footnote{The comments were obtained in the native language where the study was conducted. We translate the comments into English using publicly available LLM to ensure reproducibility. We show the full prompt used for translation in Appendix Sec.~\ref{appendix:translate}.} (P02)

\subsubsection{Positive Feedback and Appreciated Information}
Seven participants (P01, P04--P08, and P10) expressed their enjoyment while navigating with the robot, particularly with the provided surrounding descriptions, as described in the following comment:
\newanswer[\label{P07Enjoy}]\textit{``My first impression was that it was a lot of fun. The reason is, as you just mentioned, unlike the person I usually walk with, the system provided detailed explanations about things like the color of the walls and the signs we saw and even described how the chef was preparing the food. Normally, you might get some of this information from others, but it's rare to get such thorough details. I found myself thinking, ``Oh, I see, that's how it looks to sighted people,'' and I felt there was a lot of new information. In that sense, I really enjoyed it.''} (P07)

Participants appreciated a variety of real-time details about their surroundings, notable examples include patterns on the walls, lighting conditions, subjective descriptors such as ``beautiful,'' the presence and actions of nearby people, the existence of signboards, the layout of the environment, and the visibility of a chef in an open kitchen. 
Additionally, P10, who requested to walk around the floor again, noted that receiving different descriptions of the same location was beneficial, as it gave them a sense of presence:
\newanswer[\label{P10VariousAndDifferentInformation}]\textit{``The system mentioned those things, as well as details about the plants and wall decorations. It's like, you talked about so many different things that it feels like I was actually looking around myself. Honestly, most of the time, I get so occupied with just reaching my destination that I don't notice things around me. [...] The system also mentioned things in the second round of explanations that weren't covered in the first round, which was nice. It conveyed a sense of the ongoing atmosphere and gave a good understanding of the situation at the time.''} (P10)

\subsubsection{Information Needs}
\label{sec:info_needs}
Participants hoped for further polishing of the delivered information about the scenes. Six participants (P01--P03, P06, and P09--P10) felt the information conveyed about the surroundings was too abstract, indicating the need for more specific information:
\newanswer[\label{P01NeedMoreConcreteInformation}]\textit{``The system talked about there are just exhibits, or there's information on panels, but I think it would be nice if the system talked about specific titles. There are places where the system talked about them, but there are also places where it did not, so I found myself wondering about that.''} (P01)
In particular, three participants (P02, P03, and P09) commented that the descriptions neither helped them learn the environment nor make decisions such as determining which shops or exhibits to enjoy:
\newanswer[\label{P09NegativeImpression}]\textit{``I expected it to at least tell me the name of the store, but it was disappointing to find out that it didn't do that at all. I really wish there was a system that could provide pinpointed information about what I want to know. Especially in an unfamiliar restaurant area, for example, if I come alone and use the device to enter the premises, it starts running, and then when I think, ``Oh, should I have Japanese food today, or maybe tonkatsu?'', without such information, I end up just walking around aimlessly.''} (P09)

Participants also described specifics about what types of information would be beneficial to include, such as the position of objects given in meters and clock directions, the availability of seats, people on collision paths, identities of surrounding individuals (\eg staff), and specific names of objects. 
In science museums, participants also wanted to know whether exhibits are touchable. 
In shopping malls, participants also wanted to learn the store menus and whether there is a spacious area for a guide dog to rest while the user is eating.
However, three other participants (P02, P03, and P09) found certain information, such as details about lighting, surrounding people, and wall design, unnecessary. 

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figure/examples.png}
    \caption{Examples of descriptions described in the formative study. Panel A shows an example of a description generated at the science museum, and Panel B shows the one generated at a shopping mall.}
    \Description{Examples of descriptions described in the formative study. Panel A shows an example of a description generated at the science museum, saying "This is a futuristic exhibition hall that has vibrant displays. To your left, there is a uniquely shaped wooden table and archway. Ahead, you can see a curved blue sofa and a white sign that reads "Entrance." On your right, large colorful panels line the wall, displaying information about the future and health." and Panel B shows the one generated at a shopping mall, saying "This is a bright, modern corner of a commercial facility. On the left side, there are tall-backed chairs made of black metal lined up, and beyond them, round tables are arranged. Ahead, a man in a suit is standing, and in the background, there's an electronic menu board, suggesting the presence of a restaurant. To the right, there's an eatery enclosed by warm-colored walls in shades of red and orange, with many metallic chairs and tables, and menu boards are set up."}
    \label{fig:study1example}
\end{figure*}

\subsection{Design Considerations}
The results of the study affirmed that there are certain appreciations and room for improvement for the exploration robot for blind people.
Based on the above results, we derived several requirements for the system, as listed below.

\subsubsection{Vary Detail of Descriptions Based on Preferences and Contexts}
\label{sec:implication_varydetail}
We observed three types of preferences: one that enjoyed all the descriptions provided by the system (\textit{Exploration-Inclined}), another that enjoyed the descriptions but preferred to limit certain information (\textit{Intermediate}), and a third group that only wanted information useful for determining where to go (\textit{Destination-Oriented}). 
In Tab.~\ref{tab:demographics1}, we show the description preference of each participant.
To classify the preferences, we first classified three participants who did not enjoy the description of the system as \textit{Destination-Oriented}.
Then, based on the discussion between the authors, we classified the rest as \textit{Intermediate} or \textit{Exploration-Inclined}.
Furthermore, the type of information needed varied slightly depending on the experimental location. 
For instance, participants sought seating information for guide dogs in shopping areas, whereas in the science museum, they were more interested in whether the exhibits were touchable.
Given these three types of preferences and context-dependent information needs, we modified the system so that it could adjust the amount and types of information conveyed to each participant.

\subsubsection{Add Question and Answer Functionality}
\label{sec:implication_Q&A}
There was a clear need for question-and-answer (Q\&A) interaction, as seven participants (P02--P05 and P08--P10) noted that they would like the option to ask more detailed questions through conversation. 
Participants expressed interest in this functionality when they were curious about the system's descriptions. This would allow them to ask more detailed questions about the objects of interest.

\subsubsection{Add ``Take-Me-There'' Functionality} 
\label{sec:implication_takemethere}
Four participants (P02, P04, P06, and P10) mentioned that they would like to revisit locations they found interesting after walking around the floor. 
Example situations include deciding to visit a shop, engaging with touchable exhibits, or returning to chairs discovered during the exploration. 
In unfamiliar locations, where users may lose their sense of direction, participants also expressed the need for a feature that guides them back to their initial location~\cite{kuribayashi2023pathfinder}.

\subsubsection{Vary Speed and Be Able to Stop the Robot}
\label{sec:implication_speed}
While the majority found the default speed appropriate for listening and understanding the described information, there were requests for customizable speed settings. 
Eight participants stated that the robot's speed was appropriate for exploring. 
Two participants (P04 and P06) expressed a preference for a faster speed.
P01 additionally wanted to stop when the robot read out the descriptions of interest.
In conclusion, users who are \textit{Destination-Oriented} or have already determined the destination through exploration may want to increase the speed, while users who prefer to take time exploring might wish to slow down or stop the robot entirely. 

\subsubsection{Add Direction Specifying Functionality}
\label{sec:implication_directionspecification}
Participants expressed a desire for more active engagement by specifying the movement direction themselves. 
Four participants (P02--P05) mentioned that they wanted more active control over the movement direction based on their interests.
Additionally, we extrapolated that instead of simply following the robot, some users may prefer to interactively choose the direction based on the audio description of the surroundings.
This could lead to greater autonomy because it would enrich the exploratory experience by aligning the robot's movement with the users' real-time curiosity and needs, creating a more personalized and engaging exploration experience.
