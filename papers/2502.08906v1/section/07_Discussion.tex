\section{Discussion}
\subsection{Experience of Using WanderGuide}
WanderGuide provided participants with the experience of exploring unfamiliar indoor environments without a specific destination in their minds, mimicking the spontaneous wandering experience of sighted people (C\ref{P12IWantThis}). 
Participants expressed a sense of confidence when using the system, noting that it allowed them to navigate independently without relying on traditional tools like white canes (C\ref{P12IWantThis}). 
As described by C\ref{P15Insufficient} and the ratings of 4 from P11 and P13 to Q1 and Q2 in Tab.~\ref{tab:likert}, there still exists the limitation of being unable to describe specific information.
Thus, there is a need for further research on how to appropriately convey surrounding information to blind people.
Still, the system’s ability to deliver real-time descriptions of objects, walls, and spatial layouts enabled participants to form an imagination (C\ref{P12IWantThis}) of their surroundings, sparking their desire to use the system in familiar and unfamiliar environments (Tab.~\ref{tab:likert} Q5 and Q6).
In short, WanderGuide has the potential to provide users with an experience similar to that of navigating with sighted assistants to explore the environment, but the users can explore independently.
We believe this research opens a new frontier to the concept of \textit{map-less exploration} guide system for blind people.

\red{
\subsection{Scene Description by MLLM}
\label{sec:scene_description}
Our survey in Sec.~\ref{sec:quality_eval} revealed that descriptions by MLLM were rated high for their naturalness and suitability for general image description but were not for actual descriptions to be provided to blind people by sighted experts.
This may be because the style and content of the generated descriptions differ from those typically provided to blind people during live interactions. 
For example, museum guides often focus on explaining notable objects or visible exhibits, complementing their descriptions with additional knowledge about the exhibits.
In contrast, the generated description often lacked concrete explanation about exhibits and shops, such as their names (C\ref{P01NeedMoreConcreteInformation}, C\ref{P09NegativeImpression}, and C\ref{P15Insufficient}).
This problem may be more prominent because the study was conducted in a science museum, where each exhibit contains detailed information that is not visually apparent but needs to be explained. 
On the other hand, from participant feedback, participants noted that MLLM-generated descriptions are comprehensive (C\ref{P07Enjoy}, C\ref{P10VariousAndDifferentInformation}, C\ref{P12IHaveBeen}, and C\ref{P14ThereHaveBeenNoSystem}), and provide them with enjoyment (C\ref{P07Enjoy}) and imagination of vision perception (C\ref{P10VariousAndDifferentInformation}).
They noted that MLLM provided them with information that they usually do not get from sighted assistants, leading to new discoveries (C\ref{P12IHaveBeen}). The descriptions provided by MLLM additionally allow blind people to tune in without hesitation and the need to rely on sighted people (C\ref{P14ThereHaveBeenNoSystem}).
These results indicate that evaluation from sighted experts may be stricter than that from blind people.
Nonetheless, these results suggest that MLLM for blind people's exploration could be further enhanced by providing more specific information about surrounding shops or exhibits, potentially inferring details when necessary. 
}

\subsection{Personal Preferences}
The studies revealed distinct preferences among participants regarding the levels of detail in the descriptions (Sec.~\ref{sec:Implementation_description_mode}) and interaction modes (Sec.~\ref{sec:implementation_button}). 
From the formative study, participants were divided into three preference groups, highlighting users' diverse information needs regarding exploration and goal-oriented navigation (Sec.~\ref{sec:implication_varydetail}).
Differences in preferences were mainly attributed to personality traits, because participants who were ``Destination-Oriented'' (Tab.~\ref{tab:demographics1}), or were mostly concerned with reaching destinations, mentioned they did not enjoy the detailed explanation of the system and preferred short, concise information. 
For example, one early blinded participant mentioned that exploration did not interest him, as he had barely done it in his daily life (C\ref{P02Conditioned}). 
On the other hand, some participants enjoyed imagining the scenes conveyed by the system. 
Congenital users commented that the descriptions felt as if they were actually seeing the surroundings, while acquired users likened it to their recalled experiences when they could still see. 
Interestingly, those who particularly enjoyed the system and were ``Exploration-Inclined'' were all female, while the Intermediate group, who enjoyed exploration but wanted more control over the information provided, consisted mainly of male participants.
We note that ``Destination-Oriented'' users expressed dissatisfaction with the system because they felt the scene description capabilities of the MLLM did not meet their expectations for exploration. Therefore, if the system was improved and was able to convey more concrete information, they might express different opinions. 

In the main study, further differences regarding how users interacted with the system were observed. 
Firstly, we observed that participants adjusted the system's levels of description, demonstrating our design aligns with their needs, which were based on three types of preferences identified in the formative study (Sec~\ref{sec:description_level_analysis}). 
The variation in the portions used for each mode further underscores the need for configurable descriptions. 
Also, how they used the conversation mode varied. 
Three participants frequently asked questions to the system to gather information about their surroundings (Sec.~\ref{sec:activity_breakdown}), while P15 preferred having more manual control over the robot’s navigation. 
Meanwhile, P12 favored the auto mode, where the robot guided them with minimal intervention. 
These observations highlight the need to consider customizing to various dimensions of personal preference, from description details to user autonomy, for future development.


\subsection{Design Implications and Future Development Directions} 
\label{sec:design_implication}
Two key design implications were observed in our studies. 
First, allowing the users to control the level of detail in the scene descriptions emerged as one of the most important design requirements. 
The system may benefit from further \textit{personalization} by users verbally describing their personal information needs as in previous research~\cite{Kaniwa2024ChitChatGuide}. 
Second, participants expressed the need for audio-based recognition capabilities, especially in environments where sound is an integral part of the experience, such as museums (C\ref{P13Sound}). 
The ability to answer questions about sounds and potentially guide users to the sounds' sources would enhance their exploration experience.

On the development side, the primary challenge encountered throughout the two studies was the system’s inability to provide detailed information that participants required, particularly regarding the identification of POI-related objects, as described in \red{Sec.~\ref{sec:scene_description}.}
We attempted to address this by upgrading the robot's hardware, \ie, adding a 1080p resolution fisheye camera to a much higher position. 
Still, participants found the descriptions lacking in detail and conveyed information somewhat vague, as partially shown by the ratings of 4 from P11 and P13 to Q1 Tab.~\ref{tab:likert}. 
We deduce that this was because the captured images sometimes did not contain useful information, such as the names of certain objects, or because the MLLM failed to accurately identify the useful information.
As a possible improvement, the robot could utilize history images by selecting the image with the best view to generate descriptions. 
Also, the robot could utilize, other modalities, such as colored point clouds by fusing camera images with the LiDAR sensor to provide three-dimentional sensor details to MLLM~\cite{liu2024uni3d,xu2023pointllm}. 
In conclusion, the MLLM module is the bottleneck of our system's technological development.
Similar system development efforts in the future should allocate the most resources to tackling this technological challenge.
Still, the issue may be gradually solved as MLLM is the current core area actively developed by researchers.

Another significant challenge in development we encountered is the challenge of running map-less navigation algorithms in diverse novel environments, which requires extensive development. 
Incorporating vision modalities~\cite{chang2024goat}, which we did not use in this study, could potentially enhance the robot's navigation capabilities. 
Achieving this, however, demands human-level object and layout recognition and real-time processing speed, where further research is required.


\subsection{Limitation and Future Work}
We were unable to examine user preferences over the long term, as participants in our study interacted with the system only for a short duration (20-40 minutes in the formative study and 70 minutes in the main study). 
Only a small portion of the reliance on concise descriptions may be due to the study's design limiting participants' time to explore.
The time constraints may have led users to act on the cost-effective information acquisition.
However, if the system is used regularly, users may encounter more situations where they prefer to use the concise mode, as indicated by C\ref{P12NormalMode}. 
Also, their preferences might change as they become more adept at utilizing it as a tool to query information, which the MLLM is particularly proficient at.
Thus, future research should investigate the effects of long-term use of the system.

We conducted two studies in two indoor locations. 
To capture more diverse needs, future studies should also explore the system’s performance in more diverse environments. 
This may reveal various additional information needs. 
The usage of the wheeled robot, while beneficial in guiding blind users because it is silent~\cite {wang2022can}, remains a constraint when navigating stairs or uneven terrain. 
This limitation, however, could be alleviated through user collaboration, such as assisting the robot in getting onto elevators or slightly lifting the robot over small steps.
Thus, future research should investigate the system devices's capabilities in different environments, as well as how these robots can address physical limitations by interacting with users.
\red{
Finally, for the main study, we were unable to conduct it in crowd environments with bystanders potentially obstructing the cameras, because the primary study was conducted in the science museum outside of regular operational hours. 
Handling crowded environments with robots, even when prebuilt maps are used, remains a significant challenge in the field of robotics~\cite{wang2022group}. 
Therefore, in future work, we aim to address the usability limitations of our system in such scenarios by integrating novel algorithms designed to manage crowded environments~\cite{wang2022group}.
}

The MLLM often made mistakes or referred to non-existent objects, with these errors being particularly noticeable in its responses within the Q\&A functionality (Sec.~\ref{sec:error_analysis}). 
The most common misrecognitions involved either partially reading the text or confusing objects with similar-looking ones. 
However, the performance of the MLLM is not the primary focus of our research. 
To ensure users receive the most accurate information possible, we will continue updating the MLLM used in the system.
\red{Also, some of the image inputs provided to the MLLM may have been affected by motion blur, potentially leading to a degradation in the quality of the generated descriptions. 
This issue could be addressed by using cameras that are more resistant to motion blur or by implementing algorithms that detect motion blur and select alternative frames for processing.}

Recruitment was conducted through our institution's email list, which includes many participants from previous studies. 
We acknowledge that these participants may have exhibited a positive bias toward our study, as they had expectations regarding the development of the robot system.
\red{
Furthermore, we obtained valuable insights from five participants, and involving more participants might have provided additional perspectives. 
% However, the number of participants in each study, which was five, was informed by a previous study~\cite{nielsen1993mathematical} that models the relationship between the number of participants and the percentage of usability problems identified. 
% This study found that, in many cases, five participants would identify more than 75\% of usability problems. 
Given the difficulty of recruiting many blind participants, we chose to iterate the study with five participants in each study, rather than conducting a single study with a larger group.
}
