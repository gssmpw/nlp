\section{Related Work}

\noindent \textbf{Low-Rank Adaptation} (LoRA) \cite{hu2022lora} reduces trainable parameters by learning low-rank matrices that bypass full-weight updates, minimizing memory usage for gradients and optimizers. 
Different from adapter-based methods \cite{he2021towards, pfeiffer2020adapterfusion, lin2020exploring, liao2023make, liao2023parameter}, LoRA incurs no inference overhead as the low-rank updates are merged with the frozen weights. However, scaling LoRA to larger models and more complex tasks remains challenging.
Recent improvements, including AdaLoRA \cite{zhang2303adaptive}, VeRA \cite{kopiczko2023vera}, QLoRA \cite{dettmers2024qlora} and DoRA \cite{liu2024dora}, optimize parameter allocation and weight decomposition but still face scalability challenges on larger models. 
%Our proposed SSH overcomes these limitations by selectively learning the most informative spectral components using DHT and its inverse, reducing parameters, enhancing memory efficiency, and scaling effectively across uni-modal and multi-modal tasks.


\noindent \textbf{Frequency-based Spectrum Learning} has been used to reduce trainable parameters while preserving model capacity. Prior works~\cite{xu2020learning,tang2022rethinking,yang2016exact} showed the effectiveness of compact and sparse spectral representation learning. Gao et al.~\cite{gao2024parameter} applied the Fourier Transform to fine-tune a subset of spectral coefficients, highlighting the potential of sparse spectrum adaptation in large foundation models. However, the DFT introduces complex operations, and the asymmetry between the DFT and its inverse increases computational overhead.
SSH addresses these issues with the real-valued DHT, which eliminates complex arithmetic, reduces computational complexity, and enhances numerical stability through symmetric transforms. Additionally, SSH’s energy-based sparse selection further decreases trainable parameters, improving efficiency and scalability.




\noindent \textbf{DHT} has shown potential in deep learning for model compression and computational efficiency. For example, \cite{rani2024content} employed DHT in medical image retrieval, \cite{ma2021high} used it in single-pixel imaging for efficient data acquisition, and \cite{coutinho2021low} leveraged it for media image compression and recovery. These works highlight DHT’s ability to reduce parameters while maintaining performance. 
% Our work extends DHT to language models and multi-modal tasks, utilizing its efficiency for compact and parameter-efficient fine-tuning across diverse domains.
