
\begin{algorithm}[t!]
\footnotesize
\caption{\textbf{SSH} Algorithm}
\label{alg:SSH}
\textbf{Input}: Input tensor $x$, number of parameters $n$, scaling factor $\alpha$, input dimension $d_1$, output dimension $d_2$, energy ratio $\delta$, pretrained layer weights $W$\\
\textbf{Output}: Transformed tensor $h$
\begin{algorithmic}[1]
\STATE \textbf{Initialization:}
\STATE $W_{F} = \text{DHT}(base\_layer.weight)$  \textcolor{blue}{//DHT for weights, Eq(\ref{eq2})}
\STATE  \textcolor{blue}{// Select top-($n \times \delta $ ) frequencies by energy}
\STATE $n_{\text{select}} = n \times \delta $ $\leftarrow$ RankTopEnergyFreq($\mathcal{M}$)
\STATE \textcolor{blue}{// Randomly select the rest of frequencies}
\STATE $n_{\text{random}} = n \times (1-\delta) $ $\leftarrow$ RandomSelectFreq($\mathcal{M}$)

\STATE \textcolor{blue}{// Initialize spectral coefficients}
\STATE $\Delta \mathbf{H}$ $\leftarrow$ KaimingInitial()

\STATE \textbf{Forward Pass:}
\STATE \textcolor{blue}{// Set $n$ selected freq. trainable and froze ($d_1 \times d_2 - n$).}
\STATE Set $n$ frequencies $\leftarrow$ requireGrad(True)
\STATE \textcolor{blue}{// Compute $\Delta W_T$ using inverse DHT based on Eq(\ref{eq3})}
\STATE $\Delta W_T \leftarrow \text{DHT}^{-1}(\Delta \mathbf{H}) \times \alpha$
\STATE \textcolor{blue}{// Merge $\Delta W$ with base layer output} 
\STATE $h \leftarrow W + \Delta W_T$
\STATE \textbf{return} $h$
\end{algorithmic}
\end{algorithm}



\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{data/SSH.pdf}
    \caption{\small 
    Overview of \textbf{S}parse \textbf{S}pectrum Adaptation via Discrete \textbf{H}artley Transform (SSH). First, the discrete Hartley transform (DHT) is applied to the pretrained weights to extract and retain the most important frequency components. Then, a selective process identifies specific spectral coefficients to be learned as trainable parameters, which are organized into a spectral matrix. Finally, the modified spectral matrix is transformed back to the spatial domain through the symmetric application of the inverse discrete Hartley transform (iDHT), ensuring accurate reconstruction and efficient updates to the model's weights.
    }
    \label{fig:SSH}
\end{figure*}

\section{Methodology}

We propose SSH, a novel frequency selection strategy based on the discrete Hartley transform for PEFT, as illustrated in Fig.~\ref{fig:SSH}.  
It operates in the Hartley spectral domain and learns a set of spectral coefficients. 
Specifically, we employ the discrete Hartley transform to the pretrained weights, using \textit{energy-based frequency selection} to identify the most informative frequencies. In addition, we incorporate random sampling to ensure diversity in the selected frequencies while maintaining the representation ability.

The SSH algorithm~\ref{alg:SSH} selectively fine-tunes a pretrained layer's weights using the Discrete Hartley Transform (DHT) to capture the most significant frequency components. First, the layer's weights are transformed into the frequency domain using DHT, and the top frequencies, based on energy, are selected for fine-tuning, while the remaining frequencies are randomly chosen and kept frozen. The trainable spectral coefficients are initialized using Kaiming initialization, and during backpropagation, only the selected frequencies have their gradients updated. The inverse DHT is then applied to these updated spectral coefficients, scaled by a factor $\alpha$, to obtain the transformed weights in the spatial domain. These updates are merged with the original pretrained weights, resulting in the final transformed tensor. This approach ensures that only the most informative frequency components are fine-tuned, significantly reducing the number of trainable parameters while maintaining model performance.





\subsection{Sparse Hartley Spectral Learning}

Let $\mathbf{W}_0 \in \mathbb{R}^{d_1 \times d_2}$ denote the pretrained weight matrix, and $\Delta \mathbf{W} \in \mathbb{R}^{d_1 \times d_2}$ denote the weight change during fine-tuning. LoRA~\cite{hu2022lora,liu2024dora,gao2024parameter} models the weight change by low-rank decomposition, represented as $BA$, where $B \in \mathbb{R}^{d_1 \times r}$ and $A \in \mathbb{R}^{r \times d_2}$. The fine-tuned weight matrix $\mathbf{W}$ is expressed as:

\begin{small}
\begin{equation}
    \mathbf{W} = \mathbf{W}_0 + BA.
\end{equation}    
\end{small}


Rather than update the weights directly in the spatial domain, we project pretrained weights into the spectral domain using the 2D discrete Hartley transform, which helps us to select the most informative frequencies. 
DHT is an orthogonal transform similar to the discrete Fourier transform but operates solely on real-valued inputs, making it computationally efficient and suitable for fine-tuning. Given a weight matrix $\mathbf{W}_0 \in \mathbb{R}^{d_1 \times d_2}$, 
its frequency counterpart $\mathbf{H}_0$ after the 2D DHT is defined as:

\begin{small}
\begin{equation}
\label{eq2}
\begin{split}
    \mathbf{H}_0(u, v) &= \sum_{x=0}^{d_1-1} \sum_{y=0}^{d_2-1} \mathbf{W}_0(x, y) \times \\
    &\left[ \cos\left( \frac{2\pi ux}{d_1} + \frac{2\pi vy}{d_2} \right) - \sin\left( \frac{2\pi ux}{d_1} + \frac{2\pi vy}{d_2} \right) \right],
\end{split}
\end{equation}
\end{small}
where $u \in [0, d_1-1]$ and $v \in [0, d_2-1]$ represent the Hartley indices.

As demonstrated in Fig.~\ref{fig:profile}, we analyze the key and value matrices of the RoBERTa-base model before and after applying the discrete Hartley transform, which effectively compresses the weight matrix into a compact spectral form. Similar trends are observed across models such as ViT, LLaMA, and VL-BART. To optimize parameter efficiency and reduce computational complexity, we selectively update only $n$ Hartley coefficients. This selection is driven by an energy-based method to capture the most informative coefficients, complemented by random initialization for diversity.

The energy of each Hartley component $E(u,v)$ is calculated as:

\begin{small}
\begin{equation}
    E(u,v) = \left| \mathbf{H}(u,v) \right|^2,
\end{equation}
\end{small}
where $\mathbf{H}(u,v)$ is the DHT coefficient at $(u,v)$. The top $n_{\text{energy}} = \lfloor \delta \cdot n \rfloor$ components with the highest energy are selected, while the rest are randomly chosen to ensure diversity.

    
\begin{small}
\begin{equation}
    \Delta \mathbf{H}_{\text{select}} = \Delta \mathbf{H}_{n_{\text{energy}}} + \Delta \mathbf{H}_{n_{\text{random}}}.
\end{equation}
\end{small}

After updating $\Delta \mathbf{H}$, the inverse DHT is applied to project the updates back into the spatial domain:

\begin{small}
\begin{equation}
\label{eq3}
    \mathbf{W} = \mathbf{W}_0 + \text{DHT}^{-1}(\Delta \mathbf{H}).
\end{equation}    
\end{small}


During the backward pass, the gradient with respect to the learnable Hartley coefficients is computed by projecting the spatial gradient $\frac{\partial \mathcal{L}}{\partial \mathbf{W}}$ into the spectral domain. To ensure that only the selected $n$ coefficients are updated, an indicator mask $\mathbf{M} \in \{0,1\}^{d_1 \times d_2}$ is employed to the gradient:

\begin{small}
\begin{equation}
    \frac{\partial \mathcal{L}}{\partial \Delta \mathbf{H}} = \mathbf{M} \circ \text{DHT} \left( \frac{\partial \mathcal{L}}{\partial \mathbf{W}} \right),
\end{equation}    
\end{small}
where $\circ$ denotes the element-wise multiplication. 

Finally, the selected $n$ Hartley coefficients are updated using the gradient descent:

\begin{small}
\begin{equation}
    \Delta \mathbf{H} \leftarrow \Delta \mathbf{H} - \eta \left( \mathbf{M} \circ \frac{\partial \mathcal{L}}{\partial \Delta \mathbf{H}} \right),
\end{equation}
\end{small}
where $\eta$ is the learning rate. This ensures that only the learnable Hartley coefficients are modified, maintaining sparsity.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{data/profile.pdf}
    \caption{\small 
    Visual representation of the RoBERTa attention mechanism's key and value matrices before and after discrete Hartley transform (DHT). (a)(b) show the original weight distributions of the key and value matrices, respectively. (d)(e) depict the transformed DHT values, demonstrating effective spectral compression. Heatmaps (c)(f) illustrate the output weights before and after DHT, highlighting the achieved sparsity and efficient representation.}
    \label{fig:profile}
\end{figure}







\subsection{Parameter Efficiency Analysis}

\begin{table}[!t]
\centering
\resizebox{0.47\textwidth}{!}{%
\begin{tabular}{l|ccc|cccc}
\toprule
\textbf{Base Models} & \multicolumn{3}{c|}{\textbf{LoRA}} & \multicolumn{3}{c}{\textbf{SSH}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
 & \textbf{r} & \textbf{\# Tr. Para.} & \textbf{Req. Bytes} & \textbf{n} & \textbf{\# Tr. Para.} & \textbf{Req. Bytes} \\
\midrule
\multirow{2}{*}{\textbf{RoBERTa Base}} & 4 & 147K & 574KB & 200 & 4.8K & 18.8KB \\
 & 8 & 295K & 1.13MB & 200 & 24K & 94KB \\
\midrule
\multirow{2}{*}{\textbf{RoBERTa Large}} & 4 & 393K & 1.5MB & 200 & 9.6K & 36.5KB \\
 & 8 & 786K & 3MB & 750 & 36.0K & 131.6KB \\
\midrule
\multirow{2}{*}{\textbf{GPT-2 Medium}} & 4 & 400K & 1.34MB & 375 & 18.1K & 65.8KB \\
 & 8 & 786K & 3MB & 750 & 36.0K & 131.6KB\\
\midrule
\multirow{2}{*}{\textbf{GPT-2 Large}} & 4 & 737K & 2.81MB & 375 & 18.1K & 105.8KB\\
 & 8 & 1.47M & 5.74MB & 750 & 36.0K & 211.5KB \\
\midrule
\multirow{2}{*}{\textbf{LLaMA-2 7B}} & 16 & 8.39M & 32.8MB & 750 & 48.0K & 187KB \\
 & 64 & 33.5M & 131.1MB & 1500 & 96.0K & 375KB \\
\midrule
\multirow{2}{*}{\textbf{LLaMA-2 13B}} & 16 & 13.1M & 51.2MB & 750 & 60K & 234KB \\
 & 64 & 52.4M & 204.8MB & 1500 & 120K & 469KB \\
 \midrule
\multirow{2}{*}{\textbf{LLaMA-3.1 8B}} & 16 & 13.1M & 51.2MB & 750 & 53.7K & 209KB \\
 & 64 & 52.4M & 204.8MB & 1500 & 107.5K & 420.1KB \\
\midrule
\multirow{2}{*}{\textbf{ViT Base}} & 8 & 295K & 1.13MB & 2250 & 54K & 210.7KB \\
 & 16 & 590K & 2.25MB & 7500 & 179.2K & 700.5KB \\
\midrule
\multirow{2}{*}{\textbf{ViT Large}} & 8 & 786K & 2.93MB & 2250 & 108K & 422.3KB \\
 & 16 & 1.57M & 6MB & 7500 & 350K & 1.38MB \\
\bottomrule
\end{tabular}%
}
\caption{Comparison of trainable parameters and required bytes between LoRA and SSH on different base models. SSH offers a substantial reduction in both trainable parameters and memory usage.}
\label{tab:nlu}
\end{table}

We evaluate the parameter efficiency and memory requirement of SSH in comparison to LoRA across several base models. The number of trainable parameters in SSH is given by \( |\Theta| = n \times L \), where \( n \) represents the number of selected frequencies, and \( L \) is the number of layers being fine-tuned. 
For LoRA, the parameter count is calculated as \( |\Theta| = r \times (d_1 + d_2) \times L \), where \( d_1 \) and \( d_2 \) are the dimensions of each layer, and \( r \) is the rank used for LoRA's low-rank decomposition.


Tab.~\ref{tab:nlu} compares SSH and LoRA in terms of trainable parameters and memory usage. SSH consistently requires fewer parameters and significantly less memory than LoRA. For instance, in the RoBERTa Base model, SSH with \( n = 200 \) uses only 4.8K parameters and 18.8KB of memory, while LoRA with \( r = 4 \) requires 147K parameters and 574KB of memory. This trend continues with larger models, such as LLaMA-2 13B, where SSH uses 60K parameters compared to LoRA's 13.1M parameters.


The memory efficiency of SSH becomes even more pronounced in larger models like ViT Large. For \( n = 2250 \), SSH requires just 108K parameters (422.3KB), while LoRA with \( r = 8 \) needs 786K parameters (2.93MB). This considerable reduction in both the parameter number and memory footprint highlights the scalability and efficiency of SSH, making it especially suitable for resource-constrained environments.

