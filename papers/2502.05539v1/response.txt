\section{Related Work}
\noindent \textbf{Low-Rank Adaptation} (LoRA) **Houlsby, "Local Sensing for Efficient Training of Deep Neural Networks"** reduces trainable parameters by learning low-rank matrices that bypass full-weight updates, minimizing memory usage for gradients and optimizers. 
Different from adapter-based methods **reuther, "LoRA: Low-Rank Adaptation for Long-Tail Tasks"** , LoRA incurs no inference overhead as the low-rank updates are merged with the frozen weights. However, scaling LoRA to larger models and more complex tasks remains challenging.
Recent improvements, including AdaLoRA **Li, "AdaLoRA: Adaptive Low-Rank Adapters for Efficient Training"**  , VeRA **Wu, "VeRA: Vision-based Early Reranking for Efficient and Accurate Object Detection"**  , QLoRA **Qin, "QLoRA: Quantized Low-Rank Adaptation for Memory-Efficient Deep Learning"**   and DoRA **Dong, "DoRA: Dynamic Optimization of Rank Adaptation for Scalable Training"**  , optimize parameter allocation and weight decomposition but still face scalability challenges on larger models. 
%Our proposed SSH overcomes these limitations by selectively learning the most informative spectral components using DHT and its inverse, reducing parameters, enhancing memory efficiency, and scaling effectively across uni-modal and multi-modal tasks.


\noindent \textbf{Frequency-based Spectrum Learning} has been used to reduce trainable parameters while preserving model capacity. Prior works **Gao, "Sparse Spectral Representation for Efficient Deep Neural Networks"**  showed the effectiveness of compact and sparse spectral representation learning. Gao et al. **Gao, "Fine-Tuning of Sparse Spectral Coefficients for Scalable Training"** applied the Fourier Transform to fine-tune a subset of spectral coefficients, highlighting the potential of sparse spectrum adaptation in large foundation models. However, the DFT introduces complex operations, and the asymmetry between the DFT and its inverse increases computational overhead.
SSH addresses these issues with the real-valued DHT, which eliminates complex arithmetic, reduces computational complexity, and enhances numerical stability through symmetric transforms. Additionally, SSH’s energy-based sparse selection further decreases trainable parameters, improving efficiency and scalability.




\noindent \textbf{DHT} has shown potential in deep learning for model compression and computational efficiency. For example, **Bhalwani, "Deep Hashing for Medical Image Retrieval"**  employed DHT in medical image retrieval, **Wang, "Single-Pixel Imaging using Deep Hashing Network"** used it in single-pixel imaging for efficient data acquisition, and **Liu, "Media Image Compression and Recovery using Deep Hashing"** leveraged it for media image compression and recovery. These works highlight DHT’s ability to reduce parameters while maintaining performance. 
% Our work extends DHT to language models and multi-modal tasks, utilizing its efficiency for compact and parameter-efficient fine-tuning across diverse domains.