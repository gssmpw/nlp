\section{Appendix}
\label{appendix}

In this supplementary material, we first provide detailed information about the datasets used in our experiments. Next, we outline the implementation specifics and hyper-parameter settings. We then present additional experimental results that further validate the effectiveness of the proposed SSH method. Finally, we include examples of instruction tuning to highlight the practical application of our approach SSH.

% \subsection{Algorithm}
% \begin{algorithm}[!ht]
% \footnotesize
% \caption{\textbf{SSH} Algorithm}
% \label{alg:SSH}
% \textbf{Input}: Input tensor $x$, number of parameters $n$, scaling factor $\alpha$, input dimension $d_1$, output dimension $d_2$, energy ratio $\delta$, pretrained layer weights $W$\\
% \textbf{Output}: Transformed tensor $h$
% \begin{algorithmic}[1]
% \STATE \textbf{Initialization:}
% \STATE $W_{F} = \text{DHT}(base\_layer.weight)$  \textcolor{blue}{//DHT for weights, Eq(\ref{eq2})}
% \STATE  \textcolor{blue}{// Select top-($n \times \delta $ ) frequencies by energy}
% \STATE $n_{\text{select}} = n \times \delta $ $\leftarrow$ RankTopEnergyFreq($\mathcal{M}$)
% \STATE \textcolor{blue}{// Randomly select the rest of frequencies}
% \STATE $n_{\text{random}} = n \times (1-\delta) $ $\leftarrow$ RandomSelectFreq($\mathcal{M}$)

% \STATE \textcolor{blue}{// Initialize spectral coefficients}
% \STATE $\Delta \mathbf{H}$ $\leftarrow$ KaimingInitial()

% \STATE \textbf{Forward Pass:}
% \STATE \textcolor{blue}{// Set $n$ selected freq. trainable and froze ($d_1 \times d_2 - n$).}
% \STATE Set $n$ frequencies $\leftarrow$ requireGrad(True)
% \STATE \textcolor{blue}{// Compute $\Delta W_T$ using inverse DHT based on Eq(\ref{eq3})}
% \STATE $\Delta W_T \leftarrow \text{DHT}^{-1}(\Delta \mathbf{H}) \times \alpha$
% \STATE \textcolor{blue}{// Merge $\Delta W$ with base layer output} 
% \STATE $h \leftarrow W + \Delta W_T$
% \STATE \textbf{return} $h$
% \end{algorithmic}
% \end{algorithm}

% The SSH algorithm selectively fine-tunes a pretrained layer's weights using the Discrete Hartley Transform (DHT) to capture the most significant frequency components. First, the layer's weights are transformed into the frequency domain using DHT, and the top frequencies, based on energy, are selected for fine-tuning, while the remaining frequencies are randomly chosen and kept frozen. The trainable spectral coefficients are initialized using Kaiming initialization, and during backpropagation, only the selected frequencies have their gradients updated. The inverse DHT is then applied to these updated spectral coefficients, scaled by a factor $\alpha$, to obtain the transformed weights in the spatial domain. These updates are merged with the original pretrained weights, resulting in the final transformed tensor. This approach ensures that only the most informative frequency components are fine-tuned, significantly reducing the number of trainable parameters while maintaining model performance.

\subsection{Details of Datasets}
\paragraph{GLUE Benchmark.} 
\label{gluebench}
The General Language Understanding Evaluation (GLUE) benchmark\cite{wang2019glue} is a widely used platform for evaluating and advancing progress in natural language understanding (NLU). It includes nine tasks covering various NLU challenges, such as sentiment analysis, paraphrase detection, linguistic acceptability, natural language inference, and textual similarity. Notable tasks include the Stanford Sentiment Treebank (SST-2), which focuses on binary sentiment classification of movie reviews, and the Microsoft Research Paraphrase Corpus (MRPC), which assesses whether two sentences are semantically equivalent. The Corpus of Linguistic Acceptability (CoLA) measures a model's ability to distinguish grammatically correct sentences from incorrect ones, reflecting syntactic judgment skills. By offering diverse tasks and including some with limited training data, the benchmark promotes the development of models that generalize well across different language tasks and genres.

In addition to single-sentence classification tasks, GLUE includes several sentence-pair tasks. The Question Natural Language Inference (QNLI) task, derived from the Stanford Question Answering Dataset (SQuAD), requires models to determine whether a given context sentence contains the answer to a corresponding question. The Recognizing Textual Entailment (RTE) task combines several textual entailment datasets from diverse domains, such as news and Wikipedia, to test if a hypothesis can be logically inferred from a premise. The Semantic Textual Similarity Benchmark (STS-B) measures the similarity between sentence pairs using a regression-based approach, where models predict similarity scores on a continuous scale.

\paragraph{E2E Benchmark.}
The E2E dataset~\cite{novikova2017e2e} is designed for training and evaluating end-to-end data-driven natural language generation (NLG) systems within the restaurant domain. Comprising over 50,000 instances, it is notable for its linguistic complexity, including greater lexical diversity, syntactic variation, and discourse phenomena compared to earlier datasets. Evaluation is primarily based on five metrics: BLEU, NIST, METEOR, ROUGE-L, and CIDEr. BLEU measures n-gram overlap between the generated text and human references, emphasizing precision. METEOR accounts for synonymy and stemming, providing a more nuanced assessment of similarity. ROUGE-L evaluates fluency and structure through the longest common subsequence. CIDEr, by weighting n-grams according to their relevance in human references, offers a comprehensive measure of output quality.

\paragraph{Instruction Tuning Related Benchmarks}
The Alpaca dataset~\cite{taori2023stanford} consists of 51K instruction-following examples generated using OpenAI's text-davinci-003. It was developed to fine-tune Meta's LLaMA 7B model into a lightweight, instruction-following model called Alpaca. The dataset spans a wide range of tasks, including question-answering, summarization, and classification, allowing the fine-tuned model to perform similarly to much larger models while being more cost-efficient. A specific example is shown below:


\begin{tcolorbox}[colback=green!5!white, colframe=black!75!, sharp corners, boxrule=1pt]
\footnotesize
\texttt{
\{\\
\hspace*{0.5cm} "instructions": Translate the sentence from English to Spanish.\\
\hspace*{0.5cm} "input": The weather is beautiful today.\\
\hspace*{0.5cm} "output": El clima está hermoso hoy.\\
\}
}
\end{tcolorbox}

MT-Bench~\cite{zheng2024judging} is a recently introduced benchmark designed to evaluate the instruction-following capabilities of language foundation models. It features a set of open-ended questions aimed at assessing model performance across a variety of domains, including writing, roleplay, reasoning, mathematics, coding, information extraction, STEM, and the humanities. MT-Bench effectively distinguishes these abilities through domain-specific questions, providing a more comprehensive evaluation of model performance. An example from the benchmark is shown below.

\begin{tcolorbox}[colback=green!5!white, colframe=black!75!, sharp corners, boxrule=1pt]
\footnotesize
\texttt{
\{\\
\hspace*{0.5cm} "Q1": How many days are there in a leap year?\\
\hspace*{0.5cm} "Q2": How many days are there in two consecutive leap years?\\
\hspace*{0.5cm} "Solution": Q1: There are 366 days in a leap year. Q2: There are 732 days in two consecutive leap years.\\
\}
}
\end{tcolorbox}

Vicuna Eval~\cite{chiang2023vicuna} is a benchmark designed to evaluate the alignment of large language models (LLMs) with human preferences and is the predecessor to MT-Bench. Vicuna Eval assesses models across a wide range of topics, including coding, writing, mathematics, counterfactual reasoning, Fermi estimation, common sense, roleplay, knowledge, and general tasks. It offers a comprehensive framework for gauging how well models meet human expectations across various scenarios. An example from this evaluation is shown below.


\begin{tcolorbox}[colback=green!5!white, colframe=black!75!, sharp corners, boxrule=1pt]
\small
\texttt{
\{\\
\hspace*{0.5cm} "question": Describe the difference between supervised and unsupervised learning.\\
\hspace*{0.5cm} "category": machine learning.\\
\}
}
\end{tcolorbox}

\paragraph{Image Classification Datasets.}
Tab.~\ref{tab-image} provides detailed information about four widely-used vision datasets: CIFAR100, DTD, EuroSAT, and OxfordPets. It outlines key statistics, including the number of training (\#Train), validation (\#Val), and test (\#Test) samples, as well as the number of classes (\#Class) in each dataset. These datasets cover a range of domains, from object recognition (CIFAR100~\cite{krizhevsky2009learning}) and texture classification (DTD~\cite{cimpoi2014describing}) to satellite image classification (EuroSAT~\cite{helber2019eurosat}) and pet identification (OxfordPets~\cite{parkhi2012cats}). This diversity ensures that models are tested on various visual tasks, providing a robust evaluation of their performance.

To maintain consistency in model evaluation, all datasets are rescaled to a resolution of 224 × 224. This standardized input size simplifies comparisons by ensuring that all models receive uniformly sized images, which is essential for fair benchmarking. The datasets vary significantly in terms of size and complexity, with CIFAR100 containing the largest number of samples (60,000) across 100 classes, while OxfordPets is more specialized, focusing on 37 classes. This variety highlights the unique challenges posed by each dataset, contributing to comprehensive model assessments.



\begin{table}
\centering
\resizebox{0.47\textwidth}{!}{%
\begin{tabular}{l|r|r|r|r|l}
\toprule
\textbf{Dataset} & \textbf{\#Train} & \textbf{\#Val} & \textbf{\#Test} & \textbf{\#Class} & \textbf{Rescaled res.} \\
\midrule
CIFAR100  & 45,000 & 5,000 & 10,000 & 100 & \multirow{4}{*}{224 $\times$ 224} \\
DTD  & 4,060 & 452 & 1,128 & 47 &  \\
EuroSAT & 16,200 & 5,400 & 5,400 & 10 &  \\
OxfordPets & 3,312 & 368 & 3,669 & 37 & \\
\bottomrule
\end{tabular}%
}
\caption{Details about the vision datasets.}
\label{tab-image}
\end{table}

\subsection{Hyperparamaters}


\begin{table*}
\centering
\resizebox{0.7\textwidth}{!}{%
\begin{tabular}{ll|ccccccc}
\toprule
\textbf{Model} & \textbf{Hyperparameter} & \textbf{STS-B} & \textbf{RTE} & \textbf{MRPC} & \textbf{CoLA} & \textbf{SST-2} & \textbf{QNLI} \\
\midrule
\multirow{7}{*}{\textbf{Both}} 
& Optimizer & \multicolumn{6}{c}{AdamW} \\
& LR Schedule & \multicolumn{6}{c}{Linear} \\
& Warmup Ratio & \multicolumn{6}{c}{0.06} \\
& Frequency Bias & \multicolumn{6}{c}{False} \\
& $n_{SSH}$ & \multicolumn{6}{c}{750} \\
& $n_{FourierFT}$ & \multicolumn{6}{c}{1000} \\
% & Seeds & \multicolumn{6}{c}{\{0, 11111, 22222, 33333, 44444\}} \\
\midrule
\multirow{7}{*}{\textbf{Base}} 
& Epochs & 60 & 90 & 30 & 100 & 40 & 40 \\
& Learning Rate (SSH) & 9E-2 & 9E-2 & 5E-2 & 1.2E-1 & 5E-2 & 1E-2 \\
& Learning Rate (FourierFT) & 9E-2 & 9E-2 & 5E-2 & 1.2E-1 & 5E-2 & 1E-2 \\
& Learning Rate (VeRA) & 9E-2 & 9E-2 & 5E-2 & 1.2E-1 & 5E-2 & 1E-2 \\
& Learning Rate (Head) & 9E-3 & 1.1E-2 & 6E-3 & 8E-3 & 6E-3 & 1E-3 \\
& Max Seq. Len & 512 & 512 & 512 & 512 & 512 & 512 \\
& Scaling Value & 84 & 110 & 141 & 49 & 140 & 29 \\
& Batch Size & 32 & 32 & 32 & 32 & 32 & 32 \\
\midrule
\multirow{7}{*}{\textbf{Large}} 
& Epochs & 30 & 60 & 30 & 80 & 10 & 30 \\
& Learning Rate (SSH) & 7E-2 & 8E-2 & 6E-2 & 4.3E-2 & 4.3E-2 & 6E-2 \\
& Learning Rate (FourierFT) & 7E-2 & 8E-2 & 6E-2 & 4.3E-2 & 4.3E-2 & 6E-2 \\
& Learning Rate (VeRA) & 7E-2 & 8E-2 & 6E-2 & 4.3E-2 & 4.3E-2 & 6E-2 \\
& Learning Rate (Head) & 1E-3 & 5E-3 & 1E-3 & 1.1E-2 & 1E-3 & 5E-3 \\
& Max Seq. Len & 512 & 512 & 512 & 256 & 128 & 512 \\
& Scaling Value & 121 & 90 & 120 & 90 & 69 & 69 \\
& Batch Size & 32 & 32 & 32 & 128 & 32 & 32 \\
\bottomrule
\end{tabular}%
}
\caption{\small Hyperparameters used for SSH across various GLUE tasks.}
\label{tab:nluh}
\end{table*}


\paragraph{Hyperparameters on GLUE Benchmarks.} Tab.~\ref{tab:nluh} summarizes the key hyperparameters used in experiments across various GLUE tasks for both Base and Large models. The table includes details on learning rate schedules, optimizer settings, warmup ratios, and seed values to ensure reproducibility. For both model sizes, the AdamW optimizer is employed with a linear learning rate schedule and a warmup ratio of 0.06. The frequency bias is set to false, and the frequency coefficient \( n \) is fixed at 750 for SSH unless otherwise specified. Each experiment is run using five different seeds: \{0, 11111, 22222, 33333, 44444\}.

For Base models, the number of training epochs varies between 30 and 100, depending on the task, with SST-2 requiring the longest training time. The FourierFT and SSH methods use a higher learning rate for base model training compared to the learning rate applied during fine-tuning of the head layers.

In contrast, Large models typically require fewer epochs but are trained with slightly lower learning rates. The batch size remains consistent across both model sizes, set at 32 for all tasks. Additionally, maximum sequence lengths are adjusted according to the task, with longer sequences allocated for more complex tasks like CoLA and QNLI.


\begin{table}
\centering
\resizebox{0.4\textwidth}{!}{%
\begin{tabular}{l|cc}
\toprule
\textbf{Hyperparameter} & \textbf{Medium} & \textbf{Large} \\
\midrule
Optimizer & \multicolumn{2}{c}{AdamW} \\
Learning Rate (SSH) & 2E-2 & 5E-2 \\
Learning Rate (FourierFT) & 2E-2 & 5E-2 \\
Learning Rate (VeRA) & 2E-2 & 5E-2 \\
Learning Rate (Head) & 2E-4 & 1E-4 \\
Batch Size & \multicolumn{2}{c}{128} \\
Weight Decay & 0.01 & 0.03 \\
$n_{SSH}$ &\multicolumn{2}{c}{750} \\
$n_{FourierFT}$ & \multicolumn{2}{c}{1000} \\
Scaling value $\alpha$ & \multicolumn{2}{c}{300} \\
Epochs & \multicolumn{2}{c}{5}  \\
Label Smooth & \multicolumn{2}{c}{0.1}  \\
LR Schedule & \multicolumn{2}{c}{Linear} \\
\bottomrule
\end{tabular}%
}
\caption{\small Hyperparameter settings on E2E benchmark}
\label{tab:nlgh}
\end{table}

\paragraph{Hyperparameter settings on E2E benchmark.} Tab.~\ref{tab:nlgh} details the hyperparameter configurations used for the medium and large models on the E2E benchmark. Both models are optimized using AdamW with a linear learning rate schedule. For the medium model, the learning rates for SSH and FourierFT are set to \(2E-2\), while for the large model, the learning rates are set to \(5E-2\). The head layers are fine-tuned with lower learning rates of \(2E-4\) for the medium model and \(1E-4\) for the large model. Both models employ a batch size of 128, with weight decay values of 0.01 for the medium model and 0.03 for the large model. The number of selected frequencies, \(n\), is set to 750 for SSH and 1000 for FourierFT, with the scaling factor \(\alpha\) fixed at 300 for both models. Additionally, label smoothing with a value of 0.1 is applied, and the models are trained for 5 epochs.



\begin{table}
\centering
\resizebox{0.45\textwidth}{!}{%
\begin{tabular}{l|cccc}
\toprule
\textbf{Hyperparameter} & \textbf{LoRA} & \textbf{FourierFT} & \textbf{SSH} & \textbf{VeRA}\\
\midrule
Optimizer & \multicolumn{4}{c}{AdamW} \\
Warmup Ratio & \multicolumn{4}{c}{0.06} \\
Batch Size & \multicolumn{4}{c}{4} \\
Acc. Steps & \multicolumn{4}{c}{4} \\
Epochs & \multicolumn{4}{c}{1} \\
$n$ &  -- & 1000 & 750 & --\\
Scaling Value $\alpha$ & 300.0 & 16.0 & 16.0 & 300.0\\
LR Schedule & \multicolumn{4}{c}{Linear} \\
Learning Rate & 3E-2 & 3E-3 & 3E-3 &  3E-3 \\
\bottomrule
\end{tabular}%
}
\caption{\small Hyperparameter settings for instruction-tuning configurations.}
\label{tab:hyperparamsIn}
\end{table}


\paragraph{Hyperparameter Setup for Instruction-Tuning.} Table~\ref{tab:hyperparamsIn} provides a summary of the key hyperparameters used for fine-tuning the LoRA, FourierFT, and SSH models. For all methods, the optimizer is AdamW, with a warmup ratio of 0.06. A batch size of 4 is used, along with gradient accumulation steps of 4 to ensure stability during training. The default training duration is 1 epoch, although in specific cases—such as the motivation example in the introduction and the ablation study in the supplementary material—2 epochs are used.

For SSH, the parameter \( n \) is set to 750. The scaling factor \( \alpha \) varies across methods: it is set to 300.0 for LoRA, 16.0 for FourierFT, and 16.0 for SSH. Learning rates are adjusted individually, with LoRA using 3E-2, while both FourierFT and SSH use a lower rate of 3E-3. All methods utilize a linear learning rate schedule.


\paragraph{Hyperparameter setup for image classification.}


\begin{table}
\centering
\resizebox{0.47\textwidth}{!}{%
\begin{tabular}{l|cccc}
\toprule
\textbf{Hyperparameter} & \textbf{CIFAR100} & \textbf{DTD} & \textbf{EuroSAT} &  \textbf{OxfordPets} \\
\midrule
Epochs & \multicolumn{4}{c}{10} \\
Optimizer & \multicolumn{4}{c}{AdamW} \\
LR Schedule & \multicolumn{4}{c}{Linear} \\
$n_{SSH}$ & \multicolumn{4}{c}{2250} \\
$n_{FourierFT}$ & \multicolumn{4}{c}{3000} \\
$\alpha$ & \multicolumn{4}{c}{300.0} \\
Learning Rate (SSH) & 2E-1 & 3E-1 & 2E-1 & 3E-1  \\
Learning Rate (FourierFT) & 2E-1 & 3E-1 & 2E-1 & 3E-1  \\
Learning Rate (VeRA) & 2E-1 & 3E-1 & 2E-1 & 3E-1  \\
Learning Rate (Head) & 7E-4 & 1E-3 & 8E-4 & 1E-3  \\
Weight Decay & 1E-4 & 7E-5 & 3E-4 & 8E-4 \\
\bottomrule
\end{tabular}%
}
\caption{\small Hyperparameter setup for image classification.}
\label{tab:SSH_image}
\end{table}

Tab.~\ref{tab:SSH_image} outlines the hyperparameter configurations used for fine-tuning on the CIFAR100, DTD, EuroSAT, and OxfordPets datasets for image classification tasks. The table provides the common settings applied across all datasets, including the use of the AdamW optimizer, a linear learning rate schedule, and a consistent training duration of 10 epochs. The number of frequency components ($n$) is set to 2250 for SSH and 3000 for FourierFT across all datasets.

For both SSH and FourierFT, the learning rate varies slightly depending on the dataset, ranging from 2E-1 to 3E-1, while the learning rate for the classification head lies between 7E-4 and 1E-3. The weight decay is also tuned per dataset, with values between 7E-5 and 1E-4 for DTD and CIFAR100, and slightly higher at 3E-4 and 8E-4 for EuroSAT and OxfordPets, respectively.




