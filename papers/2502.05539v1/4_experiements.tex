\section{Experiments}
SSH is compared against state-of-the-art parameter-efficient fine-tuning (PEFT) methods. The experiments are conducted across multiple domains, including single-modality tasks such as natural language understanding (NLU) and natural language generation (NLG), as well as instruction tuning, text summarization, and mathematical reasoning. Additionally, SSH is evaluated on multi-modality tasks, such as vision-language image classification. Finally, an ablation study is performed to assess the effectiveness of our approach.



\subsection{Baselines}
We compare SSH with the following baselines:
\begin{itemize}
    \item \textbf{Full Fine-Tuning (FF):} The entire model is fine-tuned, with updates to all parameters.
    \item \textbf{Adapter Tuning~\cite{houlsby2019parameter,lin2020exploring,ruckle2020adapterdrop,pfeiffer2020adapterfusion}:} Methods that introduce adapter layers between the self-attention and MLP modules for parameter-efficient tuning.
    \item \textbf{LoRA~\cite{hu2022lora}:} A method that estimates weight updates via low-rank matrices.
    \item \textbf{AdaLoRA~\cite{zhang2303adaptive}:} An extension of LoRA that dynamically reallocates the parameter budget based on importance scores.
    \item \textbf{DoRA~\cite{liu2024dora}:} Decomposes pretrained weights into magnitude and direction, using LoRA for efficient directional updates.
    \item \textbf{VeRA~\cite{kopiczko2023vera}:} Employs a single pair of low-rank matrices across all layers, to reduce parameters.
    \item \textbf{FourierFT~\cite{gao2024parameter}:} Fine-tunes models by learning a subset of spectral coefficients in the Fourier domain.
    \item 
    \textbf{AFLoRA~\cite{liu2024aflora}:} Freezes low-rank adaptation parameters using a learned freezing score, reducing trainable parameters while maintaining performance.
    \item 
    \textbf{LaMDA~\cite{azizi2024lamda}:} Fine-tunes large models via spectrally decomposed low-dimensional adaptation, reducing trainable parameters and memory usage while maintaining performance.
    
\end{itemize}

\subsection{Natural Language Understanding}

\begin{table*}[!ht]
\centering
\resizebox{0.85\textwidth}{!}{%
\begin{tabular}{cl|r|ccccccccc}
\toprule
& \textbf{Model} & \textbf{\# Trainable} & \textbf{SST-2} & \textbf{MRPC} & \textbf{CoLA} & \textbf{QNLI} & \textbf{RTE} & \textbf{STS-B} & \multirow{2}{*}{\textbf{Avg.}} \\
& \textbf{\& Method} & \textbf{Parameters} & \textbf{(Acc.)} & \textbf{(Acc.)} & \textbf{(MCC)} & \textbf{(Acc.)} & \textbf{(Acc.)} & \textbf{(PCC)} \\
\midrule
\multirow{9}{*}{\rotatebox{90}{\textbf{BASE}}} 
& FF & 125M & 94.8 & 90.2 & 63.6 & 92.8 & 78.7 & 91.2 & 85.22 \\
& BitFit & 0.1M & 93.7 & \textbf{92.7} & 62.0 & 91.8 & \textbf{81.5} & 90.8 & 85.42 \\
& Adpt\textsuperscript{D} & 0.9M & 94.7 & 88.4 & 62.6 & 93.0 & 75.9 & 90.3 & 84.15 \\
& LoRA & 0.3M & \textbf{95.1} & 89.7 & 63.4 & \textbf{93.3} & 78.4 & \textbf{91.5} & 85.23 \\
& AdaLoRA & 0.3M & 94.5 & 88.7 & 62.0 & 93.1 & 81.0 & 90.5 & 84.97 \\
& DoRA & 0.3M & 94.9 & 89.9 & 63.7 & \textbf{93.3} & 78.9 & \textbf{91.5} & 85.37 \\
& AFLoRA & 0.27M & 94.1 & 89.3 & 63.5 & 91.3 & 77.2 & 90.6 & 84.33 \\
& LaMDA & 0.06M &  94.6 & 89.7 & 64.9 & 91.7 & 78.2 & 90.4 & 84.92 \\
& VeRA & 0.043M & 94.6 & 89.5 & \textbf{65.6} & 91.8 & 78.7 & 90.7 & 85.15 \\
& FourierFT & 0.024M & 94.2 & 90.0 & 63.8 & 92.2 & 79.1 & 90.8 & 85.02 \\
\rowcolor{green!17}
& \textbf{SSH} & \textbf{0.018M} & 94.1 & 91.2 & 63.6 & 92.4 & 80.5 & 90.9 & \textbf{85.46} \\
\midrule
\multirow{8}{*}{\rotatebox{90}{\textbf{LARGE}}} 
& FF & 356M & 96.3 & 90.9 & 68.0 & 94.7 & 86.6 & 92.4 & 88.11 \\
& Adpt\textsuperscript{P} & 3M & 96.1 & 90.2 & \textbf{68.3} & 94.7 & 83.8 & 92.1 & 87.55 \\
& Adpt\textsuperscript{P} & 0.8M & \textbf{96.6} & 89.7 & 67.8 & 94.7 & 80.1 & 91.9 & 86.82 \\
& Adpt\textsuperscript{H} & 6M & 96.2 & 88.7 & 66.5 & 94.7 & 83.4 & 91.0 & 86.75 \\
& Adpt\textsuperscript{H} & 0.8M & 96.3 & 87.7 & 66.3 & 94.7 & 72.9 & 91.5 & 84.90 \\
& LoRA & 0.8M & 96.2 & 90.2 & 68.2 & \textbf{94.8} & 85.2 & 92.3 & 87.82 \\
& DoRA & 0.9M & 96.4 & \textbf{91.0} & 67.2 & \textbf{94.8} & 85.4 & 92.1 & 87.82 \\
& AFLoRA & 0.76M & 96.3 & 90.0 & 67.5 & 94.3 & 86.6 & 91.9 & 87.77 \\
& LaMDA & 0.093M &  96.2 & 90.1 & 68.1 & 94.5 & 87.3 & 92.0 & 88.03 \\
& VeRA & 0.061M & 96.1 & 90.9 & 68.0 & 94.4 & 85.9 & 91.7 & 87.83 \\
& FourierFT & 0.048M & 96.0 & 90.9 & 67.1 & 94.4 & \textbf{87.4} & 91.9 & 87.95 \\
\rowcolor{green!17}
& \textbf{SSH} & \textbf{0.036M} & 96.2 & 90.9 & 67.9 & 94.5 & \textbf{87.4} & \textbf{92.2} & \textbf{88.17} \\
\bottomrule
\end{tabular}%
}
\caption{\small Performance of various fine-tuning methods on GLUE benchmark, using base and large models. Metrics include MCC for CoLA, PCC for STS-B, and accuracy for other tasks. Results are medians of 5 runs with different seeds; the best scores in each category are bolded. SSH delivers the best average performance across tasks while using significantly fewer trainable parameters.}
\label{tab:nlup}
\end{table*}





\noindent \textbf{Models and Datasets.}  
We evaluate SSH on the GLUE benchmark~\cite{wang2019glue} using RoBERTa~\cite{liu2019roberta} in both Base and Large configurations. The GLUE benchmark comprises a diverse set of NLU tasks, offering a comprehensive evaluation framework.


\noindent \textbf{Implementation Details.}  
The SSH method uses 750 of the 768\textsuperscript{2} available spectral coefficients for RoBERTa Base and 1024\textsuperscript{2} for RoBERTa Large, ensuring that each layer retains the most important spectral components. This selection remains consistent across all layers. To ensure fair comparison, we follow the same experimental settings as LoRA and FourierFT. Additional hyperparameters and details are provided in Tab.~\ref{tab:nluh} in the appendix~\ref{gluebench}.


\noindent \textbf{Results and Analysis} 
The results in Table~\ref{tab:nlup} indicate that SSH consistently delivers competitive performance across diverse NLU tasks while maintaining a significantly lower number of trainable parameters. Notably, SSH achieves 80.5\% accuracy on RTE, 92.4\% on QNLI, and 90.9 on STS-B, demonstrating its capability to generalize effectively across multiple linguistic tasks.

SSH also maintains robust performance in sentiment classification, achieving 94.1\% accuracy on SST-2, which is on par with other parameter-efficient methods such as LoRA and BitFit. On CoLA, SSH attains a score of 63.6, matching FourierFT and outperforming Adpt\textsuperscript{D} and AdaLoRA. Additionally, SSH exhibits strong generalization on MRPC with 91.2\% accuracy and achieves a 90.9 Pearson correlation on STS-B, further reinforcing its effectiveness across textual similarity and entailment tasks. These findings highlight SSH as a highly efficient and scalable fine-tuning approach, capable of achieving state-of-the-art performance with minimal parameter overhead.




% These results highlight SSH's ability to retain critical information with minimal parameters, making it an effective and resource-efficient method for fine-tuning large-scale models, particularly in computationally constrained environments.


\begin{table}[!t]
\centering
\scalebox{0.63}{
\begin{tabular}{l|lr|crcccccc}
\toprule
 & \textbf{Method} & \textbf{\# Tr. Para.} & \textbf{BLEU} & \textbf{NIST} & \textbf{METE.} & \textbf{ROU-L} & \textbf{CIDEr} \\
\midrule
\multirow{9}{*}{\rotatebox{90}{\textbf{GPT-2 Medium}}} 
& FT\textsuperscript{1} & 354.92M & 68.2 & 8.62 & 46.2 & 71.0 & 2.47 \\
& Adpt\textsuperscript{L\textsuperscript{1}} & 0.37M & 66.3 & 8.41 & 45.0 & 69.8 & 2.40 \\
& Adpt\textsuperscript{L\textsuperscript{1}} & 11.09M & 68.9 & 8.71 & 46.1 & 71.3 & 2.47 \\
& Adpt\textsuperscript{H\textsuperscript{1}} & 11.09M & 67.3 & 8.50 & 46.0 & 70.7 & 2.44 \\
& LoRA & 0.35M & 68.9 & 8.76 & 46.6 & 71.5 & 2.51 \\
& DoRA & 0.36M & 69.2 & 8.79 & 46.9 & 71.7 & 2.52\\
& VeRA & 0.35M & \textbf{70.1} & 8.81 & 46.6 & 71.5 & 2.50 \\
& FourierFT & 0.048M & 69.1 & \textbf{8.82} & 47.0 & 71.8 & 2.51 \\
\rowcolor{green!17}
& \textbf{SSH} & \textbf{0.036M} & \textbf{70.1} & \textbf{8.82} & \textbf{47.2} & \textbf{71.9} & \textbf{2.54} \\
\midrule
\multirow{8}{*}{\rotatebox{90}{\textbf{GPT-2 Large}}} 
& FT\textsuperscript{1} & 774.03M & 68.5 & 8.78 & 46.0 & 69.9 & 2.45 \\
& Adpt\textsuperscript{L\textsuperscript{1}} & 0.88M & 69.1 & 8.68 & 46.1 & 71.0 & 2.49 \\
& Adpt\textsuperscript{L\textsuperscript{1}} & 23.00M & 68.9 & 8.70 & 46.1 & 71.3 & 2.45 \\
& LoRA & 0.77M & 69.4 & 8.81 & 46.5 & \textbf{71.9} & 2.50 \\
& DoRA & 0.79M & 69.8 & 8.83 & 46.9 & \textbf{71.9} & 2.50 \\
& VeRA & 0.17M & \textbf{70.3} & 8.85 & 46.6 & 71.6 & 2.54 \\
& FourierFT & 0.072M & 70.2 & 8.90 & 47.0 & 71.8 & 2.50 \\
\rowcolor{green!17}
& \textbf{SSH} & \textbf{0.054M} & \textbf{70.3} & \textbf{8.93} & \textbf{47.2} & \textbf{71.9} & \textbf{2.55} \\
\bottomrule
\end{tabular}}
\caption{\small Performance comparison of various fine-tuning methods on GPT-2 Medium and Large models, evaluated using BLEU, NIST, METEOR, ROUGE-L, and CIDEr metrics. \textsuperscript{1} denotes results sourced from previous studies. }
\label{tab:e2e}
\end{table}



\subsection{Natural Language Generation}


\noindent \textbf{Models and Datasets.}  
We evaluate SSH on the E2E natural language generation (NLG) task~\cite{novikova2017e2e}, fine-tuning GPT-2 Medium and Large models~\cite{radford2019language}, which consist of 24 and 36 transformer blocks.

\noindent \textbf{Implementation Details.}  
We fine-tune LoRA, DoRA, FourierFT, VeRA, and the proposed SSH on GPT-2 Medium and Large, using a linear learning rate scheduler over 5 epochs. Results are averaged across 3 runs, with detailed hyperparameters provided in Tab.~\ref{tab:nlgh} in the Appendix~\ref{gluebench}.




\noindent \textbf{Results and Analysis.}  
As shown in Tab.~\ref{tab:e2e}, SSH consistently delivers superior or comparable performance across all evaluation metrics, while requiring significantly fewer trainable parameters. For GPT-2 Medium, SSH matches the highest BLEU score (70.1) and outperforms other methods in NIST (8.82), METEOR (47.2), ROUGE-L (71.9), and CIDEr (2.54), all with 10.3\% fewer parameters than LoRA and 25\% fewer than FourierFT. A similar trend is observed for GPT-2 Large, where SSH achieves the highest NIST (8.93) and METEOR (47.2) scores, while maintaining a 7.1\% parameter reduction compared to LoRA. 




\begin{table}[!t]
\centering
\resizebox{0.5\textwidth}{!}{%
\begin{tabular}{l|l|c|crcc}
\toprule
\textbf{Model} & \textbf{Method} & \textbf{\# Tr. Para.} & \textbf{MT-Bench} & \textbf{Vicuna} \\
\midrule
\multirow{5}{*}{\textbf{LLaMA2-7B}} 
& LoRA & 159.9M & 5.19 & 7.37 \\
& DoRA & 163.7M & 5.20 & 7.41 \\
& VeRA & 1.6M & 5.18 & 7.47 \\
& FourierFT & 0.064M & 5.09 & 7.50 \\
\rowcolor{green!17}
& \textbf{SSH} & \textbf{0.048M} & \textbf{5.22} & \textbf{7.51}\\
\midrule
\multirow{5}{*}{\textbf{LLaMA2-13B}} 
& LoRA & 250.3M & 5.77 & 7.89\\
& DoRA & 264.5M & 5.79 & 7.90 \\
& VeRA & 2.4M & \textbf{5.93} & 7.90 \\
& FourierFT & 0.08M & 5.82 & 7.92 \\
\rowcolor{green!17}
& \textbf{SSH} & \textbf{0.06M} & \textbf{5.93} & \textbf{7.95} \\
\midrule
\multirow{5}{*}{\textbf{LLaMA3.1-8B}} 
& LoRA & 183.3M & 5.65 & 7.52 \\
& DoRA & 186.9M & 5.66 & \textbf{7.59} \\
& VeRA & 1.9M & 5.61 & 7.49 \\
& FourierFT & 0.073M & 5.67 & 7.67 \\
\rowcolor{green!17}
& \textbf{SSH} & \textbf{0.055M} & \textbf{5.69} & \textbf{7.71} \\
\bottomrule
\end{tabular}%
}
\caption{\small Performance comparison of fine-tuning methods on LLaMA models using the Alpaca dataset. Evaluation scores on MT-Bench and Vicuna are generated and scored by GPT-4.}
\label{tab:mtbench_vicuna}
\end{table}


\subsection{Instruction Tuning}

\noindent \textbf{Models and Datasets.}  
We fine-tune LLaMA2-7B, LLaMA2-13B, and LLaMA3.1-8B using SSH and baseline methods on the Alpaca dataset~\cite{taori2023stanford}. For evaluation, we generate responses to predefined questions from the MT-Bench~\cite{zheng2024judging} and Vicuna Eval datasets, which are then scored by GPT-4 on a 10-point scale.

\noindent \textbf{Implementation Details.}  
Following previous work~\cite{dettmers2024qlora,dettmers20228bit}, we apply LoRA, DoRA, and VeRA to all linear layers except the top one. For FourierFT, we use the configuration from~\cite{gao2024parameter}, and for SSH, we set \(n = 750\). All models are trained using QLoRA’s quantization technique~\cite{dettmers2024qlora} on a single GPU. Each method is trained for one epoch, and we report the average score across all generated responses. Hyperparameter details are provided in Tab.\ref{tab:hyperparamsIn} in the Appendix~\ref{gluebench}.


\noindent \textbf{Results and Analysis.}  
The results in Tab.~\ref{tab:mtbench_vicuna} clearly demonstrate the significant efficiency of SSH compared to other fine-tuning methods such as LoRA, DoRA, and FourierFT. For LLaMA2-7B, SSH achieves the best MT-Bench (5.22) and Vicuna (7.51) scores while reducing trainable parameters by over 99.7\%, using only 0.048M parameters compared to LoRA's 159.9M. Similarly, in LLaMA2-13B, SSH ties with VeRA for the highest MT-Bench score (5.93) and surpasses all methods in Vicuna (7.95), again achieving this with a drastically lower parameter count (0.06M vs. 250.3M for LoRA). Even in the larger LLaMA3.1-8B model, SSH continues to outperform, leading in MT-Bench (5.69) and maintaining a competitive Vicuna score (7.71) with far fewer parameters (0.055M). 



\begin{table}
\centering
\resizebox{0.47\textwidth}{!}{%
\begin{tabular}{l|l|r|cccccc}
\toprule
\textbf{Model} & \textbf{Method} & \textbf{\# Train. Para.} & \textbf{CIFAR100} & \textbf{DTD} & \textbf{EuroSAT} & \textbf{OxfordPets} \\
\midrule
\multirow{7}{*}{\textbf{ViT-B}} 
& Head & - & 84.3 & 69.8 & 88.7 & 90.3 \\
& Full & 85.8M & \textbf{92.4} & \textbf{77.7} & \textbf{99.1}& \textbf{93.4} \\
& LoRA & 581K & 92.1 & 75.2 & 98.4 & 93.2 \\
& Dora & 594K & 92.3 & 75.3 & 98.7 & 93.2 \\
& VeRA & 57.3K & 91.7 & 74.6 & 98.5 & \textbf{93.4}\\
& FourierFT & 72K & 94.2 & 75.1 & 98.8 & 93.2 \\
\rowcolor{green!17}
& \textbf{SSH} & \textbf{54K} & 91.6 & 76.1 & \textbf{99.1} & \textbf{93.4} \\

\midrule
\multirow{7}{*}{\textbf{ViT-L}} 
& Head & - & 84.7 & 73.3 & 92.6 & 91.1 \\
& Full & 303.3M & 93.6 & 81.8 & \textbf{99.1} & 94.4 \\
& LoRA & 1.57M &  94.9 & 81.8 & 98.63 & \textbf{94.8} \\
& Dora & 1.62M &  \textbf{95.1} & 81.8 &   98.8 & \textbf{94.8}\\
& VeRA & 130.5K & 94.2 & 81.6& 98.6 & 93.7 \\
& FourierFT & 144K & 93.7 & 81.2 & 98.7 & 94.5 \\
\rowcolor{green!17}
& \textbf{SSH} & \textbf{108K} & 94.5 & \textbf{81.9} & 99.0& \textbf{94.8} \\
\bottomrule
\end{tabular}%
}
\caption{\small Performance of various fine-tuning methods on ViT-B and ViT-L models across different datasets. The best results for each dataset are highlighted in bold. The best results are highlighted in bold. SSH offers strong parameter efficiency, excelling on DTD and EuroSAT while delivering competitive performance on CIFAR100 and OxfordPets, making it a balanced solution for various vision tasks.}
\label{tab:vit_results}
\end{table}





\subsection{Text Summarization}
\noindent \textbf{Models and Datasets.}  
We evaluate the effectiveness of SSH against other baseline methods on the BART-Large model~\cite{lewis2019bart} for text summarization tasks. Specifically, we assess its performance on the XSUM~\cite{narayan2018don} and CNN/DailyMail~\cite{hermann2015teaching} datasets.

\begin{table}[!t]
    \centering
    \resizebox{0.52\textwidth}{!}{%
    \begin{tabular}{l|c|c|c}
        \toprule
        \textbf{Method} & \textbf{Para. (M)} & \textbf{XSUM} & \textbf{CNN/DailyMail} \\
        \midrule
        AFLoRA ($r=32$) & 5.27 & 44.71/21.92/37.33 & 44.95/21.87/42.25 \\
        LaMDA ($r=32$) & 0.85 & 43.94/20.69/35.21 & 44.16/21.17/40.48 \\
        \rowcolor{green!17}
        SSH ($n=5000$) & 0.21 & 44.72/22.05/37.42 & 44.89/21.75/42.13 \\
        \bottomrule
    \end{tabular}}
    \caption{Performance comparison of SSH, AFLoRA, and LaMDA on BART-Large for text summarization tasks. Results are reported as ROUGE-1/ROUGE-2/ROUGE-L.}
    \label{tab:nlg_bart}
\end{table}

\noindent \textbf{Implementation Details.}  
We compare SSH against AFLoRA and LaMDA under consistent experimental conditions. For AFLoRA and LaMDA, we set the rank $r=32$, while for SSH, we select $n=5000$ Hartley spectrum points. The models are trained using a learning rate of $2\times10^{-4}$, with a batch size of 32 for XSUM and 64 for CNN/DailyMail. Training is conducted for 25 epochs on XSUM and 15 epochs on CNN/DailyMail.

\noindent \textbf{Results and Analysis.}  
Table~\ref{tab:nlg_bart} presents the ROUGE evaluation scores (ROUGE-1/ROUGE-2/ROUGE-L) for different fine-tuning approaches. SSH achieves competitive performance while utilizing significantly fewer trainable parameters compared to AFLoRA and LaMDA. On the XSUM dataset, SSH attains the highest ROUGE-2 score (22.05), surpassing AFLoRA (21.92) and LaMDA (20.69) by 0.13 and 1.36 points, respectively. Furthermore, SSH achieves the highest ROUGE-L score (37.42), outperforming AFLoRA by 0.09 and LaMDA by 2.21 points.

Similarly, on the CNN/DailyMail dataset, SSH attains a ROUGE-1 score of 44.89, which is marginally lower than AFLoRA (44.95) by 0.06 points, but it outperforms LaMDA (44.16) by 0.73 points. In terms of ROUGE-2, SSH achieves 21.75, trailing AFLoRA (21.87) by 0.12 points but exceeding LaMDA (21.17) by 0.58 points. Additionally, SSH attains a ROUGE-L score of 42.13, which is 0.12 points lower than AFLoRA but significantly higher than LaMDA by 1.65 points. Overall, SSH consistently demonstrates strong performance while requiring significantly fewer trainable parameters (0.21M) compared to AFLoRA (5.27M) and LaMDA (0.85M). 

\subsection{Mathematical Reasoning}

\noindent \textbf{Models and Dataset.} We evaluate the performance of SSH against AFLoRA and LaMDA on the LLaMA3.1-8B model using the GSM8K~\cite{cobbe2021training}, a widely used dataset designed to assess mathematical reasoning abilities.

\noindent \textbf{Implementation Details.} All methods are trained with a learning rate of $3\times10^{-4}$ for six epochs using a batch size of 16. For parameter-efficient fine-tuning, AFLoRA and LaMDA employ a low-rank adaptation setting of $r=32$, while SSH leverages a Hartley spectrum selection with $n=10000$. Table~\ref{tab:llama_gsm8k} presents a comparison of the methods in terms of trainable parameters and accuracy on GSM8K.

\begin{table}[!t]
    \centering
    \resizebox{0.5\textwidth}{!}{%
    \begin{tabular}{l|c|c}
        \toprule
        \textbf{Method} & \textbf{Trainable Parameters (M)} & \textbf{GSM8K Accuracy} \\
        \midrule
        AFLoRA ($r=32$) & 20.23 & 38.63 \\
        LaMDA ($r=32$) & 4.99 & 38.11 \\
        \rowcolor{green!17}
        SSH ($n=10000$) & 1.54 & \textbf{38.67} \\
        \bottomrule
    \end{tabular}}
    \caption{Comparison of SSH with AFLoRA and LaMDA on LLaMA3.1-8B for GSM8K. Accuracy is reported as a percentage.}
    \label{tab:llama_gsm8k}
\end{table}

\noindent \textbf{Results and Analysis.} SSH achieves the highest accuracy (38.67\%), surpassing AFLoRA (38.63\%) and LaMDA (38.11\%) while using significantly fewer trainable parameters. SSH requires only 1.54M parameters, representing a \textbf{92.4\% reduction} compared to AFLoRA and a \textbf{69.1\% reduction} compared to LaMDA. 

Despite having nearly 13 times fewer parameters than AFLoRA, SSH achieves comparable accuracy, demonstrating a superior trade-off between efficiency and performance. While LaMDA exhibits the lowest accuracy, SSH maintains robustness in mathematical reasoning tasks with minimal resource requirements.

% These findings highlight SSH as a highly efficient fine-tuning method, delivering strong reasoning capabilities while significantly reducing computational overhead. This makes SSH a promising approach for scaling large language models in resource-constrained settings.


\subsection{Image Classification}

\noindent \textbf{Models and Datasets.}  
We evaluate our method on the Vision Transformer (ViT)~\cite{dosovitskiy2020image}, using both the Base and Large variants. Image classification is performed on the CIFAR-100~\cite{krause20133d}, DTD~\cite{cimpoi2014describing}, EuroSAT~\cite{helber2019eurosat}, and OxfordPets~\cite{parkhi2012cats} datasets.

\noindent \textbf{Implementation Details.}  
We evaluate SSH, LoRA, DoRA, VeRA, and FourierFT by applying them to the query and value layers of ViT. Training only the classification head is denoted as "Head". We set \( r = 16 \) for LoRA, \( n = 3000 \) for FourierFT, and \( n = 2250 \) for SSH. Learning rates and weight decay are tuned for all methods, with training limited to 10 epochs. Further hyperparameter details are provided in Tab.~\ref{tab:SSH_image} in the Appendix~\ref{gluebench}.






\noindent \textbf{Results and Analysis.}  
Tab.~\ref{tab:vit_results} highlights the performance of various fine-tuning methods on ViT-B and ViT-L across four image classification datasets. For the ViT-B model, SSH delivers competitive results with only 54K trainable parameters, significantly fewer than LoRA and DoRA, which use more than 10 times as many. Notably, SSH matches the full fine-tuning performance on EuroSAT and OxfordPets, achieving 99.1\% and 93.4\% accuracy, respectively. For the ViT-L model, SSH also proves efficient, achieving near-optimal performance with only 108K parameters. It sets the highest score on DTD with 81.9\% accuracy and matches the best performance on OxfordPets at 94.8\%. 


% \begin{table}
% \centering
% \resizebox{0.47\textwidth}{!}{%
% \begin{tabular}{l|l|r|cccccc}
% \toprule
% \textbf{Model} & \textbf{Method} & \textbf{\# Train. Para.} & \textbf{TVQA} & \textbf{How2QA} & \textbf{TVC} & \textbf{YC2C} \\
% \midrule
% \multirow{7}{*}{\textbf{VL-BART}} 
% & Full & 228.9M & \textbf{76.3} & 73.9 & 45.7& \textbf{154} \\
% & LoRA & 11.8M & 75.5 & 72.9 & 44.6 & 140.9 \\
% & Dora & 11.9M & \textbf{76.3} & 74.1 & \textbf{45.8} & 145.4 \\
% & VeRA & 1.3M & 75.9 & 73.8 & 44.7 & 142.6\\
% & FourierFT & 1.5M & 76.2 & 73.1 & 45.5 & 147.3 \\
% \rowcolor{green!17}
% & \textbf{SSH} & \textbf{1.1M} & 76.2 & \textbf{74.2}& \textbf{45.8} & 152 \\

% \bottomrule
% \end{tabular}%
% }
% \caption{\small Multi-task evaluation results on TVQA, How2QA, TVC, and YC2C using the VL-BART backbone. The best results are highlighted in bold. SSH demonstrates strong performance with significantly fewer trainable parameters, achieving top scores on How2QA, TVC, and competitive results on the other tasks.}

% \label{tab:bart_results}
% \end{table}

% \subsection{Video-Text Understanding}

% \noindent \textbf{Models and Datasets.}  
% We compare DoRA, LoRA, and full fine-tuning on VL-BART, a model that integrates a vision encoder (CLIP-ResNet101~\cite{radford2021learning}) with an encoder-decoder language model (BART-Base~\cite{lewis2019bart}). The comparison spans four video-text tasks: TVQA~\cite{lei2018tvqa} and How2QA~\cite{li2020hero} for video question answering, and TVC~\cite{lei2020tvr} and YC2C~\cite{zhou2018towards} for video captioning.

% \noindent \textbf{Implementation Details.}  
% We follow the framework from ~\cite{sung2022vl}, fine-tuning VL-BART in a multi-task setup for both video-text tasks. We set \( r = 128 \) for LoRA, \( n = 6000 \) for FourierFT, and \( n = 4500 \) for SSH. Learning rates and weight decay are tuned for each method, with training capped at 7 epochs. Further details are provided in Tab.~\ref{tab:SSH_video} in the Appendix~\ref{gluebench}.


% \noindent \textbf{Results and Analysis.}  
% Tab.~\ref{tab:bart_results} presents the multi-task evaluation results for video-text tasks using the VL-BART backbone. SSH consistently delivers competitive performance with significantly fewer trainable parameters. For TVQA, SSH achieves a near-best score of 76.2, matching FourierFT and just slightly below the full fine-tuning result of 76.3, despite using 99.5\% fewer parameters.
% For How2QA, SSH records the highest score of 74.2, outperforming all other methods. Similarly, for TVC, SSH ties for the best result with DoRA at 45.8, again with far fewer parameters. For YC2C, SSH comes close to the top score achieved by full fine-tuning, with a score of 152 compared to 154, while maintaining remarkable parameter efficiency.

% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=\linewidth]{data/rank.pdf}
%     \caption{\small Scalability comparative experiments of LoRA, FourierFT, SSH, and rSSH across GLUE tasks. SSH demonstrates robust scalability and strong performance in NLP understanding tasks.}
%     \label{fig:ablation}
% \end{figure}





\subsection{Ablation Study}
% We investigate the relationship between parameter number and model performance across different methods. For LoRA, we evaluate with ranks \( r = \{1, 2, 4, 6, 8, 16\} \). For FourierFT and SSH, we examine \( n = \{50, 100, 200, 1000, 6144, 12288\} \) spectral coefficients. The experiments are conducted on 6 GLUE tasks.





% \noindent \textbf{Parameter Scalability.} Figure~\ref{fig:ablation} demonstrates that SSH consistently outperforms other approaches as the number of spectral coefficients \( n \) increases, highlighting its robust scalability and efficiency. Unlike LoRA, where increasing the number of trainable parameters often yields diminishing returns, SSH maintains strong performance gains with fewer parameters. Additionally, SSH surpasses FourierFT across tasks such as MRPC, CoLA, RTE, SST-2, and QNLI, demonstrating its effective parameter utilization.







% A statistical analysis using the Student t-test confirms that SSH significantly outperforms FourierFT across most tasks: RTE ($p=0.0446$, $t=2.67$), MRPC ($p=0.0167$, $t=3.53$), SST-2 ($p=0.0272$, $t=3.09$), QNLI ($p=0.0066$, $t=4.46$), and CoLA ($p=0.0144$, $t=3.67$). While STS-B shows a smaller advantage ($p=0.0348$, $t=2.87$), SSH consistently demonstrates superior scalability and performance across benchmarks, reinforcing its effectiveness as \( n \) increases.





% \noindent \textbf{Informed Frequency Selection v.s. Random Sampling.}
% We further compare the proposed SSH with the scenario where the frequency points are selected randomly (denoted as rSSH).
% The results illustrate that SSH, which uses a systematic partitioning and hybrid selection strategy, consistently outperforms rSSH. This is statistically confirmed by the student t-test, where significant improvements are observed in CoLA (p=0.0260, t=3.13) and SST-2 (p=0.0050, t=4.77), highlighting the advantage of informed frequency selection. Even in tasks with smaller gaps like MRPC (p=0.0380, t=2.80) and QNLI (p=0.0165, t=3.54), SSH demonstrates clear benefits. Although the STS-B task shows a less pronounced difference (p=0.117, t=1.89), the overall performance trend strongly favors SSH, especially in tasks where precise frequency selection is crucial for effective fine-tuning.
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{data/ratio.pdf}
    \caption{\small Ablation study of SSH on GLUE tasks illustrating the effect of varying energy ratios ($\delta$) on performance with RoBERTa-base (n=750). Performance is normalized to $\delta = 0.5$, showing optimal balance and diversity in spectral representation at $\delta = 0.7$.
}
    \label{fig:ratio}
\end{figure}



\noindent \textbf{Energy Ratio Ablation Study.}
\label{subsubsec:energyratio}
Figure~\ref{fig:ratio} presents an ablation study of SSH across GLUE tasks with varying energy ratios (\(\delta\)) on RoBERTa-base with \(n=750\), where performance is normalized to \(\delta = 0.5\). The energy ratios considered are \(\delta=0.5\), \(\delta=0.6\), \(\delta=0.7\), \(\delta=0.8\), and \(\delta=0.9\). 

% The results indicate that an energy ratio of \(\delta=0.7\) generally yields stable and improved performance across most tasks. This is notably evident in MRPC and CoLA, where performance peaks with \(\delta=0.7\). However, performance tends to decline when \(\delta\) is set too low (\(\delta=0.5\) or \(\delta=0.6\)), particularly in tasks like QNLI and CoLA, with relative performance drops of up to 1\%. Similarly, higher \(\delta\) values (\(\delta=0.8\) or \(\delta=0.9\)) also lead to decreased performance, especially in tasks such as QNLI and RTE, suggesting that excessively high energy ratios do not correlate with better task performance. 


The ablation study indicates that an energy ratio of \(\delta=0.7\) optimally balances the selection of spectral components, consistently enhancing performance across natural language understanding tasks such as MRPC and CoLA. This balance prevents overfitting and underfitting, ensuring the retention of informative frequencies while excluding those that are redundant. In contrast, lower ratios (\(\delta=0.5\) or \(\delta=0.6\)) result in inadequate frequency representation, adversely affecting performance in tasks that require robust syntactic and semantic analysis, such as QNLI and CoLA. Higher ratios (\(\delta=0.8\) and \(\delta=0.9\)), while expanding the range of considered frequencies, often introduce noise that compromises the model's focus and generalization ability, particularly evident in tasks like QNLI and STS-B.

% This demonstrates that excessive inclusion of spectral components can diminish model efficacy by detracting from its ability to generalize from training to unseen data.

