\section{Related Work}
\noindent \textbf{Low-Rank Adaptation} (LoRA) ____ reduces trainable parameters by learning low-rank matrices that bypass full-weight updates, minimizing memory usage for gradients and optimizers. 
Different from adapter-based methods ____, LoRA incurs no inference overhead as the low-rank updates are merged with the frozen weights. However, scaling LoRA to larger models and more complex tasks remains challenging.
Recent improvements, including AdaLoRA ____, VeRA ____, QLoRA ____ and DoRA ____, optimize parameter allocation and weight decomposition but still face scalability challenges on larger models. 
%Our proposed SSH overcomes these limitations by selectively learning the most informative spectral components using DHT and its inverse, reducing parameters, enhancing memory efficiency, and scaling effectively across uni-modal and multi-modal tasks.


\noindent \textbf{Frequency-based Spectrum Learning} has been used to reduce trainable parameters while preserving model capacity. Prior works____ showed the effectiveness of compact and sparse spectral representation learning. Gao et al.____ applied the Fourier Transform to fine-tune a subset of spectral coefficients, highlighting the potential of sparse spectrum adaptation in large foundation models. However, the DFT introduces complex operations, and the asymmetry between the DFT and its inverse increases computational overhead.
SSH addresses these issues with the real-valued DHT, which eliminates complex arithmetic, reduces computational complexity, and enhances numerical stability through symmetric transforms. Additionally, SSH’s energy-based sparse selection further decreases trainable parameters, improving efficiency and scalability.




\noindent \textbf{DHT} has shown potential in deep learning for model compression and computational efficiency. For example, ____ employed DHT in medical image retrieval, ____ used it in single-pixel imaging for efficient data acquisition, and ____ leveraged it for media image compression and recovery. These works highlight DHT’s ability to reduce parameters while maintaining performance. 
% Our work extends DHT to language models and multi-modal tasks, utilizing its efficiency for compact and parameter-efficient fine-tuning across diverse domains.