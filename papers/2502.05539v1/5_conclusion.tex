\section{Limitations}

While SSH shows great promise, there are several limitations that need to be addressed in the future:

\begin{itemize}
    \item \textbf{One-go Additional Computational overhead:} A notable limitation is the need to perform a one-go Discrete Hartley Transformation (DHT) on pre-trained weights to guide the selection of the most informative frequencies. This step introduces additional computational overhead and memory requirements upfront.
    

    \item \textbf{Generalization across Different Tasks:} Although SSH has demonstrated strong performance across various tasks, its effectiveness might vary depending on the task or the structure of the data. Due to specific model characteristics, certain tasks may favor other methods, such as LoRA or FourierFT.


    % \item \textbf{Hyperparameter tuning:} The energy ratio $\delta$ and the number of selected frequencies $n$ are crucial to SSH's performance. However, tuning these hyperparameters can be challenging and may require extensive experimentation, especially for new or diverse datasets.

    \item \textbf{Dependency on Pretrained Model Quality:} Since SSH is a fine-tuning method, its performance is inherently dependent on the quality of the pretrained model. If the pretrained model is suboptimal, SSH may not yield significant improvements over other methods.


\end{itemize}

\section{Conclusion}

We introduced Sparse Spectrum Adaptation via Discrete Hartley Transformation (SSH), a novel PEFT method that reduces the number of trainable parameters while maintaining competitive performance. SSH leverages the real-valued DHT and its symmetric forward and backward transform to selectively update the most informative spectral components, addressing the computational and memory challenges of fine-tuning large models. Through extensive experiments across diverse tasks, SSH demonstrates robust versatility, excelling in single-modality NLP tasks such as natural language understanding (NLU), natural language generation (NLG), text summarization, and mathematical reasoning. Furthermore, SSH extends its effectiveness to multi-modality applications, including vision-language image classification. SSH not only achieves state-of-the-art performance but also surpasses existing PEFT methods in both parameter and computational efficiency, positioning it as a scalable and lightweight solution for fine-tuning large models across various domains.








