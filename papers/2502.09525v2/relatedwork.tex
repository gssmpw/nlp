\section{Prior Work}
The task of learning MIMs is fundamental in machine learning, 
with a long line of research in both statistics and computer 
science~\cite{Friedman:1980tu, Huber85-pp, Li91, 
HL93,xia2002adaptive, Xia08}.
The majority of the literature focuses on learning various structured families of MIMs under natural 
distributional assumptions with clean labels~\cite{GLM18,JSA15,DH18,BJW18,GeKLW19,DKKZ20,DLS22,BBSS22,CDGJM23}.
A recent line of work has shown that 
achieving optimal error in the adversarial label setting 
under the Gaussian distribution cannot be achieved in fixed-degree
polynomial time, even for very basic MIM classes \cite{DiakonikolasKPZ21, DKMR22b, Tieg23}. These hardness results motivate the design of faster algorithms with relaxed error guarantees, which is the focus of this paper. Despite this being a natural direction, no general algorithmic framework was previously known in this setting.



The recent work~\cite{DKKTZfocs24} developed 
agnostic learners for MIMs under the Gaussian distribution with 
fixed-parameter tractability 
guarantees similar to ours. The major difference is that, 
while our framework relies on learning from random examples, 
the latter work~\cite{DKKTZfocs24} assumes query access to the target 
function---a much stronger access model.  

The task of efficiently learning intersections of halfspaces 
under Gaussian marginals is one of the most well-studied problems 
in the computational learning theory literature. A 
long line of work focused on achieving strong provable 
guarantees for this problem in the realizable setting \cite{Vempala10,ArriagaVempala:99,KlivansLT09,Kos:04}. 
However, the aforementioned work either fails 
in the presence of noise or scales superpolynomially with respect to the dimension (i.e., does not achieve fixed-degree polynomial runtime). 
An exception is the recent work~\cite{DKS18-nasty} that 
sacrifices the optimal error guarantee for a polynomial dependence 
in the input dimension.

Binary linear classification in the presence of noise 
under well-behaved distributions is a fairly well-understood problem, 
with a substantial body of work; see, 
e.g.,~\cite{KKMS:05,ABL17, DKS18-nasty, DKTZ20, DKTZ20c, 
DiakonikolasKKT21, DKKTZ22} and references therein. However, in the 
multiclass setting, most known results focus on online or bandit 
classification \cite{banditron,BPSBDC19}. That is, 
the subject of designing efficient algorithms in the multiclass setting 
that work in the presence of noise is rather poorly understood.
It is worth pointing out that the multiclass regime 
offers several new interesting models of noisy information that are 
challenging, even if the noise is random~\cite{PRMNQ17,VanRooyencorrupted,
Fotakis2021coarselabellearning,cour2011partiallabel}.