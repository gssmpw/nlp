\documentclass[review,authoryear]{elsarticle}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

%\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.

\usepackage[colorlinks=true,allcolors=blue]{hyperref}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[all]{nowidow}
\usepackage[inline]{enumitem}
\usepackage[scaled]{beramono}
\usepackage{booktabs} % For formal tables
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[tight,footnotesize]{subfigure}
\usepackage{multirow}
\usepackage{balance}
\usepackage{listings}
%\usepackage{microtype}
\usepackage{tcolorbox}
\usepackage{cleveref}
\usepackage{soul}
\usepackage{pdflscape}
\usepackage{lineno}
\usepackage{color, colortbl}
%\usepackage[disable]{todonotes}
%\usepackage{geometry}
%\usepackage[width=12cm, left=4.8cm]{geometry}
\usepackage{geometry}
\usepackage{makecell} 
\newcommand\revised[1]{\textcolor{black}{#1}}
%\renewcommand{\revised}{}
\usepackage{ltablex}
\usepackage{rotating}
\usepackage[normalem]{ulem}
%\renewcommand{\hl}{}
% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}
\usepackage{ltablex}
\renewcommand\refname{Bibliography}


\newcommand{\maianh}[2]{\textcolor{black}{#1} \sout{#2}}
\newcommand{\anhho}[1]{\textcolor{black}{{#1}}}
\newcommand{\rebuttal}[1]{\textcolor{black}{#1}}


\newcommand{\rev}[1]{\textcolor{black}{#1}}


%\input{commands}

\lstset{
	basicstyle=\small\ttfamily,
	columns=flexible,
	breaklines=true,
	alsoletter={+},
}

% Minimal (M3-Rascal) syntax.
\newcommand*{\irsc}[1]{\lstinline[language=m3]+#1+} % inline manifest syntax
\lstdefinelanguage{m3}{
	basicstyle=\ttfamily\scriptsize,
	keywordstyle=\bfseries,
	keywords={m3,declarations,methodInvocation},
	literate={<-}{$\leftarrow$}{1},
	tabsize=2,
	alsoletter={-}
}

\newcommand{\code}[1]{{\small \texttt{#1}}}

\newcommand*\circled[1]{\tikz[baseline=(char.base)]{\color{black} 
		\node[shape=circle,draw=cyan,fill=black!0!white,inner sep=.3pt] (char) {{{\texttt\textbf #1}}};}}
\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;} 


% Usual suspects
\usepackage{xspace}
\newcommand*{\ie}{i.e.,\@\xspace}
\newcommand*{\eg}{e.g.,\@\xspace}
\newcommand*{\cf}{cf.\@\xspace}
\makeatletter
\newcommand*{\etc}{%
	\@ifnextchar{.}%
	{etc}%
	{etc.\@\xspace}%
}
\makeatother
\newcommand*{\etal}{\emph{et~al.}\@\xspace}

\newcommand{\nb}[2]{
	\fbox{\bfseries\sffamily\scriptsize#1}
	{\sf\small$\blacktriangleright$\textit{#2}$\blacktriangleleft$}
}

\definecolor{darkgray}{gray}{0.78}
\definecolor{lightgray}{gray}{0.85}
\definecolor{verylightgray}{gray}{0.95}
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{lightgray}{gray}{0.85}

\newcommand{\GH}{\textsc{GitHub}\xspace}
\newcommand{\GC}{\textsc{GitCNN}\xspace}
%\newcommand{\mC}{\textsc{$\mu$CNN}\xspace}
% \newcommand{\mC}{\textsc{memoCNN}\xspace}
\newcommand{\ES}{\textsc{EnseSmells}\xspace}
\newcommand{\DS}{\textsc{DeepSmells}\xspace}
\newcommand*{\AU}{AURORA\@\xspace}
\newcommand*{\TF}{TensorFlow\@\xspace}
\newcommand*{\KR}{Keras\@\xspace}

\newcommand*{\MN}{MixNet\@\xspace}
\newcommand*{\EN}{EfficientNet\@\xspace}

\newcommand*{\etclst}{\colorbox{blue!10}{\textbf{\vphantom{a}\dots}}}

\newcommand\PN[1]{\textcolor{blue}{\nb{Phuong}{#1}}}
\newcommand\LI[1]{\textcolor{gray}{\nb{Ludovico}{#1}}}
%\newcommand\CDS[1]{\textcolor{blue}{\nb{Claudio}{#1}}}


%\newcommand{\rqfirst}{\textbf{RQ$_1$}: \emph{How does \ES compare with %exhibit improvement in comparison to previous work on 
		%\DS?}}


\newcommand{\rqfirst}{\textbf{RQ$_1$}: \emph{How do software metrics impact the performance enhancement of \ES?}}
\newcommand{\rqsecond}{\textbf{RQ$_2$}: \emph{How do various embedding techniques impact each category of code smell?}}
\newcommand{\rqthird}{\textbf{RQ$_3$}: \emph{How eï¬€ective is \ES compared with classical ML classifiers when utilizing only structural features?}}
\newcommand{\rqfourth}{\textbf{RQ$_4$}: \emph{How does \ES perform compared to the baselines?}} 


% \newcommand{\rqfirst}{\emph{RQ$_1$: How effective is \mC at classifying the considered categories?}~} %Does a deeper network help obtain a better classification performance metamodels 
% \newcommand{\rqsecond}{\emph{RQ$_2$: Which network configuration contributes to a better performance?}~} %Does a wider and deeper network contribute to a better performance?
% \newcommand{\rqthird}{\emph{RQ$_3$: How does \mC compare with \AU?}~} % in terms of prediction accuracy
% %\newcommand{\rqfourth}{\emph{RQ$_4$: Is matrix beneficial to the classification?}~} % the selection of invocations affect the

% %[boxrule=0.86pt,left=0.3em, right=0.3em,top=0.1em, bottom=0.08em]

\newtcolorbox{shadedbox}{
	drop shadow southeast,
	breakable,
	enhanced jigsaw,
	colback=white,
	boxrule=0.80pt,
	left=0.3em,
	right=0.3em,
	top=0.1em,
	bottom=0.05em
}


%\newcommand{\rqfourth}{\textbf{RQ$_4$}: \emph{Is the performance gain proportional to the number of training images?}~}
%\newcommand{\rqfifth}{\textbf{RQ$_5$}: \emph{What is the average execution time for recognizing a fruit using \EN or \MN on an ordinary computer?}~}
%\newcommand{\rqsixth}{\textbf{RQ$_6$}: \emph{Is transfer learning beneficial to the classification of fruits?}~}



\modulolinenumbers[5]
\begin{document}
\begin{frontmatter}
    \title{\ES: Deep ensemble and programming language models for automated code smells detection}
    % \title{\anhho{\ES: A Deep Ensemble of Statistical Semantics and Structural Features for Automated Code Smell Detection}}
\author[unimelb]{Anh Ho}
\author[hust]{Anh M. T. Bui\corref{cor1}}
\author[univaq]{Phuong T. Nguyen}
\author[gssi]{Amleto Di Salle}
    \author[unimelb]{Bach Le}
    \address[unimelb]{The University of Melbourne, Australia \\		anh.ho1@student.unimelb.edu.au, bach.le@unimelb.edu.au}
\address[hust]{Hanoi University of Science and Technology, Vietnam\\
    anhbtm@soict.hust.edu.vn}
    \address[univaq]{Universit\`a degli studi dell'Aquila, 67100 L'Aquila, Italy\\
    phuong.nguyen@univaq.it}
\address[gssi]{Gran Sasso Science Institute, Italy \\
    amleto.disalle@gssi.it}
\cortext[cor1]{Corresponding author}
 
%\input{src/code_smell_detection/0_Abstract}

\begin{abstract}
	A smell in software source code denotes an indication of suboptimal design and implementation decisions, potentially hindering the code understanding and, in turn, raising the likelihood of being prone to changes and faults. Identifying these code issues at an early stage in the software development process can mitigate these problems and enhance the overall quality of the software. Current research primarily focuses on the utilization of deep learning-based models to investigate the \anhho{contextual} information concealed within source code instructions to detect code smells, with limited attention given to the importance of structural and design-related features.
	This paper proposes a novel approach to code smell detection, \anhho{constructing} a deep learning \anhho{architecture} that places importance on the fusion of \anhho{structural} features and \anhho{statistical semantics derived from pre-trained models for programming languages}.
	We further provide a thorough analysis of how different source code embedding models affect the detection performance with respect to different code smell types. 
	\anhho{Using four widely-used code smells from well-designed datasets, our empirical study shows that incorporating design-related features significantly improves detection accuracy, outperforming state-of-the-art methods on the MLCQ dataset with with improvements ranging from 5.98\% to 28.26\%, depending on the type of code smell.}
\end{abstract}

        
\end{frontmatter}

%\linenumbers

%\vspace{-.3cm}
%\input{src/code_smell_detection/1_Introduction}



\section{Introduction}\label{sec:Introduction}
%Introduce to code smells and code smell detection
%Beck and Fowler are credited with coining the term {\it code smell} within the field of software engineering~\citep{fowler1999}. 

\emph{Code smell}~\citep{fowler1999} is the term used to discernible traits or recurring patterns in software source code, indicating probable deficiencies or regions warranting enhancement. Code smells differ from conventional software bugs or errors in that they do not manifest as malfunctions; instead, they function as indications of latent design or implementation quandaries with the potential to precipitate issues in subsequent phases of development and maintenance. Identifying and addressing code smells is an important part of code review and maintenance, as it can help minimize technical debt and make code easier to understand, modify, and extend.

%Numerous research endeavors 
Various studies have been undertaken to investigate the smell metaphor in software engineering. These studies includes comprehensive elucidations of different kinds of code smells~\citep{zhang2011code,singh2018systematic}, methodologies for detecting such smells~\citep{ZHANG2022109737,sharma2021code} as well as quantifiable metrics that may serve as indicators for discerning bad smells~\citep{marinescu2005measurement,sharma2016designite}. %MohaGDM10
Software metrics~\citep{lanza2006characterizing} have been widely studied to identify design defects in the source code~\citep{marinescu2004detection, munro2005product, van2007detection}.
Approaches to code smell detection, whether based on metrics or heuristics, often depended on manually setting thresholds for various measurements. 
While these methods obtain promising outcomes, crafting optimal rules or heuristics manually proves to be highly challenging~\citep{liu2015dynamic}.
Alternatively, relying solely on software metrics could result in a potential classification bias, as code snippets exhibiting distinct behaviors may share identical metricsâ€™ value while exhibiting entirely different code smells. 
%However, given that metrics based on code complexity and design provide a systematic means of evaluating code quality and design, there remain valuable opportunities to apply them in the identification of code smells.

In order to bypass the need for manually designed heuristics and rules based on thresholds, machine learning techniques have been adopted to establish a sophisticated relationship between code metrics and smell labels.
A significant amount of research has focused on the application of traditional classification algorithms, such as SVM~\citep{maiga2012smurf}, Naive Bayes~\citep{khomh2011bdtex}, regression~\citep{fontana2017code}, for the detection of code smells. %
%However, d
Despite their promising potential, these approaches have often overlooked the examination of code representation features.
Deep learning models have subsequently been adopted for the extraction of textual representation from source code.
Numerous studies have been dedicated to the exploration of source code mining using %various deep learning architectures, including 
deep neural networks (DNNs)~\citep{hadj2018hybrid,liu2019deep,ho2022combining}, convolutional neural networks (CNNs)~\citep{liu2019deep,das2019detecting}, long short-term memory networks (LSTMs)~\citep{ho2023fusion}, recurrent neural networks (RNNs)~\citep{sharma2021code}, to name but few. %etc.
%While empirical findings have illustrated that deep learning-based code smell detectors outperform classical machine learning approaches in terms of accuracy rate, achieving maturity in this field still requires extensive research.
Prior studies have primarily regarded source code as textual data and applied natural language processing techniques for text mining. 
However, it is evident that there exists syntactical distinctions between source code and natural language.
Approaching source code as text for mining purposes and overlooking the semantic intricacies within its underlying structures (e.g., nesting control), poses the potential of failing to preserve the intended meaning.
%need to detail more
Moreover, code smells are frequently linked to distinct symptoms that manifest in control and data flows. %Therefore, a comprehensive analysis of the entire source code is crucial, encompassing not only its structure but also its semantics, to uncover underlying issues.

%Proposals
In this paper, we propose a holistic approach  to 
%leverages %the use of 
%deep learning models for 
code smell detection, focusing %particularly 
on assessing the effectiveness of incorporating both \anhho{statistical} semantic structures and design-related features to capture the relationship between various types of smells and source code.
%Going into greater detail, 
In particular, we conceptualize \ES for code smell detection on top of a two-tier deep learning architecture. \anhho{The first tier focuses on acquiring \anhho{statistical} semantic representations of code by integrating \DS~\citep{ho2023fusion}, including \anhho{CNNs} and \anhho{LSTMs}, as an adapter layer to extract code-smell-specific features from code embeddings generated by pre-trained models such as Code2Vec, CodeBERT, and CuBERT. Furthermore, as code smell symptoms are often linked to specific design metrics, the second tier employs deep neural networks (DNNs) with a single adaptive layer to emphasize critical metrics. This tier is designed to learn from selected features, preserving the relationship between code smell symptoms and these metrics. The outputs from both tiers are subsequently concatenated and fed into an Imbalanced Deep Neural Networks for a more effective imbalanced classification.}

In summary, this paper extends our previous work~\citep{ho2023fusion} with the following enhancements:
\begin{itemize}
	\item \rebuttal{We enhance the approach presented in the conference version by introducing a structural module designed to automatically learn the relationships between different code smells and software metrics.}
	\item We extend the scope of experiments with a three-fold objective:
	\begin{enumerate}
		\item Investigating the impact of adjusting various code embedding techniques on prediction performance, considering four types of code smells (\ie Long Method, Feature Envy, Data Class, God Class). %While the conference paper was limited to a specific textual token indexing approach, 
		This paper broadens the scope of code representation by exploring multiple code embedding approaches such as Code2Vec, CodeBERT, and CuBERT.
		\item Examining whether the combination of code representation vectors and hand-crafted metrics can effectively contribute to the detection of code smells.
		\item Demonstrating the effectiveness of \ES compared to state-of-the-art studies.
	\end{enumerate}
	\item We publish the packages developed alongside the metadata processed in this paper to promote future research.\footnote{\url{https://github.com/brojackvn/JSS-EnseSmells}}
\end{itemize}


%The rest of the paper is structured as follows. 

%\textbf{Structure.} 
Section~\ref{sec:Background} introduces the background related to the overview of code smells, the survey of software metrics for code smell detection, as well as pre-trained programming language models. It also provides an overview of related work in our research.
Section~\ref{sec:ProposedApproach} presents the proposed \ES model for code smell detection. Experimental settings, including the benchmark datasets, the employed evaluation metrics, and %as well as 
our evaluation plan, are specified in Section~\ref{sec:Evaluation}.
Section~\ref{sec:Results} discusses the results and address potential threats to the validity of our proposed approach in Section~\ref{sec:threat}.
Finally, Section~\ref{sec:Conclusion} concludes the paper with insights into future directions.



%\vspace{-.3cm}
%\input{src/code_smell_detection/2_Background}




\section{\rebuttal{Background and Related Work}} \label{sec:Background}
% \subsection{Code smell}
%\label{sec:Background}
%\revised{
	This section offers a brief overview of the key elements in code smell detection. Initially, we explore different code smells that have garnered attention in recent times. We also provide an overview of software metrics, considering their relevance in the study of code smells. Following this, our focus shifts to deep learning architectures, emphasizing their role in code representation and their importance in the identification of code smells.
	
	\subsection{Categorization of Code Smells}
	\label{sec:code-smells}
	%The concept of {\it bad smells}, also known as {\it code smells}, was introduced by Fowler~\citep{fowler1999} as a manner to describe possible issues in code might need refactoring.
	\rebuttal{Fowler introduced the concept of {\it code smells} to describe potential issues within software source code that require refactoring~\citep{fowler1999}.}
	Software developers have identified a \rebuttal{wide range} 
	%broad spectrum 
	of code smells which can be classified into categories such as {\it implementation}, {\it design} and {\it architecture} depending on \rebuttal{their granularity level and the overall impact on the source code}~\citep{fowler1999,suryanarayana2014refactoring}.
	Implementation smells emerge at a detailed level including {\it long method, long parameter list, complex conditional}, among others~\citep{fowler1999}. 
	%These can typically be identified by the complexity of the source code, often reflected in terms of the number of lines of code.
	Design smells \rebuttal{refer to violations} of object-oriented design principles, such as abstraction and coupling, and are exemplified by issues like {\it god class, data class}, and {\it multifaceted abstraction}, among others.~\citep{suryanarayana2014refactoring}.
	Architectural smells, including instances like {\it god component and scattered functionality}, manifest at a \rebuttal{high level} of granularity, spanning across multiple components and affecting the entire system~\citep{garcia2009identifying}.
	
%	\color{blue}
	In this study, we focus on four types of code smells including {\it feature envy, long method, data class} and {\it god class}
	as they have been widely used~\citep{azeem2019machine,sharma2018survey}. %These smells have been selected due to the following two reasons. First, these smells are extensively researched. In particular, Azeem et al. 
	%the systematic literature review published by Azeem et al.~\cite{azeem2019machine} 
	%highlighted that the god class, long method, feature envy, and data class smells ranked first, second, third, and sixth, respectively, among the most frequently considered smells in the primary studies they examined~\citep{azeem2019machine}. 
	%Moreover, Sharma and Spinellis, in their survey on software smells, emphasized the god class, feature envy, data class, and long method smells are the first, second, fourth, and fifth, respectively, the most considered smells~\citep{sharma2018survey}. 
	Moreover, we utilized the MLCQ dataset containing these four smells as the benchmark~\citep{madeyski2020mlcq}.
	%In our research, we specifically examine four well-known types of code smells: {\it feature envy}, {\it long method}, {\it data class} and {\it god class}. These smells are well-documented and extensively researched in which the first two smells are associated with method-level issues whereas the remaining relates to class-level problems in the source code. A significant body of work has focused on the exploration and identification of these particular code smells~\citep{sharma2021code,liu2019deep,das2019detecting,ZHANG2022109737}.
	\rebuttal{A detailed explanation of these code smells is provided below.}
	
	\textbf{Feature Envy.} 
	Beck and Fowler introduced the concept {\it feature envy} to describe a scenario where a method accesses the data of another object more than its own data~\citep{fowler1999}.
	%The concept {\it feature envy}, coined by Beck and Fowler, describes a scenario where methods access from an object other than the one they belong to~\citep{fowler1999}. 
	\rebuttal{This kind of code smell indicates misplaced methods,  typically characterized by frequent interactions with the attributes and methods of other classes.}
	%This kind of code smell is indicative of a misplaced method, often marked by frequent interactions with attributes and methods of other classes.
	%The identification of {\it feature envy} in source code can be performed by measuring the strength of coupling between classes or the lack of cohesion within a class~\citep{barbez2020machine}.
	
	\textbf{Long Method.} 
	A method that takes on too many responsibilities is regarded as a sign of the {\it long method}.
	This %kind of code smell 
	can be characterized by source code complexity measurements such as lines of code (LOCs), Halstead's metrics~\citep{curtis1979measuring}, McCabe Cyclomatic~\citep{mccabe1976complexity}.
	%The code smell \emph{long method} is used to describe cases where a method accumulates too many responsibilities, effectively centralizing a classâ€™s functionality by blending various concerns. 
	%This smell is linked not only to methods that are excessively lengthy but also to those with intricate control structures, which complicates testing and debugging. 
	%Metrics that gauge the complexity of source code, such as the number of lines, the length of vocabulary used, and the McCabe Cyclomatic Complexity~\citep{mccabe1976complexity}, are useful in identifying issues related to {\it long method}.
	
	\textbf{God Class.} 
	A {\it god class}, also referred to as a {\it Blob}, is characterized as a class that dominates a system's functionality by taking on too many responsibilities.
	This is reflected in its extensive number of attributes, methods and interactions with data classes.
	%This is %typically 
	%evident from its extensive array of attributes, methods, and its interactions with data classes. 
	%Such centralization usually leads to difficulties in reusing and comprehending the source code. 
	%The complexity of source code, often reflected in a high number of lines and extensive vocabulary, is a clear indicator of this smell.
	Similar to {\it long method}, this code smell is also distinguished by a high number of lines of code and a complex vocabulary.
	
	\textbf{Data Class.} %
	% The {\it data class} code smell refers to a class in a software system that primarily serves to store data without contributing much functionality~\citep{fowler1999}.
	% Such classes become data holders while the actual logic related to this data is often scattered in the code-base, leading to maintenance challenges and making the system more prone to errors.
	% The lack of behaviors and high exposure of data are two main symptoms of this code smell.
	The code smell {\it data class} describes a class in a software system that mainly functions as a repository for data, offering limited functionality.
	These classes serve primarily as data holders, while the logic related to the data is often scattered across the code-base. This dispersion can result in maintenance difficulties and increase the error-proneness~\citep{fowler1999}.
	%Two primary indicators of this code smell are the absence of significant behavior in the class and the extensive accessibility of its data~\citep{fowler1999}.
	
	
	
	\color{black}
	
	
	\subsection{\rebuttal{Software Metric-based Approaches for Code Smell Detection}}
	
	\label{sec:CodeMetrics}
	%Software metrics, a fundamental aspect of software engineering, are instrumental in quantifying various object-oriented properties such as code size, complexity, cohesion, and coupling. These metrics are vital in assessing the internal code quality, productivity, and maintainability of software systems within software project. Moreover, software metrics are indispensable in identifying code smells, which serve as early indicators of architectural weaknesses that can compromise the ease of future maintenance. In this context, our research focuses on both static and dynamic metrics, exploring their applicability in the detection of specific code smells at method-level and class-level.
	
	\begin{comment}
		\rebuttal{In software engineering, software measurements serve as crucial quantitative indicators for assessing various aspects of software development and its outcome.}
		These metrics are essential in gaining insights, conducting evaluations and enhancing both the process of software development and the final software product's quality~\citep{vesra2015study,sharma2011analysis}.
		%Fenton and Bieman have categorized software measurements into three distinct types: (i) {\it Processes} which include software-related activities like development and support; (ii) {\it Products} covering deliverables such as requirements, designs and code; (iii) {\it Resources} referring to people, equipment and tools~\citep{fenton2014software}.
		%These measurements can be derived by assessing the attributes of various software artifacts within these specified categories.
		%When it comes to boosting the quality of software products, metrics related to size and design are particularly significant.
		\rebuttal{Metrics related to size and design play an important role in enhancing the quality of software products, as they help identify potential faults or defects in the source code~\citep{marinescu2004detection,moha2009decor,fontana2017code,ho2022combining}.
		}
	\end{comment}
	
	\rebuttal{A significant amount of research has utilized software metrics to detect code smells.}
	These studies are commonly known as {\it metric-based approaches} and/or {\it rule/heuristic-based approaches}~\citep{marinescu2004detection,marinescu2005measurement,macia2010defining,vidal2015jspirit,lanza2006characterizing}.
	Marinescu et al. introduced a detection strategy based on design metrics including {\it Weighted Method Count} (WMC), {\it Tight Class Cohesion} (TCC) and {\it Access To Foreign Data} (ATFD) to identify the {\it god class} code smell~\citep{marinescu2004detection}.
	This approach was further expanded to incorporate various formulas based on source code complexity and design metrics, enabling the detection of ten different code smells~\citep{marinescu2005measurement}. 
	%For instance, metrics like the number of public attributes (NOPA) and the number of accessor methods (NOAM) are employed together to identify the {\it Data Class} code smell.
	Lanza and Marinescu combined metric-based formulas with thresholds to detect 11 anti-patterns~\citep{lanza2006characterizing}.
	They proposed heuristics by logically combined metric-threshold pairs using logical operators to define detection rules.
	%For example, they detected {\it Long Methods} by introducing an AND-expression of four metric-threshold pairs including {\it Lines of Code} (LoC), {\it McCabe's Cyclomatic Complexity}, {\it Maximal Nesting Level} and {\it Number of Accessed Variables}. 
	These heuristics have been adopted inside the {\it InCode}~\citep{marinescu2010incode} package as an Eclipse-plugin.
	Sales et al. have proposed a dependency based approach, called \textit{JMove} to identify {\it Feature Envy}~\citep{sales2013recommending}. 
	This approach involves defining metrics to quantify the similarity between the dependencies created by the considering method and those of all methods in dependent classes.
	\rebuttal{Likewise, numerous tools and methods have been proposed to detect different kinds of code smells including CCFinder~\citep{kamiya2002ccfinder}, SpIRIT~\citep{vidal2015jspirit}, DECOR~\citep{moha2009decor}, among others.} While these studies have yielded encouraging outcomes in detecting common code smells in source code, further extensive research is required to reach a level of maturity. 
	
	%A significant challenge in rule/heuristic-based approaches lies in establishing metric thresholds. 
	%Indeed, for software engineers, manually setting these thresholds in smell detection algorithms presents a considerable challenge~\citep{liu2015dynamic}.
	%machine learning based approaches for code smell detection using metrics
	\rebuttal{
		An important challenge in rule- or heuristic-based approaches is the determination of appropriate metric thresholds. Defining these thresholds manually within smell detection algorithms represents a considerable difficulty for software engineers~\citep{liu2015dynamic}.
		Machine learning-based methods have drawn considerable interest from researchers as a solution to this challenge~\citep{maiga2012support, arcelli2016comparing,azadi2018poster}. 
		These methods involve training algorithms to capture and learn the complex relationships between source code features and code smell categories.
		Maiga et al. proposed SVMDetect based on the Support Vector Machine for code smell detection~\citep{maiga2012support}. They employed an input vector encompassing 60 structural metrics for each class to detect four well known code smells including God Class, Functional Decomposition, Spagheti Code and Swiss Army Knife. 
		Fontana et al. investigated 16 different machine learning algorithms along with their boosting variants to detect four specific code smells: data class, god class, feature envy and long method~\citep{arcelli2016comparing}. 
		The experiments were carried out using independent metrics at various levels of granularity including method, class, package and project, showcasing the potential of machine learning-based approach comparing to rule-based ones. 
		However, the ambiguity in detecting code smells from a given code snippet arises because various code smells may be associated with the same metrics~\citep{sharma2021code}. This necessitates distinct characteristics to effectively differentiate one type of code smell from others.
	}
	
	%Software metrics are numerical measures that provide insights into the performance and attributes of software systems, and they are pivotal in guiding software development. 
	%These metrics influence critical aspects of software development, including estimating costs, managing complexity, scheduling, and resource allocation. 
	%They play a multifaceted role, contributing to performance assessment, quality assurance, debugging, cost estimation, and effective project management~\citep{vesra2015study,sharma2011analysis}. 
	%In addition to their core role, software metrics also play a crucial role in fault detection within the source code, assessing project risks, and predicting both project success and the likelihood of encountering defective code~\citep{ho2022combining}.
	
	% In software engineering, software metrics are essential for assessing code quality, complexity, and development efforts~\citep{khurana2013model}. 
	% These metrics serve objectives like perception, planning, software assessment, enhancement, and quality progression.
	% Software metrics can be categorized into three main areas: \textit{(i) Product metrics} describes the characteristics of the product such as size, complexity, design features, performance, and quality level; \textit{(ii) Process metrics} describes the characteristics of the product such as size, complexity, design features, performance, and quality level; \textit{(iii) Project metrics} pertain to project characteristics and execution activities, covering budget, required software developers, staff patterns, scheduling, %resource management, 
	% and customer satisfaction.
	
	% Product metrics are software product measures at any stage of their development, from requirements to established systems. Product metrics are related to software features only. Product metrics fall into two classes: \textit{(i) Static metrics} are collected by measurements made from system representations such as design, programs, or documentation; \textit{(ii) Dynamic metrics} are collected by measurements made from a program in execution.
	
	% Code smells can be identified through the application of specific code metrics that capture the characteristics associated with various smells. These metrics facilitate the detection of smells by utilizing appropriate threshold values~\citep{marinescu2005measurement}. For example, 
	% \textit{Long Method} can be detected
	% using the following set of metrics like Lines of code, Nested block depth, Cyclomatic complexity, and more~\citep{liu2011schedule,souza2017applying,fard2013jsnose}; 
	% \textit{Feature Envy} can be detected using the following set of metrics like Coupling between objects, Lack of cohesion in method and more~\citep{nongpong2015feature,tsantalis2009identification,terra2018jmove}; 
	% \textit{God Class} can be detected using the following set of metrics like Weighted methods count, Access to foreign data, Tight class cohesion, Number of methods, Number of fields and more~\citep{trifu2005diagnosing,macia2012automatically,kiefer2007mining,moha2009decor,souza2017applying,danphitsanuphan2012code,liu2011schedule}; 
	% \textit{Data Class} can be detected using %the following 
	% set of metrics like Lack of cohesion in method, Coupling intensity, Number of accessor methods, Has public variables or fields in a class and more~\citep{fontana2013code,fontana2015automatic,arcelli2016comparing}. 
	
	%While techniques based on software metrics for detecting code smells have demonstrated acceptable performance, they are far from perfect. These methods often generate an abundance of inaccurate alerts. Furthermore, we have observed a mutual complementarity in their detection results, implying that some occurrences elude detection when relying solely on structural data~\citep{palomba2013detecting}. This suggests that existing approaches frequently miss valuable supplementary information, thereby limiting their overall effectiveness. The central challenge in these techniques lies in defining metrics with associated thresholds, resulting in significant variability in their outcomes.
	
	
	
	
	
	% \subsubsection{Weighted binary cross-entropy loss function}
	% \label{subsec:WeightedLoss}
	% During the training of a neural network, the cost function plays a crucial role in adjusting the network's weights to improve the machine learning model's fit. In the forward propagation step, the neural network processes the training data and generates outputs, which, in the case of classification, represent the probabilities or confidence levels for possible labels. These probabilities are then compared to the target labels, and the loss function calculates a penalty for any deviation between the target label and the neural network's predictions. In the backpropagation step, the partial derivative of the loss function is computed for each trainable weight in the neural network. These partial derivatives are used to adjust the weights, aiming to minimize the overall loss. Through iterative backpropagation, the trainable weights of the neural network are updated to create a model with lower loss, which ultimately improves the %network's 
	% performance in making accurate predictions.
	
	% The standard binary cross-entropy loss function is defined as follows:
	% \begin{equation}
		%     Loss_{BCE} = -\frac{1}{N} \sum_{i=1}^N {\left[ \hat{y_i} log(y_i) + (1-\hat{y_i}) log(1-y_i) \right] }
		% \end{equation}
	% where $N$ is the number of training examples, $\hat{y_i}$ is the target label for $i^{th}$ training example, $y_i$ is the output of model for $i^{th}$ training sample. The first term $\hat{y_i} log(y_i)$, disincentivizes probabilistic false negatives during training. The same logic applies for the second term and probabilistic false positives. The standard weighted binary cross-entropy loss function is defined as follows:
	% \begin{equation}
		%     Loss_{weighted\_BCE} = -\frac{1}{N} \sum_{i=1}^N {\left[ \textbf{w} \hat{y_i} log(y_i) + (1-\hat{y_i}) log(1-y_i) \right] }
		% \end{equation}
	% where \textbf{$w$} is the sensitive weight, $N$ is the number of training examples, $\hat{y_i}$ is the target label for $i^{th}$ training example, $y_i$ is the output of model for $i^{th}$ training sample. The additional weight can be set to adjust the importance of the positive labels. A common application is to assign higher weight to minority classes. This allows the model to focus more on correctly predicting the less frequent positive instances, thus improving its performance in handling imbalanced datasets.
	
	%\subsection{Pre-trained Programming Language Models for Code Smell Detection}
	
	\subsection{\rebuttal{Deep Learning-based Approaches for Code Smell Detection}
	}
	\label{sec:EmbeddingTechniques}
	% \rebuttal{
		% A large body of research has shifted to the application of deep learning models to extract essential information from source code and to learn the complex relationship between code snippet and code smell classes~\citep{liu2019deep,sharma2021code,ho2023fusion}.
		% Deep learning models have been studied along with the use of
		% text representation models to learn the source code representation, demonstrating promising outcomes in numerous Software Engineering (SE) tasks.
		% } 
	\rev{A growing body of research has shifted towards applying deep learning models to capture deeper features from source code and to learn the complex relationships between code snippets and code smell classes~\citep{liu2019deep,sharma2021code,ho2023fusion}. %Various studies have trained deep learning models from scratch, including CNNs, RNNs, and auto-encoder models~\citep{sharma2021code}. %For example, Sharma et al. evaluated multiple deep learning techniques, such as CNNs, RNNs, and auto-encoders, for detecting different code smells. They also investigated the transferability of models between programming languages, demonstrating promising results~\citep{sharma2021code}.
		Building on this work, we introduced \DS as an effective tool for code smell detection, leveraging diverse deep learning methods to identify patterns and semantic features in code snippets, outperforming previous approaches~\citep{ho2023fusion}. Another research direction has focused on utilizing code embeddings, marking significant advancements in applying NLP techniques to software engineering~\citep{von2022validity}. %A common approach to source code embedding involves converting source code into structural representations using Abstract Syntax Trees (ASTs), enabling the capture of both lexical and syntactic aspects of the code. 
		Recently, there has been a shift towards Transformer-based models (e.g., BERT~\citep{devlin2018bert}, RoBERTa~\citep{liu2019roberta}, CodeBERT~\citep{feng2020codebert}, among others). These models leverage large datasets and Transformer architectures with attention mechanisms to capture various distinctive features of code, a process commonly referred to as capturing semantics. Prior studies on code smell detection have successfully applied these models to automate tasks requiring an understanding of program semantics~\citep{nguyen2024encoding}.
		Kova{\v{c}}evi{'c} et al. explored the effectiveness of code embedding models, such as \textit{code2vec}, \textit{code2seq}, and CuBERT, in comparison to traditional code metrics for detecting God Class and Long Method code smells~\citep{kovavcevic2022automatic}. %Their study analyzed code metrics and code embeddings separately for code smell detection, focusing exclusively on God Class and Long Method. The findings indicated that CuBERT features provided a more effective representation of the semantics required for detecting these code smells compared to conventional code metrics.
	} 
	
	%Considering the significant potential of text representation models in forming the cornerstone for linking human communication with machine comprehension, applying Natural Language Processing (NLP) across various Software Engineering (SE) tasks promises substantial accomplishments.
	% Early foundational word embedding models like \textit{ word2vec}~\citep{mikolov2013distributed} and \textit{ GloVe}~\citep{pennington2014glove} have been employed for code search~\citep{sachdev2018retrieval}, bug localization~\citep{xiao2019improving}, and code summarization~\citep{zhang2020retrieval}, among others. 
	% \rebuttal{
		% Liu et al. utilized the \textit{word2vec} model to encode source code element's identifications such as method, class, package, then feeding them into a deep learning model for code smell classification~\citep{liu2019deep}.}
	% These studies typically utilized pre-trained natural language models to generate embeddings for specific purposes.
	% \rebuttal{However, the use of these models in SE is limited by interpreting  technical terms based on their general meanings, overlooking their specific meanings in the software engineering context.} 
	%For instance, while â€˜bugâ€™ typically means an insect in general usage, in the SE field, it denotes a software defect.
	% \rebuttal{This limitation has led research towards exploring source code embedding models that are more attuned to the nuances of software engineering terminology.}
	
	%In recent years, there has been a growing recognition of the effectiveness of word embeddings, also referred to as word representation techniques, in numerous neural network models for natural language processing (NLP) tasks. Within the domain of NLP, diverse strategies for embedding words with various meanings into compact, low-dimensional vectors have emerged.
	%Similarly, the use of distributed representations for code segments, commonly known as \textit{code embeddings}, has gained prominence in assisting with a range of software engineering tasks.
	%These tasks encompass activities such as software defect prediction%~\citep{liu2023semantic}
	% , code summarization%~\citep{feng2020codebert}
	% , code generation%~\citep{wang2023codet5+}
	% , automated program repair%~\citep{jin2023inferfix} 
	% , automated completion%~\citep{izadi2022codefill}
	% , clone detection%~\citep{tao2022c4}
	% , program translation%~\citep{zhu2022multilingual}
	% , and more.
	%Researchers have explored various methodologies to represent source code as vectors within the context of software engineering tasks. These methodologies include direct application of word embedding techniques to source code for generating representations of code tokens, as well as the development of task-specific approaches tailored to software engineering objectives. %These resulting source code embedding, trained on extensive dataset of source code, can subsequently be utilized in a variety of software engineering tasks, earning them the designation of \textit{pre-trained code embeddings}.
	
	% Numerous methods for creating embeddings from code have emerged, marking significant advancements in the use of NLP for SE~\citep{von2022validity}. 
	%These techniques are mainly categorized into two groups based on the training context for code embeddings~\citep{ding2022can}. 
	%The first category approaches source code as simple text, using standard word embedding models on the tokenized sequence of tokens from the source code. 
	% \rebuttal{A common approach to source code embedding involves converting source code into structural representations using Abstract Syntax Trees (AST), enabling the capture of both lexical and syntactic aspects of the code.} 
	% Recently, there has been a shift towards Transformer-based models (e.g., BERT~\citep{devlin2018bert}, RoBERTa~\citep{liu2019roberta}, code2vec~\citep{alon2019code2vec}, CodeBERT~\citep{feng2020codebert}, among others). 
	% \rebuttal{These models leverage large datasets and Transformer architecture with the use of attention mechanisms to capture various distinctive features of code, a process commonly referred to as capturing semantics. 
		% Source code embeddings are then utilized as input to deep learning models such as CNNs, RNNs, LSTMs for classification purposes.
		% Previous studies have successfully applied these models to automate tasks requiring an understanding of program semantics, such as program repair~\citep{chen2022neural}, type inference~\citep{voruganti2022flextype} among others.
		% Sharma et al. evaluated various deep learning techniques, including CNNs, RNNs, and auto-encoder models, for the detection of different code smells, and also investigated the transferability of models between programming languages, showing promising results~\citep{sharma2021code}. 
		% Kova{\v{c}}evi{'c} et al. explored the effectiveness of code embedding models, such as \textit{code2vec}, \textit{code2seq}, and CuBERT, in comparison to code metrics for detecting God Class and Long Method code smells~\citep{kovavcevic2022automatic}.
		% Their study examined code metrics and code embeddings separately for code smell detection, focusing exclusively on God Class and Long Method. The findings indicated that CuBERT features provided a more effective representation of the semantics required for detecting these code smells compared to code metrics.
		% Ho et al. introduced \DS as an effective tool for code smell detection, employing diverse deep learning methods to identify patterns and semantic features in code snippets, establishing its prowess as a state-of-the-art technique in the field~\citep{ho2023fusion}. } 
	
	% \rebuttal{Motivated by these advancements, we aim to leverage pre-trained code and language models for code smell detection, focusing on extracting additional features. We refer to these as \textit{statistical semantic features} to avoid confusion with literal semantics (e.g., operational semantics in programming languages) and because they are derived from statistical patterns learned by these models. Recent research~\citep{ma2024unveiling} highlights that different models vary in their ability to capture distinct syntactic and semantic features of code.
		\rev{
			Motivated by these advancements, we aim to leverage pre-trained code and language models for code smell detection, focusing on extracting additional features. We refer to these as \textit{statistical semantic} features to distinguish them from literal semantics (\eg operational semantics in programming languages) and to highlight their derivation from statistical patterns learned by these models. The concept of ``statistical semantics'' was previously introduced in the book Statistical Semantics~\citep{sikstrom2020statistical}, where it refers to understanding and modeling the meaning of words and concepts based on their statistical co-occurrence in large text corpora. This approach assumes that meaning emerges from distributional patterns in natural language rather than from predefined dictionaries or human-labeled definitions. Extending this concept to the domain of source code analysis, we apply the term ``statistical semantics'' to describe the extraction of meaning from source code using statistical patterns. Pre-trained models such as code2vec, CodeBERT, and CuBERT are trained on a large corpora of source code, leveraging these patterns to derive semantic representations of code snippets. Similar to natural language models, these code models capture both syntactic and semantic patterns, though their effectiveness varies. Recent research~\citep{ma2024unveiling} reveals that different models exhibit distinct capabilities in encoding syntactic and semantic properties of code. Thus, we adopt ``statistical semantics'' to describe this feature, as it reflects the statistical nature of meaning extraction in source code.
		}
		
		To harness the benefits of both code metrics and statistical semantic features, we integrate these two types of features in our proposed approach, detailed in Section~\ref{sec:ProposedApproach}.
		
		% A brief overview of employed pre-trained code and language models including \textit{code2vec}, CodeBERT, and CuBERT is provided below.
		
		% CodeBERT model was initially trained using pairs of code (methods) and comments (method comments). A comment often expresses the intended meaning (semantics) of the code in the method. By analyzing a large collection of such pairs, CodeBERT learns to map a piece of code to a vector. Each element of a vector is often referred as a semantic feature.
		
		% focusing on contextual embeddings that comprehend the contextual meanings of ambiguous terms. 
		% Here, each word occurrence is assigned a dense vector, with an emphasis on its specific context. The subsequent part of this section provides a comprehensive overview of the latest advancements in code embedding methods.
		% As deep learning continues to advance rapidly within the realm of software engineering applications, a multitude of distributed code representation techniques have emerged. These techniques can be broadly divided into two main categories, primarily distinguished by the training contexts utilized for code embedding~\citep{ding2022can}: 
		% \textit{(i) textual context} treats source code as plain text and directly applies existing word embedding techniques to it. 
		% Typically, the source code is first tokenized into a sequence of tokens to prepare it for embedding training. This token sequence serves as the training data and is subsequently processed using embedding techniques, and 
		% \textit{(ii) structural context} transforms source code into abstract syntax trees (ASTs), representing it as a tree structure. ASTs are valued for their capacity to capture not only the lexical information but also the syntactic structure of the source code, making them useful in a diverse range of software engineering tasks.
		% Furthermore, another classification criterion hinges on the nature of the embedding learning techniques employed: \textit{(i) non-contextual embedding} involves learning fixed representations for tokens in the vocabulary without considering the contextual meanings of these tokens, and 
		% \textit{(ii) contextual embedding} derived from transformer-based models, which adapt token representations based on varying contextual information.
		
		%In the following section, 
		%We delve into a detailed introduction of cutting-edge code embedding techniques as follows.
		
		% \textbf{code2vec.} 
		% The \textit{code2vec} model~\citep{alon2019code2vec} accounts for the structural context present in the source code. It converts the AST of a code fragment, typically a single method or function, into a set of paths, known as path-contexts. Utilizing a neural attention-based feed-forward network, the model determines the appropriate level of attention to allocate to each path-context. This process yields an embedding vector for each individual method, derived from its path-contexts.
		
		% \textbf{CodeBERT.} 
		%It operates as a supervised model, relying on human-labeled data for training, specifically in the domain of Java programming language. Initially, its training is geared towards predicting method names in code, resulting in code embeddings customized for this task, which may not perform as well in other scenarios \citep{kang2019assessing}. Nonetheless, we extend the use of these code representations by further training the model on extensive datasets, enabling us to apply it to various purposes.
		% \begin{figure}
			% 	\centering
			% 	\includegraphics[width=0.4\linewidth]{figs/images_code_smell_detection/code2vec.jpg}
			% 	\caption{The \textit{code2vec} model architecture.}
			% 	 \vspace{-.2cm}
			%      \label{fig:Code2Vec}
			% \end{figure}
		%The \textit{code2vec} model transforms the ASTs of a code snippet, specifically a single method or function, into a collection of paths extracted from the ASTs. These paths are represented as a bag (multiset) and are referred to as \textit{path-contexts} \citep{alon2019code2vec}. To make sense of these bags and their associations with labels, the model employs a neural attention-based feed-forward network architecture.
		%, illustrated in Figure~\ref{fig:Code2Vec}.
		%This architecture learns how much attention should be assigned to each element within the bag of paths. It calculates a weighted average of the path-context vectors, resulting in a single code vector. %This code vector has various potential uses, such as predicting code issues like code smells using the softmax function layer, which is already integrated into the \textit{code2vec} network. However, you can also extract this vector and use it as input for your custom model.
		% \subsubsection{BERT for learning contextual embeddings in source code}
		%Contextual embeddings are dynamic representations of tokens, where the same token can have varying representations depending on its surrounding context. These embeddings are generated by complex transformer-based models designed to process sequences of code tokens. They produce finely-tuned embeddings for each token within the sequence. These models are typically pretrained on a large and diverse text corpus and can be adapted for specific downstream tasks by adding task-specific layers and fine-tuning their parameters using task-specific training data.
		%In this section, we delve into two recent applications that showcase the unique strengths of BERT-based models in the context of source code:
		% The CodeBERT model~\citep{feng2020codebert}, based on a multi-layer bidirectional transformer structure similar to BERT~\citep{devlin2018bert} and its variant RoBERTa~\citep{liu2019roberta}, is designed for bi-modal processing of both programming (PL) and natural languages (NL). It effectively manages source code as well as human language. This model is pre-trained using the CodeSearchNet corpus~\citep{husain2019codesearchnet}, which includes over 2 million bi-modal code-documentation pairs and 6.4 million uni-modal code fragments in diverse programming languages like Python, Java, JavaScript, PHP, Ruby, and Go. CodeBERT has shown remarkable capabilities in essential tasks such as code search and generating code documentation~\citep{feng2020codebert}.%It achieves its proficiency through a dual-objective pretraining approach: firstly, the masked language modeling (MLM) objective~\citep{devlin2018bert}, which involves generating masked code and NL tokens, proven effective for generalizable model learning~\citep{devlin2018bert, liu2019roberta, sun2019videobert}. Secondly, it employs the replaced token detection (RTD) technique introduced by~\cite{clark2020electra} to identify replaced code and NL tokens. CodeBERT leverages bimodal data (NL-PL pairs) for MLM training and unimodal data for RTD training.
		
		% \textbf{CuBERT.} 
		% The CuBERT model~\citep{kanade2020learning} is a specialized adaptation of the BERT model for deriving contextual embeddings from source code. While it shares the architectural design and pre-training approach of BERT, CuBERT differs in its pre-training dataset and tokenization process. It uses a vast collection of 7.4 million GitHub files for pre-training. Unlike traditional BERT, which is tailored for natural languages and struggles to capture the syntax of programming languages, CuBERT addresses this gap.
		% The standard BERT tokenization overlooks key programming elements like indentations, parentheses, semicolons, and arithmetic operators CuBERT, however, overcomes this by ensuring these critical structural components are maintained during the training process.
		%This enhancement allows CuBERT to attain a more profound comprehension of the meaning and context of diverse elements within source code.
		
		%With the significant progress in code representation models, a diverse range of research has been conducted, utilizing these models for efficient code smell detection. These studies can be generally categorized into two streams: (i) {\it Machine learning-based approaches} for code smell detection, and (ii) {\it Deep learning-based methods} for detecting code smells.
		% Move RELATED WORK to this section.
		%\subsubsection{Machine learning-based approaches}
		%Machine learning-based methods for detecting code smells have attracted the interest of software engineering researchers. 
		%These techniques involve training models through data-driven inference, enabling them to learn from examples and apply this knowledge to new cases. 
		%This method boosts the effectiveness and efficiency of identifying code smells, thereby enhancing the quality and maintainability of software. 
		%Maiga et al. proposed SVMDetect based on the Support Vector Machine for code smell detection~\citep{maiga2012support}.
		%They employed an input vector encompassing 60 structural metrics for each class to detect four well known code smells including God Class, Functional Decomposition, Spagheti Code and Swiss Army Knife. 
		%In an extensive study by Fontana et al.~\citep{arcelli2016comparing}, 16 different machine learning algorithms, including their boosting variants, were used to detect four specific code smells: data class, large class, feature envy, and long method.
		%The experiments were carried out using a range of independent metrics at various levels of granularity, including class, method, package, and project, showcasing the potential of the machine learning-based approach comparing to the rule-based ones.
		% Fontana and Zanoni implemented regression-based techniques, placing significant emphasis on feature engineering, to categorize instances of code smells based on their severity~\citep{fontana2017code}.
		%\cite{fontana2017code} used regression-based techniques, employing carefully engineered features for the classification of code smell instances based on their severity.
		%Along the same line of research direction, Azadi et al. developed WekaNose, a semi-automated tool aimed at detecting code smells~\citep{azadi2018poster}. 
		%This approach involves learning the relationship between code smell instances and pertinent metrics, leading to the automatic generation of rules that improve the effectiveness of code smell detection.
		%Generally, initial research in machine learning algorithms for smell detection predominantly depends on software metrics.
		% Recent work of Aleksandar et al.
		% \cite{kovavcevic2022automatic} has observed a superior performance by using a machine learning model trained on CuBERT embeddings and hand-crafted code metrics compared to heuristic metric-based approaches.
		% These advancements contribute to the ongoing improvement of software quality and maintainability through a learning-based approach.
		%Recent research by Aleksandar et al.~\citep{kovavcevic2022automatic} demonstrated enhanced performance with a machine learning model that combines CuBERT embeddings and manually crafted code metrics, outperforming traditional heuristic metric-based methods.
		%Such progress is aiding the continuous enhancement of using code representation models for effective code smell detection.
		
		%\subsubsection{Deep learning-based approaches}
		%Deep learning-based methods for detecting code smells focus on uncovering complex features in the ever-growing data volumes. 
		%Das et al. utilized CNNs to identify patterns in software metrics extracted using IPlasma, achieving notable accuracy in pinpointing Brain Class and Brain Method code smells~\citep{das2019detecting}. Liu et al. adopted word2vec for textual representation, combining class names, enclosing package names, and target package names into a word sequence, which, along with additional features, was fed into a deep learning model for classification~\citep{liu2019deep}. 
		%Sharma et al. evaluated various deep learning techniques, including CNNs, RNNs, and autoencoder models, for the detection of different code smells, and also investigated the transferability of models between programming languages, showing promising results~\citep{sharma2021code}. 
		%Ho et al. introduced \DS as an effective tool for code smell detection, employing diverse deep learning methods to identify patterns and semantic features in code snippets, establishing its prowess as a state-of-the-art technique in the field~\citep{ho2023fusion}.
		
		
		





%\vspace{-.3cm}
%\input{src/code_smell_detection/3_ProposedApproach}	




\section{The Proposed Approach}
\label{sec:ProposedApproach}

Building upon our prior research, namely \textsc{DeepSmells}~\citep{ho2023fusion}, where we concentrated on extracting unique features and patterns from source code, our current focus is on seamlessly integrating the strengths of code metrics-based and deep learning approaches. \ES combines the automated generation of both \textit{\rebuttal{statistical} semantic} and \textit{structural} features from source code. \rebuttal{The aim is to improve code smell detection accuracy and address the specific challenges encountered in previous methods.}

% \vspace{-0.3cm}
\begin{figure}[ht!]
	\centering
	\includegraphics[width=\columnwidth]{Overview-EnseSmell-pdf.pdf}
	% \vspace{-.9cm}
	\caption{The overall architecture of \ES.}
	\label{fig:overallJSS}
\end{figure}

\rebuttal{As illustrated in Figure~\ref{fig:overallJSS}, our approach comprises two primary components, including \textit{Extractor} and \textit{Classifier}. The \textit{Extractor} component is designed to maximize the capture of relevant features from code snippets. Specifically, we focus on two types of features: \textit{(i)} \rebuttal{statistical} semantic features, extending our previous work~\citep{ho2023fusion}; \textit{(ii)} \rebuttal{structural features, automatically learned through a broad view of code metrics.}
	The extracted features are then concatenated and fed into the \textit{Classifier} component, where they are processed by an imbalanced DNN for classification. Once the \textit{Classifier} is constructed, with weights and biases determined, it calculates a probability for each code snippet, indicating whether it exhibits a code smell or is clean.}


\subsection{\anhho{Extractor}}
\anhho{The \textit{Extractor} includes two modules to capture distinct features of code snippets: code structure and statistical semantics.}

\subsubsection{\anhho{Statistical Semantic Module}}
\label{subsec:SemanticModule}

\anhho{This module is designed to capture statistical semantic features by leveraging code embedding models, as illustrated in Figure~\ref{fig:overallJSS}. It operates in two stages: the encoding phase and the model-building phase.
	During the encoding phase, we have experimented with three pre-trained code and language models including CodeBERT, CuBERT and {\it code2vec}. Additionally, to evaluate the effectiveness of these models compared to a natural language embedding model, we have also employed the token indexing technique to represent a source code as a vector of token indices.
	%Due to resource limitations, we focused on smaller PTMs with distinct feature learning capabilities, specifically CodeBERT, CuBERT, and {\it code2vec}.
}
\rebuttal
{
	Unlike simple index vectors that provide minimal contextual information about source code, pre-trained code embedding models are specifically designed to represent programming languages and are trained on extensive and complex datasets. These models extract statistical and semantic features from code snippets, enabling a more comprehensive and nuanced understanding of the underlying code semantics.
	%A simple index vector offers limited contextual information about code snippets. In contrast, pre-trained code embedding models specifically designed for programming languages, such as CodeBERT, CuBERT, and {\it code2vec} (detailed in Section~\ref{sec:Background}), are trained on extensive and complex datasets. These models capture statistical semantic features from code snippets, providing a deeper and more nuanced understanding of underlying code semantics.
}

\begin{itemize} 
	\item \textbf{\textit{Token-Indexing}} involves mapping tokens to integers to convert token sequences into integer vectors. To do so, we conducted several steps, including:
	\textit{(i)} embedding code tokens with a tokenizer\footnote{Available at: \url{https://github.com/dspinellis/tokenizer}};
	\textit{(ii)} computing statistical information on the sample lengths, excluding those that deviate by more than one standard deviation from the mean; 
	\textit{(iii)} padding sequences with zeros to align with the longest input. This method was previously utilized in our prior work, \DS~\citep{ho2023fusion}.
	
	\item \textbf{\textit{CodeBERT}} formats input as two segments joined by special tokens: $[CLS], w_1, w_2, ..., w_n,$ $[SEP], c_1, c_2, ..., c_m, [EOS]$. The first segment generally contains natural language text, such as function comments, while the second segment consists of code tokens. The $[CLS]$ token at the beginning represents the entire sequence, aiding in classification tasks~\citep{feng2020codebert}. CodeBERT produces two types of outputs: \textit{(i)} contextual vector representations for each token in both the text and code segments, and \textit{(ii)} an aggregated vector for the entire sequence represented by the $[CLS]$ token.
	
	\item \textbf{\textit{CuBERT}} uses a custom Java tokenizer developed by Kanade \etal~\citep{kanade2020learning} to preprocess code snippets before feeding them into the CuBERT model. CuBERT processes individual lines of code as input, transforming each line into a 1024-dimensional embedding derived from the special $[CLS]$ token~\citep{devlin2018bert}. To encode each method in the input source code, we compute embeddings for each line of code within the method body and then aggregate them by summing the resulting vectors. Similarly, for class-level embeddings, we apply this process across all methods in the class and calculate the sum of their embeddings. This method of summing embeddings has been shown to deliver consistently strong performance in recent studies~\citep{kovavcevic2022automatic}.
	
	\item \textbf{\textit{code2vec}} generates fixed-length embeddings from code snippets, using method bodies as inputs and producing corresponding tags as outputs~\citep{alon2019code2vec}. To generate the code embedding, we apply {\it code2vec} to represent each method as a 384-dimensional vector through the following steps: \textit{(i)} extracting AST paths from each method using a Java path extractor, and \textit{(ii)} feeding these paths into a pre-trained {\it code2vec} model, omitting the \texttt{softmax} layer to obtain the code vector. %For class-level embeddings, we addressed the limitation that pre-trained {\it code2vec} models are optimized for method-level inputs, not classes. 
	\rebuttal{To compute the embedding for a class, we treat it as a collection of methods. The representation vector of a class is computed as the average of all its method embeddings. This approach was also be employed in previous studies~\citep{compton2020embedding}.}
	%Adopting the approach of Compton et al.~\citep{compton2020embedding}, we treated classes as collections of methods. After evaluating various aggregation methods, we found mean aggregation to be the most effective, enabling us to embed each class by averaging the code vectors of its methods.
	
\end{itemize}

\rebuttal{To address code smell detection, we utilize our previous model, \textsc{DeepSmells}, which has demonstrated effectiveness in this task. In the model-building phase, the \anhho{code embedding} is processed through \textsc{DeepSmells}. 
	The model initially processes the embedding through convolutional blocks, which automatically extract distinctive features. While stacking multiple convolutional blocks can capture increasingly complex and abstract features, including high-level token relationships, deeper networks often encounter a degradation problem where accuracy plateaus and then declines as the network depth grows~\citep{he2016deep}. To mitigate this issue, we use two convolutional blocks, as outlined below.}
%Specifically, it first passes through convolutional blocks that automatically extract unique features from the embedding. Typically, several convolutional blocks are stacked to capture increasingly complex and abstract features, including high-level relationships between tokens. However, deeper networks can experience a degradation problem, where accuracy initially saturates and then sharply declines as network depth increases~\citep{he2016deep}. To mitigate this, we use two convolutional blocks in our setup, as described below.

\begin{itemize}[leftmargin=*]
	\item The first convolution block consists of a 1D-CNN (\texttt{torch.nn.Conv1d}) with 16 filters with a kernel size $k$ which specifies the length of the 1D convolution window. 
	We experiment different values of $k$ (i.e., $k = 3,4,5,6,7$) to evaluate the effectiveness of the convolution layer.
	This layer employs a \texttt{ReLU} activation function, and the remaining parameters are kept as defaults, following by a 1D-Batch Normalization (\texttt{torch.nn.BatchNorm1d}) to accelerate the network training and to reduce internal covariate shift~\citep{ioffe2015batch}. The obtained feature map is then passed to a \texttt{MaxPooling} layer (\texttt{torch.nn.MaxPool1d}) by a factor of size three to reduce the spatial dimension.
	\item The second convolution block is similar to the first one, excepting that we use 32 filters. 
\end{itemize}

Once the output from the convolutional neural networks has been obtained, it is fed into an LSTM network to preserve the meaning and context of the data. The \texttt{torch.nn.LSTM} function is used to configure the LSTM network, with the input size of each LSTM unit being based on the initial size of the initial embedding source code vector. The number of LSTM units in the network is also set to 32 to match the 32 filters in the second convolution block.
Additionally, we adopt a Bi-LSTM architecture to capture long-term dependencies in sequential data, processing it in both forward and backward directions. This architecture consists of two separate LSTM layers: one processes the input sequence in the forward direction, while the other processes it in reverse. The hidden state at each time step is the concatenation of the forward and backward hidden states, enabling the model to incorporate information from both past and future contexts. We maintain the same configuration as before but set the \texttt{bidirectional} hyperparameter to \texttt{True}. 
The final hidden state from this network serves as the \textit{\anhho{statistical} semantic features} of this module.

\subsubsection{Structural Module}
\rebuttal{
	Previous studies have demonstrated the effectiveness of source code metrics in detecting code smells using rule- or heuristic-based approaches~\citep{marinescu2010incode,sales2013recommending,vidal2015jspirit}. In this research, we also take into consideration source code metrics as structural features. However, instead of relying on threshold-based rules, we propose a deep learning model to capture non-linear relationships between code metric vectors and different categories of code smells (Figure~\ref{fig:overallJSS}).
}
%To detect code smells, numerous efforts have relied on metric-based and rule/heuristic-based methodologies, which employ predefined sets of metrics, rules, and associated thresholds to achieve acceptable results. However, these established approaches are not without their challenges, notably in \textit{(i)} defining appropriate metrics and thresholds, and \textit{(ii)} addressing insufficient feature extraction.
%In response to these challenges, we conceptualized a novel module by harnessing a comprehensive set of object-oriented metrics as independent variables, enabling deep learning models to automatically capture essential structural features for code smell detection as shown in Figure~\ref{fig:overallJSS}. 

\begin{table}[h!]
	\caption{The class-level source code metrics.}
	\label{table:ClassLevelMetric}
	\scriptsize
	%\footnotesize
	\centering
	\vspace{0.1cm}
	\begin{tabular}{llll}
		\Xhline{1pt}
		AnonymousClassesQty    & AssignmentsQty        & TotalFieldsQty       & LogStatementsQty \\
		InnerClassesQty        & ReturnQty             & CisibleFieldsQty     & Modifiers              \\
		AbstractMethodsQty     & LoopQty               & VariablesQty         & TCC              \\
		PrivateMethodsQty      & UniqueWordsQty        & TryCatchQty          & LCC              \\
		ProtectedMethodsQty    & PrivateFieldsQty      & LambdasQty           & WMC              \\
		PublicMethodsQty       & ProtectedFieldsQty    & ParenthesizedExpsQty & LOC             \\
		DefaultMethodsQty      & PublicFieldsQty       & NumbersQty           & LCOM              \\
		StaticMethodsQty       & DefaultFieldsQty      & ComparisonsQty       & RFC              \\
		FinalMethodsQty        & StaticFieldsQty       & MaxNestedBlocksQty   & CBO              \\
		SynchronizedMethodsQty & FinalFieldsQty        & MathOperationsQty    & DIT        \\
		TotalMethodsQty        & SynchronizedFieldsQty & StringLiteralsQty    & NOSI             \\
		\Xhline{1pt}
	\end{tabular}
\end{table}


\begin{table}[ht!]
	\caption{The method-level source code metrics.}
	\label{table:MethodLevelMetric}
	%\footnotesiÂ¸ze
	\scriptsize
	\centering
	\vspace{0.3cm}
	\begin{tabular}{llll}
		\Xhline{1pt}
		LOC         & VariablesQty                   & TryCatchQty          & AnonymousClassesQty \\
		RFC         & ParametersQty                  & ParenthesizedExpsQty & InnerClassesQty     \\
		CBO         & MethodsInvokedQty              & StringLiteralsQty    & LambdasQty          \\
		WMC         & MethodsInvokedLocalQty         & NumbersQty           & UniqueWordsQty      \\
		LogStatementsQty  & MethodsInvokedIndirectLocalQty & AssignmentsQty       & Modifiers           \\
		Constructor & LoopQty                        & MathOperationsQty    &     \\
		ReturnsQty  & ComparisonsQty                 & MaxNestedBlocksQty   &            \\
		\Xhline{1pt}
	\end{tabular}
\end{table}

\rebuttal{
	Code snippets are first analyzed to compute software metrics. Arcelli et al. synthesized and classified these metrics into different aspects, including size, complexity, cohesion, coupling, encapsulation, and inheritance~\citep{arcelli2016comparing}. Tables~\ref{table:ClassLevelMetric} and~\ref{table:MethodLevelMetric} provide a summary of commonly used metrics in prior research, including 44 class-level metrics and 26 method-level metrics\footnote{Only the metric abbreviations are shown here; the full names and detailed descriptions are provided in the Appendix.}. 
	These metrics cover multiple dimensions of code structure, complexity, and object-oriented design, which have been demonstrated in numerous studies to have a strong correlation with different types of code smells~\citep{marinescu2005measurement,lanza2006characterizing,marinescu2010incode,maiga2012support}.
}
%In the analysis phase, we conduct a literature review, as shown in Section~\ref{sec:CodeMetrics}, to examine all the metrics used in prior work to detect code smells. These metrics were synthesized by Arcelli et al.~\citep{arcelli2016comparing} and categorized into various aspects of the code, including size, complexity, cohesion, coupling, encapsulation, and inheritance. 
%Following prior work, we defined custom metrics in Tables~\ref{table:ClassLevelMetric} and~\ref{table:MethodLevelMetric}, including 44 class level metrics and 28 method level metrics for each type of dataset, primarily based on combinations of modifiers (\eg public, private, protected, static, final, and abstract) on attributes and methods. The comprehensive names of these metrics are in Tables~\ref{table:DescriptionClassLevel} and~\ref{table:DescriptionMethodLevel} in the Appendix. 
%These metrics span multiple dimensions of code complexity, structure, and style, particularly in object-oriented programming (OOP) within Java, and have been widely applied in building rule-based detection models informed by human expertise. They have proven useful in identifying a variety of code smells, extending beyond those analyzed here.}
\rebuttal{
We utilized the \texttt{CK} tool\footnote{Available at: \url{https://github.com/mauricioaniche/ck}} to calculate the code metrics for each code snippet, based on whether it is at the class or method level.
Before feeding the code metric vector into a deep learning model, some pre-processing steps have been conducted as follows.
%After defining a comprehensive analysis of the code metrics, we use the CK tool\footnote{Available at: \url{https://github.com/mauricioaniche/ck}} to extract the specified code metrics for each snippet, depending on whether it is at the class or method level}. 
%We then apply several preprocessing steps to prepare the data for analysis as follows:
\begin{itemize}
\item Categorical value metrics are encoded using label encoding.
\item Metrics with constant values are removed as they are ineffective for making distinctions.
\item In cases where a metric value could not be calculated for a specific code sample, we used k-Nearest Neighbors from the \texttt{scikit-learn} library\footnote{Available at: \url{https://scikit-learn.org/}} to impute the missing value. However, if the number of missing data samples for a given metric exceeds 5\% of the total samples, we exclude this metric~\citep{schafer1997analysis,bennett2001can}.
\item Finally, all metric features are normalized using \texttt{StandardScaler} from the \texttt{scikit-learn} library.
\end{itemize}
%\textit{(i)} Categorical value metrics are encoded using label encoding; 
%\textit{(ii)} Metrics with constant values are removed as they are ineffective for making distinctions;
%\textit{(iii)}In cases where a metric value could not be calculated for a specific code sample, we used k-Nearest Neighbors from the \texttt{scikit-learn} library\footnote{Available at: \url{https://scikit-learn.org/}} to impute the missing value. However, if the number of missing data samples for a given metric exceeds 5\% of the total samples, we exclude this metric~\citep{schafer1997analysis,bennett2001can};
%However, if the amount of missing data for metrics exceeded \anhho{5\%~\citep{schafer1997analysis,bennett2001can}}, those metrics were removed; 
%\textit{(iv)} Finally, all metric features are normalized using \texttt{StandardScaler} from the \texttt{scikit-learn} library. StandardScaler is employed when the input dataset characteristics differ significantly in their ranges or units of measure. It removes the mean and scales the data to have unit variance.
These pre-processing steps ensure the data is properly formatted for the final stage, where we apply a deep neural network with a single adaptive layer. This layer is both efficient and convenient, as it dynamically adjusts the number of hidden nodes, unlike traditional neural networks, where parameters are tuned through iterative processes. The adaptive layer assigns higher weights to the importance of each metric and maps them into a lower-dimensional latent space~\citep{xu2019software}. This approach effectively captures the key \textit{structural features} necessary for code smell detection.
}

\vspace{-0.1cm}
\subsection{Classifier}
\anhho{The \textit{Classifier} component combines both structural and statistical semantic features from the source code using an ensemble approach (Figure~\ref{fig:overallJSS}).} 

\rebuttal{
In our previous work~\citep{ho2023fusion}, we relied solely on the token-indexing technique to encode code snippets, paired with a simple deep learning model for classifying smelly and non-smelly source code. In this study, we aim to employ a more complex model architecture that learns a richer representation of code snippets, incorporating both statistical semantic and structural features for more effective code smell detection.
Two embedding vectors resulting from the {\it Extractor} component are concatenated using the \texttt{torch.cat} operator of \texttt{PyTorch}. The ensemble vector is then fed as input into the Deep Imbalanced Neural network for classification.
}
%In some classification tasks, multiple output embeddings may be available, with each capturing a different aspect of the input space with varying strengths. These embeddings offer non-redundant information about the output space, and by ensembling them, we can create a more robust joint embedding.
% \anhho{In our prior work~\citep{ho2023fusion}, %\textsc{DeepSmells}, 
% we utilized a relatively simple deep learning model to automatically extract features from a list of mapping tokens (i.e., token-indexing)}. However, the architecture of the model was not sufficiently complex, and the data volume was insufficient to comprehensively represent the characteristics of each code smell. In this study, we aim to address these limitations by providing additional information to enhance the model's capabilities.
% To incorporate this information, we merge semantic features with structural vectors using the \texttt{torch.cat} operator in PyTorch. This ensemble of feature vectors is then fed as input to the Deep Imbalanced Neural Network to classify code snippets as either containing code smells or not. This ensemble approach forms the core of \ES, accentuating the significance of ensemble techniques in our methodology.
%For a deeper understanding of the Deep Imbalanced Neural Network's implementation, consider the core of our classification module. 
\rebuttal{This network comprises two hidden layers using \texttt{ReLU}~\citep{nair2010rectified} as the activation function. The output layer, designed for the binary classification task, consists of a single node with a \texttt{Sigmoid} activation function.  The optimal number of hidden nodes is determined empirically, allowing the model to adapt to different datasets.
Due to a significant class imbalance, with the number of smelly code snippets being much lower than non-smelly ones, the model may develop a bias toward the majority class, despite the minority class being more important. To address this issue, we introduce a sensitivity weight into the binary cross-entropy loss function.
\begin{equation}
\label{eq:loss-function}
\mathcal{L}(x_i) = \beta \hat{y_i} log(y_i) + (1-\hat{y_i}) log(1-y_i)
\end{equation}
where $\beta$ is the sensitive weight, $\hat{y_i}$ is the actual label of input $x_i$ and  $y_i$ is the model's prediction for input $x_i$.}
%, as already explained in Section~\ref{subsec:WeightedLoss}. 
This weight adjustment enables us to fine-tune the importance of the minority class in the classification process. To determine the optimal weight, we conduct a series of experiments comparing the performance of the binary cross-entropy with $\beta$ setting to 1 against weighted binary cross-entropy with varying weight values for $\beta$, such as 2, 4, 8, 12, 32, and 84. The results of these comparisons help us select the best hyper-parameter for our proposed method.

During the training and testing phase, we trained our model using a mini-batch size of 128 and set the learning rate to 0.025. Training was performed over 85 epochs using SGD to optimize the weighted loss function.





%\vspace{-.3cm}
%\input{src/code_smell_detection/4_EmpiricalSettings}



\section{\rebuttal{Experimental Settings}}
\label{sec:Evaluation}
%\subsection{Hardware specification}
We ran the evaluation with Google Colab Pro\footnote{Available at: \url{https://colab.research.google.com/}} using a virtual machine equipped with an Intel Xeon CPU with 2 vCPUs (virtual CPUs) and 13GB of RAM. %Additionally, 
The virtual machine was equipped with a NVIDIA Tesla T4 GPU with 16GB of VRAM to accelerate the deep learning computations. To evaluate %our proposed method 
\ES, we re-ran all the baselines using the same benchmark datasets introduced in Section~\ref{sec:BenchmarkDataset}. To ensure a robust evaluation, we initially split the dataset into 80\%:20\% for training and testing, respectively. Moreover, to mitigate any potential bias and variance related to the test set resulting from the dataset split, stratified 5-fold cross-validation was applied, resembling traditional k-fold cross-validation but preserving the class distribution within each split. %$k$ was set to 5, and a fixed random seed was maintained for reproducibility.% in our experiments. %This strategy helps us minimize experimental variance and establish a more stable baseline for comparing the effectiveness of various models when assessed side by side. 







\subsection{Research questions} \label{sec:ResearchQuestions}

%To evaluate %our proposed method 
%\ES, we re-ran all the baselines using the same benchmark datasets introduced in Section~\ref{sec:BenchmarkDataset}. To ensure a robust evaluation, we initially split the dataset into 80\%:20\% for training and testing, respectively. Moreover, to mitigate any potential bias and variance related to the test set resulting from the dataset split, stratified 5-fold cross-validation was applied, resembling traditional k-fold cross-validation but preserving the class distribution within each split. %$k$ was set to 5, and a fixed random seed was maintained for reproducibility.% in our experiments. %This strategy helps us minimize experimental variance and establish a more stable baseline for comparing the effectiveness of various models when assessed side by side. 



%Subsequently, we designed 
%There are the following five research questions to evaluate \ES.

\begin{comment}
	
	
	\rqfirst~\anhho{The objective of this research question is to compare the performance of \ES and \DS under their optimal configurations. To this end, we conducted extensive experiments with \ES and \DS, utilizing various embedding techniques within the semantic module.}
	
\end{comment}


% An extensive experiment was conducted with \ES and \DS, utilizing various embedding techniques within the semantic module. The aim is to identify the optimal configuration under each embedding technique, maximizing prediction performance for code smells detection.
% of code smells. %This research objective is centered on determining whether the extended approach, \ES, outperforms \DS in the realm of code smell detection.

% We conduct an extensive experiment for the \ES method, with a particular focus on embedding techniques within the semantic module, as described in Section~\ref{subsec:SemanticModule}. Among the considered network configurations, we study to find the one that brings the best prediction performance for detecting code smells.

\rqfirst~\anhho{The objective of this research question is to investigate the impact of adding structural modules to \DS, forming \ES. To achieve this, we compare \ES and \DS under the same configurations for the statistical semantic module.}

% Both \ES and \DS experiments adopt identical structures and embedding techniques for the semantic module. However, \ES utilizes both \textit{semantic} and \textit{structural} modules, while \DS solely relies on the \textit{semantic} module. This comparison explores the impact of software metrics on enhancing the overall performance of the model.

% To compare \ES with the extended method presented in our previous work known as \textsc{DeepSmells} model by~\citep{ho2023fusion} as detailed in Section~\ref{subsec:SemanticModule}, which exclusively focuses on code smell detection using code snippets, we integrate the Deep Imbalance Neural Network as the final classifier within this module. This extended approach conducts experiments with various embedding techniques to study their respective performance for each type of code smell.

\rqsecond~\anhho{The objective of this research question is to investigate code embedding techniques, each characterized by unique architectures that capture distinctive features associated with individual smells. To achieve this, we compare the performance of the aforementioned code embeddings in each architecture (\ES and \DS) under each category of code smell.}

% Our study investigates a variety of embedding techniques, each characterized by unique architectures as detailed in Section~\ref{sec:EmbeddingTechniques}, with the objective of capturing distinctive features associated with individual smells. %We conduct experiments with \ES and \DS, We compare the performance of various embedding techniques with both \ES and \DS %to discern any discernible trends. This comprehensive evaluation is designed 
% to assess the effectiveness of pre-trained programming language models in addressing different types of code smells.

\rqthird~\anhho{The objective of this research question is to evaluate the effectiveness of \ES, which combines various features within a complex architecture, compared to traditional machine learning classifiers that rely solely on software metrics proven effective for performing on tabular data (i.e., our structural features). To achieve this, we perform a grid search on classic classifiers with configurations outlined in Table 4 to determine the optimal choice and compare \ES under its optimal configuration.}



\rqfourth~\anhho{The objective of this research question is to demonstrate the performance of \ES compared to baseline models. To achieve this, we conduct experiments with existing studies, including our work~\citep{ho2023fusion}, the state-of-the-art, ML\_CuBERT model~\citep{kovavcevic2022automatic} and % %benchmarking against 
	three baseline models~\citep{sharma2021code}. Three baselines are variants of an auto-encoder model designed to compress source code and learn salient information reflected in the reconstructed output, which are: %The baseline models are: 
	\textit{(i) AE-Dense:} An auto-encoder model that employs dense layers for both the encoder and decoder. \textit{(ii) AE-CNN:} An auto-encoder model that employs two CNN networks, one for the encoder and another for the decoder. \textit{(iii) AE-LSTM:} Similar to AE-CNN, but with CNNs replaced by LSTM networks. In all cases, the encoder and decoder layers are followed by a fully connected dense layer for classification.}







\subsection{Benchmark dataset}
\label{sec:BenchmarkDataset}

%We conducted experiments using 
The MLCQ dataset, originally made available by an existing study~\citep{madeyski2020mlcq}, was used in our experiments. The dataset comprises 14,739 reviews related to approximately 5,000 Java code snippets gathered from 26 software developers. The reviews are categorized into four types of code smells, \ie God Class and Data Class at the class level, and Feature Envy and Long Method at the method level. %These code smells were selected based on their prevalence in the literature, as indicated in our unpublished internal report. %This report was prepared as part of a systematic review for code quest company in 2017 and 2018~\citep{madeyski2020mlcq}.
%The records in the MLCQ dataset include information about the type of code smell and its severity, details for retrieving the code sample such as repository link, revision, and the exact location of the code snippet within the source code. %Additionally, the dataset provides the ID of the professional developer who provided the annotation.

\anhho{The initial dataset labeling follows a structured approach: each reviewer assigns exactly one severity level for each code smell in a given code snippet, choosing from four levels: \textit{none, minor, major}, and \textit{critical}. We defined the task as a binary classification for each code snippet, labeling it as either \textit{smelly} or \textit{non-smelly}. However, since each code snippet has multiple reviews, we determine the final label using \textit{majority vote} to reconcile the varying labels. Code samples are labeled as \textit{smelly} if they receive more smelly reviews (i.e., minor, major, or critical severity) than non-smelly reviews (i.e., none severity). Conversely, they are classified as \textit{non-smelly} if they have a greater number of non-smelly reviews. To maintain a clear classification, code snippets with an equal count of smelly and non-smelly reviews are subsequently removed from the dataset, as they do not provide a definitive classification and could introduce ambiguity into the binary labeling process.} 

\anhho{The resulting dataset is summarized in Table~\ref{table:BenchmarkDataset}. As shown, this classification problem exhibits a significant class imbalance, with the positive class (smelly instances) representing the minority class. The average imbalance rate across all datasets is 10.46\%.}

\begin{table}[ht!]
	\caption{Statistics of the datasets.}
	\label{table:BenchmarkDataset}
	\centering
	%\footnotesize
	\scriptsize
	\vspace{0.3cm}
	\begin{tabular}{cccc}
		\Xhline{1pt}
		\textbf{Smell} & \textbf{Smell Alias} & \anhho{\textbf{\# Negative }} & \anhho{\textbf{\# Positive}}    \\ 
		\Xhline{1pt}
		Long Method    & LM                   & 1,993                 & 243                  \\
		Feature Envy   & FE                   & 2,126                 & 64                   \\
		Data Class     & DC                   & 1,838                 & 282                  \\
		God Class      & GC                   & 1,857                 & 228                  \\ 
		\Xhline{1pt}
	\end{tabular}
\end{table}

\subsection{Evaluation metrics}
\anhho{Performance metrics commonly used in classification problems include Precision (P), Recall (R), F1-Score (F1), and Accuracy. However, these metrics have specific limitations. For instance, Accuracy often performs poorly on imbalanced datasets as it disproportionately favors the majority class. Additionally, Precision, Recall, and F1-Score focus on three quadrants of the confusion matrix (true positives, false positives, and false negatives) while excluding true negatives, which can lead to incomplete interpretations. To enable meaningful comparisons, we report these metrics while also including Matthews Correlation Coefficient (MCC), which provides a more balanced evaluation by considering all quadrants of the confusion matrix. Consistent with prior work~\citep{sharma2021code, madeyski2023detecting, kovavcevic2022automatic}, these metrics are used to assess model performance.}

\textbf{Precision, Recall, and F1-Score.}
Confusion matrix is an effective means %the starting point 
to evaluate any classifier with four possible outcomes including {\it true positive} (TP), {\it true negative} (TN), {\it false positive} (FP) and {\it false negative} (FN). {\it Precision} measures how many of the positive predictions made by the model are actually correct: $P = \frac{TP}{TP+FP}$; {\it Recall} counts the number of positive cases in the dataset that model can identify: $R = \frac{TP}{TP+FN}$; {\it F1-Score} represents the harmonic mean of {\it precision} and {\it recall} of the prediction model: $F1 = \frac{2*P*R}{P + R}$.

\textbf{MCC.} The metric is used %useful when dealing 
with imbalanced classification, measuring the correlation between the predicted and actual class, which lies %which %is scaled 
in the range $[-1,1]$, where $1$ represents a perfect prediction, and $-1$ shows a perfect negative correlation; %, \ie the worst case; 
An MCC equal to $0$ corresponds to a random prediction.
\begin{equation}
	MCC = \frac{TP*TN-FP*FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}
\end{equation}

\anhho{Notably, MCC and F1-Score both reflect overall performance~\citep{sharma2021code, madeyski2023detecting}: MCC offers a balanced assessment by considering all quadrants of the confusion matrix, while F1-Score is widely used to capture the trade-off between precision and recall, offering insights into the classifierâ€™s ability to identify positive cases.}




% Given that our \ES method combines various features within a complex architecture, this question aims to evaluate its effectiveness in comparison to traditional machine learning classifiers that solely rely on software metrics. We utilize classic classifiers with configurations as outlined in Table~\ref{table:MachineLearningHyperparameter} to determine the most optimal choice.

% \begin{sidewaystable}
	\begin{table}[ht!]
		\caption{The parameter settings of the used machine learning classifiers.}
		\label{table:MachineLearningHyperparameter}
		\centering 
		%\footnotesize
		\scriptsize
		\vspace{0.3cm}
		\begin{tabular}{ll}
			\Xhline{1pt}
			\textbf{Classifier} & \textbf{Hyperparameter settings} (\textit{scikit-learn} library)\\ 
			\Xhline{1pt}
			
			Naive Bayes (NB)                    & \texttt{sklearn.naive\_bayes.GaussianNB}: \\ &$var\_smoothing=[1e^{-9}, 1e^{-8}, 1e^{-7}, 1e^{-6}, 1e^{-5}]$ \\ \hline
			
			Nearest Neighbor (NN)                    & \texttt{sklearn.neighbors.KNeighborsClassifier}:\\ & $n\_neighbors=[1, 3, 5, 7, 9]$ \\ \hline
			
			Random Forest (RF)                    & \texttt{sklearn.ensemble.RandomForestClassifier}: \\&$n\_estimators=[10, 50, 100, 200]$\\& $max\_depth=[None, 10, 20, 30]$ \\ \hline
			
			Logistic Regression (LR)                    & \texttt{sklearn.linear\_model.LogisticRegression}: \\& $C\_values=[0.001, 0.01, 0.1, 1, 10, 100]$ \\ \hline
			
			Classification and                  & \texttt{sklearn.tree.DecisionTreeClassifier}: \\Regression Tree (CART)&$max\_depth\_values=[None, 10, 20, 30]$ \\&$min\_samples\_split\_values=[2,5,10]$ \\&$min\_samples\_leaf\_values=[1,2,4]$ \\&$max\_features\_values=[auto, sqrt, log2]$ \\ \hline
			
			Support Vector                    & \texttt{sklearn.svm.SVC}: \\Machine (SVM)&$kernel\_function=Gaussian RBF$ \\& $gamma\_values=[0.001, 0.01, 0.1, 1, 10, 100]$ \\& $C\_values=[0.001, 0.01, 0.1, 1, 10, 100]$        \\ \hline
			
			Back Propagation                    & \texttt{sklearn.neural\_network.MLPClassifier}: \\Neural Networks (BP)& $hidden\_layer\_size=[16, 32, 64, (32, 16)]$ \\&$learning\_rate\_init=[0.001, 0.01, 0.1]$ \\& $max\_iter=[100,200,300]$ \\& $tol\_err=[1e^{-4}, 1e^{-3}, 1e^{-2}]$ \\ 
			\Xhline{1pt}
			
		\end{tabular}
	\end{table}
	% \end{sidewaystable}


%\anhho{The objective of this research question is to compare the performance of \ES and \DS under their optimal configurations. To this end, we conducted extensive experiments with \ES and \DS, utilizing various embedding techniques within the semantic module.}


% This research questions compares \ES with aforementioned baselines. This includes a comparison with the state-of-the-art ML\_CuBERT model~\citep{kovavcevic2022automatic} and benchmarking against three baseline models~\citep{sharma2021code}. %from our previous work by. 
% This comprehensive evaluation provides an in-depth understanding of \ES performance.

%================================
%Regarding the state-of-the-art model introduced for this dataset by~\citep{kovavcevic2022automatic}, it is named ML\_CuBERT. 
%The research explored various machine learning classifiers using CuBERT representation as input, including Random Forest, Bagging with Support Vector Machines as the base algorithm, and Gradient Boosted Trees (XGBoost). 
%Given the highly imbalanced nature of the problem, the experiments encompass several sampling strategies, such as Synthetic Minority Oversampling Technique (SMOTE), Neighborhood Cleaning Rule undersampling, a combined approach using both SMOTE minority oversampling and Edited Nearest Neighbor undersampling (SMOTEENN), as well as using the original dataset (without resampling) to determine the most effective architecture for code smell detection.
%================================

% As for the baseline in our previous work, conducted by~\cite{sharma2021code}, they have investigated the efficiency of different variants of an auto-encoder model to detect code smells. The objective of an auto-encoder is to compress the source code and to learn salient information which is reflected in the reconstructed output~\citep{sharma2021code}. Three architectures of auto-encoder have been studied and showed promising results, as follows: 
% \textit{(i) AE-Dense:} The auto-encoder model employed denses for encoder and decoder layers; 
% \textit{(ii) AE-CNN:} The auto-encoder model employed two CNN networks, the first one for encoder and the other for decoder; 
% and \textit{(iii) AE-LSTM:} Similar to AE-CNN but the CNNs are replaced by LSTM networks.
% The encoder and decoder layers are followed by a fully connected dense layer for classification.




	
%\vspace{-.3cm}
%\input{src/code_smell_detection/5_Results_Discussion}


\section{\rev{Experimental Results and Discussion}}\label{sec:Results}
% Set the \rqxxxxx at commands.tex file

% ============== Research Question 1 - COMPARE TO DEEPSMELLS ==========

\rebuttal{This section reports and analyzes the experimental results by answering the research questions introduced in Section~\ref{sec:ResearchQuestions}}.







% =========== Research Question 2 - SOFTWARE METRICS ===========
\subsection{\rqfirst}

\begin{figure}[t!]
	\centering
	%        \vspace{-.2cm}
	\includegraphics[width=0.80\linewidth]{role_of_SM.png}
	\label{fig:role_of_SM}
	\caption{Performance of \ES and \DS across different embedding techniques.} % evaluated on F1 and MCC metrics
	\vspace{-.2cm} 
\end{figure}

% \textcolor{blue}{@ Anh Ho:} \textcolor{red}{It is noticeable that \textbf{CuBERT} in Long Method and God Class is an exception (\ES is slightly lower than \DS by less than 1\% in both F1 and MCC metrics.). I wonder if this results is persuasive enough to reach the conclusion below.}
Figure~\ref{fig:role_of_SM} compares the performance of \ES and \DS under identical structures and embedding techniques for the semantic module. Notably, \ES incorporates an additional module, the structural model. Analyzing each embedding technique, starting with the \textit{token indexing} method, we observe that \ES outperforms \DS across all four types of smells. Particularly, in the case of DC smell, \ES exhibits significantly higher performance compared to \DS. This trend is similarly observed for \textit{CodeBERT}, with a substantial increase in performance for \ES in GC and DC smells. As for \textit{code2vec}, the same pattern persists, but the considerable outperformance is clearly evident across all smells.

Notably, \textit{CuBERT} presents an exception in LM and GC smell, where \ES slightly trails \DS by less than 1\% in both F1 and MCC metrics. However, in FE smell, \ES outperforms \DS by nearly 10\% in both F1-Score and MCC. The most significant disparity is observed in DC smell, where \ES showcases a substantial performance improvement of approximately 40\% in both F1-Score and MCC compared to \DS.

% Conclusion of RQ2
\vspace{.2cm}
\begin{tcolorbox}[boxrule=0.86pt,left=0.3em, right=0.3em,top=0.1em, bottom=0.05em]
	\textbf{Answer to RQ$_1$.} 
	% The finding highlights the crucial role of software metrics in enhancing the performance of \ES, underscoring their significant impact on overall effectiveness.
	\rev{Incorporating structural modules enables \ES to obtain a better prediction performance, highlighting the crucial role of software metrics. The performance improvement reaches up to 40\%, depending on the pre-trained model and the type of code smell.}
\end{tcolorbox}
\vspace{-.2cm}

% ============== Research Question 3 - MODEL --> SMELL ==============
\subsection{\rqsecond}

In Figure~\ref{fig:embedding-smell}, we compare the performance of embedding techniques within the same architecture for both \ES and \DS. This study aims to assess the performance of each embedding technique across various smells, examining the performance patterns for each method in relation to each type of smell.

\begin{figure}[h!]
	\centering
	\vspace{-0.1cm}
	\includegraphics[width=0.80\linewidth]{embedding-smell.png}
	\vspace{-0.3cm}
	\caption{Performance of \ES and \DS in various embedding techniques.}
	\label{fig:embedding-smell}
	\vspace{-0.2cm}
\end{figure}
% \textcolor{blue}{@ Anh Ho:} \textcolor{red}{I wonder whether we have gathered enough insights to formulate a research question, considering that we have conducted experiments on only two architectures - \ES and \DS. The results obtained are summarized in the conclusion below}
% LM smell
%     DS: Token ~ CuBERT > CodeBERT > code2vec
%     ES: Token > CuBERT > CodeBERT ~ code2vec
% FE smell
%     DS: Token >~ CuBERT > CodeBERT > code2vec
%     ES: CuBERT >~ Token > code2vec >~ CodeBERT
% GC smell
%     DS: Token > CuBERT > code2vec > CodeBERT
%     ES: Token >~ CodeBERT >~ code2vec > CuBERT
% DC smell
%     DS: code2vec >> CodeBERT > Token > CuBERT
%     ES: code2vec > CodeBERT ~ Token > CuBERT



Starting with method-level smells, the use of \textit{code2vec} in both \ES and \DS exhibits the lowest performance compared to other techniques. In contrast, \textit{token indexing} emerges as a highly recommended choice due to its consistently superior performance. Notably, in this context, the consideration of \textit{CuBERT} may be considered, given its potential impact, as it demonstrates only a slightly lower performance than \textit{token indexing}.

Shifting focus to class-level smells, in the case of GC smell, both \ES and \DS employing the \textit{token indexing} embedding technique show the best results compared to other techniques. Transitioning to DC smell, the use of \textit{code2vec} delivers the highest performance, especially in \DS, where it exhibits significant outperformance.


% Insight of RQ1
% \begin{tcolorbox}[colback=yellow!10!white,colframe=red!75!black,title=\textbf{Conclusion}]
	%     \begin{itemize}
		%             \item The common trend of RQ1 and RQ2 is the best performance in corresponding embedding technique.
		%         \item Long Method: CuBERT, CodeBERT and token indexing provides similar results - Code2Vec is the worst. 
		%         \item Feature Envy: Code2Vec is the worst. CuBERT is the best. 
		%         \item God Class: Token Indexing is the best. CodeBERT is the worst. Code2Vec works better. 
		%         \item Data Class: Code2Vec is the best - In some class-level smell, because of we have many methods, Code2Vec seems working better than in the case of only one method. Because Code2Vec investigates the AST tree of the whole class, which gives more information than only one method.
		%     \end{itemize}
	%     $\Longrightarrow$ CuBERT and CodeBERT are typically applied in the method level to represent a small source code fragment. They work less efficiently than in case of many method (class-level).
	% \end{tcolorbox}

% Conclusion of RQ3
\vspace{.2cm}
\begin{tcolorbox}[boxrule=0.86pt,left=0.3em, right=0.3em,top=0.1em, bottom=0.05em]
	\textbf{Answer to RQ$_2$.} %This finding emphasizes that 
	Each pre-trained programming language model possesses a distinct architecture, \anhho{suitable for} addressing specific code smells. Specifically, \textit{token-indexing} is effective for \textit{Long Method} and \textit{God Class}, \textit{CuBERT} is appropriate for \textit{Feature Envy}, and \textit{code2vec} is well-suited for \textit{Data Class}, showcasing the suitability of each model for different types of code smells.
\end{tcolorbox}
\vspace{-.2cm}

% =========== Research Question 4 - ML classifier ====================
\subsection{\rqthird}


Table~\ref{table:ResultsOfMLClassifier} illustrates the performance of \ES in comparison to seven classical machine learning models, which include \textit{Naive Nayes (NB), Nearest Neighbor (NN), Random Forest (RF), Logistic Regression (LR), Classification and Regression Tree (CART), Back Propagation neural networks (BP),} and \textit{Support Vector Machine (SVM)}. These machine learning models exclusively utilize software metrics. The results indicate that \ES outperforms all other models, demonstrating the highest performance across all evaluated models.

% Results
\begin{table}[h!]
	\caption{Comparing the performance of seven %machine learning 
		classifiers on software metrics with \ES model.}
	\label{table:ResultsOfMLClassifier}
	\centering
	\vspace{.3cm}
	\scriptsize
	\begin{tabular}{cccccccccc}
		\Xhline{1pt}
		\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Smell}}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Metric}}} & \multicolumn{7}{c}{\textbf{Machine learning classifier}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{\ES}}} \\ \cline{3-9}
		\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{\textbf{NB}} & \multicolumn{1}{c}{\textbf{NN}} & \multicolumn{1}{c}{\textbf{RF}} & \multicolumn{1}{c}{\textbf{LR}} & \multicolumn{1}{c}{\textbf{CART}} & \multicolumn{1}{c}{\textbf{SVM}} & \multicolumn{1}{c}{\textbf{BP}} & \multicolumn{1}{c}{} \\ \Xhline{1pt}
		
		\multirow{4}{*}{\textbf{LM}} 
		& P & 0.5698 & 0.8048 & 0.8137 & 0.8168 & 0.7595 & 0.7560 & \textbf{0.8477} & 0.8215 \\ 
		& R & \textbf{0.8196} & 0.5310 & 0.6821 & 0.6535 & 0.6429 & 0.6901 & 0.6857 & 0.7873 \\ 
		& F1 & 0.6715 & 0.6360 & 0.7416 & 0.7248 & 0.6943 & 0.7199 & 0.7566 & \textbf{0.8016} \\ 
		& MCC & 0.6339 & 0.6173 & 0.7150 & 0.6994 & 0.6625 & 0.6874 & 0.7347 & \textbf{0.7780} \\ \Xhline{1pt}
		
		\multirow{4}{*}{\textbf{FE}} 
		& P & 0.1162 & 0.1265 & 0.3000 & 0.2000 & 0.4611 & 0.2246 & 0.2126 & \textbf{0.5215} \\ 
		& R & \textbf{0.7440} & 0.1220 & 0.0462 & 0.0736 & 0.2099 & 0.1220 & 0.2451 & 0.3393 \\ 
		& F1 & 0.1993 & 0.1226 & 0.0800 & 0.1076 & 0.2784 & 0.1545 & 0.2181 & \textbf{0.4025} \\ 
		& MCC & 0.2453 & 0.0987 & 0.1084 & 0.1068 & 0.2911 & 0.1441 & 0.1978 & \textbf{0.3868} \\  \Xhline{1pt}
		
		\multirow{4}{*}{\textbf{GC}} 
		& P & 0.1620 & 0.5415 & \textbf{0.7442} & 0.6231 & 0.5296 & 0.6109 & 0.5836 & 0.6220 \\ 
		& R & \textbf{0.8308} & 0.3943 & 0.3662 & 0.2744 & 0.4505 & 0.4011 & 0.5494 & 0.5867 \\ 
		& F1 & 0.2709 & 0.4552 & 0.4895 & 0.3760 & 0.4845 & 0.4809 & 0.5656 & \textbf{0.5851} \\ 
		& MCC & 0.1280 & 0.3944 & 0.4761 & 0.3570 & 0.4166 & 0.4339 & 0.5028 & \textbf{0.5429} \\  \Xhline{1pt}
		
		\multirow{4}{*}{\textbf{DC}} 
		& P & 0.1644 & 0.5415 & 0.6849 & 0.5980 & 0.5525 & 0.6343 & 0.5839 & \textbf{0.8123} \\ 
		& R & \textbf{0.8205} & 0.3943 & 0.3979 & 0.2816 & 0.4263 & 0.3838 & 0.5393 & 0.5314 \\ 
		& F1 & 0.2734 & 0.4552 & 0.5019 & 0.3803 & 0.4808 & 0.4772 & 0.5573 & \textbf{0.6393} \\ 
		& MCC & 0.1291 & 0.3944 & 0.4703 & 0.3541 & 0.4184 & 0.4366 & 0.4971 & \textbf{0.5907} \\  \Xhline{1pt}
	\end{tabular}
\end{table}


Specifically, for the LM code smell, the F1-Score and MCC of \ES are 80.16\% and 77.80\% higher than those of BP, with improvements of 4.50\% and 4.33\%, respectively. Notably, the precision value of BP is 2.62\% higher than \ES but comes at the cost of a significant reduction in recall by 10.16\%. Additionally, while the recall value of NB achieves the highest at 81.96\%, it is higher than \ES; however, the precision, F1-Score, and MCC values are significantly lower by 25.17\%, 13.01\%, and 14.41\%, respectively. 

Regarding the FE code smell, when comparing \ES to CART, which is considered the best machine learning classifier for this type of code smell, \ES outperforms in all four evaluation metrics, showing improvements of 6.04\%, 12.94\%, 12.41\%, and 9.57\% in precision, recall, F1-Score, and MCC, respectively. The scenario is significantly different when comparing NB and \ES. While NB achieves the highest precision value among these models at 74.40\%, its recall value is only 11.62\%, highlighting its weakness in handling the high imbalanced dataset. 

Turning to GC code smell, while BP exhibits the best overall performance among these ML classifiers, when compared to \ES, our model demonstrates outperformance in all four metrics. Furthermore, NB and RF are two models with the highest values in recall and precision, but they also exhibit significant reductions in precision and recall, respectively. Consequently, this leads to a significant decrease in both F1-Score and MCC, ranging from 6.68\% to 41.49\%.

As for the remaining code smell, DC, when comparing \ES to BP, which demonstrates good performance across almost all types of code smells including this one, our model reveals significant differences in precision, F1-Score, and MCC, corresponding to 22.84\%, 8.2\%, and 9.36\%. Additionally, when comparing \ES to NB, which boasts the highest recall value at 82.05\%, the precision value of only 16.44\% highlights its weakness in handling highly imbalanced datasets leading to a remarkable decrease in both F1-Score and MCC, with a deviation of 36.59\% and 46.16\% lower than \ES, respectively.

% \textcolor{blue}{@ Anh Ho:} \textcolor{green}{Strong proofs}

% % Insight of RQ2
% \begin{tcolorbox}[colback=yellow!10!white,colframe=red!75!black,title=\textbf{Conclusion}]
	%     \begin{itemize}
		%         \item Metrics provide promising results when working with basic classification algorithm of machine learning, even better than embedding techniques when detecting class-level. We can use some characteristic of method-metrics and class-metrics to explain the result. 
		%     \end{itemize}
	%     $\Longrightarrow$ Some code smells are very sensitive with code metrics.
	% \end{tcolorbox}


\vspace{.2cm}
\begin{tcolorbox}[boxrule=0.86pt,left=0.3em, right=0.3em,top=0.1em, bottom=0.05em]
	\textbf{Answer to RQ$_3$.} \anhho{\ES outperforms all traditional ML classifiers that build prediction models using \textit{structural} features. In terms of F1-Score, \ES achieves superior performance ranging from 1.95\% to 12.41\% compared to the best ML classifier for each code smell. For MCC, the improvement ranges from 4.01\% to 14.15\%.}
\end{tcolorbox}
\vspace{-.2cm} 






% ============ Research Question 5 - BASELINE MODELS ====================
\subsection{\rqfourth}
% Results




Tables~\ref{table:ResultsOfEnsemble} and~\ref{table:ResultsOfEASE} present the optimal performance of the \ES and \DS models, respectively, evaluated using different code embedding techniques, including \textit{token-indexing, CuBERT, CodeBERT}, and \textit{code2vec}, each under their respective optimal network configurations. 

\begin{table}[h!]	
	\caption{Performance of different embedding techniques in \ES architecture under optimal configurations.}
	\label{table:ResultsOfEnsemble}
	\centering
	\scriptsize
	\vspace{0.3cm}
	\begin{tabular}{clcccc}
		\Xhline{1pt}
		\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Smell}}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Model}}} & \multicolumn{4}{c}{\textbf{Evaluation metric}} \\ 
		\cline{3-6}
		\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{\textbf{P}} & \multicolumn{1}{c}{\textbf{R}} & \multicolumn{1}{c}{\textbf{F1}} & \multicolumn{1}{c}{\textbf{MCC}} \\ 
		\Xhline{1pt}
		\multirow{4}{*}{\textbf{LM}} 
		& \textbf{$\ES_{token-indexing}$}  & 0.8215 & 0.7873 & \textbf{0.8016} & \textbf{0.7780} \\ 
		& \textbf{$\ES_{CuBERT}$} & \textbf{0.8482} & 0.7391 & 0.7877 & 0.7616 \\ 
		& \textbf{$\ES_{CodeBERT}$} & 0.7938 & 0.7824 & 0.7857 & 0.7611 \\
		& \textbf{$\ES_{code2vec}$} & 0.7526 & \textbf{0.8242} & 0.7841 & 0.7618 \\ 
		\Xhline{1pt}
		\multirow{4}{*}{\textbf{FE}}  
		& \textbf{$\ES_{token-indexing}$} & 0.4821 & 0.3554 & 0.3982 & 0.3849 \\ 
		& \textbf{$\ES_{CuBERT}$} & \textbf{0.5215} & \textbf{0.3393} & \textbf{0.4025} & \textbf{0.3868} \\ 
		& \textbf{$\ES_{CodeBERT}$} & 0.5128 & 0.2718 & 0.3527 & 0.3449 \\
		& \textbf{$\ES_{code2vec}$} & 0.4423 & 0.3727 & 0.3538 & 0.3568 \\ 
		\Xhline{1pt}
		\multirow{4}{*}{\textbf{GC}}  
		& \textbf{$\ES_{token-indexing}$} & \textbf{0.6962} & 0.5077 & \textbf{0.5801} & \textbf{0.5318} \\ 
		& \textbf{$\ES_{CuBERT}$} & 0.5565 & 0.5105 & 0.5153 & 0.4675 \\ 
		& \textbf{$\ES_{CodeBERT}$} & 0.5993 & 0.5607 & 0.5764 & 0.5241 \\
		& \textbf{$\ES_{code2vec}$} & 0.5947 & \textbf{0.5724} & 0.5729 & 0.5288 \\ 
		\Xhline{1pt}
		\multirow{4}{*}{\textbf{DC}}  
		& \textbf{$\ES_{token-indexing}$} & 0.7340 & \textbf{0.5427} & 0.6203 & 0.5638 \\ 
		& \textbf{$\ES_{CuBERT}$} & 0.8027 & 0.4567 & 0.5772 & 0.5238 \\ 
		& \textbf{$\ES_{CodeBERT}$} & 0.7447 & 0.5390 & 0.6199 & 0.5648 \\
		& \textbf{$\ES_{code2vec}$} & \textbf{0.8123} & 0.5314 & \textbf{0.6393} & \textbf{0.5907} \\ \Xhline{1pt}
	\end{tabular}
\end{table}

\begin{table}[h!]
	%\footnotesize
	\scriptsize
	\caption{\anhho{Performance of different embedding techniques in \DS architecture under optimal configurations.}}
	\label{table:ResultsOfEASE}
	\centering
	\vspace{.3cm}
	\begin{tabular}{clcccc}
		\Xhline{1pt}
		\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Smell}}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Model}}} & \multicolumn{4}{c}{\textbf{Metric}} \\ 
		\cline{3-6}
		\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{\textbf{P}} & \multicolumn{1}{c}{\textbf{R}} & \multicolumn{1}{c}{\textbf{F1}} & \multicolumn{1}{c}{\textbf{MCC}} \\ 
		\Xhline{1pt}
		\multirow{5}{*}{\textbf{LM}} 
		& \textbf{$\DS_{token-indexing}$}~\citep{ho2023fusion} & 0.8327 & 0.7470 & 0.7865 & 0.7607 \\ 
		& \textbf{$\DS_{CuBERT}$} & \textbf{0.8560} & \textbf{0.7490} & \textbf{0.7888} & \textbf{0.7673} \\ 
		& \textbf{$\DS_{CodeBERT}$} & 0.7903 & 0.7448 & 0.7661 & 0.7373  \\
		& \textbf{$\DS_{code2vec}$} & 0.7974 & 0.1982 & 0.3152 & 0.2509 \\ 
		% & \textbf{\ES} & 0.8215 & \textbf{0.7873} & \textbf{0.8016} & \textbf{0.7780} \\ 
		\Xhline{1pt}
		
		\multirow{5}{*}{\textbf{FE}}  
		& \textbf{$\DS_{token-indexing}$}~\citep{ho2023fusion} & \textbf{0.5936} & 0.2322 & 0.3198 & \textbf{0.3318} \\ 
		& \textbf{$\DS_{CuBERT}$} & 0.5143 & \textbf{0.2439} & \textbf{0.3203} & 0.3201 \\ 
		& \textbf{$\DS_{CodeBERT}$} & 0.5590 & 0.2048 & 0.2833 & 0.2947  \\
		& \textbf{$\DS_{code2vec}$} & 0.3603 & 0.1422 & 0.1815 & 0.1683 \\ 
		% & \textbf{\ES} & 0.5215 & \textbf{0.3393} & \textbf{0.4025} & \textbf{0.3868} \\ 
		\Xhline{1pt}
		
		\multirow{5}{*}{\textbf{GC}}  
		& \textbf{$\DS_{token-indexing}$}~\citep{ho2023fusion} & \textbf{0.7193} & 0.4559 & \textbf{0.5542} & \textbf{0.5049} \\ 
		& \textbf{$\DS_{CuBERT}$} & 0.6191 & \textbf{0.4809} & 0.5277 & 0.4747 \\ 
		& \textbf{$\DS_{CodeBERT}$} & 0.7007 & 0.2798 & 0.3845 & 0.3270  \\
		& \textbf{$\DS_{code2vec}$} & 0.4647 & 0.5293 & 0.4780 & 0.4272 \\
		% & \textbf{\ES} & 0.6220 & \textbf{0.5867} & \textbf{0.5851} & \textbf{0.5429} \\ 
		\Xhline{1pt}
		
		\multirow{5}{*}{\textbf{DC}}  
		& \textbf{$\DS_{token-indexing}$}~\citep{ho2023fusion} & \textbf{0.9579} & 0.1476 & 0.2531 & 0.0439 \\ 
		& \textbf{$\DS_{CuBERT}$} & 0.9158 & 0.1389 & 0.2382 & 0.0281 \\ 
		& \textbf{$\DS_{CodeBERT}$} & 0.6916 & 0.2868 & 0.3802 & 0.2573  \\
		& \textbf{$\DS_{code2vec}$} & 0.6883 & \textbf{0.5132} & \textbf{0.5839} & \textbf{0.5185} \\ 
		% & \textbf{\ES} & 0.8123 & \textbf{0.5314} & \textbf{0.6393} & \textbf{0.5907} \\ 
		\Xhline{1pt}
	\end{tabular}
\end{table}
% LM --> CuBERT best, code2vec worst
% FE --> CuBERT best, code2vec worst
% GC --> token-indexing best, CodeBERT worst
% DC --> code2vec best, CuBERT worst

\anhho{Table~\ref{table:ESandDS} demonstrates the performance of \DS and \ES under their best configurations, showing that \ES outperforms \DS. For the LM code smell, \ES achieves improvements of 1.28\% in F1-Score and 1.07\% in MCC. Although \DS exhibits a 3.45\% higher precision, this comes at the expense of a 3.83\% lower recall. Similarly, for the GC code smell, \ES surpasses \DS by 3.09\% in F1-Score and 3.8\% in MCC. While \DS achieves a higher precision by 9.73\%, its recall is 13.09\% lower compared to \ES.}

\anhho{For the remaining code smells, \ES shows superior performance across all evaluation metrics. Notably, \ES outperforms \DS by 8.27\% in F1-Score and 5.5\% in MCC for the FE code smell, and by 7.22\% in F1-Score and 5.5\% in MCC for the DC code smell.}


\begin{table}[h!]
	%\footnotesize
	\scriptsize
	\caption{\anhho{Comparison of \DS and \ES models with optimal configurations.}}
	\label{table:ESandDS}
	\centering
	\vspace{.3cm}
	\begin{tabular}{clcccc}
		\Xhline{1pt}
		\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Smell}}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Model}}} & \multicolumn{4}{c}{\textbf{Metric}} \\ 
		\cline{3-6}
		\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{\textbf{P}} & \multicolumn{1}{c}{\textbf{R}} & \multicolumn{1}{c}{\textbf{F1}} & \multicolumn{1}{c}{\textbf{MCC}} \\ 
		\Xhline{1pt}
		\multirow{2}{*}{\textbf{LM}}
		& \textbf{\DS} & \textbf{0.8560} & 0.7490 & 0.7888 & 0.7673 \\ 
		& \textbf{\ES} & 0.8215 & \textbf{0.7873} & \textbf{0.8016} & \textbf{0.7780} \\ 
		\Xhline{1pt}
		
		\multirow{2}{*}{\textbf{FE}}  
		& \textbf{\DS} & 0.5936 & 0.2322 & 0.3198 & 0.3318 \\ 
		& \textbf{\ES} & \textbf{0.5215} & \textbf{0.3393} & \textbf{0.4025} & \textbf{0.3868} \\ 
		\Xhline{1pt}
		
		\multirow{2}{*}{\textbf{GC}}  
		& \textbf{\DS} & \textbf{0.7193} & 0.4559 & 0.5542 & 0.5049 \\ 
		& \textbf{\ES} & 0.6220 & \textbf{0.5867} & \textbf{0.5851} & \textbf{0.5429} \\ 
		\Xhline{1pt}
		
		\multirow{2}{*}{\textbf{DC}}  
		& \textbf{\DS} & 0.6883 & 0.5132 & 0.5839 & 0.5185 \\ 
		& \textbf{\ES} & \textbf{0.8123} & \textbf{0.5314} & \textbf{0.6393} & \textbf{0.5907} \\ 
		\Xhline{1pt}
	\end{tabular}
\end{table}









\begin{table}[h!]
	%\footnotesize
	\scriptsize
	\caption{Comparison with baseline models.}
	\label{table:ResultOfSOTA}
	\centering
	\vspace{.3cm}
	\begin{tabular}{clcccc}
		\Xhline{1pt}
		\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Smell}}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Model}}} & \multicolumn{4}{c}{\textbf{Metric}} \\ 
		\cline{3-6}
		\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{\textbf{P}} & \multicolumn{1}{c}{\textbf{R}} & \multicolumn{1}{c}{\textbf{F1}} & \multicolumn{1}{c}{\textbf{MCC}} \\ 
		\Xhline{1pt}
		\multirow{5}{*}{\textbf{LM}} 
		& \textbf{ML\_CuBERT}~\citep{kovavcevic2022automatic} & 0.6933 & 0.8142 & 0.7481 & 0.7182 \\ 
		& \textbf{AE-Dense}~\citep{sharma2021code} & 0.7432 & 0.8274 & 0.7804 & 0.7548 \\ 
		& \textbf{AE-CNN}~\citep{sharma2021code} & 0.7363 & 0.8315 & 0.7728 & 0.7500 \\
		& \textbf{AE-LSTM}~\citep{sharma2021code} & 0.6680 & \textbf{0.8480} & 0.7471 & 0.7187 \\ 
		& \textbf{\ES} & \textbf{0.8215} & 0.7873 & \textbf{0.8016} & \textbf{0.7780} \\ 
		\Xhline{1pt}
		
		\multirow{5}{*}{\textbf{FE}}  
		& \textbf{ML\_CuBERT}~\citep{kovavcevic2022automatic} & 0.1223 & \textbf{0.8033} & 0.2121 & 0.2683 \\ 
		& \textbf{AE-Dense}~\citep{sharma2021code} & 0.2325 & 0.6295 & 0.3356 & 0.3509 \\ 
		& \textbf{AE-CNN}~\citep{sharma2021code} & 0.2704 & 0.5333 & 0.3500 & 0.3478 \\
		& \textbf{AE-LSTM}~\citep{sharma2021code} & 0.1364 & 0.7987 & 0.2329 & 0.2888 \\ 
		& \textbf{\ES} & \textbf{0.5215} & 0.3393 & \textbf{0.4025} & \textbf{0.3868} \\ 
		\Xhline{1pt}
		
		\multirow{5}{*}{\textbf{GC}}  
		& \textbf{ML\_CuBERT}~\citep{kovavcevic2022automatic} & 0.4580 & 0.5763 & 0.5092 & 0.4492 \\ 
		& \textbf{AE-Dense}~\citep{sharma2021code} & 0.5495 & 0.6444 & 0.5751 & 0.5306 \\ 
		& \textbf{AE-CNN}~\citep{sharma2021code} & 0.5334 & \textbf{0.6622} & 0.5775 & 0.5311 \\
		& \textbf{AE-LSTM}~\citep{sharma2021code} & 0.5227 & 0.6490 & 0.5706 & 0.5200 \\ 
		& \textbf{\ES} & \textbf{0.6220} & 0.5867 & \textbf{0.5851} & \textbf{0.5429} \\ 
		\Xhline{1pt}
		
		\multirow{5}{*}{\textbf{DC}}  
		& \textbf{ML\_CuBERT}~\citep{kovavcevic2022automatic} & 0.4953 & 0.2853 & 0.3615 & 0.3081 \\ 
		& \textbf{AE-Dense}~\citep{sharma2021code} & 0.1559 & 0.8576 & 0.2613 & 0.1068 \\ 
		& \textbf{AE-CNN}~\citep{sharma2021code} & 0.1489 & \textbf{0.9184} & 0.2561 & 0.0977 \\
		& \textbf{AE-LSTM}~\citep{sharma2021code} & 0.1532 & 0.8328 & 0.2567 & 0.0922 \\ 
		& \textbf{\ES} & \textbf{0.8123} & 0.5314 & \textbf{0.6393} & \textbf{0.5907} \\ 
		\Xhline{1pt}
	\end{tabular}
\end{table}

Table~\ref{table:ResultOfSOTA} presents a comparison between \ES and the state-of-the-art approach, \textit{ML\_CuBERT} \citep{kovavcevic2022automatic}, and existing approaches that include various auto-encoder variants~\citep{sharma2021code}. The table demonstrates that our proposed model outperforms the other models by all evaluation metrics. Especially for LM, \ES exhibits the lowest recall value at 78.73\%, which is lower than the others by 2.69\% to 6.07\%. However, it boasts the highest precision value, surpassing the others by 7.83\% to 15.35\%. As a result, the overall performance demonstrates the highest F1-Score and MCC, with significant differences compared to the other models. Regarding the FE code smell, following a similar pattern LM, \ES showcases an overall outperformance, with substantial differences compared to ML\_CuBERT and the variants of autoencoder. The F1-Score varies from 5.25\% to 19.04\%, while the MCC differs from 3.59\% to 11.85\%.

In relation to the GC code smell, compared to AE-CNN, which shows the best performance among the baseline models with values of 53.34\% (precision), 66.22\% (recall), 57.75\% (F1-Score), and 53.133\% (MCC), \ES exhibits a slightly higher in F1-Score and MCC by 0.76\% and 1.18\%, respectively. Notably, the precision value of \ES is 8.86\% higher than AE-CNN, while the recall value is only 7.55\% lower.

\anhho{The remaining code smell is the DC}. The autoencoder variants exhibit a weakness in dealing with highly imbalanced datasets, showcasing very high recall values of up to 91.84\%, but the precision values hover around 15\%, resulting in the lowest overall performance. Moreover, \ES outperforms ML\_CuBERT in all four evaluation metrics by 31.7\% (precision), 24.61\% (recall), 27.78\% (F1-Score), and 28.26\% (MCC).

\vspace{.2cm}
\begin{tcolorbox}[boxrule=0.86pt,left=0.3em, right=0.3em,top=0.1em, bottom=0.05em]
	\textbf{Answer to RQ$_4$.} \rebuttal{%In real-world projects, 
		\ES demonstrates superior prediction performance for all four types of code smells compared to the state-of-the-art baselines. It gains a better accuracy compared to that of \DS, with improvements ranging from approximately 5\% to 10\%. 
		The MCC (overall performance) surpasses ML\_CuBERT by approximately 5.98\% \rev{to} 28.26\% and the best Autoencoder variants by 1.18\% \rev{to} 48.39\% across the code smell categories.}
\end{tcolorbox}
\vspace{-.2cm}




%\vspace{-.2cm}
% \section{Related Work}
% \label{sec:RelatedWork}
% \input{src/code_smell_detection/6_RelatedWork}




%\vspace{-.3cm}
\section{Threat to validity}\label{sec:threat}
% ================= Threats to validity =======================
%\subsection{Threats to validity}

\rebuttal{We anticipate that there are the following threats to the validity of our findings.}

\begin{itemize}[leftmargin=*]
\vspace{-0.3cm}\item \textbf{Internal validity.} \anhho{This concerns the extent to which our evaluation reflects real-world scenarios. In our work,  we used existing datasets~\citep{madeyski2020mlcq} curated and classified by human experts. Additionally, we assigned labels to each code snippet based on majority-based approach. However, these labels may not be universally accepted, which could impact the overall dataset quality and, consequently, the accuracy of our predictions. Furthermore, the dataset also exhibits class imbalance, mirroring real-world distributions but potentially biasing the model towards more common classes. To address this, we applied class-weight adjustments during training and used metrics designed for imbalanced data, ensuring a fair evaluation across all classes.}
\vspace{-0.3cm}\item \textbf{External validity.} This concerns the generalizability of the findings beyond the scope of this study. We attempted to mitigate threats by evaluating with different experimental configurations to simulate real-world scenarios. The findings of our work might apply only to the considered datasets. For other datasets, additional empirical evidence is required before reaching a final conclusion.
\vspace{-0.3cm}\item \textbf{Construct validity.} This dimension relates to how we set up our experiments to compare \ES with the baseline models. To ensure a fair comparison, we used the original implementations provided by the authors of the baseline models, maintaining their internal structures. 
Furthermore, our method involved measuring various software metrics using well-accepted open-source tools in our field. It is crucial to note that our study focused on the extensive MLCQ dataset, comprising a substantial number of projects (792). In such a vast dataset, there is a possibility of errors in metric calculations due to parsing issues. During our error analysis, our domain expert identified a few instances where unique word counts were miscalculated. 
Nevertheless, we have confidence that the use of established tools widely recognized in the research community helps minimize the impact of these potential errors.
\end{itemize}

%\vspace{-.5cm}

%\input{src/code_smell_detection/7_Conclusions}



\section{Conclusion}\label{sec:Conclusion}
%\vspace{-.2cm}
This paper introduces \ES, an innovative approach for detecting four types of code smells - two at the method level (\textit{Long Method} and \textit{Feature Envy}) and two at the class level (\textit{God Class} and \textit{Data Class}). Our methodology leverages a unique ensemble of two feature types: \textit{semantic features} derived from novel pre-trained programming language models and \textit{structural features} obtained through object-oriented metrics extracted by a static analysis tool. Building upon the foundation of our sophisticated model in a previous work, \DS, we conduct extensive experiments to address five research questions and assess the effectiveness of \ES. The results demonstrate that \ES outperforms existing methods, achieving state-of-the-art performance on the MLCQ dataset \anhho{with improvements ranging from 5.98\% to 28.26\%, depending on the type of code smell.}
Our future work involves expanding the application of this approach to additional code smells and identifying areas for improvement to further enhance its capabilities.




\section*{Acknowledgment}
\rebuttal{Our research has been funded by Hanoi University of Science and Technology (HUST), Vietnam under project number T2023-PC-002. 
This work has been supported by the COmmunity-Based Organized Littering (COBOL) national research project funded by the MUR under the PRIN 2022 PNRR program (nr. P20224K9EK), by the ``Progetto PE 0000020 CHANGES,  PNRR Missione 4 Componente 2 Investimento 1.3'' funded by EU - NextGenerationEU, by the European HORIZON-KDT-JU research project MATISSE ``Model-based engineering of Digital Twins for early verification and validation of Industrial Systems'', HORIZON-KDT-JU-2023-2-RIA, Proposal number:  101140216-2, KDT232RIA\_00017, and by the European Union - NextGenerationEU under the Italian Ministry of University and Research (MUR) National Innovation Ecosystem grant ECS00000041 - VITALITY â€“ CUP: D13C21000430001. We acknowledge the Italian ``PRIN 2022'' project TRex-SE: \emph{``Trustworthy Recommenders for Software Engineers,''} grant n. 2022LKJWHC. 
We thank the anonymous reviewers for their useful comments and suggestions that helped us improve our manuscript.}



%\vspace{-.2cm}
\balance
%\bibliographystyle{elsarticle-num}

\begin{comment}
	

\bibliographystyle{abbrvnat}
\bibliography{main}

\end{comment}




\begin{thebibliography}{54}
	\providecommand{\natexlab}[1]{#1}
	\providecommand{\url}[1]{\texttt{#1}}
	\expandafter\ifx\csname urlstyle\endcsname\relax
	\providecommand{\doi}[1]{doi: #1}\else
	\providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi
	
	\bibitem[Alon et~al.(2019)Alon, Zilberstein, Levy, and Yahav]{alon2019code2vec}
	U.~Alon, M.~Zilberstein, O.~Levy, and E.~Yahav.
	\newblock code2vec: Learning distributed representations of code.
	\newblock \emph{Proceedings of the ACM on Programming Languages}, 3\penalty0
	(POPL):\penalty0 1--29, 2019.
	\newblock \doi{10.1145/3290353}.
	\newblock URL \url{https://doi.org/10.1145/3290353}.
	
	\bibitem[Arcelli~Fontana et~al.(2016)Arcelli~Fontana, M{\"a}ntyl{\"a}, Zanoni,
	and Marino]{arcelli2016comparing}
	F.~Arcelli~Fontana, M.~V. M{\"a}ntyl{\"a}, M.~Zanoni, and A.~Marino.
	\newblock Comparing and experimenting machine learning techniques for code
	smell detection.
	\newblock \emph{Empirical Software Engineering}, 21:\penalty0 1143--1191, 2016.
	\newblock \doi{10.1007/S10664-015-9378-4}.
	\newblock URL \url{https://doi.org/10.1007/s10664-015-9378-4}.
	
	\bibitem[Azadi et~al.(2018)Azadi, Fontana, and Zanoni]{azadi2018poster}
	U.~Azadi, F.~A. Fontana, and M.~Zanoni.
	\newblock Machine learning based code smell detection through wekanose.
	\newblock In \emph{Proceedings of the 40th International Conference on Software
		Engineering: Companion Proceedings, {ICSE} 2018, Gothenburg, Sweden, May 27 -
		June 03, 2018}, pages 288--289. {ACM}, 2018.
	\newblock \doi{10.1145/3183440.3194974}.
	\newblock URL \url{https://doi.org/10.1145/3183440.3194974}.
	
	\bibitem[Azeem et~al.(2019)Azeem, Palomba, Shi, and Wang]{azeem2019machine}
	M.~I. Azeem, F.~Palomba, L.~Shi, and Q.~Wang.
	\newblock Machine learning techniques for code smell detection: A systematic
	literature review and meta-analysis.
	\newblock \emph{Information and Software Technology}, 108:\penalty0 115--138,
	2019.
	\newblock \doi{10.1016/J.INFSOF.2018.12.009}.
	\newblock URL \url{https://doi.org/10.1016/j.infsof.2018.12.009}.
	
	\bibitem[Bennett(2001)]{bennett2001can}
	D.~A. Bennett.
	\newblock How can i deal with missing data in my study?
	\newblock \emph{Australian and New Zealand journal of public health},
	25\penalty0 (5):\penalty0 464--469, 2001.
	\newblock \doi{https://doi.org/10.1111/j.1467-842X.2001.tb00294.x}.
	\newblock URL
	\url{https://www.sciencedirect.com/science/article/pii/S1326020023036488}.
	
	\bibitem[Compton et~al.(2020)Compton, Frank, Patros, and
	Koay]{compton2020embedding}
	R.~Compton, E.~Frank, P.~Patros, and A.~M.~Y. Koay.
	\newblock Embedding java classes with code2vec: Improvements from variable
	obfuscation.
	\newblock In \emph{{MSR} '20: 17th International Conference on Mining Software
		Repositories, Seoul, Republic of Korea, 29-30 June, 2020}, pages 243--253.
	{ACM}, 2020.
	\newblock \doi{10.1145/3379597.3387445}.
	\newblock URL \url{https://doi.org/10.1145/3379597.3387445}.
	
	\bibitem[Curtis et~al.(1979)Curtis, Sheppard, Milliman, Borst, and
	Love]{curtis1979measuring}
	B.~Curtis, S.~B. Sheppard, P.~Milliman, M.~A. Borst, and T.~Love.
	\newblock Measuring the psychological complexity of software maintenance tasks
	with the halstead and mccabe metrics.
	\newblock \emph{{IEEE} Trans. Software Eng.}, 5\penalty0 (2):\penalty0 96--104,
	1979.
	\newblock \doi{10.1109/TSE.1979.234165}.
	\newblock URL \url{https://doi.org/10.1109/TSE.1979.234165}.
	
	\bibitem[Das et~al.(2019)Das, Yadav, and Dhal]{das2019detecting}
	A.~K. Das, S.~Yadav, and S.~Dhal.
	\newblock Detecting code smells using deep learning.
	\newblock In \emph{TENCON 2019-2019 IEEE Region 10 Conference (TENCON)}, pages
	2081--2086. IEEE, 2019.
	\newblock \doi{10.1109/TENCON.2019.8929628}.
	\newblock URL \url{https://doi.org/10.1109/TENCON.2019.8929628}.
	
	\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
	J.~Devlin, M.~Chang, K.~Lee, and K.~Toutanova.
	\newblock {BERT:} pre-training of deep bidirectional transformers for language
	understanding.
	\newblock In \emph{Proceedings of the 2019 Conference of the North American
		Chapter of the Association for Computational Linguistics: Human Language
		Technologies, {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume
		1 (Long and Short Papers)}, pages 4171--4186. Association for Computational
	Linguistics, 2019.
	\newblock \doi{10.18653/V1/N19-1423}.
	\newblock URL \url{https://doi.org/10.18653/v1/n19-1423}.
	
	\bibitem[Feng et~al.(2020)Feng, Guo, Tang, Duan, Feng, Gong, Shou, Qin, Liu,
	Jiang, and Zhou]{feng2020codebert}
	Z.~Feng, D.~Guo, D.~Tang, N.~Duan, X.~Feng, M.~Gong, L.~Shou, B.~Qin, T.~Liu,
	D.~Jiang, and M.~Zhou.
	\newblock Codebert: {A} pre-trained model for programming and natural
	languages.
	\newblock In \emph{Findings of the Association for Computational Linguistics:
		{EMNLP} 2020, Online Event, 16-20 November 2020}, volume {EMNLP} 2020 of
	\emph{Findings of {ACL}}, pages 1536--1547. Association for Computational
	Linguistics, 2020.
	\newblock \doi{10.18653/V1/2020.FINDINGS-EMNLP.139}.
	\newblock URL \url{https://doi.org/10.18653/v1/2020.findings-emnlp.139}.
	
	\bibitem[Fontana and Zanoni(2017)]{fontana2017code}
	F.~A. Fontana and M.~Zanoni.
	\newblock Code smell severity classification using machine learning techniques.
	\newblock \emph{Knowledge-Based Systems}, 128:\penalty0 43--58, 2017.
	\newblock \doi{10.1016/J.KNOSYS.2017.04.014}.
	\newblock URL \url{https://doi.org/10.1016/j.knosys.2017.04.014}.
	
	\bibitem[Fowler(1999)]{fowler1999}
	M.~Fowler.
	\newblock \emph{Refactoring: Improving the Design of Existing Code}.
	\newblock Addison-Wesley Longman Publishing Co., Inc., USA, 1999.
	\newblock ISBN 0201485672.
	\newblock URL \url{http://martinfowler.com/books/refactoring.html}.
	
	\bibitem[Garcia et~al.(2009)Garcia, Popescu, Edwards, and
	Medvidovic]{garcia2009identifying}
	J.~Garcia, D.~Popescu, G.~Edwards, and N.~Medvidovic.
	\newblock Identifying architectural bad smells.
	\newblock In \emph{13th European Conference on Software Maintenance and
		Reengineering, {CSMR} 2009, Architecture-Centric Maintenance of Large-SCale
		Software Systems, Kaiserslautern, Germany, 24-27 March 2009}, pages 255--258.
	{IEEE} Computer Society, 2009.
	\newblock \doi{10.1109/CSMR.2009.59}.
	\newblock URL \url{https://doi.org/10.1109/CSMR.2009.59}.
	
	\bibitem[Hadj{-}Kacem and Bouassida(2018)]{hadj2018hybrid}
	M.~Hadj{-}Kacem and N.~Bouassida.
	\newblock A hybrid approach to detect code smells using deep learning.
	\newblock In \emph{Proceedings of the 13th International Conference on
		Evaluation of Novel Approaches to Software Engineering, {ENASE} 2018,
		Funchal, Madeira, Portugal, March 23-24, 2018}, pages 137--146. SciTePress,
	2018.
	\newblock \doi{10.5220/0006709801370146}.
	\newblock URL \url{https://doi.org/10.5220/0006709801370146}.
	
	\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
	K.~He, X.~Zhang, S.~Ren, and J.~Sun.
	\newblock Deep residual learning for image recognition.
	\newblock In \emph{2016 {IEEE} Conference on Computer Vision and Pattern
		Recognition, {CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016}, pages
	770--778. {IEEE} Computer Society, 2016.
	\newblock \doi{10.1109/CVPR.2016.90}.
	\newblock URL \url{https://doi.org/10.1109/CVPR.2016.90}.
	
	\bibitem[Ho et~al.(2022)Ho, Hai, and Anh]{ho2022combining}
	A.~Ho, N.~N. Hai, and B.~T.~M. Anh.
	\newblock Combining deep learning and kernel {PCA} for software defect
	prediction.
	\newblock In \emph{The 11th International Symposium on Information and
		Communication Technology, SoICT 2022, Hanoi, Vietnam, December 1-3, 2022},
	pages 360--367. {ACM}, 2022.
	\newblock \doi{10.1145/3568562.3568587}.
	\newblock URL \url{https://doi.org/10.1145/3568562.3568587}.
	
	\bibitem[Ho et~al.(2023)Ho, Bui, Nguyen, and {Di Salle}]{ho2023fusion}
	A.~Ho, A.~M.~T. Bui, P.~T. Nguyen, and A.~{Di Salle}.
	\newblock Fusion of deep convolutional and {LSTM} recurrent neural networks for
	automated detection of code smells.
	\newblock In \emph{Proceedings of the 27th International Conference on
		Evaluation and Assessment in Software Engineering, {EASE} 2023, Oulu,
		Finland, June 14-16, 2023}, pages 229--234. {ACM}, 2023.
	\newblock \doi{10.1145/3593434.3593476}.
	\newblock URL \url{https://doi.org/10.1145/3593434.3593476}.
	
	\bibitem[Ioffe and Szegedy(2015)]{ioffe2015batch}
	S.~Ioffe and C.~Szegedy.
	\newblock Batch normalization: Accelerating deep network training by reducing
	internal covariate shift.
	\newblock In F.~R. Bach and D.~M. Blei, editors, \emph{Proceedings of the 32nd
		International Conference on Machine Learning, {ICML} 2015, Lille, France,
		6-11 July 2015}, volume~37 of \emph{{JMLR} Workshop and Conference
		Proceedings}, pages 448--456. JMLR.org, 2015.
	\newblock URL \url{http://proceedings.mlr.press/v37/ioffe15.html}.
	
	\bibitem[Kamiya et~al.(2002)Kamiya, Kusumoto, and Inoue]{kamiya2002ccfinder}
	T.~Kamiya, S.~Kusumoto, and K.~Inoue.
	\newblock Ccfinder: {A} multilinguistic token-based code clone detection system
	for large scale source code.
	\newblock \emph{{IEEE} Trans. Software Eng.}, 28\penalty0 (7):\penalty0
	654--670, 2002.
	\newblock \doi{10.1109/TSE.2002.1019480}.
	\newblock URL \url{https://doi.org/10.1109/TSE.2002.1019480}.
	
	\bibitem[Kanade et~al.(2020)Kanade, Maniatis, Balakrishnan, and
	Shi]{kanade2020learning}
	A.~Kanade, P.~Maniatis, G.~Balakrishnan, and K.~Shi.
	\newblock Learning and evaluating contextual embedding of source code.
	\newblock In \emph{Proceedings of the 37th International Conference on Machine
		Learning, {ICML} 2020, 13-18 July 2020, Virtual Event}, volume 119 of
	\emph{Proceedings of Machine Learning Research}, pages 5110--5121. {PMLR},
	2020.
	\newblock URL \url{http://proceedings.mlr.press/v119/kanade20a.html}.
	
	\bibitem[Khomh et~al.(2011)Khomh, Vaucher, Gu{\'{e}}h{\'{e}}neuc, and
	Sahraoui]{khomh2011bdtex}
	F.~Khomh, S.~Vaucher, Y.~Gu{\'{e}}h{\'{e}}neuc, and H.~A. Sahraoui.
	\newblock {BDTEX:} {A} gqm-based bayesian approach for the detection of
	antipatterns.
	\newblock \emph{J. Syst. Softw.}, 84\penalty0 (4):\penalty0 559--572, 2011.
	\newblock \doi{10.1016/J.JSS.2010.11.921}.
	\newblock URL \url{https://doi.org/10.1016/j.jss.2010.11.921}.
	
	\bibitem[Kova{\v{c}}evi{\'c} et~al.(2022)Kova{\v{c}}evi{\'c}, Slivka,
	Vidakovi{\'c}, Gruji{\'c}, Luburi{\'c}, Proki{\'c}, and
	Sladi{\'c}]{kovavcevic2022automatic}
	A.~Kova{\v{c}}evi{\'c}, J.~Slivka, D.~Vidakovi{\'c}, K.-G. Gruji{\'c},
	N.~Luburi{\'c}, S.~Proki{\'c}, and G.~Sladi{\'c}.
	\newblock Automatic detection of long method and god class code smells through
	neural source code embeddings.
	\newblock \emph{Expert Systems with Applications}, 204:\penalty0 117607, 2022.
	\newblock \doi{10.1016/J.ESWA.2022.117607}.
	\newblock URL \url{https://doi.org/10.1016/j.eswa.2022.117607}.
	
	\bibitem[Lanza and Marinescu(2006)]{lanza2006characterizing}
	M.~Lanza and R.~Marinescu.
	\newblock \emph{Object-Oriented Metrics in Practice - Using Software Metrics to
		Characterize, Evaluate, and Improve the Design of Object-Oriented Systems}.
	\newblock Springer, 2006.
	\newblock ISBN 978-3-540-24429-5.
	\newblock \doi{10.1007/3-540-39538-5}.
	\newblock URL \url{https://doi.org/10.1007/3-540-39538-5}.
	
	\bibitem[Liu et~al.(2016)Liu, Liu, Niu, and Liu]{liu2015dynamic}
	H.~Liu, Q.~Liu, Z.~Niu, and Y.~Liu.
	\newblock Dynamic and automatic feedback-based threshold adaptation for code
	smell detection.
	\newblock \emph{{IEEE} Trans. Software Eng.}, 42\penalty0 (6):\penalty0
	544--558, 2016.
	\newblock \doi{10.1109/TSE.2015.2503740}.
	\newblock URL \url{https://doi.org/10.1109/TSE.2015.2503740}.
	
	\bibitem[Liu et~al.(2021)Liu, Jin, Xu, Zou, Bu, and Zhang]{liu2019deep}
	H.~Liu, J.~Jin, Z.~Xu, Y.~Zou, Y.~Bu, and L.~Zhang.
	\newblock Deep learning based code smell detection.
	\newblock \emph{{IEEE} Trans. Software Eng.}, 47\penalty0 (9):\penalty0
	1811--1837, 2021.
	\newblock \doi{10.1109/TSE.2019.2936376}.
	\newblock URL \url{https://doi.org/10.1109/TSE.2019.2936376}.
	
	\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
	Zettlemoyer, and Stoyanov]{liu2019roberta}
	Y.~Liu, M.~Ott, N.~Goyal, J.~Du, M.~Joshi, D.~Chen, O.~Levy, M.~Lewis,
	L.~Zettlemoyer, and V.~Stoyanov.
	\newblock Roberta: A robustly optimized bert pretraining approach.
	\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.
	\newblock URL \url{http://arxiv.org/abs/1907.11692}.
	
	\bibitem[Ma et~al.(2024)Ma, Liu, Zhao, Xie, Wang, Hu, Zhang, and
	Liu]{ma2024unveiling}
	W.~Ma, S.~Liu, M.~Zhao, X.~Xie, W.~Wang, Q.~Hu, J.~Zhang, and Y.~Liu.
	\newblock Unveiling code pre-trained models: Investigating syntax and semantics
	capacities.
	\newblock \emph{ACM Trans. Softw. Eng. Methodol.}, 33\penalty0 (7), Aug. 2024.
	\newblock ISSN 1049-331X.
	\newblock \doi{10.1145/3664606}.
	\newblock URL \url{https://doi.org/10.1145/3664606}.
	
	\bibitem[Macia et~al.(2010)Macia, Garcia, and von Staa]{macia2010defining}
	I.~Macia, A.~Garcia, and A.~von Staa.
	\newblock Defining and applying detection strategies for aspect-oriented code
	smells.
	\newblock In \emph{2010 Brazilian Symposium on Software Engineering}, pages
	60--69. IEEE, 2010.
	\newblock \doi{10.1109/SBES.2010.14}.
	\newblock URL \url{https://doi.org/10.1109/SBES.2010.14}.
	
	\bibitem[Madeyski and Lewowski(2020)]{madeyski2020mlcq}
	L.~Madeyski and T.~Lewowski.
	\newblock {MLCQ:} industry-relevant code smell data set.
	\newblock In \emph{{EASE} '20: Evaluation and Assessment in Software
		Engineering, Trondheim, Norway, April 15-17, 2020}, pages 342--347. {ACM},
	2020.
	\newblock \doi{10.1145/3383219.3383264}.
	\newblock URL \url{https://doi.org/10.1145/3383219.3383264}.
	
	\bibitem[Madeyski and Lewowski(2023)]{madeyski2023detecting}
	L.~Madeyski and T.~Lewowski.
	\newblock Detecting code smells using industry-relevant data.
	\newblock \emph{Information and Software Technology}, 155:\penalty0 107112,
	2023.
	\newblock \doi{10.1016/J.INFSOF.2022.107112}.
	\newblock URL \url{https://doi.org/10.1016/j.infsof.2022.107112}.
	
	\bibitem[Maiga et~al.(2012{\natexlab{a}})Maiga, Ali, Bhattacharya, Sabane,
	Gu{\'{e}}h{\'{e}}neuc, and A{\"{\i}}meur]{maiga2012smurf}
	A.~Maiga, N.~Ali, N.~Bhattacharya, A.~Sabane, Y.~Gu{\'{e}}h{\'{e}}neuc, and
	E.~A{\"{\i}}meur.
	\newblock {SMURF:} {A} svm-based incremental anti-pattern detection approach.
	\newblock In \emph{19th Working Conference on Reverse Engineering, {WCRE} 2012,
		Kingston, ON, Canada, October 15-18, 2012}, pages 466--475. {IEEE} Computer
	Society, 2012{\natexlab{a}}.
	\newblock \doi{10.1109/WCRE.2012.56}.
	\newblock URL \url{https://doi.org/10.1109/WCRE.2012.56}.
	
	\bibitem[Maiga et~al.(2012{\natexlab{b}})Maiga, Ali, Bhattacharya, Sabane,
	Gu{\'{e}}h{\'{e}}neuc, Antoniol, and A{\"{\i}}meur]{maiga2012support}
	A.~Maiga, N.~Ali, N.~Bhattacharya, A.~Sabane, Y.~Gu{\'{e}}h{\'{e}}neuc,
	G.~Antoniol, and E.~A{\"{\i}}meur.
	\newblock Support vector machines for anti-pattern detection.
	\newblock In \emph{{IEEE/ACM} International Conference on Automated Software
		Engineering, ASE'12, Essen, Germany, September 3-7, 2012}, pages 278--281.
	{ACM}, 2012{\natexlab{b}}.
	\newblock \doi{10.1145/2351676.2351723}.
	\newblock URL \url{https://doi.org/10.1145/2351676.2351723}.
	
	\bibitem[Marinescu(2004)]{marinescu2004detection}
	R.~Marinescu.
	\newblock Detection strategies: Metrics-based rules for detecting design flaws.
	\newblock In \emph{20th International Conference on Software Maintenance
		{(ICSM} 2004), 11-17 September 2004, Chicago, IL, {USA}}, pages 350--359.
	{IEEE} Computer Society, 2004.
	\newblock \doi{10.1109/ICSM.2004.1357820}.
	\newblock URL \url{https://doi.org/10.1109/ICSM.2004.1357820}.
	
	\bibitem[Marinescu(2005)]{marinescu2005measurement}
	R.~Marinescu.
	\newblock Measurement and quality in object-oriented design.
	\newblock In \emph{21st {IEEE} International Conference on Software Maintenance
		{(ICSM} 2005), 25-30 September 2005, Budapest, Hungary}, pages 701--704.
	{IEEE} Computer Society, 2005.
	\newblock \doi{10.1109/ICSM.2005.63}.
	\newblock URL \url{https://doi.org/10.1109/ICSM.2005.63}.
	
	\bibitem[Marinescu et~al.(2010)Marinescu, Ganea, and
	Verebi]{marinescu2010incode}
	R.~Marinescu, G.~Ganea, and I.~Verebi.
	\newblock Incode: Continuous quality assessment and improvement.
	\newblock In \emph{14th European Conference on Software Maintenance and
		Reengineering, {CSMR} 2010, 15-18 March 2010, Madrid, Spain}, pages 274--275.
	{IEEE} Computer Society, 2010.
	\newblock \doi{10.1109/CSMR.2010.44}.
	\newblock URL \url{https://doi.org/10.1109/CSMR.2010.44}.
	
	\bibitem[McCabe(1976)]{mccabe1976complexity}
	T.~J. McCabe.
	\newblock A complexity measure.
	\newblock \emph{IEEE Transactions on software Engineering}, \penalty0
	(4):\penalty0 308--320, 1976.
	\newblock \doi{10.1109/TSE.1976.233837}.
	\newblock URL \url{https://doi.org/10.1109/TSE.1976.233837}.
	
	\bibitem[Moha et~al.(2009)Moha, Gu{\'e}h{\'e}neuc, Duchien, and
	Le~Meur]{moha2009decor}
	N.~Moha, Y.-G. Gu{\'e}h{\'e}neuc, L.~Duchien, and A.-F. Le~Meur.
	\newblock Decor: A method for the specification and detection of code and
	design smells.
	\newblock \emph{IEEE Transactions on Software Engineering}, 36\penalty0
	(1):\penalty0 20--36, 2009.
	\newblock \doi{10.1109/TSE.2009.50}.
	\newblock URL \url{https://doi.org/10.1109/TSE.2009.50}.
	
	\bibitem[Munro(2005)]{munro2005product}
	M.~J. Munro.
	\newblock Product metrics for automatic identification of "bad smell" design
	problems in java source-code.
	\newblock In \emph{11th {IEEE} International Symposium on Software Metrics
		{(METRICS} 2005), 19-22 September 2005, Como Italy}, page~15. {IEEE} Computer
	Society, 2005.
	\newblock \doi{10.1109/METRICS.2005.38}.
	\newblock URL \url{https://doi.org/10.1109/METRICS.2005.38}.
	
	\bibitem[Nair and Hinton(2010)]{nair2010rectified}
	V.~Nair and G.~E. Hinton.
	\newblock Rectified linear units improve restricted boltzmann machines.
	\newblock In \emph{Proceedings of the 27th International Conference on Machine
		Learning (ICML-10), June 21-24, 2010, Haifa, Israel}, pages 807--814.
	Omnipress, 2010.
	\newblock URL \url{https://icml.cc/Conferences/2010/papers/432.pdf}.
	
	\bibitem[Nguyen et~al.(2024)Nguyen, Treude, and
	Thongtanunam]{nguyen2024encoding}
	H.~Nguyen, C.~Treude, and P.~Thongtanunam.
	\newblock Encoding version history context for better code representation.
	\newblock In \emph{Proceedings of the 21st International Conference on Mining
		Software Repositories}, pages 631--636, 2024.
	
	\bibitem[Sales et~al.(2013)Sales, Terra, Miranda, and
	Valente]{sales2013recommending}
	V.~Sales, R.~Terra, L.~F. Miranda, and M.~T. Valente.
	\newblock Recommending move method refactorings using dependency sets.
	\newblock In \emph{20th Working Conference on Reverse Engineering, {WCRE} 2013,
		Koblenz, Germany, October 14-17, 2013}, pages 232--241. {IEEE} Computer
	Society, 2013.
	\newblock \doi{10.1109/WCRE.2013.6671298}.
	\newblock URL \url{https://doi.org/10.1109/WCRE.2013.6671298}.
	
	\bibitem[Schafer(1997)]{schafer1997analysis}
	J.~L. Schafer.
	\newblock Analysis of incomplete multivariate data.
	\newblock \emph{Chapman \&Hall/CRC}, 1997.
	\newblock URL \url{https://doi.org/10.1201/9780367803025}.
	
	\bibitem[Sharma and Spinellis(2018)]{sharma2018survey}
	T.~Sharma and D.~Spinellis.
	\newblock A survey on software smells.
	\newblock \emph{Journal of Systems and Software}, 138:\penalty0 158--173, 2018.
	\newblock \doi{10.1016/J.JSS.2017.12.034}.
	\newblock URL \url{https://doi.org/10.1016/j.jss.2017.12.034}.
	
	\bibitem[Sharma et~al.(2016)Sharma, Mishra, and Tiwari]{sharma2016designite}
	T.~Sharma, P.~Mishra, and R.~Tiwari.
	\newblock Designite: a software design quality assessment tool.
	\newblock In \emph{Proceedings of the 1st International Workshop on Bringing
		Architectural Design Thinking into Developers' Daily Activities, BRIDGE@ICSE
		2016, Austin, Texas, USA, May 17, 2016}, pages 1--4. {ACM}, 2016.
	\newblock \doi{10.1145/2896935.2896938}.
	\newblock URL \url{https://doi.org/10.1145/2896935.2896938}.
	
	\bibitem[Sharma et~al.(2021)Sharma, Efstathiou, Louridas, and
	Spinellis]{sharma2021code}
	T.~Sharma, V.~Efstathiou, P.~Louridas, and D.~Spinellis.
	\newblock Code smell detection by deep direct-learning and transfer-learning.
	\newblock \emph{Journal of Systems and Software}, 176:\penalty0 110936, 2021.
	\newblock \doi{https://doi.org/10.1016/j.jss.2021.110936}.
	\newblock URL
	\url{https://www.sciencedirect.com/science/article/pii/S0164121221000339}.
	
	\bibitem[Sikstr{\"o}m and Garcia(2020)]{sikstrom2020statistical}
	S.~Sikstr{\"o}m and D.~Garcia.
	\newblock Statistical semantics.
	\newblock \emph{Methods and Applications, Cham: Springer}, 2020.
	\newblock \doi{10.1145/3664606}.
	
	\bibitem[Singh and Kaur(2018)]{singh2018systematic}
	S.~Singh and S.~Kaur.
	\newblock A systematic literature review: Refactoring for disclosing code
	smells in object oriented software.
	\newblock \emph{Ain Shams Engineering Journal}, 9\penalty0 (4):\penalty0
	2129--2151, 2018.
	\newblock \doi{https://doi.org/10.1016/j.asej.2017.03.002}.
	\newblock URL
	\url{https://www.sciencedirect.com/science/article/pii/S2090447917300412}.
	
	\bibitem[Suryanarayana et~al.(2014)Suryanarayana, Samarthyam, and
	Sharma]{suryanarayana2014refactoring}
	G.~Suryanarayana, G.~Samarthyam, and T.~Sharma.
	\newblock \emph{Refactoring for software design smells: managing technical
		debt}.
	\newblock Morgan Kaufmann, 2014.
	\newblock \doi{https://doi.org/10.1016/j.asej.2017.03.002}.
	\newblock URL
	\url{https://www.sciencedirect.com/science/article/pii/S2090447917300412}.
	
	\bibitem[Van~Rompaey et~al.(2007)Van~Rompaey, Du~Bois, Demeyer, and
	Rieger]{van2007detection}
	B.~Van~Rompaey, B.~Du~Bois, S.~Demeyer, and M.~Rieger.
	\newblock On the detection of test smells: A metrics-based approach for general
	fixture and eager test.
	\newblock \emph{IEEE Transactions on Software Engineering}, 33\penalty0
	(12):\penalty0 800--817, 2007.
	\newblock \doi{10.1109/TSE.2007.70745}.
	\newblock URL \url{https://doi.org/10.1109/TSE.2007.70745}.
	
	\bibitem[Vidal et~al.(2015)Vidal, V{\'{a}}zquez, Pace, Marcos, Garcia, and
	Oizumi]{vidal2015jspirit}
	S.~A. Vidal, H.~C. V{\'{a}}zquez, J.~A.~D. Pace, C.~A. Marcos, A.~F. Garcia,
	and W.~N. Oizumi.
	\newblock Jspirit: a flexible tool for the analysis of code smells.
	\newblock In \emph{34th International Conference of the Chilean Computer
		Science Society, {SCCC} 2015, Santiago, Chile, November 9-13, 2015}, pages
	1--6. {IEEE}, 2015.
	\newblock \doi{10.1109/SCCC.2015.7416572}.
	\newblock URL \url{https://doi.org/10.1109/SCCC.2015.7416572}.
	
	\bibitem[Von~der Mosel et~al.(2022)Von~der Mosel, Trautsch, and
	Herbold]{von2022validity}
	J.~Von~der Mosel, A.~Trautsch, and S.~Herbold.
	\newblock On the validity of pre-trained transformers for natural language
	processing in the software engineering domain.
	\newblock \emph{IEEE Transactions on Software Engineering}, 49\penalty0
	(4):\penalty0 1487--1507, 2022.
	\newblock \doi{10.1109/TSE.2022.3178469}.
	\newblock URL \url{https://doi.org/10.1109/TSE.2022.3178469}.
	
	\bibitem[Xu et~al.(2019)Xu, Liu, Luo, Yang, Zhang, Yuan, Tang, and
	Zhang]{xu2019software}
	Z.~Xu, J.~Liu, X.~Luo, Z.~Yang, Y.~Zhang, P.~Yuan, Y.~Tang, and T.~Zhang.
	\newblock Software defect prediction based on kernel pca and weighted extreme
	learning machine.
	\newblock \emph{Information and Software Technology}, 106:\penalty0 182--200,
	2019.
	\newblock \doi{10.1016/j.infsof.2018.10.004}.
	\newblock URL \url{https://doi.org/10.1016/j.infsof.2018.10.004}.
	
	\bibitem[Zhang et~al.(2011)Zhang, Hall, and Baddoo]{zhang2011code}
	M.~Zhang, T.~Hall, and N.~Baddoo.
	\newblock Code bad smells: a review of current knowledge.
	\newblock \emph{Journal of Software Maintenance and Evolution: research and
		practice}, 23\penalty0 (3):\penalty0 179--202, 2011.
	\newblock \doi{10.1002/SMR.521}.
	\newblock URL \url{https://doi.org/10.1002/smr.521}.
	
	\bibitem[Zhang et~al.(2022)Zhang, Ge, Hong, Tian, Dong, and
	Liu]{ZHANG2022109737}
	Y.~Zhang, C.~Ge, S.~Hong, R.~Tian, C.~Dong, and J.~Liu.
	\newblock Delesmell: Code smell detection based on deep learning and latent
	semantic analysis.
	\newblock \emph{Knowledge-Based Systems}, 255:\penalty0 109737, 2022.
	\newblock \doi{10.1016/J.KNOSYS.2022.109737}.
	\newblock URL \url{https://doi.org/10.1016/j.knosys.2022.109737}.
	
\end{thebibliography}





\newpage

%\input{src/code_smell_detection/8_Appendix}


\centering \appendix \section{The Information of Software Metrics at Method and Class Level}
\label{sec:Appendix}


\begin{table}[ht]
	\caption{The abbreviation and software metric description for method level.}
	\label{table:DescriptionMethodLevel}
	\tiny
	\centering
	\begin{tabularx}{\textwidth}{>{\hsize=.25\hsize}X>{\hsize=.75\hsize}X}
		\Xhline{0.3pt}
		\textbf{Abbreviation}          & \textbf{Software Metric Description}          \\
		\endhead
		\Xhline{0.3pt}
		\hline
		LOC                            & Lines of code (excluding empty lines and comments). \\
		\hline
		CBO                            & Coupling between objects: measures the number of classes or methods a given method depends on, considering parameters, return types, local variables, and method calls, excluding dependencies on Java standard libraries (e.g., java.lang.String). \\
		\hline
		WMC                            & Weighted method complexity: counts the number of branch instructions (e.g., decision points like if, for, while) within methods, based on McCabe's complexity. \\
		\hline
		RFC                            & Response for a class: counts the unique method invocations within a method. \\
		\hline
		modifiers                      & The modifiers of methods, including public, private, abstract, protected, and native. \\
		\hline
		constructor                    & Indicates whether the method is a constructor. \\
		\hline
		logStatementsQty               & The number of log statements. \\
		\hline
		returnsQty                     & The number of return instructions. \\
		\hline
		variablesQty                   & The number of variables declared. \\
		\hline
		parametersQty                  & The number of parameters. \\
		\hline
		methodsInvokedQty              & The total number of methods directly invoked, including local and indirect local invocations. \\
		\hline
		methodsInvokedLocalQty         & The number of methods invoked locally within the method. \\
		\hline
		methodsInvokedIndirectLocalQty & The number of indirectly invoked methods within the method. \\
		\hline
		loopQty                        & The number of loops (e.g., for, while, do-while) within methods. \\
		\hline
		comparisonsQty                 & The number of comparison operations (e.g., ==, !=, <, >) within methods. \\
		\hline
		tryCatchQty                    & The number of try/catch blocks within methods. \\
		\hline
		parenthesizedExpsQty           & The number of expressions inside parentheses within methods. \\
		\hline
		stringLiteralsQty              & The number of string literals used in methods. \\
		\hline
		numbersQty                     & The number of numerical literals (e.g., int, long, double, float) used in methods. \\
		\hline
		assignmentsQty                 & The number of assignment operations within methods. \\
		\hline
		mathOperationsQty              & The number of mathematical operations (e.g., +, -, *, /) within methods. \\
		\hline
		maxNestedBlocksQty             & The maximum depth of nested blocks (e.g., loops, conditionals) within methods. \\
		\hline
		anonymousClassesQty            & The number of anonymous classes used within methods. \\
		\hline
		innerClassesQty                & The number of inner classes defined within methods. \\
		\hline
		lambdasQty                     & The number of lambda expressions used in methods. \\
		\hline
		uniqueWordsQty                 & The number of unique words (e.g., identifiers, keywords) used in methods. \\
		\hline
		\Xhline{0.3pt}
	\end{tabularx}
\end{table}



\begin{table}[]
	\caption{The abbreviation and software metric description for class level.}
	\label{table:DescriptionClassLevel}
	\tiny
	\centering
	\begin{tabularx}{\textwidth}{>{\hsize=.2\hsize}X>{\hsize=.8\hsize}X}
		\Xhline{0.3pt}
		\textbf{Abbreviation}           & \textbf{Software Metric Description}              \\
		\endhead
		\Xhline{0.3pt}
		\hline
		CBO                    & Coupling between objects measures the number of dependencies a class has, considering all types used within the class (fields, method return types, variables, etc.), excluding dependencies on Java standard libraries (e.g., java.lang.String). \\
		\hline
		DIT                    & Depth inheritance tree counts the number of parent classes a class has. The minimum DIT is 1 (all classes inherit \texttt{java.lang.Object}). If a class depends on an external dependency (e.g., a JAR file), the depth is counted as 2.\\
		\hline
		WMC                    & Weight method class measures the complexity of a class by counting the number of branch instructions (e.g., decision points like if, for, while) within its methods, based on McCabe's complexity.\\
		\hline
		TCC                    & Tight class cohesion measures the cohesion of a class (within a range of 0 to 1) based on direct connections between visible methods, where methods or their invocation trees access the same class variable. \\
		\hline
		LCC                    & Loose class cohesion extends TCC by including indirect connections between visible classes, ensuring LCC $\geq$ TCC. \\
		\hline
		LCOM                   & Lack of cohesion of methods measures the cohesion within a class. Higher cohesion indicates a well-structured class, while lower cohesion suggests the class may handle multiple responsibilities. \\
		\hline
		LOC                    & Lines of code (excluding empty lines and comments). \\
		\hline
		NOSI                   & The number of static invocations counts the number of invocations to static methods. \\
		\hline
		RFC                    & Response for a class counts unique method invocations in a class. \\
		\hline
		abstractMethodsQty     & The number of abstract methods. \\
		\hline
		anonymousClassesQty    & The number of anonymous classes. \\
		\hline
		assignmentsQty         & The number of assignments. \\
		\hline
		comparisonsQty         & The number of comparisons. \\
		\hline
		defaultFieldsQty       & The number of default fields. \\
		\hline
		defaultMethodsQty      & The number of default methods. \\
		\hline
		finalFieldsQty         & The number of final fields. \\
		\hline
		finalMethodsQty        & The number of final methods. \\
		\hline
		innerClassesQty        & The number of inner classes. \\
		\hline
		lambdasQty             & The number of lambda expressions. \\
		\hline
		logStatementsQty       & The number of log statements. \\
		\hline
		loopQty                & The number of loops (i.e., for, while, do while). \\
		\hline
		mathOperationsQty      & The number of math operations. \\
		\hline
		maxNestedBlocksQty     & The number of maximum nested blocks. \\
		\hline
		modifiers              & The modifiers of classes include public, abstract, private, protected, or native. \\
		\hline
		privateFieldsQty       & The number of private fields. \\
		\hline
		privateMethodsQty      & The number of private methods. \\
		\hline
		protectedFieldsQty     & The number of protected fields. \\
		\hline
		protectedMethodsQty    & The number of protected methods. \\
		\hline
		publicFieldsQty        & The number of public fields. \\
		\hline
		publicMethodsQty       & The number of public methods. \\
		\hline
		returnQty              & The number of return instructions. \\
		\hline
		staticFieldsQty        & The number of static fields. \\
		\hline
		staticMethodsQty       & The number of static methods. \\
		\hline
		stringLiteralsQty      & The number of string literals. \\
		\hline
		synchronizedFieldsQty  & The number of synchronized fields. \\
		\hline
		synchronizedMethodsQty & The number of synchronized methods. \\
		\hline
		totalFieldsQty         & The total number of fields. \\
		\hline
		totalMethodsQty        & The total number of methods. \\
		\hline
		tryCatchQty            & The number of try/catch blocks. \\
		\hline
		visibleFieldsQty       & The number of visible fields. \\
		\hline
		numbersQty             & The number of numerical literals (e.g., int, long, double, float). \\
		\hline
		parenthesizedExpsQty   & The number of expressions inside parenthesis. \\
		\hline
		uniqueWordsQty         & The number of unique words. \\
		\hline
		variablesQty           & The number of variables. \\
		
		\Xhline{0.3pt}
	\end{tabularx}
\end{table}




	
\end{document}
	