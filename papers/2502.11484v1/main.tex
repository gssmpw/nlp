\documentclass{article}

\usepackage{arxiv}

\usepackage{authblk}


\usepackage{amsmath,amsfonts,amssymb}
\usepackage{graphicx}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{subfig}
\usepackage[dvipsnames]{xcolor}
\definecolor{bg_blue}{HTML}{0B3C5D}
\definecolor{bg_red}{HTML}{B82601}
\definecolor{bg_green}{HTML}{1C6B0A}
\definecolor{bg_light_blue}{HTML}{328CC1}
\definecolor{bg_light_grey}{HTML}{A8B6C1}
\definecolor{bg_yellow}{HTML}{D9B310}
\definecolor{bg_brown}{HTML}{6C5050}
\definecolor{bg_burgundy}{HTML}{76323F}
\definecolor{bg_olive_green}{HTML}{626E60}
\definecolor{bg_muted_olive}{HTML}{918770}
\definecolor{bg_beige}{HTML}{C09F80}
\definecolor{columbia_blue}{HTML}{B9D9EB}


\title{Dictionary-Learning-Based Data Pruning for System Identification}

\renewcommand\Authfont{\bfseries}
\setlength{\affilsep}{0em}

\author[1,3]{Tingna Wang$^\dagger$ \thanks{Corresponding author. \texttt{tina\_wang@tongji.edu.cn}}}
\author[2]{Sikai Zhang$^\dagger$}
\author[1,3,4]{Limin Sun}

\affil[1]{Department of Bridge Engineering, Tongji University, Shanghai, China}
\affil[2]{Baosight Software}
\affil[3]{Shanghai Qi Zhi Institute, Shanghai, China}
\affil[4]{State Key Laboratory of Disaster Reduction in Civil Engineering, Tongji University, Shanghai, China}
\begin{document}
\maketitle
\def\thefootnote{$\dagger$}\footnotetext{Equal contribution}\def\thefootnote{\arabic{footnote}}

\begin{abstract}
System identification is normally involved in augmenting time series data by time shifting and nonlinearisation (via polynomial basis), which introduce redundancy both feature-wise and sample-wise.
Many research works focus on reducing redundancy feature-wise, while less attention is paid to sample-wise redundancy.
This paper proposes a novel data pruning method, called (mini-batch) FastCan, to reduce sample-wise redundancy based on dictionary learning.
Time series data is represented by some representative samples, called atoms, via dictionary learning.
The useful samples are selected based on their correlation with the atoms.
The method is tested on one simulated dataset and two benchmark datasets.
The R-squared between the coefficients of models trained on the full and the coefficients of models trained on pruned datasets is adopted to evaluate the performance of data pruning methods.
It is found that the proposed method significantly outperforms the random pruning method.
\end{abstract}


\section{Introduction}
% Introduce system identification. End with the benefits of NARX-based methods.\\
System identification refers to a method of identifying the mathematical description of a dynamic system using measured input-output data \cite{box2015time}. 
It can be used to forecast future values, assess the effects of input variations, design control schemes, etc. 
According to the objective of system identification,  it can be generally divided into two types \cite{billings2013nonlinear}. 
The first type focuses on the approximation scheme that produces the minimum production errors, such as fuzzy logic \cite{nelles785nonlinear} and neural networks \cite{miller1995neural}.
The second type focuses on the elucidation of the underlying rule that represents the system, such as spectral analysis \cite{jenkins1968spectral}, the Volterra series \cite{schetzen1980volterra} and Nonlinear Auto-Regressive with eXogenous Inputs (NARX) \cite{chen1989representations}. 
NARX-based methods have gained increasing attention in various fields such as engineering, finance, biology, and social sciences due to their flexibility in modelling a variety of systems while maintaining interpretability \cite{kukreja2003narmax, billings2013nonlinear, boynton2018applications}.

% Explain why data pruning is necessary for SI. (benefits).\\
To avoid model overfitting and reduce computational complexity, feature selection methods based on orthogonalisation techniques and greedy search are widely applied in constructing NARX models \cite{korenberg1988orthogonal, chen1989orthogonal, hong2008model}. 
The orthogonalisation-based method was firstly derived in \cite{korenberg1988orthogonal} to efficiently decide which terms should be included in the Nonlinear Autoregressive Moving Average with eXogenous input (NARMAX) models. 
Then this method was further developed in \cite{chen1989orthogonal} as orthogonal forward-regression estimators to identify parsimonious models of structure-unknown systems by modifying and augmenting some orthogonal least squares methods. 
A more comprehensive review of this feature selection idea and its development for system identification can be found in \cite{hong2008model}.

Similar to feature selection, input selection is also a crucial step in system identification, which can help to improve the identification performance and reduce storage and training costs \cite{goodwin1971optimal, mehra1974optimal, hong2008model}.
While optimal input selection (often referred to as data pruning) is currently an active research topic for computer vision \cite{raju2021accelerating, sorscher2022beyond, yang2024data} and natural language processing \cite{marion2023less, jin2024llm}, its application to system identification has received less attention, although it remains an intractable task. 

% Introduce the proposed DL-based data-pruning method and DL.
This paper proposes a data-pruning method based on dictionary learning to select useful time series samples for constructing the simplest mathematical model to represent a system.
Specifically, the reduced polynomial NARX method is applied to the identification of nonlinear dynamic systems as an example. 
The canonical-correlation-based fast feature selection method introduced in \cite{zhang2025canonical} is adopted to find the most important terms that should be included in the NAXR model to achieve the required accuracy. 
Dictionary learning is then creatively combined with the fast feature selection method based on canonical correlation to find the most useful time series samples to train these terms. 

Dictionary learning, initially popular in the signal processing area, is used to learn an overcomplete dictionary composed of signal atoms, where a sparse linear combination of these atoms can generate the original signal \cite{aharon2006k}.
It is currently widely studied for image processing since images usually admit sparse representation \cite{mairal2011task, vu2017fast}.
For the proposed method, k-means-based dictionary learning provides an overcomplete dictionary of time terms, which are used as pseudo-labels in the data pruning process. 
% Because of the linear combination, and linear selection, DL is suitable to provide the the pseudo-labels here.  

In this paper, the proposed method, i.e., (mini-batch) FastCan, is compared with the random pruning method.
The reduced NARX trained with the full dataset is used as the baseline model.
The coefficients learned with the pruned dataset is compared with the baseline model.
The more the coefficients are close to the baseline model, the better the pruned dataset is.
The procedural of the research in this paper is illustrated in Fig. \ref{fig:abs}.

% the structure of the paper
The next section introduces the details of the proposed data-pruning method for system identification, followed by numerical case studies to demonstrate the necessity and advantages of the method. 
Subsequently, case studies on two benchmark datasets for nonlinear system identification are presented, with a discussion on the effect of hyperparamteres in the proposed method. 
It is worth noting that the proposed method can also be used to select informative samples for general machine-learning tasks.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{Figs/abstract.pdf}
    \caption[abs]{The schematic diagram of this research. The time series $y(t)$ is generated by the target equation, then time-shifted and nonlinearised to construct a redundant term library. 
    Feature-wise redundancy is represented by a correlation heatmap, while sample-wise redundancy is visualised by a principle component analysis (PCA) plot. 
    Feature selection is applied to the term library to reduce feature-wise redundancy. 
    \textbf{\textcolor{bg_green}{Dataset A}}\footnotemark, which contains the full set of samples, is used to train the \textbf{\textcolor{bg_green}{Baseline model}}. \textbf{\textcolor{bg_red}{Dataset B}}, a random subset of Dataset A, is used to train the \textbf{\textcolor{bg_red}{Random pruned model}}. 
    \textbf{\textcolor{bg_light_blue}{Dataset C}}, consisting of samples selected based on the atoms learned from Dataset A, is used to train the \textbf{\textcolor{bg_light_blue}{FastCan pruned model}}. 
    The FastCan pruned dataset generally exhibits less sample-wise redundancy compared to the randomly pruned dataset, as illustrated in the PCA plot, where the data selected by FastCan are more widely spread, while the randomly pruned data tend to be close to each other. 
    To compare the data pruning methods, Dataset A is pruned repeatedly by random selection and FastCan, and the models are then trained on the pruned datasets, respectively. 
    The results show that the coefficients learned from the FastCan pruned data are closer to those of the baseline model, with less variance across repeated tests.}
    \label{fig:abs}
\end{figure}
\footnotetext{The colour style used this figure is from \texttt{bg-mpl-stylesheets} \cite{bg-stylesheets}.}


\section{Methodology}
This section introduces the dictionary learning-based data pruning method for a discrete system with finite orders. 

\subsection{Reduced polynomial NARX}
A NARX model is used to simulate a system, which is formulated as
\begin{equation}\label{eq:NARX}
\begin{split}
y(k) = & F[y(k-1),y(k-2),..., y(k-n_y),\\
       & u(k-d),y(k-d-1),..., y(k-d-n_u)]+e(k)
\end{split}
\end{equation}
where $y(k)$, $u(k)$ and $e(k)$ are the system output, input and noise sequences, respectively. $n_y$ and $n_u$ are the maximum terms for the system output and input. $d$ is the time delay, typically set to $d = 1$.
$F[\cdot]$ is a nonlinear function. 

The power-form polynomial model is adopted to approximate the nonlinear mapping $F[\cdot]$, and the equation (\ref{eq:NARX}) is then given as,
\begin{equation}\label{eq:NARX_Poly}
\begin{split}
y(k) = \theta_0 
&+ \sum_{i_1=1}^{n} f_{i_1}\big(x_{i_1}(k)\big) \\
&+ \sum_{i_1=1}^{n} \sum_{i_2=i_1}^{n} f_{i_1 i_2}\big(x_{i_1}(k), x_{i_2}(k)\big) 
+ \dots + \sum_{i_1=1}^{n} \cdots \\
&\sum_{i_\ell=i_{\ell-1}}^{n} f_{i_1 i_2 \cdots i_\ell}\big(x_{i_1}(k), x_{i_2}(k), \dots, x_{i_\ell}(k)\big) 
+ e(k)
\end{split}
\end{equation}
where $\ell$ is the degree of polynomial nonlinearity, $n = n_y+n_u$, and 
\begin{align*}
f_{i_1 i_2 \cdots i_m}\big(x_{i_1}(k), x_{i_2}(k), \dots, x_{i_m}(k)\big) 
= \theta_{i_1 i_2 \cdots i_m} \prod_{k=1}^{m} x_{i_k}(k), \\
x_m(k) =
\begin{cases} 
y(k-m), & 1 \leq m \leq n_y, \\
u(k-m+n_y), & n_y+1 \leq m \leq n = n_y + n_u,
\end{cases}
\end{align*}
where $ 1 \leq m \leq \ell $, $\theta_{i_1 i_2 \cdots i_m}$ are model parameters, and $\prod_{k=1}^{m} x_{i_k}(k)$ are model terms whose order is not higher than $\ell$.

The total number of model terms in the polynomial NARX model given in equation (\ref{eq:NARX_Poly}) is $M = (n + \ell)!/[n! \, \ell!]$. 
It can be seen that the full NARX model can include a large number of terms, increasing the risk of overfitting.
In addition, only a subset of these terms is typically important to capture the underlying dynamic relationship \cite{billings2013nonlinear}. % chapter2, p36
Therefore, the canonical-correlation-based fast feature selection is carried out to find out the significant model terms from $\prod_{k=1}^{m} x_{i_k}(k)$ with $y(k)$ as the target. Refer to \cite{zhang2025canonical} for details on the specific feature selection steps. 

\subsection{Mini-batch data pruning}
For simplicity, the selected model terms are represented by the matrix $\mathbf{X}$ as follows,
\begin{equation*}
\mathbf{X} = 
\begin{pmatrix}
x_{1,1} & \dots & x_{1,N} \\
\vdots & \ddots & \vdots \\
x_{p,1} & \dots & x_{p,N} 
\end{pmatrix} 
\end{equation*}
where $p$ is the number of the features, that is the selected model terms, and $N$ is the number of time series samples given the selected features.

A dictionary $\mathbf{D} \in \mathbb{R}^{p \times K}$ for the matrix $\mathbf{X} \in \mathbb{R}^{p \times N}$ is obtained by \textit{mini-batch k-means clustering} \cite{sculley2010web} and used as the target $\mathbf{Y}$ for the subsequent sample selection step, where $K$ is the number of atoms in the dictionary, and typically $K > p$ for an overcomplete dictionary.
Each data sample $\mathbf{x}_{j} \in \mathbb{R}^p$ (column of $\mathbf{X}$) can be approximated as a sparse linear combination of the dictionary atoms, given by, 
\begin{equation*}
    \mathbf{x}_j \approx  \mathbf{D}\mathbf{a}_j
\end{equation*}
where $\mathbf{a}_j$ is a sparse coefficient vector for the $j$-th sample, which means most of the entries in $\mathbf{a}_j$  are zero. 

The fast selection method based on canonical correlation is performed again, with $\mathbf{D}$ as the target, to find $q$ significant samples from the sample matrix $\mathbf{X}$. 
For selection methods which evaluate the linear association between candidates, no additional information is gained when the number of selected samples $q$ exceeds the rank of the data matrix $p$.
Therefore, to avoid invalid sample selection due to $q>p$, samples are selected in separate batches by the \textit{mini-batch sample selection algorithm}, which is given in Algorithm \ref{alg:mini_batch_DP}.
Within each batch, the redundancy and interaction of samples are considered, while they are ignored between batches.
% which results in an ill-conditioned feature covariance matrix $\mathbf{X}^{\top}\mathbf{X}$

\begin{algorithm}[htpb]
\caption{Mini-batch Sample Selection}\label{alg:mini_batch_DP}
% \KwIn{$X \in \mathbb{R}^{p \times N}$ (sample matrix), $Y \in \mathbb{R}^{p \times K}$ (target matrix), $q$ (number of samples to select), $b$ (batch size)}
\KwIn{$X \in \mathbb{R}^{p \times N}$ (sample matrix), $Y \in \mathbb{R}^{p \times K}$ (target matrix), $q$ (number of samples to select)}
\KwOut{$d$ (selected indices)}

\textbf{Step 1:} Compute the batch size $b =$  \( \lceil q/K \rceil \) and then generate the batch matrix $B \in \mathbb{R}^{K \times s}$, where $B[i, j] \leq b$ and $\sum B = q$\;  
\textbf{Step 2:} Initialize the candidate sample matrix $X_c \gets X$\;  

\For{$i \gets 1$ \textbf{to} $K$}{
    Let $y_i \gets Y[:, i]$\;  
    \For{$j \gets 1$ \textbf{to} $s$}{
        \For{$k \gets 1$ \textbf{to} $B[i, j]$}{
            Compute the MCC between each column of $X_c$ and $y_i$\;  
            Select the index $l^*$ corresponding to the largest MCC\;  
            Append $l^*$ to $d$\;  
            Remove $X_c[:, l^*]$ from the candidate matrix\;  
        }
    }
}
\Return $d$\;
\end{algorithm}

\section{Numerical Case Studis}
\subsection{Input Data Analysis}
Data pruning is crucial in system identification for the following reasons: 
\begin{enumerate}
    \item Firstly, system identification typically involves time-series data sampled from continuous physical processes. When the sampling interval is small, consecutive data points are highly similar, leading to a strong temporal correlation and redundancy. The larger dataset size due to redundant data increases the computational cost of model training.
    \item Secondly, the time-shift operation, commonly employed in system identification introduces delayed versions of the signals as new features, which makes the redundancy of the dataset worse. Redundant features do not provide additional information but increase the complexity of the model, making it more prone to overfitting and harder to interpret. 
\end{enumerate}

One example is given here to demonstrate this phenomenon intuitively. 
A time series sampled from $y(t) = \text{sin}(2 \pi t)$ over a duration $1 \text{s}$ at a sample rate of $100$ Hz is illustrated in Fig. \ref{fig:run_raw}. 
After applying time shifting,  the one-dimensional data $y(t)$ is transformed into two-dimensional data, as shown in Table 1, with a time step of $\Delta t = 0.01 \text{s}$.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.45\linewidth]{Figs/redundancy_raw_data.png}
    \caption{Time series of $y(t) = \text{sin}(2 \pi t)$ at $100$ Hz over $1 \text{s}$}
    \label{fig:run_raw}
\end{figure}

\begin{table}[htpb]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$t$ (s) & $y(t)$  & $y(t-\Delta t)$ & $y(t-2\Delta t)$  & ... & $y(t-20\Delta t)$ \\ \hline
0                 & 0       & 0               & 0        & ... & 0                 \\ \hline
0.01              & 0.063   & 0               & 0        & ... & 0                 \\ \hline
0.02              & 0.127   & 0.063           & 0        & ... & 0                 \\ \hline
0.03              & 0.189   & 0.127           & 0.063    & ... & 0                 \\ \hline
0.04              & 0.251   & 0.189           & 0.127    & ... & 0                 \\ \hline
...               & ...     & ...             & ...      & ... & ...               \\ \hline
0.99              & -0.000  & -0.063          & -0.127   & ... & -0.955            \\ \hline
\end{tabular}
\caption{Time-shifted data representation with $\Delta t = 0.01 \, \text{s}$.}
\end{table}

The feature-wise redundancy is illustrated in the correlation heatmap of Fig. \ref{fig:run_fea}, where the $i^{th}$ feature corresponds to $y(t-i\Delta t)$. 
The high Pearson's correlation near the main diagonal indicates redundancy between neighbouring features, such as $y(t-i\Delta t)$ and $y(t-(i+1)\Delta t)$.
This kind of redundancy can be mitigated via feature selection.

Fig. \ref{fig:run_sam} shows the redundancy within the time series samples using principal component analysis (PCA).
After projecting the 21-dimensional data into two-dimensional space, it is found that the data forms a continuous trajectory over time.
The proximity of adjacent samples in the lower-dimensional space given by PCA indicates that there is redundancy between these time series samples. 
This redundancy can be addressed by data pruning.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.45\linewidth]{Figs/redundancy_feature.png}
    \caption{Correlation heatmap illustrating feature-wise redundancy}
    \label{fig:run_fea}
\end{figure}

\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.45\linewidth]{Figs/redundancy_sample.png}
    \caption{Correlation heatmap illustrating sample-wise redundancy}
    \label{fig:run_sam}
\end{figure}

\subsection{Data with Dual Stable Equilibria}
The simulated time series data is generated by a non-autonomous nonlinear system, given by,
\begin{equation*}
\ddot{y} + \dot{y} - y + y^2 + y^3 = u
\end{equation*}
where $u(t) = 0.1 \cos (0.2 \pi t)$. 
By setting $u(t)=0$ and initialising $y$ and $\dot{y}$ with different values, a phase portrait of the nonlinear system is obtained, as shown in Fig. \ref{fig:pp_dsed}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.45\linewidth]{Figs/pp_dsed.png}
    \caption{Phase portrait of the nonlinear system with dual stable equilibria}
    \label{fig:pp_dsed}
\end{figure}

It can be seen that this system exhibits two stable equilibria. 
To comprehensively capture its dynamics, the data pruning algorithm should identify two distinct types of samples attracted to the different stable spirals and select samples from both categories.

Unlike feature selection, random selection serves as a robust benchmark for sample selection and often outperforms more complex algorithms \cite{ayed2023data}.
Therefore, the results of the proposed data-pruning method will be compared with those of the random selection method to demonstrate its advantages.

The structure of the dual-stable-equilibria (DSE) data is visualized using its first and second principal components. The selected samples and atoms are projected onto these principal directions, with their distribution shown in Fig. \ref{fig:pca_dsed}.
% For all PCA results, the batch size is 5, the number of atoms is 30, and the number of selected samples is 100. 
It can be found that the samples selected via mini-batch sample selection are distributed across the entire dataset, whereas the randomly selected samples are concentrated primarily on the left and right regions, with less coverage in the middle.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.45\linewidth]{Figs/pca_dsed.png}
    \caption{Visualization of the DSE data structure}
    \label{fig:pca_dsed}
\end{figure}

To quantitatively evaluate the selection performance of the mini-batch and random methods, a reduced polynomial NARX model with 10 terms is derived using the full dataset as the baseline. 
Note that in all case studies, the number of terms for system output and input is ten respectively, so the total number of terms is $20$, and the degree of polynomial nonlinearity is three. 
% Therefore, the total number of model terms in the polynomial NARX model before term selection is $ (20 + 3)!/[20! \, 3!]$. 
The model coefficients trained on the full dataset are used as the baseline results.
The performance of the baseline NARX on the test dataset is provided in Appendix 1. 
The NARX terms are then fixed, and the model coefficients are trained using the pruned datasets obtained via the mini-batch and random selection methods. 
The number of atoms for dictionary learning is set to 100, and 600 samples are then selected. 
The coefficients trained on the two pruned datasets are compared with those from the full dataset using the R-squared. 
The sample selection and training processes are repeated ten times, with the results presented in Fig. \ref{fig:box_dsed}. 

The mini-batch method gives results with a higher median and lower variance, whereas the random selection method performs significantly worse. 
The same phenomenon is observed when the number of selected samples varies between 300 and 600, as illustrated in Fig. \ref{fig:errbar_dsed}.
Furthermore, for the mini-batch method, the variance of the model coefficients decreases as the number of selected samples increases. However, this relationship does not exist when the samples are randomly selected.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.45\linewidth]{Figs/box_dsed.png}
    \caption{Comparison of sample selection performance between mini-batch and random selection methods on DSE data using R-squared}
    \label{fig:box_dsed}
\end{figure}

\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.45\linewidth]{Figs/errorbar_dsed.png}
    \caption{The effect of sample size on the performance of mini-batch and random selection methods for DSE data}
    \label{fig:errbar_dsed}
\end{figure}

\section{Case Studies on the Benchmark Datasets}
Two benchmark datasets, collected from the Electro-Mechanical Positioning System (EMPS) \cite{janot02178709} and the Wiener-Hammerstein System (WHS) \cite{schoukens2009wiener} are adopted to evaluate the performance of the mini-batch method for data pruning in real system identification. 

\subsection{Data from the Electro-Mechanical Positioning System}
The structure of the data collected from the EMPS, a standard drive configuration for prismatic joints of robots or machine tools, is visualised by its first two principal components, as shown in Fig. \ref{fig:pca_emps}. 
The primary nonlinearity of the corresponding data is introduced by friction effects. 

The atoms determined for the dictionary learning and the selected samples are also shown in Fig \ref{fig:pca_emps}. The samples selected by the mini-batch method are spread over all parts of the dataset, while the randomly selected samples are concentrated in the middle two parts but pay less attention to the left and the right parts.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.45\linewidth]{Figs/pca_emps.png}
    \caption{Visualization of the EMPS data structure}
    \label{fig:pca_emps}
\end{figure}


\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.45\linewidth]{Figs/box_emps.png}
    \caption{Comparison of sample selection performance between mini-batch and random selection methods on EMPS data using R-squared}
    \label{fig:box_emps}
\end{figure}

The baseline NARX model with 10 terms is derived again, and the model coefficients trained on the full dataset are used as the baseline to quantitatively evaluate the selection performance of the mini-batch and random methods.
See Appendix 1 for the performance of the baseline NARX on the test dataset.
The number of atoms for dictionary learning is set to 100, and 10000 samples are then selected. 
The coefficients trained on the two pruned datasets are compared with those from the full dataset using the R-squared, with the NARX terms fixed.
The sample selection and training processes are repeated ten times, with the results illustrated in Fig. \ref{fig:box_emps}. 
In addition, Fig. \ref{fig:errbar_emps} shows the results corresponding to two methods when the number of selected samples varies between 2000 and 20000. 

\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.45\linewidth]{Figs/errorbar_emps.png}
    \caption{The effect of sample size on the performance of mini-batch and random selection methods for EMPS data}
    \label{fig:errbar_emps}
\end{figure}

Similar to the observations from the DSE data, the mini-batch method generally produces results with a higher median and lower variance. 
However, there are exceptions when the number of selected samples is less than 6000, where the randomly selected samples yield higher mean values. This scenario will be analysed and further discussed in Section \ref{sec:eff_hypers}.
Additionally, as expected, the variance of the model coefficients decreases as the number of selected samples increases. 

\subsection{Data from the Wiener-Hammerstein System}
The structure of the data collected from the Wiener-Hammerstein System (WHS), a well-known block-oriented structure consisting of a static nonlinearity between two linear time-invariant blocks, is visualised in Fig. \ref{fig:pca_whbm} using its first two principal components.
The atoms determined for the dictionary learning and the selected samples are also shown in Fig \ref{fig:errbar_whbm}.
Different from the observations from the DSE data and the EMPS data, the samples obtained by the mini-batch method and random selection method have similar distributions, with the selected samples distributed over the entire dataset.

The same strategy is applied to quantitatively evaluate the sample selection performance of the mini-batch and random methods for the identification task of WHS. 
The baseline NARX model with 10 terms is derived, and its performance on the test dataset is provided in Appendix 1.
The number of atoms for dictionary learning is set to 100, and 10000 samples are selected, with the R-squared results shown in Fig. \ref{fig:box_whbm}.
Then, the number of selected samples varies between 2000 and 20000 to demonstrate the variation of R-squared with the selected sample size, as presented in Fig. \ref{fig:errbar_whbm}.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.45\linewidth]{Figs/pca_whbm.png}
    \caption{Visualization of the WHS data structure}
    \label{fig:pca_whbm}
\end{figure}

\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.45\linewidth]{Figs/box_whbm.png}
    \caption{Comparison of sample selection performance between mini-batch and random selection methods on WHS data using R-squared}
    \label{fig:box_whbm}
\end{figure}

\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.45\linewidth]{Figs/errorbar_whbm.png}
    \caption{The effect of sample size on the performance of mini-batch and random selection methods for WHS data}
    \label{fig:errbar_whbm}
\end{figure}

Fig. \ref{fig:box_whbm} demonstrates that the results obtained by the mini-batch and random methods are similar. This phenomenon is also observed in Fig. \ref{fig:errbar_whbm} when the number of the selected samples exceeds 8000. 
This result is reasonable and expected, as similar sample distributions provide similar information for system identification.
In addition, similar to the EMPS case, when the number of selected samples is small (less than 8000), the randomly selected samples yield higher mean values. This phenomenon will be analysed and further discussed in the following section.

\subsection{The Effect of Hyperparameters in the Mini-batch Method}\label{sec:eff_hypers}
In the mini-batch method, sample selection results are influenced by two hyperparameters: the number of atoms in the dictionary and the batch size. 
To explore how these two hyperparameters affect the performance of the selected samples, system identification results against the varying atom size and batch size for the three previously used datasets are given in Fig. \ref{fig:atom_three} to Fig. \ref{fig:batch_three}.
For the DSE dataset, $600$ samples are selected, while for EMPS and WHS datasets, $10000$ samples are selected.

It can be noticed that the batch size in Algorithm \ref{alg:mini_batch_DP} is determined by both the number of samples and the number of atoms, with the aim of minimising redundancy among the selected samples when all atoms are used in the sample selection process. 
% The condition in Step 1 is satisfied only if all atoms are used. If $b>$ \( \lceil q/K \rceil \), some atoms will not be used in the selection process.
Therefore, for the results in Fig. \ref{fig:atom_three}, the batch size varies for each atom size. 
Notably, a larger atom size does not leads to better performance, because some atoms may be learned from noise rather than meaningful data.
In Fig. \ref{fig:batch_three}, when changing the batch size for each dataset, the number of atoms is fixed at the best atom size obtained from Fig. \ref{fig:atom_three}. In this scenario, using a larger batch size to reduce redundancy can improve the performance of the selected samples.

\begin{figure} [htbp]
\centering
    \subfloat[DSE dataset]{%
      \includegraphics[width=0.3\columnwidth]{FigsApx/atom_dsed.png}
      \label{fig:atom_dsed}%
    }
    \subfloat[EMPS dataset]{%
      \includegraphics[width=0.3\columnwidth]{FigsApx/atom_emps.png}
      \label{fig:atom_emps}%
    }
    \subfloat[WHS dataset]{%
      \includegraphics[width=0.3\columnwidth]{FigsApx/atom_whbm.png}
      \label{fig:atom_whbm}%
    }
    \caption{The effect of atom size on the performance of mini-batch method}\label{fig:atom_three}
\end{figure}

\begin{figure} [htbp]
\centering
    \subfloat[DSE dataset]{%
      \includegraphics[width=0.3\columnwidth]{FigsApx/batch_dsed_atom60.png}
      \label{fig:batch_dsed}%
    }
    \subfloat[EMPS dataset]{%
      \includegraphics[width=0.3\columnwidth]{FigsApx/batch_emps_atom700.png}
      \label{fig:batch_emps}%
    }
    \subfloat[WHS dataset]{%
      \includegraphics[width=0.3\columnwidth]{FigsApx/batch_whbm_atom100.png}
      \label{fig:batch_whbm}%
    }
    \caption{The effect of batch size on the performance of mini-batch method with the best atom size}\label{fig:batch_three}
\end{figure}

To investigate whether a larger batch size always leads to better selection when the number of atoms is fixed, the relationship between R-squared and batch size for EMPS data with different atom sizes is presented in Fig. \ref{fig:batch_emps_diffatom}. 
The results indicate that the relationship between R-squared and batch size is not a straightforward linear relationship. 

In summary, the optimal atom size is different for each dataset, and the optimal batch size may not be the ratio of the sample number to the atom size.
A more sophisticated method is required to determine the appropriate hyperparameters for the proposed method. 
Furthermore, the possible reason for the previous-mentioned phenomenon shown in Fig. \ref{fig:errbar_emps} and Fig. \ref{fig:errbar_whbm} is that when the number of selected samples is small, the performance of the selected samples may be more sensitive to the effectiveness of the hyperparameters.


\begin{figure} [htbp!]
\centering
    \subfloat[atom size 10]{%
      \includegraphics[width=0.3\columnwidth]{FigsApx/batch_emps_atom10.png}
      \label{fig:batch_emps_atom10}%
    }
    \subfloat[atom size 100]{%
      \includegraphics[width=0.3\columnwidth]{FigsApx/batch_emps_atom100.png}
      \label{fig:batch_emps_atom100}%
    }
    \subfloat[atom size 1000]{%
      \includegraphics[width=0.3\columnwidth]{FigsApx/batch_emps_atom1000.png}
      \label{fig:batch_emps_atom1000}%
    }
    \caption{The effect of batch size on the performance of mini-batch method with difference atom sizes for EMPS data}\label{fig:batch_emps_diffatom}
\end{figure}

\section{Conclusion}
This paper introduces a data-pruning method based on dictionary learning to choose appropriate time-series samples for identification tasks of a discrete system with finite orders. 
The selection process is independent of the specific system identification technique used. 

The case studies on the synthetic and benchmark datasets show that the proposed method effectively reduces the sample size without significantly degrading identification performance. 
Compared with random selection, the data-pruning method gives better results with a higher mean and lower variance of R-squared values.
Additionally, it is found that the careful selection of hyperparamters is crucial, particularly when the number of selected samples is small. 

% \newpage
\section*{Appendix 1: Prediction Performance of the Baseline NARX}\label{appx1}
\begin{figure} [htbp]
\centering
    \subfloat[Test 1]{%
      \includegraphics[width=0.45\columnwidth]{FigsApx/pred_test_dsed_1.png}
      \label{fig:pre_test1_dsed}%
    }
    \vspace{0.2cm}
    \subfloat[Test 2]{%
      \includegraphics[width=0.45\columnwidth]{FigsApx/pred_test_dsed_2.png}
      \label{fig:pre_test2_dsed}%
    }
    \caption{The performance of the baseline NARX model for DSE test data with different initial conditions}\label{fig:pre_dsed}
\end{figure}

\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.45\linewidth]{FigsApx/pred_test_emps.png}
    \caption{The performance of the baseline NARX model for EMPS test data}
    \label{fig:pre_emps}
\end{figure}

\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.45\linewidth]{FigsApx/pred_test_whbm.png}
    \caption{The performance of the baseline NARX model for WHS test data}
    \label{fig:pre_whbm}
\end{figure}


\newpage

\bibliographystyle{unsrt}
\bibliography{ref.bib}

\end{document}