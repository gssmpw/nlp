% !TEX root = ../main.tex

\vspace{-10pt}
  The primary objective of our \textit{Rectified Conformal Prediction} (\RCP) method is to enhance the conditional coverage of any given conformity score while maintaining their exact marginal validity. Expression~\eqref{eq:oracle_cc} suggests that one could approximate the $(1-\alpha)$-quantile of the conditional distribution of the scores to construct the prediction set:
  \begin{equation*}
    \tilde{\mc{C}}_{\alpha}(x)
    =
    \ac{
      y \in \YC\colon V(x, y)\le \widehat{Q}_{1-\alpha}\pr{\textup{P}_{\mathbf{V}\mid X = x}}
    }.
  \end{equation*}
  %
  This prediction set provides approximate conditional guarantees that depend on the accuracy of the quantile estimator. However, it fails to ensure exact marginal coverage which is an essential property for conformal prediction methods. 


\paragraphformat{A motivation for RCP.}
  Our \RCP\ method is specifically designed to achieve both exact conformal marginal validity and improved approximate conditional coverage. To achieve this, \RCP\ first constructs specially transformed (rectified) scores to enhance conditional coverage. To construct the rectified scores, it builds on the key observation that marginal and conditional coverage coincide precisely when the conditional $(1-\alpha)$-quantile of the conformity score is independent of the covariates. \RCP\ then applies the SCP procedure to these rectified scores, ensuring the classical exact conformal marginal validity. 

  For any given score $V(x, y)$, referred to as the basic score, \RCP\ computes a rectified score $\tilde{V}(x, y)$, which is a transformation of the basic score that satisfies, for $\textup{P}_X$-a.e. $x \in \mathcal{X}$,
  \begin{equation}
    Q_{1-\alpha}\bigl(\textup{P}_{\tilde{V}(X, Y)}\bigr) = Q_{1-\alpha}\bigl(\textup{P}_{\tilde{V}(X, Y)|X=x}\bigr).
  \label{eq:mc_cc}
  \end{equation}
  %
  Below we present two examples that show how one can construct the rectified scores satisfying~\eqref{eq:mc_cc}.

  \begin{example}
  \label{example:linear}
    Consider the rectified score $\tilde{V}(x, y) = V(x, y) / Q_{1-\alpha}(\textup{P}_{\mathbf{V}|X=x})$, with the assumption that $Q_{1-\alpha}(\textup{P}_{\mathbf{V}|X=x})> 0$ for any $x \in \mathcal{X}$. We can define the following prediction set, equivalent to~\eqref{eq:oracle_cc}:
    $\mathcal{C}_{\alpha}(x) = \{y \in \YC\colon \tilde V(x, y) \leq 1 \}$. 
    This prediction set satisfies conditional coverage. %, as demonstrated in~\eqref{eq:ccproof}. 
    Furthermore, in Appendix~\ref{suppl:examplesA}, we prove that this rectified score satisfies the equality in~\eqref{eq:mc_cc}.
  \end{example}

  \begin{example}
  \label{example:additive} Consider the rectified score 
    $\tilde{V}(x, y) = V(x, y) - Q_{1-\alpha}(\textup{P}_{\mathbf{V}|X=x})$.
    The corresponding prediction set, also equivalent to~\eqref{eq:oracle_cc}, is:
    $\mathcal{C}_{\alpha}(x) = \{y \in \YC\colon \tilde{V}(x, y) \leq 0 \}$,
    and it satisfies conditional coverage. Furthermore, in Appendix~\ref{suppl:examplesB}, we prove that this rectified score satisfies the equality in~\eqref{eq:mc_cc}.
  \end{example}
  %
  In the following, we generalize over these two basic examples and present a rich family of general score transformations that allow for score rectification.

\paragraphformat{\RCP\ with general transformations.}
  Recall that starting from a basic score function \(V(x, y)\), we develop a transformed score \(\tilde{V}(x, y)\) to achieve conditional validity at a given confidence level $\alpha$. To do so, we introduce a transformation to rectify the basic conformity score \(V\). 
  
  Consider a parametric family \(\{\adj{t}\}_{t \in \mathbb{T}}\) with \((t, v) \in \mathbb{T} \times \mathbb{R} \mapsto \adj{t}(v) \in \mathbb{R}\) and $\mathbb{T} \subseteq \mathbb{R}$. For convenience, we define \(\tilde{f}_{v}(t) = \adj{t}(v)\) and proceed under the following assumption. 
    
  \begin{assumption}
  \label{ass:tau}
    The function $v\in\R \cup\{\infty\}\mapsto \adj{t}(v)$ is increasing for any $t\in\mathbb{T}$.
    There exists $\varphi\in\R$ such that $\adjinv$ is continuous, increasing, and surjective on $\R$.
  \end{assumption}
  %
  Under \Cref{ass:tau}, we denote by $\tilde{f}_\varphi^{-1}$ the inverse of the function $\tilde{f}_\varphi$, i.e.\ $\tilde{f}_\varphi^{-1} \circ \tilde{f}_\varphi(t)= t$, for all $t \in \mathbb{T}$. %For example, consider the parametric family \(\{\adj{t}\colon t \in \mathbb{T} = \mathbb{R}_+^*\}\), where for \(t \in \mathbb{R}_+^*\) and \(v \in \mathbb{R}\), \(\adj{t}(v) = vt\). 
  %
  Let \(\varphi \in \rset\) be such that $\tilde{f}_\varphi$ is invertible (see \Cref{ass:tau}). Set 
  \begin{equation}
  \label{eq:def:V-varphi}
  V_\varphi(x,y) = \tilde{f}_\varphi^{-1}\bigl(V(x, y)\bigr)
  \end{equation}
  and denote $\mathbf{V} = V(X, Y)$, and $\mathbf{V}_\varphi = V_\varphi(X,Y)$. 
  %
  We now define the following prediction set
  \begin{equation}
    \mc{C}_{\alpha}^*(x) = \bigl\{y \in \mc{Y}\colon V(x, y) \leq f_{\tau_\star(x)}(\varphi)\bigr\},
  \label{eq:rcp2}
  \end{equation}
  where 
  \begin{equation}
  \label{eq:definition-tau}
    \!\!\! \tau_\star(x) = \q{1 - \alpha}\big(\textup{P}_{\mathbf{V}_\varphi} \mid X = x\big)
    = \tilde{f}_\varphi^{-1}\bigl(\q{1 - \alpha}\bigl(\textup{P}_{\mathbf{V} \mid X = x}\bigr)\bigr),
  \end{equation}
  i.e. the \((1-\alpha)\) conditional quantile of the transformed score \(\mathbf{V}_\varphi\) given \(X = x\). We retrieve \Cref{example:linear} with $f_t(v)= vt$, $\varphi=1$. In this case $\tilde{f}_1^{-1}(t)=t$ and $V_1(x,y)= V(x,y)$. Similarly, for \Cref{example:additive}, $f_t(v)= v+t$, $\varphi=0$. In such a case, $\tilde{f}_0^{-1}(t)=t$ and $V_0(x,y)= V(x,y)$.   
  
  In the following, we show that the prediction set in~\eqref{eq:rcp2} satisfies the conditional validity guarantee in~\eqref{eq:def:conditional-validity} and, subsequently, the marginal coverage guarantee in~\eqref{eq:ge:coverage}. In fact, we can write
  \begin{align*}
    &\prob( Y \in \mathcal{C}_{\alpha}^*(X) \mid X=x) = \prob(\mathbf{V} \leq f_{\tau_\star(X)}(\varphi) \mid X=x) \\
    & \quad \stackrel{(a)}{=} \prob(\mathbf{V} \leq \tilde{f}_{\varphi}
    (\tau_\star(X)) \mid X=x) \\
    & \quad \stackrel{(b)}{=}\prob( \mathbf{V}_\varphi \leq \tau_\star(X) \mid X=x) \stackrel{(c)}{\geq} 1 - \alpha, 
  \end{align*}
  where we have used in (a) that $\tilde{f}_v(t)= f_t(v)$, in (b) that $\tilde{f}_\varphi$ is invertible and the definition of $\mathbf{V}_\varphi$, and in (c) the definition of $\tau_\star(x)$. We may rewrite the prediction set~\eqref{eq:rcp2} in terms of the rectified score 
  $\tilde{V}_\star(x,y)= f_{\tau_\star(x)}^{-1}\bigl(V(x,y)\bigr)$: 
  \begin{equation}
  \label{eq:rcp2-rectified}
      \mc{C}_{\alpha}^*(x) = \bigl\{y \in \mc{Y}\colon \tilde{V}_\star(x, y) \leq \varphi\bigr\}.
  \end{equation}
  In \Cref{suppl:cond-quantile}, we establish that the rectified score satisfies~\eqref{eq:mc_cc}, more precisely, setting $\tilde{\mathbf{V}}_\star = \tilde{V}_\star(X,Y)$, for all $x \in \mathcal{X}$, 
  \begin{equation}
  \label{eq:key-relation}
    \varphi = Q_{1-\alpha}\bigl(\textup{P}_{\tilde{\mathbf{V}}_\star|X=x}\bigr) = Q_{1-\alpha}\bigl(\textup{P}_{\tilde{\mathbf{V}}_\star}\bigr).
  \end{equation}
  %
  With the rectified score, conditional and unconditional coverage coincide. However, while the oracle prediction set in~\eqref{eq:rcp2} provides both conditional and marginal validity, it requires the precise knowledge of the pointwise quantile function $\tau_\star(x)$. In practice, $\tau_\star(x)$  is not known and one must construct an estimate \(\widehat{\tau}(x)\) using some hold out dataset. Below we discuss the resulting data-driven procedure.

\vspace{-15pt}
\section{Implementation of RCP}

\vspace{-10pt}
\textbf{The \RCP\ algorithm.} ~
  Rectified conformal prediction approach, as discussed above, requires a basic conformity score function \(V\), a transformation function \(\adj{t}\), and a calibration dataset of $(\tcount + \ccount)$ points. A critical step in the RCP algorithm is estimating the conditional quantile \(\widehat{\tau}(x) \approx \q{1-\alpha}\bigl(\textup{P}_{\mathbf{V}_\varphi \mid X = x} \bigr)\), which we discuss in detail below. \(\widehat{\tau}\) is learned on a separate part of calibration dataset composed of \(\ccount\) data points \( \{(X_k', Y_k') \colon k = 1, \dots , \ccount\} \). Subsequently, RCP uses SCP with the rectified scores $\tilde{V}(x, y) := \adj{\widehat{\tau}(x)}^{-1} \bigl(V(x, y)\bigr)$ instead of the basic scores $V(x, y)$. SCP is applied to the rectified scores computed on the second part of the calibration dataset: $\tilde{\mathbf{V}}_k = \tilde{V}(X_{k}, Y_{k}), k = 1, \dots, \tcount$.

  Finally, for a given test input \(x\) and miscoverage level \(\alpha\), RCP computes the prediction set as
  {\small
    \begin{equation}
    \label{eq:rcp_empirical}
      \mc{C}_{\alpha}(x) = \Bigl\{y \in \mc{Y}\colon \tilde{V}(x,y)  \leq Q_{1-\alpha}\Bigl(\sum_{k=1}^{\tcount} \frac{\delta_{\tilde{\mathbf{V}}_k}}{\tcount+1} + \frac{\delta_{\infty}}{\tcount+1} \Bigr)\Bigr\}.
    \end{equation}
  }
  %
  The resulting \RCP\ procedure is summarized in \cref{algo:rcp}. We show exact marginal validity of \RCP\ and give a bound on its approximate conditional coverage in Section~\ref{sec:theory} below. 

  \begin{algorithm}[t!] 
    \caption{The RCP algorithm}
    \label{algo:rcp}
    \begin{algorithmic}[]
      \State \textbf{Input:}
      Calibration dataset $\mathcal{D}$, miscoverage level $\alpha$, conformity score function $V$, transformation function $\adj{t}$, test input $x$.

      % --- Calibration Stage ---
      \State $\triangleright$ \textbf{Calibration Stage}
      \State Split $\mathcal{D}$ into $\{(X_{k}, Y_{k})\}_{k = 1}^\tcount$ and $\{(X_{k}', Y_{k}')\}_{k = 1}^\ccount$.

      \State $\mathcal{D}_{\tau} \gets \{ (X_k', V(X_k', Y_k')) \}_{k = 1}^m$
      \State $\widehat{q}_{1 - \alpha} \gets$ conditional quantile estimate on $\mathcal{D}_{\tau}$.
      \State $\widehat{\tau} \gets \tilde{f}_\varphi^{-1} \bigl(\widehat{q}_{1 - \alpha}\bigr)$
      \For{$k = 1$ \textbf{to} $\tcount$}
        \State $\tilde{\mathbf{V}}_{k} \gets \adj{\widehat{\tau}(X'_{k})}^{-1} \bigl(V(X_{k}, Y_{k})\bigr)$.
      \EndFor
      \State $k_{\alpha} \gets \lceil (1 - \alpha) (\tcount + 1) \rceil$.
      \State $\tilde{\mathbf{V}}_{(k_{\alpha})} \gets k_{\alpha}$-th smallest value in $\{\tilde{\mathbf{V}}_{k}\}_{k \in [\tcount]} \cup \{+\infty\}$.

      % --- Test Stage ---
      \State $\triangleright$ \textbf{Test Stage}
      \State \quad $\mathcal{C}_{\alpha}(x) \gets \bigr\{y \in \mathcal{Y}\colon \adj{\widehat{\tau}(x)}^{-1} \bigl(V(x, y)\bigr) \leq \tilde{\mathbf{V}}_{(k_{\alpha})}\bigl\}$.

      \State \textbf{Output:} $\mathcal{C}_{\alpha}(x)$.
    \end{algorithmic}
  \label{algo:RCP}
  \end{algorithm}

\paragraphformat{Estimation of $\tau_\star(x)$.}
  We present below several methods for estimating $\tau_\star(x)$. Interestingly, even coarse approximations of this conditional quantile can significantly improve conditional coverage; see the discussion in Section~\ref{sec:experiments}.

\paragraphformat{Quantile regression.}
\label{subsec:tau_qr}
  For any \(x \in \mathbb{R}^d\), the conditional quantile, denoted by \(\tau_{\star}(x)\), is a minimizer of the expected pinball loss:
  \begin{equation}
  \label{eq:def:loss-x-unconditional}
    \widehat{\tau}(x) = \arg\min_{\tau} \ \E\br{\rho_{1-\alpha}\prbig{V_{\varphi}(X,Y) - \tau(X)}},
  \end{equation}
  where the minimum is taken over the function $\tau\colon \mathcal{X} \to \rset$ and \(\rho_{1-\alpha}\)        is the pinball loss ~\cite{koenker1978regression,koenker2001quantile}: $\rho_{1-\alpha}(\tau)=(1-\alpha) \tau \1_{\tau > 0} - \alpha \tau \1_{\tau\le 0}$.
  %
  In practice, the empirical quantile function is obtained \(\widehat{\tau}\) by minimizing the empirical pinball loss:
  \begin{equation}\label{eq:def:loss-pinball}
    \widehat{\tau} \in \arg\min_{\tau \in \mathcal{C}} \frac{1}{\ccount} \sum\nolimits_{k=1}^{\ccount} \rho_\alpha \bigl(V_{\varphi}(X_k', Y_k') - \tau(X_k')\bigr) + \lambda g(\tau),
  \end{equation}
  where $g$ is a penalty function and $\mc{C}$ is a class of functions. When $\tau(x) = \theta^{\top} \Phi(x)$  where $\Phi$ is a feature map, and $g$ is convex, the optimization problem in~\eqref{eq:def:loss-pinball} becomes convex. Theoretical guarantees in this setting, are given, e.g., in~\cite{chen2005computational,koenker2005quantile}.

  Non-parametric methods have also been explored, mainly using kernel-based approaches; see~\cite{christmann2008consistency}. In these cases, $g$ is a smoothness penalty (e.g a norm of an appropriately defined RKHS). More recently, \citet{shen2024nonparametric} introduced a penalized non-parametric approach to estimating the quantile regression process (QRP) using deep neural networks with rectifier quadratic unit (ReQU) activations. They provide  non-asymptotic excess risk bounds for the estimated QRP under mild smoothness and regularity conditions.

\paragraphformat{Local quantile regression.} 
\label{subsec:tau_local}
  The local quantile can be obtained by minimizing the empirical weighted expected value of the pinball loss function \(\rho_{1-\alpha}\), defined as follows:
  \begin{equation}\label{eq:def:reg-tau}
    \!\! \widehat{\tau}(x) \in \arg\underset{t\in\R}{\min} \ac{\sum_{k=1}^{\ccount} w_{k}(x) \rho_{1-\alpha}\pr{V_{\varphi}(X_{k}',Y_{k}')-t}},
  \end{equation}
  where $\{w_{k}(x)\}_{k=1}^{\ccount}$ are positive weights; see~\cite{bhattacharya1990kernel}. 
  For instance, we can set $w_{k}(x)=\ccount^{-1} K_{h_{X}}(\|x-X_{k}'\|)$, where for $h > 0$, $K_{h}(\cdot)= h^{-1} K_1(h^{-1} \cdot)$ is a kernel function satisfying $\int K_1(x) \rmd x=1$, $\int x K_1(x) \rmd x= 0$ and $\int x^2 K_1(x) \rmd x < \infty$; $h_{X}$, the kernel bandwidth is tuned to balance bias and variance. With appropriate adaptive choice of $h(x)$, this approach can be shown to be asymptotically minimax over H\"older balls; see~\cite{bhattacharya1990kernel,spokoiny2013local,reiss2009pointwise}.
