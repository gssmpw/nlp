% !TEX root = ../main.tex

We will now proceed with the proof of \Cref{thm:coverage:marginal}, which verifies the marginal validity of our proposed approach. First, recall that \( V \colon \XC \times \YC \to \mathbb{R} \) is a conformity score function to which we apply a measurable transformation \( (t, x) \in \mathbb{T} \times \XC \to \adj{t}^{-1}(x) \).
For $x\in\XC$, recall that~\RCP\ constructs the following prediction sets
\begin{equation*}
  \mathcal{C}_{\alpha}(x)
  = \ac{
    y\in\YC\colon \adj{\tau(x)}^{-1}\circ V(x,y)
    \le \q{(1-\alpha)(1+\tcount^{-1})} \pr{\frac{1}{\tcount} \sum\nolimits_{k=1}^{\tcount} \delta_{\adj{\tau(X_{k})}^{-1}\circ V(X_{k},Y_{k})}}
  }.
\end{equation*}
%
For any $k\in\{1,\ldots,\tcount+1\}$, denote $\tilde{V}_k = \adj{\tau(X_{k})}^{-1}\circ V(X_{k},Y_{k})$.

\begin{theorem}\label{suppl:thm:coverage:marginal}
  Assume \Cref{ass:tau}-\Cref{ass:tau-in-T} hold, and let $\alpha\in[\{\tcount+1\}^{-1},1)$.
  If $\tilde{V}_1,\ldots,\tilde{V}_{\tcount+1}$ are almost surely distinct, then it yields
  \begin{equation}\label{eq:prob:YinC}
    1 - \alpha
    \le \prob\pr{Y_{\tcount+1} \in \mathcal{C}_{\alpha}(X_{\tcount+1})}
    < 1 - \alpha + \frac{1}{\tcount+1}.
  \end{equation}
\end{theorem}

\begin{proof}
  By definition, we have
  \begin{align}
    \prob\pr{Y_{\tcount+1} \in \mathcal{C}_{\alpha}(X_{\tcount+1})}
    &= \prob\pr{ \adj{\tau(X_{\tcount+1})}^{-1}\circ V(X_{\tcount+1},Y_{\tcount+1}) \le \q{(1-\alpha)(1+\tcount^{-1})} \prt{\frac{1}{\tcount} \sum_{k=1}^{\tcount} \delta_{\tilde{V}_k}}}
    \\
    &= \prob\pr{\tilde{V}_{\tcount+1} \le \q{(1-\alpha)(1+\tcount^{-1})} \prt{\frac{1}{\tcount} \sum_{k=1}^{\tcount} \delta_{\tilde{V}_k}}}.
  \end{align}
  %
  Denote by $F_{\tilde{V}}$ the cumulative density function of $\adj{\tau(X_{\tcount+1})}^{-1}\circ V(X_{\tcount+1},Y_{\tcount+1})$ and consider $\{U_{1},\ldots,U_{\tcount+1}\}$ a family of mutually independent uniform random variables.
  Given $\alpha\in[\{\tcount+1\}^{-1},1)$, define
  \begin{equation*}
    k_{\alpha}
    = \left\lceil \tcount (1+\tcount^{-1}) (1-\alpha) \right\rceil.
  \end{equation*}
  %
  Since by assumption $\alpha\ge \{\tcount+1\}^{-1}$, we have $k_{\alpha}\in\{1,\ldots,\tcount\}$.
  Additionally, remark that $\tilde{V}_k$ has the same distribution that $F_{\tilde{V}}^{-1}(U_k)$.
  Therefore, by independence of the data, we can write
  \begin{equation*}
    \prob\pr{\tilde{V}_{\tcount+1} \le \q{(1-\alpha)(1+\tcount^{-1})} \prt{\frac{1}{\tcount} \sum_{k=1}^{\tcount} \delta_{\tilde{V}_k}}}
    = \prob\pr{F_{\tilde{V}}^{-1}(U_{\tcount+1}) \le F_{\tilde{V}}^{-1}(U_{(k_\alpha)})},
  \end{equation*}
  where $U_{(1)},\ldots,U_{(\tcount)}$ denotes the order statistics.
  Additionally, since the scores $\tilde{V}_1,\ldots,\tilde{V}_{\tcount+1}$ are almost surely distinct, we deduce that
  \begin{equation*}
    \prob\pr{F_{\tilde{V}}^{-1}(U_{\tcount+1}) \le F_{\tilde{V}}^{-1}(U_{(k_\alpha)})}
    = \prob\pr{U_{\tcount+1} \le U_{(k_\alpha)}}
    = \E\br{U_{(k_\alpha)}}.
  \end{equation*}
  %
  Since $U_{(k_\alpha)}$ follows a beta distribution with parameters $(k_\alpha, \tcount+1-k_{\alpha})$, we obtain that $\E\br{U_{(k_\alpha)}}=(\tcount+1)^{-1} k_\alpha$.
\end{proof}
