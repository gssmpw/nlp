% !TEX root = ../main.tex

  \begin{figure}[t!]
    \centering
    \includegraphics[width=0.9\linewidth]{fig/toy-data}
    \caption{Oracle data distribution, sample data and predictor for the toy dataset.}
    \label{fig:toy}
  \end{figure}

  \begin{figure*}[t!]
    \centering
    \includegraphics[width=0.9\linewidth]{fig/contours/Mixture_two_moons_heteroskedastic.pdf}
    \caption{Examples of prediction sets on synthetic dataset where the output has a bivariate and bimodal distribution.}
    \label{fig:contours/Mixture_two_moons_heteroskedastic}
  \end{figure*}


\vspace{-10pt}
\subsection{Toy example}

\vspace{-10pt}
  Let us consider the following data-generating process:
  \begin{equation*}
    X\sim \mathrm{Beta}(1.2, 0.8), \quad Y \mid X=x\sim \gauss(\mu(x), x^4).
  \end{equation*}
  %
  where $\mu(x) = x \sin(x) $. Figure~\ref{fig:toy} shows a realization with $n = 100$ data points. Our goal is to investigate the influence of the quality of the \((1-\alpha)\)-quantile estimate \(\widehat{\tau}\) on performance.
  
  We set $\alpha=0.1$ and consider the conformity score \(V(x,y) = |y - \mu(x)|\). In this case, the \((1-\alpha)\)-quantile of \(V(x,Y) \mid X=x\) is known and we denote it by \(q_{1-\alpha}(x)\). Given \(\omega \in [0,1]\), we set \(\widehat{\tau}(x) \sim (1-\omega) q_{1-\alpha}(x) + \omega \epsilon(x)\), where we consider \(\epsilon(x) \sim \gauss(0, x^4)\). We perform $1000$ experiments and report the $10\%$ lower value of $x \in [0,1] \mapsto \prob(Y_{\tcount+1}\in\mathcal{C}_{\alpha}(x) \mid X=x)$; the results can be found in~\Cref{table:toy}.
  If \(\omega = 0\), \(\widehat{\tau}(x)\) corresponds to the true \((1-\alpha)\) quantile. In this case, our method is conditionally valid, as \Cref{thm:coverage:conditional} shows. However, while all settings of \(\omega\) yield marginally valid prediction sets, the conditional coverage decreases as the quantile estimate \(\widehat{\tau}(x)\) deteriorates.

  \begin{table}[t]
    \centering
    \begin{tabular}{lccccc}
      \toprule
      $\omega$ & $0$ & $1/3$ & $2/3$ & $1$ \\
      \midrule
      \textsc{Coverage} & $90 \pm 01$ & $84 \pm 01$ & $75 \pm 03$ & $59 \pm 07$ \\
      \bottomrule
    \end{tabular}
    \caption{Local coverage on the adversarially selected 10\% of the data, $\omega$ corresponds to the level of contamination of the score quantile estimate.}
    \label{table:toy}
  \end{table}

\vspace{-15pt}
\subsection{Real-world experiment}

\vspace{-10pt}
  We use publicly available regression datasets which are also considered in~\citep{Tsoumakas2011-wf,Feldman2023-cc,wang2023probabilistic} and only keep datasets with at least 2 outputs and 2000 total instances. The characteristics of the datasets are summarized in \cref{suppl:details}.

\paragraphformat{Base predictors.}
  We consider two base predictors, both parameterized by a fully connected neural network with three layers of 100 units and ReLU activations.

  The \textit{mean predictor} estimates the mean $\hat{\mu}_i(x)$ of the distribution for each dimension $i \in [d]$ given $x \in \mathcal{X}$. Since it only provides a point estimate, it does not capture uncertainty.

  The \textit{mixture predictor} models a mixture of $K$ Gaussians, enabling it to represent multimodal distributions.
  Given \(x \in \mathcal{X}\), the model outputs \(z(x) \in \mathbb{R}^K\) (logits for mixture weights), \(\mu(x) \in \mathbb{R}^{K \times d}\) (mean vectors), and \(L(x) \in \mathbb{R}^{K \times d \times d}\) (lower triangular Cholesky factors). The mixture weights \(\pi(x) \in \mathbb{R}^K\) are obtained by applying the softmax function to \(z(x)\), and the covariance matrices \(\Sigma(x) \in \mathbb{R}^{K \times d \times d}\) are computed as \(\Sigma_k(x) = L_k(x) L_k(x)^\top\).
  The conditional density at \(y \in \mathcal{Y}\), given \(x \in \mathcal{X}\), is:
  \[
  \hat{p}(y \mid x) = \sum_{k=1}^K \pi_k(x) \cdot \mc{N}\bigl(y \mid \mu_k(x), \Sigma_k(x)\bigr),
  \]
  where \(\mc{N}\bigl(y \mid \mu_k(x), \Sigma_k(x)\bigr)\) is a Gaussian density with mean \(\mu_k(x)\) and covariance \(\Sigma_k(x)\).

\paragraphformat{Methods.}
  We compare \RCP\ with four split-conformal prediction methods from the literature: \texttt{ResCP} \citep{Diquigiovanni2021-bh}, \texttt{PCP} \citep{wang2023probabilistic}, \texttt{DCP} \citep{Sadinle2016-yr}, and \texttt{SLCP} \citep{han2022split}. \texttt{ResCP} uses residuals as conformity scores. To handle multi-dimensional outputs, we follow~\citep{Diquigiovanni2021-bh} and define the conformity score as the $l^\infty$ norm of the residuals across dimensions, i.e., \( V(x, y) = \max_{i \in [d]} |\hat{\mu}_i(x) - y_i| \). \texttt{PCP} constructs the prediction set as a union of balls, while \texttt{DCP} defines the prediction set by thresholding the density.  \texttt{ResCP} is compatible with the \textit{mean predictor}, whereas \texttt{PCP} and \texttt{DCP} are compatible with the \textit{mixture predictor}. Finally, \texttt{SLCP}, like \RCP, is compatible with any conformity score and base predictor. For \RCP, we compuate an estimate $\widehat{\tau}(x)$ (see \cref{subsec:tau_qr}) using quantile regression with a fully connected neural network composed of 3 layers with 100 units.
  

\paragraphformat{Experimental setup.}
  We reserve 2048 points for calibration. The remaining data is split between 70\% for training and 30\% for testing. The base predictor is trained on the training set, while the baseline conformal methods use the full calibration set to construct prediction sets for the test points. In \RCP, the calibration set is further divided into two parts: one for estimating $\widehat{\tau}(x)$ and the other as the proper calibration set for obtaining intervals. This ensures that all methods use the same number of points for uncertainty estimation. When not specified, we used the adjustment $f_t(v) = t + v$. Additional details on implementation and hyperparameter tuning are provided in \cref{suppl:details}.

\paragraphformat{Evaluation metrics.}
  To evaluate conditional coverage, we use \textit{worst-slab coverage}~\citep{cauchois2020knowing,romano2020classification} and the \textit{conditional coverage error}, computed over a partition of \(\mathcal{X}\), following~\citet{dheur2024distribution}.  

\paragraphformat{Visualization on a synthetic dataset.}
  \cref{fig:contours/Mixture_two_moons_heteroskedastic} illustrates example prediction sets for different methods. The orange and black contour lines represent confidence levels of \(\alpha = 0.1\) and \(\alpha = 0.8\), respectively. The first panel shows the highest density regions of the oracle distribution, while the subsequent panels display prediction regions obtained by different methods, both before and after applying \RCP. We can see that combining \RCP\ with \texttt{ResCP}, \texttt{PCP}, or \texttt{DCP} results in prediction sets that more closely align with those of the oracle distribution.

  \begin{figure}[t!]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/pointplot/rcp_vs_base/wsc.pdf}
    \caption{Worst-slab coverage for three conformal methods and their RCP counterparts, on datasets sorted by total size.}
    \label{fig:pointplot/rcp_vs_base/wsc}
  \end{figure}

  \begin{figure}[t!]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/pointplot/rcp_vs_slcp/wsc.pdf}
    \caption{Worst-slab coverage for \RCP\ and \texttt{SLCP} in combination with different conformity scores, on datasets sorted by total size.}
    \label{fig:pointplot/rcp_vs_slcp/wsc}
  \end{figure}


\paragraphformat{Results.}
  \cref{fig:pointplot/rcp_vs_base/wsc} presents the worst-slab coverage for different conformity scores, both with and without \RCP. Similarly, \cref{fig:pointplot/rcp_vs_slcp/wsc} compares worst-slab coverage between \texttt{SLCP} and \RCP. Additional results, including conditional coverage error and marginal coverage, are provided in \cref{sec:additional_results}.

  In \cref{fig:pointplot/rcp_vs_base/wsc}, we observe that \texttt{ResCP}, \texttt{PCP}, and \texttt{DCP} fail to reach the nominal level of conditional coverage for most datasets. In contrast, all variants of \RCP\ significantly improve coverage across all datasets. Similarly, \cref{fig:pointplot/rcp_vs_slcp/wsc} shows that \RCP\ often achieves better conditional coverage than \texttt{SLCP}, particularly on larger datasets. \cref{fig:pointplot/rcp_vs_base/horizontal} in \cref{sec:additional_results} confirms these findings with the conditional coverage error. Finally, as expected, all methods achieve marginal coverage.
  
  \cref{sec:tau_testimation} discusses the estimation of $\widehat{\tau}(x)$ using either a neural network or local quantile regression for which we have bounds the conditional coverage. On most datasets, the neural network slightly outperforms local quantile regression, which is expected due to its flexibility.
  \cref{sec:adj_function,sec:additional_adjustment} discuss the choice of adjustment function. For certain adjustment functions, the domain of the scores $v = V(x, y)$ must be restricted to a subset of $\R$ to satisfy \Cref{ass:tau-in-T}. Notably, $f_t(v) = tv$ requires $v > 0$, $f_t(v) = \exp(tv)$ requires require $v>1$.
  
  \cref{sec:simple_baseline} presents an additional study comparing RCP with CP and CQR methods, that we adapted to multidimensional target setting. For these experiments, the model predicts parameters of a multivariate normal distribution and we use the score based on the corresponding Mahalanobis distance. We demonstrate that \RCP\ improves conditional coverage over classic CP and also benefits from the custom score to outperform CQR.
