% !TEX root = ../main.tex

  The widespread adoption of modern probabilistic and generative AI models across various applications highlights the need for reliable uncertainty quantification ~\citep{gruber2023sourcesuncertaintymachinelearning}. In fact, while these models are highly flexible and can capture complex statistical dependencies, they can also generate predictions that are either unreliable or overly confident ~\citep{Nalisnick2018-ew}. Conformal prediction~\citep[CP;][]{vovk2005algorithmic,shafer2008tutorial} is a robust framework for constructing distribution-free predictions with finite-sample validity guarantees; see~\citep{angelopoulos2023conformal,Angelopoulos2024-dr}. While most existing research has focused on univariate prediction tasks~\citep{romano2019conformalized,sesia2021conformal,rossellini2024integrating}, multivariate settings remain relatively underexplored. Conformal methods for multivariate regression include approaches that aggregate univariate prediction regions~\citep{messoudi2021copula,Zhou2024-sq}, leverage generative models~\citep{wang2023probabilistic,plassier2024conditionally}, or estimate conditional probability density functions~\citep{izbicki2022cd}.

  %\textcolor{blue}{Accurately quantifying uncertainty in model predictions is vital for many applications~\citep{gruber2023sourcesuncertaintymachinelearning}. This is particularly important in tasks involving multiple output variables with complex statistical dependencies. In medical diagnostics, for instance, disease progression is often evaluated using health indicators like blood pressure and cholesterol levels, which may exhibit nonlinear dependencies~\citep{Rajkomar2018-hg}. While modern probabilistic and generative AI models are designed to handle such complexities, they can still produce predictions that are either unreliable or overly confident~\citep{Nalisnick2018-ew}.}

  %\textcolor{blue}{Conformal prediction~\citep[CP;][]{vovk2005algorithmic,shafer2008tutorial} is a robust framework for constructing distribution-free prediction regions with finite-sample validity guarantees; see~\citet{angelopoulos2023conformal,Angelopoulos2024-dr}. While most existing research has focused on univariate prediction tasks~\citep{romano2019conformalized,sesia2021conformal,rossellini2024integrating} multivariate settings remain relatively underexplored. Conformal methods for multivariate regression include approaches that aggregate univariate prediction regions~\citep{messoudi2021copula,Zhou2024-sq}, leverage generative models~\citep{wang2023probabilistic,plassier2024conditionally}, or estimate conditional probability density functions~\citep{izbicki2022cd}.}

  Furthermore, classical CP methods ensure marginal validity but do not guarantee conditional validity, a stricter and more desirable property that tailors prediction regions to individual covariates. Unfortunately, prior research has demonstrated that constructing non-trivial prediction regions with exact conditional validity from finite samples is impossible without making additional assumptions about the data distribution~\citep{vovk2012conditional,lei2014distribution,foygel2021limits}. As a result, efforts are often directed towards developing conformal methods that produce distribution-free prediction regions with marginal validity while aiming for the best possible \textit{approximate} conditional validity~\cite{gibbs2024conditional}.

  A typical relaxation of exact conditional coverage in earlier work involves group-conditional guarantees~\cite{ding2024class, jung2022batch}, which provide coverage guarantees for a predefined set of groups. Another branch of work partitions the covariate space $\mathcal{X}$ into multiple regions and applies classical conformal prediction within each region~\cite{leroy2021md, alaa2023conformalized, kiyani2024conformal}. However, such partitioning based on the calibration set often leads to overly large prediction regions~\cite{bian2023training,plassier2024efficient}.

  An alternative approach weights the empirical cumulative distribution function with a ``localizer'' function that quantifies the similarity between calibration points and the test sample~\cite{guan2023localized}. Although this method improves the localization of predictions, it has significant limitations, especially in high-dimensional covariate spaces.

  Finally, several methods focus on the transformation of conformity scores~\cite{deutschmann2023adaptive,han2022split,dey2022conditionally,izbicki2022cd,dheur2024distribution}. These techniques adjust conformity scores to better approximate the conditional coverage. However, they usually require estimating the conditional distribution of conformity scores, which is both computationally intensive and difficult to perform accurately.

  In this work, we introduce a new conformal prediction framework, \textit{Rectified Conformal Prediction} (\RCP), which extends the score transformation approach by applying transformations to arbitrary conformity scores. \RCP\ is designed to improve conditional coverage while maintaining exact marginal coverage guarantees. By defining a new conformity score whose quantile (for a given coverage level) is independent of the covariates, \RCP\ ensures that the conformal prediction procedure yields prediction sets that are both marginally and conditionally valid. 

  One of the key advantages of the \RCP\ framework is its ability to produce multivariate prediction regions without requiring full modeling of the conditional distribution of conformity scores. Instead, \RCP\ focuses on estimating specific quantiles of the score distribution that are critical for achieving the desired guarantees. This targeted estimation not only enhances computational efficiency but also makes the method versatile and applicable to a broad range of problems.

  The main contributions of this work can be summarized as follows.
  \begin{itemize}
    \item We introduce Rectified Conformal Prediction (\RCP), a new conformal method designed to enhance conditional validity by refining conformity scores. Our method avoids the need to estimate the full conditional distribution of a multivariate response, relying instead on estimating only the conditional quantile of a univariate conformity score. 

    \item We provide a theoretical lower bound on the conditional coverage of the prediction sets generated by \RCP. This conditional coverage is explicitly governed by the approximation error in estimating the conditional quantile of the conformity score distribution. 
    
    \item We evaluate our method on several benchmark datasets and compare it against state-of-the-art alternatives. Our results demonstrate improved performance, particularly in terms of conditional coverage metrics such as worst slab coverage~\citep{romano2020classification} and conditional coverage error~\citep{dheur2024distribution}.
  \end{itemize}
