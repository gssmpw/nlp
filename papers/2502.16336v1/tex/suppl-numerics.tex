% !TEX root = ../main.tex

  This section aims to provide addtional details on our experimental setup and implementation of the~\RCP\ algorithm.

\paragraphformat{Models.}
  To facilitate fair comparison of different uncertainty estimation methods, we assume that the base prediction models are already trained. We focus on the regression problem and aim to construct prediction sets for these pre-trained models. All our models are based on a fully connected neural network of three hidden layers with 100 neurons in each layer and ReLU activations. We consider three types of base models with appropriate output layers and loss functions: the mean squared error for the \textit{mean predictor}, the pinball loss for the \textit{quantile predictor} or the negative log-likelihood loss for the \textit{mixture predictor}. Training is performed with Adam optimizer.

  Each dataset is split randomly into train, calibration, and test parts. We reserve 2048 points for calibration and the remaining data is split between 70\% for training and 30\% for testing. Each dataset is shuffled and split 10 times to replicate the experiment. This way we have 10 different models for each dataset and these models' prediction are used by every method that is tailored to the corresponding model type to estimate uncertainty. One fifth of the train dataset is reserved for early stopping.

\paragraphformat{RCP$_{\mathrm{MLP}}$.}
  This variation reserves a part ($50\%$) of the original calibration set to train a quantile regression model for the $(1-\alpha)$ level quantile of the scores $V$. We again use a three hidden layers with 100 units per layers for that task. The remaining half of the calibration set forms the ``proper calibration set'' and is used to compute the conformal correction.

% \subsection{RCP-C}
% For this method we use $25\%$ of the calibration data for the Nadaraya-Watson based estimate of score CDF and $25\%$ to compute the conformal correction. The third part of $25\%$ of calibration points are used to tune the bandwidth. We use RBF kernel functions, isotropic for $X$, so we need to tune two real-valued bandwidth parameters. We employ Dual Annealing method with bandwidths limited to $[10^{-4}, 10]$. After the bandwidth is tuned, use the calibration data as before: split in half to estimate the conditional quantile and conformal correction.

\paragraphformat{RCP$_{\mathrm{local}}$.} 
  The local quantile regression variant is similar to the previous one, so we use the same splitting of the available calibration data. Since only one bandwidth needs to be tuned, we use a simple grid search on a log-scale grid in the interval $[10^{-3}, 1]$.

\paragraphformat{Datasets.}
%\label{subsec:datasets}
  \cref{table:datasets} presents characteristics of datasets from~\citep{Tsoumakas2011-wf,Feldman2023-cc,wang2023probabilistic}, restricting our selection to those with at least two outputs and a total of 2000 instances. For data preprocessing, we follow the procedure of~\citep{Grinsztajn2022-nu}.

  \begin{table}[h]
  \centering
    \begin{tabular}{llrrr}
      \toprule
      Paper & Dataset & $n$ & $p$ & $d$ \\
      \midrule
      \multirow[t]{4}{*}{\citet{Tsoumakas2011-wf}} & scm20d & 8966 & 60 & 16 \\
       & rf1 & 9005 & 64 & 8 \\
       & rf2 & 9005 & 64 & 8 \\
       & scm1d & 9803 & 279 & 16 \\
      \multirow[t]{6}{*}{\citet{Feldman2023-cc}} & meps\_21 & 15656 & 137 & 2 \\
       & meps\_19 & 15785 & 137 & 2 \\
       & meps\_20 & 17541 & 137 & 2 \\
       & house & 21613 & 14 & 2 \\
       & bio & 45730 & 8 & 2 \\
     & blog\_data & 50000 & 55 & 2 \\
      \citet{wang2023probabilistic} & taxi & 50000 & 4 & 2 \\
      \bottomrule
    \end{tabular}
    \caption{List of datasets with their characteristics.}
  \label{table:datasets}
  \end{table}
