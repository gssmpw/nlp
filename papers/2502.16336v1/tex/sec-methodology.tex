% !TEX root = ../main.tex

  Given $\tcount$ i.i.d. samples $\{(X_k, Y_k)\}_{k=1}^{\tcount}$ from an unknown distribution $\textup{P}_{X,Y}$, and a user-specified confidence level $\alpha \in (0,1)$, the standard split conformal prediction constructs a confidence set given by
  \begin{equation}\label{eq:split_conformal}
    \resizebox{0.9\hsize}{!}{$
    \mathcal{C}_{\alpha}(x)
    = \ac{
      y \colon V(x,y) \le \q{1-\alpha}\pr{ \sum_{k=1}^{\tcount} \frac{\delta_{V(X_k,Y_k)}}{\tcount+1} + \frac{\delta_{\infty}}{\tcount+1} }
    }
    $}
  \end{equation}
  where $\delta_v$ is the Dirac mass at $v$, and for any distribution $\textup{P}$ on $\R$, $\q{1-\alpha}\pr{\textup{P}}$ is the $(1-\alpha)$-quantile defined by
  \begin{equation*}
    \q{1-\alpha}\pr{\textup{P}}
    = \inf\ac{t\in\R \colon \textup{P}(-\infty,t]\ge 1-\alpha}.
  \end{equation*}
  %
  The resulting prediction set $\mathcal{C}_{\alpha}(x)$ is marginally valid, meaning that it satisfies property~\eqref{eq:ge:coverage}; see~\cite{shafer2008tutorial}.

  In this work, we aim to construct a prediction set $\mathcal{C}_{\alpha}(x)$ for a specified point $X_{\tcount + 1} = x$ that captures the unseen output $Y_{\tcount+1}\in\YC$ with a probability near $1-\alpha$:
  \begin{equation}\label{eq:def:conditional-validity}
    \prob\pr{Y_{\tcount+1} \in \mathcal{C}_{\alpha}(X_{\tcount+1}) \mid X_{\tcount+1} = x}
    \ge 1 - \alpha.
  \end{equation}
  %
  More precisely, define $\textup{P}_{Y\mid X=x}$ the conditional distribution of $Y$ given $X=x$. With this notation, \eqref{eq:def:conditional-validity} can be rewritten as $\textup{P}_{Y\mid X=x}(\mathcal{C}_{\alpha}(x)) \ge 1-\alpha$. The property given by~\eqref{eq:def:conditional-validity} is known as the conditional validity. Ideally, the prediction sets should be adaptive, as their size changes with the difficulty of the inputs.

\paragraphformat{Proposed approach.}
  We apply a transformation to rectify the conformity score $V$. We consider a parametric family $\{\adj{t}\}_{t \in \mathbb{T}}$, where $(t, x)\in\mathbb{T}\times\R \mapsto \adj{t}(x) \in\R$ is a measurable function.
  Our goal is to determine a function $\tau(x)$ such that the conditional $(1-\alpha)$-quantile of $\adj{\tau(x)}^{-1} \circ V(x,Y_{\tcount+1})$ does not depend upon $X_{\tcount+1}=x$.

  \begin{example}
    Consider a regression problem under heteroscedastic noise, where the conditional distribution of $Y_{\tcount+1} \mid X_{\tcount+1} = x$ is given by $\mu(x) + \sigma(x) Z_{\tcount+1}$, with $Z_{\tcount+1}$ independent of $X_{\tcount+1}$. In this case, one may use the standard score function $V(x, y) = |\pred(x) - y|$ and set $\tau(x)=\sigma(x)$ and $\adj{t}(v) = v t$. Consequently, the law of $\adj{\tau(x)}^{-1} \circ V(x,Y_{\tcount+1}) = \frac{|\pred(x) - Y_{\tcount+1}|}{\sigma(x)}$ under $Y_{\tcount+1}\sim \textup{P}_{Y\mid X=x}$ does not depend on $x$.
  \end{example}

  To achieve this property in more general situations, we propose a new approach called Rectified Conformal Prediction (\RCP), which is based on the knowledge of a conditional quantile proxy $\tau(x)$ of an appropriate transform of the conformity score; we later explain how to construct such proxy.
  Given $\alpha\in[0,1]$, we want to determine a measurable function $\tau \colon \XC\to \mathbb{T}$ such that
  \begin{equation}\label{eq:prop-cond}
    % \resizebox{.9\hsize}{!}{$
    \q{1-\alpha}\pr{\textup{P}_{\adj{\tau(X)}^{-1}\circ V(x,Y)\mid X=x}} ~ \text{does not depend upon} ~ x.
    % $}
  \end{equation}
  %
  We write $\tilde{f}_{v}(t)=\adj{t}(v)$ and consider the following assumptions.

  \begin{assumption}\label{ass:tau}
    The function $v\in\R \cup\{\infty\}\mapsto \adj{t}(v)$ is increasing for any $t\in\mathbb{T}$.
    There exists $\varphi\in\R$ such that $\adjinv$ is increasing and surjective on $\R$.
  \end{assumption}

  Assumption~\Cref{ass:tau} requires the transformation \( \adj{t} \) to be increasing to ensure the marginal validity of \RCP. Later, we will demonstrate how to leverage these transformations to adjust the conformity scores, thereby maintaining the desired marginal validity while improving conditional coverage.

  For \(\varphi\) satisfying \Cref{ass:tau} and for all \(x \in \XC\), define $\tilde{V}_{\varphi}(x,y)=\adjinv^{-1}\circ V(x,y)$. If we set $\tau(x) = \q{1-\alpha}(\textup{P}_{\tilde{V}_{\varphi}(x,Y)\mid X=x})$, then
  \begin{equation}\label{eq:eq:varphi-quantile}
    \varphi
    = \q{1-\alpha}\pr{\textup{P}_{\adj{\tau(x)}^{-1} \circ V(x,Y)\mid X=x}}.
  \end{equation}
  %
  Note that \(\varphi\) does not depend upon \(x\); see the derivation in Supplementary material. Therefore, \eqref{eq:prop-cond} is satisfied for all $x \in \XC$. Importantly, note that we can choose any value of $\varphi$ such that $\adjinv$ is increasing and surjective.


\paragraphformat{Simple case.} 
  As an example, consider the parametric family $\{\adj{t}\colon t\in\mathbb{T}=\R_+^*\}$, where for $t\in\R_+^*$ and $v\in\R$, $\adj{t}(v)=vt$. In such case, $\tilde{f}_{v}(t)=v t$ and therefore \Cref{ass:tau} is satisfied for $\varphi=1$.
  % Thus, we have $\tilde{f}_{\varphi}^{-1}\circ V(X,Y) = V(X,Y)$.


\paragraphformat{Conformalization.}
  One could question the usefulness of determining the quantile of $\tilde{V}_{\varphi}(x,Y)$ at $X=x$, since knowing an approximation of the $(1-\alpha)$ conditional quantile of $V(x,Y)$ already allows the construction of the following confidence set:
  \begin{equation*}
    \ac{
      y\in\YC\colon V(x,y)\le \widehat{Q}_{1-\alpha}\pr{\textup{P}_{V(x,Y)\mid X=x}}
    }.
  \end{equation*}
   %
   However, this confidence set  does not fulfil the marginal coverage. To address this limitation, we develop an algorithm called~\RCP\ in the following section. This algorithm provides  marginal guarantees, but also conditional coverage guarantees that depend on the accuracy of the quantile estimator \(\tau(x)\).


\subsection{\RCP: Rectified Conformal Prediction}
  The first step is to calculate an approximate quantile $\tau(x)\approx \q{1-\alpha}\bigl(\textup{P}_{\tilde{V}_{\varphi}(x,Y)\mid X=x} \bigr)$, where $\tilde{V}_{\varphi}(x,y):=\adjinv^{-1}\circ V(x,y)$ is the rectified conformity score.
  Precise methods to determine $\tau$ are discussed in \Cref{sec:tau}. Our method uses the standard conformal approach with the adjusted conformity score $\tilde{V}(x,y):=\adj{\tau(x)}^{-1}\circ V(x,y)$ instead of the original score $V(x,y)$. 
  For $x\in\XC$, we construct the~\RCP\ prediction sets as
  \begin{multline}\label{eq:def:prediction-set-algo}
    \textstyle \mathcal{C}_{\alpha}(x)
    = \bigg\{
      y\in\YC\colon \tilde{V}(x,y)
      \\
      \le \q{(1-\alpha)(1+\tcount^{-1})} \pr{\frac{1}{\tcount} \sum\nolimits_{k=1}^{\tcount} \delta_{\tilde{V}(X_{k},Y_{k})}}
    \bigg\}.
  \end{multline}
  %
  In \Cref{sec:theory}, we show how this modification improves conditional coverage while generating marginally valid prediction sets. A key advantage of our method is the ability to perform conformalized quantile regression on multidimensional outputs. Instead of applying the quantile regression directly on the observations $Y_{k}$ as proposed in~\cite{romano2019conformalized,kivaranovic2020adaptive}, we propose to apply the quantile estimation on the transformed conformity score $\tilde{V}_{\varphi}$. We present theoretical guarantees for this approach in \Cref{sec:theory}. This method proves particularly valuable in various scenarios, especially when the original observations are not real-valued. In such cases, the~\RCP\ framework remains applicable, contrary to standard conformalized quantile regression methods, which typically require real-valued observations and therefore may not be suitable for these situations. Consequently, the~\RCP\ approach expands the range of data types that can be effectively dealt with.

  To explain the intuition behind our method, we will now discuss why we can expect that \(\q{1-\alpha}(\textup{P}_{\tilde{V}(x,Y) \mid X=x})\) should not significantly depend on \(x\). To simplify, let's assume that \(\tau(x)= \q{1-\alpha}(\textup{P}_{\tilde{V}_\varphi(x,Y)\mid X=x})\). In this case, we show in \Cref{sec:ccp} that \(\varphi\) is the conditional quantile of \(\tilde{V}(x,Y)\) given \(X=x\). Applying~\cite[Lemma~21.2]{van2000asymptotic}, as the number of calibration data increases, we get
  \begin{equation*}
    \textstyle
    \q{(1-\alpha)(1+\tcount^{-1})}\pr{\frac{1}{\tcount} \sum_{k=1}^{\tcount} \delta_{\tilde{V}_{k}}}
    \xrightarrow[\tcount \to \infty]{\text{a.s.}} \q{1-\alpha}\prbig{\textup{P}_{\tilde{V}(X,Y)}}.
  \end{equation*}
  where $\tilde{V}_{k}=\tilde{V}(X_k,Y_k)$. The preceding calculations demonstrate that the empirical quantile converges to the true quantile.
  Furthermore, using~\eqref{eq:eq:varphi-quantile}, one may show that for all $x\in\XC$, it holds 
  \begin{equation*}
    % \resizebox{1\hsize}{!}{$
    \q{1-\alpha}\prbig{\textup{P}_{\tilde{V}(X,Y)}}
    = \q{1-\alpha}\prbig{ \textup{P}_{\tilde{V}(x,Y)\mid X=x}}.
    % $}.
  \end{equation*}
  %
  Consequently, the empirical quantile of \RCP\ for any $x \in \XC$ converges to the conditional quantile, enabling us to achieve approximate conditional validity.
  The purpose of \RCP\ is to transform existing CP methods by improving their conditional guarantees while preserving their core behavior.
  In the ideal scenario where we have a perfect estimate \(\tau(x)\) of the conditional quantile, \RCP\ performs optimally. Empirically, we have observed that the conditional coverage approaches the targeted level as \(\tau(x)\) gets closer to the \((1-\alpha)\)-quantile of \(\tilde{V}_{\varphi}(x,Y)\). Even crude estimates of this quantile can still yield improvements in conditional coverage.


\paragraphformat{Suggestions for Choosing \( \adj{t}(v) \).}
 We recommend the setting \( \adj{t}(v) = v t \). This choice naturally ``standardizes'' the distribution of conformity scores by dividing the values by their quantile. Interestingly, this connects our method to~\cite{deutschmann2023adaptive}, where the authors normalize the conformity scores by their quantiles. Another alternative we experimentally investigated is \( \adj{t}(v) = v + t \), which corresponds to the normalization considered in~\cite{han2022split}.

\section{Estimation of $\tau$}
\label{sec:tau}
  We present below three methods for estimating the quantile. For each of these methods, \( \tau \) is learned on a separate calibration dataset composed of \( \ccount \) data points \( \{(X_k, Y_k) \colon k \in [\ccount]\} \). Additionally, denote \( \tilde{V}_{\varphi, k} = \tilde{V}_{\varphi}(X_k, Y_k) \).


\paragraphformat{Quantile regression.}
\label{subsec:tau_qr}

  For any \(x \in \mathbb{R}^d\), the conditional quantile, denoted by \(\tau_\star(x)\), is a solution to the minimization of the expected loss:
  \begin{equation}\label{eq:def:loss-x}
    \mathcal{L}_x(t)
    = \E\br{\rho_{1-\alpha}\prbig{\tilde{V}_{\varphi}(x,Y) - t} \,\big\vert\, X=x},
  \end{equation}
  where \(\rho_{1-\alpha}\) denotes the pinball loss function~\cite{koenker1978regression,koenker2001quantile}, defined by:
  \begin{equation}\label{eq:def:pinball-loss}
    \rho_{1-\alpha}(\tau)=(1-\alpha) \tau \1_{\tau > 0} - \alpha \tau \1_{\tau\le 0}.
  \end{equation}
  Quantile regression estimates are robust against outliers. In practice, the quantile estimate \(\tau(x)\) is determined by minimizing:
  \begin{equation}\label{eq:def:loss-pinball}
    \tau \in \argmin_{\tau} \frac{1}{\tcount} \sum\nolimits_{i=1}^{\tcount} \rho_\alpha \left(\tilde{V}_{\varphi}(X_k,Y_k) - \tau(X_k)\right).
  \end{equation}
  Parametric quantile regression is solved by optimizing the parameters to minimize the empirical loss. This can be done efficiently due to the convex nature of the optimization problem. When the conditional quantiles are linear functions of the explanatory variables \(x\), \eqref{eq:def:loss-pinball} becomes an optimization problem aimed at finding the best parameter \(\theta \in \XC\) such that \(\tau(x) = \theta^T x\). In this scenario, the minimization problem can be reformulated as a linear programming problem~\cite{chen2005computational}.
  Under certain regularity conditions, \cite{koenker2005quantile} demonstrate the weak convergence of the rescaled coefficients towards a normal distribution. Alternatively, one can use quantile regression forests as proposed in~\cite{meinshausen2006quantile}, which offer the advantage of providing a non-parametric estimator.


\paragraphformat{Local quantile regression.} 
\label{subsec:tau_local}
  Local quantile regression does not assume a global linear structure for the data. Instead, it allows the quantile function to adapt flexibly across different regions of the feature space. This flexibility is particularly useful when the relationship between the predictors and the response variable is complex and non-linear.
  By focusing on local regions, this method can capture local trends and variations in the data that might be missed by global models. Specifically, the local quantile can be obtained by minimizing the empirical weighted expected value of the pinball loss function \(\rho_{1-\alpha}\), defined as follows:
  \begin{equation}\label{eq:def:reg-tau}
    %\resizebox{.9\hsize}{!}{$
    \tau(x) \in \underset{t\in\R}{\min} \ac{\sum_{k=1}^{\ccount} w_{k}(x) \rho_{1-\alpha}\pr{\tilde{V}_{\varphi}(X_{k},Y_{k})-t}},
    %$}
  \end{equation}
  where $\{w_{k}(x)\}_{k=1}^{\ccount}$ are positive weights~\cite[Section~5.4]{wasserman2006all}.
  For instance, we can set $w_{k}(x)=\ccount^{-1} K_{h_{X}}^{(X)}(\|x-X_{k}\|)$, where for $h > 0$, $K_{h}^{(X)}(\cdot)= h^{-1} K_1^{(X)}(h^{-1} \cdot)$ is a kernel function measuring the similarity between $x$ and $X_{k}$. This kernel function satisfies $\int K^{(X)}_1(x) \rmd x=1$, $\int x K^{(X)}_1(x) \rmd x= 0$ and $\int x^2 K^{(X)}_1(x) \rmd x < \infty$; $h_{X}$ is a bandwidth controlling the kernel widths and can be adjusted to balance bias and variance in the density estimate.
  The choice of the kernel bandwidth can be data-driven for every $x \in \rset^d$, e.g., using Lepski's method. This approach is adaptive and asymptotically minimax over H\"older balls. For details on the algorithms, see~\cite{spokoiny2013local,reiss2009pointwise}.


\paragraphformat{Cumulative density function learning.}
\label{subsec:cdf}
  Kernel density estimation (KDE) enables to predict the density of unseen data points by interpolating between observed data using a kernel function.
 Following the approach in~\cite{bott2016adaptive}, we estimate the joint density using the Nadaraya-Watson (NW) kernel estimator; see~\cite{rosenblatt1969conditional} and~\cite[Section 4.2]{wasserman2006all}: 
  \begin{equation*}
    \resizebox{1\hsize}{!}{$
    \hat{f}_{(X, \tilde{V}_{\varphi})}(x, v)
    = \frac{1}{\ccount} \sum_{k=1}^{\ccount} K_{h_{X}}^{(X)}\pr{\|x-X_{k}\|} K_{h_{V}}^{(V)}\prn{v-\tilde{V}_{\varphi,k}}
    $},
  \end{equation*}
 where $K_{h_{X}}^{(X)}$ and $K_{h_{V}}^{(V)}$ are kernel functions and $h_X$ and $h_V$ are kernel bandwidths for the regressors and the conformity scores. Note that we use two different kernel functions for the regressors and the conformity score. The dependence of the kernel bandwidth on $m$ is implicit. 
  Define:
  \begin{equation*}
    w_{k}(x)
    = \frac{K_{h_{X}}^{(X)}\pr{\|x-X_{k}\|}}{\sum_{j=1}^{\ccount} K_{h_{X}}^{(X)}\pr{\|x-X_j\|}}.
  \end{equation*}
  %
  The conditional density is estimated as the ratio between the joint and the marginal densities (see~\cite{scott2015multivariate}): 
  \begin{equation}
    %\nonumber
    \hat{f}_{\tilde{V}_{\varphi}\mid X}(v\mid x)
    %= \frac{\hat{f}_{(X, \tilde{V}_{\varphi})}(x, v)}{\int \hat{f}_{(X, \tilde{V}_{\varphi})}(x, v) \rmd v}
    %\\
    = \sum\nolimits_{k=1}^{\ccount} w_k(x) K_{h_{V}}^{(V)}\prn{v-\tilde{V}_{\varphi,k}}.
    \label{eq:def:cond-density}
  \end{equation}
  %
  The estimator is often called the Nadaraya-Watson estimator of the conditional density because of its very close similarity with the regression estimator; see~\cite{wand1994kernel}.
  Note that it is easier to estimate the conditional density of $\tilde{V}_{\varphi} \mid X=x$ instead of the conditional density of the response $Y\mid X=x$ as this density is on $\rset$ whereas the response $y$ can belong to $\rset^q$~\cite{efromovich2010dimension}. A significant problem of kernel smoothing is bandwidth selection. The problem consists in the fact that optimal bandwidth depends on the unknown conditional and marginal density. This is the reason why some data-driven method needs to be applied. In this paper, we suggest a method for bandwidth selection based on a classical maximum likelihood approach~\cite{konevcna2019maximum}.

  Defining $G_{h_{V}}(v)= \int_{-\infty}^{v/h_{V}} K_{1}^{(V)}(\tilde{v}) \rmd \tilde{v}$ for any $v\in\R$, and integrating \Cref{eq:def:cond-density}, it yields
  \begin{equation}\label{eq:def:hatF-estimator}
    \hat{F}_{\tilde{V}_{\varphi}\mid X=x}(v \mid x)
    = \sum\nolimits_{k=1}^{\ccount} w_{k}(x) G_{h_{V}}\prn{v - \tilde{V}_{\varphi,k}}.
  \end{equation}
  %
  As the bandwidth parameter $h_{V}$ goes to zero as $\ccount \to \infty$, the bias of the empirical conditional cumulative density function (cdf) $\hat{F}_{\tilde{V}_{\varphi}\mid X}$ decreases since (see~\cite{fan1996estimation})
  \begin{equation*}
    \E\br{G_{h_{V}}\prn{ v - \tilde{V}_{\varphi} } \,\big\vert\, X_k}
    \xrightarrow[h_{V}\to 0]{\text{a.s.}} F_{\tilde{V}_{\varphi}\mid X}(v \mid X_k).
  \end{equation*}
   Therefore, as $h_{V} \to 0$, we can expect that
  \begin{equation*}
    \hat{F}_{\tilde{V}_{\varphi}\mid X}(v \mid x)
    \approx \sum\nolimits_{k=1}^{\ccount} w_{k}(x) F_{\tilde{V}_{\varphi}\mid X_k}(v \mid X_k).
  \end{equation*}
  %
  Finally, letting $h_{X} \to 0$, the empirical cdf converges to the true cdf of \( \tilde{V}_{\varphi} \mid X \) as \( \ccount \to \infty \). Precise theoretical derivations are given in \Cref{cor:epsilon-tau}.
  