% !TEX root = ../main.tex

For any $k\in[\ccount]$, set $\tilde{V}_{\varphi,k}=\adjinv^{-1}\circ V(X_{k},Y_{k})$. In the whole section, we assume that the random variables $X_1,\ldots,X_{\ccount}$ are i.i.d.
Therefore, the random variables $\tilde{w}_{k}(x) = K_{h_{X}}(\|x-X_{k}\|)$ defined for all $k\in[\ccount]$ are mutually independent. 
Moreover, let's consider the empirical cumulative function given for $x\in\XC$ and $v\in\R$, by
\begin{equation*}
  \hat{F}_{\tilde{V}_{\varphi}\mid X}(v\mid x)
  = \sum_{k=1}^{\ccount} w_{k}(x) \1_{\tilde{V}_{\varphi,k}\le v}.
\end{equation*}

\begin{theorem}\label{thm:uniform-cdf}
  If \Cref{ass:kernel-cdf} holds, then, it holds that
  \begin{multline*}
    \prob\Bigg( \norm{\hat{F}_{\tilde{V}_{\varphi}\mid X}(v\mid x) - F_{\tilde{V}_{\varphi}\mid X}(v\mid x)}_{\infty}
    \ge \pr{ \textstyle \sqrt{\frac{2\normn{K_{1}}_{\infty}}{ h_{X} }} + \sup_{t\in\R_+}\acn{\mathrm{M} t K_{1}(t)}} \sqrt{\frac{ 2 \log \ccount }{ \ccount \E[\tilde{w}_k(x)]^2 }}
    + \frac{2 D_{h_X}(x)}{\E \tilde{w}_k(x)}
    \Bigg)
    \\
    \le \frac{2 + 4 \E[\tilde{w}_k(x)]^{-1} \var[\tilde{w}_k(x)]}{\ccount}
    ,
  \end{multline*}
  where $D_{h_X}(x)$ is defined in~\eqref{eq:def:DhXr}.
\end{theorem}

\begin{proof}
  Let $x\in\XC$ and $v\in\R$ be fixed.
  First, recall that $F_{\tilde{V}_{\varphi}\mid X}(v\mid x)= \prob(V(X,Y)\le v\mid X=x)$.
  We will now control $\hat{F}_{\tilde{V}_{\varphi}\mid X}(v\mid x) - F_{\tilde{V}_{\varphi}\mid X}(v\mid x)$ as below:
  \begin{multline}\label{eq:eq:thm:uniform-cdf:2}
    \hat{F}_{\tilde{V}_{\varphi}\mid X}(v\mid x) - F_{\tilde{V}_{\varphi}\mid X}(v\mid x)
    = \sum_{k=1}^{\ccount} w_{k}(x) \ac{ \1_{\tilde{V}_{\varphi,k}\le v} -  F_{\tilde{V}_{\varphi}\mid X}(v\mid X_{k})}
    \\
    + \sum_{k=1}^{\ccount} w_{k}(x) \prob\pr{\tilde{V}_{\varphi}(X_{k},Y_{k}) \le v \,\vert\, X_{k} } - \prob\pr{\tilde{V}_{\varphi}(X,Y) \le v \,\vert\, X=x }.
  \end{multline}
  We now apply several results demonstrated later in this section:
  \begin{itemize}
    
    \item Applying~\Cref{lem:bound:concentration-Sum-wk} shows that
    \begin{equation*}
      \prob\pr{2\sum_{k=1}^{\ccount} \tilde{w}_{k}(x) \le \ccount \E[\tilde{w}_k(x)]}
      \le \frac{4 \var[\tilde{w}_k(x)]}{\ccount \E[\tilde{w}_k(x)]}.
    \end{equation*}

    \item Applying \Cref{thm:step1}, for any $\gamma\in(0,1)$, with probability at least $1-\gamma$, it holds that
    \begin{equation*}
      \sup_{v\in\R} \ac{ \sum_{k=1}^{\ccount} \tilde{w}_{k}(x) \ac{ \1_{\tilde{V}_{\varphi,k}\le v} -  F_{\tilde{V}_{\varphi}\mid X}(v\mid X_{k})} } < \sqrt{ \ccount \normn{K_{h_{X}}}_{\infty} \log \pr{\nofrac{1}{\gamma}}}.
    \end{equation*}

    \item Applying \Cref{lem:step3}, for any $\gamma\in(0,1)$, with probability at least $1-\gamma$, it follows that
    \begin{equation*}
      \sup_{v\in\R} \abs{ \sum_{k=1}^{\ccount} \tilde{w}_{k}(x) \ac{
      F_{\tilde{V}_{\varphi}\mid X}(v \mid X_k) - F_{\tilde{V}_{\varphi}\mid X}(v \mid x)
      }}
      \le \ccount D_{h_X}(x)
      + \sup_{t\in\R_+}\acn{\mathrm{M} t K_{1}(t)} \sqrt{\frac{\ccount \log(1/\gamma)}{2}}
      ,
    \end{equation*}
    where $D_{h_X}(x)$ is defined in \eqref{eq:def:DhXr}.

  \end{itemize}
  Lastly, set $\gamma=\ccount^{-1}$ and remark that $\normn{K_{h_{X}}}_{\infty} = h_{X}^{-1} \normn{K_{1}}_{\infty}$.
  Combining all the above bullet points with~\eqref{eq:eq:thm:uniform-cdf:2} implies, with probability at most $\frac{2}{\ccount}+\frac{4 \var[\tilde{w}_k(x)]}{\ccount \E[\tilde{w}_k(x)]}$, that
  \begin{equation*}
    \sup_{v\in\R} \abs{\sum_{k=1}^{\ccount} \tilde{w}_{k}(x) \1_{\tilde{V}_{\varphi,k}\le v} - F_{\tilde{V}_{\varphi}\mid X}(v\mid x)}
    \ge \pr{ \sqrt{\frac{2\normn{K_{1}}_{\infty}}{ h_{X} }} + \sup_{t\in\R_+}\acn{\mathrm{M} t K_{1}(t)}} \sqrt{\frac{ 2 \log \ccount }{ \ccount \E[\tilde{w}_k(x)]^2 }}
    + \frac{2 D_{h_X}(x)}{\E \tilde{w}_k(x)}
    .
  \end{equation*}
\end{proof}

\begin{corollary}
  If \Cref{ass:kernel-cdf} holds, then, it holds that
  \begin{equation}
  \label{eq:bound:corr-epsilon}
    \!\! \prob\pr{ \abs{\epsilon_{\tau}(x)}
    \ge \prBig{ \sqrt{2\normn{K_{h_{X}}}_{\infty}} + \sup_{t\in\R_+}\acn{\mathrm{M} t K_{1}(t)}} \sqrt{\frac{ 2 \log \ccount }{ \ccount \E[\tilde{w}_k(x)]^2 }}
    + \frac{2 D_{h_X}(x) }{\E \tilde{w}_k(x)}
    }
    \le \frac{2 + 4 \E[\tilde{w}_k(x)]^{-1} \var[\tilde{w}_k(x)]}{\ccount}
    ,
  \end{equation}
  where $D_{h_X}(x)$ is defined in~\eqref{eq:def:DhXr}, and $\lim_{h_X\to 0} D_{h_X}(x)=0$.
\end{corollary}

\begin{proof}
  For $x\in\XC$, since $\hat{F}_{\tilde{V}_{\varphi}\mid X=x}$ is continuous, applying \Cref{lem:link-cdf} with \Cref{thm:uniform-cdf} implies that \eqref{eq:bound:corr-epsilon} holds.
  Moreover, a calculation shows that 
  \begin{equation*}
    \limsup_{h_X\to 0} D_{h_X}(x)
    \le \normn{F_{X}(\cdot,x)}_{\infty} \int_{0}^{\infty} t^{d-1} K_{1}(t) \rmd t \times \limsup_{h_X\to 0} \ac{ h_{X}^{d-1} }.
  \end{equation*}
  Finally, by~\Cref{ass:kernel-cdf} we know that $\normn{F_{X}(\cdot,x)}_{\infty} < \infty$ and $\int_{\R_+} t^{d-1} K_1(t)\rmd t<\infty$. Therefore, it follows that $\limsup_{h_X\to 0} D_{h_X}(x)=0$.
\end{proof}

The next result shows that $\sum_{k=1}^{\ccount} \tilde{w}_{k}(x)$ concentrates around its mean with high probability.

\begin{lemma}\label{lem:bound:concentration-Sum-wk}
    If $\E[\tilde{w}_k(x)^2]<\infty$, then 
    \begin{equation*}\label{eq:tmh:step2:bound:5}
      \prob\pr{2\sum_{k=1}^{\ccount} \tilde{w}_{k}(x) \le \ccount \E[\tilde{w}_k(x)]}
      \le \frac{4 \var[\tilde{w}_k(x)]}{\ccount \E[\tilde{w}_k(x)]}.
    \end{equation*}
\end{lemma}

\begin{proof}
  Since the random variables $X_1,\ldots,X_{\ccount}$ are i.i.d., using the BienaymÃ©-Tchebychev inequality, we obtain
  \begin{equation*}
    \prob\pr{2\sum_{k=1}^{\ccount} \tilde{w}_{k}(x) \le \ccount \E[\tilde{w}_k(x)]}
    \le \frac{4 \var\pr{{\sum_{k=1}^{\ccount} \tilde{w}_{k}(x)}}}{\pr{\sum_{k=1}^{\ccount} \E \tilde{w}_{k}(x)}^2}
    = \frac{4 \var[\tilde{w}_k(x)]}{\ccount \E[\tilde{w}_k(x)]}.
  \end{equation*}
\end{proof}

\subsubsection{Step 1: intermediate results for \Cref{thm:uniform-cdf}}

For any $k\in[\ccount]$ and $v\in\R$, let's recall that $\tilde{w}_{k}(x) = K_{h_{X}}(\|x-X_{k}\|)$ and let's define
\begin{equation}\label{eq:def:Gv}
  G(v)
  = \sum_{k=1}^{\ccount} \tilde{w}_{k}(x) \ac{ \1_{\tilde{V}_{\varphi,k}\le v} - \prob\pr{\tilde{V}_{\varphi,k}\le v \,\vert\, X_{k}}}.
\end{equation}

  \begin{theorem}\label{thm:step1}
    Let $x\in\XC$ and $\gamma\in(0,1)$.
    With probability at least $1-\gamma$, the following inequality holds
    \begin{equation*}
      \sup_{v\in\R} \ac{ \sum_{k=1}^{\ccount} \tilde{w}_{k}(x) \ac{ \1_{\tilde{V}_{\varphi,k}\le v} -  F_{\tilde{V}_{\varphi}\mid X}(v\mid X_{k})} } < \sqrt{ \ccount \normn{K_{h_{X}}}_{\infty} \log \pr{\nofrac{1}{\gamma}}}.
    \end{equation*}
  \end{theorem}

  \begin{proof}
    Let $\theta>0$, and denote by $\acn{\epsilon_{k}}_{k\in[\ccount]}$ a sequence of i.i.d. Rademacher random variables.
    The independence of $\acn{\tilde{w}_{k}(x)}_{k\in[\ccount]}$ implies that
    \begin{equation*}
        \prod_{k=1}^{\ccount} \E\br{\cosh\pr{\theta \tilde{w}_{k}(x)}}
        = \prod_{k=1}^{\ccount} \pr{2^{-1} \E\br{\exp\pr{\theta \tilde{w}_{k}(x)}} + 2^{-1} \E\br{\exp\pr{-\theta \tilde{w}_{k}(x)}}}
        = \prod_{k=1}^{\ccount} \E\br{\exp\pr{\theta \epsilon_{k} \tilde{w}_{k}(x)}}.
    \end{equation*}
    %
    For all $x\in\R$, note that $\cosh(x)\le\exp(x^2/2)$. Thus, we deduce that
    \begin{equation*}
        \E\br{\exp\pr{\theta \epsilon_{k} \tilde{w}_{k}(x)}}
        \le \exp\pr{2^{-1} \theta^{2} \tilde{w}_{k}^{2}(x)}.
    \end{equation*}
    Hence, the previous lines yields that
    \begin{equation}\label{eq:bound:Delta:3}
        \prod_{k=1}^{\ccount} \E\br{\cosh\pr{\theta \tilde{w}_{k}(x)}}
        \le \exp\pr{
          2^{-1} \ccount \theta^2 \normn{K_{h_{X}}}_{\infty}^2 
        }.
    \end{equation}
    Set $\Delta>0$, applying~\Cref{lem:bound:DKW-revisited} with $G$ defined in~\eqref{eq:def:Gv} gives
    \begin{equation}\label{eq:bound:apply-lemma-DKW}
      \prob\pr{\sup_{v \in \R}\ac{G(v)} \ge \Delta}
      \le 2 \inf _{\theta>0} \ac{\rme^{- \theta \Delta} \prod_{k=1}^{\ccount} \E\br{\cosh\pr{\theta \tilde{w}_{k}(x)}}}.
    \end{equation}
    Now, consider the specific choice of $\theta_{\ccount}$ given by
    \[
        \theta_{\ccount}
        = \frac{\Delta}{\ccount \normn{K_{h_{X}}}_{\infty}}.
    \]
    Combining~\eqref{eq:bound:Delta:3} with the expression of $\theta_{\ccount}$, it follows that
    \begin{equation*}
        \inf_{\theta>0} \ac{e^{- \theta \Delta} \prod_{k=1}^{\ccount} \E\br{ \cosh\pr{\theta \tilde{w}_{k}(x)} }}
        \le \exp\pr{- \frac{\Delta^2}{\ccount \normn{K_{h_{X}}}_{\infty}} }
        .
    \end{equation*}
    Therefore, combining~\eqref{eq:bound:apply-lemma-DKW} with the previous inequality implies that
    \begin{equation}\label{eq:bound:Delta:4}
        \prob\pr{\sup_{v\in\R} \ac{G(v)} \ge \Delta}
        \le \exp\pr{- \frac{\Delta^2}{\ccount \normn{K_{h_{X}}}_{\infty}} }
        .
    \end{equation}
    For any $\gamma \in (0, 1)$, setting $\Delta = \sqrt{\ccount \normn{K_{h_{X}}}_{\infty} \log(1/\gamma)}$ gives
    \begin{equation}\label{eq:bound:Delta:5}
        \prob\pr{\sup_{v\in\R} \ac{G(v)} < \sqrt{ \ccount \normn{K_{h_{X}}}_{\infty} \log(\nofrac{1}{\gamma}) }}
        \ge 1 - \gamma
        .
    \end{equation}
  \end{proof}
  
  The following statement controls $\prob\prn{\sup_{v \in \R}\acn{G(v)} \ge \epsilon}$. Its proof is similar to the extension of the Dvoretzky--Kiefer--Wolfowitz inequality provided in \cite[Appendix~B]{plassier2024efficient}.

  \begin{lemma}\label{lem:bound:DKW-revisited}
      For any  $\Delta>0$, the following inequality holds
      \begin{equation*}
          \prob\pr{\sup_{v \in \R}\ac{G(v)} \ge \Delta}
          \le 2 \inf _{\theta>0} \ac{\rme^{- \theta \Delta} \prod_{k=1}^{\ccount} \E\br{\cosh\pr{\theta \tilde{w}_{k}(x)}}},
      \end{equation*}
      where $G$ is defined in~\eqref{eq:def:Gv}.
  \end{lemma}

  \begin{proof}
      First, for any $\theta>0$, applying Markov's inequality gives
      \begin{equation}\label{eq:bound:markov-theta}
          \prob\pr{\sup_{v \in \R} \ac{G(v)} \ge \Delta}
          \le \rme^{-\theta \Delta} \E\br{\exp \pr{\theta \sup_{v \in \R} \ac{G(v)}}}.
      \end{equation}
      Moreover, \Cref{lem:bound:symmetrization} shows that
      \begin{equation*}%\label{eq:bound:markov-theta:2}
          \E\br{\exp \pr{\theta \sup_{v \in \R}\ac{G(v)}}} 
          \le 2 \prod_{k=1}^{\ccount} \E\br{\cosh\pr{\theta \tilde{w}_{k}(x)}}.
      \end{equation*}
      Plugging the previous inequality into~\eqref{eq:bound:markov-theta}, and minimizing the resulting expression with respect to $\theta$ yields:
      \begin{equation*}
          \prob\pr{\sup_{v \in \R}\ac{G(v)} \ge \Delta}
          \le 2 \inf _{\theta>0} \ac{\rme^{- \theta \Delta} \prod_{k=1}^{\ccount} \E\br{\cosh\pr{\theta \tilde{w}_{k}(x)}}}.
      \end{equation*}
  \end{proof}

  \begin{lemma}\label{lem:bound:symmetrization}
    Let $\theta>0$, we have
    \begin{equation*}
        \E\br{\exp\pr{\theta \sup_{v \in \R}\ac{G(v)}}}
        \le 2 \prod_{k=1}^{\ccount} \E\br{\cosh\pr{\theta \tilde{w}_{k}(x)}}.
    \end{equation*}
  \end{lemma}

  \begin{proof}
    Let $\theta>0$ be fixed, since $t\mapsto \rme^{\theta t}$ is continuous and increasing, the supremum can be inverted with the exponential:
    \begin{equation*}
        \E\br{\exp\pr{\theta \sup_{v \in \R}\ac{G(v)}}}
        = \E\br{\sup_{v \in \R} \exp\pr{\theta G(v)}}.
    \end{equation*}
    For any $k\in[\ccount]$, consider $\tilde{Y}_{k}$ an independent copy of the random variable $Y_{k}$, and denote $\bar{V}_{\varphi,k}=\tilde{V}_{\varphi}(X_{k},\tilde{Y}_{k})$. The linearity of the expectation gives
    \begin{equation*}
        \sum_{k=1}^{\ccount} \tilde{w}_{k}(x) \pr{ \1_{\tilde{V}_{\varphi,k} \le v} - \E\br{\1_{\tilde{V}_{\varphi,k} \le v} \,\vert\, X_{k}} }
        = \E\br{\sum_{k=1}^{\ccount} \tilde{w}_{k}(x) \pr{ \1_{\tilde{V}_{\varphi,k} \le v} - \1_{\bar{V}_{\varphi,k} \le v} } \,\Big\vert\, \acn{X_{k}, Y_{k}}_{k=1}^{\ccount}}.
    \end{equation*}
    Therefore, the Jensen's inequality implies
    \begin{align*}
        \E\br{\exp\pr{\theta \sup_{v \in \R}\ac{G(v)}}}
        &= \E\br{\sup_{v \in \R} \exp\pr{\theta \E\br{\sum_{k=1}^{\ccount} \tilde{w}_{k}(x) \ac{ \1_{\tilde{V}_{\varphi,k} \le v} - \1_{\bar{V}_{\varphi,k} \le v}} \,\bigg\vert\, \acn{X_{k}, Y_{k}}_{k=1}^{\ccount}}}}
        \\
        &\le \E\br{\sup_{v \in \R} \exp\pr{\theta \sum_{k=1}^{\ccount} \tilde{w}_{k}(x) \ac{ \1_{\tilde{V}_{\varphi,k} \le v} - \1_{\bar{V}_{\varphi,k} \le v}}}}.
    \end{align*}
    Let $\{\epsilon_{k}\}_{k\in[\ccount]}$ be i.i.d. random Rademacher variables independent of $\{(X_{k}, Y_{k}, \tilde{Y}_{k})\}_{k=1}^{\ccount}$. Since $\1_{\tilde{V}_{\varphi,k} \le v} - \1_{\bar{V}_{\varphi,k} \le v}$ is symmetric, we have
    \begin{equation*}
        \E\br{\sup_{v \in \R} \exp\pr{\theta \sum_{k=1}^{\ccount} \tilde{w}_{k}(x) \ac{ \1_{\tilde{V}_{\varphi,k} \le v} - \1_{\bar{V}_{\varphi,k} \le v}}}}
        = \E\br{\sup_{v \in \R} \exp\pr{\theta \sum_{k=1}^{\ccount} \epsilon_{k} \tilde{w}_{k}(x) \ac{ \1_{\tilde{V}_{\varphi,k} \le v} - \1_{\bar{V}_{\varphi,k} \le v}}}}.
    \end{equation*}
    %
    Using the Cauchy-Schwarz's inequality, we deduce that
    \begin{equation*}
        \E\br{\exp \pr{\theta \sup_{v \in \R}\ac{G(v)}}} 
        \le \E\br{\sup_{v \in \R} \exp\pr{2 \theta \sum_{k=1}^{\ccount} \epsilon_{k} \tilde{w}_{k}(x) \1_{\tilde{V}_{\varphi,k} \le v}}}.
    \end{equation*}
    Given the random variables $\{\tilde{V}_{\varphi,k}\}_{k=1}^{\ccount}$, denote by $\sigma$ the permutation of $[\ccount]$ such that $\tilde{V}_{\varphi,\sigma(1)}\le\cdots\le \tilde{V}_{\varphi,\sigma(\ccount)}$. In particular, it holds
    \begin{equation*}
        \sum_{k=1}^{\ccount} \epsilon_{k} \tilde{w}_{k}(x) \1_{\tilde{V}_{\varphi,k} \le v}
        = 
        \begin{cases}
            0 & \text { if } v < \tilde{V}_{\varphi,\sigma(1)} 
            \\ 
            \sum_{j=1}^i  \epsilon_{\sigma(j)} \tilde{w}_{\sigma(j)}(x) & \text { if } \tilde{V}_{\varphi,\sigma(i)} \le v < \tilde{V}_{\varphi,\sigma(i+1)}
            \\ 
            \sum_{j=1}^{\ccount} \epsilon_{\sigma(j)} \tilde{w}_{\sigma(j)}(x) & \text { if } v \ge \tilde{V}_{\varphi,\sigma(\ccount)}
        \end{cases}.
    \end{equation*}
    Thus, can rewrite the supremum as
    \begin{equation*}
        \sup_{v \in \R} \exp\pr{2 \theta \sum_{k=1}^{\ccount} \epsilon_{k} \tilde{w}_{k}(x) \1_{\tilde{V}_{\varphi,k} \le v}}
        \le \sup_{0 \le i \le n} \exp \pr{2 \theta \sum_{j=1}^i \epsilon_{\sigma(j)} \tilde{w}_{\sigma(j)}(x)}.
    \end{equation*}
    Applying \Cref{lem:bound:weighted-rademacher}, we finally obtain that
    \begin{multline*}
        \E\br{\sup_{v \in \R} \exp\pr{2 \theta \sum_{k=1}^{\ccount} \epsilon_{k} \tilde{w}_{k}(x) \1_{\tilde{V}_{\varphi,k} \le v}} \,\bigg\vert\, \acn{X_{k}, Y_{k}}_{k=1}^{\ccount}} 
        \\
        \le \E\br{\sup_{0 \le i \le \ccount} \exp \pr{2 \theta \sum_{j=1}^i \epsilon_{\sigma(j)} \tilde{w}_{\sigma(j)}(x)} \,\bigg\vert\, \acn{X_{k}, Y_{k}}_{k=1}^{\ccount}}
        % \\
        \le 2 \prod_{k=1}^{\ccount} \cosh\pr{\theta \tilde{w}_{k}(x)}.
    \end{multline*}
  \end{proof}

  \begin{lemma}\label{lem:bound:weighted-rademacher}
    Let $\acn{\epsilon_{i}}_{i\in[n]}$ be i.i.d Rademacher random variables taking values in $\{-1,1\}$, then for any $\theta>0$ and $\{p_{j}\}_{j\in[\ccount]}\in\R^{\ccount}$, we have
    \[
        \E\br{\exp \pr{\theta \sup_{0\le i \le \ccount} \sum_{j=1}^i p_{j} \epsilon_{j}}} \le 2 \prod_{k=1}^{\ccount} \cosh\pr{\theta p_{k}}.
    \]
    By convention, we consider $\sum_{j=1}^0 p_{j} \epsilon_{j}=0$.
  \end{lemma}

\subsubsection{Step 2: intermediate results for \Cref{thm:uniform-cdf}}

For all $x\in\XC$ and $v\in\R$, define the conditional cumulative density function $F_{\tilde{V}_{\varphi}\mid X}(v \mid x)$ as
\begin{equation*}
  F_{\tilde{V}_{\varphi}\mid X}(v \mid x)
  = \prob\pr{\tilde{V}_{\varphi}(X,Y) \le v \,\vert\, X=x }.
\end{equation*}
Moreover, recall that we denote by $f_X$ the density with respect to the Lebesgue measure of the random variable $X$.
Using the spherical coordinates, we write by $\tilde{x}_{t,\theta}=(t\cos \theta_1, t\sin\theta_1\cos\theta_2,\ldots,t\sin\theta_1\cdots\sin\theta_{d-1})$ the coordinate of $\tilde{x}\in\XC$, where $\|\tilde{x}\|=t$. Additionally, we define 
\begin{equation}\label{eq:def:FXtx}
  F_{X}(t,x)
  = \int_{[0,\pi]^{d-2}\times [0,2\pi)} f_{X}(x-\tilde{x}_{t,\theta}) \prod_{i=1}^{d-2} \sin(\theta_i)^{d-1-i} \rmd \theta_{1}\cdots \rmd \theta_{d-1}.
\end{equation}
Note that
\begin{equation*}
  \int_{\R_+} t^{d-1} F_{X}(t,x) \rmd t
  = \int_{\XC} f_X(x-\tilde{x}) \rmd \tilde{x}
  = 1.
\end{equation*}
%
Under~\Cref{ass:kernel-cdf} the cumulative density function $x\mapsto F_{\tilde{V}_{\varphi}\mid X}(v \mid x)$ is $\mathrm{M}$-Lipschitz. In this case, for any $h_X>0$, let's consider
\begin{equation}\label{eq:def:DhXr}
  D_{h_X}(x)
  = h_{X}^{d-1} \normn{F_{X}(\cdot,x)}_{\infty} \int_{0}^{\infty} t^{d-1} K_{1}(t) \rmd t.
\end{equation}

\begin{lemma}\label{lem:step3}
  Assume that \Cref{ass:kernel-cdf} holds and let $\gamma\in(0,1)$. 
  With probability at least $1-\gamma$, it holds
  \begin{equation*}
    \sup_{v\in\R} \abs{ \sum_{k=1}^{\ccount} \tilde{w}_{k}(x) \ac{
      F_{\tilde{V}_{\varphi}\mid X}(v \mid X_k) - F_{\tilde{V}_{\varphi}\mid X}(v \mid x)
    }}
    \le \ccount D_{h_X}(x)
    + \sup_{t\in\R_+}\acn{\mathrm{M} t K_{1}(t)} \sqrt{\frac{\ccount \log(1/\gamma)}{2}}
    .
  \end{equation*}
\end{lemma}

\begin{proof}
  First of all, using~\Cref{ass:kernel-cdf} implies that
  \begin{equation*}
    \sup_{v\in\R} \abs{\sum_{k=1}^{\ccount} \tilde{w}_{k}(x) \ac{F_{\tilde{V}_{\varphi}\mid X}(v \mid X_k) - F_{\tilde{V}_{\varphi}\mid X}(v \mid x)}}
    \le \sum_{k=1}^{\ccount} \tilde{w}_{k}(x) \min\ac{1, \mathrm{M}\normn{x-X_k}}.
  \end{equation*}
  For every $k\in[\ccount]$, let's consider $Z_k = \tilde{w}_{k}(x) \min\ac{1, \mathrm{M}\normn{x-X_k}}$.
  Since $\tilde{w}_k=K_{h_X}(\norm{x-X_k})$, we have
  \begin{equation}\label{eq:bound:Zk:1}
    Z_k
    \le \max\pr{\sup_{0\le \mathrm{M} t\le 1}\ac{ \mathrm{M} t K_{h_X}(t)}, \sup_{\mathrm{M} t > 1}\ac{K_{h_X}(t)}}.
  \end{equation}
  By calculation, we get
  \begin{equation}\label{eq:bound:Zk:2}
    \sup_{0\le \mathrm{M} t\le 1}\ac{ \mathrm{M} t K_{h_X}(t) }
    = \mathrm{M} \sup_{0\le \mathrm{M} t\le 1}\ac{ \frac{t}{h_X} K_{1}\pr{\frac{t}{h_X}} }
    = \mathrm{M} \sup_{0\le t\le (h_X\mathrm{M})^{-1}}\ac{ t K_{1}\pr{t} }.
  \end{equation}
  We also have
  \begin{equation}
  \label{eq:bound:Zk:3}
    \sup_{\mathrm{M} t > 1}\ac{K_{h_X}(t)}
    = \sup_{\mathrm{M} t > 1} \ac{\frac{1}{h_X} K_{1} \pr{\frac{t}{h_{X}}}}
    \le \mathrm{M} \sup_{\mathrm{M} t > 1} \ac{\frac{t}{h_X} K_{1} \pr{\frac{t}{h_{X}}}}
    = \mathrm{M} \sup_{t> (h_X\mathrm{M})^{-1}}\ac{ t K_{1}\pr{t} }.
  \end{equation}
  Thus, combining~\eqref{eq:bound:Zk:1}-\eqref{eq:bound:Zk:2} with~\eqref{eq:bound:Zk:3} yields
  \begin{equation*}
    0 
    \le Z_k 
    \le \mathrm{M}\sup_{t\in\R_+}\acn{t K_{1}(t)}.
  \end{equation*}
  Applying Hoeffding's inequality, for any $t>0$, it follows
  \begin{equation}\label{eq:bound:Zk:4}
    \prob\pr{ \sum_{k=1}^{\ccount} (Z_k-\E Z_k) \ge t - \ccount \E Z_1}
    \le \exp\pr{-\frac{2(t-\ccount \E Z_1)^{2}}{\ccount \sup_{t\in\R_+}\acn{\mathrm{M} t K_{1}(t)}^2}}.
  \end{equation}
  Let $\gamma\in(0,1)$ and set:
  \begin{equation*}
    t_{\gamma} = \ccount \E Z_1 + \sup_{t\in\R_+}\acn{\mathrm{M} t K_{1}(t)} \sqrt{\frac{\ccount \log(1/\gamma)}{2}}.
  \end{equation*}
  Using~\eqref{eq:bound:Zk:4}, it holds that
  \begin{equation*}
    \prob\pr{ \sum_{k=1}^{\ccount} (Z_k-\E Z_k) \ge t_{\gamma} - \ccount \E Z_1}
    \le \gamma.
  \end{equation*}
  We will now bound $t_{\gamma}$. To do this, we will control $\E Z_1$:
  \begin{equation}\label{eq:bound:Zk:5}
    \E\br{\tilde{w}_{k}(x) \min\ac{1, \mathrm{M}\normn{x-X_k}}}
    = \int_{\tilde{x} \in \XC} \pr{\mathrm{M} \normn{\tilde{x}} \wedge 1} K_{h_X}(\normn{\tilde{x}}) f_{X}(x-\tilde{x}) \rmd \tilde{x}.
  \end{equation}
  % Let's find an upper bound for the right-hand side term. We have
  % \begin{multline*}
  %   \int_{\tilde{x}\colon \|\tilde{x}\| > \mathrm{M}^{-1}} K_{h_X}(\normn{\tilde{x}}) f_{X}(x-\tilde{x}) \rmd \tilde{x}
  %   \le \sup_{t > \mathrm{M}^{-1}} \ac{ K_{h_X}(t) }
  %   \\
  %   \le \sup_{\mathrm{M} t > 1} \ac{\frac{\mathrm{M} t}{h_X} K_{1}\pr{\frac{t}{h_X}}}
  %   = \mathrm{M} \sup_{t > (h_X \mathrm{M})^{-1}} \ac{ t K_{1}(t) }.
  % \end{multline*}
  Using the spherical coordinates, a change of variables gives  
  \begin{equation*}
    \int_{\tilde{x} \in \XC} \pr{\mathrm{M} \normn{\tilde{x}} \wedge 1} K_{h_X}(\normn{\tilde{x}}) f_{X}(x-\tilde{x}) \rmd \tilde{x}
    = \int_{t=0}^{\infty} t^{d-1} (\mathrm{M} t \wedge 1) K_{h_X}(t) F_{X}(t,x) \rmd t,
  \end{equation*}
  where $F_{X}(t,x)$ is given in~\eqref{eq:def:FXtx}.
  Therefore, it immediately follows that
  \begin{equation*}
    \int_{\tilde{x} \in \XC} \pr{\mathrm{M} \normn{\tilde{x}} \wedge 1} K_{h_X}(\normn{\tilde{x}}) f_{X}(x-\tilde{x}) \rmd \tilde{x}
    \le h_X^{d-1} \int_{t=0}^{\infty} t^{d-1} K_{1}(t) F_{X}(h_X t,x) \rmd t.
  \end{equation*}
  Plugging the previous bound in~\eqref{eq:bound:Zk:5} shows that
  \begin{equation*}
    \E Z_1
    \le D_{h_X}(x),
    \qquad
    \text{ where $D_{h_X}(x)$ is provided in~\eqref{eq:def:DhXr}.}
  \end{equation*}
\end{proof}
