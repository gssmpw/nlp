\section{Introduction}

Machine learning can be expensive, in both computation and memory requirements.
As a result, it is common to delegate many tasks involving neural networks---such 
as training, fine-tuning, or inference---to external compute providers with significant capacity 
and specialized hardware.
In practice this delegation is typically done with no guarantees of correctness.
A dishonest or compromised server might return incorrect outputs.
For example, a lazy server might not train an LLM for the duration claimed and return an approximate result.
An outright malicious \server could perform data poisoning attacks or insert a backdoor into the trained model.


How can clients be assured that the results they receive is honest? 
Cryptographic proof systems offer the strongest guarantee 
solution to this problem, enabling any untrusted party to produce a short and easy-to-check mathematical proof that they ran an arbitrary program correctly. 
While these proof systems are generic and can be applied directly to ML tasks, they are inefficient.
Even using state-of-the-art proof techniques, computing the proof is at least 4 orders of magnitude more expensive than running the program itself. 
For example, for a 125 million parameter neural network, a proof of inference for a single forward pass takes over a minute~\cite{sun24zkllm}, and for a training step takes about 15 minutes~\cite{abbaszadeh24zero}. 
This limits the practical application of these techniques to only very small, extremely valuable ML tasks.

Heuristic, ML-specific techniques, such as Proof-of-Learning~\cite{jia21proofoflearning} 
or Proof-of-Training-Data~\cite{choi24tools}, 
trade off the formal guarantees of cryptographic proofs for efficiency.
In both methods, model trainers log and share intermediate checkpoints with the verifiers. 
Under the assumption that valid executions tend to result in small weight updates, 
verifiers in Proof-of-Training re-execute the training segments 
representing the largest magnitude changes in weights. 
In Proof-of-Training-Data, verifiers query checkpoints to check if the models ``memorized'' 
(i.e., overfit to) the data they were just trained on.
These techniques are narrow in scope 
and lack rigorous security guarantees. 
Indeed, many practical attacks have been demonstrated~\cite{fang23proof}. 
As a result these methods are perhaps only applicable to low-value tasks.




Thus, we're still motivated to look for a method to provide concrete guarantees while imposing practical overheads on compute providers. 
A simple approach is for a client to delegate the same task to multiple compute providers.
Naively, the client could take a majority vote and ensure correctness assuming most of the servers are honest.
In fact, the client can do better and ensure correctness if \emph{any single compute provider} is honest.
The key idea is efficient \emph{dispute resolution}: if two servers produce different output, the client needs to determine which is incorrect.
A naive approach is to ask a trusted party, the ``referee'', to re-run the computation to determine the right output.
However, if we had such a trusted referee with sufficient capacity, the client could have simply asked them to run the computation in the first place.
Therefore we are most interested in the case where the referee is highly limited computationally, for example the client requesting the work themselves also acts as the referee.




Canetti et al.~\cite{canetti13refereed} shows an approach to significantly reduce the work done by the referee based on a simple observation: if two parties disagree on the output of a program, there must be some \emph{point of divergence} in their computation path. 
That is, there is some program step in which they start with the same state (for a CPU program, for example, this state would comprise all contents of registers and memory) but end up with different states. 
For short computations, each compute provider might send their full sequence of program states (or \emph{computation trace}) to the referee.
For longer computations, Canetti et al.~\cite{canetti13refereed} proposed using interaction
between the referee and the compute providers and succinct \emph{commitments} to program states
to efficiently find the point of divergence via binary search, without sending the full computation trace to the referee.
Once it is found, the referee need only re-execute a single computation step at the point of divergence, which by definition at least one of the compute providers must have executed incorrectly.
Hence, the referee can prove that at least one party is incorrect.




In this work, we apply the refereed delegation model to ML programs. This requires overcoming two main challenges:

\textbf{Challenge 1: Adapting dispute resolution to ML programs.} 
Existing refereed delegation protocols are designed for CPU programs and do not efficiently translate to the scale of modern neural networks.
Program states, consisting of neural network parameters, are many gigabytes in size, and program steps involve highly parallelized code (often run on GPUs or similar hardware).
We must also enumerate all possible deviations from honest behavior, and design commitments to intermediate program checkpoints that allow only an honest server to prove that their computation was done correctly.
We propose Verde, a dispute resolution protocol tailored to modern ML programs, such as the training of LLMs. 
We treat neural network programs as an abstract state machine defined by learnable parameters and a computational graph. 
We narrow down the dispute in two phases: the first narrows it down to a single training or inference step, and the second narrows it down to a single operator in the graph.
The referee's only needs to compute a single operator in the computational graph, which can be performed with two orders of magnitude less compute resources than it takes to run the model itself. 
The \servers are only required to store and hash intermediate training checkpoints on top of re-executing small segments of training, thus incurring low overheads. 


\paragraph{Challenge 2: Ensuring bitwise reproducibility across hardware setups.}
The refereed delegation model assumes that honest servers will always compute the same result for the same program, but this may not be true if they are using different hardware.
In particular, hardware may provide different numerical results because floating point operations are not guaranteed to be associative, even when following the \ieeefp standard: $(a+b)+c$ and $a+(b+c)$ can be different when the operands are floating-point values.
Given the same parallelized program,
owing to differences in architecture, distinct hardware implementations may (and often do) execute $a+b+c$
in different orders, ending up with different results.
To overcome this, we design \repops (Reproducible Operators), a library that implements bitwise reproducible versions of popular ML operators. 
It works by ensuring that floating point operations are performed in the same order, eliminating hardware non-determinism.
\repops currently provides CUDA implementations of all the operators needed to run \distilbert and the \llama family of models. 
We find that the overhead of \repops is moderate.
In fact, when comparing against \pytorch using cuDNN~\cite{chetlur14cuDNN} implementations of operators,
\repops matrix multiplications (of dimensions larger than $2^{10}$) incur less than 30\% overhead 
when tested on various GPUs. 
Similarly, 
both inference and training on \llama-3.1-1B incur around 60\% overhead \repops 
on an A100 GPU (with 40 GB VRAM). 


By definition, as refereed delegation requires multiple servers executing the same program, 
it incurs at least a factor 2 total overhead relative to the program itself. 
However, even with the use of \repops, 
this refereed delegation can be achieved with totally less than an order of magnitude overhead. 
This is dramatically more efficient than cryptographic proofs (4 orders of magnitude).
Our approach could be used for opportunistic auditing of delegated computation 
with a client serving as a referee, 
or as a building block in decentralized training applications 
with an on-chain smart contract serving as a referee.







