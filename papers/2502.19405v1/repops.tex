\section{Reproducible Operations (\repops) library}
\label{sec:repops}


Different GPUs, such as those from NVIDIA, AMD, or Intel, 
each come with their own architecture and capabilities.
ML practitioners often tailor to specific hardware, balancing factors like cost, power consumption, 
and computational throughput.
Even between hardware from the same manufacturer, and even when 
adhering to standards like \ieeefp, 
inherent non-determinism in floating-point operations can cause 
numerical discrepancies between different executions of the same program. 
This section describes the source of this non-determinism 
and \repops, 
our library implementing a suite of reproducible ML operators 
which overcome hardware non-determinism.

\subsection{The source of hardware non-determinism}
\label{sec:repops-issue}

Even when following the \ieeefp standard, floating points operations 
are non-associative, meaning the order in which the operations are performed impacts 
the result. 
That is, $(a+b)+c$ can be different from $(a+c)+b$ because each 
addition operation involves rounding that introduces variations. 
These discrepancies arise in parallel programs where functions are split 
and executed 
across multiple threads. 
As GPUs are designed to execute highly parallel code, 
simple differences in architecture, 
such as the number of cores and memory capacity, 
naturally exacerbate this issue.
Even within the same device, storing tensors with 
with different arrangements in memory (that is, the strides)
can alter the sequence of operations, potentially leading to discrepancies.

To overcome this issue, \repops re-implements common ML operators and mathematical 
functions (like exp, sin, cos, tanh) in a way that controls the 
order of 
floating point operators across hardware setups.
On top of this, 
\repops use built-in support for deterministic pseudorandomness generation in \pytorch and \cuda.

\subsection{Controlling the order of operations.} 
\label{sec:repops-order-control}


To enforce a fixed ordering of operations 
while still maximizing parallelism, 
we identify dimensions along which operators can be parallelized 
without introducing non-determinism. 
For dimensions where the order does not affect the outcome, 
parallelization can proceed freely. 
In the dimensions where order is critical, 
we either perform the operations serially or 
synchronize threads to enforce a deterministic execution order. 

We highlight this technique for matrix multiplication, 
which is the most prevalent operator in neural networks. 
Matrix multiplication is represented by three nested for-loops  
where only the third loop introduces potential reordering of floating-point operations.
By parallelizing the first two loops and executing the third loop sequentially 
we can ensure fixed ordering of operations.
\begin{lstlisting}[
]
# repops matrix multiplication
(*@\textcolor{darkspringgreen}{for i = 0 to M-1: \# any order}@*)
    (*@\textcolor{darkspringgreen}{for j = 0 to N-1: \# any order}@*)
        sum = 0
        (*@\textcolor{red}{for k = 0 to K-1: \# fixed order}@*)
            a = A[i][k]
            b = B[k][j]
            sum = sum + a *      b
        (*@\textcolor{black}{C[i][j] = sum}@*)
\end{lstlisting}


Perhaps surprisingly, our benchmarks (Section~\ref{sec:experiments}) reveal that this simple strategy 
incurs manageable overhead.
For matrices larger than  $2^{10}$, our implementation takes 
around 30--60\% extra time as compared 
to non-reproducible matrix multiplication for matrices 
on the GPUs we tested. 
Although this approach may under-utilize parallel processors in some cases, 
it strikes a balance between achieving reproducibility and maintaining performance efficiency. 


\subsection{Limitations and future work}

As mentioned, limiting parallelism in the code can under-utilize the compute 
resources available. From our benchmarks 
in Section~\ref{sec:experiments}), one example is matrix multiplication in smaller dimensions,
where the overhead for 256-bit matrices is around $200\%$ on the setups we tested. 
Another is end-to-end training for the relatively smaller \distilbert LLM, 
which incurs a $300\%$ overhead on certain setups.
Mitigating these overheads requires careful tuning and hard-coding of parameters 
for specific hardware.

An inherent assumption for \repops is that 
the hardware adheres to a single floating point operations standard, such as \ieeefp. 
In practice, this limits the hardware and precisions that \repops can run on. 
For example, Nvidia GPUs generally support \ieeefp-compliant FP32 arithmetic, 
whereas that for FP16 is not as widespread. 

\paragraph{Future work: Model parallelism.} 
\repops currently ensures reproducibility between setups when programs are run on a single 
GPU in each setup.
Modern neural network training and inference is 
increasingly done in a distributed setting across multiple GPUs 
using parallelization techniques like data, pipeline, and tensor parallelism. 
Ensuring reproducibility when setups use multiple nodes 
and various types of parallelism 
requires coordinating the division of operators and the order in which 
they're combined during collective communication (such as All-Reduce). 
This can be done using the similar principle of controlled parallelization.
We leave the design and implementation to future work. 




