\section{Related work}
\label{sec:related-work}

\paragraph{Refereed delegation.} 
Canetti et al.~\cite{canetti13refereed} proposed refereed delegation, 
 with a dispute resolution protocol implementation for CPU (x86) computation.
This idea was adapted to the decentralized setting as a scaling solution for blockchains,
becoming the foundation for ``optimistic rollups'' like TrueBit~\cite{teutsch19scalable}, 
  Arbitrum~\cite{kalodner18arbitrum}, and Optimism~\cite{optimism}.
  
The closest work to our own is Agatha~\cite{zheng18agatha}, which developed a smart contract
to verify neural network inference using a refereed delegation-like protocol.
Our protocol can be viewed as taking this a step further to 
design general dispute resolution protocols 
for both inference and training, which involves capturing a wider variety of dishonest behavior.




\textbf{Hardware non-determinism.} 
To our knowledge, 
  the RepDL library~\cite{repdl_library} is the first open-source implementation of operators made 
  reproducible by controlling the order of floating point operations.
We adopt this approach and expand it to support 
  inference and training for modern LLMs such as \distilbert and the \llama 
  family. 
  
The recent work of \cite{srivastava24optimistic}, also motivated by verifying ML programs,
 tackles the problem of hardware 
non-determinism in a different way, as follows: 
when performing floating point operations in a particular precision (say, in FP32), 
they perform the operator in a higher precision (here, FP64) 
and then round results back to the target precision. 
As rounding can be non-deterministic when 
the numerical error impacts the decision (higher or lower), the party additionally records 
rounding certain rounding decisions they make.
Their assumption is that the final rounded value is deterministic when assuming 
that all numerical discrepancy errors are contained within the higher precision bits.
Thus, when another party re-runs the computation, running all floating point operations 
in the higher precision and 
following the same rounding decisions when specified, they will obtain 
bitwise identical result. 

Our approach with \repops offers several advantages. 
First, the security of Srivastava et al.'s approach assumes that floating point errors 
are always contained in the higher precision bits, which is not guaranteed. 
Additionally \repops works with any lower precision, 
which is crucial as modern machine learning are commonly implemented in lower precision 
(particularly FP16)
to save significant memory and compute resources.
\repops also doesn't require collecting, storing, and transferring the rounding decisions, 
which can be very large: they 18 MB of rounding data per training step 
of the 1.5 billion parameter GPT-2 model.
While \repops incurs a higher training overhead (1.6$\times$ for \llama-1B; 
see Table~\ref{table:repops_models}) 
than their method (1.2-1.4x for GPT-2 as stated in Section 6.1 of ~\cite{srivastava24optimistic}), 
this is mitigated by the ability to use lower precision training.
