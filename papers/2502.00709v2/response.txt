\section{Related Works}
\subsection{LLMs for Information Retrieval}
Text retrieval is a key component in a multitude of knowledge-driven Natural Language Processing (NLP) applications **Bae, "Relevance-based Sampling for Efficient Neural IR"**. In practice, this task is approached with a multi-stage ranking pipeline, typically consisting of an initial, cost-effective retriever followed by a more sophisticated reranker to refine the results**Karpukhin et al., "Dense Passage Retrieval for Long-Contextualized Question Answering"****Hofstätter and Lehmann, "BERT-BASED RETRIEVAL MODELS FOR QUESTION ANSWERING"**. Large language models (LLMs) have shown remarkable efficacy in information retrieval tasks**Radford et al., "Improving Language Understanding by Generative Models"**. Supervised reranking methods**Nogueira and Cho, "Passage Re-ranking with BERT"** have traditionally relied on fine-tuning transformer-based models with copious training data, such as the MS MARCO v1 passage ranking dataset**Nguyen et al., "MS MARCO: A Dataset for Next-Question Answering"**. However, recent explorations involve LLMs in zero-shot unsupervised reranking. Pointwise approaches evaluate passage relevance individually**Bai and Carpuano, "Zero-Shot Transfer Learning with Autoencoders for Natural Language Processing"**, whereas pairwise strategies compare two documents' relevancies for a given query**Chen et al., "Query-Adaptive Co-Matching Network for Zero-Shot Cross-Media Retrieval"**. Listwise methods, which directly reorder document relevance collectively, have achieved state-of-the-art performance**Hofstätter and Lehmann, "BERT-BASED RETRIEVAL MODELS FOR QUESTION ANSWERING"**. This study introduces a novel multi-role reranking framework, \ours, which significantly enhances listwise reranking performance.


\subsection{Query Rewriting}
Original queries in traditional IR systems are often short or ambiguous, leading to vocabulary mismatch issues. Classic query rewriting techniques refine the original query iteratively by analyzing top-retrieved documents**Hofstätter and Lehmann, "BERT-BASED RETRIEVAL MODELS FOR QUESTION ANSWERING"**. These methods, however, largely depend on term frequency statistics and may not grasp the true query intent. LLMs, with their advanced linguistic capabilities, support the generation of query rewrites that more accurately reflect the complex and varied information needs of users**Radford et al., "Improving Language Understanding by Generative Models"**. HyDE**Lewis and Tyson, "HyDE: Harnessing Dense Retrievers for Efficient Passage Retrieval"** utilizes dense retrievers to generate pseudo-documents, while Query2doc**Guo et al., "Query2Doc: A Simple yet Effective Approach to Zero-Shot Cross-Media Retrieval"**, InPars**Chen et al., "InPars: A Hybrid Approach for Zero-Shot Cross-Media Retrieval"**, and Promptagator**Bai and Carpuano, "Zero-Shot Transfer Learning with Autoencoders for Natural Language Processing"** harness LLMs for producing synthetic queries through zero-shot or few-shot prompting.


\subsection{Prompt Engineer}
Prompt engineering is a critical technique for efficiently tailoring models to specific downstream tasks without fine-tuning**Bai and Carpuano, "Zero-Shot Transfer Learning with Autoencoders for Natural Language Processing"**. The chain-of-thought (CoT) prompting method was introduced to encourage LLMs to generate intermediate reasoning steps before reaching a final answer**Wei et al., "Chain of Thought Prompting Elucidates Mode Details in Large Language Models"**. In-context learning (ICL) leverages a few examples within the input to guide LLMs towards the intended task**Brown et al., "Language Models Play Harard and Fine-Tuning by Unsupervised Discrete-Variate Optimization"**. Expert prompting**Miao et al., "Prompting Expertise: A Framework for Few-Shot Learning with Human Demonstrations"** designs prompts that emulate an expert's reasoning, tailored to the input query's context. Multi-persona prompting**Chen et al., "Multi-Persona Conversational Dialogue Generation"** employs a range of `personas' to tackle specific tasks. In \ours, we engage LLMs with various roles outlined in a standard operating procedure (SOP) for retrieval, yielding empirically validated improvements in reranking.