% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{microtype}

\usepackage{inconsolata}

\usepackage{graphicx}

\title{\ours: A Multi-Role Collaborative Reranking Workflow Utilizing Large Language Models}

\author{
Can Jin\textsuperscript{\rm 1}\thanks{~~Equal contribution.}\thanks{~~Corresponding author.} \ Hongwu Peng\textsuperscript{\rm 2}$^*$ \ Anxiang Zhang \ Nuo Chen\textsuperscript{\rm 3} \ \textbf{Jiahui Zhao}\textsuperscript{\rm 2} \ \textbf{Xi Xie}\textsuperscript{\rm 2} \\ \textbf{Kuangzheng Li} \ \textbf{Shuya Feng}\textsuperscript{\rm 2} \ \textbf{Kai Zhong} \ \textbf{Caiwen Ding}\textsuperscript{\rm 2} \ \textbf{Dimitris N. Metaxas}\textsuperscript{\rm 1} \\
\textsuperscript{\rm 1}Rutgers University, USA \quad \textsuperscript{\rm 2}University of Connecticut, USA \\
\textsuperscript{\rm 3}Waseda University, Japan \\
\texttt{can.jin@rutgers.edu} \\ \texttt{\{hongwu.peng,jiahui.zhao,xi.xie,shuya.feng,caiwen.ding\}@uconn.edu} \\ \texttt{\{adamzhang1679, likuangzheng0633, kaizhong\}@gmail.com} \\ \texttt{pleviumtan@toki.waseda.jp} \quad
 \texttt{dnm@cs.rutgers.edu}
}

\usepackage{tabularx}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{url}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tcolorbox}
\usepackage{pifont}
% \usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{relsize}
\usepackage[inline]{enumitem}




\newcommand{\code}[1]{{\ttfamily#1}}
\newcommand{\ours}{\textsc{RankFlow}\,}



\definecolor{lightgreen}{rgb}{0.88, 1, 0.88}





\begin{document}
\maketitle
\begin{abstract}

In an Information Retrieval (IR) system, reranking plays a critical role by sorting candidate passages according to their relevance to a specific query. This process demands a nuanced understanding of the variations among passages linked to the query. In this work, we introduce \ours, a multi-role reranking workflow that leverages the capabilities of Large Language Models (LLMs) and role specializations to improve reranking performance. \ours enlists LLMs to fulfill four distinct roles: the query \textbf{Rewriter}, the pseudo \textbf{Answerer}, the passage \textbf{Summarizer}, and the \textbf{Reranker}. This orchestrated approach enables \ours to: (1) accurately interpret queries, (2) draw upon LLMs' extensive pre-existing knowledge, (3) distill passages into concise versions, and (4) assess passages in a comprehensive manner, resulting in notably better reranking results. Our experimental results reveal that \ours outperforms existing leading approaches on widely recognized IR benchmarks, such as TREC-DL, BEIR, and NovelEval. Additionally, we investigate the individual contributions of each role in \ours. Code is available at \url{https://github.com/jincan333/RankFlow}.

\end{abstract}

\section{Introduction}

The integration of large language models (LLMs)~\citep{brown2020language, touvron2023llama, lewis2020bart} into Information Retrieval (IR) systems has revolutionized user interactions with information and knowledge~\citep{hou2024large, fan2023recommender, xi2023towards}. LLMs enhance the IR process, including query rewriting and retrieval, through advanced linguistic understanding, semantic representation, context management, and encyclopedic knowledge~\citep{wang2023query2doc, sachan2022improving, qin2023large}.

The application of LLMs to zero-shot text ranking has seen increasing interest. Based on the type of instruction employed, Ranking strategies utilizing LLMs can be categorized into Pointwise~\citep{sachan2022improving, liang2022holistic}, Pairwise~\citep{qin2023large, sun2023instruction}, and Listwise methods~\citep{sun2023chatgpt, pradeep2023rankzephyr}. Among existing methods, listwise approaches achieve superior performance by enabling concurrent relevance judgment across a list of passages~\citep{sun2023chatgpt, pradeep2023rankzephyr}.

Although LLMs exhibit strong semantic understanding abilities, retrieval performance can still be compromised by queries that are short, ambiguous, or lack context~\citep{wang2023query2doc, nogueira2019document}. Moreover, the listwise reranking approach, which involves lengthy contexts, faces the challenge of decreased LLM instruction following and reasoning capabilities as context length increases~\citep{sun2023chatgpt, levy2024same, bai2023longbench}, potentially affecting reranking performance. Motivated by the notion that structured workflows enhance task execution consistency and accuracy~\citep{wooldridge1998pitfalls, belbin2022team, hong2023metagpt}, we introduce \ours—a multi-role workflow utilizing LLMs for reranking. As illustrated in Figure~\ref{figure_method}, \ours applies enriched queries with LLMs' prior knowledge and summarized information from passages to overcome existing challenges in LLM-based reranking systems.

\begin{figure*}
    \centering
    \includegraphics[width=0.98\linewidth]{figure/method.pdf}
    \caption{Overview of \ours. \ours is composed of four well-defined expert roles: \textbf{Rewriter}, \textbf{Answerer}, \textbf{Summarizer}, and \textbf{Reranker}, each designed to address specific issues in passage reranking. These roles work sequentially to handle the ranking task.
    }
    \label{figure_method}
\end{figure*}

We assess the performance of \ours across a broad range of datasets, including TREC-DL~\citep{craswell2020overview}, BEIR~\citep{thakur2021beir}, and NovelEval~\citep{sun2023chatgpt}. Our empirical findings consistently demonstrate \ours's superior performance. Notably, \ours outpaces current state-of-the-art (SoTA) methods, achieving higher scores than RankGPT~\citep{sun2023chatgpt} on four BEIR datasets—Covid, NFCorpus, SciFact, and Robust04—by an average of $2.5\%$ in nDCG@10, and surpassing RankZephyr~\citep{pradeep2023rankzephyr} on NovelEval by $5\%$ in nDCG@\{1, 5, 10\}.

In a nutshell, our contributions can be highlighted from four perspectives: 
\begin{enumerate}
    \item We introduce a unique multi-role reranking workflow, denoted as \ours, which is grounded in LLMs and employs well-defined role specializations. This workflow exhibits remarkable adaptability, allowing for the dynamic alteration of roles to enhance reranking efficacy.
    \item Our approach innovatively addresses the challenges of semantic ambiguity and context length constraints in listwise zero-shot reranking by incorporating query rewriting and passage summarization to augment clarity.
    \item We carry out extensive experiments on a variety of datasets, evidencing that \ours consistently surpasses SoTA methods~\citep{sun2023chatgpt, pradeep2023rankzephyr}.
    \item We thoroughly examine the impact of individual roles within \ours, providing valuable insights for further investigation.
\end{enumerate}



\section{Related Works}
\subsection{LLMs for Information Retrieval}
Text retrieval is a key component in a multitude of knowledge-driven Natural Language Processing (NLP) applications \citep{jin2024learning,jin2024apeer,wu2024cg}. In practice, this task is approached with a multi-stage ranking pipeline, typically consisting of an initial, cost-effective retriever followed by a more sophisticated reranker to refine the results~\citep{ma2023zero, craswell2020overview, nogueira2019multi}. Large language models (LLMs) have shown remarkable efficacy in information retrieval tasks~\citep{zhu2023large, sun2023chatgpt, pradeep2023rankzephyr}. Supervised reranking methods~\citep{nogueira2020document, zhuang2023rankt5, pradeep2023rankzephyr} have traditionally relied on fine-tuning transformer-based models with copious training data, such as the MS MARCO v1 passage ranking dataset~\citep{bajaj2016ms}. However, recent explorations involve LLMs in zero-shot unsupervised reranking. Pointwise approaches evaluate passage relevance individually~\citep{sachan2022improving, liang2022holistic}, whereas pairwise strategies compare two documents' relevancies for a given query~\citep{qin2023large, sun2023instruction}. Listwise methods, which directly reorder document relevance collectively, have achieved state-of-the-art performance~\citep{sun2023chatgpt, ma2023zero}. This study introduces a novel multi-role reranking framework, \ours, which significantly enhances listwise reranking performance.


\subsection{Query Rewriting}
Original queries in traditional IR systems are often short or ambiguous, leading to vocabulary mismatch issues. Classic query rewriting techniques refine the original query iteratively by analyzing top-retrieved documents~\citep{abdul2004umass, metzler2005markov, zhai2001model, metzler2007latent}. These methods, however, largely depend on term frequency statistics and may not grasp the true query intent. LLMs, with their advanced linguistic capabilities, support the generation of query rewrites that more accurately reflect the complex and varied information needs of users~\citep{mao2023large, gao2023precise, jagerman2023query, ma2023query}. HyDE~\citep{gao2023precise} utilizes dense retrievers to generate pseudo-documents, while Query2doc~\citep{wang2023query2doc} and InPars~\citep{bonifacio2022inpars}, along with Promptagator~\citep{dai2022promptagator}, harness LLMs for producing synthetic queries through zero-shot or few-shot prompting.


\subsection{Prompt Engineer}
Prompt engineering is a critical technique for efficiently tailoring models to specific downstream tasks without fine-tuning~\citep{liu2023pre, brown2020language, zhou2022conditional,jin2023visual,zhou2024adapi,zhang2023online,zhang2022stochastic,zhao2024a}. The chain-of-thought (CoT) prompting method was introduced to encourage LLMs to generate intermediate reasoning steps before reaching a final answer~\citep{kojima2022large, wei2022chain}. In-context learning (ICL) leverages a few examples within the input to guide LLMs towards the intended task~\citep{radford2019language, liu2022few}. Expert prompting~\citep{xu2023expertprompting} designs prompts that emulate an expert's reasoning, tailored to the input query's context. Multi-persona prompting~\citep{du2023improving} employs a range of `personas' to tackle specific tasks. In \ours, we engage LLMs with various roles outlined in a standard operating procedure (SOP) for retrieval, yielding empirically validated improvements in reranking.

\section{\ours}

\ours is a novel reranking framework for LLMs. Section~\ref{method_roles} presents the role specialization and overall procedure in~\ours. Sections~\ref{method_rewiter}, \ref{method_answerer}, \ref{method_summarizer}, and \ref{method_reranker} elucidate the detailed design for each role.

\subsection{Role Specialization and Overall Procedure}\label{method_roles}

Unambiguous role specialization facilitates the decomposition of intricate work into smaller, distinct tasks. In \ours, we divide the reranking task into four parts, assigning four agents with specific skills and expertise for each subtask. Defining the LLMs' roles and operational abilities establishes a workflow, allowing the LLMs to work sequentially.

As depicted in Figure~\ref{figure_method}, the Rewriter, adept at rephrasing sentences, reformulates the user query into a clearer and well-interpreted version. The rewritten query is then passed to the Answerer, which offers a standard answer. Subsequently, the rewritten query is concatenated with the generated answer, forming a new query containing more comprehensive information about the original query. During ranking, a passage Summarizer creates a concise summary for each candidate passage, effectively capturing the essential information more efficiently than the original passage. Finally, the Reranker receives the new query and the summarized candidate passages, outputting a relevance-based reranking.

\subsection{LLMs as Rewriters}\label{method_rewiter}
Original queries often exhibit brevity or ambiguity. For example, a query in TREC-DL19~\citep{craswell2020overview} is `what is wifi vs bluetooth', where the desired passage should emphasize the distinctions and similarities between `wifi' and `bluetooth'. Nevertheless, most pertinent documents in the search results utilize the term `comparative' instead of `vs'. Existing query rewriting techniques employ document corpora to supply domain-specific knowledge for subject areas~\citep{gao2023precise, ma2023query}. These approaches concentrate on examining query rewriting in the initial retrieval stage, while its application in the subsequent reranking stage remains underexplored. In~\ours, we concentrate on harnessing LLMs' language abilities for query rewriting in passage reranking tasks, employing role specialization without reliance on any corpora base.

Let $q$ be a query from the query distribution $\mathcal{Q}$. We specialize the LLM as an expert in refining user queries to enhance their suitability for ranking tasks. Next, we use a zero-shot prompt $c_{rew}$, devoid of any corpus or specific domain knowledge, for query rewriting to obtain a clearer and well-interpreted query $q_{rew}$.
\begin{equation}
    q_{rew} = f(q; c_{rew})
\end{equation}
where $f$ represents the LLM.

The specialization and prompting for Rewriter can be found in Appendix~\ref{appendix_prompt_rewriter}. Generally, the rewritten query $q_{rew}$ conveys more precise information and avoids ambiguous expressions. Examples of the original query $q$ and the rewritten query $q_{rew}$ are displayed in Table~\ref{table_appexdix_examples_query}.


\subsection{LLMs as Answerers}\label{method_answerer}
Query expansion enhances retrieval systems by enriching query representation with additional terms, expressing identical concepts or information needs, and improving lexical or semantic alignment with corpus documents~\citep{datta2008image, huang2009analyzing}. Early research on query expansion focused on using lexical knowledge bases~\citep{robertson1995okapi} or Pseudo-Relevance Feedback (PRF)~\citep{borgeaud2022improving}. \citet{wang2023query2doc} propose expanding the original query during the previous sparse or dense retrieval stage. Our Answerer adopts a similar approach to \citet{wang2023query2doc}, emphasizing reranking performance enhancement without any knowledge base or feedback, necessitating more refined relevance judgment.

We specialize the LLM as an assistant adept at providing detailed and pertinent responses to user queries. Through carefully crafted prompts, the Answerer generates a pseudo-passage $P_{gen}$ that addresses the query, serving as a `standard answer' for the given query $q_{rew}$:
\begin{equation}
    P_{gen} = f (q_{rew}; c_{gen})
\end{equation}
where $c_{gen}$ represents the zero-shot prompt for `standard answer' generation.

Owing to the LLMs' proficient language ability and world knowledge, the generated passage offers abundant information about the given query. Table~\ref{table_appexdix_examples_query} showcases some examples for the answer. The complete specialization for our Answerer is illustrated in Appendix~\ref{appendix_prompt_answerer}.

We then define a new query $q_{new}$ as follows:
\begin{equation}
    q_{new} = \mathrm{Concat}(q_{rew} * m, P_{gen})
\end{equation}
where $*$ denotes string repetition, $m$ represents the number of repetitions, and $\mathrm{Concat}$ is the string concatenation operator. Our experiments demonstrate that repeating the query an appropriate number of times strengthens the query's `weights', leading to improved passage reranking performance.


\subsection{LLMs as Summarizers}\label{method_summarizer}
Candidate passages $\mathcal{P} = \{P_1, P_2, ..., P_n\}$ from the preceding retrieval stage are often lengthy, and their contained information may be vague, ambiguous, or redundant, complicating concise relevance judgments. To tackle this issue, we suggest summarizing candidate passages before utilizing them for reranking. These summaries effectively preserve essential information without redundancy and are typically much shorter than the original passages, facilitating improved relevance judgment.

To generate enhanced summaries of the original passages, we devise a Summarizer skilled in condensing passages for better information retrieval. For each candidate passage $P\in\mathcal{P}$, we obtain a summarized passage $\hat{P}$ as follows:
\begin{equation}
    \hat{P} = f(P; c_{sum})
\end{equation}
where $c_{sum}$ represents the zero-shot prompt for passage summarization. The complete prompt can be found in Appendix~\ref{appendix_prompt_summarizer}.

Following the summarization process, we acquire a list of summarized candidate passages $\hat{\mathcal{P}}=\{\hat{P}_1, \hat{P}_2, ..., \hat{P}_n\}$. Table~\ref{table_appexdix_examples_passage} offers some examples of summarized passages, which are generally shorter and clearer than their original counterparts.


\subsection{LLMs as Reranker}\label{method_reranker}
\citet{sun2023chatgpt} propose listwise permutation generation to directly output a ranked list given a set of candidate passages. However, the listwise approach necessitates a considerable number of tokens, potentially degrading instruction following and reasoning abilities~\citep{levy2024same, bai2023longbench, sun2023instruction} and negatively affecting ranking performance. In~\ours, we mitigate this limitation by employing summarized passages for listwise reranking, which are shorter and clearer than the original ones. To enable more precise relevance judgments and further enhance ranking performance through increased reasoning, we adopt a distinct prompting strategy from~\citet{sun2023chatgpt}, encompassing three aspects:

\begin{itemize}
    \item \textbf{Relevance Standard.} To facilitate more accurate relevance judgments, we instruct the LLM to adhere to a detailed relevance standard, as proposed by~\citet{craswell2020overview}. For instance, a passage is deemed perfectly relevant if it is dedicated to the query and contains the exact answer, whereas a passage is considered irrelevant if it bears no relation to the query.
    \item \textbf{CoT.} CoT prompting can elicit the reasoning ability of LLMs~\citep{wei2022chain}. Prior studies have applied CoT in query rewriting~\citep{jagerman2023query, alaofi2023can}, while the impact of CoT in passage reranking remains underexplored. In~\ours, we prompt the LLM to rank passages thoughtfully and systematically, enabling more reasoning in the relevance judgment process.
    \item \textbf{Format Requirement.} After employing the relevance standard and CoT, the LLM's output format becomes more diverse (e.g., containing rationales). To minimize malformed outputs, we instruct the LLM to conform to the ranking format and ensure that no passages are omitted or repeated in the ranking results.
\end{itemize}

In~\ours reranking, we specialize the LLM as an adept intelligent assistant for ranking passages based on query relevance. Our carefully designed prompt strategy enables the Reranker to generate superior ranking results. The specification and prompt for our Reranker are illustrated in Appendix~\ref{appendix_prompt_reranker}.

Assuming there are $n$ summarized candidate passages $\hat{\mathcal{P}}=\{\hat{P}_1, \hat{P}_2, ..., \hat{P}_n\}$ from the Summarizer, we rerank these passages in a back-to-first order from $\hat{P_n}$ to $\hat{P}_1$ using a sliding window of length $w$. Initially, we attain a rank list $l_1$ by prompting the LLM to rank the passages $[\hat{P}_{n-w+1}, \dots, \hat{P}_{n-1}, \hat{P}_n]$ according to their relevance to $q_{new}$:
\begin{equation}
    l_1 = f(q_{new}, [\hat{P}_{n-w+1}, ..., \hat{P}_{n-1}, \hat{P}_n]; c_{list})
    \label{equation_reranker}
\end{equation}
where $c_{list}$ represents our zero-shot listwise prompt, employing the relevance standard, CoT, and format requirement.

Subsequently, we reorder the passage order based on the rank list $l_1$ to obtain a ranked passage set, still denoted as $\hat{\mathcal{P}}=\{\hat{P}_1, \hat{P}_2, ..., \hat{P}_n\}$ for simplicity. We then slide the window in steps of length $s$ and rank the passages $[\hat{P}_{n-s-w+1}, ..., \hat{P}_{n-s-1}, \hat{P}_{n-s}]$ following equation~\ref{equation_reranker}, resulting in a rank list $l_2$. We reorder the passage order in $\hat{\mathcal{P}}$ based on $l_2$. This procedure repeats until all passages are ranked.



\begin{table*}[!t]
\centering
\resizebox{1\textwidth}{!}{
\begin{tabular}{l c | ccc | ccc }
\toprule
 \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{LLM}} & \multicolumn{3}{c}{\textbf{TREC-DL19}} & \multicolumn{3}{c}{\textbf{TREC-DL20}} \\
  & & nDCG@1 & nDCG@5 & nDCG@10 & nDCG@1 & nDCG@5 & nDCG@10 \\
\midrule
BM25 & - & 54.26 & 52.78 & 50.58 & 57.72 & 50.67 & 47.96 \\ 
\midrule
\textbf{Supervised} \\ 
\midrule
monoT5 & T5 (3B) & 79.04 & 73.74 & 71.83 & 80.25 & 72.32 & 68.89 \\ 
RankT5 & T5 (3B) & 77.38 & 73.94 & 71.22 & 80.86 & 72.99 & 69.49 \\ 
RankZephyr & Zephyr (7B) & - & - & 74.20 & - & - & 70.86 \\ 
\midrule
\textbf{Unsupervised} \\ 
\midrule
RG & FLAN-UL2 (20B) & 70.93 & 66.81 & 64.61 & 75.62 & 66.85 & 65.39 \\ 
PRP & FLAN-UL2 (20B) & 78.29 & 75.49 & 72.65 & \textbf{85.80} & 75.35 & 70.46 \\ 
RankGPT-4 & GPT-4 & 80.62 & 77.83 & 74.89 & 79.73 & 73.15 & 70.14   \\ 
\midrule
\ours & GPT-4 & \textbf{83.33} & \textbf{79.44} & \textbf{76.65}   & 82.41 & \textbf{75.68} & \textbf{71.80}  \\ 
\bottomrule
\end{tabular}}
\caption{\textbf{Results (nDCG@$\{$1,5,10$\}$) on TREC.} The performance of eight reranking methods on TREC-DL19 and TREC-DL20. All the unsupervised methods use zero-shot prompts. The best performances are marked in bold.}
\label{table_trec}
\end{table*}

\begin{table*}[htbp]
  \centering
    \resizebox{1\textwidth}{!}{
    \begin{tabular}{l | cccccccc | c }
      \toprule
       \textbf{Method} & \textbf{COVID} & \textbf{NFCorpus} & \textbf{SciFact} & \textbf{Robust04} & \textbf{Touche} & \textbf{DBpedia} & \textbf{Signal} & \textbf{News} & \textbf{Avg} \\
      \midrule
      BM25 & 59.47 & 30.75 & 67.89 & 40.70 & 44.22 & 31.80 & 67.89 & 39.52 & 43.42 \\ 
      \midrule
      monoT5 & 80.71 & 38.97 & 76.57 & 56.71 & 32.41 & 44.45 & 32.55 & 48.49 & 51.36 \\ 
      RankT5 & 80.71 & 38.10 & 74.99 & - & 44.01 & 44.22 & 32.00 & - & - \\ 
      RankZephyr & 83.78 & - & - & - & - & - & - & 51.84 & - \\
      \midrule
      RankGPT-4 & 83.98 & 38.83 & 75.61 & 59.74 & 40.72 & 47.12 & 33.90 & 52.82 & 54.09  \\ 
      \midrule
      \textbf{\ours} & \textbf{85.77} & \textbf{39.74} & \textbf{77.73} & \textbf{64.88} & \textbf{42.08} & \textbf{47.43} & \textbf{34.54} & \textbf{52.97} & \textbf{55.64}  \\ 
      \bottomrule
    \end{tabular}}
    \caption{\textbf{Results (nDCG@10) on BEIR.} The performance of six reranking methods on eight BEIR datasets. RankT5 reranks the top 1000 passages returned by BM25 while other methods rerank top 100 passages retrieved by BM25.}
    \label{table_beir}
\end{table*}

\section{Experiments}
To assess the efficacy of~\ours, we perform extensive experiments to: (1) showcase the superior performance of~\ours across various benchmarks; (2) examine the influence of each component in our approach, namely, Rewriter, Answerer, Summarizer, and Reranker; and (3) evaluate the impact of distinct prompts in the listwise Reranker.

\subsection{Models and Benchmarks}\label{models_and_benchmarks}
We choose GPT-4~\citep{achiam2023gpt} as our primary model, employing the Azure API, which features a GPT-4-0613 version. Our experiments are evaluated on three benchmark datasets, including TREC-DL~\citep{craswell2020overview}, BEIR~\citep{thakur2021beir}, and NovelEval~\citep{sun2023chatgpt}. 

\textbf{TREC} is a widely adopted benchmark dataset in IR research. We use the test sets from the 2019 and 2020 competitions, which employed the MS MARCO v1 passage corpus: (i) TREC-DL19 comprises 43 queries, and (ii) TREC-DL20 consists of 54 queries.

\textbf{BEIR} encompasses diverse retrieval tasks and domains. We select the test sets of eight tasks in BEIR to evaluate our approach: \begin{enumerate*}[label=(\roman*)]
\item \emph{Covid}: Retrieves scientific articles related to COVID-19.
\item \emph{NFCorpus}: A biomedical information retrieval dataset.
\item \emph{SciFact}: Retrieves evidence for claims verification.
\item \emph{Robust04}: Assesses challenging topics.
\item \emph{Touche}: An argument retrieval dataset.
\item \emph{DBPedia}: Retrieves entities from the DBpedia corpus.
\item \emph{Signal}: Retrieves relevant tweets for a given news title.
\item \emph{News}: Retrieves relevant news articles for headlines.
\end{enumerate*}


\textbf{NovelEval} features queries not learned by GPT-4-0613~\citep{sun2023chatgpt}. Questions in current benchmarks (e.g., TREC-DL) are typically collected years ago, raising concerns that existing LLMs may already possess knowledge of these questions~\citep{yu2023generate}. Moreover, since many LLMs do not disclose information about their training data, there is a potential risk of contamination in existing benchmark test sets~\citep{achiam2023gpt}. To mitigate these concerns, we evaluate~\ours on NovelEval-2306.



\subsection{Baselines and Evaluation Metrics}\label{baselines_and_metrics}
\paragraph{Baselines.} We select several representative SoTA passage reranking methods as our baselines: (1) \textit{BM25}~\citep{lin2021pyserini} serves as a fundamental sanity check in reranking, directly using the rank results after the previous retrieval stage; (2) \textit{monoT5}~\citep{nogueira2020document} is a sequence-to-sequence reranker employing T5 (3B) to compute the relevance score with pointwise ranking loss, trained on MS MARCO; (3) \textit{RankT5}~\citep{zhuang2023rankt5} is a reranker utilizing T5 (3B) and listwise ranking loss, trained on MS MARCO; (4) \textit{RG}~\citep{liang2022holistic} is a pointwise reranking approach based on relevance generation using FLAN-UL2 (20B); (5) \textit{PRP}~\citep{qin2023large} is a pairwise reranking approach employing a sliding window strategy with 10 passes, using the FLAN-UL2 (20B) model; (6) \textit{RankZephyr}~\citep{pradeep2023rankzephyr} is a recent reranker leveraging the 7B parameter Zephyr$_{\beta}$ (built on Mistral), distilled from GPT-3.5 and GPT-4 on MS MARCO; and (7) \textit{rankGPT-4}~\citep{sun2023chatgpt} is our most crucial baseline, adopting a listwise reranking strategy with GPT-4.


\paragraph{Implementation and Metrics.} 
All baselines and~\ours rerank the top $100$ passages retrieved by BM25 using pyserini~\citep{lin2021pyserini} unless specified otherwise. We employ normalized Discounted Cumulative Gain (nDCG) at rank cutoffs of $\{1,5,10\}$ (nDCG@$\{1,5,10\}$) to evaluate performance. For~\ours and rankGPT-4, we utilize the Azure API with a context size setting of $8192$, employing the GPT-4 version GPT-4-0613, which differs from the one used in~\citet{sun2023chatgpt}. Additionally, rankGPT-4 employs GPT-4 to rerank the top $30$ passages reranked by GPT-3.5 (which reranks the top $100$ passages by BM25) on BEIR. These differences result in discrepancies between the rankGPT-4 outcomes in our paper and those in~\citet{sun2023chatgpt}. In our Answerer, we set a repeat time $m$ of $3$. In our Reranker, we use a window size $w$ of $20$ and a step size $s$ of $10$, following~\citet{sun2023chatgpt}. We set the temperature to $0$ for the GPT-4 API to reduce randomness. All our results are averaged over $3$ runs. Finally, cost of our experiments are indicated in Appendix \ref{appendix_cost}.


\begin{table}[t]
\centering
  \resizebox{0.5\textwidth}{!}{
    \begin{tabular}{l | ccc }
      \toprule
      \textbf{Method} & nDCG@1 & nDCG@5 & nDCG@10 \\
      \midrule
      BM25 & 33.33 & 45.96 & 55.77 \\ 
      \midrule
      monoT5 & 83.33 & 78.38 & 84.62 \\
      RankZephyr & 92.86 & 86.15 & 89.34 \\
      \midrule
      RankGPT-4 & 92.86 & 86.10 & 89.18  \\ 
      \midrule
      \textbf{\ours} & \textbf{97.62} & \textbf{91.79} & \textbf{94.21} \\ 
      \bottomrule
    \end{tabular}}
    \caption{\textbf{Results (nDCG@$\{$1,5,10$\}$) on NovelEval.} The performance of five reranking methods on NovelEval-$2306$.}
    \label{table_noveleval}
    \vspace{-2mm}
  \end{table}


\begin{table*}[!t]
\centering
\footnotesize % Smaller font for better fit
\setlength\tabcolsep{10pt} % Slightly increase column separation
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{l | ccc }
\toprule
\textbf{Method} & \textbf{TREC-DL20} & \textbf{COVID} & \textbf{NovelEval} \\
\midrule
RankGPT-4 & 79.73/73.15/70.14 & 88.25/86.67/83.98 & 92.86/86.10/89.18  \\ 
\midrule
RankGPT-4 + Rewriter & 80.56/74.06/70.10 & 91.33/87.06/84.50 & 91.27/86.77/91.50  \\ 
\midrule
\ours & \textbf{82.41}/\textbf{75.68}/\textbf{71.80} & \textbf{96.00}/\textbf{89.48}/\textbf{85.77} & \textbf{97.62}/\textbf{91.79}/\textbf{94.21} \\ 
\bottomrule
\end{tabular}}
\caption{Results (nDCG@$\{$1,5,10$\}$) of utilizing the Rewriter.}
\label{table_rq}
\end{table*}


\begin{table*}[!t]
\centering
\footnotesize % Smaller font for better fit
\setlength\tabcolsep{10pt}
\resizebox{1\textwidth}{!}{
\begin{tabular}{l | ccc }
\toprule
\textbf{Method} & \textbf{TREC-DL19} & \textbf{COVID} & \textbf{NovelEval} \\
\midrule
RankGPT-4 & 80.62/77.83/74.89 & 88.25/86.67/83.98 & 92.86/86.10/89.18  \\ 
\midrule
RankGPT-4 + Answerer, $m=1$ & 81.79/77.98/74.21 & 87.67/86.68/84.00 & 97.62/90.51/92.38  \\ 
RankGPT-4 + Answerer, $m=3$ & 82.17/78.41/74.70 & 89.67/88.40/85.00 & 97.62/91.63/93.85  \\ 
RankGPT-4 + Answerer, $m=10$ & 80.82/78.09/74.63 & 89.00/87.87/84.14 & 97.62/91.49/93.83  \\ 
\midrule
\ours & \textbf{83.33}/\textbf{79.44}/\textbf{76.65} & \textbf{96.00}/\textbf{89.48}/\textbf{85.77} & \textbf{97.62}/\textbf{91.79}/\textbf{94.21} \\ 
\bottomrule
\end{tabular}}
\caption{Results (nDCG@$\{$1,5,10$\}$) of utilizing the Answerer.}
\label{table_pg}
\end{table*}


\subsection{Main Results}
\paragraph{Results on TREC.}
To demonstrate the superior performance of \ours on TREC datasets, we compare it with seven baselines. The nDCG@$\{1,5,10\}$ results are presented in Table~\ref{table_trec}. We can draw the following positive observations: \ding{182} \ours exhibits superior performance compared to RankGPT-4 on TREC datasets, surpassing RankGPT-4 by an average nDCG@$\{1,5,10\}$ of $\{2.70, 2.07, 1.71\}$ on TREC-DL19 and TREC-DL20. \ding{183} \ours achieves the best performance among both supervised and unsupervised methods in terms of nDCG@5 and nDCG@10. It not only surpasses all supervised methods but also outperforms SoTA zero-shot unsupervised methods, including the pointwise method RG, the pairwise method PRP, and the listwise method RankGPT-4. This further indicates the effectiveness of our reranking framework.


\paragraph{Resutls on BEIR.}
We further evaluate the performance of~\ours on eight BEIR datasets, which contain more queries and heterogeneous topics than TREC-DL19 and TREC-DL20. The results are displayed in Table~\ref{table_beir}, from which we can observe that: \ding{182}~\ours outperforms the baselines on BEIR, achieving the best nDCG@10 across all baselines and datasets. \ding{183}~\ours is robust to diverse topics and queries. It surpasses RankGPT-4 by an average of $\mathbf{1.55}$ on wight BEIR datasets, which contain more queries than the TREC datasets. Notably,~\ours achieves a $\mathbf{5.14}$ nDCG@10 improvement over RankGPT-4 on Robust04, which consists of 249 queries and diverse topics in news articles.


\paragraph{Results on NovelEval.}
To address the concern of data contamination in LLM reranking, we further evaluate the performance of five reranking methods on NovelEval-2306. The results are shown in Table~\ref{table_noveleval}, from which we can observe that: \ding{182}~\ours maintains significant reranking performance on unlearned datasets, achieving the best performance among all reranking methods. \ding{183}~\ours exhibits a substantial performance enhancement compared to RankGPT-4 and RankZephyr, with an improvement of $\mathbf{5}$ points in terms of nDCG@1, nDCG@5, and nDCG@10.


\begin{table*}[ht]
\centering
\footnotesize % Smaller font for better fit
\setlength\tabcolsep{10pt}
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{l | ccc }
\toprule
\textbf{Method} & \textbf{TREC-DL20} & \textbf{COVID} & \textbf{Robust04} \\
\midrule
RankGPT-4 & 79.73/73.15/70.14 & 88.25/86.67/83.98 & 75.30/66.07/59.74  \\ 
\midrule
RankGPT-4 + Summarizer & 81.07/74.46/71.34 & 92.00/87.06/85.26 & 78.58/70.44/63.69  \\ 
\midrule
% RankGPT-4 + Paraphraser & 80.25/73.49/69.69 & 89.00/86.22/84.32 & 76.51/67.97/61.14  \\ 
% \midrule
% RankGPT-4 + Extractor & 80.17/74.13/70.35 & 92.00/87.19/85.05 & 77.31/67.89/61.96  \\ 
% \midrule
\ours & \textbf{82.41}/\textbf{75.68}/\textbf{71.80} & \textbf{96.00}/\textbf{89.48}/\textbf{85.77} & \textbf{80.92}/\textbf{70.99}/\textbf{64.88} \\ 
\bottomrule
\end{tabular}}
\caption{Results (nDCG@$\{$1,5,10$\}$) of utilizing the Summarizer.}
\label{table_sum}
\vspace{-2mm}
\end{table*}

\subsection{Ablation Study}
\vspace{-1mm}
\paragraph{Rewriter.}
First, we investigate the effect of the Rewriter in our multi-role reranking workflow. We incorporate the Rewriter into RankGPT-4, which rewrites the original queries in the benchmarks and utilizes the rewritten queries for listwise passage reranking. As shown in Table~\ref{table_rq}, the Rewriter demonstrates a capability to slightly enhance ranking performance by using well-interpreted queries. On TREC-DL20, COVID, and NovelEval, it achieves improvements in the majority of instances and yields an average nDCG@$\{1,5,10\}$ improvement of $\{0.76, 0.66, 0.93\}$.

\vspace{-1mm}
\paragraph{Answerer.}
We further investigate the effects of our Answerer in reranking by integrating it with RankGPT-4. For an original query $q$ from the benchmarks, we generate a passage $P$ answering the query using the Answerer. We then form a new query by repeating $q$ for $m$ times and concatenating the repeated queries with $P$, which is utilized in listwise reranking. The empirical results in Table~\ref{table_pg} yield several positive observations: \ding{182} Our Answerer can enhance the overall ranking performance. For $m \in \{1, 3, 10\}$, incorporating the Answerer leads to performance gains in most cases on TREC-DL19, COVID, and NovelEval. \ding{183} A moderate value of $m$ in the Answerer results in the best performance gains. As shown in Table~\ref{table_pg}, $m=3$ consistently outperforms $m=1$ and $m=10$. \ding{184} The Answerer is capable of generating valuable feedback, even on datasets unlearned by GPT-4. On NovelEval, it achieves performance comparable to \ours. These observations demonstrate that the answers generated by the Answerer contain rich information that can improve semantic-level matching and relevance judgment in reranking.

\vspace{-2mm}
\paragraph{Summarizer.}
Next, we evaluate the effects of Summarizer. We integrate the Summarizer into RankGPT-4, which uses the original query from benchmarks but replaces the candidate passages retrieved by BM25 with the summarized passages generated by the Summarizer. The results in Table~\ref{table_sum} reveal that: \ding{182} The Summarizer is capable of enhancing reranking performance, achieving consistent performance gains in terms of nDCG@$\{1,5,10\}$ on TREC-DL20, COVID, and Robust04. \ding{183} The Summarizer particularly improves the performance of nDCG@10, achieving an average nDCG@10 improvement of $2.14$ on three datasets and comparable nDCG@{10} with \ours. Notably, it attains an nDCG@10 improvement of $\mathbf{3.95}$ on Robust04. The findings indicate that substituting vague, ambiguous, and redundant passages with concise, clear, and well-structured ones improves reranking performance.

\vspace{-1mm}
\paragraph{Reranker.} 
We conduct additional experiments to investigate the effect of different prompt designs in the Reranker. As indicated in Section~\ref{method_reranker}, our prompting strategy comprises three aspects: Relevance Standard, CoT, and Format Requirement. We add each type of prompt to RankGPT-4 separately, resulting in `RankGPT-4 w. Relevance Standard', `RankGPT-4 w. CoT', and `RankGPT-4 w. Format Requirement'. Detailed information on the prompts can be found in Appendix~\ref{appendix_prompt_stratiges}. The experimental results are presented in Table~\ref{table_prompt}. 

\textit{Relevance Standard.} We prompt RankGPT-4 to follow a detailed four-level relevance standard: Perfectly relevant, Highly Relevant, Relevant, and Irrelevant, as described by \citet{craswell2020overview}. The results in Table~\ref{table_prompt} show that this prompting strategy yields consistent performance gains across all three datasets, indicating that detailed relevance standards can enhance relevance judgments.

\textit{CoT.} We utilize zero-shot CoT prompts in RankGPT-4, instructing it to think thoughtfully and systematically during ranking. We observe that CoT leads to significant performance gains by enabling more reasoning in relevance judgments. `RankGPT-4 w. CoT' achieves consistent performance improvements, with an average gain of $\{1.99, 2.15, 1.97\}$ in terms of nDCG@$\{1,5,10\}$ compared to RankGPT-4 on the three datasets.

\textit{Format Requirement.} We incorporate format requirement prompts to instruct RankGPT-4 to adhere to the specific rank format and ensure no repeated or missing passages in the rank list, facilitating a more convenient extraction of the final rank results. As shown in Table~\ref{table_prompt}, the format requirement instructions marginally improve reranking performance in most cases, yielding an average gain of $\{1.26, 0.69, 1.05\}$ in terms of nDCG@$\{1,5,10\}$ on TREC-DL20, COVID, and NovelEval.


\section{Conclusion}
\vspace{-1mm}
In this paper, we introduce \ours, a novel multi-role collaborative reranking workflow for Large Language Models (LLMs), comprising four roles: Rewriter, Answerer, Summarizer, and Reranker. Our extensive empirical results showcase the significant effectiveness of \ours, which consistently surpasses state-of-the-art methods on various datasets. Furthermore, we investigate the individual contributions of each role and the impacts of prompt designs in listwise passage reranking, providing valuable insights for future research. The robust performance of \ours highlights the potential of an effective reranking workflow in the era of LLMs.


\section{Lmitations}
A limitation of this paper is that the \ours framework is comprehensively investigated only on the GPT4 model, leaving other closed-source and open-source models less explored. Future work should include investigating the effect of the \ours framework on models such as Claude 3, recently released by Anthropic.

\section{Ethics Statement}
In our research, we strictly adhere to the ACM Code of Ethics by ensuring that all datasets and models used are publicly accessible. However, we recognize the potential risks and harms associated with LLMs, such as the generation of harmful, offensive, or biased content. Furthermore, LLMs are known to generate incorrect information, a phenomenon commonly referred to as hallucinations. It is important to note that the models investigated in this study are not exempt from these limitations. Furthermore, the GPT4 used in this work are licensed under the terms of OpenAI.


\bibliography{custom}

\onecolumn
\appendix
\section{Cost Analysis}\label{appendix_cost}
We compared the average token cost, running time, and USD cost per query of \ours to RankGPT. In \ours, we can preserve the new queries and summarized passages locally and reuse them in later reranking, helping to save running time and financial budget, which we name “\ours w. Local Passage”. The results are shown in Table \ref{table_cost_comparison}. While \ours costs 10.63\% more tokens and USD than RankGPT, it is 7.32\% faster and achieves an average nDCG@10 improvement of nearly 2\% on TREC, BEIR, and NovelEval as shown in Tables \ref{table_trec}, Table \ref{table_beir}, and Table \ref{table_noveleval}. Moreover, using “\ours w. Local Passages” can further reduce computational cost and financial budget, saving 26.83\% tokens and USD and being 32.26\% faster than RankGPT.


\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Dataset} & \textbf{Tokens} & \textbf{Time (s)} & \textbf{USD} \\
\midrule
RankGPT-4 & 19890 & 82 & 0.596 \\
\ours & 21938 & 76 & 0.658 \\
\ours w. Local Passages & 12027 & 60 & 0.361 \\
\bottomrule
\end{tabular}
\caption{Average token cost, running time, and USD cost per query on TREC-DL.}
\label{table_cost_comparison}
\end{table}


\section{Additional Results}
\paragraph{Additional Results of Reranker.} The effects of different prompt designs in the \ours Reranker on three datasets are shown in Table \ref{table_prompt}. Each component enhances performance, while \ours consistently achieves the best results.


\begin{table*}[ht]
\centering
\footnotesize % Smaller font for better fit
\setlength\tabcolsep{10pt}
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{l | ccc }
\toprule
\textbf{Method} & \textbf{TREC-DL20} & \textbf{COVID} & \textbf{NovelEval} \\
\midrule
RankGPT-4 & 79.73/73.15/70.14 & 88.25/86.67/83.98 & 92.86/86.10/89.18  \\ 
\midrule
RankGPT-4 w. Relevance Standard & 80.45/73.94/70.62 & 91.67/88.04/85.38 & 92.89/89.57/92.49  \\ 
RankGPT-4 w. CoT & 80.25/74.13/70.42 & 93.33/88.51/85.51 & 95.24/89.74/93.28  \\ 
RankGPT-4 w. Format Requirement & 79.63/73.68/70.38 & 91.33/87.11/84.70 & 93.65/87.19/91.24  \\ 
\midrule
\ours & \textbf{82.41}/\textbf{75.68}/\textbf{71.80} & \textbf{96.00}/\textbf{89.48}/\textbf{85.77} & \textbf{97.62}/\textbf{91.79}/\textbf{94.21} \\ 
\bottomrule
\end{tabular}}
\caption{Results (nDCG@$\{$1,5,10$\}$) of utilizing different prompting strategies in 
the Reranker.}
\label{table_prompt}
\end{table*}



\section{Specilization and Prompt for Rewriter}\label{appendix_prompt_rewriter}
\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,boxsep=2pt,left=2pt,right=2pt,top=2pt,bottom=2pt]
\textbf{system} \\
You are an AI retrieval assistant, skilled at rewriting user queries to enhance their suitability for retrieval tasks and optimizing compatibility with retrieval systems like BM25. \\

\textbf{user} \\
Rewrite the following user query into a clear, specific, and formal request suitable for retrieving relevant information from a list of passages. Keep in mind that your rewritten query will be sent to rerank system, which does relevance search for retrieving documents. \\

\textbf{assistant} \\
Kindly provide the query you would like me to rewrite. \\

\textbf{user} \\
\{query\}
\end{tcolorbox}


\section{Specilization and Prompt for Answer}\label{appendix_prompt_answerer}
\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,boxsep=2pt,left=2pt,right=2pt,top=2pt,bottom=2pt]
\textbf{system} \\
You are an AI retrieval expert, skilled at providing detailed and relevant answers to user queries. \\

\textbf{user} \\
Compose a passage to address the following user query effectively. \\

\textbf{assistant} \\
Please provide the query for which you would like an answer. \\

\textbf{user} \\
\{query\}
\end{tcolorbox}


\section{Specilization and Prompt for Summarizer}\label{appendix_prompt_summarizer}
\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,boxsep=2pt,left=2pt,right=2pt,top=2pt,bottom=2pt]
\textbf{system} \\
You are an AI assistant who is good at summarizing passages the user provides you. \\

\textbf{user} \\
I will provide you a passage. Summarize the passage to make it suit for a passage retrieval task which means the summarized passages can better reflect the information and the relevance to a giving query than the original passage. 

Passage: \{passage\}
\end{tcolorbox}

\section{Specilization and Prompt for Reranker}\label{appendix_prompt_reranker}
\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,boxsep=2pt,left=2pt,right=2pt,top=2pt,bottom=2pt]
\textbf{system} \\
You are RankGPT, an intelligent assistant that ranks passages based on their relevance to a given query. Apply the following relevance criteria when ranking passages: \\
1. Perfectly relevant: The passage directly addresses the query and contains the exact answer. \\
2. Highly relevant: The passage contains information related to the query, but the answer may be unclear or surrounded by unrelated details. \\
3. Related: The passage is related to the query but does not provide an answer. \\
4. Irrelevant: The passage is not connected to the query. \\

\textbf{user} \\
Please rank the \{num\} passages I will provide, each identified by a number in brackets []. Evaluate the passages based on their relevance to the following query: \{query\}. List the passages in descending order of relevance, with the most relevant passages at the top. Use [rankstart] to begin the ranking and [rankend] to conclude it. Ensure that no passages are missed or repeated in the ranking. The output format should be:

[rankstart] [] $>$ [] [rankend],

For example,

[rankstart] [1] $>$ [2] [rankend]. Follow the ranking format diligently and avoid missing or repeating passages. Approach the task systematically and thoughtfully. \\


\textbf{assistant} \\
Understood, I will adhere to the ranking format. Please provide the passages for evaluation and ranking. \\

\textbf{user}

[1] \{Passage1\} \\

\textbf{assistant}

Received passage [1] \\

\textbf{user}

[2] \{Passage2\} \\

\textbf{assistant}

Received passage [2] \\

(more passages) ... \\

\textbf{user} \\
Search Query:
\{query\}. \\
Rank the \{num\} passages above based on their relevance to the search query.

\end{tcolorbox}


\section{Different prompt strategies for the Reranker}\label{appendix_prompt_stratiges}
\subsection{Relevance Standard}\label{appendix_prompt_relevance_standard}
\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,boxsep=2pt,left=2pt,right=2pt,top=2pt,bottom=2pt]
\textbf{system} \\
You are RankGPT, an intelligent assistant that can rank passages based on their relevancy to the query. Your relevance judgment should follow the following standard: \\
Perfectly relevant: The passage is dedicated to the query and contains the exact answer. \\
Highly relevant: The passage has some answer for the query, but the answer may be a bit unclear, or hidden amongst extraneous information. \\
Related: The passage seems related to the query but does not answer it. \\
Irrelevant: The passage has nothing to do with the query. \\
\textbf{user} \\
I will provide you with \{num\} passages, each indicated by number identifier []. \\
Rank the passages based on their relevance to query: \{query\}. \\
\textbf{assistant} \\
Okay, please provide the passages. \\
\textbf{user}
[1] \{Passage1\} \\
\textbf{assistant}
Received passage [1] \\
(more passages) ... \\
\textbf{user} \\
Search Query: \{query\}. \\
Rank the \{num\} passages above based on their relevance to the search query. The passages should be listed in descending order using identifiers. The most relevant passages should be listed first. The output format should be [] > [], e.g., [1] > [2]. Only response the ranking results, do not say any word or explain.
\end{tcolorbox}

\subsection{CoT}\label{appendix_prompt_cot}
\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,boxsep=2pt,left=2pt,right=2pt,top=2pt,bottom=2pt]
\textbf{system} \\
You are RankGPT, an intelligent assistant that can rank passages based on their relevancy to the query. \\
\textbf{user} \\
I will provide you with \{num\} passages, each indicated by number identifier []. \\
Rank the passages based on their relevance to query: \{query\}. \\
\textbf{assistant} \\
Okay, please provide the passages. \\
\textbf{user}
[1] \{Passage1\} \\
\textbf{assistant}
Received passage [1] \\
(more passages) ... \\
\textbf{user} \\
Search Query: {query}. \\
Please evaluate the \{num\} passages provided and rank them based on their relevance to the search query. Arrange the passages in descending order of relevance by using their identifiers. Place the most relevant passage at the beginning of the list. Start your ranking with [rankstart] and end with [rankend]. The output format should be [rankstart] [] $>$ [] [rankend], \\
For example,

[rankstart] [1] $>$ [2] [rankend]. Proceed thoughtfully and systematically.
\end{tcolorbox}

\subsection{Format Requirement}\label{appendix_prompt_format_requirement}
\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,boxsep=2pt,left=2pt,right=2pt,top=2pt,bottom=2pt]
\textbf{system} \\
You are RankGPT, an intelligent assistant that can rank passages based on their relevancy to the query. \\
\textbf{user} \\
I will provide you with \{num\} passages, each indicated by number identifier []. \\
Rank the passages based on their relevance to query: \{query\}. \\
\textbf{assistant} \\
Okay, please provide the passages. \\
\textbf{user}
[1] \{Passage1\} \\
\textbf{assistant}
Received passage [1] \\
(more passages) ... \\
\textbf{user} \\
Evaluate and rank the passages based on their relevance to the query: \\
\{query\}. \\
List the passages in descending order of relevance using their identifiers, placing the most relevant passages at the top. Begin the ranking with [rankstart] and end with [rankend]. Ensure that no passages are missed or repeated in the ranking. The output format should be: 

[rankstart] [] $>$ [] [ranked], 

For example, 

[rankstart] [1] $>$ [2] [ranked]. 

Adhere to the ranking format and avoid missing or repeating passages while ranking.
\end{tcolorbox}

\section{Examples}\label{appendix_examples}
\subsection{Query}\label{appendix_examples_query}
The qualitative examples of \ours Rewriter and Answerer are shown in Table \ref{table_appexdix_examples_query}.

\begin{table*}[ht]
% \vspace{-3mm}
\centering
\scalebox{0.9}{\begin{tabular}{p{0.18\linewidth} | p{0.95\linewidth}}
\hline
original query & what is wifi vs bluetooth \\ 
\hline
rewritten query & Provide a comparative analysis of Wi-Fi and Bluetooth, detailing their differences and similarities.  \\
\hline
answer & Wi-Fi and Bluetooth are both wireless communication technologies but they are designed for different purposes and have distinct characteristics. Wi-Fi, short for Wireless Fidelity, is primarily used to provide high-speed internet access. It allows devices like computers, smartphones, or smart TVs to connect to the internet or each other wirelessly within a particular area, known as a Wi-Fi network. Bluetooth, on the other hand, is used for connecting devices to each other over short distances, typically up to 30 feet. It's commonly used for connecting peripherals like keyboards, mice, or headphones to a computer or for connecting a hands-free headset to a mobile phone. In summary, while both Wi-Fi and Bluetooth are wireless technologies that allow devices to communicate without physical connections, they differ in their range, power consumption, data transfer speed, and typical use cases.  \\
\hline
new query & Provide a comparative analysis of Wi-Fi and Bluetooth, detailing their differences and similarities.

Provide a comparative analysis of Wi-Fi and Bluetooth, detailing their differences and similarities.

Provide a comparative analysis of Wi-Fi and Bluetooth, detailing their differences and similarities.

Wi-Fi and Bluetooth are both wireless communication technologies but they are designed for different purposes and have distinct characteristics. Wi-Fi, short for Wireless Fidelity, is primarily used to provide high-speed internet access. It allows devices like computers, smartphones, or smart TVs to connect to the internet or each other wirelessly within a particular area, known as a Wi-Fi network. Bluetooth, on the other hand, is used for connecting devices to each other over short distances, typically up to 30 feet. It's commonly used for connecting peripherals like keyboards, mice, or headphones to a computer or for connecting a hands-free headset to a mobile phone. In summary, while both Wi-Fi and Bluetooth are wireless technologies that allow devices to communicate without physical connections, they differ in their range, power consumption, data transfer speed, and typical use cases. 
\\
\hline
\hline
original query & ms symptoms ms \\ 
\hline
rewritten query & Provide detailed information on the symptoms associated with Multiple Sclerosis (MS).  \\
\hline
answer & Multiple Sclerosis (MS) is a chronic disease that affects the central nervous system, particularly the brain and spinal cord. Symptoms of MS can vary widely and may change over time, but some of the most common include: 1. Fatigue: This is one of the most common symptoms of MS, affecting about 80\% of people. It can significantly interfere with a person's ability to function at work or at home. 2. Difficulty Walking: MS can affect coordination and balance, making walking difficult. This is often due to numbness in the legs, muscle weakness, or problems with balance and coordination. 3. Numbness or Tingling: MS often starts with the sensation of numbness or tingling in various parts of the body, most commonly in the face, body, or extremities. It's important to note that these symptoms can also be caused by conditions other than MS, and experiencing one or more of these symptoms does not necessarily mean you have MS. \\
\hline
new query & Provide detailed information on the symptoms associated with Multiple Sclerosis (MS).

Provide detailed information on the symptoms associated with Multiple Sclerosis (MS).

Provide detailed information on the symptoms associated with Multiple Sclerosis (MS).

Multiple Sclerosis (MS) is a chronic disease that affects the central nervous system, particularly the brain and spinal cord. Symptoms of MS can vary widely and may change over time, but some of the most common include: 1. Fatigue: This is one of the most common symptoms of MS, affecting about 80\% of people. It can significantly interfere with a person's ability to function at work or at home. 2. Difficulty Walking: MS can affect coordination and balance, making walking difficult. This is often due to numbness in the legs, muscle weakness, or problems with balance and coordination. 3. Numbness or Tingling: MS often starts with the sensation of numbness or tingling in various parts of the body, most commonly in the face, body, or extremities. It's important to note that these symptoms can also be caused by conditions other than MS, and experiencing one or more of these symptoms does not necessarily mean you have MS.
\\
\hline
\end{tabular}}
\caption{Examples of the original query, rewritten query, answer, and new query.}
\label{table_appexdix_examples_query}
\end{table*}



\subsection{Passage}\label{appendix_examples_passage}
The qualitative examples of \ours Summarizer are shown in Table \ref{table_appexdix_examples_passage}.


\begin{table*}[ht]
\centering
\scalebox{0.85}{\begin{tabular}{p{0.25\linewidth} | p{0.9\linewidth}}
\hline
original passage & 5. Cancel. A flea can live up to a year, but its general lifespan depends on its living conditions, such as the availability of hosts. Find out how long a flea's life cycle can last with tips from a pet industry specialist in this free video on fleas and pest control.Part of the Video Series: Flea Control.ancel. A flea can live up to a year, but its general lifespan depends on its living conditions, such as the availability of hosts. Find out how long a flea's life cycle can last with tips from a pet industry specialist in this free video on fleas and pest control. Part of the Video Series: Flea Control. \\ 
\hline
summarized passage & The lifespan of a flea, which can be up to a year, is influenced by its living conditions, particularly the availability of hosts. This information, along with tips on flea control, is provided by a pet industry specialist in a free video series.  \\
\hline
\hline
original passage & Surprise! The federal government's Digital Analytics Program (DAP) has found that Windows is the most popular end-user operating system with 58.4 percent of all government web site visitors. What's more interesting though is what versions of Windows we're using and what's number two and three behind Windows. The government's data, derived from 1.39 billion web visits to more than 37 government agencies over the last 90 days, reveals that the second place operating system is Apple iOS with 16.4 percent. It's followed close by Android with 13.9 percent. \\ 
\hline
summarized passage & The Digital Analytics Program (DAP) of the federal government reveals that Windows is the most used operating system by government website visitors, accounting for 58.4\% of all users. The data, collected from 1.39 billion web visits across 37 government agencies in the past 90 days, also shows that Apple iOS and Android are the second and third most popular operating systems, with 16.4\% and 13.9\% usage respectively.  \\
\hline
\hline
original passage & Pitch In For The Big Three: You know the 80/20 rule, right? Basically, 80 percent of your wedding's costs will come from 20 percent of the decisions. So the trick is to help your fiancee with the right 20 percent. That means pitching in on the Big Three: the guest list, the date and the venue.For these, you need to do more than just say, â$<$80$><$9c$>$Sure, honey, that sounds fine..itch In For The Big Three: You know the 80/20 rule, right? Basically, 80 percent of your wedding's costs will come from 20 percent of the decisions. So the trick is to help your fiancee with the right 20 percent. That means pitching in on the Big Three: the guest list, the date and the venue. \\ 
\hline
summarized passage & The passage emphasizes the importance of the 80/20 rule in wedding planning, where 80\% of the costs are determined by 20\% of the decisions. It advises the reader to assist their fiancee with these crucial decisions, specifically focusing on the "Big Three": the guest list, the date, and the venue.  \\
\hline
\hline
original passage & 1 Pelvic inflammatory disease many not produce any symptoms. 2 In other cases it can cause fever, abdominal or pelvic pain, vaginal discharge, painful urination, or painful sexual intercourse.3 Pelvic inflammatory disease is treated with antibiotics. Pelvic inflammatory disease many not produce any symptoms. 2 In other cases it can cause fever, abdominal or pelvic pain, vaginal discharge, painful urination, or painful sexual intercourse. \\ 
\hline
summarized passage & Pelvic inflammatory disease (PID) may be asymptomatic or cause symptoms like fever, abdominal or pelvic pain, vaginal discharge, painful urination, and discomfort during sexual intercourse. The treatment for PID typically involves antibiotics.  \\
\hline
\end{tabular}}
\caption{Examples of the original passage and summarized passage.}
\label{table_appexdix_examples_passage}
\end{table*}



\end{document}
