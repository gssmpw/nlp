\section{Related Works}
\subsection{LLMs for Information Retrieval}
Text retrieval is a key component in a multitude of knowledge-driven Natural Language Processing (NLP) applications \citep{jin2024learning,jin2024apeer,wu2024cg}. In practice, this task is approached with a multi-stage ranking pipeline, typically consisting of an initial, cost-effective retriever followed by a more sophisticated reranker to refine the results~\citep{ma2023zero, craswell2020overview, nogueira2019multi}. Large language models (LLMs) have shown remarkable efficacy in information retrieval tasks~\citep{zhu2023large, sun2023chatgpt, pradeep2023rankzephyr}. Supervised reranking methods~\citep{nogueira2020document, zhuang2023rankt5, pradeep2023rankzephyr} have traditionally relied on fine-tuning transformer-based models with copious training data, such as the MS MARCO v1 passage ranking dataset~\citep{bajaj2016ms}. However, recent explorations involve LLMs in zero-shot unsupervised reranking. Pointwise approaches evaluate passage relevance individually~\citep{sachan2022improving, liang2022holistic}, whereas pairwise strategies compare two documents' relevancies for a given query~\citep{qin2023large, sun2023instruction}. Listwise methods, which directly reorder document relevance collectively, have achieved state-of-the-art performance~\citep{sun2023chatgpt, ma2023zero}. This study introduces a novel multi-role reranking framework, \ours, which significantly enhances listwise reranking performance.


\subsection{Query Rewriting}
Original queries in traditional IR systems are often short or ambiguous, leading to vocabulary mismatch issues. Classic query rewriting techniques refine the original query iteratively by analyzing top-retrieved documents~\citep{abdul2004umass, metzler2005markov, zhai2001model, metzler2007latent}. These methods, however, largely depend on term frequency statistics and may not grasp the true query intent. LLMs, with their advanced linguistic capabilities, support the generation of query rewrites that more accurately reflect the complex and varied information needs of users~\citep{mao2023large, gao2023precise, jagerman2023query, ma2023query}. HyDE~\citep{gao2023precise} utilizes dense retrievers to generate pseudo-documents, while Query2doc~\citep{wang2023query2doc} and InPars~\citep{bonifacio2022inpars}, along with Promptagator~\citep{dai2022promptagator}, harness LLMs for producing synthetic queries through zero-shot or few-shot prompting.


\subsection{Prompt Engineer}
Prompt engineering is a critical technique for efficiently tailoring models to specific downstream tasks without fine-tuning~\citep{liu2023pre, brown2020language, zhou2022conditional,jin2023visual,zhou2024adapi,zhang2023online,zhang2022stochastic,zhao2024a}. The chain-of-thought (CoT) prompting method was introduced to encourage LLMs to generate intermediate reasoning steps before reaching a final answer~\citep{kojima2022large, wei2022chain}. In-context learning (ICL) leverages a few examples within the input to guide LLMs towards the intended task~\citep{radford2019language, liu2022few}. Expert prompting~\citep{xu2023expertprompting} designs prompts that emulate an expert's reasoning, tailored to the input query's context. Multi-persona prompting~\citep{du2023improving} employs a range of `personas' to tackle specific tasks. In \ours, we engage LLMs with various roles outlined in a standard operating procedure (SOP) for retrieval, yielding empirically validated improvements in reranking.