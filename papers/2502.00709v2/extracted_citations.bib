@article{abdul2004umass,
  title={UMass at TREC 2004: Novelty and HARD},
  author={Abdul-Jaleel, Nasreen and Allan, James and Croft, W Bruce and Diaz, Fernando and Larkey, Leah and Li, Xiaoyan and Smucker, Mark D and Wade, Courtney},
  journal={Computer Science Department Faculty Publication Series},
  pages={189},
  year={2004}
}

@article{bajaj2016ms,
  title={Ms marco: A human generated machine reading comprehension dataset},
  author={Bajaj, Payal and Campos, Daniel and Craswell, Nick and Deng, Li and Gao, Jianfeng and Liu, Xiaodong and Majumder, Rangan and McNamara, Andrew and Mitra, Bhaskar and Nguyen, Tri and others},
  journal={arXiv preprint arXiv:1611.09268},
  year={2016}
}

@article{bonifacio2022inpars,
  title={Inpars: Data augmentation for information retrieval using large language models},
  author={Bonifacio, Luiz and Abonizio, Hugo and Fadaee, Marzieh and Nogueira, Rodrigo},
  journal={arXiv preprint arXiv:2202.05144},
  year={2022}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{craswell2020overview,
  title={Overview of the TREC 2019 deep learning track},
  author={Craswell, Nick and Mitra, Bhaskar and Yilmaz, Emine and Campos, Daniel and Voorhees, Ellen M},
  journal={arXiv preprint arXiv:2003.07820},
  year={2020}
}

@inproceedings{dai2022promptagator,
  title={Promptagator: Few-shot Dense Retrieval From 8 Examples},
  author={Dai, Zhuyun and Zhao, Vincent Y and Ma, Ji and Luan, Yi and Ni, Jianmo and Lu, Jing and Bakalov, Anton and Guu, Kelvin and Hall, Keith and Chang, Ming-Wei},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{du2023improving,
  title={Improving factuality and reasoning in language models through multiagent debate},
  author={Du, Yilun and Li, Shuang and Torralba, Antonio and Tenenbaum, Joshua B and Mordatch, Igor},
  journal={arXiv preprint arXiv:2305.14325},
  year={2023}
}

@inproceedings{gao2023precise,
  title={Precise Zero-Shot Dense Retrieval without Relevance Labels},
  author={Gao, Luyu and Ma, Xueguang and Lin, Jimmy and Callan, Jamie},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1762--1777},
  year={2023}
}

@article{jagerman2023query,
  title={Query expansion by prompting large language models},
  author={Jagerman, Rolf and Zhuang, Honglei and Qin, Zhen and Wang, Xuanhui and Bendersky, Michael},
  journal={arXiv preprint arXiv:2305.03653},
  year={2023}
}

@misc{jin2023visual,
      title={Visual Prompting Upgrades Neural Network Sparsification: A Data-Model Perspective}, 
      author={Can Jin and Tianjin Huang and Yihua Zhang and Mykola Pechenizkiy and Sijia Liu and Shiwei Liu and Tianlong Chen},
      year={2023},
      eprint={2312.01397},
      archivePrefix={arXiv},
      primaryClass={id='cs.CV' full_name='Computer Vision and Pattern Recognition' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers image processing, computer vision, pattern recognition, and scene understanding. Roughly includes material in ACM Subject Classes I.2.10, I.4, and I.5.'}
}

@article{jin2024apeer,
  title={APEER: Automatic Prompt Engineering Enhances Large Language Model Reranking},
  author={Jin, Can and Peng, Hongwu and Zhao, Shiyu and Wang, Zhenting and Xu, Wujiang and Han, Ligong and Zhao, Jiahui and Zhong, Kai and Rajasekaran, Sanguthevar and Metaxas, Dimitris N},
  journal={arXiv preprint arXiv:2406.14449},
  year={2024}
}

@misc{jin2024learning,
      title={Learning from Teaching Regularization: Generalizable Correlations Should be Easy to Imitate}, 
      author={Can Jin and Tong Che and Hongwu Peng and Yiyuan Li and Marco Pavone},
      year={2024},
      eprint={2402.02769},
      archivePrefix={arXiv},
      primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@article{liang2022holistic,
  title={Holistic evaluation of language models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  journal={arXiv preprint arXiv:2211.09110},
  year={2022}
}

@article{liu2022few,
  title={Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning},
  author={Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin A},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={1950--1965},
  year={2022}
}

@article{liu2023pre,
  title={Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
  author={Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal={ACM Computing Surveys},
  volume={55},
  number={9},
  pages={1--35},
  year={2023},
  publisher={ACM New York, NY}
}

@inproceedings{ma2023query,
  title={Query Rewriting in Retrieval-Augmented Large Language Models},
  author={Ma, Xinbei and Gong, Yeyun and He, Pengcheng and Zhao, Hai and Duan, Nan},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={5303--5315},
  year={2023}
}

@article{ma2023zero,
  title={Zero-shot listwise document reranking with a large language model},
  author={Ma, Xueguang and Zhang, Xinyu and Pradeep, Ronak and Lin, Jimmy},
  journal={arXiv preprint arXiv:2305.02156},
  year={2023}
}

@inproceedings{mao2023large,
  title={Large Language Models Know Your Contextual Search Intent: A Prompting Framework for Conversational Search},
  author={Mao, Kelong and Dou, Zhicheng and Mo, Fengran and Hou, Jiewen and Chen, Haonan and Qian, Hongjin},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={1211--1225},
  year={2023}
}

@inproceedings{metzler2005markov,
  title={A markov random field model for term dependencies},
  author={Metzler, Donald and Croft, W Bruce},
  booktitle={Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval},
  pages={472--479},
  year={2005}
}

@inproceedings{metzler2007latent,
  title={Latent concept expansion using markov random fields},
  author={Metzler, Donald and Croft, W Bruce},
  booktitle={Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval},
  pages={311--318},
  year={2007}
}

@article{nogueira2019multi,
  title={Multi-stage document ranking with BERT},
  author={Nogueira, Rodrigo and Yang, Wei and Cho, Kyunghyun and Lin, Jimmy},
  journal={arXiv preprint arXiv:1910.14424},
  year={2019}
}

@inproceedings{nogueira2020document,
  title={Document Ranking with a Pretrained Sequence-to-Sequence Model},
  author={Nogueira, Rodrigo and Jiang, Zhiying and Pradeep, Ronak and Lin, Jimmy},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={708--718},
  year={2020}
}

@article{pradeep2023rankzephyr,
  title={RankZephyr: Effective and Robust Zero-Shot Listwise Reranking is a Breeze!},
  author={Pradeep, Ronak and Sharifymoghaddam, Sahel and Lin, Jimmy},
  journal={arXiv preprint arXiv:2312.02724},
  year={2023}
}

@article{qin2023large,
  title={Large language models are effective text rankers with pairwise ranking prompting},
  author={Qin, Zhen and Jagerman, Rolf and Hui, Kai and Zhuang, Honglei and Wu, Junru and Shen, Jiaming and Liu, Tianqi and Liu, Jialu and Metzler, Donald and Wang, Xuanhui and others},
  journal={arXiv preprint arXiv:2306.17563},
  year={2023}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{sachan2022improving,
  title={Improving Passage Retrieval with Zero-Shot Question Generation},
  author={Sachan, Devendra and Lewis, Mike and Joshi, Mandar and Aghajanyan, Armen and Yih, Wen-tau and Pineau, Joelle and Zettlemoyer, Luke},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={3781--3797},
  year={2022}
}

@inproceedings{sun2023chatgpt,
  title={Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents},
  author={Sun, Weiwei and Yan, Lingyong and Ma, Xinyu and Wang, Shuaiqiang and Ren, Pengjie and Chen, Zhumin and Yin, Dawei and Ren, Zhaochun},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={14918--14937},
  year={2023}
}

@article{sun2023instruction,
  title={Instruction distillation makes large language models efficient zero-shot rankers},
  author={Sun, Weiwei and Chen, Zheng and Ma, Xinyu and Yan, Lingyong and Wang, Shuaiqiang and Ren, Pengjie and Chen, Zhumin and Yin, Dawei and Ren, Zhaochun},
  journal={arXiv preprint arXiv:2311.01555},
  year={2023}
}

@inproceedings{wang2023query2doc,
  title={Query2doc: Query Expansion with Large Language Models},
  author={Wang, Liang and Yang, Nan and Wei, Furu},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={9414--9423},
  year={2023}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{wu2024cg,
  title={CG-FedLLM: How to Compress Gradients in Federated Fune-tuning for Large Language Models},
  author={Wu, Huiwen and Li, Xiaohan and Zhang, Deyi and Xu, Xiaogang and Wu, Jiafei and Zhao, Puning and Liu, Zhe},
  journal={arXiv preprint arXiv:2405.13746},
  year={2024}
}

@article{xu2023expertprompting,
  title={Expertprompting: Instructing large language models to be distinguished experts},
  author={Xu, Benfeng and Yang, An and Lin, Junyang and Wang, Quan and Zhou, Chang and Zhang, Yongdong and Mao, Zhendong},
  journal={arXiv preprint arXiv:2305.14688},
  year={2023}
}

@inproceedings{zhai2001model,
  title={Model-based feedback in the language modeling approach to information retrieval},
  author={Zhai, Chengxiang and Lafferty, John},
  booktitle={Proceedings of the tenth international conference on Information and knowledge management},
  pages={403--410},
  year={2001}
}

@inproceedings{zhang2022stochastic,
  title={Stochastic continuous submodular maximization: Boosting via non-oblivious function},
  author={Zhang, Qixin and Deng, Zengde and Chen, Zaiyi and Hu, Haoyuan and Yang, Yu},
  booktitle={International Conference on Machine Learning},
  pages={26116--26134},
  year={2022},
  organization={PMLR}
}

@inproceedings{zhang2023online,
  title={Online learning for non-monotone DR-submodular maximization: From full information to bandit feedback},
  author={Zhang, Qixin and Deng, Zengde and Chen, Zaiyi and Zhou, Kuangqi and Hu, Haoyuan and Yang, Yu},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3515--3537},
  year={2023},
  organization={PMLR}
}

@inproceedings{zhou2022conditional,
  title={Conditional prompt learning for vision-language models},
  author={Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={16816--16825},
  year={2022}
}

@article{zhou2024adapi,
  title={AdaPI: Facilitating DNN Model Adaptivity for Efficient Private Inference in Edge Computing},
  author={Zhou, Tong and Zhao, Jiahui and Luo, Yukui and Xie, Xi and Wen, Wujie and Ding, Caiwen and Xu, Xiaolin},
  journal={CoRR},
  year={2024}
}

@article{zhu2023large,
  title={Large language models for information retrieval: A survey},
  author={Zhu, Yutao and Yuan, Huaying and Wang, Shuting and Liu, Jiongnan and Liu, Wenhan and Deng, Chenlong and Dou, Zhicheng and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2308.07107},
  year={2023}
}

@inproceedings{zhuang2023rankt5,
  title={Rankt5: Fine-tuning t5 for text ranking with ranking losses},
  author={Zhuang, Honglei and Qin, Zhen and Jagerman, Rolf and Hui, Kai and Ma, Ji and Lu, Jing and Ni, Jianmo and Wang, Xuanhui and Bendersky, Michael},
  booktitle={Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={2308--2313},
  year={2023}
}

