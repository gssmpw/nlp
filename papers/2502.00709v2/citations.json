[
  {
    "index": 0,
    "papers": [
      {
        "key": "jin2024learning",
        "author": "Can Jin and Tong Che and Hongwu Peng and Yiyuan Li and Marco Pavone",
        "title": "Learning from Teaching Regularization: Generalizable Correlations Should be Easy to Imitate"
      },
      {
        "key": "jin2024apeer",
        "author": "Jin, Can and Peng, Hongwu and Zhao, Shiyu and Wang, Zhenting and Xu, Wujiang and Han, Ligong and Zhao, Jiahui and Zhong, Kai and Rajasekaran, Sanguthevar and Metaxas, Dimitris N",
        "title": "APEER: Automatic Prompt Engineering Enhances Large Language Model Reranking"
      },
      {
        "key": "wu2024cg",
        "author": "Wu, Huiwen and Li, Xiaohan and Zhang, Deyi and Xu, Xiaogang and Wu, Jiafei and Zhao, Puning and Liu, Zhe",
        "title": "CG-FedLLM: How to Compress Gradients in Federated Fune-tuning for Large Language Models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "ma2023zero",
        "author": "Ma, Xueguang and Zhang, Xinyu and Pradeep, Ronak and Lin, Jimmy",
        "title": "Zero-shot listwise document reranking with a large language model"
      },
      {
        "key": "craswell2020overview",
        "author": "Craswell, Nick and Mitra, Bhaskar and Yilmaz, Emine and Campos, Daniel and Voorhees, Ellen M",
        "title": "Overview of the TREC 2019 deep learning track"
      },
      {
        "key": "nogueira2019multi",
        "author": "Nogueira, Rodrigo and Yang, Wei and Cho, Kyunghyun and Lin, Jimmy",
        "title": "Multi-stage document ranking with BERT"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "zhu2023large",
        "author": "Zhu, Yutao and Yuan, Huaying and Wang, Shuting and Liu, Jiongnan and Liu, Wenhan and Deng, Chenlong and Dou, Zhicheng and Wen, Ji-Rong",
        "title": "Large language models for information retrieval: A survey"
      },
      {
        "key": "sun2023chatgpt",
        "author": "Sun, Weiwei and Yan, Lingyong and Ma, Xinyu and Wang, Shuaiqiang and Ren, Pengjie and Chen, Zhumin and Yin, Dawei and Ren, Zhaochun",
        "title": "Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents"
      },
      {
        "key": "pradeep2023rankzephyr",
        "author": "Pradeep, Ronak and Sharifymoghaddam, Sahel and Lin, Jimmy",
        "title": "RankZephyr: Effective and Robust Zero-Shot Listwise Reranking is a Breeze!"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "nogueira2020document",
        "author": "Nogueira, Rodrigo and Jiang, Zhiying and Pradeep, Ronak and Lin, Jimmy",
        "title": "Document Ranking with a Pretrained Sequence-to-Sequence Model"
      },
      {
        "key": "zhuang2023rankt5",
        "author": "Zhuang, Honglei and Qin, Zhen and Jagerman, Rolf and Hui, Kai and Ma, Ji and Lu, Jing and Ni, Jianmo and Wang, Xuanhui and Bendersky, Michael",
        "title": "Rankt5: Fine-tuning t5 for text ranking with ranking losses"
      },
      {
        "key": "pradeep2023rankzephyr",
        "author": "Pradeep, Ronak and Sharifymoghaddam, Sahel and Lin, Jimmy",
        "title": "RankZephyr: Effective and Robust Zero-Shot Listwise Reranking is a Breeze!"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "bajaj2016ms",
        "author": "Bajaj, Payal and Campos, Daniel and Craswell, Nick and Deng, Li and Gao, Jianfeng and Liu, Xiaodong and Majumder, Rangan and McNamara, Andrew and Mitra, Bhaskar and Nguyen, Tri and others",
        "title": "Ms marco: A human generated machine reading comprehension dataset"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "sachan2022improving",
        "author": "Sachan, Devendra and Lewis, Mike and Joshi, Mandar and Aghajanyan, Armen and Yih, Wen-tau and Pineau, Joelle and Zettlemoyer, Luke",
        "title": "Improving Passage Retrieval with Zero-Shot Question Generation"
      },
      {
        "key": "liang2022holistic",
        "author": "Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others",
        "title": "Holistic evaluation of language models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "qin2023large",
        "author": "Qin, Zhen and Jagerman, Rolf and Hui, Kai and Zhuang, Honglei and Wu, Junru and Shen, Jiaming and Liu, Tianqi and Liu, Jialu and Metzler, Donald and Wang, Xuanhui and others",
        "title": "Large language models are effective text rankers with pairwise ranking prompting"
      },
      {
        "key": "sun2023instruction",
        "author": "Sun, Weiwei and Chen, Zheng and Ma, Xinyu and Yan, Lingyong and Wang, Shuaiqiang and Ren, Pengjie and Chen, Zhumin and Yin, Dawei and Ren, Zhaochun",
        "title": "Instruction distillation makes large language models efficient zero-shot rankers"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "sun2023chatgpt",
        "author": "Sun, Weiwei and Yan, Lingyong and Ma, Xinyu and Wang, Shuaiqiang and Ren, Pengjie and Chen, Zhumin and Yin, Dawei and Ren, Zhaochun",
        "title": "Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents"
      },
      {
        "key": "ma2023zero",
        "author": "Ma, Xueguang and Zhang, Xinyu and Pradeep, Ronak and Lin, Jimmy",
        "title": "Zero-shot listwise document reranking with a large language model"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "abdul2004umass",
        "author": "Abdul-Jaleel, Nasreen and Allan, James and Croft, W Bruce and Diaz, Fernando and Larkey, Leah and Li, Xiaoyan and Smucker, Mark D and Wade, Courtney",
        "title": "UMass at TREC 2004: Novelty and HARD"
      },
      {
        "key": "metzler2005markov",
        "author": "Metzler, Donald and Croft, W Bruce",
        "title": "A markov random field model for term dependencies"
      },
      {
        "key": "zhai2001model",
        "author": "Zhai, Chengxiang and Lafferty, John",
        "title": "Model-based feedback in the language modeling approach to information retrieval"
      },
      {
        "key": "metzler2007latent",
        "author": "Metzler, Donald and Croft, W Bruce",
        "title": "Latent concept expansion using markov random fields"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "mao2023large",
        "author": "Mao, Kelong and Dou, Zhicheng and Mo, Fengran and Hou, Jiewen and Chen, Haonan and Qian, Hongjin",
        "title": "Large Language Models Know Your Contextual Search Intent: A Prompting Framework for Conversational Search"
      },
      {
        "key": "gao2023precise",
        "author": "Gao, Luyu and Ma, Xueguang and Lin, Jimmy and Callan, Jamie",
        "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels"
      },
      {
        "key": "jagerman2023query",
        "author": "Jagerman, Rolf and Zhuang, Honglei and Qin, Zhen and Wang, Xuanhui and Bendersky, Michael",
        "title": "Query expansion by prompting large language models"
      },
      {
        "key": "ma2023query",
        "author": "Ma, Xinbei and Gong, Yeyun and He, Pengcheng and Zhao, Hai and Duan, Nan",
        "title": "Query Rewriting in Retrieval-Augmented Large Language Models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "gao2023precise",
        "author": "Gao, Luyu and Ma, Xueguang and Lin, Jimmy and Callan, Jamie",
        "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "wang2023query2doc",
        "author": "Wang, Liang and Yang, Nan and Wei, Furu",
        "title": "Query2doc: Query Expansion with Large Language Models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "bonifacio2022inpars",
        "author": "Bonifacio, Luiz and Abonizio, Hugo and Fadaee, Marzieh and Nogueira, Rodrigo",
        "title": "Inpars: Data augmentation for information retrieval using large language models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "dai2022promptagator",
        "author": "Dai, Zhuyun and Zhao, Vincent Y and Ma, Ji and Luan, Yi and Ni, Jianmo and Lu, Jing and Bakalov, Anton and Guu, Kelvin and Hall, Keith and Chang, Ming-Wei",
        "title": "Promptagator: Few-shot Dense Retrieval From 8 Examples"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "liu2023pre",
        "author": "Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham",
        "title": "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing"
      },
      {
        "key": "brown2020language",
        "author": "Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others",
        "title": "Language models are few-shot learners"
      },
      {
        "key": "zhou2022conditional",
        "author": "Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei",
        "title": "Conditional prompt learning for vision-language models"
      },
      {
        "key": "jin2023visual",
        "author": "Can Jin and Tianjin Huang and Yihua Zhang and Mykola Pechenizkiy and Sijia Liu and Shiwei Liu and Tianlong Chen",
        "title": "Visual Prompting Upgrades Neural Network Sparsification: A Data-Model Perspective"
      },
      {
        "key": "zhou2024adapi",
        "author": "Zhou, Tong and Zhao, Jiahui and Luo, Yukui and Xie, Xi and Wen, Wujie and Ding, Caiwen and Xu, Xiaolin",
        "title": "AdaPI: Facilitating DNN Model Adaptivity for Efficient Private Inference in Edge Computing"
      },
      {
        "key": "zhang2023online",
        "author": "Zhang, Qixin and Deng, Zengde and Chen, Zaiyi and Zhou, Kuangqi and Hu, Haoyuan and Yang, Yu",
        "title": "Online learning for non-monotone DR-submodular maximization: From full information to bandit feedback"
      },
      {
        "key": "zhang2022stochastic",
        "author": "Zhang, Qixin and Deng, Zengde and Chen, Zaiyi and Hu, Haoyuan and Yang, Yu",
        "title": "Stochastic continuous submodular maximization: Boosting via non-oblivious function"
      },
      {
        "key": "zhao2024a",
        "author": "Puning Zhao and Lifeng Lai and Li Shen and Qingming Li and Jiafei Wu and Zhe Liu",
        "title": "A Huber Loss Minimization Approach to Mean Estimation under User-level Differential Privacy"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "kojima2022large",
        "author": "Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke",
        "title": "Large language models are zero-shot reasoners"
      },
      {
        "key": "wei2022chain",
        "author": "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others",
        "title": "Chain-of-thought prompting elicits reasoning in large language models"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "radford2019language",
        "author": "Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others",
        "title": "Language models are unsupervised multitask learners"
      },
      {
        "key": "liu2022few",
        "author": "Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin A",
        "title": "Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "xu2023expertprompting",
        "author": "Xu, Benfeng and Yang, An and Lin, Junyang and Wang, Quan and Zhou, Chang and Zhang, Yongdong and Mao, Zhendong",
        "title": "Expertprompting: Instructing large language models to be distinguished experts"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "du2023improving",
        "author": "Du, Yilun and Li, Shuang and Torralba, Antonio and Tenenbaum, Joshua B and Mordatch, Igor",
        "title": "Improving factuality and reasoning in language models through multiagent debate"
      }
    ]
  }
]