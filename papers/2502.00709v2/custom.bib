@article{qwen2,
  title={Qwen2 Technical Report},
  year={2024}
}

@article{llama3modelcard,
  title={Llama 3 Model Card},
  author={AI@Meta},
  year={2024},
  url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}
}

@article{jagerman2023query,
  title={Query expansion by prompting large language models},
  author={Jagerman, Rolf and Zhuang, Honglei and Qin, Zhen and Wang, Xuanhui and Bendersky, Michael},
  journal={arXiv preprint arXiv:2305.03653},
  year={2023}
}

@inproceedings{sun2023chatgpt,
  title={Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents},
  author={Sun, Weiwei and Yan, Lingyong and Ma, Xinyu and Wang, Shuaiqiang and Ren, Pengjie and Chen, Zhumin and Yin, Dawei and Ren, Zhaochun},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={14918--14937},
  year={2023}
}

@article{craswell2020overview,
  title={Overview of the TREC 2019 deep learning track},
  author={Craswell, Nick and Mitra, Bhaskar and Yilmaz, Emine and Campos, Daniel and Voorhees, Ellen M},
  journal={arXiv preprint arXiv:2003.07820},
  year={2020}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{ma2023zero,
  title={Zero-shot listwise document reranking with a large language model},
  author={Ma, Xueguang and Zhang, Xinyu and Pradeep, Ronak and Lin, Jimmy},
  journal={arXiv preprint arXiv:2305.02156},
  year={2023}
}

@article{nogueira2019multi,
  title={Multi-stage document ranking with BERT},
  author={Nogueira, Rodrigo and Yang, Wei and Cho, Kyunghyun and Lin, Jimmy},
  journal={arXiv preprint arXiv:1910.14424},
  year={2019}
}

@article{zhu2023large,
  title={Large language models for information retrieval: A survey},
  author={Zhu, Yutao and Yuan, Huaying and Wang, Shuting and Liu, Jiongnan and Liu, Wenhan and Deng, Chenlong and Dou, Zhicheng and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2308.07107},
  year={2023}
}

@article{pradeep2023rankzephyr,
  title={RankZephyr: Effective and Robust Zero-Shot Listwise Reranking is a Breeze!},
  author={Pradeep, Ronak and Sharifymoghaddam, Sahel and Lin, Jimmy},
  journal={arXiv preprint arXiv:2312.02724},
  year={2023}
}

@inproceedings{nogueira2020document,
  title={Document Ranking with a Pretrained Sequence-to-Sequence Model},
  author={Nogueira, Rodrigo and Jiang, Zhiying and Pradeep, Ronak and Lin, Jimmy},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={708--718},
  year={2020}
}

@inproceedings{zhuang2023rankt5,
  title={Rankt5: Fine-tuning t5 for text ranking with ranking losses},
  author={Zhuang, Honglei and Qin, Zhen and Jagerman, Rolf and Hui, Kai and Ma, Ji and Lu, Jing and Ni, Jianmo and Wang, Xuanhui and Bendersky, Michael},
  booktitle={Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={2308--2313},
  year={2023}
}

@article{bajaj2016ms,
  title={Ms marco: A human generated machine reading comprehension dataset},
  author={Bajaj, Payal and Campos, Daniel and Craswell, Nick and Deng, Li and Gao, Jianfeng and Liu, Xiaodong and Majumder, Rangan and McNamara, Andrew and Mitra, Bhaskar and Nguyen, Tri and others},
  journal={arXiv preprint arXiv:1611.09268},
  year={2016}
}

@inproceedings{sachan2022improving,
  title={Improving Passage Retrieval with Zero-Shot Question Generation},
  author={Sachan, Devendra and Lewis, Mike and Joshi, Mandar and Aghajanyan, Armen and Yih, Wen-tau and Pineau, Joelle and Zettlemoyer, Luke},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={3781--3797},
  year={2022}
}

@article{liang2022holistic,
  title={Holistic evaluation of language models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  journal={arXiv preprint arXiv:2211.09110},
  year={2022}
}

@article{sun2023instruction,
  title={Instruction distillation makes large language models efficient zero-shot rankers},
  author={Sun, Weiwei and Chen, Zheng and Ma, Xinyu and Yan, Lingyong and Wang, Shuaiqiang and Ren, Pengjie and Chen, Zhumin and Yin, Dawei and Ren, Zhaochun},
  journal={arXiv preprint arXiv:2311.01555},
  year={2023}
}

@article{qin2023large,
  title={Large language models are effective text rankers with pairwise ranking prompting},
  author={Qin, Zhen and Jagerman, Rolf and Hui, Kai and Zhuang, Honglei and Wu, Junru and Shen, Jiaming and Liu, Tianqi and Liu, Jialu and Metzler, Donald and Wang, Xuanhui and others},
  journal={arXiv preprint arXiv:2306.17563},
  year={2023}
}

@article{abdul2004umass,
  title={UMass at TREC 2004: Novelty and HARD},
  author={Abdul-Jaleel, Nasreen and Allan, James and Croft, W Bruce and Diaz, Fernando and Larkey, Leah and Li, Xiaoyan and Smucker, Mark D and Wade, Courtney},
  journal={Computer Science Department Faculty Publication Series},
  pages={189},
  year={2004}
}

@inproceedings{metzler2007latent,
  title={Latent concept expansion using markov random fields},
  author={Metzler, Donald and Croft, W Bruce},
  booktitle={Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval},
  pages={311--318},
  year={2007}
}

@inproceedings{zhai2001model,
  title={Model-based feedback in the language modeling approach to information retrieval},
  author={Zhai, Chengxiang and Lafferty, John},
  booktitle={Proceedings of the tenth international conference on Information and knowledge management},
  pages={403--410},
  year={2001}
}

@inproceedings{metzler2005markov,
  title={A markov random field model for term dependencies},
  author={Metzler, Donald and Croft, W Bruce},
  booktitle={Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval},
  pages={472--479},
  year={2005}
}

@inproceedings{mao2023large,
  title={Large Language Models Know Your Contextual Search Intent: A Prompting Framework for Conversational Search},
  author={Mao, Kelong and Dou, Zhicheng and Mo, Fengran and Hou, Jiewen and Chen, Haonan and Qian, Hongjin},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={1211--1225},
  year={2023}
}

@inproceedings{gao2023precise,
  title={Precise Zero-Shot Dense Retrieval without Relevance Labels},
  author={Gao, Luyu and Ma, Xueguang and Lin, Jimmy and Callan, Jamie},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1762--1777},
  year={2023}
}

@inproceedings{ma2023query,
  title={Query Rewriting in Retrieval-Augmented Large Language Models},
  author={Ma, Xinbei and Gong, Yeyun and He, Pengcheng and Zhao, Hai and Duan, Nan},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={5303--5315},
  year={2023}
}

@inproceedings{wang2023query2doc,
  title={Query2doc: Query Expansion with Large Language Models},
  author={Wang, Liang and Yang, Nan and Wei, Furu},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={9414--9423},
  year={2023}
}

@article{bonifacio2022inpars,
  title={Inpars: Data augmentation for information retrieval using large language models},
  author={Bonifacio, Luiz and Abonizio, Hugo and Fadaee, Marzieh and Nogueira, Rodrigo},
  journal={arXiv preprint arXiv:2202.05144},
  year={2022}
}

@inproceedings{dai2022promptagator,
  title={Promptagator: Few-shot Dense Retrieval From 8 Examples},
  author={Dai, Zhuyun and Zhao, Vincent Y and Ma, Ji and Luan, Yi and Ni, Jianmo and Lu, Jing and Bakalov, Anton and Guu, Kelvin and Hall, Keith and Chang, Ming-Wei},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{liu2023pre,
  title={Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
  author={Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal={ACM Computing Surveys},
  volume={55},
  number={9},
  pages={1--35},
  year={2023},
  publisher={ACM New York, NY}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{zhou2022conditional,
  title={Conditional prompt learning for vision-language models},
  author={Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={16816--16825},
  year={2022}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{liu2022few,
  title={Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning},
  author={Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin A},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={1950--1965},
  year={2022}
}

@article{xu2023expertprompting,
  title={Expertprompting: Instructing large language models to be distinguished experts},
  author={Xu, Benfeng and Yang, An and Lin, Junyang and Wang, Quan and Zhou, Chang and Zhang, Yongdong and Mao, Zhendong},
  journal={arXiv preprint arXiv:2305.14688},
  year={2023}
}

@article{du2023improving,
  title={Improving factuality and reasoning in language models through multiagent debate},
  author={Du, Yilun and Li, Shuang and Torralba, Antonio and Tenenbaum, Joshua B and Mordatch, Igor},
  journal={arXiv preprint arXiv:2305.14325},
  year={2023}
}

@article{datta2008image,
  title={Image retrieval: Ideas, influences, and trends of the new age},
  author={Datta, Ritendra and Joshi, Dhiraj and Li, Jia and Wang, James Z},
  journal={ACM Computing Surveys (Csur)},
  volume={40},
  number={2},
  pages={1--60},
  year={2008},
  publisher={ACM New York, NY, USA}
}

@inproceedings{huang2009analyzing,
  title={Analyzing and evaluating query reformulation strategies in web search logs},
  author={Huang, Jeff and Efthimiadis, Efthimis N},
  booktitle={Proceedings of the 18th ACM conference on Information and knowledge management},
  pages={77--86},
  year={2009}
}

@article{robertson1995okapi,
  title={Okapi at TREC-3},
  author={Robertson, Stephen E and Walker, Steve and Jones, Susan and Hancock-Beaulieu, Micheline M and Gatford, Mike and others},
  journal={Nist Special Publication Sp},
  volume={109},
  pages={109},
  year={1995},
  publisher={National Instiute of Standards \& Technology}
}

@inproceedings{borgeaud2022improving,
  title={Improving language models by retrieving from trillions of tokens},
  author={Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others},
  booktitle={International conference on machine learning},
  pages={2206--2240},
  year={2022},
  organization={PMLR}
}

@inproceedings{alaofi2023can,
  title={Can generative llms create query variants for test collections? an exploratory study},
  author={Alaofi, Marwah and Gallagher, Luke and Sanderson, Mark and Scholer, Falk and Thomas, Paul},
  booktitle={Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={1869--1873},
  year={2023}
}

@inproceedings{yu2023generate,
  title={Generate rather than Retrieve: Large Language Models are Strong Context Generators},
  author={Yu, Wenhao and Iter, Dan and Wang, Shuohang and Xu, Yichong and Ju, Mingxuan and Sanyal, S and Zhu, Chenguang and Zeng, Michael and Jiang, Meng},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@inproceedings{lewis2020bart,
  title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={7871--7880},
  year={2020}
}

@inproceedings{hou2024large,
  title={Large language models are zero-shot rankers for recommender systems},
  author={Hou, Yupeng and Zhang, Junjie and Lin, Zihan and Lu, Hongyu and Xie, Ruobing and McAuley, Julian and Zhao, Wayne Xin},
  booktitle={European Conference on Information Retrieval},
  pages={364--381},
  year={2024},
  organization={Springer}
}

@article{xi2023towards,
  title={Towards open-world recommendation with knowledge augmentation from large language models},
  author={Xi, Yunjia and Liu, Weiwen and Lin, Jianghao and Zhu, Jieming and Chen, Bo and Tang, Ruiming and Zhang, Weinan and Zhang, Rui and Yu, Yong},
  journal={arXiv preprint arXiv:2306.10933},
  year={2023}
}

@article{fan2023recommender,
  title={Recommender systems in the era of large language models (llms)},
  author={Fan, Wenqi and Zhao, Zihuai and Li, Jiatong and Liu, Yunqing and Mei, Xiaowei and Wang, Yiqi and Tang, Jiliang and Li, Qing},
  journal={arXiv preprint arXiv:2307.02046},
  year={2023}
}

@article{levy2024same,
  title={Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models},
  author={Levy, Mosh and Jacoby, Alon and Goldberg, Yoav},
  journal={arXiv preprint arXiv:2402.14848},
  year={2024}
}

@inproceedings{wooldridge1998pitfalls,
  title={Pitfalls of agent-oriented development},
  author={Wooldridge, Michael and Jennings, Nicholas R},
  booktitle={Proceedings of the second international conference on Autonomous agents},
  pages={385--391},
  year={1998}
}

@book{belbin2022team,
  title={Team roles at work},
  author={Belbin, R Meredith and Brown, Victoria},
  year={2022},
  publisher={Routledge}
}

@article{hong2023metagpt,
  title={Metagpt: Meta programming for multi-agent collaborative framework},
  author={Hong, Sirui and Zheng, Xiawu and Chen, Jonathan and Cheng, Yuheng and Wang, Jinlin and Zhang, Ceyao and Wang, Zili and Yau, Steven Ka Shing and Lin, Zijuan and Zhou, Liyang and others},
  journal={arXiv preprint arXiv:2308.00352},
  year={2023}
}

@inproceedings{thakur2021beir,
  title={BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models},
  author={Thakur, Nandan and Reimers, Nils and R{\"u}ckl{\'e}, Andreas and Srivastava, Abhishek and Gurevych, Iryna},
  booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
  year={2021}
}

@article{bai2023longbench,
  title={Longbench: A bilingual, multitask benchmark for long context understanding},
  author={Bai, Yushi and Lv, Xin and Zhang, Jiajie and Lyu, Hongchang and Tang, Jiankai and Huang, Zhidian and Du, Zhengxiao and Liu, Xiao and Zeng, Aohan and Hou, Lei and others},
  journal={arXiv preprint arXiv:2308.14508},
  year={2023}
}

@article{nogueira2019document,
  title={Document expansion by query prediction},
  author={Nogueira, Rodrigo and Yang, Wei and Lin, Jimmy and Cho, Kyunghyun},
  journal={arXiv preprint arXiv:1904.08375},
  year={2019}
}

@inproceedings{lin2021pyserini,
  title={Pyserini: A Python toolkit for reproducible information retrieval research with sparse and dense representations},
  author={Lin, Jimmy and Ma, Xueguang and Lin, Sheng-Chieh and Yang, Jheng-Hong and Pradeep, Ronak and Nogueira, Rodrigo},
  booktitle={Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={2356--2362},
  year={2021}
}


@inproceedings{pryzant2023automatic,
  title={Automatic Prompt Optimization with" Gradient Descent" and Beam Search},
  author={Pryzant, Reid and Iter, Dan and Li, Jerry and Lee, Yin Tat and Zhu, Chenguang and Zeng, Michael},
  booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
  year={2023}
}

@inproceedings{zhou2022large,
  title={Large Language Models are Human-Level Prompt Engineers},
  author={Zhou, Yongchao and Muresanu, Andrei Ioan and Han, Ziwen and Paster, Keiran and Pitis, Silviu and Chan, Harris and Ba, Jimmy},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@inproceedings{guo2023connecting,
  title={Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers},
  author={Guo, Qingyan and Wang, Rui and Guo, Junliang and Li, Bei and Song, Kaitao and Tan, Xu and Liu, Guoqing and Bian, Jiang and Yang, Yujiu},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@misc{ChatGPT,
  title = {Introducing ChatGPT},
  howpublished = {\url{https://openai.com/blog/chatgpt}},
  author={OpenAI},
  year={2022}
}

@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{cho2023discrete,
  title={Discrete Prompt Optimization via Constrained Generation for Zero-shot Re-ranker},
  author={Cho, Sukmin and Jeong, Soyeong and yeon Seo, Jeong and Park, Jong C},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={960--971},
  year={2023}
}

@inproceedings{schick2021exploiting,
  title={Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference},
  author={Schick, Timo and Sch{\"u}tze, Hinrich},
  booktitle={Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  pages={255--269},
  year={2021}
}

@inproceedings{sanh2021multitask,
  title={Multitask Prompted Training Enables Zero-Shot Task Generalization},
  author={Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Raja, Arun and Dey, Manan and others},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{jiang2020can,
  title={How Can We Know What Language Models Know?},
  author={Jiang, Zhengbao and Xu, Frank F and Araki, Jun and Neubig, Graham},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={423--438},
  year={2020}
}

@inproceedings{zhao2021calibrate,
  title={Calibrate before use: Improving few-shot performance of language models},
  author={Zhao, Zihao and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  booktitle={International conference on machine learning},
  pages={12697--12706},
  year={2021},
  organization={PMLR}
}

@inproceedings{lu2022fantastically,
  title={Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity},
  author={Lu, Yao and Bartolo, Max and Moore, Alastair and Riedel, Sebastian and Stenetorp, Pontus},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={8086--8098},
  year={2022}
}

@article{liu2023gpt,
  title={GPT understands, too},
  author={Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie},
  journal={AI Open},
  year={2023},
  publisher={Elsevier}
}

@inproceedings{qin2021learning,
  title={Learning How to Ask: Querying LMs with Mixtures of Soft Prompts},
  author={Qin, Guanghui and Eisner, Jason},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={5203--5212},
  year={2021}
}

@inproceedings{lester2021power,
  title={The Power of Scale for Parameter-Efficient Prompt Tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={3045--3059},
  year={2021}
}

@inproceedings{davison2019commonsense,
  title={Commonsense knowledge mining from pretrained models},
  author={Davison, Joe and Feldman, Joshua and Rush, Alexander M},
  booktitle={Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP)},
  pages={1173--1178},
  year={2019}
}

@article{yuan2021bartscore,
  title={Bartscore: Evaluating generated text as text generation},
  author={Yuan, Weizhe and Neubig, Graham and Liu, Pengfei},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={27263--27277},
  year={2021}
}

@inproceedings{reynolds2021prompt,
  title={Prompt programming for large language models: Beyond the few-shot paradigm},
  author={Reynolds, Laria and McDonell, Kyle},
  booktitle={Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages={1--7},
  year={2021}
}

@inproceedings{webson2022prompt,
  title={Do Prompt-Based Models Really Understand the Meaning of Their Prompts?},
  author={Webson, Albert and Pavlick, Ellie},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={2300--2344},
  year={2022}
}

@article{muennighoff2022sgpt,
  title={Sgpt: Gpt sentence embeddings for semantic search},
  author={Muennighoff, Niklas},
  journal={arXiv preprint arXiv:2202.08904},
  year={2022}
}

@article{fan2022pre,
  title={Pre-training Methods in Information Retrieval},
  author={Fan, Yixing and Xie, Xiaohui and Cai, Yinqiong and Chen, Jia and Ma, Xinyu and Li, Xiangsheng and Zhang, Ruqing and Guo, Jiafeng},
  journal={Foundations and Trends{\textregistered} in Information Retrieval},
  volume={16},
  number={3},
  pages={178--317},
  year={2022},
  publisher={Now Publishers Bostonâ€”Delft}
}

@inproceedings{karpukhin2020dense,
  title={Dense Passage Retrieval for Open-Domain Question Answering},
  author={Karpukhin, Vladimir and Oguz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={6769--6781},
  year={2020}
}

@inproceedings{qu2021rocketqa,
  title={RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering},
  author={Qu, Yingqi and Ding, Yuchen and Liu, Jing and Liu, Kai and Ren, Ruiyang and Zhao, Wayne Xin and Dong, Daxiang and Wu, Hua and Wang, Haifeng},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={5835--5847},
  year={2021}
}

@inproceedings{wu2017sequential,
  title={Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots},
  author={Wu, Yu and Wu, Wei and Xing, Chen and Zhou, Ming and Li, Zhoujun},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={496--505},
  year={2017}
}

@inproceedings{wang2022self,
  title={Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc V and Chi, Ed H and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@inproceedings{nye2022show,
  title={Show Your Work: Scratchpads for Intermediate Computation with Language Models},
  author={Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others},
  booktitle={Deep Learning for Code Workshop},
  year={2022}
}

@inproceedings{yao2022react,
  title={ReAct: Synergizing Reasoning and Acting in Language Models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik R and Cao, Yuan},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@inproceedings{prasad2023grips,
  title={GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models},
  author={Prasad, Archiki and Hase, Peter and Zhou, Xiang and Bansal, Mohit},
  booktitle={Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics},
  pages={3845--3864},
  year={2023}
}


@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{ye2023prompt,
  title={Prompt engineering a prompt engineer},
  author={Ye, Qinyuan and Axmed, Maxamed and Pryzant, Reid and Khani, Fereshte},
  journal={arXiv preprint arXiv:2311.05661},
  year={2023}
}

@inproceedings{formal2022distillation,
  title={From distillation to hard negative sampling: Making sparse neural ir models more effective},
  author={Formal, Thibault and Lassance, Carlos and Piwowarski, Benjamin and Clinchant, St{\'e}phane},
  booktitle={Proceedings of the 45th international ACM SIGIR conference on research and development in information retrieval},
  pages={2353--2359},
  year={2022}
}

@misc{jin2023visual,
      title={Visual Prompting Upgrades Neural Network Sparsification: A Data-Model Perspective}, 
      author={Can Jin and Tianjin Huang and Yihua Zhang and Mykola Pechenizkiy and Sijia Liu and Shiwei Liu and Tianlong Chen},
      year={2023},
      eprint={2312.01397},
      archivePrefix={arXiv},
      primaryClass={id='cs.CV' full_name='Computer Vision and Pattern Recognition' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers image processing, computer vision, pattern recognition, and scene understanding. Roughly includes material in ACM Subject Classes I.2.10, I.4, and I.5.'}
}

@misc{jin2024learning,
      title={Learning from Teaching Regularization: Generalizable Correlations Should be Easy to Imitate}, 
      author={Can Jin and Tong Che and Hongwu Peng and Yiyuan Li and Marco Pavone},
      year={2024},
      eprint={2402.02769},
      archivePrefix={arXiv},
      primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}

@article{zhou2024adapi,
  title={AdaPI: Facilitating DNN Model Adaptivity for Efficient Private Inference in Edge Computing},
  author={Zhou, Tong and Zhao, Jiahui and Luo, Yukui and Xie, Xi and Wen, Wujie and Ding, Caiwen and Xu, Xiaolin},
  journal={CoRR},
  year={2024}
}

@article{jin2024apeer,
  title={APEER: Automatic Prompt Engineering Enhances Large Language Model Reranking},
  author={Jin, Can and Peng, Hongwu and Zhao, Shiyu and Wang, Zhenting and Xu, Wujiang and Han, Ligong and Zhao, Jiahui and Zhong, Kai and Rajasekaran, Sanguthevar and Metaxas, Dimitris N},
  journal={arXiv preprint arXiv:2406.14449},
  year={2024}
}

@inproceedings{zhang2023online,
  title={Online learning for non-monotone DR-submodular maximization: From full information to bandit feedback},
  author={Zhang, Qixin and Deng, Zengde and Chen, Zaiyi and Zhou, Kuangqi and Hu, Haoyuan and Yang, Yu},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3515--3537},
  year={2023},
  organization={PMLR}
}

@inproceedings{zhang2022stochastic,
  title={Stochastic continuous submodular maximization: Boosting via non-oblivious function},
  author={Zhang, Qixin and Deng, Zengde and Chen, Zaiyi and Hu, Haoyuan and Yang, Yu},
  booktitle={International Conference on Machine Learning},
  pages={26116--26134},
  year={2022},
  organization={PMLR}
}

@article{wu2024cg,
  title={CG-FedLLM: How to Compress Gradients in Federated Fune-tuning for Large Language Models},
  author={Wu, Huiwen and Li, Xiaohan and Zhang, Deyi and Xu, Xiaogang and Wu, Jiafei and Zhao, Puning and Liu, Zhe},
  journal={arXiv preprint arXiv:2405.13746},
  year={2024}
}

@inproceedings{
zhao2024a,
title={A Huber Loss Minimization Approach to Mean Estimation under User-level Differential Privacy},
author={Puning Zhao and Lifeng Lai and Li Shen and Qingming Li and Jiafei Wu and Zhe Liu},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=TutGINeJzZ}
}