\section{Related Works}
\subsection{LLMs for Information Retrieval}
Text retrieval is a key component in a multitude of knowledge-driven Natural Language Processing (NLP) applications ____. In practice, this task is approached with a multi-stage ranking pipeline, typically consisting of an initial, cost-effective retriever followed by a more sophisticated reranker to refine the results____. Large language models (LLMs) have shown remarkable efficacy in information retrieval tasks____. Supervised reranking methods____ have traditionally relied on fine-tuning transformer-based models with copious training data, such as the MS MARCO v1 passage ranking dataset____. However, recent explorations involve LLMs in zero-shot unsupervised reranking. Pointwise approaches evaluate passage relevance individually____, whereas pairwise strategies compare two documents' relevancies for a given query____. Listwise methods, which directly reorder document relevance collectively, have achieved state-of-the-art performance____. This study introduces a novel multi-role reranking framework, \ours, which significantly enhances listwise reranking performance.


\subsection{Query Rewriting}
Original queries in traditional IR systems are often short or ambiguous, leading to vocabulary mismatch issues. Classic query rewriting techniques refine the original query iteratively by analyzing top-retrieved documents____. These methods, however, largely depend on term frequency statistics and may not grasp the true query intent. LLMs, with their advanced linguistic capabilities, support the generation of query rewrites that more accurately reflect the complex and varied information needs of users____. HyDE____ utilizes dense retrievers to generate pseudo-documents, while Query2doc____ and InPars____, along with Promptagator____, harness LLMs for producing synthetic queries through zero-shot or few-shot prompting.


\subsection{Prompt Engineer}
Prompt engineering is a critical technique for efficiently tailoring models to specific downstream tasks without fine-tuning____. The chain-of-thought (CoT) prompting method was introduced to encourage LLMs to generate intermediate reasoning steps before reaching a final answer____. In-context learning (ICL) leverages a few examples within the input to guide LLMs towards the intended task____. Expert prompting____ designs prompts that emulate an expert's reasoning, tailored to the input query's context. Multi-persona prompting____ employs a range of `personas' to tackle specific tasks. In \ours, we engage LLMs with various roles outlined in a standard operating procedure (SOP) for retrieval, yielding empirically validated improvements in reranking.