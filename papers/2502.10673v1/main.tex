%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{bbm}
\usepackage{stfloats}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{tcolorbox}
\usepackage[shortlabels]{enumitem}
\usepackage{longtable}
\usepackage{soul}
\sethlcolor{green}
\usepackage{caption}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\newcommand{\xuandong}[1]{{\color{red} [xuandong: #1]}}
\newcommand{\byh}[1]{{\color{blue} [Bu: #1]}}
\newcommand{\lyp}[1]{{\color{orange} [Liu: #1]}}
% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
% \usepackage[section]{placeins}
\usepackage{bm}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Dataset Protection via Watermarked Canaries in Retrieval-Augmented LLMs}

\begin{document}

\twocolumn[
\icmltitle{Dataset Protection via Watermarked Canaries in Retrieval-Augmented LLMs}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Yepeng Liu}{uf}
\icmlauthor{Xuandong Zhao}{ucb}
\icmlauthor{Dawn Song}{ucb}
\icmlauthor{Yuheng Bu}{uf}

\vspace{0.3em}
\icmlauthor{\textnormal{{\{yepeng.liu, buyuheng\}@ufl.edu},}}{}
\icmlauthor{\textnormal{{\{xuandongzhao, dawnsong\}@berkeley.edu},}}{}
\end{icmlauthorlist}

\icmlaffiliation{uf}{University of Florida}
\icmlaffiliation{ucb}{University of California, Berkeley}


% \icmlcorrespondingauthor{Yuheng Bu}{buyuheng@ufl.edu}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
\vspace{-0.3em}
Retrieval-Augmented Generation (RAG) has become an effective method for enhancing large language models (LLMs) with up-to-date knowledge. However, it poses a significant risk of IP infringement, as IP datasets may be incorporated into the knowledge database by malicious Retrieval-Augmented LLMs (RA-LLMs) without authorization. To protect the rights of the dataset owner, an effective dataset membership inference algorithm for RA-LLMs is needed. In this work, we introduce a novel approach to safeguard the ownership of text datasets and effectively detect unauthorized use by the RA-LLMs. Our approach preserves the original data completely unchanged while protecting it by inserting specifically designed canary documents into the IP dataset. These canary documents are created with synthetic content and embedded watermarks to ensure uniqueness, stealthiness, and statistical provability. During the detection process, unauthorized usage is identified by querying the canary documents and analyzing the responses of RA-LLMs for statistical evidence of the embedded watermark. Our experimental results demonstrate high query efficiency, detectability, and stealthiness, along with minimal perturbation to the original dataset, all without compromising the performance of the RAG system.
\end{abstract}

\vspace{-2em}

\section{Introduction}
\label{introduction}
\vspace{-0.3em}
Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to dynamically retrieve and integrate external knowledge, extending their capabilities beyond static training data to address up-to-date and domain-specific tasks. However, the reliance on external datasets in RAG raises potential concerns about dataset security and intellectual property (IP) rights. Unauthorized use or replication of proprietary datasets can lead to IP infringement, posing significant risks for data owners. This erosion of IP protection could, in turn, negatively affect economic efficiency \cite{ma2022specialization}. Therefore, safeguarding datasets in RAG systems is a critical challenge requiring methods that ensure ethical usage while preserving the dataset's original utility. One effective protection strategy is to perform pre-release operations on datasets that enable data owners to efficiently identify unauthorized use while maintaining the utility of the original IP dataset for authorized RAG systems. 

Specifically, embedding a watermark into the IP dataset and detecting it through the outputs of the trained LLMs offers an effective solution to address this issue. \citet{wei2024proving} propose inserting random character sequences into the IP dataset, enabling watermark detection by evaluating the loss of LLMs on these random character sequences. However, this approach embeds a visible watermark into the IP dataset, making it susceptible to being easily identified and removed \cite{liu2024robustifying}. Furthermore, the detection process relies on accessing the logits of LLMs, which are often inaccessible, e.g., GPT-4 \cite{brown2020language}.
% claud

To improve the stealthiness of the watermark and the applicability of the detection method, \citet{jovanovic2024ward} proposes adapting a watermarked LLM \cite{kirchenbauer2023watermark} to paraphrase each document in the IP dataset, embedding an \emph{invisible} watermark into the text. The watermark is then detected by black-box querying RA-LLMs with questions related to the watermarked documents. However, the paraphrasing process may introduce significant distortions by altering the original dataset. Moreover, since IP datasets vary significantly in their capacity for watermark embedding, low-entropy datasets often lack sufficient redundancy, making it hard to embed watermarks through paraphrasing without compromising their meaning or functionality.


\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.75\linewidth]{fig/overview.pdf}
    % \vspace{-0.6cm}
    \vspace{-0.5em}
    \caption{Overview of our DMI-RAG method. In the dataset protection stage, we generate watermarked and synthetic canary documents based on the attributes of the documents from the IP dataset to form a protected dataset. A malicious RAG system may integrate an IP dataset into its base dataset without obtaining permission from the data owner. During the detection stage, the data owner can conduct black-box queries (without logits information) targeting these canary documents using specifically crafted questions and analyze the model responses to detect the presence of the watermark.}
    \vspace{-0.5em}
    \label{fig:overview}
    \vspace{-0.8em}
\end{figure*}


These works highlight the importance of \emph{stealthy} pre-release operations on IP datasets to enable effective detection.
However, these operations must not compromise the original utility of the IP dataset in RAG. The most straightforward way to preserve utility is to keep the original data completely unchanged. Given that existing methods either compromise stealthiness or significantly alter the original data, we ask: \textit{Can we embed a stealthy and effective watermark to reliably detect unauthorized usage while keeping the original data completely unchanged?}


In this paper, we formulate the problem as Dataset Membership Inference for Retrieval-Augmented Generation of LLMs (DMI-RAG) and propose an effective solution. Our core idea is to embed a few watermarked \textit{canaries}\footnote{something acting as informers or decoys for the data owner.} into the IP dataset, enabling the dataset owner to verify their presence in suspected RA-LLMs.
% These canaries allow the dataset owner to query their presence within the dataset of suspicious RA-LLMs. 
If the watermark embedded in the canaries is detected in the responses of RA-LLMs, the dataset owner can attribute their presence to detect unauthorized usage. This operation offers two advantages: 1) maintaining accuracy and nuance: by preserving the original IP dataset, its quality and integrity are maintained, ensuring that the precise wording and stylistic elements remain intact; 2) ensuring the reliable detection for low-entropy dataset: by carefully designing synthetic canaries, we can embed watermarks without altering the original dataset, ensuring effective detection.



Specifically, these canaries are designed to be stealthy enough to evade detection or removal by malicious RA-LLMs without impacting the dataset's functionality or performance, while remaining distinctive enough to act as reliable markers for ownership verification. To achieve these properties, we use a watermarked LLM as a content creator to synthesize fictional canary documents that align with the attributes of the IP dataset. This attribute-based synthesis ensures consistency between the canary documents and the IP dataset. 
Their fictional nature ensures uniqueness, minimizing semantic overlap with other documents in the RAG dataset and improving the accuracy of canary document retrieval.
% The fictional nature of the canary documents ensures their uniqueness, which significantly reduces the likelihood of semantic overlap with other documents in the RAG dataset and enhances the accuracy of querying the canary documents. 
The watermarked LLM  embeds an invisible watermark into the canary documents, allowing the watermark to diffuse into the model responses when the canary documents are retrieved. Thanks to the robustness of the watermarking \cite{zhao2023provable}, the data owner can use the watermark in the responses as statistical evidence to detect unauthorized use of the protected dataset.



In summary, our main contributions are listed as follows:
\begin{enumerate}
[leftmargin=*,topsep=-0.4em,itemsep=-0.4em]
    \item We propose a dataset protection framework in a black-box setting that preserves the IP dataset's documents entirely intact while achieving high detection performance through LLM watermarking embedded in carefully designed canaries.
    
    \item We propose an attribute-based fictional data synthesis method that leverages the attributes of data from the IP dataset, ensuring high consistency and seamless integration. The perplexity of our canary documents is comparable to that of the original dataset, showcasing their stealthiness.
    
    \item We conduct experiments using our method on various datasets and compare the results with baseline approaches. The results demonstrate that our method achieves a $100\%$ query accuracy for retrieving the canary documents on the NFCorpus dataset. Additionally, we achieve a $100\%$ TPR@1\%FPR with only 12 queries to the suspicious RAG system.  Furthermore, our method does not impact the performance of downstream tasks.
    % , ensuring the original dataset's utility is preserved. 
    
\end{enumerate}



\vspace{-0.3em}

\section{Background and Related Work}
\vspace{-0.2em}
\subsection{Retrieval Augmented Generation}
\vspace{-0.3em}
RAG \cite{karpukhin2020dense, xiong2020approximate, lewis2020retrieval} integrates information retrieval with natural language generation to enhance the quality and relevance of generated responses \cite{gao2023retrieval}. 


In general, a RAG system involves three key components: the retriever $\mathcal{R}$, the generator $\mathcal{M}_0$, and a knowledge database $\mathcal{D}_{\mathsf{base}}$. Given a user query $q$, the retriever maps the input and documents in the knowledge database into embeddings within the same space. It then searches the knowledge database to retrieve $K$ most relevant documents, $\{d_1,...d_K\}=\mathcal{R}(q, \mathcal{D}_{\mathsf{base}})$, based on the distance metric like cosine-similarity. In the generation phase, the generator produces the response given the query and the retrieved documents, i.e., $y=\mathcal{M}_0(d_1,...,d_K,q)$.

\vspace{-0.5em}

\subsection{LLMs Watermarking} 
\vspace{-0.5em}
LLM watermarking embeds watermark into the text throughout the entire generation process \cite{li2024statistical, he2024universally, li2024robust, zhao2024sok, zhao2024permute, fu2024gumbelsoft, giboulot2024watermax, wu2023dipmark, he2025dist}. This is typically achieved by either perturbing the logits of the LLM \cite{liu2024adaptive, kirchenbauer2023watermark, zhao2023provable} or manipulating the sampling process \cite{kuditipudi2023robust, christ2024undetectable}. 

In particular, robustness is a critical property of LLM watermarking, enabling the detection of watermarks even after significant text modifications. For the DMI-RAG task, a robust watermark is essential to ensure its persistence from the watermarked document to the response of the RAG system. Therefore, among all existing LLM watermarking methods, we adopt the watermarking by \citet{zhao2023provable} in our framework, due to its simplicity and provable robustness. 


\vspace{-0.5em}

\subsection{Dataset Membership Inference for RAG} 
\vspace{-0.5em}
\textbf{Data Membership Attack.} Data membership attack for RAG \cite{liu2024mask} aims to determine whether a specific data instance is included in the knowledge dataset used by the RA-LLMs. \citet{li2024seeing} introduces a gray-box approach that computes a membership score by combining the similarity between generated text and a data member with the perplexity of the generated text. \citet{anderson2024my} propose a method that uses a specially designed prompt to black-box ask whether a dataset member appears in the context and deduces the membership status based on the model's answer.

\textbf{Backdoor Attack.} For dataset membership inference in RAG, the dataset owner can proactively perform specific operations on the original dataset to systematically accumulate strong evidence of unauthorized usage. Backdoor attack \cite{chaudhari2024phantom, cheng2024trojanrag, chen2024agentpoison} is an approach used to identify unauthorized usage by embedding triggers in the dataset. These triggers cause RA-LLMs to produce specified abnormal responses when queried with certain inputs. \citet{zou2024poisonedrag} propose injecting malicious texts into the dataset to manipulate RA-LLMs into generating a predetermined incorrect answer for a specific question. In some aspects, our method shares similarities with a backdoor. However, unlike existing backdoor attacks, which compromise the functionality or performance of the original dataset, our approach is specifically designed to avoid such issues.

% , making it more suitable for the dataset protection task.

\textbf{Dataset Membership Inference.} 
Dataset Membership Inference for protection can be achieved through proactive watermark embedding.
\citet{wei2024proving} propose inserting a random sequence repeatedly into the dataset and then computing the loss of the suspicious LLM on this sampled sequence to determine the dataset's presence. Most recently, \citet{jovanovic2024ward} proposes to use a watermarked LLM to paraphrase each document in the IP-protected dataset and detect the watermark in the responses of RA-LLM. In this paper, we embed \emph{invisible} watermarks into a small number of carefully designed canaries and insert them into the IP-protected dataset. Our approach preserves the original dataset untouched while achieving high detectability.



\begin{figure*}[!ht]
    \centering
    \includegraphics[width=1\linewidth]{fig/synthesis_workflow.pdf}
    \vspace{-2.5em}
    \caption{Workflow of our canary dataset synthesis algorithm. The process begins by randomly sampling a document from the IP dataset to serve as a reference. Next, key attributes are extracted from the reference document. Using these attributes, the descriptions and relationships between synthetic entities are created. Finally, the algorithm outputs the synthetic text and a corresponding query question.}
    %\byh{It seems that the article in 4 is not directly related to $C$ and $E$?}
    % \lyp{(4) is a summary of (3), and (3) is the detail description of entities in (2)}
    % \vspace{-0.6cm}
    \vspace{-0.9em}
    \label{fig:synthesis_workflow}
    \vspace{-0.5em}
\end{figure*}

\vspace{-0.5em}

\section{Dataset IP Protection}
\vspace{-0.3em}
\subsection{Threat Model}
\vspace{-0.3em}
Our threat model consists of two entities: the data owner, who seeks to protect datasets from unauthorized use, and a malicious RA-LLM, which owns a knowledge dataset and attempts to incorporate the IP dataset into its knowledge base without authorization. In our formulation, the data owner proactively performs operations on the IP dataset before its release, such as embedding invisible watermarks, to enable effective protection. The protected dataset is then accessible to authorized users or may be illegally obtained by unauthorized users. The data owner's capability involves performing black-box dataset membership inference by inputting a limited number of crafted queries to the RA-LLM and analyzing only the generated responses.

\vspace{-0.5em}

\subsection{Problem Formulation}
\label{section:problem_formulation}
\vspace{-0.3em}
Our high-level idea is to embed watermarked canaries into the protected dataset, serving as evidence of unauthorized use and enabling reliable detection if the dataset is misappropriated. As shown in Figure \ref{fig:overview}, our method consists of two key components. In the dataset protection phase, the data owner generates watermarked canary documents and inserts them into the IP dataset before the release. In the dataset detection phase, the data owner performs multiple queries to the RA-LLM and provably identifies unauthorized use by framing it as a hypothesis testing problem to distinguish between the following two hypotheses:

$H_0$: The response does not contain watermark  \\
$H_1$: The response contains a watermark.

\textbf{Canary Dataset Synthesis.}
Specifically, given a RA-LLM $\mathcal{M}_0$ with a knowledge dataset $\mathcal{D}_{\mathsf{base}}$, and an IP dataset $\mathcal{D}_{\mathsf{IP}}$. Our main idea is keeping original $\mathcal{D}_{\mathsf{IP}}$ untouched, and insert a canary dataset $\mathcal{D}_{s}$ to get a protected dataset $\mathcal{\hat{D}}_{\mathsf{IP}}=\mathcal{D}_{\mathsf{IP}} \cup \mathcal{D}_s$, with the number of documents $|\mathcal{D}_s|\ll |\mathcal{D}_{\mathsf{IP}}|$. 

The inserted $\mathcal{D}_s$ should assist the data owner in enhancing query efficiency, providing statistical evidence, and integrating well with $\mathcal{D}_\mathsf{IP}$.  Therefore, it should satisfy three key properties: uniqueness, stealthiness, and statistical provability. The uniqueness of the canary document minimizes semantic overlap with other documents in $\mathcal{D}_{\mathsf{base}}$ and $\mathcal{D}_s$, thereby increasing the likelihood that it can be successfully retrieved using specifically designed queries. The stealthiness of the canary document ensures its seamless integration into $\mathcal{D}_{\mathsf{IP}}$,  making it difficult to distinguish from the original IP dataset while preserving its overall coherence. 
% \byh{Need more operational explanation or should be defined in some way.} 

To achieve these properties, we design a canary data synthesis algorithm $\mathcal{S}:\mathcal{D} \mapsto \mathcal{D}_s \times \mathcal{Q}_s$, which uses a watermarked LLM to synthesize fictional text $d_s \in \mathcal{D}_s$ and the corresponding queries based on the attributes extracted from text $d \in \mathcal{D}_{\mathsf{IP}}$. This ensures that the synthesized text retains uniqueness and relevance to the original dataset while embedding the necessary watermark for detection. Then,  we synthesize query text to form the query dataset $q_s \in \mathcal{Q}_s$, specifically tailored for each $d_s \in \mathcal{D}_s$. 

% Formally, $q_s = g(d_s)$, where $g(\cdot)$ is a function that generates a query based on the given text. 

\textbf{Dataset Membership Detection.}
A watermark detector, $D_w: \mathcal{Y}^* \mapsto \mathbb{R}$, is employed to analyze the outputs $y$ of $\mathcal{M}_0$ generated in response to queries $q_s \in \mathcal{Q}_s$. Specifically, we can determine whether $\mathcal{D}_{\mathsf{IP}} \subseteq \mathcal{D}_\mathsf{base}$ if the detector satisfies the condition $D_w(y^{(1)} \oplus y^{(2)} \oplus \cdots \oplus y^{(N)}) \geq \eta$, where $\eta$ is a predefined threshold, $N$ is the number of queries, $\oplus$ represents the concatenation operation.


\vspace{-0.5em}
\subsection{Canary Dataset Synthesis}
\label{subsection:trap_dataset_synthesis}
\vspace{-0.3em}
% As described in the Section \ref{section:problem_formulation}, stealthiness and uniqueness  the synthesized text $d_s$ should have
In this section, we present our canary dataset synthesis algorithm $\mathcal{S}$. As shown in Figure \ref{fig:synthesis_workflow}, it includes: 1) attributes extraction, 2) fictional entity creation, 3) description synthesis, 4) article synthesis, and 5) query question generation.


\textbf{Attributes Extraction.} To ensure the synthesized text seamlessly integrates into $\mathcal{D}_{\mathsf{IP}}$ with high consistency, each synthesis process begins by randomly sampling a document from $d \in \mathcal{D}_{\mathsf{IP}}$. Then, we analyze and extract the key attributes $\mathcal{A}=\{A_1, A_2, ..., A_n\}$ of sampled text using an attribute extraction function, $\mathsf{attr\_extr\_func}$. This process is facilitated by an LLM $\mathcal{M}$ with appropriately designed prompts:
\begin{align}
    \mathcal{A} \leftarrow \mathcal{M}\big(\mathsf{attr\_extr\_func}(d)\big).
\end{align}
In our experiments, we extract four attributes from the sampled document $d$: $\{A_1=\mathsf{Topic}, A_2=\mathsf{Subtopic}, A_3=\mathsf{Writing\ Style}, A_4=\mathsf{Word\ Count}\}$. These attributes ensure that the synthesized documents align with the same topic, writing style, and length as the original dataset.

\textbf{Fictional Entity Creation.} The uniqueness of the canary documents significantly reduces overlap with other documents within $\mathcal{D}_\mathsf{base}$ and $\mathcal{D}_\mathsf{IP}$, ensuring a clear separation between the embeddings of the canary documents and those of other documents. This distinctiveness greatly enhances the efficiency of querying and retrieving the target canary documents. To ensure the uniqueness of the synthesized text, we extract real entities $\mathcal{E}=\{E_1,...E_n\}$ from $d$ and create fictional entities $\mathcal{\tilde{E}}=\{\tilde{E}_1,...,\tilde{E}_n\}$ conditioned on the attributes $\mathcal{A}$ using the LLM with the $\mathsf{ent\_synth\_func}$:
\begin{align}
    \{\mathcal{E}, \mathcal{\tilde{E}}\} \leftarrow \mathcal{M}\big(\mathsf{ent\_synth\_func}(d, \mathcal{A})\big).
\end{align}

\textbf{Description Synthesis.} Then, we synthesize the fictional descriptions for each entity and the relations between them $\mathcal{C}=\{C_1,...C_n\}$ using the LLM with the $\mathsf{des\_synth\_func}$:
\begin{align}
    \mathcal{C} \leftarrow \mathcal{M}\big(\mathsf{des\_synth\_func}(\mathcal{\tilde{E}}, \mathcal{A})\big).
\end{align}
This step preserves contextual relevance to $\mathcal{D}_{\mathsf{IP}}$ while ensuring that synthesized text remains distinct and identifiable.

\textbf{Article Synthesis.} Finally, we collect the synthesized descriptions $\mathcal{C}$ to generate a new article $d_s$ using the LLM with $\mathsf{article\_synth\_func}$:
\begin{align}
    d_s \leftarrow \mathcal{M}_w\big(\mathsf{article\_synth\_func}(\mathcal{C}, \mathcal{A})\big), \label{article_synthesis}
\end{align}
where $\mathcal{M}_w$ is a watermarked LLM, which will be detailed in Section \ref{subsection:watermarking_synthetic_dataset}.

\textbf{Query Question Generation.} For each $d_s \in \mathcal{D}_s$, we prompt an LLM to generate a question that can only be answered by reading $d_s$ using $\mathsf{query\_synth\_func}$:
\begin{align}
    q_s \leftarrow \mathcal{M}\big(\mathsf{query\_synth\_func}(d_s)\big)
\end{align}
The generated $q_s$ is used to retrieve the corresponding $d_s$ from the suspicious RA-LLM during the detection process. 
All the specific prompts used in our experiments are provided in Appendix \ref{sec:concrete_prompt}. Examples of the canary documents are provided in Appendix \ref{appendix:canary_example}.

\vspace{-0.5em}
\subsection{Watermarking Synthetic Dataset} 
\label{subsection:watermarking_synthetic_dataset}
\vspace{-0.3em}
The process outlined in Section \ref{subsection:trap_dataset_synthesis} introduces the uniqueness and stealthiness of the synthesized dataset. In this section, we detail the approach for ensuring the provability by integrating a watermarked LLM $\mathcal{M}_w$ into (\ref{article_synthesis}) of the algorithm $\mathcal{S}$. 

We employ a robust Unigram-Watermark scheme \cite{zhao2023provable}, which randomly partitions the LLM's vocabulary $\mathcal{V}$ into the global fixed green ($G$) and red ($R$) lists, such that $|G| = \gamma |\mathcal{V}|$ and $|R| = (1 - \gamma)|\mathcal{V}|$, where $\gamma\in (0,1)$ represents the proportion of tokens assigned to the green list. This scheme slightly increases the logits $\ell$ of tokens in the green list at each time step, such that $\hat{\ell}_t[k] \leftarrow \ell_t[k] + \delta$ for $k \in G$. Therefore, the output of the watermarked LLM will exhibit a higher probability of green tokens. The key assumption here is that the watermark embedded in the document should remain in the output \cite{sander2024radioactive} of the RA-LLM (which is shown in Section \ref{section:experiments}) when the watermarked document is retrieved in response to a query. Therefore, it provides a way to determine the dataset membership by analyzing the model's response and calculating the probability of green tokens present in the output.

\vspace{-0.5em}
\subsection{Dataset Membership Inference}
\vspace{-0.3em}
By applying the canary dataset synthesis algorithm $\mathcal{S}$, we can obtain the query dataset $\mathcal{Q}_s$ and canary dataset $\mathcal{D}_s$. The data owner can combine $\mathcal{D}_{\mathsf{IP}}$ and $\mathcal{D}_s$ to construct the new dataset $\mathcal{\hat{D}}_{\mathsf{IP}}$ prior to its release. During the dataset membership inference process, for a suspicious RA-LLM $\mathcal{M}_0$, we first query the model $N$ times using different $q_s^{(i)}\in \mathcal{Q}_s$ and obtain the response $y^{(i)}=\mathcal{M}_0(\mathcal{R}(q_s^{(i)}, \mathcal{D}_{\mathsf{base}}))$. Based on the discussion in Section \ref{subsection:watermarking_synthetic_dataset}, the dataset membership inference problem can be effectively transformed into a watermark detection problem. However, directly detecting the watermark in a single response is challenging because the embedded watermark in $d_s$ becomes significantly weakened after passing through the RAG pipeline. Therefore, we concatenate $N$ responses into a single sequence, denoted as $\mathbf{y} = y^{(1)} \oplus y^{(2)} \oplus \cdots \oplus y^{(N)}$, to enhance the statistical detectability of the watermark in responses.

Specifically, following \citet{kirchenbauer2023watermark} and \citet{zhao2023provable}, we detect the watermark by computing the z-statistic of the response $\mathbf{y}$, i.e.,
\begin{align}
    z_{\mathbf{y}} = (|\mathbf{y}|_G - \gamma T)/\sqrt{\gamma(1-\gamma)T},
\end{align}
where $T$ is the number of tokens in $\mathbf{y}$, and $|\mathbf{y}|_G$ is the count of the tokens in green list. The response $\mathbf{y}$ is identified as watermarked if  $z_{\mathbf{y}}>\eta$, where $\eta$ is a pre-defined threshold. Therefore, we can equivalently conclude if  $\mathcal{D}_{\mathsf{IP}} \subseteq \mathcal{D}$.

\vspace{-0.3em}
\section{Experiments}
\label{section:experiments}
\vspace{-0.3em}
\subsection{Experiment Setting}
\vspace{-0.3em}
\textbf{Implementation Details.} 
For our canary dataset synthesis algorithm, we use GPT4o-mini to extract attributes and generate fictional descriptions. Additionally, we use watermarked Llama-3.1-70B-Instruct to embed the watermark into the synthesized articles. For the watermarking algorithm, we set $\gamma=0.5$ and $\delta=2.0$. For retriever, we conduct experiments based on the Contriever-ms \cite{izacard2021unsupervised} and select the top $K=3$ most relevant documents based on the cosine similarity. 
In our main experiments, each canary document is queried only once with a single question. We investigate the detection performance under the setting where multiple queries are conducted per canary document in  Appendix \ref{appendix:exp}.
%\byh{500 fictional docs}

\textbf{Baselines.} We compare our methods with two existing dataset membership inference methods leveraging watermark, including Ward \cite{jovanovic2024ward}, and WWJ \cite{wei2024proving}. For Ward, consistent with its default settings, we use KGW \cite{kirchenbauer2023watermark} to paraphrase the whole dataset and set $\gamma=0.5$, $\delta=3.5$, and window size $h=2$. For WWJ, the original method is designed to protect the dataset used for training or fine-tuning. We make several modifications to adapt this method to the DMI-RAG task. Specifically, we randomly sample a watermark sequence $u$ from the ASCII table and then insert the watermark sequence into each document in the IP dataset. Next, similar to our method, we generate questions $(q_u^{(1)}, ...,q_u^{(N)})$ for each document to query the corresponding content. 
% \byh{$q_u$ will depend on the random sequence?} \lyp{revised} 
For the detection process, we first use the $q_u^{(i)}$ to compute the loss of the watermark sequence $l_u$. 
% Then, we randomly sample $N$ different random sequences $(\tilde{u}^{(1)},...,\tilde{u}^{(N)})$ and 
Then, we use the dataset-unrelated questions $(\tilde{q}^{(1)},...,\tilde{q}^{(N)})$ to query the RA-LLM to compute the loss of $u$ when retrieved documents do not contain watermark sequence. This process yields the mean $\mu$ and standard deviation $\sigma$ of loss $(\tilde{l}_{u}^{(1)},...,\tilde{l}_{u}^{(N)})$. The final decision is made by computing the statistic: $z = (l_u-\mu)/\sigma$. However, we note that this baseline requires access to the log probabilities of the suspicious model's predictions, which is not available in the black-box setting. In our experiments, we set the watermark sequence length $|u|=40$.


\textbf{Datasets.} We use the MS MARCO \cite{nguyen2016ms} as the knowledge dataset of RA-LLMs, which is a large-scale real-world web document corpus including approximately 8 million documents.  In this setting, we assume that potential semantic overlap may exist between the knowledge dataset and the IP dataset, which often happens in the real world. We evaluate our performance using the bio-medical information retrieval dataset, NFCorpus \cite{boteva2016full}, as the IP dataset. Moreover, we use CQADupStack-Mathematica \cite{hoogeveen2015cqadupstack} as IP dataset to evaluate the detection performance of our methods on the low-entropy dataset.
% Moreover, we evaluate the detection performance on low-entropy dataset, CQADupStack-Mathematica \cite{hoogeveen2015cqadupstack}.
We evaluate the downstream performance of different methods on Chinese-poem\footnote{https://huggingface.co/datasets/xmj2002/tang\_poems} and DROP dataset \cite{Dua2019DROP}.


\textbf{Evaluation Metrics.} To evaluate and compare the performance of different methods, we employ the following evaluation metrics.

\begin{itemize}
[leftmargin=*,topsep=-0.4em,itemsep=-0.4em]
    \item We use Target Retrieval Accuracy to evaluate the proportion of corresponding watermarked documents successfully retrieved by the queries. It is computed as: $\frac{1}{N} \sum_{i=1}^N \mathbbm{1}(d^{(i)} \in \mathcal{R}(q^{(i)}))$.
    \item For detectability, the detection performance is assessed using the ROC-AUC value, which measures the ability of a detector to distinguish between classes by evaluating the trade-off between true positive rate (TPR) and false positive rate (FPR) across different thresholds. 
    % Moreover, to evaluate detection performance at low FPR, 
    Moreover, we report detection performance at different FPR values, such as TPR@1\%FPR and TPR@10\%FPR.
    \item We evaluate the dataset distortion caused by different dataset protection methods using BLEU \cite{papineni2002bleu} and MAUVE \cite{pillutla2021mauve} scores.
    \item The stealthiness is measured using Perplexity and QuRating \cite{wettig2024qurating}, which are two methods to curate a dataset. For perplexity, we compute the perplexity for each document in both the original IP dataset and the watermarked dataset. Moreover, we split each document into smaller blocks (50 words/block) and calculate the perplexity for each block individually. Extremely high perplexity indicates low-quality text or potential damage to the original content. We use these blocks to calculate the Filtering Rate, measuring the proportion of blocks filtered out when a perplexity threshold is applied. The QuRating employs a rating model to select high-quality data within the dataset, evaluating across four key dimensions: writing quality, facts\&trivia, educational value, and required expertise.
    \item We calculate the response correctness using GPT4o-mini as a judge to assess the impact of each dataset protection method on the downstream performance of RA-LLMs.
\end{itemize}


\begin{table*}[htb]\scriptsize
\centering
\setlength{\tabcolsep}{4pt}
\caption{Detection performance across different methods on NFCorpus dataset with varying query quota. Our method achieves $100\%$ detection performance with only $12$ query quota.}
\vspace{-1em}
\begin{tabular}{cccccccccc}
\toprule
\multirow{2}{*}{Query Quota} & \multicolumn{3}{c}{WWJ}            & \multicolumn{3}{c}{Ward}           & \multicolumn{3}{c}{\textbf{Ours}}           \\ \cmidrule(lr){2-4} \cmidrule(lr){5-7}  \cmidrule(lr){8-10}
                              & ROC-AUC & TPR@1\%FPR & TPR@10\%FPR & ROC-AUC & TPR@1\%FPR & TPR@10\%FPR & ROC-AUC & TPR@1\%FPR & TPR@10\%FPR \\ \midrule
1                             & -        & -           & -            & $0.795$ & $0.148$    & $0.408$     & $0.910$ & $0.294$    & $0.724$            \\
2                             & -        &  -          & -            & $0.809$ & $0.274$    & $0.482$     & $0.970$ & $0.546$    & $0.908$            \\
4                             & $0.986$        &$0.968$            & $0.972$            & $0.880$ & $0.336$    & $0.660$     & $0.995$ & $0.850$    & $0.996$            \\
6                             & $0.987$        &$0.969$            &$0.974$             & $0.922$ & $0.530$    & $0.762$     & $0.998$ & $0.960$    & $1.000$     \\
8                             &$0.990$         & $0.970$           &$0.974$             & $0.940$ & $0.520$    & $0.820$     & $0.999$ & $0.994$    & $1.000$     \\
10                            &$0.990$         &$0.972$            &$0.976$             & $0.963$ & $0.660$    & $0.900$     & $0.999$ & $0.998$    & $1.000$      \\
12                            & $0.991$        & $0.972$           & $0.978$            & $0.982$ & $0.792$    & $0.954$     & $\bm{1.000}$ & $\bm{1.000}$    & $\bm{1.000}$    \\ \bottomrule
\end{tabular}
\label{tab:detectability}
\vspace{-1em}
\end{table*}

\subsection{Main Results}
\vspace{-0.3em}
\textbf{Target Retrieval Accuracy.} Target Retrieval Accuracy is a crucial performance metric for the DMI-RAG task. A high Target Retrieval Accuracy ensures that the target watermarked documents can be retrieved with high probability using specifically designed queries. In this experiment, Target Retrieval Accuracy is calculated using 500 different questions to retrieve the corresponding watermarked documents for each method. Specifically, for our method, we insert 500 canary documents into the IP dataset. 
% For other baseline methods, we also query the watermarked documents 500 times using the corresponding questions.
Figure \ref{fig:retrieval_accuracy} shows the Target Retrieval Accuracy for the original IP dataset and different baselines. The `Original' represents the original IP dataset without any modification, where we generate 500 different questions to query different documents in the original IP dataset. As shown in the table, our method outperforms all others, achieving a perfect Retrieval Accuracy of $100\%$. This is attributed to the insertion of synthetic fictional canaries, which minimizes the potential semantic repetition with other documents in the base dataset. In contrast, the methods employed by WWJ and Ward affect the Target Retrieval Accuracy to varying degrees compared to the original IP dataset.


\begin{figure}[h]
\vspace{-0.5em}
    \centering
    \includegraphics[width=0.9\linewidth]{fig/retrieval_accuracy.pdf}
    \vspace{-1em}
    \caption{Retrieval Accuracy for different methods. Our method achieves $100\%$ Retrieval Accuracy.}
    \label{fig:retrieval_accuracy}
    \vspace{-0.5em}
\end{figure}
% \lyp{font bigger}
% \lyp{query used for different tasks}

\vspace{-0.3em}

\textbf{Detectability.} We evaluate and compare the detection performance across different query quotas for various methods. Our goal is to achieve superior detection performance with fewer number of queries. As shown in Table \ref{tab:detectability}, our method outperforms the baseline approaches by achieving superior detection performance with a lower query count. In particular, we achieve $100\%$ TPR@1\%FPR by only querying the RA-LLM 12 times, while Ward is only $79.2\%$ on the same quota. The main reason is that our method achieves high Target Retrieval Accuracy, and the adopted watermarking~\cite{zhao2023provable} is provably more robust.


\textbf{Dataset Distortion.} Dataset Distortion quantifies the extent of changes introduced by various methods to the original IP dataset. To evaluate this, we use BLEU and MAUVE scores. BLEU assesses the $n$-gram word-level overlap between the original dataset and the protected dataset, while MAUVE evaluates the similarity in distributions between the two datasets. As shown in Table \ref{tab:dataset_distortion}, our method achieves the highest BLEU and MAUVE scores compared to other methods, demonstrating minimal distortion to the original dataset. This is attributed to our approach of keeping the original documents entirely unchanged while ensuring that the inserted canary documents share the same attributes as the original IP dataset. However, Ward modifies the expressions and words in the original dataset, resulting in low BLEU and MAUVE scores. The random sequence inserted by WWJ does not alter the original expressions, resulting in a high BLEU score. However, it introduces a significant distribution shift between the original IP dataset and the protected dataset, leading to a low MAUVE score.

\vspace{-0.3em}

\begin{table}[htb] \scriptsize
\centering
\setlength{\tabcolsep}{17pt}
\caption{Dataset distortion for different methods. Our method achieves the lowest dataset distortion in terms of both BLUE and MAUVE.}
\vspace{-1em}
\begin{tabular}{cccc}
\toprule
Methods & WWJ & Ward & Ours \\ \midrule
BLUE ($\uparrow$)    & $0.981$    & $0.132$     & $\bm{0.997}$     \\
MAUVE ($\uparrow$)   & $0.004$    & $0.340$     & $\bm{0.999}$    \\  \bottomrule
\end{tabular}
\label{tab:dataset_distortion}
\end{table}

\textbf{Stealthiness.} The stealthiness of our method is evaluated using two dataset curation methods: perplexity and QuRating. For perplexity, we calculate the perplexity of the canary documents generated by our method, the paraphrased documents produced by Ward, and the documents with inserted random sequences created by WWJ. We set the maximum perplexity value of the original dataset as a threshold to compute the Filtering Rate. In particular, we compute the perplexity using GPT-3. Table \ref{tab:perplexity} shows that our canary documents exhibit a low average perplexity, comparable to that of the original IP dataset, and a $0$ Filtering Rate. This demonstrates that our synthetic data is difficult to detect and remove based on the perplexity. However, WWJ shows a high Filtering Rate and average perplexity. This is because the randomly sampled sequences are inconsistent with the natural language patterns expected by a language model, resulting in extremely high perplexity. 


\vspace{-0.3em}

\begin{table}[htb] \scriptsize
\centering
\setlength{\tabcolsep}{9pt}
\caption{Perplexity and Filtering Rate across different methods. Our method achieves a perplexity comparable to that of the original dataset and $0\%$ Filtering Rate.}
\vspace{-1em}
\begin{tabular}{lcccc}
\toprule
Methods     &Original   & WWJ & Ward & Ours \\ \midrule
Avg Perplexity ($\downarrow$)&$6.736$  &$12.734$     & $9.453$  &$\bm{6.952}$    \\
Filtering Rate ($\downarrow$) &-  & $0.104$    & $0.015$     & $\bm{0.000}$      \\ \bottomrule
\end{tabular}
\label{tab:perplexity}
\vspace{-1em}
\end{table}

For QuRating, we measure the canary documents generated by our method, the paraphrased documents produced by Ward, and the documents with inserted random sequences created by WWJ from four different aspects, obtaining the corresponding scores for each. The dataset distributions across different aspects are shown in Figure \ref{fig:qurating}. As can be seen, our canary documents exhibit higher writing quality, making them less likely to be removed based on this criterion. For the remaining three aspects, our synthetic documents exhibit a similar distribution to the original documents, demonstrating their consistency and seamless integration into the original IP dataset. Both perplexity and QuRating demonstrate the high quality and stealthiness of our canary documents.

\vspace{-0.5em}

\begin{figure}[htb]
\vspace{-0.5em}
    \centering
    \includegraphics[width=1\linewidth]{fig/qurating.pdf}
    \vspace{-2.5em}
    \caption{Distribution of quality ratings using QuRating across four different aspects for various methods.}
    \label{fig:qurating}
    \vspace{-0.7em}
\end{figure}
% \lyp{font bigger}


\begin{table*}[!ht]\scriptsize
\centering
\setlength{\tabcolsep}{14pt}
\captionsetup{width=0.95\textwidth}
\caption{Detection performance for different methods on low-entropy dataset (Mathematica) with varying query quota. Our method achieves $100\%$ detection performance with a query quota of just 10, while Ward shows a significantly lower detection performance.}
% due to the minimal redundancy in low-entropy dataset.
%CQADupStack-Mathematica
\vspace{-1em}
\begin{tabular}{ccccccc}
\toprule
\multirow{2}{*}{Query Quota} & \multicolumn{3}{c}{Ward}           & \multicolumn{3}{c}{\textbf{Ours}}           \\ \cmidrule(lr){2-4}  \cmidrule(lr){5-7}
                              & ROC-AUC & TPR@1\%FPR & TPR@10\%FPR & ROC-AUC & TPR@1\%FPR & TPR@10\%FPR \\ \midrule
1                             & $0.552$        & $0.024$           & $0.145$            & $0.925$        & $0.390$            & $0.770$            \\
2                             & $0.572$        & $0.029$           & $0.116$             &$0.975$         & $0.584$           &$0.938$             \\
4                             & $0.614$        & $0.016$           & $0.215$           & $0.997$        & $0.920$           & $1.000$            \\
6                             & $0.623$        & $0.037$           & $0.244$             & $0.999$        & $0.974$           & $1.000$            \\
8                             & $0.628$        & $0.012$           & $0.228$            &$0.999$         & $0.998$           & $1.000$            \\
10                            &$0.668$         &$0.049$            & $0.219$            & $\bm{1.000}$        & $\bm{1.000}$           & $\bm{1.000}$              \\
12                            & $0.680$        & $0.041$           & $0.235$            & $\bm{1.000}$        & $\bm{1.000}$           & $\bm{1.000}$            \\ \bottomrule
\end{tabular}
\label{tab:low_entropy_detectability}
\vspace{-1em}
\end{table*}

\textbf{Downstream Performance.} We further investigate the impact of different methods on the downstream performance by measuring the correctness of the model response. We use two datasets for two different tasks. 
 

For DROP, we evaluate the correctness of the model's responses on the protected dataset, specifically for discrete reasoning and numerical computation tasks.
As shown in Table \ref{tab:downstream_performance}, our method maintains the correctness of the original IP dataset, as there is no impact on its performance. This is because our method preserves the original data without any modifications. In contrast, the correctness of Ward decreases from $0.733$ to $0.714$. The drop in Ward's performance suggests that paraphrasing may introduce additional errors, negatively affecting downstream tasks. 

Moreover, for literary datasets, such as poetry, we evaluate correctness in terms of poem appreciation. Specifically, we emphasize the critical importance of preserving the original data intact, as even slight modifications to the words in a poem can significantly alter the author's intended expression and emotional tone, thereby impacting the overall interpretation and appreciation of the work. From Table \ref{tab:downstream_performance}, the correctness of Chinese poem appreciation for Ward drops significantly from $0.908$ to $0.719$, whereas our method maintains the same level of correctness.

\vspace{-0.3em}

\begin{table}[htb]\scriptsize
\centering
\setlength{\tabcolsep}{15pt}
\caption{Downstream Performance on two datasets.}
\vspace{-1em}
\begin{tabular}{clc}
\toprule
Methods                                       & Dataset & Avg. Correctness \\ \midrule
\multicolumn{1}{c}{\multirow{2}{*}{Original}} &DROP         & $0.772$          \\
\multicolumn{1}{c}{}                          &Chinese-Poem           & $0.908$            \\ \midrule
\multirow{2}{*}{WWJ}                          &DROP         & $0.771$            \\
                                              &Chinese-Poem         &$0.900$             \\ \midrule
\multirow{2}{*}{Ward}                         &DROP         & $0.767$            \\
                                              &Chinese-Poem         &$0.719$             \\ \midrule
\multirow{2}{*}{Ours}                         &DROP         & $0.772$            \\
                                              &Chinese-Poem         &$0.908$    \\ \bottomrule  

\label{tab:downstream_performance}
\end{tabular}
\vspace{-1em}
\end{table}
\vspace{-1em}

\subsection{Detection Performance on Hard Conditions}
% \lyp{harder condition. subsection.}
\vspace{-0.3em}
\textbf{Detectability on Low Entropy IP Dataset.} We further evaluate the detection performance of our method on a low-entropy IP dataset, CQADupStack-Mathematica \cite{hoogeveen2015cqadupstack}, and compare it with Ward. As shown in Table \ref{tab:low_entropy_detectability}, our method maintains a high detection performance, achieving $100\%$ TPR@$1\%$FPR using only 10 query quotas. In contrast, Ward achieves significantly lower detection performance, with $0.049\%$ TPR@$1\%$FPR at the same query quota. This is because low-entropy IP datasets lack redundancy, making it difficult to perform modifications. As a result, paraphrasing can embed only a limited amount of watermark, which fails to persist in the model's responses. In contrast, our method remains unaffected, as we embed watermarks into carefully designed canaries, which are not constrained by the entropy of the original IP dataset.


\textbf{Hard System Prompt.} We investigate the detection performance under two types of system prompts: easy and hard. An easy system prompt is straightforward, asking the model to answer the question without imposing additional constraints or complex instructions. In contrast, a hard system prompt includes more restrictive instructions, such as strictly avoiding verbatim text or excessive paraphrasing. Table \ref{tab:detectability} shows detection performance under the easy system prompt. In Table \ref{tab:hard_prompt_detectability}, we showcase the detection performance under the hard system prompt. The results demonstrate that our method achieves the $100\%$ TPR@$1\%$FPR with $14$ query quotas, indicating that the detection performance remains unaffected even with hard system prompt.

\vspace{-0.3em}

\begin{table}[H]\scriptsize
\centering
\setlength{\tabcolsep}{10pt}
\caption{Detection performance on hard system prompt. }
% \caption{Detection performance of our method on hard system prompt. }
% Our method achieves $100\%$ detection performance using $14$ query quotas, even under hard system prompt.
\vspace{-1em}
\begin{tabular}{cccc}
\toprule
Query Quota & ROC-AUC & TPR@1\%FPR & TPR@10\%FPR \\ \midrule
1           & $0.883$        & $0.200$           & $0.650$            \\
2           & $0.962$        & $0.408$           & $0.894$            \\
4           & $0.988$        & $0.736$           & $0.980$            \\
6           & $0.997$        & $0.868$           & $0.996$            \\
8           & $0.999$        & $0.992$    & $1.000$\\
10          & $0.999$        & $0.996$           & $1.000$            \\
12          & $0.999$        & $0.998$           & $1.000$     \\ 
14         &$\bm{1.000}$        & $\bm{1.000}$           & $\bm{1.000}$      \\ \bottomrule
\end{tabular}
\label{tab:hard_prompt_detectability}
\end{table}

\vspace{-0.5em}

\textbf{Additional Results.} In Appendix \ref{appendix:exp}, we investigate two key aspects: (1) the impact of watermark strength on detection performance, and (2) detection performance when querying a single canary document multiple times. Specifically, in the multiple queries per canary document setting, we show that the data owner can achieve $100\%$ detection performance with as few as $5$ canary documents.

\vspace{-0.8em}

\section{Conclusion}
\vspace{-0.5em}
In this paper, we propose a novel method to protect proprietary datasets from unauthorized use while preserving their original integrity by embedding carefully designed canary documents into the IP-protected dataset. Our experimental results demonstrate the effectiveness of our method in both detectability and stealthiness, making it a practical and reliable approach for real-world applications.


\clearpage
\section{Discussion and Limitation}
Our method safeguards the dataset by inserting a small number of synthetic canary documents. Specifically, we generate canaries by creating fictional entities, which enhances their uniqueness and improves the accuracy of retrieving target canaries. However, there exists a trade-off between uniqueness and stealthiness. The use of fictional entities increases uniqueness, making canaries easier to retrieve and strengthening the detection of unauthorized dataset usage. On the contrary, fictional entities may reduce the stealthiness of canaries, as malicious RA-LLMs with fact-checking capabilities could flag inconsistencies between fictional entities and real-world knowledge sources. However, fact-checking tools, such as Google Fact Check Tool, typically rely on existing knowledge bases \cite{li2024fakenews, wang2024factcheck}, which may not be comprehensive or up-to-date, especially for IP-protected datasets. This limitation reduces their confidence in detecting inconsistencies, making it harder for them to flag canary documents as artificial. Additionally, our canary dataset synthesis algorithm extracts real entities from the IP dataset. This allows us to synthesize canary data using those real entities and their potential relationships, potentially sacrificing some retrieval accuracy but ensuring complete stealthiness.

% \xuandong{For example, which fact-checking tools?}\lyp{example added.}

\section*{Impact Statement}
Our work addresses the risk of intellectual property infringement in RAG systems by introducing a stealthy dataset membership inference technique. We embed canary documents with statistical watermarks into copyrighted datasets, enabling dataset owners to detect unauthorized use by malicious Retrieval-Augmented LLMs while preserving data integrity.
This method strengthens data provenance, supports ethical AI deployment, and aligns with emerging legal frameworks. However, potential misuse risks include false claims of ownership.
% and unintended model hallucinations. 
Future research should refine robustness, validation, and ethical safeguards to ensure responsible deployment.








% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Additional Experiment Results.}
\label{appendix:exp}
\textbf{Impact of Watermark Strength to Detectability.}
We examine the impact of different watermark strengths $\delta$ on detection performance. We set $\delta$ to 1, 2, and 3 and evaluate detection performance under the same query quota. As shown in Table \ref{tab:detectability_diff_watermark_para}, increasing $\delta$ enhances detection efficiency, allowing us to achieve $100\%$ detection performance with fewer queries.


\begin{table*}[htb]\scriptsize
\centering
\setlength{\tabcolsep}{4pt}
\caption{Detection performance of our method under different watermark strengths. As the watermark strength increases, our method achieves higher detection performance with fewer query quotas.}
\begin{tabular}{cccccccccc}
\toprule
\multirow{2}{*}{Query Quota} & \multicolumn{3}{c}{$\delta=1$}            & \multicolumn{3}{c}{\textbf{$\delta=2$}}           & \multicolumn{3}{c}{$\delta=3$}           \\ \cmidrule(lr){2-4} \cmidrule(lr){5-7}  \cmidrule(lr){8-10}
                              & ROC-AUC & TPR@1\%FPR & TPR@10\%FPR & ROC-AUC & TPR@1\%FPR & TPR@10\%FPR & ROC-AUC & TPR@1\%FPR & TPR@10\%FPR \\ \midrule
1                             & $0.855$        &$0.153$            &$0.556$             & $0.910$ & $0.294$    & $0.724$     &$0.962$         & $0.616$           &$0.894$             \\
2                             & $0.937$        & $0.276$           &$0.786$             & $0.970$ & $0.546$    & $0.908$     &$0.993$         & $0.86$           & $0.986$            \\
4                             & $0.981$        &$0.610$            &$0.948$             & $0.995$ & $0.850$    & $0.996$     & $0.999$        &  $0.986$          & $1.000$            \\
6                             &$0.995$         & $0.808$           & $0.998$            & $0.998$ & $0.960$    & $1.000$     &$0.999$         &$0.994$           &$1.000$             \\
8                             &$0.999$         & $0.962$           & $1.000$            & $0.999$ & $0.994$    & $1.000$     &$1.000$        &$1.000$            &$1.000$             \\
10                            & $0.999$        & $0.992$           &  $1.000$           & $0.999$ & $0.998$    & $1.000$     &$1.000$         &$1.000$            &$1.000$             \\
12                            &$0.999$         &$0.998$             &$1.000$             & $1.000$ & $1.000$    & $1.000$    &$1.000$         &$1.000$            &$1.000$             \\ \bottomrule
\end{tabular}
\label{tab:detectability_diff_watermark_para}
\end{table*}



% \textbf{The selection of K.}

\textbf{Query one document multiple times.} In Table \ref{tab:detectability}, we show that inserting only 12 canary documents into the IP dataset is sufficient to achieve strong detection performance when each canary document is queried only once. In this section, we want to explore whether inserting fewer canary documents into the IP dataset while querying a single canary document multiple times can still achieve effective detection. Specifically, we generate 14 different query questions for each canary document, allowing us to query a single canary document 14 times using diverse inputs. Table \ref{tab:multiple_query} presents the detection performance for different numbers of inserted canary documents, ranging from 1 to 5. The results show that even with just one canary document, we achieve $0.998$ ROC-AUC and $96.6\%$ TPR@1\%FPR. By inserting only five canary documents, our method attains $1.000$ ROC-AUC and $100\%$ TPR@1\%FPR, demonstrating the effectiveness of querying a single canary document multiple times to enhance detection performance.

\begin{table}[h]\scriptsize
\centering
\setlength{\tabcolsep}{25pt}
\captionsetup{width=0.75\textwidth}
\caption{Detection performance for multiple queries per canary document multiple times. Specifically, we evaluate the detection performance by varying the number of inserted canary documents while querying each canary document 14 times. }
\begin{tabular}{cccc}
\toprule
Canary Number & ROC-AUC & TPR@1\%FPR & TPR@10\%FPR \\ \midrule
1                      &$0.998$         & $0.966$           &$0.998$             \\
2                      &$0.999$         & $0.988$           & $1.000$            \\
3                      & $0.999$        & $0.996$           & $1.000$            \\
4                      & $0.999$       &  $0.998$          & $1.000$            \\
5                      & $\bm{1.000}$        & $\bm{1.000}$           & $\bm{1.000}$            \\ \bottomrule       
\end{tabular}
\label{tab:multiple_query}
\end{table}

% \lyp{synthetic example/real example for comparison}
% \lyp{prompt}

\section{Prompts used in Canary Dataset Synthesis Algorithm}
\label{sec:concrete_prompt}

\begin{tcolorbox}[title=Attributes Extraction Prompt.]
\#\#\# Task Description:

A reference text is given. You will carefully analyze the reference text and identify the following four key attributes.

1. Topic: Read the reference text and provide a high-level theme or general category of the reference text.

2. Subtopics: Based on the general topic, identify \{$n$\} distinct general sub-category.

3. Writing Style: Analyze the overall writing style of the reference text.

4. Length Range: Provide an estimate of the length range of the reference text in terms of word count.

\vspace{1em}

\#\#\# Output Format Requirements:

Output the results with the JSON format (with four keys: topic, subtopics, writing\_styles and length\_range) and nothing else, such as 
% \{``topic": ``Exercise and Immune'', ``subtopics": [``Nutrition for Athletes", ``Supplementation and Immunity", ...], ``writing\_styles": ``Informative and scientific'', ``length\_range": ``200 - 300 words"\}.   
\{``topic": `` '', ``subtopics": [`` ", `` ", ...], ``writing\_styles": `` '', ``length\_range": ``m - n words"\}.   

\vspace{1em}

\#\#\# Reference Text:

\{$sampled\_text$\}  
\end{tcolorbox}

\begin{tcolorbox}[title=Fictional Entity Creation.]
\#\#\# Task Description:

1. Identify and list \{$n$\} important entities mentioned within the reference text.

2. Synthesize \{$m$\} fictional entities that align with the \{$sub\_topic$\} topic.

\vspace{1em}

\#\#\# Synthesized Entities Requirements:

1. The synthesized entities should be creative and distinct.

2. Ensure the synthesized entities are fictional and do not overlap with real-world entities.

\vspace{1em}

\#\#\# Output Format Requirements:

Output the results with the JSON format (with two keys: real\_entity and fictional\_entity) and nothing else, such as \{``real\_entity": [``real\_entity\_1'', ``real\_entity\_2"] ``fictional\_entity": [``fictional\_entity\_1", ``fictional\_entity\_2", ...]\}.   

\vspace{1em}

\#\#\# Reference Text:

\{$sampled\_text$\} 
\end{tcolorbox}




\begin{tcolorbox}[title=Description Synthesis.]
\#\#\# Task Description:

1. Write \{$n$\} fictional descriptions in an \{$writing\_style$\} style about the following entities: \{$fictional\_entity\_1$\}, \{$fictional\_entity\_2$\}.

2. Create \{$m$\} fictional interactions and discuss how those specified entities fictionally interact within the context of the \{$sub\_topic$\} topic.

\vspace{1em}

\#\#\# Synthesized Description Requirements:

1. Create unique and imaginative content that has not been derived from existing material to avoid any issues with plagiarism.

2. Use creativity to simulate realistic scenarios that fit within the projects thematic boundaries.

3. Ensure factual accuracy where applicable, even in synthetic scenarios.

4. Incorporate diverse and inclusive content.

5. Do not mention ``fictional" or any other indication that the entity or interaction is not real.

\vspace{1em}

\#\#\# Output Format Requirements:

Output the results with the JSON format, such as \{``description\_1": `` ", ``description\_2": `` "\}.   

\end{tcolorbox}




\begin{tcolorbox}[title=Article Synthesis.]
\#\#\# Task Description:

You are a content creator. You will be given some reference descriptions.  You will carefully understand the reference descriptions and synthesize a text that satisfies the following instructions.

1. Generate a fictional text in the style of \{$writing\_styles$\} in the context of \{$sub\_topic$\} topic, with a length range of \{$length\_range$\} in terms of word count. 

2. Include the information in the given reference descriptions.

\vspace{1em}

\#\#\# Output Format Requirements:

Directly output the synthesized article in one paragraph and nothing else.

\vspace{1em}

\#\#\# Reference Descriptions:

\{$reference\_description$\}
\end{tcolorbox}




\begin{tcolorbox}[title=Query Question Generation. \cite{jovanovic2024ward}]
\#\#\# Task Description:

Given an article, generate a question that can only be answered by reading the document. The answer should be a longer detailed response, so avoid factual and simple yes/no questions and steer more towards questions that ask for opinions or explanations of events or topics described in the documents. Do not provide the answer, provide just the question.

\vspace{1em}

\#\#\# Article:

\{$canary\_document$\}
\end{tcolorbox}


\section{Example of Canary Document}
\label{appendix:canary_example}

\begin{center}
    \centering 
    \begin{longtable}{l|p{12cm}}
    \captionsetup{width=0.9\textwidth}
    \caption{An example of a canary document, its corresponding query question, and the RA-LLM's response. The tokens highlighted in green indicate those belonging to the green list.}
    \label{tab:canary_examples}  
    \setlength{\tabcolsep}{20pt}  \\
    \toprule
     \multirow{14}{*}{Canary Document} & The\hl{ Fl}avonoid\hl{ Research} Institute (F\hl{RI}) and the\hl{ Nut}ri\hl{Quest} Study\hl{ Group}\hl{ have}\hl{ collaborated} on\hl{ a}\hl{ landmark}\hl{ study}\hl{ to}\hl{ explore} the\hl{ relationship}\hl{ between} flavonoid\hl{ consumption} and cardiovascular health\hl{,}\hl{ leveraging}\hl{ data}\hl{ from} extensive\hl{ epidemi}ological studies\hl{ conducted} by\hl{ Nut}ri\hl{Quest}.\hl{ By}\hl{ combining}\hl{ F}\hl{RI}\hl{'s}\hl{ expertise}\hl{ in} flavonoid analysis and\hl{ bio}\hl{activity}\hl{ with}\hl{ Nut}ri\hl{Quest}\hl{'s}\hl{ large}-scale\hl{ research}\hl{ designs}\hl{,} the\hl{ partnership}\hl{ has}\hl{ provided}\hl{ a} comprehensive\hl{ understanding} of\hl{ how} different flavonoid\hl{ classes} impact\hl{ heart}\hl{ disease}\hl{ risk}. The\hl{ study}\hl{'s}\hl{ findings}\hl{,} which\hl{ have}\hl{ been} presented at\hl{ an} international\hl{ conference}\hl{,}\hl{ underscore} the\hl{ protective}\hl{ effects} of flavon\hl{oids} and\hl{ emphasize} the importance of\hl{ public}\hl{ awareness}\hl{ regarding} flavonoid-rich\hl{ foods}.\hl{ Not}ably\hl{,} this\hl{ collaboration}\hl{ highlights} the\hl{ significance} of\hl{ interdisciplinary}\hl{ research}\hl{,}\hl{ bringing} together\hl{ experts}\hl{ in}\hl{ epidemi}\hl{ology}\hl{,}\hl{ bi}ostat\hl{istics}\hl{,} and\hl{ nutrition}\hl{ science}\hl{ to}\hl{ tackle} the complex\hl{ inter}play\hl{ between}\hl{ diet}\hl{,} lifestyle\hl{,} and chronic diseases. The\hl{ F}\hl{RI}\hl{'s} mission\hl{ to}\hl{ promote}\hl{ awareness} of flavonoid-rich\hl{ foods}\hl{ align}\hl{s}\hl{ with}\hl{ Nut}ri\hl{Quest}\hl{'s}\hl{ emphasis} on\hl{ inclus}ivity\hl{ in}\hl{ research}\hl{,}\hl{ ensuring} that\hl{ findings} are representative and applicable\hl{ to} diverse demographic groups.\hl{ Overall}\hl{,} this\hl{ partnership}\hl{ has}\hl{ contributed}\hl{ significantly}\hl{ to} the advancement of\hl{ epidemi}ological\hl{ research}\hl{,}\hl{ ultimately}\hl{ informing} evidence-based\hl{ guidelines} for dietary\hl{ recommendations} that\hl{ can}\hl{ enhance}\hl{ public} health outcomes. \\ \midrule
     \multirow{3}{*}{Query Question}  & What key contributions can the collaboration between the Flavonoid Research Institute and the NutriQuest Study Group make to our understanding of cardiovascular health, and how might these findings influence public dietary recommendations? 
\\ \midrule
    \multirow{30}{*}{Response}  &The\hl{ collaboration}\hl{ between} the\hl{ Fl}avonoid\hl{ Research} Institute (F\hl{RI}) and the\hl{ Nut}ri\hl{Quest} Study\hl{ Group}\hl{ has} the\hl{ potential}\hl{ to} make\hl{ significant} contributions\hl{ to}\hl{ our}\hl{ understanding} of cardiovascular health\hl{,}\hl{ primarily} by\hl{ exploring} the\hl{ relationship}\hl{ between} flavonoid\hl{ consumption} and\hl{ heart}\hl{ disease}\hl{ risk}.\hl{ By}\hl{ combining}\hl{ F}\hl{RI}\hl{'s}\hl{ expertise}\hl{ in} flavonoid analysis and\hl{ bio}\hl{activity}\hl{ with}\hl{ Nut}ri\hl{Quest}\hl{'s}\hl{ large}-scale\hl{ research}\hl{ designs}\hl{,} the\hl{ partnership}\hl{ can} provide\hl{ a} comprehensive\hl{ understanding} of\hl{ how} different flavonoid\hl{ classes} impact\hl{ heart}\hl{ disease}\hl{ risk}.

The\hl{ study}\hl{'s}\hl{ findings}\hl{,} which\hl{ have}\hl{ been} presented at\hl{ an} international\hl{ conference}\hl{,}\hl{ underscore} the\hl{ protective}\hl{ effects} of flavon\hl{oids} and\hl{ emphasize} the importance of\hl{ public}\hl{ awareness}\hl{ regarding} flavonoid-rich\hl{ foods}. This\hl{ collaboration}\hl{ highlights} the\hl{ significance} of\hl{ interdisciplinary}\hl{ research}\hl{,}\hl{ bringing} together\hl{ experts}\hl{ in}\hl{ epidemi}\hl{ology}\hl{,}\hl{ bi}ostat\hl{istics}\hl{,} and\hl{ nutrition}\hl{ science}\hl{ to}\hl{ tackle} the complex\hl{ inter}play\hl{ between}\hl{ diet}\hl{,} lifestyle\hl{,} and chronic diseases.

The\hl{ findings} of this\hl{ collaboration}\hl{ can} influence\hl{ public} dietary\hl{ recommendations}\hl{ in} several ways. Firstly\hl{,} the\hl{ study}\hl{'s}\hl{ results}\hl{ can}\hl{ inform} evidence-based\hl{ guidelines} for dietary\hl{ recommendations} that\hl{ can}\hl{ enhance}\hl{ public} health outcomes. The\hl{ F}\hl{RI}\hl{'s} mission\hl{ to}\hl{ promote}\hl{ awareness} of flavonoid-rich\hl{ foods}\hl{ align}\hl{s}\hl{ with}\hl{ Nut}ri\hl{Quest}\hl{'s}\hl{ emphasis} on\hl{ inclus}ivity\hl{ in}\hl{ research}\hl{,}\hl{ ensuring} that\hl{ findings} are representative and applicable\hl{ to} diverse demographic groups.

Second\hl{ly}\hl{,} the\hl{ collaboration}\hl{ can}\hl{ raise}\hl{ public}\hl{ awareness} about the importance of flavonoid-rich\hl{ foods}\hl{ in} reducing the\hl{ risk} of\hl{ heart}\hl{ disease}.\hl{ By}\hl{ disse}minating the critical\hl{ role} of flavon\hl{oids}\hl{ in}\hl{ mitig}\hl{ating}\hl{ heart}\hl{ disease}\hl{ risk}\hl{,} the\hl{ partnership}\hl{ can}\hl{ encourage}\hl{ individuals}\hl{ to} prioritize\hl{ heart} health by incorporating flavonoid-rich\hl{ foods}\hl{ into} their diets.

Lastly\hl{,} the\hl{ study}\hl{'s}\hl{ findings}\hl{ can}\hl{ inform} the\hl{ development} of nutritional\hl{ supplements} and\hl{ products} that cater\hl{ to}\hl{ individuals} at\hl{ risk} of cardiovascular diseases. The\hl{ collaboration}\hl{ can}\hl{ also}\hl{ address}\hl{ inclus}ivity\hl{,}\hl{ ensuring} accessibility\hl{ across}\hl{ various} demographics\hl{,} including\hl{ those}\hl{ with} dietary restrictions.

Overall\hl{,} the\hl{ collaboration}\hl{ between} the\hl{ Fl}avonoid\hl{ Research} Institute and the\hl{ Nut}ri\hl{Quest} Study\hl{ Group}\hl{ has} the\hl{ potential}\hl{ to} make\hl{ significant} contributions\hl{ to}\hl{ our}\hl{ understanding} of cardiovascular health\hl{,} and\hl{ its}\hl{ findings}\hl{ can} influence\hl{ public} dietary\hl{ recommendations} by promoting\hl{ awareness} of flavonoid-rich\hl{ foods}\hl{,}\hl{ informing} evidence-based\hl{ guidelines}\hl{,} and encouraging\hl{ individuals}\hl{ to} prioritize\hl{ heart} health.   \\ \bottomrule
    
    \end{longtable}
\end{center}


\begin{center}
    \centering 
    \begin{longtable}{l|p{12cm}}
    \captionsetup{width=0.9\textwidth}
    \caption{An example of a canary dataset synthesis algorithm.}
    \label{tab:canary_algorithm_examples}  
    \setlength{\tabcolsep}{20pt}  \\
    \toprule
     \multirow{10}{*}{Sampled Document} & 2,4-Dinitrophenol (DNP) is reported to cause rapid loss of weight, but unfortunately is associated with an unacceptably high rate of significant adverse effects. DNP is sold mostly over the internet under a number of different names as a weight loss/slimming aid. It causes uncoupling of oxidative phosphorylation; the classic symptom complex associated with toxicity of phenol-based products such as DNP is a combination of hyperthermia, tachycardia, diaphoresis and tachypnoea, eventually leading to death. Fatalities related to exposure to DNP have been reported since the turn of the twentieth century. To date, there have been 62 published deaths in the medical literature attributed to DNP. In this review, we will describe the pattern and pathophysiology of DNP toxicity and summarise the previous fatalities associated with exposure to DNP. \\ \midrule
     \multirow{1}{*}{Sub-topic}  & DNP Toxicity
\\ \midrule
    \multirow{1}{*}{Writing Style}  & Academic and Informative  \\  \midrule
    \multirow{1}{*}{Length Range} & 150 - 200 words \\ \midrule
    \multirow{1}{*}{Synthetic Entities} & SlimSafe Elixir, MetaboliQ \\ \midrule
    \multirow{25}{*}{Descriptions} & Description 1: MetaboliQ is an innovative dietary supplement designed to enhance metabolic efficiency through a unique blend of botanical extracts and proprietary compounds. Its formulation includes ingredients known for their thermogenic properties, aiming to promote fat oxidation and energy expenditure. Clinical studies suggest that MetaboliQ may support weight management when combined with a balanced diet and regular exercise, although users are advised to approach its use with caution, particularly in light of the serious health risks associated with similar substances like 2,4-Dinitrophenol (DNP).
    \vspace{0.5em}
    
    Description 2: SlimSafe Elixir is marketed as a holistic weight loss solution, combining traditional herbal remedies with modern nutritional science. The elixir is formulated to assist in appetite regulation and promote a sense of satiety, utilizing a blend of adaptogenic herbs that are believed to balance hormonal responses related to hunger. While SlimSafe Elixir claims to offer a safer alternative to synthetic weight loss agents, it is essential for consumers to remain informed about the potential dangers of unregulated weight loss products, especially in the context of substances like DNP that have been linked to severe adverse effects.
    \vspace{0.5em}
    
    Description 3: In a recent study examining the interactions between various weight loss agents, researchers observed that the use of MetaboliQ alongside SlimSafe Elixir could potentially amplify the effects of both supplements. Participants reported increased energy levels and improved metabolic rates; however, the study also highlighted concerns regarding the cumulative impact of these products on cardiovascular health. Given the toxicological profile of DNP, it is crucial that individuals considering such combinations remain vigilant and consult healthcare professionals to mitigate risks associated with excessive stimulation of metabolic pathways.
    \\ \midrule

    \multirow{13}{*}{Synthetic Article} & The growing awareness of the dangers of 2,4-Dinitrophenol (DNP) has prompted a closer examination of dietary supplements that aim to enhance metabolic efficiency, such as MetaboliQ and SlimSafe Elixir. MetaboliQ, with its proprietary blend of botanical extracts, has been shown in clinical studies to support weight management when used in conjunction with a balanced diet and regular exercise, though caution is advised due to its potential thermogenic effects. SlimSafe Elixir, marketed as a holistic weight loss solution, utilizes adaptogenic herbs to regulate appetite and promote satiety, positioning itself as a safer alternative to synthetic agents like DNP, which have been linked to severe adverse effects. However, a recent study suggesting the potential for amplified effects when combining MetaboliQ with SlimSafe Elixir also highlights concerns regarding cumulative impacts on cardiovascular health, echoing the importance of vigilance and professional consultation to mitigate risks, particularly in light of DNP's toxicological profile. \\ \midrule

    \multirow{4}{*}{Query Question} & What are the potential risks and benefits of using MetaboliQ and SlimSafe Elixir for weight management, especially in light of the dangers associated with 2,4-Dinitrophenol (DNP), and how do these products compare to synthetic agents in terms of safety and effectiveness?\\ \bottomrule
    
    \end{longtable}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
