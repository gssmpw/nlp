\section{Related Work}


\paragraph{LLM Evaluation with Benchmarks} The evaluation of LLMs has traditionally relied on static benchmarks, from early benchmarks for perplexity-based  evaluation \cite{marcus-etal-1993-building} to datasets focused on specific downstream tasks such as question answering \cite{kwiatkowski2019natural}, summarization \cite{ Gliwa_2019}, math reasoning \cite{cobbe2021gsm8k}, and code generation \cite{chen2021codex}. As LLMs began to address a broader range of tasks across various domains \cite{wu2023autogenenablingnextgenllm, wang2023voyager}, more comprehensive benchmark suites \cite{hendrycks2021measuring, zhong2023agieval} were developed to assess general capabilities rather than individual task performance. Recent advancements in LLM evaluation have introduced the concept of LLM-as-a-judge, enabling the use of open-ended benchmarks without predefined answers \cite{zheng2023judging}. However, these benchmarks remain static in nature and can easily get overfit. Recently, platforms like ChatBot Arena \cite{zheng2023judging} utilize crowdsourcing to rank LLM responses and provides more dynamic evaluations. However, its reliance on human annotation makes it less scalable. Moreover, despite their utility, existing benchmarks primarily assess downstream applications that usually require multiple capabilities, making it difficult to debug and understand model weaknesses.


\paragraph{Tests and Benchmarks for Evaluating Context Utilization} As LLMs become capable of processing increasingly long inputs, designing automated tests to evaluate their ability to utilize context has become an area of active research.  A notable example is the needle-in-a-haystack (NIAH) task\footnote{\url{https://github.com/gkamradt/LLMTest_NeedleInAHaystack}}, where a small piece of information (the ``needle") is hidden within a long document, and the model needs to retrieve it. Similar tests include key-value retrieval \cite{liu-etal-2024-lost} and passkey retrieval \cite{mohtashami2023randomaccess}. The simplicity and interpretability of NIAH have made it a standard for evaluating LLM context utilization, and it has since inspired various methods for improving long-context processing \cite{mohtashami2023randomaccess, ding2024longrope, xiong2023effectivelongcontext, behrouz2024titans}.

 However, these tests focus solely on basic information retrieval, without capturing more complex aspects of context processing. To address this limitation, other tests have been proposed. Needlebench \cite{ li2024needlebench} extends simple retrieval tasks to include multi-needle reasoning and a ancestral trace task which requires navigating chains or graphs of information. \citet{song2024countingstars} introduce the Counting Stars task, which involves tallying numbers of stars embedded in phrases. Ruler \cite{hsieh2024ruler} proposes additional tasks such as variable tracking and frequent word extraction. While these tests increase task complexity or broaden the range of evaluated tasks, they remain limited in scope for systematically evaluating contextual processing.
 
Beyond individual tests, several benchmarks explicitly target long-context processing, including InftyBench \cite{zhang-etal-2024-bench}, L-Eval \cite{an-etal-2024-l}, and LongBench \cite{bai-etal-2024-longbench}. These benchmarks use NIAH-like tasks alongside question answering, summarization, and code generation over long contexts. However, like other benchmarks, they remain static and primarily measure end-to-end performance rather than systematically dissecting capabilities. 


\paragraph{Analogies to Human Cognitive Testing}
Memory tests are widely used in cognitive research to assess specific functions. Such assessments often involve evaluating short term memory via recall tests \cite{crannell1957comparison, towse2008recall}, inductive reasoning via pattern recognition tasks, or attention via instruction-following\cite{kane2007identifying, nasreddine2005montreal}. By isolating distinct abilities while minimizing confounding factors like attention, memory span, and reasoning \cite{kane2007identifying}, such tests provide detailed profiles of cognitive functions, guiding interventions and shaping broader theories of human thought. Inspired by this approach, we design atomic tests that systematically isolate core aspects of LLM context processing, aiming for a fine-grained understanding of LLM memory-usage capabilities -- analogous to memory testing in humans.


