\section{Introduction}

What capabilities should we expect from AI assistants? The AI assistants are provided with a (large) input context containing all the \textit{available} information that is potentially relevant to the user's request (e.g., all the prior emails, messages, and confirmed calendar events). This input, commonly referred to as the context (for the LLM), encapsulates what the AI assistant \textit{knows} about the world in which it is tasked to operate. This representation of the world, expressed in natural language, functions as the model's \textbf{memory}. In this paper, we address a fundamental question that is critical to improving AI assistants: \textit{What specific capabilities do large language models demonstrate in utilizing their memory?} 

One common approach to testing models is through data benchmarks. However, evaluating model capabilities using static data benchmarks--based on some user queries, their data, and expected outcomes--can be costly, imprecise, and lacks scalability. Additionally, performing tasks in realistic scenarios often requires multiple capabilities, making it challenging to identify which specific capability a model lacks when it fails a data benchmark. This limitation reduces the effectiveness of these benchmarks in designing better models. Moreover, blindly optimizing models to improve on such benchmarks risks overfitting, often rendering the data benchmarks obsolete over time.

To address some of these concerns, recently, there have been many attempts to test models using automatically generated benchmarks, but these efforts have primarily focused on evaluating basic search capabilities (e.g., passkey or key-value search) in long-contexts~\cite{kamradt2023, liu-etal-2024-lost, zhang-etal-2024-bench, anthropic2024b, wu2024longgenbench, li2024needlebench,hsieh2024ruler}. In this paper, we go beyond simple search tasks and introduce a framework to test a comprehensive range of memory-related capabilities in LLMs. Note that, we deliberately avoid conflating the \textit{memory usage} capabilities with the \textit{complex reasoning} abilities (e.g., complex mathematical or logical reasoning) in language models, as the latter is a separate skill that is currently the focus of extensive study~\cite{clark2018thinksolvedquestionanswering, cobbe2021gsm8k, hendrycksmath2021, suzgun2022challenging}.

\textit{What are memory-usage capabilities?} We define these as the abilities to retrieve relevant information, compose it for the instructed task, and recall key details when synthesizing the output. This process also involves creating associations between the instruction and the stored information, as well as among different parts of the memory itself. Without extracting these relationships, the memory remains flat and formless, rendering it less useful. Consequently, the model must be able to recognize differences, identify similarities, and take appropriate actions based on them. We design a series of \textit{atomic} tests aimed at evaluating each of these individual capabilities in isolation, to the extent that isolating such capabilities is possible.

To evaluate the more complex scenarios in memory usage, we construct composite tests that reflect real-world scenarios, where memory is divided into multiple compartments (i.e., information relevant to distinct contexts). The model is expected to recognize the boundaries of these compartments, trace the knowledge contained within them, and perform operations such as information retrieval, memory association, and other tasks while respecting these boundaries. The complexity increases further when there is \textit{interaction} between compartments. In such cases, information must flow across boundaries--for example, when stories about two parallel events converge at a particular moment, when interactions occur between different events, or when information known to certain entities in memory is shared with others. Examples include AI assistants managing calendar events, tracking financial transactions, or suggesting medical diagnoses. Handling these scenarios is highly challenging, yet it is essential for AI assistants to achieve practical and reliable performance. The core challenge is due to the fact that context memory, as provided to the model, flattens data from multiple parallel and potentially interrelated memory compartments. This requires the model to disentangle the content by leveraging the available labels and clues, and tracking the state of relevant information throughout the memory scanning, while performing the task.

Table~\ref{tab:memory_tests} presents an overview of the types of memory tests included in our benchmark. For each test, we use efficient parametric programs to generate randomized test cases. We run a comprehensive evaluation of several major open-source and black-box models (e.g., GPT-4(o), Cohere, Gemma, LLaMA, Mistral, Phi). Our experimental results show that while models perform relatively well on simple search tasks, they exhibit significant disparities across context utilization capabilities even at a context length of 4k tokens. This indicates that strong performance in basic retrieval does not necessarily translate to other context processing abilities. Our framework goes beyond search-based tests by incorporating atomic tests that pinpoint distinct capabilities, providing a more nuanced picture of the strengths and weaknesses of models in context processing. Moreover, composite tests, which combine multiple atomic capabilities, resulted in substantial performance drops for all models. These tests present the limitations of current models and provide valuable insights for guiding future model training and development.

The code and data will be publicly available at a later date.


