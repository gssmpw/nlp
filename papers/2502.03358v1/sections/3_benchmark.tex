\section{Benchmark for Memory Tests}

We define the entirety of context data available to a large language model (LLM) as its \textit{memory}. Users of an AI assistant leveraging the LLM can instruct the model to parse this memory and execute potentially complex retrieval tasks. Accordingly, we expect the AI assistant to demonstrate specific capabilities in memory utilization, including accurate retrieval, effective synthesis of relevant information, and adaptability to evolving context:
%
\begin{itemize}
    \item \textbf{Information Retrieval and Localization}: The ability to efficiently locate, search for, and extract relevant information from the memory (i.e., input data) based on specific instructions or user queries.
    %
    \item \textbf{Processing and Basic Reasoning}: The capability to perform modifications, computations, and logical operations on the input data, including identifying patterns, recognizing repetitions, and understanding relationships within the memory.
    %
    \item \textbf{Content Transfer and Synthesis}: The ability to copy, rephrase, or generate synthesized output by integrating both original and modified elements from the input.
    %
    \item \textbf{Structural Awareness and Organization}: The capacity to interpret the spatial, structural, or organizational layout of the memory, such as distinguishing labeled fragments of text, sets, lists, or hierarchical structures. 
\end{itemize}

In our benchmark, we focus on isolating the \textit{atomic} memory-related capabilities of LLMs. Success or failure in these tests provides a clear and interpretable assessment of the strengths and limitations of the models. We design multiple \textbf{atomic tests} to measure fundamental skills without interference from other factors. These tests are simple, targeted, and structured to assess specific abilities with clarity. Our benchmark includes the existing basic tests, notably the needle-in-the-haystack tests and its variations, but it goes beyond the search methods and includes a diverse set of atomic capabilities. In our benchmark, we focus on some fundamental capabilities: \textit{search, recall and edit, match and compare, spot the differences, compute on sets and lists, and stateful processing}. 

We also develop \textbf{composite tests} to evaluate how effectively models can perform more complex, integrated tasks. These tests assess the integration of multiple atomic capabilities to simulate real-world scenarios. By combining elements such as retrieval, reasoning, synthesis, and structural awareness, composite tests measure how well an AI assistant can coordinate different skills to execute complex operations. We provide two class of composite tests: processing data blocks, and composite-state tracking (theory of mind). Our objective here is to test the composition of various atomic operations and the ability to interpret segments of data (e.g., messages or paragraphs associated with a particular person or topic). These composite tests evaluate if the model can make sense of the ``spatial'' structure of the memory, and also keep track of its ``temporal'' changes (e.g., information that gets updated across many emails). This can become particularly challenging for the current architecture of major LLMs because the context has a flat structure. 

%%%
\input{tables/benchmark}
%%%
\input{figures/radar}
Table~\ref{tab:memory_tests} presents the list and description of representative tests in each category. Appendix~\ref{apd:prompt_example} presents the exact templates for all our tests. Our benchmark differs from traditional data benchmarks by allowing the generation of fresh, randomized test cases for each category. Each test acts as a \textit{programmable} script that measures the modelâ€™s capability while adjusting the hyperparameters that influence test difficulty. The programmable tests also enable us composing them easily, which is one of the key advantages of our benchmark. New categories, and new tests, can be easily added to this framework enabling a more diverse set of tests. 


%auxiliary memory which transformers don't have

% \section{Atomic Tests?/Capability Tests?/Capability Quotient Tests?/Unit Tests?}

% \subsection{Overview}

% The evaluation of large language models (LLMs) has traditionally relied on static benchmarks, which focus on assessing task-specific performance over fixed datasets. While these benchmarks provide useful insights, they often fail to thoroughly evaluate the nuanced abilities of LLMs to process and utilize their context. In this paper, we propose an alternative approach that emphasizes evaluating the contextual processing capabilities of LLMs.

% Inspired by principles from human cognitive testing, we first identify core properties that LLMs must exhibit to effectively handle context. Building on these properties, we design a suite of atomic tests, along with efficient algorithms for generating test samples, to rigorously evaluate LLM performance. Unlike downstream task-specific evaluations, our approach isolates fundamental skills, providing a fine-grained analysis of LLM capabilities.

% To evaluate the contextual processing ability of LLMs, we define five key properties that encapsulate the necessary skills for handling context:

% \paragraph{Information Fetching} This property refers to the ability of the model to identify and retrieve relevant information from the context. It is crucial for tasks requiring selective information access, such as answering questions based on a given passage. For example, retrieving a specific fact from a context in question answering.

% \paragraph{Recall and Manipulation} The ability to recall information from the context and apply manipulations to them. This skill underpins many practical tasks, such as text editing.

% \paragraph{Combining and Linking Information} This property evaluates the model's ability to synthesize multiple pieces of information within a context, such as identifying relationships, make inferences, and derive conclusions by combining contextual elements. Example tasks include multi-step reasoning problems or deriving summaries that connect scattered ideas.

% \paragraph{Recognizing Partition} Real-world contexts often have inherent structures, such as sections, paragraphs, or conversational turns. The ability to recognize and leverage these partitions, instead of treating the input as a flat sequence of tokens, is critical. For instance, in a multi-turn conversation, models should differentiate between user and assistant turns and respond appropriately based on context boundaries.

% \paragraph{State Tracking} This property evaluates whether the model can maintain and update an internal representation of the context as it evolves. It is essential for tasks that involve dynamic interactions, such as maintaining consistent behavior in long conversations or tracking changes in inventory.

% To evaluate these properties, we designed \textit{atomic tests} -- small, abstract tasks that isolate and target specific skills. Each atomic test is tailored to a single property, ensuring clarity in assessing the model's strengths and weaknesses. For example:
% An atomic test for Information Fetching might involve extracting a specific word from a list of paired words. A test for State Tracking could involve a series of commands that incrementally alter a state, requiring the model to maintain consistency.

% The tests are programmatically generated using efficient algorithms, allowing for precise control over the test parameters. This flexibility enables dynamic generation of samples, preventing overfitting and allowing adjustments in difficulty to match future advancements in model capabilities. Additionally, the programmatic approach ensures scalability, enabling large-scale evaluation across diverse configurations.

% By systematically evaluating LLMs using these atomic tests, we can generate detailed capability profiles. These profiles highlight the strengths and limitations of different models, offering insights beyond what static benchmarks provide. Such evaluations can help identify specific areas where models excel or struggle, guiding targeted improvements in training and architecture. These capability profiles also enable informed model selection for specific applications. 

% \subsection{Core Capability Properties and Tests}
% \subsubsection{Information Fetching}
% Information fetching underpins the most basic interactions with context. It evaluates the model's ability to locate and retrieve relevant pieces of information from a potentially noisy or distractor-filled context. This skill is essential for tasks like fact-checking, question answering, and data extraction. We designed the following tests:

% \textbf{Word Presence}: The test provides a list of words in the context and asks whether a query word is present or absent. All words are sampled uniquely from a dictionary with no duplicates.

% \textbf{Subsequence Presence}: Similar to Word Presence but requires determining whether a specific sequence of words appears in the context. This adds complexity by introducing similar subsequence of words to the context.

% \textbf{Paired Associate Retrieval}: The context contains a list of paired words (e.g., "sky:apple"). The test asks for the associated word given a query word.

% \textbf{Multi-Paired Associate Retrieval}: This tests extends Paired Associate Retrieval by requiring the retrieval of associated words for multiple query words, testing retrieval under increased load.

% \subsubsection{Recall and Manipulation}

% This property evaluates whether the model can recall the context and apply systematic manipulations. These skills are essential for transforming input data or performing edits.

% \textbf{Verbatim Recall}: The context consists of a list of words or numbers, and the task is to repeat them verbatim. Words are sampled uniformly from a dictionary, and numbers are generated between 1 and 999.

% \textbf{Replace Item}: The task involves repeating the list of words but replacing a specified word with another word every time it appears.

% \textbf{Replace Nth}: Similar to Replace Item but replaces every Nth word in the list with another word, introducing periodic transformations.

% \textbf{Skip Item}: Similar to Replace Item but requires skipping the specified word instead of replacing it.

% \textbf{Skip Nth}: Skip every nth word in the list.

% \textbf{Simple Compute}: The context contains a list of numbers, and the test requires applying a specified arithmetic operation (e.g., adding 3 or subtracting 1) to each number. For example, if the input is 1,2,3, and the instruction is to add 3, the model should output 4,5,6.

% \subsubsection{Combining and Linking Information}
% This property focuses on combining and linking elements in the context, which forms the basis for pattern recognition, relationship inference, and reasoning.

% \textbf{Relative Position}: The context contains a list of unique words. The task is to determine if one query word appears before or after another query word.

% \textbf{Detect Repetition}: The task involves identifying whether any word in a list of unique words is repeated.

% \textbf{Count Repetition}: The context specifies a target word, and the task is to count the number of times it appears in the list.

% \textbf{Attribute Match}: Each word in the context is paired with an attribute (e.g., "apple:ATT\_1," "banana:ATT\_2"). The task is to determine whether two specified words share the same attribute.

% \textbf{Difference Detection}: The test presents two lists of words that are mostly identical but differ in a few words. The task is to identify the differing words in a specified list.

% \textbf{Pattern Continuation}: The context consists of a sequence of words following a specific pattern. The task is to predict the nth word following the last word in the sequence.

% \textbf{Anomaly Detection}: The context contains several lists of words. Most lists are shuffled versions of the same set, but one contains anomalies. The task is to identify the anomalous list.


% \subsubsection{Recognizing Partition}

% Recognizing partition evaluates whether the model understand segments and partitions in the context.

% \textbf{N List Membership}: The context contains several lists of words, each unique. The task is to determine which list contains a specific query word.

% \textbf{N List Boundary}: Given several lists of words in the context, he task is to identify the first or last word in each list.

% \textbf{N List Match}: The test involves determining whether two given words belong to the same list in the context.

% \textbf{Role Segment Match}: The context alternates between roles (e.g., "User: ..., Assistant: ... User: ...") and their respective lists of words. The task is to determine if two words belong to the same role.

% \subsubsection{State Tracking}
% State tracking assesses the model's ability to maintain and update an internal representation of a dynamic context, which is essential for tasks involving sequential or cumulative operations.

% \textbf{Numerical State Tracking}: The context specifies an initial number and a series of operations (e.g., "Start: 5, Add 3, Subtract 2"). The task is to compute the final number.

% \textbf{Word List Tracking}: The context provides an initial list of words and a series of operations (e.g. add or remove words). The task is to determine the final state of the list after all operations.

% \subsection{Test Sample Construction and Evaluation}

% We developed custom programs to generate test samples for each of the evaluation tests. These programs enable the automatic and efficient creation of new test samples along with their reference answers. For instance, in tests where the context consists of lists of unique words, we can easily sample new contexts from a dictionary to craft new test samples. The reference answers for these tests can be efficiently generated using program code within polynomial time, allowing for dynamic regeneration of the tests for future use. 

% Additionally, these programs allow us to set various hyperparameters for each test, giving us full control over the difficulty level. Specifically, we configure the length of the context to be a fixed number, 4k. Apart from the length, each test may have its own unique hyperparameters. For example, in the Count Repetition test, the number of repetitions is a configurable hyperparameter. A comprehensive list of hyperparameters for each test is provided in the appendix.

% % TODO: add pseudo code

% To evaluate the accuracy of the tests, we default to exact match accuracy for binary classification tasks, such as the Word Presence test. For tests that require measuring sequence overlap between the answer and the reference (e.g. Verbatim Recall), we use ROUGE-L \cite{}. 