\section{Conclusions}

AI assistants powered by LLMs are expected to handle numerous operations involving memory. However, simple tests reveal that they often fall short of meeting user expectations, even in basic retrieval and processing tasks. For instance, retrieving vacation schedules for each team member from a message history that includes evolving plans over time proves challenging. To enable targeted improvements, it is essential to establish a comprehensive benchmark that tests each capability in isolation while also allowing for programmable composition to evaluate more complex scenarios. Our benchmark provides a straightforward yet effective approach to achieving this goal. We primarily focus on short-context scenarios to demonstrate that current limitations are not solely attributable to the models' challenges with parsing long contexts. Addressing these issues demands attention beyond merely solving the ``attention'' problem.