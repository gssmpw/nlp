

\begin{table}[t]
  % \label{acceleration}
\begin{subtable}{.5\linewidth}
  \resizebox{\textwidth}{!}{
  \begin{tabular}{lccc}
    \toprule
    \cmidrule(r){1-5}
%   \textbf{SMC'}  & \textbf{SMC+}    &  \textbf{SNL'} & \textbf{SNL+} & \textbf{SRE} \\
  & {LayerSkip}  & {Medusa}    &  {Ours} \\
    \midrule
  acceleration $(\uparrow)$ & $1.94\times$ & $\mathbf{3.76}\times$ & $3.71\times$  \\
  token acc rate $(\uparrow)$ & $75.1\%$ & $73.8\%$ & $74.4\%$ \\
  +memory $(\downarrow)$ & 4.38\% & 12.4\% & 8.51\%  \\
    \bottomrule
  \end{tabular}
  }
   \caption{\textbf{SQL-context}}
 \end{subtable}
\begin{subtable}{.5\linewidth}
  \resizebox{\textwidth}{!}{
  \begin{tabular}{lccc}
    \toprule
    \cmidrule(r){1-5}
%   \textbf{SMC'}  & \textbf{SMC+}    &  \textbf{SNL'} & \textbf{SNL+} & \textbf{SRE} \\
  & {LayerSkip}  & {Medusa}    &  {Ours}   \\
    \midrule
  acceleration $(\uparrow)$ & $1.32\times$ & $1.71\times$ & $\text{2.02}\times$  \\
  token acc rate $(\uparrow)$ & $45.1\%$ & $43.6\%$ & $51.2\%$  \\
  +memory $(\downarrow)$ & $2.29\%$ & $4.45\%$ & $3.09\%$  \\
    \bottomrule
  \end{tabular}
  }
   \caption{\textbf{SAMSUM}}
 \end{subtable}
\begin{subtable}{.5\linewidth}
%\begin{minipage}{.5\linewidth}
  \resizebox{\textwidth}{!}{
  \begin{tabular}{lccc}
    \toprule
    \cmidrule(r){1-5}
%   \textbf{SMC'}  & \textbf{SMC+}    &  \textbf{SNL'} & \textbf{SNL+} & \textbf{SRE} \\
  & {LayerSkip}  & {Medusa}    &  {Ours}  \\
    \midrule
  acceleration $(\uparrow)$ & $1.51\times$ & $2.55\times$ & $\mathbf{2.63}\times$  \\
  token acc rate $(\uparrow)$ & $57.8\%$ & $63.6\%$ & $65.4\%$  \\
  +memory $(\downarrow)$ & $1.33\%$ & $5.12\%$ & $2.21\%$  \\
    \bottomrule
  \end{tabular}
  }
   \caption{\textbf{GSM8K}}
 %\end{minipage}
\end{subtable}
 %\begin{minipage}{.5\linewidth}
\begin{subtable}{.5\linewidth}
  \resizebox{\textwidth}{!}{
  \begin{tabular}{lccc}
    \toprule
    \cmidrule(r){1-4}
%   \textbf{SMC'}  & \textbf{SMC+}    &  \textbf{SNL'} & \textbf{SNL+} & \textbf{SRE} \\
  & {LayerSkip}  & {Medusa}    &  {Ours}  \\
    \midrule
  acceleration $(\uparrow)$ & $1.61\times$ & $1.78\times$ & $\mathbf{2.53}\times$  \\
  token acc rate $(\uparrow)$ & $60.1\%$ & $48.3\%$ & $62.4\%$ \\
  +memory $(\downarrow)$ & $1.04\%$ & $3.18\%$ & $1.69\%$ \\
    \bottomrule
  \end{tabular}
  }
   \caption{\textbf{ChatDoctor}}
 %\end{minipage}
\end{subtable}
\caption{The performance of different (self-)speculative decoding algorithms. \emph{Skip}: the skip-layer decoding method by \citep{elhoushi-etal-2024-layerskip}. \emph{Block}: the block autoregressive decoding method by \citep{cai2024medusa}. This method is also known as Medusa. \emph{Semi}: the proposed semi-autoregressive decoding method (implemented with a simplified transformer). \emph{Redraft}: the method in \citep{zhang2024recurrent} which implements $h_q$ as a RNN. Our method offers highest acceleration on the majority of the tasks while requiring a reasonable memory cost. }
% \ankur{Caption should be more descriptive and should summarize the performance of our approach and baselines across the datasets.}} 
% \SAS{add bold to highlight the best result?}
\label{tab:main-comparison-table}
\end{table}
