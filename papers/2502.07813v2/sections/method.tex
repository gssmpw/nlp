\section{\ourmethod{}: Compositional Reasoning Evaluation Method}
\label{sec:methods}

\subsection{Overview}
\begin{figure*}[ht]
\centering
\vskip 0.2in
\includegraphics[width=\textwidth]{pics/Overview.pdf}
\caption{Overview of the \benchmark{} Construction Process. We apply instruction encryption and transformation to the tasks from common NLP benchmarks and combine them to construct our \benchmark{} Task. Then we use Exact Match, LLM as judge, UnitTest and AUC as our Evaluation Metrics to judge LLM's performance. }
\label{fig:overview}
\vskip -0.2in
\end{figure*}


\ourmethod{} presents a novel approach to evaluate LLMsâ€™ compositional reasoning ability by integrating existing benchmarks with cipher-based transformations.
The method first encodes specific words in prompts into new ciphered characters and explicitly includes encoding rules within instructions.
To further increase difficulty, some instructions require multi-step reasoning, guiding models to follow structured solution steps.
LLMs must first decode ciphered characters before answering original questions, ensuring a rigorous assessment of compositional reasoning.
Building upon this framework, we construct \benchmark{}, a benchmark set that systematically applies these principles for evaluation.

% \subsection{Benchmark Construction}
\subsection{Task Definition and Methodology}
\begin{algorithm}[h!]
\caption{Instruction Encryption}
\label{alg:encode_question}
\begin{algorithmic}
\STATE \textbf{Input:} Original question $Q=(w_1,\dots,w_n)$, where \( w_i \) are words split by spaces, encoding mapping table $\mathcal{M}$, and the number of encoded words $m$
\STATE \textbf{Output:} Encoded question $Q_{e}$
\STATE $selected\_words$ $\gets$ $[]$
\FOR{1 to $m$}
    \STATE $w_{r} \gets \text{RandomSelect}(Q)$
    \IF{$|w_{r}| > 1 \land w_{r} \notin selected\_words$}
        \STATE $w_{r} \gets \mathcal{M}(w_{r}) $
        \STATE $selected\_words.\text{append}(w_{r})$
    \ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}
Given a benchmark \( B = \{x_1, x_2, \dots, x_N\} \), where \( x_i \) is a prompt consisting of an instruction and a question \( Q \), \( B \) contains \( N \) data points.
To transform the existing prompt into a compositional reasoning prompt, we define a set of transformation rules, where each data point undergoes a series of transformations. 
Specifically, the transformation process can be described as:  

\begin{align}
x' = r_m \circ r_{m-1} \circ \dots \circ r_1(x),
\label{eq:composition}
\end{align}


where \( r_i \) denotes the \( i^{\text{th}} \) transformation rule, and \( \circ \) represents the composition of transformations.  
%The construction of \benchmark{} relies on two key transformation types: \textbf{instruction encryption}, which encodes parts of the prompt, and \textbf{instruction transformation}, which restructures it. 
Based on our formal definitions, we explore two implementation approaches of  \( r \) : \textbf{instruction encryption}, which encodes parts of the prompt, and \textbf{instruction transformation}, which restructures it. 
These transformations enable the creation of diverse compositional reasoning tasks.


\paragraph{Instruction Encryption}

Specifically, as shown in Figure \ref{fig:overview}, Instruction Encryption applies three encoding rules to the prompt, with its detailed process described in Algorithm \ref{alg:encode_question}.  
It allows encoding any number of words in the prompt, ensuring that the encoded prompts, generated by flexible self-defined rules, are unlikely to overlap with the pre-training corpus. For simplicity, we use emoji shuffle as an exemplifying encoding rule in the following sections and detailed encoding rules are explained in Appendix \ref{appendix: encoding rule}.
% For the original question, we use the function $S$ to determine whether $|w_{i}| > 1 \land w \in \mathcal{M}$. If $w_{i}$ meets the encoding criteria, $w_{i}$ will be encoded into $\mathcal{M}(w_{i})$ using the encoding mapping table. 
% Finally, we concatenate the encoded words to obtain the encoded question $Q_{e}$.
\nocite{langley00}
\paragraph{Instruction Transformation}

The vanilla approaches involve prompting the LLM to provide an answer in the format of ``Answer: A'' for the multi-choice question tasks. 
To further evaluate the compositional reasoning capabilities of LLMs in handling OOD scenarios, as shown in Figure \ref{fig:overview}, we establish instruction transformation to further increase the number of reasoning hops.
(1) \textbf{Numeric Transformation}: Based on $Q_{e}$, we perform numeric transformation. For example, mapping ``$A \rightarrow 1, B \rightarrow 2, C \rightarrow 3, D \rightarrow 4$'' will require the LLM to answer ``Answer: 1'' if the answer is ``Answer: A'', which forces the LLM to perform further reasoning after obtaining the original answer.
(2) \textbf{Alpha Transformation}: Additionally, the task can be made more complex by requiring the LLM to provide both the numerical answer and the first alphanumeric character of the corresponding answer content. For example, if the original answer is ``Answer: A'' and the answer content is ``Happiness'', the LLM would output ``Answer: 1 H''.

