\section{Experiment}
\label{sec:experiment}


\begin{table}[h]
    \centering
    \caption{\textbf{Statistics of our Benchmarks}: Avg. Len refers to the total number of characters in each question with 0-, 5-, or 10-word encoding numbers, respectively. Answer Format includes the following types: ME (Mathematical Expression), SC (Single Choice), CB (Code Blocks), TE (Textual Expression), and MC (Multiple Choices).}
    \label{tab:benchmark_statistics}
    \vskip 0.15in
    \resizebox{0.9\textwidth}{!}{
    \begin{tabular}{@{}lcccc@{}}
        \toprule
        \textbf{Category}   & \textbf{Total Nums} & \textbf{Avg. Len} & \textbf{Ans. Fmt}\\ 
        \midrule
        Crypto-Math    & 500 & 441.89 / 1410.86 / 1471.17 &ME\\ 
        Crypto-MMLU  & 285 & 627.97 / 1274.82 / 1333.6 &SC\\ 
        Crypto-MMLU-Num  & 285 & 699.97 / 1346.82 / 1405.6 &SC\\ 
        Crypto-MMLU-Alpha  & 285 & 922.97 / 1569.82 / 1628.6 &SC\\ 
        Crypto-MBPP      & 427 & 621.17 / 1268.54 / 1327.14 &CB\\ 
        Crypto-BBH        & 405 & 1585.3 / 3464.98 / 3517.24 &TE\&MC\\  
        Crypto-Needle-30K     & 100 & 64811.41 / 63535.62 / 62111.8&TE\\
        Crypto-HighResolution     & 497 & 1177.49 / 2360.61 / 2426.62 &ME\&SC\&TE\&MC\&CB\\
        \bottomrule
    \end{tabular}
    }
    \vskip -0.1in
\end{table}

\paragraph{LLMs}
We evaluate both open-source and closed-source models on \benchmark{}. For closed-source LLMs, we evaluate GPT series~\citep{gpt4} (GPT-4o), Claude series~\citep{claude35addendum}, and o1 series~\citep{openaiO1}, and the like. 
For open-source LLMs, we evaluate Qwen2.5 series~\citep{qwen2.5}, Llama-3.1 series~\citep{llama3}, Codestral~\citep{mistral} and Jamba-1.5-mini~\citep{Jamba}, and the like.
For all LLMs, we use temperature $T=0.7 \text{ to } 1.0$ and $\text{top}\ p=0.75 \text{ to } 1.0$.

\paragraph{Vanilla Benchmark}  
As shown in Table \ref{tab:benchmark_statistics}, we apply our method on five benchmarks: MATH~\citep{hendrycks2021measuring}, MMLU~\citep{MMLU}, BBH~\citep{suzgun2022challenging}, MBPP~\citep{austin2021program}, and Needle~\citep{Needle}.  
For MATH, we use the MATH 500 subset; for MMLU, we adopt MMLU-dev; for MBPP, we adopt MBPP-sanitized; and for Needle, we use the three-needle setting.  
Each benchmark is tested under three levels of Instruction Encryption: 0, 5, and 10, where 0 represents the original task without modification.  
Additionally, we construct a high-resolution (Applying Instruction Encryption with values ranging from 0 to 10) subset Crypto-HighResolution by sampling data from these benchmarks.  

\paragraph{Prompt Setting}
To systematically evaluate the models under various experimental settings, we design the following prompt templates (c.f., Appendix \ref{appendix: encoding rule}): (1) \textbf{Zero-shot Prompts} are applied on subsets including Crypto-Needle-30K, Crypto-Math, Crypto-MBPP, and Crypto-MMLU. (2) \textbf{Few-shot Prompts} are applied on the Crypto-BBH subset.

\paragraph{Evaluation Metrics}
(1) \textbf{Exact Match (EM)}: For simple answers (e.g., multiple-choice questions), we use regular expressions to extract the answer from the LLM's response and standardize its format. The standardized answer is then compared directly with the correct answer.  
(2) \textbf{LLM as judge}: For more complex answers (e.g., mathematical expressions), we employ the LLM-as-judge (Doubao-Pro-256K) to judge the answer against the correct answer. 
(3) \textbf{UnitTest}: For code-related questions, we evaluate responses using a test-case-based approach. 
The number of passed test cases is then counted, and the final score is calculated as:
score = (number of passed test cases) / (total number of test cases).
\paragraph{AUC of Compositional Reasoning}  
To better assess a model’s overall performance across different difficulty levels, we compute an area under the curve (AUC) score.  
After obtaining the evaluation metrics, we plot the number of encoded words \( k \) as the x-axis and the corresponding model performance as the y-axis.  
The AUC is then calculated using the trapezoidal rule:  

\begin{equation}
    \text{AUC} = \int_{k_{\min}}^{k_{\max}} f(k) \,dk \approx \sum_{i=1}^{N-1} (k_{i+1} - k_i) \frac{y_i + y_{i+1}}{2}
\end{equation}

where \( k_i \) represents the number of encoded words (Instruction Encryption level), and \( y_i \) is the corresponding model performance.  
A higher AUC indicates better compositional reasoning ability under varying levels of instruction encryption.



\subsection{Main Results}
% \subsubsection{The CR Ability of Models is Generally Lower than Their NCR Ability}
Table \ref{tab:common_test} presents the performance of diverse models on our \benchmark{} across various domains, revealing several key insights below. 

\paragraph{Current Models Demonstrate Limited CR Abilities} We find that as the number of encoded words increases, the reasoning difficulty also increases, leading to a decline in evaluation metrics across all models. 
As mentioned earlier, \benchmark{} forces models to conduct compositional reasoning, requiring them to decode the question before answering it. 
The increase in the number of encoded words corresponds to a higher likelihood of the models conducting compositional reasoning. 
The experimental results indicate that while most models perform poorly in compositional reasoning tasks (we analyze specific cases in the Appendix \ref{casestudy}), o1, o3-mini, and Gemini-2.0-Flash-Thinking-Exp-01-21 still demonstrate relatively strong performance. Open-source models are generally weaker than closed-source models, where open-source models exhibit a significant accuracy gap when answering questions compared to closed-source models. 


% This gap becomes even more pronounced when decoding and answering are combined. 
% As the encoding level increases, the accuracy of open-source models decreases at a much faster rate, while closed-source models remain relatively stable, without experiencing a sudden and sharp drop. 
% We hypothesize that, when confronted with a new encoding rule, open-source models struggle significantly with understanding the rule and decoding it. Furthermore, their poor compositional reasoning abilities are more evident when they are required to decode and answer the question, leading to a notable decline in performance.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[ht]
\renewcommand{\arraystretch}{1.1}
\setlength{\extrarowheight}{3pt} 
\centering
\caption{Performance of different models on \benchmark{}. 0 / 5 / 10 represents the number of words encoded. For Gemini-2.0-Flash-Thinking, we use a version of Exp-01-21. \colorbox{green!15}{\textcolor{black}{green}} represents the number with the highest accuracy, \colorbox{cyan!15}{\textcolor{black}{blue}} represents the number with the second highest accuracy, and \colorbox{orange!15}{\textcolor{black}{orange}} represents the value with the third highest accuracy.}
\label{tab:common_test}
\vskip 0.15in
\resizebox{\textwidth}{!}{
\LARGE
\begin{tabular}{lccccc ccccc ccccc ccccc ccc}
\toprule
\multicolumn{1}{c}{\multirow{2}*{\textbf{Model}}} & \multicolumn{1}{c}{\multirow{2}*{\textbf{AUC}}} & \multicolumn{1}{c}{\multirow{2}*{\textbf{Avg}}} & \multicolumn{3}{c}{\textbf{Crypto-Math}} & \multicolumn{3}{c}{\textbf{Crypto-MBPP}} & \multicolumn{3}{c}{\textbf{Crypto-BBH}} & \multicolumn{3}{c}{\textbf{Crypto-MMLU}} & \multicolumn{3}{c}{\textbf{Crypto-MMLU-Num}} & \multicolumn{3}{c}{\textbf{Crypto-MMLU-Alpha}}  & \multicolumn{3}{c}{\textbf{Crypto-Needle-30K}} \\
\cmidrule{4-24}
\multicolumn{1}{c}{~} & \multicolumn{1}{c}{~} & \multicolumn{1}{c}{~} & \textbf{0} & \textbf{5} & \textbf{10} & \textbf{0} & \textbf{5} & \textbf{10} & \textbf{0} & \textbf{5} & \textbf{10} & \textbf{0} & \textbf{5} & \textbf{10} & \textbf{0} & \textbf{5} & \textbf{10} & \textbf{0} & \textbf{5} & \textbf{10} & \textbf{0} & \textbf{5} & \textbf{10}\\
\midrule
\multicolumn{24}{c}{\textit{Closed-source LLMs}} \\
\midrule
\multicolumn{1}{l}{o1} & \multicolumn{1}{c}{\firstcolor{4.05}} & \multicolumn{1}{c}{\firstcolor{83.69}} & \multicolumn{1}{c}{\firstcolor{96.99}} & \multicolumn{1}{c}{\firstcolor{89.66}} & \multicolumn{1}{c}{\firstcolor{84.48}} & \multicolumn{1}{c}{64.93} & \multicolumn{1}{c}{\secondcolor{68.04}} & \multicolumn{1}{c}{63.9} & \multicolumn{1}{c}{84.08} & \multicolumn{1}{c}{\secondcolor{83.66}} & \multicolumn{1}{c}{\secondcolor{82.13}} & \multicolumn{1}{c}{\firstcolor{94.35}} & \multicolumn{1}{c}{\firstcolor{92.25}} & \multicolumn{1}{c}{\secondcolor{90.53}} & \multicolumn{1}{c}{\firstcolor{92.5}} & \multicolumn{1}{c}{\firstcolor{91.43}} & \multicolumn{1}{c}{\firstcolor{90.0}} & \multicolumn{1}{c}{\firstcolor{90.07}} & \multicolumn{1}{c}{\firstcolor{87.99}} & \multicolumn{1}{c}{\firstcolor{87.32}} & \multicolumn{1}{c}{\firstcolor{99.66}} & \multicolumn{1}{c}{\firstcolor{80.33}} & \multicolumn{1}{c}{\firstcolor{43.2}} \\

\multicolumn{1}{l}{o3-mini} & \multicolumn{1}{c}{\secondcolor{3.67}} & \multicolumn{1}{c}{\secondcolor{76.38}} & \multicolumn{1}{c}{\secondcolor{95.99}} & \multicolumn{1}{c}{\secondcolor{85.4}} & \multicolumn{1}{c}{\secondcolor{77.62}} & \multicolumn{1}{c}{46.96} & \multicolumn{1}{c}{60.39} & \multicolumn{1}{c}{57.49} & \multicolumn{1}{c}{80.6} & \multicolumn{1}{c}{72.03} & \multicolumn{1}{c}{72.7} & \multicolumn{1}{c}{88.34} & \multicolumn{1}{c}{86.97} & \multicolumn{1}{c}{\thirdcolor{84.91}} & \multicolumn{1}{c}{88.93} & \multicolumn{1}{c}{\thirdcolor{88.57}} & \multicolumn{1}{c}{\secondcolor{86.07}} & \multicolumn{1}{c}{\secondcolor{87.23}} & \multicolumn{1}{c}{\secondcolor{85.87}} & \multicolumn{1}{c}{\secondcolor{83.1}} & \multicolumn{1}{c}{88.55} & \multicolumn{1}{c}{52.0} & \multicolumn{1}{c}{34.35} \\

\multicolumn{1}{l}{Gemini-2.0-Flash-Thinking} & \multicolumn{1}{c}{\thirdcolor{3.58}} & \multicolumn{1}{c}{\thirdcolor{76.06}} & \multicolumn{1}{c}{87.98} & \multicolumn{1}{c}{78.3} & \multicolumn{1}{c}{\thirdcolor{73.79}} & \multicolumn{1}{c}{63.31} & \multicolumn{1}{c}{51.05} & \multicolumn{1}{c}{47.05} & \multicolumn{1}{c}{82.34} & \multicolumn{1}{c}{\firstcolor{84.65}} & \multicolumn{1}{c}{\firstcolor{82.63}} & \multicolumn{1}{c}{87.99} & \multicolumn{1}{c}{\thirdcolor{88.03}} & \multicolumn{1}{c}{83.86} & \multicolumn{1}{c}{\thirdcolor{91.07}} & \multicolumn{1}{c}{\secondcolor{89.29}} & \multicolumn{1}{c}{\thirdcolor{85.36}} & \multicolumn{1}{c}{84.75} & \multicolumn{1}{c}{\thirdcolor{83.04}} & \multicolumn{1}{c}{\thirdcolor{81.34}} & \multicolumn{1}{c}{98.32} & \multicolumn{1}{c}{51.0} & \multicolumn{1}{c}{22.11} \\

\multicolumn{1}{l}{Gemini-Exp-1206} & \multicolumn{1}{c}{3.52} & \multicolumn{1}{c}{74.72} & \multicolumn{1}{c}{85.57} & \multicolumn{1}{c}{73.22} & \multicolumn{1}{c}{68.14} & \multicolumn{1}{c}{53.3} & \multicolumn{1}{c}{56.15} & \multicolumn{1}{c}{53.99} & \multicolumn{1}{c}{79.85} & \multicolumn{1}{c}{80.45} & \multicolumn{1}{c}{76.18} & \multicolumn{1}{c}{90.11} & \multicolumn{1}{c}{84.51} & \multicolumn{1}{c}{77.9} & \multicolumn{1}{c}{\secondcolor{92.14}} & \multicolumn{1}{c}{83.93} & \multicolumn{1}{c}{80.0} & \multicolumn{1}{c}{\thirdcolor{85.82}} & \multicolumn{1}{c}{79.15} & \multicolumn{1}{c}{73.94} & \multicolumn{1}{c}{97.64} & \multicolumn{1}{c}{\secondcolor{60.67}} & \multicolumn{1}{c}{\secondcolor{36.39}} \\

\multicolumn{1}{l}{Claude-3.5-Sonnet} & \multicolumn{1}{c}{3.45} & \multicolumn{1}{c}{74.07} & \multicolumn{1}{c}{74.75} & \multicolumn{1}{c}{66.33} & \multicolumn{1}{c}{60.69} & \multicolumn{1}{c}{\thirdcolor{69.18}} & \multicolumn{1}{c}{\thirdcolor{66.93}} & \multicolumn{1}{c}{\thirdcolor{67.54}} & \multicolumn{1}{c}{\thirdcolor{84.58}} & \multicolumn{1}{c}{81.68} & \multicolumn{1}{c}{77.42} & \multicolumn{1}{c}{89.75} & \multicolumn{1}{c}{86.97} & \multicolumn{1}{c}{83.51} & \multicolumn{1}{c}{90.36} & \multicolumn{1}{c}{82.5} & \multicolumn{1}{c}{81.07} & \multicolumn{1}{c}{82.98} & \multicolumn{1}{c}{77.74} & \multicolumn{1}{c}{78.17} & \multicolumn{1}{c}{\thirdcolor{98.99}} & \multicolumn{1}{c}{38.0} & \multicolumn{1}{c}{16.33} \\

\multicolumn{1}{l}{o1-mini} & \multicolumn{1}{c}{3.43} & \multicolumn{1}{c}{73.98} & \multicolumn{1}{c}{\thirdcolor{90.78}} & \multicolumn{1}{c}{78.3} & \multicolumn{1}{c}{66.33} & \multicolumn{1}{c}{\secondcolor{72.69}} & \multicolumn{1}{c}{65.37} & \multicolumn{1}{c}{\secondcolor{67.88}} & \multicolumn{1}{c}{83.33} & \multicolumn{1}{c}{77.23} & \multicolumn{1}{c}{75.93} & \multicolumn{1}{c}{84.45} & \multicolumn{1}{c}{80.99} & \multicolumn{1}{c}{79.3} & \multicolumn{1}{c}{85.36} & \multicolumn{1}{c}{77.86} & \multicolumn{1}{c}{80.36} & \multicolumn{1}{c}{80.5} & \multicolumn{1}{c}{73.5} & \multicolumn{1}{c}{74.3} & \multicolumn{1}{c}{95.96} & \multicolumn{1}{c}{35.0} & \multicolumn{1}{c}{28.23} \\

\multicolumn{1}{l}{DeepSeek-R1} & \multicolumn{1}{c}{3.2} & \multicolumn{1}{c}{68.1} & \multicolumn{1}{c}{90.38} & \multicolumn{1}{c}{\thirdcolor{78.91}} & \multicolumn{1}{c}{70.36} & \multicolumn{1}{c}{68.53} & \multicolumn{1}{c}{\firstcolor{70.61}} & \multicolumn{1}{c}{\firstcolor{69.35}} & \multicolumn{1}{c}{\secondcolor{86.07}} & \multicolumn{1}{c}{80.45} & \multicolumn{1}{c}{\thirdcolor{78.66}} & \multicolumn{1}{c}{\secondcolor{92.93}} & \multicolumn{1}{c}{\secondcolor{90.14}} & \multicolumn{1}{c}{\firstcolor{90.88}} & \multicolumn{1}{c}{61.43} & \multicolumn{1}{c}{42.86} & \multicolumn{1}{c}{37.5} & \multicolumn{1}{c}{46.81} & \multicolumn{1}{c}{45.94} & \multicolumn{1}{c}{45.77} & \multicolumn{1}{c}{87.88} & \multicolumn{1}{c}{\thirdcolor{59.67}} & \multicolumn{1}{c}{\thirdcolor{35.03}} \\

\multicolumn{1}{l}{Gemini-2.0-Flash} & \multicolumn{1}{c}{3.14} & \multicolumn{1}{c}{68.62} & \multicolumn{1}{c}{89.18} & \multicolumn{1}{c}{65.11} & \multicolumn{1}{c}{58.87} & \multicolumn{1}{c}{49.65} & \multicolumn{1}{c}{50.86} & \multicolumn{1}{c}{46.37} & \multicolumn{1}{c}{79.1} & \multicolumn{1}{c}{72.28} & \multicolumn{1}{c}{68.73} & \multicolumn{1}{c}{88.34} & \multicolumn{1}{c}{79.93} & \multicolumn{1}{c}{77.54} & \multicolumn{1}{c}{87.14} & \multicolumn{1}{c}{79.29} & \multicolumn{1}{c}{72.14} & \multicolumn{1}{c}{73.05} & \multicolumn{1}{c}{74.56} & \multicolumn{1}{c}{66.55} & \multicolumn{1}{c}{96.63} & \multicolumn{1}{c}{44.0} & \multicolumn{1}{c}{21.77} \\

\multicolumn{1}{l}{DeepSeek-V2} & \multicolumn{1}{c}{3.08} & \multicolumn{1}{c}{68.29} & \multicolumn{1}{c}{85.37} & \multicolumn{1}{c}{64.5} & \multicolumn{1}{c}{54.23} & \multicolumn{1}{c}{60.83} & \multicolumn{1}{c}{57.86} & \multicolumn{1}{c}{57.28} & \multicolumn{1}{c}{84.08} & \multicolumn{1}{c}{77.23} & \multicolumn{1}{c}{67.49} & \multicolumn{1}{c}{90.81} & \multicolumn{1}{c}{80.99} & \multicolumn{1}{c}{75.79} & \multicolumn{1}{c}{90.36} & \multicolumn{1}{c}{80.71} & \multicolumn{1}{c}{73.93} & \multicolumn{1}{c}{71.99} & \multicolumn{1}{c}{64.31} & \multicolumn{1}{c}{58.8} & \multicolumn{1}{c}{87.54} & \multicolumn{1}{c}{31.33} & \multicolumn{1}{c}{18.71} \\

\multicolumn{1}{l}{DeepSeek-V2.5} & \multicolumn{1}{c}{3.08} & \multicolumn{1}{c}{68.23} & \multicolumn{1}{c}{85.37} & \multicolumn{1}{c}{65.31} & \multicolumn{1}{c}{56.65} & \multicolumn{1}{c}{61.45} & \multicolumn{1}{c}{61.66} & \multicolumn{1}{c}{57.93} & \multicolumn{1}{c}{82.84} & \multicolumn{1}{c}{74.5} & \multicolumn{1}{c}{68.73} & \multicolumn{1}{c}{\thirdcolor{91.17}} & \multicolumn{1}{c}{79.22} & \multicolumn{1}{c}{74.74} & \multicolumn{1}{c}{88.93} & \multicolumn{1}{c}{77.86} & \multicolumn{1}{c}{75.0} & \multicolumn{1}{c}{75.89} & \multicolumn{1}{c}{63.96} & \multicolumn{1}{c}{57.75} & \multicolumn{1}{c}{86.19} & \multicolumn{1}{c}{28.67} & \multicolumn{1}{c}{19.05} \\

\multicolumn{1}{l}{DeepSeek-V3} & \multicolumn{1}{c}{3.07} & \multicolumn{1}{c}{68.35} & \multicolumn{1}{c}{85.97} & \multicolumn{1}{c}{66.94} & \multicolumn{1}{c}{55.44} & \multicolumn{1}{c}{61.95} & \multicolumn{1}{c}{57.34} & \multicolumn{1}{c}{58.01} & \multicolumn{1}{c}{83.08} & \multicolumn{1}{c}{77.72} & \multicolumn{1}{c}{67.0} & \multicolumn{1}{c}{90.81} & \multicolumn{1}{c}{80.99} & \multicolumn{1}{c}{77.9} & \multicolumn{1}{c}{90.36} & \multicolumn{1}{c}{79.29} & \multicolumn{1}{c}{72.86} & \multicolumn{1}{c}{75.89} & \multicolumn{1}{c}{60.42} & \multicolumn{1}{c}{57.75} & \multicolumn{1}{c}{87.54} & \multicolumn{1}{c}{27.0} & \multicolumn{1}{c}{21.09} \\

\multicolumn{1}{l}{Doubao-1.5-Pro-32k} & \multicolumn{1}{c}{2.94} & \multicolumn{1}{c}{66.22} & \multicolumn{1}{c}{81.76} & \multicolumn{1}{c}{61.46} & \multicolumn{1}{c}{49.4} & \multicolumn{1}{c}{\firstcolor{72.99}} & \multicolumn{1}{c}{64.13} & \multicolumn{1}{c}{59.41} & \multicolumn{1}{c}{84.08} & \multicolumn{1}{c}{75.25} & \multicolumn{1}{c}{71.71} & \multicolumn{1}{c}{89.4} & \multicolumn{1}{c}{80.63} & \multicolumn{1}{c}{75.79} & \multicolumn{1}{c}{90.36} & \multicolumn{1}{c}{78.21} & \multicolumn{1}{c}{68.57} & \multicolumn{1}{c}{53.19} & \multicolumn{1}{c}{56.54} & \multicolumn{1}{c}{50.7} & \multicolumn{1}{c}{95.62} & \multicolumn{1}{c}{28.0} & \multicolumn{1}{c}{3.4} \\

\multicolumn{1}{l}{GPT-4o-2024-08-06} & \multicolumn{1}{c}{2.73} & \multicolumn{1}{c}{61.9} & \multicolumn{1}{c}{75.95} & \multicolumn{1}{c}{47.67} & \multicolumn{1}{c}{36.49} & \multicolumn{1}{c}{48.03} & \multicolumn{1}{c}{52.1} & \multicolumn{1}{c}{47.47} & \multicolumn{1}{c}{81.09} & \multicolumn{1}{c}{70.3} & \multicolumn{1}{c}{64.52} & \multicolumn{1}{c}{85.51} & \multicolumn{1}{c}{78.17} & \multicolumn{1}{c}{75.09} & \multicolumn{1}{c}{83.93} & \multicolumn{1}{c}{71.07} & \multicolumn{1}{c}{65.0} & \multicolumn{1}{c}{63.12} & \multicolumn{1}{c}{51.24} & \multicolumn{1}{c}{37.68} & \multicolumn{1}{c}{97.98} & \multicolumn{1}{c}{42.0} & \multicolumn{1}{c}{25.51} \\

\multicolumn{1}{l}{Doubao-Pro-32k} & \multicolumn{1}{c}{2.64} & \multicolumn{1}{c}{62.32} & \multicolumn{1}{c}{87.58} & \multicolumn{1}{c}{52.94} & \multicolumn{1}{c}{36.49} & \multicolumn{1}{c}{64.64} & \multicolumn{1}{c}{65.7} & \multicolumn{1}{c}{60.05} & \multicolumn{1}{c}{81.84} & \multicolumn{1}{c}{74.26} & \multicolumn{1}{c}{62.78} & \multicolumn{1}{c}{88.34} & \multicolumn{1}{c}{72.18} & \multicolumn{1}{c}{64.21} & \multicolumn{1}{c}{87.14} & \multicolumn{1}{c}{75.0} & \multicolumn{1}{c}{64.29} & \multicolumn{1}{c}{60.64} & \multicolumn{1}{c}{53.71} & \multicolumn{1}{c}{47.89} & \multicolumn{1}{c}{97.98} & \multicolumn{1}{c}{11.0} & \multicolumn{1}{c}{0.0} \\

\multicolumn{1}{l}{GPT-4o-2024-05-13} & \multicolumn{1}{c}{2.6} & \multicolumn{1}{c}{60.4} & \multicolumn{1}{c}{73.95} & \multicolumn{1}{c}{44.42} & \multicolumn{1}{c}{29.23} & \multicolumn{1}{c}{49.33} & \multicolumn{1}{c}{58.53} & \multicolumn{1}{c}{50.33} & \multicolumn{1}{c}{80.6} & \multicolumn{1}{c}{69.06} & \multicolumn{1}{c}{57.82} & \multicolumn{1}{c}{85.16} & \multicolumn{1}{c}{76.06} & \multicolumn{1}{c}{70.17} & \multicolumn{1}{c}{84.29} & \multicolumn{1}{c}{64.64} & \multicolumn{1}{c}{54.64} & \multicolumn{1}{c}{68.79} & \multicolumn{1}{c}{52.3} & \multicolumn{1}{c}{36.97} & \multicolumn{1}{c}{97.31} & \multicolumn{1}{c}{38.33} & \multicolumn{1}{c}{26.53} \\

\multicolumn{1}{l}{Qwen-max} & \multicolumn{1}{c}{2.57} & \multicolumn{1}{c}{61.48} & \multicolumn{1}{c}{81.16} & \multicolumn{1}{c}{45.03} & \multicolumn{1}{c}{27.82} & \multicolumn{1}{c}{59.48} & \multicolumn{1}{c}{55.27} & \multicolumn{1}{c}{46.67} & \multicolumn{1}{c}{81.84} & \multicolumn{1}{c}{71.54} & \multicolumn{1}{c}{58.56} & \multicolumn{1}{c}{89.75} & \multicolumn{1}{c}{73.94} & \multicolumn{1}{c}{64.91} & \multicolumn{1}{c}{89.29} & \multicolumn{1}{c}{73.57} & \multicolumn{1}{c}{64.64} & \multicolumn{1}{c}{81.92} & \multicolumn{1}{c}{62.19} & \multicolumn{1}{c}{52.46} & \multicolumn{1}{c}{88.55} & \multicolumn{1}{c}{16.0} & \multicolumn{1}{c}{6.46} \\

\multicolumn{1}{l}{GPT-4o-2024-11-20} & \multicolumn{1}{c}{2.55} & \multicolumn{1}{c}{57.91} & \multicolumn{1}{c}{68.94} & \multicolumn{1}{c}{47.87} & \multicolumn{1}{c}{35.08} & \multicolumn{1}{c}{41.27} & \multicolumn{1}{c}{25.88} & \multicolumn{1}{c}{37.14} & \multicolumn{1}{c}{81.34} & \multicolumn{1}{c}{70.55} & \multicolumn{1}{c}{63.52} & \multicolumn{1}{c}{86.57} & \multicolumn{1}{c}{77.82} & \multicolumn{1}{c}{71.23} & \multicolumn{1}{c}{73.57} & \multicolumn{1}{c}{68.93} & \multicolumn{1}{c}{63.93} & \multicolumn{1}{c}{51.77} & \multicolumn{1}{c}{44.88} & \multicolumn{1}{c}{38.38} & \multicolumn{1}{c}{\secondcolor{99.33}} & \multicolumn{1}{c}{43.33} & \multicolumn{1}{c}{24.83} \\

\multicolumn{1}{l}{GLM-4-Plus} & \multicolumn{1}{c}{2.53} & \multicolumn{1}{c}{59.29} & \multicolumn{1}{c}{71.34} & \multicolumn{1}{c}{44.83} & \multicolumn{1}{c}{29.64} & \multicolumn{1}{c}{45.09} & \multicolumn{1}{c}{57.79} & \multicolumn{1}{c}{51.07} & \multicolumn{1}{c}{81.34} & \multicolumn{1}{c}{69.55} & \multicolumn{1}{c}{58.56} & \multicolumn{1}{c}{87.63} & \multicolumn{1}{c}{72.54} & \multicolumn{1}{c}{60.0} & \multicolumn{1}{c}{80.0} & \multicolumn{1}{c}{71.43} & \multicolumn{1}{c}{62.14} & \multicolumn{1}{c}{74.47} & \multicolumn{1}{c}{66.08} & \multicolumn{1}{c}{52.82} & \multicolumn{1}{c}{97.64} & \multicolumn{1}{c}{8.0} & \multicolumn{1}{c}{3.06} \\

\multicolumn{1}{l}{StepFun-2-16k} & \multicolumn{1}{c}{2.35} & \multicolumn{1}{c}{52.81} & \multicolumn{1}{c}{75.75} & \multicolumn{1}{c}{48.48} & \multicolumn{1}{c}{32.46} & \multicolumn{1}{c}{58.55} & \multicolumn{1}{c}{57.88} & \multicolumn{1}{c}{52.8} & \multicolumn{1}{c}{\firstcolor{88.81}} & \multicolumn{1}{c}{\thirdcolor{82.18}} & \multicolumn{1}{c}{65.51} & \multicolumn{1}{c}{86.57} & \multicolumn{1}{c}{70.42} & \multicolumn{1}{c}{60.7} & \multicolumn{1}{c}{83.93} & \multicolumn{1}{c}{63.57} & \multicolumn{1}{c}{57.5} & \multicolumn{1}{c}{37.59} & \multicolumn{1}{c}{31.45} & \multicolumn{1}{c}{25.7} & \multicolumn{1}{c}{20.2} & \multicolumn{1}{c}{8.33} & \multicolumn{1}{c}{0.68} \\

\multicolumn{1}{l}{Gemini-1.5-Pro-001} & \multicolumn{1}{c}{2.09} & \multicolumn{1}{c}{49.06} & \multicolumn{1}{c}{64.53} & \multicolumn{1}{c}{39.96} & \multicolumn{1}{c}{27.02} & \multicolumn{1}{c}{42.07} & \multicolumn{1}{c}{50.83} & \multicolumn{1}{c}{43.84} & \multicolumn{1}{c}{60.95} & \multicolumn{1}{c}{51.98} & \multicolumn{1}{c}{47.39} & \multicolumn{1}{c}{79.51} & \multicolumn{1}{c}{60.91} & \multicolumn{1}{c}{58.6} & \multicolumn{1}{c}{73.93} & \multicolumn{1}{c}{69.29} & \multicolumn{1}{c}{57.86} & \multicolumn{1}{c}{58.51} & \multicolumn{1}{c}{41.34} & \multicolumn{1}{c}{31.34} & \multicolumn{1}{c}{66.67} & \multicolumn{1}{c}{3.67} & \multicolumn{1}{c}{0.0} \\

\multicolumn{1}{l}{GPT-4-Turbo} & \multicolumn{1}{c}{2.07} & \multicolumn{1}{c}{52.48} & \multicolumn{1}{c}{70.34} & \multicolumn{1}{c}{38.34} & \multicolumn{1}{c}{21.57} & \multicolumn{1}{c}{43.76} & \multicolumn{1}{c}{36.53} & \multicolumn{1}{c}{21.13} & \multicolumn{1}{c}{81.09} & \multicolumn{1}{c}{66.83} & \multicolumn{1}{c}{55.34} & \multicolumn{1}{c}{85.87} & \multicolumn{1}{c}{69.01} & \multicolumn{1}{c}{56.14} & \multicolumn{1}{c}{80.71} & \multicolumn{1}{c}{57.86} & \multicolumn{1}{c}{47.14} & \multicolumn{1}{c}{62.41} & \multicolumn{1}{c}{50.18} & \multicolumn{1}{c}{45.07} & \multicolumn{1}{c}{98.65} & \multicolumn{1}{c}{9.33} & \multicolumn{1}{c}{4.76} \\

\multicolumn{1}{l}{GLM-Zero-Preview} & \multicolumn{1}{c}{2.06} & \multicolumn{1}{c}{48.89} & \multicolumn{1}{c}{69.34} & \multicolumn{1}{c}{31.03} & \multicolumn{1}{c}{18.55} & \multicolumn{1}{c}{45.65} & \multicolumn{1}{c}{39.84} & \multicolumn{1}{c}{37.7} & \multicolumn{1}{c}{73.88} & \multicolumn{1}{c}{62.87} & \multicolumn{1}{c}{48.39} & \multicolumn{1}{c}{71.03} & \multicolumn{1}{c}{63.38} & \multicolumn{1}{c}{54.74} & \multicolumn{1}{c}{74.29} & \multicolumn{1}{c}{66.79} & \multicolumn{1}{c}{50.71} & \multicolumn{1}{c}{57.8} & \multicolumn{1}{c}{55.48} & \multicolumn{1}{c}{43.66} & \multicolumn{1}{c}{57.91} & \multicolumn{1}{c}{2.33} & \multicolumn{1}{c}{1.36} \\

\multicolumn{1}{l}{Doubao-lite-0115} & \multicolumn{1}{c}{1.55} & \multicolumn{1}{c}{41.34} & \multicolumn{1}{c}{74.35} & \multicolumn{1}{c}{32.66} & \multicolumn{1}{c}{16.13} & \multicolumn{1}{c}{43.4} & \multicolumn{1}{c}{39.37} & \multicolumn{1}{c}{29.57} & \multicolumn{1}{c}{72.64} & \multicolumn{1}{c}{56.68} & \multicolumn{1}{c}{46.15} & \multicolumn{1}{c}{84.1} & \multicolumn{1}{c}{63.73} & \multicolumn{1}{c}{52.98} & \multicolumn{1}{c}{71.07} & \multicolumn{1}{c}{42.14} & \multicolumn{1}{c}{37.5} & \multicolumn{1}{c}{4.25} & \multicolumn{1}{c}{3.53} & \multicolumn{1}{c}{4.58} & \multicolumn{1}{c}{84.85} & \multicolumn{1}{c}{5.67} & \multicolumn{1}{c}{2.72} \\


\midrule
\multicolumn{24}{c}{\textit{Open-source LLMs}} \\
\midrule

\multicolumn{1}{l}{Qwen2.5-72B-Instruct} & \multicolumn{1}{c}{2.44} & \multicolumn{1}{c}{59.99} & \multicolumn{1}{c}{82.6} & \multicolumn{1}{c}{41.0} & \multicolumn{1}{c}{23.8} & \multicolumn{1}{c}{66.2} & \multicolumn{1}{c}{54.58} & \multicolumn{1}{c}{43.35} & \multicolumn{1}{c}{81.48} & \multicolumn{1}{c}{68.15} & \multicolumn{1}{c}{55.56} & \multicolumn{1}{c}{88.07} & \multicolumn{1}{c}{73.68} & \multicolumn{1}{c}{60.7} & \multicolumn{1}{c}{90.18} & \multicolumn{1}{c}{75.09} & \multicolumn{1}{c}{62.81} & \multicolumn{1}{c}{70.88} & \multicolumn{1}{c}{56.14} & \multicolumn{1}{c}{47.37} & \multicolumn{1}{c}{97.78} & \multicolumn{1}{c}{12.59} & \multicolumn{1}{c}{7.78} \\

\multicolumn{1}{l}{Qwen2.5-72B} & \multicolumn{1}{c}{2.18} & \multicolumn{1}{c}{48.83} & \multicolumn{1}{c}{52.0} & \multicolumn{1}{c}{22.6} & \multicolumn{1}{c}{9.6} & \multicolumn{1}{c}{66.51} & \multicolumn{1}{c}{59.04} & \multicolumn{1}{c}{52.91} & \multicolumn{1}{c}{55.56} & \multicolumn{1}{c}{42.96} & \multicolumn{1}{c}{35.31} & \multicolumn{1}{c}{59.65} & \multicolumn{1}{c}{57.89} & \multicolumn{1}{c}{43.86} & \multicolumn{1}{c}{65.61} & \multicolumn{1}{c}{58.25} & \multicolumn{1}{c}{44.91} & \multicolumn{1}{c}{56.49} & \multicolumn{1}{c}{54.03} & \multicolumn{1}{c}{41.75} & \multicolumn{1}{c}{/} & \multicolumn{1}{c}{/} & \multicolumn{1}{c}{/} \\

\multicolumn{1}{l}{Llama-3.1-70B-Instruct} & \multicolumn{1}{c}{1.74} & \multicolumn{1}{c}{44.65} & \multicolumn{1}{c}{56.2} & \multicolumn{1}{c}{27.6} & \multicolumn{1}{c}{15.2} & \multicolumn{1}{c}{50.27} & \multicolumn{1}{c}{48.59} & \multicolumn{1}{c}{44.84} & \multicolumn{1}{c}{68.39} & \multicolumn{1}{c}{59.01} & \multicolumn{1}{c}{52.59} & \multicolumn{1}{c}{84.56} & \multicolumn{1}{c}{70.53} & \multicolumn{1}{c}{55.44} & \multicolumn{1}{c}{54.03} & \multicolumn{1}{c}{36.49} & \multicolumn{1}{c}{31.23} & \multicolumn{1}{c}{43.51} & \multicolumn{1}{c}{23.51} & \multicolumn{1}{c}{17.54} & \multicolumn{1}{c}{94.44} & \multicolumn{1}{c}{3.7} & \multicolumn{1}{c}{0.0} \\

\multicolumn{1}{l}{Qwen2.5-7B-Instruct} & \multicolumn{1}{c}{1.24} & \multicolumn{1}{c}{34.56} & \multicolumn{1}{c}{48.2} & \multicolumn{1}{c}{14.6} & \multicolumn{1}{c}{4.8} & \multicolumn{1}{c}{50.06} & \multicolumn{1}{c}{14.29} & \multicolumn{1}{c}{7.21} & \multicolumn{1}{c}{60.99} & \multicolumn{1}{c}{44.2} & \multicolumn{1}{c}{33.09} & \multicolumn{1}{c}{78.25} & \multicolumn{1}{c}{51.58} & \multicolumn{1}{c}{40.7} & \multicolumn{1}{c}{70.17} & \multicolumn{1}{c}{40.7} & \multicolumn{1}{c}{34.03} & \multicolumn{1}{c}{17.89} & \multicolumn{1}{c}{7.37} & \multicolumn{1}{c}{3.86} & \multicolumn{1}{c}{/} & \multicolumn{1}{c}{/} & \multicolumn{1}{c}{/} \\

\multicolumn{1}{l}{Codestral-22B-V0.1} & \multicolumn{1}{c}{0.83} & \multicolumn{1}{c}{22.09} & \multicolumn{1}{c}{32.2} & \multicolumn{1}{c}{9.4} & \multicolumn{1}{c}{2.2} & \multicolumn{1}{c}{60.34} & \multicolumn{1}{c}{37.47} & \multicolumn{1}{c}{22.13} & \multicolumn{1}{c}{40.99} & \multicolumn{1}{c}{26.91} & \multicolumn{1}{c}{22.47} & \multicolumn{1}{c}{32.63} & \multicolumn{1}{c}{26.67} & \multicolumn{1}{c}{21.4} & \multicolumn{1}{c}{19.3} & \multicolumn{1}{c}{15.79} & \multicolumn{1}{c}{7.72} & \multicolumn{1}{c}{12.3} & \multicolumn{1}{c}{4.56} & \multicolumn{1}{c}{3.16} & \multicolumn{1}{c}{/} & \multicolumn{1}{c}{/} & \multicolumn{1}{c}{/} \\

\multicolumn{1}{l}{Qwen2.5-7B} & \multicolumn{1}{c}{0.8} & \multicolumn{1}{c}{20.03} & \multicolumn{1}{c}{22.8} & \multicolumn{1}{c}{8.4} & \multicolumn{1}{c}{3.8} & \multicolumn{1}{c}{44.07} & \multicolumn{1}{c}{30.29} & \multicolumn{1}{c}{27.84} & \multicolumn{1}{c}{25.43} & \multicolumn{1}{c}{17.28} & \multicolumn{1}{c}{13.33} & \multicolumn{1}{c}{36.49} & \multicolumn{1}{c}{20.7} & \multicolumn{1}{c}{22.46} & \multicolumn{1}{c}{29.47} & \multicolumn{1}{c}{18.95} & \multicolumn{1}{c}{18.6} & \multicolumn{1}{c}{9.47} & \multicolumn{1}{c}{8.07} & \multicolumn{1}{c}{3.16} & \multicolumn{1}{c}{/} & \multicolumn{1}{c}{/} & \multicolumn{1}{c}{/} \\

\multicolumn{1}{l}{Llama-3.1-8B-Instruct} & \multicolumn{1}{c}{0.73} & \multicolumn{1}{c}{22.86} & \multicolumn{1}{c}{34.0} & \multicolumn{1}{c}{7.0} & \multicolumn{1}{c}{3.0} & \multicolumn{1}{c}{33.83} & \multicolumn{1}{c}{11.78} & \multicolumn{1}{c}{8.88} & \multicolumn{1}{c}{37.78} & \multicolumn{1}{c}{20.74} & \multicolumn{1}{c}{18.02} & \multicolumn{1}{c}{65.96} & \multicolumn{1}{c}{31.93} & \multicolumn{1}{c}{26.67} & \multicolumn{1}{c}{55.09} & \multicolumn{1}{c}{25.26} & \multicolumn{1}{c}{19.3} & \multicolumn{1}{c}{9.83} & \multicolumn{1}{c}{1.05} & \multicolumn{1}{c}{1.4} & \multicolumn{1}{c}{/} & \multicolumn{1}{c}{/} & \multicolumn{1}{c}{/} \\

\multicolumn{1}{l}{AI21-Jamba-1.5-mini} & \multicolumn{1}{c}{0.57} & \multicolumn{1}{c}{16.28} & \multicolumn{1}{c}{29.4} & \multicolumn{1}{c}{3.2} & \multicolumn{1}{c}{2.6} & \multicolumn{1}{c}{52.7} & \multicolumn{1}{c}{18.64} & \multicolumn{1}{c}{5.97} & \multicolumn{1}{c}{28.89} & \multicolumn{1}{c}{21.98} & \multicolumn{1}{c}{22.22} & \multicolumn{1}{c}{39.65} & \multicolumn{1}{c}{24.91} & \multicolumn{1}{c}{29.12} & \multicolumn{1}{c}{6.7} & \multicolumn{1}{c}{3.86} & \multicolumn{1}{c}{3.16} & \multicolumn{1}{c}{0.0} & \multicolumn{1}{c}{0.0} & \multicolumn{1}{c}{0.0} & \multicolumn{1}{c}{/} & \multicolumn{1}{c}{/} & \multicolumn{1}{c}{/} \\

\multicolumn{1}{l}{Qwen2.5-1.5B-Instruct} & \multicolumn{1}{c}{0.39} & \multicolumn{1}{c}{14.45} & \multicolumn{1}{c}{31.6} & \multicolumn{1}{c}{4.4} & \multicolumn{1}{c}{1.4} & \multicolumn{1}{c}{41.88} & \multicolumn{1}{c}{5.7} & \multicolumn{1}{c}{1.62} & \multicolumn{1}{c}{26.17} & \multicolumn{1}{c}{20.74} & \multicolumn{1}{c}{17.78} & \multicolumn{1}{c}{52.63} & \multicolumn{1}{c}{17.19} & \multicolumn{1}{c}{16.14} & \multicolumn{1}{c}{13.3} & \multicolumn{1}{c}{4.91} & \multicolumn{1}{c}{3.51} & \multicolumn{1}{c}{0.7} & \multicolumn{1}{c}{0.0} & \multicolumn{1}{c}{0.35} & \multicolumn{1}{c}{/} & \multicolumn{1}{c}{/} & \multicolumn{1}{c}{/} \\

\multicolumn{1}{l}{Qwen2.5-1.5B} & \multicolumn{1}{c}{0.07} & \multicolumn{1}{c}{2.46} & \multicolumn{1}{c}{4.4} & \multicolumn{1}{c}{0.2} & \multicolumn{1}{c}{0.2} & \multicolumn{1}{c}{7.44} & \multicolumn{1}{c}{0.55} & \multicolumn{1}{c}{0.47} & \multicolumn{1}{c}{4.69} & \multicolumn{1}{c}{5.43} & \multicolumn{1}{c}{2.72} & \multicolumn{1}{c}{7.37} & \multicolumn{1}{c}{2.1} & \multicolumn{1}{c}{1.05} & \multicolumn{1}{c}{3.86} & \multicolumn{1}{c}{1.75} & \multicolumn{1}{c}{1.75} & \multicolumn{1}{c}{0.35} & \multicolumn{1}{c}{0.0} & \multicolumn{1}{c}{0.0} & \multicolumn{1}{c}{/} & \multicolumn{1}{c}{/} & \multicolumn{1}{c}{/} \\

\bottomrule
\end{tabular}
}
\vskip -0.1in
\end{table*}

\paragraph{CR Ability can be Better Evaluated Using AUC}
Currently, many popular benchmarks, such as MMLU, are no longer effective at capturing the performance gaps between different models. The accuracy gaps between different models are quite limited recently, which fails to fully reflect their actual abilities. 
To help LLM practitioners more accurately assess the CR ability of models, we propose using the AUC to evaluate the model's CR ability. 
The AUC provides a more effective measure of the model’s CR ability. In Figure \ref{fig:subset}, the redder the lines, the higher the corresponding AUC values, which also indicate stronger CR abilities of the model. It can be observed that o1, o3-mini, and Gemini-2.0-Flash-Thinking-Exp-01-21 exhibit relatively strong CR abilities. 
Furthermore, the variance of the models' AUC when evaluated in our \ourmethod{} settings is higher than that when evaluated in the vanilla settings, as illustrated in Appendix \ref{Variance of AUC}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t!]
    \centering
    \vskip 0.2in
    \includegraphics[width=0.8\linewidth]{pics/subset.pdf}
    \caption{The performance of different models on \textit{Crypto-HighResolution} with 10 varying degrees of encoding.}
    \label{fig:subset}
    \vskip -0.2in
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% 随着模型size变大，模型能力变强，补一下数据
\paragraph{CR Ability in Models is Influenced by Various Factors}
We design three subtasks: question decoding, multi-hop reasoning, and answering the original question. 
We observe that as the number of subtasks or the number of encoded words increases, the model's accuracy in answering questions decreases. This indicates that the evaluation of the model's CR ability is influenced by the complexity of the question composition and the difficulty of the subtasks. 
We test the performance of Qwen2.5 models with different sizes on the hardest Crypto-MMLU-Alpha question, and the results show that when the number of encoded words reached 10, Qwen2.5-1.5B-Instruct has near-zero accuracy, while Qwen2.5-72B-Instruct demonstrates some CR ability. The same performance is observed when the number of encoded words is zero. 
To validate our findings, we also compare base models with instruct models and examine the differences across various model architectures in Section \ref{section: ablation experiments}.

% We design three subtasks: question decoding, multi-hop reasoning, and answering the original question. 
% When the task involves only decoding the question and answering the original question, the accuracy of the model's answers is primarily influenced by the difficulty of the original question. 
% When decoding errors occur, the extent of these errors significantly affects the original question, particularly in fields such as Crypto-Math and Crypto-BBH. 
% As the number of decoding steps increases, the model accuracy quickly decreases. 
% In contrast, for relatively simpler tasks like Crypto-MMLU and Crypto-MBPP, existing models can still maintain a certain level of accuracy during compositional reasoning, resulting in a slower decrease in accuracy. 
% We also observed that as the number of subtasks to be composed increases, the accuracy of the answers decreases. 
% In Crypto-MMLU, when three subtasks are composed, the accuracy gradually decreases as the number of hops increases. 
% For the most challenging Crypto-MMLU-Alpha question with 10 words encoded, the accuracy drops by at least \textbf{30\%} compared to the non-encoded MMLU question in Figure \ref{fig:model_size}. 
% This indicates a positive correlation between the increase in complexity of the composition and the decline in accuracy.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[h!]
    \centering
    \vskip 0.2in
    \includegraphics[width=0.8\linewidth]{pics/model_size/Crypto-MMLU-Alpha.pdf}
    \caption{The performance of models with different model size on Crypto-MMLU-Alpha. In \textit{Crypto-MMLU-Alpha\_Words}, \textit{Words} denotes the number of words encoded in the given question.}
    \label{fig:model_size}
    \vskip -0.2in
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Ablation Experiments}
\label{section: ablation experiments}
To validate our conclusion, we consider various factors affecting the model's CR abilities (e.g., multi-turn dialogue vs single-turn dialogue, base model vs instruct model, decoding capacity, different architectures, Doubao-Moe vs Doubao-Dense). Complete experimental results and other ablation experiments can be seen in Appendix \ref{appendix: Ablation Experiments}.

\paragraph{Multi-Turn Decomposition Reasoning is Simpler than Single-Turn Composition Reasoning}
We decompose the CR task into multiple turns, where multi-turn focuses more on individual questions, while single-turn focuses on the compositional question (The detailed design is provided in Appendix \ref{appendix: multi-turn setup}).
In Figure \ref{fig:multi-turns}, models with higher accuracy in answering questions generally perform better in multi-turn dialogues than in single-turn dialogues, indicating that the model has certain limitations in composition reasoning compared to solving individual questions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[ht!]
    \centering
    \vskip 0.2in
    \includegraphics[width=0.8\linewidth]{pics/multi-turns/Crypto-MMLU.pdf}
    \caption{The comparison of the performance of multi-turn and single-turn dialogues. In \textit{Crypto-MMLU-Words-Type}, \textit{Words} denotes the number of words encoded in the given question. \textit{Type} denotes multi-turn(mt) or single-turn(sg).}
    \label{fig:multi-turns}
    \vskip -0.2in
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-10pt} 
\paragraph{Instruct Fine-tuning Enhances the CR Ability of the Model}
With increasing encoding complexity and multi-hops, base models demonstrate degraded performance in handling complex compositional tasks, showing significant accuracy decline. 
Figure \ref{fig:base} reveals that base models exhibit markedly inferior CR abilities compared to instruct models. The average accuracy of question answering in base models decreased by no less than 20\%.
There is even a case where the larger model, Qwen2.5-72B, performs worse than the smaller model, Qwen2.5-7B-Instruct.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[h!]
    \centering
    \vskip 0.2in
    \includegraphics[width=0.8\linewidth]{pics/base_and_instruct/Crypto-MMLU.pdf}
    \caption{The performance of the Base models and the Instrcut models on Crypto-MMLU. In \textit{Crypto-MMLU\_Words}, \textit{Words} denotes the number of words encoded in the given question.}
    \label{fig:base}
    \vskip -0.2in
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%