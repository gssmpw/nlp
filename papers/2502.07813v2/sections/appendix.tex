\section{Encoding Rule}
\label{appendix: encoding rule}
Since sections \ref{sec:methods} and \ref{sec:experiment} primarily introduce and utilize the emoji shuffle encoding method, this section will present and supplement the other two encoding methods. Content \ref{appendix: encoding rule} below shows the encoding rules.
\begin{figure}[H]
    \centering
    \vskip 0.2in
    \subfigure[Morse Base]{
        \includegraphics[width=0.36\textwidth]{pics/encoding_rule/morse_base.pdf}
    }
    \subfigure[Emoji Base]{
        \includegraphics[width=0.54\textwidth]{pics/encoding_rule/emoji_base.pdf}
    }
    \subfigure[Emoji Shuffle]{
        \includegraphics[width=0.54\textwidth]{pics/encoding_rule/emoji_shuffle.pdf}
    }
    \subfigure[Morse Base Math]{
        \includegraphics[width=0.36\textwidth]{pics/encoding_rule/morse_base_math.pdf}
    }
    \subfigure[Emoji Base Math]{
        \includegraphics[width=0.49\textwidth]{pics/encoding_rule/emoji_base.pdf}
    }
    \subfigure[Emoji Shuffle Math]{
        \includegraphics[width=0.41\textwidth]{pics/encoding_rule/emoji_shuffle_math.pdf}
    }
    \caption{Encoding rule for our experiments.}
    \label{fig:encoding rule}
    \vskip -0.2in
\end{figure}
\vfill


\newpage
\section{Prompt Templates}
\label{appendix: prompt Template}
Content \ref{appendix: prompt Template} below shows the prompt templates used in our \benchmark{}.
\subsection{Prompt for Crypto-Math, Crypto-MBPP and Crypto-BBH}
\subsubsection{Zero-Shot Prompt}
\begin{figure}[H]
    \centering
    \vskip 0.2in
    \subfigure[Zero-shot prompt used for non-encoded question]{
        \includegraphics[width=0.47\textwidth]{pics/prompt_template/prompt_no_0.pdf}
    }
    \subfigure[Zero-shot prompt used for encoded question]{
        \includegraphics[width=0.42\textwidth]{pics/prompt_template/prompt_en_0.pdf}
    }
    \caption{Zero-shot prompt for Crypto-Math and Crypto-MMBP}
    \label{fig:0shotall}
    \vskip -0.2in
\end{figure}

\subsubsection{Three-Shot Prompt}
\begin{figure}[H]
    \centering
    \vskip 0.2in
    \subfigure[Three-shot prompt used for non-encoded question]{
        \includegraphics[width=0.45\textwidth]{pics/prompt_template/prompt_no_3.pdf}
    }
    \subfigure[Three-shot prompt used for encoded question]{
        \includegraphics[width=0.45\textwidth]{pics/prompt_template/prompt_en_3.pdf}
    }
    \caption{Three-shot prompt for Crypto-BBH}
    \label{fig:3shotall}
    \vskip -0.2in
\end{figure}
\vfill

\newpage
\subsubsection{Five-Shot Prompt}
\begin{figure}[H]
    \centering
    \vskip 0.2in
    \subfigure[Five-shot prompt used for non-encoded question]{
        \includegraphics[width=0.46\textwidth]{pics/prompt_template/prompt_no_5.pdf}
    }
    \subfigure[Five-shot prompt used for encoded question]{
        \includegraphics[width=0.435\textwidth]{pics/prompt_template/prompt_en_5.pdf}
    }
    \caption{Five-shot prompt for Crypto-BBH, Crypto-Math and Crypto-MMBP}
    \label{fig:5shotall}
    \vskip -0.2in
\end{figure}

\subsection{Prompt for Crypto-MMLU}
\subsubsection{Crypto-MMLU Instruction}
\begin{figure}[H]
    \centering
    \vskip 0.2in
    \includegraphics[width=0.45\textwidth]{pics/prompt_template/crypto-mmlu-instruction.pdf}
    \caption{Zero-shot prompt for Crypto-Math and Crypto-MMBP}
    \label{fig:mmlu-instruction}
    \label{mmlu prompt}
    \vskip -0.2in
\end{figure}
\vfill

\newpage
\subsubsection{Zero-Shot Prompt}
\begin{figure}[H]
    \centering
    \vskip 0.2in
    \subfigure[Zero-shot prompt used for non-encoded question]{
        \includegraphics[width=0.47\textwidth]{pics/prompt_template/prompt_no_0_mmlu.pdf}
    }
    \subfigure[Zero-shot prompt used for encoded question]{
        \includegraphics[width=0.4\textwidth]{pics/prompt_template/prompt_en_0_mmlu.pdf}
    }
    \caption{Zero-shot prompt for Crypto-MMLU}
    \label{fig:0shotmmlu}
    \vskip -0.2in
\end{figure}

\subsubsection{Five-Shot Prompt}
\begin{figure}[H]
    \centering
    \vskip 0.2in
    \subfigure[Five-shot prompt used for non-encoded question]{
        \includegraphics[width=0.46\textwidth]{pics/prompt_template/prompt_no_5_mmlu.pdf}
    }
    \subfigure[Five-shot prompt used for encoded question]{
        \includegraphics[width=0.432\textwidth]{pics/prompt_template/prompt_en_5_mmlu.pdf}
    }
    \caption{Five-shot prompt for Crypto-MMLU}
    \label{fig:5shotmmlu}
    \vskip -0.2in
\end{figure}
\vfill

\newpage
\subsection{Prompt for Crypto-Needle-30K}
\subsubsection{The Needles We Use}
\begin{itemize}
    \item Figs are one of the secret ingredients needed to build the perfect pizza. 
    \item Prosciutto is one of the secret ingredients needed to build the perfect pizza.
    \item Goat cheese is one of the secret ingredients needed to build the perfect pizza.
\end{itemize}
\subsubsection{Zero-Shot Prompt}
\begin{figure}[H]
    \centering
    \vskip 0.2in
    \subfigure[Zero-shot prompt used for non-encoded question]{
        \includegraphics[width=0.47\textwidth]{pics/prompt_template/prompt_no_needle.pdf}
    }
    \subfigure[Zero-shot prompt used for encoded question]{
        \includegraphics[width=0.43\textwidth]{pics/prompt_template/prompt_en_needle.pdf}
    }
    \caption{Zero-shot prompt for Crypto-Needle-30K}
    \label{fig:0shotneedle}
    \vskip -0.2in
\end{figure}
\vfill

\newpage
\section{Ablation Experiments}
\label{appendix: Ablation Experiments}
Content \ref{appendix: Ablation Experiments} below shows the complete results of the experiments.
\subsection{The Effect of Model Size}
As the model size increases, the accuracy of its responses improves, indicating that the model's compositional reasoning ability is related to its size.
\begin{figure}[H]
    \centering
    \subfigure[The result of Crypto-BBH]{
        \includegraphics[width=0.45\textwidth]{pics/model_size/Crypto-BBH.pdf}
    }
    \subfigure[The result of Crypto-Math]{
        \includegraphics[width=0.45\textwidth]{pics/model_size/Crypto-Math.pdf}
    }
    \subfigure[The result of Crypto-MBPP]{
        \includegraphics[width=0.45\textwidth]{pics/model_size/Crypto-MBPP.pdf}
    }
    \subfigure[The result of Crypto-MMLU]{
        \includegraphics[width=0.45\textwidth]{pics/model_size/Crypto-MMLU.pdf}
    }
    \subfigure[The result of Crypto-MMLU-Num]{
        \includegraphics[width=0.45\textwidth]{pics/model_size/Crypto-MMLU-Num.pdf}
    }
    \subfigure[The result of Crypto-MMLU-Alpha]{
        \includegraphics[width=0.45\textwidth]{pics/model_size/Crypto-MMLU-Alpha.pdf}
    }
    \caption{The performance of models with different model size. In \textit{Domain\_Words}, \textit{Words} denotes the number of words encoded in the given question.}
    \label{fig:model-size-all}
    \vskip -0.2in
\end{figure}
\vfill

\newpage
\subsection{The Performane of Doubao-Moe and Doubao-Dense}
Compared to Doubao-Dense, Doubao-Moe performs better on our \benchmark{}, indicating that MOE has superior CR capabilities.
\begin{figure}[H]
    \centering
    \subfigure[The result of Crypto-BBH]{
        \includegraphics[width=0.45\textwidth]{pics/moe_and_dense/Crypto-BBH.pdf}
    }
    \subfigure[The result of Crypto-Math]{
        \includegraphics[width=0.45\textwidth]{pics/moe_and_dense/Crypto-Math.pdf}
    }
    \subfigure[The result of Crypto-MBPP]{
        \includegraphics[width=0.45\textwidth]{pics/moe_and_dense/Crypto-MBPP.pdf}
    }
    \subfigure[The result of Crypto-MMLU]{
        \includegraphics[width=0.45\textwidth]{pics/moe_and_dense/Crypto-MMLU.pdf}
    }
    \subfigure[The result of Crypto-MMLU-Num]{
        \includegraphics[width=0.45\textwidth]{pics/moe_and_dense/Crypto-MMLU-Num.pdf}
    }
    \subfigure[The result of Crypto-MMLU-Alpha]{
        \includegraphics[width=0.45\textwidth]{pics/moe_and_dense/Crypto-MMLU-Alpha.pdf}
    }
    \caption{The performance of Doubao-Moe and Doubao-Dense. In \textit{Domain\_Words}, \textit{Words} denotes the number of words encoded in the given question.}
    \label{fig:moe and dense}
    \vskip -0.2in
\end{figure}
\vfill

\newpage
\subsection{The Comparison of Multi-Turn and Single-Turn Dialogue Effects}
\label{appendix: multi-turn setup}
\subsubsection{Experiment Setup}
\begin{itemize}
    \item \textbf{Multi-Turn}: (1) Complete the task of decoding the encoded question. (2) Answer the decoded question.
    \item \textbf{Single-Turn}: Complete the decoding and answering of the question under the given encoding rules.
\end{itemize}
\subsubsection{The Performance of Multi-Turn and Single-Turn Dialogue Effects}
The performance of multi-turn dialogues is generally better than that of single-turn dialogues, indicating that our \benchmark{} can effectively validate the model's compositional reasoning ability.
\begin{figure}[H]
    \centering
    \vskip 0.2in
    \subfigure[The result of Crypto-MMLU]{
        \includegraphics[width=0.48\textwidth]{pics/multi-turns/Crypto-MMLU.pdf}
    }
    \subfigure[The result of Crypto-MMLU-Num]{
        \includegraphics[width=0.48\textwidth]{pics/multi-turns/Crypto-MMLU-Num.pdf}
    }
    \subfigure[The result of Crypto-MMLU-Alpha]{
        \includegraphics[width=0.48\textwidth]{pics/multi-turns/Crypto-MMLU-Alpha.pdf}
    }
    \caption{The performance of models with different model size. In \textit{Domain\_Words}, \textit{Words} denotes the number of words encoded in the given question.}
    \label{fig:multi-turn-all}
    \vskip -0.2in
\end{figure}
\vfill

\newpage
\subsection{The Performance of Base and Instruct LLMs}
The performance of the base model is generally lower than that of the instruct model, indicating that instruction fine-tuning can enhance the model's compositional reasoning ability.
\begin{figure}[H]
    \centering
    \vskip 0.2in
    \subfigure[The result of Crypto-BBH]{
        \includegraphics[width=0.48\textwidth]{pics/base_and_instruct/Crypto-BBH.pdf}
    }
    \subfigure[The result of Crypto-Math]{
        \includegraphics[width=0.48\textwidth]{pics/base_and_instruct/Crypto-Math.pdf}
    }
    \subfigure[The result of Crypto-MBPP]{
        \includegraphics[width=0.48\textwidth]{pics/base_and_instruct/Crypto-MBPP.pdf}
    }
    \subfigure[The result of Crypto-MMLU]{
        \includegraphics[width=0.48\textwidth]{pics/base_and_instruct/Crypto-MMLU.pdf}
    }
    \subfigure[The result of Crypto-MMLU-Num]{
        \includegraphics[width=0.48\textwidth]{pics/base_and_instruct/Crypto-MMLU-Num.pdf}
    }
    \subfigure[The result of Crypto-MMLU-Alpha]{
        \includegraphics[width=0.48\textwidth]{pics/base_and_instruct/Crypto-MMLU-Alpha.pdf}
    }
    \caption{The performance between base models and instruct models. In \textit{Domain\_Words}, \textit{Words} denotes the number of words encoded in the given question.}
    \label{fig:base-instruct-all}
    \vskip -0.2in
\end{figure}
\vfill

\newpage
\subsection{Thr Effect of Different Architectures}
Models with non-mainstream architectures perform even worse than smaller models like Qwen2.5 and Llama-3.1, suggesting that the model architecture is an important factor affecting compositional reasoning ability.
\begin{figure}[H]
    \centering
    \vskip 0.2in
    \subfigure[The result of Crypto-BBH]{
        \includegraphics[width=0.48\textwidth]{pics/different_structure/Crypto-BBH.pdf}
    }
    \subfigure[The result of Crypto-Math]{
        \includegraphics[width=0.48\textwidth]{pics/different_structure/Crypto-Math.pdf}
    }
    \subfigure[The result of Crypto-MBPP]{
        \includegraphics[width=0.48\textwidth]{pics/different_structure/Crypto-MBPP.pdf}
    }
    \subfigure[The result of Crypto-MMLU]{
        \includegraphics[width=0.48\textwidth]{pics/different_structure/Crypto-MMLU.pdf}
    }
    \subfigure[The result of Crypto-MMLU-Num]{
        \includegraphics[width=0.48\textwidth]{pics/different_structure/Crypto-MMLU-Num.pdf}
    }
    \subfigure[The result of Crypto-MMLU-Alpha]{
        \includegraphics[width=0.48\textwidth]{pics/different_structure/Crypto-MMLU-Alpha.pdf}
    }
    \caption{The performance of different architectural models. In \textit{Domain\_Words}, \textit{Words} denotes the number of words encoded in the given question.}
    \label{fig:architectual-all}
    \vskip -0.2in
\end{figure}
\vfill

\newpage
\subsection{The Decoding Capacity of LLMs}
\label{appendix: decoding}
Content \ref{appendix: decoding} below shows the performance of different models in decoding.
\subsubsection{ROUGE-1 Score}
The statistical results of decoding accuracy using ROUGE-1\citep{lin2004rouge} are shown below.
\begin{figure}[H]
    \centering
    \vskip 0.2in
    \subfigure[The result of Crypto-BBH]{
        \includegraphics[width=0.48\textwidth]{pics/rouge/Crypto-BBH.pdf}
    }
    \subfigure[The result of Crypto-Math]{
        \includegraphics[width=0.48\textwidth]{pics/rouge/Crypto-Math.pdf}
    }
    \subfigure[The result of Crypto-MBPP]{
        \includegraphics[width=0.48\textwidth]{pics/rouge/Crypto-MBPP.pdf}
    }
    \subfigure[The result of Crypto-MMLU]{
        \includegraphics[width=0.48\textwidth]{pics/rouge/Crypto-MMLU.pdf}
    }
    \caption{The decoding performance of different models. In \textit{Domain\_Words}, \textit{Words} denotes the number of words encoded in the given question.}
    \label{fig:decoding-rouge-all}
    \vskip -0.2in
\end{figure}
\vfill

\newpage
\subsubsection{BLEU Score}
The statistical results of decoding accuracy using BLEU(1-gram)\citep{papineni2002bleu} are shown below.
\begin{figure}[H]
    \centering
    \vskip 0.2in
    \subfigure[The result of Crypto-BBH]{
        \includegraphics[width=0.48\textwidth]{pics/bleu/Crypto-BBH.pdf}
    }
    \subfigure[The result of Crypto-Math]{
        \includegraphics[width=0.48\textwidth]{pics/bleu/Crypto-Math.pdf}
    }
    \subfigure[The result of Crypto-MBPP]{
        \includegraphics[width=0.48\textwidth]{pics/bleu/Crypto-MBPP.pdf}
    }
    \subfigure[The result of Crypto-MMLU]{
        \includegraphics[width=0.48\textwidth]{pics/bleu/Crypto-MMLU.pdf}
    }
    \caption{The decoding performance of different models. In \textit{Domain\_Words}, \textit{Words} denotes the number of words encoded in the given question.}
    \label{fig:decoding-bleu-all}
    \vskip -0.2in
\end{figure}

\subsection{Variance of Accuracy and AUC for Different Models}
\label{Variance of AUC}
\begin{table}[H]
\centering
\caption{Variance of accuracy and AUC for closed-source models on Crypto-HighResolution. \textit{Num} stands for Accuracy, which corresponds to solving the question of \textit{Num} words being encoded.}
\vskip 0.15in
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccccccccccc}
\toprule
 & \textbf{0} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} & \textbf{9} & \textbf{10} & \textbf{AUC}\\
\midrule
\textbf{Variance} & 0.0043 & 0.0055 & 0.0058 & 0.0058 & 0.0066 & 0.0069 & 0.0081 & 0.0094 & 0.0091 & 0.0116 & 0.0137 & \textbf{0.6221} \\
\bottomrule
\end{tabular}
}
\vskip -0.1in
\end{table}
\vfill

\newpage
\subsection{Correlation With Chatbot Arena}
Chatbot Arena~\citep{chiang2024chatbot}, which builds its evaluation system through human interaction, is regarded as the "gold standard" in the assessment of human-computer dialogue systems. Although this method has some biases (such as differences in user background and insufficient task diversity), its large scale and high ecological validity make it one of the most representative human evaluation frameworks. 

To investigate the correlation between our approach and human evaluation systems, this section employs the Spearman correlation to analyze the relationship between \benchmark{}, MMLU and Chatbot Arena rankings. The Spearman correlation is suitable for nonlinear data distributions and effectively reflects the monotonic relationship between variables, which is why it is widely used in analyzing the correlation between evaluation metrics and human preferences, providing data support for our findings.

We calculate the Spearman correlation between \benchmark{}(AUC), \benchmark{}(Avg), MMLU, and Chatbot Arena. The AUC and Avg based on \benchmark{} yield a higher Spearman correlation(0.61, 0.57) compared to MMLU(0.19), as shown in Figure \ref{fig:correlation}.
\begin{figure}[H]
    \centering
    \vskip 0.2in
    \includegraphics[width=\linewidth]{pics/correlation.pdf}
    \caption{The correlation matrix for benchmarks. The closer the Spearman correlation is to 1, the more similar the rankings of the two benchmarks are.}
    \label{fig:correlation}
    \vskip -0.2in
\end{figure}

\newpage
\section{Logit Lens Analysis(0\%/50\%/100\% encoding ratio)}
Since section \ref{sec:analysis} primarily utilizes the emoji shuffle encoding method with 0/3/5 words encoding, this section will present and supplement the experiments using 0\%/50\%/100\% encoding ratio, which provides more comprehensive evidence supporting the conclusions presented in the section  \ref{sec:analysis}.
\subsection{Base Morse Encoding Rule}
\begin{figure}[H]
    \centering
    \vskip 0.2in
    \subfigure[The result of Crypto-MMLU-BaseMorse on Llama-3.2-3B-Instruct]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/ratio/base_morse/Llama-3.2-3B-Instruct/Llama-3.2-3B-Instruct_merged_plot.pdf}
    }
    \subfigure[The result of Crypto-MMLU-BaseMorse on Llama-3.1-8B]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/ratio/base_morse/Meta-Llama-3.1-8B/Meta-Llama-3.1-8B_merged_plot.pdf}
    }
    \subfigure[The result of Crypto-MMLU-BaseMorse on Llama-3.1-8B-Instruct]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/ratio/base_morse/Meta-Llama-3.1-8B-Instruct/Meta-Llama-3.1-8B-Instruct_merged_plot.pdf}
    }
    \subfigure[The result of Crypto-MMLU-BaseMorse on Qwen2.5-0.5B-Instruct]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/ratio/base_morse/Qwen2.5-0.5B-Instruct/Qwen2.5-0.5B-Instruct_merged_plot.pdf}
    }
    \subfigure[The result of Crypto-MMLU-BaseMorse on Qwen2.5-1.5B-Instruct]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/ratio/base_morse/Qwen2.5-1.5B-Instruct/Qwen2.5-1.5B-Instruct_merged_plot.pdf}
    }
    \subfigure[The result of Crypto-MMLU-BaseMorse on Qwen2.5-3B]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/ratio/base_morse/Qwen2.5-3B/Qwen2.5-3B_merged_plot.pdf}
    }
    
    
    \caption{The logit lens analysis on Crypto-MMLU-BaseMorse using $0\%/50\%/100\%$ encoding ratios.}
    \label{fig:appendix_logitlens0}
    \vskip -0.2in
\end{figure}
\vfill

\newpage
\begin{figure}[H]
    \centering
    \vskip 0.2in
    \subfigure[The result of Crypto-MMLU-BaseMorse on Qwen2.5-3B-Instruct]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/ratio/base_morse/Qwen2.5-3B-Instruct/Qwen2.5-3B-Instruct_merged_plot.pdf}
    }
    \subfigure[The result of Crypto-MMLU-BaseMorse on Qwen2.5-7B]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/ratio/base_morse/Qwen2.5-7B/Qwen2.5-7B_merged_plot.pdf}
    }
    \subfigure[The result of Crypto-MMLU-BaseMorse on Qwen2.5-7B-Instruct]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/ratio/base_morse/Qwen2.5-7B-Instruct/Qwen2.5-7B-Instruct_merged_plot.pdf}
    }
    \subfigure[The result of Crypto-MMLU-BaseMorse on Qwen2.5-14B-Instruct]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/ratio/base_morse/Qwen2.5-14B-Instruct/Qwen2.5-14B-Instruct_merged_plot.pdf}
    }
    \caption{The logit lens analysis on Crypto-MMLU-BaseMorse using 0\%/50\%/100\% encoding ratios.}
    \label{fig:appendix_logitlens1}
    \vskip -0.2in
\end{figure}

\subsection{Emoji Morse Encoding Rule}
\begin{figure}[H]
    \centering
    \vskip 0.2in
    \subfigure[The result of Crypto-MMLU-EmojiMorse on Llama-3.2-3B-Instruct]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/ratio/emoji_morse/Llama-3.2-3B-Instruct/Llama-3.2-3B-Instruct_merged_plot.pdf}
    }
    \subfigure[The result of Crypto-MMLU-EmojiMorse on Llama-3.1-8B]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/ratio/emoji_morse/Meta-Llama-3.1-8B/Meta-Llama-3.1-8B_merged_plot.pdf}
    }
    \subfigure[The result of Crypto-MMLU-EmojiMorse on Llama-3.1-8B-Instruct]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/ratio/emoji_morse/Meta-Llama-3.1-8B-Instruct/Meta-Llama-3.1-8B-Instruct_merged_plot.pdf}
    }
    \subfigure[The result of Crypto-MMLU-EmojiMorse on Qwen2.5-3B-Instruct]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/ratio/emoji_morse/Qwen2.5-3B-Instruct/Qwen2.5-3B-Instruct_merged_plot.pdf}
    }
    \subfigure[The result of Crypto-MMLU-EmojiMorse on Qwen2.5-7B]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/ratio/emoji_morse/Qwen2.5-7B/Qwen2.5-7B_merged_plot.pdf}
    }
    \subfigure[The result of Crypto-MMLU-EmojiMorse on Qwen2.5-7B-Instruct]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/ratio/emoji_morse/Qwen2.5-7B-Instruct/Qwen2.5-7B-Instruct_merged_plot.pdf}
    }
    \caption{The logit lens analysis on Crypto-MMLU-EmojiMorse using 0\%/50\%/100\% encoding ratios.}
    \label{fig:appendix_logitlens2}
    \vskip -0.2in
\end{figure}
\vfill
\newpage

\subsection{Emoji Shuffle Encoding Rule}
\begin{figure}[H]
    \centering
    \vskip 0.2in
    \subfigure[The result of Crypto-MMLU-EmojiShuffle on Llama-3.2-3B-Instruct]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/ratio/emoji_shuffle/Llama-3.2-3B-Instruct/Llama-3.2-3B-Instruct_merged_plot.pdf}
    }
    \subfigure[The result of Crypto-MMLU-EmojiShuffle on Llama-3.1-8B]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/ratio/emoji_shuffle/Meta-Llama-3.1-8B/Meta-Llama-3.1-8B_merged_plot.pdf}
    }
    \subfigure[The result of Crypto-MMLU-EmojiShuffle on Llama-3.1-8B-Instruct]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/ratio/emoji_shuffle/Meta-Llama-3.1-8B-Instruct/Meta-Llama-3.1-8B-Instruct_merged_plot.pdf}
    }
    \subfigure[The result of Crypto-MMLU-EmojiShuffle on Qwen2.5-3B-Instruct]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/ratio/emoji_shuffle/Qwen2.5-3B-Instruct/Qwen2.5-3B-Instruct_merged_plot.pdf}
    }
    \subfigure[The result of Crypto-MMLU-EmojiShuffle on Qwen2.5-7B]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/ratio/emoji_shuffle/Qwen2.5-7B/Qwen2.5-7B_merged_plot.pdf}
    }
    \subfigure[The result of Crypto-MMLU-EmojiShuffle on Qwen2.5-7B-Instruct]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/ratio/emoji_shuffle/Qwen2.5-7B-Instruct/Qwen2.5-7B-Instruct_merged_plot.pdf}
    }
    \caption{The logit lens analysis on Crypto-MMLU-EmojiShuffle using 0\%/50\%/100\% encoding ratios.}
    \label{fig:appendix_logitlens3}
    \vskip -0.2in
\end{figure}
\vfill
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Logit Lens Analysis(0/3/5 encoding words)}
Since sections \ref{sec:analysis} primarily utilizes the emoji shuffle encoding method with 0/3/5 words encoding, this section will present and supplement the entire experiments result using 0/3/5 encoding words, which provides more comprehensive evidence supporting the conclusions presented in the section  \ref{sec:analysis}.
\subsection{Base Morse Encoding Rule}

\begin{figure}[H]
    \centering
    \vskip 0.2in
    \subfigure[The result of Crypto-MMLU-BaseMorse on Llama-3.1-8B]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/words/base_morse/Meta-Llama-3.1-8B/Meta-Llama-3.1-8B_merged_plot.pdf}
    }
    \subfigure[The result of Crypto-MMLU-BaseMorse on Llama-3.1-8B-Instruct]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/words/base_morse/Llama-3.1-8B-Instruct/Llama-3.1-8B-Instruct_merged_plot.pdf}
    }
    \subfigure[The result of Crypto-MMLU-BaseMorse on Qwen2.5-0.5B-Instruct]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/words/base_morse/Qwen2.5-0.5B-Instruct/Qwen2.5-0.5B-Instruct_merged_plot.pdf}
    }
    \subfigure[The result of Crypto-MMLU-BaseMorse on Qwen2.5-1.5B-Instruct]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/words/base_morse/Qwen2.5-1.5B-Instruct/Qwen2.5-1.5B-Instruct_merged_plot.pdf}
    }
    \subfigure[The result of Crypto-MMLU-BaseMorse on Qwen2.5-3B]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/words/base_morse/Qwen2.5-3B/Qwen2.5-3B_merged_plot.pdf}
    }
    \subfigure[The result of Crypto-MMLU-BaseMorse on Qwen2.5-3B-Instruct]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/words/base_morse/Qwen2.5-3B-Instruct/Qwen2.5-3B-Instruct_merged_plot.pdf}
    }
    \caption{The logit lens analysis on Crypto-MMLU-BaseMorse using 0/3/5 encoding words.}
    \label{fig:appendix_logitlens4}
    \vskip -0.2in
\end{figure}
\vfill

\newpage
\begin{figure}[H]
    \centering
    \vskip 0.2in
    \subfigure[The result of Crypto-MMLU-BaseMorse on Qwen2.5-7B]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/words/base_morse/Qwen2.5-7B/Qwen2.5-7B_merged_plot.pdf}
    }
    \subfigure[The result of Crypto-MMLU-BaseMorse on Qwen2.5-7B-Instruct]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/words/base_morse/Qwen2.5-7B-Instruct/Qwen2.5-7B-Instruct_merged_plot.pdf}
    }
    \caption{The logit lens analysis on Crypto-MMLU-BaseMorse using 0/3/5 encoding words.}
    \label{fig:appendix_logitlens5}
    \vskip -0.2in
\end{figure}
\vfill

\subsection{Emoji Morse Encoding Rule}

\begin{figure}[H]
    \centering
    \vskip 0.2in
    \subfigure[The result of Crypto-MMLU-EmojiMorse on Llama-3.1-8B-Instruct]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/words/emoji_morse/Llama-3.1-8B-Instruct/Llama-3.1-8B-Instruct_merged_plot.pdf}
    }
    \subfigure[The result of Crypto-MMLU-EmojiMorse on Qwen2.5-0.5B-Instruct]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/words/emoji_morse/Qwen2.5-0.5B-Instruct/Qwen2.5-0.5B-Instruct_merged_plot.pdf}
    }
    \subfigure[The result of Crypto-MMLU-EmojiMorse on Qwen2.5-3B-Instruct]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/words/emoji_morse/Qwen2.5-3B-Instruct/Qwen2.5-3B-Instruct_merged_plot.pdf}
    }
    \subfigure[The result of Crypto-MMLU-EmojiMorse on Qwen2.5-7B-Instruct]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/words/emoji_morse/Qwen2.5-7B-Instruct/Qwen2.5-7B-Instruct_merged_plot.pdf}
    }
    \caption{The logit lens analysis on Crypto-MMLU-EmojiMorse using 0/3/5 encoding words.}
    \label{fig:appendix_logitlens6}
    \vskip -0.2in
\end{figure}
\vfill

\newpage
\subsection{Emoji Shuffle Encoding Rule}

\begin{figure}[H]
    \centering
    \vskip 0.2in
    \subfigure[The result of Crypto-MMLU-EmojiShuffle on Qwen2.5-0.5B-Instruct]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/words/emoji_shuffle/Qwen2.5-0.5B-Instruct/Qwen2.5-0.5B-Instruct_merged_plot.pdf}
    }
    \subfigure[The result of Crypto-MMLU-EmojiShuffle on Qwen2.5-1.5B-Instruct]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/words/emoji_shuffle/Qwen2.5-1.5B-Instruct/Qwen2.5-1.5B-Instruct_merged_plot.pdf}
    }

    \caption{The logit lens analysis on Crypto-MMLU-EmojiShuffle using 0/3/5 encoding words.}
    \label{fig:appendix_logitlens7}
    \vskip -0.2in
\end{figure}
\vfill

\section{Neuron Activation Analysis}
\label{Appendix_neuron}
The appendix \ref{Appendix_neuron} shows result of Neuron Activation Analysis not presented in section \ref{sec:analysis}.
\begin{figure}[H]
    \centering
    \vskip 0.2in
    \subfigure[The result of Neuron Activation on Llama-3.1-8B]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/neuron/Meta-Llama-3.1-8B_neuron_plot.pdf}
    }
    \subfigure[The result of Neuron Activation on Qwen2.5-3B]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/neuron/Qwen2.5-3B_neuron_plot.pdf}
    }
    \subfigure[The result of Neuron Activation on Qwen2.5-7B]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/neuron/Qwen2.5-7B_neuron_plot.pdf}
    }
    \subfigure[The result of Neuron Activation on Qwen2.5-7B-Instruct]{
        \includegraphics[width=0.48\textwidth]{pics/appendix/neuron/Qwen2.5-7B-Instruct_neuron_plot.pdf}
    }
    \caption{The other result of Neuron Activation Analysis.}
    \label{fig:appendix_neuron}
    \vskip -0.2in
\end{figure}
\vfill
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{figure}[H]
    \centering
    \vskip 0.2in
    \includegraphics[width=\textwidth]{pics/appendix/casestudy/case1.pdf}
    \caption{Case 1 in the Case Study: The prompts for the Crypto-MMLU-Alpha and Crypto-MMLU datasets are provided in Appendix \ref{appendix: prompt Template}.}
    \label{fig:case1}
    \vskip -0.2in
\end{figure}
\begin{figure}[H]
    \centering
    \vskip 0.2in
    \includegraphics[width=\textwidth]{pics/appendix/casestudy/case2.pdf}
    \caption{Case 2 in the Case Study: The prompts for the Crypto-MMLU-Alpha and Crypto-MMLU datasets are provided in Appendix \ref{appendix: prompt Template}.}
    \label{fig:case2}
    \vskip -0.2in
\end{figure}
\begin{figure}[H]
    \centering
    \vskip 0.2in
    \includegraphics[width=\textwidth]{pics/appendix/casestudy/case3.pdf}
    \caption{Case 3 in the Case Study: The prompts for the Crypto-MMLU-Alpha and Crypto-MMLU datasets are provided in Appendix \ref{appendix: prompt Template}.}
    \label{fig:case3}
    \vskip -0.2in
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{pics/appendix/casestudy/case4.pdf}
    \caption{Case 4 in the Case Study: The prompts for the Crypto-MMLU-Alpha and Crypto-MMLU datasets are provided in Appendix \ref{appendix: prompt Template}.}
    \label{fig:case4}
\end{figure}
\section{Case Studies}
\label{casestudy}
To validate the effectiveness of the evaluation methodology in reflecting model generalization capabilities and to investigate prevalent issues such as data leakage and model overfitting following the public release of evaluation sets across various domains, we introduce a case study module. This module aims to provide comprehensive empirical evidence.

The study focuses on the Qwen2.5-72B-instruct model, with particular emphasis on its performance discrepancies between the Crypto-MMLU and Crypto-MMLU-Alpha datasets. Notably, the Crypto-MMLU-Alpha dataset is constructed through manual partitioning of the original Crypto-MMLU into two subtasks: problem decoding and question answering. Consequently, we refer to tasks on Crypto-MMLU-Alpha as two-stage tasks  and those on Crypto-MMLU as single-stage tasks.

\subsection{\benchmark{} Effectively Evaluates Model Generalization Capabilities}
As illustrated in Figure \ref{fig:case1}, the model exhibits significant deficiencies in decoding performance when directly handling raw problems within two-stage tasks. Specifically, errors occurring during problem decoding phase lead to subsequent answers being generated based on misinterpretations, resulting in inaccurate responses. In contrast, through stepwise execution of decoding and answering subtasks in single-stage tasks, the model achieves accurate decoding outcomes and consequently produces correct answers, which demonstrates that \benchmark{} effectively reveals models' authentic generalization capabilities in complex task scenarios.
\subsection{Potential Overfitting Risks in Models}
Experimental results indicate that while the model demonstrates superior performance on publicly available unencrypted evaluation sets across domains, its performance significantly deteriorates on newly constructed encrypted evaluation sets, suggesting potential overfitting risks. This phenomenon may be attributed to either excessive reliance on training data distributions or insufficient generalization capacity. Through in-depth analysis of the Qwen2.5-72B-instruct model, we observed three typical overfitting patterns:
\paragraph{Correct Answers Despite Decoding Errors in Two-Stage Tasks}
As shown in Figure \ref{fig:case2}, the model generates correct answers even when making decoding errors in two-stage tasks. Simultaneously, it produces accurate responses in single-stage tasks without explicit decoding processes.
\paragraph{Error Propagation from Decoding Failures in Two-Stage Tasks}
Figure \ref{fig:case3} demonstrates that decoding errors in two-stage tasks lead to incorrect answers, whereas the model directly provides correct responses in single-stage tasks without explicit decoding.
\paragraph{Decoupling of Decoding and Answering as Overfitting Manifestation}
As illustrated in Figure \ref{fig:case4}, the model generates correct answers despite decoding errors occurring in both two-stage and single-stage tasks.

These three cases suggest that the model's behavior may excessively rely on pattern memorization from training data rather than semantic understanding-based reasoning, thereby revealing its inherent overfitting vulnerabilities.



\newpage
\section{Reasoning Stage Analysis}
\begin{figure}[h!]
\centering
\vskip 0.2in
\includegraphics[width=\textwidth]{pics/appendix/layer_prompt.pdf}
\caption{Prompt of Reasoning Stage Analysis. }
\label{fig:appendix_layer_prompt}
\vskip -0.2in
\end{figure}

\begin{table}[h]
\centering
\caption{The result of Reasoning Stage Analysis on Llama-3.1-8B
}
\label{tab:Reasoning_Stage_Analysis_llama}
\vskip 0.15in
\resizebox{0.85\textwidth}{!}{
\begin{tabular}{cp{18cm}}
\toprule
 \textbf{Layer} & \textbf{Functions} \\ 
\midrule
 0 & The layer functions to process code-related terms, programming terms, technical terms, symbols, encoded characters, non-English characters, and miscellaneous words. \\ 
 1 & Processes encoded, non-English, special, and diverse characters/symbols, along with technical, programming, and code-related terms. \\ 
 2 & Processes diverse characters, symbols, terms including programming, technical, encoded, non-English, multilingual, abbreviated, and seemingly random ones. \\ 
3 & Processes letters and words related to multiple-choice options and answers, including option letters, answer-related words, and elements within multiple-choice question answering contexts.\\ 
4 & Processes letters and words related to multiple-choice options and answers, including option-denoting letters, possible choices, and related symbols. \\ 
5 & Processes technical and programming terms and symbols, including encoded, random, and diverse related terms such as abbreviations, proper nouns, and possibly foreign or misspelled ones. \\ 
6 & Processes words related to answers, choices, options, and correctness in question-answering contexts.\\
7 & Processes computer programming and code-related terms, encoded or technical-looking terms and symbols, and technical and programming-related terms including identifiers, foreign language characters, and proper names.\\
8 & Processes diverse symbols, codes, special characters, and terms related to encoding, technical domains, programming, and multiple languages.\\
9 & Processes options, answers, letters, words, symbols, and related elements of multiple-choice questions.\\
10 & Processes technical and programming terms and symbols, including codes, encoded or special characters, and terms from various technical domains and languages.\\
11 & Handles encoded, technical, or specialized symbols and terms, along with diverse characters, words, and tokens from different languages, programming contexts, and with various semantic patterns related to technical, programming, and encoding aspects.\\
12 & Processes diverse characters and partial words from different languages, alphabets, and encodings, including foreign language characters, abbreviations, proper names, technical terms, and symbols without a specific semantic pattern.\\
13 & Processes various characters (including alphanumeric), punctuation, common words, special symbols, and text elements related to multiple-choice Q\&A formats, question answering, logical reasoning, code translation, and text encoding.\\
14 & Handles encoded and unrecognizable characters/words, processes code-related tokens and symbols, and deals with miscellaneous or unclear characters and tokens.\\
15 & Processes diverse elements including random characters, symbols, words (such as technical terms, abbreviations, foreign language words), code-like strings, and potentially inappropriate or specialized terms without clear semantic relation to the prompt.\\
16 & Processes diverse characters, words, symbols, and tokens including abbreviations, technical terms, words from different languages, and elements without clear semantic pattern related to the prompt.\\
17 & Processes various symbols, special characters, non-English letters, programming terms, encoded and non-standard strings, and technical terms related to programming, encoding, and foreign languages.\\
18 & Processes single letters, short letter combinations, and words related to multiple-choice options and answers in question-answering contexts.\\
19 & Processes diverse characters, words, codes, symbols, and terms including technical, programming-related, non-English, encoded, and random elements.\\
20 & Processes technical and programming-related terms, including encoded terms, symbols, foreign language characters, and various code-like elements.\\
21 & Processes diverse words, tokens, and symbols including sports-related, from different languages, technical, programming, encoded, and seemingly random or non-standard elements.\\
22 & Processes letters and words related to multiple-choice options and answers, including option letters, answer-related words, and common words in multiple-choice questions.\\
23 & Processes words related to answers, options, and correctness in various contexts including multiple-choice questions, along with some encoded or unrecognized characters and technical/coded terms.\\
24 & Processes encoded and uncommon characters/symbols, along with various technical terms, words from different languages, and random tokens without clear semantic relation to the prompt.\\
25 & Processes words related to question answering, choices, options, correctness, and lack of response.\\
26 & Processes letters and words related to options and answers in multiple-choice questions.\\
27 & Processes various characters, symbols, numbers, words from different languages and alphabets, and potentially related to encoding, programming, or without clear semantic pattern.\\
28 & Processes diverse, seemingly random words including proper names, numbers, and various terms without clear semantic pattern related to prompt.\\
29 & Processes answer options and choices in multiple-choice questions, along with related words such as those related to answering, numbers, correctness, and question-answering presentation.\\
30 & Processes letters, words, and symbols related to the options and answers of multiple-choice questions.\\
31 & Processes technical and programming-related terms, including code-related words, symbols, foreign language characters, and miscellaneous random elements.\\

\bottomrule
\end{tabular}
}
\vskip -0.1in
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[h]
\centering
\caption{The result of Reasoning Stage Analysis on Llama-3.1-8B-Instruct
}
\label{tab:Reasoning_Stage_Analysis_llama_instruct}
\vskip 0.15in
\resizebox{0.85\textwidth}{!}{
\begin{tabular}{cp{18cm}}
\toprule
 \textbf{Layer} & \textbf{Functions} \\ 
\midrule
 0 & The layer functions to process codes, symbols, technical and programming terms, encoded and less common characters/terms, foreign language characters, multilingual words and phrases, identifiers, and computer-related terms. \\ 
 1 & Processes various characters (including non-English, encoded, and special), symbols, codes, and technical/programming terms, as well as random or semantically unconnected words.\\ 
 2 & Processes diverse characters, symbols, words (including non-English, encoded, technical, programming-related, and semantically unconnected ones), and code-like elements. \\ 
3 & Processes single letters, letter combinations, and words like 'none', 'neither' related to multiple-choice question options.\\ 
4 & Processes single letters, letter combinations, and words like 'none' related to answer options, especially in the context of multiple-choice questions, potentially involving encoding or symbol recognition. \\ 
5 & Processes technical and programming-related terms, including symbols, identifiers, abbreviations, and encoded-like terms. \\ 
6 & Processes words related to choices, options, answers, absence or lack, including 'none'-related terms in programming or technical contexts.\\
7 & Processes encoded, technical, and programming-related terms, including symbols, foreign characters, abbreviations, and specialized terms.\\
8 & Processes encoded, non-standard, technical, programming-related characters/terms, symbols, and seemingly random tokens with no clear semantic connection to the prompt.\\
9 & Processes multiple-choice options, including single letters, words related to choices, and answer-choice related characters and words like 'none' and its variants.\\
10 & Processes various characters, symbols (including special, encoded, non-standard ones), codes, technical and programming terms, as well as foreign language characters and non-semantic strings.\\
11 & Processes diverse characters, words, symbols, including foreign language, encoded, technical, and programming-related ones without clear semantic coherence related to the prompt.\\
12 & Processes diverse words including proper names, foreign characters, numbers, and terms without clear semantic pattern related to the prompt.\\
13 & Processes letters, numbers, and common words in various contexts such as questions, answers, multiple-choice options, and potentially for encoding or identification.\\
14 & Processes encoded and non-standard characters/symbols, diverse tokens including code-related, technical, foreign language, and seemingly random elements without clear semantic pattern related to the prompt.\\
15 & Processes diverse characters, words, and symbols including non-English, encoded, technical, programming-related, and seemingly random elements without clear semantic pattern related to the prompt.\\
16 & Processes diverse words including scientific, foreign language, programming, technical terms, symbols, random strings, and miscellaneous words from various domains.\\
17 & Processes non-English, encoded, random, or unusual characters/symbols and diverse tokens without clear semantic connection to the prompt.\\
18 & Processes single letters and letter combinations, especially those related to multiple-choice options such as A, B, C, D, and words like 'none' and 'neither'.\\
19 & Processes diverse words including names, technical and programming terms, abbreviations, random strings, and words from different languages without clear semantic pattern related to prompt.\\
20 & Processes programming terms, various character sets including non-English and special characters, and technical terms related to encoding and different programming contexts.\\
21 & Processes technical and programming terms, symbols, and encoded or specialized characters and terms, along with diverse tokens including multilingual words, abbreviations, and code-like strings.\\
22 & Processes single letters and letter combinations, often related to multiple-choice options, potentially for encoding or representing answer choices.\\
23 & Handles code-related symbols and terms, processes encoded data, including various encoded or non-standard characters, absence/negation words, programming-related and miscellaneous terms, and null/none-related concepts.\\
24 & Handles diverse characters, symbols, words from different languages, and code-like elements, potentially involving encoding and without clear semantic pattern related to prompt.\\
25 & Processes special characters, programming terms, encoded information, and words related to absence/null values and choice options.\\
26 & Processes multiple-choice options, including single letters and words related to answer choices, option identifiers like 'none', and characters and words related to choices and answers.\\
27 & Processes various characters, words, and symbols including non-English, technical, and code-like elements without clear semantic relation to the prompt.\\
28 & Processes diverse words including proper names, numbers, and multi-language words without a clear semantic pattern related to the prompt.\\
29 & Processes multiple-choice options, related letters/words, symbols, and words indicating absence or lack of choice, along with code-related terms.\\
30 & Processes single letters and short letter combinations, often related to answer options in multiple-choice questions, potentially for encoding or identification purposes.\\
31 & Processes technical and programming-related terms, including codes, abbreviations, identifiers, and symbols, along with miscellaneous and sometimes weather-related words.\\

\bottomrule
\end{tabular}
}
\vskip -0.1in
\end{table*}
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[h]
\centering
\caption{The result of Reasoning Stage Analysis on Qwen2.5-7B
}
\label{tab:Reasoning_Stage_Analysis_qwen}
\vskip 0.15in
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{cp{18cm}}
\toprule
 \textbf{Layer} & \textbf{Functions} \\ 
\midrule
 0 & The layer processes words and phrases related to diverse topics such as development, business, emotions, research, strategy, fundamentals, consumer matters, sports, diseases, locations, etc., in multiple languages including Chinese. \\ 
 1 & Processes programming and code-related elements such as symbols, special characters, code strings, text encodings, and multilingual words and characters from different languages and coding/encoding contexts.\\ 
 2 & Processes diverse words, including Chinese and English, language fragments, symbols, partial words, and terms related to various concepts such as social development, news, medical conditions, challenges, and independence. \\ 
3 & Processes diverse words, characters, phrases, and symbols from multiple languages and various domains, covering a wide range of concepts.\\ 
4 & Processes various symbols, characters, codes, and multilingual words including special characters, programming terms, and code-like elements across different contexts and encodings. \\ 
5 & Processes diverse language elements including words, phrases, fragments, symbols, and characters from multiple languages and various domains without clear semantic focus or category. \\ 
6 & Processes technical terms, symbols, codes, and multilingual characters including programming strings, special characters, and text fragments from various technical and language contexts.\\
7 & Processes various characters, symbols, words from different languages, programming terms, code-like strings, and potentially encoding-related elements.\\
8 & Processes various characters, symbols, encodings, words from different languages (including Chinese and non-English), code-like sequences, programming-related content, and text fragments with or without clear semantic patterns related to diverse topics and coding contexts.\\
9 & Processes various characters, words, and symbols from different languages, including special and non-English ones, along with code-related strings, technical terms, and without clear semantic patterns..\\
10 & Processes diverse words including nouns, verbs, foreign language terms, encoded strings, and symbols from various semantic categories and languages.\\
11 & Processes diverse characters, partial words, symbols, code-like strings, and multilingual words related to various technical, medical, programming, and digital aspects.\\
12 & Processes diverse language elements including Chinese phrases related to satisfaction, business, etc., English words, symbols, and multilingual terms related to various concepts such as business, emotions, development, and competitions.\\
13 & Processes diverse characters, words, and symbols from multiple languages, including technical notations and terms without a clear semantic focus.\\
14 & Processes diverse characters, words, symbols, including those from different languages, special characters, code-like strings, and technical notations without clear semantic patterns.\\
15 & Processes words and phrases in multiple languages (including Chinese and Arabic), technical terms, symbols, and concepts related to business, satisfaction, development, and various other topics.\\
16 & Processes words and phrases in multiple languages related to business, development, social concepts, fundamentals, business philosophies, strategic layouts, and development opportunities, along with technical and medical terms.\\
17 & Processes diverse words including Chinese phrases, medical terms, and various social, technological, economic, and concept-related terms from different languages.\\
18 & Processes single letters, letter combinations, and words related to options, answer options in multiple-choice questions, boolean values, codes, abbreviations, and common words like "None".\\
19 & Processes diverse characters, words, symbols, code snippets, and multilingual elements including programming terms, without clear semantic patterns.\\
20 & Processes words from multiple languages (including Chinese, Arabic, Thai, Italian, Japanese, etc.) and various concepts such as business philosophy, comprehensive strength, development opportunities, news, and general terms.\\
21 & Processes various symbols, codes, characters (including special and non-English ones), words from different languages, and programming terms, often without clear semantic connection to the prompt.\\
22 & Processes various characters, symbols, and encoded content, including code-like elements, potentially from different languages and coding contexts with no clear semantic pattern related to the prompt.\\
23 & Processes diverse words, including partial words, foreign terms, and concept-related terms from different languages and covering various concepts such as scarcity, sophistication, business, development, and attributes.\\
24 & Processes diverse words including multilingual elements, technical terms, development concepts, qualities, general phrases, Chinese characters, and symbols related to various concepts.\\
25 & Processes various characters, symbols, and code-like elements, including those from different languages and programming/coding contexts.\\
26 & Processes common punctuation, numbers, and start/end tokens, along with special characters, common words, and digits for text structure and formatting.\\
27 & Processes diverse words, characters, and phrases from multiple languages, covering various concepts such as emotions, business, competition, technical terms, and programming elements.\\


\bottomrule
\end{tabular}
}
\vskip -0.1in
\end{table*}