\section{Related Work}
\subsection{Compostional Reasoning of LLMs}
To systematically evaluate the reasoning capabilities of LLMs~\cite{map_neo,llama3,qwen25,execrepobench,codearena,mdeval}, researchers have developed benchmarks encompassing multiple cognitive dimensions, including commonsense reasoning~\citep{ponti-etal-2020-xcopa,singh2021com2sense,onoe2021creak}, multi-hop reasoning~\citep{ho2020constructing, shi2022stepgame, trivedi2022musique}, and logical reasoning~\citep{saparov2023language,han2022folio,masry2022chartqa}. However, LLMs still face severe challenges when faced with CR tasks that require cross-domain integration~\citep{dziri2024faith, press2022measuring}. Early research in CR primarily employed CoT with compositional queries to guide model reasoning~\citep{zhou2022least, drozdov2022compositional}. Existing studies on CR largely utilize multi-hop reasoning benchmarks to investigate models' internal reasoning mechanisms~\citep{sakarvadia2023memory, ghandeharioun2024patchscope, li2024understanding}. This study proposes a novel framework aimed at enabling an effective combination of diverse reasoning tasks, thereby comprehensively assessing models' CR capabilities.

\subsection{Mechanistic Analysis Methods}
To further study internal reasoning mechanisms of LLM, many researchers focus on the changes in hidden states during the reasoning process. Logit lens~\citep{logitlens} is a typical analytical tool, which extracts the hidden states between layers to obtain probability distribution of each layer. Researchers use logit lens to analyze the performance of LLMs in multilingual settings~\citep{LanguageMoel} and the analysis of compositional reasoning~\citep{CompositionalReasoning}, aiming to study the implicit results in reasoning process. The research about Retrieval Head~\citep{Wu2024RetrievalHM} proposes an attention-based interpretability analysis method based on the Retrieval Head in the context, providing a visual analysis of the attention distributions during reasoning process. Meanwhile, the OpenAI team introduced a method using LLMs to generate behavior explanations through neuron activation~\citep{bills2023language}, allowing the behavior of pivotal neurons to be more intuitively presented to researchers.