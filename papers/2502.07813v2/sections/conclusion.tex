\section{Conclusion}
In this paper, we propose a bench-free compositional reasoning task generation framework \ourmethod{} and develop a CR evaluation benchmark \benchmark{}, which includes 21 sub-datasets and nearly 7K cases collected from multiple domains. We conduct several experiments on 20+ open-source and closed-source LLMs to demonstrate that there is still a huge gap between open-source models and closed-source LLMs, which shows significant performance in CR ability for o1, with an accuracy rate of 83.7\% on \benchmark{}. Furthermore, we conduct logit lens, neuron activation analysis and reasoning stage analysis to describe the inner mechanism of CR. In the future, we plan to continuously update and expand the dataset to support a wider variety of problem types. We also aim to incorporate dynamic encoding methods (e.g., Huffman coding, RSA encryption) and extend \benchmark{} to multi-modal datasets, driving further advancements in this field.