\begin{abstract}
The compositional reasoning ability has long been regarded as critical to the generalization and intelligence emergence~\citep{CompositionalReasoning,wang2024grokked} of large language models (\textbf{LLMs}). 
However, despite numerous reasoning-related benchmarks~\citep{gui2024logicgame,zebralogicbench2024,ma2024kor}, the compositional reasoning capacity of LLMs is rarely studied or quantified in the existing benchmarks.
In this paper, we introduce \textbf{\ourmethod{}}, an evaluation framework that, for the first time, combines existing benchmarks and cryptographic, to quantify the compositional reasoning capacity of LLMs.
Building upon \ourmethod{}, we construct \textbf{\benchmark{}}, which integrates these principles into several benchmarks for systematic evaluation.
We conduct detailed experiments on widely used open-source and closed-source LLMs using \benchmark{}, revealing a huge gap between open-source and closed-source LLMs.
We further conduct thorough mechanical interpretability experiments to reveal the inner mechanism of LLMs' compositional reasoning, involving subproblem decomposition, subproblem inference, and summarizing subproblem conclusions. 
Through analysis based on \benchmark{}, we highlight the value of independently studying compositional reasoning and emphasize the need to enhance the compositional reasoning abilities of LLMs.
\end{abstract}
