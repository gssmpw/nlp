% This class has a lot of options, so please check deepmind.cls for more details. 
% This is a minimal set for most needs.
%\documentclass[11pt, a4paper, logo, twocolumn, internal, copyright, nonumbering]{deepmind}
%\documentclass[11pt, a4paper, logo, twocolumn, internal, copyright]{deepmind}
\documentclass[11pt, a4paper, logo, twocolumn, copyright]{googledeepmind}

% Omit dates for reproducibility.
\pdfinfoomitdate 1
\pdftrailerid{redacted}

% This avoids duplicate hyperref bookmark entries when using \bibentry (e.g. via \citeas).
\makeatletter
\renewcommand\bibentry[1]{\nocite{#1}{\frenchspacing\@nameuse{BR@r@#1\@extra@b@citeb}}}
\makeatother

%\renewcommand*\backref[1]{\ifx#1\relax \else (page #1) \fi}

\usepackage{kantlipsum, lipsum}
\usepackage{dsfont}
\usepackage{dm-colors}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{nicefrac}
\algrenewcommand\algorithmicindent{0.7em}

% declaration of the new block
\algblock{ParFor}{EndParFor}
% customising the new block
\algnewcommand\algorithmicparfor{\textbf{parallel for}}
\algnewcommand\algorithmicpardo{\textbf{do}}
\algnewcommand\algorithmicendparfor{\textbf{end\ parallel for}}
\algrenewtext{ParFor}[1]{\algorithmicparfor\ #1\ \algorithmicpardo}
\algrenewtext{EndParFor}{\algorithmicendparfor}

% \newcommand{\algorithmautorefname}{Algorithm}

\newcommand\Tstrut{\rule{0pt}{2.6ex}}         % = `top' strut
\newcommand\Bstrut{\rule[-0.9ex]{0pt}{0pt}}   % = `bottom' strut

% for rotated headers in table
\newcolumntype{R}[2]{%
    >{\adjustbox{angle=#1,lap=\width-(#2)}\bgroup}%
    l%
    <{\egroup}%
}
\newcommand*\rot{\multicolumn{1}{R{45}{1em}}}

\newcommand{\Model}{DiLoCo}

% Sometimes you will get errors about pdflink ending up in diffrent position. Try this and 
% comment it out again when you are done with your document.
%\hypersetup{draft}

% Set the bibliography options here.
\usepackage[authoryear, sort&compress, round]{natbib} 
\usepackage{comment}

\newcommand{\alert}[1]{\textcolor{red}{#1}}
\newcommand{\todo}{\textcolor{red}{TODO}}
\newcommand{\mr}[1]{\textcolor{blue}{{\bf MR:}#1}}
\newcommand{\algorithmautorefname}{Algorithm}

\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\usepackage{subcaption}
\usepackage{url}
\def\UrlBreaks{\do\/\do-}
% Images will be looked for in this path, removes need for explicit path when including images.
\graphicspath{{figures/}}

% Important Information about your paper.
\title{Eager Updates For Overlapped Communication and Computation in DiLoCo}

% Can leave this option out if you do not wish to add a corresponding author.
\correspondingauthor{douillard@google.com}

% Remove these if they are not needed 
\keywords{eager updates, distributed learning, large-scale} 
%\paperurl{deepmind.com/papers/2019/dm001.pdf}

% Use the internally issued paper ID, if there is one
\reportnumber{} % Leave blank if n/a 

% Assign your own date to the report. 
% Can comment out if not needed or leave blank if n/a.
\renewcommand{\today}{} 

% Can have as many authors and as many affiliations as needed. Best to indicate joint 
% first-authorship as shown below.
\author[*,$\dagger$,2]{Satyen Kale}
\author[*,1]{Arthur Douillard}
\author[1]{Yanislav Donchev}

\affil[1]{Google DeepMind}
\affil[2]{Google Research}
\affil[*]{Equal core contributions}
\affil[$\dagger$]{Currently at Apple.}



\begin{abstract}
Distributed optimization methods such as DiLoCo have been shown to be effective in training very large models across multiple distributed workers, such as datacenters. These methods split updates into two parts: an \emph{inner optimization} phase, where the workers independently execute multiple optimization steps on their own local data, and an \emph{outer optimization} step, where the inner updates are synchronized. While such approaches require orders of magnitude less communication than standard data-parallel training, in settings where the workers are datacenters, even the limited communication requirements of these approaches can still cause significant slow downs due to the blocking necessary at each outer optimization step. In this paper, we investigate techniques to mitigate this issue by overlapping communication with computation in a manner that allows the outer optimization step to fully overlap with the inner optimization phase. We show that a particular variant, dubbed \emph{eager} updates, provides competitive performance with standard DiLoCo in settings with low bandwidth between workers.
\end{abstract}


\begin{document}

\maketitle

\input{content}
% \input{model}
% \input{experiments}
% \input{related_work}
% \input{limitations}
% \input{conclusions}

% Bibliography components
\clearpage
\section*{Acknowledgements}

We thank Arthur Szlam and Marc'Aurelio Ranzato for advices and general support. We also thank Gabriel Teston, Zachary Charles, Zachary Garrett, and Keith Rush for providing engineering support. Finally, we thank Jeff Dean, Raia Hadsell, and Koray Kavukcuoglu for leadership support.

\bibliographystyle{plainnat}
\nobibliography*
\bibliography{main}

% Some other useful sections you might consider having in your report.
% If you add a bibtex entry of your own paper (this paper), you can
% show its full citation inline using \citeas, as above. Note that
% this citation removes the trailing full stop. To make \citeas work,
% you need to load the bibliography data. This can be done in two
% ways:
%
%    1. If you already have a printed bibliography with \bibliography{...},
%       then add the command "\nobibliography*", no arguments, before that.
%    2. If you don't otherwise print a bibliography, add the command
%       \nobibliography{...} at the end of your document.

%\newpage
% \section*{Acknowledgements}
% We would like to thank Ross Hemsley, Bo Liu, Amal Rannen-Triki, and Jack Rae for their valuable feedback.
\clearpage
\input{appendix}


\end{document}
