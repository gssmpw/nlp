\section{Related Works}
\label{sec:related_works}



\paragraph{Federated learning / local SGD.}  Differing from model merging's single combination step, Federated Averaging (FedAvg) ____ and Local SGD ____ iteratively combine models with the goal of minimizing bandwidth requirements. They operate by performing local training, typically using SGD, across workers for a certain number of steps before implementing some form of worker parameter synchronization or parameter aggregation.  In their original formulations, both FedAvg and Local SGD employed a straightforward average of parameters across workers.  As demonstrated by ____, synchronization becomes more effective when each worker computes a ``model delta,'' which are then aggregated to produce a pseudo-gradient, also termed an \textit{outer gradient}, subsequently utilized by a first-order optimizer ____.  This yields a bi-level optimization framework with inner optimizers and an outer optimizer, referred to as FedOpt by ____, who propose using SGD as the inner optimizer and adaptive techniques such as Adam____ as the outer optimizer in resource-constrained Federated Learning settings.

\paragraph{Distributed training for LLMs.} The increasing computational demands of training large language models (LLMs) have accelerated the need for distributed methodologies, applicable to both inference ____ and training ____. More recently, DiLoCo ____ introduced a specific instantiation of FedOpt ____ utilizing AdamW ____ as the inner optimizer and Nesterov ____ as the outer optimizer ____. This simple formulation has proven effective for distributed training with LLMs, particularly in scenarios with a limited number of replicas (under 100) and without replica sampling, aligning more closely with cross-silo federated learning ____. The FedOpt algorithm has also been demonstrated to be effective in training LLMs in settings that resemble cross-device federated learning____. The empirical effectiveness of DiLoCo has been reproduced in multiple studies ____ and successfully scaled to models with 10 billion parameters ____.  In related research, a minor modification to the way the outer Nesterov accumulates outer gradients has shown improved handling of asynchronicity among workers with different processing speeds ____.  DiLoCo provides an additional axis of parallelism to distributed training ____ and is compatible ____ with other existing parallelization approaches such as FSDP ____, or even another layer of federated learning ____. More recently, ____ proposed Streaming DiLoCo where only a subset of the parameters are shared at each given synchronization round, in effect lowering the peak required bandwidth.

\paragraph{Overlapping Communication.} Overlapping communication with computation is critical in many aspects of distributed training in order to minimize the time waiting for communication, and thus maximizing computation. Methods have been designed to minimize the ``bubble-of-time'' in pipeline parallelism ____, in data-parallel ____, and federated learning ____. In particular, in the latter case of federated learning, it is particularly useful to handle the case of ``stragglers'' where some replicas are slower than others ____.