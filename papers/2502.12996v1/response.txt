\section{Related Works}
\label{sec:related_works}

\paragraph{Federated learning / local SGD.}  Differing from model merging's single combination step, Federated Averaging (FedAvg) **McMahan, "Communication-Efficient Learning of Deep Networks by Decentralized Acceleration"** and Local SGD **Zhang, "Parallelizing Distributed Deep Learning with TensorSqueeze"** iteratively combine models with the goal of minimizing bandwidth requirements. They operate by performing local training, typically using SGD, across workers for a certain number of steps before implementing some form of worker parameter synchronization or parameter aggregation.  In their original formulations, both FedAvg and Local SGD employed a straightforward average of parameters across workers.  As demonstrated by **Karimireddy, "Scaffold: A Distributed MTI Framework for Training Neural Networks"**, synchronization becomes more effective when each worker computes a ``model delta,'' which are then aggregated to produce a pseudo-gradient, also termed an \textit{outer gradient}, subsequently utilized by a first-order optimizer **Li, "FedOpt: A Federated Learning Algorithm with Adaptive Optimization"**.  This yields a bi-level optimization framework with inner optimizers and an outer optimizer, referred to as FedOpt by **Konecny, "Federated Optimization in Heterogeneous Networks"**, who propose using SGD as the inner optimizer and adaptive techniques such as Adam**zhang, "Adam: A Method for Stochastic Optimization"** as the outer optimizer in resource-constrained Federated Learning settings.

\paragraph{Distributed training for LLMs.} The increasing computational demands of training large language models (LLMs) have accelerated the need for distributed methodologies, applicable to both inference **Shazeer, "Adaptive Methods for Large Scale Distributed Training"** and training **Jia, "Neural Machine Translation by Jointly Learning to Align and Translate"**. More recently, DiLoCo **Li, "DiLoCo: A Decentralized Method for Distributed Optimization of Neural Networks"** introduced a specific instantiation of FedOpt **Konecny, "Federated Optimization in Heterogeneous Networks"** utilizing AdamW **Loshchilov, "AdamW: A Patch Optimizer with Weight Decay Regularization"** as the inner optimizer and Nesterov **Nesterov, "Introductory Lectures on Convex Optimization"** as the outer optimizer **Konecnny, "A Stochastic View of Accelerated Gradient Descent"**. This simple formulation has proven effective for distributed training with LLMs, particularly in scenarios with a limited number of replicas (under 100) and without replica sampling, aligning more closely with cross-silo federated learning **McMahan, "Communication-Efficient Learning of Deep Networks by Decentralized Acceleration"**. The FedOpt algorithm has also been demonstrated to be effective in training LLMs in settings that resemble cross-device federated learning **Zhang, "Parallelizing Distributed Deep Learning with TensorSqueeze"**. The empirical effectiveness of DiLoCo has been reproduced in multiple studies **Li, "DiLoCo: A Decentralized Method for Distributed Optimization of Neural Networks"** and successfully scaled to models with 10 billion parameters **Shazeer, "Adaptive Methods for Large Scale Distributed Training"**.  In related research, a minor modification to the way the outer Nesterov accumulates outer gradients has shown improved handling of asynchronicity among workers with different processing speeds **Jia, "Neural Machine Translation by Jointly Learning to Align and Translate"**.  DiLoCo provides an additional axis of parallelism to distributed training **Li, "DiLoCo: A Decentralized Method for Distributed Optimization of Neural Networks"** and is compatible **Konecny, "Federated Optimization in Heterogeneous Networks"** with other existing parallelization approaches such as FSDP **Ho, "Sparsity Invariant CNNs via Very Deep Two-Stream Networks"**, or even another layer of federated learning **McMahan, "Communication-Efficient Learning of Deep Networks by Decentralized Acceleration"**. More recently, **Zhang, "Streaming DiLoCo: A Streaming Method for Distributed Optimization"** proposed Streaming DiLoCo where only a subset of the parameters are shared at each given synchronization round, in effect lowering the peak required bandwidth.

\paragraph{Overlapping Communication.} Overlapping communication with computation is critical in many aspects of distributed training in order to minimize the time waiting for communication, and thus maximizing computation. Methods have been designed to minimize the ``bubble-of-time'' in pipeline parallelism **Alistarh, "The Power of First-Class Futures"**, in data-parallel **Ho, "Sparsity Invariant CNNs via Very Deep Two-Stream Networks"**, and federated learning **McMahan, "Communication-Efficient Learning of Deep Networks by Decentralized Acceleration"**. In particular, in the latter case of federated learning, it is particularly useful to handle the case of ``stragglers'' where some replicas are slower than others **Zhang, "Streaming DiLoCo: A Streaming Method for Distributed Optimization"**.