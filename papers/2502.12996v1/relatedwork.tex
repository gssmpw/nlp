\section{Related Works}
\label{sec:related_works}



\paragraph{Federated learning / local SGD.}  Differing from model merging's single combination step, Federated Averaging (FedAvg) \citep{mcmahan2017fedavg} and Local SGD \citep{stich2019local} iteratively combine models with the goal of minimizing bandwidth requirements. They operate by performing local training, typically using SGD, across workers for a certain number of steps before implementing some form of worker parameter synchronization or parameter aggregation.  In their original formulations, both FedAvg and Local SGD employed a straightforward average of parameters across workers.  As demonstrated by \cite{reddi2021adaptive}, synchronization becomes more effective when each worker computes a ``model delta,'' which are then aggregated to produce a pseudo-gradient, also termed an \textit{outer gradient}, subsequently utilized by a first-order optimizer ~\citep{reddi2021adaptive,ilharco2022patching}.  This yields a bi-level optimization framework with inner optimizers and an outer optimizer, referred to as FedOpt by \cite{reddi2021adaptive}, who propose using SGD as the inner optimizer and adaptive techniques such as Adam~\citep{kingma2014adam} as the outer optimizer in resource-constrained Federated Learning settings.

\paragraph{Distributed training for LLMs.} The increasing computational demands of training large language models (LLMs) have accelerated the need for distributed methodologies, applicable to both inference \citep{borzunov2023petals} and training \citep{presser2020stub,diskin2021distributedcollab,ryabinin2021moshpit}. More recently, DiLoCo \citep{douillard2023diloco} introduced a specific instantiation of FedOpt \citep{reddi2021adaptive} utilizing AdamW \citep{loshchilov2018adamw} as the inner optimizer and Nesterov \citep{sutskever2013nesterov} as the outer optimizer \citep{huo2020outernesterov}. This simple formulation has proven effective for distributed training with LLMs, particularly in scenarios with a limited number of replicas (under 100) and without replica sampling, aligning more closely with cross-silo federated learning \citep{kairouz2021advances}. The FedOpt algorithm has also been demonstrated to be effective in training LLMs in settings that resemble cross-device federated learning~\citep{charles2024towards}. The empirical effectiveness of DiLoCo has been reproduced in multiple studies \citep{jaghouar2024opendiloco,sani2024futurelargelanguagemodel} and successfully scaled to models with 10 billion parameters \citep{jaghouar2024intellect1}.  In related research, a minor modification to the way the outer Nesterov accumulates outer gradients has shown improved handling of asynchronicity among workers with different processing speeds \citep{liu2024asyncdiloco}.  DiLoCo provides an additional axis of parallelism to distributed training \citep{shoeybi2020megatronlmtrainingmultibillionparameter} and is compatible \citep{jaghouar2024intellect1} with other existing parallelization approaches such as FSDP \citep{zhao2023fsdp}, or even another layer of federated learning \citep{sani2024photonfederatedllmpretraining}. More recently, \cite{douillard2025streamingdiloco} proposed Streaming DiLoCo where only a subset of the parameters are shared at each given synchronization round, in effect lowering the peak required bandwidth.

\paragraph{Overlapping Communication.} Overlapping communication with computation is critical in many aspects of distributed training in order to minimize the time waiting for communication, and thus maximizing computation. Methods have been designed to minimize the ``bubble-of-time'' in pipeline parallelism \citep{narayanan2021pipedream2bw,qi2023zerobubblepipelineparallelism}, in data-parallel \citep{zhao2013butterfly,lin2020deepgradientcompressionreducing}, and federated learning \citep{xie2019asynchronous,liu2024asyncdiloco}. In particular, in the latter case of federated learning, it is particularly useful to handle the case of ``stragglers'' where some replicas are slower than others \citep{koh2006parallel, recht2011hogwild, dean2012large, lian2015asynchronous, diskin2021distributedcollab}.