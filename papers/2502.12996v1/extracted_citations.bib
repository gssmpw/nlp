@article{borzunov2023petals,
      title={Petals: Collaborative Inference and Fine-tuning of Large Models}, 
      author={Alexander Borzunov and Dmitry Baranchuk and Tim Dettmers and Max Ryabinin and Younes Belkada and Artem Chumachenko and Pavel Samygin and Colin Raffel},
      year={2023},
      journal=acl,
      url={https://arxiv.org/abs/2209.01188}, 
}

@article{charles2024towards,
  title={Towards federated foundation models: Scalable dataset pipelines for group-structured learning},
  author={Charles, Zachary and Mitchell, Nicole and Pillutla, Krishna and Reneer, Michael and Garrett, Zachary},
  journal={Advances in Neural Information Processing Systems},
  url={https://arxiv.org/abs/2307.09619},
  year={2024}
}

@article{dean2012large,
  title={Large scale distributed deep networks},
  author={Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc'aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and others},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}

@article{diskin2021distributedcollab,
    title={Distributed Deep Learning in Open Collaborations},
    author={Diskin, Michael and Bukhtiyarov, Alexey and Ryabinin, Max and Saulnier, Lucile and Lhoest, Quentin and Sinitsin, Anton and Popov, Dmitry and Pyrkin, Dmitry and Kashirin, Maxim and Borzunov, Alexander and Villanova del Moral, Albert and Mazur, Denis and Kobelev, Ilia and Jernite, Yacine and Wolf, Thomas and Pekhimenko, Gennady},
    journal=neurips,
    year={2021},
    url={https://arxiv.org/abs/2106.10207}
}

@article{douillard2023diloco,
  title={{DiLoCo}: Distributed Low-Communication Training of Language Models},
  author={Douillard, Arthur and Feng, Qixuan and Rusu, Andrei A. and Chhaparia, Rachita and Donchev, Yani and Kuncoro, Adhiguna and Ranzato, Marc'Aurelio and Szlam, Arthur and Shen, Jiajun},
  url={https://arXiv.org/abs/2311.08105},
  year={2024},
  journal=icmlws,
}

@article{douillard2025streamingdiloco,
    title={Streaming DiLoCo with overlapping communication: Towards a Distributed Free Lunch},
    author={Arthur Douillard and Yanislav Donchev and Keith Rush and Satyen Kale and Zachary Charles and Zachary Garrett and Gabriel Teston and Dave Lacey and Ross McIlroy and Jiajun Shen and Alexandre Ramé and Arthur Szlam and Marc'Aurelio Ranzato and Paul Barham},
    journal=arxiv,
    year={2025}
}

@article{huo2020outernesterov,
      title={Faster On-Device Training Using New Federated Momentum Algorithm}, 
      author={Zhouyuan Huo and Qian Yang and Bin Gu and Lawrence Carin. Heng Huang},
      year={2020},
      journal=arxiv,
      url={https://arxiv.org/abs/2002.02090}
}

@article{ilharco2022patching,
      title={Patching open-vocabulary models by interpolating weights}, 
      author={Gabriel Ilharco and Mitchell Wortsman and Samir Yitzhak Gadre and Shuran Song and Hannaneh Hajishirzi and Simon Kornblith and Ali Farhadi and Ludwig Schmidt},
      year={2022},
      journal=neurips,
}

@article{jaghouar2024intellect1,
      title={INTELLECT-1 Technical Report}, 
      author={Sami Jaghouar and Jack Min Ong and Manveer Basra and Fares Obeid and Jannik Straube and Michael Keiblinger and Elie Bakouch and Lucas Atkins and Maziyar Panahi and Charles Goddard and Max Ryabinin and Johannes Hagemann},
      year={2024},
      journal=arxiv,
      url={https://arxiv.org/abs/2412.01152}, 
}

@article{jaghouar2024opendiloco,
      title={OpenDiLoCo: An Open-Source Framework for Globally Distributed Low-Communication Training}, 
      author={Sami Jaghouar and Jack Min Ong and Johannes Hagemann},
      year={2024},
      journal=arxiv,
      url={https://arxiv.org/abs/2407.07852}, 
}

@article{kairouz2021advances,
  title={Advances and open problems in federated learning},
  author={Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and others},
  journal={Foundations and trends in machine learning},
  year={2021},
  url={https://arxiv.org/abs/1912.04977}
}

@article{kingma2014adam,
    author = {Diederik P. Kingma and Jimmy Ba},
    title = {Adam: A Method for Stochastic Optimization},
    year = {2014},
    journal=iclr,
    url={https://arxiv.org/abs/1412.6980}
}

@article{koh2006parallel,
  title={Parallel asynchronous particle swarm optimization},
  author={Koh, Byung-Il and George, Alan D and Haftka, Raphael T and Fregly, Benjamin J},
  journal={International journal for numerical methods in engineering},
  volume={67},
  number={4},
  pages={578--595},
  year={2006},
  publisher={Wiley Online Library}
}

@article{lian2015asynchronous,
  title={Asynchronous parallel stochastic gradient for nonconvex optimization},
  author={Lian, Xiangru and Huang, Yijun and Li, Yuncheng and Liu, Ji},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@misc{lin2020deepgradientcompressionreducing,
      title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training}, 
      author={Yujun Lin and Song Han and Huizi Mao and Yu Wang and William J. Dally},
      year={2018},
      journal=iclr,
      url={https://arxiv.org/abs/1712.01887}, 
}

@article{liu2024asyncdiloco,
      title={Asynchronous Local-SGD Training for Language Modeling}, 
      author={Bo Liu and Rachita Chhaparia and Arthur Douillard and Satyen Kale and Andrei A. Rusu and Jiajun Shen and Arthur Szlam and Marc'Aurelio Ranzato},
      year={2024},
      journal=icmlws,
      url={https://arxiv.org/abs/2401.09135}, 
}

@article{loshchilov2018adamw,
    title={Decoupled Weight Decay Regularization},
    author={Ilya Loshchilov and Frank Hutter},
    year={2019},
    journal=iclr,
    url={https://arxiv.org/abs/1711.05101}
}

@article{mcmahan2017fedavg,
      title={Communication-Efficient Learning of Deep Networks from Decentralized Data}, 
      author={H. Brendan McMahan and Eider Moore and Daniel Ramage and Seth Hampson and Blaise Agüera y Arcas},
      year={2017},
      journal=aistats,
      url={https://arxiv.org/abs/1602.05629}
}

@article{narayanan2021pipedream2bw,
      title={Memory-Efficient Pipeline-Parallel DNN Training}, 
      author={Deepak Narayanan and Amar Phanishayee and Kaiyu Shi and Xie Chen and Matei Zaharia},
      year={2021},
      journal=icml,
      url={https://arxiv.org/abs/2006.09503}, 
}

@misc{presser2020stub,
      title={Swarm Training}, 
      author={Shawn Presser},
      year={2020},
      url={https://battle.shawwn.com/swarm-training-v01a.pdf},
}

@article{qi2023zerobubblepipelineparallelism,
      title={Zero Bubble Pipeline Parallelism}, 
      author={Penghui Qi and Xinyi Wan and Guangxing Huang and Min Lin},
      year={2024},
      journal=iclr,
      url={https://arxiv.org/abs/2401.10241}, 
}

@article{recht2011hogwild,
  title={Hogwild!: A lock-free approach to parallelizing stochastic gradient descent},
  author={Recht, Benjamin and Re, Christopher and Wright, Stephen and Niu, Feng},
  journal={Advances in neural information processing systems},
  volume={24},
  year={2011},
  url={https://arxiv.org/abs/1106.5730}
}

@article{reddi2021adaptive,
      title={Adaptive Federated Optimization}, 
      author={Sashank Reddi and Zachary Charles and Manzil Zaheer and Zachary Garrett and Keith Rush and Jakub Konečný and Sanjiv Kumar and H. Brendan McMahan},
      year={2021},
      journal=iclr,
      url={https://arxiv.org/abs/2003.00295}
      
}

@article{sani2024futurelargelanguagemodel,
      title={The Future of Large Language Model Pre-training is Federated}, 
      author={Lorenzo Sani and Alex Iacob and Zeyu Cao and Bill Marino and Yan Gao and Tomas Paulik and Wanru Zhao and William F. Shen and Preslav Aleksandrov and Xinchi Qiu and Nicholas D. Lane},
      year={2024},
      journal=arxiv,
      url={https://arxiv.org/abs/2405.10853}, 
}

@article{sani2024photonfederatedllmpretraining,
      title={Photon: Federated LLM Pre-Training}, 
      author={Lorenzo Sani and Alex Iacob and Zeyu Cao and Royson Lee and Bill Marino and Yan Gao and Dongqi Cai and Zexi Li and Wanru Zhao and Xinchi Qiu and Nicholas D. Lane},
      year={2024},
      journal=arxiv,
      url={https://arxiv.org/abs/2411.02908}, 
}

@article{shoeybi2020megatronlmtrainingmultibillionparameter,
      title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism}, 
      author={Mohammad Shoeybi and Mostofa Patwary and Raul Puri and Patrick LeGresley and Jared Casper and Bryan Catanzaro},
      year={2020},
      journal=arxiv,
      url={https://arxiv.org/abs/1909.08053}, 
}

@article{stich2019local,
    title={Local {SGD} Converges Fast and Communicates Little},
    author={Sebastian U. Stich},
    year={2019},
    journal=iclr,
    url={https://arxiv.org/abs/1805.09767}
}

@article{sutskever2013nesterov,
    author = {Ilya Sutskever and James Martens and George Dahl and Geoffrey Hinton},
    title = {On the importance of initialization and momentum in deep learning},
    year = {2013},
    journal=icml,
    url={https://proceedings.mlr.press/v28/sutskever13.html}
}

@article{xie2019asynchronous,
  title={Asynchronous federated optimization},
  author={Xie, Cong and Koyejo, Sanmi and Gupta, Indranil},
  journal=arxiv,
  year={2019},
  url={https://arxiv.org/abs/1903.03934}
}

@article{zhao2013butterfly,
author = {Huasha Zhao and John Canny},
title = {Butterfly Mixing: Accelerating Incremental-Update Algorithms on Clusters},
journal = {Proceedings of the 2013 SIAM International Conference on Data Mining (SDM)},

URL = {https://epubs.siam.org/doi/abs/10.1137/1.9781611972832.87},
year={2013}
}

@misc{zhao2023fsdp,
      title={PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel}, 
      author={Yanli Zhao and Andrew Gu and Rohan Varma and Liang Luo and Chien-Chin Huang and Min Xu and Less Wright and Hamid Shojanazeri and Myle Ott and Sam Shleifer and Alban Desmaison and Can Balioglu and Pritam Damania and Bernard Nguyen and Geeta Chauhan and Yuchen Hao and Ajit Mathews and Shen Li},
      year={2023},
      journal=arxiv,
      url={https://arxiv.org/abs/2304.11277}, 
}

