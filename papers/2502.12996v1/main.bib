% - Vision
@string{bmvc={Proceedings of the British Machine Vision Conference (BMVC)}}
@string{cviu={Computer Vision and Image Understanding (CVIU)}}
@string{cvpr={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}}
@string{cvprws={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshop}}
@string{eccv={Proceedings of the IEEE European Conference on Computer Vision (ECCV)}}
@string{eccvws={Proceedings of the IEEE European Conference on Computer Vision (ECCV) Workshop}}
@string{iccv={Proceedings of the IEEE International Conference on Computer Vision (ICCV)}}
@string{iccvws={Proceedings of the IEEE International Conference on Computer Vision (ICCV) Workshop}}
@string{icip={Proceedings of the IEEE International Conference on Image Processing (ICIP)}}
@string{accv={Asian Conference on Computer Vision}}
@string{ijcv={International Journal of Computer Vision (IJCV)}}
@string{ijcnn={International Joint Conference on Neural Networks}}
@string{pr={Pattern Recognition}}
@string{wacv={Proceedings of the IEEE Winter Conference on Application of Computer Vision (WACV)}}
@string{icpr={International Conference on Pattern Recognition}}

% - NLP
@string{emnlp={Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)}}
@string{naacl={Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)}}
@string{acl={Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL Short Papers)}}
@string{coling={International Conference on Computational Linguistics (COLING)}}
@string{ecir={European Conference on Information Retrieval (ECIR)}}

% - Generalist
@string{arxiv={arXiv preprint library}}
@string{iclr={Proceedings of the International Conference on Learning Representations (ICLR)}}
@string{icml={International Conference on Machine Learning (ICML)}}
@string{icmlws={International Conference on Machine Learning (ICML) Workshop}}
@string{neurips={Advances in Neural Information Processing Systems (NeurIPS)}}
@string{neuripsws={Advances in Neural Information Processing Systems (NeurIPS) Workshop}}
@string{tpami={IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)}}
@string{aaai={Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)}}
@string{pnas={Proceedings of the National Academy of Sciences}}
@string{jmlr={Journal of Machine Learning Research}}
@string{icann={International Conference on Artificial Neural Networks}}
@string{sensors={Sensors}}
@string{ijcai={International Joint Conference on Artificial Intelligence}}

% - Misc
@string{sigir={ACM Conference on Research and Development in Information Retrieval (SIGIR)}}
@string{trendcogsci={Trends in cognitive sciences}}
@string{corl={Annual Conference on Robot Learning (CORL)}}
@string{uai={Conference on Uncertainty in Artificial Intelligence}}
@string{spl={IEEE Signal Processing Letters}}
@string{tkde={IEEE Transactions on Knowledge and Data Engineering}}
@string{miccai={International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)}}
@string{icassp={IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}}
@string{aistats={International Conference on Artificial Intelligence and Statistics (AISTATS)}}
@string{icra={IEEE International Conference on Robotics and Automation (ICRA)}}


@article{ryabinin2021moshpit,
    title={Moshpit SGD: Communication-Efficient Decentralized Training on Heterogeneous Unreliable Devices},
    author={Ryabinin, Max and Gorbunov, Eduard and Plokhotnyuk, Vsevolod and Pekhimenko, Gennady},
    journal=neurips,
    year={2021},
    url={https://arxiv.org/abs/2103.03239}
}

@article{diskin2021distributedcollab,
    title={Distributed Deep Learning in Open Collaborations},
    author={Diskin, Michael and Bukhtiyarov, Alexey and Ryabinin, Max and Saulnier, Lucile and Lhoest, Quentin and Sinitsin, Anton and Popov, Dmitry and Pyrkin, Dmitry and Kashirin, Maxim and Borzunov, Alexander and Villanova del Moral, Albert and Mazur, Denis and Kobelev, Ilia and Jernite, Yacine and Wolf, Thomas and Pekhimenko, Gennady},
    journal=neurips,
    year={2021},
    url={https://arxiv.org/abs/2106.10207}
}


@article{frankle2020linear,
      title={Linear Mode Connectivity and the Lottery Ticket Hypothesis}, 
      author={Jonathan Frankle and Gintare Karolina Dziugaite and Daniel M. Roy and Michael Carbin},
      year={2020},
      journal=icml,
      url={https://arxiv.org/abs/1912.05671}
}

@article{gao2022heterogeneousfed,
      title={A Survey on Heterogeneous Federated Learning}, 
      author={Dashan Gao and Xin Yao and Qiang Yang},
      year={2022},
      journal=arxiv,
}

@article{gu2023localsgdgeneralization,
      title={Why (and When) does Local SGD Generalize Better than SGD?}, 
      author={Xinran Gu and Kaifeng Lyu and Longbo Huang and Sanjeev Arora},
      year={2023},
      journal=iclr,
}

@article{gupta2023continualpretraining,
      title={Continual Pre-Training of Large Language Models: How to (re)warm your model?}, 
      author={Kshitij Gupta and Benjamin Thérien and Adam Ibrahim and Mats L. Richter and Quentin Anthony and Eugene Belilovsky and Irina Rish and Timothée Lesort},
      year={2023},
      journal=arxiv,
}

@article{gururangan2023scaling,
      title={Scaling Expert Language Models with Unsupervised Domain Discovery}, 
      author={Suchin Gururangan and Margaret Li and Mike Lewis and Weijia Shi and Tim Althoff and Noah A. Smith and Luke Zettlemoyer},
      year={2023},
      journal=arxiv,
}

@article{he2015resnet,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      journal=cvpr,
}

@article{hoffmann2022chinchilla,
      title={Training Compute-Optimal Large Language Models}, 
      author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
      year={2022},
      journal=neurips,
      url={https://arxiv.org/abs/2203.15556}
}

@article{huo2020outernesterov,
      title={Faster On-Device Training Using New Federated Momentum Algorithm}, 
      author={Zhouyuan Huo and Qian Yang and Bin Gu and Lawrence Carin. Heng Huang},
      year={2020},
      journal=arxiv,
      url={https://arxiv.org/abs/2002.02090}
}

@article{ilharco2022patching,
      title={Patching open-vocabulary models by interpolating weights}, 
      author={Gabriel Ilharco and Mitchell Wortsman and Samir Yitzhak Gadre and Shuran Song and Hannaneh Hajishirzi and Simon Kornblith and Ali Farhadi and Ludwig Schmidt},
      year={2022},
      journal=neurips,
}

@article{jin2023dataless,
      title={Dataless Knowledge Fusion by Merging Weights of Language Models}, 
      author={Xisen Jin and Xiang Ren and Daniel Preotiuc-Pietro and Pengxiang Cheng},
      year={2023},
      journal=iclr,
      url={https://arxiv.org/abs/2212.09849}
}


@article{jolicoeurmartineau2023papa,
      title={PopulAtion Parameter Averaging (PAPA)}, 
      author={Alexia Jolicoeur-Martineau and Emy Gervais and Kilian Fatras and Yan Zhang and Simon Lacoste-Julien},
      year={2023},
      journal=arxiv,
}

@article{jordan2023repair,
      title={REPAIR: REnormalizing Permuted Activations for Interpolation Repair}, 
      author={Keller Jordan and Hanie Sedghi and Olga Saukh and Rahim Entezari and Behnam Neyshabur},
      year={2023},
      journal=arxiv,
      url={https://arxiv.org/abs/2211.08403}
}


@article{kaddour2022stop,
      title={Stop Wasting My Time! Saving Days of ImageNet and BERT Training with Latest Weight Averaging}, 
      author={Jean Kaddour},
      year={2022},
      journal=neuripsws
}

@article{kandpal2023gittheta,
      title={Git-Theta: A Git Extension for Collaborative Development of Machine Learning Models}, 
      author={Nikhil Kandpal and Brian Lester and Mohammed Muqeeth and Anisha Mascarenhas and Monty Evans and Vishal Baskaran and Tenghao Huang and Haokun Liu and Colin Raffel},
      year={2023},
      journal=arxiv
}

@article{kingma2014adam,
    author = {Diederik P. Kingma and Jimmy Ba},
    title = {Adam: A Method for Stochastic Optimization},
    year = {2014},
    journal=iclr,
    url={https://arxiv.org/abs/1412.6980}
}

@article{li2022branchtrainmerge,
      title={Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models}, 
      author={Margaret Li and Suchin Gururangan and Tim Dettmers and Mike Lewis and Tim Althoff and Noah A. Smith and Luke Zettlemoyer},
      year={2022},
      journal=arxiv,
      url={https://arxiv.org/abs/2208.03306}
}

@article{Lin2020_localsgd,
    title={Don't Use Large Mini-batches, Use Local SGD},
    author={Tao Lin and Sebastian U. Stich and Kumar Kshitij Patel and Martin Jaggi},
    year={2020},
    journal=iclr,
    url={https://arxiv.org/abs/1808.07217}
}

@article{loshchilov2018adamw,
    title={Decoupled Weight Decay Regularization},
    author={Ilya Loshchilov and Frank Hutter},
    year={2019},
    journal=iclr,
    url={https://arxiv.org/abs/1711.05101}
}

@article{mcmahan2017fedavg,
      title={Communication-Efficient Learning of Deep Networks from Decentralized Data}, 
      author={H. Brendan McMahan and Eider Moore and Daniel Ramage and Seth Hampson and Blaise Agüera y Arcas},
      year={2017},
      journal=aistats,
      url={https://arxiv.org/abs/1602.05629}
}


@article{ortiz2021tradeoffs,
      title={Trade-offs of Local SGD at Scale: An Empirical Study}, 
      author={Jose Javier Gonzalez Ortiz and Jonathan Frankle and Mike Rabbat and Ari Morcos and Nicolas Ballas},
      year={2021},
      journal=arxiv
}

@misc{presser2020stub,
      title={Swarm Training}, 
      author={Shawn Presser},
      year={2020},
      url={https://battle.shawwn.com/swarm-training-v01a.pdf},
}

@article{c4,
    author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
    title = {Exploring the limits of transfer learning with a unified text-to-text transformer},
    year = {2020},
    journal=jmlr,
    url={https://arxiv.org/abs/1910.10683}
}


@article{rame2023diverse,
      title={Diverse Weight Averaging for Out-of-Distribution Generalization}, 
      author={Alexandre Ramé and Matthieu Kirchmeyer and Thibaud Rahier and Alain Rakotomamonjy and Patrick Gallinari and Matthieu Cord},
      year={2023},
      journal=neurips,
      url={https://arxiv.org/abs/2205.09739}
}

@article{rame2023rewarded,
      title={Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards}, 
      author={Alexandre Ramé and Guillaume Couairon and Mustafa Shukor and Corentin Dancette and Jean-Baptiste Gaya and Laure Soulier and Matthieu Cord},
      year={2023},
      journal=neurips,
      url={https://arxiv.org/abs/2306.04488}
}

@article{rebuffi2022revisiting,
      title={Revisiting adapters with adversarial training}, 
      author={Sylvestre-Alvise Rebuffi and Francesco Croce and Sven Gowal},
      year={2022},
      journal=iclr,
      url={https://arxiv.org/abs/2210.04886}
}

@article{reddi2021adaptive,
      title={Adaptive Federated Optimization}, 
      author={Sashank Reddi and Zachary Charles and Manzil Zaheer and Zachary Garrett and Keith Rush and Jakub Konečný and Sanjiv Kumar and H. Brendan McMahan},
      year={2021},
      journal=iclr,
      url={https://arxiv.org/abs/2003.00295}
      
}

@article{deng2009imagenet,
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  title={ImageNet: A large-scale hierarchical image database}, 
  year={2009},
  journal=cvpr
 }


@article{stich2019local,
    title={Local {SGD} Converges Fast and Communicates Little},
    author={Sebastian U. Stich},
    year={2019},
    journal=iclr,
    url={https://arxiv.org/abs/1805.09767}
}

@article{stoica2023zipit,
      title={ZipIt! Merging Models from Different Tasks without Training}, 
      author={George Stoica and Daniel Bolya and Jakob Bjorner and Taylor Hearn and Judy Hoffman},
      year={2023},
      journal=arxiv,
      url={https://arxiv.org/abs/2305.03053}
}

@article{sutskever2013nesterov,
    author = {Ilya Sutskever and James Martens and George Dahl and Geoffrey Hinton},
    title = {On the importance of initialization and momentum in deep learning},
    year = {2013},
    journal=icml,
    url={https://proceedings.mlr.press/v28/sutskever13.html}
}


@article{tang2023communicationefficient,
      title={Communication-Efficient Distributed Deep Learning: A Comprehensive Survey}, 
      author={Zhenheng Tang and Shaohuai Shi and Wei Wang and Bo Li and Xiaowen Chu},
      year={2023},
      journal=arxiv
}

@article{vaswani2017transformer,
  author       = {Ashish Vaswani and
                  Noam Shazeer and
                  Niki Parmar and
                  Jakob Uszkoreit and
                  Llion Jones and
                  Aidan N. Gomez and
                  Lukasz Kaiser and
                  Illia Polosukhin},
  title        = {Attention Is All You Need},
  year         = {2017},
  journal=neurips
}

@article{wang2020slowmo,
      title={SlowMo: Improving Communication-Efficient Distributed SGD with Slow Momentum}, 
      author={Jianyu Wang and Vinayak Tantia and Nicolas Ballas and Michael Rabbat},
      year={2020},
      journal=iclr
}

@article{wortsman2021learning,
    title={Learning Neural Network Subspaces}, 
    author={Mitchell Wortsman and Maxwell Horton and Carlos Guestrin and Ali Farhadi and Mohammad Rastegari},
    year={2021},
    journal=icml,
    url={https://arxiv.org/abs/2102.10472}
}

@article{wortsman2022lofi,
      title={lo-fi: distributed fine-tuning without communication}, 
      author={Mitchell Wortsman and Suchin Gururangan and Shen Li and Ali Farhadi and Ludwig Schmidt and Michael Rabbat and Ari S. Morcos},
      year={2022},
      journal=arxiv
}

@article{wortsman2022soup,
  title = 	 {Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},
  author =       {Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and Schmidt, Ludwig},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  year = 	 {2022},
  journal=icml,
  url={https://arxiv.org/abs/2203.05482}
}

@article{wortsman2022robust,
      title={Robust fine-tuning of zero-shot models}, 
      author={Mitchell Wortsman and Gabriel Ilharco and Jong Wook Kim and Mike Li and Simon Kornblith and Rebecca Roelofs and Raphael Gontijo-Lopes and Hannaneh Hajishirzi and Ali Farhadi and Hongseok Namkoong and Ludwig Schmidt},
      year={2022},
      journal=cvpr,
      url={https://arxiv.org/abs/2109.01903}
}

@article{yadav2023resolving,
      title={Resolving Interference When Merging Models}, 
      author={Prateek Yadav and Derek Tam and Leshem Choshen and Colin Raffel and Mohit Bansal},
      year={2023},
      journal=neurips
}

@article{zhang2019lookahead,
    title={Lookahead Optimizer: k steps forward, 1 step back},
    author={Michael R. Zhang and James Lucas and Geoffrey Hinton and Jimmy Ba},
    year={2019},
    journal=neurips
}

@article{douillard2023diloco,
  title={{DiLoCo}: Distributed Low-Communication Training of Language Models},
  author={Douillard, Arthur and Feng, Qixuan and Rusu, Andrei A. and Chhaparia, Rachita and Donchev, Yani and Kuncoro, Adhiguna and Ranzato, Marc'Aurelio and Szlam, Arthur and Shen, Jiajun},
  url={https://arXiv.org/abs/2311.08105},
  year={2024},
  journal=icmlws,
}

@article{wang2024fedpart,
      title={Why Go Full? Elevating Federated Learning Through Partial Network Updates}, 
      author={Haolin Wang and Xuefeng Liu and Jianwei Niu and Wenkai Guo and Shaojie Tang},
      year={2024},
      journal=neurips,
      url={https://arxiv.org/abs/2410.11559}, 
}

@article{wen2022feddropout,
      title={Federated Dropout -- A Simple Approach for Enabling Federated Learning on Resource Constrained Devices}, 
      author={Dingzhu Wen and Ki-Jun Jeon and Kaibin Huang},
      year={2022},
      journal={IEEE Wireless Communications Letters},
      url={https://arxiv.org/abs/2109.15258}, 
}

@article{yu2024dare,
      title={Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch}, 
      author={Le Yu and Bowen Yu and Haiyang Yu and Fei Huang and Yongbin Li},
      year={2024},
      journal=icml,
      url={https://arxiv.org/abs/2311.03099}, 
}

@article{yadav2023tiesmerging,
      title={TIES-Merging: Resolving Interference When Merging Models}, 
      author={Prateek Yadav and Derek Tam and Leshem Choshen and Colin Raffel and Mohit Bansal},
      year={2023},
      journal=neurips,
      url={https://arxiv.org/abs/2306.01708}, 
}

@article{agrawal2024exmy,
      title={eXmY: A Data Type and Technique for Arbitrary Bit Precision Quantization}, 
      author={Aditya Agrawal and Matthew Hedlund and Blake Hechtman},
      year={2024},
            journal=arxiv,
      url={https://arxiv.org/abs/2405.13938}, 
}

@article{wortsman2023smallscaleproxieslargescaletransformer,
      title={Small-scale proxies for large-scale Transformer training instabilities}, 
      author={Mitchell Wortsman and Peter J. Liu and Lechao Xiao and Katie Everett and Alex Alemi and Ben Adlam and John D. Co-Reyes and Izzeddin Gur and Abhishek Kumar and Roman Novak and Jeffrey Pennington and Jascha Sohl-dickstein and Kelvin Xu and Jaehoon Lee and Justin Gilmer and Simon Kornblith},
      year={2023},
           journal=arxiv,
      url={https://arxiv.org/abs/2309.14322}, 
}

@article{jaghouar2024intellect1,
      title={INTELLECT-1 Technical Report}, 
      author={Sami Jaghouar and Jack Min Ong and Manveer Basra and Fares Obeid and Jannik Straube and Michael Keiblinger and Elie Bakouch and Lucas Atkins and Maziyar Panahi and Charles Goddard and Max Ryabinin and Johannes Hagemann},
      year={2024},
      journal=arxiv,
      url={https://arxiv.org/abs/2412.01152}, 
}

@article{henry2020querykeynormalization,
      title={Query-Key Normalization for Transformers}, 
      author={Alex Henry and Prudhvi Raj Dachapally and Shubham Pawar and Yuxuan Chen},
      year={2020},
    journal=emnlp,
      url={https://arxiv.org/abs/2010.04245}, 
}

@article{chowdhery2023palm,
  author  = {Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathy Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
  title   = {PaLM: Scaling Language Modeling with Pathways},
  journal = {Journal of Machine Learning Research},
  year    = {2023},
  url     = {http://jmlr.org/papers/v24/22-1144.html}
}

@article{gadre2024languagemodelsscalereliably,
      title={Language models scale reliably with over-training and on downstream tasks}, 
      author={Samir Yitzhak Gadre and Georgios Smyrnis and Vaishaal Shankar and Suchin Gururangan and Mitchell Wortsman and Rulin Shao and Jean Mercat and Alex Fang and Jeffrey Li and Sedrick Keh and Rui Xin and Marianna Nezhurina and Igor Vasiljevic and Jenia Jitsev and Luca Soldaini and Alexandros G. Dimakis and Gabriel Ilharco and Pang Wei Koh and Shuran Song and Thomas Kollar and Yair Carmon and Achal Dave and Reinhard Heckel and Niklas Muennighoff and Ludwig Schmidt},
      year={2024},
      journal=arxiv,
      url={https://arxiv.org/abs/2403.08540}, 
}

@misc{soldaini2024dolmao,
      title={Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research}, 
      author={Luca Soldaini and Rodney Kinney and Akshita Bhagia and Dustin Schwenk and David Atkinson and Russell Authur and Ben Bogin and Khyathi Chandu and Jennifer Dumas and Yanai Elazar and Valentin Hofmann and Ananya Harsh Jha and Sachin Kumar and Li Lucy and Xinxi Lyu and Nathan Lambert and Ian Magnusson and Jacob Morrison and Niklas Muennighoff and Aakanksha Naik and Crystal Nam and Matthew E. Peters and Abhilasha Ravichander and Kyle Richardson and Zejiang Shen and Emma Strubell and Nishant Subramani and Oyvind Tafjord and Pete Walsh and Luke Zettlemoyer and Noah A. Smith and Hannaneh Hajishirzi and Iz Beltagy and Dirk Groeneveld and Jesse Dodge and Kyle Lo},
      year={2024},
      journal=acl,
      url={https://arxiv.org/abs/2402.00159}, 
}

@misc{zhao2023fsdp,
      title={PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel}, 
      author={Yanli Zhao and Andrew Gu and Rohan Varma and Liang Luo and Chien-Chin Huang and Min Xu and Less Wright and Hamid Shojanazeri and Myle Ott and Sam Shleifer and Alban Desmaison and Can Balioglu and Pritam Damania and Bernard Nguyen and Geeta Chauhan and Yuchen Hao and Ajit Mathews and Shen Li},
      year={2023},
      journal=arxiv,
      url={https://arxiv.org/abs/2304.11277}, 
}

@article{beaumont2022weightoffloading,
    author={Olivier Beaumont and Lionel Eyraud-Dubois and Alena Shilova and Xunyi Zhao},
    title={Weight Offloading Strategies},
    journal={HAL repository},
    year={2022},
    url={https://inria.hal.science/hal-03580767/}
}

@article{liu2024asyncdiloco,
      title={Asynchronous Local-SGD Training for Language Modeling}, 
      author={Bo Liu and Rachita Chhaparia and Arthur Douillard and Satyen Kale and Andrei A. Rusu and Jiajun Shen and Arthur Szlam and Marc'Aurelio Ranzato},
      year={2024},
      journal=icmlws,
      url={https://arxiv.org/abs/2401.09135}, 
}

@article{jaghouar2024opendiloco,
      title={OpenDiLoCo: An Open-Source Framework for Globally Distributed Low-Communication Training}, 
      author={Sami Jaghouar and Jack Min Ong and Johannes Hagemann},
      year={2024},
      journal=arxiv,
      url={https://arxiv.org/abs/2407.07852}, 
}

@article{sani2024futurelargelanguagemodel,
      title={The Future of Large Language Model Pre-training is Federated}, 
      author={Lorenzo Sani and Alex Iacob and Zeyu Cao and Bill Marino and Yan Gao and Tomas Paulik and Wanru Zhao and William F. Shen and Preslav Aleksandrov and Xinchi Qiu and Nicholas D. Lane},
      year={2024},
      journal=arxiv,
      url={https://arxiv.org/abs/2405.10853}, 
}

@article{sani2024photonfederatedllmpretraining,
      title={Photon: Federated LLM Pre-Training}, 
      author={Lorenzo Sani and Alex Iacob and Zeyu Cao and Royson Lee and Bill Marino and Yan Gao and Dongqi Cai and Zexi Li and Wanru Zhao and Xinchi Qiu and Nicholas D. Lane},
      year={2024},
      journal=arxiv,
      url={https://arxiv.org/abs/2411.02908}, 
}

@article{rush2024drjax,
      title={DrJAX: Scalable and Differentiable MapReduce Primitives in JAX}, 
      author={Keith Rush and Zachary Charles and Zachary Garrett and Sean Augenstein and Nicole Mitchell},
      year={2024},
      journal=icmlws,
      url={https://arxiv.org/abs/2403.07128}, 
}

@article{wang2023cocktailsgd,
author = {Wang, Jue and Lu, Yucheng and Yuan, Binhang and Chen, Beidi and Liang, Percy and De Sa, Christopher and Re, Christopher and Zhang, Ce},
title = {CocktailSGD: fine-tuning foundation models over 500mbps networks},
year = {2023},
journal=icml,
url={https://openreview.net/forum?id=w2Vrl0zlzA}
}

@article{kaplan2020scalinglawsneurallanguage,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      journal=arxiv,
      url={https://arxiv.org/abs/2001.08361}, 
}

@article{vogels2020powersgd,
      title={PowerSGD: Practical Low-Rank Gradient Compression for Distributed Optimization}, 
      author={Thijs Vogels and Sai Praneeth Karimireddy and Martin Jaggi},
      year={2019},
      journal=neurips,
      url={https://arxiv.org/abs/1905.13727}, 
}

@article{krizhevsky2012alexnet,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
title = {ImageNet classification with deep convolutional neural networks},
year = {2012},
journal=neurips,
url={https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html}
}

@article{zellers2019hellaswagmachinereallyfinish,
      title={HellaSwag: Can a Machine Really Finish Your Sentence?}, 
      author={Rowan Zellers and Ari Holtzman and Yonatan Bisk and Ali Farhadi and Yejin Choi},
      year={2019},
      journal=acl,
      url={https://arxiv.org/abs/1905.07830}, 
}

@article{bisk2019piqareasoningphysicalcommonsense,
      title={PIQA: Reasoning about Physical Commonsense in Natural Language}, 
      author={Yonatan Bisk and Rowan Zellers and Ronan Le Bras and Jianfeng Gao and Yejin Choi},
      year={2020},
      journal=aaai,
      url={https://arxiv.org/abs/1911.11641}, 
}

@misc{clark2018arc,
    title={Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
    author={Peter Clark and Isaac Cowhey and Oren Etzioni and Tushar Khot and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
    year={2018},
    journal=arxiv,
    url={https://arxiv.org/abs/1803.05457v1}
}

@article{peng2024demod,
      title={DeMo: Decoupled Momentum Optimization}, 
      author={Bowen Peng and Jeffrey Quesnelle and Diederik P. Kingma},
      year={2024},
      journal=arxiv,
      url={https://arxiv.org/abs/2411.19870}, 
}

@misc{lin2020deepgradientcompressionreducing,
      title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training}, 
      author={Yujun Lin and Song Han and Huizi Mao and Yu Wang and William J. Dally},
      year={2018},
      journal=iclr,
      url={https://arxiv.org/abs/1712.01887}, 
}

@software{nanodo,
  author = {Peter J. Liu and Roman Novak and Jaehoon Lee and Mitchell Wortsman and Lechao Xiao and Katie Everett and Alexander A. Alemi and  Mark Kurzeja and Pierre Marcenac and Izzeddin Gur and Simon Kornblith and Kelvin Xu and Gamaleldin Elsayed and Ian Fischer and Jeffrey Pennington and Ben Adlam and Jascha-Sohl Dickstein},
  title = {NanoDO: A minimal Transformer decoder-only language model implementation in {JAX}.},
  url = {http://github.com/google-deepmind/nanodo},
  version = {0.1.0},
  year = {2024},
}

@article{fournier2024washtrainensemblecommunicationefficient,
      title={WASH: Train your Ensemble with Communication-Efficient Weight Shuffling, then Average}, 
      author={Louis Fournier and Adel Nabli and Masih Aminbeidokhti and Marco Pedersoli and Eugene Belilovsky and Edouard Oyallon},
      year={2024},
      journal=neuripsws,
      url={https://arxiv.org/abs/2405.17517}, 
}

@article{exo2025sparta,
    title={Sparta},
    author={Mohamed Baioumy and Alex Cheema},
    journal={blog.exolabs.net},
    url={https://blog.exolabs.net/day-12/},
    year={2025}
}

@article{douillard2024dipaco,
      title={DiPaCo: Distributed Path Composition}, 
      author={Arthur Douillard and Qixuan Feng and Andrei A. Rusu and Adhiguna Kuncoro and Yani Donchev and Rachita Chhaparia and Ionel Gog and Marc'Aurelio Ranzato and Jiajun Shen and Arthur Szlam},
      year={2024},
      journal=arxiv,
      url={https://arxiv.org/abs/2403.10616}, 
}

@misc{arivazhagan2019federatedlearningpersonalizationlayers,
      title={Federated Learning with Personalization Layers}, 
      author={Manoj Ghuhan Arivazhagan and Vinay Aggarwal and Aaditya Kumar Singh and Sunav Choudhary},
      year={2019},
      eprint={1912.00818},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1912.00818}, 
}

@article{zhao2024galore,
      title={GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection}, 
      author={Jiawei Zhao and Zhenyu Zhang and Beidi Chen and Zhangyang Wang and Anima Anandkumar and Yuandong Tian},
      year={2024},
      journal=icml,
      url={https://arxiv.org/abs/2403.03507}, 
}

@misc{huang2022crosssilofederatedlearningchallenges,
      title={Cross-Silo Federated Learning: Challenges and Opportunities}, 
      author={Chao Huang and Jianwei Huang and Xin Liu},
      year={2022},
      eprint={2206.12949},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2206.12949}, 
}

@article{charles2024towards,
  title={Towards federated foundation models: Scalable dataset pipelines for group-structured learning},
  author={Charles, Zachary and Mitchell, Nicole and Pillutla, Krishna and Reneer, Michael and Garrett, Zachary},
  journal={Advances in Neural Information Processing Systems},
  url={https://arxiv.org/abs/2307.09619},
  year={2024}
}

@article{kairouz2021advances,
  title={Advances and open problems in federated learning},
  author={Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and others},
  journal={Foundations and trends in machine learning},
  year={2021},
  url={https://arxiv.org/abs/1912.04977}
}

@article{borzunov2023petals,
      title={Petals: Collaborative Inference and Fine-tuning of Large Models}, 
      author={Alexander Borzunov and Dmitry Baranchuk and Tim Dettmers and Max Ryabinin and Younes Belkada and Artem Chumachenko and Pavel Samygin and Colin Raffel},
      year={2023},
      journal=acl,
      url={https://arxiv.org/abs/2209.01188}, 
}

@article{shoeybi2020megatronlmtrainingmultibillionparameter,
      title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism}, 
      author={Mohammad Shoeybi and Mostofa Patwary and Raul Puri and Patrick LeGresley and Jared Casper and Bryan Catanzaro},
      year={2020},
      journal=arxiv,
      url={https://arxiv.org/abs/1909.08053}, 
}

@article{ainsworth2023gitrebasinmergingmodels,
      title={Git Re-Basin: Merging Models modulo Permutation Symmetries}, 
      author={Samuel K. Ainsworth and Jonathan Hayase and Siddhartha Srinivasa},
      year={2023},
      journal=arxiv,
      url={https://arxiv.org/abs/2209.04836}, 
}

@article{ryabinin2023swarmparallelismtraininglarge,
      title={SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient}, 
      author={Max Ryabinin and Tim Dettmers and Michael Diskin and Alexander Borzunov},
      year={2023},
      journal=icml,
      url={https://arxiv.org/abs/2301.11913}, 
}

@article{hooker2020hardwarelottery,
      title={The Hardware Lottery}, 
      author={Sara Hooker},
      year={2020},
      journal={Communication for the ACM},
      url={https://arxiv.org/abs/2009.06489}, 
}

@article{dean2021pathways,
    title={https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/},
    author={Jeff Dean},
    year={2021},
    url={https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/},
    journal={Google's blog}
}

@article{sutton2019bitterlesson,
    title={The Bitter Lesson},
    author={Rich Sutton},
    journal={incompleteideas.net},
    url={http://www.incompleteideas.net/IncIdeas/BitterLesson.html},
    year={2019}
}

@article{douillard2025streamingdiloco,
    title={Streaming DiLoCo with overlapping communication: Towards a Distributed Free Lunch},
    author={Arthur Douillard and Yanislav Donchev and Keith Rush and Satyen Kale and Zachary Charles and Zachary Garrett and Gabriel Teston and Dave Lacey and Ross McIlroy and Jiajun Shen and Alexandre Ramé and Arthur Szlam and Marc'Aurelio Ranzato and Paul Barham},
    journal=arxiv,
    year={2025}
}

@article{narayanan2021pipedream2bw,
      title={Memory-Efficient Pipeline-Parallel DNN Training}, 
      author={Deepak Narayanan and Amar Phanishayee and Kaiyu Shi and Xie Chen and Matei Zaharia},
      year={2021},
      journal=icml,
      url={https://arxiv.org/abs/2006.09503}, 
}

@article{qi2023zerobubblepipelineparallelism,
      title={Zero Bubble Pipeline Parallelism}, 
      author={Penghui Qi and Xinyi Wan and Guangxing Huang and Min Lin},
      year={2024},
      journal=iclr,
      url={https://arxiv.org/abs/2401.10241}, 
}

@article{zhao2013butterfly,
author = {Huasha Zhao and John Canny},
title = {Butterfly Mixing: Accelerating Incremental-Update Algorithms on Clusters},
journal = {Proceedings of the 2013 SIAM International Conference on Data Mining (SDM)},

URL = {https://epubs.siam.org/doi/abs/10.1137/1.9781611972832.87},
year={2013}
}

@article{xie2019asynchronous,
  title={Asynchronous federated optimization},
  author={Xie, Cong and Koyejo, Sanmi and Gupta, Indranil},
  journal=arxiv,
  year={2019},
  url={https://arxiv.org/abs/1903.03934}
}

@article{koh2006parallel,
  title={Parallel asynchronous particle swarm optimization},
  author={Koh, Byung-Il and George, Alan D and Haftka, Raphael T and Fregly, Benjamin J},
  journal={International journal for numerical methods in engineering},
  volume={67},
  number={4},
  pages={578--595},
  year={2006},
  publisher={Wiley Online Library}
}

@article{recht2011hogwild,
  title={Hogwild!: A lock-free approach to parallelizing stochastic gradient descent},
  author={Recht, Benjamin and Re, Christopher and Wright, Stephen and Niu, Feng},
  journal={Advances in neural information processing systems},
  volume={24},
  year={2011},
  url={https://arxiv.org/abs/1106.5730}
}

@article{dean2012large,
  title={Large scale distributed deep networks},
  author={Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc'aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and others},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}

@article{lian2015asynchronous,
  title={Asynchronous parallel stochastic gradient for nonconvex optimization},
  author={Lian, Xiangru and Huang, Yijun and Li, Yuncheng and Liu, Ji},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}