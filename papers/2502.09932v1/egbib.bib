@article{c1,
author = {A. Alpher},
title = {Frobnication},
journal = {Journal of Foo},
volume = 12,
number = 1,
pages = {234--778},
year = 2002
}

@article{c2,
author = {A. Alpher and  J.~P.~N. Fotheringham-Smythe},
title = {Frobnication revisited},
journal = {Journal of Foo},
volume = 13,
number = 1,
pages = {234--778},
year = 2003
}

@article{c3,
author = {A. Alpher and J.~P.~N. Fotheringham-Smythe and G. Gamow},
title = {Can a machine frobnicate?},
journal = {Journal of Foo},
volume = 14,
number = 1,
pages = {234--778},
year = 2004
}
@article{fsr1,
  title={Guided depth map super-resolution: A survey},
  author={Zhong, Zhiwei and Liu, Xianming and Jiang, Junjun and Zhao, Debin and Ji, Xiangyang},
  journal={ACM Computing Surveys},
  volume={55},
  number={14s},
  pages={1--36},
  year={2023},
  publisher={ACM New York, NY}
}
@article{fsr2,
  title={Deep learning-based human pose estimation: A survey},
  author={Zheng, Ce and Wu, Wenhan and Chen, Chen and Yang, Taojiannan and Zhu, Sijie and Shen, Ju and Kehtarnavaz, Nasser and Shah, Mubarak},
  journal={ACM Computing Surveys},
  volume={56},
  number={1},
  pages={1--37},
  year={2023},
  publisher={ACM New York, NY}
}
@inproceedings{fsr3,
  title={Hybrid pixel-unshuffled network for lightweight image super-resolution},
  author={Sun, Bin and Zhang, Yulun and Jiang, Songyao and Fu, Yun},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={37},
  number={2},
  pages={2375--2383},
  year={2023}
}
@inproceedings{fsr4,
  title={Learning generalizable latent representations for novel degradations in super-resolution},
  author={Li, Fengjun and Feng, Xin and Chen, Fanglin and Lu, Guangming and Pei, Wenjie},
  booktitle={Proceedings of the 30th ACM International Conference on Multimedia},
  pages={1797--1807},
  year={2022}
}
@article{fsr5,
  title={Deep learning-based face super-resolution: A survey},
  author={Jiang, Junjun and Wang, Chenyang and Liu, Xianming and Ma, Jiayi},
  journal={ACM Computing Surveys (CSUR)},
  volume={55},
  number={1},
  pages={1--36},
  year={2021},
  publisher={ACM New York, NY}
}
@inproceedings{fsr6,
  title={Learning face hallucination in the wild},
  author={Zhou, Erjin and Fan, Haoqiang and Cao, Zhimin and Jiang, Yuning and Yin, Qi},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={29},
  number={1},
  year={2015}
}
@inproceedings{fsr7,
  title={Ultra-resolving face images by discriminative generative networks},
  author={Yu, Xin and Porikli, Fatih},
  booktitle={European conference on computer vision},
  pages={318--333},
  year={2016},
  organization={Springer}
}
@inproceedings{fsr8,
  title={Wavelet-srnet: A wavelet-based cnn for multi-scale face super resolution},
  author={Huang, Huaibo and He, Ran and Sun, Zhenan and Tan, Tieniu},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1689--1697},
  year={2017}
}
@inproceedings{fsr9,
  title={Image super-resolution using very deep residual channel attention networks},
  author={Zhang, Yulun and Li, Kunpeng and Li, Kai and Wang, Lichen and Zhong, Bineng and Fu, Yun},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={286--301},
  year={2018}
}
@inproceedings{fsr10,
  title={Second-order attention network for single image super-resolution},
  author={Dai, Tao and Cai, Jianrui and Zhang, Yongbing and Xia, Shu-Tao and Zhang, Lei},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={11065--11074},
  year={2019}
}
@inproceedings{fsr11,
  title={Face hallucination via split-attention in split-attention network},
  author={Lu, Tao and Wang, Yuanzhi and Zhang, Yanduo and Wang, Yu and Wei, Liu and Wang, Zhongyuan and Jiang, Junjun},
  booktitle={Proceedings of the 29th ACM international conference on multimedia},
  pages={5501--5509},
  year={2021}
}
@article{fsr12,
  title={Dual-path deep fusion network for face image hallucination},
  author={Jiang, Kui and Wang, Zhongyuan and Yi, Peng and Lu, Tao and Jiang, Junjun and Xiong, Zixiang},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={33},
  number={1},
  pages={378--391},
  year={2020},
  publisher={IEEE}
}
@article{fsr13,
  title={MFFN: image super-resolution via multi-level features fusion network},
  author={Chen, Yuantao and Xia, Runlong and Yang, Kai and Zou, Ke},
  journal={The Visual Computer},
  volume={40},
  number={2},
  pages={489--504},
  year={2024},
  publisher={Springer}
}
@article{fsr14,
  title={Deep hyfeat based attention in attention model for face super-resolution},
  author={Tomar, Anurag Singh and Arya, KV and Rajput, Shyam Singh},
  journal={IEEE Transactions on Instrumentation and Measurement},
  volume={72},
  pages={1--11},
  year={2023},
  publisher={IEEE}
}
@article{fsr15,
  title={Atfacegan: Single face semantic aware image restoration and recognition from atmospheric turbulence},
  author={Lau, Chun Pong and Castillo, Carlos D and Chellappa, Rama},
  journal={IEEE Transactions on Biometrics, Behavior, and Identity Science},
  volume={3},
  number={2},
  pages={240--251},
  year={2021},
  publisher={IEEE}
}
@inproceedings{fsr16,
  title={Hifacegan: Face renovation via collaborative suppression and replenishment},
  author={Yang, Lingbo and Wang, Shanshe and Ma, Siwei and Gao, Wen and Liu, Chang and Wang, Pan and Ren, Peiran},
  booktitle={Proceedings of the 28th ACM international conference on multimedia},
  pages={1551--1560},
  year={2020}
}
@article{fsr17,
  title={Supervised pixel-wise GAN for face super-resolution},
  author={Zhang, Menglei and Ling, Qiang},
  journal={IEEE Transactions on Multimedia},
  volume={23},
  pages={1938--1950},
  year={2020},
  publisher={IEEE}
}
@inproceedings{fsr18,
  title={PCA-SRGAN: Incremental orthogonal projection discrimination for face super-resolution},
  author={Dou, Hao and Chen, Chen and Hu, Xiyuan and Xuan, Zuxing and Hu, Zhisen and Peng, Silong},
  booktitle={Proceedings of the 28th ACM international conference on multimedia},
  pages={1891--1899},
  year={2020}
}
@inproceedings{fsr19,
  title={Gcfsr: a generative and controllable face super resolution method without facial and gan priors},
  author={He, Jingwen and Shi, Wu and Chen, Kai and Fu, Lean and Dong, Chao},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={1889--1898},
  year={2022}
}
@article{fsr20,
  title={E-ComSupResNet: Enhanced face super-resolution through compact network},
  author={Chudasama, Vishal and Nighania, Kartik and Upla, Kishor and Raja, Kiran and Ramachandra, Raghavendra and Busch, Christoph},
  journal={IEEE Transactions on Biometrics, Behavior, and Identity Science},
  volume={3},
  number={2},
  pages={166--179},
  year={2021},
  publisher={IEEE}
}
@inproceedings{fsr21,
  title={Super-fan: Integrated facial landmark localization and super-resolution of real-world low resolution faces in arbitrary poses with gans},
  author={Bulat, Adrian and Tzimiropoulos, Georgios},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={109--117},
  year={2018}
}
@article{fsr22,
  title={Edge and identity preserving network for face super-resolution},
  author={Kim, Jonghyun and Li, Gen and Yun, Inyong and Jung, Cheolkon and Kim, Joongkyu},
  journal={Neurocomputing},
  volume={446},
  pages={11--22},
  year={2021},
  publisher={Elsevier}
}
@inproceedings{fsr23,
  title={Multi-laplacian GAN with edge enhancement for face super resolution},
  author={Ko, Shanlei and Dai, Bi-Ru},
  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)},
  pages={3505--3512},
  year={2021},
  organization={IEEE}
}
@inproceedings{fsr24,
  title={Fsrnet: End-to-end learning face super-resolution with facial priors},
  author={Chen, Yu and Tai, Ying and Liu, Xiaoming and Shen, Chunhua and Yang, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2492--2501},
  year={2018}
}
@inproceedings{fsr25,
  title={Progressive face super-resolution via attention to face landmark},
  author={Kim, Deokyun and Kim, Minseon and Kwon, Gihyun and Kim, Dae-Shik},
  booktitle={the 30th British Machine Vision Conference (BMVC) 2019},
  year={2019},
  organization={the 30th British Machine Vision Conference (BMVC) 2019}
}
@inproceedings{fsr26,
  title={Deep face super-resolution with iterative collaboration between attentive recovery and landmark estimation},
  author={Ma, Cheng and Jiang, Zhenyu and Rao, Yongming and Lu, Jiwen and Zhou, Jie},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={5569--5578},
  year={2020}
}
@inproceedings{fsr27,
  title={Joint super-resolution and alignment of tiny faces},
  author={Yin, Yu and Robinson, Joseph and Zhang, Yulun and Fu, Yun},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={07},
  pages={12693--12700},
  year={2020}
}
@article{fsr28,
  title={Dclnet: Dual closed-loop networks for face super-resolution},
  author={Wang, Huan and Hu, Qian and Wu, Chengdong and Chi, Jianning and Yu, Xiaosheng and Wu, Hao},
  journal={Knowledge-Based Systems},
  volume={222},
  pages={106987},
  year={2021},
  publisher={Elsevier}
}
@inproceedings{fsr29,
  title={Face hallucination based on key parts enhancement},
  author={Li, Ke and Bare, Bahetiyaer and Yan, Bo and Feng, Bailan and Yao, Chunfeng},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1378--1382},
  year={2018},
  organization={IEEE}
}
@inproceedings{fsr30,
  title={Component attention guided face super-resolution network: Cagface},
  author={Kalarot, Ratheesh and Li, Tao and Porikli, Fatih},
  booktitle={Proceedings of the IEEE/CVF winter conference on applications of computer vision},
  pages={370--380},
  year={2020}
}
@inproceedings{fsr31,
  title={Face super-resolution network with incremental enhancement of facial parsing information},
  author={Liu, Shuang and Xiong, Chengyi and Gao, Zhirong},
  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)},
  pages={7537--7543},
  year={2021},
  organization={IEEE}
}
@inproceedings{fsr32,
  title={Saan: Semantic attention adaptation network for face super-resolution},
  author={Zhao, Tianyu and Zhang, Changqing},
  booktitle={2020 IEEE International Conference on Multimedia and Expo (ICME)},
  pages={1--6},
  year={2020},
  organization={IEEE}
}
@inproceedings{fsr33,
  title={MSFSR: A multi-stage face super-resolution with accurate facial representation via enhanced facial boundaries},
  author={Zhang, Yunchen and Wu, Yi and Chen, Liang},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops},
  pages={504--505},
  year={2020}
}
@inproceedings{fsr34,
  title={Heatmap-aware pyramid face hallucination},
  author={Wang, Chenyang and Jiang, Junjun and Liu, Xianming},
  booktitle={2021 IEEE International Conference on Multimedia and Expo (ICME)},
  pages={1--6},
  year={2021},
  organization={IEEE}
}
@article{fsr35,
  title={Super-resolving face image by facial parsing information},
  author={Wang, Chenyang and Jiang, Junjun and Zhong, Zhiwei and Zhai, Deming and Liu, Xianming},
  journal={IEEE Transactions on Biometrics, Behavior, and Identity Science},
  volume={5},
  number={4},
  pages={435--448},
  year={2023},
  publisher={IEEE}
}

@InProceedings{fsr36_esrgan,
author = {Wang, Xintao and Yu, Ke and Wu, Shixiang and Gu, Jinjin and Liu, Yihao and Dong, Chao and Qiao, Yu and Change Loy, Chen},
title = {ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks},
booktitle = {Proceedings of the European Conference on Computer Vision (ECCV) Workshops},
month = {September},
year = {2018}
}

@inproceedings{Mediapipe,
title={Real-time Facial Surface Geometry from Monocular Video on Mobile GPUs},
author={Yury Kartynnik and Artsiom Ablavatski and Ivan Grishchenko and Matthias Grundmann},
year={2019},
URL={https://arxiv.org/abs/1907.06724},
booktitle={CVPR Workshop on Computer Vision for Augmented and Virtual Reality 2019},
address={Long Beach, CA}
}

@misc{gcn,
      title={Semi-Supervised Classification with Graph Convolutional Networks}, 
      author={Thomas N. Kipf and Max Welling},
      year={2017},
      eprint={1609.02907},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1609.02907}, 
}

@misc{msaf,
    title={MSAF: Multimodal Split Attention Fusion},
    author={Lang Su and Chuqing Hu and Guofa Li and Dongpu Cao},
    year={2020},
    eprint={2012.07175},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
@article{mmd,
  author  = {Arthur Gretton and Karsten M. Borgwardt and Malte J. Rasch and Bernhard Sch{{\"o}}lkopf and Alexander Smola},
  title   = {A Kernel Two-Sample Test},
  journal = {Journal of Machine Learning Research},
  year    = {2012},
  volume  = {13},
  number  = {25},
  pages   = {723-773},
  url     = {http://jmlr.org/papers/v13/gretton12a.html}
}

@inproceedings{celebA,
  title = {Deep Learning Face Attributes in the Wild},
  author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
  booktitle = {Proceedings of International Conference on Computer Vision (ICCV)},
  month = {December},
  year = {2015} 
}


@inproceedings{helen,
title = "Interactive facial feature localization",
abstract = "We address the problem of interactive facial feature localization from a single image. Our goal is to obtain an accurate segmentation of facial features on high-resolution images under a variety of pose, expression, and lighting conditions. Although there has been significant work in facial feature localization, we are addressing a new application area, namely to facilitate intelligent high-quality editing of portraits, that brings requirements not met by existing methods. We propose an improvement to the Active Shape Model that allows for greater independence among the facial components and improves on the appearance fitting step by introducing a Viterbi optimization process that operates along the facial contours. Despite the improvements, we do not expect perfect results in all cases. We therefore introduce an interaction model whereby a user can efficiently guide the algorithm towards a precise solution. We introduce the Helen Facial Feature Dataset consisting of annotated portrait images gathered from Flickr that are more diverse and challenging than currently existing datasets. We present experiments that compare our automatic method to published results, and also a quantitative evaluation of the effectiveness of our interactive method.",
author = "Vuong Le and Jonathan Brandt and Zhe Lin and Lubomir Bourdev and Huang, {Thomas S.}",
year = "2012",
doi = "10.1007/978-3-642-33712-3_49",
language = "English (US)",
isbn = "9783642337116",
series = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
number = "PART 3",
pages = "679--692",
booktitle = "Computer Vision, ECCV 2012 - 12th European Conference on Computer Vision, Proceedings",
edition = "PART 3",
note = "12th European Conference on Computer Vision, ECCV 2012 ; Conference date: 07-10-2012 Through 13-10-2012",

}


@article{SSIM,
  title={A universal image quality index},
  author={Zhou Wang and Alan Conrad Bovik},
  journal={IEEE Signal Processing Letters},
  year={2002},
  volume={9},
  pages={81-84},
  url={https://api.semanticscholar.org/CorpusID:14488670}
}


@INPROCEEDINGS {LPIPS,
author = { Zhang, Richard and Isola, Phillip and Efros, Alexei A. and Shechtman, Eli and Wang, Oliver },
booktitle = { 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) },
title = { The Unreasonable Effectiveness of Deep Features as a Perceptual Metric },
year = {2018},
volume = {},
ISSN = {},
pages = {586-595},
abstract = { While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called "perceptual losses"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations. },
keywords = {Distortion;Task analysis;Measurement;Visualization;Training;Network architecture;Computer architecture},
doi = {10.1109/CVPR.2018.00068},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2018.00068},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {Jun}
}





@misc{LCGE,
      title={Learning to Hallucinate Face Images via Component Generation and Enhancement}, 
      author={Yibing Song and Jiawei Zhang and Shengfeng He and Linchao Bao and Qingxiong Yang},
      year={2017},
      eprint={1708.00223},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1708.00223}, 
}

@misc{SRCNN,
      title={Image Super-Resolution Using Deep Convolutional Networks}, 
      author={Chao Dong and Chen Change Loy and Kaiming He and Xiaoou Tang},
      year={2015},
      eprint={1501.00092},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1501.00092}, 
}

@misc{EDSR,
      title={Enhanced Deep Residual Networks for Single Image Super-Resolution}, 
      author={Bee Lim and Sanghyun Son and Heewon Kim and Seungjun Nah and Kyoung Mu Lee},
      year={2017},
      eprint={1707.02921},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1707.02921}, 
}


@article{SPARNet,
   abstract = {General image super-resolution techniques have difficulties in recovering detailed face structures when applying to low resolution face images. Recent deep learning based methods tailored for face images have achieved improved performance by jointly trained with additional task such as face parsing and landmark prediction. However, multi-task learning requires extra manually labeled data. Besides, most of the existing works can only generate relatively low resolution face images (e.g., $128\times 128$ ), and their applications are therefore limited. In this paper, we introduce a novel SPatial Attention Residual Network (SPARNet) built on our newly proposed Face Attention Units (FAUs) for face super-resolution. Specifically, we introduce a spatial attention mechanism to the vanilla residual blocks. This enables the convolutional layers to adaptively bootstrap features related to the key face structures and pay less attention to those less feature-rich regions. This makes the training more effective and efficient as the key face structures only account for a very small portion of the face image. Visualization of the attention maps shows that our spatial attention network can capture the key face structures well even for very low resolution faces (e.g., $16\times 16$ ). Quantitative comparisons on various kinds of metrics (including PSNR, SSIM, identity similarity, and landmark detection) demonstrate the superiority of our method over current state-of-the-arts. We further extend SPARNet with multi-scale discriminators, named as SPARNetHD, to produce high resolution results (i.e., $512\times 512$ ). We show that SPARNetHD trained with synthetic data can not only produce high quality and high resolution outputs for synthetically degraded face images, but also show good generalization ability to real world low quality face images. Codes are available at https://github.com/chaofengc/Face-SPARNet.},
   author = {Chaofeng Chen and Dihong Gong and Hao Wang and Zhifeng Li and Kwan Yee K. Wong},
   doi = {10.1109/TIP.2020.3043093},
   issn = {19410042},
   journal = {IEEE Transactions on Image Processing},
   title = {Learning Spatial Attention for Face Super-Resolution},
   volume = {30},
   year = {2021},
}
@inproceedings{DIC,
   abstract = {Recent works based on deep learning and facial priors have succeeded in super-resolving severely degraded facial images. However, the prior knowledge is not fully exploited in existing methods, since facial priors such as landmark and component maps are always estimated by low-resolution or coarsely super-resolved images, which may be inaccurate and thus affect the recovery performance. In this paper, we propose a deep face super-resolution (FSR) method with iterative collaboration between two recurrent networks which focus on facial image recovery and landmark estimation respectively. In each recurrent step, the recovery branch utilizes the prior knowledge of landmarks to yield higher-quality images which facilitate more accurate landmark estimation in turn. Therefore, the iterative information interaction between two processes boosts the performance of each other progressively. Moreover, a new attentive fusion module is designed to strengthen the guidance of landmark maps, where facial components are generated individually and aggregated attentively for better restoration. Quantitative and qualitative experimental results show the proposed method significantly outperforms state-of-the-art FSR methods in recovering high-quality face images.},
   author = {Cheng Ma and Zhenyu Jiang and Yongming Rao and Jiwen Lu and Jie Zhou},
   doi = {10.1109/CVPR42600.2020.00561},
   issn = {10636919},
   booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   title = {Deep face super-resolution with iterative collaboration between attentive recovery and landmark estimation},
   year = {2020},
}
@inproceedings{FSRNet,
   abstract = {Face Super-Resolution (SR) is a domain-specific superresolution problem. The facial prior knowledge can be leveraged to better super-resolve face images. We present a novel deep end-to-end trainable Face Super-Resolution Network (FSRNet), which makes use of the geometry prior, i.e., facial landmark heatmaps and parsing maps, to super-resolve very low-resolution (LR) face images without well-aligned requirement. Specifically, we first construct a coarse SR network to recover a coarse high-resolution (HR) image. Then, the coarse HR image is sent to two branches: a fine SR encoder and a prior information estimation network, which extracts the image features, and estimates landmark heatmaps/parsing maps respectively. Both image features and prior information are sent to a fine SR decoder to recover the HR image. To generate realistic faces, we also propose the Face Super-Resolution Generative Adversarial Network (FSRGAN) to incorporate the adversarial loss into FSRNet. Further, we introduce two related tasks, face alignment and parsing, as the new evaluation metrics for face SR, which address the inconsistency of classic metrics w.r.t. visual perception. Extensive experiments show that FSRNet and FSRGAN significantly outperforms state of the arts for very LR face SR, both quantitatively and qualitatively.},
   author = {Yu Chen and Ying Tai and Xiaoming Liu and Chunhua Shen and Jian Yang},
   doi = {10.1109/CVPR.2018.00264},
   issn = {10636919},
   booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   title = {FSRNet: End-to-End Learning Face Super-Resolution with Facial Priors},
   year = {2018},
}

@inproceedings{FFHQ,
  title={A style-based generator architecture for generative adversarial networks},
  author={Karras, Tero and Laine, Samuli and Aila, Timo},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={4401--4410},
  year={2019}
}


@inproceedings{PyTorch,
  title={Automatic differentiation in PyTorch},
  author={Adam Paszke and Sam Gross and Soumith Chintala and Gregory Chanan and Edward Yang and Zach DeVito and Zeming Lin and Alban Desmaison and Luca Antiga and Adam Lerer},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:40027675}
}

@misc{InstanceNorm1,
      title={Improved Texture Networks: Maximizing Quality and Diversity in Feed-forward Stylization and Texture Synthesis}, 
      author={Dmitry Ulyanov and Andrea Vedaldi and Victor Lempitsky},
      year={2017},
      eprint={1701.02096},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1701.02096}, 
}

@misc{InstanceNorm2,
      title={Instance Normalization: The Missing Ingredient for Fast Stylization}, 
      author={Dmitry Ulyanov and Andrea Vedaldi and Victor Lempitsky},
      year={2017},
      eprint={1607.08022},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1607.08022}, 
}



@article{SCFace,
author = {Grgic, Mislav and Delac, Kresimir and Grgic, Sonja},
title = {SCface --- surveillance cameras face database},
year = {2011},
issue_date = {February  2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {3},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-009-0417-2},
doi = {10.1007/s11042-009-0417-2},
abstract = {In this paper we describe a database of static images of human faces. Images were taken in uncontrolled indoor environment using five video surveillance cameras of various qualities. Database contains 4,160 static images (in visible and infrared spectrum) of 130 subjects. Images from different quality cameras should mimic real-world conditions and enable robust face recognition algorithms testing, emphasizing different law enforcement and surveillance use case scenarios. In addition to database description, this paper also elaborates on possible uses of the database and proposes a testing protocol. A baseline Principal Component Analysis (PCA) face recognition algorithm was tested following the proposed protocol. Other researchers can use these test results as a control algorithm performance score when testing their own algorithms on this dataset. Database is available to research community through the procedure described at www.scface.org .},
journal = {Multimedia Tools Appl.},
month = feb,
pages = {863–879},
numpages = {17},
keywords = {Face database, Face recognition, Video surveillance cameras}
}
@article{sr6,
  title={A super-resolution reconstruction algorithm for surveillance images},
  author={Zhang, Liangpei and Zhang, Hongyan and Shen, Huanfeng and Li, Pingxiang},
  journal={Signal Processing},
  volume={90},
  number={3},
  pages={848--859},
  year={2010},
  publisher={Elsevier}
}
@inproceedings{sr7,
  title={Convolutional neural network super resolution for face recognition in surveillance monitoring},
  author={Rasti, Pejman and Uiboupin, Tonis and Escalera, Sergio and Anbarjafari, Gholamreza},
  booktitle={International conference on articulated motion and deformable objects},
  pages={175--184},
  year={2016},
  organization={Springer}
}

@article{sr8,
  title={Super-resolution reconstruction for multi-angle remote sensing images considering resolution differences},
  author={Zhang, Hongyan and Yang, Zeyu and Zhang, Liangpei and Shen, Huanfeng},
  journal={Remote Sensing},
  volume={6},
  number={1},
  pages={637--657},
  year={2014},
  publisher={Multidisciplinary Digital Publishing Institute}
}
@article{sr9,
  title={A super-resolution reconstruction algorithm for hyperspectral images},
  author={Zhang, Hongyan and Zhang, Liangpei and Shen, Huanfeng},
  journal={Signal Processing},
  volume={92},
  number={9},
  pages={2082--2096},
  year={2012},
  publisher={Elsevier}
}
@misc{sr10,
  title={Real-time video super-resolution with spatio-temporal networks and motion compensation},
  author={Caballero, Jose and Ledig, Christian and Aitken, Andrew and Diaz, Alfredo Alejandro Acosta and Theis, Lucas and Huszar, Ferenc and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
  year={2020},
  month=jun # "~30",
  publisher={Google Patents},
  note={US Patent 10,701,394}
}

@article{sr11,
  title={Video super-resolution using an adaptive superpixel-guided auto-regressive model},
  author={Li, Kun and Zhu, Yanming and Yang, Jingyu and Jiang, Jianmin},
  journal={Pattern Recognition},
  volume={51},
  pages={59--71},
  year={2016},
  publisher={Elsevier}
}
@inproceedings{sr12,
  title={Deep learning based super-resolution for improved action recognition},
  author={Nasrollahi, Kamal and Escalera, Sergio and Rasti, Pejman and Anbarjafari, Gholamreza and Baro, Xavier and Escalante, Hugo Jair and Moeslund, Thomas B},
  booktitle={2015 International Conference on Image Processing Theory, Tools and Applications (IPTA)},
  pages={67--72},
  year={2015},
  organization={IEEE}
}

@inproceedings{sr13,
  title={Privacy-preserving human activity recognition from extreme low resolution},
  author={Ryoo, Michael S and Rothrock, Brandon and Fleming, Charles and Yang, Hyun Jong},
  booktitle={Thirty-First AAAI Conference on Artificial Intelligence},
  year={2017}
}
@book{sr14,
  title={Remote sensing and image interpretation},
  author={Lillesand, Thomas and Kiefer, Ralph W and Chipman, Jonathan},
  year={2015},
  publisher={John Wiley \& Sons}
}
@article{sr15,
  title={Resolution limits in astronomical images},
  author={Lobanov, Andrei P},
  journal={arXiv preprint astro-ph/0503225},
  year={2005}
}
@article{sr16,
  title={Digital image forensics via intrinsic fingerprints},
  author={Swaminathan, Ashwin and Wu, Min and Liu, KJ Ray},
  journal={IEEE transactions on information forensics and security},
  volume={3},
  number={1},
  pages={101--117},
  year={2008},
  publisher={IEEE}
}

@article{sr17,
  title={Image-based three-dimensional human pose recovery by multiview locality-sensitive sparse retrieval},
  author={Hong, Chaoqun and Yu, Jun and Tao, Dacheng and Wang, Meng},
  journal={IEEE Transactions on Industrial Electronics},
  volume={62},
  number={6},
  pages={3742--3751},
  year={2014},
  publisher={IEEE}
}
@article{sr18,
  title={Multimodal deep autoencoder for human pose recovery},
  author={Hong, Chaoqun and Yu, Jun and Wan, Jian and Tao, Dacheng and Wang, Meng},
  journal={IEEE transactions on image processing},
  volume={24},
  number={12},
  pages={5659--5670},
  year={2015},
  publisher={IEEE}
}
@inproceedings{sr19,
  title={Gait recognition using periodic temporal super resolution for low frame-rate videos},
  author={Akae, Naoki and Makihara, Yasushi and Yagi, Yasushi},
  booktitle={2011 international joint conference on biometrics (IJCB)},
  pages={1--7},
  year={2011},
  organization={IEEE}
}
@article{sr20,
  title={Deep convolutional neural network for latent fingerprint enhancement},
  author={Li, Jian and Feng, Jianjiang and Kuo, C-C Jay},
  journal={Signal Processing: Image Communication},
  volume={60},
  pages={52--63},
  year={2018},
  publisher={Elsevier}
}
@article{sr21,
  title={Fingerprint image super-resolution via ridge orientation-based clustered coupled sparse dictionaries},
  author={Singh, Kuldeep and Gupta, Anubhav and Kapoor, Rajiv},
  journal={Journal of Electronic Imaging},
  volume={24},
  number={4},
  pages={043015},
  year={2015},
  publisher={International Society for Optics and Photonics}
}
@article{sr22,
  title={Low-resolution gait recognition},
  author={Zhang, Junping and Pu, Jian and Chen, Changyou and Fleischer, Rudolf},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
  volume={40},
  number={4},
  pages={986--996},
  year={2010},
  publisher={IEEE}
}

@misc{sr1,
  title={by P. Milanfar},
  author={Imaging, Super-Resolution},
  year={2010},
  publisher={Boca Raton: CRC Press}
}

@article{sr2,
  title={Super-resolution for biometrics: A comprehensive survey},
  author={Nguyen, Kien and Fookes, Clinton and Sridharan, Sridha and Tistarelli, Massimo and Nixon, Mark},
  journal={Pattern Recognition},
  volume={78},
  pages={23--42},
  year={2018},
  publisher={Elsevier}
}
@article{sr3,
  title={Super-resolution in medical imaging},
  author={Greenspan, Hayit},
  journal={The computer journal},
  volume={52},
  number={1},
  pages={43--63},
  year={2009},
  publisher={Oxford University Press}
}
@inproceedings{fer57,
  title={Emotion recognition in the wild via convolutional neural networks and mapped binary patterns},
  author={Levi, Gil and Hassner, Tal},
  booktitle={Proceedings of the 2015 ACM on international conference on multimodal interaction},
  pages={503--510},
  year={2015}
}
@inproceedings{sr4,
  title={Super resolution techniques for medical image processing},
  author={Isaac, Jithin Saji and Kulkarni, Ramesh},
  booktitle={2015 International Conference on Technologies for Sustainable Development (ICTSD)},
  pages={1--6},
  year={2015},
  organization={IEEE}
}
@inproceedings{s1,
  title={Learning a deep convolutional network for image super-resolution},
  author={Dong, Chao and Loy, Chen Change and He, Kaiming and Tang, Xiaoou},
  booktitle={Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part IV 13},
  pages={184--199},
  year={2014},
  organization={Springer}
}
@article{s2,
  title={Image super-resolution using deep convolutional networks},
  author={Dong, Chao and Loy, Chen Change and He, Kaiming and Tang, Xiaoou},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={38},
  number={2},
  pages={295--307},
  year={2015},
  publisher={IEEE}
}
@inproceedings{s3,
  title={Accurate image super-resolution using very deep convolutional networks},
  author={Kim, Jiwon and Lee, Jung Kwon and Lee, Kyoung Mu},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1646--1654},
  year={2016}
}
@inproceedings{s4,
  title={Memnet: A persistent memory network for image restoration},
  author={Tai, Ying and Yang, Jian and Liu, Xiaoming and Xu, Chunyan},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={4539--4547},
  year={2017}
}
@inproceedings{s5,
  title={Image super-resolution via deep recursive residual network},
  author={Tai, Ying and Yang, Jian and Liu, Xiaoming},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3147--3155},
  year={2017}
}
@inproceedings{s6,
  title={Deeply-recursive convolutional network for image super-resolution},
  author={Kim, Jiwon and Lee, Jung Kwon and Lee, Kyoung Mu},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1637--1645},
  year={2016}
}
@inproceedings{s7,
  title={“zero-shot” super-resolution using deep internal learning},
  author={Shocher, Assaf and Cohen, Nadav and Irani, Michal},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3118--3126},
  year={2018}
}
@inproceedings{s8,
  title={Accelerating the super-resolution convolutional neural network},
  author={Dong, Chao and Loy, Chen Change and Tang, Xiaoou},
  booktitle={Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14},
  pages={391--407},
  year={2016},
  organization={Springer}
}
@inproceedings{s9,
  title={Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network},
  author={Shi, Wenzhe and Caballero, Jose and Husz{\'a}r, Ferenc and Totz, Johannes and Aitken, Andrew P and Bishop, Rob and Rueckert, Daniel and Wang, Zehan},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1874--1883},
  year={2016}
}
@inproceedings{s10,
  title={Photo-realistic single image super-resolution using a generative adversarial network},
  author={Ledig, Christian and Theis, Lucas and Husz{\'a}r, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and others},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4681--4690},
  year={2017}
}
@inproceedings{s11,
  title={Enhanced deep residual networks for single image super-resolution},
  author={Lim, Bee and Son, Sanghyun and Kim, Heewon and Nah, Seungjun and Mu Lee, Kyoung},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition workshops},
  pages={136--144},
  year={2017}
}
@inproceedings{s12,
  title={Image super-resolution using dense skip connections},
  author={Tong, Tong and Li, Gen and Liu, Xiejie and Gao, Qinquan},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={4799--4807},
  year={2017}
}
@inproceedings{s13,
  title={Image super-resolution via dual-state recurrent networks},
  author={Han, Wei and Chang, Shiyu and Liu, Ding and Yu, Mo and Witbrock, Michael and Huang, Thomas S},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1654--1663},
  year={2018}
}
@inproceedings{s14,
    author    = {Lai, Wei-Sheng and Huang, Jia-Bin and Ahuja, Narendra and Yang, Ming-Hsuan}, 
    title     = {Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution}, 
    booktitle = {IEEE Conferene on Computer Vision and Pattern Recognition},
    year      = {2017}
}
@article{s15,
  title={Fast and accurate image super-resolution with deep laplacian pyramid networks},
  author={Lai, Wei-Sheng and Huang, Jia-Bin and Ahuja, Narendra and Yang, Ming-Hsuan},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={41},
  number={11},
  pages={2599--2613},
  year={2018},
  publisher={IEEE}
}
@inproceedings{s16,
  title={A fully progressive approach to single-image super-resolution},
  author={Wang, Yifan and Perazzi, Federico and McWilliams, Brian and Sorkine-Hornung, Alexander and Sorkine-Hornung, Olga and Schroers, Christopher},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition workshops},
  pages={864--873},
  year={2018}
}
@article{s17,
  title={Improving resolution by image registration},
  author={Irani, Michal and Peleg, Shmuel},
  journal={CVGIP: Graphical models and image processing},
  volume={53},
  number={3},
  pages={231--239},
  year={1991},
  publisher={Elsevier}
}
@inproceedings{s18,
  title={Seven ways to improve example-based single image super resolution},
  author={Timofte, Radu and Rothe, Rasmus and Van Gool, Luc},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1865--1873},
  year={2016}
}
@inproceedings{s19,
  title={Deep back-projection networks for super-resolution},
  author={Haris, Muhammad and Shakhnarovich, Gregory and Ukita, Norimichi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1664--1673},
  year={2018}
}
@inproceedings{s20,
  title={Feedback network for image super-resolution},
  author={Li, Zhen and Yang, Jinglei and Liu, Zheng and Yang, Xiaomin and Jeon, Gwanggil and Wu, Wei},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={3867--3876},
  year={2019}
}
@inproceedings{s21,
  title={Recurrent back-projection network for video super-resolution},
  author={Haris, Muhammad and Shakhnarovich, Gregory and Ukita, Norimichi},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={3897--3906},
  year={2019}
}
@inproceedings{s22,
  title={“zero-shot” super-resolution using deep internal learning},
  author={Shocher, Assaf and Cohen, Nadav and Irani, Michal},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3118--3126},
  year={2018}
}
@inproceedings{s23,
  title={Nonparametric blind super-resolution},
  author={Michaeli, Tomer and Irani, Michal},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={945--952},
  year={2013}
}
@inproceedings{s24,
  title={To learn image super-resolution, use a gan to learn how to do image degradation first},
  author={Bulat, Adrian and Yang, Jing and Tzimiropoulos, Georgios},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={185--200},
  year={2018}
}
@INPROCEEDINGS{s25,
  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
  booktitle={2017 IEEE International Conference on Computer Vision (ICCV)}, 
  title={Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks}, 
  year={2017},
  volume={},
  number={},
  pages={2242-2251},
  keywords={Training;Painting;Training data;Semantics;Extraterrestrial measurements;Graphics},
  doi={10.1109/ICCV.2017.244}}
@inproceedings{s26,
author = {Yuan, Yuan and Liu, Siyuan and Zhang, Jiawei and Zhang, Yongbing and Dong, Chao and Lin, Liang},
year = {2018},
month = {06},
pages = {814-81409},
title = {Unsupervised Image Super-Resolution Using Cycle-in-Cycle Generative Adversarial Networks},
doi = {10.1109/CVPRW.2018.00113}
}
@inproceedings{Zhang2018,
   abstract = {Face hallucination is a generative task to super-resolve the facial image with low resolution while human perception of face heavily relies on identity information. However, previous face hallucination approaches largely ignore facial identity recovery. This paper proposes Super-Identity Convolutional Neural Network (SICNN) to recover identity information for generating faces closed to the real identity. Specifically, we define a super-identity loss to measure the identity difference between a hallucinated face and its corresponding high-resolution face within the hypersphere identity metric space. However, directly using this loss will lead to a Dynamic Domain Divergence problem, which is caused by the large margin between the high-resolution domain and the hallucination domain. To overcome this challenge, we present a domain-integrated training approach by constructing a robust identity metric for faces from these two domains. Extensive experimental evaluations demonstrate that the proposed SICNN achieves superior visual quality over the state-of-the-art methods on a challenging task to super-resolve 12 × 14 faces with an 8 × upscaling factor. In addition, SICNN significantly improves the recognizability of ultra-low-resolution faces.},
   author = {Kaipeng Zhang and Zhanpeng Zhang and Chia Wen Cheng and Winston H. Hsu and Yu Qiao and Wei Liu and Tong Zhang},
   doi = {10.1007/978-3-030-01252-6_12},
   issn = {16113349},
   booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   title = {Super-Identity Convolutional Neural Network for Face Hallucination},
   volume = {11215 LNCS},
   year = {2018},
}
@inproceedings{s27,
   abstract = {Face Super-Resolution (SR) is a domain-specific superresolution problem. The facial prior knowledge can be leveraged to better super-resolve face images. We present a novel deep end-to-end trainable Face Super-Resolution Network (FSRNet), which makes use of the geometry prior, i.e., facial landmark heatmaps and parsing maps, to super-resolve very low-resolution (LR) face images without well-aligned requirement. Specifically, we first construct a coarse SR network to recover a coarse high-resolution (HR) image. Then, the coarse HR image is sent to two branches: a fine SR encoder and a prior information estimation network, which extracts the image features, and estimates landmark heatmaps/parsing maps respectively. Both image features and prior information are sent to a fine SR decoder to recover the HR image. To generate realistic faces, we also propose the Face Super-Resolution Generative Adversarial Network (FSRGAN) to incorporate the adversarial loss into FSRNet. Further, we introduce two related tasks, face alignment and parsing, as the new evaluation metrics for face SR, which address the inconsistency of classic metrics w.r.t. visual perception. Extensive experiments show that FSRNet and FSRGAN significantly outperforms state of the arts for very LR face SR, both quantitatively and qualitatively.},
   author = {Yu Chen and Ying Tai and Xiaoming Liu and Chunhua Shen and Jian Yang},
   doi = {10.1109/CVPR.2018.00264},
   issn = {10636919},
   booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   title = {FSRNet: End-to-End Learning Face Super-Resolution with Facial Priors},
   year = {2018},
}
@article{s28,
   abstract = {This paper proposes a face hallucination method for the reconstruction of high-resolution facial images from single-frame, low-resolution facial images. The proposed method has been derived from example-based hallucination methods and morphable face models. First, we propose a recursive error back-projection method to compensate for residual errors, and a region-based reconstruction method to preserve characteristics of local facial regions. Then, we define an extended morphable face model, in which an extended face is composed of the interpolated high-resolution face from a given low-resolution face, and its original high-resolution equivalent. Then, the extended face is separated into an extended shape and an extended texture. We performed various hallucination experiments using the MPI, XM2VTS, and KF databases, compared the reconstruction errors, structural similarity index, and recognition rates, and showed the effects of face detection errors and shape estimation errors. The encouraging results demonstrate that the proposed methods can improve the performance of face recognition systems. Especially the proposed method can enhance the resolution of single-frame, low-resolution facial images. © 2008 IEEE.},
   author = {Jeong Seon Park and Seong Whan Lee},
   doi = {10.1109/TIP.2008.2001394},
   issn = {10577149},
   issue = {10},
   journal = {IEEE Transactions on Image Processing},
   title = {An example-based face hallucination method for single-frame, low-resolution facial images},
   volume = {17},
   year = {2008},
}
@inproceedings{s29,
   abstract = {We present a novel framework for hallucinating faces of unconstrained poses and with very low resolution (face size as small as 5pxIOD). In contrast to existing studies that mostly ignore or assume pre-aligned face spatial configuration (e.g. facial landmarks localization or dense correspondence field), we alternatingly optimize two complementary tasks, namely face hallucination and dense correspondence field estimation, in a unified framework. In addition, we propose a new gated deep bi-network that contains two functionality-specialized branches to recover different levels of texture details. Extensive experiments demonstrate that such formulation allows exceptional hallucination quality on in-the-wild low-res faces with significant pose and illumination variations.},
   author = {Shizhan Zhu and Sifei Liu and Chen Change Loy and Xiaoou Tang},
   doi = {10.1007/978-3-319-46454-1_37},
   issn = {16113349},
   booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   title = {Deep cascaded Bi-network for face hallucination},
   volume = {9909 LNCS},
   year = {2016},
}
@inproceedings{s30,
   abstract = {This paper addresses 2 challenging tasks: improving the quality of low resolution facial images and accurately locating the facial landmarks on such poor resolution images. To this end, we make the following 5 contributions: (a) we propose Super-FAN: the very first end-to-end system that addresses both tasks simultaneously, i.e. both improves face resolution and detects the facial landmarks. The novelty or Super-FAN lies in incorporating structural information in a GAN-based super-resolution algorithm via integrating a sub-network for face alignment through heatmap regression and optimizing a novel heatmap loss. (b) We illustrate the benefit of training the two networks jointly by reporting good results not only on frontal images (as in prior work) but on the whole spectrum of facial poses, and not only on synthetic low resolution images (as in prior work) but also on real-world images. (c) We improve upon the state-of-the-art in face super-resolution by proposing a new residual-based architecture. (d) Quantitatively, we show large improvement over the state-of-the-art for both face super-resolution and alignment. (e) Qualitatively, we show for the first time good results on real-world low resolution images like the ones of Fig. 1.},
   author = {Adrian Bulat and Georgios Tzimiropoulos},
   doi = {10.1109/CVPR.2018.00019},
   issn = {10636919},
   booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   title = {Super-FAN: Integrated Facial Landmark Localization and Super-Resolution of Real-World Low Resolution Faces in Arbitrary Poses with GANs},
   year = {2018},
}
@inproceedings{s31,
   abstract = {State-of-the-art face super-resolution methods leverage deep convolutional neural networks to learn a mapping between low-resolution (LR) facial patterns and their corresponding high-resolution (HR) counterparts by exploring local appearance information. However, most of these methods do not account for facial structure and suffer from degradations due to large pose variations and misalignments. In this paper, we propose a method that explicitly incorporates structural information of faces into the face super-resolution process by using a multi-task convolutional neural network (CNN). Our CNN has two branches: one for super-resolving face images and the other branch for predicting salient regions of a face coined facial component heatmaps. These heatmaps encourage the upsampling stream to generate super-resolved faces with higher-quality details. Our method not only uses low-level information (i.e., intensity similarity), but also middle-level information (i.e., face structure) to further explore spatial constraints of facial components from LR inputs images. Therefore, we are able to super-resolve very small unaligned face images (16×16pixels) with a large upscaling factor of 8 ×, while preserving face structure. Extensive experiments demonstrate that our network achieves superior face hallucination results and outperforms the state-of-the-art.},
   author = {Xin Yu and Basura Fernando and Bernard Ghanem and Fatih Porikli and Richard Hartley},
   doi = {10.1007/978-3-030-01240-3_14},
   issn = {16113349},
   booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   title = {Face super-resolution guided by facial component heatmaps},
   volume = {11213 LNCS},
   year = {2018},
}
@inproceedings{s310,
   abstract = {Conventional face hallucination methods rely heavily on accurate alignment of low-resolution (LR) faces before upsampling them. Misalignment often leads to deficient results and unnatural artifacts for large upscaling factors. However, due to the diverse range of poses and different facial expressions, aligning an LR input image, in particular when it is tiny, is severely difficult. To overcome this challenge, here we present an end-to-end transformative discriminative neural network (TDN) devised for super-resolving unaligned and very small face images with an extreme upscaling factor of 8. Our method employs an upsampling network where we embed spatial transformation layers to allow local receptive fields to line-up with similar spatial supports. Furthermore, we incorporate a class-specific loss in our objective through a successive discriminative network to improve the alignment and upsampling performance with semantic information. Extensive experiments on large face datasets show that the proposed method significantly outperforms the state-of-the-art.},
   author = {Xin Yu and Fatih Porikli},
   doi = {10.1609/aaai.v31i1.11206},
   issn = {2159-5399},
   booktitle = {31st AAAI Conference on Artificial Intelligence, AAAI 2017},
   title = {Face hallucination with tiny unaligned images by transformative discriminative neural networks},
   year = {2017},
}
@inproceedings{s32,
   abstract = {Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner.In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.},
   author = {Max Jaderberg and Karen Simonyan and Andrew Zisserman and Koray Kavukcuoglu},
   issn = {10495258},
   booktitle = {Advances in Neural Information Processing Systems},
   title = {Spatial transformer networks},
   volume = {2015-January},
   year = {2015},
}
@inproceedings{s33,
   abstract = {Most of the conventional face hallucination methods assume the input image is sufficiently large and aligned, and all require the input image to be noise-free. Their performance degrades drastically if the input image is tiny, unaligned, and contaminated by noise. In this paper, we introduce a novel transformative discriminative autoencoder to 8× super-resolve unaligned noisy and tiny (16×16) low-resolution face images. In contrast to encoder-decoder based autoencoders, our method uses decoder-encoder-decoder networks. We first employ a transformative discriminative decoder network to upsample and denoise simultaneously. Then we use a transformative encoder network to project the intermediate HR faces to aligned and noise-free LR faces. Finally, we use the second decoder to generate hallucinated HR images. Our extensive evaluations on a very large face dataset show that our method achieves superior hallucination results and outperforms the state-of-the-art by a large margin of 1.82 dB PSNR.},
   author = {Xin Yu and Fatih Porikli},
   doi = {10.1109/CVPR.2017.570},
   booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
   title = {Hallucinating very low-Resolution unaligned and noisy face images by transformative discriminative autoencoders},
   volume = {2017-January},
   year = {2017},
}
@inproceedings{s34,
   abstract = {We propose a two-stage method for face hallucination. First, we generate facial components of the input image using CNNs. These components represent the basic facial structures. Second, we synthesize fine-grained facial structures from high resolution training images. The details of these structures are transferred into facial components for enhancement. Therefore, we generate facial components to approximate ground truth global appearance in the first stage and enhance them through recovering details in the second stage. The experiments demonstrate that our method performs favorably against state-of-the-art methods.},
   author = {Yibing Song and Jiawei Zhang and Shengfeng He and Linchao Bao and Qingxiong Yang},
   doi = {10.24963/ijcai.2017/633},
   issn = {10450823},
   booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
   title = {Learning to hallucinate face images via component generation and enhancement},
   volume = {0},
   year = {2017},
}
@article{s35,
   abstract = {A face hallucination algorithm is proposed to generate high-resolution images from JPEG compressed low-resolution inputs by decomposing a deblocked face image into structural regions such as facial components and non-structural regions like the background. For structural regions, landmarks are used to retrieve adequate high-resolution component exemplars in a large dataset based on the estimated head pose and illumination condition. For non-structural regions, an efficient generic super resolution algorithm is applied to generate high-resolution counterparts. Two sets of gradient maps extracted from these two regions are combined to guide an optimization process of generating the hallucination image. Numerous experimental results demonstrate that the proposed algorithm performs favorably against the state-of-the-art hallucination methods on JPEG compressed face images with different poses, expressions, and illumination conditions.},
   author = {Chih Yuan Yang and Sifei Liu and Ming Hsuan Yang},
   doi = {10.1007/s11263-017-1044-4},
   issn = {15731405},
   issue = {6},
   journal = {International Journal of Computer Vision},
   title = {Hallucinating Compressed Face Images},
   volume = {126},
   year = {2018},
}
@article{s36,
   abstract = {To perform visual search, humans, like many mammals, encode a large field of view with retinas having variable spatial resolution, and then use high-speed eye movements to direct the highest-resolution region, the fovea, towards potential target locations. Good search performance is essential for survival, and hence mammals may have evolved efficient strategies for selecting fixation locations. Here we address two questions: what are the optimal eye movement strategies for a foveated visual system faced with the problem of finding a target in a cluttered environment, and do humans employ optimal eye movement strategies during a search? We derive the ideal bayesian observer for search tasks in which a target is embedded at an unknown location within a random background that has the spectral characteristics of natural scenes. Our ideal searcher uses precise knowledge about the statistics of the scenes in which the target is embedded, and about its own visual system, to make eye movements that gain the most information about target location. We find that humans achieve nearly optimal search performance, even though humans integrate information poorly across fixations. Analysis of the ideal searcher reveals that there is little benefit from perfect integration across fixations-much more important is efficient processing of information on each fixation. Apparently, evolution has exploited this fact to achieve efficient eye movement strategies with minimal neural resources devoted to memory.},
   author = {Jiri Najemnik and Wilson S. Geisler},
   doi = {10.1038/nature03390},
   issn = {00280836},
   issue = {7031},
   journal = {Nature},
   title = {Optimal eye movement strategies in visual search},
   volume = {434},
   year = {2005},
}
@inproceedings{s37,
   abstract = {Face hallucination is a domain-specific super-resolution problem with the goal to generate high-resolution (HR) faces from low-resolution (LR) input images. In contrast to existing methods that often learn a single patch-to-patch mapping from LR to HR images and are regardless of the contextual interdependency between patches, we propose a novel Attention-aware Face Hallucination (Attention-FH) framework which resorts to deep reinforcement learning for sequentially discovering attended patches and then performing the facial part enhancement by fully exploiting the global interdependency of the image. Specifically, in each time step, the recurrent policy network is proposed to dynamically specify a new attended region by incorporating what happened in the past. The state (i.e., face hallucination result for the whole image) can thus be exploited and updated by the local enhancement network on the selected region. The Attention-FH approach jointly learns the recurrent policy network and local enhancement network through maximizing the long-term reward that reflects the hallucination performance over the whole image. Therefore, our proposed Attention-FH is capable of adaptively personalizing an optimal searching path for each face image according to its own characteristic. Extensive experiments show our approach significantly surpasses the state-of-the-arts on in-the-wild faces with large pose and illumination variations.},
   author = {Qingxing Cao and Liang Lin and Yukai Shi and Xiaodan Liang and Guanbin Li},
   doi = {10.1109/CVPR.2017.180},
   booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
   title = {Attention-aware face hallucination via deep reinforcement learning},
   volume = {2017-January},
   year = {2017},
}
@inproceedings{s38,
   abstract = {Conventional face super-resolution methods, also known as face hallucination, are limited up to 2∼4× scaling factors where 4 ∼ 16 additional pixels are estimated for each given pixel. Besides, they become very fragile when the input low-resolution image size is too small that only little information is available in the input image. To address these shortcomings, we present a discriminative generative network that can ultra-resolve a very low resolution face image of size 16 × 16 pixels to its 8× larger version by reconstructing 64 pixels from a single pixel. We introduce a pixel-wise l2 regularization term to the generative model and exploit the feedback of the discriminative network to make the upsampled face images more similar to real ones. In our framework, the discriminative network learns the essential constituent parts of the faces and the generative network blends these parts in the most accurate fashion to the input image. Since only frontal and ordinary aligned images are used in training, our method can ultra-resolve a wide range of very low-resolution images directly regardless of pose and facial expression variations. Our extensive experimental evaluations demonstrate that the presented ultra-resolution by discriminative generative networks (URDGN) achieves more appealing results than the state-of-the-art.},
   author = {Xin Yu and Fatih Porikli},
   doi = {10.1007/978-3-319-46454-1_20},
   issn = {16113349},
   booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   title = {Ultra-resolving face images by discriminative generative networks},
   volume = {9909 LNCS},
   year = {2016},
}
@inproceedings{s39,
   abstract = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image superresolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4× upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.},
   author = {Christian Ledig and Lucas Theis and Ferenc Huszár and Jose Caballero and Andrew Cunningham and Alejandro Acosta and Andrew Aitken and Alykhan Tejani and Johannes Totz and Zehan Wang and Wenzhe Shi},
   doi = {10.1109/CVPR.2017.19},
   booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
   title = {Photo-realistic single image super-resolution using a generative adversarial network},
   volume = {2017-January},
   year = {2017},
}
@inproceedings{s40,
   abstract = {We present an algorithm to directly restore a clear highresolution image from a blurry low-resolution input. This problem is highly ill-posed and the basic assumptions for existing super-resolution methods (requiring clear input) and deblurring methods (requiring high-resolution input) no longer hold. We focus on face and text images and adopt a generative adversarial network (GAN) to learn a category-specific prior to solve this problem. However, the basic GAN formulation does not generate realistic highresolution images. In this work, we introduce novel training losses that help recover fine details. We also present a multi-class GAN that can process multi-class image restoration tasks, i.e., face and text images, using a single generator network. Extensive experiments demonstrate that our method performs favorably against the state-of-the-art methods on both synthetic and real-world images at a lower computational cost.},
   author = {Xiangyu Xu and Deqing Sun and Jinshan Pan and Yujin Zhang and Hanspeter Pfister and Ming Hsuan Yang},
   doi = {10.1109/ICCV.2017.36},
   issn = {15505499},
   booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
   title = {Learning to Super-Resolve Blurry Face and Text Images},
   volume = {2017-October},
   year = {2017},
}
@inproceedings{s41,
   abstract = {Though existing face hallucination methods achieve great performance on the global region evaluation, most of them cannot recover local attributes accurately, especially when super-resolving a very low-resolution face image from 14 × 12 pixels to its 8 × larger one. In this paper, we propose a brand new Attribute Augmented Convolutional Neural Network (AACNN) to assist face hallucination by exploiting facial attributes. The goal is to augment face hallucination, particularly the local regions, with informative attribute description. More specifically, our method fuses the advantages of both image domain and attribute domain, which significantly assists facial attributes recovery. Extensive experiments demonstrate that our proposed method achieves superior visual quality of hallucination on both local region and global region against the state-of-the-art methods. In addition, our AACNN still improves the performance of hallucination adaptively with partial attribute input.},
   author = {Cheng Han Lee and Kaipeng Zhang and Hu Cheng Lee and Chia Wen Cheng and Winston Hsu},
   doi = {10.1109/CVPRW.2018.00115},
   issn = {21607516},
   booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
   title = {Attribute augmented convolutional neural network for face hallucination},
   volume = {2018-June},
   year = {2018},
}
@article{s42,
   abstract = {Given a tiny face image, existing face hallucination methods aim at super-resolving its high-resolution (HR) counterpart by learning a mapping from an exemplary dataset. Since a low-resolution (LR) input patch may correspond to many HR candidate patches, this ambiguity may lead to distorted HR facial details and wrong attributes such as gender reversal and rejuvenation. An LR input contains low-frequency facial components of its HR version while its residual face image, defined as the difference between the HR ground-truth and interpolated LR images, contains the missing high-frequency facial details. We demonstrate that supplementing residual images or feature maps with additional facial attribute information can significantly reduce the ambiguity in face super-resolution. To explore this idea, we develop an attribute-embedded upsampling network, which consists of an upsampling network and a discriminative network. The upsampling network is composed of an autoencoder with skip-connections, which incorporates facial attribute vectors into the residual features of LR inputs at the bottleneck of the autoencoder, and deconvolutional layers used for upsampling. The discriminative network is designed to examine whether super-resolved faces contain the desired attributes or not and then its loss is used for updating the upsampling network. In this manner, we can super-resolve tiny (16×16 pixels) unaligned face images with a large upscaling factor of 8× while reducing the uncertainty of one-to-many mappings remarkably. By conducting extensive evaluations on a large-scale dataset, we demonstrate that our method achieves superior face hallucination results and outperforms the state-of-the-art.},
   author = {Xin Yu and Basura Fernando and Richard Hartley and Fatih Porikli},
   doi = {10.1109/TPAMI.2019.2916881},
   issn = {19393539},
   issue = {11},
   journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
   title = {Semantic face hallucination: Super-resolving very low-resolution face images with supplementary attributes},
   volume = {42},
   year = {2020},
}
@article{s43,
   abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
   author = {Mehdi Mirza and Simon Osindero},
   journal = {arXiv:1411.1784v1 [cs.LG] 6 Nov 2014 Conditional},
   title = {Conditional Generative Adversarial Nets Mehdi},
   year = {2018},
}
@article{sr056,
  title={A deep journey into super-resolution: A survey},
  author={Anwar, Saeed and Khan, Salman and Barnes, Nick},
  journal={ACM Computing Surveys (CSUR)},
  volume={53},
  number={3},
  pages={1--34},
  year={2020},
  publisher={ACM New York, NY, USA}
}
@article{sr057,
  title={Deep learning for image super-resolution: A survey},
  author={Wang, Zhihao and Chen, Jian and Hoi, Steven CH},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={43},
  number={10},
  pages={3365--3387},
  year={2020},
  publisher={IEEE}
}
@article{sr058,
  title={Deep learning methods in real-time image super-resolution: a survey},
  author={Li, Xiaofang and Wu, Yirui and Zhang, Wen and Wang, Ruichao and Hou, Feng},
  journal={Journal of Real-Time Image Processing},
  volume={17},
  number={6},
  pages={1885--1909},
  year={2020},
  publisher={Springer}
}

@article{ddamfn,
  title={A dual-direction attention mixed feature network for facial expression recognition},
  author={Zhang, Saining and Zhang, Yuhang and Zhang, Ye and Wang, Yufei and Song, Zhigang},
  journal={Electronics},
  volume={12},
  number={17},
  pages={3595},
  year={2023},
  publisher={MDPI}
}