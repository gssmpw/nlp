 \documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{subcaption}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}


% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[nohyperref]{icmlarxiv}
\usepackage[accepted]{icmlarxiv}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{caption}
\usepackage{tikz}

\usepackage{lipsum} 

\usepackage{tcolorbox}

\usepackage{pifont}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{textcomp}

\usepackage{makecell,multirow}
\usepackage{amsmath,amssymb,amsthm,dsfont,empheq}

\usepackage{syntonly,bm,wrapfig}
\usepackage{xcolor,cases,colortbl}
\usepackage{mathtools}
\usepackage{authblk}
\usepackage[toc,page,header]{appendix}
\usepackage{minitoc}
\usepackage{enumerate}



\usepackage{tikz}
\usepackage{tcolorbox}
\newtcolorbox{thmbox}{colback=cyan!5,colframe=white}
\newtcolorbox{questionbox}{colback=red!5!white,colframe=white}
\newtcolorbox{updatebox}{colback=white,colframe=black}
\newtcolorbox{problembox}{colback=gray!15,colframe=black,width=1\textwidth}
\newtcolorbox{smallproblembox}{colback=gray!15,colframe=black,width=0.8\textwidth}

\usepackage{tablefootnote}
\setlength {\marginparwidth}{2.5cm}
\usepackage[
  color=blue!20!white,
  textsize=tiny,
  colorinlistoftodos,
  prependcaption,
]{todonotes}

\DeclareUnicodeCharacter{1EA1}{\d{a}} 

\renewcommand \thepart{}
\renewcommand \partname{}


\definecolor{ao}{rgb}{0.0, 0.5, 0.0}

\newcommand{\cmark}{\textcolor{ao}{\ding{51}}}
\newcommand{\xmark}{\textcolor{purple}{\ding{55}}}
\input{mysymbol.sty}


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\EE}{\mathbb{E}}
\DeclarePairedDelimiter{\dotp}{\langle}{\rangle}
\DeclarePairedDelimiter{\dotP}{\Big\langle}{\Big\rangle}
\DeclareMathOperator{\Var}{Var}
 
\def \bxi {{\boldsymbol \gamma}}
\def \balpha {{\boldsymbol \alpha}}
\def \bbeta {{\boldsymbol \beta}}
\def \bphi {{\boldsymbol \phi}}

\def \bu {{\boldsymbol u}}
\def \bv {{\boldsymbol v}}
\def \bw {{\boldsymbol w}}
\def \bW {{\boldsymbol W}}
\def \bz {{\boldsymbol z}}
\def \bZ {{\boldsymbol Z}}
\def\NoNumber#1{{\def\alglinenumber##1{}\State #1}\addtocounter{ALG@line}{-1}}
\DeclareMathOperator*{\mini}{minimize}
\newcommand{\blue}[1]{{\color{blue}#1}}
\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\TC}[1]{{\color{red}[#1]}}
\newcommand{\QX}[1]{{\color{blue}[#1]}}
\newcommand{\yellow}[1]{{\color{yellow}#1}}
\newcommand{\yellowsquare}[1]{\begin{tikzpicture}
    \fill[yellow] (0,0) rectangle (0.2,0.2);
\end{tikzpicture}}
\newcommand{\TD}[1]{{\color{red}[TODO: #1]}}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}
\newtheorem{observation}{Observation}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{innercustomthm}{Restatement of Theorem}
\newenvironment{customthm}[1]
  {\renewcommand\theinnercustomthm{#1}\innercustomthm}
  {\endinnercustomthm}
\newcommand{\E}{\mathbb{E}}

\newtheorem{innercustomlm}{Restatement of Lemma}
\newenvironment{customlm}[1]
  {\renewcommand\theinnercustomlm{#1}\innercustomlm}
  {\endinnercustomlm}

\definecolor{LightCyan}{rgb}{0.88,1,1} 
\definecolor{Lightpurple}{rgb}{0.9,0.9,1} 

\renewcommand \thepart{}
\renewcommand \partname{}

\makeatletter
\def\thanks#1{\protected@xdef\@thanks{\@thanks
        \protect\footnotetext{#1}}}
\makeatother
\renewcommand{\arraystretch}{1.25}

\newcommand{\FullTitle}{
A First-order Generative Bilevel Optimization
Framework for Diffusion Models
}


\begin{document}


\twocolumn[
\icmltitle{\FullTitle}


\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Quan Xiao}{RPI}
\icmlauthor{Hui Yuan}{Princeton}
\icmlauthor{A F M Saif}{RPI}
\icmlauthor{Gaowen Liu}{Cisco}
\icmlauthor{Ramana Kompella}{Cisco}
\icmlauthor{Mengdi Wang}{Princeton}
\icmlauthor{Tianyi Chen}{RPI}
\end{icmlauthorlist}

\icmlaffiliation{RPI}{Department of Electrical, Computer, and Systems Engineering, Rensselaer Polytechnic Institute, Troy, NY}
\icmlaffiliation{Princeton}{Department of Electrical and Computer Engineering, Princeton University, NJ}
\icmlaffiliation{Cisco}{Cisco Research}
\icmlcorrespondingauthor{Quan Xiao}{quanx1808@gmail.com}
\icmlcorrespondingauthor{Tianyi Chen}{chentianyi19@gmail.com}


\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]
 


\doparttoc % 
\faketableofcontents % 
 
\printAffiliationsAndNotice{ } % 

\begin{abstract}
Diffusion models, which iteratively denoise data samples to synthesize high-quality outputs, have achieved empirical success across domains. However, optimizing these models for downstream tasks often involves nested bilevel structures—such as tuning hyperparameters for fine-tuning tasks or noise schedules in training dynamics—where traditional bilevel methods fail due to the infinite-dimensional probability space and prohibitive sampling costs. We formalize this challenge as a generative bilevel optimization problem and address two key scenarios: (1) fine-tuning pre-trained models via an inference-only lower-level solver paired with a sample-efficient gradient estimator for the upper level, and (2) training diffusion model from scratch with noise schedule optimization by reparameterizing the lower-level problem and designing a computationally tractable gradient estimator. Our first-order bilevel framework overcomes the incompatibility of conventional bilevel methods with diffusion processes, offering theoretical grounding and computational practicality. Experiments demonstrate that our method outperforms existing fine-tuning and hyperparameter search baselines.  
\end{abstract}

\section{Introduction} 
Bilevel optimization, which involves nested problems where a \emph{lower-level} optimization is constrained by the solution of an \emph{upper-level} objective, has evolved from its theoretical origins in the 1970s \citep{bracken1973mathematical} into a cornerstone of modern machine learning. 
Its ability to model hierarchical dependencies makes it ideal for complex learning tasks, such as hyperparameter tuning \citep{pedregosa2016hyperparameter}, meta-learning \citep{finn2017model}, reinforcement learning \citep{stadie2020learning,shen2024principled}, adversarial training \citep{zhang2022revisiting}, neural architecture search \citep{liu2018darts} and LLM alignment  \citep{zakarias2024bissl,shen2024seal,gong2022bi,qin2024bidora}. 
 



Meanwhile, the diffusion model has achieved remarkable success across various domains, particularly in image \cite{croitoru2023diffusion,ho2020denoising,songdenoising,songscore}, audio \citep{yang2023diffsound,liu2023audioldm}, and biological sequence generation  \citep{jing2022torsional,wu2022diffusion}. Yet, optimizing these models for downstream tasks often introduces nested objectives: for instance, fine-tuning pre-trained models to maximize task-specific rewards (e.g., aesthetic quality \citep{yang2024using}) risks \emph{reward over-optimization}, where generated samples become unrealistic despite high scores. To mitigate this, incorporating an auxiliary objective can balance the optimization process and prevent excessive focus on a single metric; see Section~\ref{sec:reward_finetune}. Similarly, designing noise schedules for the forward/reverse processes \citep{chen2023importance} requires balancing sample quality against computational cost - a problem inherently requiring coordination between training dynamics (lower-level) and scheduler parameters (upper-level); see  Section~\ref{sec:noise_schedule}. An overview of the bilevel generative problem is shown in Figure \ref{fig:main_figure}. 
All of the above illustrate the need for developing bilevel algorithms that are friendly to diffusion models. However, applying existing bilevel methods to \emph{diffusion models} - which operate in infinite-dimensional probability spaces and require costly sampling steps—poses unique challenges. Traditional gradient-based bilevel approaches \citep{franceschi2017forward,maclaurin2015gradient} rely on exact lower-level solutions and dense gradient backpropagation, both infeasible for diffusion processes where generating a single sample can involve hundreds of neural network evaluations.

\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.47\linewidth]{figures/exp1.png}
    % \hfill
    \hspace{0.005\linewidth}
    \includegraphics[width=0.5\linewidth]{figures/exp2.png}
    \caption{An overview of bilevel generative optimization problems. (Left) Fine-tuning diffusion model with entropy regularization strength parameter $\lambda$. (Right) Noise parameter $q_t$ scheduling problem in the diffusion model. }
    \label{fig:main_figure}
    \vspace{-0.3cm}
\end{figure*}

In this paper, we focus on designing computationally efficient and diffusion-friendly approaches for the following problem that we call the {\em generative bilevel problem}:
\begin{equation}\label{opt0}
\min_{x\in\mathcal{X},y\in\mathcal{P}}~ f(x,y), ~~\text{ s.t. }~~ y\in\mathcal{S}(x)=\argmin_{y\in\mathcal{P}}~ g(x,y)
\end{equation}
where $x$ is some hyperparameter in the diffusion model and $y$ represents a distribution we aim to learn, which can be either an image distribution or a noise distribution. Both the upper-level $f: \mathbb{R}^{d_x}\times \mathcal{P}\rightarrow \mathbb{R}$ and lower-level objective functions $g: \mathbb{R}^{d_x}\times\mathcal{P}\rightarrow \mathbb{R}$ are continuously differentiable, $\mathcal{X}\subset \mathbb{R}^{d}$ is a closed set, and $\mathcal{P}$ is the probability space. We study the optimistic setting where we select the best response distribution $y\in\mathcal{S}(x)$ to minimize the upper-level loss. Let us denote the minimal lower-level objective value as the value function $g^*(x)=\min_{y\in\mathcal{S}(x)} g(x,y)$ and the nested objective as $F(x)=\min_{y\in\mathcal{S}(x)}f(x,y)$. 

The key challenges of solving the {\em generative bilevel problem} \eqref{opt0} are threefold. First, the lower-level variable $y$ is typically a distribution that operates in infinite dimensional probability space. However, direct access to or optimization over distribution is not feasible; we only have access to samples, and estimating distributions from samples is highly sample-inefficient. Therefore, the gradient over the distribution is inaccessible, making traditional gradient-based bilevel approaches \citep{shen2023penalty,kwon2023fully,ji2021bilevel,chen2021closing,hong2020two,ghadimi2018approximation} generally inapplicable. Second, the objectives $f(x,y)$ and $g(x,y)$ are usually some measures of the sample quality and might not have an explicit form in terms of the hyperparameter $x$, so that $\nabla_x f(x,y)$ and $\nabla_x g(x,y)$ are either not explicitly given or requires sample efficient approximation. 
Third, for fine-tuning the diffusion model, existing literature related to bilevel fine-tuning on diffusion model \citep{clarkdirectly,marion2024implicit} requires backpropagations over the pre-trained model, which often suffers from high computational and memory costs. In contrast, we design an {\em inference-only} bilevel method for this task. 

To address these challenges, we classify generative bilevel problems into two categories: (1) those with a pre-trained model, where the target lower-level distribution is the image distribution (e.g., fine-tuning diffusion models), and (2) those without a pre-trained model, where the target lower-level distribution is the noise distribution in the forward process (e.g., noise scheduling during diffusion model training); see an overview in Figure \ref{fig:main_figure}. Two applications we considered are essentially bilevel hyperparameter optimization. To the best of our knowledge, this is the first study to explore bilevel hyperparameter optimization in the context of diffusion models. 
We develop a {\em first-order} bilevel framework in both categories to solve \eqref{opt0}. The primary differences with non-generative bilevel methods are: 

{\bf D1)} for fine-tuning diffusion models that include a pre-trained model, we adopt guidance-based approaches rather than {\em gradient-based} methods for the lower-level and penalty problems concerning distribution $y$, ensuring the process is training-free and inference-only; 

{\bf D2)} for the noise scheduling problem without a pre-trained model, we optimize a noise proxy parameterized by a score neural network, rather than performing noise distribution matching directly in the lower-level problem of \eqref{opt0}; and, 

{\bf D3)} we design scalable methods to estimate $\nabla_x f(x,y)$ and $\nabla_x g(x,y)$ for two specific applications, i.e. leveraging the closed form of them and proposing sample-efficient estimation for the fine-tuned diffusion model, and using zeroth-order method to estimate $\nabla_x f(x,y)$ in noise scheduling problem without a pre-trained diffusion model. 


\section{Bilevel Optimization and Diffusion Models}
In this section, we will review some preliminaries on bilevel optimization, diffusion models, and guided generation, as well as two motivating applications of studying diffusion models in bilevel optimization. 


\subsection{Bilevel optimization}
An efficient approach to solving bilevel optimization in \eqref{opt0} is to reformulate \eqref{opt0} to its single-level penalty problem  \cite{shen2023penalty,kwon2023penalty}, given by
\begin{align}\label{penalty_form}
\min_{x\in\mathcal{X},y\in\mathcal{P}}~\mathcal{L}_\gamma (x,y):= f(x,y)+\gamma(g(x,y)-g^*(x))
\end{align}
and then optimize the upper-level and lower-level variables jointly. By setting the penalty constant $\gamma={\cal O}(\epsilon^{-0.5})$ inversely proportional to the target accuracy $\epsilon$, the single-level problem \eqref{penalty_form} is an ${\cal O}(\epsilon)$ approximate problem to the original bilevel problem \eqref{opt0}. This method builds upon equilibrium backpropagation \citep{scellier2017equilibrium,zucchet2022beyond}, which introduced a similar framework for strongly convex lower-level problems. However, recent works extend the study to accommodate some nonconvex lower-level problems and lay the foundation for broader applications \cite{shen2023penalty,kwon2023penalty}.


\noindent\textbf{Gradient-based bilevel approaches. } In the context of diffusion models, the upper-level and lower-level variables, $x$ and $y$, usually have different meanings. To facilitate algorithm design tailored to diffusion models, we decompose \eqref{penalty_form} into separate $y$- and $x$-optimization problems, that is   
\begin{align}\label{penalty-decompose}
&\min_{x\in\mathcal{X}} \mathcal{L}_\gamma^*(x), ~~~\text{ with } ~~~ \mathcal{L}_\gamma^*(x)=\min_{y\in\mathcal{S}_\gamma(x)} \mathcal{L}_\gamma (x,y)\nonumber\\
&\text{ and } ~~  \mathcal{S}_\gamma(x):=\argmin_{y\in\mathcal{P}}\mathcal{L}_\gamma(x,y).
\end{align}

Under some mild conditions specified in \citep{kwon2023penalty},  $\mathcal{L}_\gamma^*(x)$ is differentiable with the gradient given by
\begin{equation}\label{L_grad}
\!\!\nabla\mathcal{L}_\gamma^*(x)\!=\nabla_x f(x,z^*)+\gamma(\nabla_x g(x,z^*)-\nabla_x g(x,y^*))
\end{equation}
where $z^*\in\mathcal{S}_\gamma^*(x)$ in \eqref{penalty-decompose} and $y^*\in\mathcal{S}(x)$ in \eqref{opt0} are any solutions. Moreover,  $\nabla\mathcal{L}_\gamma^*(x)$ is a proxy of the original gradient, with the error bounded by $\|\nabla\mathcal{L}_\gamma^*(x) - \nabla F(x)\| \leq \mathcal{O}(1/\gamma)$; see details in Section \ref{sec:diff-bilevel}. Therefore, when the lower-level problem $g(x,y)$ and the penalty problem $\mathcal{L}_\gamma(x,y)$ are solvable with solutions $y^*,z^*$, we can approximate $\nabla F(x)$ and update the upper-level variable $x$. 



\subsection{Diffusion models}
The goal of diffusion models \citep{songscore,ho2020denoising,songdenoising} is to generate samples that match the some target distribution. This is achieved through two complementary stochastic differential equations (SDEs): a forward process, which gradually transforms input data into random noise, and a backward process, which reconstructs the data by denoising the noise.  Central to this framework is the learning of a \emph{score function}, which captures the gradient of the log-likelihood of the data distribution and is invariant to the input. This score function, learned during the forward process, is then used to guide the reverse sampling process. Below, we provide a brief overview of these processes.

\noindent\textbf{Forward process. } The forward process of a diffusion model gradually transforms the original data \({U}_0 \in \mathbb{R}^D\) into pure noise by incrementally adding noise \(\mathrm{d} {W}_t\). This transformation simplifies complex data distributions into a tractable form, enabling efficient modeling and sampling. The following SDE formally describes the process:
\begin{align}\label{eq: forward}
\mathrm{d} U_t=-\frac{1}{2} q(t) U_t ~\mathrm{d} t+\sqrt{q(t)} \mathrm{d} W_t, ~~~\text { for }~ q(t)>0
\end{align}
where the initial $U_0$ is a random variable drawn from the data distribution $p_{\text{data}}$, $\{W_t\}_{t\geq 0}$ denotes the standard Wiener process, $q(t)$ is a nondecreasing noise scheduling function, and $U_t$ represents the noise-corrupted data distribution at time $t$. Under mild conditions and for a sufficiently large timestep $T$, \eqref{eq: forward} transforms the original distribution $p_{\text{data}}$ into a distribution close to Gaussian random noise $\mathcal{N}(0, \mathbf{I}_D)$.

 
\noindent\textbf{Backward process. } Given the forward process in \eqref{eq: forward}, the reverse-time SDE is defined by 
\begin{align}\label{eq: backward}
\mathrm{d} \widetilde{U}_t&=\left[\frac{1}{2} q(T-t) \widetilde{U}_t+{\nabla \log p_{T-t}\left(\widetilde{U}_t\right)}\right] \mathrm{d} t\nonumber\\
&+\sqrt{q(T-t)}\mathrm{d} \bar{W}_t,~~~ \text { for }~ t \in[0, T),
\end{align}
where $p_t(\cdot)$ is the marginal distribution of $U_t$ in the forward process. Let $u$ denote the realization of a random variable $U_t$ and $s(u,t):=\nabla\log p_t(u)$ is the unknown score function that has to be estimated in practice. 

\noindent\textbf{Score matching loss. } 
To estimate the score function $s(u,t)$, we train a parameterized score network $s_\theta(u,t)$ by tracking the gradient of the log-likelihood of probability from the forward process, which eliminates the need for distribution normalization. Specifically, we minimize the following score-matching loss \citep{songscore}: 
\begin{align}\label{score-matching}
&\min_\theta~\mathrm{L}_{\text{SM}}(\theta,  u)\nonumber\\
&:= \mathbb{E}_{t,u_0\sim p_{\text{data}}, u_t|u_0}\left[\|\nabla \log p_t(u_t|u_0)-s_\theta(u_t,t)\|^2\right]
\end{align}
where $t$ is uniformly sampled over the interval $[0,T]$, and $p_t(u_t|u_0)$ is the conditional distribution of $u_t$ over the initial image $u_0$. Typically, the score network $s_\theta(u,t)$ is parametrized by a U-Net model \citep{ronneberger2015u}. 

 



\noindent\textbf{Guided generation for optimization. } By leveraging the estimated score function $ s_\theta(u,T-t)$ from \eqref{score-matching} to replace $\nabla \log p_{T-t}(u)$, samples can be generated through the backward process \eqref{eq: backward}. Furthermore, we can introduce guidance terms into the backward process to steer the generation toward the desired reward $V=v$. Using the conditional score function, the goal is to estimate the conditional distribution $\mathbb{P}(U| V=v)$. By Bayes' rule, we have $\nabla_{u_t} \log p_t(u_t\mid v)\!=\!\nabla\log p_t(u_t)+\nabla_{u_t}\log p_t(v\mid u_t)$. Given the {\em pre-trained} score  $s_\theta(u,t)\approx\nabla\log p_t(u_t)$ for the unconditional forward process, and the guidance term $G\approx\nabla_{u_t}\log p_t(v\mid u_t)$, the backward SDE is defined by  
\begin{align}\label{eq: backward_guide}
\mathrm{d} \widetilde{U}_t&=\left[\frac{1}{2} q(T-t)\widetilde{U}_t+ s_\theta(\widetilde{U}_t, T-t)+\textit{G}(\widetilde{U}_t, T-t)\right] \mathrm{d} t\nonumber\\
&+\sqrt{q(T-t)}\mathrm{d} \bar{W}_t, ~~t\in[0,T). 
\end{align}

With a proper guidance term and an increasing reward value $v$, the guided sampling process in \eqref{eq: backward_guide} generates samples that maximize the given reward function $r(\cdot)$ with an entropy regularization to the pre-trained distribution \citep{guo2024gradient,uehara2024fine}. The design of the guidance term and the complete procedure of guided generation are outlined in Algorithm \ref{alg:gradient_guided_diffusion} and can be found in Appendix \ref{sec:complete_alg}. 

\section{Applications of Generative Bilevel Problems}\label{sec.apply}
In this section, we introduce two bilevel optimization problems in diffusion models: one in the fine-tuning stage with a pre-trained model, and another in the pre-training stage. 







\subsection{Reward fine-tuning in diffusion models}\label{sec:reward_finetune} 

\vspace{-.1cm}
\begin{figure}[htbp]
    \centering
    \begin{tabular}{@{}c@{\hspace{.5mm}}c@{\hspace{.5mm}}c@{\hspace{.5mm}}c@{}}
        \includegraphics[width=0.12\textwidth]{figures/first_exp/all_images/other_images/horse_fake_step_7_01.png} &
        \includegraphics[width=0.12\textwidth]{figures/first_exp/all_images/other_images/horse_bilevel_step7.png} &
        \includegraphics[width=0.12\textwidth]{figures/first_exp/all_images/other_images/elephant_01.png} &
        \includegraphics[width=0.12\textwidth]{figures/first_exp/all_images/other_images/elephant_bilevel.png} \\
        (a) $\lambda=0.01$ & (b) $\lambda=55.5$ & (c) $\lambda=0.01$ & (d) $\lambda=44.3$
    \end{tabular}
    \vspace{-.3cm}
    \caption{Visualization of generated images: (a) Horse and (c) elephant generated with $\lambda = 0.01$, leading to reward over-optimization and resulting in more abstract images misaligned with captions. In contrast, (b) horse and (d) elephant, generated using the bilevel method with $\lambda$ optimized via CLIP score. This suggests CLIP score is a proper metrics for $\lambda$ selection. More visualizations are shown in Figure \ref{fig:image_change_with_lambda} and can be found in Appendix. }
\vspace{-.3cm}
    \label{fig:diff_lambda}
\end{figure}

In practice, fine-tuning a pre-trained diffusion model is often necessary to generate samples that achieve high reward. However, if the model over-focuses on reward maximization, it may generate overly aggressive samples that diverge from realistic distributions \cite{gao2023scaling}. Therefore, a well-tuned model must carefully balance reward optimization with adherence to the pre-trained data distribution. This balance can be formulated as a bilevel optimization problem:
\begin{align}\label{opt-reward}
&\min_{\lambda\in\mathbb{R}_+, p\in\mathcal{S}(\lambda)}~ f(\lambda,p):=-\mathbb{E}_{u \sim p}[r_1(u)]  \\
&\text{ s.t. }~\mathcal{S}(\lambda)={\argmin_{p'\in\mathcal{P}}}~  \underbrace{-\mathbb{E}_{u \sim p'}[r_2(u)] +\lambda \mathrm{KL}(p' \| p_{\text {data}})}_{g(\lambda,p):=}\nonumber
\end{align}
where the lower level adjusts data distribution by optimizing an entropy regularized reward learning problem \citep{uehara2024fine,fan2024reinforcement}, and the upper level selects the best-response entropy strength $\lambda$ by another realistic-measured reward. As shown in Figure \ref{fig:diff_lambda}, the upper-level reward $r_1(\cdot)$ can be caption alignment score (CLIP), further refining the generated samples to align with the provided captions. Since the pre-trained diffusion model is available, $p_{\text {data}}$ is measured by the pre-trained score $ s_{\theta}(u,t)$. 

\subsection{Noise scheduling in diffusion models} \label{sec:noise_schedule}  

Tuning the noise magnitude $q(t)$ in the forward and backward processes is essential for generating high-quality images with diffusion models. Instead of relying on cross-validation, bilevel optimization can automatically learn an effective noise scheduler, enabling the model to learn useful features while efficiently transforming inputs into noise.
In this application, bilevel problem \eqref{opt-schedule} optimizes the noise scheduler in the upper level to minimize a quality score, while the lower level learns the noise distribution from the forward process to match the true Gaussian noise. Instead of framing the lower-level problem as a distribution matching task, we optimize the distribution's parameter $\theta$ as follows.
\begin{align}\label{opt-schedule}
 &\min_{q\geq 0, \theta\in\mathcal{S}(q)}~ f(q,\theta):=\mathbb{E}_{\tilde u(q) \sim p_\theta}[\mathrm{L}_{\text{SQ}}(\tilde u(q))],\nonumber\\
&\text{ s.t. }~~\mathcal{S}(q)={\argmin_{\theta'\in\mathbb{R}^d}}~ g(q,\theta):= \mathrm{L}_{\text{SM}}(\theta',  u(q))
\end{align}
where $p_\theta$ is the probability distribution generated by the backward SDE \eqref{eq: backward} associated with parameter $\theta$ in the score network, $u(q)$ collects samples in the forward pass from $[0,T]$ defined by schedule $q$, $\mathrm{L}_{\text{SM}}(\cdot)$ is the score matching loss defined in \eqref{score-matching}, and $\mathrm{L}_{\text{SQ}}(\cdot)$ measures the scheduling quality. For a given schedule $q=\{q(t)\}_{t=1}^T$, at the lower-level, we generate samples $\{u_t\}_{t=1}^T$ according to $q$ and optimize the score network $\theta$ to predict a good proxy of $\nabla\log p_t(u_t)$. Then, at the upper level, we automatically tune the scheduling parameter $q$ by sampling from the reverse process with probability parameterized by $\theta$ and evaluating the quality of the generated samples by $\mathrm{L}_{\text{SQ}}(\cdot)$. Typical choice of $\mathrm{L}_{\text{SQ}}(\cdot)$ can be Fréchet Inception Distance (FID) score, a commonly used metric to measure the quality of the generated image, and is differentiable; see details in Appendix \ref{sec:bilevel_noise_appendix}. 


\section{Diffusion-friendly Bilevel Methods} 
In this section, we first present the meta-algorithm for the generative bilevel problem \eqref{opt0}, followed by tailored subroutines for two specific applications in Section \ref{sec.apply}. 

\emph{A meta bilevel algorithm.} 
To solve the generative bilevel problem \eqref{opt0}, we can
compute the gradient $\nabla\mathcal{L}_\gamma^*(x)$ according to \eqref{L_grad}, where the solutions $z^*\in\mathcal{S}_\gamma^*(x)$ and $y^*\in\mathcal{S}(x)$ can be approximated using numerical oracles (e.g., gradient descent or Adam \citep{kingma2014adam}). Specifically, at each iteration $k$, we first solve the $y$-problem over $\mathcal{L}_\gamma(x^k, y)$ and $g(x^k, y)$ to obtain near-optimal solutions  $y^k\approx y^*$ and $z^k\approx z^*$. We then perform gradient descent updates for the $x$-problem according to \eqref{L_grad} with $y^k$ and $z^k$.
The procedure is detailed in Algorithm \ref{alg: GBLO}. 

 

\subsection{Strategy with pre-trained diffusion models}
We first focus on algorithms to guide the generated data distribution of the pre-trained diffusion model toward the optimal solution of \eqref{opt-reward} in the application of reward fine-tuning diffusion model, with a pre-trained score network. 



\noindent\textbf{Guided-sampling from the lower-level variable. } For a given $\lambda$, we want to solve two optimization problems $\min_p g (\lambda,p)$ and $\min_p \mathcal{L}_\gamma (\lambda,p)$. For single-level optimization, the distribution generated by the guided backward sampling in Algorithm \ref{alg:gradient_guided_diffusion} converges to the optimal distribution for maximizing the reward function $r$ with an entropy regularization term to ensure the generated samples remain close to the pre-training data \citep{guo2024gradient}. This suggests Algorithm \ref{alg:gradient_guided_diffusion} can be used to solve the lower-level problem and penalty problem with respect to $p$.  The guidance terms added in the backward sampling process for the lower-level and penalty problems, are defined as 
\begin{subequations}
\begin{align}
G_{\text{lower}}(t, u, \lambda)&=\mathrm{G}\left(u_t, t; r_2\right)/\lambda\label{G_lower}\\
G_{\text{penalty}}(t, u, \lambda)&=\mathrm{G}\left(u_t, t; r_1/\gamma+r_2\right)/\lambda\label{G_penalty}
\end{align}
\end{subequations}
where $\mathrm{G}\left(u_t, t; r\right)$ is defined in \eqref{g_loss}. Then Algorithm \ref{alg:gradient_guided_diffusion} is able to generate samples that are approximately from the optimal lower-level and penalty distribution. 

\noindent\textbf{Gradient update for the upper-level $\lambda$. } By substituting the definition of the objective function into \eqref{L_grad}, we obtain 
\begin{align}\label{Grad_lambda}
\nabla\mathcal{L}_\gamma^*(\lambda)&=\gamma(\mathrm{KL}(p_\gamma^*(\lambda)  \|p_{\text {data}})-\mathrm{KL}(p^*(\lambda) \| p_{\text {data}}))
\end{align}
where $p_\gamma^*(\lambda)\in\argmin_p \mathcal{L}_\gamma(\lambda,p)$ and $p^*(\lambda)\in\argmin_p g(\lambda,p)$. Following the gradient-based bilevel method \citep{kwon2023fully,shen2023penalty}, a direct way to estimate $\nabla\mathcal{L}_\gamma^*(\lambda)$ is to use guided backward sampling in Algorithm \ref{alg:gradient_guided_diffusion} with \eqref{G_lower} and \eqref{G_penalty} to obtain samples from $p^*(\lambda)$ and $p_\gamma^*(\lambda)$, and then compute the KL divergence from samples using kernel-based probability estimation. However, it has two drawbacks: 1) each $\lambda$ update requires guided backward sampling, which is computationally expensive; and 2) when the number of samples is less than the dimensionality of the data, the covariance matrix becomes rank-deficient, which makes kernel-based density estimation impossible. 

To address these issues, by leveraging the marginal density induced by the SDE, we can derive a closed-form expression for the upper-level gradient in terms of samples. 
\begin{proposition}
The gradient in \eqref{Grad_lambda} takes the form 
\begin{align}\label{eq:upper-gradient}
\nabla\mathcal{L}_\gamma^*(\lambda)&=-\mathbb{E}_{u\sim p_{\text{data}}}\left[{\lambda}^{-1}{r_1(u)}\right]-\gamma\log \mathbb{E}_{u\sim p_{\text{data}}}\left[e^{\frac{r_2(u)}{\lambda}}\right]\nonumber\\
&+\gamma\log \mathbb{E}_{u\sim p_{\text{data}}}\left[e^{\frac{r_1(u)/\gamma+r_2(u)}{\lambda}}\right].
\end{align}
\label{prop:closed_upper}
\end{proposition}
 
\begin{remark}
This proposition enables us to estimate $\nabla\mathcal{L}_\gamma^*(\lambda)$ directly from the pre-trained distribution, eliminating the need for guided backward sampling in Algorithm \ref{alg:gradient_guided_diffusion} to compute $p^*(\lambda)$ and $p_\gamma^*(\lambda)$ for each $\lambda$ and thus, significantly reducing computational cost. The proof is in Appendix \ref{sec:KL_closed}. 
\end{remark}

\begin{algorithm}[tb]
\caption{A meta generative bilevel algorithm}
\label{alg: GBLO}
\begin{algorithmic}[1]
\STATE \textbf{Inputs:} Initialization $x_0,y_0,z_0$; target error $\epsilon_k$; stepsizes $\eta_k$; penalty constant $\gamma_k$
    \FOR{$k = 0, 1, \ldots, K-1$}
        \STATE estimate $y_k = \arg \min_{y\in\mathcal{P}} g(x^k,y)$ with $\epsilon_k$ error.
        \STATE estimate $z_k = \arg \min_{z\in\mathcal{P}} \mathcal{L}_{\gamma_k}(x^k,z)$ with $\epsilon_k$ error 
        \STATE estimate $\bar\nabla\mathcal{L}_\gamma^*(x_k)$ with $y^k\approx y^*$ and $z^k\approx z^*$ in \eqref{L_grad}
        \STATE update $x_{k+1}=\operatorname{Proj}_{\mathcal{X}}(x_k-\eta_k\bar\nabla\mathcal{L}_\gamma^*(x_k))$
    \ENDFOR
\STATE \textbf{outputs:} $(x_K,z_K)$
\end{algorithmic}
\end{algorithm}


Based on Proposition \ref{prop:closed_upper}, the upper-level gradient in \eqref{eq:upper-gradient} can be estimated via the Monte Carlo method. In other words, with samples $\{\tilde u_m\}_{m=1}^{M_0} \sim p_{\text{data}}$, it is estimated by empirical mean with detailed form deferred to Appendix \ref{MC-estimate}. 





The full procedure is summarized in Algorithm \ref{alg:A1}. 

\begin{algorithm}[tb]
\caption{Bilevel approach with pre-trained  model}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Pre-trained score network $s_\theta(\cdot, \cdot)$, differentiable reward $r_1(\cdot),r_2(\cdot)$, stepsizes $\eta_k$, penalty constant $\gamma_k$. 
\STATE sample $\{\tilde u_{m}\}_{m=1}^{M_0}$ from reverse SDE \eqref{eq: backward} using $s_\theta$
\FOR{$k = 0, 1, \ldots, K-1$}
    \STATE estimate $\bar\nabla\mathcal{L}_{\gamma_k}^*(\lambda_k)$ by \eqref{upper_grad_estimate_v1} 
    \STATE update $\lambda_{k+1} = \operatorname{Proj}_{\mathbb{R}_+}\left(\lambda_k -\eta_k \bar\nabla\mathcal{L}_{\gamma_k}^*(\lambda_k) \right)$
\ENDFOR
\STATE  sample $\{u_{K,m}^z\}_{m=1}^{M}$ from Algorithm \ref{alg:gradient_guided_diffusion} using $(s_\theta, \frac{r_1}{\gamma_K}+r_2, G_{\rm penalty}^k(\cdot,\cdot,\lambda_K))$ in \eqref{G_penalty} 
\STATE \textbf{Output:} $(\lambda_K, \{u_{K,m}^z\}_{m=1}^{M})$.
\end{algorithmic}
\label{alg:A1}
\end{algorithm}

\subsection{Strategy without pre-trained diffusion models}

We next design a bilevel algorithm to solve \eqref{opt-schedule} in the application of noise scheduling for training diffusion models. 


\noindent\textbf{Lower-level problem solver. } When $q$ is given, the samples from the forward process $u(q)$ are determined. Then optimizing the score matching objective $\mathrm{L}_{\text{SM}}(\cdot)$ on the score network gives the optimal lower-level solution, i.e. optimal weights of the score network $\theta\in\argmin_\theta \mathrm{L}_{\text{SM}}(\theta,  u(q))$. 

\noindent\textbf{Penalty problem solver with respect to $\theta$. } Gradient-based approaches on the penalty function $\mathcal{L}_\gamma (q,\theta)$ are effective in solving the penalty problem over $\theta$.  The gradient of $\mathcal{L}_\gamma (q,\theta)$ with respect to $\theta$ takes the form of 
\begin{align*}
 \nabla_\theta\mathcal{L}_\gamma (q,\theta) =\nabla_\theta\mathbb{E}_{\tilde u(q) \sim p_\theta}[\mathrm{L}_{\text{SQ}}(\tilde u(q))]+\gamma\nabla_\theta\mathrm{L}_{\text{SM}}(\theta,u(q)) 
\end{align*}
where the second term can be directly calculated by differentiating the score-matching loss over $\theta$, and the first term can be estimated by the mean of gradients over a batch of samples $\{\tilde u_{\theta,q}^m\}_{m=1}^M$ generated by the backward process \eqref{eq: backward}. 
Although it is possible to obtain the gradient of $\nabla_{\theta} \mathrm{L}_{\text{SQ}}(\tilde u_{\theta,q}^m)$ using PyTorch's auto-differentiation, it requires differentiating through the backward sampling trajectory. Since backward sampling involves 50–100 steps, even for efficient methods like the Denoising Diffusion Implicit Model (DDIM), auto-differentiation is memory-intensive. Instead, we estimate $\nabla_{\theta} \mathrm{L}_{\text{SQ}}(\tilde u_{\theta,q}^m)$ by zeroth-order (ZO) approximation \citep{nesterov2017random,shamir2017optimal} 
\begin{align}\label{two-point-ZO}
\widetilde\nabla_{\theta} \mathrm{L}_{\text{SQ}}(\tilde u_{\theta,q}^m)&=\frac{\xi}{2\nu} (\mathrm{L}_{\text{SQ}}(\tilde u_{\theta+\nu\xi,q}^m)-\mathrm{L}_{\text{SQ}}(\tilde u_{\theta-\nu\xi,q}^m))
\end{align}
where $\nu>0$ is the perturbation amount and $\xi\sim\mathcal{N}(0,I_d)$ is randomly drawn from standard Gaussian distribution. At each round, \eqref{two-point-ZO} executes two backward processes to get the query of $\mathrm{L}_{\text{SQ}}(\cdot)$ with two perturbation $\theta+\nu\xi$ and $\theta-\nu\xi$. 


\noindent\textbf{Gradient for the upper-level scheduler $q$. } According to \eqref{L_grad}, $\nabla\mathcal{L}_\gamma^*(q)$ takes the following form 
\begin{align*}
&\nabla\mathcal{L}_\gamma^*(q)=\nabla_q\mathbb{E}_{\tilde u (q)\sim p_{\theta_z^*}}[\mathrm{L}_{\text{SQ}}(\tilde u(q))]\\
&~~~~~~+\gamma(\nabla_q\mathrm{L}_{\text{SM}}(\theta_z^*,u(q))-\nabla_q\mathrm{L}_{\text{SM}}(\theta_y^*,u(q)))
\end{align*}
where $\theta_y^*\in\argmin_\theta g(\theta,q)$ and $\theta_z^*\in\argmin_\theta \mathcal{L}_\gamma(\theta,q)$ are given by the lower-level and penalty problem solver. For $\nabla_q\mathrm{L}_{\text{SM}}(\theta,u(q))$, according to the chain rule, we have
\begin{align}\label{SM_q}
\nabla_q\mathrm{L}_{\text{SM}}(\theta,u(q))= \frac{\partial u(q)}{\partial q} \nabla_u\mathrm{L}_{\text{SM}}(\theta,u(q)). 
\end{align}
Since the score matching loss for popular choices of diffusion models has explicit dependency on the generated noisy samples $u_t$ in the forward process and $u_t$ has one-step closed form with respect to the initial sample, $\nabla_q\mathrm{L}_{\text{SM}}(\theta,u(q))$ has closed form for popular choices of diffusion model; see details in Appendix \ref{sec:gradient_form}. For $\nabla_q\mathbb{E}_{\tilde u (q)\sim p_\theta}[\mathrm{L}_{\text{SQ}}(\tilde u(q))]$ 
whose explicit expression is not available, we can use similar ZO approaches in \eqref{two-point-ZO} with perturbation on noise scheduler $q$ to get the gradient estimator. 


\noindent\textbf{Parametrization for noise scheduler. } To further reduce the memory cost and ensure the nondecreasing of $q(t)$, we parameterize the noise scheduler by the commonly used cosine and sigmoid function \citep{nichol2021improved,chen2023importance} and optimize the parameters within these functions instead of directly optimizing $\{q(t)\}_{t=1}^T$. In both parametrizations, we optimize just $4$ scalar parameters, significantly reducing the dimensionality of the optimization problem from $T$ to $4$. Specifically, we use 
\begin{align}\label{cosine}
l(t)=\cos\left[\frac{(t(q_e-q_s)+q_s)/T+q_\epsilon}{1+q_\epsilon}\times\frac{\pi}{2}\right]^{2q_\tau}
\end{align}
where $q_s, q_e, q_\epsilon, q_\tau$ represent the start, end, offset error, and power effect, respectively. Sigmoid parameterization is defined similarly in \eqref{sigmoid} and can be found in Appendix \ref{sec:appendix_app2}. Since $q(t)$ should be nondecreasing, we assign $q(t)=1-l(t)/l(t-1)$.  After parameterization, ZO perturbation will be added on $q_s, q_e, q_\epsilon, q_\tau$ instead of directly on $q(t)$. 




The full procedure is summarized in Algorithm \ref{alg:A2} and can be found in Appendix \ref{sec:complete_alg}. 

\section{Theoretical Guarantee}

In this section, we quantify the theoretical benefits of the bilevel algorithms in both applications.  We make the following assumption, which is standard in bilevel optimization literature \citep{kwon2023fully,ji2021bilevel,chen2021closing,franceschi2018bilevel,hong2020two}.  

\begin{assumption}\label{as1}
The objective $g(x,\cdot)$ is $\mu_g$-strongly convex, $f(x,y)$ and $g(x,y)$ are jointly smooth over $(x,y)$ with constant $\ell_{f,1}$ and $\ell_{g,1}$ for all $x\in\mathcal{X}$ and $y\in\mathcal{P}$. Moreover, $f(x,\cdot)$  is $\ell_{f,0}$ Lipschitz continuous and $g(x,y)$ has $\ell_{g,2}$ Lipschitz Hessian jointly with respect to $(x,y)$. 
\end{assumption}
We will justify the validity of this assumption in two generative bilevel  applications in Section \ref{sec.apply} after we prove the hyperparameter improvement theorem below. 

\begin{theorem}\label{thm:descent}
Under Assumption \ref{as1}, given any intial hyperparameter $x_0$, letting the inner loop accuracy $\epsilon_k\leq\frac{B}{\gamma_k^2}$, then there exists stepsize $\eta_k\leq\frac{1}{L_F}$ and penalty constant $\gamma_k$ such that the next updates $x_{k+1}$ generated by Algorithm \ref{alg: GBLO} satisfy  
\begin{align*}
F(x_{k+1})-F(x_k)&\leq -\frac{\eta_k}{4}\|G_{\eta_k,\gamma_k}(x_k)\|^2+\frac{6B^2\eta_k}{\gamma_k^2}
\end{align*}
where $G_{\eta,\gamma}(x)=\frac{\operatorname{Proj}_{\mathcal{X}}(x-\eta\bar\nabla\mathcal{L}_{\gamma}^*(x))-x}{\eta}$ is the projected gradient and $L_F, B={\cal O}(1)$ are defined in Lemma \ref{lemma:error}. 
\end{theorem}

This theorem demonstrates that when $G_{\eta_k,\gamma_k}(x_k)\neq 0$, and setting penalty constant $\gamma_k\geq\frac{4\sqrt{3}B}{\|G_{\eta_k,\gamma_k}(x_k)\|}$, the bilevel algorithm will have strict descent over the hyper-function $F(x)$. Otherwise, since $\|\bar\nabla\mathcal{L}_{\gamma}^*(x)-\nabla F(x)\|={\cal O}(1/\gamma_k)$, $G_{\eta_k,\gamma_k}(x_k)=0$ is approximately the stationary points of $F(x)$. The proof can be found in Appendix \ref{sec: descent}. 

\noindent\textbf{Implications on generative bilevel applications. } For fine-tuning diffusion models, the KL divergence with respect to a probability distribution $p$ is strongly convex if $p$ is strongly log-concave \citep{vempala2019rapid}. This condition is usually met in diffusion models \citep{songdenoising,ho2020denoising}, as they generate Gaussian distribution with positive definite covariance matrices. Therefore, when reward function is concave \citep{guo2024gradient}, Assumption \ref{as1} holds. In this application, $\epsilon_k=0$ since the upper-level gradient estimator does not depend on the inner loop accuracy; see \eqref{eq:upper-gradient}. For noise scheduling problem, score matching loss is a composite quadratic function with respect to the noise network (see \eqref{eq:score_matching_re}), which can be viewed as a strongly convex function over the parameterized probability (functional) space \citep{petrulionyte2024functional}, so that Assumption \ref{as1} holds. In this application, the upper-level gradient estimation error includes an additional term dependent on the error of the ZO estimator, which can be made sufficiently small by appropriately choosing perturbation amount $\nu$. 


Therefore, Theorem \ref{thm:descent} applies to both settings, indicating that, regardless of initialization - whether from a random point or selected through cross-validation - the distribution generated using the hyperparameter $x_k$ of bilevel algorithms is guaranteed to perform better. For fine-tuning diffusion models, the bilevel algorithm generates images with higher CLIP scores, while the bilevel noise scheduling algorithm produces images with lower FID scores. 


\begin{table}
\small
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Baselines} & FID $\downarrow$ & CLIP $\uparrow$ & Time $\downarrow$ \\
\midrule
Grid search     & $125.77 \ {\scriptstyle \pm 1.3}$ & $31.72 \ {\scriptstyle \pm 2.1}$ & $3.34$ \\
Random search   & $123.70 \ {\scriptstyle \pm 4.3}$ & $35.70 \ {\scriptstyle \pm 1.1}$ & $3.37$ \\
Bayesian search & $140.35 \ {\scriptstyle \pm 2.4}$ & $33.29 \ {\scriptstyle \pm 2.9}$ & $12.7$ \\
Weighted sum & $116.52 \ {\scriptstyle \pm 3.9}$ & $36.50 \ {\scriptstyle \pm 1.1}$ & $4.31$ \\

\midrule
\textbf{Bilevel method} & $\textbf{102.82} \ {\scriptstyle \pm 3.5}$ & $\textbf{39.54} \ {\scriptstyle \pm 2.2}$ & $3.35$ \\
\bottomrule
\end{tabular}
\caption{Best FID and CLIP score given by different baselines and our method with penalty constant $\gamma = 10^3$ for fine-tuning diffusion model application. Running time is measured in hours. }
\label{table:application1}
\vspace{-0.1cm}
\end{table}


\section{Numerical Experiments}

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/first_exp/all_images/other_images/step_iteration2.png}
    \vspace{-0.2cm}
    \caption{Visualization of images generated at different steps: (a) Images generated with $\lambda = 0.1$ become progressively more abstract at each step, while (b) images generated with bilevel method ($\lambda = 55.5$) are more colorful and vivid than the pre-trained images and achieve a perfect balance of quality across steps.}
    \label{fig:step}
    \vspace{-0.3cm}
\end{figure}

In this section, we present the experimental results of the proposed bilevel-diffusion algorithms in two applications—reward fine-tuning and noise scheduling for diffusion models—and compare them with baseline hyperparameter optimization methods: grid search, random search, and Bayesian search \citep{snoek2012practical}. 

\subsection{Reward fine-tuning in diffusion models} 



For this experiment, we use the StableDiffusion V1.5 model as our pre-trained model and employ a ResNet-18 architecture (trained on the ImageNet dataset) as the synthetic (lower-level) reward model, following \citep{yuan2024reward}, to enhance colorfulness and vibrancy. To enable scalar reward outputs, we replace the final prediction layer of ResNet-18 with a randomly initialized linear layer. 

We evaluate bilevel reward fine-tuning Algorithm~\ref{alg:gradient_guided_diffusion} on the image generation task with complex prompts~\citep{wang2023context,wang2024discrete,clark2023directly}, comparing it to gradient guidance generation approach in \citep{guo2024gradient} combined with conventional hyperparameter search methods for tuning $\lambda$. Inspired by Figure \ref{fig:diff_lambda}, we use CLIP score as the upper-level loss to automatically tune $\lambda$ in a bilevel algorithm. To rule out the impact of the additive effect on the upper-level loss and lower-level reward, we also compare our approach with the weighted sum method, which naively combines the CLIP score and lower-level reward \(r_2(\cdot)\), with weight selected by grid search. A detailed description of the baselines is provided in the Appendix~\ref{sec:appendix_app2}. 

Table~\ref{table:application1} presents the average FID, CLIP score, and execution time for each method over prompts. The bilevel method outperformed traditional hyperparameter tuning methods, achieving superior FID and CLIP scores. Its comparable time complexity arises from requiring backpropagation through the CLIP score, unlike standard methods. For the weighted sum method, which also involves backpropagation through the CLIP score, the bilevel method is faster. Moreover, the bilevel method achieved an $11.76\%$ improvement in the FID score and an $8.32\%$ improvement in the CLIP score over the best-performing weighted sum method. 
 






Figure \ref{fig:step} shows the generated images over generation iteration in Algorithm \ref{alg:gradient_guided_diffusion} using $\lambda=0.1$ and the $\lambda$ optimized by the bilevel approach. The results demonstrate that the entropy strength selected by the bilevel approach achieves a better balance between image quality and realism. Figure \ref{fig:lambda} shows the optimization process of $\lambda$, revealing that the optimal value of $\lambda$ varies across different prompts. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/first_exp/lambda_scale.pdf}
    \caption{Change of $\lambda$ over iteration given by bilevel approach for different prompts.}
    \label{fig:lambda}
\end{figure}

More visualizations are provided in  Figures~\ref{fig:results_visualization_1st_horse}--\ref{fig:results_visualization_1st_man} and can be found in Appendix, which visually illustrate the impact of over-aggressive reward optimization, which tends to generate more abstract images (e.g., as observed in the results from grid search and Bayesian search methods). 


\begin{table}[htbp]
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{lcccccl}\toprule
\multirow{2}{*}{\textbf{Methods}} & \multicolumn{3}{c}{Cosine} & \multicolumn{3}{c}{Sigmoid} \\\cmidrule(lr){2-4}\cmidrule(lr){5-7}
& FID $\downarrow$ & IS $\uparrow$ & Time $\downarrow$ & FID $\downarrow$ & IS $\uparrow$ & Time $\downarrow$ \\\midrule
Grid search   
& $67.30$ & $1.76$ & $31.53$ & $65.31$ & $1.76$ & $39.12$\\
 Random search  & $68.97$ & $1.61$ & $29.62$  & $66.02$  & $1.69$ & $35.94$\\
 Bayesian search & $67.16$ & $1.69$ & $26.85$  & $65.17$  & $1.65$ & $29.13$\\
 DDIM (default)   & $105.27$ & $1.43$ & $1.59$ & $85.79$ & $1.54$ & $1.78$\\ 
\midrule
\textbf{Bilevel method} & $\textbf{65.41}$ & $\textbf{1.78}$ & $3.88$ & $\textbf{65.16}$ & $\textbf{1.79}$ & $3.94$\\
\bottomrule
\end{tabular}}
\vspace{-0.2cm}
\caption{Comparison of FID, IS, and running time (in hours) for different baselines and our method for the noise scheduling application with cosine and sigmoid parameterization. Default DDIM parameters are from \citep{nichol2021improved} for cosine and \citep{analyticsvidhya2024noise} for sigmoid parameterization. }
\label{table-noise}
% \vspace{-0.3cm}
\end{table}



\subsection{Noise scheduling in diffusion models}
We evaluated our bilevel noise scheduling method, detailed in Algorithm \ref{alg:A2}, paired with DDIM backward sampling for the image generation on the MNIST dataset. We trained a U-Net with $178$ layers and $10^6+$ parameters following \footnote{\footnotesize{https://github.com/bot66/MNISTDiffusion/tree/main}}. We considered both cosine and sigmoid parametrization and tuned $4$ parameters $q_s, q_e, q_\tau, q_\epsilon$ jointly. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.23\textwidth, height=0.18\textwidth]{figures/second_exp/cosine_parameter.pdf} 
    \includegraphics[width=0.24\textwidth, height=0.18\textwidth]{figures/second_exp/cosine_qt.pdf} 
    \vspace{-0.5cm}
    \caption{Varying hyperparameters start $q_s$, end $q_e$, power $q_\tau$ and offset $q_\epsilon$ in cosine parameterization learned by bilevel method along the training steps and corresponding noise scheduler $q(t)$ at iteration $k=0,100,300,469$.}
    \label{fig:varying_noise}  
    \vspace{-.5cm}
\end{figure} 

We compare our method to DDIM combined with baseline hyperparameter optimization methods. We chose greedy grid search over standard grid search as the baseline, as the latter is computationally intensive for searching across multiple hyperparameters. In greedy grid search, we sequentially optimize parameters based on sensitivity, fixing each optimized parameter before tuning the next. Additional experimental setup can be found in Appendix \ref{sec:appendix_app2}. 


Bilevel noise scheduling algorithm in Algorithm \ref{alg:A2} alternates between optimizing the weights $\theta$ in U-Net and finding the best noise scheduler $q(t)$ online, so that is computationally efficient and outperforms fixed noise schedulers. Figure \ref{fig:varying_noise} shows the learned hyperparameter $q_s, q_e, q_\tau, q_\epsilon$ by bilevel optimization versus iteration $k$ and the corresponding $q(t)$ at four timesteps. We observe the parameters are nontrivial: $q_\tau$ is the most influential factor, varying significantly throughout the training process; start $q_s$ and end $q_e$ update inversely and are utilized more extensively at the beginning of training; and $q_\epsilon$ increases progressively over the course of training. These parameters determine the noise scheduler $q(t)$, which introduces more noise at the beginning and the latter middle stages of training step $k$.



Table \ref{table-noise} presents the best FID, inception score (IS), and time complexity achieved by each method. 
The bilevel method achieves comparable performance with the hyperparameter optimization baselines in both FID and IS while being $6\times$ time faster. Generated images by each method are shown in Figure \ref{fig:results_visualization}. While the Bayesian approach achieves relatively better FID and IS, it tends to focus on generating simpler numbers, such as $1$ and $7$.  In contrast, the images produced by the bilevel method show excellent diversity across numbers $0-9$ while maintaining image fidelity. 



\vspace{-0.2cm}
\section{Related works}

\noindent\textbf{Fine-tuning diffusion models. } Fine-tuning diffusion models aims to adapt pre-trained models to boost the reward on downstream tasks. Methods in this domain include directly backpropagating the reward \citep{clarkdirectly}, RL-based fine-tuning \citep{fan2024reinforcement, black2023training}, direct latent optimization \citep{tang2024tuning, wallace2023end}, guidance-based approach \citep{guo2024gradient, chung2022diffusion} and optimal control \citep{uehara2024fine}. Although entropy regularization is often incorporated into the reward to prevent over-optimization, no existing work has explored designing an efficient bilevel method to tune its strength. 

\noindent\textbf{Noise scheduling in diffusion models. } Noise schedule is crucial to balance the computational efficiency with data fidelity during image generation. Early works, such as DDPM \citep{ho2020denoising}, employed simple linear schedules for noise variance, while \citet{nichol2021improved} introduced cosine schedules to enhance performance. Recent studies \citep{lin2024common,chen2023importance} have highlighted limitations in traditional noise schedules and proposed new parameterization to improve the image quality. However, none of them considered using bilevel optimization to automatically learn the noise schedule. 

\noindent\textbf{Bilevel hyperparameter optimization. } Bilevel optimization has been explored as an efficient hyperparameter optimization framework, including hypernetwork search \citep{mackayself,liu2018darts}, hyper-representation \citep{franceschi2018bilevel}, regularization learning \citep{shaban2019truncated} and data reweighting \citep{shaban2019truncated,franceschi2017forward}. Recently, it has been explored in federated learning \citep{tarzanagh2022fednest} and LLM fine-tuning \citep{shen2024seal,zakarias2024bissl}. None of the existing works have explored hyperparameter optimization in diffusion models, and the methods proposed so far are inapplicable due to the infinite-dimensional probability space and the high computational cost of sampling. 


\begin{figure}[tbp]
    % \vspace{-0.3cm}
    \centering
    \setlength{\tabcolsep}{1pt} % Reduce column spacing
    \resizebox{0.5\textwidth}{!}{ % Scale the table to fit the page width
        \begin{tabular}{ccc}
            \includegraphics[width=0.12\textwidth]{figures/visualization_mnist/second_exp/bilevel2.png} &
            \includegraphics[width=0.12\textwidth]{figures/visualization_mnist/second_exp/default.png} &
            \includegraphics[width=0.12\textwidth]{figures/visualization_mnist/second_exp/bayesian.png} \\
            \scriptsize{(a) Bilevel} & \scriptsize{(b) DDIM (default)} & \scriptsize{(c) Bayesian} \\
        \end{tabular}
    }
    \vspace{-0.5cm}
    \caption{Visualization of the final generated images by different methods using cosine parameterization.}
    \label{fig:results_visualization}
    \vspace{-.7cm}
\end{figure} 


\vspace{-0.2cm}
\section{Conclusions}
\vspace{-0.2cm}
In this paper, we analyze two types of generative bilevel hyperparameter optimization problems in diffusion models: fine-tuning a diffusion model (with a pre-trained model) and noise scheduling for training a diffusion model from scratch. For fine-tuning, we propose an inference-only bilevel approach to guide the diffusion model toward the target distribution and leverage the closed-form of KL divergence to update the entropy strength. For training from scratch, we optimize the parameters of the noise distribution to match the true noise and use zeroth-order optimization to determine the optimal noise scheduler for generating high-quality images in the backward process ``on the fly." Experiments demonstrate the effectiveness of the proposed method. 

 

\section*{Acknowledgment}
The work of Q. Xiao and T. Chen was supported by National Science Foundation (NSF) MoDL-SCALE project 2401297, NSF project 2412486, and the Cisco Research Award. 


\section*{Impact Statement} 

This paper aims to advance diffusion models with bilevel generative optimization, providing a novel approach to hyperparameter tuning for improved image generation. By addressing key challenges in fine-tuning diffusion model and noise scheduling, our work contributes to the broader development of more efficient and adaptive generative models. Potential societal impacts include applications in creative content generation, data augmentation, and machine learning-based simulations. While we acknowledge the possibility of unintended uses, we do not identify any specific societal risks that need to be highlighted in this context. 

\bibliographystyle{icml2025}
\bibliography{bilevel,diffusion,ZO}

\newpage
\appendix
\onecolumn

\begin{center}
{\large \bf Appendix for "A First-order Generative Bilevel Optimization Framework for Diffusion Models"}
\end{center}
%  \setlength{\parindent}{0pt}
% \vspace{-1cm}
 

% \doparttoc % Tell to minitoc to generate a toc for the parts
% \faketableofcontents % Run a fake tableofcontents command  for the partocs

% %cmg comment out, replaced by previous one
% % \addcontentsline{toc}{section}{} % Add the appendix text to the document TOC

% \part{} % Start the appendix part
% \parttoc % Insert the appendix TOC

% % \onecolumn
% \part{} % Start the appendix part
% \parttoc % Insert the appendix TOC
\allowdisplaybreaks 

\section{Additional related works}

\noindent\textbf{Bilevel optimization methods. } Bilevel optimization has a long history that dates back to \citep{bracken1973mathematical}. Recent efforts have focused on developing efficient gradient-based bilevel optimization methods with non-asymptotic convergence guarantees, inspired by works such as \citep{ghadimi2018approximation,ji2021bilevel,hong2020two,chen2021closing}. Since the gradient of the bilevel nested objective depends on the Hessian of the lower-level objective, existing literature proposed different Hessian inversion approximation methods including unrolling differentiation \citep{franceschi2017forward,franceschi2018bilevel,grazzi2020iteration}, implicit differentiation \citep{chen2021closing,ghadimi2018approximation,hong2020two,pedregosa2016hyperparameter,khanduri2021near}, conjugate gradients \citep{ji2021bilevel,yang2021provably} and its warm-started single-loop versions \citep{arbel2021amortized,li2022fully,liu2023averaged,xiao2023generalized}, and equilibrium backpropagation \citep{scellier2017equilibrium,scellier2021deep}; see \citep{zucchet2022beyond} for a comparison. 
Among these methods, equilibrium backpropagation stands out as a fully first-order approach, valued for its balance of efficiency, robustness, and simplicity. Building on this principle, recent works have extended its applicability from the strongly convex setting to convex, nonconvex and constrained settings by reformulating the bilevel optimization problem as a single-level penalty problem and solving it via first-order approaches \citep{shen2023penalty,liubome,kwon2023fully,kwon2023penalty,chen2024finding,lu2023first,jiang2024primal,yao2024overcoming}. 

\section{Background on bilevel optimization}\label{sec:diff-bilevel}
In this section, we review some background knowledge for first order bilevel optimization. 

The differentiability of the penalty problem relies on the differentiability of the value function $g^*(x)$, which is established through the extended Danskin theorem \citep[Proposition 4]{shen2023penalty}. Specifically, the gradient of value function takes  
\begin{align}\label{value_function_grad}
\nabla g^*(x)=\nabla_x g(x,y^*), ~~\forall y^*\in\mathcal{S}(x). 
\end{align}
This enables us to solve \eqref{penalty_form} by gradient-based approach. Similarly, by applying the extended Danskin theorem to the penalty function \citep{kwon2023penalty}, we know
\begin{align*}
\nabla\mathcal{L}_\gamma^*(x)=\nabla_x\mathcal{L}_\gamma (x,z^*), ~~~\text{ with } ~~~  \forall z^*\in\mathcal{S}_\gamma(x)
\end{align*}
which can be further rewritten according to \eqref{value_function_grad} as 
\begin{align}
\nabla\mathcal{L}_\gamma^*(x)=\nabla_x f(x,z^*)+\gamma(\nabla_x g(x,z^*)-\nabla_x g(x,y^*)).
\end{align}

Moreover, the following lemma shows that the penalty objective is a proxy of original bilevel hyper-function $F(x)$. 

\begin{lemma}[{\citep[Lemma 3.1]{kwon2023fully}}]\label{lemma:error}
Under Assumption \ref{as1}, let $\gamma\geq\frac{2\ell_{f,1}}{\mu_g}$, we have $F(x)$ is $L_F$- smooth and 
\begin{align*}
\|\nabla F(x)-\nabla\mathcal{L}_\gamma^*(x)\|&\leq \frac{B}{\gamma}
\end{align*} 
where $L_F=\left(1+\frac{3 l_{g, 1}}{\mu_g}\right)\left(l_{f, 1}+\frac{l_{g, 1}^2}{\mu_g}+\frac{2 l_{f, 0} l_{g, 1} l_{g, 2}}{\mu_g^2}\right)={\cal O}(1/\kappa^3)$ and $B=\frac{4 l_{f, 0} l_{g, 1}}{\mu_g^2}\left(l_{f, 1}+\frac{2 l_{f, 0} l_{g, 2}}{\mu_g}\right)={\cal O}(1/\kappa^3)$ and $\kappa=\frac{\ell_{f,1}}{\mu_g}$ is the condition number. 
\end{lemma}
This lemma indicates that $\nabla\mathcal{L}_\gamma^*(x)$ is an approximation of $\nabla F(x)$ with error controlled by enlarging penalty constant $\gamma$. 

\section{Background on diffusion models} 
In this section, we connect continuous to discrete diffusion model to enable the derivation of the closed-form gradient of the score matching function with respect to the noise scheduler in the discrete diffusion model implementation. 

Denoising diffusion probabilistic model (DDPM) \citep{ho2020denoising} and Denoising diffusion implicit model (DDIM) \citep{songdenoising} provide standard ways to discretize the continuous SDE in \eqref{eq: forward} and \eqref{eq: backward}. To be self-contained, we provide a derivation of connection between them. Let us recall the continuous forward process in \eqref{eq: forward} as 
\begin{equation}
    \mathrm{d} U_t=-\frac{1}{2} q(t) U_t ~\mathrm{d} t+\sqrt{q(t)} \mathrm{d} W_t \tag{13}
\end{equation}
which gives the following transition probabilities
\begin{align*}
    p(u_t | u_0) 
    = \mathcal{N} \left(
        u_0 e^{-\int_0^T \frac{q(s)}{2} \mathrm{d}s}, \, I \int_0^T q(t) e^{- \int_0^{T-t} q(s) \mathrm{d}s} \mathrm{d}t
    \right)=\mathcal{N} \left(
        u_0 e^{-\int_0^T \frac{q(s)}{2} \mathrm{d}s}, \, \left(1 - e^{-\int_0^T q(s) \mathrm{d}s}\right) I. 
    \right)
\end{align*}
See also \citep[Appendix B]{songscore} and \citep[Appendix A]{denker2024deft}. Therefore, by defining $\bar q(t) = e^{-\int_0^T q(s) \mathrm{d}s}$, we get the form in DDPM \citep{ho2020denoising}
\begin{align}\label{eq:forward_continuous_re}
p(u_t | u_0) = \mathcal{N}\bigl(\sqrt{\bar q(t)} u_{0}, (1-\bar q(t))\mathbf{I}_d\bigr).
\end{align}
Since the approximation $1-x \approx e^{-x}$ holds well when $ x$ is small, we have a discrete approximation of $\bar q(t)$ as 
\begin{align*}
\bar q(t)=e^{-\int_0^T q(s) d s} \approx \prod_{n=0}^{N-1}\left(1-q(t_n) \Delta t\right). 
\end{align*}
By choosing $\Delta t=1$, we get the expression of discrete DDPM in \citep{ho2020denoising} as follows. 

\noindent\textbf{Forward process.} Given a data point sampled from a data distribution $u_0\sim p_{\text{data}}$, the forward process in DDPM generates a sequence of samples \(u_1, u_2, \dots, u_T\) by gradually adding noise 
\begin{align}\label{eq:forward_discrete}
p(u_{1: T}|u_0)=\prod_{t=1}^T p(u_t | u_{t-1}), \quad p(u_t| u_{t-1})=\mathcal{N}(\sqrt{1-q_t} u_{t-1}, q_t \mathbf{I}_d)
\end{align}
where \(\{q_t\}_{t=1}^T\) corresponds to the noise scheduler in discrete DDPM. \eqref{eq:forward_discrete} can be further expressed as 
\begin{align}\label{eq:forward_discrete_re}
p(u_t | u_0) = \mathcal{N}\bigl(\sqrt{\bar q_t} u_{0}, (1-\bar q_t)\mathbf{I}_d\bigr)
\end{align}
where $\bar q_t=\prod_{s=1}^t (1-q_s)$ is the variance scheduler defined by the noise scheduler \(\{q_t\}_{t=1}^T\). 

\noindent\textbf{Backward process of DDPM.} The backward process aims to recover \(u_0\) from \(u_T\) by iteratively denoising
\begin{align}\label{eq:backward_discrete}
\tilde p_\theta(u_{0:T}) = \tilde p(u_T)\prod_{t=1}^T \tilde p_\theta(u_{t-1} | u_t),
\end{align}
where $\tilde p(u_T)=\mathcal{N}(0,\mathbf{I}_d)$ and each $p_\theta$ is modeled as a Gaussian distribution parameterized by $\theta$
\begin{align*}
&~~~~~~~~~~~~~~~\tilde p_\theta(u_{t-1} | u_t) = \mathcal{N}\bigl(\mu_\theta(u_t,t), \sigma_\theta^2(u_t,t)\mathbf{I}_d\bigr)
\end{align*}
with the mean and variance learned by optimizing the score-matching objective. 

\noindent\textbf{Score matching.} In the discrete DDPM, score-matching loss also takes a simpler form. 
To learn \(\mu_\theta\) and \(\sigma_\theta\), we first estimate the backward probability given the initial state using the Gaussian kernel estimation as follows
\begin{subequations}
    \begin{align*}
&~~~~~~~~~~~~~~~\tilde p_t(u_{t-1} | u_t, u_0) = \mathcal{N}\bigl(\mu_t(u_t, u_0), \sigma_t^2 \mathbf{I}_d\bigr)\\
&\text{where }~~~ 
\mu_t(u_t, u_0)=\frac{\sqrt{\bar q_{t-1}}q_t}{1-\bar q_t} u_0+\frac{\sqrt{1-q_t}\left(1-\bar q_{t-1}\right)}{1-\bar q_t} u_t\stackrel{\textbf{(a)}}{=}\frac{1}{\sqrt{1-q_t}}\left(u_t-\frac{q_t}{\sqrt{1-\bar q_t}} \delta_t\right)\numberthis\label{mu_t}\\
&\text { and } \quad \sigma_t^2=\frac{1-\bar q_{t-1}}{1-\bar q_t} q_t\numberthis\label{sigma_t}
\end{align*}
\end{subequations}
where $\textbf{(a)}$ is earned by reparameterizing \eqref{eq:forward_discrete_re} as $u_t(u_0,\delta_t)=\sqrt{\bar q_t}u_0+(1-\bar q_t)\delta_t$ for $\delta_t\sim\mathcal{N}(\mu_t,\sigma_t^2\mathbf{I}_d)$. As $\mu_t$ is proportional to $\delta_t$, we can fit a neural network to proxy $\mu_t$ by optimizing the score matching loss in \eqref{score-matching} in the following simplified form with explicit dependence on noise scheduler $q$ 
\begin{align}\label{eq:score_matching_re}
\mathrm{L}_{\text{SM}}(\theta,q) = \mathbb{E}_{u_0, \delta,t}\left[\left\|\delta-\delta_\theta\left(\sqrt{\bar q_t} u_0+\sqrt{1-\bar q_t} \delta, t\right)\right\|^2\right]
\end{align}
where $\delta_\theta$ is a neural network approximator (e.g. U-Net) intended to predict Gaussian noise $\delta$ from $u_t$. 

By optimizing the score matching objective $\mathrm{L}_{\text{SM}}(\theta,q)$ with respect to $\theta$, we obtain the proxy of $\delta_\theta$ and using $\delta_\theta$ instead of $\delta_t$ in \eqref{mu_t}, we can sample the backward process by 
\begin{align}\label{back_DDPM}
u_{t-1}=\frac{1}{\sqrt{q_t}}(u_t-\frac{1-q_t}{\sqrt{1-\bar q_t}}\delta_\theta)+\sigma_t v
\end{align}
with $v\sim\mathcal{N}(0,\mathbf{I}_d)$. The full training and backward sampling process in DDPM is summarized in Algorithm \ref{alg:training} and \ref{alg:sampling}. 

\vspace{-0.3cm}
\begin{minipage}{0.45\textwidth}
\begin{algorithm}[H]
\caption{Score network training}
\label{alg:training}
\begin{algorithmic}[1]
\REPEAT
    \STATE draw $\{u_0^m\}_{m=1}^M \sim p_{\text{data}}$
    \STATE $\{t_m\}_{m=1}^M \sim \text{Uniform}([T])$
    \STATE $\{\delta_m\}_{m=1}^M \sim \mathcal{N}(0, \mathbf{I}_d)$
    \STATE Take gradient descent step on 
    $\nabla_\theta \frac{1}{M}\sum_{m=1}^M\|\delta - \delta_\theta(\sqrt{\bar q_{t_m}}u_0^m + \sqrt{1 - \bar q_{t_m}}\delta_m, t_m)\|^2
    $
\UNTIL{converged}
\end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}{0.5\textwidth}
\begin{algorithm}[H]
\caption{Backward sampling}
\label{alg:sampling}
\begin{algorithmic}[1]
\STATE $\{\tilde u_T^m\}_{m=1}^M \sim \mathcal{N}(0, \mathbf{I}_d)$
\FOR{$t = T, \dots, 1$}
        \STATE $\{v^m\}_{m=1}^M \sim \mathcal{N}(0, \mathbf{I}_d)$ if $t > 1$, else $v^m = 0$
    \STATE $\tilde u_{t-1}^m = \frac{1}{\sqrt{1-q_t}}\left(\tilde u_t^m - \frac{q_t}{\sqrt{1-\bar q_t}}\delta_\theta(\tilde u_t^m, t)\right) + \sigma_t v^m$
\ENDFOR
\STATE \textbf{return} $\frac{1}{M}\sum_{m=1}^M u_0^M$
\end{algorithmic}
\end{algorithm}
\end{minipage}

DDIM \citep{songdenoising} uses the same forward process and score network training as DDPM, but employs a deterministic backward sampling strategy and eliminates redundant sampling steps to further accelerate the backward process as follows. 

\paragraph{Backward process of DDIM. } Letting $\{t_i\}$ be some selected time steps from $[0,T]$, \eqref{back_DDPM} is generalized by 
\begin{align}
u_{t_{i-1}} = \sqrt{\bar q_{t_{i-1}}} 
\left( \frac{u_{t_i} - \sqrt{1-\bar q_{t_i}} \, \delta_\theta^{(t_i)}(u_{t_i})}{\sqrt{\bar q_{t_i}}} \right)
+ \sqrt{1-\bar q_{t_{i-1}} - \sigma_{t_i}^2} \cdot \delta_\theta^{(t_i)}(u_{t_i})
+ \sigma_{t_i} v_{t_i},
\label{back_DDIM}
\end{align}
which recovers DDPM in \eqref{back_DDPM} when $\sigma_t=\sqrt{\left(1-\bar q_{t-1}\right)q_t /\left(1-\bar q_t\right)}$ in \eqref{sigma_t} and without skipping. i.e. $t_i=t$, and the resulting deterministic model when $\sigma_t=0$ is called DDIM. In DDIM, time steps $\{t_i\}$ are selected using either linear ($t_i=\lfloor c i\rfloor$ for some $c$) or a quadratic ($t_i=\lfloor c i^2\rfloor$ for some $c$) strategy. With these designs, backward sampling steps of DDIM can be reduced from $1000$ in DDPM to $50-10$ \citep{songdenoising}. 




\section{Theoretical analysis}\label{sec:theory_appendix}
In this section, we present the closed-form of $\nabla\mathcal{L}_\gamma^*(\lambda)$ for fine-tuning diffusion model application, the gradient of the score matching function with respect to noise scheduler in discrete diffusion models, and the theoretical guarantee of Algorithm \ref{alg: GBLO}. 


\subsection{Upper-level gradient: Proof for Proposition \ref{prop:closed_upper}}\label{sec:KL_closed}


\begin{proof}
According to \citep[Equation (3.7)]{tang2024fine}, we have the closed form of KL divergence of fine-tuning distribution and pre-trained distribution as follows.  
\begin{align}\label{prop:closed_KL}
&\mathrm{KL}\left(p^*(\lambda)\| p_{\text {data}}\right)=-\mathbb{E}_{u\sim p_{\text {data}}}\left[\frac{r_2(u)}{\lambda}\right]+\log \mathbb{E}_{u\sim p_{\text {data}}}\left[e^{r_2(u) / \lambda}\right]\nonumber\\
&\mathrm{KL}\left(p_\gamma^*(\lambda)\|p_{\text {data}}\right)=-\mathbb{E}_{u\sim p_{\text {data}}}\left[\frac{r_1(u)/\gamma+r_2(u)}{\lambda}\right]+\log \mathbb{E}_{u\sim p_{\text {data}}}\left[e^{\frac{r_1(u)/\gamma+r_2(u)}{\lambda}}\right]
\end{align}

Then the proof can be obtained by plugging the closed-form of KL divergence in \eqref{prop:closed_KL} into \eqref{Grad_lambda}. That is,
\begin{align*}
\nabla\mathcal{L}_\gamma^*(\lambda)&=\gamma(\mathrm{KL}(p_\gamma^*(\lambda)  \|p_{\text {data}})-\mathrm{KL}(p^*(\lambda) \| p_{\text {data}}))\\
&=-\mathbb{E}_{u\sim p_{\text {data}}}\left[\frac{r_1(u)/\gamma+r_2(u)}{\lambda}\right]+\log \mathbb{E}_{u\sim p_{\text {data}}}\left[e^{\frac{r_1(u)/\gamma+r_2(u)}{\lambda}}\right]+\mathbb{E}_{u\sim p_{\text {data}}}\left[\frac{r_2(u)}{\lambda}\right]-\log \mathbb{E}_{u\sim p_{\text {data}}}\left[e^{r_2(u) / \lambda}\right]\\
&=-\mathbb{E}_{u\sim p_{\text{data}}}\left[{\lambda}^{-1}{r_1(u)}\right]-\gamma\log \mathbb{E}_{u\sim p_{\text{data}}}\left[e^{\frac{r_2(u)}{\lambda}}\right]+\gamma\log \mathbb{E}_{u\sim p_{\text{data}}}\left[e^{\frac{r_1(u)/\gamma+r_2(u)}{\lambda}}\right]
\end{align*}
which completes the proof. 

\end{proof}


\subsection{Explicit lower-level noise scheduler's gradient in discrete diffusion models}\label{sec:gradient_form}
In this section, we derive the explicit gradient expression of the score matching function with respect to the noise scheduler. 

Since score matching loss in both DDPM and DDIM takes the form in \eqref{SM_q}, the noise scheduler's gradient in score matching objective can be earned by the chain rule 
\begin{align}\label{SM_q_prime}
\nabla_q\mathrm{L}_{\text{SM}}(\theta,u(q))\approx \frac{1}{M}\sum_{m=1}^M \frac{\partial u_{q}^m}{\partial q} \nabla_u\mathrm{L}_{\text{SM}}(\theta,u_q^m)
\end{align}
where $u_q^m=u_{t_m}^m$ with $t_m$ uniformly chosen from $t\in[T]=\{1,\cdots,T\}$ and subscript $q$ means this forward sample is generated using noise scheduler $q=[q_1,\cdots,q_T]$. In this way, using the reparameterization $u_t=\sqrt{\bar q_t}u_0+(1-\bar q_t)\delta$ and the relation of $\bar q_t=\prod_{s=1}^t (1-q_s)$, we have for any $t\leq t_m$, 
\begin{align}\label{closed_partial_1}
\frac{\partial u_{q}^m}{\partial q_{t}}=\frac{\partial u_{t_m}^m}{\partial q_{t}}=\frac{\partial \bar q_{t_m}}{\partial q_{t}}\frac{\partial u_{{t_m}}^m}{\partial \bar q_{t_m}}=-\frac{\bar q_{t_m}}{q_t}\left(\frac{u_0}{2\sqrt{\bar q_{t_m}}}-\delta\right)^\top. 
\end{align}
On the other hand, the gradient of the score-matching function with respect to sample $u$ takes the form of 
\begin{align}\label{closed_partial_2}
\nabla_u\mathrm{L}_{\text{SM}}(\theta,u_q^m)=\frac{\partial \delta_{\theta}(u_{t_m}^m)}{\partial u_{t_m}^m}\frac{\partial \mathrm{L}_{\text{SM}}(\theta,u_q^m)}{\partial \delta_{\theta}(u_{t_m}^m)}=2\frac{\partial \delta_{\theta}(u_{t_m}^m)}{\partial u_{t_m}^m}(\delta_{\theta}(u_{t_m}^m)-\delta_m)
\end{align}
where $u_{t_m}^m=\sqrt{\bar q_{t_m}}u_0^m+(1-\bar q_{t_m})\delta_m$ and the first term $\frac{\partial \delta_{\theta}(u_{t_m}^m)}{\partial u_{t_m}^m}$ is the derivative of the score network with respect to the input samples that is directly obtainable via auto-differentiation library in Pytorch. 
By plugging the above closed forms of partial derivative in \eqref{closed_partial_1} and \eqref{closed_partial_2} into \eqref{SM_q_prime}, we get for any $t\in[T]$, 
\begin{align}\label{SM_q_empirical}
\nabla_{q_t}\mathrm{L}_{\text{SM}}(\theta,u(q))&\approx \frac{1}{|\mathcal{M}_t|}\sum_{\mathcal{M}_t:=\{m| t_m\geq t\}} \frac{\partial u_{q}^m}{\partial q_t} \nabla_u\mathrm{L}_{\text{SM}}(\theta,u_q^m)\nonumber\\
&=\frac{2}{|\mathcal{M}_t|}\sum_{\mathcal{M}_\tau:=\{m| t_m\geq t\}} -\frac{\bar q_{t_m}}{q_t}\left(\frac{u_0}{2\sqrt{\bar q_{t_m}}}-\delta_m\right)^\top \frac{\partial \delta_{\theta}(u_{t_m}^m)}{\partial u_{t_m}^m}(\delta_{\theta}(u_{t_m}^m)-\delta_m).
\end{align}
In practice, we do not need to manually implement the closed form in \eqref{SM_q_empirical}, as PyTorch's auto-differentiation handles it automatically. The derivation in this section highlights the low computational cost of auto-differentiation, as only $\frac{\partial \delta_{\theta}(u_{t_m}^m)}{\partial u_{t_m}^m}$ depends on the U-Net structure, and this differential is commonly used in gradient guidance diffusion models \citep{guo2024gradient,bansal2023universal}. 



\subsection{Descent theorem: Proof of Theorem \ref{thm:descent}} \label{sec: descent}
\begin{proof}\allowdisplaybreaks
By Taylor expansion and the $L_F$ smoothness of $F(x)$, we have 
\begin{align*}
F(x_{k+1})&\leq F(x_k)+\langle\nabla F(x_k), x_{k+1}-x_k\rangle+\frac{L_F}{2}\|x_{k+1}-x_k\|^2\\
&\leq F(x_k)+\langle\nabla F(x_k), \operatorname{Proj}_{\mathcal{X}}(x_k-\eta_k\bar\nabla\mathcal{L}_{\gamma_k}^*(x_k))-x_k\rangle+\frac{L_F}{2}\|\operatorname{Proj}_{\mathcal{X}}(x_k-\eta_k\bar\nabla\mathcal{L}_{\gamma_k}^*(x_k))-x_k\|^2\\
&= F(x_k)+\langle\bar\nabla\mathcal{L}_{\gamma_k}(x_k), \operatorname{Proj}_{\mathcal{X}}(x_k-\eta_k\bar\nabla\mathcal{L}_{\gamma_k}^*(x_k))-x_k\rangle+\frac{L_F}{2}\|\operatorname{Proj}_{\mathcal{X}}(x_k-\eta_k\bar\nabla\mathcal{L}_{\gamma_k}^*(x_k))-x_k\|^2\\
&~~~~~+\langle\nabla F(x_k)-\bar\nabla\mathcal{L}_{\gamma_k}^*(x_k), \operatorname{Proj}_{\mathcal{X}}(x_k-\eta_k\bar\nabla\mathcal{L}_{\gamma_k}^*(x_k))-x_k\rangle\\
&\leq F(x_k)+\langle\bar\nabla\mathcal{L}_{\gamma_k}^*(x_k), \operatorname{Proj}_{\mathcal{X}}(x_k-\eta_k\bar\nabla\mathcal{L}_{\gamma_k}^*(x_k))-x_k\rangle+\frac{L_F}{2}\|\operatorname{Proj}_{\mathcal{X}}(x_k-\eta_k\bar\nabla\mathcal{L}_{\gamma_k}^*(x_k))-x_k\|^2\\
&~~~~~+\frac{1}{2\alpha}\|\nabla F(x_k)-\bar\nabla\mathcal{L}_{\gamma_k}^*(x_k)\|^2+\frac{\alpha}{2}\|\operatorname{Proj}_{\mathcal{X}}(x_k-\eta_k\bar\nabla\mathcal{L}_{\gamma_k}^*(x_k))-x_k\|^2\\
&\stackrel{(a)}{\leq} F(x_k)-\frac{1}{4\eta_k}\|\operatorname{Proj}_{\mathcal{X}}(x_k-\eta_k\bar\nabla\mathcal{L}_{\gamma_k}^*(x_k))-x_k\|^2+\eta_k\|\nabla F(x_k)-\bar\nabla\mathcal{L}_{\gamma_k}^*(x_k)\|^2\\
&\stackrel{(b)}{\leq} F(x_k)-\frac{1}{4\eta_k}\|\operatorname{Proj}_{\mathcal{X}}(x_k-\eta_k\bar\nabla\mathcal{L}_{\gamma_k}^*(x_k))-x_k\|^2+\frac{2B^2\eta_k}{\gamma_k^2}+4\eta_k\gamma_k^2\epsilon_k^2
\end{align*}
where $(a)$ comes from the descent lemma of projected gradient (e.g. \citep[Theorem 2.2.13]{nesterov2018lectures}) and choosing $\alpha=\frac{1}{2\eta_k}$ and $(b)$ is because 
\begin{align*}
\|\nabla F(x_k)-\bar\nabla\mathcal{L}_{\gamma_k}^*(x_k)\|^2&\leq 2\|\nabla F(x_k)-\nabla\mathcal{L}_{\gamma_k}^*(x_k)\|^2+2\|\bar\nabla\mathcal{L}_{\gamma_k}^*(x_k)-\nabla\mathcal{L}_{\gamma_k}^*(x_k)\|^2\leq \frac{2B^2}{\gamma_k^2}+4\gamma_k^2\epsilon_k^2
\end{align*}
where the last inequality comes from Lemma \ref{lemma:error} and the estimation error $\epsilon_k$ of penalty and lower-level problem. By defining the projected gradient as $G_{\eta,\gamma}(x)=\frac{\operatorname{Proj}_{\mathcal{X}}(x-\eta\bar\nabla\mathcal{L}_{\gamma}^*(x))-x}{\eta}$ and letting $\epsilon_k\leq\frac{B}{\gamma_k^2}$, we get the conclusion. 
\end{proof}


\section{Complete Algorithms}\label{sec:complete_alg}
In this section, we present the complete algorithms with additional details for gradient guided diffusion model for (single-level) generative optimization \citep{guo2024gradient}, and the bilevel diffusion algorithm for fine-tuning and noise scheduling problem proposed in this work. 

\subsection{Guided Diffusion algorithm for Generative Optimization}\allowdisplaybreaks

To generate samples that optimize a given reward function $r$, we can iteratively apply the backward SDE in \eqref{eq: backward} with the pre-trained score network $s_\theta$ and the guidance defined as 
\begin{align}\label{g_loss}
\mathrm{G}(\tilde u_t, t; r)=-\rho(t) \nabla_{\tilde u_t}\left[v-\frac{g^{\top}(\left(\tilde u_t+h(t) s_\theta(\tilde u_t, t\right)))}{\sqrt{\bar q(t)}}\right]^2
\end{align}
where $g$ is a gradient vector associated with the reward function $r(\cdot)$ evaluated at the current sample $\tilde u_t$, $v$ is a given target reward value that increase along the optimization, $\bar q(t)=\exp (-\int_0^t q(s) d s)$, $h(t)=1-\bar q(t)$ are the mean and variance of $t$-th sample and $\rho(t)$ is the tunning parameter. 

We can iteratively update the gradient guidance to steer the sample generation process maximize the reward function. Specifically, at each iteration $n$, the backward SDE \eqref{eq: backward_guide} is stimulated using the current gradient guidance from \eqref{g_loss}, evaluated at the current samples, to generate new samples. Subsequently, the gradient guidance term is updated at the newly generated samples. After $N$ steps of guidance updates, we are able to generate samples approximately that follow the target distribution with ${\cal O}(\log(1/N))$, effectively minimizing $r(\cdot)$ while incorporating regularization to align with the pre-trained model \citep{guo2024gradient}. The complete algorithm for guided diffusion model for generative optimization is outlined in Algorithm \ref{alg:gradient_guided_diffusion}. 

\begin{algorithm}[tb]
\caption{Guided Diffusion for Generative Optimization}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Pre-trained score network $s_\theta(\cdot, \cdot)$, differentiable reward $r(\cdot)$, guidance $G$. 
\STATE \textbf{Parameter:} Strength parameters $\rho(t)$, $\{v_n\}_{n=0}^{N-1}$, number of iterations $N$, batch sizes $\{B_n\}$.
\STATE \textbf{Initialization:} $G_0 = \text{NULL}$.
\FOR{iteration $n = 0, \dots, N-1$}
    \STATE \textbf{Generate:} Sample $\tilde{u}_{n,i}$ for $i \in [B_n]$ by backward SDE in \eqref{eq: backward_guide} with $(s_\theta, G_n)$ until time $T$
    \STATE \textbf{Compute Guidance:}
    \begin{itemize}
        \item[(i)] Sample mean $\bar{u}_n := \frac{1}{B_n} \sum_{i=1}^{B_n} \tilde{u}_{n,i}$.
        \item[(ii)] Query gradient $g_n = \nabla r(\bar{u}_n)$.
        \item[(iii)] Update gradient guidance $G_{n+1}(\cdot, \cdot) = \mathrm{G}(\cdot, \cdot ; r)$ via \eqref{g_loss}, using $s_\theta$, gradient vector $g_n$, and reward target $v_n$ and $\beta(t)$.
    \end{itemize}
\ENDFOR
\STATE\textbf{Generate:} Sample $\tilde u_{i}$ for $i \in [B_N]$ by backward SDE in \eqref{eq: backward_guide} with $(s_\theta, G_N)$ until time $T$
\STATE \textbf{Output:} $\{\tilde u_{i}\}_{i=1}^{B_N}$.
\end{algorithmic}
\label{alg:gradient_guided_diffusion}
\end{algorithm}

\subsection{Bilevel fine-tuning algorithm}\label{MC-estimate}
Bilevel fine-tuning diffusion algorithm is summarized in Algorithm \ref{alg:A1} with the following upper-level gradient estimation. 
\paragraph{Monte Carlo estimation of upper-level gradient. } The upper-level gradient can be estimated from the following way  
\begin{align}
\bar\nabla\mathcal{L}_\gamma^*(\lambda)&=-\frac{1}{\lambda M_0}\sum_{m=1}^{M_0}r_1(\tilde u_m)-\gamma\log \frac{1}{M_0}\sum_{m=1}^{M_0}\left[e^{\frac{r_2(\tilde u_m)}{\lambda}}\right]+\gamma\log \frac{1}{M_0}\sum_{m=1}^{M_0}\left[e^{\frac{r_1(\tilde u_m)/\gamma+r_2(\tilde u_m)}{\lambda}}\right]\label{upper_grad_estimate_v1}.
\end{align}
where $\{\tilde u_m\}_{m=1}^{M_0}$ are samples from pre-trained distribution. 

\subsection{Bilevel noise scheduling algorithm}\label{sec:bilevel_noise_appendix}

With ZO estimation and the parametrization, the complete algorithm for bilevel noise scheduling problem is summarized in Algorithm \ref{alg:A2}. Besides, the upper-level schedule quality loss is differentiable according to \citep{mathiasen2020backpropagating}. 

\noindent\textbf{Differentiable $\mathrm{L}_{\text{SQ}}(\cdot)$ loss. } FID score is a commonly used metric in computer vision to measure the distance of the generated distribution and the true distribution. Given $\{u_m\}_{m=1}^M\sim p_{\theta}$ generated from diffusion model algorithm and $\{\tilde u_m\}_{m=1}^{M_0}\sim p_{\text{data}}$, we first encode all samples $u_m,\tilde u_m$ by the pre-trained Inception network \citep{Seitzer2020FID} and then FID score is computed by the Wasserstein distance between the two multivariate normal distributions. Therefore, when the Inception network used for encoding is differentiable with respect to its input, as the one proposed by \citet{Seitzer2020FID} does,  $\mathrm{L}_{\text{SQ}}(\cdot)$ is differentiable with respect to the sample and then by the chain rule, it is differentiable with respect to $q$ and $\theta$. 



\begin{algorithm}[tb]
\caption{Bilevel Approach without Pre-trained Diffusion Model}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Differentiable loss functions $\mathrm{L}_{\text{SQ}}(\cdot)$ and $\mathrm{L}_{\text{SM}}(\cdot)$, initial samples $\{\tilde{u}_m\}_{m=1}^{M_0}$, iteration number $K,S_z,S_y$, initial noise scheduler parametrization parameter $q_{\text{param}}=\{q_s,q_e,q_\tau,q_\epsilon\}$ (cosine or sigmoid), feasible set for $q_{\text{param}}\in\mathcal{Q}$, stepsizes $\beta,\eta_k$. 
\FOR{$k = 0, 1, \ldots, K-1$}
    \STATE sample $\{u_{k,m}\}_{m=1}^{M}$ from the forward process \eqref{eq: forward} with noise scheduler $q_k$. \qquad\qquad\qquad\qquad$\triangleright$~{\blue{$q_{0,\text{param}}=q_{\text{param}}$}}
    \FOR{$s = 0, 1, \ldots, S_y^k-1$}
        \STATE update $\theta_{k,s+1}^y = \theta_{k,s}^y - \frac{\beta}{M} \sum_{m=1}^M \nabla_\theta \mathrm{L}_{\text{SM}}(\theta_{k,s}^y, u_{k,m})$.     \qquad\qquad\qquad\qquad\qquad\qquad$\triangleright$~{\blue{$\theta_{k,0}^y = \theta_k^y, \theta_{k+1}^y = \theta_{k,S_y}^y$}}
    \ENDFOR
    \FOR{$s = 0, 1, \ldots, S_z-1$}
        \STATE sample $\{\tilde{u}_{k,m}^{s,+},\tilde{u}_{k,m}^{s,-}\}_{m=1}^{M}$ from \eqref{eq: backward} with $q_k$ and $\theta_{k,s}^z+\nu\theta_{\text{perturb}}$ and $\theta_{k,s}^z-\nu\theta_{\text{perturb}}$. \qquad\qquad\qquad$\triangleright$~~{\blue{$\theta_{k,0}^z = \theta_k^z$}} 
        \STATE estimate $\{\nabla_\theta \mathrm{L}_{\text{SQ}}(\tilde u_{k,m}^s)\}_{m=1}^M$ by ZO in  \eqref{two-point-ZO}
        \STATE update $\theta_{k,s+1}^z = \theta_{k,s}^z - \frac{\beta}{M} \sum_{m=1}^M \left(\nabla_\theta \mathrm{L}_{\text{SQ}}(\tilde u_{k,m}^s)+\gamma\nabla_\theta \mathrm{L}_{\text{SM}}(\theta_{k,s}^z, u_{k,m})\right)$\qquad\qquad\qquad\qquad$\triangleright$~~{\blue{$\theta_{k+1}^z = \theta_{k,S_z}^z$}} 
    \ENDFOR
    \STATE calculate parameterization perturbation $q_{k,\text{param}}^+=q_{k,\text{param}}+\nu q_{\text{perturb}}, q_{k,\text{param}}^-=q_{k,\text{param}}-\nu q_{\text{perturb}}$ \qquad$\triangleright$~~{\blue{$\theta_{k,0}^z = \theta_k^z$}} 
    \STATE calculate noise scheduler $q_{k}^+, q_{k}^-$  from $q_{k,\text{param}}^+, q_{k,\text{param}}^-$ by cosine or sigmoid parameterization 
    \STATE sample $\{\tilde{u}_{k+1,m}^{+},\tilde{u}_{k+1,m}^{-}\}_{m=1}^{M}$ from backward process \eqref{eq: backward} with $q_{k}^+, q_{k}^-$ and $\theta_{k+1}^z$. 
    \STATE estimate $\nabla_{q_{\text{param}}} \mathrm{L}_{\text{SQ}}(\tilde u_{k+1,m})=\frac{q_{k,\text{perturb}}}{2\nu}(\mathrm{L}_{\text{SQ}}(\tilde u_{k+1,m}^+)-\mathrm{L}_{\text{SQ}}(\tilde u_{k+1,m}^-))$ by ZO 
    \STATE calculate $\{\nabla_{q_{\text{param}}} \mathrm{L}_{\text{SM}}(\theta_{k+1}^z, u_{k,m}),\nabla_{q_{\text{param}}} \mathrm{L}_{\text{SM}}(\theta_{k+1}^y, u_{k,m})\}_{m=1}^M$ by auto-differentiation     
    \STATE update $q_{k+1,\text{param}} = q_{k,\text{param}} - \frac{\eta_k}{M} \sum_{m=1}^M \left(\nabla_{q_{\text{param}}} \mathrm{L}_{\text{SQ}}(\tilde u_{k+1,m})+\gamma(\nabla_{q_{\text{param}}} \mathrm{L}_{\text{SM}}(\theta_{k+1}^z, u_{k,m})-\nabla_{q_{\text{param}}} \mathrm{L}_{\text{SM}}(\theta_{k+1}^y, u_{k,m}))\right)$
    \STATE update $q_{k+1,\text{param}} = \operatorname{Proj}_{\mathcal{Q}}(q_{k+1,\text{param}})$
\ENDFOR
\STATE calculate noise scheduler $q_{K}$ from $q_{K,\text{param}}$ by cosine or sigmoid parameterization 
\STATE  sample $\{\tilde{u}_{K,m}\}_{m=1}^{M}$ from the backward process \eqref{eq: forward} with $q_K$ and $\theta_{K}^z$.
\STATE \textbf{Output:} $(q_K, \{u_{K,m}^z\}_{m=1}^{M})$.
\end{algorithmic}
\label{alg:A2}
\end{algorithm}

\section{Experimental details}

In this section, we introduce the details of experimental setup for two applications. All experiments were conducted on two servers: one with four NVIDIA A6000 GPUs, and 256 GB of RAM; one with an Intel i9-9960X CPU and two NVIDIA A5000 GPUs. 

\subsection{Application 1: fine-tuning diffusion model with bilevel entropy regularization learning}

For the hyperparameter settings, we set the initial value of noise scheduler \(q(t)\) to 1 and tune the entropy strength \(\lambda\) using different methods. We use a batch size of 3 for the fine-tuning step and set optimization step 7 and repeat the optimization for 4 times. The prompts we used for generating these figures are mentioned in the figure~\ref{fig:results_visualization_1st_horse} --~\ref{fig:results_visualization_1st_man}. We compare our results against various baseline methods including grid search, random search, bayesian method, and weighted sum. 

\textbf{Grid Search.} For the grid search method, we selected the following \(\lambda\) values and conducted simulations for each value:
\begin{align*}
\lambda \in \{0.01, 0.1, 1.0, 10.0, 100\}
\end{align*}


\textbf{Random search.} We fine-tuned the diffusion model using \(\lambda\) values generated by a random value generator. Specifically, we sampled 5 random values logarithm uniformly from the range \([0.01, 100]\). The \(\lambda\) values generated are as follows:
\begin{align*}
\lambda \in \{62.3, 74.0, 74.18, 79.52, 94.25\}
\end{align*}


\textbf{Bayesian search.} We utilized Bayesian optimization with the objective of maximizing the reward function and the CLIP score. Specifically, the search space for the hyperparameter \(\lambda_{\text{scale}}\) was defined as a continuous range \([0.01, 100]\), sampled on a logarithmic scale using a log-uniform distribution. The optimization process was conducted using the \texttt{gp\_minimize} function from the \texttt{scikit-optimize} library, which employs Gaussian process-based Bayesian optimization. To balance computational efficiency and optimization quality, the number of function evaluations was limited to \(n_{\text{calls}} = 15\). Additionally, a fixed random seed (\texttt{random\_state = 42}) was set to ensure the reproducibility of results. 

Since Bayesian optimization minimizes the objective function by default, we reformulated the problem by negating the combined reward and CLIP score, thereby transforming the maximization task into a minimization problem. This reformulation allowed us to identify the optimal \(\lambda_{\text{scale}}\) value that best balances reward maximization and adherence to the original data distribution. The acquisition function used was the expected improvement (EI), defined as:
\[
-EI(\lambda) = -\mathbb{E} [f(\lambda) - f(\lambda_t^+)]
\]
where \(f(\lambda_t^+)\) represents the best observed value at iteration \(t\). 


\textbf{Weighted sum.} For the weighted sum method, we jointly optimized the reward function and the CLIP score during the fine-tuning of the diffusion model. The optimization was performed by taking the weighted sum of the reward value and the CLIP score, with the weight for the CLIP score set to \(0.5\). The \(\lambda\) values used for the weighted sum method are selected by grid search with the search grid: $\lambda \in \{0.01, 0.1, 1.0, 10.0, 100\}$. 
 

\subsection{Application 2: bilevel noise scheduling learning}\label{sec:appendix_app2}
This cosine parametrization in \eqref{cosine} covers both the cosine noise scheduler in \citep{nichol2021improved} when $q_s=0, q_e= q_\tau=1$ and \citep{chen2023importance} when $q_\epsilon=0$. Sigmoid parameterization is defined similarly by
\begin{align}\label{sigmoid}
l(t)=\operatorname{sigmoid}\left[\frac{T-t(q_e-q_s)-q_s}{\tau T}+q_\epsilon\right]
\end{align}
which covers \citep{chen2023importance} when $q_\epsilon=0$. Since $q(t)$ should be nondecreasing, we assign $q(t)=1-l(t)/l(t-1)$.  With the use of parameterization, ZO perturbation will be added on $q_s, q_e, q_\epsilon, q_\tau$ instead of directly on $q(t)$. 

For hyperparameter optimization for noise scheduler, we compare our method against greedy grid search, random search, Bayesian search and the default DDIM. Default parameter choices of DDIM with cosine noise scheduler in \citep{nichol2021improved} are $q_s=0,q_e=1,q_\tau=1,q_\epsilon=0.008$. Default parameter choices of DDIM with sigmoid noise scheduler are $q_s=-3,q_e=3,q_\tau=0.1,q_\epsilon=-0.5$ according to \citep{analyticsvidhya2024noise}. 

\textbf{(Greedy) grid search. } According to the sensitivity analysis shown in Figure \ref{fig:varying_noise}, the most sensitive parameter is $q_\tau$, while the last three almost equally important. Therefore, in greedy grid search, we tuned the parameters in the order $q_\tau,q_\epsilon,q_s,q_e$. We adopt the following search grid for cosine parametrization and best parameter given by greedy grid search is highlighted: 
\begin{align*}
&q_s\in\{0,\textbf{0.1},0.2,0.3,0.4\}\\
&q_e\in\{\textbf{1},0.9,0.8\}\\
&q_\tau\in\{1,2,\textbf{3},4\}\\
&q_\epsilon\in\{0.005,0.008,0.01,\textbf{0.02},0.03,0.04\}
\end{align*}
For sigmoid parametrization, we use the following search grid and the best parameter is highlighted in black: 
\begin{align*}
&q_s\in\{-6,-5,\textbf{-4},-3,-2,-1,0\}\\
&q_e\in\{2, \textbf{3},4\}\\
&q_\tau\in\{0.1,0.2,\textbf{0.3},0.4,0.5,1,10\}\\
&q_\epsilon\in\{-2,-1,\textbf{-0.5},0,1\}
\end{align*}

\textbf{Random search. } We sample $16$ random combinations of ${q_s,q_e,q_\tau,q_\epsilon}$ from $q_s\in[0,0.4], q_e\in[0.8,1], q_\tau\in [1,4], q_\epsilon\in[0.005,0.04]$ for cosine parameterization and $q_s\in[-6,0], q_e\in[2,4], q_\tau\in [0.1,1], q_\epsilon\in[-2, 1]$ for sigmoid parameterization. We report the best-performing results given by the random combination. 

\textbf{Bayesian search. } We use the same range as the random search for Bayesian search and employ the same implementation to the first application. 

\textbf{Bilevel algorithm. } We employ bilevel algorithm in Algorithm \ref{alg:A2} and set the initialization of the noise scheduler parameter ${q_s,q_e,q_\tau,q_\epsilon}$ as the default values in DDIM  \citep{nichol2021improved,analyticsvidhya2024noise}. We use a batch size of $128$ and choose the number of inner loop $S_z$ for $\theta^z$ updates as $1$. Empirically, we found that, at the beginning of the training process (i.e. when $k=0$), the number of inner loop $S_y^0$ for updating $\theta^y$ should be larger to obtain a relatively reasonable U-Net, but later on, we do not need large inner loop, i.e. we set $S_y^k=10$ for $k\geq 1$. We formalize this stage as initial epoch, where we traverse every batch and set $S_y^0=20$. We choose the ZO perturbation amount as $\nu=0.01$. Moreover, Algorithm \ref{alg:A2} leverages the warm-start strategy. 

\noindent\textbf{Warm-start strategy. } To further accelerate convergence, we avoid fully optimizing the penalty and lower-level problem with respect to $\theta$ for every $q$. Instead, we employ a warm-start strategy, initializing $\theta$ using its value from the previous epoch \citep{arbel2021amortized,vicol2022implicit,sambharya2024learning}. Empirically, this approach effectively reduces the inner loop for optimizing $\theta$ in the lower-level and penalty problems to $10$ and $1$, respectively. Moreover, only $3-4$ outer epochs are needed for optimizing $q$. Compared to the $100$ epochs required for single-level diffusion model training, this significantly enhances the computational efficiency of our method, as shown in Table \ref{table-noise}. With only $2.5\times$ the training time of a single-level diffusion model, the bilevel method achieves a $30\%$ improvement over the default model while using just $15\%$ of the time for Bayesian search. 






\begin{figure}[htbp]
\vspace{-0.1cm}
    \centering
    \begin{tabular}{@{}c@{}c@{}c@{}c@{}c@{}c@{}}
        \includegraphics[width=0.15\textwidth]{figures/first_exp/all_images/lambda_scale/elephant_0_01.png} &
        \includegraphics[width=0.15\textwidth]{figures/first_exp/all_images/lambda_scale/elephant_0_1.png} &
        \includegraphics[width=0.15\textwidth]{figures/first_exp/all_images/lambda_scale/elephant_1.png} &
        \includegraphics[width=0.15\textwidth]{figures/first_exp/all_images/lambda_scale/elephant10.png} &
        \includegraphics[width=0.15\textwidth]{figures/first_exp/all_images/lambda_scale/elephant44.png} &
        \includegraphics[width=0.15\textwidth]{figures/first_exp/all_images/lambda_scale/elephant100.png} \\
        (a) $\lambda=0.01$ & (b) $\lambda=0.1$ & (c) $\lambda=1.0$ &
        (d) $\lambda=10.0$ & (e) $\lambda=44.3$ & (f) $\lambda=100.0$
    \end{tabular}
    \vspace{-0.2cm}
    \caption{Balancing the realism and aesthetic in the image generation by controlling the entropy regularization strength parameter $\lambda$. Prompt: "An African elephant on a foggy morning, with hot air balloons landing in the background."}
    \label{fig:image_change_with_lambda}
\end{figure}



\begin{figure}[htbp]
    \centering
    \begin{tabular}{ccc}
        \includegraphics[width=0.30\textwidth]{figures/first_exp/grid_step_7.png} &
        \includegraphics[width=0.30\textwidth]{figures/first_exp/bayesian_step_7.png} &
        \includegraphics[width=0.30\textwidth]{figures/first_exp/random_step_7.png} \\
        \makecell{(a) Grid Search \\ (FID= 127.04, CLIP= 31.97)} & 
        \makecell{(b) Bayesian Search \\ (FID= 140.35, CLIP= 32.68)} & 
        \makecell{(c) Random Search \\ (FID= 117.46, CLIP= 34.67)} \\[5pt]
    \end{tabular}
    
    \vspace{5pt}
    
    \begin{tabular}{ccc}
        \includegraphics[width=0.30\textwidth]{figures/first_exp/weight_step_7.png} &
        \includegraphics[width=0.30\textwidth]{figures/first_exp/bilevelstep_7.png} &
        \includegraphics[width=0.30\textwidth]{figures/first_exp/horse_lambda_01step_7.png} \\
        \makecell{(d) Weighted Sum \\ (FID= 108.95, CLIP= 35.87)} & 
        \makecell{(e) \textbf{Bilevel} \\ \textbf{(FID= 104.35, CLIP= 36.54)}} & 
        \makecell{(f) $\lambda=0.01$ \\ (FID= 403.66, CLIP= 17.58)}
    \end{tabular}

    \caption{Visualization of the final generated images (step-7) by different methods. Prompt: "A realistic photo of a horse standing on lush green grass in a countryside meadow on a sunny day, with clear blue sky in the background." \textbf{(a) Grid Search:} The generated images do not fully adhere to the prompt, as the clear blue sky is often missing. Some images appear more abstract. \textbf{(b) Bayesian Search:} Most images lack a blue sky in the background, and some horses are deformed. \textbf{(c) Random Search:} In certain images, the mane is not well-defined. \textbf{(d) Weighted Sum:} Some images exhibit imperfections in the mane and facial features of the horses. \textbf{(e) Bilevel:} Generates visually striking, highly realistic images that closely align with the given prompt. } 
    
    \label{fig:results_visualization_1st_horse}
\end{figure}


\noindent\textbf{Exponential moving average (EMA).} We also incorporate EMA, which is an indispensable strategy in all high-quality image generation methods to stabilize training \citep{nichol2021improved, songscore, ho2022cascaded,karras2022elucidating}. EMA maintains a running average of model parameters over time, where recent updates are weighted more heavily than older ones so that it smooths out fluctuations in the training process. 


\begin{figure}[htbp]
    \centering
    \begin{tabular}{ccc}
        \includegraphics[width=0.3\textwidth]{figures/first_exp/grid_elephant_step_7.png} &
        \includegraphics[width=0.3\textwidth]{figures/first_exp/bayesian_elephant_step_7.png} &
        \includegraphics[width=0.3\textwidth]{figures/first_exp/random_elephantstep_7.png} \\
        \makecell{(a) Grid Search \\ (FID= 131.95, CLIP= 31.66)} & 
        \makecell{(b) Bayesian Search \\ (FID= 128.15, CLIP= 34.16)} & 
        \makecell{(c) Random Search \\ (FID= 127.91, CLIP= 36.34)} \\[5pt]
    \end{tabular}
    
    \vspace{5pt}
    
    \begin{tabular}{ccc}
        \includegraphics[width=0.3\textwidth]{figures/first_exp/weighted_elephant_step_7.png} &
        \includegraphics[width=0.3\textwidth]{figures/first_exp/bilevel_elephant_image.png} &
        \includegraphics[width=0.3\textwidth]{figures/first_exp/elephant_lambda_01step_7.png} \\
        \makecell{(d) Weighted Sum \\ (FID= 117.95, CLIP= 37.85)} & 
        \makecell{(e) \textbf{Bilevel} \\ \textbf{(FID= 105.72, CLIP= 39.05)}} & 
        \makecell{(f) $\lambda=0.01$ \\ (FID= 440.40, CLIP= 18.78)}
    \end{tabular}

    \caption{Visualization of the final generated images by different methods. Prompt: "An African elephant on a foggy morning, with hot air balloons landing in the background." \textbf{(a) Grid Search:} Some images exhibit deformed elephant figures, and the hot air balloons are missing. \textbf{(b) Bayesian Search:} The images appear more abstract, with deformed elephants and trees. Elephant is even missing in one figure, while another depicts elephants on top of a tree. \textbf{(c) Random Search:} The elephant and hot air balloons are unclear in some images. \textbf{(d) Weighted Sum:} Struggles to generate a recognizable elephant figure, producing a deformed trunk instead. \textbf{(e) Bilevel:} Generates relatively high-quality images with no visible deformations.  
}
    \label{fig:results_visualization_1st_elephant}
\end{figure}




\begin{figure}[htbp]
    \centering
    \begin{tabular}{ccc}
        \includegraphics[width=0.3\textwidth]{figures/first_exp/grid_man_step_7.png} &
        \includegraphics[width=0.3\textwidth]{figures/first_exp/bayesian_man_step_7.png} &
        \includegraphics[width=0.3\textwidth]{figures/first_exp/random_man_step_7.png} \\
        \makecell{(a) Grid Search \\ (FID= 144.23, CLIP= 31.37)} & 
        \makecell{(b) Bayesian Search \\ (FID= 142.45, CLIP= 32.71)} & 
        \makecell{(c) Random Search \\ (FID= 121.23, CLIP= 33.97)} \\[5pt]
    \end{tabular}
    
    \vspace{5pt}
    
    \begin{tabular}{ccc}
        \includegraphics[width=0.3\textwidth]{figures/first_exp/weighted_sum_step_7.png} &
        \includegraphics[width=0.3\textwidth]{figures/first_exp/bilevel_man_step_7.png} &
        \includegraphics[width=0.3\textwidth]{figures/first_exp/grid01_man_step_7.png} \\
        \makecell{(d) Weighted Sum \\ (FID= 118.91, CLIP= 34.53)} & 
        \makecell{(e) \textbf{Bilevel} \\ \textbf{(FID= 112.78, CLIP= 36.65)}} & 
        \makecell{(f) $\lambda=0.01$ \\ (FID= 450.34, CLIP= 16.26)}
    \end{tabular}

    \caption{Visualization of the final generated images by different methods. Prompt: "A gentleman wearing white clothes and a beard, posing in a seaside setting." \textbf{(a) Grid Search:} Struggles to generate human faces; images appear blurry. \textbf{(b) Bayesian Search:} Some faces are blurry, and hands are deformed. \textbf{(c) Random Search:} Some face images appear blurry. \textbf{(d) Weighted Sum:} Produces comparatively good-quality images. \textbf{(e) Bilevel:} Generates relatively high-quality images with no visible deformations.
}
    \label{fig:results_visualization_1st_man}
\end{figure}



\end{document}