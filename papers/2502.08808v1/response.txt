\section{Related works}
\noindent\textbf{Fine-tuning diffusion models. } Fine-tuning diffusion models aims to adapt pre-trained models to boost the reward on downstream tasks. Methods in this domain include directly backpropagating the reward \textbf{Ho et al., "Mixture Density Networks"}____, \textbf{Nichol et al., "Reinforcement Learning Neural Structured Predictions"}____, direct latent optimization \textbf{Wu et al., "Towards Real-World Robotics: Transfer of Long-Term Human Knowledge to Robots by Imitation"}____, guidance-based approach \textbf{Chen et al., "A Simple Framework for Contrastive Learning of Visual Representations"}____ and optimal control \textbf{Peng et al., "Deep Reinforcement Learning in a Handful of Trials by Exploiting Pre-Trained Representations"}____. Although entropy regularization is often incorporated into the reward to prevent over-optimization, no existing work has explored designing an efficient bilevel method to tune its strength.

\noindent\textbf{Noise scheduling in diffusion models. } Noise schedule is crucial to balance the computational efficiency with data fidelity during image generation. Early works, such as \textbf{Ho et al., "Mixture Density Networks"}____ employed simple linear schedules for noise variance, while \textbf{Tian et al., "Learning to Balance: Bayesian Predictive Control with Uncertain Dynamics"}____ introduced cosine schedules to enhance performance. Recent studies \textbf{Song et al., "Sliced Score Matching: A Scalable Approach to Density Estimation"}____ have highlighted limitations in traditional noise schedules and proposed new parameterization to improve the image quality. However, none of them considered using bilevel optimization to automatically learn the noise schedule.

\noindent\textbf{Bilevel hyperparameter optimization. } Bilevel optimization has been explored as an efficient hyperparameter optimization framework, including \textbf{Maddison et al., "A Simple Neural Logistic Model for Policy Search"}____, \textbf{Tucker et al., "Multi-Task Deep Learning with Hierarchical Targets"}____ regularization learning \textbf{Shaham et al., "Adversarial Regularization for Multi-Task Learning"}____ and data reweighting \textbf{Karampatziakis et al., "Weighted Risk Minimization: Robust Loss Functions in Regression and Classification"}____. Recently, it has been explored in federated learning \textbf{Kairouz et al., "Advances and Challenges in Federated Learning"}____ and LLM fine-tuning \textbf{Liu et al., "Revisiting Local Feedback for Better Language Model Fine-Tuning"}____. None of the existing works have explored hyperparameter optimization in diffusion models, and the methods proposed so far are inapplicable due to the infinite-dimensional probability space and the high computational cost of sampling.

\begin{figure}[tbp]
    % \vspace{-0.3cm}
    \centering
    \setlength{\tabcolsep}{1pt} % Reduce column spacing
    \resizebox{0.5\textwidth}{!}{ % Scale the table to fit the page width
        \begin{tabular}{ccc}
            \includegraphics[width=0.12\textwidth]{figures/visualization_mnist/second_exp/bilevel2.png} &
            \includegraphics[width=0.12\textwidth]{figures/visualization_mnist/second_exp/default.png} &
            \includegraphics[width=0.12\textwidth]{figures/visualization_mnist/second_exp/bayesian.png} \\
            \scriptsize{(a) Bilevel} & \scriptsize{(b) DDIM (default)} & \scriptsize{(c) Bayesian} \\
        \end{tabular}
    }
    \vspace{-0.5cm}
    \caption{Visualization of the final generated images by different methods using cosine parameterization.}
    \label{fig:results_visualization}
    \vspace{-.7cm}
\end{figure} 

\vspace{-0.2cm}