\section{Introduction}
\label{sec:introduction}

The ability to reason about visual information, particularly in spatial contexts, is a hallmark of intelligent systems. From navigating physical environments to interpreting complex diagrams, visual spatial reasoning is crucial for a wide range of tasks. While Large Language Models (LLMs) have achieved impressive performance in natural language processing and code generation, their capacity for genuine visual reasoning, especially spatial understanding and sequential decision-making in visual environments, remains a significant open question \citep{zhang2024visionlanguagemodelsvisiontasks, ma2024surveyvisionlanguageactionmodelsembodied}. Current Vision-Language Models (VLMs) often excel at pattern recognition and object identification but may struggle with tasks requiring deeper spatial inference and step-by-step planning 


\begin{figure}[htbp]
  \centering
  \resizebox{1.05\linewidth}{!}{\input{figures/mazebench.tex}}
  \caption{MazeBench scores over GRPO steps with a linear regression trendline and its $\pm1$ standard deviation bounds.}
  \label{fig:maze_plot}
\end{figure}

in visual domains \citep{ma2024surveyvisionlanguageactionmodelsembodied}. Bridging this gap and endowing standard LLMs with robust visual reasoning capabilities is a critical step towards more versatile and human-like AI.


In this paper, we address the challenge of teaching visual spatial reasoning to a standard LLM, focusing on the task of maze navigation. We hypothesize that by providing an LLM with a tokenized \textit{visual} representation of a maze, we can train it to learn step-by-step movement commands to navigate from a designated origin to a target. The core of our approach lies in a two-stage training framework. First, we employ Supervised Fine-Tuning (SFT) to equip the LLM with the foundational skill of predicting movement tokens based on the visual maze input. Subsequently, we apply Group Relative Policy Optimization (GRPO), drawing inspiration from recent advancements in reinforcement learning for reasoning in LLMs, such as DeepSeek-R1 \citep{Guo2025DeepSeekR1}. DeepSeek-R1 demonstrated that Reinforcement Learning (RL) can elicit emergent reasoning behaviors, including chain-of-thought, even without prior SFT. We adapt and extend these RL strategies, combined with carefully designed reward functions, to refine our model's visual reasoning process for maze navigation.

To systematically evaluate LLM's ability to solve maze, we introduce MazeBenchâ€”a comprehensive benchmark on solving maze. MazeBench provides a controlled yet diverse environment that spans a range of maze sizes and complexities. By evaluating our model on MazeBench, we can rigorously measure both its maze-solving accuracy and the sophistication of its emergent reasoning behavior.

Our key contributions are as follows:

\begin{itemize}
    \item We present a novel training framework that combines Supervised Fine-Tuning and Group Relative Policy Optimization to enhance \textit{visual} reasoning in standard LLMs, specifically for spatial tasks.
    \item We empirically demonstrate that this framework, using a tokenized visual maze representation, enables an LLM to achieve improved maze navigation accuracy and exhibit emergent chain-of-thought reasoning in generating movement sequences.
    \item We provide a detailed analysis of the design and impact of reward functions within the GRPO stage, highlighting their crucial role in shaping the model's visual reasoning performance.
    \item We draw comparisons with insights from state-of-the-art reasoning models like DeepSeek-R1, both in terms of methodology and observed emergent behaviors, positioning our work within the context of current advancements in LLM reasoning.
    \item We present MazeBench, a benchmark for visual maze navigation that captures a wide spectrum of spatial challenges.
\end{itemize}
