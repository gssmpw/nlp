\section{Related Work}
\label{sec:related_work}

\subsection{Chain-of-Thought Reasoning in Language Models}

Chain-of-Thought (CoT) prompting has emerged as a powerful technique to elicit complex reasoning from Large Language Models \citep{Wei2022ChainofThought}. By prompting LLMs to "think step by step," CoT encourages the generation of intermediate reasoning steps, leading to improved performance on tasks requiring multi-step inference. Prior research, including \citet{Wei2022ChainofThought}, \cite{wei2023chainofthoughtpromptingelicitsreasoning}, \cite{wang-etal-2023-towards} prompting significantly enhances LLM performance on arithmetic, commonsense reasoning, and symbolic reasoning tasks. Our work builds upon the concept of CoT reasoning, aiming to induce a similar step-by-step thought process in LLMs, but within the domain of visual spatial reasoning for maze navigation.

\subsection{Supervised Fine-Tuning for Visual and Spatial Tasks}

Supervised Fine-Tuning (SFT) is a widely adopted technique for adapting pre-trained LLMs to specific downstream tasks \citep{wei2022finetunedlanguagemodelszeroshot}. By training on task-specific datasets, SFT allows LLMs to acquire specialized skills and improve performance in targeted domains.  \citet{jiang2024supervisedfinetuningturnimproves} recently highlighted the effectiveness of SFT in enhancing visual foundation models, demonstrating its utility in visual tasks.  In our research, we leverage SFT as the initial stage of our training pipeline, using it to equip the LLM with the basic capability of processing tokenized visual maze inputs and predicting movement tokens.  This SFT phase serves as a crucial foundation upon which we build more sophisticated reasoning through reinforcement learning.

\subsection{Reinforcement Learning and GRPO for Reasoning and Reward Shaping}

Reinforcement Learning from Human Feedback (RLHF) and its variants have demonstrated significant efficacy in aligning Large Language Models (LLMs) with human preferences and enhancing their reasoning capabilities. However, RLHF faces substantial scalability challenges due to its resource-intensive nature and reliance on human feedback data. As an alternative approach, recent methodologies like Group Relative Policy Optimization (GRPO)\citep{kwon2023rewarddesignlanguagemodels} and Self-Play fIne-tuNing (SPIN) leverage self-play mechanisms, where models autonomously generate training signals and iteratively improve through self-competition \cite{chen2024selfplayfinetuningconvertsweak}. These self-play approaches show promise in achieving human-level performance without the need for extensive human feedback, potentially offering a more scalable solution to the alignment challenge. GRPO, as described by \citet{shao2024deepseekmathpushinglimitsmathematical} and implemented in DeepSeek-R1 \citep{Guo2025DeepSeekR1}, offers a computationally efficient approach to reinforcement learning by estimating advantages based on group scores, eliminating the need for a separate critic network.  Reward function design is paramount in RLHF and GRPO, as it directly guides the model's learning process.  Carefully crafted reward functions can incentivize desired behaviors and shape the model's policy towards optimal performance.  Our work draws inspiration from the reward shaping strategies used in DeepSeek-R1 and adapts them to the context of visual maze navigation, designing reward components to encourage accuracy, valid movement sequences, and proper output formatting.

\subsection{DeepSeek-R1 and Emergent Reasoning through RL}

The DeepSeek-R1 model \citep{Guo2025DeepSeekR1} represents a significant advancement in using reinforcement learning to elicit sophisticated reasoning capabilities in LLMs.  A key finding of DeepSeek-R1 is the demonstration that pure RL, specifically GRPO, can lead to the \textit{emergent} development of chain-of-thought reasoning and even "aha moments," where the model re-evaluates previous steps and corrects its reasoning process.  Furthermore, DeepSeek-R1 highlights the benefits of a multi-stage training pipeline, combining initial RL training with subsequent supervised fine-tuning to refine language coherence and readability.  We directly adapt the GRPO optimization strategy and multi-stage training insights from DeepSeek-R1 to our visual maze navigation task.  We hypothesize that similar RL techniques can drive the emergence of visual spatial reasoning in standard LLMs, enabling them to solve mazes through a step-by-step, self-corrective process.

\subsection{Visual Reasoning and Maze Solving in AI}

Maze solving has long been a benchmark task in Artificial Intelligence, serving as a testbed for various problem-solving and search algorithms \citep{MazeSolverRobotScholarWorks}. Traditional approaches include graph search algorithms like Depth-First Search, Breadth-First Search, and A* \citep{PathfindingAlgorithmsRedBlobGames}. More recently, AI techniques, particularly reinforcement learning and neural networks, have been applied to maze navigation \citep{DeepRLMazeSolvingSamyzaf}.
While Chain-of-Thought (CoT) prompting has significantly enhanced complex reasoning capabilities in Large Language Models (LLMs) and Multimodal LLMs, it shows limitations in complex spatial reasoning tasks. Recent work by Microsoft introduces Multimodal Visualization-of-Thought (MVoT), which enables models to generate visual representations during their reasoning process, similar to human visual thinking \citep{li2025imaginereasoningspacemultimodal}. This breakthrough demonstrates the potential of combining verbal and visual reasoning in AI systems.


Our research builds upon these advances, focusing on teaching visual maze reasoning to standard language models through a tokenized visual representation and a combination of SFT and GRPO. This approach differs from traditional maze solvers by leveraging the inherent reasoning capabilities of LLMs and adapting them to process and reason about visual spatial information. Furthermore, research in neural-symbolic visual reasoning \citep{NeuralSymbolicVisualReasoningArxiv} explores combining neural networks with symbolic AI for visual tasks, offering a complementary perspective on integrating reasoning and visual processing.