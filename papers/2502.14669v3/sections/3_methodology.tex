\section{Methodology}
\label{sec:methodology}

\subsection{Tokenized Visual Maze Representation}{\label{sec:3.1}}
\label{subsec:tokenized_maze}
\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figures/example_maze.png}
    \caption{Visual of the Example Maze}
    \label{fig:example_maze}
\end{figure}
To enable the LLM to process maze information visually, we designed a tokenized input format that represents the maze grid, walls, origin, and target locations. Each cell in the maze is represented by a coordinate token \texttt{<|row-col|>}, e.g., \texttt{<|0-0|>} for the top-left cell. Wall information for each cell is encoded using tokens such as \texttt{<|no\_wall|>, <|up\_wall|>, <|up\_down\_wall|>, <|up\_down\_left\_right\_wall|>, ...}. The origin and target locations are marked with \texttt{<|origin|>} and \texttt{<|target|>} tokens, respectively. Empty spaces within the maze representation are filled with \texttt{<|blank|>} tokens for consistent grid structure. This tokenization scheme provides a visual representation by explicitly encoding the spatial relationships between cells and the presence of walls, allowing the LLM to ``see'' the maze structure in a symbolic, tokenized form.

\medskip
\noindent \textbf{Example Maze Tokenization \ref{fig:example_maze}:}

\begin{lstlisting}
<|0-0|><|up_left_wall|><|blank|><|0-1|><|up_down_wall|><|blank|><|0-2|><|up_down_wall|><|blank|><|0-3|><|up_down_right_wall|><|blank|><|0-4|><|up_left_right_wall|><|blank|>
<|1-0|><|down_left_wall|><|blank|><|1-1|><|up_down_wall|><|blank|><|1-2|><|up_right_wall|><|blank|><|1-3|><|up_down_left_wall|><|blank|><|1-4|><|right_wall|><|blank|>
<|2-0|><|up_left_wall|><|blank|><|2-1|><|up_down_right_wall|><|origin|><|2-2|><|left_right_wall|><|blank|><|2-3|><|up_left_wall|><|blank|><|2-4|><|right_wall|><|blank|>
<|3-0|><|left_wall|><|blank|><|3-1|><|up_right_wall|><|blank|><|3-2|><|down_left_wall|><|blank|><|3-3|><|down_right_wall|><|blank|><|3-4|><|left_right_wall|><|blank|>
<|4-0|><|down_left_right_wall|><|blank|><|4-1|><|down_left_wall|><|blank|><|4-2|><|up_down_wall|><|target|><|4-3|><|up_down_wall|><|blank|><|4-4|><|down_right_wall|><|blank|>
\end{lstlisting}

\subsection{Baseline Models}
\label{subsec:baseline_model}

To establish performance benchmarks for our approach, we employed three distinct baseline models, leveraging the DeepSeek-R1 \citep{Guo2025DeepSeekR1} Distill-Qwen family of language models. We evaluate two distilled models: DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Qwen-1.5B. Additionally, a Direct Prediction baseline was established using a Supervised Fine-Tuning (SFT) approach on the DeepSeek-R1-Distill-Qwen-1.5B architecture. This model was trained to directly predict the complete sequence of movement tokens representing the solution path through a given maze. The training objective was the minimization of cross-entropy loss between the predicted token sequence and the ground truth solution. This baseline assesses the performance of a standard language model trained to generate complete solutions without intermediate reasoning steps or reinforcement learning techniques. 

We include these three baselines to provide a comprehensive comparison, examining the influence of model size (7B vs. 1.5B) and the effectiveness of direct prediction versus our proposed step-by-step and reinforcement learning approaches. The subsequent sections will primarily focus on the customized direct prediction model and its enhancements through SFT for step-by-step reasoning and GRPO.
\subsection{Supervised Fine-Tuning (SFT) for Step-by-Step Reasoning}
\label{subsec:sft}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/maze-reasoning.png}
    \caption{Visualization of AlphaMaze's step-by-step reasoning process while solving a maze.}
    \label{fig:visual-reasoning}
\end{figure}

For the SFT stage, we curated a training dataset. Mazes were synthetically generated with fixed sizes (5x5) and varied complexity level. The Qwen 1.5B SFT model was then trained on this dataset. The training objective was to predict the \textit{next} movement token at each step, conditioned on the maze input and the preceding movement tokens in the sequence as visually illustrated in Figure \ref{fig:visual-reasoning}. This step-by-step prediction approach was designed to encourage the model to learn sequential reasoning for maze navigation.

\subsection{Group Relative Policy Optimization (GRPO) for Enhanced Reasoning}
\label{subsec:grpo}

Following SFT, we applied Group Relative Policy Optimization (GRPO) to further enhance the model's maze-solving capabilities and encourage more robust reasoning. The GRPO training utilized a smaller set of data than SFT state. We designed a reward function 3 components.

\textbf{Correctness Reward (+0.2 per solution step):} This reward is scaled according to the number of steps in the maze solution. Each valid movement step adds 0.2 points to the total score. For example, a solution requiring 4 steps earns a reward of \(0.2 \times 4 = 0.8\) points, incentivizing both accuracy and efficiency in navigation.

\textbf{Integrity Reward (+0.5):} This reward is given for each valid movement token (\texttt{<|up|>}, \texttt{<|down|>}, \texttt{<|left|>}, \texttt{<|right|>}) in the predicted sequence, encouraging the generation of meaningful and valid movement steps.

\textbf{Thinking Reward (+0.25):} This reward is given for correctly using the \texttt{<think>} tag in the output, ensuring completeness and consistency in the reasoning format.


These reward components were weighted to prioritize correctness while also encouraging valid movement sequences and proper reasoning formatting with \texttt{<think>} tag. We adapted the Group Relative Policy Optimization (GRPO) algorithm, as employed in DeepSeek-R1 \citep{Guo2025DeepSeekR1}, to perform reinforcement learning. GRPO estimates advantages based on relative group scores, offering computational efficiency compared to critic-based methods.


\subsection{Training Procedure and Pipeline}
\label{subsec:training_pipeline}

Our training pipeline consisted of two stages. First, \textbf{Supervised Fine-Tuning (SFT)} was performed on the Qwen 1.5B model using a curated maze dataset for 10 epochs to learn step-by-step movement prediction for maze navigation. This phase established a strong initial policy, ensuring that the model could effectively interpret and respond to sequential movement tasks.  

Following SFT, \textbf{Group Relative Policy Optimization (GRPO)} was applied to refine the modelâ€™s performance. The SFT-trained model was further fine-tuned using LoRA \cite{hu2021loralowrankadaptationlarge} with the GRPO method, implemented in Unsloth \cite{unsloth} with VLLM \cite{kwon2023efficient} for efficient inference. A carefully designed reward function guided the optimization process, and model checkpoints were saved every 200 steps to track improvements.  

This two-stage pipeline mirrors the multi-stage training approach employed in DeepSeek-R1 \citep{Guo2025DeepSeekR1}, where initial RL training is followed by supervised fine-tuning for refinement. In our case, SFT provided a robust starting point for reinforcement learning (RL), allowing GRPO to focus on refining reasoning capabilities and enhancing task-specific performance.
