\section{Experiments and Results}
\label{sec:experiments}

\subsection{Dataset Details}
\label{subsec:dataset_details}
The dataset is constructed through a multi-stage process involving generation, refinement, and augmentation. The process begins with the creation of a large initial pool of 530,000 synthetic mazes. These mazes are generated using the \textbf{maze-dataset} framework \cite{ivanitskiy2023configurablelibrarygeneratingmanipulating}, which employs a randomized depth-first search algorithm. This algorithm ensures that every generated maze has a guaranteed solution path connecting the designated origin and target locations. Further details of algorithm used can be found at Appendix \ref{alg:main}. All mazes within the dataset have a fixed size of 5x5 grids. From this extensive initial pool, a subset of 30,000 mazes is randomly selected and reserved as a held-out test set. This separation guarantees that the training and evaluation data are entirely distinct, preventing data leakage and enabling a robust assessment of model generalization.

The remaining 500,000 mazes form the basis for the various training datasets used in this work. This pool of mazes undergoes a multi-stage processing and augmentation procedure to create datasets specifically tailored for different training objectives.

\textit{\textbf{Reset Dataset Creation:}} A significant portion of the training data focuses on teaching the model to recover from errors. To this end, a "reset" dataset is created. This dataset is generated by algorithmically producing \textbf{incorrect} solution paths for the mazes. These incorrect paths are designed to be plausible but ultimately unsuccessful, either leading to dead ends or deviating from the correct solution. Importantly, they adhere to constraints: they do not revisit locations already visited within the incorrect attempt, and they avoid portions of the known correct solution path.

Associated with each incorrect path, a textual "RESET" message is generated, simulating the feedback a system might provide upon encountering an error.  The content of this message depends on whether the incorrect path terminates at a dead end (three surrounding walls) or simply deviates from the wrong route.  These incorrect paths, along with their reset messages and the \textbf{correct} solution's Chain-of-Thought (COT) reasoning, are combined.  This process results in approximately 400,000 training examples where the model is presented with scenarios of initial failure(s) followed by a successful attempt after a "reset."  The intent is to train the model to recognize incorrect trajectories and adapt its strategy. Illustrative examples of reset samples are provided in Appendix.


 \textit{\textbf{SFT Training Data Construction:}} The final Supervised Fine-Tuning (SFT) training dataset is a balanced combination of "straight success" examples and "retry" examples:
\begin{itemize}
    \item \textit{Straight Success Data} (250,000 mazes):  This portion consists of mazes where the model is expected to generate the correct solution path on the first attempt, without any resets.
    \item \textit{Retry Data} (250,000 mazes): This portion is drawn from the "reset" dataset described above, providing examples where the model learns from incorrect attempts and subsequent resets. This combined 500,000-maze SFT set, encompassing success and error recovery, enables robust learning.
\end{itemize}
 \textit{\textbf{GRPO Training Data:} } The remaining 150,000 mazes from the original "straight success" data pool are used to create a dataset for GRPO stage. 



\subsection{MazeBench}

To rigorously evaluate the spatial reasoning and planning capabilities of large language models (LLMs), we introduce MazeBench, a novel benchmark consisting of a curated collection of 100 maze-solving challenges. While existing benchmarks often assess logical reasoning or commonsense knowledge, MazeBench specifically targets the ability of LLMs to understand spatial relationships, plan multi-step paths, and execute sequential actions within a constrained environment. This capacity is crucial for applications ranging from robotics and navigation to game playing and virtual agent control.



MazeBench is a collection of 100 unique mazes, randomly selected from a larger test set containing 30,000 mazes. It is designed to evaluate the performance of large language models (LLMs) by categorizing mazes into different difficulty levels. Each maze requires the model to determine an optimal path from the starting point to the target, with difficulty primarily based on the number of steps needed to reach the goal.

The benchmark is structured into three levels: Easy, Medium, and Hard, ensuring a progressive assessment of an LLM’s pathfinding and problem-solving abilities. The components are described in Table \ref{tab:maze-config}.

\begin{table}[htbp]
    \centering
    \caption{Maze Configuration by Difficulty Level}
    \label{tab:maze-config}
    \begin{tabular}{|l|c|c|}
        \hline
        Category & Number of Mazes & Steps \\
        \hline
        Easy   & 50 & 1 -- 4 \\
        Medium & 40 & 5 -- 8 \\
        Hard   & 10 & 9 -- 13 \\
        \hline
        \textbf{Total} & \textbf{100} & \textbf{1 -- 13} \\
        \hline
    \end{tabular}
\end{table}

\textbf{The Easy category} consists of 50 mazes, each requiring between 1 and 4 steps to solve. These simpler mazes establish a baseline for evaluating fundamental navigation skills.

\textbf{The Medium category} includes 40 mazes that demand solution paths of 5 to 8 steps. These mazes introduce a higher level of complexity, requiring more advanced planning and spatial reasoning. Successfully solving them indicates an LLM’s ability to manage moderately intricate environments.

\textbf{The Hard category} comprises 10 mazes, each necessitating 9 to 13 steps to reach the target. These mazes present the greatest challenge, testing an LLM’s capacity to handle long-range dependencies and navigate complex spatial structures. Performance on this level reflects the model’s ability to process and reason over extended solution paths.

As mentioned previously, the mazes are presented to the LLM in a tokenized input format; the full details of this representation, including examples, are provided in Section~\ref{sec:3.1}. The LLM is expected to produce sequence of movement tokens. During evaluation, we will parse the LLM's output to extract these tokens. The order of these tokens is crucial. The presence of extraneous characters, whitespace, or other tokens will not automatically invalidate the solution, provided that the correct sequence of movement tokens can be extracted. A solution is considered incorrect if the extracted sequence of movement tokens does not lead to the target or leads to an invalid state (e.g., attempting to move into a wall) is considered incorrect. The evaluation metric is the success rate: the percentage of mazes solved correctly.

\subsection{Quantitative Results}
\label{subsec:experimental_results}

\subsubsection{Model Performance on MazeBench}
\label{subsubsec:accuracy}
As shown in Table~\ref{tab:accuracy}, the baseline model, trained for direct path prediction without explicit reasoning, achieved 0\% accuracy on MazeBench. This highlights the necessity of step-by-step reasoning for the task. The SFT-only model reached a baseline of 86.0\%, demonstrating the effectiveness of supervised fine-tuning for learning step-by-step maze navigation. Further enhancement with GRPO led to significant improvement, reaching 93.0\% after 1600 steps of GRPO training.

\begin{table}[htbp]
    \centering
    \caption{Maze Solving Accuracy on MazeBench}
    \label{tab:accuracy}
    \begin{tabular}{|l|c|c|c|}
        \hline
        Model & SFT & GRPO & Score \\
        \hline
        Baseline-1.5B & \ding{55} & \ding{55} & 0.0 \\   
        Baseline-7B & \ding{55} & \ding{55} & 0.0 \\
        Baseline-1.5B (SFT) & \checkmark & \ding{55} & 0.0 \\
        AlphaMaze-SFT & \checkmark & \ding{55} & 86.0 \\
        AlphaMaze & \checkmark & \checkmark & \textbf{93.0} \\
        \hline
    \end{tabular}
\end{table}


\subsubsection{Model Evolution During GRPO}
\label{subsubsec:training_dynamics}

Figure~\ref{fig:maze_plot} displays the MazeBench scores (blue crosses) over GRPO steps along with a linear regression trendline (red dashed line) and its $\pm1$ standard deviation bounds. The steady increase in the trendline indicates that GRPO effectively guides the model towards improved maze-solving policies.

\subsection{Qualitative Results}
\label{subsec:qualitative_results}

Qualitative analysis of model outputs revealed notable differences in reasoning behavior. The baseline model often produced nonsensical or incomplete movement sequences, frequently failing to reach the target and exhibiting "hallucinations" by predicting movements invalid within the maze structure. The \textbf{AlphaMaze-SFT} model demonstrated improved coherence and step-by-step progression, but still struggled with longer or more complex mazes, sometimes becoming trapped in loops or making incorrect turns in later stages of the solution path.

In contrast, the \textbf{AlphaMaze-SFT+GRPO} model exhibited the most sophisticated reasoning. In many instances, emergent chain-of-thought patterns were observed, with AlphaMaze (two-stage) appearing to explicitly consider wall constraints and spatial relationships at each step before predicting the next movement. Furthermore, outputs occasionally displayed instances reminiscent of the "aha moments" reported in prior work on DeepSeek-R1. For example, in some complex mazes, AlphaMaze (two-stage) would initially begin along one path, then appear to "re-evaluate" its trajectory mid-sequence, correcting its course to find a more efficient or correct solution. Error analysis indicated that AlphaMaze (two-stage) made fewer invalid moves and was more robust to long-context reasoning challenges compared to the AlphaMaze-SFT model. However, limitations remained, particularly in mazes requiring backtracking or complex spatial planning beyond the immediate next step.