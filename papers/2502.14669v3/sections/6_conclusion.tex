\section{Conclusion}
\label{sec:conclusion}

This paper introduced AlphaMaze, a novel approach to enhance Large Language Models' spatial intelligence, focusing on maze navigation. We demonstrated the efficacy of a two-stage training framework, leveraging Supervised Fine-Tuning (SFT) followed by Group Relative Policy Optimization (GRPO). While initial pre-trained LLMs exhibited 0\% accuracy on MazeBench, highlighting the need for task-specific adaptation, our approach successfully imbued a distilled LLM with robust spatial reasoning capabilities. SFT provided a crucial foundation by teaching step-by-step movement prediction from tokenized maze inputs, reaching 86\% accuracy. This underscores the importance of structured input and targeted training for LLMs to effectively engage with visual spatial information.

Crucially, we adapted and applied the two-stage training methodology pioneered by DeepSeek-R1, demonstrating its generalizability beyond language-centric reasoning tasks. Following SFT, GRPO fine-tuning further elevated performance to 93\% on MazeBench after 1600 training steps, showcasing the power of reinforcement learning to refine reasoning processes in a novel domain. Qualitative analysis revealed that GRPO fostered more sophisticated and self-corrective reasoning strategies, including emergent chain-of-thought patterns, mirroring observations in DeepSeek-R1 and suggesting a common mechanism for enhanced reasoning through RL.

Our work contributes to the broader effort of expanding LLMs' reasoning abilities beyond natural language, demonstrating the potential of a two-stage approach for visually grounded tasks. The success of GRPO, inspired by DeepSeek-R1's advancements in language reasoning, highlights the transferability of these techniques to spatial domains. This suggests that carefully designed reinforcement learning, following an initial phase of supervised task learning, can be a powerful method to unlock and refine sophisticated reasoning capabilities in LLMs across diverse problem spaces. The implications extend beyond maze navigation to a wide array of applications demanding spatial understanding and sequential decision-making.

Future research will focus on further validating this two-stage GRPO approach across various reasoning domains beyond spatial tasks, exploring its potential to enhance LLMs' capabilities in areas such as symbolic reasoning, logical deduction, and planning. Investigating the optimal configurations of SFT and GRPO stages, diversifying training data to encompass richer and more complex reasoning scenarios, and developing more refined reward functions tailored to different reasoning challenges are critical next steps. By pursuing these directions, we aim to establish the broader applicability of this two-stage training paradigm for imbuing standard LLMs with robust and versatile reasoning abilities, paving the way for more capable and generally intelligent language models.