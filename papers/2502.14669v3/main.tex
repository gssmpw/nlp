\documentclass[twocolumn]{article} % Or use 'IEEEtran' for IEEE format, etc.

\usepackage{multicol}  % For two-column support
\usepackage[letterpaper, top=0.6in, bottom=0.5in, inner=0.75in, outer=0.75in, headsep=0.3in, footskip=0.3in]{geometry}

\usepackage{titlesec}
\titleformat{\section}{\large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection}{1em}{}

% --- Packages ---
\usepackage{graphicx}         % For including images
\usepackage{amsmath, amssymb} % For math symbols and equations (even if you don't use much, good practice)
\usepackage{hyperref}          % For hyperlinks (URLs, references)
\usepackage{natbib}          % For bibliography management (BibTeX style)
\usepackage{geometry}         % For adjusting margins if needed
\usepackage{listings}
\usepackage{pgfplots}
\usepackage{amssymb} % For \checkmark
\usepackage{pifont}  % For \xmark
\usepgfplotslibrary{fillbetween}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath} 
\usepackage{appendix}



\lstset{
    basicstyle=\ttfamily\small,  % Monospaced font, smaller size
    breaklines=true,             % Wrap long lines automatically
    frame=single,                % Adds a border around the code block
    showstringspaces=false       % Prevents spaces from being visible
}
% --- Optional: Load custom style file if you create one ---
% \usepackage{latex_style/custom_style}

% --- Metadata ---
\title{\textbf{AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via GRPO}}


\author{
    Alan Dao (Gia Tuan Dao)\textsuperscript{1}, Dinh Bach Vu\textsuperscript{1} \\
    Menlo Research \\
    \texttt{alan@menlo.ai, bach@menlo.ai} \\
    \textsuperscript{1}Equal contribution.
}


\date{February 20, 2025} % Or specify a date, or use \date{} for no date

% --- Start of Document ---
\begin{document}

\maketitle
\begin{abstract}
 % Prevents indenting the abstract paragraph
\noindent Large Language Models (LLMs) have demonstrated impressive capabilities in language processing, yet they often struggle with tasks requiring genuine visual spatial reasoning. In this paper, we introduce a novel two-stage training framework designed to equip standard LLMs with visual reasoning abilities for maze navigation. First, we leverage Supervised Fine-Tuning (SFT) on a curated dataset of tokenized maze representations to teach the model to predict step-by-step movement commands. Next, we apply Group Relative Policy Optimization (GRPO)—a technique used in DeepSeek-R1—with a carefully crafted reward function to refine the model’s sequential decision-making and encourage emergent chain-of-thought behaviors. Experimental results on synthetically generated mazes show that while a baseline model fails to navigate the maze, the SFT-trained model achieves 86\% accuracy, and further GRPO fine-tuning boosts accuracy to 93\%. Qualitative analyses reveal that GRPO fosters more robust and self-corrective reasoning, highlighting the potential of our approach to bridge the gap between language models and visual spatial tasks. These findings offer promising implications for applications in robotics, autonomous navigation, and other domains that require integrated visual and sequential reasoning.
\end{abstract}

% --- Include Sections ---
\input{sections/1_introduction.tex}
\input{sections/2_related_work.tex}
\input{sections/3_methodology.tex}
\input{sections/4_experiments.tex}
\input{sections/5_discussion.tex}
\input{sections/6_conclusion.tex}

\bibliographystyle{plainnat} % Choose your bibliography style (e.g., 'plainnat', 'abbrvnat', etc.)
\bibliography{bibliography} % 'bibliography' is the name of your .bib file (without the extension)
% Note down some papers here:

% --- Appendices (Optional) --- 
% --- Appendices (Optional) ---

% --- Appendix A: Detailed Experimental Setup ---
% \clearpage
\clearpage
\appendix
\onecolumn
\input{sections/appendices.tex}
% ... Appendix content ...

\end{document}