
\begin{abstract}

The advances in digital tools have led to the rampant spread of misinformation. While fact-checking aims to combat this, manual fact-checking is cumbersome and not scalable. It is essential for automated fact-checking to be efficient for aiding in combating misinformation in real-time and at the source. Fact-checking pipelines primarily comprise a knowledge retrieval component which extracts relevant knowledge to fact-check a claim from large knowledge sources like Wikipedia and a verification component. The existing works primarily focus on the fact-verification part rather than evidence retrieval from large data collections, which often face scalability issues for practical applications such as live fact-checking. In this study, we address this gap by exploring various methods for indexing a succinct set of factual statements from large collections like Wikipedia to enhance the retrieval phase of the fact-checking pipeline. We also explore the impact of vector quantization to further improve the efficiency of pipelines that employ dense retrieval approaches for first-stage retrieval. 
We study the efficiency and effectiveness of the approaches on fact-checking datasets such as HoVer and WiCE, leveraging Wikipedia as the knowledge source. We also evaluate the real-world utility of the efficient retrieval approaches by fact-checking 2024 presidential debate and also open source the collection of claims with corresponding labels identified in the debate. Through a combination of indexed facts together with Dense retrieval and Index compression, we achieve up to a \textbf{10.0x} speedup on CPUs and more than a \textbf{20.0x} speedup on GPUs compared to the classical fact-checking pipelines over large collections.

\end{abstract}



% \begin{abstract}

% Query rewriting refers to an established family of approaches that are applied to underspecified and ambiguous queries to overcome the vocabulary mismatch problem in document ranking.
% Queries are typically rewritten during query processing time for better query modelling for the downstream ranker.
% With the advent of large-language models (LLMs), there have been initial investigations into using generative approaches to generate pseudo documents to tackle this inherent vocabulary gap. 
% In this work, we analyze the utility of LLMs for improved query rewriting for text ranking tasks.

% We find that there are two inherent limitations of using LLMs as query re-writers -- concept drift when using only queries as prompts and large inference costs during query processing.
% We adopt a simple, yet surprisingly effective, approach called context aware query rewriting (\car{}) to leverage the benefits of LLMs for query understanding.
% Firstly, we rewrite ambiguous training queries by context-aware prompting of LLMs, where we use only relevant documents as context.
% Unlike existing approaches, we use LLM-based query rewriting only during the training phase.
% Eventually, a ranker is fine-tuned on the rewritten queries instead of the original queries during training.
% In our extensive experiments, we find that fine-tuning a ranker using re-written queries offers a significant improvement of up to \textbf{33\%} on the passage ranking task and up to \textbf{28\%} on the document ranking task when compared to the baseline performance of using original queries.

% \end{abstract}
