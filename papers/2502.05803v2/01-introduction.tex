\section{Introduction}
\label{sec:intro}


The rapid dissemination of disinformation and misinformation in the digital era has catastrophic consequences, impacting public opinion. Since manual fact-checking is not scalable and time-consuming, automated fact-checking approaches have been proposed~\cite{guo2022survey,yin-roth-2018-twowingos,read_twice,schuster2021vitamincrobustfact} to  combat misinformation. Automated fact-checking pipelines mimic the human fact-checking process by collecting multiple pieces of evidence for different aspects of the claim, followed by reasoning over evidence for claim verification~\cite{aly-vlachos-2022-natural,nakov2021automatedfactcheckingassistinghuman}.  Hence, these pipelines are usually comprised of a claim detection component, an evidence/knowledge retrieval component and a fact verification component \cite{thorne2018fever,programfc,jiang2020hover,kamoi2023wice}. Mitigating misinformation spread through platforms such as political debates and election campaigns, is time-critical due to its potential to proliferate faster, sway public opinion and its perceived reliability \cite{multimodal_spreads_faster,political_debates,multimodal_credibility}. Hence, it is essential that the verification pipeline is efficient to verify information at the source in real-time. 


\noindent \textbf{The retrieval bottleneck}: One of the primary bottlenecks in existing fact-checking pipelines is the retrieval component employed for extracting information from large knowledge sources like Wikipedia. Existing approaches primarily employ sparse retrieval approaches followed by re-ranking or dense retrieval approaches \cite{samarinas-etal-2021-improving, jiang2020hover,schlichtkrull-etal-2021-joint} which perform search over a large index incurring huge computational, memory costs and high latency during inference. For instance, indexing and performing dense retrieval over the entire Wikipedia is prohibitively expensive with embedding index size being (\textbf{9.70 GiB}) and average retrieval latency per query (from WiCE) being \textbf{610 ms}. While several efficient sparse and dense Information Retrieval (IR) approaches exist \cite{leonhardt2023efficientneuralrankingusing,efficient_inverted_index,jegou2011pq,ge2014opq}, prior works in fact-checking primarily focus on fact verification components with very few works focusing on improving retrieval effectiveness \cite{zheng-etal-2024-evidence}. However, to the best of our knowledge, our work is the first to explore principled efficient IR approaches applied for fact-checking along with a case-study of its effectiveness for real-time fact-checking.

\noindent \textbf{Objectives}: We envision a fact-checking pipeline with an efficient retrieval component that has \textit{low memory footprint, computation requirements} and \textit{low latency} compared to existing systems. This would enable to run such systems in \textbf{low-resource settings}, making the technology more \textbf{accessible} to effectively combat misinformation at scale and possibly in real-time. 

Hence, in this work, we explore several approaches to extract and index succinct facts from large knowledge sources to aid in fact-checking claims. We observe that the corpus of knowledge sources such as Wikipedia could be compressed to \textbf{59\%} of the original information, optimizing sparse retrieval. Further, we also optimize dense retrieval by exploring vector quantization approaches for compressing the vector index of extracted factual statements which brings down the embeddings index size to \textbf{672.95 MiB} from 9.70 GiB. We observe that this approach leads to speedups of up to \textbf{10x} on the CPU and \textbf{20x} over the classic retrieve-rerank pipelines with the original corpus. The approaches we study also obviate the need for the snippet selection stage in fact-checking pipelines as it already indexes the succinct facts resulting in efficient inference.
 Through extensive experiments, we answer the following research questions:

\noindent \textbf{RQ1}: How does indexing factual statements for large collections affect information retrieval efficiency?

\noindent \textbf{RQ2}: How does indexing factual statements affect overall pipeline efficiency and downstream fact-checking performance?

\noindent \textbf{RQ3}: How does does index compression affect the efficiency of dense retrieval and overall fact-checking pipeline?

\noindent Following are our core contributions: 
\begin{itemize}
    \item We explore approaches aimed at efficiently identifying relevant text spans from large corpora pertaining to the claims being fact-checked, enabling efficient sparse retrieval and verification processes.
    
    \item We show the methods for indexing factual knowledge
    coupled with index compression further optimizes dense retrieval efficiency by reducing computational overhead compared to classical retrieve-rank-select pipelines without significant loss in performance.
    
    \item We deploy the efficient retrieval system for fact-checking the 2024 presidential debate in real-time and also open source the dataset of facts identified with corresponding human annotated labels. 
      

\end{itemize}
\textbf{Reproducibility}: We open-source all our data and code under:
\texttt{\url{https://github.com/kevin-rn/Efficient-Fact-checking}}.
