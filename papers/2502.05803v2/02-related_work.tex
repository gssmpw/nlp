\vspace{-1em}
\section{Related Work}
\vspace{-1em}
The rapid proliferation of misinformation and disinformation has given rise to the development of automated fact-checking systems to combat them \cite{thorne2018fever,programfc,guo2022survey,v2024quantemprealworldopendomainbenchmark,questgen}. The automated fact-checking process involves three stages comprising \textit{claim detection}, which identifies salient spans to be fact-checked, 
followed by \textit{evidence retrieval} that focuses on identifying sources that support or refute claims and finally the fact-verification stage that uses the evidence collected to categorize the claims.

A significant challenge in verification includes source reliability \cite{guo2022survey}, hence fact-checking primarily relies on verified knowledge sources. Sources such as encyclopedias, policy documents, and scientific journals are common knowledge sources employed for retrieving information \cite{Lazarski2021nlpfact, thorne2018automated}. Recent advancements advocate a simplified two-step evidence retrieval approach: a context retriever selects a subset of passages that might contain the answer followed by a machine reader analyzing these passages to identify the correct answer. Initially, evidence retrieval relied on inverted indexes (e.g., TF-IDF, BM25) for keyword-based searches but these lack semantic understanding \cite{baranchuk2018revisiting, wei2022}. The advances in representation learning have led to the rise of dense retrieval where queries and documents are projected to a continuous vector space  \cite{karpukhin2020dense, zhao2022,Guo_2022}. While existing approaches employ brute force search in the vector space to retrieve relevant documents for the queries, this is not scalable for web-scale search \cite{bondarenko2021understanding, zhu2023survey, han2023comprehensive, wei2022,tas_b}. Compact vector representations are crucial for efficiency, despite potential noise-induced performance drops \cite{zhan2021jointly}. Efficient approaches like Approximate Nearest Neighbors (ANN) search in vector databases have emerged to reduce complexity and enhance similarity search accuracy \cite{han2023comprehensive, wei2022, zhao2023ann}. Early on hash-based and tree-based methods were used but faced limitations in large-scale databases and semantic features. Recent advancements make use of quantization techniques, such as Product Quantization\cite{jegou2011pq} and Optimized PQ (OPQ) \cite{ge2014opq}, to improve efficiency by vector dimension reduction with minimal performance loss.

    
% \textit{Fact verification} forms the third stage and entails assessing the veracity of the claim based on the retrieved evidence \cite{guo2022survey}, often employing binary or multi-class labels \cite{guo2022survey, thorne2018automated}. Automated fact-checking mainly uses supervised text classification methods \cite{thorne2018automated}, which, while effective for some tasks, lack the broader world knowledge for comprehensive checking. Improving verdict interpretations is crucial \cite{guo2022survey}, especially with black-box models, through highlighting salient evidence, designing transparent decision-making processes, and generating textual explanations. Large Language Models (LLMs) are increasingly used for natural language tasks due to their impressive performance \cite{openai2023gpt4,ouyang2022training,touvron2023llama}. However, they often generate factually inconsistent outputs, or hallucinations, which look plausible but deviate from reality \cite{azaria2023internal, ji2023, 2023arXiv230511747L, zhang2023sirens}. This issue hinders their use in critical applications like healthcare and legal fields, raising concerns about misinformation, safety, and privacy \cite{ji2023,roit2023factually, umapathi2023medhalt}. LLMs can encode vast amounts of information \cite{knowledge_base_2, knowledge_base_1} but struggle with dynamic, temporal knowledge \cite{li2023unlocking, temporal_aspect}. To address this, efforts are being made to augment LLMs with external knowledge \cite{guu2020realm,ram2023incontext,zhang2022retgen}. This includes two main approaches: 1) retrieve-and-generate models (e.g. RAG\cite{rag}) which combine a knowledge retriever with a generative model, and 2) k-Nearest Neighbor LMs (e.g. RETOMATON\cite{alon2022neurosymbolic}) which use K-NN models to improve token prediction. Additionally, post-hoc attribution and edit methods like RARR\cite{gao2023rarr} and PURR\cite{chen2023purr} retrieve relevant evidence and edit outputs to ensure factual consistency. Existing benchmarks for evaluating factual consistency in LLM-generated texts, such as FactScore \cite{min2023factscore}, HaluEval  \cite{2023arXiv230511747L}, and LLM-augmenter \cite{peng2023check}, focus on fact verification rather than efficient evidence retrieval. 
Our contributions aim to explore efficient retrieval approaches to improve the efficiency of the fact-checking process, enhancing the practical applicability of these systems in real-world scenarios such as live fact-checking over large knowledge bases.



% In this section, we discuss prior work and recent advances.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The notion of fact-checking delves into the logic, coherence, and context of claims \cite{thorne2018automated}. In the fact-checking process, fact verification serves as a crucial preliminary step in acquiring and confirming facts, ensuring the trustworthiness of the information under consideration. The surge in demand for automated fact-checking has prompted rapid advancements in the development of tools and systems for it. Successful of these pipelines rely on efficient handling of large document collections, optimal text span granularity for detailed answers, contextual awareness for appropriate text granularity selection, and versatility across domains \cite{ahmad2019}.  Currently, the typical fact-checking process involves three stages: (i) \textit{claim detection} involves identifying salient text spans from a large collection; (ii) \textit{evidence retrieval} focuses on finding sources that either support or refute the claim; and (iii) \textit{fact verification} entails assessing the veracity of the claim based on the retrieved evidence \cite{guo2022survey}. \\

% For the claim detection part, there is no formal definition of what constitutes a claim \cite{Lazarski2021nlpfact}. Some existing work establishes check-worthiness as a possible concept \cite{guo2022survey}. It determines a claim when one wants to know the truth of that assertion, which either requires binary classification or an importance-ranking to classify. Another method, used in social media settings, is whether text spans are detected for rumourness. Nonetheless, these two methods are quite subjective as the language understanding and importance of the concepts differs between social groups or even individuals \cite{guo2022survey, thorne2018automated}. Furthermore, the information pertaining to the claim can change over time or have been debunked already, no longer necessitating the need for verification. A more objective approach is to classify text as an assertion if it's checkable with available evidence. \\

% Early retrieval systems were typically complex, composed of numerous components \cite{karpukhin2020dense, zhao2022}. However, recent advancements in reading comprehension models advocate for a simplified two-step approach: initially, a context retriever selects a subset of passages, some of which potentially contain the answer to the query, followed by a thorough analysis by a machine reader to identify the correct answer. Nonetheless, a notable challenge arises concerning the sources from which information is pulled \cite{guo2022survey}. The task of fact-checking requires access to reliable and trustworthy knowledge sources that have been thoroughly verified. These sources serve as the foundation for retrieving evidence-based information. These include a diverse array of textual sources such as encyclopedia articles, policy documents, verified news articles, and scientific journals, which offer rich information for verifying claims \cite{Lazarski2021nlpfact, thorne2018automated}. Additionally, knowledge graphs or fact databases provide structured canonical information about the world, though their limitations must be considered, as not all facts may be present in them. Social media and online content analysis offer insights into the veracity of claims, especially when traditional textual or structured knowledge sources are unavailable. \\

% Lastly, for fact verification, either binary classification using supported/refuted labels or Multi-class labels are utilized \cite{guo2022survey, thorne2018automated}. The latter mimicking journalistic fact-checking practices, to include more fine-grained classification schemes or indicate when not enough information could be retrieved. 
% Automated fact-checking primarily relies on supervised text classification methods, often using labelled data from fact-checking agencies \cite{thorne2018automated}. While effective for some tasks, it lacks the broader world knowledge necessary for comprehensive fact-checking. Other approaches include network analysis, Recognizing Textual Entailment, and distant relation extraction. Speaker profiling, such as analyzing the credit history of claim originators, can enhance accuracy but raises ethical concerns. To further improve the verdict interpretations, the justification procedure is important \cite{guo2022survey}. Particularly in automated fact-checking where black-box models lack transparency. Strategies include highlighting salient evidence, designing understandable decision-making processes, and generating textual explanations. 

% \section{Efficient Retrieval Mechanisms} \label{sec:compression}
% Efficient Retrieval Mechanisms themselves play a crucial role in not only major web search engines and recommendation systems but also Natural Language Processing tasks \cite{baranchuk2018revisiting, wei2022}. With the rise of the Web and the development of early search engines, early approaches concerned that of inverted indexes, which efficiently stored term-document relationships, enabling rapid retrieval of documents containing specific terms. Examples are TF-IDF and BM25. However inverted indexes only retrieve text-like keywords, but do not include semantic recognition. Another approach was that of the Vector Space Model, which represented documents and queries as vectors in a high-dimensional space and used similarity measures like cosine similarity for ranking documents by relevance. A notable evolution in retrieval mechanisms is observed in Dense Retrieval, where dense encodings replace traditional methods \cite{karpukhin2020dense}. Dense encodings, being learnable through adjustable embedding functions, offer enhanced flexibility for task-specific representations. \\

% With the exponential growth of digital data, the deployment of models that utilize this data tends to become extensive in size to achieve cutting-edge results, sometimes even exceeding billions of parameters \cite{bondarenko2021understanding, zhu2023survey}. This expansion brings about costly repercussions: a larger memory footprint in terms of storage space and prohibitively large energy consumption. Furthermore, traditional retrieval techniques have encountered scalability challenges leading to higher latency in inference time, particularly due to the limitations posed by the needed high-dimensional vectors, known as the dimensionality catastrophe \cite{han2023comprehensive, wei2022}. Therefore, developing techniques or algorithms to create more compact representations of data vectors is crucial \cite{bondarenko2021understanding, morozov2019unsupervised}. This does come with the possible caveat of introducing some unwanted noise to achieve compressed vectors which results in some performance degradation. The goal, therefore, is to make more efficient representations by reducing the size of data structures without compromising too much of the integrity of the information they encapsulate. This optimization not only contributes to more effective storage and retrieval of data, but also due to lesser data complexity facilitates faster processing and transmission of information \cite{zhan2021jointly, zhu2023survey}. As a solution, vector databases that employ Approximate Nearest Neighbors (ANN) search have emerged as a solution to alleviate complexity and enhance the accuracy of similarity searches for vectors \cite{han2023comprehensive, wei2022, zhao2023ann}. \\

% Early on hash-based approaches \cite{han2023comprehensive, morozov2019unsupervised}, such as Locality-Sensitive Hashing, spectral hashing deep hashing, were adapted for ANN in text retrieval. These functions aimed to map similar data points to the same bucket with high probability, enabling efficient approximate retrieval in a reduced space. For faster search, Tree-based structures, like KD-trees and Ball trees, were employed to organize text data for efficient ANN by partitioning the data space into subsets. While these two approaches achieve fast high-recall searches, these do come at the cost of being extremely expensive when handling very large-scale databases. Additionally, there are still problems concerning the lack of sufficient semantic features, slow response times, and information loss on the query side \cite{zhao2023ann, morozov2019unsupervised}. More recently, word and document embeddings (such as Word2Vec and BERT embeddings) gained prominence as these dense vector representations captured semantic information and enabled efficient ANN-based text retrieval. Metric learning techniques were applied to learn similarity metrics that better captured semantic relationships in text data, further improving the quality of ANN-based retrieval. However, to go one step further, Quantization, which is widely employed in state-of-the-art systems, can be applied as it places a significant emphasis on reducing the precision of numerical representations. This involves transforming what are mostly high-dimensional vectors into low-dimensional ones, enhancing efficiency in terms of storage and computational requirements for retrieval systems \cite{bondarenko2021understanding}. While the aforementioned performance degradation is inevitable, meticulous quantization techniques can achieve significant model compression with minimal loss \cite{zhu2023survey}. \\

% A pioneering method within Quantization involves that of Product Quantization (PQ) \cite{han2023comprehensive, jegou2011pq, morozov2019unsupervised, zhan2021jointly}. PQ is a method that encodes high-dimensional vectors by breaking them into \textit{m} smaller subvectors known as codebooks each containing \textit{k} codewords. This effectively partitions the vector space into smaller cells,  allowing for fast Euclidean distance computation through precomputed distances. A further improvement is introduced in Optimized Product Quantization (OPQ) \cite{ge2014opq} which applies orthogonal transformations to vectors as preprocessing, improving the degree of independence and overall performance. In contrast to concatenation, Non-orthogonal quantization methods like Residual Vector Quantization (RVQ), Additive Quantization (AQ), and Composite Quantization (CQ) approximate vectors as sums of codewords, boosting approximation accuracy while maintaining efficient procedures. RVQ iteratively quantizes approximation residuals from previous iterations, while AQ, though slower, offers minimal compression errors without constraints on codewords. Lastly, there is also JPQ \cite{zhan2021jointly} that tries to overcome the delayed information retrieval capabilities in existing compression methods. This is by bridging the gap in the separation between encoding and compression training that is present in the existing methods. For our work, JPQ would be the most relevant one as we intend to optimize the efficiency of the index while still achieving similar performance for evidence retrieval using the claim as the query.


% \section{Factual Consistency in LLMs}
% With the huge growth of open source and commercial Large Language Models (LLM), the adoption of these models has increased exponentially. These models deliver impressive performance on a wide range of natural language tasks \cite{openai2023gpt4,ouyang2022training,touvron2023llama}.  Hence, there has been an increase in reliance on LLMs to deliver precise information needs. However, when generating responses, LLMs tend to hallucinate and generate factually inconsistent outputs which deviate from established world knowledge \cite{azaria2023internal, ji2023, 2023arXiv230511747L}. These errors are imperceptible, as the generated text is seemingly plausible \cite{zhang2023sirens}.  The limited factual consistency frequently hinders the wide-spread acceptance of these generative LLMs in real-world applications, such as in summarizing patient information in healthcare \cite{ji2023,roit2023factually} or question answering in critical scenarios such as healthcare \cite{umapathi2023medhalt} or legal proceedings. The hallucinations raise safety concerns for said applications, the spread of misinformation and disinformation on the web to be further exacerbated, and potential privacy violations. \\

% Studies have demonstrated that LLMs have sufficient parameters to encode large volumes of information, serving as a proxy for knowledge bases \cite{knowledge_base_2, knowledge_base_1}. However, with the rate of growth of information, it is not possible for LLMs to encode factual knowledge with temporal aspects where facts change over time \cite{li2023unlocking, temporal_aspect}. To tackle these limitations, there have been growing efforts to augment LLMs with external knowledge \cite{guu2020realm,ram2023incontext,zhang2022retgen}, which in general can be divided into two broad categories:
% \begin{itemize}
%     \item The first category consists of retrieve-and-generate language models, based on Retrieval Augmented Generation (RAG) \cite{rag}, which includes a knowledge retriever and a generative model that utilizes gathered evidence for text generation tasks \cite{guu2020realm, rag}. Some further improved RAG systems involve REALM \cite{guu2020realm} and RetGen \cite{zhang2022retgen}. 
%     \item The second category involves k-Nearest Neighbor LMs, such as RETOMATON \cite{alon2022neurosymbolic} and COG \cite{lan2023copy}, which interpolate the next token probabilities of a pre-trained LM using a K-NN model \cite{khandelwal2020generalization, LiLSDSLJJL22}. The model computes the k-nearest neighbours based on the distance from the representation of the generated prefix. Overall k-NN LMs have been found to have better performance compared to the regular generative task. 
% \end{itemize}
% Besides incorporating external knowledge, some efforts have also gone to post-hoc attribution and edit methods like RARR \cite{gao2023rarr} and PURR \cite{chen2023purr}. These aim to retrieve relevant evidence for the output from an LLM followed by an editing approach to ensure the LLM-generated output is factually consistent with the evidence. \\

%  To evaluate the factual consistency of these frameworks and LLM-generated texts in general, various benchmarks and metrics have been proposed such as AtrributionScore \cite{yue2023automatic}, FactScore \cite{min2023factscore}, HaluEval benchmark collection \cite{2023arXiv230511747L} and  LLM-augmenter \cite{peng2023check}. However, a common fallacy present in existing work is that most efforts towards mitigating the hallucination problem focus on assessing the fact verification part rather than also focusing on the evidence retrieval part. 
%  Here, the aforementioned approaches are expensive as they require fine-tuning multiple large-scale models that are involved in the retriever and the generator components. This leads to the retrieved context also increasing the latency at inference time. Additionally, The system is tightly coupled, as the optimization of the generative model depends on the quality of the output (relevant documents) of the retrieval module \cite{zhang2022retgen}. These problems make the use of LLMs that leverage external knowledge susceptible to scalability issues in practical applications. Therefore to close this gap, our contributions involve extending the evaluation process to include measuring the efficiency of the fact-checking process. Consequently, this can help these systems to be more of practical value for real-world use cases.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%