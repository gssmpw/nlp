\vspace{0.5cm}
\section{Conclusion}
\label{conclusion}
%\todo {needs rewrite}
In this work, we assess how indexing potentially relevant facts from large data collections can improve fact-checking pipelines. Our experiments show that indexing only factual content coupled with index compression significantly improves pipeline efficiency with minimal performance loss, making it feasible for lower-end, CPU-focused machines and applications like live fact-checking. Future work will expand this approach by testing various settings for indexing, retrieval, and compression, using diverse claim datasets and larger corpora to strengthen the robustness and real-world applicability of fact-checking systems.
\section{Acknowledgements}
This work is partly funded by the Research Council of Norway project EXPLAIN (grant no:
337133). We acknowledge valuable
contributions from Factiverse AI team: Tobias Tykvart for Frontend, Henrik Vatndal  and
others for manual analysis (Maria Amelie,
Gaute Kokkvol, Sean Jacob, Christina Monets and
Mari Holand)
%\todo {needs rewrite}
% This research study aimed to explore the impact of indexing supporting facts from large data collection as to improve fact-checking pipelines. Existing work on the fact-checking process focuses primarily on the fact-verification part, but not on the evidence retrieval part. Here we explored three distinct fact-extraction methods to attain supporting facts, namely, the use of a claim detection model, citation extraction and a fusion of the two. Aside from methods for identifying the supporting facts, we also explored various retrieval methods for indexing these. To guide us in this research, we aimed to answer the following three research questions:
% \begin{itemize}
%     \item RQ 1: How does indexing supporting facts improve information retrieval efficiency?
%     \item RQ 2: How does indexing supporting facts affect overall pipeline efficiency and down-
% stream fact-checking performance?
%     \item RQ 3: In what ways does index compression enhance the efficiency of dense retrieval and
% fact-checking systems?
% \end{itemize}

% From our in-depth analysis, we made several observations. Incorporating supporting facts into both Sparse and Dense retrieval setups leads to significant enhancements in overall pipeline efficiency. Sparse retrieval setups experience speedups of up to around 1.5x, while Dense retrieval setups show even more substantial improvements, reaching up to approximately 20.0x with GPU-based approaches. These speedups are primarily due to the removal of the Sentence Retrieval stage, which incurs considerable latency overhead. While there may be a minor decline in performance when transitioning from Sparse to Dense retrieval, particularly in citation extraction, Fusion data often yields promising results, emphasizing the importance of amalgamating various extraction techniques. Ablation experiments on the Sentence Retrieval stage reveal its marginal contribution to performance improvement. Comparisons between original data and supporting facts data show only a slight decline in performance, showcasing that utilizing only supporting facts incurs a modest loss. Further experiments incorporating index compression substantially reduce the index size for Dense retrieval by 93\%, resulting in significant speedups, particularly for CPU-based approaches. Here a key observation to be made is that CPU-based approaches, which experience up to 10.0x the speedup, come in close to the latency times of GPU-based approaches. Thereby effectively paving the way for fact-checking pipelines to be run on lower-end machines, that do not benefit from more GPU-based methodologies but do for CPU-based ones. \\

% All these findings underscore the meaningful impact of indexing just the supporting facts on retrieval efficiency and consequently the overall pipeline efficiency, with only minimal losses in downstream fact-checking performance. With optimizing efficiency comes several implications for which real-world applications stand to benefit significantly. Facilitating faster fact-checking processes enables various organizations to verify information in real-time where timely verification is paramount. For example in journalism where news organizations need to verify the authenticity of new information before being able to publish any reports. These advancements also contribute to scalability and resource efficiency, enabling fact-checking solutions to be deployed on a wider range of devices, including those with limited computational resources. This accessibility expands the reach of fact-checking technology, empowering users to make informed decisions and navigate online content more effectively. Moreover, the conservation of resources, such as reduced energy consumption, adds an eco-friendly dimension, rendering these solutions more appealing to companies and platforms. In essence, streamlining the general fact-checking process promises to yield significant benefits, ultimately aiding in the fight against misinformation and disinformation prevalent in the online sphere.

% \section{Future Work}
% To foster continued research related to the topic of our research, we identified several limitations in our work. Additionally, we have pinpointed several areas for improvement aimed at enhancing the efficiency and performance of the fact-checking process, while also facilitating a more comprehensive analysis.

% \subsection{Limitations}
% \paragraph{Indexing granularity} In our re-ranking of the corpus data, we processed the article text at a sentence level. However, for use in the fact-checking pipeline, we concatenated sentences into single texts per article. This does come with the risk of possible topic drift due to sentences, that may be claim-worthy on their own, but not pertaining to the claim also being used as part of the retrieved relevant documents. To alleviate this, exploring alternative granularities like storing sentence-level text instead of on a per-document article basis could be beneficial. However, this may sacrifice crucial contextual information that is part of the evidence sentences. Additionally, it will increase computational complexity in creating the retrieval index. It should be noted that for dense retrieval where generating text embeddings becomes more intensive, techniques like index compression would play a more crucial role in alleviating that problem. Therefore, thorough experimentation and analysis with regard to the granularity level would make for another important step towards efficiency.

% \paragraph{Retrieval methods} In our study, we focused solely on a select few retrieval methods, thereby limiting the scope of our findings to these specific techniques. Consequently, we advocate for enhanced generalizability by delving into a broader range of Sparse and Dense retrieval methods, alongside investigating varied index compression methodologies. Moreover, we encourage the exploration of alternative retrieval approaches not covered in our analysis, which could potentially yield greater efficiency gains or mitigate performance degradation. Furthermore, regarding index compression, our experimentation was confined to the utilization of 96 subvectors. While our initial findings demonstrated considerable promise in terms of efficiency, we suggest a more comprehensive examination involving varying quantities of subvectors. This is to ascertain a deeper understanding of their impact in not only the efficiency aspect but also the performance. Lastly, we also advocate for a more in-depth evaluation of the retrieval performance as for our work we only looked at the efficiency in terms of latency.

% \paragraph{Implementation efficiency} Although the code we used for running the experiments sufficed for the research we performed. We do acknowledge that certain parts of the code could have been better optimised. Notably, the fact-checking pipeline, derived from the original HoVer work, relies on outdated dependencies. For our work, we did not update these dependencies as that would require changing their code base or even re-implementing some parts of the code which fell outside the scope of our work. Additionally, comes the fact that HoVer's fact-checking pipeline is limited in its capabilities. For example, the Claim verification part can only evaluate to binary labels and not multi-labels. In a more practical setting, a more granular level of labels can be more helpful in the case of denoting when no information can be found that supports or contradicts a statement. Lastly, the citation extraction implementation, as detailed in \autoref{sec:experiment_setup}, encountered runtime issues that could have been addressed with better optimization strategies about the model usage. Going forward, refining these aspects of the codebase could contribute to more robust and efficient research outcomes.


% \subsection{Extensions}
% \paragraph{Supporting facts extraction} To enhance efficiency while also optimizing performance in our re-ranking setup, several key improvements can be made. Firstly, upgrading the claim-detection model from an off-the-shelf solution to a custom-trained model would enable better contextual understanding within text spans, allowing for the detection of single or multiple claims within a coherent context. This improvement necessitates a robust dataset for training, presenting an opportunity for further refinement. Secondly, enhancing citation extraction beyond solely selecting sentences containing the citation symbols is crucial. This as surrounding sentences may also be part of the citation and thus contribute to enriching the extracted information. Lastly, exploring alternative fusion methods that integrate both claim detection and citation extraction, along with potentially novel approaches, can lead to more effective re-ranking strategies. By combining these enhancements, we can achieve a more refined and comprehensive system that balances efficiency with performance in our re-ranking workflow. 

% \paragraph{More comprehensive analysis} To further enhance our work, it is essential to diversify the data used in experimentation by incorporating a broader range of complex claim datasets, as well as exploring corpus data beyond Wikipedia. It's worth noting that utilizing larger corpus data sizes can yield more realistic results, as practical applications often involve datasets ranging in the hundreds of millions or even larger. This expansion will broaden the scope of our analysis and ensure the robustness and adaptability of our solutions across various contexts. Moreover, analyzing the fact-checking pipeline within a scalable real-world application would be pivotal. Our experiments, conducted on a single machine, have limitations in replicating real-world complexities. By scaling up, we can assess performance under heavier workloads, optimize settings for efficiency, and evaluate robustness to failures. This approach will provide more actionable insights for the practical deployment of fact-checking solutions.
