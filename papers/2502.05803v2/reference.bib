@inproceedings{ai2019learning,
	title        = {Learning groupwise multivariate scoring functions using deep neural networks},
	author       = {Ai, Qingyao and Wang, Xuanhui and Bruch, Sebastian and Golbandi, Nadav and Bendersky, Michael and Najork, Marc},
	year         = 2019,
	booktitle    = {Proceedings of the 2019 ACM SIGIR international conference on theory of information retrieval},
	pages        = {85--92}
}
@inproceedings{samarinas-etal-2021-improving,
    title = "Improving Evidence Retrieval for Automated Explainable Fact-Checking",
    author = "Samarinas, Chris  and
      Hsu, Wynne  and
      Lee, Mong Li",
    editor = "Sil, Avi  and
      Lin, Xi Victoria",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-demos.10",
    doi = "10.18653/v1/2021.naacl-demos.10",
    pages = "84--91",
    abstract = "Automated fact-checking on a large-scale is a challenging task that has not been studied systematically until recently. Large noisy document collections like the web or news articles make the task more difficult. We describe a three-stage automated fact-checking system, named Quin+, using evidence retrieval and selection methods. We demonstrate that using dense passage representations leads to much higher evidence recall in a noisy setting. We also propose two sentence selection approaches, an embedding-based selection using a dense retrieval model, and a sequence labeling approach for context-aware selection. Quin+ is able to verify open-domain claims using results from web search engines.",
}
@misc{johnson2017billionscalesimilaritysearchgpus,
      title={Billion-scale similarity search with GPUs}, 
      author={Jeff Johnson and Matthijs Douze and Hervé Jégou},
      year={2017},
      eprint={1702.08734},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1702.08734}, 
}
@inproceedings{aly-vlachos-2022-natural,
    title = "Natural Logic-guided Autoregressive Multi-hop Document Retrieval for Fact Verification",
    author = "Aly, Rami  and
      Vlachos, Andreas",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.411",
    doi = "10.18653/v1/2022.emnlp-main.411",
    pages = "6123--6135",
    abstract = "A key component of fact verification is the evidence retrieval, often from multiple documents. Recent approaches use dense representations and condition the retrieval of each document on the previously retrieved ones. The latter step is performed over all the documents in the collection, requiring storing their dense representations in an index, thus incurring a high memory footprint. An alternative paradigm is retrieve-and-rerank, where documents are retrieved using methods such as BM25, their sentences are reranked, and further documents are retrieved conditioned on these sentences, reducing the memory requirements. However, such approaches can be brittle as they rely on heuristics and assume hyperlinks between documents.We propose a novel retrieve-and-rerank method for multi-hop retrieval, that consists of a retriever that jointly scores documents in the knowledge source and sentences from previously retrieved documents using an autoregressive formulation and is guided by a proof system based on natural logic that dynamically terminates the retrieval process if the evidence is deemed sufficient.This method exceeds or is on par with the current state-of-the-art on FEVER, HoVer and FEVEROUS-S, while using 5 to 10 times less memory than competing systems. Evaluation on an adversarial dataset indicates improved stability of our approach compared to commonly deployed threshold-based methods. Finally, the proof system helps humans predict model decisions correctly more often than using the evidence alone.",
}
@article{multimodal_spreads_faster,
author = {Yiyi Li and Ying Xie},
title ={Is a Picture Worth a Thousand Words? An Empirical Study of Image Content and Social Media Engagement},

journal = {Journal of Marketing Research},
volume = {57},
year = {2020},
eprint = { 
    
        https://doi.org/10.1177/0022243719881113
    
    

}
,
    abstract = { Are social media posts with pictures more popular than those without? Why do pictures with certain characteristics induce higher engagement than some other pictures? Using data sets of social media posts about major airlines and sport utility vehicle brands collected from Twitter and Instagram, the authors empirically examine the influence of image content on social media engagement. After accounting for selection bias on the inclusion of image content, the authors find a significant and robust positive mere presence effect of image content on user engagement in both product categories on Twitter. They also find that high-quality and professionally shot pictures consistently lead to higher engagement on both platforms for both product categories. However, the effect of colorfulness varies by product category, while the presence of human face and image–text fit can induce higher user engagement on Twitter but not on Instagram. These findings shed light on how to improve social media engagement using image content. }
}
@misc{leonhardt2023efficientneuralrankingusing,
      title={Efficient Neural Ranking using Forward Indexes and Lightweight Encoders}, 
      author={Jurek Leonhardt and Henrik Müller and Koustav Rudra and Megha Khosla and Abhijit Anand and Avishek Anand},
      year={2023},
      eprint={2311.01263},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2311.01263}, 
}

@misc{zhan2020repbertcontextualizedtextembeddings,
      title={RepBERT: Contextualized Text Embeddings for First-Stage Retrieval}, 
      author={Jingtao Zhan and Jiaxin Mao and Yiqun Liu and Min Zhang and Shaoping Ma},
      year={2020},
      eprint={2006.15498},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2006.15498}, 
}

@article{luan-etal-2021-sparse,
    title = "Sparse, Dense, and Attentional Representations for Text Retrieval",
    author = "Luan, Yi  and
      Eisenstein, Jacob  and
      Toutanova, Kristina  and
      Collins, Michael",
    editor = "Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "9",
    year = "2021",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2021.tacl-1.20",
    pages = "329--345",
    abstract = "Dual encoders perform retrieval by encoding documents and queries into dense low-dimensional vectors, scoring each document by its inner product with the query. We investigate the capacity of this architecture relative to sparse bag-of-words models and attentional neural networks. Using both theoretical and empirical analysis, we establish connections between the encoding dimension, the margin between gold and lower-ranked documents, and the document length, suggesting limitations in the capacity of fixed-length encodings to support precise retrieval of long documents. Building on these insights, we propose a simple neural model that combines the efficiency of dual encoders with some of the expressiveness of more costly attentional architectures, and explore sparse-dense hybrids to capitalize on the precision of sparse retrieval. These models outperform strong alternatives in large-scale retrieval.",
}
@inproceedings{zheng-etal-2024-evidence,
    title = "Evidence Retrieval is almost All You Need for Fact Verification",
    author = "Zheng, Liwen  and
      Li, Chaozhuo  and
      Zhang, Xi  and
      Shang, Yu-Ming  and
      Huang, Feiran  and
      Jia, Haoran",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.551",
    doi = "10.18653/v1/2024.findings-acl.551",
    pages = "9274--9281",
    abstract = "Current fact verification methods generally follow the two-stage training paradigm: evidence retrieval and claim verification. While existing works focus on developing sophisticated claim verification modules, the fundamental importance of evidence retrieval is largely ignored. Existing approaches usually adopt the heuristic semantic similarity-based retrieval strategy, resulting in the task-irrelevant evidence and undesirable performance. In this paper, we concentrate on evidence retrieval and propose a Retrieval-Augmented Verification framework RAV, consisting of two major modules: the hybrid evidence retrieval and the joint fact verification. Hybrid evidence retrieval module incorporates an efficient retriever for preliminary pruning of candidate evidence, succeeded by a ranker that generates more precise sorting results. Under this end-to-end training paradigm, gradients from the claim verification can be back-propagated to enhance evidence selection. Experimental results on FEVER dataset demonstrate the superiority of RAV.",
}
@article{multimodal_credibility,
author = {Newman, Eryn and Garry, Maryanne and Bernstein, Daniel and Kantner, Justin and Lindsay, D},
year = {2012},
title = {Nonprobative photographs (or words) inflate truthiness},
volume = {19},
journal = {Psychonomic bulletin and review},
}

@inproceedings{political_debates,
author = {Hassan, Naeemul and Li, Chengkai and Tremayne, Mark},
title = {Detecting Check-worthy Factual Claims in Presidential Debates},
year = {2015},
publisher = {Association for Computing Machinery},
abstract = {Public figures such as politicians make claims about "facts" all the time. Journalists and citizens spend a good amount of time checking the veracity of such claims. Toward automatic fact checking, we developed tools to find check-worthy factual claims from natural language sentences. Specifically, we prepared a U.S. presidential debate dataset and built classification models to distinguish check-worthy factual claims from non-factual claims and unimportant factual claims. We also identified the most-effective features based on their impact on the classification models' accuracy.},
keywords = {text classification, fact checking, computational journalism},
location = {Melbourne, Australia},
series = {CIKM '15}
}
@misc{nakov2021automatedfactcheckingassistinghuman,
      title={Automated Fact-Checking for Assisting Human Fact-Checkers}, 
      author={Preslav Nakov and David Corney and Maram Hasanain and Firoj Alam and Tamer Elsayed and Alberto Barrón-Cedeño and Paolo Papotti and Shaden Shaar and Giovanni Da San Martino},
      year={2021},
      eprint={2103.07769},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2103.07769}, 
}
@inproceedings{efficient_inverted_index,
author = {Bruch, Sebastian and Nardini, Franco Maria and Rulli, Cosimo and Venturini, Rossano},
title = {Efficient Inverted Indexes for Approximate Retrieval over Learned Sparse Representations},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657769},
doi = {10.1145/3626772.3657769},
abstract = {Learned sparse representations form an attractive class of contextual embeddings for text retrieval. That is so because they are effective models of relevance and are interpretable by design. Despite their apparent compatibility with inverted indexes, however, retrieval over sparse embeddings remains challenging. That is due to the distributional differences between learned embeddings and term frequency-based lexical models of relevance such as BM25. Recognizing this challenge, a great deal of research has gone into, among other things, designing retrieval algorithms tailored to the properties of learned sparse representations, including approximate retrieval systems. In fact, this task featured prominently in the latest BigANN Challenge at NeurIPS 2023, where approximate algorithms were evaluated on a large benchmark dataset by throughput and recall. In this work, we propose a novel organization of the inverted index that enables fast yet effective approximate retrieval over learned sparse embeddings. Our approach organizes inverted lists into geometrically-cohesive blocks, each equipped with a summary vector. During query processing, we quickly determine if a block must be evaluated using the summaries. As we show experimentally, single-threaded query processing using our method, Seismic, reaches sub-millisecond per-query latency on various sparse embeddings of the MS MARCO dataset while maintaining high recall. Our results indicate that Seismic is one to two orders of magnitude faster than state-of-the-art inverted index-based solutions and further outperforms the winning (graph-based) submissions to the BigANN Challenge by a significant margin.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {152–162},
numpages = {11},
keywords = {inverted index, learned sparse representations, maximum inner product search},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@inproceedings{dictionary_pruning_compression,
author = {Tong, Jiancong and Wirth, Anthony and Zobel, Justin},
title = {Principled dictionary pruning for low-memory corpus compression},
year = {2014},
isbn = {9781450322577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600428.2609576},
doi = {10.1145/2600428.2609576},
abstract = {Compression of collections, such as text databases, can both reduce space consumption and increase retrieval efficiency, through better caching and better exploitation of the memory hierarchy. A promising technique is relative Lempel-Ziv coding, in which a sample of material from the collection serves as a static dictionary; in previous work, this method demonstrated extremely fast decoding and good compression ratios, while allowing random access to individual items. However, there is a trade-off between dictionary size and compression ratio, motivating the search for a compact, yet similarly effective, dictionary. In previous work it was observed that, since the dictionary is generated by sampling, some of it (selected substrings) may be discarded with little loss in compression. Unfortunately, simple dictionary pruning approaches are ineffective. We develop a formal model of our approach, based on generating an optimal dictionary for a given collection within a memory bound. We generate measures for identification of low-value substrings in the dictionary, and show on a variety of sizes of text collection that halving the dictionary size leads to only marginal loss in compression ratio. This is a dramatic improvement on previous approaches.},
booktitle = {Proceedings of the 37th International ACM SIGIR Conference on Research \& Development in Information Retrieval},
pages = {283–292},
numpages = {10},
keywords = {string algorithms, retrieval efficiency, optimization, corpus compression},
location = {Gold Coast, Queensland, Australia},
series = {SIGIR '14}
}
@inproceedings{schlichtkrull-etal-2021-joint,
    title = "Joint Verification and Reranking for Open Fact Checking Over Tables",
    author = "Schlichtkrull, Michael Sejr  and
      Karpukhin, Vladimir  and
      Oguz, Barlas  and
      Lewis, Mike  and
      Yih, Wen-tau  and
      Riedel, Sebastian",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.529",
    doi = "10.18653/v1/2021.acl-long.529",
    pages = "6787--6799",
    abstract = "Structured information is an important knowledge source for automatic verification of factual claims. Nevertheless, the majority of existing research into this task has focused on textual data, and the few recent inquiries into structured data have been for the closed-domain setting where appropriate evidence for each claim is assumed to have already been retrieved. In this paper, we investigate verification over structured data in the open-domain setting, introducing a joint reranking-and-verification model which fuses evidence documents in the verification component. Our open-domain model achieves performance comparable to the closed-domain state-of-the-art on the TabFact dataset, and demonstrates performance gains from the inclusion of multiple tables as well as a significant improvement over a heuristic retrieval baseline.",
}
@misc{schuster2021vitamincrobustfact,
      title={Get Your Vitamin C! Robust Fact Verification with Contrastive Evidence}, 
      author={Tal Schuster and Adam Fisch and Regina Barzilay},
      year={2021},
      eprint={2103.08541},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2103.08541}, 
}
@inproceedings{yin-roth-2018-twowingos,
    title = "{T}wo{W}ing{OS}: A Two-Wing Optimization Strategy for Evidential Claim Verification",
    author = "Yin, Wenpeng  and
      Roth, Dan",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1010",
    doi = "10.18653/v1/D18-1010",
    pages = "105--114",
    abstract = "Determining whether a given claim is supported by evidence is a fundamental NLP problem that is best modeled as Textual Entailment. However, given a large collection of text, finding evidence that could support or refute a given claim is a challenge in itself, amplified by the fact that different evidence might be needed to support or refute a claim. Nevertheless, most prior work decouples evidence finding from determining the truth value of the claim given the evidence. We propose to consider these two aspects jointly. We develop TwoWingOS (two-wing optimization strategy), a system that, while identifying appropriate evidence for a claim, also determines whether or not the claim is supported by the evidence. Given the claim, TwoWingOS attempts to identify a subset of the evidence candidates; given the predicted evidence, it then attempts to determine the truth value of the corresponding claim entailment problem. We treat this problem as coupled optimization problems, training a joint model for it. TwoWingOS offers two advantages: (i) Unlike pipeline systems it facilitates flexible-size evidence set, and (ii) Joint training improves both the claim entailment and the evidence identification. Experiments on a benchmark dataset show state-of-the-art performance.",
}
@inproceedings{programfc,
    title = "Fact-Checking Complex Claims with Program-Guided Reasoning",
    author = "Pan, Liangming  and
      Wu, Xiaobao  and
      Lu, Xinyuan  and
      Luu, Anh Tuan  and
      Wang, William Yang  and
      Kan, Min-Yen  and
      Nakov, Preslav",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.386",
    doi = "10.18653/v1/2023.acl-long.386",
    pages = "6981--7004",
    abstract = "Fact-checking real-world claims often requires collecting multiple pieces of evidence and applying complex multi-step reasoning. In this paper, we present Program-Guided Fact-Checking (ProgramFC), a novel fact-checking model that decomposes complex claims into simpler sub-tasks that can be solved using a shared library of specialized functions. We first leverage the in-context learning ability of large language models to generate reasoning programs to guide the verification process. Afterward, we execute the program by delegating each sub-task to the corresponding sub-task handler. This process makes our model both explanatory and data-efficient, providing clear explanations of its reasoning process and requiring minimal training data. We evaluate ProgramFC on two challenging fact-checking datasets and show that it outperforms seven fact-checking baselines across different settings of evidence availability, with explicit output programs that benefit human debugging. Our codes and data are publicly available at \url{https://github.com/mbzuai-nlp/ProgramFC}.",
}


@inproceedings{read_twice,
author = {Hu, Xuming and Hong, Zhaochen and Guo, Zhijiang and Wen, Lijie and Yu, Philip},
title = {Read it Twice: Towards Faithfully Interpretable Fact Verification by Revisiting Evidence},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3592049},
doi = {10.1145/3539618.3592049},
abstract = {Real-world fact verification task aims to verify the factuality of a claim by retrieving evidence from the source document. The quality of the retrieved evidence plays an important role in claim verification. Ideally, the retrieved evidence should be faithful (reflecting the model's decision-making process in claim verification) and plausible (convincing to humans), and can improve the accuracy of verification task. Although existing approaches leverage the similarity measure of semantic or surface form between claims and documents to retrieve evidence, they all rely on certain heuristics that prevent them from satisfying all three requirements. In light of this, we propose a fact verification model named ReRead to retrieve evidence and verify claim that: (1) Train the evidence retriever to obtain interpretable evidence (i.e., faithfulness and plausibility criteria); (2) Train the claim verifier to revisit the evidence retrieved by the optimized evidence retriever to improve the accuracy. The proposed system is able to achieve significant improvements upon best-reported models under different settings.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2319–2323},
numpages = {5},
keywords = {automated fact-checking, claim verification, evidence retrieval, latent variable models, real-world systems},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{tan2016improved,
  title     = {Improved Representation Learning for Question Answer Matching},
  author    = {Tan, Ming  and
               dos Santos, Cicero  and
               Xiang, Bing  and
               Zhou, Bowen},
  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = aug,
  year      = {2016},
  address   = {Berlin, Germany},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/P16-1044},
  doi       = {10.18653/v1/P16-1044},
  pages     = {464--473}
}
@article{jagerman2023query,
  title={Query Expansion by Prompting Large Language Models},
  author={Jagerman, Rolf and Zhuang, Honglei and Qin, Zhen and Wang, Xuanhui and Bendersky, Michael},
  journal={arXiv preprint arXiv:2305.03653},
  year={2023}
}

@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}

@article{lin2010rank,
  title={Rank aggregation methods},
  author={Lin, Shili},
  journal={Wiley Interdisciplinary Reviews: Computational Statistics},
  volume={2},
  number={5},
  pages={555--570},
  year={2010},
  publisher={Wiley Online Library}
}
@inproceedings{interleaving,
author = {Radlinski, Filip and Craswell, Nick},
title = {Optimized Interleaving for Online Retrieval Evaluation},
year = {2013},
isbn = {9781450318693},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2433396.2433429},
doi = {10.1145/2433396.2433429},
abstract = {Interleaving is an online evaluation technique for comparing the relative quality of information retrieval functions by combining their result lists and tracking clicks. A sequence of such algorithms have been proposed, each being shown to address problems in earlier algorithms. In this paper, we formalize and generalize this process, while introducing a formal model: We identify a set of desirable properties for interleaving, then show that an interleaving algorithm can be obtained as the solution to an optimization problem within those constraints. Our approach makes explicit the parameters of the algorithm, as well as assumptions about user behavior. Further, we show that our approach leads to an unbiased and more efficient interleaving algorithm than any previous approach, using a novel log-based analysis of user search behavior.},
booktitle = {Proceedings of the Sixth ACM International Conference on Web Search and Data Mining},
pages = {245–254},
numpages = {10},
keywords = {interleaving, web search, evaluation},
location = {Rome, Italy},
series = {WSDM '13}
}
@inproceedings{ir_need_qpp,
author = {Zendel, Oleg and Shtok, Anna and Raiber, Fiana and Kurland, Oren and Culpepper, J. Shane},
title = {Information Needs, Queries, and Query Performance Prediction},
year = {2019},
isbn = {9781450361729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331184.3331253},
doi = {10.1145/3331184.3331253},
abstract = {The query performance prediction (QPP) task is to estimate the effectiveness of a search performed in response to a query with no relevance judgments. Existing QPP methods do not account for the effectiveness of a query in representing the underlying information need. We demonstrate the far-reaching implications of this reality using standard TREC-based evaluation of QPP methods: their relative prediction quality patterns vary with respect to the effectiveness of queries used to represent the information needs. Motivated by our findings, we revise the basic probabilistic formulation of the QPP task by accounting for the information need and its connection to the query. We further explore this connection by proposing a novel QPP approach that utilizes information about a set of queries representing the same information need. Predictors instantiated from our approach using a wide variety of existing QPP methods post prediction quality that substantially transcends that of applying these methods, as is standard, using a single query representing the information need. Additional in-depth empirical analysis of different aspects of our approach further attests to the crucial role of query effectiveness in QPP.},
booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {395–404},
numpages = {10},
keywords = {query variations, query-performance prediction, reference queries},
location = {Paris, France},
series = {SIGIR'19}
}
@inproceedings{query_perf,
author = {Zendel, Oleg and Shtok, Anna and Raiber, Fiana and Kurland, Oren and Culpepper, J. Shane},
title = {Information Needs, Queries, and Query Performance Prediction},
year = {2019},
isbn = {9781450361729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331184.3331253},
doi = {10.1145/3331184.3331253},
abstract = {The query performance prediction (QPP) task is to estimate the effectiveness of a search performed in response to a query with no relevance judgments. Existing QPP methods do not account for the effectiveness of a query in representing the underlying information need. We demonstrate the far-reaching implications of this reality using standard TREC-based evaluation of QPP methods: their relative prediction quality patterns vary with respect to the effectiveness of queries used to represent the information needs. Motivated by our findings, we revise the basic probabilistic formulation of the QPP task by accounting for the information need and its connection to the query. We further explore this connection by proposing a novel QPP approach that utilizes information about a set of queries representing the same information need. Predictors instantiated from our approach using a wide variety of existing QPP methods post prediction quality that substantially transcends that of applying these methods, as is standard, using a single query representing the information need. Additional in-depth empirical analysis of different aspects of our approach further attests to the crucial role of query effectiveness in QPP.},
booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {395–404},
numpages = {10},
keywords = {reference queries, query variations, query-performance prediction},
location = {Paris, France},
series = {SIGIR'19}
}
@inproceedings{query_difficult,
author = {Carmel, David and Yom-Tov, Elad and Darlow, Adam and Pelleg, Dan},
title = {What Makes a Query Difficult?},
year = {2006},
isbn = {1595933697},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1148170.1148238},
doi = {10.1145/1148170.1148238},
abstract = {This work tries to answer the question of what makes a query difficult. It addresses a novel model that captures the main components of a topic and the relationship between those components and topic difficulty. The three components of a topic are the textual expression describing the information need (the query or queries), the set of documents relevant to the topic (the Qrels), and the entire collection of documents. We show experimentally that topic difficulty strongly depends on the distances between these components. In the absence of knowledge about one of the model components, the model is still useful by approximating the missing component based on the other components. We demonstrate the applicability of the difficulty model for several uses such as predicting query difficulty, predicting the number of topic aspects expected to be covered by the search results, and analyzing the findability of a specific domain.},
booktitle = {Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {390–397},
numpages = {8},
keywords = {query difficulty},
location = {Seattle, Washington, USA},
series = {SIGIR '06}
}

@inproceedings{estimating_query_difficulty,
author = {Carmel, David and Yom-Tov, Elad},
title = {Estimating the Query Difficulty for Information Retrieval},
year = {2010},
isbn = {9781450301534},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1835449.1835683},
doi = {10.1145/1835449.1835683},
abstract = {Many information retrieval (IR) systems suffer from a radical variance in performance when responding to users' queries. Even for systems that succeed very well on average, the quality of results returned for some of the queries is poor. Thus, it is desirable that IR systems will be able to identify "difficult" queries in order to handle them properly. Understanding why some queries are inherently more difficult than others is essential for IR, and a good answer to this important question will help search engines to reduce the variance in performance, hence better servicing their customer needs.The high variability in query performance has driven a new research direction in the IR field on estimating the expected quality of the search results, i.e. the query difficulty, when no relevance feedback is given. Estimating the query difficulty is a significant challenge due to the numerous factors that impact retrieval performance. Many prediction methods have been proposed recently. However, as many researchers observed, the prediction quality of state-of-the-art predictors is still too low to be widely used by IR applications. The low prediction quality is due to the complexity of the task, which involves factors such as query ambiguity, missing content, and vocabulary mismatch.The goal of this tutorial is to expose participants to the current research on query performance prediction (also known as query difficulty estimation). Participants will become familiar with states-of-the-art performance prediction methods, and with common evaluation methodologies for prediction quality. We will discuss the reasons that cause search engines to fail for some of the queries, and provide an overview of several approaches for estimating query difficulty. We then describe common methodologies for evaluating the prediction quality of those estimators, and some experiments conducted recently with their prediction quality, as measured over several TREC benchmarks. We will cover a few potential applications that can utilize query difficulty estimators by handling each query individually and selectively based on its estimated difficulty. Finally we will summarize with a discussion on open issues and challenges in the field.},
booktitle = {Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {911},
numpages = {1},
keywords = {performance prediction, query difficulty estimation, retrieval robustness},
location = {Geneva, Switzerland},
series = {SIGIR '10}
}

@inproceedings{ambig_query_identity_length,
author = {Sanderson, Mark},
title = {Ambiguous Queries: Test Collections Need More Sense},
year = {2008},
isbn = {9781605581644},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390334.1390420},
doi = {10.1145/1390334.1390420},
abstract = {Although there are many papers examining ambiguity in Information Retrieval, this paper shows that there is a whole class of ambiguous word that past research has barely explored. It is shown that the class is more ambiguous than other word types and is commonly used in queries. The lack of test collections containing ambiguous queries is highlighted and a method for creating collections from existing resources is described. Tests using the new collection show the impact of query ambiguity on an IR system: it is shown that conventional systems are incapable of dealing effectively with such queries and that current assumptions about how to improve search effectiveness do not hold when searching on this common query type.},
booktitle = {Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {499–506},
numpages = {8},
keywords = {ambiguous queries, test collections, diversity},
location = {Singapore, Singapore},
series = {SIGIR '08}
}

@article{li2020parade,
  title={Parade: Passage representation aggregation for document reranking},
  author={Li, Canjia and Yates, Andrew and MacAvaney, Sean and He, Ben and Sun, Yingfei},
  journal={ACM Transactions on Information Systems},
  year={2020},
  publisher={ACM New York, NY}
}
@article{clark2020electra,
  title={Electra: Pre-training text encoders as discriminators rather than generators},
  author={Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and Manning, Christopher D},
  journal={arXiv preprint arXiv:2003.10555},
  year={2020}
}
@misc{paired_significance_test,
  key="paired_significance_test",
  title={{Pairwise t-test on TREC Run Files}},
  author={Luke Gallagher},
  year={2019},
  howpublished={\url{https://github.com/lgrz/pairwise-ttest/}}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}
@inproceedings{lavrenko2017relevance,
  title={Relevance-based language models},
  author={Lavrenko, Victor and Croft, W Bruce},
  booktitle={ACM SIGIR Forum},
  volume={51},
  number={2},
  pages={260--267},
  year={2017},
  organization={ACM New York, NY, USA}
}
@inproceedings{alammar-2021-ecco,
	title        = {Ecco: An Open Source Library for the Explainability of Transformer Language Models},
	author       = {Alammar, J},
	year         = 2021,
	booktitle    = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations},
	publisher    = {Association for Computational Linguistics}
}
@article{alayrac:2022:flamingo,
	title        = {Flamingo: a visual language model for few-shot learning},
	author       = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and others},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2204.14198}
}`
@inproceedings{albuquerque2018learning,
	title        = {Learning to rank with deep autoencoder features},
	author       = {Albuquerque, Alberto and Amador, Tiago and Ferreira, Renato and Veloso, Adriano and Ziviani, Nivio},
	year         = 2018,
	booktitle    = {2018 International Joint Conference on Neural Networks (IJCNN)},
	pages        = {1--8},
	organization = {IEEE}
}
@inproceedings{aliannejadi2019asking,
	title        = {Asking clarifying questions in open-domain information-seeking conversations},
	author       = {Aliannejadi, Mohammad and Zamani, Hamed and Crestani, Fabio and Croft, W Bruce},
	year         = 2019,
	booktitle    = {Proceedings of the 42nd international acm sigir conference on research and development in information retrieval},
	pages        = {475--484}
}

@misc{ma2023query,
      title={Query Rewriting for Retrieval-Augmented Large Language Models}, 
      author={Xinbei Ma and Yeyun Gong and Pengcheng He and Hai Zhao and Nan Duan},
      year={2023},
      eprint={2305.14283},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhu2023large,
      title={Large Language Models for Information Retrieval: A Survey}, 
      author={Yutao Zhu and Huaying Yuan and Shuting Wang and Jiongnan Liu and Wenhan Liu and Chenlong Deng and Zhicheng Dou and Ji-Rong Wen},
      year={2023},
      eprint={2308.07107},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{anantha2020open,
	title        = {Open-domain question answering goes conversational via question rewriting},
	author       = {Anantha, Raviteja and Vakulenko, Svitlana and Tu, Zhucheng and Longpre, Shayne and Pulman, Stephen and Chappidi, Srinivas},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2010.04898}
}
@inproceedings{approxNDCG,
	title        = {Revisiting Approximate Metric Optimization in the Age of Deep Neural Networks},
	author       = {Sebastian Bruch and Masrour Zoghi and Michael Bendersky and Marc Najork},
	year         = 2019,
	booktitle    = {Proceedings of the 42nd International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval, {SIGIR} 2019, Paris, France, July 21-25, 2019},
	publisher    = {{ACM}},
	pages        = {1241--1244},
	doi          = {10.1145/3331184.3331347},
	url          = {https://doi.org/10.1145/3331184.3331347},
	bibsource    = {dblp computer science bibliography, https://dblp.org},
	biburl       = {https://dblp.org/rec/conf/sigir/BruchZBN19.bib},
	editor       = {Benjamin Piwowarski and Max Chevalier and {\'{E}}ric Gaussier and Yoelle Maarek and Jian{-}Yun Nie and Falk Scholer},
	timestamp    = {Tue, 29 Dec 2020 18:37:26 +0100}
}
@inproceedings{arabzadeh2021matches,
	title        = {Matches Made in Heaven: Toolkit and Large-Scale Datasets for Supervised Query Reformulation},
	author       = {Arabzadeh, Negar and Bigdeli, Amin and Seyedsalehi, Shirin and Zihayat, Morteza and Bagheri, Ebrahim},
	year         = 2021,
	booktitle    = {Proceedings of the 30th ACM International Conference on Information and Knowledge Management},
	pages        = {4417--4425}
}
@inproceedings{arabzadeh2021matches,
  author    = {Arabzadeh, Negar and Bigdeli, Amin and Seyedsalehi, Shirin and Zihayat, Morteza and Bagheri, Ebrahim},
  year      = {2021},
  booktitle = {Proceedings of the 30th ACM International Conference on Information and Knowledge Management},
  pages     = {4417--4425},
  title     = {Matches Made in Heaven: Toolkit and Large-Scale Datasets for Supervised Query Reformulation}
}
@inproceedings{arapakis2014impact,
	title        = {Impact of response latency on user behavior in web search},
	author       = {Arapakis, Ioannis and Bai, Xiao and Cambazoglu, B Barla},
	year         = 2014,
	booktitle    = {Proceedings of the 37th international ACM SIGIR conference on Research \& development in information retrieval},
	pages        = {103--112}
}
@inproceedings{arik2021tabnet,
	title        = {Tabnet: Attentive interpretable tabular learning},
	author       = {Arik, Sercan {\"O} and Pfister, Tomas},
	year         = 2021,
	booktitle    = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = 35,
	number       = 8,
	pages        = {6679--6687}
}
@inproceedings{atanasova-etal-2020-generating-fact,
	title        = {Generating Fact Checking Explanations},
	author       = {Atanasova, Pepa  and Simonsen, Jakob Grue  and Lioma, Christina  and Augenstein, Isabelle},
	year         = 2020,
	month        = jul,
	booktitle    = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {7352--7364},
	doi          = {10.18653/v1/2020.acl-main.656},
	url          = {https://aclanthology.org/2020.acl-main.656},
	abstract     = {Most existing work on automated fact checking is concerned with predicting the veracity of claims based on metadata, social network spread, language used in claims, and, more recently, evidence supporting or denying claims. A crucial piece of the puzzle that is still missing is to understand how to automate the most elaborate part of the process {--} generating justifications for verdicts on claims. This paper provides the first study of how these explanations can be generated automatically based on available claim context, and how this task can be modelled jointly with veracity prediction. Our results indicate that optimising both objectives at the same time, rather than training them separately, improves the performance of a fact checking system. The results of a manual evaluation further suggest that the informativeness, coverage and overall quality of the generated explanations are also improved in the multi-task model.}
}

@inproceedings{selective_query_expansion_1,
author = {Cronen-Townsend, Steve and Zhou, Yun and Croft, W. Bruce},
title = {A Framework for Selective Query Expansion},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031220},
doi = {10.1145/1031171.1031220},
abstract = {Query expansion is a well-known technique that has been shown to improve <i>average</i> retrieval performance. This technique has not been used in many operational systems because of the fact that it can greatly degrade the performance of some individual queries. We show how comparison between language models of the unexpanded and expanded retrieval results can be used to predict when the expanded retrieval has strayed from the original sense of the query. In these cases, the unexpanded results are used while the expanded results are used in the remaining cases (where such straying is not detected). We evaluate this method on a wide variety of TREC collections.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {236–237},
numpages = {2},
keywords = {language modeling, query expansion, clarity},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@InProceedings{selective_query_expansion_2,
author="Amati, Giambattista
and Carpineto, Claudio
and Romano, Giovanni",
editor="McDonald, Sharon
and Tait, John",
title="Query Difficulty, Robustness, and Selective Application of Query Expansion",
booktitle="Advances in Information Retrieval",
year="2004",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="127--137",
abstract="There is increasing interest in improving the robustness of IR systems, i.e. their effectiveness on difficult queries. A system is robust when it achieves both a high Mean Average Precision (MAP) value for the entire set of topics and a significant MAP value over its worst X topics (MAP(X)). It is a well known fact that Query Expansion (QE) increases global MAP but hurts the performance on the worst topics. A selective application of QE would thus be a natural answer to obtain a more robust retrieval system.",
isbn="978-3-540-24752-4"
}

@inproceedings{qpp_deep,
author = {Datta, Suchana and Ganguly, Debasis and Greene, Derek and Mitra, Mandar},
title = {Deep-QPP: A Pairwise Interaction-Based Deep Learning Model for Supervised Query Performance Prediction},
year = {2022},
isbn = {9781450391320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488560.3498491},
doi = {10.1145/3488560.3498491},
abstract = {Motivated by the recent success of end-to-end deep neural models for ranking tasks, we present here a supervised end-to-end neural approach for query performance prediction (QPP). In contrast to unsupervised approaches that rely on various statistics of document score distributions, our approach is entirely data-driven. Further, in contrast to weakly supervised approaches, our method also does not rely on the outputs from different QPP estimators. In particular, our model leverages information from the semantic interactions between the terms of a query and those in the top-documents retrieved with it. The architecture of the model comprises multiple layers of 2D convolution filters followed by a feed-forward layer of parameters. Experiments on standard test collections demonstrate that our proposed supervised approach outperforms other state-of-the-art supervised and unsupervised approaches.},
booktitle = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
pages = {201–209},
numpages = {9},
keywords = {supervised query performance prediction, convolutional neural networks, interaction-based models},
location = {Virtual Event, AZ, USA},
series = {WSDM '22}
}
@article{selective_query_expansion_3,
author = {Mothe, Josiane and Ullah, Md. Zia},
title = {Selective Query Processing: A Risk-Sensitive Selection of Search Configurations},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3608474},
doi = {10.1145/3608474},
abstract = {In information retrieval systems, search parameters are optimized to ensure high effectiveness based on a set of past searches, and these optimized parameters are then used as the search configuration for all subsequent queries. A better approach, however, would be to adapt the parameters to fit the query at hand. Selective query expansion is one such an approach, in which the system decides automatically whether or not to expand the query, resulting in two possible search configurations. This approach was extended recently to include many other parameters, leading to many possible search configurations where the system automatically selects the best configuration on a per-query basis. One problem with this approach is the system training, which requires evaluation of each training query with every possible configuration. In real-world systems, so many parameters and possible values must be evaluated that this approach is impractical, especially when the system must be updated frequently, as is the case for commercial search engines. In general, the more configurations, the greater the effectiveness when configuration selection is appropriate but also the greater the risk of decreasing effectiveness in the case of an inappropriate configuration selection. To determine the ideal configurations to be used for each query in real-world systems, we have developed a method in which a limited number of possible configurations are pre-selected, then used in a meta-search engine that decides the best search configuration for each query. We define a risk-sensitive approach for configuration pre-selection that considers the risk-reward tradeoff between the number of configurations kept and system effectiveness. We define two alternative risk functions to apply to different goals. For final configuration selection, the decision is based on query feature similarities. We compare two alternative risk functions on two query types (ad hoc and diversity) and compare these to more sophisticated machine learning based methods. We find that a relatively small number of configurations (20) selected by our risk-sensitive model is sufficient to obtain results close to the best achievable results for each query. Effectiveness is increased by about 15\% according to the P@10 and nDCG@10 evaluation metrics when compared to traditional grid search using a single configuration and by about 20\% when compared to learning to rank documents. Our risk-sensitive approach works for both diversity- and ad hoc oriented searches. Moreover, the similarity-based selection method outperforms the more sophisticated approaches. Thus, we demonstrate the feasibility of developing per-query information retrieval systems, which will guide future research in this direction.},
journal = {ACM Trans. Inf. Syst.},
month = {aug},
articleno = {31},
numpages = {35},
keywords = {search engine parameters, risk-sensitive systems, Information retrieval, query driven parameterisation, adaptive information retrieval, learning to rank}
}



@inproceedings{ms_marco_chameleons,
author = {Arabzadeh, Negar and Mitra, Bhaskar and Bagheri, Ebrahim},
title = {MS MARCO Chameleons: Challenging the MS MARCO Leaderboard with Extremely Obstinate Queries},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482011},
doi = {10.1145/3459637.3482011},
abstract = {During the recent years and with the growing influence of neural architectures, tasks such as ad hoc retrieval have witnessed an impressive improvement in performance. For instance, the performance of rankers on the passage retrieval task on the MS MARCO dataset has improved by an order of magnitude in less than two years. In this paper, we go beyond the overall performance of the state of the art rankers and empirically study their performance from a finer-grained perspective. We find that while neural rankers have been able to consistently improve performance, this has been in part thanks to a specific set of queries from within the larger query set. We systematically show that there are subsets of queries that are difficult for each and every one of the neural rankers, which we refer to as obstinate queries. We show the obstinate queries are similar to easier queries in terms of their number of available relevant judgement documents and the length of the query itself but they are extremely more difficult to satisfy by existing rankers. Furthermore, we observe that query reformulation methods cannot help these queries. On this basis, we present three datasets derived from the MS MARCO Dev set, called the MS MARCO Chameleon datasets. We believe that the next breakthrough in performance would need to necessarily consider the queries in the MS MARCO Chameleons, as such, propose that a well-rounded evaluation strategy for any new ranker would need to include performance measures on both the overall MS MARCO dataset as well as the proposed MS MARCO Chameleon datasets.},
booktitle = {Proceedings of the 30th ACM International Conference on Information \& Knowledge Management},
pages = {4426–4435},
numpages = {10},
keywords = {query reformulation, query difficulty, information retrieval},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}
@inproceedings{augenstein-etal-2019-multifc,
	title        = {{M}ulti{FC}: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims},
	author       = {Augenstein, Isabelle  and Lioma, Christina  and Wang, Dongsheng  and Chaves Lima, Lucas  and Hansen, Casper  and Hansen, Christian  and Simonsen, Jakob Grue},
	year         = 2019,
	month        = nov,
	booktitle    = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
	publisher    = {Association for Computational Linguistics},
	address      = {Hong Kong, China},
	pages        = {4685--4697},
	doi          = {10.18653/v1/D19-1475},
	url          = {https://aclanthology.org/D19-1475},
	abstract     = {We contribute the largest publicly available dataset of naturally occurring factual claims for the purpose of automatic claim verification. It is collected from 26 fact checking websites in English, paired with textual sources and rich metadata, and labelled for veracity by human expert journalists. We present an in-depth analysis of the dataset, highlighting characteristics and challenges. Further, we present results for automatic veracity prediction, both with established baselines and with a novel method for joint ranking of evidence pages and predicting veracity that outperforms all baselines. Significant performance increases are achieved by encoding evidence, and by modelling metadata. Our best-performing model achieves a Macro F1 of 49.2{\%}, showing that this is a challenging testbed for claim veracity prediction.}
}
@inproceedings{autoprompt,
	title        = {{A}uto{P}rompt: {E}liciting {K}nowledge from {L}anguage {M}odels with {A}utomatically {G}enerated {P}rompts},
	author       = {Shin, Taylor  and Razeghi, Yasaman  and Logan IV, Robert L.  and Wallace, Eric  and Singh, Sameer},
	year         = 2020,
	month        = nov,
	booktitle    = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {4222--4235},
	doi          = {10.18653/v1/2020.emnlp-main.346},
	url          = {https://aclanthology.org/2020.emnlp-main.346},
	abstract     = {The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning.}
}
@inproceedings{baeza2004query,
	title        = {Query Recommendation Using Query Logs in Search Engines.},
	author       = {Baeza-Yates, Ricardo A and Hurtado, Carlos A and Mendoza, Marcelo and others},
	year         = 2004,
	booktitle    = {EDBT workshops},
	volume       = 3268,
	number       = 2005,
	pages        = {588--596},
	organization = {Springer}
}
@inproceedings{baeza2006intention,
	title        = {The intention behind web queries},
	author       = {Baeza-Yates, Ricardo and Calder{\'o}n-Benavides, Liliana and Gonz{\'a}lez-Caro, Cristina},
	year         = 2006,
	booktitle    = {String Processing and Information Retrieval: 13th International Conference, SPIRE 2006, Glasgow, UK, October 11-13, 2006. Proceedings 13},
	pages        = {98--109},
	organization = {Springer}
}
@article{bai2019impact,
	title        = {Impact of response latency on sponsored search},
	author       = {Bai, Xiao and Cambazoglu, B Barla},
	year         = 2019,
	journal      = {Information Processing \& Management},
	publisher    = {Elsevier},
	volume       = 56,
	number       = 1,
	pages        = {110--129}
}
@inproceedings{balin2019concrete,
	title        = {Concrete autoencoders: Differentiable feature selection and reconstruction},
	author       = {Bal{\i}n, Muhammed Fatih and Abid, Abubakar and Zou, James},
	year         = 2019,
	booktitle    = {International conference on machine learning},
	pages        = {444--453},
	organization = {PMLR}
}
@inproceedings{barreda2015unconscious,
	title        = {Unconscious physiological effects of search latency on users and their click behaviour},
	author       = {Barreda-{\'A}ngeles, Miguel and Arapakis, Ioannis and Bai, Xiao and Cambazoglu, B Barla and Pereda-Ba{\~n}os, Alexandre},
	year         = 2015,
	booktitle    = {Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages        = {203--212}
}
@inproceedings{beitzel2005improving,
	title        = {Improving automatic query classification via semi-supervised learning},
	author       = {Beitzel, Steven M and Jensen, Eric C and Frieder, Ophir and Lewis, David D and Chowdhury, Abdur and Kolcz, Aleksander},
	year         = 2005,
	booktitle    = {Fifth IEEE International Conference on Data Mining (ICDM'05)},
	pages        = {8--pp},
	organization = {IEEE}
}
@misc{bertscore,
	title        = {BERTScore: Evaluating Text Generation with BERT},
	author       = {Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q. and Artzi, Yoav},
	year         = 2019,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.1904.09675},
	url          = {https://arxiv.org/abs/1904.09675},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
misc{bertscore,
  author    = {Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q. and Artzi, Yoav},
  year      = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.1904.09675},
  keywords  = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
  title     = {BERTScore: Evaluating Text Generation with BERT},
  url       = {https://arxiv.org/abs/1904.09675}
}
@misc{bertscore_app,
	title        = {Leveraging Summary Guidance on Medical Report Summarization},
	author       = {Zhu, Yunqi and Yang, Xuebing and Wu, Yuanyuan and Zhang, Wensheng},
	year         = 2023,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.2302.04001},
	url          = {https://arxiv.org/abs/2302.04001},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@inproceedings{bi2021asking,
	title        = {Asking clarifying questions based on negative feedback in conversational search},
	author       = {Bi, Keping and Ai, Qingyao and Croft, W Bruce},
	year         = 2021,
	booktitle    = {Proceedings of the 2021 ACM SIGIR International Conference on Theory of Information Retrieval},
	pages        = {157--166}
}
@article{bonifacio2022inpars,
	title        = {InPars: Data Augmentation for Information Retrieval using Large Language Models},
	author       = {Bonifacio, Luiz and Abonizio, Hugo and Fadaee, Marzieh and Nogueira, Rodrigo},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2202.05144}
}
misc{bonifacio2022inpars,
  author    = {Bonifacio, Luiz and Abonizio, Hugo and Fadaee, Marzieh and Nogueira, Rodrigo},
  year      = {2022},
  copyright = {Creative Commons Attribution 4.0 International},
  doi       = {10.48550/ARXIV.2202.05144},
  keywords  = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
  title     = {InPars: Data Augmentation for Information Retrieval using Large Language Models},
  url       = {https://arxiv.org/abs/2202.05144}
}
@inproceedings{bowman-etal-2016-generating,
	title        = {Generating Sentences from a Continuous Space},
	author       = {Bowman, Samuel R.  and Vilnis, Luke  and Vinyals, Oriol  and Dai, Andrew  and Jozefowicz, Rafal  and Bengio, Samy},
	year         = 2016,
	month        = aug,
	booktitle    = {Proceedings of The 20th {SIGNLL} Conference on Computational Natural Language Learning},
	publisher    = {Association for Computational Linguistics},
	address      = {Berlin, Germany},
	pages        = {10--21},
	doi          = {10.18653/v1/K16-1002},
	url          = {https://aclanthology.org/K16-1002}
}
@inproceedings{broder2002taxonomy,
	title        = {A taxonomy of web search},
	author       = {Broder, Andrei},
	year         = 2002,
	booktitle    = {ACM Sigir forum},
	volume       = 36,
	number       = 2,
	pages        = {3--10},
	organization = {ACM New York, NY, USA}
}
@article{brown2020language,
	title        = {Language models are few-shot learners},
	author       = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
	year         = 2020,
	journal      = {Advances in neural information processing systems},
	volume       = 33,
	pages        = {1877--1901}
}
@inproceedings{bruch2019analysis,
	title        = {An analysis of the softmax cross entropy loss for learning-to-rank with binary relevance},
	author       = {Bruch, Sebastian and Wang, Xuanhui and Bendersky, Michael and Najork, Marc},
	year         = 2019,
	booktitle    = {Proceedings of the 2019 ACM SIGIR international conference on theory of information retrieval},
	pages        = {75--78}
}
@inproceedings{bruch2021alternative,
	title        = {An alternative cross entropy loss for learning-to-rank},
	author       = {Bruch, Sebastian},
	year         = 2021,
	booktitle    = {Proceedings of the Web Conference 2021},
	pages        = {118--126}
}
@article{buck2017ask,
	title        = {Ask the right questions: Active question reformulation with reinforcement learning},
	author       = {Buck, Christian and Bulian, Jannis and Ciaramita, Massimiliano and Gajewski, Wojciech and Gesmundo, Andrea and Houlsby, Neil and Wang, Wei},
	year         = 2017,
	journal      = {arXiv preprint arXiv:1705.07830}
}
@inproceedings{burges2005learning,
	title        = {Learning to rank using gradient descent},
	author       = {Burges, Chris and Shaked, Tal and Renshaw, Erin and Lazier, Ari and Deeds, Matt and Hamilton, Nicole and Hullender, Greg},
	year         = 2005,
	booktitle    = {Proceedings of the 22nd international conference on Machine learning},
	pages        = {89--96}
}
@article{burges2006learning,
	title        = {Learning to rank with nonsmooth cost functions},
	author       = {Burges, Christopher and Ragno, Robert and Le, Quoc},
	year         = 2006,
	journal      = {Advances in neural information processing systems},
	volume       = 19,
	pages        = {193--200}
}
@article{burges2010ranknet,
	title        = {From ranknet to lambdarank to lambdamart: An overview},
	author       = {Burges, Christopher JC},
	year         = 2010,
	journal      = {Learning},
	publisher    = {Citeseer},
	volume       = 11,
	number       = {23-581},
	pages        = 81
}
@inproceedings{cai2010unsupervised,
	title        = {Unsupervised feature selection for multi-cluster data},
	author       = {Cai, Deng and Zhang, Chiyuan and He, Xiaofei},
	year         = 2010,
	booktitle    = {Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining},
	pages        = {333--342}
}
@inproceedings{cao2007learning,
	title        = {Learning to rank: from pairwise approach to listwise approach},
	author       = {Cao, Zhe and Qin, Tao and Liu, Tie-Yan and Tsai, Ming-Feng and Li, Hang},
	year         = 2007,
	booktitle    = {Proceedings of the 24th international conference on Machine learning},
	pages        = {129--136}
}
% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@inproceedings{cao2017joint,
	title        = {Joint copying and restricted generation for paraphrase},
	author       = {Cao, Ziqiang and Luo, Chuwei and Li, Wenjie and Li, Sujian},
	year         = 2017,
	booktitle    = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = 31,
	number       = 1
}
@inproceedings{carbonell1998use,
	title        = {The use of MMR, diversity-based reranking for reordering documents and producing summaries},
	author       = {Carbonell, Jaime and Goldstein, Jade},
	year         = 1998,
	booktitle    = {Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval},
	pages        = {335--336}
}
inproceedings{carbonell1998use,
  author    = {Carbonell, Jaime and Goldstein, Jade},
  year      = {1998},
  booktitle = {Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval},
  pages     = {335--336},
  title     = {The use of MMR, diversity-based reranking for reordering documents and producing summaries}
}
@inproceedings{cascade-ranking,
	title        = {Efficient Cost-Aware Cascade Ranking in Multi-Stage Retrieval},
	author       = {Ruey{-}Cheng Chen and Luke Gallagher and Roi Blanco and J. Shane Culpepper},
	year         = 2017,
	booktitle    = {Proceedings of the 40th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval, Shinjuku, Tokyo, Japan, August 7-11, 2017},
	publisher    = {{ACM}},
	pages        = {445--454},
	doi          = {10.1145/3077136.3080819},
	url          = {https://doi.org/10.1145/3077136.3080819},
	bibsource    = {dblp computer science bibliography, https://dblp.org},
	biburl       = {https://dblp.org/rec/conf/sigir/ChenGBC17.bib},
	editor       = {Noriko Kando and Tetsuya Sakai and Hideo Joho and Hang Li and Arjen P. de Vries and Ryen W. White},
	timestamp    = {Tue, 06 Nov 2018 11:07:23 +0100}
}
@inproceedings{cascade-ranking-cost-aware,
	title        = {Joint optimization of cascade ranking models},
	author       = {Gallagher, Luke and Chen, Ruey-Cheng and Blanco, Roi and Culpepper, J Shane},
	year         = 2019,
	booktitle    = {Proceedings of the twelfth ACM international conference on web search and data mining},
	pages        = {15--23}
}
@inproceedings{cascade-ranking-early,
	title        = {A cascade ranking model for efficient ranked retrieval},
	author       = {Lidan Wang and Jimmy Lin and Donald Metzler},
	year         = 2011,
	booktitle    = {Proceeding of the 34th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval, {SIGIR} 2011, Beijing, China, July 25-29, 2011},
	publisher    = {{ACM}},
	pages        = {105--114},
	doi          = {10.1145/2009916.2009934},
	url          = {https://doi.org/10.1145/2009916.2009934},
	bibsource    = {dblp computer science bibliography, https://dblp.org},
	biburl       = {https://dblp.org/rec/conf/sigir/WangLM11.bib},
	editor       = {Wei{-}Ying Ma and Jian{-}Yun Nie and Ricardo Baeza{-}Yates and Tat{-}Seng Chua and W. Bruce Croft},
	timestamp    = {Fri, 27 Aug 2021 11:12:59 +0200}
}
@misc{chainofthought,
	title        = {Chain of Thought Prompting Elicits Reasoning in Large Language Models},
	author       = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	year         = 2022,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.2201.11903},
	url          = {https://arxiv.org/abs/2201.11903},
	copyright    = {Creative Commons Attribution 4.0 International},
	keywords     = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@book{chang2020query,
	title        = {Query understanding for search engines},
	author       = {Chang, Yi and Deng, Hongbo},
	year         = 2020,
	publisher    = {Springer}
}
@article{chapelle2010efficient,
	title        = {Efficient algorithms for ranking with SVMs},
	author       = {Chapelle, Olivier and Keerthi, S Sathiya},
	year         = 2010,
	journal      = {Information retrieval},
	publisher    = {Springer},
	volume       = 13,
	number       = 3,
	pages        = {201--215}
}
@inproceedings{chapelle2011yahoo,
	title        = {Yahoo! learning to rank challenge overview},
	author       = {Chapelle, Olivier and Chang, Yi},
	year         = 2011,
	booktitle    = {Proceedings of the learning to rank challenge},
	pages        = {1--24},
	organization = {PMLR}
}
@inproceedings{chen2018learning,
	title        = {Learning to explain: An information-theoretic perspective on model interpretation},
	author       = {Chen, Jianbo and Song, Le and Wainwright, Martin and Jordan, Michael},
	year         = 2018,
	booktitle    = {International Conference on Machine Learning},
	pages        = {883--892},
	organization = {PMLR}
}
@article{chen2022reinforced,
	title        = {Reinforced question rewriting for conversational question answering},
	author       = {Chen, Zhiyu and Zhao, Jie and Fang, Anjie and Fetahu, Besnik and Rokhlenko, Oleg and Malmasi, Shervin},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2210.15777}
}
@article{chowdhery:2022:palm,
	title        = {Palm: Scaling language modeling with pathways},
	author       = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2204.02311}
}
@article{clarkeoverview,
	title        = {Overview of the TREC 2009 Web Track},
	author       = {Clarke, Charles LA and Craswell, Nick and Soboroff, Ian},
	publisher    = {Citeseer}
}
@misc{clover,
	title        = {Diversity driven Query Rewriting in Search Advertising},
	author       = {Mohankumar, Akash Kumar and Begwani, Nikit and Singh, Amit},
	year         = 2021,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.2106.03816},
	url          = {https://arxiv.org/abs/2106.03816},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Information Theory (cs.IT), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@misc{codebertscore,
	title        = {CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code},
	author       = {Zhou, Shuyan and Alon, Uri and Agarwal, Sumit and Neubig, Graham},
	year         = 2023,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.2302.05527},
	url          = {https://arxiv.org/abs/2302.05527},
	copyright    = {Creative Commons Attribution 4.0 International},
	keywords     = {Software Engineering (cs.SE), Machine Learning (cs.LG), Programming Languages (cs.PL), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@inproceedings{contextrank,
	title        = {Learning a Deep Listwise Context Model for Ranking Refinement},
	author       = {Qingyao Ai and Keping Bi and Jiafeng Guo and W. Bruce Croft},
	year         = 2018,
	booktitle    = {The 41st International {ACM} {SIGIR} Conference on Research {\&} Development in Information Retrieval, {SIGIR} 2018, Ann Arbor, MI, USA, July 08-12, 2018},
	publisher    = {{ACM}},
	pages        = {135--144},
	doi          = {10.1145/3209978.3209985},
	url          = {https://doi.org/10.1145/3209978.3209985},
	bibsource    = {dblp computer science bibliography, https://dblp.org},
	biburl       = {https://dblp.org/rec/conf/sigir/AiBGC18.bib},
	editor       = {Kevyn Collins{-}Thompson and Qiaozhu Mei and Brian D. Davison and Yiqun Liu and Emine Yilmaz},
	timestamp    = {Sun, 25 Oct 2020 23:03:58 +0100}
}
@misc{contriever,
	title        = {Unsupervised Dense Information Retrieval with Contrastive Learning},
	author       = {Izacard, Gautier and Caron, Mathilde and Hosseini, Lucas and Riedel, Sebastian and Bojanowski, Piotr and Joulin, Armand and Grave, Edouard},
	year         = 2021,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.2112.09118},
	url          = {https://arxiv.org/abs/2112.09118},
	copyright    = {Creative Commons Attribution 4.0 International},
	keywords     = {Information Retrieval (cs.IR), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@misc{controlled_counter,
	title        = {Generate Your Counterfactuals: Towards Controlled Counterfactual Generation for Text},
	author       = {Madaan, Nishtha and Padhi, Inkit and Panwar, Naveen and Saha, Diptikalyan},
	year         = 2020,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.2012.04698},
	url          = {https://arxiv.org/abs/2012.04698},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@inproceedings{Dai_2022,
	title        = {Ask to Know More},
	author       = {Shih-Chieh Dai and Yi-Li Hsu and Aiping Xiong and Lun-Wei Ku},
	year         = 2022,
	month        = {aug},
	booktitle    = {Proceedings of the 28th {ACM} {SIGKDD} Conference on Knowledge Discovery and Data Mining},
	publisher    = {{ACM}},
	doi          = {10.1145/3534678.3539205}
}
@article{dai2022promptagator,
	title        = {Promptagator: Few-shot dense retrieval from 8 examples},
	author       = {Dai, Zhuyun and Zhao, Vincent Y and Ma, Ji and Luan, Yi and Ni, Jianmo and Lu, Jing and Bakalov, Anton and Guu, Kelvin and Hall, Keith B and Chang, Ming-Wei},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2209.11755}
}
@misc{dataset_condensation,
	title        = {DC-BENCH: Dataset Condensation Benchmark},
	author       = {Cui, Justin and Wang, Ruochen and Si, Si and Hsieh, Cho-Jui},
	year         = 2022,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.2207.09639},
	url          = {https://arxiv.org/abs/2207.09639},
	copyright    = {Creative Commons Attribution 4.0 International},
	keywords     = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@misc{dataset_distillation,
	title        = {A Comprehensive Survey of Dataset Distillation},
	author       = {Lei, Shiye and Tao, Dacheng},
	year         = 2023,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.2301.05603},
	url          = {https://arxiv.org/abs/2301.05603},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@article{dato2016fast,
	title        = {Fast ranking with additive ensembles of oblivious and non-oblivious regression trees},
	author       = {Dato, Domenico and Lucchese, Claudio and Nardini, Franco Maria and Orlando, Salvatore and Perego, Raffaele and Tonellotto, Nicola and Venturini, Rossano},
	year         = 2016,
	journal      = {ACM Transactions on Information Systems (TOIS)},
	publisher    = {ACM New York, NY, USA},
	volume       = 35,
	number       = 2,
	pages        = {1--31}
}
@inproceedings{dauphin2017language,
	title        = {Language modeling with gated convolutional networks},
	author       = {Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David},
	year         = 2017,
	booktitle    = {International conference on machine learning},
	pages        = {933--941},
	organization = {PMLR}
}
@inproceedings{del2021question,
	title        = {Question rewriting for open-domain conversational qa: Best practices and limitations},
	author       = {Del Tredici, Marco and Barlacchi, Gianni and Shen, Xiaoyu and Cheng, Weiwei and de Gispert, Adri{\`a}},
	year         = 2021,
	booktitle    = {Proceedings of the 30th ACM International Conference on Information and Knowledge Management},
	pages        = {2974--2978}
}
@article{devlin_bert_2018,
	title        = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language Understanding},
	author       = {Jacob Devlin and Ming{-}Wei Chang and Kenton Lee and Kristina Toutanova},
	year         = 2018,
	journal      = {CoRR},
	volume       = {abs/1810.04805},
	url          = {http://arxiv.org/abs/1810.04805}
}
@inproceedings{Dietz2018TRECCA,
	title        = {TREC Complex Answer Retrieval Overview},
	author       = {Laura Dietz and Manisha Verma and Filip Radlinski and Nick Craswell},
	year         = 2018,
	booktitle    = {Text Retrieval Conference}
}
@misc{diffusion_lm,
	title        = {The Infinite Index: Information Retrieval on Generative Text-To-Image Models},
	author       = {Deckers, Niklas and Fröbe, Maik and Kiesel, Johannes and Pandolfo, Gianluca and Schröder, Christopher and Stein, Benno and Potthast, Martin},
	year         = 2022,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.2212.07476},
	url          = {https://arxiv.org/abs/2212.07476},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Information Retrieval (cs.IR), Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences}
}

@article{wang2023generative,
  title={Generative Query Reformulation for Effective Adhoc Search},
  author={Wang, Xiao and MacAvaney, Sean and Macdonald, Craig and Ounis, Iadh},
  journal={arXiv preprint arXiv:2308.00415},
  year={2023}
}

@misc{doc2query,
	title        = {Document Expansion by Query Prediction},
	author       = {Nogueira, Rodrigo and Yang, Wei and Lin, Jimmy and Cho, Kyunghyun},
	year         = 2019,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.1904.08375},
	url          = {https://arxiv.org/abs/1904.08375},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Information Retrieval (cs.IR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@inproceedings{dolan-etal-2004-unsupervised,
	title        = {Unsupervised Construction of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources},
	author       = {Dolan, Bill  and Quirk, Chris  and Brockett, Chris},
	year         = 2004,
	month        = {aug 23{--}aug 27},
	booktitle    = {{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics},
	publisher    = {COLING},
	address      = {Geneva, Switzerland},
	pages        = {350--356},
	url          = {https://aclanthology.org/C04-1051}
}
@article{du2019techniques,
	title        = {Techniques for interpretable machine learning},
	author       = {Du, Mengnan and Liu, Ninghao and Hu, Xia},
	year         = 2019,
	journal      = {Communications of the ACM},
	publisher    = {ACM New York, NY, USA},
	volume       = 63,
	number       = 1,
	pages        = {68--77}
}
@article{elgohary2019can,
	title        = {Can you unpack that? learning to rewrite questions-in-context},
	author       = {Elgohary, Ahmed and Peskov, Denis and Boyd-Graber, Jordan},
	year         = 2019,
	journal      = {Can You Unpack That? Learning to Rewrite Questions-in-Context}
}
@inproceedings{ellsworth-janin-2007-mutaphrase,
	title        = {{M}utaphrase: Paraphrasing with {F}rame{N}et},
	author       = {Ellsworth, Michael  and Janin, Adam},
	year         = 2007,
	month        = jun,
	booktitle    = {Proceedings of the {ACL}-{PASCAL} Workshop on Textual Entailment and Paraphrasing},
	publisher    = {Association for Computational Linguistics},
	address      = {Prague},
	pages        = {143--150},
	url          = {https://aclanthology.org/W07-1424}
}
@inproceedings{explain_study,
	title        = {Why Am I Not Seeing It? Understanding Users’ Needs for Counterfactual Explanations in Everyday Recommendations},
	author       = {Shang, Ruoxi and Feng, K. J. Kevin and Shah, Chirag},
	year         = 2022,
	booktitle    = {2022 ACM Conference on Fairness, Accountability, and Transparency},
	location     = {Seoul, Republic of Korea},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {FAccT '22},
	pages        = {1330–1340},
	doi          = {10.1145/3531146.3533189},
	isbn         = 9781450393522,
	url          = {https://doi.org/10.1145/3531146.3533189},
	abstract     = {Intelligent everyday applications typically rely on automated Recommender Systems (RS) to generate recommendations that help users make decisions among a large number of options. Due to the increasing complexity of RS and the lack of transparency in its algorithmic decision-making, researchers have recognized the need to support users with explanations. While many traditional Explainable AI methods fall short in disclosing the internal intricacy of recommender systems, counterfactual explanations provide many desirable explainable features by offering human-like explanations that contrast an existing recommendation with alternatives. However, there is a lack of empirical research in understanding users’ needs of counterfactual explanations in their usage of everyday intelligent applications. In this paper, we investigate whether and when to provide counterfactual explanations to support people’s decision-making with everyday recommendations through a question-driven approach. We conducted a preliminary survey study and an interview study to understand how existing explanations might be insufficient to support users and elicit the triggers that prompt them to ask why not questions and seek additional explanations. The findings reveal that the utility of decision is a primary factor that may affect their counterfactual information needs. We then conducted an online scenario-based survey to quantify the correlation between utility and explanation needs and found significant correlations between the measured variables.},
	keywords     = {User studies, Explainable recommender system, Counterfactual explanations},
	numpages     = 11
}
@inproceedings{expred,
	title        = {Explain and Predict, and Then Predict Again},
	author       = {Zhang, Zijian and Rudra, Koustav and Anand, Avishek},
	year         = 2021,
	booktitle    = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
	location     = {Virtual Event, Israel},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {WSDM '21},
	pages        = {418–426},
	doi          = {10.1145/3437963.3441758},
	isbn         = 9781450382977,
	url          = {https://doi.org/10.1145/3437963.3441758},
	abstract     = {A desirable property of learning systems is to be both effective and interpretable. Towards this goal, recent models have been proposed that first generate an extractive explanation from the input text and then generate a prediction on just the explanation called explain-then-predict models. These models primarily consider the task input as a supervision signal in learning an extractive explanation and do not effectively integrate rationales data as an additional inductive bias to improve task performance. We propose a novel yet simple approach ExPred, which uses multi-task learning in the explanation generation phase effectively trading-off explanation and prediction losses. Next, we use another prediction network on just the extracted explanations for optimizing the task performance. We conduct an extensive evaluation of our approach on three diverse language datasets -- sentiment classification, fact-checking, and question answering -- and find that we substantially outperform existing approaches.},
	keywords     = {multitask learning, machine learning interpretation, learning with rationales, interpretable by design, explanation},
	numpages     = 9
}
@inproceedings{nguyen:2016:msmarco,
  author    = {Tri Nguyen and
               Mir Rosenberg and
               Xia Song and
               Jianfeng Gao and
               Saurabh Tiwary and
               Rangan Majumder and
               Li Deng},
  editor    = {Tarek Richard Besold and
               Antoine Bordes and
               Artur S. d'Avila Garcez and
               Greg Wayne},
  title     = {{MS} {MARCO:} {A} Human Generated MAchine Reading COmprehension Dataset},
  booktitle = {Proceedings of the Workshop on Cognitive Computation: Integrating
               neural and symbolic approaches 2016 co-located with the 30th Annual
               Conference on Neural Information Processing Systems {(NIPS} 2016),
               Barcelona, Spain, December 9, 2016},
  series    = {{CEUR} Workshop Proceedings},
  volume    = {1773},
  publisher = {CEUR-WS.org},
  year      = {2016},
  url       = {http://ceur-ws.org/Vol-1773/CoCoNIPS\_2016\_paper9.pdf},
  timestamp = {Wed, 12 Feb 2020 16:44:20 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/NguyenRSGTMD16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{ms_marco_hard,
      title={How Deep is your Learning: the DL-HARD Annotated Deep Learning Dataset}, 
      author={Iain Mackie and Jeffery Dalton and Andrew Yates},
      year={2021},
      eprint={2105.07975},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@inproceedings{bert_qpp,
author = {Arabzadeh, Negar and Khodabakhsh, Maryam and Bagheri, Ebrahim},
title = {BERT-QPP: Contextualized Pre-Trained Transformers for Query Performance Prediction},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482063},
doi = {10.1145/3459637.3482063},
abstract = {Query Performance Prediction (QPP) is focused on estimating the difficulty of satisfying a user query for a certain retrieval method. While most state of the art QPP methods are based on term frequency and corpus statistics, more recent work in this area have started to explore the utility of pretrained neural embeddings, neural architectures and contextual embeddings. Such approaches extract features from pretrained or contextual embeddings for the sake of training a supervised performance predictor. In this paper, we adopt contextual embeddings to perform performance prediction, but distinguish ourselves from the state of the art by proposing to directly fine-tune a contextual embedding, i.e., BERT, specifically for the task of query performance prediction. As such, our work allows the fine-tuned contextual representations to estimate the performance of a query based on the association between the representation of the query and the retrieved documents. We compare the performance of our approach with the state-of-the-art based on the MS MARCO passage retrieval corpus and its three associated query sets: (1) MS MARCO development set, (2) TREC DL 2019, and (3) TREC DL 2020. We show that our approach not only shows significant improved prediction performance compared to all the state-of-the-art methods, but also, unlike past neural predictors, it shows significantly lower latency, making it possible to use in practice.},
booktitle = {Proceedings of the 30th ACM International Conference on Information \& Knowledge Management},
pages = {2857–2861},
numpages = {5},
keywords = {contextualized pre-trained transformers, query performance prediction, cross-encoder, bi-encoder},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}
@inproceedings{zamani:2017:relevance-based-word-embedding,
 author = {Hamed Zamani and
W. Bruce Croft},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/sigir/ZamaniC17.bib},
 booktitle = {Proceedings of the 40th International {ACM} {SIGIR} Conference on
Research and Development in Information Retrieval, Shinjuku, Tokyo,
Japan, August 7-11, 2017},
 doi = {10.1145/3077136.3080831},
 editor = {Noriko Kando and
Tetsuya Sakai and
Hideo Joho and
Hang Li and
Arjen P. de Vries and
Ryen W. White},
 pages = {505--514},
 publisher = {{ACM}},
 timestamp = {Tue, 06 Nov 2018 00:00:00 +0100},
 title = {Relevance-based Word Embedding},
 url = {https://doi.org/10.1145/3077136.3080831},
 year = {2017}
}

@book{ir_query_exp,
author = {Croft, Bruce and Metzler, Donald and Strohman, Trevor},
title = {Search Engines: Information Retrieval in Practice},
year = {2009},
isbn = {0136072240},
publisher = {Addison-Wesley Publishing Company},
address = {USA},
edition = {1st},
abstract = {KEY BENEFIT: Written by a leader in the field of information retrieval, this text provides the background and tools needed to evaluate, compare and modify search engines. KEY TOPICS: Coverage of the underlying IR and mathematical models reinforce key concepts. Numerous programming exercises make extensive use of Galago, a Java-based open source search engine. MARKET: A valuable tool for search engine and information retrieval professionals.}
}
@article{falcon2019pytorch,
	title        = {Pytorch lightning},
	author       = {Falcon, William and others},
	year         = 2019,
	journal      = {GitHub. Note: https://github. com/PyTorchLightning/pytorch-lightning},
	volume       = 3,
	pages        = 6
}
@misc{fast_decoding,
	title        = {Fast Inference from Transformers via Speculative Decoding},
	author       = {Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
	year         = 2022,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.2211.17192},
	url          = {https://arxiv.org/abs/2211.17192},
	copyright    = {Creative Commons Attribution 4.0 International},
	keywords     = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@misc{feature-transformer,
	title        = {Revisiting Deep Learning Models for Tabular Data},
	author       = {Gorishniy, Yury and Rubachev, Ivan and Khrulkov, Valentin and Babenko, Artem},
	year         = 2021,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.2106.11959},
	url          = {https://arxiv.org/abs/2106.11959},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@article{freund2003efficient,
	title        = {An efficient boosting algorithm for combining preferences},
	author       = {Freund, Yoav and Iyer, Raj and Schapire, Robert E and Singer, Yoram},
	year         = 2003,
	journal      = {Journal of machine learning research},
	volume       = 4,
	number       = {Nov},
	pages        = {933--969}
}
@inproceedings{gam-cognitive,
	title        = {{COGAM:} Measuring and Moderating Cognitive Load in Machine Learning Model Explanations},
	author       = {Ashraf M. Abdul and Christian von der Weth and Mohan S. Kankanhalli and Brian Y. Lim},
	year         = 2020,
	booktitle    = {{CHI} '20: {CHI} Conference on Human Factors in Computing Systems, Honolulu, HI, USA, April 25-30, 2020},
	publisher    = {{ACM}},
	pages        = {1--14},
	doi          = {10.1145/3313831.3376615},
	url          = {https://doi.org/10.1145/3313831.3376615},
	bibsource    = {dblp computer science bibliography, https://dblp.org},
	biburl       = {https://dblp.org/rec/conf/chi/AbdulWKL20.bib},
	editor       = {Regina Bernhaupt and Florian 'Floyd' Mueller and David Verweij and Josh Andres and Joanna McGrenere and Andy Cockburn and Ignacio Avellino and Alix Goguey and Pernille Bj{\o}n and Shengdong Zhao and Briane Paul Samson and Rafal Kocielnik},
	timestamp    = {Wed, 04 May 2022 13:02:17 +0200}
}
@inproceedings{gao2010clickthrough:mt:discriminative,
	title        = {Clickthrough-based translation models for web search: from word models to phrase models},
	author       = {Gao, Jianfeng and He, Xiaodong and Nie, Jian-Yun},
	year         = 2010,
	booktitle    = {Proceedings of the 19th ACM international conference on Information and knowledge management},
	pages        = {1139--1148}
}
@inproceedings{GAS-geng2007feature,
	title        = {Feature selection for ranking},
	author       = {Geng, Xiubo and Liu, Tie-Yan and Qin, Tao and Li, Hang},
	year         = 2007,
	booktitle    = {Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval},
	pages        = {407--414}
}
@misc{GenEx,
	title        = {Explaining Documents' Relevance to Search Queries},
	author       = {Rahimi, Razieh and Kim, Youngwoo and Zamani, Hamed and Allan, James},
	year         = 2021,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.2111.01314},
	url          = {https://arxiv.org/abs/2111.01314},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Information Retrieval (cs.IR), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@inproceedings{gigli2016fast,
	title        = {Fast feature selection for learning to rank},
	author       = {Gigli, Andrea and Lucchese, Claudio and Nardini, Franco Maria and Perego, Raffaele},
	year         = 2016,
	booktitle    = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
	pages        = {167--170}
}
@article{goldberg1988genetic,
	title        = {Genetic algorithms and machine learning},
	author       = {Goldberg, David E and Holland, John Henry},
	year         = 1988,
	publisher    = {Kluwer Academic Publishers-Plenum Publishers; Kluwer Academic Publishers~…}
}
@inproceedings{goyal-durrett-2020-neural,
	title        = {Neural Syntactic Preordering for Controlled Paraphrase Generation},
	author       = {Goyal, Tanya  and Durrett, Greg},
	year         = 2020,
	month        = jul,
	booktitle    = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {238--252},
	doi          = {10.18653/v1/2020.acl-main.22},
	url          = {https://aclanthology.org/2020.acl-main.22},
	abstract     = {Paraphrasing natural language sentences is a multifaceted process: it might involve replacing individual words or short phrases, local rearrangement of content, or high-level restructuring like topicalization or passivization. Past approaches struggle to cover this space of paraphrase possibilities in an interpretable manner. Our work, inspired by pre-ordering literature in machine translation, uses syntactic transformations to softly {``}reorder{''} the source sentence and guide our neural paraphrasing model. First, given an input sentence, we derive a set of feasible syntactic rearrangements using an encoder-decoder model. This model operates over a partially lexical, partially syntactic view of the sentence and can reorder big chunks. Next, we use each proposed rearrangement to produce a sequence of position embeddings, which encourages our final encoder-decoder paraphrase model to attend to the source words in a particular order. Our evaluation, both automatic and human, shows that the proposed system retains the quality of the baseline approaches while giving a substantial increase in the diversity of the generated paraphrases.}
}
title = "Retrieval-guided Counterfactual Generation for {QA}",
    author = "Paranjape, Bhargavi  and
      Lamm, Matthew  and
      Tenney, Ian",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.117",
    doi = "10.18653/v1/2022.acl-long.117",
    pages = "1670--1686",
    abstract = "Deep NLP models have been shown to be brittle to input perturbations. Recent work has shown that data augmentation using counterfactuals {---} i.e. minimally perturbed inputs {---} can help ameliorate this weakness. We focus on the task of creating counterfactuals for question answering, which presents unique challenges related to world knowledge, semantic diversity, and answerability. To address these challenges, we develop a Retrieve-Generate-Filter(RGF) technique to create counterfactual evaluation and training data with minimal human supervision. Using an open-domain QA framework and question generation model trained on original task data, we create counterfactuals that are fluent, semantically diverse, and automatically labeled. Data augmentation with RGF counterfactuals improves performance on out-of-domain and challenging evaluation sets over and above existing methods, in both the reading comprehension and open-domain QA settings. Moreover, we find that RGF data leads to significant improvements in a model{'}s robustness to local perturbations.",
}
@misc{gpt3_knowledge_base,
	title        = {An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA},
	author       = {Yang, Zhengyuan and Gan, Zhe and Wang, Jianfeng and Hu, Xiaowei and Lu, Yumao and Liu, Zicheng and Wang, Lijuan},
	year         = 2021,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.2109.05014},
	url          = {https://arxiv.org/abs/2109.05014},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@misc{gpt3_prompting,
	title        = {Prompting GPT-3 To Be Reliable},
	author       = {Si, Chenglei and Gan, Zhe and Yang, Zhengyuan and Wang, Shuohang and Wang, Jianfeng and Boyd-Graber, Jordan and Wang, Lijuan},
	year         = 2022,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.2210.09150},
	url          = {https://arxiv.org/abs/2210.09150},
	copyright    = {Creative Commons Attribution 4.0 International},
	keywords     = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@misc{wei2023larger,
      title={Larger language models do in-context learning differently}, 
      author={Jerry Wei and Jason Wei and Yi Tay and Dustin Tran and Albert Webson and Yifeng Lu and Xinyun Chen and Hanxiao Liu and Da Huang and Denny Zhou and Tengyu Ma},
      year={2023},
      eprint={2303.03846},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{bevilacqua2022autoregressive,
      title={Autoregressive Search Engines: Generating Substrings as Document Identifiers}, 
      author={Michele Bevilacqua and Giuseppe Ottaviano and Patrick Lewis and Wen-tau Yih and Sebastian Riedel and Fabio Petroni},
      year={2022},
      eprint={2204.10628},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{yu2023generate,
      title={Generate rather than Retrieve: Large Language Models are Strong Context Generators}, 
      author={Wenhao Yu and Dan Iter and Shuohang Wang and Yichong Xu and Mingxuan Ju and Soumya Sanyal and Chenguang Zhu and Michael Zeng and Meng Jiang},
      year={2023},
      eprint={2209.10063},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{query_expansion_e_commerce,
author = {Lee, Mu-Chu and Gao, Bin and Zhang, Ruofei},
title = {Rare Query Expansion Through Generative Adversarial Networks in Search Advertising},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219850},
doi = {10.1145/3219819.3219850},
abstract = {Generative Adversarial Networks (GAN) have achieved great success in generating realistic synthetic data like images, tags, and sentences. We explore using GAN to generate bid keywords directly from query in sponsored search ads selection, especially for rare queries. Specifically, in the query expansion (query-keyword matching) scenario in search advertising, we train a sequence to sequence model as the generator to generate keywords, conditioned on the user query, and use a recurrent neural network model as the discriminator to play an adversarial game with the generator. By applying the trained generator, we can generate keywords directly from a given query, so that we can highly improve the effectiveness and efficiency of query-keyword matching based ads selection in search advertising. We trained the proposed model in the clicked query-keyword pair dataset from a commercial search advertising system. Evaluation results show that the generated keywords are more relevant to the given query compared with the baseline model and they have big potential to bring extra revenue improvement.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {500–508},
numpages = {9},
keywords = {query keyword matching, online advertising, generative adversarial networks},
location = {London, United Kingdom},
series = {KDD '18}
}

@inproceedings{taobao_search,
author = {Li, Sen and Lv, Fuyu and Jin, Taiwei and Li, Guiyang and Zheng, Yukun and Zhuang, Tao and Liu, Qingwen and Zeng, Xiaoyi and Kwok, James and Ma, Qianli},
title = {Query Rewriting in TaoBao Search},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557068},
doi = {10.1145/3511808.3557068},
abstract = {In e-commerce search engines, query rewriting (QR) is a crucial technique that improves shopping experience by reducing the vocabulary gap between user queries and product catalog. Recent works have mainly adopted the generative paradigm. However, they hardly ensure high-quality generated rewrites and do not consider personalization, which leads to degraded search relevance. In this work, we present Contrastive Learning Enhanced Query Rewriting (CLE-QR), the solution used in Taobao product search. It uses a novel contrastive learning enhanced architecture based on "query retrieval-semantic relevance ranking-online ranking". It finds the rewrites from hundreds of millions of historical queries while considering relevance and personalization. Specifically, we first alleviate the representation degeneration problem during the query retrieval stage by using an unsupervised contrastive loss, and then further propose an interaction-aware matching method to find the beneficial and incremental candidates, thus improving the quality and relevance of candidate queries. We then present a relevance-oriented contrastive pre-training paradigm on the noisy user feedback data to improve semantic ranking performance. Finally, we rank these candidates online with the user profile to model personalization for the retrieval of more relevant products. We evaluate CLE-QR on Taobao Product Search, one of the largest e-commerce platforms in China. Significant metrics gains are observed in online A/B tests. CLE-QR has been deployed to our large-scale commercial retrieval system and serviced hundreds of millions of users since December 2021. We also introduce its online deployment scheme, and share practical lessons and optimization tricks of our lexical match system.},
booktitle = {Proceedings of the 31st ACM International Conference on Information and Knowledge Management},
pages = {3262–3271},
numpages = {10},
keywords = {lexical match, e-commerce search, query rewriting},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}
@misc{asai2022taskaware,
      title={Task-aware Retrieval with Instructions}, 
      author={Akari Asai and Timo Schick and Patrick Lewis and Xilun Chen and Gautier Izacard and Sebastian Riedel and Hannaneh Hajishirzi and Wen-tau Yih},
      year={2022},
      eprint={2211.09260},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{lee2022generative,
      title={Generative Multi-hop Retrieval}, 
      author={Hyunji Lee and Sohee Yang and Hanseok Oh and Minjoon Seo},
      year={2022},
      eprint={2204.13596},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@misc{tay2022transformer,
      title={Transformer Memory as a Differentiable Search Index}, 
      author={Yi Tay and Vinh Q. Tran and Mostafa Dehghani and Jianmo Ni and Dara Bahri and Harsh Mehta and Zhen Qin and Kai Hui and Zhe Zhao and Jai Gupta and Tal Schuster and William W. Cohen and Donald Metzler},
      year={2022},
      eprint={2202.06991},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{gospodinov2023doc2query,
      title={Doc2Query--: When Less is More}, 
      author={Mitko Gospodinov and Sean MacAvaney and Craig Macdonald},
      year={2023},
      eprint={2301.03266},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}
@article{Metzler_2021,
	doi = {10.1145/3476415.3476428},
  
	url = {https://doi.org/10.1145%2F3476415.3476428},
  
	year = 2021,
	month = {jun},
  
	publisher = {Association for Computing Machinery ({ACM})},
  
	volume = {55},
  
	number = {1},
  
	pages = {1--27},
  
	author = {Donald Metzler and Yi Tay and Dara Bahri and Marc Najork},
  
	title = {Rethinking search},
  
	journal = {{ACM} {SIGIR} Forum}
}

@inproceedings{docT5query,
  title={From doc2query to docTTTTTquery},
  author={David R. Cheriton},
  year={2019}
}

@misc{incontext_survey,
      title={A Survey on In-context Learning}, 
      author={Qingxiu Dong and Lei Li and Damai Dai and Ce Zheng and Zhiyong Wu and Baobao Chang and Xu Sun and Jingjing Xu and Lei Li and Zhifang Sui},
      year={2023},
      eprint={2301.00234},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{gpt3incontext,
	title        = {What Makes Good In-Context Examples for {GPT}-3?},
	author       = {Liu, Jiachang  and Shen, Dinghan  and Zhang, Yizhe  and Dolan, Bill  and Carin, Lawrence  and Chen, Weizhu},
	year         = 2022,
	month        = may,
	booktitle    = {Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures},
	publisher    = {Association for Computational Linguistics},
	address      = {Dublin, Ireland and Online},
	pages        = {100--114},
	doi          = {10.18653/v1/2022.deelio-1.10},
	url          = {https://aclanthology.org/2022.deelio-1.10},
	abstract     = {GPT-3 has attracted lots of attention due to its superior performance across a wide range of NLP tasks, especially with its in-context learning abilities. Despite its success, we found that the empirical results of GPT-3 depend heavily on the choice of in-context examples. In this work, we investigate whether there are more effective strategies for judiciously selecting in-context examples (relative to random sampling) that better leverage GPT-3{'}s in-context learning capabilities.Inspired by the recent success of leveraging a retrieval module to augment neural networks, we propose to retrieve examples that are semantically-similar to a test query sample to formulate its corresponding prompt. Intuitively, the examples selected with such a strategy may serve as more informative inputs to unleash GPT-3{'}s power of text generation. We evaluate the proposed approach on several natural language understanding and generation benchmarks, where the retrieval-based prompt selection approach consistently outperforms the random selection baseline. Moreover, it is observed that the sentence encoders fine-tuned on task-related datasets yield even more helpful retrieval results. Notably, significant gains are observed on tasks such as table-to-text generation (44.3{\%} on the ToTTo dataset) and open-domain question answering (45.5{\%} on the NQ dataset).}
}
@inproceedings{grbovic2015context,
	title        = {Context-and content-aware embeddings for query rewriting in sponsored search},
	author       = {Grbovic, Mihajlo and Djuric, Nemanja and Radosavljevic, Vladan and Silvestri, Fabrizio and Bhamidipati, Narayan},
	year         = 2015,
	booktitle    = {Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval},
	pages        = {383--392}
}
@article{gu2012generalized,
	title        = {Generalized fisher score for feature selection},
	author       = {Gu, Quanquan and Li, Zhenhui and Han, Jiawei},
	year         = 2012,
	journal      = {arXiv preprint arXiv:1202.3725}
}
@article{gumbel-jang-2016,
	title        = {Categorical Reparametrization with Gumble-Softmax},
	author       = {Jang, Eric and Gu, Shixiang and Poole, Ben},
	year         = 2017,
	booktitle    = {International Conference on Learning Representations (ICLR 2017)},
	organization = {OpenReview.net}
}
@article{hallucination,
	title        = {Survey of Hallucination in Natural Language Generation},
	author       = {Ziwei Ji and Nayeon Lee and Rita Frieske and Tiezheng Yu and Dan Su and Yan Xu and Etsuko Ishii and Yejin Bang and Andrea Madotto and Pascale Fung},
	year         = 2022,
	month        = {nov},
	journal      = {{ACM} Computing Surveys},
	publisher    = {Association for Computing Machinery ({ACM})},
	doi          = {10.1145/3571730},
	url          = {https://doi.org/10.1145%2F3571730}
}
@inproceedings{han2018autoencoder,
	title        = {Autoencoder inspired unsupervised feature selection},
	author       = {Han, Kai and Wang, Yunhe and Zhang, Chao and Li, Chao and Xu, Chao},
	year         = 2018,
	booktitle    = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	pages        = {2941--2945},
	organization = {IEEE}
}
@article{he2005laplacian,
	title        = {Laplacian score for feature selection},
	author       = {He, Xiaofei and Cai, Deng and Niyogi, Partha},
	year         = 2005,
	journal      = {Advances in neural information processing systems},
	volume       = 18
}
@inproceedings{he2016learning,
	title        = {Learning to rewrite queries},
	author       = {He, Yunlong and Tang, Jiliang and Ouyang, Hua and Kang, Changsung and Yin, Dawei and Chang, Yi},
	year         = 2016,
	booktitle    = {Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
	pages        = {1443--1452}
}
@article{he2022rethink:retrieve,
	title        = {Rethinking with Retrieval: Faithful Large Language Model Inference},
	author       = {He, Hangfeng and Zhang, Hongming and Roth, Dan},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2301.00303}
}
@inproceedings{hosking_factorising,
	title        = {Factorising Meaning and Form for Intent-Preserving Paraphrasing},
	author       = {Hosking, Tom  and Lapata, Mirella},
	year         = 2021,
	month        = aug,
	booktitle    = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {1405--1418},
	doi          = {10.18653/v1/2021.acl-long.112},
	url          = {https://aclanthology.org/2021.acl-long.112},
	abstract     = {We propose a method for generating paraphrases of English questions that retain the original intent but use a different surface form. Our model combines a careful choice of training objective with a principled information bottleneck, to induce a latent encoding space that disentangles meaning and form. We train an encoder-decoder model to reconstruct a question from a paraphrase with the same meaning and an exemplar with the same surface form, leading to separated encoding spaces. We use a Vector-Quantized Variational Autoencoder to represent the surface form as a set of discrete latent variables, allowing us to use a classifier to select a different surface form at test time. Crucially, our method does not require access to an external source of target exemplars. Extensive experiments and a human evaluation show that we are able to generate paraphrases with a better tradeoff between semantic preservation and syntactic novelty compared to previous methods.}
}
@inproceedings{hotflip,
	title        = {{H}ot{F}lip: White-Box Adversarial Examples for Text Classification},
	author       = {Ebrahimi, Javid  and Rao, Anyi  and Lowd, Daniel  and Dou, Dejing},
	year         = 2018,
	month        = jul,
	booktitle    = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Melbourne, Australia},
	pages        = {31--36},
	doi          = {10.18653/v1/P18-2006},
	url          = {https://aclanthology.org/P18-2006},
	abstract     = {We propose an efficient method to generate white-box adversarial examples to trick a character-level neural classifier. We find that only a few manipulations are needed to greatly decrease the accuracy. Our method relies on an atomic flip operation, which swaps one token for another, based on the gradients of the one-hot input vectors. Due to efficiency of our method, we can perform adversarial training which makes the model more robust to attacks at test time. With the use of a few semantics-preserving constraints, we demonstrate that HotFlip can be adapted to attack a word-level classifier as well.}
}
@misc{https://doi.org/10.48550/arxiv.1810.04805,
	title        = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	author       = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year         = 2018,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.1810.04805},
	url          = {https://arxiv.org/abs/1810.04805},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@misc{https://doi.org/10.48550/arxiv.1907.10529,
	title        = {SpanBERT: Improving Pre-training by Representing and Predicting Spans},
	author       = {Joshi, Mandar and Chen, Danqi and Liu, Yinhan and Weld, Daniel S. and Zettlemoyer, Luke and Levy, Omer},
	year         = 2019,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.1907.10529},
	url          = {https://arxiv.org/abs/1907.10529},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@misc{https://doi.org/10.48550/arxiv.2206.08263,
	title        = {'John ate 5 apples' != 'John ate some apples': Self-Supervised Paraphrase Quality Detection for Algebraic Word Problems},
	author       = {Gupta, Rishabh and V, Venktesh and Mohania, Mukesh and Goyal, Vikram},
	year         = 2022,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.2206.08263},
	url          = {https://arxiv.org/abs/2206.08263},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@inproceedings{hua2010hierarchical,
	title        = {Hierarchical feature selection for ranking},
	author       = {Hua, Guichun and Zhang, Min and Liu, Yiqun and Ma, Shaoping and Ru, Liyun},
	year         = 2010,
	booktitle    = {Proceedings of the 19th international conference on world wide web},
	pages        = {1113--1114}
}
@inproceedings{huang-chang-2021-generating,
	title        = {Generating Syntactically Controlled Paraphrases without Using Annotated Parallel Pairs},
	author       = {Huang, Kuan-Hao  and Chang, Kai-Wei},
	year         = 2021,
	month        = apr,
	booktitle    = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {1022--1033},
	doi          = {10.18653/v1/2021.eacl-main.88},
	url          = {https://aclanthology.org/2021.eacl-main.88},
	abstract     = {Paraphrase generation plays an essential role in natural language process (NLP), and it has many downstream applications. However, training supervised paraphrase models requires many annotated paraphrase pairs, which are usually costly to obtain. On the other hand, the paraphrases generated by existing unsupervised approaches are usually syntactically similar to the source sentences and are limited in diversity. In this paper, we demonstrate that it is possible to generate syntactically various paraphrases without the need for annotated paraphrase pairs. We propose Syntactically controlled Paraphrase Generator (SynPG), an encoder-decoder based model that learns to disentangle the semantics and the syntax of a sentence from a collection of unannotated texts. The disentanglement enables SynPG to control the syntax of output paraphrases by manipulating the embedding in the syntactic space. Extensive experiments using automatic metrics and human evaluation show that SynPG performs better syntactic control than unsupervised baselines, while the quality of the generated paraphrases is competitive. We also demonstrate that the performance of SynPG is competitive or even better than supervised models when the unannotated data is large. Finally, we show that the syntactically controlled paraphrases generated by SynPG can be utilized for data augmentation to improve the robustness of NLP models.}
}
@misc{hyde,
	title        = {Precise Zero-Shot Dense Retrieval without Relevance Labels},
	author       = {Gao, Luyu and Ma, Xueguang and Lin, Jimmy and Callan, Jamie},
	year         = 2022,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.2212.10496},
	url          = {https://arxiv.org/abs/2212.10496},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Information Retrieval (cs.IR), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@inproceedings{ijcai2019p876,
	title        = {Counterfactuals in Explainable Artificial Intelligence (XAI): Evidence from Human Reasoning},
	author       = {Byrne, Ruth M. J.},
	year         = 2019,
	month        = 7,
	booktitle    = {Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, {IJCAI-19}},
	publisher    = {International Joint Conferences on Artificial Intelligence Organization},
	pages        = {6276--6282},
	doi          = {10.24963/ijcai.2019/876},
	url          = {https://doi.org/10.24963/ijcai.2019/876}
}
@inproceedings{ilmart,
	title        = {{ILMART:} Interpretable Ranking with Constrained LambdaMART},
	author       = {Claudio Lucchese and Franco Maria Nardini and Salvatore Orlando and Raffaele Perego and Alberto Veneri},
	year         = 2022,
	booktitle    = {{SIGIR} '22: The 45th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022},
	publisher    = {{ACM}},
	pages        = {2255--2259},
	doi          = {10.1145/3477495.3531840},
	url          = {https://doi.org/10.1145/3477495.3531840},
	bibsource    = {dblp computer science bibliography, https://dblp.org},
	biburl       = {https://dblp.org/rec/conf/sigir/LuccheseN00V22.bib},
	editor       = {Enrique Amig{\'{o}} and Pablo Castells and Julio Gonzalo and Ben Carterette and J. Shane Culpepper and Gabriella Kazai},
	timestamp    = {Sun, 02 Oct 2022 16:15:16 +0200}
}
@inproceedings{integratedgradients,
	title        = {Axiomatic Attribution for Deep Networks},
	author       = {Mukund Sundararajan and Ankur Taly and Qiqi Yan},
	year         = 2017,
	booktitle    = {Proceedings of the 34th International Conference on Machine Learning, {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017},
	publisher    = {{PMLR}},
	series       = {Proceedings of Machine Learning Research},
	volume       = 70,
	pages        = {3319--3328},
	url          = {http://proceedings.mlr.press/v70/sundararajan17a.html},
	bibsource    = {dblp computer science bibliography, https://dblp.org},
	biburl       = {https://dblp.org/rec/conf/icml/SundararajanTY17.bib},
	editor       = {Doina Precup and Yee Whye Teh},
	timestamp    = {Wed, 29 May 2019 08:41:45 +0200}
}
@inproceedings{interpretation-usage,
	title        = {Interpreting Interpretability: Understanding Data Scientists' Use of Interpretability Tools for Machine Learning},
	author       = {Harmanpreet Kaur and Harsha Nori and Samuel Jenkins and Rich Caruana and Hanna M. Wallach and Jennifer Wortman Vaughan},
	year         = 2020,
	booktitle    = {{CHI} '20: {CHI} Conference on Human Factors in Computing Systems, Honolulu, HI, USA, April 25-30, 2020},
	publisher    = {{ACM}},
	pages        = {1--14},
	doi          = {10.1145/3313831.3376219},
	url          = {https://doi.org/10.1145/3313831.3376219},
	bibsource    = {dblp computer science bibliography, https://dblp.org},
	biburl       = {https://dblp.org/rec/conf/chi/KaurNJCWV20.bib},
	editor       = {Regina Bernhaupt and Florian 'Floyd' Mueller and David Verweij and Josh Andres and Joanna McGrenere and Andy Cockburn and Ignacio Avellino and Alix Goguey and Pernille Bj{\o}n and Shengdong Zhao and Briane Paul Samson and Rafal Kocielnik},
	timestamp    = {Tue, 05 Jul 2022 08:30:26 +0200}
}
@article{ishii2022can,
	title        = {Can Question Rewriting Help Conversational Question Answering?},
	author       = {Ishii, Etsuko and Xu, Yan and Cahyawijaya, Samuel and Wilie, Bryan},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2204.06239}
}
@inproceedings{ishii2022integrating,
	title        = {Integrating Question Rewrites in Conversational Question Answering: A Reinforcement Learning Approach},
	author       = {Ishii, Etsuko and Wilie, Bryan and Xu, Yan and Cahyawijaya, Samuel and Fung, Pascale},
	year         = 2022,
	booktitle    = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop},
	pages        = {55--66}
}
@article{jarvelin2002cumulated,
	title        = {Cumulated Gain-Based Evaluation of IR Techniques},
	author       = {J{\"a}rvelin, Kalervo and Kek{\"a}l{\"a}inen, Jaana},
	year         = 2002,
	journal      = {ACM Transactions on Information Systems (TOIS)},
	publisher    = {ACM New York, NY, USA},
	volume       = 20,
	number       = 4,
	pages        = {422--446}
}
@inproceedings{joachims2002optimizing,
	title        = {Optimizing search engines using clickthrough data},
	author       = {Joachims, Thorsten},
	year         = 2002,
	booktitle    = {Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining},
	pages        = {133--142}
}
@inproceedings{jones2006generating,
	title        = {Generating query substitutions},
	author       = {Jones, Rosie and Rey, Benjamin and Madani, Omid and Greiner, Wiley},
	year         = 2006,
	booktitle    = {Proceedings of the 15th international conference on World Wide Web},
	pages        = {387--396}
}
inproceedings{jones2006generating,
  author    = {Jones, Rosie and Rey, Benjamin and Madani, Omid and Greiner, Wiley},
  year      = {2006},
  booktitle = {Proceedings of the 15th international conference on World Wide Web},
  pages     = {387--396},
  title     = {Generating query substitutions}
}
@inproceedings{kaiser2021reinforcement,
	title        = {Reinforcement learning from reformulations in conversational question answering over knowledge graphs},
	author       = {Kaiser, Magdalena and Saha Roy, Rishiraj and Weikum, Gerhard},
	year         = 2021,
	booktitle    = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages        = {459--469}
}
@article{keknowledge,
	title        = {Knowledge-augmented Self-training of A Question Rewriter for Conversational Knowledge Base Question Answering},
	author       = {Ke, Xirui and Zhang, Jing and Lv, Xin and Xu, Yiqi and Cao, Shulin and Li, Cuiping and Chen, Hong and Li, Juanzi}
}
@article{khattab2022:dsp,
	title        = {Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP},
	author       = {Khattab, Omar and Santhanam, Keshav and Li, Xiang Lisa and Hall, David and Liang, Percy and Potts, Christopher and Zaharia, Matei},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2212.14024}
}
@article{kingma2013auto,
	title        = {Auto-encoding variational bayes},
	author       = {Kingma, Diederik P and Welling, Max},
	year         = 2013,
	journal      = {arXiv preprint arXiv:1312.6114}
}
@inproceedings{kitaev-klein-2018-constituency,
	title        = {Constituency Parsing with a Self-Attentive Encoder},
	author       = {Kitaev, Nikita  and Klein, Dan},
	year         = 2018,
	month        = jul,
	booktitle    = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Melbourne, Australia},
	pages        = {2676--2686},
	doi          = {10.18653/v1/P18-1249},
	url          = {https://www.aclweb.org/anthology/P18-1249}
}
@article{knowledge_distillation,
	title        = {Knowledge Distillation: A Survey},
	author       = {Jianping Gou and Baosheng Yu and Stephen J. Maybank and Dacheng Tao},
	year         = 2021,
	month        = {mar},
	journal      = {International Journal of Computer Vision},
	publisher    = {Springer Science and Business Media {LLC}},
	volume       = 129,
	number       = 6,
	pages        = {1789--1819},
	doi          = {10.1007/s11263-021-01453-z},
	url          = {https://doi.org/10.1007%2Fs11263-021-01453-z}
}
@article{kohavi1997wrappers,
	title        = {Wrappers for feature subset selection},
	author       = {Kohavi, Ron and John, George H},
	year         = 1997,
	journal      = {Artificial intelligence},
	publisher    = {Elsevier},
	volume       = 97,
	number       = {1-2},
	pages        = {273--324}
}
@inproceedings{kurland2022:competitive:search,
	title        = {Competitive Search},
	author       = {Kurland, Oren and Tennenholtz, Moshe},
	year         = 2022,
	booktitle    = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages        = {2838--2849}
}
@article{lai2012fenchelrank,
	title        = {Sparse learning-to-rank via an efficient primal-dual algorithm},
	author       = {Lai, Hanjiang and Pan, Yan and Liu, Cong and Lin, Liang and Wu, Jie},
	year         = 2012,
	journal      = {IEEE Transactions on Computers},
	publisher    = {IEEE},
	volume       = 62,
	number       = 6,
	pages        = {1221--1233}
}
@article{lai2012sparse,
	title        = {Sparse learning-to-rank via an efficient primal-dual algorithm},
	author       = {Lai, Hanjiang and Pan, Yan and Liu, Cong and Lin, Liang and Wu, Jie},
	year         = 2012,
	journal      = {IEEE Transactions on Computers},
	publisher    = {IEEE},
	volume       = 62,
	number       = 6,
	pages        = {1221--1233}
}
@article{lai2013fsmrank,
	title        = {FSMRank: Feature selection algorithm for learning to rank},
	author       = {Lai, Han-Jiang and Pan, Yan and Tang, Yong and Yu, Rong},
	year         = 2013,
	journal      = {IEEE transactions on neural networks and learning systems},
	publisher    = {IEEE},
	volume       = 24,
	number       = 6,
	pages        = {940--952}
}
@article{laporte2013nonconvex,
	title        = {Nonconvex regularizations for feature selection in ranking with sparse SVM},
	author       = {Laporte, L{\'e}a and Flamary, R{\'e}mi and Canu, St{\'e}phane and D{\'e}jean, S{\'e}bastien and Mothe, Josiane},
	year         = 2013,
	journal      = {IEEE Transactions on Neural Networks and Learning Systems},
	publisher    = {IEEE},
	volume       = 25,
	number       = 6,
	pages        = {1118--1130}
}
@article{lasso-tibshirani1996regression,
	title        = {Regression shrinkage and selection via the lasso},
	author       = {Tibshirani, Robert},
	year         = 1996,
	journal      = {Journal of the Royal Statistical Society: Series B (Methodological)},
	publisher    = {Wiley Online Library},
	volume       = 58,
	number       = 1,
	pages        = {267--288}
}
@inproceedings{lee:2018:kdd:gan:expansion,
	title        = {Rare Query Expansion Through Generative Adversarial Networks in Search Advertising},
	author       = {Lee, Mu-Chu and Gao, Bin and Zhang, Ruofei},
	year         = 2018,
	booktitle    = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	location     = {London, United Kingdom},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {KDD '18},
	pages        = {500–508},
	doi          = {10.1145/3219819.3219850},
	isbn         = 9781450355520,
	url          = {https://doi.org/10.1145/3219819.3219850},
	keywords     = {query keyword matching, generative adversarial networks, online advertising},
	numpages     = 9
}
@article{lemhadri2021lassonet,
	title        = {LassoNet: A Neural Network with Feature Sparsity},
	author       = {Lemhadri, Ismael and Ruan, Feng and Abraham, Louis and Tibshirani, Robert},
	year         = 2021,
	journal      = {Journal of Machine Learning Research},
	volume       = 22,
	number       = 127,
	pages        = {1--29}
}
@article{leonhardt2021learnt,
	title        = {Extractive Explanations for Interpretable Text Ranking},
	author       = {Leonhardt, Jurek and Rudra, Koustav and Anand, Avishek},
	year         = 2022,
	month        = {dec},
	journal      = {ACM Trans. Inf. Syst.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	doi          = {10.1145/3576924},
	issn         = {1046-8188},
	url          = {https://doi.org/10.1145/3576924},
	keywords     = {information retrieval, fact checking, sentence selection, interpretability, ranking}
}
@article{lewis2019bart,
	title        = {Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
	author       = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1910.13461}
}
@article{li2007mcrank,
	title        = {Mcrank: Learning to rank using multiple classification and gradient boosting},
	author       = {Li, Ping and Wu, Qiang and Burges, Christopher},
	year         = 2007,
	journal      = {Advances in neural information processing systems},
	publisher    = {Citeseer},
	volume       = 20,
	pages        = {897--904}
}
@inproceedings{li2012generalized,
	title        = {A generalized hidden markov model with discriminative training for query spelling correction},
	author       = {Li, Yanen and Duan, Huizhong and Zhai, ChengXiang},
	year         = 2012,
	booktitle    = {Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval},
	pages        = {611--620}
}
@article{lian2021:end:generativeretrieval,
	title        = {An End-to-End Generative Retrieval Method for Sponsored Search},
	author       = {Lian, Yijiang and Chen, Zhijie and Jia, Jing and You, Zhenjun and Tian, Chao and Hu, Jinlong and Zhang, Kefeng and Yan, Chunwei and Tong, Muchenxuan and Han, Wenying and others},
	year         = 2021
}
@inproceedings{lightgbm,
	title        = {LightGBM: {A} Highly Efficient Gradient Boosting Decision Tree},
	author       = {Guolin Ke and Qi Meng and Thomas Finley and Taifeng Wang and Wei Chen and Weidong Ma and Qiwei Ye and Tie{-}Yan Liu},
	year         = 2017,
	booktitle    = {Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, {USA}},
	pages        = {3146--3154},
	url          = {https://proceedings.neurips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html},
	bibsource    = {dblp computer science bibliography, https://dblp.org},
	biburl       = {https://dblp.org/rec/conf/nips/KeMFWCMYL17.bib},
	editor       = {Isabelle Guyon and Ulrike von Luxburg and Samy Bengio and Hanna M. Wallach and Rob Fergus and S. V. N. Vishwanathan and Roman Garnett},
	timestamp    = {Thu, 21 Jan 2021 15:15:21 +0100}
}
@misc{ling2017program,
	title        = {Program Induction by Rationale Generation : Learning to Solve and Explain Algebraic Word Problems},
	author       = {Wang Ling and Dani Yogatama and Chris Dyer and Phil Blunsom},
	year         = 2017,
	archiveprefix = {arXiv},
	eprint       = {1705.04146},
	primaryclass = {cs.AI}
}

@misc{capannini2011efficient,
      title={Efficient Diversification of Web Search Results}, 
      author={Gabriele Capannini and Franco Maria Nardini and Raffaele Perego and Fabrizio Silvestri},
      year={2011},
      eprint={1105.4255},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@inproceedings{GRF,
	doi = {10.1145/3539618.3591992},
  
	url = {https://doi.org/10.1145%2F3539618.3591992},
  
	year = 2023,
	month = {jul},
  
	publisher = {{ACM}
},
  
	author = {Iain Mackie and Shubham Chatterjee and Jeffrey Dalton},
  
	title = {Generative Relevance Feedback with Large Language Models},
  
	booktitle = {Proceedings of the 46th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval}
}
@article{query_routing,
author = {Sarnikar, Surendra and Zhang, Zhu and Zhao, J.},
year = {2014},
month = {08},
pages = {},
title = {Query-Performance Prediction for Effective Query Routing in Domain-Specific Repositories},
volume = {65},
journal = {Journal of the Association for Information Science and Technology},
doi = {10.1002/asi.23072}
}
@inproceedings{qpp_adhoc,
	doi = {10.1145/3539618.3591919},
  
	url = {https://doi.org/10.1145%2F3539618.3591919},
  
	year = 2023,
	month = {jul},
  
	publisher = {{ACM}
},
  
	author = {Chuan Meng and Negar Arabzadeh and Mohammad Aliannejadi and Maarten de Rijke},
  
	title = {Query Performance Prediction: From Ad-hoc to Conversational Search},
  
	booktitle = {Proceedings of the 46th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval}
}
@inproceedings{listwise_exp,
	title        = {Towards Explainable Search Results: A Listwise Explanation Generator},
	author       = {Yu, Puxuan and Rahimi, Razieh and Allan, James},
	year         = 2022,
	booktitle    = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	location     = {Madrid, Spain},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {SIGIR '22},
	pages        = {669–680},
	doi          = {10.1145/3477495.3532067},
	isbn         = 9781450387323,
	url          = {https://doi.org/10.1145/3477495.3532067},
	abstract     = {It has been shown that the interpretability of search results is enhanced when query aspects covered by documents are explicitly provided. However, existing work on aspect-oriented explanation of search results explains each document independently. These explanations thus cannot describe the differences between documents. This issue is also true for existing models on query aspect generation. Furthermore, these models provide a single query aspect for each document, even though documents often cover multiple query aspects. To overcome these limitations, we propose LiEGe, an approach that jointly explains all documents in a search result list. LiEGe provides semantic representations at two levels of granularity -- documents and their tokens -- using different interaction signals including cross-document interactions. These allow listwise modeling of a search result list as well as the generation of coherent explanations for documents. To appropriately explain documents that cover multiple query aspects, we introduce two settings for search result explanation: comprehensive and novelty explanation generation. LiEGe is trained and evaluated for both settings. We evaluate LiEGe on datasets built from Wikipedia and real query logs of the Bing search engine. Our experimental results demonstrate that LiEGe outperforms all baselines, with improvements that are substantial and statistically significant.},
	keywords     = {query aspects, novelty and diversity, explainable search},
	numpages     = 12
}
@inproceedings{liu-etal-2020-unsupervised,
	title        = {Unsupervised Paraphrasing by Simulated Annealing},
	author       = {Liu, Xianggen  and Mou, Lili  and Meng, Fandong  and Zhou, Hao  and Zhou, Jie  and Song, Sen},
	year         = 2020,
	month        = jul,
	booktitle    = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {302--312},
	doi          = {10.18653/v1/2020.acl-main.28},
	url          = {https://aclanthology.org/2020.acl-main.28},
	abstract     = {We propose UPSA, a novel approach that accomplishes Unsupervised Paraphrasing by Simulated Annealing. We model paraphrase generation as an optimization problem and propose a sophisticated objective function, involving semantic similarity, expression diversity, and language fluency of paraphrases. UPSA searches the sentence space towards this objective by performing a sequence of local editing. We evaluate our approach on various datasets, namely, Quora, Wikianswers, MSCOCO, and Twitter. Extensive results show that UPSA achieves the state-of-the-art performance compared with previous unsupervised methods in terms of both automatic and human evaluations. Further, our approach outperforms most existing domain-adapted supervised models, showing the generalizability of UPSA.}
}
@inproceedings{liu-soh-2022-towards,
	title        = {Towards Better Characterization of Paraphrases},
	author       = {Liu, Timothy  and Soh, De Wen},
	year         = 2022,
	month        = may,
	booktitle    = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Dublin, Ireland},
	pages        = {8592--8601},
	doi          = {10.18653/v1/2022.acl-long.588},
	url          = {https://aclanthology.org/2022.acl-long.588},
	abstract     = {To effectively characterize the nature of paraphrase pairs without expert human annotation, we proposes two new metrics: word position deviation (WPD) and lexical deviation (LD). WPD measures the degree of structural alteration, while LD measures the difference in vocabulary used. We apply these metrics to better understand the commonly-used MRPC dataset and study how it differs from PAWS, another paraphrase identification dataset. We also perform a detailed study on MRPC and propose improvements to the dataset, showing that it improves generalizability of models trained on the dataset. Lastly, we apply our metrics to filter the output of a paraphrase generation model and show how it can be used to generate specific forms of paraphrases for data augmentation or robustness testing of NLP models.}
}

@inproceedings{rank_special,
  title={Query-level Ranker Specialization},
  author={Rolf Jagerman and Harrie Oosterhuis and M. de Rijke},
  booktitle={International Conference on the Theory of Information Retrieval},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:2034438}
}

@inproceedings{Fox1993CombinationOM,
  title={Combination of Multiple Searches},
  author={Edward A. Fox and Joseph A. Shaw},
  booktitle={Text Retrieval Conference},
  year={1993},
  url={https://api.semanticscholar.org/CorpusID:1309301}
}
@article{liu2009learning,
	title        = {Learning to rank for information retrieval},
	author       = {Liu, Tie-Yan and others},
	year         = 2009,
	journal      = {Foundations and Trends{\textregistered} in Information Retrieval},
	publisher    = {Now Publishers, Inc.},
	volume       = 3,
	number       = 3,
	pages        = {225--331}
}
@article{liu2021pre,
	title        = {Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
	author       = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2107.13586}
}
@inproceedings{lm_core,
	title        = {{LM}-{CORE}: Language Models with Contextually Relevant External Knowledge},
	author       = {Kaur, Jivat  and Bhatia, Sumit  and Aggarwal, Milan  and Bansal, Rachit  and Krishnamurthy, Balaji},
	year         = 2022,
	month        = jul,
	booktitle    = {Findings of the Association for Computational Linguistics: NAACL 2022},
	publisher    = {Association for Computational Linguistics},
	address      = {Seattle, United States},
	pages        = {750--769},
	doi          = {10.18653/v1/2022.findings-naacl.57},
	url          = {https://aclanthology.org/2022.findings-naacl.57},
	abstract     = {Large transformer-based pre-trained language models have achieved impressive performance on a variety of knowledge-intensive tasks and can capture factual knowledge in their parameters. We argue that storing large amounts of knowledge in the model parameters is sub-optimal given the ever-growing amounts of knowledge and resource requirements. We posit that a more efficient alternative is to provide explicit access to contextually relevant structured knowledge to the model and train it to use that knowledge. We present LM-CORE {--} a general framework to achieve this{--} that allows \textit{decoupling} of the language model training from the external knowledge source and allows the latter to be updated without affecting the already trained model. Experimental results show that LM-CORE, having access to external knowledge, achieves significant and robust outperformance over state-of-the-art knowledge-enhanced language models on knowledge probing tasks; can effectively handle knowledge updates; and performs well on two downstream tasks. We also present a thorough error analysis highlighting the successes and failures of LM-CORE. Our code and model checkpoints are publicly available.}
}
@inproceedings{lucchese:2022:sigir:ilmart,
	title        = {{ILMART:} Interpretable Ranking with Constrained LambdaMART},
	author       = {Claudio Lucchese and Franco Maria Nardini and Salvatore Orlando and Raffaele Perego and Alberto Veneri},
	year         = 2022,
	booktitle    = {{SIGIR} '22: The 45th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022},
	publisher    = {{ACM}},
	pages        = {2255--2259},
	doi          = {10.1145/3477495.3531840},
	url          = {https://doi.org/10.1145/3477495.3531840},
	bibsource    = {dblp computer science bibliography, https://dblp.org},
	biburl       = {https://dblp.org/rec/conf/sigir/LuccheseN00V22.bib},
	timestamp    = {Sun, 02 Oct 2022 01:00:00 +0200}
}
@article{lundberg2017unified,
	title        = {A unified approach to interpreting model predictions},
	author       = {Lundberg, Scott M and Lee, Su-In},
	year         = 2017,
	journal      = {Advances in neural information processing systems},
	volume       = 30
}

@misc{zerveas2020brown,
      title={Brown University at TREC Deep Learning 2019}, 
      author={George Zerveas and Ruochen Zhang and Leila Kim and Carsten Eickhoff},
      year={2020},
      eprint={2009.04016},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@inproceedings{riezler-etal-2007-statistical,
    title = "Statistical Machine Translation for Query Expansion in Answer Retrieval",
    author = "Riezler, Stefan  and
      Vasserman, Alexander  and
      Tsochantaridis, Ioannis  and
      Mittal, Vibhu  and
      Liu, Yi",
    booktitle = "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics",
    month = jun,
    year = "2007",
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P07-1059",
    pages = "464--471",
}

@misc{wang2020deep,
      title={Deep Reinforced Query Reformulation for Information Retrieval}, 
      author={Xiao Wang and Craig Macdonald and Iadh Ounis},
      year={2020},
      eprint={2007.07987},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@article{prf_effectiveness,
author = {Carpineto, Claudio and Romano, Giovanni},
title = {A Survey of Automatic Query Expansion in Information Retrieval},
year = {2012},
issue_date = {January 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/2071389.2071390},
doi = {10.1145/2071389.2071390},
abstract = {The relative ineffectiveness of information retrieval systems is largely caused by the inaccuracy with which a query formed by a few keywords models the actual user information need. One well known method to overcome this limitation is automatic query expansion (AQE), whereby the user’s original query is augmented by new features with a similar meaning. AQE has a long history in the information retrieval community but it is only in the last years that it has reached a level of scientific and experimental maturity, especially in laboratory settings such as TREC. This survey presents a unified view of a large number of recent approaches to AQE that leverage various data sources and employ very different principles and techniques. The following questions are addressed. Why is query expansion so important to improve search effectiveness? What are the main steps involved in the design and implementation of an AQE component? What approaches to AQE are available and how do they compare? Which issues must still be resolved before AQE becomes a standard component of large operational information retrieval systems (e.g., search engines)?},
journal = {ACM Comput. Surv.},
month = {jan},
articleno = {1},
numpages = {50},
keywords = {query refinement, pseudo-relevance feedback, search, document ranking, word associations, Query expansion}
}

@misc{ms_marco,
      title={MS MARCO: A Human Generated MAchine Reading COmprehension Dataset}, 
      author={Payal Bajaj and Daniel Campos and Nick Craswell and Li Deng and Jianfeng Gao and Xiaodong Liu and Rangan Majumder and Andrew McNamara and Bhaskar Mitra and Tri Nguyen and Mir Rosenberg and Xia Song and Alina Stoica and Saurabh Tiwary and Tong Wang},
      year={2018},
      eprint={1611.09268},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{zukerman-raskutti-2002-lexical,
    title = "Lexical Query Paraphrasing for Document Retrieval",
    author = "Zukerman, Ingrid  and
      Raskutti, Bhavani",
    booktitle = "{COLING} 2002: The 19th International Conference on Computational Linguistics",
    year = "2002",
    url = "https://aclanthology.org/C02-1161",
}

@incollection{rocchio71relevance,
  added-at = {2009-12-17T17:15:39.000+0100},
  author = {Rocchio, J. J.},
  biburl = {https://www.bibsonomy.org/bibtex/271ef756e9a6009bc06010ecbdb33f7a2/zeno},
  booktitle = {The Smart retrieval system - experiments in automatic document processing},
  editor = {Salton, G.},
  interhash = {c18d843e34fe4f8bd1d2438227857225},
  intrahash = {71ef756e9a6009bc06010ecbdb33f7a2},
  keywords = {information-retrieval},
  pages = {313--323},
  publisher = {Englewood Cliffs, NJ: Prentice-Hall},
  timestamp = {2009-12-17T17:35:11.000+0100},
  title = {Relevance feedback in information retrieval},
  year = 1971
}




@inproceedings{relevance_lm,
author = {Lavrenko, Victor and Croft, W. Bruce},
title = {Relevance Based Language Models},
year = {2001},
isbn = {1581133316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383952.383972},
doi = {10.1145/383952.383972},
abstract = {We explore the relation between classical probabilistic models of information retrieval and the emerging language modeling approaches. It has long been recognized that the primary obstacle to effective performance of classical models is the need to estimate arelevance model: probabilities of words in the relevant class. We propose a novel technique for estimating these probabilities using the query alone. We demonstrate that our technique can produce highly accurate relevance models, addressing important notions of synonymy and polysemy. Our experiments show relevance models outperforming baseline language modeling systems on TREC retrieval and TDT tracking tasks. The main contribution of this work is an effective formal method for estimating a relevance model with no training data.},
booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {120–127},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {SIGIR '01}
}
@article{wordnet,
author = {Miller, George A.},
title = {WordNet: A Lexical Database for English},
year = {1995},
issue_date = {Nov. 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/219717.219748},
doi = {10.1145/219717.219748},
abstract = {Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].},
journal = {Commun. ACM},
month = {nov},
pages = {39–41},
numpages = {3}
}
@misc{wang2023query2doc,
      title={Query2doc: Query Expansion with Large Language Models}, 
      author={Liang Wang and Nan Yang and Furu Wei},
      year={2023},
      eprint={2303.07678},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@misc{nogueira2019document,
      title={Document Expansion by Query Prediction}, 
      author={Rodrigo Nogueira and Wei Yang and Jimmy Lin and Kyunghyun Cho},
      year={2019},
      eprint={1904.08375},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}


@inproceedings{lv2009comparative,
	title        = {A comparative study of methods for estimating query language models with pseudo feedback},
	author       = {Lv, Yuanhua and Zhai, ChengXiang},
	year         = 2009,
	booktitle    = {Proceedings of the 18th ACM conference on Information and knowledge management},
	pages        = {1895--1898}
}
@inproceedings{martins2016softmax,
	title        = {From softmax to sparsemax: A sparse model of attention and multi-label classification},
	author       = {Martins, Andre and Astudillo, Ramon},
	year         = 2016,
	booktitle    = {International conference on machine learning},
	pages        = {1614--1623},
	organization = {PMLR}
}
@article{masoomi2020instance,
	title        = {Instance-wise Feature Grouping},
	author       = {Masoomi, Aria and Wu, Chieh and Zhao, Tingting and Wang, Zifeng and Castaldi, Peter and Dy, Jennifer},
	year         = 2020,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 33
}
@inproceedings{metzler2021:rethinking,
	title        = {Rethinking search: making domain experts out of dilettantes},
	author       = {Metzler, Donald and Tay, Yi and Bahri, Dara and Najork, Marc},
	year         = 2021,
	booktitle    = {ACM SIGIR Forum},
	volume       = 55,
	number       = 1,
	pages        = {1--27},
	organization = {ACM New York, NY, USA}
}
@article{miller1995wordnet,
	title        = {WordNet: a lexical database for English},
	author       = {Miller, George A},
	year         = 1995,
	journal      = {Communications of the ACM},
	publisher    = {ACM New York, NY, USA},
	volume       = 38,
	number       = 11,
	pages        = {39--41}
}
@article{Mitra2016a,
	title        = {A Dual Embedding Space Model for Document Ranking},
	author       = {Bhaskar Mitra and Eric T. Nalisnick and Nick Craswell and Rich Caruana},
	year         = 2016,
	journal      = {arXiv preprint},
	volume       = {arXiv:1602.01137},
	url          = {http://arxiv.org/abs/1602.01137},
	archiveprefix = {arXiv},
	eprint       = {1602.01137}
}
@inproceedings{mmr,
	title        = {The use of MMR, diversity-based reranking for reordering documents and producing summaries},
	author       = {Carbonell, Jaime and Goldstein, Jade},
	year         = 1998,
	booktitle    = {Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval},
	pages        = {335--336}
}
@book{molnar2020interpretable,
	title        = {Interpretable machine learning},
	author       = {Molnar, Christoph},
	year         = 2020,
	publisher    = {Lulu. com}
}
@article{mq2008:qin2010letor,
	title        = {LETOR: A benchmark collection for research on learning to rank for information retrieval},
	author       = {Qin, Tao and Liu, Tie-Yan and Xu, Jun and Li, Hang},
	year         = 2010,
	journal      = {Information Retrieval},
	publisher    = {Springer},
	volume       = 13,
	number       = 4,
	pages        = {346--374}
}
@inproceedings{nakano2021webgpt,
	title        = {WebGPT: Browser-assisted question-answering with human feedback},
	author       = {Reiichiro Nakano and Jacob Hilton and Suchir Balaji and Jeff Wu and Long Ouyang and Christina Kim and Christopher Hesse and Shantanu Jain and Vineet Kosaraju and William Saunders and Xu Jiang and Karl Cobbe and Tyna Eloundou and Gretchen Krueger and Kevin Button and Matthew Knight and Benjamin Chess and John Schulman},
	year         = 2021,
	booktitle    = {arXiv}
}
@inproceedings{narayan-etal-2016-paraphrase,
	title        = {Paraphrase Generation from Latent-Variable {PCFG}s for Semantic Parsing},
	author       = {Narayan, Shashi  and Reddy, Siva  and Cohen, Shay B.},
	year         = 2016,
	month        = sep # { 5-8},
	booktitle    = {Proceedings of the 9th International Natural Language Generation conference},
	publisher    = {Association for Computational Linguistics},
	address      = {Edinburgh, UK},
	pages        = {153--162},
	doi          = {10.18653/v1/W16-6625},
	url          = {https://aclanthology.org/W16-6625}
}
@article{nardini2022distilled,
	title        = {Distilled Neural Networks for Efficient Learning to Rank},
	author       = {Nardini, Franco Maria and Rulli, Cosimo and Trani, Salvatore and Venturini, Rossano},
	year         = 2022,
	journal      = {IEEE Transactions on Knowledge and Data Engineering},
	publisher    = {IEEE}
}
@article{natural_questions,
	title        = {Natural Questions: a Benchmark for Question Answering Research},
	author       = {Tom Kwiatkowski and Jennimaria Palomaki and Olivia Redfield and Michael Collins and Ankur Parikh and Chris Alberti and Danielle Epstein and Illia Polosukhin and Matthew Kelcey and Jacob Devlin and Kenton Lee and Kristina N. Toutanova and Llion Jones and Ming-Wei Chang and Andrew Dai and Jakob Uszkoreit and Quoc Le and Slav Petrov},
	year         = 2019,
	journal      = {Transactions of the Association of Computational Linguistics}
}
@incollection{NEURIPS2019_9015,
	title        = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
	author       = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	year         = 2019,
	booktitle    = {Advances in Neural Information Processing Systems 32},
	publisher    = {Curran Associates, Inc.},
	pages        = {8024--8035},
	url          = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
	editor       = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett}
}
@inproceedings{ng-etal-2020-ssmba,
	title        = {{SSMBA}: Self-Supervised Manifold Based Data Augmentation for Improving Out-of-Domain Robustness},
	author       = {Ng, Nathan  and Cho, Kyunghyun  and Ghassemi, Marzyeh},
	year         = 2020,
	month        = nov,
	booktitle    = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {1268--1283},
	doi          = {10.18653/v1/2020.emnlp-main.97},
	url          = {https://aclanthology.org/2020.emnlp-main.97},
	abstract     = {Models that perform well on a training domain often fail to generalize to out-of-domain (OOD) examples. Data augmentation is a common method used to prevent overfitting and improve OOD generalization. However, in natural language, it is difficult to generate new examples that stay on the underlying data manifold. We introduce SSMBA, a data augmentation method for generating synthetic training examples by using a pair of corruption and reconstruction functions to move randomly on a data manifold. We investigate the use of SSMBA in the natural language domain, leveraging the manifold assumption to reconstruct corrupted text with masked language models. In experiments on robustness benchmarks across 3 tasks and 9 datasets, SSMBA consistently outperforms existing data augmentation methods and baseline models on both in-domain and OOD data, achieving gains of 0.8{\%} on OOD Amazon reviews, 1.8{\%} accuracy on OOD MNLI, and 1.4 BLEU on in-domain IWSLT14 German-English.}
}
@inproceedings{niu-etal-2021-unsupervised,
	title        = {Unsupervised Paraphrasing with Pretrained Language Models},
	author       = {Niu, Tong  and Yavuz, Semih  and Zhou, Yingbo  and Keskar, Nitish Shirish  and Wang, Huan  and Xiong, Caiming},
	year         = 2021,
	month        = nov,
	booktitle    = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	publisher    = {Association for Computational Linguistics},
	address      = {Online and Punta Cana, Dominican Republic},
	pages        = {5136--5150},
	doi          = {10.18653/v1/2021.emnlp-main.417},
	url          = {https://aclanthology.org/2021.emnlp-main.417},
	abstract     = {Paraphrase generation has benefited extensively from recent progress in the designing of training objectives and model architectures. However, previous explorations have largely focused on supervised methods, which require a large amount of labeled data that is costly to collect. To address this drawback, we adopt a transfer learning approach and propose a training pipeline that enables pre-trained language models to generate high-quality paraphrases in an unsupervised setting. Our recipe consists of task-adaptation, self-supervision, and a novel decoding algorithm named Dynamic Blocking (DB). To enforce a surface form dissimilar from the input, whenever the language model emits a token contained in the source sequence, DB prevents the model from outputting the subsequent source token for the next generation step. We show with automatic and human evaluations that our approach achieves state-of-the-art performance on both the Quora Question Pair (QQP) and the ParaNMT datasets and is robust to domain shift between the two datasets of distinct distributions. We also demonstrate that our model transfers to paraphrasing in other languages without any additional finetuning.}
}
@article{nogueira2017task,
	title        = {Task-oriented query reformulation with reinforcement learning},
	author       = {Nogueira, Rodrigo and Cho, Kyunghyun},
	year         = 2017,
	journal      = {arXiv preprint arXiv:1704.04572}
}
@misc{nucleus_sampling,
	title        = {The Curious Case of Neural Text Degeneration},
	author       = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
	year         = 2019,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.1904.09751},
	url          = {https://arxiv.org/abs/1904.09751},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@inproceedings{pan2009featuretree,
	title        = {Feature selection for ranking using boosted trees},
	author       = {Pan, Feng and Converse, Tim and Ahn, David and Salvetti, Franco and Donato, Gianluca},
	year         = 2009,
	booktitle    = {Proceedings of the 18th ACM conference on Information and knowledge management},
	pages        = {2025--2028}
}
@article{pandey2018linear,
	title        = {Linear feature extraction for ranking},
	author       = {Pandey, Gaurav and Ren, Zhaochun and Wang, Shuaiqiang and Veijalainen, Jari and de Rijke, Maarten},
	year         = 2018,
	journal      = {Information Retrieval Journal},
	publisher    = {Springer},
	volume       = 21,
	number       = 6,
	pages        = {481--506}
}
@inproceedings{pang2020setrank,
	title        = {Setrank: Learning a permutation-invariant ranking model for information retrieval},
	author       = {Pang, Liang and Xu, Jun and Ai, Qingyao and Lan, Yanyan and Cheng, Xueqi and Wen, Jirong},
	year         = 2020,
	booktitle    = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages        = {499--508}
}
@inproceedings{paranjape-etal-2022-retrieval,
	title        = {Retrieval-guided Counterfactual Generation for {QA}},
	author       = {Paranjape, Bhargavi  and Lamm, Matthew  and Tenney, Ian},
	year         = 2022,
	month        = may,
	booktitle    = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Dublin, Ireland},
	pages        = {1670--1686},
	doi          = {10.18653/v1/2022.acl-long.117},
	url          = {https://aclanthology.org/2022.acl-long.117},
	abstract     = {Deep NLP models have been shown to be brittle to input perturbations. Recent work has shown that data augmentation using counterfactuals {---} i.e. minimally perturbed inputs {---} can help ameliorate this weakness. We focus on the task of creating counterfactuals for question answering, which presents unique challenges related to world knowledge, semantic diversity, and answerability. To address these challenges, we develop a Retrieve-Generate-Filter(RGF) technique to create counterfactual evaluation and training data with minimal human supervision. Using an open-domain QA framework and question generation model trained on original task data, we create counterfactuals that are fluent, semantically diverse, and automatically labeled. Data augmentation with RGF counterfactuals improves performance on out-of-domain and challenging evaluation sets over and above existing methods, in both the reading comprehension and open-domain QA settings. Moreover, we find that RGF data leads to significant improvements in a model{'}s robustness to local perturbations.}
}
@inproceedings{pavlick2015ppdb,
	title        = {PPDB 2.0: Better paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification},
	author       = {Pavlick, Ellie and Rastogi, Pushpendre and Ganitkevitch, Juri and Van Durme, Benjamin and Callison-Burch, Chris},
	year         = 2015,
	booktitle    = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
	pages        = {425--430}
}
@article{pca,
	title        = {LIII. On lines and planes of closest fit to systems of points in space},
	author       = {Karl   Pearson   F.R.S.},
	year         = 1901,
	journal      = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
	publisher    = {Taylor & Francis},
	volume       = 2,
	number       = 11,
	pages        = {559--572},
	doi          = {10.1080/14786440109462720}
}
@inproceedings{pennington2014glove,
	title        = {Glove: Global vectors for word representation},
	author       = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
	year         = 2014,
	booktitle    = {Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
	pages        = {1532--1543}
}
@inproceedings{polyjuice,
	title        = {Polyjuice: Generating Counterfactuals for Explaining, Evaluating, and Improving Models},
	author       = {Wu, Tongshuang  and Ribeiro, Marco Tulio  and Heer, Jeffrey  and Weld, Daniel},
	year         = 2021,
	month        = aug,
	booktitle    = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {6707--6723},
	doi          = {10.18653/v1/2021.acl-long.523},
	url          = {https://aclanthology.org/2021.acl-long.523},
	abstract     = {While counterfactual examples are useful for analysis and training of NLP models, current generation methods either rely on manual labor to create very few counterfactuals, or only instantiate limited types of perturbations such as paraphrases or word substitutions. We present Polyjuice, a general-purpose counterfactual generator that allows for control over perturbation types and locations, trained by finetuning GPT-2 on multiple datasets of paired sentences. We show that Polyjuice produces diverse sets of realistic counterfactuals, which in turn are useful in various distinct applications: improving training and evaluation on three different tasks (with around 70{\%} less annotation effort than manual generation), augmenting state-of-the-art explanation techniques, and supporting systematic counterfactual error analysis by revealing behaviors easily missed by human experts.}
}
@misc{PPLM,
	title        = {Plug and Play Language Models: A Simple Approach to Controlled Text Generation},
	author       = {Dathathri, Sumanth and Madotto, Andrea and Lan, Janice and Hung, Jane and Frank, Eric and Molino, Piero and Yosinski, Jason and Liu, Rosanne},
	year         = 2019,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.1912.02164},
	url          = {https://arxiv.org/abs/1912.02164},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@misc{ppo,
	title        = {Proximal Policy Optimization Algorithms},
	author       = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	year         = 2017,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.1707.06347},
	url          = {https://arxiv.org/abs/1707.06347},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@inproceedings{prakash-etal-2016-neural,
	title        = {Neural Paraphrase Generation with Stacked Residual {LSTM} Networks},
	author       = {Prakash, Aaditya  and Hasan, Sadid A.  and Lee, Kathy  and Datla, Vivek  and Qadir, Ashequl  and Liu, Joey  and Farri, Oladimeji},
	year         = 2016,
	month        = dec,
	booktitle    = {Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers},
	publisher    = {The COLING 2016 Organizing Committee},
	address      = {Osaka, Japan},
	pages        = {2923--2934},
	url          = {https://aclanthology.org/C16-1275},
	abstract     = {In this paper, we propose a novel neural approach for paraphrase generation. Conventional paraphrase generation methods either leverage hand-written rules and thesauri-based alignments, or use statistical machine learning principles. To the best of our knowledge, this work is the first to explore deep learning models for paraphrase generation. Our primary contribution is a stacked residual LSTM network, where we add residual connections between LSTM layers. This allows for efficient training of deep LSTMs. We evaluate our model and other state-of-the-art deep learning models on three different datasets: PPDB, WikiAnswers, and MSCOCO. Evaluation results demonstrate that our model outperforms sequence to sequence, attention-based, and bi-directional LSTM models on BLEU, METEOR, TER, and an embedding-based sentence similarity metric.}
}
@inproceedings{purpura2021neural,
	title        = {Neural feature selection for learning to rank},
	author       = {Purpura, Alberto and Buchner, Karolina and Silvello, Gianmaria and Susto, Gian Antonio},
	year         = 2021,
	booktitle    = {European Conference on Information Retrieval},
	pages        = {342--349},
	organization = {Springer}
}
@inproceedings{qin2021neural,
	title        = {Are Neural Rankers still Outperformed by Gradient Boosted Decision Trees?},
	author       = {Zhen Qin and Le Yan and Honglei Zhuang and Yi Tay and Rama Kumar Pasumarthi and Xuanhui Wang and Michael Bendersky and Marc Najork},
	year         = 2021,
	booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=Ut1vF\_q\_vC},
	bibsource    = {dblp computer science bibliography, https://dblp.org},
	biburl       = {https://dblp.org/rec/conf/iclr/0002YZTPWBN21.bib},
	timestamp    = {Thu, 02 Dec 2021 17:27:16 +0100}
}
@misc{quantization,
	title        = {A Comprehensive Survey on Model Quantization for Deep Neural Networks},
	author       = {Rokh, Babak and Azarpeyvand, Ali and Khanteymoori, Alireza},
	year         = 2022,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.2205.07877},
	url          = {https://arxiv.org/abs/2205.07877},
	copyright    = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
	keywords     = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@inproceedings{quirk-etal-2004-monolingual,
	title        = {Monolingual Machine Translation for Paraphrase Generation},
	author       = {Quirk, Chris  and Brockett, Chris  and Dolan, William},
	year         = 2004,
	month        = jul,
	booktitle    = {Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing},
	publisher    = {Association for Computational Linguistics},
	address      = {Barcelona, Spain},
	pages        = {142--149},
	url          = {https://aclanthology.org/W04-3219}
}
@article{radford2019language,
	title        = {Language Models are Unsupervised Multitask Learners},
	author       = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	year         = 2019
}
@article{radlinski2022natural,
	title        = {On Natural Language User Profiles for Transparent and Scrutable Recommendation},
	author       = {Radlinski, Filip and Balog, Krisztian and Diaz, Fernando and Dixon, Lucas and Wedin, Ben},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2205.09403}
}
@article{radlinski2022natural:scrutable,
	title        = {On Natural Language User Profiles for Transparent and Scrutable Recommendation},
	author       = {Radlinski, Filip and Balog, Krisztian and Diaz, Fernando and Dixon, Lucas and Wedin, Ben},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2205.09403}
}
@article{rahangdale2019deep,
	title        = {Deep neural network regularization for feature selection in learning-to-rank},
	author       = {Rahangdale, Ashwini and Raut, Shital},
	year         = 2019,
	journal      = {IEEE Access},
	publisher    = {IEEE},
	volume       = 7,
	pages        = {53988--54006}
}
@article{rajani2019explain,
	title        = {Explain yourself! leveraging language models for commonsense reasoning},
	author       = {Rajani, Nazneen Fatema and McCann, Bryan and Xiong, Caiming and Socher, Richard},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1906.02361}
}
@inproceedings{RankSVM-joachims2006training,
	title        = {Training linear SVMs in linear time},
	author       = {Joachims, Thorsten},
	year         = 2006,
	booktitle    = {Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining},
	pages        = {217--226}
}
% Embedded methods using l1
@article{rao2018learning,
	title        = {Learning to ask good questions: Ranking clarification questions using neural expected value of perfect information},
	author       = {Rao, Sudha and Daum{\'e} III, Hal},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1805.04655}
}
% Embedded methods limited to SVM.
@misc{Realm,
	title        = {REALM: Retrieval-Augmented Language Model Pre-Training},
	author       = {Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei},
	year         = 2020,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.2002.08909},
	url          = {https://arxiv.org/abs/2002.08909},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@misc{relevance_feedback,
	title        = {Incorporating Relevance Feedback for Information-Seeking Retrieval using Few-Shot Document Re-Ranking},
	author       = {Baumgärtner, Tim and Ribeiro, Leonardo F. R. and Reimers, Nils and Gurevych, Iryna},
	year         = 2022,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.2210.10695},
	url          = {https://arxiv.org/abs/2210.10695},
	copyright    = {Creative Commons Attribution 4.0 International},
	keywords     = {Information Retrieval (cs.IR), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
%Embedded methods limited to trees.
@misc{retro,
	title        = {Improving language models by retrieving from trillions of tokens},
	author       = {Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Driessche, George van den and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and Casas, Diego de Las and Guy, Aurelia and Menick, Jacob and Ring, Roman and Hennigan, Tom and Huang, Saffron and Maggiore, Loren and Jones, Chris and Cassirer, Albin and Brock, Andy and Paganini, Michela and Irving, Geoffrey and Vinyals, Oriol and Osindero, Simon and Simonyan, Karen and Rae, Jack W. and Elsen, Erich and Sifre, Laurent},
	year         = 2021,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.2112.04426},
	url          = {https://arxiv.org/abs/2112.04426},
	copyright    = {Creative Commons Attribution 4.0 International},
	keywords     = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@inproceedings{ribeiro2016should,
	title        = {" Why should i trust you?" Explaining the predictions of any classifier},
	author       = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	year         = 2016,
	booktitle    = {Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
	pages        = {1135--1144}
}
@inproceedings{rigutini2008sortnet,
	title        = {Sortnet: Learning to rank by a neural-based sorting algorithm},
	author       = {Rigutini, Leonardo and Papini, Tiziano and Maggini, Marco and Scarselli, Franco},
	year         = 2008,
	booktitle    = {In proceedings of the SIGIR 2008 Workshop on Learning to Rank for Information Retrieval (LR4IR)},
	volume       = 42,
	number       = 2,
	pages        = {76--79}
}
@misc{rlhf,
	title        = {Deep reinforcement learning from human preferences},
	author       = {Christiano, Paul and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
	year         = 2017,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.1706.03741},
	url          = {https://arxiv.org/abs/1706.03741},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Machine Learning (stat.ML), Artificial Intelligence (cs.AI), Human-Computer Interaction (cs.HC), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@inproceedings{rose2004understanding,
	title        = {Understanding user goals in web search},
	author       = {Rose, Daniel E and Levinson, Danny},
	year         = 2004,
	booktitle    = {Proceedings of the 13th international conference on World Wide Web},
	pages        = {13--19}
}
@inproceedings{ross-etal-2021-explaining,
	title        = {Explaining {NLP} Models via Minimal Contrastive Editing ({M}i{CE})},
	author       = {Ross, Alexis  and Marasovi{\'c}, Ana  and Peters, Matthew},
	year         = 2021,
	month        = aug,
	booktitle    = {Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {3840--3852},
	doi          = {10.18653/v1/2021.findings-acl.336},
	url          = {https://aclanthology.org/2021.findings-acl.336}
}
@inproceedings{rosset2020leading,
	title        = {Leading conversational search by suggesting useful questions},
	author       = {Rosset, Corbin and Xiong, Chenyan and Song, Xia and Campos, Daniel and Craswell, Nick and Tiwary, Saurabh and Bennett, Paul},
	year         = 2020,
	booktitle    = {Proceedings of the web conference 2020},
	pages        = {1160--1170}
}
@inbook{rotom,
	title        = {Rotom: A Meta-Learned Data Augmentation Framework for Entity Matching, Data Cleaning, Text Classification, and Beyond},
	author       = {Miao, Zhengjie and Li, Yuliang and Wang, Xiaolan},
	year         = 2021,
	booktitle    = {Proceedings of the 2021 International Conference on Management of Data},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	pages        = {1303–1316},
	isbn         = 9781450383431,
	url          = {https://doi.org/10.1145/3448016.3457258},
	abstract     = {Deep Learning revolutionizes almost all fields of computer science including data management. However, the demand for high-quality training data is slowing down deep neural nets' wider adoption. To this end, data augmentation (DA), which generates more labeled examples from existing ones, becomes a common technique. Meanwhile, the risk of creating noisy examples and the large space of hyper-parameters make DA less attractive in practice. We introduce Rotom, a multi-purpose data augmentation framework for a range of data management and mining tasks including entity matching, data cleaning, and text classification. Rotom features InvDA, a new DA operator that generates natural yet diverse augmented examples by formulating DA as a seq2seq task. The key technical novelty of Rotom is a meta-learning framework that automatically learns a policy for combining examples from different DA operators, whereby combinatorially reduces the hyper-parameters space. Our experimental results show that Rotom effectively improves a model's performance by combining multiple DA operators, even when applying them individually does not yield performance improvement. With this strength, Rotom outperforms the state-of-the-art entity matching and data cleaning systems in the low-resource settings as well as two recently proposed DA techniques for text classification.},
	numpages     = 14
}
@article{rudin:stopexplain,
	title        = {Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead},
	author       = {Cynthia Rudin},
	year         = 2019,
	journal      = {Nat. Mach. Intell.},
	volume       = 1,
	number       = 5,
	pages        = {206--215},
	doi          = {10.1038/s42256-019-0048-x},
	url          = {https://doi.org/10.1038/s42256-019-0048-x},
	bibsource    = {dblp computer science bibliography, https://dblp.org},
	biburl       = {https://dblp.org/rec/journals/natmi/Rudin19.bib},
	timestamp    = {Wed, 16 Mar 2022 23:50:17 +0100}
}
@article{self-attention-pobrotyn2020context,
	title        = {Context-aware learning to rank with self-attention},
	author       = {Pobrotyn, Przemys{\l}aw and Bartczak, Tomasz and Synowiec, Miko{\l}aj and Bia{\l}obrzeski, Rados{\l}aw and Bojar, Jaros{\l}aw},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2005.10084}
}
@inproceedings{selfexplaining,
	title        = {Towards Robust Interpretability with Self-Explaining Neural Networks},
	author       = {David Alvarez{-}Melis and Tommi S. Jaakkola},
	year         = 2018,
	booktitle    = {Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr{\'{e}}al, Canada},
	pages        = {7786--7795},
	url          = {https://proceedings.neurips.cc/paper/2018/hash/3e9f0fc9b2f89e043bc6233994dfcf76-Abstract.html},
	bibsource    = {dblp computer science bibliography, https://dblp.org},
	biburl       = {https://dblp.org/rec/conf/nips/Alvarez-MelisJ18.bib},
	editor       = {Samy Bengio and Hanna M. Wallach and Hugo Larochelle and Kristen Grauman and Nicol{\`{o}} Cesa{-}Bianchi and Roman Garnett},
	timestamp    = {Mon, 16 May 2022 15:41:51 +0200}
}
@article{shang1996tries,
	title        = {Tries for approximate string matching},
	author       = {Shang, Heping and Merrettal, TH},
	year         = 1996,
	journal      = {IEEE Transactions on Knowledge and Data Engineering},
	publisher    = {IEEE},
	volume       = 8,
	number       = 4,
	pages        = {540--547}
}
@inproceedings{shrikumar2017learning,
	title        = {Learning important features through propagating activation differences},
	author       = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
	year         = 2017,
	booktitle    = {International conference on machine learning},
	pages        = {3145--3153},
	organization = {PMLR}
}
@inproceedings{sigir2016-wang:exposure-bias,
	title        = {Learning to Rank with Selection Bias in Personal Search},
	author       = {Xuanhui Wang and Michael Bendersky and Donald Metzler and Marc Najork},
	year         = 2016,
	booktitle    = {SIGIR},
	pages        = {115--124}
}
@article{simonyan2013deep,
	title        = {Deep inside convolutional networks: Visualising image classification models and saliency maps},
	author       = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
	year         = 2013,
	journal      = {arXiv preprint arXiv:1312.6034}
}
@inproceedings{sordoni2015hierarchical,
	title        = {A hierarchical recurrent encoder-decoder for generative context-aware query suggestion},
	author       = {Sordoni, Alessandro and Bengio, Yoshua and Vahabi, Hossein and Lioma, Christina and Grue Simonsen, Jakob and Nie, Jian-Yun},
	year         = 2015,
	booktitle    = {proceedings of the 24th ACM international on conference on information and knowledge management},
	pages        = {553--562}
}
@inproceedings{sparcassist,
	title        = {SparCAssist: {A} Model Risk Assessment Assistant Based on Sparse Generated Counterfactuals},
	author       = {Zijian Zhang and Vinay Setty and Avishek Anand},
	year         = 2022,
	booktitle    = {{SIGIR} '22: The 45th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022},
	publisher    = {{ACM}},
	pages        = {3219--3223},
	doi          = {10.1145/3477495.3531677},
	url          = {https://doi.org/10.1145/3477495.3531677},
	bibsource    = {dblp computer science bibliography, https://dblp.org},
	biburl       = {https://dblp.org/rec/conf/sigir/0006SA22.bib},
	editor       = {Enrique Amig{\'{o}} and Pablo Castells and Julio Gonzalo and Ben Carterette and J. Shane Culpepper and Gabriella Kazai},
	timestamp    = {Sat, 09 Jul 2022 09:25:34 +0200}
}
@inproceedings{sparcassist,
	title        = {{SparCAssist}},
	author       = {Zijian Zhang and Vinay Setty and Avishek Anand},
	year         = 2022,
	month        = {jul},
	booktitle    = {Proceedings of the 45th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	publisher    = {{ACM}},
	doi          = {10.1145/3477495.3531677},
	url          = {https://doi.org/10.1145%2F3477495.3531677}
}
@misc{sparse_gpt,
	title        = {SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot},
	author       = {Frantar, Elias and Alistarh, Dan},
	year         = 2023,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.2301.00774},
	url          = {https://arxiv.org/abs/2301.00774},
	copyright    = {Creative Commons Attribution 4.0 International},
	keywords     = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@misc{ssmba,
	title        = {SSMBA: Self-Supervised Manifold Based Data Augmentation for Improving Out-of-Domain Robustness},
	author       = {Ng, Nathan and Cho, Kyunghyun and Ghassemi, Marzyeh},
	year         = 2020,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.2009.10195},
	url          = {https://arxiv.org/abs/2009.10195},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Computation and Language (cs.CL), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@inproceedings{stochasticscoring,
	title        = {A Stochastic Treatment of Learning to Rank Scoring Functions},
	author       = {Sebastian Bruch and Shuguang Han and Michael Bendersky and Marc Najork},
	year         = 2020,
	booktitle    = {{WSDM} '20: The Thirteenth {ACM} International Conference on Web Search and Data Mining, Houston, TX, USA, February 3-7, 2020},
	publisher    = {{ACM}},
	pages        = {61--69},
	doi          = {10.1145/3336191.3371844},
	url          = {https://doi.org/10.1145/3336191.3371844},
	bibsource    = {dblp computer science bibliography, https://dblp.org},
	biburl       = {https://dblp.org/rec/conf/wsdm/BruchHBN20.bib},
	editor       = {James Caverlee and Xia (Ben) Hu and Mounia Lalmas and Wei Wang},
	timestamp    = {Sun, 25 Oct 2020 22:34:54 +0100}
}
@inproceedings{stoyanchev2014towards,
	title        = {Towards natural clarification questions in dialogue systems},
	author       = {Stoyanchev, Svetlana and Liu, Alex and Hirschberg, Julia},
	year         = 2014,
	booktitle    = {AISB symposium on questions, discourse and dialogue},
	volume       = 20
}
@article{su2017cross,
	title        = {Cross-domain semantic parsing via paraphrasing},
	author       = {Su, Yu and Yan, Xifeng},
	year         = 2017,
	journal      = {arXiv preprint arXiv:1704.05974}
}
@inproceedings{sun2009rsrank,
	title        = {Robust sparse rank learning for non-smooth ranking measures},
	author       = {Sun, Zhengya and Qin, Tao and Tao, Qing and Wang, Jue},
	year         = 2009,
	booktitle    = {Proceedings of the 32nd international acm sigir conference on research and development in information retrieval},
	pages        = {259--266}
}
@inproceedings{sundararajan2017axiomatic,
	title        = {Axiomatic attribution for deep networks},
	author       = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
	year         = 2017,
	booktitle    = {International conference on machine learning},
	pages        = {3319--3328},
	organization = {PMLR}
}
@misc{tabnet-survey,
	title        = {Deep Neural Networks and Tabular Data: A Survey},
	author       = {Borisov, Vadim and Leemann, Tobias and Seßler, Kathrin and Haug, Johannes and Pawelczyk, Martin and Kasneci, Gjergji},
	year         = 2021,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.2110.01889},
	url          = {https://arxiv.org/abs/2110.01889},
	copyright    = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
	keywords     = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@article{tay2022:differentiable:index,
	title        = {Transformer memory as a differentiable search index},
	author       = {Tay, Yi and Tran, Vinh Q and Dehghani, Mostafa and Ni, Jianmo and Bahri, Dara and Mehta, Harsh and Qin, Zhen and Hui, Kai and Zhao, Zhe and Gupta, Jai and others},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2202.06991}
}
@inproceedings{taylor2008softrank,
	title        = {Softrank: optimizing non-smooth rank metrics},
	author       = {Taylor, Michael and Guiver, John and Robertson, Stephen and Minka, Tom},
	year         = 2008,
	booktitle    = {Proceedings of the 2008 International Conference on Web Search and Data Mining},
	pages        = {77--86}
}
@inproceedings{thorne-etal-2018-fever,
	title        = {{FEVER}: a Large-scale Dataset for Fact Extraction and {VER}ification},
	author       = {Thorne, James  and Vlachos, Andreas  and Christodoulopoulos, Christos  and Mittal, Arpit},
	year         = 2018,
	month        = jun,
	booktitle    = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {New Orleans, Louisiana},
	pages        = {809--819},
	doi          = {10.18653/v1/N18-1074},
	url          = {https://aclanthology.org/N18-1074},
	abstract     = {In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo by annotators achieving 0.6841 in Fleiss kappa. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87{\%}, while if we ignore the evidence we achieve 50.91{\%}. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources.}
}
@inproceedings{trienes2019identifying,
	title        = {Identifying unclear questions in community question answering websites},
	author       = {Trienes, Jan and Balog, Krisztian},
	year         = 2019,
	booktitle    = {Advances in Information Retrieval: 41st European Conference on IR Research, ECIR 2019, Cologne, Germany, April 14--18, 2019, Proceedings, Part I 41},
	pages        = {276--289},
	organization = {Springer}
}
@misc{uda,
	title        = {Unsupervised Data Augmentation for Consistency Training},
	author       = {Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Minh-Thang and Le, Quoc V.},
	year         = 2019,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.1904.12848},
	url          = {https://arxiv.org/abs/1904.12848},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@misc{unlikelihood_training,
	title        = {Neural Text Generation with Unlikelihood Training},
	author       = {Welleck, Sean and Kulikov, Ilia and Roller, Stephen and Dinan, Emily and Cho, Kyunghyun and Weston, Jason},
	year         = 2019,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.1908.04319},
	url          = {https://arxiv.org/abs/1908.04319},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Machine Learning (cs.LG), Computation and Language (cs.CL), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@misc{unnatural_instructions,
	title        = {Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor},
	author       = {Honovich, Or and Scialom, Thomas and Levy, Omer and Schick, Timo},
	year         = 2022,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.2212.09689},
	url          = {https://arxiv.org/abs/2212.09689},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@misc{uPA,
	title        = {Unsupervised Paraphrase Generation using Pre-trained Language Models},
	author       = {Hegde, Chaitra and Patil, Shrikumar},
	year         = 2020,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.2006.05477},
	url          = {https://arxiv.org/abs/2006.05477},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@article{userstudy-survey,
	title        = {Towards Human-centered Explainable {AI:} User Studies for Model Explanations},
	author       = {Yao Rong and Tobias Leemann and Thai{-}trang Nguyen and Lisa Fiedler and Tina Seidel and Gjergji Kasneci and Enkelejda Kasneci},
	year         = 2022,
	journal      = {CoRR},
	volume       = {abs/2210.11584},
	doi          = {10.48550/arXiv.2210.11584},
	url          = {https://doi.org/10.48550/arXiv.2210.11584},
	bibsource    = {dblp computer science bibliography, https://dblp.org},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2210-11584.bib},
	eprint       = {2210.11584},
	eprinttype   = {arXiv},
	timestamp    = {Tue, 25 Oct 2022 14:25:08 +0200}
}
@inproceedings{vakulenko2021comparison,
	title        = {A comparison of question rewriting methods for conversational passage retrieval},
	author       = {Vakulenko, Svitlana and Voskarides, Nikos and Tu, Zhucheng and Longpre, Shayne},
	year         = 2021,
	booktitle    = {European Conference on Information Retrieval},
	pages        = {418--424},
	organization = {Springer}
}
@inproceedings{vakulenko2021question,
	title        = {Question rewriting for conversational question answering},
	author       = {Vakulenko, Svitlana and Longpre, Shayne and Tu, Zhucheng and Anantha, Raviteja},
	year         = 2021,
	booktitle    = {Proceedings of the 14th ACM international conference on web search and data mining},
	pages        = {355--363}
}
@article{vaswani2017attention,
	title        = {Attention is all you need},
	author       = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
	year         = 2017,
	journal      = {Advances in neural information processing systems},
	volume       = 30
}
inproceedings{vaswani2017attention,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year      = {2017},
  booktitle = {Advances in neural information processing systems},
  pages     = {5998--6008},
  title     = {Attention is all you need}
}
@misc{vijayakumar2018diverse,
	title        = {Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models},
	author       = {Ashwin K Vijayakumar and Michael Cogswell and Ramprasath R. Selvaraju and Qing Sun and Stefan Lee and David Crandall and Dhruv Batra},
	year         = 2018,
	archiveprefix = {arXiv},
	eprint       = {1610.02424},
	primaryclass = {cs.AI}
}
@article{wang_transformer,
	title        = {A task in a suit and a tie: paraphrase generation with semantic augmentation},
	author       = {Wang, Su and Gupta, Rahul and Chang, Nancy and Baldridge, Jason},
	year         = 2018,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.1811.00119},
	url          = {https://arxiv.org/abs/1811.00119},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@inproceedings{wang-2017-liar,
	title        = {{``}Liar, Liar Pants on Fire{''}: A New Benchmark Dataset for Fake News Detection},
	author       = {Wang, William Yang},
	year         = 2017,
	month        = jul,
	booktitle    = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Vancouver, Canada},
	pages        = {422--426},
	doi          = {10.18653/v1/P17-2067},
	url          = {https://aclanthology.org/P17-2067},
	abstract     = {Automatic fake news detection is a challenging problem in deception detection, and it has tremendous real-world political and social impacts. However, statistical approaches to combating fake news has been dramatically limited by the lack of labeled benchmark datasets. In this paper, we present LIAR: a new, publicly available dataset for fake news detection. We collected a decade-long, 12.8K manually labeled short statements in various contexts from PolitiFact.com, which provides detailed analysis report and links to source documents for each case. This dataset can be used for fact-checking research as well. Notably, this new dataset is an order of magnitude larger than previously largest public fake news datasets of similar type. Empirically, we investigate automatic fake news detection based on surface-level linguistic patterns. We have designed a novel, hybrid convolutional neural network to integrate meta-data with text. We show that this hybrid approach can improve a text-only deep learning model.}
}

@inproceedings{query_substitution,
author = {Jones, Rosie and Rey, Benjamin and Madani, Omid and Greiner, Wiley},
title = {Generating Query Substitutions},
year = {2006},
isbn = {1595933239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1135777.1135835},
doi = {10.1145/1135777.1135835},
abstract = {We introduce the notion of query substitution, that is, generating a new query to replace a user's original search query. Our technique uses modifications based on typical substitutions web searchers make to their queries. In this way the new query is strongly related to the original query, containing terms closely related to all of the original terms. This contrasts with query expansion through pseudo-relevance feedback, which is costly and can lead to query drift. This also contrasts with query relaxation through boolean or TFIDF retrieval, which reduces the specificity of the query. We define a scale for evaluating query substitution, and show that our method performs well at generating new queries related to the original queries. We build a model for selecting between candidates, by using a number of features relating the query-candidate pair, and by fitting the model to human judgments of relevance of query suggestions. This further improves the quality of the candidates generated. Experiments show that our techniques significantly increase coverage and effectiveness in the setting of sponsored search.},
booktitle = {Proceedings of the 15th International Conference on World Wide Web},
pages = {387–396},
numpages = {10},
keywords = {query substitution, sponsored search, paraphrasing, query rewriting},
location = {Edinburgh, Scotland},
series = {WWW '06}
}

@article{wang2020deep:query:rewrites,
	title        = {Deep reinforced query reformulation for information retrieval},
	author       = {Wang, Xiao and Macdonald, Craig and Ounis, Iadh},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2007.07987}
}
@inproceedings{wang2021queen,
	title        = {QUEEN: Neural Query Rewriting in E-commerce},
	author       = {Wang, Yaxuan and Lu, Hanqing and Xu, Yunwen and Goutam, Rahul and Song, Yiwei and Yin, Bing},
	year         = 2021,
	booktitle    = {The Web Conference 2021, Workshop on Knowledge Management in E-Commerce}
}
@article{wang2023zero,
	title        = {Zero-shot Clarifying Question Generation for Conversational Search},
	author       = {Wang, Zhenduo and Tu, Yuancheng and Rosset, Corby and Craswell, Nick and Wu, Ming and Ai, Qingyao},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2301.12660}
}
@misc{web_gpt,
	title        = {WebGPT: Browser-assisted question-answering with human feedback},
	author       = {Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and Jiang, Xu and Cobbe, Karl and Eloundou, Tyna and Krueger, Gretchen and Button, Kevin and Knight, Matthew and Chess, Benjamin and Schulman, John},
	year         = 2021,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.2112.09332},
	url          = {https://arxiv.org/abs/2112.09332},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@article{web30k:DBLP:journals/corr/QinL13,
	title        = {Introducing {LETOR} 4.0 Datasets},
	author       = {Tao Qin and Tie{-}Yan Liu},
	year         = 2013,
	journal      = {CoRR},
	volume       = {abs/1306.2597},
	url          = {http://arxiv.org/abs/1306.2597},
	bibsource    = {dblp computer science bibliography, http://dblp.org},
	biburl       = {http://dblp.uni-trier.de/rec/bib/journals/corr/QinL13},
	timestamp    = {Mon, 01 Jul 2013 20:31:25 +0200}
}
@article{wei2021finetuned,
	title        = {Finetuned language models are zero-shot learners},
	author       = {Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2109.01652}
}
@inproceedings{wolf-etal-2020-transformers,
	title        = {Transformers: State-of-the-Art Natural Language Processing},
	author       = {Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},
	year         = 2020,
	month        = oct,
	booktitle    = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {38--45},
	url          = {https://www.aclweb.org/anthology/2020.emnlp-demos.6}
}
@inproceedings{wsdm2017-joachims:position-bias,
	title        = {Unbiased Learning-to-Rank with Biased Feedback},
	author       = {Thorsten Joachims and Adith Swaminathan and Tobias Schnabel},
	year         = 2017,
	booktitle    = {WSDM},
	pages        = {781--789}
}
@article{wu2010adapting,
	title        = {Adapting boosting for information retrieval measures},
	author       = {Wu, Qiang and Burges, Christopher JC and Svore, Krysta M and Gao, Jianfeng},
	year         = 2010,
	journal      = {Information Retrieval},
	publisher    = {Springer},
	volume       = 13,
	number       = 3,
	pages        = {254--270}
}
@article{wu2021conqrr,
	title        = {CONQRR: Conversational Query Rewriting for Retrieval with Reinforcement Learning},
	author       = {Wu, Zeqiu and Luan, Yi and Rashkin, Hannah and Reitter, David and Tomar, Gaurav Singh},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2112.08558}
}
@inproceedings{www2020-ovaisi:selection-bias,
	title        = {Correcting for Selection Bias in Learning-to-Rank Systems},
	author       = {Zohreh Ovaisi and Ragib Ahsan and Yifan Zhang and Kathryn Vasilaky and Elena Zheleva},
	year         = 2020,
	booktitle    = {Proceedings of The Web Conference 2020},
	pages        = {1863--1873}
}
@inproceedings{www2021-zheng:trust-bias,
	title        = {Disentangling User Interest and Conformity for Recommendation with Causal Embedding},
	author       = {Yu Zheng and Chen Gao and Xiang Li and Xiangnan He and Yong Li and Depeng Jin},
	year         = 2021,
	booktitle    = {Proceedings of the Web Conference 2021},
	pages        = {2980--2991}
}
@inproceedings{xia2008listwise,
	title        = {Listwise approach to learning to rank: theory and algorithm},
	author       = {Xia, Fen and Liu, Tie-Yan and Wang, Jue and Zhang, Wensheng and Li, Hang},
	year         = 2008,
	booktitle    = {Proceedings of the 25th international conference on Machine learning},
	pages        = {1192--1199}
}
@inproceedings{xquad,
	title        = {Exploiting Query Reformulations for Web Search Result Diversification},
	author       = {Santos, Rodrygo L.T. and Macdonald, Craig and Ounis, Iadh},
	year         = 2010,
	booktitle    = {Proceedings of the 19th International Conference on World Wide Web},
	location     = {Raleigh, North Carolina, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {WWW '10},
	pages        = {881–890},
	doi          = {10.1145/1772690.1772780},
	isbn         = 9781605587998,
	url          = {https://doi.org/10.1145/1772690.1772780},
	abstract     = {When a Web user's underlying information need is not clearly specified from the initial query, an effective approach is to diversify the results retrieved for this query. In this paper, we introduce a novel probabilistic framework for Web search result diversification, which explicitly accounts for the various aspects associated to an underspecified query. In particular, we diversify a document ranking by estimating how well a given document satisfies each uncovered aspect and the extent to which different aspects are satisfied by the ranking as a whole. We thoroughly evaluate our framework in the context of the diversity task of the TREC 2009 Web track. Moreover, we exploit query reformulations provided by three major Web search engines (WSEs) as a means to uncover different query aspects. The results attest the effectiveness of our framework when compared to state-of-the-art diversification approaches in the literature. Additionally, by simulating an upper-bound query reformulation mechanism from official TREC data, we draw useful insights regarding the effectiveness of the query reformulations generated by the different WSEs in promoting diversity.},
	keywords     = {relevance, diversity, web search},
	numpages     = 10
}

@inproceedings{yilmaz2019cross,
  title={Cross-domain modeling of sentence-level evidence for document retrieval},
  author={Yilmaz, Zeynep Akkalyoncu and Yang, Wei and Zhang, Haotian and Lin, Jimmy},
  booktitle={Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP)},
  pages={3490--3496},
  year={2019}
}

@inproceedings{rudra2020distant,
  title={Distant supervision in BERT-based adhoc document retrieval},
  author={Rudra, Koustav and Anand, Avishek},
  booktitle={Proceedings of the 29th ACM International Conference on Information and Knowledge Management},
  pages={2197--2200},
  year={2020}
}
@misc{zuo2022contextaware,
      title={Context-Aware Query Rewriting for Improving Users' Search Experience on E-commerce Websites}, 
      author={Simiao Zuo and Qingyu Yin and Haoming Jiang and Shaohui Xi and Bing Yin and Chao Zhang and Tuo Zhao},
      year={2022},
      eprint={2209.07584},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@inproceedings{dai_sigir_2019,
 author = {Dai, Zhuyun and Callan, Jamie},
 title = {Deeper Text Understanding for IR with Contextual Neural Language Modeling},
 booktitle = {ACM SIGIR'19},
 series = {},
 pages = {985--988},
 year = {2019}
} 
@inproceedings{zhang-etal-2022-active,
    title = "Active Example Selection for In-Context Learning",
    author = "Zhang, Yiming  and
      Feng, Shi  and
      Tan, Chenhao",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.622",
    pages = "9134--9148",
    abstract = "With a handful of demonstration examples, large-scale language models demonstrate strong capability to perform various tasks by in-context learning from these examples, without any fine-tuning. We demonstrate that in-context learning performance can be highly unstable across samples of examples, indicating the idiosyncrasies of how language models acquire information. We formulate example selection for in-context learning as a sequential decision problem, and propose a reinforcement learning algorithm for identifying generalizable policies to select demonstration examples. For GPT-2, our learned policies demonstrate strong abilities of generalizing to unseen tasks in training, with a 5.8{\%} improvement on average. Examples selected from our learned policies can even achieve a small improvement on GPT-3 Ada. However, the improvement diminishes on larger GPT-3 models, suggesting emerging capabilities of large language models.",
}

@inproceedings{xu2007adarank,
	title        = {Adarank: a boosting algorithm for information retrieval},
	author       = {Xu, Jun and Li, Hang},
	year         = 2007,
	booktitle    = {Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval},
	pages        = {391--398}
}
@inproceedings{xu2014gradient,
	title        = {Gradient boosted feature selection},
	author       = {Xu, Zhixiang and Huang, Gao and Weinberger, Kilian Q and Zheng, Alice X},
	year         = 2014,
	booktitle    = {Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining},
	pages        = {522--531}
}
@inproceedings{yang2011l2,
	title        = {L2, 1-norm regularized discriminative feature selection for unsupervised},
	author       = {Yang, Yi and Shen, Heng Tao and Ma, Zhigang and Huang, Zi and Zhou, Xiaofang},
	year         = 2011,
	booktitle    = {Twenty-Second International Joint Conference on Artificial Intelligence}
}
@inproceedings{yoon2018invase,
	title        = {INVASE: Instance-wise variable selection using neural networks},
	author       = {Yoon, Jinsung and Jordon, James and van der Schaar, Mihaela},
	year         = 2018,
	booktitle    = {International Conference on Learning Representations}
}
@inproceedings{yu2009efficient,
	title        = {Efficient feature weighting methods for ranking},
	author       = {Yu, Hwanjo and Oh, Jinoh and Han, Wook-Shin},
	year         = 2009,
	booktitle    = {Proceedings of the 18th acm conference on information and knowledge management},
	pages        = {1157--1166}
}
@article{yu2018qanet,
	title        = {Qanet: Combining local convolution with global self-attention for reading comprehension},
	author       = {Yu, Adams Wei and Dohan, David and Luong, Minh-Thang and Zhao, Rui and Chen, Kai and Norouzi, Mohammad and Le, Quoc V},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1804.09541}
}
@inproceedings{yu2020few,
	title        = {Few-shot generative conversational query rewriting},
	author       = {Yu, Shi and Liu, Jiahua and Yang, Jingqin and Xiong, Chenyan and Bennett, Paul and Gao, Jianfeng and Liu, Zhiyuan},
	year         = 2020,
	booktitle    = {Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval},
	pages        = {1933--1936}
}
@article{yuan2022mcqueen,
	title        = {McQueen: a Benchmark for Multimodal Conversational Query Rewrite},
	author       = {Yuan, Yifei and Shi, Chen and Wang, Runze and Chen, Liyi and Jiang, Feijun and You, Yuan and Lam, Wai},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2210.12775}
}
@inproceedings{yue2007support,
	title        = {A support vector method for optimizing average precision},
	author       = {Yue, Yisong and Finley, Thomas and Radlinski, Filip and Joachims, Thorsten},
	year         = 2007,
	booktitle    = {Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval},
	pages        = {271--278}
}
@inproceedings{zaidan2007using,
	title        = {Using “annotator rationales” to improve machine learning for text categorization},
	author       = {Zaidan, Omar and Eisner, Jason and Piatko, Christine},
	year         = 2007,
	booktitle    = {Human language technologies 2007: The conference of the North American chapter of the association for computational linguistics; proceedings of the main conference},
	pages        = {260--267}
}
@inproceedings{zamani2020generating,
	title        = {Generating clarifying questions for information retrieval},
	author       = {Zamani, Hamed and Dumais, Susan and Craswell, Nick and Bennett, Paul and Lueck, Gord},
	year         = 2020,
	booktitle    = {Proceedings of the web conference 2020},
	pages        = {418--428}
}
@article{zamani2022:reml,
	title        = {Retrieval-enhanced machine learning},
	author       = {Zamani, Hamed and Diaz, Fernando and Dehghani, Mostafa and Metzler, Donald and Bendersky, Michael},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2205.01230}
}
@article{zamani2022conversational,
	title        = {Conversational information seeking},
	author       = {Zamani, Hamed and Trippas, Johanne R and Dalton, Jeff and Radlinski, Filip},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2201.08808}
}
@inproceedings{zhang-etal-2019-syntax-infused,
	title        = {Syntax-Infused Variational Autoencoder for Text Generation},
	author       = {Zhang, Xinyuan  and Yang, Yi  and Yuan, Siyang  and Shen, Dinghan  and Carin, Lawrence},
	year         = 2019,
	month        = jul,
	booktitle    = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	publisher    = {Association for Computational Linguistics},
	address      = {Florence, Italy},
	pages        = {2069--2078},
	doi          = {10.18653/v1/P19-1199},
	url          = {https://aclanthology.org/P19-1199},
	abstract     = {We present a syntax-infused variational autoencoder (SIVAE), that integrates sentences with their syntactic trees to improve the grammar of generated sentences. Distinct from existing VAE-based text generative models, SIVAE contains two separate latent spaces, for sentences and syntactic trees. The evidence lower bound objective is redesigned correspondingly, by optimizing a joint distribution that accommodates two encoders and two decoders. SIVAE works with long short-term memory architectures to simultaneously generate sentences and syntactic trees. Two versions of SIVAE are proposed: one captures the dependencies between the latent variables through a conditional prior network, and the other treats the latent variables independently such that syntactically-controlled sentence generation can be performed. Experimental results demonstrate the generative superiority of SIVAE on both reconstruction and targeted syntactic evaluations. Finally, we show that the proposed models can be used for unsupervised paraphrasing given different syntactic tree templates.}
}
@inproceedings{zhang:2021:wsdm:expred,
	title        = {Explain and Predict, and then Predict Again},
	author       = {Zijian Zhang and Koustav Rudra and Avishek Anand},
	year         = 2021,
	booktitle    = {{WSDM} '21, The Fourteenth {ACM} International Conference on Web Search and Data Mining, Virtual Event, Israel, March 8-12, 2021},
	publisher    = {{ACM}},
	pages        = {418--426},
	doi          = {10.1145/3437963.3441758},
	url          = {https://doi.org/10.1145/3437963.3441758},
	bibsource    = {dblp computer science bibliography, https://dblp.org},
	biburl       = {https://dblp.org/rec/conf/wsdm/0006RA21.bib},
	timestamp    = {Wed, 07 Apr 2021 01:00:00 +0200}
}
@inproceedings{zhuang:2021:wsdm:gam,
	title        = {Interpretable Ranking with Generalized Additive Models},
	author       = {Honglei Zhuang and Xuanhui Wang and Michael Bendersky and Alexander Grushetsky and Yonghui Wu and Petr Mitrichev and Ethan Sterling and Nathan Bell and Walker Ravina and Hai Qian},
	year         = 2021,
	booktitle    = {{WSDM} '21, The Fourteenth {ACM} International Conference on Web Search and Data Mining, Virtual Event, Israel, March 8-12, 2021},
	publisher    = {{ACM}},
	pages        = {499--507},
	doi          = {10.1145/3437963.3441796},
	url          = {https://doi.org/10.1145/3437963.3441796},
	bibsource    = {dblp computer science bibliography, https://dblp.org},
	biburl       = {https://dblp.org/rec/conf/wsdm/ZhuangWBGWMSBRQ21.bib},
	timestamp    = {Wed, 07 Apr 2021 01:00:00 +0200}
}
@misc{paired_significance_test,
  key="paired_significance_test",
  title={{Pairwise t-test on TREC Run Files}},
  author={Luke Gallagher},
  year={2019},
  howpublished={\url{https://github.com/lgrz/pairwise-ttest/}}
}

@article{nogueira_prr_2019,
  author    = {Rodrigo Nogueira and
               Kyunghyun Cho},
  title     = {Passage Re-ranking with {BERT}},
  journal   = {CoRR},
  volume    = {abs/1901.04085},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.04085}
}

@inproceedings{rouge2004package,
  title={A package for automatic evaluation of summaries},
  author={ROUGE, Lin CY},
  booktitle={Proceedings of Workshop on Text Summarization of ACL, Spain},
  year={2004}
}

@inproceedings{cambazoglu2021intent,
  title={An intent taxonomy for questions asked in web search},
  author={Cambazoglu, B Barla and Tavakoli, Leila and Scholer, Falk and Sanderson, Mark and Croft, Bruce},
  booktitle={Proceedings of the 2021 Conference on Human Information Interaction and Retrieval},
  pages={85--94},
  year={2021}
}

__________________________

@INPROCEEDINGS{9288228,
  author={Samarinas, Chris and Hsu, Wynne and Lee, Mong Li},
  booktitle={2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={Latent Retrieval for Large-Scale Fact-Checking and Question Answering with NLI training}, 
  year={2020},
  volume={},
  number={},
  pages={941-948},
  keywords={Training;Semantic search;Pandemics;Conferences;Tools;Knowledge discovery;Engines;passage retrieval;fact-checking;question answering;natural language inference},
  doi={10.1109/ICTAI50040.2020.00147}}

@article{ji2023,
author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
title = {Survey of Hallucination in Natural Language Generation},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {12},
issn = {0360-0300},
url = {https://doi.org/10.1145/3571730},
doi = {10.1145/3571730},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {248},
numpages = {38},
keywords = {consistency in NLG, faithfulness in NLG, intrinsic hallucination, factuality in NLG, extrinsic hallucination, Hallucination}
}

@inproceedings{zhang2022retgen,
      title={RetGen: A Joint framework for Retrieval and Grounded Text Generation Modeling}, 
      author={Yizhe Zhang and Siqi Sun and Xiang Gao and Yuwei Fang and Chris Brockett and Michel Galley and Jianfeng Gao and Bill Dolan},
      year={2022},
      eprint={2105.06597},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{sun2022contrastive,
      title={Contrastive Learning Reduces Hallucination in Conversations}, 
      author={Weiwei Sun and Zhengliang Shi and Shen Gao and Pengjie Ren and Maarten de Rijke and Zhaochun Ren},
      year={2022},
      eprint={2212.10400},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{west2022probing,
      title={Probing Factually Grounded Content Transfer with Factual Ablation}, 
      author={Peter West and Chris Quirk and Michel Galley and Yejin Choi},
      year={2022},
      eprint={2203.10133},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{zakka2023almanac,
      title={Almanac: Knowledge-Grounded Language Models for Clinical Medicine}, 
      author={Cyril Zakka and Akash Chaurasia and Rohan Shad and William Hiesinger},
      year={2023},
      eprint={2303.01229},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{borji2023categorical,
      title={A Categorical Archive of ChatGPT Failures}, 
      author={Ali Borji},
      year={2023},
      eprint={2302.03494},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{tam2022evaluating,
      title={Evaluating the Factual Consistency of Large Language Models Through Summarization}, 
      author={Derek Tam and Anisha Mascarenhas and Shiyue Zhang and Sarah Kwan and Mohit Bansal and Colin Raffel},
      year={2022},
      eprint={2211.08412},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{
lan2023copy,
title={Copy is All You Need},
author={Tian Lan and Deng Cai and Yan Wang and Heyan Huang and Xian-Ling Mao},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=CROlOA9Nd8C}
}

@misc{alon2022neurosymbolic,
      title={Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval}, 
      author={Uri Alon and Frank F. Xu and Junxian He and Sudipta Sengupta and Dan Roth and Graham Neubig},
      year={2022},
      eprint={2201.12431},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{khandelwal2020generalization,
      title={Generalization through Memorization: Nearest Neighbor Language Models}, 
      author={Urvashi Khandelwal and Omer Levy and Dan Jurafsky and Luke Zettlemoyer and Mike Lewis},
      year={2020},
      eprint={1911.00172},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{hernandez2023inspecting,
      title={Inspecting and Editing Knowledge Representations in Language Models}, 
      author={Evan Hernandez and Belinda Z. Li and Jacob Andreas},
      year={2023},
      eprint={2304.00740},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@InProceedings{10.1007/978-3-319-24027-5_20,
author="Baudi{\v{s}}, Petr
and {\v{S}}ediv{\'y}, Jan",
editor="Mothe, Josanne
and Savoy, Jacques
and Kamps, Jaap
and Pinel-Sauvagnat, Karen
and Jones, Gareth
and San Juan, Eric
and Capellato, Linda
and Ferro, Nicola",
title="Modeling of the Question Answering Task in the YodaQA System",
booktitle="Experimental IR Meets Multilinguality, Multimodality, and Interaction",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="222--228",
abstract="We briefly survey the current state of art in the field of Question Answering and present the YodaQA system, an open source framework for this task and a baseline pipeline with reasonable performance. We take a holistic approach, reviewing and aiming to integrate many different question answering task definitions and approaches concerning classes of knowledge bases, question representation and answer generation. To ease performance comparisons of general-purpose QA systems, we also propose an effort in building a new reference QA testing corpus which is a curated and extended version of the TREC corpus.",
isbn="978-3-319-24027-5"
}



@misc{khandelwal2020generalization,
      title={Generalization through Memorization: Nearest Neighbor Language Models}, 
      author={Urvashi Khandelwal and Omer Levy and Dan Jurafsky and Luke Zettlemoyer and Mike Lewis},
      year={2020},
      eprint={1911.00172},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{ram2023incontext,
      title={In-Context Retrieval-Augmented Language Models}, 
      author={Ori Ram and Yoav Levine and Itay Dalmedigos and Dor Muhlgay and Amnon Shashua and Kevin Leyton-Brown and Yoav Shoham},
      year={2023},
      eprint={2302.00083},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{ma-etal-2022-open-domain,
    title = "Open-domain Question Answering via Chain of Reasoning over Heterogeneous Knowledge",
    author = "Ma, Kaixin  and
      Cheng, Hao  and
      Liu, Xiaodong  and
      Nyberg, Eric  and
      Gao, Jianfeng",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.392",
    pages = "5360--5374",
    abstract = "We propose a novel open-domain question answering (ODQA) framework for answering single/multi-hop questions across heterogeneous knowledge sources.The key novelty of our method is the introduction of the intermediary modules into the current retriever-reader pipeline.Unlike previous methods that solely rely on the retriever for gathering all evidence in isolation,our intermediary performs a chain of reasoning over the retrieved set.Specifically, our method links the retrieved evidence with its related global context into graphs and organizes them into a candidate list of evidence chains.Built upon pretrained language models, our system achieves competitive performance on two ODQA datasets, OTT-QA and NQ, against tables and passages from Wikipedia.In particular, our model substantially outperforms the previous state-of-the-art on OTT-QA with an exact match score of 47.3 (45{\%} relative gain).",
}

@misc{guu2020realm,
      title={REALM: Retrieval-Augmented Language Model Pre-Training}, 
      author={Kelvin Guu and Kenton Lee and Zora Tung and Panupong Pasupat and Ming-Wei Chang},
      year={2020},
      eprint={2002.08909},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{tam2022evaluating,
      title={Evaluating the Factual Consistency of Large Language Models Through Summarization}, 
      author={Derek Tam and Anisha Mascarenhas and Shiyue Zhang and Sarah Kwan and Mohit Bansal and Colin Raffel},
      year={2022},
      eprint={2211.08412},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{knowledge_base_1,
    title = "Language Models as Knowledge Bases: On Entity Representations, Storage Capacity, and Paraphrased Queries",
    author = "Heinzerling, Benjamin  and
      Inui, Kentaro",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.153",
    doi = "10.18653/v1/2021.eacl-main.153",
    pages = "1772--1791",
    abstract = "Pretrained language models have been suggested as a possible alternative or complement to structured knowledge bases. However, this emerging LM-as-KB paradigm has so far only been considered in a very limited setting, which only allows handling 21k entities whose name is found in common LM vocabularies. Furthermore, a major benefit of this paradigm, i.e., querying the KB using natural language paraphrases, is underexplored. Here we formulate two basic requirements for treating LMs as KBs: (i) the ability to store a large number facts involving a large number of entities and (ii) the ability to query stored facts. We explore three entity representations that allow LMs to handle millions of entities and present a detailed case study on paraphrased querying of facts stored in LMs, thereby providing a proof-of-concept that language models can indeed serve as knowledge bases.",
}
@misc{knowledge_base_2,
      title={A Review on Language Models as Knowledge Bases}, 
      author={Badr AlKhamissi and Millicent Li and Asli Celikyilmaz and Mona Diab and Marjan Ghazvininejad},
      year={2022},
      eprint={2204.06031},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{azaria2023internal,
      title={The Internal State of an LLM Knows When its Lying}, 
      author={Amos Azaria and Tom Mitchell},
      year={2023},
      eprint={2304.13734},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{peng2023check,
      title={Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback}, 
      author={Baolin Peng and Michel Galley and Pengcheng He and Hao Cheng and Yujia Xie and Yu Hu and Qiuyuan Huang and Lars Liden and Zhou Yu and Weizhu Chen and Jianfeng Gao},
      eprint={2302.12813},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{izacard2022atlas,
      title={Atlas: Few-shot Learning with Retrieval Augmented Language Models}, 
      author={Gautier Izacard and Patrick Lewis and Maria Lomeli and Lucas Hosseini and Fabio Petroni and Timo Schick and Jane Dwivedi-Yu and Armand Joulin and Sebastian Riedel and Edouard Grave},
      year={2022},
      eprint={2208.03299},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{kwiatkowski-etal-2019-natural,
    title = "Natural Questions: A Benchmark for Question Answering Research",
    author = "Kwiatkowski, Tom  and
      Palomaki, Jennimaria  and
      Redfield, Olivia  and
      Collins, Michael  and
      Parikh, Ankur  and
      Alberti, Chris  and
      Epstein, Danielle  and
      Polosukhin, Illia  and
      Devlin, Jacob  and
      Lee, Kenton  and
      Toutanova, Kristina  and
      Jones, Llion  and
      Kelcey, Matthew  and
      Chang, Ming-Wei  and
      Dai, Andrew M.  and
      Uszkoreit, Jakob  and
      Le, Quoc  and
      Petrov, Slav",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    year = "2019",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q19-1026",
    doi = "10.1162/tacl_a_00276",
    pages = "452--466",
    abstract = "We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.",
}
@misc{jiang2023active,
      title={Active Retrieval Augmented Generation}, 
      author={Zhengbao Jiang and Frank F. Xu and Luyu Gao and Zhiqing Sun and Qian Liu and Jane Dwivedi-Yu and Yiming Yang and Jamie Callan and Graham Neubig},
      year={2023},
      eprint={2305.06983},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{liu2019knowledge,
      title={Knowledge Aware Conversation Generation with Explainable Reasoning over Augmented Graphs}, 
      author={Zhibin Liu and Zheng-Yu Niu and Hua Wu and Haifeng Wang},
      year={2019},
      eprint={1903.10245},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{ghazvininejad2018knowledgegrounded,
      title={A Knowledge-Grounded Neural Conversation Model}, 
      author={Marjan Ghazvininejad and Chris Brockett and Ming-Wei Chang and Bill Dolan and Jianfeng Gao and Wen-tau Yih and Michel Galley},
      year={2018},
      eprint={1702.01932},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{rag,
      title={Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}, 
      author={Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
      year={2021},
      eprint={2005.11401},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{berant-etal-2013-semantic,
    title = "Semantic Parsing on {F}reebase from Question-Answer Pairs",
    author = "Berant, Jonathan  and
      Chou, Andrew  and
      Frostig, Roy  and
      Liang, Percy",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D13-1160",
    pages = "1533--1544",
}

@inproceedings{joshi-etal-2017-triviaqa,
    title = "{T}rivia{QA}: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",
    author = "Joshi, Mandar  and
      Choi, Eunsol  and
      Weld, Daniel  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1147",
    doi = "10.18653/v1/P17-1147",
    pages = "1601--1611",
    abstract = "We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23{\%} and 40{\%} vs. 80{\%}), suggesting that TriviaQA is a challenging testbed that is worth significant future study.",
}

@inproceedings{gao-etal-2023-rarr,
    title = "{RARR}: Researching and Revising What Language Models Say, Using Language Models",
    author = "Gao, Luyu  and
      Dai, Zhuyun  and
      Pasupat, Panupong  and
      Chen, Anthony  and
      Chaganty, Arun Tejasvi  and
      Fan, Yicheng  and
      Zhao, Vincent  and
      Lao, Ni  and
      Lee, Hongrae  and
      Juan, Da-Cheng  and
      Guu, Kelvin",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.910",
    doi = "10.18653/v1/2023.acl-long.910",
    pages = "16477--16508",
    abstract = "Language models (LMs) now excel at many tasks such as question answering, reasoning, and dialog. However, they sometimes generate unsupported or misleading content. A user cannot easily determine whether their outputs are trustworthy or not, because most LMs do not have any built-in mechanism for attribution to external evidence. To enable attribution while still preserving all the powerful advantages of recent generation models, we propose RARR (Retrofit Attribution using Research and Revision), a system that 1) automatically finds attribution for the output of any text generation model, and 2) post-edits the output to fix unsupported content while preserving the original output as much as possible. When applied to the output of several state-of-the-art LMs on a diverse set of generation tasks, we find that RARR significantly improves attribution while otherwise preserving the original input to a much greater degree than previously explored edit models. Furthermore, the implementation of RARR requires only a handful of training examples, a large language model, and standard web search.",
}

@misc{chen2023purr,
      title={PURR: Efficiently Editing Language Model Hallucinations by Denoising Language Model Corruptions}, 
      author={Anthony Chen and Panupong Pasupat and Sameer Singh and Hongrae Lee and Kelvin Guu},
      year={2023},
      eprint={2305.14908},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{rashkin2022measuring,
      title={Measuring Attribution in Natural Language Generation Models}, 
      author={Hannah Rashkin and Vitaly Nikolaev and Matthew Lamm and Lora Aroyo and Michael Collins and Dipanjan Das and Slav Petrov and Gaurav Singh Tomar and Iulia Turc and David Reitter},
      year={2022},
      eprint={2112.12870},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{shuster-etal-2021-retrieval-augmentation,
    title = "Retrieval Augmentation Reduces Hallucination in Conversation",
    author = "Shuster, Kurt  and
      Poff, Spencer  and
      Chen, Moya  and
      Kiela, Douwe  and
      Weston, Jason",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.320",
    doi = "10.18653/v1/2021.findings-emnlp.320",
    pages = "3784--3803",
    abstract = "Despite showing increasingly human-like conversational abilities, state-of-the-art dialogue models often suffer from factual incorrectness and hallucination of knowledge (Roller et al., 2020). In this work we explore the use of neural-retrieval-in-the-loop architectures - recently shown to be effective in open-domain QA (Lewis et al., 2020b; Izacard and Grave, 2020) - for knowledge-grounded dialogue, a task that is arguably more challenging as it requires querying based on complex multi-turn dialogue context and generating conversationally coherent responses. We study various types of architectures with multiple components - retrievers, rankers, and encoder-decoders - with the goal of maximizing knowledgeability while retaining conversational ability. We demonstrate that our best models obtain state-of-the-art performance on two knowledge-grounded conversational tasks. The models exhibit open-domain conversational capabilities, generalize effectively to scenarios not within the training data, and, as verified by human evaluations, substantially reduce the well-known problem of knowledge hallucination in state-of-the-art chatbots.",
}

@misc{umapathi2023medhalt,
      title={Med-HALT: Medical Domain Hallucination Test for Large Language Models}, 
      author={Logesh Kumar Umapathi and Ankit Pal and Malaikannan Sankarasubbu},
      year={2023},
      eprint={2307.15343},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{touvron2023llama,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{yue2023automatic,
      title={Automatic Evaluation of Attribution by Large Language Models}, 
      author={Xiang Yue and Boshi Wang and Kai Zhang and Ziru Chen and Yu Su and Huan Sun},
      year={2023},
      eprint={2305.06311},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{RAG_domain_adaptation,
    author = {Siriwardhana, Shamane and Weerasekera, Rivindu and Wen, Elliott and Kaluarachchi, Tharindu and Rana, Rajib and Nanayakkara, Suranga},
    title = "{Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {11},
    pages = {1-17},
    year = {2023},
    month = {01},
    abstract = "{Retrieval Augment Generation (RAG) is a recent advancement in Open-Domain Question Answering (ODQA). RAG has only been trained and explored with a Wikipedia-based external knowledge base and is not optimized for use in other specialized domains such as healthcare and news. In this paper, we evaluate the impact of joint training of the retriever and generator components of RAG for the task of domain adaptation in ODQA. We propose RAG-end2end, an extension to RAG that can adapt to a domain-specific knowledge base by updating all components of the external knowledge base during training. In addition, we introduce an auxiliary training signal to inject more domain-specific knowledge. This auxiliary signal forces RAG-end2end to reconstruct a given sentence by accessing the relevant information from the external knowledge base. Our novel contribution is that, unlike RAG, RAG-end2end does joint training of the retriever and generator for the end QA task and domain adaptation. We evaluate our approach with datasets from three domains: COVID-19, News, and Conversations, and achieve significant performance improvements compared to the original RAG model. Our work has been open-sourced through the HuggingFace Transformers library, attesting to our work’s credibility and technical consistency.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00530},
    url = {https://doi.org/10.1162/tacl\_a\_00530},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00530/2067834/tacl\_a\_00530.pdf},
}





@inproceedings{kryscinski-etal-2020-evaluating,
    title = "Evaluating the Factual Consistency of Abstractive Text Summarization",
    author = "Kryscinski, Wojciech  and
      McCann, Bryan  and
      Xiong, Caiming  and
      Socher, Richard",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.750",
    doi = "10.18653/v1/2020.emnlp-main.750",
    pages = "9332--9346",
    abstract = "The most common metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and generated summaries. Training data is generated by applying a series of rule-based transformations to the sentences of source documents.The factual consistency model is then trained jointly for three tasks: 1) predict whether each summary sentence is factually consistent or not, 2) in either case, extract a span in the source document to support this consistency prediction, 3) for each summary sentence that is deemed inconsistent, extract the inconsistent span from it. Transferring this model to summaries generated by several neural models reveals that this highly scalable approach outperforms previous models, including those trained with strong supervision using datasets from related domains, such as natural language inference and fact checking. Additionally, human evaluation shows that the auxiliary span extraction tasks provide useful assistance in the process of verifying factual consistency. We also release a manually annotated dataset for factual consistency verification, code for training data generation, and trained model weights at https://github.com/salesforce/factCC.",
}
@misc{li2023unlocking,
      title={Unlocking Temporal Question Answering for Large Language Models Using Code Execution}, 
      author={Xingxuan Li and Liying Cheng and Qingyu Tan and Hwee Tou Ng and Shafiq Joty and Lidong Bing},
      year={2023},
      eprint={2305.15014},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{narayan2018dont,
      title={Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization}, 
      author={Shashi Narayan and Shay B. Cohen and Mirella Lapata},
      year={2018},
      eprint={1808.08745},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{zhang2023sirens,
      title={Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models}, 
      author={Yue Zhang and Yafu Li and Leyang Cui and Deng Cai and Lemao Liu and Tingchen Fu and Xinting Huang and Enbo Zhao and Yu Zhang and Yulong Chen and Longyue Wang and Anh Tuan Luu and Wei Bi and Freda Shi and Shuming Shi},
      year={2023},
      eprint={2309.01219},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{temporal_aspect,
    title = "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models",
    author = "Tan, Qingyu  and
      Ng, Hwee Tou  and
      Bing, Lidong",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.828",
    doi = "10.18653/v1/2023.acl-long.828",
    pages = "14820--14835",
    abstract = "Reasoning about time is of fundamental importance. Many facts are time-dependent. For example, athletes change teams from time to time, and different government officials are elected periodically. Previous time-dependent question answering (QA) datasets tend to be biased in either their coverage of time spans or question types. In this paper, we introduce a comprehensive probing dataset TempReason to evaluate the temporal reasoning capability of large language models. Our dataset includes questions of three temporal reasoning levels. In addition, we also propose a novel learning framework to improve the temporal reasoning capability of large language models, based on temporal span extraction and time-sensitive reinforcement learning. We conducted experiments in closed book QA, open book QA, and reasoning QA settings and demonstrated the effectiveness of our approach.",
}

@ARTICLE{2023arXiv230511747L,
       author = {{Li}, Junyi and {Cheng}, Xiaoxue and {Zhao}, Wayne Xin and {Nie}, Jian-Yun and {Wen}, Ji-Rong},
        title = "{HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2023,
        month = may,
          eid = {arXiv:2305.11747},
        pages = {arXiv:2305.11747},
          doi = {10.48550/arXiv.2305.11747},
archivePrefix = {arXiv},
       eprint = {2305.11747},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230511747L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@misc{xue2023rcot,
      title={RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought}, 
      author={Tianci Xue and Ziqi Wang and Zhenhailong Wang and Chi Han and Pengfei Yu and Heng Ji},
      year={2023},
      eprint={2305.11499},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{shuster-etal-2021-retrieval-augmentation,
    title = "Retrieval Augmentation Reduces Hallucination in Conversation",
    author = "Shuster, Kurt  and
      Poff, Spencer  and
      Chen, Moya  and
      Kiela, Douwe  and
      Weston, Jason",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.320",
    doi = "10.18653/v1/2021.findings-emnlp.320",
    pages = "3784--3803",
    abstract = "Despite showing increasingly human-like conversational abilities, state-of-the-art dialogue models often suffer from factual incorrectness and hallucination of knowledge (Roller et al., 2020). In this work we explore the use of neural-retrieval-in-the-loop architectures - recently shown to be effective in open-domain QA (Lewis et al., 2020b; Izacard and Grave, 2020) - for knowledge-grounded dialogue, a task that is arguably more challenging as it requires querying based on complex multi-turn dialogue context and generating conversationally coherent responses. We study various types of architectures with multiple components - retrievers, rankers, and encoder-decoders - with the goal of maximizing knowledgeability while retaining conversational ability. We demonstrate that our best models obtain state-of-the-art performance on two knowledge-grounded conversational tasks. The models exhibit open-domain conversational capabilities, generalize effectively to scenarios not within the training data, and, as verified by human evaluations, substantially reduce the well-known problem of knowledge hallucination in state-of-the-art chatbots.",
}

@misc{mündler2023selfcontradictory,
      title={Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation}, 
      author={Niels Mündler and Jingxuan He and Slobodan Jenko and Martin Vechev},
      year={2023},
      eprint={2305.15852},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Manakul2023SelfCheckGPTZB,
  title={SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models},
  author={Potsawee Manakul and Adian Liusie and Mark John Francis Gales},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.08896}
}

@unknown{unknown,
author = {McKenna, Nick and Li, Tianyi and Cheng, Liang and Hosseini, Mohammad and Johnson, Mark and Steedman, Mark},
year = {2023},
month = {05},
pages = {},
title = {Sources of Hallucination by Large Language Models on Inference Tasks}
}



@inproceedings{LiLSDSLJJL22,
  author       = {Shaobo Li and
                  Xiaoguang Li and
                  Lifeng Shang and
                  Zhenhua Dong and
                  Chengjie Sun and
                  Bingquan Liu and
                  Zhenzhou Ji and
                  Xin Jiang and
                  Qun Liu},
  editor       = {Smaranda Muresan and
                  Preslav Nakov and
                  Aline Villavicencio},
  title        = {How Pre-trained Language Models Capture Factual Knowledge? {A} Causal-Inspired
                  Analysis},
  booktitle    = {Findings of the Association for Computational Linguistics: {ACL} 2022,
                  Dublin, Ireland, May 22-27, 2022},
  pages        = {1720--1732},
  publisher    = {Association for Computational Linguistics},
  year         = {2022},
  url          = {https://doi.org/10.18653/v1/2022.findings-acl.136},
  doi          = {10.18653/v1/2022.findings-acl.136},
  timestamp    = {Mon, 01 Aug 2022 16:27:46 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/LiLSDSLJJL22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{yang2021designing,
      title={Designing a Minimal Retrieve-and-Read System for Open-Domain Question Answering}, 
      author={Sohee Yang and Minjoon Seo},
      year={2021},
      eprint={2104.07242},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{min2023factscore,
      title={FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation}, 
      author={Sewon Min and Kalpesh Krishna and Xinxi Lyu and Mike Lewis and Wen-tau Yih and Pang Wei Koh and Mohit Iyyer and Luke Zettlemoyer and Hannaneh Hajishirzi},
      year={2023},
      eprint={2305.14251},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{chen2022,
	doi = {10.1145/3477495.3531827},
	url = {https://doi.org/10.1145%2F3477495.3531827},
	year = 2022,
	month = {jul},
	publisher = {{ACM}},
	author = {Jiangui Chen and Ruqing Zhang and Jiafeng Guo and Yixing Fan and Xueqi Cheng},
	title = {{GERE}: Generative Evidence Retrieval for Fact Verification},
	booktitle = {Proceedings of the 45th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval}
}

@inproceedings{ahmad2019,
    title = {ReQA: An Evaluation for End-to-End Answer Retrieval Models},
    author = {Ahmad, Amin and Constant, Noah and Yang, Yinfei and Cer, Daniel},
    booktitle = {Proceedings of the 2nd Workshop on Machine Reading for Question Answering},
    month = {nov},
    year = 2019,
    address = {Hong Kong, China},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/D19-5819},
    doi = {10.18653/v1/D19-5819},
    pages = {137--146},
}

@inproceedings{zoupanos2022,
    author = {Zoupanos, Spyros and Kolovos, Stratis and Kanavos, Athanasios and Papadimitriou, Orestis and Maragoudakis, Manolis},
    title = {Efficient Comparison of Sentence Embeddings},
    year = 2022,
    isbn = {9781450395977},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3549737.3549752},
    doi = {10.1145/3549737.3549752},
    booktitle = {Proceedings of the 12th Hellenic Conference on Artificial Intelligence},
    articleno = {11},
    numpages = {6},
    keywords = {sentence embeddings, Elasticsearch, vector performance comparison, FAISS},
    location = {Corfu, Greece},
    series = {SETN '22},
}

@inproceedings{xu2023,
    author = {Hu, Xuming and Hong, Zhaochen and Guo, Zhijiang and Wen, Lijie and Yu, Philip},
    title = {Read It Twice: Towards Faithfully Interpretable Fact Verification by Revisiting Evidence},
    year = {2023},
    isbn = {9781450394086},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3539618.3592049},
    doi = {10.1145/3539618.3592049},
    booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
    pages = {2319–2323},
    numpages = {5},
    keywords = {claim verification, real-world systems, latent variable models, automated fact-checking, evidence retrieval},
    location = {Taipei, Taiwan},
    series = {SIGIR '23},
}

@inproceedings{guzman2023,
    title = "Enhancing Information Retrieval in Fact Extraction and Verification",
    author = "Guzman Olivares, Daniel  and
      Quijano, Lara  and
      Liberatore, Federico",
    booktitle = "Proceedings of the Sixth Fact Extraction and VERification Workshop (FEVER)",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.fever-1.4",
    pages = "38--48",
}

@misc{chern2023,
      title={FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios}, 
      author={I-Chun Chern and Steffi Chern and Shiqi Chen and Weizhe Yuan and Kehua Feng and Chunting Zhou and Junxian He and Graham Neubig and Pengfei Liu},
      year={2023},
      eprint={2307.13528},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{laban2023,
      title={SWiPE: A Dataset for Document-Level Simplification of Wikipedia Pages}, 
      author={Philippe Laban and Jesse Vig and Wojciech Kryscinski and Shafiq Joty and Caiming Xiong and Chien-Sheng Wu},
      year={2023},
      eprint={2305.19204},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhao2022,
      title={Dense Text Retrieval based on Pretrained Language Models: A Survey}, 
      author={Wayne Xin Zhao and Jing Liu and Ruiyang Ren and Ji-Rong Wen},
      year={2022},
      eprint={2211.14876},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@misc{zhu2021leveraging,
      title={Leveraging Lead Bias for Zero-shot Abstractive News Summarization}, 
      author={Chenguang Zhu and Ziyi Yang and Robert Gmyr and Michael Zeng and Xuedong Huang},
      year={2021},
      eprint={1912.11602},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{rothe2021,
    title = "A Thorough Evaluation of Task-Specific Pretraining for Summarization",
    author = "Rothe, Sascha  and
      Maynez, Joshua  and
      Narayan, Shashi",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.12",
    doi = "10.18653/v1/2021.emnlp-main.12",
    pages = "140--145",
}

@misc{raffel2020,
      title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}, 
      author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
      year={2020},
      eprint={1910.10683},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{lewis2019bart,
      title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension}, 
      author={Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Ves Stoyanov and Luke Zettlemoyer},
      year={2019},
      eprint={1910.13461},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{goyal2021annotating,
      title={Annotating and Modeling Fine-grained Factuality in Summarization}, 
      author={Tanya Goyal and Greg Durrett},
      year={2021},
      eprint={2104.04302},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{roit2023factually,
      title={Factually Consistent Summarization via Reinforcement Learning with Textual Entailment Feedback}, 
      author={Paul Roit and Johan Ferret and Lior Shani and Roee Aharoni and Geoffrey Cideron and Robert Dadashi and Matthieu Geist and Sertan Girgin and Léonard Hussenot and Orgad Keller and Nikola Momchev and Sabela Ramos and Piotr Stanczyk and Nino Vieillard and Olivier Bachem and Gal Elidan and Avinatan Hassidim and Olivier Pietquin and Idan Szpektor},
      year={2023},
      eprint={2306.00186},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{dong2022faithful,
      title={Faithful to the Document or to the World? Mitigating Hallucinations via Entity-linked Knowledge in Abstractive Summarization}, 
      author={Yue Dong and John Wieting and Pat Verga},
      year={2022},
      eprint={2204.13761},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{gao2023rarr,
      title={RARR: Researching and Revising What Language Models Say, Using Language Models}, 
      author={Luyu Gao and Zhuyun Dai and Panupong Pasupat and Anthony Chen and Arun Tejasvi Chaganty and Yicheng Fan and Vincent Y. Zhao and Ni Lao and Hongrae Lee and Da-Cheng Juan and Kelvin Guu},
      year={2023},
      eprint={2210.08726},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{wei2022,
  author={Wei, Zhimin and Xu, Xiaowei and Wang, Chenglin and Liu, Zhenyu and Xin, Peng and Zhang, Wei},
  booktitle={2022 7th International Conference on Image, Vision and Computing (ICIVC)}, 
  title={An Index Construction and Similarity Retrieval Method Based on Sentence-Bert}, 
  year={2022},
  volume={},
  number={},
  pages={934-938},
  doi={10.1109/ICIVC55077.2022.9886134}}

@misc{baranchuk2018revisiting,
      title={Revisiting the Inverted Indices for Billion-Scale Approximate Nearest Neighbors}, 
      author={Dmitry Baranchuk and Artem Babenko and Yury Malkov},
      year={2018},
      eprint={1802.02422},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{zhao2023ann,
    author = {Zhao, Xi and Tian, Yao and Huang, Kai and Zheng, Bolong and Zhou, Xiaofang},
    title = {Towards Efficient Index Construction and Approximate Nearest Neighbor Search in High-Dimensional Spaces},
    year = {2023},
    issue_date = {April 2023},
    publisher = {VLDB Endowment},
    volume = {16},
    number = {8},
    issn = {2150-8097},
    url = {https://doi.org/10.14778/3594512.3594527},
    doi = {10.14778/3594512.3594527},
    journal = {Proc. VLDB Endow.},
    month = {jun},
    pages = {1979–1991},
    numpages = {13}
}

@misc{nallapati2016abstractive,
      title={Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond}, 
      author={Ramesh Nallapati and Bowen Zhou and Cicero Nogueira dos santos and Caglar Gulcehre and Bing Xiang},
      year={2016},
      eprint={1602.06023},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{musiqueQA,
  author       = {Harsh Trivedi and
                  Niranjan Balasubramanian and
                  Tushar Khot and
                  Ashish Sabharwal},
  title        = {MuSiQue: Multi-hop Questions via Single-hop Question Composition},
  journal      = {CoRR},
  volume       = {abs/2108.00573},
  year         = {2021},
  url          = {https://arxiv.org/abs/2108.00573},
  eprinttype    = {arXiv},
  eprint       = {2108.00573},
  timestamp    = {Thu, 05 Aug 2021 14:27:08 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2108-00573.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{wikimultihopQA,
  author       = {Xanh Ho and
                  Anh{-}Khoa Duong Nguyen and
                  Saku Sugawara and
                  Akiko Aizawa},
  title        = {Constructing {A} Multi-hop {QA} Dataset for Comprehensive Evaluation
                  of Reasoning Steps},
  journal      = {CoRR},
  volume       = {abs/2011.01060},
  year         = {2020},
  url          = {https://arxiv.org/abs/2011.01060},
  eprinttype    = {arXiv},
  eprint       = {2011.01060},
  timestamp    = {Fri, 06 Nov 2020 15:32:47 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2011-01060.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{jiang2020hover,
      title={HoVer: A Dataset for Many-Hop Fact Extraction And Claim Verification}, 
      author={Yichen Jiang and Shikha Bordia and Zheng Zhong and Charles Dognin and Maneesh Singh and Mohit Bansal},
      year={2020},
      eprint={2011.03088},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{morozov2019unsupervised,
      title={Unsupervised Neural Quantization for Compressed-Domain Similarity Search}, 
      author={Stanislav Morozov and Artem Babenko},
      year={2019},
      eprint={1908.03883},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{shu2017compressing,
      title={Compressing Word Embeddings via Deep Compositional Code Learning}, 
      author={Raphael Shu and Hideki Nakayama},
      year={2017},
      eprint={1711.01068},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{shazeer2017outrageously,
      title={Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer}, 
      author={Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
      year={2017},
      eprint={1701.06538},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{nie2018combining,
      title={Combining Fact Extraction and Verification with Neural Semantic Matching Networks}, 
      author={Yixin Nie and Haonan Chen and Mohit Bansal},
      year={2018},
      eprint={1811.07039},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{thorne2018fever,
      title={FEVER: a large-scale dataset for Fact Extraction and VERification}, 
      author={James Thorne and Andreas Vlachos and Christos Christodoulopoulos and Arpit Mittal},
      year={2018},
      eprint={1803.05355},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhan2021jointly,
      title={Jointly Optimizing Query Encoder and Product Quantization to Improve Retrieval Performance}, 
      author={Jingtao Zhan and Jiaxin Mao and Yiqun Liu and Jiafeng Guo and Min Zhang and Shaoping Ma},
      year={2021},
      eprint={2108.00644},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@inproceedings{kamoi2023wice,
    title = "{W}i{CE}: Real-World Entailment for Claims in {W}ikipedia",
    author = "Kamoi, Ryo  and
      Goyal, Tanya  and
      Rodriguez, Juan  and
      Durrett, Greg",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.470",
    pages = "7561--7583",
}

@misc{bondarenko2021understanding,
      title={Understanding and Overcoming the Challenges of Efficient Transformer Quantization}, 
      author={Yelysei Bondarenko and Markus Nagel and Tijmen Blankevoort},
      year={2021},
      eprint={2109.12948},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zhu2023survey,
      title={A Survey on Model Compression for Large Language Models}, 
      author={Xunyu Zhu and Jian Li and Yong Liu and Can Ma and Weiping Wang},
      year={2023},
      eprint={2308.07633},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{han2023comprehensive,
      title={A Comprehensive Survey on Vector Database: Storage and Retrieval Technique, Challenge}, 
      author={Yikun Han and Chunjiang Liu and Pengfei Wang},
      year={2023},
      eprint={2310.11703},
      archivePrefix={arXiv},
      primaryClass={cs.DB}
}

@inproceedings{questgen,
author = {Setty, Ritvik and Setty, Vinay},
title = {QuestGen: Effectiveness of Question Generation Methods for Fact-Checking Applications},
year = {2024},
isbn = {9798400704369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627673.3679985},
doi = {10.1145/3627673.3679985},
abstract = {Verifying fact-checking claims poses a significant challenge, even for humans. Recent approaches have demonstrated that decomposing claims into relevant questions to gather evidence enhances the efficiency of the fact-checking process. In this paper, we provide empirical evidence showing that this question decomposition can be effectively automated. We demonstrate that smaller generative models, fine-tuned for the question generation task using data augmentation from various datasets, outperform large language models by up to 8. Surprisingly, in some cases, the evidence retrieved using machine-generated questions proves to be significantly more effective for fact-checking than that obtained from human-written questions. We also perform manual evaluation of the decomposed questions to assess the quality of the questions generated.},
booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
pages = {4036–4040},
numpages = {5},
keywords = {claim decomposition, fact-checking, question generation},
location = {Boise, ID, USA},
series = {CIKM '24}
}
@misc{v2024quantemprealworldopendomainbenchmark,
      title={QuanTemp: A real-world open-domain benchmark for fact-checking numerical claims}, 
      author={V Venktesh and Abhijit Anand and Avishek Anand and Vinay Setty},
      year={2024},
      eprint={2403.17169},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.17169}, 
}

@misc{petroni2022improving,
      title={Improving Wikipedia Verifiability with AI}, 
      author={Fabio Petroni and Samuel Broscheit and Aleksandra Piktus and Patrick Lewis and Gautier Izacard and Lucas Hosseini and Jane Dwivedi-Yu and Maria Lomeli and Timo Schick and Pierre-Emmanuel Mazaré and Armand Joulin and Edouard Grave and Sebastian Riedel},
      year={2022},
      eprint={2207.06220},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@misc{thorne2018automated,
      title={Automated Fact Checking: Task formulations, methods and future directions}, 
      author={James Thorne and Andreas Vlachos},
      year={2018},
      eprint={1806.07687},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@Article{Lazarski2021nlpfact,
AUTHOR = {Lazarski, Eric and Al-Khassaweneh, Mahmood and Howard, Cynthia},
TITLE = {Using NLP for Fact Checking: A Survey},
JOURNAL = {Designs},
VOLUME = {5},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {42},
URL = {https://www.mdpi.com/2411-9660/5/3/42},
ISSN = {2411-9660},
DOI = {10.3390/designs5030042}
}

@inproceedings{sathe2020automated,
    title = "Automated Fact-Checking of Claims from {W}ikipedia",
    author = "Sathe, Aalok  and
      Ather, Salar  and
      Le, Tuan Manh  and
      Perry, Nathan  and
      Park, Joonsuk",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.849",
    pages = "6874--6882",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@article{guo2022survey,
    title = "A Survey on Automated Fact-Checking",
    author = "Guo, Zhijiang  and
      Schlichtkrull, Michael  and
      Vlachos, Andreas",
    editor = "Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "10",
    year = "2022",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2022.tacl-1.11",
    pages = "178--206"
}

@inproceedings{zhan21jointly,
    author = {Zhan, Jingtao and Mao, Jiaxin and Liu, Yiqun and Guo, Jiafeng and Zhang, Min and Ma, Shaoping},
    title = {Jointly Optimizing Query Encoder and Product Quantization to Improve Retrieval Performance},
    year = {2021},
    isbn = {9781450384469},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3459637.3482358},
    doi = {10.1145/3459637.3482358},
    pages = {2487–2496},
    numpages = {10},
    location = {Virtual Event, Queensland, Australia},
    series = {CIKM '21}
}

@misc{zhan2021optimizing,
      title={Optimizing Dense Retrieval Model Training with Hard Negatives}, 
      author={Jingtao Zhan and Jiaxin Mao and Yiqun Liu and Jiafeng Guo and Min Zhang and Shaoping Ma},
      year={2021},
      eprint={2104.08051},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@ARTICLE{ge2014opq,
  author={Ge, Tiezheng and He, Kaiming and Ke, Qifa and Sun, Jian},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Optimized Product Quantization}, 
  year={2014},
  volume={36},
  number={4},
  pages={744-755},
  keywords={Quantization (signal);Vectors;Artificial neural networks;Optimization;Encoding;Indexing;Linear programming;Vector quantization;nearest neighbor search;image retrieval;compact encoding;inverted indexing},
  doi={10.1109/TPAMI.2013.240}
}
@inproceedings{tas_b,
author = {Hofst\"{a}tter, Sebastian and Lin, Sheng-Chieh and Yang, Jheng-Hong and Lin, Jimmy and Hanbury, Allan},
title = {Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462891},
doi = {10.1145/3404835.3462891},
abstract = {A vital step towards the widespread adoption of neural retrieval models is their resource efficiency throughout the training, indexing and query workflows. The neural IR community made great advancements in training effective dual-encoder dense retrieval (DR) models recently. A dense text retrieval model uses a single vector representation per query and passage to score a match, which enables low-latency first-stage retrieval with a nearest neighbor search. Increasingly common, training approaches require enormous compute power, as they either conduct negative passage sampling out of a continuously updating refreshing index or require very large batch sizes. Instead of relying on more compute capability, we introduce an efficient topic-aware query and balanced margin sampling technique, called TAS-Balanced. We cluster queries once before training and sample queries out of a cluster per batch. We train our lightweight 6-layer DR model with a novel dual-teacher supervision that combines pairwise and in-batch negative teachers. Our method is trainable on a single consumer-grade GPU in under 48 hours. We show that our TAS-Balanced training method achieves state-of-the-art low-latency (64ms per query) results on two TREC Deep Learning Track query sets. Evaluated on NDCG@10, we outperform BM25 by 44\%, a plainly trained DR by 19\%, docT5query by 11\%, and the previous best DR model by 5\%. Additionally, TAS-Balanced produces the first dense retriever that outperforms every other method on recall at any cutoff on TREC-DL and allows more resource intensive re-ranking models to operate on fewer passages to improve results further.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {113–122},
numpages = {10},
keywords = {batch sampling, dense retrieval, knowledge distillation},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}
@article{Guo_2022,
   title={Semantic Models for the First-Stage Retrieval: A Comprehensive Review},
   volume={40},
   ISSN={1558-2868},
   url={http://dx.doi.org/10.1145/3486250},
   DOI={10.1145/3486250},
   number={4},
   journal={ACM Transactions on Information Systems},
   publisher={Association for Computing Machinery (ACM)},
   author={Guo, Jiafeng and Cai, Yinqiong and Fan, Yixing and Sun, Fei and Zhang, Ruqing and Cheng, Xueqi},
   year={2022},
   month=mar, pages={1–42} }

@ARTICLE{jegou2011pq,
  author={Jégou, Herve and Douze, Matthijs and Schmid, Cordelia},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Product Quantization for Nearest Neighbor Search}, 
  year={2011},
  volume={33},
  number={1},
  pages={117-128},
  keywords={Quantization;Nearest neighbor searches;Indexing;Neural networks;Euclidean distance;File systems;Scalability;Image databases;Permission;Electronic mail;High-dimensional indexing;image indexing;very large databases;approximate search.},
  doi={10.1109/TPAMI.2010.57}}

@article{burges2010lambdarank,
author = {Burges, Christopher},
year = {2010},
month = {01},
pages = {},
title = {From ranknet to lambdarank to lambdamart: An overview},
volume = {11},
journal = {Learning}
}

@misc{karpukhin2020dense,
      title={Dense Passage Retrieval for Open-Domain Question Answering}, 
      author={Vladimir Karpukhin and Barlas Oğuz and Sewon Min and Patrick Lewis and Ledell Wu and Sergey Edunov and Danqi Chen and Wen-tau Yih},
      year={2020},
      eprint={2004.04906},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{chen2017reading,
      title={Reading Wikipedia to Answer Open-Domain Questions}, 
      author={Danqi Chen and Adam Fisch and Jason Weston and Antoine Bordes},
      year={2017},
      eprint={1704.00051},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


