\section{Methodology}
In this section, we describe our proposed improvements on how to enhance fact-checking efficiency by optimizing the retrieval of evidence from knowledge sources. This involves constructing a pruned index of succinct facts from large sources, like Wikipedia, coupled with vector quantization for index compression to improve retrieval latency and reduce resource consumption, as illustrated in \autoref{fig:original_pipeline}. 

\begin{figure*}[hbt!]
    \centering
    \includegraphics [width=1.0\textwidth]{figs/pipeline_new.jpg}
    \caption{Comparison of Existing and Proposed Fact-Checking Pipelines}
    \label{fig:original_pipeline}
\end{figure*}
\vspace{-2em}
\subsection{Corpus Compression through Extraction of Facts}\label{ssec:reranking}
 Large collections like Wikipedia are usually employed as knowledge sources for fact-checking \cite{jiang2020hover,kamoi2023wice}. While each Wikipedia article has detailed information on a topic, all the information is not factual, lacking citations and less informative for purposes of verifying claims. Hence, identifying useful factual information for fact-checking from such a large corpus and pruning other parts can significantly reduce pipeline runtime and save disk space for indexing. We employ different mechanisms to identify and store factual statements from Wikipedia corpus.
 
Since factual statements are precise statements that are verifiable, we posit that claim detection can help identify such succinct facts. Hence, we first employ a claim-detection model to identify significant (salient) sentences within the text, aiming to extract key information effectively. For experimental purposes, a model trained on a claim detection dataset such as Claimbuster is leveraged. We term this approach \textbf{Fact Extraction (FE)}

However, this approach could result in false positives leading to statements that do not contain factual information or aid in verifying claims. It would also result in false negatives as the statements the claim detection model fails to identify due to distribution mismatch would lead to incorrect verdicts. To circumvent this, we explore an alternative approach with a stricter criterion of retaining only verified factual statements on Wikipedia articles. We retain only those snippets that comprise citations following Wikipedia's \textit{verifiability} \footnote{https://en.wikipedia.org/wiki/Wikipedia:Verifiability} and \textit{citation} \footnote{https://en.wikipedia.org/wiki/Wikipedia:Citing\_sources} guidelines that the cited statements ensure verifiability of information conveyed. This method eliminates the need for training a specific model, providing a straightforward way to identify factual statements. We term this approach \textbf{Citation Extraction (CE)} in our experiments. However, it may miss contextually related sentences, such as those in Wikipedia, where consecutive sentences can be supported by a single citation, resulting in false negatives. 

To balance the tradeoffs in the above approaches, we focus on minimizing false negatives to capture all information that would aid in verifying claims. We propose to fuse the snippets identified by the claim detection-based approach and the citation extraction approach while pruning other information from the Wiki pages. While the problem of false positives from claim detection would persist, we posit that this can be minimized through retrieval and re-ranking approaches that optimize for recall.
The fusion of the former approaches aims to leverage the strengths of each approach: citations for reliable support and claim detection for comprehensive coverage. We term this approach \textbf{Fusion (Fu)} in our experiments.

While pruning the Wikipedia collection helps reduce memory requirements for storing and using the plaintext collection, it also helps optimize latency for sparse retrieval approaches like BM25. However, dense retrieval approaches provide better semantic relevance estimates than sparse retrieval and hence are employed in many fact-checking pipelines. However, dense retrieval is expensive due to index size comprising document vector representations and also has high latency due to brute force search approaches for retrieving relevant documents for a query. Hence, in this work, we explore an index compression mechanism that reduces index size, while ensuring optimal performance. We term this approach \textbf{Index Compression} in our experiments.


% \subsection{Evidence Retrieval}\label{ssec:dense_retrieval}
% In enhancing fact-checking pipelines, Dense Retrieval offers an efficient alternative to Sparse Retrieval and re-ranking by directly retrieving top-k documents based on semantic similarity. Typically Neural network-based re-rankers are used to further enhance Sparse retrieval by leveraging advanced techniques like semantic similarity assessment and pre-trained models such as BERT. This as to refine document selection and improving overall retrieval performance. By using Dense retrieval, it eliminates the need for such separate modules and iterative processes, potentially improving retrieval accuracy by capturing comprehensive semantic relationships between queries and documents during the first stage retrieval. While  Dense Retrieval offers a streamlined approach, it does face some limitations compared to traditional two-step methods. Its dense representations often struggle to encapsulate as much semantic information as sparse representations, especially for extensive texts \cite{karpukhin2020dense}. While Dense Retrieval improves computational efficiency, it may not consistently outperform traditional methods in capturing comprehensive semantic meaning, resulting in comparable performance differences.

% \begin{figure}[hbt!]
%     \begin{subfigure}{\textwidth}
%     \centering
%     \includegraphics[scale=0.4]{figs/sparse_retrieval.png}
%     \caption{Sparse Retrieval with Re-ranker}
%     \label{subfig:a}
%     \end{subfigure}
%     \begin{subfigure}{\textwidth}
%     \centering
%     \includegraphics[scale=.4]{figs/dense_retrieval.png}
%     \caption{Dense Retrieval}
%     \label{subfig:b}
%     \end{subfigure}
%     \caption{Retrieval methods for retrieving top-k relevant documents pertaining to a query.}\label{fig:retrieval_methods}
% \end{figure}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Index Compression}
Existing fact-checking systems usually adopt sparse retrieval followed by re-ranking or dense retrieval approaches to capture better semantics for first-stage retrieval. However, dense retrieval using brute force search for web-scale data is not scalable. To enhance efficiency in Dense retrieval, we propose to compress the corpus embeddings index through the JPQ index compression algorithm \cite{zhan2021jointly}. Unlike existing approaches that treat training of encoders and learning compressed index separately, JPQ optimizes them jointly by employing a ranking-oriented loss produced by the encoders and the index. JPQ achieves an impressive compression ratio of \textit{4D/M} (where \textit{D} is vector dimensionality and \textit{M} is the number of codebooks (embedding sets)) \cite{zhan2021jointly}. This approach ensures efficient retrieval with a speedup ratio of $({D + \log n})/({M + \log n})$, where \textit{n} is the total number of documents, maintaining performance comparable to standard Dense Retrieval setups while maintaining a small memory footprint for the index. JPQ is based on Product Quantization (PQ) which quantizes the document embeddings $\vec{d}^\phi$ where $\phi$ denotes they are quantized. Quantization occurs by generating $M$ sets of embedding, where each set has $K$ embeddings of dimension $D/M$ called centroid embeddings $\vec{c}_{i,j}$. A quantized embedding for a document is constructed by selecting one embedding from each set and concatenating them. The matching between queries and quantized embeddings is performed using approximate score function $s^\phi(q, d) = \langle \vec{q},\vec{d}^\phi \rangle$. The search in PQ happens efficiently between query embeddings and quantized document embeddings by generating query sub-vectors and matching them to centroid embeddings.\\

The traditional dense retrieval models \cite{luan-etal-2021-sparse, zhan2020repbertcontextualizedtextembeddings} use the pairwise loss,
$l(s(q,d^+), \\ s(q,d^-))$ where $q$ is the query and $d^+ \in D_q^+$ indicates a positive document and $d^- \in D_q^+$ indicates a negative document with respect to the query. To adopt this loss for quantization setting the score function $s(q,d)$ has to be replaced by $s^\phi(q,d)$. Prior quantization approaches do not adopt a ranking-oriented loss and train the retrievers/dual-encoders independently of compression. To jointly train the quantization and retrieval approach end-end,
JPQ trains the query encoder and PQ index jointly for the complete optimization objective formulated as
\begin{equation}
\langle f^*, \{c_{i,j}\}^* \rangle = \arg\min_{f,\{c_{i,j}\}} \sum_{i}\sum_{d^+ \in D^{+}_i}\sum_{d^- \in D^{\phi-}_i} \ell(s^\phi(q, d^+), s^\phi(q, d^-))
\end{equation} 
, where $D^{\phi-}_i$ refers to the retrieved hard-negatives. Optimizing encoders to penalize hard-negatives is crucial to retrieval performance and JPQ obtains real-time hard-negatives by retrieving hard-negatives with respect to the current query from the quantized index being learned simultaneously thus optimizing for retrieval and quantization performance.

% A ranking-oriented loss is used to precisely measure the disparity between the PQ index and dual encoders. Rebuild the quantized document embeddings, denoted as $d^{\dagger} = c_{1,\varphi_1(d)}, c_{2, \varphi_2(d)}, \ldots, c_{M, \varphi_M(d)}$ and calculate the relevance scores by measuring the similarity between the query and document with $s^{\dagger}(q,d) = \langle q, d^{\dagger} \rangle$. Subsequently, these scores are fed into a pair-wise loss function $ \ell(s^\dagger(q, d^+), s^\dagger(q, d^-)) $ to compute the loss accurately.
    
    % \item Training the PQ index with the ranking-oriented loss is challenging due to differentiability issues and the large number of index assignments, which can lead to overfitting. Therefore the PQ centroid optimization approach uses supervised signals for updating the PQ index and allows PQ parameters to evolve the query encoder through $\vec{q_i}$, ensuring efficient refinement and adaptation. This is done by initialising a small set of PQ centroid embeddings, which are differentiable and compact, and updates them through gradient descent:
    % \begin{equation}
    %     \small
    %     \frac{\partial \ell (s^\dagger(q, d^+), s^\dagger(q, d^-))}{\partial c_{i,j}} = 
    %     \begin{cases}
    %         -\alpha \vec{q_i}, & \text{if } j = \varphi_i(d^+), j \neq \varphi_i(d^-) \\
    %         \alpha \vec{q_i}, & \text{if } j \neq \varphi_i(d^+), j = \varphi_i(d^-) \\
    %         0, & \text{if } j = \varphi_i(d^+), j = \varphi_i(d^-) \\
    %         0, & \text{if } j \neq \varphi_i(d^+), j \neq \varphi_i(d^-) \\
    %     \end{cases}
    % \end{equation}
    
    % \item Incorporating end-to-end dynamic hard negative sampling, as demonstrated in the author's previous work\cite{zhan2021optimizing}, can further bolster top-ranking performance. This technique involves penalizing the scores of top-ranked irrelevant documents treated as negatives as these have more significance on ranking performance compared to low-ranked documents that are often cut-off by truncated evaluation metrics. The process entails the real-time retrieval of negative samples by leveraging current PQ parameters to extract the top-$\hat{n}$ irrelevant documents as negatives at each training step. The incorporation of retrieved negatives ($D^{-^\dagger}_{q}$) helps minimize the top-$\hat{n}$ pairwise errors, thereby aligning with the truncated evaluation metric. The formulation for $D^{-^\dagger}_{q}$, with C being the entire document corpus and $D^+_{q^\dagger}$ the labeled relevant documents, can be expressed as:
    % \begin{equation}
    % D^{-^\dagger}_{q} = \text{sort} \left( d \in C \setminus D^+_{q^\dagger} \text{ based on } s^\dagger(q, d) \right) [: \hat{n}]
    % \end{equation} 




% \section{Problem Statement}
% The presence of misinformation in claims poses a significant challenge that needs to be addressed. There are numerous ways to face this challenge. Therefore for our research, we will solely focus on enhancing the efficiency of detecting whether claims are supported or not supported through a fact-checking pipeline. Our approach involves constructing an efficient index of facts sourced from known knowledge repositories to facilitate grounding the generated content. Our emphasis lies on optimizing for efficiency in terms of both time and space consumption, enabling the deployment of end-to-end pipelines on less resource-intensive hardware without compromising performance. Thus our primary contribution lies in refining the index retrieval process, while maintaining comparable fact-checking performance. This refinement allows for faster information retrieval, improving the overall efficiency of the pipeline. \\

% A core part of our research revolves around indexing supporting facts. This is illustrated in \autoref{fig:original_pipeline} where we display how we intend to change the general existing state-of-the-art pipeline structure (\autoref{subfig:a_pipeline}) to one with more efficient components (\autoref{subfig:b_pipeline}). Here for our study, emphasis is put on how to identify relevant text spans, thus filtering out anything that is not a fact to construct a concise fact database. To guide our research, we formulate the following research questions: 
% \begin{itemize}
%     \item \textbf{RQ 1: How does indexing supporting facts improve information retrieval efficiency?} \\
%         Specifically, we seek to understand how incorporating the supporting facts into the indexing process influences the retrieval latency of relevant information about a claim and the index size on disk. Additionally, this also involves utilising different retrieval methods (sparse and dense retrieval) to more generally see the effects it has on the latency. In the end, we want to see to what extent storing just the supporting facts in the index and performing retrieval on it will degrade performance loss. Additionally we also want to showcase the benefits of this approach over the existing approach of storing the entire document corpus as an index.
        
%     \item \textbf{RQ 2: How does indexing supporting facts affect overall pipeline efficiency and downstream fact-checking performance?} \\
%         The focus here shifts from the information retrieval part towards assessing the broader implications of indexing supporting facts on the overall efficiency of the fact-checking pipeline. Here analysing the overall inference time as well as fact-verification performance becomes important. The aim here is to replace parts of the pipeline to make it more efficient in inference and memory footprint, while trying to achieve approximately the same results compared to the original full-pipeline setting.
    
%     \item \textbf{RQ 3: In what ways does index compression enhance the efficiency of dense retrieval and fact-checking systems?} \\
%     In particular to the dense retrieval part, our objective is to leverage an existing index compression technique to significantly diminish the size of the index stored on disk. Furthermore, we aim to evaluate whether comparable retrieval results can be achieved despite the substantial reduction in index size. By exploring the feasibility of maintaining retrieval effectiveness through a significantly smaller index footprint, we aim to not only optimize storage utilization, but also enable faster query processing, ultimately improving overall system efficiency.
% \end{itemize}

% By addressing these questions, we aim to provide insights into optimizing the efficiency of fact-checking processes for existing pipelines, ensuring robustness in terms of performance while minimizing resource requirements.



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Improving efficiency}\label{sec:improving efficiency}
% In this section, some concepts for improving efficiency are explained. These are the re-ranking setup that tackles selecting the supporting facts, the use of Dense retrieval compared to Sparse retrieval and lastly, Index compression to further improve Dense retrieval.



% \subsection{Extracting Supporting Facts}\label{ssec:reranking}
% \begin{figure}[hbt!]
%   \centering
%   \includegraphics[width=\textwidth]{figures/pipeline architecture/hover reranking setup.png}
%   \caption{The first adjustment to the pipeline using the Re-ranking setup, eliminating the need for fine-grained top-k sentence selection.}
%   \label{fig:hover_reranking}
% \end{figure}

% A first improvement that could be made is storing only the salient sentences in the data corpus instead of every sentence, as seen in \autoref{fig:hover_reranking}. As a result, this will reduce the total pipeline runtime drastically and also save disk space for the index. There are various ways to extract supporting facts from a text document from the data corpus. Manually curating the text of the document to only keep the most important ones is the best way to get great performance results. However, this is not only a very time-consuming process, but also does not scale well when using a different data corpus for different use cases.

% \begin{figure}[hbt!]
%   \centering
%   \includegraphics[width=\linewidth]{figures/claim_detection.png}
%   \caption{A general depiction of claim detection model training and deployment.}
%   \label{fig:claim_detection}
% \end{figure}

% \paragraph{Claim-detection model} As an automated alternative, a claim-detection model can be employed. The idea here is to train a model to specifically detect important sentences out of a text span (see \autoref{fig:claim_detection}). Important here is that the model is trained with enough training data to distinguish non-salient from salient sentences. For our experiment, we will be utilising an off-the-shelf pre-trained model as we do not particularly focus on getting the best performance and instead will just be comparing different methods for efficiency. This choice allows us to fully put our efforts towards experimentation and facilitates easy comparison of various methods without the need for extensive training or fine-tuning efforts.

% \begin{figure}[hbt!]
%   \centering
%   \includegraphics[width=1\linewidth]{figures/cite_extracts.png}
%   \caption{Citation extraction for the lead section of the Delft Wikipedia page.}
%   \label{fig:cite_extract}
% \end{figure}

% \paragraph{Citation Extraction} An alternative approach for our use case involves sentence extraction based on citations. This concept relies on the underlying assumption that authors include references and citations of reliable sources in parts of the text, that they consider particularly crucial. In other words, citations serve as indicators of claim-worthy sentences within a text. This methodology accelerates the extraction process, eliminating the need to train a model specifically for the task of identifying or selecting claim-worthy sentences. Furthermore, this approach provides a higher level of confidence that the retained sentences are indeed claim-worthy. This stands in contrast to models trained for claim detection or claim extraction, which, despite achieving high accuracy, may still make errors by misclassifying non-salient sentences as salient. A commonly used knowledge data collection is that of Wikipedia, where there are three distinct citation types\footnote{https://en.wikipedia.org/wiki/Wikipedia:Citing\_sources}. These are: `Inline citations' that place citations close to the text they support, `In-text attribution' that names the source of a statement, and `General reference' that is not linked to any particular piece of material. In our use case, as depicted in the Delft Wikipedia page\footnote{https://en.wikipedia.org/wiki/Delft} example in \autoref{fig:cite_extract}, the strategy involves using the original evidence page and extracting the sentences containing references. These are indicated by its numerical reference notation or footnote. By doing so, we will effectively be able to retrieve the inline and in-text attribution types of citations. However, we will also miss the general reference type as we only extract the closest sentence. In Wikipedia's case, a single cited source can support consecutive sentences, which is indicated by a single citation at the end of the final sentence and thus not for each sentence. In our simplistic automated approach, we will not know what sentences belong to a consecutive sequence and just extract the closest sentence to its citation. Consequently, this will cause our approach to miss the surrounding sentences that may also belong to that citation.

% \begin{figure}[hbt!]
%   \centering
%   \includegraphics[width=1\linewidth]{figures/Fusion.png}
%   \caption{Fusion setup showing claim detection being used when citation extraction is not possible. Based on the `Shonen Jump (magazine)` Wikipedia page.}
%   \label{fig:fusion_pipeline}
% \end{figure}

% \paragraph{Fusion of Citation extraction and Claim Detection} While cited sentences are valuable for providing trustworthy support, they may not always be abundant enough to rely on exclusively. While some pages boast numerous well-sourced references, others may have only a handful or even none at all. Additionally, due to the way we extract citations, there may be some surrounding sentences missed that are also part of the citation. In both such instances, citation extraction might overlook valuable claim-worthy information present in the text. Take for example \autoref{fig:fusion_pipeline} showing that the \textit{`Shonen Jump (magazine)`}\footnote{https://en.wikipedia.org/wiki/Shonen\_Jump\_(magazine)} article does not have any citations in the given text span. Here one can see that while no references are used, there are sentences present that are worthwhile to consider as supporting facts. While preserving the original text could be considered a valid option, it is important to note that for external knowledge sources beyond Wikipedia, citations may only be available for a small percentage of data. As a consequence, this leads to a negligible reduction of the data in those cases. Thus, to optimize efficiency and achieve our end goal, a more effective approach is to utilize claim detection whenever citations have not been extracted. This strategy ensures that entries in the data collection without citations still capture potential supporting facts, while simultaneously reducing the overall corpus size. In this approach, it's important to keep in mind that the inference latency for detecting supporting facts now depends not only on the number of citations present in a corpus, but also on the inference process of the claim detection model. 

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Evidence Retrieval}\label{ssec:dense_retrieval}
% \begin{figure}[hbt!]
%   \centering
%   \includegraphics[width=0.8\textwidth]{figures/pipeline architecture/hover dense retrieval.png}
%   \caption{The second adjustment to the pipeline in replacing Sparse Retrieval with Re-ranking stages with a single Dense Retrieval module.}
%   \label{fig:hover_dens_retrieval}
% \end{figure}

% In addition to employing re-ranking to reduce the corpus size, we can incorporate Dense Retrieval as a more efficient alternative to Sparse Retrieval and re-ranking. This is as depicted in \autoref{fig:hover_dens_retrieval}. Like is mentioned in \autoref{sec:fact_checking}, the evidence retrieval consists of retrieving data that most likely contains the answer followed by selecting the top relevant ones pertaining to the query/claim. The first step is usually done using a Sparse retrieval setup, as depicted in \autoref{subfig:a}. This setup however is very costly to run, as one would require a module for retrieving and a module for re-ranking. As the re-ranking setup eliminates the need to perform sentence selection to reduce non-relevant sentences, we can essentially directly retrieve the top-k documents from the newly formed data corpus using Dense Retrieval (see \autoref{subfig:b}) This is because we no longer need the other two-step process for iteratively increasing the chances of getting the most relevant documents pertaining to a claim. Additionally, in the current state-of-the-art systems, Neural network-based approaches are used as re-rankers to enhance Sparse retrieval by leveraging techniques like word2vec for semantic similarity assessment, pre-trained models such as BERT for predicting term importance, and term expansion methods to enhance recall. These advancements significantly augment traditional methods by providing more nuanced understandings of query-document relationships and improving retrieval performance from a semantic aspect. Here Dense retrieval can potentially benefit the fact-checking pipeline, as it can immediately in its first stage retrieval give high similarity scores in the to semantically relevant text pairs, even without exact token matching \cite{karpukhin2020dense}. \\

% \begin{figure}[hbt!]
%     \begin{subfigure}{\textwidth}
%     \centering
%     \includegraphics[scale=0.4]{figures/sparse_retrieval.png}
%     \caption{Sparse Retrieval with Re-ranker}
%     \label{subfig:a}
%     \end{subfigure}
%     \begin{subfigure}{\textwidth}
%     \centering
%     \includegraphics[scale=.4]{figures/dense_retrieval.png}
%     \caption{Dense Retrieval}
%     \label{subfig:b}
%     \end{subfigure}
%     \caption{Retrieval methods for retrieving top-k relevant documents pertaining to a query.}\label{fig:retrieval_methods}
% \end{figure}


 

% Although implementing Dense Retrieval offers a more streamlined approach, it is important to also acknowledge its limitations compared to the original two-step retrieval approach. A notable limitation is that the dense representation is typically inferior to the sparse representation of data \cite{karpukhin2020dense}. To elaborate, the use of Dense Retrieval necessitates the creation of vector embeddings capable of encapsulating the entirety of semantic information within the text. This task becomes very challenging, particularly for extensive textual spans. Thus, while Dense Retrieval enhances overall computational efficiency by simplifying the process, it may not necessarily surpass the original pipeline's retrieval methods due to the complexity of capturing comprehensive semantic representations. However, we do expect the performance to be within a few points difference. 

% % To further enhance efficiency and impact performance, an additional step can be incorporated by selecting only the top-k sentences that exhibit the highest similarity to the given claim. This strategic inclusion aims to mitigate the issue of topic drift during claim verification. Instead of analyzing entire document texts, we focus on extracting the most pertinent sentences directly related to the claim. This process involves calculating the cosine similarity score for each sentence within the top candidate documents obtained from dense retrieval. By cherry-picking the top-k sentences based on their similarity scores, we narrow down our analysis to the most relevant and significant content, thereby optimizing the claim verification process.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Index Compression}

% \begin{figure}[hbt!]
%   \centering
%   \includegraphics[width=\textwidth]{figures/pipeline architecture/hover index compression.png}
%   \caption{The third adjustment to the pipeline with the added Index Compression module to enhance Dense Retrieval.}
%   \label{fig:hover_index_compression}
% \end{figure}

% As described in \autoref{sec:compression}, leveraging Quantization can significantly enhance efficiency in the Dense Retrieval setup. In our research, we will adopt the JPQ model introduced by Zhan et al. \cite{zhan2021jointly} into the fact-checking pipeline (see \autoref{fig:hover_index_compression}). JPQ, short for \textit{Joint optimization of query encoding and Product Quantization}, offers a novel approach by optimizing ranking performance in an end-to-end manner, departing from the traditional encoding-compression two-step process. Conventional compression techniques indeed enhance the efficiency of Dense Retrieval. However, these methods typically rely on a task-independent reconstruction error as the loss function during training. Moreover, the encoders and compressed index are trained separately. As a consequence of these design choices, compression fails to capitalize on supervised information, and the compatibility between the query-doc encoders and the compressed index might not be optimally aligned, overall leading to sub-optimal performance. \\

% To resolve this, JPQ trains the query encoder and PQ index jointly based on three optimization strategies. 
% \begin{enumerate}
%     \item First, the model employs a ranking-oriented loss to precisely measure the disparity between the PQ index and dual encoders.  Computation of this loss involves reconstructing the quantized document embeddings, denoted as $d^{\dagger} = c_{1,\varphi_1(d)}, c_{2, \varphi_2(d)}, \ldots, c_{M, \varphi_M(d)}$. This is then used in embedding similarity computation between the query and document, to derive the relevance scores $s^{\dagger}(q,d) = \langle q, d^{\dagger} \rangle$ utilized by PQ for ranking purposes. Subsequently, these scores are fed into a pair-wise loss function to compute the loss accurately: 
%     \begin{equation} 
%     \ell(s^\dagger(q, d^+), s^\dagger(q, d^-))
%     \end{equation}
    
    
%     \item Second, PQ centroid optimization is used to train the PQ index with the ranking-oriented loss. This would otherwise be non-trivial due to problems related to differentiability and the substantial number of index assignments (proportional to the corpus size) leading to potential overfitting. The PQ centroid optimization approach initializes a small set of PQ centroid embeddings and updates the embedding through gradient descent, defined as:
%     \begin{equation}
%         \small
%         \frac{\partial \ell (s^\dagger(q, d^+), s^\dagger(q, d^-))}{\partial c_{i,j}} = 
%         \begin{cases}
%             -\alpha \vec{q_i}, & \text{if } j = \varphi_i(d^+), j \neq \varphi_i(d^-) \\
%             \alpha \vec{q_i}, & \text{if } j \neq \varphi_i(d^+), j = \varphi_i(d^-) \\
%             0, & \text{if } j = \varphi_i(d^+), j = \varphi_i(d^-) \\
%             0, & \text{if } j \neq \varphi_i(d^+), j \neq \varphi_i(d^-) \\
%         \end{cases}
%     \end{equation}
    
%     These embeddings are differentiable and due to their compact size avoid the problem of overfitting. The PQ centroid optimization strategy benefits from supervised signals guiding updates within the PQ index compared to other existing approaches. Additionally, PQ parameters directly evolve the query encoder through $\vec{q_i}$, which is part of the encoder's output, ensuring efficient refinement and adaptation.
   


    
%     \item  Finally, incorporating end-to-end negative sampling can further bolster ranking performance. Previous research by the authors \cite{zhan2021optimizing} demonstrated that dynamic hard negative sampling contributes to enhancing top-ranking performance. This technique involves penalizing the scores of top-ranked irrelevant documents treated as negatives. The rationale behind this approach lies in the significant impact that top-ranked negatives have on ranking performance, whereas low-ranked documents are often cut-off by truncated evaluation metrics. The process entails the real-time retrieval of negative samples by leveraging current PQ parameters to extract the top-$\hat{n}$ irrelevant documents as negatives at each training step. The incorporation of retrieved negatives ($D^{-^\dagger}_{q}$) helps minimize the top-$\hat{n}$ pairwise errors, thereby aligning with the truncated evaluation metric. The formulation for $D^{-^\dagger}_{q}$, with C being the entire document corpus and $D^+_{q^\dagger}$ the labeled relevant documents, can be expressed as:
%     \begin{equation}
%     D^{-^\dagger}_{q} = \text{sort} \left( d \in C \setminus D^+_{q^\dagger} \text{ based on } s^\dagger(q, d) \right) [: \hat{n}]
%     \end{equation} 

% \end{enumerate}


% In essence, JPQ jointly optimizes the query encoder and PQ Centroid Embeddings through end-to-end negative sampling and ranking-oriented loss computation. The entire optimization objective of JPQ can thus be formulated as follows: 
% \begin{equation}
% f^*, \{c_{i,j}\}^* = \arg\min_{f,\{c_{i,j}\}} \sum_{q}\sum_{d^+ \in D^{+}_q}\sum_{d^- \in D^{-}_q} \ell(s^\dagger(q, d^+), s^\dagger(q, d^-))
% \end{equation} 


% Given our reliance on establishing an efficient index for accurate retrieval, the work by JPQ \cite{zhan2021jointly} aligns closely with our work. Having optimally compatible trained dual-encoders with a PQ index can help ensure the corpus index footprint in memory is small. This can be seen in the reported compression ratio of \textit{4D/M}, where \textit{D} represents the vector dimensionality and \textit{M} denotes the number of codebooks used for compression. Concurrently, this approach facilitates efficient retrieval of supporting facts while maintaining performance levels comparable to those of standard Dense Retrieval setups, albeit without the overhead of a compressed index. This is evidenced by the reported speedup ratio of $({D + \log n})/({M + \log n})$, with \textit{n} being the total number of documents in the index.