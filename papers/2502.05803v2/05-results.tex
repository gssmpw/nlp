\section{Results}
\label{sec:results}
% In this section, we answer the research questions formulated in Section \ref{experiments}.

\subsection{Effectiveness of indexing succinct facts to improve information retrieval efficiency} To answer \textbf{RQ1}, we measure the memory and computational costs of fact-checking using full-Wikipedia compared to the pruned version proposed in this work. We first measure the index size on disk, measuring the raw JSON file size containing the article titles and texts, for each experiment setting. In \autoref{fig:disk_size}, we observe a significant reduction in disk space usage with HoVer having a reduction ranging from \textbf{44-55\%}, and WiCE from \textbf{44-57\%}. Additionally, the number of sentences stored in the index also decreases, with HoVer showing a reduction from 52-61\% and WiCE 52-59\%, indicating that at least half of the sentences are not helpful in claim verification.

\input{tables/comparison_size}

 Following the reduction in disk size, a notable improvement in retrieval latency is evident, as demonstrated in \autoref{tab:bm25_latency}.
Regarding document retrieval latency (which encompasses both column values), there's an observed speedup ranging from approximately 1.5x (334 ms) to 1.6x (316 ms) compared to the original experimental setting for HoVer (495 ms). Similarly, in WiCE experiments, we witness a comparable speedup rate ranging from 1.4x (446 ms) to 1.6x (399 ms) compared to the original experimental setting (636 ms). This observation suggests that while the reduced text size contributes to efficient retrieval, it could further be improved.

\input{tables/latency/bm25_latency}

% \input{tables/latency/faiss_latency}

% However, sparse retrieval does not capture semantic information and requires a costly re-ranking stage. While transitioning from Sparse to Dense retrieval may help improve the performance dense retrieval introduces additional computational costs as seen in Table \ref{tab:faiss_latency}. This is due to our use of FAISS utilising fixed-dimensionality vectors where despite varying article text lengths, the constant number of text embeddings minimizes impact on retrieval speed. However, dense retrieval libraries offer GPU support, which can yield substantial speedups compared to the CPU-based retrieval of BM25 and FAISS. GPU retrieval shows substantial speedups: 16.6-22.3x for HoVer and 17.9-20.2x for WiCE compared to CPU retrieval. Compared to BM25, GPU retrieval offers 16.0-21.5x speedup for HoVer and 18.7-20.5x speedup for WiCE. Thus, making Dense Retrieval a highly efficient and viable option over standard Sparse Retrieval with re-ranking.
%\vspace{-0.5em}

\noindent \textbf{Insight 1}: \textit{
Extraction of succinct facts reduces storage requirements and improves latency for Sparse retrieval while only leading to a minor loss in task performance.}
\vspace{-2em}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Effectiveness of pruned knowledge sources on overall pipeline efficiency and downstream fact-checking performance?}
To answer \textbf{RQ2}, we now shift focus to analyzing the inference time throughout the entire pipeline instead of solely the retrieval part. Extraction of just supporting facts not only has a improvement in the retrieval stage but also on the overall inference latency across the pipeline. For HoVer this being a 1.9-2.0x speedup and 1.8-2.0x for WiCE experiments. This improvement can be attributed to not only faster retrieval times but also the elimination of the Sentence Retrieval stage, which previously imposed significant latency overhead. 

%\input{tables/hover/hover_performance}
% \begin{figure}
%     \centering
%     \includegraphics[width=0.82\linewidth]{figs/graphs/hover_perf.png}
%     \caption{HoVer performance comparison}
%     \label{fig:enter-label}
% \end{figure}
% %\input{tables/wice/wice_performance}

% \begin{figure}[hbt!]
%     \centering
%     \includegraphics[width=0.85\linewidth]{figs/graphs/wice_perf.png}
%     \caption{WiCe performance comparison}
%     \label{fig:enter-label}
% \end{figure}

\begin{figure*}[hbt!]
    \begin{subfigure}{.5\textwidth}
        \input{figs/Hover_plot}
\subcaption{HoVer}
    \end{subfigure}
        \begin{subfigure}{.5\textwidth}
    \input{figs/Wice_plot}
    \subcaption{Wice}

    \end{subfigure}
    \caption{HoVer and WiCe task performance (FW- Full-Wiki, FE - Fact Extraction, IC- Index Compression, CE - Citation Extraction, Fu - Fusion)}
    \label{fig:performance_plot}
    \end{figure*}

\begin{figure*}[hbt!]
    \begin{subfigure}{.5\textwidth}
        \input{figs/Hover_retrieval_plot}
\subcaption{WiCE (nDCG@10)}
    \end{subfigure}
        \begin{subfigure}{.5\textwidth}
    \input{figs/Wice_retrieval_plot}
    \subcaption{WiCE (Recall@10)}

    \end{subfigure}
    \caption{Retrieval performance comparison}
    \label{fig:retrieval_perf}
    \end{figure*}
The evaluation of Sparse and Dense Retrieval setups in HoVer and WiCE experiments reveals that Sparse Retrieval, particularly fact extraction (FE) and Fusion approaches, maintains performance closest to the Full-Wiki setup as measured by weighted F1 in Figure \ref{fig:performance_plot}, while citation extraction has a larger drop in performance. Most notably, the Fusion method compared to the other methods has relatively high scores, underscoring the importance of combining supporting facts extraction methods for optimal results. We also report retrieval performance for WiCE Figure \ref{fig:retrieval_perf} using measures nDCG@10 and Recall@10 using annotated documents provided for WiCE. We observe trends similar to overall task performance demonstrating that efficient retrieval approaches explored do not negatively impact task performance to a significant extent.

\noindent\textbf{Insight 2}: \textit{We find that extracting supporting facts improves efficiency across the entire pipeline, with Sparse setups achieving up to 2.0x speedups with only a minimal performance decline.}
\vspace{-1em}

\input{tables/latency/summarized_results}
%\input{tables/latency/jpq_latency}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-2em}
\subsection{Effectiveness of index compression on enhancing the efficiency of dense retrieval and fact-checking systems?}

To answer \textbf{RQ3}, we make use of index compression to further improve upon Dense Retrieval setups, not only with respect to memory requirements but also enhancing total inference latency compared to the sparse retrieve + re-rank setups in classical pipelines.  The index sizes of Wikipedia collection for standard dense retrieval are 7.51 GiB for HoVer and 9.70 GiB for WiCE. Using the JPQ index compression model with M=96 subvectors, we significantly reduced the storage space for vector embeddings from 1.5 KiB to 104.12 B. This reduced the HoVer index size to 544.89 MiB and the WiCE index to \textbf{672.95 MiB}, achieving a \textbf{93\% reduction (14.4:1 compression ratio)}. Further reducing subvectors could decrease the index size but may impact performance.


The utilization of JPQ index compression leads to significant reductions in retrieval latency compared to dense Retrieval and sparse retrieval, as demonstrated in \autoref{tab:jpq_latency}. CPU retrieval gains notable speedups of approximately 10.0x for HoVer experiments and 7.0x for WiCE experiments, while GPU retrieval shows about 2.0x and 0.8x speedups, respectively. These improvements are attributed to learned compression in JPQ, enhancing computational efficiency. 
Significant improvements are also observed when examining the inference latency of the whole pipeline. The CPU-based approaches shows impressive speedups (upto \textbf{12.9x} for HoVer and \textbf{8.5x} for WiCE), and GPU-based approaches achieve even higher gains (\textbf{33.0x} for HoVer and \textbf{18.1x} for WiCE). 

Surprisingly, in our experiments we observe that JPQ yields better results than standard Dense Retrieval as shown in Figure \ref{fig:performance_plot}. This is particularly due to joint training of the query encoder and index compression. In addition, JPQ employs end-end negative sampling, which further improves retrieval performance despite significant compression of embeddings.

\mpara{Insight 3}: \textit{We find that index compression reduces index size by \textbf{93\%} resulting in speedups for CPU-based setups up to 10x and GPU-based setups more than 20x compared to classical fact-checking pipeline.}

\subsection{Discussion of Live Fact-checking results}
We employ the pruned index (2024 Wiki collection) using our Fusion approach followed by compression of the index for live Fact-checking of 2024 presidential debate. The pipeline comprises a dense retrieval using compressed index followed by claim verification. We use the query encoder and NLI models trained on HoVer for this application. We compare this approach to also the classical sparse-retrieval+re-rank fact-checking pipeline over the Full-Wiki collection (without pruning). The task performance is shown in Figure \ref{fig:livefc} and the corresponding pipeline efficiency is shown in Table \ref{tab:livefc}. We observe that the pruned collection coupled with retrieval using index compression leads to impressive speedups (\textbf{3.4x}) over classical pipeline over the full collection without significant drop in task performance (Figure \ref{fig:livefc}). The results demonstrate that efficient retrieval is critical for live fact-checking. Our experiments demonstrate that our approach for efficient retrieval provides significant speedups on CPUs further making the technology accessible even in low-resource scenarios which has significant implications in aiding detection of misinformation and disinformation at scale.
\begin{figure}
\centering
 \hspace{6em}     \begin{subfigure}{.8\textwidth}
        \input{figs/live_fc}

    \end{subfigure}
    \caption{Live fact-checking performance across different corpus setups}
    \label{fig:livefc}
\end{figure}
\vspace{-2em}
\input{tables/live_factcheck/latency_live_factchecking}
% \section{RQ 1: How does indexing supporting facts improve information retrieval efficiency?}
% In this section, we investigate the impact of indexing supporting facts on information retrieval efficiency by comparing the disk space utilization and retrieval latency across different experiment settings. Here we aim to discern the benefits of storing only supporting facts in the index as opposed to the entire corpus. 

% \subsection{Corpus Size}
% \input{tables/comparison_size}
% To get an idea of how storing just the supporting facts data in the index improves efficiency compared to storing the entire corpus, a comparison can be made on how much these different settings occupy disk space. As mentioned in \autoref{sec:metrics}, to get an accurate estimate, only the dictionaries containing the article's title and document text are saved to raw JSON files. Across all experiment settings as seen in \autoref{fig:disk_size},  a notable reduction in disk space usage is observed compared to the original Wikipedia document corpus. This reduction ranges from approximately 45\% (claim detection) to 55\% depending on the setting for the HoVer corpus data. Likewise, for the WiCE corpus data, we can observe approximately 44\% to 57\% reduction. Moreover, in correlation with the reduced disk size, it is evident that the number of sentences stored in the index also decreases across each experiment setting compared to the original corpus data. For HoVer this ranges from 52\%  (claim detection) to 61\% (citation extraction) and WiCE ranges from 52\% to 59\%. This indicates that at least half of the sentences are considered as not claim-worthy across the different re-ranking methods.

% \subsection{Retrieval Latency}\label{ssec:retrieval_latency}
% \paragraph{Sparse retrieval} Following the reduction in disk size, a notable enhancement in retrieval latency is evident, as demonstrated in both the Term-based and Neural-based document retrieval columns of \autoref{tab:bm25_latency}. To avoid any ambiguity, it's crucial to clarify that the speedup listed in the table pertains to the total latency, which is relevant for addressing RQ2, rather than solely focusing on document retrieval.
% Regarding document retrieval latency (which encompasses both column values), there's an observed speedup ranging from approximately 1.5x (334 ms) to 1.6x (316 ms) compared to the original experimental setting for HoVer (495 ms). Similarly, in WiCE experiments, we witness a comparable speedup rate ranging from 1.4x (446 ms) to 1.6x (399 ms) compared to the original experimental setting (636 ms). This observation suggests that while the reduced text size contributes to expedited retrieval, the enhancement is only somewhat proportional.

% \input{tables/latency/bm25_latency}
% \paragraph{CPU-based Dense Retrieval} One might typically anticipate a more pronounced disparity between the original data and the reranked data in the document retrieval phase. However when transitioning from the Sparse retrieval setup to the Dense retrieval setup, as depicted in the first column of \autoref{tab:faiss_latency}, only negligible differences between the different settings are observed. This is attributed to FAISS utilizing vectors instead of computing the relevance ranking of documents to the query, as is the case with BM25. Despite variations in the length of each article across settings, the number of text embeddings (with fixed dimensionality size) created remains constant, corresponding to the number of encoded text spans, which is consistent across settings. Thus minimizing the impact of extracting supporting facts on document retrieval latency when using Dense Retrieval. Comparing the Dense document retrieval (CPU) column in \autoref{tab:faiss_latency} to the baselines listed in \autoref{tab:bm25_latency}, it is observed to be of a similar latency or even slightly slower. For HoVer, we can observe a 0.9x (523 ms) to 1.0x (479 ms) compared to the baseline (495 ms). Likewise, for WiCE we can observe a similar latency speedup of 0.9x (685 ms) to 1.0x (610 ms) speedup compared to its baseline (636 ms). This suggests that the indexing of supporting facts would not significantly impact information retrieval efficiency in such scenarios. 

% \input{tables/latency/faiss_latency}

% \paragraph{GPU-based Dense Retrieval} However, it is worth noting that Dense retrieval can still be faster, particularly with dense retrieval libraries such as FAISS offering GPU support, which can yield substantial speedups compared to both CPU retrieval of BM25 and FAISS. This advantage is evident in the data, showcasing notable speedups ranging from 16.6x to 22.3x speedup for HoVer GPU retrieval over CPU retrieval, and 17.9x to 20.2x speedup for WiCE. Furthermore, when comparing FAISS GPU retrieval to the BM25 retrieval, we can see an approximate 16.0x (31 ms) to 21.5x (23 ms) speedup for HoVer and 18.7x (34 ms) to 20.5x (31 ms) speedup for WiCE. Therefore the GPU-based approach makes Dense Retrieval a viable option, unlike the CPU-based variant. 

% \subsection{Key Takeaways} 
% Extracting supporting facts from the data corpus can lead to only requiring to store at least half of the data. Although this has a positive effect on the latency for Sparse retrieval, with Dense document retrieval this is not the case due to how the vector embeddings are constructed (being per article rather than per sentence). Furthermore, while CPU-based Dense retrieval may not necessarily outperform Sparse retrieval methods in terms of latency, thereby presenting less immediate appeal, the incorporation of GPU support leads to significant speed enhancements. Thus, although extracting supporting facts does not help much in Dense document retrieval unlike Sparse retrieval in terms of retrieval latency, the incorporation of the GPU-based Dense retrieval renders it a much more compelling option for achieving efficiency. 

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \section{RQ 2: How does indexing supporting facts affect overall pipeline efficiency and downstream fact-checking performance?}
% In continuation of the previous research inquiry concerning retrieval latency and disk size, this section delves into an analysis of the overall inference time across the entire pipeline. Additionally, recognizing that faster processing times do not necessarily equate to better performance a further analysis will be done on the performance metrics.

% \subsection{Inference Latency}
% \paragraph{Sparse Retrieval Setup:} The enhancement in retrieval latency, as evidenced in \autoref{tab:bm25_latency}, mirrors a noticeable improvement in the overall inference latency across the pipeline. This improvement spans approximately 1.9x to 2.0x for the HoVer experiments and 1.8x to 2.0x for WiCE experiments. However, the reduction in total latency cannot be solely ascribed to faster retrieval times. It also arises from the elimination of the Sentence Retrieval stage, which previously imposed significant latency overhead. Upon closer inspection of \autoref{tab:bm25_latency}, it becomes apparent that the absence of the Sentence Retrieval stage impacts the Claim Verification stage. Notably, experiments conducted on the original corpus data exhibit much lower inference latency compared to supporting facts data. Nevertheless, the variance between these experiment settings is minimal, and the impact on total latency results is insignificant. This overall trend indicates that indexing supporting facts for the BM25 retrieval setup predominantly benefits inference times for the Rule-based document retrieval and Sentence Retrieval stages. Furthermore, it reveals that the Claim Verification stage is slightly, yet negligibly, affected when considering the entire pipeline inference.

% \paragraph{Dense Retrieval Setup:} In a similar vein as the document retrieval comparisons of RQ1 (see \autoref{ssec:retrieval_latency}), the total inference of the Dense retrieval setup presents notable differences in results between CPU- and GPU-based Dense retrieval compared to Sparse retrieval. This divergence is evident in \autoref{tab:faiss_latency}, where for HoVer experiments, the CPU-based approach exhibits a 1.2x to 1.4x speedup, while the GPU-based approach demonstrates a 16.9x to 21.3x speedup compared to the baseline. Similarly, WiCE experiments show approximately a 1.3x speedup for the CPU-based approach and 19.3x to 20.8x speedup for the GPU-based approach. The key distinction lies in the influence of omitting the Sentence Retrieval stage for the original corpus data. Its omission introduces significant overhead to the total latency. For the CPU-based approach, this translates to a 1.3x speedup for HoVer (676 ms vs. 532 ms) and a 1.4x speedup for WiCE (878 ms vs. 619 ms). Conversely, the GPU-based approach experiences a 4.9x speedup for HoVer (192 ms vs. 39 ms) and a 5.3x speedup for WiCE (227 ms vs. 43 ms). Overall, this underscores that including Sentence Retrieval adds substantial overhead, especially for GPU-based approaches operating with lower latency magnitudes. Therefore, the supporting facts data for Dense Retrieval, while not significantly impacting document retrieval, offers significant speedup for total inference latency, allowing for the effective omission of the Sentence Retrieval stage and its associated latency overhead.

% \subsection{Performance Metrics Evaluation}

% \input{tables/hover/hover_performance}

% \input{tables/wice/wice_performance}

% \paragraph{Sparse Retrieval performance} Utilising the metrics laid out in \autoref{sec:metrics}, the pipeline results have been evaluated for the different settings and laid out in \autoref{tab:hover_performance_metrics} for the HoVer experiments and \autoref{tab:wice_performance_metrics} for WiCE experiments. When comparing the different HoVer experiment settings within the Sparse Retrieval setup, Claim detection comes the closest to the baseline with close to 5.5 points difference across the metrics for the HoVer experiments. Important to note is that Fusion follows close with less than a point difference. For the WiCE Sparse retrieval setup, the opposite occurred with the Fusion data being the closest with a marginal 0.3 point difference followed by Claim detection with a 1.5 points difference. In both datasets, the Citation extraction takes the biggest loss in accuracy that being 6.9 points for HoVer and 2.7 points difference for WiCE. We can reason the fact that citation extraction takes the biggest performance degradation to the fact that not all claim-worthy sentences contain citations, therefore missing out on crucial evidence sentences. Unlike the other settings which consider the complete text instead of only the cited sentences and determine claim-worthiness on what the claim-detection model selects. Overall, relating to the inference time, we can see that for HoVer with a speedup of at least 1.5x to 1.6x, we only lose 6.9 to 5.5 points in performance across various metrics for the best re-ranking setup. Likewise, for WiCE, with a speedup of 1.4x to 1.6x we only lose 2.7 to 0.3 points. This positively demonstrates that indexing just the supporting facts does show meaningful results in terms of overall pipeline efficiency, while maintaining roughly the same performance. Additionally, this also indicates we can achieve good results by using a combination of citation extraction together with another supporting facts extraction method such as Claim detection.

% \paragraph{Dense Retrieval performance} When examining the performance results of Dense retrieval compared to Sparse Retrieval, it becomes evident that there is a slight decline across all experiments. For HoVer, this decline ranges from a modest 0.8 point difference in Claim detection to a more substantial 2.9 points in Fusion data. Similarly, WiCE experiences a loss ranging from a 0.9 difference in accuracy between Claim detection settings to approximately 2.4 points in Citation extraction. Crucially, it is to assess how these performances compare against the baselines. In HoVer, the accuracy loss ranges from 8.3 points for Fusion data to 6.3 points for Claim detection. WiCE experiences a loss ranging from 5.1 points in Citation extraction to 2.4 points in Fusion. These findings suggest that while transitioning from Sparse retrieval to just a Dense retrieval component incurs some loss, it's not substantial across various experiments involving supporting facts data. Moreover, the performance is notably strong in claim detection, while citation extraction lags behind by only a few points. Interestingly, while Fusion performs as well as Citation extraction in HoVer experiments, Fusion data outperforms Claim detection in WiCE. This highlights the significance of combining citation extraction with another supporting facts extraction method to achieve optimal results, similar to the Sparse retrieval setup.

% \paragraph{Sentence Retrieval stage ablation} Comparing experiments on the original data between the two retrieval methods reveals a more significant decline for HoVer, with a loss of 3.2 points with Sentence Selection and 4.9 points without it. For WiCE, the difference is 2.1 points with Sentence Selection and 3.3 points without it. When assessing these losses against the baselines, it becomes evident that both methods generally outperform the supporting facts data experiments by a few points. This suggests that the contribution of the Sentence Retrieval stage in the pipeline to performance improvement is marginal. With the supporting facts extraction thus becomes quite effective in achieving nearly the same performance. Consequently, to enhance efficiency, eliminating this Sentence Retrieval stage would result in only a loss of less than a few points.

% \subsection{Key Takeaways} 
% Incorporating supporting facts into both Sparse and Dense retrieval setups yields noteworthy enhancements in overall pipeline efficiency. Sparse retrieval setups demonstrate speedups ranging from up to around 1.5x, while Dense retrieval setups exhibit even more substantial improvements, achieving up to approximately 20.0x with GPU-based approaches. These notable speedups are primarily attributed to the removal of the Sentence Retrieval stage, which incurs considerable latency overhead. Further evaluation indicates a minor decline in performance when transitioning from Sparse to Dense retrieval, though the loss is not substantial. Specifically, claim detection remains robust, while citation extraction may lag behind by a few points. However, Fusion data yields promising results, often comparable to or outperforming other extraction methods, emphasizing the significance of amalgamating various extraction techniques for supporting facts. Moreover, ablation experiments on the Sentence Retrieval stage reveal its marginal contribution to performance improvement. Comparisons between original data and supporting facts data show only a slight decline in performance, showcasing that utilising only the supporting facts only incurs a modest loss in performance (around 6 points for HoVer and 3 points for WiCE). This suggests that although supporting facts do not affect document retrieval latency in the Dense Retrieval setup, it does help with overall pipeline latency due to avoiding the latency overhead of Sentence Selection. In conclusion, these results underscore the meaningful impact of indexing supporting facts on the overall pipeline efficiency, with only minimal losses in downstream fact-checking performance.

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \section{RQ 3: In what ways does index compression enhance the efficiency of dense retrieval and fact-checking systems?}

% In this final research inquiry concerning the addition of index compression, this section explores how index compression improves upon Dense Retrieval in not only the constructed index size, but also document retrieval and total inference latency. Additionally, a final comparison will be made on the overall performance against Sparse retrieval and standard Dense Retrieval.

% \subsection{Compressed Index Size}
% In our FAISS experiments, we consistently observe an index size of approximately 7.51 GiB across all HoVer settings and 9.70 GiB across all WiCE settings. While one might anticipate that re-ranking would influence the amount of text utilized for generating vector embeddings, it's crucial to note that the index size remains unchanged. This is due to the fact that we generate vector embeddings on a per-article basis with only the text itself being altered. To address this issue, we employed JPQ, an index compression model. Despite using a relatively high number of subvectors for the JPQ model (M=96), we observed a significant reduction in the total index size. Specifically, the individual vector embeddings now occupy only 104.12 B in storage space, down from 1.5 KiB previously. This reduction is remarkable. For the HoVer experiments, the index size decreased from 7.51 GiB to 544.89 MiB, and for the WiCE experiments, we observed a decrease from 9.70 GiB to 672.95 MiB. Overall, this constitutes an impressive reduction of nearly 93\% or a compression ratio of 14.4:1 in index size for both experiment setups. It's worth noting that employing fewer sub-vectors could potentially lead to an even more substantial reduction in index size; however, this would come at the cost of decreased performance.

% \input{tables/latency/jpq_latency}


% \subsection{Pipeline Efficiency}
%  \paragraph{Document Retrieval Latency} When examining the retrieval latency outlined in \autoref{tab:jpq_latency}, a notable observation can be made towards the Dense document retrieval compared to the Dense Retrieval results outlined in \autoref{tab:faiss_latency}. This significant enhancement can be primarily attributed to the utilization of the index compression model, which effectively reduces the index size. As a result, retrieval latency experiences a considerable improvement due to the smaller vector embeddings, facilitating faster similarity computation. Here one can observe a substantial speedup achieved in CPU retrieval of approximately 10.0x across the HoVer experiment settings and 7.0x for WiCE experiments. Similarly, GPU retrieval exhibits a speedup of approximately 2.0x for HoVer experiments and 0.8x for WiCE experiments. This is generally in line with the reported results in the original JPQ paper \cite{zhan2021jointly}. Although the measurements for HoVer fall in line with these reported results, one may notice that the WiCE retrieval speedup is lower than that of HoVer. This is even worse for the GPU-based retrieval latency instead of being better than the standard GPU-based Dense retrieval. We reason this to the fact that the WiCE claim dataset is a lot more complex. In the original WiCE paper, the results that were reported already indicate a not so particularly high performance being achieved. This coupled with the use of a different model for creating the embeddings results in marginally worse performance instead of a speedup such as the case with HoVer. 
 
%  \paragraph{Pipeline Inference Latency} In examining the total inference latency, as further detailed in \autoref{tab:jpq_latency}, the utilization of compressed indexing and the ensuing document retrieval speed enhancements result in a notable boost across the board. The advancements brought about by JPQ, which further build upon the foundations of Dense Retrieval, are particularly significant. Notably, CPU latency has seen a substantial improvement compared to previous benchmarks on the supporting fact data, exhibiting a noteworthy speedup ranging from 10.5x to 12.9x for HoVer experiments, and 8.1x to 8.5x for WiCE experiments relative to their respective baselines. Meanwhile, the GPU-based approach, especially in the case of HoVer experiments, has yielded even more impressive results, achieving speedups ranging from 27.5x to 33.0x. While WiCE experiments on the GPU may not experience such dramatic speedups, they still showcase marked enhancements over their original baselines that range from 17.3x to 18.1x speedups. When assessing the impact of the Sentence Selection stage on the original corpus data settings, the findings reinforce the observations made in the standard Dense Retrieval setup. Furthermore, the disparity in the reported speedups between the tables underscores the significance of incorporating index compression. 
  
% \subsection{Performance Metrics Evaluation}
% When comparing the performance of JPQ in the HoVer experiments (as shown in \autoref{tab:hover_performance_metrics}) as well as the performance of the WiCE experiments (presented in \autoref{tab:wice_performance_metrics}), a notable trend emerges. The index compression brought by JPQ generally yields higher scores compared to the standard Dense retrieval experiments. This improvement is particularly striking as the gap between the JPQ experiments and the baseline performances is further narrowed. In the HoVer experiments, this enhancement ranges from marginal increases of less than a point in Claim detection and Citation extraction to a significant 2-point boost in the Fusion data. Conversely, in the WiCE experiments, while Claim detection experiences a slight decline of almost 2 points, Citation extraction and Fusion demonstrate the opposite trend.
% Typically, one might expect index compression techniques to yield inferior results compared to the standard Dense retrieval setup due to the lossy nature of compressing embeddings. However, a straightforward explanation for this unexpected improvement lies in the utilization of different pre-trained models for generating the embeddings. In the standard Dense retrieval, we rely on the all-MiniLM-L6-v2 model, which maps sentences and paragraphs to a 384-dimensional dense vector space. In contrast, the JPQ model employed for index compression initially generates embeddings of size 768 and subsequently reduces the embedding size using PQ centroids to achieve smaller vector sizes. Furthermore, it's worth noting that JPQ learns the index for the query vectors, unlike the approach in standard Dense retrieval where the index is kept separate. The latter essentially operates in a zero-shot inference manner, as we do not fine-tune the encoders on specific datasets but instead store and retrieve the created embeddings directly in our FAISS setup.


% \subsection{Key Takeaways} 
% Enhancing Dense retrieval through the use of index compression via the JPQ model has remarkably reduced the index size for Dense retrieval by a substantial 93\%. Further analysis indicates significant speedups of up to 10.0x for the CPU-based approach, while the GPU-based approach achieves a modest speedup of up to 2.0x in the HoVer experiments. However, it experiences a slight slowdown in the WiCE experiments. A huge emphasis on achieving efficiency is particularly pertinent in the context of CPU-based Dense Retrieval with index compression. Here the latency times of the CPU-based approach come in close to the GPU-based approach. These findings not only signify efficiency gains concerning resource utilization for index storage, but also pave the way for experiments on lower-end machines especially ones lacking GPU capabilities. Thereby maximizing the benefits of CPU-based methodologies. Regarding performance, experiments involving index compression generally outperform standard Dense retrieval. This superiority can be attributed to the utilization of different pre-trained models and learned index techniques, resulting in slightly enhanced outcomes.
