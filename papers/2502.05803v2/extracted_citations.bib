@ARTICLE{2023arXiv230511747L,
       author = {{Li}, Junyi and {Cheng}, Xiaoxue and {Zhao}, Wayne Xin and {Nie}, Jian-Yun and {Wen}, Ji-Rong},
        title = "{HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2023,
        month = may,
          eid = {arXiv:2305.11747},
        pages = {arXiv:2305.11747},
          doi = {10.48550/arXiv.2305.11747},
archivePrefix = {arXiv},
       eprint = {2305.11747},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230511747L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{Guo_2022,
   title={Semantic Models for the First-Stage Retrieval: A Comprehensive Review},
   volume={40},
   ISSN={1558-2868},
   url={http://dx.doi.org/10.1145/3486250},
   DOI={10.1145/3486250},
   number={4},
   journal={ACM Transactions on Information Systems},
   publisher={Association for Computing Machinery (ACM)},
   author={Guo, Jiafeng and Cai, Yinqiong and Fan, Yixing and Sun, Fei and Zhang, Ruqing and Cheng, Xueqi},
   year={2022},
   month=mar, pages={1–42} }

@Article{Lazarski2021nlpfact,
AUTHOR = {Lazarski, Eric and Al-Khassaweneh, Mahmood and Howard, Cynthia},
TITLE = {Using NLP for Fact Checking: A Survey},
JOURNAL = {Designs},
VOLUME = {5},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {42},
URL = {https://www.mdpi.com/2411-9660/5/3/42},
ISSN = {2411-9660},
DOI = {10.3390/designs5030042}
}

@inproceedings{ahmad2019,
    title = {ReQA: An Evaluation for End-to-End Answer Retrieval Models},
    author = {Ahmad, Amin and Constant, Noah and Yang, Yinfei and Cer, Daniel},
    booktitle = {Proceedings of the 2nd Workshop on Machine Reading for Question Answering},
    month = {nov},
    year = 2019,
    address = {Hong Kong, China},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/D19-5819},
    doi = {10.18653/v1/D19-5819},
    pages = {137--146},
}

@misc{alon2022neurosymbolic,
      title={Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval}, 
      author={Uri Alon and Frank F. Xu and Junxian He and Sudipta Sengupta and Dan Roth and Graham Neubig},
      year={2022},
      eprint={2201.12431},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{azaria2023internal,
      title={The Internal State of an LLM Knows When its Lying}, 
      author={Amos Azaria and Tom Mitchell},
      year={2023},
      eprint={2304.13734},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{baranchuk2018revisiting,
      title={Revisiting the Inverted Indices for Billion-Scale Approximate Nearest Neighbors}, 
      author={Dmitry Baranchuk and Artem Babenko and Yury Malkov},
      year={2018},
      eprint={1802.02422},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{bondarenko2021understanding,
      title={Understanding and Overcoming the Challenges of Efficient Transformer Quantization}, 
      author={Yelysei Bondarenko and Markus Nagel and Tijmen Blankevoort},
      year={2021},
      eprint={2109.12948},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{chen2023purr,
      title={PURR: Efficiently Editing Language Model Hallucinations by Denoising Language Model Corruptions}, 
      author={Anthony Chen and Panupong Pasupat and Sameer Singh and Hongrae Lee and Kelvin Guu},
      year={2023},
      eprint={2305.14908},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{gao2023rarr,
      title={RARR: Researching and Revising What Language Models Say, Using Language Models}, 
      author={Luyu Gao and Zhuyun Dai and Panupong Pasupat and Anthony Chen and Arun Tejasvi Chaganty and Yicheng Fan and Vincent Y. Zhao and Ni Lao and Hongrae Lee and Da-Cheng Juan and Kelvin Guu},
      year={2023},
      eprint={2210.08726},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@ARTICLE{ge2014opq,
  author={Ge, Tiezheng and He, Kaiming and Ke, Qifa and Sun, Jian},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Optimized Product Quantization}, 
  year={2014},
  volume={36},
  number={4},
  pages={744-755},
  keywords={Quantization (signal);Vectors;Artificial neural networks;Optimization;Encoding;Indexing;Linear programming;Vector quantization;nearest neighbor search;image retrieval;compact encoding;inverted indexing},
  doi={10.1109/TPAMI.2013.240}
}

@article{guo2022survey,
    title = "A Survey on Automated Fact-Checking",
    author = "Guo, Zhijiang  and
      Schlichtkrull, Michael  and
      Vlachos, Andreas",
    editor = "Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "10",
    year = "2022",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2022.tacl-1.11",
    pages = "178--206"
}

@misc{guu2020realm,
      title={REALM: Retrieval-Augmented Language Model Pre-Training}, 
      author={Kelvin Guu and Kenton Lee and Zora Tung and Panupong Pasupat and Ming-Wei Chang},
      year={2020},
      eprint={2002.08909},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{han2023comprehensive,
      title={A Comprehensive Survey on Vector Database: Storage and Retrieval Technique, Challenge}, 
      author={Yikun Han and Chunjiang Liu and Pengfei Wang},
      year={2023},
      eprint={2310.11703},
      archivePrefix={arXiv},
      primaryClass={cs.DB}
}

@ARTICLE{jegou2011pq,
  author={Jégou, Herve and Douze, Matthijs and Schmid, Cordelia},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Product Quantization for Nearest Neighbor Search}, 
  year={2011},
  volume={33},
  number={1},
  pages={117-128},
  keywords={Quantization;Nearest neighbor searches;Indexing;Neural networks;Euclidean distance;File systems;Scalability;Image databases;Permission;Electronic mail;High-dimensional indexing;image indexing;very large databases;approximate search.},
  doi={10.1109/TPAMI.2010.57}}

@article{ji2023,
author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
title = {Survey of Hallucination in Natural Language Generation},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {12},
issn = {0360-0300},
url = {https://doi.org/10.1145/3571730},
doi = {10.1145/3571730},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {248},
numpages = {38},
keywords = {consistency in NLG, faithfulness in NLG, intrinsic hallucination, factuality in NLG, extrinsic hallucination, Hallucination}
}

@misc{karpukhin2020dense,
      title={Dense Passage Retrieval for Open-Domain Question Answering}, 
      author={Vladimir Karpukhin and Barlas Oğuz and Sewon Min and Patrick Lewis and Ledell Wu and Sergey Edunov and Danqi Chen and Wen-tau Yih},
      year={2020},
      eprint={2004.04906},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{knowledge_base_1,
    title = "Language Models as Knowledge Bases: On Entity Representations, Storage Capacity, and Paraphrased Queries",
    author = "Heinzerling, Benjamin  and
      Inui, Kentaro",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.153",
    doi = "10.18653/v1/2021.eacl-main.153",
    pages = "1772--1791",
    abstract = "Pretrained language models have been suggested as a possible alternative or complement to structured knowledge bases. However, this emerging LM-as-KB paradigm has so far only been considered in a very limited setting, which only allows handling 21k entities whose name is found in common LM vocabularies. Furthermore, a major benefit of this paradigm, i.e., querying the KB using natural language paraphrases, is underexplored. Here we formulate two basic requirements for treating LMs as KBs: (i) the ability to store a large number facts involving a large number of entities and (ii) the ability to query stored facts. We explore three entity representations that allow LMs to handle millions of entities and present a detailed case study on paraphrased querying of facts stored in LMs, thereby providing a proof-of-concept that language models can indeed serve as knowledge bases.",
}

@misc{knowledge_base_2,
      title={A Review on Language Models as Knowledge Bases}, 
      author={Badr AlKhamissi and Millicent Li and Asli Celikyilmaz and Mona Diab and Marjan Ghazvininejad},
      year={2022},
      eprint={2204.06031},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{li2023unlocking,
      title={Unlocking Temporal Question Answering for Large Language Models Using Code Execution}, 
      author={Xingxuan Li and Liying Cheng and Qingyu Tan and Hwee Tou Ng and Shafiq Joty and Lidong Bing},
      year={2023},
      eprint={2305.15014},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{min2023factscore,
      title={FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation}, 
      author={Sewon Min and Kalpesh Krishna and Xinxi Lyu and Mike Lewis and Wen-tau Yih and Pang Wei Koh and Mohit Iyyer and Luke Zettlemoyer and Hannaneh Hajishirzi},
      year={2023},
      eprint={2305.14251},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@misc{peng2023check,
      title={Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback}, 
      author={Baolin Peng and Michel Galley and Pengcheng He and Hao Cheng and Yujia Xie and Yu Hu and Qiuyuan Huang and Lars Liden and Zhou Yu and Weizhu Chen and Jianfeng Gao},
      eprint={2302.12813},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{programfc,
    title = "Fact-Checking Complex Claims with Program-Guided Reasoning",
    author = "Pan, Liangming  and
      Wu, Xiaobao  and
      Lu, Xinyuan  and
      Luu, Anh Tuan  and
      Wang, William Yang  and
      Kan, Min-Yen  and
      Nakov, Preslav",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.386",
    doi = "10.18653/v1/2023.acl-long.386",
    pages = "6981--7004",
    abstract = "Fact-checking real-world claims often requires collecting multiple pieces of evidence and applying complex multi-step reasoning. In this paper, we present Program-Guided Fact-Checking (ProgramFC), a novel fact-checking model that decomposes complex claims into simpler sub-tasks that can be solved using a shared library of specialized functions. We first leverage the in-context learning ability of large language models to generate reasoning programs to guide the verification process. Afterward, we execute the program by delegating each sub-task to the corresponding sub-task handler. This process makes our model both explanatory and data-efficient, providing clear explanations of its reasoning process and requiring minimal training data. We evaluate ProgramFC on two challenging fact-checking datasets and show that it outperforms seven fact-checking baselines across different settings of evidence availability, with explicit output programs that benefit human debugging. Our codes and data are publicly available at \url{https://github.com/mbzuai-nlp/ProgramFC}.",
}

@inproceedings{questgen,
author = {Setty, Ritvik and Setty, Vinay},
title = {QuestGen: Effectiveness of Question Generation Methods for Fact-Checking Applications},
year = {2024},
isbn = {9798400704369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627673.3679985},
doi = {10.1145/3627673.3679985},
abstract = {Verifying fact-checking claims poses a significant challenge, even for humans. Recent approaches have demonstrated that decomposing claims into relevant questions to gather evidence enhances the efficiency of the fact-checking process. In this paper, we provide empirical evidence showing that this question decomposition can be effectively automated. We demonstrate that smaller generative models, fine-tuned for the question generation task using data augmentation from various datasets, outperform large language models by up to 8. Surprisingly, in some cases, the evidence retrieved using machine-generated questions proves to be significantly more effective for fact-checking than that obtained from human-written questions. We also perform manual evaluation of the decomposed questions to assess the quality of the questions generated.},
booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
pages = {4036–4040},
numpages = {5},
keywords = {claim decomposition, fact-checking, question generation},
location = {Boise, ID, USA},
series = {CIKM '24}
}

@misc{rag,
      title={Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}, 
      author={Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
      year={2021},
      eprint={2005.11401},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{ram2023incontext,
      title={In-Context Retrieval-Augmented Language Models}, 
      author={Ori Ram and Yoav Levine and Itay Dalmedigos and Dor Muhlgay and Amnon Shashua and Kevin Leyton-Brown and Yoav Shoham},
      year={2023},
      eprint={2302.00083},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{roit2023factually,
      title={Factually Consistent Summarization via Reinforcement Learning with Textual Entailment Feedback}, 
      author={Paul Roit and Johan Ferret and Lior Shani and Roee Aharoni and Geoffrey Cideron and Robert Dadashi and Matthieu Geist and Sertan Girgin and Léonard Hussenot and Orgad Keller and Nikola Momchev and Sabela Ramos and Piotr Stanczyk and Nino Vieillard and Olivier Bachem and Gal Elidan and Avinatan Hassidim and Olivier Pietquin and Idan Szpektor},
      year={2023},
      eprint={2306.00186},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{tas_b,
author = {Hofst\"{a}tter, Sebastian and Lin, Sheng-Chieh and Yang, Jheng-Hong and Lin, Jimmy and Hanbury, Allan},
title = {Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462891},
doi = {10.1145/3404835.3462891}

@inproceedings{temporal_aspect,
    title = "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models",
    author = "Tan, Qingyu  and
      Ng, Hwee Tou  and
      Bing, Lidong",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.828",
    doi = "10.18653/v1/2023.acl-long.828",
    pages = "14820--14835",
    abstract = "Reasoning about time is of fundamental importance. Many facts are time-dependent. For example, athletes change teams from time to time, and different government officials are elected periodically. Previous time-dependent question answering (QA) datasets tend to be biased in either their coverage of time spans or question types. In this paper, we introduce a comprehensive probing dataset TempReason to evaluate the temporal reasoning capability of large language models. Our dataset includes questions of three temporal reasoning levels. In addition, we also propose a novel learning framework to improve the temporal reasoning capability of large language models, based on temporal span extraction and time-sensitive reinforcement learning. We conducted experiments in closed book QA, open book QA, and reasoning QA settings and demonstrated the effectiveness of our approach.",
}

@misc{thorne2018automated,
      title={Automated Fact Checking: Task formulations, methods and future directions}, 
      author={James Thorne and Andreas Vlachos},
      year={2018},
      eprint={1806.07687},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{thorne2018fever,
      title={FEVER: a large-scale dataset for Fact Extraction and VERification}, 
      author={James Thorne and Andreas Vlachos and Christos Christodoulopoulos and Arpit Mittal},
      year={2018},
      eprint={1803.05355},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{touvron2023llama,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{umapathi2023medhalt,
      title={Med-HALT: Medical Domain Hallucination Test for Large Language Models}, 
      author={Logesh Kumar Umapathi and Ankit Pal and Malaikannan Sankarasubbu},
      year={2023},
      eprint={2307.15343},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{v2024quantemprealworldopendomainbenchmark,
      title={QuanTemp: A real-world open-domain benchmark for fact-checking numerical claims}, 
      author={V Venktesh and Abhijit Anand and Avishek Anand and Vinay Setty},
      year={2024},
      eprint={2403.17169},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.17169}, 
}

@inproceedings{wei2022,
  author={Wei, Zhimin and Xu, Xiaowei and Wang, Chenglin and Liu, Zhenyu and Xin, Peng and Zhang, Wei},
  booktitle={2022 7th International Conference on Image, Vision and Computing (ICIVC)}, 
  title={An Index Construction and Similarity Retrieval Method Based on Sentence-Bert}, 
  year={2022},
  volume={},
  number={},
  pages={934-938},
  doi={10.1109/ICIVC55077.2022.9886134}}

@misc{zhan2021jointly,
      title={Jointly Optimizing Query Encoder and Product Quantization to Improve Retrieval Performance}, 
      author={Jingtao Zhan and Jiaxin Mao and Yiqun Liu and Jiafeng Guo and Min Zhang and Shaoping Ma},
      year={2021},
      eprint={2108.00644},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@inproceedings{zhang2022retgen,
      title={RetGen: A Joint framework for Retrieval and Grounded Text Generation Modeling}, 
      author={Yizhe Zhang and Siqi Sun and Xiang Gao and Yuwei Fang and Chris Brockett and Michel Galley and Jianfeng Gao and Bill Dolan},
      year={2022},
      eprint={2105.06597},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhang2023sirens,
      title={Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models}, 
      author={Yue Zhang and Yafu Li and Leyang Cui and Deng Cai and Lemao Liu and Tingchen Fu and Xinting Huang and Enbo Zhao and Yu Zhang and Yulong Chen and Longyue Wang and Anh Tuan Luu and Wei Bi and Freda Shi and Shuming Shi},
      year={2023},
      eprint={2309.01219},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhao2022,
      title={Dense Text Retrieval based on Pretrained Language Models: A Survey}, 
      author={Wayne Xin Zhao and Jing Liu and Ruiyang Ren and Ji-Rong Wen},
      year={2022},
      eprint={2211.14876},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@article{zhao2023ann,
    author = {Zhao, Xi and Tian, Yao and Huang, Kai and Zheng, Bolong and Zhou, Xiaofang},
    title = {Towards Efficient Index Construction and Approximate Nearest Neighbor Search in High-Dimensional Spaces},
    year = {2023},
    issue_date = {April 2023},
    publisher = {VLDB Endowment},
    volume = {16},
    number = {8},
    issn = {2150-8097},
    url = {https://doi.org/10.14778/3594512.3594527},
    doi = {10.14778/3594512.3594527},
    journal = {Proc. VLDB Endow.},
    month = {jun},
    pages = {1979–1991},
    numpages = {13}
}

@misc{zhu2023survey,
      title={A Survey on Model Compression for Large Language Models}, 
      author={Xunyu Zhu and Jian Li and Yong Liu and Can Ma and Weiping Wang},
      year={2023},
      eprint={2308.07633},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

