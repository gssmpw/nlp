\section{Introduction (2 pages)}
\label{sec:intro}
\input{tables/table-intro-anecdotal}


In Information Retrieval (IR), the discrepancy between user queries and documents, termed ``vocabulary mismatch'' is a central problem. 
User queries, often in keyword form, might be ambiguous and unclear, making it challenging to discern user intentions, which in turn influences retrieval accuracy. 
For instance, the query \texttt{define sri} could refer to ``sanskrit word sri'' or ``\textbf{S}ocially \textbf{R}esponsible \textbf{I}nvestment''.
Consequently, to disambiguate the users' information need, classical approaches like pseudo-relevance feedback mechanism \cite{lv2009comparative,ir_query_exp}, KNN-search \cite{grbovic2015context,zamani:2017:relevance-based-word-embedding,he2016learning} expand the original query with keywords from top-ranked results to the original query. 
However, all the above approaches are term-based approaches that improve retrieval but not document ranking and lack interpretability.  \note{Avishek}{Previous sentence is confusing.}
Going beyond keywords, researchers have explored the possibility of natural language generation for clarifying queries~\cite{rao2018learning,zerveas2020brown,doc2query}.
Recently, LLMs have shown of large language models or LLMs~\cite{gpt3_prompting,gpt3incontext}, recent approaches have started to investigate the utility of LLMs to aid in enriching query and document representations~\cite{wang2023query2doc,gospodinov2023doc2query}. 
However, there are two major limitations to using LLMs for the task of query re-writing -- misaligned intent ?? and high inference costs and latency.


\mpara{The problem of misaligned intents.}
When using LLM-based query expansions, the expanded query results in choosing one aspect among the many possible aspects as query might have.
This entails in a problem that the chosen intent by the expansion might not align with the ground-truth intent due to topic drift or the issue of hallucination when generating long context for a given query.
Specifically, such intent non-alignment results in the case of forced match of an erroneously expanded query and a document. For instance, in Table~\ref{tab:intro_anecdotal}, we observe that for the query \texttt{define sri}, \qd{} drifts away from the intent and generates a query related to \textit{Stanford Research Institute }instead of denoting the Sanskrit word.  

\mpara{The problem of efficient inference.}
Secondly, a serious limitation (as acknowledged  in~\cite{wang2023query2doc}) is the computational overhead from both the retrieval and re-ranking phases.
The query expansions use the expensive tokenwise auto-regressive decoding during inference. Therefore, to stay within sub-second ranking requirements without using prohibitively expensive compute the choice of a LLM during query processing should be avoided.


\mpara{Context Aware Generative Query Enrichment.}
We make two simple yet important design decisions to overcome the problem of misalignment and efficiency.
Firstly, we generate query rewrites using LLMs by additionally providing the relevant document as context during training. 
Secondly, and unlike existing works, we fully avoid any query rewriting using LLMs \textbf{during inference}. 

Methodologically, we propose an LLM-based context-aware query enrichment framework \car{}, that replaces the traditional query rewriter with a LLM for rewriting ambiguous queries. 
At \textbf{inference time}, the ranker fine-tuned on rewritten queries, is equipped with knowledge of query disambiguation mechanism and improves the document ranking performance for subsequent ambiguous queries without the rewriter component. The proposed framework can be used with any off-the-shelf ranker. 
From Table \ref{tab:intro_anecdotal} we observe that the proposed context-aware query reformulation approach \car{}, can generate concise rewrites that reflect the intent when compared to \qd{} and other approaches. 


\mpara{The need for identification of ambiguous queries}: Our primary goal is to ameliorate the document ranking performance for ambiguous queries without compromising performance on other queries. The ranker fine-tuned only on rewritten ambiguous queries specializes in improving the document ranking performance for similar ambiguous queries. While the ranker fine-tuned on all queries (\textit{base ranker}) can be employed for well-formed queries. Hence, automated identification of ambiguous queries aid in routing them to the \textit{specialized ranker} and rest of the queries to the base ranker. Inspired by BERT-QPP \cite{bert_qpp}, we propose to use a post-retrieval neural Query Performance Predictor (QPP) fine-tuned to predict performance of a query based on initial document set retrieved using sparse methods like BM25. During inference, the QPP model is used to categorize a query as ambiguous based on the performance score predicted and is used to weigh the relevance scores from both ranking models.

We perform extensive experiments using LLMs of different parameter scales and demonstrate that the proposed approach results in better ranking performance. 
Specifically, we find that fine-tuning a ranker using re-written queries offers a significant improvement of up to \textbf{33\%} on the passage ranking task and up to \textbf{23\%} on the document ranking task when compared to the baseline performance of using original queries.


\noindent In summary, here is a list of our \textbf{contributions}: 
\begin{enumerate}
    \item We propose the use of existing techniques aimed at efficiently identifying relevant text spans from large corpora pertaining to the claims being fact-checked, enabling more targeted retrieval and verification processes.
    
    \item We show the methods for indexing supporting facts optimizes retrieval efficiency by reducing computational overhead and accelerating the retrieval process.
    
    \item We show a comprehensive evaluation of various retrieval approaches, mainly showing the difference of effectiveness and efficiency in indexing supporting facts versus indexing the entire data corpus. 
\end{enumerate}