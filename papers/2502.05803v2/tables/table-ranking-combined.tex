\begin{table*}
    \centering
    \begin{tabular}{p{.13\textwidth}p{.13\textwidth}p{.12\textwidth}p{.12\textwidth}p{.12\textwidth}|lp{.12\textwidth}p{.12\textwidth}p{.12\textwidth}p{.12\textwidth}}
    %\begin{tabular}{lcccc|lcccc}
        \toprule
            & \multicolumn{4}{c}{\textbf{\trecdl{} Passage}} && \multicolumn{4}{c}{\textbf{\trecdl{} Document}}\\
             \midrule
            & \multicolumn{2}{c}{\textbf{\trecdl{}-19}} & \multicolumn{2}{c}{\textbf{\trecdl{}-20}} && \multicolumn{2}{c}{\textbf{\trecdl{}-19}} & \multicolumn{2}{c}{\textbf{\trecdl{}-20}}\\
            \cmidrule(lr){2-3}
            \cmidrule(lr){4-5}
            \cmidrule(lr){7-8}
            \cmidrule(lr){9-10}
            \textit{Ranking Models}
            &  $\text{RR}$ & $\text{nDCG}_\text{10}$ &  $\text{RR}$ & $\text{nDCG}_\text{10}$ &&  $\text{RR}$ & $\text{nDCG}_\text{10}$ &  $\text{RR}$ & $\text{nDCG}_\text{10}$ \\
            \midrule

%% RR and nDCG@10 scores
\multicolumn{5}{l}{\bf Baseline} \\
\bert{} Baseline &  0.653 & 0.381 &  0.523 & 0.288 && 	0.750 &	0.453 & 	0.735 &	0.393 \\	
\qd{}~\cite{wang2023query2doc} &  0.578\down{11.6}$^{\#}$ & 0.321\down{15.7} &  0.608\up{16.2} & 0.323\up{12.3} && 	\textbf{0.82\up{9.7}} &	0.467\up{3.1}$^{*}$ & 	0.749\up{2} &	0.415\up{5.6}$^{*}$ \\
\midrule
\multicolumn{5}{l}{\bf Query expansion} \\
\bart{} (topic)     &  0.500\down{23.5}$^{*}$ & 0.254\down{33.3}$^{*}$ &  0.380\down{27.3}$^{*}$ & 0.202\down{29.9}$^{*}$ && - & - & - & - \\
\ada{} &  0.634\down{2.9} & 0.370\down{2.9} &  0.479\down{8.4} & 0.278\down{3.6} && - & - & - & - \\
\babbage{} &  0.697\up{6.7} & 0.370\down{3.0} &  0.580\up{10.9} & 0.327\up{13.5} && - & - & - & - \\
\curie{} &  0.563\down{13.9} & 0.328\down{13.9} &  0.498\down{4.8} & 0.248\down{13.8} && - & - & - & - \\
\davinci{} &  0.795\up{21.7}$^{*}$ & 0.391\up{2.6} &  0.507\down{3.1} & 0.284\down{1.3} && - & - & - & - \\
 \vinci{} (prompt1) &  0.714\up{9.4} & 0.386\up{1.3} &  0.493\down{5.7} & 0.289\up{0.4} && - & - & - & - \\
\vinci{} (prompt2) &  0.717\up{9.9} & 0.397\up{4.1} &  0.574\up{9.7} & 0.338\up{17.3} && - & - & - & - \\
\vinci{}  &  0.766\up{17.3}$^{\#}$ & 0.411\up{7.9} &  0.552\up{5.6} & 0.308\up{7.1} && 	\underline{0.818\up{9.1}} &	0.490\up{8.2}$^{\#}$ & 	0.764\up{4.0} &	0.465\up{18.5}$^{\#}$ \\
\chatgpt{} &  0.689\up{5.5} & 0.396\up{4.0} &  \underline{0.623\up{19.2}}$^{\#}$ & 0.348\up{20.7}$^{\#}$ && - & - & - & - \\	
\midrule
\midrule
\multicolumn{5}{l}{\bf \car{}} \\
\bart{} (topic+PAA) &  0.745\up{14.2} & 0.391\up{2.6} &  0.614\up{17.4}$^{*}$ & \underline{0.363\up{26.0}} && 	0.762\up{1.6} &	0.504\up{11.3}$^{\#}$ & 	\underline{0.819\up{11.5}} &	\underline{0.485\up{23.4}}$^{\#}$ \\
\vinci{} & \textbf{0.798\up{22.2}}$^{*}$ & \underline{0.417\up{9.5}} &  0.533\up{2.0} & 0.316\up{9.9} &&	0.774\up{3.3} &	0.507\up{12.0}$^{*}$ & 	0.777\up{5.8} &	0.447\up{13.7}$^{*}$ \\
\chatgpt{} &  \underline{0.794\up{21.6}}$^{*}$ & \textbf{0.494\up{29.7}}$^{*}$ &  \textbf{0.653\up{24.8}}$^{*}$ & \textbf{0.383\up{33.1}}$^{*}$ && 	0.769\up{2.6} &	\underline{0.509\up{12.3}}$^{\#}$ & 	0.7826\up{6.5} &	0.430\up{9.4}$^{*}$ \\
\vinci{} (\attention{}) & - & - & - & - && 0.81\up{8.1} & \textbf{0.517\up{14.1}}$^{*}$ & 0.785\up{6.9} & 0.459\up{17}$^{\#}$ \\
\vinci{} (\linear{}) & - & - & - & - && 0.782\up{4.3} & 0.497\up{9.7}$^{*}$ & \textbf{0.848\up{15.5}} & \textbf{0.504\up{28.4}}$^{*}$ \\
\chatgpt{} (\attention{}) & - & - & - & - && 0.759\up{1.3} &	0.465\up{2.6} & 0.773\up{5.3} &	0.417\up{6.2}$^{*}$ \\
\chatgpt{} (\linear{}) & - & - & - & - && 0.778\up{3.8}	& \textbf{0.517\up{14.1}}$^{*}$ & 0.799\up{8.8} &	0.461\up{17.5}$^{*}$ \\


        \bottomrule        
    \end{tabular}

    
    \caption{Ranking performance of \bert{} model trained using query expansions from different query generative models on \trecdl{} '19 and \trecdl{} '20. We show the relative improvement of our approaches against a baseline (fine-tuned on original queries without query expansion) in parentheses. Statistically significant improvements at a level of $95\%$ and $90\%$ are indicated by $*$ and $\#$ respectively~\cite{paired_significance_test}. The best results for each dataset and each model is in \textbf{bold} and second is \underline{underlined}. \todo {Remove RR values to fit or have different tables(already present in ictir project) for passage and document collection.}}
    \vspace{-5mm}
    \label{tab:reranking-msmarco}
\end{table*}
