\begin{table}[hbt!]
\centering
\begin{tabular}{llll|r}
\hline
\multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c|}{\textbf{BERTScore}} & \multirow{2}{*}{\textbf{Rouge-L}} \\
\cmidrule(lr){2-4}
  & \textbf{P}     & \textbf{R}     & \textbf{F1}    &  \\
\hline

GPT2 (topic){$^\dagger$}                   & 0.457          & 0.447          & 0.451          & 7.47            \\
\bart{} (topic){$^\dagger$}        & 0.792          & 0.791          & 0.792          & 28.89           \\
\qd{}  & 0.672 & 0.749 & 0.708 & 6.76 \\
\ada{}  (in-context)         & 0.734          & 0.776          & 0.754            & 17.43            \\
\babbage{} (in-context)           & 0.804          & 0.802          & 0.802          & 32.16            \\
\curie{} (in-context)          & 0.812          & 0.800          & 0.805          & 34.25            \\
\davinci{} (in-context)           & 0.818         & 0.823          & 0.820        & 33.81            \\
\vinci{} (prompt1)            & 0.727          & 0.743          & 0.734          & 14.38          \\
\vinci{} (prompt2)            & 0.811          & 0.792          & 0.801          & 32.07           \\
\vinci{} (in-context)         & 0.817 & 0.818 & 0.817 & 34.68  \\
\chatgpt{}           & 0.768          & 0.812          & 0.789          & 25.77           \\
\hline
\multicolumn{5}{l}{\bf  \car{}} \\
\bart{} (topic+PAA){$^\dagger$}  & 0.777          & 0.783          & 0.779          & 25.85           \\
\vinci{} &\underline{0.821} & \underline{0.834} & \underline{0.827} & \underline{38.57}  \\
\chatgpt{} & \textbf{0.825} &  \textbf{0.847} &  \textbf{0.836} & \textbf{40.51}\\ \hline
\end{tabular}
\caption{BERTScore and ROUGEL scores for different rewrites on TREC web 2012. The best results for each dataset and each model is in \textbf{bold} and second is \underline{underlined}. {$^\dagger$}indicates that the model has been fine-tuned with topic or topic + PAA data.}
\label{tab:rewriter}
\end{table}