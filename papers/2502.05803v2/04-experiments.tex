\section{Experimental Setup}
\label{experiments}


\subsection{Datasets}

\noindent \textbf{HoVer}\cite{jiang2020hover} consists of 26k claims requiring evidence from up to four English Wikipedia articles (2017 data dump) and was developed in multiple stages with the help of trained crowd-workers. The dataset is split into 18,171 labeled training examples (11,023 Supported and 7,148 Not-Supported), 4,000 labeled development examples (2,000 Supported and 2,000 Not-Supported), and 4,000 unlabeled test examples. 

\noindent \textbf{WiCE}\cite{kamoi2023wice} focuses on claims from Wikipedia articles. It uses the same base claims as SIDE \cite{petroni2022improving} and breaks down long claims into simpler sub-claims, enhancing annotation and entailment prediction. For our experiments, we used the original WiCE claims instead of sub-claims to retain context. We further adapted WiCE's three-way entailment to a binary scheme by relabelling the partially supported claims as unsupported for aligning with the HoVer dataset. The dataset is split into 1,260 labeled training examples (460 Supported and 800 Not-Supported), 349 labeled development examples (115 Supported and 234 Not-Supported, and 358 unlabeled test examples.

%It should be noted that the unsupported claims are likely to be true but lack specific evidence in the document corpus.

\noindent \textbf{Wikipedia corpus}: For HoVer, we used the processed 2017 English Wikipedia dump from the HotPotQA website, containing 5,486,211 articles. For WiCE, we used the latest available English Wikipedia dump from January 1, 2024, which includes 6,777,401 articles. Both dumps were processed using the HotPotQA fork\footnote{https://github.com/qipeng/wikiextractor} of the Wikiextractor tool\footnote{https://github.com/attardi/wikiextractor} to format the data into a structured folder system containing Bzip2 files.

\noindent \textbf{2024 Presidential debate:} We employ the fact-checking pipeline for the task of live fact-checking of the 2024 US presidential debate. We identified 281 claims in the debate with corresponding labels. We employ the 2024 Wikipedia dump as knowledge source for fact-checking the claims.


% \subsection{Fact-checking Pipeline}
% We use and adapt the baseline pipeline, for fact extraction and claim verification to address the complex multi-hop challenge, that was introduced in the work of HoVer\cite{jiang2020hover}. The pipeline comprises four stages: (1) Term-based Document Retrieval, which uses cosine similarity on TF-IDF vectors to identify the most relevant Wikipedia documents; (2) Neural-based Document Retrieval, where a BERT-base model evaluates the relatedness between documents and claims, selecting the top relevant documents; (3) Neural-based Sentence Selection, utilizing another BERT-base model to score and select relevant sentences from the chosen documents; and (4) Claim Verification, where a BERT-base model fine-tuned for textual entailment classifies the claim based on the retrieved evidence. 


% \subsection{Overview Experiments} We consider three different 1) Sparse retrieval using BM25 from Elasticsearch as the baseline, 2) Dense retrieval with FAISS replacing Term-based retrieval stages, and 3) Index compression to further optimize Dense retrieval. Besides running the original data as baseline, each method is also evaluated across three fact-extraction data settings with the goal of bypassing the Sentence Selection stage. Namely: i) utilizing a pre-trained claim-detection model, ii) extracting citations, and a iii) fusion of both approaches to streamline fact-checking without the need for on-the-fly computation during inference. 
%Lastly, an ablation study has also been conducted comparing the settings between baseline configurations with and without Sentence Selection stages, aiming to quantify the impact of Dense Retrieval and re-ranking methods on overall pipeline performance and resource utilization.


\vspace{-1.2em}
\subsection{Experiment Setup}
\noindent \textbf{Computational resources} 
The experiments were conducted on a dedicated server running Arch Linux, equipped with a 16-core 2nd Gen AMD EPYC™ 7302 processor, 256 GB RAM and two NVIDIA GeForce RTX 3090 GPUs.
%Our code relies mostly on Python 3.10.9 to utilise the latest models and tools, except for parts of the legacy HoVer pipeline code which relies on Python 3.7.16 to maintain stability. 

\noindent \textbf{Tools, Models and Hyperparameters} The Wikipedia dumps are processed using WikiExtractor, JobLib (parallelization) and SpaCy's `en\_core\_web\_lg' model\footnote{https://spacy.io/models/en} for sentence splitting instead of StanfordCoreNLP due to performance issues (processing large volumes of text in parallel). For \textbf{sparse retrieval}, we utilize BM25 from Elasticsearch\footnote{https://elasticsearch-py.readthedocs.io/en/v8.12.1/}. For \textbf{dense retrieval}, we employ \textit{`all-MiniLM-L6-v2'} model from Sentence Transformers\footnote{https://www.sbert.net/docs/pretrained\_models.html} to encode the query and all the documents. We employ ANN search using a flat index structure over the document embeddings leveraging FAISS library\footnote{https://github.com/facebookresearch/faiss}.
 For index compression JPQ, configured with \textit{K=256} codewords and \textit{M=96} codebooks, utilizes Roberta-based dual-encoders with OPQ for linear transformation and PQ for compression. Training involved AdamW optimizer with batch size 32, LambdaRank for pair-wise loss, and optimization settings specific to query and PQ learning rates. For claim detection, a pre-trained BERT model\footnote{https://huggingface.co/Nithiwat/bert-base\_claimbuster} fine-tuned on the ClaimBuster dataset is used. Other pre-trained BERT-base uncased models are employed in the pipeline, fine-tuned with a batch size of 16, a learning rate of 3e-5, and varying epochs (3 for Sentence Selection, 5 for other stages).
 %Model parameters like kr, kp, κp, and κs are adjusted based on memory constraints and performance on development claim datasets (e.g., kr=20, kp=5, κp=0.5, and κs=0.3).


\noindent \textbf{Performance and Efficiency Evaluation:} To evaluate end-end task performance, we employ weighted F1 score. For retrieval latency, we'll measure CPU and GPU implementations of FAISS and CPU for BM25. Metrics such as index size, creation time, retrieval time, total runtime, dataset size, and disk writes will be monitored using tools like psutil\footnote{https://psutil.readthedocs.io/en/latest/} and nvidia-ml-py3\footnote{https://github.com/nicolargo/nvidia-ml-py3}. Inference latency will be measured in milliseconds.


% \section{Datasets} 
% Our experiments utilize two state-of-the-art comprehensive claim datasets for evaluating the fact-checking pipeline, namely  HoVer \cite{jiang2020hover} and WiCE \cite{kamoi2023wice}. These datasets pose challenges to textual entailment models in evidence extraction and fact verification due to their intricate multi-hop reasoning requirements. Besides the claim data, also required is the Wikipedia data dumps as a vast source of evidence data collection. The distribution of entailment labels across dataset splits for both HoVer and WiCE, as utilized in our experiments, is illustrated in \autoref{tab:claim_dataset_sizes}.

% \input{tables/claim_datasets}

% \subsection{HoVer dataset}
% The work by Jiang et al. \cite{jiang2020hover} will be used. The paper with regards to other datasets states that, while valuable for community engagement, single-hop datasets (e.g. FEVER) and current multi-hop question-answering datasets (e.g. HotPotQA) face limitations related to the number of reasoning steps and word overlap between questions and evidence. Therefore the paper introduces HOppy VERification (HoVer) consisting of 26k claims that require evidence from up to four English Wikipedia articles. Additionally, those claims contain significantly less semantic overlap between the claims and some supporting documents. This is to avoid reasoning shortcuts such as shallowly performing direct semantic matching with only the claims. The data corpus comprises only the first Wikipedia paragraph (the lead section). To stay in line, we will do the same and also add another setup of using the whole Wikipedia article text. \\

% The HoVer dataset is curated using the HotPotQA\footnote{https://hotpotqa.github.io/wiki-readme.html} dataset as a basis, which consists of multi-hop question-answer pairs derived from the 2017 English Wikipedia dump. The creation of the HoVer dataset unfolds in three stages, guided by the collaborative efforts of trained crowd-workers. 
% \begin{enumerate}
%     \item \textbf{Claim Creation:} In the initial stage, question-answer pairs sourced from HotPotQA are rewritten into claims. These claims incorporate information from two distinct English Wikipedia articles. To enhance complexity, extra hops are introduced by substituting an entity with information from another article that pertains to the original entity. To improve readability, many-hop claims are articulated across multiple sentences, interconnected through coreferences. To ensure the quality of the claims, a separate group of crowd workers validate the claims. Here only claims for which at least two out of three annotators agree on a valid statement and cover the same information from the original question-answer pair are kept. 
    
%     \item \textbf{Claim Mutation:} The second stage involves the generation of unsupported claims by modifying those produced in the first stage. This process includes automatic word/entity substitution, followed by human editing. Crowd-workers refine claims to make them either more specific or more general, and some claims are negated. A human validation process ensures the quality of machine-generated claims.
    
%     \item \textbf{Claim Labeling:} In the third stage, a binary classification system is adopted for claim labels: SUPPORTED and NOT-SUPPORTED. Recognizing the inherent ambiguity in distinguishing between REFUTED and NOTENOUGHINFO, particularly in many-hop claims, the label NOT-SUPPORTED is employed. This decision streamlines the categorization process and addresses challenges in determining refutation or insufficient information.
% \end{enumerate}

% \subsection{WiCE dataset}
% Besides HoVer, we also aim to use the WiCE dataset proposed in the work of Kamoi et al. \cite{kamoi2023wice}. WiCE, an acronym for \textit{Wikipedia Citation Entailment} is a dataset consisting of real-world claims from Wikipedia articles. The dataset aims to mitigate the challenges that arise for modern entailment systems. This is due to Natural Language Inference (NLI) datasets having a limited analysis scope due to too much focus on short premises, causing entailment systems to rely solely on local scores or retrieval methods. There are a few exceptions like DocNLI, which has synthetic negatives. However, this introduces a different challenge of spurious correlations and insufficient annotations of claim support due to a lack of ecologically valid negatives hampering the training. These gaps highlight the need for more robust methodologies and diverse training data for better inference accuracy. \\

% The WiCE dataset uses the same base claim data as SIDE \cite{petroni2022improving} and consists of sentences constructed from Wikipedia together with the corresponding articles it cites. To help in the annotation process, the authors introduce the Claim-Split method, which decomposes hypotheses into sub-claims using few-shot prompting with `GPT-3.5'. By breaking down long claims into short sub-claims, they manage to not only simplify the annotation process, but also simplify the entailment prediction task and enhance the classification performance. Additionally, this method also allows for a more fine-grained view of which parts of the claims are supported. For the annotation process itself, attained are the entailment label, the list of the article's sentences that support the claim sentence, and tokens in the claim that are unsupported. \\

% For our experiment evaluation, we conducted the pipeline analysis on the WiCE claim dataset rather than their sub-claim dataset. This decision was made because the sub-claims often lack the necessary surrounding context to be considered claims in their own right. This limitation becomes apparent when considering an example claim: \textit{"The couple married in New York in December 2012, and their son, Bear, was born the next year,"} which pertains to the actress Kate Winslet. However, the corresponding sub-claim: \textit{"The couple married in New York in December 2012."} is highly ambiguous and tends to yield non-relevant documents when used in isolation. Another decision made concerning the dataset is the change of labelling. WiCE utilizes a three-way entailment system, which we adapted into a binary labelling scheme. It's important to acknowledge that many of the unsupported claims in the dataset are likely true. However, they are labelled as unsupported due to the lack of specific evidence documents. Furthermore, the original partially supported label can be considered ambiguous, because it doesn't necessarily imply that all parts of the claim are verified by the retrieved evidence documents. Therefore, in line with the principles of the HoVer dataset, we adapted the instances labelled as `PARTIALLY-SUPPORTED' to `NOT-SUPPORTED'. This adjustment ensures consistency and clarity in our evaluation process.

% \begin{figure}[hbt!]
%   \centering
%   \includegraphics[width=\textwidth]{figures/wikipedia structure.png}
%   \caption{Layout of processed Wikipedia dump used for our experiments.}
%   \label{fig:wikipedia_structure}
% \end{figure}

% \subsection{Wikipedia corpus}
% As both of the datasets contain claims with supporting documents related to Wikipedia articles, we used those Wikipedia dumps as corpus collection to ensure relevant documents were retrieved. For HoVer, as mentioned before, we used the same processed 2017 English Wikipedia dump, retrieved from the HotPotQA website. This dump consists of in total of 5,486,211 Wikipedia articles which in total constitute a total of 94,914,378 sentences. Similarly, the WiCE dataset also uses an English Wikipedia where they re-retrieved relevant articles using Common Crawl. Instead of doing the same and to speed up the process, we used one of the latest available English Wikipedia dumps at the moment (namely 01-Jan-2024). This dump consists of in total of 6,777,401 Wikipedia articles which constitutes a total of 126,533,841 sentences. For processing the corpus to be of the same format as the HotPotQA Wikipedia dump, we utilised the HotPotQA fork\footnote{https://github.com/qipeng/wikiextractor} of the official Wikiextractor\footnote{https://github.com/attardi/wikiextractor} tool. This tool helps format the dump into a folder structure, as seen in \autoref{fig:wikipedia_structure}, containing multiple sub-folders holding one hundred Bzip2 files, where each zip file contains multiple Wikipedia articles.
 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Experimental design}
% It is crucial to establish a baseline for our experiments amidst the myriad of discussed approaches for improvements and setups. Additionally, we provide an overview of the various ablations and other studies conducted for our experiments, as to offer a more streamlined perspective.

% \begin{figure}[hbt!]
%   \centering
%   \includegraphics[width=\linewidth]{figures/pipeline architecture/hover architecture.png}
%   \caption{Example of HoVer's fact-extraction and -verification pipeline with the 4-stage architecture }
%   \label{fig:hover_architecture}
% \end{figure}

% \subsection{Fact-checking Pipeline} 
% The authors of HoVer have developed an initial baseline pipeline system for fact extraction and claim verification, following the state-of-the-art model by Nie et al. \cite{nie2018combining}  on FEVER \cite{thorne2018fever}, to highlight the complex multi-hop challenge. This pipeline\footnote{https://github.com/hover-nlp/hover} forms the core part of our work as we will be comparing the performance and efficiency of it, to the improvements we will introduce. The pipeline, depicted in \autoref{fig:hover_architecture} consists of the following stages:

% \begin{enumerate}
%     \item \textbf{Term-based Document Retrieval:} The system employs Chen et al.'s \cite{chen2017reading} document retrieval component, utilizing cosine similarity on binned uni-gram and bi-gram TF-IDF vectors. This process identifies and returns the k most relevant Wikipedia documents and their probabilities for a given query.

%     \item \textbf{Neural-based Document Retrieval:} The BERT-base model utilizes a single document p from the set Pr and a claim c as inputs. It produces a relatedness score reflecting the connection between p and c. The system selects a set Pn consisting of the top kp documents with relatedness scores surpassing a threshold kp.

%     \item \textbf{Neural-based Sentence Selection:} Another BERT-base model is fine-tuned to encode the claim c and all sentences from a chosen document p in the set Pn. It predicts sentence relatedness scores using the first token of each sentence. A set Sn is then selected, comprising the top sentences from Pn with relatedness scores exceeding a threshold ks.

%     \item \textbf{Claim Verification Model:} A BERT-base model is fine-tuned for textual entailment between the claim c and the retrieved evidence Sn. The model takes the claim and evidence, separated by a [SEP] token, as input and conducts binary classification based on the output representation of the [CLS] token at the first position.
% \end{enumerate}

% \subsection{Overview Experiments} 
% As the main baseline, we will take the original datasets before using any of the aforementioned adjustments (see \autoref{sec:improving efficiency}) and run the HoVer pipeline on it. After having established the baselines on both datasets, for our experiments we aim to compare the three types of improvements for efficiency, as explained in \autoref{cha:methodology}. To provide a streamlined view, we will iteratively go through them and show how each iteration improves the pipeline. 

% \paragraph{Retrieval Setups} We distinguish three types of retrieval methods in our experimental setup. The First consists of the Sparse retrieval which is part of the original baseline pipeline. For this, we will use the BM25 retrieval from Elasticsearch. For the second method, as mentioned in \autoref{ssec:dense_retrieval}, the Term-based retrieval stages (Sparse retrieval and Neural-based) in the HoVer pipeline can be replaced by a single Dense Retrieval setup. For our specific use case, we will implement FAISS as our chosen dense retrieval method, aiming to retrieve the top 5 documents similar to the original pipeline. Lastly, there is also the Index compression setup aims to further improve the Dense Retrieval setup. Unlike the previous two improvements, this setup does not replace any component in the HoVer pipeline. Instead, it introduces a new module. Here instead of storing text in its original form in the index, as in previous setups, we utilize a document encoder segment to compress the index into a PQ index, significantly reducing its size compared to the Dense Retrieval Setup. During retrieval, the query encoder encodes the claim with the learned index, which is then utilized for retrieval purposes in the Dense Retrieval process. The retrieval process is expected to exhibit a speedup owing to the smaller size of vector embeddings in the index to search through. Moreover, in terms of performance, it is anticipated that there will be only marginal differences compared to the standard Dense Retrieval setup.

% \paragraph{Re-ranking data} Within each retrieval method, we will execute the same three distinct fact-extraction data settings. These settings, as explained in \autoref{cha:methodology}, involve the utilization of a pre-trained claim-detection model, the extraction of citations, and a fusion of both approaches. The goal of these methods is to bypass the Sentence Selection stage. This means there's no need to train and employ a separate model for identifying claim-worthy sentences in each candidate document, as we pre-compute and store them in the index. This is to essentially eliminate the need for on-the-fly computation, resulting in the amount of resources and time needed to be reduced for the fact-checking pipeline. It's important to highlight that although we still employ a BERT model for claim detection, it's only utilized during the curation of the data corpus, rather than during the inference time of the pipeline. This shift enhances the efficiency of our pipeline since the claim detection step is executed less frequently, resulting in overall improved performance.

% \paragraph{Sentence Retrieval Ablation} Expanding upon our experiments involving the three re-ranking data settings, we will also aim to gauge the impact of Dense Retrieval on the original corpus data across two distinct setups through an ablation study. Firstly, we'll assess its influence in a manner akin to the baseline pipeline configuration, which encompassed the Sentence Selection stage. Secondly, we'll examine its effects in a setup resembling the re-ranking approach, but without the Sentence Selection stage. This comparative investigation will elucidate the degree to which Sentence Selection contributes to the overall pipeline performance.
% Furthermore, this provides us with a valuable means to compare and contrast the outcomes against those derived from fact-extracted data settings. It also lets us see how much performance degradation these settings experience compared to the original data.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \section{Experiment Setup}\label{sec:experiment_setup}
% To ensure the reproducibility of our work, we provide a comprehensive overview of both our hardware setup and the implementation details of our code. The latter part involves which models are used in our experiments and some design decisions. 

% \subsection{Hardware configuration} 
% The experiments were run on a dedicated private server equipped with high-performance hardware. This to ensure robust and efficient processing of our experiments could be carried out, without facing problems due to hardware limitations. The server, powered by Arch Linux, consists of a 16-core 2nd Gen AMD EPYC™ 7302 processor coupled with two NVIDIA GeForce RTX 3090 GPUs, providing ample computational resources for our tasks. Complementing this powerful hardware configuration was a substantial 256 GB of RAM, enabling smooth and uninterrupted operation even during intensive computational tasks. \\

% The HoVer pipeline boasts support for multi-GPU usage, yet we encountered some issues in that regard. Consequently, we executed the pipeline and all GPU-enabled models in a single GPU configuration to ensure seamless operation and consistency across the different experiments.

% \subsection{Implementation details}
% \paragraph{Text Processing} For the re-ranking setup, the corpus files needed to be processed before being able to use them in our experiments. As mentioned before the Wikipedia dumps that we used are processed through the WikiExtractor tool. To speed up the process, we used JobLib to multiprocess the files in parallel. For the re-ranking setup, the claim-detection model used is a pre-trained BERT model \footnote{https://huggingface.co/Nithiwat/bert-base\_claimbuster} that has been finetuned on the ClaimBuster dataset. The model has scored an 82\% accuracy score in correctly identifying claims and thus made for a proper model to use for our experiments. Overall processing the entire corpus for Claim detection takes around two to three days on our hardware setup. On the other hand processing for citation extraction takes around eight days to process. This is due to the need to retrieve the html webpages asynchronously as in a multi-processing setup we can miss some pages to be retrieved. Additionally, NLP models are used for sentence splitting. Using a single model instance would not work in multi-processing due to lack of multi-processing support and there not being any shared memory between the processes. This results in no retained text despite articles having citations, therefore the multi-threading option should be used instead. However in Python, as stated in their documentation\footnote{https://docs.python.org/3/library/threading.html}, the `Global Interpreter Lock' threading has been implemented as a concurrency instead of parallelism way. This has as a consequence that only one thread is to be run at a time instead of multiple ones concurrently. Even more in our case, we noticed it slowed the code even further compared to running it in a serial manner. Therefore to mitigate these issues we use the default multiprocessing and instantiate the NLP model in each process rather than share the model between the processes. Although this adds some overhead in latency and memory for each process, in the end, it significantly speeds up our text processing for citation extraction on the data collection. Finally, it's important to highlight that the initial setup of the HoVer pipeline utilized only the lead section of the Wikipedia corpus in its downloaded database file. However, since Wikipedia's lead sections don't include citations and serve as self-contained summaries of the entire articles, we decided to utilize the full article text for our experiments. Despite this adaptation, we've retained the option in our code to choose whether to use the full text or just the lead section.

% \paragraph{NLP models} For extracting supporting facts in our text processing pipeline, we leveraged several state-of-the-art models to facilitate various tasks. The HoVer pipeline, by default, incorporates StanfordCoreNLP\footnote{https://stanfordnlp.github.io/CoreNLP/} for sentence splitting. However, StanfordCoreNLP requires the Java Runtime Environment to be installed, which may not always be convenient for Python-centric workflows. Additionally, the setup of StanfordCoreNLP acts like a server. This works well in the pipeline as it works on smaller amounts of text. Unfortunately, for processing the entire Wikipedia corpus for extracting supporting facts, we noticed it suffered from disconnections as the response was not being received at the server endpoint. To address this, we have integrated instead the \textit{`en\_core\_web\_lg'}\footnote{https://spacy.io/models/en} model from the SpaCy library as an alternative. This choice offers several advantages, including SpaCy's renowned efficiency and speed, often surpassing that of StanfordCoreNLP. This ensures quicker processing times and optimized resource utilization, which is particularly beneficial for real-time applications. Moreover, Spacy enables the creation of custom NLP pipelines, allowing us to tailor the functionality to our specific needs. For instance, if only sentence splitting is required, we can configure Spacy to load only the Sentencizer component, thereby avoiding unnecessary overhead from other components. 

% \paragraph{Sparse Retrieval} The current implementation of the HoVer pipeline includes the utilization of retrieved article files from the Term-based Document Retrieval stage, although it lacks specific setup details. It does reference the use of the DrQA library\footnote{https://github.com/facebookresearch/DrQA} as an option for processing HoVer data, which could enable a more fair comparison. However, for our specific use case, we plan to replace the TF-IDF Document Retrieval step using BM25 from Elasticsearch\footnote{https://elasticsearch-py.readthedocs.io/en/v8.12.1/}. This decision is primarily motivated by the fact that our system already has an Elasticsearch instance set up, making integration with BM25 retrieval more straightforward compared to incorporating DrQA. Additionally, we anticipate that the Sparse Retrieval between the two methods should yield comparable results.

% \paragraph{Dense Retrieval} As mentioned before, for our Dense Retrieval we will employ the FAISS\footnote{https://github.com/facebookresearch/faiss} library with GPU support. Here we used the FAISS flat index data structure for a fast similarity search of dense vectors, using dot product against the query as a scoring function. This particular index data structure stores all vectors directly in a single structure, making it simple and efficient for exact searches. However, it may not scale as well to very large datasets compared to more complex index structures of the FAISS library (e.g. IVF). For generating text embeddings, we employed the pre-trained \textit{`all-MiniLM-L6-v2'}  model from Sentence Transformers\footnote{https://www.sbert.net/docs/pretrained\_models.html}, a versatile and effective solution for creating dense representations of textual data. This model has only been used for the Dense Retrieval setup without index compression, as the index compression used the JPQ\footnote{https://github.com/jingtaozhan/JPQ} model to encode the textual data. 

% \paragraph{Index Compression} As detailed in the JPQ paper \cite{zhan2021jointly}, variable \textit{K} (the number of codewords) is set to 256 to store embedding in one byte, and \textit{M} (number of codebooks/sub-vectors) can be set to any number of sub-vectors smaller than the embedding size. For our use case, we only ran it for M = 96, to see how much reduction can be achieved in index size, while still getting as high performance as possible from using a relatively high number of sub-vectors to represent the data. The dual-encoders are based on the Roberta model from Huggingface Transformers. The document encoder uses OPQ \cite{ge2014opq} to learn a linear transformation of embeddings and PQ \cite{jegou2011pq}  for compression. Furthermore, for training, the AdamW optimizer has been used with a batch size of 32, LambdaRank \cite{burges2010lambdarank} as a pair-wise loss function, and the top-200 irrelevant documents as hard negatives. For the query encoder, the learning rate is set to \textit{5 × 10-6} and the PQ learning rate is set to \textit{1 × 10-4} for our setting (depends on \textit{M}). Creating the index for the original Wikipedia dumps (without re-ranking applied) for our experiments took roughly between \textit{10-12} hours on our setup. Subsequently, the reduced corpus size through re-ranking took \textit{6-7} hours, roughly half the time. 

% \paragraph{Fact-checking pipeline} In our custom scripts, we predominantly utilize Python version 3.10.9 due to its compatibility with various existing tools and libraries essential for our experimental work, such as retrieval libraries and the wiki extractor. However, we made an exception for the HoVer codebase, which contains legacy code relying on older dependencies. For this particular codebase, we opted for Python version 3.7.16 to maintain compatibility and avoid disrupting existing functionality. Updating to a newer Python version would have necessitated not only updating dependencies, but also extensive re-implementation of core components, which is beyond the scope of our study. The models employed in different stages of the HoVer fact-checking pipeline are based on pre-trained BERT-base uncased models, with variations arising from their fine-tuning for specific pipeline stages. Fine-tuning involves using a batch size of 16 and a default learning rate of 3e-5 without warmup. The number of training epochs is set to 3 for the Sentence Selection stage and 5 for the remaining stages. For model parameters, kr = 20, kp = 5, κp = 0.5, and κs = 0.3 are used depending on the memory limitations and performance on the development claim dataset.


% \section{Assessment and Metrics}\label{sec:metrics}
% \paragraph{Performance Evaluation:} In alignment with the original HoVer methodology, we will utilize accuracy as the primary metric to assess the correct identification of claims as either \textit{`Supporting'} or \textit{`Not Supporting'}. However, to provide a comprehensive evaluation, we will supplement accuracy with additional metrics including \textit{F1 score, recall}, and \textit{precision}. Given the uneven distribution of entailment labels in the WiCE dataset, both macro and weighted metrics will be reported for a more nuanced understanding (\autoref{tab:claim_dataset_sizes}). Since the HoVer claim data is well-balanced and much larger, the various metrics do not vary by much compared to the WiCE claim data where accuracy is not particularly indicative of how well the experiments score. This as the accuracy can be quite misleading with incorrectly labelling every claim as `not supported' would already yield a 66\% accuracy. Therefore by using the combination of both \textit{weighted and macro} scores, these metrics allow us to take into account class imbalances of the WiCE data, while also ensuring equal considerations between the two labels.
% As end results, we select and report the model checkpoints with the best predictions. The selection process will prioritize sorting based on taking the five best F1-weighted scores, followed by selecting the one with the highest F1-macro score. This approach ensures the choice of the model checkpoint is less biased towards specific labels. For example, it may occur that a checkpoint scores highly in accuracy and f1-weighted, but perhaps very poorly in f1-macro. To mitigate the risk of selecting such a checkpoint, this selection approach minimizes the chances of under- and over-prediction for any particular class label, causing checkpoints to be selected that have an overall well-balanced score across the different metrics. 

% \paragraph{Efficiency Evaluation} The evaluation of retrieval latency will encompass both CPU and GPU implementations of FAISS (with and without Index Compression). As BM25 lacks GPU support, latency measurements will be limited to CPU performance. In addition to the above, the following metrics will be recorded for each retrieval setting: index size on disk, time taken for index creation, and time for document retrieval in both BM25 and FAISS setups. Furthermore, secondary measurements will include total runtime, corpus data size on disk, disk writes for constructing train and development sets at each stage, and disk writes for model checkpoints and predictions. These are measured using Python's in-built `psutil'\footnote{https://psutil.readthedocs.io/en/latest/} tool. Lastly, maximum CPU usage, maximum memory consumption, and GPU utility and memory usage will be monitored throughout the experiments using the `nvidia-ml-py3' library\footnote{https://github.com/nicolargo/nvidia-ml-py3}, which is a Python wrapper around the NVML library\footnote{https://developer.nvidia.com/nvidia-management-library-nvml}. Important for retrieval and inference operations is to mimic a realistic setting, where each search operation should act as if it would be operating in real-time practical applications. To ensure this realism in performance estimation, although not fully simulated, the sequential retrieval latency on the dev dataset will be measured (essentially batch size of 1). Since the training dataset is not used in the inference of the various stages and also is much larger, it will undergo batched measurement with a consistent batch size of 128 to process it in a reasonable time. For the inference latency comparisons, the time to perform the average inference on a sample in milliseconds will not only encompass the inference time, but also the preparation time of the data for each stage. It should be noted that the time to load the model for each inference step will be excluded from the latency comparisons.
