\begin{figure*}[hbt!]
    \centering
    % \includegraphics [width=0.57\textwidth]{figs/ictir_figure.pdf}
    \includegraphics [width=1.05\textwidth]{figs/ecir_2024_cagqer.pdf}
    \caption{Overview of the proposed \car{} framework and inference workflow}
    \label{system}
\end{figure*}
\section{Method}

%\todo {Rewrite this Section 3 so as to describe our approach and include reference to subsections.}
%In this section, we describe the proposed Context Aware Query Enrichment Ranking framework for ambiguous query reformulation. Figure \ref{system} shows the working of the proposed framework during the training and inference stages. During training, we employ a context-aware prompting of a LLM (\car{}) (Section \ref{rewriter:method}) to get query rewrites and use them to fine-tune a ranker specialized for ambiguous queries. During inference, the ambiguous queries are identified based on the query performance scores obtained using a fine-tuned query performance predictor model. Then the specialized ranker and base ranker are directly employed to rank documents for new queries without rewriting them, as shown in Figure \ref{system}. The relevance scores from both the approaches are combined using rank fusion, yielding the final ranked list.

%In this section, we describe the proposed Context Aware Query Enrichment Ranking framework for ambiguous query reformulation. Figure \ref{system} shows the proposed framework during the training and inference stages. During training, we utilize LLMs(CAQE)(Section \ref{rewriter:method}), to generate alternative query formulations. These rewritten queries serve as training data for our specialised ranker for ambiguous queries(SR). During inference we first get query performance using a trained QPP(Section \ref{method:qpp}) model and then use the predictor score along with  rank fusion to get the final ranking.

In this section, we describe our proposed ranking framework as shown in Figure \ref{system}. It consists of two stages, In first stage, we enrich hard queries using LLMs (\car{})(Section \ref{rewriter:method}). We then train two models, one where enriched queries serve as training data for a specialized ranker for hard queries ($SR$) and the other using the original queries ($BR$)(Section \ref{method:ranker}). In second stage, at inference time, we use query performance prediction(Section\ref{method:qpp}) and routing(Section \ref{method:rank_fusion} and Section \ref{method:qpp_rf}) to get the best performance for the query using the above rankers.


\subsection{Context Aware Query Enrichment(\car{})}
\label{rewriter:method}
Consider a set of queries $Q=\{HQ \cup Q^+\}$, that comprises sets of hard queries $HQ$ and well-formed queries $Q^+$.
\begin{tcolorbox}[title= Example Prompt]
\small
\begin{minipage}{\textwidth}
\textbf{Instruction}: Being a ranking model, your first task is to do query
expansion. This means that a query and a document expand
the query so that it is relevant to the document. Expand and
contextualize the query as best as you can in one or two
short sentences.\\
\textbf{Query}: hs worms \texttt{} \\
\textbf{Context}: Established in 1996, Hochschule Worms (Worms University of Applied Sciences) is a non-profit public higher education institution 
 \dots \texttt{}
 \end{minipage}
 
% \textbf{LLM Generated Output:}

\end{tcolorbox}

Given a hard query $hq_i \in HQ$, our goal is to generate a rewritten query $q_i^*$ that enriches the original under specified query to result in a more clear and concise query for a ranker. We generate $q_i^*$ by conditioning the LLM on a few-shot prompt using the top-1 relevant document $d^+$ (from set of documents $D$) for the query. An example of a few-shot prompt employed is  shown above.
\[q_i^* = LLM(aq_i,d^+)\]

We first instruct the LLM as shown in the example above to perform the task of query enrichment in the context of the given document. We then provide an hard query ($hq_i$) concatenated with the corresponding relevant document as a part of the prompt. This \textit{context-aware prompting} of LLMs results in better rewrites without topic drifts, as the LLM is conditioned on intents conveyed in the document. We also introduce a constraint on the maximum length of the output sequence generated. This constraint coupled with grounding using document context helps prevent hallucination and topic drift.
 % We also experiment with different prompting approaches by varying the document context. We discuss the different variations of few-shot prompting and the corresponding results in Section~\ref{models} and Section~\ref{results} respectively.
% \subsubsection{Hyperparameters}
% We use a temperature of 0.5 to balance exploration and deterministic generation. We set presence penalty and frequency penalty to 0.6 and 0.8 respectively to minimize redundancy in generated reformulations of the original query. 
The resulting enriched query is used as an input to fine-tune the ranker.
Note that we only rely on LLMs to enrich hard queries for fine-tuning the ranker. During inference, the ranker directly ranks relevant documents for the new queries without the LLM based enrichment component. 

\subsubsection{Passage selector for Documents}
%\noindent \textbf{Passage selector for Documents}: 
\label{method:attention_linear}
We noticed additional \textit{topic drift} in the context of utilizing the \car{} approach with the document corpus. \textbf{Topic drift} is characterized by the divergence of a rewritten query's specificity when applying the \car{} approach to documents. This occurs due to documents encompassing multiple topics, potentially relating to one or more aspects of the query. To mitigate this occurrence of \textit{topic drift}, we employ supervised passage selection techniques (\attention{}, \linear{}) as proposed in ~\cite{leonhardt2021learnt}. These techniques aid in selecting the most relevant passage from a document, aligning it more closely with a given query.

%\subsection{Ranking Phase}
\subsection{Training Ranker}
\label{method:ranker}

 We employ the pointwise loss to train the ranker. Assume a mini batch of $N$ training examples $\left\{x_i, y_i\right\}_{i=1, ..., N}$. The ranking task is cast as a binary classification problem, where each training instance $x_i = (q^*_i, d_i)$ is a query-document pair and $y_i \in {0, 1}$ is a relevance label. The predicted score of $x_i$ is denoted as $\hat{y}_i$. The cross-entropy loss function is defined as follows:
\begin{equation}
    \mathcal{L}_{\mathtt{Point}} = -\frac{1}{N} \sum_{i=1}^{N} \left( y_{i} \cdot \log \hat{y}_{i} + (1 - y_{i}) \cdot \log (1-\hat{y}_{i}) \right)
\end{equation}

%Note that we only fine-tune the ranker on query reformulations of ambiguous or under-specified queries.
%At inference time, the ranker equipped with the knowledge of user information needs is directly deployed to rank relevant documents.
We train two rankers, one trained on hard enriched query using \car{} (Specialised ranker for Hard queries or $SR$) and the other on original queries 
 (Base Ranker or $BR$). As we generate rewrites using different LLM's, the $SR$ are thus named after their respective LLM's.


\subsection{Rank Fusion (\car{} + \rf{})}
\label{method:rank_fusion}
The $SR$ is specialized to improve the ranking performance for hard queries. To employ $SR$ for hard queries, there is a need for an effective automated way of identifying such queries during inference.
Hence, we propose to combine ranked lists from the base ranker $BR$ and the 
%ranker specialized for hard queries 
$SR$ during inference to prevent compromise in document ranking performance on other queries. We adopt the score based rank fusion approach, namely CombSUM \cite{Fox1993CombinationOM} as we observe the best performance compared to other fusion methods. More formally, for a given test query, $q_i$ and corresponding document $d_i$, 
$r_i = BR(q_i,d_i) ; r_j = SR(q_i,d_i)$
The final relevance score is calculated as the sum of document retrieval scores: $r_i+r_j$
\subsection{ Routing using Query Performance Prediction (\car{} + QPP{})}
\label{method:qpp}
While rank fusion helps us to combine the capabilities of both the models, it assigns equal weights to relevance scores from the two ranking models ($BR$ and $SR$), thereby not distinguishing hard and well-formed queries. In practice, giving higher weight to the $SR$ model score for hard queries would result in improved retrieval quality. Hence, if we have a method for determining the hard query at inference time, would choose an appropriate ranking model or assign appropriate weights to relevance scores for the rank fusion. To determine if a query is hard at inference time, we propose to fine-tune a Query Performance Prediction (QPP) model inspired by BERT-QPP \cite{bert_qpp}. Given a query $q_i$ and top-k retrieved documents $D^+_k$ for the query using BM25, we fine-tune a cross-encoder based QPP model $\psi$  using binary cross-entropy loss as follows:
\[
    \mathcal{L} = -\frac{1}{N} \sum_{i=1}^{N} \left( M(q_i,D^+) \cdot \log  \psi(q_i,D^+_k)  + 
    \\ (1 - M(q_i,D^+)) \cdot \log (1-\psi(q_i,D^+_k)) \right)
\]
where $M$ refers to nDCG score and $D^+_k$ denotes top-k relevant documents ($D^+$).

We then use a threshold decided based on range of scores estimated across queries to route the queries to $SR$ or $BR$. 
%Queries with scores below the threshold are ranked using the $SR$ and the rest using the $BR$.

\subsection{Rank Fusion using Query Performance Prediction (\car{} + QPP + \rf{})}
\label{method:qpp_rf}
Finally, we propose a QPP based rank fusion approach which combines the two ranking scores ($BR$ and $SR$) weighted by the QPP score.
%the base ranker and the ranker specialized for hard queries. 
Given a query at inference time, $q_i$ we first obtain an estimate of the query performance using the fine-tuned BERT-QPP model ($\psi$) defined in Section \ref{method:qpp} and the set of initial documents retrieved ($D^+$) using BM25.
\[qp_i = \psi\left(q_i,D^+\right)\]
Then the score is used to weigh the relevance scores obtained for the query document pairs from BR  and the SR .

\begin{equation}
    s_i = qp_i * SR(q_i,d_i) + (1-qp_i) * BR(q_i,d_i)
\label{eq:qpp_rf}
\end{equation}


The resulting score is used to rank documents. We observe that our QPP based rank fusion helps obtain better performance on hard queries without compromising performance on other queries.
