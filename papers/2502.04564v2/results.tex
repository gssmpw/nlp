%\sandra{moved the following here from methodology} We found that, .......\chris{Findings from the scenarios based questions here}We also asked participants to prioritize the scenarios where they feel AAE/AAL will be most useful for them and found that ...\chris{preference }.

%\hal{need a bit of text here just to roadmap what is coming}

We outline the results from our study below. We review our findings from our scenarios-based survey, which explored Black American perspectives and expectations for interactions with language-based generative AI tools, looking at a range of personal and professional settings. We then review the results from the data annotation effort, whereby we gathered Black Americans' judgments of the effectiveness of large language models at production of AAE and MUSE relative to our human baselines of Black American transcribed interviews -- CORAAL, Tweets, and NPR Interviews. 


% We discuss here the results from the first portion of our survey, which requested study participant insights into their comfort levels with LLM use of AAE in human-computer interactions in several contexts, delineated in the survey under ~\ref{subsec:survey}. The responses were analyzed to determine how users wish their AI systems to interact in terms of language settings—whether in MUSE or AAE. This examination helps us understand the degree of personalization and adaptability expected from AI systems in different contexts.
% The results of our senario-based questions are discussed here. 
%To this end, we analyzed the responses to understand user preferences for language settings in AI systems, specifically focusing on the use of MUSE and AAE. 

%\hal{do we ever actually point to the heatmap figure anywhere?}

% \subsubsection{Results by Scenario} 

% The results discussed below are from the full set of 104 participants.  In \autoref{fig:heatmap}, we show the number of participants who chose each possible way the LLM might interact with them (e.g., AlwaysAAE or AutoDetect) for each of our seven scenarios.

% %complete results are visually represented in a heatmap (see Figure \ref{fig:heatmap}), facilitating a clearer understanding of the dominant preferences across various applications.
\subsection{Survey (Scenarios-based Questions)}

\begin{figure}[t]
\centering
\includegraphics[width=0.85\columnwidth,trim=8 32 60 8,clip]{responses_heatmap2.pdf}
\caption{\footnotesize{This heatmap depicts participant ($n=$ 104) preferences (horizontal axis) for the use of language varieties in seven scenarios (vertical axis). A greater number of participants preferred either for the system to use MUSE or to allow them to select between MUSE and AAE. There were some exceptions: e.g., auto-detection was considered more acceptable in SMS, and MUSE was preferred for email.}}% Overall, it was rare that participants expressed no preference.}
\label{fig:heatmap}
\end{figure}

% \paragraph{Household AI Assistant.}
% In this scenario, {31\%} of our participants demonstrated a strong preference for the  \textit{UserOption} indicating a significant interest in tailoring AI interactions to their personal lifestyle and language preferences. Notably, this scenario also records the lowest support for \textit{AlwaysMUSE}, with only {26\%} participants selecting this option, and the second highest combined score for \textit{UserOption, AutoDetect} and \textit{AlwaysAAE} ({64\%}), emphasizing the value placed on flexibility and customization in the use of home devices. 

% \paragraph{Personal AI Assistant.}
% In contrast, in the personal setting, responses show a more distributed preference, highlighting the personal nature of informal AI interactions where users appreciate a blend of adaptability and personal control over language settings. This scenario suggests a preference for AI that can seamlessly adapt to the user's communication style. This is supported by the total sum of response being {66\%} for \textit{UserOption, AutoDetect} and \textit{AlwaysAAE} being the highest across all scenarios.

% \paragraph{Professional AI Assistant.}
% Preference for \textit{AlwaysMUSE} and \textit{UserOption} indicates a strong desire for both ``formal'' language use and control over language settings in professional contexts. 
% {34\%} of respondents favor MUSE in professional contexts, while {38\%} prefer manual control. This totals {72\%}, signalling a significant preference for a formal and controlled linguistic environment.

% % totaling {72\%}  who desire a formal and controlled linguistic environment. This highlights the value placed on clarity and formality in professional interactions

% % which shows that {33.65\%} (35 persons) of respondents favor the use of MUSE in professional settings with {37.5\%} (39 persons) also favoring manual control over language settings making a total of {71.15\%} of our total respondents. 
% % This suggests a desire for a formal and controlled linguistic environment in professional interactions, where clarity and formality are likely valued.

% \paragraph{Educational Avatar.}
% We observed varied preferences, with {38\%} favoring the \textit{AlwaysMUSE} and {35\%} preferring {UserOption}. This split highlights the diverse linguistic needs in educational settings, underscoring the importance of inclusivity and flexibility in language usage to cater to different learner backgrounds. 

% \paragraph{Customer Service Chatbot.}
% % This scenario saw the highest preference for \textit{AlwaysMUSE}. Our analysis for the customer service chatbot scenario shows a predominant preference for the chatbot to always communicate in Mainstream U.S. English (MUSE), with the highest count of responses  {41\%} favoring this option. This indicates a strong preference among users for maintaining a standard language style in customer service interactions.
% {41\%} of respondents showed a clear preference for \textit{AlwaysMUSE}, the second highest for any other scenario they were presented with. 

% %This indicates a strong user preference for maintaining a standard language style in customer service interactions.
% % \kac{move this to discussion}
% % The preference for always using MUSE suggests that users value consistency and possibly perceive it as more efficient and less prone to misunderstandings in service resolutions. This trend could be reflective of a broader concerns regarding reliability and professionalism in customer-facing AI applications, emphasizing the importance of addressing dialect prejudice in language models. Such biases, as discussed in Hofmann's recent study \citeposs{hofmann2024dialect}, can lead to prejudiced AI decisions affecting character judgments and employability, potentially influencing how customer inquiries are handled and responded to by AI systems.



% % In the educational avatar scenario, preferences varied significantly: {38\%} of respondents favored the \textit{AlwaysMUSE} option, while {35\%} supported the \textit{UserOption}. This division underscores the diverse linguistic needs in educational settings and highlights the necessity for inclusivity and flexibility to accommodate learners from different backgrounds.
% % \kac{move this to discussion}
% % The need for such adaptability is further supported by \cite{mayfield-etal-2019-equity}, which discusses the broader impacts of NLP and AI on educational equity. The study stresses that technology must be designed to be inclusive and sensitive to diverse linguistic backgrounds, ensuring that all students have equitable access to educational resources.


% \paragraph{SMS Autocomplete.}
% This setting is the one in which participants wanted the greatest ability to customize explicitly (UserOption) or implicitly (AutoDetect) the behavior of the system (68\% in total). Nonetheless, a full 22\% preferred SMS autocompletion to only be in MUSE.
% %In contrast to the formal email setting, the text messaging assistant saw a more balanced distribution of preferences: {21\%} of our participants preferred \textit{AlwaysMUSE}, and an even split of {64\%} of participants preferred \textit{UserOption} and \textit{AutoDetect}. 
% %Indicating that, for the same tool in a personal setting, users value flexibility and the capability of AI to adapt linguistically a more casual and personalized approach that can potentially enhance user engagement and satisfaction.

% \paragraph{Email Autocomplete.}
% This scenario shows an incredibly strong preference for \textit{Always MUSE} {69\%}, consistent with the perception that MUSE is a more appropriate language variety in professional settings.
% In contrast to SMS autocomplete, only {3\%} preferred autodetection, the lowest of the seven scenarios.
% %This preference possibly reflects concerns over misinterpretation and the desire to maintain a professional tone in all workplace communications.


%\subsubsection{Discussion of Survey Results}
As discussed previously in \autoref{subsec:survey}, our scenario-based questions were designed to elicit the degree to which participants wanted LLMs to use AAE in various interaction contexts. The findings from our survey reveal a nuanced interplay between user preferences and the linguistic contexts in which AI technologies might be used. Number of study participants by preference (horizontal axis) and scenario (vertical axis) as seen in  \autoref{fig:heatmap} reveal a preference gradient that spans from a strong inclination towards using MUSE in more formal or task-specific interactions to a marked openness for dialectical variability in more personalized or casual settings. This preference spectrum not only reflects current user expectations but also aligns with broader societal shifts towards more personalized and context-aware technologies.

The strong preference for MUSE in formal scenarios suggests that users prioritize consistency and efficiency, likely due to perceptions of professionalism in customer-facing AI applications. This finding is crucial as it highlights concerns about dialect prejudice in LLMs, where biases may influence AI decisions on character judgments and employability, affecting how customer inquiries are handled and responded to by AI systems. For example, \citet{hofmann2024dialect} emphasizes the need to address these biases, illustrating how they can impact equity and fairness in AI interactions.


Moreover, the demand for dialectical flexibility in casual or personal use scenarios underscores the importance of adaptable and culturally competent AI designs. Such adaptability is essential for ensuring that AI technologies cater to a diverse user base with varied linguistic backgrounds, thereby promoting inclusivity. This need aligns with findings from \citet{mayfield-etal-2019-equity}, which discuss the broader impacts of NLP and AI on educational equity, stressing that technology must be sensitive to diversity to ensure equitable access to educational resources. The insights from our study suggest a nuanced approach to AI communication strategies, balancing standardization with personalization to meet the complex preferences of users.

%In crafting AI solutions, it is imperative for researchers, developers, and policymakers to consider these diverse needs. By integrating these insights, we can develop AI systems that are not only innovative but also inclusive, ensuring they meet the broad spectrum of user expectations and contribute positively to societal and technological advancements.


% The findings from the scenario-specific questions in our survey underscore the nuanced interplay between user preferences and the linguistic settings of AI technologies in various application scenarios. Consistently, the data reveal a preference gradient that spans from a strong inclination towards using MUSE in more formal or task-specific interactions to a marked openness for dialectical variability in more personalized or casual settings.
% These preferences captured through our survey not only reflect the current landscape of user expectations from AI but also align with broader societal shifts towards more personalized and context-aware technologies. 
% This insight is crucial for researchers,developers and policymakers in crafting AI solutions that are both effective and culturally competent, ensuring that AI technologies are as inclusive as they are innovative.

%\subsection{Results of Data Annotation Task}

%\begin{table}[t]
 % \centering
  %\footnotesize
      %\begin{tabular}{@{~}l@{~~~}r@{}l@{~~}r@{}l@{~~}r@{}l@{~~}r@{}l@{~~}r@{}l@{~~}r@{}l@{~}}
      %\toprule   
    %& \rotatebox{90}{\textbf{Coherent}}~~
    %&& \rotatebox{90}{\textbf{\parbox{1cm}{AAE\\Features}}}
    %&& \rotatebox{90}{\textbf{Authentic}}~~
    %&& \rotatebox{90}{\textbf{\parbox{1cm}{White\\Sounding}}}
    %&& \rotatebox{90}{\textbf{Mocking}}~~
    %&& \rotatebox{90}{\textbf{Offensive}}~~
    %\\
    
     % \midrule
      %GPT & 0.31 && -0.51 && -0.05 && -0.42 && -0.98 && \bf-1.06&* \\ 
      %Llama & \bf 0.57&** & \bf0.10&** & \bf0.35&** & -0.46 && -0.81  && -0.95&  \\
      %Mixtral & \bf0.56&** & -0.32  && 0.07  && \bf-0.17&** & \bf-1.05&* & \bf-1.20&** \\
      %\midrule
      %Human  & 0.20 && -0.37  && -0.04  && -0.39  && -0.88  &&  -0.89& \\
      %(CORAAL) & && && && && && \\
      %\midrule
      %$\Delta$GPT & 0.11 && -0.14 && -0.01 && -0.03 && -0.11 && \bf-0.17&* \\ 
      %$\Delta$Llama & \bf 0.37&** & \bf0.47&** & \bf0.39&** & 0.85 && 0.07  && -0.06&  \\
      %$\Delta$Mixtral & \bf0.36&** & 0.05  && 0.11  && %\bf0.22&** & \bf-0.17&* & \bf-0.31&** \\
      %\bottomrule
      %\end{tabular}
      %}
      %\caption{(Top) Mean Likert scores for each LLM for a given linguistic judgment ($n$ ranged from 478 to 491 observations for a given sample). (Middle) Scores for the original human text. (Bottom) Differences between LLM scores and human text scores. Bold results represent statistically significant differences to the human-written text. $p<0.05$ is marked with *; $p<0.01$ with **.}
      %\label{tab:ttests_full_results}
  %\end{table}


\begin{table*}[t]
    \centering
    \footnotesize
    \renewcommand{\arraystretch}{0.85} % Reduce row height
\setlength{\tabcolsep}{4pt}
    % \resizebox{\textwidth}{!}{%
    % \begin{tabular}{l@{\hspace{20pt}}ccc@{\hspace{30pt}}ccc}
    \begin{tabular}{l@{\hspace{20pt}}ccc@{\hspace{40pt}}ccc}
        \toprule
        \multirow{2}{*}{\textbf{System}} 
        & \multicolumn{3}{c}{\textbf{Coherence}} 
        & \multicolumn{3}{c}{\textbf{AAE Features}} \\
        \cmidrule{2-4} \cmidrule{5-7}
        & \textbf{CORAAL} & \textit{\textbf{NPR}} & \textbf{TWEETS} 
        & \textbf{CORAAL} & \textit{\textbf{NPR}} & \textbf{TWEETS} \\
        \midrule
        \textbf{Human} 
        & 0.35$_{\scalebox{0.7}{$\pm0.30$}}$ & 0.99$_{\scalebox{0.7}{$\pm0.51$}}$ & 0.04$_{\scalebox{0.7}{$\pm0.54$}}$
        & 0.18$_{\scalebox{0.7}{$\pm0.39$}}$ & -0.55$_{\scalebox{0.7}{$\pm0.84$}}$ & -0.57$_{\scalebox{0.7}{$\pm0.87$}}$ \\[0.5em]
        \textbf{GPT} 
        & 0.80$_{\scalebox{0.7}{$\pm0.07$}}$ & 1.29$_{\scalebox{0.7}{$\pm0.36$}}$ & 0.64$_{\scalebox{0.7}{$\pm0.24$}}$
        & 1.18$^{***}_{\scalebox{0.7}{$\pm 0.17$}}$ & -0.61$_{\scalebox{0.7}{$\pm0.87$}}$ & 0.99$^{***}_{\scalebox{0.7}{$\pm0.07$}}$ \\
        \textbf{Llama} 
        & 0.66$_{\scalebox{0.7}{$\pm0.20$}}$ & 1.31$_{\scalebox{0.7}{$\pm0.34$}}$ & 0.43$_{\scalebox{0.7}{$\pm0.35$}}$
        & 0.86$^{**}_{\scalebox{0.7}{$\pm0.03$}}$ & -0.67$_{\scalebox{0.7}{$\pm0.90$}}$ & 0.57$^{***}_{\scalebox{0.7}{$\pm0.29$}}$ \\
        \textbf{Mixtral} 
        & 0.60$_{\scalebox{0.7}{$\pm0.15$}}$ & 1.21$_{\scalebox{0.7}{$\pm0.44$}}$ & 0.48$_{\scalebox{0.7}{$\pm0.33$}}$
        & 0.72$_{\scalebox{0.7}{$\pm0.11$}}$ & -0.67$_{\scalebox{0.7}{$\pm0.90$}}$ & 0.66$^{***}_{\scalebox{0.7}{$\pm0.24$}}$ \\
        \bottomrule
    \end{tabular}
    
    \vspace{0.3em}
    
    \begin{tabular}{l@{\hspace{20pt}}ccc@{\hspace{40pt}}ccc}
        \toprule
        \multirow{2}{*}{\textbf{System}} 
        & \multicolumn{3}{c}{\textbf{Black Sounding}} 
        & \multicolumn{3}{c}{\textbf{White Sounding}} \\
        \cmidrule{2-4} \cmidrule{5-7}
        & \textbf{CORAAL} & \textit{\textbf{NPR}} & \textbf{TWEETS} 
        & \textbf{CORAAL} & \textit{\textbf{NPR}} & \textbf{TWEETS} \\
        \midrule
        \textbf{Human} 
        & 0.39$_{\scalebox{0.7}{$\pm0.28$}}$ & 0.15$_{\scalebox{0.7}{$\pm0.42$}}$ & -0.30$_{\scalebox{0.7}{$\pm0.72$}}$
        & -0.23$_{\scalebox{0.7}{$\pm 0.62$}}$ & 0.83$_{\scalebox{0.7}{$\pm0.42$}}$ & -0.65$_{\scalebox{0.7}{$\pm1.00$}}$ \\[0.5em]
        \textbf{GPT} 
        & 1.01$^{**}_{\scalebox{0.7}{$\pm 0.07$}}$ & 0.21$_{\scalebox{0.7}{$\pm0.38$}}$ & 0.79$^{***}_{\scalebox{0.7}{$\pm0.13$}}$
        & -0.83$^{*}_{\scalebox{0.7}{$\pm 0.88$}}$ & 0.89$_{\scalebox{0.7}{$\pm0.39$}}$ & -1.07$_{\scalebox{0.7}{$\pm1.21$}}$ \\
        \textbf{Llama} 
        & 0.85$_{\scalebox{0.7}{$\pm0.02$}}$ & 0.31$_{\scalebox{0.7}{$\pm0.34$}}$ & 0.22$_{\scalebox{0.7}{$\pm0.45$}}$
        & -0.74$_{\scalebox{0.7}{$\pm0.87$}}$ & 1.02$_{\scalebox{0.7}{$\pm0.32$}}$ & -0.96$_{\scalebox{0.7}{$\pm1.16$}}$ \\
        \textbf{Mixtral} 
        & 0.85$_{\scalebox{0.7}{$\pm0.04$}}$ & 0.11$_{\scalebox{0.7}{$\pm0.43$}}$ & 0.37$^{*}_{\scalebox{0.7}{$\pm0.37$}}$
        & -0.49$_{\scalebox{0.7}{$\pm0.75$}}$ & 0.81$_{\scalebox{0.7}{$\pm0.43$}}$ & -1.06$_{\scalebox{0.7}{$\pm1.21$}}$ \\
        \bottomrule
    \end{tabular}

    \vspace{0.3em}
    
    \begin{tabular}{l@{\hspace{20pt}}ccc@{\hspace{40pt}}ccc}
        \toprule
        \multirow{2}{*}{\textbf{System}} 
        & \multicolumn{3}{c}{\textbf{Mocking}} 
        & \multicolumn{3}{c}{\textbf{Offensive}} \\
        \cmidrule{2-4} \cmidrule{5-7}
        & \textbf{CORAAL} & \textit{\textbf{NPR}} & \textbf{TWEETS} 
        & \textbf{CORAAL} & \textit{\textbf{NPR}} & \textbf{TWEETS} \\
        \midrule
        \textbf{Human} 
        & -0.88$_{\scalebox{0.7}{$\pm1.35$}}$ & -1.47$_{\scalebox{0.7}{$\pm2.25$}}$ & -0.79$_{\scalebox{0.7}{$\pm1.21$}}$
        & -0.78$_{\scalebox{0.7}{$\pm 1.19$}}$ & -1.48$_{\scalebox{0.7}{$\pm2.27$}}$ & -0.96$_{\scalebox{0.7}{$\pm1.47$}}$ \\[0.5em]
        \textbf{GPT} 
        & -0.57$_{\scalebox{0.7}{$\pm 1.19$}}$ & -1.61$_{\scalebox{0.7}{$\pm2.32$}}$ & -0.57$_{\scalebox{0.7}{$\pm1.27$}}$
        & -0.86$_{\scalebox{0.7}{$\pm 1.23$}}$ & -1.60$_{\scalebox{0.7}{$\pm2.33$}}$ & -0.57$_{\scalebox{0.7}{$\pm1.27$}}$ \\
        \textbf{Llama} 
        & -0.46$_{\scalebox{0.7}{$\pm1.13$}}$ & -1.63$_{\scalebox{0.7}{$\pm2.33$}}$ & 0.14$^{***}_{\scalebox{0.7}{$\pm0.74$}}$
        & -0.53$_{\scalebox{0.7}{$\pm1.07$}}$ & -1.56$_{\scalebox{0.7}{$\pm2.25$}}$ & -0.07$^{***}_{\scalebox{0.7}{$\pm1.02$}}$ \\
        \textbf{Mixtral} 
        & -0.74$_{\scalebox{0.7}{$\pm1.28$}}$ & -1.60$_{\scalebox{0.7}{$\pm2.31$}}$ & -0.20$_{\scalebox{0.7}{$\pm0.91$}}$
        & -0.86$_{\scalebox{0.7}{$\pm1.24$}}$ & -1.60$_{\scalebox{0.7}{$\pm2.33$}}$ & -0.55$_{\scalebox{0.7}{$\pm1.26$}}$ \\
        \bottomrule
    \end{tabular}
   % }
   \caption{Mean Likert scores for each LLM for a given linguistic judgment and corpus ($n$ ranged from 119 to 126 Likert score observations for a given sample in a two sample comparison; 72 two-sample comparisons were conducted.). Scores for the original human text are shown in the Human row. $p<0.05$ is marked with *; $p<0.01$ with ** and $p<0.001$ with ***.
   %\sandra{we need to ensure that we note here and maybe in the text that one set of within corpus comparisons for MUSE are not shown}
   }
    \label{tab:coherence_aae_feats}
\end{table*}

% \begin{table*}[h]
%     \centering
%     \footnotesize
%     % \resizebox{\textwidth}{!}{%
%     % \begin{tabular}{l@{\hspace{20pt}}ccc@{\hspace{30pt}}ccc}
%     \begin{tabular}{l@{\hspace{10pt}}ccc@{\hspace{15pt}}ccc@{\hspace{15pt}}ccc}
%         \toprule
%         \multirow{2}{*}{\textbf{System}} 
%         & \multicolumn{3}{c}{\textbf{Coherence}} 
%         & \multicolumn{3}{c}{\textbf{AAE Features}} 
%         & \multicolumn{3}{c}{\textbf{Black Sounding}} \\
%         \cmidrule{2-4} \cmidrule{5-7} \cmidrule{8-10}
%         & \textbf{CORAAL} & \textit{\textbf{NPR}} & \textbf{TWEETS} 
%         & \textbf{CORAAL} & \textit{\textbf{NPR}} & \textbf{TWEETS} 
%         & \textbf{CORAAL} & \textit{\textbf{NPR}} & \textbf{TWEETS} \\
%         \midrule
%         \textbf{Human} 
%         & 0.35$_{\scalebox{0.7}{$\pm0.30$}}$ & 0.99$_{\scalebox{0.7}{$\pm0.51$}}$ & 0.04$_{\scalebox{0.7}{$\pm0.54$}}$
%         & 0.18$_{\scalebox{0.7}{$\pm0.39$}}$ & -0.55$_{\scalebox{0.7}{$\pm0.84$}}$ & -0.57$_{\scalebox{0.7}{$\pm0.87$}}$
%         & 0.39$_{\scalebox{0.7}{$\pm0.28$}}$ & 0.15$_{\scalebox{0.7}{$\pm0.42$}}$ & -0.30$_{\scalebox{0.7}{$\pm0.72$}}$ \\[0.5em]
%         \textbf{GPT} 
%         & 0.80$_{\scalebox{0.7}{$\pm0.07$}}$ & 1.29$_{\scalebox{0.7}{$\pm0.36$}}$ & 0.64$_{\scalebox{0.7}{$\pm0.24$}}$
%         & 1.18$^{***}_{\scalebox{0.7}{$\pm 0.17$}}$ & -0.61$_{\scalebox{0.7}{$\pm0.87$}}$ & 0.99$_{\scalebox{0.7}{$\pm0.07$}}$
%         & 1.01$^{**}_{\scalebox{0.7}{$\pm 0.07$}}$ & 0.21$_{\scalebox{0.7}{$\pm0.38$}}$ & 0.79$^{***}_{\scalebox{0.7}{$\pm0.13$}}$ \\
%         \textbf{Llama} 
%         & 0.66$_{\scalebox{0.7}{$\pm0.20$}}$ & 1.31$_{\scalebox{0.7}{$\pm0.34$}}$ & 0.43$_{\scalebox{0.7}{$\pm0.35$}}$
%         & 0.86$^{**}_{\scalebox{0.7}{$\pm0.03$}}$ & -0.67$_{\scalebox{0.7}{$\pm0.90$}}$ & 0.57$^{***}_{\scalebox{0.7}{$\pm0.29$}}$
%         & 0.85$_{\scalebox{0.7}{$\pm0.02$}}$ & 0.31$_{\scalebox{0.7}{$\pm0.34$}}$ & 0.22$_{\scalebox{0.7}{$\pm0.45$}}$ \\
%         \textbf{Mixtral} 
%         & 0.60$_{\scalebox{0.7}{$\pm0.15$}}$ & 1.21$_{\scalebox{0.7}{$\pm0.44$}}$ & 0.48$_{\scalebox{0.7}{$\pm0.33$}}$
%         & 0.72$_{\scalebox{0.7}{$\pm0.11$}}$ & -0.67$_{\scalebox{0.7}{$\pm0.90$}}$ & 0.66$^{***}_{\scalebox{0.7}{$\pm0.24$}}$
%         & 0.85$_{\scalebox{0.7}{$\pm0.04$}}$ & 0.11$_{\scalebox{0.7}{$\pm0.43$}}$ & 0.37$^{*}_{\scalebox{0.7}{$\pm0.37$}}$ \\
%         \bottomrule
%     \end{tabular}
    
%     \vspace{1em}

%     \begin{tabular}{l@{\hspace{10pt}}ccc@{\hspace{15pt}}ccc@{\hspace{15pt}}ccc}
%         \toprule
%         \multirow{2}{*}{\textbf{System}} 
%         & \multicolumn{3}{c}{\textbf{White Sounding}} 
%         & \multicolumn{3}{c}{\textbf{Mocking}} 
%         & \multicolumn{3}{c}{\textbf{Offensive}} \\
%         \cmidrule{2-4} \cmidrule{5-7} \cmidrule{8-10}
%         & \textbf{CORAAL} & \textit{\textbf{NPR}} & \textbf{TWEETS} 
%         & \textbf{CORAAL} & \textit{\textbf{NPR}} & \textbf{TWEETS} 
%         & \textbf{CORAAL} & \textit{\textbf{NPR}} & \textbf{TWEETS} \\
%         \midrule
%         \textbf{Human} 
%         & -0.23$_{\scalebox{0.7}{$\pm 0.62$}}$ & 0.83$_{\scalebox{0.7}{$\pm0.42$}}$ & -0.65$_{\scalebox{0.7}{$\pm1.00$}}$
%         & -0.88$_{\scalebox{0.7}{$\pm1.35$}}$ & -1.47$_{\scalebox{0.7}{$\pm2.25$}}$ & -0.79$_{\scalebox{0.7}{$\pm1.21$}}$
%         & -0.78$_{\scalebox{0.7}{$\pm 1.19$}}$ & -1.48$_{\scalebox{0.7}{$\pm2.27$}}$ & -0.96$_{\scalebox{0.7}{$\pm1.47$}}$ \\[0.5em]
%         \textbf{GPT} 
%         & -0.83$^{*}_{\scalebox{0.7}{$\pm 0.88$}}$ & 0.89$_{\scalebox{0.7}{$\pm0.39$}}$ & -1.07$_{\scalebox{0.7}{$\pm1.21$}}$
%         & -0.57$_{\scalebox{0.7}{$\pm 1.19$}}$ & -1.61$_{\scalebox{0.7}{$\pm2.32$}}$ & -0.57$_{\scalebox{0.7}{$\pm1.27$}}$
%         & -0.86$_{\scalebox{0.7}{$\pm 1.23$}}$ & -1.60$_{\scalebox{0.7}{$\pm2.33$}}$ & -0.57$_{\scalebox{0.7}{$\pm1.27$}}$ \\
%         \textbf{Llama} 
%         & -0.74$_{\scalebox{0.7}{$\pm0.87$}}$ & 1.02$_{\scalebox{0.7}{$\pm0.32$}}$ & -0.96$_{\scalebox{0.7}{$\pm1.16$}}$
%         & -0.46$_{\scalebox{0.7}{$\pm1.13$}}$ & -1.63$_{\scalebox{0.7}{$\pm2.33$}}$ & 0.14$^{***}_{\scalebox{0.7}{$\pm0.74$}}$
%         & -0.53$_{\scalebox{0.7}{$\pm1.07$}}$ & -1.56$_{\scalebox{0.7}{$\pm2.25$}}$ & -0.07$^{***}_{\scalebox{0.7}{$\pm1.02$}}$ \\
%         \textbf{Mixtral} 
%         & -0.49$_{\scalebox{0.7}{$\pm0.75$}}$ & 0.81$_{\scalebox{0.7}{$\pm0.43$}}$ & -1.06$_{\scalebox{0.7}{$\pm1.21$}}$
%         & -0.74$_{\scalebox{0.7}{$\pm1.28$}}$ & -1.60$_{\scalebox{0.7}{$\pm2.31$}}$ & -0.20$_{\scalebox{0.7}{$\pm0.91$}}$
%         & -0.86$_{\scalebox{0.7}{$\pm1.24$}}$ & -1.60$_{\scalebox{0.7}{$\pm2.33$}}$ & -0.55$_{\scalebox{0.7}{$\pm1.26$}}$ \\
%         \bottomrule
%     \end{tabular}
    
%    % }
%    \caption{Mean Likert scores for each LLM for a given linguistic judgment and corpus ($n$ ranged from 119 to 126 Likert score observations for a given sample in a two sample comparison; 72 two-sample comparisons were conducted.). Scores for the original human text are shown in the Human row. $p<0.05$ is marked with *; $p<0.01$ with ** and $p<0.001$ with ***.}
%     \label{tab:coherence_aae_feats2}
% \end{table*}


% \begin{table*}[h]
%     \centering
%     \footnotesize
%     %\resizebox{\textwidth}{!}{%
%     % \begin{tabular}{l@{\hspace{20pt}}ccc@{\hspace{30pt}}ccc}
%     \begin{tabular}{l@{\hspace{20pt}}ccc@{\hspace{40pt}}ccc}
%         \toprule
%         \multirow{2}{*}{\textbf{System}} 
%         & \multicolumn{3}{c}{\textbf{Black Sounding}} 
%         & \multicolumn{3}{c}{\textbf{White Sounding}} \\
%         \cmidrule{2-4} \cmidrule{5-7}
%         & \textbf{CORAAL} & \textit{\textbf{NPR}} & \textbf{TWEETS} 
%         & \textbf{CORAAL} & \textit{\textbf{NPR}} & \textbf{TWEETS} \\
%         \midrule
%         \textbf{Human} 
%         & 0.39$_{\scalebox{0.7}{$\pm0.28$}}$ & 0.15$_{\scalebox{0.7}{$\pm0.42$}}$ & -0.30$_{\scalebox{0.7}{$\pm0.72$}}$
%         & -0.23$_{\scalebox{0.7}{$\pm 0.62$}}$ & 0.83$_{\scalebox{0.7}{$\pm0.42$}}$ & -0.65$_{\scalebox{0.7}{$\pm1.00$}}$ \\[0.5em]
%         \textbf{GPT} 
%         & 1.01$^{**}_{\scalebox{0.7}{$\pm 0.07$}}$ & 0.21$_{\scalebox{0.7}{$\pm0.38$}}$ & 0.79$^{***}_{\scalebox{0.7}{$\pm0.13$}}$
%         & -0.83$^{*}_{\scalebox{0.7}{$\pm 0.88$}}$ & 0.89$_{\scalebox{0.7}{$\pm0.39$}}$ & -1.07$_{\scalebox{0.7}{$\pm1.21$}}$ \\
%         \textbf{Llama} 
%         & 0.85$_{\scalebox{0.7}{$\pm0.02$}}$ & 0.31$_{\scalebox{0.7}{$\pm0.34$}}$ & 0.22$_{\scalebox{0.7}{$\pm0.45$}}$
%         & -0.74$_{\scalebox{0.7}{$\pm0.87$}}$ & 1.02$_{\scalebox{0.7}{$\pm0.32$}}$ & -0.96$_{\scalebox{0.7}{$\pm1.16$}}$ \\
%         \textbf{Mixtral} 
%         & 0.85$_{\scalebox{0.7}{$\pm0.04$}}$ & 0.11$_{\scalebox{0.7}{$\pm0.43$}}$ & 0.37$^{*}_{\scalebox{0.7}{$\pm0.37$}}$
%         & -0.49$_{\scalebox{0.7}{$\pm0.75$}}$ & 0.81$_{\scalebox{0.7}{$\pm0.43$}}$ & -1.06$_{\scalebox{0.7}{$\pm1.21$}}$ \\
%         \bottomrule
%     \end{tabular}
%     %}
%    \caption{TODO CAPTION ME}
%     \label{tab:black_white_sounding}
% \end{table*}

% \begin{table*}[h]
%     \centering
%     \footnotesize
% %    \resizebox{\textwidth}{!}{%
%     % \begin{tabular}{l@{\hspace{20pt}}ccc@{\hspace{30pt}}ccc}
%     \begin{tabular}{l@{\hspace{20pt}}ccc@{\hspace{40pt}}ccc}
%         \toprule
%         \multirow{2}{*}{\textbf{System}} 
%         & \multicolumn{3}{c}{\textbf{Mocking}} 
%         & \multicolumn{3}{c}{\textbf{Offensive}} \\
%         \cmidrule{2-4} \cmidrule{5-7}
%         & \textbf{CORAAL} & \textit{\textbf{NPR}} & \textbf{TWEETS} 
%         & \textbf{CORAAL} & \textit{\textbf{NPR}} & \textbf{TWEETS} \\
%         \midrule
%         \textbf{Human} 
%         & -0.88$_{\scalebox{0.7}{$\pm1.35$}}$ & -1.47$_{\scalebox{0.7}{$\pm2.25$}}$ & -0.79$_{\scalebox{0.7}{$\pm1.21$}}$
%         & -0.78$_{\scalebox{0.7}{$\pm 1.19$}}$ & -1.48$_{\scalebox{0.7}{$\pm2.27$}}$ & -0.96$_{\scalebox{0.7}{$\pm1.47$}}$ \\[0.5em]
%         \textbf{GPT} 
%         & -0.57$_{\scalebox{0.7}{$\pm 1.19$}}$ & -1.61$_{\scalebox{0.7}{$\pm2.32$}}$ & -0.57$_{\scalebox{0.7}{$\pm1.27$}}$
%         & -0.86$_{\scalebox{0.7}{$\pm 1.23$}}$ & -1.60$_{\scalebox{0.7}{$\pm2.33$}}$ & -0.57$_{\scalebox{0.7}{$\pm1.27$}}$ \\
%         \textbf{Llama} 
%         & -0.46$_{\scalebox{0.7}{$\pm1.13$}}$ & -1.63$_{\scalebox{0.7}{$\pm2.33$}}$ & 0.14$^{***}_{\scalebox{0.7}{$\pm0.74$}}$
%         & -0.53$_{\scalebox{0.7}{$\pm1.07$}}$ & -1.56$_{\scalebox{0.7}{$\pm2.25$}}$ & -0.07$^{***}_{\scalebox{0.7}{$\pm1.02$}}$ \\
%         \textbf{Mixtral} 
%         & -0.74$_{\scalebox{0.7}{$\pm1.28$}}$ & -1.60$_{\scalebox{0.7}{$\pm2.31$}}$ & -0.20$_{\scalebox{0.7}{$\pm0.91$}}$
%         & -0.86$_{\scalebox{0.7}{$\pm1.24$}}$ & -1.60$_{\scalebox{0.7}{$\pm2.33$}}$ & -0.55$_{\scalebox{0.7}{$\pm1.26$}}$ \\
%         \bottomrule
%     \end{tabular}
% %    }
%    \caption{TODO CAPTION ME}
%     \label{tab:mock_offensive}
% \end{table*}

% \begin{table*}[h]
%     \centering
%     \footnotesize
%     \begin{tabular}{l@{\hspace{0.5cm}}cc@{\hspace{1cm}}cc@{\hspace{1cm}}cc}
%         \toprule
%         \multirow{2}{*}{\textbf{System}} & \multicolumn{2}{c}{\textbf{Coherence}} & \multicolumn{2}{c}{\textbf{AAE Features}} & \multicolumn{2}{c}{\textbf{Black Sounding}} \\
%         \cmidrule(lr{0.5cm}){2-3} \cmidrule(lr{0.5cm}){4-5} \cmidrule(l){6-7}
%         & \hspace{1cm}Coraal & Tweets & \hspace{1cm}Coraal & Tweets & \hspace{1cm}Coraal & Tweets \\
%         \midrule
%         Human   & a  & b  & c  & d  & e  & f \\
%         Llama   & g  & h  & i  & j  & k  & l \\
%         Mixtral & m  & n  & o  & p  & q  & r \\
%         GPT     & s  & t  & u  & v  & w  & x \\
%         \bottomrule
%     \end{tabular}
%     % \caption{Combined Table with Broken Horizontal Lines}
%    \caption{TODO CAPTION ME}
%     \label{tab:combined_table_broken_lines}
% \end{table*}
% \begin{table*}[h]
%     \centering
%     \footnotesize
%     \begin{tabular}{l@{\hspace{0.5cm}}cc@{\hspace{1cm}}cc@{\hspace{1cm}}cc}
%         \toprule
%         \multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{Table 4}} & \multicolumn{2}{c}{\textbf{Table 5}} & \multicolumn{2}{c}{\textbf{Table 6}} \\
%         \cmidrule(lr{0.8cm}){2-3} \cmidrule(lr{0.8cm}){4-5} \cmidrule(l){6-7}
%         & Coraal & Tweets & Coraal & Tweets & Coraal & Tweets \\
%         \midrule
%         Human   & a  & b  & c  & d  & e  & f \\
%         Llama   & g  & h  & i  & j  & k  & l \\
%         Mixtral & m  & n  & o  & p  & q  & r \\
%         GPT     & s  & t  & u  & v  & w  & x \\
%         \bottomrule
%     \end{tabular}
%    \caption{TODO CAPTION ME}
%     \label{tab:combined_table_broken_lines}
% \end{table*}

\subsection{Annotation of AAE and MUSE texts across Six Linguistic Judgments}
Our annotators made six assessments (via Likert score ratings) as seen in \autoref{table:Label Descriptions}, providing their linguistic judgments regarding the human or LLM-generated text. This allows us to study how well LLMs are able to generate AAE-like text comparing across AAE and MUSE texts.


%\paragraph{Inter-annotator Agreement.} Overall annotator agreement on a subset of 208 triply-annotated samples, measured by Krippendorff's $\alpha$~\cite{krippendorff2018content}, was $0.45$. This is considered ``moderate agreement''~\cite{hughes2021krippendorffsalpha}, indicating that while there is some level of consensus among the annotators, inconsistencies remain significant.
%Agreement was highest ($0.53$) for the last question (offensive), and lowest for the previous two ($0.21$ for White Sounding and $0.15$ for mocking). The others ranged from $0.30$ to $0.39$.

%We selected the Krippendorff’s $\alpha$ metric \cite{krippendorff2018content} to understand the quality and consistency of the annotations for the 208 triply data annotated samples. Overall, the inter-annotator agreement with  Krippendorff’s $\alpha$ for the triply annotated data annotated was 0.45. This is interpreted as moderate agreement \cite{hughes2021krippendorffsalpha} between our three annotators. We speculate that this moderate agreement indicates that while there is some level of consensus among the annotators, inconsistencies remain significant. The complexity or the subjective nature of the material being annotated can also contribute to this level of agreement.


\paragraph{Mapping to Numeric Scores.}
We map Likert scores to the range $-2$ (strongly disagree) to $+2$ (strongly agree) and compute the overall score as the average across all samples. 

\paragraph{Results Analysis Approach.}
%\autoref{tab:ttests_full_results} show the average score across systems and human completions. 
We investigate how Black Americans viewed texts with regard to specific linguistic judgments. To this end, we conduct two-tailed t-tests to determine whether differences in mean scores were significantly different than zero\footnote{Unless otherwise noted, we take ``significant'' to mean false discovery rate of $5\%$. We apply Bonferroni corrections to our p-values, reporting 95 percent confidence intervals, since we performed multiple t-tests assessing differences between Likert score means between samples for each linguistic judgment (within and between corpora for each annotation survey we administered). Bonferroni corrections conservatively report statistical significance. We had six within-corpus comparisons and four between-corpus comparisons per label per each of the two annotation surveys, resulting in a total of 10 comparisons used for each judgment’s bonferroni correction.} to ascertain any statistical differences in mean Likert scores between two independent data samples at a time. Our samples were independent in that the prefixes that annotators labeled, and our annotators themselves, were non-overlapping. 

We conduct two types of between-sample comparisons. The first (\autoref{sec:llmgenaae}) and most critical to our approach involves \emph{Human to Model (within corpus) comparisons of mean Likert scores}. This test is fundamentally a ``does an LLM produce text like a human would, in AAE'' (and ``in MUSE'', as a point of reference). The second (\autoref{sec:llmgenmuse}), as assurance that the generated AAE is actually more like AAE than like MUSE, considers differences in judgments between AAE text and MUSE text (as opposed to human text vs. machine text).

 %Perhaps surprisingly, for the Human to Model comparisons, we found that across most of our linguistic judgment types, our study participant ratings indicated that the LLM AAE generations were equivalent to and sometimes slightly preferable to the human baseline AAE data sets (the original CORAAL interview exchanges and the original AAE Tweets). In other words, differences in average Likert scores were largely statistically insignificant for this set of human baseline to model generation comparisons.

%\footnote{t tests were two-tailed assuming samples had equal variances and hypothesizing that there was no difference between sample means in each of our pair-wise comparisons, including comparisons of the human baseline to GPT, human to Llama, and human to Mixtral. Means that are statistically different  from the human baseline at the 5\% confidence level (p <= 0.05) are marked with an asterisk (*), and with two asterisks (**) for confidence at the 1\% level (p <=0.01) in Table \ref{tab:ttests_full_results}.}

%\textbf{Results of Linguistic Judgments}

%\subsubsection{Analysis of Significant Differences}
\subsubsection{How well do LLMs generate AAE? Analysis of Black Americans' Ratings of LLM versus AAE suffixes} \label{sec:llmgenaae}

%\hal{go through and make sure we point to all tables from somewhere}

In our first analysis, we compare the mean Likert scores for human baseline texts for each corpus (i.e., the human continuation of a text) to the model continuations for prefixes from that corpus as seen in \autoref{tab:coherence_aae_feats}. For these tests, mean Likert scores are assumed to be equal (have zero difference in means between the two samples in question). %Specifically, the null hypothesis was that LLM generations of AAE are equivalent to AAE texts originating from people, as judged by Black Americans with respect to several linguistic assessments.

In general, study participants rated the highlighted suffixes from the AAE produced by all three LLMs, GPT, Llama and Mixtral, equally or better than the original AAE texts (human baselines) across the linguistic judgment assessments of: 1) text continuation coherency, 2) the texts containing AAE features, and 3) the texts sounding like something a Black American might say. Specifically, their Likert scores were in the Agreement range ($\mu> 0$) for these three judgments for the AAE generations they annotated, which is positive for Black Americans favoring AAE or language choice in LLMs (the AlwaysAAE, UserOption and AutoDetect columns of \autoref{fig:heatmap}). 

For the first linguistic judgment \emph{Coherence}, Black Americans agreed that the original human CORAAL AAE text as well as the model continuations were coherent (the difference in means was not statistically significant), with Tweets considered slightly less coherent hovering around neutral ($\mu$ close to 0) for the original posts and slightly agree for the model generations.  Regarding the second linguistic judgment \emph{AAE Features}, while annotators considered some model-generated texts for CORAAL ($\mu=1.18$ for GPT and $\mu=0.86$ for Llama) to possess a greater extent of AAE features in comparison to the CORAAL AAE human baseline ($\mu=0.18$), they viewed both as displaying features indicative of AAE. In contrast, annotators \emph{disagreed} that the original Tweets ($\mu=-0.57$) contained AAE features unlike the model-generated-Tweets, where all models were rated more highly, particularly GPT ($\mu=0.99$).

Similar to the judgment on AAE feature presence, annotators generally agreed for the third linguistic judgment, abbreviated \emph{Black Sounding}, that the CORAAL suffixes read like something a Black American would say, with slight agreement for the human texts at $\mu=0.39$ and stronger agreement for the model-generated text across systems, where GPT had the highest mean $\mu=1.01$ that was also statistically different from the human baseline text. Again, the Twitter human texts ($\mu=-.30$)  were judged to \emph{not} sound like something a Black American would say whereas annotators agreed that the model-generated Twitter-continuations resembled something a Black American might say. $\mu=0.37$ for Mixtral and $\mu=0.79$ for GPT showed a statistical difference from the human baseline Tweets.

The last three linguistic judgments annotators were asked to assess would be less favorable  if Black Americans agreed with them for the AAE texts; these included: 1)  the text sounded like something a White American would say, 2) the text could be perceived as mocking how some Black Americans speak, and finally 3) the text could be construed as offensive coming from a chatbot. Likert ratings from these judgments of the AAE texts were largely on the ``disagree'' side ($\mu< 0$).

For our fourth linguistic judgment -- \emph{White Sounding}, Annotators felt strongly that the highlighted AAE suffixes \emph{did not} sound like something a White American would say. All scores, whether for the AAE human baselines ($\mu=-0.23$ for CORAAL and $\mu=-0.65$ for Tweets) or for the model-generated AAE continuations, were in the disagreement range, with GPT showing a statistically different mean of $\mu=-0.83$ for CORAAL generations relative to the CORAAL human text. 

For the fifth linguistic judgment \emph{Mocking}, annotators disagreed for both human (with $\mu=-0.88$ for CORAAL and $\mu=-0.79$ for Tweets) and most model-generated AAE (ranging from $\mu=-0.74$ for CORAAL Mixtral generations to $\mu=0.13$ for Llama Tweets generations), that the texts sounded like someone making fun of the way some Black Americans speak. The only statistical difference in means relative to the Tweets-human baseline was for Llama generations; for these, annotators slightly agreed that the model generations could be perceived as making fun of the way some Black Americans speak. Annotators more strongly disagreed with this judgment regarding the MUSE human and model-generated texts, perceiving both as not like mocking Black Americans and assessing them equivalentally in their Likert scores.

%\hal{TODO:make sure it's explained somewhere WHY we're doing a MUSE analysis here:we want to ensure that we have a point of comparison for the numbers we see on AAE because they might be difficult to make sense of on their own}

For our final linguistic judgment, annotators generally disagreed that they would be offended by either the CORAAL or Tweets human or model-generated texts with an exception for Tweets, where they felt neutral that the model-generated text for Llama would be offensive ($\mu=-0.07$) relative to the human posts which they disagreed ($\mu=-0.96$) would be offensive. They yet more strongly disagreed that the MUSE human or model-generated texts could be interpreted as offensive, judging them roughly equivalent in Likert scores.

For all six judgment types for the human to model comparisons, most differences in means %\hal{between Xand Y} 
between a given corpus human baseline and an LLM generation for that corpus were not statistically different from zero except those mentioned for the AAE corpora. Model-generated MUSE was not statistically different from the human MUSE.

To clarify, the NPR corpus was included as a proxy for MUSE, the predominant version of English for communicating in the United States, to facilitate understanding judgments of the AAE texts relative to MUSE. We would expect LLMs to be proficient in the production of MUSE, and it is understood that LLMs in the United States default to MUSE. While the human-to-model comparisons within each AAE corpus could provide insight into how well an LLM generates AAE, given examples of it, we also must assess ratings of AAE texts relative to the NPR MUSE baseline.

In general, as one might expect, our study’s Black American annotators agreed most that the MUSE human baselines and model generations had the most coherent continuations, were most White American sounding, and disagreed that the MUSE texts contained AAE features and sounded like something a Black American would say. Conversely, the AAE continuations were considered to be most like something a Black American would say and as having AAE features, but it is worth noting that Black Americans also felt that the MUSE continuations could sound like something a Black American might say. This suggests that while AAE is distinct and should be treated as such, Black Americans speak a wide range of English and these findings highlight Black Americans' bidialectal nature, also previously highlighted in \autoref{sec:related_works} where we discuss Black Americans' needs to code-switch and dialect prejudice in LLMs.
\begin{figure}[h]
\noindent\fbox{%
    \begin{minipage}{1\columnwidth}
    \scriptsize
   \textbf{Prefix}\newline Interviewer: [Right, right. <laugh>] Interviewee: You know, a lot of people got their Archie Bunker [and are k- in Queens.], and lots of drugs. So it becam

\textbf{Suffix:} \newline  \hl{real hard fo' me tuh concentrate, ya feel me? But ah managed tuh keep mah grades up, even though ah wuz livin' in dat kinda environment. Ah mean, education wuz always important tuh me, so ah made su' tuh prioritize it, no matter whut else wuz happenin'. Dat's why ah say - it ain' jus 'bout where yuh from, it's 'bout where yuh want tuh go in life. An' ah knew ah wanted tuh go far, ya dig?}

\textbf{Response Generated by:} Mixtral-8x7B

\textbf{Linguistic Judgment:} The highlighted text sounds like something someone would say making fun of the way some Black Americans speak.

\textbf{Annotation: } 2 - Strongest Agreement
    \end{minipage}
}

 %\hal{can we point back to the related work section for this?}



\noindent\fbox{%
    \begin{minipage}{1\columnwidth}
    \scriptsize
   \textbf{Prefix}\newline Interviewer: Yeah. So do you like the school that, uh, /RD-NAME-2/'s at right now? Interviewee: [Mm-hm.] Oh yeah, that's what I told her, I said- like we were looking at the rental house. She's like, oh my god. I 

\textbf{Suffix:} \newline  \hl{I said, "Girl, this ain't nothing." I mean, she thought it was rough, but I grew up in some tough neighborhoods too, you feel me? I told her, "You gotta see the beauty in it," 'cause once you start seeing the beauty, you start appreciating the struggle. And that's what makes us strong, you know what I'm saying? We came from nothing, built ourselves up, and now we're trying to build our own empire, you hear me? So yeah, I love that school, I love what it represents, resilience, perseverance... that's the DC way}

\textbf{Response Generated by:} Meta-Llama-3-70B-Instruct

\textbf{Linguistic Judgment:} The highlighted text sounds like something someone would say making fun of the way some Black Americans speak.

\textbf{Annotation: } 2 - Strongest Agreement

    \end{minipage}
    
}


\noindent\fbox{%
    \begin{minipage}{1\columnwidth}
    \scriptsize
   \textbf{Prefix}\newline Interviewer: Yeah. So do you like the school that, uh, /RD-NAME-2/'s at right now? Interviewee: [Mm-hm.] Oh yeah, that's what I told her, I said- like we were looking at the rental house. She's like, oh my god. I 

\textbf{Suffix:} \newline  \hl{was like, "Girl, you ain't seen nothin' yet!" I love the vibe there, real good community feel, and the teachers, they be lookin' out for the kids. I told her, "You gon' thrive here, no doubt!" Plus, the after-school programs, they got a little somethin' for everybody‚ music, sports, you name it. It's a place where they celebrate our culture and they encourage the kids to express themselves. That's important, you know? I just want her to soak it all up and enjoy the ride. This is a whole new chapter for us, and I'm just tryna make sure she gets the most outta it!}

\textbf{Response Generated by:} GPT 4o-mini

\textbf{Linguistic Judgment:} I would be offended if a chatbot said something like the highlighted text.

\textbf{Annotation: } 2 - Strongest Agreement
    \end{minipage}
}
\caption{Examples of response continuations generated by Mixtral, Llama, and GPT, with annotation scores based on human participants’ linguistic judgments.}
    \label{fig:linguistic_examples}
\end{figure}
%In general, study participants found the highlighted text suffixes that all three LLMs produced to be relatively coherent continuations of the previous text; all average scores were in the slight agreement range from {GPT}= 0.31 to  {Llama}= 0.57. Llama and Mixtral ($\mu = 0.56$) were seen to have suffixes more coherent than the human baseline ($\mu=0.2$).

%Participants found Llama, in particular, to perform better in generating AAE across the more favorable linguistic judgment types than our human baseline -- specifically participants expressed minor agreement (mean Likert scores between a neutral score of 0 and an agreement score of 1) that the Llama highlighted suffixes were more coherent ($\Delta = 0.37$), showed greater presence of AAE features ($\Delta = 0.47$), and were more authentic AAE ($\Delta = 0.39$). All three Llama results showed a statistically significant difference from our human baseline at the 1\% confidence level. 

%Similarly, the Mixtral model also stood out as having several statistically significant differences from our human baseline, namely its generations were seen as more coherent than the human baseline ($\Delta=0.36$), less mocking of Black Americans ($\Delta = -0.23$), and more White-sounding ($\Delta=0.22$).

%The GPT model and our human baseline both scored relatively lower for the more favorable linguistic judgment categories in terms of their mean Likert scores, though GPT was not statistically different from CORAAL and therefore comparable for coherency of its generations (with low to neutral to agreement scores of \emph{GPT mean} = 0.31 versus \emph{human mean} = 0.20), its presence of AAE features {\emph{human mean} = -0.37 versus \emph{GPT mean} = -0.51}, and the authenticity of AAE in its generations (\emph{human mean}= -0.04 versus \emph{GPT mean}=-0.05).

%No single system (not even the human baseline of Black American transcribed speech) stood out as being particularly representative of AAE features -- in fact,  participants generally disagreed or felt neutral that the texts displayed features indicative of AAE (Likert scores ranged from the lowest disagreement with $\mu=-0.51$ for GPT to a high of neutral agreement for Llama with $\mu=0.10$).

%Participants were generally neutral about whether the suffixes were authentic AAE, or specifically something a Black American would say. Likert scores ranged from $\mu=-0.05$ for GPT to a high of $\mu=0.35$ for Llama, with the latter being the only result having a statistically significant difference from the human baseline for authenticity. 

%In contrast, study participants felt slightly more strongly that the highlighted suffixes \emph{did not} sound like something a White American would say -- all scores were in the disagreement range (negative) and were further from neutral (ranging from $\mu$ of -0.17 to -0.46) than for the previous question asking if it sounded like something a Black American would say (which tended to be close to neutral). Even so, they were still closer to neutral (0) than to outright disagreement (-1) on our scale.

%\paragraph{Offensiveness of Generated Text}
%Regarding the less favorable linguistic judgment types of whether the text could be perceived as mocking AAE or offensive, generally study participants disagreed (scores were close to -1) that the highlighted suffixes could be interpreted as mocking the way some Black Americans speak ($\mu$ from -0.81 to -1.05). Only Mixtral showed statistical difference from our human baseline, suggesting that people more strongly disagreed that Mixtral-generated highlighted suffixes could be conceived of as mocking versus the human baseline; again both values were in the disagreement range regarding the question, with the $\mu=-0.88$ for our human baseline.

%Participants also disagreed that the highlighted texts were offensive, with scores again being definitively closer to -1 across the board for all the LLMs and our human baseline ($\mu$ from -0.89 to -1.2). GPT and Mixtral showed statistical difference from the baseline for this category, at $\mu=-1.06$ and $\mu=-1.20$ respectively, suggesting that participants slightly perceived GPT and Mixtral texts as less offensive than the human baseline ($\mu = -0.89$).
%\subsubsection{Analysis of Insignificant Differences}
\subsubsection{How Black Americans Perceived AAE versus MUSE texts} \label{sec:llmgenmuse}

\begin{table*}[t]
    \centering
    \footnotesize
    \renewcommand{\arraystretch}{0.85} % Reduce row height
\setlength{\tabcolsep}{4pt}
    %\resizebox{\textwidth}{!}{%
    \begin{tabular}{lcccccccccccccccccc}
    %{l@{\hspace{0.5cm}}cc@{\hspace{1cm}}cc@{\hspace{1cm}}cc@{\hspace{1cm}}cc@{\hspace{1cm}}cc@{\hspace{1cm}}cc}
        \toprule
        \multirow{2}{*}{\textbf{System}} 
        && \multicolumn{2}{l}{\textbf{Coherent}} 
        && \multicolumn{2}{l}{\textbf{AAE Feats}} 
        && \multicolumn{2}{l}{\textbf{Black-Snd}} 
        && \multicolumn{2}{l}{\textbf{White-Snd}} 
        && \multicolumn{2}{l}{\textbf{Mocking}} 
        && \multicolumn{2}{l}{\textbf{Offensive}} \\
        % \cmidrule(lr{0.5cm}){2-3} \cmidrule(lr{0.5cm}){4-5} \cmidrule(lr{0.5cm}){6-7} \cmidrule(lr{0.5cm}){8-9} \cmidrule(lr{0.5cm}){10-11} \cmidrule(l){12-13}
        && \textbf{Co} & \textbf{Tw} && \textbf{Co} & \textbf{Tw} && \textbf{Co} & \textbf{Tw} && \textbf{Co} & \textbf{Tw} && \textbf{Co} & \textbf{Tw} && \textbf{Co} & \textbf{Tw} \\
        \midrule
        Human   && **  & **  && **  & -  && -  & -  && ***  & ***  && *  & **  && **  & - \\
        GPT     && -  & ***  && ***  & ***  && ***  & *  && ***  & ***  && ***  & ***  && ***  & *** \\
        Llama   && **  & ***  && ***  & ***  && *  & -  && ***  & ***  && ***  & ***  && ***  & *** \\
        Mixtral && -  & ***  && ***  & ***  && **  & -  && ***  & ***  && ***  & ***  && ***  & *** \\
        \bottomrule
    \end{tabular}%
    %}
   \caption{Statistical significance indicated for 48 between-corpus comparisons of mean Likert scores ($n$ ranged from 119 to 126 Likert score observations for a given sample in a two sample comparison), where values shown resulted from each t-test comparing an AAE corpus versus the MUSE corpus of NPR Interviews, latter not labeled. We indicate Linguistic judgment and the AAE Corpus (CORAAL or Tweets) for each t-test. $p<0.05$ is marked with *; $p<0.01$ with ** and $p<0.001$ with ***.}
    \label{tab:combined_table_with_vertical_headers}
\end{table*}

The previous results showed, roughly, that the LLM-generated AAE was on par with human-written AAE across many linguistic axes.
%To review, our results comparing the original human continuations (suffixes) to model-generated continuations for each corpus primarily revealed how much the annotators agreed (or not) with each of the linguistic judgments for the AAE texts (or MUSE as a point of reference) where agreement was indicted by Likert score  means. However, statistically significant differences in means (scores on human texts versus model generated-texts) were few which indicated to us that the LLM generations in general were perceived as mostly equivalent in quality of AAE or MUSE to the human-originated texts. 
Thus, we seek to ensure that the models are actually generating AAE (versus MUSE) when prompted to do so, that the human text was actually AAE (versus MUSE), \emph{and that people could tell the difference}. To answer this question, we conduct between-corpus tests of \emph{AAE to MUSE} as seen in \autoref{tab:combined_table_with_vertical_headers} (our previous tests only compared human to model-generated texts within one corpus at a time).

Similar to the previous analysis, we compared two sample means at a time with t-tests, one from an AAE corpus and another from the MUSE corpus (e.g., CORAAL AAE human baseline compared to MUSE human baseline, or Twitter GPT continuation compared to MUSE GPT continuation). In contrast, in testing between two dialects, MUSE and AAE, we expected the alternative hypothesis -- a non-zero difference in mean Likert scores between the two samples -- to be true.

\autoref{tab:combined_table_with_vertical_headers} results show that across our set of linguistic judgments (as seen in \autoref{table:Label Descriptions}), when we compare MUSE against CORAAL or against Tweets, virtually all between MUSE and AAE comparisons show a statistically significant difference in means; in other words, Black Americans perceived the AAE human-originated texts and the model-generated texts as distinct from the MUSE texts. The statistical significance of the comparisons of MUSE versus CORAAL, or MUSE versus Tweets, Likert scores for each linguistic judgment can be seen in the columns, alternating within each judgment between CORAAL versus Tweets.
%For the AAE generations in both sets of tests, we consider higher scores to be preferable for the first three judgments (coherency, presence of AAE features, and Black American-sounding) and lower scores to be preferable for the last three judgments (White American-sounding, mocking, and offensive).

%Our Black American Annotators agreed that MUSE text continuations were more coherent than AAE text continuations for our first linguistic judgment. Their ratings indicated a lack of meaningful differences between AAE corpora. Additionally, ratings were not statistically different for GPT model-generations between CORAAL AAE continuations and MUSE continuations, but they were for GPT model-generations for Tweets and MUSE. The continuations generated by the GPT, Llama, and Mixtral models were considered more coherent for MUSE than for AAE Tweets. The MUSE human baseline was considered more coherent than either the CORAAL AAE or Tweets AAE human baselines.

%For the second linguistic judgment annotators were asked to make, unsurprisingly, they disagreed that the MUSE continuations contained text indicative of AAE, and they agreed that CORAAL and Tweets continuations were indicative of AAE, not indicating much difference in their ratings between continuations from these two corpora.

%Annotators agreed more that the AAE texts sounded like something a Black American would say relative to MUSE (our third linguistic judgment), slightly agreeing that MUSE sounded like something a Black American would say. They found the CORAAL corpus to be a bit more representative of something a Black American would say than the Tweets corpus, but found both to be representative of how Black Americans might say something.

%For the fourth judgment, annotators agreed that the MUSE text sounded like something a White American would say relative to the AAE continuations, ranging in their ratings from slightly disagreeing to disagreeing that the AAE continuations sounded like something a White American would say. They disagreed slightly more for Tweets versus for CORAAL.

%Annotators disagreed more that the MUSE text sounded like someone making fun of Black Americans (our fifth linguistic judgment), than did the AAE continuations, but disagreed for both types of continuations that they sounded like someone making fun of Black Americans. They disagreed a bit more that the CORAAL continuations sounded like someone making fun of Black Americans, versus slight disagreeing with this judgment for Tweets.

%For the sixth and final judgment, Annotators strongly disagreed (more than for other corpora) that the MUSE continuations would be offensive than for AAE text, but they disagreed for both the AAE and MUSE continuations that they would be offensive. They felt about the same for the two AAE corpora --  CORAAL and Tweets.


\subsection{Discussion of Linguistic Judgments}

For transparency, we initially had relatively low expectations for the capabilities and performance of the text-based generative AI models with respect to AAE, given the issues we outlined regarding AI technologies' abilities to understand or process spoken language (\autoref{sec:related_works}), as well as the likelihood that AAE has a distinct minority representation in the training corpora for these systems. Therefore, one of our most noteworthy results is contrary to our expectations: that LLMs in general performed similarly to our human baseline, and in some cases were actually seen as containing more AAE features or sounding more like something a Black American might say than our human baseline AAE texts. The suffixes for all three LLMs were judged being more coherent or easier to understand and flow better from the prefix than the human baseline. Furthermore, it is encouraging that the suffixes generated by LLMs were on average judged to be inoffensive, not mocking of Black Americans, and not White Sounding. However, ``on average'' does not mean that none of the outputs were problematic. 

As seen in \autoref{fig:linguistic_examples}, there may be a minority of cases where generative AI may produce text containing AAE that is undesirable for any number of reasons (whether because of the nature of the AAE in it, the text content or other factors), and different people may respond negatively or positively to the same text given inherently varying perspectives. In the worst cases, AAE generated could perpetuate stereotypes, mock Black Americans, or otherwise generate inauthentic AAE. However, we have shown that LLM systems generally do not seem to be doing that, and we believe that there are meaningful and highly impactful benefits to be gained from the generation of AAE in popular language-based technologies, such as the increased representation of Black American expression through AAE and the promotion of inclusivity and improved quality of service for them as stakeholders. In our opinion, these gains would outweigh the risks, and extrapolation of our results (see \autoref{tab:coherence_aae_feats} and \autoref{fig:heatmap}) would indicate that the large population of Black Americans (\autoref{sec:intro}) would support this, preferring AAE in more informal contexts but wanting the autonomy to choose between AAE and Mainstream U.S. English (MUSE). In fact, our approach may be generalized to hundreds of dialects of English to verify and promote their acceptance in LLM products. Please see our motivation for dialectical diversity in \autoref{sec:related_works}. If Black Americans engage more directly in AAE, when given the choice, there would be limited risk of the AAE generations by LLMs becoming more artificial since LLMs already produce credible AAE. With a greater number and variety of Black American users to train LLM systems, production of AAE will increasingly meet Black Americans’ expectations. Of course, AAE could have negative impacts if the LLM-generated AAE is inconsistent with Black American user preferences for AAE. However, in our survey, Black Americans unequivocally expressed that there are contexts in which they are interested in AAE generations, wanting the freedom to choose this as desired. Thus, with appropriate safeguards to avoid offensive or mocking text generation, these risks could be well-mitigated.

%\subsubsection{Annotator Reliability}
% \begin{table}[!ht]
% \centering
% \resizebox{\linewidth}{!}{ 

% \begin{tabular}{@{}l|llllll@{}}
% \toprule
%                  & \multicolumn{6}{c}{\textbf{Inter-annotator Agreement}} \\
% \midrule
%                  & \textbf{Label 1} & \textbf{Label 2} & \textbf{Label 3} & \textbf{Label 4} & \textbf{Label 5} & \textbf{Label 6} \\
% \midrule
% Human     & 0.11& 0.19& 0.17& 0.18& -0.003& 0.22\\ \midrule
% GPT              & 0.07& 0.30& 0.26& 0.16& -0.03& 0.1\\ \midrule
% Llama            & 0.07& 0.26& 0.02& 0.05& -0.04& 0.23 \\ \midrule
% Mixtral          & -0.12& 0.17& -0.005& 0.21& -0.04& 0.16\\
% \bottomrule
% \end{tabular}
%  }
% \caption{
% Overall inter-annotator agreement with \cite{krippendorff2018content} Krippendorff’s $\alpha$ grouped by Condition and Label. 
% }
% \label{tab:baselines}
% \end{table}

% In our study, we used Krippendorff's \textit{ $\alpha$} to evaluate the inter-annotator agreement on a \textit{Likert scale} of -2 to 2 for the highlighted text across the different labels detailed in section~\ref{sec:labels}. The labels, or questions answered about the text, helped us to understand how study participants perceived AAE produced by the LLMs in our study. The results varied significantly, demonstrating the complexity of reliably interpreting language characteristics. 
% Notably, the coherence of text (Label 1) demonstrated high inter-annotator agreement, such as a score of 0.78 for Llama, indicating a clearer consensus among annotators, in contrast to the features indicative of AAE (Label 2), which showed more variability with notably low or even negative agreement scores, such as -0.30 for GPT, suggesting subjective interpretations among annotators and possibly a need for clearer guidelines. 
% Conversely, higher agreement was observed in label5 and 6 assessing the authenticity of White American English,with perfect agreement (score of 1.00) for Llama in Label 5 and scores like 0.95 for Mixtral and GPT in Label 6.
% These labels, concerning the authenticity and offensiveness of the text, likely invoked stronger and more definite responses due to their sensitive nature, compelling participants to make judgments on personally impactful matters such as cultural authenticity and potential offense. These discrepancies in inter-annotator agreement underline the importance of refining training and annotation protocols, along with the development of questions which holds meaning for participants, to enhance the reliability and interpretability of data in studies focusing on linguistic and cultural nuances.

% The variability in Krippendorff's \textit{$\alpha$} scores across the different labels may be attributed to several factors.  Notably, the features indicative of African American English (Label 2) demonstrated low agreement scores, such as -0.30 for GPT, suggesting that these judgments can be highly subjective, influenced by individual annotators' experiences and familiarity with AAE. This subjectivity points to a need for more comprehensive training and clearer annotation guidelines. Conversely, the higher consistency observed in labels like White American English authenticity and offensiveness likely reflects more universally recognized linguistic features, social norms, and otherwise socially meaningful questions, which reduce subjective interpretation. Enhancing annotator training to cover cultural sensitivities and linguistic nuances, and ensuring a diverse pool of annotators, may help mitigate these disparities and improve the overall reliability of annotations in capturing complex language characteristics.