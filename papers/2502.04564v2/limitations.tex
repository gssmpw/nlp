\paragraph{Data Limitations.} As in most data annotation tasks, we were limited by the data available to us. We chose to work with a corpus of transcribed interviews of Black Americans from the reputable CORAAL online repository to represent authentic AAE to our best ability. We chose this corpus because it was based on conversation and not written language (AAE is most commonly spoken), was informal and likely to have more AAE than most transcribed interviews due to the fact that interviews were often amongst acquaintances including friends and community members, and the corpus was rich in  regional variation having both male and female interviewees represented. Even so, there were many cases of annotations in the exchanges labeled by our study participants (for punctuation or laughter, for example) which may have seemed awkward or confusing. These annotations were made by the CORAAL data stewards via the transcription and editing process, and could have influenced how authentic or coherent the text was perceived. Furthermore, there was a great range in the extent of AAE linguistic features we observed in the transcribed speech between interviews; given this, even though we randomly sampled which exchanges would be labeled by annotators, some annotators may have been exposed to more or less AAE in the exchanges than others.

\paragraph{Researcher positionality.} 
When this manuscript was drafted, one author self-identified as a bicultural White American Latina female who does not speak AAE,
one identified as an African female who does not speak AAE,
one identified as Black African male who is familiar with AAE,
one identified as a Kashmiri male who does not speak AAE,
and one identified as a White male who does not speak AAE. 
Our background and positionality has limited our direct, personal understanding of Black American preferences for how generative AI technologies should perform regarding AAE. We aimed to mitigate our limitations by soliciting feedback from Black Americans in the pilot phase of the survey, and also by limiting our survey study respondents to only Black American adults.
%In addition, we acknowledge our personal limitations as researchers who are not Black Americans ourselves; our understanding of Black American preferences for how generative AI technologies should perform regarding AAE production, as well as the many nuances of AAE as a language itself are limited. We tried to mitigate these limitations by soliciting feedback from Black Americans in the pilot phase of the survey and also limited our survey study respondents to only Black American adults based on the Prolific Platform's sample development service through which we requested this specific population.

\paragraph{Participant Limitations.}
All languages including AAE are complex, and by having annotators label single pairs of statements, certainly some of the nuance of language is lost that might otherwise be present in a full dialogue between Black American speakers. Furthermore, while our annotator pool consisted of a relatively diverse set of Black Americans, they tended toward more educated and may have been unrepresentative in other ways we did not measure (as is typical for online crowdsourced studies). Our IRB also restricted our reporting on study participant and annotator demographics other than at the aggregate level. To ensure statistically significant insights, we focused our analysis on the groups of participants as a whole. 

\paragraph{LLM Limitations.} 
Finally, the LLM generations that resulted from our prompting, no matter how careful, are inherently limited by the text corpora upon which they are trained. Our ability to ``get the LLM to use AAE'' is limited by our ability to prompt the models well; it is possible---indeed likely---that alternative prompts would lead to substantially different results.


