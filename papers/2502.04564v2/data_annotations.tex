%i would reorganize as:

%1. We want to collect judgments of AAE naturalness on LLM-produced text.
%2. We conduct this by showing people a piece of text where the prefix is transcribed human AAE speech and the suffix is LLM-generated.
%3. We then ask people XYZ about that suffix.
%4. The prefixes come from this dataset.
%5. The LLMs are prompted as follows.

%probably each of these is one paragraph.



\begin{table}[t]
\centering
\footnotesize
%\rowcolors{2}{gray!10}{}
\renewcommand{\arraystretch}{0.85} % Reduce row height
\setlength{\tabcolsep}{4pt}
\begin{tabular}{ @{~}l @{~~~} p{55mm}@{~}}
  \toprule	
    % & \textbf{Linguistic} \\
  \textbf{Description} & \textbf{Linguistic Judgment} \\
  % \textbf{Linguistic} &  \\ 		
  % \textbf{Judgment} & \textbf{Question} \\ 		
  \midrule			
\textit{Coherent}	&	The text is a coherent
continuation.	\\
\textit{AAE Features}	&	The text contains AAE features. \\
\textit{Black Sounding}	&	The text sounds like something a Black \\&\quad American would say.\\
%\rowcolors{0}{}{gray!10}
\textit{White Sounding}	&	The text sounds like something
a White\\&\quad American would say.\\
\textit{Mocking}	&	The text is like someone mocking AAE.\\
\textit{Offensive}	& I would be offended if a chatbot said this. \\
  \bottomrule			
\end{tabular}	
\caption{Assessments made via Likert score rating by Black American annotators regarding the AAE and MUSE text continuations they reviewed. The continuations were either human or LLM-produced, but annotators were not told which.} %; their judgments were recorded on a five point Likert scale ranging from ``Strongly Disagree'' to ``Strongly Agree.''} % The suffixes were generated by an LLM (GPT, Llama, or Mixtral) or were original CORAAL or NPR interviewee statements, or Tweets (Twitter).}%\kac{Updated with comments from Hal/Sandra}} %, and were continuations of the preceding text, which consisted of the original interviewer statement plus half the original interviewer response from the CORAAL interviews.}
\label{table:Label Descriptions}
\end{table}
% Change column labels:
% column 1 header: Description, column 2 header: Linguistic Judgment.

% Change Authentic in column 1 to Black Sounding and change it's entry in  column 2  to "The text sounds like something a Black American would say."

% Caption -- "These were the ratings that participants gave..."

We used text annotation to explore our second research question: how well do current LLMs generate AAE-like text (including relative to generation of MUSE-like text). We obtained human judgments by our Black American annotators as to how they perceived LLM-produced AAE text relative to their expectations for AAE. 
We show participants text, including a prefix which was transcribed (human) speech from interviews or from X (Twitter) posts, paired with a suffix which was either the natural (human) continuation or an LLM-generated continuation (by GPT, Llama, or Mixtral; see \autoref{fig:linguistic_examples} for examples).
In some cases the human (and LLM-generated) text is AAE; in others it is MUSE.
We adopt this \textit{continuation} methodology to ensure that that LLM generations are comparable---similar topic, etc.---to the human generations to facilitate more controlled comparisons.
%Specifically, the prefix included an interviewer statement plus the first half of the corresponding interviewee response. The second half of the interviewee response, or the suffix, was derived either via machine-generation (by one of the three LLMs -- GPT, LLAMA or Mixtral), or was the ending to the original interviewee statement. 
In the annotation task, the suffix was highlighted and participants were asked to assess it along the six dimensions from \autoref{table:Label Descriptions} on a five-point Likert scale from ``Strongly Disagree'' to ``Strongly Agree''
(the full interface annotators saw is shown in \autoref{fig:annotation_task} in the Appendix, and see \autoref{fig:linguistic_examples} for examples with annotations).
Participants were unaware if the suffix was human or machine-generated. %The six questions participants answered about the suffix can be seen in \autoref{table:Label Descriptions}. Their response choices (i.e. the annotations on the highlighted text) were recorded on a five-point Likert scale.

 The AAE and MUSE prefixes and human baseline texts for this annotation task were drawn from the Corpus of Regional African American Language (CORAAL) \cite{KendallFarrington2023}, which is a corpus of transcribed interviews of Black Americans, an X posts (Twitter) AAE corpus \cite{aave_corpora}, and a National Public Radio interview corpus containing MUSE text \cite{majumder2020large,majumder2020open} %\kac{Update with more details}. %We first obtained CORAAL in order to have a human baseline of AAE for our studay. This enabled us to compare the human judgments or ratings on the baseline (with respect to authenticity of AAE, etc.) to those on the LLM-produced texts. 
 Hereafter, we refer to X posts as Tweets for brevity.
 We lightly cleaned the corpora for annotator assessments (see \autoref{apdx:corral_desc}). %We discuss below our process for developing prompts to the LLMs for the machine-generated suffixes.

\subsection{Prompting LLMs}

To produce the LLM suffixes, for each prompt, we provided the following inputs to the LLMs to encourage the systems to generate completions of the interviewee statements in AAE for the CORAAL interview and X post (Twitter) prefixes, or in MUSE for the NPR interview-sourced prefixes: 1) an instruction to generate a completion (the suffix) \textit{in AAE or in MUSE} with a suggested response length limit; 2) a list of 2-5 randomly sampled (and randomly varying in number of) training examples of authentic AAE (or MUSE for those continuations) for in context learning~\cite{brown2020language} -- the AAE training examples were drawn from other CORAAL interview exchanges or other AAE Tweets, including both the interviewer statements and the corresponding interviewee responses, or the original Tweets; and 3) a test prefix as described above, that the LLM then was tasked to continue in AAE (or MUSE for the texts originally from NPR interviews). See \autoref{apdx:gtp} for details and \autoref{apdx:prompts} for examples of the prompt templates used.





