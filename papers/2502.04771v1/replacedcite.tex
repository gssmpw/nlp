\section{Background and Related Work}
This section provides an introduction to the security problem in DFL and a summary of the current research on model poisoning attacks in FL.
\subsection{Security Problem in DFL}
Differing from traditional CFL, neighboring clients in DFL exchange local model parameters or gradients over a P2P network and construct consensus models independently. DFL solves the problem of risk associated with a single point of failure, enhances its scalability, and is well suited for applications in the Industrial Internet of Things (IIoT) ____. However, the decentralized nature of DFLs rather increases their risk of being exposed to malicious attacks, especially poisoning attacks____. Poisoning attacks, which aim to diminish the resilience of FL models, can be classified into data poisoning and model poisoning attacks. Data poisoning attacks typically involve malicious clients interfering with the training process by introducing harmful data (such as backdoor attacks or label flipping) into the local training dataset, thereby inducing biased learning outcomes ____. Conversely, model poisoning attacks are characterized by malicious clients directly altering their local model parameters and affecting the global model training via harmful model updates. To improve the model robustness of FL and defend against the poisoning attacks, ____ proposed the use of averaging model parameters to improve training efficiency. ____ introduced the Krum method, which excludes malicious updates by calculating the Euclidean distance of a multi-node model and selecting the model with the smallest distance. This method reduces the impact of malicious attacks and is now widely used for robust aggregation in DFL.
____ developed two robust distributed learning algorithms for Byzantine errors and potential adversarial behavior, a robust distributed gradient descent algorithm based on Median and Trimmed Mean operations, respectively. These two methods are widely used in both CFL and DFL because they can be implemented with only one communication round and achieve optimal model robustness.

\subsection{Model Poisoning Attack}
Compared with data poisoning attacks, model poisoning attacks are easier to execute as they directly modify the shared model. Therefore, there has been research suggesting how to optimize the attack method to enhance the attack effectiveness. Existing model poisoning attacks mainly target CFL. The attack assumes that the attacker has access to a large number of compromised clients. During the training process, malicious clients send carefully designed local model updates to the server, resulting in problems such as a decline in the accuracy of the global model ____. The MIN-MAX and MIN-SUM methods proposed by ____ respectively minimize the maximum and sum of the distance between toxic updates and all benign updates in CFL. These methods assume that malicious model updates are the sum of aggregated model updates and a fixed disturbance vector scaled by factors before the attack, and implement the attack by maximizing the distance between toxic updates and benign updates in the inverse direction of the global model. 
%These methods show remarkable results in traditional federated learning, and because they do not rely on training deep learning models, they require fewer computational resources, making them useful in FL's model poisoning attacks.
The "a little is enough" (LIE) attack ____ directly modifies the model parameters, which generates malicious model updates in each training round by calculating the average of the real model updates of the malicious client and perturbing them. %In a DFL system, it is assumed that an attacker has a set of malicious client models, and although these malicious clients may not be directly connected, they can still use these participants to implement model poisoning attacks. Therefore, this paper will try to apply the above methods to DFL system and study its attack logic to help explore the model poisoning attack in DFL.

Most model poisoning attacks rely on additional information from the central server, such as aggregation methods, global models, or even the training data utilized by nodes. This dependency renders external attacks impractical. Nevertheless, some attacks are engineered to maintain efficacy even in the absence of such additional knowledge. For instance, ____ employs global historical data to build an estimator that forecasts the subsequent round of global models as a benign reference. Despite this, the approach necessitates substantial resources and encounters difficulties in accessing the global model within DFL systems, thereby limiting its practical application in DFL.

To conclude, while a substantial amount of research has been dedicated to optimizing model poisoning attacks against the FL, these efforts predominantly concentrate on the CFL paradigm, with minimal attention given to the DFL paradigm. Furthermore, the current attack methodologies exhibit limitations, such as inconsistent efficacy and susceptibility to detection by defensive mechanisms. To address these research gaps, this paper presents the design and implementation of a novel attack strategy tailored for DFL. This proposed attack demonstrates broad effectiveness across various datasets, diverse ML model architectures, and multiple types of DFL overlay topologies.