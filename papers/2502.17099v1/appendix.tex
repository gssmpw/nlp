\section{Proofs in Section \ref{sec:diffusion model as multi-step}}\label{app:proofs in sec:diffusion model as multi-step}
In this section, we present the proofs of the results in Section \ref{sec:diffusion model as multi-step}. 

\subsection{Proofs in Section \ref{sec:Distributional Robustness in DPM}}
\elboupperbound*
\begin{proof}
	We prove the first equivalence, by Jensen's inequality. For any $0 \leq t < T$, we have 
	\begin{equation}
    \small
		\begin{aligned} 
            & -\mE_{q}\left[\log{p}_{\btheta}(\bx_{t})\right]\\
            \leq &\mE_{q}\left[-\log{\frac{p_{\btheta}(\bx_{t:T})}{q(\bx_{t+1:T}\mid \bx_{t})}}\right] \\
            =& \mE_{q}\left[-\log{p}_{\btheta}(\bx_{T}) -\sum_{t \le s < T } \log{\frac{p_{\btheta}(\bx_{s} \mid \bx_{s+1})}{q(\bx_{s+1} \mid \bx_{s})}}\right] \\
            =& \mE_{q}\left[-\log{p}_{\btheta}(\bx_{T}) -\sum_{t \le s < T } \log{\frac{p_{\btheta}(\bx_{s} \mid \bx_{s+1})}{q(\bx_{s} \mid \bx_{s+1})} \cdot \frac{q(\bx_{s})}{q(\bx_{s+1})}  }\right] \\
            =&  \mE_{q}\left[-\log \frac{p_{\btheta}(\bx_{T})}{q(\bx_{T}) } -\sum_{t \le s < T } \log{\frac{p_{\btheta}(\bx_{s} \mid \bx_{s+1})}{q(\bx_{s} \mid \bx_{s+1})} - \log q(\bx_{t})  }\right] \\
            =&  D_{KL}(q(\bx_{T})\parallel p_{\btheta}(\bx_{T})) + \mE_{q}\left[ \sum_{s = t}^{T-1}\underbrace{D_{KL}(q(\bx_{s}\mid \bx_{s+1}) \parallel p_{\btheta}(\bx_{s}\mid \bx_{s+1}))}_{L_{t}} \right] + H(\bx_t)
		\end{aligned}
	\end{equation}
	Taking $t = 0$, we prove the first equivalence. Besides that, the entropy $H(x_{t})$ of $\bx_t$ is a constant for $\btheta$ given data distribution $\bx_0$ for any $0 \leq t < T$. The second conclusion holds due to the non-negative property of KL-divergence.  
\end{proof}
\par
\elboupperboundxt*
\begin{proof}
We have the following decomposition due to the chain rule of KL-divergence
\begin{equation}\label{eq:decomp on kl}
    \small
    \begin{aligned}
        D_{KL}(q(\bx_{t}, \bx_{t + 1}) \parallel p_{\btheta}(\bx_{t}, \bx_{t + 1})) & = D_{KL}(q(\bx_{t}\mid \bx_{t + 1}) \parallel p_{\btheta}(\bx_{t}\mid \bx_{t + 1})) + D_{KL}(q(\bx_{t+1})\parallel p_{\btheta}(\bx_{t+1})) \\
        & = D_{KL}(q(\bx_{t+1}\mid \bx_{t}) \parallel p_{\btheta}(\bx_{t+1}\mid \bx_{t})) + D_{KL}(q(\bx_{t})\parallel p_{\btheta}(\bx_{t})),
    \end{aligned}
\end{equation}
The transition probability $p_{\btheta}(\bx_{t}\mid \bx_{t + 1})$ matches $q(\bx_{t}\mid \bx_{t + 1})$, so that the above equality implies 
\begin{equation}
\small
\begin{aligned}
    &D_{KL}(q(\bx_{t})\parallel p_{\btheta}(\bx_{t}))\\
    = &D_{KL}(q(\bx_{t+1})\parallel p_{\btheta}(\bx_{t+1})) + D_{KL}(q(\bx_{t}\mid \bx_{t + 1}) \parallel p_{\btheta}(\bx_{t}\mid \bx_{t + 1})) - D_{KL}(q(\bx_{t+1}\mid \bx_{t}) \parallel p_{\btheta}(\bx_{t+1}\mid \bx_{t}))\\
    \leq & D_{KL}(q(\bx_{t+1})\parallel p_{\btheta}(\bx_{t+1})) + \frac{\gamma}{T}.
\end{aligned}
\end{equation}
The proposition holds due to initial condition $D_{KL}(q(\bx_{T})\parallel p_{\btheta}(\bx_{T})) \le \gamma_0$ and simple induction.
\end{proof}

\gaussianinverse*
\begin{proof}
	Due to Bayes' rule, we have 
	\begin{equation}
		\small
		\begin{aligned}
			& q(\bx_{t} \mid \bx_{t + 1}) = \frac{q(\bx_{t + 1} \mid \bx_{t})q(\bx_{t})}{q(\bx_{t + 1})} \\
			\propto & \exp\left(-\frac{\left\|\bx_{t + 1} - \sqrt{\alpha_{t + 1}}\bx_{t}\right\|^{2}}{2(1 - \alpha_{t + 1})} + \log{q(\bx_{t})} - \log{q(\bx_{t + 1})}\right) \\
			\propto & \exp\left(-\frac{\left\|\bx_{t + 1} - \sqrt{\alpha_{t + 1}}\bx_{t}\right\|^{2}}{2(1 - \alpha_{t + 1})} + \left\langle\nabla_{\bx}\log{q(\bx_{t + 1})}, \bx_{t} - \bx_{t + 1}\right\rangle \right) \cdot  \\
            & \exp\left( \frac{1}{2}(\bx_{t} - \bx_{t + 1})^{\top}\nabla^{2}_{\bx}\log{q(\bx_{t + 1})}(\bx_{t} - \bx_{t + 1}) +  O(\|\bx_{t + 1} - \bx_{t}\|^{3})\right).
		\end{aligned}
	\end{equation}
	As can be seen, the conditional probability can be approximated by Gaussian only if $\nabla^{3}_{\bx}\log{q(\bx_{t+1})}$ is zero or $\|\bx_{t + 1} - \bx_{t}\|^{3}$ is extremely small with high probability. The two conditions can be respectively satisfied when $q(\bx_{t})$ is a Gaussian or $\bx_{t}$ close to $\bx_{t+1}$. 
\end{proof}


\subsection{Proofs in Section \ref{sec:Distributional Robustness in DPM}}\label{app:proofs in sec:Distributional Robustness in DPM}
\advelboupperbound*
\begin{proof}
   W.o.l.g., suppose $p_{\btheta}(\bx_{t}, \bx_{t + 1}) = p_{\btheta}(\bx_{t} \mid \bx_{t + 1})q(\bx_{t + 1})$ and $\tq(\bx_{t}, \bx_{t + 1}) = \tq(\bx_{t + 1}\mid \bx_{t})q(\bx_{t})$. By Jensen's inequality, we have  
\begin{equation}\label{eq:variational bound}
    \small
    \begin{aligned}
        & \mE_{q}\left[-\log{p_{\btheta}}(\bx_{t})\right] \\
        =& -\int q(\bx_{t})\left(\log{\int p_{\btheta}(\bx_{t}, \bx_{t + 1})}d\bx_{t + 1}\right)d\bx_{t} \\
        =&  -\int q(\bx_{t})\left(\log{\int \frac{p_{\btheta}(\bx_{t}, \bx_{t + 1})}{\tq(\bx_{t + 1}\mid \bx_{t})}\tq(\bx_{t + 1}\mid \bx_{t})}d\bx_{t + 1}\right)d\bx_{t} \\
        \leq &  -\int q(\bx_{t})\left(\int \log{ \frac{p_{\btheta}(\bx_{t}, \bx_{t + 1})}{\tq(\bx_{t + 1}\mid \bx_{t})}}\tq(\bx_{t + 1}\mid \bx_{t})d\bx_{t + 1}\right)d\bx_{t} \\
        = &   -\int q(\bx_{t})\left(\int \tq(\bx_{t + 1}\mid \bx_{t})\log{\frac{p_{\btheta}(\bx_{t} \mid \bx_{t + 1})}{\tq(\bx_{t + 1}\mid \bx_{t})}}d\bx_{t + 1}\right)d\bx_{t}\\
        & - \int q(\bx_{t})\left(\int \tq(\bx_{t + 1}\mid \bx_{t})\log{\frac{q(\bx_{t + 1})}{\tq(\bx_{t + 1}\mid \bx_{t})}}d\bx_{t + 1}\right)d\bx_{t} \\
        = & -\int \tq(\bx_{t}, \bx_{t+1})\log{\frac{p_{\btheta}(\bx_{t} \mid \bx_{t + 1})}{\tq(\bx_{t + 1}\mid \bx_{t})}} d\bx_{t} d\bx_{t + 1} + C_1\\
        = & -\int \tq(\bx_{t}, \bx_{t+1})\log{\frac{p_{\btheta}(\bx_{t} \mid \bx_{t + 1})}{\tq(\bx_{t}\mid \bx_{t + 1})}\cdot \frac{q(\bx_t)}{\tq(\bx_{t+1})}} d\bx_{t} d\bx_{t + 1} + C_1\\
        = & -\int \tq(\bx_{t}, \bx_{t+1})\log{\frac{p_{\btheta}(\bx_{t} \mid \bx_{t + 1})}{\tq(\bx_{t}\mid \bx_{t + 1})}} d\bx_{t} d\bx_{t + 1} + C_1 + C_2\\
        = &  D_{KL}(\tq(\bx_{t}\mid \bx_{t + 1}) \parallel p_{\btheta}(\bx_{t}\mid \bx_{t + 1})) + C \\
        = & L_{\mathrm{vlb}}^{\tq}(\btheta, t) + C, 
    \end{aligned}
\end{equation}
    where $C$, $C_{1}$, $C_{2}$ are all constants independent of $\btheta$. 
\end{proof}

\subsubsection{Proof of Theorem \ref{thm:equivalence}}\label{app:proof of equivalence}

In this section, we prove the Theorem \ref{thm:equivalence}. To simplify the notation, let  $p_{\btheta}(\bx_{t} \mid \bx_{t + 1})\sim \cN(\bmu_{\btheta}(\bx_{t + 1}, t + 1), \sigma_{t + 1}$ \footnote{Here $\sigma_{t + 1}$ can be also optimized as in \citep{bao2022analytic}, but we find optimizing it in practice does not improve the empirical results.} in \eqref{eq:transition}, then the optimal solution (Lemma 9 in \citep{bao2022analytic}) of minimizing $L^{\tq_{t}}_{t + 1}$ is 
	\begin{equation}\label{eq:optimal couple}
		\small
		\begin{aligned}
			\bmu_{\btheta}(\bx_{t + 1}, t + 1) = \mE_{\tq_{t}}[\bx_{t}\mid \bx_{t + 1}].
		\end{aligned}
	\end{equation} 
	For every specific $t$, we consider the following $\tq_{t}$ in \eqref{eq:dro objective} \footnote{We can do this since \eqref{eq:dro objective} only relates to $\tq_{t}(\bx_{t + 1})$}, such that 
	\begin{equation}\label{eq:formulation of qt}
		\small
		\begin{aligned}
			& \tq_{t}(\bx_{t + 1} \mid \bx_{t}) \neq q(\bx_{t + 1} \mid \bx_{t}); \\
            & \tq_{t}(\bx_{t + 1}) \neq q(\bx_{t + 1}); \\
			& \tq_{t}(\bx_{0:t}) = q(\bx_{0:t}). \\
			& \tq_{t}(\bx_{t}\mid \bx_{0}, \bx_{t + 1}) = q(\bx_{t}\mid \bx_{0}, \bx_{t + 1}) = \cN(\mu_{t + 1}(\bx_{0}, \bx_{t + 1}), \sigma_{t}). 
		\end{aligned}
	\end{equation}
    where $\mu_{t + 1}(\bx_{0}, \bx_{t + 1}) = \frac{\sqrt{\baralpha_{t}}(1 - \alpha_{t+1})}{1 - \baralpha_{t + 1}}\bx_{0} + \frac{\sqrt{\alpha_{t + 1}}(1 - \baralpha_{t})}{1 - \baralpha_{t + 1}}\bx_{t + 1}$. The $\tq_{t}$ can be taken due to the Bayesian rule. Next, we analyze the optimal formulation in \eqref{eq:optimal couple}. Due to the property of conditional expectation, we have 
	\begin{equation}
		\small
		\bmu_{\btheta}(\bx_{t + 1}, t + 1) = \mE_{\tq_{t}}\left[\mE_{\tq_{t}}\left[\bx_{t} \mid \bx_{0}, \bx_{t + 1}\right]\mid \bx_{t+1}\right] = \mu_{t + 1}\left(\mE_{\tq_{t}}[\bx_{0}\mid \bx_{t + 1}], \bx_{t + 1}\right).
	\end{equation}
As can be seen, the optimal transition rule is decided by the conditional expectation $\mE_{\tq_{t}}[\bx_{0}\mid \bx_{t + 1}]$ for some $\tq_{t}(\bx_{t + 1})\in B_{D_{KL}}(\tq(\bx_{t + 1}), \eta_{0})$ in \eqref{eq:dro objective}. Then, we have the following lemma to get the desired conditional expectation.

\begin{lemma}\label{lemma:equivalence}
    There exists some $\eta \geq \eta_{0}$ in  \eqref{eq:x prediction} which makes \eqref{eq:x prediction} equivalent to problem \eqref{eq:dro objective}. 
    \begin{equation}\label{eq:x prediction}
    \small
    \min_{\btheta}\sum_{t=0}^{T - 1}\mE_{\tq_{t}(\bx_{0})}\sup_{\tq_{t}(\bx_{t + 1}\mid \bx_{0})\in B_{D_{KL}}(q_{t}(\bx_{t + 1}\mid \bx_{0}), \eta)}\mE_{\tq_{t}(\bx_{t + 1}\mid \bx_{0})}\left[\left\|\bx_{\btheta}(\bx_{t + 1}, t + 1) - \bx_{0}\right\|^{2}\right],
\end{equation}
where $\mE_{p_{\btheta}}[\bx_{0}\mid \bx_{t + 1}] = \bx_{\btheta}(\bx_{t + 1}, t + 1)$.
\end{lemma}

\begin{proof}
	Let us check the training objective $\min_{\btheta}\sup_{\tq_{t}\in B_{D_{KL}}(q_{t + 1}, \eta)}D_{KL}(\tq_{t}(\bx_{t}\mid \bx_{t + 1})\parallel p_{\btheta}(\bx_{t}\mid \bx_{t + 1}))$. During this proof, we abbreviate $B_{D_{KL}}(q_{t + 1}(\bx_{t + 1}), \eta)$ as $B$. Since $p_{\btheta}(\bx_{t}\mid \bx_{t + 1})\sim \cN(\bmu_{\btheta}(\bx_{t + 1}, t + 1), \sigma_{t + 1})$, then 
	\begin{equation}
		\small
		\begin{aligned}
			&\sup_{\tq_{t}(\bx_{t + 1})\in B} D_{KL}(\tq_{t}(\bx_{t}\mid \bx_{t + 1})\parallel p_{\btheta}(\bx_{t}\mid \bx_{t + 1}))\\
             \propto &-\frac{d}{2}\log{2\pi\sigma_{t + 1}^{2}}
			 - \frac{1}{2\sigma_{t + 1}^{2}}\sup_{\tq_{t}(\bx_{t + 1})\in B}\mE_{\tq(\bx_{t}, \bx_{t + 1})}\left[\left\|\bx_{t} - \bmu_{\btheta}(\bx_{t + 1}, t + 1)\right\|^{2}\right].\\ 
		\end{aligned} 
	\end{equation}
	As we consider $\sigma_{t + 1}$ as constant, an analysis of the expectation term is enough. Due to 
	\begin{equation}\label{eq:lower bound}
		\small
		\begin{aligned}
			\mE_{\tq_{t}(\bx_{t}, \bx_{t + 1})}\left[\left\|\bx_{t} - \bmu_{\btheta}(\bx_{t + 1}, t + 1)\right\|^{2}\right] & \geq \inf_{f}\mE_{\tq_{t}(\bx_{0}, \bx_{t}, \bx_{t + 1})}\left[\left\|\bx_{t} - f(\bx_{0}, \bx_{t+1})\right\|^{2}\right]\\
			 & = \mE_{\tq_{t}(\bx_{0}, \bx_{t}, \bx_{t + 1})}\left[\left\|\bx_{t} - \mE_{\tq}[\bx_{t}\mid \bx_{0}, \bx_{t + 1}]\right\|^{2}\right],
		\end{aligned}
	\end{equation}
	where the last term is invariant over $\tq_{t}\in B$ so that it is a uniform lower bound over all possible $\tq_{t}$ and $p_{\btheta}(\bx_{t} \mid \bx_{t + 1})$. The above inequality indicates that the optimal $\bmu_{\btheta}(\bx_{t + 1}, t + 1)$ is achieved when the left in \eqref{eq:lower bound} becomes the right in \eqref{eq:lower bound}.
	\par
	On the other hand, for any $\tq_{t}\in B$, let us compute the gap such that
	\begin{equation}
		\small
		\begin{aligned}
			& \mE_{\tq_{t}(\bx_{t}, \bx_{t + 1})} \left[\left\|\bx_{t} - \bmu_{\btheta}(\bx_{t + 1}, t + 1)\right\|^{2}\right] \\
            &= 
            \mE_{\tq_{t}}\left[\left\|\bx_{t} - \mE_{\tq_{t}}[\bx_{t}\mid \bx_{0}, \bx_{t + 1}] + \mE_{\tq_{t}}[\bx_{t}\mid \bx_{0}, \bx_{t + 1}] - \bmu_{\btheta}(\bx_{t + 1}, t + 1)\right\|^{2}\right] \\
            & = 
            \mE_{\tq_{t}}\left[\left\|\bx_{t} - \mE_{\tq_{t}}[\bx_{t}\mid \bx_{0}, \bx_{t + 1}]\right\|^{2}\right]\\
			& + \mE_{\tq_{t}}\left[\left\|\bmu_{\btheta}(\bx_{t + 1}, t + 1) - \mE_{\tq_{t}}[\bx_{t}\mid \bx_{0}, \bx_{t + 1}]\right\|^{2}\right] \\
			& - 2\mE_{\tq_{t}}\left[\left\langle\bx_{t} - \mE_{\tq_{t}}[\bx_{t}\mid \bx_{0}, \bx_{t + 1}], \bmu_{\btheta}(\bx_{t + 1}, t + 1) - \mE_{\tq_{t}}[\bx_{t}\mid \bx_{0}, \bx_{t + 1}]\right\rangle\right] \\
			& = \mE_{\tq_{t}}\left[\left\|\bx_{t} - \mE_{\tq_{t}}[\bx_{t}\mid \bx_{0}, \bx_{t + 1}]\right\|^{2}\right]\\
			& + \left(\sqrt{\baralpha_{t}} - \sqrt{1 - \baralpha_{t} - \sigma_{t + 1}^{2}}\sqrt{\frac{\baralpha_{t + 1}}{1 - \baralpha_{t +1}}}\right)\mE_{\tq_{t}(\bx_{0}, \bx_{t + 1})}\left[\left\|\bx_{0} - \bx_{\btheta}(\bx_{t + 1}, t + 1)\right\|^{2}\right],
		\end{aligned}
	\end{equation}
	where the equality is due to the property of conditional expectation leads to $\mE_{\tq_{t}}[\langle\bx_{t} - \mE_{\tq_{t}}[\bx_{t}\mid \bx_{0}, \bx_{t + 1}], \bmu_{\btheta}(\bx_{t + 1}, t + 1) - \mE_{\tq_{t}}[\bx_{t}\mid \bx_{0}, \bx_{t + 1}]\rangle]=0$, and rewriting $\mE_{\tq_{t}}[\|\bmu_{\btheta}(\bx_{t + 1}, t + 1) - \mE_{\tq_{t}}[\bx_{t}\mid \bx_{0}, \bx_{t + 1}]\|^{2}]$ as in equations (5)-(10) in \citep{ho2020denoising}. Due to this, we know that minimizing the square error is equivalent to minimizing the $\mE_{\tq_{t}(\bx_{t}, \bx_{t + 1})}[\|\bx_{0} - \bx_{\btheta}(\bx_{t + 1}, t + 1)\|^{2}]$. On the other hand, since $\tq_{t}^{*}\in B$, then we have 
	\begin{equation}
		\small
        \begin{aligned}
            &D_{KL}(q(\bx_{t + 1}\mid \bx_{0})\parallel \tq_{t}^{*}(\bx_{t + 1}\mid \bx_{0}))\\
            = &D_{KL}(q(\bx_{0}\mid \bx_{t + 1})\parallel \tq_{t}^{*}(\bx_{0}\mid \bx_{t + 1})) + D_{KL}(q(\bx_{t + 1})\parallel \tq_{t}^{*}(\bx_{t + 1}))\\
            \geq &\eta_{0}.  
        \end{aligned}
	\end{equation}
	Thus, we prove our conclusion.  
\end{proof}

\equivalencedro*
\begin{proof}
    By combining Lemma \ref{lemma:equivalence}, suppose the supreme is attained under $\tq_{t - 1}$ such that $\bx_{t}\sim \tq_{t - 1}(\bx_{t})$ with 
    \begin{equation}
        \bx_{t} = \sqrt{\baralpha_{t}}\bx_{0} + \sqrt{1 - \baralpha_{t}}\beps_{t} + \bdelta_{t},
    \end{equation}
    with $\bdelta_{t}$ depends on $\bx_{0}$ and $\bx_{t}$. Then we prove the conclusion. 
\end{proof}

\subsubsection{Proof of Proposition \ref{pro:effectiveness}}
\effectivenessofdro*
\begin{proof}
    This theorem can proved by induction. Since $D_{KL}(q(\bx_{T})\parallel p_{\btheta}(\bx_{T})) \leq \eta_{0}$, then, let $\tq_{T - 1}^{*}(\bx_{T}) = p_{\btheta}(\bx_{T})$ and satisfies $\tq_{T - 1}^{*}(\bx_{T}) = q(\bx_{T - 1})$. The existence of such distribution is due to Kolmogorov existence theorem \citep{shiryaev2016probability}. Then, we have 
    \begin{equation}
    \small
    \begin{aligned}
        D_{KL}(\tq_{T - 1}^{*}(\bx_{T - 1})\parallel p_{\btheta}(\bx_{T - 1})) & \leq D_{KL}(\tq_{T - 1}^{*}(\bx_{T})\parallel p_{\btheta}(\bx_{T})) \\
        & + D_{KL}(\tq^{*}_{T - 1}(\bx_{T - 1}\mid \bx_{T})\parallel p_{\btheta}(\bx_{T - 1}\mid \bx_{T})) \\
        & \leq L_{t}^{\mathrm{DRO}}(\btheta) \\
        & \leq \eta_{0}, 
    \end{aligned}
    \end{equation}
    where the first inequality is due to the definition of $L_{t}^{\mathrm{DRO}}(\btheta)$ and $\tq_{T - 1}^{*}(\bx_{T}) = p_{\btheta}(\bx_{T})$. Then, we prove our conclusion by induction over $t$. 
\end{proof}

\subsubsection{Proof of Proposition \ref{pro:ddpm adv}}\label{app:proof of at}
\wtokl*
\begin{proof}
	Due to the definition of the first order Wasserstein distance $\sW_{1}(\cdot, \cdot)$ \citep{villani2009optimal} for any specific $\bx_{0}$, suppose 
	\begin{equation}
		\small
			\pi^{*} \in \argmin_{\pi(\bx_{t}, \tilde{\bx}_{t})\in q_{t}(\bx_{t}\mid \bx_{0})\times \tq_{t}(\tilde{\bx}_{t}\mid \bx_{0})}\mE\left[\|\tilde{\bx}_{t} - \bx_{t}\|_{1}\right],
	\end{equation}
	so that 
	\begin{equation}
		\small
			\mE_{\pi^{*}}\left[\|\tilde{\bx}_{t} - \bx_{t}\|_{1}\right] = \sW_{1}(q_{t}(\bx_{t}\mid \bx_{0}), \tq_{t}(\bx_{t}\mid \bx_{0})). 
	\end{equation}
	Let $\bdelta_{t}$ be the one of \eqref{eq:eps dro} under $\pi^{*}$ derived by Lemma \ref{lemma:equivalence}, then 
	\begin{equation}
		\small
		\begin{aligned}
			\bbP\left(\|\bdelta_{t}\|_{1} \geq \eta\mid \bx_{0}\right) & \leq \frac{\mE_{\pi^{*}}[\|\bdelta_{t}\|_{1}]}{\eta}\\
			& = \frac{\sW_{1}(q_{t}(\bx_{t}\mid \bx_{0}), \tq_{t}(\bx_{t}\mid \bx_{0}))}{\eta} \\
			& \overset{\leq}{a} \frac{\sqrt{2(1 - \baralpha_{t})D_{KL}(q_{t}(\bx_{t}\mid \bx_{0})\parallel \tq_{t}(\bx_{t}\mid \bx_{0}))}}{\eta} \\
			& \leq \sqrt{\frac{2(1 - \baralpha_{t})}{\eta}},
		\end{aligned}
	\end{equation}
	where inequality $a$ is due to the Talagrand’s inequality \citep{wainwright2019high}. Then we prove our conclusion. 
\end{proof}


%\gapexample*
%\begin{proof}
%	It has been proven in \citep{bao2022analytic} that the optimal solution of $p_{\btheta}(\bx_{t}\mid \bx_{t + 1})$ satisfies 
%	\begin{equation}\label{eq:conditional mean and variance}
%		\small
%		\begin{aligned}
%			\bmu_{\btheta^{*}}(\bx_{t + 1}) & = \mE_{q}[\bx_{t} \mid \bx_{t + 1}] = \frac{1}{\sqrt{\alpha_{t + 1}}}\left(\bx_{t + 1} - \beta_{t + 1}\frac{\bx_{t + 1} - \sqrt{\baralpha_{t + 1}}\bmu}{1 - \baralpha_{t + 1}}\right); \\
%			\sigma_{\btheta^{*}}(t + 1) & = \frac{\beta_{t + 1}}{\alpha_{t + 1}}\left(1 - \frac{\beta_{t + 1}}{d}\mE_{q}\left[\left\|\frac{\bx_{t + 1} - \sqrt{\baralpha_{t + 1}}\bmu}{1 - \baralpha_{t + 1}}\right\|^{2}\right]\right).
%		\end{aligned}
%	\end{equation}
%	Since $p_{\btheta}(\bx_{t + 1})$ is a Gaussian, and the transition probability $p_{\btheta}(\bx_{t}\mid \bx_{t + 1})$ is also a Gaussian, then $p_{\btheta}(\bx_{t})$ is also Gaussian, which is decided by its mean and variance. Due to \eqref{eq:conditional mean and variance}, we have 
%	\begin{equation}
%		\small
%		\begin{aligned}
%			\mE_{p_{\btheta^{*}}}[\bx_{t}] & = \frac{1}{\sqrt{\alpha_{t + 1}}}\left(\frac{\alpha_{t + 1} - \baralpha_{t + 1}}{1 - \baralpha_{t + 1}}\right)\mE_{p_{\btheta^{*}}}[\bx_{t + 1}] + \frac{\beta_{t + 1}}{1 - \baralpha_{t + 1}}\sqrt{\baralpha_{t}}\bmu \\
%			& = \frac{1}{\sqrt{\alpha_{t + 1}}}\left(\frac{\alpha_{t + 1} - \baralpha_{t + 1}}{1 - \baralpha_{t + 1}}\right)\left(\mE_{p_{\btheta^{*}}}[\bx_{t + 1}] - \mE_{q}[\bx_{t + 1}] + \mE_{q}[\bx_{t + 1}]\right) + \frac{\beta_{t + 1}}{1 - \baralpha_{t + 1}}\sqrt{\baralpha_{t}}\bmu \\
%			& = \mE_{q}[\bx_{t}] + \sqrt{\alpha_{t + 1}}\frac{1 - \baralpha_{t}}{1 - \baralpha_{t + 1}}\left(\mE_{p_{\btheta^{*}}}[\bx_{t + 1}] - \mE_{q}[\bx_{t + 1}]\right).
%		\end{aligned}
%	\end{equation}
%	Let $\mE_{p_{\btheta^{*}}}[\bx_{t + 1}] - \mE_{q}[\bx_{t + 1}] = \bdelta_{t + 1}$, the above equality indicates that 
%	\begin{equation}
%		\small
%		\bdelta_{t} = \prod_{s = t + 1}^{T - 1}\underbrace{\sqrt{\alpha_{s + 1}}\frac{1 - \baralpha_{s}}{1 - \baralpha_{s + 1}}}_{\zeta_{s}<1}\bdelta.
%	\end{equation}
%	Then with $T\to \infty$, we must have $\mE_{p_{\btheta^{*}}}[\bx_{0}]$ approximates the ground-truth one $\mE_{q}[\bx_{0}]$. Thus the generated data can match the target one in terms of expectation. On the other hand, let us check variance. From \eqref{eq:conditional mean and variance}, and the fact that $\Var_{p_{\btheta}}(\bx_{T})$ is a diagonal matrix, we have 
%	\begin{equation}
%		\small
%		\begin{aligned}
%			& \Var_{p_{\btheta^{*}}}(\bx_{t}) = \sigma_{\btheta^{*}}(t + 1)\bI + \Var_{p_{\btheta^{*}}}(\bmu_{\btheta^{*}}(\bx_{t + 1})) \\
%			& = \frac{\beta_{t + 1}}{\alpha_{t + 1}}\left(1 - \frac{\beta_{t + 1}}{d(1 - \baralpha_{t + 1})^{2}}\tr(\Var_{q}(\bx_{t + 1}))\right)\bI + \frac{\alpha_{t + 1}(1 - \baralpha_{t})^{2}}{(1 - \baralpha_{t + 1})^{2}}\Var_{p_{\btheta^{*}}}(\bx_{t + 1}) \\
%			& = \frac{\beta_{t + 1}}{1 - \baralpha_{t + 1}}\bI + \frac{\alpha_{t + 1}(1 - \baralpha_{t})^{2}}{(1 - \baralpha_{t + 1})^{2}}\Var_{p_{\btheta^{*}}}(\bx_{t + 1}) \\
%			& = \frac{\beta_{t + 1}}{\alpha_{t + 1}}\bI - \frac{1 - 2\alpha_{t + 1} + \baralpha_{t + 1}}{\alpha_{t + 1}(1 - \baralpha_{t + 1})}\left(\Var_{q}(\bx_{t + 1}) + \Var_{p_{\btheta^{*}}}(\bx_{t + 1}) - \Var_{q}(\bx_{t + 1})\right) - \frac{\beta_{t + 1}^{2}}{d\alpha_{t + 1}(1 - \baralpha_{t + 1})^{2}}\left\|\bdelta_{t + 1}\right\|^{2}\bI \\
%			& = \Var_{q}(\bx_{t}) - \frac{1 - 2\alpha_{t + 1} + \baralpha_{t + 1}}{\alpha_{t + 1}(1 - \baralpha_{t + 1})}\left(\Var_{p_{\btheta^{*}}}(\bx_{t + 1}) - \Var_{q}(\bx_{t + 1})\right) - \frac{\beta_{t + 1}^{2}}{d\alpha_{t + 1}(1 - \baralpha_{t + 1})^{2}}\left\|\bdelta_{t + 1}\right\|^{2}\bI.
%		\end{aligned}
%	\end{equation}
%	Let $\Var_{p_{\btheta}}(\bx_{t}) - \Var_{q}(\bx_{t}) = \Delta_{t}$, we get the formulation 
%	\begin{equation}
%		\small
%		\Delta_{t} = \underbrace{\frac{1 - 2\alpha_{t + 1} + \baralpha_{t + 1}}{\alpha_{t + 1}(1 - \baralpha_{t + 1})}}_{\gamma_{t + 1} > 1}\Delta_{t + 1} - \frac{\beta_{t + 1}^{2}}{d\alpha_{t + 1}(1 - \baralpha_{t + 1})^{2}}\left\|\bdelta_{t + 1}\right\|^{2}\bI,
%	\end{equation}
%	which indicates that 
%	\begin{equation}
%		\small
%		\Delta_{0} = -\sum\limits_{t=1}^{T}\frac{\beta_{t + 1}^{2}}{d\alpha_{t + 1}(1 - \baralpha_{t + 1})^{2}}\left\|\bdelta_{t + 1}\right\|^{2}\bI\prod_{s = t}^{T}\gamma_{t} = -\bI\sum\limits_{t=1}^{T}\frac{\beta_{t + 1}^{2}}{d\alpha_{t + 1}(1 - \baralpha_{t + 1})^{2}}\prod_{s = t}^{T}\gamma_{s}\zeta_{s - 1}\|\bdelta\|^{2}.
%	\end{equation}
%	Since $\gamma_{s}\zeta_{s - 1} > 1$, we know that $\Delta_{0} =  O(\delta)\bI$
%\end{proof}


%\jensengap*
%\begin{proof}
%	For any $x, y \in \mathrm{supp}(f)$, we have 
%	\begin{equation}
%		\small
%		\begin{aligned}
%			f(x) - f(y) = f^{\prime}(y)(x - y) + \frac{1}{2}\int_{0}^{1}\int_{0}^{t}(x - y)^{2}f^{\prime\prime}(y + s(x - y))dsdt.
%		\end{aligned}
%	\end{equation}
%	By taking $x = X$, $y = E[X]$, and taking expectation over the equality, we get the conclusion. On the other hand, the Jensen's inequality $\mE[f(X)] \leq f(\mE[X])$ is also obtained, since $f^{\prime\prime} \geq 0$  
%\end{proof}

\section{Proofs in Section \ref{sec:adversarial under consistency model}}\label{app:proofs of consistency model}
Next, we give the proof of results in Section \ref{sec:adversarial under consistency model}. Firstly, let us check the definition of the $\Phi_{t}(\bx_{t + 1})$. For the variance-preserving stochastic differential equation in \citet{song2020denoising}
\begin{equation}\label{eq:sde}
    \small
        d\bz_{s} = -\frac{\beta_{s}}{2}\bz_{s}dt + \sqrt{\beta_{s}}dW_{s}.
\end{equation}
Due to the solution of $\bz_{s}$ in \citet{song2023consistency}, we know $\bz_{s_{t}}$ has the same distribution with $\bx_{t}$ in \eqref{eq:xt} for $\{s_{t}\}_{t=1}^{T}$ satisfies 
\begin{equation}
    \small
        \exp\left(-\int_{0}^{s_{t}}\beta(u)du\right) = \baralpha_{t} \qquad (s_{0} = 0).    
\end{equation}
In the rest of this section, we use $d(\bx, \by)$ in \eqref{eq:cm objective} as $\ell_{2}$ distance $\|\bx - \by\|^{2}$, whereas the conclusions under other distance can be similarly derived.
Owing the the discussion in above, similar to \citep{song2023consistency}, when $\bx_{t + 1} = \bz_{s_{t + 1}}$, let $\Phi_{t}(\bx_{t + 1}) = \Psi_{s_{t}}(\bz_{s_{t + 1}})$, we can rewrite the objective \eqref{eq:cm objective} as follows. 

\begin{equation}\label{eq:cd objective expected}
    \small
    \min_{\btheta}\cL_{CD}(\btheta) = \min_{\btheta}\sum_{t = 0}^{T - 1}\mE_{\bz_{s_{t}}}\left[\left\|f_{\btheta}(\Psi_{s_{t}}(\bz_{s_{t + 1}}), t) - f_{\btheta}(\bz_{s_{t + 1}}, t + 1)\right\|^{2}\right].
\end{equation}
Here $\bz_{s}$ follows the following reverse time ODE of \eqref{eq:sde} with $\bz_{0}\sim q(\bx_{0})$,
\begin{equation}
    \small
        d\bz_{s} = \underbrace{-\frac{\beta_{s}}{2}\left(\bz_{s} + \frac{1}{2}\nabla_{\bz}\log{q_{s}(\bz_{s})}\right)}_{\phi_{s}}ds,  
\end{equation}
and such $\bz_{s}$ has the same distribution with the ones in \eqref{eq:sde} \citep{song2020denoising}, where $q_{s}$ is the density of $\bz_{s}$. $\Psi_{s_{t}}(\bz_{s_{t + 1}}) = \bz_{s_{t + 1}} - \int_{s_{t}}^{s_{t + 1}} \phi_{s}(\bz_{s})ds$, which is a deterministic function of $\bz_{s_{t + 1}}$, and $f_{\btheta}(\bz_{s_{0}}, 0) = \bz_{s_{0}} = \bz_{0}$. 
\par
Now, we are ready to prove the Theorem \ref{thm:expected cd gap} as follows. 

%Minimizing the training objective makes $f_{\btheta}(\bz_{s_{T}}, T)\sim q(\bz_{0})$. To see this, when $f_{\btheta}(\Phi_{s_{t}}(\bz_{s_{t + 1}}), t) = f_{\btheta}(\bz_{s_{t + 1}}, t + 1)$ for all $(\bz_{s_{t + 1}}, t)$. Then, for any $\bz_{s_{T}}$, we have 
% \begin{equation}
%     \small
%         f_{\btheta}(\bz_{s_{T}}, T) = f_{\btheta}(\Phi_{T - 1}(\bz_{s_{T}}), T - 1) = \cdots = f_{\btheta}\left(\Phi_{s_{0}}\left(\Phi_{s_{1}}\left(\cdots\Phi_{s_{T - 1}}\left(\bz_{s_{T}}\right)\right)\right), s_{0}\right) = \bz_{s_{T}} - \int_{0}^{s_{T}}\phi_{u}(u)du,
% \end{equation}
% where the last one follows the distribution $q(\bz_{0})$. Though minimizing the consistency training objective \eqref{eq:cd objective expected} obtains model $f_{\btheta}$ transfers $\bz_{T}$ into target one $\bz_{0}$. The following theorem indicates the gap between target distribution $q(\bz_{0})$ and the generated one $f_{\btheta}(\bz_{s_{T}}, T)$. 

\expectedcdgap*
\begin{proof}
	Owing to the definition of $\sW_{1}$-distance, and the discussion in above, we have 
	\begin{equation}
		\small
		\begin{aligned}
			\sW_{1}(f_{\btheta}(\bx_{T}, T), \bx_{0}) & = \sW_{1}(f_{\btheta}(\bz_{s_{T}}, T), \bz_{s_{0}}) \\
            & = \sW_{1}\left(f_{\btheta}(\bz_{s_{T}}, T), \Psi_{s_{0}}\left(\Psi_{s_{1}}\left(\cdots\Psi_{s_{T - 1}}\left(\bz_{s_{T}}\right)\right)\right)\right) \\
			& \leq \mE\left[\|f_{\btheta}(\bz_{s_{T}}, T) - \Psi_{s_{0}}\left(\Psi_{s_{1}}\left(\cdots\Psi_{s_{T - 1}}\left(\bz_{s_{T}}\right)\right)\right)\|\right] \\
			& \leq \sum_{t = 0}^{T - 1}\mE\left[\left\|f_{\btheta}(\bz_{s_{t + 1}}, t + 1) - f_{\btheta}(\Psi_{s_{t}}(\bz_{s_{t + 1}}), t)\right\|\right] \\
			& \leq \sqrt{T\cL_{CD}(\btheta)},
		\end{aligned}
	\end{equation}
	where the first inequality is due to the definition of Wasserstein distance, the second and last inequalities respectively use the triangle inequality and Schwarz's inequality.  
\end{proof}


\subsection{Proof of Theorem \ref{thm:cd upper bound}}\label{app:proof of cd upper bound}
As pointed out in the above, the used $\hat{\Phi}_{t}(\bx_{t + 1}, \beps_{\bphi})$ is a numerical estimator of $\Phi_{t}(\bx_{t + 1})$. In the sequel, let us consider $\hat{\Phi}$ is an Euler estimator as follows, whereas our analysis can be similarly generalized to the other estimators. 

    \begin{equation}\label{eq:estimated phi}
		\small
			\hat{\Phi}_{t}(\bx_{t + 1}, \beps_{\bphi}) = \hat{\Psi}_{s_{t}}(\bz_{s_{t + 1}}, \beps_{\bphi}) = \bz_{s_{t + 1}} + (s_{t + 1} - s_{t})\underbrace{\frac{\beta_{s_{t + 1}}}{2}\left(\bz_{s_{t + 1}} + \beps_{\bphi}(\bz_{s_{t + 1}}, t + 1) / \sqrt{1 - \baralpha_{t + 1}}\right)}_{\hat{\phi}_{s_{t + 1}}}, 
	\end{equation}
	where $\sqrt{1 - \baralpha_{t + 1}}\beps_{\bphi}(\bz_{s_{t + 1}}, t + 1)$ estimates $\nabla_{\bz}\log{q_{s_{t + 1}}(\bz_{s_{t + 1}})}$ as pointed out in \citep{song2020score}, and the condition $\bx_{t + 1} = \bz_{s_{t + 1}}$ is hold. 
    \par
    Next, we illustrate the used regularity conditions to derive Theorem \ref{thm:cd upper bound}.
    
    \begin{assumption}\label{ass:continuity}
    The discretion error of $\hat{\Psi}_{s_{t}}(\bz_{s_{t + 1}}, \beps_{\bphi})$ is smaller than $C(s_{t + 1} - s_{t})^{2}$ for constant $C$, that says 
    \begin{equation}
        \small
        \left\|\hat{\Psi}_{s_{t}}(\bz_{s_{t + 1}}, \beps_{\bphi}) - \bz_{s_{t + 1}} - \int_{s_{t}}^{s_{t + 1}}\hat{\phi}_{s}(\bz_{s})ds\right\| \leq C(s_{t + 1} - s_{t})^{2}
    \end{equation} 
\end{assumption}
\begin{assumption}\label{ass:estimation error}
    The estimated score $\nabla_{\bz}\log{\hat{q}_{s}(\bz)}$ has bounded expected error, i.e., 
    \begin{equation}
        \small
        \mE_{\bz\sim q_{s_{t}}(\bz)}\left[\left\|\hat{\phi}_{s_{t}}(\bz) - \phi_{s_{t}}(\bz)\right\|^{2}\right] \leq \epsilon.
    \end{equation}
    for all $0 \leq t < T$. 
\end{assumption}
\begin{assumption}\label{ass:bounded f}
    For the learned model $f_{\btheta}$, it holds $\|f_{\btheta}\| \leq D$.
\end{assumption}

The Assumption \ref{ass:continuity} describes the discretion error of the Euler method under ODE with drift term $\hat{\phi}_{s}$, which can be satisfied under proper continuity conditions of model $\beps_{\phi}$. On the other hand, Assumption \ref{ass:estimation error} describes the estimation error of $\hat{\phi}_{s_{t}}(\bz)$, which terms out to be the training objective of obtaining it, see \citep{song2020score} for more details. The Assumption \ref{ass:bounded f} is natural, since $f_{\btheta}$ predicts $\bx_{0}$, which is usually an image data with bounded norm. Now, we are ready to prove the Theorem \ref{thm:cd upper bound}, which is presented by proving the following formal version.   

\begin{theorem}
    Under Assumptions \ref{ass:continuity}, \ref{ass:estimation error}, and \ref{ass:bounded f}, for all $\bdelta_{s_{t}}$, we have $\mE_{\bz_{s_{t}}}[\|\bdelta_{s_{t}}(\bz_{s_{t}})\|] \leq  O(\Delta^{2} _{s_{t}} + \epsilon\sqrt{\Delta_{s_{t}}})$ for $\Delta_{s_{t}} = s_{t + 1} - s_{t}$. Besides that, we have 
	 	\begin{equation}
	 		\small
	 		\sW_{1}(f_{\btheta}(\bz_{T}, T), \bz_{0}) \leq \sqrt{T\hat{\cL}_{CD}^{Adv}(\btheta) + \frac{4D^{2}}{\eta}\left[C\Delta_{s_{t}}^{2} + \epsilon O(\sqrt{\Delta_{s_{t}}})\right]}.
	 	\end{equation}
\end{theorem}

\begin{proof}
	Noting that $\Phi_{t}(\bx_{t + 1}) = \Psi_{s_t}(\bz_{s_{t + 1}})$ and $\hat{\Phi}_{t}(\bx_{t + 1}, \beps_{\bphi}) = \hat{\Psi}_{s_{t}}(\bz_{s_{t + 1}}, \beps_{\bphi})$, the key problem is to upper bound the difference between $\hat{\Psi}_{s_{t}}(\bz, \beps_{\bphi})$ and $\Psi_{s_{t}}(\bz)$ for all $t$ and $\bz$. To do so, we note that 
	\begin{equation}\label{eq:one step error}
		\small
		\left\|\hat{\Psi}_{s_{t}}(\bz, \beps_{\bphi}) - \Psi_{s_{t}}(\bz)\right\| \leq \left\|\hat{\Psi}_{s_{t}}(\bz, \beps_{\bphi}) - \bz - \int_{s_{t}}^{s_{t + 1}}\hat{\phi}_{s}(\bz_{s})ds\right\| + \left\|\bz - \int_{s_{t}}^{s_{t + 1}}\hat{\phi}_{s}(\bz_{s})ds - \Psi_{s_{t}}(\bz)\right\|,
	\end{equation}
	where the first one in r.h.s can be upper bounded by $C(s_{t + 1} - s_{t})^{2}$ according to Assumption \ref{ass:continuity}. On the other hand, define $\frac{d\hat{\bz}_{s}}{ds} = \hat{\phi}_{s}(\hat{\bz}_{s})$, then when $\hat{\bz}_{s_{t + 1}} = \bz_{s_{t + 1}} = \bz$ and $s\in[s_{t}, s_{t + 1}]$.  
	\begin{equation}
		\small
		\begin{aligned}
			\frac{d}{ds}\|\hat{\bz}_{s} - \bz_{s}\|^{2} & = \left\langle\hat{\bz}_{s} - \bz_{s}, \hat{\phi}_{s}(\hat{\bz}_{s}) - \phi_{s}(\bz_{s})\right\rangle \\
			& = \left\langle\hat{\bz}_{s} - \bz_{s}, \hat{\phi}_{s}(\hat{\bz}_{s}) - \hat{\phi}_{s}(\bz_{s}) + \hat{\phi}_{s}(\bz_{s}) - \phi_{s}(\bz_{s})\right\rangle \\
			& \leq L\|\hat{\bz}_{s} - \bz_{s}\|^{2} + \left\langle\hat{\bz}_{s} - \bz_{s}, \hat{\phi}_{s}(\bz_{s}) - \phi_{s}(\bz_{s})\right\rangle \\
			& \leq \left(\frac{1}{2} + L\right)\|\hat{\bz}_{s} - \bz_{s}\|^{2} + \frac{1}{2}\left\|\hat{\phi}_{s}(\bz_{s}) - \phi_{s}(\bz_{s})\right\|^{2}. 
		\end{aligned}
	\end{equation}
	Taking expectation over $\bz$, by Gronwall's inequality, Assumption \ref{ass:estimation error} and $\hat{\bz}_{s_{t + 1}} = \bz_{s_{t + 1}}$, we have 
	\begin{equation}
		\small
		\mE\left[\|\hat{\bz}_{s_{t}} - \bz_{s_{t}}\|^{2}\right] \leq \int_{s_{t}}^{s_{t + 1}}\frac{e^{(1/2 + L)(s - s_{t})}}{2}\mE\left[\|\hat{\phi}_{s}(\bz_{s}) - \phi_{s}(\bz_{s})\|^{2}\right]ds \leq \frac{\epsilon}{4}\int_{s_{t}}^{s_{t + 1}}\beta_{s}e^{(1/2 + L)(s - s_{t})}ds.
	\end{equation}
	Plugging this into \eqref{eq:one step error}, we know 
	\begin{equation}
		\small
		\mE\left[\left\|\hat{\Psi}_{s_{t}}(\bz_{s_{t}}, \beps_{\bphi}) - \Psi_{s_{t}}(\bz_{s_{t}})\right\|\right] \leq C(s_{t + 1} - s_{t})^{2} + \epsilon O(\sqrt{s_{t + 1} - s_{t}}). 
	\end{equation}
	By Markov's inequality, we have 
	\begin{equation}
		\small
      \begin{aligned}
      \bbP\left(\left\|\hat{\Psi}_{s_{t}}(\bz_{s_{t}}, \beps_{\bphi}) - \Psi_{s_{t}}(\bz_{s_{t}})\right\| \geq \eta\right) &\leq \frac{\mE\left[\left\|\hat{\Psi}_{s_{t}}(\bz_{s_{t}}, \beps_{\bphi}) - \Psi_{s_{t}}(\bz_{s_{t}})\right\|\right]}{\eta} \\
      & \leq \frac{1}{\eta}\left[C(s_{t + 1} - s_{t})^{2} + \epsilon O(\sqrt{s_{t + 1} - s_{t}})\right].
    \end{aligned}
	\end{equation}
	Thus, 
	\begin{equation}
		\small
		\begin{aligned}
			& \mE\left[\left\|f_{\btheta}(\bx_{t + 1}, t + 1) - f_{\btheta}(\Phi_{t}(\bx_{t + 1}), t)\right\|^{2}\right] \\
            & = \mE\left[\left\|f_{\btheta}(\bz_{s_{t + 1}}, t + 1) - f_{\btheta}(\Psi_{s_{t}}(\bz_{s_{t + 1}}), t)\right\|^{2}\right] \\
            & = \mE\left[\left\|f_{\btheta}(\bz_{s_{t + 1}}, t + 1) - f_{\btheta}(\hat{\Psi}_{s_{t}}(\bz_{s_{t + 1}} + \bdelta_{s_{t}}, \beps_{\bphi}), t)\right\|\right] \\
			& = \mE\left[\left(\textbf{1}_{\|\bdelta_{s_{t}}\| > \eta} + \textbf{1}_{\|\bdelta_{s_{t}}\| \leq \eta}\right)\left\|f_{\btheta}(\bz_{s_{t + 1}}, t + 1) - f_{\btheta}(\hat{\Psi}_{s_{t}}(\bz_{s_{t + 1}} + \bdelta_{s_{t}}, \beps_{\bphi}), t)\right\|^{2}\right] \\
			& \leq \mE\left[\sup_{\|\bdelta\| \leq \eta}\left\|f_{\btheta}(\bz_{s_{t + 1}}, t + 1) - f_{\btheta}(\hat{\Psi}_{s_{t}}(\bz_{s_{t + 1}} + \bdelta_{s_{t}}, \beps_{\bphi}), t)\right\|\right] + 4D^{2}\bbP\left(\left\|\delta_{s_{t}}\right\|^{2} \geq \eta\right) \\
			& \leq \mE\left[\sup_{\|\bdelta\| \leq \eta}\left\|f_{\btheta}(\bz_{s_{t + 1}}, t + 1) - f_{\btheta}(\hat{\Psi}_{s_{t}}(\bz_{s_{t + 1}} + \bdelta, \beps_{\bdelta}), t)\right\|^{2}\right] + \frac{4D^{2}}{\eta}\left[C(s_{t + 1} - s_{t})^{2} + \epsilon O(\sqrt{s_{t + 1} - s_{t}})\right].
		\end{aligned}
	\end{equation}
	Taking sum over $t$ and combining Theorem \ref{thm:expected cd gap}, we prove our conclusion. 
\end{proof}
Therefore, in this theorem, by taking $\Delta_{s_{t}} = s_{t + 1} - s_{t}$ close to zero, we get the results in Theorem \ref{thm:cd upper bound}. 

\section{The Connection to Standard Adversarial Training}\label{app:connection to AT}
In this section, we clarify why the proposed AT objective \eqref{eq:dpm at} is a general version of the standard AT objective proposed in \citep{madry2018towards} used for classification problems. 
\par
For classification problem, given model $f_{\btheta}(\bx)$, data $\bx$, and label $y$, it aims to minimize the adversarial training objective 
\begin{equation}\label{eq:standard AT class}
    \small
        \min_{\btheta}\mE_{(\bx, y)}\left[\sup_{\bdelta:\|\bdelta\|\leq \eta_{0}}\ell(f_{\btheta}(\bx + \bdelta), y)\right],
\end{equation}
for some loss function $\ell$ (e.g. cross entropy) and adversarial radius $\eta_{0}$. However, the objective is not directly generalized to the diffusion model, as its training objective is a regression problem instead of classification \eqref{eq:standard AT class}. Thus, we should refer to the general version of adversarial training as in \citep{yi2021improved,sinha2018certifying}, where the training objective is $\min_{\btheta}\mE_{\bx}[\ell_{\btheta}(\bx)]$, and the adversarial training objective becomes
\begin{equation}
    \small
    \min_{\btheta}\mE_{\bx}\left[\sup_{\bdelta:\|\bdelta\|\leq \eta_{0}}\ell_{\btheta}(\bx + \bdelta))\right],
\end{equation}
where $\ell_{\btheta}$ is the parameterized loss function, and $\bx$ is data. Then, we can conclude our objective \eqref{eq:dpm at} follows the above formulation, such that the goal is represented as 
\begin{equation}
    \small
        \min_{\btheta}\sum_{t=0}^{T - 1}\mE_{\bx_{0}}\left[\mE_{\bx_{t}\mid \bx_{0}}\left[\sup_{\bdelta:\|\bdelta\|\leq \eta_{0}}\ell_{\btheta}^{\bx_{0}}(\bx_{t} + \bdelta)\right]\right],    
\end{equation}
compared with the original noise prediction objective $\min_{\btheta}\sum_{t=0}^{T - 1}\mE_{\bx_{0}}\left[\mE_{\bx_{t}\mid \bx_{0}}\left[\ell_{\btheta}^{\bx_{0}}(\bx_{t})\right]\right]$ \eqref{eq:noise prediction}, such that the loss function 
\begin{equation}
    \small
    \ell_{\btheta}^{\bx_{0}}(\bx_{t}) = \left\|\beps_{\btheta}(t, \bx_{t}) - \frac{\bx_{t} - \sqrt{\baralpha_{t}}\bx_{0}}{\sqrt{1 - \baralpha_{t}}}\right\|^{2}. 
\end{equation}
This clarifies the equivalence of our objective \eqref{eq:dpm at} to general adversarial training. 
\section{Adversarial Training on Consistency Training Model}

In \citep{song2023consistency}, the consistency model can be even trained without estimator $\hat{\phi}_{s}$. They prove that the empirical consistency distillation loss $\hat{\cL}_{CD}(\btheta)$ can be approximated by the following $\cL_{CT}(\btheta)$
\begin{equation}
    \small
        \cL_{CT}(\btheta) = \sum_{t = 0}^{T - 1}\mE_{\bx_{t + 1}\sim q(\bx_{t + 1})}\left[\left\|f_{\btheta}(\bx_{t}, t) - f_{\btheta}(\bx_{t + 1}, t + 1)\right\|^{2}\right].
\end{equation}
In our adversarial regime, we can also prove that the desired $\hat{\cL}_{CD}^{Adv}(\btheta)$ can be approximated by the following $\cL_{CT}^{Adv}(\btheta)$ with adversarial perturbation
\begin{equation}
    \small
        \cL_{CT}^{Adv}(\btheta) = \sum_{t = 0}^{T - 1}\mE_{\bx_{t + 1}\sim q(\bx_{t + 1})}\left[\sup_{\|\bdelta\|\leq \eta}\left\|f_{\btheta}(\bx_{t} + \bdelta, t) - f_{\btheta}(\bx_{t + 1}, t + 1)\right\|^{2}\right].
\end{equation}
The results can be checked by the following theorem. 
\begin{restatable}{theorem}{cdandct}\label{thm:cd and ct gap}
    Suppose $f_{\btheta}(\bx_{t}, t)$ is twice continuously differentiable with a bounded second derivative. Then 
    \begin{equation}
        \small
            \hat{\cL}_{CD}^{Adv}(\btheta) \lesssim \cL_{CT}^{Adv}(\btheta) +  O\left(T - \sum_{t=1}^{T}\sqrt{\alpha_{t}} + T\eta^{2}\right), 
    \end{equation}
    where ``$\lesssim$'' means approximately less than or equal.  
\end{restatable}

\begin{proof}
	Due to the continuity of $f_{\btheta}(\bx, t)$, for any $\bdelta$ with $\|\bdelta\|\leq \eta$, by Taylor's expansion on $\bx_{t +1}$ from $\bx_{t} + \bdelta$, we have  
	\begin{equation}\label{eq:taylor expansion}
		\small
		\begin{aligned}
			& \mE\left[\left\|f_{\btheta}(\bx_{t} + \bdelta, t) - f_{\btheta}(\bx_{t + 1}, t + 1)\right\|^{2}\right] = \mE\left[\left\|f_{\btheta}(\bx_{t + 1}, t) - f_{\btheta}(\bx_{t + 1}, t + 1)\right\|^{2}\right] \\
			& + \mE\left[(f_{\btheta}(\bx_{t + 1}, t) - f_{\btheta}(\bx_{t + 1}, t + 1))^{\top}\nabla f_{\btheta}(\bx_{t + 1}, t)(\bx_{t} + \bdelta - \bx_{t + 1})\right] +  O\left(\mE\left[\|\bx_{t + 1} - \bx_{t} - \bdelta\|^{2}\right]\right).
		\end{aligned}
	\end{equation}
    Due to the Taylor's expansion $f_{\btheta}(\bx_{t} + \bdelta, t) = f_{\btheta}(\bx_{t + 1}, t) + \nabla f_{\btheta}(\bx_{t + 1}, t)(\bx_{t} + \bdelta - \bx_{t + 1}) + \cO(\|\bx_{t + 1} - \bx_{t} - \bdelta\|^{2})$. Then, from the formulation of $\bx_{t}$, we know $\mE\left[\|\bx_{t + 1} - \bx_{t} - \bdelta\|^{2}\right] =  O(1 - \sqrt{\alpha_{t}} + \eta^{2})$. Noting that due to definition of $s_{t}$, we have 
	\begin{equation}\label{eq:approximation}
		\small
		\begin{aligned}
			\mE[\bx_{t}\mid \bx_{t + 1} = \bz_{s_{t + 1}}] 
            & = \mE[\bz_{s_t}\mid \bz_{s_{t + 1}}] \\
            & = \frac{1}{\sqrt{\alpha_{t + 1}}}\left(\bz_{s_{t + 1}} - (1 - \alpha_{t + 1})\nabla_{\bx}\log{q_{s_{t + 1}}(\bz_{s_{t + 1}})}\right) \\
			& = \exp\left(\frac{1}{2}\int_{s_{t}}^{s_{t + 1}}\beta_{s}ds\right)\left(\bz_{s_{t + 1}} - \left(1 - e^{\int_{s_{t}}^{s_{t + 1}}\beta_{s}ds}\right)\nabla_{\bz}\log{q_{s_{t + 1}}(\bz_{s_{t + 1}})}\right) \\
			& \approx \left(1 + \frac{1}{2}\int_{s_{t}}^{s_{t + 1}}\beta_{s}ds\right)\bz_{s_{t + 1}} + \frac{1}{2}\int_{s_{t}}^{s_{t + 1}}\beta_{s}ds\nabla_{\bz}\log{q_{s_{t + 1}}(\bz_{s_{t + 1}})} \\
			& \approx \hat{\Psi}_{s_{t}}(\bz_{s_{t + 1}}, \sqrt{1 - \baralpha_{t + 1}}\nabla_{\bz}\log{q_{s_{t + 1}}}),
		\end{aligned}
	\end{equation}
    where the first equality is due to  Tweedie’s formula i.e., Lemma 11 in \citep{bao2022analytic}, the ``$\approx$'' is due to $e^{a}\approx 1 + a$ when $a\to 0$, and the last $\approx$ is due to Euler-Mayaruma discretion.
	Due to this, we notice that  
	\begin{equation}
		\small
		\begin{aligned}
			& \mE\left[(f_{\btheta}(\bx_{t + 1}, t) - f_{\btheta}(\bx_{t + 1}, t + 1))^{\top}\nabla f_{\btheta}(\bx_{t + 1}, t)(\bx_{t} + \bdelta - \bx_{t + 1})\mid \bx_{t + 1} = \bz_{s_{t + 1}}\right] \\
			& = \mE\left[(f_{\btheta}(\bx_{t + 1}, t) - f_{\btheta}(\bx_{t + 1}, t + 1))^{\top}\nabla f_{\btheta}(\bx_{t + 1}, t)\left(\mE\left[\bx_{t} + \bdelta \mid\bx_{t + 1} = \bz_{s_{t + 1}}\right] - \bx_{t + 1}\right)\mid \bx_{t + 1} = \bz_{s_{t + 1}}\right] \\
			& \approx \mE\left[(f_{\btheta}(\bz_{s_{t + 1}}, t) - f_{\btheta}(\bz_{s_{t + 1}}, t + 1))^{\top}\nabla f_{\btheta}(\bz_{s_{t + 1}}, t)\left(\hat{\Psi}_{s_{t}}(\bz_{s_{t + 1}}, \nabla_{\bz}\log{q_{s_{t + 1}}}) + \mE[\bdelta\mid \bz_{s_{t + 1}}] - \bz_{t + 1}\right)\right],
		\end{aligned}
	\end{equation}
    where the first equality is due to the property of conditional expectation, and the second ``$\approx$'' is due to \eqref{eq:approximation}.
	Combining this with \eqref{eq:taylor expansion}, we have 
	\begin{equation}
		\small
		\begin{aligned}
			& \mE\left[\left\|f_{\btheta}(\bx_{t} + \bdelta, t) - f_{\btheta}(\bx_{t + 1}, t + 1)\right\|^{2}\mid \bx_{t + 1}
             = \bz_{s_{t + 1}}\right] \\
             & = \mE\left[\left\|f_{\btheta}(\bz_{s_{t}} + \bdelta, t) - f_{\btheta}(\bz_{s_{t + 1}}, t + 1)\right\|^{2}\mid 
             \bz_{s_{t + 1}}\right] \\
             & = \mE\left[\left\|f_{\btheta}(\bz_{s_{t + 1}}, t) - f_{\btheta}(\bz_{s_{t + 1}}, t + 1)\right\|^{2}\right] \\
			& + \mE\left[(f_{\btheta}(\bz_{s_{t + 1}}, t) - f_{\btheta}(\bz_{s_{t + 1}}, t + 1))^{\top}\nabla f_{\btheta}(\bz_{s_{t + 1}}, t)\left(\hat{\Psi}_{s_{t}}(\bz_{s_{t + 1}}, \nabla_{\bz}\log{q_{s_{t + 1}}}) + \mE[\bdelta\mid \bz_{s_{t + 1}}] - \bz_{s_{t + 1}}\right)\right] \\
            & +  O(1 - \sqrt{\alpha_{t}} + \eta^{2}) \\
			& = \mE\left[\left\|f_{\btheta}(\hat{\Psi}_{s_{t}}(\bz_{s_{t + 1}}, \nabla_{\bz}\log{q_{s_{t + 1}}}) + \bdelta, t) - f_{\btheta}(\bz_{s_{t + 1}}, t + 1)\right\|^{2}\right] +  O(1 - \sqrt{\alpha_{t}} + \eta^{2}),
		\end{aligned}
	\end{equation}
	where the last equality is due to Taylor's expansion from $f_{\btheta}(\hat{\Psi}_{s_{t}}(\bz_{s_{t + 1}}, \nabla_{\bz}\log{q_{s_{t + 1}}}) + \bdelta, t)$ to $f_{\btheta}(\bz_{s_{t + 1}}, t)$. Due to the arbitrariness of $\bdelta$, we prove our conclusion.   
\end{proof}

% For continuous framework, the goal is making 
% \begin{equation}
%     \small
%     \sup_{D_{KL}(q_{\bx_{t + 1}}\parallel \tq_{\bx_{t + 1}})\leq \eta}D_{KL}(f(\bx_{t + 1})\parallel f_{\btheta}(\tx_{t + 1})) \leq \eta.
% \end{equation}
% If the above inequality holds, by iteration, we can make $D_{KL}(q_{\bx_{0}}\parallel p_{\btheta}(\bx_{0}))\leq \eta$, which is certifiable NLL. On the other hand, $D_{KL}(q_{\bx_{t + 1}}\parallel \tq_{\bx_{t + 1}})\leq \eta$ means with high probability $\|\bx_{t + 1} - \tx_{t + 1}\|\leq \eta$. Then
% \begin{equation}
%     \small
%     D_{KL}(f(\bx_{t + 1})\parallel f_{\btheta}(\tx_{t + 1}))\approx D_{KL}(f(\bx_{t + 1})\parallel f_{\btheta}(\bx_{t + 1} + \bdelta)) \leq \sup_{\bdelta}\int_{s_{t}}^{s_{t + 1}}\mE\left[\|\nabla_{\bz}\log{q_{s}}(\bz_{s}) - \nabla_{\bz}q_{\btheta}(\bz_{s} + \bdelta)\|^{2}\right]ds,
% \end{equation}
% which is the adversarial training objective. \yi{This part is used for continuous framework of adversarial training.} 	 


\section{Implementation Details}
\subsection{Hyperparameters of Diffusion Models}\label{app:hyper_dm}
For the diffusion models, all methods adopt the ADM model~\citep{dhariwal2021diffusion} as the backbone and follow the same training pipeline.
Following existing work~\citep{dhariwal2021diffusion,diffusion-ip}, we train models using the AdamW optimizer~\citep{adamw} with mixed precision training and the EMA rate is set to 0.9999.
For \texttt{CIFAR-10}, the pretrained ADM is trained using a batch size of 128 for 250K iterations with a learning rate set to 1e-4.
For \texttt{ImageNet}, the pretrained model is trained with a batch size of 1024 for 400K iterations, employing a learning rate of 3e-4.
The models are trained in a cluster of NVIDIA Tesla V100s.
More hyperparameters are reported in Table~\ref{tab:hyper_dm}. 
\input{tables/hyper_dm}


\subsection{Hyperparameters of Latent Consistency Models}\label{app:hyper_lcm}

For experiments on Latent Consistency Models (LCM)~\citep{luo2023latent}, we train models on LAIOIN-Aesthetic-6.5+~\citep{laion} 
at the resolution of 512$\times$512, comprising 650K text-image pairs with predicted aesthetic scores higher than 6.5.
Stable Diffusion v1.5~\citep{Rombach_2022_CVPR} is adopted as the teacher model and initialized the student and target models in the latent consistency distillation framework.
% Following~\citet{luo2023latent}, we train models with 100K iterations, using a batch size of 64, the learning rate 8e-6, and the EMA rate of the target model 0.95.
% The models are trained with a batch size of 64 for 100K iterations.
We set the range of the guidance scale $[w_{min}, w_{max}] = [3,5]$ during training and use $w = 4$ in sampling because it performs better in our preliminary experiments, which is similar to DMD \citep{yin2024onestep}.
The models are trained in a cluster of NVIDIA Tesla V100s.
Both models of our AT and the original LCM training are trained from scratch with the same hyperparameters.
We select the adversarial learning rate $\alpha$ from $\left\{0.02, 0.05\right\}$ and adversarial step $K$ from $\left\{2, 3\right\}$.
More details of hyperparameters are shown in Table~\ref{tab:hyper_cm} and other details of implementations can be found in the original LCM paper~\citep{luo2023latent}.
\input{tables/hyper_cm}



\section{Additional Results}

\subsection{Results of Classification Accuracy Score}\label{app:cas}

\input{tables/cas_results}

Classification Accuracy Score (CAS)~\citep{ravuri2019cas} is proposed to evaluate the utility of the images produced by the generative model for downstream classification tasks.
The underlying motivation for this metric is that if the generative model captures the real data distribution, the real data distribution can be replaced by the model-generated data and achieve similar results on downstream tasks like image classification.

% Limited by computational resources,
Following the evaluation pipeline in \citet{ravuri2019cas}, we train the image classifier in two settings: only on synthetic data or real data augmented with synthetic data, and use the classifier to predict labels on the test set of real data.
Synthetic images are generated with a DDIM sampler under 20 NFEs.
We use ResNet-18~\citep{he2016deep} as the image classifier and train it for 200 epochs with a learning rate of 0.1 and a batch size of 128. 
We report CAS in the \texttt{CIFAR-10} dataset at a resolution of 32$\times$32 in Table~\ref{tab:cas_results}.
The results indicate that our method consistently performs better than other baseline methods on CAS metric in both settings. Although CAS with synthetic data cannot surpass real data, it demonstrates significant potential for enhancing classifier accuracy when employed as an augmentation technique alongside real data.

\input{tables/atvsts}
\subsection{Comparison to TS-DDIM}
\citet{li2024alleviating} introduces another approach named Time-Shift (TS) to alleviate the DPM distribution mismatch by searching for coupled time steps in sampling.
Table~\ref{tab:atvsts} shows the comparison between our AT method with TS on \texttt{CIFAR-10} with the DDIM Sampler.
Both methods are based on the ADM pretrained model~\citep{dhariwal2021diffusion} as a backbone, which is the same as Section~\ref{sec:dpm_exp}.
We observe our method consistently better than the TS method across various sampling steps.


\subsection{Results of More NFEs}
\label{app:more_nfes}
\input{tables/c10_more_nfe_results}
\input{tables/i64_more_nfe_results}

We present results obtained with various samplers under 100 or 200 NFEs on \texttt{CIFAR10} 32x32 and \texttt{ImageNet} 64x64 in Table~\ref{tab:c10_more_nfe} and Table~\ref{tab:i64_more_nfe}, respectively.
The results show that our method is still effective for samplers under hundreds of NFEs.



\subsection{Results of More Metrics}
\label{app:more_metrics}
\input{tables/sfid_is_c10}
\input{tables/precision_recall_c10}
\input{tables/sfid_is_i64}
\input{tables/precision_recall_i64}

We present the results of more generation quality metrics, including sFID, Inception Score (IS), Precision, and Recall, on \texttt{CIFAR10} 32x32 (Table~\ref{tab:sfid_is_c10} and Table~\ref{tab:pr_c10}) and \texttt{ImageNet} 64x64 (Table~\ref{tab:sfid_is_i64} and Table~\ref{tab:pr_i64}). 
The evaluation is performed following ~\cite{dhariwal2021diffusion}. 
We observe that our method shows effectiveness across these metrics.



\section{More Analysis}
\subsection{Efficient AT vs Standard AT}\label{app:at_ablation}


In this section, we conduct an ablation of the AT method in diffusion model training.
We compare the performance of our used efficient AT and a standard AT method PGD on \texttt{CIFAR-10} dataset at the resolution of 32$\times$32.
The adversarial step $K$ is set to be 3 for both methods.
We fine-tune both models from the same pretrained ADM model with 100K update iterations of the model. 
The results are shown in Table~\ref{tab:at_ablation_results}.
We report the results of 4 sampler settings (method-NFEs): IDDPM-50, DDIM-50, ES-20, and DPM-Solver-10.
\input{tables/at_ablation_results}

We observe that efficient AT achieves performance comparable to or even better than PGD with the same model update iterations while accelerating the training (2.6$\times$ speed-up). 
Thus, we propose applying the efficient AT method for our adversarial training framework.



\subsection{Convergence of AT on Diffusion Models}\label{app:convergence}

\begin{figure}[h]
    \centering
\includegraphics[width=0.5\textwidth]{pic/converge_from_scratch.pdf}
    \caption{The convergence of methods trained from scratch on \texttt{CIFAR-10} $32\times32$. We use the DDIM sampler with 50 NFEs for sampling.}\label{fig:convergence_from_scratch}
\end{figure}


\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{pic/converge_finetune.pdf}
    \caption{The convergence of methods fine-tuned from a same pretrained model on \texttt{CIFAR-10} $32\times32$. We compare the performance of methods on various samplers.}
    \label{fig:convergence_finetune}
\end{figure}

In classification tasks, adding adversarial perturbations usually slows the convergence of model training~\citep{freelb}. 
We are interested to see whether AT also affects the convergence of the diffusion training process. 

Firstly, we explore the convergence of models trained from scratch.
We utilize DDIM as the sampler with 50 NFEs and the results are shown in Figure~\ref{fig:convergence_from_scratch}. 
We observe that our AT method and ADM-IP exhibit slower convergence compared to ADM at the beginning (before 100K iterations), while as training more iterations (200K), our AT method shows a notable advantage.

Moreover, we explore the convergence of models under fine-tuning setting and the results are shown in Figure~\ref{fig:convergence_finetune}.
We observe under this setting, when given a pretrained diffusion model like ADM, fine-tuning it with our proposed AT improves performance faster than other baselines. 
Overall, we observe that incorporating AT with a diffusion framework does not affect the convergence of the model much, especially in the fine-tuning setting. 


\subsection{More Ablation Study}
\label{app:ablation_hyperparams}
\input{tables/alr_ablation_c10}
\input{tables/alr_ablation_i64}
\paragraph{Ablation on $\alpha$} We investigate the impact of adversarial learning rate $\alpha$ in our framework.
The results of various $\alpha$ on \texttt{CIFAR10} 32x32 and \texttt{ImageNet} 64x64 are shown in Table~\ref{tab:alr_ablation_c10} and Table~\ref{tab:alr_ablation_i64}, respectively.
We observe that $\alpha$ set to 0.1 is better on \texttt{CIFAR10} 32x32 and $\alpha=0.5$ is better for \texttt{ImageNet} 64x64. That says, the image in larger size corresponds to larger optimal perturbation level $\alpha$. We speculate this is because we use the perturbation measured under $\ell_{2}$-norm, where the $\ell_{2}$-norm of vector will increase with its dimension.

\input{tables/norm_ablation}
\paragraph{Ablation on perturbation norm} During our experiments, we adopt $\ell_{2}$-adversarial perturbation. Actually, perturbations in Euclidean space under different $\ell_{p}$ norm are equivalent with each other, e.g., for vector $\bdelta\in \mathbb{R}^{d}$, it holds $\|\bdelta\|_{\infty}\leq \|\bdelta\|_{2} \leq \sqrt{d}\|\bdelta\|_{\infty}$. Therefore, we select $\|\cdot\|_{2}$ as representation in our paper. Next, we explore the proposed ADM-AT under different adversarial perturbations.  

The results are in Table~\ref{tab:norm_ablation}. We found that our method under $\ell_2$-perturbation is more stable and indeed has better performance, thus we suggest to use $\ell_2$-perturbation as in the main body of this paper. 

\subsection{Qualitative Comparisons}

\begin{figure}[h]
    \centering
\includegraphics[width=0.7\textwidth]{pic/c10_vis.pdf}
    \caption{The qualitative comparsions of ADM-AT (top, FID 6.60), ADM-IP (middle, FID 7.81), and ADM (bottom, FID 10.58) on \texttt{CIFAR10} $32\times32$. We use the IDDPM sampler with 20 NFEs for sampling.}\label{fig:vis_cifar}
\end{figure}


\begin{figure}[h]
    \centering
\includegraphics[width=0.7\textwidth]{pic/img64_vis.pdf}
    \caption{The qualitative comparsions of ADM-AT (top, FID 6.20), ADM-IP (middle, FID 8.40) and ADM (bottom, FID 8.32) on \texttt{ImageNet} $64\times64$. We use the DDIM sampler with 20 NFEs for sampling.}\label{fig:vis_imagenet64}
\end{figure}

\begin{figure}[h]
    \centering
\includegraphics[width=0.8\textwidth]{pic/lcm_vis1.pdf}
    \caption{The qualitative comparsions of LCM (left) and LCM-AT (right) with one-step generation. The text prompt is \textit{A photo of beautiful mountain with realistic sunset and blue lake, highly detailed, masterpiece.}}\label{fig:lcm_vis1}
\end{figure}

\begin{figure}[h]
    \centering
\includegraphics[width=0.8\textwidth]{pic/lcm_vis2.pdf}
    \caption{The qualitative comparsions of LCM (left) and LCM-AT (right) with one-step generation. The text prompt is \textit{Astronaut in a jungle, cold color palette, muted colors, detailed, 8k.}}\label{fig:lcm_vis2}
\end{figure}

Figure~\ref{fig:vis_cifar},~\ref{fig:vis_imagenet64},~\ref{fig:lcm_vis1},~\ref{fig:lcm_vis2} show the qualitative comparisons between our proposed AT method and baselines. Our proposed AT method generates more realistic and higher-fidelity samples. We attribute this to our AT algorithm mitigates the distribution mismatch problem.


