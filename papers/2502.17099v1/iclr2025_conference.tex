
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
% \input{math_commands.tex}

\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsfonts}
\usepackage{algorithm}
% \usepackage{algorithmic}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{xcolor}
\usepackage{makecell}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathtools}
% \usepackage{natbib}
\usepackage{thmtools}
\usepackage{thm-restate}
% \usepackage{pdfsync}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{thm-restate}
\usepackage{multicol}
\usepackage{wrapfig}
% \definecolor{rebuttal}{HTML}{0000FF}


\newcommand{\bX}{\text{\boldmath{$X$}}}
\newcommand{\bw}{\text{\boldmath{$w$}}}
\newcommand{\bW}{\text{\boldmath{$W$}}}
\newcommand{\bH}{\text{\boldmath{$H$}}}
\newcommand{\bHtil}{\tilde{\text{\boldmath{$H$}}}}
\newcommand{\bytil}{\tilde{\text{\boldmath{$y$}}}}
\newcommand{\bxtil}{\tilde{\text{\boldmath{$x$}}}}
\newcommand{\bwtil}{\tilde{\text{\boldmath{$w$}}}}
\newcommand{\bB}{\text{\boldmath{$B$}}}
\newcommand{\bD}{\text{\boldmath{$D$}}}
\newcommand{\bR}{\text{\boldmath{$R$}}}
\newcommand{\br}{\text{\boldmath{$r$}}}
\newcommand{\bQ}{\text{\boldmath{$Q$}}}
\newcommand{\bJ}{\text{\boldmath{$J$}}}
\newcommand{\bZ}{\text{\boldmath{$Z$}}}
\newcommand{\bmu}{\text{\boldmath{$\mu$}}}


\newcommand{\bz}{\boldsymbol{z}}
\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\bk}{\boldsymbol{k}}
\newcommand{\bbf}{\boldsymbol{f}}
\newcommand{\be}{\boldsymbol{e}}
\newcommand{\bdelta}{\boldsymbol{\delta}}
\newcommand{\bepsilon}{\boldsymbol{\epsilon}}
\newcommand{\bxi}{\boldsymbol{\xi}}
\newcommand{\bXi}{\boldsymbol{\Xi}}
\newcommand{\bzeta}{\boldsymbol{\zeta}}
\newcommand{\wpre}{\blodsymbol{w}_{\text{pre}}}
\newcommand{\ba}{\boldsymbol{a}}
\newcommand{\bs}{\boldsymbol{s}}
\newcommand{\bu}{\boldsymbol{u}}
\newcommand{\bb}{\boldsymbol{b}}
\newcommand{\bg}{\boldsymbol{g}}
\newcommand{\by}{\boldsymbol{y}}
\newcommand{\bY}{\boldsymbol{Y}}
\newcommand{\bS}{\boldsymbol{S}}
\newcommand{\bU}{\boldsymbol{U}}
\newcommand{\bzero}{\boldsymbol{0}}
\newcommand{\bone}{\boldsymbol{1}}
\newcommand{\beps}{\boldsymbol{\epsilon}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bphi}{\boldsymbol{\phi}}

\newcommand{\bl}{\boldsymbol{l}}
\newcommand{\bI}{\boldsymbol{I}}
\newcommand{\bh}{\boldsymbol{h}}
\newcommand{\bq}{\boldsymbol{q}}
\newcommand{\bA}{\boldsymbol{A}}
\newcommand{\bM}{\boldsymbol{M}}
\newcommand{\bV}{\boldsymbol{V}}
\newcommand{\bbP}{\mathbb{P}}
\newcommand{\bbI}{\mathbb{I}}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bbC}{\mathbb{C}}
\newcommand{\grad}{\mathrm{grad}}
\newcommand{\bt}{\boldsymbol{t}}
\newcommand{\bv}{\boldsymbol{v}}
\newcommand{\col}{\text{col}}
\newcommand{\imcol}{\text{im2col}}
\newcommand{\row}{\text{row}}
\newcommand{\median}{\mathsf{med}}
\newcommand{\dist}{\text{dist}}
\newcommand{\erf}{\text{erf}}
\newcommand{\erfc}{\text{erfc}}
\newcommand{\vect}{\mathsf{vec}}
\newcommand{\diag}{\mathsf{diag}}
\newcommand{\back}{\mathsf{Back}}
\newcommand{\sW}{\mathsf{W}}
\newcommand{\dirac}{\delta_{\text{Dirac}}}

\DeclareMathOperator*{\argmin}{arg\,min}

\newtheorem{result}{\indent \em Result}
\newtheorem{conj}{\textbf{Conjecture}}
\newtheorem{assumption}{\textbf{Assumption}}
\newtheorem{definition}{\textbf{Definition}}
\newtheorem{corollary}{\textbf{Corollary}}
\newtheorem{lemma}{\textbf{Lemma}}
\newtheorem{theorem}{\textbf{Theorem}}
\newtheorem{proposition}{\textbf{Proposition}}
\newtheorem{remark}{\textbf{Remark}}
\newtheorem{apprx}{\textbf{Approximation}}
\newtheorem{example}{\textbf{Example}}
\newtheorem{claim}{\textbf{Claim}}
\newtheorem{fact}{\indent \em Fact}
\newtheorem{step}{\em Step}


\newcommand{\nn}{\nonumber}
\newcommand{\scr}{\scriptstyle}
\newcommand{\disp}{\displaystyle}
\newcommand{\mE}{\mathbb{E}}
\newcommand{\rank}{\mathrm{rank}}
\newcommand{\Var}{\mathsf{Var}}
%\newcommand{\Var}{\textrm{Var}}
\newcommand{\Cov}{\textrm{Cov}}
\newcommand{\TV}{\mathsf{TV}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cZ}{\mathcal{Z}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cW}{\mathcal{W}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\dalpha}{\dot{\alpha}}
\newcommand{\dbeta}{\dot{\beta}}

\newcommand{\tx}{\tilde{\bx}}
\newcommand{\tg}{\tilde{g}}
\newcommand{\ty}{\tilde{y}}
\newcommand{\tz}{\tilde{z}}
\newcommand{\ts}{\tilde{s}}
\newcommand{\tw}{\tilde{w}}
\newcommand{\tu}{\tilde{u}}
\newcommand{\tp}{\tilde{p}}
\newcommand{\tq}{\tilde{q}}
\newcommand{\tR}{\Tilde{R}}
\newcommand{\tX}{\Tilde{X}}
\newcommand{\tY}{\Tilde{Y}}
\newcommand{\tU}{\Tilde{U}}
\newcommand{\tP}{\Tilde{P}}
\newcommand{\tZ}{\Tilde{Z}}
\newcommand{\tN}{\Tilde{N}}
\newcommand{\tf}{\tilde{f}}
\newcommand{\talpha}{\tilde{\alpha}}
\newcommand{\tbeta}{\tilde{\beta}}
\newcommand{\tlambda}{\tilde{\lambda}}
\newcommand{\tepsilon}{\tilde{\epsilon}}
\newcommand{\ttheta}{\Tilde{\boldsymbol{\theta}}}
\newcommand{\tr}{\mathrm{tr}}



\newcommand{\chZ}{\check{Z}}
\newcommand{\chN}{\check{N}}
\newcommand{\chR}{\check{R}}

\newcommand{\ut}{\underline{t}}
\newcommand{\ux}{\underline{x}}
\newcommand{\uy}{\underline{y}}
\newcommand{\uu}{\underline{u}}
\newcommand{\uh}{\underline{h}}
\newcommand{\uhy}{\underline{\hat{y}}}
\newcommand{\up}{\underline{p}}

\newcommand{\brw}{\breve{w}}
\newcommand{\brR}{\breve{R}}

\newcommand{\hW}{\hat{W}}
\newcommand{\hR}{\hat{R}}
\newcommand{\hs}{\hat{s}}
\newcommand{\hw}{\hat{w}}
\newcommand{\hY}{\hat{Y}}
\newcommand{\hy}{\hat{y}}
\newcommand{\hz}{\hat{z}}
\newcommand{\hZ}{\hat{Z}}
\newcommand{\hv}{\hat{v}}
\newcommand{\hN}{\hat{N}}
\newcommand{\hX}{\hat{X}}
\newcommand{\hhz}{\hat{\hat{z}}}
\newcommand{\hhs}{\hat{\hat{s}}}
\newcommand{\hhw}{\hat{\hat{w}}}
\newcommand{\hhv}{\hat{\hat{v}}}

\newcommand{\baralpha}{\bar{\alpha}}
\newcommand{\barbeta}{\bar{\beta}}
\newcommand{\bargamma}{\bar{\gamma}}
\newcommand{\bartheta}{\bar{\theta}}

\newenvironment{oneshot}[1]{\@begintheorem{#1}{\unskip}}{\@endtheorem}

\providecommand{\lemmaname}{Lemma}
\providecommand{\theoremname}{Theorem}





\providecommand{\lemmaname}{Lemma}
\providecommand{\theoremname}{Theorem}

\makeatother

\providecommand{\lemmaname}{Lemma}
\providecommand{\remarkname}{Remark}
\providecommand{\theoremname}{Theorem}

% \newcommand{\yi}[1]{\textcolor{red}{#1}}
% \newcommand{\revise}[1]{\textcolor{blue}{#1}}
% \newcommand{\fix}{\marginpar{FIX}}
% \newcommand{\new}{\marginpar{NEW}}

\title{Improved Diffusion-based Generative Model with Better Adversarial Robustness}


% \title{Formatting Instructions for ICLR 2025 \\ Conference Submissions}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
\renewcommand\footnotemark{}
\author{
    \!\!Zekun Wang$^{*1}$, Mingyang Yi$^{*\dagger2}$, Shuchen Xue$^{3,4}$, Zhenguo Li$^{5}$, Ming Liu$^{\dagger1}$, Bing Qin$^{1}$, \\
    \textbf{Zhi-Ming Ma$^{3,4}$}%
    \thanks{$^*$Equal contribution}%
    \thanks{$^{\dagger}$Corresponding author}\\
    $^{1}$Harbin Institute of Technology ~~~~ $^{2}$Renmin University of China \\
    $^{3}$Academy of Mathematics and Systems Science, Chinese Academy of Sciences \\
    $^{4}$University of Chinese Academy of Sciences ~~~~$^{5}$Huawei Noah’s Ark Lab \\
    \footnotesize{\texttt{zkwang@ir.hit.edu.cn}} ~~~\footnotesize{\texttt{yimingyang@ruc.edu.cn}}
    \\\footnotesize{\texttt{xueshuchen17@mails.ucas.ac.cn}}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\maketitle
\begin{abstract}
  Diffusion Probabilistic Models (DPMs) have achieved significant success in generative tasks. However, their training and sampling processes suffer from the issue of distribution mismatch. During the denoising process, the input data distributions differ between the training and inference stages, potentially leading to inaccurate data generation. To obviate this, we analyze the training objective of DPMs and theoretically demonstrate that this mismatch can be alleviated through Distributionally Robust Optimization (DRO), which is equivalent to performing robustness-driven Adversarial Training (AT) on DPMs. Furthermore, for the recently proposed Consistency Model (CM), which distills the inference process of the DPM, we prove that its training objective also encounters the mismatch issue. Fortunately, this issue can be mitigated by AT as well. Based on these insights, we propose to conduct efficient AT on both DPM and CM. Finally, extensive empirical studies validate the effectiveness of AT in diffusion-based models. The code is available at \url{https://github.com/kugwzk/AT_Diff}.
\end{abstract}


\section{Introduction}\label{sec:intro}
Diffusion Probabilistic Models (DPMs)~\citep{ ho2020denoising, song2020score,yi2024towards} have achieved remarkable success across a wide range of generative tasks such as image synthesis~\citep{dhariwal2021diffusion, Rombach_2022_CVPR, ho2022cascaded}, video generation~\citep{ho2022vdm,blattmann2023videoldm}, text-to-image generation~\citep{2021glide,dalle2,imagen}, \emph{etc}. The core mechanism of DPMs involves a forward diffusion process that progressively injects noise into the data, followed by a reverse process that learns to generate data by denoising the noise.
Unlike traditional generative models such as GANs\citep{goodfellow2014generative} or VAEs \citep{kingma2013auto}, which directly map an easily sampled latent variable (e.g., Gaussian noise) to the target data through a single network function evaluation (NFE), DPMs adopt a gradual denoising approach that requires multiple NFEs~\citep{song2020denoising, salimans2022progressive, lu2022dpmsa, ma2024surprising}. However, this noising-then-denoising process introduces a distribution mismatch between the training and sampling stages, potentially leading to inaccuracies in the generated outputs.
\par
Concretely, during the training stage, the model is learned to predict the noise in ground-truth noisy data derived from the training set. In contrast, during the inference stage, the input distribution is obtained from the output generated by the DPM in the previous step, which differs from the training phase, caused by the inaccurate estimation of the score function due to training \citep{song2021maximum,yi2023generalization} and the discretization error \citep{chen2022sampling,li2023towards,xue2024sa,xue2024accelerating} brought by sampling. 
Such distribution mismatches are referred to as \textit{Exposure Bias}, which has been discussed in auto-regressive language models \citep{bengio2015scheduled, ranzato2016sequence}. 
\par
Recently, the aforementioned distribution mismatch problem in diffusion has been also recognized by \citep{diffusion-ip,li2024on,ren2024multistep,es,li2024alleviating,Reflected_diffusion_models}. However, these studies are either rely on strong mismatch distributional assumptions (e.g., Gaussian) \citep{diffusion-ip,es,ren2024multistep} or incur significant additional computational costs \citep{li2024on}. 
This indicates that a more practical solution to this problem has been overlooked until now. To bridge this gap, we begin with the discrete DPM introduced in \citep{ho2020denoising}. Intuitively, although there is a mismatch between training and inference, the distributions of intermediate noise generated during the inference stage are close to the ground-truth distributions observed during training. Therefore, improving the distributional robustness \citep{yi2021improved,namkoong2019reliable,shapiro2017distributionally} (which measures the robustness of the model to distributional perturbations in training data) of DPM mitigates the distribution mismatch problem. To achieve this, we refer to Distribution Robust Optimization (DRO) \citep{shapiro2017distributionally,namkoong2019reliable}, which aims to improve the distributional robustness of models. We then prove that applying DRO to DPM is mathematically equivalent to implementing \emph{robustness-driven} Adversarial Training (AT) \citep{madry2018towards,freeat,yi2021improved} on DPM. \footnote{Note that the ``adversarial'' here refers to perturbation to input training data, instead of the adversarial of generator-discriminator in GAN \citep{goodfellow2014generative}.} Following the DRO framework, we also analyze the recently proposed diffusion-based Consistency Model (CM)~\citep{song2023consistency,luo2023latent} which distills the trajectory of DPM into a model with one NFE generation. We first prove that the training objective of CM similarly suffers from the mismatch issue as in multi-step DPM. Moreover, the issue can also be mitigated by implementing AT. Therefore, for both DPM and CM, we propose to apply efficient AT (e.g., ``Free-AT'' \citep{freeat}) during their training stages to mitigate the distribution mismatch problem.\footnote{Notably, the standard AT \citep{madry2018towards} solves a minimax problem that slows the training process. The efficient AT has no extra computational cost compared to the standard training ones \citep{freeat}.} Finally, we summarize our contributions as follows.
\begin{itemize}
\item We conduct an in-depth analysis of the diffusion-based models (DPM and CM) from a theoretical perspective and systematically characterize its distribution mismatch problem. 
% We find that minimizing the evidence lower bound of the negative log likelihood of training data implicitly optimizes the likelihood of all latent variables.
\item For both DPM and CM, we theoretically show that their mismatch problem is mitigated by DRO, which is equivalent to implementing AT with proved error bounds during training. 
\item We propose to conduct efficient AT on both DPM and CM in various tasks, including image generation on \texttt{CIFAR10} 32$\times$32\citep{krizhevsky2009learning} and \texttt{ImageNet} 64$\times$64 \citep{deng2009imagenet}, and zero-shot Text-to-Image (T2I) generation on MS-COCO 512$\times$512~\citep{mscoco}. Extensive experimental results illustrate the effectiveness of the proposed AT training method in alleviating the distribution mismatch of DPM and CM. 
\end{itemize}
\input{related_work}

% Despite their superior performance in generation tasks, one of the drawbacks of DPM is they require a considerable number of network function evaluations (NFEs) when compared to alternative methodologies like Generative Adversarial Networks (GANs) \citep{goodfellow2014generative} or Variational Autoencoders (VAEs) \citep{kingma2013auto}, which is a notable computational limitation for their broader application. A series of training-based and sampling-based methods \citep{song2020denoising,salimans2022progressive,lu2022dpmsa,xue2024sa,luo2024diff,xue2024accelerating,ma2024surprising} have been proposed to tackle the issues. Another drawback is the distribution mismatch phenomenon during the training and sampling stages. The input distribution of the training stage is the true noisy data distribution derived from the training set. In contrast, during the inference stage, the input distribution is derived from the output generated by the Diffusion model in the previous step, which differs from the training phase due to the inaccurate score function estimation during training and discretization error during sampling. Moreover, the discrepancy error accumulates across the whole generation process. Such distribution mismatches are commonly referred to as Exposure Bias or Distribution Shift. Some studies \citep{diffusion-ip,li2024on,ren2024multistep,es,li2024alleviating} attempt to alleviate the exposure bias in DPM. 
% \par
% In this work, we focus on modeling and mitigating the distribution mismatch in Diffusion Models. Our main contributions are summarized as follows:


\section{Preliminary}

\paragraph{Diffusion Probabilistic Models.} DPM~\citep{sohl2015deep, ho2020denoising} constructs the Markov chain $\bx_{t}$ by transition kernel $q(\bx_{t+1}\mid\bx_{t}) = \cN(\sqrt{\alpha_{t+1}}  \bx_{t}, (1-\alpha_{t+1})\bI)$, where $\alpha_1, \cdots, \alpha_T$ are in $[0, 1]$. Let $\baralpha_t := \Pi_{s=1}^t \alpha_s$, and $\bx_{0}\sim q$ be ground-truth data. Then, for $\bx_{t}$, it holds   
\begin{equation}\label{eq:xt}
    \small
        \bx_{t} = \sqrt{\baralpha_{t}}\bx_{0} + \sqrt{1 - \baralpha_{t}}\beps_{t} \qquad t=1, \cdots, T,
\end{equation}
with $\beps_{t}\sim \cN(0, \bI)$. The reverse process $p_{\btheta}(\bx_{t} \mid \bx_{t + 1})$ is parameterized as
\begin{equation}
    p_{\btheta}(\bx_{t} \mid \bx_{t + 1}) = \cN(\mu_{\btheta}(\bx_{t + 1}, t+1), \sigma_{t+1}^2 \bI),
\end{equation}
where $\sigma_{t+1}^2 = 1 - \alpha_{t+1}$.  %$\frac{1-\baralpha_t}{1-\baralpha_{t+1}}(1 - \alpha_{t+1})$. 
To learn $p_{\btheta}(\bx_{t} \mid \bx_{t + 1})$, a standard method is to minimize the following evidence lower bound of negative log-likelihood (NLL) \citep{ho2020denoising}, 
\begin{equation}\label{eq:nll loss}
    \small
    \begin{aligned}
        -\mE_{q}\left[\log{p}_{\btheta}(\bx_{0})\right] \leq \mE_{q}\left[-\log{\frac{p_{\btheta}(\bx_{0:T})}{q(\bx_{1:T}\mid \bx_{0})}}\right].
    \end{aligned}
\end{equation}
Here, minimizing the ELBO in the r.h.s. of above inequality links to $p_{\btheta}(\bx_{t} \mid \bx_{t+1})$ since it is equivalent to minimizing the following rewritten objective  
\begin{equation}\label{eq:rewrite nll upper bound}
    \small
    \min_{\btheta} \left\{D_{KL}(q(\bx_{T})\parallel p_{\btheta}(\bx_{T})) + \sum_{t = 0}^{T - 1}\underbrace{D_{KL}(q(\bx_{t}\mid \bx_{t + 1}) \parallel p_{\btheta}(\bx_{t}\mid \bx_{t + 1}))}_{L_{t}}\right\}, % + H(\bx_0),
\end{equation}
as in \citep{ho2020denoising,bao2022analytic,yi2023generalization}. Here, the conditional Kullback–Leibler (KL) divergence $D_{KL}(q(\bx_{t}\mid \bx_{t + 1})\parallel p(\bx_{t}\mid \bx_{t + 1})) = \int q(\bx_{t}\mid \bx_{t + 1})\log{\frac{q(\bx_{t}\mid \bx_{t + 1})}{p(\bx_{t}\mid \bx_{t + 1})}}d\bx_{t} d\bx_{t + 1}$ \citep{duchi2016lecture}, and minimizing $L_{t}$ is equivalent to solve the following noise prediction problem
\begin{equation}\label{eq:noise prediction}
    \small
    \min_{\btheta}\mE\left[\left\|\beps_{\btheta}(\sqrt{\baralpha_{t}}\bx_{0} + \sqrt{1 - \baralpha_{t}}\beps_{t}, t) - \beps_{t}\right\|^{2}\right]. 
\end{equation}
We use $\|\cdot\|_{p}$ to denote $\ell_{p}$-norm. Unless specified, the norm $\|\cdot\|$ refers to the $\ell_2$-norm $\|\cdot\|_2$. Since $\baralpha_{t}\rightarrow 0$ for $t\to T$, $\bx_{0}$ is obtained by conducting the reverse diffusion process $p_{\btheta}(\bx_{t}\mid \bx_{t + 1})$ starting from $\bx_{T}\sim\cN(0, \bI)$ and $\beps\sim\cN(0, \bI)$, under the learned model $\beps_{\btheta}$ with 
\begin{equation}\label{eq:transition}
    \small
       \bx_{t} = \frac{1}{\sqrt{\alpha_{t + 1}}}\left(\bx_{t + 1} - \frac{1 - \alpha_{t + 1}}{\sqrt{1 - \baralpha_{t + 1}}} \beps_{\btheta}(\bx_{t + 1}, t+1)\right) + \sqrt{1 - \alpha_{t + 1}}\beps.
\end{equation} 

\paragraph{Wasserstein Distance.} For integer $p>0$, $\Gamma(\mu, \nu)$ as the set of union distributions with marginal $\mu$ and $\nu$, the Wasserstein $p$-distance \citep{villani2009optimal} between distributions $\mu$ and $\nu$ with finite $p$-moments is
\begin{equation}
    \sW_{p}^{p}(\mu, \nu) = \inf_{\gamma \in \Gamma(\mu, \nu)} \mE_{(\bx,\by) \sim \gamma} \|\bx - \by\|_{p}^p.
\end{equation}
% where $\Gamma(\mu, \nu)$ is the set of all joint distributions whose marginal distributions correspond to $\mu$ and $\nu$.
% Furthermore, there exists a deterministic (implicit) reverse process pointed out by \citep{song2020denoising}
% \begin{equation}
%     \small
%         \bx_{t} = \sqrt{\baralpha_{t}}\left( \frac{\bx_{t+1} - \sqrt{1-\baralpha_{t+1}} \beps_{\btheta}(\bx_{t + 1}, t+1) }{\sqrt{\baralpha_{t+1}}} \right) + \sqrt{1 - \baralpha_{t}}\beps_{\btheta}(\bx_{t + 1}, t+1).
% \end{equation}

\section{Robustness-driven Adversarial Training of Diffusion Models}\label{sec:diffusion model as multi-step}
In this section, we formally show that the success of DPM relies on specific conditions, i.e., $\bx_{t}$ is close to $\bx_{t+1}$. Next, to mitigate the drawbacks brought by the restriction, we propose to consider the distribution mismatch problem as discussed in Section \ref{sec:intro}, and connect the problem to a rewritten ELBO. Finally, we apply DRO for this ELBO to mitigate the distribution mismatch problem and finally link it to AT to be implemented in practice. 

\subsection{How Does DPM Works in Practice?}\label{sec:How Does DPM Works in Practice}
Notably, minimizing \eqref{eq:rewrite nll upper bound} potentially obtains a sharp NLL under target distribution $q(\bx_{0})$. However, in the following proposition, we show that \eqref{eq:rewrite nll upper bound} also implicitly minimizes the NLL of each $\bx_{t}$.
\begin{restatable}{proposition}{elboupperbound}\label{pro:elbo upper bound}
    The minimization problem \eqref{eq:rewrite nll upper bound} is equivalent to minimizing an upper bound of $\mE_{q}[-\log{p_{\btheta}}(\bx_{t})]$ for any $0\leq t \leq T$.
\end{restatable}
The proof is provided in Appendix \ref{app:proofs in sec:diffusion model as multi-step}. It shows that though \eqref{eq:rewrite nll upper bound} is proposed to generate $\bx_{0}\sim q(\bx_{0})$, it also guides the model to generate $\bx_{t}$ such that $p_{\btheta}(\bx_{t})$ approximates the ground-truth distribution $q(\bx_{t})$. The conclusion is nontrivial as minimizing the ELBO of NLL $\mE_{q}\left[-\log{p}_{\btheta}(\bx_{0})\right]$ does not necessarily impose any restrictions on $\bx_{t}$ for $t \geq 1$. % Moreover, it does not necessarily make $\bx_{t}$ follow $q(\bx_{t})$ during the inference stage, to generate $\bx_{0}\sim q(\bx_{0})$.   
\par
Next, we will further explain why \eqref{eq:rewrite nll upper bound} leads to a small NLL of $\bx_{t}$. In $L_{t}$ of \eqref{eq:rewrite nll upper bound}, $p_{\btheta}(\bx_{t}\mid \bx_{t + 1})$ approximates $q(\bx_{t}\mid \bx_{t + 1})$ with $\bx_{t + 1}\sim q(\bx_{t + 1})$ representing ground-truth data. Consequently, $p_{\btheta}(\bx_{t})$ approximates $q(\bx_{t})$ by recursively applying such a relationship as in the following proposition. 
% \begin{remark}
% If the NLL $-\mE_{q}\left[\log{p}_{\btheta}(\bx_{0})\right]$ is the optimization target, it is not necessary for $p_{\btheta}(\bx_{t})$ to approximate $q(\bx_{t})$ in order to generate the desired $\bx_{0}$ because NLL  does not impose any requirements on $\bx_{t}$ ($t \geq 1$).
% \end{remark}
\par
\begin{restatable}{proposition}{elboupperboundxt}\label{pro:elbo upper bound xt}
    Suppose $p_{\btheta}(\bx_{t}\mid \bx_{t + 1})$ matches $q(\bx_{t}\mid \bx_{t + 1})$ well such that
    \begin{equation}
        \small
        L_{t} = D_{KL}(q(\bx_{t}\mid \bx_{t + 1}) \parallel p_{\btheta}(\bx_{t}\mid \bx_{t + 1}))\le \frac{\gamma}{T},
    \end{equation}
    and the discrepancy satisfies $D_{KL}(q(\bx_{T})\parallel p_{\btheta}(\bx_{T})) \le \gamma_0$, then for any $0\leq t \leq T$, we have  
    \begin{equation}\label{eq:accumulated error}
        \small
        D_{KL}(q(\bx_{t})\parallel p_{\btheta}(\bx_{t})) \leq D_{KL}(q(\bx_{T})\parallel p_{\btheta}(\bx_{T})) + L_{t} \le \gamma_0 + \frac{(T-t)\gamma}{T}.
    \end{equation}
\end{restatable}
The results is similarly obtained in \citep{chen2023improved}, while their result is applied for $D_{KL}(q(\bx_{0})\parallel p_{\btheta_{0}})$, which is narrowed compared with Proposition \ref{pro:elbo upper bound xt}. The proof is provided in Appendix \ref{app:proofs in sec:diffusion model as multi-step}, which formally explains why \eqref{eq:rewrite nll upper bound} results in $p_{\btheta}(\bx_{t})$ approximating $q(\bx_{t})$. However, this proposition is built upon small $L_{t}$, and notably, the error introduced by $L_{t}$ will be accumulated on the r.h.s. of \eqref{eq:accumulated error}, as it increases w.r.t. $t$. This phenomenon is caused by the \emph{distribution mismatch problem} discussed in Section \ref{sec:intro}. Concretely, in \eqref{eq:rewrite nll upper bound}, minimizing $L_{t}$ learns the transition probability $p_{\btheta}(\bx_{t}\mid \bx_{t + 1})$ based on $\bx_{t + 1}\sim q(\bx_{t + 1})$, while in practice, $\bx_{t}$ in \eqref{eq:transition} is generated from $\bx_{t + 1}\sim p_{\btheta}(\bx_{t + 1})$. The error between $p_{\btheta}(\bx_{t + 1})$ and $q(\bx_{t + 1})$ will propagates into the error between $p_{\btheta}(\bx_{t})$ and $q(\bx_{t})$ as in \eqref{eq:accumulated error}.  
\par
Therefore, owing to the existence of distribution mismatch, only if $L_{t}$ is minimized, the gap between $p_{\btheta}(\bx_{t})$ and $q(\bx_{t})$ can be guaranteed. However, the following proposition proved in Appendix \ref{app:proofs in sec:diffusion model as multi-step} indicates that $L_{t}$ is theoretically minimized with restrictions. 
\begin{restatable}{proposition}{gaussianinverse}
    $L_{t}$ in \eqref{eq:rewrite nll upper bound} is well minimized, only if $q(\bx_{t + 1})$ is Gaussian or $\|\bx_{t + 1} - \bx_{t}\|\to 0$.
\end{restatable}
\par
In practice, the $q(\bx_{t + 1})$ is usually non-Gaussian. Besides, the gap $\|\bx_{t + 1} - \bx_{t}\|$ is not necessarily small, especially for samplers with few sampling steps, e.g., DDIM \citep{song2020denoising}, DPM-Solver \citep{lu2022dpm}. Therefore, in practice, the accumulated error in \eqref{eq:accumulated error} caused by the distribution mismatch problem may become large, and degenerate the quality of $\bx_{0}$. 


\begin{figure}[t]
		\centering
        \vspace{-0.2in}
    	\includegraphics[scale=0.5]{./pic/adversarial_training.pdf}
		\caption{A comparison between standard training and the proposed distributional robust optimization in \eqref{eq:dro objective}. When minimizing $D_{KL}(\tq_{t}(\bx_{t}\mid \bx_{t + 1})\parallel p_{\btheta}(\bx_{t}\mid \bx_{t + 1}))$, the $\bx_{t + 1}$ is sampled from $\tq_{t}(\bx_{t + 1})$, such that both $\tq_{t}(\bx_{t + 1})$ in training stage and $p_{\btheta}(\bx_{t + 1})$ in inference stage are in $B_{D_{KL}}(q(x_{t + 1}), \eta_{0})$, so that $p_{\btheta}(\bx_{t})$ tends to locates in $B_{D_{KL}}(q(x_{t}), \eta_{0})$ as well as $\tq_{t}(\bx_{t})$. Then, the distributional robustness captured by \eqref{eq:dro objective} guarantees the generated $p_{\btheta}(\bx_{t})$ always locates around $q(\bx_{t})$ for all $t$.}
		\vspace{-0.2in}
		\label{fig:adversarial training}
	\end{figure}

\subsection{Distributional Robustness in DPM}\label{sec:Distributional Robustness in DPM}
% Then the condition $D_{KL}(q(\bx_{t - 1}\mid \bx_{t}) \parallel p_{\btheta}(\bx_{t - 1}\mid \bx_{t})) \approx 0$ that enables the stable generating process is broken. Thus learning the transition probability $p_{\btheta}(\bx_{t - 1}\mid \bx_{t})$ by matching $q(\bx_{t - 1}\mid \bx_{t})$ is not a wise choice. Next, we consider reformulate the formulation of $p_{\btheta}(\bx_{t - 1}\mid \bx_{t})$. 
% \par
Inspired by the discussion above, we propose a new training objective as the sum of NLLs under $\bx_{t}$,  
\begin{equation}\label{eq:new objective}
    \small
    \min_{\btheta}\cL(\btheta) = \sum_{t = 0}^{T}\mE_{q}\left[-\log{p_{\btheta}}(\bx_{t})\right].
\end{equation} 
Then the following proposition constructs ELBOs for each of $\mE_{q}[-\log{p_{\btheta}}(\bx_{t})]$. 
\begin{restatable}{proposition}{advelboupperbound}\label{pro:adv elbo upper bound}
    For any distribution $\tq$ satisfies $\tq(\bx_{t}) = q(\bx_{t})$ for specific $t$, we have  
    \begin{equation}\label{eq:new elbo}
        \small
        \mE_{q}\left[-\log{p_{\btheta}}(\bx_{t})\right] \le \underbrace{D_{KL}(\tq(\bx_{t}\mid \bx_{t + 1}) \parallel p_{\btheta}(\bx_{t}\mid \bx_{t + 1}))}_{L^{\tq}_{t}} + C, 
    \end{equation}
    for a constant $C$ independent of $\btheta$. 
\end{restatable}
The proof is in Appendix \ref{app:proofs in sec:Distributional Robustness in DPM}. This proposition generalizes the results in Proposition \ref{pro:elbo upper bound} since $\tq$ can be taken as $q$ in Proposition \ref{pro:elbo upper bound}. During minimizing $L^{\tq}_{t}$, the transition probability $p_{\btheta}(\bx_{t}\mid \bx_{t + 1})$ matches $\tq(\bx_{t}\mid \bx_{t + 1})$, while $\bx_{t + 1}\sim \tq(\bx_{t + 1})$ in the training stage has no restriction. Thus, one may take $\tq(\bx_{t + 1}) \approx p_{\btheta}(\bx_{t + 1})$, then in $L_{t}^{\tq}$, $p_{\btheta}(\bx_{t}\mid \bx_{t + 1})$ matches $\tq(\bx_{t}\mid \bx_{t + 1})$ leads $p_{\btheta}(\bx_{t})\approx \tq(\bx_{t}) = q(\bx_{t})$, which mitigates the distribution mismatch problem, when minimizing such $L_{t}^{\tq}$. 
\par
Unfortunately, for each $t$, obtaining such specific $\tq_{t}(\bx_{t + 1}) =  p_{\btheta}(\bx_{t + 1})$ is computationally expensive \citep{li2024on}, which prevents us using desired $\tq_{t}(\bx_{t + 1})$. However, we know $p_{\btheta}(\bx_{t + 1})$ is around $q(\bx_{t + 1})$. Therefore, by borrowing the idea from DRO \citep{shapiro2017distributionally}, for each $t$, we propose to minimize the maximal value of $L_{t}^{\tq_{t}}$ over all possible $\tq_{t}(\bx_{t + 1})$ around $q(\bx_{t + 1})$. This leads to a small $L_{t}^{p_{\btheta}}$, as $p_{\btheta}(\bx_{t + 1})$ locates around $q(\bx_{t + 1})$, so that is included in the ``maximal range''. Technically, the DRO-based EBLO of \eqref{eq:new elbo} is formulated as follows. Here $p_{\btheta}(x_{t + 1})$ is supposed in $B_{D_{KL}}(q(\bx_{t + 1}), \eta_{0})$, and it capatures the distributional robustness of $p_{\btheta}(\bx_{t}\mid \bx_{t + 1})$ w.r.t. input $\bx_{t + 1}$. 
\begin{equation}\label{eq:dro objective}
    \small
    \begin{aligned}
         & \min_{\btheta} \sum_{t = 0}^{T - 1} L_{t}^{\mathrm{DRO}}(\btheta) = \min_{\btheta} \sum_{t = 0}^{T - 1} \sup_{\tq_{t}(\bx_{t + 1})\in B_{D_{KL}}(q(\bx_{t + 1}), \eta_{0})}D_{KL}(\tq_{t}(\bx_{t}\mid \bx_{t + 1})\parallel p_{\btheta}(\bx_{t}\mid \bx_{t + 1})); \\
         & s.t. \qquad \tq_{t}(\bx_{t}) = q(\bx_{t}).
    \end{aligned}
\end{equation}
Here $\tq_{t}(\bx_{t + 1})\in B_{D_{KL}}(q(\bx_{t + 1}), \eta_{0})$ means $D_{KL}(q(\bx_{t + 1})\parallel \tq_{t}(\bx_{t + 1})) \leq \eta_{0}$. By solving problem \eqref{eq:dro objective}, if the desired $\tq_{t}(\bx_{t + 1}) = p_{\btheta}(\bx_{t + 1})$ is in $B_{D_{KL}}(q(\bx_{t + 1}), \eta_{0})$, then the conditional probability in \eqref{eq:dro objective} transfers $\bx_{t + 1}\sim p_{\btheta}(\bx_{t + 1})$ to target $\bx_{t}\sim q(\bx_{t})$ is learned, which mitigates the distribution mismatch problem. The theoretical clarification is in the following Proposition proved in Appendix \ref{app:proofs in sec:Distributional Robustness in DPM}, which indicates that small DRO loss \eqref{eq:dro objective} guarantees the quality of generated $\bx_{0}$. 
\begin{restatable}{proposition}{effectivenessofdro}\label{pro:effectiveness}
    If $L_{t}^{\mathrm{DRO}}(\btheta) \leq \eta_{0}$ in \eqref{eq:dro objective} for all $t$, and $D_{KL}(q(\bx_{T})\parallel p_{\btheta}(\bx_{T})) \leq \eta_{0}$, then $D_{KL}(q(x_{0})\parallel p_{\btheta}(\bx_{0})) \leq \eta_{0}$.
\end{restatable}
\par
Up to now, we do not know how to compute the DRO-based training objective \eqref{eq:dro objective} we derived. Fortunately, the following theorem corresponds \eqref{eq:dro objective} to a ``perturbed'' noise prediction problem similar to \eqref{eq:noise prediction}. The theorem is proved in Appendix \ref{app:proofs in sec:Distributional Robustness in DPM}. 
\begin{restatable}{theorem}{equivalencedro}\label{thm:equivalence}
    There exists $\bdelta_{t}$ depends on $\bx_{0}$ and $\beps_{t}$ makes \eqref{eq:eps dro} equivalent to problem \eqref{eq:dro objective}. 
    \begin{equation}\label{eq:eps dro}
    \small
        \min_{\btheta}\sum_{t=0}^{T - 1}\mE_{q(\bx_{0}),\beps_{t}}\left[\left\|\beps_{\btheta}(\sqrt{\baralpha_{t}}\bx_{0} + \sqrt{1 - \baralpha_{t}}\beps_{t} + \bdelta_{t}, t) - \beps_{t} - \frac{\bdelta_{t}}{\sqrt{1 - \baralpha_{t}}}\right\|^{2}\right].
\end{equation}
\end{restatable}
This theorem connects the proposed DRO problem \eqref{eq:dro objective} with noise prediction problem \eqref{eq:eps dro}. Naturally, we can solve \eqref{eq:eps dro}, if we know the exact $\bdelta_{t}$. Fortunately, we have the following proposition to characterize the range of $\bdelta_{t}$, and it is proved in Appendix \ref{app:proofs in sec:Distributional Robustness in DPM}. 
% In fact, the proposed objective \eqref{eq:eps dro} originate$s from the distributional robust optimization (DRO) \citep{yi2021improved}. However, minimizing \eqref{eq:eps dro} requires solving the inner maximization problem over a distributions set, which can be hard to be implemented in practice \citep{levy2020large}. To tackle this, we consider borrowing the training objective from adversarial training \citep{madry2018towards}, which is shown to be an approximation to DRO \citep{yi2021improved}. Before presenting the adversarial training objective, we need the following proposition. 
\begin{restatable}{proposition}{wtokl}\label{pro:ddpm adv}
    For $\eta > 0$ and $\bdelta_{t}$ in \eqref{eq:eps dro}, $\|\bdelta_{t}\|_{1} \leq \eta$ holds with probability at least $1 - \sqrt{2(1 - \baralpha_{t}) / \eta}$. 
\end{restatable}
The proposition indicates that for any $\bdelta_{t}$ depends on $\bx_{0}, \beps_{t}$ in \eqref{eq:eps dro}, it is likely in a small range (measured under any $\ell_{p}$-norm, since they can bound each other in Euclidean space). Thus, to resolve \eqref{eq:eps dro} (so that \eqref{eq:dro objective}), we propose to directly consider the following adversarial training \citep{madry2018towards} objective with the perturbation $\bdelta$ is taken over its possible range as proved in Proposition \ref{pro:ddpm adv}, which captures the input (instead of distribution) robustness of model $\beps_{\btheta}$. 
\begin{equation}\label{eq:dpm at}
    \small
    \min_{\btheta}\sum_{t=0}^{T - 1}\mE_{q(\bx_{0})}\left[\mE_{q(\bx_{t}\mid \bx_{0})}\left[\sup_{\bdelta: \|\bdelta\| \leq \eta}\left\|\beps_{\btheta}(\sqrt{\baralpha_{t}}\bx_{0} + \sqrt{1 - \baralpha_{t}}\beps_{t} + \bdelta) - \beps_{t} - \frac{\bdelta}{\sqrt{1 - \baralpha_{t}}}\right\|^{2}\right]\right].
\end{equation}
We present a fine-grained connection between \eqref{eq:dpm at} and classical AT in Appendix \ref{app:connection to AT}. Notably, our objective \eqref{eq:dpm at} is different from the ones in \citep{diffusion-ip}, whereas $\bdelta$ in it is a Gaussian, and $\beps_{\btheta}$ predicts $\beps_{t}$ instead of $\beps_{t} + \bdelta / \sqrt{1 - \baralpha_{t}}$ as ours. 
\par
To make it clear, we summarize the rationale from DRO objective \eqref{eq:dro objective} to AT our objective \eqref{eq:dpm at}. Since Theorem \ref{thm:equivalence} shows solving \eqref{eq:dro objective} is equivalent to \eqref{eq:eps dro}, which conducts noise prediction \eqref{eq:noise prediction} with a perturbation $\bdelta_{t}$ in a small range added (Proposition \ref{pro:ddpm adv}). Thus, we propose to minimize the maximal loss over the possible $\bdelta_{t}$, which is indeed our AT objective \eqref{eq:dpm at}.   

\section{Adversarial Training under Consistency Model}\label{sec:adversarial under consistency model}
Although the DPM generates high-quality target data $\bx_{0}$, the multi-step denoising process \eqref{eq:transition} requires numerous model evaluations, which can be computationally expensive. To resolve this, the diffusion-based consistency model (CM) is proposed in \citep{song2023consistency}. Consistency model $f_{\btheta}(\bx_{t}, t)$ transfers $\bx_{t}\sim q(\bx_{t})$ into a distribution that approximates the target $q(\bx_{0})$. $f_{\btheta}$ is optimized by the following consistency distillation (CD) loss \footnote{In practice,  \eqref{eq:cm objective} is updated under target model $f_{\btheta^{-}}(\Phi_{t}(\bx_{t + 1}), t)$ with exponential moving average (EMA) $\btheta^-$ under a stop gradient operation. \citep{song2023consistency} find that it greatly stabilizes the training process. In this section, we focus on the theory of consistency model and still use $\btheta$ in formulas.}
\begin{equation}\label{eq:cm objective}
    \small
    \min_{\btheta}\cL_{CD}(\btheta) = \sum_{t = 0}^{T - 1}\mE_{\bx_{t + 1}\sim q(\bx_{t + 1})}\left[d\left(f_{\btheta}(\Phi_{t}(\bx_{t + 1}), t), f_{\btheta}(\bx_{t + 1}, t + 1)\right)\right],
\end{equation}
where $\Phi_{t}(\bx_{t + 1})$ is a solution of a specific ordinary differential equation (ODE) (\eqref{eq:sde} in Appendix \ref{app:proofs of consistency model}) which is a deterministic function transfers $\bx_{t + 1}$ to $\bx_{t}$, i.e., $\Phi_{t}(\bx_{t + 1})\sim q(\bx_{t})$, and $d(\bx, \by)$ is a distance between $\bx$ and $\by$ e.g., $\ell_{1}, \ell_{2}$ distance. 
\begin{remark}
    In \citep{song2023consistency,luo2023latent}, the noisy data $\bx_{t}$ in \eqref{eq:cm objective} is described by an ODE  \eqref{eq:sde} in Appendix \ref{app:proofs of consistency model}. However, we use the discrete $\bx_{t}$ \eqref{eq:xt} here to unify the notations with Section \ref{sec:diffusion model as multi-step}. The two frameworks are mathematically equivalent as all $\bx_{t}$ in \eqref{eq:xt} located in the trajectory of ODE in \citep{song2023consistency}. More details of this claim refer to Appendix \ref{app:proofs of consistency model}.   
\end{remark}

Next, we use the following theorem to illustrate that solving problem \eqref{eq:cm objective} indeed creates $f_{\btheta}(\bx_{t}, t)$ with distribution close target $q(\bx_{0})$. The theorem is proved in Appendix \ref{app:proofs of consistency model}. 

\begin{restatable}{theorem}{expectedcdgap}\label{thm:expected cd gap}
    For $\cL_{CD}(\btheta)$ in \eqref{eq:cm objective} with $d(\cdot, \cdot)$ is $\ell_{2}$ distance, then $\sW_{1}(f_{\btheta}(\bx_{t}, t), \bx_{0}) \leq \sqrt{t\cL_{CD}(\btheta)}$ \footnote{Here $\sW_{1}(f_{\btheta}(\bx_{t}, t), \bx_{0})$ is the Wasserstein 1-distance between distributions of $f_{\btheta}(\bx_{t}, t)$ and $\bx_{0}$.}. 
\end{restatable}

Though solving problem \eqref{eq:cm objective} creates the desired CM $f_{\btheta}$, computing the exact $\Phi_{t}(\bx_{t + 1})$ involves solving an ODE as pointed out in Appendix \ref{app:proofs of consistency model}. Thus, in practice \citep{song2023consistency,luo2023latent}, the $\Phi_{t}(\bx_{t + 1})$ is approximated by a computable numerical estimation $\hat{\Phi}_{t}(\bx_{t + 1}, \beps_{\bphi})$ of it, e.g., Euler (\eqref{eq:estimated phi} in Appendix \ref{app:proof of cd upper bound}) or DDIM \citep{song2023consistency}, where $\beps_{\bphi}$ is a pretrained noise prediction model as in \eqref{eq:noise prediction}. Therefore, the practical training objective of \eqref{eq:cm objective} becomes
\begin{equation}\label{eq:cd objective}
    \small
        \min_{\btheta}\sum_{t = 0}^{T - 1}\hat{\cL}_{CD}(\btheta) = \mE_{\bx_{t + 1}\sim q(\bz_{t})}\left[d\left(f_{\btheta}(\hat{\Phi}_{t}(\bx_{t + 1}, \beps_{\bphi}), t), f_{\btheta}(\bx_{t + 1}, t + 1)\right)\right].
\end{equation}

In \eqref{eq:cd objective}, $\hat{\Phi}_{t}(\bx_{t + 1}, \beps_{\bphi})$ is an estimation to $\Phi_{t}(\bx_{t + 1})$, which causes an inaccurate training objective $\hat{\cL}_{CD}$ in \eqref{eq:cd objective}, compared with target $\cL_{CD}$ \eqref{eq:cm objective}. Thus, this results in the distribution mismatch problem in CM, as in DPM of Section \ref{sec:diffusion model as multi-step}. However, similar to Section \ref{sec:Distributional Robustness in DPM}, if we train $f_{\btheta}$ with robustness to the gap between $\hat{\Phi}_{t}(\bx_{t + 1}, \beps_{\bphi})$ and $\Phi_{t}(\bx_{t + 1})$, the distribution mismatch problem in CM is mitigated. 
\par
Technically, suppose $\Phi_{t}(\bx_{t + 1}) = \hat{\Phi}_{t}(\bx_{t + 1}, \beps_{\bphi}) +  \bdelta_{t}(\bx_{t + 1})$, we can consider minimizing the following adversarial training objective of CM, if $\|\bdelta_{t}(\bx_{t + 1})\| \leq \eta$ uniformly over $t$, for some constant $\eta$, so that the target $\Phi_{t}(\bx_{t + 1})$ is included in the maximal range as well.    
\begin{equation}\label{eq:cd at}
        \small
            \hat{\cL}_{CD}^{Adv}(\btheta) = \sum_{t = 0}^{T - 1}\mE_{\bx_{t + 1}}\left[\sup_{\|\bdelta\| \leq \eta}d\left(f_{\btheta}(\hat{\Phi}_{t}(\bx_{t + 1}, \beps_{\bphi}) + \bdelta, t), f_{\btheta}(\bx_{t + 1}, t + 1)\right)\right].
\end{equation}

By doing so, the learned model $f_{\btheta}$ can be robust to the perturbation brought by $\bdelta_{t}(\bx_{t + 1})$, so that results in a small $\cL_{CD}(\btheta)$, as well as the small $\sW_{1}(f_{\btheta}(\bx_{T}, T), \bx_{0})$ as proved in Theorem \ref{thm:expected cd gap}. Next, we use the following theorem to show that $\|\bdelta_{t}(\bx_{t + 1})\|$ is indeed small, and minimizing $\hat{\cL}_{CD}^{Adv}(\btheta)$ results in $f_{\btheta}(\bx_{T}, T)$ with distribution approximates $\bx_{0}$.  

 \begin{restatable}{theorem}{adversarialcd}\label{thm:cd upper bound}
    Under proper regularity conditions, for $0\leq t< T$, we have $\mE_{\bx_{t+1}}[\|\bdelta_{t}(\bx_{t + 1})\|] \leq o(1)$. On the other hand, it holds 
    \begin{equation}
        \small
        \sW_{1}(f_{\btheta}(\bx_{T}, T), \bx_{0}) \leq \sqrt{T\hat{\cL}_{CD}^{Adv}(\btheta) + o(1)}.
    \end{equation}
 \end{restatable}
The theorem is proved in Appendix \ref{app:proof of cd upper bound}, and it indicates that using the proposed adversarial training objective \eqref{eq:cd at} of CM indeed guarantees the learned CM transfers $\bx_{T}$ into data from $q(\bx_{0})$.  

\input{algorithm}

\section{Experiments}
\subsection{Algorithms} 
In the standard adversarial training method like Projected Gradient Descent (PGD) \citep{madry2018towards}, the perturbation $\bdelta$ is constructed by implementing numbers (3-8) of gradient ascents to $\bdelta$ before updating the model, which slows down the training process. To resolve this, we adopt an efficient implementation \citep{freeat} in Algorithms \ref{alg:adv dpm}, \ref{alg:adv cm} to solve AT \eqref{eq:dpm at} and \eqref{eq:cd at} of DPM and CM, \emph{which has similar computational cost compared to standard training}, and significantly accelerate standard AT. Notably, unlike PGD, in Algorithms \ref{alg:adv dpm} and \ref{alg:adv cm}, every maximization step of perturbation $\bdelta$ follows an update step of the model $\btheta$. 
% Therefore, the AT used in our work has faster convergence rates compared to \citep{madry2018towards}. 
Thus, the efficient AT do not require further back propagations to construct adversarial samples as in PGD.   
We provide a comparison between our efficient AT and standard AT (PGD) with the same update iterations of model $\btheta$ in Appendix~\ref{app:at_ablation}. Moreover, we observe that efficient AT can yield comparable and even better performance than PGD while accelerating the training (2.6$\times$ speed-up), further verifying the benefits of our efficient AT. \footnote{For the experts in AT, they would recognize that the AT in Algorithms \ref{alg:adv dpm}, \ref{alg:adv cm} actually constructs the adversarial augmented data to improve the performance of the model \citep{freelb,jiang2020smart,yi2021improved}.} 
% Thus, we adopt the efficient adversarial training method to train models, ensuring that the updated iterations of models are synchronized with the actual training iterations. 
  
\subsection{Performance on DPM}
\label{sec:dpm_exp}
\paragraph{Settings.} The experiments are conducted on the unconditional generation on \texttt{CIFAR-10} 32$\times$32 \citep{krizhevsky2009learning} and the class-conditional generation on \texttt{ImageNet} $64\times64$ \citep{deng2009imagenet}. Our model and training pipelines in adopted from ADM \citep{dhariwal2021diffusion} paper, where ADM is a UNet-type network \citep{ronneberger2015u}, with strong performance in image generation under diffusion model.

To save training costs, our methods and baselines are fine-tuned from pretrained models, rather than training from scratch.
By doing so, we can efficiently assess the performance of methods, which is more practical for general scenarios.
% For \texttt{CIFAR-10}, the pretrained ADM is trained using a batch size of 128 for 250K iterations with a learning rate set to 1e-4.
% For \texttt{ImageNet}, the pretrained model is trained with a batch size of 1024 for 400K iterations, employing a learning rate of 3e-4.
We also explore training from scratch in Appendix \ref{app:convergence}, which also verifies the effectiveness of our method in this regime. 
During training, we fine-tune the pretrained models (details are in Appendix~\ref{app:hyper_dm}) with batch size 128 for 150K iterations under learning rate 1e-4 on \texttt{CIFAR-10}, and batch size 1024 for 50K iterations under learning rate of 3e-4 on \texttt{ImageNet}.
For the hyperparameters of AT, we select the adversarial learning rate $\alpha$ from $\left\{0.05, 0.1, 0.5\right\}$ and the adversarial step $K$ from $\left\{3, 5\right\}$. 
% We report the best results among the checkpoints for all methods.
More details are in Appendix~\ref{app:hyper_dm}.

We use the Frechet Inception Distance (FID)~\citep{heusel2017gans} to evaluate image quality. Unless otherwise specified, 50K images are sampled for evaluation.
Other results of metric Classification Accuracy Score (CAS)~\citep{ravuri2019cas}, sFID, Inception Score, Precision, and Recall are in Appendix~\ref{app:cas} and~\ref{app:more_metrics} for comprehensive evaluation.

\paragraph{Baselines.}
For experiments on diffusion models, we consider the following baselines.
1): the original pretrained model.
Compared with it, we verify whether the models are overfitting during fine-tuning.
2): continue fine-tuning the pretrained model, which is fine-tuned with the standard diffusion objective \eqref{eq:noise prediction}. 
Compared to it, we validate whether performance improvements come only from more training costs.
We also compare with the existing typical method to alleviate the DPM distribution mismatch, 3): ADM-IP~\citep{diffusion-ip}, which adds a Gaussian perturbation to the input data to simulate mismatch errors during the training process.
The last two fine-tuning baselines are based on \textbf{the same} pretrained model and hyperparameters as in the original literature. 

\input{tables/subtable_cifar}
\input{tables/subtable_imagenet}

\paragraph{Results.} To verify the effectiveness of our AT method, we conduct experiments with four diffusion samplers: IDDPM \citep{dhariwal2021diffusion}, DDIM \citep{song2020denoising}, DPM-Solver \citep{lu2022dpmsa}, and ES \citep{es} under various NFEs. The sampler choices contain the three most popular samplers: IDDPM, DDIM, DPM-Solver, and ES, a sampler that scales down the norm of predicted noise to mitigate the distribution mismatch from the perspective of sampling. The experimental results of \texttt{CIFAR-10} and \texttt{ImageNet} are shown in Table~\ref{tab:C10} and Table~\ref{tab:I64}, respectively. Results of more than hundreds of NFEs are shown in Appendix~\ref{app:more_nfes}
\par
%具体数字
As can be seen, the proposed AT for DPM significantly improves the performance of the original pretrained model and outperforms the other baselines (continue fine-tuning and ADM-IP) overall for all diffusion samplers and NFEs we take. Moreover, we have the following observarions. 
% For example, we improve FID from $22.98$ to $18.40$ with DPM-Solver at only 5 NFEs and improve FID from $4.15$ to $3.07$ with DDIM at 50 NFEs on \texttt{CIFAR-10}. 
% On \texttt{ImageNet}, we improve the FID from $27.72$ to $17.36$ with DPM-Solver for only 5 NFEs and improve the FID from $5.48$ to $4.67$ with DDIM for 50 NFEs. 
%少步数好
\par
1): Fewer (practically used) sampling steps (5,10) will result in larger mismatching errors, while our AT method demonstrates significant improvements in this regime across various samplers, e.g., AT improves FID 27.72 to 17.36 under 5 NFEs DPM-Solver on \texttt{ImageNet}. This suggests that our method is indeed effective in alleviating the distribution mismatch of DPM.
% 在各个sampler上都好
The results also indicate that our method consistently beats the baseline methods, regardless of stochastic (IDDPM) or deterministic samplers (DDIM, DPM-Solver). 
2): The ES sampler results show that our AT is orthogonal to the sampling-based method to mitigate the distribution mismatch problem and can be combined to further alleviate the issue. Notably, we further verify in Appendix \ref{app:convergence} that our methods will not slow the convergence unlike AT in classification \citep{madry2018towards}. 
%收敛性
% In addition, we report the convergence of our methods in Appendix~\ref{app:convergence}.
% Under the continual fine-tuning setting, we observe that our efficient AT method achieves better performance with the same training iterations.
We also perform ablation analysis of hyperparameters in our AT framework in Appendix~\ref{app:ablation_hyperparams}.

\subsection{Performance on Latent Consistency Models}

\paragraph{Settings.} 
We further evaluate the proposed AT for consistency models on text-to-image generation tasks with Latent Consistency Models \citep{luo2023latent} Stable Diffusion (SD) v1.5~\citep{Rombach_2022_CVPR} backbone, which generates 512$\times$512 images.
Both our AT and the original LCM training (baseline) are trained from scratch with the same hyperparameters (the IP method \citep{diffusion-ip} is not applied straightforwardly).
The training set is LAION-Aesthetics-6.5+~\citep{laion} with hyperparameters following \citet{song2023consistency,luo2023latent}. 
%The learning rate is set at 8e-6 and the EMA rate of the target model at 0.95, following \citet{song2023consistency} and \citet{luo2023latent}. 
%We set the range of the guidance scale $[w_{min}, w_{max}] = [3,5]$ during training and use $w = 4$ in sampling because it performs better in our preliminary experiments, which is similar to DMD \citep{yin2024onestep}.
We select the adversarial learning rate $\alpha$ from $\left\{0.02, 0.05\right\}$ and adversarial step $K$ from $\left\{2, 3\right\}$.
The models are trained with a batch size of 64 for 100K iterations. More details are shown in Appendix~\ref{app:hyper_lcm}.

Following~\citet{luo2023latent} and \citet{ chen2024pixart}, we evaluate models on MS-COCO 2014~\citep{lin2014microsoft} at a resolution of 512$\times$512 by randomly drawing 30K prompts from its validation set. Then, we report the FID between the generated samples under these prompts and the reference samples from the full validation set following~\citet{imagen}. 
% \revise{a little redundant?}
We also report CLIP scores~\citep{clipscore} to evaluate the text-image alignment by CLIP-ViT-B/16.

\paragraph{Results.}
\input{tables/lcm_results}
The methods are evaluated under various sampling steps in Table~\ref{tab:lcm_res}, which shows that the LCM with AT consistently improves FID under various sampling steps. Besides, though the AT is not specified to improve text-image alignment, we observe that it has comparable or even better CLIP scores across various sampling steps, which shows that AT will not degenerate text-image alignment. 


\section{Conclusion}
In this paper, we novelly introduce efficient Adversarial Training (AT) in the training of DPM and CM to mitigate the issue of distribution mismatch between training and sampling. We conduct an in-depth analysis of the DPM training objective and systematically characterize the distribution mismatch problem. Furthermore, we prove that the training objective of CM similarly faces the distribution mismatch issue. We theoretically prove that DRO can mitigate the mismatch for both DPM and CM, which is equivalent to conducting AT. Experiments on image generation and text-to-image generation benchmarks verify the effectiveness of the proposed AT method in alleviating the distribution mismatch of DPM and CM.

\subsubsection*{Acknowledgments}
We thank anonymous reviewers for insightful feedback that helped improve the paper.
% Ming Liu and Mingyang Yi are the corresponding authors.
Zekun Wang, Ming Liu, Bing Qin are supported by the National Science Foundation of China (U22B2059, 62276083), the Human-Machine Integrated Consultation System for Cardiovascular Diseases (2023A003). 
They also appreciate the support from China Mobile Group Heilongjiang Co., Ltd.


\bibliography{reference}
\bibliographystyle{iclr2025_conference}

\newpage
\appendix
\input{appendix}
% \section{Appendix}
% You may include other additional sections here.


\end{document}
