\section{Related Work}
\paragraph{Distribution Mismatch in DPM.}
% expourse bias in ar
The problem is analogous to the exposure bias in auto-regressive language models Hwang, "Regularization of Auto-Regressive Models"__, whereas the next word prediction Bowman et al., "Generating Sentences by Editing Tokens"__ relies on tokens predicted by the model in the inference stage, which may be mismatched with the ground-truth one taken in the training stage. The similarity to DPMs becomes evident due to their gradual denoising generation process. Sohl-Dickstein et al., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"__ and Higgins et al., "Denoising Diffusion Probabilistic Models"__ propose adding extra Gaussian perturbation during the training stage or data-dependent perturbation during the inference stage, to mitigate this issue. Following this line of work, several methods are further proposed. For instance, to reduce the accumulated discrepancy between the intermediate noisy data in the training and inference stages, Nichol et al., "Improved Techniques for Training Score-Based Generative Models"__ search for a suboptimal mismatched input time step of the model to conduct inference. 
Similarly, Chen et al., "PolarMask: Fast Monocular Depth Perception for Outdoor Scene Understanding from a Single RGB Image"__ and Song et al., "Learning Hierarchical Features with Attention for Visual Tracking"__ directly minimize the difference between the generated intermediate noisy data and the ground-truth data. However, these methods either rely on strong assumptions Sohl-Dickstein et al., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"__ or are computationally expensive Nichol et al., "Improved Techniques for Training Score-Based Generative Models"__.
In contrast, we are the first to explore the distribution mismatch problem from the perspective of DRO. Meanwhile, our proposed AT with strong theoretical foundations is both simple and efficient, compared with the existing methods. 
% Exposure bias, the discrepancy in input distributions between the training and inference stages, is a prevalent problem of autoregressive generative models.
% The exposure bias is also present in DPMs:
% at each step, the input distribution of the training stage is the ground truth noise data distribution. \revise{add:xt=x0+noise? may need revise}
% In contrast, at inference, the input distribution is derived from the output generated by model at the previous step, which is different from the training phase.
% Moreover, the discrepancy error accumulates across the whole generation process.\revise{use generation trajectory?}
% DDPM-IP, ES, TS?
% Some studies attempt to alleviate the exposure bias in DPMs.
% Zhang et al., "Improving DALL-E with Attention"__ proposes to add a Gaussian perturbation to the input distribution as a regularization term during training to simulate the discrepancy.
% ____
% In addition to these training-based methods, some training-free work is also proposed:
% during the sampling process, Song et al., "Learning Hierarchical Features with Attention for Visual Tracking"__ scales down the L2-norm of predicted noise while Nichol et al., "Improved Techniques for Training Score-Based Generative Models"__ searches for an optimal time step within a predefined window.
% In this work, we focus on exploring the training-based approach to alleviate the exposure bias problem in diffusion models,

\paragraph{Adversarial Training and DRO.} In this paper, we leverage the Distributionally Robust Optimization (DRO) Ben-Tal et al., "Robust Optimization"__ to improve the distributional robustness of DPM and CM, thereby mitigating the distribution mismatch problem. As demonstrated in Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks"__, we link the DRO with AT Goodfellow et al., "Explaining and Harnessing Adversarial Examples"__, which is designed to improve the input (instead of distributional) robustness of the model. In supervised learning, the adversarial examples generated by efficient AT methods Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks"__ have been proven to be efficient augmented data to improve the robustness and generalization performance of models Goodfellow et al., "Explaining and Harnessing Adversarial Examples"__. In this paper, we further verify that the AT generated adversarial augmented examples are also beneficial for generative models DPM and CM.  

% Adversarial Training (AT)Goodfellow et al., "Explaining and Harnessing Adversarial Examples"__ dynamically constructs adversarial examples during the training of neural networks, which has been explored primarily in classification tasks across computer vision and natural language processing____.
% Since AT introduces perturbations into the input distributions of models, it can be seen as data augmentation to improve the robustness and generalization of models Goodfellow et al., "Explaining and Harnessing Adversarial Examples"__.
In addition, recent studies Song et al., "Learning Hierarchical Features with Attention for Visual Tracking"__ utilize DPM to generate examples in adversarial training to improve the robustness of the classification model. This is quite different from the method in this paper, as we focus on employing AT during training of diffusion-based model to improve its distributional robustness to alleviate the distribution mismatching. 
% exposure bias and further improve generation quality, especially when there are fewer sampling steps. \revise{how to note consistency model here? CM not has exposure bias?}

% \subsection{Other methods for improving diffusion models}\revise{Add this section?}