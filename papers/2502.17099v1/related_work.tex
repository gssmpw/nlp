\section{Related Work}
\paragraph{Distribution Mismatch in DPM.}
% expourse bias in ar
The problem is analogous to the exposure bias in auto-regressive language models \citep{bengio2015scheduled, ranzato2016sequence, shen2016minimum,steven2017self,zhang2019bridging}, whereas the next word prediction \citep{radford2019language} relies on tokens predicted by the model in the inference stage, which may be mismatched with the ground-truth one taken in the training stage. The similarity to DPMs becomes evident due to their gradual denoising generation process. \citet{diffusion-ip} and \citet{es} propose adding extra Gaussian perturbation during the training stage or data-dependent perturbation during the inference stage, to mitigate this issue. Following this line of work, several methods are further proposed. For instance, to reduce the accumulated discrepancy between the intermediate noisy data in the training and inference stages, \citet{li2024alleviating} search for a suboptimal mismatched input time step of the model to conduct inference. 
Similarly, \citet{li2024on} and \citet{ren2024multistep} directly minimize the difference between the generated intermediate noisy data and the ground-truth data. However, these methods either rely on strong assumptions \citep{diffusion-ip, es, li2024alleviating, ren2024multistep} or are computationally expensive \citep{li2024on}.
In contrast, we are the first to explore the distribution mismatch problem from the perspective of DRO. Meanwhile, our proposed AT with strong theoretical foundations is both simple and efficient, compared with the existing methods. 
% Exposure bias, the discrepancy in input distributions between the training and inference stages, is a prevalent problem of autoregressive generative models.
% The exposure bias is also present in DPMs:
% at each step, the input distribution of the training stage is the ground truth noise data distribution. \revise{add:xt=x0+noise? may need revise}
% In contrast, at inference, the input distribution is derived from the output generated by model at the previous step, which is different from the training phase.
% Moreover, the discrepancy error accumulates across the whole generation process.\revise{use generation trajectory?}
% DDPM-IP, ES, TS?
% Some studies attempt to alleviate the exposure bias in DPMs.
% \citep{diffusion-ip} proposes to add a Gaussian perturbation to the input distribution as a regularization term during training to simulate the discrepancy.
% \citep{li2024on,ren2024multistep}
% In addition to these training-based methods, some training-free work is also proposed:
% during the sampling process, \citep{es} scales down the L2-norm of predicted noise while \citep{li2024alleviating} searches for an optimal time step within a predefined window.
% In this work, we focus on exploring the training-based approach to alleviate the exposure bias problem in diffusion models,

\paragraph{Adversarial Training and DRO.} In this paper, we leverage the Distributionally Robust Optimization (DRO) \citep{shapiro2017distributionally,namkoong2019reliable,yi2021improved,sinha2018certifying,wang2022out,yi2023breaking} to improve the distributional robustness of DPM and CM, thereby mitigating the distribution mismatch problem. As demonstrated in \citep{sinha2018certifying,yi2021improved,lee2018minimax}, we link the DRO with AT \citep{madry2018towards,fgsm}, which is designed to improve the input (instead of distributional) robustness of the model. In supervised learning, the adversarial examples generated by efficient AT methods \citep{freeat, yopo, zhang2019theoretically, freelb, jiang2020smart} have been proven to be efficient augmented data to improve the robustness and generalization performance of models~\citep{rebuffi2021fixing, awp, yi2021improved}. In this paper, we further verify that the AT generated adversarial augmented examples are also beneficial for generative models DPM and CM.  

% Adversarial Training (AT)~\citep{fgsm, madry2018towards} dynamically constructs adversarial examples during the training of neural networks, which has been explored primarily in classification tasks across computer vision and natural language processing~\citep{freeat, yopo, zhang2019theoretically, freelb, jiang2020smart}.
% Since AT introduces perturbations into the input distributions of models, it can be seen as data augmentation to improve the robustness and generalization of models~\citep{rebuffi2021fixing, awp, yi2021improved}.
In addition, recent studies~\citep{nie2022diffusion, wang2023better,zhang2023enhancing} utilize DPM to generate examples in adversarial training to improve the robustness of the classification model. This is quite different from the method in this paper, as we focus on employing AT during training of diffusion-based model to improve its distributional robustness to alleviate the distribution mismatching. 
% exposure bias and further improve generation quality, especially when there are fewer sampling steps. \revise{how to note consistency model here? CM not has exposure bias?}

% \subsection{Other methods for improving diffusion models}\revise{Add this section?}