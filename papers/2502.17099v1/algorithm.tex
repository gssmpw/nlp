\begin{algorithm}[t!]
    \caption{Adversarial Training for Diffusion Model}
    \begin{algorithmic}[1]
        \State{\bf Input:} dataset $\mathcal{D}$, model parameter $\btheta$, learning rate $\kappa$, loss weighting $\lambda(\cdot)$, adversarial steps $K$, adversarial learning rate $\alpha$
        \While{do not converge}
            \State Sample $\bx\sim \mathcal{D}$ and $t\sim \mathcal{U}[1,T]$
            \State Sample $\beps \sim \cN(\boldsymbol{0}, \bI)$
            \State $\boldsymbol{\delta} \gets \boldsymbol{0}$
            \For {$i = 1,2,\ldots,K$}
                \State $\mathcal{L} \gets \left\|\beps_{\btheta}(\sqrt{\baralpha_{t}}\bx_{0} + \sqrt{1 - \baralpha_{t}}\beps + \bdelta) - \beps - \frac{\bdelta}{\sqrt{1 - \baralpha_{t}}}\right\|^{2}$ in \eqref{eq:dpm at}
                \State {$\boldsymbol{\delta} \gets \boldsymbol{\delta} + \alpha \cdot \frac{\nabla_{\boldsymbol{\delta}} \mathcal{L}} {\|\nabla_{\boldsymbol{\delta}} \mathcal{L}\|}$} \Comment{\emph{\texttt{maximize perturbation}}}
                \State {$\btheta \gets \btheta - \kappa \cdot \nabla_{\boldsymbol{\btheta}} \mathcal{L}$} \Comment{\emph{\texttt{update model}}}    
            \EndFor
        \EndWhile
    \end{algorithmic}
    \label{alg:adv dpm}
\end{algorithm}
\begin{algorithm}[t!]
    \caption{Adversarial Training for Consistency Distillation}\label{alg:adv cm}
    \begin{algorithmic}[1]
        \State{\bf Input:} dataset $\mathcal{D}$, initial model parameter $\btheta$, learning rate $\kappa$, pretrained noise prediction model $\beps_{\bphi}$, ODE solver $\hat{\Phi}_{\cdot}(\cdot, \beps_{\bphi}$, metric $d(\cdot,\cdot)$, loss weighting $\lambda(\cdot)$, target model EMA $\mu$, adversarial steps $K$, adversarial learning rate $\alpha$
        \State $\btheta^- \gets \btheta$
        \While{do not converge}
            \State Sample $\bx\sim \mathcal{D}$ and $t\sim \mathcal{U}[0,T-1]$
            \State Sample $\bx_{t + 1}$ from \eqref{eq:xt}
            \State $\boldsymbol{\delta} \gets \boldsymbol{0}$
            \For {$i = 1,2,\ldots,K$}
                \State $\mathcal{L} \gets \lambda(t) d(f_{\btheta}(\bx_{t + 1}, t + 1), f_{\btheta^-}(\hat{\Phi}_{t}(\bx_{t + 1}, \beps_{\bphi}) + \boldsymbol{\delta}, t))$ in \eqref{eq:cd at}
                \State $\boldsymbol{\delta} \gets \boldsymbol{\delta} + \alpha \cdot \frac{\nabla_{\boldsymbol{\delta}} \mathcal{L}}{\|\nabla_{\boldsymbol{\delta}} \mathcal{L}\|}$ \Comment{\emph{\texttt{maximize perturbation}}}
                \State $\btheta \gets \btheta - \kappa \cdot \nabla_{\boldsymbol{\btheta}} \mathcal{L}$ \Comment{\emph{\texttt{update model}}}
                \State $\btheta^- \gets \text{stopgrad}(\mu \btheta^- + (1-\mu)\btheta)$
            \EndFor
        \EndWhile
    \end{algorithmic}
\end{algorithm}