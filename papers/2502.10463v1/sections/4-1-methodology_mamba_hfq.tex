\section{Primarily State Space Model}

 And the detail proofs of Equation \eqref{eq:SSM_con} to \eqref{eq:SSM_dis} are shown as followings:
\begin{equation}
\label{eq:SSM_Parallel}
    \begin{aligned}
        h_0 & = 0 \\
        h_1 & = \overline{B} x_1 \\
        h_2 & = \overline{A}\overline{B}x_1+\overline{B}x_2 \\
        h_3 & = \overline{A}^2\overline{B}x_1+\overline{A}\overline{B}x_2+\overline{B}x_3 \\
        & \quad \vdots \\
        h_t & = \overline{A}^{t-1}\overline{B}x_1 + \overline{A}^{t-2}\overline{B}x_2 + \cdots + \overline{B}x_t. \\
    \end{aligned}
\end{equation}

Since the continuous formula is transformed to discrete form and it can be trained in parallel. Therefore, for the details of sequence-to-sequence transformation, it follows:
\begin{equation}
\label{eq:SSM_formula}
    h_t = \sum_{j=0}^{t-1} \overline{A}^j \overline{B} x_{t-j}.
\end{equation}
This is a feedforward form and this is mainly caused by the power $j$ of transition matrix $\overline{A}$ where $1 \leq j \leq t-1$. Therefore, it is an effort to block diagonalize $\overline{A}$ such that the Mamba form can be broken down more simply.

Now suppose that the transition matrix $\overline{A}$ has rank $R \leq d$. With loss of generality, we can assume that the nonzero eigenvalues of $\overline{A}$ are all distinct. Obviously, $\overline{A}$ has real nonzero eigenvalues and complex eigenvalues and the number are $r$ and $s$ respectively. Therefore the eigenvalues can be written as $\lambda_1,\cdots,\lambda_r,\lambda_{r+1},\lambda_{r+2s}$, where $(\lambda_{r+2k-1},\lambda_{r+2k}) = (\gamma_k e^{i\theta},\gamma_k e^{-i\theta})$ and $R=r+2s$ for $1 \leq i \leq s$ where $i$ is the imaginary unit. 

Therefore it is obvious that we can use Jordan decomposition \citep{} to rewrite the form of $\overline{A}^j$: $\overline{A}^j = \boldsymbol{T J}^j \boldsymbol{T}^{-1}$ where $\boldsymbol{T} \in \mathbb{R}^{d \times d}$ is invertible and $\boldsymbol{J} \in \mathbb{R}^{d \times d}$ is a Jordan block diagonal matrix. With this decomposition, the complex matrix can be broken down into some similar Jordan blocks with the dimension $p \times p$ matrix in $\boldsymbol{J}$ where $p=1$ or $2$. Therefore $\boldsymbol{J} = \text{diag}\{\lambda_1,\cdots,\lambda_r,\boldsymbol{C}_1,\cdots,\boldsymbol{C}_s,\boldsymbol{0}\}$, where $\boldsymbol{C}_k$ follows:

$$
\boldsymbol{C}_k=\gamma_k \cdot\left(\begin{array}{cc}
\cos (\theta_k) & \sin (\theta_k) \\
-\sin (\theta_k) & \cos (\theta_k)
\end{array}\right) \in \mathbb{R}^{2 \times 2}, \quad 1 \leq k \leq s.
$$

Then the form of formula \eqref{eq:SSM_formula} can be rewritten like this: 
\begin{equation}
    h_t = \sum_{j=0}^{t-1} \overline{A}^j \overline{B} x_{t-j}=\sum_{j=0}^{t-1} \boldsymbol{T}\boldsymbol{J}^j \boldsymbol{T}^{-1}\overline{B} x_{t-j}.
\end{equation}
Since $\boldsymbol{J}$ has a real Jordan form, $h_t$ follows:
\begin{equation}
\label{eq:SSM_Jordan}
    \begin{aligned}
        h_t & = \sum_{j=0}^{t-1} \boldsymbol{T}\boldsymbol{J}^j \boldsymbol{T}^{-1}\overline{B} x_{t-j} \\
        & = \sum_{j=1}^{t-1}\sum_{k=1}^r \lambda_k^j \boldsymbol{M}_k^R \overline{B}x_{t-j}+\sum_{j=1}^{t-1}\sum_{k=1}^s \gamma_k^j \{\cos(j\theta_k)\boldsymbol{M}_k^{C_1}+\sin(j\theta_k)\boldsymbol{M}_k^{C_2}\} \overline{B}x_{t-j}+\overline{B}x_t \\
        & = \sum_{k=1}^r h_t^R(\lambda_k) + \sum_{k=1}^s h_t^{C_1}(\gamma_k,\theta_k)+\sum_{k=1}^s h_t^{C_2}(\gamma_k,\theta_k)+ \overline{B}x_t,
    \end{aligned}
\end{equation}
where $h_t^R(\lambda_k) = \sum_{j=1}^{t-1}\boldsymbol{M}_k^R \overline{B}x_{t-j}$, $h_t^{C_1}(\gamma_k,\theta_k)=\sum_{j=1}^{t-1}\gamma_k^j \cos(j\theta_k)\boldsymbol{M}_k^{C_1}\overline{B}x_{t-j}$ and $h_t^{C_2}(\gamma_k,\theta_k)=\sum_{j=1}^{t-1}\gamma_k^j \sin(j\theta_k)\boldsymbol{M}_k^{C_2}\overline{B}x_{t-j}$. $\boldsymbol{M}_k^R,\boldsymbol{M}_k^{C_1},\boldsymbol{M}_k^{C_2}$ represent the real metrics determined jointly by $\boldsymbol{T}$ and $\boldsymbol{T}^{-1}$ and they are the functions of $\Delta A$ which can be written as $g_1(\Delta A)$ and the function $g$ is trainable. $\overline{B}$ is the combination of function $\Delta A$ and $B$ which can be written as $g_2(\Delta A) \otimes f(B)$.

Followed by Equation \ref{eq:SSM_Jordan}, the state space model has been broken down to three parts: real decomposition part, complex decomposition part and constant decomposition part.

\subsection{Mamba insert into Multi-head self attention}

Consider the input of self attention mechanism $X = (x_1,x_2,\cdots,x_T)' \in \mathbb{R}^{T \times d_{in}}$, a standard transformer contains Query, Key and Value. And the self attention mechanism is as following:
\begin{equation}
\label{eq:SA}
Attention(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = \text{Softmax}(\boldsymbol{QK}')\boldsymbol{V},
\end{equation}
where $\boldsymbol{Q}$, $\boldsymbol{K}$ and $\boldsymbol{V}$ are all linear projection from input $X$. In this way, Mamba can be inserted into transformer and with the advantages of Mamba structure which can obtain the sequential properties, we combine this idea into the positional embedding used to keep sequential in transformer. Therefore it can be represented as this form:
\begin{equation}
SA(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = \text{Softmax}(\boldsymbol{QK}'+\textbf{P})\boldsymbol{V},
\end{equation}
where $\textbf{P}$ is the positional embedding given by Mamba. And the form of $\textbf{P}$ is shown as two parts next:

$$
\mathbf{P}^R(\lambda)=\left(\begin{array}{ccccc}
0 & 0 & 0 & \cdots & 0 \\
f_1(\lambda) & 0 & 0 & \cdots & 0 \\
f_2(\lambda) & f_1(\lambda) & 0 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
f_{T-1}(\lambda) & f_{T-2}(\lambda) & f_{T-3}(\lambda) & \cdots & 0
\end{array}\right)
$$
$$
\mathbf{P}^{C_k}(\gamma,\theta)=\left(\begin{array}{ccccc}
0 & 0 & 0 & \cdots & 0 \\
g^k_1(\gamma,\theta) & 0 & 0 & \cdots & 0 \\
g^k_2(\gamma,\theta) & g^k_1(\gamma,\theta) & 0 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
g^k_{T-1}(\gamma,\theta) & g^k_{T-2}(\gamma,\theta) & g^k_{T-3}(\gamma,\theta) & \cdots & 0
\end{array}\right),
$$

where $\mathbf{P}^R(\lambda)$ is the real decomposition part, and $\mathbf{P}^{C_k}(\gamma,\theta)$ for $k\in\{1,2\}$ are the complex decomposition part. Looking back to Equation \ref{eq:SSM_Jordan}, if we want to add Mamba structure into transformer as the positional embedding, it is clear that we can treat $\boldsymbol{M}_k^F \overline{B}$ for $F \in \{R,C_1,C_2\}$ as the linear projection from $X$ to $\boldsymbol{V}$ which is $W_V$. Therefore, for the real part, $f_t(\lambda)=\lambda^t$ for $1 \leq t \leq T-1$ and for the complex part $g_1^1(\gamma,\theta)=\gamma^t \cos(t\theta)$ and $g_1^2(\gamma,\theta)=\gamma^t \sin(t\theta)$.

For the choice of real or complex part, it is surprisingly that we can find the state space model discretization can be represented into a multi-head self attention with $r+2s+1$ heads, where the query and key matrices are zero, and the relative positional embeddings are $\{\mathbf{P}^R(\lambda_k),1 \leq k \leq r\}$, $\{\mathbf{P}^{C_i}(\gamma_k,\theta_k),i \in \{1,2\},1 \leq k \leq s\}$ and an identity matrix, respectively. Therefore, we can insert different parts of decomposition into each head. $h_t^R$ provides the regular exponential decay induced by the real eigenvalues $\lambda_k$'s and $h_t^{C_i},i\in \{1,2\}$ provide the cyclical damoped cosine or sine decay induced by the pair of complex eigenvalues with $\gamma$ and $\theta$. These temporal patterns are further summarized into $\mathbf{P}^R(\lambda_k)$ and $\mathbf{P}^{C_i}(\gamma_k,\theta_k),i \in \{1,2\}$.

\textbf{Selective Scan Mechanism.} However for mamba structure, the advantage is that it can select the importance of input. For example, for simple copy task, it is easily solved by time-invariant models such as linear recurrences and global convolutions. But for selective copying task, we should give a solution based on the input which means the selective mechanism used in Mamba. Therefore for the parameters $\Delta,B$ in Equation \ref{eq:SSM_dis}, they both depend on input $X$ which always a linear projection from $X$ to $\Delta$ and $B$. In our time series forecasting task, it is clear that the time interval is certain. So compared with other natural language process tasks, the time interval is uncertain and it is need to depend on the input. So consider the Equation \eqref{eq:SSM_dis}, $\overline{A}$ is the transition matrix which not depends on the input so we can use the Jordan decomposition to transform this into constant calculation. And for $\overline{B}$, it depends on $\Delta,A,B$ where $B$ depends on $X$ which means we can add the selective mechanism in the value projection. Compared with vanilla transformer, for the multi-head self attention, $\boldsymbol{V} = W_V X$ for $W_V \in \mathbb{R}^{d_{in} \times d}$, we transform linear projection to the selective linear projection which means replace $W_V$ from a matrix to $W_V = W_W X$ for $W_W \in \mathbb{R}^{d \times d}$ which depends on the input $X$.

\section{Combine Mamba and self attention}

For vanilla transformer models, in the self attention mechanism, it lacks of the significant sample efficiency in modeling the sequential patterns. Therefore we add the Mamba patterns into self attention to capture the recurrent parts. Also the state space model discretization can be represented into a multi-head self attention (MHSF) where the query and key are zero for a standard MHSF. These motivate us to combine the strengths of Mamba idea with MHSF:
\begin{equation}
\label{eq:MHSF}
    \text{MambaSA}(X)=\{[1-\sigma(\mu)]\text{softmax}(\boldsymbol{QK}')+\sigma(\mu)\textbf{P}\}\boldsymbol{V},
\end{equation}
where $\textbf{P}$ is a regular or cyclical part, and $\sigma(\mu)$ is the gate-control structure used to control the probability of positional embedding given by Mamba.

Also note that $\textbf{P}$ are all lower triangular matrices, which correspond to unidirectional Mamba. Therefore, from the idea of VMamba \citep{liu2024vmamba}, for a figure, this paper gives a scan approach which need to scan from top-left, bottom-right, top-right and bottom-left. While for time series dataset, it is only need two scan merge on the sequential data. Followed by \citet{huang2022encoding} and \citet{liu2024vmamba}, we propose the bidirectional Mamba scan mechanism and we define the double scan version of $\textbf{P}$ as $\textbf{P}_{bi}$ which is $\textbf{P}^R_{bi}(\lambda)=\textbf{P}^R(\lambda)+[\textbf{P}^R(\lambda)]'$ and $\textbf{P}^{C_i}_{bi}(\gamma,\theta)=\textbf{P}^{C_i}(\gamma,\theta)+[\textbf{P}^{C_i}(\gamma,\theta)]'$ for regular and cyclical part. Also in order to limit the explosive of parameters, we give the limitation of $|\lambda| \leq 1$ and $|\gamma| \leq 1$. Therefore follows \citet{huang2022encoding}, they further bound these two parameters by transformation with hyperbolic tangent function (tanh) activation function which means $\lambda = \text{tanh}(\phi)$ and $\gamma = \text{tanh}(\psi)$. Therefore, the notations become $\textbf{P}_{bi}^R(\phi)$ and $\textbf{P}_{bi}^R(\psi,\theta)$.

On one hand, comparing with the baseline Transformer, the inclusion of REMs
will lead to a significant improvement of sample efficiency in modeling the recurrent patterns (at least it will perform similar with baseline since $\sigma(\mu) \rightarrow 0$), and hence a higher accuracy in prediction can be expected from the proposed RSA. On the other hand, comparing with the traditional Mamba models, our method also considers the correlation between any two tokes with selective. And for the multihead MambaSA, the gate-control parameter $\mu$ only varies across layers, while the parameters controlling the matrix $\textbf{P}$ vary across all heads and layers.

Additionally, followed by \citet{huang2022encoding}, it is essential to use dilated variant of $\textbf{P}$. It can be further obtained by considering the block matrix formed by the first $T$ colomns and first $T$ rows of $\textbf{P} \otimes \textbf{I}_d$ where $d$ is the dilating factor. This describes the periodic of long time series, especially for the dataset used by us. Therefore our positional embeddings contain six parts, one regular $\textbf{P}^R(\phi)$, two cyclical $\textbf{P}^{C_i}(\psi,\theta)$ for $i \in \{1,2\}$, one dilated regular $\textbf{P}^R(\phi) \otimes \textbf{I}_d$ and two dilated cyclical $\textbf{P}^{C_i}(\psi,\theta) \otimes \textbf{I}_d$ for $i \in \{1,2\}$.




