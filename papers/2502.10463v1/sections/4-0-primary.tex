
%

\section{Preliminary and Problem Formulation}
\label{SSM_theory}

% This section gives the main formula for the selective state space model given by \cite{gu2021efficiently} and \cite{gu2023mamba}. 

\subsection{Revisiting State Space Models}
The state space model is defined below, and it maps a $1$-dimensional input signal $x(t)$, a continuous process, to an $N$-dimensional latent state $h(t)$, another continuous process:
\begin{equation}
\label{eq:SSM_con}
h^{\prime}(t) =A h(t)+B x(t),
\end{equation}
where $A \in \mathbb{R}^{N \times N}$ and $B \in \mathbb{R}^{N \times 1}$ are the structured state matrix and weight of influence from input to latent state, respectively. We then can obtain the discretization solution of the above equation:
\begin{equation}
        h^{t} = e^{\Delta A}h^{t-1} + \int_{t-1}^{t} e^{A(t-\tau)} Bx(\tau) d\tau.
\end{equation}
Together with the zero-order hold (ZOH) condition \citep{karafyllis2011nonlinear}, i.e. $x(\tau)$ is a constant at intervals $[t-1,t]$ for all integers $t$, we have
\begin{equation}
    h^{t} = e^{\Delta A}h^{t-1} + (\Delta A)^{-1}(\text{exp}(\Delta A)-I) \cdot \Delta B x^{t}.
\end{equation}
As a result, the continuous process at \Eqref{eq:SSM_con} can be replaced by the following discrete sequence:
\begin{equation}
\label{eq:SSM_dis}
%\begin{aligned}
h^t  =\overline{A} h^{t-1}+\overline{B} x^t \hspace{5mm}\text{with}\hspace{5mm}
\overline{A} = \text{exp}(\Delta A) \quad \text{and}   \quad \overline{B} = (\Delta A)^{-1}(\text{exp}(\Delta A)-I) \cdot \Delta B.
%\end{aligned}
\end{equation}
Following \cite{gu2023mamba}, we refine the approximation of $\overline{B}$ using the first-order Taylor series:
\begin{equation}
    \overline{B} = (\Delta A)^{-1}(\text{exp}(\Delta A)-I) \cdot \Delta B \approx (\Delta A)^{-1}(\Delta A) \cdot \Delta B = \Delta B.
\end{equation}
The formulas $\overline{A}=f_A(\Delta, A)$ and $\overline{B}=f_B(\Delta, A, B)$ are called the discretization rule, where $B = \text{Linear}_N(x)$ is a linear projection of input $x$ into $N$-dimension vector, and $\Delta = \text{Linear}_1(x)$; see \citet{gu2021efficiently,gu2023mamba} for details.

% Then \cite{} proposes the settings about transition matrix $A$ and in this paper, they give an exact solution about how to choose this in state space model. 


%This section gives the overview of recurrent mechanism and recalls the mathematical formulation about layer aggregations.
\subsection{CNN Layer Aggregation}
\label{5.1}
Consider a neural network, and let $\boldsymbol{X}^{t-1}$ be the output from its $t$th layer. We then can mathematically formulate the layer aggregation at the $t$th layer below,
\begin{equation}
\label{eq:CNN_agg}
    A^t =g^t(\boldsymbol{X}^{0},\boldsymbol{X}^{1},\cdots,\boldsymbol{X}^{t-2},\boldsymbol{X}^{t-1}), \quad
    \boldsymbol{X}^t = f^t(A^{t-1},\boldsymbol{X}^{t-1}), 
\end{equation}
where $g^t$ is used to summarize the first $t$ layers, $A^t$ is the aggregated information, and $f^t$ produces the new layer output from the last hidden layer and the given aggregation which contains the previous information. 
The Hierarchical Layer Aggregation proposed \citep{yu2018deep} can be shown to have such similar mechanism which satisfies  \Eqref{eq:CNN_agg}.

This formulation could be generalized to the special case of CNNs.
The traditional CNNs do not contain layer aggregation since the layer output only depends on the last layer output, which overlooks the connection between the several previous layers' influence.
DenseNet \citep{huang2018denselyconnectedconvolutionalnetworks} perhaps is the first one for the layer aggregation, and its output at $t$th layer can be formulated into
\begin{equation}
\label{eq:densenet}
\boldsymbol{X}^t=\text{Conv3}^t[\text{Conv1}^t(\text{Concat}(\boldsymbol{X}^0, \boldsymbol{X}^1, \ldots, \boldsymbol{X}^{t-1}))].
\end{equation}
Let $A^t = \text{Conv1}^t(\text{Concat}(\boldsymbol{X}^0, \boldsymbol{X}^1, \ldots, \boldsymbol{X}^{t-1}))$ and $\boldsymbol{X}^t = \text{Conv3}^t (A^t)$, and then DenseNet can be rewritten into our framework at \Eqref{eq:CNN_agg}.
RLA \citep{zhao2021recurrence} considers a more convenient additive form for the layer aggregation, and it has the form of $A^t = \sum _{i=0}^{t-1} \text{Conv1}^{t+1}_i(\boldsymbol{X}^i)$, where the kernel weights of $\text{Conv1}^t_i$ form a partition of the weights in $\text{Conv1}^t$.
% Consider that we can treat all the $\text{Conv1}^t_i$ as $\text{Conv1}^t$ which means the aggregation function does not depend on the current layer. 
As a result, a lightweight aggregation can be formed:
\begin{equation}
\label{eq:densenet_re}
    \boldsymbol{X}^t=\text{Conv3}^t [ A^{t-1} + \text{Conv1}^{t}_{t-1}(\boldsymbol{X}^{t-1}) ].
\end{equation}

Without loss of generality, ResNets \citep{he2016deep,he2016identity} can also be treated as a layer aggregation. Specifically, we can treat the update of $\boldsymbol{X}^t = \boldsymbol{X}^{t-1} + f^{t-1}(\boldsymbol{X}^{t-1})$ with applying the update recursively as $A^t = \sum_{i=0}^{t-1} f^i(\boldsymbol{X}^i) + \boldsymbol{X}^0$ and $\boldsymbol{X}^t = A^{t-1}+\boldsymbol{X}^{t-1}$.


\subsection{Attention Layers Aggregation}
In this section, we show how to generalize the layer aggregation within a transformer. Consider a simple attention layer with general input $\mathbf{X} \in \mathbb{R}^{L \times D}$ and output $\mathbf{O} \in \mathbb{R}^{L \times D}$.
Its query $\mathbf{Q}$, key $\mathbf{K}$ and value $\mathbf{V}$ are given by linear projections $\boldsymbol{W}_q \in \mathbb{R}^{D \times D}$, $\boldsymbol{W}_k \in \mathbb{R}^{D \times D}$ and $\boldsymbol{W}_v \in \mathbb{R}^{D \times D}$, i.e. $\mathbf{Q}^T = \boldsymbol{W}_q \mathbf{X}$, $\mathbf{K}^T = \boldsymbol{W}_k \mathbf{X}$ and $\mathbf{V}^T = \boldsymbol{W}_v \mathbf{X}$. As a result, the output $\mathbf{O}$ has the following mathematical formulation:
\begin{equation}
\mathbf{O} = \text{Self-Attention}(\boldsymbol{X}) = \text{softmax}(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{D}})\mathbf{V}.
\end{equation}
Let $\boldsymbol{X}^t \in \mathbb{R}^{L \times D}$ with $1 \leq t \leq T$ be the output from $t$th layer, where $L$ is the number of tokens, $D$ represents the channel of each token, and $T$ is the number of attention layers. 
A vanilla transformer can then be formulated into:
\begin{equation}
    \label{eq:vanillatransformer}
        A^{t} =\boldsymbol{X}^{t-1}+\text{Self-Attention}(\boldsymbol{X}^{t-1}), \quad
        \boldsymbol{X}^{t} = A^{t} + \text{MLP}(\text{Norm}(A^{t})).
\end{equation}

Note that these simple self-attention layers can only capture the connection between the current layer output and the last output; they are supposed to perform better if the information from previous layers can be considered. To this end, we may leverage the idea given by CNN aggregation to concatenate the previous outputs. Specifically, the vanilla transformer at \Eqref{eq:vanillatransformer} has the form of:
\begin{equation}
    \label{eq:vanillatransformerre}
    \boldsymbol{X}^t=f^t(g^t(\boldsymbol{X}^0,\cdots,\boldsymbol{X}^{t-1})),
\end{equation}
where $g^t$ is the attention layer, and $f^t$ is the Add \& Norm layer for the $t$-th layer. 
Following \citet{zhao2021recurrence} at \Eqref{eq:densenet_re}, we may then use the recurrent mechanism to combine all the outputs given by attention layers, i.e. replacing $A^{t} = g^t(\boldsymbol{X}^0,\cdots,\boldsymbol{X}^{t-1})$ by $A^{t} = A^{t-1} + g^{t-1}(\boldsymbol{X}^{t-1})$.