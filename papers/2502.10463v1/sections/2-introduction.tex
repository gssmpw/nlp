\section{Introduction}
\label{Introduction}

Until now various statistical models have gained traction in the machine learning domain, each offering unique advantages for specific tasks. For instance, Hidden Markov Models (HMM) are frequently employed in reinforcement learning tasks, where they effectively model sequential data and capture temporal dependencies \citep{sloin2007support, dethlefs2011hierarchical}. Bayesian Networks serve as powerful tools for inference and decision support, allowing for the representation of complex relationships among variables \citep{friedman1997bayesian,chen2012good}. Additionally, Gaussian Mixture Models (GMM) excel at processing complex data distributions, making them suitable for clustering and density estimation \citep{reynolds2009gaussian,viroli2019deep}. Generative Adversarial Networks (GANs) \cite{goodfellow2020generative} and Markov chains for diffusion models \citep{sohldickstein2015deepunsupervisedlearningusing,ho2020denoisingdiffusionprobabilisticmodels} have also revolutionized image generation tasks, enabling the creation of highly realistic images through adversarial training. And now the State Space Model (SSM), as a traditional framework, is widely utilized in computer vision (CV) \citep{gu2023mamba,liu2024vmamba} tasks. Therefore, it is only natural for us to focus our attention on this model.

%
% (1) model depth在增加，depth对performance很重要: 举例子，xx model有上百layer（画一个图：performance(y-axis) vs. depth(x-axis)）；（得写之前大家是怎么解决这个问题的，但是，他们缺乏有理论支撑的数学模型的支撑的）
% 分段
% (2) 介绍SSM，点出SSM是一个有理论支撑数学模型，同时他最适合处理long sequence modelling（会介绍到H，A，B）;
% 分段
% (3) 我们propose，Network 实际上可以建模为一个SSM，那么，very deep network的layer aggregation可以formulate成一个long sequence modeling task，为什么需要设计一个SSIL（其实是因为SSM里有H，A，B）；
% (4) 具体的module设计（具体怎么在CNN/ViT），和experimental results，总结contribution：
%   a. novel formulation of Network;
%   b. 提出SSIL；
%   c. 在CV上验证了我们的propose的有效性；
% 画图：overview;CNN;ViT.

Meanwhile, recent researches have increasingly highlighted the importance of strengthening layer interactions within deep neural networks. Studies show that enhancing these interactions can significantly improve the flow of information throughout the network \citep{he2016deep,huang2018denselyconnectedconvolutionalnetworks,zhao2021recurrence,fang2023cross}. 


As our known, it is apparently that longer layer models give the better performance in vision works since it can capture more information. Therefore, some combination of models (CNNs \citep{he2016deep,ronneberger2015unetconvolutionalnetworksbiomedical,tan2020efficientnetrethinkingmodelscaling} or transformers \citep{dosovitskiy2020image,touvron2021training,liu2021swin,wang2022pvt}) appeared and they all use the larger receptive fields to capture the global semantic features. While some papers proved that encouraging interactions between layers can enhance the network's representational power by effectively combining features at different levels of abstraction. Notable architectures have emerged that exemplify this approach. ResNet \citep{he2016deep} introduced skip connections, allowing gradients to flow more easily during training by connecting non-adjacent layers. DenseNet \citep{huang2018denselyconnectedconvolutionalnetworks} took this concept further by enabling each layer to access all preceding layers within a stage, thereby fostering a rich exchange of information. More recently, GLOM \citep{hinton2023represent} adopted an intensely interactive architecture that incorporates bottom-up, top-down, and same-level connections, aiming to represent part-whole hierarchies effectively.

Building on these foundations, smaller receptive fields effectively capture local texture features, while larger receptive fields encompass more global information. This insight into the importance of layer interactions in large models inspires us to propose a novel perspective: neural networks can be conceptualized as state space models (SSMs). By leveraging the interactions between layers, outputs from different layers can be interpreted as sequential data, which serves as input to an SSM. This integration enables the SSM to encapsulate a richer representation of the information derived from the original data. Such a framework not only enhances the network's performance across various tasks but also provides a cohesive perspective that bridges traditional statistical methods with contemporary deep learning techniques. To the best of our knowledge, our work represents the first systematic investigation into cross-layer interaction within the context of SSMs. This approach distinguishes itself from existing methodologies by emphasizing the dynamic interplay between layers in neural networks, potentially leading to enhanced performance in various tasks. For CNNs-based models such as ResNet \cite{he2016deep}, and transformer-based model such as Deit \cite{touvron2021training}, Swin-Transformer \cite{liu2021swin} and PVT-v2 \cite{wang2022pvt}, we introduce a novel structure to integrate existing traditional models into sequential architectures. With the proposed State-Space-Interaction-Layer (SSIL), we effectively leverage the benefits of interactions in large-layer models while incorporating statistical models into vision tasks. By conceptualizing neural networks as state space models, we pave the way for new research avenues that bridge traditional statistical methods with contemporary deep learning techniques.

Our contributions contains:
% cv在最后一个contribution说就可以，只是一个验证的方式
\begin{enumerate}
    \item We propose that deep neural networks used in computer vision tasks can also be conceptualized as state space models. To our knowledge, this is the first time such a perspective has been presented. Our approach redefines how to utilize state space models to integrate different layers, advancing traditional multi-layer models.
    \item We introduce the State-Space-Interaction-Layer (SSIL), which conceptualizes neural networks as state space models (SSMs) to enhance cross-layer interactions. This framework integrates traditional CNNs, like ResNet, and transformer-based models, such as Deit and Swin Transformer, into sequential architectures, representing the first systematic investigation of such interactions within SSMs.
    \item In comparison with other state-of-the-art methods focusing on layer interactions—such as the ResNet \cite{he2016deep} architecture and transformer-based models like Deit \cite{touvron2021training}, Swin Transformer \cite{liu2021swin}, and PVT-v2 \cite{wang2022pvt}—our method demonstrates superior performance in classification, detection, and instance segmentation tasks.
\end{enumerate}
