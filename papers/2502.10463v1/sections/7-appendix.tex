\appendix
\section{The correlation between accuracy and layer size}

\begin{figure}[h]
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=0.7\linewidth]{line_chart.pdf}
\end{center}
\caption{The correlation between accuracy and layer size with model PVT-v2.}
\label{fig_corr_layer}
\end{figure}

Figure \ref{fig_corr_layer} illustrates that deeper neural networks can get better performance. 

\section{Full Caption of Figures}

\begin{figure}[htbp]
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=0.9\linewidth]{overview.pdf}
\end{center}
\caption{\textbf{Schematic Diagram of a Network with Selective State Space Model Layer Aggregation.} The green arrow represents the hidden state connection, while the grey arrow indicates communication between layers. The updated latent layer is derived from the previous latent layer \( h^{t-1} \) and the last input layer \( \boldsymbol{X}^{t} \). The output of the \( t \)-th layer is generated from the input \( \boldsymbol{X}^{t-1} \) and the latent layer \( h^{t} \).}
\label{fig:overview_copy}
\end{figure}

\begin{figure}[htbp]
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=0.9\linewidth]{overview_CNN.pdf}
\end{center}
\caption{\textbf{Detailed Operations in the S6LA Module with Convolutional Neural Network.} The green arrow represents the hidden state connection, while the grey arrow indicates communication between layers. The temporary output \( O^t \) is the concatenation of the latent layer \( h^{t-1} \) and the input layer \( \boldsymbol{X}^t \). After passing through the convolutional layers, the updates for the latent layer \( h^t \) and the input layer \( \boldsymbol{X}^{t+1} \) are derived from the last layers.}
\label{fig:overview_CNN}
\end{figure}

\section{Pseudo Code}

In this section we will propose our pseudo of our method for CNNs and transformers.

\begin{algorithm}[!h]
    \caption{S6LA used in CNNs}
    \label{alg:S6LA_CNNs}
    \renewcommand{\algorithmicrequire}{\textbf{Input:}}
    \renewcommand{\algorithmicensure}{\textbf{Output:}}
    
    \begin{algorithmic}[1]
        \REQUIRE Output of the first CNN block $\boldsymbol{X}^{1} \in \mathbb{R}^{1 \times H \times W \times D}$, latent state dimension $N$.
        %%input
        \ENSURE Output of the $T$-th CNN block $\boldsymbol{X}^{T} \in \mathbb{R}^{1 \times H \times W \times D}$.
        %%output
        
        \STATE  Initial the latent state $h^0$, the trainable structured state matrices $A$;
        
        % \WHILE{$A=B$}
        %     \STATE xxxxx
        % \ENDWHILE
        
        \FOR{each $t \in [1,T]$}
            % \IF {$C = 0$}
            %     \STATE xxxxx
            % \ELSE
            %     \STATE xxxxx
            % \ENDIF
            \STATE // $h$ influence $\boldsymbol{X}$ 
            \STATE Concatenate the $t-1$-th output $\boldsymbol{X}^{t-1}$ and the $t-1$-th latent state $h^{t-1}$;
            \STATE Get the information $\boldsymbol{O}^{t-1}$ concluding the two parts with CNNs and residuals;
            \STATE Treat with Conv1d and average pooling to the next step;
            \STATE // selective mechanism
            \STATE Through linear projection to get the selective parameters $\Delta$ and $B$ from the last step;
            \STATE // $X$ influence $h$
            \STATE Calculate the next output $\boldsymbol{X}^t$ and the update latent state $h^t$ with $\boldsymbol{X}^{t} = \boldsymbol{O}^{t-1} + \boldsymbol{X}^{t-1}$.
        \ENDFOR
        
        \RETURN Outputs $\boldsymbol{X}^T$.
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[!h]
    \caption{S6LA used in Vision Transformers}
    \label{alg:S6LA_Transformer}
    \renewcommand{\algorithmicrequire}{\textbf{Input:}}
    \renewcommand{\algorithmicensure}{\textbf{Output:}}
    
    \begin{algorithmic}[1]
        \REQUIRE Output of the first CNN block $\boldsymbol{X}^{1} \in \mathbb{R}^{1 \times L \times D}$, latent state dimension $N$.
        %%input
        \ENSURE Output of the $T$-th CNN block $\boldsymbol{X}^{T} \in \mathbb{R}^{1 \times L \times D}$.
        %%output
        
        \STATE  Initial the latent state $h^0$, the trainable structured state matrices $A$;
        
        % \WHILE{$A=B$}
        %     \STATE xxxxx
        % \ENDWHILE
        
        \FOR{each $t \in [1,T]$}
            % \IF {$C = 0$}
            %     \STATE xxxxx
            % \ELSE
            %     \STATE xxxxx
            % \ENDIF
            \STATE Combine the class token with the $t-1$-th output $\boldsymbol{X}^{t-1}$ and add positional embeddings;
            \STATE Attention mechanism with $\boldsymbol{X}^{t-1}_{\text{input}} = \text{Add} \& \text{Norm}(\text{Attn}(\boldsymbol{X}^{t-1}))$ and get $\boldsymbol{X}^{t-1}_{\text{input}} \in \mathbb{R}^{(L+1) \times D}$;
            \STATE Split $\boldsymbol{X}^{t-1}_{\text{input}}$ and get the two components $\boldsymbol{X}^{t-1}_p, \boldsymbol{X}^{t-1}_c$;
            \STATE // $h$ influence $\boldsymbol{X}$, multiplication not simple concatenation
            \STATE The new patch tokens: $\widehat{\boldsymbol{X}}_p^{t-1} = \boldsymbol{X}^{t-1}_p + W\boldsymbol{X}^{t-1}_p h^t$;
            \STATE // selective mechanism
            \STATE Through linear projection to get the selective parameters $\Delta$ and $B$ from the class token: $\Delta = W_{\Delta} (\boldsymbol{X}^{t-1}_c),
            B = W_B (\boldsymbol{X}^{t-1}_c)$;
            \STATE // $X$ influence $h$
            \STATE Calculate the next output $\boldsymbol{X}^T$ and the update latent state $h^T$: $ h^t = e^{(\Delta A)} h^{t-1} + \Delta B \boldsymbol{X}^{t-1}_c$ and $\boldsymbol{X}^t = \text{Concat}(\widehat{\boldsymbol{X}}_p^{t-1},\boldsymbol{X}^{t-1}_c)$.
        \ENDFOR
        
        \RETURN Outputs $\boldsymbol{X}^T$.
    \end{algorithmic}
\end{algorithm}


\section{Experiments}
\label{app:experiment}

\subsection{Imagenet Classification}

\subsubsection{Implementation details}

\paragraph{ResNet} For training ResNets with our method, we follow exactly the same data augmentation and hyper-parameter settings in original ResNet \citep{he2016deep}. Specifically, the input images are randomly cropped to 224 Ã— 224 with random horizontal flipping. The networks are trained from scratch using SGD with momentum of 0.9, weight decay of 1e-4, and a mini-batch size of 256. The models are trained within 100 epochs by setting the initial learning rate to 0.1, which is decreased by a factor of 10 per 30 epochs. Since the data augmentation and training settings used in ResNet are outdated, which are
not as powerful as those used by other networks, strengthening layer interactions leads to overfitting on ResNet. Pretraining on a larger dataset and using extra training settings can be an option.

\paragraph{DeiT, Swin Transformer, PVTv2} We adopt the same training and augmentation strategy as that in DeiT. All models are trained for 300 epochs using the AdamW optimizer with weight decay of 0.05. We use the cosine learning rate schedule and set the initial learning rate as 0.001 with batch size of 1024. Five epochs are used to gradually warm up the learning rate at the beginning of the training. We apply RandAugment \citep{cubuk2020randaugment}, repeated augmentation \citep{hoffer2020augment}, label smoothing \citep{szegedy2016rethinking} with $\epsilon = 0.1$, Mixup \citep{zhang2017mixup} with 0.8 probability, Cutmix \citep{yun2019cutmix} with 1.0 probability and random erasing \citep{zhong2020random} with 0.25 probability.
Similarly, our model shares the same probability of the stochastic depth with the MHSA and FFN layers of DeiT/CeiT/PVTv2.

\subsubsection{Model complexity with respect to input resolution}

Figure \ref{fig_resolution_FLOPs} illustrates the FLOPs induced by our model S6LA with respect to input resolution. We use the model PVTv2-b1 as the backbone and then derive the differences under various settings of input resolution. From this, it is apparent that complexity of our method is linear to input resolution.

\begin{figure}[t]
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=0.7\linewidth]{line_chart_GFLOPs.pdf}
\end{center}
\caption{The GFLOPs induced by our method with respect to input resolution with backbone PVT-v2-b1.}
\label{fig_resolution_FLOPs}
\end{figure}

\subsubsection{Comparisons with relevant networks}

\paragraph{Layer-interaction networks} For the comparison of layer-interaction-based models using CNNs, we first compare our method S6LA with Densenet \citep{huang2018denselyconnectedconvolutionalnetworks}, DIANet \citep{huang2020dianet}, RLA \citep{zhao2021recurrence} and MRLA \citep{fang2023cross}. The comparison on the ImageNet-1K validation set are given in Table \ref{table_CNN-based}. From the table, it is obvious that our method outperforms in the classification task and also compared with the similar model size of DenseNet, our model performs better.

\paragraph{Other relevant networks} The methods in the last part, they all use the same implemental settings from ours in the training of Imagenet. However for some other models, such as BA-Net \citep{zhao2022ba}, adopted other settings. It applied cosine learning schedule and label smoothing in their training process. However, the different settings of our method and their method will give the unfair comparison. Therefore, we adopt the results given in MRLA \citep{fang2023cross} since our method settings are same to this paper. The results are given in Table \ref{table_other_relevant}.

\begin{table}[htbp]
\caption{Performances of layer-interaction-based models on ImageNet-1K validation dataset. The \textbf{bold} one denotes the best performance. (The performances of DIANet and DenseNet are from the two papers)}
\label{table_layer_interaction}
\begin{center}
\begin{tabular}{c|cc|cc}
\toprule
    Model & Params & FLOPs & Top-1 Acc. & Top-5 Acc. \\
\midrule
    ResNet-50 \citep{he2016deep} & 25.6 M & 4.1 B & 76.1 & 92.9 \\
    + DIA \citep{huang2020dianet} & 28.4 M & - & 77.2 & - \\
    + RLA \citep{zhao2021recurrence} & 25.9 M & 4.5 B & 77.2 & 93.4 \\
    + MRLA \citep{fang2023cross} & 25.7 M & 4.6 B & 77.5 & 93.7 \\
    + S6LA (Ours) & 25.8 M & 4.4 B & \textbf{78.0} & \textbf{94.2} \\
    \midrule
    ResNet-101 \citep{he2016deep} & 44.5 M & 7.8 B & 77.4 & 93.5 \\
    + DIA \citep{huang2020dianet} & 47.6 M & - & 78.0 & - \\
    + RLA \citep{zhao2021recurrence} & 45.0 M & 8.4 B & 78.5 & 94.2 \\
    + MRLA \citep{fang2023cross}  & 44.9 M & 8.5 B & 78.7 & 94.4 \\
    + S6LA (Ours) & 45.0 M & 8.3 B & \textbf{79.1} & \textbf{94.8} \\
    \midrule
    DenseNet-161 (k=48) \citep{huang2018denselyconnectedconvolutionalnetworks} & 27.4 M & 7.9 B & 77.7 & 93.8 \\
    DenseNet-264 (k=32) \citep{huang2018denselyconnectedconvolutionalnetworks} & 31.8 M & 5.9 B & 77.9 & 93.8 \\
    \bottomrule
\end{tabular}
\end{center}
\end{table}

\begin{table}[htbp]
\caption{Performances of BA-Net model on ImageNet-1K validation dataset under our settings. The \textbf{bold} one denotes the best performance.}
\label{table_other_relevant}
\begin{center}
\begin{tabular}{c|cc|cc}
\toprule
    Model & Params & FLOPs & Top-1 Acc. & Top-5 Acc. \\
\midrule
    BA-Net-50 \citep{zhao2022ba} & 28.7 M & 4.2 B & 77.8 & 93.7 \\
    % BA-Net-50 \citep{zhao2022ba} & 28.7 M & 4.2 B & 77.2 & - \\
    ResNet-50 + S6LA (Ours) & 25.8 M & 4.4 B & \textbf{78.0} & \textbf{94.2} \\
    \bottomrule
\end{tabular}
\end{center}
\end{table}

\subsection{Object detection and instance segmentation on COCO dataset}

\paragraph{Implementation details} We adopt the commonly used settings by \citet{hu2018squeeze,wang2020eca,cao2019gcnet,wang2021evolving,zhao2021recurrence,fang2023cross}, which are same to the default settings in MMDetection toolkit \citep{chen2019mmdetectionopenmmlabdetection}. For the optimizer, we use SGD with weight decay of 1e-4, momentum of 0.9 and batchsize of 16 for all experiments. The learning rate is 0.02 and is decreased by a factor of 10 after 8 and 11 epochs for within the total 12 epochs. For RetinaNet, we modify the initial learning rate to 0.01 to avoid training problems. For the pretrained model, we use the model trained in ImageNet tasks.

\paragraph{Results} For the experiments of detection tasks in the similar settings models, it is illustrated in Table \ref{table_segmentation} and \ref{table_detection}. For almost backbone models, our model performs better and it proves that our model is also useful in detection tasks. For some other implemental settings such as ResNeXT \citep{xie2017aggregated} RelationNet++ \citep{chi2020relationnet++}, our method also outperforms in object detection results.

\begin{table}[htbp]
\caption{Object detection results of different methods on MS COCO2017. The \textbf{bold} one denotes the best performance.}
\label{table_detection}
\begin{center}
\begin{tabular}{l|ccc|ccc}
\toprule
    Backbone Model &$AP^{bb}$ & $AP^{bb}_{50}$ & $AP^{bb}_{75}$ & $AP^{bb}_S$ & $AP^{bb}_M$ & $AP^{bb}_L$ \\
\midrule
    ResNet-101 \citep{he2016deep} &  37.7 & 57.5 & 40.4 & 21.2 & 42.2 & 49.5 \\
    ResNeXT-101-32x4d \citep{xie2017aggregated}
    & 39.9 & 59.6 & 42.7 & 22.3 & \textbf{44.2} & 52.5 \\
    RelationNet++ \citep{chi2020relationnet++}
     & 39.4 & 58.2 & 42.5 & - & - & - \\
    + S6LA (Ours) & \textbf{40.3} & \textbf{61.7} & \textbf{43.8} & \textbf{24.2} & 44.0 & \textbf{52.5} \\
    \bottomrule
\end{tabular}
\end{center}
\end{table}


% \subsection{Discussion on ablation study}



\subsection{Visualizations}

\begin{figure}
    \centering
    \includegraphics[width=0.875\linewidth]{visualization1.pdf}
    \caption{The visualizations of the feature maps extracted from the end convolutional layer of ResNet, RLA, MRLA and our S6LA. The left ones are original images, the middle colomn is the CAM and the right ones are the combinations of left and middle. Compared with others, the red areas of our method are concentrated in the more critical regions of the object (fish) of classification task.}
    \label{fig:visualization1}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.875\linewidth]{visualization2.pdf}
    \caption{The visualizations of the feature maps extracted from the end convolutional layer of ResNet, RLA, MRLA and our S6LA. The left ones are original images, the middle colomn is the CAM and the right ones are the combinations of left and middle. Compared with others, the red areas of our method are concentrated in the more critical regions of the object of classification task.}
    \label{fig:visualization2}
\end{figure}

To investigate how S6LA contributes to representation learning in convolutional neural networks (CNNs), we utilize the ResNet-50 model \citep{he2016deep} as our backbone. In this study, we visualize the feature maps using score-weighted visual explanations generated by ScoreCAM \citep{wang2020score}, as illustrated in Figure \ref{fig:visualization1} and \ref{fig:visualization2}.

We specifically focus on the final convolutional layer of the ResNet-50 model and our S6LA-enhanced model. This choice is grounded in our observation that the feature maps from the initial three layers of both models exhibit remarkable similarity. In the visualization, the first column presents the original images, the second column displays the ScoreCAM images, and the third column showcases the combination of the original images and their corresponding ScoreCAM.
Both two images in our analysis are randomly selected from the ImageNet validation set, ensuring a diverse representation of the data. According to the definition of the CAM method, areas highlighted in warmer colors indicate stronger contributions to the classification decision.

From our visualizations, it is evident that models enhanced with S6LA exhibit larger warm regions, which align more closely with the classification labels. In contrast, the vanilla ResNet-50 model struggles to identify all relevant object areas compared to our method. This disparity suggests that our approach not only improves the localization of important features but also enhances the model's overall classification performance.

The findings presented in the figure provide direct evidence of the efficacy of our method in the classification task. By leveraging S6LA, we can significantly improve the interpretability of CNNs, allowing for better insights into how these models make decisions based on the features they learn. In summary, our results highlight the advantages of incorporating S6LA into standard architectures like ResNet-50, ultimately leading to more robust and accurate classification outcomes.
