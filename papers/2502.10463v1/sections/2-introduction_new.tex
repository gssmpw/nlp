\section{Introduction}
\label{Introduction}

% 深度模型很好
In recent years, the depth of neural network architectures has emerged as a crucial factor influencing performance across various domains, including computer vision, natural language processing, and speech recognition.
The network models are capable of capturing increasingly complex features and representations from data as they become deeper, and various methods have emerged to utilize larger numbers of layers to improve performance. 
For instance, the VGG network \citep{simonyan2015deepconvolutionalnetworkslargescale} achieves higher classification accuracy by increasing the number of layers, although its foundation primarily relies on empirical results rather than systematical analysis. Other significant advancements, such as those demonstrated by CNNs \citep{he2016deep, ren2016faster, tan2020efficientnetrethinkingmodelscaling} and Transformers \citep{brown2020language, dosovitskiy2020image, touvron2021training, liu2021swin, wang2022pvt}, showcase how deeper architectures can enhance accuracy and generalization. 

% 怎么enhance深度模型performance: layer aggregation
Growing evidence indicates that strengthening layer interactions can encourage the information flow of a deep neural network.
%
For CNN-based networks, ResNet \citep{he2016deep} employed skip connections, allowing gradients to flow more easily by connecting non-adjacent layers. 
DenseNet \citep{huang2018denselyconnectedconvolutionalnetworks} extended this concept further by enabling each layer to access all preceding layers within a stage, fostering a rich exchange of information. 
Later, GLOM \citep{hinton2023represent} proposed an intensely interactive architecture that incorporates bottom-up, top-down, and same-level connections to effectively represent part-whole hierarchies. 
%
Recently, some studies have begun to frame layer interactions with recurrent models and attention mechanisms, such as RLA \citep{zhao2021recurrence} and MRLA \citep{fang2023cross}. 
%
All of the above models have been shown by empirical evidence to outperform those without interdependence across layers, and their achievements are obtained by treating the outputs of network layers as discrete states. 

However, the perspective of discrete treatment may not be suitable when a neural network is very deep; say, for example, ResNet-152 has 152 layers.
\cite{sander2022residualneuralnetworksdiscretize} proposed to treat the ResNet as a discretized version of neural ordinary differential equations, i.e. the whole ResNet is considered as a continuous process with the outputs from layers being the corresponding states; see also \cite{liu2020does}.
Moreover, \citet{queiruga2020continuous} argued that deep neural network models can learn to represent continuous dynamical systems by embedding them into continuous perspective. 
%
This motivates us to conduct layer aggregation among numerous layers of a neural network by alternatively assuming a continuous process to the outputs of layers.
%
% However, this may not be suitable mathematically when a neural network is very deep; say, for example, ResNet has 152 layers \citep{he2016deep}.\ylq{change this claim...}


Meanwhile, the State Space Model (SSM), a mathematical framework for continuously updating physical systems, enabled the modeling of dynamic processes and temporal dependencies in deep learning \citep{gu2023mamba,liu2024vmamba}. Then, Mamba, a selective state space model \citep{gu2023mamba}, proposed selective mechanism and hardware-aware algorithm, which was particularly adept at addressing long sequence modeling challenges. The selection mechanism allows the model to filter out irrelevant information and remember relevant information infinitely.

% For the development of state space model, \citet{gu2021efficiently} proposed structured state space sequence models (S4) which belong to a type of sequence models for deep learning related to RNNs. They are inspired by a particular continuous system and give the definition of latent state $h$ and structured state matrices $A$ \citep{gu2021efficiently}, with interval $\Delta$ and matrices $B$ being the coefficient for the model input signal $x$. \ylq{Can delete this sentence: Then Mamba model \citep{gu2023mamba} proposed selective mechanism and hardware-aware algorithm, which is particularly adept at addressing long sequence modeling challenges and hence is more suitable for our scenario. The details of these are provided in Section \ref{SSM_theory}.}\ylq{shorten}
%


The significance of layer aggregation in deep models and the popularity of SSMs lead us to propose a novel perspective: layer dynamics in very deep networks can be viewed as a continuous process with long sequence modeling task solvable by selective state space model (S6). By leveraging interactions between layers, outputs from different layers can be treated as sequential data input for an S6, allowing the model to encapsulate a richer representation of the information derived from the original data. 
%
By conceptualizing neural networks as state space models, we introduce a novel structure that integrates traditional models into sequential architectures. This approach opens new research avenues that connect traditional statistical methods with contemporary deep learning techniques. Our proposed Selective State Space Model Layer Aggregation (S6LA) effectively harnesses the benefits of layer interactions while incorporating statistical modeling into vision tasks, such as those performed by CNNs and Vision Transformers (ViTs). A schematic of our model is illustrated in Figure \ref{fig:overview}, with parameters $(\Delta,A,B)$ indicating the influence of {\color{blue}$\boldsymbol{X}$} on the implicit latent state $h$. Here {\color{blue}$\boldsymbol{X}^{t-1}$} represents the output at the $(t-1)$-th layer, which can be either a hidden layer in a deep CNN or an attention layer in a transformer model.

\begin{figure}[t]
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=0.9\linewidth]{overview.pdf}
\end{center}
\caption{Schematic diagram of a Network with Selective State Space Model Layer Aggregation.}
\label{fig:overview}
\end{figure}

The main contributions of our work are given below: (1) For a deep neural network, we treat the outputs from layers as states of a continuous process and attempt to leverage the SSM to design the aggregation of layers. To our best knowledge, this is the first time such a perspective has been presented. (2) This leads to a proposed lightweight module, the Selective State Space Model Layer Aggregation (S6LA) module, and it conceptualizes a neural network as a selective state space model (S6), hence solving the layer interactions by the long sequence modelling selective mechanism. (3) Compared with other SOTA convolutional and transformer-based layer aggregation models, S6LA demonstrates superior performance in classification, detection, and instance segmentation tasks.

% cv在最后一个contribution说就可以，只是一个验证的方式
% \begin{enumerate}
%     \item We propose that very deep neural networks can be formulated as state space models. To our knowledge, this is the first time such a perspective has been presented. Our approach redefines how to utilize state space models to integrate different layers, advancing traditional multi-layer models.
%     \item We introduce the State-Space-Interaction-Layer (SSIL), which conceptualizes neural networks as state space models (SSMs) to enhance cross-layer interactions. This framework integrates traditional CNNs, like ResNet, and transformer-based models, such as Deit and Swin Transformer, into sequential architectures, representing the first systematic investigation of such interactions within SSMs.
%     \item In comparison with convolutional and transformer-based vision models, our method demonstrates superior performance in classification, detection, and instance segmentation tasks.
% \end{enumerate}
