\section{Related work}
\label{RelatedWork}


\paragraph{State Space Models.} In the realm of state space models, considerable efforts have been directed toward developing statistical theories. These models are characterized by equations that map a $1$-dimensional input signal $x(t)$ to an $N$-dimensional latent state $h(t)$, with the details provided in \Eqref{eq:SSM_con}. Inspired by continuous state space models in control systems and combined with HiPPO initialization \citep{gu2020hippo}, LSSL \citep{gu2021combiningrecurrentconvolutionalcontinuoustime} showcased the potential of state space models in addressing long-range dependency problems. 
However, due to limitations in memory and computation, their adoption was not widespread until the introduction of structured state space models (S4) \citep{gu2021efficiently}, which proposed normalizing parameters into a diagonal structure. S4 represents a class of recent sequence models in deep learning, broadly related to RNNs, CNNs and classical state space models. Subsequently, \cite{gu2023mamba} introduced the selective structured state space model (S6), which builds upon S4 and demonstrates superior performance compared to transformer backbones in various deep learning tasks, including natural language processing and time series forecasting. 
More recently, VMamba \citep{liu2024vmamba} was developed, leveraging the S6 model to replace the transformer mechanism and employing a scanning approach to convert images into patch sequences. Additionally, Graph-Mamba \citep{wang2024graph} represented a pioneering effort to enhance long-range context modeling in graph networks by integrating a Mamba block with an input-dependent node selection mechanism. These advancements indicate that state space models have also been successfully applied to complex tasks across various domains.

\paragraph{Layer Interaction.} The depth of neural network architectures has emerged as a crucial factor influencing performance. And Figure \ref{fig_corr_layer} of Appendix illustrates the enhanced performance of deeper neural networks. To effectively address the challenges posed by deeper models, increasing efforts have been directed toward improving layer interactions in both CNN and transformer-based architectures. Some studies \citep{hu2018squeeze,woo2018cbam,dosovitskiy2020image} lay much emphasis on amplifying interactions within a layer. DIANet \citep{huang2018denselyconnectedconvolutionalnetworks} employed a parameter-sharing LSTM throughout the network's depth to capture cross-channel relationships by utilizing information from preceding layers. In RLANet \citep{zhao2021recurrence}, a recurrent neural network module was used to iteratively aggregate information from different layers. For attention mechanism, \cite{fang2023cross} proposed to strengthen cross-layer interactions by retrieving query-related information from previous layers. Additionally, RealFormer \citep{he2020realformer} and EA-Transformer \citep{wang2021evolving} both incorporated attention scores from previous layers into the current layer, establishing connections through residual attention. However, these methods face significant memory challenges due to the need to retain features from all encoders, especially when dealing with high-dimensional data and they may lack robust theoretical supports.

