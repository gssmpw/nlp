\section{Experiment}

\begin{table}[t]
\caption{Comparisons of the Top-1 and Top-5 accuracy on the ImageNet-1K validation set with CNNs. All the results are reproduced by us with 4 GeForce RTX 3090 GPUs with the same parameters. The \textbf{bold} fonts denote the best performance.}
\label{table_CNN-based}
\begin{center}
\resizebox{0.80\textwidth}{!}{
\begin{tabular}{c|l|cc|cc}
\toprule
    Model & Method & Params & FLOPs & Top-1 Acc. & Top-5 Acc. \\
\midrule
    \multirow{12}{*}{ResNet-50} & Vanilla \citep{he2016deep} & 25.6 M & 4.1 B & 76.1 & 92.9 \\
    & + SE \citep{hu2018squeeze} & 28.1 M & 4.1 B & 76.7 & 93.4 \\
    & + CBAM \citep{woo2018cbam} & 28.1 M & 4.1 B & 77.3 & 93.7 \\
    & + $A^2$ \citep{chen20182} & 34.6 M & 7.0 B & 77.0 & 93.5 \\
    & + AA \citep{bello2019attention} & 27.1 M & 4.5 B & 77.4 & 93.6 \\
    & + 1 NL \citep{wang2018non} & 29.0 M & 4.4 B & 77.2 & 93.5 \\
    & + 1 GC \citep{cao2019gcnet} & 26.9 M & 4.1 B & 77.3 & 93.5 \\
    & + all GC \citep{cao2019gcnet} & 29.4 M & 4.2 B & 77.7 & 93.7 \\
    & + ECA \citep{wang2020eca} & 25.6 M & 4.1 B & 77.4 & 93.6 \\
    & + RLA \citep{zhao2021recurrence} & 25.9 M & 4.3 B & 77.2 & 93.4 \\
    & + MRLA \citep{fang2023cross} & 25.7 M & 4.6 B & 77.5 & 93.7 \\ 
    % \rowcolor{c1}
    & + S6LA (Ours) & 25.8 M & 4.4 B & \textbf{78.0} & \textbf{94.2} \\
    \midrule
    \multirow{8}{*}{ResNet-101} & Vanilla \citep{he2016deep} & 44.5 M & 7.8 B & 77.4 & 93.5 \\
    & + SE \citep{hu2018squeeze} & 49.3 M & 7.8 B & 77.6 & 93.9 \\
    & + CBAM \citep{woo2018cbam} & 49.3 M & 7.9 B & 78.5 & 94.3 \\
    & + AA \citep{bello2019attention} & 47.6 M & 8.6 B & 78.7 & 94.4\\
    & + ECA \citep{wang2020eca} & 44.5 M & 7.8 B & 78.7 & 94.3 \\
    & + RLA \citep{zhao2021recurrence} & 45.0 M & 8.2 B & 78.5 & 94.2 \\
    & + MRLA \citep{fang2023cross} & 44.9 M & 8.5 B & 78.7 & 94.4 \\
    & + S6LA (Ours) & 45.0 M & 8.3 B & \textbf{79.1} & \textbf{94.8} \\
    \midrule
    \multirow{8}{*}{ResNet-152} & Vanilla \citep{he2016deep} & 60.2 M & 11.6 B & 78.3 & 94.0 \\
    & + SE \citep{hu2018squeeze} & 66.8 M & 11.6 B & 78.4 & 94.3 \\
    & + CBAM \citep{woo2018cbam} & 66.8 M & 11.6 B & 78.8 & 94.4 \\
    & + AA \citep{bello2019attention} & 66.6 M & 11.9 B & 79.0 & 94.6\\
    & + ECA \citep{wang2020eca} & 60.2 M & 11.6 B & 78.9 & 94.5 \\
    & + RLA \citep{zhao2021recurrence} & 60.8 M & 12.1 B & 78.8 & 94.4 \\
    & + MRLA \citep{fang2023cross} & 60.7 M & 12.4 B & 79.1 & 94.6 \\
    & + S6LA (Ours) & 60.8 M & 12.2 B & \textbf{79.4} & \textbf{94.9} \\
    \bottomrule
\end{tabular}
}
\end{center}
\vspace{-0.8cm}
\end{table}

\begin{table}[h]
\caption{Comparisons of the Top-1 and Top-5 accuracy on the ImageNet-1K validation set with vision transformer-based models. All the results are reproduced by us with 4 GeForce RTX 3090 GPUs with the same parameters. The \textbf{bold} fonts denote the best performance.}
\label{table_transformer-based}
\begin{center}
\resizebox{0.80\textwidth}{!}{
\begin{tabular}{c|l|cc|cc}
\toprule
    {\makecell[c]{Backbone}} & Method & Params & FLOPs & Top-1 & Top-5 \\
\midrule
    \multirow{9}{*}{DeiT} 
    & DeiT-Ti \citep{touvron2021training} & 5.7 M & 1.2 B & 72.6 & 91.1 \\
    & + MRLA \citep{fang2023cross} & 5.7 M & 1.4 B & 73.0 & 91.7 \\
    & + S6LA (Ours) & 6.1 M & 1.5 B & \textbf{73.3} & \textbf{92.0} \\
    \cmidrule(lr){2-6}
    & DeiT-S \citep{touvron2021training} & 22.1 M & 4.5 B & 79.9 & 95.0 \\
    & + MRLA \citep{fang2023cross} & 22.1 M & 4.6 B & 80.7 & 95.3 \\
    & + S6LA (Ours) & 23.3 M & 4.8 B & \textbf{81.3} & \textbf{96.0} \\
    \cmidrule(lr){2-6}
    & DeiT-B \citep{touvron2021training} & 86.4 M & 16.8 B & 81.8 & 95.6 \\
    & + MRLA \citep{fang2023cross} & 86.5 M & 16.9 B & 82.9 & 96.3 \\
    & + S6LA (Ours) & 86.9 M & 17.1 B & \textbf{83.3} & \textbf{96.5} \\
    \cmidrule(lr){1-6}
    \multirow{9}{*}{Swin} 
    & Swin-T \citep{liu2021swin} & 28.3 M & 4.5 B & 81.0 & 95.4 \\
    & + MRLA \citep{fang2023cross} & 28.9 M & 4.5 B & 80.9 & 95.2 \\
    & + S6LA (Ours) & 30.5 M & 4.5 B & \textbf{81.5} & \textbf{95.6} \\
    \cmidrule(lr){2-6}
    & Swin-S \citep{liu2021swin} & 49.6 M & 8.7 B & 82.8 & 96.1 \\
    & + MRLA \citep{fang2023cross} & 50.9 M & 8.7 B & 82.5 & 96.0 \\
    & + S6LA (Ours) & 52.5 M & 8.7 B & \textbf{83.3} & \textbf{96.5} \\
    \cmidrule(lr){2-6}
    & Swin-B \citep{liu2021swin} & 87.8 M & 15.4 B & 83.2 & 96.4 \\
    & + MRLA \citep{fang2023cross} & 89.8 M & 15.5 B & 82.9 & 96.3 \\
    & + S6LA (Ours) & 91.3 M & 15.5 B & \textbf{83.5} & \textbf{96.6} \\
    \cmidrule(lr){2-6}
    \cmidrule(lr){1-6}
    \multirow{9}{*}{PVTv2} 
    & PVTv2-B0 \citep{wang2022pvt} & 3.4 M & 0.6 B & 70.0 & 89.7 \\
    & + MRLA \citep{fang2023cross} & 3.4 M & 0.9 B & 70.6 & 90.0 \\
    & + S6LA (Ours) & 3.8 M & 0.6 B & \textbf{70.8} & \textbf{90.2} \\
    \cmidrule(lr){2-6}
    & PVTv2-B1 \citep{wang2022pvt} & 13.1 M & 2.3 B & 78.3 & 94.3 \\
    & + MRLA \citep{fang2023cross} & 13.2 M & 2.4 B & \textbf{78.9} & \textbf{94.9} \\
    & + S6LA (Ours) & 14.5 M & 2.2 B & 78.8 & 94.6 \\
    \cmidrule(lr){2-6}
    & PVTv2-B2 \citep{wang2022pvt} & 25.4 M & 4.0 B & 81.4 & 95.5 \\
    & + MRLA \citep{fang2023cross} & 25.5 M & 4.2 B & 81.6 & 95.2 \\
    & + S6LA (Ours) & 26.1 M & 4.1 B & \textbf{82.3} & \textbf{95.9} \\
    \bottomrule
\end{tabular}
}
\end{center}
\vspace{-0.8cm}
\end{table}
% \vspace{-1.0cm}


This section evaluates our S6LA model in image classification, object detection, and instance segmentation tasks, and provides an ablation study. All models are implemented by the PyTorch toolkit on 4 GeForce RTX 3090 GPUs. More implementation details, and comparisons are provided in Appendix \ref{app:experiment}.

\subsection{Experiments on Image Classification}
\label{imagenet_classification}

\paragraph{Backbone.}
For the dataset, we use Imagenet-1K dataset \citep{5206848} directly. For the CNN backbone, we choose different layers of ResNet \citep{he2016deep}. For transformer-based model, DeiT \citep{touvron2021training}, Swin Transformer \citep{liu2021swin} and PVTv2 \citep{wang2022pvt} are considered. We compare our S6LA with baselines and other layer aggregation SOTA methods with different backbones alone as the baseline models.

\paragraph{Experimental settings.} The hyperparameter of state space model channel $N$ shown in Section \ref{CNN_application} is introduced to control the dimension of feature of $h$ in per S6LA hidden layer module. After comparison of different $N=16,32,64$ for ResNet, we choose 32 as our baseline feature channel, for others we talk about in Section \ref{sec:abl}. In order to compare the baseline models and the models enhanced by S6LA fairly, we use the same data augmentation and training strategies as in their original papers  \citep{zhao2021recurrence,fang2023cross} in all our experiments.

\paragraph{Main results.} The performance of our main results, along with comparisons to other methods, is presented in Tables \ref{table_CNN-based} and \ref{table_transformer-based}. To ensure a fair comparison, all results for the models listed in these tables were reproduced using the same training setup on our workstation. Notably, our model outperforms nearly all baseline models. We specifically compare our S6LA model with other layer interaction methods using ResNets as baselines. The results in Table \ref{table_CNN-based} demonstrate that our S6LA surpasses several layer aggregation methods on CNN backbones, including SENet \citep{hu2018squeeze}, CBAM \citep{woo2018cbam}, $A^2$-Net \citep{chen20182}, NL \citep{wang2018non}, ECA-Net \citep{wang2020eca}, RLA-Net \citep{zhao2021recurrence} and MRLA \citep{fang2023cross}. Additionally, we find that our model consistently outperforms them, achieving nearly a 2\% improvement in Top-1 accuracy with only 0.3 B FLOPs compared to vanilla ResNet models.
Moreover, in comparison with the latest state-of-the-art method MRLA \citep{fang2023cross}, our approach demonstrates fewer FLOPs and higher accuracy. As indicated in Table \ref{table_transformer-based}, our S6LA achieves nearly a 1.5\% improvement in Top-1 accuracy on vanilla vision transformer-based backbones such as DeiT \citep{touvron2021training}, Swin Transformer \citep{liu2021swin}, and PVTv2 \citep{wang2022pvt}, with only a slight increase in parameters (+0.2 M) and FLOPs (+0.3 B), all within acceptable limits for hardware. Again, when compared to the latest SOTA method MRLA \citep{fang2023cross}, our model shows fewer FLOPs and better performance.



\subsection{Experiments on Object detection and instance segmentation}

This subsection validates the transferability and the generalization ability of our model on object detection and segmentation tasks using the three typical detection frameworks: Faster R-CNN \citep{ren2016faster}, RetinaNet \citep{lin2018focallossdenseobject} and Mask R-CNN \citep{he2018maskrcnn}.

\paragraph{Experimental settings.} For the dataset, we choose MS COCO 2017 \citep{lin2014microsoft} for experiments. All the codes are based on the toolkits of MMDetection \citep{chen2019mmdetectionopenmmlabdetection}. The hyperparameter of state space model channel $N$ is introduced to control the dimension of feature of $h$ in per S6LA hidden layer module same to the settings in classification tasks.

\paragraph{Results of object detection and instance segmentation.} For the results of object detection task, Table \ref{table_detection} illustrates the details about AP of bounding box with the notation $AP^{bb}$. It is apparent that the improvements on all metrics are significant. Also compared with the other stronger backbones and detectors, our method outperforms in this task while we only add a little parameters and FLOPs which can be overlooked by the servers.  Meanwhile, Table \ref{table_segmentation} illustrates our S6LA method's improvements about AP of bounding box and mask on all the metrics with Mask R-CNN as the framework. Also similar to the advantages in object detection task, our method balance the parameters and FLOPs with traditional backbones. From the tables' results, it is proved that our S6LA model is feasible.

\begin{table}[t]
\caption{Object detection results of different methods on MS COCO2017. The \textbf{bold} fonts denote the best performance.}
\label{table_detection}
\begin{center}
\resizebox{0.80\textwidth}{!}{
\begin{tabular}{l|c|cccccc}
\toprule
    Method & Detector & $AP^{bb}$ & $AP^{bb}_{50}$ & $AP^{bb}_{75}$ & $AP^{bb}_S$ & $AP^{bb}_M$ & $AP^{bb}_L$ \\
\midrule
    ResNet-50 \citep{he2016deep} & \multirow{12}{*}{\makecell[c]{Faster \\ R-CNN}} & 36.4 & 58.2 & 39.2 & 21.8 & 40.0 & 46.2 \\
    + SE \citep{hu2018squeeze} & & 37.7 & 59.1 & 40.9 & 22.9 & 41.9 & 48.2 \\
    + ECA \citep{wang2020eca} & & 38.0 & 60.6 & 40.9 & 23.4 & 42.1 & 48.0 \\
    + RLA \citep{zhao2021recurrence} & & 38.8 & 59.6 & 42.0 & 22.5 & 42.9 & 49.5 \\
    + MRLA \citep{fang2023cross} & & 40.1 & 61.3 & \textbf{43.8} & 24.0 & 43.9 & 52.2 \\
    + S6LA (Ours) & & \textbf{40.3} & \textbf{61.7} & \textbf{43.8} & \textbf{24.2} & \textbf{44.0} & \textbf{52.5} \\
    \cmidrule(lr){1-1} \cmidrule(lr){3-8} 
    ResNet-101 \citep{he2016deep} &  & 38.7 & 60.6 & 41.9 & 22.7 & 43.2 & 50.4 \\
    + SE \citep{hu2018squeeze} & & 39.6 & 62.0 & 43.1 & 23.7 & 44.0 & 51.4 \\
    + ECA \citep{wang2020eca} & & 40.3 & 62.9 & 44.0 & 24.5 & 44.7 & 51.3 \\
    + RLA \citep{zhao2021recurrence} & & 41.2 & 61.8 & 44.9 & 23.7 & 45.7 & 53.8 \\
    + MRLA \citep{fang2023cross} & & 41.3 & 62.9 & 45.0 & \textbf{24.7} & 45.5 & 53.8 \\
    + S6LA (Ours) & & \textbf{41.7} & \textbf{63.0} & \textbf{45.2} & 24.6 & \textbf{45.6} & \textbf{53.9} \\    
    \midrule
    ResNet-50 \citep{he2016deep} & \multirow{12}{*}{RetinaNet} & 35.6 & 55.5 & 38.2 & 20.0 & 39.6 & 46.8 \\
    + SE \citep{hu2018squeeze} & & 37.1 & 57.2 & 39.9 & 21.2 & 40.7 & 49.3 \\
    + ECA \citep{wang2020eca} & & 37.3 & 57.7 & 39.6 & 21.9 & 41.3 & 48.9 \\
    + RLA \citep{zhao2021recurrence} & & 37.9 & 57.0 & 40.8 & 22.0 & 41.7 & 49.2 \\
    + MRLA \citep{fang2023cross} & & 39.1 & 58.6 & \textbf{42.0} & 23.6 & \textbf{43.3} & 50.8 \\
    + S6LA (Ours) & & \textbf{39.3} & \textbf{59.0} & 41.9 & \textbf{23.7} & 42.9 & \textbf{51.0} \\
    \cmidrule(lr){1-1} \cmidrule(lr){3-8}
    ResNet-101 \citep{he2016deep} &  & 37.7 &57.5& 40.4& 21.1 & 42.2& 49.5 \\
    + SE \citep{hu2018squeeze} & & 38.7 &59.1 &41.6 &22.1 &43.1 &50.9 \\
    + ECA \citep{wang2020eca} & & 39.1 & 59.9 & 41.8 & 22.8 & 43.4 & 50.6 \\
    + RLA \citep{zhao2021recurrence} & & 40.3 & 59.8 & 43.5& 24.2 &43.8 &52.7 \\
    + MRLA \citep{fang2023cross} & & 41.0 & 60.0 & 43.5 & 24.3& 44.1 & 52.8 \\
    + S6LA (Ours) & & \textbf{41.2} & \textbf{60.4} & \textbf{43.8} & \textbf{24.9} & \textbf{45.1} & \textbf{53.0} \\
    \bottomrule
\end{tabular}
}
\end{center}
\vspace{-0.8cm}
\end{table}

\begin{table}[htbp]
\caption{Object detection and instance segmentation results of different methods on MS COCO2017 with Mask R-CNN as a framework. The \textbf{bold} fonts denote the best performance.}
\label{table_segmentation}
\begin{center}
\resizebox{0.80\textwidth}{!}{
\begin{tabular}{l|c|cccccc}
\toprule
    Method & Params & $AP^{bb}$ & $AP^{bb}_{50}$ & $AP^{bb}_{75}$ & $AP^{m}$ & $AP^{m}_{50}$ & $AP^{m}_{75}$ \\
\midrule
    ResNet-50 \citep{he2016deep} & 44.2 M & 37.2 & 58.9 & 40.3 & 34.1 & 55.5 & 36.2\\
    + SE \citep{hu2018squeeze} & 46.7 M & 38.7 & 60.9& 42.1& 35.4& 57.4& 37.8 \\
    + ECA \citep{wang2020eca} & 44.2 M & 39.0 &61.3& 42.1 &35.6& 58.1& 37.7 \\
    + 1 NL \citep{wang2018non} & 46.5 M & 38.0& 59.8& 41.0& 34.7& 56.7& 36.6 \\
    + GC \citep{cao2019gcnet} & 46.9 M & 39.4 &61.6 &42.4 &35.7 &58.4 &37.6 \\
    + RLA \citep{zhao2021recurrence} & 44.4 M & 39.5& 60.1 &43.4 &35.6& 56.9& 38.0 \\
    + MRLA \citep{fang2023cross} & 44.4 M & 40.4& \textbf{61.8}& 44.0& \textbf{36.9} &57.8& \textbf{38.3} \\
    + S6LA (Ours) & 44.9 M & \textbf{40.6} & 61.5 & \textbf{44.2} & 36.7 & \textbf{58.3} & \textbf{38.3} \\
    \midrule
    ResNet-101 \citep{he2016deep} & 63.2 M  & 39.4 &60.9 &43.3 &35.9 &57.7& 38.4 \\
    + SE \citep{hu2018squeeze} & 67.9 M & 40.7 &62.5& 44.3& 36.8& 59.3& 39.2 \\
    + ECA \citep{wang2020eca} & 63.2 M & 41.3 &63.1 &44.8 &37.4& 59.9& 39.8 \\
    + 1 NL \citep{wang2018non} & 65.5 M & 40.8 &63.1 &44.5 &37.1 &59.9 &39.2 \\
    + GC \citep{cao2019gcnet} & 68.1 M &41.1 &63.6& 45.0& 37.4& 60.1& 39.6 \\
    + RLA \citep{zhao2021recurrence} & 63.6 M & 41.8& 62.3 &46.2 &37.3& 59.2& 40.1 \\
    + MRLA \citep{fang2023cross} & 63.6 M & 42.5 &\textbf{63.3}& 46.1& 38.1& 60.3& 40.6 \\
    + S6LA (Ours) & 64.0 M & \textbf{42.7} &\textbf{63.3} &\textbf{46.2} &\textbf{38.3} &\textbf{60.5} &\textbf{41.0} \\    
    \bottomrule
    
\end{tabular}
}
\end{center}
\vspace{-0.8cm}
\end{table}

\vspace{-6pt}

\subsection{Ablation study}
\label{sec:abl}

\paragraph{Different variants of S6LA.} 
Due to resource limitations, we only experiment with the ResNet-50 and DeiT models on the ImageNet dataset. Our investigation considers several factors: (a) the influence of $\boldsymbol{X}$ on $h$ (where the opposite is $h$ randomized for each iteration); (b) the hidden state channels set to 16, 32, and 64; (c) the selective mechanism involving the interval $\Delta$ and coefficient $B$; (d) for the Transformer-based method, using simple concatenation instead of multiplication.

From our analysis of the results presented in Tables \ref{tab:abl1} and \ref{tab:abl2}, several key findings emerge. Firstly, models incorporating our S6LA framework demonstrate superior performance compared to those without it. Notably, using a trainable parameter $h$ ($h$ is influenced by $\boldsymbol{X}$) yields better performance. Secondly, regardless of whether we use ResNet or DeiT, we find that a channel dimension of $N=32$ yields the best results. Finally, the selective mechanism is crucial for our model; specifically, for the vision Transformer method (in this case, DeiT), the multiplication of $\boldsymbol{X}$ and $h$ outperforms simple concatenation used in CNN backbones.

\begin{table}[htbp]
\centering
\begin{minipage}{\textwidth}
\hspace{0.02\textwidth}
 \begin{minipage}[t]{0.45\textwidth}
 \centering
     \caption{The influence of trainable $h$ and selective mechanism of $\Delta$ and $B$.}
     \vspace{0.25cm}
     \resizebox{0.85\textwidth}{!}{
     \begin{tabular}{cccc}
        \toprule
        \multicolumn{2}{c}{Model} & Params & Top-1 \\
        \midrule
        \multirow{2}{*}{{ResNet}} &  S6LA & 25.8 M & \textbf{{78.0}} \\
        & w/o trainable $h$ & 25.8 M & 77.4 \\
        \midrule
         \multirow{2}{*}{{DeiT-Ti}} & S6LA & 6.1 M & \textbf{{73.3}} \\
        & w/o trainable $h$ & 6.1 M & 72.5 \\
        \midrule
        \multirow{2}{*}{{ResNet}} & S6LA & 25.8 M & \textbf{{78.0}} \\
        & w/o selective & 25.8 M & 77.3 \\
        % & w/o selective (only $\Delta$) & 25.8 M & 77.2 \\
        \midrule
        \multirow{2}{*}{{DeiT-Ti}} & S6LA & 6.1 M & \textbf{{73.3}} \\
        & w/o selective & 6.1 M & 72.7 \\
       
        \bottomrule
        \label{tab:abl1}
    \end{tabular}
    }
  \end{minipage}
  \hspace{0.04\textwidth}
  \begin{minipage}[t]{0.45\textwidth}
  \centering
        \caption{The influence of latent dimension $N$ and the treatment of DeiT-Ti.}
        \vspace{0.25cm}
        \resizebox{0.85\textwidth}{!}{
         \begin{tabular}{cccc}
    \toprule
        \multicolumn{2}{c}{Model} & Params & Top-1 \\
        \midrule

       \multirow{3}{*}{{ResNet}} & $N = 16$ & 25.8 M & 77.9 \\
       & $N = 32$ & 25.8 M & \textbf{{78.0}} \\
       &  $N = 64$ & 25.9 M & 77.7 \\
       \midrule
        \multirow{3}{*}{{DeiT-Ti}} &$N = 16$ & 5.9 M & 72.7 \\
       & $N = 32$ & 6.1 M & \textbf{{73.3}} \\
       & $N = 64$ & 6.3 M & 72.9 \\
        \midrule
        \multicolumn{2}{c}{DeiT-Ti (S6LA)} & 6.1 M & \textbf{{73.3}}  \\
        \multicolumn{2}{c}{DeiT-Ti (Concatenation)} & 6.1 M & 72.6 \\
        \bottomrule
        
        \label{tab:abl2}
    \end{tabular}
    }
   \end{minipage}
   \hspace{0.02\textwidth}
\end{minipage}
\vspace{-0.8cm}
\end{table}


% \textcolor{red}{
% 1. visualization results;
% Ablation on S6:
% 2. channel of state;
% 3. selective of B, delta, C;
% 4. initialization of A;
% Ablation of implementation of CNN and ViT:
% 5. our other unsuccessful intermediate implementation;
% }