Layer Aggregation via Selective State Space Model
\subsection{The Formula of S6LA}

Denote a sequence $\mathbf{X} = \{\boldsymbol{X}^1, \cdots , \boldsymbol{X}^T\}$, where $\boldsymbol{X}^t$ is the output from $t$th layer, say Convolutional layers/blocks or Attention layers, of a deep neural network, and $T$ is the number of layers.
In financial statistics, the price of an asset can be treated as a process with discrete time when it is sampled in a low frequency, say weekly data, while it will be treated as a process with continuous time the sampling frequency is high, say one-minute data; see \cite{yuan2023haritomodelshighdimensionalhar}.
Accordingly, we may treat $\mathbf{X}$ as a sequence with discrete time as the number of layers $T$ is small or even moderate, and all existing methods for layer aggregation in the literature follow this way.
%, while we may look it as discretized observations of a continuous process as in \cite{a} and \cite{pavan2007high}. 
%time series models with discrete states are employed for low-frequency data, while the diffusion model with continuous processes is a standard tool for high-frequency data.
%In previous literature \citep{pavan2007high}}, all existing methods for layer aggregation treat  $\boldsymbol{X}^t$'s to be discrete states, and hence they all correspond to time series tools in statistics.
However, for a very deep neural network, it is more like the scenario of high-frequency data, and hence a continuous process is more suitable for the sequence $\mathbf{X}$ \citep{sander2022residualneuralnetworksdiscretize,queiruga2020continuous}.
This section further conducts layer aggregation by considering state space models in Section 3.1; see Figure \ref{fig:overview} for the illustration.

Specifically, we utilize the Mamba model \citep{gu2023mamba} due to its effectiveness in processing long sequences.
This model is based on S6 models and can provide a better interpretation on how to leverage the previous information and then how to store it based on its importance. Moreover, it has been demonstrated to have a better performance than traditional transformers and RNNs.
Following its principle, we propose our selective state space model layer aggregation below:
\begin{equation}
    \label{S6_rec_ori}
        h^t = g^t(h^{t-1},\boldsymbol{X}^{t}),  \quad
        \boldsymbol{X}^t = f^t(h^{t-1},\boldsymbol{X}^{t-1}),
\end{equation}
where $h^t$ is a hidden state similar to $A^t$ in \Eqref{eq:CNN_agg}, and it represents the recurrently aggregated information up to $(t-1)$th layer.
We may consider an additive form, as in \Eqref{eq:densenet_re}, for $h^t$. 
Moreover, $g^t$ is the relation function between the current SSM hidden layer state and previous hidden layer state with input. As a result, similar to \Eqref{eq:SSM_dis}, the update of $h^t$ can be formulated as:
\begin{equation}
    \label{S6_rec_up}
        h^t = \overline{A}h^{t-1} + \overline{B}\boldsymbol{X}^t,  \quad
        \boldsymbol{X}^{t} = f^t(h^{t-1},\boldsymbol{X}^{t-1}).
\end{equation}
The choice of function $f^t$ is different for CNNs and Transformer-based models, and they are detailed in the following two subsections; see Figures \ref{fig:overview_CNN} and  \ref{fig:overview_Transformer} for their illustrations, respectively. 

%先总体介绍一个Network怎么对应到SSM的H,A,B,delta
%接下来，具体给你们展示两个例子：

\subsection{Application to Deep CNNs}
\label{CNN_application}

\begin{figure}[t]
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=0.9\linewidth]{overview_CNN.pdf}
\end{center}
Detailed operations in S6LA module with Convolutional Neural Network. The green arrow shows the hidden state connection, while the grey arrow indicates layers communications.
\label{fig:overview_CNN}
\end{figure}

For CNNs backbones, we adopt ResNet \citep{he2016deep} as the baseline architecture. We propose to concatenate the input at each layer, say $\boldsymbol{X}^t \in \mathbb{R}^{H \times W \times D}$, where $H$ and $W$ represent the height and width, and $D$ indicates the embedding dimension. For each CNN layer, the input to each block in ResNet—comprising a combination of 1D and 3D convolutional operations—is formed by concatenating $\boldsymbol{X}^{t}$ with the state $h^{t-1} \in \mathbb{R}^{H \times W \times N}$ from the previous layer, where $N$ is the dimension of latent states. This integration effectively incorporates SSM information into the convolutional layers, enhancing the network's capacity to learn complex representations. 
% Upon obtaining the output from the $t$-th CNN block, 

For the specific implementation of S6LA in CNNs, we initialize the SSM state $h^0$ using Kaiming normal initialization method \citep{he2015delving}. This initialization technique is crucial for ensuring effective gradient flow throughout the network, and we will further clarify this point in ablation studies. Next, we employ a selective mechanism to derive two components, the coefficient $B$ for input and the interval $\Delta$ as specified in \Eqref{eq:SSM_dis}. For transition matrix $A$, the initial setting is same as in Mamba models \citep{gu2023mamba}. Then with \Eqref{eq:SSM_dis}, we can get the next hidden layer $h^{t}$ based on the last $h^{t-1}$ and $\boldsymbol{X}^{t}$ for each layer in CNNs.

Utilizing \Eqref{eq:SSM_dis}, we compute the subsequent latent state $h^{t}$ based on the previous state $h^{t-1}$ and the input $\boldsymbol{X}^{t}$ for each layer within the CNN architecture. This methodological framework facilitates improved information flow and retention across layers, thereby enhancing the model's performance. Therefore, the specifics of leveraging our S6LA method with CNNs backbones can be outlined as follows:

% \begin{itemize}
%     \item Layer ${t-1}$: We begin by merging the input $\boldsymbol{X}^{t-1}$ and the hidden state $h^{t-1}$ through a simple concatenation along the feature dimension. This concatenated representation allows us to generate the output $\boldsymbol{O}^{t-1}$ with the CNNs backbone (such as ResNet).
%     \item Next Step Computation: The output component $\boldsymbol{O}^{t-1}$ from previous step, contributes to the next input $\boldsymbol{X}^{t}$ and the hidden state $h^t$. Here, the dimensions of $\boldsymbol{O}^{t-1}$ are $H \times W \times D$ where $H$ and $W$ correspond to the height and width of the input images, respectively, and $D$ represents the feature dimension.
%     \item State Update: For the input state update, we define $\boldsymbol{X}^t$ as the sum of $\boldsymbol{X}^{t-1}$ and $\boldsymbol{O}^{t-1}$. For the hidden state, $h^t$ is derived as a function of these two components, following the formulation provided in Equations \ref{eq:SSM_dis}. The equations are as follows with two trainable parameters $W_{\Delta}$ and $W_B$ (for the $t-1$ layer):
% \begin{equation}
%     \label{CNN_s6_1}
%     h^t = e^{(\Delta A)} h^{t-1} + \Delta B \boldsymbol{O}^{t-1},
% \end{equation}
% where $\Delta = W_{\Delta} (Conv(\boldsymbol{O}^{t-1})),
%     B = W_B (Conv(\boldsymbol{O}^{t-1}))$.
% \end{itemize}

\para{Input Treatment:} We begin by merging the input $\boldsymbol{X}^{t}$ and the hidden state $h^{t-1}$ through a simple concatenation along the feature dimension. This concatenated representation allows us to generate the output $\boldsymbol{O}^{t}$ with the CNNs backbone (such as ResNet).

\para{Latent State Update:} For the input state update, we define $\boldsymbol{X}^{t+1}$ as the sum of $\boldsymbol{X}^{t}$ and $\boldsymbol{O}^{t}$. For the hidden state, $h^t$ is derived as a function of these two components, following the formulation provided in \Eqref{eq:SSM_dis}. The equations are as follows with two trainable parameters $W_{\Delta}$ and $W_B$ (for the $t-1$ layer):
\begin{equation}
    \label{CNN_s6_1}
    h^t = e^{(\Delta A)} h^{t-1} + \Delta B \boldsymbol{O}^{t},
\end{equation}
where $\Delta = W_{\Delta} (\text{Conv}(\boldsymbol{O}^{t})),
    B = W_B (\text{Conv}(\boldsymbol{O}^{t}))$.

\para{Output Computation:} The output component $\boldsymbol{O}^{t}$ from the input treatment step, contributes to the next input $\boldsymbol{X}^{t+1}$ and the computation is: $\boldsymbol{X}^{t+1} = \boldsymbol{O}^{t} + \boldsymbol{X}^{t}$.

\begin{figure}[t]
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=0.9\linewidth]{overview_Transformer.pdf}
\end{center}
Diagram of the S6LA architecture with Transformer. The green arrow shows the hidden state connection, while the grey arrow indicates communication between layers. The input consists of image patch tokens (\( \boldsymbol{X}^{t}_p \)) and a class token (\( \boldsymbol{X}^{t}_c \)), processed through positional embedding and attention. The class token is cloned as $\boldsymbol{X}_c^{t'}$, and parameters \( W_\Delta \) and \( W_B \) update the hidden state (\( h^t \)). Updated patch tokens are combined with the class token to form the next input (\( \boldsymbol{X}^{t+1} \)).
\label{fig:overview_Transformer}
\end{figure}

\subsection{Application to Deep ViTs}

In our exploration of S6LA implementation in deep ViT backbones, we draw parallels between the integration of the state space model (SSM) state and the mechanisms used in convolutional neural networks (CNNs). However, the methods of combining inputs within transformer blocks differ significantly from those in CNNs. Like the treatment of attention mechanism, we utilize multiplication combination instead of simply concatenating to deal with the input $\boldsymbol{X}^{t}$ and $h^{t-1}$ in transformer-based models. This approach enhances the interaction between input features and SSM, enabling richer feature representation. Then the next paragraghs gives the specifics of leveraging our S6LA method with ViTs backbones as follows:

\paragraph{Input Treatment:} We begin by combining the class token and input, alongside the application of positional embeddings. 
% The input dimension for the transformer network is defined as: $\boldsymbol{X}^{t-1}_{input} \in \mathbb{R}^{1 \times D}$ where $D = (N+1) \times C$, $N$ is the number of patches and $C$ is the embedding dimension. 
Then following the attention mechanism, $\boldsymbol{X}^{t}_{\text{input}} \in \mathbb{R}^{(L+1)\times D}$ appeared where $L$ is the number of patches and $D$ is the embedding dimension, and it is split into two components next: image patch tokens $\boldsymbol{X}^{t}_p \in \mathbb{R}^{L\times D}$ and a class token $\boldsymbol{X}^{t}_c \in \mathbb{R}^{1 \times D}$.
\begin{small}
\begin{equation}
\boldsymbol{X}^{t}_{\text{input}} = \text{Add} \& \text{Norm}(\text{MLP}(\text{Add} \& \text{Norm}(\text{Attn}(\boldsymbol{X}^{t}))));   \quad \boldsymbol{X}^{t}_p, \boldsymbol{X}^{t}_c = \text{Split}(\boldsymbol{X}^{t}_{\text{input}}).
\end{equation}
\end{small}
The class token plays a crucial role in assessing the correlation between $\boldsymbol{X}^{t}$ and $h^{t-1}$. Our model setting can effectively bridge the features extracted from the patches with the SSM state by facilitating a better integration into the hidden state since it could be considered as a summary feature of the image in sequential layers. 

\paragraph{Latent State Update:} Given the split class token in last step, the hidden state is updated similar to application in CNNs:
\begin{equation}
    h^t = e^{(\Delta A)} h^{t-1} + \Delta B \boldsymbol{X}^{t}_c,
\end{equation}
where $\Delta$ and $B$ are calculated from class token with selective mechanism:
\begin{equation}
\Delta = W_{\Delta} (\boldsymbol{X}^{t}_c), \quad
    B = W_B (\boldsymbol{X}^{t}_c).
\end{equation}

\paragraph{Output Computation:} At the same time, the new patch tokens $\widehat{\boldsymbol{X}}_p^{t}$ are computed as the sum of the previous patch tokens and the product of the previous patch tokens with $h^t$:
\begin{equation}
    \widehat{\boldsymbol{X}}_p^{t} = \boldsymbol{X}^{t}_p + W\boldsymbol{X}^{t}_p h^t.
\end{equation}
Then the next input, $\boldsymbol{X}^{t+1}$, is derived from the concatenation of the updated patch and class tokens:
\begin{equation}
   \boldsymbol{X}^{t+1} = \text{Concat}(\widehat{\boldsymbol{X}}_p^{t},\boldsymbol{X}^{t}_c). 
\end{equation}



