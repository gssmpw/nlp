\begin{figure*}[t]
    \centering
    \includegraphics[width=.9\textwidth]{fig/pipeline_method.pdf}
    \caption{Mask2Tag systems rely on class-agnostic masking models, where masks are associated with names. Oppositely, in our Tag2Mask pipeline, the image tagger supervises the text encoder, while the similarity between the text and vision embeddings produces the segmentation prediction. The segmentation module benefits from pretraining on a highly curated, close-set dataset.}
    \label{fig:method}
\end{figure*}

\section{Best practice for VSS}
\label{ch:method}
In this section, we describe our experimental framework for analyzing the role of each component in Vocabulary-Free Semantic Segmentation. %
Unlike previous methods that rely on zero-training segmentation modules with limited class transfer capabilities \cite{conti2024vocabulary} or implement simplistic mask2tag pipelines \cite{rewatbowornwong2023zero,kawano2024tag}, our approach leverages the complementary strengths of two state-of-the-art models: a text-based image tagger for semantic supervision and an open-set segmentation model, as shown in \Cref{fig:method}.
Specifically, we introduce a system that utilizes a text tagger to provide class-level semantic information, employs a segmentation model with global conceptual knowledge from CLIP, and fine-tunes the model on a training dataset to learn precise segmentation skills. We first formalize the Vocabulary-Free Semantic Segmentation (VSS) problem (\Cref{sec:formulation}), then detail how we integrate these models in our training setup (\Cref{sec:training}). Finally, we describe the evaluation protocol for assessing segmentation quality (\Cref{sec:assignment}).
The approach emphasizes the novelty of the system's strategy rather than claiming to propose entirely new components, highlighting the integration of existing state-of-the-art models and explaining the specific improvements over previous methods through its innovative class-aware strategy. By combining these approaches, we provide a comprehensive framework that advances segmentation capabilities in open-set scenarios.

\subsection{Problem Formulation}\label{sec:formulation}
In the standard open-vocabulary segmentation (OVSS) setting, given an image \( I \), the set of class names (labels) \( \mathcal{C} = \{c_1, c_2, \dots, c_N\} \) is known a priori. In contrast, our approach assumes that the class name set \( \mathcal{C} \) is pre-defined during training time, yet at inference time, we associate the image with an open-ended set of words predicted by a vision-language model, rather than the fixed set \( \mathcal{C} \).

Specifically, the goal of Vocabulary-Free Semantic Segmentation (VSS) is to segment an image \( x \) into regions corresponding to semantic concepts without any prior knowledge of the semantic categories \( C \). Instead of using a fixed vocabulary of classes, we operate directly within a vast semantic space \( S \), which includes all possible semantic concepts.
In the VSS setup, the function \( f \) maps the image space \( X \) to a semantic map in \( S^{(H \times W)}\), and each pixel is assigned a label from the large semantic space \( S \). The function is defined as \( f: X \to S^{(H \times W)} \).
At test time, the function \( f \) relies solely on the input image \( x \) and a broad repository of semantic concepts approximating \( S \). This enables zero-shot segmentation, where the model can segment an image into novel categories  not part of the training data.

To implement this, our pipeline first performs image tagging to recognize objects present in the image. This step generates a set of descriptive tags that may include both known concepts and unexpected objects. Next, we leverage the broader semantic understanding of the vision-language model to provide additional contextual information about these detected objects, going beyond the limited set of predefined class names. Finally, we use this %
textual representation to guide the segmentation process, allowing our model to handle a dynamic and open-ended set of visual concepts without requiring retraining for new categories. The pipeline of the approach is shown in \Cref{fig:method}.

This task is inherently challenging due to the vast size of the semantic space \( S \), which is much larger than typical predefined label sets. For example, while the largest segmentation benchmark for open vocabulary has roughly 900 classes, resources like the vast web or a large-scale knowledge base contain millions of potential semantic concepts, covering a much broader range of categories. The model must differentiate between concepts across diverse domains, and handle long-tailed distributions and ambiguous regions in images. The VSS approach also needs to address fine-grained recognition and segmentation of objects, which is difficult without predefined class labels.

\subsection{Two-stage Pipeline}\label{sec:training}
While theoretically, a training dataset may introduce biases due to its limited label lexicon, training-free or class-free open-vocabulary methods often yield inferior segmentation results compared to trained approaches \cite{shin2024towards}. Additionally, vocabulary-free methods present a more challenging scenario, as classes are not predefined and may be overlooked if the tagger fails to recognize them. Nevertheless, we opted for a trained segmentation backbone, by %
adopting the open-vocabulary training scheme. 
During inference, tagging is guided by a pre-trained VLM to predict potential classes within the images. %
The segmentation baseline %
follows.

\textbf{Segmentation framework:} 
Initially, dense image embeddings \( D^V \in \mathbb{R}^{(H \times W) \times d} \) are generated using the CLIP image encoder \( \Phi^V \), where \( H \) and \( W \) represent the height and width of the image, respectively, and \( d \) is the embedding dimension. Concurrently, each class name in  \( \mathcal{C} \) is used in a template \( T \) - "A photo of  a \{class name\}" - to produce the text embeddings \( D^L \in \mathbb{R}^{N \times d} \) that are generated using a CLIP text encoder \( \Phi^L \), where \( N \) is the number of classes.
A cost volume \( C \in \mathbb{R}^{(H \times W) \times N} \) is computed by calculating the cosine similarity between each spatial location in the image embeddings and each class embedding. The cost volume is defined as:
\begin{equation}
    C_{i,n} = \frac{D^V(i) \cdot D^L(n)}{|D^V(i)| \, |D^L(n)|}
\end{equation}
where \( i \) indexes the spatial locations and \( n \) indexes the classes.
The algorithm  \cite{cho2024cat} then performs \( K \) iterations of feature aggregation to refine the initial cost embedding \( F \). Each iteration consists of two steps: 

1. \textbf{Spatial aggregation:}
\begin{equation}
    F'(:,n) = T^{\text{sa}}([F(:,n); P^V(D^V)])
\end{equation}

A spatial aggregation transformer \( T^{\text{sa}} \) is applied to each class channel, incorporating guidance from the projected image embeddings \( P^V(D^V) \). 

2. \textbf{Class aggregation:}
\begin{equation}
    F''(i,:) = T^{\text{ca}}([F'(i,:); P^L(D^L)])
\end{equation}
A class aggregation transformer \( T^{\text{ca}} \) is applied to each spatial location, incorporating guidance from the projected text embeddings \( P^L(D^L) \).
After each iteration, the refined features \( F'' \) are used as input for the next iteration.
At the end of the \( K \) iterations of feature aggregation, an upsampling decoder is applied to the final refined features \( F \). This decoder produces the segmentation prediction for all \( N \) classes.

\subsection{Evaluation Assignment}
\label{sec:assignment}
The predictions of the image tagger are not necessarily aligned with the dataset labels as the latter does not allow for synonyms or coarse-to-fine assignments (e.g., if "tower" is not a label, it should be interpreted as "building"). While not strictly necessary for functionality, a remapping process is required to enable a fairer evaluation consistent with those used in prior studies \cite{kawano2024tag,rewatbowornwong2023zero}. This results in two types of mean Intersection over Union (mIoU) metrics: a hard assignment, calculated when the predictions from the image tagger are exactly the same as the ground truth class names, and a soft mIoU, calculated using the remapping process.
For the soft mIoU, the goal is to map each predicted class name $p_i$ to the most appropriate ground truth class name $c_j$. If a predicted class name $p_i$ is already present in the set of ground truth class names $\mathcal{C}$, it is directly assigned to the corresponding $c_j$.
For the remaining predicted class names, we compute sentence embeddings using a pre-trained model, e.g., Sentence-BERT \cite{reimers2019sentence}. Then, for each $p_i \in \mathcal{P}$ and $c_j \in \mathcal{C}$, we calculate the cosine similarity between their embeddings. The ground truth class name $c^*$ that maximizes this similarity is assigned to the predicted class name $p_i$:
\begin{equation}
    c^* = \argmax_{c_j \in \mathcal{C}} \text{sim}(\Phi^L(p_i), \Phi^L(c_j))
\end{equation}
where $\text{sim()}$ is the cosine similarity between the embeddings.
This remapping approach allows the model's predictions to be better aligned with the ground-truth labels, even when the predicted and ground-truth texts do not exactly match. Note that, if the highest cosine similarity score is below a threshold $T_\text{SBERT}$, the match is discarded.
