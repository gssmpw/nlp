\section{Related Work}
\label{sec:related}
\textbf{Open-Set Understanding:}
The open-set task, first introduced by Scheirer et al. \cite{scheirer2012toward}, challenges the conventional closed-set paradigm commonly assumed in image recognition. In closed-set models, the testing phase only includes samples from a predefined set of classes known to the model during training. Conversely, open-set recognition addresses scenarios where samples can belong to previously unknown classes that were not present during training. This requires models to both recognize and reject instances from unfamiliar classes, ensuring robustness against unknown inputs. The open-set framework has seen extensive study across multiple areas in computer vision, such as image classification~\cite{bendale2016towards, vaze2022open, yoshihashi2019classification, oza2019c2ae, perera2020generative, chen2021adversarial,zhang2020hybrid}, %
object detection~\cite{han2022expanding, miller2020uncertainty, zhou2023open}, %
and image segmentation~\cite{hwang2021exemplar, pham2018bayesian, cen2021deep}.%


\textbf{Open-vocabulary Semantic Segmentation (OVSS):}
Recent advances in vision-language models (VLMs) such as CLIP~\cite{radford2021learning} have demonstrated that robust, transferable visual representations can be effectively learned from large-scale datasets using only weakly structured natural language descriptions.
Initially adopted in image-level tasks like classification, VLMs leverage both visual and textual embeddings to recognize a diverse set of classes. %
By aligning image features with semantic concepts in a shared space, VLMs achieve a form of zero-shot learning that allows them to identify new classes at test time, offering a flexible framework for generalization \cite{liu2024open,xie2024sed,cho2024cat}. 
Although open-vocabulary learning presents an appealing solution for handling arbitrary classes, scaling this approach to accommodate an ever-growing set of classes poses significant challenges. In theory, a VLM could achieve perfect generalization if its query set contained every conceivable class label. However, as demonstrated by Miller et al. \cite{miller2025open}, adding more classes to the query set does not lead to better performance. In fact, increasing the number of class labels introduces a greater likelihood of misclassifications, leading to degraded model accuracy. This degradation occurs because, as more classes are added, the semantic space becomes increasingly crowded, causing overlaps that make accurate distinctions between classes harder to achieve. 
To tackle scalability, one solution is training class-free models \cite{shin2024towards}, while distinguishability can be improved by enhancing the textual descriptions of the classes \cite{ma2024open,jiao2024collaborative}. However, all these approaches assume that the inference label set is predefined and available at inference time.%




\textbf{Vocabulary-Free Semantic Segmentation (VSS):} Recent research in VSS has focused on developing end-to-end solutions while reducing bias from  ground truth data annotations. The majority of current methods decompose the task into a class-agnostic mask generation and a class association (Mask2Tag). Zero-Seg \cite{rewatbowornwong2023zero} and TAG \cite{kawano2024tag} leverage DINO \cite{caron2021emerging} to generate the masks, followed by CLIP-based \cite{radford2021learning} embedding generation for class association. Zero-Seg processes these embeddings %
following ZeroCap \cite{tewel2021zero} to obtain textual classes, while TAG matches them against an external database. 
Conversely, CaSED \cite{conti2024vocabulary} identifies potential classes by querying an external caption database using a pre-trained VLM. %
Similarly, Auto-Seg \cite{ulger2024autovocabularysemanticsegmentation} %
fine-tunes a captioning model to extract class names at multiple scales, followed by a second stage where an open-vocabulary model generates segmentation masks, with predictions remapped to ground truth classes using LLMs. While these approaches demonstrate promising results, they do not fully explore how this pipeline decomposition impacts model performance, nor do they investigate methods to enrich the textual information in the CLIP encoder. We address these limitations by providing a comprehensive analysis of the text encoder's role and exploring techniques to enhance visual-language understanding through richer textual representations. Moreover, we rigorously test the sensitivity of CLIP to tagger errors, evaluating how inaccuracies in image tagging propagate and impact the final segmentation performance.
