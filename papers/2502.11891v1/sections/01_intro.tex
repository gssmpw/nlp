\section{Introduction}
\label{sec:intro}

Scene understanding is fundamental in computer vision, with segmentation playing a crucial role by providing pixel-level semantic distinctions between objects within an image. Traditionally, segmentation models were trained on closed-set, dataset-specific classes, limiting their flexibility to predefined object categories. However, as the need for a more generalized understanding of visual scenes grows, segmentation has transitioned towards open-world settings \cite{pham2018bayesian,cen2021deep}.
In open-vocabulary segmentation, models can segment any user-defined object outside the set of classes seen during training by giving as input an image and a free-form set of class names of interest \cite{xu2023side,cho2024cat}.
While open-vocabulary segmentation enables a more adaptable understanding across a range of real-world scenarios, it presents several practical challenges: %
(1) the feasibility of knowing all relevant object classes a priori -- which is also not scalable --, and (2) the quality of the class names themselves. 
Regarding (1), for open-vocabulary segmentation to work effectively, users must often specify in advance the list of the classes that the model should recognize. 
\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{fig/teaser_voc-free_1.pdf}
    \caption{Traditional open-set understanding separates training and test classes, with an overlap allowing for unseen classes in testing.  In Vision-Language Models (VLMs), trained on broad, internet-scale data, open-set recognition persists but is constrained by a finite query set. In a Vocabulary-Free setting, test classes are unrestricted, allowing any concept known to the VLM, introducing more realistic, complex variations beyond predefined vocabularies.
    }
    \label{fig:teaser}
\end{figure}

        

This requirement can be impractical or even prohibitive in real-world applications where objects might be unexpected or difficult to enumerate. For instance, if a dataset of hotel rooms unexpectedly contains a bathtub in the middle of a room, not listing “bathtub” in the input classes could lead the model to misclassify it as something more generic or expected for the scene, like “bed” or “table.” This difficulty of predefining classes is akin to a \textbf{\textit{chicken-and-egg}} problem: users need knowledge of all potential objects within a scene to identify them, yet the purpose of segmentation is often to discover these very objects. This limitation restricts open-vocabulary segmentation’s applicability in settings where out-of-context or out-of-distribution objects appear.
Moreover, for (2) the quality of class names themselves often adds to these challenges. As highlighted in recent works, existing annotations frequently use imprecise or overly general terms, which may not align with human categorizations \cite{huang2024renovating}. Large datasets typically involve multiple annotators, who may assign different names for the same visual concept, leading to inconsistent or non-intuitive labels. For example, a class like ``vehicle'' may sometimes be labeled as ``car'' or ``transport,'' depending on annotators' preferences. This inconsistency can hinder segmentation models’ ability to learn precise object distinctions, as they may interpret general labels more broadly than intended, or miss specific instances altogether.

To address these limitations, we propose a fully automated pipeline that first performs image tagging to recognize objects, and then generates descriptive context when needed, before proceeding with segmentation. More in detail, we integrate the original annotations of the training and validation datasets with language model capabilities to create a more comprehensive and adaptive system. Differently from prompt-based methods - like SAM \cite{kirillov2023segment} - or other user-guided frameworks such as traditional open-vocabulary approaches \cite{cho2024cat,shin2024towards}, our pipeline operates in a completely automated manner, leveraging both human-verified labels (in training only) and the model's broader semantic understanding to handle unexpected objects and scenarios.
An overview of the task is shown in \Cref{fig:teaser}.
Our contribution can be decomposed into different stages:
\begin{itemize}
    \item First, we propose a two-stage pipeline for Vocabulary-Free segmentation, composed of an image tagging module and a class-specific decoder based on VLMs. Due to its simplicity and performance superiority over existing approaches, it serves as a benchmark for the community to accelerate future research on VSS.
    \item Next, under the assumption of a perfect image tagger, we explore the effects of providing enriched textual inputs to the text encoder in the segmentation model.
    \item  Finally, we assess the impact of unreliable tagging on segmentation performances, particularly the effect of undetected objects - false negatives - and wrong ones - false positives.
\end{itemize}
