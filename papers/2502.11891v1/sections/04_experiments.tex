\begin{table}[t]
\centering
\caption{Results over the benchmark datasets. The mIoU is reported. %
}
\label{tab:sota_results}
\resizebox{\columnwidth}{!}{
\begin{tabular}{ccccccc}
\toprule
\textbf{Method} & \begin{tabular}[c]{@{}c@{}}Inference\\ Vocab. \end{tabular} & A-847 & PC-459 & A-150 & PC-59 & VOC-20 \\ \midrule
SAN \cite{xu2023side} & \checkmark & 12.4 & 15.7 & 27.5 & 53.8 & 94.0 \\
AttrSeg \cite{ma2024open} & \checkmark & -- & -- & -- & 56.3 & 91.6 \\
SCAN \cite{liu2024open} & \checkmark & 14.0 & 16.7 & 30.8 & \textbf{58.4} & \textbf{97.0} \\
EBSeg \cite{shan2024open} & \checkmark & 13.7 & 21.0 & 30.0 & 56.7 & 94.6 \\
SED \cite{xie2024sed} & \checkmark & 11.4 & 18.6 & 31.6 & 57.3 & 94.4 \\
CAT-Seg \cite{cho2024cat} & \checkmark & \textbf{16.0} & \textbf{23.8} & \textbf{31.8} & 57.5 & 94.6 \\ \midrule
CaSED + SAM \cite{conti2024vocabulary} & \xmark & -- & -- & 6.1 & 7.5 & 13.7 \\
CaSED + SAN \cite{conti2024vocabulary} & \xmark & -- & -- & 7.2 & 15.5 & 26.9 \\
DenseCaSED \cite{conti2024vocabulary} & \xmark & -- & -- & 8.6 & 13.4 & 20.5 \\
\textbf{Chick.-and-egg} (CaSED) & \xmark & 3.2 & 4.4 & 9.7 & \textbf{23.1} & \textbf{47.6} \\
\textbf{Chick.-and-egg} (RAM) & \xmark & \textbf{3.7} & \textbf{7.1} & \textbf{15.6} & 23.0 & 47.5  \\
\bottomrule
\end{tabular}
}
\end{table}

\section{Experiments}
\label{ch:results}

We conduct a comprehensive experimental analysis to investigate how different components affect VSS performance.
First, we evaluate the proposed two-stage approach on standard benchmarks to establish the baseline (\Cref{sec:benchmark}). We then present an in-depth analysis of the text encoder's behaviour and its impact on segmentation quality (\Cref{sec:text}). To better understand the relationship between the two-stages, we examine the image tagging accuracy and its influence on the segmentation task (\Cref{sec:tagging}). Finally, we study how different assignment thresholds in the evaluation protocol affect the reported performance (\Cref{sec:thresholds}).

\textbf{Implementation Details:}
The model is trained on the COCO-Stuff dataset \cite{caesar2018coco}, which contains 118k annotated images across 171 categories, following \cite{cho2024cat}. All results are based on CLIP \cite{radford2021learning} with a ViT-B/16 backbone. The image encoder and cost aggregation module are trained with per-pixel binary cross-entropy loss. 
The training parameters follow \cite{cho2024cat}. The batch size is 4, and models are trained for 80k iterations.
We performed image tagging and instance description using a frozen VLM model not trained on the testing dataset. More in detail, we examined the robustness of two models RAM \cite{zhang2024recognize} and Llava-1.6  \cite{liu2024llavanext}.

\textbf{Test Datasets:} The evaluation covers several datasets to ensure comprehensive testing. We used ADE20K \cite{zhou2019semantic} with both 150 and 847 class configurations, Pascal Context \cite{mottaghi2014role} with 59 and 459 class setups, and Pascal VOC \cite{everingham2010pascal} with its 20 classes. 

\begin{figure*}[t]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}ccccc@{}}
        
        
        \includegraphics[width=0.25\textwidth]{fig/qualitative/ADE_val_00000049_img.png} &
        \includegraphics[width=0.25\textwidth]{fig/qualitative_new/ADE_val_00000049_zeroseg.png} &
        \includegraphics[width=0.25\textwidth]{fig/qualitative/ADE_val_00000049_cased_labels_bigger.png} &
        \includegraphics[width=0.25\textwidth]{fig/qualitative/ADE_val_00000049_ours_labels_bigger.png} &
        \includegraphics[width=0.25\textwidth]{fig/qualitative/ADE_val_00000049_labels_bigger.png} \\[0.2cm]
        



        \includegraphics[width=0.25\textwidth] {fig/qualitative_new/ADE_val_00000683_img.png} &
        \includegraphics[width=0.25\textwidth] {fig/qualitative_new/ADE_val_00000683_zero_seg.png} &
        \includegraphics[width=0.25\textwidth]{fig/qualitative_new/ADE_val_00000683_real_image_cased.png} &
        \includegraphics[width=0.25\textwidth]{fig/qualitative_new/ADE_val_00000683_real_image.png} &
        \includegraphics[width=0.25\textwidth]{fig/qualitative_new/ADE_val_00000683_ground_truth.png} \\[0.2cm]


        \includegraphics[width=0.25\textwidth] {fig/qualitative_new/2007_008415_img.png} &
        \includegraphics[width=0.25\textwidth] {fig/qualitative_new/2007_008415_zeroseg.png} &
        \includegraphics[width=0.25\textwidth]{fig/qualitative_new/2007_008415_real_image_cased.png} &
        \includegraphics[width=0.25\textwidth]{fig/qualitative_new/2007_008415_real_image.png} &
        \includegraphics[width=0.25\textwidth]{fig/qualitative_new/2007_008415_ground_truth.png} \\[0.2cm]
        
        
        \textbf{Image} &
        \textbf{ZeroSeg} & 
        \textbf{Chicken-and-egg} (CaSED) & 
        \textbf{Chicken-and-egg} (RAM) & 
        \textbf{GT}
    \end{tabular}
    }
    \caption{Comparison of segmentation results across ZeroSeg \cite{rewatbowornwong2023zero} and \textbf{Chicken-and-Egg} (CaSED \cite{conti2024vocabulary} and RAM \cite{zhang2024recognize}), and ground-truth labels.}
    \label{fig:sota_qualitative_comparison}
\end{figure*}





\begin{table*}[t]
\centering
\caption{Results over the benchmark datasets by using soft assignment. † Results come from their original work. * mapped with Llama-2 \cite{ulger2024autovocabularysemanticsegmentation} rather than Sentence-BERT \cite{reimers2019sentence}. The soft assignment has threshold zero (i.e., all the words are assigned to a class in the evaluation vocabulary).}
\label{tab:mapping_results}
\resizebox{.99\textwidth}{!}{%
    \begin{tabular}{l|cc|cc|cccccccccc}
    \toprule
    \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Vision}\\ \textbf{Backbone}\end{tabular}} & \multirow{2}{*}{\textbf{Stages}} &\multicolumn{2}{c}{\textbf{Components}} & A-847 & PC-459 & A-150 & PC-59 & VOC-20 \\
     & & & Tagging & Segmentation & &&&&& \\ \midrule
    Zero-Seg† \cite{rewatbowornwong2023zero} & ViT-B/16 & Mask2Tag & CLIP+GPT-2 & DINO & -- & -- & -- & 11.2 & 8.1 \\
    Auto-Seg† \cite{ulger2024autovocabularysemanticsegmentation} & ViT-L/16 & Tag2Mask & BLIP-2 & X-Decoder & 5.9* & -- & -- & 11.7* & \textbf{87.1}* \\
    TAG† \cite{kawano2024tag} & ViT-L/14 & Mask2Tag &CLIP+DB & DINO & -- & -- & 6.6 & 20.2 & 56.9 \\
    \textbf{Chicken-and-egg} (CaSED) & ViT-B/16 & Tag2Mask & CLIP+DB & CAT-Seg & 4.3 & 3.1 & 7.8 & \textbf{27.9} & 82.3 \\
    \textbf{Chicken-and-egg} (RAM) & ViT-B/16 & Tag2Mask & CLIP+Swin & CAT-Seg & \textbf{6.7} & \textbf{8.0} & \textbf{18.8} & 27.8 & 81.8 \\
        \bottomrule
    \end{tabular}
}
\end{table*}

\subsection{Benchmark Evaluation}\label{sec:benchmark}
We first conducted a comprehensive benchmark evaluation comparing existing approaches to establish a strong foundation for VSS and identify the most promising direction. This analysis served two key purposes: (1) to understand the current state-of-the-art performance in VSS and (2) to determine which baseline architecture would be the most suitable.

\textbf{Quantitatives:} \Cref{tab:sota_results} compares the mIoU across the Open-Vocabulary benchmarks. The proposed pipeline outperforms the previous VSS methods by a constant margin in all the datasets. To better accommodate VSS methods, they adopt a class remapping strategy that reduces penalization in cases where an exact class match is not found. This approach is reflected in \Cref{tab:mapping_results}, where the soft evaluation assignment takes place as described in \Cref{sec:assignment}.

\textbf{Qualitatives:} As shown in \Cref{fig:sota_qualitative_comparison}, the current approach fills the gap between the predictions and original dataset labels without a predefined vocabulary, offering finer-grained details across diverse scenarios (indoor and outdoor). %
The maps obtained suggest that current evaluation metrics might be overly pessimistic about the qualitative performance of the results. This issue arises from dataset limitations, where many instances struggle to find appropriate matches (e.g., in the third image, "husky" instead of "dog"). Mask2Tag methods like ZeroSeg \cite{rewatbowornwong2023zero} tend to over-segment the instances, getting improper text matches. On the other hand, Chicken-and-egg with CaSED tends to limit the number of predicted tags, %
while coupled with RAM it reaches the best compromise.




\subsection{Segmentatation Analysis} \label{sec:text}
\textbf{Perfect Tagger:} Our empirical results on the OVSS task - presented in \Cref{tab:gt_labels} - revealed that providing only image-specific text labels, %
rather than the entire vocabulary, during training led to improved segmentation performance when applying the same adjustment at inference. Although having access to inference labels is unrealistic, this setup represents the best achievable performance if tagger predictions were 100\% accurate. 
More in detail, in \Cref{tab:gt_labels}, the set of class names is defined for each batch as \(\mathcal{C}_b \subset \mathcal{C}\) during training, where \(\mathcal{C}_b\) represents the batch-specific subset of the entire class vocabulary \(\mathcal{C}\), dynamically selected based on the batch's unique context or requirements. This subset approach allows the model to focus on relevant classes without being overwhelmed by the entire vocabulary. However, we observed no gain when the text labels in inference are predicted from an image tagger. Nevertheless, this represents the upper bound currently obtainable with the state-of-the-art open-vocabulary method \cite{cho2024cat}. Moreover, we show in \Cref{tab:attvsadj} that when performing inference on perfect predictions (100\% accuracy from the tagger) we can boost performance by providing additional textual information.
\input{tab/attributes}
\textbf{Aiding Text Encoder with Descriptions:} Previous works \cite{ma2024open} used adjectives with the assumption to find the common class features that better describe each class. For example, a "dalmata" could be described as "a white dog with black spots". However, in typical recognition tasks, the categories are much broader, such as simply "dog", and a "dog" could be described very differently in terms of color and size. Hence, AttrSeg \cite{ma2024open} have focused on training strategies to find the optimal set of descriptions that could enhance class distinguishability while still being able to represent each class. While this approach has merit, it can result in the loss of fine-grained details. For instance, a "table" or "hat" could be of any size or color, and even a "wall" that is typically "white" could be "bricked" or some other texture.
Zhao et al. \cite{zhao2024gradient} experimented on CLIP's ability to identify different types of object attributes, including shape, material, color, size, and position. 
For shape and material attributes, CLIP showed a certain but limited knowledge, with the heat maps highlighting partial correct attention on obvious objects, but also exhibiting false positive and false negative errors. For color attributes, the results further verified that CLIP has a good ability to distinguish different colors.
For comparative attributes like size and position, CLIP produced some erroneous results, demonstrating that it relies more on the primary object (e.g., "cube", "red") rather than the comparative attribute (e.g., "small", "left"). Overall, their analysis suggests that CLIP has advantages with common perceptual attributes.
Therefore, we adopted a pre-trained VLM to find the corresponding descriptions given each image and its specific set of class names - the text labels of each image-, and we tried to enforce general language descriptions.
The prompts used are shown in \Cref{tab:prompt}. For generating captions, we employed the BLIP-2 model \cite{li2023blip} without any query input, whereas for the multimodal model, we utilized Llava-1.6 \cite{liu2024llavanext}. These models were selected because they both incorporate CLIP as their text encoder.
The text embedding of the captions is employed as a query within an additional cross-attention module, linking it to the embeddings of the classes. In the case of the adjectives, they are sampled and used within the template "A photo of a \{adjective\} \{class name\}".
We report the results in table \Cref{tab:attvsadj}, where adding image-specific content results beneficial, specially for large numbers of classes.
It is important to notice that, when using predicted labels from the image tagger or applying the complete set of image labels during inference, we did not observe the same benefit. %
In the VSS scenario, ambiguities with other classes are largely resolved during the CLIP segmentation stage by directly predicting the image's content using the image tagger. However, misclassifications may still occur at this stage, a behaviour explored in the next paragraph.
\begin{table}[t]
\centering
\caption{Class recognition accuracy of different VLMs with $T_\text{SBERT}$=$0.0$. \\ * using vocabulary.
\# FN = average number of missed classes, \# FP = average number of classes predicted but not in the ground truth.}
\label{tab:accuracy}
\resizebox{.5\textwidth}{!}{%
\begin{tabular}{ccccccccccc}
\toprule
\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Predicted\\ Classes\end{tabular}}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Mapping\\ Model\end{tabular}}} & \multicolumn{3}{c}{A-150} & \multicolumn{3}{c}{PC-59} & \multicolumn{3}{c}{VOC-20} \\ \cmidrule{3-11} 
 &  & Acc & \#FP & \#FN & Acc & \#FP & \#FN & Acc & \#FP & \#FN \\ \midrule
CaSED & - & 10 & 10.7 & 7.8 & 22 & 9.3 & 4.0 & 50 & 9.5 & 0.9 \\
CaSED & SBERT & 23 & 7.4 & 6.8 & 42 & 5.4 & 3.1 & 84 & 4.2 & 0.3 \\
Llava-1.6 & - & 26 & 4.9 & 6.3 & 29 & 3.7 & 3.5 & 53 & \textbf{3.5} & 0.8 \\
Llava-1.6 & SBERT & 39 & \textbf{2.6} & 5.2 & 47 & \textbf{1.8} & 2.7 & 91 & 1.9 & 0.2 \\
RAM & - & 34 & 10.4 & 5.9 & 41 & 11.8 & 3.1 & 68 & 12.2 & 0.5 \\
RAM & SBERT & \textbf{46} & 5.7 & \textbf{4.8} & \textbf{61} & 5.4 & \textbf{2.2} & \textbf{96} & 4.8 & \textbf{0.1} \\
\midrule
RAM* & - & 79 & 16.7 & 1.95 & 80 & 6.2 & 1.1 & 97 & 1.5 & 0.1  \\
\bottomrule
\end{tabular}
}
\end{table}

\subsection{Image Tagging Analysis}\label{sec:tagging}
In \Cref{tab:accuracy}, we investigated various image tagging methods to understand how different types of errors affect the sensitivity of the segmentation module, particularly the text encoder since we use the tags as input to CLIP. We evaluated the impact of three architectures: a training-free method, CaSED \cite{conti2024vocabulary}, a multi-step trained method, RAM \cite{zhang2024recognize}, and a general-purpose multimodal model, Llava \cite{liu2024llavanext}.
CaSED %
uses a pre-trained vision-language model and an external database to extract candidate categories and assign the image to the best match. 
On the other hand, RAM %
generates large-scale image tags through automatic semantic parsing, followed by training a model to annotate images using both captioning and tagging tasks. A data engine then refines these annotations, and the model is retrained on this enhanced data, with final fine-tuning on a higher-quality subset.
\Cref{tab:accuracy} shows that using SBERT for evaluation avoids discarding words merely due to the absence of an exact match with the chosen word by the annotators. RAM achieves the best overall results across the evaluated datasets. In the table, the performance of Llava \cite{liu2024llavanext} %
demonstrates the versatility of powerful vision-language architectures. Note that the current baseline, RAM, does not reach a perfect accuracy even when the whole list of desired classes (i.e., non-vocabulary free), hence this represents the current limitation of such an approach. 
Furthermore, compared to CaSED, RAM demonstrates higher class recognition accuracy, but with more false positives on average. To investigate this further, we examined in \Cref{fig:miss_vs_false_sim} how the model is influenced by simulating a drop rate and false positives on top of the ground truth text classes in each image. In the table, the false positives are randomly selected from the vocabulary set. The influence of false negatives deeply influences the performance, while introducing false positives only leads to marginal degradation. These results confirm why RAM outperforms current alternatives: it has the fewest misclassifications, despite having a higher rate of false positives.

\begin{figure}
    \centering
    \includegraphics[width=.9\columnwidth, trim=0cm 0.55cm 0cm 0.75cm, clip]{fig/fpfn_full_stefano.png}
    \caption{Simulating missing classes or adding wrong ones over the OVSS baseline by assuming the labels are known at inference time.}
    \label{fig:miss_vs_false_sim}
\end{figure}


\subsection{Evaluation Assignment Thresholds} \label{sec:thresholds}
In \Cref{fig:thresh} we show the effect of providing different values for $T_\text{SBERT}$. Unlike Zero-Seg \cite{rewatbowornwong2023zero}, we did not observe a consistent trend in the optimal threshold across datasets. Respectively, $0.6-0.7$ for A-847, PC-459 and A-150, $0.5$ for PC-59 and $0.1$ for VOC-20. 
Our findings suggest that as the number of classes increases, we need to be more confident in the assignment, hence a higher threshold leads to a better score.

\begin{figure}
    \centering
    \includegraphics[width=.9\columnwidth, trim=0cm 0.55cm 0cm 0.75cm, clip]{fig/thresholds_stefano.png}
    \caption{Ablation over different thresholds for the evaluation mapping.%
    }
    \label{fig:thresh}
\end{figure}



