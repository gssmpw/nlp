\begin{table*}[t]
    \centering
    \resizebox{0.9\textwidth}{!}{%
        \begin{tabular}{c l cccc cccc cccc}
        \toprule
          & & \multicolumn{4}{c}{\textit{Husky vs. Wolf}} & \multicolumn{4}{c}{\textit{Leaves}} & \multicolumn{4}{c}{\textit{``Kit Fox'' vs ``Red Fox''}} \\
         \midrule
       & Session~n$^{\circ}$               & 1 & 2 & 3 & \metric & 1 & 2 & 3 & \metric & 1 & 2 & 3 & \metric \\
         \midrule
        & Baseline                    & 55.7 & 66.2 & 62.9 &  &       70.1 & 76.8 & 78.6 &  &       58.8 & 62.2 & 58.8 &  \\
        & Control                             & 53.3 & 61.0 & 61.4 & 0.95 &       72.0 & 78.0 & 80.2 & 1.02 &     60.7 & 59.2 & 48.5 & 0.94 \\
        \midrule
        \multirow{6}{*}{\rotatebox[origin=c]{90}{{\footnotesize Attributions}}}
        & Saliency                  & 53.9 & 69.6 & 73.3 & 1.06  &        83.2 & 88.7 & 82.4 & \underline{1.13} &      61.7 & 60.2 & 58.2 & 1.00 \\ 
        & Integ.-Grad.     & 67.4 & 72.8 & 73.2 & 1.15 &      82.5 & 82.5 & 85.3 & 1.11 &       59.4 & 58.3 & 58.3 & 0.98\\
        & SmoothGrad           & 68.7 & 75.3 & 78.0 & 1.20 &    83.0 & 85.7 & 86.3 & \underline{1.13} &       50.3 & 55.0 & 61.4 & 0.93 \\
        & GradCAM               & 77.6 & 85.7 & 84.1 & 1.34 &       81.9 & 83.5 & 82.4 & 1.10 &      54.4 & 52.5 & 54.1 & 0.90 \\
        & Occlusion            & 71.0 & 75.7 & 78.1 & 1.22 &       78.8 & 86.1 & 82.9 & 1.10 &     51.0 & 60.2 & 55.1 & 0.92 \\
        & Grad.-Input               & 65.8 & 63.3 & 67.9 & 1.06 &      76.5 & 82.9 & 79.5 & 1.05 &      50.0 & 57.6 & 62.6 & 0.95 \\

        \midrule
        
        \multirow{3}{*}{\rotatebox[origin=c]{90}{{\footnotesize Concepts}}}
        
        & ACE & 68.8 & 71.4 & 72.7 & 1.15 &   79.8 & 73.8 & 82.1 & 1.05 &    48.4 & 46.5 & 46.1 & 0.78 \\

        & CRAFTCO (ours) & 82.4 & 87.0 & 85.1 & \underline{1.38} &  78.8 & 85.5 & 89.4 & 1.12 &    55.5 & 49.5 & 53.3 & 0.88 \\
        & CRAFT (ours) & 90.6 & 97.3 & 95.5 & \textbf{1.53} &   86.2 & 86.6 & 85.5 & \textbf{1.15} &    56.5 & 50.6 & 49.4 & 0.87 \\
        \bottomrule
        \end{tabular}%
    }
    \caption{\textbf{\metric~scores on 3 datasets from 
    \cite{fel2021cannot}} (presented in \autoref{sec:meta_pred}). \metric~benchmark evaluates how well explanations help users identify general rules driving classifications that readily transfer to unseen instances. At training time, users are asked to infer rules driving the decisions of the model given a set of images, and their associated predictions and explanations. At test time, the \metric~metric measures the accuracy of users at predicting the model decision on novel images averaged over 3 sessions, and normalized by the baseline accuracy of users trained without explanations.
    The higher the \metric~score, the more useful the explanation, and the more crucial the information provided is for understanding --and thus predicting the model's output-- on novel samples. %
    CRAFTCO stands for ``CRAFT Concept Only'' and designates an experimental condition where only global concepts are given to users, without local explanations (i.e., the concept attribution maps).
    The first and second best results above the baseline are in \textbf{bold} and \underline{underlined}, respectively. 
    }
    \label{tab:utility}
\end{table*}
