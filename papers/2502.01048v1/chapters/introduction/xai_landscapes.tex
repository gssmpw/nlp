\section{Explainability Landscape}
\label{sec:intro:xai_landscape}


The objective of this section is twofold: firstly, to underscore the imperative of explainability within machine learning, and secondly, to delineate a concise overview of the diverse methodologies underpinning explainability -- to say it otherwise, to ``flag'' the existing sub-fields. We aim to acquaint the reader with pivotal terms and explainability methods discussed throughout this manuscript. To do so, we propose a taxonomy categorizing explainability methods into three dimensions: methods that explain individual predictions, those studying the model internal mechanics, and those interpreting the data's influence. This classification, albeit simplistic, facilitates a structured introduction to the landscape of explainability.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/introduction/black_box_schema.png}
    \caption{\textbf{Illustration of the Black-Box Problem.} Neural networks undergo training on a \textit{Training Dataset} through a specific \textit{Learning Algorithm}. After training, the model performs inferences using the learned parameters to make \textit{Predictions}. However, the multitude of operations from \textit{Input} to prediction is excessively complex for human comprehension, thus the name Black-box.}
    \label{fig:intro:blackbox}
\end{figure}

\subsection{Motivation}
\label{sec:intro:motivation}

Tracing the origins of explainability in AI is akin to exploring the very essence of science, as the pursuit of explanations, particularly within the realm of scientific thought, has historically been a foundational pillar, as highlighted by~\cite{hospers1946explanation}, suggesting that the impetus for explanation is deeply rooted in the fabric of scientific discourse itself. 
A closer intellectual lineage to modern explainability could be found back over half a century, finding ground in the domain of mathematical logic~\cite{hempel1948studies}.
However, it was not until the advent of deep learning, that the modern conceptualization of explainability -- as it is addressed within this manuscript -- emerged. Unsurprisingly, it is deep learning that has been the catalyst for the establishment of this burgeoning research field, and it is important to delineate the goal of XAI as well as its expected outcomes prior to examining the existing body of work.

It would typically be prudent to start with a definition; however, the quest for a formal definition of explainability is unlikely to be straightforward. \cite{lombrozo2006structure} notes that explanations serve as the currency of our belief systems, a medium through which we exchange and interrogate our understanding of the world. This discourse raises fundamental questions about the nature of explanations and the criteria that distinguish more effective explanations from their less compelling counterparts. The academic community has variously characterized explanations as embodying a \textit{deductive-nomological} essence \cite{hempel1948studies}, akin to logical proofs, or as mechanisms that provide a deeper understanding of underlying processes, as proposed by \cite{bechtel2005explanation}. \cite{keil2006explanation} proposed a broader conceptualization, advocating for an understanding of explanations as embodying an implicit explanatory comprehension\footnote{Interestingly, one could interpret the essence of this article from an informational perspective on explainability as an addition of information \textit{given} a common body of knowledge.}.

Given the rich literature on this subject, attempting to distill a singular definition that encompasses the entire spectrum of use-cases and motivations within the field would be a \textit{Sisyphean} task and would take us too far. Therefore, we propose to adopts a pragmatic approach to defining explainable AI (XAI), not through abstract or absolute terms but by aligning with the specific objectives it seeks to achieve. This approach will thus have to settle for a localized and use-case specific definition of explainability, allowing us to focus on the technical aspects of the domain. Among the myriad objectives identified in the literature~\cite{jacovi2021formalizing,miller2017explanation,survey2019metrics,saeed2023explainable,weber2023beyond,antoniadi2021current,das2020opportunities}
, six primary goals could be noted, as central to the discourse on XAI:

\begin{itemize}
    \item \textbf{Building trust in model predictions.} For example, in healthcare, AI-assisted diagnostics can use explainability to highlight influential areas in medical images, helping clinicians trust and verify AI diagnoses by visually indicating regions of interest.
    
    \item \textbf{Elucidating important aspects of learned models.} The SNCF for example, could need explainability in autonomous railway systems to help engineers understand the decision-making process behind navigational actions, ensuring the AI correctly recognizes stop signs and detect obstacles.
    
    \item \textbf{Assisting in meeting regulatory requirements and facilitating the certification process.} Financial services leveraging AI for credit scoring can use explainability to detail how individual features influence credit scores, aiding in compliance with regulations like GDPR.
    
    \item \textbf{Uncovering and addressing biases or unintended effects learned by models.} Explainability can reveal if an AI recruitment tool unfairly weighs certain demographics, allowing developers to correct these biases.
    
    \item \textbf{Detecting and preempting potential failure cases.} In predictive maintenance for manufacturing, explainability reveals conditions leading to equipment failures, enabling preemptive actions to prevent or mitigate effects.
    
    \item \textbf{Debugging models to enhance training methodologies:} Still in the context of the SNCF's autonomous railway systems, explainability can help engineers and developers understand why a model might misinterpret sensor data or fail to correctly predict maintenance needs. By analyzing instances where the model's performance deviates from expectations, the teams can refine data inputs, adjust model parameters, and ultimately improve the reliability and safety of autonomous railway operations.
    
\end{itemize}

These objectives highlight the heterogeneity of aims within the field and underscore the magnitude of the challenges that confront us. Having established that XAI presents a real \textbf{conceptual challenge}, we will now see that it is also a real \textbf{technical challenge}.


\subsection{Explaining Predictions}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.\textwidth]{assets/introduction/attributions.jpeg}
    \caption{\textbf{Attribution Methods.} Attribution methods will be the subject of the \autoref{chap:attributions}. These methods aim to explain a specific prediction through heatmaps, where hotter areas indicate a greater significance of the pixel for the decision.}
    \label{fig:intro:attributions}
\end{figure}

The development of methods to explain model predictions has been a critical aspect of research, originating with the introduction of attribution methods~\cite{Zeiler2011}. These approaches aim to clarify the rationale behind a model's decision, whether it is the classification of an instance, the detection of an object within an image, or the prediction of a value in regression tasks. \textit{Attribution methods}, which produce a heatmap to represent the importance of each input variable (see \autoref{fig:intro:attributions}), are among the most widely used due to their straightforward implementation in automatic differentiation frameworks such as TensorFlow~\cite{tensorflow2015} and PyTorch~\cite{paszke2019pytorch}.

A broad range of attribution techniques exists, using gradients~\cite{zeiler2013visualizing,shrikumar2017learning,sundararajan2017axiomatic,smilkov2017smoothgrad}, perturbations~\cite{ancona2017better,petsiuk2018rise,Fong_2017,fel2021sobol,novello2022making}, or internal model activations~\cite{Selvaraju_2019,chattopadhay2018grad} to generate explanations. A general definition is given below:

\begin{definition}[Attribution Method.] 
\label{def:attributions}
For a model $\f : \sx \to \sy$ and an input $\vx \in \sx$, an attribution method is a functional:

\[
\explainer : \fspace \times \sx \to \Real^{|\sx|}
\]

where $\explanation = \explainer(\f, \vx)$ (with $\f \in \fspace$) represents an attribution map that explains the prediction of $\f$ for input $\vx$. The higher the scalar value in $\explanation$, the more important the variable is considered.
\end{definition}

Despite their utility, attribution methods face challenges related to reliability~\cite{adebayo2018sanity,sixt2020explanations,ghorbani2017interpretation,slack2021counterfactual,sturmfels2020visualizing,hsieh2020evaluations,hase2021out}, computational efficiency~\cite{novello2022making}, and the implicit assumptions about importance~\cite{eva2}. A dedicated chapter (\autoref{chap:attributions}) further explores these methods, addressing their complexities and constraints.

\subsection{Explaining the Model}

Explaining a model involves uncovering the internal mechanics that drive its predictions. This can be approached through various methodologies, each aiming to make the model's operations or internal states more transparent.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.\textwidth]{assets/introduction/cav.jpeg}
    \caption{\textbf{Concept Activation Vector (CAV).} An example of extracting the "striped" concept using images featuring this concept and random images. A classifier in the intermediate space is utilized to identify the CAV as the vector orthogonal to the decision boundary. Methods for analyzing concepts will be discussed in \autoref{chap:concepts}.}
    \label{fig:intro:concepts}
\end{figure}

\paragraph{Concept-based Explainability.} Recent developments in explainability have underscored the need to go beyond attribution methods~\cite{doshivelez2017rigorous}. A flagship of these methods is \textit{concept-based} explainability~\cite{kim2018interpretability}, which involves identifying human-understandable concepts within a model. Briefly, this method compares two datasets, one containing the concept of interest and a 'random' dataset used for one-class detection with a linear model. The orthogonal to the decision boundary is called a concept vector, see \autoref{fig:intro:concepts}. Further methods have been proposed to not just retrieve human-defined concepts, but to study concepts utilized by the model itself~\cite{ghorbani2019towards,fel2023craft,lrpconcepts,graziani2023concept,zhang2021invertible,fel2023holistic}. Unlike attribution methods that provide a heatmap of input importance, concept-based explainability seeks to discover "what" triggers a feature. A general approach to defining a concept within a model's operational framework is as follows:

\begin{definition}[Concept Vector.]
\label{def:intro:cav}
Given a Fully Connected Feedforward Neural Network (FCNN) $\f : \sx \to \actspace \subseteq \Real^d$, a concept vector $\v{v} \in \Real^d$ is identified as a vector representing a concept in the activation space $\actspace$ of the FCNN. Depending on the context, the alignment or dot product between an activation $\v{a} \in \actspace$ and the concept vector $\v{v}$ indicates the extent to which $\v{a}$ it embodies the concept.
\end{definition}

An entire chapter is dedicated to these methods, offering a more nuanced understanding of what the model has learned.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.\textwidth]{assets/introduction/feature_viz.jpeg}
    \caption{\textbf{Illustration of Feature Visualization (FV).} An example of visualization for neurons (ladybug and goldfish), channels of a convolutional network as well as for CAV using FV. Feature Visualizations will be discussed in \autoref{chap:concepts}.}
    \label{fig:intro:fviz}
\end{figure}

\paragraph{Feature Visualization.}
Feature visualization~\cite{olah2017feature} aims to generate images that maximally activate specific parts of the network, providing insights into the kinds of features to which the network is sensitive and identifying what each component of the network is looking for in its inputs.

\begin{definition}[Feature Visualization]
\label{def:intro:feature_viz}
Given a FCNN $\f : \sx \to \Real^d$ and a target structure (neurons, channels, concept) $\v{v} \in \Real^d$, the feature visualization $\vx^\star$ associated with $\v{v}$ is defined as:
\[
\vx^\star = \argmax_{\vx \in \sx} \langle \f(\vx), \v{v} \rangle - \Omega(\vx),
\]
where $\Omega(\vx)$ typically applies a penalty to ensure the resulting image remains within a natural image manifold, and $\v{v}$ can be a one-hot vector targeting a specific neuron or a more complex structure representing a concept.
\end{definition}

Feature visualization techniques have been instrumental in uncovering fascinating phenomena and features in convolutional models~\cite{nguyen2016multifaceted,nguyen2019understanding} and especially~\cite{cammarata2020thread}. \autoref{chap:concepts} has a section dedicated to this subject and proposes improvements to existing feature visualization methods.


\paragraph{Interpretability by Design.} Creating models with interpretability as a foundational goal entails architecting models to output not only predictions but also an explanation understandable to humans. 

Recently, a promising methodology~\cite{bohle2022b,bohle2023holistically} has been proposed, it involves training models that dynamically adjust their internal parameters in response to input data, reminiscent of synaptic plasticity~\cite{abraham1996metaplasticity}. Specifically, for a given input point $\vx$, these models generate a unique set of linear weights that directly map $\vx$ to its prediction $y = \f(\vx) \vx^\tr$ with $\f : \sx \to \Real^{|\sx|}$. This innovative approach promises not only enhanced interpretability, but also a direct mechanism for solving (at least locally) the problem of prediction specific explanations.

Historically, other methodologies have aimed at achieving interpretability by adhering to specific desiderata during model construction. For instance, the seminal work by~\cite{alvarezmelis2018robust} focused on developing models that are both robust and interpretable by design, ensuring that the model's behavior remains consistent and faithful to the data it was trained on, thereby enhancing trustworthiness and reliability.

Furthermore, the concept of prototypical networks, as discussed by~\cite{rudin2019stop}, introduces a framework where predictions are based on the similarity of input features to prototype examples. This methodology not only simplifies the interpretability of predictions by anchoring them to recognizable instances but also facilitates a more intuitive understanding by comparing new inputs to known, labeled examples.

In summary, focusing on making models interpretable has the literature to propose promising methods. This approach is a serious candidate to make deep neural networks work in a manner that humans can understand. We will  dive deeper into these methods in the chapter dedicated to Alignment (\autoref{chap:alignment}).

\subsection{Explaining through Data}

Understanding model behavior extends to examining the influence of training data on the model's learning and predictions. Influence functions~\cite{cook1980characterizations} are a key tool in this domain, enabling the estimation of how the model's parameters or predictions would change if a particular data point were \textit{removed}\footnote{The actual formulation is expressing the difference in the parameter space for an infinitesimal perturbation.} from the training set.

\begin{definition}[Influence Function]
Given a learning algorithm $\mathcal{A} : \sx^n \times \Real^n \to \bm{\Theta}$, where $\parameters \in \bm{\Theta}$ represents the parameter space of a predictor $\f(\cdot; \parameters)$. A dataset $\s{D} = \{(\vx_1, y_1), \ldots, (\vx_n, y_n)\} \subseteq \mathcal{X}^n$, and a vector of weights $\v{w} = (w_1, \ldots, w_n)$ for the data points in $\mathcal{D}$, the influence function approximates the effect on the parameter vector $\parameters$ when the weight of the data point indexed by $i$ is infinitesimally perturbed by an amount $\xi$. This is mathematically expressed as:
\[
\mathcal{I}(i ; \mathcal{A}, \s{D}, \v{w}) \defas \lim_{\xi \to 0} \frac{1}{\xi} \left( \mathcal{A}(\s{D}, \v{w} + \xi \mathbf{e}_i) - \mathcal{A}(\s{D}, \v{w}) \right),
\]
where $\mathbf{e}_i$ is the canonical vector with respect to the $i$-th data point's weight.
\end{definition}

Influence functions trace their roots to robust statistics, offering a lens to examine the sensitivity of parameter estimates to changes in the underlying data distribution. This concept has been instrumental in identifying leverage points and outliers in data analysis, where the influence of such points on statistical estimations can lead to biased or misleading conclusions.

Recent works~\cite{koh2017understanding} have significantly extended and refined the application of influence functions, offering more possibility to shape better insights into the data's role in shaping model behavior. 
