\section{Outline \& Contributions}
\label{sec:intro:contrib}

This doctoral dissertation is organized to further the field of explainability for computer vision. It delves into a variety of specific methodologies across several chapters, outlined as follows:

\paragraph{\autoref{chap:intro}: Introduction.} This initial section provides a concise introduction to deep learning and explainability, establishing the foundational knowledge necessary for the remainder of this document.

\paragraph{\autoref{chap:attributions}: Attribution Methods.} The second chapter is dedicated to attribution methods. It begins by illustrating the feasibility of identifying models that offer superior explanations through the lens of algorithmic stability of its attributions maps. Subsequently, a state-of-the-art black-box attribution method based on \textit{Sobol} indices and Quasi-Monte Carlo sampling is introduced, which reduces computational costs by a factor of two compared to its predecessors. The discussion progresses to the development of an attribution method, \eva, that offers strong formal guarantees using perturbation verification analysis. The practical applicability of these methods, particularly in real-world scenarios and from a human perspective, is subsequently evaluated. It is found that while attribution methods prove to be sufficient and highly useful for straightforward scenarios, their utility vanish when faced with more intricate situations. The chapter concludes by proposing two hypotheses to address these limitations: (1) the need for models that better align with human reasoning, and (2) the necessity to go beyond current attribution methods. These hypotheses are then investigated in \autoref{chap:alignment} and \autoref{chap:concepts}, respectively.

\paragraph{\autoref{chap:alignment}: Model Alignment.} This chapter explores how explainable AI (XAI) can be employed to align current vision models with human cognition through novel training routines. It highlights a trend towards decreasing alignment between models and human understanding and demonstrates how the proposed routine can counter this trend, even improving accuracy. The chapter concludes by noting an intriguing link between model robustness and explainability, exemplified by 1-Lipschitz models.

\paragraph{\autoref{chap:concepts}: Concept based Explainability.} Building on the insights from \autoref{chap:attributions}, this section argues for a shift from explaining solely on \where~a model looks to understanding \what~it sees (\what features the model recognizes at its focal points). A novel method, \craft, is introduced for automatically extracting and evaluating the importance of concepts within trained networks, offering both global and local (heatmap-based) explanations. This approach significantly addresses the issues raised in \autoref{chap:attributions} and opens up new avenues for explainability. The discussion extends to competing methods, proposing a unified framework for automatic concept extraction under the paradigm of \textit{dictionary learning}. It is shown that existing concept importance methods are essentially specific cases of attribution methods applied to concepts. The chapter concludes with \maco~a novel method of feature visualization that scale to deep network for better representation of extracted concepts.

\paragraph{\autoref{chap:conclusion}: Concluding Remarks and Future Directions.} The dissertation concludes with a comprehensive review of the developed methods and tools, reflections on the journey, and thoughts on the future of explainability in AI.

\section{Related publications}

This dissertation integrates and builds upon a series of peer-reviewed publications, open-source projects, and contributions to the wider research community. Below, we categorize these works based on their relevance to the core chapters of this thesis and additional contributions that extend beyond the thesis scope\footnote{The symbol \equal denote equal contributions.}.

\subsection{Foundational Contributions}

This section outlines the peer-reviewed publications that form the backbone of the thesis, organized by the relevant chapters.

\paragraph{Attributions (\autoref{chap:attributions})}

\begin{itemize}
    \item \textbf{Thomas Fel}, David Vigouroux, Remi Cadene, Thomas Serre (2022). \textit{``How Good is your Explanation? Algorithmic Stability Measures to Assess the Quality of Explanations for Deep Neural Networks''.} In: \textit{Proceedings of the Winter Conference on Computer Vision} (\textcolor{confcolor}{WACV})

    \item \textbf{Thomas Fel}\equal, Remi Cadene\equal, Mathieu Chalvidal, Matthieu Cord, David Vigouroux, Thomas Serre, (2021). \textit{``Look at the Variance! Efficient Black-box Explanations with Sobol-based Sensitivity Analysis''.} In: \textit{Advances in Neural Information Processing Systems}  (\textcolor{confcolor}{NeurIPS})

    \item \textbf{Thomas Fel}\equal, Melanie Ducoffe\equal, David Vigouroux\equal, Remi Cadene, Mikael Capelle, Claire Nicodeme, Thomas Serre, (2023). \textit{``Don't Lie to Me! Robust and Efficient Explainability with Verified Perturbation Analysis''.} In: \textit{IEEE/CVF Conference on Computer Vision and Pattern Recognition}  (\textcolor{confcolor}{CVPR})

    \item Julien Colin\equal, \textbf{Thomas Fel}\equal, Remi Cadène, Thomas Serre, (2021). \textit{``What I Cannot Predict, I Do Not Understand: A Human-Centered Evaluation Framework for Explainability Methods''.} In: \textit{Advances in Neural Information Processing Systems} (\textcolor{confcolor}{NeurIPS})

\end{itemize}

\paragraph{Model Alignment (\autoref{chap:alignment})}

\begin{itemize}

    \item \textbf{Thomas Fel}\equal, Ivan F Rodriguez\equal, Drew Linsley\equal, Thomas Serre, (2022). \textit{``Harmonizing the object recognition strategies of deep neural networks with humans''.} In: \textit{Advances in Neural Information Processing Systems}  (\textcolor{confcolor}{NeurIPS})

    \item Mathieu Serrurier, Franck Mamalet, \textbf{Thomas Fel}, Louis Béthune, Thibaut Boissin, (2023). \textit{``On the explainable properties of 1-Lipschitz Neural Networks: An Optimal Transport Perspective''.} In: \textit{Advances in Neural Information Processing Systems} (\textcolor{confcolor}{NeurIPS})

\end{itemize}


\paragraph{Concept-based Explainability (\autoref{chap:concepts})}

\begin{itemize}
    \item \textbf{Thomas Fel}\equal, Agustin Picard\equal, Louis Bethune\equal, Thibaut Boissin\equal, David Vigouroux, Julien Colin, Rémi Cadène, Thomas Serre, (2023). \textit{``CRAFT: Concept Recursive Activation FacTorization for Explainability''.} In: \textit{IEEE/CVF Conference on Computer Vision and Pattern Recognition} (\textcolor{confcolor}{CVPR})

    \item \textbf{Thomas Fel}\equal, Victor Boutin\equal, Mazda Moayeri, Rémi Cadène, Louis Bethune, Mathieu Chalvidal, Thomas Serre, et al., (2023). \textit{``A Holistic Approach to Unifying Automatic Concept Extraction and Concept Importance Estimation''.} In: \textit{Advances in Neural Information Processing Systems}  (\textcolor{confcolor}{NeurIPS})

    \item \textbf{Thomas Fel}\equal, Thibaut Boissin\equal, Victor Boutin\equal, Agustin Picard\equal, Paul Novello\equal, Julien Colin, Drew Linsley, Tom Rousseau, Rémi Cadène, Lore Goetschalckx, et al., (2024). \textit{``Unlocking feature visualization for deep network with MAgnitude constrained optimization''.} In: \textit{Advances in Neural Information Processing Systems}  (\textcolor{confcolor}{NeurIPS})
\end{itemize}

\subsection{Open Source Contributions}
\label{sec:open-source-contributions}

Throughout my PhD, I have actively contributed to the open-source community, leading to the development and maintenance of several projects, notably \xplique~which now count 500+ stars on GitHub and implement more than 50 articles in Explainability and lead to the following publication:

\begin{itemize}
    \item \textbf{Thomas Fel}\equal, Lucas Hervier\equal, David Vigouroux, Antonin Poche, Justin Plakoo, Remi Cadene, Mathieu Chalvidal, Julien Colin, Thibaut Boissin, Louis Bethune, Agustin Picard, Claire Nicodeme, Laurent Gardes, Gregory Flandin, Thomas Serre, (2022). \textit{``Xplique: A Deep Learning Explainability Toolbox''.} In: \textit{Workshop on Explainable Artificial Intelligence for Computer Vision} (\textcolor{confcolor}{CVPR} W.)
\end{itemize}


in total, I open sourced and mainteaned 5 projects, all of them available on GitHub:
\begin{itemize}

    \item \textbf{Xplique:}, an open source Explainability toolbox implementing more than 50 papers of explainability, with a proper documentation, tutorials and notebooks. Available at \url{https://github.com/deel-ai/xplique} or available through \lstinline[language=bash]{pip install xplique}.

    \item \textbf{CRAFT:} an open source repo to reproduce our work on Concept based explainability (see \autoref{chap:concepts}), in Tensorflow and Pytorch, with tutoriels. Available at \url{https://github.com/deel-ai/Craft} or available through \lstinline[language=bash]{pip install craft-xai}.

    \item \textbf{Harmonization:} an open source zoo of harmonized models (see \autoref{chap:alignment}) trained as well as notebook and tutorial to evaluate other models, in Tensorflow and Pytorch. Available at \url{https://github.com/serre-lab/Harmonization} or available through \lstinline[language=bash]{pip install harmonization}.

    \item \textbf{Sobol: } an open source version of Sobol attribution method (see \autoref{chap:attributions}), in Tensorflow and Pytorch. Available at \url{ https://github.com/fel-thomas/Sobol-Attribution-Method}.

    \item \textbf{Numkdoc:} an open source parser of MkDocs for numpy style documentation, now use to build the documentation of \xplique~and other library. Available at \url{ https://github.com/fel-thomas/numkdoc}.

\end{itemize}

Lastly, I have open-sourced a public interactive demo titled \Lens, which encapsulates 3 of the research contributions presented in the final chapter. This demonstration leverages the findings discussed in the last chapter of this manuscript, showcasing the interconnections and collaborative potential among the different studies explored throughout my thesis \url{https://serre-lab.github.io/Lens/}.


\subsection{Extended Contributions}

In addition to my direct thesis work, I have also contributed to several projects that, while not the main focus of this dissertation, address related challenges in the field.

\begin{itemize}
    \item \textbf{Thomas Fel}\equal, Louis Bethune\equal, Andrew Kyle Lampinen, Thomas Serre, Katherine Hermann, (2024). \textit{``Understanding Visual Feature Reliance through the Lens of Complexity''.} In: \textit{Advances in Neural Information Processing Systems} (\textcolor{confcolor}{NeurIPS})

    \item Katherine L. Hermann, Hossein Mobahi, \textbf{Thomas Fel}, Michael C. Mozer, (2024). \textit{``On the Foundations of Shortcut Learning''.} In: \textit{Proceedings of the International Conference on Learning Representations} (\textcolor{confcolor}{ICLR})


    \item Paul Novello, \textbf{Thomas Fel}, David Vigouroux, (2022). \textit{``Making Sense of Dependence: Efficient Black-box Explanations Using Dependence Measure''.} In: \textit{Advances in Neural Information Processing Systems} (\textcolor{confcolor}{NeurIPS})

    \item Victor Boutin, \textbf{Thomas Fel}, Lakshya Singhal, Rishav Mukherji, Akash Nagaraj, Julien Colin, Thomas Serre, (2023). \textit{``Diffusion Models as Artists: Are we Closing the Gap between Humans and Machines?''.} In: \textit{Proceedings of the International Conference on Machine Learning} (\textcolor{confcolor}{ICML})

    \item Drew Linsley, Ivan F Rodriguez, \textbf{Thomas Fel}, Michael Arcaro, Saloni Sharma, Margaret Livingstone, Thomas Serre, (2023). \textit{``Performance-optimized deep neural networks are evolving into worse models of inferotemporal visual cortex''.} In: \textit{Advances in Neural Information Processing Systems} (\textcolor{confcolor}{NeurIPS}).

    \item Sabine Muzellec, \textbf{Thomas Fel}, Victor Boutin, Leo Andeol, Rufin VanRullen, Thomas Serre, (2024). \textit{``Saliency strikes back: How filtering out high frequencies improves white-box explanations''.} In: \textit{Proceedings of the International Conference on Machine Learning} (\textcolor{confcolor}{ICML})

    \item Agustin Martin Picard, Lucas Hervier, \textbf{Thomas Fel}, David Vigouroux, (2023). \textit{``Influenci{\ae}: A library for tracing the influence back to the data-points''.} In: \textit{Proceedings of World Conference on eXplainable Artificial Intelligence} (\textcolor{confcolor}{xAI}).

    \item Fanny Jourdan, Agustin Picard, \textbf{Thomas Fel}, Laurent Risser, Jean Michel Loubes, Nicholas Asher, (2023). \textit{``COCKATIEL: COntinuous Concept ranKed ATtribution with Interpretable ELements for explaining neural net classifiers on NLP tasks''.} In: \textit{Proceedings of the Association for Computational Linguistics} (\textcolor{confcolor}{ACL}).

    \item Léo Andéol, \textbf{Thomas Fel}, Florence De Grancey, Luca Mossina, (2023). \textit{``Confident Object Detection via Conformal Prediction and Conformal Risk Control: an Application to Railway Signaling''.} In: \textit{Symposium on Conformal and Probabilistic Prediction with Applications} (\textcolor{confcolor}{COPA}).

    \item Christopher J Hamblin, \textbf{Thomas Fel}, Srijani Saha, Talia Konkle, George A Alvarez, (2023). \textit{``Feature Accentuation: Explaining 'what' features respond to in natural images''}. \textit{Preprint}.

\end{itemize}
