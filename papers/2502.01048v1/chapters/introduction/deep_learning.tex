\newcommand{\risk}{\s{R}}
\newcommand{\emprisk}{{\risk_{\text{emp}}}}

\section{Deep Learning Background}
\label{sec:intro:deep_learning}

In this section, we will briefly revisit the framework of this study, namely deep neural networks. To do so, we will briefly review the statistical learning framework in which we operate, and then we will explore the different components of neural network architectures for vision tasks. For a more comprehensive background, we encourage the reader to refer to \cite{goodfellow2016deep}.

\subsection{Statistical Learning}

Deep learning methodologies are firmly rooted in the principles of statistical learning theory~\cite{vapnik1999overview}. Consider measurable spaces $\sx \subset \Real^d$ and $\sy \subset \Real$, representing the input and output spaces respectively\footnote{All topological spaces are equipped with their Borel $\sigma$-algebra}. In supervised learning, we are presented with a dataset of labeled instances:
\[
\s{D} = \{ (\vx_1, y_1), \ldots, (\vx_n, y_n) \} \in (\sx \times \sy)^n.
\]
Our objective in this context is to ascertain the best approximation $\f$, the stochastic relationship between input $\vx \in \sx$ and its corresponding label $y \in \sy$, which is formalized as the conditional probability measure $\E(y | \vx)$ under the probability measure $\P_{\rvx,\ry}$ on $(\sx \times \sy)$.

Achieving the best approximation entails specifying a hypothesis space $\fspace$ comprising potential predictors and defining an appropriate loss function $\ell : \sy \times \sy \to \Real$ that quantifies the discrepancy between a predictor and the true label. The concept of a loss function originated in statistical decision theory, pioneered by~\cite{wald1949statistical}, with roots tracing back to Laplace’s theory of errors.

For instance, in a dataset containing images of dogs and cats, $\sx$ denotes the space of images and $\sy$ represents the labels $\{+1, -1\}$, where $+1$ denotes the presence of a dog and $-1$ denotes the presence of a cat. A conceivable loss function could measure the Euclidean distance between the predicted label and the ground truth.

In essence, the learning problem can be formulated as:
\[
\risk(\f) \defas \underset{(\rvx, \ry) \sim \P_{\rvx,\ry}}{\E}\ell(\f(\rvx), \ry)
~~~~ \text{and} ~~~~
\f^\star = \argmin_{\f \in \fspace} \risk(\f).
\]

With a $\fspace$ the set of all possible functions from $\sx$ to $\sy$. In practical scenarios, access to the true distribution $\P_{\rvx,\ry}$ is typically unavailable\footnote{As described in the Notations section, we use $(\rvx, \ry)$ to denote random variables representing theoretical constructs of inputs and outputs from the probability distribution, and $(\vx, y)$ to denote specific samples or instances from our dataset. This notation clarifies the distinction between theoretical models and empirical data in our analysis.}. Hence, we resort to approximating the learning problem using the available training set $\s{D}$ to minimize the so-called \textit{empirical risk} $\emprisk(\f)$:

\begin{definition}[Empirical risk.]
For a training dataset $\s{D} = \{(\vx_1, y_1), \ldots,  (\vx_n, y_n) \}$ and a function $\f : \sx \to \sy$, the empirical risk with respect to the loss $\ell$ is defined as:
\[
\emprisk(\f) \defas \frac{1}{n}\sum_{i=1}^n \ell(\f(\vx_i), y_i).
\]
\end{definition}

The objective is to minimize the average loss over the training data. This fundamental learning approach is known as \textit{empirical risk minimization}(ERM):

\begin{definition}[ERM learning algorithm]
Given a hypothesis set $\fspace$, the ERM selects $\f^\star$, which minimizes the empirical risk within $\fspace$:
\[
\f^\star = \argmin_{\f \in \fspace} \emprisk(\f)
\]
\end{definition}

However, we face two challenges here. Firstly, the problem is not always convex (unless employing a simple model like linear regression), rendering empirical risk minimization computationally intractable in practice. Secondly, this approach does not guarantee minimization of errors on unseen data points, leading to overfitting issues that hinder generalization and necessitate regularization.

\paragraph{Regularization}

Minimizing $\emprisk(\cdot)$ alone does not suffice for achieving robust generalization. A common strategy involves augmenting the objective with a regularization term $\Omega(\f)$:
\[
\f^\star = \argmin_{\f \in \fspace} \emprisk(\f) + \Omega(\f).
\]

Here, $\Omega(\f)$ regulates the complexity of the function. Optimization of the adjusted loss function mitigates overfitting and facilitates better generalization. 



\paragraph{Stochastic Gradient Descent (SGD)}

In the pursuit of optimizing the empirical risk, our functions within the hypothesis space $\fspace$ are usually parameterized by a set of parameters $\parameters \in \Theta$. The goal of learning in this context becomes the optimization of these parameters to minimize the loss function, effectively finding the best approximation $\f^\star$ that represents our model. A cornerstone for this optimization of empirical risk, especially within the realm of deep learning, is \textit{Stochastic Gradient Descent} (SGD). This method stands in contrast to the classical Gradient Descent approach, which necessitates computing the gradient of the loss function $\ell$ across the entire dataset to execute a single parameter update. Instead, SGD opts for a more dynamic and computationally efficient route by iteratively adjusting the model parameters utilizing a randomly selected subset of the data at each iteration. This strategy markedly diminishes computational demands, thereby enabling the training of sophisticated models on voluminous datasets.

\begin{definition}[SGD]
Given a loss function $\ell$, a learning rate $\eta$, a training dataset $\s{D}$ and a mini-batch $\s{B} = \{ \vx_i, y_i \}_{i=0}^{|\s{B}|} \subset \s{D}$, SGD iteratively updates the model's parameters $\parameters$ by calculating the gradient of $\ell$ with respect to $\parameters$ on $\s{B}$:
\[
\parameters_{t+1} = \parameters_t - \eta \sum_i^{|\s{B}|} \nabla_{\parameters}  \ell(\f(\vx_i; \parameters_t), y_i),
\]
where $\parameters_{t}$ represents the successive parameters, $\f(\cdot; \parameters)$ ny prediction function parametrized by $\parameters$ and $t$ the current iteration step.
\end{definition}

This process of iterative parameter adjustment via SGD is a direct application of the empirical risk minimization principle, adapted for the practical challenges of training deep neural networks. It allows for efficient computation and robust search through the parameter space, even in the face of complex models and large datasets.

The element of randomness in SGD, by way of selecting data points, injects a beneficial noise into the optimization trajectory. This aspect can aid in circumventing local minima, thus potentially steering the optimization towards more optimal solutions in the complex, non-convex problem spaces typical of deep neural networks. Moreover, the ability of SGD to operate efficiently with mini-batches underscores its indispensability for deep learning models. This is particularly relevant in scenarios where the sheer scale of the dataset and the model's complexity render full-batch processing impractical.

For more detail on statistical learning theory, we refer the reader to \cite{hastie2009elements}. Having discuss the learning framework in which we operate, we proceed to introduce the focal point of this work: neural networks.

\subsection{Neural Networks}

In this section, we revisit the core components of deep learning: neural networks, emphasizing their parameterization and the pivotal role of convolution operations, particularly for image data. Neural networks, parameterized by weights and biases collectively denoted as $\parameters$, are foundational to deep learning's success in various domains.

\begin{definition}[Neuron]
A neuron is a function $\neuron: \Real^d \to \Real$, parameterized by $\parameters = \{\v{w}, b\}$, and defined as:
\[
\neuron(\vx; \parameters) \defas \sigma(\v{w}^\tr \vx + b),
\]
where $\v{w} \in \Real^d$ is the weight vector, $b \in \Real$ is the bias, and $\sigma: \Real \to \Real$ is a non-linear activation function.
\end{definition}

Neurons aggregate input signals linearly weighted by $\v{w}$, add a bias $b$, and apply a non-linear function $\sigma$ to produce an output. This process enables the model to learn complex relationships between inputs and outputs.

A neural network combines multiple neurons in layers, and multiple layers can be stacked to form a deep neural network. Let's denote $\parameters$ as the collection of all parameters across the network. Then, a fully connected feedforward neural network (FCNN) can be defined as follows:

\begin{definition}[Fully Connected Feedforward Neural Network (FCNN)]
A FCNN $\f(\vx; \parameters)$ of $L$ layers is defined as a composition of layers of neurons:
\[
\f(\vx; \parameters) \defas (\layer^{(L)} \circ \ldots \circ \layer^{(1)})(\vx),
\]
where $\layer^{(i)}(\v{a}; \parameters^{(i)}) = \sigma(\m{W}_{(i)}^\tr \v{a} + \v{b}_{(i)})$ denotes the $i$-th layer function, with $\parameters^{(i)} = \{\m{W}_{(i)}, \v{b}_{(i)}\}$ being the parameters of the $i$-th layer, and $\v{a}$ the activations from the previous layer.
\end{definition}

At the core, the ensemble of neural networks we will study in this work is characterized by this structured aggregation of distinct layers, or "blocks," each serving a unique computational purpose. For the rest of this work, we will refer to this architecture interchangeably as FCNN or MLP.
Among these, certain blocks hold particular relevance to our investigation. Consequently, we will dedicate the concluding segment of this section to a description of these components. Specifically, our focus will encompass convolutional blocks, residual connections, batch normalization and finally, we will delve into the attention block.

\paragraph{Convolution layer.} Those layers are particularly adept at handling grid-like data, such as images, through the use of convolution operations. Convolution leverages the spatial structure of data, allowing the network to learn filters that capture local patterns, and stacking convolution able the model to build more global features such as shape.

\begin{definition}[Convolution Operation]
The convolution of an input $\vx$ with a filter $\v{w}$, parameterized by $\parameters = \{\v{w}, b\}$, for a single channel, is defined as:
\[
(\vx \bigotimes \v{w})_{i,j} \defas \sum_{m}\sum_{n} \vx_{m,n} \cdot \v{w}_{i-m, j-n} + b,
\]
where $\bigotimes$ denotes the convolution operation. For multichannel inputs, this operation is performed independently for each channel and summed to produce a single output.
\end{definition}

Alternatively, the convolution operation can be understood in the frequency domain through the Fourier Transform~\cite{chi2020fast}, which translates the convolution into a point-wise product in the frequency space. Following a convolution operation in a CNN, the output is typically passed through a non-linear activation function, (e.g., ReLU), to introduce non-linearity into the model. The result of applying a convolution followed by an activation function is known as an activation or \textit{feature map}. Each feature map has a dimensionality of $W \times H \times C$, where $W$ and $H$ are the width and height of the map, respectively, and $C$ refers to the number of channels. These dimensions correspond to the spatial dimensions of the image being processed and the depth of the feature map, which represents the number of filters applied during the convolution. 

Modern neural networks often cascade multiple convolution layers, alternating them with pooling layers and activation functions. Pooling layers reduce the spatial dimensions ($W$ and $H$) of the feature maps, helping to decrease the computational load and increase the \textit{receptive field} of the features. The combination of convolution, activation, and pooling layers allows the network to learn hierarchical representations of the input data, where higher-level features are composed of lower-level ones.

\paragraph{Residual Connections.}
The introduction of residual connections marked a significant advancement in deep learning architectures. Residual connections was introduced in 
\cite{he2016deep} to address the \textit{vanishing gradient} problem~\cite{hochreiter1998vanishing} that arises in very deep networks by allowing gradients to flow through a shortcut path. It consists in re-applying activations of previous layer into the next layer: 

\begin{definition}[Residual Connection]
A residual connection in a neural network allows the input of a layer to be added to its output, facilitating the learning of an identity function. This is defined as:
\[
\f(\vx; \parameters) \defas \vx + \layer(\vx; \parameters),
\]
where $\f$ represents the function implemented by the layer with residual connection, $\layer$ is the layer's original transformation function, and $\vx$ is the input to the layer. The parameters $\parameters$ denote the weights and biases of $\layer$.
\end{definition}

It turns out that allowing information to bypass one or more layers facilitate the backpropagation, thus ensuring that deeper networks can still learn effectively. This innovation has been fundamental in the development of state-of-the-art architectures. 


\paragraph{Batch Normalization.} Still in the purpose of enhancing the training stability of deep neural networks, Batch Normalization~\cite{ioffe2015batch} emerges as a crucial innovation. This technique propose to adjust the internal covariate shift -- the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. To do so, Batch Normalization standardizes the inputs to a layer for each mini-batch, thus stabilizing the learning process and allowing for higher learning rates and quicker convergence.

\begin{definition}[Batch Normalization]
Given a mini-batch of inputs $\s{B} = \{\vx_1, \ldots, \vx_n \}$, Batch Normalization normalizes the input of each feature to have zero mean and unit variance. Additionally, it introduces two trainable parameters, $\v{\gamma}$ and $\v{\beta}$, to scale and shift the normalized value. Mathematically, for an input feature $\vx$, the Batch Normalization transform is defined as:
\[
\text{BN}_{\v{\gamma}, \v{\beta}}(\vx) \defas \v{\gamma} \left(\frac{\vx - \v{\mu}_{\s{B}}}{\sqrt{\v{\sigma}_{\s{B}}^2 + \varepsilon}}\right) + \v{\beta},
\]
where $\v{\mu}_{\s{B}} = \frac{1}{n} \sum_i^n \v{x}_i$ is the empirical mean over the mini-batch $\s{B}$ ($\v{\sigma}_{\s{B}}^2$ the empirical variance), and $\varepsilon$ is a small constant\footnote{This constant may have a real impact on the training (see ~\cite{nado2020evaluating}) and are not usually well defined: $1e^{-3}$ for Tensorflow~\cite{tensorflow2015} and $1e^{-5}$ for Pytorch~\cite{paszke2019pytorch}.} added for numerical stability. The parameters $\v{\gamma}$ and $\v{\beta}$ are learned along with the original model parameters, allowing the network to undo the normalization if it is found to be counter-productive for the learning of certain layers.
\end{definition}

The $(\v{\gamma}, \v{\beta})$ parameters are usually of size $p$ with $p$ the number of features, which means that $\text{BN}_{\v{\gamma}, \v{\beta}}(\cdot)$ control the mean and variance on each channels for convolution neural net, or neurons for a MLP.

As previously stated, batch Normalization not only accelerates the training process by reducing the number of epochs required to train deep networks but also mitigates the problem of gradient vanishing/exploding, making it easier to train deep networks with saturating non-linearities. It has since become a standard component in the architecture of modern neural networks. However, lately new kind of normalization have emerged such as \textit{LayerNorm}~\cite{ba2016layer}.

\paragraph{Attention Mechanisms in Vision.} Attention mechanisms were introduced in~\cite{vaswani2017attention} and have had a profound impact across the deep learning community. Originating in NLP with the advent of Large Language Models (LLMs), these mechanisms have also significantly influenced the field of computer vision with the ViT architecture~\cite{dosovitskiy2020image,zhai2022scaling,steiner2021train}. The Attention, as initially described by~\cite{vaswani2017attention}, involves dynamically computing weights --- attention weights -- among multiple \textit{tokens} (e.g., words in a sentence, patches in an image) to facilitate their "mixing" to create a feature. This interaction among all input variables is often not possible with a single convolution (e.g., when we use filters smaller than the image size, the top-left pixel does not interact with the bottom-right pixel). From this perspective, convolution imposes an inductive bias of local interactions, whereas attention mechanisms enable all sorts of interactions, even between distant image patches. Formally, an image $\vx$ is divided into patches called tokens $\{\v{t}_1, \ldots, \v{t}_n\}, \v{t}_i \in \Real^{p}$ of dimension $p$. Each of these tokens is then processed through multiple MLPs to reduce their dimensions to $p' << p$, producing three matrices $\m{Q}, \m{K}, \m{V}$, termed key, query, and value, upon which the attention operation is then applied:



\begin{definition}
    
Given an input image $\vx$, segmented into a sequence of tokens $\m{T} = \{ \v{t}_1, \ldots, \v{t}_n \}$, where each $\v{t}_i \in \Real^p$ represents a patch of the image. These tokens are then processed through three separate MLPs, each one differently parametrized. The attention mechanism is then applied. Formally:

\begin{align*}
&\m{Q} \defas \text{MLP}_Q(\m{T}; \parameters_Q) ~~~
\m{K} \defas \text{MLP}_K(\m{T}; \parameters_K) ~~~
\m{V} \defas \text{MLP}_V(\m{T}; \parameters_V) \\
&\text{Attention}(\m{Q}, \m{K}, \m{V}) \defas \text{softmax}\left(\frac{\m{Q}\m{K}^\top}{\sqrt{p'}}\right)\m{V},
\end{align*}

where $\m{Q}$, $\m{K}$, and $\m{V}$ all lie in $\Real^{n \times p'}$. The softmax operation is applied to the rows of the resulting matrix, allowing the model to dynamically allocate attention across different regions of the input based on the relevance of each token to another token.

\end{definition}


This mechanism is especially advantageous in vision for its capacity to adaptively enhance the receptive field, enabling extensive interactions (as necessary for recognizing shapes, for instance). Leveraging attention allows models to process large volumes of visual data efficiently, focusing computational resources on the most informative parts of an image. This approach has led to the development of Transformer models, such as the Vision Transformer (ViT)~\cite{dosovitskiy2020image}, which is now considered as state-of-the-art across a wide array of computer vision tasks.

However, the attention mechanism's computational efficiency is hampered by the quadratic growth of the matrix-matrix multiplication $\m{Q}\m{K}^\top$ cost in relation to the number of tokens. This issue limits its scalability, particularly for high-resolution images or large datasets. In response, subsequent research has focused on devising strategies to mitigate this computational burden. Alternative approaches, such as sparse attention patterns, low-rank approximations, and locality-sensitive hashing, have been proposed to reduce the complexity from quadratic to sub-quadratic or even linear, with respect to the number of tokens. For a more in-depth discussion on these solutions, readers are encouraged to refer to \cite{zhang2023cab}.

\paragraph{Closing Note.} Recognizing the critical role of parameters ($\parameters$) in the various blocks we've discussed is essential. Deep neural networks are incredibly effective across numerous domains, largely due to their extensive parameterization -— for instance, ResNet50 with 25 million parameters and ViT-H boasting 632 million. This complex parametrization does not only boost their performance but also obscures their decision-making processes, making them \textit{black boxes}. This opacity underscores the necessity for Explainable AI (XAI). In the following section, we'll delve into the motivations behind XAI and explore how it can reveal the inner workings of these complex models, making their operations more transparent and understandable.
