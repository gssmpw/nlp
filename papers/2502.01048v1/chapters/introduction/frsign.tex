\section{Application: FRSign Dataset}
\label{sec:intro:frsign}

During this thesis, we propose the exploration and application of our work on the FRSign dataset~\cite{2020frsign}, a recent railway dataset, as a case study to monitor the progression of work and the development of new tools. The FRSign dataset is introduced and presented, as well as models trained on it, and will be referenced in subsequent chapters, specifically in Chapter \autoref{chap:attributions} and Chapter \autoref{chap:concepts}.

\subsection{Introduction to the FRSign Dataset}

The FRSign dataset is a recent open-source dataset, released in~\cite{2020frsign}, with the aim of pushing advancements in autonomous transportation, particularly in the less-explored area of rail systems. Despite the prevalent focus on datasets tailored for autonomous driving applications in recent years, alternative modes of transportation, such as railways, have not received comparable attention. FRSign tries to address this gap by providing a meticulously collected dataset from various locations across France, focusing exclusively on railway infrastructure. This vision-based dataset is primarily geared towards enhancing the detection and recognition capabilities concerning railway traffic lights, and thus adapted as an application for our work on Explainable AI for vision.

The dataset labelisation benefits from detailed, hand-labeled annotations,
encompassing over 100,000 images. Each image is labeled over six distinct types of French railway traffic lights, complete with metadata such as acquisition date, time, sensor parameters, and bounding boxes. This dataset was developed collaboratively by IRT SystemX as a part of the TAS (Safe Autonomous Land Transport) project, in conjunction with industry leaders in railway traffic SNCF.

\subsection{Detailed Dataset Statistics}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{assets/frsign/frsign_stats.png}
    \caption{\textbf{FRSign Dataset Statistics.} Distribution of Images per Video Sequence. The FRSign dataset exhibits a general trend of having a modest number of images per sequence, despite the presence of an outlier sequence containing more than 5000 images. Interestingly, there is a notable peak in the distribution, with a significant number of sequences having around 1000 images each.}
    \label{fig:frsign:stats}
\end{figure}

The FRSign dataset is voluminous, with a total size of 310GB, comprising 393 sequences that depict the journey of trains from one station to another. Each sequence, captured in video format, can be decomposed into individual images, leading to an average of 469 images per sequence when sampled at one image per second. The distribution of images per sequence exhibits a high degree of variability, with a standard deviation of 469, following an exponential distribution pattern, see~\autoref{fig:frsign:stats}. This distribution features a predominance of sequences with a relatively small image count (ranging from 1 to 10) to sequences with the highest image count, reaching up to 5200 images. It is noteworthy that a significant proportion of sequences contain approximately 1000 images. 

The sequence composition of the dataset underscores the importance of considering the non-independent and identically distributed (i.i.d.) nature of the images when training machine learning models. To address this, we propose a tailored train/test split strategy. But first, we describe briefly the type of data.

\paragraph{Data Types and Structure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{assets/frsign/frsign_samples.jpg}
    \caption{\textbf{FRSign Dataset Samples.} Examples of cropped images derived from the dataset. These images exhibit varying resolutions and collectively represent the six possible classes of railway signals within the dataset.}
    \label{fig:frsign:samples}
\end{figure}

At its core, the FRSign dataset comprises tuples of images and labels, with the images representing cropped segments of railway signaling lights at varying resolutions. These cropped images, derived from original footage with a uniform resolution of $1980\times1080$, vary in size due to the differing dimensions of the region of interest across sequences. This variability introduces a unique challenge in maintaining consistency across the dataset. Each cropped image is associated with one of 6 distinct classes, encompassing various states of railway signals (e.g., red light, yellow-red light). An illustrative sample of these cropped images is provided in \ref{fig:frsign:samples}.

\paragraph{Strategic Train/Test Split}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\textwidth]{assets/frsign/frsign_split_class.png}
    \includegraphics[width=0.4\textwidth]{assets/frsign/frsign_split_chassis.png}
    \caption{\textbf{FRSign Dataset Splits:} Statistics of the training and testing splits utilized for the dataset. A deliberate effort was made to consider both the classes and the distribution of different types of railway signals (``chassis'') to achieve a balanced representation across splits. Despite these efforts, there remains a degree of class imbalance.}
    \label{fig:frsign:splits}
\end{figure}

Given the sequential nature of the images within the FRSign dataset, employing a traditional random train/test split could potentially introduce significant biases and negatively impact the performance of models trained on this data. Such biases arise because images within a sequence are not independent; rather, they are closely related in both time and appearance, which could lead to overfitting if images from the same sequence are distributed across both training and testing sets.

Consequently, we advocate for a sequence-based split strategy, ensuring that all images from a given sequence are exclusively assigned to either the training or testing set, and not mixed between them. This approach not only preserves the integrity of the dataset's sequential nature but also facilitates a more balanced and effective model evaluation process by minimizing data leakage and ensuring that the model is tested on truly unseen data.

Formally, for a given sequence \(i\) containing \(n\) data points \(\mathcal{S}^{(i)} = \{ (\vx_1^{(i)}, y_1^{(i)}), \ldots, (\vx_n^{(i)}, y_n^{(i)}) \}\), each point in this sequence will be placed in the same split. For the 400 sequences \(\mathcal{S}^{(1)}, \ldots, \mathcal{S}^{(400)}\), we propose a split strategy that not only balances the classes present in each split but also takes into consideration the distribution of different lights models (``chassis''), as seen in \autoref{fig:frsign:splits}. This nuanced approach ensures that the split is not only balanced in terms of the number of images or sequences but also in terms of labels and representativeness of the various spurious cues. 

\subsection{Model Training and Evaluation}

A key focus of this thesis is the comprehensive evaluation of various models trained on the FRSign dataset, with an emphasis on examining different explanatory methods and their impact on the field of autonomous transportation. The models were subjected to a series of data augmentation techniques, including mild geometric transformations and noise addition, to enhance the robustness and generalizability of the trained models. Specific details on the augmentation strategies and their implementation will be provided in the following sections.

We examined three distinct vision models for their performance on the FRSign dataset:

\subsection{Models}

We have trained several models that we will use to showcase our explainability methods and track the progression in this manuscript. The primary objective with these models is not necessarily to achieve the highest performance, but rather to observe and compare the benefits of XAI tools. We will now describe the set of models trained on this dataset. For each of these models, a mild form of data augmentation was applied, consisting of left-right flipping, stochastic noise addition with a probability of $0.5$ from a Gaussian distribution:

\[
\v{\delta}(\r{u}) = 
\begin{cases} 
\mathcal{N}(0, \Id\sigma^2) & \text{if } \r{u} > 0.5, \\
0 & \text{otherwise}.
\end{cases}
\]

for $\r{u} \sim \mathcal{U}([0,1])$ and $\sigma = 0.1$. Additionally, we implemented slight cropping augmentation that crops from $0.8$ to $1.0$ of the original image size and contrast augmentation $\r{c} \sim \s{U}([0.8, 1.2])$. Overall, we focused on three vision models for our experiments, described as follows:

\begin{itemize}
    
    \item \textbf{VGG-16.} The first model trained is a modified version of the classic VGG-16, which accepts input images of size $128\times128$. This variant of VGG-16 includes Batch Normalization added after each convolutional layer and before ReLU activation functions. The architecture head is a global average pooling layer, omitting the original dual dense layers due to their significant memory consumption without a corresponding increase in performance. The model is trained with a dropout rate of $0.7$ and a weight decay of $1e-5$, using AdamW optimizer and cosine annealing scheduling with a warmup over 500 iterations (half an epoch) across a total of 60 epochs. The learning rate varies from a maximum of $1e^{-3}$ to a minimum of $1e^{-5}$. This model achieved an accuracy of 85\% in testing.
    
    \item \textbf{ResNet-50.} The second model is a ResNet50, configured to accept input images of size $224\times224$. The architecture adheres to the original ResNet design, the head being a global average pooling layer followed by a linear layer at the top. This model was trained without dropout but with a weight decay of $1e-5$, using the AdamW optimizer and a cosine annealing schedule with a warmup phase spanning 500 iterations (half an epoch) over a total of 60 epochs. The learning rate ranges from $1e^{-2}$ at its highest to $1e^{-5}$ at its lowest. This configuration led to a testing accuracy of 92\%.

    \item \textbf{ViT-S-32.} The third model trained is a Vision Transformer (ViT-S), designed for input images of size $224\times224$. The architecture consists of 12 blocks, with a width of 384 and 6 attention heads for each attention block, totaling 22M parameters. Additionally, the patch size used is 32. This model incorporates a stochastic depth probability of $0.1$ and a weight decay of $5e-5$, employing the AdamW optimizer and cosine annealing scheduling with a warmup over 500 iterations (half an epoch) across a total of 60 epochs. The learning rate varies from $3e^{-3}$ at its highest to $1e^{-5}$ at its lowest. The ViT model reached a testing accuracy of 90\%.
\end{itemize}

