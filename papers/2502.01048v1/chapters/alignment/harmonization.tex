\definecolor{CNN}{RGB}{99, 143, 238}
\definecolor{CNN_data}{RGB}{213, 149, 101}
\definecolor{transformer}{RGB}{241,194,70}
\definecolor{selfsup}{RGB}{217, 88, 73}
\definecolor{robust}{RGB}{103, 174, 108}
\definecolor{meta}{RGB}{236, 178, 46}



Richard Sutton's \textit{bitter lesson} articulates that seven decades of AI research have taught us that "general methods leveraging computational power overwhelmingly outperform more specialized approaches"~\cite{Sutton2019-vf}. This insight has been underscored by the advent of deep learning, particularly following the groundbreaking success of AlexNet~\cite{krizhevsky2012imagenet} on the ImageNet challenge~\cite{imagenet_cvpr09} over ten years ago. Deep neural networks (DNNs) have since seen continuous advancements, further validating Sutton's observation as these networks now match or even surpass human capabilities on the benchmark, primarily through the sheer scale of computational resources: significantly expanding the network's parameters and the volume of training images far beyond what was utilized for AlexNet~\cite{Liu2022-es,Zhai2021-al,Kaplan2020-zx}. The triumphs of these "scaling laws" are incontrovertible. However, this relentless pursuit of performance has often overlooked a critical inquiry vital for the advancement of brain sciences and the practical deployment of object recognition models: \textit{Do the visual strategies developed by DNNs mirror those employed by humans?}

The visual strategies that mediate object recognition in humans can be decomposed into two related but distinct processes: identifying \textit{where} the important features for object recognition are in a scene, and determining \textit{how} to integrate the selected features into a categorical decision~\cite{DiCarlo2012-nx, ullman2016atoms}. It has been known for nearly a century~\cite{Buswell1935-uu, Yarbus_undated-cq, Posner1980-hh, Mannan2009-xq} that different humans attend to similar locations when asked to find and recognize objects. After selecting these important features, human observers are also consistent in how they use those features to categorize objects -- the inclusion of a few pixels in an image can be the difference between recognizing an object or not~\cite{ullman2016atoms, Gruber2021-uq}.

Has the past decade of DNN development produced any models that are aligned with these human visual strategies for object recognition? Such a model could transform cognitive science by supporting a better mechanistic understanding of how vision works. More human-like models of object recognition would also resolve the problems with predictablity and interpretablity of DNNs~\cite{fel2021cannot}, and control their alarming tendency to rely on ``shortcuts'' and dataset biases to perform well on tasks~\cite{geirhos2020shortcut}. In this work, we perform the first large-scale and systematic comparison of the visual strategies of DNNs and humans for object recognition on ImageNet. 

\paragraph{Contributions.}
In order to compare human and DNN visual strategies, we first turn to the human feature importance maps collected by Linsley et al.~\cite{linsley2018learning, Lin2017-mj}. Their datasets, \textit{ClickMe} and \textit{Clicktionary}, contain maps of nearly 200,000 unique images in ImageNet that highlight the visual features humans believe are important for recognizing them. These datasets amount to a reverse inference on \textit{where} important visual features are in ImageNet images (Fig.~\ref{fig:harmonization:intro}). We complement these datasets with new psychophysics experiments that directly test \text{how} important visual features are used for object recognition (Fig.~\ref{fig:harmonization:intro}). \textbf{As DNN performance has increased on ImageNet, their alignment with human visual strategies captured in these datasets has worsened.} This trade-off is found over 84 different DNNs representing all popular model classes -- from those trained for adversarial robustness to those pushing the scaling laws in network capacity and training data. To summarize our findings:
\begin{itemize}[leftmargin=*]
    \item The trade-off between DNN object recognition accuracy and alignment with human visual strategies replicates across three unique datasets: \textit{ClickMe}~\cite{Linsley2019-ew}, \textit{Clicktionary}~\cite{Lin2017-mj}, and our psychophysics experiments.
    \item We shift this trade-off with our neural harmonizer, a novel drop-in module for co-training any DNN to align with human visual strategies while also achieving high task accuracy. Harmonized DNNs learn visual strategies that are significantly more aligned with humans \textit{than any other DNN we tested}.
    \item We release our data and code at \url{https://serre-lab.github.io/Harmonization/} to help the field tackle the growing misalignment between DNNs and humans.
\end{itemize}

\subsection{Background}

\begin{figure}[ht]
  \centering
    \includegraphics[width=0.9\textwidth]{assets/harmonization/intro.pdf}
  \caption{\textbf{Visual strategies of object recognition}. We investigate the alignment of human and DNN visual strategies in object categorization. We decompose human visual strategies into descriptions of \textit{where} important features are~\cite{Linsley2017-qe,Linsley2019-ew}, and \textit{how} those features are integrated into visual decisions.}\label{fig:harmonization:intro}
\end{figure}
\paragraph{Do DNNs explain human visual perception?} Despite the continued success of DNNs on computer vision benchmarks, there are conflicting accounts on their ability to explain human vision. On the one hand, there is evidence that DNNs are improving as models of human visual perception on challenging tasks, such as recognizing objects obscured by noise~\cite{Geirhos2021-rr}. On the other hand, there is also evidence that DNNs struggle to explain perceptual phenomena in human vision like contextual illusions~\cite{Linsley2020-en}, perceptual grouping~\cite{Kim2020-yw,Linsley2021-vx,Geirhos2020-nl}, and categorical prototypes~\cite{Golan2020-zw}. Others have found differences between human attention data and DNN models of visual attention~\cite{Linsley2019-ew,Langlois2021-ns}. Moreover, DNNs have stopped improving as models of the ventral visual system in humans and primates over recent years. While the original theory was that model explanations of object-evoked neural activity patterns improved alongside model categorization accuracy~\cite{Yamins2014-ba}, recent large-scale DNNs are worse at explaining neural data than older ones with lower ImageNet accuracy~\cite{Schrimpf2020-wp}.

\paragraph{What are the visual strategies underlying human object recognition?} Ever since its inception, a goal of vision science has been to characterize the neural processes supporting object recognition in humans. It has been discovered that object recognition can be decomposed into different processing stages that emerge over time~\cite{Fabre-Thorpe2011-js,Roelfsema2000-op,DiCarlo2012-nx,Serre2007-hq,Kietzmann2019-xy,Jagadeesh2022-df,Berrios2022-qm}, where the earliest stage is associated with processing through feedforward connections in the visual system, and the later stage is associated with processing through feedback connections. Since the DNNs used today mostly rely on feedforward connections, it is likely that they are better models for that rapid feedforward phase of processing than the subsequent feedback phase~\cite{Serre2019-bb, Serre2007-hq}. To maximize the likelihood that the visual strategies learned by DNNs align with those used by humans, our experiments focus on the visual strategies of rapid feedforward object recognition in humans.

Most closely related to our work, are studies of ``top-down'' image saliency and \textit{where} category diagnostic visual features are in images. These studies typically involve asking participants to search for an object in an image, or find visual features that are diagnostic for an object's category or identity~\cite{Linsley2017-qe, Linsley2019-ew, Koehler2014-li, Buswell1935-uu, Yarbus_undated-cq, Posner1980-hh, Mannan2009-xq}. In our work, we complement these descriptions of \textit{where} important features are in images with psychophysics testing \textit{how} those features are used to categorize objects.

\paragraph{Comparing visual strategies of humans and machines.} As methods in explainable artificial intelligence have developed over the past decade, they have opened up opportunities for comparing the visual regions selected by humans and DNNs when solving tasks. Many of these comparisons have focused on human image saliency measurements captured by eye tracking or mouse clicks during passive or active viewing~\cite{Linsley2017-qe,Linsley2019-ew,Jiang2015-vl,Peterson2018-pu,Lai2019-ln,Ebrahimpour2019-dc}. Others have compared categorical representation distances~\cite{Peterson2018-pu,Roads2020-gd} or combined those distances with measures of human attention~\cite{Langlois2021-ns}. The most direct comparisons between human and DNN visual strategies involved analyzing the minimal image patches needed to recognize objects~\cite{Ullman2016-wy,Funke2018-ft,Srivastava2019-jg}. However, these studies were limited and compared humans with older DNNs on tens of images. To the best of our knowledge, the largest-scale evaluation of human and DNN visual strategies relied on the \textit{ClickMe} dataset to compare visual regions preferred by humans and attention models trained for object recognition~\cite{Linsley2019-ew}. What is noticeably missing from each of these studies is an large-scale analysis spanning many images and models of how human and DNN alignment has changed as a function of model performance.

\paragraph{Improving the correspondence between humans and machines.} Inconsistencies between human and DNN representations can be resolved by directly training models to act more like humans. DNNs have been trained to have more human-like attention, or human-like representational distances in their output layers~\cite{Peterson2018-pu,Roads2020-gd,Linsley2019-ew,Boyd2021-mh,Bomatter2021-zs}. Here, we add to these successes with the neural harmonizer, a training routine that automatically aligns the visual strategies (Fig.~\ref{fig:harmonization:intro}) of any two observers by minimizing the dissimilarity of their decision explanations.

\subsection{Methods}\label{sec:methods}
\paragraph{Human feature importance datasets.} We focused on the ImageNet dataset to compare the visual strategies of humans and DNNs for object recognition at scale. We relied on the two significant efforts for gathering feature importance data from humans on ImageNet: the \textit{Clicktionary}~\cite{Linsley2017-qe} and \textit{ClickMe}~\cite{Linsley2019-ew} games, which use slightly different methods to collect their data. Both games begin with the same basic setup: two players work together to locate features in an object image that they believe are important for categorizing it. As one of the players selects important image regions, those regions are filled into a blank canvas for the other observer to see and categorize the image as quickly as possible. In \textit{Clicktionary}~\cite{Linsley2017-qe}, both players are humans, whereas in \textit{ClickMe}~\cite{Linsley2019-ew}, the player selecting features is a human and the player recognizing images is a DNN (VGG16~\cite{Simonyan2014-jd}). For both games, feature importance maps depicting the average object category diagnosticity of every pixel was computed as the probability of it being clicked by a participant. In total, \textit{Clicktionary}~\cite{Linsley2017-qe} contained feature importance maps for 200 images from the ImageNet validation set, whereas \textit{ClickMe}~\cite{Linsley2019-ew} contained feature importance maps for a non-overlapping set of 196,499 images from ImageNet training and validation sets. Thus, \textit{ClickMe} has far more data than \textit{Clicktionary}, but \textit{Clicktionary} data has more reliable human feature importance data than \textit{ClickMe}. Our experiments measure the alignment between human and DNN visual strategies using \textit{ClickMe} and \textit{Clicktionary} feature importance maps captured on the ImageNet validation set. As we describe in \textsection{\ref{sec:meta_pred}}, \textit{ClickMe} feature importance maps from the ImageNet training set are used to implement our neural harmonizer.

\paragraph{Psychophysics participants and dataset.} We complemented the feature importance maps from \textit{Clicktionary} and \textit{ClickMe} with psychophysics experiments on rapid visual categorization. We recruited 199 participants from Amazon Mechanical Turk (\url{mturk.com}) to complete the experiments. Participants viewed a psychophysics dataset consisting of the 100 animal and 100 non-animal images in the Clicktionary game taken from the ImageNet validation set~\cite{Linsley2017-qe}. We used the feature importance maps for each image as masks for the object images, allowing us to control the proportion of important features observers were shown when asked to recognize objects (Fig.~\ref{fig:harmonization:psychophysics}a). We generated versions of each image that reveal anywhere between 1\% to 100\% (at log-scale spaced intervals) of the important object pixels against a phase scrambled noise background (see Appendix \textsection{1} for details on mask generation). The total number of revealed pixels was equal for every image at a given level of image masking, and the revealed pixels were centered against the noise background. Each participant saw only one masked version of each object image.

\paragraph{Psychophysics experiment.} Participants were instructed to categorize images in the psychophysics dataset as animals or non-animals as quickly and accurately as possible. Each experimental trial consisted of the following sequence of events overlaid onto a white background (SI Fig. 1): (\textit{i}) a fixation cross displayed for a variable time (1,100â€“1,600ms); (\textit{ii}) an image for 400ms; (\textit{iii}) an additional 150ms of response time. In other words, the experiment forced participants to perform rapid object categorization. They were given a total of 550ms to view an image and press a button to indicate its category (feedback was provided on trials in which responses were not provided within this time limit). Images were sized at 256 x 256 pixel resolution, which is equivalent to a stimulus size approximately between 5 -- 11 degrees of visual angle across a likely range of possible display and seating setups we expect participants used for the experiment. Similar paradigms and timing parameters have been shown to capture pre-attentive visual system processing~\cite{Eberhardt2016-cw, Kirchner2006-xc, Fabre-Thorpe2011-js, Muriel2007-co}. Participants provided informed consent electronically and were compensated \$3.00 for their time ($\sim$ 10--15 min; approximately \$15.00/hr).

\paragraph{Models.}
We compared humans with 84 different DNNs representing the variety of approaches used in the field today: 50 CNNs trained on ImageNet~\cite{Chen2021-is,Tan2019-uh,Radosavovic2020-cs,Howard2019-cr,Simonyan2014-jd,Huang2018-yt,He2015-lm,Zhang2020-my,Gao2021-er,Kolesnikov2019-gg,Sandler2018-lh,Liu2022-es,Szegedy2016-fd,Szegedy2015-pr,Chollet2016-np,Radford2021-km,Xie2019-ju,Xie2016-ol,Szegedy2015-pr,Brendel2019-mw,Mehta2020-ad,Chen2017-wp,Wang2019-jm,Tan2018-zk}, 6 {\color{CNN}{CNNs}} trained on other datasets in addition to ImageNet (which we refer to as ``{\color{CNN_data}{CNN extra data}}'')~\cite{Xie2019-rp,Radford2021-km,Liu2022-es}, 10 {\color{transformer}{vision transformers}}~\cite{DAscoli2021-xw,Touvron2020-fo,Tolstikhin2021-hw,Dosovitskiy2020-if,Steiner2021-pl}, 6 CNNs trained with {\color{selfsup}{self-supervision}}~\cite{Chen2020-lw,Zeki_Yalniz2019-yo}, and 13 models trained for {\color{robust}{robustness}} to noise or adversarial examples~\cite{Geirhos2018-ag,Salman2020-lo}. We used pretrained weights for each of these models supplied by their authors, with a variety of licenses (detailed in SI~\textsection{2}), implemented in Tensorflow 2.0, Keras, or PyTorch.

\begin{figure}[!t]
  \centering
    \includegraphics[width=0.99\textwidth]{assets/harmonization/qualitative.pdf}
  \caption{\textbf{Human and DNNs rely on different features to recognize objects.} In contrast, our neural harmonizer aligns DNN feature importance with humans. We smooth feature importance maps from humans (\textit{ClickMe}) and DNNs with a Gaussian kernel for visualization.}
\label{fig:harmonization:clickme_qualitative}
\end{figure}

\subsection{Results}\label{sec:meta_pred}
\subsubsection{\textit{Where} are diagnostic object features for humans and DNNs?} To systematically compare the visual strategies of object recognition for humans and DNNs on ImageNet, we first turned to the \textit{ClickMe} dataset of feature importance maps~\cite{Linsley2019-ew}. In order to derive comparable feature importance maps for DNNs, we needed a method that could be efficiently and consistently applied to each of the 84 DNNs we tested without any idiosyncratic hyperparameters. This led us to choose a classic method for explainable artificial intelligence, image feature saliency~\cite{Simonyan2013-ln}. We prepared human feature importance maps from \textit{ClickMe} by taking the average importance map produced by humans for every image that also appeared in ImageNet validation. We then used Spearman's rank-correlation to measure the similarity between human feature maps and DNN feature maps for each image~\cite{Eberhardt2016-cw}. We also computed the inter-rater alignment of human feature importance maps as the mean split-half correlation across 1000 random splits of the participant pool ($\rho=0.66$). We then normalized each human-DNN correlation by this score~\cite{Linsley2019-ew}.

There were dramatic qualitative differences between the features selected by humans and DNNs on ImageNet. In general, humans selected less context and focused more on object parts: for animals, parts of their faces; for non-animals, parts that enable their usage, like the spade of a shovel (see Fig.~\ref{fig:harmonization:clickme_qualitative} and SI Fig.~5. The DNN that was most aligned with humans, the DenseNet121, was still only 38\% aligned with humans (Fig.~\ref{fig:harmonization:clickme_results}).

Plotting the relationship between DNNs' top-1 accuracy on ImageNet with their human alignment revealed a striking trade-off: as the accuracy of DNNs has improved beyond DenseNet121, their alignment with humans has worsened (Fig.~\ref{fig:harmonization:clickme_results}). For example, consider the ConvNext~\cite{Liu2022-es}, which achieved the best top-1 accuracy in our experiments (85.8\%), was only 22\% aligned with humans -- equivalent to the alignment of the BagNet33~\cite{Brendel2019-mw} (63\% top-1 accuracy). As an additional control, we computed the similarity between the average \textit{ClickMe} map, which exhibits a center bias~\cite{Deza2020-fq,Wang2017-dp} (SI Fig.~5), and each individual \textit{ClickMe} map. This center-bias control was only outperformed by 42/84 CNNs we tested ($\dagger$ in Fig.~\ref{fig:harmonization:clickme_results}). Overall, we observe that human and DNN alignment has considerably worsened since the introduction of these two models.

\paragraph{The neural harmonizer.} While scaling DNNs has immensely helped performance on popular benchmark tasks, there are still fundamental differences in the architectures of DNNs and the human visual system~\cite{Serre2019-bb} which could part of the reason to blame for poor alignment. While introducing biological constraints into DNNs could help this problem, there is plenty of evidence that doing so would hurt benchmark performance and require bespoke development for every different architecture~\cite{Tang2018-vg,Kubilius2019-qr,Schrimpf2020-em}. \textit{Is it possible to align a DNN's visual strategies with humans without hurting its performance?}

Such a general-purpose method for aligning human and DNN visual strategies should satisfy the following criteria: (\textbf{\textit{i}}) The method should work with any fully-differentiable network architecture. (\textbf{\textit{ii}}) It should not present optimization issues that interfere with learning to solve a task, and the task-accuracy of a model trained with the method should not be worse than a model trained without the method. We created the neural harmonizer to satisfy these criteria.

\begin{figure}[ht!]
\begin{center}
   \includegraphics[width=.99\linewidth]{assets/harmonization/pareto_v2.png}
\end{center}
   \caption{\textbf{The trade-off between DNN performance and alignment with human feature importance from \textit{ClickMe}\cite{Linsley2019-ew}}. Human feature alignment is the mean Spearman correlation between human and DNN feature importance maps, normalized by the average inter-rater alignment of humans. The shaded region denotes the pareto frontier of the trade-offs between ImageNet accuracy and human feature alignment for unharmonized models.  {\color{meta}{Harmonized}} models (VGG16, ResNet50, ViT, and EfficientNetB0) are more accurate and aligned than versions of those models trained only for categorization. Error bars are bootstrapped standard deviations over feature alignment. Arrows show a shift in performance after training with the {\color{meta}{neural harmonizer}}. The feature alignment of an average of \textit{ClickMe} maps with held-out maps is denoted by $\dagger$.}
\label{fig:harmonization:clickme_results}
\end{figure}

We propose to recall some notations before formally introducing our loss. Still within the standard supervised learning framework, we define an input space, $\sx$, and an output space, $\sy$, alongside a parameterized predictor function $\f : \sx \to \sy$. This function maps an input vector $\vx \in \sx$ to an output $\f(\vx;\parameters)$. We introduce an explanation functional, $\explainer : \fspace \times \s{X} \to \s{X}$, that produces a feature importance map $\explainer(\f, \vx)$. 

Our goal is to synchronize the model's explanation, denoted as $\explanation$, with a human-provided explanation, denoted as $\explanation^\star_{\vx}$, here a \textit{Clickmap}, without compromising the model's accuracy. We aim to achieve this alignment without sacrificing the model's accuracy. A simplicist approach to this alignment might be simply to force the model's explanations to match the human explanations as closely as possible and add a cross-entropy loss:

$$
\s{L}_{\text{naive}} = \s{L}_{\text{cross-entropy}}(\f, \vx, \vy) + \lambda \norm{\explainer(\f, \vx) - \explanation^\star_{\vx}}_2^2
$$

Interestingly, under mild assumption, one can show that \textit{aligning the explanations implicitly align the predictions} (up to a constant), a concept we encapsulate in a theorem formalizing this relationship.

\begin{definition}[$\explainer$-Aligned predictors.]
Let $\fspace : \sx \to \sy$, with $\sx = (0, 1]^d$, $\sy \subseteq \Real$, and given an explanation functionnal $\explainer : \fspace \times \sx \to \sx$. For any couple of predictors $(\f, \fbis) \in \fspace^2$ we say that the two predictor are $\explainer$-Aligned if and only if:
$$
\forall ~ \vx \in \sx ~~~~
\explainer(\f, \vx) = \explainer(\v{\psi}, \vx)
$$

\end{definition}

In other terms, this denotes that two predictors are aligned if, for each point in the input space, they yield identical explanations. An interesting property of many popular attribution methods, such as Saliency, Gradient-Input, Integrated-Gradients, or Occlusion, is that if two predictors are $\explainer$-Aligned, their predictions are also aligned (up to a constant):

\begin{theorem}[$\explainer$-Aligned imply Aligned predictions.]
Let $(\f, \fbis)$ be two $\explainer$-Aligned predictors. For any explanation functionnal $\explainer \in \{ \explainer_{\text{Sa}}, \explainer_{\text{GI}}, \explainer_{\text{IG}}, \explainer_{\text{OC}} \}$, aligning the explanations implies aligning the predictions (up to a constant):

$$
\forall \vx \in \sx ~~ \f(\vx) = \v{\psi}(\vx) + \kappa
$$

With $\kappa$ a constant independent of $\vx$.

\end{theorem}

Proofs in \autoref{app:harmonization:thm}. This suggests that theoretically, by learning the explanation of a model, one implicitly learns its decision function. Surprisingly, this is a commonality among attribution methods with apparent diverse definitions.

While this observation and theorem are insightful, the current loss present two main issues: first, the human attributions, $\explanation^\star$, are not pixel-perfect, meaning we don't have a pixel-resolution explanation for the \textit{Clickmaps}. Second, the range of values of these attributions isn't well-defined, meaning we prefer to align with the model's gradient value range to avoid overly penalizing the model. This ensures our routine can adapt to a wide range of models. To address these issues, we refine our loss function further.

\begin{itemize}
    \item To tackle the ``not pixel-perfect'' issue, we employ a multi-scale alignment strategy. This means we do not insist on a perfect pixel-wise match between explanations. Instead, we seek an approximate alignment within a reasonable delta. We utilize a Gaussian pyramid representation to demand alignment at lower-resolution versions of the explanation, thus accommodating for the lack of pixel perfection. Formally, we employ a Gaussian pyramid representation, $\pyramid_i(\cdot)$, to rescale the feature importance map $\explanation$ over $n$ levels, where $i \in \{1, \ldots, n\}$. This is accomplished by iteratively downsampling the map with a Gaussian kernel, starting from $\pyramid_1(\explanation) = \explanation$. Our objective is to minimize $\sum_{i}^n || \pyramid_i(\explainer(\f, \vx)) - \pyramid_i(\explanation^\star_{\vx}) ||^2$, ensuring the alignment of DNNs' feature importance maps with those of humans across each pyramid level.

    \item Regarding the issue of value range, we propose to stabilize the loss by standardizing both heatmaps and gradients. This standardization ensures that each explanations operate within their respective value ranges. Importantly, it allows for the most critical image regions to match, regardless of their exact values. Formally, we define the standardization function as $\v{z}(\cdot)$, which normalizes the explanation $\explanation$ so that $\v{z}(\explanation)$ has an average value of zero and a standard deviation of one. To emphasize alignment on the most critical image regions, we only consider the positive part of the standardized explanation, $\v{z}(\explanation)^{+}$.
\end{itemize}

This leads to the the complete neural harmonization loss illustrated in \autoref{fig:harmonization:loss}:

\begin{align}
    \mathcal{L}_{\text{Harmonization}} =&
    ~\mathcal{L}_{CCE}(\f, \vx, \vy) + \beta ||\parameters||_2^2 + \\
    &\lambda \sum_{i}^n || \big( (\v{z} \circ \pyramid_i \circ \explainer)(\f, \vx) \big)^+ ~ - ~ \big( (\v{z} \circ \pyramid_i)(\explanation^\star_{\vx}) \big)^+ ||_2^2 
\end{align}

\begin{figure}[t]
  \centering
    \includegraphics[width=0.9\textwidth]{assets/harmonization/loss.jpg}
  \caption{\textbf{Harmonization loss}. This figure illustrates the proposed harmonization loss, aiming to align the model's decision gradient (explanation) with the click-map. Instead of merely computing an $\ell_2$ distance between the two explanations, we introduce a multi-scale representation to compensate for the click-maps not being pixel-perfect. Additionally, we standardize both explanations to ensure that the ground truth explanations are adjusted to match the model gradient's value range.}\label{fig:harmonization:loss}
\end{figure}


\begin{figure}[ht]
\begin{center}
   \includegraphics[width=.99\linewidth]{assets/harmonization/clicktionary.pdf}
\end{center}
   \caption{\textbf{The trade-off between DNN performance and alignment with human feature importance from \textit{Clicktionary}\cite{Linsley2017-qe}}. Human feature alignment is the mean Spearman correlation between human and DNN feature importance maps, normalized by the average inter-rater alignment of humans. The shaded region denotes the pareto frontier of the trade-offs between ImageNet accuracy and human feature alignment for unharmonized models.  {\color{meta}{Harmonized}} models (VGG16, ResNet50, MobileNetV1, and EfficientNetB0) are more accurate and aligned than versions of those models trained only for categorization. Error bars are bootstrapped standard deviations over feature alignment. Arrows denote a shift in performance after training with the {\color{meta}{neural harmonizer}}.}
\label{fig:harmonization:clicktionary_results}
\end{figure}

\paragraph{Training.} We trained four different DNNs with the neural harmonizer: VGG16, ViT, ResNet50, and EfficientNetB0. These models were selected because they are popular convolutional and transformer networks with open-source architectures that are straightforward to train and also sit near the boundary of the trade-off between DNN performance and alignment with humans. Models were trained using the neural harmonizer to optimize categorization performance on ImageNet and feature importance map alignment with human data from \textit{ClickMe}. We trained models on all images in the ImageNet training set, but because \textit{ClickMe} only contains human feature importance maps for a portion of those images, we computed the categorization loss but not the neural harmonizer loss for images without importance maps. Models were trained using 8 cores V4 TPUs on the Google Cloud Platform, and training lasted approximately one day. Models were trained with an augmented ResNet training recipe (built from \url{https://github.com/tensorflow/tpu/}). Models were optimized with SGD and momentum over batches of 512 images, a learning rate of $0.3$, and label smoothing~\cite{Muller2019-td}. Images were augmented with random left-right flips and mixup~\cite{Zhang2017-hw}. The learning rate was adjusted over the course of training with a schedule that began with an initial warm-up period of 5 epochs and then  decaying according to a cosine function over 90 epochs, with decay at step 30, 50 and 80. We validated that a ResNet50 and VGG16 trained with these hyperparameters and schedule using standard cross-entropy (but not the neural harmonizer) matched published performance.


\begin{figure}[t!]
\begin{center}
   \includegraphics[width=1\linewidth]{assets/harmonization/psycho.jpg}
\end{center}
   \caption{\textbf{Comparing \textit{how} humans and DNNs use visual features during object recognition}. \textbf{(a)} Humans and DNNs categorized ImageNet validation images as animals or non-animals. The images revealed only a portion of the most important visual features according to the \textit{Clicktionary} game~\cite{Linsley2017-zt}. \textbf{(b)} There was a trade-off between DNN top-1 accuracy on ImageNet and alignment with human visual decision making. The shaded region denotes the pareto frontier of the trade-off between ImageNet accuracy and human feature alignment for unharmonized models. Arrows denote a shift in performance after training with the {\color{meta}{neural harmonizer}}. Error bars are bootstrapped standard deviations over decision-making alignment. \textbf{(c)} A state-of-the-art DNN like the ViT learned a different strategy for integrating visual features into decisions than humans or a harmonized ViT.}
 \label{fig:harmonization:psychophysics}
\end{figure}

\paragraph{The neural harmonizer aligns human and DNN visual strategies.} We found that harmonized models broke the trade-off between ImageNet accuracy and model alignment with \textit{ClickMe} human feature importance maps (Fig.~\ref{fig:harmonization:clickme_results}). Harmonized models were significantly more aligned with feature importance maps and also performed better on ImageNet. The changes in \textit{where} harmonized models find important features in images were dramatic: a harmonized ViT had feature importance maps that are far less reliant on context (Fig.~\ref{fig:harmonization:clickme_qualitative}) and approximately 150\% more aligned with humans (Fig.~\ref{fig:harmonization:clickme_results}; ViT goes from 28.7\% to 72.6\% alignment after harmonization). The same model also performed 4\% better in top-1 accuracy without any changes to its architecture. Similar improvements were found for the harmonized VGG16 and ResNet50. While the EfficientNetB0 had only a minimal improvement in accuracy, it too exhibited a large boost in human feature alignment.

\paragraph{Clicktionary.} To test if the trade-off between DNN ImageNet accuracy and alignment with humans is a general phenomenon we next turned to \textit{Clicktionary}~\cite{Linsley2017-qe}. Indeed, we observed a similar trade-off on this dataset as we found for \textit{ClickMe}: alignment with human feature importance from \textit{Clicktionary} has worsened as DNN accuracy has improved on ImageNet (Fig.~\ref{fig:harmonization:clicktionary_results}). As with \textit{ClickMe}, harmonized DNNs shift the accuracy-alignment trade-off on this dataset.

\subsubsection{\textit{How} do humans and DNNs integrate diagnostic object features into decisions?}
The trade-off we discovered between DNN accuracy on ImageNet and alignment with human visual feature importance suggests that the two use different visual strategies for object classification. However, there is potential for an even deeper problem. Even if two observers deem the same regions of an image as important for recognizing it, there is no guarantee that they use the selected features in the same way to render their decisions. We posit that if two observers have aligned visual strategies, the will agree on both \textit{where} important features are in an image and \textit{how} they use those features for decisions.

We developed a psychophysics experiment to measure how different humans use features in ImageNet images to recognize objects. Participants viewed versions of these images where only a proportion of the features that were deemed most important in the \textit{Clicktionary} game were visible (Fig.~\ref{fig:harmonization:psychophysics}a). Participants had to accurately detect whether or not the image contained an animal within 550ms, which forced them to rely on feedforward processing as much as possible~\cite{Serre2007-hq}. Each of the 200 images we used were shown to a single participant only once. We accumulated responses from all participants to construct decision curves that showed how accurately the average human converted any given proportion of image features into an object decision. We performed the same experiment on DNNs as we did on humans, recording animal \textit{vs} non-animal decisions according to whether or not the most probable category in the model's 1000-category output was an animal. Because the experiment was speeded, humans did not achieve perfect accuracy. Thus, we normalized performance for humans and DNNs to compare the rate at which each integrated features into accurate decisions.

We discovered a similar trade-off between ImageNet accuracy and alignment with human visual decision making in this experiment as we did in \textit{ClickMe} and \textit{Clicktionary} (Fig.~\ref{fig:harmonization:psychophysics}b). Indeed, the model that was most aligned with human decision-making -- the BagNet33~\cite{Brendel2019-mw} -- only achieved 63.0\% accuracy on ImageNet. Surprisingly, harmonized models broke this trend, particularly the harmonized ViT (Fig.~\ref{fig:harmonization:psychophysics}b, top-right), despite no explicit constraints in that procedure which forced consistent decision-making with humans. In contrast, an unharmonized ViT integrates visual information into accurate decisions less efficiently than humans or harmonized models (Fig.~\ref{fig:harmonization:psychophysics}c).

\subsection{Conclusion}
Models that reliably categorize objects like humans do would shift the paradigms of the cognitive sciences and artificial intelligence. But despite continuous progress over the past decade on the ImageNet benchmark, DNNs are becoming \textit{worse} models of human vision, less aligned. Our solution to this problem, the neural harmonizer, can be applied to any DNN to align their visual strategies with humans and even improve performance.

We observed the greatest benefit of harmonization on the visual transformer, the ViT. This finding is particularly surprising given that transformers eschew the locality bias of convolutional neural networks that has helped them become the new standard for modeling human vision and cognition~\cite{Serre2019-bb}. Thus, we suspect that the neural harmonizer is especially well-suited for large-scale training on low-inductive bias models, like transformers. We also hypothesize that the improvements in human alignment provided by the neural harmonizer will yield a variety of downstream benefits for a model like the ViT, including better predictions of perceptual similarity, stimulus-evoked neural responses, and even performance on visual reasoning tasks. We leave these analyses for future work.

The field of computer vision today is following Sutton's prescient lesson: benchmark tasks can be scaling architectural capacity and the size of training data. However, as we have demonstrated here, these scaling laws are exchanging performance for alignment with human perception. We encourage the field to re-analyze the costs and benefits of this exchange, particularly in light of the growing concerns about DNNs leveraging shortcuts and dataset biases to achieve high performance~\cite{Geirhos2020-nl}. Alignment with human vision need not be exchanged with performance if DNNs are harmonized. Our codebase (\url{https://serre-lab.github.io/Harmonization/}) can be used to incorporate the neural harmonizer into any DNN created and measure its alignment with humans on the datasets we describe in this paper.

\paragraph{Limitations.} One possible explanation for the misalignment between DNNs and humans that we observe is that recent DNNs have achieved superhuman accuracy on ImageNet. Superhuman DNNs have been described in biomedical applications~\cite{Linsley2021-tb,Lee2017-ip} where there is definitive biological ground-truth labels, but ImageNet labels are noisy, making it unclear if such an achievement is laudable. Thus, an equally likely explanation is that the continued improvements of DNNs at least partially reflect their exploitation of shortcuts in ImageNet~\cite{Geirhos2020-nl}. 

The scope of our work is also limited in that it focuses on object recognition in ImageNet. It is possible that models trained on other tasks, such as segmentation, may be more aligned with humans.

Finally, our modeling efforts were hamstrung for the largest-scale models in existence. Our work does not answer how much harmonization would help a model like CLIP because of the massive investment needed to train it. The neural harmonizer can be applied to CLIP but it is possible that more \textit{ClickMe} human feature importance maps are needed for successful harmonization.



