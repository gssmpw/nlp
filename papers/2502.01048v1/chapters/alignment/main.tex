\chapter{Alignment}
\label{chap:alignment}

\begin{chapterabstract}
\textit{
This chapter confronts a problem highlighted in the preceding chapter: Can we build models that align more closely with human cognition to amplify our understanding of their mechanisms ? 
Our investigation commences in \autoref{sec:harmonization}, where we study the Alignment issue through the prism of explainability. Our methodology is inspired by \cite{linsley2018learning}'s seminal work, which amassed a large dataset via psychophysical experiments. Utilizing explainability techniques, we craft a metric to gauge the similarity between model and human explanations. Initially, our metric reveals an unexpected pattern: there is a discernible trend where higher model performance correlates with decreased alignment with human in terms of explanation. In simpler terms, the more performant the models, the less they align with human explanations, evidenced by a divergence from human-generated attribution maps or heatmaps.
In response, we introduce a novel training paradigm designed to synchronize machine learning models' explanations mechanisms with human heatmaps called ``Click-maps''. We leverage attribution methods -- particularly their differentiable characteristic -- to directly steer the model's focus towards alignment with human. 
We observe that this apparent performance-alignment dichotomy can be effectively addressed with our ``harmonization'' training approach. This technique not only increase alignment between models and human explanations but also enhances model accuracy.
In \autoref{sec:lipschitz}, we explore an alternative thesis pathway: the study of robust models, with a focus on 1-Lipschitz networks. Remarkably, we discover that these networks inherently exhibit a greater alignment with human explanations, bypassing the need for specialized training routines.
}
\end{chapterabstract}

The work in this chapter has led to the publication of the following conference papers:
{\small{
\begin{itemize}

    \item \textbf{Thomas Fel}\equal, Ivan F Rodriguez\equal, Drew Linsley\equal, Thomas Serre, (2022). \textit{``Harmonizing the object recognition strategies of deep neural networks with humans''.} In: \textit{Advances in Neural Information Processing Systems}  (\textcolor{confcolor}{NeurIPS})

    \item Mathieu Serrurier, Franck Mamalet, \textbf{Thomas Fel}, Louis BÃ©thune, Thibaut Boissin, (2023). \textit{``On the explainable properties of 1-Lipschitz Neural Networks: An Optimal Transport Perspective''.} In: \textit{Advances in Neural Information Processing Systems} (\textcolor{confcolor}{NeurIPS})

    \item Drew Linsley, Ivan F Rodriguez, \textbf{Thomas Fel}, Michael Arcaro, Saloni Sharma, Margaret Livingstone, Thomas Serre, (2023). \textit{``Performance-optimized deep neural networks are evolving into worse models of inferotemporal visual cortex''.} In: \textit{Advances in Neural Information Processing Systems} (\textcolor{confcolor}{NeurIPS}).

\end{itemize}
}}

\minitoc
\clearpage

\section{Introduction}

\textit{Are there shared strategies between neural networks and humans?} This question has long motivated neuroscientists, cognitive scientists, and machine learning researchers. It is fascinating to observe that although today's neural networks outperform humans in many tasks, certain anomalies, such as adversarial examples~\cite{goodfellow2014explaining}, strongly suggest that the underlying mechanisms of these networks are not entirely the same as those of humans; they are not \textit{aligned}.

In the previous chapter, we hypothesized that a model more closely aligned with human understanding would be more interpretable. What remains to be clarified is our definition of alignment. This research field draws from neuroscience, cognitive science, and machine learning and has thus many definitions, depending on the domain. However, a common point is that Alignment aims to (1) measure how closely the internal representations of two systems match and (2) if possible, to correct any differences between them. Recently, numerous methods have been proposed, many of which are discussed in this chapter. For an excellent review of the state of the art in alignment, see~\cite{sucholutsky2023getting}.


In addressing the challenge of aligning models with human cognition, we identify two primary obstacles. The first is the difficulty in accurately capturing human cognitive processes or judgments, a task that is inherently complex and resource-intensive. Human cognition encompasses a vast array of processes, including perception, decision-making, and problem-solving, each influenced by subjective experiences and external contexts. Quantifying such a multifaceted construct requires sophisticated methodologies that often entail significant time and financial resources.

The second challenge lies in integrating these human cognitive metrics into the training of machine learning models. This integration demands innovative strategies, such as incorporating supplementary types of data, designing novel loss functions, or embedding specific biases within model architectures. The objective is to ensure that the models not only process information but also interpret and act upon it in a manner that aligns with human reasoning and judgment.

Recent advancements, particularly in the field of large language models (LLMs), have highlighted the potential of alignment through methods like Reinforcement Learning from Human Feedback (RLHF)~\cite{ouyang2022training}. RLHF integrates human judgments directly into the fine-tuning process, thereby enhancing the model's performance and alignment with human expectations. However, the deployment of such techniques is often hampered by the high cost associated with gathering and processing the requisite human feedback data. This underscores the need for efficient and targeted alignment strategies that judiciously select and utilize data and methodologies to align models effectively with human cognition.

In this chapter, we propose to take an explainability perspective on Alignment, leveraging what we have previously observed with attribution methods and aligning not explicitly the representations themselves but rather implicitly through the attribution explanations of the models. This approach guides the models toward more human-like explanations. In \autoref{sec:harmonization} we will craft a metric as well as a routine to build more aligned models, and in \autoref{sec:lipschitz} we will briefly see an alternative to the Harmonization procedure using robust models.

\clearpage

\section{Harmonizing Human and Machine Explanations}
\label{sec:harmonization}
\input{chapters/alignment/harmonization}
\clearpage

\section{On the Intriguing Effect of Robustness Towards Alignment}
\label{sec:lipschitz}
\input{chapters/alignment/lipschitz}
\clearpage

\section{Conclusion}

In this chapter, we have explored Hypothesis~\ref{hyp:alignment}, stating that alignment represents a valuable avenue for enhancing our understanding of neural networks. This approach led us to focus on the alignment between models and humans, particularly through explanations. Specifically, we have concentrated on training models to share explanations with humans. This effort addresses a critical need in the field of Deep Learning to narrow the divide between machine learning models and human interpretability. We propose two promising directions: a training routine paired with an innovative metric for assessing alignment, grounded in explainable AI, and an analysis of model robustness as a potential facilitator of alignment.

Our research was bifurcated into distinct yet complementary approaches. The first one revolved around the proposition of a new loss function to encourage regularization. The second propose to directly constrain the model architecture to be robust by design -- more specifically, to optimize over the $\lip_1$ function spaces. Both avenues, through preliminary findings, suggest seems promising for achieving more human-aligned models.

\paragraph{Perspective.} The potential for alignment goes well beyond these first steps. Tasks that more accurately reflect human cognitive processes could open up new dimensions of alignment. Additionally, the influence of diverse types of data -- such as video -- deserves in-depth exploration. Investigating these aspects could reveal how various data modalities and complexities affect the path to model alignment.

I believe that a promising direction would be to have a \textit{holistic} approach, to truly have substantial progress. This would involve integrating more accurate human \textbf{data}, using more biologically realistic \textbf{architectures}, and focusing on more \textbf{human-like tasks}.

\begin{itemize}

    \item From a data perspective, our investigation has so far focused on explanations through heatmaps, while emerging studies highlight the advantages of integrating human preferences~\cite{muttenthaler2024improving}. Many other approaches could be considered, and undoubtedly, data is likely to be a pivotal factor.
    
    \item Regarding architecture, we've explored robust designs like 1-Lipschitz networks, but there are also more biologically plausible architectures~\cite{serre2006learning} available. Notably, modeling recurrent connections found in the visual cortex offers a promising direction for aligning internal mechanisms more closely with human processes. Seminal works~\cite{linsley2020stable,chalvidal2020go} demonstrate the potential benefits of incorporating biological realism into artificial systems, suggesting these architectures could lead to computational models that align more closely with human cognition.

    \item Finally, the deep learning field's focus on classification tasks might not fully represent the complexity of human cognition. Classification is fundamental but captures just a narrow slice of human cognitive skills, which include problem-solving, learning from minimal examples, nuanced context understanding or more interestingly learning to learn~\cite{chalvidal2022meta}. 

\end{itemize}

Pursuing a broader array of actionable components to enhance model alignment, coupled with adopting this holistic viewpoint, lays the groundwork towards models that are not only more interpretable but also more deeply aligned with human thinking and learning processes.
