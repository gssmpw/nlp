In this section, we examine the application of attribution methods to models trained on the FRSign dataset~\cite{2020frsign}, and use our recently introduced Sobol method. This dataset, containing images of French railway signals, serves as a practical case for assessing our attribution technique's effectiveness in making models more transparent. Detailed setup is documented in~\autoref{sec:intro:frsign}. Our analysis primarily features results from ResNet50, yet findings are applicable to VGG and ViT models.

For this application, our focus will be twofold: firstly, we will analyze fidelity scores to determine which attribution methods is more faithful; secondly, we aim to understand the model's strategies for the classes under study. We will observe that for most classes, the model appears to use plausible features. However, for one class, the attributions are somewhat mysterious. To have deeper understanding, we will employ feature visualization to formulate a diagnosis and hypotheses.

\subsection{Fidelity Scores}

We begin with a fidelity measure to identify which attribution methods best transcribe the model's behavior. \autoref{tab:frsign:fidelity} displays the results computed from 100 randomly selected images from the test dataset\footnote{It should be noted that the question of whether it is relevant to apply explainability to the training set remains open. Up to my knowledge, I see no a priori issues with it, but out of an abundance of caution and to ensure that nothing is overlooked, we will exclusively conduct explainability analyses on the test set.}.

\begin{table}[h]
\centering
\begin{tabular}{l c c}
\textbf{Attribution Method}&\textbf{Deletion Score}&\textbf{Insertion Score}\\
\hline
Sobol&\textbf{0.329}&0.377\\
RISE&0.348&\textbf{0.396}\\
Saliency&0.402&0.325\\
Integrated Gradient&0.396&0.348\\
Grad-CAM&0.419&0.372\\
SmoothGrad&\underline{0.338}&0.363\\
Occlusion&0.345&\underline{0.380}\\
\\
\hline
\end{tabular}
\caption{\textbf{Insertion and Deletion Scores for Seven Attribution Methods on the FRSign Dataset.} This table presents the scores for each attribution method according to the fidelity metrics of insertion and deletion. It's important to remember that a lower deletion score is preferable, and a higher insertion score is considered better. The best method is highlighted in \textbf{bold}, while the second best is \underline{underlined}. The methods that appear to be the most effective are RISE, Sobol, and SmoothGrad.}
\label{tab:frsign:fidelity}
\end{table}

In \autoref{tab:frsign:fidelity}, we observe that the method we previously introduced also achieves favorable deletion scores. This finding is reassuring as it suggests that Sobol's performance generalizes beyond the datasets studied earlier. Additionally, RISE and SmoothGrad both exhibit strong performance across both metrics. Therefore, for the remainder of our study, we will primarily focus on these three methods to draw our conclusions.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.9\textwidth]{assets/frsign/good_attributions1.jpg}
\includegraphics[width=0.9\textwidth]{assets/frsign/good_attributions2.jpg}
\caption{\textbf{Comparative Visual Analysis of Attribution Methods.} We applied seven attribution methods to our model trained on the FRSign dataset. According to~\autoref{tab:frsign:fidelity}, the most faithful methods are Sobol, RISE, and SmoothGrad. Remarkably, the model tends to focus on the areas it should, specifically the traffic lights (or light) for the target class, which is reassuring.}
\label{fig:frsign:good_attributions}
\end{figure}

\subsection{Comparative Visual Analysis}

After computing the fidelity scores, we have a clearer understanding of which attribution methods more accurately reflect the model's decisions. This allows us to place greater trust in certain methods over others based on these initial tests. Sobol, RISE, and SmoothGrad emerge as the top three methods. However, we will continue to consider all methods to comprehensively assess our results. An interesting observation is that when all methods achieve good fidelity scores but highlight different areas of importance, this could be interpreted as indicating multiple ways to explain the model's reasoning. This is an intriguing aspect to explore~\footnote{Some preliminary remarks have been done on this topic in~\cite{aggregating2020}, but the ``diversity'' of explanation and the capability to aggregate them is still an interesting open questions.}. Nonetheless, it is important to remember that methods with lower fidelity scores should be approached with caution.

Figure~\autoref{fig:frsign:good_attributions} displays examples of attributions for each class that appear to be accurate. For critical signals such as violet, red, and yellow lights, the model seems to focus on the specific light or lights it is supposed to, which could increase our confidence in the model's decision-making for these types of signals.

These examples focus solely on instances where the model's predictions are correct. Next, we will apply our attribution methods to investigate failure cases, that is, instances where the model has made incorrect predictions.

\subsection{Explaining Failure Cases}

Despite the ResNet-50 model being our most performant, with an accuracy above 90\%, it is not without its share of incorrect predictions. Figure \autoref{fig:frsign:bad_attributions} presents several examples of explanations for misclassified points.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.95\textwidth]{assets/frsign/bad_labels.jpg}
\caption{\textbf{Attribution Methods on wrongly classified points.} We applied the same seven attribution methods to points where the model's predictions were incorrect. ``P'' denotes the model's prediction, $\f(\vx)$, and ``GT'' for the label $y$. Upon review, the human eye tends to agree with the model's predictions, which might lead us to suspect incorrect labeling. However, for some images, the labeling is indeed accurate, and it is light aberrations or capture problem that obscure the correct ground truth from view.}
\label{fig:frsign:bad_attributions}
\end{figure}

Upon further analysis, it appears that a portion of the data points were indeed incorrectly labeled, while a significant number are correctly labeled, although human observation alone may not always accurately identify the correct label due to noise, errors, or anomalies in the image capture process. This leads to an intriguing question that extends beyond the scope of this thesis: whether the model or the label is at fault. In other words, if in reality a signal was violet but appears red in our images, should we expect the model to perceive it as humans do, with all associated biases, or should it interpret the data optimally for the task at hand, potentially employing mechanisms or perceptions different from those of humans? These considerations open up a broader discourse, yet there is one final observation to be made before concluding this section.

The analysis of failure cases does not encompass the entirety of our observations. There remains one particularly perplexing scenario, observed post-analysis: the case of the white signals.

\subsection{White Signal}

The interpretability of white signals poses a challenge, as the focal points of the model remain unclear. This is illustrated in Figure \autoref{fig:frsign:white_signals}.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.95\textwidth]{assets/frsign/white_fire_attributions.jpg}
\caption{\textbf{Attribution Methods on White Signals.} The set of explainability methods applied to images correctly predicted as white signals is concerning. The model appears to focus on areas other than the traffic lights; however, it is unclear what specifically garners the model's attention.}
\label{fig:frsign:white_signals}
\end{figure}

The areas of attention for white signals appear cryptic and are not consistently focused on the lights. Furthermore, the focus does not always seem to be located in the same manner, which prevents a clear understanding of what the model is observing or relying upon for its decisions. We will now employ feature visualizations to delve deeper into this issue.

\paragraph{Feature Visualization.} To gain a better understanding of the potential strategies employed by our model, we utilized feature visualization. The results are shown in \autoref{fig:frsign:fviz}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.95\textwidth]{assets/frsign/fviz_fire_1.jpg}
\includegraphics[width=0.95\textwidth]{assets/frsign/fviz_fire_2.jpg}
\caption{\textbf{Feature Visualization for the Logits of $\f$.} The images represent the results of two settings of feature visualization (in Fourier space) for the image that maximizes the logits for the classes yellow, red, green, white, and violet.}
\label{fig:frsign:fviz}
\end{figure}

For crucial traffic lights such as red, yellow, and violet, the feature visualizations seem to make sense, which is reassuring. However, for white, the interpretations remain somewhat cryptic. Nonetheless, we can hypothesize that the model focuses on the frame (contour of the light) as indicated in the top feature visualization for the white light in \autoref{fig:frsign:fviz}.

\subsection{Conclusion}

Attribution methods serve as a valuable tool for understanding the model and verifying that it relies on plausible features. They provide reassurance in most cases by ensuring that the areas most important to the model are also those containing information meaningful to humans.

However, two main issues arise. Firstly, we wish to extend our methods to offer stronger guarantees; that is, to establish confidence bounds around our explanations to ensure the model's reliance on these interpretations. Secondly, in some instances, the features the model focuses on remain ambiguous, such as with the case of white signals. This observation suggests that further research is necessary to make attribution methods both safer and more informative.
