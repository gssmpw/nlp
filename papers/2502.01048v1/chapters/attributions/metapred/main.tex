
\newcommand{\metapred}{\bm{\psi}}
\newcommand{\metabase}{\bm{\psi}^{(0)}}
\newcommand{\method}{\explainer}
\newcommand{\complexity}{\textit{Complexity}}

\renewcommand{\mp}{Meta-predictor}

\newcommand{\expsa}{Saliency}
\newcommand{\expgi}{Gradient $\odot$ Input}
\newcommand{\expig}{Integrated Gradients}
\newcommand{\expsg}{SmoothGrad}
\newcommand{\expgc}{Grad-CAM}
\newcommand{\expoc}{Occlusion}

As we have seen so far, a multitude of explainability methods has been described to try to help users better understand how modern AI systems make decisions. However, most performance metrics developed and used in this manuscript to evaluate these methods have remained largely theoretical -- without much consideration for the human end-user. In particular, it is not yet clear (1) how useful current explainability methods are in real-world scenarios; and (2) whether current performance metrics accurately reflect the usefulness of explanation methods for the end user. To fill this gap, we conducted psychophysics experiments at scale ($n=1,150$) to evaluate the usefulness of representative attribution methods in three real-world scenarios. Our results demonstrate that the degree to which individual attribution methods help human participants better understand an AI system varies widely across these scenarios. This suggests the need to move beyond quantitative improvements of current attribution methods, towards the development of complementary approaches that provide qualitatively different sources of information to human end-users.


\subsection{Background.}

There is now broad consensus that modern AI systems might not be safe to be deployed in the real world~\cite{deel2021whitepaper} despite their exhibiting very high levels of accuracy on held-out data because these systems have been shown to exploit dataset biases and other statistical shortcuts~\cite{geirhos2020shortcut, d2020underspecification, shahamatdar2022deceptivelearning, fel2022aligning,moayeri2021sample,moayeri2022comprehensive,moayeri2022hard,moayeriexplicit}. A growing body of research thus focuses on the development of explainability methods to help better interpret these systems' predictions~\cite{ribeiro2016lime, sundararajan2017axiomatic, smilkov2017smoothgrad, petsiuk2018rise, selvaraju2017gradcam, linsley2018learning, fel2021sobol, eva2, novello2022making} to make them more trustworthy. The application of these explainability methods will find broad societal uses, like easing the debugging of self-driving vehicles~\cite{zablocki2021explainability} 
and helping to fulfill the ``right to explanation'' that European laws guarantee to its citizens~\cite{goodman2017european}.

In this work, we restrict our research on the most commonly used methods in eXplainable AI (XAI): attribution methods~\cite{simonyan2014deep, zeiler2014visualizing, ribeiro2016lime, selvaraju2017gradcam, sundararajan2017axiomatic, ancona2017better, smilkov2017smoothgrad,  petsiuk2018rise, fel2021sobol}. Despite a large array of methods, assessing the quality and reliability of these methods remains an open problem. So far the community has mostly focused on evaluating these methods using surrogate measures defined axiomatically such as fidelity or faithfulness measures.

\paragraph{Evaluations based on faithfulness measures.} Common approaches~\cite{samek2015evaluating, petsiuk2018rise} measure the faithfulness of an explanation through the change in the classification score when the most important pixels are progressively removed. The bigger the drop, the more faithful is the explanation method.
To ensure that the drop in score does not come from a change in the distribution of the perturbed images, the ROAR~\cite{hooker2018benchmark} methods include an additional step whereby the image classifier is re-trained between each removal step.
Because these methods do not require ground-truth annotations (i.e. object masks or bounding boxes), they are quite popular in computer vision~\cite{samek2015evaluating, petsiuk2018rise, fel2021sobol, fong2017perturbation, fong2019extremal, kapishnikov2019xrai} and natural language processing~\cite{arras2017explaining, arras2017relevant, fel2021sobol}.

Nevertheless, {\em faithfulness} measures have recently been criticized as they all rely on a baseline for removing important areas, a baseline that will obviously give better scores to methods relying internally on the same baseline~\cite{hsieh2020evaluations}. More importantly, they do not consider humans at any time in the evaluation. As a result, it is unclear if the most faithful attribution method is practically useful to humans.

\paragraph{Evaluations based on humans}
A second class of approaches consists in evaluating the ability of humans to leverage explanations for different purposes~\cite{ribeiro2016lime, selvaraju2017gradcam, mac2018teaching, chandrasekaran2018explanations, alufaisan2020does, biessmann2021quality, nguyen2021effectiveness, shen2020useful,nguyen2018comparing,hase2020evaluating,nguyen2022visual}.
\cite{ribeiro2016lime} were the first to evaluate the usefulness of explanations. Their work focused on the use case of bias detection: they trained a classifier on a biased dataset of wolves and huskies and found that the model consistently used the background to classify. They asked participants if they trusted the model before and after seeing the explanation for the model's predictions, and found that explanations helped detect bias here. We use a similar dataset to reproduce those results, but our evaluation differs greatly from theirs as we do not ask if participants trust the model but instead measure directly if they understand it. 

Closest to this work are~\cite{nguyen2018comparing,shen2020useful,hase2020evaluating, kim2021hive, sixt2022users}.~\cite{nguyen2018comparing,shen2020useful,hase2020evaluating} design their evaluation around the notion of \textit{simulatability}~\cite{kim2016examples, doshivelez2017rigorous}. They introduce different experimental procedures to measure if humans can learn from the explanations how to copy the model prediction on unseen data. Some provide the explanations at test time~\cite{nguyen2018comparing, shen2020useful}. Similar to us but for tabular data, \cite{hase2020evaluating} proposes to hide explanations at test time, this forces the participants to learn the rules driving the model's decision at training time where the explanations are shown. There are two limitations to their work: (1) they provide ground-truth labels associated with input images during training, (2) the participants see the same set of images without explanations, and then with explanations, always in that order. This creates learning effects that can heavily bias their results. We differ from their work by: (1) removing ground-truth labels from our framework as they serve no purpose and can bias participants, and (2) we have different participants go through the different conditions. This removes any learning effect, and more importantly, new explainability methods can be evaluated independently and still be compared to the previously evaluated methods. 
A recent study~\cite{kim2021hive} evaluated how AI systems may be able to assist human decisions by asking participants to identify the correct prediction out of four prediction-explanations pairs shown simultaneously. This measure reflects how well explanations help users tell apart correct from incorrect predictions. While the approach was useful to evaluate explanations in this specific scenario, it is not clear how this framework could be used to evaluate explainability methods more generally. Furthermore, when comparing different types of methods, they adapt the complexity of certain explainability methods to ease the task for participants. We argue that the complexity of explanations is an important property of explanations and that abstracting it away from the evaluation lead to unfair comparisons between methods.
In contrast, we propose a more general evaluation framework that can be used for any kind of explainability method without the need to adapt them for the evaluation procedure -- hence allowing for an unbiased and scalable comparison between methods.
Finally, \cite{sixt2022users} proposes to evaluate if users are able to identify important features biasing the predictions of a model using a synthetic dataset. By controlling the generation process of the dataset, they have access to the ground-truth attributes biasing the classifier, and can measure the accuracy of users at identifying these features. They evaluate if concept-based or counterfactual explanations help users improve over a baseline accuracy when no explanations are provided, and find no explanation tested to be useful. While both works highlights the importance of human evaluation, they differ in: the metrics employed (identifying relevant features for the model \textit{vs.} meta-prediction), the type of dataset used (synthetic \textit{vs.} real-world scenarios), and the type of methods evaluated (counterfactual and concept-based methods \textit{vs.} attribution methods). 


The main contributions of this paper are as follows:
\begin{itemize}[leftmargin=*]
    \item We propose a novel human-centered explainability performance measure together with associated psychophysics methods to experimentally evaluate the practical usefulness of modern explainability methods in real-world scenarios.
    \item Large-scale psychophysics experiments $(n=1,150)$ revealed that SmoothGrad~\cite{smilkov2017smoothgrad} is the most useful attribution method amongst all tested and that none of the faithfulness performance metrics appear to predict if and when attribution methods will be of practical use to a human end-user. 
    \item Perceptual scores derived from attribution maps, characterizing either the complexity of an explanation or the challenge associated with identifying ``what'' features drive the system's decision, appear to predict failure cases of explainability methods better than faithfulness metrics.
\end{itemize}

\begin{figure}[t!] %
  \centerline{\includegraphics[width=0.99\textwidth]{assets/metapred/figure1.jpg}}
  \caption{
  We study the practical usefulness of recent explainability methods in three real-world scenarios, each corresponding to different use cases for XAI. 
  The first dataset is Husky vs. Wolf where the goal of the explanations is to help the user to identify a source of bias in a model (classification is based on the background (snow, grass) as opposed to the animal). 
  The second dataset corresponds to a real-world leaf classification problem which is complex for non-experts. The goal of the explanations is to help the end-user identify the strategy discovered by the vision system. 
  Finally, the third dataset is a subset of ImageNet, which consists of a collection of images where half have been misclassified by the system. The goal of the explanations here is to help the end-user understand the failure sources of a high performing model.   
  }
  \label{fig:metapred:big_picture}
\end{figure}

\begin{figure*}[t!]
  \includegraphics[width=0.99\textwidth]{assets/metapred/method.png}

  \caption{
  We describe a human-centered framework to evaluate explainability methods borrowing the concept of \mp. 
  The framework requires a black box model $\pred$ (the predictor), an explanation method $\method$ and a human subject $\metapred$ which will try to predict the predictor, hence, the name \mp.
  The first step is the learning phase where the \mp~is training using $K$ samples $\bm{x}$, together with the associated model predictions $\pred(\bm{x})$ and explanations $\method(\pred, \bm{x})$. 
  The goal of this learning phase is for the \mp~to uncover the rules driving the decisions of the model from the triplets $(\bm{x}, \method(\pred, \bm{x}), \pred(\bm{x}))$.
  Then, the second step is the evaluation phase where we test the Meta-predictor's ability to correctly predict the model's outputs on new samples $\bm{\tilde{x}}$ by comparing its predictions $\metapred(\bm{\tilde{x}})$ to those of $\pred(\bm{\tilde{x}})$.
  The \metric~score of the explanation method is then computed as the relative accuracy improvement of \mp~trained with vs. without explanations.
  }
  \label{fig:metapred:metapred}
\end{figure*}


\subsection{A \mp~ perspective}

Before proposing a rigorous definition of interpretability, let us motivate our approach with an example: a linear classifier is often considered to be readily interpretable because its inner working is sufficiently intuitive that it can be comprehended by a human user. A user can in turn build a mental model of the classifier -- predicting the classifier's output for arbitrary inputs. 
In essence, we suggest that the model is interpretable because the output can be predicted -- i.e, we say we understand the rules used by a model, if we can use those inferred rules to correctly predict its output.
This concept of predicting the classifier's output is central to our approach and we conceptualize the human user as a \textit{\mp}~of the machine learning model. This notion of \mp~is also closely related to the notion of \textit{simulatability}~\cite{doshivelez2017rigorous, hase2020evaluating, pruthi2020evaluating, kim2016examples, fong2017perturbation}. 
We will now define the term more formally. 

\label{sec:metapred:framework}


We consider a standard supervised learning setting where $\pred$ is a black-box predictor that maps an input $\bm{x} \in \mathcal{X}$ (e.g., an image) to an output $\pred(\bm{x}) \in \mathcal{Y}$ (e.g., a class label).
One of the main goals of eXplainable AI is to yield useful rules to understand the inner-working of a model $\pred$ such that it is possible to infer its behavior on unseen data points.
To correctly infer those rules, the usual approach consists in studying explanations (from Attribution Map, Concept Activation Vectors, Feature Visualization, \textit{etc..}) for several predictions.
Formally, $\method$ is any explanation functional which, given a predictor $\pred$ and a point $\bm{x}$, provides an information $\method(\pred, \bm{x})$ about the prediction of the predictor. In our experiments, $\method$ is an attribution method but we would like to remind that the framework is naturally adaptable to other explainability methods such as concept-based methods or feature visualization.

\paragraph{The understandability-completeness trade-off} 

Different attribution methods will typically produce different heatmaps -- potentially highlighting different image regions and/or presenting the same information in a different format. The quality of an explanation can thus be affected by two factors:  {\em faithfulness} of the explanation (i.e., how many pixels or input dimensions deemed important effectively drive the classifier's prediction) and the understandability of the explanation for an end-user (i.e., how much of the pattern highlighted by the explanation is grasped by the user).


At one extreme, an explanation can be entirely {\em faithful} and provide all the information necessary to predict how a classifier will assign a class label to an arbitrary image (i.e., by giving all the parameters of the classifiers). However, such information will obviously be too complex to be understood by a user and hence it is not {\em understandable}. 
Conversely, an explanation that overly simplifies the model might offer an approximation of the rule used by the model that will be more easily grasped by the user --a more {\em understandable} explanation-- but this approximation might ultimately mislead the user if it is not {\em faithful}. That is to say, just because a human agrees with the evidence pointed out by an explanation does not necessarily mean that it reflects how the model works.


Overall, this means that there is a trade-off between the amount of information provided by an explanation and its comprehensibility to humans. The most useful explanations should lie somewhere in the middle of this trade-off.

\paragraph{The {\em usefulness} metric} 

We describe a new human-centered measure that incorporates this trade-off into a single {\em usefulness} measure by empirically evaluating the ability of human participants to learn to ``predict the predictor'', i.e.,  to be an accurate \mp.
Indeed, if an explanation allows users to infer precise rules for the functioning of the predictor on past data, the correct application of these same rules should allow the user to correctly anticipate the model's decisions on future data. 
Scrutable but inaccurate explanations will result in an inaccurate \mp~ -- just like accurate inscrutable ones.
This \mp~framework avoids current pitfalls such as confirmation bias - just because a user likes the explanation does not mean they will be a better \mp~- or prediction leakage on the explanation - in simulatability experiments, as the explanation is available during the test phase, any explanation that leaks the prediction would have a perfect score, without giving us any additional information about the model. We will now formally describe the metric build using this framework.


We assume a dataset\footnote{We note that, in this paper, we only considered binary dataset --Class 1 vs Class 2-- because having the participants classify more than 2 classes would increase their cognitive load and bring unnecessary difficulty to the task. Nonetheless, any dataset could have been used as classification problems with more than 2 classes can always be trivially reformulated as Target class vs. Other / binary classification problems, instead of Class 1 vs Class 2, without lack of generality.} $\mathcal{D} = \{(\bm{x}_i, \pred(\bm{x}_i), \method(\pred, \bm{x}_i) \}_{i=1}^K$ used to train human participants to learn to predict a classifier's output $\pred$ from $K$ samples made of an input image $\bm{x}_i$, the associated  predictions $\pred(\bm{x}_i)$ and explanations $\method(\pred, \bm{x}_i)$. 
We denote $\metapred^{(K)}$ a human \mp~after being trained on the dataset $\mathcal{D}$ (see Fig.~\ref{fig:metapred:metapred}) using explanations. In addition, let $\metabase$ be the human \mp~after participants were trained on the same dataset but without explanations to offer baseline accuracy scores. 
We can now define the usefulness of an explainability method $\method$ after training participants on $K$ samples through the accuracy score of the \mp~normalized by the baseline \mp~accuracy:

\begin{equation}
    \label{eq:metapred:metric_k}
    \metrick = \frac{
    \mathbb{P}(\metapred^{(K)}(\bm{x}) = \pred(\bm{x}))
    }{
    \mathbb{P}(\metabase(\bm{x}) = \pred(\bm{x}))
    }
\end{equation}

with $\mathbb{P}(\cdot)$ the probability over a test set. Thus, \metrick~score measures the improvement in accuracy that the explanation has brought. It is important to emphasize that this \metric~measure only depends on the classifier prediction and not on the ground-truth label as recommended by~\cite{jacovi2020towards}. After fixing the number of training samples $K$, we compare the normalized accuracy of different Meta-predictors. The \mp~with the highest score is then the one whose explanations were the most useful as measures compared to a no-explanation baseline. 

\paragraph{\metric~metric} In practice, we propose to vary the number of observations $K \in \{ K_0, ..., K_n \}$ and to report an aggregated $\metric$~score by computing the area under the curve (AUC) of the \metrick. 
The higher the AUC the better the corresponding explanation method is. Formally, given a curve represented by a set of $n$ points $\mathcal{C} = \{ (K_0, \metrick_0), ...,  (K_n, \metrick_n) \}$ where $K_{i-1} < K_i$ we define the metric as $\metric = AUC(\mathcal{C})$. 

\subsection{Experimental design}
\label{sec:metapred:exp_design}

We first describe how participants were enrolled in the study, then our general experimental design (See SI for more informations).

\paragraph{Participants} Behavioral data were gathered from $n=1,150$ participants using Amazon
Mechanical Turk (AMT) ({\url{www.mturk.com}}). 
All participants provided informed consent electronically and were compensated $\$1.4$ for their time ($\sim 5 - 8$ min). The protocol was approved by the University IRB and was carried out in accordance with the provisions of the World Medical Association Declaration of Helsinki. For each of the three tested datasets, we ensured that there was a sufficient number of participants after filtering out uncooperative participants ($n = 240$ participants, 30 per condition, 8 conditions) to guarantee sufficient statistical power (See SI for details).
Overall, the cost of evaluating one method using our benchmark is relatively modest (\$50 per test scenario). 

\paragraph{General study design}

It included 3 conditions: an experimental condition where an explanation is provided to human participants during their training phase (see Fig.~\ref{fig:metapred:metapred}), a baseline condition where no explanation was provided to the human participants, and a control condition where a bottom-up saliency map~\cite{itti2005bottomup} was provided as a non-informative explanation. This last control is critical, and indeed lacking from previous work~\cite{hase2020evaluating, ribeiro2016lime}, because it  provides a control for the possibility that providing explanations along with training images simply increases participants' engagement in the task. As we will show in Sec.~\ref{sec:metapred:results}, such non-informative explanations actually led to a decrease in participants' ability to predict the classifier's decisions -- suggesting that giving a wrong explanation is worse than giving no explanations at all.


Each participant was only tested on a single condition to avoid possible experimental confounds. 
The main experiment was divided into 3 training sessions (with 5 training samples in each) each followed by a brief test. In each individual trial, an image was presented with the associated prediction of the model, either alone for the baseline condition or together with an explanation for the experimental and control condition. After a brief training phase (5 samples), participants' ability to predict the classifier's output was evaluated on 7 new samples during a test phase. During the test phase, no explanation was provided to limit confounding effects: one possible effect is if the explanation leaks information about the class label.\footnote{Imagine an attribution method that would solely encode the classifiers' prediction. Participants would be able to guess the classifier's prediction perfectly from the explanation but the explanation per se would not help participants understand how the classifier works.}
We also propose to use a reservoir that subjects can refer to during the testing phase to minimize memory load as a confounding factor which was reported in~\cite{hase2020evaluating} (see SI for an illustration).

\paragraph{Datasets and models} 

We performed three distinct experiments in total -- using a variety of neural network architectures and $6$ representative attributions methods. Each of these experiments aimed at testing the usefulness of the explanation in a different context.


\begin{table}[!t]
    \centering
    \resizebox{1.00\textwidth}{!}{%
        \begin{tabular}{|c|cccc|cccc|cccc|}
        \toprule
         Method & \multicolumn{4}{c|}{Husky vs. Wolf} & \multicolumn{4}{c|}{\textit{Leaves}} & \multicolumn{4}{c|}{\textit{ImageNet}} \\
         \midrule
        Session~n$^{\circ}$               & 1 & 2 & 3 & \metric & 1 & 2 & 3 & \metric & 1 & 2 & 3 & \metric \\
         \midrule
        Baseline                    & 55.7 & 66.2 & 62.9 &  &       70.1 & 76.8 & 78.6 &  &       58.8 & 62.2 & 58.8 &  \\
        Control                             & 53.3 & 61.0 & 61.4 & 0.95 &       72.0 & 78.0 & 80.2 & 1.02 &     60.7 & 59.2 & 48.5 & 0.94 \\
        \midrule
        Saliency~\cite{simonyan2014deep}                  & 53.9 & 69.6 & 73.3 & 1.06  &        83.2 & 88.7 & 82.4 & \textbf{1.13} &      61.7 & 60.2 & 58.2 & 1.00 \\ 
        Integ.-Grad.~\cite{sundararajan2017axiomatic}     & 67.4 & 72.8 & 73.2 & 1.15 &      82.5 & 82.5 & 85.3 & \textbf{1.11} &       59.4 & 58.3 & 58.3 & 0.98\\
        SmoothGrad~\cite{smilkov2017smoothgrad}           & 68.7 & 75.3 & 78.0 & \textbf{1.20} &    83.0 & 85.7 & 86.3 & \textbf{1.13} &       50.3 & 55.0 & 61.4 & 0.93 \\
        GradCAM~\cite{selvaraju2017gradcam}               & 77.6 & 85.7 & 84.1 & \textbf{1.34} &       81.9 & 83.5 & 82.4 & 1.10 &      54.4 & 52.5 & 54.1 & 0.90 \\
        Occlusion~\cite{zeiler2014visualizing}            & 71.0 & 75.7 & 78.1 & \textbf{1.22} &       78.8 & 86.1 & 82.9 & 1.10 &     51.0 & 60.2 & 55.1 & 0.92 \\
        Grad.-Input~\cite{shrikumar2017learning}               & 65.8 & 63.3 & 67.9 & 1.06 &      76.5 & 82.9 & 79.5 & 1.05 &      50.0 & 57.6 & 62.6 & 0.95 \\
        \bottomrule
        \end{tabular}%
    }
    \caption{\textbf{\metrick~and \metric~scores}. \metrick scores across the 3 sessions for each attribution method, for each of the 3 datasets considered, followed by the \metric~scores. Higher is better. The \metric~scores of attribution methods that are statistically significant are \textbf{bolded}.}
    \label{tab:metapred:big_tab}
\end{table}

Our first scenario focuses on the detection of biases in AI systems using the popular Wolf vs. Husky dataset from~\cite{ribeiro2016lime} where an evaluation measure was already proposed around the usefulness of explanations for humans to detect biases. This makes it a good control experiment to measure the effectiveness of the framework proposed in Sec.~\ref{sec:metapred:framework}.
For this first experiment, we used the same model as in the original paper: InceptionV1~\cite{szegedy2015going}, and a similar dataset of Husky and Wolf images to bias the model. In this situation where prior knowledge of subjects can affect their \mp~score, we balance data correctness ($50\%$ of correct/incorrect examples shown). Therefore, a subject relying only on their prior knowledge will  end up as a bad \mp~of the model. For this experiment, the results come from $n=242$ subjects who all passed our screening process.

In our second scenario, we focus on a representative challenging image categorization task which would be hard to solve by a non-expert untrained human participant and the goal is for the end-user to understand the strategy that was discovered by the AI system. Here, we chose the leaf dataset described in~\cite{wilf2016computer}. We selected 2 classes from this dataset (Betulaceae and Celastracea) that could not be classified by shape to reduce the chances that participants will discover the solution on their own -- forcing them instead to rely on non-trivial features highlighted by the explanations (veins, leaf margin, etc). This scenario is far from being artificial as it reflects a genuine problem for the paleobotanist~\cite{spagnuolo2022decoding}. Can explainability methods help non-specialists discover the strategies discovered by an AI system? As participants are lay people from Amazon Mechanical Turk we do not expect them to be experts in botany, therefore we did not explicitly try to control for prior knowledge. In this experiment, $n=240$ subjects passed all our screening and other filtering processes. 

Finally, our last scenario focuses on identifying cases where an AI system fails\footnote{We acknowledge the existence of some overlap between the scenario 1 and scenario 3 as bias detection is a special case of a failure case. The reason we still use scenario 1 is because of the work previously done on it, allowing us  to validate our framework.} using ImageNet~\cite{imagenet_cvpr09}, also used in previous explainability work~\cite{fong2017perturbation,elliott2021perceptualball,hooker2018benchmark,fel2021sobol, shen2020useful,nguyen2021effectiveness}. We used this dataset because we expect it to be representative of real-world scenarios where it is difficult to understand what the model relies on for classification which makes it very difficult to understand these failure cases. 
Moreover, previous work has pointed out that attribution methods are not useful on this dataset ~\cite{shen2020useful}, we have thus chosen to extend our analysis to this particular case.
We use a ResNet50~\cite{he2016deep} pretrained on this dataset as predictor. Because prior knowledge is a major confounding factor on ImageNet, we select a pair of classes that was heavily miss-classified by the model, to be able to show subjects 50\% of correct/incorrect predictions: the pair Kit Fox and Red Fox fits this requirement.
In this experiment, we analyzed data from $n=241$ participants who passed our screening and filtering processes.


For all experiments, we compared $6$ representative attribution methods: \expsa~(SA)~\cite{simonyan2014deep}, 
\expgi~(GI)~\cite{ancona2017better}, 
\expig~(IG)~\cite{sundararajan2017axiomatic},
\expoc~(OC)~\cite{zeiler2014visualizing},
\expsg~(SG)~\cite{smilkov2017smoothgrad} and 
\expgc~(GC)~\cite{selvaraju2017gradcam}.
Further information on these methods can be found in SI.
Table \ref{tab:metapred:big_tab} summarizes all the results from our psychophysics experiments. 

\begin{figure*}[t!]
    \includegraphics[width=0.48\textwidth]{assets/metapred/lime_utility.png}
    \includegraphics[width=0.48\textwidth]{assets/metapred/pnas_utility.png}
    \caption{\textbf{\metrick~for both Husky vs. Wolf (left) and the Leaves (right) dataset.} 
    The \metrick~of the explanation, or the accuracy of the human \mp~after training, is measured after each training session (3 in total) for the scenario (1) of bias detection (on the left) and the scenario (2) concerning the identification of new strategies. 
    Concerning the first scenario, all methods have a positive effect on the score obtained - they improve the subjects' ability to predict the model - and are thus useful to better understand the model. \expgc, \expoc~and \expsg~are particularly useful for bias detection. On the Leaves dataset~\cite{wilf2016computer}, explanations are also useful, but specifically \expsa, \expsg~and \expig.
    }
    \label{fig:metapred:utility}
\end{figure*}

\subsection{Results}
\label{sec:metapred:results}

\paragraph{Scenario (1): Bias detection}

Fig.~\ref{fig:metapred:utility} shows the \metrick~scores for each method after different numbers of training samples were used to train participants for the biased dataset of Husky vs. Wolf.
The \metric~score encodes the quality of the explanations provided by a method, the higher the score, the better the method, with the baseline score being 1 (every score is divided by the baseline score corresponding to human accuracy after training without explanations). 


A first observation is that the explanations have a positive effect on the \metrick~score: the explanation allows participants to better predict the model's decision (as the \metric~scores are above 1). These results are consistent with those reported in ~\cite{ribeiro2016lime}. This is confirmed with an Analysis of Variance (ANOVA) for which we found a significant main effect, with a medium effect size ($F(7, 234) = 9.19,\ p < .001,\ \eta^2 = 0.089$). 
Moreover, the only score below the baseline is that of the control explanation, which do not make use of the model. 
We further explore our results by performing pairwise comparisons using Tukey’s Honestly Significant Difference~\cite{tukey1949comparing} to compare the different explanations against the baseline. We found 3 explainability methods to be significantly better than the baseline: \expgc~($p<0.001$), \expoc~($p = 0.01$) and \expsg~($p = 0.034$).
Thus, participants who received the \expgc, ~\expoc~ or ~\expsg~  explanations performed much better than those who did not receive them. 

\paragraph{Scenario (2): Identifying an expert strategy}

In Fig.~\ref{fig:metapred:utility}, we show results on the Leaves dataset. An ANOVA analysis across all conditions revealed a significant main effect, albeit small ($F(7, 232) = 4.29,\ p < .001,\ \eta^2 = 0.042$).
This implies that explanation also had a positive effect resulting in better \mp~ in this use case.
A Tukey’s Honestly Significant Difference test suggests that the best explanations are \expsa~, \expsg~and \expig~as they are the only ones to be significantly better than our baseline (WE) ($p = .004$, $p = .007$ and $p = .03$ respectively). An interesting result is that \expsg~seems to be consistently useful across both use cases where explanations are indeed practically useful.
A more surprising result is that \expsa~which was one of the worst explanations for bias detection, is now the best explanation on this use case (We discuss possible reasons in SI).

\paragraph{Scenario (3): Understanding failure cases}

Table~\ref{tab:metapred:big_tab} shows that, on the ImageNet dataset, none of the methods tested exceeded baseline accuracy.  Indeed, the experiment carried out, even with an improved experimental design, led us to the same conclusion as previous works~\cite{shen2020useful}: none of the tested attribution methods are useful (ANOVA: $F(7, 233) = 1.26,\ p>.05$). 
In the use case of understanding failure cases on ImageNet, \textbf{no attributions methods} seem to be useful.

\subsection*{Why do attribution methods fail?}
After studying the usefulness of attribution methods across 3 real-world scenarios for eXplainable AI, we found that attribution methods help, sometimes, but not always. We are interested in better understanding why sometimes attribution methods fail to help. Because this question has yet to be properly studied, there is no consensus if we can still make attribution methods work on those cases with incremental quantitative improvements. In the follow-up sections we explore 3 hypothesis to answer that question. 

\paragraph{Faithfulness as a proxy for Utility?} 
\begin{figure}[ht]
  \centering
  \begin{center}
    \includegraphics[width=0.8\textwidth]{assets/metapred/overall_correlation_deletion.png}
  \end{center}

  \caption{\textbf{\metric~vs Faithfulness correlation.}
        The results suggest that current faithfulness metrics are poor predictors of the end-goal usefulness of explanation methods. 
        Concerning the ImageNet dataset (triangle marker), the \metric~scores are insignificant since none of the methods improves the baseline.
        }
    \label{fig:metapred:correlations_plot}
\end{figure}

Faithfulness is often described as one of the key desiderata for a good explanation~\cite{aggregating2020,yeh2019infidelity,fel2020representativity}. If an explanation fails to be sufficiently faithful, the rules it highlights won't allow a user to understand the inner-working of the model. Thus, a lack of faithfulness on ImageNet could explain our results. 
To test this hypothesis, we use the  {\em faithfulness} metrics: Deletion\cite{samek2015evaluating, petsiuk2018rise},  commonly used to compare attribution methods~\cite{samek2015evaluating, petsiuk2018rise, fel2021sobol, fong2017perturbation, fong2019extremal, kapishnikov2019xrai}. A low Deletion score indicates a good {\em faithfulness}, thus for ease of reading we report the  {\em faithfulness} score as $1 -$ Deletion such that a higher {\em faithfulness} score is better. 

Fig \ref{fig:metapred:correlations_plot} shows the linear relationship between our Utility metrics and the faithfulness scores computed for every attribution method across all 3 datasets. We observe two main trends: 1) \textbf{There does not appear to be any specific pattern regarding faithfulness  that could explain why attribution methods are not useful for ImageNet}, and 2) the least useful attribution methods for both use cases for which methods help (Bias and Leaves) are some of the leading methods in the field measured by the faithfulness metric.
We also found a weak, if maybe anti-correlated, relation between faithfulness and usefulness: just focusing on making attribution methods more faithful does not translate to having methods with higher practical usefulness for end-users. And, in fact, focusing too heavily on faithfulness seem to come at the expense of usefulness, resulting in explanations that are counter-intuitively less useful. This second observation may seems rather alarming for the field given that the faithfulness measure is one of the driving benchmarks.


\paragraph{Are explanations too complex?} 

Using the trade-off between completeness and understandability previously discussed in Section~\ref{sec:metapred:framework}, we formulate another hypothesis: some explanations may be faithful  but too complex and therefore cannot be understood by humans. In that view, an explanation with low complexity would tend to be more useful.

As a simple measure of the complexity of visual explanations, it would be ideal to be able to compute the Kolmogorov complexity~\cite{li2004similarity} of each explanation. It was shown in previous work~\cite{da2011image} to correlate well with human-derived ratings for the complexity of natural images~\cite{forsythe2008confounds,forsythe2009visual}. As suggested by~\cite{li2004similarity,de2006approximating} we used a standard compression technique (JPEG) to approximate the Kolmogorov complexity.
Fig.~\ref{fig:metapred:complexity} shows the \metric~vs complexity score of attribution methods for each dataset. For one of the datasets where attribution methods help, the results suggest the presence of a strong correlation between \textit{usefulness} and \textit{complexity}: the least complex method is the most useful to end-users. For the other datasets, the results are either not conclusive (Leaves), or are not relevant as methods are not useful (ImageNet). 
\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.8\textwidth]{assets/metapred/complexity_correlation.png}
  \end{center}
  \caption{\textbf{\metric~vs Complexity correlation.} 
  This suggest a weak but possibly existing link between \complexity~ and \metric~ scores. 
    }
    \label{fig:metapred:complexity}
\end{figure}
Overall, \textbf{across datasets there is no significant difference in the complexity of explanations} that can explain why attribution methods do not help on ImageNet.
This could be because the Kolmogorov Complexity does not perfectly reflect human visual complexity, or because this is not the key element to explain failure cases of attribution methods.

\paragraph{An intrinsic limitation of Attribution methods?} 

The role of attribution methods is to  help identify ``where'' to look in an image to understand the basis for a system's prediction. However, attribution methods do not tell us ``what'' in those image regions is driving decisions. For categorization problems which involve perceptually similar classes (such as when discriminating between different breeds of dogs) and fine-grained categorization problems more generally, simply looking at diagnostic image regions tells the user very little about the specific shape property being relevant. For instance, knowing that the ear shape is being used for recognition does not say what specific shape feature is being encoded (e.g., pointed vs. round or narrow vs. broad base, etc). Our main hypothesis is that such a lack of explicit ``what'' information is precisely what is driving the failure of attribution methods on our ImageNet use-case.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/metapred/correlation_all_similarity.png}
    \caption{\textbf{\textit{Perceptual Similarity} scores vs. \metric.} The perceptual similarity of highlighted regions by a given attribution method for both classes is measured, for each method, for each dataset. We observe a strong correlation between the perceptual similarity of features highlighted for both classes and the practical usefulness of methods.
    }
    \label{fig:metapred:similarity}
\end{figure}

To test this hypothesis, we estimated the perceptual similarity between classes measured within diagnostic regions (see SI for more details) using the Learned Perceptual Image Patch Similarity (LPIPS) metric~\cite{zhang2018unreasonable} as it has been shown to approximate human perceptual similarity judgments well~\cite{zhang2018unreasonable, nanda2021exploring}. We report the perceptual similarity score as 1 - LPIPS score so that a high score means a high similarity.
Fig.~\ref{fig:metapred:similarity} shows the correlation between the perceptual similarity scores vs. our \metric~scores on all methods and datasets studied. Our results suggest a strong correlation  between perceptual similarity and practical usefulness: the more perceptually similar discriminative features of both classes are, the less useful attribution methods become. More importantly, the results across datasets show that on ImageNet, where attribution methods do not help, every method has a high similarity score. This result suggests that \textbf{after a certain threshold of perceptual similarity, attribution methods might no longer be useful, no matter how faithful or low in complexity the explanation is.} 
Overall, the results suggest that the perceptual similarity of discriminative features could explain why attribution methods fail on ImageNet.

\subsection{Discussion \& Hypothesis}

In summary, we conducted a large-scale human psychophysics experiment to test the utility of explainability methods in real-world scenarios. 
Our work shows that in two of the three tested scenarios (bias detection and identification of new strategies), explainability methods have indeed progressed and they provide meaningful assistance to human end-users. 
Nevertheless, we identified a scenario (understanding failure case) for which none of the tested attribution methods were helpful. This result is consistent with previous work~\cite{shen2020useful} and highlights a fundamental challenge for XAI.

Further analysis of associated faithfulness performance metrics driving the development of explainability methods revealed that they did not correlate with our empirical measure of utility -- suggesting that they might not be suited anymore to move the field forward.
We also investigated the possibility that the complexity of individual explanations may play a role in explaining human failures to learn to leverage those explanations to understand the model and, while we found a weak correlation between complexity and our empirical measure of utility, this correlation appears too low to explain the failure of these methods.

Finally, because attribution methods appear to be just as faithful and low in complexity whether they are useful or not, we explored the possibility that their failure lies, not in the quality of their explanations, but in the intrinsic limitations of attribution methods. If fully grasping the strategy of a model requires understanding, not just ``where'' to look (as revealed by attribution maps) but also ``what'' to look at, something not currently revealed by these methods, attribution methods will not help. Our assumption is that the need for finer ``what'' information should arise when diagnostic image locations across classes look perceptually very similar and potentially semantically related for certain classification problems (e.g., looking at the ears or the snout to discriminate between breeds of cats and dogs) and one needs to identify what visual features are driving decisions. We computed a perceptual score for classification problems by estimating the perceptual similarity between diagnostic image regions (as predicted by attribution methods) and found that, indeed, when this score predicts a certain level of perceptual similarity between classes, attribution methods fail to contribute useful information to human users, regardless of the faithfulness or complexity of the explanations. This suggests that explainability methods may need to communicate additional information to the end user beyond attribution maps.
