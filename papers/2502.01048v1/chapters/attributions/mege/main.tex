\newcommand{\fidelity}{\textit{Fidelity}}
\newcommand{\stability}{\textit{Stability}}
\newcommand{\comprehensibility}{\textit{Comprehensibility}}
\newcommand{\consistency}{\textit{Consistency}}
\newcommand{\representativity}{\textit{Generalizability}}
\newcommand{\mege}{MeGe}
\newcommand{\reco}{ReCo}
\newcommand{\Setp}{\boldsymbol{\mathcal{S}^{ \ne }}}
\newcommand{\Setm}{\boldsymbol{\mathcal{S}^{ = }}}
\newcommand{\Seta}{\boldsymbol{\mathcal{S}}}
\newcommand{\fold}{\mathcal{V}}
\newcommand{\deltaij}{\delta_{\vx}^{(i,j)}}

In this section, we aim to develop a new metric for explainability to identify better models. As we've seen in \autoref{sec:attributions:intro}, many attributions methods have been proposed to explain how deep neural networks make decisions, but there hasn't been much effort to ensure that the explanations they provide are objectively relevant. While several desirable \textit{properties} for trustworthy explanations have been identified, it's been challenging to come up with \textit{objective measures} for them. Here, we propose two new measures to assess explanations, borrowed from the field of algorithmic stability: mean generalizability (\mege) and relative consistency (\reco).

We'll begin by briefly reviewing related work on metrics, then we'll introduce our methods and the two metrics. Afterward, we conduct extensive experiments using various network architectures, common explainability methods, and several image datasets to showcase the advantages of these measures. We'll demonstrate that they pass sanity checks, allowing us to move on to the experimental phase where we'll show (1) that current fidelity measures are not sufficient to guarantee algorithmically stable and trustworthy explanations, (2) that our metrics can be use to select the best attribution method for a given model, and finally (3) that our metrics can be use to identify models with better explanations.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.80\textwidth]{assets/mege/big_picture.png}
  \caption{ 
  The trustworthiness of a predictor's explanations hinges on its algorithmic stability. This concept implies that when an image $\vx$ is excluded from the training dataset $\s{D}$, a separate predictor $\f_{\textcolor{red}{|\vx}}$ trained using the same algorithm $\s{A}$ (but without $\vx$) should yield a comparable explanation for that image. This stability indicates that the explanations are drawn from multiple points, making them more general. For instance, when classifying images as "pandas," the explanations should consistently highlight the dark areas around their eyes, with or without $\vx$ in the dataset.
  }
  \label{fig:big_picture}
\end{figure}

\subsection{Background.} 

In this section, we focus on evaluating explanations provided by explainability methods, which give insight into how a given neural network architecture reaches a particular decision~\cite{doshivelez2017rigorous}. These explainability methods produce an influence score for each variables. In the case of image classification, these methods will produce heatmaps indicating the diagnosticity of individual image regions. Most of these explainability methods rely on backpropagating the gradient with respect to a given input image ~\cite{Zeiler2011, simonyan2013deep, bach2015pixel, Fong_2017, shrikumar2017learning, sundararajan2017axiomatic, smilkov2017smoothgrad, Selvaraju_2019, hartley2021swag} or with respect to a perturbation of the input~\cite{zeiler2013visualizing, zhou2014object, ribeiro2016lime, li2016understanding, zintgraf2017visualizing, ribeiro2018anchors}. We refer the reader to \autoref{sec:attributions:intro} for more details.

Despite a wide range of explainability methods, assessing the quality and trustworthiness of these explanations is still an open problem. It is in part due to the difficulty of obtaining objective ground truths~\cite{samek2015evaluating,linsley2018learning}. Several criteria have been proposed to evaluate the quality of explanations~\cite{tintarev2007survey, miller2017explanation,robnik2018perturbation,gilpin2018explaining,alvarezmelis2018robust, survey2019metrics,ferrettini2021coalitional}. According to~\cite{survey2019metrics}, the five major properties include: \fidelity, \stability, \comprehensibility, \representativity~and \consistency. Yet, properties such as \representativity~and \consistency~do not come with a practical definition.

In order to measure these different properties, there are two main approaches currently used. The first subjective approach consists in putting the human at the heart of the process, either by explicitly asking for human feedback~\cite{Selvaraju_2019, ribeiro2016lime, lundberg2017unified}, or by indirectly measuring the performance of the human/classifier duo~\cite{lage2019evaluation, narayanan2018humans, schmidt2019quantifying}. Nevertheless, human intervention sometimes brings undesirable effects, including a possible confirmation bias~\cite{adebayo2018sanity}.

A second type of approaches has also started to emerge specifically for computer vision applications. The main idea is to build objective proxy tasks that a good explanation must be able to solve.
These measures aim to evaluate explanations based on two properties: \fidelity~and \stability. The first method to measure \fidelity~was first proposed in~\cite{samek2015evaluating} based on estimating the drop in prediction score resulting from deleting pixels deemed important by an explanation method. To ensure that the drop in score does not come from a change in distribution, ROAR~\cite{hooker2018benchmark} was proposed which re-train a classifier model between each deletion step. This boils down to measuring the correlation between the attributions for each pixel and the difference in the prediction score when they are modified and has been clearly formalized~\cite{yeh2019infidelity, aggregating2020, rieger2020irof}.
Nevertheless, it should be noted that the different fidelity metrics proposed requires defining a baseline state which might favor explainability methods that internally relies on the same baseline~\cite{sturmfels2020visualizing}.

Those \fidelity~metrics are a first step toward trustworthy explanations: by making sure that we have faithful explanations, we can then look at other criteria to quantitatively measure these explanations. 

\paragraph{Algorithmic Stability} represents a nuanced form of Sensitivity Analysis, focusing on the impact that modifications to the learning dataset have on an algorithm's output. This concept is pivotal for deriving various generalization bounds, as highlighted by Bousquet et al.~\cite{bousquet2002stability}. Essentially, an algorithm demonstrates stability if it yields consistent predictions across datasets that only differ by a single instance. More precisely, an algorithm $\mathcal{A}(\vx ; \s{D})$, which train on $\s{D}$ and output a prediction on $\vx$, is deemed $\xi$-uniformly stable if, for any two datasets $\mathcal{D}$ and $\mathcal{D}'$ differing by at most one element, the subsequent inequality is satisfied for a given loss function $\ell$:

\begin{definition}[Uniform Stability~\cite{bousquet2002stability}] An algorithm $\mathcal{A}$ exhibits uniform stability $\xi$ with respect to a loss function $\ell$ if for every input $\vx$ and output $y$, the following condition is met:

\begin{equation}
\sup_{\vx} \left| \ell(\mathcal{A}(\vx; \mathcal{D}), y) - \ell(\mathcal{A}(\vx; \mathcal{D}'), y) \right| \leq \xi
\end{equation}

\end{definition}

Here, $\xi$ denotes a small, non-negative constant. This stability criterion is integral for assessing the generalization performance of various statistical learning models, offering a theoretical basis to gauge the efficacy of a model on novel data. Algorithmic stability suggests that an algorithm's reliance on any particular training instance is minimal, thus mitigating the risk of overfitting and bolstering the model's generalization capabilities. Consequently, there is a direct correlation between algorithmic stability and generalization error, illuminating the delicate equilibrium between training data fidelity and resilience to data variability.

Below, we briefly provide some motivation that rely on this notion for the proposed  \mege~and \reco~ measures before describing a training procedure applicable to a large family of machine learning models in order to estimate these two values. 

\subsection{Algorithmic Stability measure for Explainability}


\paragraph{Notations.} We consider a standard supervised learning setting where a datapoint is denoted $\v{z} = (\vx, \vy)$ s.t. $\vx \in \sx$ is an observation (e.g., $\sx = \mathbb{R}^d$) and $\vy \in \sy$ is a class label (e.g., $\sy = \mathbb{R}^p$).
The data set is denoted as $\s{D} = \{ \v{z}_1, ..., \v{z}_m \}$,  we designate $\fold = \{ \fold_1, ..., \fold_k \}$ the set of $k$ disjoints subsets (\textit{folds}) of size $m/k$ at random where each $\fold_i \subset \s{D}$. Throughout this work, we will assume $k$ divides $m$ for convenience.
Let $\s{A}$ be a deterministic learning algorithm that maps any number of data points onto a predictor function $\s{A} : \s{D} \to (\sx \to \sy)$.
In particular, we consider the \textit{fold} $\fold_i$ and the associated predictor $\f_i = \s{A}(\fold \setminus \fold_i)$.

An explanation method is a functional, denoted $\explainer$, which, given a predictor $\f_i$ and a datapoint $\vx$, assigns an importance score for each input dimension $\explanation_{\vx}^{(i)} = \explainer(\f_i, \vx)$. Moreover, we assume a distance $d(\cdot,\cdot)$ over the explanations.
Finally, the following Boolean connectives are used: $\neg$ denotes a negation, $\land$ denotes a conjunction, and $\oplus$ denotes an exclusive or (XOR).

\paragraph{Motivation.} We first consider \representativity: we provide a definition, discuss the inherent difficulties associated with its measurement, and describe a method for estimating it.
We then motivate the need for assessing the \consistency~of an explanation and propose a measure.

\begin{definition}{\representativity}\\
A measure of how generalizable an explanation is, and the extent to which it captures the underlying patterns or features across various datasets or scenarios. 
\label{def:mege:representativity}
\end{definition}

Intuitively, a representative explanation would be an explanation that holds for a large number of samples.
To assess the number of samples that can be covered by a given explanation, it might be tempting to compute a distance between the explanations associated with those samples. However,  because of the large variations in the appearance of objects that arise because of translation, scale, and 3D rotation in natural images, two explanations can be similar (i.e., close in pixel space) without necessarily reflecting a similar visual strategy used by the predictor (for instance, decisions could be driven by the same pixel locations -- yet driven by different visual features). Conversely, two spatially distant explanations could be based on the same features that appear at different locations because of translation. 

Our proposed solution to this problem is to only use distance measured between explanations for the same sample. This constraint leads us to consider the notion of algorithmic stability as a proxy for generalization: intuitively, given a predictor and a training data set, a good explanation for a decision made for a given data point should be robust to the addition or removal of that data point from the training set. One benefit of such a characteristic is that it can be evaluated based solely on a distance between explanations from the same sample.

In what follows, we will propose a relaxed version of the algorithmic stability -- computationally more manageable -- applied to the explanations using several predictors trained on different \textit{folds}.
It is important to note that the term algorithmic stability \cite{bousquet2002stability} is not related to the \stability~of an explanation as defined in~\cite{aggregating2020}.

Following this consideration, we will be looking at how well a predictor's explanations generalize from seen to unseen data points:

\begin{equation}
    \label{eq:mege:delta}
    \deltaij = d(\explanation_{\vx}^i, \explanation_{\vx}^j) \; s.t. \; \vx \in \fold_i, \vx \notin \fold_j.
\end{equation}

\noindent By making sure that $\vx$ only belongs to the \textit{fold} $\fold_i$, we measure the distance between two explanations, one of which comes from a predictor that was not fitted to the sample $\vx$. 
By computing these distances, we hope to characterize the \representativity~of the explanations.
We now propose to study the consistency property.

\begin{definition}{\consistency} \\
The extent to which different predictors trained on the same task do not exhibit logical contradictions.
\label{def:mege:consistency}
\end{definition}

A statement, or a set of statements, is said to be logically consistent when it has no logical contradictions.
A logical contradiction occurs when both a statement and its negation are found to be true.
In logic, a fundamental law -- the law of non-contradiction -- is that a statement and its negation cannot both be true simultaneously.  
Similarly, we measure the consistency between explanations by ensuring that contradictory predictions lead to different explanations.

Following this definition, if the same explanation gets associated with two contradictory predictions the explanation is said to be inconsistent. 
This means avoiding the case where for an observation $\vx \in \fold_i$, two predictors $\f_i, \f_j$ (where $i \neq j$), trained on the same task, give the same explanation but different predictions:

\begin{align}
    \label{eq:mege:consistency}
    \f_i(\vx) \neq \f_j(\vx) \implies \explanation_{\vx}^{(i)} \neq \explanation_{\vx}^{(j)}
\end{align}

Nevertheless, we have to define what it means for two explanations to be different. For this, we use a measure of dissimilarity between explanations and a threshold to judge whether the explanations are consistent or not. This threshold will be relative to the distance between explanations when predictions are not contrary.
By measuring the rate of inconsistent explanations, we hope to capture the notion of \consistency~for explanations.

\paragraph{$k$-Fold Cross-Training}

\begin{figure*}[ht]
  \centering
  \includegraphics[width=0.99\textwidth]{assets/mege/procedure.png}
  \caption{Application of the proposed procedure for $3$ \textit{folds}. Each predictor is trained on two of the 3 \textit{folds}, e.g, $\f_{\textcolor{blue}{1}}$ is trained on $\s{D} \setminus \fold_1$. For a given sample $\vx$ such that $\vx \in \fold_1$, the explanations for each predictors are calculated ($\explanation_{\vx}^{(\textcolor{blue}{1})}, \explanation_{\vx}^{(\textcolor{red}{2})}, \explanation_{\vx}^{(\textcolor{yellow}{3})}$). The distance between $\explanation_{\vx}^{(\textcolor{blue}{1})}$ and the  other two explanations  $\explanation_{\vx}^{(\textcolor{red}{2})}, \explanation_{\vx}^{(\textcolor{yellow}{3})}$ are computed. 
  All distances for which predictions do not contradict each other are added to $\Setm$ while the others are added to $\Setp$ (note that this is the case for $\delta^{(\textcolor{blue}{1},\textcolor{yellow}{3})}_{\vx}$ since $\f_{\textcolor{blue}{1}}(\vx) \neq \f_{\textcolor{yellow}{3}}(\vx)$).
  }
  \label{fig:mege:methodology}
\end{figure*}

We recall that our data set is divided into $k$-\textit{folds} of the same size $\s{D} = \{\fold_i\}_{i=0}^k$, and that each predictor is trained through a learning algorithm $\f_i = \s{A}(\fold \setminus \fold_i)$.
We assume that the predictors exhibit comparable performance across folds. In our experiments, we ensure a similar accuracy on the test set.

We will now measure the distances between two explanations associated with these different predictors.
To be more precise, we are really only interested in computing $\deltaij$ (see Eq.~\ref{eq:mege:delta}):, the distance between two explanations whereby one of the two predictors was not fitted on $\vx$. Otherwise, it may be trivial for two predictors that were trained on that sample to yield the same explanation -- especially if overfitting occurs (see Fig. \ref{fig:mege:methodology}).

In the case where both predictors gave a correct prediction, a small distance between the two explanations suggests that the explanations receive support from several samples. In other words, the fact that  explanations do not vary widely when adding or removing a particular sample or set of samples suggests good \representativity. Alternatively, if the two predictors give contrary predictions, the corresponding explanations should be different. Indeed, the very notion of \consistency~between explanations implies that the same explanation cannot account for two different outcomes. 

We separate distances into two sets, $\Setm$~when the predictors have made correct predictions s.t. it is desirable to have a small distance between explanations, $\Setp$~when one of the predictors have given a wrong prediction s.t. it is desirable to have higher distances between the pairs of explanations. The case where both predictors give a bad prediction is ignored (for details, see the Alg.~\ref{alg:mege:procedure} in the appendix).

\begin{align}
    \Setm &= \{ \deltaij : \f_i(\vx) = \vy \land  \f_j(\vx) = \vy \} \\
    \Setp &= \{ \deltaij : \f_i(\vx) = \vy \oplus  \f_j(\vx) = \vy \}
\end{align}
$$ \hspace{10000pt minus 1fil} \forall (i, j) \in \{1, ..., k\}^2 ~s.t.~ i \neq j, ~\forall (\vx, \vy) \in \fold_i  \hfilneg $$


\paragraph{Mean generalizability : \mege}
\label{MeGemeasure}
From Def.~\ref{def:mege:representativity}, the distance between explanations arising from predictors trained on a dataset that contained vs. did not contain a given sample should be small. 
As those distances are contained in $\Setm$, one way to measure the \representativity~of explanations is to compute the average over $\Setm$.

As a reminder, the average of $\Setm$ corresponds to the average change of explanation when the sample is removed from the training set. This change is related to the \representativity~of the explanation: the more representative an explanation is, the more it persists when we remove a point.

To ensure a high value of our metric for low distances, we define the \mege~measure as a similarity measure:
\begin{align} 
    MeGe &= \Big(1 + \frac{1}{|\Setm|}\sum_{\delta ~\in~ \Setm} \delta \Big)^{-1}
    \label{eq:mege:mege}
\end{align}

Explanations with good \representativity~ will therefore be associated with higher similarity scores between explanations (close to 1).

\paragraph{Relative consistency : \reco}
\label{ReComeasure}


From Def.~\ref{def:mege:consistency} and Eq.~\ref{eq:mege:consistency}, explanations arising from different predictors are said to be consistent if they are close when the predictions agree with one another.
As a reminder, the distance between explanations for the consistent predictions are represented by $\Setm$, and those associated with inconsistent predictions by $\Setp$. 
Visually, we seek to maximize the shift between the corresponding distributions for the sets $\Setm$ and $\Setp$.
Formally, we are looking for a distance value that separates $\Setm$ and $\Setp$, e.g., such that all the lower distances belong to $\Setm$ and the higher ones to $\Setp$. The clearer the separation, the more consistent the explanations are.
In order to find this separation, we introduce \reco, a statistical measure based on maximizing the balanced accuracy.

Where $ \Seta = \Setm \cup \Setp $ and $\lambda \in \Seta$ a fixed threshold value, we can define the true positive rate $TPR$ as the rate for which distances below a threshold come from consistent predictions among all distances below the threshold $TPR(\lambda) = \frac{|\{\delta \in \Setm : \delta \leqslant \lambda \}|} {|\{ \delta \in \Seta~:~ \delta \leqslant \lambda \}| \hfill}$. In a similar way, $TNR$ denotes the rate for which distances above a threshold come from opposite predictions among all the distances above the threshold $TNR(\lambda) = \frac{|\{ \delta \in \Setp : \delta > \lambda \}|}{|\{ \delta \in \Seta~:~ \delta > \lambda \}| \hfill}$. Basing our measure on these rates allows us to assess the quality of these explanations independently of the accuracy of the predictor, we define \reco~ as the maximal balanced accuracy:

\begin{align}
    ReCo &= \max_{\lambda \in \Seta}\ TPR(\lambda) + TNR(\lambda) - 1, 
    \label{eq:mege:reco}
\end{align}

with a score of 1 indicating perfect consistency of the predictors' explanations, and a score of 0 indicating a complete inconsistency.

\subsection{Experiments}

We carried out three sets of experiments using a variety of neural network architectures and explanation methods. 
The first one consisted in ensuring the functioning and the reliability of the measures via a simple sanity check done over a large number of predictors ($175$ in total).
The second set of experiments consisted in highlighting a limitation of the fidelity measure -- namely its \textbf{independence} with respect to the quality of the explanations. 
We developed these considerations in a dedicated section where we demonstrate an application to the selection of a method using the two new criteria \mege~and \reco.
Finally, in a third set of experiments, we showed quantitatively that some predictors are more interpretable: our analyses revealed that 1-Lipschitz neural networks yield explanations that are more representative and coherent.

\paragraph{Setup.} For all experiments, we used 5 splits ($k = 5$), i.e., $5$ predictors with comparable accuracy ($ \pm 3\%$), which allows us to study the explanations in common training conditions (80\% of the data are used for training and 20\% for testing).
For ILSVRC 2012, our predictors are based on a ResNet-50 architecture~\cite{he2016deep}, and a ResNet-18 for the other datasets.
\paragraph{Explanation methods.}
In order to produce the necessary explanations for the experiment, we used $7$ methods of explanation. 
The methods selected are those commonly found in the literature in addition to one control method (Random).
The explanations methods chosen are as follow: Saliency \textbf{(SA)}~\cite{simonyan2013deep}, Gradient $\odot$ Input \textbf{(GI)}~\cite{ancona2017better}, Integrated Gradients \textbf{(IG)}~\cite{sundararajan2017axiomatic}, SmoothGrad \textbf{(SG)}~\cite{smilkov2017smoothgrad}, Grad-CAM \textbf{(GC)}~\cite{Selvaraju_2019}, Grad-CAM++ \textbf{(G+)}~\cite{chattopadhay2018grad} and RISE \textbf{(RI)}~\cite{petsiuk2018rise}. Further information on these methods can be found in \autoref{sec:attributions:intro}.

\paragraph{Datasets.} We applied the procedure described above and evaluated the proposed measures for each of the degradations on $4$ image classification datasets: 
\textbf{ILSVRC 2012}~\cite{imagenet_cvpr09}: a subset of the ImageNet dataset from which we randomly selected $50$ classes. The size of the images considered was $224 \times 224$.
The reduced number of classes being sufficient to show that the metrics pass the test performed even in the case of high dimensional images.
\textbf{CIFAR10}~\cite{krizhevsky2009learning}: a low-resolution labeled datasets with 10 classes respectively, consisting of $60,000$ ($32 \times 32$) color images. 
\textbf{EuroSAT}~\cite{helber2019eurosat}: a labeled dataset with $10$ classes consisting of $27,000$ color images ($64 \times 64$) from the Sentinel-2 satellite.
\textbf{Fashion MNIST}~\cite{xiao2017fashion}: a dataset containing $70,000$ low-resolution ($28 \times 28$) grayscale images labeled in $10$ categories.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.98\textwidth]{assets/mege/ImageNet_Cifar_results.pdf}
    \caption{\textbf{\mege~ and \reco~scores} for predictors trained with no degradations (first point from the left), as well as for progressively randomized predictors and predictors trained with switched labels.
    For all the methods tested, the more the predictor is degraded, the more the \consistency ~and \representativity~scores drop, which means that the associated metrics pass the sanity check.
    \textbf{Top} ImageNet. \textbf{Bottom} Cifar-10.
    }
    \label{fig:mege:sanity_check}
\end{figure*}

\paragraph{Distance over explanations.} The procedure introduced in the previous section requires to define a distance between two explanations derived for the same sample. 
Since a feature attribution consists of ranking the features most sensitive to the predictor's decision, it seems natural to consider the  Spearman rank correlation~\cite{spearman1904measure} to compare the similarity between explanations. Several authors have provided theoretical and experimental arguments in line with this choice~\cite{ghorbani2017interpretation, adebayo2018sanity, tomsett2019sanity}. However, it is important to note that the problem of measuring similarity between explanations is still an open problem. We conduct two sanity checks: spatial correlation, and noise test on several candidates distances to ensure they could respond to the problem.  
The distances tested were built from: 1-Wasserstein distance (the Earth mover distance from~\cite{flamary2017pot}), Sørensen–Dice~\cite{dice1945} coefficient, Spearman rank correlation, SSIM~\cite{ssim2004}, and $\ell_1$ and $\ell_2$ norms.
The results of those sanity checks can be found in Appendix \autoref{ap:mege:distances}.
In line with prior work, we chose to use one minus the absolute value of the Spearman rank correlation (see~Appendix \autoref{ap:mege:distances} for more details).

\paragraph{Sanity check for explanation measures.} Our first set of experiments aims to ensure that the propose metrics approximate the desired quantities by performing a sanity check: on average, as the learning is degraded, we expect to see an overall increase in the number of inconsistent explanations.
To ensure that the metric captures these notions, we applied two different types of degradation on the predictors for each data set: weight randomizations and label shuffling.
\begin{itemize}
\setlength\itemsep{-0.2em}
\item Randomizing the weights, inspired by~\cite{adebayo2018sanity}. We gradually randomize $5$\%, $10$\% and $30$\% of the predictor layers by adding Gaussian noise. By degrading the weights learned by the network, we expect to find degraded explanations. 
\item Shuffling of labels, inspired by~\cite{neyshabur2017exploring, adebayo2018sanity} the predictors are trained on a data set with $5$\%, $10$\% and $30$\% of bad labels. By artificially breaking the relationship between the labels, we expect the explanations to lose their consistency.
\end{itemize}

The \mege~measure encodes the \representativity~of the explanations, which is related to the ability of the predictor to derive general strategies. 
Thus, the degradation of the parameters of a predictor directly affects these strategies.
Fig.~\ref{fig:mege:sanity_check} shows the correlation of the measures with the intensity of the degradation applied: \mege~and \reco~capture the degradation of the explanation and pass the sanity check.

We note that all the tested methods perform better than the random baseline (random). However, the drop in score, is not the same and some methods are more sensitive to predictor changes, such as Grad-CAM or RISE, in accordance with previous work~\cite{adebayo2018sanity, sixt2020explanations}. 
It was subsequently observed that this sensitivity seems to translate into a better \fidelity~score for the methods.
Nevertheless, it should be noted that this sanity test is a \textit{necessary but not sufficient} condition for a \representativity~and \consistency~metric.

\paragraph{The implications of the fidelity metric.} To mark the difference between the proposed measures and the \fidelity, we applied the \muf~measure from~\cite{aggregating2020} (see \autoref{sec:attributions:intro}) to the normally trained predictors and those progressively degraded. We observe that this metric does not pass the sanity check: the fidelity measure is invariant to the performancee of the predictor as well as to the quality of its explanations. For \muf, the score obtained is averaged over $10,000$ test samples, and the size of the subset is $15$\% of the image.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.70\textwidth]{assets/mege/Imagenet.pdf}
    \caption{
    \fidelity~scores on ImageNet for normally trained ResNet-50 predictors (first point on the left) as well as for progressively randomized predictors and predictors trained with switched labels.
    Even a strong degradation of the predictor does not impact the \fidelity~of the tested methods. Hence, the \fidelity~is intended to ensure that the explanations correctly reflect the underlying strategies of the model, regardless of whether these strategies are general or consistent.
    }
    \label{fig:fidelity}
\end{figure}

As shown in Fig.~\ref{fig:fidelity}, predictor degradation does not impact the \fidelity~ metric on the methods tested. 
The \fidelity~property is essential in a good explanation since it allows us to make sure that we are studying the strategies of the predictor.
However, it is not sufficient: if the explanation reflects well the strategies of the predictor, the latter may use specific and inconsistent strategies. In that, the \fidelity~measure is only a first step towards a good explanation.


\paragraph{Method selection criterion.} The \mege~and \reco~measures can be used as additional criteria for choosing an explainability method.  
As a reminder, a good method should provide explanations that are as faithful as possible and, if possible, consistent and representative.
Thus, the tested methods can be compared using the scores obtained for these measures. We note that these measures are complementary in that the fidelity score can be interpreted as a confidence bound on the other measures performed on the explanations.

\input{assets/mege/tables/imagenet}

Table~\ref{tab:metrics_imagenet} reports the \fidelity~(\muf), \consistency~(\reco) and \representativity~(\mege) scores obtained for the ResNet-50 predictors trained without degradation on ImageNet. 
We can exploit a selection criterion from the differences in scores.
First of all, we notice that the two methods obtaining a good fidelity score are RISE and Grad-CAM, they reflect well the predictor functioning. 
Their high fidelity score acts as a confidence bound on the \mege~and \reco~metrics: by correctly transcribing the functioning of the predictor, we obtain at the same time the \representativity~and the \consistency~of the explanations.
This score can then be used as a criterion to separate RISE from Grad-CAM. In view of the differences, RISE method seems preferable.

Concerning the \representativity~score, it is important to note that two methods tested here involve the element-wise product of the explanation with the input: Integrated Gradients and Gradient Input. This operation could eliminate the attribution score on a part of the image, thus reducing the distance between the two explanations. The result is an artificially better \mege~score which is in fact due to the dominance of input in the element-wise product.

It can be observed that the change of predictor has an effect on this ranking, and that a good method of explainability must be chosen according to a context: predictor and data set. However, even considering these effects, the experiments carried out suggest $3$ methods that give faithful, representative and consistent explanations: Grad-CAM, Grad-CAM++ and RISE (for more results on Cifar-10, EuroSAT and Fashion MNIST).

\paragraph{Towards predictors with better explanations.} In an attempt to find predictors that give better explanations, we extend the experience on the Cifar-10 dataset by adding a family of 1-Lipschitz networks. Indeed different works mention the Lipschitz constrained networks as particularly robust~\cite{usama2018robust, scaman2019lipschitz, pauli2020training, louislip} and have good generalizability. As a reminder, a $\f$ function is called $L$-Lipschitz, with $L \in \mathbb{R}^+$ if 
$| \f(\vx) - \f(\v{z}) | \leq L |\vx - \v{z}|$
For every pair $(\vx, \v{z}) \in \sx^2$. The smallest of these $L$ is called the Lipschitz constant of $\f$. This constant certifies that the input gradients ($\grad_{\vx} \f(\vx)$) of the function represented by the deep neural network are bounded and that this bound is known. This robustness certificate also comes with new generalization bounds that critically rely on the Lipschitz constant of the neural network~\cite{von2004distance, neyshabur2017exploring, bartlett2017spectrallynormalized}.

The predictors were trained using the Deel-Lip library~\cite{deelLip}. All the predictors, including the 1-Lipschitz, have comparable accuracy ($78 \pm 4\%$). To our knowledge, no previous work has made the link between Lipschitz networks and the chosen explainability methods.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.85\textwidth]{assets/mege/lip_vs_normal_grad.png}
    \caption{Lipschitz predictors (right column) on Cifar10.
    As explained in this paper, a clear separation between the $\Setm$ and $\Setp$ histograms is a sign of consistent explanations.
    }
    \label{fig:lipVsNormalDistrib}
\end{figure}

\input{assets/mege/tables/lip_mege}
\input{assets/mege/tables/lip_reco}

The Fig.~\ref{fig:lipVsNormalDistrib} shows the difference in $\Setp$ and $\Setm$ between ResNet and 1-Lipschitz predictors. In the left column, the results come from ResNet-18 predictors trained on Cifar-10 while the right column is dedicated to 1-Lipschitz predictors. We observe a clear improvement of the consistency and generalization of the explanations respectively as a result of better separation of the histograms and a smaller expectation of $\Setm$. SmoothGrad is the method that obtains the most consistent explanations as indicated in the table~\ref{tab:lip_reco}, in front of RISE and Saliency.

Concerning \mege, the results reported in Table~\ref{tab:lip_mege} show an improvement in the \representativity~of the explanations for the 1-Lipschitz predictors. Indeed, the \representativity~score has increased compared to the ResNet predictors for all tested methods, except Grad-CAM++. 


Like \mege, the results in Table~\ref{tab:lip_reco} show an improvement for the 1-Lipschitz predictors in the \consistency~of the explanations for all the methods tested except for Grad-CAM++, reflecting the more marked separation between the two histograms of $\Setm$ and $\Setp$ in Fig.~\ref{fig:lipVsNormalDistrib}.


In general, the experiments carried out allow us to observe a clear improvement in the quality of explanations from the 1-Lipschitz predictor.
These encouraging results show that there is a close link between the methods used and predictor architectures, as well as the usefulness of Lipschitz networks for explainability.
Furthermore, it underlines the fact that the search for new methods is not the only path to explainability: the search for predictors with better explanations is another under-exploited avenue. 

\subsection{Conclusion}

We introduced a procedure to derive two new measures to characterize important properties of a good explanation: \representativity~and~\consistency~using Algorithmic Stability inspired procedure.
We highlight the fact that current \fidelity~metrics are intended to ensure that the explanations correctly reflect the underlying strategies of the model, regardless of whether these strategies are general or consistent.
We conducted several experimental sanity checks to ensure the proposed measures capture the notion of \representativity~and \consistency.
In addition, we showed that it is possible to use these measures as criteria for selecting an explanation method in conjunction with the fidelity metric.
Finally, as a case in point, we presented a novel analysis using 1-Lipschitz networks. We  used our measures to quantify the consistency of their explanations and showed that this class of networks gives much more stable and trustworthy explanations compared to standard neural networks. The code for reproducing and computing the proposed metrics is available in \xplique.
