As mentioned in \autoref{chap:intro}, attribution methods (see Definition \ref{def:attributions}) aim to explain a specific prediction of a model. That is, for a topological input space $\sx \subset \Real^d$ and $\sy \subset \Real$ an output space, we study a \textit{predictor}\footnote{For brevity, we intentionally omit the parameters $\f(\vx; \parameters)$ of the predictor.} $\f : \sx \to \Real$, which is a measurable\footnote{All topological spaces are still equipped with their Borel $\sigma$-algebra.} function map any image $\vx \in \sx$ to a prediction $\f(\vx) \in \sy$.
The goal of attribution methods is to explain which variables of $\vx$ are most important in explaining the decision $\f(\vx)$. We will later see that the crux of the matter boils down to defining \textit{what we mean by importance}. 
Formally, an attribution method $\explainer : \mathfrak{F} \times \sx \to \Real^{|\sx|}$ is a functional that given a predictor and an input return a real for each variable in the input -- we note that in this definition, the score is not necessarily bounded. The higher the score, the more important the variable is considered; the lower the score, the more dispensable the variable may be. 
In this section, we aim not to exhaustively cover all attribution methods, but to highlight the most popular ones for vision models. We'll begin by examining gradient-based methods, followed by those utilizing internal states, and conclude with black box methods relying solely on forward calls to the models. Subsequently, we will recall the most common automatic metrics used for evaluating attributions explanations.


\subsection{Gradient-based methods.}

When exploring the importance of variables in a system or model, one of the primary approaches is through local sensitivity methods. These methods offer quantitative techniques for evaluating the impact of infinitesimal changes around the nominal value of an input. By studying how outputs vary with small shifts in inputs, these methods focus on partial derivatives concerning each input parameter. In essence, they allow us to understand how sensitive a system is to infinitesimal alterations in its initial conditions or parameters. 
In practice, those methods use the auto-differentiation framework and thus assume derivability, $\f$ need to be at least of class $C^1$ -- which is not strictly true for ReLU networks~\cite{bertoin2021numerical}.

\paragraph{Saliency.} It turns out that one of the first attribution methods for deep neural network, Saliency -- introduced in \cite{simonyan2013deep} -- is a local sensitivity method and is using absolute value of gradient as importance measure. Formally, Saliency (Sa) defined as:
$$
\explainer_{\text{Sa}}(\f, \vx) = \grad_{\vx} \f(\vx)
$$

In essence, as $\f(\cdot)$ often represent the logit value for a specific class, indicating which pixels in a small neighborhood need modification to most significantly impact the class score, whether positively or negatively\footnote{In the original paper, the authors propose to apply the $\ell_{\infty}$-norm over the channel in case of RGB images.}.

\paragraph{Gradient-Input.} Another close variant is the Gradient-Input (GI) method proposed by \cite{shrikumar2017learning}. This method involves element-wise multiplication of the input with the gradient of the target score. Formally:
$$
\explainer_{\text{GI}}(\f, \vx) = \vx \odot \grad_{\vx} \f(\vx).
$$
It was introduced to improve the sharpness of the attribution maps. A theoretical analysis conducted by~\cite{ancona2017better} showed that Gradient $\odot$ Input is equivalent to two other popular method $\epsilon$-LRP and DeepLIFT~\cite{shrikumar2017learning}, under certain conditions -- using a baseline of zero, and with all biases to zero. 

However, it turns out that gradient-based methods can be quite noisy when visualized. Several reasons have been identified for this phenomenon, and various methods have been proposed to address it. 

\paragraph{SmoothGrad.} One such method is SmoothGrad (SG)~\cite{smilkov2017smoothgrad}, which, as the name implies, aims to smooth out the noise in the gradients. SmoothGrad computes the average gradient over multiple points generated by small perturbations drawn independently and identically from an isotropic normal distribution with standard deviation $\sigma$ around the point of interest. The smoothing effect induced by this averaging process helps reduce visual noise, thereby improving the quality of explanations. Formally:
$$ 
\explainer_{\text{SG}}(\f, \vx) = \underset{\v{\delta} \sim \mathcal{N}(0, \Id\sigma)}{\E}(\nabla_{\vx} \f( \vx + \v{\delta}) ).
$$

\paragraph{Integrated gradients.} Another method aimed at mitigating the noise issue, based on axiomatic principles, is Integrated gradients (Ig)~\cite{sundararajan2017axiomatic}. Integrated gradients involve summing the gradient values along a path from a baseline state $\vx_0$ to the current value $\vx$. The baseline $\vx_0$ used is the zero vector, and the integral can be easily approximated by evaluating the gradient at a set of points evenly spaced between the baseline and the point of interest. Formally:
$$ 
\explainer_{\text{Ig}}(\f, \vx) = (\vx - \vx_0) \int_0^1 \grad_{\vx} \f(\vx_0 + \alpha(\vx - \vx_0)) \dif\alpha. 
$$

\paragraph{VarGrad, SquareGrad.} Other methods have been proposed as variants of SmoothGrad such as VarGrad (VG)~\cite{hooker2018benchmark} or SquareGrad~\cite{hooker2018benchmark} that resp. take the variance of the gradient or the squared gradient to diminish and reduce the noise. For an in depth study of those methods, we refer the reader to the excellent work of~\cite{seo2018noise}. Formally:


$$ 
\explainer_{\text{VG}}(\f, \vx) = \underset{\v{\delta} \sim \mathcal{N}(0, \Id\sigma)}{\V}(\nabla_{\vx} \f( \vx + \v{\delta}) ).
$$

$$ 
\explainer_{\text{S2}}(\f, \vx) = \underset{\v{\delta} \sim \mathcal{N}(0, \Id\sigma)}{\E}(\nabla_{\vx} \f( \vx + \v{\delta})^2 ).
$$

\paragraph{Meaningful Perturbation.} Ruth Fong's seminal work, presented in \cite{fong2017meaningful} and further elaborated in \cite{fong2019extremal}, introduces a novel perspective on attribution methods through the concept of \textit{meaningful perturbation}. Diverging from traditional gradient-based methods, this approach focuses on manipulating the input image to identify the smallest subset of pixels whose alteration -- be it through deletion, inpainting or  blurring -- most significantly affects the model's output. The core of this method lies in the optimization of a mask $\v{m}$, applied to the original image $\vx$, through a perturbation function $\v{\tau}$:

$$
\explainer_{\text{Mp}}(\f, \vx) = \argmin_{\v{m} \in \Real^{|\sx|}} \f(\v{\tau}(\vx, \v{m})) + \lambda \norm{\v{1} - \v{m}}_1
$$

Here, $\v{\tau}(\vx, \v{0}) = \vx$ implies the absence of perturbation: the image remains unchanged. The objective is to determine the minimal set of variables whose removal most dramatically decreases the model's confidence in its decision. The optimization of the mask $\v{m}$ is achieved through gradient descent. The article propose additional mechanisms designed to enhance and stabilize the optimization. These include total variation (TV) regularization to promote spatial coherence and smoothness in the mask, low-dimensional parameterization to reduce the high-frequency, and stochastic augmentation to ensure robustness against variations in input.  For full detail, we refer the reader to the excellent~\cite{fong2017meaningful}\footnote{We also note that the idea of Meta-predictor that we will found later in the manuscript, \autoref{sec:meta_pred}, originate from this article.}.

\vspace{0.3cm}


Gradient-based methods rely on the assumption of differentiability, but may not fully exploit the architectural components of models. In contrast, the upcoming methods we will discuss leverage these structural components to provide more faithful explanations.

\subsection{Internal methods.}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{assets/attributions/grad_cam.png}
    \caption{\textbf{Illustration of CAM and Grad-CAM.} Both methods utilize internal feature maps $\m{A}^{(k)}$ and an associated weight $w^{(k)}$ for each feature map (one per channel) to construct the explanation. In CAM, each channel is linked exclusively to a single logit, enabling the upsampling of the $k$-th feature map corresponding to the $k$-th logit. For standard ConvNets, Grad-CAM suggests using the average gradient per channel to determine the weights.}
    \label{fig:attributions:gradcam}
\end{figure}

\paragraph{Grad-CAM (GC).} The most popular method specifically designed for Convolutional Neural Networks (CNN) is Grad-CAM~\cite{Selvaraju_2019}. This method leverages the gradients and the $k$ feature maps $\m{A}^{(k)}(\vx)$ of the last convolutional layer concerning the input $\vx$. To generate the attribution map, we compute weights $w^{(k)}$, essentially scalars corresponding to each filter $\m{A}^{(k)}(\vx)$, where $k$ represents the number of filters (or channels) and $Z$ denotes a constant, which is the number of features in each feature map. The computation of $w^{(k)}(\f, \vx) = \frac{1}{Z} \sum_i \sum_j \frac{\partial \f(\vx)}{\partial \m{A}^{(k)}_{i,j}(\vx)}$ is crucial for this process. The Grad-CAM explanation is then obtained as:
$$\explainer_{\text{GC}}(\f, \vx) = \max(0, \sum_k w^{(k)}(\f,\vx) \m{A}^{(k)}(\vx)) .$$
Since the size of the resulting explanation depends on the dimensions (width, height) of the last feature map, the author performs a bilinear interpolation to match the dimensions of the input. 

\paragraph{Grad-CAM++.} Another close method is Grad-CAM++ (G++)~\cite{chattopadhay2018grad} which is an extension of Grad-CAM that combine the
positive partial derivatives of feature maps of a convolutional layer with a weighted special score. The weights $w^{(k)}$ associated to each feature map is computed as follows : 
$$w^{(k)}(\f, \vx) = 
    \sum_i \sum_j \Big(
    \frac{ \frac{\pd^2 \f(\vx) }{ (\pd \m{A}_{i,j}^{(k)})^2 } }
    { 2 \frac{\pd^2 \f(\vx) }{ (\pd \m{A}_{i,j}^{(k)})^2 } + \sum_i \sum_j \m{A}^{(k)}_{i,j}  \frac{\pd^3 \f(\vx) }{ (\pd \m{A}_{i,j}^{(k)})^3 } }
    \Big).
$$ 
These methods are generally very fast as they only require a forward pass, and the backward pass on the last convolutional layer is usually not computationally expensive. This efficiency often allows them to outperform Saliency in terms of computation time. Moreover, they address the issue of gradient noise by computing a coarse attribution map of the size of the last feature map (e.g., $7\times7$ for a ResNet50) and extrapolating it using a bilinear (or bicubic) interpolation, resulting in a smooth and non-noisy attribution.

However, they only work on a specific type of architecture, namely differentiable convolutional models. In the final part, we will explore the most agnostic methods, which require no assumptions about the model, and we will refer to them as black-box methods.


\subsection{Black-box methods.}

The final section explores black-box methods, which exclusively rely on model forward passes and manipulate input perturbations to infer variable importance. These methods, known for their causal influence on the model, often offer straightforward interpretability and mitigate confidence issues inherent in gradient-based approaches \cite{adebayo2018sanity,ghorbani2017interpretation,sixt2020explanations}. However, they typically demand extensive computational resources and exhibit poor scalability. These challenges form the core focus of the work presented in the thesis, \autoref{sec:attributions:sobol}, which proposes a novel, efficient, and theoretically sound black-box method based on Sobol indices.

One simple way to study model sensitivity through image perturbations is the One-At-a-Time (OAT) method, where each input variable is sequentially modified while keeping others at nominal values. This process observes the resulting effect on the output. OAT typically involves shifting one input variable while keeping others at a nominal value. The nominal value often represents the target image, while perturbations can span an entire space or adhere to a specific baseline state.

For instance, the Occlusion method involves setting each variable $x_i$ of the input to a baseline state $x_0$ and measuring the score difference to determine variable importance:

$$
\explainer_{\text{OC}}^{(i)}(\f, \vx) = \f(\vx) - \f(\vx_{[x_i = x_0]})
$$

Here, $\vx_{[x_i = x_0]}$ denotes the change to the baseline state for variable $x_i$. In practice, Occlusion operates not on a pixel-wise but on an entire patch-wise level to reduce computational costs and obtain coarser maps, which are later extrapolated similarly to Grad-CAM.

\paragraph{LIME (LI).} Another popular method, LIME~\cite{ribeiro2016lime}, involves dropping patches instead of pixels and fitting a linear model to deduce variable importance, formally:

$$
\explainer_{\text{Li}}(\f, \vx) = \argmin_{\v{w}} 
    \underset{\rv{u} \sim \s{U}(\{0,1\}^d)}{\E} 
        \big( \v{\pi}(\rv{u}) \norm{\f(\v{\tau}(\vx, \rv{u})) - \rv{u} \v{w}^\tr }_2 + \Omega(\v{w}) \big)
$$

Here, $\tau(\vx, \cdot)$ segments an image to return super-pixels according to the second argument $\rv{u}$, a randomly drawn binary vector in $\{0,1\}^d$ (e.g., $\v{\tau}(\vx, \v{1}) = \vx$). Furthermore, $\Omega(\cdot)$ represents a complexity penalty on the predictor, namely the weight $\v{w}$ of the linear model\footnote{It's worth noting that a more general formulation exists that does not specify the predictor's form, but a linear model is generally used to maintain interpretability.}. In the end, LIME identifies the weight of each super-pixel, effectively reconstructing each prediction score as an independent sum of super-pixels.

\paragraph{Shapley Values.} Shapley value is another popular that offer a principled approach to attributing the contribution of each variable to a model's prediction. Originating from cooperative game theory, Shapley values aim to fairly distribute the value generated by cooperation among players. In the context of vision, pixels (variables) are akin to players, and the prediction outcome represents the value generated by their cooperation. Given a prediction function $\f$ and a set of features $\vx = \{x_1, x_2, ..., x_d\}$, the Shapley value $\explainer_{Sh}^{(i)}$ for a feature $x_i$ is defined as the \textbf{weighted}\footnote{The Shapley value is not computed as the average marginal contribution, as is often perceived.} average marginal contribution of a feature $x_i$ across all possible feature combinations. Mathematically, it is expressed as:
$$
\explainer_{Sh}^{(i)}(\f, \vx) = \sum_{\v{u} \in \{0,1\}^d, u_i = 0} \frac{\norm{\v{u}}_1!(d - \norm{\v{u}}_1 - 1)!}{d!} \big(
(\f \circ \v{\tau})(\vx, \v{u} + \e_i) - (\f \circ \v{\tau})(\vx, \v{u})
\big)
$$
Here, $\v{u}$ represents a subset of features excluding $x_i$, and $\v{\tau}(\vx, .)$ is a function that selectively reveals or hides the pixel $i$ based on the presence of $u_i$. Direct computation of Shapley values involves evaluating the model for every possible subset of features, making it computationally expensive for high-dimensional data.

\paragraph{RISE (RI).} Another notable method, RISE (Randomized Input Sampling), was introduced by \cite{petsiuk2018rise} and represents a cutting-edge approach to black-box explainability in vision models. It builds upon the Occlusion method by simultaneously probing the model with multiple randomly removed patches to compute the conditional expectation of the score concerning patch presence.
In practical terms, RISE generates low-dimensional patches, typically $7 \times 7$, and extrapolates them to cover approximately half of the image. Once these images are generated, the conditional expectation of the score with respect to the presence of the patches is computed. Formally:
$$
 \explainer_{\text{RI}}^{(i)}(\f, \vx) = \E_{\rv{m} \sim \P_{\rv{m}}}(\f(\vx \odot \rv{m}) | \rv{m}_i = 1).
$$
Despite its effectiveness, RISE requires approximately 8000 forward passes for one explanation, posing challenges for real-time or efficient explanations.

\subsection{Metrics}

\subsubsection{Plausibility}
The initial metrics proposed in the field of explainable AI were based on the concept of plausibility, aligning with the terminology introduced by \cite{jacovi2021formalizing}. These metrics aim to measure the extent to which an attribution-based explanation correlates with a "ground truth" explanation, i.e., an ideal representation that precisely indicates the model's rationale. For instance, to explain a model's recognition of a cat, an ideal explanation might be a segmentation map highlighting the cat or the hottest point on a heatmap situated directly on the cat.

Among such approaches is the framework proposed by \cite{fong2017meaningful}, which, along with its associated library, simplifies the measure of plausibility. Other notable mentions include~\cite{poerner2018evaluating,lundberg2017unified}, which provides a benchmark for evaluating explanations against ground truth annotations.

However, these plausibility metrics have been critiqued for a significant limitation: a high plausibility score does not necessarily affirm the explanation method's effectiveness, but rather the quality of the explanation itself. To accurately evaluate explanation methods, the criteria should reflect how well an explanation reveals the true basis of the model's decision-making process, regardless of whether the model's decisions are correct or desirable. For example, if an explanation method uncovers that the model is using grass to identify a cat, it demonstrates the method's accuracy in capturing the model's focus but might be penalized by plausibility metrics. To address this, various fidelity metrics have been developed.

\subsubsection{Fidelity}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{assets/introduction/sanity_check_repro.jpg}
    \caption{\textbf{Reproduction of findings from \cite{adebayo2018sanity} using Xplique \cite{fel2022xplique}.} This experiment assesses explanation methods by progressively randomizing the weights of the model's layers, culminating in a fully randomized model on the right. Interestingly, for certain explanation methods, the generated explanations remain visually coherent and plausible, even when the underlying model is random. This observation led the authors to speculate that some explanation techniques might primarily be performing contour detection rather than revealing meaningful insights into the model's decision-making process. Such a phenomenon raises concerns about the fidelity of attribution methods: the mere coherence of an explanation image does not necessarily validate its relevance in depicting the model's operational rationale.}
    \label{fig:intro:sanity}
\end{figure}

As we have described, plausibility metrics introduce a significant issue related to confirmation bias: the fact that an explanation appears coherent and plausible does not necessarily mean it accurately reflects the underlying model processes \cite{adebayo2018sanity}. A seminal study by \cite{adebayo2018sanity} demonstrated that certain explainability methods provided similar explanations for both a randomized model and a trained model. This phenomenon, illustrated in~\autoref{fig:intro:sanity}, is problematic as some methods resembles contour detection rather than a meaningful explanation, validating a critical concern: an explanation's coherence does not guarantee its relevance to the prediction's evidential basis.

This issue has led to the development of fidelity metrics that we will now describe. They should ensure that the attribution accurately transcribe what is happening within the model, regardless of whether the outcome is aesthetically pleasing or seems plausible. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{assets/introduction/metrics.jpeg}
    \caption{\textbf{Example of Two Fidelity Metrics: Deletion \& Insertion.} These two fidelity metrics operate similarly, using a heatmap to calculate a path from the original image to a baseline image. For Deletion, this is done by starting from the original image and removing the most significant areas according to the heatmap. For Insertion, the process begins from the baseline and progressively adds pixels from the most to the least important, as indicated by the heatmap. The resulting graphs are called the Deletion Curve and the Insertion Curve, respectively. Consequently, a lower Area Under Curve (AUC) value for Deletion is desirable, while a higher AUC value for Insertion is preferred.}
    \label{fig:attribution:deletion}
\end{figure}

\paragraph{Deletion.}~\cite{petsiuk2018rise} 
The first metric is Deletion, it consists in measuring the drop in the score when the important variables are set to a baseline state. Intuitively, a sharper drop indicates that the explanation method has well-identified the important variables for the decision. The operation is repeated on the whole image until all the pixels are at a baseline state. Formally, at step $k$, with $\v{u}$ the $k$-most important variables according to an attribution method, the Deletion$^{(k)}$ score is given by:

$$
\text{Deletion}^{(k)} = \f(\vx_{[x_i = x_0, i \in \v{u}]})
$$

At the initial step, we have $\text{Deletion}^{(0)} = \f(\vx)$, representing the model's output using the original input image. For the final step, $\text{Deletion}^{(d)} = \f(\vx_0)$ corresponds to the model's output when provided with the baseline image, with $\vx \in \Real^d$. The sequence of scores obtained through this process forms a curve $\mathcal{C} = \{ (0, \text{Deletion}^{(0)}), \ldots, (k, \text{Deletion}^{(k)}), \ldots, (d, \text{Deletion}^{(d)}) \}$, capturing the impact of gradually deleting information from the input on the model's performance. The final Deletion score is derived from the Area Under the Curve (AUC) of this deletion curve, denoted as $\text{Deletion} = \text{AUC}(\mathcal{C})$. The baseline is usually a scalar (e.g, $0$) but could also be a random variable drawn from a distribution (e.g. $\rvx_0 \sim \mathcal{N}(0, 1)$).

\paragraph{Insertion.}~\cite{petsiuk2018rise}
Insertion consists in performing the inverse of Deletion, starting with an image in a baseline state and then progressively adding the most important variables. Formally, at step $k$, with $\v{u}$ the $k$-most important variables according to an attribution method, the Insertion$^{(k)}$ score is given by:
$$
\text{Insertion}^{(k)} = \f(\vx_{[x_i = x_0, i \in \complementary\v{u}]})
$$

With $\complementary\v{u}$ the complementary set of $\v{u}$ on $\{1, \ldots, d\}$. The final score is also computed using the AUC of the curve, and the baselines are similar to those of Deletion.

These two metrics, therefore, generate a sequence based on the original explanation $\explainer(\f, \vx)$. In the case of the Deletion metric, the process starts with a minimal number of pixels being masked—close to zero—and progressively moves towards the image being nearly entirely replaced by the baseline state. Conversely, the third metric, \muf, adopts a different approach. Rather than gradually obscuring the image until it reaches a baseline state, aims to maintain closeness to the original image and only drops a specified percentage of pixels (or patches) at random.

\paragraph{$\mu$Fidelity}~\cite{aggregating2020} consists in measuring the correlation between the fall of the score when variables are put at a baseline state and the importance of these variables. Formally:

$$
\mu\text{Fidelity} = \underset{\v{u} \subseteq \{1, ..., d\}}{\operatorname{Corr}}\left( \sum_{i \in \v{u}} \explainer(\f, \vx)_i  , \f(\vx) - \f(\vx_{[x_i = x_0, i \in \v{u}]})\right) ~~s.t.~~ |\v{u}| = k
$$

In various studies, the parameter $k$ is often set to 20\% of the total number of variables, and the baseline used is generally consistent with those employed by the Deletion and Insertion metrics. It is noteworthy that the concepts of Deletion and Insertion metrics have been independently rediscovered and named differently across multiple studies\footnote{Surprisingly, even though literature frequently cites \cite{petsiuk2018rise} for introducing these concepts, earlier works had already proposed similar metrics, albeit under different terminologies, such as those by \cite{samek2015evaluating}, \cite{fong2017meaningful}, and \cite{kapishnikov2019xrai}.}.

Another important metric not covered in this work is Infidelity, presented in \cite{yeh2019infidelity}, which is defined as:

$$
\text{Infidelity} = \underset{\rv{m} \sim \P_{\rv{m}}}{\E} \left(( \explainer(\f, \vx) \rv{m}^\tr - (\f(\vx) - \f(\vx - \rv{m})))^2 \right).
$$

Infidelity can be seen as a variation of the \muf~metric, where the focus shifts from measuring correlation to assessing the $\ell_2$ distance. This distance is calculated between the heatmap scores of the removed variable subsets and the change in the model's output score when the features within this set are excluded. 

These metrics serve a crucial role in ensuring that an explanation accurately captures the model's decision-making process. A pertinent question arising from this is how well an explanation holds up when deviating from a specific input, $\vx$, or, more precisely, the degree of stability of this explanation across varying inputs.

\subsubsection{Stability}

Stability is often highlighted as a desirable attribute of an explanation. \cite{alvarezmelis2018robust} were among the first to formally propose a stability metric for attribution methods, advocating that ``similar inputs should lead to similar explanations''. They conceptualized the stability metric as a measure of robustness within a local neighborhood around $\vx$, denoted by $\ball = \{\v{z} : \norm{\vx - \v{z}}_p \leq \delta \}$:

$$
\text{Stability} = \underset{\v{z} \in \ball}{\max} \frac{\norm{\explainer(\f, \vx) -\explainer(\f, \v{z})}_p}{\norm{\vx - \v{z}}_p}
$$

This formulation can be interpreted as a local Lipschitz constant, not of the function $\f$ itself, but of the explanation function $\explainer$. Essentially, it seeks to quantify how significantly an explanation changes as inputs vary slightly around our original point, underlining the \textit{explanation method's sensitivity} to input perturbations. 

More recently, alternative versions of the stability metric have been proposed, which instead of focusing on maximal deviations, consider an average measure of robustness across the local neighborhood \cite{aggregating2020}:

$$
\text{Stability}_{\text{avg}} = \int_{\ball} \norm{\explainer(\f, \vx) -\explainer(\f, \v{z})}_p \diff \v{z}
$$

These two metrics thereby assess the extent to which an explanation—or a heatmap provided to a user—remains valid within a vicinity of points. This vicinity is often defined as an $\ell_2$ ball around the focal point but has been refined and rethought in more recent works, such as in~\cite{agarwal2022rethinking}. Through these measures, we gain insights into the explanation's reliability, ensuring that the importance it provides are not just accurate for a single point but hold across a set of points around $\vx$.


\paragraph{Closing note.} All these automated metrics enable a fair and objective comparison between different attribution methods. However, as we will explore in the last part of this chapter (\autoref{sec:attributions:metapred}), dedicated to human-centric metrics, the ultimate goal of explainability is to be useful to humans for a set of specific tasks. Armed with this knowledge, we are now ready to tackle the first part of this chapter that precisely seeks to expand the set of available metrics. This effort aims to establish a metric that evaluates the explanations provided by a model (and not just the attribution method), in order to identify models that offer the best explanations.
