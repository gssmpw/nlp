\chapter{Attributions Methods}
\label{chap:attributions}

\begin{chapterabstract}
\textit{
Attributions are commonly used tools to explain neural networks. They help reveal where the model is paying attention, aiding users in determining the relevance of elements deemed important by the model. In the context of images, results are often presented as heatmaps, where hotter areas indicate greater importance, while cooler areas suggest less importance.
In this chapter, we introduce four key contributions to the field of Attributions methods. First, we propose in \autoref{sec:attributions:mege}~a new explainability metric based on algorithmic stability, aimed at identifying models with more general and consistent explanations. Next, we explore in \autoref{sec:attributions:sobol}~a new black-box attribution methods by introducing a state-of-the-art method using Sobol indices and Quasi-Monte Carlo sampling. This method is notably twice as fast as existing approaches and is grounded in a strong theoretical foundation in Sensitivity Analysis. We then show that it is possible to further extend theoretical guarantees by presenting in \autoref{sec:attributions:eva}~ the first explainability method that scales to large vision models with formal guarantees. The chapter concludes with \autoref{sec:attributions:metapred}~where an evaluation of the utility of these methods is performed, revealing that they are most useful in simple scenarios. Based on these findings, we propose several hypotheses to address this limitation, which will naturally leads to the topics of the next chapters.
}
\end{chapterabstract}

The work in this chapter has led to the publication of the following conference papers:
{\small{
\begin{itemize}
    \item \textbf{Thomas Fel}, David Vigouroux, Remi Cadene, Thomas Serre (2022). \textit{``How Good is your Explanation? Algorithmic Stability Measures to Assess the Quality of Explanations for Deep Neural Networks''.} In: \textit{Proceedings of the Winter Conference on Computer Vision} (\textcolor{confcolor}{WACV})
    
    \item \textbf{Thomas Fel}\equal, Remi Cadene\equal, Mathieu Chalvidal, Matthieu Cord, David Vigouroux, Thomas Serre, (2021). \textit{``Look at the Variance! Efficient Black-box Explanations with Sobol-based Sensitivity Analysis''.} In: \textit{Advances in Neural Information Processing Systems}  (\textcolor{confcolor}{NeurIPS})
    
    \item \textbf{Thomas Fel}\equal, Melanie Ducoffe\equal, David Vigouroux\equal, Remi Cadene, Mikael Capelle, Claire Nicodeme, Thomas Serre, (2023). \textit{``Don't Lie to Me! Robust and Efficient Explainability with Verified Perturbation Analysis''.} In: \textit{IEEE/CVF Conference on Computer Vision and Pattern Recognition}  (\textcolor{confcolor}{CVPR})

    \item Julien Colin\equal, \textbf{Thomas Fel}\equal, Remi Cad√®ne, Thomas Serre, (2021). \textit{``What I Cannot Predict, I Do Not Understand: A Human-Centered Evaluation Framework for Explainability Methods''.} In: \textit{Advances in Neural Information Processing Systems} (\textcolor{confcolor}{NeurIPS})    

\end{itemize}
}}

\minitoc
\clearpage









\section{Overview}
\label{sec:attributions:intro}
\input{chapters/attributions/intro}
\clearpage

\section{Algorithmic Stability to find model with better explanations}
\label{sec:attributions:mege}
\input{chapters/attributions/mege/main}
\clearpage

\section{Global sensitivity for Explainable AI with Sobol' indices}
\label{sec:attributions:sobol}
\input{chapters/attributions/sobol/main}
\clearpage

\section{Application to FRSign}
\label{sec:attribution:frsign}
\input{chapters/attributions/frsign}
\clearpage

\section{Guarantee for Explainable AI with Verified perturbation Analysis}
\label{sec:attributions:eva}
\input{chapters/attributions/eva/main}
\clearpage

\section{How useful are attributions method ? A Meta-predictor perspective.}
\label{sec:attributions:metapred}
\input{chapters/attributions/metapred/main}
\clearpage

\section{Conclusion}

This chapter has provided a detailed examination of attribution methods in explainability, covering gradient-based, internal, and black-box approaches. 
Initially, we explored a novel metric inspired by Algorithmic Stability to assess the quality of explanations for a given model. This investigation revealed that fidelity metrics are not enough and that robust models tend to offer more general and consistent explanations.
Subsequently, we introduced the Sobol method, an efficient black-box attribution technique grounded in global sensitivity analysis. This method identify significant pixel regions via perturbation and use quasi-Monte Carlo sampling, marking a notable advancement in computational efficiency while maintaining strong theoretical grounding in Global Sensitivity Analysis.
Additionally, we discussed Explainability with Verified Perturbation Analysis (\eva), which introduces formal guarantees to importance estimation, thereby enhancing trust in the insights derived from models.
To finish, the chapter evaluated the practical utility of attribution in real-life scenarios and highlighted a fundamental shortcoming of current methods: their tendency to falter in complex situations. This shortfall underscores a need for further development in our understanding of models.

\paragraph{Additional Remarks.} Through this research, several noteworthy observations about attribution methods were made:

\begin{itemize}
    \item \textbf{Explainable AI need to adapt GSA tools.} Global Sensitivity Analysis (GSA) is already a mature field and a fertile research area that \textit{could} significantly contribute to Explainable AI (XAI), especially in attribution methods. Our work with Sobol is merely a starting point, and the active exploration of GSA in XAI, including recent advancements using Hilbert Schmidt Independence Criterion (HSIC)~\cite{novello2022making}, promises further reductions in computational time while improving interpretability scores. The integration of kernels or novel tools~\cite{da2015global,sarazin2023new} from this research domain could potentially fuel an entire thesis. 

    \item \textbf{Attribution methods are not always consistent.} On a more practical note, after inspection of thousands of heatmaps, certain methods appear to cluster together, such as gradient-based methods (e.g., Saliency and SmoothGrad) on one hand, and black-box methods (e.g., RISE, Occlusion) on the other, with Grad-CAM and Grad-CAM++ forming another cluster. Each cluster tends to offer similar explanations. During analysis, prompting a diversified approach when examining results should be recommended, ideally combining methods from different clusters, e.g. SmoothGrad, Sobol, and Grad-CAM.

    \item \textbf{A frequency perspective on Attribution is promising.} Another observation that could be made is the significant variance in the frequency spectrum of methods -- as showcased in \autoref{fig:attribution:spectrum}. The impact of high-frequency energy on interpretability remains unclear. An initial investigation in \autoref{sec:attributions:metapred} with the complexity measure is a starting point, as well as the recent work of \cite{muzellec2023gradient}. Analyzing methods from a frequency perspective could offer valuable insights into attribution methods.

\end{itemize}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{assets/attributions/spectrum_condensed.jpg}
    \caption{\textbf{Fourier footprint of attribution methods, adapted from \cite{muzellec2023gradient}.} We show on the top row the Fourier spectrum of prediction-based attribution methods and of the gradient-based methods on the bottom row, computed with a ResNet50. The two families can be distinguished by methods but also by their signature in the Fourier domain. The former method has magnitudes largely concentrated in the low frequencies, while the latter is more spread out: it features non-trivial magnitudes almost everywhere, including in high frequencies.}
    \label{fig:attribution:spectrum}
\end{figure}

As we conclude this chapter, it's evident that attribution methods have not fully resolved the spectrum of use-cases in explainability. To move forward effectively, we propose utilizing the conclusion of the meta-prediction metric and hypothesize the reasons behind the shortcomings of attribution methods, and then suggest solutions that verify these hypotheses.


\paragraph{Hypotheses on the Shortcomings of Attribution Methods.} We posit two hypotheses for the failures of attribution methods, attributing them either to the model itself or to the explanation method -- implying that solutions may involve changing the model or the explainability method.

The first hypothesis questions the necessity of altering models. Despite the goal of explainability to elucidate any system, we could easily admit that among models achieving the same accuracy on benchmarks like ImageNet, they might vary in interpretability score (such as the \metric~score proposed). This is the idea behind \textit{Rashomon set}~\cite{xin2022exploring}: among the set of predictor with the same accuracy level $\kappa$ defined as $\{ \f \in \fspace : \P_{\rvx, \ry}(\f(\rvx) = \ry) = \kappa \}$ some of them may use strategies that are more aligned with humans (or easier to \textit{meta-predict}). This hypothesis leads us to ask, \textit{``Among high-performing models, how can we identify the most explainable ones?''} We will explore potential strategies to address this model alignment hypothesis in the \autoref{chap:alignment}.

\begin{customhypothesis}{Hypothesis 1: Model Alignment.}
\label{hyp:alignment}
Attribution methods fail in various scenarios due to the model's fault. The model might employ processes and strategies too divergent from human reasoning. Aligning the model with human understanding could make these processes more transparent.
\end{customhypothesis}


Our second hypothesis defends the notion that attribution methods are inherently limited. A commonly discussed limitation is the difference between the "\where" and the "\what"‚Äîthat is, attribution methods show \where~the model focuses but not \what~ it perceives. Future research could aim to develop methods that explain the very nature of the features influencing the model's decisions. This approach has begun to be explored, and we dedicate \autoref{chap:concepts} to this investigation, formally stating this hypothesis:

\begin{customhypothesis}{Hypothesis 2: The Need to Go Beyond Attributions.}
\label{hyp:what}
Attribution methods fail in various scenarios because they do not convey sufficient information. They only show where the model looks, leaving humans to guess the type of features seen by the model. This can lead to ambiguities and misalignment between what humans think the model uses and what it actually does.
\end{customhypothesis}

Moving forward, we will propose methods to address these two hypotheses in \autoref{chap:alignment} and \autoref{chap:concepts}, respectively.
