\newcommand{\sob}{\mathcal{S}}
\newcommand{\Vemp}{\hat{\text{V}}}
\newcommand{\perturbation}{\v{\pi}}
\newcommand{\estA}{\rm{A}}
\newcommand{\estB}{\rm{B}}
\newcommand{\estAB}{\rm{C}}

In this section, we address a challenge faced by black-box attribution methods by presenting a new approach grounded in Sensitivity Analysis and utilizing Sobol indices. These indices offer a streamlined method to not only model the individual contributions of different parts of an image but also to capture complex interactions among these parts and their impact on a neural network's prediction, as seen through variations in output.
Our method involves efficiently computing these indices for high-dimensional problems, such as those posed by images, by employing perturbation masks along with efficient estimators. This strategy effectively handles the large number of dimensions.
Crucially, we demonstrate that our proposed method achieves favorable performance on standard benchmarks for both vision and language models, while significantly reducing computational time compared to other black-box methods. Remarkably, it even surpasses the accuracy of state-of-the-art white-box methods, which rely on access to internal representations of the model.

We will begin by briefly reviewing relevant prior work, then introduce our method based on random perturbation and Sobol indices. We will propose an efficient estimator and finally, demonstrate through experiments that the Sobol attribution method not only outperforms previous methods in faithfulness but also offers faster computation and enables the discovery of intricate interactions within the model.


\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.99\linewidth]{assets/sobol/sobol.png}
    \caption{\textbf{(Left)} \textbf{Sobol Attribution Method overview.}
    Our method aims to explain the prediction of a black-box model for a given image. We first sample a set of real-valued masks $\rm{M}$ drawn from a Quasi-Monte Carlo (QMC) sequence.
    We apply these masks to the input image through a perturbation function $\perturbation(\cdot)$ (here the \textit{Inpainting} function) to form perturbed inputs $\rm{X}$ that we forward to the black box $\f$ to obtain prediction scores.
    Using the masks $\rm{M}$ and the associated prediction scores, we finally produce an explanation $\sob_{T_i}$ which characterizes the importance of each region by estimating the total order Sobol indices.
    While $\sob_{T_i}$ encompasses the effects of first and all higher-order non-linear interactions between pixel regions, we can also produce the first-order Sobol indices $\sob_i$ that reflect the importance of a region in isolation (e.g., the eyes of the cats).
    \textbf{(Right)} \textbf{Sample explanations for ResNet50V2.}
    Comparing explanations produced with $\sob_i$ and $\sob_{T_i}$ helps highlight the importance of individual image regions in isolation vs. jointly (e.g., the lynx tips are important but conditioned on the presence of the presence of an eye).
    }
    \vspace{-0.5cm}
    \label{fig:sobol}
\end{figure*}


\subsection{Background}
\label{sobol:sec:related_work}

\paragraph{Attribution methods for black-box models.} Most similar to our approach are attribution methods that can be used to explain the predictions of truly black-box models.
These methods probe a neural network's responses to perturbations over image regions and combine the resulting predictions into an influence score for each individual pixel or group of pixels.
The simplest method, ``Occlusion''~\cite{zeiler2014visualizing}, masks individual image regions -- one at a time -- with an occluding mask set to a baseline value and assigns the corresponding prediction scores to all pixels within the occluded region. Then the explanation is given by these prediction scores and can be easily interpreted.
However, occlusion fails to account for the joint (higher-order) interactions between multiple image regions. For instance, occluding two image regions -- one at a time -- may only decrease the model's prediction minimally (say a single eye or mouth component on a face)  while occluding these two  regions together may yield a substantial change in the model's prediction if these two regions interact non-linearly as is expected for a deep neural network.

This work, together with related methods such as LIME~\cite{ribeiro2016lime} and RISE~\cite{petsiuk2018rise}, addresses this problem by randomly perturbating the input image in multiple regions at a time. Obviously, perturbating multiple image locations simultaneously leads to a combinatorial explosion in the number of combinations and methods have been proposed to make these approaches more tractable.
For instance, a popular method, LIME~\cite{ribeiro2016lime}, takes superpixels as regions to perturbate instead of individual pixels.
An influence score is then computed for a set of connected pixel patches indicating how strongly a patch is correlated to the model predictions.

RISE~\cite{petsiuk2018rise} relies on Monte Carlo sampling to generate a set of binary masks, each value in the masks representing a pixel region.
By probing the model with  randomly masked versions of the input, RISE~\cite{petsiuk2018rise} produces a importance map by considering the average of the masks weighted by their associated prediction scores.
Instead of using binary masks, our method considers a continuous range of perturbations which allows for a finer exploration of the model's response.
Our method can still use the same perturbations as used in Occlusion~\cite{zeiler2014visualizing}, LIME~\cite{ribeiro2016lime} and RISE~\cite{petsiuk2018rise}, but it also enables the use of more advanced perturbation functions that take continuous inputs.

More importantly, the aforementioned methods lack a rigorous framework. Here, we introduce a theoretical framework that decomposes the influence score of each individual region between multiple orders of influence.
The first-order approximates Occlusion~\cite{zeiler2014visualizing} by considering the influence of one region at a time, while the second-order considers two regions at a time, etc. The decomposition also includes higher-orders.

\paragraph{Variance-based sensitivity analysis.} Our attribution method builds on the variance-based sensitivity analysis framework.
The approach was introduced in the 70s~\cite{cukier1973study} and reached a cornerstone with the Sobol indices~\cite{sobol1993sensitivity}.
Sobol indices are currently used in many fields (including those that are said to be safety-critical), especially for the analysis of physical phenomena~\cite{iooss2015}. More recently, connections have been successfully made between these indices and existing metrics of fairness~\cite{benesse2021fairness}.

They are used to identify the input dimensions that have the highest influence on the output of a model or a mathematical system.
Several statistical estimators to compute these indices are available~\cite{saltelli2010variance, marrel2009calculations, janon2014asymptotic, owen2013better, tarantola2006random} and have asymptotic guarantees~\cite{janon2014asymptotic, da2013efficient, tissot2012bias}.
We build on this literature by adapting these Sobol indices in the context of black-box models to compute the influence of regions of an image on the output predictions using perturbation masks.

\subsection{Sobol attribution method}
\label{sobol:sec:method}

In this work, we formulate the feature attribution problem as quantifying the contribution of a collection of $d$ real-valued variables $\v{x} = (x_1, ..., x_d)$  with respect to a model decision. Specifically, we consider a black-box decision function $\f : \sx \to \sy$ whose internal states and analytical form are unknown (for instance, $\f$ can score the probability for the input to belong to a specific class). Our goal is to quantify the importance of each feature to the decision score $\f(\vx)$, not just individually but also collectively. To capture these higher-order interactions, our method consists in estimating the Sobol indices of the variables $\vx$ by randomly perturbating them and evaluating the impact of these perturbations on the prediction of the black-box model (Fig.~\ref{fig:sobol}).

Considering variations of $\f(\vx)$ in response to meaningful perturbations of the input $\vx$ is a natural way to interpret the local behavior of the decision function around $\vx$. Several methods build on this idea, e.g., by removing one or a group of input variables~\cite{zeiler2014visualizing, ribeiro2016lime, fong2017perturbation, petsiuk2018rise, fong2019extremal} or by back-propagating the gradient to the input space through the model~\cite{simonyan2014deep, sundararajan2017axiomatic, smilkov2017smoothgrad, Selvaraju_2019}. Most of these methods use the model's internal representations and/or require computing the gradient w.r.t. the input, which makes them unusable in a black-box setting. Moreover, these methods focus on estimating the intrinsic contribution of each feature, neglecting the combinatorial components. Our method applies perturbations directly on the input in order to deal with a black-box scenario, and allows us to estimate higher-order interactions between the variables.

\paragraph{Random Perturbation}
\label{sobol:sec:perturbation_masks}

Formally, let us define a probability space $(\Omega,\sx,\P)$ of possible input perturbations of $\vx$
There are several ways to define random perturbations corresponding to different coverage of the data manifold around $\vx$. For instance, we can consider the perturbation mask operator $\perturbation: \sx \times \s{M} \to \sx$ which combines a stochastic mask $\rv{m} = (\r{m}_1, ..., \r{m}_d) \in \s{M}$ (i.e., an i.i.d sequence of real-valued random variables on $[0, 1]^d$) with the original input $\vx$. This formulation encompasses \textit{Inpainting} perturbations: $\perturbation(\vx, \rv{m}) = \vx \odot \rv{m} + (\v{1} - \rv{m}) \mu$ with $\mu \in \mathbb{R}$ a baseline value, and $\odot$ the Hadamard product. This consists in linearly varying the pixel intensities towards a baseline intensity such as a pure black with a value of zero~\cite{fong2017perturbation,ribeiro2016lime,zeiler2014visualizing,petsiuk2018rise}. Similarly, \textit{Blurring} consists of applying a blur operator with various intensities to certain regions of the image~\cite{fong2017perturbation}.
Different perturbation domains can be considered for other types of data such as textual or tabular data that we discuss further in the experimental section. In the next section, we explain how we adapt the Sobol-based sensitivity analysis using a class of perturbations to explain the predictions of a black-box model.

\paragraph{Sensitivity analysis using Sobol indices}
\label{sec:sobol_indices}

We first briefly review the classical Sobol-Hoeffding decomposition from~\cite{hoeffding1948} and introduce the Sobol indices. Let $(\r{x}_1,...,\r{x}_d)$ be independent variables and assume that $\f$ belongs to $\mathbb{L}^2(\sx,\P)$. Moreover we denote the set $\mathcal{U} =\{1, ..., d\}$, $\v{u}$ a subset of $\s{U}$, its complementary ${\complementary}\v{u}$ and $\E(\cdot)$ the expectation over the perturbation space. The Hoeffding decomposition allows us to express the function $\f$ into summands of increasing dimension, denoting $\f_{\v{u}}$ the partial contribution of variables $\rvx_{\v{u}} = (\r{x}_i)_{i \in \v{u}}$ to the score $\f(\rvx)$:

\begin{equation}
    \label{eq:anova}
    \begin{aligned}
    \f(\rvx) &= \f_{\emptyset} + \sum_i^d \f_i(\rx_i)
    + \sum_{1 \leqslant i < j \leqslant d} \f_{i,j}(\rx_i, \rx_j)
    + \cdots + \f_{1,...,d}(\rx_1, ..., \rx_d) \\
    &= \sum_{\substack{\v{u} \subseteq \mathcal{U}}} \f_{\v{u}}(\rvx_{\v{u}})
    \end{aligned}
\end{equation}

Eq.~\ref{eq:anova} consists of $2^d$ terms and is unique under the following orthogonality constraint:

\begin{equation}
    \label{eq:anova_ortho}
    \begin{aligned}
    \forall (\v{u},\v{v}) \subseteq \mathcal{U}^2 \; s.t. \;  \v{u} \neq \v{v}, \;\; \E\big(\f_{\v{u}}(\rvx_{\v{u}}) \f_{\v{v}}(\rvx_{\v{v}})\big) = 0
    \end{aligned}
\end{equation}

Furthermore, orthogonality yields the characterization $\f_{\v{u}}(\rvx) = \E(\f(\rvx)|\rvx_{\v{u}}) - \sum_{\bm{v}\subset \v{u}}\f_{\v{v}}(\rvx)$ and allows us to decompose the model variance as:
\begin{equation}
    \label{eq:var_decomposition}
    \begin{aligned}
        \V(\f(\rvx)) &= \sum_i^d \V(\f_i(\rx_i)) +
        \sum_{1 \leqslant i < j \leqslant d} \V(\f_{i,j}(\rx_i, \rx_j)) +
        ... + \V(\f_{1,...,d}(\rx_1, ..., \rx_d)) \\
        &=\sum_{\substack{\rv{u} \subseteq \mathcal{U}}} \V(\f_{\v{u}}(\rvx_{\v{u}}))
        \end{aligned}
\end{equation}
Building from Eq.~\ref{eq:var_decomposition}, it is natural to characterize the influence of any input subset $\v{u}$ as its own variance w.r.t. the total variance. This yields, after normalization by $\V(\f(\rvx))$, the general definition of Sobol indices.
\begin{definition}[Sobol indices~\cite{sobol1993sensitivity}]
\label{def:sobol_indice}
The sensitivity index $\sob_{\v{u}}$ which measures the contribution of the variable set $\rvx_{\v{u}}$ to the model response $\f(\rvx)$ in terms of fluctuation is given by:
\begin{equation}
    \label{eq:sobol_indice}
    \sob_{\v{u}}  = \frac{ \V(\f_{\v{u}}(\rvx_{\v{u}})) }{ \V(\f(\rvx)) }
    = \frac{ \V(\E(\f(\rvx) | \rvx_{\v{u}})) - \sum_{\bm{v}\subset \v{u}}\V(\E(\f(\rvx) | \rvx_{\bm{v}} ))}{ \V(\f(\rvx)) }
\end{equation}
\end{definition}
Sobol indices give a quantification of the importance of any subset of features with respect to the model decision, in the form of a normalized measure of the model output deviation from $\f(\rvx)$. Thus, Sobol indices sum to one : $\sum_{\v{u} \subseteq \mathcal{U}} \sob_{\v{u}} = 1$.

For each subset of variables $\rvx_{\v{u}}$, the associated Sobol index $\sob_{\v{u}}$ describes the proportion of the model's output variance explained by this subset. In particular, the first-order Sobol indices $\sob_i$ capture the intrinsic share of total variance explained by a particular variable, without taking into account its interactions.
Many attribution methods construct such intrinsic importance estimator. However, the framework of Sobol indices enables us to capture higher-order interactions between features. In this view, we define the Total Sobol indices.
\begin{definition}[Total Sobol indices~\cite{homma1996importance}]
\label{def:total_sobol_indice}
The total Sobol index $\sob_{T_i}$ which measures the contribution of the variable $\rx_i$ as well as its interactions of any order with any other input variables to the model output variance  is given by:
\begin{equation}
    \label{eq:sobol_total}
    \sob_{T_i}
    = \sum_{\substack{\v{u} \subseteq \mathcal{U} \\ i \in \v{u} }} \sob_{\v{u} }
    = 1 - \frac{\V_{ \rvx_{\complementary i} }(\E_{ \rx_i }(\f(\rvx) | \rvx_{\complementary i})) }{ \V(\f(\rvx))}
    = \frac{ \E_{\rvx_{\complementary i}}( \V_{\rx_i} ( \f(\rvx) | \rvx_{\complementary i} )) }{ \V(\f(\rvx)) }
\end{equation}
\end{definition}
Where $\E_{\rvx{\complementary i}}( \V_{\rx_i} ( \f(\rvx) | \rvx_{\complementary i}))$ is the expected variance that would be left if all variables but $\rx_i$ were to be fixed. $\sob_{T_i}$ is the sum of the Sobol indices for the all the possible groups of variables where $i$ appears, i.e. first and higher order interactions of variable $\rx_i$.

Since the total interaction index contains the first order index, it is natural that it is greater than or equal to the first order index. We thus note the property which can easily be deduced: $\forall i, 0 \leq \sob_i \leq \sob_{T_i} \leq 1$. We remind that naturally the score is bounded between 0 and 1 as it represents a (relative) part of the model's variance.
We will now see why these two indices and the difference between them make them relevant for the explainability of a black-box model.

These statistics quantify the intrinsic (first-order indices) and relational (total indices) impact of each variable to the model output.
A variable with a low total Sobol index is therefore not important to explain the model decision. Also, a variable has a weak interaction with other variables when $\sob_{T_i} \approx \sob_i$, while it has a strong interaction when the difference between its two indices is high $\sob_{T_i} \gg \sob_i$. A strong interaction means that the effect of one variable on the variation of the model output depends on other variables.
Thus, using Sobol indices allows to describe fine grained interactions between inputs which leads to the model decision.
We next present an efficient method to estimate these indices.

\begin{figure*}[ht]
  \centering
  \includegraphics[width=0.85\textwidth]{assets/sobol/sobol_vs_mc.png}
  \caption{\textbf{Quasi-Monte Carlo vs. Crude Monte Carlo Sampling.} Comparison of Sampling Techniques in a Two-Dimensional Unit Cube: On the left, we observe a crude Monte Carlo sampling where points are distributed according to a uniform random sequence, exhibiting clusters and gaps due to the stochastic nature of the sampling process. On the right, a quasi-Monte Carlo sampling using a Sobol sequence is displayed, demonstrating a more uniform and structured distribution of points across the space, which is characteristic of low-discrepancy sequences that aim for even coverage in the unit cube $[0,1]^2$. This illustrates the advantage of quasi-Monte Carlo methods in achieving a more evenly distributed set of sample points, which can lead to more efficient numerical integration and optimization within multidimensional domains.}
  \label{fig:sobol:qmc}
\end{figure*}

\paragraph{Efficient estimator.} As models are becoming more and more complex, the proposed estimator must take into account the computational cost of model evaluation. Many efficient estimators have been proposed in the literature~\cite{iooss2015}. In this work, we use the Jansen~\cite{jansen1999} estimator which is often considered as one of the most efficient~\cite{puy2020comprehensive}.
Jansen is typically used with a Monte Carlo sampling strategy.
We improve over Monte Carlo by using a Quasi-Monte Carlo (QMC) sampling strategy which generates low-discrepancy sample sequences allowing a faster and more stable convergence rate~\cite{gerber2015}, see Figure~\ref{fig:sobol:qmc}.
Interestingly, QMC samling allow us to add posteriori points to refine the result.
We will now describe the procedure to implement these estimators.

We start by drawing two independent matrices of size $N \times d$ of $N$ perturbation masks from a Sobol low discrepancy $LP_{\tau}$ sequences.
$N$ will be our number of designs and we recall that $d$ is our dimensions (e.g, $d=121$ for $11$ by $11$ mask).
Once the perturbation operator is applied to our input $\perturbation(\vx, \rm{M})$ with these masks, we obtain two matrices $\estA$ and $\estB$ of the same size as the perturbed inputs (i.e., partially masked images). We note $\estA_{ji}$ and $\estB_{ji}$ the elements of the matrices such that $i = 1, ..., d$ the number of variables studied and $j = 1, ..., N$ the number of samples in each matrix.
We form the new matrix $\estAB^{(i)}$ in the same way as $\estA$ except for the fact that the column corresponding to the variable $i$ is now replaced by the column of $\estB$.
We denote $f_{\emptyset} = \frac{1}{N} \sum_{j=0}^N \f(\estA_j)$ and the empirical variance $\hat{V} = \frac{1}{N-1} \sum_{j=0}^N (\f(\estA_j) - f_{\emptyset})^2 $. The empirical estimators for first ($\hat{\sob}_i$) and total order ($\hat{\sob}_{T_i}$) can be formulated as:
\vspace{-1mm}
\begin{equation}
\label{eq:jansen_estimator}
        \hat{\sob}_i = \frac
        { \Vemp - \frac{1}{2N} \sum_{j=1}^N (\f(\estB_j) - \f(\estAB_j^{(i)}))^2 }
        { \Vemp }
        \text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }
        \hat{\sob}_{T_i} = \frac
        { \frac{1}{2N} \sum_{j=1}^N ( \f(\estA_j) - \f(\estAB_j^{(i)}) )^2  }
        { \Vemp } \\
\end{equation}
Hence, to compute the set of first order and total indices, it is necessary to perform $N(d+2)$ forwards of the model. We study in section \ref{sec:sobol:efficient} how to choose a sufficient number of forwards ($N$).
To ease understanding and demonstrate that these estimators can be easily implemented, we show in Algorithm~\ref{alg:sobol:sti} a minimal pythonic implementation of the total order estimator that outputs $\hat{\sob}_{T_i}$ indices. The input $Y$ contains the prediction scores of the $N \times (d+2)$ forwards. The scores are ordered following the same QMC sampling ordering of their associated mask. The output \texttt{STis} contains $d$ importance scores, one for each dimension of the mask. In the case of images, we obtain our final explanation map by applying a bilinear upsampling to match the dimensions of the input image.

\begin{figure}
\begin{lstlisting}
def total_order_estimator(Y, N=32, d=11*11):
    fA, fB = Y[:N], Y[N:N*2]
    fC = [Y[N*2+N*i:N*2+N*(i+1)] for i in range(d)]
    f0 = mean(fA)
    V = sum([(val - f0)**2 for val in fA]) / (len(fA) - 1)
    STis = [sum((fA - fC[i])**2) / (2 * N) / V for i in range(d)]
    return STis
\end{lstlisting}
\caption{\textbf{Pythonic implementation of the estimator.} We just need to have access to the output of the model for the corresponding $\rm{A},\rm{B}$ and $\rm{C}$ matrix. Using only the output, we can efficiently estimate the total Sobol indices $\sob_T$.}
\label{alg:sobol:sti}
\end{figure}

\paragraph{Signed estimator}
Although the proposed Sobol-based attribution method allows us to determine the impact of any variables for a given prediction and thus to identify diagnostic ones, it lacks the ability to highlight the type of contributions made, whether positive or negative. Simple methods such as ``Occlusion'' typically include this information. Hence, we propose a variant that combines the importance scores of the total Sobol indices with the sign of the occlusion. We compute the difference in score between the prediction on the original input $\vx$ and a partial version $\vx_{[x_i = 0]}$ with the variable $x_i$ occluded. Intuitively, this provides an estimate of the direction of the variations generated by the variables studied with respect to a reference state.
\vspace{-3mm}\begin{equation}
    \label{eq:sobol_signed}
    \hat{\sob}_{T_i}^{\Delta} = \hat{\sob}_{T_i} \times \text{sign}( \f(\vx) - \f(\vx_{[x_i = 0]})
\end{equation}

\subsection{Experiments}

To evaluate the benefits and the reliability of the Sobol attribution method, we performed multiple systematic experiments on vision and natural language models using common explainability metrics.

For our vision experiments, we compared the plausibility of the explanations produced on the Pointing Game~\cite{zhang2018top} benchmark. We evaluate the fidelity of our explanations using the Deletion metric for $4$ representative models commonly used in explainability studies: ResNet50V2~\cite{he2016deep} , VGG16~\cite{simonyan2014deep}, EfficientNet~\cite{tan2019efficientnet} and MobileNetV2~\cite{sandler2018mobilenetv2} trained on  ILSVRC-2012~\cite{imagenet_cvpr09}.
In addition, we also compared the speed of convergence of the proposed estimator with that of the leading approach, RISE~\cite{petsiuk2018rise}, on the same models.
For our NLP experiments, we fine-tuned a Bert model and trained a bi-LSTM on the IMDB sentiment analysis dataset~\cite{maas2011} before comparing  fidelity scores using word-deletion for representative methods.

Throughout this work, explanations were generated using the Sobol total estimator $\hat{\sob}_{T_i}$ on the target class output.
In the supplementary material, we demonstrate the effectiveness of modeling higher-order interactions between image regions by comparing $\hat{\sob}_{T_i}$ against $\hat{\sob}_{i}$ which only models the main effects.
For the experiments involving images, the masks were generated at a resolution of $d' = 11 \times 11$ pixels, then upsampled with a nearest-neighbor interpolation method before being applied with the \textit{Inpainting} perturbation function. Finally,  $N$ was set to $32$ which is equivalent to $3,936$ forward passes, half the number of forward used by RISE (see Section~\ref{sec:sobol:efficient} for details).
For $\smash{\hat{\sob}_{T_i}^\Delta}$, an occlusion using the same resolution as the masks was used to sign $\smash{\hat{\sob}_{T_i}}$, with zero as baseline.
For RISE~\cite{petsiuk2018rise}, we have followed the recommendations of the original paper with $8,000$ forward passes for all models.


\begin{table*}[t]
\centering
\begin{tabular}{c lccc}
\toprule
 & & Pointing Game & Deletion & \textit{Time (s)} \\
\midrule
& Baseline Center &  27.8 & 0.235 & - \\
\midrule
\multirow{7}{*}{\rotatebox[origin=c]{90}{{\footnotesize White box}}}
& Saliency~\cite{simonyan2014deep} & 37.7 & 0.174 & 0.031 \\
& Guided-Backprop.~\cite{springenberg2014striving} & 39.1 & 0.142 & 0.051 \\
& MWP~\cite{zhang2018top}  &  39.8 & - & 0.039 \\
& cMWP~\cite{zhang2018top} &  49.7 & - & 0.040 \\
& Integ.-Grad. ~\cite{shrikumar2017learning} &  49.7 & \underline{0.123} & 0.040 \\
& GradCAM~\cite{Selvaraju_2019} & \underline{54.2} & 0.141 & 0.015 \\
& ExtremalPerturbation~\cite{fong2019extremal} & 51.5 & - & 26.48\\
\midrule
\multirow{3}{*}{\rotatebox[origin=c]{90}{{\footnotesize Black box}}}
& Occlusion & 35.6 & 0.350 & 1.134 \\
& RISE~\cite{petsiuk2018rise} & 50.8 & 0.127 & 13.19 \\
& Sobol ($\hat{\sob}_{T_i}$) (ours) & \textbf{54.6} & \textbf{0.121} & 6.381 \\
\bottomrule
\end{tabular}
\caption{\textbf{Pointing game.} Accuracy over the full test set and a subset of difficult images (defined in~\cite{zhang2018top}).
The first and second best results are  \textbf{bolded} and \underline{underlined}.
Results are based on PyTorch re-implementations using the TorchRay package.
The reported execution time is an average over 100 runs on ResNet50 using an Nvidia Tesla P100 on Google Colab and a batch size of 64. Lower execution time can be reached with higher batch size. \vspace{-3mm}
}\label{tab:pointing_game}
\end{table*}

\subsubsection{Pointing game}

Different evaluation methods have been proposed to compare attribution methods and their explanations~\cite{samek2016evaluating, hooker2018benchmark, aggregating2020, fel2020representativity}.
The first common approach consists in measuring the plausibility of an explanation as the correlation between attribution maps and human-provided semantic annotations. Here, we focused on the Pointing Game used in~\cite{zhang2018top, fong2017perturbation, fong2019extremal, petsiuk2018rise}. For each attribution method, we compute a contribution score for each pixel of a given class of objects, e.g., bike or car.
We then calculated the percentage of times the pixel with the highest score is included in the bounding box surrounding the object of interest. In this benchmark, a good attribution method should point to the most important evidence of the object appearance in accordance with a human user.

In Table~\ref{tab:pointing_game}, a report results for the Pascal VOC~\cite{everingham2010pascal} and MS COCO~\cite{coco} datasets using VGG16~\cite{simonyan2014deep} and ResNet50\cite{he2016deep}.
In the last column we report the computation times for each method averaged over $100$ MS COCO samples for the ResNet50 model.
We subdivided explanation methods into two categories: white-box methods which require the use of backpropagation, such as Gradient~\cite{zeiler2014visualizing} and Extremal Perturbation~\cite{fong2019extremal}, and/or access to the internal states of the model, such as GradCAM~\cite{Selvaraju_2019} versus black-box methods such as Occlusion~\cite{zeiler2014visualizing}, RISE~\cite{petsiuk2018rise}, or the proposed Sobol method $\sob_{T_i}$ which only require the final model predictions. The proposed method outperforms RISE~\cite{petsiuk2018rise} on all of the tested cases, while reducing the number of forward passes by half.
Surprisingly, white-box methods do not always lead to higher scores, and indeed $\hat{\sob}_{T_i}^\Delta$ is the leading method for Pascal VOC / VGG16 and our two estimators $\hat{\sob}_{T_i}^\Delta, \hat{\sob}_{T_i}$ prevail on COCO / VGG16.
Also note that our signed version of the estimator obtains higher scores overall.
This might be due to the fact that images from VOC and COCO often feature several types of objects. Thus, the maximum variance in the output is not always induced by the object of interest but can be due to the masking of another object in the image. This result suggests that our signed version $\hat{\sob}_{T_i}^\Delta$ should be used on multi-label datasets, while $\hat{\sob}_{T_i}$ should be used on multi-class datasets. We indeed confirm this in the next set of experiments on a multi-class dataset.

\subsubsection{Fidelity}
There is a broad consensus that measuring the plausibility of an explanation alone is insufficient~\cite{adebayo2018sanity, ghorbani2017interpretation}. Indeed, if an explanation is used to make a critical decision, users expect an explanation to reflect the true underlying decision process of the model and not just a consensus with humans. Failures to do so could have disastrous consequences.
A first major limitation of current evaluation methods based on human-provided groundtruth such as the pointing game is that they do not work when a model prediction is wrong. In this case, an explanation method can be penalized for not pointing to the correct evidence even though explaining prediction errors is a critical use case for explanation methods. Another limitation of these evaluation methods is that they make the implicit assumption that the models should be relying on the same image regions than humans for recognition~\cite{ullman2016atoms, linsley2018learning}, which is likely to be an incorrect assumption. We thus use the fidelity metric as a complementary type of evaluation. This metric assumes that the more faithful an explanation is, the quicker the prediction score should drop when pixels that are considered important are reset to a baseline value (e.g., gray values).

In Table~\ref{tab:deletion}, we report results for the Deletion Metric~\cite{petsiuk2018rise} (or $1 - AOPC$~\cite{samek2016evaluating}) for 4 different pre-trained models: ResNet50~\cite{he2016deep} , VGG16~\cite{simonyan2014deep}, EfficientNet~\cite{tan2019efficientnet} and MobileNet~\cite{sandler2018mobilenetv2} on 2,000 images sampled from the ImageNet validation set. TensorFlow~\cite{tensorflow2015} and the Keras~\cite{chollet2015keras} API were used to run the models.
Several baseline values can be used~\cite{sturmfels2020visualizing}, but we chose the standard approach with gray values.
We observe that the proposed Sobol $\hat{\sob}{T_i}$ is the most faithful black-box methods with the lowest deletion scores across all models.
Overall $\hat{\sob}_{T_i}$ is able to match the scores of the most faithful white-box method, namely Integrated Gradients~\cite{sundararajan2017axiomatic}, and gets the lowest score on ResNet50V2 with 0.121 against 0.123 (lower is better).
We also report that our signed version $\hat{\sob}_{T_i}^\Delta$ is less faithful that the standard Sobol $\hat{\sob}{T_i}$. This can be explained by the fact that ImageNet images contains only one object and therefore the main variance area generally coincides with the class to be explained.
This confirms our observation on the previous pointing game benchmark that $\hat{\sob}{T_i}$ should be preferred in  a multi-class setup and $\hat{\sob}_{T_i}^\Delta$ in a multi-label setup.


\begin{table*}[t]
\vspace{10mm}
\centering
\scalebox{0.85}{\begin{tabular}{c lcccc}
\toprule
 & Method & \textit{ResNet50V2} & \textit{VGG16} & \textit{EfficientNet} & \textit{MobileNetV2} \\
\midrule
& Baseline Random (ours) & 0.235 & 0.168 & 0.124 & 0.137 \\
\midrule
\multirow{7}{*}{\rotatebox[origin=c]{90}{{\footnotesize White box}}}
& Saliency~\cite{simonyan2014deep} & 0.174 & 0.134 & 0.105 & 0.125 \\
& Guided-Backprop.~\cite{springenberg2014striving} & 0.142 & 0.138 & 0.105 & 0.102 \\
& DeconvNet~\cite{zeiler2014visualizing} & 0.159 & 0.146 & 0.105 & 0.111 \\
& Grad.-Input~\cite{shrikumar2017learning} & 0.140 & \underline{0.096} & \underline{0.093} & 0.103 \\
& Integ.-Grad.~\cite{sundararajan2017axiomatic} & \textbf{0.123} & \textbf{0.095} & \textbf{0.091} & \textbf{0.093} \\
& SmoothGrad~\cite{smilkov2017smoothgrad} & \underline{0.130} & 0.106 & 0.094 & \underline{0.098} \\
& GradCAM~\cite{Selvaraju_2019} & 0.141 & 0.118 & 0.130 & 0.122 \\

\midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{{\footnotesize Black box}}}
& Occlusion~\cite{zeiler2014visualizing} & 0.350 & 0.357 & 0.252 & 0.357 \\
& RISE~\cite{petsiuk2018rise} & \underline{0.127} & 0.121 & \underline{0.119} & \underline{0.114} \\
& Sobol ($\hat{\sob}_{T_i}$) (ours) & \textbf{0.121} & \textbf{0.109} & \textbf{0.104} & \textbf{0.107} \\
& Sobol signed ($\hat{\sob}^{\Delta}_{T_i}$) (ours) & 0.145 & \underline{0.114} & 0.147 & 0.141 \\
\bottomrule
\end{tabular}}
\vspace{0mm}\caption{\textbf{Deletion} scores obtained on 2,000 ImageNet validation set images. Lower is better.
Random consists in removing  pixels at each step at random.
The first and second best results are \textbf{bolded} and \underline{underlined}. \vspace{-0.5cm}
}\label{tab:deletion}
\end{table*}
Another metric called Insertion has been proposed by the authors of RISE~\cite{petsiuk2018rise}.
Instead of deleting pixels in the original image like with Deletion, Insertion consists in adding pixels on a baseline image, e.g. one gray image, starting with pixels that are associated with the highest importance scores for a given explanation method.
An issue with Insertion is that the score computed along the insertion path is highly influenced by the first inserted pixels which contributes disproportionately. A good score on this metric therefore requires exploring a region very far from the original image and closer to the baseline. For this reason, we rather preferred to focus our study on Deletion than Insertion. However, we also report results on Insertion in the supplementary material using the same hyperparameters as used in Deletion.

\subsubsection{Efficiency}
\label{sec:sobol:efficient}

The black-box methods presented so far compete with white-box methods that do not require access to the internal representation of the model at the cost of a large number of forward passes, e.g., around $8,000$ for RISE~\cite{petsiuk2018rise}. This weakness leads us to take a more serious look at the performance of the proposed method.
It seems critical for the deployment of black-box methods to lower the amount of compute required to produce correct explanations.
We describe an experiment to show that beyond producing higher quality explanations, our estimator converges quickly.
We first generate an explanation with a high number of forward passes that is large enough to reach convergence, e.g. $10,000$ forward passes. Then we compare this explanation that ``converged'' to other explanations obtained with lower numbers of forward passes. It allows us to measure the stability and rate of convergence towards this explanation that ``converged'', but more practically to find the proper trade-off between the amount of compute and the quality of explanations.
This procedure requires defining a measure of similarity between two explanations.
Since the proper interpretation method is to rank the features most sensitive to the model's decision, it seems natural to consider the  Spearman rank correlation~\cite{spearman1904measure} to compare the similarity between explanations (see the Appendix \autoref{ap:mege:distances}). Moreover, prior work has provided theoretical and experimental arguments in line with this choice~\cite{ ghorbani2017interpretation, adebayo2018sanity, tomsett2019sanity}.

\begin{figure*}
  \centering
    \includegraphics[width=0.98\linewidth]{assets/sobol/efficient.png}
  \caption{
  Comparison between the rate and stability of convergence of Sobol $\hat{\sob}_{T_i}$ and RISE~\cite{petsiuk2018rise}.
  A high Spearman correlation rank corresponds to producing an explanation that is similar to the explanation that ``converged'', i.e. with $10,000$ forward passes.
  We report mean and variance computed over $500$ images from the ImageNet dataset using EfficientNet.
  } \label{fig:convergence_efficient}
\end{figure*}

In Fig.~\ref{fig:convergence_efficient}, we compare the proposed Sobol attribution method $\hat{\sob}_{T_i}$ against RISE~\cite{petsiuk2018rise}, which is the current state-of-the-art for black-box methods.
We use their respective gold explanation generated after $10,000$ forwards. To allow a fair comparison, both methods use masks generated in $7\times7$ dimensions (as recommended by RISE~\cite{petsiuk2018rise}).
We report the average results and variance over $500$ images from the ImageNet validation set using EfficientNet, a convolutional neural network optimized for fast forward computing times.
We observe that our method exhibit higher convergence rate by getting higher Spearman's rank correlation of 0.8 after only 1,000 forwards against 0.65 for RISE, and consistently obtain higher scores until reaching 0.97 with 7,000 forwards against 0.73 for RISE. Additionally, we observe that Sobol has a more stable convergence by getting an overall lower variance than RISE. This implies that the number of forward passes used in Sobol can be greatly reduced to accommodate computational resources constraints compared to RISE. Indeed, RISE proposes to use $8,000$ forwards, but our method is faster and reaches better results with half the number of passes.
Finally, we perform an ablation study of Sobol to show the impact of lowering the number of forwards on the Deletion benchmark. We report competitive scores with 16 times fewer number of forwards than RISE by reaching 0.151 in Deletion score with 492 forwards. For reference, Sobol was reaching state-of-the-art results of 0.121 with 3,936 forwards.

\paragraph{Word deletion}

\begin{table*}[t]
\centering
\scalebox{0.95}{\begin{tabular}{l ccccccc}
\toprule
 & Saliency & Grad-Input & SmoothGrad & Integ-Grad & Occlusion & $\hat{\sob}_{T_i}$ & $\hat{\sob}^{\Delta}_{T_i}$  \\
\midrule
BERT & 0.684 & 0.682 & 0.682 & 0.689&  \textbf{0.531} & 0.662& \underline{0.598}   \\
LSTM & 0.541 & 0.529 & 0.541 & 0.538 & \textbf{0.440}& 0.523 & \underline{0.461} \\
\bottomrule
\end{tabular}}
\caption{\textbf{Word deletion} scores, obtained on 1,000 sentences. Delete up to 20 words per sentence accordingly to their relevance and track the impact on the classification performance. Lower is better.
The first and second best results are \textbf{bolded} and \underline{underlined}. \vspace{-0.2cm}
}\label{tab:word_deletion}
\vspace{-0.5cm}
\end{table*}

For NLP,  black-box methods require the use of perturbations that can be applied to the space of characters, words or sentences. For instance, a common perturbation consists in simply removing one word of the sentence to be explained. Therefore, the \textit{Inpainting} perturbation that we used with Sobol to reduce the intensity of pixels in a continuous manner cannot be directly applied in this context. Instead, we adapt it by binarizing the masks such that $\perturbation(\vx, \rm{M}) = \vx \odot \lceil\rm{M} - 0.5\rceil $, i.e., if the value is greater than 0.5 the word is kept, otherwise it is removed. We then verify that our Sobol method can be used to identify words that support a specific decision of a text classifier. Inspired by previous work~\cite{arras2017relevant, arras2017explaining, bach2015pixel}, we introduce an experimental benchmark on the IMDB Review dataset~\cite{imdb2011}. It is similar to the previous Deletion benchmark for images in that it focuses on assessing the faithfulness of the explanation and does not require specific human annotations.
More precisely, we first trained two models: a bi-LSTM from scratch and a BERT model fine-tuned for the task. We generate explanations on $1,000$ sentences from the validation dataset. An explanation associates an importance score to each word. Similar to Deletion, we use these scores to successively remove the most relevant word of the sentence and measure the corresponding drop in the prediction score of the model.

In Table~\ref{tab:word_deletion}, we report results for explanation methods that are commonly used in NLP.
Both our two Sobol methods have a better faithfulness than all the tested gradient-based white-box methods including Saliency, Grad-Input, SmoothGrad, and Integr-Grad. For instance, Sobol $\hat{\sob}^{\Delta}_{T_i}$ even reaches a low Word deletion scores of 0.461 (lower is better) for the bi-LSTM compared to 0.529 for the best white-box approach.
However, the proposed methods are only the second and third most faithful methods. Occlusion (often called Omit-1 in NLP) reaches the lowest score of 0.440 for LSTM and 0.531 for BERT, against 0.461 and 0.598 for Sobol $\hat{\sob}^{\Delta}_{T_i}$. This is due to the fact that the default distribution of masks used in Sobol is centered around $0.5$, which corresponds to removing on average half of the words as opposed to a single word for Occlusion.
In IMDB, this causes the frequent removal of critical words that support the model decision.
Indeed, we report comparable results with Occlusion (e.g., $0.527$ for BERT) for a lower threshold of $0.05$ to remove far fewer words. Since Sobol can model higher-order interactions between words, we believe that it could successfully be used for NLP tasks that are more complex than sentiment classification on IMDB.

\subsection{Conclusion}

We have presented a novel explainability method to study and understand the predictions of a black-box model. This new approach tackle important challenges, namely the efficiency of current black-box method, by proposing an efficient method grounded within the theoretical framework of sensitivity analysis using Sobol indices.
A non-trivial contribution of this work  was to make the approach tractable and efficient for high-dimensional data such as images. For this purpose, we have introduced a method using perturbation masks coupled, a Quasi-Monte Carlo sampling coupled with efficient estimators from the sensitivity analysis literature. One additional benefit of the approach is that it provides a way to study the importance of not just the main effects of input variables but also higher-order interactions between them.
We showed that our method can be efficiently used to explain the decisions of image classifiers.
It reaches performance on par with or better than the current best black-box methods while being twice as fast. It even reaches comparable results to the best white-box methods without requiring access to internal states.
We also showed that our method could be applied to language models and reported initial competitive results, and we hope that further links will be made with the field of sensitivity analysis and Attribution methods.
