\newcommand{\lowerf}{\f_{\text{min}}}
\newcommand{\upperf}{\f_{\text{max}}}
\newcommand{\evaEmp}{\eva\textsubscript{emp}}
\newcommand{\evaH}{\eva\textsubscript{hybrid}}
\newcommand{\adv}{\textit{adversarial overlap}}
\newcommand{\AO}{\textit{AO}}
\newcommand{\AOup}{\overline{\textit{AO}}}
\newcommand{\AOemp}{\hat{\textit{AO}}}
\newcommand{\Adv}{\textit{Adversarial overlap}}

\newcommand{\ballu}{\ball_{\bm{u}}}

\newcommand{\rsr}{{Robustness\text{-}S\textsubscript{r}}}

In this section, we tackle the challenge of generating attributions maps with strong formal guarantee. 

We first remark that among the  plethora of attribution methods have recently been developed to explain deep neural networks, many methods use different classes of perturbations (e.g, occlusion, blurring, masking, etc.) to estimate the importance of individual image pixels to drive a model's decision.
Nevertheless, the space of possible perturbations is vast and current attribution methods typically require significant computation time to accurately sample the space in order to achieve high-quality explanations. To say it otherwise, the actual methods cannot ``scan'' the entire space of perturbation, and the ability to do so would enable us to derive strong guarantee.

In this work, we introduce EVA (Explaining using Verified Perturbation Analysis) -- the first explainability method which comes with guarantees that an entire set of possible perturbations has been exhaustively searched. We leverage recent progress in verified perturbation analysis methods to directly propagate bounds through a neural network to exhaustively probe a -- potentially infinite-size --  set of perturbations in a single forward pass. Our approach takes advantage of the beneficial properties of verified perturbation analysis, i.e., time efficiency and guaranteed complete -- sampling agnostic -- coverage of the perturbation space -- to identify image pixels that drive a model's decision. 
We  evaluate EVA systematically and demonstrate state-of-the-art results on multiple benchmarks.

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.9\textwidth]{assets/eva/big_picture.png}
  \caption{
  \textbf{Manifold exploration of current attribution methods.}
  Current methods assign an importance score to individual pixels using perturbations around a given input image $\vx$. Saliency~\cite{simonyan2013deep} uses infinitesimal perturbations around $\vx$, Occlusion~\cite{zeiler2013visualizing} switches individual pixel intensities on/off. More recent approaches~\cite{ribeiro2016lime, lundberg2017unified, petsiuk2018rise, fel2021sobol, novello2022making} use (Quasi-) random sampling methods in specific perturbation spaces (occlusion of segments of pixels, blurring, ...). However, the choice of the perturbation space undoubtedly biases the results -- potentially even introducing serious artifacts~\cite{sturmfels2020visualizing,hsieh2020evaluations,haug2021baselines,kindermans2019reliability}.
  We propose to use verified perturbation analysis to efficiently perform a complete coverage of a perturbation space around $\vx$ to produce reliable and faithful explanations.
  }
  \label{fig:eva:big_picture}
\end{figure}

\subsection{Background}

The applications of Attributions methods these methods are multiple -- from helping to improve or debug their decisions to helping instill confidence in the reliability of their decisions~\cite{doshivelez2017rigorous}, as explained in our  \autoref{sec:intro:motivation}. 
Unfortunately, a severe limitation of these approaches is that they are subject to a confirmation bias: while they appear to offer useful explanations to a human experimenter, they may produce incorrect explanations~\cite{adebayo2018sanity, ghorbani2017interpretation, slack2021counterfactual}.
In other words, just because the explanations make sense to humans does not mean that they actually convey what is actually happening within the model.
Therefore, the community is actively seeking for better benchmarks involving humans~\cite{hsieh2020evaluations,nguyen2021effectiveness,fel2021cannot,kim2021hive}.

In the meantime, it has been shown that some of our current and commonly used benchmarks are biased and that explainability methods reflect these biases -- ultimately providing the wrong explanation for the behavior of the model~\cite{sturmfels2020visualizing,hsieh2020evaluations,hase2021out}.
For example, some of the current fidelity metrics \cite{petsiuk2018rise, aggregating2020,jacovi2020towards,hedstrom2022quantus,fel2022xplique} mask one or a few of the input variables (with a fixed value such as a gray mask) in order to assess how much they contribute to the output of the system. Trivially, if these variables are already set to the mask value in a given image (e.g., gray), masking these variables will not yield any effect on the model's output and the importance of these variables is poised to be underestimated. 
Finally, these methods rely on sampling a space of perturbations that is far too vast to be fully explored -- e.g., LIME on a image divided in $64$ segments image would need more than $10^{19}$ samples to test all possible perturbations. 
As a result, current attribution methods may be subject to bias and are potentially not entirely reliable.

\paragraph{Explainability through the lens of Robustness.} To try to address the aforementioned limitations, several groups~\cite{ignatiev2019abduction, ignatiev2019relating, slack2021reliable, hsieh2020evaluations, boopathy2020proper, lin2019explanations, fel2020representativity} have focused on the development of a new set of robustness-based evaluation metrics for trustworthy explanations. 
These new metrics are in contrast with the previous ones, which consisted in removing the pixels considered important in an explanation by substituting them with a fixed baseline -- which inevitably introduces bias and artifacts~\cite{hsieh2020evaluations,sturmfels2020visualizing,haug2021baselines,kindermans2019reliability,hase2021out}. 
Key to these new metrics is the assumption that when the important pixels are in their nominal (fixed) state, then perturbations applied to the complementary pixels -- deemed unimportant -- should not affect the model's decision to any great extent. The corollary that follows is that perturbations limited to the pixels considered important should easily influence the model's decision~\cite{lin2019explanations,hsieh2020evaluations}.
Going further along the path of robustness, abductive reasoning was used in~\cite{ignatiev2019abduction} to compute optimal subsets with guarantees.  The challenge consists  in looking for the subset with the smallest possible  cardinality -- to guarantee the decision of the model. This work constituted one of the early successes of formal methods for explainability, but the approach was limited to low-dimensional problems and shallow neural networks. It was later extended to relax the subset minimum explanation by either providing multiple explanations, aggregating pixels in bundles~\cite{bassan2022towards} or by using local surrogates~\cite{boumazouza2021asteryx}.

Some heuristics-oriented works also propose to optimize these new robustness based criteria and design new methods using a generative model~\cite{o2020generative} or adversarial attacks~\cite{hsieh2020evaluations}.
The latter approach requires searching for the existence or lack of an adversarial example for a multitude of $\ell_p$ balls around the input of interest. As a result, the induced computational cost is quite high as the authors used more than $50000$  computations of adversarial examples to generate a single explanation. 

More importantly, a failure to find an adversarial perturbation for a given radius does not guarantee that none exists. In fact, it is not uncommon for adversarial attacks to fail to converge --  or fail to find an adversarial example -- which will result in a failure to output an importance score.
Our method addresses these issues while drastically reducing the computation cost.
An added benefit of our approach is that verified perturbation analysis provides additional guarantees and hence opens the doors of certification which is a necessity for safety-critical applications.

\paragraph{Verified Perturbation Analysis.} This growing field of research focuses on the development of methods that outer-approximate neural network outputs given some input perturbations. 
Simply put, for a given input $\vx$ and a bounded perturbation $\v{\delta}$, verification methods yield minimum $\lowerf(\vx)$ and maximum $\upperf(\vx)$ bounds on the output of a model. Formally $\forall~ \v{\delta} ~s.t~ ||\v{\delta}||_p \leq \varepsilon$:

$$
\lowerf(\vx) \leq \pred(\vx + \v{\delta}) \leq \upperf(\vx). 
$$
This allows us to explore the whole perturbation space without having to explicitly sample points in that space.

Early works focused on computing reachable lower and upper bounds based on satisfiability modulo theories~\cite{katz2017reluplex, ehlers2017formal}, and mixed-integer linear programming problems~\cite{tjeng2017verifying}. While these early results were encouraging, the proposed methods struggled even for small networks and image datasets. More recent work has led to the independent development of methods for computing looser certified lower and upper bounds more efficiently thanks to convex linear relaxations either in the primal or dual space~\cite{salman2019convex}. 
While looser, those bounds remain tight enough to yield non-ubiquitous robustness properties on medium size neural networks. CROWN (hereafter called Backward) uses Linear Relaxation-based Perturbation Analysis (LiRPA) and achieves the tightest bound for efficient single neuron linear relaxation~\cite{zhang2018efficient, singh2019abstract, wang2021beta}. 
In addition, linear relaxation methods offer a wide range of possibilities with a vast trade-off between ``tigthness'' of the bounds and efficiency. 
These methods form two broad classes: `forward' methods which propagate constant bounds (more generally affine relaxations from the input to the output of the network) also called Interval Bound Propagation (IBP, Forward, IBP+Forward) vs. `backward' methods which bound the output of the network by affine relaxations given the internal layers of the network, starting from the output to the input. Note that these methods can be combined, e.g. (CROWN + IBP + Forward).
For a thorough description of the LiRPA framework and theoretical analysis of the worst-case complexities of each variant, see~\cite{xu2020automatic}.
In this work, we remain purposefully agnostic to the verification method used and opt for the most accurate LiRPA method applicable to the predictor. Our approach is based on the formal verification framework DecoMon, based on Keras~\cite{ducoffe2021decomon}.


In this work, we propose to address this limitation by introducing \eva~(Explaining using Verified perturbation Analysis), a new explainability method based on robustness analysis. Verified perturbation analysis is a rapidly growing toolkit of methods to derive bounds on the outputs of neural networks in the presence of input perturbations. In contrast to current attributions methods based on gradient estimation or sampling, verified perturbation analysis allows the full exploration of the perturbation space, see Fig.~\ref{fig:eva:big_picture}. We use a tractable certified upper bound of robustness confidence to derive a new estimator to help quantify the importance of input variables (i.e., those that matter the most). That is, the variables most likely to change the predictor's decision.


\subsection{Explainability with Verified Perturbation Analysis}
\label{sec:eva:method}

\paragraph{Notation.} We still consider a standard supervised machine-learning classification setting with input space $\mathcal{X} \subseteq \mathbb{R}^d$, an output space $\mathcal{Y} \subseteq \mathbb{R}^c$, and a predictor function $\pred : \mathcal{X} \to \mathcal{Y}$ that maps an input vector $\vx~=~(x_1,~\ldots{},~x_d)$ to an output $\pred(\vx)~=~\left(\pred^{(1)}(\vx),~\ldots{},~\pred^{(c)}(\vx)\right)$.
We denote $\ball~=~\{\v{\delta} \in \mathbb{R}^d ~:~  || \v{\delta}||_{p} \leq \radius \}$ 
the perturbation ball with radius $\radius > 0$,  with 
$p \in \{1, 2, \infty\}$.
For any subset of indices $\v{u}\subseteq\{1,~\ldots{},~d\}$, we denote 
$\ballu$ the ball without perturbation on the variables in $\v{u}$:
$\ballu = \{ \v{\delta} ~:~ \v{\delta} \in \ball, ~ \v{\delta}_{\v{u}} = 0 \}$ and $\ball(\bm{x})$ the perturbation ball centered on $\bm{x}$. We denote the lower (resp. upper) bounds obtained with verification perturbation analysis as: 

$$
\lowerf(\vx, \ball) = \left(\lowerf^{(1)}(\vx, \ball),~\ldots{},~\lowerf^{(c)}(\vx, \ball)\right)
$$
$$
\upperf(\vx, \ball) = \left(\upperf^{(1)}(\vx, \ball),~\ldots{},~\upperf^{(c)}(\vx, \ball)\right).
$$
Intuitively, these bounds delimit the output prediction for any perturbed sample in $\ball(\vx)$, such that:
$$
\forall \v{\delta} \in \ball, \lowerf(\vx, \ball) \leq \f(\vx + \v{\delta}) \leq \upperf(\vx, \ball).
$$

\subsubsection{The importance of setting the importance right}

Different attribution methods implicitly assume different definitions of the notion of importance for input variables based either on game theory~\cite{lundberg2017unified}, the notion of conditional expectation of the score logits~\cite{petsiuk2018rise}, their variance~\cite{fel2021sobol} or on some  measure of statistical dependency between different areas of an input image and the output of the model~\cite{novello2022making}.
For this work, we build on robustness-based explainability methods~\cite{hsieh2020evaluations} which assume that a variable is important if small perturbations of this variable lead to large changes in the model decision.
Conversely, a variable is said to be unimportant if changes to this variable only yield small changes in the model decision.
From this intuitive assertion, we construct an estimator that we call \Adv.





\subsubsection{Adversarial overlap}



We go one step beyond previous work and propose to compute importance by taking into account not only the ability of individual variables to change the network's decision but also its confidence in the prediction.
\Adv ~ measures the extent to which a modification on a group of pixels can generate overlap between classes, i.e. generate a point close to $\vx$ such that the attainable maximum of an unfavorable class $c'$ can match the minimum of the initially predicted class $c$.

Indeed, if a modification of a pixel -- or group of pixels -- allows generating a new image that changes the decision of $\pred$, this variable must be considered important. 
Conversely, if the decision does not change regardless of the value of the pixel, then the pixel can be left at its nominal value and should be considered unimportant. 

Among the set of possible variable perturbations $\v{\delta}$ around a point $\vx$, we, therefore, look for points that can modify the decision\footnote{Throughout this section, when $c$ is not specified, it is assumed that $c = \argmax \pred(\vx)$.} with the most confidence.
Hence our scoring criterion can be formulated as follows:

\begin{equation}\label{eq:eva:adv_surface}
    \AO(\vx, \ball) = 
    \max_{\substack{\v{\delta} \in \ball, c'\neq c}} \f^{(c')}(\vx + \v{\delta}) - \f^{(c)}(\vx + \v{\delta}).
\end{equation}

Intuitively, this score represents the confidence of the ``best'' adversarial perturbation that can be found in the perturbation ball $\ball$ around $\vx$.

In order to estimate this criterion, a naive strategy could be to use adversarial attacks to search within $\ball$. 
However, when they converge - which is not ensured, such methods only explore certain points of the considered space, thus giving no guarantee regarding the optimality of the solution. 
Moreover, adversarial methods have no guarantee of success and therefore cannot ensure a valid score under every circumstance.
Finally, the large dimensions of the current datasets make exhaustive searches impossible.


To overcome these issues, we take advantage of one of the main results from verified perturbation analysis to derive a guaranteed upper bound on the criterion introduced in Eq.~\ref{eq:eva:adv_surface}. 
We can upper bound the \adv{} criterion as follows: 

$$ 
\AO(\vx, \ball) \leq \AOup(\vx, \ball) = \max\limits_{c'\neq c} \upperf^{(c')}(\vx, \ball) - \lowerf^{(c)}(\vx, \ball). 
$$
The computation of this upper bound becomes tractable using any verified perturbation analysis method. 


For example, $\AOup(\vx, \ball) \leq 0$  guarantees that no adversarial perturbation is possible in the perturbation space.\footnote{Note that with adversarial attacks, failure to find an adversarial example does not guarantee that it does not exist.}
Our upper bound $\AOup(\vx, \ball)$ corresponds to the difference between the verified lower bound of the class of interest $c$ and the maximum over the verified upper bounds among the other classes.
Thus, when important variables are modified (e.g the head of the dog in Fig.~\ref{fig:eva:method}, using $ \textcolor{pink}{\ball} $), the lower bound for the class of interest will get smaller than the upper bound of the adversary class. On the other hand, this overlap is not possible when important variables are fixed (e.g in Fig.~\ref{fig:eva:method} when the head of the dog is fixed, using $ \textcolor{indigo}{\ballu} $).
We now demonstrate how to leverage this score to derive an efficient estimator of variable importance.


\begin{figure*}[t!]
  \includegraphics[width=0.99\textwidth]{assets/eva/figure_eva.pdf}
  \caption{
  \textbf{\eva~attribution method.} In order to compute the importance for a group of variables $\v{u}$ -- for instance the dog's head -- the first step (1) consists in designing the perturbation ball $\textcolor{indigo}{\ballu}(\vx)$. This ball is centered in $\vx$ and contain all the possible images perturbed by $\textcolor{indigo}{\v{\delta}} ~s.t~ ||\textcolor{indigo}{\v{\delta}}||_{p} \leq \varepsilon, ||\textcolor{indigo}{\v{\delta}}_{\v{u}}||_p = 0$ which do not perturb the variables $\textcolor{indigo}{\v{u}}$. Using verified perturbation analysis, we then compute the \adv~ $\AOup(\vx, \textcolor{indigo}{\ballu})$ which corresponds to the overlapping between the class $c$ -- here dog -- and $c'$, the maximum among the other classes. Finally, the importance score for the variable $\v{u}$ corresponds to the drop in \adv~ when $\v{u}$ cannot be perturbed, thus the difference between $\AOup(\vx, \textcolor{pink}{\ball})$ and $\AOup(\vx, \textcolor{indigo}{\ballu})$. 
  Specifically, this measures how important the variables $\v{u}$ are for changing the model's decision.
  }
  \label{fig:eva:method}
\end{figure*}

\subsubsection{\eva}

We are willing to assign a higher importance score for a variable allowing (1) a change in a decision, (2) a greater adversarial -- thus a solid change of decision. Modifying all variables gives us an idea of the robustness of the model.
In the same way, the modification of all variables without the subset $\v{u}$ allows quantifying the change of the strongest adversarial perturbation and thus quantifies the importance of the variables $\v{u}$. Intuitively, if an important variable $\v{u}$ is discarded, then it will be more difficult, if not impossible, to succeed in finding any adversarial perturbation. Specifically, removing the possibility to modify $\vx_{\v{u}}$ allows us to reveal its importance by taking into account its possible interactions.

The complexity of current models means that the variables are not only treated individually in neural network models, but collectively. In order to capture these higher-order interactions, our method consists in measuring the \adv~ allowed by all the variables together $\AOup(\vx, \ball)$ -- thus taking into account their interactions -- and then forbidding to play on a group of variables $\AOup(\vx, \ballu)$ to estimate the importance of $\v{u}$. Making the interactions of $\v{u}$ disappear reveals their importance. Note that several works have mentioned the importance of taking into account the interactions of the variables when calculating the importance~\cite{petsiuk2018rise,fel2021sobol, ferrettini2021coalitional,idrissi2023coalitional}. Formally:

\begin{definition}[\eva]
We introduce \eva~(Explainability using Verified perturbation Analysis) that measure the drop in \adv~ when we fixed the variables $\v{u}$:

\begin{equation}
    \label{eq:eva:tod_estimator}
    \bm{\eva}(\vx, \v{u}, \ball) \defas \AOup(\vx, \ball) - \AOup(\vx, \ballu).
\end{equation}

\end{definition}


As explained in Fig.~\ref{fig:eva:method}, the estimator requires two passes of the perturbation analysis method; one for $\AOup(\ball)$, and the other for $\AOup(\ballu)$: the first term consists in measuring the \adv~ by modifying all the variables, the second term measures the adversarial surface when fixing the variables of interest $\v{u}$.
In other words, \eva~measures the \adv~that would be left if the variables $\v{u}$ were to be fixed.


From a theoretical point of view, we notice that \eva~- under reasonable assumptions - yield the optimal subset of variables to minimize the theoretical \rsr.

\begin{theorem}\textbf{\eva~provide the optimal set from step $|\v{u}|$ to the last step.}
\label{thm:eva:rsr}
With $\v{u}$ the essential variables of $\v{\delta}^*$, \eva~will rank the $\v{u}$ variables first and provide the optimal set from the step $|\v{u}|$ to the last step. 
\end{theorem}

Proof in Appendix \autoref{app:eva:rsr}. Moreover, we note that the explanation stability can be easily bounded by the model Lipschitz constant.

\begin{theorem}\textbf{\eva~ has bounded Stability}
\label{thm:eva:stab}
Given a $L$-lipschitz predictor $\f$, $\radius$ the radius of $\ball$ and $\radiusbis$ the Stability radius, then
$$
\textit{Stability}(\vx, \eva) \leq 4L(\radius + \radiusbis)
$$
\end{theorem}
Proof in Appendix \autoref{app:eva:stab}.
From a computational point of view, we can note that the first term of the \adv~$\AOup(\vx, \ball)$ -- as it does not depend on $\v{u}$ -- can be calculated once and re-used to evaluate the importance of any other variables considered. 
Moreover, contrary to an iterative process method~\cite{Fong_2017, hsieh2020evaluations, ignatiev2019abduction}, each importance can be evaluated independently and thus benefit from the parallelization of modern neural networks. Finally, the experiments in Section~\ref{sec:eva:experiments} show that even with two calls to $\AOup$~ per variables, our method remains much faster than the one based on sampling or on adversarial attacks (such as Greedy-AS or Greedy-AO, see appendix \ref{ap:eva:benchmarks}). %

In this work, the verified perturbation-based analysis considered is not always adapted to high dimensional models, especially those running on ImageNet~\cite{imagenet_cvpr09}. We are confident that the verification methods will progress towards more scalability in the near future, enabling the original version of \eva~ on deeper models. 

In the meantime, we introduce an empirical method that allows to scale \eva~to high dimensional models. This method sacrifices theoretical guarantees, but the results section reveals that it may be a good compromise.

\begin{figure*}[t!]
  \centering
  \includegraphics[width=0.95\textwidth]{assets/eva/eva_hybrid.pdf}
  \caption{\textbf{Scaling strategy.} 
  In order to scale to very large models, we propose to estimate the bounds of an intermediate layer's activations empirically by (1) Sampling $N$ input perturbations and (2) calculating empirical bounds on the resulting activations for the layer $\bm{h}(\cdot)$. 
  We can then form the set $\textcolor{pink}{\mathcal{P}_{\ball}^{\vx}}$ which is a subset of the true bounds $\textcolor{indigo}{\mathcal{H}_{\ball}^{\vx}}$ since the sampling is never exhaustive. We can then plug this set into a verified perturbation analysis method (3) and continue the forward propagation of the inputs through the rest of the network.
  }
  
  \label{fig:eva:eva_hybrid}
\end{figure*}

\subsubsection{Scaling to larger models}
\label{sec:eva:scaling}

We propose a second version of \eva, 
which is a combination of sampling and verification perturbation analysis. 
The aim of this hybrid method is twofold: (\textit{\textbf{i}}) take advantage of sampling to approach the bounds of an intermediate layer in a potentially very large model, (\textit{\textbf{ii}}) then complete only the rest of the propagations with verified perturbation analysis and thus move towards the native \eva~method which benefits from theoretical guarantees.
Note that, combining verification methods with empirical methods (a.k.a adversarial training) has notably been proposed in~\cite{balunovic2019adversarial} for robust training.

Specifically, our technique consists of splitting the model into two parts, and (\textit{\textbf{i}}) estimating the bounds of an intermediate layer using sampling, (\textit{\textbf{ii}}) propagating these empirical intermediate bounds onto the second part of the model with verified perturbation analysis methods.

For the first step (\textit{\textbf{i}}) we consider the original predictor $\pred$ as a composition of functions 
$\pred(\vx) = (\bm{g} \circ \bm{h})(\vx)$. 
For deep neural networks, $\bm{h}(\cdot)$ is a function that maps input to an intermediate feature space
and $\bm{g}(\cdot)$ is a function that maps this same feature space to the classification.

We propose to empirically estimate bounds 
$( \underline{\bm{h}}_{\ball}^{\vx}, \overline{\bm{h}}_{\ball}^{\vx} )$ for the intermediate activations $ \bm{h}(\cdot) \in \mathbb{R}^{d'}$ using Monte-Carlo sampling on the perturbation $\v{\delta} \in \ball$. Formally:

\begin{equation*}
\begin{split}
    \forall j \in \{0, \ldots, d'\}, ~
& \underline{\bm{h}}_{\ball}^{\vx}[j] = 
  \min\limits_{\v{\delta}_1,\ldots, \v{\delta}_n \overset{\text{iid}}{\sim} \P(\ball)}
  \bm{h}(\vx+\v{\delta}_i)[j]\\
& \overline{\bm{h}}_{\ball}^{\vx}[j] =  
  \max\limits_{\v{\delta}_1,\ldots,\v{\delta}_n  \overset{\text{iid}}{\sim} \P(\ball)}
  \bm{h}(\vx+\v{\delta}_i)[j].
\end{split}
\end{equation*}

With $\P(\ball)$ the uniform distribution over $\ball$. Obviously, since the sampling is never exhaustive, the bounds obtained underestimate the true maximum $ \overline{\bm{h}}_{\ball}^{\vx} \leq \max \bm{h}(\vx + \v{\delta})$ and overestimates the true minimum $ \underline{\bm{h}}_{\ball}^{\vx} \geq \min \bm{h}(\vx + \v{\delta})$ as illustrated in the Fig.~\ref{fig:eva:eva_hybrid}.
In a similar way, we define $\underline{\bm{h}}_{\ballu}^{\vx}$ and $\overline{\bm{h}}_{\ballu}^{\vx}$ when  $\v{\delta} \in \ballu$. 
Once the empirical bounds are estimated, we may proceed to the second step and use the obtained bounds to form the new perturbation set 

$$ 
\mathcal{P}^{\vx}_{\ball} = 
    [\underline{\bm{h}}_{\ball}^{\vx} - \bm{h}(\vx), 
    \overline{\bm{h}}_{\ball}^{\vx} - \bm{h}(\vx)].
$$ 
Intuitively, this set bounds the intermediate activations obtained empirically and can then be fed to a verified perturbation verification method.



We then carry out the end of the bounds propagation in the usual way, using verified perturbation analysis. This amounts to computing bounds for the outputs of the network for all possible activations contained in our empirical bounds. The only change is that we no longer operate in the pixel space $\vx$ with the ball $\ball$, but in the activation space $\bm{h}(\cdot)$  with the perturbations set $\mathcal{P}^{\vx}_{\ball}$. The importance score of a set of variables $\v{u}$ is then : 

$$ \evaH(\vx, \v{u}, \ball) \defas \eva(\bm{h}(\vx), \v{u}, \mathcal{P}^{\vx}_{\ball}).$$

This hybrid approach allows us to use \eva~ on state-of-the-art models and thus to benefit from our method while remaining tractable. We believe this extension to be a promising step towards robust explanations on deeper networks.

\subsection{Experiments}
\label{sec:eva:experiments}

\input{assets/eva/fidelity}

To evaluate the benefits and reliability of our explainability method, we performed several experiments on a standard dataset, using a set of common explainability metrics against \eva.
In order to test the fidelity of the explanations produced by our method, we compare them to that of 10 other explainability methods using the (1) Deletion, (2) Insertion, and (3) MuFidelity metrics. As it has been shown that these metrics can exhibit biases, we completed the benchmark by adding the (4) \rsr metric. Each score is averaged over 500 samples.

We evaluated these 4 metrics on 3 image classification datasets, namely MNIST~\cite{lecun2010mnist}, CIFAR-10~\cite{krizhevsky2009learning} and ImageNet~\cite{imagenet_cvpr09}.

Through these experiments, the explanations were generated using \eva~estimator introduced in Equation~\ref{eq:eva:tod_estimator}. The importance scores were not evaluated pixel-wise but on each cell of the image after having cut it into a grid of 12 sides (see Fig.~\ref{fig:eva:method}). For MNIST and Cifar-10, we used 
$\varepsilon = 0.5$, whereas for ImageNet $\varepsilon = 5$. Concerning the verified perturbation analysis method, we used (IBP+Forward+Backward) for MNIST, and (IBP+Forward) on Cifar-10 and $p=\infty$. For computational purposes, we used the hybrid approach introduce in Section~\ref{sec:eva:scaling} for ImageNet using the penultimate layer (FC-4096) as the intermediate layer $\bm{h}(\cdot)$. We give in Appendix the complete set of hyperparameters used for the other explainability methods, metrics considered as well as the architecture of the models used on MNIST and Cifar-10.

\subsubsection{Comparison with the state of the art}


There is a general consensus that fidelity is a crucial criterion for an explanation method. That is, if an explanation is used to make a critical decision, then users are expecting it to reflect the true decision-making process underlying the model and not just a consensus with humans. Failure to do so could have disastrous consequences. Pragmatically, these metrics assume that the more faithful an explanation is, the faster the prediction score should drop when pixels considered important are changed.
In Table~\ref{tab:eva:cifar_mnist_metrics}, we present the results of the Deletion~\cite{petsiuk2018rise} (or $1 - AOPC$~\cite{samek2016evaluating}) metric for the MNIST and Cifar-10 datasets on 500 images sampled from the test set. TensorFlow~\cite{tensorflow2015} and the Keras API~\cite{chollet2015keras} were used to run the models and Xplique~\cite{fel2022xplique} for the explainability methods.
In order to evaluate the methods, the metrics require a baseline and several were proposed~\cite{sturmfels2020visualizing, hsieh2020evaluations}, but we chose to keep the choice of~\cite{hsieh2020evaluations} using their random baseline.
 
We observe that \eva~is the explainability method getting the best Deletion, Insertion, and $\mu$Fidelity scores on MNIST, and is just behind Greedy-AS on \rsr. This can be explained by the fact that the Robustness metric uses the adversarial attack PGD~\cite{madry2017pgd}, which is the same one used to generate Greedy-AS, thus biasing the adversarial search. Indeed, if PGD does not find an adversarial perturbation using a subset $\v{u}$ does not give a guarantee of the robustness of the model, just that the adversarial perturbation could be difficult to reach with PGD.

For Cifar-10, \eva~remains overall the most faithful method according to Deletion and $\mu$Fidelity, and obtains the second score in Insertion behind Grad-Cam++~\cite{chattopadhay2018grad}. 
Finally, we notice that if Greedy-AS~\cite{hsieh2020evaluations} allows us to obtain a good \rsr~score, but this comes with a considerable computation time, which is not the case of \eva~which is much more efficient. Eventually, EVA is a very good compromise for its relevance to commonly accepted explainability metrics and more recent robustness metrics.

\paragraph{ImageNet}

After having demonstrated the potential of the method on vision datasets of limited size, we consider the case of ImageNet which has a significantly higher level of dimension.
The use of verified perturbation analysis methods other than IBP is not easily scalable on these datasets. We, therefore, used the hybrid method introduced in Section ~\ref{sec:eva:scaling} in order to estimate the bounds in a latent space  and then plug those bounds into the perturbation analysis to get the final \adv~ score.

Table~\ref{tab:eva:cifar_mnist_metrics} shows the results obtained with the empirical method proposed in Section~\ref{sec:eva:scaling}. We observe that even with this relaxed estimation, \eva~is able to score high on all the metrics. Indeed, \eva~obtains the best score on the Insertion metric and ranks second on $\mu$Fidelity and 
\rsr.
Greedy-AS ranks first on \rsr~at the expense of the other scores where it performs poorly. Finally, both RISE and SmoothGrad perform  well on all the fidelity metrics but collapse on the robustness metric. Extending results with ablations of \eva, including Greedy-AO, are available in Table~\ref{tab:eva:ablation_ao}.

Qualitatively, Fig.~\ref{fig:eva:imagenet_explanations} shows examples of explanations produced on the ImageNet VGG-16 model. The explanations produced by \eva~ are more localized than Grad-CAM or RISE, while being less noisy than the gradient-based or Greedy-AS methods.

In addition, as the literature on verified perturbation analysis is evolving rapidly we can conjecture that the advances will benefit the proposed explainability method.
Indeed, \eva~proved to be the most effective on the benchmark when an accurate formal method was used.
After demonstrating the performance of the proposed method, we study its ability to generate class explanations specific. 

\subsubsection{Tighter bounds lead to improved explanations}

\input{assets/eva/ablation_vpa}

The choice of the verified perturbation analysis method is a hyperparameter of EVA.
Hence, it is interesting to see the effect of the choice of this hyperparameter on the previous benchmark. 
We recall that only the MNIST dataset could benefit from the (IBP+Forward+Backward) combo. Table~\ref{tab:eva:ablation_vpa}  reports the results of the fidelity metrics using other verified perturbation analysis methods. We also report a tightness score which corresponds to the average of the \adv~: 
$\mathbb{E}_{\vx \sim \mathcal{X}}(\AOup(\vx, \ball))$.
Specifically, a low score indicates that the verification method is precise, meaning that the over-approximation is closer to the actual value. It should be noted that the true value is intractable, but remains the same across all three tested cases. We observe that the tighter the bounds, the higher the scores. This allows us to conjecture that the more scalable the formal methods will become, the better the quality of the generated explanations will be.
We perform additional experiments to ensure that the certified component of \eva~score is significant by comparing \eva~ to 
a sampling-based version of \eva. The details of these experiments are available in Appendix~\ref{ap:eva:benchmarks}.

\subsubsection{Targeted Explanations}
\label{sec:eva:targeted_explanations}

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.9\textwidth]{assets/eva/targeted_explanations.png}
  \caption{\textbf{Targeted explanations.} Generated explanations for a decision other than the one predicted by the model. The class explained is indicated at the bottom of each sample, e.g., the first sample is a `4' and the explanation is for the class `9'. As indicated in section~\ref{sec:eva:targeted_explanations}, the red areas indicate that a black line should be added and the blue areas that it should be removed. More examples are available in the Appendix.
  }
  \label{fig:eva:targeted_explanations}
\end{figure}

In some cases, it is instructive to look at the explanations for unpredicted classes in order to get information about the internal mechanisms of the models studied. Such explanations allow us to highlight contrastive features: elements that should be changed or whose absence is critical. 
Our method allows us to obtain such explanations: for a given input, we are then exclusively interested in the class we are trying to explain, without looking at the other decisions. Formally, for a given targeted class $c'$
the \adv~ (Equation~\ref{eq:eva:adv_surface}) become $\AO(\vx, \ball) = \max_{\substack{\v{\delta} \in \ball}} \f^{(c')}(\vx + \v{\delta}) - \f^{(c)}(\vx + \v{\delta})$. Moreover, by splitting the perturbation ball into a positive one $\ball^{(+)} = \big\{ \v{\delta} \in \ball ~:~ \v{\delta}_i \geq 0,~ \forall i \in \{1, ..., d \} \big\}$ and a negative one $\ball^{(-)} = \big\{ \v{\delta} \in \ball ~:~ \v{\delta}_i \leq 0,~ \forall i \in \{1, ..., d \} \big\}$, one can deduce which direction -- adding or removing the black line in the case of gray-scaled images -- will impact the most the model decision.

We generate targeted explanations on the MNIST dataset using (IBP+Forward+Backward). For several inputs, we generate the explanation for the 10 classes. Fig.~\ref{fig:eva:ap_targeted} shows 4 examples of targeted explanations, the target class $c'$ is indicated at the bottom. The red areas indicate that adding a black line increases the \adv~ with the target class. Conversely, the blue areas indicate where the increase of the score requires removing black lines.
All other results can be found in the Appendix.
In addition to favorable results on the fidelity metrics and guarantees provided by the verification methods, \eva~can provide targeted explanations that are easily understandable by humans, which are two qualities that make them a candidate of choice to meet the recent General Data Protection Regulation (GDPR) adopted in Europe~\cite{kaminski2021right}. More examples are available in the Appendix~\ref{ap:eva:targeted}.

\subsection{Conclusion}

In this work, we presented the first explainability method that uses verification perturbation analysis that exhaustively explores the perturbation space to generate explanations. We presented an efficient estimator that yields explanations that are state-of-the-art on current metrics. We also described a simple strategy to scale up perturbation verification methods to complex models. Finally, we showed that this estimator can be used to form easily interpretable targeted explanations.

We hope that this work will %
for searching for safer and more efficient explanation methods for neural networks -- and that it will inspire further synergies with the field of formal verification.
