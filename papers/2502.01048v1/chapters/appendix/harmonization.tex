\section{Psychophyics}\label{si_sec:psychophysics}

The psychophysics experiments of \textsection{4.2} were implemented with the psiTurk framework \cite{Gureckis2016-if} and custom javascript functions. Each trial sequence was converted to a HTML5-compatible video for the fastest reliable presentation time possible in a web browser. Videos were cached before each trial to optimize reliability of experiment timing within the web browser. A photo-diode verified the reliability of stimulus timing in our experiment was consistently accurate within $\sim10\mathrm{ms}$ across different operating system, web browser, and display type configurations.

\paragraph{Participants:} We recruited 199 participants from Amazon Mechanical Turk (\url{mturk.com}) for the experiments. Participants were based in the United States, used either the Firefox or Chrome browser on a non-mobile device, and had a minimal average approval rating of 95\% on past Mechanical Turk tasks. 

\paragraph{Stimuli:} Experiment images were taken from the \textit{Clicktionary} dataset~\cite{Linsley2017-qe}. Images were sampled from 5 target and 5 distractor categories: border collie, sorrel (horse), great white shark, bald eagle, and panther; trailer truck, sports car, speedboat, airliner, and school bus. Images were presented to human participants (and DNNs) either intact or with a perceptual phase scrambled mask that exposed a proportion of their most important visual features, as described in the main text. Images were cast to greyscale to control for trivial color-based cues for classification and blend the scrambled mask background into the foreground. Responses to intact images were used to normalize the performance of each observer on masked images relative to their maximum performance on these images.


\begin{figure}[h!]
\begin{center}
   \includegraphics[width=1\linewidth]{assets/harmonization/rapid_behavior_paradigm.pdf}
\end{center}
   \caption{\textbf{Overview of the psychophysics paradigm.} Participants performed a rapid animals vs. vehicles categorization paradigm (top). Stimuli were created using feature importance maps derived from humans or DNNs via a ``stochastic flood-fill'' algorithm that revealed image regions of different sizes centered on important features. Sample stimuli are shown (bottom) for different percentages of image revelation. Note that 100\% revelation corresponds to all non-zero pixels in a feature importance map.}
\label{si_fig:psychophysics}
\end{figure}

\begin{figure}[h!]
\begin{center}
   \includegraphics[width=1\linewidth]{assets/harmonization/instructions.png}
\end{center}
   \caption{\textbf{Psychophysics experiment instructions.}}
\label{si_fig:instructions}
\end{figure}

Image masks were created for each image to reveal only a proportion of the most important visual features. For each image, we created masks that revealed between 1\% and 100\% (at log-scale spaced intervals) of the object pixels in the corresponding image's \textit{Clicktionary} feature importance map. We generated these masks in two steps. First, we computed a phase-scrambled version of the image~\cite{Oppenheim1981-sl, Thomson1999-rp}. Next, we used a novel ``stochastic flood-fill'' algorithm to reveal a contiguous region of the most important visual features in the image according to humans. Our flood-fill algorithm was seeded on the pixel deemed most important by humans in the image, then grew outwards anisotropically and biased towards pixels with higher feature importance scores (Figure~\ref{si_fig:psychophysics}). The revealed region was always centered on the image. Each participant saw every category exemplar only once, with its amount of image revelation randomly selected from all possible configurations. 

After providing online consent, participants were instructed to complete a rapid visual categorization task in which they had to classify stimuli revealing a portion of the most diagnostic object features (Fig.~\ref{si_fig:instructions}). Each experimental trial began with a cross for participants to fixate for a variable time (1,100â€“1,600ms), then a stimulus for 400ms, then another cross and additional time for participants to render a decision. Participants were instructed to provide a decision after the first fixation cross, but that they only had 650ms to answer. If they were too slow to respond they were told to respond faster and the trial was discarded.


\begin{figure}[h!]
\begin{center}
   \includegraphics[width=0.5\linewidth]{assets/harmonization/clickme_examples.pdf}
\end{center}
   \caption{\textbf{Example \textit{ClickMe} feature importance maps on ImageNet images.}}
\label{si_fig:clickme_examples}
\end{figure}

\section{Additional Results}
\subsection{\textit{ClickMe}}
The \textit{ClickMe} game by \cite{Linsley2019-ew} was used to identify category diagnostic features in ImageNet images. These feature importance maps largely focus on object regions rather than context, and in contrast to segmentation maps select features on the ``front'' or ``face'' of objects (Fig.~\ref{si_fig:clickme_examples}).

\begin{figure}[h!]
\begin{center}
   \includegraphics[width=1\linewidth]{assets/harmonization/big_qualitative.pdf}
\end{center}
   \caption{\textbf{Feature importance maps of humans, harmonized, and unharmonized models on ImageNet}.}
 \label{fig:qualitative_figure_big}
\end{figure}

As discussed in the main text, we found a trade-off between DNN top-1 ImageNet accuracy and the alignment of their feature importance maps with humans importance maps from \textit{ClickMe}. This trade-off persists across multiple scales of feature importance maps, including 16$\times$ (Fig.~\ref{si_fig:clickme_results_16}) sub-sampled maps, meaning that simple smoothing is not sufficient to fix the trade-off.

\begin{figure}[h!]
\begin{center}
   \includegraphics[width=1\linewidth]{assets/harmonization/clickme_16x.pdf}
\end{center}
   \caption{\textbf{The neural harmonizer's effect is robust across image scales.} Here, we show that the trade-off between ImageNet accuracy and alignment with humans holds across downsizing by a factor of \textit{16}. The Neural harmonizer once again yields the model with the best alignment with humans. Grey-shaded area captures the trade-off between accuracy and alignment in standard DNNs. Error bars are bootstrapped standard deviations over feature alignment.}
\label{si_fig:clickme_results_16}
\end{figure}

\subsection{\textit{ViT attention}} While in the main text we investigate alignment between humans and models using gradient feature importance visualizations, the attention maps in transformer  models like the ViT provide another avenue for investigation. To understand whether or not attention maps from ViT are more aligned with humans than their gradient-based decision explanation maps, we computed attention rollouts for harmonized and unharmonized ViTs~\cite{Abnar2020-sb}. We found that both versions of the ViT had similar correlations between their attention rollouts and human \textit{ClickMe} maps: 0.38 for the harmonized ViT and 0.393 for the unharmonized model. This surprising result suggests that the harmonizer affects the process by which ViTs integrate visual information into their decisions rather than how they allocate attention. Through manipulating ViT decision making processes, the harmonizer can induce the large changes in gradient-based visualizations and psychophysics that we describe in the main text.

\subsection{\textit{Correlations between measurements of human visual strategies}}
Our results rely on three independent datasets measuring different features of human visual strategies: \textit{ClickMe}, \textit{Clicktionary}, and the psychophysics experiments we introduce in this manuscript. The fact that all three evoke similar trade-offs between top-1 accuracy and human alignment is a surprising result that deserves further attention. We investigated these trade-offs by measuring the correlation between human alignment on each dataset, with and without models trained with the neural harmonizer. We found that correlations between datasets were lower across the board when neural harmonizer models were not included. Each correlation improved when the neural harmonizer models were included in the calculation. This finding indicates that the neural harmonizer successfully aligned visual strategies between humans and DNNs, and was not merely benefiting from either \textit{where} humans versus DNNs considered important visual features to be or \textit{how} humans versus DNNs incorporated those features into their decisions.

\begin{figure}[h!]
\begin{center}
   \includegraphics[width=1\linewidth]{assets/harmonization/clicktionary_vs_psych.pdf}
\end{center}
   \caption{\textbf{The association between \textit{Clicktionary} alignment versus psychophysics alignment.} These scores are significantly correlated, $\rho=0.53, p<0.001$.}
\label{si_fig:clicktionary_vs_psych}
\end{figure}

\begin{figure}[h!]
\begin{center}
   \includegraphics[width=1\linewidth]{assets/harmonization/clickme_center_bias.png}
\end{center}
   \caption{\textbf{The mean of \textit{ClickMe} feature importance maps exhibits a center bias, likely due to the positioning of objects in ImageNet images rather than a purely spatial bias of human participants (compare to individual maps shown in \ref{si_fig:clickme_examples})}.}
\label{si_fig:center_bias}
\end{figure}

\clearpage

\section{Theoretical consideration.}

We recall the stated theorem concerning the alignment of predictions given aligned explanations. The theorem asserts the consistency of predictions up to a constant when the explanations generated by any pair of aligned predictors are equivalent.

\begin{theorem}[$\explainer$-Aligned Imply Aligned Predictions]
\label{app:harmonization:thm}
Given a function space where $\fspace : \sx \to \sy$, with $\sx = (0, 1]^d$ denoting the input space and $\sy \subseteq \Real$ the output space, an explanation functional $\explainer : \fspace \times \sx \to \sx$. Assume $(\f, \fbis)$ are two predictors that are $\explainer$-Aligned within this space. For any explanation functional $\explainer$ from the set $\{ \explainer_{\text{Sa}}, \explainer_{\text{GI}}, \explainer_{\text{IG}}, \explainer_{\text{OC}} \}$, the alignment of explanations imply the alignment of predictions up to a constant difference:
\[
\forall \vx \in \sx, ~~ \f(\vx) = \fbis(\vx) + \kappa
\]
where $\kappa$ is a constant independent of $\vx$.
\end{theorem}

\begin{proof}


\textbf{Saliency:} Let $\vx_0 \in \sx$ serve as a reference point in the input space, and define the constant $\kappa = \f(\vx_0) - \fbis(\vx_0)$. Given the alignment of explanations by $\explainer_{\text{Sa}}$, we have:

\begin{align}
\explainer_{\text{Sa}}(\f, \vx) &= \explainer_{\text{Sa}}(\fbis, \vx), \\
 \grad \f(\vx) &= \grad \fbis(\vx).
\end{align}

From the equality of gradients, the fundamental theorem of calculus permits us to deduce:

\begin{align}
\int_{\vx_0}^{\vx} \grad \f(\vx') \mathrm{d} \vx' &= \int_{\vx_0}^{\vx} \grad \fbis(\vx') \mathrm{d} \vx', \\
 \f(\vx) - \f(\vx_0) &= \fbis(\vx) - \fbis(\vx_0), \\
 \f(\vx) &= \fbis(\vx) + \kappa.
\end{align}

Similar reasoning could be applied to \textbf{Gradient-Input}.


\textbf{Integrated-Gradients:} we recall that $\sx = (0, 1]^d$, $\sy \subseteq \Real$, for $\explainer_{\text{IG}}$, we have:

\begin{align}
\explainer_{\text{IG}}(\f, \vx) &= \explainer_{\text{IG}}(\fbis, \vx) \\
(\vx - \vx_0) \int_{0}^{1} \grad \f((1 - \alpha) \vx_0 + &\alpha(\vx - \vx_0)) \diff \alpha = \\
(\vx - \vx_0) \int_{0}^{1} \grad & \fbis((1 - \alpha) \vx_0 + \alpha(\vx - \vx_0)) \diff \alpha.
\end{align}

With $\tilde{\vx} = (1 - \alpha) \vx_0 + \alpha(\vx - \vx_0)$, 

\begin{align}
(\vx - \vx_0) \int_{0}^{1} \grad \f(\tilde{\vx}) \diff \alpha &= 
(\vx - \vx_0) \int_{0}^{1} \grad  \fbis(\tilde{\vx}) \diff \alpha \\
\f(\vx) - \f(\vx_0) &= \fbis(\vx) - \fbis(\vx_0) \\
\f(\vx) &= \fbis(\vx) + \kappa.
\end{align}


\textbf{Occlusion:} by definition,

\begin{align}
\explainer_{\text{OC}}(\f, \vx) &= \explainer_{\text{OC}}(\fbis, \vx) \\
\f(\vx) - \f(\vx_{[x_i = x_0]}) &= \fbis(\vx) - \fbis(\vx_{[x_i = x_0]}).
\end{align}

To say it simply, the difference when one feature is set to a baseline state is the same between the two predictor. However, we observe that if we removed another pixel, the equality still hold, (one could pose $\vx' = \vx_{[x_i = x_0]}$. Intuitively, we are building a discrete path from any image $\vx$ to the image full of the baseline vector $\vx_0$ by flipping each element $x_i$ of $\vx$ one by one.

\begin{align}
\f(\vx') - \f(\vx_{[x_j = x_0]}) &= \fbis(\vx') - \fbis(\vx_{[x_j = x_0]}).
\end{align}

Thus, by recursion:

\begin{align}
\f(\vx) - \f(\vx_{[x_i = x_0]}) + \f(\vx_{[x_i = x_0]}) - \f(\vx_{[x_i = x_0, x_j = x_0]}) + \ldots - \f(\vx_0)  = \\ \fbis(\vx) - \fbis(\vx_{[x_i = x_0]}) + \fbis(\vx_{[x_i = x_0]}) - \fbis(\vx_{[x_i = x_0, x_j = x_0]}) + \ldots - \fbis(\vx_0).
\end{align}

We reduce the following telescopic sum:

\begin{align}
\f(\vx) - \f(\vx_0)  &= \fbis(\vx) - \fbis(\vx_0) \\
\f(\vx) &= \fbis(\vx) + \kappa.
\end{align}

As whatever the $\vx$ choosen, $\vx_0$, the last element of the telescopic sum is always the same is constant, the vector in $\Real^d$ full of the baseline state $x_0$. 

This comprehensive approach across different explanation functionals substantiates the theorem, confirming that explanation alignment under any of these functionals imply prediction alignment up to a constant.

\end{proof}
