\subsection{Limitations}
\label{apx:craft:limitations}

Although we believe concept-based XAI to be a promising research direction, it isn't without pitfalls. It is capable of producing explanations that are ideally easy to understand by humans, but to what extent is a question that remains unanswered. The fact that there is no way to mathematically measure this prevents researchers from easily comparing the different techniques in the literature other than through time consuming and expensive experiments with human subjects. We think that developing a metric should be one of the field's priorities.%

With \craft, we address the question of \what~by showing a cluster of the images that better represent each concept. However, we recognize that it's not perfect: in some cases, concepts are difficult to clearly define -- put a label on what it represents --, and might induce some confirmation and selection bias. Feature visualization~\cite{olah2017feature} might help in better illustrating the specific concept (as done in appendix \ref{app:craft:feature-viz-val}), but we believe there's still space for improvement. For instance, an interesting idea could be to leverage image captioning methods to describe the clusters of image crops, as textual information could help humans in better understanding clusters.

Although we believe \craft~to be a considerable step in the good direction for the field of concept-based XAI, it also have some pitfalls. Namely, we chose the NMF as the activation factorization, which, while drastically improving the quality of extracted concepts, also comes with it's own caveats. For instance, it is known to be NP-hard to compute exactly, and in order to make it scalable, we had to use a tractable approximation by alternating the optimization of $\m{U}$ and $\m{W}$ through ADMM~\cite{boyd2011distributed}. This approach might indeed yield non-unique solutions. Our experiments (section \ref{sec:craft:expSobol}), have shown a low variance on between the runs, which comforts us about the stability of our results.%
However the absence of formal guarantee for uniqueness must be kept in mind: this subject is still an active topic of research and improvement could be expected in the near future. Namely, sparsity constraints and regularization seem to be promising paths.
Naturally, we also need enough samples of the class under study to be available for the factorization to construct a relevant concept bank, which might affect the quality of the explanations on frugal applications where data is very scarce. %

\subsection{Additional results}
\label{apx:craft:more-craft}

\subsubsection{Qualitative comparison with ACE}\label{apx:qualitative}

Figure~\ref{fig:app:craft:qualitative} compares the examples of concepts found by CRAFT against those found by ACE~\cite{ghorbani2019towards} for 3 classes of Imagenette. 
For each class the concepts are ordered by importance (the highest being the most important). 
ACE uses a clustering technique and TCAV to estimate importance, while CRAFT uses the method introduced in \ref{sec:craft:method} and Sobol to estimate importance. These examples illustrate one of the weaknesses of ACE: the segmentation used can introduce biases through the baseline value used~\cite{sturmfels2020visualizing,fong2017meaningful}. The concepts found by CRAFT seem distinct: (vault, cross, stained glass) for the Church class, (dumpster, truck door, two-wheeler) for the garbage truck, and (eyes, nose, fluffy ears) for the English Springer.

\begin{figure*}[ht]
  \includegraphics[width=0.99\textwidth]{assets/craft/craft_vs_ace.jpg}
  \caption{ \textbf{Qualitative comparison.} We compare concepts found by our method (top) to those extracted with ACE~\cite{ghorbani2019towards} (bottom) for the classes \textit{Church}, \textit{Garbage truck} and \textit{English springer} from ILSVRC2012~\cite{imagenet_cvpr09}. %
  }
  \label{fig:app:craft:qualitative}
\end{figure*}

\subsubsection{Most important concepts.} We show more example of the 4 most importants concepts for 6 classes: `Chain saw', `English springer', `Gas pump', `Golf ball', `French horn' and `Garbage Truck' (Figure~\ref{fig:craft:segments_all}).%

\begin{figure*}[ht]
    \centering
      \includegraphics[width=\textwidth]{assets/craft/segments/all.jpg}
      \caption{ \textbf{CRAFT most important concepts}. The 4 most important concepts ranked by importance (left to right) for the following classes: `English springer', `Chain saw',  `Gas pump', `Golf ball', `French horn',  and `Garbage truck'.
      }
      \label{fig:craft:segments_all}
\end{figure*}
    



\clearpage

\subsubsection{Feature Visualization validation} \label{app:craft:feature-viz-val}

Another way of interpreting concepts -- as per~\cite{kim2018interpretability} -- is to employ feature visualization methods: through optimization, find an image that maximizes an activation pattern.
In our case, we used the set of regularization and constraints proposed by \cite{olah2017feature}, which allow us to successfully obtain realistic images. In Figures~[\ref{fig:craft:feature_viz_chainsaw}-\ref{fig:craft:feature_viz_golf}], we showcase these synthetic images obtained through feature visualization, along with the segments that maximize the target concept. We observe that they do reflect the underlying concepts of interest.

Concretely, to produce those feature visualization, we are looking for an image $\vx^*$ that is optimized to correspond to a concept from the concept bank $\m{W}_i$. We use the so called `dot-cossim' loss proposed by ~\cite{olah2017feature}, which give the following objective:

\begin{equation*}
    \vx^* = \argmax_{\vx \in \mathcal{X}} ~ \langle \bm{g}(\vx), \m{W}_i \rangle 
\frac{\langle \bm{g}(\vx), \m{W}_i \rangle^2}{||\bm{g}(\vx)|| ~ ||\m{W}_i||  } - \mathcal{R}(\vx)    
\end{equation*}

With $\mathcal{R}(\cdot)$, the regularizations applied to $\vx$  -- the default regularizations in the \textbf{Xplique} library~\cite{xplique}. As for the specific parameters, we used Fourier preconditioning on the image with a decay rate of $0.8$ and an Adam optimizer ($lr = 1e-1$). 

\begin{figure}[ht]
\centering
  \includegraphics[width=0.45\textwidth]{assets/craft/feature_viz/chainsaw.jpg}
  \caption{ \textbf{Feature visualization for chainsaw CRAFT concepts.}
  }
  \label{fig:craft:feature_viz_chainsaw}
\end{figure}

\begin{figure}[ht]
\centering
  \includegraphics[width=0.45\textwidth]{assets/craft/feature_viz/english.jpg}
  \caption{ \textbf{Feature visualization for english springer CRAFT concepts.}
  }
  \label{fig:craft:feature_viz_englishspringer}
\end{figure}

\begin{figure}[ht]
\centering
  \includegraphics[width=0.45\textwidth]{assets/craft/feature_viz/golf.jpg}
  \caption{ \textbf{Feature visualization for golf CRAFT concepts.} 
  }
  \label{fig:craft:feature_viz_golf}
\end{figure}

\clearpage
\newpage






\subsection{Backpropagating through the NMF block}

\subsubsection{Alternating Direction Method of Multipliers (ADMM) for NMF}

We recall that NMF decomposes the positive features vector $\m{A} \in \mathbb{R}^{n \times p}$ of $n$ examples lying in dimension $p$, into a product of positive low rank matrices $\m{U}(\m{A})\in\mathbb{R}^{n\times r}$ and $\m{W}(\m{A})\in\mathbb{R}^{p\times r}$ (with $r<<\min(n,p)$), i.e the solution to the problem:
\begin{align}\label{apeq:craft:nmf}
\min_{\m{U}\geq 0,\m{W}\geq 0} & \frac{1}{2}\|\m{A}-\m{U}\m{W}^T\|^2_F. %
\end{align}

For simplicity we used a non-regularized version of the NMF objective, following Algorithms 1 and 3 in paper~\cite{huang2016flexible}, based on ADMM~\cite{boyd2011distributed}. This algorithm transforms the non-linear equality constraints into indicator functions $\bm{\delta}$. Auxiliary variables $\tilde{\m{U}},\tilde{\m{W}}$ are also introduced to separate the optimization of the objective on the one side, and the satisfaction of the constraint on $\m{U}, \m{W}$ on the other side. The equality constraints $\tilde{\m{U}}=\m{U},\tilde{\m{W}}=\m{W}$ are linear and easily handled by the ADMM framework through the associated dual variables $\bar{\m{U}},\bar{\m{W}}$. In our case, the problem in Equation~\ref{apeq:craft:nmf} is transformed into:
  
\begin{equation}
\begin{aligned}\label{apeq:craft:admm}
\min_{\m{U},\tilde{\m{U}}, \m{W},\tilde{\m{W}}} & \frac{1}{2}\|\m{A}-\tilde{\m{U}} \tilde{\m{W}}^T\|^2_F+\bm{\delta}(\m{U})+\bm{\delta}(\m{W}), 
\\%\quad&\textcolor{red}{\text{non convex, NP hard}}\\
~ ~ s.t. ~ ~ &\tilde{\m{U}}=\m{U}, \tilde{\m{W}}=\m{W} \\
     \text{with} ~ & \bm{\delta}(\bm{H})=\begin{cases}
                            0 \text{ if } \bm{H} \geq 0,\\
                            +\infty \text{ otherwise.}
                            \end{cases}
\end{aligned}
\end{equation}

Note that $\tilde{\m{U}}$ and $\m{U}$ (resp. $\tilde{\m{W}}$ and $\m{W}$) seem redundant: they are meant to be equal thanks to constraints $\tilde{\m{U}}=\m{U}, \tilde{\m{W}}=\m{W}$. This is standard practice within ADMM framework: introducing redundancies allows to disentangle the (unconstrained) optimization of the objective on one side (with $\tilde{\m{U}}$ and $\tilde{\m{W}}$) and constraint satisfaction on the other side with $\m{U}$ and $\m{W}$. During the optimization process the variables $\tilde{\m{U}},\m{U}$ (resp. $\tilde{\m{W}},\m{W}$) are different, and only become equal in the limit at convergence. The dual variables $\bar{\m{U}},\bar{\m{W}}$ control the balance between optimization of the objective $\frac{1}{2}\|\m{A}-\tilde{\m{U}} \tilde{\m{W}}^T\|^2_F$ and constraint satisfaction $\tilde{\m{U}}=\m{U}, \tilde{\m{W}}=\m{W}$. The constraints are simplified at the cost of a non-smooth (and even a non-finite) objective function $\frac{1}{2}\|\m{A} -\bar{\m{U}} \bar{\m{W}}^T\|^2_F+\bm{\delta}(\m{U})+\bm{\delta}(\m{W})$ due to the term $\bm{\delta}(\m{U})+\bm{\delta}(\m{W})$. ADMM proceeds to create a so-called \textit{augmented Lagrangian} with $l_2$ regularization $\rho>0$:
\begin{equation}
    \begin{aligned}
    \Lagrangian&(\m{A},\m{U},\m{W},\tilde{\m{U}},\tilde{\m{W}},\bar{\m{U}},\bar{\m{W}})=\\
    &\frac{1}{2}\|\m{A}-\tilde{\m{U}}\tilde{\m{W}}^T\|^2_F+\bm{\delta}(\m{U})+\bm{\delta}(\m{W})\\
    &+\bar{\m{U}}^T(\tilde{\m{U}}-\m{U})+\bar{\m{W}}^T(\tilde{\m{W}}-\m{W})\\
    &+\frac{\rho}{2}\left(\|\tilde{\m{U}}-\m{U}\|_2^2+\|\tilde{\m{W}}-\m{W}\|_2^2\right).
    \end{aligned}
\end{equation}

This regularization ensures that the dual problem is well posed and that it remain convex, even with the non smooth and infinite terms $\bm{\delta}(\m{U})+\bm{\delta}(\m{W})$. Once again, this is standard practice within ADMM framework. The (regularized) problem associated to this Lagrangian is decomposed into a sequence of convex problems that alternate minimization over the $\m{U},\tilde{\m{U}},\bar{\m{U}}$ and the $\m{W},\tilde{\m{W}},\bar{\m{W}}$ triplets.
  
\begin{align}\label{apeq:craft:pairnnls}
\m{U}_{t+1}&=\argmin_{\m{U}=\tilde{\m{U}}} \frac{1}{2}\|\m{A}-\tilde{\m{U}}\m{W}_t^T\|^2_F+\bm{\delta}(\m{U})+\frac{\rho}{2}\|\tilde{\m{U}}-\m{U}\|_2^2. %
\\
\m{W}_{t+1}&=\argmin_{\m{W}=\tilde{\m{W}}} \frac{1}{2}\|\m{A}-\m{U}_t\tilde{\m{W}}^T\|^2_F+\bm{\delta}(\m{W})+\frac{\rho}{2}\|\tilde{\m{W}}-\m{W}\|_2^2.%
\end{align}

This guarantees a monotonic decrease of the objective function $\|\m{A}-\tilde{\m{U}}_t\tilde{\m{W}}_t^T\|_F^2$. Each of these sub-problems is thus solved with ADMM separately, by alternating minimization steps of $\frac{1}{2}\|\m{A}-\tilde{\m{U}}\m{W}_t^T\|^2_F+\bar{\m{U}}^T(\tilde{\m{U}}-\m{U})+\frac{\rho}{2}\|\m{U}-\tilde{\m{U}}\|_2^2$ over $\tilde{\m{U}}$ (\textbf{\textit{i}}), with minimization steps of $\bm{\delta}(\m{U})+\frac{\rho}{2}\|\m{U}-\tilde{\m{U}}\|_2^2$ over $\m{U}$ (\textbf{\textit{ii}}), and gradient ascent steps (\textbf{\textit{iii}}) on the dual variable $\bar{\m{U}}\gets \bar{\m{U}}+(\tilde{\m{U}}-\m{U})$. A similar scheme is used for $\m{W}$ updates. Step (\textbf{\textit{i}}) is a simple convex quadratic program with equality constraints, whose KKT~\cite{karush1939minima,kuhn1951nonlinear} conditions yield a linear system with a Positive Semi-Definite (PSD) matrix. Step (\textbf{\textit{ii}}) is a simple projection of $\tilde{\m{U}}$ onto the convex set $\bm{\delta}^{-1}(\bm{0})$. Finally, step (\textbf{\textit{iii}}) is inexpensive.

Concretely, we solved the quadratic program using Conjugate Gradient, from \textit{jax.scipy.sparse.linalg.cg}. This indirect method only involves \textit{matrix-vector} products and can be more GPU-efficient than methods that are based on matrix factorization (such as Cholesky decomposition). Also, we re-implemented the pseudo code of~\cite{huang2016flexible} in \textit{Jax} for a fully GPU-compatible program. We used the primal variables $\m{U}_0,\m{W}_0$ returned by \textit{sklearn.decompose.nmf} as a \textit{warm start} for ADMM and observe that the high quality initialization of these primal variables considerably speeds up the convergence of the dual variables.

\subsubsection{Implicit differentiation}\label{app:craft:implicit}

The Lagrangian of the NMF problem reads $\mathcal{L}(\m{U},\m{W},\bar{\m{U}},\bar{\m{W}})=\frac{1}{2}\|\m{A}-\m{U}\m{W}^T\|_F^2-\bar{\m{U}}^T\m{U}-\bar{\m{W}}^T\m{W}$, with dual variables $\bar{\m{U}}$ and $\bar{\m{W}}$ associated to the constraints $\m{U}\geq 0, \m{W} \geq 0$. It yields a function $\bm{F}$ based on the KKT conditions~\cite{karush1939minima,kuhn1951nonlinear} whose optimal tuple $\m{U},\m{W},\bar{\m{U}},\bar{\m{W}}$ is a root.  
  
For single NNLS problem (for example, with optimization over $\m{U}$) the KKT conditions are:

\begin{equation} %
    \begin{cases}
    \nabla_{\m{U}}\left(\frac{1}{2}\|\m{A}-\tilde{\m{U}} \tilde{\m{W}}^T\|^2_F+\bar{\m{U}}^T(-\m{U})\right)    =0, \text{ stationarity,}\\
    -\m{U}\leq 0, \text{ primal feasability,}\\
    \bar{\m{U}} \odot \m{U}   =0, \text{ complementary slackness,}\\
    \bar{\m{U}}   \geq 0, \text{ dual feasability.}\\
\end{cases}
\label{apeq:craft:optimality_fun}
\end{equation}

By stacking the KKT conditions of the NNLS problems the we obtain the so-called \textit{optimality function} $\bm{F}$:

\begin{equation} %
    \bm{F}((\m{U},\m{W},\bar{\m{U}},\bar{\m{W}}),\m{A})=\begin{cases}
    (\m{U}\m{W}^T-\m{A})\m{W}-\bar{\m{U}}    ,& \\ %
    (\m{W}\m{U}^T-\m{A}^T)\m{U}-\bar{\m{W}}  ,& \\ %
    \bar{\m{U}} \odot \m{U}   ,& \\ %
    \bar{\m{W}} \odot \m{W}   .& \\ %
\end{cases}
\label{eq:craft:optimality_fun_2}
\end{equation}

The implicit function theorem~\cite{griewank2008evaluating} allows us to use implicit differentiation~\cite{krantz2002implicit,griewank2008evaluating,bell2008algorithmic} to efficiently compute the Jacobians $\frac{\partial \m{U}}{\partial \m{A}}$ and $\frac{\partial \m{W}}{\partial \m{A}}$ without requiring to back-propagate through each of the iterations of the NMF solver:
\begin{equation}
    \frac{\partial (\m{U},\m{W},\bar{\m{U}},\bar{\m{W}})}{\partial \m{A}}=-(\partial_1 \bm{F})^{-1}\partial_2 \bm{F}.
\end{equation}

Implicit differentiation requires access to the dual variables of the optimization problem in equation~\ref{eq:craft:nmf}, which are not computed by Scikit-learn's popular implementation. Scikit-learn uses Block coordinate descent algorithm~\cite{cichocki2009fast,fevotte2011algorithms}, with a randomized SVD initialization. Consequently, we leverage our implementation in Jax based on ADMM~\cite{boyd2011distributed}.

Concretely, we perform a two-stage backpropagation \textit{Jax (2)}$\to$\textit{Tensorflow (1)} to leverage the advantage of each framework. The lower stage (1) corresponds to feature extraction $\m{A}=\v{h}_l(\vx)$ from crops of images $\vx$, and upper stage (2) computes NMF $\m{A} \approx \m{U}\m{W}^T$.  
  
We use the \textit{Jaxopt}~\cite{blondel2021implicitdiff} library that allows efficient computation of $\frac{\partial (\m{U},\m{W},\bar{\m{U}},\bar{\m{W}})}{\partial \m{A}}=-(\partial_1 \bm{F})^{-1}\partial_2 \bm{F}$. The matrix $(\partial_1 \bm{F})^{-1}$ is never explicitly computed -- that would be too costly. Instead, the system $\partial_1 \bm{F}\frac{\partial (\m{U},\m{W},\bar{\m{U}},\bar{\m{W}})}{\partial \m{A}}=-\partial_2 \bm{F}$ is solved with Conjugate Gradient through the use of Jacobian Vector Products (JVP) $\bm{v}\mapsto (\partial_1 \bm{F})\bm{v}$.  
  
The chain rule yields:
$$\frac{\partial \m{U}}{\partial \vx}=\frac{\partial \m{A}}{\partial \vx}\frac{\partial \m{U}}{\partial \m{A}}.$$

Usually, most Autodiff frameworks (e.g Tensorflow, Pytorch, Jax) handle it automatically. Unfortunately, combining two of those framework raises a new difficulty since they are not compatible. Hence, we re-implement manually the two stages auto-differentiation.  
  
Since $r$ is far smaller ($r=25$ in all our experiments) than input dimension $\vx$ (typically $224\times 244$ for ImageNet images), back-propagation is the preferred algorithm in this setting over forward-propagation. We start by computing sequentially the gradients $\nabla_{\vx} \m{U}_i$ for all concepts $1\leq i\leq r$. This amounts to compute $\bm{v}=\nabla_{\m{A}} \m{U}_i$ with Implicit Differentiation in Jax, convert the Jax array $\bm{v}$ into Tensorflow tensor, and then to compute $\nabla_{\vx} \m{U}_i=\frac{\partial \m{A}}{\partial \vx}\nabla_{\m{A}} \m{U}_i=\nabla_{\vx} (\v{h}_l(\vx) \cdot \bm{v})$. The latter is easily done in Tensorflow. Finally we stack the gradients $\nabla_{\vx} \m{U}_i$ to obtain the Jacobian $\frac{\partial \m{U}}{\partial \vx}$.



\subsection{Sobol indices for concepts} \label{apdx:sobol}

We propose to formally derive the Sobol indices for the estimation of the importance of concepts.
Let us define a probability space  $(\Omega, \mathcal{F}, \mathbb{P})$ of possible concept perturbations. In order to build these concept perturbations, we start from an original vector of concepts coefficient\footnote{We choose to name $\vx$ the concept coefficient vector here, instead to avoid any confusion with $\v{u}$ that will be the set of indices.} $\v{x} \in \mathbb{R}^r$ and use i.i.d. stochastic masks $\rv{m} = (\r{m}_1, ..., \r{m}_r) \sim \mathcal{U}([0, 1]^r)$, as well as a perturbation operator $\bm{\tau}(\cdot)$ to create stochastic perturbation of $\vx$ that we call concept perturbation $\rvx = \bm{\tau}(\vx, \rv{m})$. 


Concretely, to create our concept perturbation we consider the inpainting function as our perturbation operator (as in \cite{ribeiro2016lime, petsiuk2018rise, fel2021sobol}) : $\bm{\tau}(\vx, \rv{m}) = \vx \odot \rv{m} + (\bm{1} - \rv{m}) \mu$ with $\odot$ the Hadamard product and $\mu \in \mathbb{R}$ a baseline value, here zero.
For the sake of notation, we will note $\pred$ the function mapping a random concept perturbation $\rvx$ from an intermediat layer to the output $\pred(\rvx)$ (e.g., the final layer if we do the concept extraction on the penultimate layer).
We denote the set $\mathcal{U} = \{1, ..., r\}$, $\bm{u}$ a subset of $\mathcal{U}$, its complementary $\sim \bm{u}$ and $\mathbb{E}(\cdot)$ the expectation over the perturbation space.
Finally, we assume that $\pred \in \mathbb{L}^2(\mathcal{F}, \mathbb{P})$ i.e. $|\mathbb{E}(\pred(\rvx))| < + \infty$.

The Hoeffding decomposition allows us to express the function $\pred$ into summands of increasing dimension, denoting $\pred_{\bm{u}}$ the partial contribution of the concepts $\rvx_{\bm{u}} = (\r{x}_i)_{i\in \bm{u}}$ to the score $\pred(\rvx)$:
\begin{equation}
    \label{eq:craft:anova}
    \begin{aligned}
    \bm{f}(\rvx) &= \bm{f}_{\emptyset}
     + \sum_i^r \bm{f}_i(\r{x}_i)
     + \sum_{1 \leqslant i < j \leqslant r} \bm{f}_{i,j}(\r{x}_i, \r{x}_j)
     + \cdots 
     + \bm{f}_{1,...,r}(\rvx) \\
    &= \sum_{\substack{\bm{u} \subseteq \mathcal{U}}} \bm{f}_{\bm{u}}(\rvx_{\bm{u}}).
    \end{aligned}
\end{equation}

Eq.~\ref{eq:craft:anova} consists of $2^r$ terms and is unique under the following orthogonality constraint:
\begin{equation}
    \label{eq:craft:anova_ortho}
    \begin{aligned}
    \forall (\bm{u},\bm{v}) \subseteq \mathcal{U}^2 \; s.t. \;  \bm{u} \neq \bm{v}, \;\; \mathbb{E}\big(\bm{f}_{\bm{u}}(\rvx_{\bm{u}}) \bm{f}_{\bm{v}}(\rvx_{\bm{v}})\big) = 0.
    \end{aligned}
\end{equation}

Furthermore, orthogonality yields the characterization $\bm{f}_{\bm{u}}(\rvx_{\bm{u}}) = \mathbb{E}(\bm{f}(\rvx)|\rvx_{\bm{u}}) - \sum_{\bm{v}\subset \bm{u}}\bm{f}_{\bm{v}}(\rvx_{\bm{v}})$ and allows us to decompose the model variance as:
\begin{equation}
    \label{eq:craft:var_decomposition}
    \begin{aligned}
        \V(\pred(\rvx)) &= \sum_i^r \V(\bm{f}_i(\r{x}_i)) 
        +\sum_{1 \leqslant i < j \leqslant r} \V(\bm{f}_{i,j}(\r{x}_i, \r{x}_j))
        + ... + \V(\bm{f}_{1,...,r}(\rvx)) \\
        &=\sum_{\substack{\bm{u} \subseteq \mathcal{U}}} \V(\bm{f}_{\bm{u}}(\rvx_{\bm{u}})).
        \end{aligned}
\end{equation}

Building from Eq.~\ref{eq:craft:var_decomposition}, it is natural to characterize the influence of any subset of concepts $\bm{u}$ as its own variance w.r.t. the total variance. This yields, after normalization by $\V(\bm{f}(\rvx))$, the general definition of Sobol' indices.
\begin{definition}[Sobol indices~\cite{sobol1993sensitivity}]
The sensitivity index $\mathcal{S}_{\bm{u}}$ which measures the contribution of the concept set $\rvx_{\bm{u}}$ to the model response $\bm{f}(\rvx)$ in terms of fluctuation is given by:
\begin{equation}\label{eq:craft:sobol_indice}
\begin{aligned}
    \mathcal{S}_{\bm{u}}  &= \frac{ \V(\bm{f}_{\bm{u}}(\rvx_{\bm{u}})) }{ \V(\pred(\rvx)) }\\
    &= \frac{ \V(\mathbb{E}(\bm{f}(\rvx) | \rvx_{\bm{u}})) - \sum_{\bm{v}\subset \bm{u}}\V(\mathbb{E}(\bm{f}(\rvx) | \rvx_{\bm{v}} ))}{ \V(\bm{f}(\rvx)) }.
\end{aligned}
\end{equation}
\end{definition}

Sobol indices give a quantification of the importance of any subset of concepts with respect to the model decision, in the form of a normalized measure of the model output deviation from $\bm{f}(\rvx)$. Thus, Sobol indices sum to one : $\sum_{\bm{u} \subseteq \mathcal{U}} \mathcal{S}_{\bm{u}} = 1$. 

\vspace{2mm}
Furthermore, the framework of Sobol' indices enables us to easily capture higher-order interactions between features. Thus, we can view the Total Sobol indices defined in \ref{eq:craft:total_sobol} as the sum of of all the Sobol indices containing the concept $i$ : $\mathcal{S}^{T}_i = \sum_{\bm{u} \subseteq \mathcal{U}, i \in \bm{u}} \mathcal{S}_{\bm{u}}$. Concretely, we estimate the total Sobol indices using the Jansen estimator~\cite{janon2014asymptotic} and Quasi-Monte carlo Sequence (Sobol $LP_{\tau}$ sequence).

\clearpage

\subsection{Human experiments}\label{app:craft:human-exp}

We first describe how participants were enrolled in our studies, then the general experimental design they went through.


\subsubsection{Utility evaluation}
\label{app:craft:utility}

\paragraph{Participants}
The participants that went through our experiments are users from the online platform Amazon Mechanical Turk (AMT), specifically, we recruit users with high qualifications (number of HIT completed $=5 000$ and HIT accepted $> 98 \%$). All participants provided informed consent electronically in order to perform the experiment ($\sim 5 - 8$ min), for which they received 1.4\$.\\

For the \textit{Husky vs. Wolf} scenario, $n=84$ participants passed all our screening and filtering process, respectively $n=32$ for CRAFT, $n=22$ for ACE and $n=22$ for CRAFTCO.


For the \textit{Leaves} scenario, after filtering, we analyzed data from $n=87$ participants, respectively $n=32$ for CRAFT, $n=24$ for ACE and $n=31$ for CRAFTCO.


For the \textit{"Kit Fox" vs. "Red Fox"} scenario, the results come from $n=79$ participants who passed all our screening processes, respectively $n=22$ for CRAFT, $n=31$ for ACE and $n=26$ for CRAFTCO.

\paragraph{General study design}
We followed the experimental design described in \autoref{sec:meta_pred}, in which explanations are evaluated according to their ability to help training participants at getting better at predicting their models' decisions on unseen images.

Each of those participants are only tested on a single condition to avoid possible experimental confounds. 

The main experiment is divided into 3 training sessions (with 5 training samples in each) each followed by a brief test. In each individual training trial, an image was presented with the associated prediction of the model, together with an explanation. After a brief training phase (5 samples), participants' ability to predict the classifier's output was evaluated on 7 new samples during a test phase. During the test phase, no explanation was provided.
We also use the reservoir that subjects can refer to during the testing phase to minimize memory load as a confounding factor.

We implement the same 3-stage screening process: First we filter participants not successful at the practice session done prior to the main experiment used to teach them the task, then we have them go through a quiz to make sure they understood the instructions. Finally, we add a catch trial in each testing phase --that users paying attention are expected to be correct on-- allowing us to catch uncooperative participants.

\begin{figure*}[hb]
    \centering
    \begin{subfigure}{0.9\textwidth}
        \centering
        \includegraphics[width=0.45\textwidth]{assets/craft/website/utility_husky_study.png}
        \includegraphics[width=0.45\textwidth]{assets/craft/website/utility_leaves_study.png}
        \caption{\textbf{Utility experiment.} Training trials taken from the \textit{Husky vs. Wolf} scenario (left) and the \textit{Leaves} scenario (right).}
        \label{fig:craft:website_utility}
    \end{subfigure}
\end{figure*}


\subsubsection{Validation of Recursivity}

\paragraph{Participants} Behavioral accuracy data were gathered from $n=73$ participants. All participants provided informed consent electronically in order to perform the experiment ($\sim 4 - 6$ min). The protocol was approved by the University IRB and was carried out in accordance with the provisions of the World Medical Association Declaration of Helsinki. 
For each of the 2 experiment tested, we had prepared filtering criteria for uncooperative people (namely based on time), but all participants passed these filters.

\paragraph{General study design}

For the first experiment -- consisting in finding the intruder among elements of the same concept and an element from a different concept (but of the same class, see Figure~\ref{fig:craft:website_intruder}) -- the order of presentation is randomized across participants so that it does not bias the results.
Moreover, in order to avoid any bias coming from the participants themselves (one group being more successful than the other) all participants went through both conditions of finding intruders in batches of images coming from either concepts or sub-concepts.
Concerning experiment 2, the order was also randomized (see Figure~\ref{fig:craft:website_choice}).

The participants had to successively find 30 intruders (15 block concepts and 15 block sub-concepts) for experiment 1 and then make 15 choices (sub-concept vs concept) for experiment 2, see Figure~\ref{fig:craft:website}.

The expert participants are people working in machine learning (researchers, software developers, engineers) and have participated in the study following an announcement in the authors' laboratory/company. The other participants (Laymen) have no expertise in machine learning.

\begin{figure*}[ht]
    \centering
    \begin{subfigure}{0.95\textwidth}
        \includegraphics[width=\textwidth]{assets/craft/figure_website.jpg}
        \caption{\textbf{Recursivity Experiment Website.}}
        \label{fig:craft:website}
      
    \end{subfigure}
    \begin{subfigure}{0.95\textwidth}
        \centering
        \includegraphics[width=0.32\textwidth]{assets/craft/website/intruder2.png}
        \includegraphics[width=0.32\textwidth]{assets/craft/website/intruder3.png}
        \includegraphics[width=0.32\textwidth]{assets/craft/website/intruder1.png}
        \caption{\textbf{Binary choice experiment.}}
        \label{fig:craft:website_intruder}
    \end{subfigure}
    
    \begin{subfigure}{0.95\textwidth}
        \centering
        \includegraphics[width=0.32\textwidth]{assets/craft/website/choice3.png}
        \includegraphics[width=0.32\textwidth]{assets/craft/website/choice1.png}
        \includegraphics[width=0.32\textwidth]{assets/craft/website/choice2.png}
        \caption{\textbf{Intruder experiment.}}
        \label{fig:craft:website_choice}
    \end{subfigure}
\end{figure*}

\clearpage

\subsection{Fidelity experiments}\label{app:craft:fidelity}

\begin{figure}[ht]
  \includegraphics[width=.99\linewidth]{assets/craft/deletion_full.jpg}
  \includegraphics[width=.99\linewidth]{assets/craft/insertion_full.jpg}
  \caption{ \textbf{(1) Deletion curves} for different concept extraction methods, Sobol outperforms TCAV not only for NMF to correctly estimate concept importance (lower is better). \textbf{(2) Insertion curves} for different concept extraction methods, Sobol outperforms TCAV to correctly estimate concept importance (higher is better).}
  \label{fig:craft:deletion_full}
\end{figure}

For our experiments on the concept importance measure, we focused on certain classes of ILSRVC2012~\cite{imagenet_cvpr09} and used a ResNet50V2~\cite{he2016deep} that had already been trained on this dataset. Just like in~\cite{ghorbani2017interpretation, zhang2021invertible}, we measure the insertion and deletion metrics for our concept extraction technique -- as well as concepts vectors extracted using PCA, ICA and RCA as dimensionality reduction algorithms, see Figure~\ref{fig:craft:deletion_full} -- and we compare them when we add/remove the concepts as ranked by the TCAV score~\cite{kim2018interpretability} and by the Sobol importance score. As originally explained in~\cite{petsiuk2018rise}, the objective of these metrics is to add/remove parts of the input according to how much an explainability method considers that it is influential and looking at the speed at which the logit for the predicted class increases/decreases.

In particular, for our experimental evaluations, we have randomly chosen 100000 images from ILSVRC2012~\cite{imagenet_cvpr09} and computed the deletion and insertion metrics for 5 different seeds -- for a total of half a million images. In Figure~\ref{fig:craft:deletion_full}, the shade around the curves represent the standard deviation over these 5 experiments.

\clearpage

\subsection{Sanity Check}
\label{app:craft:sanity-checks}

Following the work from~\cite{adebayo2018sanity}, we performed a sanity check on our method, by running the concept extraction pipeline on a randomized model. This procedure was performed on a ResNet-50v2 model with randomized weights. As showcased in Figure~\ref{fig:craft:sanity_check}, the concepts drastically differ from trained models, thus proving that CRAFT passes the sanity check.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{assets/craft/sanity_check/rand_c1.png}
    \includegraphics[width=0.95\linewidth]{assets/craft/sanity_check/rand_c2.png}
    \includegraphics[width=0.95\linewidth]{assets/craft/sanity_check/rand_c3.png}
    \caption{\textbf{Sanity check of the method:} we ran the method on a Resnet50 with randomized weights, and extracted the 3 most relevant concepts for the class `Chain saw'. When weights are randomized, concepts are mainly based on color histograms.}
    \label{fig:craft:sanity_check}
\end{figure}
