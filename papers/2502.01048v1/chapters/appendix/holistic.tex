\subsection{Attribution methods for Concepts}\label{sup:holistic:all_cams}

In the following section, we will re-derive the different attribution methods in the literature. We use the Xplique library and adapted each methods~\cite{fel2022xplique}.
We quickly recall that we seek to estimate the importance of each concept for a set of concept coefficients $\v{u} = (\v{u}_1, \ldots, \v{u}_k) \in \mathbb{R}^k$ in the concept basis $\m{V} \in \mathbb{R}^{p \times k}$. This concept basis is a re-interpretation of a latent space (in $\mathbb{R}^{p}$) and the function $\fb: \mathbb{R}^{p} \to \mathbb{R}$ is a signal used to compute importance from (e.g., logits value, cosine similarity with a sentence...). Each Attributions method will map a set of concept values to an importance score $\cam: \mathbb{R}^k \to \mathbb{R}^k$, a greater score $\cam(\v{u})_i$ indicates that a concept $\v{u}_i$ is more important. 

\textbf{Saliency (SA)}~\cite{simonyan2013deep} was originally a visualization technique based on the gradient of a class score relative to the input, indicating in an infinitesimal neighborhood, which pixels must be modified to most affect the score of the class of interest. In our case, it indicates which concept in an infinitesimal neighborhood has the most influence on the output:

$$ \cam^{(SA)}(\v{u}) = \nabla_{\v{u}} \fb(\v{u} V^\tr) .$$

\textbf{Gradient $\odot$ Input (GI)}~\cite{shrikumar2017learning} is based on the gradient of a class score relative to the input, element-wise with the input, it was introduced to improve the sharpness of the attribution maps. A theoretical analysis conducted by~\cite{ancona2017better} showed that Gradient $\odot$ Input is equivalent to $\epsilon$-LRP and DeepLIFT~\cite{shrikumar2017learning} methods under certain conditions -- using a baseline of zero, and with all biases to zero. In our case, it boils down to:

$$ \cam^{(GI)}(\v{u}) = \v{u} \odot \nabla_{\v{u}} \fb(\v{u} \m{V}^\tr) .$$

\textbf{Integrated Gradients (IG)}~\cite{sundararajan2017axiomatic} consists of summing the gradient values along the path from a baseline state to the current value. The baseline $\v{u}_0$ used is zero. This integral can be approximated with a set of $m$ points at regular intervals between the baseline and the point of interest. In order to approximate from a finite number of steps, we use a trapezoidal rule and not a left-Riemann summation, which allows for more accurate results and improved performance (see~\cite{sotoudeh2019computing} for a comparison). For all the experiments $m = 30$.

$$ \cam^{(IG)}(\v{u}) = (\v{u} - \v{u}_0) \int_0^1 \nabla_{\v{u}} \fb((\v{u}_0 + \alpha(\v{u} - \v{u}_0))\m{V}^\tr) \dif\alpha. $$

\textbf{SmoothGrad (SG)}~\cite{smilkov2017smoothgrad} is also a gradient-based explanation method, which, as the name suggests, averages the gradient at several points corresponding to small perturbations (drawn i.i.d from an isotropic normal distribution of standard deviation $\sigma$) around the point of interest. The smoothing effect induced by the average helps to reduce the visual noise, and hence improves the explanations. In our case, the attribution is obtained after averaging $m$ points with noise added to the concept coefficients. For all the experiments, we took $m = 30$ and $\sigma = 0.1$.

$$ \cam^{(SG)}(\v{u}) = \underset{\bm{\delta} \sim \mathcal{N}(0, \mathbf{I}\sigma)}{\mathbb{E}}(\nabla_{\v{u}} \fb( \v{u} + \bm{\delta}) ).
$$

\textbf{VarGrad (VG)}~\cite{hooker2018benchmark} was proposed as an alternative to SmoothGrad as it employs the same methodology to construct the attribution maps: using a set of $m$ noisy inputs, it aggregates the gradients using the variance rather than the mean. For the experiment, $m$ and $\sigma$ are the same as SmoothGrad. Formally:

$$ \cam^{(VG)}(\v{u}) = \underset{\bm{\delta} \sim \mathcal{N}(0, \mathbf{I}\sigma)}{\mathbb{V}}(\nabla_{\v{u}} \fb( \v{u} + \bm{\delta}) ).
$$

\textbf{Occlusion (OC)}~\cite{zeiler2013visualizing} is a simple -- yet effective -- sensitivity method that sweeps a patch that occludes pixels over the images using a baseline state and use the variations of the model prediction to deduce critical areas. In our case, we simply omit each concept one-at-a-time to deduce the concept's importance. For all the experiments, the baseline state $\v{u}_0$ was zero.

$$ \cam^{(OC)}(\v{u})_i = \fb(\v{u} \m{V}^\tr) - \fb(\v{u}_{[i = \v{u}_0]} \m{V}^\tr)  $$


\textbf{Sobol Attribution Method (SM)}~\cite{fel2021sobol} then used for estimating concept importance in \cite{fel2023craft} is a black-box attribution method grounded in Sensitivity Analysis. Beyond modeling the individual contributions of image regions, Sobol indices provide an efficient way to capture higher-order interactions between image regions and their contributions to a neural networkâ€™s prediction through the lens of variance. In our case, the score for a concept $\v{u}_i$ is the expected variance that would be left if all variables but $i$ were to be fixed : 

$$ \cam^{(SM)}(\v{u})_i = \frac{ \mathbb{E}( \mathbb{V}( \fb( (\v{u} \odot \mathbf{M} ) \m{V}^\tr ) | \mathbf{M}_{\sim i} ) ) }{ \mathbb{V}( \fb( (\v{u} \odot \mathbf{M} ) \m{V}^\tr)) } . $$

With $\mathbf{M} \sim \mathcal{U}([0, 1])^k$. For all the experiments, the number of designs was $32$ and we use the Jansen estimator of the Xplique library.

\textbf{HSIC Attribution Method (HS)}~\cite{novello2022making} seeks to explain a neural network's prediction for a given input image by assessing the dependence between the output and patches of the input. In our case, we randomly mask/remove concepts and measure the dependence between the output and the presence of each concept through $N$ binary masks. Formally:

$$ \cam^{(HS)}(\v{u}) = \frac{1}{(N-1)^2} \mathrm{Tr}(KHLH). $$

With $H, L, K \in \mathbb{R}^{N \times N}$ and $K_{ij} = k(\mathbf{M}_i, \mathbf{M}_j)$, $L_{ij} = l(\vy_i, \vy_j)$ and $H_{ij} = \delta(i=j)-N^{-1}$. Here, $k(\cdot, \cdot)$ and $l(\cdot, \cdot)$ denote the chosen kernels and $\mathbf{M} \sim \{0, 1\}^p$ the binary mask applied to the input $\v{u}$.

\textbf{RISE (RI)}~\cite{petsiuk2018rise} is also a black-box attribution method that probes the model with multiple version of a masked input to model the most important features. Formally, with $\bm{m} \sim \mathcal{U}([0, 1])^k$. : 

$$ \cam^{(RI)}_i(\v{u}) =  
\mathbb{E}(\fb( \v{u} \odot \bm{m} ) | \bm{m}_i = 1).
$$

\subsection{Closed-form of Attributions for the last layer}\label{sup:holistic:closed_form}

Without loss of generality, we focus on the decomposition in the last layer, that is $\v{a} = \v{u}\m{V}^\tr$ with parameters $(\m{W}, \bias)$ for the weight and the bias respectively, hence we obtain $\vy = (\v{u} \m{V}^\tr)\m{W} + \bias$ with $\m{W} \in \mathbb{R}^{p}$ and $\bias \in \mathbb{R}$.



We start by deriving the closed form of Saliency (SA) and naturally Gradient-Input (GI):

\begin{flalign*}
\cam^{(SA)}(\v{u}) 
&= \nabla_{\v{u}} \fb(\v{u} \m{V}^\tr)
= \nabla_{\v{u}} (\v{u} \m{V}^\tr \m{W} + \bias) &\\
&= \m{W}^\tr \m{V}&.
\end{flalign*}
\begin{flalign*}
\cam^{(GI)}(\v{u}) 
&= \nabla_{\v{u}} \fb(\v{u} \m{V}^\tr) \odot \v{u} 
= \nabla_{\v{u}} (\v{u} \m{V}^\tr \m{W} + \bias) \odot \v{u} &\\
&= \m{W}^\tr \m{V} \odot \v{u} &.
\end{flalign*}

We observe two different forms that will in fact be repeated for the other methods, for example with Integrated-Gradient (IG) which will take the form of Gradient-Input, while SmoothGrad (SG) will take the form of Saliency.

\begin{flalign*}
\cam^{(IG)}(\v{u})
 &= (\v{u} - \v{u}_0) \odot \int_0^1 \nabla_{\v{u}} \fb((\v{u}_0 + \alpha (\v{u} - \v{u}_0)) \m{V}^\tr) \dif \alpha &\\
 &= \v{u} \odot \int_0^1 \nabla_{\v{u}}((\alpha \v{u})) \m{V}^\tr\m{W} + \bias + (\alpha-1)\v{u}_0\m{V}^\tr\m{W}) \dif \alpha &\\
 &= \v{u} \odot \int_0^1 \alpha\m{W}^\tr \dif \alpha = \v{u} \odot \m{W}^\tr \m{V} \left[\frac{1}{2}\alpha^2\right]_0^1\\
 &= \frac{1}{2}\v{u} \odot \m{W}^\tr \m{V}.
\end{flalign*}

\begin{flalign*}
\cam^{(SG)}(\v{u})
&= \underset{\bm{\delta} \sim \mathcal{N}(0, \mathbf{I}\sigma)}{\mathbb{E}}(\nabla_{\v{u}} \fb( \v{u} + \bm{\delta}) ) 
= \underset{\bm{\delta} \sim \mathcal{N}(0, \mathbf{I}\sigma)}{\mathbb{E}}(\nabla_{\v{u}}( (\v{u} + \bm{\delta}) \m{V}^\tr\m{W} + \bias) ) & \\
& = \underset{\bm{\delta} \sim \mathcal{N}(0, \mathbf{I}\sigma)}{\mathbb{E}}(\nabla_{\v{u}}(\v{u} \m{V}^\tr\m{W})) & \\
& = \m{W}^\tr \m{V} &.
\end{flalign*}

The case of VarGrad is specific, as the gradient of a linear system being constant, its variance is null.

\begin{flalign*}        
\cam^{(VG)}(\v{u})
&= \underset{\bm{\delta} \sim \mathcal{N}(0, \mathbf{I}\sigma)}{\mathbb{V}}(\nabla_{\v{u}} \fb( \v{u} + \bm{\delta}) )
= \underset{\bm{\delta} \sim \mathcal{N}(0, \mathbf{I}\sigma)}{\mathbb{V}}(\nabla_{\v{u}} ( (\v{u} + \bm{\delta}) \m{V}^\tr \m{W} + \bias) ) & \\
&= \underset{\bm{\delta} \sim \mathcal{N}(0, \mathbf{I}\sigma)}{\mathbb{V}}(\m{W}^\tr \m{V}) &\\
&= 0&.
\end{flalign*}

Finally, for Occlusion (OC) and RISE (RI), we fall back on the Gradient Input form (with multiplicative and additive constant for RISE).

\begin{flalign*}
\cam^{(OC)}_i(\v{u})
&= \fb(\v{u} \m{V}^\tr) - \fb(\v{u}_{[i = \v{u}_0]} \m{V}^\tr)
= \v{u} \m{V}^\tr\m{W} + \bias - (\v{u}_{[i = \v{u}_0]} \m{V}^\tr\m{W} + \bias) & \\
& = (\sum_{j}^{r} \v{u}_j \m{V}_j^\tr)\m{W} - (\sum_{j \neq i}^{r} \v{u}_j \m{V}_j^\tr)\m{W} &\\
& = \v{u}_i \m{V}_i^\tr \m{W} &
\end{flalign*}
thus $\cam^{(OC)}(\v{u}) = \v{u} \odot \m{W}^\tr \m{V}$

\begin{flalign*}
\cam^{(RI)}_i(\v{u})
&= \mathbb{E}(\fb( \v{u} \odot \bm{m} ) | \bm{m}_i = 1)
= \mathbb{E}( (\v{u}\odot\bm{m}) \m{V}^\tr\m{W} + \bias | \bm{m}_i = 1) & \\
& = \bias + \sum_{j \neq i}^r \v{u}_j \mathbb{E}(\bm{m}_j) \m{V}_j^\tr\m{W} + \v{u}_i \m{V}_i^\tr\m{W} &\\
& = \bias + \frac{1}{2} (\v{u}\m{V}^\tr\m{W} + \v{u}_i \m{V}_i^\tr\m{W})&
\end{flalign*}

\subsection{Fidelity optimality}\label{sup:holistic:fidelity_theorem}

Before showing that some methods are optimal with regard to C-Deletion and C-Insertion, we start with a first metric that studies the fidelity of the importance of concepts: $\mu$Fidelity, whose definition we recall

$$
\mu F = \underset{\substack{S \subseteq \{1, \ldots, k\} \\ |S| = m} }{\rho}(
\sum_{i \in S} \cam(\v{u})_i,
\fb(\v{u}) - \fb(\v{u}_{[\v{u}_i = \v{u}_0, i \in S]})
)
$$

With $\rho$ the Pearson correlation and $\v{u}_{[\v{u}_i = \v{u}_0, i \in S]}$ means that all $i$ components of $\v{u}$ are set to zero.

\begin{theorem}[Optimal $\mu$Fidelity in the last layer]
When decomposing in the last layer,~\textbf{Gradient Input}, \textbf{Integrated Gradients}, \textbf{Occlusion}, and \textbf{Rise} yield the optimal solution for the $\mu$Fidelity metric.
In a more general sense, any method $\cam(\v{u})$ that is of the form
$\cam_{i}(\v{u}) = a (\v{u}_i\m{V}_i^\tr \m{W}) + b $ with $a \in \mathbb{R}^+, b \in \mathbb{R}$ yield the optimal solution, thus having a correlation of 1.
\end{theorem}
\begin{proof}
In the last layer case, $\mu$Fidelity boils down to:

\begin{flalign*}
\mu F &= \underset{\substack{S \subseteq \{1, \ldots, k\} \\ |S| = m} }{\rho}\big(\sum_{i \in S} \cam(\v{u})_i,
\v{u} \m{V}^\tr \m{W} + \bias - ( \sum_{i \notin S} \v{u}_i \m{V}_i^\tr \m{W}) - \bias
\big) & \\
&= \underset{\substack{S \subseteq \{1, \ldots, k\} \\ |S| = m} }{\rho}\big(\sum_{i \in S} \cam(\v{u})_i,
\sum_{i \in S} \v{u}_i \m{V}_i^\tr \m{W}
\big) &
\end{flalign*}

We recall that for \textbf{Gradient Input}, \textbf{Integrated Gradients}, \textbf{Occlusion}, $\cam_i(\v{u}) \propto \v{u}_i \m{V}_i^\tr \m{W}$, thus 
\begin{flalign*}
\mu F &= \underset{\substack{S \subseteq \{1, \ldots, k\} \\ |S| = m} }{\rho}\big(
\sum_{i \in S} \v{u}_i \m{V}_i^\tr \m{W},
\sum_{i \in S} \v{u}_i \m{V}_i^\tr \m{W}
\big) = 1 &
\end{flalign*}
For \textbf{RISE}, we get the following characterization:
\begin{flalign*}
\mu F &= \underset{\substack{S \subseteq \{1, \ldots, k\} \\ |S| = m} }{\rho}\big(
\sum_{i \in S} \bias + \frac{1}{2} (\v{u}\m{V}^\tr\m{W} + \v{u}_i \m{V}_i^\tr\m{W})
,
\sum_{i \in S} \v{u}_i \m{V}_i^\tr \m{W}
\big) & \\
&= \underset{\substack{S \subseteq \{1, \ldots, k\} \\ |S| = m} }{\rho}\big(
|S|(\bias + \frac{1}{2} (\v{u}\m{V}^\tr\m{W})) + 
\sum_{i \in S} \frac{1}{2} \v{u}_i \m{V}_i^\tr\m{W}
,
\sum_{i \in S} \v{u}_i \m{V}_i^\tr \m{W}
\big) & \\
&= \underset{\substack{S \subseteq \{1, \ldots, k\} \\ |S| = m} }{\rho}\big(
a(  
\sum_{i \in S} \v{u}_i \m{V}_i^\tr\m{W}) + b
,
\sum_{i \in S} \v{u}_i \m{V}_i^\tr \m{W}
\big)  = 1 & \\
\end{flalign*}

with $a = \frac{1}{2}, b = m(\bias + \frac{1}{2} (\v{u}\m{V}^\tr\m{W}))$. 

\end{proof}

\subsection{Optimality for C-Insertion and C-Deletion}\label{sup:holistic:matroid}

In order to prove the optimality of some attribution methods on the C-Insertion and C-Deletion metrics, we will use the Matroid theory of which we recall some fundamentals.




Matroids were introduced by Whitney in 1935~\cite{whitney1992abstract}. 
It was quickly realized that they unified properties of various domains such as graph theory, linear algebra or geometry. 
Later, in the '60s, a connection was made with combinatorial optimization, nothing that they also played a central role in combinatorial optimization. 

The power of this tool is that it allows us to show easily that greedy algorithms are optimal with respect to some criterion on a broad range of problems. Here, we show that insertion is a greedy algorithm (since the concepts inserted are chosen sequentially based on the model score).

For the rest of this section, we assume $E = \{ e_1, \ldots, e_k \}$ the set of the canonical vectors in $\mathbb{R}^k$, with $e_i$ being the element associated with the $i^{th}$ concept.

\begin{definition}[Matroid] A matroid $M$ is a tuple $(E, \mathcal{J})$, where E is a finite ground set and $\mathcal{J} \subseteq 2^E$ is the power set of $E$, a collection of independent sets, such that:

\begin{enumerate}
  \item $\mathcal{J}$ is nonempty, $\emptyset \in \mathcal{J}$.
  \item $\mathcal{J}$ is downward closed; i.e., if $S \in \mathcal{J}$ and $S' \subseteq S$, then $S' \in \mathcal{J}$ 
  \item If $S, S' \in \mathcal{J}^2$ and $|S| < |S'|$, then $\exists s \in S' \setminus S$ such that $S \cup \{s\} \in \mathcal{J}$
\end{enumerate}

\end{definition}

In particular, we will need uniform matroids: 

\begin{definition}[Uniform Matroid] 
\label{def:matroid}
Let $E$ be a set of size $k$ and let $n \in \{1, \ldots, k \}$. If $\mathcal{J}$ is the collection of all subsets of $E$ of size at most $n$, then $(E, \mathcal{J})$ is a matroid, called a uniform matroid and denoted $M^{(n)}$.
\end{definition}


Finally, we need to characterize the concept set chosen at each step.

\begin{definition}[Base of Matroid] 
Let $M = (E, \mathcal{J})$ be a matroid. A subset $B$ of $E$ is called a basis of $M$ if and only if:
\begin{enumerate}
  \item $B \in \mathcal{J}$
  \item $\forall e \in E \setminus B, ~ B \cup \{e\} \notin \mathcal{J}$
\end{enumerate}
Moreover, we denote $\mathcal{B}(M)$ the set of all the basis of $M$.
\end{definition}


At each step, the insertion metric selects the concepts of maximum score given a cardinality constraint. At each new step, the concepts from the previous step are selected and it add a new concept from the whole available set, the one not selected so far with the highest score.  
This criterion requires an additional ingredient: the \emph{weight} associated to each element of the matroid - here an element of the matroid is a concept.

\paragraph{Ponderated Matroid}

Let $M^{(n)} = (E, \mathcal{J})$ be a uniform matroid and $w : E \to \mathbb{R}$ a weighting function associated to an element of $E$ (a concept).
The goal of C-Insertion at step $n$ is to find a basis (a set of concepts) $B^\star$ subject to $|B| = n$, that maximizes the weighting function : 

$$
\forall B \in \mathcal{J}, ~~ \sum_{e \in B^\star} w(e) \geq \sum_{e \in B} w(e).
$$

Such a basis is called the basis of maximum weights (MW) of the weighted matroid $M^{(n)}$. We will see that the greedy algorithm associated with this weighting function gives the optimal solution to the MW problem on C-Insertion. First, let's define the \emph{Greedy algorithm}.

\begin{algorithm}[ht]
\caption{Greedy algorithm}\label{alg:greedy_matroide}
\begin{algorithmic}
  \REQUIRE A $n$-uniform weighted matroid $M^{(n)} = (E, \mathcal{J}, w)$
  \STATE Sort the concepts by their weight $w(e_i)$ in non-increasing order, and store them in a list $\bar{e}$ such that~${\forall (i, j) \subseteq \{1, \ldots, k\}^2, w(\bar{e}_i) \geq w(\bar{e}_j) ~ \text{if} ~ i < j}$.
  \STATE $B^{\star} = \{\}$
  \FOR{$k = 1$ to $n$}
    \STATE $B^{\star} = B^{\star} \cup \bar{e}_k$ %
  \ENDFOR
  \STATE \textbf{Return} $B^{\star}$
\end{algorithmic}
\end{algorithm}


\begin{theorem}[Greedy Algorithm is an optimal solution to MW.] Let $M = (E, \mathcal{J}, w)$ a weighted matroid. The greedy Algorithm~\ref{alg:greedy_matroide} returns a maximum basis of $M$.
\end{theorem}

\begin{proof}
First, by definition, $B^\star$ is a basis and thus an independent set, i.e., $B^\star \in \mathcal{B}(M)$ (as $\forall (e,e') \in E^2, ~ \langle e,e' \rangle = 0$).
Now, suppose by contradiction that there exists a base $B'$ with a weight strictly greater than $B^\star$. We will obtain a contradiction with respect to the augmentation axiom of the matroid definition.
Let $e_1, \ldots, e_k$ be the elements of $M$ sorted such that $w(e_i) > w(e_j)$ whenever $i < j$. 
Let $n$ be the rank of our weighted uniform matroid $M^{(n)}$. 
Then we can write $B^\star = (e_{i_1}, \ldots, e_{i_n})$ and $B' = (e_{j_1}, \ldots, e_{j_n})$ with $j_k < j_l$ and $i_k < i_l$ for any $k < l$.

Let $\ell$ be the smallest positive integer such that $i_\ell$ > $j_\ell$. In particular, $\ell$ exists and is at most $n$ by assumption. Consider the independent set $S_{\ell-1} = \{e_{i_1}, \ldots e_{\ell-1}\}$ (in particular, $S_{\ell-1} = \emptyset$ if $\ell =1$). According to the augmentation axiom (Definition \ref{def:matroid}, I3), there exist $k \in \{1, \ldots, \ell \}$ such that $S_{\ell-1} + e_{j_k} \in \mathcal{J}$ and $e_{j_k} \notin S_{\ell-1}$. However, $j_k \leq j_\ell < i_\ell$, thus $w(e_{j_k}) \leq w(e_{j_\ell}) <w(e_{i_\ell})$. This contradicts the definition of the greedy algorithm.
\end{proof}

Now, we notice that for the last layer, Insertion is a weighted matroid. We insist that this result is \emph{only true for the concepts in the penultimate layer}, as our demonstrations rely on the linearity of the decomposition. Here, the weight is given by the score of the model, which is a linear combination of concepts.  


\begin{theorem}[Optimal Insertion in the last layer]
When decomposing in the last layer,~\textbf{Gradient Input}, \textbf{Integrated Gradients}, \textbf{Occlusion}, and \textbf{Rise} yield the optimal solution for the C-Insertion metric.
In a more general sense, any method $\cam(\v{u})$ that  satisfies the condition 
$\forall (i, j) \in \{1, \ldots, k\}^2, 
(\v{u} \odot \e_i) \m{V}^\tr\m{W} \geq (\v{u} \odot \e_j) \m{V}^\tr \m{W}
\implies 
\cam(\v{u})_i \geq \cam(\v{u})_j 
$ yield the optimal solution.
\end{theorem}

\begin{proof}
Each $n$ step of the C-Insertion algorithm corresponds to the $n$-uniform weighted matroid with weighting function $w(e_i) = (\v{u} \odot e_i) \m{V}^\tr\m{W} + b = \v{u}_i \m{V}^\tr\m{W} + b$. Therefore, any $\cam(\cdot)$ method that produces the same ordering as $w(\cdot)$ will yield the optimal solution. 
It easily follows that \textbf{Gradient Input}, \textbf{Integrated Gradients}, \textbf{Occlusion} are optimal as they all boil down to $\cam_i(\v{u}) = \v{u}_i \m{V}^\tr\m{W}+b$.
Concerning RISE, suppose that $w(e_i) \geq w(e_j)$, then $\v{u}_i \m{V}_i^\tr\m{W} + b \geq \v{u}_j \m{V}_j^\tr\m{W} + b$, and  
$\cam_i^{(RI)}(\v{u}) - \cam_j^{(RI)}(\v{u})
= \bias + \frac{1}{2} (\v{u}\m{V}^\tr\m{W} + \v{u}_i \m{V}_i^\tr\m{W}) - \bias + \frac{1}{2} (\v{u}\m{V}^\tr\m{W} + \v{u}_j \m{V}_j^\tr\m{W})
= \v{u}_i \m{V}_i^\tr\m{W} - \v{u}_j \m{V}_j^\tr\m{W}
\geq 0.
$ Thus, RISE importance will order in the same manner and is also optimal.
\end{proof}

\begin{corollary}[Optimal Deletion in the last layer]
When decomposing in the last layer,~\textbf{Gradient Input}, \textbf{Integrated Gradients}, \textbf{Occlusion}, and \textbf{Rise} yield the optimal solution for the C-Deletion metric.
\end{corollary}
\begin{proof}
It is simply observed that the C-Deletion problem seeks a minimum weight basis and corresponds to the same weighted matroid with weighting function $w'(\cdot) = -w(\cdot)$.
\end{proof}


\subsection{Sparse Autoencoder}

As a remainder, a general method (as it encompasses both PCA and K-means) to obtain the loading-dictionary pair and achieve a matrix reconstruction $\mathbf{A} = \mathbf{U} \mathbf{V}^\tr$ is to train a neural network to obtain $\mathbf{U}$ from $\mathbf{A}$ such that the reconstruction of $\mathbf{A}$ is linear in $\mathbf{U}$. This can be formally represented as:

$$
(\bm{\psi}^\star, \mathbf{V}^\star) = \arg\min_{\bm{\psi},\mathbf{V}} \| \mathbf{A} - \bm{\psi}(\mathbf{A}) \mathbf{V}^\top \|_F^2
$$

Here, $\mathbf{U}^\star = \bm{\psi}^\star(\mathbf{A}).$ An interesting characteristic of NMF and K-means is the non-linear relationship between $\mathbf{A}$ and $\mathbf{U}$. Specifically, the transformation from $\mathbf{A}$ to $\mathbf{U}$ is non-linear, while the transformation from $\mathbf{U}$ to $\mathbf{A}$ is linear, as explained in \cite{fel2022xplique}, which need to introduce a method based on implicit differentiation to obtain the gradient of $\mathbf{U}$ with respect to $\mathbf{A}$. Indeed, the sequence of operations to optimize $\mathbf{U}$ causes us to lose information about which elements of $\mathbf{A}$ contributed to obtaining $\mathbf{U}$. We believe that this non-linear relationship (absent in PCA) may be an essential ingredient for effective concept extraction.

Finally, as described in this article, other characteristics that appear to make it interpretable include its compositionality (due to non-extreme sparsity), good reconstruction, and positivity, which aids in interpretation. Thus, the architecture of $\bm{\psi}$ used for Figure~\ref{fig:holistic:qualitative_comparison} consists of a sequence of dense layers and batch normalization with ReLU activation to obtain positive scores and sparsity similar to NMF, without imposing constraints on $\mathbf{V}$. More formally, $\bm{\psi}$ is a sequence of layers as follows:

$$
\textsc{Dense(128) - BatchNormalization - ReLU}
$$
$$
\textsc{Dense(64) - BatchNormalization - ReLU}
$$
$$
\textsc{Dense(10) - BatchNormalization - ReLU}
$$

While the vector $\m{V}$ is initialized using a truncated SVD~\cite{fathi2023initialization}. We used Adam optimizer\cite{kingma2014adam} with a learning rate of $1e^{-3}$. However, it's worth noting that there is a wealth of literature on dictionary learning that remains to be explored for the task of concept extraction~\cite{dumitrescu2018dictionary}.








