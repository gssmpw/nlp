






In this section, we provide additional results for logit and internal feature visualizations, and feature inversion. 

For all of the following visualizations, we used the same parameters as in the main paper.
For the feature visualizations derived from~\cite{olah2017feature}, we used all 10 transformations set from the Lucid library\footnote{\href{https://github.com/tensorflow/lucid}{https://github.com/tensorflow/lucid}}.
For \magfv, $\augmentation$ only consists of two transformations; first we add uniform noise $\bm{\delta} \sim \mathcal{U}([-0.1, 0.1])^{W \times H}$ and crops and resized the image with a crop size drawn from the normal distribution $\mathcal{N}(0.25, 0.1)$, which corresponds on average to 25\% of the image.
We used the NAdam optimizer \cite{dozat2016incorporating} with a $lr=1.0$ and $N = 256$ optimization steps. Finally, we used the implementation of \cite{olah2017feature} and CBR which are available in the Xplique library~\cite{fel2022xplique} \footnote{\href{https://github.com/deel-ai/xplique}{https://github.com/deel-ai/xplique}} which is based on Lucid.

\subsubsection{Logit and Internal State Visualization}\label{app:maco:fviz}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.99\textwidth]{assets/maco/logits_qualitative.jpg}
    \caption{\textbf{Feature visualizations on FlexiViT, ViT and ResNet50.} We compare the feature visualizations from \magfv~generated for \textbf{(a)} FlexiViT, \textbf{(b)} ViT and \textbf{(c)} ResNet50 on a set of different classes from ImageNet. We observe that the visualizations get more abstract as the complexity of the model increases.}
    \label{fig:supp-qualitative}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.99\textwidth]{assets/maco/internal_our.jpg}
    \caption{\textbf{Logits and internal representation of a ViT.} Using \magfv, we maximize the activations of specific channels in different blocks of a ViT, as well as the logits for 4 different classes.}
    \label{fig:supp-internal}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.99\textwidth]{assets/maco/hue_inv.jpg}
    \caption{\textbf{Hue invariance.} Through feature visualization, we are able to determine the presence of hue invariance on our pre-trained ViT model manisfesting itself through phantom objects in them. This can be explained the data-augmentation that is typically employed for training these models.}
    \label{fig:hue-inv}
\end{figure}

\subsubsection{Feature Inversion}\label{app:maco:inversion}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.99\textwidth]{assets/maco/inversion.jpg}
    \caption{\textbf{Feature inversion and Attribution-based transparency.} We performed feature inversion on the images on the first column to obtain the visualizations (without transparency) on the second column. During the optimization procedure, we saved the intensity of the changes to the image in pixel space, which we showcase on the third column, we used this information to assign a transparency value, as exhibited in the final column.}
    \label{fig:supp-inv}
\end{figure}

\subsection{Human psychophysical study}\label{sup:maco:psychophysics}

To evaluate \magfv~'s ability to improve humans' causal understanding of a CNN's activations, we conducted a psychophysical study closely following the paradigm introduced in \cite{zimmermann2021well}. In this paradigm, participants are asked to predict which of two query inputs would be favored by the model (i.e., maximally activate a given unit), based on example "favorite" inputs serving as a reference (i.e., feature visualizations for that unit). The two queries are based on the same natural image, but differ in the location of an occludor which hides part of the image from the model.

\paragraph{Participants.} We recruited a total of 191 participants for our online psychophysics study using Prolific (www.prolific.com) [September 2023]. As compensation for their time (roughly 7 minutes), participants were paid 1.4\$. Of those who chose to disclose their age, the average age was 39 years old ($SD = 13$). Ninety participants were men, 86 women, 8 non-binary and 7 chose not to disclose their gender. The data of 17 participants was excluded from further analyses because they performed significantly below chance ($p < .05$, one-tailed).

\paragraph{Design.} Participants were randomly assigned to one of four Visualization conditions: Olah~\cite{olah2017feature}, \magfv~with mask, \magfv~without mask, or a control condition in which no visualizations were provided. Furthermore, we varied Network (VGG16, ResNet50, ViT) as a within-subjects variable. The specific units whose features to visualize were taken from the output layer, meaning they represented concrete classes. The classes were: Nile crocodile, peacock, Kerry Blue Terrier, Giant Schnauzer, Bernese Mountain Dog, ground beetle, ringlet, llama, apiary, cowboy boot, slip-on shoe, mask, computer mouse, muzzle, obelisk, ruler, hot dog, broccoli, and mushroom. For every class, we included three natural images to serve as the source image for the query pairs. This way, a single participant would see all 19 classes crossed with all 3 networks, without seeing the same natural image more than once (which image was presented for which network was randomized across participants). The main experiment thus consisted of 57 trials, with a fully randomized trial order.

\paragraph{Stimuli.} The stimuli for this study included 171 ((4-1)x3x19) reference stimuli, each displaying a 2x2 grid of feature visualizations, generated using the respective visualization method. The query pairs were created from each of the 57 (19x3) source images by placing a square occludor on them. In one member of the pair, the occludor was placed such that it minimized the activation of the unit. In the other member of the pair, the occludor was placed on an object of a different class in the same image or a different part of the same object. Here, we deviated somewhat from the query geneation in \cite{zimmermann2021well}, where the latter occludor was placed where it maximized the activation of the unit. However, we observed that this often resulted in the occludor being on the background, making the task trivial. Indeed, a pilot study ($N=42$) we ran with such occludor placement showed that even the participants in the control condition were on average correct in $83\%$ of the trials. 

\paragraph{Task and procedure.} The protocol was approved by the University IRB and was carried out in accordance with the provisions of the World Medical Association Declaration of Helsinki. Participants were redirected to our online study through Prolific and first saw a page explaining the general purpose and procedure of the study (Fig.~\ref{fig:psychophysics-welcome}). Next, they were presented with a form outlining their rights as a participant and actively had to click ``I agree'' in order to give their consent. More detailed instructions were given on the next page (Fig.~\ref{fig:psychophysics-instructions}, Fig.~\ref{fig:psychophysics-instructions-control}). Participants were instructed to answer the following question on every trial: ``Which of the two query images is more favored by the machine?''. The two query images were presented on the right-hand side of the screen. The feature visualizations were displayed on the left-hand side of the screen (Fig.~\ref{fig:psychophysics-trial}). In the control condition, the left-hand side remained blank (Fig.~\ref{fig:psychophysics-trial-control}). Participants could make their response by clicking on the radio button below the respective query image. They first completed a practice phase, consisting of six trials covering two additional classes, before moving on to the main experiment. For the practice trials, they received feedback in the form of a green (red) frame appearing around their selected query image if they were correct (incorrect). No such feedback was given during the main experiment.

\paragraph{Analyses and results.} We analyzed the data through a logistic mixed-effects regression analysis, with trial accuracy (1 vs. 0) as the dependent variable. The random-effects structure included a by-participant random intercept and by-class random intercept. We compared two regression models, both of which had Visualization and Network as a fixed effect, but only one also fitted an interaction term between the two. Based on the Akaike Information Criterion (AIC), the former, less complex model was selected ($AIC=11481 vs. 11482$). Using this model, we then analyzed all pairwise contrasts between the levels of the Visualization variable. We found that the logodds of choosing the correct query were overall significantly higher in both \magfv~conditions compared to the control condition: $\beta_{\magfv~Mask}-\beta_{Control} = 0.69, SE=0.13, z=5.38, p<.0001;\beta_{\magfv~NoMask}-\beta_{Control} = 0.92, SE=0.13, z=7.07, p<.0001.$ Moreover, \magfv~visualizations helped more than Olah visualizations: $\beta_{\magfv~Mask}-\beta_{Olah} = 0.43, SE=0.13, z=3.31, p=.005;\beta_{\magfv~NoMask}-\beta_{Olah} = 0.66, SE=0.13, z=4.99, p<.0001.$ No other contrasts were statistically significant (at a level of $p < .05$). $P$-values were adjusted for multiple comparisons with the Tukey method. Finally, we also examined the pairwise contrasts for the Network variable. We found that ViT was the hardest model to interpret overall: $\beta_{ResNet50}-\beta_{ViT} = 0.49, SE=0.06, z=8.65, p<.0001;\beta_{VGG16}-\beta_{ViT} = 0.35, SE=0.06, z=6.38, p<.0001.$ There was only marginally significant evidence that participants could better predict ResNet50's behavior in this task than VGG16: $\beta_{ResNet50}-\beta_{VGG16} = 0.13, SE=0.06, z=2.30, p=0.056.$

Taken together, these results suggest that \magfv~indeed helps humans causally understand a CNN's activations and that it outperforms Olah's method \cite{olah2017feature} on this criterion.

\clearpage

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.99\textwidth]{assets/maco/screenshots_psychophysics/welcome_crop.png}
    \caption{\textbf{Welcome page.} This is a screenshot of the first page participants saw when entering our online psychophysics study.}
    \label{fig:psychophysics-welcome}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.99\textwidth]{assets/maco/screenshots_psychophysics/instructions_crop.png}
    \caption{\textbf{Instructions page.} After providing informed consent, participants in our online psychophysics task received more detailed instructions, as shown here.}
    \label{fig:psychophysics-instructions}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.99\textwidth]{assets/maco/screenshots_psychophysics/instructions_control_crop.png}
    \caption{\textbf{Instructions page for control condition.} After providing informed consent, participants in our online psychophysics task received more detailed instructions, as shown here. If they were randomly assigned to the control condition, they were informed that they would not see examples of the machine's favorite images. }
    \label{fig:psychophysics-instructions-control}
\end{figure}


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.99\textwidth]{assets/maco/screenshots_psychophysics/trial_crop.png}
    \caption{\textbf{Example trial.} On every trial of our psychophysics study, participants were asked to select which of two query images would be favored by the machine. They were shown examples of the machine's favorite inputs (i.e., feature visualizations) on the left side of the screen.}
    \label{fig:psychophysics-trial}
\end{figure}


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.99\textwidth]{assets/maco/screenshots_psychophysics/trial_control_crop.png}
    \caption{\textbf{Example trial in the control condition.} On every trial of our psychophysics study, participants were asked to select which of two query images would be favored by the machine. In the control condition, they were not shown examples of the machine's favorite inputs and the left side of the screen remained empty.}
    \label{fig:psychophysics-trial-control}
\end{figure}
