
\section{Human experiments}
\label{ap:protocole}
\subsection{Experimental design}
Figure \ref{fig:design} summarizes the experimental design used for our experiments. The participants that went through our experiments are users from the online platform Amazon Mechanical Turk (AMT). Through this platform, users stay anonymous, hence, we do not collect any sensitive personal information about them. We prioritized users with a Master qualification (which is a qualification attributed by AMT to users who have proven to be of excellent quality) or normal users with high qualifications (number of HIT completed $=10 000$ and HIT accepted $> 98 \%$). 

Before going through the experiment, participants are asked to read and agree to a consent form, which specifies: the objective and procedure of the experiment, as well as the time expected to completion ($\sim 5$ - $8$ min) with the reward associated ($\$1.4$), and finally, the risk, benefits, and confidentiality of taking part in this study. 
There are no anticipated risks and no direct benefits for the participants taking part in this study.


\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.85\textwidth]{assets/metapred/design.png}
    \caption{\textbf{Experimental design.} 
    First, every participant goes through a practice session (fig \ref{fig:practice}) to make sure they understand how to use attribution methods to infer the rules used by a model, and a quiz (fig \ref{fig:quiz}) to make sure they actually read and understand the instructions. Then, participants are split into the different conditions -- every participant will only go through one condition. The 3 possible conditions are: an Explanation condition where an explanation is provided to human participants during their training phase, a Baseline condition where no explanation was provided to the human participants, and a Control condition where a non-informative explanation was provided.
    The main experiment was divided into 3 training sessions each followed by a brief test. In each individual training trial, an image was presented with the associated prediction of the model, either alone for the baseline condition or together with an explanation for the experimental and control condition. After a brief training phase (5 samples), participants' ability to predict the classifier's output was evaluated on 7 new samples (only the image, no explanation) during a test phase. To filter out uncooperative participants we also add a catch trial (fig \ref{fig:catch}) in each test session.}
    \label{fig:design}
\end{figure*}

\paragraph{Controlling for prior class knowledge}

To control for users' own semantic knowledge, we balanced the samples shown to participants so that the classifiers were correct/incorrect 50\% of the time. This way, the baseline (participants who try to simply predict the true class label of an image as opposed to learning to predict the model's outputs) is at 50\%. Any higher score reflects a certain understanding of the rules used by the model.


\subsection{Pruning out uncooperative participants}

\paragraph{3-stage screening proccess.}

To prune out uncooperative participants, we subjected them to a 3-stage screening process. First, participants completed a short practice session to make sure they understood the task and how to use the attribution methods to infer the rules used by the model (fig \ref{fig:practice}). 
Second, we asked participants to answer a few questions regarding the instructions provided to make sure they actually read and understood them (fig \ref{fig:quiz}). 
Third, during the main experiment, we took advantage of the reservoir to introduce a catch trial (fig \ref{fig:catch}). The reservoir is the place where we store the training example of the current session, which can be accessed during the testing phase. We added a trial in the testing phase of each session where the input image corresponded to one of the training samples used in the current session: since the answer is still on the screen (or a scroll away) we expect participants to be correct on these catch trials. Participants that failed any of the 3 screening processes were excluded from further analysis. 


\begin{figure*}
    \centering
    \includegraphics[width=0.65\textwidth]{assets/metapred/figpractice.png}
    \caption{\textbf{Practice session.} Through a practice session, which is a simplified version of the main experiment, we evaluate if users understand how to read and use explanations. Participants that failed to predict correctly any of the 5 cat test images on the first try were excluded from further analysis.}
    \label{fig:practice}
\end{figure*}

\clearpage
\begin{figure*}
    \centering
    \includegraphics[width=0.7\textwidth]{assets/metapred/figquiz.png}
    \caption{\textbf{Quiz.} Through a quiz, we make sure that users read and understood the instructions. Participants that did not answer correctly every question on the first try were excluded from further analysis.}
    \label{fig:quiz}
\end{figure*}

\subsection{More results}
\paragraph{Reaction time.}
We explored whether the usefulness of a method is reflected in the reaction time of participants -i.e., the more useful the explanation the faster the participants are able to grasp the strategy of the model-. Table \ref{tab:time} shows the reaction time of participants across methods, across datasets. We do not find any trend linking reaction time with usefulness.
\begin{table}[h]
\vspace{2mm}
\centering
\begin{tabular}{lccc}
\toprule
 Method & \textit{Husky vs. Wolf} & \textit{Leaves} & \textit{ImageNet} \\
\midrule
Saliency~\cite{simonyan2014deep}                  & \underline{207.7} & \textbf{212.9} & 202.3 \\
Integ.-Grad.~\cite{sundararajan2017axiomatic}     & 213.1 & 216.5 & 218.5 \\
SmoothGrad~\cite{smilkov2017smoothgrad}           & 215.8 & \textbf{268.8} & 243.9 \\
GradCAM~\cite{selvaraju2017gradcam}               & \textbf{168.9} & 154.6 & 268.9\\
Occlusion~\cite{zeiler2014visualizing}            & 221.2 & 229.2 & 274.4 \\
Grad.-Input~\cite{shrikumar2017learning}               & \underline{210.4} & \underline{238.1} & 208.0 \\
\bottomrule
\end{tabular}
\vspace{2mm}
\caption{\textbf{ Average total \textit{time} per method per dataset (in second).} For each dataset, we \textbf{bold} the most useful method, and we \underline{underline} the least useful method.}
\label{tab:time}
\vspace{-2mm}
\end{table}
\clearpage


\begin{figure*}
    \centering
    \includegraphics[width=0.7\textwidth]{assets/metapred/catch.png}
    \caption{\textbf{Catch trial.} We use a reservoir (to store all the examples of the current training session) that participants can refer to during the testing phase to minimize memory load. At the top of the screen is the reservoir, at the bottom of the screen is a trial from the testing phase. We take advantage of the reservoir to introduce a catch trial. We added a trial in the testing phase of each session where the input image corresponded to one of the training samples used in the current session: since the answer is still on the screen (or a scroll away) we expect participants to be correct on these catch trials. Participants that failed any of the 3 catch trials (one per session) were excluded from further analysis.}
    \label{fig:catch}
\end{figure*}
\clearpage

\section{Why do the best methods for the use cases Bias detection and Identifying an expert strategy (leaves) differ?}
\label{ap:differ}

The most interesting case is Saliency, which is the worst method on the bias dataset but the best on the “leaves” dataset. On the bias dataset, the model seems to focus on the background (i.e., a coarse feature), and on the “leaves'' dataset the model seems to focus either on the margin or on the vein of the leaf (i.e., very fine features). We hypothesize that different methods suit different granularity of features (coarse vs fine). \cite{smilkov2017smoothgrad}~make the hypothesis that “the saliency maps are faithful descriptions of what the network is doing” but because “the derivative of the score function with respect to the input [is] not [...] continuously differentiable”, the saliency map can appear noisy. Because of this local discontinuity of the gradient, a large patch of important pixels is often portrayed in the saliency map as a collection of smaller patches of important pixels (i.e., a coarse feature vs multiple individual fine features) which can make it hard to identify if the strategy is the coarse feature or a more complex interaction of the smaller features. In the bias dataset, because the model relies on the background, the Saliency maps appear very noisy and the explanation ends-up not being useful. We note that SmoothGrad, which proposes to fix that discontinuity, is useful. On the other hand, on the leaves dataset, the model uses very fine features, therefore the Saliency maps suffer less from the discontinuity, it does not appear noisy, Saliency is useful. We also note that in this case, SmoothGrad is not better than Saliency, which can arguably be attributed to the fact that we do not need to fix the discontinuity of the gradient. Conversely, because the granularity of both Grad-CAM (the feature map is much smaller than image size) and Occlusion (the patch size is much bigger than a pixel) is too high, the heatmaps they offer on the “leaves” dataset are too coarse to specifically highlight the fine features and it seems to take more time for the subjects to pick-up on them. But on the biased dataset, Grad-CAM and Occlusion are the best performing methods.


\section{Why do attribution methods fail?}

\subsection{Faithfulness}

While the Deletion\cite{petsiuk2018rise} measure is the most commonly used faithfulness metric, for completeness we also consider 2 others faithfulness metric available in the Xplique library\cite{fel2022xplique}: Insertion\cite{petsiuk2018rise} and $\mu$Fidelity\cite{aggregating2020}. 
Fig \ref{fig:insertion_mufidelity} shows the correlation between either measure and our \metric. We find them to be no better predictor of the practical usefulness of attribution methods than the Deletion measure. \\

\begin{figure*}[h]
    \includegraphics[width=0.48\textwidth]{assets/metapred/overall_correlation_insertion.png}
    \includegraphics[width=0.48\textwidth]{assets/metapred/overall_correlation_mu_fidelity.png}
    \caption{\textbf{\metric~vs Insertion correlation \& \metric~vs $\mu$Fidelity correlation}
        The results suggest that every faithfulness metrics tested are poor predictors of the practical usefulness of attribution methods. 
        Concerning the ImageNet dataset (triangle marker), the \metric~scores are insignificant since none of the methods improves the baseline.
        }
    \label{fig:insertion_mufidelity}
    \vspace{-4mm}
\end{figure*}



\subsection{Perceptual Similarity}

\begin{table}[h]
\vspace{2mm}
\centering
\begin{tabular}{lccc}
\toprule
 Method & \textit{Husky vs. Wolf} & \textit{Leaves} & \textit{ImageNet} \\
\midrule
Saliency~\cite{simonyan2014deep}                  & 0.304 & 0.334 & \textbf{0.378} \\ 
Integ.-Grad.~\cite{sundararajan2017axiomatic}     & 0.292 & \textbf{0.411} & \textbf{0.388} \\
SmoothGrad~\cite{smilkov2017smoothgrad}           & 0.285 & 0.286 & \textbf{0.384} \\
GradCAM~\cite{selvaraju2017gradcam}               & 0.241 & 0.312 & \textbf{0.38} \\
Occlusion~\cite{zeiler2014visualizing}            & 0.282 & 0.277 & \textbf{0.41} \\
Grad.-Input~\cite{shrikumar2017learning}               & 0.309 & \textbf{0.44} & \textbf{0.378} \\
\bottomrule
\end{tabular}
\vspace{2mm}
\caption{\textbf{\textit{Perceptual Similarity} scores.} The perceptual similarity of highlighted regions by a given attribution method for both classes is measured, for each method, for each dataset. The perceptual similarity scores that are higher than $0.378$ (the minimum score on ImageNet) are \textbf{bolded}. Higher is more similar.}
\label{tab:similarity}
\vspace{-2mm}
\end{table}
Tab \ref{tab:similarity} shows the Perceptual Similarity scores obtained for each method, on every dataset. We observe that on ImageNet, where attribution methods do not help, the perceptual similarity scores are clearly higher than on the two other datasets, where attribution methods help. \\
Fig \ref{fig:patchs_examples} shows examples of patches for each dataset using \expgc.

\begin{figure*}[ht]
    \includegraphics[width=0.95\textwidth]{assets/metapred/patchs.jpg}
    \caption{\textbf{Examples of extracted patches.} The perceptual similarity score is performed on the locations considered most important by the attribution methods. Examples of patches extracted for the three datasets with the \expgc~ method.}
    \label{fig:patchs_examples}
    \vspace{-4mm}
\end{figure*}
