\subsection{Qualitative comparison}

Regarding the visual consistency of our method, Figure~\ref{fig:eva:imagenet_explanations} shows a side-by-side comparison between our method and the attribution methods  tested in our benchmark. 
To allow better visualization, the gradient-based
methods were 2 percentile clipped.

\begin{figure*}[!ht]
  \centering
  \includegraphics[width=0.99\textwidth]{assets/eva/eva_qualitative.png}
  \caption{\textbf{Qualitative comparison} with other attribution methods. To allow for better visualization, the gradient-based methods (Saliency, Gradient-Input, SmoothGrad, Integrated-Gradient, VarGrad) are clipped at the 2nd percentile. For more results and details on each method and choice of hyperparameters, see Appendix.
  }
  \vspace{-2mm}
  \label{fig:eva:imagenet_explanations}
\end{figure*}

\subsection{Ablation studies}
\label{ap:eva:benchmarks}

\input{assets/eva/appendix_table}

For a more thorough understanding of the impact of the different components that made EVA - the adversarial overlap and the use of verification tools- we proposed different ablation versions of EVA which are the following:
(\textbf{\textit{i}}) Empirical EVA, (\textbf{\textit{ii}}) GreedyAO which is the equivalent of Greedy-AS but with the $\AO$ estimator. This allow us to perform ablation on the proposed $\AO$~estimator. Results can be found in Table~\ref{tab:eva:ablation_ao}.
\subsection{Empirical EVA.}
    
In this section, we describe the ablation consisting in estimating \eva~ without any use of verified perturbation analysis -- thus without any guarantees.

A first intuitive approach would be to replace verification perturbation analysis with adversarial attacks (as used in \textit{Greedy-AS}~\cite{hsieh2020evaluations}); we denote this approach as \textit{Greedy-AO}. 
In addition, we go further with a purely statistical approach based on a uniform sampling of the domain; we denote this approach \evaEmp. 
    
This estimator proves to be a very good alternative in terms of computation time but also with respect to the considered metrics as shown in Section ~\ref{sec:eva:experiments}. Unfortunately the lack of guarantee makes it not as relevant as \eva.
Formally, it consists in directly estimating empirically \AO  using $N$ randomly sampled perturbations.
    
\begin{equation}\label{eq:eva:adv_empirique}
    \AOemp(\vx, \ball) = 
    \max_{\substack{\v{\delta}_1,\cdots \v{\delta}_i,\cdots \v{\delta}_N  \overset{\mathrm{iid}}{\sim} U(\ball)\\c'\neq{}c}} \pred_{c'}(\vx + \v{\delta}_i) - \pred_c(\vx + \v{\delta}_i).
\end{equation}
    
We then denote accordingly \evaEmp which uses $\AOemp$:

\begin{equation}
    \label{eq:eva:tod_estimator_emp}
    \evaEmp(\vx, \v{u}, \ball) = \AOemp(\vx, \ball) - \AOemp(\vx, \ballu)
\end{equation}




\subsection{\eva~and Robustness-Sr}

We show here that the explanations generated by \eva~  provide an optimal solution from a certain stage to the $\rsr$ metric proposed by~\cite{hsieh2020evaluations}. We admit a unique closest adversarial perturbation $\v{\delta}^* = \min ||\v{\delta}||_p : \f(\vx + \v{\delta}) \neq \f(\vx)$, and we define $\varepsilon$, the radius of $\ball$ as $\varepsilon = ||\v{\delta}||_p$. 
Note that $||\v{\delta}||_p$ can be obtained by binary search using the verified perturbation analysis method.

We briefly recall the $\rsr$ metric. With $\vx = (x_1, ..., x_d)$, the set $\mathcal{U} = \{1, ..., d\}$, $\v{u}$ a subset of $\mathcal{U}$ : $\v{u} \subseteq \mathcal{U}$ and $\overline{\v{u}}$ its complementary. Moreover, we denote the minimum distance to an adversarial example $\varepsilon^*_{\v{u}}$: 
$$ \varepsilon^*_{\v{u}} = \big\{ \min || \v{\delta} ||_p ~:~ \f(\vx + \v{\delta}) \neq \f(\vx), \v{\delta}_{\overline{\v{u}}} = 0  \big\} $$ 

The $\rsr$ score is the AUC of the curve formed by the points $\{ (1,  \varepsilon^{*}_{(1)}), ..., (d,  \varepsilon^{*}_{(d)})  \}$ where $\varepsilon^{*}_{(k)}$ is the minimum distance to an adversarial example for the $k$ most important variables.
From this, we can deduce that $||\v{\delta}^*|| \leq \varepsilon^*_{\v{u}}$, $\forall \v{u} \subseteq \{1, ..., d\}$.

The goal here is to minimize this score, which means for a number of variables $|\v{u}| = k$, finding the set of variables $\v{u}^*$ such that $\varepsilon^*_{\v{u}}$ is minimal. We call this set the \textit{optimal set at $k$}. 

\begin{definition}
The \textit{optimal set at $k$} is the set of variables $\v{u}^{*}_k $ such that 
$$ \v{u}^{*}_k = \underset{ \v{u} \subseteq \mathcal{U},~ |\v{u}| = k}{\argmin ~~ \varepsilon^*_{\v{u}} }. $$
\end{definition}

We note that finding the minimum cardinal of a variable to guarantee a decision is also a standard research problem  ~\cite{ignatiev2019abduction, ignatiev2019relating} and is called subset-minimal explanations. 

Intuitively, the optimal set is the combination of variables that allows finding the closest adversarial example.
Thus, minimizing $\rsr$ means finding the optimal set $\v{u}^*$ for each $k$. 
Note that this set can vary drastically from one step to another, it is therefore potentially impossible for attribution to satisfy this optimality criterion at each step.
Nevertheless, an optimal set that is always reached at some step is the one allowing to build $\v{\delta}^*$.
We start by defining the notion of an essential variable before showing the optimality of $\v{\delta}^*$.

\begin{definition}
Given an adversarial perturbation $\v{\delta}$, we call \textit{essentials variables} $\v{u}$ all variables such that $|\v{\delta}_{i}| > 0, i \in \v{u}$. Conversely, we call \textit{inessentials variables} variables that are not essential.
\end{definition}

For example, if $\v{\delta}^*$ has $k$ \textit{essential variables}, it is reachable by modifying only $k$ variables. 
This allows us to characterize the optimal set at step $k$.

\begin{proposition} 
\label{prop:eva:uoptimal}
Let $\v{u}$ be the set of essential variables of $\v{\delta}^*$, then $\v{u}$ is an optimal set for $k$, with $k \in [\![|\v{u}|,d]\!] $.
\end{proposition}

\begin{proof}
Let $\v{v}$ be a set such that $ \varepsilon^*_{\v{v}} < \varepsilon^*_{\v{u}} $, then $ \varepsilon^*_{\v{v}} < || \v{\delta}^* || $ which is a contradiction.
\end{proof}

Specifically, as soon as we have the variables allowing us to build $\v{\delta}^*$, then we reach the minimum possible for $\rsr$.
We will now show that \eva~allows us to reach this in $|\v{u}|$ steps, with $|\v{u}| \leq d$ by showing (1) that $\v{\delta}^*$ \textit{essential variables} obtain a positive attribution and (2) that $\v{\delta}^*$ \textit{inessential variables} obtain a zero attribution.

\begin{proposition}
\label{prop:eva:ess}
All essential variables \(\v{u}\) w.r.t \(\v{\delta}^*\) have a strictly positive importance score \(\eva(\v{u}) > 0\). 
\end{proposition}

\begin{proof}
Let us assume that $i$ is \textit{essential} and $\eva(i) = 0$, then $\bm{F}(\ball) = \bm{F}(\ball_i)$ which implies
$$
\max_{\substack{\v{\delta} \in \ball\\c'\neq{}c}} \f_{c'}(\vx + \v{\delta}) - \f_c(\vx + \v{\delta}) = 
\max_{\substack{\v{\delta}' \in \ball_i\\c'\neq{}c}} \f_{c'}(\vx + \v{\delta}') - \f_c(\vx + \v{\delta}')
$$
by uniqueness of the adversarial perturbation, $ \v{\delta} = \v{\delta}' $ which is a contradiction as $\v{\delta}' \notin \ball_i$ since $\v{\delta}'_i \neq 0$ by definition of an \textit{essential variable}. Thus $x_i$ cannot be \textit{essential}, which is a contradiction.
\end{proof}

Essentially, if the variable $i$ is necessary to reach $\v{\delta}^*$, then removing it prevents the adversarial example from being reached and lowers the \adv, giving a strictly positive attribution.

\begin{proposition}
\label{prop:eva:iness}
All inessential variables \(\v{v}\) w.r.t. \(\v{\delta}^*\) have a zero importance score \(\eva(\v{v}) = 0\). 
\end{proposition}

\begin{proof}
With $i$ being an \textit{inessential} variable, then $\v{\delta}^*_i = 0$. It follow that $\v{\delta}^* \in \ball_i \subseteq \ball$. Thus
\begin{align*} 
\bm{F}(\ball) &= \max_{\substack{\v{\delta} \in \ball\\c'\neq{}c}} \f_{c'}(\vx + \v{\delta}) - \f_c(\vx + \v{\delta}) \\
              &= \f_{c'}(\vx + \v{\delta}^*) - \f_c(\vx + \v{\delta}^*)
\end{align*}
as $\v{\delta}^*$ is the unique adversarial perturbation in $\ball$, similarly 
\begin{align*} 
\bm{F}(\ball_i) &= \max_{\substack{\v{\delta}' \in \ball\\c'\neq{}c}} \f_{c'}(\vx + \v{\delta}') - \f_c(\vx + \v{\delta}') \\
              &= \f_{c'}(\vx + \v{\delta}^*) - \f_c(\vx + \v{\delta}^*)
\end{align*}
thus $\bm{F}(\ball) = \bm{F}(\ball_i)$ and $\eva(i) = 0$.
\end{proof}

Finally, since \eva~ranks the \textit{essential variables} of $\v{\delta}^*$ before the \textit{inessential variables}, and since $\v{\delta}^*$ is the \textit{optimal set} from the step $|\v{u}|$ to the last one $d$, then \eva~provide the \textit{optimal set}, at least from the step $|\v{u}|$.

\begin{theorem}\textbf{\eva~provide the optimal set from step $|\v{u}|$ to the last step.}
\label{app:eva:rsr}
With $\v{u}$ the essential variables of $\v{\delta}^*$, \eva~will rank the $\v{u}$ variables first and provide the optimal set from the step $|\v{u}|$ to the last step. 
\end{theorem}

\begin{proof}
Let $\v{u}$ denote the \textit{essential variables} of $\v{\delta}^*$ and $\v{v}$ the \textit{inessential variables}. Then according to Proposition~\ref{prop:eva:ess} and Proposition~\ref{prop:eva:iness}, $\forall i \in \v{u}, \forall j \in \v{v}: \eva(i) > \eva(j)$. It follow that $\v{u}$ are the most important variables at step $|\v{u}|$. Finally, according to Proposition~\ref{prop:eva:uoptimal}, $\v{u}$ is the optimal set for $k$, with $k \in [\![|\v{u}|,d]\!]$.
\end{proof}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{assets/eva/robsr_optimal.png}
  \caption{\textbf{\eva~yield optimal subset of variable from step $|\v{u}|$.} $\rsr$ measures the AUC of the distances to the nearest adversary for the $k$ most important variables. With $\v{\delta}^*$ the nearest reachable adversarial perturbation around $\vx$, then \eva~yield the optimal set -- the variables allowing to reach the nearest adversarial example for a given cardinality -- at least from $||\v{u}|| \leq d$ step to the last one, $\v{u}$ being the so-called essential variables.
  }
  \label{fig:eva:eva_optimal}
\end{figure}


\subsection{\eva~and \textit{Stability}}

Stability is one of the most crucial properties of an explanation. Several metrics have been proposed~\cite{aggregating2020,yeh2019infidelity} and the most common one consists in finding around a point $\vx$, another point $\v{z}$ (in a radius $\radiusbis$) such that the explanation changes the most according to a given distance between explanation $d$ and a distance over the inputs $\rho$:

$$
\textit{Stability}(\vx, \explainer) = \max_{\v{z} : \rho(\v{z}, \vx) \leq \radius} d( \explainer(\vx), \explainer(\v{z}) ) 
$$

and $\explainer$ an explanation functional.
It can be shown that the proposed ~\eva~ estimator is bounded by the stability of the model as well as by the radii $\radius$ and $\radiusbis$, $\radius$ being the radius of $\ball$ and $\radiusbis$ the radius of stability.
From here, we assume $d$ and $\rho$ are the $\ell_2$ distance.

Let assume that $\f$ is $L$-lipschitz. We recall that a function $\f$ is said $L$-lipschitz over $\mathcal{X}$ if and only if $\forall (\vx, \v{z}) \in \mathcal{X}^2, || \f(\vx) - \f(\v{z}) || \leq L || \vx - \v{z} ||$.


\begin{theorem}\textbf{\eva~ has bounded Stability}
\label{app:eva:stab}
Given a $L$-lipschitz predictor $\f$, $\radius$ the radius of $\ball$ and $\radiusbis$ the Stability radius, then
$$
\textit{Stability}(\vx, \eva) \leq 4L(\radius + \radiusbis)
$$
\end{theorem}

\begin{proof}
With $c' \neq c$ we denote the so-called \textit{margin} $\vm(\vx) = \f_{c'}(\vx) - \f_{c}(\vx)$. We note that by additivity of the Lipschitz constant $\vm$ is 2$L$-Lipschitz.
\begin{align*} 
&\textit{Stability}(\vx, \eva) = \max_{\v{z} : \rho(\v{z}, \vx) \leq \radiusbis} || \eva(\vx), \eva(\v{z}) || \\
  &= \max_{\v{z} : \rho(\v{z}, \vx) \leq \radiusbis} 
  ||\max_{\v{\delta}} \vm(\vx + \v{\delta}) 
  - \max_{\v{\delta}_{\v{u}}} \vm(\vx + \v{\delta}_{\v{u}}) \\
  &~~~~- \max_{\v{\delta}} \vm(\v{z} + \v{\delta})
  + \max_{\v{\delta}_{\v{u}}} \vm(\v{z} + \v{\delta}_{\v{u}}) || \\
  &\leq \max_{\v{z} : \rho(\v{z}, \vx) \leq \radiusbis} 
  ||\max_{\v{\delta}} \vm(\vx + \v{\delta}) 
  - \max_{\v{\delta}} \vm(\v{z} + \v{\delta}) || \\
  &~~~~+ || \max_{\v{\delta}_{\v{u}}} \vm(\v{z} + \v{\delta}_{\v{u}}) 
  - \max_{\v{\delta}_{\v{u}}} \vm(\vx + \v{\delta}_{\v{u}}) || \\
  &= \max_{\v{\xi} : ||\v{\xi}|| \leq \radiusbis} 
  ||\max_{\v{\delta}} \vm(\vx + \v{\delta}) 
  - \max_{\v{\delta}} \vm(\vx + \v{\delta} + \v{\xi}) || \\
  &~~~~+ || \max_{\v{\delta}_{\v{u}}} \vm(\vx + \v{\delta}_{\v{u}} + \v{\xi}) 
  - \max_{\v{\delta}_{\v{u}}} \vm(\vx + \v{\delta}_{\v{u}}) || \\
  &\leq 2L (||\v{\delta}|| + ||\v{\xi}||) + 2L (||\v{\delta}|| + ||\v{\xi}||)\\
  &= 4L (\radius + \radiusbis)
\end{align*}

\end{proof}

\subsection{Targeted explanations}
\label{ap:eva:targeted}

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.9\textwidth]{assets/eva/targeted_explanations_full.png}
  \caption{\textbf{Targeted Explanations} Attribution-generated explanations for a decision other than the one predicted. Each column represents the class explained, e.g., the first column looks for an explanation for the class `0' for each of the samples. As indicated in section~\ref{sec:eva:targeted_explanations}, the red areas indicate that a black line should be added and the blue areas that it should be removed. More examples are available in the Appendix.
  }
  \label{fig:eva:ap_targeted}
\end{figure}

In order to generate targeted explanations, we split the calls to $\eva(\cdot, \cdot)$ in two: the first one with `positive' perturbations from $\ball^{(+)}$ (only positive noise), a call with `negative' perturbations from $\ball^{(-)}$ (only negative-valued noise) as defined in Section~\ref{sec:eva:targeted_explanations}. 

We then get two explanations, one for positive noise
$\explanation^{(+)}_{\v{u}} = \bm{F}_c(\ball^{(+)}(\vx)) - \bm{F}_c(\ball^{(+)}_{\v{u}}(\vx))$, the other for negative noise $\explanation^{(-)}_{\v{u}} = \bm{F}_c(\ball^{(-)}(\vx)) - \bm{F}_c(\ball^{(-)}_{\v{u}}(\vx))$. Intuitively, high importance for $\explanation^{(+)}_{\v{u}}$  means that the model is sensitive to the addition of a white line. Conversely, high importance for $\explanation^{(-)}_{\v{u}}$  means that removing it changes the decision model. These two explanations being opposed, we construct the final explanation as $\explanation_{\v{u}} = \explanation^{(+)}_{\v{u}} - \explanation^{(-)}_{\v{u}}$. More examples of results are given in Fig.~\ref{fig:eva:ap_targeted}.
