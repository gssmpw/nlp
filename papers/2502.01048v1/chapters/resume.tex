\begin{center}
{\Large Résumé \\ \adforn{21}}
\end{center}

Cette thèse doctorale vise à pousser les frontières de l'état de l'art ainsi que le développement d'outils dans le domaine de l'explicabilité en vision par ordinateur. Elle se focalise spécifiquement sur la construction d'un ensemble d'outils destinés à améliorer notre compréhension des caractéristiques (ou \textit{features}) exploitées par les réseaux de neurones profonds actuellement employés dans des tâches de vision. L'explicabilité représente un domaine clé pour améliorer les interactions entre les humains et les systèmes d'intelligence artificielle, pour certifier ces même système dans des applications critique mais aussi d'un point de vue scientifique pour décrypter un nouveau type d'intelligence: l'intelligence artificielle. 
De manière plus concrète, la complexité et le manque de transparence de ces modèles constituent un obstacle majeur à leur adoption dans des applications qui demande un haut niveau de sécurité et de confiance. L'explicabilité est au coeur de ces problématiques, et les réponses que ce champ de recherche doit apporter sont attendus et permettraient des avancées technologiques significatives et une adoption plus rapide de l'intelligence artificielle. À travers ce manuscrit, nous explorons et proposons de nouvelles méthodes d'explicabilité, apportant chacune une contribution à la compréhension des modèles de vision.


Nous entamons cette thèse par une analyse détaillée des méthodes d'attribution, également connues sous le nom de cartes de saillance. Ces techniques révèlent où le modèle porte son attention pour prendre une décision, grâce à l'utilisation de cartes thermiques. La première section propose une métrique, inspirée de la stabilité algorithmique, qui se fonde sur ces attributions pour évaluer la qualité des explications fournies par les modèles, permettant ainsi d'identifier ceux offrant les meilleures explications. Nous introduisons ensuite une nouvelle méthode d'attribution, inspirée du champ de l'Analyse de Sensibilité Globale, basée sur les indices de Sobol. Cette approche de type boîte noire, soutenue par un fondement théorique solide, permet de réduire de moitié le temps de calcul par rapport à l'état de l'art grâce à l'utilisation de séquences quasi-Monte Carlo. Nous poursuivons en présentant la première méthode d'attribution dotée de garanties formelles, \eva, qui repose sur l'analyse de perturbation vérifiée.


De manière surprenante, nous avons constaté que ces méthodes, lorsqu'elles sont testées dans des cas d'usage réels avec de véritables utilisateurs, s'avèrent peu utiles pour comprendre les modèles. Plus spécifiquement, dans des scénarios complexes, ces techniques se révèlent inefficaces, alors qu'elles suffisent pour identifier des biais dans des contextes plus simples. Deux hypothèses sont alors formulées pour surmonter ces défis : la première suggère la nécessité d'aligner les modèles de vision par ordinateur avec le raisonnement humain, remettant ainsi en question le modèle lui-même ; la seconde avance que les méthodes d'attribution actuelles ne sont pas suffisantes et ne révèlent pas assez d'informations. Ces hypothèses sont ensuite examinées dans des chapitres dédiés.


Pour aborder la première hypothèse, nous proposons une nouvelle routine d'entraînement qui vise non seulement à minimiser la fonction de coût habituelle mais aussi à imiter les explications humaines, autrement dit, à avoir raison pour les bonnes raisons. Étonnamment, non seulement les modèles parviennent à généraliser, adoptant des stratégies humaines, mais leur précision augmente également. Nous explorons ensuite une seconde approche visant à aligner les modèles non pas par régularisation mais par contrainte, optimisant dans un espace fonctionnel restreint : celui des fonctions 1-Lipschitz. L'analyse établit un lien entre la robustesse des modèles, notamment ceux caractérisés par une propriété 1-Lipschitz, et leur capacité à fournir des explications alignées avec le raisonnement humain.


Ensuite, nous examinons la seconde hypothèse, selon laquelle les méthodes d'attribution actuelles sont insuffisantes car elles révèlent uniquement \textit{où} le modèle porte son attention, sans expliciter \textit{ce qu'il perçoit}. Nous adoptons une approche d'explicabilité basée sur les concepts, évoluant de la focalisation sur le « où » vers une compréhension du « quoi » perçu par le modèle. Cette transition est concrétisée par la méthode \craft, qui automatise l'extraction des concepts utilisés par un modèle et évalue ensuite l'importance de chaque concept extrait. Nous analysons en profondeur les composantes des méthodes actuelles d'extraction de concept et démontrons qu'elles comprennent deux phases : une phase d'extraction et une phase d'estimation de l'importance. Nous unifions ensuite les différentes approches de la littérature en montrant que la phase d'extraction peut être conceptualisée comme un problème d'apprentissage de dictionnaire, et que la phase d'estimation d'importance utilise implicitement des méthodes d'attribution. Après avoir établi ce cadre unificateur, nous introduisons \maco, une méthode de visualisation des caractéristiques que nous appliquons aux concepts, permettant de visualiser les concepts extraits. Nous concluons en intégrant ces différentes méthodes dans une démonstration interactive, qui offre une exploration et une compréhension des concepts les plus importants pour les 1000 classes d'ImageNet d'un modèle ResNet.

La thèse se termine par une réflexion approfondie sur les méthodes développées, les progrès réalisés et les défis rencontrés, ouvrant des perspectives sur les futures directions de recherche en explicabilité en Intelligence Artificielle (IA). Nous soulignons l'importance de poursuivre la recherche de synergies entre les différentes méthodes étudiées, ainsi que les voies prometteuses pour exploiter pleinement le potentiel de l'explicabilité.

\clearpage

\begin{center}
{\Large Abstract \\ \adforn{21}}
\end{center}

This doctoral thesis aims to advance the state of the art and the development of tools in the field of explainability in computer vision. It specifically focuses on creating a set of tools designed to enhance our understanding of the features utilized by deep neural networks currently employed in vision tasks. Explainability represents a key area for improving interactions between humans and artificial intelligence systems, as well as from a scientific standpoint to decipher a new type of intelligence: artificial intelligence. More concretely, the complexity and lack of transparency of these models pose a major obstacle to their adoption in critical systems and raise crucial questions, potentially capable of leading to significant advances in our understanding of intelligence, provided their mechanisms can be deciphered. Through this manuscript, we explore several explainability methods, each contributing to the understanding and improvement of the explainability of vision models while acknowledging their respective limitations.

We begin this thesis with a detailed analysis of attribution methods, also known as saliency maps or heat maps. These techniques reveal where the model focuses its attention to make a decision, through the use of heatmaps. The first paper proposes a metric inspired by algorithmic stability that is based on these attributions to assess the quality of explanations provided by the models, thus identifying those offering the best explanations. We then introduce a new attribution method inspired by the field of Global Sensitivity Analysis based on Sobol indices. This black-box approach, supported by a solid theoretical foundation, allows for halving the computation time compared to the state of the art through the use of quasi-Monte Carlo sequences. We continue by presenting the first attribution method with formal guarantees, \eva, which relies on verified perturbation analysis.

Surprisingly, we found that these methods, when tested in real-use cases with actual users, prove to be of little use in understanding the models. More specifically, in complex scenarios, these techniques prove ineffective, while they are sufficient to identify biases in simpler contexts. Two hypotheses are then formulated to overcome these challenges: the first suggests the need to align computer vision models with human reasoning, thereby questioning the model itself; the second advances that current attribution methods are not sufficient and do not reveal enough information. These hypotheses are then examined in dedicated chapters.

To address the first hypothesis, we propose a new training routine aimed not only at minimizing the usual cost function but also at mimicking human explanations, in other words, being right for the right reasons. Surprisingly, not only do the models manage to generalize, adopting human strategies, but their accuracy also increases. We then explore a second approach aimed at aligning models not through regularization but through constraint, optimizing in a restricted functional space: that of 1-Lipschitz functions. The analysis establishes a link between the robustness of the models, especially those characterized by a 1-Lipschitz property, and their ability to provide explanations aligned with human reasoning.

Next, we examine the second hypothesis, according to which current attribution methods are insufficient because they reveal only \textit{where} the model focuses its attention, without specifying \textit{what it perceives}. We adopt an explainability approach based on concepts, moving from focusing on the "where" to understanding the "what" perceived by the model. This transition is materialized by the \craft~method, which automates the extraction of concepts used by a model and then assesses the importance of each extracted concept. We thoroughly analyze the components of current concept extraction methods and demonstrate that they include two phases: an extraction phase and an importance estimation phase. We then unify the different approaches in the literature by showing that the extraction phase can be conceptualized as a dictionary learning problem, and that the importance estimation phase implicitly uses attribution methods. After establishing this unifying framework, we introduce \maco, a feature visualization method that we apply to concepts, allowing the visualization of extracted concepts. We conclude by integrating these different methods into an interactive demonstration, offering exploration and understanding of the most important concepts for the 1000 ImageNet classes of a ResNet model.

The thesis concludes with a thorough reflection on the developed methods, the progress made, and the challenges encountered, opening perspectives on future research directions in AI explainability. We emphasize the importance of continuing the search for synergies between the different methods studied, as well as promising avenues for fully leveraging the potential of explainable AI.





