\chapter{Conclusion \& Perspective}
\label{chap:conclusion}

\textbf{Much work remains to be done}. Deep Learning is incredibly efficient and the field of Explainable Artificial Intelligence (XAI) is now under intense scrutiny, tasked with the hard task of understanding the inner-working of these models. The fulfillment of this colossal goal remains uncertain, yet our progress is undeniable, gradually providing methods and insights that enhance our understanding of artificial intelligence models.

In that perspective, this thesis aimed to develop a modest suite of explainability methods for computer vision models, making contributions across various chapters.

The \autoref{chap:attributions} dedicated to attribution methods has thoroughly investigated these techniques, introducing a new metric to identify models providing better explanations (\autoref{sec:attributions:mege}), a black-box method based on Sobol indices (\autoref{sec:attributions:sobol}), and an advanced approach utilizing formal methods (\autoref{sec:attributions:eva}). An in-depth human experiment (\autoref{sec:attributions:metapred}) highlighted the usefulness of these methods in simplified scenarios, marking a notable advancement for explainability. However, their efficacy proved limited in more complex contexts, leading us to formulate two hypotheses guiding further research.

The first hypothesis, explored in \autoref{chap:alignment}, focused on aligning vision models to share coherent explanations (\autoref{sec:harmonization}), thereby increasing their accuracy and generalization beyond training explanations. We also sketched an alternative theoretical approach aimed at constraining our neural networks to adhere to a specific functional norm (\autoref{sec:lipschitz}), suggesting a promising alignment with human reasoning. This path would require a more holistic design incorporating improved data, tasks closer to human capabilities, and a more plausible architecture.

The final section of this thesis, \autoref{chap:concepts}, ventured into the emerging domain of concepts-based explainability, proposing an innovative method for their extraction and establishing a theoretical framework that unify concept extraction and dictionary learning as well as concept importance and attribution methods. The chapter culminates with the introduction of a technique for visualizing the extracted concepts, showcased through LENS. This illustration serves as evidence of the synergistic approach achievable by integrating Attribution, Concepts, and Feature Visualization.

\section{Perspective}

It is customary to conclude this manuscript with predictions. However, I must confess a certain reluctance in the art of forecasting, and therefore, I will reformulate this as remarks that have accompanied me throughout this journey. They are fourfold: (i) the paths that seem promising for the future of explainability, (ii) the disease of dimensionality reduction in XAI, (iii) the link between generalization and explainability, and (iv) finally, the human aspect of this research field.

\subsection{Promising Avenues.} 
This work has consistently aimed to build upon established research foundations rather than attempting to invent anew. Given the relative infancy and lack of established benchmarks in our field, it is imperative to construct our understanding based on robust frameworks. An important realization during my investigation into attribution methods was the relevance of Global Sensitivity Analysis (GSA), which is a field that has been tackling analogous challenges for over three decades, offering both tools and insights that are directly applicable. The core challenge addressed by GSA involves identifying and ranking the inputs $\rx_i$ that most significantly influence a model's random output:


$$
\rv{y} = \f(\rvx).
$$

The tools available in GSA range from estimating the impact of each variable in the output to measuring the dependence and interactions between variables themselves. I believe that a cross-disciplinary contributions (XAI and GSA) could enrich our understanding of attribution methods and the broader concept of importance estimation.

To continue on this vein, the exploration in \autoref{chap:concepts}~reveals that current attribution methods often only scratch the surface, suggesting a deeper examination of internal activations is necessary for a thorough understanding. In our case, we have seen that many currently developed methods could be framed as dictionary learning:

$$
\s{R}(\vx) = \min_{\v{u},\m{V}} \norm{ \vx - \v{u}\m{V} }_p + \lambda \Omega(\v{u}).
$$

I believe that this field is incredibly valuable for researchers focused on explainability. Numerous studies have introduced a variety of analysis techniques and extensions, ranging from hierarchical dictionaries to supervised dictionaries, which I find particularly well-suited for explainability.
In summary, we have a solid arsenal at our disposal to tackle the problem of reinterpreting latent space. The insights from this document offer several avenues  --including one discussed below on the disease of dimensionality reduction.

Furthermore, during the last chapter, we advocate for a paradigm shift from \textit{separation} to \textit{integration}, highlighting the importance of interpreting the methods discussed as part of a synergistic framework. Each method -- Attribution, Concept, Feature visualization -- reveal a fragment of the puzzle, suggesting that a comprehensive understanding requires the amalgamation of these complementary insights. The \Lens~demonstration project modestly aspires to guide the field towards this integrated approach, emphasizing the collective interpretation of diverse methodologies to achieve a holistic understanding of the research landscape.


\subsection{Dimensionality Reduction Disease.} In the quest for explainability, there is a legitimate expectation for it to simplify the processes, to reduce the cognitive load of the internal mechanisms governing neural networks. However, it has become apparent that the methods sometimes used to simplify the problem can, paradoxically, obscure more than they elucidate the internal realities or phenomena we aim to explain. Resorting to indiscriminately reducing the dimensions of activations or summarizing a model's decision-making process with a linear model may be useful as a preliminary approach, but I am concerned that these methods might lead to more confusion than clarification. We must now look beyond these initial simplifications and confront the question: \textit{How can we embrace complexity in a non-reductive manner?}

I believe the theory presented in \autoref{sec:concepts:holistic} suggests a path forward: \craft~succeeds not because it reduces dimensions, but \textit{because it expands it}! Indeed, \Lens~encompasses over 10,000 concepts, far exceeding the dimensionality of the model's latent space. If our models can be likened to tangled balls of yarn made up of features, our goal should then be to untangle these to understand the strands and thereby connect the features in a potentially much larger space. In other words, contrary to intuition, I am convinced that explainability must now aim to increase dimensionality (appropriately), rather than reduce it. This expanded approach necessitates developing methodologies that can navigate and articulate the increased complexity, ensuring that the additional dimensions serve to clarify rather than confound our understanding of neural network behaviors. By embracing and effectively managing this complexity, we can move closer to achieving true explainability and reveals the intricate interplay of features and their contributions to model decisions in a comprehensive and nuanced manner.


\subsection{Generalization, Algorithmic Complexity, and Explainability}

\paragraph{Generalization.}
The fate of explainability is intimately linked to the challenge of generalization. The Vapnik-Chervonenkis (VC) Dimension, a fundamental concept in statistical learning, offers a theoretical framework for assessing a model's generalization capability. It quantifies the complexity of a "hypothesis set" or functions a model can learn, suggesting that a high VC-Dimension indicates a model's potential for precise adaptation to training data, potentially leading to overfitting. In theory, a model with a higher VC-Dimension could exhibit reduced generalization capability, becoming less adept at making accurate predictions on unseen data.

However, the empirical performance of deep neural networks, with their often really large number of parameters, challenges this traditional understanding. Despite their complexity, these models have shown remarkable generalization abilities, questioning the adequacy of existing tools and theories, including the VC-Dimension, to fully explain this phenomenon in deep learning.

This realization prompts a deeper exploration into the \textbf{algorithmic dimension of XAI}, emphasizing the importance of acknowledging AI models' computational specificities. The distinction between ideal mathematical operations and their actual computational implementations necessitates a consideration of computational complexity. For instance, the implementation invariance axiom used to build an interpretable surrogate suggests that functions $\f_a$ and $\f_b$ are considered strictly equal if they produce the same output. However, it is possible for two equal functions to have significantly different underlying computational processes (different computational graphs). Early pioneers at the Dartmouth Conference already recognized this specificity and emphasized the need for further research into the algorithmic study of AI.

\paragraph{Algorithmic Complexity.} The field of Algorithmic Information Theory (AIT)~\cite{chaitin1977algorithmic,grunwald2008algorithmic} \footnote{This section was heavily inspired by discussions with Louis Béthune.} appears well-suited for a better understanding of models. AIT formalizes simplicity and complexity from an algorithmic standpoint, positing that an object is simple if it can be concisely described and complex if no such succinct description exists. A central notion in AIT is the \textit{Kolmogorov Complexity}, it is the length of the shortest program that outputs a string $\vx$ when run on a universal Turing machine $U$ and denoted by:

$$K(\vx) = \min_{p} \{ \ell(p) : U(p) = \vx \}.$$

With $\ell(p)$ the length of the program $p$. It can be demonstrated that the definition of $K(\vx)$ is robust with respect to the choice of the universal Turing machine $U$. Specifically, $K(\vx)$ varies by at most an additive constant that is independent of $\vx$ when a different $U$ is selected. This principle, known as the invariance theorem~\cite{solomonoff1964formal,kolmogorov1965three,chaitin1969length}, marks a foundational moment in the development of algorithmic information theory. Despite the non-computability of this measure, which introduces its own set of challenges, it yields significant insights on what the underlying mechanisms are doing, and on the nature of generalization of machine learning models. Other notable concepts include the Algorithmic "Solomonoff" Probability~\cite{solomonoff1997discovery} and the Levin Search~\cite{levin1973universal}, which propose alternative approaches to understanding algorithmic complexity:

\begin{align}
K_{\text{Levin}}(\vx) &\defas \min_{p} \{ \ell(p) + \log(T(p)) : U(p) = \vx \}, ~~
K_{\text{Solomonoff}}(\vx) &\defas \sum_{p : U(p) = \vx} 2^{-\ell(p)}. \nonumber
\end{align}

Where $T(p)$ denote the computational time of the program $p$, that could translate into either the inference time, or the learning time.

Interestingly, this perspective represents a paradigm shift, focusing not on the \textit{quantity of parameters} but on the \textit{algorithm discovered by the model}. At first glance, some may wonder about the direction of this argument, as it might seem self-evident that an increase in parameters leads to a more complex algorithm. However, I will present an example intended to be instructive, without promising anything beyond offering the intuition that more parameters do not necessarily equate to increased complexity (see \autoref{fig:conclusion:toy_abs})\footnote{Example inspired by the excellent \cite{elhage2022superposition} article.}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/abs_toy.jpg}
    \caption{\textbf{Toy Example: Adding Parameters Simplifies the Algorithm.} Contrary to expectations, one might anticipate a more complex algorithm for a model with an increased number of parameters. However, this example demonstrates a DNN trained to predict $\vy = |\vx|$ with $\vx \in \mathbb{R}^2$. \textbf{(Top)} Illustrates a DNN with 3 hidden layers that struggles with generalization, as evidenced by the error map on the right, which depicts $\int_{\mathbb{R}^2}(|\vx| - f(\vx))^2 \mathrm{d} \vx$ and employs a convoluted algorithmic method to approximate $|\vx|$. \textbf{(Bottom)} Conversely, in the lower section, adding a neuron -- thereby increasing the number of parameters --results in not only improved generalization but also a more intuitive algorithm: it effectively divides $\vx$ into two by applying $\text{ReLU}(x_1), \text{ReLU}(-x_1)$ and similarly for $x_2$.}
    \label{fig:conclusion:toy_abs}
    \vspace{-4mm}
\end{figure}

Let's consider the task of learning the function $\vy = |\vx|$ where $(\vx, \vy) \in \mathbb{R}^d$ using a neural network with a single hidden layer, ReLU activations, and $n$ intermediate neurons. Starting with $d=2$ and $n=3$, we have 3 neurons to store intermediate states and then output $(|x_1|, |x_2|)$. The algorithm discovered by the model is complex, and the model fails to generalize well, as evident from the error map in \autoref{fig:conclusion:toy_abs}. However, by adding just one more parameter $n=4$, not only does the model begin to generalize better, but the mechanism it uses also becomes clear and simple: the increase in parameters leads to a reduction in algorithmic complexity. With this example, I aim not to prove a point definitively, but to suggest that the notion of complexity could be much richer than merely the count of parameters\footnote{Code to reproduce is available in \autoref{ap:conclusion:toy_abs}}.

I want to acknowledge that the possible notions of complexity are varied, and other perspectives are possible, ranging from principles of parsimony, symmetry or to the algorithmic complexity I wished to highlight. This diversity in understanding complexity underscores the multifaceted nature of what we deem "complex" and suggests a richer tapestry of factors that can influence the interpretability and functionality of models beyond just their parameter count.

\paragraph{Explainability.} Turning our focus back to explainability, explainability and generalization are poised to evolve in tandem. The development of new explainability tools to study model $\f(\cdot)$, as outlined in this manuscript, marks an initial step. However, we initiated this discussion (\autoref{sec:attributions:mege}) with the study of $\s{A}(.)$, that is, the algorithm generating $\f$, and often, it is these foundational ideas that resurface towards the end, much like a recurring theme. It seems now that the flow should take us into understanding the learning algorithms themselves, rather than merely the models they produce. Do neural networks strive to minimize the complexity of their programs in the vein of Kolmogorov, or perhaps, akin to Levin, seek a balance between program simplicity and execution time? Or is there an element of randomness in program emergence, with certain programs becoming more useful than others in a manner reminiscent of Solomonoff's complexity? What is the relationship between features and programs? What are the inductive biases of our models, and can these be interpreted as routines available in a conditional complexity manner? 

These questions bring the algorithmic aspect of deep learning back into focus, which I believe is a promising direction for continuing this research. Of course, the perspective is certainly not perfectly accurate, yet it has the merit of sparking a new set of questions to which we can now begin to provide some answers, thanks to the tools developed throughout this thesis. 

\vspace{-2mm}
\subsection{The Human Dimension}
\vspace{-2mm}

Explainability also intersects significantly with the human aspect of science. It transcends the mere decoding of models and touches upon our cognitive capacity to comprehend complex systems. Explainability thus challenges our limitations, especially our cognitive boundaries. To put it simply, other forms of intelligence might regard our quest for explainability as unusual, given their potential to directly decipher the weights and biases within AI systems without the intermediary steps humans require.

Echoing Camus, ``To understand the world is to reduce it to the human,'' we might say that to comprehend a neural network means to render it intelligible \textit{to us}, taking into account our cognitive limitations and capabilities. Is it truly possible to distill AI to a level that aligns with our understanding? It seems we are faced with two paths: one where our models harbor an inherent simplicity we have yet to discover, whether it be through symmetry or algorithmic simplicity, and another, more daunting (and exciting) path that recognizes our models as genuinely complex systems. 

The risk, then, lies in not confusing complexity with completeness. Understanding the intricate inner workings of these systems will requires a multidimensional approach, rich in interactions and dependencies. Kolmogorov noted, "The human brain is incapable of creating anything that is truly complex," suggesting that our understanding will be built gradually by discovering, constructing, and assembling simple concepts along the way. Thus, our challenge in comprehending neural networks will require avoiding oversimplification, ensuring we capture both their detailed structures and the fundamental motifs governing their behavior.

\vspace{0mm}
{
\begin{center} 
\Large \adforn{21} 
\end{center}
}
\vspace{0mm}

These insights bridge the human and computational aspects of this challenge. Recognizing the complexity in the face of a \textit{desert} of unknowns, we strive to find equilibrium between our cognitive capabilities and the unique algorithmic features of artificial intelligence. In doing so, we may already be uncovering some \textit{Sparks} of understanding.

\vspace{0mm}

\epigraph{``What makes the desert beautiful'', the little prince said, ``is that it hides a well somewhere...''}{Antoine de Saint-Exupéry}
