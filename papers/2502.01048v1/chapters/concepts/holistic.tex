\definecolor{color1}{RGB}{67, 160, 71}
\definecolor{color2}{RGB}{53, 183, 121}
\definecolor{color3}{RGB}{62, 74, 137}

\newcommand{\ACE}{\textbf{\textcolor{color1}{ACE}}}
\newcommand{\ICE}{\textbf{\textcolor{color2}{ICE}}}

\newcommand{\fa}{\bm{g}}
\newcommand{\fb}{\bm{h}}

In the first section (\autoref{sec:concepts:craft}), we have introduced a first framework able to automatically extract concept and estimate their importance. Recently, other approaches have been proposed, either for concept extraction or concept importance estimation. However no proper metric, benchmark or theoretical framework have been proposed. In this section, we start by noticing that all current concept-based approaches seek discover intelligible visual ``concepts'' buried within the complex patterns of activations using two key steps: (1) concept extraction followed by (2) importance estimation. Again, while these two steps are shared across methods, they all differ in their specific implementations.

Starting from that, we introduce a unifying theoretical framework that recast the first step -- concept extraction problem -- as a special case of \textbf{dictionary learning}, and we formalize the second step -- concept importance estimation -- as a more general form of \textbf{attribution method}.
This framework offers several advantages as it allows us: \tbi{i} to propose new evaluation metrics for comparing different concept extraction approaches; \tbi{ii} to leverage modern attribution methods and evaluation metrics to extend and systematically evaluate state-of-the-art concept-based approaches and importance estimation techniques; \tbi{iii}  to derive theoretical guarantees regarding the optimality of such methods.

We further leverage our framework to try to tackle a crucial question in explainability: how to \textit{efficiently} identify clusters of data points that are classified based on a similar shared strategy.
To illustrate these findings and to highlight the main strategies of a model, we introduce a visual representation called the strategic cluster graph.


\subsection{Introduction}

One promising set of explainability methods to address the issue posed in~\autoref{hyp:what} includes concept-based explainability methods, which are methods that aim to identify high-level concepts within the activation space of ANNs~\cite{kim2018interpretability}. These methods have recently gained renewed interest due to their success in providing human-interpretable explanations~\cite{ghorbani2019towards, zhang2021invertible, fel2023craft, graziani2023concept}. However, concept-based explainability methods are still in the early stages, and progress relies largely on researchers' intuitions rather than well-established theoretical foundations.  A key challenge lies in formalizing the notion of concept itself~\cite{genone2012concept}. 
Researchers have proposed desiderata such as meaningfulness, coherence, and importance~\cite{ghorbani2019towards} but the lack of formalism in concept definition has hindered the derivation of appropriate metrics for comparing different methods.

This section presents a theoretical framework to unify and characterize current concept-based explainability methods. Our approach builds on the fundamental observation that all concept-based explainability methods share two key steps: (1) concepts are extracted, and (2) importance scores are assigned to these concepts based on their contribution to the model's decision~\cite{ghorbani2019towards}. Here, we show how the first extraction step can be formulated as a dictionary learning problem while the second importance scoring step can be formulated as an attribution problem in the concept space. To summarize, our contributions are as follows:

\setlist[itemize]{leftmargin=5.5mm}
\begin{itemize}
    \item We describe a novel framework that unifies all modern concept-based explainability methods and we borrow metrics from different fields (such as sparsity, reconstruction, stability, FID, or OOD scores) to evaluate the effectiveness of those methods. 
    \item We leverage modern attribution methods to derive seven novel concept importance estimation methods and provide theoretical guarantees regarding their optimality. 
    Additionally, we show how standard faithfulness evaluation metrics used to evaluate attribution methods (i.e., Insertion, Deletion~\cite{petsiuk2018rise}, and $\mu$Fidelity~\cite{aggregating2020}) can be adapted to serve as benchmarks for concept importance scoring.
    In particular, we demonstrate that Integrated Gradients, Gradient Input, RISE, and Occlusion achieve the highest theoretical scores for 3 faithfulness metrics when the concept decomposition is on the penultimate layer. 
    \item We introduce the notion of local concept importance to address a significant challenge in explainability: the identification of image clusters that reflect a shared strategy by the model (see Figure~\ref{fig:holistic:clustering_graph}). We show how the corresponding cluster plots can be used as visualization tools to help with the identification of the main visual strategies used by a model to help explain false positive classifications. 
\end{itemize}



\begin{figure}[ht]
\begin{center}
   \includegraphics[width=.99\textwidth]{assets/holistic/clustering_graph.jpg}
\end{center}
   \caption{\textbf{Strategic cluster graphs for the espresso and zucchini classes.}
    The framework presented in this section provides a comprehensive approach to uncover local importance using any attribution methods. 
    Consequently, it allow us to estimate the critical concepts influencing the model's decision for each image.
    As a results, we introduced the Strategic cluster graph, which offers a visual representation of the main strategies employed by the model in recognizing an entire object class.
    For espresso (left), the main strategies for classification appear to be: \textcolor{anthracite}{$\bullet$} bubbles and foam on the coffee, \textcolor{green}{$\bullet$} Latte art, \textcolor{yellow}{$\bullet$} transparent cups with foam and black liquid, \textcolor{red}{$\bullet$} the handle of the coffee cup, and finally \textcolor{blue}{$\bullet$} the coffee in the cup, which appears to be the predominant strategy.
    As for zucchini, the strategies are: \textcolor{blue}{$\bullet$} a zucchini in a vegetable garden, \textcolor{yellow}{$\bullet$} the corolla of the zucchini flower, \textcolor{red}{$\bullet$} sliced zucchini, \textcolor{green}{$\bullet$} the spotted pattern on the zucchini skin and \textcolor{anthracite}{$\bullet$} stacked zucchini.
   }
\label{fig:holistic:clustering_graph}
\end{figure}

\subsection{A Unifying perspective}

\paragraph{Notations.} Throughout, $||\cdot||_2$ and $||\cdot||_F$ represent the $\ell_2$ and Frobenius norm, respectively. 
We consider a general supervised learning setting, where a classifier $\f : \sx \to \sy$ maps inputs from an input space $\mathcal{X} \subseteq \Real^d$ to an output space $\sy \subseteq \Real^c$. 
For any matrix $\mx \in \Real^{n \times d}$, $\vx_i$ denotes the $i^{th}$ row of $\mx$, where $i \in \{1, \ldots, n \}$ and $\vx_i \in \Real^{d}$.
Without loss of generality, we assume that $\f$ admits an intermediate space $\s{I} \subseteq \Real^p$. In this setup, $\fa : \sx \to \s{I}$ maps inputs to the intermediate space, and $\fb : \s{I} \to \sy$ takes the intermediate space to the output. Consequently, $\f(\vx) = (\fb \circ \fa)(\vx)$. Additionally, let $\bm{a} = \fa(\vx) \in \s{I}$ represent the activations of $\vx$ in this intermediate space.
We also abuse notation slightly: $\f(\mx) = (\fb \circ \fa)(\mx)$ denotes the vectorized application of $\f$ on each element $\vx$ of $\mx$, resulting in $(\f(\vx_1),\ldots, \f(\vx_n))$.

\paragraph{2 Fundamental steps.} Prior methods for concept extraction, namely \ACE~\cite{ghorbani2019towards}, \ICE~\cite{zhang2021invertible}~and \CRAFT~\cite{fel2023craft}, can be distilled into two fundamental steps:

\begin{enumerate}[label=(\textit{\textbf{\roman*}}), labelindent=0pt,leftmargin=5mm]

\item {\bf Concept extraction:} A set of images $\mx \in \Real^{n\times d}$ belonging to the same class is sent to the intermediate space giving activations $\m{A} = \fa(\mx) \in \Real^{n \times p}$.
These activations are used to extract a set of $k$ CAVs using K-Means~\cite{ghorbani2019towards}, PCA (or SVD)~\cite{zhang2021invertible, graziani2023concept} or NMF~\cite{zhang2021invertible, fel2023craft}. Each CAV is denoted $\cav_i$ and $\m{V} = (\cav_1, \ldots, \cav_k) \in \Real^{p \times k}$ forms the dictionary of concepts.

\item {\bf Concept importance scoring:} 
It involves calculating a set of $k$ global scores, which provides an importance measure of each concept $\cav_i$ to the class as a whole. Specifically, it quantifies the influence of each concept $\cav_i$ on the final classifier prediction for the given set of points  $\mx$. Prominent measures for concept importance include TCAV~\cite{kim2018interpretability} and the Sobol indices~\cite{fel2023craft}. 

\end{enumerate}


The two-step process described above is repeated for all classes. In the following subsections, we theoretically demonstrate that the concept extraction step \tbi{i} could be recast as a dictionary learning problem (see~\ref{sec:dico_learning}). It allows us to reformulate and generalize the concept importance step \tbi{ii} using attribution methods (see~\ref{sec:importance}). 

\subsubsection{Concept Extraction}

\paragraph{A dictionary learning perspective.}
\label{sec:dico_learning} 
The purpose of this section is to redefine all current concept extraction methods as a problem within the framework of dictionary learning. Given the necessity for clearer formalization and metrics in the field of concept extraction, integrating concept extraction with dictionary learning enables us to employ a comprehensive set of metrics and obtain valuable theoretical insights from a well-established and extensively researched domain. 

The goal of concept extraction is to find a small set of interpretable CAVs (i.e., $\m{V}$) that allows us to faithfully interpret the activation $\m{A}$. By preserving a linear relationship during the reconstruction, from $\m{U}$ to $\m{A}$ (and not necessarily from $\m{A}$ to $\m{U}$), we facilitate the understanding and interpretability of the learned concepts~\cite{kim2018interpretability, elhage2022superposition}. Therefore, we look for a coefficient matrix $\m{U} \in \Real^{n \times k}$ (also called loading matrix) and a set of CAVs $\m{V}$, so that $\m{A} \approx \m{U} \m{V}^\tr$.
In this approximation of $\m{A}$ using the two low-rank matrices $(\m{U}, \m{V})$,
$\m{V}$ represents the concept basis used to reinterpret our samples, and $\m{U}$ are the coordinates of the activation in this new basis. Interestingly, such a formulation allows a recast of the concept extraction problem as an instance of dictionary learning problem~\cite{mairal2014sparse} %
in which all known concept-based explainability methods fall:%

\begin{numcases}{(\m{U}^\star, \m{V}^\star) = \argmin_{\m{U},\m{V}} || \m{A} - \m{U} \m{V}^\tr ||^2_F ~~s.t~~}
 \forall ~ i, \v{u}_i \in \{ \e_1, \ldots, \e_k \} ~~ \text{\small\cite{ghorbani2019towards})}, \label{eq:holistic:dico_kmeans}\nonumber\\
  \m{V}^\tr \m{V} = \mathbf{I} ~~~ \text{\small(\cite{graziani2023concept,zhang2021invertible})}, \label{eq:holistic:dico_pca}\nonumber\\
 \m{U} \geq 0, \m{V} \geq 0 ~~~ \text{\CRAFT} \nonumber\\
 \m{U} = \bm{\psi}(\m{A}), ||\m{U}||_0 \leq \kappa  ~~\text{\small \cite{makhzani2013k}} \label{eq:holistic:dico_nmf}\nonumber
\end{numcases}

with $\bm{e}_i$ the $i$-th element of the canonical basis, $\mathbf{I}$ the identity matrix and $\bm{\psi}$ any neural network. 
In this context, $\m{V}$ is the \emph{dictionary} and $\m{U}$ the \emph{representation} of $\m{A}$ with the atoms of $\m{V}$. $\v{u}_i$ denote the $i$-th row of $\m{U}$. 
These methods extract the concept banks $\m{V}$ differently, thereby necessitating different interpretations\footnote{Concept extractions are typically overcomplete dictionaries, meaning that if the dictionary for each class is combined, $k >> p$, as noted in our previous section. The collapse problem in \autoref{sec:concepts:craft}, and a more detailed work \cite{bricken2023monosemanticity} suggest that overcomplete dictionaries are serious candidates to the superposition problem~\cite{elhage2022superposition}.}. 

In \ACE, the CAVs are defined as the centroids of the clusters found by the K-means algorithm.
Specifically, a concept vector $\cav_i$ in the matrix $\m{V}$ indicates a dense concentration of points associated with the corresponding concept, implying a repeated activation pattern. 
The main benefit of ACE comes from its reconstruction process, involving projecting activations onto the nearest centroid, which ensures that the representation will lie within the observed distribution (no out-of-distribution instances). %
However, its limitation lies in its lack of expressivity, as each activation representation is restricted to a single concept ($||\v{u}||_{0}=1$). As a result, it cannot capture compositions of concepts, leading to sub-optimal representations that fail to fully grasp the richness of the underlying data distribution.

On the other hand, the PCA benefits from superior reconstruction performance due to its lower constraints, as stated by the Eckart-Young-Mirsky~\cite{eckart1936approximation} theorem. %
The CAVs are the eigenvector of the covariance matrix: they indicate the direction in which the data variance is maximal. %
An inherent limitation is that the PCA will not be able to properly capture stable concepts that do not contribute to the sample variability (e.g. the dog-head concept might not be considered important by the PCA to explain the dog class if it is present across all examples).
Neural networks are known to cluster together the points belonging to the same category in the last layer to achieve linear separability (\cite{paypan2020collapse, fel2023craft}). Thus, the orthogonality constraint in the PCA might not be suitable to correctly interpret the manifold of the deep layer induced by points from the same class (it is interesting to note that this limitation can be of interest when studying all classes at once).
Also, unlike K-means, which produces strictly positive clusters if all points are positive (e.g., the output of ReLU), PCA has no sign constraint and can undesirably reconstruct out-of-distribution (OOD) activations, including negative values after ReLU. %

In contrast to K-Means, which induces extremely sparse representations, and PCA, which generates dense representations, the NMF (used in \CRAFT~and \ICE) strikes a harmonious balance as it provides moderately sparse representation. This is due to NMF relaxing the constraints imposed by the K-means algorithm (adding an orthogonality constraint on $\m{V}$ such that $\m{V} \m{V}^\tr = \mathbf{I}$ would yield an equivalent solution to K-means clustering~\cite{ding2005equivalence}). This sparsity facilitates the encoding of compositional representations that are particularly valuable when an image encompasses multiple concepts. Moreover, by allowing only additive linear combinations of components with non-negative coefficients, %
NMF inherently fosters a parts-based representation. This distinguishes NMF from PCA, which offers a holistic representation model. Interestingly, the NMF is known to yield representations that are interpretable by humans~\cite{zhang2021invertible, fel2023craft}.
Finally, the non-orthogonality of these concepts presents an advantage as it accommodates the phenomenon of superposition~\cite{elhage2022superposition}, wherein neurons within a layer may contribute to multiple distinct concepts simultaneously.

To summarize, we have explored three approaches to concept extraction, each necessitating a unique interpretation of the resulting Concept Activation Vectors (CAVs). Among these methods, NMF (used in \CRAFT~ and \ICE) emerges as a promising middle ground between PCA and K-means. Leveraging its capacity to capture intricate patterns, along with its ability to facilitate compositional representations and intuitive parts-based interpretations (as demonstrated in Figure~\ref{fig:holistic:qualitative_comparison}), NMF stands out as a compelling choice for extracting meaningful concepts from high-dimensional data. These advantages have been underscored by our human studies, and also evidenced by works such as~\cite{zhang2021invertible}.

\input{assets/holistic/table_uv}

\begin{figure}[t]
\begin{center}
   \includegraphics[width=.99\textwidth]{assets/holistic/qualitative_extraction.jpg}
\end{center}
   \caption{\textbf{Most important concepts extracted for the studied methods.} This qualitative example shows the three most important concepts extracted for the 'rabbit' class using a ResNet50 trained on ImageNet. The crops correspond to those maximizing each concepts $i$ (i.e., $\vx$ where $\m{U}(\vx)_i$ is maximal). As demonstrated in previous works \cite{zhang2021invertible,fel2023craft,parekh2022listen}, NMF (requiring positive activations) produces particularly interpretable concepts despite poorer reconstruction than PCA and being less sparse than K-Means. Details for the sparse Autoencoder architecture are provided in the appendix.}
\label{fig:holistic:qualitative_comparison}
\end{figure}

\paragraph{Evaluation of concept extraction}
Following the theoretical discussion of the various concept extraction methods, we conduct an empirical investigation of the previously discussed properties to gain deeper insights into their distinctions and advantages. In our experiment, we apply the PCA, K-Means, and NMF concept extraction methods on the penultimate layer of three state-of-the-art models. We subsequently evaluate the concepts using five different metrics (see Table \ref{tab:quantitative_comparison}). 
All five metrics are connected with the desired characteristics of a dictionary learning method. They include achieving a high-quality reconstruction (Relative l2), sparse encoding of concepts (Sparsity), ensuring the stability of the concept base in relation to $\m{A}$ (Stability), performing reconstructions within the intended domain (avoiding OOD), and maintaining the overall distribution during the reconstruction process (FID).
All the results come from 10 classes of ImageNet (the one used in Imagenette \cite{imagenette}), and are obtained using $n=16k$ images for each class. 



We begin our empirical investigation by using a set of standard metrics derived from the dictionary learning literature, namely Relative $l_2$ and Sparsity. 
Concerning the Relative $\ell_2$, PCA achieves the highest score among the three considered methods, confirming the theoretical expectations based on the Eckart–Young–Mirsky theorem~\cite{eckart1936approximation}, followed by NMF.
Concerning the sparsity of the underlying representation $\v{u}$, we compute the proportion of non-zero elements $||\v{u}||_0 / k$. Since K-means inherently has a sparsity of $1 / k$ (as induced by equation \ref{eq:holistic:dico_kmeans}), it naturally performs better in terms of sparsity, followed by NMF.


We deepen our investigation by proposing three additional metrics that offer complementary insights into the extracted concepts. Those metrics are the Stability, the FID, and the OOD score.
The Stability (as it can be seen as a loose approximation of algorithmic stability~\cite{bousquet2002stability}) measures how consistent concepts remain when they are extracted from different subsets of the data.
To evaluate Stability, we perform the concept extraction methods $N$ times on $K$-fold subsets of the data. Then, we map the extracted concepts together using a Hungarian loss function and measure the cosine similarity of the CAVs. If a method is stable, it should yield the same concepts (up to permutation) across each $K$-fold, where each fold consists of $1000$ images.
K-Means and NMF demonstrate the highest stability, while PCA appears to be highly unstable, which can be problematic for interpreting the results and may undermine confidence in the extracted concepts.

The last two metrics, FID and OOD, are complementary in that they measure: (i) how faithful the representations extracted are w.r.t the original distribution, and (ii) the ability of the method to generate points lying in the data distribution (non-OOD).
Formally, the FID quantifies the 1-Wasserstein distance~\cite{villani2009optimal} $\mathcal{W}_1$ between the empirical distribution of activation $\m{A}$, denoted $\mu_{\bm{a}}$, and the empirical distribution of the reconstructed activation $\m{U}\m{V}^\tr$ denoted $\mu_{\bm{u}}$. Thus, FID is calculated as $\text{FID} = \mathcal{W}_1(\mu_{\bm{a}}, \mu_{\bm{u}})$.
On the other hand, the OOD score measures the plausibility of the reconstruction by leveraging Deep-KNN~\cite{sun2022out}, a recent state-of-the-art OOD metric. More specifically,  we use the Deep-KNN score to evaluate the deviation of a reconstructed point from the closest original point. In summary, a good reconstruction method is capable of accurately representing the original distribution (as indicated by FID) while ensuring that the generated points remain within the model's domain (non-OOD). 
K-means leads to the best OOD scores because each instance is reconstructed as a centroid, resulting in proximity to in-distribution (ID) instances. However, this approach collapses the distribution to a limited set of points, resulting in low FID. On the other hand, PCA may suffer from mapping to negative values, which can adversely affect the OOD score. Nevertheless, PCA is specifically optimized to achieve the best average reconstructions. NMF, with fewer stringent constraints, strikes a balance by providing in-distribution reconstructions at both the sample and population levels.

In conclusion, the results clearly demonstrate NMF as a method that strikes a balance between the two approaches as NMF demonstrates promising performance across all tested metrics. Henceforth, we will use the NMF to extract concepts without mentioning it.



\paragraph{The Last Layer as a Promising Direction}
The various methods examined, namely \ACE, \ICE, and \CRAFT, generally rely on a deep layer to perform their decomposition without providing quantitative or theoretical justifications for their choice. 
To explore the validity of this choice, we apply the aforementioned metrics to each block's output in a ResNet50 model.
Figure~\ref{fig:holistic:metrics_across_layer} illustrates the metric evolution across different blocks, revealing a trend that favors the last layer for the decomposition. This empirical finding aligns with the practical implementations discussed above.


\begin{figure}[ht]
\begin{center}
   \includegraphics[width=.99\textwidth]{assets/holistic/metrics_across_layer.pdf}
\end{center}
   \caption{\textbf{Concept extraction metrics across layers.} The concept extraction methods are applied on activations probed on different blocks of a ResNet50 (B2 to B5). Each point is averaged over 10 classes of ImageNet using $16$k images for each class. We evaluate $3$ concept extraction methods: PCA (\dashed), NMF (\full), and KMeans (\dotted).
   }
\label{fig:holistic:metrics_across_layer}

\end{figure}
















\subsubsection{Concept importance}\label{sec:importance}


In this section, we leverage our framework to unify concept importance scoring using the existing attribution methods. Furthermore, we demonstrate that specifically in the case of decomposition in the penultimate layer, it exists optimal methods for importance estimation, namely RISE~\cite{petsiuk2018rise}, Integrated Gradients~\cite{sundararajan2017axiomatic}, Gradient-Input~\cite{shrikumar2017learning}, and Occlusion~\cite{zeiler2014visualizing}. We provide theoretical evidence to support the optimality of these methods.

\paragraph{From concept importance to attribution methods}
The dictionary learning formulation allows us to define the concepts $\m{V}$ in such a way that they are optimal to reconstruct the activation, i.e., $\m{A} \approx \m{U} \m{V}^\tr$. Nevertheless, this does not guarantee that those concepts are important for the model's prediction. For example, the ``grass'' concept might be important to characterize the activations of a neural network when presented with a 
St-Bernard image, but it might not be crucial for the network to classify the same image as a St Bernard~\cite{kim2018interpretability, adebayo2018sanity, ghorbani2017interpretation}. The notion of concept importance is precisely introduced to avoid such a confirmation bias and to identify the concepts used to classify among all detected concepts. 


We use the notion of Concept ATtribution methods (which we denote as \emph{CAT}s) to assess the concept importance score. The CATs are a generalization of the attribution methods: 
while attribution methods assess the sensitivity of the model output to a change in the pixel space, the concept importance evaluates the sensitivity to a change in the concept space. To compute the CATs methods, it is necessary to link the activation $\v{a} \in \Real^p$ to the concept base $\m{V}$ and the model prediction $\v{y}$. To do so, we feed the second part of the network ($\fb$) with the activation reconstruction ($\v{u} \m{V}^\tr \approx \v{a}$) so that $\v{y} = \fb(\v{u}\m{V}^\tr)$. Intuitively, a CAT method quantifies how a variation of  $\v{u}$ will impact $\v{y}$. 
We denote $\cam_i(\bm{u})$ the $i$-th coordinate of $\cam(\bm{u})$, so that it represents the importance of the $i$-th concept in the representation $\bm{u}$.  Equipped with these notations, we can leverage the sensitivity metrics introduced in standard attribution methods to re-define the current measures of concept importance, as well as introduce the new CATs borrowed from the attribution methods literature:


\scalebox{0.9}{\parbox{\linewidth}{%
\begin{empheq}[left={\cam_{i}(\v{u}) =\empheqlbrace}]{alignat=1}
&\nabla_{\v{u}_i} \fb(\v{u} \m{V}^\tr)
~~
\text{(\small TCAV: \cite{ghorbani2019towards,zhang2021invertible,graziani2021sharpening})}, \nonumber\\
&\displaystyle \frac{ \mathbb{E}_{\mathbf{m}_{\sim i}}( \mathbb{V}_{\mathbf{m}}( \fb( (\v{u} \odot \mathbf{m} ) \m{V}^\tr ) | \mathbf{m}_{\sim i} ) ) }{ \mathbb{V}( \fb( (\v{u} \odot \mathbf{m} ) \m{V}^\tr)) }
\qquad \qquad \qquad \qquad ~~~~~~~~~ \text{\small(Sobol: \CRAFT),}  \nonumber\\
&(\v{u}_i - \v{u}_i')  \times \int_0^1\nabla_{\v{u}_i}\fb((\v{u}' \alpha + (1 - \alpha)(\v{u} - \v{u}'))\m{V}^\tr) d\alpha
\qquad \text{\small(Int.Gradients)}, \nonumber\\
&\displaystyle \underset{\bm{\delta} \sim \mathcal{N}(0, \mathbf{I}\sigma)}{\mathbb{E}}(\nabla_{\v{u}_i} \fb( (\v{u} + \bm{\delta})\m{V}^T) )
\qquad \qquad \qquad \qquad \qquad \qquad ~~ \text{\small(Smoothgrad)}. \nonumber \\
\ldots \nonumber
\end{empheq} 
}}
\vspace{3mm}



The complete derivation of the 7 new CATs is provided in the appendix. 
In the derivations, $\nabla_{\v{u}_i}$ denotes the gradient with respect to the $i$-th coordinate of $\v{u}$, while $\mathbb{E}$ and $\mathbb{V}$ represent the expectation and variance, respectively, $\mathbf{m}$ is a mask of real-valued random variable between $0$ and $1$ (i.e $\mathbf{m}\sim\mathcal{U}([0,1]^p)$). We note that, when we use the gradient (w.r.t to $\v{u}_i$) as an importance score, we end up with the directional derivative used in the TCAV metric~\cite{kim2018interpretability}. In other words, one could say that TCAV is the Saliency of the \textit{Concept Attribution} methods. 

\CRAFT~leverages the Sobol-Hoeffding decomposition (used in sensitivity analysis), to estimate the concept importance. The Sobol indices measure the contribution of a concept as well as its interaction of any order with any other concepts to the output variance. Intuitively, the numerator for the Sobol importance formula is the expected variance that would be left if all variables but $\v{u}_i$ were to be fixed.

\begin{figure}[ht]
\begin{subfigure}[b]{0.49\textwidth}
\includegraphics[width=.99\textwidth]{assets/holistic/deletion_curves.jpg}
    \caption{}
\end{subfigure}
\hfil
\begin{subfigure}[b]{0.49\textwidth}
\includegraphics[width=.99\textwidth]{assets/holistic/deletion_layers_craft.png}
    \caption{}
\end{subfigure}
\caption{\textbf{(a) C-Deletion, C-Insertion curves.} Fidelity curves for C-Deletion depict the model's score as the most important concepts are removed. The results are averaged across 10 classes of ImageNet using a ResNet50 model.
\textbf{(b) C-Deletion, C-Insertion and C-$\mu$Fidelity across layer.} 
We report the $3$ metrics to evaluate CATs for each block (from B2 to B5) of a ResNet50. 
We evaluate $8$ Concept Attribution methods, all represented with different colors (see legend in  Figure~\ref{fig:holistic:deletion_curves}(a). The average trend of these eight methods is represented by the black dashed line (\dashed). Lower C-Deletion is better, higher C-Insertion and C-$\mu$Fidelity is better. Overall, it appears that the estimation of importance becomes more faithful towards the end of the model.
}
\label{fig:holistic:deletion_curves}
\end{figure}


\paragraph{Evaluation of concept importance methods}

Our generalization of the concept importance score, using the Concept ATtributions (CATs), allows us to observe that current concept-based explainability methods are only leveraging a small subset of concept importance methods. In Appendix~\ref{sup:holistic:all_cams}, we provide the complete derivation of $7$ new CATs based on the following existing attribution methods, notably: Gradient input~\cite{shrikumar2017learning}, Smooth grad~\cite{smilkov2017smoothgrad}, Integrated Gradients~\cite{sundararajan2017axiomatic}, VarGrad~\cite{hooker2018benchmark}, Occlusion~\cite{zeiler2014visualizing}, HSIC~\cite{novello2022making} and RISE~\cite{petsiuk2018rise}.



With the concept importance scoring now formulated as a generalization of attribution methods, we can borrow the metrics from the attribution domain to evaluate the faithfulness~\cite{jacovi2020towards,petsiuk2018rise,aggregating2020} of concept importance methods. In particular, 
we adapt three distinct metrics %
to evaluate the significance of concept importance scores: the C-Deletion~\cite{petsiuk2018rise}, C-Insertion~\cite{petsiuk2018rise}, and C-$\mu$Fidelity~\cite{aggregating2020} metrics.
In C-Deletion, we gradually remove the concepts (as shown in Figure \ref{fig:holistic:deletion_curves}), in decreasing order of importance, and we report the network's output each time a concept is removed. When a concept is removed in C-Deletion, the corresponding coordinate in the representation is set to $\bm{0}$. 
The final C-Deletion metrics are computed as the area under the curve in Figure~\ref{fig:holistic:deletion_curves}. For C-Insertion, this is the opposite: we start from a representation vector filled with zero, and we progressively add more concepts, following an increasing order of importance. 


For the C-$\mu$Fidelity, we calculate the correlation between the model's output when concepts are randomly removed and the importance assigned to those specific concepts.
The results across layers for a ResNet50 model are depicted in Figure \ref{fig:holistic:deletion_curves}b. We observe that decomposition towards the end of the model is preferred across all the metrics. As a result, in the next section, we will specifically examine the case of the penultimate layer.




\paragraph{A note on the last layer}
Based on our empirical results, it appears that the last layer is preferable for both improved concept extraction and more accurate estimation of importance. 
Herein, we derive theoretical guarantees about the optimality of concept importance methods in the penultimate layer. %
Without loss of generality, we assume $y \in \Real$ the logits of the class of interest. In the penultimate layer, the score $y$ is a linear combination of activations: $y=\bm{a}\m{W}+\bias$ for weight matrix $\m{W}$ and bias $\bias$. 
In this particular case, all CATs have a closed-form (see appendix~\ref{sup:holistic:closed_form}), that allows us to derive $2$ theorems. The first theorem tackles the CATs optimality for the C-Deletion and C-Insertion methods (demonstration in Appendix~\ref{sup:holistic:matroid}). We observe that the C-Deletion and C-Insertion problems can be represented as weighted matroids. Therefore the greedy algorithms lead to optimal solutions for CATs and a similar theorem could be derived for C-$\mu$Fidelity.
\begin{theorem}[Optimal C-Deletion, C-Insertion in the penultimate layer]
When decomposing in the penultimate layer,~\textbf{Gradient Input}, \textbf{Integrated Gradients}, \textbf{Occlusion}, and \textbf{Rise} yield the optimal solution for the C-Deletion and C-Insertion metrics.
More generally, any method $\cam(\v{u})$ that satisfies the condition 
$\forall (i, j) \in \{1, \ldots, k\}^2, 
(\v{u} \odot \e_i) \m{V}^\tr\m{W} \geq (\v{u} \odot \e_j) \m{V}^\tr \m{W}
\implies 
\cam(\v{u})_i \geq \cam(\v{u})_j 
$ yields the optimal solution.
\end{theorem}
\begin{theorem}[Optimal C-$\mu$Fidelity in the penultimate layer]
When decomposing in the penultimate layer,~\textbf{Gradient Input}, \textbf{Integrated Gradients}, \textbf{Occlusion}, and \textbf{Rise} yield the optimal solution for the C-$\mu$Fidelity metric.
\end{theorem}


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{assets/holistic/reliability.jpg}
    \caption{\textbf{From global (class-based) to local (image-based) importance.} Global importance can be decomposed into \textit{reliability} and \textit{prevalence} scores. Prevalence quantifies how frequently a concept is encountered, and reliability indicates how diagnostic a concept is for the class. The bar-charts are computed for the class ``Espresso'' on a ResNet50 (see Figure~\ref{fig:holistic:clustering_graph}, left panel)
    }
    \label{fig:holistic:barchart}
\end{figure}

Therefore, for all $3$ metrics, the concept importance methods based on Gradient Input, Integrated Gradient, Occlusion, and Rise are optimal, when used in the penultimate layer.

In summary, our investigation of concept extraction methods from the perspective of dictionary learning demonstrates that the NMF approach, specifically when extracting concepts from the penultimate layer, presents the most appealing trade-off compared to PCA and K-Means methods. In addition, our formalization of concept importance using attribution methods provided us with a theoretical guarantee for $4$ different CATs. Henceforth, we will then consider the following setup: a NMF on the penultimate layer to extract the concepts, combined with a concept importance method based on Integrated Gradient.

\subsubsection{Unveiling main strategies}

So far, the concept-based explainability methods have mainly focused on evaluating the global importance of concepts, i.e., the importance of concepts for an entire class~\cite{kim2018interpretability,fel2023craft}. This point can be limiting when studying misclassified data points, as we can speculate that the most important concepts for a given class might not hold for an individual sample (local importance). Fortunately, our formulation of concept importance using attribution methods gives us access to importance scores at the level of individual samples (\textit{i.e.,} $\cam(\v{u})$). Here, we show how to use these local importance scores to efficiently cluster data points based on the strategy used for their classification. 

The local (or image-based) importance of concepts can be integrated into global measures of importance for the entire class with the notion of \textit{prevalence} and \textit{reliability} (see Figure~\ref{fig:holistic:barchart}). A concept is said to be prevalent at the class level when it appears very frequently. A \textit{prevalence} score is computed based on the number of times a concept is identified as the most important one, i.e., $\argmax \cam(\v{u})$. At the same time, a concept is said to be reliable if it is very likely to trigger a correct prediction. The \textit{reliability} is quantified using the mean classification accuracy on samples sharing the same most important concept.


\paragraph{Strategic cluster graph.} In the strategic cluster graph (Figure~\ref{fig:holistic:clustering_graph} and Figure~\ref{fig:holistic:lemon}), we combine the notions of concept \textit{prevalence} and \textit{reliability} to reveal the main strategies of a model for a given category, more precisely, we reveal their repartition across the different samples of the class.
We use a dimensionality reduction technique (UMAP~\cite{mcinnes2018umap}) to arrange the data points based on the concept importance vector $\cam(\v{u})$ of each sample. Data points are colored according to the associated concept with the highest importance -- $\argmax \cam(\v{u})$. 
Interestingly, one can see in Figure~\ref{fig:holistic:clustering_graph} and Figure~\ref{fig:holistic:lemon} that spatially close points represent samples classified using \textit{similar strategies} -- as they exhibit similar concept importance -- and not necessarily similar embeddings.
For example, for the ``lemon'' object category (Figure \ref{fig:holistic:lemon}), the texture of the lemon peel is the most \textit{prevalent} concept, as it appears to be the dominant concept in $90\%$ of the samples (see the green cluster in Figure~\ref{fig:holistic:lemon}). We also observe that the concept ``pile of round, yellow objects'' is not reliable for the network to properly classify a lemon as it results in a mean classification accuracy of $40\%$ only (see top-left graph in Figure~\ref{fig:holistic:lemon}).

In Figure~\ref{fig:holistic:lemon} (right panel), we have exploited the strategic cluster graph to understand the classification strategies leading to bad classifications. For example, an orange ($1^{st}$ image, $1^{st}$ row) was classified as a lemon because of the peel texture they both share. Similarly, a cathedral roof was classified as a lemon because of the wedge-shaped structure of the structure ($4^{th}$ image, $1^{st}$ row). 




\begin{figure}[ht]
\begin{center}
   \includegraphics[width=1\textwidth]{assets/holistic/lemon.jpg}
\end{center}
   \caption{\textbf{Strategic cluster graph for the lemon category.} \textbf{Left}: U-MAP of lemon samples, in the concept space. Each concept is represented with its own color and is exemplified with example belonging to the cluster. The concepts are  \textcolor{red}{$\bullet$} the lemon wedge shape, \textcolor{yellow}{$\bullet$} a pile of round, yellow objects, \textcolor{blue}{$\bullet$} green objects hanging on a tree, and finally \textcolor{green}{$\bullet$} the peel texture, which is the predominant strategy. The reliability of each concept is shown in the top-left bar-chart. \textbf{Right}: 
   Example of images predicted as lemon along with their corresponding explanations. These misclassified images are recognized as lemons through the implementation of strategies that are captured by our proposed strategic cluster graph.
   }
\label{fig:holistic:lemon}
\end{figure}


\subsection{Discussion}

In this section, we have introduced a theoretical framework that unifies all modern concept-based explainability methods. Breaking down and formalizing the two essential steps in these methods, concept extraction and concept importance scoring, allowed us to better understand the underlying principles driving concept-based explainability. We leveraged this unified framework to propose new evaluation metrics for assessing the quality of extracted concepts. Through experimental and theoretical analyses, we justified the standard use of the last layer of an ANN for concept-based explanation. Finally, we harnessed the parallel between concept importance and attribution methods to gain insights into global concept importance (at the class level) by examining local concept importance (for individual samples). We proposed the strategic cluster graph, which provides insights into the strategy used by an ANN to classify images. We have provided an example use of this approach to better understand the failure cases of a system. Overall, our work demonstrates the potential benefits of the \textbf{dictionary learning} framework for automatic concept extraction and we hope this work will pave the way for further advancements and methodologies in the field au concept-based explainability.


In this research, we deliberately overlooked a particular challenge associated with the automatic concept approach, namely, the comprehensibility of the features extracted by dictionary-based methods. Indeed, relying solely on image segments to elucidate a concept could be restrictive. In the next section, we will examine an alternative approach to visualize concepts with feature visualization.
