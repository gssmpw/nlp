\definecolor{metalgreen}{RGB}{51, 157, 144}
\definecolor{metalorange}{RGB}{244, 161, 97}

We propose to directly start with our first work, \craft. As we have seen in \autoref{chap:attributions}, Attribution methods employ heatmaps to identify the most influential regions of an image that impact model decisions, and those methods have gained widespread popularity as a type of explainability method.
However, they only reveal~\where~the model looks, failing to elucidate \what~the model sees in those areas.
In this section, we will try to fill in this gap with \craft -- a novel approach to identify both ``\what'' and ``\where'' by generating concept-based explanations.
We introduce 3 new ingredients to the automatic concept extraction literature: (\textbf{i}) a recursive strategy to detect and decompose concepts across layers, (\textbf{ii}) a novel method for a more faithful estimation of concept importance using Sobol indices, and (\textbf{iii}) the use of implicit differentiation to unlock Concept Attribution Maps.

We conduct both human and computer vision experiments, specifically the one proposed in \autoref{sec:attributions:metapred}, to demonstrate the benefits of the proposed approach. We show that the proposed concept importance estimation technique -- based on Sobol indices -- is more faithful to the model than previous methods. Moreover, we have open-sourced our code at
\href{https://github.com/deel-ai/Craft}{\nolinkurl{github.com/deel-ai/Craft}}, and also in the \href{https://github.com/deel-ai/xplique}{Xplique} library.

\begin{figure}[ht]\centering
\includegraphics[width=0.99\textwidth]{assets/craft/moon.pdf}
\caption{\textbf{The ``Man on the Moon'' incorrectly classified as a ``shovel'' by an ImageNet-trained ResNet50.} Heatmap generated by a classic attribution method~\cite{petsiuk2018rise} (left) vs.  \textit{concept attribution maps} generated with the proposed \craft~approach (right) which highlights the two most influential concepts that drove the ResNet50's decision along with their corresponding locations. 
\craft~suggests that the neural net arrived at its decision because it identified the concept of ``dirt'' \textcolor{green}{$\bullet$} commonly found in members of the image class ``shovel'' and the concept of ``ski pants'' \textcolor{violet}{$\bullet$} typically worn by people clearing snow from their driveway with a shovel instead the correct concept of astronaut's pants (which was probably never seen during training).
}
\label{fig:craft:shovel}
\end{figure}


\subsection{Background}

\begin{figure*}[t!]\centering
\includegraphics[width=0.99\textwidth]{assets/craft/big_picture.pdf}
\caption{\textbf{\craft~results for the prediction ``chain saw''.} 
First, our method uses Non-Negative Matrix Factorization (NMF) to extract the most relevant concepts used by the network (ResNet50V2) from the train set (ILSVRC2012~\cite{imagenet_cvpr09}). The global influence of these concepts on the predictions is then measured using Sobol indices (right panel). Finally, the method provides local explanations through \textit{concept attribution maps} (heatmaps associated with a concept, and computed using grad-CAM by backpropagating through the NMF concept values with implicit differentiation).
Besides, concepts can be interpreted by looking at crops that maximize the NMF coefficients. For the class ``chain saw'', the detected concepts seem to be:
\textcolor{blue}{$\bullet$} the chainsaw engine, 
\textcolor{purple}{$\bullet$} the saw blade, 
\textcolor{green}{$\bullet$} the human head, 
\textcolor{orange}{$\bullet$} the vegetation, 
\textcolor{red}{$\bullet$} the jeans and
\textcolor{dark}{$\bullet$} the tree trunk.
}
\label{fig:craft:craft_demo}
\end{figure*}


\paragraph{Attribution methods}

Attribution methods are widely used as post-hoc explainability techniques to determine the input variables that contribute to a model's prediction by generating importance maps, such as the ones shown in Fig.\ref{fig:craft:shovel}. The first attribution method, Saliency, introduced in~\cite{zeiler2014visualizing}, generates a heatmap by utilizing the gradient of a given classification score with respect to the pixels. This method was later improved upon in the context of deep convolutional networks for classification in subsequent studies, such as~\cite{zeiler2013visualizing, springenberg2014striving, sundararajan2017axiomatic, smilkov2017smoothgrad}.

Unfortunately, a severe limitation of these approaches -- apart from the fact that they only show the ``\where'' -- is that they are subject to confirmation bias: while they may appear to offer useful explanations to a user, sometimes these explanations are actually incorrect~\cite{adebayo2018sanity, ghorbani2017interpretation,slack2020fooling}.
These limitations raise questions about their usefulness, as recent research has shown by using human-centered experiments to evaluate the utility of attribution~\cite{hase2020evaluating,nguyen2021effectiveness,fel2021cannot,kim2021hive,shen2020useful}.

In particular, in our previous \autoref{sec:attributions:metapred}, we have proposed a protocol to measure the usefulness of explanations, corresponding to how much they help users identify rules driving a model's predictions (correct or incorrect) that transfer to unseen data -- using the concept of meta-predictor (also called simulatability)~\cite{kim2016examples,doshivelez2017rigorous,fong2017meaningful}. 
The main idea is to train users to predict the output of the system using a small set of images along with associated model predictions and corresponding explanations. 
A method that performs well on this this benchmark is said useful, as it help users better predict the output of the model by providing meaningful information about the internal functioning of the model.
This framework being agnostic to the type of explainability method, we have chosen to use it in Section~\ref{sec:craft:exp} in order to compare \craft~with attribution methods.




\paragraph{Concepts-based methods}
\cite{kim2018interpretability} introduced a method aimed at providing explanations that go beyond attribution-based approaches by measuring the impact of pre-selected concepts on a model's outputs. Although this method appears more interpretable to human users than standard attribution techniques, it requires a database of images describing the relevant concepts to be manually curated.
Ghorbani et al.~\cite{ghorbani2019towards} further extended the approach to extract concepts  without the need for human supervision. The approach, called ACE~\cite{ghorbani2019towards}, uses a segmentation scheme on images, that belong to an image class of interest. %
The authors leveraged the intermediate activations of a neural network for specific image segments. These segments were resized to the appropriate input size and filled with a baseline value. The resulting activations were clustered to produce prototypes, which they referred to as "concepts". However, some concepts contained background segments, leading to the inclusion of uninteresting and outlier concepts. To address this, the authors implemented a postprocessing cleanup step to remove these concepts, including those that were present in only one image of the class and were not representative. While this improved the interpretability of their explanations to human subjects, the use of a baseline value filled around the segments could introduce biases in the explanations~\cite{hsieh2020evaluations,sturmfels2020visualizing,haug2021baselines,kindermans2019reliability}.

Zhang et al.~\cite{zhang2021invertible} developed a solution to the unsupervised concept discovery problem by using matrix factorizations in the latent spaces of neural networks. However, one major drawback of this method is that it operates at the level of convolutional kernels, leading to the discovery of localized concepts. For example, the concept of "grass" at the bottom of the image is considered distinct from the concept of "grass" at the top of the image.


Here, we try to fill these gaps with a novel method called \craft~which uses Non-Negative Matrix Factorization (NMF)~\cite{lee1999learning} for concept discovery. In contrast to other concept-based explanation methods, our approach provides an explicit link between their global and local explanations (Fig.~\ref{fig:craft:craft_demo}) and identifies the relevant layer(s) to use to represent individual concepts (Fig.~\ref{fig:craft:collapse}). Our main contributions can be described as follows:

{\textbf{(i)}} A novel approach for the automated extraction of high-level concepts learned by deep neural networks. We validate its practical utility to users with human psychophysics experiments.

{\textbf{(ii)}} A recursive procedure to automatically identify concepts and sub-concepts at the right level of granularity -- starting with our decomposition at the top of the model and working our way upstream. We validate the benefit of this approach with human psychophysics experiments showing that (i) the decomposition of a concept yields more coherent sub-concepts and (ii) that the groups of points formed by these sub-concepts are more refined and appear meaningful to humans.

{\textbf{(iii)}} A novel technique to quantify the importance of individual concepts for a model's prediction using Sobol indices~\cite{sobol1993sensitivity,da2013efficient,sobol2001,sobol2005global,saltelli2002} -- a technique borrowed from Sensitivity Analysis.

{\textbf{(iv)}} The first concept-based explainability method which produces  \textit{concept attribution maps} by backpropagating  concept scores  into the pixel space by leveraging the implicit function theorem in order to localize the pixels associated with the concept of a given input image. This effectively opens up the toolbox of both white-box~\cite{smilkov2017smoothgrad, zeiler2014visualizing, sundararajan2017axiomatic, selvaraju2017gradcam, springenberg2014striving, eva2} and black-box~\cite{ribeiro2016lime, lundberg2017unified, petsiuk2018rise, fel2021sobol} explainability methods to derive concept-wise attribution maps.


\begin{figure*}[t!]
\centering\includegraphics[width=1.0\textwidth]{assets/craft/figure2.pdf}
\caption{ 
\textbf{(1) Neural collapse (amalgamation).}
A classifier needs to be able to linearly separate classes by the final layer. It is commonly assumed that in order to achieve this, image activations from the same class get progressively ``merged'' such that these image activations converge to a one-hot vector associated with the class at the level of the logits layer~\cite{paypan2020collapse}. 
In practice, this means that different concepts get ultimately blended together along the way. 
\textbf{(2) Recursive process.} When a concept is not understood (e.g., $\mathcal{C}$), we propose to decompose it into multiple sub-concepts (e.g., $\mathcal{C}_{\textcolor{green}{1}}, \mathcal{C}_{\textcolor{purple}{2}}, \mathcal{C}_{\textcolor{blue}{3}}$) using the activations from an earlier layer to overcome the aforementioned neural collapse issue.
\textbf{(3) Example of recursive concept decomposition} using \craft~on the ImageNet class ``parachute''.
}
\label{fig:craft:collapse}
\end{figure*}


\subsection{Overview of the method} \label{sec:craft:method}

In this section, we first describe our concept activations factorization method. Below we highlight the main differences with related work.
We then proceed to introduce the three novel ingredients that make up \craft: %
(1) a method to recursively decompose concepts into sub-concepts, 
(2) a method to better estimate the importance of extracted concepts, and 
(3) a method to use any attribution method to create \textit{concept attribution maps}, using implicit differentiation~\cite{krantz2002implicit,griewank2008evaluating,blondel2021implicitdiff}.%



\paragraph{Notations}
In this work, we consider a general supervised learning setting, where $(\vx_1, ..., \vx_n) \in \sx^n \subseteq \Real^{n \times d}$ are $n$ inputs images and $(y_1, ..., y_n) \in \sy^n$ their associated labels. 
We are given a (machine-learnt) black-box predictor $\f : \sx \to \sy$, which at some test input $\vx$ predicts the output $\f(\vx)$.
Without loss of generality, we establish that $\f$ is a neural network that can be decomposed into two distinct components. The first component is a function $\v{g}$ that maps the input to an intermediate state, and the second component is $\v{h}$, which takes this intermediate state to the output, such that $\f(\vx) = (\v{h} \circ \v{g})(\vx)$. In this context, $\v{g}(\vx) \subseteq \Real^p$ represents the intermediate activations of $\vx$ within the network.
Further, we will assume non-negative activations: $ \v{g}(\vx) \geq 0$. In particular, this assumption is verified by any architecture that utilizes \textit{ReLU}, but any non-negative activation function works. 



\subsubsection{Concept activation factorization.}\label{subsec:caf}

We use Non-negative matrix factorization to identify a basis for concepts based on a network's activations (Fig.\ref{fig:craft:craft}). Inspired by the approach taken in ACE~\cite{ghorbani2019towards}, we will use image sub-regions to try to identify coherent concepts. 

The first step involves gathering a set of images that one wishes to explain, such as the dataset, in order to generate associated concepts. In our examples, to explain a specific class $y \in \sy$, we selected the set of points $\mathcal{C}$ from the dataset for which the model's predictions matched a specific class $\mathcal{C} = \{ \vx_i : \f(\vx_i) = y, 1 \leq i \leq n \}$.
It is important to emphasize that this choice is significant. The goal is not to understand how humans labeled the data, but rather to comprehend the model itself. By only selecting correctly classified images, important biases and failure cases may be missed, preventing a complete understanding of our model.

Now that we have defined our set of images, we will proceed with selecting sub-regions of those images to identify specific concepts within a localized context. It has been observed that the implementation of segmentation masks suggested in ACE can lead to the introduction of artifacts due to the associated inpainting with a baseline value.
In contrast, our proposed method takes advantage of the prevalent use of modern data augmentation techniques such as randaugment, mixup, and cutmix during the training of current models.
These techniques involve the current practice of models being trained on image crops, which enables us to leverage a straightforward crop and resize function denoted by $\bm{\pi}(\cdot)$ to create sub-regions (illustrated in Fig.\ref{fig:craft:craft}). By applying $\bm{\pi}$ function to each image in the set $\mathcal{C}$, we obtain an auxiliary dataset $\mx \in \Real^{n \times d}$ such that each entries $\mx_i = \bm{\pi}(\vx_i)$ is an image crop.

To discover the concept basis, we start by obtaining the activations for the random crops $\m{A} = \v{g}(\mx) \in \Real^{n \times p}$.
In the case where $\f$ is a convolutional neural network, a global average pooling is applied to the activations.


We are now ready to apply Non-negative Matrix Factorization (NMF) to decompose  positive activations $\m{A}$ into a product of non-negative, low-rank matrices $\m{U} \in \Real^{n \times r}$ and $\m{W} \in \Real^{p \times r}$ by solving:

\begin{equation}
\label{eq:craft:nmf}
(\m{U}, \m{W}) = \argmin_{\m{U} \geq 0, \m{W} \geq 0} ~ \frac{1}{2}\|\m{A} - \m{U}\m{W}^\tr \|^2_{F}, %
\end{equation}  
where $||\cdot||_F$ denotes the Frobenius norm.

This decomposition of our activations $\m{A}$ yields two matrices: $\m{W}$ containing our Concept Activation Vectors (CAVs) and $\m{U}$ that redefines the data points in our dataset according to this new basis. Moreover, this decomposition in this new basis has some interesting properties that go beyond the simple low-rank factorization -- since $r \ll \min(n,p)$.
First, NMF can be understood as the joint learning of a dictionary of Concept Activation Vectors -- called a ``concept bank'' in Fig.~\ref{fig:craft:craft} -- that maps a $\Real^p$ basis onto $\Real^r$, and $\m{U}$ the coefficients of the vectors $\m{A}$ expressed in this new basis. 
The minimization of the reconstruction error $\frac{1}{2}\|\m{A} - \m{U}\m{W}\|^2_F$ ensures that the new basis contains (mostly) relevant concepts. Intuitively, the non-negativity constraints $\m{U} \geq 0, \m{W} \geq 0$ encourage (\textbf{\textit{i}}) $\m{W}$ to be sparse (useful for creating disentangled concepts), (\textbf{\textit{ii}})  $\m{U}$ to be sparse (convenient for selecting a minimal set of useful concepts)  and (\textbf{\textit{iii}})  missing data to be imputed~\cite{ren2020using}, which corresponds to the sparsity pattern of \textit{post-ReLU} activations $\m{A}$. 

It is worth noting that each input $\mx_i$ can be expressed as a linear combination of concepts denoted as $\m{A}_i = \sum_{j=1}^r U_{i,j} \m{W}_j^\tr$.  This approach is advantageous because it allows us to interpret each input as a composition of the underlying concepts. Furthermore, the strict positivity of each term -- NMF is working over the anti-negative semiring, -- enhances the interpretability of the decomposition. Another interesting interpretation could be that each input is represented as a superposition of concepts~\cite{elhage2022superposition}.

While other methods in the literature solve a similar problem (such as low-rank factorization using SVD or ICA), the NMF is both fast and effective and is known to yield concepts that are meaningful to humans~\cite{fu2019nonnegative,zhang2021invertible}. Finally, once the concept bank $\m{W}$ has been precomputed, we can associate the concept coefficients $\bm{u} \in \Real^r$ to any new input $\vx$ (e.g., a full image) by solving the underlying Non-Negative Least Squares (NNLS) problem $\min_{\bm{u} \geq 0} ~ \frac{1}{2}\|\v{g}(\vx) - \bm{u}\m{W}^\tr\|^2_{F}$, and therefore recover its decomposition in the concept basis.


\begin{figure*}[t!]
\centering
\centering
\includegraphics[width=1.0\textwidth]{assets/craft/figure3.pdf}

\caption{
\textbf{Overview of \craft.}
Starting from a set of crops $\vx$ containing a concept $\mathcal{C}$ (e.g., crops images of the class ``parachute''), we compute activations $\v{g}(\vx)$ corresponding to an intermediate layer from a neural network for random image crops. 
We then factorize these activations into two lower-rank matrices, $(\textcolor{metalgreen}{\m{U}}, \textcolor{metalorange}{\m{W}})$. $\textcolor{metalorange}{\m{W}}$ is what we call a ``concept bank'' and is a new basis used to express the activations, while $\textcolor{metalgreen}{\m{U}}$ corresponds to the corresponding coefficients in this new basis.
We then extend the method with 3 new ingredients: (1) recursivity -- by proposing to re-decompose a concept (e.g., take a new set of images containing $\mathcal{C}_{\textcolor{red}{1}}$) at an earlier layer, (2) a better importance estimation using Sobol indices and (3) an approach to leverage implicit differentiation to generate \textit{concept attribution maps} to localize concepts in an image.
}
\label{fig:craft:craft}
\end{figure*}

In essence, the core of our method can be summarized as follows: using a set of images, the idea is to re-interpret their embedding at a given layer as a composition of concepts that humans can easily understand. 
In the next section, we show how one can recursively apply concept activation factorizations to preceding layer for an image containing a previously computed concept.



\subsubsection{Ingredient 1: A pinch of recursivity}\label{subsec:rec}



One of the most apparent issues in previous work~\cite{ghorbani2019towards,zhang2021invertible} is the need for choosing a priori a layer at which the activation maps are computed. This choice will critically affect the concepts that are identified  because certain concepts get amalgamated~\cite{paypan2020collapse} into one at different layers of the neural network, resulting in incoherent and indecipherable clusters, as illustrated in Fig.~\ref{fig:craft:collapse}. We posit that this can be solved by iteratively applying our decomposition at different layer depths, and for the concepts that remain difficult to understand, by looking for their sub-concepts in earlier layers by isolating the images that contain them. This allows us to build hierarchies of concepts for each class. 


We offer a simple solution consisting of reapplying our method to a concept by performing a second step of concept activation factorization on a set of images that contain the concept $\mathcal{C}$ in order to refine it and create sub-concepts (e.g., decompose $\mathcal{C}$ into $\{ \mathcal{C}_1,\mathcal{C}_2,\mathcal{C}_3 \}$) see Fig.~\ref{fig:craft:collapse} for an illustrative example. 
Note that we generalize current methods in the sense that taking images $(\vx_1, ..., \vx_n)$ that are clustered in the logits layer (belonging to the same class) and decomposing them in a previous layer -- as done in \cite{ghorbani2019towards, zhang2021invertible} -- is a valid recursive step.
For a more general case, let us assume that a set of images that contain a common concept is obtained using the first step of concept activation factorization. 

We will then take a subset of the auxiliary dataset points to refine any concept $j$. To do this, we select the subset of points that contain the concept $\mathcal{C}_j = \{\vx_i : U_{i,j} > \lambda_j, 1 \leq i \leq n \}$, where $\lambda_j$ is the 90th percentile of the values of the concept $\m{U}_{:,j}$ across the $n$ points. In other words, the 10\% of images that activate the concept $j$ the most are selected for further refinement into sub-concepts.
Given this new set of points, we can then re-apply the Concept Matrix Factorization method to an earlier layer to obtain the sub-concepts decomposition from the initial concept -- as illustrated in Fig.\ref{fig:craft:collapse}.



\subsubsection{Ingredient 2: A dash of sensitivity analysis}\label{subsec:sobol}



A major concern with concept extraction methods is that concepts that makes sense to humans are not necessarily the same as those being used by a model to classify images.
In order to prevent such confirmation bias during our concept analysis phase, a faithful estimate the overall importance of the extracted concepts is crucial. 
Kim et al.~\cite{kim2018interpretability} proposed an importance estimator based on directional derivatives: the partial derivative of the model output with respect to the vector of concepts. 
While this measure is theoretically grounded, it relies on the same principle as gradient-based methods, and thus, suffers from the same pitfalls: neural network models have noisy gradients~\cite{smilkov2017smoothgrad,sundararajan2017axiomatic}. Hence, the farther the chosen layer is from the output, the noisier the directional derivative score will be.


Since we essentially want to know which concept has the greatest effect on the output of the model, it is natural to consider the field of sensitivity analysis~\cite{sobol2005global, sobol1993sensitivity, sobol2001, cukier1973study,idrissi2021developments}.
In this section, we briefly recall the classic ``total Sobol indices'' on wich we based our previous method in the \autoref{sec:attributions:sobol}, and how to apply them to our problem. The complete derivation of the Sobol-Hoeffding decomposition for concepts is presented in Section~\ref{apdx:sobol} of the supplementary materials.
Formally, a natural way to estimate the importance of a concept $i$ is to measure the fluctuations of the model's output $\v{h}(\m{U} \m{W}^\tr)$ in response to meaningful perturbations of the concept coefficient $\m{U}_{:,i}$ across the $n$ points.
Concretely, we will use perturbation masks $\rm{M}  = (\r{M}_1, ..., \r{M}_r) \sim \mathcal{U}([0, 1]^r)$, here an i.i.d sequence of real-valued random variables, we introduce a concept fluctuation to generate a perturbed activation $\rm{A} = (\m{U} \odot \rm{M})\m{W}^\tr$ where $\odot$ denote the Hadamard product (e.g., the masks can be used to remove a concept by setting its value to zero). We can then propagate this perturbed activation to the model output and get the associated random output $\rm{Y} = \v{h}(\rm{A})$.
Simply put, removing or applying perturbation of an important concept will result in a substantial variation in the output, whereas an unused concept will have minimal effect on the output.

Finally, we can capture the importance that a concept might have as a main effect -- along with its interactions with other concepts -- on the model's output by calculating the expected variance that would remain if all the concepts except the $i$ were to be fixed. This yields the general definition of the total Sobol indices.


\begin{definition}[\textbf{Total Sobol indices for Concept}]
\textit{The total Sobol index $\mathcal{S}^T_i$, which measures the contribution of a concept $i$ as well as its interactions of any order with any other concepts to the model output variance, is given by:}

\begin{align}
\label{eq:craft:total_sobol}
\mathcal{S}^T_i 
& = \frac{ \E_{\rm{M}_{\sim i}}( \V_{M_i} ( \rm{Y} | \rm{M}_{\sim i} )) }{ \V(\rm{Y}) } \\
& = \frac{ \E_{\bm{M}_{\sim i}}( \V_{M_i} ( \v{h}((\m{U} \odot \rm{M})\m{W}^\tr) | \rm{M}_{\sim i} )) }{ \V( \v{h}((\m{U} \odot \rm{M})\m{W}^\tr)) }.
\end{align}
\end{definition}



In practice, this index can be calculated very efficiently~\cite{saltelli2010variance, marrel2009calculations, janon2014asymptotic, owen2013better, tarantola2006random}, more details on the Quasi-Monte Carlo sampling and the estimator used are left in appendix~\ref{apdx:sobol}.


\begin{figure*}[ht]
\centering
\includegraphics[width=0.95\textwidth]{assets/craft/qualitative.jpg}
\caption{
\textbf{Qualitative Results:} \craft~results on 6 classes of ILSVRC2012~\cite{imagenet_cvpr09} for a trained ResNet50V2. The results showcase the top 3 most important concepts for each class. This is done by displaying crop images that activate the concept the most (using $\m{U}$) and also feature visualization~\cite{olah2017feature} of the associated CAVs (using $\m{W}$). 
}
\label{fig:craft:qualitative}
\end{figure*}

\subsubsection{Ingredient 3: A smidgen of implicit differentiation}\label{subsec:cam}

Attribution methods are useful for determining the regions deemed important by a model for its decision, but they lack information about what exactly triggered it.
We have seen that we can already extract this information from the matrices $\m{U}$ and $\m{W}$, but as it is, we do not know in what part of an image a given concept is represented.
In this section, we will show how we can leverage attribution methods (forward and backward modes) to find where a concept is located in the input image (see Fig.~\ref{fig:craft:craft_demo}). Forward attribution methods do not rely on any gradient computation as they only use inference processes, whereas backward methods require back-propagating through a network's layers. By application of the chain rule, computing $\partial \m{U} / \partial \mx$ requires access to $\partial \m{U} /\partial \m{A}$.  





To do so, one could be tempted to solve the linear system $\m{U}\m{W}^\tr=\m{A}$. 
However, this problem is ill-posed since $\m{W}^\tr$ is low rank. A standard approach is to calculate the Moore-Penrose pseudo-inverse $(\m{W}^\tr)^\pinv$, which solves rank deficient systems by looking at the minimum norm solution~\cite{barata2012moore}. In practice, $(\m{W}^\tr)^{\dagger}$ is computed with the Singular Value Decomposition (SVD) of $\m{W}^\tr$. Unfortunately, SVD is also the solution to the \textit{unstructured minimization} of $\frac{1}{2}\|\m{A}-\m{U}\m{W}^\tr\|^2_F$ by the Eckart-\-Young-\-Mirsky theorem~\cite{eckart1936approximation}. Hence, the non-negativity constraints of the NMF are ignored, which prevents such approaches from succeeding. Other issues stem from the fact that the $\m{U},\m{W}$ decomposition is generally not unique.



Our third contribution consists of tackling this problem to allow the use of attribution methods, i.e., \textit{concept attribution maps}, by proposing a strategy to differentiate through the NMF block.

\paragraph{Implicit differentiation of NMF block}


The NMF problem~\ref{eq:craft:nmf} is NP-hard~\cite{vavasis2010complexity}, and it is not convex with respect to the input pair $(\m{U},\m{W})$. However, fixing the value of one of the two factors and optimizing the other turns the NMF formulation into a pair of Non-Negative Least Squares (NNLS) problems, which are convex. This ensures that alternating minimization (a standard approach for NMF) of $(\m{U},\m{W})$ factors will eventually reach a local minimum.
Each of this alternating NNLS problems fulfills the Karush-–Kuhn-–Tucker (KKT) conditions~\cite{karush1939minima,kuhn1951nonlinear}, which can be encoded in the so-called \textit{optimality function} $\implicit$ from \cite{blondel2021implicitdiff}, see Eq.~\ref{apeq:craft:optimality_fun} Appendix~\ref{app:craft:implicit}. The implicit function theorem~\cite{griewank2008evaluating} allows us to use implicit differentiation~\cite{krantz2002implicit,griewank2008evaluating,bell2008algorithmic} to efficiently compute the Jacobians $\partial \m{U}/ \partial \m{A}$ and $\partial \m{W} / \partial \m{A}$ without requiring to back-propagate through each of the iterations of the NMF solver:

Let the optimality function $\implicit$, as introduced in Blondel et al. (2021) and based on the Karush-Kuhn-Tucker (KKT) conditions (Karush, 1939; Kuhn and Tucker, 1951), encapsulate the optimality conditions of the Non-negative Matrix Factorization (NMF) problem as formulated in Equation \ref{eq:craft:nmf}. The function $\implicit$ is defined for a given matrix $\m{A}$ and the tuple of matrices $(\m{U},\m{W},\bar{\m{U}},\bar{\m{W}})$ as follows:

\begin{theorem}[Implicit differentiation of NMF.] Let the optimality function $\implicit$ as introduced in~\cite{blondel2021implicitdiff} adapted for the Karush-Kuhn-Tucker (KKT) conditions~\cite{karush1939minima,kuhn1951nonlinear} capturing the optimality conditions of the problem~\ref{eq:craft:nmf} reads:

\begin{equation}
    \implicit((\m{U},\m{W},\bar{\m{U}},\bar{\m{W}}),\m{A})=
    \begin{cases}
    (\m{U}\m{W}^T-\m{A})\m{W}-\bar{\m{U}}    ,& \\ 
    (\m{W}\m{U}^T-\m{A}^T)\m{U}-\bar{\m{W}}  ,& \\ 
    \bar{\m{U}} \odot \m{U}   ,& \\ 
    \bar{\m{W}} \odot \m{W}   .& \\
    \end{cases}
\end{equation}

Given the optimal tuple $(\m{U},\m{W},\bar{\m{U}},\bar{\m{W}})$ that constitutes a root of $\implicit$ which is a root of $\implicit$, then, the implicit differentiation yields:

\begin{equation}
\frac{\partial (\m{U},\m{W},\bar{\m{U}},\bar{\m{W}})}{\partial \m{A}}=-(\partial_1 \implicit)^{-1}\partial_2 \implicit.
\end{equation}
\end{theorem}

See \autoref{app:craft:implicit} for full derivation. In particular, this requires the dual variables $\bar{\m{U}}$ and $\bar{\m{W}}$, which are not computed in scikit-learn's~\cite{pedregosa2011scikit} popular implementation\footnote{Scikit-learn uses a block coordinate descent algorithm~\cite{cichocki2009fast,fevotte2011algorithms}, with a randomized SVD initialization.}. Consequently, we leverage the work of~\cite{huang2016flexible} and we re-implement our own solver with Jaxopt~\cite{blondel2021implicitdiff} based on ADMM~\cite{boyd2011distributed}, a GPU friendly algorithm (see Appendix~\ref{app:craft:implicit}).


\input{assets/craft/utility}

Concretely, given our concepts bank $\m{W}$, the concept attribution maps of a new input $\vx$ are calculated by solving the NNLS problem $\min_{\m{U} \geq 0} \frac{1}{2}\|\v{g}(\vx)-\m{U}\m{W}^\tr\|^2_F$. The implicit differentiation of the NMF block $\partial \m{U} / \partial \m{A}$ is integrated into the classic back-propagation to obtain $\partial \m{U} / \partial \vx$. Most interestingly, this technical advance enables the use of all white-box explainability methods~\cite{smilkov2017smoothgrad, zeiler2014visualizing, sundararajan2017axiomatic, selvaraju2017gradcam, springenberg2014striving} to generate concept-wise attribution maps and trace the part of an image that triggered the detection of the concept by the network. Additionally, it is even possible to employ black-box methods~\cite{ribeiro2016lime, petsiuk2018rise, lundberg2017unified, fel2021sobol} since it only amounts to solving an NNLS problem. %



\subsection{Experimental evaluation}\label{sec:craft:exp}


In order to evaluate the interest and the benefits brought by \craft, we start in Section~\ref{sec:craft:expUtility} by assessing the practical utility of the method on a human-centered benchmark composed of 3 XAI scenarios and presented in~\autoref{sec:attributions:metapred}.

After demonstrating the usefulness of the method using these human experiments, we independently validate the 3 proposed ingredients.
First, we provide evidence that recursivity allows refining concepts, making them more meaningful to humans using two additional human experiments in Section \ref{sec:craft:expRecursivity}.
Next, we evaluate our new Sobol estimator and show quantitatively that it provides a more faithful assessment of concept importance in Section~\ref{sec:craft:expSobol}.
Finally, we run an ablation experiment that measures the interest of local explanations based on concept attribution maps coupled with global explanations.
Additional experiments, including a sanity check and an example of deep dreams applied on the concept bank, as well as many other examples of local explanations for randomly picked images from ILSVRC2012, are included in Section~\ref{apx:craft:more-craft} of the supplementary materials.
We leave the discussion on the limitations of this method and on the broader impact in appendix~\ref{apx:craft:limitations}.

\subsubsection{Utility Evaluation}
\label{sec:craft:expUtility}


As emphasized by Doshi-Velez et al.~\cite{doshivelez2017rigorous}, the goal of XAI should be to develop methods that help a user better understand the behavior of deep neural network models. An instantiation of this idea was proposed in \autoref{sec:attributions:metapred} where we described an experimental framework to quantitatively measure the practical usefulness of explainability methods in real-world scenarios. In the initial setup, we recruited  $n=1,150$ online participants (evaluated over 8 unique conditions and 3 AI scenarios) -- making it the largest benchmark to date in XAI. Here, we extend our framework to allow for the robust evaluation of the utility of our proposed \craft~method and the related ACE.
The 3 representative real-world scenarios are: (1) identifying bias in an AI system (using Husky vs Wolf dataset from~\cite{ribeiro2016lime}), (2) characterizing the visual strategy that are too difficult for an untrained non-expert human observer (using  the Paleobotanical dataset from \cite{wilf2016computer}), (3) understanding complex failure cases (using ImageNet ``Red fox'' vs ``Kit fox'' binary classification).
Using this benchmark, we evaluate \craft, ACE, as well as \craft~with only the global concepts (\craft CO) to allow for a fair comparison with ACE.
To the best of our knowledge, we are the first to systematically evaluate concept-based methods against attribution methods.

Results are shown in Table~\ref{tab:utility} and demonstrate the benefit of \craft, which achieves higher scores than all of the attribution methods tested as well as ACE in the first two scenarios. To date, no method appears to exceed the baseline on the third scenario suggesting that additional work is required.
We also note that, in the first two scenarios, \craft CO is one of the best-performing methods and it always outperforms ACE -- meaning that even without the local explanation of the concept attribution maps, \craft~largely outperforms ACE. Examples of concepts produced by \craft~are shown in the Appendix~\ref{app:craft:utility}.


\subsubsection{Validation of Recursivity}
\label{sec:craft:expRecursivity}

\begin{table}
\centering
\begin{tabular}{lll}
\toprule
& Experts ($n=36$) & Laymen ($n=37$)\\
\cmidrule[0.1pt](lr){2-3}
\textit{Intruder}  \\
\cmidrule[0.1pt](lr){1-3}
Acc. Concept     & 70.19\%  & 61.08\%     \\
Acc. Sub-Concept & 74.81\% ($p = 0.18$)  & \textbf{67.03}\% ($p = 0.043$)      \\
\midrule
\textit{Binary choice} \\
\cmidrule[0.1pt](lr){1-3}
Sub-Concept & \textbf{76.1}\% ($p < 0.001$) & \textbf{74.95}\% ($p < 0.001$)\\
Odds Ratios & $3.53$ & $2.99$\\
\bottomrule
\end{tabular}
\caption{\textbf{Results from the psychophysics experiments to validate the recursivity ingredient. }}\label{tab:results}
\end{table}


To evaluate the meaningfulness of the extracted high-level concepts, we performed psychophysics experiments with human subjects, whom we asked to answer a survey in two phases. Furthermore, we distinguished two different audiences: on the one hand, experts in machine learning, and on the other hand, people with no particular knowledge of computer vision. Both groups of participants were volunteers and did not receive any monetary compensation. Some examples of the developed interface are available the appendix~\ref{app:craft:human-exp}. It is important to note that this experiment was carried out independently from the utility evaluation and thus it was setup differently.
\newline\textbf{Intruder detection experiment} First, we ask users to identify the intruder out of a series of five image crops belonging to a certain class, with the odd one being taken from a different concept but still from the same class. Then, we compare the results of this intruder detection with another intruder detection, this time, using a concept (e.g., $\mathcal{C}_1$) coming from a layer $l$ and one of its sub-concepts (e.g., $\mathcal{C}_{12}$ in Fig.\ref{fig:craft:collapse}) extracted using our recursive method. If the concept (or sub-concept) is coherent, then it should be easy for the users to find the intruder.
Table~\ref{tab:results} summarizes our results, showing that indeed both concepts and sub-concepts are coherent, and that recursivity can lead to a slightly higher understanding of the generated concepts (significant for non-experts, but not for experts) and might suggest a way to make concepts more interpretable.
\newline\textbf{Binary choice experiment} In order to test the improvement of coherence of the sub-concept generated by recursivity with respect to the larger parent concept, we showed participants an image crop belonging to both a subcluster and a parent cluster (e.g.,  $\bm{\pi}(\vx) \in \mathcal{C}_{11} \subset \mathcal{C}_1$) and asked them which of the two clusters (i.e., $\mathcal{C}_{11}$ or $\mathcal{C}_{1}$) seemed to accommodate the image the best. If our hypothesis is correct, then the concept refinement brought by recursivity should help form more coherent clusters.
The results in Table~\ref{tab:results} are satisfying since in both the expert and non-expert groups, the participants chose the sub-cluster more than 74\% of the time. We measure the significance of our results by fitting a binomial logistic regression to our data, and we find that both groups are more likely to choose the sub-concept cluster (at a $p < 0.001$).

\subsubsection{Fidelity analysis} \label{sec:craft:expSobol}

We propose to simultaneously verify that identified concepts are faithful to the model and that the concept importance estimator performs better than that used in TCAV~\cite{kim2018interpretability} by using the fidelity metrics introduced in \cite{ghorbani2019towards, zhang2021invertible}. These metrics are similar to the ones used for attribution methods, which consist of studying the change of the logit score when removing/adding pixels considered important. Here, we do not introduce these perturbations in the pixel space but in the concept space: once $\m{U}$ and $\m{W}$ are computed, we reconstruct the matrix $\m{A}\approx \m{U}\m{W}^\tr$ using only the most important concept (or removing the most important concept for deletion) and compute the resulting change in the output of the model.  As can be seen from Fig.~\ref{fig:craft:deletion}%
, ranking the extracted concepts using Sobol's importance score results in steeper curves than when they are sorted by their TCAV scores. %
We confirm that these results generalize with other matrix factorization techniques (PCA, ICA, RCA) in Section~\ref{app:craft:fidelity} of the Appendix.

\begin{figure}[ht]
\includegraphics[width=\linewidth]{assets/craft/nmf_fidelity.png}
\caption{
\textbf{(Left)} Deletion curves (lower is better). \textbf{(Right)} Insertion curves (higher is better). 
For both the deletion or insertion metrics, Sobol indices lead to better estimates (calculated on >100K images) of important concepts. %
}
\label{fig:craft:deletion}
\end{figure}




\subsection{Conclusion}

In this first section, we introduced \craft, a method for automatically extracting human-interpretable concepts from deep networks. Our method aims to explain a pre-trained model's decisions both on a per-class and per-image basis by highlighting both ``\what'' the model saw and ``\where'' it saw it  -- with complementary benefits. The approach relies on 3 novel ingredients: \tbi{i} a recursive formulation of concept discovery to identify the correct level of granularity for which individual concepts are understandable; \tbi{ii} a novel method for measuring concept importance through Sobol indices to more accurately identify which concepts influence a model's decision for a given class; and \tbi{iii} the use of implicit differentiation methods to backpropagate through non-negative matrix factorization (NMF) blocks to allow the generation of concept-wise local explanations or \textit{concept attribution maps} independently of the attribution method used. Using our previously introduced human-centered utility benchmark, we conducted psychophysics experiments to confirm the validity of the approach: and that the concepts identified by \craft~are useful and meaningful to human experimenters. 
