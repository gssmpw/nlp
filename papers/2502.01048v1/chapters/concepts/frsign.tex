In this comprehensive examination, we extend our investigation into the utility of concept-based methods applied to models trained on the FRSign dataset~\cite{2020frsign}, aiming to delve deeper than conventional attribution methods allow. This inquiry builds upon our previous work outlined in~\autoref{sec:attribution:frsign}, where we expressed reservations about the strategies the model employs, particularly concerning the interpretation of white signals.

In this section, our exploration is structured in three parts: initially, we conduct a review of classes for which we hypothesize the model's behavior aligns closely with expectations. Subsequently, we direct our focus towards the more enigmatic white signal. Finally, we venture further by examining secondary concepts, leading us to propose a hypothesis we term ``support concepts''.

\begin{figure}[ht]
\centering
\includegraphics[width=0.9\textwidth]{assets/frsign/frsign_concepts_red.jpg}
\includegraphics[width=0.9\textwidth]{assets/frsign/frsign_concepts_orange.jpg}
\caption{\textbf{Most Important Concepts for Red and Orange Class.} Applying~\craft method, we extracted the most significant concepts for the ResNet50 model trained on the FRSign dataset. Consistent with findings from~\autoref{sec:attribution:frsign}, the key concepts appear reasonable and are focused on the traffic light itself, reaffirming the model's attention to relevant features.}
\label{fig:frsign:good_concepts}
\end{figure}


\subsection{Visual inspection using concepts}

We commence with a visual inspection of the model's behavior across various classes using our concept-based method, \craft. We visualize the most important concepts that the ResNet50 model -- as detailed in~\autoref{sec:intro:frsign} --  leverages. 

The~\autoref{fig:frsign:good_concepts} illustrates these concepts which, as hypothesized in~\autoref{sec:attribution:frsign}, appear aligned and plausible. In this instance, the concepts do not seem to offer substantial new insights at first glance. We will now proceed to address the challenging case highlighted in the previous section: the interpretation of white signals.

\begin{figure}[ht]
\centering
\includegraphics[width=0.48\textwidth]{assets/frsign/concept_white_signals.jpg}
\includegraphics[width=0.48\textwidth]{assets/frsign/second_concept_white.jpg}
\caption{\textbf{Most and second most important concepts for White class.} The predominant concept for the white signal class seems to be the shear effect on image borders. This finding not only confirms our earlier hypothesis but also sheds additional light on the significance of the frame's edge in the model's decision-making process.}
\label{fig:frsign:white_concepts}
\end{figure}

\subsection{Understanding the White Signal Case}

Our analysis takes a deeper dive into the peculiar case of the white signal, where the primary concept identified appears to be the shear effect on images around the edges of the frame (\autoref{fig:frsign:white_concepts}). This observation supports and further illuminates our previous hypothesis from~\autoref{sec:attribution:frsign}, suggesting that the frame's edge plays a crucial role in the model's interpretation.


\begin{figure}[ht]
\centering
\includegraphics[width=0.45\textwidth]{assets/frsign/concept_orange_curve.jpg}
\includegraphics[width=0.45\textwidth]{assets/frsign/concept_red_curve.jpg}
\includegraphics[width=0.45\textwidth]{assets/frsign/blur_concept.jpg}
\includegraphics[width=0.45\textwidth]{assets/frsign/catenair_concept.jpg}
\caption{\textbf{Examples of Four Secondary, Yet Significant, Concepts.} These concepts are used for the orange, red, yellow, and violet logits, respectively. Although they are not the top-1 concepts, this does not mean they do not contribute to the logits. A portion of the logits is influenced by these concepts, underscoring their importance in the model's decision-making process.}
\label{fig:frsign:support}
\end{figure}


\subsection{Hypothesis: Support Concepts}

In an effort to further our understanding, we investigate secondary, yet influential concepts, which we refer to as "support concepts." These concepts, while not being the top-1 most important for a given class, still significantly contribute to the model's logits for various signals, as illustrated in \autoref{fig:frsign:support}.

We can conjecture that, more problematically, attribution methods that highlight the most important pixels or areas may overlook several features that also drive decision-making and could represent shortcuts. Thus, these concepts could be "hidden" by the attribution maps but still present internally. We introduce the idea of "support concepts" as an avenue for future work, suggesting that a deeper exploration into these underlying influences could unveil additional layers of model reasoning not immediately apparent through conventional attribution techniques.

\subsection{Conclusion}

The application of concept-based explanations has provided us with a more granular understanding of the model's behavior, particularly elucidating the case of the white signal. It appears that biases, possibly inherent in the dataset, necessitate a broader collection of images to mitigate such issues. Alarmingly, our analysis confirms that "support concepts," while not paramount for a class, play a critical role in achieving high performance levels. This revelation affirms the pervasive nature of biases and shortcuts in model training. Consequently, concept-based explainability holds promising potential for unveiling these complexities in model interpretations, offering a path towards more transparent and interpretable machine learning models.
