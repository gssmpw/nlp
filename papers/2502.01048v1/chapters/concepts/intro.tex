This chapter is a direct response to the critical question raised at the conclusion of \autoref{chap:attributions}: why are existing attribution methods not enough to properly understand deep learning models? Our ambition is to investigate Hypothesis~\ref{hyp:what} by extending the methods of explainability beyond  attribution, aiming to probe into the deeper, more intricate aspects of internal features of deep neural network.

This investigation is not a novel expedition. Prior research has ventured beyond simple attribution methods, among which we can identify two strong candidates: Feature visualization (see~\autoref{def:intro:feature_viz}) and concept-based analysis (also briefly presented in~\autoref{def:intro:cav}). Yet, these approaches, while rich with potential, are fraught with challenges due to their early stages of development. In this chapter, we will develop new tools and methodologies within this emerging field.

To frame our investigation, we introduce several critical challenges that must be addressed to advance the state of those new approaches. Concerning the Concept-based approach, we identify 2 main problems in the literature:

\begin{customchallenge}{Challenge 1: Automatically discover concepts used by the model.}
\end{customchallenge}

One of the most pressing challenges is developing methodologies capable of automatically and efficiently uncovering the concepts that models inherently use in decision-making processes. Current methodologies are  focused on testing pre-defined concepts~\cite{kim2018interpretability}. This approach is limited, as models may develop and rely on unexpected features or "shortcuts"~\cite{geirhos2020shortcut} for decision-making, which can surprise researchers. Therefore, there's a significant need for methods that not only test for anticipated concepts but also uncover and interpret the full range of strategies a model might employ, including those unanticipated by developers. This capability would mark a substantial advancement in our understanding of how models process information, offering insights into their internal logic and potentially highlighting biases or unintended behaviors. 

\begin{customchallenge}{Challenge 2: Theoretical framework deficiency.}
\end{customchallenge}

The field's reliance on empirical research has led to a significant gap in theoretical foundations, leaving concept-based methods without solid ground for evaluating the relevance and significance of identified concepts.
Concerning Feature visualization, they offer profound insights yet struggles with scalability and adaptability to the increasing complexity of contemporary models.

\begin{customchallenge}{Challenge 3: Scalability of Feature Visualization.}
\end{customchallenge}

The scalability of feature visualization techniques is limited, often resulting in noisy and less interpretable images on advanced models, highlighting the need for methodological renewal and innovation.
Finally, a more global issue is the lack of clear link between Attributions methods, Concepts and Feature Visuzaliation. 

\begin{customchallenge}{Challenge 4: Lack of synergy across methods}
\end{customchallenge}

Despite underlying conceptual connections, attribution, feature visualization, and concept-based methods have evolved in isolation, lacking integration and synergy.

Addressing these challenges, this chapter proposes a unified theoretical framework aimed at not just incrementally improving model explainability but trying to lay a ground for a more robust and deeper understanding of deep neural network. 
