\chapter{From Pixels to Features: Towards Deeper Explainability with Concepts}
\chaptermark{\protect\parbox{.5\textwidth}{From Pixels to Features}}
\label{chap:concepts}

\begin{chapterabstract}
\textit{
In this chapter, we address a challenge identified in \autoref{chap:attributions}: Is it possible to transcend attributions methods to forge methods that do more than just spotlight where a model directs its attention -- \where~the model is looking -- but also clarify \what~exactly it perceives? Essentially, existing methods primarily disclose the ``\where'' in terms of the model's focus, rather than elucidating the "\what" it discerns, in terms of feature. The question then becomes, how can we define and characterize this ``\what''? This is the subject of this chapter that aims to extend beyond attribution methods to lay a more robust foundation for a deeper and more precise Explainability. \\
Our exploration begins in \autoref{sec:concepts:craft}, which propose a significant advancement in concept-based explainability by introducing an automated method, \craft, for extracting a model's learned concepts. We demonstrate that it is feasible to easily assess the significance of these derived concepts using Sobol indices presented in \autoref{sec:attributions:sobol}. The findings from this work substantially improve upon the benchmarks established in \autoref{sec:attributions:metapred}, and offer new avenues for addressing complex scenarios requiring in-depth explainability.
Progressing to \autoref{sec:concepts:holistic}, the cornerstone of this chapter, we show \tbi{i} how \craft~and related research fit within a broader framework of dictionary learning. We propose a unified framework for concept extraction, paving the way for new methodologies. Further, \tbi{ii} we establish a link between concept importance estimation and traditional attribution methods, demonstrating that concept importance estimation methods can be viewed as attribution methods recontextualized within the concept space for evaluative purposes.
With this framework in place, we find it possible to derive insightful answers to literature questions such as ``where should concept decomposition be performed?'' or ``which importance method to choose''. Furthermore, we delve into the importance measure of concepts, revealing that this information can be utilized to address a significant open problem in Explainability: ``how to identify points classified for similar reasons'', by proposing the strategic clustering plot.
The final section of this chapter, \autoref{sec:concepts:maco}, is dedicated to scaling feature visualization through a reformulation of the optimization problem within the Fourier space, by constraining magnitude. This new module allows for the use of feature visualization to create prototypes of the concepts extracted with \craft.
In conclusion, we will showcase the powerful synergies this new framework offers with \Lens, a demo that enables the visualization of the concepts used by a ResNet50 model for the 1000 ImageNet classes.
In sum, this chapter not only tackles foundational questions within the domain of machine learning explainability, but also sets forth a comprehensive framework that integrates advanced methodologies for concept extraction and importance estimation.
}
\end{chapterabstract}

The work in this chapter has led to the publication of the following conference papers:
{\small{
\begin{itemize}

    \item \textbf{Thomas Fel}\equal, Agustin Picard\equal, Louis Bethune\equal, Thibaut Boissin\equal, David Vigouroux, Julien Colin, Rémi Cadène, Thomas Serre, (2023). \textit{``CRAFT: Concept Recursive Activation FacTorization for Explainability''.} In: \textit{IEEE/CVF Conference on Computer Vision and Pattern Recognition} (\textcolor{confcolor}{CVPR})
    
    \item \textbf{Thomas Fel}\equal, Victor Boutin\equal, Mazda Moayeri, Rémi Cadène, Louis Bethune, Mathieu Chalvidal, Thomas Serre (2023). \textit{``A Holistic Approach to Unifying Automatic Concept Extraction and Concept Importance Estimation''.} In: \textit{Advances in Neural Information Processing Systems}  (\textcolor{confcolor}{NeurIPS})
    
    \item \textbf{Thomas Fel}\equal, Thibaut Boissin\equal, Victor Boutin\equal, Agustin Picard\equal, Paul Novello\equal, Julien Colin, Drew Linsley, Tom Rousseau, Rémi Cadène, Lore Goetschalckx, Thomas Serre (2024). \textit{``Unlocking feature visualization for deep network with MAgnitude constrained optimization''.} In: \textit{Advances in Neural Information Processing Systems}  (\textcolor{confcolor}{NeurIPS})

\end{itemize}
}}

\minitoc
\clearpage

\section{Introduction}
\input{chapters/concepts/intro}
\clearpage

\section{CRAFT : Concept Recursive Activation FacTorization}
\label{sec:concepts:craft}
\input{chapters/concepts/craft}
\clearpage

\section{Application: FRSign}
\label{sec:concepts:frsign}
\input{chapters/concepts/frsign}
\clearpage

\section{Unifying Automatic Concept Extraction and Concept Importance Estimation}
\label{sec:concepts:holistic}
\input{chapters/concepts/holistic}
\clearpage
  
\section{Modern Feature Visualization with MACO}
\label{sec:concepts:maco}
\input{chapters/concepts/maco}
\clearpage

\section{Conclusion}

The conclusion of this chapter serves as an opportune moment for reflection and synthesis. Our research has led us through an in-depth examination of Hypothesis~\ref{hyp:what}, which posited that existing attribution methods fall short, as they primarily reveal ~\where but overlook the crucial aspect of the \what.

This chapter was dedicated to developing appropriate tools to address this issue. We began by constructing \craft, a method for decomposing the activations of a model into a set of concepts, demonstrating indeed its enhanced utility for human understanding compared to traditional attribution methods.
We decided to go one step further, in \autoref{sec:concepts:holistic}, where we established a theoretical framework that make: \tbi{i} show that concept extraction is \textbf{Dictionary learning}, and \tbi{ii} make a link between attribution methods and concept importance. The formulas used to determine the importance of a pixel, as seen in \autoref{chap:attributions}, are identical to those applied in evaluating the significance of concepts after decomposition. In the final section, we explored concept visualization as a way to visualize concept by introducing \maco. 

To summarize our novel framework, it consists in reinterpreting the intricate latent space of neural networks through a collection of atomic units termed concepts. While these concepts are mathematically abstract, we employed two methods to imbue them with meaning: maximally activating crops and feature visualization techniques. Additionally, it became evident that among these concepts, some offer greater utility than others, with attribution methods precisely identifying the most relevant ones.

\paragraph{A New Synergetic Approach to Explainability.} This new framework is distinct in its ability to \textit{synthesize all existing tools for explainability into a cohesive and synergetic system}. Our goal was to demonstrate the potential of this approach -- and the powerful synergy it creates -- through the visual demonstration offered by \Lens~(illustrated in \autoref{fig:concepts:lens}).

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.49\textwidth]{assets/lens_website.jpg}
    \includegraphics[width=0.49\textwidth]{assets/lens_click.jpg}
    \caption{\textbf{LENS Project.} Example of results from the LENS demo for the espresso class. \textbf{(Left)} The page displays the top 10 concepts, ranked from most to least important for the class. These concepts are extracted using \craft~and visualized with \maco; their importance is calculated using the optimal formula found in \autoref{sec:concepts:holistic}. \textbf{(Right)} Clicking on a feature visualization that illustrates a concept reveals the image crops that most strongly activate the concept.}
    \label{fig:concepts:lens}
\end{figure}

This platform organizes, for each of the 1000 ImageNet classes, the ten most significant concepts, along with their feature visualizations and respective importance. 

\paragraph{Perspective.} While the potential of concept-based methods is clear, it is now critical to establish distinct research directions to fully unlock their potential in the wake of preliminary studies. Four key areas emerge, meriting further exploration:

\begin{itemize}

    \item \textbf{Revisiting Dictionary Learning:} The evident parallels between concept extraction and dictionary learning highlight a pressing need for the XAI community to reassess and tailor dictionary learning methodologies for application in explainability. This adaptation could bridge gaps in our understanding and application of these techniques within XAI.

    \item \textbf{Beyond Classification:} The necessity of extending our investigative scope beyond mere classification tasks is crucial. Diverse models, including bounding box detection, segmentation, generative and Vision-Language models present intricate challenges and vast opportunities for enhancing explainability. Diversifying our focus will enable a general comprehension of AI systems, integrating a broader spectrum of tasks and functionalities, thus deepening the XAI field with richer insights and more adaptable explainability tools. An illustrative example is given in \autoref{fig:holistc:conceptbbox}.
    
    \item \textbf{Exploring Hierarchical Concepts and Compositionality:} Investigating hierarchical concepts and their compositionality also offers a very promising path to deepen our understanding of how neural networks operate. Recent research has highlighted that models can exhibit compositional behaviors~\cite{lepori2024break}. Understanding the ways in which concepts are combined and interact at various abstraction levels could offer a nuanced perspective on decision-making processes within models, paving the way for more refined interpretability strategies.
    
    \item \textbf{Expanding on Synergies:} The demonstrated synergy among different explainability methods within our framework suggests a fertile area for research. A comprehensive examination of how these methods can be cohesively integrated, and the resultant synergistic effects could lead to groundbreaking insights and the development of potent tools for explainability.
    
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=1.05\textwidth]{assets/holistic/concept_bbox.jpg}
    \caption{\textbf{Concepts of \craft~on ResNet50 Trained on Different Tasks.} The concepts extracted for classification tasks, specifically for the St. Bernard class, seem to focus on the head and spots of the St. Bernard. In the case of the bounding box (bbox) model, the legs are also deemed important, possibly because they help delineate the edges of the bounding boxes? Interestingly, for the CLIP model, the dog head concept is also activated by human heads, suggesting that despite visual differences (in the pixel space), the concept of 'head' seems present for models trained with language components like CLIP.}
    \label{fig:holistc:conceptbbox}
\end{figure}

While this framework does not solve all the challenges presented in the \autoref{chap:intro}, and there remains a significant journey toward fully understanding models such as ResNet50 or ViT, it opens a novel avenue. We encourage the academic community to explore the synergies between attribution methods, concepts, and feature visualization for deeper explainability.


