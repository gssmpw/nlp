\section{A Self-play Solver-verifier Framework}
\subsection{Problem Formulation}
\label{sec:problem-formulation}
\paragraph{Setup} We consider that an LLM can play two roles:
\vspace{-2mm}
\begin{itemize}[leftmargin=*]
    \item \textbf{Solver ($S$):} Given a coding problem description $P$, it produces a candidate solution $C$ (e.g., a piece of code).
    \item \textbf{Verifier ($V$):} Given a proposed solution $C$ and the original problem $P$, the verifier tries to produce test cases $\mathbf{T}$\footnote{We use bold Italic to represent a set.} (e.g., a set of inputs and expected outputs) and can catch errors in $C$ if it is incorrect. Essentially, it produces and selects challenging unit tests to determine if the code is correct or not.%\footnote{In practice, solver and verifier can generate multiple candidates.} 
    %through multi-time inference.}
\end{itemize}

The objective of the solver is to produce a correct solution $C$ that will pass any tests the verifier can come up with. The objective of the verifier is to produce a set of tests $\mathbf{T}$ that will fail any incorrect solutions and thus distinguish correct solutions from incorrect ones.

Let $p(\mathbf{P})$ be the distribution over problem statements. We can think of having a training set of problems or a domain from which we can sample problems. The sampling strategies we consider are detailed  in Section~\ref{sec:practical-settings}.

The solver $S_{\theta}$ is a model parameterized by $\theta$ that, given a problem $P$, generates a candidate solution $C$: $C \sim S_{\theta}(\cdot | P)$. The verifier $V_{\phi}$ is a model parameterized by $\phi$, given the problem $P$ and a candidate solution $C$, generates a test suite $\mathbf{T}$: $\mathbf{T} \sim V_{\phi}(\cdot | P, C)$.

In practice, the solver and verifier can be the same LLM.

\textbf{Scoring Function}\ \ \ We define a function that executes $C$ on the tests $\mathbf{T}$ as $\text{Score}(C, \mathbf{T}) \in [0, 1]$, which is the fraction of tests passed by solution $C$. A score of 1 means $C$ passes every test $T$; a score of 0 means it failed all tests. Formally,
\begin{align}
    \text{Score}(C, \mathbf{T}) = \mathbb{E}_{T\sim \mathbf{T}}[\mathbb{I}(C(T)=\text{expected\_output}(T))]
\end{align}
where $\mathbb{I}$ is the indicator function, and $C(T)$ means running one single test on code solution $C$.

We sample a set of problems $P \sim p(\mathbf{P})$, generate some candidate solutions $C \sim S_{\theta}(\cdot | P)$, and generate candidate tests $\mathbf{T} \sim V_{\phi}(\cdot | P, C)$. We now have tuple $(P, C, \mathbf{T})$. We consider:

\begin{align}
y = \begin{cases}
1 & \text{if } \text{Score}(C,\mathbf{T}) = 1 \text{ (i.e., passes all tests)} \\
0 & \text{otherwise}.
\end{cases}
\end{align}

We employ two stages of  training to make use of both chosen ($y=1$) and rejected ($y=0$) examples for training the solver and verifier, described as follows:

\textbf{Stage 1: SFT Training}\ \ \ For pairs where $y = 1$, we have a correct solution-test pair. These are high-quality examples that reflect desired behavior, i.e., the solution $C$ solves the problem $P$, and the test suite $\mathbf{T}$ properly validates that the solution is correct. We use $(P, C, \mathbf{T}, y = 1)$ tuples to fine-tune the model directly. The training signal here encourages the model (1) as a solver, to generate similar correct solutions for similar problems, and (2) as a verifier, to produce meaningful tests that confirm correctness. We call this the supervised fine-tuning (SFT) stage, where we optimize for both solver and verifier:

\begin{align}
    \mathcal{L}_{\text{SFT}_\text{solver}}(\theta) = -\mathbb{E}_{(P, C, \mathbf{T}):y=1}[\log S_{\theta}(C|P)] \\
    \mathcal{L}_{\text{SFT}_\text{verifier}}(\phi) = -\mathbb{E}_{(P, C, \mathbf{T}):y=1}[\log V_{\phi}(\mathbf{T}|P, C)]
\end{align}

In practice, both objectives can be trained using a mixture of data consisting of chosen examples for solver and verifier.

\textbf{Stage 2: DPO Training}\ \ 
\label{sec:dpo-training}
We now aim to form pairwise comparisons (preferences) to train both solver and verifier roles more effectively. In practice, we adopted the Direct Preference Optimization (DPO) method, but any preference tuning methods can be used at this stage.

For the solver perspective, for each problem $P$, and each chosen tuple $(P, C^+, \mathbf{T}, y=1)$, we find a rejected tuple $(P, C^-, \mathbf{T}, y=0)$. Following standard DPO training~\cite{rafailov2024direct}, we can formulate our policy objective as:
\begin{align}
    &\mathcal{L}_{\text{DPO}_\text{solver}}(S^*_{\theta}; S_{\theta}) = \nonumber \\ &-\mathbb{E}[\log\sigma(\beta\log \frac{S^*_{\theta}(C^+|P)}{S_{\theta}(C^+|P)} - \beta\log \frac{S^*_{\theta}(C^-|P)}{S_{\theta}(C^-|P)}]
\end{align}

where $\beta$ is the hyperparameter to regulate the strength of weight updates; $S_{\theta}(C|P)$ is the probability that our model (with parameter $\theta$) assigns to generating code solution $C$ given problem $P$.

For the verifier perspective, similarly, for each problem $P$, and each chosen tuple $(P, C, \mathbf{T}^+): y=1$, find a rejected tuple $(P, C, \mathbf{T}^-):y=0$\footnote{This can be achieved by selecting any expected output that is not equal to the chosen one in the sampling space for $T$.}. The verifier-related DPO loss is then:
\begin{align}
    &\mathcal{L}_{\text{DPO}_\text{verifier}}(V^*_{\phi}; V_{\phi}) = \nonumber \\ &-\mathbb{E}[\log\sigma(\beta\log \frac{V^*_{\phi}(\mathbf{T}^+|P, C)}{V_{\phi}(\mathbf{T}^+|P, C)} - \beta\log \frac{V^*_{\phi}(\mathbf{T}^-|P, C)}{V_{\phi}(\mathbf{T}^-|P, C)}]
\end{align}

\subsection{Synthetic Data Generation}
\label{sec:practical-settings}
In this section, we describe our approach to generating the synthetic data, including problem description generation, code generation, test generation and preference data generation. All related prompts can be found in Appendix~\ref{app:prompt-template}.

\textbf{Problem Description Generation}\ \ \ Following Magicoder~\cite{wei2024magicoder}, we generate a large collection of programming problem descriptions that span a diverse range of topics, including those in the long tail distribution. To achieve this diversity, we sample random code snippets from various sources and prompt the model to generate programming problems inspired by these examples. This allows us to tap into a wide range of topics and create a comprehensive set of problem descriptions (as demonstrated in Figure~\ref{fig:embedding-plot}).

% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=\textwidth]{figs/test-gen-overview.pdf}
%     \caption{An overview of the test generation pipeline.}
%     \label{fig:test-generation-overview}
% \end{figure*}

It is noted that code generation tasks can follow different problem description formats despite having the same content. For example, here is the same problem but stated in different ways:
\begin{itemize}[nosep,leftmargin=*]
    \item Write a python function to remove the kth element from a given list.
    \item In the ancient Library of Alexandria, scrolls are stored in a mystical list. The librarian needs to remove a specific scroll whenever a visitor requests it.
\end{itemize}

To make our prompt sets accommodate these diverse problem description formats, we adopt some templates from the training set of different coding benchmarks (e.g., MBPP, APPS) into the original prompt to generate the problem description.

To create a self-contained coding problem description, we need to ensure it not only includes a clear problem statement but also a well-defined starter code. The starter code should contain all necessary built-in libraries and a detailed function signature description to show what should be the input and output. Therefore, after obtaining the original problem description generated by the model, we then ask the model to generate function signatures. After deduplication, we get 103,280 problem descriptions in total.

\textbf{Code Generation}\ \ \ We prompt the LLM to solve each problem given the generated function signature. Following the Llama 3.1~\cite{dubey2024llama}, we also require the model to explain its thought process in comments, which improves code generation in both accuracy and readability.

\textbf{Test Generation}\ \ \ 
\label{sec:test-generation}
We use a pipelined approach to generate unit test sets. Specifically, we first ask the model to generate a set of valid inputs and then ask it to generate expected outputs based on the inputs.

For input generation, we ask the model to generate different types of function inputs to cover different cases including general, corner or difficult cases. For example, the following problem description should contain two different cases:

\fbox{%
  \parbox{\columnwidth}{%
\textbf{Problem Description:}

Write a function to find the longest string in a list of strings. If the strings are not comparable due to being of different lengths, the function should return None.

{\tt longest\_string(strings: list[str])}
% \newline

\textbf{Case 1:} {\tt strings} is a list of strings.

\textbf{Case 2:} {\tt strings} is empty.

  }%
}

For output generation, we ask the model to generate the expected output based on the problem description and input. We also find that two strategies can boost the performance of unit test generation (as demonstrated in Section~\ref{sec:base-performance}):
\begin{itemize}[nosep, leftmargin=*]
    \item \textbf{Majority Voting}: The majority voting mechanism in the self-consistency approach~\cite{wang2022self} asks the model to generate multiple candidate outputs for a query, and aggregates them using a majority voting procedure to select the most commonly occurring answer.
    \item \textbf{Chain-of-Thought reasoning:} Following \citet{wei2022chain}, before outputting the expected values, we asked the model to first generate its reasoning steps.
\end{itemize}

Additionally, we employ the following strategies to ensure the quality and robustness of the generated unit tests:

\begin{itemize}[nosep, leftmargin=*]
    \item \textbf{Test Coverage Optimization:} We sample multiple candidate test cases and strategically select a subset that maximizes branch coverage of the solution code (a maximum coverage problem), ensuring comprehensive testing of different execution paths.
    \item \textbf{Output Diversification:} We notice that if the outputs of the test cases are not diverse enough, the solution code can cheat by exploiting patterns in test cases. For example, if all test cases return the same value (e.g., {\tt True}), the model could trivially pass by implementing a function that always returns that value. To address this issue, we explicitly select test samples with diverse output values.
\end{itemize}

\textbf{Synthetic Preference Data Generation}\ \ \ 
The key goal of constructing synthetic data for preference tuning, which is DPO training in our case, is to construct pairs of ``chosen'' and ``rejected'' responses to the given prompts. As illustrated in Section~\ref{sec:dpo-training}, the ``chosen'' examples are those examples where the solver  agrees with the verifier, i.e., the generated codes can pass the generated tests. However, identifying ``rejected'' examples is considerably more complex, as it is not always clear which side is at fault when they disagree.

In previous work, \citet{dong2024self} adopts an automated quality cross verification process by selecting both test case and functions with an accuracy rate grater than 0.5.
We utilize a similar cross validation strategy, but with a focus on both code and test generation. Specifically, if at least one generated solution passes all the generated tests, we will assume the solution and the test are ``correct''.
For training the solver, any other sampled code solutions that fail to pass these ``correct'' tests are treated as ``rejected'' examples.
Similarly, for training the verifier, when we find a ``correct'' test $f(x) == y$, we revisit the original sampling space for generating expected outputs, and treat any expected outputs $y' \neq y$ as ``rejected'' tests $f(x) == y'$. In this way, we can reuse all the chain-of-thought explanations generated by the model  during the majority voting process.
