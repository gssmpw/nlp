\section{Introduction}
% \textsc{Sol-Ver}


% This seems to be a better flow: synthetic data by the same size model is not good -> data has a lot of fales positive (fales positive is high cite paper: alphaCode) ->  data need to be filtered/checked -> we build unit test verifier for filtering. -> iteratively improve both

Large language models (LLMs) have demonstrated impressive ability in code generation, significantly enhancing the programming efficiency and productivity of human developers~\cite{li2022competition,roziere2023code,codealpaca}.
The ability to code is largely driven by high-quality online resources where coding problems, human-written solutions, and corresponding unit tests are freely available. However, as these free resources being depleted, the momentum of LLMs' improvement diminishes.
% Building on these successes, researchers are exploring additional methods to optimize LLM performance across more diverse coding scenarios.

To address the scarcity of supervised data for code generation, recent studies have adopted synthetic data generation techniques such as {\sc Self-Instruct}~\cite{wang2023self} to augmenting LLM training sets. 
% leveraging the robust code generation abilities of teacher models, enabling the distillation of synthetic data that further enhances LLMs.
% the capabilities of LLMs in diverse programming contexts.
Specifically, previous work collects and designs code instructions and generates corresponding responses using a high-capacity teacher LLM. This generated data is then employed to fine-tune a student LLM, thereby enhancing its code generation abilities. 
Although synthetic code data produced in this manner has demonstrated success, it relies on the availability of a strong teacher model, presumably with a larger parameter size and higher computation costs.
Additionally, existing work has shown that training a model on data generated by itself is ineffective because errors introduced during generation tend to accumulate over iterations~\cite{dubey2024llama}. As a result, there is a critical need for effective methods to verify the generated data.

Despite this pressing need, evaluating the correctness of the generated code is not trivial and often demands substantial programming expertise, even for human annotators.
Recently, some research has explored the use of LLM-as-a-judge~\cite{mcaleese2024llm,alshahwan2024automated,dong2024self}, which can automatically provide feedback on the code it has generated, for example by running the code against unit tests it produces. However, as we will show in this work, the current capability of LLMs to generate unit tests, herein referred to as LLM-as-a-verifier, is substantially worse than their ability to produce code solutions, herein referred to as LLM-as-a-solver (Section~\ref{sec:base-performance}). This is because of the lack of high-quality data specifically for unit test generation during the post-training phase, i.e., most fine-tuning data focuses on code generation, while only a small portion targets unit test generation.
%(both data quantity and quality)

% To address this limitation, one effective approach is to utilize synthetic data generated by LLMs, known as \textit{self-instruct} data~\cite{wang2023self}.
% This method has shown promising results when the model is trained on data generated by larger, more competent models. However, previous research has revealed that training models on their own generated data may not be beneficial (Llama 3.1 report). This may be due to the model accumulating its own mistakes during fine-tuning. Therefore, it is crucial to check the correctness of the generated solutions. 

In this work, we propose \textsc{Sol-Ver}, a self-play solver-verifier framework to iteratively train a model for both code and test generation. The main idea is to let the LLM-as-a-solver and LLM-as-a-verifier help each other. Specifically, we ask the model to generate code solutions and unit tests for the same set of coding problems. By  executing the generated test against the generated code, we obtain feedback for training, involving two steps: (1) SFT training: we take the passed examples for fine-tuning the model, and (2) DPO training: we take both passed and failed examples as preference pairs to further train the model aligning with the preference. These training steps are for both code generation and unit test generation, and they can be repeated in an iterative manner. 

The experimental results on Llama 3.1 8B model show that we can successfully improve the model’s performance on both code and test generation without relying on human-annotated data or larger models. Specifically, on MBPP and LiveCodeBench, we achieve an average of 19.63\% and 17.49\% relative improvement for code and test generation respectively.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/icml-overview.pdf}
    \caption{An overview of the {\sc Sol-Ver} framework.
We train an LLM to both generate coding solutions (solver) and unit tests (verifier) in an iterative self-play framework, whereby synthetic preference pairs are constructed at each iteration depending on whether the code passes the generated tests or not.
We show that this approach enables the model to self-improve in both capabilities (see \autoref{tab:full-performance}).
    }
\if 0
insimultan
    We thus propose SOL-VER, a
self-play solver-verifier framework that jointly im-
proves a single model’s code and test generation
capacity. By iteratively refining code (LLM-as-
a-solver) and tests (LLM-as-a-verifier) together,
we boost both capabilities without relying on hu-
man annotations or larger teacher models
\fi 
    \label{fig:enter-label}
\end{figure*}

In summary, our work makes the following contributions:
\begin{itemize}[leftmargin=*]
    \item \textbf{Identification of a {\em critical gap}:} We analyze and highlight the significant gap in LLMs' abilities between code generation and unit test generation.
    \item \textbf{Novel \textit{Self-Play} Framework:} We propose a novel iterative framework where the model simultaneously functions as a code solver and a verifier. This methodology effectively self-aligns the model's outputs with desired performance criteria without relying on external annotations or teacher models.
    \item \textbf{High-Quality Synthetic Data Generation:} We contribute a generalizable method for creating high-quality synthetic data for both code and unit test generation. This data augmentation approach can be extended to various model training scenarios in the coding domain.
\end{itemize}


