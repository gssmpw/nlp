\section{Experiments}
\begin{table*}[ht]
\caption{Evaluation results over training iterations for our Solver-Verifier ( {\sc Sol-Ver}) method. $\Delta$(\%) means the relative percentage change from Baseline to Iter3$_\text{+DPO}$. Pass\% means average pass rate, Acc\% is accuracy, and FP\% is false positive rate.  {\sc Sol-Ver} provides large gains over the baseline on both code generation (solver) and unit test generation (verifier) tasks, which increase across iterations.}
\begin{center}
\resizebox{\textwidth}{!}{
\begin{tabular}{ll|lllllll|r}
\toprule
 &
   &
  \textbf{Baseline} &
  \textbf{Iter1$_\text{+SFT}$} &
  \textbf{Iter1$_\text{+DPO}$} &
  \textbf{Iter2$_\text{+SFT}$} &
  \textbf{Iter2$_\text{+DPO}$} &
  \multicolumn{1}{c}{\textbf{Iter3$_\text{+SFT}$}} &
  \multicolumn{1}{c|}{\textbf{Iter3$_\text{+DPO}$}} &
  \textbf{$\Delta$(\%)} \\
\midrule
\multirow{2}{*}{\textbf{MBPP}}          & Code$_\text{Pass\%}$ & 38.60 & 38.60 & 40.80 & 38.60 & 40.80 & 40.80 & 40.98 & 6.17  \\
                                        & Test$_\text{Acc\%}$ & 42.68 & 49.01 & 49.50 & 50.69 & 51.01 & 51.54 & 51.76 & 17.54 \\
                                        & Test$_\text{FP\%}$ & 12.75 & 12.57 & 10.32 & 10.12 & 10.06 & \ \ 9.80 & \ \ 9.60 & 24.71 \\
\midrule
\multirow{2}{*}{\textbf{LiveCodeBench}} & Code$_\text{Pass\%}$ & 18.23 & 24.86 & 25.97 & 25.12 & 26.38 & 26.41 & 27.24 & 33.08 \\
                                        & Test$_\text{Acc\%}$ & 20.14 & 36.43 & 37.09 & 39.40 & 40.65 & 41.25 & 41.50 & 51.47 \\
                                        & Test$_\text{FP\%}$ & 20.76 & 20.65 & 19.34 & 19.28 & 19.05 & 18.94 & 18.63 & 10.26 \\
\bottomrule
\end{tabular}}
\end{center}
\label{tab:full-performance}
\end{table*}

\subsection{Experimental Setup}
% Below is our experiment setup, with more details in Appendix~\ref{app:prompt-template}.

\textbf{Models and Datasets}\ \ \ We conduct experiments using Llama 3.1 8B~\cite{dubey2024llama}.\footnote{\url{https://huggingface.co/meta-llama/Llama-3.1-8B}} For the SFT and DPO training, we use the fairseq2 infrastructure~\cite{balioglu2023fairseq2} and run inference using vLLM~\cite{kwon2023efficient}.

We sample the problem descriptions using Llama 3.1 based on open source snippets from the OSS-Instruct dataset~\cite{wei2024magicoder}\footnote{\url{https://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K}}. For the problem description templates, we use the training sets of some  standard coding benchmark: MBPP~\cite{austin2021program}, APPS~\cite{hendrycksapps2021} and CodeContest~\cite{li2022competition}. For our experoiments we only focus on Python-related coding questions.

We test both code generation and unit test generation on standard python coding benchmarks including:
\begin{itemize}[nosep,leftmargin=*]
    \item MBPP~\cite{austin2021program}: A popular benchmark for Python code generation which focuses on relatively simple, self-contained functions.
    \item LiveCodeBench~\cite{jain2024livecodebench}: A comprehensive and contamination-free evaluation of LLMs for code, which continuously collects new problems over time from contests across three competition platforms, namely LeetCode, AtCoder, and CodeForces.
\end{itemize}

LiveCodeBench contains both code generation and test output prediction tasks. Since MBPP does not originally include a unit test generation task, we utilize the existing gold unit tests to assess the model’s accuracy in generating expected outputs given the inputs in gold unit tests.

\textbf{Evaluation Metrics}\ \ \ For the code generation task, all results are obtained using greedy decoding with the pass@1 metric (Pass\%). For unit test generation, we consider the following metrics:
\begin{itemize}[nosep,leftmargin=*]
    \item Accuracy (Acc\%): The accuracy of test output prediction. A test output is correct only when its literal value is equal to the gold one.
    \item False Positive Rate (FP\%): A robust unit test set should effectively differentiate between correct and incorrect code solutions. To evaluate this capability, in addition to the pass rate on correct solutions, we also evaluate the pass rate for known flawed solutions (false positive rate). To obtain flawed solutions for testing, we generate 20 candidate code solutions for each coding problem and use the gold unit tests to identify those that fail, treating them as negative examples.\footnote{We use the same Llama 3.1 8B base model to generate negative examples (temperature is set to 0.6 and top p is set to 0.9). As a result, we get 400 examples for both MBPP and LiveCodeBench separately.} 
\end{itemize}

\subsection{Evaluating {\sc Sol-Ver}}
Table~\ref{tab:full-performance} reports  {\sc Sol-Ver}'s iterative training performance for both code and test generation tasks on the MBPP and LiveCodeBench datasets.\footnote{Our baseline performance for code generation differs from that reported in the Llama 3.1 technical report because we employ a unified three-shot prompt template for both MBPP and LiveCodeBench, rather than using the prompts specifically provided for MBPP. We do not include the gold test in the prompt.}
To further show the improvement trend for {\sc Sol-Ver}, we plot the performance change across iterations for test generation in Figure~\ref{fig:test-gen-perf} (other figures for iterative change can be found in Appendix~\ref{app:performance-change}).
The results demonstrate that {\sc Sol-Ver} can consistently improve the performance of the base Llama 3.1 model in both code generation and unit test generation, as illustrated by the increased pass rate for code generation, increased accuracy and decreased false positive rate for test generation at each iteration. Specifically, we have achieved an average of 19.63\% and 17.49\% relative improvement for code and test generation respectively. In particular, two interesting conclusions can be made:
\begin{itemize}[nosep, leftmargin=*]
    \item The improvement in unit test generation performance is more significant than that observed in code generation. This greater enhancement may be due to the relatively limited availability of unit test–related code data compared to code generation–related data during the pre-training phase of Llama 3.1.
    \item The difference between SFT+DPO and SFT-only models suggests that incorporating negative examples during preference tuning helps the model learn from errors and refine its generation strategies.
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figs/test-gen-perf.pdf}
    \vspace{-0.5em}
    \caption{Iterative performance of our method, {\sc Sol-Ver}, for test generation. Our method outperforms the baseline and SFT training for both LiveCodeBench and MBPP benchmarks, and improve across training iterations.  }
    \label{fig:test-gen-perf}
\end{figure}

\subsection{Evaluating the Base Model}
\label{sec:base-performance}
In Section~\ref{sec:practical-settings}, we discuss several strategies for improving test generation. To evaluate their effectiveness and assess the initial performance of both code generation (LLM-as-a-solver) and test generation (LLM-as-a-verifier), we conduct base model evaluation using Llama 3.1 8B on the MBPP benchmark. Specifically, we measure: (1) the pass rate of generated code as evaluated by gold standard unit tests; and (2) the pass rate of generated unit tests when executed against the gold standard code solutions. The results are presented in the first two rows of Table~\ref{tab:base-model-eval}.

\begin{table}[t]
\caption{Code generation and test generation performance for Llama 3.1 8B on the MBPP benchmark. The case pass rate represents the average pass rate per test set. CoT means Chain-of-Thought reasoning, and MV means majority voting.}
\vspace{-0.5em}
\begin{center}
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|cc}
\toprule
\textbf{Llama 3.1 8B}               & \textbf{Pass Rate} & \textbf{Case Pass Rate} \\
\midrule
Code Generation        & 38.60\%            & 44.20\%                      \\
Test Generation & 18.60\%            & 37.60\%\\
\ \ + CoT & 19.60\% & 39.60\%\\
\ \ + MV & 24.80\%  & 42.40\% \\
\ \ + CoT + MV & 31.40\% & 47.60\%\\
\midrule
Code Reranked by Synthetic Test Rate & 35.00\% & 42.40\%\\
\bottomrule
\end{tabular}}
% \vspace{-1em}
\end{center}
\label{tab:base-model-eval}
\end{table}

% 3-shots should be the base lines, iter1, 2, 3(our methods)
% Full table + one highlight the figure and show the basic trending thing (it's still in the table, and dump the other two in the appendix)?
% Table 1 is all baselines, which is very confusing
% expand table 1

\begin{table}[t]
\caption{Agreement between Iter 1 and Iter 2, and the test accuracy for the model ensemble. The ensemble approach (Ens.) refers to selecting code solutions that successfully pass both the tests generated in the first iteration and those from the second iteration.}
\label{tab:agreement}
\begin{center}
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|lll|l}
\toprule
\textbf{Dataset}       & \textbf{Acc\%$_\text{Iter1}$}   & \textbf{Acc\%$_\text{Iter2}$}   & \textbf{Agreemnt} & \textbf{Acc\%$_\text{Ens.}$} \\
\midrule
MBPP          & 49.50\% & 51.01\% & 75.14\%  & 51.12\%   \\
LiveCodeBench & 37.09\% & 40.65\% & 72.38\%  & 40.68\%  \\
\bottomrule
\end{tabular}}
\end{center}
\end{table}

As shown in the table, unit test generation significantly underperforms compared to code generation on the same set of examples. This highlights a performance gap between employing LLMs as solvers and as verifiers. This discrepancy motivates our work, as we aim to enhance verification capabilities by leveraging the strong code generation abilities of LLMs. Additionally, improvements in unit test generation can, in turn, contribute to stronger code generation.

To further improve the baseline performance on unit test generation, we utilize two sampling strategies as discussed in Section~\ref{sec:test-generation}, i.e., Majority Voting (MV) and Chain-of-Thought (CoT) Prompting, that can improve the accuracy of generated unit tests. These strategies were originally proven effective for other tasks, such as mathematical reasoning. In Table~\ref{tab:base-model-eval}, we report the results after applying these strategies. As can be seen, both strategies can significantly improve the quality of unit test generation on the MBPP datasets, and combining them together yields the best result.

Nevertheless, despite these improvements, the combined strategies remain insufficient for generating an optimal test set for verification.
To substantiate this claim, we report the performance of code generation reranked by the generated tests (see the last row of Table~\ref{tab:base-model-eval}). Specifically, multiple code solution candidates are sampled and ranked using the generated tests as outlined in ``Test Generation + CoT + MV'' from Table~\ref{tab:base-model-eval}. The results indicate a decline in performance compared to the original code generation, thereby demonstrating that poor-quality test generation adversely affects the final outcomes. In contrast, our proposed method, {\sc Sol-Ver}, effectively addresses this limitation.

\subsection{Evaluating Agreement between Iterations}

Since models from different iterations are trained on different sets of synthetic data, which may introduce varying biases, we examine the agreement between the first and second iteration models to monitor progress across iterations. Specifically, both models are employed to generate unit tests for the MBPP benchmarks, and we measure the extent to which the unit tests produced by each model can agree on the gold code solutions. The results are given in Table~\ref{tab:agreement}.
Both datasets exhibit performance enhancements from the first to the second iteration and maintain a high level of agreement across iterations. This indicates that, despite being trained on distinct sets of synthetic data, both iterations produce largely consistent unit test generation outputs.

Given the agreement between iterations, we evaluate the performance of a model ensemble.
In this context, the ensemble approach refers to selecting code solutions that successfully pass both the tests generated in the first iteration and those from the second iteration.
The ensemble demonstrated a modest improvement over the second iteration alone, indicating that integrating the outputs from both iterations can enhance overall performance. As a result, we adopt this ensemble method when generating synthetic data for the third iteration for {\sc Sol-Ver}, as reported in Table~\ref{tab:full-performance}.

\begin{table*}[h]
\caption{Case study of iterative improvement made by \textsc{Sol-Ver}'s test generator. \colorbox{pink}{Red highlighted tests} refer to incorrect tests.}
% \vspace{2mm}
\begin{center}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|l|l}
\toprule
Problem &
  \begin{tabular}[c]{@{}l@{}}Write a function that takes an integer number of seconds\\as input and returns the number of minutes in that time,\\disregarding any remaining seconds.\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}Write a function that calculates and returns the greatest\\common divisor (GCD) of two integers using the Euclidean\\algorithm, with the output formatted as "GCD(a, b)= result".\end{tabular} \\\midrule
Iter 1 &
  \begin{tabular}[c]{@{}l@{}}\texttt{assert minutes\_in(1) == 0}\\ \texttt{\colorbox{pink}{assert minutes\_in(3660 + 60 + 60 + 1) == 3}}\\ \texttt{\colorbox{pink}{assert minutes\_in(3660 + 60 + 60 + 60 + 60 + 1) == 5}}\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}\texttt{\colorbox{pink}{assert pgcd(8, 7) == 1}}\\ \texttt{\colorbox{pink}{assert pgcd(20, 25) == 5}}\\ \texttt{\colorbox{pink}{assert pgcd(14, 6) == 2}}\end{tabular} \\
\midrule
Iter2 &
  \begin{tabular}[c]{@{}l@{}}\texttt{assert minutes\_in(1) == 0}\\ \texttt{\colorbox{pink}{assert minutes\_in(3660 + 60 + 60 + 1) == 61}}\\ \texttt{\colorbox{pink}{assert minutes\_in(3660 + 60 + 60 + 60 + 60 + 1) == 61}}\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}\texttt{\colorbox{pink}{assert pgcd(8, 7) == "PGCD(8,7) = 1"}}\\ \texttt{\colorbox{pink}{assert pgcd(20, 25) == "PGCD(20,25) = 5"}}\\ \texttt{\colorbox{pink}{assert pgcd(14, 6) == "PGCD(14,6) = 2"}}\end{tabular} \\
\midrule
Iter3 &
  \begin{tabular}[c]{@{}l@{}}\texttt{assert minutes\_in(1) == 0}\\ \texttt{assert minutes\_in(3660 + 60 + 60 + 1) == 63}\\ \texttt{assert minutes\_in(3660 + 60 + 60 + 60 + 60 + 1) == 65}\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}\texttt{assert pgcd(8, 7) == "GCD(8, 7) = 1"}\\ \texttt{assert pgcd(20, 25) == "GCD(20, 25) = 5"}\\ \texttt{assert pgcd(14, 6) == "GCD(14, 6) = 2"}\end{tabular} \\
\bottomrule
\end{tabular}}
\end{center}
\label{tab:case-study}
\end{table*}
