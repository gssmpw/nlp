\section{Related Work}
% \paragraph{Synthetic Data Generation for Coding}
Various empirical studies applying scaling laws to the training of foundation models have highlighted the critical role of the data size~\cite{kaplan2020scaling,hoffmann2022training}. To address the need for larger datasets, synthetic data generation has become a popular and cost-effective solution, which leverages advanced LLMs to produce high-quality data.
One notable method is {\sc Self-Instruct}~\cite{wang2023self}, which employs a pre-trained LLM to generate instruction-output pairs from a small seed dataset.

In the realm of code, previous work commonly devises synthetic coding instructions using a stronger teacher model (e.g., ChatGPT or GPT-4) and then finetunes a weaker student model (e.g., {\sc CodeAlpaca}~\cite{codealpaca} and {\sc CodeLlama}~\cite{roziere2023code}) with the generated data to distill knowledge from the teacher. For example, code alpaca consists of 20K automatically generated code instructions by applying {\sc Self-Instruct} on ChatGPT using 21 seed tasks. To further enhance the code abilities of LLMs, \citet{luo2023wizardcoder} proposes \textit{Code Eval-Instruct} that employs various heuristics to increase the complexity of seed code instructions. Magicoder~\cite{wei2024magicoder} proposes to generate new coding problems by drawing inspiration from random snippets collected from open source code.


While previous work has shown significant improvements for models trained on data generated by larger, more competent models, training an LLM on its own generated data is not helpful and can even degrade performance~\cite{zhou2024lima}. Therefore, to prevent the model from learning errors present in its own generated data, some post-processing steps are essential. For example, Llama 3.1~\cite{dubey2024llama} utilizes error feedback from test execution and adopts an iterative self-correction procedure to revise potential errors.
% Add CodeDPO here (02/03):
CodeDPO~\cite{zhang2024codedpo} replaces teacher models with a self-generation-and-validation process that uses a PageRank-like algorithm to rank code snippets by correctness and efficiency, yielding diverse preference optimization data without external resources.

% I feel our work is very similar to AutoIF (https://arxiv.org/pdf/2406.13542), but they do not focus on code genration tasks.

In this work, we propose leveraging both positive and negative examples generated by the model, treating pairs of passing and failing responses as chosen-rejected pairs for Direct Preference Optimization (DPO)~\cite{rafailov2024direct}. Note that our method is complementary to self-correction, rather than orthogonal. By improving the quality of unit tests, our framework enhances the accuracy of unit test execution feedback, and thereby can benefit self-correction scenarios as well. While CodeDPO also employs a self-verification method, it is fundamentally constrained by the quality of its generated tests. In contrast, our approach simultaneously improves the generation of both code and tests in a self-play method, mitigating this bottleneck and enabling more robust training.
% no experiment has show this.