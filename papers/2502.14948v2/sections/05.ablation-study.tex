\section{Ablation Study}
\subsection{Prompt and Test Analysis}
\textbf{Prompt Coverage Analysis}\ \ \ To analyze the domain coverage of our generated coding problem set, in Figure~\ref{fig:embedding-plot}, we visualize the embedding distributions of our synthetic problem descriptions with those from established coding benchmarks, including MBPP, APPS, and LiveCodeBench.
Specifically, we use Gecko, a compact and versatile text embedding model distilled from LLMs~\cite{lee2024gecko} for obtaining the sentence embeddings.
% By mapping these embeddings into a common feature space, we can effectively compare the semantic and structural characteristics of our generated problems with those from existing datasets.
The embedding distribution plot reveals that our synthetic problem set exhibits a broad and diverse coverage, encompassing a wide range of topics, difficulty levels, and programming paradigms present in the compared benchmarks.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{figs/embedding-plot.pdf}
    \caption{Prompt distribution comparison with other standard coding benchmarks. We use Principal Component Analysis (PCA) for embedding dimension reduction.}
    \label{fig:embedding-plot}
\end{figure}

\textbf{Progress Analysis per Iteration}\ \ \ To evaluate the iterative advancements of our model, we present a case study on test generation across iterations in Table~\ref{tab:case-study}. The results illustrate that \textsc{Sol-Ver} progressively refines its test generation for the same set of coding problems, thereby enhancing the quality of the synthetic data. These enhancements include the generation of more accurate expected values and better adherence to required format specifications.
Additionally, we monitor the execution results at each iteration and display the distribution of pass and error rates in Figure~\ref{fig:error-dist}. As shown, the pass rate increases with each iteration, primarily due to a reduction in assertion errors, indicating an improvement in the accuracy of the predicted expected outputs.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figs/error-dist.pdf}
    \caption{Pass or error distribution of synthetic data generated at each iteration.}
    \label{fig:error-dist}
\end{figure}


\begin{table}[h]
\caption{Code generation performance for different settings of the scoring function for selecting DPO pairs. Results indicate better results are obtained with less, but higher quality, data ($\epsilon>0$).}
\begin{center}
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|cccc}
\toprule
$\epsilon$    & \multicolumn{1}{c}{$> 0$} & \multicolumn{1}{c}{$> 0.5$} & \multicolumn{1}{c}{$ > 0.75$} & \multicolumn{1}{c}{\textsc{Sol-Ver}}\\
\midrule
Data Size &25,525 & 20,457 & 13,158 & 12,525\\
MBPP          & \multicolumn{1}{c}{36.00} & \multicolumn{1}{c}{37.00}   & \multicolumn{1}{c}{\textbf{40.80}}  &   \multicolumn{1}{c}{\textbf{40.80}} \\
LiveCodeBench & \multicolumn{1}{c}{22.41} & \multicolumn{1}{c}{25.75}   & \multicolumn{1}{c}{\textbf{26.18}}   & \multicolumn{1}{c}{25.97}  \\
\hline \hline
$\text{Score}(C^-, T)$ & Random & Lowest & Median & \textsc{Sol-Ver}\\
\midrule
MBPP                   & \textbf{40.80}  & 39.00  & 38.60  & \textbf{40.80}\\
LiveCodeBench          & \textbf{26.18}  & \textbf{26.18}  & 25.98  & 25.97\\
\bottomrule
\end{tabular}}
\end{center}
\label{tab:scoring-func}
\end{table}

\subsection{Discussion on the Scoring Function}
In Section~\ref{sec:problem-formulation}, we define the scoring function for selecting the chosen / rejected solution-test pair as a binary function.
In our experiments for Iter 1, we find that only 45\% examples can get agreed solution-test pairs, where there is at least one code generated by the model will pass all the generated test. Consequently, the total number of preference tuning pairs is limited by the number of these selected examples.

To explore whether we can utilize the rest of the data where the pass rate is not necessarily 100\%, but is still high enough to rely on, we conduct a series of experiments to discuss the potential of using a \textit{soft pass rate} for selecting preference pairs. Specifically, we change the chosen / rejected pair as $(C^-, C^+)$, where for the same test suite $\mathbf{T}$, $\text{Score}(C^+, \mathbf{T}) > \text{Score}(C^-, \mathbf{T})$, and $\text{Score}(C^+, \mathbf{T}) \geq \epsilon$, where $\epsilon$ is a threshold to determine above which pass rate the test set is relatively reliable. For simplicity, we discuss three cases for $\epsilon$: (1) $\epsilon > 0$ (can be any number); (2) $\epsilon > 0.5$; (3) $\epsilon > 0.75$. For $\text{Score}(C^-, \mathbf{T})$, we also consider three cases: (1) $\text{Score}(C^-, \mathbf{T})$ is a random score; (2) $\text{Score}(C^-, \mathbf{T})$ is the lowest score among all sampling candidate; (3) $\text{Score}(C^-, \mathbf{T})$ is the median score from the lowest to $\text{Score}(C^+, \mathbf{T})$.

In Table~\ref{tab:scoring-func}, we first present various $\epsilon$ settings for assigning $\text{Score}(C^-, \mathbf{T})$ as a random score. After identifying the optimal setting from our results ($\epsilon > 0.75$), we examine the impact on $\text{Score}(C^-, \mathbf{T})$. The findings reveal that randomly selecting the threshold for a high-quality test set significantly degrades performance, despite an increase in data size. This underscores that the quality of synthetic data is more critical than its quantity. Regarding $\text{Score}(C^-, \mathbf{T})$, we found that its impact is less sensitive compared to $\epsilon$. Considering the overall performance, we have chosen to retain the original settings for \textsc{Sol-Ver} to maintain simplicity. We also encourage future research to explore the potential of scoring functions design in more diverse ways.
