\section{Conclusion}

In this work, we introduced {\sc Sol-Ver}, a novel self-play solver-verifier framework that iteratively enhances both code and unit test generation capabilities of large language models (LLMs). By enabling the model to act simultaneously as a solver and a verifier, our approach effectively leverages synthetic data to bridge the critical gap between code generation and unit test generation. Experimental results on the Llama 3.1 8B model demonstrate significant improvements in generating both high-quality code solutions and corresponding unit tests without the necessity of human-annotated data or reliance on larger teacher models. {\sc Sol-Ver} not only advances the state-of-the-art in automated code and test generation but also offers a scalable solution that can be adapted to various programming tasks.

There are several avenues for further enhancement of {\sc Sol-Ver}. Future work could explore more controlled input generation techniques to optimize the relevance and quality of synthetic data. Additionally, extending the framework to accommodate more complex coding scenarios beyond function-level generation can broaden its applicability.
Further optimization of the iterative training process could also reduce computational overhead, making the approach more efficient.
Finally, evaluating the framework on larger and more diverse models will help to determine its generalizability and effectiveness across different architectures.

% \newpage
% \section*{Impact Statement}
% This paper presents work aimed at advancing the field of machine learning by enhancing large language models’ capabilities in code and test generation through the {\sc Sol-Ver} framework. By improving the verification ability of LLMs, our approach enables the development of more accurate and reliable software solutions, thereby increasing productivity and reducing the likelihood of errors across diverse coding scenarios. The iterative self-play process, which leverages the model itself to verify and enhance its outputs, eliminates the need for extensive human annotations or larger teacher models, making the methodology both efficient and scalable. This advancement has the potential to democratize access to sophisticated coding tools, empowering a broader range of developers and industries to innovate and create robust applications. Additionally, enhanced automated coding capabilities can accelerate software development cycles, foster educational initiatives in programming, and support the creation of more complex and reliable software systems. 


%As a result, the {\sc Sol-Ver} framework can further , enabling more effective and intelligent software engineering practices that can drive future advancements in various sectors.
%{\sc Sol-Ver} are built based on open-source models and datasets (all available online). We do not anticipate any major ethical concerns.

% Authors are required to include a statement of the potential broader impact of their work, including its ethical aspects and future societal consequences. This statement should be in a separate section at the end of the paper (co-located with Acknowledgements, before References), and does not count toward the paper page limit. In many cases, where the ethical impacts and expected societal implications are those that are well established when advancing the field of Machine Learning, substantial discussion is not required, and a simple statement such as: 

% “This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.”

% The above statement can be used verbatim in such cases, but we encourage authors to think about whether there is content which does warrant further discussion, as this statement will be apparent if the paper is later flagged for ethics review.

% In particular, Reviewers and ACs may flag submissions for ethics review. Flagged submissions will be sent to an ethics review committee for comments. Ethics reviewers do not have the authority to reject papers, but in extreme cases papers may be rejected by the program chairs on ethical grounds, regardless of scientific quality or contribution.

% Note: Impact Statements are optional for papers submitted to the position paper track this year.