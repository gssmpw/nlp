Reinforcement Learning from Human Feedback and its variants excel in aligning with human intentions to generate helpful, harmless, and honest responses. 
However, most of them rely on costly human-annotated pairwise comparisons for supervised alignment, which is not suitable for list-level scenarios, such as community question answering.
Additionally, human preferences are influenced by multiple intrinsic factors in responses, leading to decision-making inconsistencies.
Therefore, we propose \textbf{Se}lf-supervised \textbf{A}ttribute-aware \textbf{d}ynamic \textbf{p}reference \textbf{ra}nking, called \shortname. \
It quantifies preference differences between responses based on Attribute-Perceptual Distance Factors (APDF) and dynamically determines the list-wise alignment order.
Furthermore, it achieves fine-grained preference difference learning and enables precise alignment with the optimal one.
We specifically constructed a challenging code preference dataset named StaCoCoQA, and introduced more cost-effective and scalable preference evaluation metrics: PrefHit and PrefRecall.
Extensive experimental results show that SeAdpra exhibits superior performance and generalizability on both StaCoCoQA and preference datasets from eight popular domains.
% \footnote{Our dataset and project codes are accessible https://anonymous.4open.science/r/SeAdpra-D8E0}.

% Community Question Answering (CoQA) aims to generate semantically accurate responses that align with the preferences of the community users.  
% Reinforcement Learning from Human Feedback precisely controls the behavior of large language models, facilitating human-like response generation. 
% However, its application in CoQA remains underexplored.
% Existing methods mostly rely on manually labeled pairwise comparisons for supervised alignment, which is costly and ignores the attributes of the responses, and list-wise ranking methods are relatively rare.
% Moreover, the different attributes of list-wise responses on a community can lead to inconsistent preferences.
% To address these issues, we propose an \textbf{Se}lf-supervised \textbf{A}ttribute-aware \textbf{d}ynamic \textbf{p}reference \textbf{ra}nking, called \shortname. 
% It quantifies preference differences between responses based on Attribute-Perceptual Distance Factors (APDF), enabling preference difference learning on chains and precise alignment with the optimal.
% We specifically constructed a preference dataset for the code domain, named StaCoCoQA, and introduced more cost-effective and scalable preference evaluation metrics: PrefHit and PrefRecall. Extensive experimental results show that \shortname ~ exhibits superior performance and generalizability on both StaCoCoQA and preference datasets from eight popular domains\footnote{Our dataset and project codes are accessible https://anonymous.4open.science/r/SeAdpra-D8E0}.