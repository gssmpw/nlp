\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{latex/pic/intro1.pdf}
    \caption{Which response should the LLMs align with? In the code community, each response has different attributes such as semantics, popularity, and timeliness, leading to potentially different optimal responses.}
    \label{intro}
        % \vspace{-0.2cm}
\end{figure}
Community Question Answering (CoQA) \cite{romeo2018flexible,wu2018question} seeks to generate responses that are semantically accurate and 
match the preferences of community members.
Currently, Reinforcement Learning from Human (or AI) Feedback (RLHF/RLAIF) \cite{christiano2017deep, bai2022constitutional} has enabled precise control of large language models (LLMs) for generating human-like responses \cite{stiennon2020learning, ouyang2022training}. 
However, applying it to CoQA remains underexplored.
Moreover, human preferences do not always follow a singular, value-based hierarchy. 
Decision-making can be influenced by various factors and may exhibit inconsistencies \cite{tversky1969intransitivity}, which undoubtedly presents a challenge for aligning LLMs with CoQA.


Existing methods are limited to pairwise comparison (one chosen and one rejected), such as reward model-based RLHF \cite{ouyang2022training}, offline supervised Direct Preference Optimization (DPO) \cite{rafailov2024direct}, as well as other variants like SLiC \cite{zhao2023slic} and pseudo-list RRHF \cite{yuan2024rrhf} that adopt pairwise hinge loss.
However, a real-world prompt may have multiple high-quality responses \cite{cui2023ultrafeedback}.
For example, in the coding community, the optimal one may vary with thier different attributes, such as semantics, popularity, and timeliness, as illustrated in Figure~\ref{intro}. 
Recently, some alignment methods have attempted to rank multiple preferred candidates.
PRO \cite{song2024preference} introduces a list-level maximum likelihood estimation loss to shift towards preference ranking but overlooks the attributes of responses. 
LiPO \cite{liu2024lipo} directly optimizes list-based ranking preferences and begins to address response labels, but has not yet addressed the integration of multiple labels.
Moreover, these supervised learning methods depend on human or AI annotations of preference pairs or lists to specify the best responses for alignment. However, preference data are relatively scarce and expensive to collect in practice \cite{casper2023open}.


% Earlier, RLHF \cite{ouyang2022training,stiennon2020learning,christiano2017deep} fitted a reward model to human preference datasets and then used Proximal Policy Optimization (PPO) \cite{schulman2017proximal} to optimize the language model policy (\(\pi_{Ref}\)) to generate responses that receive high rewards. To circumvent the complexities of the RLHF, the popular offline supervised Direct Preference Optimization (DPO) \cite{rafailov2024direct}, under the pair-wise Bradley-Terry assumption \cite{bradley1952rank} or list-wise Plackett-Luce model \cite{luce1959individual}, replaced rewards with the relative logistic probabilities of responses; the higher the relative log probability, the greater the preference.
% Parallel to this, SLiC \cite{zhao2023slic} and pseudo-list RRHF \cite{yuan2024rrhf} both use pair-wise hinge loss to align policy responses.
% These methods\cite{liu2024lipo,azar2024general,guo2024direct} are limited to single pair-wise responses for each prompt (one chosen and one rejected). 
% However, there might be multiple high-quality responses for a single prompt \cite{cui2023ultrafeedback}.
% To better align LLMs with CoQA
To address the above issues, we propose \shortname, a \textbf{Se}lf-supervised \textbf{a}ttribute-aware \textbf{d}ynamic \textbf{p}reference \textbf{ra}nking framework. It consists of three stages.
% It focuses on the intrinsic attributes through Attribute-Perceptual Distance Factors (APDF), quantifying the preference-level differences among multiple responses,
First, the Multi-Attribute Perception quantifies preference-level differences through Attribute-Perceptual Distance Factors (APDF), enabling the integration of multiple attributes for self-supervised dynamic ranking.
Second, the Perception Alignment aims to quickly adapts to domain knowledge and achieve precise alignment by aligning the optimal.
Third, the Perceptual Comparison performs multiple iterative comparisons on all candidates to learn on-chain preference differences. 
% In each iteration, it maximizes the reward for the optimal response and minimizes the penalty for the remaining responses based on preference levels.

% To accommodate improvements to our method and the expansion of comparison benchmarks, while 

For enhancing the cost-efficiency and domain applicability of the preference evaluation scheme, we propose new metrics that follow the 'CSTC' criterion (details in Appendix \ref{sec::cstc}), as an alternative to the costly win rate \cite{dudik2015contextual}, namely PrefHit and PrefRecall.
They can accommodate the expansion of benchmarks.
Aiming to validate the effectiveness of SeAdpra in specific domains, we have constructed a programming CoQA preference dataset, called StaCoCoQA, which contains over 60,738 programming directories and 9,978,474 entries. 
Our main contributions are as follows:

% \noindent (1) We introduce the Attribute Perceptual Distance Factor (APDF) to gauge the in preference-level gaps of multiple responses, replacing the binary judgment of preferred versus non-preferred. We propose an self-supervised dynamic preference ranking framework that achieves label-free list-wise preference alignment.

% \noindent (2) We present the StaCoCoQA, a large-scale, high-quality, real-time (as of May 2024) dataset for preference alignment in programming CoQA, and develop two new alignment metrics abided by the 'CSTC' criterion.

% \noindent (3) We conducted extensive experiments on eight hot public datasets and StaCoCoQA, providing a reference benchmark. 
% Along with the increase in safety, the experimental results demonstrate the superiority of the alignment of \shortname.
% \item We conducted extensive experiments on eight public datasets (such as Security, Chemistry, Cooking, Academia, etc.) and StaCoCoQA, providing a reference benchmark. Along with the increase in safety, the experimental results demonstrate the superiority of the preference alignment of \shortname.

\begin{itemize}[leftmargin=*]
\item We introduce the Attribute Perceptual Distance Factor (APDF) to gauge the in preference-level gaps of multiple responses, replacing the binary judgment of preferred versus non-preferred. We propose an self-supervised dynamic preference ranking framework that achieves label-free list-wise preference alignment.
\item We present the StaCoCoQA, a large-scale, high-quality, real-time (as of May 2024) dataset for preference alignment in programming CoQA, and develop two new alignment metrics abided by the 'CSTC' criterion.
\item We conducted extensive experiments on eight hot public datasets and StaCoCoQA, providing a reference benchmark. 
The experimental results demonstrate that \shortname \ excels in alignment while maintaining safety. 
% Additionally, we analyzed the effectiveness of the new metrics from multiple perspectives.
% \item We conducted extensive experiments on eight public datasets (such as Security, Chemistry, Cooking, Academia, etc.) and StaCoCoQA, providing a reference benchmark. Along with the increase in safety, the experimental results demonstrate the superiority of the preference alignment of \shortname.
\end{itemize}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{latex/pic/dataset2.pdf}
    \caption{Showcasing the top-15 primary programming language categories in StaCoCoQA.}
    \label{fig:dataset}
    \vspace{-0.2cm}
\end{figure}
